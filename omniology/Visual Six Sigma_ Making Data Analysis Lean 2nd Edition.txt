Visual
Six Sigma

Wiley & SAS Business
Series
The Wiley & SAS Business Series presents books that help senior-level managers with their
critical management decisions.
Titles in the Wiley & SAS Business Series include:
Agile by Design: An Implementation Guide to Analytic Lifecycle Management by Rachel Alt-
Simmons
Analytics in a Big Data World: The Essential Guide to Data Science and Its Applications by Bart
Baesens
Bank Fraud: Using Technology to Combat Losses by Revathi Subramanian
Big Data, Big Innovation: Enabling Competitive Differentiation through Business Analytics by
Evan Stubbs
Business Forecasting: Practical Problems and Solutions edited by Michael Gilliland, Len
Tashman, and Udo Sglavo
Business Intelligence Applied: Implementing an Effective Information and Communications Tech-
nology Infrastructure by Michael Gendron
Business Intelligence and the Cloud: Strategic Implementation Guide by Michael S. Gendron
Business Transformation: A Roadmap for Maximizing Organizational Insights by Aiman Zeid
Data-Driven Healthcare: How Analytics and BI Are Transforming the Industry by Laura Madsen
Delivering Business Analytics: Practical Guidelines for Best Practice by Evan Stubbs
Demand-Driven Forecasting: A Structured Approach to Forecasting, Second Edition by Charles
Chase
Demand-Driven Inventory Optimization and Replenishment: Creating a More Efﬁcient Supply
Chain by Robert A. Davis
Developing Human Capital: Using Analytics to Plan and Optimize Your Learning and Development
Investments by Gene Pease, Barbara Beresford, and Lew Walker
Economic and Business Forecasting: Analyzing and Interpreting Econometric Results by John
Silvia, Azhar Iqbal, Kaylyn Swankoski, Sarah Watt, and Sam Bullard
Financial
Institution
Advantage
and
the
Optimization
of
Information
Processing
by
Sean C. Keenan
Financial Risk Management: Applications in Market, Credit, Asset, and Liability Management and
Firmwide Risk by Jimmy Skoglund and Wei Chen
Fraud Analytics Using Descriptive, Predictive, and Social Network Techniques: A Guide to Data
Science for Fraud Detection by Bart Baesens, Veronique Van Vlasselaer, and Wouter Verbeke
Harness Oil and Gas Big Data with Analytics: Optimize Exploration and Production with Data
Driven Models by Keith Holdaway
Health Analytics: Gaining the Insights to Transform Health Care by Jason Burke
Heuristics in Analytics: A Practical Perspective of What Inﬂuences Our Analytical World by Carlos
Andre, Reis Pinheiro, and Fiona McNeill
Hotel Pricing in a Social World: Driving Value in the Digital Economy by Kelly McGuire
Implement, Improve and Expand Your Statewide Longitudinal Data System: Creating a Culture of
Data in Education by Jamie McQuiggan and Armistead Sapp
Killer Analytics: Top 20 Metrics Missing from your Balance Sheet by Mark Brown
Mobile Learning: A Handbook for Developers, Educators, and Learners by Scott McQuiggan,
Lucy Kosturko, Jamie McQuiggan, and Jennifer Sabourin

The Patient Revolution: How Big Data and Analytics Are Transforming the Healthcare Experience
by Krisa Tailor
Predictive Analytics for Human Resources by Jac Fitz-enz and John Mattox II
Predictive Business Analytics: Forward-Looking Capabilities to Improve Business Performance by
Lawrence Maisel and Gary Cokins
Statistical Thinking: Improving Business Performance, Second Edition by Roger W. Hoerl and
Ronald D. Snee
Too Big to Ignore: The Business Case for Big Data by Phil Simon
Trade-Based Money Laundering: The Next Frontier in International Money Laundering Enforce-
ment by John Cassara
The Visual Organization: Data Visualization, Big Data, and the Quest for Better Decisions by
Phil Simon
Understanding the Predictive Analytics Lifecycle by Al Cordoba
Unleashing Your Inner Leader: An Executive Coach Tells All by Vickie Bevenour
Using Big Data Analytics: Turning Big Data into Big Money by Jared Dean
Visual Six Sigma, Second Edition by Ian Cox, Marie Gaudard and Mia Stephens
For more information on any of the above titles, please visit www.wiley.com.

Visual
Six Sigma
Making Data Analysis Lean
Ian Cox
Marie A. Gaudard
Mia L. Stephens
Second Edition

Copyright © 2016 by SAS Institute, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, (978) 750-8400, fax (978) 646-8600, or on the Web at www.copyright.com. Requests
to the Publisher for permission should be addressed to the Permissions Department, John Wiley &
Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online
at http://www.wiley.com/go/permissions.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best
efforts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and speciﬁcally disclaim any implied
warranties of merchantability or ﬁtness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any other
commercial damages, including but not limited to special, incidental, consequential, or other
damages.
For general information on our other products and services or for technical support, please
contact our Customer Care Department within the United States at (800) 762-2974, outside the
United States at (317) 572-3993 or fax (317) 572-4002.
Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some
material included with standard print versions of this book may not be included in e-books or in
print-on-demand. If this book refers to media such as a CD or DVD that is not included in the
version you purchased, you may download this material at http://booksupport.wiley.com. For
more information about Wiley products, visit www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Names: Cox, Ian, 1956–
Title: Visual six sigma : making data analysis lean / Ian Cox,
Marie A. Gaudard, Mia L. Stephens.
Description: Second edition. | Hoboken : Wiley, 2016. | Series: Wiley and SAS
business series | Revised edition of Visual six sigma, 2010. | Includes index.
Identiﬁers: LCCN 2016001878 (print) | LCCN 2016003459 (ebook) |
ISBN 9781118905685 (hardback) | ISBN 9781119222262 (epdf) |
ISBN 9781119222255 (epub)
Subjects: LCSH: Six sigma (Quality control standard) | Decision support
systems. | Decision making—Statistical methods. | Organizational
effectiveness. | BISAC: BUSINESS & ECONOMICS / Strategic Planning.
Classiﬁcation: LCC HD30.213 .C69 2016 (print) | LCC HD30.213 (ebook) |
DDC 658.4/013—dc23
LC record available at http://lccn.loc.gov/2016001878
Cover Design: Wiley
Cover Image: ©Studio-Pro/iStock.com
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

Contents
Preface to the Second Edition
ix
Preface to the First Edition
xiii
Acknowledgments
xv
About the Authors
xvii
PART ONE
BACKGROUND
1
Chapter 1
Introduction
3
Chapter 2
Six Sigma and Visual Six Sigma
7
Chapter 3
A First Look at JMP
27
Chapter 4
Managing Data and Data Quality
67
PART TWO
CASE STUDIES
101
Chapter 5
Reducing Hospital Late Charge Incidents
103
Chapter 6
Transforming Pricing Management in a Chemical
Supplier
157
Chapter 7
Improving the Quality of Anodized Parts
223
Chapter 8
Informing Pharmaceutical Sales and Marketing
297
Chapter 9
Improving a Polymer Manufacturing Process
345
Chapter 10
Classiﬁcation of Cells
437
PART THREE
SUPPLEMENTARY MATERIAL
509
Chapter 11
Beyond “Point and Click” with JMP
511
Index
539
vii

Preface to the Second
Edition
The ﬁrst edition of this book appeared in 2010, so we decided to produce
an updated and expanded second edition. The purpose of the book remains
unchanged—to show how, using the three principles of Visual Six Sigma,
you can exploit data to make better decisions more quickly and easily than you
would otherwise. And, as you might expect given their power and utility, these
principles are also unchanged. However, production of this second edition
allows us to take advantage of some interim developments that make the
implementation of Visual Six Sigma even easier, further increasing the scope
and efﬁcacy of its application. It also allows us to improve and enhance the
content and form of the ﬁrst edition.
The staying power of Six Sigma as a methodology can be attributed to the
fact that it can provide a common language for, and approach to, project-based
improvement initiatives. Nonetheless, as we pointed out in the ﬁrst edition,
there is a clear need to evolve the mechanics of Six Sigma both to accommo-
date the greater availability of data and to address the fact that, historically,
approaches to analyzing data were overly concerned with hypothesis testing, to
the detriment of the hypothesis generation and discovery needed for improve-
ment. We believe that Visual Six Sigma can foster this evolution, and this is part
of our motivation for keeping this text current.
At the same time, the past ﬁve years have seen the explosion of “big data,”
at least as an identiﬁable area that software providers and implementation con-
sultants make strenuous efforts to market to. In this language, the increased
data availability mentioned above is measured using three dimensions: volume,
variety, and velocity. Even though the precise deﬁnition of big data is not always
clear, we think there is much for would-be data scientists to learn from the prin-
ciples of Visual Six Sigma and their application. In addition, if a project-based
approach is warranted, the language of Six Sigma may also be useful.
Although the principles of Visual Six Sigma are general, their effective and
efﬁcient adoption in practice is reliant on good enabling software. The ﬁrst edi-
tion was tied to version 8.01 of JMP, Statistical Discovery software from SAS
Institute®. This second edition has been revised to be consistent with the ver-
sion current at the time of writing, JMP 12.2.0. Generally, JMP aims to exploit
the synergy between visualization and analysis, and its continuing development
has opened up new possibilities for Visual Six Sigma. In some cases, these are
simply matters of detail and efﬁciency, but in others there are important new
capabilities we can use.
ix

x
P R E F A C E T O T H E S E C O N D E D I T I O N
A key feature of the book remains the six self-contained case studies. Given
feedback from the ﬁrst edition, we are even more convinced of the advantage
of this format in showing how seemingly disparate techniques can be used in
concert to accomplish something useful. We interweave the new capabilities of
JMP where they usefully support or extend the case studies.
Consistent with the requirements of Visual Six Sigma in the new era of big
data, we have introduced two new chapters:
◾Chapter 4, “Managing Data and Data Quality,” precedes the case studies
and addresses the management of data and data quality. Data quality, at
an organizational level, is a ubiquitous topic that is often seen as main-
stream to the point of being boring. However, the importance of data
quality for project teams and anyone making decisions with data can-
not be overstated. As we shall see, the Visual Six Sigma context leads to
some important and interesting nuances.
◾Chapter 11, “Beyond ‘Point and Click’ with JMP,” follows the case
studies and shows how to go beyond the interactive usage of JMP for
discovery and improvement. No matter how simple or complex, the
performance of empirical models always degrades over time. Once
improvements are made, there is always the need to monitor and adapt
with an appropriate frequency. In turn, this means that analyses need
to be repeated as new data arrive, and this is often best done with an
element of automation.
The case studies appear in Part Two of the book. Chapter 4 is appended
to Part One, making this section four chapters long. Given the nature of the
content, Chapter 11 appears as a singleton chapter in Part Three.
Finally, we have tried to make the case studies easier to use by having clearer
typographic separation between the narrative (consisting of the why, the what,
and the ﬁndings of each technique as it is used in a speciﬁc context) and the
“how to” steps required in JMP. As well as helping to keep things concise, this
arrangement better accommodates users with different levels of prior familiarity
with JMP, and may make it easier to use other software should this be required
or mandated.
As in the ﬁrst edition, we have used different fonts to help identify the
names of data tables, of columns in data tables, and commands. Data table
names are shown in MeridienLTStd-Bold, the names of columns (which are
variable names) are shown in italic Helvetica, and the names of commands and
other elements of the user interface are shown in bold Helvetica.
We are now living through a time of rapid change in the world of data
analysis. We have tried to reﬂect this in our changes and additions. We hope
that this second edition on Visual Six Sigma contains even more of interest for
current or would-be Six Sigma practitioners, or more generally for anyone with

P R E F A C E T O T H E S E C O N D E D I T I O N
xi
a stake in exploiting data for the purpose of gaining new understanding or of
driving improvement.
Supplemental Materials
We anticipate that you will follow along, using JMP, as you work through the
case studies and Chapters 4 and 11. You can download a trial copy of JMP
at www.jmp.com/try. Chapter 10 requires JMP Pro. You can request a trial
version of JMP Pro at www.jmp.com/en_us/software/jmp-pro-eval.html. JMP
instructions in this book are based on JMP 12.2.0. Although the menu structure
may differ if you use a different version of JMP, all the functionality described
in this book is available in JMP 12.2.0 or newer versions.
The data sets used in the book are available at http://support.sas.com/
visualsixsigma. This folder contains a journal ﬁle, Visual Six Sigma.jrn, that
contains links to the data tables, scripts, and add-ins discussed in this book. The
color versions of the exhibits shown in the book are also available here. Exhibits
showing JMP results were taken using JMP 12.2.0 running on Windows.

Preface to the First Edition
The purpose of this book is to show how, using the principles of Visual Six
Sigma, you can exploit data to make better decisions more quickly and easily
than you would otherwise. We emphasize that your company does not need to
have a Six Sigma initiative for this book to be useful. Clearly there are many
data-driven decisions that, by necessity or by design, fall outside the scope of a
Six Sigma effort, and in such cases we believe that Visual Six Sigma is ideal. We
seek to show that Visual Six Sigma can be used by a lone associate, as well as a
team, to address data-driven questions, with or without the support of a formal
initiative like Six Sigma.
To this end, we present six case studies that show Visual Six Sigma in
action. These case studies address complex problems and opportunities faced
by individuals and teams in a variety of application areas. Each case study
was addressed using the Visual Six Sigma Roadmap, described in Chapters 2
and 3. As these case studies illustrate, Visual Six Sigma is about exploration
and discovery, which means that it is not, and never could be, an entirely
prescriptive framework.
As well as using the case studies to convey the Visual Six Sigma Roadmap,
we also want to use them to illustrate Visual Six Sigma techniques that you can
reuse in your own setting. To meet this goal, sometimes we have deliberately
compromised the lean nature of the Visual Six Sigma Roadmap in order to take
the opportunity to show you extra techniques that may not be strictly nec-
essary to reach the conclusion or business decision. Striking the balance this
way means that you will see a wider repertoire of techniques from which to
synthesize an approach to Visual Six Sigma that works for you.
Because of its visual emphasis, Visual Six Sigma opens the doors for
non-statisticians to take active roles in data-driven decision making, empow-
ering them to leverage their contextual knowledge to pose relevant questions,
get good answers, and make sound decisions. You may ﬁnd yourself working
on a Six Sigma improvement project, a design project, a data mining inquiry,
or a scientiﬁc study—all of which require decision making based on data. After
working through this book, we hope that you will be able to make data-driven
decisions in your speciﬁc situation quickly, easily, and with greater assurance.
How This Book Is Organized
This book is organized in two parts. Part I contains an introductory chapter that
presents the three Visual Six Sigma strategies, a chapter on Visual Six Sigma,
xiii

xiv
P R E F A C E T O T H E F I R S T E D I T I O N
and a chapter introducing JMP statistical software (from SAS® Institute), which
will be used throughout the case studies.
Case studies are presented in Part Two. These case studies follow challeng-
ing real-world projects from start to ﬁnish. Through these case studies, you will
gain insight into how the three Visual Six Sigma strategies combine to expedite
project execution in the real world. Each case study is given its own chapter,
which can be read independently from the rest. A concise summary of the story-
line opens each case study. Although these case studies are real, we use ﬁctitious
names for the companies and individuals to preserve conﬁdentiality.
Within each case study, visualization methods and other statistical tech-
niques are applied at various stages in the data analysis process in order to better
understand what the data are telling us. For those not familiar with JMP, each
case study also contains the relevant how-to steps so that you may follow along
and see Visual Six Sigma in action.
The data sets used in the case studies are available at http://support.sas
.com/visualsixsigma. Here you can also ﬁnd the exhibits shown in the case
studies, allowing you to see screen captures in color. Additional Visual Six Sigma
resource materials will be made available on the website, as appropriate.
A Word about Software
The ideas behind Visual Six Sigma are quite general, but active learning—in our
view, the only kind of learning that works—requires that you step through the
case studies and examples in this book to try things out for yourself. For more
information about JMP, and to download a trial version of the software, visit
www.jmp.com/demo.
JMP is available on Windows, Mac, and Linux platforms. The step-by-step
instructions in this book assume that you are working in Windows. Mac and
Linux users should refer to the JMP documentation for details on differences.
This book is based on JMP version 8.0.1.

Acknowledgments
Stating the obvious, this book would not exist without its ﬁrst edition. Even
though some have moved on, we remain deeply indebted to all those listed
who made the ﬁrst edition possible. Most importantly, we want to thank Leo
Wright of SAS and Phil Ramsey of the North Haven Group, LLC, our co-authors
on the ﬁrst edition, who provided some of the original case studies and helped
to make this book possible.
Both editions of the book were substantially improved by suggestions from
Mark Bailey of SAS. We greatly appreciate his time, interest, valuable feed-
back, and insights. We want to thank Andy Liddle, now of Process Insight
Consulting Limited, who assisted with the review of the original version of “Im-
proving a Polymer Manufacturing Process” (now Chapter 9). We also want to
thank Volker Kraft of SAS, who provided valuable feedback in connection with
updates to this case study for the book’s second edition.
This project was greatly facilitated by Stacey Hamilton and Stephenie Joyner
of SAS Publishing. Their support, encouragement, and attention to detail at
every step of this adventure were invaluable.
Finally, we would like to thank Jon Weisz and Curt Hinrichs of JMP for
their support and encouragement in updating this book. And, as before, a spe-
cial thank-you goes to John Sall, Bradley Jones, Chris Gotwalt, Xan Gregg,
Brian Corcoran, and the JMP Development Team for their continuing work
on a visionary product that makes Visual Six Sigma possible.
xv

About the Authors
Ian Cox currently works in the JMP Division of SAS. Before joining SAS in
1999, he worked for Digital Equipment Corporation, Motorola, and BBN Soft-
ware Solutions Ltd. and has been a consultant for many companies on data
analysis, process control, and experimental design. A Six Sigma Black Belt, he
was a Visiting Fellow at Cranﬁeld University and is a Fellow of the Royal Sta-
tistical Society. Cox holds a Ph.D. in theoretical physics.
Marie A. Gaudard is a consultant specializing in statistical training with the
use of JMP. She is currently a statistical writer with the JMP documentation
team. She earned her Ph.D. in statistics in 1977 and was a professor of statistics
at the University of New Hampshire from 1977 until 2004. She has been heavily
involved in statistical consulting since 1981. Gaudard has worked with a vari-
ety of clients in government agencies, medical areas, and manufacturing. She
has extensive experience in consulting and training in the areas of Six Sigma,
Design for Six Sigma, forecasting and demand planning, and data mining.
Mia L. Stephens is an academic ambassador with the JMP division of SAS.
Prior to joining SAS, she was an adjunct professor at the University of New
Hampshire and a partner in the North Haven Group, a statistical training and
consulting company. Also a coauthor of JMP Start Statistics: A Guide to Statistics
and Data Analysis Using JMP, Fifth Edition and Building Better Models with JMP Pro,
she has developed courses and training materials, taught, and consulted within
a variety of manufacturing and service industries. Stephens holds an M.S. in
statistics from the University of New Hampshire.
xvii

Visual
Six Sigma

PART
ONE
Background
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

C H A P T E R 1
Introduction
3
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

4
V I S U A L S I X S I G M A
WHAT IS VISUAL SIX SIGMA?
Visual Six Sigma is about leveraging interactive and dynamic graphical displays
to help transform data into sound decisions. It is not an algorithm. It is a cre-
ative process that employs visual techniques in the discovery of new and useful
knowledge, leading to quicker and better decisions than do the methods in
general use today. It signals a new generation of Six Sigma techniques.
At the heart of Six Sigma is the concept of data-driven decision making, that
is, of exploiting the data from measurements or simulations at various points
in the life cycle of your product or service. Visual Six Sigma aims to produce
better alignment between Six Sigma practice and the key idea of discovery,
providing beneﬁts for all those who have a stake in solving problems and in
making improvements through data.
Visual Six Sigma consists of three main strategies:
1. Using dynamic visualization to literally see the sources of variation in
your data.
2. Using exploratory data analysis techniques to identify key drivers and
models, especially for situations involving many variables.
3. Using conﬁrmatory statistical methods only when the conclusions are
not obvious.
Six Sigma programs often use the so-called DMAIC approach for team-based
process improvement or problem-solving efforts. The acronym DMAIC stands
for the major phases in a team’s project: Deﬁne, Measure, Analyze, Improve, and
Control. DMAIC provides a structure for a team’s efforts, just as an overall Six
Sigma program provides a structure for a company’s efforts. Each phase of
DMAIC comes with a list of techniques that are considered appropriate in that
phase; the team moves from one phase to another, using this sequence of tech-
niques as a general guide. In a similar way, Six Sigma projects aimed at design
follow various structures, such as Deﬁne, Measure, Analyze, Design, and Validate
(DMADV) and Identify, Design, Optimize, and Validate (IDOV).
Visual Six Sigma is not a replacement for the DMAIC, DMADV, or IDOV
frameworks. Rather, Visual Six Sigma supports these frameworks by simplifying
and enhancing methods for data exploration and discovery whenever they are
needed. In addition, when circumstances make a full-blown project-based or
team-based approach undesirable or unworkable, Visual Six Sigma can still be
used by individual contributors such as you. In a nutshell, Visual Six Sigma helps
to make the DMAIC and design structures—and data analysis in general—lean.
Moving beyond Traditional Six Sigma
It is our belief that the tools, techniques, and workﬂows in common use with
Six Sigma efforts are typically not aligned with the key idea of discovery. In

I N T R O D U C T I O N
5
the early days of Six Sigma, relevant data rarely existed, and a team was often
challenged to collect data on its own. As part of the Measure phase, a team usu-
ally conducted a brainstorming session to identify which features of a process
should be measured. In some sense, this brainstorming session was the team’s
only involvement in hypothesis generation. The data collected were precious,
and hypothesis testing methods were critical in separating signals from noise.
Project teams struggling with a lack of useful data generally rely on an abun-
dance of subjective input, and often require hypothesis testing to minimize the
risk of bad decisions. This emphasis on hypothesis testing is reasonable in an
environment where data are sparse. In contrast, today’s Six Sigma teams often
ﬁnd warehouses of data that are relevant to their efforts. Their challenge is to
wade through the data to discover prominent features, to separate the remark-
able from the unremarkable.
These data-rich environments call for a shift in emphasis from conﬁrma-
tory methods, such as hypothesis testing, to exploratory methods, with a major
emphasis on the display of data to reveal prominent features that are hidden
in the data. Since the human interpretation of the data context is a vital part
of the discovery process, these exploratory techniques cannot be fully auto-
mated. Also, with large quantities of data, hypothesis testing itself becomes less
useful—statistical signiﬁcance comes easily and may have little to do with prac-
tical importance.
Of course, the simple abundance of data in a warehouse does not guarantee
its relevance for improvement or problem solving. In fact, it is our experience
that teams working in what they believe to be data-rich environments some-
times ﬁnd that the available data are of poor quality or are largely irrelevant
to their efforts. Visualization methods can be instrumental in helping teams
quickly reach this conclusion. In these cases, teams need to revert to techniques
such as brainstorming, cause-and-effect diagrams, and process maps, which
drive efforts to collect the proper data. But, as we shall see, even in situations
where only few relevant data are available, visualization techniques, supported
as appropriate by conﬁrmatory methods, prove invaluable in identifying telling
features of the data.
Making Data Analysis Lean
Discovery is largely supported by the generation of hypotheses—conjectures
about relationships and causality. Today’s Six Sigma teams, and data analysts
in the business world in general, are often trained with a heavy emphasis on
hypothesis testing, with comparatively little emphasis given to hypothesis gen-
eration and discovery. They are often hampered in their problem-solving and
improvement efforts by the inability to exploit exploratory methods, which
could enable them to make more rapid progress, often with less effort.

6
V I S U A L S I X S I G M A
In recent times, we have seen incredible advances in visualization methods,
supported by phenomenal increases in computing power. We strongly believe
that the approaches now allowed by these methods are underutilized in current
Six Sigma practice. It is this conviction that motivated us to write the ﬁrst edition
of this book and, following its success, to produce a second edition that takes
advantage of recent software advances. We hope you ﬁnd this book useful as
you shape and build your own real-world Six Sigma experience.
Requirements of the Reader
This leads to another important point, namely, that you are “part of the system.”
Discovery, whether practiced as an individual or as a team sport, involves both
divergent and convergent thinking; both creativity and discipline are required
at different times. You should bear this in mind when forming a team or when
consulting with individuals, since each person will bring his or her own skill
set, perspective, and strength to the discovery process.
Given the need to be data driven, we also need to recognize one of the basic
rules of using data, which is that any kind of analysis that treats data simply as a
list of numbers is doomed to failure. To say it differently: All data are contextual,
and it is this context and the objectives set out for the project that must shape
the analysis and produce useful recommendations for action. As a practitioner,
your main responsibility should always be to understand what the numbers in
the data actually mean in the real world. In fact, this is the only requirement
for putting the ideas in this book into practice in your workplace.

C H A P T E R 2
Six Sigma and Visual
Six Sigma
7
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

8
V I S U A L S I X S I G M A
T
his chapter introduces the key ideas behind Six Sigma and Visual Six Sigma;
our focus is on the latter. Six Sigma is a potentially huge topic, so we only
have space to mention some of its essential ideas. There are already numer-
ous well-written books and articles dealing with the many and diverse aspects
of Six Sigma as commonly practiced.1 We also note that today, digital tools
(software, databases, visual media, etc.) are leveraged extensively in Six Sigma
initiatives.2
Our goal in this chapter is to provide an overview of Six Sigma so that you
start to see how Visual Six Sigma ﬁts into this picture. However, it is worth
pointing out in advance that you can only gain a proper appreciation of the
power of visualization techniques by working with data that relate to real prob-
lems in the real world.
BACKGROUND: MODELS, DATA, AND VARIATION
There is no doubt that science and technology have transformed the lives of
many and will continue to do so. Like many ﬁelds of human endeavor, science
proceeds by building pictures, or models, of what we think is happening. These
models can provide a framework in which we attempt to inﬂuence or control
inputs so as to provide better outputs. Unlike the models used in some other
areas, the models used in science are usually constructed using data that arise
from measurements made in the real world.
At the heart of the scientiﬁc approach is the explicit recognition that we may
be wrong in our current world view. Saying this differently, we recognize that
our models will always be imperfect, but by confronting them with data, we can
strive to make them better and more useful. Echoing the words of George Box,
one of the pioneers of industrial statistics, we can say, “Essentially, all models
are wrong, but some are useful.”3
MODELS
The models of interest in this book can be conceptualized as shown in
Exhibit 2.1. This picture demands a few words of explanation:
◾In this book, and generally in Six Sigma, the outcomes of interest to us
are denoted with a Y. For example, Y1 in Exhibit 2.1 could represent the
event that someone will apply for a new credit card after receiving an
offer from a credit card company.
◾Causes that may inﬂuence a Y will be shown as an X. To continue the
example, X1 may denote the age of the person receiving the credit
card offer.
◾Rather than using a lengthy expression such as “we expect the age of
the recipient to inﬂuence the chance that he or she will apply for a credit

S I X S I G M A A N D V I S U A L S I X S I G M A
9
Causes We Don’t Understand,
Know About, or Care About
Y1 = Signal Function1(X1, X2, X3) + Noise Function1(X4, X5, X6)
Y2 = Signal Function2(X1, X2, X3) + Noise Function2(X4, X5, X6)
Measured Effects
or Outcomes of
Interest
Causes We
Understand
X4
X1
X2
Y1
Y2
System of Interest
X3
X5
X6
Exhibit 2.1 Modeling of Causes before Improvement
card after receiving an offer,” we can just write Y = f(X). Here, f is called
a function, and Y = f(X) describes how Y changes as X changes. If we think
that Y depends on more than one X, we simply write an expression like
Y = f(X1, X2). Since the function f describes how the inputs X1 and X2
affect Y, the function f is called a signal function.
◾
Note that we have two different kinds of causes: (X1, X2, X3), shown
in the diagram with solid arrows, and (X4, X5, X6), shown with dot-
ted arrows. The causes with dotted arrows are the causes that we do
not know about or care about, or causes that it is impossible or uneco-
nomic to control. Often, these are called nuisance or noise variables. For
example, X4 could be the number of credit cards that the recipient of
the offer already has, or the time since the recipient received a similar
offer. The function that represents the combined effect of the noise vari-
ables on Y is called a noise function, and the result is sometimes referred
to simply as error.
◾
Just because we do not know about the noise variables does not mean
that they do not inﬂuence Y. If X4, X5, or X6 change, as they typically
will, then they will necessarily lead to some apparently inexplicable vari-
ation in the outcome Y, even when we do our best to keep X1, X2, and X3
ﬁxed. For example, whether an offer recipient applies for a new credit
card may well be inﬂuenced by the number of credit cards that the recip-
ient already has.
As you can see in the exhibit, a key aspect of such a model is that it focuses
on some speciﬁc aspects (i.e., X1, X2, and X3) in order to better understand
them. By intention or simply lack of current knowledge, the model necessarily
omits some aspects that may actually be important (X4, X5, and X6).
Depending on whether you are being optimistic or pessimistic, Six Sigma
can be associated with improvement or problem solving. Very often, an explicit
model relating the Ys to Xs may not exist; to effect an improvement or to solve
a problem, you need to develop such a model. The process of developing this

10
V I S U A L S I X S I G M A
Causes We Don’t Understand,
Know About, or Care About
Y1 = Signal Function3(X1, X2, X3, X4) + Noise Function3(X5, X6)
Y2 = Signal Function4(X1, X2, X3, X4) + Noise Function4(X5, X6)
Measured Effects
or Outcomes of
Interest
Causes We
Understand
X1
X2
X3
X4
Y1
Y2
System of Interest
X5
X6
Exhibit 2.2 Modeling of Causes after Improvement
model ﬁrst requires arriving at a starting model and then confronting that model
with data to try to reﬁne it. Later in this chapter, in the section “Visual Six Sigma:
Strategies, Process, Roadmap, and Guidelines,” we discuss a process for reﬁning
the model.
If you succeed in reﬁning it, then the new model might be represented as
shown in Exhibit 2.2. Now X4 has a solid arrow rather than a dotted arrow and
is within the scope of the signal function rather than the noise function. When
we gain a new understanding of a noise variable, we gain leverage in explaining
the outcome (Y) and so can often make the outcome more favorable to us. In
other words, we are able to make an improvement.
The use of the term error to refer to a noise function has technical origins,
and its use is pervasive, though noise might be a better term. Useful models
that encompass variation rely on making a correct separation of the noise and
the signal implied by the data. Indeed, the inclusion of noise in the model is
essentially the deﬁnition of a statistical model (see the section “Variation and
Statistics”), and in such models the relevance or statistical signiﬁcance of a signal
variable is assessed in relation to the noise.
MEASUREMENTS
The use of data-driven models to encapsulate and predict how important aspects
of a business operate is still a new frontier. Moreover, there is a sense in which
a scientiﬁc approach to business is more challenging than the pursuit of sci-
ence itself. In science, the prevailing notion is that knowledge is valuable for
its own sake. But for any business striving to deliver value to its customers
and stakeholders—usually in competition with other businesses doing the same
thing—knowledge does not necessarily have an intrinsic value. This is particularly
so since the means to generate, store, and use data and knowledge are in them-
selves value-consuming, including database and infrastructure costs, training
costs, cycle time lost to making measurements, and so on.

S I X S I G M A A N D V I S U A L S I X S I G M A
11
Therefore, for a business, the only legitimate driving force behind a scien-
tiﬁc, data-driven approach that includes modeling is a failure to produce or deliver
what is required. This presupposes that the business can assess and monitor what
is needed, which is a nontrivial problem for at least two reasons:
1. A business is often a cacophony of voices, expressing different views as
to the purpose of the business and needs of the customer.
2. A measurement process implies that a value is placed on what is being
measured, and it can be very difﬁcult to determine what should be
valued.
It follows that developing a useful measurement scheme can be a difﬁcult,
but vital, exercise. Moreover, the analysis of the data that arise when mea-
surements are actually made gives us new insights that often suggest the need
for making new measurements. We will see some of this thinking in the case
studies that follow.
OBSERVATIONAL VERSUS EXPERIMENTAL DATA
Before continuing, it is important to note that the data we will use come in two
types, depending on how the measurements of Xs and Ys are made: observa-
tional data and experimental data. Exhibit 2.1 allows us to explain the crucial
difference between these two types of data.
1. Observational data arise when, as we record values of the Ys, the values
of the Xs are allowed to change at will. This occurs when a process runs
naturally and without interference.
2. Experimental data arise when we deliberately manipulate the Xs and then
record the corresponding Ys.
Observational data are collected with no control over associated Xs. Often
we simply assume that the Xs are essentially constant over the observational
period, but sometimes the values of a set of Xs are recorded along with the
corresponding Y values.
In contrast, the collection of experimental data requires us to force variation
in the Xs. This involves designing a plan that tells us exactly how to change
the Xs in the best way, leading to the topic of experimental design, or design
of experiments (DOE). DOE is a powerful and far-reaching approach that has
been used extensively in manufacturing and design environments.4 Today, DOE
is ﬁnding increasing application in nonmanufacturing environments as well.5
The book Optimal Design of Experiments: A Case Study Approach guides readers in
designing and analyzing experiments using JMP.6
In both manufacturing and nonmanufacturing settings, DOE is starting to
ﬁnd application in the Six Sigma world through discrete choice experiments.7

12
V I S U A L S I X S I G M A
In such experiments, users or potential users of a product or service are given
the chance to compare attributes and express their preferences or choices. This
allows market researchers and developers to take a more informed approach
to tailoring and trading off the attributes of the product or service in advance.
Because one attribute can be price, such methods allow you to address an impor-
tant question: What will users pay money for? We note that JMP has extensive,
easy-to-use facilities for both the design and analysis of choice models.
Even in situations where DOE is relevant, preliminary analysis of observa-
tional data is advised to set the stage for designing the most appropriate and
powerful experiment. The case studies in this book deal predominantly with
the treatment of observational data, but Chapters 7 and 9 feature aspects of
DOE as well.
SIX SIGMA
Some common perceptions and deﬁnitions of Six Sigma include:
◾A management philosophy
◾Marketing hype
◾A way to transform a company
◾A way to create processes with no more than 3.4 defects per million
opportunities
◾Solving problems using data
◾A way to use training credits
◾Something a company has to do before Lean
◾Making improvements using data
◾A way to make money from consulting, training, and certiﬁcation
◾A pseudo-religion
◾A way to get your next job
◾Something a company does after Lean
In spite of this diversity of perspectives, there seems to be broad agreement
that a Six Sigma initiative involves a variety of stakeholders and is a project-
based method utilizing cross-functional teams. A performance gap is the only
legitimate reason for spending the time and resources needed to execute a Six
Sigma project. From this point of view, questions such as the following are vital
to a Six Sigma deployment:
◾How big should the performance gap be to make a project worth doing?
◾How can you verify that a project did indeed have the expected impact?

S I X S I G M A A N D V I S U A L S I X S I G M A
13
However, for reasons of space, our brief discussion will only address the
steps that are followed in a typical Six Sigma project once it has been kicked off.
Using the background presented in the beginning of this chapter, we offer
our own succinct deﬁnition of Six Sigma:
Six Sigma is the management of sources of variation in relation to performance
requirements.
Here, management refers to some appropriate modeling activity fed by data.
Depending on both the business objectives and the current level of understand-
ing, management of sources of variation can mean:
◾
Identifying and quantifying sources of variation
◾
Controlling sources of variation
◾
Reducing sources of variation
◾
Anticipating sources of variation
A Six Sigma deployment effort typically starts with the following infra-
structure:
◾
A senior executive, often a president or chief executive ofﬁcer, provides
the necessary impetus and alignment by assuming a leadership role.
◾
An executive committee, working operationally at a level similar to that of
the senior executive, oversees the Six Sigma deployment.
◾
A champion sponsors and orchestrates an individual project. This indi-
vidual is usually a member of the executive committee and has enough
inﬂuence to remove obstacles or allocate resources without having to
appeal to a more senior individual.
◾
A process owner has the authority and responsibility to make improve-
ments to operations.
◾
A black belt supports project teams, taking a leadership role in this effort.
This individual is a full-time change agent who is allocated to several
projects. A black belt is usually a quality professional, but is often not an
expert on the operational processes within the scope of the project.
◾
A green belt works part-time on a project or perhaps leads a smaller-scope
project.
◾
A master black belt mentors the Six Sigma community (black belts and
green belts), often provides training, and advises the executive commit-
tee. A master black belt must have a proven track record of effecting
change and be a known and trusted ﬁgure. This track record is estab-
lished by having successfully completed and led numerous Six Sigma
projects, ideally within the same organization.

14
V I S U A L S I X S I G M A
To guide Six Sigma projects that seek to deliver bottom-line results in the
short or medium term, black belts typically use the Deﬁne, Measure, Analyze,
Improve, and Control (DMAIC) structure, where DMAIC is an acronym for the
ﬁve phases involved:
1. Deﬁne. Deﬁne the problem or opportunity that the project seeks to
address, along with the costs, beneﬁts, and the customer impact. Deﬁne
the team, the speciﬁc project goals, the project timeline, and the process
to be improved.
2. Measure. Construct or verify the operational deﬁnitions of the Ys, also
called the critical to quality (CTQ) metrics or measures. Plot a baseline
showing the level and current variation of the Ys. Quantify how much
variation there is in the measurement process itself, in order to adjust the
observed variation in the Ys and to improve the measurement process,
if needed. Brainstorm or otherwise identify as many Xs as possible, in
order to include the Xs that represent root causes.
3. Analyze. Use process knowledge and data to determine which Xs repre-
sent root causes of variation in the Ys.
4. Improve. Find the settings for Xs that deliver the best possible values
for the Ys, develop a plan to implement process changes, pilot the pro-
cess changes to verify improvement in the Ys, and institutionalize the
changes.
5. Control. Lock in the performance gains from the Improve phase.
Depending on the state of the process, product, or service addressed by the
project, a different set of steps is sometimes used. For instance, for products or
processes that are being designed or redesigned, the Deﬁne, Measure, Analyze,
Design, Verify (DMADV) or the Identify, Design, Optimize, Validate (IDOV) frame-
work is often used. These structures form the basis of Design for Six Sigma
(DFSS).8 Brieﬂy, the phases of the DMADV approach consist of the following:
1. Deﬁne. Similar to the Deﬁne phase of DMAIC.
2. Measure. Determine internal and external customer requirements, mea-
sure baseline performance against these requirements, and benchmark
against competitors and industry standards.
3. Analyze. Explore product and process design options for satisfying cus-
tomer requirements, evaluate these options, and select the best design(s).
4. Design. Create detailed designs of the product and process, pilot these,
and evaluate the ability to meet customer requirements.
5. Verify. Verify that the performance of the product and process meets cus-
tomer requirements.
This brings us back full circle to our own deﬁnition of Six Sigma:
management of sources of variation in relation to performance requirements.

S I X S I G M A A N D V I S U A L S I X S I G M A
15
With a little thought, perhaps you can see how large parts of DMAIC, DMADV,
or IDOV involve different ways to manage variation. For example, a DFSS
project would involve techniques and tools to “anticipate sources of variation”
in the product, process, or service.
VARIATION AND STATISTICS
In the previous section, we mentioned the following aspects of managing
variation:
◾
Identify and quantify sources of variation.
◾
Control sources of variation.
◾
Reduce sources of variation.
◾
Anticipate sources of variation.
The ﬁrst point, “Identify and quantify sources of variation,” is a vital step
and typically precedes the others. In fact, Six Sigma efforts aside, many busi-
nesses can derive useful new insights and better knowledge of their processes
and products simply by understanding what their data represent and by inter-
acting with their data to literally see what has not been seen before. Identiﬁ-
cation of sources of variation is a necessary step before starting any modeling
associated with the other Six Sigma steps. Even in those rare situations where
there is already a high level of understanding about the data and the model, it
would be very unwise to begin modeling without ﬁrst investigating the data.
Every set of data is unique, and in the real world, change is ubiquitous, includ-
ing changes in the patterns of variation.
Given that the study of variation plays a central role in Six Sigma, it would
be useful if there were already a body of knowledge that we could apply to
help us make progress. Luckily, there is: statistics! One of the more enlightened
deﬁnitions of statistics is learning in the face of uncertainty; since variation is a
result of uncertainty, then the relevance of statistics becomes immediately clear.
However, statistics tends to be underutilized in understanding uncertainty.
We believe that one of the reasons is that the fundamental difference between
an exploratory study and a conﬁrmatory study is not sufﬁciently emphasized or
understood. This difference can be loosely expressed as the difference between
statistics as detective and statistics as lawyer. Part of the difﬁculty with fully
appreciating the relevance of statistics as detective is that the process of discovery
it addresses cannot fully be captured within an algorithmic or theoretical
framework. Rather, producing new and valuable insights from data relies on
heuristics, rules of thumb, serendipity, and contextual knowledge. In contrast,
statistics as lawyer relies on deductions that follow from a structured body of
knowledge, formulas, statistical tests, and p-values.
The lack of appreciation of statistics as detective is part of our motivation in
writing this book. A lot of traditional Six Sigma training overly emphasizes

16
V I S U A L S I X S I G M A
statistics as lawyer. This generally gives an unbalanced view of what Six Sigma
should be, as well as making unrealistic and overly time-consuming demands
on practitioners and organizations.
Six Sigma is one of many applications where learning in the face of uncer-
tainty is required. In any situation where statistics is applied, the analyst will
follow a process, more or less formal, to reach ﬁndings, recommendations, and
actions based on the data.9 There are two phases in this process:
1. Exploratory Data Analysis
2. Conﬁrmatory Data Analysis
Exploratory Data Analysis (EDA) is nothing more than a fancy name for statis-
tics as detective, whereas Conﬁrmatory Data Analysis (CDA) is simply statistics as
lawyer. In technical jargon, the emphasis in EDA is on hypothesis generation. In
EDA efforts, the analyst searches for clues in the data that help identify theories
about underlying behavior. In contrast, the focus of CDA is hypothesis testing
and inference. CDA consists of conﬁrming these theories and behaviors. CDA fol-
lows EDA, and together they make up statistical modeling. A paper by Jeroen de
Mast and Albert Trip provides a detailed discussion of the crucial role of EDA in
Six Sigma.10
MAKING DETECTIVE WORK EASIER THROUGH DYNAMIC
VISUALIZATION
To solve a mystery, a detective has to spot clues and patterns of behavior and
then generate working hypotheses that are consistent with the evidence. This is
usually done in an iterative way, by gathering more evidence and by enlarging
or shifting the scope of the investigation as knowledge is developed. So it is with
generating hypotheses through EDA.
We have seen that the ﬁrst and sometimes only step in managing uncer-
tainty is to identify and quantify sources of variation. Building on the old adage
that “a picture is worth a thousand words,” it is clear that graphical displays
should play a key role here. This is especially desirable when the software allows
you to interact freely with these graphical views. Thanks to the advance of tech-
nology, most Six Sigma practitioners now have capabilities on their desktops
that were only the province of researchers 10 years ago, and were not even
foreseen 30 years ago. Although it is not entirely coincidental, we are fortunate
that the wide availability of this capability comes at a time when data volumes
continue to escalate.
Incidentally, many of the statistical methods that fall under CDA, which
are in routine use by the Six Sigma community, were originally developed for
squeezing the most out of a small volume of data, often with the use of noth-
ing more than a calculator or a pen and paper. Increasingly, the Six Sigma

S I X S I G M A A N D V I S U A L S I X S I G M A
17
practitioner is faced with a quite different challenge: The sheer volume of data
(rows and columns) can make the naïve application of statistical testing, should
it be needed, difﬁcult and questionable.
At this point, let us consider the appropriate role of visualization and, tan-
gentially, data mining within Six Sigma. Visualization, which has a long and
interesting history of its own, is conventionally considered valuable in three
ways:11
1. Checking raw data for anomalies (EDA)
2. Exploring data to discover plausible models (EDA)
3. Checking model assumptions (CDA)
Given the crucial role of communication in Six Sigma, we can add two
additional ways in which visualization has value:
1. Investigating model outcomes (EDA and CDA)
2. Communicating results to others (EDA and CDA)
There are a wide variety of ways to display data visually. Many of these,
such as histograms, scatterplots, Pareto plots, and box plots, are already in
widespread use. However, the simple idea of providing multiple linked views of
data with which you can interact via software takes current Six Sigma analysis
to another level of efﬁciency and effectiveness. For example, imagine clicking
on a bar in a Pareto chart and seeing the corresponding points in a scatterplot
become highlighted. Imagine what can be learned! Unfortunately, however, a
lot of software is still relatively static, offering little more than a computerized
version of what is possible on the printed page. In contrast, we see the dynamic
aspect of good visualization software as critical to the detective work of EDA,
which relies on an unfolding, rather than preplanned, set of steps.
Visualization remains an active area of research, particularly when data vol-
umes are high,12 but there are already many new, useful graphical displays.
For example, the parallel coordinates plots used for visualizing data with many
columns are well known within the visualization community, but have not yet
spread widely into the Six Sigma world.13
Additionally, although there are established principles about the correct
ways to represent data graphically, the fact that two individuals will perceive
patterns differently means that good software should present a wide repertoire
of representations, ideally all dynamically linked with one another.14 We hope
to demonstrate through the case studies that this comprehensive dynamic link-
ing is a powerful capability for hypothesis generation. To emphasize this desir-
able aspect, from now on, we will refer to dynamic visualization, rather than
simply visualization.
Not only does dynamic visualization support EDA when data volumes are
large, but it is also our experience that dynamic visualization is very powerful

18
V I S U A L S I X S I G M A
when data volumes are modest. For instance, if the distributions of two or more
variables are linked together, you can quickly and easily see the balance of the
data, that is, which values or levels of one variable occur with those of another.
If the data are perfectly balanced, then tabulation may also provide the same
insight, but if the data are only nearly balanced or if they are unbalanced, as is
more often the case, the linked distributions will usually be much more easily
interpreted. With dynamic visualization, we can assess many views of the data
quickly and efﬁciently.
The mention of large data volumes inevitably raises the topic of data mining.
This is a rapidly moving ﬁeld, so a precise deﬁnition is difﬁcult. Essentially, data
mining (also known as predictive analytics) is the process of sorting through large
amounts of data and picking out relevant information using techniques from
machine learning and statistics.15 In many cases, the data are split into at least
two sets, and a model is built using one set, then validated or tested on the
second set. Once the model is built, it is used to score new data as they arrive,
thereby making (hopefully) useful predictions.
As with traditional statistical analysis, there are several processes that you
can use in data mining.16 In most data-mining applications, the software used
automates each step in the process, usually involving some prescribed stopping
rule to determine when there is no further structure in the data to model. As
such, many data-mining efforts have a strong ﬂavor of CDA. However, EDA can
bring high value to data-mining applications, especially in Six Sigma settings.
In our case studies, we will see two such applications.
VISUAL SIX SIGMA: STRATEGIES, PROCESS, ROADMAP,
AND GUIDELINES
In this section, we will explore the three strategies that underlie Visual Six
Sigma. We then present the Visual Six Sigma Data Analysis Process that supports
these strategies through six steps and deﬁne the Visual Six Sigma Roadmap that
expands on three of the key steps. This section closes with guidelines that help
you assess your performance as a Visual Six Sigma practitioner.
Visual Six Sigma Strategies
As mentioned earlier, Visual Six Sigma exploits the following three key
strategies to support the goal of managing variation in relation to performance
requirements:
1. Using dynamic visualization to literally see the sources of variation in
your data
2. Using exploratory data analysis techniques to identify key drivers and mod-
els, especially for situations with many variables

S I X S I G M A A N D V I S U A L S I X S I G M A
19
3. Using conﬁrmatory statistical methods only when the conclusions are
not obvious
Note that with reference to the section “Variation and Statistics,” Strategy 1
falls within what was called EDA, or statistics as detective. Strategy 3 falls within
what we deﬁned as CDA, or statistics as lawyer. Strategy 2 has aspects of both
EDA and CDA.
Earlier, we stressed that by working in the EDA mode of statistics as detective
we have to give up the possibility of a neat conceptual and analytical frame-
work. Rather, the proper analysis of our data has to be driven by a set of informal rules
or heuristics that allow us to make new, useful discoveries. However, there are still
some useful principles that can guide us. Jeroen de Mast and Albert Trip offer an
excellent articulation and positioning of these principles in the Six Sigma con-
text.17 Unsurprisingly, these principles are applicable within Visual Six Sigma
and appear in a modiﬁed form in the Visual Six Sigma Roadmap presented later
(Exhibit 2.4).
If you recall from Chapter 1, one of the goals of Visual Six Sigma is to equip
users who know their business with some simple ideas and tools to get from data
to decisions easily and quickly. Indeed, we would argue that the only prereq-
uisite for a useful analysis, other than having high-quality data, is knowledge
of what the different variables that are being analyzed actually represent. We
cannot emphasize strongly enough this need for contextual knowledge to guide
interpretation; it is not surprising that this is one of the key principles listed by
de Mast and Trip.
As mentioned earlier, a motivating factor for this book is our conviction
that the balance in emphasis between EDA and CDA in Six Sigma is not always
correct. Yet another motivation for this book is to address the perception that a
team must strictly adhere to the phases of DMAIC, even when the data or prob-
lem context does not warrant doing so. The use of the three key Visual Six Sigma
strategies provides the opportunity to reengineer the process of going from data
to decisions. In part, this is accomplished by freeing you, the practitioner, from
the need to conduct unnecessary analyses.
Visual Six Sigma Data Analysis Process
We have found the simple process shown in Exhibit 2.3 to be effective in many
real-world situations. We refer to this in the remainder of the book as the Visual
Six Sigma (VSS) Data Analysis Process.
This process gives rise to the subtitle of this book, Making Data Analysis Lean.
As the exhibit shows, it may not always be necessary to engage in the “Model
Relationships” activity. This is reﬂective of the third Visual Six Sigma strategy.
An acid test for a Six Sigma practitioner is to ask, “If I did have a model of Ys

20
V I S U A L S I X S I G M A
Frame Problem
Collect Data
Uncover
Relationships
Statistics as
Detective (EDA)
Utilize
Knowledge
Revise
Knowledge
Model
Relationships
Statistics as
Lawyer (CDA)
Exhibit 2.3 Visual Six Sigma Data Analysis Process
against Xs from CDA, how would it change my recommended actions for the
business?”
The steps in the VSS Data Analysis Process may be brieﬂy described as
follows:
1. Frame Problem. Identify the speciﬁc failure to produce what is required
(see prior section titled “Measurements”). Identify your general strategy
for improvement, estimate the time and resources needed, and calculate
the likely beneﬁt if you succeed. Identify the Y or Ys of interest.
2. Collect Data. Identify potential Xs using techniques such as brainstorming,
process maps, data mining, failure modes and effects analysis (FMEA),
and subject matter knowledge. Passively or actively collect data that
relate these to the Ys of interest.
3. Uncover Relationships. Assess your data’s strengths, weaknesses, and rele-
vance to your problem. Using exploratory tools and your understanding
of the data context, generate hypotheses and explore whether and how
the Xs relate to the Ys.
4. Model Relationships. Build statistical models relating the Xs to the Ys.
Determine statistically which Xs explain variation in the Ys and may
represent causal factors.
5. Revise Knowledge. Optimize settings of the Xs to give the best values for the
Ys. Explore the distribution of Ys as the Xs are allowed to shift a little from
their optimal settings. Collect new data to verify that the improvement
is real.

S I X S I G M A A N D V I S U A L S I X S I G M A
21
6. Utilize Knowledge. Implement the improvement and monitor or review
the Ys with an appropriate frequency to see that the improvement is
maintained.
Visual Six Sigma Roadmap: Uncover Relationships, Model
Relationships, and Revise Knowledge
In this section, we expand on the three steps in the VSS Data Analysis Process
that beneﬁt the most from the power of visual methods: Uncover Relationships,
Model Relationships, and Revise Knowledge. These activities are reﬂective of
where we see the biggest opportunities for removing waste from the process of
going from data to decisions.
The Visual Six Sigma Roadmap in Exhibit 2.4 guides you through these
three important steps. Given that the displays used for visualization and discov-
ery depend upon your own perceptive and cognitive style, the Visual Six Sigma
Roadmap focuses on the goal, or the what, of each step. However, in Chapter 3,
we will make speciﬁc suggestions about how each step can be accomplished
using JMP.
This Roadmap uses the Six Sigma convention that a variable is usually
assigned to a Y role (an outcome or effect of interest) or to an X role (a pos-
sible cause that may inﬂuence a Y). The phrase Hot X in Exhibit 2.4 relates to
the fact that according to the available data this variable really does appear to
have an impact on the Y of interest. Of course, in order to make such a deter-
mination, this X variable must have been included in your initial picture of
Exhibit 2.4 The Visual Six Sigma Roadmap: What We Do
Visual Six Sigma Roadmap—What We Do
Uncover Relationships
Dynamically visualize the variables one at a time
Dynamically visualize the variables two at a time
Dynamically visualize the variables more than two at a time
Visually determine the Hot Xs that affect variation in the Ys
Model Relationships
For each Y, identify the Hot Xs to include in the signal function
Model Y as a function of the Hot Xs; check the noise function
If needed, revise the model
If required, return to the Collect Data step and use DOE
Revise Knowledge
Identify the best Hot X settings
Visualize the effect on the Ys should these Hot X settings vary
Verify improvement using a pilot study or conﬁrmation trials

22
V I S U A L S I X S I G M A
how the process operates. Those X variables that are not Hot Xs, in spite of
prior expectations, can be thought of as being moved into the noise function
for that Y. Other terms for Hot X are Red X and Vital X. Whatever terminology
is used, it is important to understand that for any given Y, there may be more
than one X that has an impact, and, in such cases, it is important to understand
the joint impact of these Xs.
Note that, although the designations of Y or X for a particular variable are
useful, whether a variable is a Y or an X depends both on how the problem is
framed and on the stage of the analysis. Processes are often modeled as both
serial (a set of connected steps) and hierarchical (an ordered grouping of levels
of steps, where one step at a higher level comprises a series of steps at a lower
level). Indeed, one of the tough choices to be made in the Frame Problem
step (Exhibit 2.3) is to decide on an appropriate level of detail and granularity
for usefully modeling the process. Even when a manufacturing process is only
moderately complex, it is often necessary to use a divide-and-conquer approach
in process and product improvement and design projects, which are often sub-
divided into pieces that reﬂect how the ﬁnal product is made and operates. In
transactional situations, modeling the process is usually more straightforward.
Uncover Relationships and Model Relationships
Earlier, we used the phrase “data of high quality.” Although data cleansing is
often presented as an initial step prior to any data analysis, we feel that it is bet-
ter to include this vital activity as part of the Uncover and Model Relationships
steps (Exhibit 2.3), particularly when there are large numbers of variables. For
example, it is perfectly possible to have a multivariate outlier that is not outlying
in any single variable. Therefore the assessment of data quality and any required
remedial action is understood to be woven into the Visual Six Sigma Roadmap.
Chapter 4, “Managing Data and Data Quality,” shows some examples.
The Uncover and Model Relationships steps also require a sound under-
standing and validation of the measurement process for each variable in your
data. You can address measurement process variation using a Gauge Repeatability
and Reproducibility (Gauge R&R) study or a Measurement System Analysis (MSA)
study. Whatever your approach, addressing measurement variability is critically
important. It is only when you understand the pattern of variation resulting
from repeatedly measuring the same item that you can correctly interpret the
pattern of variation when you measure different items of that type.18
In many ways, an MSA is best seen as an application of DOE to a measure-
ment process, and properly the subject of a Visual Six Sigma effort of its own.
To generalize, we would say that:
◾In a transactional environment, the conventional MSA is often too
sophisticated.

S I X S I G M A A N D V I S U A L S I X S I G M A
23
◾
In a manufacturing environment, the conventional MSA is often not
sophisticated enough.
As an example of the second point: If the process to measure a small feature
is automated, involving robot handling and vision systems, then the two Rs in
Gauge R&R (corresponding to repeatability and reproducibility variation) may
not be of interest. Instead we may be concerned with the variation when the
robot loads and orients the part, when the camera tracks to supposedly ﬁxed
locations, and when the laser scans in a given pattern to examine the feature.
Revise Knowledge
The Revise Knowledge activity is where we integrate what we have learned
in the Uncover Relationships and possibly the Model Relationships steps with
what we already know. There are many aspects to this, and most of them are
particular to the speciﬁc context.
Regardless, one of the vital tasks associated with the Revise Knowledge step
is to consider how, or if, our new ﬁndings will generalize. Note that Step 4 in
Model Relationships already alerts us to this kind of problem, but this represents
an extreme case.
Perhaps unsurprisingly, the best way to tackle this issue is to collect addi-
tional, new data via conﬁrmatory runs to check how these ﬁt with what we
now expect. This is particularly important when we have changed the settings
of the Hot Xs to achieve what appear to be better outcomes. As we acquire
and investigate more and more data under the new settings, we have more
and more assurance that we did indeed make a real improvement. Many busi-
nesses develop elaborate protocols to manage the risk of making such changes.
Although there are some statistical aspects, there are at least as many contextual
ones, so it is difﬁcult to give general guidance.
In any case, conﬁrmatory runs, no matter how they are chosen, are an
expression of the fact that learning should be cumulative. Assuming that the
performance gap continues to justify it, the continued application of the VSS
Data Analysis Process (Exhibit 2.3) gives us the possibility of a virtuous circle.
Guidelines
Finally, the following are some guidelines that may help you as a practitioner
of Visual Six Sigma:
◾
Customer requirements of your process or product should establish the
context and objectives for all the analyses you conduct.
◾
These objectives can always be rephrased in terms of the identiﬁcation,
control, reduction, and/or anticipation of sources of variation.

24
V I S U A L S I X S I G M A
◾If you do not measure it, then you are guessing.
◾If you do not know the operational deﬁnition of your measurement or
the capability of your measurement process, then you are still guessing.
◾If you spend more time accessing and integrating data than with
Visual Six Sigma, then your information system needs to be carefully
examined.
◾The choice of which variables and observational units to include in
constructing a set of data should be driven by your current process or
product understanding and the objectives that have been set.
◾Given that you have made such a choice, you need to be concerned
about how your ﬁndings are likely to generalize to other similar
situations.
◾Any analysis that ignores business and contextual information and tries
to just manipulate numbers will always fail.
◾Any data set has information that can be revealed by dynamic
visualization.
◾Models can be used to make predictions, but a useful prediction need
not involve a formal model.
◾All models are wrong, but some are useful.
◾The more sophisticated the model you build, the more opportunity for
error in constructing it.
◾If you cannot communicate your ﬁndings readily to business stakehold-
ers, then you have failed.
◾If the course of action is not inﬂuenced by your ﬁndings, then the anal-
ysis was pointless.
CONCLUSION
In this chapter, we have given an overview of Six Sigma and Visual Six
Sigma. The “Six Sigma” section presented our deﬁnition of Six Sigma as
the management of variation in relation to performance requirements, and
brieﬂy described some wider aspects of Six Sigma. The section “Variation and
Statistics” emphasized the key role of statistics as detective, namely, EDA. The
next section dealt brieﬂy with dynamic visualization as a prerequisite for suc-
cessful detective work while the section “Visual Six Sigma: Strategies, Process,
Roadmap, and Guidelines” aimed to summarize the three key strategies and
the process that will allow you to solve data mysteries more quickly and with
less effort. Through the Visual Six Sigma Data Analysis Process and the Visual
Six Sigma Roadmap, the application of these strategies will be illustrated in the
case studies.

S I X S I G M A A N D V I S U A L S I X S I G M A
25
Chapter 3 aims to familiarize you a little with JMP, the enabling technology
we use for Visual Six Sigma. Its purpose is to equip you to follow the JMP
usage in the Visual Six Sigma case studies that form the heart of this book.
With the background in Chapter 3 and the step-by-step details given in the case
studies, you will be able to work through the case study chapters, reproducing
the appropriate graphs and reports. Maybe you will even venture beyond these
analyses to discover new knowledge on your own! In any case, you will learn
to use a large repertoire of techniques that you can then apply to your own data
and projects.
NOTES
1. Mikel Harry and Richard Schroeder, Six Sigma: The Breakthrough Management Strategy Revolution-
izing the World’s Top Corporations (New York, NY: Random House, 2006); Thomas Pyzdek, The Six
Sigma Handbook: A Complete Guide for Greenbelts, Blackbelts and Managers at All Levels (New York,
NY: McGraw-Hill, 2003); and George Eckes, The Six Sigma Revolution: How General Electric and
Others Turned Process into Proﬁts (New York, NY: John Wiley & Sons, Inc., 2003).
2. http://www.bptrends.com/publicationﬁles/12-03 ART Digital Six Sigma-Smith-Fingar1.pdf
(accessed 10 February 2016).
3. George E. P. Box and Norman R. Draper, Empirical Model-Building and Response Surfaces
(New York, NY: John Wiley & Sons, Inc., 1987), 424.
4. George E. P. Box, William G. Hunter, and Stuart J. Hunter, Statistics for Experimenters: Design,
Innovation, and Discovery (Hoboken, NJ: John Wiley & Sons, Inc., 2005); Marvin Lentner and
Thomas Bishop, Experimental Design and Analysis, 2nd Edition (Blacksburg, VA:Valley Book Co.,
1986); Ronald Moen, Thomas W. Nolan, and Lloyd P. Provost, Improving Quality through Planned
Experimentation (New York, NY: McGraw-Hill, 1991); and Douglas C. Montgomery, Design and
Analysis of Experiments, 6th Edition (Hoboken, NJ: John Wiley & Sons, Inc., 2005).
5. Charles W. Holland and David W. Cravens, “Fractional Factorial Experimental Designs in
Marketing Research,” Journal of Marketing Research 10, no. 3 (1973): 270–276; and Forrest
W. Breyfogle, Implementing Six Sigma: Smarter Solutions Using Statistical Methods, 2nd Edition
(Hoboken, NJ: John Wiley & Sons, Inc., 2003).
6. Peter Goos and Bradley Jones, Optimal Design of Experiments: A Case Study Approach (Wiley-
Blackwell, 2011).
7. Bryan K. Orme, Getting Started with Conjoint Analysis: Strategies for Product Design and Pricing
Research (Research Publishers LLC, 2004).
8. Clyde M. Creveling, Jeff Slutsky, and Dave Antis, Design for Six Sigma in Technology and Product
Development (New York, NY: Pearson Education, 2003); and Basem El-Haik and David M. Roy,
Service Design for Six Sigma: A Roadmap for Excellence (Hoboken, NJ: John Wiley & Sons, Inc.,
2005).
9. There are many variations of this process, but one example, not too far removed from Six
Sigma, can be found in Chris Chatﬁeld, Problem Solving: A Statistician’s Guide, 2nd Edition
(New York, NY: Chapman & Hall, 1995).
10. Jeroen de Mast and Albert Trip, “Exploratory Data Analysis in Quality Improvement Projects,”
Journal of Quality Technology 4, no. 39 (2007): 301–311.
11. See article at the York University Consulting Service website www.math.yorku.ca/SCS/
Gallery/historical.html (accessed 13 June 2015); Leland Wilkinson and Anand Anushka,
“High-Dimensional Visual Analytics: Interactive Exploration Guided by Pairwise Views of
Point Distributions,” IEEE Transactions on Visualization and Computer Graphics 12, no. 6 (2006):
1363–1372.
12. For an example, see Antony Unwin, Martin Theus, and Heike Hofmann, Graphics of Large
Datasets: Visualizing a Million (New York, NY: Springer, 2006).

26
V I S U A L S I X S I G M A
13. Alfred Inselberg, Parallel Coordinates: Visual Multidimensional Geometry and Its Applications
(New York, NY: Springer, 2008).
14. Edward R. Tufte, The Visual Display of Quantitative Information, 2nd Edition (Cheshire, CT:
Graphics Press, 2001).
15. See, for example, Trevor Hastie, R. Tibshirani, and J. H. Freidman, The Elements of Statistical
Learning: Data Mining, Inference and Prediction (New York, NY: Springer, 2001).
16. Wikipedia, “SEMMA,” https://en.wikipedia.org/wiki/SEMMA (accessed 26 June 2015).
17. de Mast and Trip, “Exploratory Data Analysis in Quality Improvement Projects.”
18. Larry B. Barrentine, Concepts for R&R Studies, 2nd Edition (Milwaukee, WI: ASQ Quality Press,
2002); and Richard K. Burdick, Connie M. Borror, and Douglas C. Montgomery, Design and
Analysis of Gauge R&R Studies: Making Decisions with Conﬁdence Intervals in Random and Mixed
ANOVA Models (Philadelphia, PA: Society for Industrial and Applied Mathematics, 2005).

C H A P T E R 3
A First Look at JMP
27
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

28
V I S U A L S I X S I G M A
T
his chapter provides you with some initial familiarity with JMP, the
enabling software that we use for Visual Six Sigma. The purpose of this
chapter is to provide sufﬁcient background to allow you to use JMP in
the six case studies that appear in Part Two of the book. In Chapter 2, we
explained the roles of statistics as detective, also known as exploratory data analysis
(EDA), and statistics as lawyer, also known as conﬁrmatory data analysis (CDA),
within Visual Six Sigma. Although we emphasize the usefulness of EDA in this
book, it is important to mention that JMP also has very comprehensive CDA
capabilities, some of which are illustrated in our case studies.
You can download a trial copy of JMP at www.jmp.com/try. JMP instruc-
tions in this book are based on JMP 12.2.0. Although the menu structure may
differ if you use a different version of JMP, the functionality described in this
book is available in JMP 12.2.0 or newer versions.
JMP is a statistical package that was developed by the SAS Institute Inc.
First appearing in October 1989, it was originally designed to take full advan-
tage of a graphical user interface that at the time was only available through the
Apple Macintosh. JMP has enjoyed continual development ever since those
early days. Today JMP is available on the Windows and Macintosh operating
systems, having a similar look and feel on each. The speciﬁcs presented in this
chapter relate to the Windows version of JMP. If you are using the Mac operat-
ing system, please refer to the appropriate manual.1
From the beginning, JMP was conceived as software for Statistical Discovery
(a synergistic blend of EDA and CDA, and JMP’s tagline). It has a visual empha-
sis, and is nimble and quick owing to the fact that data tables are completely
placed into local memory. This gives high-speed performance since any delays
in accessing the hard disk occur only when a table is initially read or ﬁnally
written. It is worth noting that, although other software packages running in
a Windows environment target Six Sigma analysis, many have a DOS heritage
that makes it difﬁcult or impossible to fully support dynamic visualization and
the unfolding analysis style required for EDA.2
What makes JMP visual and nimble is best illustrated with examples. We
start to give examples in the next section and invite you to follow along.
We remind you that the data tables used in this book are available on the
book’s website (http://support.sas.com/visualsixsigma).
THE ANATOMY OF JMP
This section gives you some basic background on how JMP works. We talk about
the structure of data tables, modeling types, obtaining reports, visual displays
and dynamic linking of displays, and window management.

A F I R S T L O O K A T J M P
29
For completeness, we show the menus and toolbars that appear in JMP Pro
version 12. These reﬂect some features not in JMP 12, due to JMP Pro’s enhanced
functionality. Also, there can sometimes be slight but inessential differences
between Windows and Mac; in such cases, we refer to and show the Windows
version.
Opening JMP
When you ﬁrst open JMP, an initial splash screen showing the details of your
version displays and then disappears. After this, two windows appear. The one
in the foreground is the Tip of the Day window, which provides helpful tips and
features about JMP (Exhibit 3.1 shows Tip 2). In Exhibit 3.1 these windows are
tiled so that you can see the initial contents of each.
We recommend that you use the tips until you become familiar with JMP.
Unchecking Show tips at startup at the bottom left of the window disables the
display of the Tip of the Day window.
Behind the Tip of the Day window, you will ﬁnd the JMP Home Window.
The JMP Home Window has four panels, each providing links to its contents.
The top right pane lists all open JMP windows. The content of the other three
panes is self-explanatory, and their state persists between JMP sessions. You can
resize the panes in the JMP Home Window, and hide or unhide these panes
using the menu commands under View > Home Window Panes.
The Window List pane in the JMP Home Window, which you can also
obtain by selecting View > Window List, gives a tree-based view of all other
windows currently open in your JMP session. The tree structure shows the
name of a currently open table, and the names of any open tables or report
windows that depend on it. Tooltips show the contents of a window in minia-
ture, and clicking on the name brings that window forward. Using a right mouse
click gives other options for managing that window in relation to others cur-
rently open.
By default, the JMP Home Window is designated as the Initial JMP Window.
You can change this by selecting File > Preferences > General and selecting
from the Initial JMP Window dropdown list. The possible choices are Home
Window, JMP Starter, or Window List. If you make a change, click OK to save
your new preference.
The JMP Starter Window can also be made visible by selecting View > JMP
Starter. It provides an alternative way to access commands that are available
in the main menu and toolbars. The commands are organized in groupings that

30
V I S U A L S I X S I G M A
Exhibit 3.1 JMP Home Window and Tip of the Day Window
may be more intuitive to some users. You may prefer using the JMP Starter to
using the menus. However, to standardize our presentation in this book, we
will illustrate features using the main menu bar. At this point, if they are open,
please close the JMP Starter and Tip of the Day windows.

A F I R S T L O O K A T J M P
31
Data Tables
Data structures in JMP are called data tables. Exhibit 3.2 shows a portion of a
data table that is used in the case study in Chapter 8 and which can be opened
via the link in the book’s Journal under the “Chapter 8—Informing Pharma-
ceuticals Sales and Marketing” outline. This data table, PharmaSales.jmp,
contains monthly records on the performance of pharmaceutical sales repre-
sentatives. The observations are keyed to 11,833 physicians of interest. Each
month, information is recorded concerning the sales representative assigned to
each physician and the related activity. The data table covers an eight-month
period. (A thorough description of the data is given in Chapter 8.)
A data table consists of a data grid and data table panels. The data grid con-
sists of row numbers and data columns. Each row number corresponds to an
observation, and the columns correspond to the variables. For observation 17,
for example, the Practice Name is Clapton Park, Greater London, Visits has a
value of 0, and Prescriptions has a value of 5. Note that for row 17, the entry in
Visits with Samples is a dot, an indicator that the value for this numeric variable
is missing for this row. For a character variable, a blank is used to represent a
missing value.
There are three data table panels, shown to the left of the data grid. The
Table panel is at the top left. In our example, it is labeled PharmaSales. Below
this panel, you see the Columns panel and the Rows panel.
The Table panel shows a listing of scripts. These are pieces of JMP code that
produce analyses. You can run a script by clicking on the red triangle to the
left of the script name and choosing Run Script from the dropdown menu. As
you will see, you will often want to save scripts to the data table in order to
reproduce analyses. This is very easy to do.
The Columns panel, located below the Table panel, lists the columns, or
variables, that are represented in the data grid. In Exhibit 3.2, we see that the
data table has 16 columns. The columns list contains a grouping of variables,
called IDs, which groups four variables together. Clicking on the gray disclosure
icon next to IDs reveals the four columns that are grouped.
In the Columns panel, note the small icons to the left of the column
names. Each icon represents the modeling type of the data in the correspond-
ing column. JMP indicates the modeling type for each variable as shown in
Exhibit 3.3.
Speciﬁcation of these modeling types tells JMP which graphs and analyses
are appropriate for these variables. For example, JMP will construct a histogram
for a variable with a continuous modeling type, but it will construct a bar graph
and frequency table for a variable with a nominal modeling type. Note that our
PharmaSales.jmp data table contains variables representing all three model-
ing types.

Exhibit 3.2 Partial View of PharmaSales.jmp Data Table
32

A F I R S T L O O K A T J M P
33
Exhibit 3.3 Icons Representing Modeling Types
The Rows panel appears beneath the Columns panel. Here we learn that
the data table consists of 95,224 observations. The numbers of rows that are
Selected, Excluded, Hidden, and Labelled are also given in this panel. These
four properties, called row states, reﬂect attributes that can be associated with a
given row and that deﬁne how JMP utilizes that row. For example, Selected
rows are highlighted in the data table and graphs, and can easily be turned
into subsets. In Exhibit 3.2, row 17 is selected and a 1 appears to the right of
Selected in the Rows panel. Excluded rows are not included in calculations.
Hidden rows are not shown in graphs. Labelled rows are assigned persistent
labels with values taken from chosen columns that can be viewed in most plots.
Excluded, hidden, and labeled rows are also appropriately ﬂagged next to the
row number in the data grid.
Visual Displays and Reports
Commands can be chosen using the menu bar, the icons on the toolbar, or, as
mentioned earlier, the JMP Starter window. We will use the menu bar in our
discussion. Exhibit 3.4 shows the menu bar and default toolbars for a table in
JMP. Note that the name of the active data table is shown as part of the window’s
title. Also shown are the File, Edit, and Data Table toolbars. Toolbars can be
customized by selecting View > Customize > Menus and Toolbars.
Reports can be obtained using the Analyze and Graph menus. JMP reports
obtained from commands under the Analyze menu provide visual displays
(graphs) along with numerical results (in text tables). Commands under the
Graph menu primarily provide visual displays, some of which also provide ana-
lytical information.
The Analyze and Graph menus are shown in Exhibits 3.5 and 3.6. These
high-level menus lead to submenus containing a wide array of visual and ana-
lytical tools, also displayed in Exhibits 3.5 and 3.6. Some of these tools appear
later in this chapter and in the case studies.
The menu choices under Analyze and Graph launch platforms. A platform
dialog allows you to make choices about variable roles, plots, and analyses.
Exhibit 3.4 Menu Bar and Default Data Table Toolbars

34
V I S U A L S I X S I G M A
Exhibit 3.5 Analyze Menu for JMP Pro 12.2.0
Exhibit 3.6 Graph Menu for JMP Pro 12.2.0

A F I R S T L O O K A T J M P
35
A platform generates a report consisting of a set of related plots and tables,
with which you can interact so that the relevant features of the chosen vari-
ables reveal themselves clearly. We will illustrate this idea with the Distribution
platform.
Whenever you have a new data table, you should use the Distribution
platform to understand each variable. At this point, please open the data table
PharmaSales.jmp. With this data table active, select Analyze > Distribution
to open the dialog window shown in Exhibit 3.7.
Suppose that you want to see distribution reports for Region Name, Vis-
its, and Prescriptions. Select all three variable names in the Select Columns
list by holding down the Control key while selecting them. Enter them in the
box called Y, Columns; you can do this either by dragging and dropping the
column names from the Select Columns box to the Y, Columns box, or by
selecting them and clicking Y, Columns. In Exhibit 3.8, these variables have
been entered and given the Y, Columns role. Note that the modeling types of
the three variables are shown in the Y, Columns box.
Click OK to obtain the output shown in Exhibit 3.9. The graphs are given in
a vertical layout to facilitate dynamic visualization of numerous variables at one
time. This is the JMP default. However, under File > Preferences > Platforms
> Distribution (use the JMP menu on the Mac) you can set a preference for a
stacked horizontal view by checking the Stack command. You can also make
this choice directly in the report: Click the red triangle next to Distribution and
select Stack.
Exhibit 3.7 Distribution Dialog for PharmaSales.jmp

36
V I S U A L S I X S I G M A
Exhibit 3.8 Distribution Dialog with Three Variables Entered as Ys
Exhibit 3.9 Distribution Reports for Region Name, Visits, and Prescriptions

A F I R S T L O O K A T J M P
37
By default, menus and toolbars for reports are set to auto-hide based on the
size of the report window. You can change this preference via the Windows
Specific Preference Group in File > Preferences. If menus and toolbars are
hidden, you can reveal them by moving your mouse to the top of the win-
dow or by clicking the ALT key on your keyboard. Once menus and toolbars
are exposed, a right mouse click will show additional toolbar selections. (These
are also available via View > Toolbars.) Selecting these options allows you to
control which toolbars are shown.
Note that, by default, the toolbars associated with a report are different from
those associated with a table (compare the toolbars in Exhibits 3.9 and 3.4).
The report provides a visual display, as well as supporting analytic informa-
tion, for the chosen variables. Note that the Distribution reports for the different
modeling types are tailored to the modeling type. For Region Name, a nominal
variable representing unordered categories, JMP provides a bar graph as well as
frequency counts and proportions (using the default alphanumeric ordering).
Visits, which is an ordinal variable, is displayed using an ordered bar graph,
accompanied by a frequency tabulation of the ordered values. Finally, the graph
for Prescriptions, which has a continuous modeling type, is a histogram accom-
panied by a box plot. The analytic results consist of sample statistics, such as
quantiles, sample mean, standard deviation, and so on.
Looking at the report for Region Name, we see that some regions have rela-
tively few observations. Northern England is the region associated with roughly
43 percent of the rows. For Visits, we see that the most frequent value is one,
and that at most ﬁve visits occur in any given month. Finally, we see that Pre-
scriptions, the number of prescriptions written by a physician in a given month,
has a highly right-skewed distribution, which is to be expected.
Additional analysis and graphical options are available in the menus
obtained by clicking on the red triangle icons in JMP. If we click on the red
triangle next to Distributions in the report shown in Exhibit 3.9, a list of
commands appears from which we can choose Stack, for example (see
Exhibit 3.10). This gives us the report in a stacked horizontal layout as shown
in Exhibit 3.11. (Although the red triangle icons will not appear red in this
text, we will continue to refer to them in this fashion, as it helps identify them
when you are working directly in JMP.)
Click the red triangle next to the variable name to see a menu containing
commands that are speciﬁc to the modeling type of each variable being stud-
ied. Exhibit 3.12 shows the commands that are revealed by clicking on the red
triangle next to Prescriptions. These commands are speciﬁc to a variable with
a continuous modeling type.
The red triangles support the unfolding style of analysis required for EDA.
They put context sensitive commands right where you need them, allowing you
to look at your data in a graphical format before deciding which analysis might
best be used to describe, further investigate, or model these data.

38
V I S U A L S I X S I G M A
Exhibit 3.10 Distribution Report Options
Exhibit 3.11 Stacked Layout for Three Distribution Reports

A F I R S T L O O K A T J M P
39
Exhibit 3.12 Variable-Speciﬁc Report Commands
You also see gray triangles, called disclosure icons, in the report. These serve to
conceal or reveal certain portions of the output in order to make viewing more
manageable. In Exhibit 3.12, to better focus on Prescriptions, the disclosure
icons for Region Name and Visits are closed to conceal their report contents.
Note that the orientation of the disclosure icon changes depending on whether
it is revealing or concealing contents.
When you launch a platform, the initial state of the report can be controlled
with the Platforms option under File > Preferences. Here, you can specify
which plots and tables you want to see, and control the form of these plots
and tables. For the analyses in this book, we use the default settings unless we explicitly
mention otherwise. (If you have already set platform preferences, you may need
to reset your preferences to exactly reproduce the reports shown above and in
the remainder of the book.)
Finally, notice the three icons and arrow in the right area on the lower
border of the report window shown in Exhibit 3.12. Clicking on the ﬁrst icon
brings the JMP Home Window to the front. The home icon appears in all report
and data table windows. Clicking on the second icon brings the data table used
to create the report or table to the front. The data table icon only appears in
reports or tables that are based on a parent data table (a similar icon appears on
the top right border on the Mac). You will ﬁnd these two icons incredibly useful

40
V I S U A L S I X S I G M A
when you have multiple data tables and reports open. If you have more than
one report window open, selecting the check box and using the arrow widget
on the far right allows you to arrange or combine the selected windows.
Dynamic Linking to Data Table
JMP dynamically links all graphs and plots that are based on the same data
table. This is arguably its most valuable capability for EDA. To see what this
means, consider the PharmaSales.jmp data table (Exhibit 3.2). Using Analyze
> Distribution, select Salesrep Name and Region Name as Y, Columns, and
click OK. In the resulting report, click the red triangle icon next to Distribution
to select Stack. Also, close the disclosure icon for Frequencies next to Salesrep
Name. This gives the plots shown in Exhibit 3.13.
Now, in the report for Region Name, click in the bar representing Scotland.
This selects the rows where Region Name = Scotland, as shown in Exhibit 3.14.
Simultaneously, areas in the Salesrep Name bar graph corresponding to rows
with the Region Name of Scotland are also highlighted. In other words, we
have identiﬁed the sales representatives who work in Scotland. Moreover, in
the Salesrep Name graph, no bars are partially highlighted, indicating that for
each of the sales representatives identiﬁed, all of their activity is in Scotland.
The proportion of a bar that is highlighted corresponds to the proportion of
rows where the selected variable value (in this case, Scotland) is represented.
Click the data table icon in the bottom right of your Distribution report to
bring the data table to the front. What has happened behind the scenes is that
the rows in the data table corresponding to observations having Scotland as the
value for Region Name have been selected in the data grid. Exhibit 3.15 shows
part of the data table. Note that the Scotland rows are highlighted. Also, note
that the Rows panel indicates that 11,048 rows have been selected.
Since these rows are selected in the data table, points and areas on plots and
graphs corresponding to these rows will be highlighted, as appropriate. This is
why the bars of the graph for Salesrep Name that correspond to representa-
tives working in Scotland are highlighted. The sales representatives whose bars
are highlighted have worked in Scotland, and because no bar is partially high-
lighted, you can also conclude that they have not worked in any other region.
Suppose that you are interested in identifying the region where a speciﬁc
sales representative works. Look at the second sales representative in the
Salesrep Name graph, Adrienne Stoyanov, who has a large number of rows.
You can simply click on the bar corresponding to Adrienne in the Salesrep
Name bar graph to highlight it, as shown in Exhibit 3.16.
This has the effect of selecting the 2,440 records corresponding to Adrienne
in the data table (check the Rows panel). Note that the bar correspond-
ing to Northern England in the Region Name plot is partially highlighted.

Exhibit 3.13 Distribution Reports for Region Name and Salesrep Name
41

Exhibit 3.14 Bar for Region Name Scotland Selected
42

Exhibit 3.15 Partial View of Data Table Showing Selection of Rows with Region Name Scotland
43

Exhibit 3.16 Distribution of Salesrep Name with Adrienne Stoyanov Selected
44

A F I R S T L O O K A T J M P
45
Exhibit 3.17 Data Table Consisting of 2,440 Rows with Salesrep Name Adrienne Stoyanov
This indicates that Adrienne works in Northern England, but that she is only a
small part of the Northern England sales force.
To view a data table consisting only of Adrienne’s 2,440 rows, simply
double-click on her bar in the Salesrep Name bar graph. A table (Exhibit 3.17)
appears that contains only these 2,440 rows—note that all of these rows
are selected, since the rows correspond to the selected bar of the histogram.
This table is also assigned a descriptive name. If you have previously selected
columns in the main data table, only those columns will appear in the new
data table. Otherwise all columns will display in the new table, as shown in
Exhibit 3.17.
To deselect the rows in this data table, click in the blank space in the lower
triangular region located in the upper left of the data grid (Exhibit 3.18). Click
in the upper right triangular region to deselect columns.
Note that JMP also tries to link between data tables when it is useful. For
example, when using the Tables > Summary menu command (described
later), reports generated from summary data can be linked to reports based on
the underlying data.
By way of review, in this section you have seen the ability of JMP to dynam-
ically link data among visual displays and to the underlying data table. You have
done this in a very basic setting, but this capability carries over to many other
exciting visual tools. The ﬂexibility to identify and work with observations based

46
V I S U A L S I X S I G M A
Exhibit 3.18 Deselecting Rows or Columns
on visual identiﬁcation is central to the software’s ability to support Visual Six
Sigma. The six case studies in Chapters 5 through 10 elaborate on this theme.
Window Management
In exploring your data, you will often ﬁnd yourself with many data tables and
reports simultaneously open. Of course, these can be closed individually. How-
ever, it is sometimes useful to close all windows, to close only data tables, or
to close only report windows. You can do this and more from the Window List
pane in the JMP Home Window.
To see the current state of your JMP session, click the home window icon
in the bottom right of your top-most window to bring the JMP Home Window
to the front (Exhibit 3.19). To open a similar home window on the Mac, select
JMP Home from the Windows menu. In the Window List panel, you see that
you have ﬁve open windows—two data tables, two reports, and the journal
ﬁle. The list of open windows also appears in the bottom panel of the Window
menu. When running analyses on data, it is important to make sure that the
Exhibit 3.19 JMP Home Window with List of Open Windows

A F I R S T L O O K A T J M P
47
appropriate data table is active—when a launch dialog is executed, commands
are run on the active data table.
At this point, you could select Window > Close All. However, you want
to continue your work with PharmaSales.jmp. Right-click on PharmaSales
in the Window List and select Close All But This. Then click Save None in
the window that appears. Alternatively, you can close the other three windows
individually selecting them in the Window List, right-clicking, and selecting
Close. Or you can close them by navigating to each window separately and
clicking on the close button (X) in the top right corner of each window.
VISUAL DISPLAYS AND ANALYSES FEATURED IN THE BOOK
The visual displays and analyses that are featured in this book show many of the
different menu items in JMP. Remember, though, that the effective use of Visual
Six Sigma will usually require the coordinated, often linked, use of these items
to accomplish something worthwhile. The linking we have shown earlier in this
chapter becomes even more compelling when combined with JMP’s ability to
dynamically ﬁlter rows and switch out columns. We show this combination in
later chapters.
Graph
Techniques that have visual displays as their primary goal are available from the
Graph menu shown in Exhibit 3.6. Many of these displays allow you to view
patterns and anomalies in your data in one, two, three or more dimensions. In
the remaining chapters, you will see examples of the following:
◾
Graph Builder allows highly interactive simultaneous visualization of
multiple variables using a wide variety of graphical elements.
◾
Bubble Plot is a dynamic extension of a scatterplot that is capable of
showing up to ﬁve dimensions (using x position, y position, size, color,
and time).
◾
Scatterplot Matrix gives a matrix of scatterplots.
◾
Scatterplot 3D gives a three-dimensional data view.
◾
Surface Plot creates three-dimensional, rotatable displays.
◾
Profiler shows traces of a function, often a prediction model, and is used
in optimization and simulation.
◾
Contour Profiler gives a multidimensional view of a function for use in
optimization.
◾
Treemap is a two-dimensional version of a bar graph that allows better
visualization of variables with many levels.

48
V I S U A L S I X S I G M A
Analyze
Techniques that combine analytic results with supporting visual displays are
found in the Analyze menu, shown in Exhibit 3.5. Analyze commands often
include displays similar to those found under the Graph menu. For example,
a Fit Model analysis allows access to the Profiler, which can also be accessed
under Graph. As another example, under Multivariate Methods > Multivari-
ate, a scatterplot matrix is presented. The selection Graph > Scatterplot Matrix
also produces a scatterplot matrix. However, Multivariate Methods > Multi-
variate allows you to choose analyses not directly accessible from Graph >
Scatterplot Matrix, such as pairwise correlations.
Each platform under Analyze performs analyses that are consistent with the
modeling types of the variables involved. Consider the Fit Y by X command,
which addresses the relationship between two variables. If both are continuous,
then the Bivariate platform presents a scatterplot, allows you to ﬁt a line or
curve, and provides regression results. If Y is continuous and X is nominal, then
Fit Y by X produces a plot of the data with comparison box plots, and allows
you to choose an analysis of variance (ANOVA) report. If both X and Y are
nominal, then a mosaic plot and contingency table output are presented. If X
is continuous and Y is nominal, a logistic regression plot and the corresponding
analytic results are given. If one or both variables are ordinal, then, again, an
appropriate report is presented.
This philosophy carries over to other platforms. The Fit Model platform
is used to model the relationship between one or more responses and one or
more predictors. In particular, this platform performs multiple linear regression
analysis. The Modeling menu includes various modeling techniques, including
neural nets and partitioning, which are usually associated with data mining.
The remaining chapters will take you to the following parts of the Analyze
menu:
◾Distribution provides histograms and bar graphs, distributional ﬁts, and
capability analysis.
◾Fit Y by X gives scatterplots, linear ﬁts, comparison box plots, mosaic
plots, contingency tables, and associated statistical tests.
◾Tabulate is an interactive approach to constructing tables of descriptive
statistics and other summary tables.
◾Fit Model ﬁts a large variety of models and gives access to a prediction
proﬁler that is linked to the ﬁtted model.
◾Modeling > Partition provides recursive partitioning models, similar to
classiﬁcation and regression trees.
◾Modeling > Neural Net ﬁts ﬂexible nonlinear models using hidden
layers.

A F I R S T L O O K A T J M P
49
◾
Modeling > Model Comparison (JMP Pro only) compares models rel-
ative to various performance measures.
◾
Multivariate Methods > Multivariate gives scatterplot matrices and var-
ious correlations.
◾
Multivariate Methods > Cluster attempts to arrange rows into groups
where the variation within a group is less than the variation between
groups.
◾
Multivariate Methods > Principal Components exploits correlations
between variables to represent most of the variability in a space of
reduced dimensionality.
◾
Quality and Process > Control Chart Builder provides the interactivity
of Graph Builder and constructs a variety of control charts.
◾
Quality and Process > Measurement Systems Analysis addresses
measurement system variability and encompasses both a traditional
approach and Wheeler’s Evaluate the Measurement Process (EMP)
approach.
◾
Quality and Process > Variability/Attribute Gauge Chart is useful for
measurement system analysis and for displaying data across the levels of
multiple categorical variables, especially when the focus is on displaying
the variation within and between groups.
◾
Quality and Process > Process Capability provides a goal plot and
box plots to assess the performance of numerous responses, as well as
related capability measures.
◾
Quality and Process > Pareto Plot gives a bar chart ordered by decreas-
ing frequency of occurrence.
◾
Quality and Process > Diagram is used to create cause-and-effect
diagrams.
Tables
Another menu that is used extensively in the remaining chapters is the Tables
menu (Exhibit 3.20). This menu contains commands that perform operations
on data tables. In the remaining chapters you will see examples of the following:
◾
Summary provides summary information, such as means and standard
deviations, for variables in a data table.
◾
Subset creates a new data table from selections of rows and columns in
the current data table.
◾
Sort sorts the rows according to the values of a column or columns.
◾
Transpose creates new data tables from the data in the current data
table.

50
V I S U A L S I X S I G M A
Exhibit 3.20 Tables Menu
◾Join operates on two data tables, joining them by adding columns of
data.
◾Concatenate operates on two or more data tables, joining them by
adding rows of data.
◾Missing Data Pattern produces a table that helps you determine if there
are patterns or relationships in the structure of missing data in the active
data table.
Rows
You will also use many features that are found under the Rows menu. The
Rows menu is shown in Exhibit 3.21, with the commands under Row Selec-
tion expanded. The Rows menu allows you to exclude, hide, and label obser-
vations. You can assign colors and markers to the points representing observa-
tions. Row Selection allows you to specify various criteria for selecting rows,
as shown.
Recall that a row state is a property that is associated with a row. A row state
consists of information on whether a speciﬁc row is selected, excluded from
analysis, hidden so that it does not appear in plots, labeled, colored, or has a
marker assigned to it. You often need to change row states of rows interactively
based on visual displays. The Clear Row States command removes any row
states that are currently in effect.
Data Filter and the Local Data Filter, which ﬁlters a speciﬁc report, are
used extensively in the case studies. These options provide a ﬂexible and inter-
active way to change the row states in order to identify meaningful, and possibly

A F I R S T L O O K A T J M P
51
Exhibit 3.21 Rows Menu with Commands for Row Selection Shown
complex, subsets of rows. They also provide a way of animating many of the
visual displays in JMP. Depending on the analysis goal, subsets that you deﬁne
using Data Filter can easily be selected and placed into their own data tables
for subsequent analysis or excluded from the current analysis.
Columns
The Cols menu, shown in Exhibit 3.22, provides commands dealing with col-
umn properties and roles, information stored along with columns, formulas that
deﬁne column values, recoding of values, and more.
Note that some of the commands in Exhibit 3.22 are active because columns
were selected in the table.
The Column Info command opens a dialog that allows you to deﬁne prop-
erties that are saved as part of the column information. To access Column Info
for a column, right-click in the column header area and choose Column Info.
Exhibit 3.23 shows the Column Info dialog for Visits.
A column can have column properties associated with it. You can add a note
describing the column using the Note property. (The Note column property
has already been added for each column in PharmaSales.jmp.) You can also

52
V I S U A L S I X S I G M A
Exhibit 3.22 Cols Menu with Utilities Shown
have speciﬁcation limits using the Spec Limits property, control limits using
the Control Limit property, and so forth. To deﬁne a column using a formula,
select the Formula column property to deﬁne that column.
Note that you can specify the Data Type and Modeling Type for your data
in the Column Info window. The Numeric and Character data types are the
most common, but you can also deﬁne a column to have a data type of Row
State or Expression. The Row State data type allows you to construct a col-
umn that contains row states, providing a permanent record of row states. The
Expression data type allows you, among other things, to store images in table
cells. These can be displayed on certain plots, which can be very useful for pro-
viding extra context and meaning. You can specify an appropriate data format
in the Column Info window as well. For more information refer to the JMP
documentation.

A F I R S T L O O K A T J M P
53
Exhibit 3.23 Column Info Dialog for Visits Showing Column Properties
DOE
Chapter 2 introduced the idea of experimental data, which arise when you
deliberately manipulate the Xs. An experimental design is a list of trials or runs
deﬁned by speciﬁc settings for the Xs, with an order in which these runs
should be performed. This design is used by an experimenter in order to obtain

54
V I S U A L S I X S I G M A
Exhibit 3.24 DOE Menu
experimental data. JMP provides comprehensive support for generating
experimental designs and modeling the results.
The DOE menu, shown in Exhibit 3.24, generates settings for a designed
experiment based on your choices of experimental design type, responses, fac-
tors, number of runs, and other settings. There are ﬁve major groupings.
The Custom Design platform allows great ﬂexibility in design choices. In
particular, Custom Design accommodates both continuous and categorical fac-
tors, provides designs that estimate user-speciﬁed interactions and polynomial
terms, and allows for situations with hard-to-change and easy-to-change factors
and covariates (split-plot, split-split-plot, and strip-plot designs). You can spec-
ify inequality constraints on the factors or disallowed combinations of factor
settings. The Custom Design platform is featured in one of the case stud-
ies (Chapter 7) while the Full Factorial Design platform is used in another
(Chapter 9).
The Evaluate Design platform allows you to assess the effectiveness of a
design prior to actually running the experiment. If you construct your design
using JMP’s Custom Design platform, design evaluation results are presented
before you construct your design table. The Augment Design platform provides
options for adding runs to an existing design. The Sample Size and Power
platform computes power, sample size, or effect size for a variety of situations,
based on values that you set.3

A F I R S T L O O K A T J M P
55
SCRIPTS
As you have seen, the menus in JMP can be used to produce an incredible
variety of visual displays and analyses. When you are starting out with JMP,
you rarely need to look beyond these menus. However, as you use JMP more
heavily, or perhaps need to start to provide insights for others, you may ﬁnd the
need to simplify repetitive tasks, or even to see custom analyses. These tasks can
be programmed within JMP by using its scripting language, aptly named JMP
Scripting Language (JSL).
A JMP script can be saved as a separate ﬁle with a .jsl extension or it can be
saved as part of a data table. The data table PharmaSales.jmp contains several
JMP scripts that are saved as part of the data table in the Table panel area.
Consider the script Distribution Plots for Three Outcome Variables. You can
run this script by clicking on the red triangle to the left of the script and choosing
Run Script, as shown in Exhibit 3.25.
When you run this script, you obtain the report shown in Exhibit 3.26. This
script adds a Local Data Filter to the Distribution report. Scripts provide an
easy way to document your work. When you have obtained a report that you
Exhibit 3.25 Running the Script Distribution Plots for Three Outcome Variables

56
V I S U A L S I X S I G M A
Exhibit 3.26 Distribution Report Obtained by Running Distribution Plots for Three Outcome
Variables
want to reproduce, instead of saving the report, or parts of it, in a presentation
ﬁle or document, you can simply save the script that produces the report to
your data table. So long as the required columns are still in the table, the script
will work even if the data (rows) are refreshed or changed.
To save a script to produce this report to the data table, do the following:
◾Obtain the report by completing the launch dialog (see Exhibit 3.8 for
an example) and clicking OK.
◾From the red triangle menu commands, select Script > Local Data
Filter.
◾Click on the red triangle next to Distributions and choose Script > Save
Script to Data Table, as shown in Exhibit 3.27.
◾A new script, called Distribution, appears in the Table panel.
You can rename or edit this script by clicking on its red triangle and choos-
ing Edit or by double-clicking on the script name. When you do this, you obtain
the script window shown in Exhibit 3.28. This is the code that reproduces
the report. Scripts can be saved in a similar way from platforms other than
Distribution.

A F I R S T L O O K A T J M P
57
Exhibit 3.27 Saving a Script to the Data Table
Exhibit 3.28 Distribution Script
Scripts can be constructed from scratch or pieced together from scripts that
JMP automatically produces, such as the script in Exhibit 3.28. Some of the
scripts in the data table PharmaSales.jmp are of this type.
In the case studies, you will frequently save scripts to your data tables. There
are two reasons for this. First, we want you to have the scripts available in case
you want to rerun your analyses quickly, so as to get to a point in the case study
where you might have been interrupted. Second, we want to illustrate saving
scripts because they provide an excellent way of documenting your analysis,
allowing you to follow and recreate it in the future.

58
V I S U A L S I X S I G M A
In Chapter 1 we mentioned that, on occasion, we have deliberately com-
promised the lean nature of the Visual Six Sigma Roadmap to show some JMP
functionality that will be useful to you in the future. The saving of scripts is an
example, because at one level scripts are not necessary and are therefore not
lean. However, at another level, when you have obtained the critical reports
that form the basis for your conclusions, scripts take on a lean aspect, because
they help document results and mistake-proof further work.
PERSONALIZING JMP
JMP provides many ways to personalize the user experience. These options
range from speciﬁcation of what you want to see by default in existing reports,
to creating customized reports for speciﬁc analysis or reporting tasks. Many fea-
tures can be customized in the Preferences menu, located under File (or under
JMP on the Mac). More advanced customization may require you to write or
adapt scripts. The ability to Journal and Layout, both located under Edit, also
allows you to document, annotate, and save results from an analysis session.
Customization options include:
◾Conﬁguring the initial state of the report that a platform produces using
Preferences
◾Deleting from or adding to a report
◾Combining several reports from different platforms
◾Deﬁning custom analyses and reports
◾Adding to, deleting from, or rearranging menu items
◾Using Journal ﬁles to lay out a series of steps for users to take
◾Developing JMP-based applications using Application Builder
◾Deploying applications via add-ins
Some of these possibilities are further developed in Chapter 11.
By allowing such a high degree of user customization, JMP enables every
user to best leverage their skills and capabilities. With relatively little effort, the
software can become a good ﬁt for an individual or group of similar individuals.
VISUAL SIX SIGMA DATA ANALYSIS PROCESS AND ROADMAP
As this quick tour of functionality may suggest, JMP has a very diverse set of
features for both EDA and CDA. The earlier section “Visual Displays and Analy-
ses Featured in the Book” gave a preview of those parts of JMP that you will see
again in later chapters. In this section, we put the list of techniques into context

A F I R S T L O O K A T J M P
59
to help you see how they can be combined to support Visual Six Sigma’s goal
of getting value from data.
In Chapter 2, we discussed the outcome of interest to us, represented by Y,
and the causes, or inputs that affect Y, represented by Xs. As you saw, Six Sigma
practitioners often refer to the critical inputs, resources, or controls that deter-
mine Y as Hot Xs. Although many Xs have the potential to affect an outcome,
Y, the data may show that only certain of these Xs actually have an impact on
the variation in Y. In the credit card example from Chapter 2, whether a person
is an only child or not may have practically no impact on whether that person
responds to a credit card offer. In other words, the number of siblings is not a
Hot X. However, an individual’s income level may well be a Hot X.
Consider the Visual Six Sigma Data Analysis Process, illustrated in
Exhibit 3.29, which was ﬁrst presented in Chapter 2. In your Six Sigma
projects, ﬁrst you determine the Y or Ys of interest during the Frame Problem
step. Usually, these are explicit in the project charter or they follow as process
outputs from the process map. In Design for Six Sigma (DFSS) projects, the Ys
are usually the Critical to Quality Characteristics (CTQs).
The Xs of potential interest must be identiﬁed prior to the Collect Data step.
To identify Xs that are potential drivers of the Ys, a team uses process maps,
contextual knowledge, brainstorming sessions, cause-and-effect diagrams,
cause-and-effect matrices, and other techniques. Once the Xs have been listed,
you seek data that relate these Xs to the Ys. Sometimes these observational
data exist in databases. Sometimes you have to begin data collection efforts to
obtain the required information.
Frame Problem
Collect Data
Uncover
Relationships
Statistics as
Detective (EDA)
Utilize
Knowledge
Revise
Knowledge
Model
Relationships
Statistics as
Lawyer (CDA)
Exhibit 3.29 Visual Six Sigma Data Analysis Process

60
V I S U A L S I X S I G M A
Once the data have been obtained, you face the issue of identifying the Hot
Xs. This is part of the Uncover Relationships step. Once you have identiﬁed the
Hot Xs, you may or may not need to develop an empirical model of how they
affect the Ys. Developing this more detailed understanding is part of the Model
Relationships step, which brings us back to the signal function, described in
Chapter 2. You may need to develop a model that expresses the signal function
for each Y in terms of the Hot Xs. Here we illustrate with r Hot Xs:
Y = f (X1, X2, … , Xr)
Only by understanding this relationship at an appropriate level can you set
the Xs correctly to best manage the variation in Y.
Identifying the Hot Xs and modeling their relationship to Y (for each Y) is
to a large extent the crux of the Analyze phase of a DMAIC project, and a large
part of the Analyze and Design phases of a DMADV project.
Exhibit 3.30 shows an expansion of the Visual Six Sigma Roadmap that was
presented in Chapter 2. Recall that this Roadmap focuses on the three Visual
Six Sigma Data Analysis Process steps that most beneﬁt from dynamic visual-
ization: Uncover Relationships, Model Relationships, and Revise Knowledge.
In this expanded version, we show how a subset of the techniques listed ear-
lier in this chapter can be used in a coordinated way to accomplish the goals of
Visual Six Sigma. In our experience, this represents an excellent how-to guide
for green belts who are starting the Analyze phase of a traditional Six Sigma
Exhibit 3.30 Visual Six Sigma Roadmap
Visual Six Sigma Roadmap—What We Do
Uncover Relationships
Dynamically visualize the variables one at a time
Dynamically visualize the variables two at a time
Dynamically visualize the variables more than two at a time
Visually determine the Hot Xs that affect variation in the Ys
Model Relationships
For each Y, identify the Hot Xs to include in the signal function
Model Y as a function of the Hot Xs; check the noise function
If needed, revise the model
If required, return to the Collect Data step and use DOE
Revise Knowledge
Identify the best Hot X settings
Visualize the effect on the Ys should these Hot X settings vary
Verify improvement using a pilot study or conﬁrmation trials

Exhibit 3.31 Platforms and Options Illustrated in the Remaining Chapters
JMP Menu Option
Chapter 4
Chapter 5
Chapter 6
Chapter 7 Chapter 8
Chapter 9
Chapter 10
Chapter 11
Managing
Data and
Its Quality
Reducing
Hospital Late
Charges
Transforming
Pricing
Management
Quality of
Anodized
Parts
Informing
Pharmaceutical
Sales
Improving
Polymer
Manufacturing
Classiﬁcation
of Cells
Beyond
Point and
Click
File
Open > Data with Preview
X
X
Tables
Summary
X
X
X
Subset
X
X
X
Sort
X
X
Transpose
X
Join
X
Concatenate
X
Missing Data Pattern
X
X
X
X
X
Rows
Hide and Exclude
X
X
X
Exclude/Unexclude
X
Hide/Unhide
X
Label/Unlabel
Colors/Markers
X
X
Next Selected
X
Row Selection
X
X
X
X
Clear Row States
X
X
X
Color or Mark by Column
X
X
X
Select Matching Cells
X
Data Filter
X
X
(continued)
61

Exhibit 3.31 (Continued)
JMP Menu Option
Chapter 4
Chapter 5
Chapter 6
Chapter 7 Chapter 8
Chapter 9
Chapter 10
Chapter 11
Managing
Data and
Its Quality
Reducing
Hospital Late
Charges
Transforming
Pricing
Management
Quality of
Anodized
Parts
Informing
Pharmaceutical
Sales
Improving
Polymer
Manufacturing
Classiﬁcation
of Cells
Beyond
Point and
Click
Cols
New Column
X
X
x
Column Info
X
X
X
X
X
X
X
Column Properties
X
X
X
X
X
X
X
Preselect Role
X
Formula
X
X
X
X
X
X
Hide/Unhide
X
X
Exclude/Unexclude
X
x
Columns Viewer
X
X
X
Utilities
Recode
X
Modeling Utilities
x
X
Explore Outliers
x
X
Explore Missing Values
X
Make Validation Column
x
Group Columns
X
X
X
DOE
Custom Design
X
Save Factors and Responses
X
Full Factorial Design
x
62

Analyze
Distribution
X
X
X
X
X
X
X
Histogram
X
X
X
X
X
X
X
Continuous Fit
X
Frequency Distribution
X
X
X
X
X
X
Fit Y by X
X
X
X
Bivariate Fit
X
X
Fit Line
X
Fit Special
X
Fit Polynomial
X
Contingency
X
X
Oneway
X
X
X
Box Plots
X
Means Diamonds
X
Compare Means
X
X
Tabulate
X
X
X
Fit Model
X
X
X
X
X
Standard Least Squares
X
X
X
X
Stepwise
x
X
All Possible Models
X
Generalized Regression
X
Random Effects (REML)
X
Nominal Logistic
X
Macros—Response Surface
X
Macros—Factorial to degree
X
Effect Summary
X
X
X
(continued)
63

Exhibit 3.31 (Continued)
JMP Menu Option
Chapter 4
Chapter 5
Chapter 6
Chapter 7 Chapter 8
Chapter 9
Chapter 10
Chapter 11
Managing
Data and
Its Quality
Reducing
Hospital Late
Charges
Transforming
Pricing
Management
Quality of
Anodized
Parts
Informing
Pharmaceutical
Sales
Improving
Polymer
Manufacturing
Classiﬁcation
of Cells
Beyond
Point and
Click
Modeling
X
X
Partition
X
Decision Tree
X
X
Boosted Tree
X
Neural Net
X
Model Comparison
X
Multivariate
Multivariate
X
Methods
Correlation and
Scatterplot Matrix
X
Cluster
X
Principal Components
X
Quality and
Control Chart Builder
X
X
X
x
X
Process
Process Capability Analysis
X
x
X
Measurement Systems Analysis
X
Variability/Attribute Gauge Chart
X
Process Capability
X
Goal Plot
X
Pareto Plot
X
X
Diagram (Ishikawa C&E)
X
64

Graph
Graph Builder
X
X
X
X
X
Bubble Plot
X
Scatterplot Matrix
X
X
X
Scatterplot 3D
X
Surface Plot
X
Proﬁler
X
X
Maximize Desirability
X
X
Sensitivity Indicators
X
X
Simulator
X
X
Contour Proﬁler
X
Tree Map
X
Tools
Lasso
X
Other
Local Data Filter
X
X
X
X
X
Column Switcher
X
X
X
JSL—Writing Scripts
X
Application Builder
X
65

66
V I S U A L S I X S I G M A
project, or anyone who is simply faced with the need to understand the relation-
ship between some set of inputs and some outcome measure outside a DMAIC
or DMADV framework.
However, remember that the EDA approach to uncovering relationships
requires an unfolding style of analysis in which your next step is determined
by your interpretation of the previous results. So, although it is a great starting
point to guide your usage of JMP in many situations, Exhibit 3.30 should never
be followed slavishly or without thought. As you gain more familiarity with
your data and with JMP, you may well develop your own Visual Six Sigma
style that works better for you and your business.
TECHNIQUES ILLUSTRATED IN THE REMAINING CHAPTERS
Chapters 5 through 10 contain six case studies drawn from real situations. Each
illustrates a selection of techniques that support the Visual Six Sigma Roadmap.
All of the case studies strongly emphasize uncovering relationships and there-
fore rely heavily on visualization techniques directed at discovery. The table in
Exhibit 3.31 indicates which platforms and options, presented as JMP menu
items, are illustrated in the remaining chapters.
We invite you to work through the case studies using the data tables pro-
vided. As mentioned earlier, the case studies assume that you are using the
default settings in JMP (which you can reset under File > Preferences by click-
ing the Reset to Defaults button).
CONCLUSION
In this chapter, you have been given an initial overview of JMP as the enabling
technology for Visual Six Sigma. Our goal has been to familiarize you with JMP
so that you are now able to follow the JMP usage in the case studies that show
Visual Six Sigma in action.
It has been said, “Quality is (or should be) in everything.” The next chapter,
Chapter 4, discusses data quality and management in detail. The open-ended
nature of Six Sigma projects will likely mean that you have to handle these kind
of issues routinely.
Chapter 4 is followed by the six case study chapters in Part Two of the book.
NOTES
1. JMP, “JMP 12 Online Documentation,” www.jmp.com/support/help.
2. For more information about JMP, see www.jmp.com.
3. http://www.jmp.com/content/dam/jmp/documents/en/white-papers/jmp-doe-advantage.pdf
(accessed 23 January 2016).

C H A P T E R 4
Managing Data
and Data Quality
67
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

68
V I S U A L S I X S I G M A
E
veryone knows the expression “Garbage in, garbage out.” Data quality is
clearly a vital topic if you hope to get value from data. Though it seems
obvious, the concept is difﬁcult to pin down. Quality is generally taken to
mean “ﬁtness for purpose.”1 Because data can be used for so many reasons and
in so many different contexts, once we get beyond a few general principles, it
can be difﬁcult to give guidance that is not speciﬁc to the case in point.
Consequently, this chapter differs from the ones that follow in that we don’t
give a single, end-to-end narrative. Instead we examine two small case stud-
ies that show some of the capabilities and features of JMP that are useful in
addressing data quality, and which you are likely to need in other situations
when addressing the quality of your own data.
The data sets used in this chapter are available at http://support.sas.com/
visualsixsigma.
DATA QUALITY FOR VISUAL SIX SIGMA
There are numerous frameworks for assessing data quality. Probably one of the
simplest uses the dimensions2 of:
◾Accuracy
◾Interpretability
◾Availability
◾Timeliness
In the enterprise setting, data quality is usually considered a mature topic.
The investments required to build and support the large-scale IT systems that
can deliver the promise of what SAS calls “The Power to Know™” necessarily
imply a high degree of repetition, and the end result is often a suite of fairly
simple reports or data tailored to the needs of speciﬁc users. As a consequence,
the meaning of data quality for enterprise applications is well understood,3
and there is a lot of information about related areas such as master data man-
agement4 and data governance.5 Similarly, there are many well-established
systems,6 usually embedded in extract, transform, and load (ETL) processes,7
that safeguard the quality of the end reports. Because the data domain is both
restricted and familiar (albeit sometimes on a very large scale), such systems
often rely on fairly simple rules to deliver data of high quality for the intended
use, usually by considering just one variable at a time. In the world of data
modeling8 and data warehousing,9 the slogan “One version of the truth” is
used as an aphorism for the desired end result.
Once the purpose of using data moves beyond standard reporting into
attempts to model relationships (often for the purpose of making predictions),
the question of data quality becomes more subtle and interesting.10 As you

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
69
will see, some modeling approaches can even ﬁnd application in assessing
data quality. In the enterprise setting, these discussions are facilitated because
the data to be used are already within the scope of the enterprise systems
mentioned earlier and because the analytical objectives are clear and known
at the start of the project (for example, to make predictions using a boosted
tree model about the propensity of customers with particular usage patterns to
leave an Internet service provider).
However, questions about data quality in Visual Six Sigma are even more
interesting, because projects are much more open-ended than those mentioned
above. So in our situation, traditional approaches to data quality are necessary,
but not sufﬁcient.
Speciﬁcally, Visual Six Sigma projects often rely on the following:
◾
Data sources that are new in the sense that the data have not been stud-
ied before. As a result, the data domain is not well understood before
the data are actually inspected.
◾
Ad hoc access to data from a number of sources.
◾
Ad hoc data manipulation to produce a single consolidated JMP data
table that will be consumed by the various JMP platforms.
◾
The techniques in “Uncover Relationships” to make sense of the data in
the given context, also generating new questions along the way.
◾
The interplay between “Uncover Relationships” and “Model Relation-
ships,” often in an iterative, unfolding style as the information content
in the data becomes clearer and feasible modeling objectives emerge.
So, along with the issue of extending traditional concepts of data quality to
problems that may require deeper analysis, Visual Six Sigma forces us to relax
the idea that all data quality issues can be addressed once, up front. Instead, and
as discussed in Chapter 2 in the section “Visual Six Sigma: Strategies, Process,
Roadmap and Guidelines,” it is more useful to address data quality issues as
we develop our understanding both of the data and of the questions the data
can reasonably be expected to answer. In other words, the techniques in this
chapter are likely to be useful at any point in the Visual Six Sigma process where
you are handling data.
Of course, if the Visual Six Sigma project goes well and the objectives are
successfully met, there may be sufﬁcient business value in bringing such data
within the scope of enterprise systems. In fact, many large, enterprise-scale
projects can use the Visual Six Sigma approach within feasibility or pilot studies
to help reﬁne the scope and objectives of larger projects.
Finally, a word about technology: JMP is a desktop product that supports
Visual Six Sigma. It does this by holding data in memory to give the speed
and agility that you require to make new, data-driven discoveries. However,

70
V I S U A L S I X S I G M A
as shown in Chapter 11, it is easily automated via the JMP Scripting Language
(JSL). Although desktop computers are becoming ever more powerful, it would
be wrong to think that JMP can also adequately support robust, batch-oriented
processing in the enterprise setting. It can, however, easily be used as a desktop
client in such situations, allowing you to work visually with data of high quality
(in the traditional, enterprise sense).
THE COLLECT DATA STEP
The “Collect Data” step in Visual Six Sigma is brieﬂy described in Chapter 2
in the section “Visual Six Sigma: Strategies, Process, Roadmap and Guidelines.”
This discussion is from the viewpoint of the preceding “Frame Problem” step.
However, given that it makes sense to make an initial assessment of data quality
as soon as possible, we now consider this step in more detail and from a more
data-oriented point of view (see Exhibit 4.1).
Your ﬁrst step is to get your data into JMP. Data access is a big topic in
itself and as we will see, there can be an interaction between the way data are
read into JMP and the subsequent data management activities. The JMP menu
commands File > Open, File > Database, File > SAS and File > Internet
Open provide different ways to import data, but there are other options, too,
particularly if you are working with Microsoft Excel®, R, or MATLAB. As usual,
the use of JSL opens up even more possibilities for accessing data (for example,
by using dynamically linked libraries and sockets).
The examples in this chapter only use some of the simpler data access
methods, and the case studies in this book start with data already in a JMP
data table. For more details on other access methods, refer to Help > Books >
Using JMP.
Similarly, data management has many aspects, and the title of the chapter
is meant to convey that we will only directly consider those that are required
by our two illustrative examples. In general, this functionality is found under
Clean
Reshape
Combine
Derive
Describe
Data Access
Data Management
Exhibit 4.1 Data Management Activities in the Collect Data Step of Visual Six Sigma

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
71
JMP’s Tables, Rows, and Columns menus. These menus are brieﬂy described
in Chapter 3, “Visual Displays and Analyses Featured in the Case Studies.” If
you want to know more about what Tables > Split does, for example, select
this option, then press Help in the launch dialog.
Exhibit 4.1 shows several different tasks within data management:
◾
Reshape. Get data in the correct arrangement for analysis, with rows
representing observations and columns representing variables.
◾
Combine. Join separate tables using a common key. Input tables may ﬁrst
need to be appropriately summarized so that the level of data granularity
is common.
◾
Clean. Check for obvious errors in the data and take remedial action.
◾
Derive. Construct useful new columns that are dependent on existing
columns.
◾
Describe. Produce simple numerical and graphical summaries.
Note that “Describe” leads to the Uncover Relationships Visual Six Sigma
step, and, in line with our comments above, there is not necessarily a clear
boundary.
Bear in mind that an approach that works well with 100 rows may not work
so well when you have 1,000,000 rows. Similarly, handling 10,000 columns
can be very different from dealing with 10. Thankfully, JMP usually provides
many ways to accomplish the required goal. You need to use methods that
deal effectively with the size and structure of your own data. An appropriate
approach inevitably leads to many insights.
The following two examples show how to accomplish many of these tasks in
JMP and also how to address some data quality issues in the context of modeling
as opposed to traditional reporting. Chapter 11 also contains a further example
about missing values, a vital consideration in data quality. In these examples,
we omit some of the detailed JMP steps and keystrokes that you’ll see in the
case studies. If you are relatively new to JMP, you may want to skip ahead to
the case studies and return to these examples once you are comfortable with
basic navigation, graphing, and data analysis in JMP.
EXAMPLE 1: DOMESTIC POWER CONSUMPTION
The ﬁle household_power_consumption.txt contains a subset of data down-
loaded from the UCI Machine Learning Repository.11 The page is http://archive
.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption.
This gives some details for you to peruse before reading on. To locate this ﬁle,
ﬁnd the book’s Journal using File Explorer (Windows) or Finder (Mac), then
move to the subfolder Chapter 4—Data Quality and Management.

72
V I S U A L S I X S I G M A
The page describes measurements of the power consumption of a single
household in the vicinity of Paris over a period of two years. The fact that the ﬁle
size is about 65 MB suggests that the usage data is very ﬁne-grained, presumably
captured by some form of automated monitoring.
Importing the Data
The ﬁle household_power_consumption.txt is located in the Chapter 4 sub-
folder of the supplemental materials that you downloaded. Select File > Open
in JMP and browse to the location of the text ﬁle. Because of the .txt ﬁle exten-
sion, JMP provides different options for importing the ﬁle.
◾On Windows, select Data with Preview from the Open as: radio button.
◾On Mac, select Data (Using Preview) from the Open As list.
Then click Open. Be patient; you are dealing with more than a million
records.
Exhibit 4.2 Text Import Preview Options

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
73
Using this option, JMP infers the structure of the contents of the ﬁle but
also allows you some control over the import process. The dialog shown in
Exhibit 4.2 appears. The code 0x3b in the End of Field panel represents the
semicolon, which is used as a delimiter in the text ﬁle.
After reviewing the dialog to see that it is consistent with your understand-
ing of how the ﬁle contents should be imported, click Next to move to the next
screen (Exhibit 4.3).
The icons to the left of the column names at the top of the window indicate
whether JMP will create a Numeric (“123”) or Character (“ABC”) variable from
the contents of the corresponding ﬁeld. You can change the data type or exclude
the variable by clicking repeatedly on its icon. Variables that are designated to
be Numeric will be imported using an inferred format. However, you can view
and change that format by clicking on the variable’s red triangle. Looking at
the variable names and the values that are displayed, and recalling that JMP
represents Date Time values as Numeric, it seems clear that all variables should
be treated as Numeric. So, click on each “ABC” icon to change it to the icon for
a Numeric data type. Then click Import.
Exhibit 4.3 Text Import Preview Window with Column Option

74
V I S U A L S I X S I G M A
Exhibit 4.4 Warning Dialog
After showing a progress bar, JMP issues the warning shown in Exhibit 4.4.
You will explore the reason for the warning in the next section. For now, click
OK to obtain the resulting table, which has 1,023,663 rows and 9 Columns
(Exhibit 4.5). Again, be patient! (The journal link household_power_
consumption.jmp gives the imported JMP table.)
Identifying Missing Data
But what about the warning that JMP issued? To investigate the reason for the
warning, repeat the import process, simply accepting all the JMP recommen-
dations. This produces a second JMP table in which the columns from Global_
active_power to Sub_metering_2 (variables 3 to 8) are imported as character
variables.
With the ﬁrst table active, select Tables > Missing Data Pattern. Select all
the variables, click Add Columns, and click OK. This produces Exhibit 4.6.
This table shows that most of the rows in the ﬁrst table have no missing
values, and that 21910 rows have an identical pattern of missing data. For these
rows, the columns from Global_active_power to Sub_metering_3 (columns 3
to 9) have missing values.
The Missing Data Pattern table is linked to the original data table. Select
row 2 and return to the original table. Note that 21,910 rows are selected. In
the original data table, select Rows > Next Selected to see that row 6635 is
the ﬁrst row that has missing values.
Now close Missing Data Pattern, switch to the second table, select Rows
> Row Selection > Go To Row, enter 6635, and click OK. You see that, in
the original text ﬁle, a “?” character was used to indicate that a value was not
recorded. It is the presence of these characters that caused JMP to recommend
importing variables 3 to 8 into character columns.
Note that the missing value indicator in Sub_metering_3 is a dot, the
missing data indicator for numeric data. In the original text ﬁle, the ﬁeld for

Exhibit 4.5 The JMP Table with Data from household_power_consumption.txt
75

Exhibit 4.6 Missing Data Pattern
76

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
77
Sub_metering_3 was left blank. JMP inferred that the values were missing
numeric values. For this reason, Sub_metering_3 was imported with a numeric
data type.
Close the second table, select the heading of each column in household_
power_consumption.jmp in turn, and review the contents of the Cols > Col-
umn Info dialog. Note that the ﬁrst two columns, Date and Time, are formatted
correctly. These columns have a Numeric data type, a Continuous modeling
type, and have been given an appropriate format (the Day/Month/Year format
for Date being reﬂective of a European locale). The remaining columns have a
Numeric data type and a Continuous modeling type.
In general, JMP has done a good job in reading in the data we need in a
format that makes sense.
How Often Were Measurements Made?
A casual inspection of the data table seems to indicate that measurements were
made on the minute, since Time entries seem to always end in “:00”. But it’s a
little difﬁcult to tell with over a million rows! To investigate this, select Cols >
New Column. Select Formula from the Column Properties dropdown list and
build the formula shown in Exhibit 4.7. The Second function can be found
under the Date Time group under Functions (grouped). The Second function
returns the number of seconds in a date-time value.
Click OK twice to make a new column (Column 10) in the table. Now we
just need to identify the unique values in this new column. There are a number
of ways to do this, including using the Distribution platform.
In fact, Distribution is always a good way to take a ﬁrst look at any data.
Select Analyze > Distribution, select all the variables, and click OK. This results
in Exhibit 4.8.
Note that the Summary Statistics table for Date is hard to interpret, and for
good reason. The values result from the fact that JMP represents a date quantity
as the number of seconds since midnight on January 1, 1904. By right-clicking
on the column of numbers in this table, you can select the Format Column
option. This allows you to change the format appropriately to make things
more intelligible. Many or even most of the display boxes that compose a JMP
report respond to such a right-click, offering options to change the appearance
of the display.
Select Display Options > Customize Summary Statistics from the red
triangle for Column 10. This allows you to add N Missing and N Unique to the
report for this variable. This conﬁrms that there are no missing values (as we
knew already), and there is only one unique value, so measurements were
indeed only made on the minute. (Alternatively, you might note that the min-
imum and maximum values, given under Quantiles, are both 0.)

78
V I S U A L S I X S I G M A
Exhibit 4.7 Formula to Find Number of Seconds
Were Measurements Made Every Minute?
Now that Column 10 has served its purpose, you can select it in the data table
(household_power_consumption.jmp) and then select Cols > Delete
Columns. Note that JMP automatically removes this variable from the open
Distribution report to maintain consistency.
To investigate whether the measurements were attempted every minute in
the day, we can use Tables > Summary. Enter Time as a Group variable and
click OK to obtain the table partially shown in Exhibit 4.9.
You see that the summary table has 1,440 rows (the number of minutes in
a day). You also see that the study apparently ran for 711 days. Close the sum-
mary table. In the Distribution report that is still open, note that the Date distri-
bution shows that the study ran from about December 2008 to November 2010.
Were Measurements Made Continuously and on Every Day?
Let’s now investigate this in more detail. Presumably the measurements were
made on consecutive days, but perhaps not on weekends?

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
79
Exhibit 4.8 Distribution of Columns (Partial View)
Exhibit 4.9 Were Measurements Attempted Every Minute?

80
V I S U A L S I X S I G M A
Exhibit 4.10 Constructing a Virtual Column
Select Graph > Graph Builder to open a window containing the list of
columns in the table and various zones into which you can drag column names
to build the graph you want. Right-click on Date to open a cascading context
menu, and select Date Time > Month Year as in Exhibit 4.10.
This adds a new ordinal column, Month Year, to the bottom of the column
list. This is called a virtual column, since it is only accessible from within the
Graph Builder report and is not in the table itself. Drag this column to the
X zone. (Be patient, as this may take a few seconds.) This produces a horizon-
tal boxplot, which is not particularly informative. Click Undo, right-click on
the green icon to the left of Month Year in the list of columns, and change the
modeling type to Nominal (the icon should become red). Now drag the col-
umn back to the X zone to produce Exhibit 4.11, which is a much more useful
display.
Note that the months at both ends of the time series are not as tall (do not
have as many rows) as the majority, and this is to be expected. But there is also
some variation among the months in the body of the series, which might be a
cause for concern. However, by looking at the horizontal scale, you can see that
these just correspond to the fact that all months except February have either
30 or 31 days, and that February has 28 days since no leap years occurred in
this date range.
Proceeding in a similar way, deﬁne a Day of Week virtual column by select-
ing Day of Week from the context menu for Date. Drag the new virtual column
to the Wrap zone (in the top right corner) to produce Exhibit 4.12.

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
81
Exhibit 4.11 Number of Measurements by Month
Exhibit 4.12 Number of Measurements Each Month by Day of the Week
Now you can easily see that measurements were taken on weekends (Day
of Week is 1 or 7) as well as on weekdays.
Note that, in general:
◾
Even though the modeling type of a column may be obvious when the
intention is to use it for analysis, for visualization another modeling type

82
V I S U A L S I X S I G M A
may be better. JMP makes it easy to switch between modeling types
and to see the effect. If JMP produces a graph that is unexpected or
unhelpful, try changing the modeling type.
◾Virtual columns are a quick and easy way to construct derived columns
without having to return to the table itself and build a new column using
the Formula Editor. If a virtual column is not useful, it will not persist
when the report window is closed. But if it is valuable, you can use the
context menu to save that column to the table and make it available to
other JMP platforms.
How Does Global_active_power Vary over Time?
Now let’s look at how Global_active_power varied over time. Knowing how
households typically operate, we might anticipate some seasonality, and varia-
tion through time of day and/or day of week.
Click Start Over in Graph Builder. Drag Global_active_power to the Y zone
and Month Year to the X zone. Drop Day of Week next to the right of Month
Year. Click Done to gain some screen area and to produce Exhibit 4.13.
The variation in total power usage between months, and the expected sea-
sonal effect, is very clear. The variation related to Day of Week is perhaps less
than would be expected if there were only one or two working occupants, so
perhaps the home is a family residence.
To investigate the variation throughout the day, you could group the Time
column into a new virtual column Hour. However, we take another approach.
Select Show Control Panel from the red triangle menu, and click Start Over
in Graph Builder. Drag Global_active_power to the Y zone and Time to the
X zone. Right-click in the graph area, select Smoother > Change To > Points,
and then Add > Smoother. This shows all of the measured values, and also
brings the smoother to the front to make it visible. (Alternatively, you can click
the Points element, which is the left-most icon above the graph. Then click the
Smoother element to remove the smoother, and click it again to reinstate the
smoother over the points.)
Next, double-click on the horizontal axis. In the Scale panel, change
the Minimum value to 00:00:00 and the Maximum value to 23:59:59. In the
Tick/Bin Increment panel, set the Increment to 1 hour. Then click OK. This
produces Exhibit 4.14. The smoother shows the expected decrease in total
power consumption in the early morning hours, with local peaks at breakfast
and in the evening.
In addition, the day of the week might account for additional structure in
the data and is also worth investigating. To do this, right-click on Day of Week
in the list of columns, and select Add To Data Table. Uncheck Show Control
Panel in Graph Builder and click Done. Then select Script > Local Data Filter

Exhibit 4.13 Seasonal and Weekly Trends in Global_active_power
83

84
V I S U A L S I X S I G M A
Exhibit 4.14 Daily Trend in Global_active_power
from the red triangle menu of Graph Builder, which allows you to conditionally
ﬁlter the report window by values of any of the variables in the parent data
table. Select Day of Week in the Add Filter Columns list, and click Add. Click
on the value 7 in the local data ﬁlter, to produce Exhibit 4.15.
Select each day in turn in the local data ﬁlter to view the daily trend for
particular days of the week. It seems clear that the time variation within a day is
reasonably consistent during the week, but is different on Saturday and Sunday.
Additionally, the stratiﬁcation in the data that was detected in Exhibit 4.14 is
more readily apparent in Exhibit 4.15. You can conﬁrm that all the days show
this feature to some extent as you click through the other levels of Day of Week.
(Alternatively, select Animation from the red triangle menu and click the right
arrow in the Animation Controls panel.) When you are ﬁnished, close the
report window.
How do Power, Current, and Voltage Vary?
Since we expect power, voltage, and current to be related, we next study
them together. Select Analyze > Distribution and select Global_active_power,
Global_reactive_power, Voltage, and Global_intensity. Check Histograms
Only and then click OK. Next, press and hold the Control key, click the red

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
85
Exhibit 4.15 Daily Trend in Global Active Power on Day 7
triangle for any one of the variables and select Outlier Box Plot, then release
the Control key. This broadcasts the command to all plots so that box plots
appear for all four variables, producing the results shown in Exhibit 4.16. (Note
that to replicate Exhibit 4.16 exactly, you also have to use the hand tool to
decrease the histogram bin size so that the histograms display ﬁner detail. With
over a million rows, each bin still contains a large number of observations.)
The supply voltage (Voltage) varies between about 223 and 254 Volts,
which is a typical range of values in Europe. But the remaining three variables
have a clear bimodal distribution, and Global_reactive_power has a signiﬁcant
number of zero values beyond the main distribution. Household power is
distributed over a large grid of transmission lines with many consumers (both
domestic and industrial). Even though the physics is simple,12 the dynamics
of this system are complex.13 The power company uses reactive power (which
does not produce useful energy) to try to stabilize the supply voltage as
the load on the grid ﬂuctuates, and it appears as if their operating policy is
activated only when a small threshold is passed. The variable Global_intensity
represents the current drawn by all the appliances of the household. Note that
the information in Exhibit 4.16 was actually present in Exhibit 4.8, but at that
time the focus of our investigation was different so it was easy to overlook.
Of course, the household only has a relatively small number of appliances,
each of which can be on or off, according to the needs of the occupants. Even
though the current drawn and the voltage supplied to a particular appliance
may ﬂuctuate depending on the dynamics of the grid, you would still expect

Exhibit 4.16 Univariate Distributions of Power, Voltage, and Current
86

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
87
the overall “on-off” state for the household (which will vary through the day)
to produce some quantization of the power consumed, Global_active_power.
Additionally, and as you have seen already, the daily usage schedule of the
household will vary by month and year. Therefore, to investigate this possibility
further, you will look in detail at how the power, voltage, and current vary
throughout a typical day: June 23, 2009.
Close the Distribution report window and return to household_power_
consumption.jmp. Select Rows > Row Selection > Select Where and com-
plete the dialog box that appears as shown in Exhibit 4.17.
Clicking OK selects the 1,440 rows corresponding to this day. Select Tables
> Subset and click OK to produce a new (linked) table that contains only the
1440 rows for June 23, 2009. Note that Subset respects any row selection in
the active data table.
The variables Sub_metering_1, Sub_metering_2 and Sub_metering_3 are
the power consumed in particular areas of the household. Subtracting the
sub-metering values from Global_active_power and taking into account the
different measurement units gives the power consumption in the remainder of
the household.
Select Cols > New Column, and deﬁne a new column called No_Sub_
metering via the formula shown in Exhibit 4.18. (Alternatively, open the data
Exhibit 4.17 Selecting All Rows for June 23, 2009

88
V I S U A L S I X S I G M A
Exhibit 4.18 Deﬁning No_Sub_metering with a Formula
table household_power_consumption_June23rd_2009.jmp, where the
following steps have been completed.)
For convenience, select the three constituent variables and the new
variable in the columns panel of the table and use a right-click to group them
into a single column group using Group Columns. Rename this group by
double-clicking on it and entering Sub Metering.
Using Graph > Graph Builder, you can easily construct Exhibit 4.19. (Date
table is household_power_consumption_June23rd_2009.jmp, script is
Graph Builder.) You need to adjust the axis settings for Time as described
earlier and use the red triangle options or click Done to hide the control panel
and legend. The usage of various appliances throughout the day does exhibit
the banding you would expect. The causes of the spike in No_Sub_metering at
about 07:00:00 and the peak toward the end of the day are not clear without
some background knowledge.
Of course, the concept of a typical day is a little dubious, so on reﬂection, it
might have been better to look at this using the full table household_power_
consumption.jmp rather than just our subset table. As mentioned before, this
unfolding rather than predetermined style of working with data is typical of

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
89
Exhibit 4.19 Power Drawn by Different Appliances on June 23rd 2009
Visual Six Sigma. Thankfully, this is easy to do by using the data ﬁlter to tame
complexity.
Copy and paste the formula shown in Exhibit 4.18 from the subset table to
a new column in the full table, and then close the subset table. Set the mod-
eling type of Date to be Nominal, and use Graph Builder in conjunction with
Local Data Filter to produce Exhibit 4.20. Note that you need to drag the four
variables to the Y zone one at a time, placing them carefully in relation to the
variables already in the Y zone.
Selecting Animation from the red triangle menu of the Local Data Filter
allows you to loop quickly over every day in the study (711 of them), and to
review the usage pattern on each. If your focus is on between-day variability
rather than within-day variability, you can use the Lock Scales option on the
red triangle menu of Graph Builder to hold the axis scale ﬁxed.
Referring back to Exhibit 4.1, this example shows some aspects of the Clean,
Derive, and Describe steps in a data table with more than a million rows and
nine initial columns. The dynamic nature of the investigation, depending on
what catches your attention, should be clear.
The table household_power_consumption_ﬁnal.jmp contains the
columns deﬁned in this section as well as scripts that reproduce the plots.

Exhibit 4.20 Power Drawn by Different Appliances, Filtering by Day
90

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
91
EXAMPLE 2: BISCUIT SALES
In many Visual Six Sigma (and other) situations, the data you need for an anal-
ysis are spread among two or even more tables. There can be many causes for
this situation and this example considers such a scenario. But almost all JMP
platforms operate on a single data table. In keeping with the goal of this chapter,
which is to provide you some familiarity with mechanics common to managing
data, the example in this section shows you how to combine data from two
sources.
Here is the scenario. You have purchased data about biscuit sales from a
data provider. You want to combine these sales data variables with data you
already have about the products themselves.
Two Data Tables
Using the book’s journal, open the data table Biscuit Products.jmp using the
link under the Chapter 4 > Example 2 – Biscuits outline. Exhibit 4.21 shows
that this data table has 23,880 rows and seven columns, all with a Character
data type and Nominal modeling type.
The column names are descriptive of the meaning of the variables. You see
right away that some of the columns contain empty cells.
Next, open the data table Biscuit Sales.jmp using the appropriate journal
link. Exhibit 4.22 shows that this table has 238,880 rows and six columns,
two with a Nominal modeling type and four with a Continuous modeling
type. The two Nominal columns, PRODUCT_ID and RETAILER, appear to
be similar to the columns Product ID and Retailer in Biscuit Products.jmp.
The meaning of Volume Purchased and Value Purchased is fairly obvious,
and AT_RISK_VOLUME and AT_RISK_VALUE contain ﬁgures that the data
provider calculates using a secret, but industry-respected, algorithm. In broad
terms, they represent the propensity of purchasers to switch to another product.
Finding the Primary Key
To focus your efforts, let’s suppose you want to investigate the pack sizes used to
package and sell different biscuit categories, and the value at risk for the various
pack sizes. This pack size information is contained in the column Number in
Multipack in the Biscuit Products.jmp data table. As usual, the steps required
are inﬂuenced by what you already know about the data (or what you believe
you know).
First, you need to determine which column or columns to use to connect or
join the two tables. In data modeling jargon, you need to decide on the primary
key—that ﬁeld or combination of ﬁelds that serve to uniquely deﬁne or label
each row in the table.

Exhibit 4.21 Table Biscuit Products.jmp (Partial View)
92

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
93
Exhibit 4.22 Table Biscuit Sales.jmp (Partial View)
With Biscuit Products.jmp active, select Tables > Summary, assign
Product ID to the Group role, and click OK. The resulting table has 2,985 rows,
telling us that Product ID has 2,985 levels or distinct values.
The values in N Rows tell us that each level occurs 8 times. You can easily
check this in several ways:
◾
By making a second summary table.
◾
By selecting Analyze > Distribution and entering N Rows as Y,
Columns.
◾
By selecting Cols > Columns Viewer and Show Summary.
◾
By selecting the N Rows column, then Cols > Utilities > Recode and
inspecting the result.
Close the summary table. Following the same process, you can also establish
that Retailer has 8 distinct levels, each of which occurs 2,985 times.
Given that 2, 985 × 8 = 8 × 2, 985 = 23, 880, which is the number of rows in
Biscuit Products.jmp, you might expect that it is the combination of Product
ID and Retailer that deﬁnes the primary key for Biscuit Products.jmp. You
can easily check that this is indeed the case by making a summary table using
both these variables in the Group role and noting that each row in the resulting
table occurs once and only once.
Note that JMP provides several ways to get the number of distinct values in
a column, but if you want to do this for combinations of columns, you need to
use the Tables > Summary method described above.

94
V I S U A L S I X S I G M A
Joining the Tables
By making Biscuit Sales.jmp active and repeating similar steps, you can con-
ﬁrm that each combination of PRODUCT_ID and RETAILER occurs 10 times in
this table. It may or may not be reasonable to assume that each occurrence rep-
resents the sales ﬁgures for that combination for a particular time period, and
that the row ordering in the table corresponds to increasing time. Given that
this ordering won’t be used in your subsequent analysis, it doesn’t matter in this
case, but this is the kind of detail that you should check with the data provider.
Make Biscuit Products.jmp the active table and select Tables > Join to
inspect the options that JMP provides for joining tables together. In this case,
you want to join using Match Columns (the default method). Populate the
dialog as shown in Exhibit 4.23. To do this:
◾Select Biscuit Sales from the list in the upper left to reveal a second list
of columns in the dialog. These are the columns in the table you wish
to join with the active table, in this case Biscuit Products.jmp.
◾Select Product ID in the upper Source Columns list and PRODUCT_ID
in the lower Source Columns list.
◾Press the Match button.
◾Repeat this process with Retailer and RETAILER. Note that you are per-
forming the join by matching the contents of more than one column.
◾Under Output Options, check the Select columns for joined table
option.
◾In the Select box, enter all columns from the Biscuit Products list and
all except the ﬁrst two columns from the Biscuit Sales list. By including
only one copy of Product ID and Retailer, you avoid getting two columns
containing this information, one from each source table. (Rather than
selecting columns in this way, you can alternatively check Merge Same
Name Columns in the dialog.)
◾Click OK to produce the table Biscuits.jmp, which should have
238,800 rows and 11 columns (Exhibit 4.24).
Recoding Number in Multipack
Make the joined data table active. Or open the joined table, called Biscuits
.jmp, from the journal ﬁle. This table also contains scripts for the analyses that
you conduct later in this example.
The information about pack size is in the column Number in Multipack. Run
Distribution on this column to see that the values in this column are UNIDEN-
TIFIED, SINGLE, or of the form X PACK, where X is the number of items in
the pack.

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
95
Exhibit 4.23 The Join Dialog
However, for the purposes of displays that you will create later on, you want
to consider the number of items in a pack as numeric with an ordinal modeling
type. To do this, you need to recode the values and recast the column to have
a Numeric data type and an Ordinal modeling type.
The recode feature, Cols > Utilities > Recode, provides a convenient way
to do this. Select the column Number in Multipack and then select Cols >
Utilities > Recode. Complete the dialog as shown in Exhibit 4.25. (Script is
Recode Number in Multipack in Biscuits.jmp.)
Note that the UNIDENTIFIED level initially appears last. But when you
delete UNIDENTIFIED under New Values, indicating that you want to recode it

96
V I S U A L S I X S I G M A
Exhibit 4.24 Biscuits.jmp (Partial View)
Exhibit 4.25 Recoding Number in Multipack
as missing, Recode groups it with the level that is already missing. This moves
the value UNIDENTIFIED to the top, as shown in the exhibit.
When you have completed the dialog, click Done > New Column. The new
column, Number in Multipack 2, inherits the Character data type and the Nom-
inal modeling type.
From the Recode dialog in Exhibit 4.25 you will notice that 66,160 rows
contained the null string and 240 rows contained UNIDENTIFIED. So you
should expect that the ﬁnal column Number in Multipack 2 contains 66,400

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
97
missing values among the 238,800 rows in the table. You can check this using
Tables > Missing Data Pattern.
Select the column Number in Multipack 2 followed by Cols > Column Info
to change its data type from Character to Numeric and its modeling type from
Nominal to Ordinal.
Investigating Packaging
Now that you have your data properly structured, it’s easy to build the displays
you need to investigate the pack sizes used for the various biscuit categories.
Select Graph > Graph Builder and drag Number in Multipack 2 to the X
zone. Right-click on the display, and select Boxplot > Change To > Histogram.
Double-click on the horizontal axis and change the increment to 1. Then drag
Category to the Y zone. Click Done to produce Exhibit 4.26. (Script is Graph
Builder in Biscuits.jmp.)
It’s clear that CHOCOLATE BISCUIT BARS are made available in many
more pack sizes than the other biscuit categories.
To look at the AT_RISK_VALUE ﬁgures, use Graph Builder with AT_
RISK_VALUE in the X zone and Category in the Y zone. Because of the many
Exhibit 4.26 Pack Sizes for Different Biscuit Categories

98
V I S U A L S I X S I G M A
Exhibit 4.27 Value at Risk for Different Biscuit Categories
large values, the box plots, which represent the bulk of the data, are hard to
see. Double-click on the horizontal axis and set the Minimum to 0 and the
Maximum to 220,000. This gives the view shown in Exhibit 4.27. (Script is
Graph Builder 2 in Biscuits.jmp.)
Perhaps, as might be suggested by the name, the value from SEASONAL
ASSORTMENTS seems to be most at risk. The values from CHOCOLATE BIS-
CUIT BARS and HEALTHIER BISCUITS are least at risk. Keep in mind that, in
Exhibit 4.27, some extreme values are not shown because the upper value for
the horizontal was set to make it easier to compare the body of the distributions.
Keys and Duplicate Rows
As the example shows, a common requirement for joining tables is to ﬁnd
unique record identiﬁers in the tables. As shown above, Tables > Summary
allows you to do this. Assign all the columns that are considered to compose
the primary key to the Group role and inspect N Rows in the resulting table
for values bigger than one. It may also help to use Tables > Sort to bring the
largest values to the top of the summary table.
If you want to examine your data table rows for complete duplicates, in the
sense of identical rows, add all the columns in the table, irrespective of their
modeling type as Group variables. The fact that the summary table is linked to
the source table is also useful in selecting duplicate records in the source table.
If duplicates do exist, the next question that arises is what you should do
about them. Unfortunately, there is no general answer. It might make sense to
delete all but the ﬁrst or last occurrence of each record. It is easy to write the
required formula or a JSL script that automates such a protocol and produces a

M A N A G I N G D A T A A N D D A T A Q U A L I T Y
99
ﬁnal table. Generally, doing such cleaning manually can be very tedious, even
with only a moderate number of rows. As with many data issues, this is an area
where even a little knowledge of JSL can really help you out. We consider this
topic in more detail in Chapter 11, Beyond “Point and Click” with JMP.
Finally, if you expect to be using similar data in the future, you should
determine why duplicates appeared in the ﬁrst place and address that cause.
CONCLUSION
In this chapter you have seen two representative examples that give an idea of
how you can use JMP to assess the quality and makeup of your data. As pointed
out in the Introduction, the question of to what extent a given set of data is ﬁt
for purpose is strongly inﬂuenced by what that purpose actually is. Visual Six
Sigma projects are by their nature exploratory, and often involve data that are
under scrutiny for the ﬁrst time. Therefore, it is natural that considerations of
data quality and structure arise throughout the analysis ﬂow, rather than being
exclusively a front-end activity.
There is a real sense in which every set of data you are confronted with is
unique. Teasing out the nuances of which questions that data can reasonably
be expected to answer, and how well, is an essential part of a Visual Six Sigma
project. In the worst case, a proper assessment of data quality will prevent you
from pushing the data too far and should indicate how to collect better data in
the future.
Although you might think that the topics of data quality and management
are unglamorous, some facility with their mechanics is vital for anyone using
Visual Six Sigma, or indeed anyone working with data more generally. As you
work with JMP, you will ﬁnd that the interactive and visual nature, coupled
with the menu options under Cols > Utilities and Cols > Modeling Utilities
that you see in later chapters, make it very well suited to these tasks.
The case studies are presented in the next six chapters. They are the essential
feature of this book, and each one shows the coordinated use of the requisite
techniques in the context of a real-world narrative. To help you keep track of
the narrative, but also to grasp the details, we separate most of the JMP “how
to” instructions from the story by placing them in a box.
We remind you that there is a tension between illustrating Visual Six Sigma
as a lean process and helping you see a variety of techniques and approaches
that you can use in your future work. As we have stressed, Visual Six Sigma
is not, and never can be, prescriptive. One of the keys to success is familiarity
with JMP, so, as noted earlier, we have sometimes deliberately compromised
the purity of our Visual Six Sigma vision to show you patterns of use that will
be useful as you embark on your own Visual Six Sigma journey.
Now, let’s start our detective work!

100
V I S U A L S I X S I G M A
NOTES
1. L. Vlãsceanu, L. Grünberg, and D. Pârlea, 2007, Quality Assurance and Accreditation: A Glossary
of Basic Terms and Deﬁnitions (Bucharest, UNESCO-CEPES), revised and updated edition. ISBN
92-9069-186-7.
2. For precise descriptions, see Richard Y. Wang and Diane M. Strong, “Beyond Accuracy: What
Data Quality Means to Data Consumers,” Journal of Management Information Systems 12, no. 4
(1996).
3. Larry English, Improving Data Warehouse and Business Information Quality: Methods for Reducing
Costs and Increasing Proﬁts (Hoboken, NJ: Wiley, 1999).
4. David Loshin, Master Data Management (Morgan Kaufman OMG Press, 2008).
5. John Ladley, Data Governance: How to Design, Deploy and Sustain an Effective Data Governance
Program (Morgan Kaufmann Series on Business Intelligence, 2012).
6. SAS Data Governance, www.sas.com/en_us/software/data-management/data-governance
.html (accessed 29 June 2015).
7. SAS
Data
Quality,
www.sas.com/en_us/software/data-management/data-quality.html
(accessed 29 June 2015).
8. Ralph Kimbal and Margy Ross, The Data Warehouse Toolkit: The Complete Guide to Dimensional
Modeling (Hoboken, NJ: Wiley, 2002).
9. Ralph Kimbal and Margy Ross, The Kimball Group Reader: Relentlessly Practical Tools for Data Ware-
housing and Business Intelligence (Hoboken, NJ: Wiley, 2010).
10. See, for example, Gerhard Svolba, Data Quality for Analytics Using SAS (SAS Publishing, 2012).
11. University of California at Irvine, “UCI Machine Learning Repository,” http://archive.ics.uci
.edu/ml/ (accessed 29 June 1015).
12. Wikipedia, “AC Power,” https://en.wikipedia.org/wiki/AC_power (accessed 29 June 1015).
13. Wikipedia, “Smart Grid,” https://en.wikipedia.org/wiki/Smart_grid (accessed 29 June 1015).

PART
TWO
Case Studies
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

C H A P T E R 5
Reducing Hospital
Late Charge
Incidents
103
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

104
V I S U A L S I X S I G M A
T
his case study is set early in the deployment of a Lean Six Sigma initiative at
a community hospital. You were hired as a master black belt to help support
the Lean Six Sigma effort. Management identiﬁed the billing department,
and in particular late charges, as an area that tied up millions of dollars per year,
and hence an area badly in need of improvement. This case study describes how
you, along with a team from the billing department, analyze billing data in order
to deﬁne project opportunities for green belt teams.
The case study is almost entirely embedded in the Uncover Relationships
stage of the Visual Six Sigma Data Analysis Process introduced in Chapter 2
(Exhibit 2.3). You are a detective, searching for clues that might explain the
large number of late charges. Your work encompasses a number of the tech-
niques described under Uncovering Relationships in Exhibit 2.4. Your ﬁndings
will initiate the work of project teams; these teams will work through all six of
the stages of the Visual Six Sigma Data Analysis Process to solve the problems
you identify.
In this case study, you analyze the late charge data for January 2015. The
data set consists of 2,032 records with late charges. The columns include both
nominal and continuous variables, as well as dates. You visualize the data using
histograms, bar graphs, Pareto plots, control charts, scatterplots, mosaic plots,
and tree maps, learning about potential outliers and conducting a missing data
analysis along the way.
Exhibit 5.1 lists the JMP platforms and options that you and the team use
in your discovery process. The data sets are available at http://support.sas.com/
visualsixsigma. Based on what you learn, you will make several important rec-
ommendations to management.
FRAMING THE PROBLEM
Niceland Community Hospital has recently initiated a Lean Six Sigma pro-
gram to improve the efﬁciency and quality of its service, to increase patient
satisfaction, and to enhance the hospital’s already solid reputation within the
community.
The hospital has hired you, a master black belt, to provide much needed
support and momentum for the Lean Six Sigma initiative. You will be respon-
sible for identifying and mentoring projects, providing training, and support-
ing and facilitating Lean Six Sigma efforts. Upper management has identiﬁed
the billing department as being particularly prone to problems and inefﬁcien-
cies, and directs you to work with members of the department to identify and
prioritize realistic green belt projects within that area. Speciﬁcally, upper man-
agement mentions late charges as an area that ties up several million dollars
per year.
In accordance with your mandate, you proceed to address late charges,
which have been a huge source of customer complaints, rework, and lost

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
105
Exhibit 5.1 Platforms and Options Illustrated in This Case
Menus
Platforms and Options
File
Open > Data with Preview
Tables
Summary
Sort
Missing Data Pattern
Rows
Hide and Exclude
Row Selection
Select Matching Cells
Data Filter
Cols
New Column
Column Info
Column Properties
Formula
Columns Viewer
Analyze
Distribution
Histogram
Frequency Distribution
Fit Y by X
Bivariate Fit
Fit Special
Contingency
Oneway
Tabulate
Quality and Process
Control Chart Builder
Pareto Plot
Graph
Tree Map
Tools
Lasso
revenue. You start by working with members of the department to learn as
much as you can about late charges. Here is what you learn:
Late charges can apply to both inpatients and outpatients. In both
cases, tests or procedures are performed on a patient. The charge
for each test or procedure is ideally captured at the time that the
procedure is performed. However, a charge cannot be captured until
the doctor’s notes are completed, because the charge must be
allocated to the relevant note. Sometimes notes aren’t dictated and
transcribed for as much as a week after the procedure date.
Once the charge is captured, it waits for the billing activity to come
around. The billing activity occurs a few days after the procedure
date, allowing only a short time for all the charges related to that
patient to be accumulated. However, it is never really clear what

106
V I S U A L S I X S I G M A
might still be outstanding or if all of these charges have rolled in.
At this point, the hospital drops the bill; this is when the insurance
companies or, more generally, the responsible parties, are billed for
the charges as they appear at that point.
Now, once the bill is dropped, no additional charges can be billed
for that work. If charges happen to roll in after this point, then a
credit has to be applied for the entire billed amount, and the whole
bill has to be recreated and submitted. Charges that roll in after the
hospital drops the bill are called late charges. For example, an
invoice of $200,000 might have to be redone for a $20 late charge,
or the charge might simply be written off.
If a patient is an inpatient, namely a patient who stays at least one
night in the hospital, charges are captured during the patient’s stay.
No bill can be issued until the patient is discharged. A few days after
discharge, the bill is dropped.
By the way, the date is February 11, 2015. Now that you believe you under-
stand what a late charge is, you plan on examining some recent late charge data.
You start by looking at the January 2015 listing of late charges.
COLLECTING DATA
You obtain the late charge report for January 2015, along with all other available information. You
import this data to a JMP ﬁle, and save the ﬁle as LateCharges.jmp (see Exhibit 5.2 for a
partial view of the data table). There are 2,032 records of late charges in January.
There are seven columns of interest (the eighth column, which is hidden
and excluded, gives the row number). The seven columns are described below:
1. Account. The account identiﬁer. A patient can have several account
numbers associated with a single stay. An account number is generated
for each group of procedures or tests.
2. Discharge Date. The date when the patient was discharged from the
hospital.
3. Charge Date. The date when the charge was captured internally. In other
words, this is the date when the charge ofﬁcially makes it into the hos-
pital’s billing system.
4. Description. The procedure description as it appears on the charge
invoice.

Exhibit 5.2 Partial View of LateCharges.jmp Data Table
107

108
V I S U A L S I X S I G M A
5. Charge Code. The originating ﬁnance group charge area. For example,
RXE refers to medication ordered in an emergency.
6. Charge Location. The physical area within the hospital where the charge
originated.
7. Amount. The dollar amount of the late charge. Credits have a negative
value.
You save these descriptions for each variable in the data table as notes. The
Column Info panel for Account, with the column property Notes, is shown in
Exhibit 5.3.
To add descriptions for a variable to the data table, as shown in Exhibit 5.3 (note that these
descriptions are already included in LateCharges.jmp):
1.
Right-click in the column header for the variable and select Column Info.
2.
Click on Column Properties and choose Notes.
3.
Type (or copy and paste) the column description into the text box (see Exhibit 5.3).
Tip: To view a note for a variable, hold your mouse over the variable name in the Columns panel in
the data table.
Exhibit 5.3 Column Info Window with Note Describing the Account Column

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
109
UNCOVERING RELATIONSHIPS
Given that your goal is to identify projects, you will not need to work through
all six steps in the Visual Six Sigma Data Analysis Process. You nonetheless fol-
low the guidance that the Visual Six Sigma Roadmap provides for the Uncover
Relationships step. You will use dynamic linking to explore the variables one at
a time and several at a time. Given the nature of the data, you will often need
to connect any anomalous behavior that you identify using plots to the records
in the data table. This is made easy by dynamic linking. Your exploration will
also include an analysis of missing records.
Visualizing the Variables One at a Time
Your ﬁrst step in exploring the data is to gain an understanding of each of your
variables. You will do this using Columns Viewer.
Columns Viewer
The Columns Viewer report in Exhibit 5.4 displays summary statistics for the
seven variables. It shows the number of unique values, or categories, for each
categorical variable, and high-level summary statistics for each continuous
variable.
The Columns Viewer also tells us something about the quality of our data.
For example, of the 2,032 late charges in the data table, the Charge Code is
missing for 467 of the late charges.
Exhibit 5.4 Columns Viewer, Seven Variables

110
V I S U A L S I X S I G M A
To create the table of summary statistics for each variable using the Columns Viewer, as shown
in Exhibit 5.4 (LateCharges.jmp, script is Columns Viewer):
1.
Select Cols > Columns Viewer.
2.
Select all of the variables from the Select Columns list and click Show
Summary.
The seven variables are already selected. Click on the Distribution button
in the Summary Statistics outline to obtain Distribution reports for all of the
variables. The report that is partially shown in Exhibit 5.5 appears. This conve-
nient shortcut produces the same reports that you would obtain using Analyze
> Distribution.
You note from Exhibit 5.4 that Account and Description are nominal vari-
ables with many levels. The bar graphs in Exhibit 5.5 are not necessarily helpful
in understanding such variables. So, for each of these, you click on the red tri-
angle for each variable, choose Histogram Options, and uncheck Histogram.
This leaves only the Frequencies for these two variables in the report. The saved
script is Distribution in LateCharges.jmp.
Dates in JMP
Consider the results for Discharge Date and Charge Date shown in Exhibit 5.6.
The data for these two variables consist of dates in a month/day/year format.
The values that appear in the Quantiles report are in date format. However, the
values in the Summary Statistics report are numeric.
JMP dates are stored as the number of seconds since January 1, 1904. To
see this, consider Discharge Date. Click on its column header in the data table
and select Column Info. In the resulting dialog, you see that JMP is using m/d/y
as the Format to display the dates. If you change the Format to Best, you will
see the date in terms of seconds since January 1, 1904.
When Distribution is run on a column that has a date format, calculations
are performed in terms of seconds. For the histogram and quantiles, the seconds
are converted back to dates. However, the values that appear in the Summary
Statistics report are reported in terms of seconds. This is because some of the
reported statistics, such as the standard deviation, would be meaningless if they
were converted back to dates.
The mean, however, would be meaningful. To convert the Summary
Statistics values to a date format, double-click within the outline and select
the format Date > m/d/y.

Exhibit 5.5 Partial View of Distribution Report
111

112
V I S U A L S I X S I G M A
Exhibit 5.6 Reports for Discharge Date and Charge Date
Summarizing Your Findings
By studying the graphs and results for all seven columns in the Columns Viewer
and the Distribution reports, you learn the following:
◾Account: There are 390 account numbers involved, none of which are
missing.
◾DischargeDate:Thedischargedatesrangefrom9/15/2013to1/27/2015,
with 50 percent of the discharge dates ranging from 12/17/2014 to
12/27/2014, a one-month period (see Exhibit 5.6). None of these are
missing.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
113
◾
Charge Date: As expected, the charge dates all fall within January 2015
(see Exhibit 5.6). Note that about 50 percent fall between 1/10 and 1/15.
The rest seem evenly distributed with the month. None of these are
missing.
◾
Description: There are 414 descriptions involved, none of which are
missing.
◾
Charge Code: There are 46 distinct charge codes, but 467 records are
missing charge codes.
◾
Charge Location: There are 39 distinct charge locations, with 81 records
missing charge location.
◾
Amount: The amounts of the late charges range from −$6,859 to
$28,280. None of these are missing.
Look more closely at the distribution of Amount shown in Exhibit 5.7 (to
produce a horizontal layout, select Display Options > Horizontal Layout from
the red triangle for Amount). The Quantiles report, together with the histogram,
shows that there is a single, unusually large value of $28,280 and a single out-
lying credit of $6,859. Otherwise, the distribution is roughly symmetric and is
centered at zero. About 50 percent of the amounts are negative, and so consti-
tute credits.
Understanding the Missing Data
You are concerned about the missing values in the Charge Code and Charge
Location columns, as they represent a fairly large proportion of the late charge
records.
Missing Data Pattern
To understand the structure of missing values across a collection of variables
you use Missing Data Pattern.
Exhibit 5.7 Distribution Report for Amount

114
V I S U A L S I X S I G M A
To create the Missing Data Pattern report shown in Exhibit 5.8 (LateCharges.jmp,
script is Missing Data Pattern):
1.
Select Tables > Missing Data Pattern.
2.
Enter all seven of the columns in the Add Columns column box, and click OK.
The ﬁrst three columns give the Count, the Number of columns missing,
and the Patterns of missing values. Each row corresponds to a unique pattern
of missing values. The pattern is deﬁned by the original columns in the data
table. A value of one in a column for an original variable indicates that, for that
variable, there are missing values for all rows in the Count, while a value of
zero indicates no missing values for those rows. You see that:
◾There are 1,491 records with no missing values.
◾There are 74 records with missing values only in the Charge Location
column.
◾There are 460 records with missing values only in the Charge Code
column.
◾There are 7 records with missing values in both the Charge Location
and Charge Code columns.
So, 534 records contain only one of Charge Location or Charge Code. Are
these areas where the other piece of information is considered redundant? Or
would the missing information actually be needed? These 534 records represent
over 25 percent of the late charge records for January 2015.
You believe that improving or reﬁning the process of providing this infor-
mation is worthy of a project. If the information is redundant, or can be entered
automatically, then this could simplify the information ﬂow process. If the infor-
mation is not input because it is difﬁcult to do so, the project might focus on
ﬁnding ways to facilitate and mistake-proof the information ﬂow.
Locations with Most Missing Charge Codes
Just to get a little more background on the missing data, you check to see which
locations have the largest numbers of missing charge codes.

Exhibit 5.8 Missing Data Pattern
115

116
V I S U A L S I X S I G M A
To view the locations with the largest number of missing charge codes (LateCharges.jmp,
script is Pareto Plot of Charge Location):
1.
While holding down the shift key, select rows 3 and 4 of the Missing Data
Pattern table.
This selects the 467 rows in LateCharges.jmp where Charge Code is
missing.
2.
In the LateCharges.jmp data table, right-click on Selected in the Rows panel
and select Data View (see Exhibit 5.9).
This produces a new data table consisting of only the 467 rows where Charge Code is missing.
All of the rows in this new data table are selected since they are linked to the main data table. To
deselect the rows, click in the bottom triangle in the upper left corner of the data grid
(see Exhibit 3.18).
With this 467-row Data View table as the active data table, you construct
a Pareto Plot to see if any of the charge locations are missing a large number
of charge codes. The plot in Exhibit 5.10 shows that Charge Locations LB1 and
T2 have the highest occurrence of missing data for Charge Code.
To produce the Pareto Plot shown in Exhibit 5.10 (LateCharges.jmp, script is Pareto
Plot of Charge Location):
1.
Make the Data View table the active table.
2.
From the Analyze menu select Quality and Process > Pareto Plot.
3.
Enter Charge Location as Y, Cause, and click OK.
Number of Missing Charge Codes by Location
You would like to see a table that shows the number of records for each Charge
Code and the number of records that are missing. You can construct such a table
using Tables > Summary.

Exhibit 5.9 Selection of Data View from Rows Panel
117

118
V I S U A L S I X S I G M A
Exhibit 5.10 Pareto Plot of Charge Location
To construct the summary data table shown in Exhibit 5.12 (LateCharges.jmp, script is
Missing Charge Code):
1.
Close the Data View table and make sure that LateCharges.jmp is the active
data table.
2.
Select Tables > Summary and enter Charge Location into the Group panel.
3.
Select Charge Code in the Select Columns list, then click the Statistics button and
select N Missing from the dropdown menu (see Exhibit 5.11).
JMP will compute the percent missing for Charge Code for each Charge
Location.
4.
Click OK.
The resulting data table is automatically named LateCharges By (Charge
Location). Each of the 40 Charge Location values deﬁnes a row. Note the
following:
◾The value of Charge Location in row 1 is the missing value indicator.
◾The N Rows column gives the number of rows in LateCharges.jmp
with the given value of Charge Location.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
119
Exhibit 5.11 Populated Summary Launch Dialog
◾
The N Missing(Charge Code) column gives the number of rows in
LateCharges.jmp for which Charge Code is missing.
◾
The ﬁrst row shows that 81 rows in LateCharges.jmp are missing
Charge Location. Of these, 7 are missing Charge Code.
To identify the charge locations that have the most missing charge codes,
sort the table in descending order of N Missing(Charge Code). Right-click on
the column header for N Missing(Charge Code) and select Sort > Descending.
The ﬁrst 15 rows of the resulting table are shown in Exhibit 5.12. (The script is
Missing Charge Code Sorted in LateCharges.jmp.)
The N Missing(Charge Code) column gives the frequencies shown in the
Pareto Plot in Exhibit 5.10. You learn that all of the charge codes for LB1 are
missing, while a smaller proportion of the charge codes for T2 are missing.
Percent of Missing Charge Codes by Location
Now you’re wondering about the percentage of Charge Code entries that are
missing for these areas. Is the percentage much higher than for other areas?
You want to add a percentage column to your table. Use the Formula Editor to
construct a new column with this information (see Exhibit 5.15).

120
V I S U A L S I X S I G M A
Exhibit 5.12 Partial Sorted Summary Data Table
To calculate the percentage using the Formula Editor, as shown in Exhibit 5.13
(LateCharges.jmp, script is Percent Missing Charge Code):
1.
In the data table LateCharges By (Charge Location), right-click in the
column header area to the right of the third column, N Missing(Charge Code), and
select New Column.
2.
In the New Column window, enter Percent Missing Charge Code next to
Column Name.
3.
Click on Column Properties and select Formula.
4.
Click on N Missing(Charge Code) in the Table Columns list to enter this
column into the formula.
5.
Click the division symbol on the keypad.
This creates a fraction and opens a highlighted box for the denominator contents.
6.
Select N Rows from the Table Columns list.
Since the denominator box was highlighted, the column N Rows is placed in
that box.
7.
Click OK to close the Formula Editor.
8.
From the dropdown menu next to Format in the Column Info window, select
Percent (Exhibit 5.14).
9.
Change the value next to Dec to 1 to display one decimal place.
10.
Click OK to close the Column Info window.
You right-click on the Percent Missing Charge Code column in the data
table and select Sort > Descending to produce the table shown in Exhibit 5.15.
The Percent Missing Charge Code column indicates that, for the January 2015

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
121
Exhibit 5.13 Formula for Percent Missing Charge Code
Exhibit 5.14 Column Info Dialog Showing Selected Format

122
V I S U A L S I X S I G M A
Exhibit 5.15 Partial Table with Percent Missing Charge Code
data, LB1 and OR1 are always missing Charge Code for late charges, while T2
has missing Charge Code values about 25 percent of the time. This is useful
information, and you decide that this will provide a good starting point for a
team whose goal is to address the missing data issue.
Analyzing Amount
When you studied the histogram for Amount (Exhibit 5.7), you identiﬁed two
outliers. You would like to study these more carefully.
With LateCharges.jmp as the active data table, run a Distribution analysis
for Amount. From the top red triangle menu, choose Stack to obtain the report
in Exhibit 5.16 (script is Distribution of Amount). You want to examine the
entire record for each outlier. You will do this by selecting the points in the plot
and creating a table using Data View.
Exhibit 5.16 Distribution of Amount with Two Outliers Selected

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
123
To select the two outliers in the histogram and create a data view table of the two points as shown in
Exhibit 5.17 (LateCharges.jmp, script is Select Two Outliers):
1.
Select the outlier to the left, click and drag to draw a rectangle around the point.
2.
Hold the Shift key while you drag a rectangle around the outlier to the right.
Holding the Shift key retains the previous selection.
3.
In the Rows panel in the data table, look at the number Selected to verify that two
points are selected.
4.
Right-click on Selected and select Data View.
You note that the ﬁrst record is a credit (having a negative value). You exam-
ine the late charge data for several prior months and eventually ﬁnd that this
appears to be a credit against an earlier late charge for an amount of $6,859.30.
The second record corresponds to a capital equipment item, so you make a note
to discuss this with the group that has inadvertently charged it here, where it
does not belong.
For now, you decide to exclude these two points from further analysis.
To exclude and hide the two points (LateChargesjmp, script is Exclude Two Outliers):
1.
Close the data table created by Data View and make the data table
LateCharges.jmp active.
2.
With the two points still selected, select Rows > Hide and Exclude.
This has the effect of both excluding the two selected rows from further calculations
and hiding them in all graphs.
3.
Check the Rows panel to verify that the two points are Excluded and Hidden.
4.
Deselect the two points.
Run the script Distribution of Amount to rerun the Distribution report.
This gives the report shown in Exhibit 5.18. Note that N is now 2,030, reﬂecting
the fact that the two rows containing the outlying values are excluded.

Exhibit 5.17 Data View of the Two Outliers for Amount
124

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
125
Exhibit 5.18 Distribution of Amount with Two Outliers Removed
The symmetry of the histogram for Amount about zero is striking. You note
that the percentiles shown in the Quantiles report are nearly balanced about
zero, meaning that many late charges are in fact credits. Are these credits for
charges that were billed in a timely fashion? The fact that the distribution is
balanced about zero raises the possibility that this is not so, and that they are
actually credits for charges that were also late. You decide that this phenomenon
warrants further investigation.
Visualizing the Variables Two at a Time
Days after Discharge
Reviewing Exhibit 5.6, you are surprised that late charges are being accu-
mulated in January 2015 for patients with discharge dates in 2014. In fact,
25 percent of the late charges are for patients with discharge dates preceding
12/17/2014, making them very late indeed.
To get a better idea of how delinquent these are, you deﬁne a new column
called Days after Discharge and write the required formula. The date formats
convert such a value in seconds to a readable date. For example, the value
86400, which is the number of seconds in one day, will convert to 01/02/1904
using the JMP m/d/y format.
To create a formula for the number of days after discharge (included in LateCharges.jmp,
script is Define Days after Discharge):
1.
Double-click in the column header area to the right of Amount.
2.
Right-click in the header area and select Formula.
3.
In the list of Functions (grouped), select Date Time.
4.
Select Date Difference.

126
V I S U A L S I X S I G M A
5.
With dt1 selected in the formula, select Discharge Date from the Table
Columns list.
6.
Select dt2 in the formula and select Charge Date from the Table Columns list.
7.
Select intervalName in the formula and enter “Day”.
8.
With “Day” selected in the formula, click the up-arrow at the top right of the keypad to
enter the alignment argument.
9.
Type “Start” and click OK.
Your formula should appear as shown in Exhibit 5.19.
Alternatively, you can use the Formula Editor to create a formula that takes
the difference, in days, between Charge Date and Discharge Date. Charge
Date – Discharge Date gives the number of seconds between these two dates.
This means that to get a result in days, you need to divide by the number of sec-
onds in a day, namely, 60 × 60 × 24, which JMP can calculate via the function In
Days (1). After clicking OK, the difference in days appears in the new column.
Exhibit 5.19 Formula for Days after Discharge

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
127
You note that this difference is only an indicator of lateness and not a solid
measure of days late. For example, a patient might be in the hospital for two
weeks, and the late charge might be the cost of a procedure done on admittance
or early in that patient’s stay. For various reasons, including the fact that charges
cannot be billed until the patient leaves the hospital, the procedure date is not
tracked by the hospital billing system. Thus, Discharge Date is simply a rough
surrogate for the date of the procedure, and Days after Discharge undercounts
the number of days that the charge is truly late.
Obtain a Distribution report of Days after Discharge, again choosing Stack
from the top red triangle menu. This report is shown in Exhibit 5.20. (The script
is saved as Distribution Days after Discharge.)
You see that there were some charges that were at least 480 days late. Drag a
rectangle around the points corresponding to the 480 days in the box plot above
the histogram to select them. In the data table, right-click on Selected in the
Rows panel and select Data View. This produces the table shown in Exhibit 5.21.
Only two Account numbers are involved, and you suspect, given that there
is only one value of Discharge Date, that these might have been for the same
patient. You deselect the rows. To arrange the charges by account number, you
sort on Account by right-clicking on its column header and choosing Sort >
Ascending (Exhibit 5.22).
You notice something striking: Every single charge to the ﬁrst Account
(rows 1 to 5) is credited to the second Account (rows 6 to 10)! This is very
interesting. You realize that you need to learn more about how charges make
it into the late charges database. Might it be that a lot of these records involve
charge and credit pairs, that is, charges along with corresponding credits?
A View by Entry Order
At this point, you close this Data View table. You are curious as to whether there
is any pattern to how the data are entered in the LateCharges.jmp data table.
Exhibit 5.20 Distribution of Days after Discharge

Exhibit 5.21 Data View of Rows with 480 Days after Discharge
128

Exhibit 5.22 Data View of Rows with 480 Days after Discharge Grouped by Account
129

130
V I S U A L S I X S I G M A
You notice that the entries are not in time order—neither Discharge Date nor
Charge Date is in order. You consider constructing control charts with the time
order deﬁned by one of these two time variables, but neither would be partic-
ularly informative about the process by which late charges appear.
To quickly check if there is any clear pattern in how the entries are posted,
you construct an Individual Measurement chart, plotting the Amount values
as they appear in row order (see Exhibit 5.23).
To create the Individual Measurement Chart in Exhibit 5.23 (LateCharges.jmp, script is called
Control Chart of Amount):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Drag Amount to the Y drop zone.
3.
To better see the plot, double-click on the vertical axis for the chart for Amount to open
the Y Axis Settings window.
4.
Change the Minimum and Maximum values under Scale to −3000 and 3000,
respectively.
Note that many positive charges seem to be offset by negative charges
(credits) later in the data table. For example, as shown by the ellipses in
Exhibit 5.23, there are several large charges that are credited in subsequent
rows of the data table.
Select the points in the ellipse with positive charges using the arrow tool,
dragging a rectangle around these points. This selects ten points. Then, while
holding down the shift key, select the points in the ellipse with negative charges.
Alternatively, you can select Tools > Lasso or go to the tool bar to get the
Lasso tool, which allows you to select points by enclosing them with a freehand
curve. (The script Selection of Twenty Points selects these points.)
In all, 20 points are selected, as you can see in the Rows panel of Late-
Charges.jmp. Right-click on Selected in the Rows panel and select Data View
to obtain the table in Exhibit 5.24. The table shows that the ﬁrst ten amounts
were charged to a patient who was discharged on 12/17/2014 and credited to
a patient who was discharged on 12/26/2014.
Is this just an isolated example of a billing error? Or is there more activity
of this type? This ﬁnding propels you to take a deeper look at credits versus
charges.

Exhibit 5.23 Individual Measurement Chart for Amount by Row Number
131

Exhibit 5.24 Data View of Charges That Are Later Credited
132

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
133
Amount and Abs(Amount)
To investigate these apparent charge and credit occurrences more fully, con-
struct a new variable representing the absolute Amount of the late charge. This
new variable will help you investigate the amount of money tied up in late
charges. It will also allow you to group the credits and charges.
At this point, close all open data tables except for LateCharges.jmp. To
create a formula for the absolute value of Amount, you use a shortcut that allows
you to create the formula directly from the data table column, rather than by
constructing a new column and then using the Formula Editor.
Right-click on the header for Amount and select New Formula Column
> Transform > Absolute Value, as shown in Exhibit 5.25. This applies the
absolute value function to Amount and places the new column, Abs[Amount],
after the last column in the data table. (This column can also be obtained by
running the script Define Abs [Amount].)
To calculate the amount of money tied up in the late charge problem, use
Analyze > Tabulate (see Exhibit 5.26).
Exhibit 5.25 Construction of Absolute Amount Formula

134
V I S U A L S I X S I G M A
To produce the tabulation results shown in Exhibit 5.26 (LateCharges.jmp, script is
Tabulate Amount and Abs[Amount]):
1.
Select Analyze > Tabulate.
2.
Drag Amount to the Drop Zone for Rows.
3.
Drag Abs[Amount] and place it just below Amount.
4.
Click Done.
The resulting tabulation shows that the total for Abs[Amount] is $186,533
while the sum of actual dollars involved, Sum(Amount), is $27,443. This means
that a high dollar amount is being tied up in credits.
You want to better understand the credit and charge situation. Sorting the
data table on Abs[Amount] may provide some insight into potential patterns.
Close the Tabulate window, go back the LateCharges.jmp table, and construct
a new data table using Tables > Sort. Enter Abs[Amount] and then Amount as
By (see Exhibit 5.27). The saved script in LateCharges.jmp is Make Late-
Charges_Sorted.jmp.
In the sorted data table, in the Columns panel, drag Amount so that it pre-
cedes Abs[Amount], in order to juxtapose Amount and Abs[Amount] for easier
viewing.
The new table reveals that 82 records have $0 listed as the late charge
amount. You make note that someone needs to look into these records to ﬁgure
out why they are appearing as late charges. Are they write-offs? Or are the zero
values errors?
Moving beyond the zeros, you start seeing interesting charge and credit pat-
terns. For an example, see Exhibit 5.28. There are 21 records with Abs[Amount]
equal to $4.31. Seven of these have Amount equal to $4.31, while the remaining
Exhibit 5.26 Tabulation Showing Sum of Amount and Abs(Amount)

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
135
Exhibit 5.27 Dialog for Sort on Abs[Amount] and Amount
14 records have Amount equal to −$4.31. You notice similar patterns repeatedly
in this table—one account is credited, another is charged.
To better understand the charge and credit issue, you decide to compare
actual charges to what the revenue would be if there were no credits (that is, if
all transactions produced revenue). Use Tables > Summary to produce a new
table, with the sum of the Amount values for each value of Abs[Amount]. Then,
in this new data table, create a new variable, Sum If No Credits, which gives
the total amount of the billed charges (if all records were credits).
To create the summary table in Exhibit 5.29 (LateCharges_Sorted.jmp, script is
Summary Table):
1.
Select Tables > Summary,
2.
Select Amount from the list of columns, and select Sum from the list of Statistics.
3.
Select Abs[Amount] as a Group variable.
4.
Click OK.
A table named LateCharges_Sorted By (Abs[Amount]) is created.
5.
In the LateCharges_Sorted By (Abs[Amount]) data table, create a new
column called Sum If No Credits,
6.
Use the Formula Editor to create the formula Abs[Amount] × N Rows.

Exhibit 5.28 Partial View of LateCharges_Sorted.jmp Showing Charges Offset by Credits
136

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
137
Exhibit 5.29 LateCharges_Sorted By (Abs[Amount]).jmp Comparing Net Charges to
Revenue If No Credits
Save this summary table, LateCharges_Sorted By (Abs[Amount]).jmp,
for future reference. A portion of the resulting table is shown in Exhibit 5.29.
For example, note the net credit of $30.17 shown for the 21 records with
Abs[Amount] of $4.31 in row 10 of this summary table. You can only conclude
that a collection of such transactions represents a lot of non-value-added time
and customer frustration.
This table by itself is interesting. There is clearly an issue with credits and
charges. But you want to go one step further. You want a plot that shows how
the actual charges compare to the charges if there were no credits. You decide to
construct a scatterplot of Sum(Amount) by Sum if No Credits (see Exhibit 5.30).
To create the scatterplot shown in Exhibit 5.30 (LateCharges_Sorted By
(Abs[Amount]).jmp, script is Bivariate):
1.
Select Analyze > Fit Y by X.
2.
Enter Sum(Amount) as Y, Response.
3.
Enter Sum if No Credits as X, Factor.
4.
Click OK.
The scatterplot shows some linear patterns, which you surmise represent
the situations where there are either no credits at all or where all instances of a

138
V I S U A L S I X S I G M A
Exhibit 5.30 Bivariate Fit of Sum(Amount) by Sum if No Credits
given Abs[Amount] are credited. To better see what is happening, you ﬁt three
lines to the data on the plot, all with intercept 0:
◾The line with slope 1 covers points where the underlying Amount values
are all positive.
◾The line with slope 0 covers points where the charges are exactly offset
with credits.
◾The line with slope −1 covers points where all of the underlying late
charge Amount values are negative (credits).
To ﬁt the lines shown in Exhibit 5.32 (LateCharges_Sorted By (Abs[Amount])
.jmp, script is Bivariate with Fits):
1.
Click on the red triangle at the top of the report and choose Fit Special.
This opens a window that allows you to choose among various special ﬁts.
2.
Check the two boxes at the bottom of the dialog to constrain the intercept and the slope,
and enter the values 0 and 1, respectively, as shown in Exhibit 5.31. Click OK.
This ﬁts a ﬁrst line.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
139
3.
Choose Fit Special again. Check the two constraint boxes and enter 0 and –1 as the
constraints for the intercept and slope, respectively. Click OK.
This ﬁts a second line.
4.
Choose Fit Special again. Check the two constraint boxes and enter 0 for both
constraints for the intercept and slope. Click OK.
This ﬁts the third line through the points with Sum(Amount) equal to 0.
Your ﬁnal plot is shown in Exhibit 5.32. The points on or near the line with
slope 0 result from a balance between charges and credits, while the points
on or near the line with slope −1 reﬂect almost all credits. The graph paints a
striking picture of how few values of Abs[Amount], only those points along the
line with slope 1, are not affected by credits.
As expected, the graph shows many charges being offset with credits. From
inspection of the data (not shown), you realize that these often reﬂect credits
to one account and charges to another. This is deﬁnitely an area where a green
belt project would be of value.
Revisiting Days after Discharge
The team raises the question of whether there is a pattern in charges that relates
to the amount of time since the patient’s discharge. For example, do credits tend
to appear early after discharge while overlooked charges show up later?
To address this question, you close all of your open reports and data
tables, leaving only LateCharges.jmp open. Select Analyze > Fit Y by X,
enter Amount as Y, Response and Days after Discharge as X, Factor. In the
Exhibit 5.31 Dialog Choices for Line with Slope 1

140
V I S U A L S I X S I G M A
Exhibit 5.32 Scatterplot of Sum(Amount) by Sum If No Credits
resulting plot, you will add a horizontal reference line at the $0 Amount to
help you see potential patterns (see the scatterplot in Exhibit 5.34).
To add a horizontal reference line to the bivariate plot, as shown in Exhibit 5.34
(LateCharges.jmp, script is Bivariate with Reference Line):
1.
Move your cursor to the vertical axis until it becomes a hand.
2.
Double-click to open the Y Axis Settings menu shown in Exhibit 5.33.
3.
In the Reference Lines panel, enter 0 into the Value box and click Add. Click
OK.
The resulting scatterplot shows no systematic relationship between the
Amount of the late charge and Days after Discharge (see Exhibit 5.34).

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
141
Exhibit 5.33 Adding a Reference Line to the Scatterplot
Exhibit 5.34 Scatterplot of Amount versus Days after Discharge
However, the plot does show a pattern consistent with charges and corre-
sponding credits. It also suggests that large-dollar charges and credits are dealt
with sooner after discharge, rather than later.
UNCOVERING THE HOT XS
At this point, you and your team have learned a great deal about your data.
You are ready to begin exploring the drivers of late charges, to the extent

142
V I S U A L S I X S I G M A
possible with your limited data. You are interested in whether late charges
are associated with particular accounts, charge codes, or charge locations. You
suspect that there are too many distinct descriptions to address this without
expert knowledge and that the Description data should be reﬂected in the
Charge Code entries.
In the pursuit of the Hot Xs, you and the team will use Pareto plots, tree
maps, and the data ﬁlter, together with dynamic linking.
Exploring Two Unusual Accounts
A Pareto Plot displays values of a variable, sorted in descending order. Pareto
plots are useful in identifying the most frequently occurring values of a vari-
able. At this point, it seems reasonable to construct a Pareto Plot for Account,
Charge Code, and Charge Location. To gauge the effect of each variable on late
charges, you weight each variable by Abs[Amount], as this gives a measure of
the magnitude of this impact.
To construct the Pareto Plot shown in Exhibit 5.35 (LateCharges.jmp):
1.
Select Analyze > Quality and Process > Pareto Plot.
2.
Insert Account as Y, Cause and Abs[Amount] as Weight.
3.
Click OK.
The resulting Pareto Plot has 389 bars, one for each account represented in the data. Some bars
are so small that they are barely visible (see Exhibit 5.35).
These barely visible bars correspond to Accounts with very small frequen-
cies and/or Abs[Amount]. You decide to combine these into a single bar. The
resulting plot is shown in Exhibit 5.36.
To combine causes into a single bar (LateCharges.jmp, script is Pareto Plot of
Account):
1.
Click on the red triangle and select Causes > Combine Causes.
2.
In the resulting dialog, click the radio button next to Last causes and enter 370.
This asks JMP to combine the last 370 causes.
3.
Click OK.

Exhibit 5.35 Pareto Plot for Account
143

144
V I S U A L S I X S I G M A
Exhibit 5.36 Pareto Plot of Account with Last 370 Causes Combined
The plot shows that two patient accounts represent the largest proportion
of absolute dollars. In fact, they each account for $21,997, for a total of $43,994
in absolute dollars out of the total of $186,533. To see this, in each bar, click
and hold the click.
You are interested in the raw values of Amount that are involved for these
accounts. Select the records corresponding to these two accounts by holding
down the control key while clicking on their bars in the Pareto plot. This selects
948 records. Double-click on one of the two selected bars in the Pareto chart
while holding the control key to create a table containing only these 948 records
(alternatively, in the LateCharges.jmp data table, you can select Data View
to create this table). All rows are selected in this new data table. Save this table
with the name TwoAccounts.jmp. To clear the row selection, choose Rows
> Clear Row States.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
145
You want to see if the Amount values have similar distributions for these two
accounts. You will use Analyze > Fit Y by X to compare these distributions.
To create the one-way plot shown in Exhibit 5.37 (TwoAccounts.jmp, script is Oneway):
1.
Select Analyze > Fit Y by X.
2.
Enter Amount as Y, Response and Account as X, Factor, and click OK.
3.
From the red triangle menu, select Display Options > Points Jittered.
This option randomly jitters the points horizontally so individual observations are
visible.
The plot in Exhibit 5.37 strongly suggests that amounts from the second
account are being credited and charged to the ﬁrst account. To investigate this,
sort the 948-record table in descending order by Abs[Amount]. The pattern of
charges and credits is clearly seen (Exhibit 5.38).
Exhibit 5.37 Oneway Plot for Two Accounts

146
V I S U A L S I X S I G M A
To create the data table view shown in Exhibit 5.38 (TwoAccounts.jmp, script is Sorted
Absolute Amounts):
1.
Right-click on Abs[Amount] and select Sort > Descending.
2.
Select a single cell containing account number A0434300267, and then use Rows >
Row Selection > Select Matching Cells to select all rows corresponding to
that account number.
This allows you to better distinguish the rows corresponding to the two accounts. Alternatively, you
could have selected one such cell and right-clicked in the highlighted row area to the left of the
account number, and selected Select Matching Cells.
This analysis conﬁrms the need to charter a team to work on applying
charges to the correct account. You document what you have learned and then
close the table TwoAccounts.jmp.
Exploring Charge Code and Charge Location
Your team now turns its attention to Charge Code and Charge Location. Earlier
you learned that about 25 percent of records have missing values for at least one
of these variables. You consider constructing Pareto plots, but again, because of
the large number of levels, you would have to combine causes. You decide to
try a tree map, a plot that uses rectangles to represent categories.
To create the tree map shown in Exhibit 5.39 (LateCharges.jmp, script is Tree Map
of Charge Code):
1.
Select Graph > Tree Map.
2.
Enter Charge Code as Categories.
3.
Enter Abs[Amount] as Sizes.
The sizing results in rectangles with areas that are approximately proportional to the
sum of the values in the size column. In other words, a size variable is analogous to a
weighting variable.
4.
Enter Abs[Amount] as Coloring.
This assigns an intensity scale to the rectangles based on the mean of the speciﬁed
value within each category.

Exhibit 5.38 Partial Data Table Showing Amounts for Top Two Accounts
147

148
V I S U A L S I X S I G M A
5.
Enter Abs[Amount] as the Ordering variable.
This asks JMP to try to put rectangles that are of similar sizes relative to Abs(Amt)
close to each other.
6.
Click OK.
Tip: On the screen, the graph displays the default blue-to-red intensity scale. For this book, we have
changed the color theme for the tree map to a white-to-black scale, with black representing the
grouping with the highest mean Abs[Amount].
Exhibit 5.39 Tree Map of Charge Code: Sized, Colored, and Ordered by Abs[Amount]
Studying the Tree Map in Exhibit 5.39, you note that there are ﬁve charge
codes—LBB, RAD, BO2, ND1, and ORC—that have large areas. This means that
these charge codes account for the largest percentages of absolute dollars (one
can show that, in total, they account for 85,839 out of a total of 186,533 absolute
dollars). One of these charge codes, ND1, is colored dark gray. This means that
although the total absolute dollars accounted for by ND1 is less than, say, for
LBB, the mean of the absolute dollars is higher.
Click in the ND1 area in the Tree Map; this selects the seven rows with an
ND1 Charge Code in the data table. In the Rows panel, right-click on Selected
and select Data View to see these rows. All seven rows contain relatively large
charges (see Exhibit 5.40). You note that these charges appear from seven to
eighteen days after discharge. Also, none of these are credits. You and your team
conclude that addressing late charges in the ND1 Charge Code is important.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
149
Exhibit 5.40 Data View of the Seven Charge Code ND1 Rows
Exhibit 5.41 Data View of the 14 Charge Code ORC Rows
You apply this same procedure to select the ORC records, which are shown
in Exhibit 5.41. Here, there are 14 records, and a number of these are credits.
Some are charge and credit pairs. The absolute amounts are relatively large.
You also look at each of the remaining top ﬁve Charge Codes in turn. For
example, the LBB Charge Code consists of 321 records and represents a mean
absolute dollar amount of $87 and a total absolute dollar amount of $28,005.
There appear to be many credits, and many charge and credit pairs. From this
analysis, you and your team agree that Charge Code is a good way to stratify
the late charge problem.
But how about Charge Location? You realize that Charge Location deﬁnes
the area in the hospital where the charge originates and consequently identi-
ﬁes the staff groupings who must properly manage the information ﬂow. You
construct a tree map to investigate charge location.
To construct the tree map shown in Exhibit 5.42 (LateCharges.jmp, script is Tree Map
of Charge Location):
1.
Select Graph > Tree Map.

150
V I S U A L S I X S I G M A
2.
Click on the Recall button.
This reinserts the settings used in the previous analysis (Exhibit 5.39).
3.
Replace Charge Code with Charge Location in the Categories text box.
4.
Click OK.
Again, the color theme has been changed to a white-to-black scale for Exhibit 5.42.
Exhibit 5.42 Tree Map of Charge Location: Sized, Colored, and Ordered by Abs[Amount]
The eight largest rectangles in this graph are: T2, LAB, RAD, ED, 7, LB1,
NDX, and OR. These eight Charge Locations consist of 1,458 of the total 2,030
records, and represent 140,115 of the total of 186,533 absolute dollars. You
wonder about location 7, and make a note to ﬁnd out why this location uses
an apparently odd code.
Next, you examine each of these groupings individually by selecting the
appropriate rectangle and creating a table using Data View. One interesting
ﬁnding is that Charge Location NDX consists of exactly the seven Charge Code
ND1 records depicted in Exhibit 5.40. So it may be that the NDX location only
deals with the ND1 code, suggesting that the late charge problem in this area
might be easier to solve than in other areas.
But this raises an interesting question. You are wondering how many late
charge Charge Code values are associated with each Charge Location. The
team suggests that the more charge codes involved, the higher the complex-
ity of information ﬂow, and the higher the likelihood of errors, particularly of
the charge and credit variety.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
151
To address this question in a manageable way, you select all records corre-
sponding to the eight largest Charge Location rectangles and use Fit Y by X to
create a mosaic plot (see Exhibit 5.43).
To produce the mosaic plot shown in Exhibit 5.43 (LateCharges.jmp):
1.
In the tree map, select the eight charge locations T2, LAB, RAD, ED, 7, LB1, NDX, and OR,
holding down the Shift key to add values to the selection.
In the Rows panel of the data table 1,458 rows appear as Selected.
2.
Right-click on Selected in the Rows panel and select Data View.
3.
In the resulting table, deselect the rows.
(The script Charge Location Subset produces this data table. The data table
is labeled as Charge Location Subset in the exhibits that follow.)
4.
Using this data table, select Analyze > Fit Y by X.
5.
Select Charge Code as Y, Response, enter Charge Location as X, Factor,
and enter Abs[Amount] as Weight.
6.
Click OK.
In the data table produced by the script Charge Location Subset, this mosaic plot is
obtained by running the script called Contingency.
You ﬁnd it noteworthy that Charge Location T2 deals with a relatively large
number of Charge Code values. This is precisely the location that had 25 per-
cent of its Charge Code values missing, so the sheer volume of codes associated
with T2 could be a factor. The remaining areas tend to deal with small numbers
of Charge Code values. In particular, NDX and RAD only have one Charge
Code represented in the late charge data. NDX and RAD may deal with other
codes, but the January 2015 late charges for each of these two areas are only
associated with a single code.
You note that, except perhaps for T2, these charge locations tend to have
charge codes that appear to be unique to the locations. Viewing the colored
version of the mosaic plot on your monitor, you easily see that there is very
little redundancy, if any, in the colors within the vertical bars for the various
charge locations.
You note something interesting. The LB1 Charge Location does not appear
in the mosaic plot—only seven locations are shown. The team reminds you that
Charge Code is entirely missing for the LB1 Charge Location, as you learned
in the earlier analysis of the relationship between Charge Code and Charge
Location.

152
V I S U A L S I X S I G M A
Exhibit 5.43 Mosaic Plot of Charge Code by Charge Location, Weighted by Abs[Amount]
With that mystery solved, you turn your attention back to the mosaic plot in
Exhibit 5.43. You decide to use distribution plots to better see how Charge Code
varies by Charge Location and to use the Data Filter to explore the distribution
of charge codes in the different locations (see Exhibits 5.44 and 5.45).
To create the distribution plots shown in Exhibit 5.44 and 5.45 (use the Charge Subset
Location data set created by running the script with the same name in LateCharges.jmp;
the script in this subset data table is Distributions and Data Filter):
1.
Select Analyze > Distribution.
2.
Enter Charge Code as Y and Charge Location as the By variable.
3.
Click OK.
An alert message indicating: “Column Charge Code from table Charge Location=LB1
has only missing values” displays. This is because Charge Location LB1 has missing
data for all rows.

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
153
4.
Click OK to move beyond the error alert.
5.
Click on the red triangle next to the ﬁrst Distribution report, hold the Control key
and choose Stack.
This converts the report to a horizontal layout with the histograms stacked, as shown
in Exhibit 5.44.
6.
Select Rows > Data Filter, choose Charge Code as the variable of interest, and
click Add.
The reports shown in Exhibit 5.45 are among those produced by the script Distributions and
Data Filter.
Click on each of the Charge Code values in turn in the Data Filter’s Charge
Code list and scroll through the Distribution reports to see which bar graphs
reﬂect records with that Charge Code. You ﬁnd that almost all of the Charge
Code values only appear in one of the locations. RXA is one exception, appear-
ing both for ED and T2 (see Exhibit 5.45).
Next, you conduct a small study of the Description information, using tree
maps and other methods. You conclude that the Description information could
be useful to a team addressing late charges, given appropriate contextual knowl-
edge. But you believe that Charge Location might provide a better starting point
for deﬁning a team’s charter.
Given all you and the team have learned to this point, you decide that a
team should be assembled to address late charges, with a focus on the eight
Charge Location areas. For some of these areas, late charges seem to be asso-
ciated with speciﬁc Charge Codes that are identiﬁed as prevalent by the tree
map in Exhibit 5.42, and that would give the team a starting point in focusing
their efforts.
IDENTIFYING PROJECTS
You and the team are ready to make recommendations to management relative
to areas that should be addressed by green belt project teams. Recall that the
January data showed that the late charge problem involves 186,533 absolute
dollars. Here are the recommendations that you make to management, along
with your rationale:
◾
First and foremost, a team should be assembled and chartered with
developing a value-stream map of the billing process. This will provide
future teams with a basis for understanding and simplifying the process.
The non-value-added time, or wait times, will indicate where cycle
time improvement efforts should be focused. Meanwhile, teams can

154
V I S U A L S I X S I G M A
Exhibit 5.44 Partial View of Stacked Distributions for Charge Locations
be chartered to address the problems below. These teams and the
value-stream mapping team need to stay in close contact.
◾There is a pervasive pattern of charges being offset by credits. A team
should be organized to determine what drives this phenomenon and to
ﬁnd a way to eliminate this rework.
◾There are 85,839 absolute dollars tied up in six charge codes, and
140,115 absolute dollars tied up in eight charge locations. Since the

R E D U C I N G H O S P I T A L L A T E C H A R G E I N C I D E N T S
155
Exhibit 5.45 Data Filter and Distribution Display Showing Charge Code RXA
codes used in the locations seem proprietary to the locations, your
recommendation is that a green belt project team should be given the
task of reducing the level of late charges, and that the initial focus be
these eight areas. Note that one of these areas, T2, deals with many
charge codes, which may entail using a different approach in that area.
◾
Two complex accounts were found that contributed greatly to the late
charges in January. In fact, 984 of the 2,030 records of late charges
in January involved transactions for these two accounts. The absolute
dollars involved were $43,995. A green belt team should investigate
whether this was the result of a special cause or whether such occur-
rences are seen regularly. In either case, the team should suggest appro-
priate ways to keep the problem from recurring.

156
V I S U A L S I X S I G M A
◾There is also a tendency to not report the Charge Code. For example,
one of the locations, LB1, is always missing Charge Code. A team should
be formed to address this issue. Is the information redundant and hence
unnecessary? Is the information not useful? Is it just too difﬁcult to input
this information? The team should address this information ﬂow, and
make appropriate recommendations.
CONCLUSION
As mentioned at the start of this chapter, this case study is almost entirely
embedded in the Uncover Relationships stage of the Visual Six Sigma Data
Analysis Process (Exhibit 3.29). You assumed the role of a detective, search-
ing for clues. You used a number of the How-We-Do-It techniques described
under Uncovering Relationships in Exhibit 3.30. Your exploration has been
quite unstructured and oriented toward your personal learning and thinking
style, yet you have learned a great deal very quickly.
Using the dynamic visualization and dynamic linking capabilities available
in JMP, you and your team have discovered interesting relationships that have
clear business value. JMP has facilitated data screening, exploration, and anal-
ysis. In a very short time, you performed analyses that allowed you to deﬁne
ﬁve major problem focus areas. Also, your visual displays made sense to man-
agement, lending credibility and strength to your recommendations.

C H A P T E R 6
Transforming Pricing
Management in a
Chemical Supplier
157
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

158
V I S U A L S I X S I G M A
S
ix Sigma is often assumed to be the prerogative of black belts, quality
engineers, or continuous improvement professionals working in manufac-
turing environments where the culture and approach are amenable to the
rigor and discipline of statistically based data analysis. This case study illustrates
how Visual Six Sigma may be successfully applied in the world of sales and mar-
keting, a world conventionally seen as being driven more by art and intuition
than by data analysis and science. Speciﬁcally, we will see how Visual Six Sigma
is used to drive a new way of conceptualizing and managing the pricing process
of a multinational chemicals supplier.
Your employer, Polymat Ltd., is a manufacturer of polymeric materials that
are sold into a range of commodity and specialty applications. Recently, Poly-
mat has been facing growing competition from new manufacturing facilities,
primarily in China and India. Market prices have declined steadily over a period
of several years. Given your reputation as a skilled black belt, you are enlisted
to help arrest the steady price decline.
You assemble a small team and construct what turns out to be a useful set of
data. These data capture information from two sales campaigns that attempted
to renegotiate prices so as to meet at least a 5 percent price increase. The cam-
paigns cover two very different periods, one where Polymat’s product was in
undersupply and one where it was in oversupply. In addition to the actual per-
centage price increase that the sales representative was able to negotiate for
each sale, the data set also includes ratings of the experience levels of the sales
representative and buyer, as well as a classiﬁcation of the value of each sale as
seen from the buyer’s point of view.
The two experience levels, together with the perceived value of the sale and
a few other variables, constitute your Xs. Your Ys are percentage price increase
and a nominal variable that reﬂects whether the target 5 percent increase was
met. Failure to meet the 5 percent target deﬁnes a defect in the negotiating
process.
Together with your team you explore these data, ﬁrst assessing the capabil-
ity of the pricing process and then uncovering relationships between the Xs and
Ys. Given that one Y is continuous and the other is nominal, you use traditional
modeling techniques as well as recursive partitioning to explore relationships
with the Xs.
Guided by your data exploration, you identify several key areas for
improvement: training of sales representatives, negotiating sales based on the
knowledge of the product’s value to the buyer, providing appropriate guidelines
for sales managers, and accounting for the prevailing supply/demand balance
when setting pricing targets. Addressing these areas leads to improvements in
the pricing process, which your team continues to monitor over a period of time.
You and your team work through all of the steps of the Visual Six Sigma
Data Analysis Process. Exhibit 6.1 lists the JMP platforms and options that

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
159
Exhibit 6.1 Platforms and Options Illustrated in This Case Study
Menus
Platforms and Options
Rows
Clear Row States
Color or Mark by Column
Cols
Column Info
Column Properties
Analyze
Distribution
Histogram
Frequency Distribution
Fit Y by X
Contingency
Oneway
Means Diamonds
Compare Means
Fit Model
Standard Least Squares
Macros—Factorial to Degree
Effect Summary
Modeling
Partition
Decision Tree
Quality and Process
Control Chart Builder
Diagram (Ishikawa C&E)
Other
Local Data Filter
you use in your analysis. The data sets are available at http://support.sas.com/
visualsixsigma. This case study illustrates how Visual Six Sigma can lead to sub-
stantial improvements in sales and marketing.
SETTING THE SCENE
Polymat Ltd., headquartered in England, is a manufacturer of polymeric materi-
als that are sold into a range of commodity and specialty applications. A leader
in a steadily growing market, Polymat faces increased competition owing to
large-scale competitive investments in new manufacturing facilities, primarily
in China and India. As a consequence of the growing availability of product
from these far eastern factories, market prices have shown a steady decline over
a period of several years.
Bill Roberts is the recently appointed commercial director for Polymat. He
has an excellent track record of business turnaround and has been appointed
speciﬁcally to reverse Polymat’s steady decline in proﬁtability. During his ﬁrst

160
V I S U A L S I X S I G M A
three months, Bill carries out a major review of each part of the Polymat
business. He ﬁnds many things that concern him, but is particularly worried
about the possibility of further price decline, because he knows that further
signiﬁcant cost cutting is not possible. In reviewing the situation with his
colleagues, Bill realizes that despite several recent attempts to impose price
increases, both margin and proﬁtability have continued to be eroded.
Bill is aware that the market ultimately deﬁnes the price. However, having
visited a number of customers, he has a hunch that his sales representatives
are leaving money on the table in their price negotiations. Motivated by his
recent attendance at an exciting conference on process management, Bill is
determined to apply process thinking to this difﬁcult challenge. He knows that
new perspectives in sales and marketing are required to help Polymat halt or
reverse the price decline. So he decides to go out and hire the best Six Sigma
black belt he can ﬁnd.
Being a highly experienced black belt trained in Visual Six Sigma, you are
Bill’s ﬁrst choice and you quickly ﬁnd yourself working for Polymat. Bill gives
you very clear instructions: “Go out and ﬁx my pricing process—it’s broken!”
FRAMING THE PROBLEM: UNDERSTANDING THE CURRENT STATE
PRICING PROCESS
You have a few key ideas in mind that help you focus your initial efforts. You
realize that you need to form a team in order to have the relevant exper-
tise available. You also need data in order to identify the root causes that will
direct you to solutions. Your initial thinking is to design a data set using histor-
ical data.
In a previous project for another employer, you employed an approach
based on a Product Categorization Matrix. This tool is based on the idea that
the seller should view the product from the perspective of the buyer. You feel
strongly that such a tool could put sales representatives in a stronger negotiat-
ing position for certain products. Consequently, you plan to integrate product
categorization data into your team’s investigation.
Deﬁning the Process
Quickly, you pull together a small team consisting of two sales representatives,
a sales clerk, and a Polymat ﬁnancial analyst familiar with the IT system who
knows how to access invoice and sales data. The team members agree that they
should focus on improving the pricing management process—that is, the process
that consists of setting price targets, negotiating prices with customers, and
invoicing and tracking orders after a successful sale. To deﬁne the scope
and focus of the initial work, the team starts by drawing a high-level map of
this process (see Exhibit 6.2).

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
161
Market Planning
Develop Pricing Strategy
Pricing Management
Market/Customer
Product Strategy
Product Mix
Management
Set Price
Targets
Negotiate
Price
Invoice and
Track Orders
Value
Enhancement
Your team will
focus here
Exhibit 6.2 Process Map of Polymat’s Price Management Process
When you review this scope with Bill, he conﬁrms that this is the right place
to start. His management team is already undertaking a strategic review of the
market planning process and is looking to simplify Polymat’s product range.
Bill makes it clear that he expects you to help drive improvement in the oper-
ational implementation of the stipulated market plans and pricing strategies.
This is where he believes a Six Sigma approach will be most beneﬁcial.
Based on discussions with Bill and other members of the management team,
you also realize that recent attempts to renegotiate prices with customers have
by and large been unsuccessful. Working with the ﬁnancial analyst on your
team, you uncover data connected to four separate attempts to increase prices
over the last two years. Each of these was directed at achieving a 5 percent
price increase across Polymath’s customer base. None of the four attempts were
successful in meeting this target.
Constructing an Analysis Data Set
In thinking about your analysis data set, you decide that your primary Y will be
a measure of the price increase. For each sale, you deﬁne this as the difference
between the price charged for the product after the negotiated increase and
the price that would have been charged before the increase, divided by the
price before the increase. This measure eliminates any currency exchange rate
ﬂuctuations that might otherwise complicate the analysis.
You also realize that prevailing market conditions have a pronounced effect
on price. The market for Polymat products is highly volatile and can change

162
V I S U A L S I X S I G M A
from demand exceeding supply (a shortage) to supply exceeding demand (an
oversupply) in just six to twelve months, depending on a number of factors:
◾Cyclical characteristics of end user markets (e.g., packaging, electronics)
◾Fluctuating oil prices (a key raw material)
◾New polymer supply factories coming online (mainly in China and the
Far East)
Based simply on market forces, you expect that when demand exceeds sup-
ply, a higher price should be viable (and vice versa). However, you need to
verify that this expectation is supported by the data. So, to better understand the
situation, you decide to baseline the capability of the current pricing manage-
ment process using a detailed assessment of two of the recent attempts to impose
a unilateral price increase.
After intensive planning and some brainstorming with the team to iden-
tify useful data for this investigation, it becomes clear that information on the
experience of the sales representative and on the sophistication of the buyer
involved in each sale will be vital. With this in mind, you realize that it does
not make sense simply to use a large set of unstructured and possibly unin-
formative sales data. Instead, you decide to retroactively design and assemble
a data set for your baseline assessment. This will both ensure that you have
data on the relevant background variables and minimize the time you spend
researching and constructing the values of related variables.
You construct the data set for the baseline assessment as follows:
◾Products. Working with a member of the Polymat marketing department,
you select 20 products as the basis for the study. These products repre-
sent a range of both commodity and specialty product types as deﬁned
in the Polymat marketing plan. The products are sold in volume and
have a respectable (>30 percent) combined market share.
◾Customers. Polymat is the market leader in four territories: the United
Kingdom, France, Germany, and Italy. You want to ensure that the study
includes a range of customers from each of these territories. To that end,
for each of the 20 products, 3 customers representing a range of different
sizes are chosen from each of the 4 regions. Here, customer size is deﬁned
as the annual volume in sales made by Polymat to that customer. A
total of 240 customers are represented: three customers for each of 20
products for each of four regions.
◾Supply/Demand Balance. To include the effect of market conditions in
the analysis, you identify two price increase campaigns that were run
under different conditions. In the ﬁrst case, the market was tight; that
is, demand was close to exceeding supply, and there was a relative

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
163
shortage of polymer product. In the second case, 12 months later, the
market had shifted to a point where, owing to new factories coming
online, there was oversupply.
Based on this retroactive design, you construct a data table whose rows are
deﬁned by all possible combinations of the 20 products, the 12 customers (three
in each of the four regions), and the two supply/demand balance periods. This
leads to 20 × 12 × 2 = 480 records. For each of the combinations of product,
customer, and supply/demand balance period, you obtain the following infor-
mation for a sale:
◾
Sales Representative Experience. You believe that the experience of the sales
representative is a factor of interest, because at Polymat there is no ﬁxed
price for each product. Sales representatives have the responsibility of
negotiating the best price based on general guidelines and pricing tar-
gets set by their sales manager. Consequently, the price that a customer
pays for a product depends on the outcome of the negotiation with the
buyer. The sales manager helps you to classify the sales representatives
into three categories of experience: high, medium, and low. The classiﬁ-
cation is based on each representative’s number of years of general sales
experience and his or her industry-speciﬁc knowledge. For example, a
sales representative designated with high experience has more than ten
years of sales experience and in excess of ﬁve years selling at Polymat
or a similar business.
◾
Buyer Sophistication. Because negotiation is a two-way undertaking, you
want to explore the relationship between the experience of the sales
representative and the experience of the buyer. You expect that price
negotiations will differ based on whether an inexperienced sales repre-
sentative is selling to an experienced and skilled buyer, or vice versa.
You sit down with the sales manager to categorize the buyer sophis-
tication for each customer that bought one of the 20 products under
consideration. High buyer sophistication is allocated to customers
whose buyers are highly professional and highly trained, whereas low
buyer sophistication is assigned to customers whose buyers are less
highly trained or skilled.
◾
Product Category. This is a categorization of a product and customer into
one of four classes, based on how the buying organization views and
uses the product. The Product Categorization Matrix section that follows
describes this categorization and your use of it.
◾
Annual Volume Purchased. This is the total amount spent by each
customer for this product over the year in which the supply/demand
balance period falls.

164
V I S U A L S I X S I G M A
◾% Price Increase (Y). This is computed by taking the difference between
the price charged for the product after the negotiated increase and the
price that would have been charged before the increase, and dividing
this difference by the price before the increase.
Product Categorization Matrix
You are concerned about the simple specialty–commodity split that marketing
uses to describe Polymat’s products. Luckily, in a previous role, you were
responsible for a project in which purchasing operations were redesigned.
As part of that project, you developed a simple tool to encourage buyers to
think differently about what they did, where they spent their time, and where
they should focus to reduce the costs of the products they bought.
You decide to turn this thinking on its head and apply the same idea to
Polymat’s pricing process. “After all,” you reﬂect, “if you are selling a product
and want to get the best price, it’s certainly a good idea to think of the product
and its value in the same way that a customer would.”
You pull the team members together over lunch for a discussion of your
proposed approach. You explain that the Product Categorization Matrix will
help everyone to see the product from the buyer’s viewpoint, and so should
be more informative than the specialty–commodity split that just sees things
from the seller’s point of view. You go on to say that each sale (of a product to a
customer) can be placed into a two-by-two grid based on the requirements of
the buying organization (Exhibit 6.3).
Non Critical
(Many equivalent sources,
Low proportion of total spend)
Tactical Profit
(No differentiation,
High price sensitivity)
Strategic Security
(Important product,
Low proportion of total spend)
Low
Many
Few
High
Product Volume Purchased
Alternative Sources of Product
Strategic Critical
(Important product
High proportion of total spend)
Exhibit 6.3 The Product Categorization Matrix

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
165
The vertical axis, Alternative Sources of Product, represents the buyer’s supply
vulnerability relative to the product in question. The axis is scaled from Few to
Many, referring to the number of potential suppliers. It represents the buyer’s
risk, posing the question, “What happens if the current seller cannot provide
the product?” For example, if the buyer cannot obtain an equivalent product
elsewhere, or if the product is speciﬁc to the buyer’s process, then that sale’s
value on Alternative Sources of Product is Few.
The horizontal axis, Product Volume Purchased, represents the product’s rela-
tive cost to the buyer. This axis is scaled from Low to High. If a product represents
a high proportion of the buyer’s spend, the Product Volume Purchased rating
of the sale is High.
The four quadrants of the matrix are:
1. Strategic Security. This quadrant contains products for which the buyer has
few alternatives. These products may be unique or speciﬁc to the cus-
tomer’s process but represent a low proportion of the buyer’s spending.
These products should be able to command a high price and the buyer
should be relatively insensitive to price increases. The key task of the
buyer is to ensure the security of his business by guaranteeing a supply
of these strategic products almost at any cost.
2. Strategic Critical. This quadrant contains products for which the buyer’s
spending is very high. Small changes in price will have a high impact
on the buyer’s overall spending. Therefore, the buyer will typically be
more sensitive to price. The critical task of the buyer is to purchase these
strategic products at minimal cost.
3. Tactical Proﬁt. For products in this quadrant, the buyer has several options,
as there are competitive products with similar characteristics available
from alternative vendors. The product in this category represents a high
proportion of the buyer’s spending, so the buyer will make tactical pur-
chasing decisions based on maximizing his proﬁt. In marketing terms,
products in this category are typical commodity products. There is little
differentiation and high price sensitivity—hence, it will be very difﬁ-
cult to increase the price for these products. Any attempt to do so will
encourage the buyer to purchase from a competitor and the business will
be lost.
4. Non Critical. Products in this quadrant are commodity products that
represent small-volume purchases for the buyer. There are many equiv-
alent products available, and the products represent a small overall cost.
Decisions regarding the purchase of these products will be based on
criteria other than price, such as ease of doing business, lead time, and
similar factors.

166
V I S U A L S I X S I G M A
The buyer will typically expend effort in two areas of the matrix: the
Strategic Security products and the Strategic Critical products. The buyer will
be sensitive to price in two areas: Strategic Critical and Tactical Proﬁt. The
buyer’s ultimate strategy is to move all of his products to the Tactical Proﬁt
quadrant, where he can play one supplier against another, or at least to make
his suppliers believe that their products are in this quadrant!
As you emphasize, this product categorization is based on the use of the
product. A particular product may be Strategic Security for customer A because
alternative products do not give the same consistency of performance in cus-
tomer A’s manufacturing process, whereas for customer B, that same product
may be Non Critical, being one of several alternatives that can deliver the con-
sistency that customer B requires.
As the team quickly realizes, this approach to thinking about price sensi-
tivity is situational—it is quite different from simply setting a list price for each
product—and will give Polymat’s sales representatives greater insight into the
price increase a speciﬁc transaction may bear. With the team’s support, you
hold a series of meetings with the appropriate sales and marketing personnel
to apply their account knowledge in categorizing the transactions included in
the baseline data set. When this work is completed, you are happy to see that
all four types of sale are broadly represented in the baseline data.
COLLECTING BASELINE DATA
Now you and your team embark on the process of obtaining the historical data
and verifying its integrity. Once this is done, you will compute some baseline
measures.
Obtaining the Data
From the two periods during which 5 percent price increases were targeted, you
have identiﬁed 20 products and, for each product, 12 customers representing
various sizes and regional locations (three customers per region). Each of these
product and customer combinations had sales in each of two periods: oversup-
ply and shortage. This results in 480 different product-by-customer-by-period
combinations.
The invoice ﬁgures for these sales are directly downloaded from Polymat’s
data warehouse into a standard database. After entering the Product Categoriza-
tion Matrix categories determined earlier, along with the sales representative
experience and buyer sophistication rankings, you load the data from the two
pricing intervention periods into JMP for further investigation. The raw data are
given in the ﬁrst ten columns in the 480-row data table BaselinePricing.jmp
(Exhibit 6.4).

Exhibit 6.4 Partial View of Baseline Pricing Data Table
167

168
V I S U A L S I X S I G M A
Exhibit 6.5 Description of Columns in the Baseline Pricing.jmp Data Table
Column
Description
Product Code
Product identiﬁer
Region
Region of facility making the purchase
Customer ID
Customer identiﬁer
Supply Demand Balance
Prevailing market conditions when a price increase was made (Shortage
or Oversupply)
Sales Rep
Name of sales representative
Sales Rep Experience
Sales representative’s experience level
Buyer Sophistication
Customer’s level of buying sophistication
Product Category
Category of product when sold to a particular customer
Annual Volume Purchased Total amount spent by the customer for this product over the year in which
the supply/demand balance period falls
% Price Increase
Percentage price increase deﬁned as 100 x (Price After – Price
Before)/Price Before, where Price After = invoiced price after the price
increase was implemented, and Price Before = invoiced price before the
price increase was implemented
Defective
Categorization as defective when the % Price Increase is <= 5% and not
defective otherwise
You have heard some members of the management team refer to a failure
to achieve a 5 percent increase as a pricing defect. This makes sense to you, so you
add an 11th column to the data table that indicates whether the given sale has
this defect. For your current purposes, you will consider any sale that exhibits
a pricing defect to be defective.
You call your new column Defective, and you deﬁne it using a formula.
Once you have deﬁned the column using JMP’s Formula Editor, a plus sign
(+) appears next to the column name in the Columns panel to the left of the
data grid. Clicking on the plus sign shows you the formula. You foresee using
this column as an easy categorization of whether the price obtained for the
given sale met the mandated increase.
The columns in the data table are described in Exhibit 6.5. Note that only
the last two variables are Ys. The rest are Xs that you believe may be useful
in your understanding of the resulting price increases. You assign appropriate
data modeling types to your variables: Most are nominal, but Sales Rep
Experience and Buyer Sophistication are ordinal while Annual Volume
Purchased and % Price Increase are continuous.
Veriﬁcation of Data Integrity
You begin your analysis with a quick check to make sure that the data you
have imported into JMP are what you expect. You also want to check that the

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
169
incidence of the categories in your nominal variables is large enough to allow
useful analysis. For example, if only 2 percent of records were not defective,
then the data would be of questionable value in helping to determine which
Xs affect the response Defective. There would simply not be sufﬁciently many
records with a nondefective outcome.
To take a quick look at your data, you use the Distribution platform to ana-
lyze all of your columns.
To obtain the analysis partially shown in Exhibit 6.6 (BaselinePricing.jmp, script is
Distributions):
1.
Select Analyze > Distribution.
2.
Select all 11 variables in the Select Columns list and click Y, Columns.
3.
Click OK.
From the ﬁrst four reports, you verify that your 20 product codes, 4 regions,
240 customers, and 2 periods of supply/demand balance are represented in
equal proportions. You look at the remaining reports to learn that:
◾
Sales Rep: A reasonable number of sales representatives are repre-
sented.
◾
Sales Rep Experience: The experience levels of these sales representa-
tives span the three categories, with reasonable representation in each.
◾
Buyer Sophistication: Buyers of various sophistication levels are repre-
sented in almost equal proportions.
◾
Product Category: Each of the four product categories is well-
represented, with the smallest having a 17.5 percent representation.
◾
Annual Volume Purchased: The distribution of volume purchased for
these products is consistent with what you expected.
◾
% Price Increase: This shows a slightly right-skewed distribution, which
is to be expected, with a mean of 3.7 percent—you will analyze this Y
further once you ﬁnish verifying the data.
◾
Defective: This shows that about 72 percent of all records are defective
(according to the 5 percent cutoff) and that 28 percent are not. This rep-
resentation in the two groupings is adequate to allow further analysis.
Just in passing, you notice that JMP does something very nice in order-
ing the categories for nominal variables. The default is for JMP to list these

Exhibit 6.6 Partial View of Distribution Reports for All Variables
170

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
171
in alphabetical order in plots; for example, the Sales Rep names are listed in
reverse alphabetical order in the plot and in direct order in the frequency table
in Exhibit 6.6. But JMP uses some intelligence in ordering the categories of
Sales Rep Experience and Buyer Sophistication. Both have Low, Medium, and
High categories, which JMP places in their context-based, rather than alphabet-
ical order.
You conclude from this analysis that your data are consistent with your
intended design and that the distributions of the nondesigned variables make
sense and provide reasonable representation of the categories of interest. Now
you initiate your baseline analysis.
Baseline Analysis
The purpose of your baseline analysis is to understand the capability of the
current pricing management process under the two different market conditions
of Oversupply and Shortage. You have already seen that the overall defect rate
is about 0.72. An easy way to break this down by the two Supply Demand
Balance conditions is to construct a mosaic plot and contingency table
(Exhibit 6.7).
To obtain the report in Exhibit 6.7 (BaselinePricing.jmp, script is Defective Baseline):
1.
Select Analyze > Fit Y by X.
2.
Select Defective from the Select Columns list and click Y, Response.
You are thinking of Defective as the response (Y) and Supply Demand
Balance as an X that might explain some of the variation in Y.
3.
Select Supply Demand Balance from the Select Columns list and click X,
Factor.
The schematic in the lower left area of the Fit Y by X launch window indicates that
because both X and Y are nominal, the resulting report will be a Contingency analysis.
4.
Click OK.
5.
In the resulting report, close the disclosure icon for Tests.
You see immediately that the percentage of defective sales is much larger in
periods of Oversupply than in periods of Shortage, as represented by the areas
in the Mosaic Plot (these appear blue on a computer screen). Click in the report
and then place your cursor over each rectangular area in the mosaic plot to see
the percentage (called Frequency) of sales represented by the rectangle.

172
V I S U A L S I X S I G M A
Exhibit 6.7 Contingency Report for Defective
The Contingency Table below the plot gives the Count of records in each of
the classiﬁcations as well as the Row %. You see that in periods of Oversupply,
about 86 percent (speciﬁcally, 85.83 percent) of the sales in your data table are
defective, while in periods of Shortage, about 58 percent (speciﬁcally, 57.92
percent) are defective.
This ﬁnding makes sense. However, if you relate what you see to the
expectation of Polymat’s leadership team, you start to understand some of Bill
Roberts’s frustrations. Even in times of Shortage, when the account managers

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
173
are in a strong negotiating position, Polymat suffers a high pricing defect
rate—an estimated 58 percent of the sales negotiations are defective.
Recreate the Distribution report for % Price Increase by running the script
Distributions. Select the option Display Options > Horizontal Layout from
the red triangle menu next to % Price Increase to obtain the layout shown in
Exhibit 6.8. The Summary Statistics report shows a mean overall increase of
3.7 percent. The Quantiles report indicates that the price increases vary quite
a bit, from 0 percent to about 9.3 percent.
Exhibit 6.8 Distribution Report for % Price Increase
But what about the breakdown by Supply Demand Balance? Again, you
want to see the relationship between a Y, % Price Increase, and an X, Supply
Demand Balance (Exhibit 6.9).
Exhibit 6.9 Oneway Report for % Price Increase by Supply Demand Balance

174
V I S U A L S I X S I G M A
To obtain the report in Exhibit 6.9 (BaselinePricing.jmp, script is % Price Increase
Baseline):
1.
Select Analyze > Fit Y by X.
2.
Select % Price Increase from the Select Columns list and click Y,
Response.
3.
Select Supply Demand Balance from the Select Columns list and click X,
Factor.
The schematic in the lower left area of the Fit Y by X launch window indicates that
because Y is continuous and X is nominal, the resulting report will be a Oneway
analysis.
4.
Click OK.
5.
In the resulting report, from the red triangle menu, select Display Options >
Points Jittered.
This spreads the points out so that you can more easily see where they are dense.
6.
From the red triangle menu, select Display Options > Histograms.
Histograms are added to the plot for each group.
7.
From the red triangle menu, select Means and Std Dev.
The plot shows that % Price Increase is higher in periods of Shortage than
in periods of Oversupply. This is not unexpected. The range of % Price Increase
is fairly large in both periods. The Means and Standard Deviations report shows
that, in periods of Oversupply, the mean % Price Increase is about 2.7 percent
while in periods of Shortage it is 4.7 percent. Both means are below the desired
5 percent increase, with a much lower mean during periods of Oversupply.
Yet this is an exciting initial ﬁnding. There is a high potential for improve-
ment if the business can increase prices when market conditions are advanta-
geous. You immediately use your baseline analysis to entice Polymat’s senior
staff to increase their commitment to the project. Recognizing the power of
appropriate language to change behavior, you also start to reinforce the con-
cept of a pricing defect within the leadership team. You are pleasantly surprised
by how quickly Bill starts to use this language to drive a new way of thinking.
UNCOVERING RELATIONSHIPS
You are anxious to embark on the task of identifying Hot Xs. But ﬁrst, you take
a little time to think through how you will structure the analysis of your data.
You think back to the Visual Six Sigma Data Analysis Process (Exhibit 3.29) and

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
175
observe that you are now at the Uncover Relationships step, which, if necessary,
is followed by the Model Relationships step. Referring to the Visual Six Sigma
Roadmap (Exhibit 3.30), you decide to do the following:
◾
Use the Distribution platform and dynamic visualization to better
understand potential relationships among the variables.
◾
Plot the variables two at a time to determine if any of the Xs might have
an effect on the Ys.
◾
Use multivariate visualization techniques to explore higher-dimensional
relationships among the variables.
If you ﬁnd evidence of relationships using these exploratory techniques, and
if you can corroborate this evidence with contextual knowledge, you could stop
your analysis here. You would then proceed directly to proposing and verify-
ing improvement actions as part of the Revise Knowledge step. However, since
your data set is not very large, you decide that before moving to Revise Knowl-
edge you will follow up with the Model Relationships step to try to conﬁrm the
hypotheses that you obtain from data exploration.
Dynamic Visualization of Variables Using Distribution
To explore which Xs might be inﬂuencing the Ys, you rerun your Distributions
script. Now you use dynamic linking in the Distribution platform to identify the
Xs that might inﬂuence the success or the lack of success of the two historical
price increases under investigation.
In the Distribution plot for Defective, click in the No bar. (See Exhibit 6.10.)
This has the effect of selecting all those rows in the data table where Defective
has the value No. In turn, this highlights the areas that represent these points in
all of the other Distribution plots. Note that the graph for Customer ID is only
partially shown in Exhibit 6.10, as it involves 240 bars.
As you toggle between the Yes and No bars for Defective you see that Supply
Demand Balance, Buyer Sophistication, and Product Category have different
distributions based on whether Defective is Yes or No. It also appears that Defec-
tive may be related to Product Code and Customer ID. This raises the question
of how, in terms of root causes, these last two variables might affect pricing
defects. You believe that the causal link relates to how the customer views the
product. The root cause may be related to the Product Category as assigned
using the Product Categorization Matrix.
By interacting with the Defective bar chart in this way, you also learn that
price increase has little or no association with:
◾
Region. Whether a price increase is defective does not appear to depend
on region.

Exhibit 6.10 Highlighted Portions of Distributions Linked to Defective = No
176

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
177
◾
Sales Rep. There is some variation in how sales representatives perform,
but there is no indication that some are strikingly better than others at
achieving the target price increases.
◾
Sales Rep Experience. Interestingly, the highly experienced account
managers appear to be no more effective in increasing the price than
those who are less experienced. (This is much to everyone’s surprise.)
◾
Annual Volume Purchased. Sales representatives appear to be no better
at raising prices with small customers than with large customers.
You also want to see the impact of Product Category on pricing defects. In
the Product Category plot, you want to highlight both the Strategic Security
and Strategic Critical bars. To do this, ﬁrst click in the Strategic Security bar.
Then hold the Control key while you click in the Strategic Critical bar. The
impact on Defective is shown in Exhibit 6.11.
Note that almost all of the sales that met the price increase target, namely,
where Defective has the value No, come from these two categories. Alterna-
tively, clicking on the No bar in the Defective graph shows that almost all
of these sales are either Strategic Security or Strategic Critical. This supports
your belief that Product Category captures the effect of Product Code and
Customer ID.
However, the most interesting X appears to be Buyer Sophistication.
Select High values of Buyer Sophistication by clicking on the appropriate
Distribution plot bar. After examining the other plots, click on the Medium
and Low values. With highly sophisticated buyers, the proportion of pricing
defects is much higher than for buyers who have Medium or Low sophistica-
tion. You conclude that these buyers use highly effective price negotiations to
keep prices low (Exhibit 6.12).
To date, your exploratory analysis leaves you suspecting that the main Xs
of interest are Supply Demand Balance, Buyer Sophistication, and Product
Category. Although you have simply run Distribution analyses for your vari-
ables, you have used dynamic linking to study your variables two at a time by
viewing the highlighted or conditional distributions. In the next section, you
use Fit Y by X to continue viewing your data two variables at a time.
Dynamic Visualization of Variables Two at a Time
You have already used the Fit Y by X platform, found under Analyze, in your
baseline analysis. As you know, Fit Y by X provides reports that help iden-
tify relationships between pairs of variables. You want to get a better look at
how Supply Demand Balance, Buyer Sophistication, and Product Category
are related to Defective.

Exhibit 6.11 Impact of Strategic Product Categories on Defective
178

Exhibit 6.12 Impact of High Buyer Sophistication on Defective
179

180
V I S U A L S I X S I G M A
To see how these Xs are related to Defective (BaselinePricing.jmp, script is
Contingency for Defective):
1.
Select Analyze > Fit Y by X.
2.
Select Defective from the Select Columns list and click Y, Response.
3.
Select Supply Demand Balance, Buyer Sophistication, and Product
Category from the Select Columns list and click X, Factor.
Since two of the Xs are nominal and one is ordinal while Y is nominal, the small
schematic in the bottom left of the launch window indicates that JMP will provide a
Contingency analysis for each pair of variables.
4.
Click OK.
In the report window the contingency tables that are shown by default
include Total % and Col %. But you are only interested in Count and Row %.
You would like to remove Total % and Col % from all three analyses. You also
would prefer not to have to go through the keystrokes to do this individually
for all three reports.
In most report windows, JMP allows you to run through the keystrokes
once and have the commands broadcast to all similar reports. Simply hold the
Control key while selecting the desired menu options. This applies your selec-
tions to all other similar objects in the report.
To remove include Total % and Col % from all three reports:
1.
In the report window, hold the Control key to broadcast your selections.
2.
Click the red triangle menu for any one of the contingency tables.
3.
You may release the Control key now.
4.
Click Total % to deselect it.
5.
Again, hold the Control key and click the red triangle menu for any one of the
contingency tables.
6.
You may release the Control key now.
7.
Click Col % to deselect it.
Your report now appears as shown in Exhibit 6.13.

Exhibit 6.13 Three Contingency Reports for Defective
181

182
V I S U A L S I X S I G M A
The plots and contingency tables show:
◾Supply Demand Balance. Not surprisingly, there are fewer defective
sales in periods of Shortage than in periods of Oversupply: 58 percent
versus 86 percent, respectively. (Note that this is the same plot as in
Exhibit 6.7.)
◾Buyer Sophistication. There are more defective sales when dealing
with highly sophisticated buyers (90 percent) than when dealing with
buyers of medium or low sophistication (about 61 percent for each
group). In fact, there appears to be little difference between low- and
medium-sophistication buyers relative to defective sales, although
other variables might differentiate these categories.
◾Product Category. This has a striking impact on defective sales. The
defective rates for Strategic Critical and Strategic Security are 56 per-
cent and 47 percent, respectively, compared to Non Critical and Tactical
Proﬁt, with defective rates of 92 percent and 94 percent, respectively.
This analysis is conducted with the nominal response, Defective. You real-
ize that the variable Defective is simply a coarsened version of the continuous
variable % Price Increase. Will these results carry through for the continuous
response, % Price Increase?
To see how Supply Demand Balance, Buyer Sophistication, and Product
Category are related to % Price Increase (BaselinePricing.jmp, script is Oneway
for % Price Increase):
1.
Select Analyze > Fit Y by X.
2.
Select % Price Increase from the Select Columns list and click Y,
Response.
3.
Select Supply Demand Balance, Buyer Sophistication, and Product
Category from the Select Columns list and click X, Factor.
Since two of the Xs are nominal and one is ordinal while Y is continuous, the small
schematic in the bottom left of the launch window indicates that JMP will provide a
Oneway analysis for each pair of variables.
4.
Click OK.
The points in the plots seem to overlap each other. You would like the plots to show
the points individually without overlap.
5.
In the report, hold the Control key while you click on a red triangle for one of the
Oneway reports.
6.
Select Display Options > Points Jittered.
This has the effect of randomly jittering the points.

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
183
Exhibit 6.14 shows you the distributions across the levels of each variable.
You see what appear to be differences across the levels of each X.
But you would also like a statistical guide to determine which levels differ.
You know that the Compare Means option, obtained from the red triangle
menu, constructs Comparison Circles. These provide a visual representation of a
statistical test for signiﬁcant differences among the levels of the Xs.
When you choose Compare Means you will notice four options. The two
most often used are Each Pair, Student’s t and All Pairs, Tukey HSD. The
difference between these two procedures is that the Each Pair option controls
the risk of incorrectly concluding that two speciﬁc groups differ, whereas the
All Pairs option controls this risk for all possible comparisons of two groups.
The default level for both risks is 5 percent. The All Pairs, Tukey HSD option
is the more conservative, and for this reason, you choose to use this option.
(It generally makes sense to use the All Pairs, Tukey HSD option unless you
have good reason not to.)
To obtain Comparison Circles (BaselinePricing.jmp, script is Oneway with
Comparison Circles):
1.
In the report window, hold the Control key to broadcast your selections while clicking
the red triangle menu for any one of the Oneway reports. You may release the Control
key now.
2.
Select Compare Means > All Pairs, Tukey HSD.
This adds comparison circles to all three plots.
3.
Hold the Control key to broadcast your selections while clicking the red triangle menu
for any one of the Oneway reports.
4.
Select Display Options > Means Diamonds.
For each group in each plot, this adds a diamond-like ﬁgure with a line in the middle
at the mean.
5.
Hold the Control key to broadcast your selections while clicking the red triangle menu
for any one of the Oneway reports.
6.
Select Means and Std Dev.
This adds a report showing means, standard deviations, and other statistics.
7.
Hold the Control key to broadcast your selections while clicking on any of the
disclosure icons for a Means Comparisons report.
This closes all Means Comparisons reports.

Exhibit 6.14 Three Oneway Reports for % Price Increase
184

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
185
The resulting report, except for the coloring of the comparison circles, is
shown in Exhibit 6.15. The report shows plots with means diamonds overlaid
on the jittered points. The central horizontal line in each of these diamonds is
plotted at the level of the sample mean for % Price Increase for that category.
The top and bottom of each diamond deﬁnes a 95 percent conﬁdence interval
for the true category mean. Looking at these, you realize that there are probably
a number of statistically signiﬁcant differences. For each variable, a table of
means and standard deviations for each grouping is given below the plot.
The plot in Exhibit 6.15 shows an area to the right of the points containing
comparison circles. These circles are a graphical way to test for differences in
the means of the levels of each predictor.
For example, take Product Category, which has four levels. There is one
circle corresponding to each level. The top two circles in the plot for Product
Category in Exhibit 6.15 correspond to the levels Strategic Security and Strate-
gic Critical.
Click on the smaller of the two top circles. The Strategic Security label on
the horizontal axis changes to red (given the default JMP settings) and is given
in boldface type. Meanwhile, the circle for Strategic Critical changes to red but
is not bolded. (See Exhibit 6.15.)
Since both circles are red, this means that the mean % Price Increase does
not differ signiﬁcantly for these two levels. However, the circles for Non Critical
and Tactical Proﬁt are gray. This means that the mean % Price Increase for each
of these two categories differs signiﬁcantly from the mean % Price Increase for
Strategic Security, the bolded category.
Now click on the larger of the two top circles. This selects the Strategic Crit-
ical circle and its label appears in boldface text, while the label for Strategic
Security is not bolded. This again indicates that these two groups do not dif-
fer statistically. The other two labels, Non Critical and Tactical Proﬁt, are in
non-boldface, italicized text, indicating that these two do differ statistically from
Strategic Critical.
For Supply Demand Balance, Oversupply is selected in Exhibit 6.15. The
two circles corresponding to Oversupply and Undersupply do not overlap.
This indicates that the two groups differ statistically in their effect on % Price
Increase.
For Buyer Sophistication, High-sophistication buyers are selected in
Exhibit 6.15. High-sophistication buyers differ statistically in their effect on
% Price Increase from Medium- and Low-sophistication buyers. By clicking
on the circle for Medium-sophistication buyers, you see that the Medium- and
Low-sophistication groups do not differ statistically.
Continuing your analysis, you click on circles corresponding to other levels
of variables in Exhibit 6.15 one by one to see which differ relative to % Price
Increase. Recalling that two categories differ with statistical signiﬁcance only if,

Exhibit 6.15 Reports Showing Means Diamonds and Comparison Circles
186

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
187
when you click on one of the circles, the other changes to gray, you conclude
that:
◾
The mean % Price Increase differs signiﬁcantly based on the Supply
Demand Balance, with higher increases in periods of Shortage.
◾
The mean % Price Increase for High Buyer Sophistication is signiﬁcantly
lower than for Medium- or Low-sophistication levels, whereas these last
two do not differ signiﬁcantly.
◾
The mean % Price Increase for the Strategic Critical and Strategic
Security Product Category levels are signiﬁcantly higher than for the
Non Critical and Tactical Proﬁt levels, although the means for Strategic
Critical and Strategic Security do not differ signiﬁcantly.
◾
The mean % Price Increase for the Non Critical products is signiﬁcantly
higher than for the Tactical Proﬁt products.
The Means Comparisons reports, whose disclosure icons are closed in
Exhibit 6.15, are analytic reports containing the results that give rise to the
comparison circles. For exploratory purposes, it sufﬁces to examine the com-
parison circles.
You realize that you have identiﬁed which X category levels differ with
statistical signiﬁcance. Practical importance is quite another matter. In the report
(Exhibit 6.15), you study the actual means and their conﬁdence intervals. For
example, relative to Buyer Sophistication, you note that High-sophistication
buyers had mean % Price Increase values of about 2.7 percent, with a
conﬁdence interval for the true mean ranging from about 2.4 to 2.9 percent
(Lower 95% and Upper 95% refer to the conﬁdence interval limits). For less
sophisticated buyers, these increases are much higher—about 4.3 percent for
Medium- and 4.4 percent for Low-sophistication levels. In a similar fashion,
you study the means and conﬁdence intervals for the other Xs.
This exploratory Fit Y by X analysis provides you with evidence that there
are differences in % Price Increase based on a number of levels of these three
Xs. Given this evidence, you are comfortable thinking of Supply Demand Bal-
ance, Buyer Sophistication, and Product Category as Hot Xs for the continuous
Y, % Price Increase. Of course, you need to verify that these are indeed Hot Xs.
You realize that there may well be interactions among the potential Xs and that
these will not be evident without a multivariate analysis. To that end, you now
proceed to explore multivariate relationships among these variables.
Dynamic Visualization of Several Variables at a Time
The Initial Partition Report
Your Fit Y by X analysis looks at one X at a time in relation to the Ys. This
means that your conclusions to date may overlook joint relationships among

188
V I S U A L S I X S I G M A
the Xs and a Y. You have a nominal Y, Defective, and a continuous Y, % Price
Increase. You could explore Defective using logistic regression or a partition
analysis. But because of its ease of interpretation and the fact that you have
several nominal Xs, some of which have many levels, you decide to use the
Partition platform.
Your use of Partition will be for exploration rather than modeling. There
will be no statistical hypothesis tests, so validation of any conclusions you reach
will need to rely on contextual knowledge of the process and on data that you
collect in the future. (Note that later on, to study % Price Increase, you will
use a multiple linear regression model, using Fit Model. This will provide some
conﬁrmatory analysis from modeling, and also reveals a new insight.)
To obtain a partition analysis of Defective (BaselinePricing.jmp, script is
Partition—No Splits):
1.
Select Analyze > Modeling > Partition.
2.
Enter Defective as Y, Response.
3.
Enter Product Code through Annual Volume Purchased as X, Factor.
Note that these are all of your Xs.
4.
Click OK.
5.
Select Display Options > Show Split Prob from the red triangle menu at the
top of the report.
This displays the levels of Defective as well as the Prob, or proportion, of sales
falling in each group (shown in Exhibit 6.16).
6.
Select Display Options > Show Split Count from the red triangle menu at the
top of the report.
This displays the number of observations that fall into each level of Defective
(shown in Exhibit 6.16).
7.
Click the Color Points button, found beneath the plot.
This assigns the color blue to rows associated with a pricing defect (Yes) and the
color red to rows where there is no pricing defect (No).
Changing Colors
“But wait a minute,” you think. “This coloring sends the wrong message: Points
corresponding to nondefective sales are colored red (a color associated with
danger), while the points corresponding to defective sales are colored blue.”
You decide it would be better, especially for the purpose of presenting your
results, if the points corresponding to nondefective sales were always colored
green and those corresponding to defective sales were always colored red.

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
189
Exhibit 6.16 Initial Partition Report with Split Probabilities and Counts
To make this happen, you deﬁne a Column Property, namely a property
associated with a column or variable. The property you will deﬁne is called a
Value Colors column property.
To deﬁne the Value Colors column property (BaselinePricing.jmp, script is Color by
Defective):
1.
Close your Partition report.
2.
Select Rows > Color or Mark by Column.
3.
Select Defective.
4.
Click Reverse Scale.
This assigns the color red to “Yes”, as seen in the Row States list on the right.
5.
In the Row States list, right-click on No.
6.
Select Colors and choose a dark green shade. See Exhibit 6.17.

190
V I S U A L S I X S I G M A
7.
Click Save to Column Property.
Note that the Save to Table Property option will create a script for you.
8.
Click OK.
9.
Run the script Partition—No Splits.
Exhibit 6.17 Changing the Color Assigned to No from Red to Green
In the data table, the rows have the red or green row state indicators to the
left of the row numbers. Also, the cells in the column Defective are colored red
and green. The plot in the partition report shows the new colors.
Splitting
Now you are ready to start splitting your data into groups or nodes that differen-
tiate between defective and nondefective sales. At each split step, the partition
algorithm ﬁnds the variable that best explains the difference between the two
levels of Defective. The two groupings of values that best explain this difference
are added as nodes to the diagram, so that repeated splits of the data produce a
tree-like structure.
You realize that you can split as many times as you like, with the only built-
in constraint being the minimum size split. This is the size of the smallest split
grouping that is allowed. To see where this is set by default, click on the red
triangle at the top of the report and select Minimum Size Split. The dialog
window that appears indicates that the minimum size is set at ﬁve.
With 480 records, you think this might be small. It could allow for
overﬁtting the data, namely, modeling noise rather than structure. After some
thought, you decide to specify a minimum size of 25 for your splits. This should
help ensure that your analysis reveals true structure, rather than just the
vagaries of this speciﬁc data set. So you enter 25 as the Minimum Size Split in
the dialog window and click OK.

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
191
Note that you could also use a Validation Portion, which is an option on
the launch window, to avoid overﬁtting. This would be the better approach
if you were interested in a predictive model. However, in this case study, you
are interested in data exploration, so controlling splitting with a minimum size
split is a reasonable approach.
Now you are ready to begin splitting. Click once on the Split button to
obtain the report in Exhibit 6.18. The ﬁrst split is on the variable Customer ID.
You see that for a large group of customers all sales are defective, while for the
other group of customers only about 33 percent of sales are defective.
However, the variable Customer ID, in its current state, is not useful for
understanding the root causes of defective sales. Perhaps if you had combined
customer IDs into broad categories, the binned version of Customer ID might be
useful. For this reason, you regret having included Customer ID as a predictor.
But, you do not need to start your analysis over. Instead, click on the top red
triangle and select Lock Columns. The tooltip for the option points out that
this will allow you to lock the Customer ID column out of your analysis.
Selecting Lock Columns inserts a list of the Xs to the right of the plot at the
top of the report. In this list, check Customer ID. Then click the Prune button
to remove the split on Customer ID from the report (Exhibit 6.19).
Now you are ready to split once more. Clicking Split once provides a split
on Product Category (Exhibit 6.20). You see that the two categories Tactical
Proﬁt and Non Critical result in a 93 percent defective rate (the node on the
left). The Strategic Critical and Strategic Security categories have a 50 percent
defective rate (the node on the right). These proportions are reﬂected in the
plot above the tree. You hope that further splits will help explain more of the
variation currently left unexplained, especially within the node on the right.
A second split brings Buyer Sophistication into play (Exhibit 6.21). For the
Strategic Critical and Strategic Security product categories, High-sophistication
buyers are associated with an 82 percent rate of defective sales, while Medium-
and Low-sophistication buyers are associated with a 30 percent rate. This is
fantastic information, reinforcing the notion that for high-proﬁle product cate-
gories, Polymat’s sales representatives need to be better equipped to deal with
highly sophisticated buyers.
At this point, you decide to split until the stopping rule, a minimum node
size of 25, ends the splitting process. Repeatedly click on Split as you monitor
the Number of Splits in the information panel to the right of the Split and
Prune buttons. As a result of the speciﬁed minimum size split, splitting stops
after nine splits.
The tree is now so big that it no longer ﬁts on your screen. You have to
navigate to various parts of it to see what is going on.
Click the top red triangle menu and select Small Tree View. A small tree
(Exhibit 6.22) appears to the right of the plot and Lock Columns list. This small

192
V I S U A L S I X S I G M A
Exhibit 6.18 Partition with One Split on Customer ID
tree allows you to see the columns where splits have occurred, but it does not
give the node detail. However, with the exception of Region (bottom, middle),
the splits are on the three variables that you have been thinking of as your
Hot Xs.
To better understand the split on Region, navigate to that split in your
large tree (see Exhibit 6.23). The split on Region is a split of the 72 records

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
193
Exhibit 6.19 Locking Out Customer ID
Exhibit 6.20 Partition with First Split on Product Category
that fall into the Supply Demand Balance(Shortage) node at this point in
the tree (note that there are three splits on Supply Demand Balance). The
Supply Demand Balance(Shortage) node contains relatively few records
(2.78 percent) reﬂecting defective sales. The further split into the two groupings
of regions attempts to explain the Yes values, using the fact that there are no
defective sales in France or Italy at that point in the tree. But the proportions
of Yes values in the two nodes are not very different (0.06 and 0.00).
You do not see this as useful information. You suspect this was one of the
last splits. Click the Prune button once. Looking at the Small Tree View, you

194
V I S U A L S I X S I G M A
Exhibit 6.21 Partition with Two Splits
see that this split has been removed, indicating that it was indeed the ninth split.
You are content to proceed with your analysis based on the tree with eight splits.
The script for this analysis is Partition—Eight Splits.
Partition Conclusions
Studying your large tree, you see evidence of several local interactions. For
example, Exhibit 6.24 shows that for Tactical Proﬁt and Non Critical products,
Buyer Sophistication explains variation in times of Shortage but not necessarily
in times of Oversupply, with the Low-sophistication buyers resulting in sub-
stantially fewer defects (66.67 percent) than High- and Medium-sophistication
buyers (96.25). Again the message surfaces that sales representatives need to
know how to negotiate with sophisticated buyers.

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
195
All Rows
Tactical Profit, Non Critical
Supply Demand Balance
Shortage
Medium, High
Product Category
Oversupply
Product Category
Shortage
Region
Oversupply
Shortage
UK, Germany
France, Italy
Strategic Critical
Strategic Security
Tactical Profit
Non Critical
Low
Oversupply
Buyer Sophistication
Low, Medium
Supply Demand Balance
High
Strategic Critical, Strategic Security
Buyer Sophistication
Supply Demand Balance
Product Category
Exhibit 6.22 Small Tree View after Nine Splits
Exhibit 6.23 Nodes Relating to Split on Region

196
V I S U A L S I X S I G M A
Exhibit 6.24 Example of a Local Interaction
You can obtain a summary of all the terminal nodes by selecting Leaf
Report from the red triangle menu at the top of the report. The resulting
Leaf Report describes all nine terminal nodes and gives their response
probabilities (proportions) and counts. See Exhibit 6.25.
You would like to see the listing of the node descriptions in decreasing order
of proportion defective, in other words, with the proportions under the Yes
heading in the Response Prob table listed in descending order.
To sort the report by Yes (Exhibit 6.26) (BaselinePricing.jmp, script is Sorted Leaf
Report):
1.
Right-click in the Response Prob part of the Leaf Report.
2.
Select Sort by Column from the list of options that appears.
3.
In the resulting dialog window, select Yes.
4.
Click OK.

Exhibit 6.25 Leaf Report
197

Exhibit 6.26 Leaf Report Sorted by Proportion Defective Sales
198

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
199
Study both the Leaf Report and the large tree carefully to arrive at these
conclusions:
◾
Product Category is the key determining factor for pricing defects.
Pricing defects are generally less likely with Strategic Critical or Strategic
Security products. However, with High-sophistication buyers, a high
defective rate can result even for these products, both in Oversupply
(93.48 percent) and Shortage (69.57 percent) situations.
◾
Buyer Sophistication interacts in essential ways with Product Category
and Supply Demand Balance. The general message is that for Strategic
Critical and Strategic Security sales, pricing defects are much more likely
with buyers of High sophistication (81.52 percent) than for those with
Low and Medium sophistication (29.86 percent). In periods of Short-
age, sales of these products to Low- and Medium-sophistication buyers
result in very few defects (2.78 percent), but sales to High-sophistication
buyers have a high defective rate (69.57 percent).
◾
Supply Demand Balance also interacts in a complex way with Product
Category and Buyer Sophistication. The general message is that for
Strategic Critical and Strategic Security sales involving Low- and
Medium-sophistication buyers, pricing defects are less likely when
there is a Shortage rather than an Oversupply.
When you review this analysis with the team, your team members concur
that these conclusions make sense to them. Their lively discussion supports
the suggestion that sales representatives are not equipped with tools to deal
effectively with sophisticated buyers. Team members believe that sophisticated
buyers exploit their power in the negotiation much more effectively than do
the sellers. In fact, regardless of buyer sophistication, sales representatives may
not know how to exploit their negotiating strength to its fullest potential in
times of oversupply.
Also, the experiences of the team members are consistent with the parti-
tion analysis’ conclusion that the only occasions when sales representatives are
almost guaranteed to achieve at least a 5 percent price increase is when they are
selling products for which buyers have few other options for purchase (Strate-
gic Critical and Strategic Security) and dealing with less sophisticated buyers
(Low and Medium) in times of product shortage. However, the team members
do express some surprise that Sales Rep Experience did not surface as a factor
of interest.
At this point, you take stock of where you are relative to the Visual Six
Sigma Data Analysis Process (Exhibit 6.27). You have successfully completed
the Uncover Relationships step, having performed some ﬁne detective work in
uncovering actionable relationships among the Xs and Ys. Although all of your
work to this point has been exploratory, given the amount of corroboration of

200
V I S U A L S I X S I G M A
Frame Problem
Collect Data
Uncover
Relationships
Statistics as
Detective (EDA)
Utilize
Knowledge
Revise
Knowledge
Model
Relationships
Statistics as
Lawyer (CDA)
Exhibit 6.27 Visual Six Sigma Data Analysis Process
your exploratory results by team members who know the sales area intimately,
you feel that you could skip the Model Relationships step and move directly to the
Revise Knowledge step. This would have the advantage of keeping your analysis
lean. But you reﬂect that your data set is not large. So a quick modeling step
using traditional conﬁrmatory analysis might be good insurance. You proceed
to undertake this analysis.
MODELING RELATIONSHIPS
For a traditional conﬁrmatory analysis, you need to use an analysis method that
permits hypothesis testing. You could use logistic regression, with Defective
as your nominal response. However, the partition analysis provided a good
examination of this response and, besides, the continuous variable % Price
Increase should be more informative. For this reason, you decide to model
% Price Increase as a function of the Xs using Fit Model.
But which Xs should you include? And what about interactions, which the
partition analysis clearly indicated were of importance? Fit Model ﬁts a multi-
ple linear regression. You realize that for a regression model, nominal variables
with too many values can cause issues relative to estimating model coefﬁcients.
So you need to be selective relative to which nominal variables to include. In
particular, there is no reason to include Customer ID or Sales Rep, as these vari-
ables would not easily help you address root causes even if they were signiﬁcant.
Now, Region is an interesting variable. There could well be interactions
between Region and the other Xs, but you are not sure these would be helpful
in addressing root causes. The sales representatives need to be able to sell

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
201
in all regions. You decide to include Region in the model to verify whether
there is a Region effect (your exploratory work has suggested that there
is not), but not to include any interactions with Region. You will include all
other interactions in your model.
Build your model as follows (BaselinePricing.jmp, script is Model—% Price
Increase):
1.
Select Analyze > Fit Model.
2.
Enter % Price Increase as Y.
3.
From the Select Columns list, select the ﬁve variables highlighted in Exhibit 6.28.
4.
Check that the Degree box beneath the Macro button contains the number 2.
5.
Click on the arrow next to Macro and select Factorial to degree from the
dropdown menu. See Exhibit 6.28.
This enters model effects for a model that contains main effects and all two-way
interactions.
6.
Select Region from the Select Columns list and click Add.
This adds Region to the Construct Model Effects list.
Exhibit 6.28 Fit Model Dialog

202
V I S U A L S I X S I G M A
Click Run to ﬁt the model. The report in Exhibit 6.29 appears. (The script
is Fit Model—% Price Increase.) The Actual by Predicted plot shows that
the data appear randomly spread about the solid line that describes the model.
There are no apparent patterns or outliers. (We will not delve into the details of
residual analysis or, more generally, multiple regression at this time; there are
many good texts that cover this topic.) You conclude that the model provides a
reasonable ﬁt to the data.
Exhibit 6.29 Fit Least Squares Report

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
203
The Analysis of Variance table shows a Prob > F value that is less than
0.0001. This indicates that the model successfully explains variability in the
response.
The Effect Summary report at the top lists the model effects and gives
p-values (PValue column) for each. The effects are listed in order of increas-
ing p-values. This places the most signiﬁcant effects closest to the top. You will
consider any effect with a p-value less than or equal to 0.05 to be signiﬁcant.
The signiﬁcant effects are:
◾
Product Category
◾
Buyer Sophistication
◾
Buyer Sophistication*Product Category
◾
Supply Demand Balance
◾
Sales Rep Experience*Product Category
What is very interesting is that Sales Rep Experience has appeared in the
list. On its own, it is not signiﬁcant, but it is inﬂuential through its interac-
tion with Product Category. This is an insight that did not appear in your
exploratory analyses. In fact, your exploratory results led you to believe that
Sales Rep Experience did not have an impact on defective sales negotiations.
But you need to keep in mind that your multivariate exploratory technique,
partition analysis, used a nominal version of the response. Nominal variables
tend to carry less information than do their continuous counterparts. Also, par-
tition and multiple linear regression are very different techniques. These two
comments may offer an explanation of why this interaction was not seen ear-
lier, which in turn reinforces the need to use complementary approaches to
analysis rather than simply relying on one method in isolation.
To get a visual picture of the Sales Rep Experience*Product Category and the Buyer
Sophistication*Product Category interactions (BaselinePricing.jmp, script is Fit
Model—% Price Increase):
1.
Locate the Effect Details panel in the report and click its disclosure icon to open the
panel.
2.
Find the subpanel corresponding to Sales Rep Experience*Product Category.
3.
Click the red triangle for that panel and select LSMeans Plot.
4.
Find the subpanel corresponding to Buyer Sophistication*Product Category.
5.
Click the red triangle for that panel and select LSMeans Plot.

204
V I S U A L S I X S I G M A
Exhibit 6.30 LSMeans Plot for Two Signiﬁcant Interactions
These two plots show the means predicted by the model (Exhibit 6.30).
From the ﬁrst plot, you conclude that there is some evidence that
High-experience sales representatives tend to get slightly higher price increases
than do Medium- and Low-experience sales representatives for Strategic Secu-
rity products. (Open the Least Squares Means Table to see that they average
about 0.8 percent more.) You wonder, “Do they know something that could
be shared?”
In the second plot, you see the effect of High-sophistication buyers—they
negotiate lower price increases across all categories, but especially for Strate-
gic Security products (about 3 percent lower than do Low- or Medium-
sophistication buyers for those products). For Tactical Proﬁt products,

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
205
Medium-sophistication buyers get lower price increases than do Low-
sophistication buyers. For other product categories, there is little difference
between Medium- and Low-sophistication buyers. Clearly, there would be
much to gain by addressing High-sophistication buyers, especially when
dealing with Strategic Security products.
These ﬁndings are consistent with and extend the conclusions that you drew
earlier from your exploratory analyses. You are ready to move on to addressing
the problem of too many pricing defects.
REVISING KNOWLEDGE
At this point, you are conﬁdent that you have identiﬁed a number of Hot Xs.
You are ready to move on to the Revise Knowledge step of the Visual Six Sigma
Data Analysis Process.
To ﬁnd conditions that will optimize the pricing process, as well as to ensure
that no signiﬁcant Xs have been overlooked, you involve your team members
and a larger group of sales representatives in discussing your observations and
conclusions and in suggesting ways to improve the pricing process. After all, the
sales representatives are the ones working with the pricing management process
on a daily basis, and you know that their knowledge and support will be critical
when you move into the imminent improvement phase of this project.
Once this is accomplished, you will meet with Bill to formulate an improve-
ment plan. You intend to monitor pricing over a short pilot period in a limited
market segment to verify that the improvement plan will lead to sustainable
beneﬁts before changes are institutionalized.
Identifying Optimal Strategies
You organize a number of workshops where you present the ﬁndings of your
data analysis. You are delighted and even surprised by the enthusiastic feedback
that you receive. Typical comments are:
◾
“I really like what you showed me . . . . It’s simple to understand and tells
a really clear story.”
◾
“I’ve been frustrated for a while. Now at last I can see what is going on.”
◾
“At last—someone prepared to listen to us—we’ve been saying for some
time that we need sales managers to understand some of our problems.”
◾
“It really looks like we are being out-negotiated. If we focus on our
strengths and exploit the great products we have, then I’m sure we can
win.”
◾
“We’ve allowed our competition to drive prices down for too long. We
need to do something quickly, and this gives us some really good ideas.”

206
V I S U A L S I X S I G M A
Speciﬁcally, the sales representatives highlight:
◾The need for more training in negotiation skills.
◾The need for more realistic sales management guidelines and price
targets through a tailored price management process. A target 5 percent
price increase across the board is seen by them as a very blunt instru-
ment. They want a more ﬁnely tuned approach that aims for higher
increases where Polymat has strong competitive advantage (as in the
Strategic Security and Strategic Critical product categories), but lower
target increases in commodity areas (as in the Tactical Proﬁt product
category). They strongly feel that this would allow them to focus
their time and negotiating energy where the return on their investment
is the highest.
You capture all the Xs that might lead to effective price increases, both
from your analyses and ideas generated by the sales representatives, in a
cause-and-effect diagram (Exhibit 6.31). You construct the cause-and-effect
diagram in JMP, using Analyze > Quality and Process > Diagram.
Using boldface, italicized text, you highlight potential causes that are iden-
tiﬁed as important based on your data analysis and the sales representatives’
discussions. The data are in the table CauseAndEffectTable.jmp and the
script is Cause-and-Effect Diagram.
We encourage you to consult the Help ﬁles to see how the cause-and-effect
diagram is constructed: Go to Help > Books > Quality and Process Methods
and ﬁnd the chapter called “Cause-and-Effect Diagrams.”
You now consolidate your ﬁndings for a summary meeting with Bill. You
summarize the Xs identiﬁed from a combination of the data analysis and
the sales representatives’ input in an Impact and Control Matrix, shown in
Exhibit 6.32. From your experience, you ﬁnd this to be a powerful tool for
crystallizing ﬁndings and directing an improvement strategy.
The Impact axis is the relative size of the impact that an X has on overall
process performance. For those Xs for which you have quantitative informa-
tion, you use this information to help make decisions on their degree of impact.
However, for Xs where you have only qualitative information from the sales
representatives, this assignment is necessarily more subjective.
Exhibit 6.31 Cause-and-Effect Diagram for Price Increase

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
207
Key Areas for Improvement
Impact
Control
Low
Low
Medium
Medium
High
High
Competitor Behavior
Product Category
Supply Demand
Balance
Sales Manager
Guidelines
Sales Rep Training
Sensible Targets
Buyer Sophistication
Sales Rep Experience
Exhibit 6.32 Impact and Control Matrix
The Control axis relates to the degree of control or inﬂuence that can be exer-
cised on this X through process redesign. Environmental factors, such as Buyer
Sophistication, are not things that can be inﬂuenced (absent unethical business
practices)—sales representatives have to deal with the buyers who are facing
them in the negotiation. However, the training of sales representatives, such as
the type of training, frequency of training, and appropriateness of training, is
within the control of Polymat.
You decide to place Supply Demand Balance in the Medium Control
category despite its appearance as an environmental factor outside Polymat’s
control. The reasoning behind this is that although Supply Demand Balance
itself cannot be directly controlled, Polymat’s leadership team can control the
timing of any price increase in response to market conditions. You believe that
this timing could be better managed to ensure that price increases are only
attempted when the overall supply/demand balance is favorable, such as in
periods of relative shortage.
The Xs toward the top right of the Impact and Control Matrix are clearly
the ones on which you and your team should focus in the Improve phase of
DMAIC. These Xs have a big impact and can be strongly inﬂuenced or controlled
by better process design.
Improvement Plan
After his sponsor review meeting with you and your team, Bill is delighted.
“I told you that I thought we were leaving money on the table in our pricing
negotiations. Now, thanks to your ﬁne work, I have the analysis that really
convinces me!”

208
V I S U A L S I X S I G M A
Moreover, Bill is extremely enthusiastic about the visual capabilities of JMP.
He admits privately to you that he was concerned that he might not be able to
convince his colleagues of the need to adopt a Six Sigma approach. His col-
leagues are not very process-oriented and have little time for what they view
as the “statistical complexity of the Six Sigma approach.” Bill can see that by
using Visual Six Sigma he is able to tell a simple but compelling story that shows
a clear direction for Polymat’s pricing efforts.
With your help and input, Bill pulls together an improvement plan focusing
on four key areas:
1. Product Category. The analysis has illustrated the power of your product
categorization method for Polymat. Because this work used only sample
products and customers, Bill agrees to convene workshops to classify all
of the signiﬁcant products in Polymat’s portfolio in this way. You work
with Bill to develop a review process to keep this classiﬁcation current.
2. Sales Representative Training. The analysis highlighted the relative weak-
ness of sales representatives in the price negotiation process. Sophisti-
cated buyers are much more successful in keeping prices low than are
sales representatives in negotiating prices up. This was reinforced by
the sales representatives themselves, who requested more training in
this area.
Consequently, Bill agrees to focused training sessions to improve
negotiation skills. These are to be directly linked to the new product
categorization matrix to ensure that sales representatives fully exploit
the negotiating strength provided by Polymat’s Strategic Critical and
Strategic Security products.
3. New Rules for Supply/Demand Balance Decisions. Recognizing the need
to understand the prevailing supply/demand balance before adjusting
pricing targets, Bill allocates one of his senior market managers to join
your team.
His role is to develop a monitoring process using trade association
data to track market dynamics by quarter. This process will be designed
to take account of ﬂuctuations in demand and also to track imports,
exports, and the startup of new suppliers. Business rules will ﬂag when
price increases are likely to be successful, and Polymat’s leadership
team will trigger the price management process to make an appropriate
response.
4. Price Management Process Redesign. Bill agrees that a total reengineering of
the price management process is needed. You and Bill set up a work-
shop to begin the process redesign, and, during an intensive two-day

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
209
offsite meeting, the workshop team develops a redesign with the follow-
ing features:
Step 1. Based on a target-setting worksheet, Polymat sales managers
will review each key customer and product combination and agree to
the percent price increase target for that particular product at that cus-
tomer. This replaces the blanket 5 percent increase that has been used
up to now. Typically, the targeted increase will be in the range of 0–10
percent.
Step 2. Using the worksheet, price negotiations will be tracked at
weekly account reviews. The current status will be ﬂagged using
red-yellow-green trafﬁc signals, so that sales managers can support
the sales representatives’ negotiations as needed.
Step 3. At the agreed date of the price increases, the newly mandated
price targets are input into the order entry system for subsequent trans-
actions.
Step 4. Following the price increases, the actual invoiced prices will
be downloaded from the order entry system and checked against the
new target. Any remaining pricing defects can then be remediated as
necessary. A dashboard of summary reports will allow the Polymat
business leadership team to track planned versus actual performance.
The leadership team agrees to take a phased approach to implementing the
new process design, starting with a six-month pilot in one market segment.
Unfortunately, the market seems to be switching from Oversupply to Shortage
just as the pilot starts.
Verifying Improvement
About six months later, you sit down with Bill to review some new data. The
market did switch from Oversupply to Shortage, so this data reﬂects a Supply
Demand Balance period of Shortage.
Your new data table is called PilotPricing.jmp. It consists of data on all
of the product and company combinations used in the previous study that had
activity during the pilot period. Over the period of the pilot study, only three
companies did not purchase the product that they had purchased during the
baseline study. Consequently, the data table consists of 237 rows.
You are interested in comparing the current level of % Price Increase with
your baseline. However, since Polymat is in a Supply Demand Balance period
of Shortage, you need to compare you pilot study data with baseline data for
Shortage periods.

210
V I S U A L S I X S I G M A
To obtain a Distribution analysis for % Price Increase during the pilot period
(PilotPricing.jmp, script is Distribution for % Price Increase):
1.
Select Analyze > Distribution.
2.
Select % Price Increase in the Select Columns list and click Y, Columns.
3.
Click OK.
4.
In the report, from the red triangle menu next to % Price Increase, select Display
Options > Horizontal Layout.
The new pricing management process has delivered a mean price increase
of about 5.8 percent.
You would like to see the histograms for % Price Increase during the pilot
period and for % Price Increase during the baseline period of Shortage next to
each other. You also want the histograms plotted with the same axis settings to
facilitate comparing the two histograms.
To obtain a Distribution analysis for % Price Increase during the baseline period in a period
of Shortage (BaselinePricing.jmp, script is % Price Increase by Supply
Demand Balance):
1.
Select Analyze > Distribution.
2.
Select % Price Increase in the Select Columns list and click Y, Columns.
3.
Select Supply Demand Balance in the Select Columns list and click By.
4.
Click OK.
5.
In the report, hold down the Control key as you click on either red triangle menu next
to % Price Increase.
This action broadcasts your selected option to other similar reports.
6.
Select Display Options > Horizontal Layout.
Two histograms appear: the ﬁrst for Supply Demand Balance = Oversupply and the
second for Supply Demand Balance = Shortage.
7.
In the report for the pilot pricing histogram, place your cursor over the horizontal axis in
a location where it turns into a hand pointer.
8.
Right-click and select Edit > Copy Axis Settings.
This action copies the horizontal axis settings for the plot.

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
211
9. In the report for the baseline pricing histogram for the Shortage period (the second
histogram), place your cursor over the horizontal axis in a location where it turns into a
hand pointer.
10.
Right-click and select Edit > Paste Axis Settings.
This action pastes the axis settings from the ﬁrst plot’s horizontal axis into the
current plot.
Exhibit 6.33 shows the Distribution report for the pilot period (top plot) and
the Distribution report for the baseline period when Supply Demand Balance
is Shortage (bottom plot).
The improvement is obvious. The % Price Increase histogram for the pilot
period (top, in Exhibit 6.33) is shifted to the right of the histogram for the
baseline in the period of shortage. The mean increase in the pilot period is
5.8 percent, compared with a mean increase of 4.7 percent for the baseline data
when there was a supply shortage. Although an approximate 1 percent gain in
% Price Increase does not appear dramatic, you calculate that for the complete
Polymat portfolio this is worth in excess of 2 million British pounds per year.
Next you delve a little more deeply into the data to see the nature of under-
lying changes. You are particularly interested in your Hot Xs and how they
relate to % Price Increase.
Construct a Distribution report as follows (PilotPricing.jmp, script is Distribution
for Five Variables):
1.
Select Analyze > Distribution.
2.
Select the following columns in the Select Columns list and click Y, Columns:
% Price Increase, Supply Demand Balance, Sales Rep Experience,
Product Category, Buyer Sophistication.
3.
Click OK.
A report showing all ﬁve histograms appears.
You explore the data by clicking on bars in the graphs, just as you did when
exploring the baseline data. You are intrigued by the following: In the Product
Category bar plot, when you shift-click on the Strategic Security and Strategic
Critical bars, you see a noticeable impact on % Price Increase (Exhibit 6.34).
You would like to compare the mean % Price Increase for these two cate-
gories to the % Price Increase during the baseline period of shortage.

Exhibit 6.33 Distribution Reports for % Price Increase during Pilot Study and during Baseline Shortage Period
212

Exhibit 6.34 Distribution Report Showing Product Category Impact
213

214
V I S U A L S I X S I G M A
To obtain the mean % Price Increase for these two categories during the pilot period
(PilotPricing.jmp, script is Local Data Filter for Product Category):
1.
From the red triangle menu next to Distributions, select Script > Local Data
Filter.
2.
From the Add Filter Columns list, select Product Category.
3.
Click Add.
4.
In the list of Product Category levels, hold down the Shift key and select Strategic
Critical and Strategic Security. See Exhibit 6.35.
The Distribution analysis automatically updates to show the report for only Strategic
Security and Strategic Critical sales.
Exhibit 6.35 Local Data Filter and % Price Increase for Strategic Security and Strategic
Critical Only

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
215
The mean % Price Increase for these two categories during the pilot period
is 8.12 percent. You go back to your BaselinePricing.jmp data table and con-
duct a similar analysis using the Local Data Filter. This analysis shows that for
the baseline data, in the Shortage period, the % Price Increase for these two
categories was only 5.88 percent.
You remind Bill that, for these product categories, sales representatives have
a relatively strong negotiating position due to the small number of alterna-
tive options available to the buyer. After the improvements were put in place,
the price increase for these products is strongly skewed to markedly higher
levels, suggesting much more effective and targeted negotiations during the
pilot period.
You want to further study the impact of Product Category and Buyer
Sophistication on % Price Increase over the pilot period.
To study the relationship between Product Category and Buyer Sophistication on % Price
Increase (PilotPricing.jmp, script is Oneway with Comparison Circles):
1. Select Analyze > Fit Y By X.
2. Enter % Price Increase as Y, Response.
3. Enter Product Category and Buyer Sophistication as X, Factor.
4. Click OK.
5. Hold down the Control and the ALT keys as you click on the red triangle menu for
one of the Oneway Analysis reports.
A menu showing all platform options appears.
6. Check the following options (see Exhibit 6.36):
◾Means and Std Dev
◾All Pairs, Tukey HSD
◾Mean Diamonds
◾Points Jittered
7. Click OK.
The report shown in Exhibit 6.37 appears. The comparison circles (and
associated tests) show no evidence of a Buyer Sophistication effect on % Price
Increase. As expected, though, the Product Category effect is present. The
means for all four categories are given in this report: Strategic Security and
Strategic Critical sales average an increase of 7.68 percent and 8.37 percent
respectively while the other two categories average increases of 4.87 percent
and 2.64 percent.
You take the data, these reports, and a summary of your key ﬁndings
from the pilot to a review meeting with Bill and the Polymat leadership team.

Exhibit 6.36 Select Options Window for Oneway Platform
216

Exhibit 6.37 Oneway Reports for % Price Increase
217

218
V I S U A L S I X S I G M A
Convinced by this study, the leadership team immediately gives the go-ahead
to implement the new pricing management process as rapidly as possible across
all operations.
UTILIZING KNOWLEDGE: SUSTAINING THE BENEFITS
Your pricing project was initiated by Bill in response to frustration over a con-
sistent Polymat price decline during an extended period due to:
◾Increasing commoditization of products
◾Increasing competition from low-cost, Far Eastern suppliers
Bill had a hunch that sales representatives were weak in pricing negotia-
tions, and you were able to clearly show that this was the case. You were also
able to target speciﬁc areas where those negotiation skills could be improved.
Exploiting the insights from a Visual Six Sigma analysis of failed attempts to
increase price unilaterally, you adapted and promoted a practical and simple
tool that was better able to capture the value proposition of products to
customers.
Using this tool allowed sales representatives to become better prepared
for speciﬁc price negotiations and to receive support if necessary. The pricing
management process was redesigned as an ongoing, data-driven, business
process to be triggered periodically when market or business conditions were
deemed conducive to price changes. The results from the pilot indicate yearly
revenue increases on the order of 2 million British pounds for Polymat’s
current operations.
In line with the price management process redesign, Bill now wants a mon-
itoring mechanism that allows the Polymat leadership team to easily track what
is happening to their prices over time, picking up early shifts and patterns that
might give another means to trigger their pricing review activity. So, as the ﬁnal
step in the project, you are asked to provide Bill with a simple way to visually
track shifts and trends in Polymat’s prices, ideally using leading rather than
lagging indicators, so that Polymat can react appropriately.
To do this, you draw inspiration from the idea of the Retail Price Index. You
create a Polymat Price Index based on a collection of Polymat products. This
index is based on the price of a ﬁxed volume of these selected products and is
corrected for currency exchange ﬂuctuations and inﬂation. It will be monitored
quarterly by the leadership team. The leadership team hopes to see the Polymat
Price Index rise over time.
You write a JMP script to calculate the Polymat Price Index by quarter. You
calculate the index by quarter for the year preceding the pilot and for the pilot
period itself. The data are given in PriceIndex1.jmp. Your plan is to monitor
this Price Index using an individual measurement chart.

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
219
To construct the control chart (PriceIndex1.jmp, script is Control Chart Builder):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Drag Time to the Subgroup zone.
3.
Drag Stage to the Phase zone.
4.
Drag Price Index to the Y zone.
You would like to remove the Moving Range chart. Locate its vertical axis, Moving
Range(Price Index).
5.
Right click on Moving Range(Price Index) and select Remove.
6.
From the red triangle next to Control Chart Builder, click Done.
This last step removes the Control Panel, making the display more appealing for
presentations.
The resulting chart is shown in Exhibit 6.38. The upper and lower horizontal
lines shown here for each level of Stage are control limits (they appear red on a
Exhibit 6.38 Control Chart for Polymat Price Index over Historical and Pilot Periods

220
V I S U A L S I X S I G M A
computer screen). You are somewhat disappointed because, although there may
be a trend forming over the project period, there is no statistical evidence that
that trend is anything more than random variation. Still the index has increased
relative to the Historical period. However, the early part of the Historical period
saw oversupply and that might explain the lower values. You realize that it may
take time for the improvements to reveal themselves.
You continue to update your chart with quarterly data over the next three
years. Now you are able to calculate the Polymat Price Index over three Stage
periods:
◾Historical—the year prior to the initiation of the project
◾Project—the year during which the project was conducted and tested
◾Post-Project—the three subsequent years
The data table containing your data is PriceIndex2.jmp. You construct a
three-phase control chart using instructions identical to those used to construct
the control chart in Exhibit 6.38. The control chart is shown in Exhibit 6.39 and
data table script is Control Chart Builder.
Exhibit 6.39 Control Chart for Polymat Price Index over Five Years

T R A N S F O R M I N G P R I C I N G M A N A G E M E N T I N A C H E M I C A L S U P P L I E R
221
Bill and the Polymat management team realize that control charts, such as
Exhibit 6.39, are powerful tools for tracking long-term trends in Polymat pric-
ing. Clearly the improvement work is having an impact in halting and reversing
the historical downward trend. Once the process stabilizes, control limits based
on that stable period can be used as objective decision limits to ﬂag changes in
pricing.
CONCLUSION
This case study illustrates how Visual Six Sigma can be exploited beyond the
traditional data-driven manufacturing and engineering areas. Almost any busi-
ness involved in selling products or services generates data in their transactional
activities, but such data are often used exclusively for accounting, rather than
for improvement. In process terms, pricing and price management generally
remain something of an art.
Pricing data are a very rich source of information about dynamics of the
market and the relationship between suppliers and their customers. In our case
study, using Visual Six Sigma reveals some dramatic insights. In response to
these ﬁndings, a simple and pragmatic value analysis tool is developed to help
sales representatives focus their price negotiations more effectively. In addition,
new measurement tools are developed that enable the supplier to better align
its price adjustments with shifts in market conditions.
Using Visual Six Sigma, the messages that were contained in Polymat’s his-
torical price change data revealed themselves as clear and unambiguous. These
ﬁndings could be presented in ways that were not clouded in statistical language
and that told a compelling story leading quickly to improvement opportunities.
Although access to subject matter expertise was vital, Visual Six Sigma allowed
you to make rapid progress with only the need for periodic consultations with
other Polymat personnel.
These ﬁndings led to the development of simple but powerful approaches
founded on measurements that were easily implemented. The sales represen-
tatives, sales managers, and Polymat’s leadership team readily adopted these
approaches because they were practical, gave new insights, and delivered visi-
ble and measurable beneﬁts.

C H A P T E R 7
Improving the
Quality of Anodized
Parts
223
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

224
V I S U A L S I X S I G M A
E
ven though defect reduction is often viewed as the overarching goal of a
Six Sigma project, optimization is just as important. In this case study, we
follow the efforts of your Six Sigma team as you work to both optimize an
existing manufacturing process and reduce the number of defects it produces.
Your company is Components Inc., a manufacturer of aluminum compo-
nents used in high-end audio equipment. The surfaces of these components are
anodized and then dyed to produce a visually smooth, rich, black surface.
Unfortunately, Components Inc. currently has a signiﬁcant problem with
discoloration of their black components. Lot yields in manufacturing are
extremely low, causing rework and compromising on-time delivery. Failure to
ﬁx this problem could cost Components Inc. its major customer and more than
a million dollars in losses.
Management assembles a Six Sigma project team, and you are the black
belt. You and the team are charged with improving the yield of the anodizing
process. This case study follows you as you and the team go through all of the
steps of the Visual Six Sigma Data Analysis Process.
You make extensive use of dynamic visualization in achieving your goal.
You identify four Ys and ﬁve Hot Xs that relate to yield. Measurement System Anal-
ysis (MSA) studies are conducted and the results are explored using variability
charts. Speciﬁcation limits for the four Ys are determined using Exploratory
Data Analysis (EDA) and visualization tools that include: distributions with
dynamic linking, graph builder, scatterplot matrices, and scatterplot 3D displays.
To understand how the yield can be increased, you design an experiment
that has to satisfy various constraints, some practical and some due to process
limitations. You ﬁt models to the four Ys, simultaneously optimize the Xs using
the prediction proﬁler, and then conduct simulations to estimate capability at
the new optimal settings. The predicted capability is explored using a goal plot.
The new settings for the Xs are implemented in production and the project
moves into the Control phase. A control chart of post-implementation data
shows that the process is stable and is highly capable, delivering predictable
performance and high yields. The project is deemed a resounding success. The
increased predictability of supply means that Components Inc. is able to retain
its key customer, and the increased yield reduces annual scrap and rework costs
by more than a million dollars.
The platforms and options used by you and your team are listed in
Exhibit 7.1. The data sets are available at http://support.sas.com/visualsixsigma.
SETTING THE SCENE
Components Inc. is a manufacturer of aluminum components for high-end
audio equipment. Components are anodized to protect against corrosion
and wear, and the anodized parts are dyed to produce a smooth, rich, black
surface that helps to make the ﬁnal product visually pleasing. Components Inc.
has one major customer and, given the premium price of the equipment

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
225
Exhibit 7.1 Platforms and Options Illustrated in This Case Study
Menus
Platforms and Options
Rows
Colors/Markers
Clear Row States
Cols
Column Info
Column Properties
Formula
DOE
Custom Design
Save Factors and Responses
Analyze
Distribution
Histogram
Frequency Distribution
Fit Model
Standard Least Squares
Effect Summary
Quality and Process
Control Chart Builder
Process Capability Analysis
Variability/Attribute Gauge Chart
Process Capability
Goal Plot
Graph
Graph Builder
Scatterplot Matrix
Scatterplot 3D
Proﬁler
Maximize Desirability
Sensitivity Indicators
Simulator
Contour Proﬁler
Other
Local Data Filter
Column Switcher
Components Inc. assembles and sells, buyers are very sensitive to its work-
manship and aesthetics, not just its audio performance.
In late 2014, Components Inc. begins to experience what becomes a signif-
icant problem with chronic discoloration of the components it is making. Lot
yield, determined by an outgoing visual inspection, ranges from 0 to 40 percent,
so there is substantial rework and scrap. The low yield means that on-time deliv-
ery of components in sufﬁcient quantity is very poor. In addition, the quality of
even the shipped components is often considered marginal when assessed by
Component Inc.’s customers, so some lots that are shipped are returned. Unless
Components Inc. can successfully improve yield and optimize the quality of
its components, it stands to lose millions of dollars, as well as the business of its
major customer.

226
V I S U A L S I X S I G M A
Anodizing is an electrolytic process used to increase the thickness and den-
sity of the natural oxide layer on the surface of metal parts. The anodized
surfaces are porous, and the pores may be ﬁlled with a dye or a corrosion sealer
to improve corrosion resistance. In the case of Components Inc., the pores are
ﬁlled with a dye to obtain the required black color.
The anodizing process used by Components Inc. is referred to as a Type
II anodize, where the anodizing is done in a sulfuric acid bath. The parts are
suspended in the acid bath and a direct current is applied in such a way that
the parts become the anodes of an electrolytic cell (hence the term anodize).
Oxygen is generated on the surfaces of the aluminum parts, causing a buildup
of aluminum oxide. The parameters used in the anodizing process not only have
a signiﬁcant impact on the coated thickness, but also affect the shape and size of
the pores that form in the coating. This in turn affects the ability of the anodized
surface to retain dye or other coatings.
For Component Inc., a defect occurs and yield is reduced when the surface
of an anodized part has either a purple or a smutty black appearance. The purple
color varies from a very light to a deep purple while the smutty black appear-
ance gives the impression that the ﬁnish is smudged and not blemish-free. An
acceptable surface has a rich, black, clear, and blemish-free appearance.
FRAMING THE PROBLEM
In January 2015, a Six Sigma project team is assembled and charged with
improving the yield of the anodizing process. As an accomplished black belt
working for Components Inc., you are assigned the task of leading the Six
Sigma project.
Along with your team, you initiate the Deﬁne phase of the project. In con-
junction with the project sponsor, you develop the initial project charter, shown
in Exhibit 7.2. You identify process yield as the Key Performance Indicator (KPI),
realizing that if this process measure improves, so will delivery performance.
You then begin working on the project by constructing a process map for the
anodizing process. To do this, you enlist the help of manufacturing personnel.
The resulting map, shown in Exhibit 7.3, contains the basic process steps as well
as key inputs (the boxes below each step) and outputs (the boxes above each
arrow connecting the steps) at each step.
You then work to deﬁne critical to quality (CTQ) output variables. The key
measures of quality are the thickness of the anodized coating and the color of
the parts:
◾Anodized thickness is measured in thousandths of an inch using a
backscatter radiation gauge.
◾Color is qualitatively assessed by inspectors and measured quantitatively
with a spectrophotometer.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
227
Exhibit 7.2 Project Charter
Project Title
Improve Black Anodize Quality and Yield
Business Case
Specialty anodizing is considered an area for substantial proﬁtable growth
for Components Inc. The ability to manufacture high-quality specialty
anodized items will increase Earnings Before Interest and Taxes (EBIT) and
open new potential markets for Component Inc.’s products.
Problem/Opportunity Statement
Currently, the black anodizing process has very low daily yields, usu-
ally below 40%, and averaging 19%. This results in high scrap and
rework costs. Also, Component Inc.’s largest customer is threatening to
ﬁnd another supplier if quality and on-time delivery are not substantially
improved. In the past six months, scrap and rework costs have totaled
approximately $450,000 with on- time delivery below 60%.
Project Goal Statement and KPI
(Key Performance Indicator)
Improve the black anodize process yield from 19% to a minimum of 90%
by July 2015 (a six-month timeframe).
The KPI is the lot-by-lot yield plotted on an Individual’s control chart.
Project Scope
The project will address only the black anodizing process. All other man-
ufacturing steps are out of scope for this project.
Project Team
Sponsor: John Good
Black Belt: This is you!
Team Members: Mike Knott, David Barry, Nancy Wiles, Bob Barr,
Mary Kendall
Cleaned
Parts
Pre-
Clean
Anodize
Tank
Hot H2O
Rinse
Dye
Tank
Rinse
Dry
Solvent
Time
Temp.
Current
Voltage
Time
Temp.
Acid Conc.
Number of parts
Location
Time
Temp.
Water
Time
Temp.
Water
Time
Temp.
Humidity
Dye Conc.
Time
Temp.
Tank pH
Start
Stop
Anodized
Parts
Rinsed
Parts
Dyed
Parts
Rinsed
Parts
Finished
Parts
Exhibit 7.3 Process Map of the Anodizing Process
The spectrophotometer provides a color assessment based on the three-axis
coordinate color scheme also known as the CIELAB (Commission Interna-
tionale de l’Eclairage) color space. Using CIELAB units, every color can be
uniquely deﬁned in a three-dimensional space in terms of the attributes L*, a*,
and b*, where:
◾
L* is a measure of lightness of the color (the range is 0–100, with lower
values indicating darker color).
◾
a* is a measure of red/green (positive values indicate redness and nega-
tive values indicate greenness).

228
V I S U A L S I X S I G M A
◾b* is a measure of yellow/blue (positive values indicate yellowness and
negative values indicate blueness).
Thus, there are four continuous CTQs for the anodizing process: anodized
coating thickness and the three color coordinates. Although the project charter
identiﬁes yield as the KPI, you believe that these four continuous Y measure-
ments determine yield. You also realize that these measures will prove much
more informative than would the attribute-based percent yield measurement
in identifying the root causes of the problem.
COLLECTING DATA
Data collection is the usual focus of the Measure Phase of a Six Sigma project.
Here, a team typically assesses the measurement systems for all key input and
output factors, formulating operational deﬁnitions if required and studying
variation in the measurement process. Once this is done, a team constructs a
baseline for current process performance.
For the measurement process, you are particularly concerned about the
visual inspection that classiﬁes parts as good or bad. However, the measure-
ment systems for the four Ys, namely coating thickness and the three color
coordinates, should also be examined. You ask the team to conduct Measure-
ment System Analyses (MSAs) on three measurement systems: the backscatter
gauge, the spectrophotometer, and the visual color inspection rating that clas-
siﬁes parts as good or bad.
Backscatter Gauge MSA
Since the capability of the backscatter gauge used to measure thickness has not
been assessed recently, the team members decide to perform their ﬁrst MSA on
this measurement system. They learn that typically only one gauge is used, but
that as many as 12 operators may use it to measure the anodize thickness of
the parts.
You realize that it is not practical to use all 12 operators in the MSA. Instead,
you suggest an MSA using three randomly selected operators and ﬁve randomly
selected production parts. You also suggest that each operator measure each part
twice, so that an estimate of repeatability can be calculated. The resulting MSA
design is a typical gauge repeatability and reproducibility (R&R) study, with
two replications, ﬁve parts, and three operators. (Such a design can easily be
generated using DOE > Full Factorial Design.) Note that, since the operators
are randomly chosen from a larger group of operators, the variation due to
operator will be of great interest.
To prevent any systematic effects from impacting the study (equipment
warmup, operator fatigue, etc.), the study is run in a completely random order.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
229
Exhibit 7.4 Data Table for Backscatter Gauge MSA
The data table ThicknessMSA_1.jmp contains the run order and results.
The ﬁrst ten rows are shown in Exhibit 7.4. Thickness is measured in thou-
sandths of an inch.
The variability in measurements between operators (reproducibility) and
within operators (repeatability) is of primary interest. You decide to use a vari-
ability chart, also called a Multi-Vari chart1 to better understand this variation
(see Exhibit 7.5).
To create the chart in Exhibit 7.5 (ThicknessMSA_1.jmp, script is Variability Chart):
1.
Select Analyze > Quality and Process > Variability / Attribute Gauge
Chart.
2.
Enter Thickness as Y, Response, and Operator then Part as X, Grouping,
and click OK.

230
V I S U A L S I X S I G M A
Exhibit 7.5 Initial Variability Chart for Thickness
The chart for Thickness shows, for each Operator and Part, the two mea-
surements obtained by that Operator. These two points are connected by range
bars, and a small horizontal dash is placed at the mean of the two measure-
ments. Below the chart for Thickness is a chart showing the standard deviations
of the two values for each Operator and Part combination.
Since there are only two values for each Operator and Part combination,
and since the range is illustrated in the Thickness chart, you decide to remove
the Std Dev chart. You also want to add some additional visual information
to the Thickness chart: the group means, the overall mean, and lines connecting
the cell means.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
231
To remove the standard deviation plot and to add graphical summaries to the variability chart, as
shown in Exhibit 7.6 (ThicknessMSA_1.jmp, script is Variability Chart 2):
1.
Hold the ALT key on your keyboard and click on the red triangle.
2.
Select Connect Cell Means, Show Group Means, and Show Grand
Mean, and deselect Std Dev Chart.
Tip: Holding the ALT key while clicking on the red triangle produces a dialog containing all of the
menu commands and options available.
The resulting variability chart is shown in Exhibit 7.6. The chart shows that,
for the ﬁve parts measured, Thickness values range from about 0.22 (Carl’s
lowest reading for Part 4) to around 2.00 (Lisa’s highest reading for Part 3).
However, this variation includes Part variation. In an MSA, the focus is on the
variation inherent in the measurement process itself rather than on studying
part-to-part variation.
Accordingly, the team members turn their focus to the measurement pro-
cess. They realize that Thickness measurements should be accurate to at least
Exhibit 7.6 Variability Chart for Thickness with Means

232
V I S U A L S I X S I G M A
0.1 thousandths of an inch, since they need to be able to measure in a range of
0.0–1.5 thousandths of an inch. The Thickness plot in Exhibit 7.6 immediately
signals that there are issues.
◾Measurements on the same Part made by the same Operator can differ
by 0.20–0.45 thousandths of an inch. To see this, look at the vertical line
(range bar) connecting the two measurements for any one Part within
an Operator, and note the magnitude of the difference in the values of
Thickness for these two measurements.
◾Different operators differ in their overall measurements of the parts. For
example, for the ﬁve parts, Carl gets an average value slightly over 1.0
(see the solid line across the panel for Carl), while Lisa gets an average of
about 1.4, and Scott averages about 1.2. For Part 1, for example, Carl’s
average reading is about 0.9 thousandths (see the small horizontal tick
between the two measured values), while Lisa’s is about 1.5, and Scott’s
is about 1.3.
◾There are differential effects in how some operators measure some parts.
In other words, there is an Operator by Part interaction. For example,
relative to their other part measurements, Part 2 is measured high by
Carl, low by Lisa, and at about the same level as the four other parts by
Scott.
Even without a formal analysis, the team knows that the measurement pro-
cess for Thickness must be improved. To gain an understanding of the situation,
three of the team members volunteer to observe the measurement process,
attempting to make measurements of their own and conferring with the oper-
ators who routinely make Thickness measurements.
These three team members observe that operators have difﬁculty repeating
the exact positioning of parts that they measure using the backscatter gauge.
Moreover, the gauge proves to be very sensitive to the positioning of the part
being measured. They also learn that the amount of pressure applied to the
gauge head on the part affects the thickness measurement. In addition, the
team members notice that operators do not calibrate the gauge in the same
manner, which leads to reproducibility variation.
Armed with this knowledge, the team and a few of the operators work
with the metrology department to design a ﬁxture that automatically locates
the gauge on the part and adjusts the pressure of the gauge head on the part.
At the same time, the team works with the operators to deﬁne and implement
a standard calibration practice.
Once these changes are implemented, the team conducts a new MSA study
to see if they can conﬁrm improvement. Three new operators are chosen for
this study. The ﬁle ThicknessMSA_2.jmp contains the results (the saved

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
233
Exhibit 7.7 New Variability Chart for Thickness
script is Variability Chart). The variability chart for this new study is shown in
Exhibit 7.7.
For comparison, you would like to see this new variability chart with the
same axis scaling as is used in the chart shown in Exhibit 7.6. To accomplish this,
you copy axis settings from the initial variability chart and paste these settings
into the new chart.
To copy and paste axis settings (ThicknessMSA_2.jmp, script is Variability Chart
Original Scaling):
1.
Open the variability chart for the initial study, and move your cursor over the vertical axis
until it becomes a hand.
2.
Right-click and choose Edit > Copy Axis Settings.
This copies the axis settings to the clipboard.
3.
Return to the variability chart for the new study, and hover over the vertical axis until it
becomes a hand.
4.
Right-click, and select Edit > Paste Axis Settings.
The resulting chart (shown in Exhibit 7.8) now has the same vertical scale as
used in the plot for the initial study. The plot conﬁrms dramatic improvement
in both repeatability and reproducibility. In fact, the repeatability and repro-
ducibility variation is not discernable given the scaling of the chart.

234
V I S U A L S I X S I G M A
Exhibit 7.8 Rescaled Variability Chart
The team follows this visual analysis with a formal gauge R&R analysis.
Although there are no speciﬁcation limits for Thickness, the gauge R&R anal-
ysis allows the team to estimate the variability in the improved measurement
system.
To perform the gauge R&R analysis (ThicknessMSA_2.jmp, script is Gauge R&R):
1.
Click on the red triangle and choose Gauge Studies > Gauge RR.
2.
In the resulting Variability Model window click OK to accept the default model
(Crossed).
A crossed model is used since each operator measured each part.
3.
In the resulting Gauge R&R Specifications window (Exhibit 7.9), click OK.
The value of 6 in the box next to K, Sigma Multiplier tells JMP to estimate the
measurement system variation based on a window of six standard deviations.
Recall that the measurement process should be able to detect differences
of 0.1 thousandths of an inch. The resulting analysis is shown in Exhibit 7.10.
It indicates a Gauge R&R value of 0.0133. This means that the measurement
system variation, comprised of both repeatability and reproducibility variation,
will span a range on the order of only 0.0133 thousandths of an inch.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
235
Exhibit 7.9 Gauge R&R Speciﬁcations Dialog
Exhibit 7.10 Gauge R&R Report for Backscatter Gauge

236
V I S U A L S I X S I G M A
This indicates that the measurement system will easily distinguish parts that
differ by 0.1 thousandths of an inch. Thanks to the new ﬁxture and new pro-
cedures, the Thickness measurement process is now extremely capable!
Having completed the study of the backscatter gauge, you guide the team
in conducting an MSA on the spectrophotometer used to measure color. Using
an analysis similar to the one above, the team ﬁnds the spectrophotometer to
be extremely capable.
Visual Color Rating MSA
At this point, you address the visual inspection process that results in the lot
yield ﬁgures. Parts are classiﬁed into one of three Color Rating categories:
Normal Black, Purple/Black, and Smutty Black. Normal Black characterizes
an acceptable part. Purple/Black and Smutty Black signal defective parts, and
these may result from different sets of root causes. So, not only is it important
to differentiate good parts from bad parts, it is also important to differentiate
between these two kinds of bad parts.
You work with the team to design an attribute MSA for the visual inspec-
tion process. Eight different inspectors are involved in inspecting color. Three
inspectors are randomly chosen as raters for the study. The team chooses 50
parts from production, structuring this sample so that each of the three cate-
gories is represented at least 25 percent of the time. That is, the sample of 50
parts contains at least 12 each of the Normal Black, Purple/Black, and Smutty
Black parts. This is so that accuracy and agreement relative to all three cate-
gories can be estimated with similar precision. Each part is uniquely labeled,
but the labels are not visible to the raters.
To choose such a sample and study the accuracy of the visual inspection
process, the team identiﬁes an in-house expert rater. Given that customers sub-
sequently return some parts deemed acceptable by Components Inc., the expert
rater suggests that he also work with an expert rater from their major customer
to rate the parts to be used in the MSA. For the purposes of the MSA, the con-
sensus classiﬁcation of the parts by the two experts will be considered correct
and will be used to evaluate the accuracy of the inspectors.
The experts rate the 50 parts that the team has chosen for the study. The
data are given in the table AttributeMSA_PartsOnly.jmp. You use Analyze
> Distribution to create a distribution of the color ratings (see Exhibit 7.11;
the saved script is Distribution). This conﬁrms that all three categories are well
represented, and you deem the 50-part sample appropriate for the study.
Your team randomly selects Hal, Carly, and Jake to be the three raters. Each
rater will inspect each part twice. To minimize the potential for recall, the parts
will be presented in random order to each rater on two consecutive days. The
random presentation order is shown in the table AttributeMSA.jmp. The Part

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
237
Exhibit 7.11 Distribution of Parts Used in Color Rating MSA
column shows the order of presentation of the parts on each of the two days.
The order for each of the Days differs, but to keep the study manageable, the
same order was used for all three raters on a given day. Ideally, the study would
have been conducted in a completely random order. Note that the Expert Rating
is also given in this table.
The team conducts the MSA and records the rating for each rater in
the AttributeMSA.jmp table. Part of the analysis of this data is shown in
Exhibit 7.12.
To analyze the attribute MSA data (AttributeMSA.jmp, script is Attribute Chart):
1.
Select Analyze > Quality and Process > Variability / Attribute Gauge
Chart
2.
Enter Hal, Carly and Jake as Y, Response, Expert Rating as the Standard,
and Part as the X, Grouping variable, and click OK.
You ﬁrst focus on the kappa criterion in the Agreement Comparisons
panel to evaluate interrater agreement, as well as agreement with the expert
(these results are shown toward the bottom of the report window). Kappa pro-
vides a measure of beyond chance agreement. It is generally accepted that a
kappa value between 0.60 and 0.80 indicates substantial agreement, while a
kappa value greater than 0.80 reﬂects almost perfect agreement.

238
V I S U A L S I X S I G M A
Exhibit 7.12 Agreement Reports for Color Rating MSA
In the Agreement Comparisons panel, you see that all kappa values
exceed 0.60. For comparisons of raters to other raters (the top report, in
Exhibit 7.12) kappa is always greater than 0.72. The kappa values that mea-
sure rater agreement with the experts all exceed 0.80 (the middle report in
Exhibit 7.12).
Next, you observe that the Agreement within Raters panel (bottom report
in Exhibit 7.12) indicates that raters are fairly repeatable. Each rater rated at
least 80 percent of parts the same way on both days.
The effectiveness of the measurement system is a measure of accuracy, that is,
of the degree to which the raters agree with the experts. Loosely speaking, the
effectiveness of a rater is the proportion of correct decisions made by that rater.
An effectiveness of 90 percent or higher is generally considered acceptable. The
Effectiveness Report (under the disclosure icon) reports the effectiveness of
the measurement system (Exhibit 7.13).
There is room for improvement, as Hal has an effectiveness score below
90 percent and Carly is at 91 percent. Also since these three raters are a random
selection from a larger group of raters, it may well be that other raters not used
in the study will have effectiveness scores below 90 percent as well. The Mis-
classiﬁcations table gives some insight on the nature of the misclassiﬁcations.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
239
Exhibit 7.13 Effectiveness Report for Color Rating MSA
Based on the effectiveness scores, you and the team take note that a study
addressing improvements in accuracy is warranted. You include this as a rec-
ommendation for a separate project. However, for the current project, the team
agrees to treat the visual Color Rating measurement system as capable.
Summarizing progress to date, the team has validated that the measure-
ment systems for the four CTQ Ys and for yield are capable. You can now safely
proceed to collect and analyze data from the anodize process.
Baseline
The anodize process is usually run in lots of 100 parts, where typically only one
lot is run per day. However, occasionally, for various reasons, a complete lot of
100 parts is not available. Only those parts classiﬁed by the inspectors as Normal
Black are considered acceptable. The project KPI, process yield, is deﬁned as the
number of Normal Black parts divided by the lot size, that is, the proportion of
parts that are rated Normal Black.
The baseline data consist of two months’ worth of lot yields, which are given
in the ﬁle BaselineYield.jmp. You have computed Yield in this ﬁle using a
formula. To view this formula in the Formula Editor, click on the plus sign next
to Yield in the Columns panel (Exhibit 7.14). To display Yield as a percentage,
you apply the Percent format in the Column Info window.
It might appear that Yield, which is intrinsically a proportion, would be
monitored by a p chart. However, the process is not likely to be purely binomial
(with a single, ﬁxed probability for generating a defective part). More likely,
it will be a mixture of binomials because there are many extraneous sources
of variation. For example, materials for the parts are purchased from different

240
V I S U A L S I X S I G M A
Exhibit 7.14 Formula for Yield
suppliers, the processing chemicals come from various sources and have varying
shelf lives, and different operators assemble the parts. All of these contribute to
the likelihood that the underlying proportion defective is not constant from lot
to lot.
For this reason, you choose to display the baseline data on an individual
measurement chart.
To create the chart in Exhibit 7.15 (BaselineYield.jmp, script is Baseline Control
Chart):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Drag Yield to the Y drop zone.
3.
Click Done to close the control panel.
Your team is astounded to see that the average process yield is so low—the
average yield is 18.74 percent. The process is apparently stable, except perhaps
for an indication of an upward shift starting at lot 34. In other words, the process

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
241
Exhibit 7.15 Individual and Moving Range Chart of Yield
is producing this unacceptably low yield primarily as a result of common causes
of variation, namely, variation that is inherent to the process. Consequently,
improvement efforts will have to focus on common causes. In a way, this is
good news—it should be easy to improve from such a low level. However, your
team’s goal of a yield of 90 percent or better is a big stretch!
Data Collection Plan
At this point, you and your team engage in serious thinking and animated dis-
cussion about the direction of the project. Color Rating is a visual measure of
acceptability given in terms of a nominal (attribute) measurement. You realize
that a nominal measure does not provide a sensitive indicator of process behav-
ior. This is why you focused, right from the start, on Thickness and the three
color measures, L*, a*, and b*, as continuous surrogates for Color Rating.
Your long-range strategy is this: Your team will design an experiment to
model how each of the four continuous Ys varies as a function of various process
factors. Assuming that there are signiﬁcant relationships, you will ﬁnd optimal
settings for the process factors. But what does “optimal” mean? It presumes
that you know where the four responses need to be in order to provide a Color
Rating of Normal Black.
No speciﬁcation limits for Thickness, L*, a*, and b* have ever been deﬁned.
So you realize that the team also needs to collect data on how Color Rating and
the four continuous Ys are related. In particular, you want to determine if there
are ranges of values for Thickness, L*, a*, and b* that essentially guarantee that
the Color Rating will be acceptable. These ranges would provide speciﬁcation
limits for the four responses, allowing you and, in the long term, production
engineers to assess process capability with respect to these responses.

242
V I S U A L S I X S I G M A
You decide to proceed as follows. Team members will obtain quality inspec-
tion records for lots of parts produced over the past six weeks. For ﬁve randomly
selected parts from each lot produced, they will research and record the values
of Color Rating, Thickness, L*, a*, and b*.
It happens that 48 lots were produced during that six-week period. The
team collects the data on ﬁve parts from each of these 48 lots, resulting in a
data table, Anodize_ColorData.jmp, that contains 5 × 48 = 240 rows.
UNCOVERING RELATIONSHIPS
A visual analysis of the data that the team has collected (Anodize_ColorData
.jmp) will give some insight on whether certain ranges of Thickness, L*, a*, and
b* are associated with the acceptable Normal Black value of Color Rating while
other ranges are associated with the defective values Purple/Black and Smutty
Black. You would like to conclude that good parts can be separated from bad
parts based on the values of the four continuous Ys. If so, then those values
would suggest speciﬁcation limits that should result in good parts.
This is a multivariate question. Even so, it makes sense to you to follow the
Visual Six Sigma Roadmap (Exhibit 3.30), uncovering relationships by viewing
the data one variable at a time, then two at a time, and then more than two at
a time.
Using Distribution
To begin the process of uncovering relationships, you construct plots for Color
Rating and for each of Thickness, L*, a*, and b* using the Distribution platform
(Exhibit 7.16).
To create the graphs in Exhibit 7.16 (Anodize_ColorData.jmp, script is Distribution
for Five Responses):
1.
Select Analyze > Distribution.
2.
Select Color Rating through b* as Y, Columns.
3.
Check the Histograms Only option, and click OK.
When you check the Histograms Only option, the default numerical summaries
and box plots do not display in the Distribution report.
The distribution of Color Rating shows a proportion of good parts (Normal
Black) of about 20 percent. This is not unexpected, given the results of the

Exhibit 7.16 Distribution Reports for Five Response Variables
243

244
V I S U A L S I X S I G M A
baseline analysis. However, you are mildly surprised to see that the proportion
of Smutty Black parts is about twice the proportion of Purple/Black parts. You
also notice that the distributions for Thickness, L*, a*, and b* show clumpings
of points, rather than the expected mound-shaped pattern.
You would really like to see the values of Thickness, L*, a*, and b* strat-
iﬁed by the three categories of Color Rating. There are many ways to do this
in JMP. Start with the simple approach of clicking on the bars in the bar graph
for Color Rating. When you click on the bar for Smutty Black, the 126 rows
corresponding to Smutty Black parts are selected in the data table and JMP
shades all open histograms to represent these 126 points. Studying the graphs
in Exhibit 7.17, you begin to see that only certain ranges of values correspond
to Smutty Black parts.
Next, click on the Purple/Black bar. The shaded areas change substantially
(Exhibit 7.18). Again, you see that very speciﬁc regions of Thickness, L*, a*,
and b* values correspond to Purple/Black parts.
However, you are most interested in which values of Thickness, L*, a*, and
b* correspond to Normal Black parts. Click on Normal Black in the Color Rating
distribution graph (Exhibit 7.19). Note that there is a speciﬁc range of values for
each of the four responses where the parts are of acceptable quality. In general,
Normal Black parts have Thickness values in the range of 0.70 to 1.05 thou-
sandths of an inch. For Normal Black parts, you note that L* values range from
roughly 8.0 to 12.0, a* values from 0.0 to 3.0, and b* values from −1.0 to 2.0.
Using Graph Builder
You realize that it would be more efﬁcient to see these distributions in a single
display. The JMP Graph Builder is an interactive graphing platform for explor-
ing many variables at a time, with zones for dragging and dropping variables
and element icons for displaying various types of graphs. The Graph Builder
template is shown in Exhibit 7.20.
Start by exploring how the values of Thickness differ across the three cat-
egories of Color Rating (Exhibit 7.21).
To create the graph in Exhibit 7.21 (Anodize_ColorData.jmp, script is Graph Builder
Thickness):
1.
First, clear the previously selected rows using Rows > Clear Row States.
2.
Select Graph > Graph Builder.
3.
Drag Thickness to the Y zone (see Exhibit 7.20)
4.
Drag Color Rating to the Group X zone at the top of the display area.

Exhibit 7.17 Histograms for Thickness, L*, a*, and b*, Shaded by Smutty Black
245

Exhibit 7.18 Histograms for Thickness, L*, a*, and b*, Shaded by Purple/Black
246

Exhibit 7.19 Histograms for Thickness, L*, a*, and b*, Shaded by Normal Black
247

248
V I S U A L S I X S I G M A
Exhibit 7.20 Graph Builder Template
Exhibit 7.21 Graph Builder with Thickness and Color Rating
The graph stratiﬁes the Thickness measurements according to the three
levels of Color Rating: Normal Black, Purple/Black, and Smutty Black. It is easy
to see that the distribution of Thickness differs across the three color ratings.
You would like to see similar graphs for all four responses. There are a num-
ber of ways of doing this. One approach is to use the Column Switcher (select
Scripts > Column Switcher from the red triangle menu) to view each of the

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
249
four responses in turn. Another approach is to simply add all of the responses
to the graph. You opt for the latter approach, which results in one display con-
taining all four responses (Exhibit 7.23).
To add the other three response variables to the Y zone (Anodize_ColorData.jmp, script is
Graph Builder All Responses):
1.
Drag L* to the Y zone below Thickness, and drop it in the zone when a bottom-justiﬁed
blue polygon appears (see Exhibit 7.22).
2.
Repeat for the other two response variables.
Exhibit 7.22 Adding Additional Variables to the Y Zone
You study the resulting plot (in Exhibit 7.23) and conclude that Normal
Black parts and Purple/Black parts generally appear to have distinct ranges of
response values, although there is some overlap in L* values. Normal Black
parts and Smutty Black parts seem to share common response values, although
there are some systematic tendencies. For example, Normal Black parts tend to
have lower Thickness values than do Smutty Black parts.
Although the dot plots reveal the differences in the distributions across the
categories of Color Rating, a different graph type, such as a histogram or box
plot, might be a better tool for highlighting these differences. Exhibit 7.24 shows
box plots for each response across the three color ratings.

250
V I S U A L S I X S I G M A
Exhibit 7.23 Graph Builder Plot with Four Ys, Grouped by Color Rating
Exhibit 7.24 Final Graph Builder Plot with Box Plots

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
251
To display boxplots in the Graph Builder (Anodize_ColorData.jmp, script is Graph
Builder Box Plots):
1.
Click on the box plot icon at the top of the template (see Exhibit 7.20).
2.
Click Done to close the control panel.
Tip: Click and drag icons onto the template to add graph elements to the existing display. Right-click
in the graph template to see other graphing options.
This is a compact way to view and present the information that you and
your team visualized earlier by clicking on the bars in the bar graph for Color
Rating. It shows the differences in the values of the four response variables
across the categories of Color Rating, all in a single plot.
Using Scatterplot Matrix
A Scatterplot Matrix (found in the Graph menu) is another tool that might
help in the effort to deﬁne speciﬁcation ranges for the four Ys. This graph
allows you to explore the color ratings across pairs of response variables (see
Exhibit 7.25).
To create the scatterplot matrix in Exhibit 7.25 (Anodize_ColorData.jmp, script is
Scatterplot Matrix):
1.
Select Graph > Scatterplot Matrix.
2.
Enter the four responses (Thickness, L*, a*, and b*) as Y, Columns, and click OK.
3.
To obtain markers and colors for the points by Color Rating, right-click in any one of
the scatterplot panels and select Row Legend.
4.
In the resulting dialog, choose Color Rating, set Markers to Standard, and click
OK.
Tip: When setting the row legend, an interactive legend panel appears. Note that the colors and
markers are applied to the row states in the data table.
When you click on the text Normal Black in the legend (Exhibit 7.26),
the points that correspond to Normal Black in all the scatterplots (the circles)
are highlighted and are colored bright red (on a computer screen). Using the

252
V I S U A L S I X S I G M A
Exhibit 7.25 Scatterplot Matrix with Row Legend
legend, click on each Color Rating in turn to highlight the corresponding points
in the graphs. (To deselect points, click in the white space in any scatterplot.)
The regions that deﬁne each value of Color Rating are even more strik-
ing than when viewed in the histograms or box plots. The Purple/Black parts
occur in very different regions from Normal Black and Smutty Black parts. More
interestingly, whereas the Normal Black and Smutty Black parts were difﬁcult
to distinguish using single responses, in the scatterplot matrix you see that they
seem to fall into fairly distinct regions of the b* and L* space. In other words,
joint values of b* and L* might well distinguish these two groupings.
Using a Scatterplot 3D
The regions that differentiate Color Rating become even more striking when
viewed in three dimensions. Scatterplot 3D (found in the Graph menu) pro-
vides this three-dimensional view of the data.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
253
Exhibit 7.26 Scatterplot Matrix with Normal Black Parts Selected
Exhibit 7.27 shows the plot with Thickness, L*, and a* on the axes, and
with points showing colors and markers for the different color ratings. Recall
that these were saved to the data table when we selected the Row Legend
earlier.
To create the graph in Exhibit 7.27 (Anodize_ColorData.jmp, script is Scatterplot 3D):
1.
Select Graph > Scatterplot 3D.
2.
Enter Thickness, L*, a*, and b* as Y, Columns, and click OK.
Tip: Use the dropdown menu at the bottom of the scatterplot to change the response measures
displayed.

254
V I S U A L S I X S I G M A
Exhibit 7.27 Scatterplot 3D
To explore where the color ratings fall, you use a local data ﬁlter for Color
Rating. This works like a row legend, allowing you to display points on the
Scatterplot 3D corresponding to selected color rating.
In Exhibit 7.28 the points corresponding to Normal Black (again shown by
red circles) are displayed using the local data ﬁlter, while the points correspond-
ing to Purple/Black and Smutty Black are hidden. You rotate the plot to get a
better idea where the Normal Black points fall with respect to Thickness and a*.
To create the plot in Exhibit 7.28 (Anodize_ColorData.jmp, script is Scatterplot 3D
Local Data Filter):
1.
Click on the red triangle, and select Script > Local Data Filter.
2.
Select Color Rating from the list of variables, and click Add.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
255
3.
Select Normal Black to display only these points on the graph.
4.
To rotate the plot (to see patterns more clearly), place your cursor in the plot, then click
and drag.
Tip: The Local Data Filter applies only to the active window. For a global ﬁlter, which will apply
to the data table and to all open windows, select Rows > Data Filter.
Exhibit 7.28 Scatterplot 3D with Local Data Filter
As you rotate the plot and use the Local Data Filter to explore the four
responses, you can see patterns emerge. It seems clear that Color Rating values
are associated with certain ranges of Thickness, L*, a*, and b* as well as with
multivariate functions of these values.
Proposing Speciﬁcations
Using the information from the histograms and the two- and three-dimensional
scatterplots, you feel comfortable in proposing speciﬁcations for the four Ys that
shouldgenerallyresultinacceptableparts.Althoughthemultidimensionalviews
suggest that combinations of the response values successfully distinguish the
three Color Rating groupings, you decide that, because they are easier to work
with in practice, you will propose speciﬁcation limits for each Y individually.
Exhibit 7.29 summarizes the proposed targets and speciﬁcation ranges for
the four Ys. The data indicate that these will generally distinguish Normal Black

256
V I S U A L S I X S I G M A
Exhibit 7.29 Speciﬁcations for the Four CTQ Variables
Variable
Target
Speciﬁcation Range
Thickness
0.9
±0.2
L*
10.0
±2.0
a*
2.0
±2.0
b*
0.0
±2.0
parts (good parts) from the other two groupings (bad parts), and in particular,
from the Smutty Black parts. (You might like to check these limits against the
appropriate three-dimensional scatterplots!)
We note that, at this point, you could have used more sophisticated analyt-
ical techniques such as discriminant analysis and logistic regression to further
your knowledge about the relationship between Color Rating and the four CTQ
variables. However, the simple graphical analyses provided you and the team
with sufﬁcient knowledge to move to the next step.
LOCATING THE TEAM ON THE VSS ROADMAP
This project was cast as a formal Six Sigma project, following the DMAIC struc-
ture. Let’s take a step back to see how your application of the DMAIC cycle ﬁts
with the Visual Six Sigma Data Analysis Process and with the Visual Six Sigma
Roadmap (Exhibits 3.29 and 3.30).
◾Framing the Problem corresponds to the Deﬁne phase.
◾Collecting Data corresponds to the Measure phase. Here, you collected
data for the MSA studies and for the baseline control chart. You also
collated a set of historical data that relates Color Rating, the team’s pri-
mary, but nominal, Y, to four continuous Ys, namely Thickness, L*, a*,
and b*, that provide more detailed information than Color Rating.
◾Uncovering Relationships occurs in the Analyze phase. You ﬁrst visual-
ized the ﬁve variables using Distribution. You dynamically explored
relationships between Color Rating and Thickness, L*, a*, and b*. You
constructed a plot using Graph Builder that summarized this informa-
tion. Then, you dynamically visualized the variables two at a time with
a scatterplot matrix. Finally, you dynamically visualized the variables
three at a time using Scatterplot 3D.
◾Modeling Relationships, which will occur in the next section, bridges the
Analyze and Improve phases, where the team identiﬁes and determines
the impact of potential Hot Xs by modeling the relationships between
the Xs and the Ys and optimizing settings of the Xs.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
257
◾
Revising Knowledge, where a team addresses the question of how new
knowledge will generalize, is part of the Improve phase. Often, in revis-
ing knowledge, a team runs conﬁrmation trials to assess if its expecta-
tions will be met.
◾
Utilizing Knowledge includes part of the Improve phase as well as all of the
Control phase. Here, the solution identiﬁed by the team is implemented
together with a way to check that the improvement is real and to assure
that it is maintained over time.
MODELING RELATIONSHIPS
Together with process experts, you and your team reexamine the process map
in Exhibit 7.3. You conduct brainstorming sessions to identify potential causes
of bad parts. These sessions identify ﬁve possible Hot Xs:
◾
Coating variables: Anodize Temperature, Anodize Time, and Acid Con-
centration.
◾
Dye variables: Dye Concentration and Dye pH.
You need to determine if these are truly Hot Xs and to model the rela-
tionships that link Thickness, L*, a*, and b* and these Xs. So, you will guide
the team in conducting a designed experiment. These ﬁve process factors may
or may not exert a causal inﬂuence. The designed experiment will indicate
whether they do, and, if they do, it will allow you to model the Ys as func-
tions of the Xs. You will use the resulting models to optimize the settings of the
input variables in order to maximize yield.
Although Color Rating is the variable that ultimately deﬁnes yield, using
a nominal response in a designed experiment is problematic—a large number
of trials would be required in order to detect real effects. Fortunately, you’ve
learned that there are strong relationships between Thickness, L*, b*, and a*,
and the levels of Color Rating. Accordingly, you decide to design an experiment
where the four responses will be L*, b*, a* and Thickness. There will be ﬁve
factors: Anodize Temp, Anodize Time, Acid Conc, Dye Conc, and Dye pH.
With the data from this experiment in hand, you will move to the
Model Relationships phase of the Visual Six Sigma Data Analysis Process
(Exhibit 3.29). You will use the guidance given under Model Relationships in
the Visual Six Sigma Roadmap (Exhibit 3.30) to direct your analysis.
Developing the Design
You now face a dilemma in terms of designing the experiment, which must be
performed on production equipment. Due to the poor yields of the current pro-
cess, the equipment is in continual use by manufacturing. Negotiating with the
production superintendent, you secure the equipment for a single production

258
V I S U A L S I X S I G M A
shift, during which the team will be allowed to perform the experiment. Unfor-
tunately, at most 12 experimental trials can be performed on a single shift
(assuming that the team works very efﬁciently).
A two-level factorial treatment structure for the ﬁve factors (a 25 design)
would require 32 runs. Obviously, the size of the experiment needs to be
reduced. You consider various options.
Your ﬁrst thought is to perform a 25–2 fractional factorial experiment, which
has 8 runs and is a quarter-fraction of the full factorial experiment. With the
addition of two center runs, this experiment would have a total of ten runs.
However, you realize with the help of the JMP Screening Design platform
(DOE > Screening Design) that the 25–2 factional factorial is a resolution III
design, which means that some main effects are confused with, or aliased with,
the joint effect of two factors, which makes it impossible to tell them apart. In fact,
for this particular design, each main effect is aliased with a two-way interaction.
You discuss this idea with your teammates, but they decide that it is quite
possible that there are two-way interactions among the ﬁve factors under study.
As a result, you determine that a resolution III fractional factorial design is not
the best choice. As you also point out, with ﬁve experimental factors there are
ten two-way interaction terms. You would need at least 16 runs to estimate the
5 main effects, the 10 interactions, and the intercept in any model. However,
due to the limit of 12 runs, such a 16-run design is not feasible, so an appropriate
compromise is needed.
You decide to continue this discussion with two experts, who join the team
temporarily. Recall that the factors to be studied relate to two steps in the
anodize process (Exhibit 7.3):
◾Anodize Tank, where the anodize coating is applied
◾Dye Tank, where the coated parts are dyed in a separate tank
The two experts maintain that interactions cannot occur between the two
dye tank factors and the three anodize tank factors, although two-way inter-
actions can certainly occur among the factors within each of the two steps. If
the team does not estimate interactions between factors from the two anodize
process steps, only four two-way interactions need to be estimated:
◾Anodize Temperature*Anodize Time
◾Anodize Temperature*Acid Concentration
◾Anodize Time*Acid Concentration
◾Dye Concentration*Dye pH
It is possible to estimate the ﬁve main effects, the four two-way interactions
of importance, and the intercept in a model with only 10 runs. Given the 12-run

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
259
limitation, this design is feasible. In fact, you can add two runs. If the effects are
large relative to the error variation, the resulting design is likely to identify
them. You are reasonably comfortable proceeding under the experts’ assump-
tion, realizing that any proposed solution will be veriﬁed using conﬁrmation
trials before it is adopted.
Another critical piece of knowledge relative to designing the experiment
involves the actual factors settings to be used. With the help of the two experts
that the team has commandeered, low and high levels for the ﬁve factors are
speciﬁed. You are careful to ensure that these levels are aggressive relative to
the production settings. The thinking is that, if a factor or interaction has an
effect, you want to maximize the chance of detecting it.
You now proceed to design the experiment. The design requirements can-
not be met using a classical design, so you generate a Custom Design (from the
DOE menu). The Custom Design platform allows you to specify a constraint
on the total number of trials and to specify the effects to be estimated. The plat-
form then searches for an optimal design that satisﬁes your requirements.2 The
custom design window, with the four responses and ﬁve factors, is shown in
Exhibit 7.30.
To specify the responses and factors, as shown in Exhibit 7.30:
1.
Select DOE > Custom Design.
2.
Add the four responses in the Responses panel, and enter the lower and upper
speciﬁcation limits.
Leave the Importance value set to 1 for each response, because all responses are
considered equally important.
3.
Add the ﬁve factors in the Factors panel, and specify the high and low levels.
Tip: Use the red triangle options to Save Responses and Save Factors to data tables. If
you don’t want to enter this information manually, click on the red triangle to Load Responses
and Load Factors. The ﬁles are called Anodize_CustomDesign_Responses.jmp
and Anodize_CustomDesign_Factors.jmp, respectively.
Initially, the Model panel shows the list of main effects. You add the required
four two-way interactions manually. Then, you indicate the number of runs.
You request 12 runs in all, reserving two runs for center points to provide an
estimate of repeatability. The completed dialog is shown in Exhibit 7.31.

260
V I S U A L S I X S I G M A
Exhibit 7.30 Custom Design Window with Responses and Factors
Exhibit 7.31 Completed Dialog Showing Custom Design Settings

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
261
To add the two-way interactions and specify the number of runs, as shown in Exhibit 7.31:
1.
Select the three anodize factors, Anodize Temp, Anodize Time, and Acid Conc,
in the list of Factors.
2.
Click on Interactions in the Model panel, selecting 2nd.
This adds the three interactions to the Model panel.
3.
Add the single Dye pH*Dye Conc interaction in a similar fashion.
4.
In the Design Generation panel, enter 2 for the Number of Center Points
and 12 next to User Specified under Number of Runs.
5.
Click Make Design to generate the design.
The design appears in the Design panel (see Exhibit 7.32). It is constructed
so that the runs are in random order. Note that the two center points appear as
runs 10 and 12.
The design that you obtain will very likely differ from the one shown in
Exhibit 7.32. This is because the algorithm used to construct custom designs
requires a random seed to determine a starting design. The algorithm then runs
for a ﬁxed number of seconds, during which a number of designs are con-
structed based on random starts. The ﬁnal design is the best one found, based
on the optimality criterion.
To obtain the exact design shown in Exhibit 7.33 (Anodize_CustomDesign_Table.jmp):
1.
Before clicking Make Design, select Set Random Seed from the top red triangle
and enter 3744528.
2.
Select Number of Starts from the red triangle and enter 1.
3.
Click Make Design and Make Table.
The script for the model that you will eventually ﬁt to the data you collect,
called Model, is located in the Table panel in the upper left corner of the data
table (see Exhibit 7.33). This script deﬁnes the model that you speciﬁed when
you built the design, namely, a model with ﬁve main effects and four speciﬁc
two-way interactions (to see this, run the Model script).
JMP has saved two other scripts to this data table. One of these scripts,
Screening, runs a screening analysis that ﬁts a saturated model. You will not be

262
V I S U A L S I X S I G M A
Exhibit 7.32 Custom Design Runs and Additional Options
interested in this analysis since you have some knowledge of the model that is
appropriate and you have enough observations to estimate error for that model.
The other script, DOE Dialog, reproduces the DOE > Custom Design dialog
used to obtain this design.
Notice the asterisks next to the variable names in the Columns panel
(Exhibit 7.34). When you click on the asterisks, you see that JMP has saved a
number of Column Properties for each of the factors: Coding, Design Role,
and Factor Changes. Clicking on any one of these takes you directly to that
property in Column Info. Similarly, for each response, Response Limits have
been saved as a Column Property.
You will use the results of this experiment to identify the optimal settings for
the four responses, and then to estimate the capability and PPM values at these
settings. So, you add speciﬁcation limits for each response as a Spec Limits
column property. These are based on the values listed in Exhibit 7.29.

Exhibit 7.33 Design Table Generated by Custom Design
263

264
V I S U A L S I X S I G M A
Exhibit 7.34 Column Properties for Anodize Temp
To set speciﬁcation limits, as shown in Exhibit 7.35
(Anodize_CustomDesign_Table.jmp, script is Set Spec Limits):
1.
Right-click in the header for the Thickness column and select Column Info.
2.
In the Column Info window, from the Column Properties list, select Spec
Limits.
3.
Complete the Spec Limits dialog as shown in Exhibit 7.35.
4.
Repeat for L*, a*, and b*.
Conducting the Experiment
You are now ready to perform the experiment. You explain the importance of
following the randomization order and of resetting all experimental conditions
between runs, and the team appreciates the importance of these procedures.
You plan the details of how the experiment will be conducted, and number
the parts produced to mistake-proof the process of taking measurements of the
responses.
The design and measured responses from the experiment are given in the
data table Anodize_CustomDesign_Results.jmp (Exhibit 7.36).
Uncovering the Hot Xs
It is time to analyze the data and you and your team are very excited! Since JMP
has saved the Model script to the data table, you start by running this script.
This takes you to the Fit Model Speciﬁcation window. You see that JMP has
included all of the responses and the model effects that you speciﬁed when you
designed the experiment, so you select Run to ﬁt the speciﬁed model to each
of the responses.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
265
Exhibit 7.35 Dialog for Thickness Spec Limits
At the top of the report (in Exhibit 7.37) is an Effect Summary table, which
provides a summary of the signiﬁcance of each of the terms across all of the
models. Every term, except the interaction Anodize Temp*Anodize Time, is
highly signiﬁcant (with a PValue < 0.01) in at least one of the models.
Since you are interested in understanding which factors and interactions
are signiﬁcant in predicting each of your four responses, you decide to identify
signiﬁcant effects by modeling each response separately. For each response, you
follow this strategy:
◾
Examine the data for outliers and possible lack of ﬁt, using the Actual
by Predicted plot as a visual guide. Check the Lack Of Fit Test, which
can be conducted thanks to the two center points, in order to conﬁrm
your visual assessment of the Actual by Predicted plot.
◾
Find a best model by eliminating effects, one at a time, that appear
insigniﬁcant.
◾
Save the prediction formula for the best model as a column in the data
table.
◾
Save the script for the best model to the data table for future
reference.

Exhibit 7.36 Results of Designed Experiment
266

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
267
Exhibit 7.37 Report for Full Model for All Responses
Your plan, once this work is completed, is to use the Profiler from the
Graph menu to ﬁnd factor level settings that will simultaneously optimize all
four responses.
To ﬁt models for each response, starting with Thickness:
1.
Run the Model script saved in the Table panel.
2.
In the Model Specification window, select L*, a* and b* next to Y and click the
Remove button.
3.
Click Run to run the full model for Thickness.
4.
To reduce this model manually, use the Remove button in the Effect Summary
table and methodically remove nonsigniﬁcant terms, starting with the two-way interactions.
The report for the model for Thickness, shown in Exhibit 7.38, shows
no signiﬁcant lack of ﬁt—both the Actual by Predicted plot and the formal
Lack Of Fit test support this conclusion (the p-value for the Lack Of Fit test is
Prob > F = 0.4763). Note that this is a nice example of Exploratory Data Analysis
(the Actual by Predicted plot) being reinforced by Conﬁrmatory Data Analysis
(the Lack Of Fit test).

268
V I S U A L S I X S I G M A
Exhibit 7.38 Actual by Predicted and Lack of Fit for Thickness
Since the model appears to ﬁt the data well, you check the Analysis of
Variance table and see that the overall model is signiﬁcant (Exhibit 7.39). You
could examine the Effect Tests tables to see which effects are signiﬁcant,
but you ﬁnd it easier to interpret the Sorted Parameter Estimates table
(Exhibit 7.39), which gives a graph where the size of a bar is proportional to
the corresponding effect’s signiﬁcance.
You examine the p-values provided under Prob > |t|, and see that two of the
two-way interactions, Anodize Temp*Anodize Time and Dye pH*Dye Conc, do
not appear to be signiﬁcant (you use the 0.05 p-value guideline for signiﬁcance).

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
269
Exhibit 7.39 Effect Summary, ANOVA, and Sorted Parameter Estimates for Thickness
The Effect Summary table at the top of the window (Exhibit 7.39) repeats
the p-values shown in the Sorted Parameter Estimates (displaying one addi-
tional decimal place). However, this table also provides an interactive way of
reﬁning the model. For example, nonsigniﬁcant terms can be taken out with
the Remove button.
The caret (^) next to a p-value indicates that the corresponding term is
involved in one or more interactions with smaller p-values. Signiﬁcant or not,
terms marked with ^ should be retained in the model if they are involved in sig-
niﬁcant interactions. This practice follows from the principle of Effect Heredity.3

270
V I S U A L S I X S I G M A
The implication for reducing models is that nonsigniﬁcant higher-order terms
are removed prior to removing main effects.
At this point, you start reducing the model, one term at a time. Dye pH
and Dye Conc are the two least signiﬁcant terms, but both are involved in
the Dye pH*Dye Conc interaction. This interaction is the next least signiﬁcant
term. Remove the Dye pH*Dye Conc interaction term from the model using the
Remove button. All of the results in the window automatically update based
on the new model. You note that Dye pH is now the next least signiﬁcant term
and it is not a component of an interaction, so you remove it from the model.
Next you successively remove Dye Conc and Anodize Temp*Anodize Time. At
this point, all remaining terms are signiﬁcant at the 0.05 level (Exhibit 7.40).
To reduce the Thickness model manually using the Effect Summary table
(Anodize_CustomDesign_Results.jmp, script is Thickness Model):
1.
Select the least signiﬁcant term that is not contained in a signiﬁcant higher-order
interaction.
2.
Click the Remove button at the bottom of the table.
3.
Repeat steps 1 and 2, one term at a time, until only signiﬁcant terms remain in the model.
You notice that the ﬁnal Thickness model contains two signiﬁcant interac-
tions, and that only factors from the anodizing step of the process are signiﬁcant.
Thus, the team ﬁnds that the model is in good agreement with engineering
knowledge, which indicates that factors in the dye step should not have an
impact on anodize thickness.
You use this same approach to determine ﬁnal models for each of the other
three responses. Scripts that show results for all four models are saved to the
Table panel in the data table Anodize_CustomDesign_Results.jmp (see
Exhibit 7.41). Each of the models for L*, a*, and b* includes factors from both
the anodizing and dyeing processes.
REVISING KNOWLEDGE
In the Visual Six Sigma Data Analysis Process, the Model Relationships step is
followed by Revise Knowledge (see Exhibit 3.29). This is where we identify the
best settings for the Hot Xs, visualize the effect of variation in the settings of
the Hot Xs, and grapple with the extent to which our conclusions generalize.
Having developed models for the four responses and, in so doing, identiﬁed the
Hot Xs, you and the team now proceed to the Revise Knowledge step.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
271
Exhibit 7.40 Report for Reduced Model for Thickness
Determining Optimal Factor Level Settings
Now that you have statistical models for the four responses, your intent is to
identify settings of the ﬁve Xs that optimize these four Ys. You suspect that
factor settings that optimize one response are likely to degrade the performance
measured by another response. For this reason, it is important that simultaneous
optimization be conducted to give a sound measure of overall performance.

Exhibit 7.41 Final Model Scripts for Four Responses
272

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
273
In the Analyze Phase, your team deﬁned target values and speciﬁcation lim-
its for the four Ys, hoping to guarantee acceptable color quality for the anodized
parts. Using these targets and speciﬁcations as a basis for optimization, you will
perform multiple response optimization in JMP.
JMP bases multiple optimization on a desirability function. Recall that, when
you entered the responses in the Custom Design dialog, you noted that the
goal for each response was to Match Target, and you entered the speciﬁcation
limits as response limits—see the Lower Limit and Upper Limit entries under
Responses in Exhibit 7.30. You also assigned Importance values of 1 to each
response (also Exhibit 7.30), indicating that the responses are of equal impor-
tance. (What is relevant is the ratio of these values; they could equally well
have all been assigned as 0.25.)
The desirability function constructs a single criterion from the response lim-
its and importance values. This function weights the responses according to
importance, and, in a Match Target situation, places the highest desirability on
values in the middle of the response range (the user can manually set the target
elsewhere, if desired). The desirability function is a function of the set of factors
that is involved in the union of the four models. In this case, since each factor
appears in at least one of the models, the desirability function is a function of
all ﬁve process factors.
In JMP, desirability functions are accessed from the Profiler, which is often
called the Prediction Proﬁler to distinguish it from the several other proﬁlers that
JMP provides. The Profiler for a single response can be found in the Fit Model
report for that response. When different models are ﬁt to multiple responses,
the Profiler can also be accessed from the Graph menu. But, you must ﬁrst save
prediction formulas for each of the responses; otherwise, JMP will not have the
underlying models available to optimize.
To save the four prediction formulas to the data table, for each response
(Anodize_CustomDesign_Results.jmp, script is Four Prediction Formulas):
1.
Run the script that ﬁts the model for the given response (Thickness Model, L*
Model, a* Model, or b* Model).
2.
In the report, click the red triangle and select Save Columns > Prediction
Formula.
3.
Repeat for each response model.
This inserts a Pred Formula column in the data table for each response—these
columns appear as the ﬁnal four columns in the data table (see Exhibit 7.42).

Exhibit 7.42 Columns Containing Prediction Formulas for Final Models
274

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
275
Eachofthesecolumnsisdeﬁnedbytheformulaforthemodelfortheresponse
speciﬁed. For example, the prediction formula for a* is given in Exhibit 7.43.
Exhibit 7.43 Prediction Formula for a*
Once the prediction formulas for the four models have been saved, you can
proﬁle the four prediction formulas together. The resulting Profiler is shown in
Exhibit 7.44.
To generate a prediction proﬁler using saved prediction formulas
(Anodize_CustomDesign_Results.jmp, script is Profiler 1):
1.
Select Graph > Profiler,
2.
Enter the four prediction formulas as Y, Prediction Formula
3.
Click OK.

276
V I S U A L S I X S I G M A
Exhibit 7.44 Prediction Profiler for Four Responses
The desirability functions for each individual response are displayed in the
rightmost column. For each response, the maximum desirability value is 1.0,
and this occurs at the midpoint of the response limits. The least desirable value
is 0.0, and this occurs near the lower and upper response limits. (Since your
speciﬁcations are symmetric, having the highest desirability at the midpoint
makes sense.) The cells in the bottom row in Exhibit 7.44 show traces, or
cross-sections, for the desirability function associated with the simultaneous
optimization of all four responses.
Recall that the response limits for b* were −2.0 and +2.0. To better under-
stand the desirability function, you double-click in the desirability panel for b*,
in the rightmost column. The resulting dialog is displayed in Exhibit 7.45. You
note that −2.0 and +2.0 are given desirability close to 0, namely, 0.0183, and
that the midpoint between the response limits, 0, is given desirability 1. If you
want to change any of these settings, you can do so in this dialog (click Cancel
to close this dialog).
The Profiler is dynamically linked to the models for the responses. When
the Profiler ﬁrst appears, the dotted (red) vertical line in each panel is set to
the average predictor value, which is the midpoint of the design interval. By
moving the dotted (red) vertical line for a given process factor, one can see the
effect of changes on the four responses. This powerful dynamic visualization

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
277
Exhibit 7.45 Response Goal Dialog for b*
technique enables what-if inquiries, such as, “What happens if we increase
Anodize Time?” The team explores various scenarios using this feature, before
returning to the goal of optimization.
To perform the optimization, you select the Maximize Desirability option
from the red triangle next to Prediction Profiler. Results for the optimization
of the four responses are shown in Exhibit 7.46. However, since there are usu-
ally many equivalent solutions to such an optimization problem, the results
you obtain may differ from those shown in Exhibit 7.46 (these speciﬁc results
can be obtained by running the Profiler 2 script in Anodize_CustomDesign_
Results.jmp).
A wealth of information concerning the responses and the process vari-
able settings is provided in this visual display. At the bottom of the display, you
see optimal settings for each of the ﬁve process variables (the ﬁgures in red in
Exhibit 7.46). These are vastly different from the settings currently used in pro-
duction. To the left of the display you see the predicted mean response values
associated with these optimal settings (also in red).
You note that the predicted mean levels of all four responses are reasonably
close to their speciﬁed targets and are well within the speciﬁcation limits. It does
appear that further optimization could be achieved by considering higher values
of Anodize Time and Acid Conc, since the optimal settings of these variables are
at the extremes of their design ranges. You make a note to consider expanding
these ranges in a future experiment.
Linking with Contour Profiler
Even though the optimal factor settings obtained are feasible in this case, it
is always informative to investigate other possible optimal or near-optimal set-
tings. You believe that the Contour Profiler would be useful in this context. You
create a Contour Profiler to explore the four prediction formulas. The report
in Exhibit 7.47 shows contours of the four prediction formulas, as well as small
surface plots of those formulas.

278
V I S U A L S I X S I G M A
Exhibit 7.46 Results of Simultaneous Optimization of Four Responses
Exhibit 7.47 Contour Profiler for the Four Prediction Formulas

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
279
To create a contour plot in a separate window, as shown in Exhibit 7.47
(Anodize_CustomDesign_Results.jmp, script is Contour Profiler):
1.
Select Graph > Contour Profiler.
2.
Enter all four prediction formulas as Y, Prediction Formula in the dialog.
3.
Click OK.
Note that the Contour Profiler can also be accessed from the red-triangle menu next to
Profiler.
In the top part of the Contour Profiler report, the Current X values are
the midpoints of the design intervals. Recall that these were the initial settings
for the factors in the Prediction Profiler as well. You would like to set these
at the optimal settings obtained using the Prediction Profiler. You learn that
this can be achieved by linking the two proﬁlers. After the proﬁlers are linked,
the settings for Current X in the Contour Profiler update to match the optimal
settings found in the Prediction Profiler (Exhibit 7.48).
To link the Contour Profiler to the Prediction Profiler
(Anodize_CustomDesign_Results.jmp):
1.
Place the two proﬁlers side-by-side.
2.
Click on the red arrow next to Prediction Profiler and choose Factor Settings
> Link Profilers.
This causes the Contour Profiler to update to the settings in the Prediction
Profiler. Note that if you have lost your optimal settings in the Prediction Profiler,
you can rerun Maximize Desirability.
Now, by choosing pairs of Horiz and Vert factors in the top corner of the
Contour Profiler, and by moving the sliders next to these or by moving the
crosshairs in the contour plot, you can see the effect of changing factor settings
on the predicted responses. Since the proﬁlers are linked, you can see the effect
on overall desirability by checking the Prediction Profiler, which updates as
the factor settings are changed in the Contour Profiler.
The Contour Profiler, with its ability to link to the Prediction Profiler, is an
extremely powerful tool in terms of exploring alternative factor level settings.

280
V I S U A L S I X S I G M A
(In fact, all JMP Proﬁlers, including the Surface Plot, link together.) In some
cases it might be more economical, or necessary for other reasons, to run at
settings different from those found as optimal by the Prediction Profiler. These
tools allow you to ﬁnd practically equivalent or superior alternative settings by
assessing the loss in performance relative to the theoretical, but unworkable,
optimum.
Exhibit 7.48 Contour Profiler after Linking
At this point, you close the Contour Profiler and the Prediction Profiler.
Since you have been exploring various settings of the predictors, you rerun the
script Profiler 2 to retrieve the optimal settings.
Sensitivity
The Prediction Profiler report provides three ways to assess the sensitivity of
the responses to the settings of the process variables: desirability traces, a sensitivity
indicator, and Variable Importance. We illustrate the ﬁrst two methods. The third
method, Variable Importance, is an option under the Profiler red triangle menu.
For each factor, Variable Importance provides an index of predictive variability
that is based on a collection of simulated settings. See the JMP documentation
for details.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
281
Exhibit 7.49 Desirability Traces in Last Row of Prediction Proﬁler
Notice that the last row of the Prediction Profiler display, repeated in
Exhibit 7.49, contains desirability traces for each of the process variables. These
traces represent the overall sensitivity of the combined desirability functions
to variation in the settings of the process factors. For example, the desirability
trace for Anodize Temp is peaked, with sharply descending curves on either
side of the peak. Thus, the desirability function is more sensitive to variation in
the setting of Anodize Temp than, say, to Dye pH, which is much less peaked
in comparison. Variation in the setting of Anodize Temp will cause signiﬁcant
variation in the desirability of the responses.
Click the red triangle in the Prediction Profiler report panel and select
Sensitivity Indicator. These indicators appear in Exhibit 7.50 as small
triangles in each of the response proﬁles. (Note that the grabber tool has been
Exhibit 7.50 Prediction Profiler Report with Sensitivity Indicators

282
V I S U A L S I X S I G M A
used to rescale some of the axes so that the triangles are more visible; when you
place your cursor over the ends of an axis, the grabber tool will automatically
appear.) The height of each triangle indicates the relative sensitivity of that
response at the corresponding process variable’s current setting. The triangle
points up or down to indicate whether the predicted response increases or
decreases, respectively, as the process variable increases.
For Anodize Temp, you notice that Pred Formula L* and Pred Formula a*
both have relatively tall downward-pointing triangles, indicating that, accord-
ing to your models, both L* and a* will decrease fairly sharply with an increase
in Anodize Temp. Similarly, you see that Pred Formula Thickness and Pred
Formula b* have upward-pointing triangles, indicating that those responses will
increase with an increase in Anodize Temp.
You notice the horizontal traces and lack of sensitivity indicators for Dye
pH and Dye Conc in the row for Pred Formula Thickness. Remember that not
all factors appear in all prediction formulas. Speciﬁcally, the dye variables did
not appear in the model for Thickness. So, it makes sense that horizontal lines
appear, and that no sensitivity indicators are given, since the data lead to a
model in which Dye pH and Dye Conc are unrelated to Thickness.
From this analysis, you conclude that the joint desirability of the responses
will be quite sensitive to variation in the process variables in the region of their
new optimal settings. The team reminds you that some process experts did not
believe, prior to the experiment, that the anodize process, and especially color,
was sensitive to Anodize Temp. It is because of this unfounded belief that tem-
perature is not controlled well in the current process. The team views this lack
of control over temperature as a potentially large contributor to the low yields
and substantial run-to-run variation seen in the current process.
Conﬁrmation Runs
The team now thinks it has a potential solution to the color problem.
Namely, the process should be run at the optimized settings for the Ys, while
controlling the Xs as tightly as possible. The Revise Knowledge step in the Visual
Six Sigma Roadmap (see Exhibit 3.30) addresses the extent to which our
conclusions generalize. Gathering new data through conﬁrmation trials at the
optimal settings will either provide support for the model or indicate that the
model falls short of describing reality.
So, to see if the optimal settings actually do result in good product, you
suggest that the team conduct some conﬁrmation runs. Such conﬁrmation is
essential before implementing a systemic change to how a process operates. In
addition to being good common sense, this strategy will address the skepticism
of some of the subject matter experts who are not involved with the team.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
283
With support from the production manager, your team performs two con-
ﬁrmatory production runs at the optimized settings for the process variables.
The results of these conﬁrmation runs are very favorable—not only do both
lots have 100 percent yield, the outgoing inspectors declare that these parts
have the best visual appearance they have ever seen. The team also ships some
of these parts to Component Inc.’s main customer, who reports that these are
the best they have received from any supplier.
Projected Capability
At this point, your team is ready to develop an implementation plan to run the
process at the new optimized settings. However, you restrain the team from
doing this until the capability of the new process is estimated. You point out
that this is very important, since some of the responses are quite sensitive to
variation in the process variables.
In Design for Six Sigma (DFSS) applications, estimation of response distribu-
tion properties is sometimes referred to as response distribution analysis. Predictive
models, or more generally transfer functions, are used to estimate or simulate the
amount of variation that will be observed in the responses as a function of
variation in the model inputs.
The Simulator
The Prediction Profiler includes an excellent simulation feature. Even when
set at their optimal values, most process variables will have some variation in
their settings. Also, there is variation in the response that is caused by other pro-
cess factors. You want to include both of these sources of variation in obtaining
capability estimates. Once you have obtained these estimates of variability, you
will use the Simulator in the Prediction Profiler to estimate the overall process
capability at the new settings.
To obtain information on the variability of factors and responses, your team
collects data on batches produced over a two-week period. The ﬁve factors are
set to their optimal settings. They are measured at periodic intervals to obtain
estimates of variation in those settings. Process engineers indicate that you can
control Anodize Time with essentially no error, and so you can treat this factor
as ﬁxed during simulations. Standard deviations for the other process factors
are computed.
The responses are also measured at the same time as the factors. You ﬁt a
regression model for each response using the data collected on the factors and
responses. The root mean squared error (RMSE) from each model provides an
estimate of the standard deviation of the response at the optimal settings. It
represents the variation not explained by the ﬁve process factors.

284
V I S U A L S I X S I G M A
Exhibit 7.51 Standard Deviations for Factors and Responses
Variable
Variable Type
Estimated Standard Deviation
Anodize Temp
Factor
1.542
Acid Conc
Factor
1.625
Dye pH
Factor
0.100
Dye Conc
Factor
0.323
Pred Formula Thickness
Response
0.015
Pred Formula L*
Response
0.200
Pred Formula a*
Response
0.205
Pred Formula b*
Response
0.007
These estimates of standard deviation for factors and responses are given in
Exhibit 7.51.
Add these estimates as Sigma column properties for each of the four fac-
tors and for the four prediction formulas for the responses. Then return to the
Prediction Profiler to run the simulation. When you add the Sigma column
property for a factor or prediction formula, the Simulator default is to simu-
late random values with standard deviations equal to the speciﬁed values for
Sigma. The default for factors is to set the mean equal to the Profiler setting
for the factor.
To enter the standard deviations for the four factors and the four prediction formulas as column
properties and then simulate results (Anodize_CustomDesign_Results.jmp, scripts
are Set Sigma, Profiler 3, and Profiler 4):
1.
For each variable, right-click on the column header and select Column Properties
> Sigma and enter the estimated standard deviation as Sigma.
2.
Run the Profiler 2 script to obtain the Prediction Profiler with the optimized
settings.
3.
Select Simulator from the red triangle next to Prediction Profiler (see
Exhibit 7.52).
Normal distribution plots appear at the bottom of the report. These have means equal
to the proﬁler settings and standard deviations given by the Sigma column property.
Handles in the plots allow you to increase or decrease the standard deviation interactively.
4.
For each response under the Simulator outline, change No Noise to Add
Random Noise.
Note that the Std Dev is the Sigma value that you speciﬁed.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
285
5.
Click on the Simulate button, located to the far right (circled in Exhibit 7.52).
A panel with information on defects appears beneath the Simulate button.
6.
Right-click in the table that contains the simulated results and select Columns >
PPM.
This adds estimates of defective parts per million (PPM) to the panel for defects.
Exhibit 7.52 Simulation at Optimal Settings of Process Factors
When you click the Simulate button, JMP simulates 5,000 combinations
of factor settings using the speciﬁed normal distributions. JMP calculates pre-
dicted values plus error variation, as speciﬁed by the Std Dev values, for each
of the four responses (script is Profiler 4). Histograms in the rightmost col-
umn of the Prediction Profiler show the simulated values plotted against their

286
V I S U A L S I X S I G M A
Exhibit 7.53 Simulation Results
speciﬁcation limits (see Exhibit 7.53). Keep in mind that your optimal settings
may differ from those shown and that simulated results will vary.
Based on the simulation results and using the Spec Limits column prop-
erties, the Prediction Profiler calculates estimated defect rates for each of the
four responses. The estimated defect rate for L* is 0.0064, or 6,400 parts per mil-
lion, which is higher than the team would like. For the other three responses,
at least to four decimal places, the estimated defect rate is 0.
Simulated Capability
Rerunning the simulation a few times indicates that the overall defect rate is
not likely to exceed 0.008, corresponding to a PPM level of 8,000. To obtain an
estimate of the capability of the process when run at the new settings, save the
5,000 simulated response values from one of your simulations to a new data
table. Click Make Table under Simulate to Table. (See bottom of Exhibit 7.53.)
This runs the simulation and creates a new data table with the simulated results
and a Distribution script.
Open Anodize_CustomDesign_Simulation.jmp to see a set of simu-
lated results. Run the Distribution script in this data table to see histograms and
capability analyses for the four predicted responses. The capability analyses are

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
287
provided because speciﬁcation limits were saved as column properties for the
original responses and JMP carried those forward to the prediction formulas.
Because the capability analyses are based on randomly simulated data, the
results will change slightly each time you simulate. To reproduce the results
shown here, use the simulated data in Anodize_CustomDesign_Simulation
.jmp.
The report for Pred Formula Thickness is shown in Exhibit 7.54. For Pred
Formula Thickness, CPK is 2.66. Note that the process is slightly off center.
Exhibit 7.54 Capability Report for Predicted Thickness
You can use JMP’s Process Capability platform to visualize capability-
related data when several responses are of interest. Although you have only
four responses, you are interested in exploring the simulated capability using
this platform.
To analyze your results using the Process Capability platform
(Anodize_CustomDesign_Simulation.jmp, script is Process Capability):
1.
Make sure that the simulated data table is active. Then select Analyze > Quality
and Process > Process Capability.
2.
In the capability launch dialog, enter all four prediction formulas as Y, Process.
3.
Click OK.
The capability report for the four responses is shown in Exhibit 7.55. The
Goal Plot displays a point for each predicted response. The horizontal value for
a response is its mean shift from the target divided by its spec range. The vertical

288
V I S U A L S I X S I G M A
Exhibit 7.55 Capability Report for Four Simulated Responses
value is its standard deviation divided by its spec range. The ideal location for a
response is near (0, 0); it should be on target and its standard deviation should
be small relative to its spec limits.
The slider at the bottom of the Goal Plot is set by default to a Ppk of 1.0.
The slider deﬁnes a triangular, or goal, area in the plot, within which responses
have Ppk values that exceed 1.0. The slider can be moved to change this thresh-
old Ppk value and the corresponding goal area. Recall that a stable and centered
process with a Ppk (long term or overall process capability) of 1.0 has a 0.27 per-
cent defect rate; such a process is generally considered unacceptable. Assuming
that a process is stable and centered, a Ppk of 1.5 (corresponding to a one-sided

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
289
rate of 3.4 defective items per million) is generally the hallmark of an acceptable
process.
When a Ppk of 1.5 is entered, the triangular region gets smaller, but three
of the four responses still fall in (or on the border of) the goal area. Only Pred
Formula L* falls outside the 1.5 Ppk region.
To change the Ppk to 1.5 and label points in the Goal Plot, as shown in Exhibit 7.56
(Anodize_CustomDesign_Simulation.jmp, script is Process Capability 2):
1.
Enter the value 1.5 into the Ppk text box.
2.
Select Label Overall Sigma Points from the red triangle next to Goal Plot.
3.
Click on a label and drag to change the positioning of the label.
From the red triangle menu at the top level of the report, next to Capabil-
ity, select Summary Reports > Overall Sigma Report Summary. This report
shows summary information for all four predicted responses, as well as their
Ppk values (Exhibit 7.57). It is clear that L* would beneﬁt from further study
and work.
Note that some default summary information is not displayed in
Exhibit 7.57, and that Expected PPM Outside has been added. You can
control what appears in the Overall Sigma Capability Summary Report by
right-clicking in the report, selecting Columns, and then making selections to
add or remove summary information. The saved script is Process Capability 3.
The second plot in Exhibit 7.55 shows Capability Box Plots. These are box
plots constructed from appropriately centered and scaled data that allow a fair
comparison between responses. Because each of your responses has two spec-
iﬁcation limits that are symmetric about the target, the box plot is constructed
from values obtained as follows: The response target is subtracted from each
measurement, and then this difference is divided by the speciﬁcation range.
This scaling places the spec limits at 0.50 and −0.50. You see at a glance that
the simulated responses fall off target. In the case of L*, which is the most vari-
able relative to the speciﬁcation range, some simulated values fall below the
lower spec limit.
New Temperature Control System
As you reexamine results from the earlier sensitivity analysis, you recall that
L* and a* are particularly sensitive to variation in Anodize Temp, which is not

290
V I S U A L S I X S I G M A
Exhibit 7.56 Goal Plot with Labels and Ppk Region at 1.5
Exhibit 7.57 Estimated Capability Values from Simulation

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
291
well controlled in the current process (Exhibit 7.51). You suspect that, if the
variation in Anodize Temp can be reduced, then conformance to speciﬁcations
will improve, particularly for L*.
Your team members engage in an effort to ﬁnd an affordable temperature
control system for the anodize bath. They ﬁnd a system that will virtually elimi-
nate variation in the bath temperature during production runs. Before initiating
the purchasing process, the team asks you to estimate the expected process
capability if they were to control temperature with this new system.
Conservative estimates indicate that the new control system will cut the
standard deviation of Anodize Temp in half, from 1.50 to 0.75. To explore
the effect of this change, return to Anodize_CustomDesign_Results.jmp
and rerun the script Profiler 4. Change the standard deviation for Anodize
Temp at the bottom of the Prediction Proﬁler panel to 0.75. The saved script is
Profiler 5.
Now construct a new simulated data table, and again use the Process
Capability platform to obtain capability analyses. The Overall Sigma Capa-
bility Summary Report is shown in Exhibit 7.58. Again, since these values
are based on a random simulation, the values you obtain may differ slightly.
To obtain the same results as in Exhibit 7.58, use the data table Anodize_
CustomDesign_Simulation2.jmp. The saved script is Process Capability.
The new capability analyses indicate that Thickness, a*, and b* have
extremely high capability values and very low PPM defect rates. Most impor-
tantly, the PPM rate for L* has dropped dramatically, and it now has a Ppk
value of about 1.074.
The team runs some additional conﬁrmation runs at the optimal settings,
exerting tight control of Anodize Temp. Everyone is thrilled when these all
result in 100 percent yield! You wonder if perhaps the speciﬁcation limits for
L* could be widened, without negatively impacting the yield. You make a note
to launch a follow-up project to investigate this further.
At this point, the team is ready to recommend purchase of the new tem-
perature control system and to begin operating at the settings identiﬁed in the
optimization. You guide the team in preparing an implementation plan. The
team, with your support, reports its ﬁndings to a management team. Based on
Exhibit 7.58 Estimated Capability Values Based on Reduction of Anodize Temp Standard Deviation

292
V I S U A L S I X S I G M A
your rigorous approach, and more importantly the projected capability ﬁgures,
management accepts your recommendations and instructs the team to imple-
ment their solution. With this, the project enters the Control Phase.
UTILIZING KNOWLEDGE
In a formal DMAIC project, the utilization of knowledge begins in the Improve
Phase and continues into the Control Phase. As part of its Control Phase activ-
ities, the team prepares a comprehensive control plan for the anodize process.
The plan includes speciﬁcation of the optimum settings for the ﬁve Xs, as well
as the new protocol for controlling the variation of these variables. The control
plan also speciﬁes the use of statistical process control to monitor the Xs, the
four Ys, and the project KPI, process yield. Recall that the project goal was to
improve the anodize process yield from 19 percent to a minimum of 90 percent,
and to sustain that improvement.
About four months after the new process settings and controls are imple-
mented, your team collects the associated data, including the ﬁnal yield num-
bers. You add the yield values to a data table that contains yields for the initial
60-lot baseline period (BaselineYieldAll.jmp).
You continue to use an Individual Measurement chart to monitor process
yield.
To create the Individual Measurement chart in Exhibit 7.59 (BaselineYieldAll.jmp, script is
Control Phase Control Chart):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Drag Yield to the Y drop zone.
3.
Drag Phase to the Phase drop zone.
4.
Click Done to close the control panel.
The chart is shown in Exhibit 7.59. The team is delighted! The chart shows
that the process is yielding, on average, more than 99 percent! This greatly
exceeds the team’s goal of improving daily yield to at least 90 percent.
To better see the Control phase detail, you select Phase as a By variable in
Control Chart Builder, rather than as a Phase variable. Exhibit 7.60 shows the
resulting control charts (the script is Control Charts by Phase). The process is
consistently yielding at least 96 percent.

Exhibit 7.59 Before and After Control Charts for Yield
293

294
V I S U A L S I X S I G M A
To create the Individual Measurement charts in Exhibit 7.60 (BaselineYieldAll.jmp, script is
Control Charts by Phase):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Drag Yield to the Y drop zone.
3.
Select Phase, and click the By button at the top right of the control panel.
This will produce two separate control charts.
4.
Click Done to close each control panel.
Tip: To save scripts for both control charts to the data table, click on either red triangle and select
Script > Save Script for All Objects to Data Table.
Exhibit 7.60 Before and After Control Charts Plotted Separately
At this point, the project is deemed a success. Prior to celebrating and dis-
banding, the team members transition the process monitoring responsibility to
the production manager, who will ensure that the process continues to perform
at this new, high, level. You and the team also document what was learned
and make recommendations for future improvement projects relating to this
process.

I M P R O V I N G T H E Q U A L I T Y O F A N O D I Z E D P A R T S
295
CONCLUSION
Using this case study, let us review how the Visual Six Sigma Data Analysis Pro-
cess aligns with the DMAIC framework, and how the Visual Six Sigma Roadmap
was used to make progress quickly:
◾
Framing the Problem occurred in the Deﬁne phase.
◾
Collecting Data began in the Measure phase, where the team collected
data for its MSA studies and for the baseline control chart. Also, the
team collected a set of historical data relating Color Rating, the team’s
primary, but nominal, Y, to four continuous Ys, namely Thickness, L*,
a*, and b*, that were thought to provide more detailed information than
Color Rating itself.
◾
Uncovering Relationships was the goal of the Analyze phase. The team
members ﬁrst visualized the ﬁve Ys one at a time using Distribution, also
using dynamic linking to start to explore conditional distributions. Then,
they dynamically visualized the variables two at a time with a Scatter-
plot Matrix. Finally, they dynamically visualized the variables more than
two at a time using Scatterplot 3D. From the relationships that they
uncovered, they were able to deﬁne speciﬁcation limits for Thickness,
L*, a*, and b* that corresponded to nondefective Normal Black parts.
◾
Modeling Relationships occurred in the Analyze and Improve phases. Here,
the team studied ﬁve potential Hot Xs for the four continuous Ys. A cus-
tomized experiment that allowed the team to identify which Hot Xs to
include in each of the four signal or transfer functions was designed
and conducted. The resulting models were visualized using the Predic-
tion Profiler. New settings for the Hot Xs were identiﬁed that would
simultaneously optimize all four Ys,
◾
Revising Knowledge also occurred as part of the Improve phase. Conﬁrma-
tion runs were obtained to provide some assurance that operating at the
new optimal settings was likely to deliver the expected results. Finally,
the JMP Simulator was used to visualize the impact that variation about
these optimal settings would have on the Ys.
◾
Utilizing Knowledge was the goal of both the Improve and Control phases.
Here, the knowledge developed by the team was institutionalized as the
new way of running the process.
This case study shows how a Six Sigma team used exploratory visualization
and conﬁrmatory methods to solve a challenging industrial problem. The team’s
efforts resulted in a multimillion-dollar cost reduction for Components Inc.

296
V I S U A L S I X S I G M A
In addition, the elimination of rework resulted in signiﬁcantly increased capac-
ity in the anodize process. Components Inc. was able to use this newfound
capacity to accommodate the increased demand for the parts that resulted from
the dramatic improvements in quality and on-time delivery.
Our case study demonstrates how the dynamic visualization, analytic, and
simulation capabilities of JMP played a prominent role in uncovering and mod-
eling relationships that led to the resolution of a tough problem. Without these
capabilities, and the Visual Six Sigma Roadmap to guide them, the team would
have faced a much longer and more difﬁcult path trying to ﬁnd a workable
solution.
NOTES
1. Robert D. Zaciewski and Lou Németh, “The Multi-Vari Chart: An Underutilized Quality Tool,”
Quality Progress 28, no. 10 (1995): 81–83.
2. For more detail on optimality criteria, see www.jmp.com/support/help/Design_of_Experiments_
Guide.shtml.
3. For more detail on effect heredity, hierarchy, and sparsity, see www.jmp.com/support/help/
Design_of_Experiments_Guide.shtml.

C H A P T E R 8
Informing
Pharmaceutical Sales
and Marketing
297
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

298
V I S U A L S I X S I G M A
S
ix Sigma is often positioned as a project-based approach. Certainly, using
identiﬁable and well-deﬁned projects aligns well with approaches to man-
aging change, and change management can be an important aspect in
spreading the use of data within a company. But, insofar as Visual Six Sigma
makes data analysis lean, it has an important role to play both within companies
whose operations and organizations do not (or cannot) support the traditional
project infrastructure, as well as for projects that, by their nature, do not require
a formal project structure. This scenario looks at one such “project” set in one
such company.
You have recently been hired as the sales manager for PharmaInc and
are now responsible for their U.K. operations. PharmaInc markets drugs that
address several therapeutic areas. Although you have a lot of sales experience,
as a new hire you are still learning about how PharmaInc’s own sales operations
run and perform.
Executive management perceives that PharmaInc has lost ground recently.
Consequently, you have been asked to look into the performance of your sales
force. Speciﬁcally, you have been asked to:
◾Verify the claims of the marketing group that a limited promotion they
ran for PharmaInc’s major product from May through December of 2014
was very successful.
◾Determine whether there are regional differences in the sales perfor-
mance of PharmaInc’s major product.
In addition, you see this as an opportunity to learn more about what actually
happens in the ﬁeld.
To answer the questions posed above, you download company data on the
monthly performance of sales representatives for May through December of
2014. The data table is fairly large, containing 95,224 rows and 16 columns. The
data consist of a number of nominal demographic variables as well as a few con-
tinuous and ordinal variables. One of the variables, the number of prescriptions
for PharmaInc’s product, is the major Y of interest.
Your data is typical of large observational data sets. As such, you ﬁrst spend
some time assessing and dealing with the quality of the data. You move on to
obtaining a better understanding of the deployment of your sales force using a
bubble plot to display the location of physician’s practices and see who has been
selling to them. In answering the speciﬁc questions posed, you use summary
tables, oneway analyses with comparison circles, regression plots, and even an
animated bubble plot. (See Exhibit 8.1 for a full list of the JMP Platforms and
Options you will encounter in this Chapter.)
After the investigation, you ﬁnd that the 2014 marketing promotion was,
indeed, successful, and you are able to identify some regional differences in sales
effectiveness. You also uncover what seems to be an uncanny and suspicious

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
299
Exhibit 8.1 Platforms and Options in This Case Study
Menus
Platforms and Options
Tables
Summary
Missing Data Pattern
Rows
Color or Mark by Column
Cols
Column Info
Column Properties
Formula
Hide/Unhide
Exclude/Unexclude
Columns Viewer
Group Columns
Analyze
Distribution
Histogram
Frequency Distribution
Fit Y by X
Bivariate Fit
Fit Line
Fit Polynomial
Oneway
Box Plots
Compare Means
Tabulate
Fit Model
Standard Least Squares
Random Effects (REML)
Quality and Process
Control Chart Builder
Process Capability Analysis
Graph
Graph Builder
Bubble Plot
Other
Local Data Filter
adherence to an unstated operational rule that each physician should be visited
once each month.
The upshot of your single-handed analysis is that you can easily answer
the questions posed by your executive, using results and arguments supported
by data and informed by visual displays that make it easy to get you r points
across. In addition, you uncover a number of interesting facts that will help you
to manage and deploy your sales force more effectively in the future.
Had you simply used a spreadsheet, by far the most common tool used
for such ad hoc analyses, you would not have arrived at your conclusions so

300
V I S U A L S I X S I G M A
quickly, if at all. You certainly would have struggled to construct informative
graphical displays and to apply appropriate statistical techniques, and it is very
likely that you would not have noticed the one-visit-per-month working prac-
tice. In short, it is probable that you would not have gained the useful new
insights that you obtained from your quick visual analysis. It would also be
harder to reproduce a similar analysis at a later date.
The platforms and options used by you and your team are listed in
Exhibit 8.1. The data sets are available at http://support.sas.com/visualsixsigma.
SETTING THE SCENE
You are the sales manager for PharmaInc, responsible for their U.K. operations.
PharmaInc markets drugs that address several therapeutic areas, competing for
market share and revenue with other companies doing the same thing. You
have only been working for PharmaInc for three months, having previously
been a senior account executive with a larger company. Although, as an industry
veteran,youareveryfamiliarwiththegenerallandscape,youhaveyettocometo
terms with the nuances of exactly how PharmaInc’s operations run and perform.
Even though the markets view PharmaInc as generally doing well, execu-
tive management thinks that the company has lost ground over the last couple
of years. So, early in 2015, you are chartered to look into the recent perfor-
mance of your sales force to shed some light on this perceived decline.
More speciﬁcally, your manager, the vice president (VP) of sales and mar-
keting, has asked you to:
◾Verify the claims of the marketing group that a limited promotion they
ran for PharmaInc’s major product from May to December of 2014 was
very successful.
◾Determine whether there are regional differences in the sales perfor-
mance of PharmaInc’s major product.
You also see this investigation as a chance to get better acquainted with exactly
what happens in the ﬁeld, and why.
Outside of work, you are a keen golfer, and regularly play against Arthur,
an engineering manager in a high-tech manufacturing company. While you
were in your previous position, Arthur had convinced you to use JMP for data
analysis. Knowing how visually oriented JMP is, you see that it will be very
useful in exploring the data relating to the task at hand. You also know that, if
you need help, you can count on your golﬁng buddy to give you guidance!
COLLECTING THE DATA
Each PharmaInc sales rep works his or her own territory, which is an agglom-
eration of U.K. postal codes lying wholly within a speciﬁc region of the United
Kingdom. A given territory contains physicians working in practices, and each

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
301
physician has a particular medical specialty. The physicians write prescriptions
for patients who visit their practices, aiming to provide them with the best pos-
sible care. A sales rep will periodically make calls on a prescribing physician to
promote the use of PharmaInc’s products, and reps from competing companies
do the same thing. During a visit, if a promotion is running, the sales rep may
leave behind a free promotional sample kit.
You download company data on the performance of the sales reps for May
through December of 2014 into a Microsoft® Excel spreadsheet. You realize that
there has likely been some turnover in the sales force in 2014, but that this was
probably minor. For your purposes, you decide that you will only extract data
about sales reps who worked for the entire eight-month period.
You easily import your Excel data into a single JMP table called Phar-
maSales_RawData.jmp. The resulting data table contains 95,224 rows and
16 columns. A partial view of the data table is given in Exhibit 8.2.
The columns are listed and described in Exhibit 8.3. Note that there are four
pairs of ID and corresponding Name columns. The ID columns are a coding of
the Name columns so that one of each pair is redundant.
As indicated in Exhibit 8.3, each row in Exhibit 8.2 provides the number of
events in a particular month (visits made, visits when a sample kit is left, prescrip-
tions written). Each record or row is uniquely deﬁned by the combined values
of Date, Salesrep Name, and Physician Name. Note that we assume that each
sales rep has more than enough sample kits, so if a kit is not left during a visit,
it is not because they were out of supply.
Exhibit 8.2 Partial View of PharmaSales_RawData.jmp

302
V I S U A L S I X S I G M A
Exhibit 8.3 Column Descriptions for PharmaSales_RawData.jmp
Column Name
Description
Date
Month and year of current record
SalesrepID
Sales representative identiﬁer
Salesrep Name
Name of the sales representative
RegionID
Region identiﬁer
Region Name
Name of the region
PhysicianID
Physician identiﬁer
Physician Name
Name of the prescribing physician
Physician Specialty
Specialty of physician
PracticeID
Practice identiﬁer
Practice Name
Name of the practice
Postcode
Location (postal code) of the practice
Practice Latitude
Latitude of the practice
Practice Longitude
Longitude of the practice
Visits
Number of visits made by the sales rep to this physician this month
Visits with Samples
Number of visits during which a promotional sample kit is left by the sales repre-
sentative with this physician this month
Prescriptions
Number of prescriptions for PharmaInc’s product written by this physician this
month
Recall that we use Ys to represent responses and Xs to represent variables
that might impact the Ys. Prescriptions is the main Y variable of interest. Visits
and Visits with Samples may be considered as input or X variables (because
they are under the control of the sales reps and are expected to have an impact
on Prescriptions) or as Y variables (since they are the outcomes of past choices
made by the sales reps). Aside from Practice Latitude and Practice Longitude,
which will be used for mapping purposes, the other variables are either ancil-
lary variables, or descriptive (stratifying) variables that relate to how your sales
system is currently conﬁgured and operated and the environment in which it
functions.
VALIDATING AND SCOPING THE DATA
Before starting to understand how the data can help answer the questions raised
by your VP, your ﬁrst goal is to get a feel for what is actually in the data.
Questions such as the following come to mind:
◾How many sales reps are there and where do they operate?
◾How many practices are there?
◾How many physicians are in each practice?

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
303
In your initial review of the data, you also want to look at the general quality
of the data, since you have seen problems with this in the past and are reminded
of the old saying, “Garbage in, garbage out.”
Preparing the Data Table
You decide to arrange the data table in a way that will facilitate your data anal-
ysis. You will document and rearrange your data table and save it in a new ﬁle
called PharmaSales.jmp.
The steps described in this section illustrate some features of JMP that are
useful for documenting and streamlining your work. For a quick-and-dirty
analysis, the steps in this section are unnecessary. But when you are asked
to conduct a similar analysis a year from now, you will see the value of
having documented your work. That said, we leave you to judge whether
to follow this section. Feel free to skip ahead to “Dynamic Visualization of
Variables One and Two at a Time.” Otherwise, to follow along, open the ﬁle
PharmaSales_RawData.jmp.
Defining the Notes Column Property
First, to document the data, you insert a Notes property in each column describ-
ing the contents of that column using the information in Exhibit 8.3.
To insert the column descriptions given in Exhibit 8.3 as column notes
(PharmaSales_RawData.jmp, script is Add Column Notes):
1.
Click the heading of a column and select Cols > Column Info.
2.
Choose Notes under the Column Properties menu.
3.
Enter the required notes in the text edit box and click OK.
Tip: For Step 1 you can also right-click on a column heading and choose Column Info. As well
as adding notes to speciﬁc columns, you can also add notes to the table itself by creating a New
Table Variable in the Table panel.
Excluding and Hiding Columns
Next, you remember that the four ID columns are redundant. You do not want
to delete them, so you hide them so that they do not appear in the data table grid
and exclude them so they don’t appear in variable selection lists for analyses.

304
V I S U A L S I X S I G M A
To hide and exclude columns (PharmaSales_RawData.jmp, script is Hide and
Exclude ID Columns):
1.
Select the headers for the four ID columns, holding down the Control key (or the
Command key on a Mac) to extend the selection.
Alternatively, you can also select the columns in the Columns panel. Once a
column has been hidden you have to select it in the Columns panel since it no longer
appears in the data table grid. Steps 2 and 4 can also be accomplished with a right-click
on a column header or in the Columns panel.
2.
Select Cols > Hide/Unhide.
3.
Reselect the four column headings.
4.
Select Cols > Exclude/Unexclude.
For convenience, you group the four ID columns into a new IDs column
group and move this to the bottom of the Columns panel.
To group columns, as shown in Exhibit 8.4 (PharmaSales_RawData.jmp, script is
Group ID Columns):
1.
Select the four hidden and excluded columns in the Columns panel.
2.
Select Cols > Group Columns.
You can also select Group Columns via a right-click in the Columns panel.
3.
Click on the name of the new column group (SalesrepID, etc.) in the Columns
panel, and then click a second time to rename it IDs.
4.
Drag the column group to the bottom of the list in the Columns panel.
Exhibit 8.4 is a partial view of the data table after deﬁning the IDs column
group.
Saving the Data Table
Finally, save the data table with the new name PharmaSales.jmp (select File
> Save As, enter the desired table name, and press Save), or close it without
saving and open the same table from the Journal.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
305
Exhibit 8.4 Partial View of PharmaSales_RawData.jmp with IDs Column Group
Dynamic Visualization of Variables One and Two at a Time
We continue our analysis with PharmaSales.jmp, the data table that you just
prepared for analysis. (You will notice that the table PharmaSales.jmp linked
to in the book’s Journal contains scripts for convenience. If you prefer to repli-
cate this work on your own, then continue using your own instance of Phar-
maSales.jmp, the data table obtained from PharmaSales_RawData.jmp,
after following the steps in the previous section, “Preparing the Data Table”.)
Using Column Viewer and Distribution
To get an idea of the complexity of the data, you ﬁrst obtain the high-level view
of all the variables in the table shown in Exhibit 8.5. Then you produce linked
histograms and bar charts of all the variables that do not have a large number
of categories. You remove the numeric summaries in Distribution to focus on
the graphical displays. (Variables with a large number of categories are difﬁcult
to assess using a bar chart representation because there is one bar per category.)
To obtain the summary statistics in Exhibit 8.5 and the Distribution report in Exhibit 8.6
(PharmaSales.jmp, script is Overview):
1.
Select Cols > Columns Viewer.
2.
Select all the columns and click Show Summary.
This displays the summary statistics for each of the variables.

306
V I S U A L S I X S I G M A
3.
Holding the Control key (or Command on the Mac), deselect the variables
Physician Name, Practice Name and Postcode in the Summary
Statistics outline node.
You do not want to see bar charts of these columns when you produce the
Distribution report, since these all have more than a thousand categories.
4.
Select Distribution to produce a new report window.
5.
Holding the Control key, click on the red triangle for Date and deselect Display
Options > Quantiles and Display Options > Summary Statistics.
Holding the Control key when making selections broadcasts the same selection to
all variables in the report with the same data type.
6.
Holding the Control key, click on the red triangle for Salesrep Name and deselect
Display Options > Frequencies.
Tip: The Distribution report, showing only histograms and bar graphs, can be produced using
the Distribution platform from the Analyze menu (check the Histograms Only box).
Exhibit 8.5 A High-Level View of the Variables in PharmaSales.jmp

Exhibit 8.6 Partial View of the Distribution Report for the Variables in PharmaSales.jmp
307

308
V I S U A L S I X S I G M A
From the information in reports partially shown in Exhibits 8.5 and 8.6,
you observe the following:
◾Date. JMP stores dates internally as the number of seconds since
January 1, 1904. Any summary statistics such as Min, Max, Mean, and
Std Dev reﬂect this internal representation and are not immediately
interpretable unless you apply an appropriate format. Nonetheless, the
histogram indicates that eight months of data are represented and that
there are roughly equal numbers of rows for each month. You also see
that there are no missing values, so each of the 95,224 rows has a value
for date.
◾Salesrep Name. There are 102 sales reps represented.
◾Region Name. There are nine regions, and Northern England and Mid-
lands have the most rows (the longest bars).
◾Physician Name. There are 11,833 physicians represented.
◾Physician Specialty. There are 15 specialties represented, and there are
368 rows for which this information is missing.
◾Practice Name. There are 1,149 practices.
◾Visits. There can be anywhere from 0 to 5 visits made to a physician in a
given month. Typically, a physician receives one or no visits. There are
975 rows for which this information is missing.
◾Visits with Samples. There can be anywhere from 0 to 5 visits with sam-
ples made to a physician in a given month. Typically, a physician receives
one or no such visits. There are 62,156 rows for which this information
is missing.
◾Postcode, Practice Latitude, Practice Longitude. There is no missing
data and the values seem to make sense.
◾Prescriptions. The distribution for the number of prescriptions for Phar-
maInc’s product written by a physician in a given month is skewed, with
a long “tail” of high values. The average number is 7.40, but the number
can range from 0 to 58.
Note that Visits and Visits with Samples have been deﬁned in the table with
Ordinal Modeling Type in anticipation of the fact that both contain count values
with relatively small counts. Even though Prescriptions also contains counts,
the Continuous Modeling Type is used. For this variable, the use of Ordinal
rather than Continuous Modeling Type would make for unwieldy displays given
the relatively large number of values.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
309
Using Local Data Filter for Descriptive Variables
Now that you have taken a preliminary look at all of data, you are ready
to study some of the key descriptive variables more carefully. These include
Date, Salesrep Name, Region Name, Practice Name, and Physician Specialty.
Given the large number of physicians, you decide not to include Physician
Name in this list.
You can think of these variables in two ways; use Region Name as an
example. First, Region Name allows you to stratify the data, namely, to view all
other variables in terms of the layers, or strata, deﬁned by this variable. Using
it as a stratiﬁcation variable allows you to look for interesting and important dif-
ferences in the data between the levels (for instance, how Prescriptions change
between Midlands and Scotland).
However, you can also think of such variables as chunking variables.1 Use of
this term emphasizes the fact that the data values arise from the outcomes of
large, indiscriminate groupings of (presumably) very speciﬁc root causes active
within each stratum. Each of these variables will require further investigation
if it turns out to be of interest. For example, an identiﬁed difference between
Midlands and Scotland could be caused by any combination of the following
more speciﬁc causes within these regions: the educational, ﬁnancial, or social
level of patients; the knowledge level or specialties of physicians; the causes of
underlying medical conditions; the hiring practices and management of sales
representatives, or their training or attitude; and so on. Most importantly, you
note that such causes are not usually represented directly in the data you have.
You will use the Local Data Filter, in conjunction with Distribution, to
investigate the other four variables.
To use the Local Data Filter with Distribution (PharmaSales.jmp, script is
Distribution for Four Chunking Variables):
1.
Select Analyze > Distribution and enter Date, Salesrep Name, Physician
Specialty, and Practice Name as Y, Columns.
2.
Select Histograms Only and click OK.
3.
Select Script > Local Data Filter from the Distributions red triangle.
4.
Select Region Name from the list of variables in the Local Data Filter and click
Add.
5.
Select Order By Count from the Region Name red triangle menu in the Local
Data Filter and select Northern England from the list of region names.

310
V I S U A L S I X S I G M A
6.
Select the bar for Ronda Guyman in the Salesrep Name barchart. See Exhibit 8.7.
7.
Now, by scrolling the report window you see, for example, the names of the 29 practices
that Ronda has visited.
Tip: The Local Data Filter can be used to ﬁlter the report results by the values of any variable or
combination of variables in the table. The Local Data Filter affects the report window it is
attached to, but you can also use Rows > Data Filter to ﬁlter multiple report windows and the
data table simultaneously.
Using the report you have generated, you can investigate the relationships
between the ﬁve chunking variables. You quickly discover, for example:
◾The names of the sales reps who work in Northern England
◾The names of the practices located in Northern England
◾The physician specialties represented in Northern England
◾The practices that handle the Internal Medicine specialty
Using the Local Data Filter for Response Variables
Having developed a feel for some of the chunking variables describing sales
operations, you now turn your attention to the monthly outcomes. You will use
the Local Data Filter to explore the three outcome variables by month.
To ﬁlter by month (PharmaSales.jmp, script is Plots for Three Outcome
Variables):
1.
Select Analyze > Distribution and enter Visits, Visits with Samples, and
Prescriptions as Y, Columns. Click OK.
2.
Select Script > Local Data Filter from the Distributions red triangle.
3.
Right-click on Date and select Date Time > Month to generate a new, temporary
variable called Month.
4.
Select Month and click Add.
5.
Select Animation from the Local Data Filter red triangle menu and click the right
arrow under Animation Controls to start the animation.
Tip: Note that Date is a continuous variable. Although you can ﬁlter by a continuous variable such
as Date, because of its many values, this is not a good choice. In this case, you construct an ordinal
variable by binning the variable Date on the ﬂy. Then you ﬁlter by this new variable to look for
month-to-month variations. Most of the column selection boxes in JMP allow you to create virtual
variables, but note that these variables are temporary unless you choose to make them permanent.
To make a virtual variable permanent, right-click on the variable and select Add to Data Table.

Exhibit 8.7 Exploring the Chunking Variables Using Distribution and Local Data Filter
311

312
V I S U A L S I X S I G M A
By interacting with the report in Exhibit 8.8 for Visits, Visits with Samples,
and Prescriptions, you easily ﬁnd that:
◾The numbers of monthly Visits by sales reps run from 0 to 5.
◾About 23 percent of the time, a physician is not visited in a given month.
About 65 percent of the time, a physician receives exactly one visit in
a given month. However, about 12 percent of the time, a physician
receives two or more visits in a given month.
◾There are 975 rows for which a value for Visits is missing.
◾There are 62,156 rows missing a value for Visits with Samples, probably
because the promotion was limited in scope.
◾For the rows where Visits with Samples was reported, about 37 percent
of the time, no sample kit was left.
◾The monthly numbers of Prescriptions written by each physician vary
from 0 to 58.
◾Generally, physicians write relatively few prescriptions for PharmaInc’s
major product each month. They write six or fewer prescriptions 50 per-
cent of the time.
Exhibit 8.8 Exploring the Three Outcomes Variables Filtering by Month

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
313
In some cases, these ﬁndings are simply conﬁrmation of what you discov-
ered earlier. By adding the Local Data Filter, you can also gain a ﬁrst impression
of whether, and how, outcomes have changed over time. The updating report
conﬁrms to you that there is no large month-to-month variation in any of the
outcomes.
Using Graph Builder
You can also conﬁrm that there is no large month-to-month variation in
the outcomes using the more conventional Graph Builder representation in
Exhibit 8.9.
To produce Exhibit 8.9 (PharmaSales.jmp, script is Plots for Three Outcomes
Variables by Month):
1.
Select Graph > Graph Builder.
2.
Right-click on Date and select Date Time > Month to construct a new temporary
variable Month in the list of columns.
3.
Drag Month to the X drop zone.
4.
Drag Prescriptions to the Y drop zone.
5.
Drag Visits to the Y drop zone above Prescriptions.
6.
Right-click in the graphics frame for Visits and select Bar > Change To > Points.
7.
Follow steps similar to 4, 5, and 6 to include Visits with Samples.
Using the Local Data Filter to Find Relationships
To take a ﬁrst look for any relationships between the ﬁve chunking variables
and three outcomes, you combine Exhibits 8.7 and 8.8 into a single display and
use dynamic linking. Given that Date (or Month) does not have a large effect,
you choose to ﬁlter the report by Region Name, to produce Exhibit 8.10.
By clicking on the bars in the report and using the Local Data Filter, you
can see, for instance, whether certain regions are associated with larger num-
bers of visits than others, or whether certain physician specialties tend to write
more prescriptions. You note that Midlands is associated with a large proportion
of Visits with Samples and that it has a bimodal distribution in terms of Pre-
scriptions. But you do not see any other convincing relationships at this point
and realize that you may need to aggregate the data in some way to better see
relationships.

314
V I S U A L S I X S I G M A
Exhibit 8.9 Graph Builder Plot for Three Outcomes by Month
First, however, you want to get a better sense of the quality of the data.
Then you will move on to your private agenda, namely, to try to understand
how your sales force is deployed.
Missing Data Analysis
Earlier you noted that some variables have a signiﬁcant number of missing
values. You would like to get a better understanding of this phenomenon.
Exhibit 8.11 is obtained using Tables > Missing Data Pattern and selecting
all the columns (saved script is Missing Data Pattern).
In the Missing Data Pattern table, consider the columns from Date to Pre-
scriptions. Each of these columns contains the values 0 or 1, with a 0 indicating
no missing data and a 1 indicating missing data. You observe that the Patterns
column is a 12-digit string of 0s and 1s formed by concatenating the entries of
these columns. A 0 or 1 is used to indicate if there are missing data values in
the column corresponding to that digit’s place in the string.
The table has eight rows, reﬂecting the fact that there are eight distinct
missing data patterns. For example, row 1 of the Count column indicates that
there are 32,917 rows that are not missing any data on any of the 12 variables.
Row 2 of the Count column indicates that there are 60,967 rows where only the
11th variable, Visits with Samples, is missing. Row 3 indicates that there are 347
rows where only the two variables Visits and Visits with Samples are missing.

Exhibit 8.10 Exploring Relationships between Region and Outcome Variables Using Distribution and Local Data Filter
315

316
V I S U A L S I X S I G M A
Exhibit 8.11 Partial View of Missing Data Pattern Table
You notice the saved scripts Treemap and Cell Plot in the Missing Data
Pattern table, but decide not to run them because the main message is already
clear: You need to ﬁnd out if data on Visits with Samples was only entered for
those locations where the promotion was run.
Before picking up the phone, you decide to see if the data offer any further
clues about this issue. Go to the data table PharmaSales.jmp and run Analyze
> Distribution on Region Name. In the Missing Data Pattern data table,
select rows 1 and 5. These are the rows where Visits with Samples is not missing.
Back in the Distribution plot, you see that Visits with Samples is not missing
for only three regions: Southern England, Northern Ireland, and Midlands.
You call the VP of sales and marketing, whose administrative assistant
researches the question. Within a few hours, you receive a phone call conﬁrm-
ing that the promotion was indeed run in only the three regions you identiﬁed.
To record this in the data table you make a new column in PharmaSales.jmp
called Promotion?.
To construct the Promotion? column (PharmaSales.jmp, script is Add Promotion
Column):
1.
Select Analyze > Distribution.
2.
Enter Region Name as Y, Columns and click OK.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
317
3.
Using the Control key, select the bars corresponding to Southern England, Northern
Ireland, and Midlands.
4.
Select Rows > Row Selection > Name Selection in Column.
5.
Enter Promotion? as the column name and click OK.
6.
Double-click on the column heading of Promotion?
7.
Select Value Labels from the Column Properties list.
8.
Enter the Value 0 with the Label No, and click Add. Then enter the Value 1 and the
Label Yes and click Add. Click OK.
Tip: Note that, when the bars you want to select are small, it may be easier to select the required
categories in the Frequencies report. Additionally (for steps 5 to 8) you can alternatively enter
Yes and No directly in the Name Selection in Column dialog, though this will produce a
new column that is Character rather than Numeric.
Further, the IT associate who was responsible for loading data from the
promotion conﬁrms that, for those six regions where the promotion was not
run, the Visits with Samples ﬁeld was populated with missing values. You ask
her to modify the data recording protocol so that, in the future, such values are
logged as zero rather than missing. She also conﬁrms that, in the three regions
where the promotion was run, missing values appear when the sales reps did
not report whether a sample kit was left on a given visit. Given what you have
seen so far, you think this was a relatively rare occurrence, but decide to conﬁrm
this with a summary table (Exhibit 8.12).
To produce Exhibit 8.12 (PharmaSales.jmp, script is Missing Values for Visits
With Samples):
1.
In PharmaSales.jmp, select Tables > Summary.
2.
Select Visits with Samples from the Select Columns list and N Missing from
the Statistics list.
3.
Enter Region Name and Promotion? as Group variables. Click OK.
You feel much better upon seeing this result, since the largest number of
missing values in any of the three regions where the promotion was run is
only 218.
It’s time for a little housekeeping. You select Window > Close All Reports
to close any reports that are open, and close all open data tables other than

318
V I S U A L S I X S I G M A
Exhibit 8.12 Missing Values of Visits with Samples by Region Name
PharmaSales.jmp. Finally, you deselect any selected rows by selecting
Rows > Clear Row States (or click in the lower triangle at the upper left of
the data grid).
UNCOVERING RELATIONSHIPS
Dynamic Visualization of Sales Reps and Practices
Geographically
From the Distribution plots you have seen, you know that most of the sales
rep activity is in Northern England and the Midlands. You are curious to see a
geographical picture showing the practices being called on by your sales reps,
and how these are grouped into regions. Thinking about the fact that sales reps
serve several practices, and that each practice is only served by a single sales rep,
you realize that a bubble plot may be a useful way to show this information,
since it allows you to view hierarchical data (Exhibit 8.13).
To produce the plot shown in Exhibit 8.13 (PharmaSales.jmp, script is Which
Salesreps Serve which Practice Name):
1.
Select Graph > Bubble Plot.
2.
Enter Practice Latitude as Y.
3.
Enter Practice Longitude as X.
4.
Enter Salesrep Name, then Practice Name as ID.
5.
Enter Region Name as Coloring and click OK.
6.
Right-click on the graph, then select Background Map.
7.
Select Street Map Service and click OK.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
319
Tip: If you want to see how one of the outcomes varies geographically, follow the steps above but
add the required variable to the Sizes role. You may have to use the Bubble Size slider to
resize all the bubbles. Note also that you can use Tools > Magnifier and drag a selection of the
map to zoom in.
Exhibit 8.13 shows you that Yvonne Taketa has a territory that appears to
be rather wet. Select the bubble corresponding to Yvonne Taketa and press the
Split button. This reveals all of her practices, which are indeed all on dry land!
Splitting bubbles in this way allows you to drill down from the summary level
data (Salesrep Name) to the detailed level data (Practice Name), so that you
gain insights at both levels of the data hierarchy. You can see, for instance,
that Lawrence Goetsche (in Wales) has some far-ﬂung practices, so he probably
enjoys driving.
Exhibit 8.13 Sales Reps and Practices (Partial View)

320
V I S U A L S I X S I G M A
Dynamic Visualization of Prescriptions with a Tabular Display
At this point, you decide to create a listing to show which physicians are pre-
scribing PharmaInc’s major product. Because a static list will be rather long and
unmanageable, you use the Local Data Filter in conjunction with Tabulate to
produce Exhibit 8.14.
To produce Exhibit 8.14 (PharmaSales.jmp, script is Prescriptions by Physicians
at Practices):
1.
Select Analyze > Tabulate.
2.
Drag Physician Name to the Drop zone for rows.
3.
Drag Practice Name just to the left of the heading for Physician Name.
4.
Drag Region Name just to the left of the heading for Practice Name.
5.
Drag Prescriptions on top of N in the rightmost column.
6.
Click Done.
7.
Select Script > Local Data Filter from the Tabulate red triangle.
8.
Select Salesrep Name and click Add.
9.
From the red triangle next to Salesrep Name, select Order by Count.
10.
Make selections from the values of Salesrep Name shown in the list, or use
animation to cycle through these values.
Tip: In a general situation, you can build up complex ﬁltering criteria using the And and Or
buttons in Local Data Filter. Once you have deﬁned these selections, you can save and recall
these settings via the Favorites list in the Local Data Filter.
The resulting table is partially shown in Exhibit 8.14. Scroll through the list
of salesrep names to see that there are some sales reps who call on just a very
few physicians, whereas some are calling on very many. You assume this is due
to some sales reps working part time, and make a note to check this with the
HR Department.
At this point, you conclude that you have a clean set of data and a
good grasp of its content. You also feel that you have a better idea of how
your sales force is deployed. Using JMP’s visual and dynamic capabilities has
been very helpful in this pursuit, and you have very quickly learned things
you could never have learned using a spreadsheet. Now, ﬁnally, you are
ready to roll up your sleeves and to address the business questions posed by
your Executive.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
321
Exhibit 8.14 Prescriptions by Physicians at Practices, Filtered by Salesrep Name (Partial View)
INVESTIGATING PROMOTIONAL ACTIVITY
Your ﬁrst task is to verify the marketing group’s claims that the test promotion
they ran in 2014 was successful. You know that the promotion was run in three
regions, Midlands, Southern England, and Northern Ireland, and it was run for
the full year of 2014.
Your ﬁrst thought is to look at total prescriptions written per physician
across the eight months for which you have data to see if the three promotion
regions stand out. To accomplish this, you must ﬁrst construct a summary table
giving the sum for relevant variables over the eight-month period. You then
intend to compare regions in terms of the 2014 physician totals, taking into
account physician variability. But you are also interested in how prescription
totals vary by region when considering the number of visits by region. Do the
promotional regions stand out from the rest in terms of prescriptions written if
we account for the numbers of visits per physician?

322
V I S U A L S I X S I G M A
Preparing a Summary Table
Given your discovery that Region Name is one of the key chunking variables,
you decide to color and mark rows in PharmaSales.jmp by this variable so
that any further displays that show rows as points will color and mark these
points appropriately. You do this by selecting Rows > Color or Mark by Col-
umn (saved script is Color and Mark by Region Name).
You construct the Summary table shown in Exhibit 8.15 and note that the
row colors and markers are inherited from PharmaSales.jmp.
To produce Exhibit 8.15 (PharmaSales.jmp, script is Summary Table 1):
1.
Select Tables > Summary.
2.
Select the three outcome variables and Sum from the Statistics list.
3.
Enter Physician Name, then Region Name, then Salesrep Name as Group
variables (in this order).
4.
Click OK.
Tip: The number of rows in the summary table is the same as the number of physicians (11,833), as
seen in Exhibit 8.5. Including the additional grouping variables in step 3 does not change the
number of rows due to the structure of the data (each physician name is only associated with a
single sales rep in a single region). But it does include these variables as additional columns in the
summary table, which is useful.
Exhibit 8.15 PharmaSales By (Physician Name, Region Name, Salesrep Name)

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
323
Exhibit 8.16 Oneway Plot of Sum(Prescriptions) against Region Name
Uncovering Relationships: Prescriptions versus Region with Fit Y by X
Your ﬁrst thought is to use Fit Y by X to investigate the relationship between
Sum(Prescriptions) and Region Name to see if the promotion regions had sig-
niﬁcantly more prescriptions written than did the non-promotion regions (see
Exhibit 8.16).
To produce Exhibit 8.16 (PharmaSales By (Physician Name, Region Name,
Salesrep Name), script is Oneway):
1.
Select Analyze > Fit Y by X.
2.
Enter Sum(Prescriptions) as Y, Response.
3.
Enter Region Name as X, Factor and select OK.
4.
Deselect Display Options > X Axis Proportional in the Oneway red
triangle menu.
5.
Select Display Options > Points Jittered from the Oneway red
triangle menu.
Jittering the points avoids overplotting, which is useful in this case since there are
11,833 points to display.
6.
Select Display Options > Boxplot from the Oneway red triangle menu.
7.
Select Compare Means > All Pairs, Tukey HSD from the Oneway red
triangle menu.

324
V I S U A L S I X S I G M A
You note that that Midlands and Northern England have comparatively
large numbers of rows. But remember that Southern England, Midlands, and
Northern Ireland are of particular interest because this is where the promotion
was run. Looking at the report again, it does appear to you that more prescrip-
tions were indeed being written in these three regions!
You think about how you can verify this ﬁnding more formally. Given that
there are nine different regions, you decide that the All Pairs, Tukey HSD
(Honestly Signiﬁcant Difference) test should be used rather than Each Pair,
Student’s t because there will be pairwise comparisons of nine regions. There
are (9 × 8) / 2 = 36 possible pairs to compare. You want to control the overall
false alarm rate for the set of 36 tests to 0.05. If you had chosen the Each Pair,
Student’s t option, each individual comparison would have a 0.05 false alarm
rate. Over 36 tests, these individual false alarm rates could combine to a very
large overall false alarm rate (about 1 – (1 – 0.05)36 = 0.84, or 84 percent). Note
that JMP refers to the false alarm rate as error rate. Using All Pairs, Tukey HSD
also produces the comparison circles as an aid to the interpretation of the test
results (the rightmost panel in Exhibit 8.16).
You remind yourself of how the comparison circles work. There is one circle
for each value of the nominal variable (so here, there is one circle for each
region). Each circle is centered vertically at the mean for the category to which
it corresponds. When you click on one of the circles, it turns a bold red, as
does its corresponding label on the x-axis. Each other circle either turns bold
gray or normal red (but not bold red). The circles that turn gray correspond
to categories that signiﬁcantly differ from the category that corresponds to the
chosen, bold red, circle. The other circles that turn red correspond to categories
that do not signiﬁcantly differ from the chosen category.
With this in mind, you click on the tiny topmost circle. You see, from the
bold red label on the graph, that this circle corresponds to Midlands. (You
infer that it is tiny because Midlands has so many observations.) Once that
circle is selected, you see that all of the remaining circles (and graph labels)
are gray. This means that Midlands differs signiﬁcantly from all of the other
regions. Technically stated, each of the eight pairwise tests comparing the mean
of Sum(Prescriptions) for Midlands to the mean Sum(Prescriptions) for the
other regions is signiﬁcant using the Tukey procedure.
Next, you click on the big circle that is second from the top, shown in
Exhibit 8.16. You are not surprised that this corresponds to Northern Ireland—
the circle is large because there are so little data for Northern Ireland, compared
with the other regions. All the other circles turn gray except for the very small
circle contained within, but near the top of, Northern Ireland’s circle.
That small circle corresponds to Southern England. This is easy to see on a
computer monitor, where the label for Southern England is red on the graph.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
325
With gray-scale printed output, one must look carefully at the labels on the
graph. Note that the label for Northern Ireland is bold. Labels that correspond
to gray circles (signiﬁcantly different from the bold label) are italicized. Those
that do not differ signiﬁcantly from the bold label are not italicized. Note that
the label for Southern England is the only label, other than Northern Ireland’s,
that is not italicized.
You look around in the output under the Means Comparisons heading,
and discover a table that summarizes the signiﬁcant differences among regions
(see Exhibit 8.17, where the table of interest is enclosed in a rectangle). You see
that this table divides the nine regions into six groups, based on an associated
letter, and that these groups differ signiﬁcantly from the others. Midlands (letter
A) has signiﬁcantly more sales than all of the other groups. Next come South-
ern England and Northern Ireland, both of which are associated with the letter
B and so can’t be distinguished statistically; however, both of these have signif-
icantly more sales than the regions associated with the letters C, D, E, and F.
The smallest numbers of sales are associated with Wales and Northern England
(letter F).
You are excited about these results! You have learned that the three promo-
tional regions differ signiﬁcantly from the six nonpromotional regions in terms
of total prescriptions written over the eight months. You have also learned that
Midlands had signiﬁcantly more prescriptions written than did Northern Ireland
or Southern England, even though they all ran promotions. You are intrigued
by why this would be the case and make a note to follow up at your next
meeting with the sales reps.
Exhibit 8.17 Signiﬁcant Differences Summary Table for Sum(Prescriptions) by Region Name

326
V I S U A L S I X S I G M A
You also note that there are signiﬁcant differences in the nonpromotional
regions as well. Northern England and Wales have the smallest numbers of
prescriptions written, and you want to understand why this is the case. But,
you keep ﬁrmly in mind that many factors could be driving such differences. It
is all too easy to think that the sales reps in these two regions are not working
hard or smart enough. But, for example, there could be cultural differences
among patients and physicians, making them less likely to request or prescribe
medications. There could be age, experience, or specialty differences among the
physicians. You realize that many causal factors could be driving these regional
differences.
Uncovering Relationships: Prescriptions versus Region with Fit Model
At your next golf outing, you discuss these results with your golﬁng buddy,
Arthur. He indicates that there could be a problem with conducting these sta-
tistical tests on the data in your summary table. Each row represents the total
number of prescriptions written by a given physician. But the number of pre-
scriptions written by a given physician might (you hope!) be inﬂuenced by the
physician’s sales rep. This means that the values of Sum(Prescriptions) in each
row may not be independent, as required by the Means Comparisons test that
you utilized. Rather, they are potentially correlated: Sum(Prescriptions) within
a sales rep are likely to be more similar than Sum(Prescriptions) between sales
reps. In fact, your buddy points out that, in technical terms, the variable Physi-
cian Name deﬁnes a subsample of Salesrep Name.
Arthur tells you that there are two options if you want to be sure that the
assumptions behind your statistical approach are more appropriate for these
data: Summarize the data over sales reps and then utilize comparison circles
as before, or construct a model that accounts for the subsampling. You are
intrigued by this second approach and so ask Arthur to show you how to do this.
Arthur reminds you that because a given sales rep only works in one
region, his or her effect on another region cannot be assessed. He points
out that because the sales reps are nested within region in this way, only the
variability contributed by sales reps can be estimated in a statistical model.
Arthur explains to you that one of the many things Fit Model can do is ﬁt
these so-called random effects models. He completes the Fit Model dialog for you
as shown in Exhibit 8.18. After some thought, you realize that completing the
dialog this way will mean that the individual observations for Physician Name
will be treated as the data values for each sales rep. In other words, the role
of Physician Name as deﬁning subsamples of Salesrep Name is built into this
model.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
327
Exhibit 8.18 Fit Model Dialog for Model with Subsampling
To produce Exhibit 8.18 (PharmaSales By (Physician Name, Region Name,
Salesrep Name), script is Model with Random Effect):
1.
Make sure PharmaSales By (Physician Name, Region Name,
Salesrep Name) is the active data table.
2.
Select Analyze > Fit Model.
3.
Enter Sum(Prescriptions) as Y.
4.
Select Region Name and click Add.
5.
Select Salesrep Name and click Add.
6.
Select Salesrep Name in the Model Effects box, click the Attributes red
triangle at the bottom left of the Model Effects box, and select Random Effect.
7.
Select Region Name in the column selection box on the left (not in the Model
Effects box) and click Nest.
This speciﬁes that the random effect Salesrep Name is nested within Region
Name.
Tips: Random effects will have the & Random designation to the right of them in the Model
Effects box. The Fit Model dialog can be saved by selecting Save to Data Table from the
Model Specification red triangle. If you run the Model with Random Effect script,
the resulting Fit Model dialog will separate ﬁxed effects and random effects you have deﬁned
using tabs.

328
V I S U A L S I X S I G M A
Exhibit 8.19 Fit Model Report
When you run the model, you obtain the output in Exhibit 8.19. The very
small value for Prob > F in the Fixed Effect Tests report indicates that Region
Name is signiﬁcant. The REML Variance Component Estimates report indi-
cates that Salesrep Name contributes only about 1 percent of the variation
(with a variance component of about 2.1). It follows that, within a region, most
of the variation is due to physician differences. This indicates that the earlier
conclusions you drew from your simpliﬁed analysis in Fit Y by X are actually
not misleading in this case.
Arthur points out that you can also obtain an analog of comparison circles
from the Fit Model report, shown in Exhibit 8.20.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
329
To produce Exhibit 8.20 (PharmaSales By (Physician Name, Region Name,
Salesrep Name)):
1.
Open the Effect Details outline shown in Exhibit 8.19 (at the bottom).
The Region Name report shows the Least Squares Means Table for
Sum(Prescriptions).
2.
From the red triangle menu for the Region Name report, select LSMeans Tukey
HSD.
This choice conducts pairwise comparisons based on the current model.
The test results are represented in matrix form as shown in Exhibit 8.20. The
ﬁrst row in each cell of the matrix gives the mean difference between the two
groups. You note that some differences are large. Regions with results in gray
(red on the screen) are signiﬁcantly different (with a false alarm rate of 0.05).
If the results are in bold (black on the screen), the difference is not statistically
signiﬁcant. The summary table below the matrix summarizes the differences.
You note that these conclusions are exactly the same as obtained using your
simpliﬁed approach. But, had the data contained more variability within region
due to Salesrep Name, these conclusions may well have differed from those
you obtained earlier.
With this, you thank Arthur for his help. You feel much more comfortable
now that you have a more rigorous way to analyze your data. However, you
reﬂect that you learned a lot from the simple, slightly incorrect, comparison
circle analysis, which would also be much easier to explain to your VP. You
close all report windows, but keep the summary table open.
Uncovering Relationships: Prescriptions versus Visits by Region
Reﬂecting on the regional differences, you start to wonder if the number of
visits has an effect on a physician’s prescribing habits. Do more visits tend to
lead to more prescriptions for PharmaInc’s major product? Or, do physicians
tire of visits by sales representatives, leading perhaps to a negative effect? Do
more sample kits lead to more prescriptions, as one might hope?
You proceed to get some insight on these questions by running Fit Y by X
again, producing Exhibit 8.21.

330
V I S U A L S I X S I G M A
Exhibit 8.20 Tukey HSD Pairwise Comparisons for Random Effects Model
To produce Exhibit 8.21 (PharmaSales By (Physician Name, Region Name,
Salesrep Name)):
1.
Make sure that PharmaSales By (Physician Name, Region Name,
Salesrep Name) is the active data table.
2.
Select Analyze > Fit Y by X.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
331
3.
Enter Sum(Prescriptions) as Y, Response.
4.
Enter Sum(Visits) as X, Factor.
5.
Click OK.
6.
Select Fit Line from the Bivariate Fit red triangle menu.
Tips: You can remove a ﬁt in the Fit Y by X report by selecting the Remove Fit red triangle
option for that ﬁt. You can also use Group By, which (for subsequent ﬁts) gives a separate line
for each distinct level of the variable you have selected. Right-clicking on the graph allows you to
ask for a Row Legend. After assigning a variable, this adds a row legend to the report that
allows you to select any level of that variable and highlight the corresponding points.
You remind yourself that the x-axis represents the total number of visits
paid to the given physician by the sales rep over the eight-month period. It
does appear that, at least to a point, more visits result in more prescriptions
being written. Given the pattern of points on the graph and the fact that, at
some point, additional visits will have diminishing returns, you suspect that
there may be a little curvature to the relationship.
So, you start thinking about a quadratic ﬁt. But, given the regional differ-
ences found already, you would like to see such a ﬁt for each of the nine regions,
that is, by the values of Region Name.
To generate the output shown in Exhibit 8.22, select Group By from the
Bivariate Fit red triangle menu, select Region Name, and click OK. Then, from
Exhibit 8.21 Linear Bivariate Fit of Sum(Prescriptions) versus Sum(Visits)

332
V I S U A L S I X S I G M A
Exhibit 8.22 Quadratic Fits to Each of the Nine Regions (Southern England Selected)
the Bivariate Fit red triangle menu, select Fit Polynomial > 2,quadratic (saved
script is Bivariate Fit 1).
When you see this, you are immediately struck by the three curves that
start out fairly low in comparison to the others, but which exceed the others as
Sum(Visits) increases (within the range of most of the data). You can tell from
the color-coding on the screen that these curves are for the three promotional
regions. This suggests that leaving sample kits behind makes a big difference.
You make a mental note to run this analysis with Sum(Visits with Samples) as
X, Factor in a few minutes.
The other six regions may or may not see increased prescriptions with
increased visits. You quickly peruse the Prob > F values for the model tests
in the Analysis of Variance tables (realizing that these tests suffer from the
same deﬁciency as did the HSD-tests in your previous Fit Y by X analysis and
so are not technically correct). Models that are associated with Prob > F values
less than 0.05, which are marked by asterisks, are considered signiﬁcant. You
see that for Eastern England, Southwest England, and Wales, the p-values
do not indicate that the quadratic ﬁts are signiﬁcant (nor are linear ﬁts,
which you also check). This suggests that, in these regions, the sales reps
may need to do something other than increase visit frequency if they wish to
increase sales.
Picking up the thought about the efﬁcacy (or otherwise) of leaving promo-
tional kits behind with physicians, you quickly generate Exhibit 8.23 (saved
script is Bivariate Fit 2).

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
333
Exhibit 8.23 Bivariate Plot for Promotional Regions, with Sum(Visits with Samples) on X-Axis
Since only the three promotional regions have non-missing data for Visits
with Samples, these are the only three regions for which ﬁts are possible. All
three polynomial ﬁts are signiﬁcant. It seems clear that, over the range of the
number of visits studied, the more frequently the sales rep visits and leaves
sample kits behind, the larger the number of prescriptions written. This analysis
provides more evidence that the promotion was indeed successful!
Of course, it raises the question about tradeoffs: When does one reach the
point of diminishing returns relative to the number of visits with sample kits
being left and the costs of running the promotion? This is an important market-
ing question. You decide to do some thinking and reading about how to address
this question before raising it with your VP. For now, though, you have strong
evidence that the promotion was successful and that you can use Exhibits 8.17,
8.22, and 8.23 to communicate this to your VP. You close your summary table
and all open reports.
A DEEPER UNDERSTANDING OF REGIONAL DIFFERENCES
Your next charge is to understand more about the regional differences in perfor-
mance of sales of PharmaInc’s major product. You realize that you have already
gone a long way toward answering this question. Yes, there are statistical
differences between regions, as indicated by the oneway comparison circles in
the section “Uncovering Relationships: Prescriptions versus Region with Fit Y
by X.” You can say that, in terms of total prescriptions written over the

Exhibit 8.24 Partial View of Summary Table with YTD Aggregation
334

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
335
eight-month period, the regions fall into ﬁve or six groupings, as shown in
Exhibits 8.17 and 8.20. The important questions now revolve around the
practical importance of such differences and which chunk variables are related
to the differences.
So at this point, you want to go beyond simply the counts of prescriptions
written. Again, you have already done some of the work to understand the rela-
tionship between the number of prescriptions and the number of visits. Now
you want to see if you can gain additional insight by seeing this from the per-
spective of your sales reps, and by taking into account behavior over time. In
fact, when you think about this, you realize that you would like to see, on a
year-to-date basis, how the number of prescriptions written by each sales rep’s
physicians is related to the total number of visits. But you would also like to
be able to visualize the effect of the total number of physicians assigned to
a sales rep and to easily identify the sales rep’s region. In short, you want a
more informative version of the scatterplots presented in the bivariate reports
in “Uncovering Relationships: Prescriptions versus Visits by Region” section.
You have seen demonstrations of JMP’s Bubble Plot using animation
to show changes in the scatter of points over time. You quickly realize that
you must do some data preparation before using such a visualization. So you
summarize your data across sales reps and deﬁne year-to-date summaries of
Prescriptions and Visits, producing the table partially shown in Exhibit 8.24.
To produce the table shown in Exhibit 8.24 (PharmaSales.jmp, script is Summary
Table 2):
1.
Make sure PharmaSales.jmp is the active data table.
2.
Select Tables > Summary.
3.
Select Visits and Prescriptions. From the Statistics menu, select Sum.
4.
Select Salesrep Name and Region Name and click Group.
5.
Select Date and click Group.
6.
Select column from the statistics column name format list.
7.
Click OK.
This produces a Summary table.
8.
Right-click on the heading of N Rows in the Summary table, select Column Info,
and change the column name to Number of Physicians. Click OK.
In the next steps, you create year-to-date summary columns.
9.
Right-click on the heading of Salesrep Name and select New Formula Column
> Group By.
10.
Right-click on the heading of Visits and select New Formula Column > Row >
Cumulative Sum.

336
V I S U A L S I X S I G M A
11.
Rename the new column to Visits YTD.
12.
Right-click on the heading of Prescriptions and select New Formula Column >
Row > Cumulative Sum.
13.
Rename the new column to Prescriptions YTD.
14.
Use the columns created in steps 8 and 10 to make two new formula columns, Visits
YTD per Physician and Prescriptions YTD per Physician.
You recall there are 102 sales reps (Exhibit 8.5). There should be eight YTD
values for each sales rep, so the summary table you just made should contain
816 rows, which it does.
Now you are ready to construct an animated bubble plot. You decide to
look at the relationship between Prescriptions YTD (Y) and Visits YTD (X), with
bubbles sized by Number of Physicians and colored by Region Name, over the
time period deﬁned by Date. You enter Region Name and Salesrep Name as
ID columns, in that order. This will allow you to use the drill-down capability
of the bubble plot (see Exhibit 8.13). After completing the Bubble Plot dialog,
you obtain Exhibit 8.25 (showing the relationship between Prescriptions YTD
and Visits YTD for May 2014). The saved script is Bubble Plot 1.
Exhibit 8.25 Bubble Plot for Prescriptions YTD versus Visits YTD, May 2014

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
337
Now you take a moment to ﬁgure out what is being plotted. The vertical
center of each bubble is at the average of Prescriptions YTD for the given region.
The horizontal center is at the average of Visits YTD for the given region. The
sizes of the bubbles are proportional to the number of physicians in the regions.
By using the Date slider at the bottom left of the plot, you can follow the rela-
tionship through the eight-month period to December 2014.
Notice that you can animate the plot by clicking the right-pointing arrow in
the animation controls at the bottom left of the window. You do this and ﬁnd
it interesting that the two top regions, as shown in the December plot, achieve
the same general average prescription totals, yet one requires many more visits,
on average, than the other.
To help ﬁnd out which regions these are, you select Label > All from the
red triangle menu in the plot report. The labels appear on the plot and you see
that the regions of interest are Midlands and Northern England. Now, Midlands
was part of the promotion, while Northern England was not. You ﬁnd it strik-
ing that Midlands achieved roughly the same mean number of prescriptions per
sales rep as did Northern England with far fewer visits. But, you note, the Mid-
lands, whose bubble is smaller than Northern England’s, evidently has fewer
physicians (it looks to be about half as many as in Northern England).
With this in mind, you click the right-pointing arrow to restart the anima-
tion. You notice that Midlands and Northern England tend to have the same
general mean level of Prescriptions YTD over time. But, you think, Northern
England does have more physicians, and so perhaps those sales reps have a
larger physician workload than do the sales reps in Midlands, and so have to
make more visits. Perhaps you should be looking at year-to-date prescriptions
and year-to-date visits per physician?
But ﬁrst, you want to look at this plot with the bubbles split by Salesrep
Name. You go to the red triangle, select Label > None, and then go back to the
red triangle menu and select Split All. Now, for each month, you see a single
bubble for each sales rep. You animate the plot and observe what is happening.
(Exhibit 8.26 shows the plot for December 2014.)
Looking at the plot over time, you ﬁnd it interesting that the sales rep
bubbles within a region stay tightly clustered, indicating that the aggregated
number of visits is fairly homogeneous within regions, as are the aggregated
prescription totals. You also notice that the circle sizes differ greatly from
region to region, but are fairly uniform within regions, meaning that the
number of physicians assigned to sales reps may differ quite radically for
different regions, but that, within regions, the allocation is fairly consistent. It
does appear that a typical sales rep in Northern England has more physicians
than does a typical sales rep in Midlands. This plot provides very useful

338
V I S U A L S I X S I G M A
Exhibit 8.26 Bubble Plot for Prescriptions YTD versus Visits YTD, December 2014, Split by
Salesrep Name
information and could help you in thinking about ways to realign your sales
force in 2015.
Now, you go back to your idea of normalizing by Number of Physicians.
You already created the two columns that you will need, Prescriptions YTD per
Physician and Visits YTD per Physician. You complete the Bubble Plot dialog
as before, but using the new variables, entering the ﬁrst as Y and the second
as X. From the red triangle menu, you select the options Trail Bubbles > All,
Trial Lines > All, and Label > All. This generates Exhibit 8.27. The saved script
is Bubble Plot 2.
You are struck by how similar the regions are in terms of Visits YTD per
Physician. You realize that the norm for visits to physicians is one visit per
month. But the data show this happening with almost uncanny regularity.
You wonder if, perhaps, sales reps make one visit per month to each prac-
tice, and then count this as a visit to all physicians at that practice even though
they do not meet individually with all those physicians. In other words, what
does it mean to “make a visit to a physician”? Does it mean that the sales rep
talks with the physician face to face? Or that the rep talks with a secretary or
technician? Or that the rep simply drops in and leaves a card? And, is it possible
that the data are not quite representative of reality? You note that you need to
discuss this with the reps when you next meet.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
339
Exhibit 8.27 Bubble Plot for Prescriptions YTD per Physician versus Visits YTD per Physician,
December 2014
By selecting various bubbles, and holding the Control key to select more
than one region, you are able to select Midlands and Northern England. Since
you have enabled Trail Bubbles and Trail Lines, as you step through the
months, the plot shows the bubbles for the preceding months. You step through
the eight months, looking at these two regions. You see them drift further apart
over time, with Midlands greatly exceeding Northern England in mean number
of Prescriptions YTD per Physician. So, even accounting for number of physi-
cians, Midlands is ahead. This is further conﬁrmation that the promotion did
enhance sales.
You deselect these two regions, and then deselect Label > All and select the
Split All option to see the individual sales rep behavior. Once again, you ani-
mate the plot. You note that there is a little less regularity in the Visits YTD per
Physician for individual sales reps, with Scotland showing the most variability.
Again, the question of what the sales reps are recording comes to mind. Are the
reps in Scotland unknowingly using different criteria?
You would like to show these results in a form that the reps can understand,
but you realize that, typically, you do not carry your laptop in the ﬁeld. Given
the reps only have access to the usual ofﬁce software and a browser, you decide
to export Exhibit 8.27 as dynamic HTML, to produce Exhibit 8.28.

340
V I S U A L S I X S I G M A
Exhibit 8.28 Bubble Plot for Prescriptions YTD per Physician versus Visits YTD per Physician,
December 2014 as Interactive HTML
To produce Exhibit 8.28 (open Journal: Visual Six Sigma, Chapter 8, script is Bubble
Plot 2 as Interactive HTML with Data):
1.
With the report shown in Exhibit 8.27 active, select File > Save As (or File >
Export on a Mac).
2.
Select Interactive HTML with Data from the Save As Type selection list (or
select the Interactive HTML with Data radio button, followed by Next on a Mac).
3.
Navigate to a convenient location at which to save the ﬁle, then click OK.
4.
Using File Explorer (Windows) or Finder (Mac), locate the saved ﬁle and double-click on it
to open it in the default browser.

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
341
Tip: You can preserve much of the interactivity in the reports produced by many JMP platforms by
saving to HTML. For example, in the case of the Bubble Plot, the Split and Combine
features are preserved. In cases when interactivity is not needed or is not available, static HTML will
be generated. Note that the interactivity outside JMP also requires the export of the requisite data.
Summary
You now gather your ﬁndings and recommendations from your investigations
in relation to the two tasks you were given by your VP of sales and marketing:
◾
Verify the claims of the Marketing Group that a limited promotion they
ran for PharmaInc’s major product from May through December of 2014
was very successful.
◾
Determine if there are regional differences in the sales performance of
PharmaInc’s major product.
As for the ﬁrst task, given the available evidence, the 2014 promotional
activity in Midlands, Southern England, and Northern Ireland did have a
positive impact, just as the Marketing Group had claimed. Physicians in
these regions averaged 82.3, 76.9, and 72.9 prescriptions, respectively, over the
eight-month period. Meanwhile, physicians in the best nonpromotional region,
Greater London, averaged 67.3 prescriptions. In the worst nonpromotional
regions, Wales and Northern England, physicians averaged 42.6 and 42.8
prescriptions over that eight-month period. Exhibit 8.16 (oneway plot) shows
the picture by region with the data summarized by physician and Exhibit 8.26
(bubble plot) shows the picture summarized by sales rep.
You also learn that more visits by sales reps generally lead to more prescrip-
tions for PharmaInc’s major product. This effect is especially true when reps
leave behind promotional sample kits.
Relating to the second task, your initial analysis of the data shows that some
sales reps are in contact with many physicians, whereas others are in contact
with far fewer physicians, and that this depends on region. Even though you
do not have to account for sales reps’ time, you would like to understand better
why this difference occurs, because you might then be able to increase the yield
of your sales force as a whole in 2015. But, more to the point, you have learned
that there are large differences in sales between the different regions. Rather
than simply attribute this to a failure of your sales force in some regions, you
are prepared to at least consider that this may also be due to regional differences
in the physician and population demographics.

342
V I S U A L S I X S I G M A
One issue that your data analysis has surfaced is the extreme regularity of
monthly visits over almost all regions. When data are this regular, you know
that there is probably some underlying reason. You need to convey to the sales
reps that there will not be negative consequences if they fail to adhere to a
one-visit-per-month doctrine, so long as they show good performance overall.
As a ﬁrst step, you decide to initiate interviews with some selected sales
reps so that you can learn something about this issue, as well as why there is
such large variation in number of physicians per sales rep between regions. You
expect to need to realign your sales force for 2015 and would like to do this in
a logical fashion. You also need to enlist the sales reps’ help in more clearly
deﬁning measures such as “a visit to a physician” that will help to clarify the
meaning of the data by which their performance will be measured.
CONCLUSION
Using visual techniques, you have been able to construct a good understanding
of the operation of your sales force and to quickly answer the two questions
posed by your VP of sales and marketing in a complete and compelling man-
ner. Your work in the “Validating and Scoping the Data” section was largely
directed toward your personal agenda, namely to obtain better knowledge of
how your sales force is currently deployed. In the sections “Investigating Pro-
motional Activity” and “A Deeper Understanding of Regional Differences,” you
addressed the business questions posed by your executive. The analysis that
led to the answers in these two sections was clean, quick, and compelling, and
much more efﬁcient than anything you could have done without JMP.
This case study is a good example of where force-ﬁtting important busi-
ness questions into a project framework or the traditional Six Sigma DMAIC
methodology would be ill advised and probably counterproductive.
The analysis you have done in this scenario raises two further important,
more general, points:
1. If anything, the data used are probably too simplistic. Real-world data in
such a setting are likely to be more, rather than less, complex and there
are also likely to be more records. Questions relating to data quality, the
distribution of missing values, and the balance of the data itself (which
levels of descriptive variables occur jointly) become crucial. As this case
study seeks to show, understanding these issues is a necessary step in any
kind of sensible analysis. Once these patterns of variation are understood,
the analysis itself is often relatively straightforward given a typical set of
business objectives.
2. A key requirement of software used in an analysis such as this one is that
it should allow you to easily construct insights from real-world, unruly

I N F O R M I N G P H A R M A C E U T I C A L S A L E S A N D M A R K E T I N G
343
data. Answering one question typically leads to several other questions,
and static reports with ﬁxed hierarchies do not lend themselves to this
pattern of use. The process of discovery is necessarily personal, so soft-
ware should aspire to provide a wide repertoire of techniques that allow
you to visualize patterns of variation in your data in an unfettered way.
Your analysis took one direction, but there are many others that might
have been taken.
NOTE
1. Ellis R. Ott, “Process Quality Control,” Chapter 4, McGraw-Hill, 1975, and Ronald Moen, Thomas
W. Nolan, and Lloyd P Provost, Quality Improvement Through Planned Experimentation, 2nd edition,
McGraw-Hill, 1999.

C H A P T E R 9
Improving a Polymer
Manufacturing
Process
345
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

346
V I S U A L S I X S I G M A
Y
our employer, a British company called MoldMat Ltd., manufactures gran-
ulated white plastic at a plant in Britain and supplies it to a molding plant
in Italy, where it is made into white garden chairs and tables. However,
the molding process goes through intermittent phases when its product quality
drops, leading to yield losses at both the polymer and the molding plants. When
a crisis occurs, teams are formed to tackle the problem, but the problem usually
disappears for no apparent reason.
After yet another mysterious crisis occurs and resolves itself, you are
selected to solve the problem once and for all. Your selection is due in large
part to your recent Six Sigma black belt training. Together with a small project
team, you identify two characteristics (Ys) that are of paramount importance
relative to quality and yield: the polymer’s melt ﬂow index (MFI) and its color
index (CI).
Together with your team, you reanalyze the most recent crisis team’s data.
The analysis fails to reveal suspected relationships between the two responses
and eight process factors. This leads you to suspect that measurement variation
may be clouding results. Consequently, you and your team conduct Measure-
ment System Analysis (MSA) studies on the measured Ys and Xs. The problem-
atic variables turn out to be MFI (one of the two Ys) and ﬁller concentration
(one of the Xs).
Once the repeatability and reproducibility issues for these two variables
are addressed, your team gathers new data. Your analysis begins with data
visualization and exploration. Then it proceeds to modeling relationships. You
develop useful models for MFI and CI that include terms that might otherwise
have been overlooked, had your team not done extensive data visualization.
You use the Profiler to optimize MFI and CI simultaneously. Using sound
estimates of the expected variation in the Hot Xs, you simulate the expected
distributions for MFI and CI at the optimal settings. The simulations indicate
that the parts per million (PPM) rate should be greatly reduced. After running
some successful conﬁrmation trials, management implements the changes.
One and a half years later, not a single batch of white polymer has been
rejected by the molding plant. The savings from rejected batches alone amount
to about £750,000 per annum. Additionally, because there are now no pro-
cessing restrictions on the molding plant, savings of £2,100,000 per annum are
being realized by MoldMat’s big customer. This in turn leads to increased sales
for MoldMat. These savings come at very little cost, as project-related expendi-
tures were minimal.
Your odyssey takes you and your team through all of the steps of the Visual
Six Sigma Data Analysis Process. In particular, you engage in interesting work
involving MSAs and modeling using stepwise regression. A list of platforms and
options used is given in Exhibit 9.1. The data sets can be found at http://support
.sas.com/visualsixsigma.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
347
Exhibit 9.1 Platforms and Options Illustrated in this Case Study
Menus
Platforms and Options
Tables
Concatenate
Rows
Exclude/Unexclude
Hide/Unhide
Colors/Markers
Row Selection
Clear Row States
Data Filter
Cols
New Column
Column Info
Column Properties
Hide/Unhide
Exclude/Unexclude
DOE
Full Factorial Design
Analyze
Distribution
Histogram
Continuous Fit
Frequency Distribution
Fit Model
Standard Least Squares
Stepwise
All Possible Models
Macros—Response Surface
Macros—Factorial to Degree
Effect Summary
Modeling
Quality and Process
Control Chart Builder
Process Capability Analysis
Measurement Systems Analysis
Graph
Graph Builder
Scatterplot Matrix
Surface Plot
Proﬁler
Maximize Desirability
Sensitivity Indicators
Simulator
Other
Local Data Filter
Column Switcher
Non-normal Capability

348
V I S U A L S I X S I G M A
SETTING THE SCENE
For the past 25 years, MoldMat Ltd. has supplied the plastic that one of its
major customers in Italy uses in making white garden furniture. Over the years,
mysterious crises occur during which the ﬂowability of the plastic leads to low
yields for both MoldMat and their Italian customer. To date, all efforts to ﬁnd
the root cause of these crises have failed. After the most recent crisis, due to
your training in Visual Six Sigma, you are asked to lead a new team whose
mandate is to ﬁnd a permanent solution to the problem.
Manufacturing Process
White garden chairs and furniture command a very good price, but they are
difﬁcult to make owing to the impact of whitening agents on plastic ﬂowability.
Getting the right balance of whiteness and ﬂow is not easy. As the proportion of
additives in the mix increases to make the plastic whiter, the ﬂow of the plastic
is impaired.
The process for making white plastic begins with the preparation of a ﬁller
mixture, or slurry. The white ﬁller, which is an inert powder, is sourced from
a number of quarries in Africa. It is mixed with unpuriﬁed river water in a
stirred tank in the ﬁller preparation section of the MoldMat polymer plant
(Exhibit 9.2). The ﬁller preparation tank is agitated and held at a target concen-
tration. The tank is topped off each day with ﬁller and water. Small amounts of
a viscosity modiﬁer are added to the slurry if the viscosity gets too high.
Clear plastic is made by heating and stirring a monomer in a batch reac-
tor until it polymerizes. To make the white plastic, the ﬁller slurry is added to
the monomer in the polymerization reactor at the start of the polymerization
process. When the polymerization reaction is complete, the molten polymer is
granulated and packed. The MoldMat plant in England makes three batches of
white plastic per day, running a 24-hour schedule every day of the week.
Filler
Water
Viscosity
Modifier
Monomer
Filler Slurry
Polymer
Polymerization
Filler
Preparation
Granulate
and Pack
Exhibit 9.2 White Polymer Manufacturing Process

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
349
The polymer plant tests every batch of polymer. A sample from each com-
pleted batch is taken and tested for:
◾
Color (whiteness), measured on a colorimeter using a color index
◾
Melt ﬂow, measured as a melt ﬂow index in an ofﬂine laboratory test;
this is an indicator of how well the polymer will process in the down-
stream molding plant
◾
Filler content of the polymer
A Typical Crisis
Crises have occurred two or three times a year ever since the new product was
introduced ten years ago. Here is a typical sequence of events.
The Italian molding plant has several months of normal processing before
starting to experience problems with ﬂowability. When this happens, techni-
cians in the molding plant check the processing parameters, and if these look
reasonable, they question the quality of the polymer. The MoldMat plant engi-
neers check that the polymer is in speciﬁcation and verify that there is nothing
wrong with the test equipment. This leads the processing plant engineers to
suspect that the molding processing parameters have changed.
After a few more days of bad processing, the molding plant engineers ask for
some different polymer to run as a trial. This requires a fresh start for molding
production. The molding plant must empty the polymer silos to run the trial
polymer. The purged material is sold as scrap, which is accounted for as a loss
in the MoldMat plant yield.
By this time, the output of the molding plant is well behind schedule, and
customers are running out of chairs. The business suffers substantial lost margin
and opportunity.
Meanwhile, rapid action teams have been assembled from across Europe.
A plethora of helpful theories and their associated solutions are developed,
such as:
◾
The ﬁller supplier is inconsistent and should be replaced.
◾
Last week’s heavy rain has altered the pH of the water supply, which
has affected the reaction chemistry.
◾
The MFI speciﬁcation is too high, so batches of polymer at the bottom
end of the speciﬁcation range should be the only ones used.
◾
Abnormal ambient temperatures and humidity are to blame.
◾
The ﬁller is not evenly distributed through the polymer, and agglomer-
ates are blocking the ﬂow channels in the molds.
Process changes are made, trials are run, and data are gathered. But none
of the changes ever conclusively solve the problem.

350
V I S U A L S I X S I G M A
Then, mysteriously, the problem goes away. The molding process gradually
improves, with everyone convinced that their pet theory or solution was the
one that made the difference. All is well until the next crisis.
Forming a Team
After one particularly bad crisis, the manufacturing director, Edward Constant,
has ﬁnally had enough. MoldMat has started to implement Visual Six Sigma,
and the black belts from the ﬁrst wave of training are anxious to start driving
improvement. Edward is skeptical about Visual Six Sigma, but he is prepared to
give it a go—after all, nothing else has worked.
You are a bright young process engineer who has only recently moved to
the polymer plant, and you were one of the ﬁrst trainees. Edward has met you a
few times and is impressed by your openness to new ideas and your approach to
problem solving. Given the numerous false starts, Edward ﬁgures that your lack
of detailed knowledge of MoldMat’s operations could actually be an advantage,
provided that you work with people who have the right mix of experience.
At your ﬁrst meeting, Edward indicates that he will act as project sponsor
and offers you all the support you need. He tells you: “Everyone has an opinion
on the best solution, but I have never been satisﬁed that anyone has properly
done any rigorous analysis, let alone identiﬁed the root cause of the problem so
that it can be conclusively ﬁxed. This problem has been around for ten years, so
a few more months are not going to make that much difference. The best advice
I can give you is to take your time and to trust nothing and no one, unless you
have personally veriﬁed the data and have worked through it in a methodical
way. I don’t want any more crises. If the process can work most of the time,
then it should be able to work all of the time.”
Edward knows that a change of polymer can immediately affect the pro-
cessing performance of the molding plant, even if the polymer batches meet
the polymer speciﬁcations. So he urges you to focus on the polymer plant ﬁrst
and to talk to a wide range of people in both the polymer and molding plants.
But above all, he directs you to collect some data.
Edward helps you form a small project team consisting of you and the fol-
lowing associates:
◾Henry, the polymer plant quality manager
◾Bill, a polymer chemist from a technical support group
◾Roberto, a process engineer from the Italian molding plant
◾Tom, a master black belt
Tom’s role is to ensure that the Visual Six Sigma methodology and tools are
correctly applied. He is a well-seasoned and culturally savvy master black belt

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
351
from MoldMat’s training partner. Together with Tom, you assemble the team
and review your objectives. To ensure that all team members share a common
language and approach, Tom schedules and conducts an impromptu training
session in Visual Six Sigma and JMP.
FRAMING THE PROBLEM
At this point, the team needs to develop a project charter, using a high-level
process map and some baseline data. Also, you decide to develop a Suppliers,
Inputs, Process, Outputs, and Customers (SIPOC) map and gather customer input.
The result of this process is that you and the team decide to focus on two critical
process characteristics: melt ﬂow index and color index.
Developing a Project Charter
During your ﬁrst team meeting, the team draws a high-level process map
(Exhibit 9.3). You also decide to review yield data from both the polymer and
molding plants to conﬁrm the size and frequency of the problem.
There have been many arguments about white polymer quality. Although
there is a polymer speciﬁcation, the molding plant has long suspected that it
does not fully reﬂect the true requirements of their process. After a long dis-
cussion, the team agrees on the following Key Performance Indicator (KPI)
deﬁnitions for the project:
Daily yield is calculated as the weight of good polymer divided by the weight
of total polymer produced.
Good polymer is polymer that can be successfully processed by the molding
plant.
Total polymer produced will include product that fails to meet the polymer
plant speciﬁcations, plus any polymer that, although meeting polymer plant
speciﬁcations, is subsequently scrapped or rejected in the molding plant.
The team collects some historical data on daily yield and imports it into
a data table named BaselineYieldData.jmp. The data table contains two
Prepare
Slurry
Make
Polymer
Granulate
Polymer Plant
Pack
Mold
Chairs
Exhibit 9.3 High-Level Process Map of White Polymer Molding Process

352
V I S U A L S I X S I G M A
Exhibit 9.4 Partial View of BaselineYieldData.jmp
columns, Date and Yield, and has 1,125 rows covering a period of a little more
than three years. (The data table also contains two scripts for later use.)
Note that Yield is designated as a Label variable, as evidenced by the yellow
label icon next to Yield in the Columns panel in Exhibit 9.4. With this property,
when the arrow tool hovers over a point in a graph, that point’s Yield value will
appear. You gave Yield the Label role by right-clicking on it in the Columns
panel and selecting Label/Unlabel.
You proceed to construct an Individuals control chart to see how Yield varies
over time. The distribution of Yield measurements is likely to be skewed, since
there is an upper limit of 100 percent, so that control limits calculated using
an individual measurement control chart may not be appropriate. Nonetheless,
you decide to use the individual measurement chart in an exploratory fashion.
You also decide that the Moving Range part of the IR chart is not of interest to
you at this point, so you remove it. This allows you and others to focus on the
Yield values.
To construct an individual measurement control chart for Yield (BaselineYieldData.jmp,
script is Control Chart Builder):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Select both Date and Yield in the Select Columns list.
3.
Drag and drop these two variables into the center of the template.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
353
4.
Right-click on the Moving Range(Yield) axis and select Remove. (Alternatively,
drag the title Moving Range(Yield) out of the template area.)
5.
Click Done to close the Control Panel.
The control chart shown in Exhibit 9.5 appears. The chart clearly shows
periods of high yields, each followed by a crisis, with a total of nine crises over
the period. The average Yield over this time period is about 88 percent, but
the real issue is the repeated occurrence of causes that are not intrinsic to the
system, called special causes.
To conveniently reproduce this analysis and to document your work, you
can save a script to the data table to reproduce this chart. The script has already
been saved for you with the default name Control Chart Builder. To save this
script yourself, click on the red triangle next to Control Chart Builder in the
report window and choose Script > Save Script to Data Table. By default,
your new script is called Control Chart Builder 2, but you can click on the
name to change it.
To better understand the crisis Yield values, use your cursor to hover over
various points to see their values. You come to the realization that crisis peri-
ods can be loosely deﬁned by collections of batches with Yield values below
85 percent.
You become curious about the likely yield of the process had it not been
affected by these crisis periods. Just to get a sense of the noncrisis yields, you
construct a control chart with crisis batches, deﬁned as batches with yields
below 85 percent, excluded. The Local Data Filter provides an easy way to
ﬁlter out data values without affecting other plots or the data table.
Exhibit 9.5 Individuals Chart of Baseline Yield Data

354
V I S U A L S I X S I G M A
To ﬁlter out the crisis yields (BaselineYieldData.jmp, script is CCB with Local Data
Filter):
1.
In the Control Chart Builder window, select Script > Local Data Filter
from the red triangle menu.
2.
In the Local Data Filter panel, select Yield and click Add.
3.
To include only those rows with yields of at least 85 percent, click on the minimum value
for Yield above the slider in the dialog box, shown as 41.905, and enter 0.85 into the
text box.
4.
Click Enter (or click elsewhere in the window) to accept the settings.
The control chart updates to show only those rows where Yield is at least 85 percent.
The Local Data Filter panel and plot are shown in Exhibit 9.6. The Yield
Limit Summaries report to the right of the chart, shown beneath the plot in
Exhibit 9.6, indicates that the mean Yield for the noncrisis periods is about
94 percent. Close your data table without saving changes.
With this information as background, the team reconvenes. You need to
agree on the problem statement and project goal, and to deﬁne the speciﬁc
scope and focus of the project.
The team drafts a project charter, shown in Exhibit 9.7. As instructed, you
decide to focus on the polymer plant and set a goal of achieving an average
yield of 95 percent by the end of the year. It is late August—this gives you four
months. You know that if you can eliminate the crises, a 94 percent yield can be
achieved. But, knowing that the team will be constructing detailed knowledge
of the process, you feel that you can even do a little better.
At this point, you check in with Edward to obtain his support for the
project charter and the team’s proposed direction. As it turns out, Edward is
very impressed with the clarity of your work to date and likes the idea that
the project goal was chosen based on sound data. He is quick to approve of the
team’s charter and direction.
Identifying Customer Requirements
Next, the team explores the following questions:
◾What are the true requirements of the molding plant?
◾Why are these requirements met at some times but not at others?
◾What is changing?

Exhibit 9.6 Local Data Filter Settings to Exclude Crisis Rows
355

356
V I S U A L S I X S I G M A
Exhibit 9.7 Project Charter
Project Title
Improve White Polymer Process Yield
Business Case
The manufacture of white polymer results in periodic ﬂowability crises at a
large Italian customer’s molding plant. The molding plant sells suspect poly-
mer at scrap prices. These crises have been going on for years and, although
the crises resolve temporarily, they continue to recur, causing signiﬁcant dis-
ruption and great ﬁnancial loss for both MoldMat and its customer.
Demand for white furniture keeps increasing, and the molding plant in Italy
can’t afford to be down due to lack of acceptable white polymer. The molding
plant has to turn orders away in crisis periods, causing a signiﬁcant loss in
revenue and great dissatisfaction.
Problem/Opportunity
Statement
It is estimated that, due to the crisis periods, the polymer plant suffers a yield
loss of about £700,000 per year in scrap material. There is the opportunity to
recover at least £700,000 annually in what would otherwise be scrap.
Also, a signiﬁcant margin loss is generated by the molding plant, which
has to turn orders away in crisis periods. If the problem could be ﬁxed, the
accounting department estimates that the company would realize an addi-
tional £2,000,000 of revenue annually.
Project Goal Statement and KPI
(Key Performance Indicator)
Increase the average yield of white polymer from 88 percent to 95 percent or
higher by March 1, 2015 (four months).
Daily yield will be plotted using an individual measurement control chart.
Project Scope
The polymer plant’s part of the process.
Project Team
Sponsor: Edward Constant
Black belt and polymer process engineer: This is you!
Team members:
Henry Doyle, the polymer plant quality manager
Bill Wright, a polymer chemist from a technical support group
Roberto Valentino, a process engineer from the Italian molding plant
To this end, during its next meeting, the team produces a SIPOC map to
help gain a better understanding of the process steps and to identify where to
focus within the polymer plant (Exhibit 9.8).
The team also proceeds to collect voice of the customer (VOC) information
from the immediate customers of the process, namely, the stakeholders at the
molding plant. Through interviews, team members collect information from
molding plant technicians and managers. There are many comments reﬂecting
that plant’s frustration, such as the following:
◾“I don’t want any crises caused by poor polymer.”
◾“Your polymer is not consistent.”
◾“I don’t believe you when you say you are in spec.”
◾“I need to be able to make good white molding all the time.”
◾“You are killing my business.”
◾“We can’t continue with these scrap levels.”

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
357
Exhibit 9.8 SIPOC Map for White Polymer Process
Suppliers
Inputs
Process
Outputs
Customers
Umboga A
Filler
Prepare Slurry
Filler Slurry
Polymerization
Kuanga A
Kuanga B
North West Water Authority
Water
Slurry Preparation
Filler Slurry
Make Polymer
Polymer
Granulation
Monomers Inc.
Monomer
Granulation
Polymer
Granulate
Granules
Packing
Packing
Granules
Pack
Bags
Molding Plant
But the team also collects speciﬁc information about the technical require-
ments of the molding process. You diagram your analysis in the form of a Critical
to Quality Tree, a portion of which is shown in Exhibit 9.9. This Critical to Quality
Tree ﬂows from top to bottom. It ﬁrst lists the critical customer needs, then their
drivers, and ﬁnally the measurable requirements needed for improvement.
Two primary characteristics quickly emerge:
◾
The molding plant has speciﬁed that in order for the polymer to process
well on their equipment, the polymer’s MFI must fall between lower
and upper speciﬁcation limits of 192 and 198 (with a target of 195).
◾
The polymer’s CI must meet the whiteness speciﬁcation. The maximum
possible CI value is 100, but the only requirement is that CI must exceed
a lower speciﬁcation limit of 80.
Reliable supply
of chairs to end customers
Low scrap costs
Molding plant
yield
Polymer plant
yield
Reliable running
of molding plant
Polymer
processability in
the molding plant
Consistent
polymer color
Color Index
> 80
Melt Flow Index
192–198
Exhibit 9.9 Partial Critical to Quality Tree for Molding Plant VOC

358
V I S U A L S I X S I G M A
REVIEWING HISTORICAL DATA
Your team decides to review a set of data collected by a team that had been
assembled to address one of the recent yield crises. This earlier team was unable
to identify the root cause of the crisis. The fact that the team’s data do not lead
to the identiﬁcation of Hot Xs leads you to suspect that one or more variables
suffers from large measurement error.
Data from Prior Crisis Team
You and your team begin to review the last crisis team’s data and analysis. To
your surprise and delight, you ﬁnd that the crisis team had used many Six Sigma
tools in investigating possible causes of the problem.
In particular, the team members had developed an Input/Output process map
(Exhibit 9.10) to help identify the potential Xs that might be driving variation
in MFI, CI, and, consequently, Yield. They used the Xs and Ys identiﬁed in their
process map to determine the data they should collect.
Reanalyzing the Historical Data
You obtain a spreadsheet of the data collected by the crisis team and import
this into a JMP table, which you call CrisisTeamData.jmp. The data con-
sist of measurements for the Xs and Ys identiﬁed in the process map for 127
batches over about a six-week period. A partial view of the data table is shown
in Exhibit 9.11.
The columns in the data table are described in Exhibit 9.12. There are three
Ys and eight Xs of interest. You note that, even though the table does not have
an explicit date or time column, the sequential values of Batch Number deﬁne
the processing order.
Filler Water
Monomer
Make
Polymer
Pack
Mold
Chairs
MFI
CI
Yield
Granulate
SA
Xf
Ambient Temp
Shift
M%
Viscosity
Prepare
Slurry
Quarry
X7
X4
X1
X3
X2
X5
X6
Y1
Y2
Y3
X8
pH
Exhibit 9.10 Input/Output Process Map of White Polymer Process

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
359
Exhibit 9.11 Partial View of Table Containing Crisis Team Data
Exhibit 9.12 Description of Variables in CrisisTeamData.jmp
Variable Type
Name
Description
ID
Batch Number Identifying number for slurry batch
Ys
MFI
Melt ﬂow index of the polymer
Cl
Color index of the polymer
Yield
Weight of good polymer as determined by the molding
plant, divided by weight of total polymer produced
Xs
SA
Amps for slurry tank stirrer
M%
Viscosity modiﬁer percent measured in the ﬁller slurry tank
Xf
Percent of ﬁller in the polymer
pH
pH of the slurry
Viscosity
Viscosity of the slurry
Ambient Temp Ambient temperature in the slurry tank area
Quarry
Quarry of origin for ﬁller
Shift
Shift during which batch was processed
Distribution Plots
The ﬁrst thing that your team wants to know is how the process behaved, in
terms of Ys, over the six-week period reviewed by the previous crisis team.
But you ﬁrst run a Distribution analysis for all of the variables (except Batch
Number). In your training, you learned that this is an important ﬁrst step in
any data analysis.

360
V I S U A L S I X S I G M A
To obtain a distribution analysis for all variables of interest (CrisisTeamData.jmp, script is
Distribution):
1.
Select Analyze > Distribution.
2.
In the Select Columns list, select all columns other than Batch Number.
3.
Click Y, Columns.
4.
Click OK.
You review the resulting plots. Recall that, in the VOC analysis, you
learned that MFI should fall between 192 and 198, and CI should be 80 or
higher. The histogram for MFI shows that it varies far beyond the desired limits
(Exhibit 9.13). The histogram for CI shows a signiﬁcant percentage of values
below 80.
Of particular interest are the Yield values that fall below 85 percent. Select
these in the box plot to the right of the Yield histogram. To do this, click and
drag a rectangle that includes these points as shown in Exhibit 9.13.
Exhibit 9.13 Partial View of Distribution Report with Crisis Yield Values Selected

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
361
This action selects the corresponding rows in the data table—check to see
that 14 rows have been selected. Consequently, the values corresponding to
these 14 low-yielding batches are highlighted in the histograms for all of the
variables. For example, you see that the 14 crisis Yield batches have very low SA
values. To remove the selection of the 14 rows, select Rows > Clear Row States.
Looking at the histograms, one of the team members points out that the dis-
tribution of CI is not bell-shaped. You agree and explain that this is not unusual,
given that CI has a natural upper bound of 100 percent. The team is now aware
that they must keep this in mind when using certain statistical techniques, such
as individual measurement control charts and capability analysis, both of which
assume that measured values are normally distributed.
Control Charts
At this point, you are eager to see how the three Ys behave over time. To see
this you construct control charts.
To construct individual measurement control charts for these three responses
(CrisisTeamData.jmp, script is Control Chart Builder):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Drag Batch Number to the Subgroup area (horizontal axis) in the template.
3.
Select MFI, CI and Yield and click New Y Chart below the Select Columns
panel.
Three Individual and Moving Range charts appear.
4.
Click Done to close the Control Panel.
For the most part, MFI seems to be stable. On the Individuals chart for MFI,
there are no points outside the control limits (results not shown). But what
is troubling is that MFI averages 198.4 over this time period. The team’s VOC
analysis indicated that 198 is the upper speciﬁcation limit for MFI!
The control chart for CI immediately shows the problem with applying
a control chart based on normally distributed data to highly skewed data
(Exhibit 9.14). The control limits do not reﬂect the skewed distribution.
Nonetheless, there are indications of special causes. The plot also shows some
very large and regular dips in CI. There are many excursions below the lower
speciﬁcation limit of 80. This leaves the team members puzzled, especially
because the dips do not align with the one crisis period that is so evident in the
Yield control chart (Exhibit 9.15).

362
V I S U A L S I X S I G M A
Exhibit 9.14 Individuals Chart for CI, Crisis Team Data
Exhibit 9.15 Individuals Chart for Yield, Crisis Team Data

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
363
You brieﬂy consider the idea of running capability analyses for these three
responses. However, the Individuals chart clearly shows that MFI is not capable,
since the process average exceeds the upper speciﬁcation limit. Each of CI and
Yield is affected by special causes. The corresponding points could be removed
prior to running capability analyses, but it is not all that easy to tell exactly
which points result from special causes. The team decides that there is not much
to gain by running formal capability analyses on these data.
Now you turn your attention to the crisis team’s modeling efforts. Reading
through the crisis team’s documentation, you see that the team analyzed the
data using multiple regression. The crisis team hoped to determine which Xs
had a signiﬁcant effect on the three key responses. The team identiﬁed M% and
Viscosity as being signiﬁcantly related to MFI, but it did not ﬁnd any of the Xs
to be related to CI.
This last result seems especially curious. You realize that there are several
reasons that such an analysis might lead to no signiﬁcant factors:
◾
One or more key Xs are missing.
◾
One or more higher-order terms involving the speciﬁed Xs are missing.
◾
Measurement variation in the Ys or Xs is too large and is masking the
systematic patterns in the Ys caused by process variation in the Xs.
With this as background, you meet with members of the crisis team to dis-
cuss the reasoning that led to their ﬁnal choice of Xs and Ys. They convince you
that they did not overlook any critical Xs. After this meeting, you reanalyze
their data, introducing higher-order terms. This uncovers some signiﬁcant rela-
tionships, but they don’t seem conclusive in terms of the process, especially as
they relate to CI. At this point, you have a strong suspicion that measurement
variation may be clouding results.
You meet with your team to determine how to proceed. Your team members
agree that it is possible that measurement variation in the Ys or Xs could be large
relative to the systematic variation caused by the Xs. They fully support your
proposal to assess the magnitude of the measurement variation.
MEASUREMENT SYSTEM ANALYSIS (MSA)
As a general principle, measurement systems for key variables should always
be evaluated before engaging in data collection efforts. Since you suspect
that previous attempts to ﬁnd root causes of the polymer problems failed
because of measurement uncertainty, it is all the more important to thor-
oughly study the measurement systems for all the variables identiﬁed by the
process map.

364
V I S U A L S I X S I G M A
You learn that recent routine MSAs indicate that CI, SA, M%, pH, Viscos-
ity, and Ambient Temp are being measured with very capable instruments and
methods. The same is true of polymer weight, which forms the basis for the
Yield calculation. However, the measurement systems for MFI and Xf have not
been evaluated in the recent past. Furthermore, given how these measurements
are made, you realize that they may be prone to problems.
JMP provides two platforms to perform measurement systems analysis,
both found under Analyze > Quality and Process: Measurement Systems
Analysis and Variability/Attribute Gauge Chart. The Measurement Systems
Analysis platform includes the EMP (Evaluate the Measurement Process)
approach.1 This approach is new to you, and you are anxious to try it out for
your team’s MSAs.
MSA for MFI
MFI is measured using a melt ﬂow meter during an ofﬂine laboratory test. Four
instruments are available within the laboratory to perform the test, and there
are three different laboratory technicians who do the testing. There is no formal
calibration for the instruments. When team members interview the technicians
who perform the test, they get the impression that the technicians do not nec-
essarily use a common, standardized procedure.
Properties of a Good Measurement System
You meet with the technicians and their manager to discuss the desired con-
sistency of the measurement process and to enlist their support for an MSA.
You discuss the fact that, for characteristics with two-sided speciﬁcation limits,
a guideline that is often used is that the measurement system range, measured
as six standard deviations, should take up at most 10 percent of the tolerance
range, which is the difference between the upper and the lower speciﬁcation
limits. Since the upper and lower speciﬁcation limits for MFI are 198 and 192,
respectively, the guideline would thus require the measurement system range
not to exceed 10% × (198 – 192) = 0.6 MFI units.2
You also mention that there are guidelines as to how precise the measure-
ment system should be relative to part-to-part, or process, variation: The range
of variability of a highly capable measurement system should not exceed 10
percent of the part-to-part (or, in this case, the batch-to-batch) variability.
Designing the MSA
Given these guidelines, you suggest that an MSA for MFI might be useful,
and the technicians and their manager agree. You learn from the technicians
that the test is destructive. MFI is reported in units of grams per ten minutes.
The protocol calls for the test to run over a half-hour period, with three

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
365
measurements taken on each sample at prescribed times, although due to
other constraints in the laboratory the technicians may not always be available
precisely at these set times. Each of these three measurements is normalized
to a ten-minute interval, and the three normalized values are averaged. From
preparation to ﬁnish, a test usually takes about 45 minutes to run.
Using this information, you design the structure for the MSA. Since the test
is destructive, true repeatability of a measurement is not possible. However, you
reason, and the technicians agree, that a well-mixed sample from a batch can
be divided into smaller samples that can be considered identical. The three tech-
nicians who perform the test all want to be included in the study, and they also
want to have all four instruments included. You suggest that the MSA should
be conducted using samples from three randomly chosen batches of polymer
and that the technicians make two repeated measurements for each batch and
instrument combination.
This leads to 72 tests: 3 batches × 3 technicians × 4 instruments × 2 measure-
ments. Since each test is destructive, a sample from a given batch of polymer
will have to be divided into 24 aliquots for testing. For planning purposes, it
is assumed that the MSA design will permit three tests to be run per hour, on
average, using three of the four instruments. This leads to a rough estimate
of 24 hours for the total MSA. With other work intervening, the technicians
conclude that they can ﬁnish the MSA comfortably in four or ﬁve workdays.
Designing the MSA Experiment
You construct the designed MSA experiment using JMP’s design of experiments
(DOE) capabilities.
To design the experiment for the MSA (script is MSA_MFI_Design.jsl):
1.
Select DOE > Full Factorial Design.
The resulting window is shown in Exhibit 9.16.
2.
Double-click on the response, Y, and rename it MFI.
3.
Click the Categorical button under Factors to add two three-level categorical
factors and one four-level categorical factor.
4.
Rename these factors and specify their values as shown in Exhibit 9.17.
5.
Click Continue.
Note that the default Run Order is set to Randomize.
6.
Insert a value of 1 in the box for Number of Replicates, as shown in Exhibit 9.18.
This is because you want two samples to be run at each of these 36 settings.
7.
Select Make Table.

366
V I S U A L S I X S I G M A
Exhibit 9.16 DOE Full Factorial Design Dialog
Exhibit 9.17 DOE Full Factorial Design Dialog with Response and Factors Speciﬁed
The design table, partially shown in Exhibit 9.19, appears. Most likely, your
table will be different because the run order is randomized.
Note that the Full Factorial Design dialog remains open. This is useful in
case changes need to be made to the design that has been generated. In fact, to
obtain the precise run order shown in Exhibit 9.19, go back to the Full Factorial
Design window. There, select Set Random Seed from the red triangle menu,
enter 123, and click OK. Then select Make Table.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
367
Exhibit 9.18 DOE Full Factorial Design Output Options
Exhibit 9.19 MSA Design Table
When JMP creates the data table, it automatically includes two data table
scripts, Model and DOE Dialog. The Model script speciﬁes a model that you can
ﬁt, while the DOE Dialog script allows you to recreate the dialog that generated
the data table.
Since the purpose of the design is to collect data for a MSA, you decide
that the Model script is not necessary and so you delete it. You retain the DOE
Dialog script in case you want to set up another study like this one.
As mentioned earlier, the runs are randomized. You stress the importance
of running the trials in this order. The technicians conduct the experiment over
the course of the next week and enter their results into the data table.

368
V I S U A L S I X S I G M A
Conducting the Analysis
Your team regroups to analyze the data. The table that contains the design and
results is called MSA_MFI_Initial.jmp.
To conduct the MSA analysis (MSA_MFI_Initial.jmp, script is EMP Analysis):
1.
Select Analyze > Quality and Process > Measurement Systems
Analysis.
Note that MSA Method is set to EMP. If you select Gauge R&R, you obtain
the traditional Variability/Attribute Gauge Chart analysis.
2.
Enter MFI as Y, Response.
3.
Enter Batch as Part, Sample ID.
4.
Enter Operator and Instrument as X, Grouping. See Exhibit 9.20.
5.
Click OK.
Exhibit 9.20 Launch Window for MFI MSA

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
369
Interpreting the Results—The Average and Range Charts
The report displays an Average Chart and a Range Chart (Exhibit 9.21). The
Average Chart plots the average of the repeated MFI measurements and displays
control limits calculated from the variation within subgroups. To get a better
understanding, select Show Data from the Average Chart’s red triangle menu
to display the individual data points. The control limits are those for an X-bar
R chart using the two-measurement subgroups. This means that the limits are
for repeatability variation: the variability when the same Batch is measured by
the same Operator using the same Instrument.
In a good measurement process, the measurement variability is small rela-
tive to the part—in this case, Batch—variation. The measurement system allows
you to distinguish different parts. For a good measurement process, almost all
averages should fall outside the repeatability control limits. But this is not the
case here, indicating that the repeatability variation is large compared to the
Batch variation.
Exhibit 9.21 MSA Average and Range Charts for MFI Initial MSA

370
V I S U A L S I X S I G M A
The Range Chart shows that the repeatability variation is stable. Although
this is a positive result, the level at which the process is functioning is not
acceptable. The Average Range indicates that the mean difference between two
measurements of the same batch by the same operator using the same instru-
ment is 1.1036. The UCL indicates that this could be as large as 3.6050.
Given the fact that the MFI measurement system variability should not
exceed 0.6 units, this signals a big problem. A sample that falls within the spec-
iﬁcation limits can easily give a measured MFI value that falls outside the
speciﬁcation limits. For example, when Bob used Instrument D to measure
Batch 3, one of his measurements, 196.42 (see row 9 of the data table),
indicated that the batch fell within the speciﬁcation limits (192 to 198). But
another measurement, 199.44 (row 62), indicated that it was not within the
speciﬁcation limits. The variability in measurements made on the same batch
by the same operator with the same instrument is too large to permit accurate
assessment of whether product is acceptable.
Interpreting the Results—Parallelism Plots
For additional graphical insight, select Parallelism Plots from the Measure-
ment Systems Analysis red triangle menu (Exhibit 9.22). The Parallelism plots
show the Mean MFI values across Batch for both Operator and Instrument.
These plots help you see Operator differences across batches, Instrument differ-
ences across batches, and possible interactions between Operator and Batch or
Instrument and Batch. Note the following:
◾The Operator plot suggests that Janet’s measurements have higher MFI
than Bob’s. The means of their measurements for the same batches differ
by at least 0.5 units.
◾It appears that there might be an interaction between Instrument
and Batch. By an interaction between Instrument and Batch, denoted
Instrument*Batch, we mean that instruments behave differently across
batches. Here, Instrument C seems to give higher readings for Batch 2
and Batch 3 than would be expected based on the other instruments.
Interpreting the Results—Variance Components
For a more complete picture of the sources of measurement variation, select
EMP Gauge RR Results from the Measurement Systems Analysis red
triangle menu. This report (Exhibit 9.23) shows the standard deviations and
variance components associated with the components of the measurement
process.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
371
Exhibit 9.22 Parallelism Plots for MFI Initial MSA
Exhibit 9.23 EMP Gauge R&R Results for MFI Initial MSA

372
V I S U A L S I X S I G M A
A variance component is an estimate of a variance. The variance components
listed in EMP Gauge R&R Results give estimates of the variance in MFI values
due to:
◾Repeatability: Repeated measurements of the same part by the same oper-
ator with the same instrument.
◾Reproducibility: Repeated measurements of the same part by different
operators using different instruments.
◾Product Variation: Differences in the parts used in the MSA; here parts
are represented by batches.
◾Interaction Variation: Differences due to the interaction components. Here
the interaction components are Operator*Instrument, Operator*Batch,
and Instrument*Batch.
The bar graph in the EMP Gauge R&R Results panel shows that the
Gauge R&R variance component (1.81) is very large compared to the Product
Variation (Part-to-Part) variance component (0.09). This suggests that the
measurement system has difﬁculty distinguishing batches. A control chart
monitoring this process is mainly monitoring measurement variation. The
bar graph also indicates that the repeatability and reproducibility variances,
where Instrument variation is included in the latter, are large and very close in
magnitude, again pointing out that both must be addressed.
The % of Total column shows the following:
◾Both Reproducibility and Repeatability variation stand out as the large
contributors, accounting for 39.9 percent and 46.7 percent of the varia-
tion, respectively.
◾The combined effect of Reproducibility and Repeatability variation is
given by the Gauge R&R component, and is 86.5 percent of the observed
variation when measuring the three batches.
◾Interaction Variation accounts for 9.3 percent of the total variation.
◾The Batch variation (Product Variation) accounts for only 4.2 percent of
the total variation.
Note the following:
◾The variance components for Repeatability and Reproducibility sum to
the variance component for Gauge R&R, which is an estimate of the
measurement process variance.
◾The Total Variation variance component is the sum of the Gauge R&R,
Product Variation, and Interaction Variation variance components.
◾The individual interaction components are given in the Variance Com-
ponents report, which you can select from the red triangle menu.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
373
◾
The Std Dev values are the square roots of the variance components.
◾
The % of Total is calculated from the Variance Component column.
◾
In an MSA, we are typically not directly interested in part-to-part vari-
ation. In fact, we often intentionally choose parts that represent a range
of variability.
Recall that the measurement system range, measured as six standard devi-
ations, should take up at most 10 percent of the tolerance range. Since the upper
and lower speciﬁcation limits for MFI are 198 and 192, respectively, the guide-
line requires that the measurement system range not exceed 10% × (198 – 192)
= 0.6 MFI units.
The Std Dev column in the report indicates that the measurement system
standard deviation is 1.344. Six times 1.344 is 8.064 MFI units! The measure-
ment variation vastly exceeds the desired precision relative to the speciﬁcation
limits.
Also, the range of variability of a highly capable measurement system
should not exceed 10 percent of the Product Variation. Here the Batch standard
deviation is 0.295 and the Gauge R&R standard deviation is 1.344.
Interpreting the Results—Intraclass Correlation
Select EMP Results from the Measurement Systems Analysis red triangle
menu. This gives you the report in Exhibit 9.24. The intraclass correlation (with
bias and interactions) is deﬁned as the variance component of Product Variation
divided by the variance component of Total Variation. In other words, it is the
part variance divided by the sum of the part variance and measurement error
variance. When the measurement error is zero, the intraclass correlation is one.
When the measurement error variation is large compared to the part variation,
the intraclass correlation is small.
The EMP Results report indicates that the Intraclass Correlation is
0.0418, assuming that you include the reproducibility (bias) and interaction
variation inherent in the measurement process. In other words, actual Batch
variation (0.087) is only about 4 percent of the variation that is measured
(2.088).
The EMP Results report goes on to classify this measurement system as a
Fourth Class monitor.3 This means that even a large shift in the true Batch
measurements is unlikely to be detected quickly by a process control chart. For
example, the probability of detecting a large three-sigma shift in the true Batch
values in the next ten subgroups, using the single test of a point beyond the con-
trol limits, is very low (between 0.03 and 0.40, see the Monitor Classification
Legend at the bottom of Exhibit 9.24).

374
V I S U A L S I X S I G M A
Exhibit 9.24 EMP Results Report for MFI Initial MSA
MSA for Xf
Next, your team enlists technicians in conducting an MSA for Xf. The test that
measures Xf, called the ash test, measures the concentration of ﬁller in the ﬁnal
polymer as a percent of total weight. The Xf value reported for a given batch is
the mean of two or three ash measurements per batch. Adjustments are made
to the next batch based on the results of a test on the previous batch. The ash
measurement involves taking a sample of polymer, weighing it, heating it in an
oven to remove all the organic material, then weighing the remaining inorganic
content, which is virtually all ﬁller. The ratio of the ﬁller weight to the initial
weight is the reported result and reﬂects the concentration of ﬁller.
This is a relatively time-consuming test that takes about one hour per
sample, so it is imperative that you take this into consideration when designing
the study. Also, like the test for MFI, the ash test is a destructive test, meaning
that repeated measurements will not be true replicates. However, as in the
MSA for MFI, a sample will be collected from the batch and divided into
aliquots. These aliquots will be considered similar enough to form a basis for
repeatability estimates.
Designing the MSA
A single dedicated oven is used for the test. Three instruments are available
within the laboratory to perform the weight measurements, and there are six
different technicians who do the testing. An MSA involving all six technicians
would be too time-consuming, so three of the technicians are randomly chosen

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
375
to participate. You design a study using samples from three randomly chosen
batches of polymer. These will be measured on each of the three instruments
by each of the three technicians. Again, two repetitions of each measurement
will be taken.
Your design results in a total of 54 tests, with each technician performing
18 tests. At an hour per test, this will take each technician about 18 hours in
total; however, the technicians can complete other work while the samples are
in the oven. The laboratory manager agrees that this is an acceptable amount
of time. The constraint on the duration of the study is the oven, which can be
used for only one sample at a time. So, a two-week period is designated for
the study, with the intent that Xf samples will be worked in between other ash
tests performed as part of the laboratory’s regular work. To come as close to
true repeated measurements as possible, a single sample is taken from each of
the three batches of polymer, and the resulting sample is divided into the 18
required aliquots.
You construct the design in a fashion similar to the way you constructed
the design for MFI. The Xf study is completely randomized. Run the script DOE
Dialog in the data table MSA_Xf_Initial.jmp to review how you designed
the study.
Conducting the Analysis
The study is conducted and the results are recorded in the data table
MSA_Xf_Initial.jmp.
Conduct your analysis as you did for MFI (MSA_Xf_Initial.jmp, script is EMP
Analysis):
1.
Select Analyze > Quality and Process > Measurement Systems
Analysis.
2.
Enter Xf as Y, Response.
3.
Enter Batch as Part, Sample ID.
4.
Enter Operator and Instrument as X, Grouping.
5.
Click OK.
The Average Chart, shown in Exhibit 9.25, indicates that Batches are not
distinguished by the measurement process. Individual Xf measurements range
from about 12 to 18. To see this, select Show Data from the Average Chart’s
red triangle menu. The Range Chart shows that repeated measurements on

376
V I S U A L S I X S I G M A
Exhibit 9.25 MSA Average and Range Charts for Xf Initial MSA
the same batch can differ by up to 2.96 units, almost half of the range of the
observed data.
The EMP Gauge R&R Results are shown in Exhibit 9.26. Repeatability
variation is on a par with Product Variation, and Reproducibility variation is
more than double the Product Variation.
Your team members observe that, given the variability in readings, it is
impossible to differentiate the batches. For example, a team member points out
that measurements made by one of the technicians, Eduardo, using Instrument
B do not distinguish among the three batches. Repeatability variation seems
large as well. Consider Eduardo’s two measurements of Batch 2 using Instru-
ment C—click on the point in the Average Chart and view the selected points
in the data table. (See Exhibit 9.27.) The two measurements differ by about
2.5 units. Moreover, it appears that measurements made with Instrument A
are systematically lower than those made with the other two instruments.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
377
Exhibit 9.26 EMP Gauge R&R Results for Xf Initial MSA
Exhibit 9.27 Eduardo’s Batch 2 Measurements with Instrument C
Since no tolerance range for Xf has ever been determined, whether Xf is
being measured with enough precision is determined by whether measure-
ments can distinguish different batches, in this case the three batches that were
used for the study. (In the production setting, two or three measurements are
typically taken, which increases precision. But your intent in this MSA is to
estimate the precision of a single measurement.)
The EMP Gauge R&R Results indicate that the Gauge R&R variance com-
ponent (1.27) is much larger than the Product Variation variance component
(0.38). As was the case for MFI, this is indicative of a measurement system in
trouble. The Reproducibility and Repeatability variance components are both

378
V I S U A L S I X S I G M A
large. The Gauge R&R standard deviation (1.1278) is almost twice the Product
Variation standard deviation (0.614).
Select EMP Results from the Measurement Systems Analysis red trian-
gle menu (Exhibit 9.28). The EMP Results report indicates that the Intraclass
Correlation (with bias and interactions) is 0.2187. This means that actual
Batch variation accounts for only about 22 percent of the variation that is
measured.
The EMP Results report also indicates that this measurement system is a
Third Class monitor. The probability of detecting a three-sigma shift in the true
Batch values in the next ten subgroups, using the single test of a point beyond
the control limits, is only between 0.40 and 0.88.
Note that the variance components given in the EMP Gauge R&R Results
report can also be obtained using Analyze > Quality and Process > Variability
/ Attribute Gauge Chart or using Fit Model. In the MSA_Xf_Initial.jmp data
table, the scripts Variability Chart and Var Comps Model (see the Random
Effects tab) illustrate how this is done. Although variances are never negative,
sometimes their estimates are negative. Note that, using the script Var Comps
Model, the variance components are estimated using the bounded method so
that negative components are set to zero.
Setting a New Timeline and Fixing the Measurement Systems
The ﬁndings from the MSAs for MFI and Xf deal a major blow to the project
timetable. But the team can’t make progress until these measurement issues are
resolved. Furthermore, based on the results of the MSAs, you determine that
the historical data obtained by the crisis team are largely useless. Your team
needs to collect new data. You estimate that, in total, this will cause at least a
three-month delay to the project—six weeks to address the measurement issues
and then another six weeks to obtain enough new data to analyze.
You explain the situation to your sponsor, Edward. Despite the projected
delay, Edward is very supportive. “So, that means that we have been ﬂying
blind for the last ten years. But at least it explains why we have never been
able to resolve this issue. Go ﬁx the measurement systems and get some good
data and let’s see what it tells us!”
With this guidance, you and the team enlist the help of the technicians and
engineers who know the measurement processes for MFI and Xf. Together you
begin work on improving the measurement systems.
Fixing the MFI measurement process requires that the team address the
root causes of the high repeatability variation, reproducibility variation due to
instrument, and variation due to the instrument and batch interaction. Obser-
vation of the measurement process reveals that, after being removed from the
slurry tank, samples can sit for various lengths of time waiting to be analyzed.

Exhibit 9.28 EMP Results Report for Xf Initial MSA
379

380
V I S U A L S I X S I G M A
Also, the analysis steps can occur in various sequences and suffer different time
delays. It is suspected that this is a major cause of the repeatability variation.
After careful study, a standard operating procedure is developed specifying the
sequence of operations and timeline to be followed in processing the samples.
The other issue, reproducibility, revolves around the four melt ﬂow meters.
Here, a quick examination of the four meters in use shows that two of them are
older units, and that the dies are quite worn. This could account for the differ-
ences in how the instruments measure overall, as well as for the batch-speciﬁc
differences, which are quantiﬁed in the Instrument*Batch interaction. As part
of the improvement strategy, these two units are replaced with new units, and
the set of four is tested and calibrated to ensure consistent readings.
Analysis of the measurement process for Xf by your team and technicians
also reveals two key issues whose root causes need to be addressed. The ﬁrst
of these is oven control. Sometimes technicians have other tasks that make it
difﬁcult to wait until the oven has reached its target temperature or to leave
the sample in for the prescribed length of time. This explains the repeatability
issues that surfaced in the MSA.
A two-part solution is proposed and approved. First, an oven probe, with a
portable remote monitor capable of broadcasting alerts, is purchased. In addi-
tion, work assignments are reorganized so that a technician is always in the
area of the oven when a test is being run.
The second issue relates to the instrument reproducibility problem. Your
team learns that the scales that were being used to weigh the ﬁller were archaic
analog scales. They are replaced by high-precision digital scales. Finally, to con-
trol other sources of variation, standardized operating procedures for testing Xf
are developed and implemented with the help of the technicians.
Follow-Up MSAs for MFI and Xf
After implementing these improvements to both measurement systems, the
team designs follow-up measurement analysis system studies.
Follow-Up for MFI
The follow-up MSA for MFI has the same structure as the initial study. The
results are given in the table MSA_MFI_Final.jmp. The Average and Range
Charts are shown in Exhibit 9.29 and the EMP Gauge R&R Results are given
in Exhibit 9.30. The script is EMP Analysis.
The team members and technicians look at the Average Chart and are
delighted! Measurements clearly distinguish the three parts. All measurements
fall beyond the control limits and, for a given batch, they are very close. The
Range Chart indicates that an upper control limit on the range of measurements
is 0.4083. There is very little repeatability or reproducibility variation evident.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
381
Exhibit 9.29 MSA Average and Range Charts for MFI Final MSA
The Gauge R&R Std Dev, given in the EMP Gauge R&R Results report,
is 0.105 (Exhibit 9.30). This means that the measurement system only takes up
about 6(0.105) = 0.63 units, which is almost exactly 10 percent of the tolerance
range (recall that the speciﬁcation limits are 192 and 198). The measurement
system is now sufﬁciently reliable in classifying batches as good or bad relative
to the speciﬁcation limits on MFI.
Also, note that the Intraclass Correlation (with bias and interactions) is
0.9954 (Exhibit 9.31). This value indicates that there is very little measurement
Exhibit 9.30 EMP Gauge R&R Results for MFI Final MSA

382
V I S U A L S I X S I G M A
Exhibit 9.31 EMP Results for MFI Final MSA
variation relative to batch variation. The EMP Results report also indicates that
the current system is a First Class monitor. This means that a control chart for
MFI will signal within ten subgroups with probability of at least 0.99, based on a
single point beyond the control limits, if there is shift in the mean that exceeds
three standard errors.
Shift Detection Profiler
You can investigate the sensitivity of a control chart for MFI in more detail
by selecting the Shift Detection Profiler from the topmost red triangle menu
in the report. The Shift Detection Profiler, which estimates the probability
of detecting shifts in the process mean, appears as shown in Exhibit 9.32
(MSA_MFI_Final.jmp).
The control limit calculations for the chart include the sources of measure-
ment variation. For the part variation, the control limit calculations use the
In Control Part Std. Dev. This is initially set to the part standard deviation as
estimated from parts used in the MSA. But because parts for MSAs are often
not selected at random from process output, you can set the In Control Part
Std Dev to an appropriate value using a red triangle option.
The Profiler shows six cells:
◾Number of Subgroups: The number of subgroups over which the proba-
bility of a warning is computed. This is set to 10 by default.
Exhibit 9.32 Shift Detection Profiler—Initial View

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
383
◾
Part Mean Shift: The shift in the part mean that you want the control
chart to detect. The initial value is set to one standard deviation of the
part variation estimated by the MSA analysis. (See Product Variation
in the EMP Gauge R&R Results report, Exhibit 9.30.)
◾
Part Std Dev: The value of the part standard deviation for new points.
The initial value is set to one standard deviation of the part variation
estimated by the MSA. (See Exhibit 9.30.) You can set the Part Std Dev
value to reﬂect changes in the process.
◾
Bias Factors Std Dev: The standard deviation of factors related to
reproducibility, including operator and instrument variability. (See
Exhibit 9.30.)
◾
Test-Retest Std Dev: The standard deviation of the test-retest, or repeatabil-
ity, variation in the model. The initial value is the standard deviation of
the Repeatability component estimated by the MSA. (See Exhibit 9.30.)
◾
Subgroup Size: The sample size for each subgroup. This is set to 1 by
default.
The initial settings of the Shift Detection Profiler are shown in Exhibit 9.32.
These settings indicate that, given the current process, the probability of detect-
ing a one standard deviation shift in the mean in the next 10 subgroups, using
an individual measurements control chart (Subgroup Size = 1), is about 0.205
(Probability of Warning = 0.204815).
What if you were to monitor the process with an Xbar and S chart using
a Subgroup Size of 5? Slide the vertical bar in the rightmost cell to 5. Then
the probability of seeing a warning in the next 10 subgroups is 0.917. (See
Exhibit 9.33.)
You can also explore other scenarios, such as the consequences of reducing
part or measurement variation. What if you were able to reduce the In-Control
Part Std Dev to 1.2? Select the option to Change In-Control Part Std Dev
from the red triangle menu for the Profiler. Also, click above Part Std Dev in
Exhibit 9.33 Shift Detection Profiler, Subgroup Size 5

384
V I S U A L S I X S I G M A
Exhibit 9.34 What-If Scenario for Shift Detection Profiler
the third cell and set that standard deviation to 1.2. Change the Subgroup Size
to 4. (See Exhibit 9.34.) You learn that the probability of detecting the 1.5374
mean shift in the next 10 subgroups, using an X-bar chart based on subgroups
of size 4, is about 0.981.
The Shift Detection Profiler is a versatile tool, enabling you to interactively
explore various scenarios relating to further changes or improvements in how
MFI is measured, how the process changes, and how the chart itself is con-
structed. See Help > Books > Quality and Process Methods for more details.
Follow-Up for Xf
As for Xf, the follow-up MSA is conducted with the three technicians who were
not part of the original study. The results are given in MSA_Xf_Final.jmp. The
script is EMP Analysis.
The Average and Range Charts show that the measurement system now
clearly distinguishes among the parts. The EMP Gauge R&R Results are shown
in Exhibit 9.35. The bar graph shows that most of the variation is due to the
product.
Once again, the team members and technicians are pleased. Compared to
the variability among the three batches, there is very little repeatability or repro-
ducibility variation.
The Gauge R&R Std Dev is 0.034, compared to the Product Variation Std
Dev of 0.292. The EMP Results report indicates that the Intraclass Correlation
Exhibit 9.35 EMP Gauge R&R Results for Xf Final MSA

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
385
(with bias and interactions) is 0.982, indicating that 98.2 percent of the
observed variation is due to part (results not shown).
To ensure that both the MFI and Xf measurement systems continue to
operate at their improved levels, measurement control systems are introduced
in the form of periodic checks, monthly calibration, semiannual training, and
annual MSAs.
UNCOVERING RELATIONSHIPS
With reliable measurement systems in place, your team now embarks on the
task of collecting meaningful process data. The team members collect data on
all batches produced during a ﬁve-week period. They measure the same vari-
ables as were measured by the crisis team, with the assurance that these new
measurements have greater precision.
Your analysis plan is to do preliminary data exploration, to plot control charts
forMFIandCI,tocheckthecapabilityofthesetworesponses,andthentoattempt
to uncover relationships between the Xs and these two Ys. You keep your Visual
Six Sigma Roadmap, repeated in Exhibit 9.36, clearly in view at all times.
Exhibit 9.36 Visual Six Sigma Roadmap
Visual Six Sigma Roadmap—What We Do
Uncover Relationships
Dynamically visualize the variables one at a time
Dynamically visualize the variables two at a time
Dynamically visualize the variables more than two at a time
Visually determine the Hot Xs that affect variation in the Ys
Model Relationships
For each Y, identify the Hot Xs to include in the signal function
Model Y as a function of the Hot Xs; check the noise function
If needed, revise the model
If required, return to the Collect Data step and use DOE
Revise Knowledge
Identify the best Hot X settings
Visualize the effect on the Ys should these Hot X settings vary
Verify improvement using a pilot study or conﬁrmation trials
Visualizing One Variable at a Time
The new data are presented in the data table VSSTeamData.jmp.
Distribution Plots
Your ﬁrst step is to run a Distribution analysis for all of the variables except
Batch Number.

386
V I S U A L S I X S I G M A
The ﬁrst ﬁve histograms are shown in Exhibit 9.37 (script is Distribution).
You note the following:
◾MFI appears to have a mound-shaped distribution, except for some val-
ues of 206 and higher.
◾CI is, as expected, left-skewed.
◾Yield is also left-skewed, and may exhibit some outliers in the form of
low values.
You select those points that reﬂect low Yield values, speciﬁcally, those
ﬁve points that fall below 60 percent, by clicking and drawing a rectangle
that includes these points using the arrow tool inside the box plot area. The
highlighting in the other histograms in Exhibit 9.38 indicates that these ﬁve
very low Yield rows correspond to four very high MFI values, one very low
CI value, and generally low SA values. This is good news, since it is consistent
with knowledge that the crisis team obtained. It also suggests that crisis yields
are related to Ys, such as MFI and CI, and perhaps inﬂuenced by Xs, such as
SA. Interestingly, though, four of these rows have CI values that exceed the
lower speciﬁcation limit of 80.
A striking aspect of the histograms for MFI and CI is the relationship of
measurements to the speciﬁcation limits. Recall that MFI has lower and upper
speciﬁcation limits of 192 and 198, respectively, and that CI has a lower speci-
ﬁcation limit of 80.
From the Quantiles panel for MFI, notice that all 110 observations
exceed the lower speciﬁcation limit of 192 and that about 50 percent of
these exceed the upper speciﬁcation of 198. From the Quantiles panel for CI,
notice that about 25 percent of CI values fall below the lower speciﬁcation of
80. To see how often both MFI and CI meet the speciﬁcation limits, you use
the Data Filter.
To select the points for which the speciﬁcations on MFI and CI are jointly met
(VSSTeamData.jmp, script is Data Filter):
1.
Select Rows > Data Filter.
2.
Select MFI from the Add Filter Columns list and click Add.
3.
Click on the maximum value of 209.350 above the MFI slider and change it to 198.
This selects all rows where MFI meets the speciﬁcation limits.
4.
Click on AND in the bottom left corner of the Data Filter dialog.
5.
Select CI from the Add Filter Columns list and click Add.
6.
Click on the minimum value of 21.90 above the CI slider and change it to 80.
7.
Click Enter (or click outside the text box).

Exhibit 9.37 Five of the Eleven Distribution Reports
387

Exhibit 9.38 Distribution Reports with Five Crisis Yield Values Selected
388

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
389
Exhibit 9.39 Data Filter Dialog to Select In-Speciﬁcation Batches
The completed Data Filter dialog is shown in Exhibit 9.39. The Data Filter
panel indicates that only 34 rows match the conditions you have speciﬁed. This
means that only 34 out of 110 batches conform to the speciﬁcations for both
responses.
All rows that meet both speciﬁcation limits are highlighted in all of the
plots obtained using the Distribution platform (Exhibit 9.40). As you review the
plots, you notice that the batches that meet the joint MFI and CI speciﬁcations
tend to result in Yield values of 85 percent and higher. They also tend to be in
the upper part of the SA distribution and the lower part of the M% and Ambient
Temp distributions. This suggests that there might be relationships between the
Xs and these two Ys.
Before you close the Data Filter dialog, you click the Clear button
at the top of the dialog to remove the Select row states imposed by the
Data Filter.
Control Charts
With this as background, the team proceeds to see how the three Ys behave
over time. As before, you construct individual measurement charts for these
three responses using the Control Chart Builder. But, this time, you start by
constructing an Individuals chart for MFI and then use the Column Switcher
to construct charts for CI and Yield.

Exhibit 9.40 Points That Meet Both MFI and CI Speciﬁcations
390

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
391
Exhibit 9.41 Column Switcher
To use the Column Switcher from the MFI individual measurement control chart to plot CI
and Yield (VSSTeamData.jmp, script is Control Charts):
1.
Create the individual measurement control chart for MFI as shown earlier.
2.
From the red triangle menu next to Control Chart Builder, select Script >
Column Switcher.
3.
From the Choose column to switch list, select MFI and click OK.
4.
From the Choose set of columns to switch to list, select MFI, CI, and Yield
and click OK.
5.
Select the Ys from the Column Switcher list to view each control chart in turn. See
Exhibit 9.41.
These control charts are quite informative (see Exhibit 9.42). You learn that
the average yield is roughly 87 percent, and that there are ﬁve batches below
the lower control limit. Four of these are consecutive batches. You see that the
grouping of four batches with low yields greatly exceed the upper control limit
for MFI.
From the control chart for CI you learn that the ﬁfth yield outlier, cor-
responding to row 14, falls below the lower control limit. However, you are
reminded that the control limits for CI are suspect because of the extreme
non-normality of its distribution.
You decide that it might be a good idea to assign markers to the ﬁve batches
that are Yield outliers in order to identify them in subsequent analyses.
To mark these ﬁve batches in future analyses (VSSTeamData.jmp, script is Markers for
Outliers):
1.
Make sure that these ﬁve points are selected in the Yield control chart.
2.
Select Rows > Markers and choose square markers.

392
V I S U A L S I X S I G M A
3.
Select Rows > Colors and choose a bright red color for the square markers.
4.
Deselect the selected rows by clicking in an empty part of the control chart.
Exhibit 9.42 shows the three control charts with the markers you con-
structed for the ﬁve crisis batches.
Visualizing Two Variables at a Time
One of your team members observes that the MFI and Yield control charts
gave advance warning of the crisis in the form of a trend that began perhaps
around batch 4,070. Had MFI been monitored by a control chart, this trend
might have alerted engineers to an impending crisis. You agree but observe
that this assumes a strong relationship between MFI and Yield.
This is as good time as any to see if there really is a strong relationship
between MFI and Yield and CI and Yield. You also want to explore other bivari-
ate relationships.
Relationships between the Ys
MFI and Yield
To explore the relationship between MFI and Yield, you use Graph Builder.
To investigate the relationship between MFI and Yield (VSSTeamData.jmp, script is
Graph Builder—Yield and MFI):
1.
Select Graph > Graph Builder.
2.
Select MFI from the Variables list and and drag it to the X zone.
3.
Select Yield from the Variables list and drag it to the Y zone.
4.
Click Done to close the Control Panel.
The points and a default smoother are plotted (Exhibit 9.43). If you wish,
you can deselect the smoother by clicking the second icon from the left at the
top of the plot before you click Done (or, right-click on the graph and select
Smoother > Remove).
The plot suggests that there is a strong nonlinear relationship between these
two Ys. Four of the ﬁve outliers clearly suggest that high MFI values are associ-
ated with crisis level yields. You recognize the outlier with a Yield of about 40
as row 14, the point with the low CI value identiﬁed on the control chart for
CI. (If you hover over this point, its row number, MFI, and Yield will appear.)

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
393
(a)
(b)
(c)
Exhibit 9.42 Control Charts with Markers

394
V I S U A L S I X S I G M A
Exhibit 9.43 Graph Builder Plot of Yield by MFI
CI and Yield
You follow the instructions above, replacing MFI with CI, to construct a plot of
Yield versus CI (see Exhibit 9.44—the script is Graph Builder—Yield and CI).
The plot shows a general tendency for Yield to increase as CI increases. Four
of the ﬁve outliers have high values of CI. The outlier with the low value of CI
is row 14.
MFI and CI
Now you investigate the relationship between MFI and CI. You construct the
plot shown in Exhibit 9.45, replacing Yield with MFI (the script is Graph
Builder—MFI and CI).
MFI and CI seem to have a weak positive relationship. There may be pro-
cess factors that affect both of these Ys jointly. In the next section, we look at
relationships between the process factors and these responses.
Relationships between Ys and Xs
As a more efﬁcient way to view bivariate relationships, you decide to create
a scatterplot matrix of all responses (Ys) with all factors (Xs), shown in
Exhibit 9.46.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
395
Exhibit 9.44 Graph Builder Plot of Yield by CI
Exhibit 9.45 Graph Builder Plot of Yield by CI

Exhibit 9.46 Scatterplot Matrix of Ys by Xs
396

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
397
To construct the Scatterplot Matrix (VSSTeamData.jmp, script is Scatterplot
Matrix 1):
1.
Select Graph > Scatterplot Matrix.
2.
Enter MFI, CI, and Yield as Y, Columns.
3.
Enter all of the variables from SA to Shift as X.
4.
Click OK.
5.
From the red triangle menu in the report, select Density Ellipses > Shaded
Ellipses.
6.
By dragging the axes, resize them to make each cell include the entire density ellipse.
The matrix shows a scatterplot for each Y and X combination, including
scatterplots that involve the two nominal variables, Quarry and Shift. For the
nominal values involved in these scatterplots, the points are jittered randomly
within the appropriate level. Density ellipses assume a joint bivariate normal
distribution, so they are not shown for the plots involving Quarry and Shift
because these factors are nominal.
The ﬁve outliers appear prominently in the MFI and Yield scatterplots. In
fact, they affect the scaling of the plots, making it difﬁcult to see other relation-
ships. To better see the other points, select the ﬁve points in one of the Yield
plots, then right-click in an empty part of the plot and choose Row Hide and
Exclude from the menu that appears. (Alternatively, run the script Hide and
Exclude Outliers.) Notice the following:
◾
Excluding the points causes the ellipses to be automatically recalculated.
◾
Hiding the points ensures that they are not shown in any of the plots.
Check the Rows panel in the data table to verify that the ﬁve points are
excluded and hidden. Rescale the vertical axes to remove whitespace and
accommodate all density ellipses. The updated scatterplot matrix is shown in
Exhibit 9.47 (script is Scatterplot Matrix 2).
Viewing down the columns from left to right, you see evidence of moderate-
to-strong relationships between:
◾
MFI and SA
◾
MFI and M%
◾
MFI and CI (and Yield) and Xf
◾
MFI and Ambient Temp

Exhibit 9.47 Scatterplot Matrix of Ys by Xs with Five Outliers Excluded
398

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
399
Exhibit 9.48 Graph Builder Plot of CI by Xf
The relationship between CI and Xf appears to be highly nonlinear. To
see this relationship more clearly, you again use the Graph Builder (see
Exhibit 9.48—the script is Graph Builder—CI and Xf).
Everyone on your team notes the highly nonlinear relationship. An engi-
neer on your team observes that the relationship is not quadratic. She speculates
that it might be cubic. You also observe that both low and high values of Xf are
associated with CI values that fail to meet the lower speciﬁcation limit of 80.
The engineer also states that the relationship between MFI and SA might
also be nonlinear, based on the underlying science. In a similar fashion, you
construct a plot for these two variables using Graph Builder (script is Graph
Builder—MFI and SA). The plot is shown in Exhibit 9.49.
The relationship does appear to be nonlinear. The bivariate behavior sug-
gests that the underlying relationship might be quadratic or even cubic.
One team member starts speculating about setting a speciﬁcation range on
Xf, maybe requiring Xf to fall between 14.0 and 15.5. You point out that in
fact the team must ﬁnd operating ranges for all of the Xs. You caution that
setting these operating ranges one variable at a time is not a good approach.
The operating ranges have to simultaneously satisfy speciﬁcation limits on two

400
V I S U A L S I X S I G M A
Exhibit 9.49 Graph Builder Plot of MFI by SA
Ys, both MFI and CI. A statistical model relating the Xs to the Ys would reveal
appropriate operating ranges and target settings for the Xs. This brings the team
to the Model Relationships step of the Visual Six Sigma Data Analysis Process.
MODELING RELATIONSHIPS
The data exploration up to now has revealed several relationships between the
two Ys of primary interest, MFI and CI, and some of the Xs. In particular, Xf
is related to both MFI and CI while SA, M%, and Ambient Temp appear to be
related to MFI. The relationships between CI and Xf and between MFI and SA
appear to be nonlinear.
At this point, your team embarks on the task of modeling the relationships
in a multivariate framework.
Dealing with the Preliminaries
Your ﬁrst issue involves determining how to deal with the ﬁve MFI outliers.
Your goal is to develop a model that is descriptive of the process operating

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
401
under common cause conditions. Your main concern about these ﬁve points
is whether they are the result of the common cause system that you want to
model. If the ﬁve points are consequences of a different (special cause) failure
mode, their inclusion in the modeling process could result in a less useful model
for the common cause process of interest.
The most effective course of action relative to outliers of this kind is to iden-
tify and remove the special causes that may have produced them. This is usually
best accomplished in real time, when the circumstances that produce an outlier
are fresh in peoples’ minds.
Looking back at the histograms and control charts for MFI, you see that four
of these points are very much beyond the range of most of the MFI measure-
ments. In fact, they are far beyond the speciﬁcation limits for MFI. The ﬁfth
batch is potentially a CI outlier. The fact that all ﬁve points are associated with
extremely low yields also suggests that they are not typical of operating condi-
tions. This evidence seems to suggest that a different set of causes is operative
for the ﬁve outliers.
Given these ﬁndings, you are concerned that the outliers might be detri-
mental in developing a model for the common cause system. With this as your
rationale, you decide to exclude the ﬁve crisis observations from the model
development process.
However, we suggest the following as an exercise for the reader. Develop
models for MFI and CI that include these ﬁve rows. Check to see if the ﬁve
points are inﬂuential, using visual techniques and statistical measures such as
Cook’s D. Determine how your conclusions would change if you had taken this
modeling approach.
Excluding and Hiding the Crisis Rows
The ﬁve crisis observations have already been excluded and hidden in connec-
tion with your scatterplots for the three responses. You can check the Rows
panel of the data table to make sure that ﬁve points are Excluded and Hidden.
However, if you have cleared row states since then, reselect the outliers as
shown in Exhibit 9.50, right-click in the plot, and select Row Hide and Exclude
(the script is Hide and Exclude Outliers).
Saving the Specification Limits as Column Properties
In the interests of expediency for subsequent analyses, you decide to store the
speciﬁcation limits for MFI and CI in the data table. You do this by entering the
information in the Spec Limits column property for each column.

402
V I S U A L S I X S I G M A
Exhibit 9.50 Selecting the Five Outliers to Hide and Exclude
To save the speciﬁcation limits for MFI as a column property (VSSTeamData.jmp, script for
both MFI and CI is Spec Limits):
1.
Right-click in the MFI column header and choose Column Properties > Spec
Limits.
2.
In the Spec Limits panel, enter 192 as Lower Spec Limit, 195 as Target,
and 198 as Upper Spec Limit.
See Exhibit 9.51. Do not click the box next to Show as graph reference
lines. Seeing the spec limits on control charts can be misleading.
3.
Click OK.
4.
Proceed in a similar fashion for CI, recording only a lower speciﬁcation limit of 80.
In the Columns panel of the data table, you see that asterisks have appeared
to the right of CI and MFI. Click on one of the asterisks to see that the Spec
Limits property is listed (Exhibit 9.52). If you click on Spec Limits, the Column
Info window opens and displays the Spec Limit property panel.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
403
Exhibit 9.51 Spec Limits Column Property Panel for MFI
Exhibit 9.52 Asterisk Showing Spec Limits Column Property

404
V I S U A L S I X S I G M A
Plan for Modeling
Now you are ready to build your model. You have seen evidence of nonlinearity
between CI and Xf and between MFI and SA. How can you build this into a
model?
You decide to get some advice on how to proceed from your mentor, Tom.
Over lunch together, Tom suggests the following:
◾Build a model for each of MFI and CI as responses using Fit Model.
◾In the initial model, include response surface terms for all effects.
Quadratic terms for the nominal effects do not make sense and will
not be added. However, interaction terms involving the nominal terms
will be added. Also include cubic terms in Xf and SA, based on the
nonlinearity suggested by your bivariate analysis.
◾Construct the models using the Stepwise personality. Tom shows you a
quick example.
◾Construct models for MFI and CI using the Hot Xs that you have
identiﬁed.
◾Use the Profiler to simultaneously optimize these models, thereby
obtaining settings of the Hot Xs that optimize both MFI and CI.
◾Quantify the anticipated variability in both responses, based on the likely
variation exhibited by the Hot Xs in practice.
Tom’s plan makes sense to you, and you proceed to implement it.
Building the Model
Your ﬁrst step is to deﬁne your initial model. It will contain main effects,
two-way interactions, quadratic effects for all continuous variables, and cubic
terms in Xf and SA.
To ﬁt your initial model (VSSTeamData.jmp, script for completed Model Speciﬁcation
window is Model—Initial):
1.
Select Analyze > Fit Model.
2.
Enter MFI and CI as Y.
This will produce a ﬁt for each response in a single report.
3.
In the Select Columns list, select the columns from SA to Shift.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
405
4.
From the Macros menu, select Response Surface.
This selection adds all main effect, two-way interactions, and all quadratic terms in
the continuous variables. In the Construct Model Effects list, the notation & RS follows
each continuous main effect, indicating that response surface terms for that effect have
been added. There are 70 effects in all.
5.
Select SA and Xf in the Select Columns list.
6.
Enter 3 in the Degree text box beneath the Macros button.
7.
From the Macros menu, select Polynomial to Degree.
This selection adds cubic terms in Xf and SA.
8.
From the Personality menu, select Stepwise.
9.
Click Run.
Stepwise Variable Selection
The Stepwise report appears (Exhibit 9.53), showing two main outline nodes,
one entitled Stepwise Fit for MFI and the other entitled Stepwise Fit for CI (the
Current Estimates panels have been minimized). The Stepwise Regression
Control panel in each of these reports gives you control over how stepwise
selection is performed. You decide to accept the JMP default settings, which
specify a Minimum BIC stopping rule, the Forward direction, and the Combine
rule that combines effects in determining their signiﬁcance.
The Bayesian Information Criterion, or BIC, is a measure of model ﬁt based
on the likelihood function. It includes a penalty for the number of parameters
in the model. A lower value of BIC indicates a better model. For details on
the BIC and other selections, see www.jmp.com/support/help/Fitting_Linear_
Models.shtml.
To use Stepwise to reduce your initial models for MFI and CI (VSSTeamData.jmp,
script is Stepwise Models):
1.
In the Stepwise Regression Control panel for MFI, click Go.
Model terms are selected in accordance with the control panel settings.
2.
Click Make Model.
A Model Speciﬁcation window for MFI appears.
3.
In the Stepwise Regression Control panel for CI, click Go.
4.
Click Make Model.
A Model Speciﬁcation window for CI appears.

406
V I S U A L S I X S I G M A
Exhibit 9.53 Stepwise Fit Window with Current Estimates Outlines Closed
Model Speciﬁcation windows for MFI and CI are shown in Exhibit 9.54.
The Hot Xs for MFI are SA, M%, and Xf. The only Hot X for CI is Xf. You
observe that SA and its quadratic and cubic effects appear in the model for MFI
and that Xf and its quadratic and cubic effects appear in the model for CI.
Checking and Revising Stepwise Models
Next, you run each model to check the ﬁt, starting with the model for MFI. In
the Model Speciﬁcation window for MFI, you click Run. Exhibit 9.55 shows the
Effect Summary report and the Actual by Predicted Plot.
In the Actual by Predicted plot, you see a point that might be considered
an outlier. Given its location, though, it doesn’t appear to have high inﬂuence
on the model ﬁt. You don’t see any other issues with the Actual by Predicted
Plot, so you proceed to look at the effect tests.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
407
(a)
(b)
Exhibit 9.54 Models Obtained Using Stepwise
The Stepwise Stopping Rule, Minimum BIC, can result in a model contain-
ing terms that are not active or signiﬁcant. At this point, you reduce the model
by removing terms that are not signiﬁcant.
In the Effect Summary report, you notice that the interaction effect M%*Xf
is not signiﬁcant. Click on M%*Xf in the Source list to select it, as shown in

408
V I S U A L S I X S I G M A
Exhibit 9.55 Partial Model Fit Report for MFI
Exhibit 9.56 Effect Summary Panel for MFI
Exhibit 9.56. Then click Remove. This removes the effect from the model and
updates all the reports in the window. Now all remaining effects are signiﬁcant.
You want to save the model you have constructed to the data table as doc-
umentation for your analysis. To do this, from the Response MFI report’s red
triangle menu, select Script > Relaunch Analysis. Then save the model script
to the data table. (The script for the saved model is Model for MFI.)

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
409
Exhibit 9.57 Partial Model Fit Report for CI
Next, run the Stepwise model for CI. Exhibit 9.57 shows the Effect Sum-
mary report and the Actual by Predicted Plot.
Based on the Actual by Predicted plot, the model seems to ﬁt adequately.
Because there is only one variable, namely Xf, in the model, JMP provides a
Regression Plot. This shows how the model ﬁts CI as a function of Xf and
gives you an additional view of how the model ﬁts. You see that the nonlinearity
seems to be well modeled.
The Effect Summary report shows that all three effects are signiﬁcant. The
script for this model is saved to the data table as Model for CI.

410
V I S U A L S I X S I G M A
All Possible Models
This seemed easy. You realize that your initial model has a large number of terms
(70, to be exact, not counting the intercept). With that many terms, there are
2^70 possible models (with intercept). This is a huge number, on the order of
10^21. Stepwise takes a selected path through the space of possible models that
is based on p-values. This can be limiting.
You’re tempted to do a quick check of your newfound models against other
models that could be considered, using the All Possible Models option. You
know that you will have to limit the search somehow because the total number
of all possible models is daunting.
To obtain ﬁts for a subset of all possible models for each of MFI and CI
(VSSTeamData.jmp, scripts for sorted data tables are All Possible Models for MFI
and All Possible Models for CI):
1.
From the Stepwise Fit red triangle menu, select All Possible Models.
2.
Enter 6 as the Maximum number of terms in a model.
Asking for more terms results in too many models for JMP to process.
3.
Check the box next to Restrict to models where interactions imply lower
effects (Heredity Restriction).
This selection enforces strong effect heredity.
4.
Click OK.
5.
Click OK in the JMP Alert that appears.
6.
Click Continue in the next JMP Alert that asks if you are sure that you want to ﬁt 10
million models.
Be patient as the calculation may take a while.
7.
When the All Possible Models report appears, right-click in the report and select
Sort by Column.
8.
In the Select Columns window, select BIC, click Ascending, and then click OK.
This sorts the report by BIC in ascending order.
9.
Right-click in the All Possible Models report and select Make into Data
Table.
For MFI, the model with the smallest BIC, in row 1, is precisely the
ﬁve-term model that you derived from the Stepwise model, once you
removed the insigniﬁcant M%*Xf terms. For CI, the model with the smallest
BIC, in row 1, is precisely the model selected by Stepwise.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
411
Exhibit 9.58 Plot of BIC versus Number of Terms for MFI
You can view the model information in the data tables produced by
All Possible Models graphically by constructing a Graph Builder plot of BIC
versus Number. The tables created by the scripts All Possible Models for MFI
and All Possible Models for CI contain Graph Builder scripts to construct
these plots. Exhibit 9.58 shows the plot for MFI, with your ﬁnal model selected.
Your All Possible Models analysis conﬁrms that the terms that you
included in your models for MFI and CI are appropriate.
Saving the Prediction Formulas
Your plan is to obtain a prediction equation for each response and then to use
the proﬁler to conduct multiple optimization.
To save the prediction formula for MFI to the data table (VSSTeamData.jmp, script is Pred
Formula for MFI):
1.
Run the script Model for MFI.
2.
Click Run.
3.
From the red triangle menu next to Response MFI, select Save Columns >
Prediction Formula.

412
V I S U A L S I X S I G M A
A column called Pred Formula MFI is added to the data table. To view the
formula, right-click in the column header area and select Formula. This opens
a formula editor box that shows the prediction equation.
Alternatively, in the Columns panel, the Pred Formula MFI column is now
listed. To the right of the column name, you see a plus sign. Click on the plus
sign to open the formula editor window displaying the formula.
Save the prediction formula for CI in the same fashion. The saved script is
Pred Formula for CI.
REVISING KNOWLEDGE
At this point, you have constructed models for MFI and CI and you have identi-
ﬁed the relevant Hot Xs. You and your team are ready to proceed to the Revise
Knowledge step of the Visual Six Sigma Data Analysis Process. You will identify
optimal settings for the Hot Xs, evaluate process behavior relative to variation in
the Hot Xs using simulation, and run conﬁrmatory trials to verify improvement.
Using the Profiler for Multiple Optimization
With both prediction formulas saved to the data table, you are ready to ﬁnd
settings for the Xs that will simultaneously optimize MFI and CI. Use Graph >
Profiler to conduct multiple optimization.
Construct the Prediction Profiler plot in Exhibit 9.59 as follows (VSSTeamData.jmp,
script is Profiler):
1.
Select Graph > Profiler.
2.
Select Pred Formula MFI and Pred Formula CI and enter these as Y,
Prediction Formula.
3.
Click OK.
From your training, you recall that each prediction formula deﬁnes a
response surface. The Profiler shows cross-sections, called traces, of both
response surfaces, with the ﬁrst row of panels corresponding to MFI and the
second row corresponding to CI. The cross-sections are given for the designated
values of SA, M%, and Xf. You can drag the vertical red dotted lines using the

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
413
Exhibit 9.59 Initial Profiler Report
arrow tool to see how the two predicted responses change at various factor
settings.
Consider the ﬁrst row of plots, which relate to the prediction model for MFI.
The three continuous factors SA, M%, and Xf deﬁne a response surface in four
dimensions: There are three factor values, and these result in one predicted
value. Given the settings in Exhibit 9.59, the Profiler is telling us, for example,
that when M% = 1.75 and Xf = 15.75, the predicted MFI values, for various
settings of SA, are those given in the panel above SA. In other words, for speciﬁc
settings of any two of the continuous factors, the proﬁler gives the cross-section
of the response surface for the third continuous factor.
Consider the second row of plots, corresponding to Pred Formula CI. The
traces for SA and M% are perfectly horizontal. This is because SA and M% do
not appear in the prediction formula for CI. The plot in the second row in the
cell above Xf shows predicted values of CI for various values of Xf.
Understanding the Profiler Traces
It is getting late in the day, and the idea of visualizing these traces by plotting a
response surface sounds like a wonderful diversion. You suspect that this will
help you explain to your team members what these traces represent. You are

414
V I S U A L S I X S I G M A
especially interested in visualizing the interesting trace for MFI above SA in the
Prediction Profiler in Exhibit 9.59.
To visualize the trace for SA and to construct the plot in Exhibit 9.60 (VSSTeamData.jmp,
script is Surface Plot—SA and Xf):
1.
Select Graph > Surface Plot.
2.
Enter Pred Formula MFI, SA, M%, and Xf as Columns.
3.
Click OK.
A surface plot appears.
4.
In the Independent Variables panel, select SA as X and Xf as Y.
5.
Set M% to 1.75, its value in the Prediction Profiler.
6.
Set Xf to 15.75, its value in the Prediction Profiler.
7.
Check the box for Grid next to Xf.
8.
In the Dependent Variables panel, click the color box to the right of Pred
Formula MFI.
9.
In the menu that appears, under Fill Type, select Continuous Gradients.
This applies a gradient coloring to Pred Formula MFI.
10.
Click OK.
11.
Click and drag your cursor in the plot to rotate it.
The vertical grid cuts the surface in a curved shape. This is precisely the
trace shown for SA in the proﬁler in Exhibit 9.59 when Xf = 15.75 and
M% = 1.75.
This exercise in three dimensions provides some intuition about the
Profiler traces. When you change the settings of factors in the Prediction
Profiler, the traces correspond to cross-sections of the prediction equation at
the new settings.
Back to Simultaneous Optimization
“Well,” you think, “that was fun! I need to show that to the team members
later on. They will love it.” Now you proceed to your next task, which is to ﬁnd
optimal settings for the Hot Xs. To do this, you return to the proﬁler. But, by
now, you have lost that window. So you simply double-click on one of your
reports, select Window > Close All Reports, and run the Profiler script that is
saved to the data table.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
415
Exhibit 9.60 Surface Plot for Pred Formula MFI
To ﬁnd optimal settings, you use the JMP desirability functions. Click on
the red triangle next to Prediction Profiler and choose Desirability Functions.
The saved script is Profiler—Desirability Functions.
This adds a row and a column of cells to the proﬁler. Two cells are added at
the right, one for each of the responses (Exhibit 9.61). These show the desirabil-
ity of various values of the responses, based on the speciﬁcation limits that you
entered into Column Info for MFI and CI earlier (see the section “Dealing with
the Preliminaries”). For example, values of MFI near 195 have desirability near
1 while values above 198 and below 192 have desirabilities near 0. Similarly,
large values of CI have high desirability.
To see the description of the desirability function for a given response,
double-click in the desirability function panel for that response. For MFI, you
obtain the Response Goal window shown in Exhibit 9.62. After reviewing
the entries, click Cancel to exit with no changes.
A new row of cells, called Desirability, is added to the Profiler
(Exhibit 9.61). This row gives traces for the joint desirability function,
which, like the predicted surface, is a function of SA, M%, and Xf. For example,
if SA = 65 and M% = 1.75, then low and very high values of Xf are not very
desirable. Values of Xf around 16.3 are more desirable.

416
V I S U A L S I X S I G M A
Exhibit 9.61 Profiler Showing Desirability Functions
Exhibit 9.62 Response Goal Dialog for MFI
The goal is to ﬁnd settings of the factors that maximize the desirability value.
To ﬁnd such settings, you select Maximize Desirability from the red triangle
menu next to Prediction Profiler. The saved script is Profiler—Desirability
Maximized.
You obtain the settings shown in Exhibit 9.63. The plot indicates that opti-
mal settings for the three continuous Hot Xs are SA = 79.7, M% = 0, and
Xf = 15.07. At these settings, the predicted value of MFI is 195.1857 and of CI is
99.4645. Both predicted values are well within their speciﬁcation limit ranges.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
417
Exhibit 9.63 Profiler with Desirability Functions Maximized
Note that, when you maximize desirability, you may not obtain these exact
settings. This is because, in most cases, settings that produce the same maximum
value are not unique.
The fact that the optimal setting for M%, the viscosity modiﬁer, is 0 indi-
cates that adding the modiﬁer actually degrades MFI (M% is not involved in the
model for CI). This surprises your team members. It was always believed that
the modiﬁer helped the melt ﬂow index. The team members talk with engi-
neers about this, and it is decided that perhaps the modiﬁer is not required—at
least they are interested in testing a few batches without it.
The plot also indicates that, for the optimal settings of SA and M%, if Xf
drifts to the 16.0 or higher, then desirability drops dramatically. By dragging
the vertical dashed line for Xf to 16.5 and higher while looking at the Pred
Formula CI trace for Xf, you see that Pred Formula CI drops below 80 when Xf
exceeds 16.55 or so. CI also drops below 80 if Xf drifts to 13.65 or lower. You
conclude that CI is very sensitive to variation in Xf at the optimal settings of
M% and SA.
Assessing Sensitivity to Predictor Settings
In your training, you learned about a sensitivity indicator. The sensitivity indi-
cator helps you assess how sensitive the predicted value is to factor settings.

418
V I S U A L S I X S I G M A
Exhibit 9.64 Profiler with Sensitivity Indicators and Xf near 16.5
From the Prediction Profiler red triangle, select Sensitivity Indicator.
Hollow triangles are added to the prediction traces. Notice that no triangles are
added to the traces for CI and SA or CI and M%, since SA and M% are not
involved in the prediction formula for CI.
You leave M% and SA at their optimal values and move the Xf setting to
about 16.5 (Exhibit 9.64). The sensitivity indicator for CI in the Xf panel gets
larger as you increase Xf to 16.5. The sensitivity indicator points in the direction
of change for the response as the factor level increases and its size is proportional
to the change. You conclude that maintaining tight control over Xf is critical.
The saved script in VSSTeamData.jmp is Profiler—Sensitivity Indicators.
Now you want to see how sensitive the prediction is to SA at the optimized
settings of M% and Xf. To reinstate the optimal settings, rerun the data table
script Profiler—Desirability Maximized and select Sensitivity Indicator from
the Proﬁler’s red triangle menu.
Move the SA setting to its left. Note that the Sensitivity Indicator for MFI
becomes very small at around 64.5, much smaller than it was at the optimal
setting of 79.7, which was close to the upper bound of the experimental range.
In fact, there is a range of values around 64.5 for which the desirability is almost
as high as when SA is set to 79.7. See Exhibit 9.65 and compare the predicted
values to those in Exhibit 9.59.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
419
Exhibit 9.65 Profiler with SA Set to 64.5
Given the possibility of a more robust operating range for values of SA
around 64.5, you want to consider using 64.5 as the optimal setting for SA.
You do note, however, that the sensitivity indicator for MFI shows that MFI is
highly sensitive to SA excursions to 60 and below.
Your sensitivity analysis allows you to reach two conclusions:
◾
Optimal settings that involve SA near 64.5 would result in a more robust
process.
◾
The important sensitivities are those of MFI to low SA and CI to high
and low Xf.
Determining Optimal Process Settings
You realize that there are many combinations of settings of the factors that
optimize both MFI and CI. When you select Maximize Desirability, the proﬁler
provides one such set of settings.
You have seen that setting SA to 64.5 results in essentially the same pre-
dicted values of MFI and CI as did the settings provided when you maximized
the desirability function. You decide to use 64.5 as the setting for SA. But you
wonder if, for this setting of SA, there might be better settings for M% and Xf.

420
V I S U A L S I X S I G M A
To ﬁnd optimal settings for M% and Xf when SA is set to 64.5 (VSSTeamData.jmp, script
is Optimal Settings):
1.
ALT-click in the Desirability cell for SA.
2.
Set the Current Value to 64.5.
3.
Check the Lock Factor Setting box.
4.
Click OK.
5.
Select Maximize Desirability from the Prediction Profiler red triangle menu.
Exhibit 9.66 shows that the optimal setting for M% is still 0 and that the
optimal setting for Xf has changed only slightly, to 15.12.
Controlling SA and Xf
Your sensitivity analysis makes it clear that both SA and Xf must be controlled
tightly around the optimal settings that you have derived. Your team mem-
bers get started on developing an understanding of why these two factors vary
and on proposing procedures that will ensure tighter control over them. They
Exhibit 9.66 Optimal Settings with SA Locked at 64.5

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
421
propose a new control mechanism to monitor SA (amps for slurry tank stirrer)
and to keep it on target at the optimal value of 64.5 and within narrow tol-
erances. They also develop procedures to ensure that ﬁller and water will be
added at regular intervals to ensure that Xf (percent of ﬁller in the polymer) is
kept at a constant level close to 15.12 percent.
The team tests these new controls and collects data from which you can
estimate the variability in the SA and Xf settings in practice.
Simulating Process Outcomes
This last task takes a few weeks. But the resulting data indicate that when SA
and Xf are held on target their actual settings do vary. Both factors appear to
have approximately normal distributions with standard deviations of 1.9 and
0.4, respectively. Your plan is to use these estimates of variation, together with
simulation, to investigate how variation in SA and Xf propagates to variation
in MFI and CI. In particular, you want to obtain reliable estimates of capability
for both responses of interest.
To use the Simulator from the Profiler to see how variation propagates
(VSSTeamData.jmp, script is Profiler—Simulator):
1.
Rerun the script Optimal Settings.
2.
From the red triangle menu next to Prediction Profiler, select Simulator.
This adds a Simulator outline to the report, as well as menus below the factor
settings in the proﬁler. The dropdown menus enable you to specify distributions to use in
simulating factor variability.
3.
In the menu beneath SA, select Random.
4.
In the dropdown menu beneath the normal plot that appears, select Normal weighted.
The Normal Weighted option samples from a normal distribution, but in a
fashion that oversamples the tails of the distribution. This gives more precise estimates of
the PPM defective rate than would otherwise be possible.
5.
Enter 1.9 as the SD (see Exhibit 9.67).
Note that the mean is set by default at the optimal setting.
6.
In the menu beneath Xf, select Random.
7.
In the dropdown menu beneath the normal plot that appears, select Normal weighted.
8.
Enter 0.4 as the SD (see Exhibit 9.67).
9.
Click the Simulate button found at the right beneath the proﬁler cells.

422
V I S U A L S I X S I G M A
Exhibit 9.67 Simulator with Normal Weighted Selected and SD Entered
Exhibit 9.68 Simulator Results
A Defect table appears below the Simulate button (Exhibit 9.68). Because
the predicted values for MFI and CI are simulated, you will see different results
than those shown here. If you run the script Profiler—Simulator, however,
you will obtain the same results.
Right-click in the table and select Columns > PPM. When your team mem-
bers see the overall PPM value of 413.55 as given in Exhibit 9.68, they give
a cheer. You click on Simulate to run the simulation a few more times, and
although there is some variation in the PPM values, the PPM values rarely
exceed 500.
The Defect table indicates that the majority of defective values are coming
from CI. Recall that CI is highly sensitive to Xf values in the region of the optimal
settings. If the standard deviation for Xf could be reduced, this might further
reduce the estimated PPM rate. Nonetheless, the team has very good news to
report. Everyone is anxious to meet with Edward, the manufacturing director,
and to test these predictions in the plant.
Conﬁrming the Improvement
Edward is thrilled with the team’s report and gives the go-ahead to run formal
conﬁrmation trials, measuring yield through to the molding plant. In the lim-
ited experience you have had to date with running the process at the optimal
settings, you have only monitored the polymer plant yield.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
423
Exhibit 9.69 Conﬁrmation Data from Five Batches
The team decides to perform tests on ﬁve batches using the target settings
and controls for SA and Xf. The batches are followed through the molding
plant so that, in line with the original deﬁnition, Yield is measured using
both polymer plant and molding plant waste. The measurements for these
ﬁve batches are given in the data table ConﬁrmationData.jmp, shown in
Exhibit 9.69.
What a delight to see all MFI and CI values well within their speciﬁcations
and to see 100 percent Yield on all ﬁve batches! You and your team members
are ecstatic.
UTILIZING KNOWLEDGE
In spite of this very promising result, you remind your teammates to remain
cautious. Even if you had made no process changes, history shows that a crisis
is only expected about once every quarter. During the single crisis that occurred
over the team’s baseline period, MFI values were unusually high. So there is
statistical evidence that keeping MFI within the speciﬁcation limits will alleviate
crisis periods. But it will be important to continue to monitor Yield to ensure
that the new optimal settings and controls really do address the root cause of
a crisis. As you gather additional data over a longer time span, you will have
more assurance that keeping MFI and CI within their speciﬁcation limits really
does alleviate crises.
This is an interesting situation where, although success has been conﬁrmed
in the short term, the longer time horizon of the historical problem requires a
heightened sense of vigilance during the control phase. So, although you are
ready to see this project move to the Utilize Knowledge step of the Visual Six

424
V I S U A L S I X S I G M A
Sigma Data Analysis Process, that step retains some aspect of providing conﬁr-
mation that the problem has been solved.
With this in mind, you and your team report your results to Edward. He
is very impressed by the outcome of the conﬁrmation runs and is extremely
pleased by your team’s work. He appreciates the logic and discipline you have
imposed throughout the knowledge-building process. It appears that because
of your diligence, the crisis periods may ﬁnally be eliminated.
Edward approves the ﬁtting of a new control mechanism for the ﬁll process
to monitor SA and to keep it on target and within tight control. He approves
of the procedures proposed by your team to maintain Xf on target with little
variation. These procedures are shared with technicians and operators in a for-
mal training session. Speciﬁcation limits are deﬁned for SA and Xf based on the
standard deviations used in the simulation study.
Once these procedures are implemented, production batches are monitored
closely, to ensure that nothing goes amiss, for the ﬁrst month of production.
Data on MFI, CI, Yield, SA, M%, and Xf for the 79 batches produced that month
are given in the data table ControlPhaseData.jmp.
Verifying Process Stability
Open ControlPhaseData.jmp. The ﬁrst thing that you notice is that all Yield
values are 100 percent! But you are interested in the process measurements
that drive Yield, namely, MFI and CI. The ﬁrst thing that you check is whether
MFI and CI are stable.
Use Control Chart Builder to construct Individuals and Moving Range
charts for MFI and CI (script is Control Charts). Because speciﬁcation limits
have been entered in the columns for these two variables, JMP automatically
computes normality-based capability analyses. The Individuals and Moving
Range control charts are shown in Exhibit 9.70.
The MFI measurements are stable and fall well below the upper speciﬁca-
tion limit of 198. But the CI measurements appear unstable. However, note
that their distribution is highly skewed, as we would expect. The individual
measurement chart’s control limits are valid for normally distributed data, and
CI is not normally distributed. What to do?
Even in a case where the underlying data are highly non-normal, the means
of samples are approximately normal, even for small samples or subgroups (this
follows from the Central Limit Theorem). This means that an XBar chart can
be used to assess process stability. So, for CI, you decide to construct an XBar
and R chart, subgrouping the measurements into consecutive subgroups of
size ﬁve.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
425
(a)
(b)
Exhibit 9.70 Control Charts for Control Phase Data
To construct an XBar and R chart (ControlPhaseData.jmp, script is XBar Chart for CI):
1.
Select Analyze > Quality and Process > Control Chart Builder.
2.
Select CI and click the New Y Chart button beneath the Select Columns list.
3.
From the red triangle menu next to Control Chart Builder, select Set Sample Size.
4.
In the window that opens, a default sample size of 5 is speciﬁed. Click OK.

426
V I S U A L S I X S I G M A
The resulting chart is shown in Exhibit 9.71. The absence of out-of-control
points and unstable patterns is evidence that the CI measurements have come
from a stable process.
Now that you know that MFI and CI are stable, you are interested in assess-
ing their capability relative to meeting the speciﬁcations.
Estimating Process Capability
You return to the report obtained when you ﬁrst constructed your Individuals
and Moving Range charts using the data table ControlPhaseData.jmp. The
script is Control Charts.
The Process Capability Analysis for MFI is repeated in Exhibit 9.72. The
MFI distribution falls within the speciﬁcation limits. Both short- and long-term
capability indices are over 1.0: Cpk is 1.143, based on the moving range, and
Ppk is 1.165. These values are close, as expected, because the process is stable.
You right-click in the Nonconformance report and select Columns >
Expected Overall PPM. This selection adds a column that shows predicted
parts per million out of spec. The calculation uses the standard deviation for all
values, which is reasonable, since the process is stable. The estimated defective
rate is 273.6 batches per million.
Since CI is highly skewed, a capability determination based on a normal
distribution is not meaningful and potentially misleading. In this case, the Cpk
value, based on a normal ﬁt, is 1.044. This seems optimistic.
A team member asks, “What do we do to assess capability?”
“Well,” you reply, “let’s try a nonparametric approach!”
For a left-skewed distribution, such as this one, it makes sense to use a
method that does not specify a particular functional shape or form.
To obtain a capability analysis using a nonparametric ﬁt (Exhibit 9.73), use Distribution
(ControlPhaseData.jmp, script is Capability CI):
1.
Select Analyze > Distribution.
2.
Enter CI as Y, Columns.
3.
Click OK.
4.
From the red triangle menu next to CI, select Continuous Fit > Smooth Curve.
The nonparametric ﬁt is smooth but a bit bumpy. Yet, it describes the dis-
tribution quite well. The computed capability index is 0.775, and the PPM rate
is 23,518, or about 2.35 percent. Based on the graph, your team is comfortable

Exhibit 9.71 XBar and R Chart for Subgrouped CI Measurements
427

428
V I S U A L S I X S I G M A
Exhibit 9.72 Process Capability Analysis for MFI
with this estimate of capability and PPM. The normality-based Cpk value of
1.044 was optimistic and incorrect.
It is also apparent to your team members that their simulation underesti-
mated the PPM rates for both MFI and CI. Recall that the combined estimate
of PPM was about 400. The actual data, especially for CI, shows a much higher
estimated PPM rate. This may indicate that additional variability is coming from
other sources.
The 2.35 percent out-of-speciﬁcation rate for CI is clearly not acceptable.
This is an area that will need further investigation. One of the team mem-
bers suggests that the lower speciﬁcation limit of 80 may in fact be too tight,
given that all batches had a Yield of 100 percent. Certainly, if it is possible to
loosen the speciﬁcation without incurring any loss in quality, this is a worth-
while endeavor. You document the need for a resolution to this CI capability
issue and put it on a list of future projects.
Tracking Improvement
To visualize progress over the life of the project, you want to concatenate the
tables VSSTeamData.jmp, ConﬁrmationData.jmp, and ControlPhase-
Data.jmp into a single data table. You want to be able to plot your data over

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
429
Exhibit 9.73 Smooth Curve Fit to CI
three phases: the phase consisting of the old process, the conﬁrmation phase,
and the control phase.
Before concatenating your tables, you engage in some housekeeping. Check
your list of open windows under Window. If it shows a lot of open windows
(and you’re on a Windows machine), you can simply select Windows > Close
All and reopen the three data tables. But the three data tables that you need
may be on the dropdown list of open data. To close all but the data tables, select
a report to make it active and then select Window > Close All Reports. Close
the data tables that are not of interest. When you are ﬁnished, make sure that
only the three data tables mentioned above are open.
You want to concatenate the three data tables, meaning that you want to
append rows from one data table to another. You want your concatenated data

430
V I S U A L S I X S I G M A
Exhibit 9.74 Column Panels for Three Data Tables
table to contain columns showing the basic information and a column for the
Phase of the study.
So you start by examining the column structure in the three data tables
(Exhibit 9.74). Note that ConﬁrmationData.jmp and ControlPhase.jmp
have the same column structure, but VSSTeamData.jmp contains columns
that the previous two data tables do not.
Follow the two sets of instructions below to concatenate the three tables.
Alternatively, simply open the concatenated data table, called VSSTeamFinal-
Data.jmp.
To adjust the column structure of the three data tables and to add a Phase column:
1.
Click on VSSTeamData.jmp to make it active and save it as TempData.jmp.
The name indicates that you can delete it later on.
2.
Select Rows > Clear Row States.
This ensures that no rows will be hidden and excluded in the concatenated table.
3.
In the Columns panel of TempData.jmp, select all columns that follow Xf,
right-click in the Columns panel, and select Delete Columns.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
431
4.
Double-click in the column header area to the right of the last column, Xf.
This creates a new column.
5.
Right-click in the column area and select Column Info.
6.
In the Column Info window, for Column Name, type Phase and click OK.
7.
In row 1 of the Phase column, type Old Process and then click away from
this cell.
8.
Right-click back into the cell and select Fill > Fill to end of table.
This copies the text “Old Process” into all of the data table rows.
Now that your tables are prepared, you can concatenate them.
To concatenate the three tables:
1.
Select Tables > Concatenate.
Since TempData.jmp is the active data table, it appears ﬁrst in the list of Data
Tables to be Concatenated.
2.
From the Opened Data Table list, select ConﬁrmationData and click Add.
3.
From the Opened Data Table list, select ControlPhaseData and click Add.
4.
In the Output table name text box, enter MyFinalData.jmp.
See Exhibit 9.75.
5.
Click OK.
Exhibit 9.75 Concatenate Dialog for the Three Tables

432
V I S U A L S I X S I G M A
The new table appears. You check to make sure it is what you intended.
Close TempData.jmp without saving it: Select File > Close and select No
when asked about saving changes.
Notice that all scripts have been carried over to the new table. Also, there is
a new script called Source. This script tells you how the current data table was
obtained and is useful for documentation. Some of the old scripts may not be
meaningful since they were developed for portions of the data. However, you
want to see control charts by Phase. You realize that the Control Chart and
XBar Chart for CI scripts could be useful. To avoid confusion, you delete all
the scripts other than Source, Control Charts, and XBar Chart for CI. (If you
prefer, at this point, you may close your concatenated data table and open the
saved data table called VSSTeamFinalData.jmp.)
In the data table VSSTeamFinalData.jmp, notice that Phase has the
Row Order Levels property. Click on the asterisk to the right of Phase
in the Columns panel to see this. This property ensures that the levels of
Phase appear in plots in order of their occurrence in the data. When you plot
control charts by Phase, the order of levels will be: Old Process, Conﬁrmation,
Control.
Obtain IR charts for MFI, CI, and Yield by Phase as follows (VSSTeamFinalData.jmp,
script is Control Charts by Phase):
1.
Run the script Control Charts.
This chart was carried over from VSSTeamData.jmp.
2.
Select Phase from the Select Columns list and drag it to the Phase zone at the
top of the template.
3.
In turn, select MFI, CI, and Yield from the Column Switcher list at the far left of
the report.
The MFI control chart, shown in Exhibit 9.76, shows how the measure-
ments have changed over the three phases of the study. The MFI measurements
are now less variable and within speciﬁcations.
Since the distribution for CI is skewed, the control limits for the Individuals
Chart for CI are not appropriate. As earlier, you construct an XBar and R chart
with subgroups of size 5.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
433
Exhibit 9.76 Phase Control Chart for MFI
To construct an XBar and R chart for CI by phase, shown in Exhibit 9.77 (the saved script is XBar
Chart for CI by Phase):
1.
Run the script XBar Chart for CI.
2.
Select Phase in the Columns list and drag it to the Phase zone.
Starting with the conﬁrmation phase, the CI process looks stable, both in
terms of centering and spread. It is operating with much less variability and
appears to be centered at about 95. Since this is an XBar chart, we cannot com-
pare the control limits (based on averages) directly with the speciﬁcation limits

434
V I S U A L S I X S I G M A
Exhibit 9.77 Phase XBar and R Chart for CI
(based on individual values). But that has already been done for the Control
phase in the non-normal capability analysis (Exhibit 9.73).
CONCLUSION
The original project goal was to eliminate the periods of crisis with a goal of
achieving a yield rate of 95 percent. One and a half years have now passed
since completing this project, and although sometimes a very few batches do
fall slightly below the lower limit for color index (CI), this issue is resolved by
blending these low-color index batches with high-color index batches. To date,
the molding plant has not rejected a single batch of white polymer since the
changes have been implemented.
The savings from rejected batches alone are slightly over £750,000 per
annum. Additionally, there are now no processing restrictions on the molding
plant, resulting in an additional annualized margin of £2,100,000 per annum
and a very happy customer. The actual costs incurred on the project were
minimal.

I M P R O V I N G A P O L Y M E R M A N U F A C T U R I N G P R O C E S S
435
This was the project that convinced the skeptics, Edward included. It solved
a major manufacturing problem with huge business beneﬁts. In the ﬁnal project
close-out meeting, you summarized the key lessons as follows:
◾
Never trust a measurement system—it was only when these issues were
resolved that it was possible to get meaningful results from the analysis.
◾
The DMAIC methodology in conjunction with the Visual Six Sigma
Roadmap delivers high levels of process understanding in a simple,
straightforward manner.
◾
Visual Six Sigma techniques enable technical and business users to apply
the principles of statistical thinking to solve problems rapidly and effec-
tively.
◾
Visual Six Sigma greatly facilitates the communication of results to a
wide audience in a simple visual form.
NOTES
1. D. J. Wheeler, EMP III: Evaluating the Measurement Process & Using Imperfect Data (SPC Press, 2006).
2. Wheeler, EMP III.
3. Automotive Industry Action Group, Measurement Systems Analysis Reference Manual, 4th edition
(Chrysler Group LLC, Ford Motor Company, General Motors Corporation, 2010).

C H A P T E R 10
Classiﬁcation of Cells
437
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

438
V I S U A L S I X S I G M A
This case study relies heavily on features available only in JMP Pro.
C
ellularplex is a small cell phone provider anxious to attract new customers.
Jeremy Halls, the director of marketing, faces the challenge of running
targeted and well-timed marketing campaigns. To that end, he wants to
explore the value of predictive analytics, namely, the use of modeling techniques
to predict future trends and behavior. You are a statistical analyst with prior
experience in both marketing and medical research. Jeremy enlists your help
in developing strong marketing programs that will expand Cellularplex’s cus-
tomer base.
A test campaign aimed at identifying new customers who would take
advantage of a speciﬁc offer is being launched. The data collected will allow
the marketing group to explore the impact of a large number of demographic
characteristics, as well as how the offer was made, on the response rate. The key
response is the category into which a respondent falls, and there are four possi-
ble outcomes: an inquiry about the offer, the purchase of the promoted offer, the
purchase of a different offer, or a rejection of the offer. The information from this
test campaign will be used to shape a large-scale drive to attract new customers.
The test campaign duration is set to three months. During this time, you
start learning about JMP Pro, focusing on its powerful visualization and model-
ing capabilities. Because you will need to be ready to analyze the campaign data
quickly when the responses arrive, you search for some practice data, preferably
a data set with a large number of potential predictors and where classiﬁcation
into two or more outcomes is required. You ﬁnd a published data set called the
Wisconsin Breast Cancer Diagnostic Data. These data resulted from a study to
accurately classify breast growths as malignant or benign, based on characteri-
zation of a ﬁne needle aspirate. The data includes 569 records and 30 potential
predictor variables.
This case study follows your self-study as you explore features of JMP that
support classiﬁcation and data mining. You make extensive use of visualization
techniques to build an understanding of the data set. After dividing the data into
a training set, a validation set, and a test set, you ﬁt seven models to the train-
ing data: two regression tree models, a logistic model, two penalized regression
models using the Generalized Regression platform, and two neural net models.
Speciﬁcally, these models are:
◾A Decision Tree model (Partition platform)
◾A Boosted Tree model (Partition platform)
◾A Logistic model (Stepwise personality in Fit Model)
◾A Lasso model (Generalized Regression personality in Fit Model)
◾An Elastic Net model (Generalized Regression personality in Fit Model)

C L A S S I F I C A T I O N O F C E L L S
439
◾
A Neural Net model without boosting (Neural platform)
◾
A Neural Net model with boosting (Neural platform)
Comparing the performance of these models on the test set leads you to
choose one of these as your preferred model.
A list of the JMP platforms and options that you use in this case study is
shown in Exhibit 10.1. Many of these require JMP Pro. The JMP data tables
that you use are available at http://support.sas.com/visualsixsigma.
Exhibit 10.1 Platforms and Options Illustrated in This Case Study
Menus
Platforms and Options
Tables
Subset
Sort
Missing Data Pattern
Rows
Hide and Exclude
Color or Mark by Column
Cols
Column Info
Column Properties
Formula
Modeling Utilities
Explore Outliers
Make Validation Column
Group Columns
Analyze
Distribution
Histogram
Frequency Distribution
Tabulate
Fit Model
Stepwise
Generalized Regression
Nominal Logistic
Modeling
Partition
Decision Tree
Boosted Tree
Neural Net
Model Comparison
Multivariate Methods
Multivariate
Correlation and Scatterplot Matrix
Graph
Graph Builder
Scatterplot Matrix
Other
Column Switcher

440
V I S U A L S I X S I G M A
SETTING THE SCENE
Cellularplex is a small cell phone provider that is poised and anxious to expand
its customer base. Jeremy Halls, the director of marketing, has been hearing
about predictive analytics and how it can be successfully used to focus market-
ing strategies. He believes that statistical and data-mining models of customer
characteristics and proclivities could greatly enhance his ability to run market-
ing campaigns that are targeted and well timed, reaching potential customers
with the right offers and using the right marketing channels.
You have recently been hired by Cellularplex. Your previous position was
with a medical research ﬁrm, where you conducted statistical analysis relating
to clinical trials. Prior to that position, you worked with a retail ﬁrm doing
predictive analytics in the marketing area.
You are a key member of a team that Jeremy forms to design a test cam-
paign aimed at identifying new customers for a speciﬁc offer that Cellularplex
will market in the coming year. The goal of the test campaign is to identify
demographic characteristics of individuals who would likely purchase the offer
and to determine the best delivery method for various combinations of demo-
graphic characteristics. The knowledge gained will be employed in designing a
subsequent large-scale campaign to attract new customers.
The team members brainstorm a large number of characteristics that they
think are indicative of people who might respond positively. Then they work
with a data vendor to obtain a list of people with these characteristics. They
also obtain the contact details for a small random sample of people who do
not have the characteristics that they have identiﬁed, realizing that informa-
tion from a group outside their chosen demographics could yield information
on customers who might otherwise be overlooked. The team also determines
different delivery methods for the offer, and with your help they include these
in their design. Finally they agree on how to measure customer response—for
each contact, they will record whether the result was a customer inquiry, pur-
chase of the offer, purchase of a different offer, or rejection. You point out to the
team that if they later collapse this information into only two categories (pur-
chase or nonpurchase), then uplift modeling will provide another possibility for
analyzing the resulting data.
The duration of the test campaign is set at three months, starting in the
second quarter. You will support the effort, but in the meantime you want to
prepare yourself to analyze the kind of data it will generate. You will need to
use the numerous measured characteristics and the response of each individual
to classify each into one of the four possible categories, and thus determine
those likely to become customers and those unlikely to become customers. You
start learning JMP Pro, which is used by the engineers at Cellularplex, and soon
realize that it has powerful visualization and modeling capabilities that you can
use when the real data arrive.

C L A S S I F I C A T I O N O F C E L L S
441
Knowing that you will have to undertake this analysis quickly once the data
become available, you look for a published data set that you can use for practice,
both to learn how to use JMP Pro and to see how it performs relative to other
software that you have used. Speciﬁcally, you would like a data set with a large
number of descriptive characteristics where classiﬁcation of subjects into two
or more categories is of primary interest.
Given your medical background, you easily ﬁnd and download an appro-
priate data set—the Wisconsin Breast Cancer Diagnostic Data Set. Your plan is
to use various techniques in JMP Pro to ﬁt classiﬁcation models to this data set.
Realizing that some of the Cellularplex engineers are experienced JMP users,
you ask a few of them if they would be willing to help you if necessary. James,
who has used JMP Pro for several years, spends a couple of hours with you,
giving you an introduction and offering to help you with further questions as
they arise. What James shows you provides a good starting point for learning
more about modeling high-dimensional data on your own.
In this case study, you work with various JMP capabilities for exploring
and modeling data. You begin by using visualization techniques to help build
an understanding of the Wisconsin Breast Cancer Diagnostic Data Set. After
dividing the data into a training set, a validation set, and a test set, you construct
seven models using four different modeling approaches. All of the models you
construct use the validation set for cross-validation. Your study ends with a
comparison of the performance of your seven models on the test set.
This case study uses the principles of Visual Six Sigma to construct knowl-
edge. This type of knowledge can eventually be used to guide sound business
decisions. By its nature, this case study focuses on the Model Relationships
step of the Visual Six Sigma Data Analysis Process, and does not involve the
Revise Knowledge and Utilize Knowledge activities. These activities will become
relevant once the classiﬁcation scheme that you eventually develop for the mar-
keting data is implemented as part of the formal marketing campaign.
FRAMING THE PROBLEM AND COLLECTING THE DATA:
THE WISCONSIN BREAST CANCER DIAGNOSTIC DATA SET
The Wisconsin Breast Cancer Diagnostic Data Set arises in connection with
diagnosing breast tumors based on a ﬁne needle aspirate.1 In this study, a
small-gauge needle is used to remove ﬂuid directly from the lump or mass. The
ﬂuid is placed on a glass slide and stained so as to reveal the nuclei of the cells.
An imaging system is used to determine the boundaries of the nuclei. A typical
image consists of 10 to 40 nuclei. The associated software computes ten
characteristics for each nucleus: radius, perimeter, area, texture, smoothness,
compactness, number of concave regions, size of concavities (a concavity is an
indentation in the cell nucleus), symmetry, and fractal dimension of the boundary
(the higher the value of fractal dimension, the less regular the contour).2

442
V I S U A L S I X S I G M A
A set of 569 lumps was sampled, and each resulting image was processed
as described above. Since a typical image can contain from 10 to 40 nuclei,
the measurements were summarized. For each characteristic, the mean, max,
and standard error of the mean were computed, resulting in 30 variables. The
model developed by the researchers was based on separating hyperplanes.3
A best model was chosen by applying cross-validation to estimate prediction
accuracy, using all 569 records as a training set. This best model involved only
three variables—mean texture, max area, and max smoothness—and achieved
an estimated classiﬁcation accuracy of 97.5 percent. Even more remarkably, 131
subsequent patients were diagnosed with 100 percent accuracy.
Since your goal is to become familiar with the various modeling techniques
you might use later, your modeling approach will differ from that of the study
authors. You will construct seven models using a training and validation set.
Then you will assess the performance of your models on a test set. Your ﬁgure
of merit will be the overall misclassiﬁcation rate, which you want to be as small
as possible.
The data are publicly available at http://archive.ics.uci.edu/ml/datasets
.html. The data set is called “Breast Cancer Wisconsin (Diagnostic).” You
download the data and arrange them in a JMP data table called CellClassiﬁ-
cation.jmp.
INITIAL DATA EXPLORATION
The data table CellClassiﬁcation.jmp has 32 columns. The ﬁrst column is
ImageID, which is simply an identiﬁer. The second column is Diagnosis, the
variable you want to predict. Each of the following 30 variables has a name
beginning with Mean, Max, or SE, indicating which summary statistic has been
calculated—the mean, max, or standard error of the mean of the measured
quantity.
In exploring the data and ﬁtting your models, you realize that you will often
want to treat all 30 of your predictors as a group. When a large number of
columns will be treated as a unit, say, as potential predictors, this can make
analyzing data more convenient.
To group your 30 predictors (CellClassiﬁcation.jmp, script is Group 30 Columns):
1.
In the Columns panel, select all 30 predictors, from Mean Radius to SE Fractal
Dim.
2.
Right-click in the highlighted area and select Group Columns.

C L A S S I F I C A T I O N O F C E L L S
443
3.
Double-click on the name that JMP has assigned, Mean Radius etc., and change it to
Predictors.
4.
To view the columns in the group, you need only click the disclosure icon to the left of
Predictors.
Before you engage in any modeling, you want to get a better understanding
of the data. Do you have missing data values? What do the data look like? Are
there outliers?
Also, as suggested by the Visual Six Sigma Roadmap (Exhibit 3.30), you
want to visualize your data one variable at a time and then two variables at
a time. This will provide you with the knowledge that there are strong rela-
tionships between the 30 predictors and the diagnosis into benign or malignant
masses.
Missing Data Analysis
You begin by checking to see if you have any missing data.
To check for missing data values (CellClassiﬁcation.jmp, script is Missing Data
Pattern):
1.
Select Tables > Missing Data Pattern.
2.
In the Select Columns list, select all 32 columns.
3.
Click Add Columns.
4.
Click OK.
The resulting report, partially shown in Exhibit 10.2, is a data table. The
second column, Number of columns missing, indicates that none of the 569
columns has any missing values. Had there been missing values, the Patterns
column would indicate the columns in which they occur.
Rather than use Missing Data Pattern, you realize that you can also use
Cols > Columns Viewer to simply get a count of missing values in each column.
Exhibit 10.2 Missing Data Pattern Report, Partial View

444
V I S U A L S I X S I G M A
Exploring All Distributions
Now you want to see how the data are distributed. You want to obtain distri-
bution reports for all of the variables other than ImageID, which is simply an
identiﬁer for each row.
To obtain distribution reports for all columns except ImageID (CellClassiﬁcation.jmp,
script is Distribution—31 Variables):
1.
Select Analyze > Distribution.
2.
Select all columns from Diagnosis to SE Fractal Dim.
3.
Click Y, Columns.
4.
Check the box next to Histograms Only, located beneath the Select Columns list.
5.
Click OK.
The ﬁrst four of the 31 distribution reports are shown in Exhibit 10.3. The
vertical layout for the graphs is the JMP default. You realize that you can change
this either for just this report or more permanently for all subsequent reports
in File > Preferences, but you are happy with this layout for now.
The bar graph corresponding to Diagnosis indicates that 212, or 37.258 per-
cent, of the tumors in the study were assigned the value M, indicating they were
malignant rather than benign. This means that neither of the two categories is
highly underrepresented. A sparse category would cause problems for your anal-
ysis, since standard models would favor the more highly represented category.
Scrolling through the plots for the 30 predictors, you assess the shapes of
the distributions and look for the presence of possible outliers. You note that
Exhibit 10.3 First 4 of 31 Distribution Reports

C L A S S I F I C A T I O N O F C E L L S
445
most distributions are skewed toward higher values and that there may be some
outliers, for example, for SE Radius, SE Perimeter, SE Area, and SE Concavity.
You make a mental note to examine outliers in depth later on.
All in all, the data are well behaved—you expect your marketing data to
be much messier—but you decide that this is a good practice data set given
your goal.
Exploring Relationships for Two Variables at a Time
Distribution and Dynamic Linking
Of special interest to you is whether the predictors are useful in predicting the
values of Diagnosis, the dependent variable.
To get some initial insight on this issue, return to your distribution report. In
the graph for Diagnosis, click on the bar corresponding to M. This selects all rows
in the data table for which Diagnosis has the value M. These rows are dynami-
cally linked to all plots, and so, in the 30 histograms corresponding to predictors,
areas that correspond to the rows where Diagnosis is M are highlighted.
Scroll across the histograms to see that malignant masses tend to have high
values for all of the Mean and Max variables except for Mean Fractal Dim
and Max Fractal Dim. Five of the distributions for Max variables are shown
in Exhibit 10.4. The malignant observations are highlighted. Max Fractal Dim
is shown in the last histogram.
Next, click on the bar for Diagnosis equal to B and scroll across the his-
tograms. You conclude that there are clear relationships between Diagnosis
and the 30 potential predictors. You should be able to build good models for
classifying a mass as malignant or benign based on these predictors.
Correlations and Scatterplot Matrix
You are also interested in how the 30 predictors relate to each other. To see
bivariate relationships among these 30 continuous predictors, you obtain cor-
relations and construct scatterplots.
To obtain correlations and scatterplots for all 30 predictors (CellClassiﬁcation.jmp, script is
Scatterplots—30 predictors):
1.
Select Analyze > Multivariate Methods > Multivariate.
2.
From the Select Columns list, select the Predictors group and click Y,
Columns.
3.
Click OK.
4.
From the red triangle in the Multivariate report, select Scatterplot Matrix.

Exhibit 10.4 Histograms for Five Max Variables with Malignant Masses Highlighted
446

C L A S S I F I C A T I O N O F C E L L S
447
Exhibit 10.5 Partial View of Correlation Matrix: First Six Mean Variables
The report shows a Correlations outline where you see the correlations
between the predictors. It also shows a 30 × 30 Scatterplot Matrix. This is a
matrix of scatterplots for all pairs of variables. The 6 × 6 portion of the matrix
corresponding to the ﬁrst six Mean variables is shown in Exhibit 10.5.
You scroll through the matrix and notice that some variables are highly
related. For example, the Mean and Max values of the size variables, Perime-
ter, Area, and Radius, are strongly related, as you might expect. Also, as you
might expect, Perimeter and Area have a nonlinear relationship. Weaker, but
noticeable, relationships exist among other pairs of variables as well.
You would like to see how Diagnosis ﬁts into the scatterplot display. James
had shown you that in many plots you can simply right-click to color or mark
points by the values in a given column.
To color and mark the points by Diagnosis (CellClassiﬁcation.jmp, script is Colors
and Markers):
1. In the Scatterplot Matrix, right-click in an off-diagonal square.
2. Locate Row Legend in the menu that appears and read the tooltip that displays when
you hover over Row Legend with your cursor.

448
V I S U A L S I X S I G M A
This assures you that Row Legend will enable you to color the points and that it
will create a legend window.
3.
Select Row Legend.
4.
A Mark by Column dialog appears.
5.
Select Diagnosis.
The default colors are red, a color typically associated with danger, for B, which is
actually a good outcome, and blue for M, a bad outcome. You would like to see these
colors reversed.
6.
Check the box next to Reverse Scale.
This switches the colors so that B is colored blue and M is colored red.
7.
From the Markers menu, select Hollow markers.
You select hollow markers because you think these will show well in grayscale
printouts.
8.
Check Make Window with Legend.
This is to create a legend window that will be handy, especially when viewing large
plots.
9.
Check Save to Column Property.
This will save your color scheme to the Diagnosis column. This may be useful
later on. The Mark by Column window with your selections is shown in Exhibit 10.6.
10.
Click OK.
Exhibit 10.6 Completed Mark by Column Window
You notice that the colored markers now appear in your data table next to
the row numbers. Also, an asterisk appears next to Diagnosis in the Columns
panel. When you click on it, it indicates that Diagnosis has the Value Col-
ors column property. When you click on Value Colors, it takes you to the

C L A S S I F I C A T I O N O F C E L L S
449
Diagnosis Column Info dialog, where you see that the colors have been applied
to B and M as you speciﬁed.
The colors and markers appear in the scatterplots as well, and a legend is
inserted to the right of the matrix as part of the report. You can use this leg-
end to select points. But you have also created a Legend window, shown in
Exhibit 10.7. (Script is Colors and Markers.)
Move the Legend window into the upper right of your scatterplot matrix,
as shown in Exhibit 10.7. Click on the B level for Diagnosis. This selects all
rows where Diagnosis is B and highlights these points in plots, as is shown in
the portion of the scatterplot matrix shown in Exhibit 10.7. To unselect the B
rows, hold the Control key as you click on the B level in the legend.
Viewing the scatterplot matrix, it is clear that Diagnosis is associated with
patterns of bivariate behavior.
Pairwise Correlations
You are interested in which variables are most strongly correlated with each
other, either in a positive or negative sense. The scatterplot certainly gives you
some clear visual information. However, a quick numerical summary showing
correlations for all pairs of variables would be nice.
Exhibit 10.7 Legend Window with B Selected

450
V I S U A L S I X S I G M A
To obtain a sorted list of pairwise correlations (CellClassiﬁcation.jmp, script is Pairwise
Correlations):
1.
In the Multivariate report for the 30 predictors (script Scatterplots—
30 Predictors), close the Correlations and Scatterplot Matrix outlines.
2.
From the red triangle menu, select Pairwise Correlations.
3.
Right-click anywhere in the Pairwise Correlations panel.
A menu appears.
4.
Select Sort by Column.
5.
In the Select Columns window, select Correlation.
6.
Click OK.
Because you have 30 variables, each of which can be paired with 29 other
variables, there are 30 × 29 = 870 combinations of two variables. Because
these combinations double-count the correlations, there are 870 ∕2 = 435
(a)
(b)
Exhibit 10.8 Largest 20 and Smallest 10 Correlations

C L A S S I F I C A T I O N O F C E L L S
451
distinct pairwise correlations. These 435 correlations are sorted in descending
order. Exhibit 10.8 shows the largest 20 and the smallest 10 correlations.
The report shows the high positive correlations among the Mean size vari-
ables, the Max size variables, and the SE size variables. All of these are 0.9377
or higher. This is expected. Scanning further down the report, you observe that
the Mean and Max variables for the same characteristics tend to have fairly high
correlations. This is also expected.
Many of the large negative correlations involve fractal dimension and the
size measures. Recall that the larger the fractal dimension, the more irregular
the contour. Six such correlations are shown in the bottom part of Exhibit 10.8.
This suggests that larger cells tend to have more regular contours than smaller
cells. How interesting!
A Better Look at Mean Fractal Dimension
To get a better feeling for relationships between size variables and fractal dimen-
sion, you decide to explore Mean Radius and Mean Fractal Dim.
To obtain a plot and linear ﬁts for Mean Radius and Mean Fractal Dim
(CellClassiﬁcation.jmp, script is Radius vs Fractal Dim):
1.
Select Graph > Graph Builder.
2.
Drag Mean Radius to the Y zone.
3.
Drag Mean Fractal Dim to the X zone.
4.
Drag Diagnosis to the Overlay box at the top right of the template.
5.
At the top of the template, click on the third icon from the left.
This is the Line of Fit icon. Notice that a Line of Fit panel appears beneath the
Variables list. You can select various options for the plot.
6.
Click Done.
You obtain the plot shown in Exhibit 10.9.
The combined collection of benign and malignant cells exhibits a decreasing
relationship between Mean Radius and Mean Fractal Dim. Given a value of
Mean Fractal Dim, Mean Radius is generally higher for malignant cells than
for benign cells. The ﬁtted lines and conﬁdence curves suggest that there is a
big difference between the two diagnosis groups based on how Mean Radius
and Mean Fractal Dim are related.
Given values on these two variables, you could make a fairly good guess
as to whether a mass is benign or malignant. But to help distinguish the

452
V I S U A L S I X S I G M A
Exhibit 10.9 Plot of Mean Radius by Mean Fractal Dim
two groups in the murky area between the ﬁtted lines, you will need addi-
tional variables. This is encouraging—it should be possible to devise a good
classiﬁcation scheme.
In examining the scatterplot matrix further, you observe that there may be
a few bivariate outliers. If this were your marketing data set, you would attempt
to obtain more background on these records to get a better understanding of
how to deal with them. For now, you decide not to take any action relative
to these points. However, you make a mental note that you might want to
revisit their inclusion in model-building since they have the potential to be
inﬂuential.
CONSTRUCTING THE TRAINING, VALIDATION, AND TEST SETS
By now you have accumulated enough knowledge to realize that you should
be able to build a strong classiﬁcation model. You are ready to progress to the
Model Relationships step of the Visual Six Sigma Data Analysis Process. You
anticipate that the marketing study will result in a large and unruly data set,
probably with many outliers, some missing values, irregular distributions, and
some categorical data. It will not be nearly as small or as clean as this practice
data set.

C L A S S I F I C A T I O N O F C E L L S
453
Some data-mining techniques, such as recursive partitioning and neural
nets, ﬁt highly parameterized nonlinear models that have the potential to ﬁt
the anomalies and noise in a data set, as well as the signal. These data-mining
techniques do not allow for variable selection based on hypothesis tests, which,
in classical modeling, help the analyst choose models that do not overﬁt or
underﬁt the data.
To balance the competing forces of overﬁtting and underﬁtting in data-
mining efforts, one often divides the available data into at least two and some-
times three distinct sets. A portion of the data, called the training set, is used
to construct several potential models. The performance of these models is then
assessed on a holdout portion of the data called the validation set. A best model is
chosen based on performance on the validation data. You will use this strategy
in ﬁtting models using recursive partitioning, logistic regression, generalized
(penalized) regression, and neural nets.
Choosing a model based on a validation set can also lead to overﬁtting. In
situations where a model’s predictive ability is important, a third independent
portion of the data, called the test set, is often reserved to assess the model’s
performance. The test set can also be used to compare several models and select
a best predictive model.4
For these reasons, you decide to construct three analysis sets: a training set,
a validation set, and a test set. You will construct a column called Validation in
your data table that assigns each row to one of these three sets. Then you will
run a quick visual check to see that there are no obvious issues arising from
how you divided the data.
Deﬁning the Training, Validation, and Test Sets
Admittedly, your data set of 569 observations is small and well-behaved com-
pared to most data sets where data-mining techniques are applied. But keep
in mind that you are working with this smaller, more manageable data set in
order to learn how to apply techniques to the much larger marketing data set
that you will soon need to analyze, as well as to any other large databases with
which you may work in the future.
You will randomly divide your data set of 569 rows into three portions:
◾
A Training set consisting of about 60 percent of the data
◾
A Validation set consisting of about 20 percent of the data
◾
A Test set consisting of the remaining 20 percent of the data
The suggested split proportions are not derived from theory, but rather from
practical considerations. They can vary somewhat from application to applica-
tion. However, it is common to allocate a large share of the data to develop
models and smaller shares to compare and assess them.

454
V I S U A L S I X S I G M A
To construct your three analysis sets (CellClassiﬁcation.jmp, script is Make Validation
Column):
1.
Select Cols > Modeling Utilities > Make Validation.
2.
In the box to the right of Training Set, enter 0.6.
3.
In the box to the right of Validation Set, enter 0.2.
4.
In the box to the right of Test Set, enter 0.2.
5.
Click Purely Random.
A new column called Validation appears in your data table. This column
designates each row as belonging to the Training, Validation, or Test set. In the
Columns panel (see Exhibit 10.10), you notice that Validation has a plus sign
(+) and an asterisk next to it:
◾The plus sign indicates that the column is deﬁned by a formula. Click on
the plus sign to see the Formula.
◾The asterisk indicates that the column has the Value Labels property.
Click on the asterisk to see that the underlying values are 0, 1, and
2. These values deﬁne an order that will be used in reports. The labels
Training, Validation, and Test are associated with the integer values.
Since rows are assigned to analysis sets at random, your analysis sets will
differ from those used in this chapter. Note, however, that if you want to be able
to reproduce a validation column, you can use the Random Seed Reset add-in
found at https://community.jmp.com/docs/DOC-6601. Alternatively, you can
set a seed using a script. The script Make Validation Column speciﬁes a random
seed and then completes the dialog obtained when you select Cols > Modeling
Utilities > Make Validation.
Exhibit 10.10 Columns Panel Showing Plus Sign and Asterisk Next to Validation Column

C L A S S I F I C A T I O N O F C E L L S
455
To follow along with exactly the same assignments as used in this chapter
(CellClassiﬁcation.jmp, script is Make Validation Column):
1.
Delete the column you have just created by right-clicking on the column header and
selecting Delete Columns.
2.
Run the script Make Validation Column. This script sets the random seed to make
the assignment reproducible.
Distribution of the Three Analysis Data Sets
At this point, you should check that the assignment of rows to the three
analysis sets worked out as you expected. Run Distribution on Validation
(Exhibit 10.11). You observe that the proportion of rows in each set is about
what you wanted.
Exhibit 10.11 Distribution Report for Validation

456
V I S U A L S I X S I G M A
Checking for Outliers
Your next step is to develop models. Before proceeding, you want to ensure that
your data sets don’t contain outliers that might inﬂuence the models or your
validation of the models. When you constructed individual distributions for
your predictors early on, you noticed some potential outliers for SE Radius, SE
Perimeter, SE Area, and SE Concavity. You need to address these observations
at this point.
JMP Pro provides various ways to explore outliers in Cols > Modeling Util-
ities > Explore Outliers (see Exhibit 10.12). The Quantile Range Outliers
and Robust Fit Outliers options identify rows that contain outliers relative to
individual variables. The Multivariate Robust Outliers option identiﬁes obser-
vations that are outliers based on a multivariate normal distribution. The Mul-
tivariate k-Nearest Neighbor Outliers option identiﬁes observations that are
outliers based on their multivariate distance from their nearest neighbors.
You have 30 predictors and your main interest is in rows that contain out-
liers from the multivariate distribution. However, your data are far from mul-
tivariate normal. So you decide to use the Multivariate k-Nearest Neighbor
Outliers option.
To identify multivariate outliers (CellClassiﬁcation.jmp, script is K Nearest Neighbor
Outliers; click OK in the pop-up window):
1.
In the Columns panel, select the Predictors group.
2.
Select Cols > Modeling Utilities > Explore Outliers.
3.
Click on Multivariate k-Nearest Neighbor Outliers.
4.
In the pop-up window, click OK to accept the suggestion to show distances from up to
eight nearest neighbors.
Exhibit 10.12 Explore Outliers Options

C L A S S I F I C A T I O N O F C E L L S
457
Exhibit 10.13 K Nearest Neighbor Plots
The plot in Exhibit 10.13 appears. The plot for Distance to 1 Closest shows
several points that might be outliers. The remaining plots show three rows with
observations that are distant from their nearest two, three, ﬁve, or eight neigh-
bors. These points are distant from a clump of near points. Hover over these
points to see that they are in rows 153, 213, and 462.
These points are difﬁcult to see in the histograms constructed using the
Distribution—31 Variables script. If you broadcast the Outlier Boxplot Option
from one of the histograms (by holding the Control key when you request this
option), you can see these points. However, another easy way to see them is
using Graph Builder and Column Switcher.
To see the three outliers relative to the distributions of the 30 predictors
(CellClassiﬁcation.jmp, script is Graph Builder—Outliers):
1.
Drag a rectangle around the three points in the K Nearest Neighbors plot for Distance
to 8 closest to select them.
2.
Select Graph > Graph Builder.

458
V I S U A L S I X S I G M A
3.
Open the Predictors group, select Mean Radius, and drag it to the Y zone.
4.
From the red triangle menu, select Script > Column Switcher.
5.
From the pop-up menu, select the Predictors group as the list of columns you want to
switch to, and click OK.
6.
In the Graph Builder window, click Done.
7.
Click on Mean Radius in the Column Switcher list.
8.
Use the down-arrow on your keyboard to scroll through all 30 predictors. Or click on the
animation control arrow beneath the Columns list to scroll through automatically.
The slider below the animation controls allows you to set the animation speed.
Exhibit 10.14 shows the Graph Builder window showing SE Radius and
the three selected points.
As you scroll through all distributions, you see that rows 213 and 462 con-
tain observations that are outliers for SE Radius, SE Perimeter, and SE Area.
Row 153 contains an outlier for SE Concavity and perhaps SE Concave Points
and SE Fractal Dimension. These three points might be highly inﬂuential on
some of the models that you plan to ﬁt.
Exhibit 10.14 Graph Builder and Columns Switcher Showing SE Radius

C L A S S I F I C A T I O N O F C E L L S
459
In the plot for SE Concavity, there appears to be a second outlier, row 69.
Select that point. Scroll through the other variables to see that row 69 does not
seem to be outlier for other predictors. Yet, because it is so distant from the
other SE Concavity measurements, it might be inﬂuential on some model ﬁts.
It is possible for outlying observations to be inﬂuential without distorting
model ﬁts, or to be part of the common cause distribution that you are studying.
In both cases, it makes sense to retain them in analyses. In every case, though,
you should always investigate each outlier to determine if it is the result of an
unrelated or special cause distribution, or a recording error.
In your situation, because you are handling secondary data, you are unable
to further investigate these four observations directly. To be prudent, you decide
to exclude them from further analyses. You exclude and hide rows 69, 153,
213, and 462 in the data table. This excludes the points from models that you
construct and does not show them in plots.
To exclude and hide these four points (CellClassiﬁcation.jmp, script is Hide and
Exclude Outliers):
1.
In the K Nearest Neighbors plot for Distance to 8 closest, drag a rectangle around
the three points with the highest values. See Exhibit 10.15.
2.
Hold the Control key and click on the point for row 69. See Exhibit 10.16.
3.
Once all four points are selected, right-click in the plot and select Row Hide and
Exclude.
Check the Rows panel in the data table to see that four rows are excluded and four
rows are hidden.
To document that these rows have been hidden and excluded as potential
outliers, a Table Variable called Potential Outliers has been added to the data
table.
Exhibit 10.15 Selection of Rows 153, 213, and 462

460
V I S U A L S I X S I G M A
Exhibit 10.16 Selection of Row 69
To document your decision about outliers as a Table Variable (CellClassiﬁcation.jmp):
1.
In your data table, click the red triangle to the left of the data table name and select New
Table Variable.
2.
Fill out the text window that opens as shown in Exhibit 10.17.
3.
Click OK.
Exhibit 10.17 Table Variable Text Window
This information now appears in the table panel, directly beneath the name
of the data table. Deﬁning a Table Variable is a convenient way to document
information about the data table and changes you have made.
Checking Your Analysis Data Sets
You are also interested in conﬁrming that your training, validation, and test
sets do not differ greatly in terms of the distributions of the predictors. You can
easily do this using Graph Builder and Column Switcher. You use these tools
to quickly explore the 30 distributions broken down by training, validation, or
test set.

C L A S S I F I C A T I O N O F C E L L S
461
To explore the distributions of the various variables for the three analysis sets
(CellClassiﬁcation.jmp, script is Graph Builder—Analysis Sets):
1.
Select Graph > Graph Builder.
2.
Drag Validation to the X Zone.
3.
Open the Predictors group, select Mean Radius, and drag it to the Y zone.
4.
From the red triangle menu, select Script > Column Switcher.
5.
From the Column Switcher pop-up window, select Mean Radius as the column
that you will switch, and click OK.
6.
From the next pop-up window, select the Predictors group as the list of columns you
want to switch to, and click OK.
7.
In the Graph Builder window, click Done.
8.
Click on Mean Radius in the Column Switcher list.
9.
Scroll through all 30 predictors or use the animation control arrow to scroll through
automatically.
Exhibit 10.18 sows the plot for Mean Concavity. The plots indicate that the
distributions for the three analysis groups are similar, accounting for the fact
that the training set is three times as big as the two other data sets. There do
not appear to be any pronounced discrepancies in the distributions. Also, there
do not appear to be any observations that are serious outliers relative to the
combined analysis sets. This conclusion extends to the other variables.
At this point, you close all of your open reports. Go the Home window
by clicking on the house icon at the bottom right of any report or data table.
In the Window list, select the reports that you want to close, right-click, and
select Close.
PREDICTION MODELS
Your goal is to explore four modeling approaches:
◾
Partitioning, where you will construct a Decision Tree and a Boosted Tree
model
◾
Stepwise Regression, which you will use to construct a Logistic model

462
V I S U A L S I X S I G M A
Exhibit 10.18 Graph Builder with Mean Concavity as Y
◾Generalized Regression, where you will construct two penalized regression
models: a Lasso and an Elastic Net model
◾Neural, where you will construct a Neural Net model without boosting
and one with boosting
You consider using discriminant analysis, which classiﬁes observations into
groups based on continuous variables. Discriminant analysis is based on the
assumption of multivariate normality of the predictors. That is not the case for
the CellClassiﬁcation.jmp data. Also, although all the predictors in this data
table are continuous, thinking ahead to your test campaign data, you realize
that some of your predictors will be nominal. For these reasons, you decide not
to pursue discriminant analysis.
Each of your seven models will be constructed using your Training set as
a base and by tuning model parameters using the Validation set. To select the
best model, you will assess the performance of your models on the Test set.
Your objective is to minimize the overall misclassiﬁcation rate. One could
argue that your chosen criterion should weight the misclassiﬁcations differently.
Misclassifying malignant tumors as benign may be a more serious error than
misclassifying benign tumors as malignant. To address such a situation, you can
assign a column property called Profit Matrix to a categorical response. The
Profit Matrix allows you to specify losses due to misclassiﬁcation. For simplicity

C L A S S I F I C A T I O N O F C E L L S
463
of exposition, we will not employ the Profit Matrix. But we encourage you to
explore its potential on your own.
RECURSIVE PARTITIONING
The JMP Partition platform provides several methods for constructing deci-
sion trees or classiﬁcation and regression models.5 The platform allows both
the response and predictors to be either continuous or categorical. The methods
include:
◾
Decision Tree
◾
Bootstrap Forest
◾
Boosted Tree
◾
K Nearest Neighbors
The ﬁrst three methods are tree-based algorithms. The Decision Tree
method is the basis for both Bootstrap Forest and Boosted Tree. The K Nearest
Neighbors method is not tree-based, but rather relies on distances to construct
a classiﬁcation or regression model.
You decide to explore the use of just two of these methods to keep your
workload reasonable: Decision Tree and Boosted Tree. The Boosted Tree method
builds a large additive decision tree by ﬁtting small trees to the residuals of
previously ﬁtted trees.
The tree-based methods construct trees by progressively splitting the values
of predictors into two non-overlapping sets, or nodes. Continuous predictors are
split into two nodes according to a cut value, while predictors that are nominal
or ordinal are split into two groups of levels. At each step the two nodes deﬁning
the split are selected so as to maximize the difference in the response across
observations in the node being split. Note that this involves picking a predictor
to split on.
If the response is continuous, the sum of squares due to the difference
between means is a measure of the difference in the two groups. Both the vari-
able to be split on and the cut value for the split are determined by maximizing
a quantity, called the LogWorth, which is related to the p-value associated with
the sum of squares due to the difference between means. The ﬁtted values are
the means within the two groups.
If the response is categorical, as in your case, the splits are determined by
maximizing a LogWorth statistic that is related to the p-value of the likelihood
ratio chi-square statistic, which is referred to as G^2. In this case, the ﬁtted
values are the estimated proportions, or response rates, within the resulting
two groups.

464
V I S U A L S I X S I G M A
The Boosted Tree method builds a large tree using smaller trees that are ﬁt
to the scaled residuals of the previous tree that is ﬁt. The hope is that the new
tree does a better job in ﬁtting observations that were not ﬁt well so far, helping
to reduce unexplained variation. This goes on for a number of stages and then
the trees are weighted and combined into a ﬁnal tree.
The Partition platform is useful both for exploring relationships and for
modeling. It is very ﬂexible, allowing a user to ﬁnd not only splits that are
optimal in a global sense, but also node-speciﬁc splits that satisfy various criteria.
The platform provides various stopping rules, that is, criteria to end splitting.
Stopping rules help prevent you from overﬁtting the data.
Decision Tree Model
You begin by ﬁtting a Decision Tree model. It is the only method available in
the standard version of JMP.
Fitting a Decision Tree Model
Decision Tree is the method that JMP ﬁts by default when you launch the Par-
tition platform.
To ﬁt a Decision Tree model (CellClassiﬁcation.jmp, script is Decision Tree Initial):
1.
Select Analyze > Modeling > Partition.
In the launch window, note that the Method is set to Decision Tree.
2.
Enter Diagnosis as Y, Response.
3.
Enter the group Predictors as X, Factor.
4.
Enter the Validation column as Validation.
5.
Click OK.
The Partition Report
The report shown in Exhibit 10.19 appears. The initial node is shown, indicating
under Count that there are 327 rows of data. These are the observations in your
training set.
In the bar graph in the node and the graph above the node, the color red
indicates malignant values (red triangles) and blue indicates benign values (blue
circles). The colors and markers are those that you assigned in the script Colors
and Markers. Currently there is only one node (All Rows), and the graph above
the node shows the points within a node with random jitter in the horizontal
direction.

C L A S S I F I C A T I O N O F C E L L S
465
Exhibit 10.19 Initial Partition Report
You would like JMP to display the proportions of malignant and benign
records in each node. To see these, select Display Options > Show Split Prob
from the red triangle at the top of the report. The diagram updates to show the
Rate and Prob columns for the two levels of Diagnosis in the initial node, and
the report will show these in all subsequent nodes as well. The Rate statistic
gives the proportion of observations that are in each response level. The Prob
value is the predicted value for the response level, calculated in such a way that
the predicted probability is always nonzero.
Now the initial node appears as shown in Exhibit 10.20. You see that the
benign proportion is 0.6514 and the malignant proportion is 0.3486. The hori-
zontal line separating the benign points from the malignant points in the graph
at the top of the report is at 0.6514.
Splitting
To get a sense of what Partition does, click once on the Split button. JMP deter-
mines the best variable on which to split and the best cutting value for that
variable.
The report, after the ﬁrst split, is shown in Exhibit 10.21. The ﬁrst split is
on the variable Max Concave Points, and the observations are split at the value

466
V I S U A L S I X S I G M A
Exhibit 10.20 Initial Node Showing Split Probabilities
Exhibit 10.21 Decision Tree Report after First Split

C L A S S I F I C A T I O N O F C E L L S
467
where Max Concave Points = 0.15. Of the 98 observations where Max Con-
cave Points ≥0.15 (the leftmost node), 97.96 percent are malignant. Of the
229 for which Max Concave Points < 0.15, 92.14 percent are benign. The plot
at the top updates to incorporate the information about the ﬁrst split, suggest-
ing that the ﬁrst split has done very well in discriminating between benign and
malignant tumors.
You can continue to split manually by clicking the Split button or you can
ﬁt a model automatically by clicking the Go button.
If you continue to split manually, splitting continues until splitting is
stopped by the Minimum Size Split. To see the default value for Minimum Size
Split, select Minimum Size Split from the red triangle menu next to Partition
for Diagnosis. The minimum default size is ﬁve. So you will be able to split
until no further splits of size ﬁve or larger are possible. You can set a larger
value for Minimum Size Split if you think the value of ﬁve results in overﬁtting.
Automatic Splitting
If you split using the Go button, the Partition platform splits until the RSquare
of the validation set no longer improves. Speciﬁcally, splitting is continuous
until the RSquare of the validation set fails to improve for the next ten splits.
To construct the tree automatically (CellClassiﬁcation.jmp, script is Decision Tree
Model):
1.
In the Partition for Diagnosis report window, click Go.
2.
From the red triangle menu next to Partition for Diagnosis, select Show Fit
Details.
The summary report immediately beneath the graph and above the tree
shows that four splits occurred. (See Exhibit 10.22.) The RSquare for the train-
ing set is 0.890 and the RSquare for the Validation set is 0.596. Since the tree
is trained to ﬁt the training set, it is not surprising that the RSquare for the val-
idation set is lower than the RSquare for the training set. The RSquare for the
test set is also given, but it is not of interest to us now.
Split History
When you clicked the Go button, a report called Split History, shown in
Exhibit 10.23, appeared beneath the tree. For each of the training, validation,
and test sets, this report shows the RSquare value plotted against the number

468
V I S U A L S I X S I G M A
Exhibit 10.22 Decision Tree Graph and Summary after Automatic Fitting
Exhibit 10.23 Split History

C L A S S I F I C A T I O N O F C E L L S
469
of splits. The plot only shows RSquare values for up to seven splits. This is
because the Minimum Size Split rule, with the default minimum size for a
node set at ﬁve, ends splitting at seven splits.
You see that the RSquare value for the training set (the top line, shown
in blue) continues to increase over the seven splits. This is expected, since the
models keep ﬁtting ﬁner features of the training set.
The RSquare value for the validation set (the middle line, shown in red)
begins to decrease after the fourth split. This is an indication that, after the
fourth split, models are ﬁtting nuances of the training set that are not shared by
the independent validation set. This suggests that the model has moved from
ﬁtting structure to ﬁtting noise. Since the maximum RSquare for the validation
set occurs for a model with four splits, the four-split model is selected by the
stopping rule and displayed in the tree.
Confusion Matrices
The graph in Exhibit 10.22 indicates that the nodes do a good job of separating
the malignant from the benign cases. Select Show Fit Details from the report’s
red triangle menu to obtain the Fit Details report, which shows numerical
results (Exhibit 10.24). The ﬁrst part of the report shows various measures and
summary information. The Misclassiﬁcation Rate for the Training set is 0.0275.
The Misclassiﬁcation Rate for the Validation set is 0.0789.
The Confusion Matrix report shows the nature of the misclassiﬁcations. The
matrix for the validation set shows that 61 benign tumors were correctly clas-
siﬁed as benign, while 3 benign tumors were misclassiﬁed as malignant. It also
shows that 6 malignant tumors were misclassiﬁed as benign while 44 malignant
tumors were correctly classiﬁed as malignant. This gives the misclassiﬁcation
rate of (3 + 6) ∕(61 + 3 + 6 + 44) = 0.0789.
Exhibit 10.24 Fit Details Report

470
V I S U A L S I X S I G M A
For all three sets, there are more cases of malignant tumors being mis-
classiﬁed as benign than of benign tumors being misclassiﬁed as malignant.
Misclassifying malignant tumors as benign may be a more serious error than
misclassifying benign tumors as malignant. This is something you want to keep
in mind in your model selection.
Decision Tree and Options
Now you go back to your decision tree to see which variables were important
(Exhibit 10.25). The terminal nodes, that is, nodes where no further splitting
occurs, involve Max Concave Points, Max Area, Max Texture, and Max Con-
cavity. So, concavity, area, and texture variables are involved. It’s interesting
that these involve the maximum values of the related measurements, and not
their means or standard deviations. But you need to keep in mind that these
selections might be different with a different validation set, especially because
Exhibit 10.25 Decision Tree

C L A S S I F I C A T I O N O F C E L L S
471
some of the predictors are highly correlated. The Decision Tree method tends to
be sensitive to the nuances of the training and validation sets and to correlations
among predictors.6
You notice that there are many options available in the report’s red trian-
gle menu. These include Leaf Report, Column Contributions, K-Fold Cross
Validation, ROC Curve, Lift Curve, and so on. ROC and lift curves are used
to assess model ﬁt. When a tree is large, the Small Tree View option provides
a plot that condenses the information in the large tree. This small schematic
shows the split variables and the split values in the tree. For reasons of space,
we will not discuss these additional options. We encourage you to explore these
using the documentation in Help.
Saving the Decision Tree Prediction Formula
At this point, you want to save the prediction equation to the data table. You
do this by selecting Save Columns > Save Prediction Formula from the red
triangle at the top of the report (CellClassiﬁcation.jmp, script is Decision
Tree Prediction Formula). This saves three new columns to the data table,
Prob(Diagnosis==B), Prob(Diagnosis==M), and Most Likely Diagnosis.
By clicking on the plus sign to the right of Prob(Diagnosis==M) in the
Columns panel, you are able to view its formula, shown in Exhibit 10.26. The
formula has the effect of placing a new observation into a terminal node based
on its values for the split variables and then assigning to that new observa-
tion a probability of malignancy equal to the proportion of malignant outcomes
observed for training data records in that node. It follows that the classiﬁca-
tion rule consists of determining the terminal node into which a new observa-
tion falls and classifying it into the class with the higher sample proportion in
that node.
The Most Likely Diagnosis column contains the classiﬁcation for each
observation. Click on the plus sign to the right of Most Likely Diagnosis in
the Columns panel. The formula shows that an observation is classiﬁed into
the group “B” or “M” for which the probability of diagnosis into that group is
larger.
With the prediction formula saved to the data table, you are ready to move
on to your next ﬁtting technique.
Boosted Tree
Your next model will be constructed using the Boosted Tree method. The term
boosted connotes ﬁtting models adaptively based on the residuals of previous
models. The “boost” comes from increasing the weights assigned to misclas-
siﬁed observations and decreasing the weights assigned to correctly classiﬁed

Exhibit 10.26 Formula for Prob(Diagnosis==M)
472

C L A S S I F I C A T I O N O F C E L L S
473
observations. Using this approach, subsequent models are forced to focus on
the observations that previous models failed to classify correctly. The results of
all models are combined in a weighted fashion to provide a single ﬁnal decision
tree, called the boosted tree.
In JMP’s implementation, the Boosted Tree method ﬁts a large, additive
decision tree using a sequence of smaller trees. The smaller trees are short
and typically have one to ﬁve splits. Each tree in the sequence is ﬁt on scaled
residuals derived from the previous tree. If a validation column is provided,
the algorithm has the option of terminating tree growth at a point where the
validation statistic no longer improves. The ﬁnal prediction model is obtained
by summing the estimates for each terminal node over all the stages. For a
categorical response such as yours, the ﬁnal predicted probability is a logistic
transformation of the sum of the trees that ﬁt the residuals at each stage.
Fitting a Boosted Tree Model
To ﬁt a boosted tree (CellClassiﬁcation.jmp, script is Boosted Tree Model):
1.
Select Analyze > Modeling > Partition.
2.
Enter Diagnosis as Y, Response.
3.
Enter the group Predictors as X, Factor.
4.
Enter the Validation column as Validation.
5.
From the Method menu beneath the Select Columns list, select Boosted Tree.
6.
Click OK.
The control panel shown in Exhibit 10.27 appears. You decide to ﬁt a single boosted
tree model using the default values provided.
7.
In the control panel, click OK.
You obtain the report shown in Exhibit 10.28.
The Boosted Tree Control Panel
In the control panel (see Exhibit 10.27), you can set the following options that
control the boosted tree ﬁt:
Number of Layers
The maximum number of small trees to ﬁt.
Splits Per Tree
The number of splits for each of the small trees.

474
V I S U A L S I X S I G M A
Exhibit 10.27 Control Panel for Boosted Tree
Exhibit 10.28 Boosted Tree Report
Learning Rate
A value between 0 and 1 that controls the learning speed. Larger values
place higher weight on the residuals and result in faster convergence to a
ﬁnal tree, but because they give high weight to initial trees, they tend to
overﬁt. A default Learning Rate of 0.1 is speciﬁed.

C L A S S I F I C A T I O N O F C E L L S
475
Overﬁt Penalty
A biasing parameter that prevents ﬁtting probabilities of zero. This only
appears for categorical responses.
Minimum Size Split
The minimum number of observations required for a terminal node. Split-
ting stops when all possible splits would result in nodes smaller than the
Minimum Size Split.
Early Stopping
This is checked by default and only appears if validation is used. When the
option is checked, the boosting process stops if ﬁtting additional small trees
fails to improve the validation statistic. If you uncheck this option, then
boosting continues until the speciﬁed Number of Layers is reached.
Multiple Fits over splits and learning rate
Fits a boosted tree for a grid of combinations of Splits Per Tree and Learning
Rate. The grid ranges from the lower values speciﬁed as Number of Layers
and Learning Rate to the higher values speciﬁed below the option as Max
Splits per Tree and Max Learning Rate.
The Boosted Tree Report
The Boosted Tree report is shown in Exhibit 10.28. It echoes the model spec-
iﬁcations, gives overall measures for the three analysis sets, and provides a
confusion matrix for each analysis set.
The Specifications report gives you a summary of various aspects of the
model ﬁt you requested. Note that 43 layers were ﬁt before the Early Stopping
Rule terminated ﬁtting.
The Overall Statistics report is similar to the report shown in Exhibit 10.24
for the Decision Tree ﬁt. Note that the Misclassiﬁcation Rate in the Training set
is 0.0 and in the Validation set is 0.0526. The Confusion Matrix report shows
the nature of the misclassiﬁcations.
Cumulative Validation Plot
The Cumulative Validation graph (Exhibit 10.29) shows various statistics plot-
ted over models with various numbers of layers. You can obtain the exact values
in the plot by selecting Save Columns > Save Cumulative Details from the red
triangle menu. The RSquare statistic achieves its maximum value for a 33-layer
model. From that point, ten more layers are constructed. Since none of these
models improve on the 33-layer model, ﬁtting stops with the 43-layer model
and that model becomes the ﬁnal model.
You can see the small tree ﬁts by selecting Show Trees > Show names cat-
egories estimates from the report’s red triangle menu. A Tree Views outline

476
V I S U A L S I X S I G M A
10
Exhibit 10.29 Cumulative Validation Plot
appears at the bottom of the report. Click its disclosure icon. There are reports
for all 43 layers. Each layer is a small decision tree that has been ﬁt to the resid-
uals of the tree constructed from the layers that precede it. The residuals are
weighted using the Learning Rate.
Open the reports for Layer 1, Layer 24, and Layer 43. Notice how different
predictors enter the splits in an attempt to explain the residuals. Note that the
trees show Estimates for each terminal node. The option Save Columns > Save
Tree Details saves the estimates to a separate data table.
Column Contributions Report
To obtain a sense of which predictors are important in this boosted tree model,
select Column Contributions from the red triangle menu for the report. The
report, shown in Exhibit 10.30, measures a predictor’s contribution using the
number of times it deﬁned a split and the likelihood ratio chi-square (G^2)
for a test that the dichotomized predictor is signiﬁcant. The main contributors
are Max and Mean predictors that measure size, smoothness, concavity, and
texture. Recall that for your simple Decision Tree model, the four splits were
on Max Concave Points, Max Area, Max Texture, and Max Concavity.
Saving the Boosted Tree Prediction Equation
As you did for the Decision Tree model, you save the prediction formula
for the Boosted Tree model to the data table by selecting Save Columns
> Save Prediction Formula from the red triangle at the top of the report
(CellClassiﬁcation.jmp, script is Boosted Tree Prediction Formula).
This saves three new columns to the data table, Prob(Diagnosis==M) 2,
Prob(Diagnosis==B) 2, and Most Likely Diagnosis 2. The sufﬁx “2” is added

C L A S S I F I C A T I O N O F C E L L S
477
Exhibit 10.30 Column Contributions Report
to these column names to distinguish them from the columns with the same
names saved as the Decision Tree prediction formulas.
Click on the plus sign to the right of Prob(Diagnosis==M) 2 in the Columns
panel to view its formula. The formula is a logistic transformation of a constant
(−0.6251) plus 43 “If” functions, each one corresponding to a small tree. Each
“If” function assigns to an observation the Estimate for that observation shown
in the corresponding tree. To see this, take the ﬁrst “If” function and compare
it to the tree shown in the Tree Report for Layer 1.
Note that many predictors are involved in the prediction formula. These
have entered through the Layers. It is difﬁcult to judge the relative importance
of the predictors by viewing the prediction formula. The Column Contribu-
tions report (Exhibit 10.30) is very helpful in assessing which predictors are
important for the boosted tree model.
The Most Likely Diagnosis 2 column contains the classiﬁcation for each
observation. As with the Decision Tree prediction formula, the formula for Most

478
V I S U A L S I X S I G M A
Likely Diagnosis 2 classiﬁes an observation into the group for which the prob-
ability of diagnosis into that group is larger.
With the prediction formula saved to the data table, you are ready to move
on to your next ﬁtting technique.
STEPWISE LOGISTIC MODEL
Next, you proceed to ﬁt a logistic model. Even with only 30 predictors, there
are well over one billion different combinations of these predictors, resulting in
over one billion possible logistic models (without considering potential interac-
tion effects). To settle on one of these models, you will use stepwise regression
to reduce the number of potential predictors that you include in your logistic
model.
Stepwise Regression
To run a Stepwise logistic regression on all 30 predictors (CellClassiﬁcation.jmp, script is
Stepwise Logistic Model):
1.
Select Analyze > Fit Model.
2.
Select Diagnosis and click Y.
3.
Select the Predictors column group and click Add.
4.
Select the Validation column and click Validation.
5.
From the Personality menu, select Stepwise.
6.
Click Run.
A Stepwise report appears. The Stepwise Regression Control panel, where you can
make selections, is shown in Exhibit 10.31.
7.
You accept the defaults and click Go.
Because you have entered a Validation column in the Fit Model launch win-
dow, the default Stopping Rule, shown in Exhibit 10.31, is the Max Validation
RSquare. When a term is entered by Stepwise, the model is ﬁt using the train-
ing set. Then the model is applied to the validation set and the RSquare value
is computed. The Max Validation RSquare stopping rule attempts to maximize
the RSquare value computed for the validation set.
The Direction is set to Forward by default. A forward selection procedure
consists of sequentially entering the most desirable terms into the model while a

C L A S S I F I C A T I O N O F C E L L S
479
Exhibit 10.31 Stepwise Regression Control Panel before Clicking Go
backward selection procedure consists of sequentially removing the least desir-
able terms from the model.
The Go, Stop, and Step buttons control how terms are entered into the
model. Go automatically enters (or removes) terms until the Stopping Rule
limit is reached. Stop interrupts the automatic procedure. Step allows you to
enter or remove terms one step at a time.
Beneath the Go, Stop, and Step buttons you see statistics that summarize
the current model ﬁt. These update as terms are entered or removed. RSquare
Validation is the quantity that the Max Validation RSquare stopping rule
attempts to maximize.
The Current Estimates panel shows estimates of parameters and signiﬁ-
cance probabilities for the model under consideration at a given time. Once you
click Go, three predictors are entered (Exhibit 10.32). Note that the RSquare
Validation value is 0.7829.
The Step History panel (Exhibit 10.33) gives a log of the sequence in
which variables were entered. Also provided are various statistics of interest
with RSquare Validation in the last column. Notice that at Step 3, at the point
where the three predictors have been entered, RSquare Validation is 0.7829.
Stepwise enters ten more predictors after that point. If the RSquare Vali-
dation fails to increase for any of these ten additional steps, then the Stopping
Rule terminates the procedure. Note that the RSquare Validation values for
Steps 4 to 13 are all less than 0.7829. Step 14 reports that the Best model has an
RSquare Validation of 0.7829. The Current Estimates panel shows the selected
predictors for that best model.
Fitting the Logistic Model
To ﬁt the three-predictor model obtained using Stepwise and the training set,
click on Make Model in the Stepwise Regression Control panel, and then Run.

480
V I S U A L S I X S I G M A
Exhibit 10.32 Current Estimates Panel in Stepwise Fit
Exhibit 10.33 Step History Panel in Fit Stepwise

C L A S S I F I C A T I O N O F C E L L S
481
Exhibit 10.34 Fit Nominal Logistic Report for Stepwise Model
From the red triangle next to Nominal Logistic Fit for Diagnosis, select Con-
fusion Matrix. The report is shown in Exhibit 10.34. (Script is Logistic Model.)
Contribution of Predictors
All three predictors (Max Concave Points, Max Radius, and Max Texture) are
signiﬁcant. As in the Decision Tree model, all three predictors are “Max” mea-
surements. Recall that, for the Decision Tree model, the order of contribution
was Max Concave Points, Max Area, Max Texture, and Max Concavity. For the
boosted tree model, Max Concave Points, Max Radius, and Max Texture were
among the ﬁrst six predictors in terms of contribution (Exhibit 10.30).
Confusion Matrices
The Confusion Matrix report shows correct and incorrect classiﬁcation counts.
For the Training set, 9 out of 327 observations were misclassiﬁed. For the Vali-
dation set, 4 out of 114 observations were misclassiﬁed. Keep in mind that the

482
V I S U A L S I X S I G M A
choice of predictors by Stepwise, and hence the choice of model, made integral
use of the training set. The Test set confusion matrix gives you an idea of how
this model will predict for independent data. For the Test set, 6 of the 124 obser-
vations are misclassiﬁed. Four of the misclassiﬁcations result from predicting a
benign mass when the mass is in fact malignant.
Saving the Logistic Prediction Equation
At this point, you save the prediction formula so that you can compare this
Stepwise model with models obtained using other techniques. To save the pre-
diction formula, select Save Probability Formula from the red triangle menu
next to Nominal Logistic Fit for Diagnosis (CellClassiﬁcation.jmp, script is
Logistic Prediction Formula).
This saves a number of formulas to the data table. The columns Prob[M] and
Prob[B] give the probability that the tumor is malignant or benign, respectively.
The column Most Likely Diagnosis gives the diagnosis class with the highest
probability, conditional on the values of the predictors.
Now that you have saved the prediction formula to the data table, you can
close the reports that deal with the logistic ﬁt. Your next adventure is to con-
struct and explore penalized models using the Generalized Regression platform.
GENERALIZED REGRESSION
The Generalized Regression platform in JMP Pro provides shrinkage meth-
ods that construct biased parameter estimates. The biased estimates can provide
better predictive models than do least squares estimates, especially when pre-
dictors are correlated or when there is high dimensionality. Various response
distributions are available. In your case, you will be interested in the Binomial
distribution for Diagnosis.
The platform provides ﬁve estimation methods, but you will be interested
in only two of these, the Lasso and the Elastic Net. These two methods conduct
variable selection as part of the estimation process. The Lasso shrinks parame-
ters by imposing a penalty on the sum of the absolute values of the parameters.
The Elastic Net imposes a penalty on a convex combination of the sum of the
absolute values and the sum of the squares of the parameters. The size of the
penalty is determined by a tuning parameter. The value of the tuning parameter
is selected to optimize a criterion that you select.
Both the Lasso and the Elastic Net have adaptive versions that attempt to
penalize active predictors less than inactive predictors. However, the adaptive
techniques are based on the maximum likelihood estimator (MLE). When pre-
dictors are highly correlated, using the MLE in the adaptive ﬁt can be problem-
atic, and so in these cases it is prudent to avoid using the adaptive versions.

C L A S S I F I C A T I O N O F C E L L S
483
An interesting difference between the Lasso and the Elastic Net techniques
is the following. When predictors are correlated, the Lasso tends to include only
one of the variables from the correlated group in the model. The Elastic Net, on
the other hand, tends to include all variables from the correlated group.7
Your plan is to construct both a Lasso and an Elastic Net model using the
Generalized Regression platform.
Specifying the Generalized Regression Model
Generalized Regression is a Personality that you access from the Fit Model
speciﬁcation window. When you click Run in the Model Speciﬁcation window,
you can then launch various ﬁts using the Model Launch outline.
To ﬁt a Generalized Regression model (CellClassiﬁcation.jmp, script is Gen Regr
Model Launch):
1.
Select Analyze > Fit Model.
2.
Select Diagnosis and click Y.
3.
Select the Predictors column group and click Add.
4.
Select the Validation column and click Validation.
5.
From the Personality menu, select Generalized Regression.
6.
From the Distribution list, select Binomial.
7.
Click Run.
A Generalized Regression report appears. The Model Launch panel, with
the Advanced Controls outline open, is shown in Exhibit 10.35.
The Model Launch window lets you choose one of ﬁve estimation methods.
One of these is Maximum Likelihood, which allows you to compare a penalized
ﬁt to a traditional ﬁt. The Adaptive Lasso model is the default. Note that you
can also select Forward Selection, Elastic Net, and Ridge.
The Advanced Controls outline enables you to make selections regarding
the tuning parameter grid. It also provides a way to force terms into the model.
You will ﬁt models accepting the defaults in the Advanced Controls panel.
Several Validation Methods are available. Because you included a Validation
column in the Model Speciﬁcation window, the Validation Method is set to
Validation Column.
A viable alternative approach to cross-validation would be to combine your
Training and Validation sets and conduct KFold cross-validation on the 441

484
V I S U A L S I X S I G M A
Exhibit 10.35 Model Launch Panel Showing Advanced Controls
observations in those sets. However, for simplicity, consistency, and continuity,
in this section you will ﬁt penalized regression models using the Validation
column for cross-validation. (Note that while Partition calculates a KFold
cross-validation statistic as a report option, it does not actually ﬁt models using
a KFold approach.)
Fitting a Lasso Model
You uncheck the box next to Adaptive and click Go to ﬁt a Lasso model
(CellClassiﬁcation.jmp, script is Lasso Model).
The Lasso with Validation Column Validation Report
The report shown in Exhibit 10.36 appears. The Parameter Estimates for
Original Predictors outline, which is closed in Exhibit 10.36, is shown in
Exhibit 10.37.
The Model Summary outline provides descriptive information about the
model you selected, followed by measures of ﬁt for the model for each of the
Training, Validation, and Tests sets. Since the statistics under Measure depend
on the sample size, they cannot be compared across the three sets. But they can
be used to compare models built on the Training set. The Estimation Details
outline echoes the selections under Advanced Controls.

Exhibit 10.36 Lasso Report
485

486
V I S U A L S I X S I G M A
Exhibit 10.37 Parameter Estimates for Original Predictors
The Solution Path outline shows two plots:
◾The plot on the left shows the scaled parameter estimates as a func-
tion of the Magnitude of Scaled Parameter Estimates. The parameter
estimates are for a model given in terms of centered and scaled pre-
dictors. The Magnitude of Scaled Parameter Estimates is the sum of the
absolute values of the scaled parameter estimates for the model (exclud-
ing the intercept, dispersion, and zero-inﬂation parameters). It reﬂects
the amount of shrinkage, or bias, used in obtaining the given parameter
estimates. Estimates with large magnitudes are close to the maximum
likelihood estimate. Those with small values are heavily penalized.
◾The plot on the right shows the Scaled-LogLikelihood as a function of
the Magnitude of Scaled Parameter Estimates. The Scaled-LogLikelihood
is the statistic used to assess model ﬁt when holdback validation is per-
formed. Smaller values indicate better ﬁt than larger values.

C L A S S I F I C A T I O N O F C E L L S
487
Parameter Estimates Plot
In ﬁtting a generalized regression model, predictors are ﬁrst centered and scaled.
The horizontal axis is given in terms of the sums of the absolute values of the
parameter estimates for the centered and scaled predictors. The tuning param-
eter increases as the Magnitude of the Scaled Parameter Estimates decreases.
The tuning parameter is 0 at the maximum likelihood estimator (MLE).
When possible, the horizontal axis in the Parameter Estimates plot ranges
from 0 to the magnitude of the scaled parameter estimates at the MLE. However,
in the case of a binomial response, separation often occurs before the estimates
reach the MLE. Separation occurs when, for some interval deﬁned by a linear
combination of the predictors, all observations assume the same value for the
response. In such a situation, the MLE does not exist and estimates for values
of the tuning parameter near zero are unstable. For this data set and model,
separation occurs at some point, so the horizontal scale is cut off once separation
is reached.
The blue paths show the values of the parameter estimates. Clicking on any
of the paths highlights the corresponding term in the list Parameter Estimates
for Original (or Scaled) Predictors report and selects the related column in the
data table.
The red vertical line in the plot marks the solution obtained using the val-
idation procedure. The Parameter Estimates for Original Predictors report
for this solution is shown in Exhibit 10.37. You can move the red vertical line
to explore other solutions (models). The estimates shown in the list Parameter
Estimates for Original (or Scaled) Predictors update as you move it.
At the optimal solution, 21 terms have parameter estimates of 0, leaving
9 terms in the model. The lasso procedure has effectively conducted variable
selection as part of the estimation procedure.
Note that the ﬁrst three options in the red triangle menu for Lasso with
Validation Column Validation give you control over your view of the paths.
You can highlight paths for the non-zeroed terms or the zeroed terms or hide
paths for the zeroed terms.
Contribution of Predictors
To get a sense of which predictors are important, open the Parameter Esti-
mates for Centered and Scaled Predictors report. Centering and scaling the
predictors gives them comparable ranges of values and so, in a sense, puts them
on equal footing. Comparing the absolute values of the parameter estimates
for the transformed predictors gives you a sense of which are most important
relative to the predictive model.
Right-click in the Parameter Estimates for Centered and Scaled Predictors
report and select Sort by Column. In the Select Columns window, select

488
V I S U A L S I X S I G M A
Exhibit 10.38 Parameter Estimates for Centered and Scaled Predictors
Estimate and check the Ascending box, and then click OK. The report now
appears as shown in Exhibit 10.38.
Since all estimates except for the intercept are negative, the report now
lists the predictors in order of the absolute value of their estimates. (This was
an expedient way to list the predictors in that order. In general, you would
right-click on the table, select Make into Data Table, create a column of abso-
lute values for Estimate in the data table, and then sort that column.)
The variable Max Radius is the largest contributor to the model, followed
by Max Texture. Compare Exhibit 10.38 with Exhibit 10.30 to see that the order
of contribution is fairly consistent with the Boosted Tree model. Another way to
assess sensitivity to predictors is to use the Variable Importance feature found
in the proﬁler. Variable Importance measures sensitivity to predictors in a way
that does not depend on the model type, but only on the prediction equation.
The method is based on simulating predictor values. You are given several
choices for how to do the simulation. See the JMP documentation for details.8

C L A S S I F I C A T I O N O F C E L L S
489
Scaled -LogLikelihood Plot
In the Solution Path report, the graph on the right plots the average
-LogLikelihood for both the Training and Validation sets against the Magnitude
of the Scaled Parameter Estimates. In your case, the solution is the set of
parameter estimates that minimize the -LogLikelihood for the Validation set.
Because -LogLikelihood depends on the number of observations, you can’t
compare the -LogLikelihood of the Training and Validation sets directly. But you
can compare the mean -LogLikelihood for each set. The Scaled -LogLikelihood
plot shows that the Training set has uniformly smaller average -LogLikelihood
than the Validation set, which is not surprising.
The point where the -LogLikelihood of the Validation set is smallest is
around a magnitude of 146. To the right of that point, -LogLikelihood increases,
indicating overﬁtting. However, for a range of magnitudes smaller than 146,
-LogLikelihood does not increase noticeably, suggesting that perhaps another
term (SE Radius) could be removed from the prediction equation. However,
for the purpose of this example you will retain SE Radius as a predictor and
obtain a prediction equation for the optimal solution identiﬁed by the platform.
Saving the Lasso Prediction Equation
You now save the prediction equation for your Lasso model by selecting
Save Columns > Save Prediction Formula from the red triangle menu
next to Lasso with Validation Column Validation (CellClassiﬁcation.jmp,
script is Lasso Prediction Formula). Three columns are saved to the data
table: Probability(Diagnosis=B), Probability(Diagnosis=M), and Most Likely
Diagnosis 4.
To see how well this model predicts, you can use Tabulate to construct
tables that are analogous to confusion matrices.
To construct Confusion Matrices for this model (CellClassiﬁcation.jmp, script is Lasso
Confusion Matrices):
1.
Select Analyze > Tabulate.
2.
Drag Diagnosis from the list of columns to the Drop zone for rows in the
template.
3.
Drag Most Likely Diagnosis 4 from the list of columns to the top right cell in the
table template, where you see N.
4.
Drag Validation from the list of columns and deposit it above Most Likely
Diagnosis 4 in the table template (release it when you see a horizontal blue line in the
cell, as shown in Exhibit 10.39).

490
V I S U A L S I X S I G M A
Exhibit 10.39 View of Template Immediately before Depositing Validation
Exhibit 10.40 Completed Table Showing Confusion Matrices for Lasso Model
The resulting table appears as shown in Exhibit 10.40. Three observations
are misclassiﬁed in the Validation set.
Fitting an Elastic Net Model
You now proceed to ﬁt an elastic net model.
To ﬁt an Elastic Net model (CellClassiﬁcation.jmp, script is Elastic Net Model):
1.
Specify the model by following the steps in the section “Specifying the Generalized
Regression Model” or run the script Gen Regr Model Launch.
2.
From the Estimation Method menu, select Elastic Net.
3.
Uncheck the box next to Adaptive.
4.
Click Go.
The Elastic Net with Validation Column Validation Report
The Model Summary and Solution Path reports are shown in Exhibit 10.41.
The Solution Path report shows that the optimal solution occurs at the right
end of the Magnitude of Scaled Parameter Estimates range. This is where

Exhibit 10.41 Model Summary and Solution Path Reports for Elastic Net
491

492
V I S U A L S I X S I G M A
separation begins to be a problem. The plot indicates that there are more
non-zero estimates involved in the solution for the Elastic Net than for the
Lasso. The Parameter Estimates report indicates that only nine terms have
zero as their estimate, leaving 21 terms in the model.
The -LogLikehood values for the Training, Validation, and Test sets are
smaller for the Elastic Net model than for the Lasso model. The Bayesian
Information Criterion (BIC) and corrected Akaike’s Information Criterion
(AICc) values are all larger for the Elastic Net model than for the Lasso model.
This is because the Elastic Net tends to include all predictors within a correlated
group, whereas the Lasso tends to only choose one.
Contribution of Predictors
To get a sense of which predictors are important, open the Parameter Esti-
mates for Centered and Scaled Predictors report. Recall that centering and
scaling the predictors puts them on equal footing. You will compare the abso-
lute values of the parameter estimates for the transformed predictors to get a
sense of which are most important in the predictive model.
To rank the absolute values of the parameter estimates (CellClassiﬁcation.jmp, script is
Elastic Net Ranked Predictors):
1.
Right-click in the Parameter Estimates for Centered and Scaled
Predictors report.
2.
Select Make into Data Table.
3.
Right click on the Estimate column head and select New Formula Column
Transform > Absolute Value.
A new column, with a stored formula, is created.
4.
Rename the new column Absolute Estimate.
5.
Right-click on the Absolute Estimate column header and select Sort >
Descending.
6.
In the Columns panel, drag the Absolute Estimate column name to immediately
follow the Term column name.
The ﬁrst three columns in your new data table now appear as shown in
Exhibit 10.42. Note that eight of the ten “Max” predictors appear ﬁrst on the
list. The predictors Max Radius and Max Texture were the top two predictors
for the Lasso model, they were two of the three predictors selected by stepwise
for the Logistic model, and they were among the highest contributors for the

C L A S S I F I C A T I O N O F C E L L S
493
Exhibit 10.42 Parameter Estimates for Centered and Scaled Predictors Ranked by Absolute Value
Boosted Tree model. Keep in mind, though, that many of the predictors are
highly correlated, making it difﬁcult to compare your models in terms of which
predictors are important.
Saving the Elastic Net Prediction Equation
You now save the prediction equation for your Elastic Net model by selecting
Save Columns > Save Prediction Formula from the red triangle menu next to
Elastic Net with Validation Column Validation (CellClassiﬁcation.jmp, script
is Elastic Net Prediction Formula).
Three columns are saved to the data table: Probability(Diagnosis=B) 2,
Probability(Diagnosis=M) 2, and Most Likely Diagnosis 5.

494
V I S U A L S I X S I G M A
Exhibit 10.43 Completed Table Showing Confusion Matrices for Elastic Net Model
To see how well this model predicts, you can use Tabulate, as described
in the section Saving the Lasso Prediction Equation, to construct tables that
are analogous to Confusion Matrices. You will obtain the table shown in
Exhibit 10.43. Again, three observations are misclassiﬁed in the Validation set.
NEURAL NET MODELS
In your study of data-mining techniques, you have read about how neural nets
can be used for classiﬁcation and prediction. So you are interested in exploring
neural net classiﬁcation models for your data. You will ﬁt one neural net model
without boosting and one with boosting.
Background
Neural net algorithms were originally inspired by how biological neurons are
believed to function. Starting in the 1940s, scientists in the area of artiﬁcial
intelligence pursued the idea of designing algorithms that can learn in a way
that emulates neuron function in the human brain.
In fact, the science of biologically informed computation has its origins
in a seminal paper called “A Logical Calculus of Ideas Immanent in Nervous
Activity.”9 Implicit in this paper’s use of logic and computation to describe the
nervous system was the concept that ideas are carried by the collection of neu-
rons as a whole, rather than being tied to a speciﬁc neuron. Research since
these early days has leveraged this idea of distributed processing, and neural
nets typically have an input layer of neurons, an output layer, and a hidden
layer where processing occurs.10
In a mathematical sense, a neural net is nothing more than a nonlinear
regression model. In its implementation of neural net models, JMP uses stan-
dard nonlinear least squares regression methods. Although a general neural
net can have many hidden layers, one layer is considered sufﬁcient for most
modeling situations. JMP Pro provides two hidden layers, but uses a single
hidden layer as the default. Each hidden neuron or node is modeled using a
transformation (TanH, Linear, or Gaussian) applied to a linear function of the

C L A S S I F I C A T I O N O F C E L L S
495
predictors. In a classiﬁcation situation, the output value results from a logistic
function applied to a linear function of the outputs from the hidden nodes. For
example, this means that for 30 input variables, one response, and one layer
with k hidden nodes, the number of parameters to be estimated is (31 × k) +
(k + 1).
With so many parameters, it is easy to see that a major advantage of a neural
net is its ability to model a variety of patterns of response. But ﬁtting this many
parameters comes at a cost. Because the criterion that is optimized usually has
many local optima, convergence to a global optimum can be difﬁcult. Also,
with so many parameters, overﬁtting is problematic. This is why validation sets
are critical to neural net modeling strategies. Another disadvantage of neural
net models is that they are not interpretable beyond the relationship between
inputs and output, due to the hidden layer.
Neural Platform in JMP
The Neural Net algorithm in JMP uses a ridge-type penalty to help minimize
overﬁtting. Because of its similarity to a ridge-regression penalty function, the
neural net overﬁtting penalty not only addresses overﬁtting, but also helps mit-
igate the effects of multicollinearity.11
You can set the number of nodes in the hidden layer. Note that a small
number of hidden nodes can lead to underﬁtting and a large number can lead to
overﬁtting, but small and large are relative to the speciﬁc problem under study.
For a given number of hidden nodes, each application of the ﬁtting algorithm
has a random start, and JMP refers to these individual ﬁts as tours. Based on
experience, about 16 to 20 tours are recommended in order to ﬁnd a useful
optimum.
JMP provides four validation methods to help you select a neural net model
that predicts well for new data:
◾
Excluded Rows Holdback
◾
Holdback
◾
KFold
◾
Validation Column
Excluded Rows Holdback, Holdback, and Validation Column are holdback
methods. Excluded Rows Holdback treats excluded rows as a holdback sample.
Holdback allows you to set a holdback proportion and then randomly selects a
sample of that size to use as a holdback sample. If you specify a Validation set in
the model launch window, then Validation Column treats your validation set
as the holdback sample.
In the holdback methods, a sample of the observations is withheld (the hold-
back sample) while the remaining observations are used to train a neural net.

496
V I S U A L S I X S I G M A
JMP constructs a neural net model for the training sample. Then it applies the
model to the holdback sample and calculates a LogLikelihood value for that
sample. The LogLikelihood of the holdback sample determines when the algo-
rithm stops ﬁtting.
You can vary the number of hidden nodes and other aspects of the model
in an attempt to ﬁnd a ﬁt that generalizes well to the holdback sample. This
method works well for large numbers of observations, where one can easily ﬁt
a model to 75 percent or fewer of the observations. JMP uses two-thirds of the
complete data set by default.
The fourth method is called KFold cross-validation. Here, a neural net
model is ﬁt to all of the data to provide starting values. Then, the observations
are divided randomly into K groups (or folds). In turn, each of these groups
is treated as a holdback sample. For each of the K groups, a model is ﬁt
to the data in the other (K −1) folds, using the starting values from the
full ﬁt. The model is then extended to the holdback group. The LogLikelihood
is calculated for each holdback sample, and these are averaged to give an
average LogLikelihood that represents how the model might perform on new
(Validation) observations. The starting values from the full ﬁt are used because
the function being optimized is multimodal, and this practice attempts to bias
the estimates for the submodels to the mode of the overall ﬁt.
A First Model: Neural Net 1
Your ﬁrst model will ﬁt a neural net without boosting. You will use your Vali-
dation column to deﬁne a validation set.
To obtain the neural net model launch dialog, do the following (CellClassiﬁcation.jmp,
script is Neural Net Model Launch):
1.
Select Analyze > Modeling > Neural.
2.
Select Diagnosis and click Y, Response.
3.
Select the group Predictors and click X, Factor.
4.
Select Validation and click Validation.
5.
Click OK.
The Neural report appears showing the Model Launch panel (Exhibit 10.44).
The note above the Model Launch panel indicates that the Validation col-
umn will be used for cross-validation.

C L A S S I F I C A T I O N O F C E L L S
497
Exhibit 10.44 Neural Net Model Launch Panel
There are three panels in the Model Launch outline:
◾
The Hidden Layer Structure panel allows you to specify whether you
want to use one or two layers, and how many and which activation
functions you want to select. This deﬁnes the base structure.
◾
The Boosting panel allows you to request boosting. The idea is simi-
lar to the concept behind boosted trees. A sequence of small models is
constructed, each ﬁt to the scaled residuals of the previous model. The
predictions from the models are combined to form a ﬁnal model. Boost-
ing uses cross-validation to determine a stopping point. The number of
nodes in the ﬁnal boosted model is the number of nodes speciﬁed for
the base structure times the number of models ﬁt by boosting.
◾
The Fitting Options panel allows you to transform continuous predic-
tors to normality, to specify a penalty, and to set the number of tours.
Recall that a tour is a restart of the ﬁtting process.
The default Hidden Layer Structure is a model with one layer and three
hidden nodes. Each of the three nodes uses the TanH, or hyperbolic tangent,
activation function.

498
V I S U A L S I X S I G M A
Fitting the Neural Net 1 Model
You will ﬁrst ﬁt this model, and then you will be able to obtain a diagram of its
structure.
To ﬁt your ﬁrst Neural Net model and to see its diagram (CellClassiﬁcation.jmp, script is
Neural Net Model 1):
1.
In the Neural Net Model Launch panel, set the Number of Tours to 20.
Increasing the number of tours increases the likelihood of ﬁnding a global optimum.
2.
Click Go.
3.
From the red triangle next to Model NTanH(3), select Diagram.
The Model NTanH(3) Report
The Model NTanH(3) report appears (Exhibit 10.45), giving Measures, Con-
fusion Matrices, and a Diagram plot. Because ﬁtting a neural net involves a
random component, your results will differ from those shown in the exhibit.
If you want to replicate the results in Exhibit 10.45, run the script Neural Net
Model 1. The script controls the random component by specifying a value for
the random seed random seed.
Exhibit 10.45 Model NTanH(3) Report

C L A S S I F I C A T I O N O F C E L L S
499
Note that the Misclassiﬁcation Rates across the data sets are small, on the
order of 0.03. The Confusion Matrices give more detail on the performance of
your model relative to misclassiﬁcation.
The Diagram plot, shown in Exhibit 10.46, shows the 30 input variables, the
three hidden nodes (H1, H2, and H3), and their combination in the prediction
of the probability of Diagnosis classes. The ﬁnal model is a logistic function of
a linear combination of three models, each of which relates one of the hidden
nodes, H1, H2, and H3, to the 30 predictors.
From the red triangle next to Model NTanH(3), select Estimates. The Esti-
mates report provides a list of parameters estimated. Note that a total of 97
parameters have been estimated. To see this, right-click in the report and select
Make into Data Table. The table has 97 rows. (Note that because of the random
starts, when you run this report on your own, your results will differ slightly
from those shown here.)
To get some insight on the 30-predictor model, you explore a few other
options provided by the Neural Net report. From the red triangle for Model
NTanH(3), select Categorical Profiler. This adds a Prediction Proﬁler plot, a
portion of which is shown in Exhibit 10.47.
The dotted vertical red lines represent settings for each of the predictors.
The trace shown in the proﬁler cell above a given predictor represents the cross
Exhibit 10.46 Neural Net 1 Diagram: 30 Predictors and 3 Hidden Nodes

Exhibit 10.47 Partial View of Categorical Profiler for Neural Net Model 1
500

C L A S S I F I C A T I O N O F C E L L S
501
section of the ﬁtted model for Prob(Diagnosis=B) for that variable, at the given
settings of the other variables. When you change one variable’s value, you can
see the impact of the change on the surface for all other variables. Note that the
surfaces appear to be fairly smooth with some steep peaks, but no very jagged
areas.
For another way to visualize the surface, select Surface Profiler from
the top red triangle menu. This gives a three-dimensional view of the effect
of predictor variables, taken two at a time, on Prob(Diagnosis=B) and
Prob(Diagnosis=M).
Saving the Neural Net 1 Prediction Equation
You now save the prediction equation for your Neural Net model by selecting
Save Profile Formulas from the red triangle menu next to ModelNTanH(3)
(CellClassiﬁcation.jmp, script is Neural Net 1 Prediction Formula).
Three columns are saved to the data table: Probability(Diagnosis=B) 3,
Probability(Diagnosis=M) 3, and Most Likely Diagnosis 6.
If you had selected Save Formulas, you would have obtained six formu-
las: the three formulas described above; and the formulas for the three hidden
nodes, called H1_1, H1_2, and H1_3. The Save Formulas option shows the
Probability(Diagnosis=B) and Probability(Diagnosis=M) formulas in terms of
the three hidden nodes. Probability(Diagnosis=B) applies a logistic function
to the estimated linear function of the hidden nodes. If you would like some
insight on what a neural net ﬁt is like, explore these formulas.
A Second Model: Neural Net 2
For your second Neural Net model, you decide to explore boosting.
Fitting the Neural Net 2 Model
The Neural Net report allows you to ﬁt multiple models in the single report
window. To do this, go to the top of the report and open the Model Launch
panel. Alternatively, you can click the red triangle next to Neural and select
Script > Relaunch Analysis and click Go in the window that appears.
To ﬁt your second Neural Net model (CellClassiﬁcation.jmp, script is Neural Net
Model 2):
1.
Return to the Neural Net Model Launch panel in one of the following ways:
Open the Model Launch outline in your open Neural report;
Run the script Neural Net Model Launch and click OK in the window.

502
V I S U A L S I X S I G M A
2.
In the Boosting panel, set the Number of Models to 10.
3.
In the Fitting Options panel, set the Number of Tours to 8.
4.
Click Go.
The Model NTanH(3)NBoost(10) Report
The report shown in Exhibit 10.48 appears. This second model has a lower
misclassiﬁcation rate than the ﬁrst model on the Training and Validation data,
but a higher misclassiﬁcation rate on the Test data.
The Diagram report for this model, shown in Exhibit 10.49, shows 30 acti-
vation functions. There are three nodes, and ten models were ﬁt as part of the
boosting process.
This model requires the estimation of 961 parameters. Again, to see this,
select Estimates from the red triangle menu for Model NTandH(3)NBoost(10).
Then right-click in the report and select Make into Data Table.
Saving the Neural Net 2 Prediction Equation
You now save the prediction equation for your second Neural Net model. To
save the prediction formula, select Save Profile Formulas from the red triangle
Exhibit 10.48 Model NTanH(3)NBoost(10) Report

C L A S S I F I C A T I O N O F C E L L S
503
Exhibit 10.49 Neural Net 2 Diagram: 30 Predictors, 3 Hidden Nodes, and 10 Boosted Models
menu next to Model NTanH(3)NBoost(10) (CellClassiﬁcation.jmp, script is
Neural Net 2 Prediction Formula).
Three columns are saved to the data table: Probability(Diagnosis=B) 4,
Probability(Diagnosis=M) 4, and Most Likely Diagnosis 7.
For simplicity and continuity in the exposition, we have ﬁt neural nets using
the Validation column. However, your data set, containing 569 observations,
is a moderate-sized data set and KFold cross-validation might provide better
models. You might want to explore the use of Kfold cross-validation in ﬁtting
neural net models to these data.
COMPARISON OF CLASSIFICATION MODELS
You have ﬁt seven different models to your data:
◾
Decision Tree (Partition)
◾
Boosted Tree
◾
Logistic

504
V I S U A L S I X S I G M A
◾Lasso
◾Elastic Net
◾Neural Net (no Boosting)
◾Neural Net (with Boosting)
Each of these models was ﬁt using your Validation data set for
cross-validation. At this point, you are ready to select a single model by
comparing the performance of all seven models on your test set. Your main
interest is the misclassiﬁcation rate.
To compare your seven models on the Test set (CellClassiﬁcation.jmp, script is Model
Comparison):
1.
Select Analyze > Modeling > Model Comparison.
2.
Enter Validation as By.
This will give you an analysis for each of your validation sets for all of the saved
prediction formulas. Your interest is in the results for the Test set.
3.
Click OK.
Tip: You could enter the 14 Prob formulas as Y, Predictors. But if you click OK without entering
Prob formulas, JMP assumes that you want to enter all probability formula columns into the analysis.
The Model Comparison report is shown in Exhibit 10.50. You will use the
test set results to select a ﬁnal model. But before you do this, compare the mis-
classiﬁcation rates for:
◾The training set (Model Comparison Validation = Training outline)
◾The validation set (Model Comparison Validation = Validation out-
line)
◾The test set (Model Comparison Validation = Test outline)
The rates for the training set are uniformly smaller than for the validation
set. This is expected, because the models are being ﬁt to the training data, but
using the validation set for selection within a given class of models. Note that the
rates for the validation set are generally smaller than the rates for the test set.
This is also to be expected, because the validation set is used in ﬁtting the mod-
els. This phenomenon explains the need for an independent test set to assess
the predictive ability of each model and to help choose a ﬁnal model.
For this reason, you base your choice of a ﬁnal model on the test set results.
In the Measures of Fit for Diagnosis report for the Test set (in the Model

C L A S S I F I C A T I O N O F C E L L S
505
Exhibit 10.50 Model Comparison Report
Comparison Validation = Test outline), the Misclassiﬁcation Rate column indi-
cates that the smallest misclassiﬁcation rates are given by the Elastic Net model
and Neural Net 1. (Open the Predictors outline to see which Probability columns
are associated with each of the Neural Net models.) Neural Net 2 is the next best
model based on misclassiﬁcation rate.
As well as matching the best misclassiﬁcation rate, the Elastic Net model
also provides interpretability in the form of information about the predictors
that are important in the prediction equation. You saw this in Exhibit 10.42.
A neural net is not interpretable in this sense.
Select Confusion Matrix from the red triangle menu in the Model Com-
parison Validation = Test outline. This shows the misclassiﬁcation detail for
all the models. Exhibit 10.51 shows the detail for the Elastic Net and the two
Neural Net models.
Although the overall misclassiﬁcation rates for the Elastic Net and Neural
Net 1 models are the same, the Neural Net 1 model errs in the direction of clas-
sifying benign tumors as malignant, whereas the Elastic Net model errs in the

506
V I S U A L S I X S I G M A
Exhibit 10.51 Confusion Matrices for Elastic Net, Neural Net 1, and Neural Net 2
other direction, tending to classify malignant tumors as benign. Interestingly,
Neural Net 2 tends in this direction as well.
Your goal was to evaluate models based on overall misclassiﬁcation rate. On
that basis, the Elastic Net model and Neural Net 1 are tied. The Elastic Net is
more interpretable and might be preferred for this reason.
If overlooking a malignant tumor is more serious than incorrectly classify-
ing a benign tumor, you might select Neural Net 1 as the best model. However,
if misclassiﬁcations differ in the severity of their consequences, you might base
your classiﬁcations on a Profit Matrix. Alternatively, you might use a Weight
column that gives greater weight to rows with malignant outcomes and ﬁt mod-
els using that Weight column.
We encourage you to explore these approaches on your own. The Profit
Matrix uses your existing models, but bases classiﬁcation on expected proﬁt.
To apply the Profit Matrix column property to Diagnosis, give a large loss

C L A S S I F I C A T I O N O F C E L L S
507
(for example, −5) to the cell Actual = M and Decision of Prediction = B. Clear
all the current prediction formulas from your data table. Rerun the seven pre-
diction scripts. (The script Fit All Prediction Formulas ﬁts all seven models.)
Run Model Comparison by entering the Validation column as a By variable and
then clicking OK. Review the Confusion Matrices.
CONCLUSION
Your goal in this analysis was to explore some of the features that JMP provides
to support classiﬁcation and data mining. You began by using various visual-
ization techniques to develop an understanding of the data and relationships
among the variables.
You were interested in investigating partition, logistic, generalized regres-
sion, and neural net ﬁts. Given that your goal was to learn about these plat-
forms, you constructed various models in a fairly straightforward way. The best
classiﬁcation, based on performance on your test set, was obtained by two mod-
els: an Elastic Net model and a Neural Net model. We note that you could have
taken a number of more sophisticated approaches to your modeling endeavor,
had you so desired.
Among your seven models, the Decision Tree model had the worst perfor-
mance. In our experience, JMP Partition models tend not to perform as well as
nonlinear (or linear) regression techniques when the predictors are continuous.
They can be very useful when there are categorical predictors, and especially
when these have many levels. Moreover, unlike Neural Net models, Partition
models are very intuitive and interpretable, which makes them all the more
valuable for data exploration. In your situation, where classiﬁcation is the pri-
mary goal, the interpretability of the model is less important than its ability to
classify accurately.
We also want to underscore the importance of guarding against overﬁt-
ting, which, in the case of Neural Net models, often results in claims of exag-
gerated model performance. In the case study, you used a validation set for
cross-validation in constructing your Neural Net models to maintain consis-
tency with the partition and generalized regression approaches. However, using
KFold cross-validation in constructing a Neural Net model might lead to better
models.
Also, in the case of neural nets, where overﬁtting is so easy, we strongly
recommended that model performance be assessed on a genuinely indepen-
dent data set. Without such an approach, claims about the model’s predictive
performance are likely to be overly optimistic.

508
V I S U A L S I X S I G M A
Although prediction was your goal in this case study, the interpretability of
the predictive model is often of interest. You saw that penalized regression mod-
els, obtained using the Generalize Regression personality in Fit Model, provide
variable reduction and a high level of interpretability.
Armed with this new understanding of JMP’s capabilities for predictive
modeling, you feel conﬁdent in tackling the Cellularplex test campaign data
when they arrive in a couple of months.
NOTES
1. Olvi L. Mangasarian, W. Nick Street, and William H. Wolberg, “Breast Cancer Diagnosis and
Prognosis via Linear Programming,” Mathematical Programming Technical Report 94, no. 10
(December 19, 1994): 1–9.
2. For more detail on these characteristics, see W. Nick Street, William H. Wolberg, and Olvi L.
Mangasarian, “Nuclear Feature Extraction for Breast Tumor Diagnosis,” International Symposium
on Electronic Imaging: Science and Technology 1905 (1993): 861–870.
3. Mangasarian, Street, and Wolberg, “Breast Cancer Diagnosis and Prognosis via Linear Pro-
gramming”; and Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Sta-
tistical Learning: Data Mining, Inference, and Prediction (New York, NY: Springer, 2001), 108–111,
371–389.
4. See Christopher M. Bishop, Neural Networks for Pattern Recognition (New York, NY: Oxford Uni-
versity Press, 1995), 372; and B. D. Ripley, Pattern Recognition and Neural Networks (New York,
NY: Cambridge University Press, 1996), 3–8.
5. John P. Sall and Cathy Maahs-Fladung, “Trees, Neural Nets, PLS, I-Optimal Designs and Other
New JMP® Version 5 Novelties,” SUGI 27, http://www2.sas.com/proceedings/sugi27/p268-27
.pdf (accessed February 10, 2016).
6. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data
Mining, Inference, and Prediction (New York, NY: Springer, 2001), 274.
7. Hui Zou and Trevor Hastie, “Regularization and Variable Selection via the Elastic Net,” Journal
of the Royal Statistical Society, Series B, 67, Part 2, 301–320.
8. SAS Institute Inc. 2015, “JMP 12® Proﬁlers” (Cary, NC: SAS Institute Inc.).
9. Warren S. McCulloch and Walter Pitts, “A Logical Calculus of the Ideas Immanent in Nervous
Activity,” Bulletin of Mathematical Biophysics 5 (1943): 115–133.
10. Michael J. A. Berry and Gordon Linoff, Data Mining Techniques: For Marketing, Sales, and Cus-
tomer Relationship Management (New York, NY: John Wiley & Sons, Inc., 1997), 286–334; Hastie,
Tibshirani, and Friedman, The Elements of Statistical Learning, 347–369; Simon Haykin, Neural
Networks: A Comprehensive Foundation, 2nd edition (New York, NY: Prentice Hall, 1998); and
Ripley, Pattern Recognition and Neural Networks, 143–180.
11. Christopher M. Gotwalt, “JMP 9 Neural Platform Numerics,” JMP White Paper, www.jmp
.com/content/dam/jmp/documents/en/white-papers/wp-jmp9-neural-104886.pdf (retrieved
July 4, 2015).

PART
THREE
Supplementary
Material
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

C H A P T E R 11
Beyond “Point and
Click” with JMP
511
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

512
V I S U A L S I X S I G M A
T
hus far we have emphasized that Visual Six Sigma projects, because they
are focused on discovery, necessarily involve working with data interac-
tively to uncover and model relationships. The case histories in the pre-
ceding six chapters illustrate how JMP can support this pattern of use better
than most other software.
However, as a project nears completion in the Utilize Knowledge step, there
is a need to preserve and perpetuate the resulting performance gains. This usu-
ally requires monitoring the process over time. In turn, this leads to the broader
question of how we can automate an analysis so it will work with new data
with minimal effort from us. Here we use the word automate in the general
sense of saving our collective effort and time, rather than in any technical sense
(an example of the latter is the COM standard from Microsoft for interoperation
of software components, which JMP also supports).1
This chapter provides a brief discussion of automation. This topic is of impor-
tance not just for Visual Six Sigma but also for the effective and efﬁcient use
of JMP in more general contexts. In Chapter 3 we touched on a related, even
overlapping, topic when we reviewed how to personalize JMP to make it better
ﬁt the skills and requirements of a user or group of users. Generally, appropri-
ate personalization saves time and effort, sometimes to the point of making an
analysis viable when it otherwise would not be.
So this chapter is a discussion of how JMP supports more automated and
personalized usage patterns that go beyond the informed “pointing and click-
ing” that lies at the heart of discovery. This is a big topic, so to keep the discussion
manageable, we anchor it in a speciﬁc example and then conclude with a sum-
mary containing some speciﬁc recommendations for further study. We start
with a few comments about the science and art of programming and of building
applications designed to do speciﬁc things.
The data sets used in this chapter are available at http://support.sas.com/
visualsixsigma and can be accessed using the journal, Visual Six Sigma.jrn.
PROGRAMMING AND APPLICATION BUILDING IN JMP
Given that our intention is simply to convey general ideas, we can afford to
be a little lax in terminology. With this in mind, we deﬁne an application as
something added to the core product (in this case JMP) that does something
that is considered useful.
Generally, an application, A, has three parts:
Part A1: A user interface, to gather inputs from a user each time it is run
Part A2: Some processing logic
Part A3: A report containing the results

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
513
The application usually also contains aspects of automation and of person-
alization. Application building often requires a programming or coding effort
from specialists, and different software environments can make this activity
easy or hard.
Depending on the scope and intended purpose of the application, Part A1
is sometimes not needed because the inputs do not change. Part A3 is always
needed if we interpret the report as the means of conveying results either to
the user or to some other processing system.
In Chapter 3 we introduced the idea of scripts using the JMP Scripting
Language (JSL). We repeatedly used saved scripts to expedite and reproduce
analysis steps in the subsequent chapters.
Although coming to terms with a new programming language can be an
interesting and rewarding intellectual exercise, building useful applications in
JMP often does not require such a commitment. This is because, as you point
and click in JMP, JMP automatically generates JSL that can regenerate the state
of the report with which you are working. This code can be saved for reuse.
For simple applications, this code forms the basis, sometimes the entirety, of
Part A2 above.
As pointed out in Chapter 3, if you save a script to the data table that
generates a report, you can easily reproduce the analysis in question even
if you change the rows in the table (by adding or deleting rows, or changing
the contents of cells) or if you add columns to the table (which the saved
script ignores). However, if you delete a column that is needed by the script,
then the script will fail and it will write an appropriate message to the JMP
log window.
The fact that JMP generates code for you is a great time-saver and also
means that the barrier to understanding and becoming proﬁcient in JSL is con-
siderably reduced should you wish to dig deeper. However, whatever your
attitude toward programming, you might want to become familiar with the
basics of how to write JSL by hand and the facilities that JMP offers in support
of writing JSL. So before moving on to our main example, we consider how to
approach the traditional “Hello World!” example in JSL.
To write a new script, select File > New > Script to open an empty editor
window. Right-click in the editor window and select Show Embedded Log
from the context menu. The window is split and any messages generated by
your code will appear in the lower pane of the window. (Alternatively, you can
open a separate log window using View > Log.)
Type (or copy and paste) the following on a single line in the upper pane of
the editor window:
For(counter = 1, counter <= 5, counter++, Print("Hello World
"||Char(counter)||"!") );

514
V I S U A L S I X S I G M A
Exhibit 11.1 JSL Script and Output
To run the code, select Edit > Run Script or right-click in the script window
and select Run Script from the context menu. The code produces Exhibit 11.1.
(This code is contained in the script Hello World!)
You have successfully built your ﬁrst working application by hand. Here is
a little background on how it works:
◾The numeric variable counter starts with the value 1 and is incremented
in steps of 1 to the value of 5 by the For() loop. Note that the expression
counter++ is an abbreviation for counter = counter + 1.
◾The For() function contains four arguments that are separated by
commas:
◾
The initial argument (counter = 1)
◾
The while argument (counter <= 5)
◾
The next argument (counter++) that controls iteration
◾
The body argument (the Print() command)
The For() function is a typical looping construct found in
programming languages. It repeatedly executes the body argument
(in this case Print()) so long as the while argument (counter <= 5)
remains true.
◾As you might expect, Print() echoes its arguments to the JMP log, one
per line. In this case, there is only one argument, made up of three pieces
of text concatenated together using the symbol || (alternatively written
as Concat()). To obtain the number in text format, we have to con-
vert the numeric value of counter to a character format. This is done
using Char().
Like every language, JSL has its own syntax rules. We do not attempt
to treat these in detail, but refer you instead to Help > Books > Scripting
Guide for all the details if you need them. If your code does not satisfy
these rules, an appropriate error message is sent to the log. This message

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
515
should help you to diagnose the failure. A few general comments may
help, though:
◾
Generally speaking, JSL is case-insensitive and ignores embedded white-
space.
◾
As with every language, brackets, parentheses and braces have to
balance.
◾
JSL expressions are glued together with a semicolon (;). The trailing ; at
the end of the only line in our “Hello World!” example is not required
because there is no following expression.
◾
On your computer screen, the code that you type will be color-coded.
Keywords are shown in blue, literal text in purple, and comments in
green.
◾
The editor provides autocompletion, so if you type the ﬁrst part of
a keyword and press Control-Space on Windows (Option-Esc on
Macintosh), you will be given a list of keywords that match what you
have typed so far.
◾
If you hover the mouse over a keyword, the editor displays a tooltip
showing short help on the syntax and arguments that keyword requires.
◾
Scripts can be saved to ﬁles with a .jsl extension (and restored from such
ﬁles) using File > Save As and File > Open, respectively.
If you are writing more extensive code, you may need some of the options
that are available in the Edit menu or with a right-click in the script window.
The Reformat Script option reformats your code to make it easier to read. By
turning on a Script Editor preference (File > Preferences), you can also use
code folding to hide blocks of code to improve readability.
A MOTIVATING EXAMPLE: DEMOCRACY AND TRADE POLICY
This example shows the steps in building an application to study missing data,
and relates to Chapter 4, since missing data is a key aspect of data quality. As you
will see, the application extends the functionality offered in earlier versions of
JMP, although some of this is now provided in the core product itself. We ﬁrst
walk through the manual steps to produce the required analysis and then intro-
duce an application that accomplishes the same thing. We then brieﬂy review
the new, related functionality in JMP. Finally, we dissect the application to get
an insight into how it works.
Understanding the pattern of missing data can give you important clues
about the data-generating or data-recording processes. These clues can help you

516
V I S U A L S I X S I G M A
avoid the occurrence of missing values in the future. For some problems you
may be forced to guess (or impute) values for missing cells just to make a sub-
sequent analysis more viable or informative. This is done through a numerical
process called imputation. Of course, any imputation technique makes assump-
tions that you need to check. Speciﬁcally, most imputation techniques rely on
the assumption that the values of variables are missing at random, which may
or may not hold.2
The Free Trade Data
For simplicity of illustration and discussion, we will use a small example con-
sisting of only ten variables. But the power of the principles and techniques you
will see are equally relevant and even more valuable in situations where you
have a large number of variables, especially if your focus is on prediction.
This example shows you how to conduct a deeper analysis of missing values.
The data address the effect of democracy on the trade policy of nine developing
countries (or polities) in Asia from 1980 to 1999.3 The table Freetrade.jmp,
shown in Exhibit 11.2, includes ten variables: year (year), country (country),
average tariff rates (tariff), Polity IV score4 (polity), total population (pop), gross
domestic product per capita (gdp.pc), gross international reserves (intresmi), a
dummy variable signifying whether the country had signed an IMF agreement
in that year (signed), a measure of ﬁnancial openness (fiveop), and a measure
of U.S. hegemony (usheg).
Open the data table Freetrade.jmp. Select Tables > Missing Data
Pattern, select all the columns, click Add Columns, and then click OK.
This gives the new table Missing Data Pattern shown in Exhibit 11.3.
Exhibit 11.2 Table Freetrade.jmp (Partial View)

Exhibit 11.3 Table Missing Data Pattern (Partial View)
517

518
V I S U A L S I X S I G M A
(Running the saved script Missing Data Pattern in Freetrade.jmp generates
the same table.)
The Missing Data Pattern data table contains three scripts. Selecting Run
Script from the red triangle menu for either the Treemap or Cell Plot script
gives a visual representation of the missing data.
Although very useful, Missing Data Pattern may not directly contain all
the information you need. For example, suppose that you want to see the num-
ber of rows where a given number of columns are missing. Using the Missing
Data Pattern table, you need to select Tables > Summary, select Count and
then select Statistics > Sum, assign Number of Columns Missing to the Group
role, and click OK. Similarly, if you want to rank the columns in order of miss-
ingness, you need to use the Cols > Column Viewer menu option with the table
Freetrade.jmp active, then make an auxiliary table and sort it appropriately.
To further investigate the pattern of missing data, we will use two comple-
mentary techniques to see how the columns group together: Principal Com-
ponents Analysis (PCA) and Clustering. The ﬁrst technique can be conducted
directly from the Missing Data Pattern table, but the second requires some
additional manipulations.
Principal Components Analysis
PCA exploits correlations among variables to produce a data description that
is more concise, in the sense that it requires fewer dimensions. In so doing, it
can also show which, if any, variables group together to achieve this reduction.4
Your hope is that PCA will help you better understand how your missing values
are structured.
Because you are interested in how missing values occur across your
variables, rows with no missing values contain no useful information. So you
begin by excluding the row in the Missing Data Pattern table that represents
no missing values. Then you conduct a principal components analysis on the
Missing Data Pattern table. (There is a copy of the Missing Data Pattern
table, with scripts that reproduce the work in this section, in the journal ﬁle.
But this table is not linked to Freetrade.jmp.)
Ensure that Missing Data Pattern is your active data table, select the
ﬁrst row, and select Rows > Hide and Exclude. Because the Missing Data
Pattern table that you created and the Freetrade.jmp tables are linked, this
also hides and excludes the corresponding rows in Freetrade.jmp. (Keep in
mind that the Missing Data Pattern table from the journal is not linked to
Freetrade.jmp.)
Now select Analyze > Multivariate Methods > Principal Components.
Note that Count was automatically assigned to the Freq (Frequency) role in
the Missing Data Pattern table. (A column can be assigned the Freq role

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
519
by right-clicking it in the Columns panel and selecting Preselect Role >
Freq.) Enter your original variables, year through usheg, as Y, Columns
(see Exhibit 11.4).
Click OK. Click Continue when a JMP Alert warns you that columns are not
Continuous. Click OK when a JMP Alert warns you that ﬁve columns are being
dropped because they are constant. The report appears as shown in Exhibit 11.5.
Keep in mind that, in this analysis, we are not analyzing the data them-
selves, but the nominal columns that indicate whether a cell is missing. These
are the nominal columns marked by the red icons in the Columns panel of
Missing Data Pattern.
The plot on the right in Exhibit 11.5 is called a loadings plot. It represents how
the indicator variables appear in the reduced two-dimensional space. Note that
Exhibit 11.4 Principal Components Launch Dialog
Exhibit 11.5 Principal Components Report for Missing Data Pattern

520
V I S U A L S I X S I G M A
the plot shows only ﬁve points, since ﬁve other variables are omitted because
they are constant.
The Pareto plot on the left of this ﬁgure indicates the amount of variation
explained by each eigenvalue. The ﬁrst two eigenvalues, which deﬁne the ﬁrst
two dimensions or principal components, account for about 71 percent of the
variability between the indicator variables. The rest of the variability is spread
among the higher components. For other data, though, the two ﬁrst compo-
nents may not capture enough variability, in which case the alignment and
lengths of the arrows in the loadings plot would not be so helpful in seeing
which columns group together.
For these data the loadings plot shows, with some credibility, that intresmi, a
measure of gross international reserves, groups with fiveop, a measure of ﬁnan-
cial openness in terms of missing values. As an exercise, see if you can verify
that fiveop is missing for the years 1998 and 1999 and that intresmi is miss-
ing for all nine countries in 1999, but only missing for four countries in 1998.
(In Freetrade.jmp, select the columns intresmi and fiveop. Use the column
modeling utility Cols > Modeling Utilities > Explore Missing Values to select
the rows where intresmi and fiveop are missing. Then use Tables > Subset.)
Given that 1998 and 1999 were the ﬁnal years for this study, is it possible that
the two measures in question either were not available yet, or were just begin-
ning to be available?
Cluster Analysis
As mentioned previously, clustering of variables5 is a complementary technique
to PCA for grouping variables together. However, it requires some additional
manipulations because the entities to be clustered have to appear as rows in a
data table. Your Missing Data Pattern table contains an indicator column for
each variable that you could transpose. But you also need the information in
the Count column. This information would be lost if you simply transposed the
columns in the Missing Data Pattern table.
So you need to return to the table Freetrade.jmp to make some progress.
Close Missing Data Pattern without saving it.
As you did for the PCA, you will drop all rows that have no missing
values. If you have followed the steps above (with your own Missing Data
Pattern table), then you ﬁnd that 96 rows in Freetrade.jmp are hidden
and excluded, but also selected. If these rows are not excluded, run the script
Hide, Exclude, and Select Complete Rows in the Freetrade.jmp data table
that you can open from the journal. Use Rows > Row Selection > Invert
Row Selection to select all rows that contain one or more missing values.

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
521
Then select Tables > Subset and click OK to construct a new table called
Subset of Freetrade that has 75 rows.
You now have to create the ten indicator columns that were generated
automatically for you in Missing Data Pattern. Although you can do this
by deﬁning formula columns like the one shown in Figure 11.6, this becomes
very tedious when there are many columns. It is more efﬁcient to use a little
JSL instead.
Note that the formula uses the Is Missing() function combined with a
conditional If statement to indicate missing values of fiveop with a 1 and
nonmissing values with a 0. Your script will use this structure.
Select File > New > Script to open a script editor window, and then type
in the lines shown in Exhibit 11.7. (This script is Make Indicator Columns.jsl,
found in the journal.)
Exhibit 11.6 Formula Column for Missing Row Indicator for fiveop
Exhibit 11.7 Rudimentary JSL for Table of Indicator Columns

522
V I S U A L S I X S I G M A
Select Edit > Run Script to produce a new table with the same number
of rows and columns as the original, but containing the appropriate pattern of
zeroes and ones. The code works as follows:
◾Line 1 saves a reference to the currently active data table (Subset of
Freetrade) as dt.
◾Line 2 tells JMP to place all columns in dt into the matrix m.
◾Line 3 replaces entries in m that are not missing by 0.
◾Line 4 replaces entries in m that are missing by 1.
◾Line 5 constructs a new data table from the matrix m.
The Is Missing() function returns the value 1 if a value is missing and
value 0 otherwise. The exclamation point in Line 3 (!Is Missing()) indicates
that a value is not missing. The Loc() function ﬁnds all positions in a matrix
where a logical condition is satisﬁed. You can run the lines one by one (select
each in the editor window, then Edit > Run Script) and view their effect in the
log window.
The new table will be called Untitled X where the value of X is deter-
mined by what else you have done in your current JMP session. Note also
that, although the columns are in one-to-one correspondence with those
in Freetrade.jmp and Subset of Freetrade, the new column names are
generic (Col1, Col2, and so on). You can retain the original column names by
using the slightly more complicated script shown in Exhibit 11.8 and listed
as Make Indicator Columns 2.jsl in the journal. Lines 2 and 7 give the new
table a better name, and line 6 (corresponding to line 5 in Make Indicator
Columns.jsl) is modiﬁed to ensure that the column names are those in the
original table.
Close Untitled X without saving it. Run the new script using Edit > Run
Script to produce the table Subset of Freetrade Missing Cells shown in
Exhibit 11.9. (You can also open this data table by clicking on the link for Sub-
set of Freetrade Missing Cells.jmp in the journal ﬁle.)
Now you need to rearrange the data so that each variable occupies a row.
Select Tables > Transpose. Add all columns to the Transpose Columns
list. Enter For Clustering as the Output table name and Variable as the
Exhibit 11.8 More Useful JSL for Table of Indicator Columns

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
523
Exhibit 11.9 Table Subset of Freetrade Missing Cells (Partial View)
Label column name. (See Exhibit 11.10.) Click OK to produce the table
For Clustering. This table should have 10 rows and 76 columns.
Now your data are in the appropriate form for the cluster analysis. Select
Analyze > Multivariate Methods > Cluster. Add Row 1 through Row 75 as
Y, Columns. To select all 75 rows easily, click Row 1, scroll to Row 75, then
Exhibit 11.10 Completed Transpose Dialog for Subset of Freetrade Missing Cells

524
V I S U A L S I X S I G M A
press the Shift key, and click Row 75. In the Method panel, select Fast Ward.
(See Exhibit 11.11.) Click OK. (We use the Fast Ward method for efﬁciency in
clustering other data with a large number of rows.)
In the report that appears, the rows in For Clustering (columns in Subset
of Freetrade) have been grouped together as shown in the tree-like plot, called
a dendrogram. Select Color Clusters and then Mark Clusters from the red
triangle menu.
Note that the dendrogram is interactive, and that you can drag the
diamond-shaped hotspot to change the number of clusters that you want to
consider. In Exhibit 11.12 we have chosen three clusters. The variables year,
country, pop, gdp.pc, usheg, polity, and signed form one cluster. These are
the variables that are either never or rarely missing. The variables intresmi and
fiveop form a second cluster, and tariff forms a singleton cluster.
Exhibit 11.11 Completed Cluster Dialog for For Clustering

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
525
Exhibit 11.12 Cluster Report for For Clustering
Remember that the three clusters relate to missing values for these vari-
ables, and not to their actual values. In other words, the clusters group variables
that are similar in terms of which values could not be obtained, for some reason.
Recall that the PCA analysis also indicated that intresmi and fiveop had some
commonality in terms of missing values.
This analysis of missing values beyond that provided by Tables > Missing
Data Pattern can provide additional insight. As we mentioned earlier, it is
likely to be more useful when your data contain many more columns than
Freetrade.jmp. If you analyze large observational data sets frequently, you
might like to automate the manual process you just performed. Fortunately, it
is relatively easy to automate all of the preceding steps using JSL, particularly
since, as we mentioned earlier, JMP itself will generate most of the JSL code
required. In a sense, you simply string the code together to form an application.
The Missing Data Add-In
When developing applications, an important question to consider is how they
can be deployed and updated in a simple and foolproof way. As well as making
the application development easy, JMP also eases deployment issues through
an “add-in” architecture. An application can be packaged up into a single
.jmpadin ﬁle that can be given to each user who needs it; the user can install
it with a single click.6
The ﬁle Missing Data.jmpaddin is a JMP add-in that contains the applica-
tion that automates the above steps. Click on Missing Data.jmpaddin in the
journal ﬁle. A JMP Alert asks if you would like to install the add-in. Click Install.

526
V I S U A L S I X S I G M A
This creates a new Add-Ins menu in the JMP toolbar and installs a Missing Data
submenu in that menu. If you already have an Add-Ins menu, the new item
will just be appended.
Making sure that Freetrade.jmp is the active data table, and that no
rows are excluded, select Add-Ins > Missing Data > Missing Analysis to
produce Exhibit 11.13. The report created by the add-in combines the results
in Exhibit 11.5 and 11.12 into a single report, and also includes Pareto charts
for missingness by row and by column. In the report generated by the add-in,
note that menus are active and plots are interactive.
The Missing Data add-in contains other options:
◾Missing Data > Demo Data generates another copy of Freetrade.jmp.
◾Missing Data > Missing Map shows the arrangement of missing values
in the table in the form of a cell plot.
◾Missing Data > Impute Using Amelia provides an advanced way to
impute data values using the R package Amelia,7 and requires R to be
installed locally.8
Exhibit 11.13 Missing Analysis for Freetrade.jmp Using the Add-In

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
527
Exhibit 11.14 Explore Missing Values for Continuous Columns in Freetrade.jmp
As well as showing some further useful analysis of missingness, this analysis
of Freetrade.jmp illustrates a number of points:
◾
A method for addressing a speciﬁc data issue interactively
◾
An insight into the utility of JSL to assist such one-time data tasks
◾
A view of the value of add-ins when you need to routinely supplement
existing JMP functionality and make this available to others
JMP 12 Functionality for Missing Data
However, JMP itself now offers much of this functionality directly, greatly
speeding up and simplifying the workﬂow for handling missing data.
Make Freetrade.jmp your active table and select all the Continuous
columns in the data table. Then select Cols > Modeling Utilities > Explore
Missing Values. This generates the report shown in Exhibit 11.14.
The view in Exhibit 11.14 is for the Missing Value Report option. There
are four other options:
◾
Missing Value Clustering provides clustering similar to that described
above, but directly.
◾
Missing Value Snapshot shows the cell plot given by the Missing Map
option provided by the add-in.
◾
Multivariate Normal Imputation imputes missing values of continu-
ous values under the assumption of multivariate normality, but also
allows you to shrink the estimated covariance matrix, which can provide
more reliable values. Note that this option updates the source data table,

528
V I S U A L S I X S I G M A
highlighting cells containing imputed values in light blue. This option
should provide somewhat similar imputed estimates to the add-in that
uses the Amelia package, but does not require a local R installation.
◾Multivariate SVD Imputation provides an efﬁcient computational
approach for imputation for data tables with a large number (hundreds
or thousands) of variables.
Note that JMP and JMP Pro also provide other ways to handle missing data,
depending on the objectives of the subsequent analysis:
◾Analyze > Multivariate Methods > Multivariate allows you to impute
missing values from a set of continuous variables. Like Multivariate
Normal Imputation, it assumes that the variables follow a multivari-
ate normal distribution. Use Impute Missing Data or Save Imputed
Formula from the Multivariate red triangle. Note that this option does
not provide shrinkage estimates.
◾For predictive modeling, Analyze > Fit Model, Analyze > Modeling >
Partition, and Analyze > Modeling > Neural all provide an informative
missing approach. For a continuous column, this option replaces miss-
ing values with the mean of the nonmissing values and adds an indicator
column for the missing values. Both columns are included in the mod-
eling process. By using the informative missing option, you avoid the
problem of decimating your data through deletion of rows with one or
more missing values. This process usually produces models with better
predictive ability.
Finally, note that JMP also provides utilities to explore outliers. Select the
columns of interest in the data table and then select Cols > Modeling Utilities
> Explore Outliers. Exploring outliers is often of great value in the early phases
of handing real-world data.
BUILDING THE MISSING DATA APPLICATION
In this section we give some insight into how the missing data application was
built. The discussion is at a high level, but the source code is provided if you
want to dig deeper to understand more of the details.
The Structure of the Add-In
As you have seen, the Missing Data.jmpaddin creates an Add-In submenu,
Missing Data, that has the four suboptions shown in Exhibit 11.15. Each option
invokes the code in the single JSL ﬁle whose name is shown in the table.
The script makeDemoData.jsl, found in the journal, only contains the com-
mands required to make a new copy of the table Freetrade.jmp. This script
was generated by using the Copy Table Script option from the red triangle of

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
529
Exhibit 11.15 Missing Data Add-In: Options and Scripts
Menu Item
JSL File Name
Demo Data
makeDemoData.jsl
Missing Map
missingMap.jsl
Missing Analysis
missingDataAnalysis.jsl
Impute Using Amelia multipleImputationWithAmelia.jsl
the table, pasting the results into an editor window, and saving the ﬁle with the
required name.
Using the Missing Map option when Freetrade.jmp is active produces
Exhibit 11.16.
The list shows all the columns in the data table with missing values, but
ranked in descending order in terms of how many values in that column are
missing. Selecting the ﬁrst three columns in the list gives Exhibit 11.17, with
the positions of missing cells shown in red. Moving the sliders allows you to
change the size of the plotted cells.
The JSL Code for the Missing Data Add-In
The script Missing Data.jmpaddin contains all of the elements of an application
(Parts A1, A2, and A3) listed earlier. The section of code that creates the win-
dow shown in Exhibit 11.16 is shown in Exhibit 11.18. You can see that the user
interface is built up from a number of components or objects (display boxes)
such as TextBox(), ButtonBox(), and SliderBox() with names describing
what they do. As mentioned earlier, you can use the editor tooltips to learn
more about these objects. Alternatively, you can use Help > Scripting Index
to search for the object you require. The Scripting Index describes the object
and provides one or more examples of how that object can be used. You can
run these examples and experiment with them.
Exhibit 11.16 Missing Map of Freetrade.jmp (No Columns Selected)

530
V I S U A L S I X S I G M A
Exhibit 11.17 Missing Map of Freetrade.jmp (First Three Columns Selected)
The display boxes in the script in Exhibit 11.18 are laid out according to
the other display boxes that appear at higher levels in the code (PanelBox(),
HListBox(), LineUpBox(), SpacerBox(), and VListBox()).
Each object understands a set of messages that inﬂuence its behavior.
For example, the SetWrap() and FontColor() messages are sent to the ﬁrst
TextBox() using the JSL “≪” operator. Each object can optionally be given a
name so that it can be referenced in other parts of your code (for example, clb
= ColumnListBox()).
The user interface updates when you interact with the ListBox(),
ButtonBox(), or SliderBox() objects. The action that occurs is determined
by the JSL expression associated with the object. For example, the expression
clbChangeScript is associated with clb. The code in clbChangeScript is
shown in Exhibit 11.19.

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
531
Exhibit 11.18 JSL Code to Produce Exhibit 11.16
Exhibit 11.19 Code that Runs When ListBox() clb Updates

532
V I S U A L S I X S I G M A
Note the following:
◾To deﬁne a JSL expression, you use the syntax variable = Expr
(expression) where the required statements are inside Expr(). This
postpones the evaluation of these statements until the action is needed.
In this case, the action is postponed until the selection in clb is changed
by the user.
◾Line 72 puts the names of the selected items in clb into a list called sc
(by sending the message GetSelected).
◾Line 73 calculates ncs, the number of items in the list sc.
◾Line 74 invokes the expression gbUpdateScript, which uses the
information now in the list sc and the variable ncs. This expression
adds or updates a GraphBox(). This is the cell plot that is seen in
Exhibit 11.17.
Application Builder
Rather than write the code for the user interface, Part A1, by hand, an alter-
native approach, preferable for most people, is to use the Application Builder
in JMP. As the name implies, the Application Builder is a design-time tool that
allows you to build user interfaces simply by “drag and drop,” rather than by
coding by hand. You can also add the required processing logic, Part A2, to each
object in your application, producing modularized and maintainable code with
minimal effort.
Select File > New > Application to open the Application Builder window.
This window consists of three parts:
1. On the left is a list of all the objects that you can put into an application,
arranged into major groupings by outline nodes (Reports, Data Table,
Containers, and so on).
2. In the middle is the canvas, initially blank, on which you will design your
application by dragging the required objects from the left.
3. On the right are the Objects and Properties outlines. The Objects
panel gives an alternative, tree-based view of your application, and
the Properties panel allows you to modify the properties of the object
currently selected on the canvas. By unchecking the red triangle option
Show Objects and Properties, you can make the canvas bigger if you
need to.
The ﬁrst display box needed is a PanelBox(), found in the Containers out-
line. Exhibit 11.20 shows the Application Builder after you have dragged this
component onto the canvas and updated its properties. In this case, the only
property is the text that is displayed as a title.
By dragging the required components into the template and modifying their
properties appropriately, you can construct the canvas shown in Exhibit 11.21.

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
533
Exhibit 11.20 Starting to Build Your User Interface with a PanelBox()
You can compare the code in Exhibit 11.18 to Exhibit 11.16 to help ﬁgure out
which components are needed from the palette and what their relationship
should be. (Alternatively, simply open the ﬁle missingDataUI.jmpappsource.)
To view the ﬁnal user interface, select Run Application from the red
triangle menu in Application Builder. This produces the window shown in
Exhibit 11.22. Compare this with Exhibit 11.16.
Application Builder is a capable and complete platform. If you need addi-
tional help to develop applications, we encourage you to look at the relevant
documentation (Help > Books > Scripting Guide). Finally, we note in pass-
ing that the very useful Window > Combine Windows feature mentioned
in Chapter 3 (for combining several report windows into one) actually builds
an application, which you can further modify with Application Builder if you
wish to.
Interoperability with R
Finally, we turn to the last menu item in the Missing Data add-in, Impute
Using Amelia. If you open the associated code in multipleImputationWith
Amelia.jsl and run it with Freetrade.jmp as the active table, you will see that
a dialog appears that allows you to select columns and options. Having made

534
V I S U A L S I X S I G M A
Exhibit 11.21 Missing Data User Interface in Application Builder
Exhibit 11.22 Missing Data User Interface from Application Builder

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
535
some selections, when you click OK you see a message indicating that data is
being submitted to R. If you do not have R installed, then you receive an error
message to that effect. If you do have R installed, along with the Amelia package
and all of its dependencies, then a copy of Freetrade.jmp appears with missing
cells replaced by imputed values. These cells are colored orange to distinguish
them from data that are real.
The code that produces the initial dialog appears in lines 23 to 78 of the
script multipleImputationWithAmelia.jsl. You can verify this if you select all
the lines from the ﬁrst down to 78 in the editor window and then Edit >
Run Script. Although this interface is more complex than the interface for the
Missing Map, the principles are the same, and this code could be generated by
Application Builder or written by hand.
The report produced by this script, Part A3, is also simple (just making a
new table). So the essentially different feature is the processing logic, Part A2,
and the fact that this is not done by JMP, but by another system (in this case R).
Even though JMP is incredibly functional, there may be times when you need
capabilities that it does not provide. Clearly, using this approach will involve
some knowledge of the other system.
To see how JMP can interoperate with R, select Help > Scripting Index,
select Functions from the dropdown list, and type “R” in the search box at the
upper left (Exhibit 11.23). Select Exact Phrase from the tools menu, which
you obtain by clicking the gear icon to the right of the search box. You will see
that there are 13 related functions whose names begin with “R”. Essentially we
need to be able to connect to an R session (R Init()), and disconnect from an
Exhibit 11.23 Functions That Allow JMP to Interoperate with R

536
V I S U A L S I X S I G M A
(a)
(b)
Exhibit 11.24 Loading the R Package Amelia
R session (R Term()), to move data both ways across an active connection, to
run R code, and to retrieve textual and graphical output from R. The R functions
listed handle all these aspects. We will look at some code fragments from
multipleImputationWithAmelia.jsl to get a feel for how they are used.
Exhibit 11.24 shows the deﬁnition of a JSL function (packageInstalled()),
and how it is used in the later code. Similar to the add-in framework in JMP,
R uses the concepts of packages that can be downloaded from repositories and
installed locally. Amelia is one such package. Once we have an active R connec-
tion, we need to check that the required package is available. If it is available,
we load it (line 162), and if it is not, we present a warning message and stop
the execution of the script (line 161).
Lines 10 to 14 show the syntax for deﬁning a function in JSL. The vari-
able pName contains the name of the package we are looking for. Using R
Submit(), line 11 runs the R code required to see which packages are available,
and puts the results into plist. Line 12 then uses R Get() to move the contents
of plist into a JSL variable, also a list, called pckgLst. Line 13 then evaluates to
a Boolean value that is returned when packageInstalled() is evaluated (the
value 1 if pckgLst contains pName and the value 0 if not). Line 162 loads the
Amelia package once we are sure it is available.
Exhibit 11.25 shows how the imputation is performed. Line 163 sends the
JMP data table referenced as dt2 to R: R Send() translates a JMP table to an R
Exhibit 11.25 Using the R Package Amelia for Imputation

B E Y O N D “ P O I N T A N D C L I C K” W I T H J M P
537
data frame with the same name. Line 165 builds the R code for using Amelia,
and returns the results to the R structure imputation.results. Note that the
Amelia package is very functional and provides a lot of control over how the
imputation is carried out. Only the simplest options are surfaced in the initial
JMP dialog, and this is reﬂected in the code in line 165. Line 167 then uses
RGet() to retrieve the results into a JSL variable, dt2List. This is actually a
list of data tables (each containing imputed values). The variable n comes from
the initial dialog and is the number of imputations we asked for. If the dia-
log option Keep Each Imputation is left at the default value of No, dt2List
will contain a single table with imputed values equal to the average of the n
that were performed. Otherwise, it contains n tables with no averaging of the
imputed values.
To understand all the details of how the code works, see the comments in
the code itself.
The mechanics of packaging code and other resources into an add-in are
straightforward (see Help > Books > Scripting Guide). If you are working
entirely with the Application Builder, you can simply select Save > Save Script
to Add-in from the red triangle menu.
Note that JMP provides the same style of interoperation with MATLAB as
it does with R. So, if you have an investment in specialized code in this system,
you can make its capabilities available to users who might be intimidated by, or
unwilling to learn, that software. If you search for MATLAB in the JSL Scripting
Index, you will see JSL functions analogous to those used in the Missing Data
application. You can compare the results of your search with Exhibit 11.23.
You should also be aware that JMP can act as a client to SAS, and can
interoperate with SAS in many different ways. The built-in client functionality
allows point-and-click access to many SAS resources such as stored processes,
and as a SAS product it has a depth of integration that is deeper than with
other software. If you search for SAS in the JSL Scripting Index, you will ﬁnd
40 associated JSL functions, which indicates a world of many possibilities.
CONCLUSION
The intention of this chapter was to show some of the possibilities for building
useful applications using JMP. In the Visual Six Sigma context, such applica-
tions are usually associated with the Utilize Knowledge step when you need to
assure that the performance gains are institutionalized. Left alone, any system
degrades over time. Appropriate ongoing monitoring is an essential part of this
endeavor.
Of course the usefulness of applications extends far beyond the Visual Six
Sigma context. If you have made an investment in JMP, it’s good to be aware
of what is possible. As should now be clear, the JMP Scripting Language,

538
V I S U A L S I X S I G M A
JSL, is what opens up these possibilities. JSL is a ﬂexible and powerful
language—ﬂexible because (if you need to program with it) it supports a variety
of programming styles and paradigms, and powerful because it is intimately
connected with platforms and objects that comprise JMP itself. In fact, one of
the unwritten principles of the development of JMP is that, whenever new
functionality is added, that functionality is always accessible via JSL.
JMP makes building applications as easy as possible, to the point that if you
have read and understood this chapter, you can already accomplish a lot. Using
the fact that JMP generates JSL automatically, that report windows are easy
to combine, that Application Builder makes it easy to design user interfaces or
build and manage entire applications, the need for extensive coding efforts is
much reduced. Coupled with the fact that JMP can also interoperate with R,
MATLAB, and SAS (should these be needed for specialist algorithms), and that
JMP’s add-in framework makes it easy to deploy and manage applications, it
becomes easy for even a single practitioner to have a very positive impact on
the way his or her organization exploits data.
We need to ﬁnish with a reminder that, though it can be deployed in virtual-
ized environments, JMP is a desktop tool. As mentioned in Chapter 4, it is not
designed for large-scale, batch-oriented processing. Even when the intended
usage pattern of your application respects this caveat, it will work best when
JMP is used as an analytic hub to orchestrate actions and present results. Appli-
cations that require JMP to be the “servant” rather than the “master” tend to
work less well. If correctly conceived and implemented, your application can
maintain the interactivity and agility that is the hallmark of JMP itself.
NOTES
1. Microsoft, “COM: Component Object Model Technologies,” www.microsoft.com/com/default
.mspx (accessed 28 June 2015).
2. Wikipedia, “Missing Data,” https://en.wikipedia.org/wiki/Missing_data (accessed 28 June 2015).
3. Helen Milner and Keiko Kubota, “Why the Move to Free Trade? Democracy and Trade Policy in
the Developing Countries,” International Organization 59, no. 1 (2005).
4. JMP,
“Principal
Components,”
www.jmp.com/support/help/Principal_Components.shtml#
158049 (accessed 28 June 2015).
5. JMP, “Cluster Analysis,” www.jmp.com/support/help/Cluster_Analysis.shtml#65478 (accessed
28 June 2015).
6. This add-in was inspired by Gerhard Svolba, Data Quality for Analytics Using SAS® (SAS Institute:
2012).
7. James Honaker, Gary King, and Matthew Blackwell, “Amelia II: A Program for Missing Data,”
http://gking.harvard.edu/amelia (accessed 28 June 2015).
8. “The R Project for Statistical Computing,” www.r-project.org (accessed 28 June 2015).

Index
A
absolute values, ranking, 492
Actual By Predicted plot, 265, 406,
409
Add-In
missing data, 525–527
structure of, 528–529
Add-Ins menu, 525–526
Advanced Controls outline, 483
Agreement Comparisons panel, 237,
238
Agreement within Raters panel, 238
All Pairs option, 183, 324
All Possible Models option,
410–411
Alt key, 37, 231
amounts, analyzing, 122–125
analysis
of amounts, 122–125
baseline, 171–174
cluster, 520–525
conducting, 368
in JMP, 47–54
missing data, 314–318, 443
results using Process Capability
platform, 287
analysis data sets
checking, 460–461
constructing, 161–164
Analysis of Variance table, 203, 332
analysis sets
constructing, 454
distribution of, 455
Analyze menu, 33–34, 48–49, 236,
363, 378, 518–519, 528
anatomy, of JMP, 28–40
Application Builder, 58
Augment Design platform, 54
automatic splitting, 467
Average Chart, 369–370
axis settings, copying and pasting,
233, 234
B
baseline analysis, 171–174
baseline data, 166–174, 239–241
Bayesian Information Criterion
(BIC), 405
bias, 373
Bias Factors Std Dev, in Proﬁler, 383
BIC (Bayesian Information
Criterion), 405
bivariate plots, adding horizontal
reference lines to, 140, 141
black belt, 13
Boosted Tree model, 471–478
Boosted Tree report, 475
Boosting panel, 497
Box, George, 8
boxplots, displaying in Graph Builder,
251
broadcasting commands to all
reports, 180
Bubble Plot (Graph menu), 47,
335–341
Bubble Size slider, 319
ButtonBox(), 528, 531
buyer’s viewpoint, 164
C
capability
obtaining analysis of, 426
projected, 283–292
simulated, 286–289
Capability Box Plots, 288, 289
case studies
Classiﬁcation of Cells, 437–508
Improving a Polymer
Manufacturing Process, 345–435
Improving the Quality of Anodized
Parts, 223–296
Informing Pharmaceutical Sales
and Marketing, 297–343
Reducing Hospital Late Charge
Incidents, 103–156
539
Visual Six Sigma: Making Data Analysis Lean, Second Edition. Ian Cox, Marie A. Gaudard, Mia L. Stephens
© 2016 by SAS Institute, Inc. Published by John Wiley & Sons, Inc.

540
I N D E X
case studies (Continuted)
Transforming Pricing Management
in a Chemical Supplier, 157–221
Categorical Proﬁler, 499
cause-and-effect diagram,
constructing in JMP, 206
causes
combining, 142, 144
types of, 9
CDA (Conﬁrmatory Data Analysis),
16–18, 19
Cell Plot script, 518
champion, 13
Char() command, 514
chunking variables, 309
CIELAB (Commission Internationale
de l’Eclairage), 227–228
classiﬁcation models, comparing,
503–507
Classiﬁcation of Cells case study
about, 438–439, 507–508
background, 440–441
Collect Data step, 441–442
comparing classiﬁcation models,
503–507
constructing training, validation,
and test sets, 452–461
data exploration, 442–452
Frame Problem step, 441–442
Generalized Regression platform,
482–494
neural net models, 494–503
prediction models, 461–463
recursive partitioning, 463–478
Stepwise Logistic Model, 478–482
clbChangeScript, 531
Clean task, in data management, 70,
71
Clear button, 389
Clear Row States command, 50
closing reports, 317–318, 414, 429
cluster analysis, 520–525
Collect Data step, in VSS Data
Analysis Process
about, 20, 70–71, 295
in Classiﬁcation of Cells case study,
441–442
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
and, 256
examples of, 71–99
in Improving the Quality of
Anodize Parts case study,
228–242
in Informing Pharmaceutical Sales
and Marketing case study,
300–302
in Reducing Hospital Late Charge
Incidents case study, 106–108
in Transforming Pricing
Management in a Chemical
Supplier case study, 166–174
coloring points, 447–448
colors, changing in reports, 188–190
Colors and Markers script, 464
Column Contribution report, 476,
477
Column Info command, 51
Column Info window, 52, 239
Column Property, 189
Column Switcher, 248–249,
389–392, 457–458, 460–461
Column Viewer, 305–308, 518
columns
adjusting structure of, 430–431
excluding, 303–304
grouping, 304, 305
hiding, 303–304
inserting descriptions in, 302, 303
properties of, 51
saving speciﬁcation limits as,
401–403
Columns menu
about, 71
displays in, 51–53
illustrated, 52
Columns panel (JMP), 31, 33, 134,
168, 239, 262, 351, 402, 412,
448, 454, 471, 477, 518–519
Columns Viewer report, 109–113,
443
Combine task, in data management,
70, 71
Combine Windows feature, 532

I N D E X
541
commands, 180
See also speciﬁc commands
Commission Internationale de
l’Eclairage (CIELAB), 227–228
Compare Means option, 183
Comparison Circles, obtaining,
183–187
Concat() command, 514
Concatenate (Tables menu), 50
concatenating data tables, 431
concavity, 441
conﬁrmation runs, 23, 282–283
Conﬁrmatory Data Analysis (CDA),
15–19
conﬁrmatory study, exploratory
study versus, 15
Confusion Matrices, constructing,
489
Confusion Matrix report, 469–470,
475, 481–482, 505
Containers outline, 532
context sensitive commands, 37
Contingency Table, 172
contour plot, creating, 278, 279
Contour Proﬁler (Graph menu), 47,
277–280
Control Chart Builder, 220, 292, 353,
389–392, 424–426
Control Charts, 219, 361–363,
389–392, 426–428
Control key, 84–85, 177, 180
Control Limit property, 52
control limits, 219–220
Control Matrix, 206–207
control panel (Boosted Tree model),
473–475
correlations
pairwise, 449–451
Scatterplot Matrix and, 445
crisis yields, ﬁltering, 353–354
critical to quality (CTQ), 14, 226
Critical to Quality Characteristics
(CTQs), 59
Critical to Quality Tree, 356–357
CTQ (critical to quality), 14, 226
CTQs (Critical to Quality
Characteristics), 59
Cumulative Validation plot, 475–476
Current Estimates panel, 479
Custom Design platform, 54,
259, 273
customers, in data sets, 162
D
data
See also Collect Data step, in VSS
Data Analysis Process; data
quality
baseline, 239–241
collection examples, 71–99
exploring, 442–452
free trade, 516–518
importing, 72–74
missing, 74–77, 113–122, 314–318,
443, 525–537
observational versus experimental,
11–12
scoping, 302–318
splitting, 190–194
validating, 302–318
verifying integrity of, 168–171
data analysis
See also Visual Six Sigma (VSS) Data
Analysis Process
making lean, 5–6
obtaining, 169, 170
Data Filter dialog, 50–51, 389
data grid, 31
data management, 70, 71
See also data
data mining, 18, 438, 440
data quality, 68–70
See also data
data table panels, 31, 273
Data Table toolbar, 33
data tables
adding descriptions for variables in,
108
concatenating, 431
creating views of, 146, 147
dynamic linking to, 40–46
in JMP, 31–33
joining, 94
preparing, 303–305

542
I N D E X
data tables (Continued)
saving, 304–305
using two, 91
Data Type, 52
Data View table, 116, 122, 127, 144,
150
dates, in JMP, 110–112
Decision Tree model, 464–471
Defect table, 422
Deﬁne, Measure, Analyze, Design,
and Validate (DMADV), 4
Deﬁne, Measure, Analyze, Improve,
and Control approach
See DMAIC (Deﬁne, Measure,
Analyze, Improve, and Control)
approach
Deﬁne Abs [Amount] script, 133–134
democracy, trade policy and, 515–528
dendogram, 524
Derive task, in data management, 70,
71
Describe task, in data management,
70, 71
descriptions, adding for variables in
data tables, 108
descriptive variables, using Local Data
Filter for, 309–310
Design for Six Sigma (DFSS), 59
design of experiments (DOE),
11–12
Design panel, 261
designs, developing, 257–264
Desirability Functions, 273, 415
desirability traces, 280
detective, statistics as, 15–19, 24, 224
DFSS (Design for Six Sigma), 59
Diagram plot, 499
disclosure icons, 39
distribution
about, 35–36, 256, 295
of analysis sets, 455
dynamic linking and, 445
dynamic visualization of variables
using, 175–177
exploring, 444–445
using, 305–308
of variables, 461
Distribution (Analyze menu), 48
Distribution Analysis, obtaining,
210–211, 360
Distribution platform (JMP), 35, 77,
175, 242–244, 389
Distribution plot, 152, 153, 154, 316,
318–319, 359–361, 385–389
Distribution report, 55, 78, 110, 154,
173, 211, 212, 213, 242, 243,
306, 307–308, 444
Distribution script, 286
DMADV (Deﬁne, Measure, Analyze,
Design, and Validate), 4, 14
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
application of, 256–257
deﬁned, 4
using to deliver bottom-line results
in short or medium term, 14
DOE (design of experiments), 11–12
DOE Dialog script, 262, 367
DOE menu, 53–54, 228
duplicate rows, keys and, 98–99
dynamic linking
to data tables, 40–46
distribution and, 445
dynamic visualization
See also visualizing
of multiple variables at once,
187–200
of prescriptions with tabular
displays, 320–321
of sales reps and practices
geographically, 318–319
using, 16–18
of variables one and two at a time,
305–314
of variables two at a time, 177–187
of variables using distribution,
175–177
E
Each Pair option, 324
Early Stopping, 475
EDA (Exploratory Data Analysis),
15–19, 24, 224
Edit toolbar, 33

I N D E X
543
Effect Summary report, 203, 265,
269, 406–408, 409
Effect Tests table, 268
effectiveness, of measurement
system, 238
Effectiveness Report, 238
Elastic Net model, ﬁtting, 490–494
Elastic Net Prediction Equation,
saving, 493–494
EMP Gauge R&R Results panel, 372,
376–378
EMP Gauge R&R Results report, 373,
380–381, 382, 384
entry order, viewing by, 127–132
error, 9, 10
Estimation Details outline, 484
ETL (extract, transform and load)
processes, 68
Evaluate Design platform, 54
examples, of Collect Data step in VSS
Data Analysis Process, 71–99
excluding
columns, 303–304
outliers, 459
points on histograms, 123
rows, 33, 401
executive committee, 13
experimental data, observational data
versus, 11–12
experimental design, 53
experiments, conducting, 264
Exploratory Data Analysis (EDA),
15–19, 24, 224
exploratory study, conﬁrmatory
study versus, 15
extract, transform and load (ETL)
processes, 68
F
factors, specifying, 259, 260
Factors panel, 259
File toolbar, 33
ﬁltering
crisis yields, 353–354
data values with Local Data Filter,
353–354
by month, 310
ﬁndings, summarizing, 112–113
Fit All Prediction Formulas
script, 507
Fit Model (Analyze menu), 48, 188,
200, 264, 326–329, 378, 478
Fit Model report, 48, 273, 328
Fit Special, 138, 140
Fit Y by X (Analyze menu), 48, 145,
151, 152, 177, 180, 187,
323–326, 329–342
ﬁtting
Boosted Tree model, 473
Decision Tree model, 464
Elastic Net model, 490–494
Lasso model, 484–490
lines, 138, 140
logistic model, 479–481
models, 267
Neural Net models, 498,
501–502
Fitting Options panel, 497
Fixed Effects Tests report, 328
FontColor(), 529–530
For() loop, 514
Formula column property, 52
Formula Editor, 119–121, 126, 168,
239
formulas
creating, 125–126
viewing in Formula Editor, 239
fractal dimension, 441
Frame Problem step, in VSS Data
Analysis Process
about, 20, 22, 295
in Classiﬁcation of Cells case study,
441–442
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
and, 256
in Improving a Polymer
Manufacturing Process case
study, 350–357
in Improving the Quality of
Anodize Parts case study,
226–228
in Reducing Hospital Late Charge
Incidents case study, 104–106

544
I N D E X
in Transforming Pricing
Management in a Chemical
Supplier case study, 160–166
free trade data, 516–518
frequency, of measurements, 77–82
Full Factorial Design platform, 54,
228, 366
G
Gauge Repeatability and
Reproducibility (Gauge R&R)
study, 22–23, 228, 234
Gauge R&R Std Dev, 381
gbUpdateScript, 531
Generalized Regression modeling,
462, 482–484
Generalized Regression report, 483
Go button, 467, 479
Goal Plot, 287, 288, 289, 290
Goos, Peter
Optimal Design of Experiments: A Case
Study Approach, 11
Graph Builder (Graph menu)
about, 47, 256
checking analysis data sets,
460–461
constructing a plot for two variables
with, 399–400
using, 244–251, 313, 314
viewing outliers with, 457–458
visualizing two variables at a time
with, 392–394
Graph menu (JMP)
about, 267, 273
displays in, 47
getting reports from, 33–34
illustrated, 34
GraphBox(), 531
graphs
creating, 244, 248
displaying elements of in Graph
Builder, 251
green belt, 13
grouping
columns, 304, 305
predictors, 442–443
guidelines, Visual Six Sigma and,
23–24
H
Hidden Layer Structure panel, 497
hiding
columns, 303–304
outliers, 459
panes in JMP, 29
points on histograms, 123
rows, 33, 401
hierarchical processes, 22
histograms, selecting outliers on, 123
historical data, reviewing, 358–363
HListBox(), 530
horizontal reference lines, adding to
bivariate plots, 140, 141
Hot Xs, 21–22, 23, 59, 141–155, 187,
264–270
I
Identify, Design, Optimize, and
Validate (IDOV), 4, 14
importing data, 72–74
improvements
conﬁrming, 422–423
planning, 207–209
tracking, 428–434
verifying, 209–218
Improving a Polymer Manufacturing
Process case study
about, 346–348, 434–435
background, 348
forming teams, 350–351
Frame Problem step, 351–357
manufacturing process, 348
Measurement System Analysis
(MSA), 363–385
Model Relationships step,
400–412
reviewing historical data,
358–363
Revise Knowledge step, 412–423
typical crisis, 349
Uncover Relationships step,
385–400
Utilize Knowledge step, 423–434
Improving the Quality of Anodized
Parts case study
about, 224, 295–296
background, 224–226

I N D E X
545
Collect Data step, 228–242
Frame Problem step, 226–228
Model Relationships step, 257–270
Revise Knowledge step, 270–292
Roadmap, 256–257
Uncover Relationships step,
242–256
Utilize Knowledge step, 292–294
In-Control Part Std Dev, 382, 383
Individual Measurement chart, 130,
131, 240–241, 292–293, 294,
352, 361
Informing Pharmaceutical Sales and
Marketing case study
about, 298–300, 342–343
background, 300
Collect Data step, 300–302
promotional activity, 321–333
regional differences for, 333–342
scoping data, 302–318
Uncover Relationships step,
318–321
validating data, 302–318
infrastructure, of typical Six Sigma
deployment, 13
Input/Output process map, 358
integrity, of data, 168–171
interactions, 373
interoperability, with R, 533–537
Intraclass Correlation, 373, 378, 381,
385
IR charts, obtaining, 432
Is Missing() function, 521
J
JMPⓇ
about, 28, 66, 69–70, 512
Analyze menu, 33–34, 48–49, 236,
363, 378, 518–519, 528
anatomy of, 28–40
application building in, 512–515
Columns menu, 51–53, 71
data tables, 31–33
DOE menu, 53–54, 228
dynamic linking to data tables,
40–46
featured analyses, 47–54
featured visual displays, 47–54
Graph menu, 33–34, 47, 267, 273
opening, 29–30
personalizing, 58
programming in, 512–515
reports, 33–40
Rows menu, 50–51, 71
scripts, 55–58
Tables menu, 49–50, 71
techniques, 66
visual displays, 33–40
Visual Six Sigma Roadmap and,
58–66
VSS Data Analysis Process and,
58–66
window management, 46–47
JMP Home Window, 29–30, 39, 46,
110–112, 495–496
JMP Pro version, 12 29
JMP Scripting Language (JSL), 55,
514
JMP Starter Window, 29–30, 33
Join (Tables menu), 50
Jones, Bradley
Optimal Design of Experiments: A Case
Study Approach, 11
JSL (JMP Scripting Language), 55,
514
K
kappa value, 237
Key Performance Indicator
(KPI), 226
keys, duplicate rows and, 98–99
KPI (Key Performance Indicator), 226
L
labelled rows, 33
Lack Of Fit Test, 265
Lasso model, ﬁtting, 484–490
Lasso Prediction Equation, saving,
489–490
Lasso tool, 130
Lasso with Validation Column
Validation, 487
launch platforms, 33, 35
lawyer, statistics as, 15–19
Leaf Report, 196, 199
Learning Rate, 474

546
I N D E X
Least Squares Mean Table, 204
Legend window, 449
lines, ﬁtting, 138, 140
LineUpBox(), 530
linking with Contour Proﬁler,
277–280
ListBox(), 531
Local Data Filter
about, 50, 55, 215, 254–255
ﬁltering data values with, 353–354
ﬁnding relationships using,
313–314
using for descriptive variables,
309–310
using for response variables,
310–313
Lock Columns, 191
Lock Scales option, 89
logistic model, ﬁtting, 479–481
long term, 288
M
Magniﬁer tool, 319
Make Validation Column script, 454
management, 13
manufacturing process, in Improving
a Polymer Manufacturing Process
case study, 348
marking points, 447–448
Mast, Jeroen de, 16, 19
master black belt, 13
Maximize Desirability, 416, 419–420
mean fractal dimension, 451–452
Means Comparison report, 187, 325
Measurement System Analysis
(MSA) study
about, 22–23, 224, 228, 236–239,
346
following-up with, 380–385
in Improving a Polymer
Manufacturing Process case
study, 363–385
for MFI, 363–374
for Xf, 374–378
measurement systems, ﬁxing,
378–380
measurements, 10–11, 77–82
Measures of Fit for Diagnosis report,
504–505
Minimum Size Split, 190, 475
missing data
add-in for, 525–527
analyzing, 314–318, 443
application building, 528–537
identifying, 74–77
JMP 12 functionality for, 527–528
understanding, 113–122
Missing Data Pattern (Tables menu),
50, 113–114, 314–318, 443,
516–525
Missing Data Pattern report, 114, 115
Missing Value Clustering, 527
Missing Value report, 527
Missing Value Snapshot, 527
Model Comparison report, 504–505
Model Launch outline, panels in, 497
Model Launch panel, 483, 501–502
Model NTanH(3) report, 498–501
Model NTanH(3)NBoost(10) report,
502
Model Relationships step, in VSS
Data Analysis Process
about, 20, 21, 22–23, 60, 257, 295
conducting the experiment, 264
design development, 257–264
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
and, 256
Improving a Polymer
Manufacturing Process case
study, 400–412
Improving the Quality of Anodize
Parts case study, 257–270
Transforming Pricing Management
in a Chemical Supplier case
study, 200–205
uncovering Hot Xs, 264–270
Model script, 261, 264, 367
Model Summary report, 484, 490
Model with Random Effect script, 327
modeling, planning for, 404
Modeling menu, 48
Modeling Type, 52
Modeling>Model Comparison
(Analyze menu), 49
Modeling>Neural Net (Analyze
menu), 48

I N D E X
547
Modeling>Partition (Analyze menu),
48
models
about, 8–10
building, 201–202, 404–412
ﬁtting, 267
prediction, 462–463
month, ﬁltering by, 310
mosaic plots, creating, 151, 152
MSA (Measurement System
Analysis) study
about, 22–23, 224, 228, 236–239,
346
following-up with, 380–385
for MFI, 363–374
for Xf, 374–378
Multiple Fits over Splits and learning
rate, 475
Multi-Vari chart, creating, 229–230
Multivariate k-Nearest Neighbor
Outliers option, 456
Multivariate Methods>Cluster
(Analyze menu), 49
Multivariate Methods>Multivariate
(Analyze menu), 49
Multivariate Methods>Principal
Components (Analyze menu), 49
Multivariate Normal Imputation, 527
multivariate outliers, identifying, 456
Multivariate Robust Outliers option,
456
Multivariate SVD Imputation, 528
N
Neural modeling, 462
neural net model launch dialog,
obtaining, 496–497
Neural Net models
about, 494
background, 494–495
Neural Net, 1 496–501
Neural Net, 2 501–503
Neural platform in JMP, 495–496
noise, 10
noise function, 9
Non Critical quadrant, in Product
Categorization Matrix, 165
nonparametric ﬁt, 426
notes, viewing for variables, 108
Notes Column property, 303
Number of Subgroups, in Proﬁler, 382
numbers, recoding, 94–97
O
Objects outline, 532
Objects panel, 532
observational data, experimental data
versus, 11–12
one way plot, creating, 145
opening JMP, 29–30
Optimal Design of Experiments: A Case
Study Approach (Goos and Jones),
11
optimal factor level settings,
determining, 271–277
optimal strategies, identifying,
205–207
optimization
simultaneous, 271, 414–417
using Proﬁler for multiple, 412–413
options
See speciﬁc options
for Add-Ins menu, 525
in Classiﬁcation of Cells case study,
439
for Decision Tree model, 470–471
in Improving a Polymer
Manufacturing Process case
study, 348
in Informing Pharmaceutical Sales
and Marketing case study, 299
Outlier Boxplot Option, 457
outliers
checking for, 456–460
excluding, 459
hiding, 459
selecting on histogram, 123
overall process capability, 288
Overall Sigma Summary Report, 290,
291
Overall Statistics report, 475
Overﬁt Penalty, 475
P
pairwise correlations, 449–451
PanelBox(), 530, 532
panes, manipulating in JMP, 29

548
I N D E X
Parallelism Plots, 370
Parameter Estimates for Centered and
Scaled Predictors report, 487, 492
Parameter Estimates for Original
Predictors outline, 484, 487
Parameter Estimates report, 487, 492
Pareto Plot, 116, 118, 142, 143
Part Mean Shift, in Proﬁler, 383
Part Std Dev, in Proﬁler, 383
partition analysis, obtaining, 188
Partition platform, 463–464, 467
Partition report, 187–188, 464–465
partitioning, recursive, 463–478
Partitioning modeling, 461
PCA (principal components analysis),
518–520
Percent format, 239
percentages, calculating using
Formula Editor, 120–121
performance gap, 12
platforms
See also speciﬁc platforms
in Classiﬁcation of Cells case study,
439
in Improving a Polymer
Manufacturing Process case
study, 348
in Informing Pharmaceutical Sales
and Marketing case study, 299
Platforms option, 39
Potential Outliers, 459
"The Power to KnowTM", 68
Ppk, 288–289
prediction formulas
entering as column properties,
284–285
saving, 411–412
saving to data table, 273, 411–412
prediction models, 462–463
Prediction Proﬁler, 273, 275,
276–277, 279–280, 281,
283–284, 285–286, 295,
412–413, 418
predictive analytics, 18, 438, 440
predictors
assessing sensitivity to settings,
417–421
contribution of, 481, 487–488,
492–494
grouping, 442–443
Preferences menu, 58
pricing deﬁcit, 168, 174
pricing management process, 160
primary key, ﬁnding, 91–93
principal components analysis (PCA),
518–520
Print() command, 514
Process Capability platform, 287, 291,
426–428
process maps, 160–161
process owner, 13
processes
deﬁning, 160–161
verifying stability, 424–426
Product Categorization Matrix,
164–166
Product Variation variance
component, 377–378
products, in data sets, 162
Proﬁler (Graph menu), 47, 267,
275–276, 280, 412–414
Proﬁt Matrix, 462–463, 506
project charters, developing,
351–354
projected capability, 283–292
projects, identifying, 155–156
promotional activity, 321–333
properties, of good measurement
systems, 364
Properties outline, 532
Prune button, 191, 193
Q
Quality and Process>Control Chart
Builder (Analyze menu), 49
Quality and Process>Diagram
(Analyze menu), 49
Quality and Process>Measurement
Systems Analysis (Analyze
menu), 49
Quality and Process>Pareto Plot
(Analyze menu), 49
Quality and Process>Process
Capability (Analyze menu), 49

I N D E X
549
Quality and
Process>Variability/Attribute
Gauge Chart (Analyze menu), 49
Quantile Range Outliers option, 456
Quantiles panel, 386
Quantiles report, 113, 125, 173
R
R chart, constructing, 425, 433–434
Range Chart, 369–370
recoding numbers, 94–97
recursive partitioning
about, 463–464
Boosted Tree model, 471–478
Decision Tree model, 464–471
red triangle icons, 37
Red X
See Hot Xs
Reducing Hospital Late Charge
Incidents case study
about, 104
Collect Data step, 106–108
Frame Problem step, 104–106
identifying projects, 155–156
Uncover Relationships step,
109–141
uncovering Hot Xs, 141–155
Reformat Script option, 515
regional differences, 333–342
Regression Plot, 409
relationships
exploring for two variables at a
time, 445–452
ﬁnding using Local Data Filter,
313–314
REML Variance Component
Estimates report, 328
Remove button, 269, 270
Repeatability variance component,
377–378
reports
See also speciﬁc reports
broadcasting commands to all,
180
changing colors in, 188–190
closing, 317–318, 414, 429
deﬁned, 35
in JMP, 33–40
sorting, 196, 198
Reproducibility variance component,
377–378
Reshape task, in data management,
70, 71
response distribution analysis, 283
Response Goal window, 415
response variables
adding, 249
using Local Data Filter for,
310–313
responses, specifying, 259, 260
results
analyzing using Process Capability
platform, 287
interpreting, 369–374
Revise Knowledge step, in VSS Data
Analysis Process
about, 20, 21, 23, 60, 270–271, 295
addressing conclusions with, 282
conﬁrmation runs, 282–283
determining optimal factor level
settings, 271–277
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
and, 257
Improving a Polymer
Manufacturing Process case
study, 412–423
Improving the Quality of Anodize
Parts case study, 270–292
linking with Contour Proﬁler,
277–280
projected capability, 283–292
sensitivity, 280–282
Transforming Pricing Management
in a Chemical Supplier case
study, 205–218
RMSE (root mean squared error), 283
Roadmap
Improving the Quality of Anodize
Parts case study, 256–257
JMP and, 58–66
Visual Six Sigma, 21–23
Robust Fit Outliers option, 456
root mean squared error (RMSE), 283

550
I N D E X
row states, 33, 50
rows, excluding and hiding, 401
Rows menu
about, 71
displays in, 50–51
illustrated, 51
Rows panel (JMP), 31, 33, 40,
397
Run Script, 55
running scripts in JMP, 31
S
Sample Size and Power platform, 54
Save Formulas option, 501
saving
data tables, 304–305
Elastic Net Prediction Equation,
493–494
Lasso Prediction Equation, 489–490
Logistic Prediction Equation, 482
Neural Net 2 Prediction Equation,
502–503
prediction formulas, 273, 411–412
prediction formulas to data table,
273
Scale panel, 82
Scaled-LogLikelihood plot, 489
scatterplot, creating, 137, 138
Scatterplot 3D (Graph menu), 47,
252–255, 256, 295
Scatterplot Matrix (Graph menu), 47,
48, 251–252, 295, 394–400, 397,
445
Screening Design platform, 258
Scripting Index, 529–530, 535
scripts
See also speciﬁc scripts
JMP, 55–58
running in JMP, 31
Second function, 77
Select Columns window, 487–488
selected rows, 33
seller’s viewpoint, 164
senior executive, 13
sensitivity, 280–282, 417–421
Sensitivity Indicator, 280, 281, 418
serial processes, 22
SetWrap(), 529–530
Shift Detection Proﬁler, 382–384
Sigma column properties, 284
signal function, 9
Simulate button, 285, 422
simulated capability, 286–289
simulating process outcomes,
421–422
Simulator, 283–286, 295, 421–422
simultaneous optimization, 271,
414–417
SIPOC (Suppliers, Inputs, Process,
Outputs, and Customers) map,
351
Six Sigma
See also Visual Six Sigma
about, 8, 13, 24–25
background of, 8
beyond traditional, 4–5
common perceptions and
deﬁnitions of, 12
DMADV approach and, 14–15
DMAIC structure and, 14
dynamic visualization, 16–18
infrastructure, 13
measurements, 10–11
models, 8–10
observational versus experimental
data, 11–12
questions related to deployment of,
12
statistics, 15–16
variation, 15–16
SliderBox(), 528, 531
Small Tree View option, 191–194,
471
Solution Path report, 486, 489, 490
Sort (Tables menu), 49
Sorted Parameter Estimates table,
268, 269
sorting reports, 196, 198
SpacerBox(), 530
Spec Limits column property, 52,
286, 401–403
speciﬁcation limits
saving as column properties,
401–403
setting, 264
speciﬁcations, proposing, 255–256

I N D E X
551
Speciﬁcations report, 475
Split button, 191, 319, 465
Split History report, 467–469
splitting
about, 465–467
automatic, 467
data, 190–194
history for, 467–469
Stack command, 35, 37, 40, 122, 127
standard deviation plot, removing,
231
standard deviations, entering as
column properties, 284–285
Statistical Discovery, 28
statistical modeling, 10, 16
statistical signiﬁcance, 10, 187
statistics
deﬁned, 15
as detective, 15–19, 24, 224
as lawyer, 15–16, 19
Six Sigma and, 15–16
Std Dev chart, 230
Step button, 479
Step History panel, 479
Stepwise Logistic model, 478–479
stepwise models
checking and revising, 406–409
variable selection with, 404–406
Stepwise Regression
control panel, 405
modeling, 461
Stepwise report, 405
Stop button, 479
Strategic Critical quadrant, in Product
Categorization Matrix, 165
Strategic Security quadrant, in
Product Categorization Matrix,
165
strategies, Visual Six Sigma, 18–19
structure, of Add-Ins, 528–529
Subgroup Size, in Proﬁler, 383
Subset (Tables menu), 49
Summary (Tables menu), 49
Summary Reports, 289, 290
Summary Statistics report, 110, 173,
305–306
Summary tables, 118, 119, 135, 137,
322–323
Suppliers, Inputs, Process, Outputs,
and Customers (SIPOC) map, 351
supply/demand balance, in data sets,
162–163
Surface Plot (Graph menu), 47, 280
Surface Proﬁler, 501
T
Table panel, 261, 270
Table Variable, 459–460
tables, creating for summary statistics
of variables, 110
Tables menu
about, 71
displays in, 49–50
illustrated, 50
tabular displays, dynamic
visualization of prescriptions
with, 320–321
Tabulate (Analyze menu), 48, 489,
494
tabulation results, producing, 134
Tactical Proﬁt quadrant, in Product
Categorization Matrix, 165
TanH, 497
teams, forming, 350–351
techniques, JMP, 66
test sets
comparing models on, 504
constructing, 452–461
testing and inference, 16
Test-Retest Std Dev, in Proﬁler, 383
TextBox(), 528, 530–531
timelines, setting new, 378–380
Tip of the Day window, 29–30
toolbars, customizing in JMP, 33
traces, visualizing, 414
trade policy, democracy and,
515–528
training sets, constructing, 452–461
transfer functions, 283
Transforming Pricing Management in
a Chemical Supplier case study
about, 158–159, 221
background, 162–163
Collect Data step, 166–174
Frame Problem step, 160–166
Model Relationships step, 200–205

552
I N D E X
Transforming Pricing Management in
a Chemical Supplier case study
(Continued)
Revise Knowledge step, 205–218
Uncover Relationships step,
174–200
Utilize Knowledge step, 218–221
Transpose (Tables menu), 49
tree map, 146, 148, 149–150
tree structure, in JMP, 29
Treemap (Graph menu), 47, 518
Trip, Albert, 16, 19
Tukey HSD option, 183, 324
two-way interactions,
adding, 261
U
Uncover Relationships step, in VSS
Data Analysis Process
about, 20, 21, 22–23, 60, 295
in case studies, 323–326
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
and, 256
Improving the Quality of Anodize
Parts case study, 242–256
Informing Pharmaceutical Sales and
Marketing case study, 318–321
in Reducing Hospital Late Charge
Incidents case study, 109–141
Transforming Pricing Management
in a Chemical Supplier case
study, 174–200
uncovering Hot Xs, 141–155,
264–270
unhiding panes in JMP, 29
Utilize Knowledge step, in VSS Data
Analysis Process
about, 21, 292–294, 295
DMAIC (Deﬁne, Measure, Analyze,
Improve, and Control) approach
and, 257
Improving a Polymer
Manufacturing Process case
study, 423–424
Improving the Quality of Anodize
Parts case study, 272–294
Transforming Pricing Management
in a Chemical Supplier case
study, 218–221
V
Validation Column, 483, 484–489
Validation Method, 483
Validation Portion, 191
Validation report, 484–489
validation sets, constructing,
452–461
Value Colors column property,
189–190, 448–449
Value Labels property, 454
Var Comps Model, 378
Variability Chart, 232–233
Variable Importance feature, 280, 488
variables
adding descriptions for in data
tables, 108
chunking, 309
creating tables for summary
statistics of, 110
distribution of, 461
dynamic visualization of multiples
at once, 187–200
dynamic visualization of one and
two at a time, 305–314
dynamic visualization of two at a
time, 177–187
dynamic visualization of using
distribution, 175–177
exploring relationships for two at a
time, 445–452
response, 249, 310–313
viewing notes for, 108
visualizing one at a time, 109–113,
385–392
visualizing two at a time, 125–141,
392–400
variance components, 370–373
variation
deﬁned, 9
Six Sigma and, 15–16
versions, JMP, 29
viewing
by entry order, 127–132
formulas in Formula Editor, 239
notes for variables, 108

I N D E X
553
virtual column, 80–82
visual displays, in JMP, 33–40,
47–54
Visual Six Sigma
See also Six Sigma; speciﬁc topics
about, 4, 24–25
data analysis process, 19–21
data quality for, 68–70
guidelines for, 23–24
roadmap for, 21–23
strategies of, 4, 18–19
Visual Six Sigma (VSS) Data Analysis
Process
about, 19–21
illustrated, 200
JMP and, 58–66
visualizing
See also dynamic visualization
about, 16–18
one variable at a time, 109–113,
385–392
traces, 414
two variables at a time, 125–141,
392–400
Vital X
See Hot X
VListBox(), 530
voice of the customer (VOC),
355–356
W
Weight column, 506
Window List pane (JMP Home
Window), 29, 46
windows, managing in JMP, 46–47
Wisconsin Breast Cancer Diagnostic
Data Set
See Classiﬁcation of Cells case study
X
XBar chart, constructing, 425,
433–434


Kamran Iqbal
Fundamental Engineering
Optimization Methods
Download free books at

2 
 
Kamran Iqbal
Fundamental Engineering Optimization 
Methods
Download free eBooks at bookboon.com

3 
 
Fundamental Engineering Optimization Methods
1st edition
Â© 2013 Kamran Iqbal & bookboon.com
ISBN 978-87-403-0489-3
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
4 
Contents
Contents
	
Preface	
8
1	
Engineering Design Optimization	
10
1.1	
Introduction 	
10
1.2	
Optimization Examples in Science and Engineering	
11
1.3	
Notation	
18
2	
Mathematical Preliminaries	
19
2.1	
Set Definitions 	
19
2.2	
Function Definitions	
20
2.3	
Taylor Series Approximation	
21
2.4	
Gradient Vector and Hessian Matrix	
23
2.5	
Convex Optimization Problems	
24
2.6	
Vector and Matrix Norms	
26
2.7	
Matrix Eigenvalues and Singular Values	
26
2.8	
Quadratic Function Forms	
27
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Fundamental Engineering Optimization 
Methods
5 
Contents
2.9	
Linear Systems of Equations	
28
2.10	
Linear Diophantine System of Equations	
30
2.11	
Condition Number and Convergence Rates	
30
2.12	
 Conjugate-Gradient Method for Linear Equations	
32
2.13	
Newtonâ€™s Method for Nonlinear Equations	
33
3	
Graphical Optimization	
34
3.1	
Functional Minimization in One-Dimension 	
35
3.2	
Graphical Optimization in Two-Dimensions	
36
4	
Mathematical Optimization	
43
4.1	
The Optimization Problem	
44
4.2	
Optimality criteria for the Unconstrained Problems	
45
4.3	
Optimality Criteria for the Constrained Problems	
48
4.4	
Optimality Criteria for General Optimization Problems	
54
4.5	
Postoptimality Analysis	
59
4.6	
Lagrangian Duality 	
60
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Fundamental Engineering Optimization 
Methods
6 
Contents
5	
Linear Programming Methods	
68
5.1	
The Standard LP Problem	
68
5.2	
The Basic Solution to the LP Problem	
70
5.3	
The Simplex Method	
72
5.4	
Postoptimality Analysis	
84
5.5	
Duality Theory for the LP Problems	
89
5.6	
Non-Simplex Methods for Solving LP Problems	
97
5.7	
Optimality Conditions for LP Problems	
101
5.8	
The Quadratic Programming Problem	
104
5.9	
The Linear Complementary Problem	
108
6	
Discrete Optimization 	
113
6.1	
Discrete Optimization Problems	
113
6.2	
Solution Approaches to Discrete Problems	
114
6.3	
Linear Programming Problems with Integral Coefficients	
115
6.5	
Integer Programming Problems	
119
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Fundamental Engineering Optimization 
Methods
7 
Contents
7	
Numerical Optimization Methods	
126
7.1	
The Iterative Method	
127
7.2	
Computer Methods for Solving the Line Search Problem	
128
7.3	
Computer Methods for Finding the Search Direction	
134
7.4	
Computer Methods for Solving the Constrained Problems	
146
7.5	
Sequential Linear Programming	
151
7.6	
Sequential Quadratic Programming 	
153
	
References	
162
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Fundamental Engineering Optimization 
Methods
8 
Preface
Preface
This book is addressed to students in fields of engineering and technology as well as practicing engineers. 
It covers the fundamentals of commonly used optimization methods used in engineering design. 
Optimization methods fall among the mathematical tools typically used to solve engineering problems. 
It is therefore desirable that graduating students and practicing engineers are equipped with these tools 
and are trained to apply them to specific problems encountered in engineering practice. 
Optimization is an integral part of the engineering design process. It focuses on discovering optimum 
solutions to a design problem through systematic consideration of alternatives, while satisfying resource 
and cost constraints. Many engineering problems are open-ended and complex. The overall design 
objective in these problems may be to minimize cost, to maximize profit, to streamline production, to 
increase process efficiency, etc. Finding an optimum solution requires a careful consideration of several 
alternatives that are often compared on multiple criteria. 
Mathematically, the engineering design optimization problem is formulated by identifying a cost function 
of several optimization variables whose optimal combination results in the minimal cost. The resource 
and other constraints are similarly translated into mathematical relations. Once the cost function and 
the constraints have been correctly formulated, analytical, computational, or graphical methods may 
be employed to find an optimum. The challenge in complex optimization problems is finding a global 
minimum, which may be elusive due to the complexity and nonlinearity of the problem.
This book covers the fundamentals of optimization methods for solving engineering problems. Written 
by an engineer, it introduces fundamentals of mathematical optimization methods in a manner that 
engineers can easily understand. The treatment of the topics presented here is both selective and concise. 
The material is presented roughly at senior undergraduate level. Readers are expected to have familiarity 
with linear algebra and multivariable calculus. Background material has been reviewed in Chapter 2.
The methods covered in this book include: a) analytical methods that are based on calculus of variations; 
b) graphical methods that are useful when minimizing functions involving a small number of variables; 
and c) iterative methods that are computer friendly, yet require a good understanding of the problem. 
Both linear and nonlinear methods are covered. Where necessary, engineering examples have been used 
to build an understanding of how these methods can be applied. Though not written as text, it may be 
used as text if supplemented by additional reading and exercise problems from the references.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
9 
Preface
There are many good references available on the topic of optimization methods. A short list of prominent 
books and internet resources appears in the reference section. The following references are main sources 
for this manuscript and the topics covered therein: Arora (2012); Belegundu and Chandrupatla (2012); 
Chong and Zak (2013); and, Griva, Nash & Sofer (2009). In addition, lecture notes of eminent professors 
who have regularly taught optimization classes are available on the internet. For details, the interested 
reader may refer to these references or other web resources on the topic. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Fundamental Engineering Optimization 
Methods
10 
Engineering  Desig n Optimizatio
1	 Engineering Design 
Optimization
This chapter introduces the topic of optimization through example problems that have been selected 
from various fields including mathematics, economics, computer science, and engineering.
Learning Objectives: The learning goal in this chapter is to develop an appreciation for the topic as 
well as the diversity and usefulness of the mathematical and computational optimization techniques.
1.1	
Introduction 
Engineering system design comprises selecting one or more variables to meet a set of objectives. A better 
design is obtained if an appropriate cost function can be reduced. The design is optimum when the cost 
is the lowest among all feasible designs. Almost always, the design choices are limited due to resource 
constraints, such as material and labor constraints, as well as physical and other restrictions. A feasible 
region in the design space is circumscribed by the constraint boundaries. More importantly, both the 
cost function and the constraints can be cast as mathematical functions involving design variables. The 
resulting mathematical optimization problem can then be solved using methods discussed in this book.
Engineering system design is an interdisciplinary process that necessitates cooperation among designers 
from various engineering fields. Engineering design can be a complex process. It requires assumptions to 
be made to develop models that can be subjected to analysis and verification by experiments. The design 
of a system begins by analyzing various options. For most applications the entire design project must be 
broken down into several subproblems which are then treated independently. Each of the subproblems 
can be posed as an optimum design problem to be solved via mathematical optimization.
A typical optimum engineering design problem may include the following steps: a descriptive problem 
statement, preliminary investigation and data collection as a prelude to problem formulation, identification 
of design variables, optimization criteria and constraints, mathematical formulation of the optimization 
problem, and finding a solution to the problem. This text discusses the last two steps in the design process, 
namely mathematical formulation and methods to solve the design optimization problem.
Engineering design optimization is an open-ended problem. Perhaps the most important step toward 
solving the problem involves correct mathematical formulation of the problem. Once the problem has 
been mathematically formulated, analytical and computer methods are available to find a solution. 
Numerical techniques to solve the mathematical optimization problems are collectively referred as 
mathematical programming framework. The framework provides a general and flexible formulation for 
solving engineering design problems. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
11 
Engineering  Desig n Optimizatio
Some mathematical optimization problems may not have a solution. This usually happens due to 
conflicting requirements of incorrect formulation of the optimization problem. For example, constraints 
may be restrictive so that no feasible region can be found, or the feasible region may be unbounded due 
to a missing constraint. In this text we will assume that the problem has been correctly formulated so 
that the feasible region is closed and bounded.
1.2	
Optimization Examples in Science and Engineering
We wish to introduce the topic of optimization with the help of examples. These examples have been 
selected from various STEM (science, technology, engineering, mathematics) fields. Each example 
requires finding the optimal values of a set of design variables in order to optimize (maximize or minimize) 
a generalized cost that may represent the manufacturing cost, profit, energy, power, distance, mean square 
error, and so on. The complexity of the design problem grows with number of variables involved. Each 
of the simpler problems, presented first, involves a limited number of design variables. The problems 
that follow are more complex in nature and may involve hundreds of design variables. Mathematical 
formulation of each problem is provided following the problem definition. While the simpler problems 
are relatively easy to solve by hand, the complex problems require the use of specialized optimization 
software in order to find a solution. 
Problem 1: Student diet problem
A student has a choice of breakfast menu (eggs, cereals, tarts) and a limited ($10) budget to 
fulfill his/her nutrition needs (1000 calories, 100 g protein) at minimum cost. Eggs provide 
500 calories and 50g protein and cost $3.50; cereals provide 500 calories and 40g protein and 
cost $3; tarts provide 600 calories and 20g protein and cost $2. How does he/she choose his/
her breakfast mix?
Formulation: Let İ”à¯àµŒáˆ¾İ”à¬µÇ¡ İ”à¬¶Ç¡ İ”à¬·áˆ¿ represent the quantities of eggs, cereals and tarts chosen 
for breakfast. Then, the optimization problem is mathematically formulated as follows:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®Ç¡à¯«à°¯İ‚àµŒÍµÇ¤Í·İ”à¬µàµ…Íµİ”à¬¶àµ…Í´İ”à¬·
6XEMHFWWRÍ·Í²Í²áˆºİ”à¬µàµ…İ”à¬¶áˆ»àµ…Í¸Í²Í²İ”à¬·àµ’Í³Í²Í²Í²Ç¡ Í·Í²İ”à¬µàµ…Í¶Í²İ”à¬¶àµ…Í´Í²İ”à¬·àµ’Í³Í²Í²Ç¡


ÍµÇ¤Í·İ”à¬µàµ…Íµİ”à¬¶àµ…Í´İ”à¬·àµ‘Í³Í²Ç¢İ”à¬µÇ¡ İ”à¬¶Ç¡ İ”à¬·× Ôº

(1.1)
Problem 2: Simplified manufacturing problem
A manufacturer produces two products: tables and chairs. Each table requires 10 kg of material 
and 5 units of labor, and earns $7.50 in profit. Each chair requires 5 kg of material and 12 units 
of labor, and earns $5 in profit. A total of 60 kg of material and 80 units of labor are available. 
Find the best production mix to earn maximum profit.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
12 
Engineering  Desig n Optimizatio
Formulation: Let /HWİ”à¯àµŒáˆ¾İ”à¬µÇ¡ İ”à¬¶áˆ¿ represent the quantities of tables and chairs to be 
manufactured. Then, the optimization problem is mathematically formulated as follows:
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ‚àµŒÍ¹Ç¤Í·İ”à¬µàµ…Í·İ”à¬¶
6XEMHFWWRÍ³Í²İ”à¬µàµ…Í·İ”à¬¶àµ‘Í¸Í²Ç¡ Í·İ”à¬µàµ…Í³Í´İ”à¬¶àµ‘ÍºÍ²Ç¢ İ”à¬µÇ¡ İ”à¬¶× Ôº
(1.2)
Problem 3: Shortest distance problem
Find the shortest distance from a given point áˆºİ”à¬´Ç¡ İ•à¬´áˆ» to a given curve: İ•àµŒİ‚áˆºİ”áˆ»
Formulation: The optimization problem is mathematically formulated to minimize the 
Euclidian distance from the given point to the curve:
ÂÂ‹Â
à¯«Ç¡à¯¬İ‚àµŒà¬µ
à¬¶áˆ¼áˆºİ”àµ†İ”à¬´áˆ»à¬¶àµ…áˆºİ•àµ†İ•à¬´áˆ»à¬¶áˆ½
6XEMHFWWRİ•àµŒİ‚áˆºİ”áˆ»

(1.3)
Problem 4: Data-fitting problem
Given a set of Ü° data points áˆºİ”à¯œÇ¡ İ•à¯œáˆ»Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ Ü° fit a polynomial of degree to the data such 
that the mean square error Ïƒ
àµ«İ•à¯œàµ†İ‚áˆºİ”à¯œáˆ»àµ¯
à¬¶
à¯‡
à¯œà­€à¬µ
 is minimized.
Formulation: Let the polynomial be given as: İ•àµŒİ‚áˆºİ”áˆ»àµŒÜ½à¬´àµ…Ü½à¬µİ”àµ…Ú® àµ…Ü½à¯ İ”à¯ Ç¢ then, the 
unconstrained optimization problem is formulated as:
ÂÂ‹Â
à¯”à°¬Ç¡à¯”à°­İ‚àµŒà¬µ
à¬¶à·
áˆºİ•à¯œàµ†Ü½à¬´àµ†Ü½à¬µİ”à¯œàµ†Ú® àµ†Ü½à¯ İ”à¯œ
à¯ áˆ»à¬¶
à¯‡
à¯œà­€à¬µ

(1.4)
Problem 5: Soda can design problem
Design a soda can (choose diameter d and height h) to hold a volume of 200 ml, such that the 
manufacturing cost (a function of surface area) is minimized and the constraint İ„àµ’Í´İ€ is obeyed.
Formulation: Let à¢à¯àµŒáˆ¾İ€Ç¡ İˆáˆ¿ represent the diameter and length of the can. Then, the 
optimization problem is formulated to minimize the surface area of the can as:
ÂÂ‹Â
à¯—Ç¡à¯Ÿİ‚àµŒà¬µ
à¬¸ß¨İ€à¬¶àµ…ß¨İ€İˆ
6XEMHFWWRà°­
à°°ß¨İ€à¬¶İˆàµŒÍ´Í²Í²Ç¡ Í´İ€àµ†İ„àµ‘Í²
(1.5)
Problem 6: Open box problem
What is the largest volume for an open box that can be constructed from a given sheet of paper 
(8.5â€³Ã—11â€³) by cutting out squares at the corners and folding the sides?
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
13 
Engineering  Desig n Optimizatio
Formulation: Let x represent the side of the squares to be cut; then, the unconstrained 
optimization problem is formulated as:
ÂÂƒÂš
à¯«İ‚àµŒİ”áˆºÍºÇ¤Í· àµ†Í´İ”áˆ»áˆºÍ³Í³ àµ†Í´İ”áˆ»
(1.6)
Problem 7: Ladder placement problem
What are the dimensions (width, height) of the largest box that can be placed under a ladder 
of length l when the ladder rests against a vertical wall?
Formulation: Let [x, y] represent the dimensions of the box, and let áˆºÜ½Ç¡ Í²áˆ» and áˆºÍ²Ç¡ Ü¾áˆ» represent 
the horizontal and vertical contact points of the ladder with the floor and the wall, respectively. 
Then, the optimization problem is mathematically formulated as:
ÂÂƒÂš
à¯«Ç¡à¯¬İ‚àµŒİ”İ•
6XEMHFWWR
à¯«
à¯”àµ…
à¯¬
à¯•àµ‘Í³Ç¡ Ü½à¬¶àµ…Ü¾à¬¶àµŒİˆ
(1.7)
Problem 8: Logging problem
What are the dimensions of a rectangular beam of maximum dimensions (or volume) that can 
be cut from a log of given dimensions?
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Fundamental Engineering Optimization 
Methods
14 
Engineering  Desig n Optimizatio
Formulation: Let [x, y] represent the width and height of the beam to be cut, and let d represent 
the diameter of the log. Then, the optimization problem is formulated as:
max f = xy
  xy
Subject to: x2 + y2 â€“ d2 â‰¤ 0

(1.8)
Problem 9: Knapsack problem
Given an assortment of n items, where each item i has a value Ü¿à¯œàµÍ² and a weight İ“à¯œàµÍ² 
fill a knapsack of given capacity (weight W) so as to maximize the value of the included items. 
Formulation: Without loss of generality, we assume that Ü¹àµŒÍ³/HWİ”à¯œ× áˆ¼Í²Ç¡Í³áˆ½ . Let denote the 
event that item i is selected for inclusion in the sack; then, the knapsack problem is formulated as:
ÂÂƒÂš
à¯«à³”İ‚àµŒà·
Ü¿à¯œİ”à¯œ
à¯¡
à¯œà­€à¬µ

Â—Â„ÂŒÂ‡Â…Â–Â–Â‘Ç£à·
İ“à¯œİ”à¯œ
à¯¡
à¯œà­€à¬µ
àµ‘Í³

(1.9)
Problem 10: Investment problem
Given the stock prices İŒà¯œ and anticipated rates of return İà¯œ associated with a group of investments, 
choose a mix of securities to invest a sum of $1M in order to maximize return on investment.
Formulation: /HWİ”à¯œ× áˆ¼Í²Ç¡Í³áˆ½ express the inclusion of security i in the mix, then the investment 
problem is modeled as the knapsack problem (Problem 9).
Problem 11: Set covering problem
Given a set ÜµàµŒáˆ¼İà¯œÇ£ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰áˆ½ and a collection à£­àµŒàµ›Üµà¯Ç£İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠàµŸ of subsets of Üµ with 
associated costs Ü¿à¯Ç¡ find the smallest sub-collection È­ of à£­ that covers i.e., ÜµÇ¡LHÇ¡ Ú‚
Üµà¯àµŒÜµ
à¯Œà³•×à®Š

Formulation: /HWÜ½à¯œà¯× áˆ¼Í²Ç¡Í³áˆ½ denote the condition that İà¯œ× Üµà¯Ç¡DQGOHWİ”à¯× áˆ¼Í²Ç¡Í³áˆ½ and let 
denote the condition that Üµà¯× È­Ç¢ then, the set covering problem is formulated as:
ÂÂƒÂš
à¯«à³•İ‚àµŒà·
Ü¿à¯İ”à¯
à¯¡
à¯à­€à¬µ

Â—Â„ÂŒÂ‡Â…Â–Â–Â‘Ç£à·
Ü½à¯œà¯İ”à¯œàµ’Í³
à¯¡
à¯à­€à¬µ
Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¢ İ”à¯× áˆ¼Í²Ç¡Í³áˆ½Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ
(1.10)
Problem 12: Airline scheduling problem
Given the fixed costs and operating costs per segment, design an optimum flight schedule to 
minimize total operating cost for given passenger demand on each segment over a network 
of routes to be serviced under given connectivity, compatibility, and resource (aircrafts, 
manpower) availability constraints.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
15 
Engineering  Desig n Optimizatio
Formulation: Let ÜµàµŒáˆ¼İà¯œÇ£ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰áˆ½ denote the set of flight segments required to be covered, 
and let each subset Üµà¯Ùƒ Üµ denote a set of connected flight segments that can be covered by an 
aircraft or a crew; then the least cost problem to cover the available routes can be formulated 
as a set covering problem (Problem 10).
Problem 13: Shortest path problem
Find the shortest path from node p to node q in a connected graph (V, E), where V denotes the 
vertices and denotes the edges.
Formulation: Let İà¯œà¯ denote the edge incident to both nodes İ… and İ†Ç¡ and let İ‚Ç£ Ü§Õœ Ô¹ 
represent a real-valued weight function; further, let Ü²àµŒáˆºİ’à¬µÇ¡ İ’à¬¶Ç¡ Ç¥ Ç¡ İ’à¯¡áˆ» denote a path, where 
İ’à¬µàµŒİŒÇ¡ İ’à¯¡àµŒİÇ¢ then, the unconstrained single-pair shortest path problem is formulated as:
ÂÂ‹Â
à¯¡İ‚àµŒà·
İà¯œÇ¡à¯œà¬¾à¬µ
à¯¡à¬¿à¬µ
à¯œà­€à¬µ

(1.11)
Alternatively, let İ”à¯œà¯ denote a variable associated with İà¯œà¯Ç¢ then, an integer programming 
formulation (Chapter 6) of the shortest path problem is given as:
ÂÂ‹Â
à¯«à³”à³•İ‚àµŒà·İà¯œà¯İ”à¯œà¯
à¯œÇ¡à¯

Â—Â„ÂŒÂ‡Â…Â–Â–Â‘Ç£à·İ”à¯œà¯
à¯
àµ†à·İ”à¯à¯œ
à¯
àµŒàµ
Í³ÂˆÂ‘Â”İ…àµŒİŒ
àµ†Í³ÂˆÂ‘Â”İ…àµŒİ
Í²Â‘Â–ÂŠÂ‡Â”Â™Â‹Â•Â‡


(1.12)
Note: the shortest path problem is a well-known problem in graph theory and algorithms, such 
as Dijkstraâ€™s algorithm or Bellman-Ford algorithm, are available to solve variants of the problem.
Problem 14: Traveling salesman problem
A company requires a salesman to visit its N stores (say 50 stores) that are geographically 
distributed in different locations. Find the visiting sequence that will require the least amount 
of overall travel. 
Formulation: The traveling salesman problem is formulated as shortest path problem in an 
undirected weighted graph where the stores represent the vertices of the graph. The problem 
is then similar to Problem 10.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
16 
Engineering  Desig n Optimizatio
Problem 15: Transportation problem
Goods are to be shipped from m supply points with capacities: İà¬µÇ¡ İà¬¶Ç¡ Ç¥ Ç¡ İà¯  to distribution 
points with demands: İ€à¬µÇ¡ İ€à¬¶Ç¡ Ç¥ Ç¡ İ€à¯¡ Given the transportation cost Ü¿à¯œà¯ for each of the network 
routes, find the optimum quantities, İ”à¯œà¯Ç¡ to be shipped along those routes to minimize total 
shipment cost.
Formulation: let İ”à¯œà¯Ç¡ denote the quantity to be shipped node i to node j; then, the optimization 
problem is formulated as:
ÂÂ‹Â
à¯«à³”à³•İ‚àµŒà·Ü¿à¯œà¯İ”à¯œà¯
à¯œÇ¡à¯

Â—Â„ÂŒÂ‡Â…Â–Â–Â‘Ç£à·İ”à¯œà¯
à¯
àµŒİà¯œÇ¡ÂˆÂ‘Â”İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¢à·İ”à¯œà¯
à¯œ
àµŒİ€à¯Ç¡ÂˆÂ‘Â”İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠÇ¢ İ”à¯œà¯àµ’Í²

(1.13)
Problem 16: Power grid estimation problem 
Given the measurements of active and reactive power flows àµ«İŒà¯œà¯Ç¡ İà¯œà¯àµ¯ between nodes i, j and the 
measurements İ’à¯œ of the node voltages in an electric grid, obtain the best estimate of the state of 
the grid, i.e., solve for complex node voltages: Â˜à¯œàµŒİ’à¯œ×¡ßœà¯œÇ¡ where ßœà¯œ represents the phase angle. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
17 
Engineering  Desig n Optimizatio
Formulation: let İ’Ò§à¯œÇ¡ İŒÒ§à¯œà¯Ç¡ İà´¤à¯œà¯ represent the measured variables, and İ‡à¯œ
à¯©Ç¡ İ‡à¯œà¯
à¯£Ç¡ İ‡à¯œà¯
à¯¤Ç¡ let respectively, 
represent the confidence in measurements of the node voltages and the power flows; further 
let Âœà¯œà¯àµŒİ–à¯œà¯×¡ß à¯œà¯ represent the complex impedance between nodes İ…Ç¡ İ†Ç¢ then, power grid state 
estimation problem is formulated as (Pedregal, p. 11):

ÂÂ‹Â
à¯©à³”Ç¡à°‹à³”İ‚àµŒà·İ‡à¯œ
à¯©áˆºİ’à¯œàµ†İ’Ò§à¯œáˆ»à¬¶
à¯œ
àµ…à·İ‡à¯œà¯
à¯£àµ«İŒà¯œà¯àµ†İŒÒ§à¯œà¯àµ¯
à¬¶
à¯œÇ¡à¯
àµ…à·İ‡à¯œà¯
à¯¤àµ«İà¯œà¯àµ†İà´¤à¯œà¯àµ¯
à¬¶
à¯œÇ¡à¯

6XEMHFWWRàµ
İŒà¯œà¯àµŒ
à¯©à³”
à°®
à¯­à³”à³•Â…Â‘Â• ß à¯œà¯àµ†
à¯©à³”à¯©à³•
à¯­à³”à³•Â…Â‘Â•àµ«ß à¯œà¯àµ…ßœà¯œàµ†ßœà¯àµ¯
İà¯œà¯àµŒ
à¯©à³”
à°®
à¯­à³”à³•Â•Â‹Â ß à¯œà¯àµ†
à¯©à³”à¯©à³•
à¯­à³”à³•Â•Â‹Âáˆºß à¯œà¯àµ…ßœà¯œàµ†ßœà¯áˆ»


(1.14)
Problem 17 Classification problem
Given a set of data points: à¢à¯œ× Ô¹à¯¡Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠÇ¡ with two classification labels: İ•à¯œ× áˆ¼Í³Ç¡ àµ†Í³áˆ½ 
find the equation of a hyperplane separating data into classes with maximum inter-class distance.
Formulation: To simplify the problem, we assume that data points lie in a plane, i.e., à¢à¯œ× Ô¹à¬¶Ç¡ 
and that they are linearly separable. We consider a hyperplane of the form: à¢à¯à¢àµ†Ü¾àµŒÍ²Ç¡ 
where w is a weight vector that is normal to the hyperplane. For separating given data points, 
we assume that à¢à¯à¢à¯œàµ†Ü¾àµ’Í³ for points labeled as 1, and à¢à¯à¢à¯œàµ†Ü¾àµ‘àµ†Í³ for points labeled 
as â€“1. The two hyperplanes (lines) are separated by à¬¶
Ô¡à¢Ô¡ Thus, optimization problem is defined as:
ÂÂƒÂš
à¢à¬µ
à¬¶Ô¡à¢Ô¡à¬¶
6XEMHFWWRÍ³ àµ†İ•à¯œáˆºà¢à¯à¢à¯œàµ†Ü¾áˆ»àµ‘Í²Ç¢ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ

(1.15)
Problem 18: Steady-state finite element analysis problem 
Find nodal displacements İ‘à¯œ that minimize the total potential energy associated with a set of 
point masses İ‰à¯œ connected via springs of constants İ‡à¯œà¯Ç¡ while obeying structural and load 
constraints. 
Formulation: For simplicity we consider a one-dimensional version of the problem, where 
the nodal displacements are represented as: İ‘à¬µÇ¡ İ‘à¬¶Ç¡ Ç¥ Ç¡ İ‘à¯‡Ç¤ Let İ‚à¯œ represent an applied force at 
node İ…Ç¢ then, the potential energy minimization problem is formulated as:
ÂÂ‹Â
à¯¨à³”Ï‚ àµŒà¬µ
à¬¶à·İ‡à¯œà¯İ‘à¯œİ‘à¯
à¯œÇ¡à¯
àµ…à·İ‘à¯œİ‚à¯œ
à¯œ

(1.16)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
18 
Engineering  Desig n Optimizatio
Problem 19: Optimal control problem
Find an admissible control sequence İ‘áˆºİáˆ»that minimizes a quadratic cost function Ü¬áˆºİ”Ç¡ İ‘Ç¡ İáˆ»Ç¡
while moving a dynamic system: İ”áˆ¶àµŒÜ£İ”àµ…Ü¤İ‘ between prescribed end points. The class of 
optimal control problems includes minimum energy and minimum time problems, among 
others.
Formulation: As a simplified problem, we consider the optimal control of an inertial system 
of unit mass modeled with position İ”áˆ» and velocity İ’ The system dynamics are given as: 
İ”áˆ¶àµŒİ’Ç¡ İ’áˆ¶àµŒİ‘Ç¡ where İ‘áˆºİáˆ»Ç¡ İ×áˆ¾Í²Ç¡ Ü¶áˆ¿ represents the input. We consider a quadratic cost that 
includes time integral of square of position and input variables. The resulting optimal control 
problem is formulated as:
ÂÂ‹Â
à¯«à³”İ‚àµŒà¶±
à¬µ
à¬¶áˆºİ”à¬¶àµ…ß©İ‘à¬¶áˆ»İ€İ
à¯
à¬´

6XEMHFWWRİ”áˆ¶àµŒİ’Ç¡ İ’áˆ¶àµŒİ‘

(1.17)
1.3	
Notation
The following notation is used throughout this book: Ô¹ denotes the set of real numbers; Ô¹à¯¡ denotes 
the set of real n-vectors; Ô¹à¯ àµˆà¯¡ denotes the set of real İ‰àµˆİŠ matrices; İ‚Ç£ Ô¹à¯¡Õœ Ô¹à¯  denotes an Ô¹à¯  
valued function defined over ; Ô¹à¯¡Ôº denotes the set of integers, and Ôºà¯¡ denotes integer vectors. In the 
text, small bold face letters such as à¢Ç¡ à¢Ÿ are used to represent vectors or points in Ô¹à¯¡ capital bold face 
letters such à¡­Ç¡ à¡® as are used to represent matrices; à¡­à¯¤ represents qth column of A; and à¡µ represents an 
identity matrix.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
19 
Mathematical Preliminaries
2	 Mathematical Preliminaries
This chapter introduces essential mathematical concepts that are required to understand the material 
presented in later chapters. The treatment of the topics is concise and limited to presentation of key 
aspects of the topic. More details on these topics can be found in standard mathematical optimization 
texts. Interested readers should consult the references (e.g., Griva, Nash & Sofer, 2009) for details. 
Learning Objectives: The learning goal in this chapter is to understand the mathematical principles 
necessary for formulating and solving optimization problems, i.e., for understanding the optimization 
techniques presented in later chapters. 
2.1	
Set Definitions 
Closed Set. A set is closed if for any sequence of points áˆ¼İ”à¯áˆ½İ”à¯× ÜµÂÂ‹Âà¯Õœà®¶İ”à¯àµŒİ” we have İ”× Üµ 
For example, the set ÜµàµŒáˆ¼İ”Ç£ Èİ”È àµ‘Ü¿áˆ½ where c is a finite number, describes a closed set. 
Bounded Set. A set S is bounded if for every İ”× ÜµÇ¡ Ô¡İ”Ô¡ àµÜ¿ where Ô¡Î®Ô¡ represents a vector norm and 
c is a finite number. 
Compact set. A set S is compact if it is both closed and bounded.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Fundamental Engineering Optimization 
Methods
20 
Mathematical Preliminaries
Interior point. A point is 
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ‚àµŒÍ¹Ç¤Í·İ”à¬µàµ…Í·İ”à¬¶
6XEMHFWWRÍ³Í²İ”à¬µàµ…Í·İ”à¬¶àµ‘Í¸Í²Ç¡ Í·İ”à¬µàµ…Í³Í´İ”à¬¶àµ‘ÍºÍ²Ç¢ İ”à¬µÇ¡ İ”à¬¶× Ôº interior to the set if áˆ¼İ•Ç£ Ô¡İ•àµ†İ”Ô¡ àµß³áˆ½Ø¿ Üµ for some ß³àµÍ²
Open Set. A set S is open if every 
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ‚àµŒÍ¹Ç¤Í·İ”à¬µàµ…Í·İ”à¬¶
6XEMHFWWRÍ³Í²İ”à¬µàµ…Í·İ”à¬¶àµ‘Í¸Í²Ç¡ Í·İ”à¬µàµ…Í³Í´İ”à¬¶àµ‘ÍºÍ²Ç¢ İ”à¬µÇ¡ İ”à¬¶× Ôº is an interior point of S. For example, the set ÜµàµŒáˆ¼İ”Ç£ Èİ”È àµÜ¿áˆ½ 
where c is a finite number, is an open set. 
Convex Set. A set S is convex if for each pair İ”Ç¡ İ•× Üµ their convex combination ß™İ”àµ…áˆºÍ³ àµ†ß™áˆ»İ•× Üµ
for Í² àµ‘ß™àµ‘Í³ Examples of convex sets include a single point, a line segment, a hyperplane, a halfspace, 
the set of real numbers Ô¹áˆ»DQGÔ¹à¯¡ 
Hyperplane. The set WÜµàµŒáˆ¼à¢Ç£ à¢‡à¯à¢àµŒÜ¾áˆ½Ç¡Z where a and b are constants defines a hyperplane. Note that 
in two dimensions a hyperplane is a line. Also, note that vector a is normal to the hyperplane.
Halfspace. The set ÜµàµŒáˆ¼à¢Ç£ à¢‡à¯à¢àµ‘Ü¾áˆ½Ç¡Z
I
$O
K
 where a and b are constants defines a halfspace. Note that vectora 
is normal to the halfspace. Also, note that a halfspace is convex.
Polyhedron. A polyhedron represents a finite intersection of hyperplanes and halfspaces. Note that a 
polyhedron is convex.
Convex Hull. The convex hull of a set S is the set of all convex combinations of points in S. Note that 
convex hull of S is the smallest convex set that contains S.
Extreme Point. A point İ”× Üµ is an extreme point (or vertex) of a convex S set if it cannot be expressed 
as İ”àµŒß™İ•àµ…áˆºÍ³ àµ†ß™áˆ»İ–Ç¡ZLWKİ•Ç¡ İ–× ÜµZKHUHİ•Ç¡ İ–àµİ”Ç¡DQGÍ² àµß™àµÍ³
2.2	
Function Definitions
Function. A function İ‚áˆºà¢áˆ» describes a mapping from a set of points called domain to a set of points 
called range. Mathematically, İ‚Ç£ à£Õœ à£¬ where à£ denotes the domain and à£¬ the range of the function.
Continuous Function. A function İ‚áˆºà¢áˆ» is said to be continuous at a point à¢à¬´ if limà¢Õœà¢à°¬İ‚áˆºà¢áˆ»àµŒİ‚áˆºà¢à¬´áˆ» 
Alternatively, if a sequence of points áˆ¼à¢à¯áˆ½in the function domain à£áˆºİ‚áˆ»converges toà¢à¬´ then İ‚áˆºà¢à¯áˆ»must 
converge to İ‚áˆºà¢à¬´áˆ» for a function to be continuous. Note, that for functions of single variable, this implies 
that left and right limits coincide.
Affine Function. A function of the form İ‚áˆºà¢áˆ»àµŒà¢‡à¯à¢àµ…Ü¾ of the form represents an affine function.
Quadratic Function. A function of the form İ‚áˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ†à¢ˆà¯à¢Ç¡ where Q is symmetric, represents 
a quadratic function.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
21 
Mathematical Preliminaries
Level Sets. The level sets of a function are defined as ÜµàµŒáˆ¼İ”Ç£ İ‚áˆºà¢áˆ»àµŒÜ¿áˆ½Ç¤ For functions of a single 
variable, the level sets represent discrete points. For functions of two variables, level sets are contours 
plotted in the İ”İ• plane.
Stationary Point. From elementary calculus, a single-variable function İ‚áˆºà¢áˆ» has a stationary point at 
İ”à¬´ if the derivative İ‚Ô¢áˆºİ”áˆ» vanishes at Wİ”à¬´, i.e., İ‚á‡±áˆºİ”à¬´áˆ»àµŒÍ² Graphically, the slope of the function is zero 
at the stationary point, which may represent a minimum, a maximum, or a point of inflecion. 
Local Minimum. A multi-variable function, İ‚áˆºà¢áˆ», has a local minimum at à¢×›LIİ‚áˆºà¢×›áˆ»àµ‘İ‚áˆºà¢áˆ» in a 
small neighborhood around à¢×›Ç¡ defined by Èà¢àµ†à¢×›È àµß³ 
Global Minimum. The multi-variable function İ‚áˆºà¢áˆ» has a global minimum at à¢×›LIİ‚áˆºà¢×›áˆ»àµ‘İ‚áˆºà¢áˆ» for 
all à¢ in a feasible region defined by the problem. 
Convex Functions. A function İ‚áˆºİ”áˆ» defined on a convex set Üµ is convex if and only if for all à¢Ç¡ à¢Ÿ× ÜµÇ¡ 
İ‚áˆºáˆ»
İ‚áˆºß™à¢àµ…áˆºÍ³ àµ†ß™áˆ»à¢Ÿáˆ»àµ‘ß™İ‚áˆºà¢áˆ»àµ…áˆºÍ³ àµ†ß™áˆ»İ‚áˆºà¢Ÿáˆ»ß™× áˆ¾Í²Ç¡Í³áˆ¿ Note that affine functions defined over convex 
sets are convex. Similarly, quadratic functions defined over convex sets are convex.
2.3	
Taylor Series Approximation
Taylor series approximates a differentiable function İ‚áˆºİ”áˆ» in the vicinity of an operating point İ”à¬´. Such 
approximation is helpful in several problems involving functions.
An infinite Taylor series expansion of İ‚áˆºİ”áˆ» around İ”à¬´ (where İ€àµŒİ”àµ†İ”à¬´) is given as:
İ‚áˆºİ”à¬´àµ…İ€áˆ»àµŒİ‚áˆºİ”à¬´áˆ»àµ…İ‚á‡±áˆºİ”à¬´áˆ»İ€àµ…Í³
Í´Ç¨ İ‚á‡±á‡±áˆºİ”à¬´áˆ»İ€à¬¶àµ…Ú®
As an example, the Taylor series for sin and cosine functions around İ”à¬´àµŒÍ² are given as: 
Â•Â‹Âİ”àµŒİ”àµ†
à¯«à°¯
à¬·Ç¨ àµ…
à¯«à°±
à¬¹Ç¨ àµ†Ú®
Â…Â‘Â• İ”àµŒÍ³ àµ†
à¯«à°®
à¬¶Ç¨ àµ…
à¯«à°°
à¬¸Ç¨ àµ†Ú®
These series are summed in the Euler formula: Â…Â‘Â• İ”àµ…İ…Â•Â‹Â İ”àµŒİà¬¿à¯œà¯«
The İŠth order Taylor series approximation of İ‚áˆºİ”áˆ» is given as:
İ‚áˆºİ”à¬´àµ…İ€áˆ»Ø† İ‚áˆºİ”à¬´áˆ»àµ…İ‚á‡±áˆºİ”à¬´áˆ»İ€àµ…Í³
Í´Ç¨ İ‚á‡±á‡±áˆºİ”à¬´áˆ»İ€à¬¶àµ…Ú® àµ…Í³
İŠÇ¨ İ‚áˆºà¯¡áˆ»áˆºİ”à¬´áˆ»İ€à¯¡
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
22 
Mathematical Preliminaries
We note that first or second order approximation often suffice in the close neighborhood of İ”à¬´. As an 
example, the local behavior of a function is frequently approximated by a tangent line defined as:
İ‚áˆºİ”áˆ»àµ†İ‚áˆºİ”à¬´áˆ»Ø† İ‚á‡±áˆºİ”à¬´áˆ»áˆºİ”àµ†İ”à¬´áˆ»
Next, the Taylor series expansion of a function İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ» of two variables at a point áˆºİ”à¬µà¬´Ç¡ İ”à¬¶à¬´áˆ» is given as:
İ‚áˆºİ”à¬µàµ…İ€à¬µÇ¡ İ”à¬¶àµ…İ€à¬¶áˆ»àµŒİ‚áˆºİ”à¬µà¬´Ç¡ İ”à¬¶à¬´áˆ»àµ…ß²İ‚
ß²İ”à¬µ
İ€à¬µàµ…ß²İ‚
ß²İ”à¬¶
İ€à¬¶àµ…Í³
Í´ á‰ˆß²à¬¶İ‚
ß²İ”à¬µ
à¬¶İ€à¬µ
à¬¶àµ…
ß²à¬¶İ‚
ß²İ”à¬µß²İ”à¬¶
Â†à¬µÂ†à¬¶àµ…ß²à¬¶İ‚
ß²İ”à¬¶
à¬¶İ€à¬¶
à¬¶á‰‰àµ…Ú®
where İ€à¬µàµŒİ”à¬µàµ†İ”à¬µà¬´Ç¡ İ€à¬¶àµŒİ”à¬¶àµ†İ”à¬¶à¬´Ç¡ and all partial derivatives are computed at the point: áˆºİ”à¬µà¬´Ç¡ İ”à¬¶à¬´áˆ». 
Further, let İ–àµŒİ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»Ç¢then, the tangent plane of İ– at áˆºİ”à¬µà¬´Ç¡ İ”à¬¶à¬´áˆ» is defined by the equation:
İ–àµŒİ‚áˆºİ”à¬µà¬´Ç¡ İ”à¬¶à¬´áˆ»àµ…
à°¡à¯™
à°¡à¯«à°­á‰š
áˆºà¯«à°­à°¬Ç¡à¯«à°®à°¬áˆ»
áˆºİ”à¬µàµ†İ”à¬µà¬´áˆ»àµ…
à°¡à¯™
à°¡à¯«à°®á‰š
áˆºà¯«à°­à°¬Ç¡à¯«à°®à°¬áˆ»
áˆºİ”à¬¶àµ†İ”à¬¶à¬´áˆ»
Taylor series expansion in the case of a multi-variable function is given after defining gradient vector 
and Hessian matrix in Sec. 2.4. Finally, it is important to remember that Taylor series only approximates 
the local behavior of the function, and therefore should be used with caution. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Fundamental Engineering Optimization 
Methods
23 
Mathematical Preliminaries
2.4	
Gradient Vector and Hessian Matrix
The gradient vector and Hessian matrix play an important role in optimization. These concepts are 
introduced as such:
The Gradient Vector. Let İ‚áˆºà¢áˆ»àµŒİ‚áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ Ç¥ Ç¡ İ”à¯¡áˆ» be a real-valued function of variables with continuous 
partial derivatives, i.e., İ‚× ×‹à¬µ. Then, the gradient of İ‚ is a vector defined by:
×İ‚áˆºà¢áˆ»à¯àµŒàµ¬ß²İ‚
ß²İ”à¬µ
Ç¡ ß²İ‚
ß²İ”à¬¶
Ç¡ Ç¥ Ç¡ ß²İ‚
ß²İ”à¯¡
àµ°
The gradient vector has several important properties. These include: 
1.	 The gradient points in the direction of maximum rate of increase in the function value 
at a given point. This can be seen by considering the directional derivative of İ‚áˆºà¢áˆ» 
along any direction à¢Š defined as:İ‚à¢Š
á‡±áˆºà¢áˆ»àµŒ×İ‚áˆºà¢áˆ»à¯à¢ŠàµŒÈ×İ‚áˆºà¢áˆ»ÈÈà¢ŠÈ Â…Â‘Â• ß  where ß  is 
the angle between the two vectors. Then, the maximum rate of increase occurs when 
ß àµŒÍ²LHDORQJ×İ‚áˆºà¢áˆ»
2.	 The magnitude of the gradient gives the maximum rate of increase in İ‚áˆºà¢áˆ». Indeed, 
Ô¡à¯—Ô¡à­€à¬µİ‚à¯—
á‡±áˆºİ”áˆ»àµŒÔ¡×İ‚áˆºà¢áˆ»Ô¡
3.	 The gradient vector at a point à¢×› is normal to the tangent hyperplane defined by İ‚áˆºà¢áˆ» 
constant. This can be shown as follows: let C be any curve in the tangent space passing 
through à¢, and let İ be a parameter along C. Then, a unit tangent vector along Ü¥ is given as: 
à°¡à¢
à°¡à¯¦àµŒáˆº
à°¡à¯«à°­
à°¡à¯¦Ç¡
à°¡à¯«à°®
à°¡à¯¦Ç¡ Ç¥ Ç¡
à°¡à¯«à³™
à°¡à¯¦áˆ» Further, we note that 
à¯—à¯™
à¯—à¯¦àµŒ
à°¡à¯™
à°¡à¢
à°¡à¢
à°¡à¯¦àµŒ×İ‚áˆºà¢áˆ»à¯à°¡à¯«
à°¡à¯¦àµŒÍ²LH×İ‚áˆºà¢áˆ» is 
normal to à°¡à¢
à°¡à¯¦.
The Hessian Matrix. The Hessian of İ‚ is a İŠàµˆİŠ matrix given by ×à¬¶İ‚áˆºà¢áˆ», where áˆ¾×à¬¶İ‚áˆºà¢áˆ»áˆ¿à¯œà¯àµŒ
à°¡à°®à¯™
à°¡à¯«à³”à°¡à¯«à³•
Note that Hessian is a symmetric matrix, since 
à°¡à°®à¯™
à°¡à¯«à³”à°¡à¯«à³•àµŒ
à°¡à°®à¯™
à°¡à¯«à³•à°¡à¯«à³”.
As an example, we consider a quadratic function: İ‚áˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ†à¢ˆà¯à¢ where Q is symmetric. Then 
its gradient and Hessian are given as: ; ×İ‚áˆºà¢áˆ»àµŒà¡½à¢×à¬¶İ‚áˆºà¢áˆ»àµŒà¡½.
Composite functions. Gradient and Hessian in the case of composite functions are computed as follows: 
Let İ‚áˆºà¢áˆ»àµŒİƒáˆºà¢áˆ»İ„áˆºà¢áˆ» be a product of two functions; then, 
×İ‚áˆºà¢áˆ»àµŒ×İƒáˆºà¢áˆ»İ„áˆºà¢áˆ»àµ…İƒáˆºà¢áˆ»×İ„áˆºà¢áˆ»
×à¬¶İ‚áˆºà¢áˆ»àµŒ×à¬¶İƒáˆºà¢áˆ»İ„áˆºà¢áˆ»àµ…İƒáˆºà¢áˆ»×à¬¶İ„áˆºà¢áˆ»àµ…×İƒáˆºà¢áˆ»×İ„áˆºà¢áˆ»à¯àµ…×İƒáˆºà¢áˆ»×İ„áˆºà¢áˆ»à¯
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
24 
Mathematical Preliminaries
For vector-valued functions, let àª¸İ‚ be a matrix defined by áˆ¾×İ‚áˆºà¢áˆ»áˆ¿à¯œà¯àµŒ
à°¡à¯™à³•áˆºà¢áˆ»
à°¡à¯«à³”WKHQàª¸İ‚áˆºà¢áˆ»à¯, then defines 
the Jacobian of f at point à¢. Further, let İ‚àµŒà¢à¯à¢, where à¢Ç¡ à¢Ç£ Ô¹à¯¡Õœ Ô¹à¯  then: 
àª¸İ‚àµŒáˆ¾àª¸à¢áˆ¿à¢àµ…áˆ¾àª¸à¢áˆ¿à¢
Taylor series expansion for multi-variable functions. Taylor series expansion in the case of a multi-
variable function is given as:
İ‚áˆºà¢à¬´àµ…à¢Šáˆ»àµŒİ‚áˆºà¢à¬´áˆ»àµ…×İ‚áˆºà¢à¬´áˆ»à¯à¢Šàµ…Í³
Í´Ç¨ à¢Šà¯×à¬¶İ‚áˆºà¢à¬´áˆ»à¢Šàµ…Ú®
where ×İ‚áˆºà¢à¬´áˆ» and ×à¬¶İ‚áˆºà¢à¬´áˆ» are, respectively, the gradient and Hessian of İ‚ computed at à¢à¬´. In particular, 
a first-order change in İ‚áˆºà¢áˆ» at à¢à¬´ along d is given as: ßœİ‚àµŒ×İ‚áˆºà¢à¬´áˆ»à¯à¢Š, where ×İ‚áˆºà¢à¬´áˆ»à¯à¢Š defines the 
directional derivative of İ‚áˆºà¢áˆ»DWà¢à¬´ at along d.
2.5	
Convex Optimization Problems
Convex optimization problems are easier to solve due to the fact that convex functions have a unique global 
minimum. As defined in Sec. 2.2 above, a function İ‚áˆºİ”áˆ» defined on a convex set is convex if and only if 
for all à¢Ç¡ à¢Ÿ× ÜµÇ¡ İ‚áˆºß™à¢àµ…áˆºÍ³ àµ†ß™áˆ»à¢Ÿáˆ»àµ‘ß™İ‚áˆºà¢áˆ»àµ…áˆºÍ³ àµ†ß™áˆ»İ‚áˆºà¢Ÿáˆ»Ç¡ ß™× áˆ¾Í²Ç¡Í³áˆ¿ In general, this condition may 
be hard to verify and other conditions based on properties of convex functions have been developed. 
Convex functions have following important properties: 
1.	 If İ‚ × ×‹à¬µ (i.e., is differentiable), then f is convex over a convex set S if and only if for all 
à¢Ç¡ à¢Ÿ× ÜµÇ¡ İ‚áˆºà¢Ÿáˆ»àµ’İ‚áˆºà¢áˆ»àµ…×İ‚áˆºà¢áˆ»à¯áˆºà¢Ÿàµ†à¢áˆ» Graphically, it means that a function is on or 
above the tangent hyperplane (line in two dimensions) passing through à¢. 
2.	 If İ‚× ×‹à¬¶ (i.e., is twice differentiable), then f is convex over a convex set Üµ if and only if for 
all à¢× ÜµÇ¡ İ‚Ô¢Ô¢áˆºà¢áˆ»àµ’Í² In the case of multivariable functions, f is convex over a convex set S 
if and only if its Hessian matrix is positive semi-definite everywhere in S, i.e., for all à¢× Üµ 
and for all à¢ŠÇ¡ à¢Šà¯àª¸à¬¶İ‚áˆºà¢áˆ»à¢Šàµ’Í² 
This can be seen by considering second order Taylor series expansion of İ‚áˆºà¢áˆ» at two points 
equidistant from a midpoint,à¢à´¥, given as: İ‚áˆºà¢à´¥àµ‡à¢Šáˆ»Ø† İ‚áˆºà¢à´¥áˆ»àµ‡×İ‚áˆºà¢à´¥áˆ»à¯à¢Šàµ…
à¬µ
à¬¶à¢Šà¯×à¬¶İ‚áˆºà¢à´¥áˆ»à¢Š
Adding these two points with ß™àµŒ
à¬µ
à¬¶ and applying the definition of convex function gives: 
İ‚áˆºà¢à´¥áˆ»àµ‘İ‚áˆºà¢à´¥áˆ»àµ…à¢Šà¯×à¬¶İ‚áˆºà¢à´¥áˆ»à¢ŠRUà¢Šà¯×à¬¶İ‚áˆºà¢à´¥áˆ»à¢Šàµ’à«™
3.	 If the Hessian is positive definite, i.e., for all à¢× Üµ and for all à¢ŠÇ¡ à¢Šà¯àª¸à¬¶İ‚áˆºà¢áˆ»à¢ŠàµÍ² then the 
function is strictly convex. This is, however, a sufficient but not necessary condition, and a 
strictly convex function may have only a positive semidefinite Hessian at some points. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
25 
Mathematical Preliminaries
4.	 If İ‚áˆºà¢×›áˆ» is a local minimum for a convex function f defined over a convex set S, then it is 
also a global minimum. This can be shown as follows: assume that İ‚áˆºà¢×›áˆ»àµŒÍ² and replace x 
with à¢×› in property one above to get: İ‚áˆºà¢áˆ»àµ’İ‚áˆºà¢×›áˆ»Ç¡ à¢× Üµ Thus, for a convex function f, 
any point à¢×› that satisfies the necessary condition: ×İ‚áˆºà¢×›áˆ»àµŒÍ² is a global minimum of f.
Due to the fact that convex functions have a unique global minimum, convexity plays an important role 
in optimization. For example, in numerical optimization convexity assures a global minimum to the 
problem. It is therefore important to first establish the convexity property when solving optimization 
problems. The following characterization of convexity applies to the solution spaces in such problems. 
Further ways of establishing convexity are discussed in (Boyd & Vandenberghe, Chaps. 2&3).
If a function İƒà¯œáˆºà¢áˆ» is convex, then the set İƒà¯œáˆºà¢áˆ»àµ‘İà¯œ
áˆ½
 is convex. Further, if functions İƒà¯œáˆºà¢áˆ»Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¡ 
are convex, then the set áˆ¼à¢Ç£İƒà¯œáˆºà¢áˆ»àµ‘İà¯œÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰áˆ½ is convex. In general, finite intersection of convex 
sets (that include hyperplanes and halfspaces) is convex. 
For general optimization problems involving inequality constraints: İƒà¯œáˆºà¢áˆ»àµ‘İà¯œÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰DQGİˆ, and 
equality constraints: İ„à¯áˆºà¢áˆ»àµŒÜ¾à¯Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¡W the feasible region for the problem is defined by the set: 
ÜµàµŒáˆ¼à¢Ç£İƒà¯œáˆºà¢áˆ»àµ‘İà¯œÇ¡ İ„à¯áˆºà¢áˆ»àµŒÜ¾à¯áˆ½ The feasible region is a convex set if the functions: İƒà¯œİ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¡ are 
convex and the functions: İ„à¯Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¡ are linear. Note that these convexity conditions are sufficient 
but not necessary.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
26 
Mathematical Preliminaries
2.6	
Vector and Matrix Norms
Norms provide a measure for the size of a vector or matrix, similar to the notion of absolute value in the 
case of real numbers. A norm of a vector or matrix is a real-valued function with the following properties:
1.	 Ô¡à¢Ô¡ àµ’Í² for all à¢
2.	 Ô¡à¢Ô¡ àµŒÍ² if and only if à¢àµŒà«™
3.	 Ô¡ß™à¢Ô¡ àµŒÈß™ÈÔ¡à¢Ô¡ for all ß™× Ô¹
4.	 Ô¡à¢àµ…à¢ŸÔ¡ àµ‘Ô¡à¢Ô¡ àµ…Ô¡à¢ŸÔ¡
Matrix norms additionally satisfy:
5.	 ||AB||â‰¤||A|| ||B||
Vector Norms. Vector p-norms are defined by Ô¡à¢Ô¡à¯£àµŒáˆºÏƒ
Èİ”à¯œÈ
à¯¡
à¯œà­€à¬µ
áˆ»
à°­
à³›Ç¡ İŒàµ’Í³ They include the 1-norm 
Ô¡à¢Ô¡à¬µàµŒÏƒ
Èİ”à¯œÈ
à¯¡
à¯œà­€à¬µ
 the Euclidean norm PÔ¡à¢Ô¡à¬¶àµŒà¶¥Ïƒ
Èİ”à¯œÈà¬¶
à¯¡
à¯œà­€à¬µ
 and the âˆ-norm ||x||âˆ = max|xi|.
Matrix Norms. Popular matrix norms are induced from vector norms, given as: ||A|| = max||Ax||
                     ||x||=1
 All 
induced norms satisfy ||Ax||â‰¤||A|| ||x||. Examples of induced matrix norms are: 
1.	 ||A||1 = max Î£n
i=1|Ai,j|
                    1â‰¤j<n
 (the largest column sum of A) 
2.	 ||A||2 = âˆšÎ»max(ATA), where denotes the maximum eigenvalue of the matrix 
3.	 ||A||âˆ = max Î£n
j=1|Ai,j|
                      1â‰¤j<n
 (the largest row sum of A)
2.7	
Matrix Eigenvalues and Singular Values
Let A be an n Ã— n matrix and assume that for some vector v and scalar Î», Av = Î»v; then Î» is an eigenvalue 
and v is an eigenvector of A. The eigenvalues of A may be solved from:det (A â€“ Î»I) = 0. The nth degree 
polynomial on the left-hand side of the equation is the characteristic polynomial of A whose roots are 
the eigenvalues of A. Let these roots be given as: Î»i,i = 1,â€¦,n then their associated eigenvectors are 
solved from: (A â€“ Î»iI) v = 0. 
A matrix with repeated eigenvalues may not have a full set of eigenvectors which, by definition, are 
linearly independent. This happens, for instance, when the nullity of (A â€“ Î»iI) is less than the degree of 
repetition of Î»i. In such cases, generalized eigenvectors may be substituted to make up the count.
Spectral Decomposition of a Symmetric Matrix. If A is symmetric, it has real eigenvalues and a full 
set of eigenvectors. Labeling them à¢œà¬µÇ¡ à¢œà¬¶Ç¡ Ç¥ Ç¡ à¢œà¯¡ it is possible to choose them to be orthonormal, such 
that à¢œà¯œ
à¯à¢œà¯œ DQGà¢œà¯œ
à¯à¢œà¯àµŒÍ²ÂˆÂ‘Â”İ…àµİ† By defining à¢‚àµŒáˆºà¢œà¬µÇ¡ à¢œà¬¶Ç¡ Ç¥ Ç¡ à¢œà¯¡áˆ»DQGàª©àµŒİ€İ…Ü½İƒáˆºß£à¬µÇ¡ ß£à¬¶Ç¡ Ç¥ Ç¡ ß£à¯¡áˆ» 
we have à¡­à¢‚àµŒàª©à¢‚RUà¡­àµŒà¢‚àª©à¢‚à¯ This is referred to as spectral decomposition of A. 
1â‰¤iâ‰¤n
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
27 
Mathematical Preliminaries
Singular Value Decomposition of a Non-square Matrix. For non-square à¡­× Ô¹à¯ àµˆà¯¡, the singular value 
decomposition (SVD) of A is given as: à¡­àµŒà¢àª±à¢‚à¯àµŒÏƒ
ßªà¯œà¢›à¯œà¢œà¯œ
à¯
à¯¥
à¯œà­€à¬µ
 whereHİàµŒUDQNáˆºà¡­áˆ»à¢× Ô¹à¯ àµˆà¯¥Ç¡ 
à¢à¯à¢àµŒà¡µà¯ àµˆà¯ Ç¢à¢‚× Ô¹à¯¡àµˆà¯¥Ç¡ à¢‚à¯à¢‚àµŒà¡µà¯¡àµˆà¯¡Ç¢ àª±àµŒÂ†Â‹ÂƒÂ‰áˆºÉà¬µÇ¡ Éà¬¶Ç¡ Ç¥ Ç¡ Éà­°áˆ» where 
Éà¬µàµ’Éà¬¶àµ’Ú® Ç¡ àµ’Éà­°are 
termed as singular values of A.
2.8	
Quadratic Function Forms
The function İ‚áˆºà¢áˆ»àµŒà¢à¯à¡½à¢àµŒÏƒ
Ïƒ
Ü³à¯œÇ¡à¯İ”à¯œİ”à¯
à¯¡
à¯à­€à¬µ
à¯¡
à¯œà­€à¬µ
 describes a quadratic form. Quadratic forms in one 
and two variables are, respectively, given as: İ‚áˆºİ”áˆ»àµŒİİ”à¬¶DQGİ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒÜ³à¬µÇ¡à¬µİ”à¬µ
à¬¶àµ…Ü³à¬¶Ç¡à¬¶İ”à¬¶
à¬¶àµ…Í´Ü³à¬µÇ¡à¬¶İ”à¬µİ”à¬¶
We note that replacing matrix Q by its symmetric counterpart 
à¬µ
à¬¶áˆºà¡½àµ…à¡½à¯áˆ» does not change İ‚áˆºà¢áˆ». 
Therefore, in a quadratic form Q can always assumed to be symmetric. 
The quadratic form is classified as:
a)	 Positive definite if à¢à¯à¡½à¢àµÍ²
b)	 Positive semidefinite if à¢à¯à¡½à¢àµ’Í²
c)	 Negative definite if I à¢à¯à¡½à¢àµÍ²
d)	 Negative semidefinite if à¢à¯à¡½à¢àµ‘Í²
e)	 Infinite otherwise
Let ß£à¯ à¯œà¯¡DQGß£à¯ à¯”à¯« and denote the minimum and maximum eigenvalues of Q; then the quadratic form 
obeys:
ß£à¯ à¯œà¯¡à¢à¯à¢àµ‘à¢à¯à¡½à¢àµ‘ß£à¯ à¯”à¯«à¢à¯à¢
Thus, positive definiteness of à¢à¯à¡½à¢ can be determined from the positivity of the eigenvalues of Q. In 
particular, let ß£à¯œÇ¡ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İŠ be the eigenvalues of Q; then Q is:
a)	 Positive definite only if ß£à¯œàµÍ²Ç¡ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İŠ 
b)	 Positive semidefinite only if I ß£à¯œàµ’Í²Ç¡ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İŠ
c)	 Negative definite only if ß£à¯œàµÍ²Ç¡ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İŠ
d)	 Negative semidefinite only if I ß£à¯œàµ‘Í²Ç¡ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İŠ
e)	 Indefinite otherwise
Geometrically, the set ÜµàµŒáˆ¼à¢Ç£à¢à¯à¡½à¢àµ‘Ü¿áˆ½ describes an ellipsoid in Ô¹à¯¡ centered at 0 with its maximum 
eccentricity given by à¶¥ß£à¯ à¯”à¯«È€ß£à¯ à¯œà¯¡ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
28 
Mathematical Preliminaries
2.9	
Linear Systems of Equations
Systems of linear equations arise in solving the linear programming problems (Chapter 5). In the 
following, we briefly discuss the existence of solutions in the case of such systems.
Consider a system of m (independent) linear equations in n unknowns described as: à¡­à¢àµŒà¢ˆ Then, from 
linear algebra, the system has a unique solution if m = n; multiple solutions if m < n; and, the system is 
over-determined (and can be solved in the least-squares sense) if m > n.
For m = n, Gaussian elimination with partial pivoting results in a matrix decomposition à¡­àµŒà¡¼à¡¸à¢Z where 
à¡¼Ç¡ à¡¼à¯à¡¼àµŒà¡µ is a permutation matrix; L is a lower triangular matrix with ones on the diagonal; and U 
is an upper triangular with eigenvalues of A on the main diagonal (Griva, Nash & Sofer, p.669). Then, 
using y, z as intermediate variables, the system can be solved in steps as: à¡¼à¢ àµŒà¢ˆÇ¡ à¡¸à¢ŸàµŒà¢ Ç¡à¢à¢àµŒà¢Ÿ If 
A is symmetric and positive definite, then Gaussian elimination results in à¡­àµŒà¡¸à¢àµŒà¡¸à¡°à¡¸à¯ where D is 
a diagonal matrix of (positive) eigenvalues of A. In this case, the solution to the linear system is given 
as: à¢àµŒà¡¸à¡°à¬¿à¬µà¡¸à¯à¢ˆ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Fundamental Engineering Optimization 
Methods
29 
Mathematical Preliminaries
Assume now that I İ‰àµİŠ and that matrix A has full row rank. Then, we can arbitrarily choose áˆºİŠàµ†İ‰áˆ» 
variables as independent (nonbasic) variables, and solve the remaining áˆºİ‰áˆ» variables as dependent 
(basic) variables. The Gauss-Jordan elimination can be used to convert the system of equations into 
its canonical form given as: à¡µáˆºà¯ áˆ»à¢áˆºà¯ áˆ»àµ…à¡½à¢áˆºà¯¡à¬¿à¯ áˆ»àµŒà¢ˆ Then, the general solution to the linear system 
includes the independent variables: à¢áˆºà¯¡à¬¿à¯ áˆ», and the dependent variables: à¢áˆºà¯ áˆ»àµŒà¢ˆàµ†à¡½à¢áˆºà¯¡à¬¿à¯ áˆ» A 
particular solution to the linear system can be obtained by setting:à¢áˆºà¯¡à¬¿à¯ áˆ»àµŒà«™ and obtaining: à¢áˆºà¯ áˆ»àµŒà¢ˆ
For non-square matrices with İ‰àµİŠ, Gram-Schmidt orthogonalization or Householder transformations 
can be applied to obtain à¡­àµŒà¡½à¡¾ZKHUHà¡½à¡½à¯àµŒà¡µDQGà¡¾, where , and is upper triangular (QR 
factorization). The original system is equivalent to Rà¡¾à¢àµŒà¡½à¯à¢ˆ which can then be solved via back-
substitution. Following are two examples of practical situations that result in linear least-squares problems 
involving over-determined systems of linear equations (İ‰àµİŠ). 
Linear Estimation Problem. Originally tackled by Carl Frederic Gauss, the linear estimation problem 
arises when estimating the state à¢ of a linear system using a set of observations denoted as y. 
Consider a linear system of equations: à¡­à¢àµŒà¢ŸÇ¡ à¡­× Ô¹à¯ àµˆà¯¡Ç¡ İ‰àµİŠ where à¢Ÿ is the observation vector. 
Let à¢˜àµŒà¡­à¢àµ†à¢Ÿ define a residual vector, and consider the unconstrained minimization problem: 
ÂÂ‹Âà¢Ô¡à¢˜Ô¡à¬¶àµŒáˆºà¡­à¢àµ†à¢Ÿáˆ»à¯áˆºà¡­à¢àµ†à¢Ÿáˆ» 
Using derivatives, the problem is solved as: à¯—
à¯—à¢áˆ¾à¢à¯à¡­à¯à¡­à¢àµ†à¢Ÿà¯à¡­à¢àµ†à¢à¯à¡­à¢Ÿàµ…à¢Ÿà¯à¢Ÿáˆ¿àµŒÍ² which leads to: 
à¡­à¯à¡­à¢àµŒà¡­à¯à¢Ÿ Thus, the solution to the least-squares problem is given as: à¢à·àµŒáˆºà¡­à¯à¡­áˆ»à¬¿à¬µà¡­à¯à¢Ÿ, where 
had denotes the estimated value of the variable. Further, let R describe the measurement covariance 
matrix: à¡¾àµŒÜ§áˆ¾à¢˜à¢˜à¯áˆ¿Ç¤ Then, the best linear estimator for x is given as: à¢à·àµŒáˆºà¡­à¯à¡¾à¬¿à¬µà¡­áˆ»à¬¿à¬µà¡­à¯à¡¾à¬¿à¬µà¢ˆ.
Data Fitting Problem. The data-fitting problem involves fitting an İŠth degree polynomial given as: 
İŒáˆºİ”áˆ»àµŒİŒà¬´àµ…İŒà¬µİ”àµ…Ú® àµ…İŒà¯¡İ”à¯¡ to a set of data points: where áˆºİ”à¯œÇ¡ İ•à¯œáˆ»Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ Ü°ZKHUHÜ°àµİŠ. 
To solve this problem, we similarly define a residual: İà¯œàµŒİ•à¯œàµ†İŒáˆºİ”à¯œáˆ»àµŒİ•à¯œàµ†áˆºİŒà¬´àµ…İŒà¬µİ”à¯œàµ…Ú® àµ…İŒà¯¡İ”à¯œ
à¯¡áˆ»
and define the following unconstrained minimization problem: minà¯£à³•Ïƒ
İà¯œ
à¬¶
à¯‡
à¯œà­€à¬µ
 where İŒà¯ represents the 
coefficients of the polynomial. Then, by defining a coefficient vector: à¢àµŒáˆ¾İŒà¬´Ç¡ İŒà¬µÇ¡ Ç¥ Ç¡ İŒà¯¡áˆ¿à¯, and an 
Ü°àµˆáˆºİŠàµ…Í³áˆ» matrix A whose rows are observation vectors of the form áˆ¾Í³Ç¡ İ”à¯œÇ¡ İ”à¯œ
à¬¶Ç¡ Ç¥ Ç¡ İ”à¯œ
à¯¡áˆ¿, we can 
solve for the coefficients using the linear least-squares framework. 
For example, in the linear case, İŒáˆºİ”áˆ»àµŒİŒà¬´àµ…İŒà¬µİ”DQGà¡­LVDÜ°àµˆÍ´ matrix whose rows are áˆ¾Í³Ç¡ İ”à¯œáˆ¿ 
vectors. The least-squares method then results in the following equations: 
á‰†Ïƒ
Í³
à¯‡
à¯œà­€à¬µ
Ïƒ
İ”à¯œ
à¯‡
à¯œà­€à¬µ
Ïƒ
İ”à¯œ
à¯‡
à¯œà­€à¬µ
Ïƒ
İ”à¯œ
à¬¶
à¯‡
à¯œà­€à¬µ
á‰‡á‰€İŒà¬´
İŒà¬µá‰àµŒá‰†Ïƒ
İ•à¯œ
à¯‡
à¯œà­€à¬µ
Ïƒ
İ”à¯œİ•à¯œ
à¯‡
à¯œà­€à¬µ
á‰‡
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
30 
Mathematical Preliminaries
Using averages: à¬µ
à¯‡Ïƒ
İ”à¯œ
à¯‡
à¯œà­€à¬µ
àµŒİ”Ò§Ç¡
à¬µ
à¯‡Ïƒ
İ•à¯œ
à¯‡
à¯œà­€à¬µ
àµŒİ•à´¤Ç¡ the solution is given as: 

İŒà¬µàµŒÏƒ
áˆºİ”à¯œàµ†İ”Ò§áˆ»áˆºİ•à¯œàµ†İ•à´¤áˆ»
à¯‡
à¯œà­€à¬µ
Ïƒ
áˆºİ”à¯œàµ†İ”Ò§áˆ»à¬¶
à¯‡
à¯œà­€à¬µ
Ç¢İŒà¬´àµŒİ•à´¤àµ†İŒà¬µİ”Ò§
Finally, the above solution can also be obtained through application of optimality conditions (Chapter 3).
2.10	
Linear Diophantine System of Equations
A Linear Diophantine system of equations (LDSE) is represented as: à¡­à¢àµŒà¢ˆÇ¡ à¢× Ôºà¯¡ The following 
algebra concepts are needed to formulate and solve problems involving a solution to LDSE.
Unimodular Matrices. Matrix à¡­× Ôºà¯¡àµˆà¯¡ is unimodular if det áˆºà¡­áˆ»àµŒàµ‡Í³Ç¤ Further, if à¡­× Ôºà¯¡àµˆà¯¡ is 
unimodular, then Qà¡­à¬¿à¬µ× Ôºà¯¡àµˆà¯¡0DWUL[à¡­× Ôºà¯¡àµˆà¯¡ is totally unimodular if every square submatrix 
[à¡¯RIà¡­Ç¡ has det áˆºà¡¯áˆ»× áˆ¼Í²Ç¡ àµ‡Í³áˆ½Ç¤
Hermite Normal Form of a Matrix. /HWà¡­× Ôºà¯ àµˆà¯¡Ç¡ İÜ½İŠİ‡áˆºà¡­áˆ»àµŒİ‰Ç¢ then, A has a unique hermite 
normal form given as: +1)áˆºà¡­áˆ»àµŒáˆ¾à¡°à«™áˆ¿ where D is lower triangular with İ€à¯œà¯àµİ€à¯œà¯œÇ¡ İ†àµİ…. Further, 
there exists a unimodular matrix U such that à¡­à¢àµŒ+1)áˆºà¡­áˆ»Ç¡ where we note that post-multiplication by 
a unimodular matrix involves performing elementary column operations. Moreover, let à¢›à¬µÇ¡ à¢›à¬¶Ç¡ Ç¥ Ç¡ à¢›à¯¡ 
represent the U columns of then Qáˆ¼à¢›à¯ à¬¾à¬µÇ¡ Ç¥ Ç¡ à¢›à¯¡áˆ½ form a basis for ker (A).
Solution to the LDSE. Assume that à¡­× Ôºà¯ àµˆà¯¡İÜ½İŠİ‡áˆºà¡­áˆ»àµŒİ‰Ç¡, and let à¡­à¢àµŒ+1)áˆºà¡­áˆ»Ç¢ then, we 
may consider: à¢ˆàµŒà¡­à¢àµŒà¡­à¢à¢à¬¿à¬µà¢àµŒà¡´à¡ºà¡²áˆºà¡­áˆ»à¢ŸÇ¡ à¢ŸàµŒà¢à¬¿à¬µà¢Ç¤ Assume that we have a solution à¢Ÿà¬´ to: 
+1)áˆºà¡­áˆ»à¢Ÿà¬´àµŒà¢ˆÇ¢ then, the general solution to the LDSE is given as:à¢àµŒà¢à¬´àµ…Ïƒ
ß™à¯œà¢à¯œ
à¯¡à¬¿à¯ 
à¯œà­€à¬µ
Ç¡ where 
à¢à¬´àµŒà¢à¢Ÿà¬´Ç¡ à¢à¯œ× áˆ¼à¢›à¯ à¬¾à¬µÇ¡ Ç¥ Ç¡ à¢›à¯¡áˆ½ 
2.11	
Condition Number and Convergence Rates
The condition number of a matrix is defined as: cond áˆºà¡­áˆ»àµŒÔ¡à¡­Ô¡ Î® Ô¡à¡­à¬¿à¬µÔ¡ Note that and cond İ€áˆºà¡­áˆ»àµ’Í³
and cond áˆºà¡µáˆ»àµŒÍ³Ç¡ where I is an identity matrix. If A is symmetric with real eigenvalues, and 2-norm is 
used, then áˆºà¡­áˆ»àµŒß£à¯ à¯”à¯«áˆºà¡­áˆ»È€ß£à¯ à¯œà¯¡áˆºà¡­áˆ»
The condition number of the Hessian matrix affects the convergence rates of the optimization algorithms. 
Ill-conditioned matrices give rise to numerical errors in computations. In certain cases, it is possible 
to improve the condition number by scaling the variables. The convergence property implies that the 
generated sequence converges to the true solution in the limit. The rate of convergence dictates how 
quickly the approximate solutions approach the exact solution. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
31 
Mathematical Preliminaries
Assume that a sequence of points áˆ¼İ”à¯áˆ½ converges to a solution point İ”×› and define an error sequence: 
İà¯àµŒİ”à¯àµ†İ”×› Then, we say that the sequence áˆ¼İ”à¯áˆ½ converges to İ”×› with rate and rate constant Ü¥ 
if ÂÂ‹Âà¯Õœà®¶
Ô¡à¯˜à³–à°¶à°­Ô¡
Ô¡à¯˜à³–Ô¡à³àµŒÜ¥ Further, if uniform convergence is assumed, then Ô¡İà¯à¬¾à¬µÔ¡ àµŒÜ¥Ô¡İà¯Ô¡à¯¥ holds for 
all İ‡ Thus, convergence to the limit point is faster if İ is larger and Ü¥ is smaller. Specific cases for 
different choices of İ and Ü¥ are mentioned below. 
Linear convergence. For İàµŒÍ³DQGÍ² àµÜ¥àµÍ³Ô¡İà¯à¬¾à¬µÔ¡ àµŒÜ¥Ô¡İà¯Ô¡ signifying linear convergence. In 
this case the speed of convergence depends only on Ü¥, which can be estimated as Ü¥àµ
à¯™àµ«à¯«à³–à°¶à°­àµ¯à¬¿à¯™áˆºà¯«×›áˆ»
à¯™àµ«à¯«à³–àµ¯à¬¿à¯™áˆºà¯«×›áˆ». 
Quadratic Convergence. For r = 2, the convergence is quadratic, i.e., Ô¡İà¯à¬¾à¬µÔ¡ àµŒÜ¥Ô¡İà¯Ô¡à¬¶ In this case 
if additionally C = 1, then the number of correct digits in double at every iteration.
Superlinear Convergence. For Í³ àµİàµÍ´, the convergence is superlinear. Superlinear convergence is 
achieved by numerical algorithms that only use the gradient (first derivative) of the cost function, and 
thus can qualitatively match quadratic convergence. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Fundamental Engineering Optimization 
Methods
32 
Mathematical Preliminaries
2.12	
 Conjugate-Gradient Method for Linear Equations
The conjugate-gradient method is an iterative method designed to solve a system of linear equations 
described as: à¡­à¢àµŒà¢ˆÇ¡ where A is assumed normal, i.e., 
J
à¡­à¯à¡­àµŒà¡­à¡­à¯ The method initializes with à¢à¬´àµŒà«™Ç¡ 
and uses an iterative process to obtain an approximate solution à¢à¯¡ in İŠ iterations. The solution is exact 
in the case of quadratic functions of the form: İáˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡­à¢àµ†à¢ˆà¯à¢) For general nonlinear functions, 
convergence in 2İŠ iterations is to be expected. The method is named so because à¡­à¢àµ†à¢ˆ represents 
the gradient of the quadratic function. Solving a linear system of equations thus amounts to solving the 
minimization problem involving a quadratic function. 
The conjugate-gradient method generates a set of vectors à¢œà¬µÇ¡ à¢œà¬¶Ç¡ Ç¥ Ç¡ à¢œà¯¡ that are conjugate with respect 
to A matrix, i.e., à¢œà¯œ
à¯à¡­à¢œà¯àµŒÍ²Ç¡ İ…àµİ†/HWà¢œà¬¿à¬µàµŒà«™Ç¡ ßšà¬´àµŒÍ² and define a residual à¢˜à¯œàµŒà¢ˆàµ†à¡­à¢à¯œ Then, a 
set of conjugate vectors is iteratively generated as:
à¢œà¯œàµŒà¢˜à¯œàµ…ßšà¯œà¢œà¯œà¬¿à¬µÇ¡ßšà¯œàµŒ
à¢œà³”
à³…à¡­à¢˜à³”
à¢œà³”
à³…à¡­à¢œà³”
We note that the set of conjugate vectors of a matrix is not unique. Further, nonzero conjugate vectors 
with respect to a positive-definite matrix are linearly independent. 
In conjugate-gradient and other iterative methods, scaling of variables, termed as preconditioning, helps 
reduce the condition number of the coefficient matrix, which aids in fast convergence of the algorithm. 
Towards that end, we consider a linear system of equations: à¡­à¢àµŒà¢ˆ and use a linear transformation to 
formulate an equivalent system that is easier to solve. Let P be any nonsingular İŠàµˆİŠ matrix, then an 
equivalent left-preconditioned system is formulated as: à¡¼à¬¿à¬µà¡­à¢àµŒà¡¼à¬¿à¬µà¢ˆ, and a right-preconditioned 
system is given as: à¡­à¡¼à¬¿à¬µà¡¼à¢àµŒà¢ˆ. As the operator à¡¼à¬¿à¬µ is applied at each step of the iterative solution, 
it helps to choose a simple à¡¼à¬¿à¬µ with a small computational cost. An example of a simple preconditioner 
is the Jacobi preconditioner: à¡¼àµŒİ€İ…Ü½İƒáˆºà¡­áˆ» 
Further, if A is symmetric and positive-definite, then à¡¼à¬¿à¬µ should be chosen likewise. If both à¡¼à¬¿à¬µ and à¡­
are positive-definite, then we can use the Cholesky decomposition of à¡¼à¡¼àµŒà¡¯à¯à¡¯, to write à¡¯à¬¿à¬µà¡¯à¬¿à¯à¡­à¢àµŒ
S

à¡¯à¬¿à¬µà¡¯à¬¿à¯à¢ˆRUà¡¯à¬¿à¯à¡­à¡¯à¬¿à¬µà¢àµŒà¡¯à¬¿à¯à¢ˆ Then, by defining à¡¯à¬¿à¯à¡­à¡¯à¬¿à¬µàµŒà¡­à·¡à¡¯à¬¿à¯à¢ˆàµŒà¢ˆà·¡ we obtain à¡­à·¡à¢àµŒà¢ˆà·¡ 
where à¡­à·¡ is positive-definite.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
33 
Mathematical Preliminaries
2.13	
Newtonâ€™s Method for Nonlinear Equations
Newtonâ€™s method, also known as Newton-Raphson method, iteratively solves a nonlinear equation: 
İ‚áˆºİ”áˆ»àµŒÍ²Ç¡starting from an initial point İ”à¬´ The method generates a series of solutions áˆ¼İ”à¯áˆ½ that are 
expected to converge to a fixed point İ”×› that represents a root of the equation. To develop the method, we 
assume that an estimate of the solution is available as İ”à¯Ç¡ and use first order Taylor series to approximate 
İ‚áˆºİ”áˆ» around İ”à¯Ç¡ i.e., let
İ‚áˆºİ”à¯àµ…ßœİ”áˆ»àµŒİ‚áˆºİ”à¯áˆ»àµ…İ‚á‡±áˆºİ”à¯áˆ»ßœİ”
Then, by setting İ‚áˆºİ”à¯àµ…ßœİ”áˆ»àµŒÍ² we can solve for the offset ßœİ” and use it to update our estimate İ”à¯Ç¡ as: 
İ”à¯à¬¾à¬µàµŒİ”à¯àµ†İ‚áˆºİ”à¯áˆ»È€İ‚á‡±áˆºİ”à¯áˆ»
Next, Newtonâ€™s method can be extended to a system of nonlinear equations, given as: 
İ‚à¬µáˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ Ç¥ Ç¡ İ”à¯¡áˆ»àµŒÍ²
İ‚à¬¶áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ Ç¥ Ç¡ İ”à¯¡áˆ»àµŒÍ²
Ú­
İ‚à¯¡áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ Ç¥ Ç¡ İ”à¯¡áˆ»àµŒÍ²
Let a gradient matrix ×İ‚áˆºà¢áˆ» be formed with columns: ×İ‚à¬µáˆºà¢áˆ»Ç¡ ×İ‚à¬¶áˆºà¢áˆ»Ç¡ Ç¥ Ç¡ ×İ‚à¯¡áˆºà¢áˆ»Ç¢ then, the transpose 
of the gradient matrix defines the Jacobian matrix given as: Ü¬áˆºà¢áˆ»àµŒ×İ‚áˆºà¢áˆ»à¯ Using the Jacobian matrix, 
the update rule in the İŠ-dimensional case is given as:
à¢à¯à¬¾à¬µàµŒà¢à¯àµ†àµ«Ü¬áˆºà¢à¯áˆ»àµ¯
à¬¿à¬µİ‚áˆºà¢à¯áˆ»
Convergence Rate. We first note that Newtonâ€™s method requires a good initial guess for it to converge. 
Newtonâ€™s method, if it converges, exhibits quadratic rate of convergence near the solution point. The 
method can become unstable if İ‚áˆºİ”×›áˆ»àµÍ²Ç¤ Assuming İ‚á‡±áˆºİ”×›áˆ»àµÍ² and İ”à¯Ç¡ is sufficiently close to İ”×›, 
we can use second order Taylor series to write:
İ”à¯à¬¾à¬µàµ†İ”×› àµÍ³
Í´ á‰†İ‚á‡±á‡±áˆºİ”×›áˆ»
İ‚á‡±áˆºİ”×›áˆ»á‰‡áˆºİ”à¯àµ†İ”×›áˆ»à¬¶
which shows that Newtonâ€™s method has quadratic convergence with a rate constant: Ü¥àµŒ
à¬µ
à¬¶á‰š
à¯™á‡²á‡²áˆºà¯«×›áˆ»
à¯™á‡²áˆºà¯«×›áˆ»á‰š 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
34 
Graphical Optimization
3	 Graphical Optimization
We briefly discuss the graphical optimization concepts in this chapter before proceeding to formal 
mathematical optimization method in Chapter 4 and computational methods in Chapter 7. Graphical 
approach is recommended for problems of low dimensions, typically those involving one or two variables. 
Apart from being simple, the graphical method provides a valuable insight into the problem, which may 
not be forthcoming in the case of mathematical and computational optimization methods, particularly 
in the case of two-dimensional problems. 
The graphical method is applicable when the optimization problem is formulated with one or two 
variables. Graphical optimization helps enhance our understanding of the underlying problem and 
develop an appeal for the expected solution. The method involves plotting contours of the cost function 
over a feasible region enclosed by the constraint boundaries. In most cases, the desired optimum can 
be spotted by inspection. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
35 
Graphical Optimization
Software implementation of the graphical method uses a grid of paired values for the optimization 
variables to plot the objective function contours and the constraint boundaries. The minimum of the 
cost function can then be identified on the plot. Graphical minimization procedure thus involves the 
following steps:
1.	 Establishing the feasible region. This is done by plotting the constraint boundaries.
2.	 Plotting the level curves (or contours) of the cost function and identifying the minimum.
The graphical method is normally implemented in a computational software package such as Matlab Â© 
and Mathematica Â©. Both packages include functions that aid the plotting and visualization of cost 
function contours and constraint boundaries. Code for Matlab implementation of graphical optimization 
examples considered in this chapter is provided in the Appedix. 
Learning Objectives: The learning goals in this chapter are:
1.	 Recognize the usefulness and applicability of the graphical method.
2.	 Learn how to apply graphical optimization techniques to problems of low dimensions.
3.1	
Functional Minimization in One-Dimension 
Graphical function minimization in one-dimension is performed by computing and plotting the function 
values at a set of discrete points and identifying its minimum value on the plot. We assume that the 
feasible region for the problem is a closed interval: ÜµàµŒáˆ¾İ”à¯ŸÇ¡ İ”à¯¨áˆ¿Ç¢ then, the procedure can be summarized 
as follows:
1.	 Define a grid over the feasible region: let Wİ”àµŒİ”à¯Ÿàµ…İ‡ßœÇ¡ İ‡àµŒÍ²Ç¡Í³Ç¡Í´Ç¡ Ç¥Zwhere ßœ defines the 
granularity of the grid. 
2.	 Compute and compare the function values over the grid points to find the minimum.
An illustrative example for one-dimensional minimization is provided below.
Example 3.1: Graphical function minimization in one-dimension
Let the problem be defined as: Minimize İà¯« subject to İ”à¬¶àµ‘Í³ Then, to find a solution, we define a 
grid over the feasible region as follows: OHWàµŒÍ²Ç¤Í²Í³Ç¡İ”àµŒàµ†Í³Ç¡ àµ†Í²Ç¤Í»Í»Ç¡ Ç¥ Ç¡ àµ†Í²Ç¤Í²Í³Ç¡Í²Ç¡Í²Ç¤Í²Í³Ç¡Ç¥ Ç¡Í²Ç¤Í»Í»Ç¡Í³ Then, 
İ‚áˆºİ”áˆ»àµŒİà¬¿à¬µÇ¡ İà¬¿à¬´Ç¤à¬½à¬½Ç¡ Ç¥ Ç¡ İà¬¿à¬´Ç¤à¬´à¬µÇ¡ Í³Ç¡ İà¬´Ç¤à¬´à¬µÇ¡ Ç¥ Ç¡ İà¬´Ç¤à¬½à¬½Ç¡ İà¬µ By comparison, İ‚à¯ à¯œà¯¡àµŒİà¬¿à¬µDWİ”àµŒàµ†Í³Ç¤
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
36 
Graphical Optimization
3.2	
Graphical Optimization in Two-Dimensions
Graphical optimization is most useful for optimization problems involving functions of two variables. 
Graphical function minimization in two-dimensions is performed by plotting the contours of the 
objective function along with the constraint boundaries on a two-dimensional grid. In Matlab Â©, the 
grid points can be generated with the help of â€˜meshgridâ€™ function. Mathematica Â© also provide similar 
capabilities.
In the following we discuss three examples of applying graphical method in engineering design 
optimization problems where each problem contains two optimization variables. 
Example 3.2: Hollow cylindrical cantilever beam design (Arora, p. 85)
We consider the minimum-weight design of a cantilever beam of length L, with hollow circular cross-
section (outer radius Ü´à¯¢ inner radius Ü´à¯œ) subjected to a point load P. The maximum bending moment 
on the beam is given as PL, the maximum bending stress is given as:ßªà¯”àµŒ
à¯‰à¯…à¯‹à³š
à¯‚Ç¡ and the maximum shear 
stress is given as:ß¬àµŒ
à¯‰
à¬·à¯‚àµ«Ü´à¯¢à¬¶àµ…Ü´à¬´Ü´à¯œàµ…Ü´à¯œ
à¬¶àµ¯Ç¡ZKHUHÜ«àµŒ
à°—
à¬¸áˆºÜ´à¯¢à¬¸àµ†Ü´à¯œ
à¬¸áˆ» is the moment of inertia of the 
cross-section. The maximum allowable bending and shear stresses are given as ßªà¯” and ß¬à¯”, respectively. 
Let the design variables be selected as the outer radius Ü´à¯¢ and the inner radius Ü´à¯œ; then, the optimization 
problem is stated as follows: 
Minimize İ‚áˆºÜ´à¬´Ç¡ Ü´à¯œáˆ»àµŒß¨ß©Ü®áˆºÜ´à¬´
à¬¶àµ†Ü´à¯œ
à¬¶áˆ»
Subject to: 
à°™
à°™à³Œàµ†Í³ àµ‘Í²Ç¡
à°›
à°›à³Œàµ†Í³ àµ‘Í²Ç¢Ü´à¬´Ç¡ Ü´à¯œàµ‘Í²Ç¤Í´İ‰
The following data are provided for the problem: Ü²àµŒÍ³Í²İ‡Ü°Ç¡ Ü®àµŒÍ·İ‰Ç¡ ßªà¯”àµŒÍ´Í·Í²Ü¯Ü²Ü½Ç¡ ß¬à¯”àµŒÍ»Í²Ü¯Ü²Ü½Ç¡  
Ü§àµŒÍ´Í³Í²Ü©Ü²Ü½Ç¡ ß©àµŒÍ¹ÍºÍ·Í²İ‡İƒÈ€İ‰à¬· After substituting the values, and dropping the constant terms in f,
 the optimization problem is stated as: 
Minimize İ‚áˆºÜ´à¬´Ç¡ Ü´à¯œáˆ»àµŒÜ´à¬´
à¬¶àµ†Ü´à¯œ
à¬¶
à°°
Subject to: İƒÍ³Ç£
à¬¼àµˆà¬µà¬´à°·à°°à¯‹à³š
à°—áˆºà¯‹à°¬
à°°à¬¿à¯‹à³”
à°°áˆ»àµ†Í³ àµ‘Í²Ç¢ İƒÍ´Ç£
à¬¸àµ«à¯‹à³šà°®à¬¾à¯‹à°¬à¯‹à³”à¬¾à¯‹à³”
à°®àµ¯
à¬¶à¬»à°—àµ«à¯‹à°¬
à°°à¬¿à¯‹à³”
à°°àµ¯
àµ†Í³ àµ‘Í²Ç¢Ü´à¬´Ç¡ Ü´à¯œàµ‘Í´Í²Ü¿İ‰
The graphical solution to the problem, obtained from Matlab, is shown in Figure 3.1. The optimal solution 
is given as: Ü´à¯¢àµŒÍ²Ç¤Í³Í´İ‰Ç¡ Ü´à¯œàµŒÍ²Ç¤Í³Í³Í·İ‰Ç¡ İ‚×› àµŒÍ²Ç¤Í²Í²Í³Í³Í¹Í·
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
37 
Graphical Optimization
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.001
0.001
0.001
0.001
0.002
0.002
0.002
0.002
0.003
0.003
0.003
0.004
0.004
0.004
0.005
0.005
0.005
0.006
0.006
0.006
0.007
0.007
0.007
0.008
0.008
0.008
0.009
0.009
0.009
0.01
0.01
0.01
X= 0.12
Y= 0.115
Level= 0.001175
Ro
Ri
Hollow Cylindrical Cantilever Beam Design
Figure 3.1: Graphical solution to the minimum`-weight hollow cantilever beam design (Example 3.2)
Example 3.3: Symmetrical two-bar truss design (Arora, p. 59)
We wish to design a symmetrical two-bar truss to withstand a load Ü¹àµŒÍ·Í²İ‡Ü° The truss consists of 
two steel tubes pinned together at the top and supported on the ground at the other (figure). The truss 
has a fixed span İàµŒÍ´İ‰ and a height İ„àµŒÎ¾İˆà¬¶àµ†Í³ where l is the length of the tubes; both tubes have 
a cross-sectional area: Ü£àµŒÍ´ß¨Ü´İ where R is the radius of the tube and t is the thickness. The objective 
is to design a minimum-weight structure, where total weight is Í´ß©İˆÜ£ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Fundamental Engineering Optimization 
Methods
38 
Graphical Optimization
The truss design is subject to the following constraints: 
1.	 The height of the truss is to be limited as: Í´ àµ‘İ„àµ‘Í·Ç¢
2.	 The tube thickness is to be limited as: Ü´àµ‘Í¶Í·İÇ¢
3.	 The maximum allowable stress is given as: ßªà¯”àµŒÍ´Í·Í²Ü¯Ü²Ü½Ç¢
4.	 To prevent buckling, tube loading should not exceed a critical value: à¯à¯Ÿ
à¬¶à¯›àµ‘
à¯‰à³à³
à®¿à¯ŒàµŒ
à¬µ
à®¿à¯Œ
à°—à°®à®¾à¯‚
áˆºà¯„à¯Ÿáˆ»à°® 
where Ü­àµŒÍ²Ç¤Í¹Ç¡ Ü§àµŒÍ´Í³Í²Ü©Ü²Ü½Ç¡ the moment of inertia: Ü«Ø† ß¨Ü´à¬·İÇ¡ and Ü¨ÜµàµŒÍ´ denotes a 
safety factor.
Let the design variables be selected as: İ„Ç¡ Ü´Ç¡ İÇ¢ then, the optimization problem is formulated as:
Minimize İ‚áˆºİ„Ç¡ Ü´Ç¡ İáˆ»àµŒÍ¶ß¨ß©Î¾İ„à¬¶àµ…Í³Ü´İ
Subject to: İƒÍ³Ç£
à¯Î¾à¯›à°®à¬¾à¬µ
à¬¸à°—à¯›à¯‹à¯§à°™à³Œàµ†Í³ àµ‘Í²Ç¡İƒÍ´Ç£
à¬´Ç¤à¬¸à¬½à¯àµ«à¯›à°®à¬¾à¬µàµ¯
à°¯
à°®Ç¤
à°—à°¯à®¾à¯›à¯‹à°¯à¯§
àµ†Í³ àµ‘Í²Ç¡ İƒÍµÇ£Ü´àµ†Í¶Í·İàµ‘Í²Ç¡ İƒÍ¶Ç£Í´ àµ‘İ„àµ‘Í·
In the above formulation, there are three design variables: İ„Ç¡ Ü´Ç¡ İ. Consequently, we need to fix the value 
of one variable in order to perform the graphical design with two variables. We arbitrarily fix İ„àµŒÍµİ‰ 
and graphically solve the resulting minimization problem stated, after dropping the constant terms in 
f , as follows:
Minimize İ‚áˆºÜ´Ç¡ İáˆ»àµŒÜ´İ
Subject to: İƒÍ³Ç£
à¬µÇ¤à¬ºà¬»à¬»àµˆà¬µà¬´à°·à°±
à¯‹à¯§
àµ†Í³ àµ‘Í²Ç¡İƒÍ´Ç£
à¬·Ç¤à¬½à¬ºà¬ºàµˆà¬µà¬´à°·à°´
à¯‹à°¯à¯§
àµ†Í³ àµ‘Í²Ç¡ İƒÍµÇ£Ü´àµ†Í¶Í·İàµ‘Í²
A graph of the objective function and the constraints for the problem is shown in the Figure 3.2. From 
the figure, the optimum values of the design variables are: Ü´àµŒÍµÇ¤Í¹Ü¿İ‰Ç¡ İàµŒÍ²Ç¤Íºİ‰İ‰Ç¡ İ‚×› àµŒÍµ àµˆÍ³Í²à¬¿à¬¹.
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
x 10
-3
5e-005
5e-005
5e-005
0.0001
0.0001
0.0001
0.00015
0.00015
0.0002
X= 0.037
Y= 0.0008
Level= 0.001
Two bar truss design
R(m)
t(m)
1e-005
1e-005
1e-005
1e-005
1e-005
2e-005
2e-005
2e-005
2e-005
3e-005
3e-005
3e-005
3e-005
4e-005
4e-005
4e-005
4e-005
Figure 3.2: Graphical solution to the minimum-weight symmetrical two-bar truss design (Example 3.3)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
39 
Graphical Optimization
Example 3.4: Symmetrical three-bar truss design (Arora, p. 46, 86)
We consider the minimum-weight design of a symmetric three-bar truss supported over-head. Members 
1 and 3 have the same cross-sectional area Ü£à¬µ and the middle member 2 has cross-sectional area Ü£à¬¶
Let l be the height of the truss, then the lengths of member 1 and 3 are Î¾Í´İˆ and that of member 2 is l. 
A load P at the joint is applied at an angle ß  so that the horizontal and vertical components of the applied 
load are given, respectively, as: Ü²à¯¨àµŒÜ²Â…Â‘Â• ß Ç¡ Ü²à¯©àµŒÜ²Â•Â‹Â ß  The design variables for the problem are 
selected as Ü£à¬µ and Ü£à¬¶ The design objective is to minimize the total massàµŒß©İˆáˆºÍ´Î¾Í´Ü£à¬µàµ…Ü£à¬¶áˆ» 
The constraints in the problem are formulated as follows: 
a)	 The stresses in members 1, 2 and 3, computed as: 
ßªà¬µàµŒ
à¬µ
Î¾à¬¶á‰‚
à¯‰à³ 
à®ºà°­àµ…
à¯‰à³¡
áˆºà®ºà°­à¬¾Î¾à¬¶à®ºà°®áˆ»á‰ƒÇ¢ßªà¬¶àµŒ
Î¾à¬¶à¯‰à³¡
áˆºà®ºà°­à¬¾Î¾à¬¶à®ºà°®áˆ»Ç¢ßªà¬·àµŒ 
à¬µ
Î¾à¬¶á‰‚àµ†
à¯‰à³ 
à®ºà°­àµ…
à¯‰à³¡
áˆºà®ºà°­à¬¾Î¾à¬¶à®ºà°®áˆ»á‰ƒ, are to be limited 
by the allowable stress for the material. 
b)	 The axial force in members under compression, given as: Ü¨à¯œàµŒßªà¯œÜ£à¯œ, is limited by the 
buckling load, LHàµ†Ü¨à¯œàµ‘
à°—à°®à®¾à¯‚
à¯Ÿà³”
à°®RUàµ†ßªà¯œàµ‘
à°—à°®à®¾à°‰à®ºà³”
à¯Ÿà³”
à°®
àµ‘ßªà¯”Ç¡ or where the moment of inertia is 
estimated as: Ü«à¯œàµŒßšÜ£à¯œ
à¬¶ ßšàµŒ constant. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Fundamental Engineering Optimization 
Methods
40 
Graphical Optimization
c)	 The horizontal and vertical deflections of the load point, given as: 
İ‘àµŒÎ¾à¬¶à¯Ÿà¯‰à³ 
à®ºà°­à®¾Ç¡ İ’àµŒ
Î¾à¬¶à¯Ÿà¯‰à³¡
áˆºà®ºà°­à¬¾Î¾à¬¶à®ºà°®áˆ»à®¾Ç¡ are to be limited by İ‘àµ‘Î¿à¯¨Ç¡ İ’àµ‘Î¿à¯©
d)	 To avoid possible resonance, the lowest eigenvalue of the structure, given as: 
ßàµŒ
à¬·à®¾à®ºà°­
à°˜à¯Ÿà°®áˆºà¬¸à®ºà°­à¬¾Î¾à¬¶à®ºà°®áˆ» where ß© is the mass density should be higher than a specified frequency, 
i.e., ßàµ’áˆºÍ´ß¨ß±à¬´áˆ»à¬¶
e)	 The design variables are required to be greater than some minimum value, i.e.,
Ü£à¬µÇ¡ Ü£à¬¶àµ’Ü£à¯ à¯œà¯¡ 
For a particular problem, let İˆàµŒÍ³Ç¤Í²İ‰Ç¡ Ü²àµŒÍ³Í²Í²İ‡Ü°Ç¡ ß àµŒÍµÍ²Î¹Ç¡ ß©àµŒÍ´ÍºÍ²Í²
à¯à¯š
à¯ à°¯Ç¡ Ü§àµŒÍ¹Í²Ü©Ü²Ü½Ç¡ ßªà¯”àµŒ
Í³Í¶Í²Ü¯Ü²Ü½Ç¡ Î¿à¯¨àµŒÎ¿à¯©àµŒÍ²Ç¤Í·Ü¿İ‰Ç¡ ß±à¬´àµŒÍ·Í²Üªİ–Ç¡ ßšàµŒÍ³Ç¤Í²Ç¡ÂƒÂÂ†Ü£à¯ à¯œà¯¡àµŒÍ´Ü¿İ‰à¬¶7KHQÜ²à¯¨àµŒÎ¾à¬·à¯‰
à¬¶Ç¡ Ü²à¯©àµŒ
à¯‰
à¬¶Ç¢D 
and Then, and the resulting optimal design problem is formulated as:
Minimize İ‚áˆºÜ£à¬µÇ¡ Ü£à¬¶áˆ»àµŒÍ´Î¾Í´Ü£à¬µàµ…Ü£à¬¶
Subject to: 
İƒÍ³Ç£Í´Ç¤Í· àµˆÍ³Í²à¬¿à¬¸á‰ˆÎ¾Íµ
Ü£à¬µ
àµ…
Í³
àµ«Ü£à¬µàµ…Î¾Í´Ü£à¬¶àµ¯
á‰‰àµ†Í³ àµ‘Í²Ç¡
İƒÍ´Ç£Í´Ç¤Í· àµˆÍ³Í²à¬¿à¬¸á‰ˆàµ†Î¾Íµ
Ü£à¬µ
àµ…
Í³
àµ«Ü£à¬µàµ…Î¾Í´Ü£à¬¶àµ¯
á‰‰àµ†Í³ àµ‘Í²Ç¡
İƒÍµÇ£
Í· àµˆÍ³Í²à¬¿à¬¸
àµ«Ü£à¬µàµ…Î¾Í´Ü£à¬¶àµ¯
àµ†Í³ àµ‘Í²Ç¡
İƒÍ¶Ç£Í³Ç¤Í²Í´ àµˆÍ³Í²à¬¿à¬»á‰ˆÎ¾Íµ
Ü£à¬µ
à¬¶àµ†
Í³
Ü£à¬µàµ«Ü£à¬µàµ…Î¾Í´Ü£à¬¶àµ¯
á‰‰àµ†Í³ àµ‘Í²Ç¡
İƒÍ·Ç£ ÍµÇ¤Í· àµˆÍ³Í²à¬¿à¬¸
Ü£à¬µ
àµ†Í³ àµ‘Í²Ç¡
İƒÍ¸Ç£ Í´ àµˆÍ³Í²à¬¿à¬¸
Ü£à¬µàµ…Î¾Í´Ü£à¬¶
àµ†Í³ àµ‘Í²Ç¡
İƒÍ¹Ç£ Í´ àµˆÍ³Í²à¬¿à¬¸
Ü£à¬µ
àµ†Í³ àµ‘Í²Ç¡
İƒÍºÇ£ Í´ àµˆÍ³Í²à¬¿à¬¸
Ü£à¬¶
àµ†Í³ àµ‘Í²Ç¡
İƒÍ»Ç£Í³Ç¤ÍµÍ³Í¸ àµˆÍ³Í²à¬¿à¬¹àµ«Í¶Ü£à¬µàµ…Î¾Í´Ü£à¬¶àµ¯àµ†Í³ àµ‘Í²
İƒÍ³Í²Ç£ Í´Í¶Í¸Í¹Ü£à¬µàµ†Í³ àµ‘Í²
The problem was graphically solved in Matlab (see Figure 3.3). The optimum solution is given as: 
Ü£à¬µàµŒÜ£à¬·àµŒÍ¸Ü¿İ‰à¬¶Ç¡ Ü£à¬¶àµŒÍ´Ü¿İ‰à¬¶Ç¡ İ‚×› àµŒÍ²Ç¤Í²Í²Í¶ÍºÍ¸Ü¿İ‰à¬¶
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
41 
Graphical Optimization
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x 10
-3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x 10
-3
0.0005
0.001
0.001
0.001
0.0015
0.0015
0.0015
0.002
0.002
0.002
0.0025
0.0025
0.0025
0.003
0.003
0.0035
X= 0.0006
Y= 0.0002
Level= 0.004864
A1
A2
Figure 3.3: Graphical solution to the minimum-weight symmetrical three-bar truss design (Example 3.4)
Appendix to Chapter 3: Matlab Code for Examples 3.2â€“3.4 
Example 3.2: Cantilever beam design (Arora, Prob. 2.23, p. 64)
% cantilever beam design, Prob. 2.23 (Arora)
ro=.01:.005:.2;
ri=.01:.005:.2;
[Ro,Ri]=meshgrid(ro,ri);
F=Ro.*Ro-Ri.*Ri;
G1=8e-4/pi*Ro./(Ro.^4-Ri.^4)-1;
G2=4/27/pi*(Ro.*Ro+Ro.*Ri+Ri.*Ri)./(Ro.^4-Ri.^4)-1;
figure, hold
contour(ro,ri,G1,[0 0])
contour(ro,ri,G2,[0 0])
[c,h]=contour(ro,ri, F, .001:.001:.01);
clabel(c,h);
Example 3.3: Two-bar truss design (Arora, Prob. 2.16, p. 61)
% two-bar trus; prob. 2.16 (Arora)
W=50e3;
r=0:.001:.05; 
t=0:.0001:.005;
[R,T]=meshgrid(r,t);
F=R.*T;
G1=sqrt(10)*W/12/pi./(250e6*R.*T)-1; 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
42 
Graphical Optimization
G2=4.9*sqrt(10)*W/3/pi/pi/pi./(210e9*R.^3.*T)-1;
G3=R-45*T;
figure, hold
contour(r,t,G1,[0 0]), pause
contour(r,t,G2,[0 0]), pause
contour(r,t,G3,[0 0]), pause
[c,h]=contour(r,t,F);
clabel(c,h)
Example 3.4: Symmetric three-bar truss (Arora, Prob. 3.29, p. 86)
%three-bar truss prob. 3.29 (Arora)
a1=0:1e-4:1e-3;
a2=0:1e-4:1e-3;
[A1,A2]=meshgrid(a1,a2);
F=2*sqrt(2)*A1+A2;
G1=2.5e-4*(sqrt(3)./A1+1./(A1+sqrt(2)*A2))-1;
G2=2.5e-4*(-sqrt(3)./A1+1./(A1+sqrt(2)*A2))-1;
G3=5e-4./(A1+sqrt(2)*A2)-1;
G4=1.02e-7*(sqrt(3)./A1./A1-1./A1./(A1+sqrt(2)*A2))-1;
G5=3.5e-4./A1-1;
G6=2e-4./(A1+sqrt(2)*A2)-1;
G7=2e-4./A1-1;
G8=2e-4./A2-1;
G9=1.316e-5*(A1+sqrt(2)*A2)-1;
G10=2467*A1-1;
figure, hold
contour(a1,a2, G1,[0 0]), pause
contour(a1,a2, G2,[0 0]), pause
contour(a1,a2, G3,[0 0]), pause
contour(a1,a2, G4,[0 0]), pause
contour(a1,a2, G5,[0 0]), pause
contour(a1,a2, G6,[0 0]), pause
contour(a1,a2, G7,[0 0]), pause
contour(a1,a2, G8,[0 0]), pause
contour(a1,a2, G9,[0 0]), pause
contour(a1,a2, G10,[0 0]), pause
[c,h]=contour(a1,a2,F); 
clabel(c,h)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
43 
Mathematical Optimization
4	 Mathematical Optimization
In this chapter we discuss the mathematical optimization problem, including its formulation and the 
techniques to solve it. The mathematical optimization problem involves minimization (or maximization) 
of a real-valued cost function by systematically choosing the values of a set of variables that are subject 
to inequality and/or equality constraints. Both cost and constraint functions are assumed analytical 
so that they can be locally approximated by Taylor series and their first and second derivatives can be 
computed. The analytical techniques used to solve the optimization problem include determination of 
first and second order necessary conditions that reveal a set of possible candidate points, which are then 
evaluated using sufficient conditions for an optimum. In convex optimization problems the feasible 
region, i.e., the set of points that satisfy the constraints, is a convex set and both object and constraint 
functions are also convex. In such problems, the existence of a single global minimum is assured.
Learning Objectives: the learning goals in this chapter are:
1.	 Understand formulation of unconstrained and constrained optimization problems
2.	 Learn the application of first and second order necessary conditions to solve optimization 
problems
3.	 Learn solution techniques used for convex optimization problems
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
44 
Mathematical Optimization
4.	 Understand the geometric viewpoint associated with optimization algorithms
5.	 Understand the concept of Lagrangian duality and how it helps toward finding a solution.
6.	 Learn the techniques used for post-optimality analysis for nonlinear problems
4.1	
The Optimization Problem
The general nonlinear optimization problem (the nonlinear programming problem) is defined as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
6XEMHFWWRá‰
İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¢
İƒà¯áˆºà¢áˆ»àµ‘Í²Ç¡ İ†àµŒİ…Ç¡ Ç¥ Ç¡ İ‰Ç¢
İ”à¯œà¯…àµ‘İ”à¯œàµ‘İ”à¯œà¯Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ


(4.1)
The above problem assumes minimization of a multi-variable scalar cost function İ‚áˆºà¢áˆ»Ç¡ZKHUHà¢× Ô¹à¯¡ 
à¢à¯àµŒáˆ¾İ”à¬µÇ¡ İ”à¬¶Ç¡ Ç¥ Ç¡ İ”à¯¡áˆ¿ that is subjected to İˆ equality constraints and İ‰Ç¢ inequality constraints. Additionally, 
lower and upper bounds on the optimization variables are considered, where these bounds may be 
grouped with the inequality constraints. 
Special cases involving variants of the general problem can also be considered. For example, the absence 
of both equality and inequality constraints specifies an unconstrained optimization problem; the problem 
may only involve a single type of constraints; the linearity of the objective and constraint functions 
specifies a linear programming problem (discussed in Chapter 5); and the restriction of optimization 
variables to a discrete set of values specifies a discrete optimization problem (discussed in Chapter 6).
We begin with defining the feasible region for the optimization problem and a discussion of the existence 
of points of minima or maxima of the objective function in that region.
Feasible Region. The set È³ àµŒàµ›İ”Ç£İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İƒà¯áˆºà¢áˆ»àµ‘Í²Ç¡ İ”à¯œà¯…àµ‘İ”à¯œàµ‘İ”à¯œà¯àµŸ is termed as the feasible 
region for the problem. If the feasible region is convex, and additionally İ„à¯œÇ¡ İƒà¯ are convex functions, 
then the problem is a convex optimization problem with some obvious advantages, e.g., f only has a 
single global minimum in Î©.
The Extreme Value Theorem in Calculus (attributed to Karl Weierstrass) provides sufficient conditions 
for the existence of minimum (or maximum) of a function defined over a complex domain. The theorem 
states: A continuous function İ‚áˆºà¢áˆ» defined over a closed and bounded set ß—Ùƒ Ü¦áˆºİ‚áˆ» attains its maximum 
and minimum in Î©. 
Thus, according to this theorem, if the feasible region Î© of the problem is closed and bounded, a minimum 
for the problem exists. The rest of the book discusses various ways to find that minimum. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
45 
Mathematical Optimization
Finding the minimum is relatively easy in the case of linear programming problems, but could be 
considerably difficult in the case of nonlinear problems with an irregular of the constraint surface. As 
a consequence, numerical methods applied to a nonlinear problem may only return a local minimum. 
Stochastic methods, such as Simulated Annealing, have been developed to find a global minimum with 
some certainty in the case of nonlinear problems. These methods are, however, not covered in this text.
Finally, we note that the convexity property, if present, helps in finding a solution to the optimization 
problem. If convexity can be ascertained through application of appropriate techniques, then we are at 
least assured that any solution found in the process would be the global solution. 
4.2	
Optimality criteria for the Unconstrained Problems
We begin by reviewing the concept of local and global minima and a discussion of the necessary and 
sufficient conditions for existence of a solution. 
Local Minimum. A point x* is a local minimum of I İ‚LIİ‚áˆºà¢×›áˆ»àµ‘İ‚áˆºà¢áˆ» in a neighborhood defined by 
Èà¢àµ†à¢×›È àµßœIRUVRPHßœàµÍ² 
Global Minimum. The point x* is a global minimum if I İ‚áˆºà¢×›áˆ»àµ‘İ‚áˆºà¢áˆ»à¢× È³Ç¡ where Î© is the feasible 
region. Further, the point x* is a strong global minimum if: İ‚áˆºà¢×›áˆ»àµİ‚áˆºà¢áˆ»à¢× È³ 
The local and global minima are synonymous in the case of convex optimization problems. In the 
remaining cases, a distinction between the two needs to be made. Further, local or global minimum in 
the case of non-convex optimization problems is not necessarily unique.
Necessary and Sufficient Conditions. The conditions that must be satisfied at the optimum point are 
termed as necessary conditions. However, the set of points that satisfies the necessary conditions further 
includes maxima and points of inflection. The sufficient conditions are then used to qualify the solution 
point as an optimum point. If a candidate point satisfies the sufficient conditions, then it is indeed the 
optimum point.
We now proceed to derive the first and second order conditions of optimality in the case of unconstrained 
optimization problems.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
46 
Mathematical Optimization
4.2.1	
 First Order Necessary Conditions (FONC)
We consider a multi-variable function İ‚áˆºà¢áˆ»àµŒİ‚áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ Ç¥ Ç¡ İ”à¯¡áˆ» and wish to investigate the behavior 
of a candidate point x*. By definition, the point x* is a local minimum of I İ‚áˆºà¢áˆ»RQO\LIİ‚áˆºà¢×›áˆ»àµ‘İ‚áˆºà¢áˆ» 
only if in the neighborhood of x*. To proceed, let ßœà¢àµŒà¢àµ†à¢×› define a small neighborhood around 
x*, then we may use first-order Taylor series expansion of İ‚ given as: 
J
İ‚áˆºà¢áˆ»àµŒİ‚áˆºà¢×›áˆ»àµ…×İ‚áˆºà¢×›áˆ»à¯ßœà¢ to 
express the condition for local minimum as:
ßœİ‚àµŒ×İ‚áˆºà¢×›áˆ»à¯ßœà¢àµ’Í²
(4.2)
We first note that the above condition is satisfied for ×İ‚áˆºà¢×›áˆ»àµŒÍ² Further, since ßœà¢ is arbitrary, ×İ‚áˆºà¢×›áˆ» 
must be zero to satisfy the above non-negativity condition on Qßœİ‚ Therefore, the first-order necessary 
condition (FONC) for optimality of I İ‚áˆºà¢áˆ» is stated as follows: 
FONC: If I İ‚áˆºà¢áˆ» has a local minimum at x*, then ×İ‚áˆºà¢×›áˆ»àµŒÍ², or equivalently, 
à°¡à¯™áˆºà¢×›áˆ»
à°¡à¯«à³•
àµŒÍ²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠÇ¤ 
The points that satisfy FONC are called stationary points of I İ‚áˆºà¢áˆ». Besides minima, these points include 
maxima and the points of inflection. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
47 
Mathematical Optimization
4.2.2	
Second Order Conditions (SOC)
Assume now that FONC are satisfied, i.e., ×İ‚áˆºà¢×›áˆ»àµŒÍ² Then, we may use second-order Taylor series 
expansion of I İ‚áˆºà¢áˆ» to write the optimality condition as:

ßœİ‚àµŒßœà¢à¯×à¬¶İ‚áˆºà¢×›áˆ»ßœà¢àµ’Í²
(4.3)
As ßœà¢ are arbitrary, the above quadratic form is positive (semi)definite if and only if the Hessian matrix, 
×à¬¶İ‚áˆºà¢×›áˆ» is positive (semi)definite. Therefore, the second order necessary condition (SONC) is stated as:
SONC: If à¢×› is a local minimizer of I İ‚áˆºà¢áˆ» , then ß˜à¬¶İ‚áˆºà¢×›áˆ»àµ’Í² 
Whereas, a second order sufficient condition (SOSC) is stated as: 
SOSC: If à¢×› satisfies ß˜à¬¶İ‚áˆºà¢×›áˆ»àµÍ² , then à¢×› is a local minimizer of I İ‚áˆºà¢áˆ». 
Further, if ×à¬¶İ‚áˆºà¢×›áˆ» is indefinite, then x* is an inflection point. In the event that ×İ‚áˆºà¢×›áˆ»àµŒ×à¬¶İ‚áˆºà¢×›áˆ»àµŒÍ², 
the lowest nonzero derivative must be even-ordered for stationary points (necessary condition), and it 
must be positive for local minimum (sufficient condition).
Two examples of unconstrained optimization problems are now considered:
Example 4.1: Polynomial data-fitting
As an example of unconstrained optimization, we consider the polynomial data-fitting problem defined 
in Sec. 2.11. The problem is to fit an th degree polynomial:İŒáˆºİ”áˆ»àµŒÏƒ
İŒà¯İ”à¯
à¯¡
à¯à­€à¬´
W to a set of data points: 
áˆºİ”à¯œÇ¡ İ•à¯œáˆ»Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ Ü°àµİŠ7 The objective is to minimize the mean square error (MSE, also termed as the 
variance of data points). The resulting unconstrained minimization problem is formulated as:
ÂÂ‹Â
à¯£à³•İ‚àµ«İŒà¯àµ¯àµŒÍ³
Í´Ü°à·
àµ«İ•à¯œàµ†áˆºİŒà¬´àµ…İŒà¬µİ”à¯œàµ…Ú® àµ…İŒà¯¡İ”à¯œ
à¯¡áˆ»àµ¯
à¬¶
à¯‡
à¯œà­€à¬µ

Then, the FONC for the problem are given as: 
ß²İ‚
ß²İŒà¯
àµŒÍ³
Ü°à·
àµ«İ•à¯œàµ†áˆºİŒà¬´àµ…İŒà¬µİ”à¯œàµ…Ú® àµ…İŒà¯¡İ”à¯œ
à¯¡áˆ»àµ¯áˆºàµ†İ”à¯œ
à¯áˆ»
à¯‡
à¯œà­€à¬µ
àµŒÍ²
For İŠàµŒÍ³ they result in the following equations: 
 á‰Œ
Í³
à¬µ
à¯‡Ïƒ İ”à¯œ
à¯œ
à¬µ
à¯‡Ïƒ İ”à¯œ
à¯œ
à¬µ
à¯‡Ïƒ İ”à¯œ
à¬¶
à¯œ
á‰á‰€İŒà¬´
İŒà¬µá‰àµŒá‰Œ
à¬µ
à¯‡Ïƒ İ•à¯œ
à¯œ
à¬µ
à¯‡Ïƒ İ”à¯œİ•à¯œ
à¯œ
á‰
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
48 
Mathematical Optimization
As 
à°¡à°®à¯™
à°¡à¯£à³•à¯£à³–àµŒ
à¬µ
à¯‡Ïƒ İ”à¯œ
à¯à¬¾à¯
à¯œ
 the SONC for the problem are evaluated as:
àµ®
Í³
Ú®
à¬µ
à¯‡Ïƒ İ”à¯œ
à¯¡
à¯œ
Ú­
Ú°
Ú­
à¬µ
à¯‡Ïƒ İ”à¯œ
à¯¡
à¯œ
Ú®
à¬µ
à¯‡Ïƒ İ”à¯œ
à¬¶à¯¡
à¯œ
àµ²àµ’Í²
 
For İŠàµŒÍ³ the determinant of the Hessian evaluates as: à¬µ
à¯‡Ïƒ İ”à¯œ
à¬¶
à¯œ
àµ†á‰€
à¬µ
à¯‡Ïƒ İ”à¯œ
à¯œ
á‰
à¬¶
 which defines the variance 
in the case of independent and identically distributed random variables. 
Finally, we note that since the data-fitting problem is convex, FONC are both necessary and sufficient 
for a minimum. 
Example 4.2: Open box problem
We wish to determine the dimensions of an open box of maximum volume that can be constructed 
form a sheet of paper (8.5 Ã—11 in) by cutting squares from the corners and folding the sides upwards. 
Let x denote the width of the paper that is folded up, then the problem is formulated as: 
ÂÂƒÂš
à¯«
İ‚áˆºİ”áˆ»àµŒáˆºÍ³Í³ àµ†Í´İ”áˆ»áˆºÍºÇ¤Í· àµ†Í´İ”áˆ»İ”
The FONC for the problem evaluate as: İ‚á‡±áˆºİ”áˆ»àµŒÍ´İ”áˆºÍ³Í»Ç¤Í· àµ†Í¶İ”áˆ»àµ†áˆºÍ³Í³ àµ†Í´İ”áˆ»áˆºÍºÇ¤Í· àµ†Í´İ”áˆ»àµŒÍ² 
Using Matlab Symbolic toolbox â€˜solveâ€™ command, we obtain two candidate solutions: İ”×› àµŒÍ³Ç¤Í·ÍºÍ·Ç¡ Í¶Ç¤Í»Í³Í·
Application of SOC results in: İ‚á‡±á‡±áˆºİ”áˆ»àµŒàµ†ÍµÍ»Ç¤Í»Í·Ç¡ÍµÍ»Ç¤Í»Í· respectively, indicating a maximum of I İ‚áˆºà¢áˆ» at 
İ”×› àµŒÍ³Ç¤Í·ÍºÍ·ZLWKİ‚áˆºİ”×›áˆ»àµŒÍ¸Í¸Ç¤Í³Í·FXLQ
4.3	
Optimality Criteria for the Constrained Problems
The majority of engineering design problems involve constraints (LE, GE, EQ) that are modeled as 
functions of optimization variables. In this section, we explore how constraints affect the optimality 
criteria. An important consideration when applying the optimality criteria to problems involving 
constraints is whether x* lies on a constraint boundary. This is implied in the case for problems involving 
only equality constraints, which are discussed first.
4.3.3	
Equality Constrained Problems
The optimality criteria for equality constrained problems involve the use of Lagrange multipliers. To 
develop this concept, we consider a problem with a single equality constraint, stated as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
VXEMHFWWRİ„áˆºà¢áˆ»àµŒÍ²
(4.4)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
49 
Mathematical Optimization
We first note that the constraint equation can be used to solve for and substitute one of the variables (say 
İ”à¯¡) in the objective function, and hence develop an unconstrained optimization problem in variables. n-1 
This, however, depends on the form of İ„áˆºà¢áˆ» and may not always be feasible. In order to develop more 
general optimality criteria, we follow Lagrangeâ€™s approach to the problem and consider the variation in 
the objective and constraint functions at a stationary point, given as:
İ€İ‚àµŒß²İ‚
ß²İ”à¬µ
İ€İ”à¬µàµ…Ú® àµ…ß²İ‚
ß²İ”à¯¡
İ€İ”à¯¡àµŒÍ²
(4.5)
İ€İ„àµŒß²İ„
ß²İ”à¬µ
İ€İ”à¬µàµ…Ú® àµ…ß²İ„
ß²İ”à¯¡
İ€İ”à¯¡àµŒÍ²
We now combine these two conditions via a scalar weight (Lagrange multiplier, Î») to write:
à·
á‰†ß²İ‚
ß²İ”à¯
àµ…ß£ß²İ„
ß²İ”à¯
á‰‡İ€İ”à¯
à¯¡
à¯à­€à¬µ
àµŒÍ²
(4.6)
Since variations İ€İ”à¯ are independent, the above condition implies that: à°¡à¯™
à°¡à¯«à³•àµ…ß£
à°¡à¯›
à°¡à¯«à³•àµŒÍ²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Fundamental Engineering Optimization 
Methods
50 
Mathematical Optimization
We further note that application of FONC to a Lagrangian function defined as:à£¦áˆºà¢Ç¡ ß£áˆ»àµŒİ‚áˆºà¢áˆ»àµ…ß£İ„áˆºà¢áˆ» 
also gives rise to the above condition. For multiple equality constraints, the Lagrangian function is 
similarly formulated as:
à£¦áˆºà¢Ç¡ à£…áˆ»àµŒİ‚áˆºà¢áˆ»àµ…à·
ß£à¯œİ„à¯œáˆºà¢áˆ»
à¯Ÿ
à¯œà­€à¬µ
 
(4.7)
Then, in order for İ‚áˆºà¢áˆ» to have a local minimum at x*, the following FONC must be satisfied:
ß²à£¦
ß²İ”à¯
àµŒß²İ‚
ß²İ”à¯
àµ…à·
ß£à¯œ
ß²İ„à¯œ
ß²İ”à¯
İ”à¯
à¯Ÿ
à¯œà­€à¬µ
àµŒÍ²Ç¢ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ
(4.8)
İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ
The above FONC can be equivalently stated as: 
×à¢à£¦áˆºà¢×›Ç¡ à£…×›áˆ»àµŒÍ²Ç¡
×à£…à£¦áˆºà¢×›Ç¡ à£…×›áˆ»àµŒÍ²
These conditions suggest that à£¦áˆºà¢×›Ç¡ à£…×›áˆ» is stationary with respect to both x and Î»; therefore, minimization 
of à£¦áˆºà¢Ç¡ à£…áˆ» amounts to an unconstrained optimization problem. Further, the Lagrange Multiplier Theorem 
(Arora, p.135) states that if x* is a regular point (defined below) then the FONC result in a unique 
solution to Rß£à¯œ
×›
We note that the above FONC further imply: ×İ‚áˆºà¢×›áˆ»àµŒàµ†Ïƒ
ß£à¯œ×İ„à¯œáˆºà¢×›áˆ»
à¯£
à¯œà­€à¬µ
 Algebraically, it means that 
the cost function gradient is a linear combination of the constraint gradients. Geometrically, it means 
that the negative of the cost function gradient lies in the convex cone spanned by the constraint normals 
(Sec. 4.3.5). 
SOSC for equality constrained problems are given as: ×à¬¶à£¦áˆºà¢×›Ç¡ à£…×›áˆ»àµŒ×à¬¶İ‚áˆºà¢×›áˆ»àµÍ² Further discussion 
on SOC for constrained optimization problems is delayed till Sec. 4.4.3. 
An example is now presented to explain the optimization process for equality constrained problems.
Example 4.3: We consider the following optimization problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒàµ†İ”à¬µİ”à¬¶
Subject to: İ„áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµŒÍ²
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
51 
Mathematical Optimization
We first note that the equality constraint can be used to develop an unconstrained problem in one 
variable, given as: ÂÂ‹Âà¯«à°­İ‚áˆºİ”à¬µáˆ»àµŒàµ†İ”à¬µà¶¥Í³ àµ†İ”à¬µ
à¬¶RUÂÂ‹Âà¯«à°®İ‚áˆºİ”à¬¶áˆ»àµŒàµ†İ”à¬¶à¶¥Í³ àµ†İ”à¬¶
à¬¶ or . Instead, we follow 
the Lagrangian approach to solve the original problem below.
Next, we assess the objective function and observe that the origin is saddle point of the function. It is 
also instructive to review the problem from a graphical perspective (Figure 4.1). The figure shows the 
feasible region, i.e., the perimeter of a unit circle superimposed on the level sets of the objective function. 
Then, by inspection, the optimum can be located in the first and the third quadrant where the level 
curves are tangent to the circle.
The Lagrangian function for the problem is formulated as: à£¦áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ ß£áˆ»àµŒàµ†İ”à¬µİ”à¬¶àµ…ß£áˆºİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³áˆ» 
The FONC evaluate as: Í´ß£İ”à¬µàµ†İ”à¬¶àµŒÍ²Ç¡ Í´ß£İ”à¬¶àµ†İ”à¬µàµŒÍ²Ç¡ İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµŒÍ² 
Thus, there are four candidate solutions at: áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€àµ‡
à¬µ
Î¾à¬¶Ç¡ àµ‡
à¬µ
Î¾à¬¶á‰Ç¡ ß£×› àµŒàµ‡
à¬µ
à¬¶ 
The SONC for the problem evaluate as: á‰‚Í´ß£
àµ†Í³
àµ†Í³
Í´ß£á‰ƒàµ’Í² Application of SONC reveals multiple minima 
at áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€
à¬µ
Î¾à¬¶Ç¡
à¬µ
Î¾à¬¶á‰×« á‰€àµ†
à¬µ
Î¾à¬¶Ç¡ àµ†
à¬µ
Î¾à¬¶á‰ZLWKİ‚áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒàµ†
à¬µ
à¬¶
Figure 4.1: Level sets of the objective function superimposed on the equality constraint.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
52 
Mathematical Optimization
The above example underscores some of the pitfalls in the case of nonlinear optimization problems: 
Application of FONC results in multiple nonlinear equations whose simultaneous solution reveals several 
candidate points, which pertain to maxima, minima and points of inflection. The minima may then be 
obtained via application of SOSC or via a comparison of function values at the individual points.
4.3.4	
 Inequality Constrained Problems
We next consider an optimization problem involving a single inequality constraint. The problem is 
stated as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
VXEMHFWWRİƒáˆºà¢áˆ»àµ‘Í²
(4.10)
We note that we can add a slack variable to the inequality constraint to turn it into equality. Further, to 
ensure constraint compliance, the slack variable is restricted to be non-negative. We therefore replace 
the inequality constraint with equality: İƒáˆºà¢áˆ»àµ…Â•à¬¶àµŒÍ² A Lagrangian function for the problem is now 
developed as:
à£¦áˆºà¢Ç¡ ß£Ç¡ İáˆ»àµŒİ‚áˆºà¢áˆ»àµ…ß£áˆºİƒáˆºà¢áˆ»àµ…Â•à¬¶áˆ»
(4.11)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Fundamental Engineering Optimization 
Methods
53 
Mathematical Optimization
The resulting FONC evaluate as: 
×à£¦áˆºà¢×›Ç¡ ß£×›Ç¡ İ×›áˆ»àµŒ×İ‚áˆºà¢×›áˆ»àµ…ß£×›×İƒáˆºà¢×›áˆ»àµŒÍ²
İƒáˆºà¢×›áˆ»àµ…Â•×›à¬¶àµŒÍ²
ß²à£¦
ß²İàµŒÍ´ß£×›İ×› àµŒÍ²

(4.12)
The latter condition, known as the switching or complementarity condition, further evaluates as: ß£àµŒÍ² 
(thus implying an inactive constraint) or İàµŒÍ² (implying an active/binding constraint). Each of these 
cases is to be explored for feasible solutions, which can be checked for optimality via application of SOC. 
We note that by substituting: Â•à¬¶àµŒàµ†İƒáˆºà¢×›áˆ» the FONC can be equivalently expressed as: ×İ‚áˆºà¢×›áˆ»àµŒÍ²Ç¡
İƒáˆºà¢×›áˆ»àµ‘Í²Ç¡ ß£×›İƒáˆºà¢×›áˆ»àµŒÍ²which provides an equivalent characterization of FONC in the case of 
inequality constrained problems.
Finally, the above results can be extended to multiple inequality constraints as: 
à£¦áˆºà¢Ç¡ ß£Ç¡ İáˆ»àµŒİ‚áˆºà¢áˆ»àµ…à·ß£à¯œàµ«İƒà¯œáˆºà¢áˆ»àµ…İà¯œ
à¬¶àµ¯
à¯œ

(4.13)
Then, in order for İ‚áˆºà¢áˆ» to have a local minimum at x*, the following FONC must be satisfied:
ß²à£¦
ß²İ”à¯
àµŒß²İ‚
ß²İ”à¯
àµ…à·
ß£à¯œ
ß²İƒà¯œ
ß²İ”à¯
İ”à¯
à¯ 
à¯œà­€à¬µ
àµŒÍ²Ç¢ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ 
(4.14)
İƒà¯œáˆºà¢áˆ»àµ…İà¯œ
à¬¶àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
ß£à¯œİà¯œàµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
where we note that for İ‰ inequality constraints, application of the switching conditions results in cases, 
each of which needs to be explored for feasibility and optimality. 
Next, we present an example of the inequality constrained problem.
Example 4.4: We consider the following optimization problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒàµ†İ”à¬µİ”à¬¶
6XEMHFWWRİƒáˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµ‘Í²
The graphical consideration of the equality constrained problem was earlier presented in Fig. 4.1. From 
that figure, it is obvious that the inequality constrained problem will have a solution at the boundary of the 
constraint set, i.e., at the perimeter of the circle. This view is supported by the analysis presented below.
We first convert the inequality to equality constraint via: İƒáˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµ…İà¬¶àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµ…İà¬¶àµŒÍ²
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
54 
Mathematical Optimization
Then, the Lagrangian function is formulated as: à£¦áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ ß£Ç¡ İáˆ»àµŒàµ†İ”à¬µİ”à¬¶àµ…ß£áˆºİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ…İà¬¶àµ†Í³áˆ» 
The resulting FONC evaluate as: Í´ß£İ”à¬µàµ†İ”à¬¶àµŒÍ²Ç¡ Í´ß£İ”à¬¶àµ†İ”à¬µàµŒÍ²Ç¡ İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ…İà¬¶àµ†Í³ àµŒÍ²Ç¡ ß£İàµŒÍ² 
The switching condition further evaluates as: ß£×› àµŒÍ²RUİ×› àµŒÍ² The former condition evaluates as: 
áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒáˆºÍ²Ç¡Í²áˆ»Ç¡ İ×› àµŒàµ‡Í³ while latter condition evaluates as: áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€àµ‡
à¬µ
Î¾à¬¶Ç¡ àµ‡
à¬µ
Î¾à¬¶á‰Ç¡ ß£×› àµŒàµ‡
à¬µ
à¬¶ 
Function evaluation at the candidate points reveals multiple minima at áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€
à¬µ
Î¾à¬¶Ç¡
à¬µ
Î¾à¬¶á‰×« á‰€àµ†
à¬µ
Î¾à¬¶Ç¡ àµ†
à¬µ
Î¾à¬¶á‰ 
with İ‚áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒàµ†
à¬µ
à¬¶
4.4	
Optimality Criteria for General Optimization Problems
The general nonlinear optimization problem was defined in (4.1) above, where we can group the variable 
limits with the inequality constraints to state the problem as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
(4.15)
6XEMHFWWRİ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¢İƒà¯áˆºà¢áˆ»àµ‘Í²Ç¡ İ†àµŒİ…Ç¡ Ç¥ Ç¡ İ‰
The feasible region for the problem is given as: 
È³ àµŒàµ›à¢Ç£İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¢İƒà¯áˆºà¢áˆ»àµ‘Í²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰àµŸ
(4.16)
To solve the problem via the Lagrangian function approach, we first add slack variables to the inequality 
constraints; we then associate Lagrange multiplier vectors u and v with the inequality and equality 
constraints, respectively, and develop a Lagrangian function, which is given as:
à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œÇ¡ à¢™áˆ»àµŒİ‚áˆºà¢áˆ»àµ…à·
İ’à¯œİ„à¯œáˆºà¢áˆ»
à¯Ÿ
à¯œà­€à¬µ
àµ…à·
İ‘à¯áˆºİƒà¯áˆºà¢áˆ»àµ…İà¯
à¬¶
à¯ 
à¯à­€à¬µ
áˆ» 
(4.17)
The resulting FONC evaluate as:
1.	 Gradient conditions: à°¡à£¦
à°¡à¯«à³–àµŒ
à°¡à¯™
à°¡à¯«à³–àµ…Ïƒ
İ’à¯œ
×› à°¡à¯›à³”
à°¡à¯«à³–
à¯Ÿ
à¯œà­€à¬µ
àµ…Ïƒ
İ‘à¯
×› à°¡à¯šà³•
à°¡à¯«à³–
à¯ 
à¯à­€à¬µ
àµŒÍ²Ç¢ İ‡àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ 
2.	 Switching conditions: İ‘à¯
×›İà¯àµŒÍ²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
3.	 Feasibility conditions: İƒà¯áˆºà¢×›áˆ»àµ‘Í²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¢İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŒ
4.	 Non-negativity condition: İ‘à¯
×› àµ’Í²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
5.	 Regularity condition: for those İ‘à¯
×›Ç¡ that satisfy \İ‘à¯
×› àµÍ²Ç¡ ×İƒà¯áˆºà¢×›áˆ» are linearly independent
The above FONC are better known as the KKT (Krush-Kuhn-Tucker) conditions. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
55 
Mathematical Optimization
We note that à¢Ç¡ à¢›Ç¡ à¢œÇ¡ à¢™ are, respectively, İŠİ‰İ‰ and İˆG dimensional vectors. Thus, the total number 
of variables in the problem is: İŠàµ…Í´İ‰àµ…İˆ meaning simultaneous nonlinear equations must be solved 
to obtain a candidate solution. Further, in accordance with the switching conditions, a total of Í´à¯  such 
solutions must be explored.
Further, since İà¯
à¬¶àµŒàµ†İƒà¯áˆºà¢áˆ» non-negativity of İà¯
à¬¶ ensures feasibility of the inequality constraint. 
Therefore İà¯
à¬¶àµŒÍ², implies an active constraint, whereby \İ‘à¯
×› àµÍ² and an inactive constraint is implied 
by: İ‘à¯
×› àµŒÍ²Ç¡İà¯
à¬¶àµÍ² We also note that for regular points the Lagrange Multiplier Theorem (Arora, p.135) 
ensures a unique solution to the Lagrange multipliers İ’à¯œ
×› and İ‘à¯
×›
An example of general optimization problem is presented below.
Example 4.5: We consider adding a linear equality constraint to Example 4.4 above:

ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒàµ†İ”à¬µİ”à¬¶
Subject to:İƒáˆºİ”à¬µÇ¡ İ”à¬¶áˆ»Ç£ İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµ‘Í²Ç¢ İ„áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»Ç£İ”à¬µàµ…İ”à¬¶àµ†Ü¿àµŒÍ²
We first convert the inequality to equality constraint via: İƒáˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµ…İà¬¶àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµ…İà¬¶àµŒÍ²
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Fundamental Engineering Optimization 
Methods
56 
Mathematical Optimization
We then use Lagrange multipliers to formulate a Lagrangian function, which is given as:
à£¦áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ İ‘Ç¡ İ’Ç¡ İáˆ»àµŒàµ†İ”à¬µİ”à¬¶àµ…İ‘áˆºİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ…İà¬¶àµ†Í³áˆ»àµ…İ’áˆºİ”à¬µàµ…İ”à¬¶àµ†Ü¿áˆ»
The resulting KKT conditions evaluate as: Í´İ‘İ”à¬µàµ…İ’àµ†İ”à¬¶àµŒÍ²Ç¡ Í´İ‘İ”à¬¶àµ…İ’àµ†İ”à¬µàµŒÍ²Ç¡ İ”à¬µàµ…İ”à¬¶àµ†Ü¿àµŒÍ²Ç¡
İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ…İà¬¶àµ†Í³ àµŒÍ²Ç¡İ‘İàµŒÍ² From the switching condition: or İ‘×› àµŒÍ²RUİ×› àµŒÍ²
The former condition evaluates as: áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€
à¯–
à¬¶Ç¡
à¯–
à¬¶á‰Ç¡ İ×› àµŒàµ‡à¶§Í³ àµ†
à¯–à°®
à¬¶Ç¡ İ’×› àµŒ
à¯–
à¬¶
The latter condition has no feasible solution. 
Function evaluation at the sole candidate points results in: İ‚áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒàµ†
à¯–à°®
à¬¸
4.4.1	
Optimality Criteria for Convex Optimization Problems
In this section we consider the class of optimization problems where the feasible region is a convex set 
and the objective and constraint functions are convex. The convexity property is desirable since it implies 
the existence of a global minimum to the optimization problem.
We consider the general optimization problem defined in (4.15) with the feasible region given by (4.16). 
Then Î©, is a convex set if functions İ„à¯œ are linear and İƒà¯ are convex. If additionally İ‚áˆºà¢áˆ» is a convex 
function, then the optimization problem is convex.
We now assume that İ‚áˆºà¢áˆ» is a convex function defined over a convex set Î©. Then, if İ‚áˆºà¢áˆ» attains a local 
minimum at à¢×› × È³ then x* is also a global minimum over Î©. Furthermore, İ‚áˆºà¢×›áˆ» is a local/global 
minimum if and only if it satisfies the KKT conditions, i.e., the KKT conditions are both necessary and 
sufficient for a global minimum in the case of convex optimization problems. 
We, however, note that convexity is a sufficient but not necessary condition for a global minimum, i.e., 
nonexistence of convexity does not preclude the existence of a global minimum. An example of a convex 
optimization problem is presented below.
Example 4.6: We consider the following optimization problem:
ÂÂ‹Âà¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†İ”à¬µİ”à¬¶
subject to İƒáˆºİ”à¬µÇ¡ İ”à¬¶áˆ»Ç£İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµ‘Í²Ç¢ İ„áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»Ç£İ”à¬µàµ…İ”à¬¶àµ†Ü¿àµŒÍ²
As was done in Example 4.5, we convert the inequality constraint to equality via: İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†Í³ àµ…İà¬¶àµŒÍ²
We then use Lagrange multipliers to formulate a Lagrangian function given as:
à£¦áˆºİ”à¬µÇ¡ İ”à¬¶Ç¡ İ‘Ç¡ İ’Ç¡ İáˆ»àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†İ”à¬µİ”à¬¶àµ…İ‘áˆºİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ…İà¬¶àµ†Í³áˆ»àµ…İ’áˆºİ”à¬µàµ…İ”à¬¶àµ†Ü¿áˆ» 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
57 
Mathematical Optimization
The resulting KKT conditions evaluate as: áˆºÍ´İ‘àµ…Í³áˆ»İ”à¬µàµ…İ’àµ†İ”à¬¶àµŒÍ²Ç¡ áˆºÍ´İ‘àµ…Í³áˆ»İ”à¬¶àµ…İ’àµ†İ”à¬µàµŒÍ²Ç¡  
İ”à¬µàµ…İ”à¬¶àµ†Ü¿àµŒÍ²Ç¡ İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ…İà¬¶àµ†Í³ àµŒÍ²Ç¡ İ‘İàµŒÍ² From the switching condition: İ‘×› àµŒÍ²RUİ×› àµŒÍ²
Similar to Example 4.5, the former condition evaluates as: áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€
à¯–
à¬¶Ç¡
à¯–
à¬¶á‰Ç¡ İ×› àµŒàµ‡à¶§Í³ àµ†
à¯–à°®
à¬¶Ç¡ İ’×› àµŒ
à¯–
à¬¶Ç¢  
while the latter condition has no feasible solution. Function evaluation at the sole candidate points results 
in:İ‚áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒàµ†
à¯–à°®
à¬¸ 
4.4.2	
A Geometric Viewpoint
The optimality criteria for constrained optimization problems have geometrical connotations. The 
following definitions help understand the geometrical viewpoint associated with the KKT conditions.
Active constraint set. The set of active constraints at x is defined as: à££àµŒàµ›İ…×« İ†Ç£ İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İƒà¯áˆºà¢áˆ»àµŒÍ²àµŸ
The set of active constraint normals is given as: à£­àµŒáˆ¼×İ„à¯œáˆºà¢áˆ»Ç¡ ×İƒà¯áˆºà¢áˆ»Ç¡ İ†× à££áˆ½
Constraint tangent hyperplane. The constraint tangent hyperplane is defined by the set of vectors 
à£­à­„àµŒàµ›à¢ŠÇ£×İ„à¯œáˆºà¢áˆ»à¯à¢ŠàµŒÍ²Ç¡ ×İƒà¯áˆºà¢áˆ»à¯à¢ŠàµŒÍ²Ç¡ İ†× à££àµŸ
Regular point. Assume x is a feasible point. Then, x is a regular point if the vectors in the active constraint 
set à£­ are linearly independent. 
Feasible direction. Assume that x is a regular point. A vector d is a feasible direction if İ„à¯œáˆºà¢áˆ»à¯à¢ŠàµŒÍ²Ç¡ 
×İƒà¯áˆºà¢áˆ»à¯à¢ŠàµÍ²Ç¡ İ†× à££Ç¢ where the feasibility condition for each active inequality constraint defines a half 
space. The intersection of those half spaces is a feasible cone within which a feasible vector d should lie.
Descent direction. A direction d is a descent direction if the directional derivative of f along d is negative, 
i.e., ×İ‚áˆºà¢áˆ»à¯à¢ŠàµÍ²
Extreme point. Assume x is a feasible point. Then, x is an extreme point if the active constraint set à££ 
at x is non-empty; otherwise it is an interior point.
Assume now that we are at an extreme point x of the feasible region. We seek a search direction which is 
both descent and feasible. If no such direction can be found then we have already reached the optimum. 
Geometrical categorization of the optimal point rests on the following lemma.
Farkaâ€™s Lemma. For à¡­× Ô¹à¯¡àµˆà¯ Ç¡ à¢‰× Ô¹à¯¡ only one of the two problems has a solution:
1.	 à¡­à¯à¢àµ’à«™Ç¡ à¢‰à¯à¢àµÍ²
2.	 à¢‰àµŒà¡­à¢ŸÇ¡ à¢Ÿàµ’à«™
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
58 
Mathematical Optimization
Corollary. For any à¡­× Ô¹à¯¡àµˆà¯ Ç¡ à¢‰× Ô¹à¯¡ZHKDYHà¡­à¯à¢àµ’à«™Ç¡ à¢‰à¯à¢àµ’Í² if and only if à¢‰àµŒà¡­à¢ŸÇ¡ à¢Ÿàµ’à«™
Farkaâ€™s lemma was used in the proof of Karush-Kuhn-Tucker (KKT) Theorem on NLP by Tucker. The 
lemma states that if a vector à¢‰ does not lie in the convex cone: à¡¯àµŒáˆ¼à¡­à¢ŸÇ¡ à¢Ÿàµ’à«™áˆ½Ç¡ then there is a vector 
à¢Ç¡ à¡­à¯à¢àµ’à«™Ç¡ that is normal to a hyperplane separating c from C. 
To apply this lemma we consider a matrix A whose columns represent the active constraint gradients at 
the optimum point x*, a vector that represents the objective function gradient ×İ‚áˆºà¢×›áˆ» and d represents 
a search direction. Then, there is no d satisfying the descent and feasibility conditions: ×İ‚áˆºà¢×›áˆ»à¯à¢ŠàµÍ² 
and ×İƒà¯áˆºà¢áˆ»à¯à¢ŠàµÍ²Ç¡ İ†× à££ if and only if the objective function gradient can be expressed as: àµ†×İ‚áˆºà¢×›áˆ»àµŒ 
Ïƒ
ß£à¯×İƒà¯áˆºà¢×›áˆ»
à¯×à££
Ç¡ ß£à¯àµ’Í² 
The above lemma also explains the non-negativity condition on Lagrange multipliers for inequality 
constraints in the KKT conditions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Fundamental Engineering Optimization 
Methods
59 
Mathematical Optimization
4.4.3	
Second Order Conditions 
The second order necessary and sufficient conditions assume that x* satisfies the FONC (the KKT 
conditions) and use the Hessian of the Lagrangian function to investigate the behavior of the candidate 
point x* where we. The Hessian of the Lagrangian is defined as:
×à¬¶à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œÇ¡ à¢™áˆ»àµŒ×à¬¶İ‚áˆºà¢áˆ»àµ…à·
İ’à¯œ×à¬¶İ„à¯œáˆºà¢áˆ»
à¯Ÿ
à¯œà­€à¬µ
àµ…à·
İ‘à¯×à¬¶İƒà¯áˆºà¢áˆ»
à¯ 
à¯à­€à¬µ

(4.18)
Second Order Necessary Condition (SONC). Assume d is a feasible vector that lies in the constraint 
tangent hyperplane, i.e., let àµ›à¢ŠÇ£×İ„à¯œáˆºà¢áˆ»à¯à¢ŠàµŒÍ²Ç¡ ×İƒà¯áˆºà¢áˆ»à¯à¢ŠàµŒÍ²Ç¡ İ†× à££àµŸ,Ià¢×› is a local minimizer of f, 
then it satisfies the following SONC: 
ßœİ‚àµŒà¢Šà¯×à¬¶à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›Ç¡ à¢™×›áˆ»à¢Šàµ’Í²
(4.19)
Second Order Sufficient Condition (SOSC). If for some à¢×›Ç¡à¢Šà¯×à¬¶à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›Ç¡ à¢™×›áˆ»à¢ŠàµÍ² for all 
àµ›à¢ŠÇ£×İ„à¯œáˆºà¢×›áˆ»à¯à¢ŠàµŒÍ²Ç¡ ×İƒà¯áˆºà¢×›áˆ»à¯à¢ŠàµŒÍ²Ç¡ İ‘à¯
×› àµÍ²àµŸ then x* is a local minimizer of İ‚áˆºà¢áˆ» 
Further, a stronger SOSC is given as: ×à¬¶à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›Ç¡ à¢™×›áˆ»àµÍ² then à¢×› is a local minimizer of İ‚áˆºà¢áˆ»
An example of SOC is now presented.
Example: Second order conditions
We consider the optimization problem in Example 4.5. The constraint tangent hyperplane for active 
constraints in Example 4.5 is computed as: İ€à¬µàµ†İ€à¬¶àµŒÍ²Ç¤  The Hessian of the Lagrangian at the optimum 
point áˆºİ”à¬µ
×›Ç¡ İ”à¬¶
×›áˆ»àµŒá‰€
à¯–
à¬¶Ç¡
à¯–
à¬¶á‰LVJLYHQDVá‰€Í²
àµ†Í³
àµ†Í³
Í² á‰Therefore, the SONC evaluate as: à¢Šà¯×à¬¶à£¦à¢ŠàµŒÍ´ 
indicating that the candidate point is indeed an optimum point. Similar analysis applies to Example 4.6.
4.5	
Postoptimality Analysis
Postoptimality analysis refers to the study of the effects of parametric changes on the optimal solution. 
In particular, we are interested in the objective function variation resulting from relaxing the constraint 
limits. To study these changes, we consider the following perturbed optimization problem (Arora, p.153):
	
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
(4.20)
Subject to İ„à¯œáˆºà¢áˆ»àµŒÜ¾à¯œÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¢İƒà¯áˆºà¢áˆ»àµ‘İà¯Ç¡ İ†àµŒİ…Ç¡ Ç¥ Ç¡ İ‰
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
60 
Mathematical Optimization
where Ü¾à¯œ and İà¯ are small variations in the neighborhood of zero. Let the optimum point for the perturbed 
problem be expressed as: à¢×›áˆºà¢ˆÇ¡ à¢‹áˆ»Ç¡ with the optimal cost given as: İ‚×›áˆºà¢ˆÇ¡ à¢‹áˆ» Then, the implicit first 
order derivatives of the cost function are computed as: à°¡à¯™áˆºà¢×›áˆ»
à°¡à¯•à³”
àµŒàµ†İ’à¯œ
×›Ç¡
à°¡à¯™áˆºà¢×›áˆ»
à°¡à¯˜à³•
àµŒàµ†İ‘à¯
×›Ç¢ and, the resulting 
cost function variation due to constraint relaxation is given as:
ßœİ‚áˆºà¢×›áˆ»àµŒàµ†à·İ’à¯œ
×›Ü¾à¯œ
à¯œ
àµ†à·İ‘à¯
×›İà¯
à¯

(4.21)
The above result implies that the non-zero Lagrange multipliers accompanying the active constraints 
determine the cost function sensitivity to constraint relaxation. Non-active constraints have zero Lagrange 
multipliers, and hence do not affect the solution. Further, if the Lagrange multipliers for active constraints 
were to take on negative values, then constraint relaxation would result in a reduction in the optimum 
cost function value, which is counter-intuitive.
The cost function variation resulting from changes to parameters embedded in the constraints, İ„à¯œáˆºİáˆ» 
and İƒà¯áˆºİáˆ» can be similarly examined by considering how individual constraint variations affect the 
cost function, i.e., 
ßœİ‚áˆºà¢×›áˆ»àµŒà·İ’à¯œ
×›ßœİ„à¯œ
à¯œ
àµ…à·İ‘à¯
×›ßœİ’à¯
à¯

(4.22)
where, once again, we observe that Lagrange multipliers for the individual constraints determine the 
sensitivity of I ßœİ‚áˆºà¢×›áˆ» to the parameter variations related to those constraints. 
4.6	
Lagrangian Duality 
Lagrangian duality in optimization problems stems from the fact that the Lagrangian function is 
stationary at the optimal point. The duality theory associates with every optimization problem (termed 
as primal) a dual problem that uses Lagrange multipliers as the optimization variables. 
To develop the duality concepts, we consider a general optimization problem (4.15), where a Lagrangian 
function for the problem was defined in (417). Equivalently, the Lagrangian function can be written 
without the slack variables, and in vector format the function and its derivatives are given as:
à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒİ‚áˆºà¢áˆ»àµ…à¢›à¯à¢àµ…à¢œà¯à¢
(4.23)
×à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒ×İ‚áˆºà¢áˆ»àµ…áˆ¾àª¸à¢áˆ¿à¢›àµ…áˆ¾àª¸à¢áˆ¿à¢œ
where áˆ¾àª¸à¢áˆ¿Ç¡ áˆ¾àª¸à¢áˆ¿ are Jacobian matrices containing individual constraint derivatives as column vectors. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
61 
Mathematical Optimization
Next, let x* represent an optimal solution to the problem and let áˆºà¢›×›Ç¡ à¢œ×›áˆ» be the associated Lagrange 
multipliers, then the Lagrangian function is stationary at the optimum point, i.e.×à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»àµŒÍ²
To proceed further, we assume that the Hessian of the Lagrangian is positive definite in the feasible 
region, i.e., ×à¬¶à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµ’à«™Ç¡à¢× È³ We can now state the following duality theorem (Belegundu and 
Chandrupatla, p. 269):
Duality theorem: The following are equivalent:
1.	 x* together with áˆºà¢›×›Ç¡ à¢œ×›áˆ» solves the primal problem (4.15).
2.	 áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ» is a saddle point of the Lagrangian function à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ», i.e., 
à£¦áˆºà¢×›Ç¡ à¢›Ç¡ à¢œáˆ»àµ‘à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»àµ‘à£¦áˆºà¢Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»
(4.24)
3.	 áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ» solves the following dual problem:
ÂÂƒÂš
à¢Ç¡à¢›à®¹à«™Ç¡à¢œà£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ» Subject to ×à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒÍ²
(4.25)
Further, the two extrema are equal, i.e., à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»àµŒİ‚áˆºà¢×›áˆ» 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Fundamental Engineering Optimization 
Methods
62 
Mathematical Optimization
In (4.24) above,à£¦áˆºà¢×›Ç¡ à¢›Ç¡ à¢œáˆ»àµŒÂÂ‹Âà¢×à®à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ» represents a minimizer of à£¦ when à¢›àµ’à«™Ç¡ à¢œ are 
fixed; similarly,à£¦áˆºà¢Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»àµŒÂÂƒÂšà¢›à®¹à«™Ç¡à¢œà£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ» is a maximizer of à£¦ when à¢× È³ is fixed. These 
two functions, respectively, lower and upper bound the Lagrangian at the optimum point. Hence,
İ‚áˆºà¢×›áˆ»àµ‘İ‚áˆºà¢áˆ» for any x that is primal-feasible, andà£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµ‘İ‚áˆºà¢×›áˆ» for any à¢Ç¡ à¢›Ç¡ à¢œ that are dual 
feasible ×à£¦àµŒÍ²Ç¡ à¢›àµ’à«™
Further,ÂÂƒÂšà¢›à®¹à«™Ç¡à¢œà£¦áˆºà¢×›Ç¡ à¢›Ç¡ à¢œáˆ»àµ‘ÂÂ‹Âà¢×à®à£¦áˆºà¢Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»Ç¡ which signifies weak duality. As a consequence of 
duality, if the dual objective is unbounded then primal problem has no feasible solution and vice versa.
We note that in nonlinear problems achieving strong duality (equality in 4.24) is not always possible, 
and, in general, a duality gap exists between the primal and dual solutions. Nevertheless, existence of 
strong duality is ensured in the case of convex optimization problems that satisfy the positive definite 
assumption on the Hessian matrix. Those problems are discussed following the dual function formulation.
4.6.1	
Formulation of the Dual Function
The definition of the dual problem in (4.24) assumes that ×à¬¶à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ» is positive definite. If this 
assumption is valid, then by Implicit Function Theorem there exists a neighborhood Gà£² around áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ» 
in which à¢àµŒà¢áˆºà¢›Ç¡ à¢œáˆ» is a differentiable function, ×à£¦áˆºà¢áˆºà¢›Ç¡ à¢œáˆ»Ç¡ à¢›Ç¡ à¢œáˆ»àµŒÍ²DQG×à¬¶à£¦áˆºà¢áˆºà¢›Ç¡ à¢œáˆ»Ç¡ à¢›Ç¡ à¢œáˆ», and 
is positive definite. Moreover, everyáˆºà¢›àµ’à«™Ç¡ à¢œáˆ»FORVHWRáˆºà¢›×›Ç¡ à¢œ×›áˆ» close to has a unique corresponding x 
that is a global minimizer of à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»LQà£² This allows us to define a dual function Qß®áˆºà¢›Ç¡ à¢œáˆ» as:
ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒÂÂ‹Â
à¢×à£²à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»
(4.26)
where the minimum can be found from the application of FONC. In terms of the dual function the dual 
optimization problem is defined as:
áˆºáˆ»ÂÂƒÂš
à¢›à®¹à«™Ç¡à¢œß®áˆºà¢›Ç¡ à¢œáˆ»
	

(4.27)
Let áˆºà¢›×›Ç¡ à¢œ×›áˆ» be the optimal solution to the dual problem, then ß®áˆºà¢›×›Ç¡ à¢œ×›áˆ»àµŒİ‚áˆºà¢×›áˆ» 
The dual problem may be similarly solved via application of FONC. Towards this end, we use the chain 
rule to compute the derivative of the dual function as (Griva, Nash & Sofer, p. 537):
×ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒ×à¢›Ç¡à¢œà¢áˆºà¢›Ç¡ à¢œáˆ»×à¯«ß®áˆºà¢›Ç¡ à¢œáˆ»àµ…×à¢›Ç¡à¢œß®áˆºà¢›Ç¡ à¢œáˆ»
(4.28)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
63 
Mathematical Optimization
where, by definition,×à¯«ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒÍ² Further, ×à¢›ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒà¢àµ«à¢áˆºà¢›Ç¡ à¢œáˆ»àµ¯Ç¡ ×à¢œß®áˆºà¢›Ç¡ à¢œáˆ»àµŒà¢àµ«à¢áˆºà¢›Ç¡ à¢œáˆ»àµ¯Ç¡ and 
×à¢›Ç¡à¢œß®áˆºà¢›Ç¡ à¢œáˆ»àµŒàµ¤à¢áˆºà¢áˆºà¢›Ç¡ à¢œáˆ»áˆ»
à¢áˆºà¢áˆºà¢›Ç¡ à¢œáˆ»áˆ»àµ¨ Next, we note that: ×à¬¶ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒ×à¢áˆºà¢›Ç¡ à¢œáˆ»àµ¤àª¸à¢à¯
àª¸à¢à¯àµ¨Ç¡
àª¸
where ×à¢áˆºà¢›Ç¡ à¢œáˆ» may 
be found from differentiating ×à¯«ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒà«™DV×àµ«×à¯«ß®áˆºà¢›Ç¡ à¢œáˆ»àµ¯àµŒá‰‚àª¸à¢
àª¸à¢á‰ƒàµ…×à¢áˆºà¢›Ç¡ à¢œáˆ»×à¯«à¯«ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒà«™ 
Therefore, ×à¢áˆºà¢›Ç¡ à¢œáˆ»àµŒá‰‚àª¸à¢
àª¸à¢á‰ƒáˆ¾×à¯«à¯«
à¬¶ß®áˆºà¢›Ç¡ à¢œáˆ»áˆ¿à¬¿à¬µ and we finally get:

×à¬¶ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒàª¸à¢áˆº×à¯«à¯«
à¬¶à£¦áˆ»à¬¿à¬µàª¸à¢à¯àµ…àª¸à¢áˆº×à¯«à¯«
à¬¶à£¦áˆ»à¬¿à¬µàª¸à¢à¯
(4.29)
An example of the dual function is given in the next section under convex optimization problems.
4.6.2	
 Duality in Convex Optimization Problems
In the case of convex optimization problems, if x* is a regular point that solves the primal problem, and 
if à¢›×›Ç¡ à¢œ×› are the associated Lagrange multipliers, then áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ» is dual feasible and solves the dual 
problem. To develop these concepts, we consider the following quadratic programming (QP) problem:
	
Minimize İáˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢
(4.30)
	
Subject to: à¡­à¢àµ’à¢ˆ
where Q is symmetric and positive definite. Then, using Lagrange multiplier vector Î» for the inequality 
constraint, a Lagrangian function for the QP problem is given as: 
	
à£¦áˆºà¢Ç¡ à£…áˆ»àµŒÍ³
Í´ à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ†à£…à¯áˆºà¡­à¢àµ†à¢ˆáˆ»
(4.31)
The associated dual QP problem is defined as:
	
ÂÂƒÂš
à¢Ç¡à£…à®¹à«™à£¦áˆºà¢Ç¡ à£…áˆ»àµŒÍ³
Í´ à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ†à£…à¯áˆºà¡­à¢àµ†à¢ˆáˆ»
(4.32)
	
Subject to à¡½à¢àµ…à¢‰àµ†à¡­à¯à£…àµŒà«™
To obtain a solution, we first solve the constraint equation for à¢WRJHWà¢áˆºà£…áˆ»àµŒà¡½à¬¿à¬µáˆºà¡­à¯à£…àµ†à¢‰áˆ», and 
substitute it in the objective function to define the dual problem in terms of the dual function as:
	
ÂÂƒÂš
à£…à®¹à«™ß®áˆºà£…áˆ»àµŒÍ³
Í´ à£…à¯áˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à£…àµ…áˆºà¡­à¡½à¬¿à¬µà¢‰àµ…à¢ˆáˆ»à¯à£…àµ†Í³
Í´ à¢‰à¯à¡½à¬¿à¬µà¢‰ 
(4.33)
The gradient and Hessian of the dual function with respect to Î» are computed as: 
	

×ß®áˆºà£…áˆ»àµŒáˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à£…àµ…à¡­à¡½à¬¿à¬µà¢‰àµ…à¢ˆÇ¡
×à¬¶ß®áˆºà£…áˆ»àµŒà¡­à¡½à¬¿à¬µà¡­à¯
(4.34)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
64 
Mathematical Optimization
If the optimal point is a regular point, then matrix A has full row rank. Then, from FONC, the solution 
to the dual QP problem is given as:
à£…àµŒáˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à¬¿à¬µáˆºà¡­à¡½à¬¿à¬µà¢‰àµ…à¢ˆáˆ»
(4.35)
where a non-negative solution à£…àµ’à«™ has been assumed. 
As an example, we consider the following QP problem:
Example 4.6: quadratic optimization problem (Griva, Nash & Sofer, p.528)
Let the primal problem be defined as: ÂÂ‹Âà¯«İ‚áˆºİ”áˆ»àµŒİ”à¬¶VXEMHFWWRİ”àµ’Í³ Then, a solution to the primal 
problem is given as: İ”×› àµŒÍ³ 
The Lagrangian function for the problem is formulated as: à£¦áˆºİ”Ç¡ ß£áˆ»àµŒİ”à¬¶àµ…ß£áˆºÍ³ àµ†İ”áˆ» The resulting dual 
optimization problem is defined as:ÂÂƒÂš
à°’à®¹à¬´à£¦áˆºİ”Ç¡ ß£áˆ»àµŒİ”à¬¶àµ…ß£áˆºÍ³ àµ†İ”áˆ»VXEMHFWWR×à£¦áˆºİ”Ç¡ ß£áˆ»àµŒÍ´İ”àµ…ß£àµŒÍ² 
Eliminating the constraint redefines the dual problem as: ÂÂƒÂš
à°’à®¹à¬´ß®áˆºß£áˆ»àµŒß£àµ†
à°’à°®
à¬¸, with the solution: ß£×› àµŒÍ´
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Fundamental Engineering Optimization 
Methods
65 
Mathematical Optimization
We may note that the saddle point condition is satisfied in this case, i.e.,
ÂÂƒÂšà°’à®¹à¬´à£¦áˆºİ”×›Ç¡ ß£áˆ»àµŒÂÂƒÂš
à°’à®¹à¬´ß£àµ†
à°’à°®
à¬¸àµ‘à£¦áˆºİ”×›Ç¡ ß£×›áˆ»àµŒÍ³ àµ‘ÂÂ‹Â
à¯«à®¹à¬µà£¦áˆºİ”Ç¡ ß£×›áˆ»àµŒÂÂ‹Â
à¯«à®¹à¬µİ”à¬¶ 
with equality satisfied on both sides.
4.6.3	
Local Duality Concepts
The existence of strong duality is only assured in the case of convex optimization problems. Nonetheless, 
using second order Taylor series expansion, any function can be locally approximated by a convex 
function. This prompts us to explore the possibility that the Lagrangian function is locally convex, and 
that strong duality may be locally achieved. 
Towards this end, we consider the general optimization problem (4.15) with the Lagrangian function 
given by (4.17). Let x* denote a solution to the optimization problem; if x* is a regular point, then there 
exist unique Lagrange multipliers áˆºà¢›×›Ç¡ à¢œ×›áˆ» such that: ×à£¦áˆºà¢×›Ç¡ à¢›×›Ç¡ à¢œ×›áˆ»àµŒÍ² As discussed in 4.6.1 above, 
a local dual function for the problem can be defined via (4.26), and the dual optimization problem can 
be locally defined via (4.17). The local problem can be solved via application of FONC. Let áˆºà¢›×›Ç¡ à¢œ×›áˆ» be 
the optimal solution to the dual problem, then à£¦×›áˆºà¢›×›Ç¡ à¢œ×›áˆ»àµŒİ‚áˆºà¢×›áˆ»
We note that local duality is applicable to both convex and non-convex problems. For example, we can 
apply local duality to the QP problem, to write: 
à¢áˆºà£…áˆ»àµŒà¡½à¬¿à¬µáˆºà¡­à¯à£…àµ†à¢‰áˆ»Ç¡
×à¢àµŒà¡­à¯Ç¡
à£¦×›áˆºà£…áˆ»àµŒÍ³
Í´ à£…à¯áˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à£…àµ…áˆºà¡­à¡½à¬¿à¬µà¢‰àµ…à¢ˆáˆ»à¯à£…àµ†Í³
Í´ à¢‰à¯à¡½à¬¿à¬µà¢‰Ç¡
(4.36)
×à£¦×›áˆºà£…áˆ»àµŒàµ†à¡­à¡½à¬¿à¬µà¡­à¯à£…àµ…à¡­à¡½à¬¿à¬µà¢‰àµ…à¢ˆÇ¡ÂƒÂÂ†×à¬¶à£¦×›áˆºà£…áˆ»àµŒà¡­à¡½à¬¿à¬µà¡­à¯
As an example of a non-convex optimization problem we consider the following problem:
Example 4.7: Local duality
We consider the following optimization problem (Arora, p. 205):
ÂÂ‹Âà¯«İ‚áˆºİ”áˆ»àµŒàµ†İ”à¬µİ”à¬¶Ç¡VXEMHFWWRáˆºİ”à¬µàµ†Íµáˆ»à¬¶àµ…İ”à¬¶
à¬¶àµŒÍ·
The Lagrangian function and its derivative are given as:
à£¦áˆºà¢Ç¡ à£…áˆ»àµŒàµ†İ”à¬µİ”à¬¶àµ…ß£áˆºáˆºİ”à¬µàµ†Íµáˆ»à¬¶àµ…İ”à¬¶
à¬¶àµ†Í·áˆ»Ç¡ZKHUH
×à¯«à£¦áˆºà¢Ç¡ à£…áˆ»àµŒàµ¤àµ†İ”à¬¶àµ…Í´ß£áˆºİ”à¬µàµ†Íµáˆ»
àµ†İ”à¬µàµ…Í´ß£İ”à¬¶
àµ¨àµŒá‰‚Í´ß£
àµ†Í³
àµ†Í³
Í´ß£á‰ƒá‰‚İ”à¬µ
İ”à¬¶á‰ƒàµ†á‰‚Í¸ß£
Í² á‰ƒ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
66 
Mathematical Optimization
Solving the FONC together with the equality constraint, the optimum solution to the problem is given as:
à¢×› àµŒáˆºÍ¶Ç¡Í´áˆ»Ç¡ ß£×› àµŒÍ³Ç¡ İ‚×› àµŒàµ†Íº
The Hessian at the optimum point is computed as: ×à¬¶áˆºà¢×›Ç¡ ß£×›áˆ»àµŒá‰‚Í´
àµ†Í³
àµ†Í³
Í´ á‰ƒ and is positive definite. 
Therefore, using local duality theory, ×à¯«à£¦áˆºà¢Ç¡ à£…áˆ»àµŒÍ² is solved for x, which gives: á‰‚İ”à¬µ
İ”à¬¶á‰ƒàµŒ
à¬µ
à¬¸à°’à°®à¬¿à¬µàµ¤Í³Í´ß£à¬¶
Í¸ß£àµ¨ 
Next, the dual problem defined by ÂÂƒÂšà£…à®¹à«™à£¦áˆºà¢áˆºß£áˆ»Ç¡ ß£áˆ»àµŒ
à¬¸àµ«à°’à¬¾à°’à°¯à¬¿à¬¶à¬´à°’à°±àµ¯
áˆºà¬¸à°’à°®à¬¿à¬µáˆ»à°®
Ç¡ ß£àµàµ‡
à¬µ
à¬¶ which has a local 
solution at ß£×› àµŒÍ³ with à£¦áˆºà¢áˆºß£×›áˆ»Ç¡ ß£×›áˆ»àµŒàµ†Íº which matches the primal solution.
4.6.4	
Separable Problems
Dual methods are particularly powerful when applied to separable problems that are structured as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»àµŒà·İ‚à¯œáˆºİ”à¯œáˆ»
à¯œ

6XEMHFWWRÏƒ İƒà¯œáˆºİ”à¯œáˆ»
à¯œ
àµ‘Í²Ç¡ Ïƒ İ„à¯œáˆºİ”à¯œáˆ»àµŒÍ²
à¯œ

(4.37)
The dual function for the separable problem is formulated as:
ß®áˆºà¢›Ç¡ à¢œáˆ»àµŒÂÂ‹Â
à¢àµ­à·İ‚à¯œáˆºİ”à¯œáˆ»
à¯œ
àµ…İ‘à·İƒà¯œáˆºİ”à¯œáˆ»
à¯œ
àµ…İ’à·İ„à¯œáˆºİ”à¯œáˆ»
à¯œ
àµ±
which decomposes into m separate single-variable problems given as: ÂÂ‹Âà¯«à³”İ‚à¯œáˆºİ”à¯œáˆ»àµ…İ‘İƒà¯œáˆºİ”à¯œáˆ»àµ…İ’İ„à¯œáˆºİ”à¯œáˆ»
EO
E
L
O
which can be relatively easy to solve. Thus, the formulation of a dual problem becomes simple.
The next example shows how local duality can be applied to engineering problems that are separable.
Example 4.8: truss design problem (Belegundu and Chandrupatla, p. 274) 
A truss contains a total of 16 elements of length Ü®à¯œàµŒÍµÍ²Â‹ÂÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡Í³Í´Ç¢Ü®à¯œàµŒÍµÍ²Î¾Í´Â‹ÂÇ¡ İ…àµŒÍ³Í´Ç¡ Ç¥ Ç¡Í³Í¸ 
and cross-sectional area Dİ”à¯œ (design variable). The truss bears a load Ü²àµŒÍ´Í·Ç¡Í²Í²Í²OE at the tip. The 
weight of the structure is to be minimized with a bound on tip deflection, ßœàµ‘Í³ in. The problem is 
formulated as:
ÂÂ‹Â
à¢à·
Ü®à¯œİ”à¯œ
à¬µà¬º
à¯œà­€à¬µ

6XEMHFWWRÏƒ
á‰€
à¯–à³”
à¯«à³”àµ†ß™á‰
à¬µà¬º
à¯œà­€à¬µ
àµ‘Í²İ”à¯œàµ’İ”à¯œ
à¯…ZKHUHÜ¿à¯œàµŒ
à¯‰à¯…à³”à¯™à³”
à°®
à®¾à°‹à³†Ç¡ ß™àµŒ
à¬µ
à¬µà¬ºÇ¡ İ”à¯œ
à¯…àµŒÍ³Í²à¬¿à¬ºLQ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
67 
Mathematical Optimization
The dual function is defined as: ß®áˆºß¤áˆ»àµŒÂÂ‹Âà¯«à³”à®¹à¯«à³”
à²½Ïƒ
Ü®à¯œİ”à¯œ
à¬µà¬º
à¯œà­€à¬µ
àµ…ß¤á‰€
à¯–à³”
à¯«à³”àµ†ß™á‰
which leads to individual problems of the form: ÂÂ‹Âà¯«à³”à®¹à¯«à³”
à²½ß°àµŒÜ®à¯œİ”à¯œàµ…ß¤á‰€
à¯–à³”
à¯«à³”àµ†ß™á‰
Application of FONC gives: if and if İ”à¯œ
×› àµŒà¶§
à°“à¯–à³”
à¯…à³”LIÜ¿à¯œàµÍ²Ç¡DQGİ”à¯œ
×› àµŒİ”à¯œ
à¯…LIÜ¿à¯œàµŒÍ² 
The resulting dual maximization problem is defined as: ÂÂƒÂšà°“ß®áˆºß¤áˆ»àµŒÍ´ Ïƒ
à¶¥Ü¿à¯œÜ®à¯œÎ¾ß¤àµ†ß¤
à¬µà¬º
à¯œà­€à¬µ
àµ…Ü¿
where c is a constant. Application of FONC then gives: ß¤àµŒàµ«Ïƒ à¶¥Ü¿à¯œÜ®à¯œ
à¯œ
àµ¯
à¬¶ 
For the given data, a closed-form solution is obtained as: ß¤×› àµŒÍ³ÍµÍ·ÍºÇ¤Í´Ç¡ İ‚×› àµŒÍ³ÍµÍ·ÍºÇ¤Í´Â‹Âà¬·Ç¡
à¢àµŒáˆ¾Í·Ç¤Í¸Í¹Í¶Ç¤Í´Í·Í¶Ç¤Í´Í·Í´Ç¤ÍºÍ¶Í´Ç¤ÍºÍ¶Í³Ç¤Í¶Í´Í³Ç¤Í¶Í´Í³Í²à¬¿à¬ºÍ³Ç¤Í²Í¸Í³Ç¤Í²Í¸Í³Ç¤Í²Í¸Í³Í²à¬¿à¬ºÍ³Ç¤Í¹Í¹Í³Ç¤Í¹Í¹Í³Ç¤Í¹Í¹Í³Ç¤Í¹Í¹áˆ¿ in.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Fundamental Engineering Optimization 
Methods
68 
Linear Programming Methods
5	 Linear Programming Methods
Linear programming (LP) problems form an important subclass of the optimization problems. The 
distinguishing feature of the LP problems is that the objective function and the constraints are linear 
functions of the optimization variables. LP problems occur in many real-life economic situations where 
profits are to be maximized or costs minimized with constraints on resources. Specialized procedures, 
such as the Simplex method, were developed to solve the LPP. The simplex method divides the variables 
into basic and nonbasic, the latter being zero, in order to develop a basic feasible solution (BFS). It then 
iteratively updates basic variables thus generating a series of BFS, each of which carries a lower objective 
function value than the previous. Each time, the reduced costs associated with nonbasic variables are 
inspected to check for optimality. An optimum is reached when all the reduced costs are non-negative.
Learning Objectives: The learning objectives in this chapter are:
1.	 Understand the general formulation of a linear programming (LP) problem 
2.	 Learn the Simplex method to solve LP problems and its matrix/tableau implementation 
3.	 Understand the fundamental duality properties associated with LP problems
4.	 Learn sensitivity analysis applied to the LP problems
5.	 Grasp the formulation of KKT conditions applied to linear and quadratic programming 
problems
6.	 Learn to formulate and solve the linear complementarity problem (LCP)
5.1	
The Standard LP Problem
The general LP problem is described in terms of minimization (or maximization) of a scalar objective 
function of n variables, that are subject to m constraints. These constraints may be specified as EQ 
(equality constraints), GE (greater than or equal to inequalities), or LE (less than or equal to inequalities). 
The variables themselves may be unrestricted in range, specified to be non-negative, or upper and/or 
lower bounded. 
Mathematically, the LP problem is expressed as:
ÂÂ‹Â
à¯«à³•áˆºRUÂÂƒÂš
à¯«à³•áˆ» İ–àµŒÏƒ
Ü¿à¯İ”à¯
à¯¡
à¯à­€à¬µ

6XEMHFWWRÏƒ
Ü½à¯œà¯İ”à¯áˆºàµ‘Ç¡ àµŒÇ¡ àµ’áˆ»Ü¾à¯œ
à¯¡
à¯à­€à¬µ
Ç¡İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İ‰

İ”à¯àµ’İ”à¯à¯…IRUVRPHİ†İ”à¯àµ‘İ”à¯à¯IRUVRPHİ†İ”à¯IUHHIRUVRPHİ†
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
69 
Linear Programming Methods
While the general LP problem may be specified in different ways, the standard LP problem refers to 
a problem involving minimization of a scalar cost function, subject to only equality constraints and 
with optimization variables restricted to take on non-negative values. The inequality constraints can 
be converted to equality by adding (subtracting) slack (surplus) variables to LE (GE) type constraints. 
Further, the original variables can be replaced by new variables, which take on non-negative values. The 
standard LP problem is defined as:
ÂÂ‹Â
à¯«à³” İ–àµŒà·
Ü¿à¯İ”à¯
à¯¡
à¯à­€à¬µ

6XEMHFWWRÏƒ
Ü½à¯œà¯İ”à¯àµŒÜ¾à¯œ
à¯¡
à¯à­€à¬µ
Ç¡ İ”à¯àµ’Í²Ç¢ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İ‰

(5.1)
The standard LP problem thus has the following characteristics:
1.	 It involves minimization of a scalar cost function. 
2.	 The variables can only take on non-negative values, i.e., İ”à¯œàµ’Í²
3.	 The r.h.s. is assumed to be non-negative, i.e., Ü¾à¯àµ’Í²
Additionally, 
1.	 The constraints are assumed to be linearly independent, which implies that İÜ½İŠİ‡áˆºà¡­áˆ»àµŒİ‰
2.	 The problem is assumed to be well-formulated, which implies that ÂÂ‹Âà¢à¢‰à¯à¢àµÎ»
In the vector-matrix format, the standard LP problem is expressed as:
ÂÂ‹Â
à¢İ–àµŒà¢‰à¯à¢
(5.2)
	
   subject to à¡­à¢àµŒà¢ˆÇ¡ à¢àµ’à«™
where à¡­× Ô¹à¯ àµˆà¯¡Ç¢ à¢Ç¡ à¢‰× Ô¹à¯¡Ç¡ à¢ˆ× Ô¹à¯  
When encountered, exceptions to the standard LP problem formulation are dealt as follows:
1.	 A maximization problem is changed to a minimization problem by taking negative of the 
cost function, i.e.,ÂÂƒÂšà¢à¢‰à¯à¢Ø  ÂÂ‹Âà¢áˆºàµ†à¢‰à¯à¢áˆ» 
2.	 Any constant terms in z can be dropped.
3.	 Any İ”à¯œ× Ô¹ (unrestricted in sign) is replaced by İ”à¯œàµŒİ”à¯œ
à¬¾àµ†İ”à¯œ
à¬¿ZKHUHİ”à¯œ
à¬¾Ç¡ İ”à¯œ
à¬¿àµ’Í² 
4.	 The inequality constraints are converted to equality constraints by the addition of slack 
variables (to LE constraint) or subtraction of surplus variables (from GE constraint).
5.	 If Ü¾à¯àµÍ², the constraint is first multiplied by â€“1, followed by the introduction of slack or 
surplus variables.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
70 
Linear Programming Methods
5.2	
The Basic Solution to the LP Problem
We first note that the feasible set defined by linear equalities (and inequalities) in an LP problem is 
convex. In addition, the cost function is linear, hence convex. Therefore, the LP problem represents a 
convex optimization problem and a single global minimum for the problem exists. 
Further, due to only equality constraints present in the problem, the optimum solution, if it exists, lies on 
the boundary of the feasible region. This is also true in the case of inequality constraints, since if none 
of the constraints is active, the application of first order optimality conditions to the problem would 
result in Ü¿à¯œàµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠLH there will be no optimal solution.
Algebraically, the LP problem represents a linear system of m equations in n unknowns. Accordingly,
a)	 If İ‰ àµŒİŠ the solution may be obtained from the constraints only.
b)	 If İ‰ àµİŠ some of the constraints may be redundant, or the system may be inconsistent.
c)	 If İ‰ àµİŠ the LP problem has an optimum solution, and can be solved using methods 
described below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Fundamental Engineering Optimization 
Methods
71 
Linear Programming Methods
Next, we consider the İ‰ àµİŠ case, and assume that matrix A has full row rank; then, we arbitrarily 
choose independent (nonbasic) variables, to solve for the remaining (m) dependent (basic) variables. Let 
the system be transformed into canonical form:à¡µáˆºà¯ áˆ»à¢áˆºà¯ áˆ»àµ…à¡½à¢áˆºà¯¡à¬¿à¯ áˆ»àµŒà¢ˆÇ¢ then, the general solution 
includes áˆºİŠàµ†İ‰áˆ» independent variables: à¢áˆºà¯¡à¬¿à¯ áˆ» and (m) dependent variables: à¢áˆºà¯ áˆ»àµŒà¢ˆàµ†à¡½à¢áˆºà¯¡à¬¿à¯ áˆ»$
particular solution to the linear system can be obtained by setting: à¢áˆºà¯¡à¬¿à¯ áˆ»àµŒà«™, and obtaining: à¢áˆºà¯ áˆ»àµŒà¢ˆ
A basic solution x to a standard LP problem satisfies two conditions:
1.	 à¢ is a solution to à¡­à¢àµŒà¢ˆ
2.	 The columns of A corresponding to the nonzero components of x are linearly independent.
Since A can have at the most m independent columns, it implies that A has at the most m nonzero 
components. When A has a full row rank, a basic solution is obtained by choosing İŠàµ†İ‰ variables as 
zero. The resulting solution, x, contains m basic variables, à¢à¡®, and İŠàµ†İ‰ nonbasic variables, à¢à¡º the 
latter taking on zero values. The columns of A corresponding to à¢à¡® are termed as the basis vectors.
The set à£­àµŒáˆ¼à¢Ç£ à¡­à¢àµŒà¢ˆÇ¡ à¢àµ’à«™áˆ½U represents the feasible region for the LP problem. We note that a 
basic solution, à¢× à£­ that is in the feasible region is termed as a basic feasible solution (BFS). Further, 
the feasible region is a polytope (polygon in n dimensions), and each BFS represents an extreme point 
(a vertex) of the polytope.
Let à¢à¯àµŒáˆ¾à¢à®»Ç¡ à¢à¯‡áˆ¿ where the basic variables occupy leading positions; we accordingly partition the cost 
function coefficients as: à¢‰à¯àµŒáˆ¾à¢‰à®»Ç¡ à¢‰à¯‡áˆ¿ and represent the constraint matrix as: à¡­àµŒáˆ¾à¡®Ç¡ à¡ºáˆ¿Ç¡ where B is a 
İ‰àµˆİ‰ nonsingular matrix and N is a Dİ‰àµˆáˆºİŠàµ†İ‰áˆ» then, the original LP problem is reformulated as: 
ÂÂ‹Â
à¢İ–àµŒà¢‰à®»
à¯à¢à®»àµ…à¢‰à¯‡
à¯à¢à¯‡Ç¡
6XEMHFWWRà¡­à¢àµŒáˆ¾à¡®à¡ºáˆ¿á‰‚à¢à®»
à¢à¯‡á‰ƒàµŒà¢ˆÇ¡ à¢à®»àµ’à«™Ç¡ à¢à¯‡àµ’à«™
(5.3)
Then, a BFS corresponding to basis B is represented as: à¢à¯àµŒáˆ¾à¢à®»Ç¡ à«™áˆ¿Ç¡ à¢à¡®àµŒà¡®à¬¿à¬µà¢ˆàµà«™ Since, by 
assumption, İÜ½İŠİ‡áˆºà¡­áˆ»àµŒİ‰à¡®, can be selected from the various permutations of the columns of A. 
Further, since each basic solution has exactly m non-zero components, the total number of basic solutions 
is finite, and is given as: á‰€İŠ
İ‰á‰àµŒ
à¯¡Ç¨
à¯ Ç¨áˆºà¯¡à¬¿à¯ áˆ»Ç¨ The number of BFS is smaller than number of basic solutions 
and can be determined by comparing the objective function values at the various basic solutions.
The Basic Theorem of Linear Programming (e.g., Arora, p. 201) states that if there is a feasible solution 
to the LP problem, there is a BFS; and if there is an optimum feasible solution, there is an optimum 
BFS. The basic LP theorem implies that an optimal BFS must coincide with one of the vertices of the 
feasible region. This fact can be used to compare the objective function value at all BFSs, and find the 
optimum by comparison if the number of vertices is small. Finally, there can also be multiple optimums 
if an active constraint boundary is parallel to the level curves of the cost function.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
72 
Linear Programming Methods
5.3	
The Simplex Method
The simplex method iteratively solves the standard LP problem. It does so by starting from a known BFS 
and successively moving to an adjacent BFS that carries a lower objective function value. Each move 
involves replacing a single variable in the basis with a new variable, such that the objective function 
value decreases. The previously nonbasic variable entering the basis is termed as entering basic variable 
(EBV), and the one leaving it is termed as leaving basic variable (LBV). An optimum is reached when 
no neighboring BFS with a lower objective function value can be found.
5.3.1	
The Simplex Algorithm
In order to mathematically formulate the simplex algorithm, let à¢à¯àµŒáˆ¾à¢à¡®Ç¡ à¢à¡ºáˆ¿ represent a non-basic 
solution to the LP problem, and let the constraints be expressed as: à¡®à¢à®»àµ…à¡ºà¢à¯‡àµŒà¢ˆ Then, we can 
solve for à¢à®»DVà¢à®»àµŒà¡®à¬¿à¬µáˆºà¢ˆàµ†à¡ºà¢à¯‡áˆ»Ç¡ and substitute it in the objective function to obtain: 
İ–àµŒà¢‰à®»
à¯à¡®à¬¿à¬µà¢ˆàµ…áˆºà¢‰à¯‡
à¯àµ†à¢‰à®»
à¯à¡®à¬¿à¬µà¡ºáˆ»à¢à¯‡àµŒà¢Ÿà¯à¢ˆàµ…à¢‰à·œà¯‡
à¯à¢à¯‡àµŒİ–Æ¸ àµ…à¢‰à·œà¯‡
à¯à¢à¯‡
(5.4)
where à¢Ÿà¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µ defines a vector of simplex (Lagrange) multipliers, à¢‰à·œà¯‡
à¯àµŒà¢‰à¯‡
à¯àµ†à¢Ÿà¯à¡º represents the 
reduced costs for the nonbasic variables (reduced costs are zero for the basic variables), and İ–Æ¸ àµŒà¢Ÿà¯à¢ˆ 
represents the objective function value corresponding to a basic solution, where İ•à¯œàµÍ² represents an 
active constraint.
The significance of the reduced costs is as follows: let Ü¿Æ¸à¯×à¢‰à·œà¯‡
à¯ then, we note that assigning a nonbasic 
variable İ”à¯ a nonzero value ßœà¯ will change the objective function by Ü¿Æ¸à¯ßœà¯ Therefore, any Ü¿Æ¸à¯àµÍ² has 
the potential to decrease the value of I İ–, and the corresponding İ”à¯ may be selected as the EBV. It is 
customary to select the variable İ”à¯ with the lowest Ü¿Æ¸à¯ as the EBV.
To select an LBV, we examine the update to the basic solution from the introduction of EBV, given 
as: à¢à®»àµŒà¢ˆà·¡àµ†à¡­à·¡à¯¤İ”à¯¤ZKHUHà¢ˆà·¡àµŒà¡®à¬¿à¬µà¢ˆÇ¡à¡­à·¡à¯¤àµŒà¡®à¬¿à¬µà¡­à¯¤Ç¡ and à¡­à¯¤ represents the İWK column of à¡­ Then 
İ”à¯¤, can be increased so long as à¢à®»àµ’à«™Ç¢ element wise considerations require that: Ü¾à· à¯œàµ†Ü£áˆ˜à¯œÇ¡à¯¤İ”à¯¤àµÍ² 
Therefore, the maximum value of İ”à¯¤LVİ”Ò§à¯¤àµŒÂÂ‹Âà¯œàµœ
à¯•à· à³”
à®ºà· à³”Ç¡à³œÇ£ Ü£áˆ˜à¯œÇ¡à¯¤àµÍ²àµ  is , and the variable corresponding 
to the lowest ratio from this ratio test is picked as LBV. 
The steps involved in the Simplex algorithm are summarized below.
The Simplex Algorithm (Griva, Nash & Sofer, p.131):
1.	 Initialize: Find an initial BFS to start the algorithm; accordingly, determine à¢à¡®Ç¡ à¢à¡ºÇ¡ à¡®Ç¡ à¡ºÇ¡  
à¢Ÿà¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µÇ¡İ–Æ¸ àµŒà¢Ÿà¯à¢ˆ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
73 
Linear Programming Methods
2.	 Optimality test: Compute à¢ˆà·¡àµŒà¡®à¬¿à¬µà¢ˆÇ¡ à¢‰à·œà¯‡
à¯àµŒà¢‰à¯‡
à¯àµ†à¢Ÿà¯à¡º7 Then, evaluate à¢‰à·œà¯‡
à¯ associated with 
current nonbasic variables. If all Ü¿Æ¸à¯àµÍ² the optimal has been reached. Otherwise, select a 
variable İ”à¯¤ with Ü¿Æ¸à¯¤àµÍ² as EBV.
3.	 Ratio test: Compute à¡­à·¡à¯¤àµŒà¡®à¬¿à¬µà¡­à¯¤ Determine: ÂÂ‹Âà¯œàµœ
à¯•à· à³”
à®ºà· à³”Ç¡à³œÇ£ Ü£áˆ˜à¯œÇ¡à¯¤àµÍ²àµ àµŒ
à¯•à· à³›
à®ºà· à³›Ç¡à³œ6HWÜ£áˆ˜à¯£Ç¡à¯¤ Set as 
the pivot element. 
4.	 Update: Assign İ”à¯¤Õš
à¯•à· à³›
à®ºà· à³›Ç¡à³œÇ¡ à¢à®»Õš à¢ˆà·¡àµ†Ü£áˆ˜à¯¤İ”à¯¤Ç¡ İ–Æ¸ Õš İ–Æ¸ àµ…Ü¿Æ¸à¯¤İ”à¯¤ Update à¢à¡®Ç¡ à¢à¡ºÇ¡ à¡®Ç¡ à¡º
The following example illustrates the application of simplex algorithm to LP problems.
Example 5.1: The Simplex algorithm
We consider the following LP problem:

ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍ´İ”à¬µàµ…İ”à¬¶àµ‘Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¢İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í²
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
74 
Linear Programming Methods
The problem is first converted to standard LP form by changing the sign of the objective function and 
adding slack variables İà¬µÇ¡ İà¬¶ to the LE constraints. The resulting optimization variables, cost coefficients, 
and constraint coefficients are given below:
à¢à¯àµŒáˆ¾İ”à¬µ
İ”à¬¶
İà¬µ
İà¬¶áˆ¿Ç¡
à¢‰à¯àµŒáˆ¾àµ†Íµ àµ†Í´Í²Í²áˆ¿Ç¡
à¡­àµŒá‰‚Í´
Í´Í³
ÍµÍ³
Í²Í²
Í³á‰ƒÇ¡
à¢ˆàµŒá‰‚Í³Í´
Í³Í¸á‰ƒ
The steps of the Simplex algorithm for the problem are shown below:
Step 1:
à¢à®»àµŒá‰‚İà¬µ
İà¬¶á‰ƒàµŒá‰‚Í³Í´
Í³Í¸á‰ƒÇ¡ à¢à¯‡àµŒá‰‚İ”à¬µ
İ”à¬¶á‰ƒàµŒá‰‚Í²
Í²á‰ƒÇ¡ à¢‰à®»
à¯àµŒáˆ¾Í²Ç¡Í²áˆ¿Ç¡ à¢‰à¯‡
à¯àµŒáˆ¾àµ†ÍµÇ¡ àµ†Í´áˆ¿Ç¡ à¡®àµŒà¡µÇ¡ à¢ˆà·¡àµŒà¢ˆàµŒá‰‚Í³Í´
Í³Í¸á‰ƒÇ¡İ–Æ¸ àµŒÍ²Ç¡
àµœ
àµ 
à¢à®»àµŒá‰‚İà¬µ
İà¬¶á‰ƒàµŒá‰‚Í³Í´
Í³Í¸á‰ƒÇ¡ à¢à¯‡àµŒá‰‚İ”à¬µ
İ”à¬¶á‰ƒàµŒá‰‚Í²
Í²á‰ƒÇ¡ à¢‰à®»
à¯àµŒáˆ¾Í²Ç¡Í²áˆ¿Ç¡ à¢‰à¯‡
à¯àµŒáˆ¾àµ†ÍµÇ¡ àµ†Í´áˆ¿Ç¡ à¡®àµŒà¡µÇ¡ à¢ˆà·¡àµŒà¢ˆàµŒá‰‚Í³Í´
Í³Í¸á‰ƒÇ¡İ–Æ¸ àµŒÍ²Ç¡
àµœ
àµ 
Update:İ”à¬µÕš Í¸Ç¡ à¢à®»Õš á‰‚Í²
Í¶á‰ƒÇ¡ İ–Æ¸ Õš àµ†Í³Íº 
Step 2: 
S
à¢à®»àµŒá‰‚İ”à¬µ
İà¬¶á‰ƒàµŒá‰‚Í¸
Í¶á‰ƒÇ¡ à¢à¯‡àµŒá‰‚İ”à¬¶
İà¬µá‰ƒàµŒá‰‚Í²
Í²á‰ƒÇ¡ à¢‰à®»
à¯àµŒáˆ¾àµ†ÍµÇ¡ Í²áˆ¿Ç¡ à¢‰à¯‡
à¯àµŒáˆ¾àµ†Í´Ç¡ Í²áˆ¿Ç¡ à¡®àµŒá‰‚Í´
Í´Í²
Í³á‰ƒÇ¡ à¡ºàµŒá‰‚Í³
ÍµÍ³
Í²á‰ƒÇ¡ à¢ˆà·¡àµŒá‰‚Í¸
Í¶á‰ƒÇ¡
à¢Ÿà¯àµŒáˆ¾àµ†ÍµÈ€Í´Ç¡ Í²áˆ¿Ç¡ à¢‰à·œà¯‡
à¯àµŒáˆ¾àµ†Í³È€Í´Ç¡ ÍµÈ€Í´áˆ¿Ç¡ İ”à¯¤àµŒİ”à¬¶Ç¡ Ü£áˆ˜à¬µàµŒá‰‚Í³È€Í´
Í´ á‰ƒÇ¡ àµœ
à¯•à· à³”
à®ºà· à³”Ç¡à°­Ç£ Ü£áˆ˜à¯œÇ¡à¬µàµÍ²àµ àµŒáˆ¼Í³Í´Ç¡ Í´áˆ½Ç¡ Ü£áˆ˜à¯£Ç¡à¯¤àµŒÜ£áˆ˜à¬¶Ç¡à¬¶
 
Update: İ”à¬¶Õš Í´Ç¡ à¢à®»Õš á‰‚Í·
Í²á‰ƒÇ¡ İ–Æ¸ Õš àµ†Í³Í»
Step 3: 
à¢à®»àµŒá‰‚İ”à¬µ
İ”à¬¶á‰ƒàµŒá‰‚Í·
Í´á‰ƒÇ¡ à¢à¯‡àµŒá‰‚İà¬µ
İà¬¶á‰ƒàµŒá‰‚Í²
Í²á‰ƒÇ¡ à¢‰à®»
à¯àµŒáˆ¾àµ†ÍµÇ¡ àµ†Í´áˆ¿Ç¡ à¢‰à¯‡
à¯àµŒáˆ¾Í²Ç¡ Í²áˆ¿Ç¡ à¡®àµŒá‰‚Í´
Í´Í³
Íµá‰ƒÇ¡ à¡ºàµŒà¡µÇ¡
à¢Ÿà¯àµŒáˆ¾àµ†Í·È€Í¶Ç¡ àµ†Í³È€Í¶áˆ¿Ç¡ à¢‰à·œà¯‡
à¯àµŒáˆ¾Í·È€Í¶Ç¡ Í³È€Í¶áˆ¿
Since all Ü¿Æ¸à¯àµÍ² an optimal has been reached and İ–à¯¢à¯£à¯§àµŒàµ†Í³Í»
5.3.2	
Tableau Implementation of the Simplex Algorithm
It is customary to use tableaus to capture the essential information about the LP problem. A tableau is an 
augmented matrix that includes the constraints matrix, the right-hand-side (rhs), and the coefficients of 
the cost function (represented in the last row). Each preceding row of the tableau represents a constraint 
equation, and each column represents a variable. The tableau method implements the simplex algorithm 
by iteratively computing the inverse of the basis (B) matrix. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
75 
Linear Programming Methods
We consider a standard linear program with n variables and m equality constraints, and assume that at the 
current iteration the vectors of basic and nonbasic variables are represented as İ”à®» and İ”à¯‡, respectively. 
Then, the original linear program corresponds to the following tableau:
%DVLF
à¢à¡®
à¢à¡º
5KV
à¢à¡®
à¡®
à¡º
à¢ˆ
àµ†à¢ 
à¢‰à®»
à¯
à¢‰à¯‡
à¯
Í²
In the above, basic variables are identified in the left-most column, the next columns m pertain to basis 
vectors and the right-most column represents the rhs. By pre-multiplying the tableau with the matrix: 
J
àµ¤à¡®à¬¿à¬µ
Í²
àµ†à¢Ÿà¯
Í³àµ¨Ç¡ à¢Ÿà¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µÇ¡ we obtain the tableau representation in the current basis:
%DVLF
à¢à¡®
à¢à¡º
5KV
à¢à¡®
à¡µ
à¡®à¬¿à¬µà¡º
à¡®à¬¿à¬µà¢ˆ
àµ†à¢ 
à«™
à¢‰à¯‡
à¯àµ†à¢Ÿà¯à¡º
àµ†à¢Ÿà¯à¢ˆ

where à¢Ÿà¯ represents the vector of Lagrange multipliers, à¢‰à¯‡
à¯àµ†à¢Ÿà¯à¡º represents the reduced costs, and 
à¢Ÿà¯à¢ˆ represents the current objective function value. 
The simplex iteration begins with the optimality test: we inspect the reduced costs for the nonbasic 
variables (represented in the last row under nonbasic variables), and pick the variable with most negative 
reduced cost (or another variable with negative reduced cost) as EBV. Next, we use the ratio test to 
determine LBV from the current basis. Once the pivot element has been identified, Gaussian elimination 
steps (elementary row operations) are completed to reduce the EBV column to a unit vector, effectively 
replacing LBV with EBV in the current basis. This completes an iteration of the simplex method. The 
process is then repeated till all the reduced costs are non-negative, thus signifying that an optimum has 
been reached. 
We note that only the original matrices: à¡­Ç¡ à¢ˆÇ¡ à¢‰ and the inverse of current basis matrix [à¡®à¬¿à¬µáˆ» are needed to 
compute the remaining entries in the current tableau. Further, only the EBV column among the nonbasic 
variable columns needs to be computed. These facts are useful when developing a computationally 
efficient implementation of the simplex method. Additionally, some mechanism is needed to update 
the representation of the inverse matrix for the next iteration. Computationally efficient ways of doing 
so are discussed in (Griva, Nash & Sofer, Ch. 7).
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
76 
Linear Programming Methods
Some abnormal terminations of the Simplex algorithm are described as follows: 
1.	 If the reduced cost for a nonbasic variable in the final tableau is zero, then there exists a 
possibility for multiple optimum solutions with equal cost function value. This happens 
when cost function contours (level curves) are parallel to one of the constraint boundaries.
2.	 If the reduced cost is negative but the pivot step cannot be completed due to all coefficients 
in the LBV column being negative, it reveals a situation where the cost function is 
unbounded below. 
3.	 If, at some point during Simplex iterations, a basic variable attains a zero value (i.e., the 
rhs has a zero), it is called degenerate variable and the corresponding BFS is termed as 
degenerate solution, as the degenerate row hence forth becomes the pivot row, with no 
improvement in the objective function. While it is theoretically possible for the algorithm to 
fail by cycling between two degenerate BFSs, this is not known to happen in practice.
An example for tableau implementation of the simplex method is presented below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Fundamental Engineering Optimization 
Methods
77 
Linear Programming Methods
Example 5.2: the Tableau method
As an example of the tableau method, we resolve example 5.1 using tableaus, where the optimization 
problem is stated as: 
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍ´İ”à¬µàµ…İ”à¬¶àµ‘Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¢İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í²
The problem is first converted to the standard LP form. The constraints and cost function coefficients 
were entered in an initial simplex tableau, where the EBV, LBV, and the pivot element are identified 
underneath the tableau:

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› 5KV
à¢™à«š





à¢™à«›





à¢ 




Í²
(%9İ”à¬µ/%9Â•à¬µSLYRW
The subsequent simplex iterations result in the series of tableaus appearing below:
%DVLF à¢à«š
à¢à«›
à¢™à«š
à¢™à«› UKV
à¢à«š





à¢™à«›





àµ†à¢ 

 


(%9İ”à¬¶/%9Â•à¬¶SLYRW

%DVLF à¢à«š à¢à«›
à¢™à«š
à¢™à«›
UKV
à¢à«š


 

à¢à«›





àµ†à¢ 





At this point, since all reduced costs are positive, an optimum has been reached with: 
İ”à¬µ
×› àµŒÍ·Ç¡ İ”à¬¶
×› àµŒÍ´Ç¡ İ–à¯¢à¯£à¯§àµŒàµ†Í³Í»
5.3.1	
Final Tableau Properties
The final tableau from the simplex algorithm has certain fundamental properties that relate to the initial 
tableau. To reveal those properties, we consider the following optimization problem:
ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢
6XEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ à¢àµ’à«™
(5.5)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
78 
Linear Programming Methods
Adding surplus variables to the constraints results in the following standard LP problem:
ÂÂ‹Â
à¢İ–àµŒàµ†à¢‰à¯à¢
6XEMHFWWRà¡­à¢àµ…à¡µà¢™àµŒà¢ˆÇ¡ à¢àµ’à«™

(5.6)
An initial tableau for this problem is given as:
%DVLF
à¢
à¢™
5KV
à¢™
à¡­
à¡µ
à¢ˆ
àµ†à¢ 
àµ†à¢‰à¯
à«™
Í²
Assuming that the same order of the variables is maintained, then at the termination of the Simplex 
algorithm the final tableau is given as:
%DVLF
à¢
à¢™
UKV
à¢à¡®
à¡­à·©
à¡¿
à¢ˆà·©
àµ†à¢ 
à¢‰à·œà¯
à¢Ÿà¯
İ–×›
We note that the coefficients in the final tableau are related to those in the initial tableau as follows: 

à¡­à·©àµŒà¡¿à¡­Ç¡à¢ˆà·©àµŒà¡¿à¢ˆÇ¡
à¢‰à·œà¯àµŒà¢Ÿà¯à¡­àµ†à¢‰à¯Ç¡
İ–×› àµŒà¢Ÿà¯à¢ˆ
(5.7)
Thus, given the initial tableau à¡­Ç¡ à¢ˆÇ¡ à¢‰à¯ and the final coefficients in the slack variable columns: à¢Ÿà¯Ç¡à¡¿ 
we can reconstruct the final tableau as:
áˆ¾Ü¶Ü½Ü¾áˆ¿ÂˆÂ‹ÂÂƒÂ àµŒàµ¤à¡¿
à«™
à¢Ÿà¯
Í³àµ¨áˆ¾Ü¶Ü½Ü¾áˆ¿Â‹ÂÂ‹Â–Â‹ÂƒÂ
(5.8)
Therefore, in a computer implementation of the Simplex algorithm, only the coefficients: à¡­Ç¡ à¢ˆÇ¡ à¢‰à¯Ç¡ à¢Ÿà¯Ç¡ à¡¿ 
need to be stored in order to recover the final tableau when the algorithm terminates. 
5.3.2	
Obtaining an Initial BFS
The starting point of the simplex algorithm is a valid BFS. This is trivial in the case of a maximization 
problems modeled with LE constraints (Example 5.1), where an obvious initial BFS is to choose the 
slack variables as the basic variables. Initial BFS is not so obvious when the problem involves GE or EQ 
constraints. It is so because the feasible region in the problem does not normally include the origin. Then, 
in order to initiate the simplex algorithm, we need to choose an initial B matrix, such that à¡®à¢à®»àµŒà¢ˆ 
yields a non-negative solution for à¢à®» The two-phase Simplex method described below obtains an initial 
BFS by first solving an auxiliary LP problem.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
79 
Linear Programming Methods
The two-phase Simplex method works as follows: we add a set of İ‰ auxiliary variables à¢à·¥Ç¡ to the original 
optimization variables x, and define an auxiliary LP problem where the auxiliary objective function is 
selected to reduce the auxiliary variables. The auxiliary problem is defined as:
ÂÂ‹Â
à¯«à·¤à³”à·
İ”à·¤à¯œ
à¯ 
à¯œà­€à¬µ

6XEMHFWWRáˆ¾à¡­à¡µáˆ¿á‰‚à¢
à¢à·¥á‰ƒàµŒà¢ˆÇ¡ à¢àµ’à«™Ç¡ à¢à·¥àµ’à«™

(5.9)
The auxiliary problem is solved in Phase I of the Simplex algorithm. We note that à¢à·¥àµŒà¢ˆ is a valid BFS 
for the auxiliary problem and serves as a starting point for Phase I Simplex algorithm. Further, since 
only the GE and EQ constraints require auxiliary variables, their number can be accordingly chosen 
less than or equal to İ‰. 
The starting tableau for the Phase I Simplex algorithm is given as:

%DVLF
à¢à¡®
à¢à¡º
à¢à·¥
5KV
à¢à¡®
à¡®
à¡º
à¡µ
à¢ˆ
àµ†à¢ 
à¢‰à®»
à¯
à¢‰à¯‡
à¯
à«™
à«™
àµ†à¢ à¢‡
à«™
à«™
à«šà¢€
à«™
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Fundamental Engineering Optimization 
Methods
80 
Linear Programming Methods
where à«šà¯àµŒáˆ¾Í³Ç¡ Ç¥ Ç¡Í³áˆ¿ represents a unit vector. The first step in the Phase I Simplex is to make auxiliary 
variables the basic variables. This is done by row reductions aimed to generate unit vectors in the basis 
columns, which results in the following tableau:

%DVLF
à¢à¡®
à¢à¡º
à¢à·¥
5KV
à¢à¡®
à¡®
à¡º
à¡µ
à¢ˆ
àµ†à¢ 
à¢‰à®»
à¯
à¢‰à¯‡
à¯
à«™
Í²
àµ†à¢ à¢‡
àµ†à«šà¯à¡®
àµ†à«šà¯à¡º
à«™
àµ†à«šà¯à¢ˆ
The Phase I Simplex algorithm continues till all the reduced costs in the auxiliary objective row become 
non-negative and the auxiliary objective function value reduces to zero, thus signaling the end of Phase I. 
If the auxiliary objective value at the end of Phase I does not equal zero, it indicates that no feasible 
solution to the original problem exists. 
Once the auxiliary problem has been solved, we turn to the original problem, and drop the auxiliary 
objective İ–à¯”áˆ» row and the auxiliary variable à¢à·¥ columns from the current tableau (or ignore them). 
We then follow up with further Simplex iterations in Phase II of the algorithm till an optimum value 
for z is obtained. 
Two examples involving GE and EQ constraints are solved below to illustrate the implementation of the 
two-phase Simplex algorithm. 
Example 5.3: Two-phase Simplex algorithm for GE constraints
We consider the following LP problem:
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍµİ”à¬µàµ…Í´İ”à¬¶àµ’Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¡ İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í² 
We first convert the problem to standard form by subtracting surplus variable (s1) from GE constraint 
and adding slack variable (s2) to LE constraint. The standard form LP problem is given as:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ–àµŒàµ†Íµİ”à¬µàµ†Í´İ”à¬¶
6XEMHFWWRÍµİ”à¬µàµ…Í´İ”à¬¶àµ†Â•à¬µàµŒÍ³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ…Â•à¬¶àµŒÍ³Í¸Ç¢İ”à¬µÇ¡ İ”à¬¶Ç¡ İà¬µÇ¡ İà¬¶àµ’Í²
There is no obvious BFS to start the simplex algorithm. To solve the problem using two-phase simplex 
method, we add an auxiliary variable a1 to GE constraint and define the following auxiliary LP problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ–à¯”àµŒÜ½à¬µ
6XEMHFWWRÍµİ”à¬µàµ…Í´İ”à¬¶àµ†İà¬µàµ…Ü½à¬µàµŒÍ³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ…İà¬¶àµŒÍ³Í¸Ç¢İ”à¬µÇ¡ İ”à¬¶Ç¡ İà¬µÇ¡ İà¬¶Ç¡ Ü½à¬µàµ’Í²
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
81 
Linear Programming Methods
The starting tableau for the auxiliary problem (Phase I) is given as:
%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢‡à«š 5KV







à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






We first a1 bring into the bases by reducing the a1 column to a unit vector. 

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢‡à«š 5KV
à¢™à«š






à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






(%9İ”à¬µ/%9İà¬µSLYRW
The above step is followed by an additional Simplex iteration to reach the end of Phase I. The resulting 
final tableau for phase I is shown below:

%DVLF à¢à«š
à¢à«›
à¢™à«š
à¢™à«›
à¢‡à«š
5KV
à¢à«š

 



à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






Since the auxiliary variable is now nonbasic and the auxiliary objective has a zero value, the auxiliary 
problem has been solved. To turn to the original problem, we drop the İ–à¯” row and the  a1 column from the 
tableau. This results in the tableau below, which represents a valid BFS:İ”à¬µàµŒÍ¶Ç¡ İà¬¶àµŒÍºW to start Phase II.

%DVLF à¢à«š
à¢à«›
à¢™à«š
à¢™à«› 5KV
à¢à«š

 


à¢™à«›





àµ†à¢ 





(%9Â•à¬µ/%9Â•à¬¶SLYRW
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
82 
Linear Programming Methods
Phase II: To continue, we perform another iteration of the Simplex algorithm leading to the final tableau:

%DVLF à¢à«š
à¢à«›
à¢™à«š
à¢™à«›
5KV
à¢à«š





à¢™à«š





àµ†à¢ 





At this point the original LP problem has been solved and the optimal solution is given as: 
İ”à¬µ
×› àµŒÍºÇ¡ İ”à¬¶
×› àµŒÍ²Ç¡ İ–×› àµŒàµ†Í´Í¶
The second example of the two-phase simplex method involves equality constraints and bounds on the 
optimization variables. 
Example 5.4: Two-phase Simplex algorithm for EQ constraints
We consider the following LP problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ–àµŒÍ´İ”à¬µàµ…İ”à¬¶
6XEMHFWWRİ”à¬µàµ…İ”à¬¶àµŒÍµÇ¡ Í² àµ‘İ”à¬µàµ‘Í´Ç¡ Í² àµ‘İ”à¬¶àµ‘Í´
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
83 
Linear Programming Methods
We first add slack variables İà¬µÇ¡ İà¬¶ to the LE constraints. The resulting standard LP problem is given as:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ–àµŒÍ´İ”à¬µàµ…İ”à¬¶
6XEMHFWWRİ”à¬µàµ…İ”à¬¶àµŒÍµÇ¡ İ”à¬µàµ…Â•à¬µàµŒÍ´Ç¡ İ”à¬¶àµ…Â•à¬¶àµŒÍ´Ç¢İ”à¬µÇ¡ İ”à¬¶Ç¡ İà¬µÇ¡ İà¬¶àµ’Í²
We note that no obvious BFS for the problem exists. In order to solve the problem via two-phase simplex 
method, we add an auxiliary variable a1 to the EQ constraint and define the following auxiliary problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ–à¯”àµŒÜ½à¬µ
6XEMHFWWRİ”à¬µàµ…İ”à¬¶àµ…Ü½à¬µàµŒÍµÇ¡ İ”à¬µàµ…İà¬µàµŒÍ´Ç¡ İ”à¬¶àµ…İà¬¶àµŒÍ´Ç¢İ”à¬µÇ¡ İ”à¬¶Ç¡ İà¬µÇ¡ İà¬¶Ç¡ Ü½à¬µàµ’Í²
The starting tableau for the auxiliary problem is given below:

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢‡à«š 5KV







à¢™à«š






à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






First, the auxiliary variable is made basic by producing a unit vector in the column. This is followed by 
additional Simplex iterations to reach the Phase I solution as shown in the tableaus below:

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢‡à«š 5KV
à¢‡à«š






à¢™à«š






à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






(%9İ”à¬µ/%9İà¬µSLYRW

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢‡à«š 5KV
à¢‡à«š






à¢à«š






à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






(%9İ”à¬¶/%9Ü½à¬µSLYRW
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
84 
Linear Programming Methods

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢‡à«š 5KV
à¢à«›






à¢à«š






à¢™à«›






àµ†à¢ 






àµ†à¢ à¢‡






At this point, since the reduced costs are non-negative and the auxiliary objective has a zero value, Phase I 
Simplex is completed with the initial BFS: İ”à¬µàµŒÍ´Ç¡ İ”à¬¶àµŒÍ³ After dropping the auxiliary variable column 
and the auxiliary objective row, the starting tableau for Phase II Simplex is given as:

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› 5KV
à¢à«›





à¢à«š





à¢™à«›





àµ†à¢ 





We follow the initial step with further simplex iterations. The optimum is reached in one iteration and 
the final tableau is given as:

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› 5KV
à¢à«›





à¢à«š





à¢™à«›





àµ†à¢ 





Since the reduced costs are non-negative, the current solution is optimal, i.e., İ”à¬µ
×› àµŒÍ³Ç¡ İ”à¬¶
×› àµŒÍ´Ç¡ İ–×› àµŒÍ¶. 
Later, we show that this problem is more easily solved via dual Simplex method (Sec. 5.4.2). 
5.4	
Postoptimality Analysis
Postoptimality, or sensitivity analysis, aims to study how variations in the original problem parameters 
affect the optimum solution. Postoptimality analysis serves the following purposes:
1.	 To help in managerial decision making, regarding the potential effects of increase/decrease 
in resources or raising/lowering the prices. 
2.	 To analyze the effect of modeling errors, reflected in the uncertainty in parameter values in 
the coefficient matrices à¡­Ç¡ à¢ˆÇ¡ à¢‰à¯áˆ» on the final LP solution. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
85 
Linear Programming Methods
In postoptimality analysis, we are interested to explore the effects of parametric changes in Ü¾à¯œÇ¡ Ü¿à¯Ç¡ and Ü£à¯œà¯
on the optimal solution. There are five basic parametric changes affecting the LP solution (Arora, p. 229):
1.	 Changes in cost function coefficients, Ü¿à¯Ç¢ these changes affect the level curves of the 
function.
2.	 Changes in resource limitations, Ü¾à¯œÇ¢Wthese changes affect the set of active constraints.
3.	 Changes in constraint coefficients, Ü½à¯œà¯Ç¢ these changes affects the active constraint gradients.
4.	 The effect of including additional constraints
5.	 The effect of including additional variables
We note that we can use the final tableau to study the effects of parameter changes on the optimal solution. 
Toward that end, we recall, from Sec. 5.3.1, that the instantaneous cost function value in the Simplex 
algorithm is represented as: 

İ–àµŒà¢Ÿà¯à¢ˆàµ…à¢‰à·œà¯‡
à¯à¢à¯‡ZKHUHà¢Ÿà¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µDQGà¢‰à·œà¯‡
à¯àµŒà¢‰à¯‡
à¯àµ†à¢Ÿà¯à¡º
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Fundamental Engineering Optimization 
Methods
86 
Linear Programming Methods
Then, writing İ–àµŒÏƒ İ•à¯œÜ¾à¯œ
à¯œ
àµ…Ïƒ Ü¿Æ¸à¯İ”à¯
à¯
Ç¡ and taking the differentials with respect to Ü¾à¯œÇ¡ Ü¿à¯ we obtain: 
ßœÜ¿Æ¸à¯àµŒßœÜ¿à¯àµ†à¢Ÿà¯ßœà¡ºà¯Ç¡ ßœİ–àµŒÏƒ İ•à¯œßœÜ¾à¯œ
à¯œ
àµ…Ïƒ ßœÜ¿Æ¸à¯İ”à¯
à¯
 where à¡ºà¯ represents the jth column of N. The above 
formulation may be used to analyze the effects of changes to Ü¾à¯œÇ¡ Ü¿à¯Ç¡ and à¡ºà¯ on z. Those results are 
summarized below (Belegundu & Chandrupatla, p. 167):
1.	 Changes to the resource constraints (rhs). A change in Ü¾à¯œ has the effect of moving the 
associated constraint boundary. Then,
a)	 If the constraint is currently active İ•à¯œàµÍ² the change will affect the current basic 
solution: İ”à¡®àµŒà¢ˆà·©Ç¡ as well as İ–à¯¢à¯£à¯§ If the new İ”à¡® is feasible, then İ–à¯¢à¯£à¯§àµŒà¢Ÿà¯à¢ˆ is the 
new optimum value. If the new İ”à¡® is infeasible, then dual Simplex steps may be used to 
restore feasibility (and hence optimality). 
b)	 If the constraint is currently non-active İ•à¯œàµŒÍ² then İ–à¯¢à¯£à¯§and İ”à¡® are not affected.
2.	 Changes to the objective function coefficients. Changes to Ü¿à¯ affect the level curves of z. 
Then,
a)	 If Ü¿à¯× à¢‰à®» then since the new Ü¿Æ¸à¯àµÍ² Gauss-Jordan eliminations are needed to return İ”à¯
to the basis. If optimality is lost in the process (any Ü¿Æ¸à¯àµÍ²áˆ»), further Simplex steps will 
be needed to restore optimality. If optimality is not affected, then İ–à¯¢à¯£à¯§àµŒà¢Ÿà¯à¢ˆ is the new 
optimum.
b)	 If I Ü¿à¯× à¢‰à¯‡ though it does not affect İ–, still Ü¿Æ¸à¯ needs to be recomputed and checked for 
optimality.
3.	 Changes to the coefficient matrix. Changes to the coefficient matrix affect the constraint 
boundaries. For a change in à¡­à¯ İ†WKFROXPQRIà¡­
a)	 If à¡­à¯× à¡® corresponding to a basic variable, then Gauss-Jordan eliminations are 
needed to reduce Ü£à¯ to a unit vector; then Ü¿Æ¸à¯ needs to be recomputed and checked for 
optimality.
b)	 If I à¡­à¯× à¡ºÇ¡ corresponding to a nonbasic variable, then the reduced cost Ü¿Æ¸à¯ needs to be 
recomputed and checked for optimality. 
4.	 Adding Variables. If we add a new variable İ”à¯¡à¬¾à¬µ to the problem, then the cost function 
is updated as: İ–àµŒà¢‰à¯à¢àµ…Ü¿à¯¡à¬¾à¬µİ”à¯¡à¬¾à¬µ In addition, a new column Ü£à¯¡à¬¾à¬µ is added to the 
constraint matrix. The reduced cost corresponding to the new column is computed as: 
Ü¿à¯¡à¬¾à¬µàµ†à¢Ÿà¯Ü£à¯¡à¬¾à¬µ. Then, if this cost is positive, optimality is maintained; otherwise, further 
Simplex iterations are needed to recover optimality. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
87 
Linear Programming Methods
5.	 Adding inequality Constraints. Assume that we add an inequality constraint to the 
problem. Adding a constraint adds a row and the associated slack/surplus variable 
adds a column to the tableau. In this case, we need to check if adding a column 
to the basis affects the current optimum. We define an augmented B matrix as: 
à¡®àµŒàµ¤à¡®
Í²
à¢‡à®»
à¯
Í³àµ¨Ç¡ZKHUHà¡®à¬¿à¬µàµŒàµ¤à¡®à¬¿à¬µ
Í²
à¢‡à®»
à¯à¡®à¬¿à¬µ
Í³àµ¨ and write the augmented final tableau as:

%DVLF
à¢à¡®
à¢à¡º
5KV
à¢à¡®
à¡µ
à¡®à¬¿à¬µà¡º
à¡®à¬¿à¬µà¢ˆ
à¢à¢”à¬¾à«š
à¡µ
à¢‡à®»
à¯à¡®à¬¿à¬µà¡º
à¢‡à®»
à¯à¡®à¬¿à¬µà¢ˆàµ…Ü¾à¯¡à¬¾à¬µ
àµ†à¢ 
à«™
à¢‰à¯‡
à¯àµ†à¢Ÿà¯à¡º
àµ†à¢Ÿà¯à¢ˆ
Since the reduced costs are not affected, if à¢‡à®»
à¯à¡®à¬¿à¬µà¢ˆàµ…Ü¾à¯¡à¬¾à¬µàµÍ² optimality is maintained. If 
not, we choose this row as the pivot row and apply dual Simplex steps (Sec. 5.5.2) to recover 
optimality. 
Further results on sensitivity analysis involve parametric linear programming, aimed at analyzing the 
range of parameters values in the perturbed solution that maintain feasibility and/or optimality. These 
ranges are normally reported by commercial analysis software. Interested readers may consult Sec. 6.5 
in (Griva, Nash & Sofer, 2009).
The following problem adopted from (Belegundu & Chandrupatla, p. 122) is used to illustrate the ideas 
presented in this section. 
Example 5.5: Postoptimality Analysis 
A vegetable farmer has the choice to grow tomatoes, green peppers, or cucumbers on his 200 acre farm. 
The man-days/per acre needed for growing the three vegetables are 6, 7 and 5, respectively. A total of 
500 man-hours are available. The yield/acre for the three vegetables are in the ratios: 4.5:3.6:4.0. We wish 
to determine the optimum crop combination that maximizes total yield.
The optimization problem was solved using the Simplex method. The initial and the final tableaus for 
the problem are reproduced below: 
,QLWLDO
%DVLF
à¢à«š
à¢à«›
à¢à«œ à¢™à«š à¢™à«› 5KV
à¢™à«š






à¢™à«›






àµ†à¢ 
 




Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
88 
Linear Programming Methods
)LQDO
%DVLF
à¢à«š
à¢à«›
à¢à«œ à¢™à«š
à¢™à«›
5KV
à¢™à«š
 


 
à¢à«œ






àµ†à¢ 






From the final tableau, the optimum crop combination is given as: İ”à¬µ
×› àµŒÍ²Ç¡ İ”à¬¶
×› àµŒÍ²Ç¡ İ”à¬·
×› àµŒÍ³Í²Í² with 
İ–×› àµŒ 400. Further, the shadow prices for the slack variables are: 
à¬·
à¢Ÿà¯àµŒáˆ¾Í²Ç¡ Í²Ç¤Íºáˆ¿ZLWKİ–×› àµŒà¢Ÿà¯à¢ˆàµŒÍ¶Í²Í²
Next, without re-solving the problem, we wish to answer the following questions:
a)	 If an additional 50 acres are added, what is the expected change in yield? The answer is 
found from: İ–×› àµŒà¢Ÿà¯áˆºà¢ˆàµ…Î¿áˆ»ZKHUHÎ¿àµŒáˆ¾Í·Í²Ç¡Í²áˆ¿à¯ZLWKİ–×› àµŒÍ¶Í²Í² i.e., there is no expected 
change in yield. This also means that the land area constraint is not binding in the current 
optimum solution.
b)	 If an additional 50 man-days are added, what is the expected change in yield? The answer is 
found from: İ–×› àµŒà¢Ÿà¯áˆºà¢ˆàµ…Î¿áˆ»ZKHUHÎ¿àµŒáˆ¾Í²Ç¡ Í·Í²áˆ¿à¯ZLWKİ–×› àµŒÍ¶Í¶Í² i.e., the yield increases by 
40 units. This also means that the man-days constraint is binding in the optimum solution.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Fundamental Engineering Optimization 
Methods
89 
Linear Programming Methods
c)	 If the yield/acre for tomatoes increases by 10%, how is the optimum affected? The answer 
is found by re-computing the reduced costs as: à¢‰à·œà¯àµŒà¢Ÿà¯à¡­àµ†à¢‰à¯àµŒáˆ¾àµ†Í²Ç¤Í³Í·Ç¡ Í´Ç¡ Í²áˆ¿ Since a 
reduced cost is now negative, additional Simplex steps are needed to regain optimality. This 
is done and the new optimum is: İ”à¬µ
×› àµŒÍºÍµÇ¤ÍµÍµÇ¡ İ”à¬¶
×› àµŒÍ²Ç¡ İ”à¬·
×› àµŒÍ²ZLWKİ–×› àµŒÍ¶Í³Í´Ç¤Í· 
d)	 If the yield/acre for cucumbers drops by 10%, how is the optimum be affected? The answer 
is found by re-computing the reduced costs as: à¢‰à·œà¯àµŒà¢Ÿà¯à¡­àµ†à¢‰à¯àµŒáˆ¾Í²Ç¤ÍµÇ¡ Í´Ç¡ Í²Ç¤Í¶áˆ¿ The reduced 
costs are non-negative, but à¢à«œ is no more a basic variable. Regaining the basis results in 
reduced cost for à¢à¬µ becoming negative. Additional Simplex steps are performed to regain 
optimality, and the new optimum is: İ”à¬µ
×› àµŒÍºÍµÇ¤ÍµÍµÇ¡ İ”à¬¶
×› àµŒÍ²Ç¡ İ”à¬·
×› àµŒÍ²ZLWKİ–×› àµŒÍµÍ¹Í· 
e)	 If the man-hours needed to grow green peppers increase to 5/acre, how is the optimum 
affected? The answer is found by re-computing the reduced cost: Ü¿Æ¸à¬¶àµŒİ•à¯Ü£à¬¶àµ†Ü¿à¬¶àµŒÍ²Ç¤Í¶ 
Since İ”à¬¶Z was non-basic and the revised reduced cost is non-negative, there is no change in 
the optimum solution.
5.5	
Duality Theory for the LP Problems
In this section we extend the Lagrangian duality (Sec. 4.5) to the LP problems. Duality theory applies to 
practical LP problems in engineering and economics. In engineering, for example, the primal problem in 
electric circuit theory may be posed in terms of electric potential, and its dual in terms of current flow. 
Similarly, an optimization problem in mechanics may be modeled with strains, and its dual modeled 
with stresses. In economics, if the primal problem seeks to optimize price per unit of product, its dual 
may seek to minimize cost per unit of resources. 
The LP duality is defined in the following terms: associated with every LP problem is a dual problem 
that is formulated in terms of dual variables, i.e., the Lagrange multipliers. In the symmetric form of 
duality, the primal (P) and the dual (D) LP problems are stated as:
3ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢VXEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ à¢àµ’à«™
'ÂÂ‹Â
à¢Ÿ
İ“àµŒà¢Ÿà¯à¢ˆVXEMHFWWRà¢Ÿà¯à¡­àµ’à¢‰à¯Ç¡ à¢Ÿàµ’à«™
(5.10)
where à¢× Ô¹à¯¡ denotes the primal variables and Gà¢Ÿ× Ô¹à¯  denotes the dual variables. We note that, based 
on the definition of duality, the dual of the dual (D) is the same as primal (P). 
To define the dual of an arbitrary LP problem we first convert the problem into an equivalent problem 
of the primal form. The dual problem can then be formulated accordingly. For example, when (P) is 
given in the standard LP form, then (D) takes the following form: 
3ÂÂ‹Â
à¢İ–àµŒà¢‰à¯à¢VXEMHFWWRà¡­à¢àµŒà¢ˆÇ¡ à¢àµ’à«™
'ÂÂƒÂš
à¢Ÿ
İ“àµŒà¢Ÿà¯à¢ˆVXEMHFWWRà¢Ÿà¯à¡­àµ‘à¢‰à¯
(5.11)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
90 
Linear Programming Methods
where Lagrange multipliers y for the equality constraints in the dual formulation are unrestricted in sign. 
To obtain the above dual formulation, we use the following equivalence: à¡­à¢àµŒà¢ˆ
Ö à¡­à¢àµ’à¢ˆÇ¡ àµ†à¡­à¢àµ’àµ†à¢ˆÇ¡ 
or: áˆ¾à¡­ àµ†à¡­áˆ¿á‰‚à¢
à¢á‰ƒàµ’á‰‚à¢ˆ
àµ†à¢ˆá‰ƒ We can then use the symmetric form of duality where the dual variable vector 
is given as: áˆ¾à¢›à¯Ç¡ à¢œà¯áˆ¿ We obtain the above result by designating dual variables as: à¢ŸàµŒà¢›àµ†à¢œÇ¢ à¢›Ç¡ à¢œàµ’à«™
where à¢Ÿ is unrestricted in sign. 
The following example is used to explain LP duality.
Example 5.6: Duality in LP problems

$
%
&
'




To illustrate duality, we consider the problem of sending goods from node A to node D in a simplified 
network (Pedregal, p. 45). Assuming that the total quantity to be shipped equals 1, let İ”à¯œà¯ denote the 
fractional quantity to be shipped via link İ…İ† with associated transportation cost Ü¿à¯œà¯ (shown in the figure). 
Then, the primal objective is to minimize the transportation costs and the primal problem is formulated as: 
ÂÂ‹Â
à¢İ–àµŒÍ´İ”à®ºà®»àµ…Íµİ”à®ºà®¼àµ…İ”à®»à®¼àµ…Í¶İ”à®»à®½àµ…Í´İ”à®¼à®½
Subject to: İ”à®ºà®»àµŒİ”à®»à®¼àµ…İ”à®»à®½Ç¡ İ”à®ºà®¼àµ…İ”à®»à®¼àµŒİ”à®¼à®½Ç¡ İ”à®»à®½àµ…İ”à®¼à®½àµŒÍ³ (equivalently, İ”à®ºà®»àµ…İ”à®ºà®¼àµŒÍ³áˆ»Ç¢ 
İ”à®ºà®»Ç¡ İ”à®»à®¼Ç¡ İ”à®ºà®¼Ç¡ İ”à®»à®½Ç¡ İ”à®¼à®½àµ’Í²
Alternatively, we may consider İ•à¯‚ to be the price of goods at node I, and İ•à¯‚àµ†İ•à®ºÇ¡ as the profit to be 
made in transferring the goods from A to I. Then, the dual objective is to maximize the profit at node 
D. Then, if we arbitrarily assign: İ•à®ºàµŒÍ² the dual formulation is given as: 
ÂÂƒÂš
à¢Ÿ
İ•à®½
Subject to: İ•à®»àµ‘Í´Ç¡ İ•à®¼àµ‘ÍµÇ¡ İ•à®¼àµ†İ•à®»àµ‘Í³Ç¡ İ•à®½àµ†İ•à®»àµ‘Í¶Ç¡ İ•à®½àµ†İ•à®¼àµ‘Í´
Finally, we note that both problems can be formulated in terms of following coefficient matrices:

Ü£àµŒàµ¥
Í³
Í²
Í²
Í³
Í²
Í²

àµ†Í³
àµ†Í³
Í²
Í²
Í²
Í³

Í²
àµ†Í³
Í³
àµ©Ç¡ Ü¾àµŒàµ¥
Í²
Í²
Í³
àµ©Ç¡ Ü¿à¯àµŒáˆ¾Í´ÍµÍ³Í¶Í´áˆ¿
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
91 
Linear Programming Methods
5.5.1	
 Fundamental Duality Properties
Duality theory confers fundamental properties on the optimization problem that relate the primal and 
dual linear programs. Specifically, these properties specify bounds on the two objectives and are useful 
in developing computational procedures to solve the primal and dual problems. These properties are 
stated below for the symmetric form of duality where (P) solves the maximization problem. 
Weak Duality. Let x denote a feasible solution to (P) and y a feasible solution to (D), then, 
à¢Ÿà¯à¢ˆàµ’à¢Ÿà¯Û¯à¢àµ’à¢‰à¯à¢LHİ“áˆºà¢Ÿáˆ»àµ’İ–áˆºà¢áˆ» Further, the difference between these two objective functions, 
à¢ˆà¯à¢Ÿàµ†à¢‰à¯à¢Ç¡ is referred to as the duality gap. 
Optimality. If x is a feasible solution to (P) and a y feasible solution to (D), and, further, à¢‰à¯à¢àµŒà¢ˆà¯à¢Ÿ 
then x is an optimal solution to (P), and y an optimal solution to (D). 
Unboundedness. If the primal (dual) problem is unbounded, then the dual (primal) problem is infeasible 
(i.e., the feasible region is empty). We note that this is a necessary consequence of weak duality.
Strong Duality. If the primal (dual) problem has a finite optimal solution, then so does the dual (primal) 
problem; further, these two optimums are equal, i.e., İ“à¯¢à¯£à¯§àµŒà¢Ÿà¯à¢ˆàµŒà¢Ÿà¯Û¯à¢àµŒà¢‰à¯à¢àµŒİ–à¯¢à¯£à¯§ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
92 
Linear Programming Methods
Further, x if is the optimal solution to (P), then à¢Ÿà¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µ is the optimal solution to (D), which can 
be seen from: İ“àµŒà¢Ÿà¯à¢ˆàµŒà¢‰à®»
à¯à¡®à¬¿à¬µà¢ˆàµŒà¢‰à®»
à¯İ”à¡®àµŒà¢‰à¯à¢àµŒİ– 
We also note that the optimality of (P) implies the feasibility of (D), and vice versa. In particular, İ”à¡®àµ’à«™ 
RUà¢àµ’à«™ implies primal feasibility and dual optimality; whereas, à¢‰à·œà¯‡
à¯àµ’à«™RUà¢‰à·œàµŒà¢‰àµ†à¡­à¯à¢Ÿàµ’à«™ 
implies primal optimality and dual feasibility. 
Complementary Slackness. At the optimal point, we have: à¢à¯à¢‰àµŒà¢à¯à¡­à¯à¢Ÿ implying: à¢à¯áˆºà¢‰àµ†à¡­à¯à¢Ÿáˆ»àµŒ
Ïƒ İ”à¯áˆºÜ¿àµ†Ü£à¯İ•áˆ»à¯
à¢
àµŒÍ² which shows that it is not possible to have both İ”à¯àµÍ² and áˆºÜ£à¯İ•áˆ»à¯àµÜ¿à¯ at the 
optimum. 
Thus, if the jth primal variable is basic, i.e., İ”à¯àµŒÍ² then the th dual constraint is binding, i.e., áˆºÜ£à¯İ•áˆ»à¯ = Ü¿à¯; 
and, if the jth primal variable is non-basic, i.e., İ”à¯àµŒÍ² then the jth dual constraint is non-binding, i.e., 
áˆºÜ£à¯İ•áˆ»à¯àµÜ¿à¯
5.5.2	
 The Dual Simplex Method 
The dual simplex method involves application of the simplex method to the dual problem. Complementary 
to the regular simplex algorithm that initializes with a valid BFS and moves through primal feasible 
solutions, the dual simplex algorithm initializes with and moves through dual feasible (primal infeasible) 
solutions. The dual simplex algorithm thus iterates outsides of the feasible region. As such, the dual 
simplex method provides a convenient alternative to the two-phase simplex method in the event the 
optimization problem has no feasible solution (Sec. 5.3.2). 
To develop the dual simplex algorithm, we consider the minimization problem formulated with dual 
variables (5.10). We note that primal optimality áˆºà¢‰à·œàµ’à«™áˆ» corresponds to dual feasibility áˆºà¢Ÿà¯à¡­àµ’à¢‰à¯áˆ» 
and primal feasibility áˆºà¢àµ’à«™áˆ» corresponds to dual optimality. We therefore assume that the objective 
function coefficients are positive and the rhs is partly negative VRPHÜ¾à¯œàµÍ² The dual simplex algorithm 
then proceeds in a similar fashion to the primal algorithm except that:
1.	 The points generated during dual simplex iterations are primal infeasible as some basic 
variables have negative values. 
2.	 The solutions are always optimal in the sense that the reduced cost coefficients for nonbasic 
variables are non-negative.
3.	 An optimal is reached when a feasible solution with non-negative values for the basic 
variables has been found.
A tableau implementation of the dual Simplex algorithm proceeds as follows: after subtracting the surplus 
variables from GE constraints in (5.12) we multiply those constraints by â€“1 We then enter the constraints 
and the cost function coefficients in a tableau noting that the initial basic solution is infeasible. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
93 
Linear Programming Methods
At each iteration, the pivot element in the dual simplex method is determined as follows:
1.	 A pivot row Ü£à¯¤à¯ is selected as the row that has the basic variable with most negative value.
2.	 The ratio test to select the pivot column is conducted as: ÂÂ‹Â
à¯œ
àµœ
à¯–à³•
à¬¿à®ºà³œÇ¡à³•Ç£Ü¿à¯àµÍ²Ç¡ Ü£à¯¤Ç¡à¯àµÍ²àµ 
The dual simplex algorithm terminates when the rhs has become non-negative. 
5.5.3	
 Recovery of the Primal Solution
The final tableaus resulting from the application of simplex methods to the primal and dual problems 
are intimately related. In particular, the elements in the last row of the final dual tableau replicate the 
elements in the last column of the final primal tableau, and vice versa. This fact allows the recovery 
of primal solution from the final dual tableau. Let the dual problem be solved using standard simplex 
method, then the value of the ith primal variable equals the reduced cost coefficient of the slack or 
surplus variable associated with the ith dual constraint in the final dual tableau. In addition, if the dual 
variable is nonbasic, then its reduced cost coefficient equals the value of the slack or surplus variable for 
the corresponding primal constraint.
To reveal the above relationship, we re-consider the dual problem in (5.12), which, after subtracting 
surplus variables, is represented in the following equivalent form: 
ÂÂ‹Â
à¢Ÿİ“àµŒà¢Ÿà¯à¢ˆ
6XEMHFWWRà¢Ÿà¯à¡­àµ†à¡µà¢™àµŒà¢‰à¯Ç¡ à¢Ÿàµ’à«™
An initial tableau for the dual problem, with s as the basic variables, is given as:
%DVLF
à¢Ÿ
à¢™
5KV
à¢™
àµ†à¡­à¯
à¡µ
àµ†à¢‰
àµ†à¢
à¢ˆà¯
à«™
Í²
Assuming that the same order of the variables is maintained, the final tableau at the termination of dual 
simplex algorithm may be given as:
%DVLF
à¢
à¢™
5KV
à¢Ÿà¡®
à¡­à·©
à¡¿
à¢‰à·¤
àµ†à¢
à¢ˆà·¡à¯
à¢à¯
àµ†İ–×›
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
94 
Linear Programming Methods
where we note that the primal variables appear in the last row under the slack/surplus variable columns. 
Further, the coefficients in the final tableau are related to those in the initial tableau as follows: 
à¡­à·©àµŒàµ†à¡¿à¡­à¯Ç¡à¢‰à·¤àµŒàµ†à¡¿à¢‰Ç¡
à¢ˆà·¡à¯àµŒà¢ˆà¯àµ†à¢à¯à¡­à¯Ç¡
İ–×› àµŒà¢‰à¯à¢
(5.13)
The following examples illustrate the efficacy of the dual Simplex algorithm.
Example 5.7: Dual Simplex algorithm
We consider the dual of Example 5.1 where the original LP problem was defined as:
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍ´İ”à¬µàµ…İ”à¬¶àµ‘Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¢İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í²
Using the symmetric form of duality, the dual optimization problem is defined as:
ÂÂ‹Â
à¯¬à°­Ç¡à¯¬à°®İ“àµŒÍ³Í´İ•à¬µàµ…Í³Í¸İ•à¬¶
6XEMHFWWRÍ´İ•à¬µàµ…Í´İ•à¬¶àµ’ÍµÇ¡ İ•à¬µàµ…Íµİ•à¬¶àµ’Í´Ç¢İ•à¬µàµ’Í²Ç¡ İ•à¬¶àµ’Í²
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Fundamental Engineering Optimization 
Methods
95 
Linear Programming Methods
We subtract surplus variables from the GE constraints and multiply them with â€“1 before entering them 
in the initial tableau. We then follow with dual simplex iterations. The resulting series of tableaus is 
given below: 
%DVLF à¢Ÿà«š à¢Ÿà«› à¢™à«š à¢™à«› 5KV
à¢™à«š





à¢™à«›





àµ†à¢




Í²
(%9İ•à¬µ/%9Â•à¬µSLYRW

%DVLF à¢Ÿà«š à¢Ÿà«›
à¢™à«š
à¢™à«› 5KV
à¢Ÿà«š





à¢™à«›





àµ†à¢





/%9Â•à¬¶(%9İ•à¬¶SLYRW

%DVLF à¢Ÿà«š à¢Ÿà«›
Ü›à«š
Ü›à«›
5KV
à¢Ÿà«š





à¢Ÿà«›




Ã³
àµ†à¢





At this point the dual LP problem is solved and the optimal solution is: İ•à¬µàµŒÍ³Ç¤Í´Í·Ç¡ İ•à¬¶àµŒÍ²Ç¤Í´Í·Ç¡ İ“à¯¢à¯£à¯§àµŒÍ³Í» 
We note that the first feasible solution obtained above is also the optimal solution. We further note that:
a)	 The optimal value of the objective function for (D) is the same as the optimal value for (P). 
b)	 The optimal values for the basic variables for (P) appear as reduced costs associated with 
non-basic variables in (D).
As an added advantage, the dual simplex method obviates the need for the two-phase simplex method to 
obtain a solution when an initial BFS is not readily available. This is illustrated by re-solving Example 5.3 
using the dual simplex algorithm. 
Example 5.8: Dual Simplex algorithm
We consider the dual problem of Example 5.3. The original LP problem is stated as:
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍµİ”à¬µàµ…Í´İ”à¬¶àµ’Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¡ İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í²
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
96 
Linear Programming Methods
The GE constraint in the problem is first multiplied by â€“1; the problem is then converted to dual problem 
using the symmetric form of duality. The dual optimization problem is given as:
ÂÂ‹Â
à¯¬à°­Ç¡à¯¬à°®İ–à¬µàµŒàµ†Í³Í´İ•à¬µàµ…Í³Í¸İ•à¬¶
6XEMHFWWRàµ†Íµİ•à¬µàµ…Í´İ•à¬¶àµ’ÍµÇ¡ àµ†Í´İ•à¬µàµ…Íµİ•à¬¶àµ’Í³Ç¢İ•à¬µàµ’Í²Ç¡ İ•à¬¶àµ’Í² 
The series of tableaus leading to the optimal solution via the dual simplex method is given below:

%DVLF
à¢Ÿà«š
à¢Ÿà«› à¢™à«š à¢™à«› 5KV
à¢™à«š





à¢™à«›





àµ†à¢
 


Í²
/%9Â•à¬µ(%9İ•à¬¶SLYRW

%DVLF
à¢Ÿà«š
à¢Ÿà«›
à¢™à«š
à¢™à«› 5KV
à¢Ÿà«›





à¢™à«›





àµ†à¢





At this point the dual LP problem is solved with the optimal solution: İ•à¬µ
×› àµŒÍ²Ç¡ İ•à¬¶
×› àµŒÍ³Ç¤Í·Ç¡ İ“×› àµŒÍ´Í¶ 
We note that this is the same solution obtained for Example 5.3. We further note that the reduced costs 
for nonbasic variables match with the optimal values of the primal basic variables. 
The final dual Simplex example involves a problem with equality constraints.
Example 5.9: Equality Constraints
We re-consider Example 5.4 where the optimization problem was given as:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ–àµŒÍ´İ”à¬µàµ…İ”à¬¶
6XEMHFWWRİ”à¬µàµ…İ”à¬¶àµŒÍµÇ¡ Í² àµ‘İ”à¬µÇ¡ İ”à¬¶àµ‘Í´
In order to solve this problem via the dual Simplex method, we replace the equality constraint with twin 
inequality constraints: áˆ¼İ”à¬µàµ…İ”à¬¶àµŒÍµáˆ½Õ áˆ¼İ”à¬µàµ…İ”à¬¶àµ‘ÍµÇ¡ İ”à¬µàµ…İ”à¬¶àµ’Íµáˆ½ Next, we multiply GE constraint 
with , and add slack variables to all inequalities. Finally, we identify: İà¬µİà¬¶İà¬·İà¬¸ as basic variables, 
and construct an initial tableau for the dual simplex method. This is followed by two iterations of the 
dual simplex algorithm leading to the optimum. The resulting tableaus for the problem are given below: 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
97 
Linear Programming Methods

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢™à«œ à¢™à« 5KV
à¢™à«š
Í³
Í³
Í³
Í²
Í²
Í²
Íµ
à¢™à«›
Ç¦Í³
Ç¦Í³
Í²
Í³
Í²
Í²
Ç¦Íµ
à¢™à«œ







à¢™à«







àµ†à¢ 






Í²
/%9Â•à¬¶(%9İ”à¬¶SLYRW

%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢™à«œ à¢™à« 5KV
à¢™à«š
Í²
Í²
Í³
Í³
Í²
Í²
Í²
à¢à«›
Í³
Í³
Í²
Ç¦Í³
Í²
Í²
Íµ
à¢™à«œ







à¢™à«







àµ†à¢ 






àµ†Íµ
/%9Â•à¬¸(%9İ”à¬µSLYRW
%DVLF à¢à«š à¢à«› à¢™à«š à¢™à«› à¢™à«œ à¢™à« 5KV
à¢™à«š
Í²
Í²
Í³
Í³
Í²
Í²
Í²
à¢à«›
Í²
Í³
Í²
Í²
Í²
Í³
Í´
à¢™à«œ







à¢à«š







àµ†à¢ 






àµ†Í¶
The dual Simplex algorithm terminates with İ–à¯¢à¯£à¯§àµŒÍ¶ 
5.6	
Non-Simplex Methods for Solving LP Problems
The non-simplex methods to solve LP problems include the interior-point methods that iterate through 
the interior of the feasible region, and attempt to decrease the duality gap between the primal and dual 
feasible solutions. These methods can have good theoretical efficiency and practical performance that 
is comparable with the simplex methods. In the following, we discuss the primal-dual interior-point 
method that has been particularly successful in the case of LP problems (Griva, Nash & Sofer, p. 321).
To introduce the primal-dual method, we consider asymmetrical form of duality where the primal and 
dual problems are described as:
3ÂÂ‹Â
à¢İ–àµŒà¢‰à¯à¢
VXEMHFWWRà¡­à¢àµŒà¢ˆÇ¡ à¢àµ’à«™
(5.14)
'ÂÂƒÂš
à¢
İ“àµŒà¢ˆà¯à¢Ÿ
VXEMHFWWRà¡­à¯à¢Ÿàµ…à¢™àµŒà¢ˆÇ¡ à¢™àµ’à«™
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
98 
Linear Programming Methods
We note that for x and y to be the feasible solutions to the primal and dual problems (at the optimum), 
they must satisfy the following complementary slackness condition: İ”à¯İà¯àµŒÍ²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ The primal-
dual method begins with İ”à¯İà¯àµŒß¤Ç¡: for some ß¤àµÍ²Ç¡ and iteratively reduces the values of ß¤, generating a 
series of vectors: à¢áˆºß¤áˆ»Ç¡ à¢Ÿáˆºß¤áˆ»Ç¡ à¢™áˆºß¤áˆ» along the way, in an effort to reduce the duality gap: à¢‰à¯à¢àµ†à¢ˆà¯à¢ŸàµŒİŠß¤.
To develop the primal-dual algorithm, let the updates to the current estimates: à¢Ç¡ à¢ŸÇ¡ à¢™Ç¡ be given as: 
à¢àµ…Î¿à¢Ç¡ à¢Ÿàµ…Î¿à¢ŸÇ¡ à¢™àµ…Î¿à¢™Ç¢ then, these updates are required to satisfy the following feasibility and 
complementarity conditions: à¡­áˆºà¢àµ…Î¿à¢áˆ»àµŒà¢ˆÇ¡ à¡­à¯áˆºà¢Ÿàµ…Î¿à¢Ÿáˆ»áˆºà¢™àµ…Î¿à¢™áˆ»àµŒà¢‰Ç¡ áˆºà¢àµ…Î¿à¢áˆ»à¯áˆºà¢™àµ…Î¿à¢™áˆ»àµŒà«™ 
Accordingly,
à¡­Î¿à¢àµŒà«™
à¡­à¯Î¿à¢Ÿàµ…Î¿à¢™àµŒà«™
àµ«İ”à¯àµ…Î¿İ”à¯àµ¯àµ«İà¯àµ…Î¿İà¯àµ¯Ø† İ”à¯İà¯àµ…İ”à¯Î¿İà¯àµ…İà¯Î¿İ”à¯àµŒß¤

(5.15)
where the latter condition has been linearized for ease of implementation. To proceed further, we define: 
à¢„àµŒİ€İ…Ü½İƒáˆºà¢áˆ»Ç¡ à¡¿àµŒİ€İ…Ü½İƒáˆºà¢™áˆ»Ç¡ à¢‹àµŒáˆ¾Í³Ç¡ Ç¥ Ç¡Í³áˆ¿à¯Ç¡ to express the complementarity condition as: à¢„à¡¿à¢‹àµŒß¤à¢‹ 
Next, let à¡°àµŒà¡¿à¬¿à¬µà¢„Ç¡ à¢œáˆºß¤áˆ»àµŒáˆºß¤à¡µàµ†à¢„à¡¿áˆ»à¢‹Ç¡Wthen a solution to the linear system (5.16) is given as:
Î¿à¢àµŒà¡¿à¬¿à¬µà¢œáˆºß¤áˆ»àµ†à¡°Î¿à¢™
Î¿à¢ŸàµŒàµ†áˆºà¡­à¡°à¡­à¯áˆ»à¬¿à¬µà¡­à¡¿à¬¿à¬µà¢œáˆºß¤áˆ»
Î¿à¢™àµŒàµ†à¡­à¯Î¿à¢Ÿ

(5.16)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Fundamental Engineering Optimization 
Methods
99 
Linear Programming Methods
In practice, to ensure primal and dual feasibility, the following update rule for the solution vectors has 
been suggested (Griva, Nash & Sofer, p. 324):
à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¯Î¿à¢à¯Ç¡
à¢Ÿà¯à¬¾à¬µàµŒà¢Ÿà¯àµ…ß™à¯Î¿à¢Ÿà¯Ç¡
à¢™à¯à¬¾à¬µàµŒà¢™à¯àµ…ß™à¯Î¿à¢™à¯
ß™à¯àµÂÂ‹Âáˆºß™à¯‰Ç¡ ß™à®½áˆ»Ç¡
ß™à¯‰àµŒÂÂ‹Â
Î¿à¯«à³•à®´à¬´àµ†İ”à¯
ÈŸİ”à¯
Ç¡
ß™à®½àµŒÂÂ‹Â
Î¿à¯¦à³•à®´à¬´àµ†İà¯
ÈŸİà¯

Finally, an initial estimate that satisfies (5.9) is needed to start the primal-dual method. To find that 
estimate, let the constraint equation for the primal problem (5.7) be written as: à¡µà¢à®»àµ…à¡½à¢à¯‡àµŒà¢ˆÇ¢then, 
for some à¢à¬´Ç¡ à¢Ÿà¬´Ç¡ a set of feasible vectors satisfying (5.9) is obtained as: à¢àµŒá‰‚
à¢à¬´
à¢ˆàµ†à¡½à¢à¬´á‰ƒÇ¡ à¢ŸàµŒà¢Ÿà¬´Ç¡ à¢™àµŒ 
àµ¤à¢‰àµ†à¡­à¯à¢Ÿà¬´
àµ†à¢Ÿà¬´
àµ¨
Further, the bounding parameter Î¼ is updated in successive iterations as: ß¤à¯à¬¾à¬µàµŒß›ß¤à¯Ç¡ Í² àµß›àµÍ³Ç¡
where ß›àµŒÍ²Ç¤Í³ is considered a reasonable choice. 
The primal-dual algorithm is given as follows:
Primal-Dual Algorithm:
Given à¡­Ç¡ à¢ˆÇ¡ à¢‰
Initialize: select: ß³àµÍ²Ç¡ ß¤àµÍ²Ç¡ Í² àµß›àµÍ³Ü° (maximum number of iterations). 
Find initial à¢Ç¡ à¢ŸÇ¡ à¢™àµà«™ to satisfy (5.9).
For Uİ‡àµŒÍ³Ç¡Í´Ç¡ Ç¥
1.	 Check termination: if à¢à¯à¢™àµ†İŠß¤àµß³Ç¡RULIİ‡àµÜ°Ç¡ quit.
2.	 Use (5.16) to compute the updates vectors.
3.	 Use (5.17) to compute ß™à¯ and perform the update. 
4.	 Set ß¤à¸¶ß›ß¤
An example of the primal-dual algorithm is presented below.
Example 5.10: Primal-Dual Algorithm
We re-consider Example 5.1 where the original optimization problem was given as:
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍ´İ”à¬µàµ…İ”à¬¶àµ‘Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¢İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í²
The coefficient matrices for the problem are: à¡­àµŒá‰‚Í´
Í³
Í´
ÍµÍ³
Í²
Í²
Í³á‰ƒÇ¡ à¢ˆàµŒá‰‚Í³Í´
Í³Í¸á‰ƒÇ¡ à¢‰à¯àµŒáˆ¾àµ†Íµ
àµ†Í´áˆ¿
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
100 
Linear Programming Methods
To initialize the primal-dual algorithm, we select the following parameters: 
à¢à¬´àµŒáˆ¾Í´Ç¡ Í´áˆ¿à¯Ç¡ à¢Ÿà¬´àµŒáˆ¾àµ†Í³Ç¡ àµ†Í³áˆ¿à¯ß³àµŒÍ³Í²à¬¿à¬ºÇ¡ ß¤àµŒÍ³Í²Ç¡ ß›àµŒÍ²Ç¤Í³Ü°àµŒÍ³Í²
Then, the initial estimates for the variables are: 
ß›
İ”à¯àµŒáˆ¾Í´Ç¡ Í´Ç¡ Í¸Ç¡ Í¸áˆ¿Ç¡ İ•à¯àµŒáˆ¾àµ†Í³Ç¡ àµ†Í³áˆ¿Ç¡ İà¯àµŒáˆ¾Í³Ç¡ Í´Ç¡ Í³Ç¡ Í³áˆ¿
The variable updates for the first eight iterations are given below, where the last column contains the 
residual:
à¢à«š
à¢à«›
à¢™à«š
à¢™à«›
à¢à¢€à¢™àµ†à¢”à£†








The optimum solution is given as: İ”à¬µ
×› àµŒÍ·Ç¤Í²Ç¡ İ”à¬¶
×› àµŒÍ´Ç¤Í²Ç¡ which agrees with the results of Example 5.1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
101 
Linear Programming Methods
5.7	
Optimality Conditions for LP Problems
This section discusses the application of FONC to the LP problems. The first order optimality conditions 
in the case of general optimization problems are known as the KKT conditions. For convex optimization 
problems, the KKT conditions are both necessary and sufficient for optimality. 
5.7.1	
 KKT Conditions for LP Problems
To derive the KKT conditions for the LP problems, we consider a maximization problem proposed in 
(5.10) above. Using slack variables, the problem is converted into standard form as:
ÂÂ‹Â
à¢İ–àµŒàµ†à¢‰à¯à¢
VXEMHFWWRà¡­à¢àµ†à¢ˆàµ…à¢™àµŒà«™Ç¡ à¢àµ’à«™
(5.18)
We now use Lagrange multiplier vectors à¢›Ç¡ à¢œ for the constraints to write a Lagrangian function as:
à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒàµ†à¢‰à¯à¢àµ†à¢›à¯à¢àµ…à¢œà¯áˆºà¡­à¢àµ†à¢ˆàµ…à¢™áˆ»
Then, the first order KKT conditions for the optimality of the solution vector are:
Feasibility: à¡­à¢àµ†à¢ˆàµ…à¢™àµŒà«™
Optimality: ×à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒà¡­à¯à¢œàµ†à¢‰àµ†à¢›àµŒà«™
Complementarity: à¢›à¯à¢àµ…à¢œà¯à¢™àµŒà«™
Non-negativity: à¢àµ’à«™Ç¡ à¢™àµ’à«™Ç¡ à¢›àµ’à«™Ç¡ à¢œàµ’à«™
The above equations need to be simultaneously solved for the unknowns: à¢Ç¡ à¢™Ç¡ à¢›Ç¡ à¢œ to find the optimum. 
By substituting à¢™Ç¡ à¢› from the first two equations into the third, the optimality conditions are reduced to:
à¢œà¯áˆºà¡­à¢àµ†à¢ˆáˆ»àµŒà«™Ç¡
à¢à¯áˆºà¢‰àµ†à¡­à¯à¢œáˆ»àµŒà«™Ç¡
à¢àµ’à«™Ç¡
à¢œàµ’à«™
Therefore, the following duality conditions are implied at the optimum point: 
a)	 Lagrange multipliers for the active (binding) constraints are positive, and 
b)	 Dual constraints associated with basic variables are binding. 
Alternatively, we can solve the optimality conditions by partitioning the problem into basic and 
nonbasic variables as: à¢à¯àµŒáˆ¾à¢à®»
à¯Ç¡ à¢à¯‡
à¯áˆ¿Ç¢ à¢‰à¯àµŒáˆ¾à¢‰à®»
à¯Ç¡ à¢‰à¯‡
à¯áˆ¿Ç¢ à¡­àµŒáˆ¾à¡®Ç¡ à¡ºáˆ¿Ç¢à¢›à¯àµŒáˆ¾à¢›à®»
à¯Ç¡ à¢›à¯‡
à¯áˆ¿ Then, in terms 
of partitioned variables, the optimality conditions are given as:
á‰‚à¡®à¯
à¡ºà¯á‰ƒà¢œàµ†á‰‚à¢›à®»
à¢›à¯‡á‰ƒàµ†á‰‚à¢‰à®»
à¢‰à¯‡á‰ƒàµŒá‰‚à«™
à«™á‰ƒÇ¡
áˆ¾à¢à®»à¢à¯‡áˆ¿á‰‚à¢›à®»
à¢›à¯‡á‰ƒàµŒà«™
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
102 
Linear Programming Methods
Since à¢à®»àµÍ²Ç¡à¢›à®»àµŒÍ²Then, from the first equation, à¢œà¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µÇ¡ and from the second equation, 
à¢›à¯‡
à¯àµŒà¢‰à®»
à¯à¡®à¬¿à¬µà¡ºàµ†à¢‰à¯‡
à¯àµŒà¢‰à·œà¯‡
à¯ Thus, the reduced cost coefficients for nonbasic variables are the Lagrange 
multipliers, which are required to be non-negative at the optimum, i.e., à¢›à¯‡àµÍ²Ç¤
We can extend the optimality conditions to the dual problem formulated in (5.10). For the symmetric 
form of duality, the KKT conditions for the primal and dual problems are given as (Belegundu and 
Chandrapatla, p. 161):
	
Primal	
Dual
Feasibility: 	
	
à¡­à¢àµ…à¢™àµŒà¢ˆ 	 	
	
à¡­à¯à¢œàµ†à¢›àµŒà¢‰	
Optimality: 	
	
à¢‰àµŒà¡­à¯à¢œàµ†à¢›	 	
	
à¢ˆàµŒà¡­à¢àµ…à¢™
Complementarity: 	
	
à¢›à¯à¢àµ…à¢œà¯à¢™àµŒà«™	
Non-negativity: 	
	
à¢àµ’à«™Ç¡ à¢™àµ’à«™Ç¡ à¢›àµ’à«™Ç¡ à¢œàµ’à«™	
We note that the optimality condition for (P) is equivalent to the feasibility condition for (D) and vice 
versa, i.e., by interchanging the feasibility and optimality conditions, we may view the problem as primal 
or dual. It also shows that if (P) is unbounded, then (D) is infeasible, and vice versa. 
5.7.2	
 A Geometric Viewpoint
Further insight into the solution is obtained from geometrical consideration of the problem. Towards that 
end, let A be expressed in terms of row vectors as: à¡­à¯àµŒáˆ¾à¢‡à¬µ
à¯Ç¡ à¢‡à¬¶
à¯Ç¡ Ç¥ Ç¡ à¢‡à¯ 
à¯áˆ¿ where à¢‡à¬µ
à¯ represents a vector 
normal to the constraint: à¢‡à¯œ
à¯à¢àµ…İà¯œàµŒÜ¾à¯œ Similarly, let àµ†à¢‹à¯ denote a vector normal to the non-negativity 
constraint: àµ†İ”à¯àµ‘Í² Then, the optimality requires that there exist real numbers, İ’à¯œàµ’Í²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
and İ‘à¯àµ’Í²Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ such that the following conditions hold: 
à¢‰àµŒÏƒ İ’à¯œ
à¯œ
à¢‡à¯œ
à¯àµ†Ïƒ İ‘à¯
à¯
à¢‹à¯
Ïƒ İ’à¯œİà¯œ
à¯œ
àµ…Ïƒ İ‘à¯İ”à¯
à¯
àµŒÍ²

(5.21)
Let the Lagrange multipliers be grouped as: ß¤à¯œ× àµ›İ‘à¯œÇ¡ İ’à¯àµŸ and let Ü°à¯œ× áˆ¼à¢‡à¯œ
à¯Ç¡ àµ†à¢‹à¯áˆ½ denote the set of 
active constraint normals, then the complementarity condition is expressed as: à¢‰àµŒàµ†×İ–àµŒÏƒ
ß¤à¯œ
à¯œ×à££
Ü°à¯œ 
where à££ denotes the set of active constraints. 
The above condition states that at the optimal point the negative of objective function gradient lies in the 
convex cone spanned by the active constraint normals. When this condition holds, the descent-feasible 
cone is empty, i.e., we cannot move in a direction that further decreases the objective function without 
leaving the feasible region. This result is consistent with Farkas Lemma, which for the LP problems is 
stated as follows:
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
103 
Linear Programming Methods
Farkaâ€™s Lemma (Belegundu and Chandrupatla, p. 204): Given a set of vectors, à¢‡à¯œÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰ and a 
vector c, there is no vector d satisfying the conditions à¢‰à¯à¢ŠàµÍ² and à¢‡à¯œ
à¯à¢ŠàµÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰ if and only 
if c can be written as: à¢‰àµŒÏƒ
ß¤à¯œà¢‡à¯œ
à¯ 
à¯œà­€à¬µ
Ç¡ ß¤à¯œàµ’Í²
An illustrative example for the optimality conditions appears below:
Example 5.11: Optimality Conditions for the LP problem
We reconsider example 5.1 that was formulated as:
ÂÂƒÂš
à¯«à°­Ç¡à¯«à°®İ–àµŒÍµİ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRÍµİ”à¬µàµ…Í´İ”à¬¶àµ’Í³Í´Ç¡ Í´İ”à¬µàµ…Íµİ”à¬¶àµ‘Í³Í¸Ç¡ İ”à¬µàµ’Í²Ç¡ İ”à¬¶àµ’Í²
Application of the optimality conditions results in the following equations:
İ”à¬µáˆºÍ´İ’à¬µàµ…Í´İ’à¬¶àµ†Í´áˆ»àµ…İ”à¬¶áˆºİ’à¬µàµ…Íµİ’à¬¶àµ†Íµáˆ»àµŒÍ²
İ’à¬µáˆºÍ´İ”à¬µàµ…İ”à¬¶àµ†Í³Í´áˆ»àµ…İ’à¬¶áˆºÍ´İ”à¬µàµ…Íµİ”à¬¶àµ†Í³Í¸áˆ»àµŒÍ²
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
104 
Linear Programming Methods
We split these into four equations and use Matlab symbolic toolbox to solve them, which gives the 
following candidate solutions:
áˆ¼İ”à¬µÇ¡ İ”à¬¶Ç¡ İ’à¬µÇ¡ İ’à¬¶áˆ½àµŒáˆºÍ²Ç¡Í²Ç¡Í²Ç¡Í²áˆ»Ç¡ áˆºÍ¸Ç¡Í²Ç¡Í³Ç¡Í²áˆ»Ç¡ áˆºÍºÇ¡Í²Ç¡Í²Ç¡Í³áˆ»Ç¡ áˆºÍ·Ç¡Í´Ç¡Í²Ç¡Í³áˆ»Ç¡ áˆºÍ²Ç¡Í³Í´Ç¡ÍµÇ¡Í²áˆ»Ç¡ áˆºÍ²Ç¡Í·Ç¤ÍµÍµÇ¡Í²Ç¡Í³áˆ»Ç¡ á‰€Íº àµ†
à¬·à¯­
à¬¶Ç¡ İ–Ç¡ Í²Ç¡Í³á‰
Then, it can be verified that the optimality conditions hold only in the case of: áˆ¼İ”à¬µÇ¡ İ”à¬¶Ç¡ İ’à¬µÇ¡ İ’à¬¶áˆ½àµŒáˆºÍ·Ç¡Í´Ç¡Í²Í³áˆ»Ç¤ 
The optimum value of the objective function is: z* = 17.
5.8	
The Quadratic Programming Problem
Theory developed for the LP problems easily extends to quadratic programming (QP) problems. The 
QP problem arises frequently in convex programming when the energy associated with a problem is to 
be minimized. An example of that is the finite element analysis (FEA) problem in structures.
The QP problem involves minimization of a quadratic cost function subject to linear constraints, and 
is described as:
min İáˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢ 
(5.22)
Subject to: à¡­à¢àµ’à¢ˆÇ¡ à¢àµ’à«™
where Q is symmetric positive semidefinite. We first note that the feasible region for the QP problem 
is convex; further, for the given condition on à¡½Ç¡ İáˆºà¢áˆ» is convex. Therefore, QP problem is a convex 
optimization problem, and the KKT conditions are both necessary and sufficient for a global solution. 
5.8.1	
 Optimality Conditions for QP Problems
To derive KKT conditions for the QP problem, we consider a Lagrangian function that includes Lagrange 
multipliers u, v for the non-negativity and inequality constraints. The Lagrangian function and its 
gradient are given as:
à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ†à¢›à¯à¢àµ†à¢œà¯áˆºà¡­à¢àµ†à¢ˆàµ†à¢™áˆ»
(5.23)
×à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒà¡½à¢àµ…à¢‰àµ†à¢›àµ†à¡­à¯à¢œ
where s is a vector of slack variables. The resulting KKT conditions for the QP problem are given as:
Feasibility: à¡­à¢àµ†à¢™àµŒà¢ˆ
Optimality: à¡½à¢àµ…à¢‰àµ†à¢›àµ†à¡­à¯à¢œàµŒà«™
Complementarity: à¢›à¯à¢àµ…à¢œà¯à¢™àµŒà«™
Non-negativity: à¢àµ’à«™Ç¡ à¢™àµ’à«™Ç¡ à¢›àµ’à«™Ç¡ à¢œàµ’à«™
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
105 
Linear Programming Methods
By eliminating variables s, u we can concisely express the KKT conditions as:
à¢à¯áˆºà¡½à¢àµ…à¢‰àµ†à¡­à¯à¢œáˆ»àµŒà«™Ç¡
à¢œà¯áˆºà¡­à¢àµ†à¢ˆáˆ»àµŒà«™Ç¡à¢àµ’à«™Ç¡
à¢œàµ’à«™
(5.24)
Alternatively, we combine the optimality and feasibility conditions in matrix form as:
àµ¤à¡½
àµ†à¡­à¯
à¡­
à«™àµ¨á‰‚à¢
à¢œá‰ƒàµ…á‰‚à¢‰
àµ†à¢ˆá‰ƒàµ†á‰‚à¢›
à¢™á‰ƒàµŒá‰‚à«™
à«™á‰ƒ
Next, let: à¡¹àµŒàµ¤à¡½
àµ†à¡­à¯
à¡­
à«™àµ¨Ç¡ à¢ àµŒá‰‚à¢
à¢œá‰ƒÇ¡ à¢àµŒá‰‚à¢›
à¢™á‰ƒÇ¡ à¢—àµŒá‰‚à¢‰
àµ†à¢ˆá‰ƒÇ¢ then, the problem is transformed as: 
à¡¹à¢ àµ…à¢—àµŒà¢Ç¡ where the complementarity conditions are: à¢à¯à¢ àµŒà«™ The resulting problem is known in 
linear algebra as the Linear Complementarity Problem (LCP) and is solved in Sec. 5.8 below. 
The above QP problem may additionally include linear equality constraints of the form: à¡¯à¢àµŒà¢ŠÇ¡ in 
which case, the problem is defined as:
min İáˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢
(5.26)
subject to à¡­à¢àµ’à¢ˆÇ¡ à¡¯à¢àµŒà¢ŠÇ¡ à¢àµ’à«™
We similarly add slack variables to the inequality constraint, and formulate the Lagrangian function as:
à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒà¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ†à¢œà¯áˆºà¡­à¢àµ†à¢ˆàµ†à¢™áˆ»àµ†à¢›à¯à¢àµ…à¢à¯áˆºà¡¯à¢àµ†à¢Šáˆ»
The modified KKT conditions are given as: 
Feasibility: à¡­à¢àµ†à¢ˆàµ†à¢™àµŒà«™Ç¡ à¡¯à¢àµŒà¢Š
Optimality: à¡½à¢àµ…à¢‰àµ†à¢›àµ†à¡­à¯à¢œàµ…à¡¯à¯à¢àµŒà«™
Complementarity: à¢›à¯à¢àµ…à¢œà¯à¢™àµŒà«™
Non-negativity: à¢àµ’à«™Ç¡ à¢™àµ’à«™Ç¡ à¢›àµ’à«™Ç¡ à¢œàµ’à«™
where the Lagrange multipliers w for the equality constraints are not restricted in sign. By introducing: 
à¢àµŒÜ¡àµ†Ü¢Ç¢ Ü¡Ç¡ Ü¢àµ’à«™ we can represent the combined optimality and feasibility conditions as:
àµ¥
à¡½
àµ†à¡­à¯
à¡­
à¡¯
à«™
à«™
àµ©á‰‚à¢
à¢œá‰ƒàµ†àµ¥
à¡µ
à«™
à«™
à¡µ
à«™
à«™
àµ©á‰‚à¢›
à¢™á‰ƒàµ…àµ¥
à¡¯à¯
àµ†à¡¯à¯
à«™
à«™
à«™
à«™
àµ©á‰‚à¢Ÿ
à¢ á‰ƒàµ…á‰ˆ
à¢‰
àµ†à¢ˆ
àµ†à¢Š
á‰‰àµŒá‰ˆ
à«™
à«™
à«™
á‰‰ 
(5.28)
The above problem can be similarly solved via LCP framework, which is introduced in Sec. 5.8. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
106 
Linear Programming Methods
5.8.2	
 The Dual QP Problem
We reconsider the QP problem (5.22) and observe that the Lagrangian function (5.23) is stationary at 
the optimum with respect to x, u, v. Then, as per Lagrangian duality (Sec. 4.5), it can be used to define 
the following dual QP problem (called Wolfeâ€™s dual):
ÂÂƒÂš
à¢Ç¡à¢›Ç¡à¢œà£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒà¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ†à¢›à¯à¢àµ…à¢œà¯áˆºà¡­à¢àµ†à¢ˆáˆ»
Subject to: ×à£¦áˆºà¢Ç¡ à¢›Ç¡ à¢œáˆ»àµŒà¡½à¢àµ…à¢‰àµ†à¢›àµ…à¡­à¯à¢œàµŒà«™Ç¡ à¢àµ’à«™Ç¡ à¢œàµ’à«™
(5.29)
Further, by relaxing the non-negativity condition on the design variable x, we can eliminate u from the 
formulation, which results in a simpler dual problem defined as:
ÂÂƒÂš
à¢Ç¡à¢œà®¹à«™à£¦áˆºà¢Ç¡ à¢œáˆ»àµŒà¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ…à¢œà¯áˆºà¡­à¢àµ†à¢ˆáˆ»
Subject to: à¡½à¢àµ…à¢‰àµ…à¡­à¯à¢œàµŒà«™
(5.30)
The implicit function theorem allows us to express the solution vector x in the vicinity of the optimum 
point as a function of the Lagrange multipliers à¢œDVà¢àµŒà¢áˆºà¢œáˆ» Next, the Lagrangian is expressed as 
an implicit function È°áˆºà¢œáˆ» of the multipliers, termed as the dual function. Further, the dual function is 
obtained as a solution to the following minimization problem: 
È°áˆºà¢œáˆ»àµŒÂÂ‹Â
à¢à£¦áˆºà¢Ç¡ à¢œáˆ»àµŒà¬µ
à¬¶à¢à¯à¡½à¢àµ…à¢‰à¯à¢àµ…à¢œà¯áˆºà¡­à¢àµ†à¢ˆáˆ»
(5.31)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Fundamental Engineering Optimization 
Methods
107 
Linear Programming Methods
The solution is obtained by solving the FONC, the constraint in (5.30), for x as: 
à¢áˆºà¢œáˆ»àµŒàµ†à¡½à¬¿à¬µáˆºà¡­à¯à¢œàµ…à¢‰áˆ»
(5.32)
and substituting it in the Lagrangian function to obtain:
È°áˆºà¢œáˆ»àµŒàµ†à¬µ
à¬¶áˆºà¡­à¯à¢œàµ…à¢‰áˆ»à¯à¡½à¬¿à¬µáˆºà¡­à¯à¢œàµ…à¢‰áˆ»àµ†à¢œà¯à¢ˆ
àµŒàµ†à°­
à°®à¢œà¯áˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à¢œàµ†áˆºà¢‰à¯à¡½à¬¿à¬µà¡­à¯àµ…à¢ˆà¯áˆ»à¢œàµ†à°­
à°®à¢‰à¯à¡½à¬¿à¬µà¢‰
(5.33)
In terms of the dual function, the dual QP problem is defined as: 
ÂÂƒÂš
à¢œà®¹à«™È°áˆºà¢œáˆ»àµŒàµ†à¬µ
à¬¶áˆºà¡­à¯à¢œàµ…à¢‰áˆ»à¯à¡½à¬¿à¬µáˆºà¡­à¯à¢œàµ…à¢‰áˆ»àµ†à¢œà¯à¢ˆ
(5.34)
The dual problem can also be solved by application of FONC, where the gradient and Hessian of È°áˆºà¢œáˆ»
are given as:
×È° àµŒàµ†à¡­à¡½à¬¿à¬µáˆºà¡­à¯à¢œàµ…à¢‰áˆ»àµ†à¢ˆÇ¡
×à¬¶È° àµŒàµ†à¡­à¡½à¬¿à¬µà¡­à¯
(5.35)
By solving ×à¢œÈ° àµŒÍ²Ç¡ we obtain the solution to the Lagrange multipliers as:
à¢œàµŒàµ†áˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à¬¿à¬µáˆºà¡­à¯à¡½à¬¿à¬µà¢‰àµ…à¢ˆáˆ»
(5.36)
where the non-negativity of v is implied. Finally, the solution to the design variables is obtained from 
(5.32) as:
à¢àµŒà¡½à¬¿à¬µà¡­à¯áˆºà¡­à¡½à¬¿à¬µà¡­à¯áˆ»à¬¿à¬µáˆºà¡­à¯à¡½à¬¿à¬µà¢‰àµ…à¢ˆáˆ»àµ†à¡½à¬¿à¬µà¢‰
(5.37)
The dual methods have been successfully applied in structural mechanics. As an example of the dual QP 
problem, we consider a one-dimensional finite element analysis (FEA) problem involving two nodes.
Example 5.10: Finite Element Analysis (Belegundu and Chandrupatla, p. 187)
Let İà¬µÇ¡ İà¬¶represent nodal displacements in the simplified two node structure, and assume that a load P, 
where Ü²àµŒÍ¸Í²İ‡Ü°Ç¡ is applied at node 1. The FEA problem is formulated as minimization of the potential 
energy function given as:
ÂÂ‹Â
à¢—Ï‚ àµŒÍ³
Í´ à¢—à¯à¡·à¢—àµ†à¢—à¯à¢Œ
Subject to: İà¬¶àµ‘Í³Ç¤Í´
In the above problem, à¢—à¯àµŒáˆ¾İà¬µÇ¡ İà¬¶áˆ¿ represents the vector of nodal displacements. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
108 
Linear Programming Methods
The stiffness matrix K for the problem is given as: à¡·àµŒ
à¬µà¬´à°±
à¬·á‰‚Í´
àµ†Í³
àµ†Í³
Í³ á‰ƒ
à¯‡
à¯ Ç¤
For this problem: à¡½àµŒà¡·Ç¡ à¢ŒàµŒáˆ¾Ü²Ç¡ Í²áˆ¿à¯Ç¡ à¢‰àµŒàµ†à¢ŒÇ¡à¡­àµŒáˆ¾Í²
Í³áˆ¿Ç¡ à¢ˆàµŒÍ³Ç¤Í´Ç¤
Further, à¡­à¡½à¬¿à¬µà¡­à¯àµŒÍ¸ àµˆÍ³Í²à¬¿à¬¹Ç¡ à¢‰à¯à¡½à¬¿à¬µà¡­à¯àµŒàµ†Í³Ç¤ÍºÇ¡ à¢‰à¯à¡½à¬¿à¬µà¢‰àµŒÍ³Ç¤Í²Íº àµˆÍ³Í²à¬¿à¬¹Ç¤
We use (5.33) to obtain the dual function as: È°áˆºà¢œáˆ»àµŒàµ†Íµ àµˆÍ³Í²à¬¿à¬¹İ’à¬¶àµ†Í²Ç¤Í¸İ’àµ†Í³Ç¤Í²Íº àµˆÍ³Í²à¬¿à¬¹ 
From (5.36) the solution to Lagrange multiplier is: İ’àµŒÍ³ àµˆÍ³Í²à¬¸ 
Then, from (5.37), the optimum solution to the design variables is: İà¬µàµŒÍ³Ç¤Í·İ‰İ‰Ç¡ İà¬¶àµŒÍ³Ç¤Í´İ‰İ‰Ç¤
The optimum value of potential energy function is: Ï‚ àµŒÍ³Í´Í»Ü°İ‰Ç¤
Next, we proceed to define and solve the Linear Complementarity Problem.
5.9	
The Linear Complementary Problem
The application of optimality conditions to LP and QP problems leads to the Linear Complementary 
Problem (LCP), which can be solved using Simplex based methods. The LCP aims at finding vectors 
that satisfy linear equality, non-negativity, and complementarity conditions. When used in the context 
of optimization, the LCP simultaneously solves both primal and dual problems.
The general LCP problem is defined as follows: Given a real symmetric positive definite matrix M and 
a vector, q, find a vector à¢ àµ’à«™ such that: à¢àµŒà¡¹à¢ àµ…à¢—àµ’à«™Ç¡ à¢à¯à¢ àµŒà«™ 
In the case of QP problem, we define: à¡¹àµŒàµ¤à¡½
àµ†à¡­à¯
à¡­
à«™àµ¨Ç¡ à¢ àµŒá‰‚à¢
à¢œá‰ƒÇ¡ à¢àµŒá‰‚à¢›
à¢™á‰ƒÇ¡ à¢—àµŒá‰‚à¢‰
àµ†à¢ˆá‰ƒ to cast the 
problem into the LCP framework. Further, if Q is positive semidefinite, so is M, resulting in a convex 
LCP, which can be solved by Simplex methods, in particular, the Lemkeâ€™s algorithm. 
Toward finding a solution to the LCP, we observe that if all İà¯œàµ’Í²Ç¡ then z = 0 Qà¢ àµŒà«™solves the LCP. It 
is, therefore, assumed that one or more İà¯œàµÍ² Lemkeâ€™s algorithm introduces an artificial variable, z0, 
where İ–à¬´àµŒÈÂÂ‹Âáˆºİà¯œáˆ»È to cast LCP into Phase I Simplex framework. The resulting problem is given as:
ÂÂ‹Â İ–à¬´
Subject to: à¡µà¢àµ†à¡¹à¢ àµ†à¢‹İ–à¬´àµŒà¢—Ç¡ à¢à¯à¢ àµŒà«™Ç¡ à¢àµ’à«™Ç¡ à¢ àµ’à«™
(5.38)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
109 
Linear Programming Methods
where à¢‹àµŒáˆ¾Í³Í³ Ú® Í³áˆ¿à¯ and I is an identity matrix. The linear constraint is used to define the starting 
tableau for the Simplex method, where an initial BFS is given as: à¢àµŒà¢—àµ…à¢‹İ–à¬´àµ’Í²Ç¡ à¢ àµŒÍ² The 
algorithm starts with a pivot operation aimed to bring z0 into the basis. Thereafter, the EBV is selected as 
complement of the LBV in the previous iteration. Thus, if İ“à¯¥ leaves the basis, İ–à¯¥ enters the basis in the 
next tableau, or vice versa, which maintains the complementarity condition İ“à¯¥İ–à¯¥àµŒÍ²Ç¤ The algorithm 
terminates when z0 has become nonbasic.
Lemkeâ€™s Algorithm for solving LCP (Belegundu and Chandrupatla, p. 178): 
1.	 If all qi > 0, then LCP solution is: İ–à¬´àµŒÍ²Ç¡ à¢àµŒà¢—Ç¡ à¢ àµŒà«™ No further steps are necessary. 
2.	 If some İà¯œàµÍ²Ç¡ select İ–à¬´àµŒÈÂÂ‹Âáˆºİà¯œáˆ»È to construct the initial tableau. 
3.	 Choose the most negative İà¯œ row and the z0 column to define the pivot element. In the first 
step z0 enters the basis, İ“à¯œ corresponding to most negative İà¯œ exits the basis. Henceforth, all 
qi â‰¥ 0.
4.	 If basic variable in column i last exited the basis, its complement in column j enters the 
basis. (At first iteration, İ“à¯œ exits and İ–à¯œ enters the basis). Perform the ratio test for column j 
to find the least among İà¯œ /(positive row element i). The basic variable corresponding to row 
i now exits the basis. If there are no positive row elements, there is no solution to the LCP
5.	 If the last operation results in the exit of the basic variable z0, then the cycle is complete, 
stop. Otherwise go to step 3.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Fundamental Engineering Optimization 
Methods
110 
Linear Programming Methods
Two examples of Lemkeâ€™s algorithm are presented below:
Example 5.11: Lemkeâ€™s algorithm
We consider the following QP problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµ†İ”à¬µİ”à¬¶àµ†İ”à¬µàµ…Í´İ”à¬¶
6XEMHFWWRİ”à¬µàµ…İ”à¬¶àµ‘Í·Ç¡ İ”à¬µàµ…Í´İ”à¬¶àµ‘Í³Í²Ç¢ İ”à¬µÇ¡ İ”à¬¶àµ’Í²
For the given problem: à¡½àµŒá‰‚Í´
àµ†Í³
àµ†Í³
Í´á‰ƒÇ¡ à¢‰à¯àµŒáˆ¾àµ†Í³
Í´áˆ¿Ç¡ à¡­àµŒá‰‚Í³
Í³
Í³
Í´á‰ƒÇ¡ à¢ˆàµŒá‰‚Í·
Í³Í²á‰ƒÇ¡ İ–à¬´àµŒàµ†Í³Ç¤
The resulting initial tableau for the problem is given as:

%DVLF à¢à«š à¢à«› à¢à«œ à¢à« à¢ à«š à¢ à«› à¢ à«œ à¢ à« à¢ à«™
à¢—
à¢à«š










à¢à«›










à¢à«œ










à¢à«










SLYRW
We begin by a pivot step aimed at bringing İ–à¬´ into the basis as represented by the following tableau:
%DVLF à¢à«š à¢à«› à¢à«œ à¢à« à¢ à«š à¢ à«› à¢ à«œ à¢ à« à¢ à«™ à¢—
à¢ à«™










à¢à«›










à¢à«œ










à¢à«










3LYRW
This is followed by further simplex iterations that maintain the complementarity conditions. The 
algorithm terminates when exits the basis. The resulting series of tableaus is given below:

%DVLF
à¢à«š
à¢à«› à¢à«œ à¢à« à¢ à«š
à¢ à«›
à¢ à«œ
à¢ à«
à¢ à«™
à¢—
à¢ à«š










à¢à«›





    
à¢à«œ






   
à¢à«






   
The algorithm terminates after two steps as İ–à¬´ has exited the basis. The basic variables are given as: 
İ–à¬µİ“à¬¶İ“à¬·İ“à¬¸ so that the complementarity conditions are satisfied, and the optimum solution is given 
as: İ”à¬µàµŒÍ²Ç¤Í·Ç¡ İ”à¬¶àµŒÍ²Ç¡ İ‚×› àµŒàµ†Í²Ç¤Í´Í·Ç¤
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
111 
Linear Programming Methods
As the second LCP example, we reconsider the one-dimensional finite element analysis (FEA) problem 
that was solved earlier (Example 5.8).
Example 5.12: Finite Element Analysis (Belegundu and Chandrupatla, p. 187)
The problem is stated as:
ÂÂ‹Â
à¢—Ï‚ àµŒÍ³
Í´ à¢—à¯à¡·à¢—àµ†à¢—à¯à¢Œ
Subject to: İà¬¶àµ‘Í³Ç¤Í´
In the above problem, à¢—à¯àµŒáˆ¾İà¬µÇ¡ İà¬¶áˆ¿ represents a vector of nodal displacements. A load Ü²Ç¡Ü²àµŒÍ¸Í²İ‡Ü°Ç¡ 
is applied at node 1, so that à¢ŒàµŒáˆ¾Ü²Ç¡ Í²áˆ¿à¯Ç¤The stiffness matrix K is given as: à¡·àµŒ
à¬µà¬´à°±
à¬·á‰‚Í´
àµ†Í³
àµ†Í³
Í³ á‰ƒ
à¯‡
à¯ Ç¤
For this problem: à¡½àµŒà¡·Ç¡ à¢‰àµŒàµ†à¢ŒÇ¡à¡­àµŒáˆ¾Í²
Í³áˆ¿Ç¡ à¢ˆàµŒÍ³Ç¤Í´Ç¡ İ–à¬´àµŒàµ†Í³Ç¤
The initial and the subsequent tableaus leading to the solution of the problem are given below:

%DVLF à¢à«š à¢à«› à¢à«œ
à¢ à«š
à¢ à«›
à¢ à«œ à¢ à«™
à¢—
à¢à«š








à¢à«›








à¢à«œ








3LYRW

%DVLF à¢à«š à¢à«› à¢à«œ
à¢ à«š
à¢ à«›
à¢ à«œ à¢ à«™
à¢—
à¢ à«™








à¢à«›



 



à¢à«œ








3LYRW

%DVLF
à¢à«š
à¢à«›
à¢à«œ à¢ à«š
à¢ à«›
à¢ à«œ
à¢ à«™
à¢—
à¢ à«™








à¢ à«š
 






à¢à«œ








3LYRW
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
112 
Linear Programming Methods

%DVLF
à¢à«š
à¢à«›
à¢à«œ
à¢ à«š à¢ à«›
à¢ à«œ
à¢ à«™
à¢—
à¢ à«™
  





à¢ à«š
 






à¢ à«›
 






3LYRW

%DVLF
à¢à«š
à¢à«›
à¢à«œ
à¢ à«š à¢ à«›
à¢ à«œ
à¢ à«™
à¢—
à¢ à«œ






 
à¢ à«š








à¢ à«›








The algorithm terminates when İ–à¬´ has exited the basis. The final solution to the problem is given as:
İ–à¬µàµŒÍ³Ç¤Í·İ‰İ‰Ç¡ İ–à¬¶àµŒÍ³Ç¤Í´İ‰İ‰Ç¡ Ï‚ àµŒÍ³Í´Í»Ü°İ‰Ç¤
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Fundamental Engineering Optimization 
Methods
113 
Discrete Optimization
6	 Discrete Optimization 
This chapter is devoted to the study of solution approaches to discrete optimization problems that 
involve decision making, when the variables must be chosen from a discrete set. Many real world design 
problems fall in this category. For example, variables in optimization problems arising in production 
or transportation of goods represent discrete quantities and can only take on integer values. Further, 
scheduling and networking problems (e.g., assigning vehicles to transportation networks, frequency 
assignment in cellular phone networks, etc.) are often modeled with variables that can only take on 
binary values. The integer programming problem and binary integer programming problem are special 
cases of optimization problems where solution choices are limited to discrete sets.
Discrete optimization is closely related to combinatorial optimization that aims to search for the 
best object from a set of discrete objects. Classical combinatorial optimization problems include the 
econometric problems (knapsack problem, capital budgeting problem), scheduling problems (facility 
location problem, fleet assignment problem) and network and graph theoretic problems (traveling 
salesman problem, minimum spanning tree problem, vertex/edge coloring problem, etc.). Combinatorial 
optimization problems are NP-complete, meaning they are non-deterministic polynomial time problems, 
and finding a solution is not guaranteed in finite time. Heuristic search algorithms are, therefore, 
commonly employed to solve combinatorial optimization problems. Considerable research has also been 
devoted to finding computation methods that utilize polyhedral structure of integer programs. 
Learning Objectives. The learning aims in this chapter are:
1.	 Study the structure and formulation of a discrete optimization problem.
2.	 Learn common solution approaches to the discrete optimization problems.
3.	 Learn to use the branch-and-bound and cutting plane methods to solve the mixed integer 
programming problem.
6.1	
Discrete Optimization Problems
A discrete optimization problem may be formulated in one of the following ways:
1.	 An integer programming (IP) problem is formulated as: 
ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢
VXEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ à¢× Ôºà¯¡Ç¡ à¢àµ’à«™ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
114 
Discrete Optimization
2.	 A binary integer programming (BIP) problem is formulated as:
ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢
VXEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ à¢× áˆ¼Í²Ç¡Í³áˆ½à¯¡
 
3.	 A combinatorial optimization (CO) problem is formulated as:
ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢
VXEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ İ”à¯œ× áˆ¼Í²Ç¡Í³áˆ½áˆºİ…× Ü¤áˆ»Ç¡ İ”à¯œ× Ôºáˆºİ…× Ü«áˆ» 
4.	 A Mixed integer programming (MIP) problem is formulated as:
ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢
VXEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ İ”à¯œàµ’Í²Ç¡ İ”à¯œ× ÔºÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠà¯—Ç¢İ”à¯œà¯…àµ‘İ”à¯œàµ‘İ”à¯œà¯Ç¡ İ…àµŒİŠà¯—àµ…Í³Ç¡ Ç¥ Ç¡ İŠ  
5.	 A general mixed variable design optimization problem is formulated as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
VXEMHFWWRİ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¢İƒà¯áˆºİ”áˆ»àµ‘Í²Ç¡ İ†àµŒİ…Ç¡ Ç¥ Ç¡ İ‰Ç¢İ”à¯œ× Ü¦Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠà¯—Ç¢İ”à¯œà¯…àµ‘İ”à¯œàµ‘ 
İ”à¯œà¯Ç¡ İ…àµŒİŠà¯—àµ…Í³Ç¡ Ç¥ Ç¡ İŠ 
In the following, we discuss solution approaches to linear discrete optimization problems (1â€“4 above). 
6.2	
Solution Approaches to Discrete Problems
We first note that the discrete optimization problems may be solved by enumeration, i.e., an ordered 
listing of all solutions. The number of combinations to be evaluated to solve the problem is given as:
Ü°à¯–àµŒÏ‚
İà¯œ
à¯¡à³
à¯œà­€à¬µ
 where İŠà¯— is the number of design variables and İà¯œ represents the number of discrete 
values for the design variable İ”à¯œ This approach is, however, not practically feasible as the Ü°à¯– increases 
rapidly with increase in İŠà¯— and İà¯œ
Further, two common approaches to solve linear discrete optimization problems are: 
1.	 The branch and bound (BB) technique that divides the problem into a series of 
subprograms, where any solution to the original problem is contained in exactly one of the 
subprograms. 
2.	 The cutting plane method that iteratively refines the solution by adding additional linear 
inequality constraints (cuts) aimed at excluding non-integer solutions to the problem. 
These two approaches are discussed below. Besides, other approaches for solving discrete optimization 
problems include heuristic methods, such as tabu (neighborhood) search, hill climbing, simulated 
annealing, genetic algorithms, evolutionary programming, and particle swarm optimization. These topics 
are, however, not discussed here. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
115 
Discrete Optimization
In the following, we begin with the methods to solve an LP problem involving integral coefficients, 
followed by the BIP problems, and finally the IP/MIP problems. 
6.3	
Linear Programming Problems with Integral Coefficients
In this section, we consider an LP problem modeled with integral coefficients described as:
ÂÂ‹Â
à¢İ–àµŒà¢‰à¯à¢
6XEMHFWWRà¡­à¢àµŒà¢ˆÇ¡ à¢àµ’à«™Ç¡ à¡­× Ôºà¯ àµˆà¯¡Ç¡ à¢ˆ× Ôºà¯ Ç¡ à¢‰× Ôºà¯¡
(6.1)
We further assume that A is totally unimodular, i.e., every square submatrix C of A, has det áˆºà¡¯áˆ»× áˆ¼Í²Ç¡ àµ‡Í³áˆ½Ç¤
In that case, every vertex of the feasible region, or equivalently, every BFS of the LP problem is integral. In 
particular, the optimal solution returned by the Simplex algorithm is integral. Thus, total unimodularity 
of A is a sufficient condition for integral solution to LP problems.
To show that an arbitrary BFS, x, to the problem is integral, let à¢à®» represent the elements of x 
corresponding to the basis columns, then there is a square nonsingular submatrix B of A, such that à¡®à¢à®»àµŒ
b. Further, by unimodularity assumption, detáˆºà¡®áˆ»àµŒàµ‡Í³Ç¡DQGà¡®à¬¿à¬µàµŒàµ‡Ü£İ€İ†à¡®Ç¡Z
à¬µ
 where Adj represents the 
adjoint matrix and is integral. Therefore, à¢à®»àµŒà¡®à¬¿à¬µà¢ˆ is integral.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Fundamental Engineering Optimization 
Methods
116 
Discrete Optimization
Further, if A is totally unimodular, so is áˆ¾à¡­à¡µáˆ¿ This applies to problems involving inequality constraints:
à¡­à¢àµ‘à¢ˆÇ¡ which, when converted to equality via addition of slack variable à¢™ are represented as: à¡­à¢àµ…à¡µà¢™àµŒ
à¢ˆÇ¡or áˆ¾à¡­à¡µáˆ¿á‰‚à¢
à¢™á‰ƒàµŒà¢ˆ7KHQLIà¡­× Ôºà¯ àµˆà¯¡ is totally unimodular and à¢ˆ× Ôºà¯  all BFSs to the problem have 
integral components.
We, however, note that total unimodularity of A is a sufficient but not necessary condition for an integral 
solution; integral BFS may still be obtained if A is not totally unimodular. An example would be a matrix 
with isolated individual elements that do not belong to the set: áˆ¼àµ†Í³Ç¡Í²Ç¡Í³áˆ½. Indeed, a necessary condition 
for integral solutions to LP problem is that each İ‰àµˆİ‰ basis submatrix B of A has determinant equal 
to àµ‡.
An example of an LP problem with integral coefficients is considered below.
Example 6.1: Integer BFS
We consider the following LP problem with integer coefficients:
ÂÂƒÂš
à¢
İ–àµŒÍ´İ”à¬µàµ…Íµİ”à¬¶
6XEMHFWWRİ”à¬µàµ‘ÍµÇ¡ İ”à¬¶àµ‘Í·Ç¡ İ”à¬µàµ…İ”à¬¶àµ‘Í¹Ç¡à¢× Ôºà¬¹Ç¡ à¢àµ’à«™
Following the introduction of slack variables, the constraint matrix and the right hand side are given as: 
à¡­àµŒàµ¥
Í³
Í²
Í³

Í²
Í³
Í³

Í³
Í²
Í²

Í²
Í³
Í²

Í²
Í²
Í³
àµ©à¢ˆàµŒàµ¥
Íµ
Í·
Í¹
àµ©× Ôºà¬·Ç¡Z where we note that A is unimodular and à¢ˆ× Ôºà¬· Then, using the 
simplex method, the optimal integral solution is obtained as: à¢à¯àµŒáˆ¾Í´Ç¡Í·Ç¡Í³Ç¡Í²Ç¡Í²áˆ¿Ç¡ZLWKİ–×› àµŒÍ³Í»
6.4	
Binary Integer Programming Problems
In this section, we discuss solution of the BIP problem defined as:
ÂÂ‹Â
à¢İ–àµŒà¢‰à¯à¢
6XEMHFWWRà¡­à¢àµ’à¢ˆÇ¡ İ”à¯œ× áˆ¼Í²Ç¡Í³áˆ½Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ
(6.2)
where we additionally assume that à¢‰àµ’à«™ We note that this is not a restrictive assumption, as any variable 
İ”à¯œ with negative Ü¿à¯œ in the objective function can be replaced by: İ”à¯œ
á‡±àµŒÍ³ àµ†İ”à¯œ 
Further, we note that under not-too-restrictive assumptions most LP problems can be reformulated in 
the BIP framework. For example, if the number of variables is small, and the bounds İ”à¯ à¯œà¯¡àµİ”à¯œàµİ”à¯ à¯”à¯«
on the design variables are known, then each İ”à¯œ can be represented as a binary number using İ‡ bits, 
where Í´à¯à¬¾à¬µàµ’İ”à¯ à¯”à¯«àµ†İ”à¯ à¯œà¯¡ The resulting problem involves selection of the bits and is a BIP problem.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
117 
Discrete Optimization
The BIP problem can be solved by implicit enumeration. In implicit enumeration, obviously infeasible 
solutions are eliminated and the remaining ones are evaluated (i.e., enumerated) to find the optimum. The 
search starts from İ”à¯œàµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠÇ¡ which is optimal. If this is not feasible, then we systematically 
adjust individual variable values till feasibility is attained. The implicit enumeration procedure is coded 
in the following algorithm that does not require an LP solver:
Binary Integer Programming Algorithm (Belegundu and Chandrupatla, p. 364): 
1.	 Initialize: set Wİ”à¯œàµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ if this solution is feasible, we are done.
2.	 For some i set İ”à¯œàµŒÍ³ If the resulting solution is feasible, then record it if this is the first 
feasible solution, or if it improves upon a previously recorded feasible solution.
3.	 Backtrack VHWİ”à¯œàµŒÍ²áˆ» if a feasible solution was reached in the previous step, or if feasibility 
appears impossible in this branch.
4.	 Choose another i and return to 2.
The progress of the algorithm is graphically recorded in a decision-tree, using nodes and arcs with node 
0 representing the initial solution İ”à¯œàµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠ and node i representing a change in the value 
of variable İ”à¯œ. From node k, if we choose to raise variable İ”à¯œ to one, then this is represented as an arc 
from node k to node i. At node i the following possibilities exist:
1.	 The resulting solution is feasible, meaning no further improvement in this branch is 
possible.
2.	 Feasibility is impossible from this branch.
3.	 The resulting solution is not feasible, but feasibility or further improvement are possible.
In the first two cases, the branch is said to have been fathomed. We then backtrack to node k, where 
variable İ”à¯œ is returned to zero. We next seek another variable to be raised to one. The algorithm continues 
till all branches have been fathomed, and returns an optimum 0-1 solution.
We consider the following example of a BIP problem.
Example 6.2: Implicit enumeration (Belegundu and Chandrupatla, p. 367)
A machine shaft is to be cut at two locations to given dimensions 1, 2, using one of the two available 
processes, A and B. The following information on process cost and three-sigma standard deviation is 
available, where the combined maximum allowable tolerance is to be limited to 12mils:
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
118 
Discrete Optimization
3URFHVV?-RE
-RE
-RE
&RVW
6'
&RVW
6'
3URFHVV$

Â“PLOV

Â“PLOV
3URFHVV%

Â“PLOV

Â“PLOV
Let İ”à¯œÇ¡ İ…àµŒÍ³ àµ†Í¶ denote the available processes for both jobs, and let İà¯œ denote their associated 
tolerances. The BIP problem is formulated as:
ÂÂ‹Â
à¢İ–àµŒÍ¸Í·İ”à¬µàµ…Í·Í¹İ”à¬¶àµ…Í¶Í´İ”à¬·àµ…Í´Í²İ”à¬¸
6XEMHFWWRİ”à¬µàµ…İ”à¬¶àµŒÍ³Ç¡ İ”à¬·àµ…İ”à¬¸àµŒÍ³Ç¡ Ïƒ İà¯œ
à¯œ
àµ‘Í³Í´Ç¡ İ”à¯œ× áˆ¼Í²Ç¡Í³áˆ½Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡Í¶Ç¤
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Fundamental Engineering Optimization 
Methods
119 
Discrete Optimization
The problem is solved via implicit enumeration; the resulting decision-tree is represented below:
Í³
Í¶
Íµ
İ‚àµŒÍ³Í²Í¹
1),
Í²
1),
İ”à¬·àµŒÍ³
İ”à¬¸àµŒÍ³
Í´
Í¶
Íµ
İ‚àµŒÍ»Í»
2SWLPXP
İ”à¬·àµŒÍ³
İ”à¬¸àµŒÍ³
1),
İ”à¬µàµŒÍ´
İ”à¬µàµŒÍ³
Fig 6.1: The decision tree for Example 6.2 (NFI: No further improvement
6.5	
Integer Programming Problems
This section discusses the solution approaches to the IP problem formulated as:
ÂÂƒÂš
à¢
İ–àµŒà¢‰à¯à¢
6XEMHFWWRà¡­à¢àµ‘à¢ˆÇ¡ à¢× Ôºà¯¡Ç¡ à¢àµ’à«™

(6.3)
To start with, the optimization problem that results when integrality constraint in the above problem 
is ignored is termed as LP relaxation of the IP problem. While a naÃ¯ve solution to the IP problem may 
be to round off the non-integer LP relaxation solution, in general, this approach does not guarantee a 
satisfactory solution to IP problem. 
In the following, we discuss two popular methods for solving IP problems: the branch and bound 
method and the cutting plane method. Both methods begin by first solving the LP relaxation problem 
and subsequently using the LP solution to subsequently bind the IP solution. 
6.5.1	
 The Branch and Bound Method
The BB method is the most widely used method for solving IP problems. The method has its origins in 
computer science, where search over a large finite space is performed by using bounds on the objective 
function to prune the search tree. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
120 
Discrete Optimization
The BB method iteratively solves an IP problem as follows: it first obtains the LP relaxation solution; 
next, it introduces integrality constraints to define subprograms that effectively divide the feasible region 
into smaller subsets (branching); it then calculates objective function bounds for each subprogram 
(bounding); finally, it uses those bounds to discard non-promising solutions from further consideration 
(fathoming). The procedure ends when every branch has been fathomed and an optimum integer solution, 
if one exists, has been found. 
A decision tree is normally used to record the progress of the BB algorithm, where the LP relaxation 
solution is represented as node 0. Subsequently, at each node k, the algorithm sequences through the 
following phases:
1.	 Selection. If some variables in the simplex solution at node k have non-integer values, the 
algorithm selects the one with the lowest index (or the one with greatest economic impact) 
for branching.
2.	 Branching. The solution at node k is partitioned into two mutually exclusive subsets, each 
represented by a node in the decision tree and connected to node k by an arc. It involves 
imposition of two integer constraints İ”à¯œàµ‘Ü«Ç¡ İ”à¯œàµ’Ü«àµ…Í³Ç¡ Ü«àµŒÛİ”à¯œÛ‚ generating two new 
subprograms where each solution to the original IP problem is contained in exactly one of 
the subprograms. 
3.	 Bounding. In this phase, upper bounds on the optimal subproblem solutions are 
established. Solving a subprogram via LP solver results in one of the following possibilities: 
a)	 There is no feasible solution.
b)	 The solution does not improve upon an available IP solution.
c)	 An improved IP solution is returned and is recorded as current optimal.
d)	 A non-integer solution that is better than the current optimal is returned.
4.	 Fathoming. In the first three cases above the current branch is excluded from further 
consideration. The algorithm then backtracks to the most recently unbranched node in the 
tree and continues with examining the next node in a last in first out (LIFO) search strategy. 
Finally, the process ends when all branches have been fathomed, and an integer optimal solution to the 
problem, if one exists, has been found. 
Let NF denote the set of nodes not yet fathomed, F denote the feasible region for the original IP problem, 
Ü¨à¯‹ denote the feasible region for the LP relaxation, Ü¨à¯ denote the feasible region at node İ‡Üµà¯ denote 
the subproblem defined as: ÂÂƒÂš
à¢
İ–à¯àµŒà¢‰à¯à¢Ç¡ à¢× Ü¨à¯ and let İ–à¯… denote the lower bound on the optimal 
solution. Then, the BB algorithm is given as follows:
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
121 
Discrete Optimization
Branch-and-bound Algorithm (Sierksma, p. 219): 
Initialize: set Ü¨à¬´àµŒÜ¨à¯‹Ç¡ Ü°Ü¨àµŒáˆ¼Í²áˆ½Ç¡ İ–à¯…àµŒàµ†Î».
While Ü°Ü¨àµ×Ç¡
1.	 Select a label İ‡× Ü°Ü¨. 
2.	 Determine if there exists an optimal solution áˆºİ–à¯Ç¡ à¢à¯áˆ»WRÜµà¯Ç¡HOVHVHWİ–à¯àµŒàµ†Î»
3.	 If İ–à¯àµİ–à¯…Ç¡WKHQLIà¢à¯× Ü¨Ç¡VHWİ–à¯…àµŒİ–à¯
4.	 If İ–à¯àµ‘İ–à¯…Ç¡VHWÜ°Ü¨àµŒÜ°Ü¨Ì³áˆ¼İ‡áˆ½
5.	 If İ–à¯àµİ–à¯…DQGà¢à¯×‘ Ü¨Ç¡ partition Ü¨à¯L into two or more subsets as follows: choose a variable 
İ”à¯œ×à¢à¯with fractional value, İ”à¯œàµŒÜ«àµ…ßœà¯œÇ¡ Ü«àµŒÛİ”à¯œÛ‚Ç¡ Í² àµßœà¯œàµÍ³Ç¤ Define two new 
subprograms: Ü¨à¯à°­àµŒÜ¨à¯×ª áˆ¼İ”à¯œàµ‘Ü«áˆ½Ç¡ Ü¨à¯à°®àµŒÜ¨à¯à°®×ª áˆ¼İ”à¯œàµ’Ü«àµ…Í³áˆ½6HWÜ°Ü¨àµŒÜ°Ü¨×« áˆ¼İ‡à¬µÇ¡ İ‡à¬¶áˆ½
An example is now presented to illustrate the BB algorithm.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Fundamental Engineering Optimization 
Methods
122 
Discrete Optimization
Example 6.3: Branch and bound algorithm
We consider the following IP problem (Belegundu and Chandrupatla, p. 383): A tourist bus company 
having a budget of $10M is considering acquiring a fleet with a mix of three models: a 15-seat van costing 
$35,000, a 30-seat minibus costing $60,000, and a 60-seat bus costing $140,000. A total capacity of 2000 
seats is required. At least one third of the vehicles must be the big buses. If the estimated profits per 
seat per month for the three models are: $4, $3, and $2 respectively, determine the number of vehicles 
of each type to be acquired to maximize profit.
Let İ”à¬µÇ¡ İ”à¬¶Ç¡ İ”à¬· denote the quantities to be purchased for each of the van, minibus, and big bus; then, the 
optimization problem is formulated as:
0D[LPL]Hİ–àµŒÍ¸Í²İ”à¬µàµ…Í»Í²İ”à¬¶àµ…Í³Í´Í²İ”à¬·
6XEMHFWWRÍ·İ”à¬µàµ…Í¸Í²İ”à¬¶àµ…Í³Í¶Í²İ”à¬·àµ‘Í³Í²Í²Í²Ç¡ Í³Í·İ”à¬µàµ…ÍµÍ²İ”à¬¶àµ…Í¸Í²İ”à¬·àµ’Í´Í²Í²Í²Ç¡ İ”à¬µàµ…İ”à¬¶àµ†Í´İ”à¬·àµ‘Í²Ç¢
İ”à¬µÇ¡ İ”à¬¶Ç¡ İ”à¬·àµ’Í²DQGLQWHJHU
 
Following steps are taken to solve the problem. The progress is also shown in a decision tree in Fig. 6.2:
1.	 Üµà¬´Ç£ the LP relaxation problem áˆºÜ¨à¬´àµŒÜ¨à¯‹áˆ» is first solved and produces an optimum solution: 
İ”à¬µ
×› àµŒ Í²Ç¡ İ”à¬¶
×› àµŒÍ¹Ç¤Í¸Í»Ç¡ İ”à¬·
×› àµŒÍµÇ¤ÍºÍ·Ç¡ İ‚×› àµŒÍ³Í³Í·ÍµÇ¤Íº which serves as an upper bound for IP 
solution.
2.	 Üµà¬µÇ£ Ü¨à¬´×« áˆ¼İ”à¬·àµ‘Íµáˆ½ is solved and produces an integer solution: 
İ”à¬µ
×› àµŒÍ²Ç¡ İ”à¬¶
×› àµŒÍ¸Ç¡ İ”à¬·
×› àµŒÍµÇ¡ İ‚×› àµŒÍ»Í²Í² This is recorded as current optimum. 
3.	 Üµà¬¶Ç£ Ü¨à¬´×« áˆ¼İ”à¬·àµ’Í¶áˆ½ produces a non-integer solution: 
İ”à¬µ
×› àµŒÍ³Ç¤Í¸Ç¡ İ”à¬¶
×› àµŒÍ¸Ç¤Í¶Ç¡ İ”à¬·
×› àµŒÍ¶Ç¡ İ‚×› àµŒÍ³Í³Í·Í´. 
4.	 Üµà¬·Ç£ Ü¨à¬¶×« áˆ¼İ”à¬¶àµ‘Í¸áˆ½ produces a non-integer solution: 
İ”à¬µ
×› àµŒÍ´Ç¤Í³Ç¡ İ”à¬¶
×› àµŒÍ¸Ç¡ İ”à¬·
×› àµŒÍ¶Ç¤Í²Í·Ç¡ İ‚×› àµŒÍ³Í³Í·Í³Ç¤Í¶
5.	 Üµà¬¸Ç£ Ü¨à¬·×« áˆ¼İ”à¬·àµ‘Í¶áˆ½ produces an integer solution: İ”à¬µ
×› àµŒÍ´Ç¡ İ”à¬¶
×› àµŒÍ¸Ç¡ İ”à¬·
×› àµŒÍ¶Ç¡ İ‚×› àµŒÍ³Í³Í¶Í² This 
is recorded as the new optimum and the branch is fathomed.
6.	 Üµà¬¹Ç£ Ü¨à¬·×« áˆ¼İ”à¬·àµ’Í·áˆ½ produces a non-integer solution: 
İ”à¬µ
×› àµŒÍºÇ¤Í·Í¹Ç¡ İ”à¬¶
×› àµŒÍ²Ç¡ İ”à¬·
×› àµŒÍ·Ç¡ İ‚×› àµŒÍ³Í³Í³Í¶Ç¤Íµ which is lower than the current optimum, so the 
branch is fathomed. 
7.	 Üµà¬ºÇ£ Ü¨à¬¶×« áˆ¼İ”à¬¶àµ’Í¹áˆ½ produces a non-integer solution: İ”à¬µ
×› àµŒÍ²Ç¤Í·Í¹Ç¡ İ”à¬¶
×› àµŒÍ¹Ç¡ İ”à¬·
×› àµŒÍ¶Ç¡ İ‚×› àµŒÍ³Í³Í¶Í¶Ç¤Íµ 
8.	 Üµà¬»Ç£ Ü¨à¬º×« áˆ¼İ”à¬µàµ‘Í²áˆ½ produces a non-integer solution: İ”à¬µ
×› àµŒÍ²Ç¡ İ”à¬¶
×› àµŒÍ¹Ç¤ÍµÍµÇ¡ İ”à¬·
×› àµŒÍ¶Ç¡ İ‚×› àµŒÍ³Í³Í¶Í² 
which does not improve upon the current optimum. The branch is fathomed.
9.	 Üµà¬¼Ç£ Ü¨à¬º×« áˆ¼İ”à¬µàµ’Í³áˆ½ has no feasible solution. The branch is fathomed. 
10.	All branches having been fathomed, the optimal solution is: İ”×› àµŒáˆºÍ´Ç¡Í¸Ç¡Í¶áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í¶Í² 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
123 
Discrete Optimization
Üµà¬´Ç£ Ü¨à¬´àµŒÜ¨à¯‹
à¢×› àµŒáˆºÍ²Ç¡ Í¹Ç¤Í¸Í»Ç¡ ÍµÇ¤ÍºÍ·áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í·ÍµÇ¤Íº
Üµà¬µÇ£ Ü¨à¬´×« áˆ¼İ”à¬·àµ‘Íµáˆ½
à¢×› àµŒáˆºÍ²Ç¡ Í¸Ç¡Íµáˆ»Ç¡ İ‚×› àµŒÍ»Í²Í²
Üµà¬¶Ç£ Ü¨à¬´×« áˆ¼İ”à¬·àµ’Í¶áˆ½
à¢×› àµŒáˆºÍ³Ç¤Í¸Ç¡ Í¸Ç¤Í¶Ç¡ Í¶áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í·Í´
Üµà¬·Ç£ Ü¨à¬¶×« áˆ¼İ”à¬¶àµ‘Í¸áˆ½
à¢×› àµŒáˆºÍ´Ç¤Í³Ç¡ Í¸Ç¡ Í¶Ç¤Í²Í·áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í·Í³Ç¤Í¶
Üµà¬ºÇ£ Ü¨à¬¶×« áˆ¼İ”à¬¶àµ’Í¹áˆ½
à¢×› àµŒáˆºÍ²Ç¤Í·Í¹Ç¡ Í¹Ç¡ Í¶áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í¶Í¶Ç¤Íµ
Üµà¬¸Ç£ Ü¨à¬·×« áˆ¼İ”à¬·àµ‘Í¶áˆ½
à¢×› àµŒáˆºÍ´Ç¡ Í¸Ç¡ Í¶áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í¶Í²
Üµà¬¹Ç£ Ü¨à¬·×« áˆ¼İ”à¬·àµ’Í·áˆ½
à¢×› àµŒáˆºÍºÇ¤Í·Í¹Ç¡ Í²Ç¡ Í·áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í³Í¶Ç¤Íµ
Üµà¬»Ç£ Ü¨à¬º×« áˆ¼İ”à¬µàµ‘Í²áˆ½
à¢×› àµŒáˆºÍ²Ç¡ Í¹Ç¤ÍµÍµÇ¡ Í¶áˆ»Ç¡ İ‚×› àµŒÍ³Í³Í¶Í²
Üµà¬¼Ç£ Ü¨à¬º×« áˆ¼İ”à¬µàµ’Í³áˆ½
Ü°Ü¨Üµ
 Fig. 6.2: The decision tree for Example 6.3.
6.5.2	
The Cutting Plane Method
Proposed by Gomory in 1958, the cutting plane method or Gomoryâ€™s method similarly begins with LP 
relaxation of the IP problem. It then trims the feasible region by successively adding linear constraints 
aimed to prune the non-integer solutions without losing any of the integer solutions. The new constraints 
are referred to as Gomory cuts. The process is repeated till an optimal integer solution has been obtained 
(Belegundu and Chandrupatla, p. 372; Chong and Zak, p. 438). 
To develop the cutting plan method, we assume that the partitioned constraint matrix for the LP relaxation 
problem is given in canonical form as:
à¡µà¢à®»àµ…à¡­à¯‡à¢à¯‡àµŒà¢ˆ
(6.4)
where à¢à®» and à¢à¯‡ refer to the basic and nonbasic variables. The corresponding BFS is given as: à¢à®»àµŒà¢ˆÇ¡
à¢à¯‡àµŒà«™Ç¤ Next, we consider the ith component of the solution: İ”à¯œàµ…Ïƒ
Ü½à¯œà¯İ”à¯
à¯¡
à¯à­€à¯ à¬¾à¬µ
àµŒÜ¾à¯œÇ¡ and use the 
floor operator to separate it into integer and non-integer parts as:
İ”à¯œàµ…
à·àµ«à¶‹Ü½à¯œà¯à¶àµ…ß™à¯œà¯àµ¯İ”à¯
à¯¡
à¯à­€à¯ à¬¾à¬µ
àµŒÛÜ¾à¯œÛ‚ àµ…ßšà¯œ
(6.5)
Then, since à¶‹Ü½à¯œà¯à¶àµ‘Ü½à¯œà¯ a feasible solution that satisfies (6.5) also satisfies:
İ”à¯œàµ…
à·à¶‹Ü½à¯œà¯à¶İ”à¯
à¯¡
à¯à­€à¯ à¬¾à¬µ
àµ‘Ü¾à¯œ
(6.6)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
124 
Discrete Optimization
whereas, an integer feasible solution can be characterized by: 
İ”à¯œàµ…
à·à¶‹Ü½à¯œà¯à¶İ”à¯
à¯¡
à¯à­€à¯ à¬¾à¬µ
àµ‘ÛÜ¾à¯œÛ‚
(6.7)
The integer feasible solution also satisfies the difference of the two inequalities, which is given as:
à·ß™à¯œà¯İ”à¯
à¯¡
à¯à­€à¯ à¬¾à¬µ
àµ’ßšà¯œ
(6.8)
The above inequality is referred to as the Gomory cut. We note that, since the left-hand-side equals 
zero, the optimal non-integer BFS does not satisfy this inequality. Thus, introduction of the inequality 
constraint (6.8) makes the current LP solution infeasible without losing any IP solutions. 
The constraint introduced by Gomory cut is first brought into standard form by subtracting a surplus 
variable. The resulting problem is solved using simplex method for a new optimal BFS, which is then 
inspected for non-integer components. The process is repeated till an integer BFS has been obtained.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Fundamental Engineering Optimization 
Methods
125 
Discrete Optimization
The cutting plane algorithm generates a family of polyhedra which satisfy: È³ Ù€ È³à¬µÙ€ È³à¬¶Ù€ Ú® Ù€ È³ ×ª Ôºà¯¡Ç¡
where È³ àµŒáˆ¼İ”× Ô¹à¯¡Ç£ à¡­à¢àµ‘à¢ˆáˆ½denote the polyhedral associated with the LP relaxation problem. The 
cutting plane algorithm terminates in finite steps.
An example of the cutting plane method is presented below.
Example 6.4: Cutting Plane method
We consider the IP problem in Example 6.3 above where the LP relaxation solution was found as: İ”à¬µ
×› àµŒ 
Í²Ç¡ İ”à¬¶
×› àµŒÍ¹Ç¤Í¸Í»Ç¡ İ”à¬·
×› àµŒÍµÇ¤ÍºÍ·Ç¡ İ‚×› àµŒÍ³Í³Í·ÍµÇ¤Íº The final tableau for the LP relaxation solution is given as: 
%DVLF
à¢à«š
à¢à«›
à¢à«œ
à¢™à«š
à¢™à«›
à¢™à«œ
5KV
à¢à«›







à¢à«œ







Ü›à«œ







àµ†Ü¢







The following series of cuts then produces an integer optimum solution:
1R
&XW
2SWLPDOVROXWLRQ
Ä† Í²Ç¤ÍºÍ²Íºİ”à¬µàµ…Í²Ç¤Í·ÍµÍ»İà¬µàµ…Í²Ç¤Í²ÍµÍ»İà¬¶àµ†İà¬¸àµŒÍ²Ç¤Í¸Í»Í´
İ”à¬µ
×› àµŒÍ²Ç¤ÍºÍ·Í¹Ç¡ İ”à¬¶
×› àµŒÍ¹Ç¡ İ”à¬·
×› àµŒÍµÇ¤Í»Í´Í»Ç¡ İ‚×› àµŒÍ³Í³Í·Í´Ç¤Í»
Ä† Í²Ç¤ÍºÍµÍµİà¬µàµ…Í²Ç¤Í²Í´Í¶İà¬¶àµ…Í²Ç¤ÍºÍºÍ³İà¬¸àµ†İà¬¹àµŒÍ²Ç¤Í»Í´Í» İ”à¬µ
×› àµŒÍ´Ç¤Í³Í¸Í´Ç¡ İ”à¬¶
×› àµŒÍ·Ç¤Í»Í¶Í¸Ç¡ İ”à¬·
×› àµŒÍ¶Ç¤Í²Í·Í¶Ç¡ İ‚×› àµŒÍ³Í³Í·Í³Ç¤Íµ
Ä† Í²Ç¤Í²Í·Í¶İà¬µàµ…Í²Ç¤Í»Í¹Íµİà¬¶àµ…Í²Ç¤Í³ÍµÍ·İà¬¹àµ†İà¬ºàµŒÍ²Ç¤Í»Í¶Í¸ İ”à¬µ
×› àµŒÍ´Ç¤Í²ÍºÍµÇ¡ İ”à¬¶
×› àµŒÍ·Ç¤Í»Í¹Í´Ç¡ İ”à¬·
×› àµŒÍ¶Ç¤Í²Í´ÍºÇ¡ İ‚×› àµŒÍ³Í³Í¶Í·Ç¤Íº
Ä† Í²Ç¤Í²Í·Í¸İà¬µàµ…Í²Ç¤Í³ÍµÍ»İà¬¹àµ…Í²Ç¤Í»Í¹Í´İà¬ºàµ†İà¬»àµŒÍ²Ç¤Í»Í¹Í´
İ”à¬µ
×› àµŒÍ´Ç¡ İ”à¬¶
×› àµŒÍ¸Ç¡ İ”à¬·
×› àµŒÍ¶Ç¡ İ‚×› àµŒÍ³Í³Í¶Í²
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
126 
NNumerica l Optimizati on Method
7	 Numerical Optimization 
Methods
This chapter describes the numerical methods used for solving both unconstrained and constrained 
optimization problems. These methods have been used to develop computational algorithms that form 
the basis of commercially available optimization software. The process of computationally solving the 
optimization problem is termed as mathematical programming and includes both linear and nonlinear 
programming. The basic numerical method to solve the nonlinear problem is the iterative solution 
method that starts from an initial guess, and iteratively refines it in an effort to reach the minimum (or 
maximum) of a multi-variable objective function. The iterative scheme is essentially a two-step process 
that seeks to determine: a) a search direction that does not violate the constraints and along which the 
objective function value decreases; and b) a step size that minimizes the function value along the chosen 
search direction. Normally, the algorithm terminates when either a minimum has been found, indicated 
by the function derivative being approximately zero, or when a certain maximum number of iterations 
has been exceeded indicating that there is no feasible solution to the problem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Fundamental Engineering Optimization 
Methods
127 
NNumerica l Optimizati on Method
Learning Objectives: The learning objectives in this chapter are:
1.	 Understand numerical methods employed for solving optimization problems
2.	 Learn the approaches to numerically solve the line search problem in one-dimension
3.	 Learn the direction finding algorithms, including gradient and Hessian methods
4.	 Learn the sequential linear programming (SLP) and sequential quadratic programming 
(SQP) techniques
7.1	
The Iterative Method
The general numerical optimization method begins with an initial guess and iteratively refines it so as 
to asymptotically approach the optimum. To illustrate the iterative method of finding a solution, we 
consider an unconstrained nonlinear programming problem defined as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
(7.1)
where x denotes the set of optimization variables. Let xk denote the current estimate of the minimum; 
then, the solution algorithm seeks an update, à¢à¯à¬¾à¬µÇ¡ that further reduces the function value, i.e., it results 
in: İ‚àµ«à¢à¯à¬¾à¬µàµ¯àµİ‚àµ«à¢à¯àµ¯ also termed as the descent condition. 
In the general iterative scheme, the optimization variables are updated as per the following rule:
à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¯à¢Šà¯
(7.2)
In the above, dk represents any search direction and ß™à¯ is the step size along that direction. The iterative 
method is thus a two-step process:
1.	 Find the suitable search direction dk along which the function value locally decreases
2.	 Perform line search along dk to find à¢à¯à¬¾à¬µ such that İ‚àµ«à¢à¯à¬¾à¬µàµ¯ attains its minimum value 
We first consider the problem of finding a descent direction dk and note that it can be determined by 
checking the directional derivative of İ‚àµ«à¢à¯àµ¯ along dk, which is given as the scalar product: ß˜İ‚àµ«à¢à¯àµ¯
à¯à¢Šà¯
Since the scalar product is a function of the angle between the two vectors, the descent condition is 
satisfied if the angle between ß˜İ‚àµ«à¢à¯àµ¯ and à¢Šà¯ is larger than 90Â°.
If the directional derivative of the function İ‚àµ«à¢à¯àµ¯DORQJà¢Šà¯ is negative, then the descent condition 
is satisfied. Further, dk is a descent direction only if it satisfies: ß˜İ‚àµ«à¢à¯àµ¯
à¯à¢Šà¯àµÍ²,Ià¢Šà¯ If is a descent 
direction, then we are assured that at least for small positive values of ß™à¯İ‚àµ«à¢à¯àµ…ß™à¯à¢Šà¯àµ¯àµİ‚áˆºà¢à¯áˆ» 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
128 
NNumerica l Optimizati on Method
Next, assuming a suitable search direction dk has been determined, we next seek to determine a suitable 
step length ß™à¯, where an optimal value of ß™à¯ minimizes İ‚àµ«à¢à¯àµ¯DORQJà¢Šà¯. Since both xk and dk are 
known, the projected function value along dk depends on ß™à¯ alone, and can be expressed as:

İ‚àµ«à¢à¯àµ…ß™à¯à¢Šà¯àµ¯àµŒİ‚àµ«à¢à¯àµ…ß™à¢Šà¯àµ¯àµŒİ‚áˆºß™áˆ»
(7.3)
The problem of choosing ß™ to minimize İ‚àµ«à¢à¯à¬¾à¬µàµ¯ along dk  thus amounts to a single-variable functional 
minimization problem, also known as the line search problem, and is defined as:
ÂÂ‹Â
à°ˆİ‚áˆºß™áˆ»àµŒİ‚àµ«à¢à¯àµ…È½à¢Šà¯àµ¯
(7.4)
Assuming that a solution exists, it is found at a point where the derivative of the function goes to zero. 
Thus, by setting İ‚Ô¢áˆºß™áˆ»àµŒÍ² we can solve for the desired step size and update the current estimate xk. 
As an example of the line search problem, we consider minimizing a quadratic function: 
İ‚áˆºà¢áˆ»àµŒà¬µ
à¬¶à¢à¯à¡­à¢àµ†à¢ˆà¯à¢Ç¡ ß˜İ‚àµŒà¡­à¢àµ†à¢ˆ 
(7.5)
where A is a symmetric positive definite matrix. Let d be a given descent direction; then, the line search 
problem reduces to the following minimization problem:
ÂÂ‹Â
à°ˆİ‚áˆºß™áˆ»àµŒàµ«à¢à¯àµ…ß™à¢Šàµ¯
à¯à¡­àµ«à¢à¯àµ…ß™à¢Šàµ¯àµ†à¢ˆà¯àµ«à¢à¯àµ…ß™à¢Šàµ¯
(7.6)
A solution is found by setting İ‚á‡±áˆºß™áˆ»àµŒà¢Šà¯à¡­àµ«à¢à¯àµ…ß™à¢Šàµ¯àµ†à¢Šà¯à¢ˆàµŒÍ² and is given as: 
ß™àµŒß˜İ‚áˆºà¢à¯áˆ»à¯à¢Š
à¢Šà¯à¡­à¢Š
àµŒà¢Šà¯àµ«à¡­à¢à¯àµ†à¢ˆàµ¯
à¢Šà¯à¡­à¢Š

(7.7)
An update then follows as: à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¢Š
In the following, we first discuss numerical methods used to solve the line search problem in Sec. 7.2, 
followed by a discussion of the methods to solve the direction finding problem in Sec. 7.3.
7.2	
Computer Methods for Solving the Line Search Problem
In order to solve the line search problem, we assume that a suitable search direction dk has been 
determined, and wish to minimize the objective function: İ‚àµ«à¢à¯àµ…ß™à¢Šà¯àµ¯àµŒİ‚áˆºß™áˆ»DORQJà¢Šà¯ We further 
assume dk that is a descent direction, i.e., it satisfies: ×İ‚áˆºà¢à¯áˆ»à¯à¢Šà¯àµÍ² so that only positive values of 
ß™ need to be considered. Then, the line search problem reduces to finding a solution to (7.4) above. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
129 
NNumerica l Optimizati on Method
In the following, we address the problem of finding the minimum of a function,İ‚áˆºİ”áˆ»Ç¡ İ”× Ô¹Ç¡ where 
we additionally assume that the function is unimodal, i.e., it has a single local minimum. Prominent 
computer methods for solving the line search problem are described below. 
7.2.1	
 Interval Reduction Methods
The interval reduction methods are commonly used to solve the line search problem. These methods 
find the minimum of a unimodal function in two steps: 
a)	 Bracketing the minimum to an interval 
b)	 Reducing the interval of uncertainty to desired accuracy 
The bracketing step aims to find a three-point pattern, such that for İ”à¬µÇ¡ İ”à¬¶Ç¡ İ”à¬·İ‚áˆºİ”à¬µáˆ»àµ‘İ‚áˆºİ”à¬¶áˆ»àµİ‚áˆºİ”à¬·áˆ» 
The bracketing algorithm can be started from any point in the domain of İ‚áˆºİ”áˆ» though a good guess 
will reduce the number of steps involved. In the following description of the bracketing algorithm İ‚à¯œ 
denotes İ‚áˆºİ”áˆ».
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
130 
NNumerica l Optimizati on Method
Bracketing Algorithm (Belegundu & Chandrupatla p. 54): 
1.	 Initialize: choose İ”à¬µÇ¡ Î¿Ç¡ ß›HJß›àµŒÍ³Ç¤Í¸Í³Íº
2.	 Set İ”à¬¶àµŒİ”à¬µàµ…Î¿HYDOXDWHİ‚à¬µÇ¡ İ‚à¬¶ 
3.	 If İ‚à¬µàµİ‚à¬¶VHWİ”à¬´Õš İ”à¬µÇ¡ İ”à¬µÕš İ”à¬¶Ç¡ İ”à¬¶Õš İ”à¬´Ç¡ Î¿àµŒàµ†Î¿ 
4.	 Set WÎ¿àµŒß›Î¿İ”à¬·àµŒİ”à¬¶àµ…Î¿HYDOXDWHİ‚à¬·
5.	 If İ‚à¬¶àµ’İ‚à¬·VHWİ‚à¬µÕš İ‚à¬¶Ç¡ İ‚à¬¶Õš İ‚à¬·Ç¡ İ”à¬µÕš İ”à¬¶Ç¡ İ”à¬¶Õš İ”à¬·, then go to step 3
6.	 Quit; points 1,2, and 3 satisfy İ‚à¬µàµ’İ‚à¬¶àµİ‚à¬·
Next, we assume that the minimum has been bracketed to a closed interval áˆ¾İ”à¯ŸÇ¡ İ”à¯¨áˆ¿ The interval 
reduction step aims to find the minimum in that interval. A common interval reduction approach is to 
use either the Fibonacci or the Golden Section methods; both methods are based on the golden ratio 
derived from Fibonacciâ€™s sequence.
Fibonacciâ€™s Method. The Fibonacciâ€™s method uses Fibonacci numbers to achieve maximum interval 
reduction in a given number of steps. The Fibonacci number sequence is generated as: Ü¨à¬´àµŒÜ¨à¬µàµŒÍ³Ç¡ Ü¨à¯œàµŒ
Ü¨à¯œà¬¿à¬µàµ…Ü¨à¯œà¬¿à¬¶Ç¡ İ…àµ’Í´ Fibonacci numbers have some interesting properties, among them:
1.	 The ratio ß¬àµŒÂÂ‹Â
à¯¡Õœà®¶
à®¿à³™à°·à°­
à®¿à³™àµŒÎ¾à¬¹à¬¿à¬µ
à¬¶
àµŒÍ²Ç¤Í¸Í³ÍºÍ²ÍµÍ¶ is known as the golden ratio. 
2.	 Using Fibonacci numbers, the number of interval reductions required to achieve a desired 
accuracy ß is the smallest n such that Í³È€Ü¨à¯¡àµß and can be specified in advance. 
3.	 For given l1 and İŠ we have Ü«à¬¶àµŒ
à®¿à³™à°·à°­
à®¿à³™Ü«à¬µÇ¡ Ü«à¬·àµŒÜ«à¬µàµ†Ü«à¬¶Ç¡ Ü«à¬¸àµŒÜ«à¬¶àµ†Ü«à¬· etc. 
The Fibonacci algorithm is given as follows:
Fibonacci Algorithm (Belegundu & Chandrupatla p. 60):
Initialize: specify İ”à¬µÇ¡ İ”à¬¸áˆºÜ«à¬µàµŒÈİ”à¬¸àµ†İ”à¬µÈáˆ»Ç¡ ßÇ¡ İŠÇ£
à¬µ
à®¿à³™àµß
Compute ß™à¬µàµŒ
à®¿à³™à°·à°­
à®¿à³™İ”à¬¶àµŒß™à¬µİ”à¬µàµ…áˆºÍ³ àµ†ß™à¬µáˆ»İ”à¬¸HYDOXDWHİ‚à¬¶
For İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠàµ†Í³ 
1.	 Introduce İ”à¬·àµŒáˆºÍ³ àµ†ß™à¯œáˆ»İ”à¬µàµ…ß™à¯œİ”à¬¸Ç¡HYDOXDWHİ‚à¬· 
2.	 If İ‚à¬¶àµİ‚à¬·Ç¡VHWİ”à¬¸Õš İ”à¬µÇ¡ İ”à¬µÕš İ”à¬·
3.	 Else set İ”à¬µÕš İ”à¬¶Ç¡ İ”à¬¶Õš İ”à¬·Ç¡ İ‚à¬¶Õš İ‚à¬·
4.	 Set ß™à¯œà¬¾à¬µàµŒ
à¯‚à³™à°·à³”à°·à°­
à¯‚à³™à°·à³”
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
131 
NNumerica l Optimizati on Method
Golden Section Method. The golden section method uses the golden ratio à¯‚à³”à°¶à°­
à¯‚à³”àµŒß¬àµŒÍ²Ç¤Í¸Í³ÍºÍ²ÍµÍ¶for 
interval reduction in the above Fibonacci algorithm. This results in uniform interval reduction strategy 
independent of the number of trials. Further, since the final interval Ü«à¯¡ is related to the initial interval
Ü«à¬µ as: Ü«à¯¡àµŒß¬à¯¡à¬¿à¬µÜ«à¬µ given I1 and a desired Ü«à¯¡Ç¡ the number of interval reductions may be computed as:
İŠàµŒá‰”
à­ªà­¬à¯‚à³™à¬¿à­ªà­¬à¯‚à°­
à­ªà­¬à°›
àµ…
à¬·
à¬¶á‰•ZKHUHÛÎ®Û‚ represents the floor function.
The golden section method can be integrated with the three-point bracketing algorithm by choosing 
ß›àµŒ
à¬µ
à°› and renaming İ”à¬·DVİ”à¬¸ Stopping criteria for the golden section algorithm may be specified in 
terms of desired interval size, reduction in function value, or the number of interval reductions.
Next, the bracketing step can also be combined with the interval reduction step, and the integrated 
bracketing and interval reduction algorithm is given below.
Integrated Bracketing and Golden Section Algorithm (Belegundu & Chandrupatla p. 65):
Initialize: specify İ”à¬µÇ¡ Î¿Ç¡ ß¬àµŒÍ²Ç¤Í¸Í³ÍºÍ²ÍµÍ¶Ç¡ ß
1.	 Set İ”à¬¶àµŒİ”à¬µàµ…Î¿HYDOXDWHİ‚à¬µÇ¡ İ‚à¬¶
2.	 If İ‚à¬µàµİ‚à¬¶VHWİ”à¬´Õš İ”à¬µÇ¡ İ”à¬µÕš İ”à¬¶Ç¡ İ”à¬¶Õš İ”à¬´Ç¡ Î¿àµŒàµ†Î¿
Î¿
3.	 Set Î¿àµŒ
Î¿
à°›İ”à¬¸àµŒİ”à¬¶àµ…Î¿HYDOXDWHİ‚à¬¸
4.	 If İ‚à¬¶àµ’İ‚à¬¸VHWİ‚à¬µÕš İ‚à¬¶Ç¡ İ‚à¬¶Õš İ‚à¬¸Ç¡ İ”à¬µÕš İ”à¬¶Ç¡ İ”à¬¶Õš İ”à¬¸then go to step 3
5.	 Introduce İ”à¬·àµŒáˆºÍ³ àµ†ß¬áˆ»İ”à¬µàµ…ß¬İ”à¬¸Ç¡ evaluate İ‚à¬·
6.	 If İ‚à¬¶àµİ‚à¬·Ç¡VHWİ”à¬¸Õš İ”à¬µÇ¡ İ”à¬µÕš İ”à¬·
7.	 Else set İ”à¬µÕš İ”à¬¶Ç¡ İ”à¬¶Õš İ”à¬·Ç¡ İ‚à¬¶Õš İ‚à¬· 
8.	 Check stopping criteria: ,IÈİ”à¬µàµ†İ”à¬·È àµß quit; else go to 5 
7.2.2	
 Approximate Search Algorithms
The calculations of the exact step size in the line search step are time consuming. In most cases, 
approximate function minimization suffices to advance to the next iteration. Since crude minimization 
methods may give rise to convergence issues, additional conditions on both dk and ß™à¯ are prescribed to 
ensure convergence of the numerical algorithm. These conditions include, for dk: a) sufficient descent 
condition, and b) gradient related condition; and for ß™à¯: a) sufficient decrease condition, and b) non 
trivial condition. They are described below.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
132 
NNumerica l Optimizati on Method
Sufficient Descent Condition. The sufficient descent condition, or the angle condition guards against dk 
becoming too close to ß˜İ‚àµ«à¢à¯àµ¯ The condition is normally stated as: àµ†
à°‡à¯™àµ«à¢à³–àµ¯
à³…à¢Šà³–
à¸®à°‡à¯™àµ«à¢à³–àµ¯à¸®à¸®à¢Šà³–à¸®àµ’ß³àµÍ²IRUDVPDOOß³
Alternatively, the sufficient descent condition may be specified as:ß˜İ‚àµ«à¢à¯àµ¯
à¯à¢Šà¯àµÜ¿à¸®ß˜İ‚àµ«à¢à¯àµ¯à¸®
à¬¶Ç¡Ü¿àµÍ² 
Gradient Related Condition. The search direction is gradient related if à¸®à¢Šà¯à¸®àµ’Ü¿à¸®ß˜İ‚àµ«à¢à¯àµ¯à¸®Ç¡ Ü¿àµÍ²
This condition aids in convergence.
Sufficient Decrease Condition. The sufficient decrease condition on ß™à¯ ensures that a nontrivial 
reduction in the function value is obtained at each step. The condition is derived from Taylor series 
expansion of İ‚àµ«à¢à¯àµ…ß™à¯à¢Šà¯àµ¯ and is stated as: İ‚àµ«à¢à¯àµ…ß™à¯à¢Šà¯àµ¯àµ†İ‚àµ«à¢à¯àµ¯àµ‘ß¤ß™à¯ß˜İ‚àµ«à¢à¯àµ¯
à¯à¢Šà¯Ç¡ Í² àµß¤àµÍ³
Arjimoâ€™s Rule. An alternative sufficient decrease condition, referred to as Arjimoâ€™s rule, is given as: 
İ‚áˆºß™áˆ»àµ‘İ‚áˆºÍ²áˆ»àµ…ß¤ß™İ‚á‡±áˆºÍ²áˆ»Ç¡
Í² àµß¤àµÍ³
(7.8)
Curvature Condition. A curvature condition is added to Arjimoâ€™s rule to improve convergence. The 
curvature condition is given as:
Èİ‚á‡±áˆºß™áˆ»È àµ‘ßŸÈİ‚á‡±áˆºÍ²áˆ»ÈÇ¡
Í² àµ‘ßŸàµÍ³ 
(7.9)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Fundamental Engineering Optimization 
Methods
133 
NNumerica l Optimizati on Method
Further, the curvature condition implies that: á‰š×İ‚àµ«à¢à¯àµ…ß™à¯à¢Šà¯àµ¯
à¯à¢Šà¯á‰šàµ‘ßŸá‰š×İ‚àµ«à¢à¯àµ¯
à¯à¢Šà¯á‰šÇ¡ Í² àµ‘ßŸàµÍ³
Conditions (7.8) and (7.9) together with ß¤àµ‘ßŸ are known as Wolfe conditions, which are commonly 
used by all line search algorithms. A line search based on Wolfe conditions proceeds by bracketing the 
minimizer in an interval, followed by estimating it via polynomial approximation. These two steps are 
explained below:
Bracketing the Minimum. In the bracketing step we seek an interval àµ£ß™Ç¡ ß™àµ§ such that Wİ‚á‡±àµ«ß™àµ¯àµÍ² and 
İ‚á‡±áˆºß™áˆ»àµÍ² Since for any descent direction, İ‚á‡±áˆºÍ²áˆ»àµÍ² therefore, ß™àµŒÍ² serves as initial lower bound 
on ß™ To find an upper bound, increasing ß™ values, e.g., ß™àµŒÍ³Ç¡Í´Ç¡ Ç¥ are tried. Assume that for some 
ß™à¯œàµÍ² İ‚á‡±áˆºß™à¯œáˆ»àµÍ²ÂƒÂÂ†İ‚á‡±áˆºß™à¯œà¬¾à¬µáˆ»àµÍ² then, ß™à¯œ serves as an upper bound. 
Estimating the Minimum. Once the minimum has been bracketed to a small interval, a quadratic or 
cubic polynomial approximation is used to find the minimizer. If the polynomial minimizer ß™à·œ satisfies 
Wolfeâ€™s condition for the desired ßŸ value VD\ßŸàµŒÍ²Ç¤Í·áˆ» and the sufficient decrease condition for the 
desired ß¤ value (say ß¤àµŒÍ²Ç¤Í´), it is taken as the function minimizer, otherwise ß™à·œ is used to replace one 
of the ß™Â‘Â”ß™ and the polynomial approximation step repeated.
Quadratic curve Fitting. Assuming that the interval áˆ¾ß™à¯ŸÇ¡ ß™à¯¨áˆ¿ contains the minimum of a unimodal 
function, İ‚áˆºß™áˆ» it can be approximated by a quadratic function: İáˆºß™áˆ»àµŒÜ½à¬´àµ…Ü½à¬µß™àµ…Ü½à¬¶ß™à¬¶ A quadratic 
approximation uses three points áˆ¼ß™à¯ŸÇ¡ ß™à¯ Ç¡ ß™à¯¨áˆ½ where the mid-point of the interval may be used for ß™à¯  
The quadratic coefficients áˆ¼Ü½à¬´Ç¡ Ü½à¬µÇ¡ Ü½à¬¶áˆ½ are solved from: İ‚áˆºß™à¯œáˆ»àµŒÜ½à¬´àµ…Ü½à¬µß™à¯œàµ…Ü½à¬¶ß™à¯œ
à¬¶Ç¡ß™à¯œß³áˆ¼ß™à¯ŸÇ¡ ß™à¯ Ç¡ ß™à¯¨áˆ½ 
which results in the following expressions:

Ü½à¬¶àµŒ
Í³
ß™à¯¨àµ†ß™à¯ 
á‰ˆİ‚áˆºß™à¯¨áˆ»àµ†İ‚áˆºß™à¯Ÿáˆ»
ß™à¯¨àµ†ß™à¯Ÿ
àµ†İ‚áˆºß™à¯ áˆ»àµ†İ‚áˆºß™à¯Ÿáˆ»
ß™à¯ àµ†ß™à¯Ÿ
á‰‰Ç¢
Ü½à¬µàµŒ
Í³
ß™à¯ àµ†ß™à¯Ÿ
àµ«İ‚áˆºß™à¯ áˆ»àµ†İ‚áˆºß™à¯Ÿáˆ»àµ¯àµ†Ü½à¬¶áˆºß™à¯Ÿàµ…ß™à¯ áˆ»Ç¢ 
(7.10)
Ü½à¬´àµŒİ‚áˆºß™à¯Ÿáˆ»àµ†Ü½à¬µß™à¯Ÿàµ†Ü½à¬¶ß™à¯Ÿ
à¬¶
The minimum for İáˆºß™áˆ» can be computed by setting İá‡±áˆºß™áˆ»àµŒÍ²Ç¡ and is given as: ß™à¯ à¯œà¯¡àµŒàµ†
à¯”à°­
à¬¶à¯”à°® An 
explicit formula for ß™à¯ à¯œà¯¡ in terms of the three interval points can also be derived and is given as:
ß™à¯ à¯œà¯¡àµŒß™à¯ àµ†Í³
Í´
áˆºß™à¯ àµ†ß™à¯Ÿáˆ»à¬¶áˆºİ‚áˆºß™à¯ áˆ»àµ†İ‚áˆºß™à¯¨áˆ»áˆ»àµ†áˆºß™à¯ àµ†ß™à¯¨áˆ»à¬¶áˆºİ‚áˆºß™à¯ áˆ»àµ†İ‚áˆºß™à¯Ÿáˆ»áˆ»
áˆºß™à¯ àµ†ß™à¯Ÿáˆ»áˆºİ‚áˆºß™à¯ áˆ»àµ†İ‚áˆºß™à¯¨áˆ»áˆ»àµ†áˆºß™à¯ àµ†ß™à¯¨áˆ»áˆºİ‚áˆºß™à¯ áˆ»àµ†İ‚áˆºß™à¯Ÿáˆ»áˆ» 
(7.11)
An example of the approximate search algorithm is now presented.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
134 
NNumerica l Optimizati on Method
Example 7.1: Approximate search algorithm (Ganguli, p. 121)
We wish to approximately solve the following minimization problem: ÂÂ‹Â
à°ˆİ‚áˆºß™áˆ»àµŒİà¬¿à°ˆàµ…ß™à¬¶ We use 
Arjimoâ€™s rule with: ß¤àµŒÍ²Ç¤Í´ and ß™àµŒÍ²Ç¤Í³Ç¡ Í²Ç¤Í´Ç¡ Ç¥ to estimate the minimum. The Matlab commands 
used for this purpose and the corresponding results appear below:
>> f=inline(â€˜x.*x+exp(-x)â€™); mu=0.2; al=0:.1:1; 
>> feval(f,al)
1.0000	
0.9148	
0.8587	
0.8308	
0.8303	
0.8565	
0.9088	
0.9866	
1.0893	
1.2166	
1.3679
>> 1-mu*al
1.0000	
0.9800	
0.9600	
0.9400	
0.9200	
0.9000	
0.8800
0.8600	
0.8400	
0.8200	
0.8000
Then, according to Arjimoâ€™s condition, an estimate of the minimum is given as: ß™àµŒÍ²Ç¤Í·Ç¤ Further, 
since İ‚á‡±áˆºÍ²áˆ»àµÍ²DQGİ‚á‡±áˆºß™áˆ»àµÍ²Wand the minimum is bracketed by [0, 0.5]. We next use quadratic 
approximation of the function over áˆ¼Í²Ç¡
à°ˆ
à¬¶Ç¡ ß™áˆ½ to estimate the minimum as follows:
al=0; ai=0.25; au=0.5;
a2 = ((f(au)-f(al))/(au-al)-(f(ai)-f(al))/(ai-al))/(au-ai);
a1 = (f(ai)-f(al))/(ai-al)-a2*(al+ai);
xmin = -a1/a2/2 = 0.3531
An estimate of the minimum is given as: ß™à·œàµŒÍ²Ç¤ÍµÍ·ÍµÍ³ We note that the exact solution is given as: 
ß™à¯ à¯œà¯¡àµŒÍ²Ç¤ÍµÍ·Í³Í¹.
Next, we describe the computer methods for finding the search direction. Our initial focus is on 
unconstrained problems. The constrained problems are discussed later in Sec. 7.4.
7.3	
Computer Methods for Finding the Search Direction
The computer methods for finding the search direction à¢Šà¯ are normally grouped into first order and 
second order methods, where the order refers to the derivative order of the function approximation 
used. Thus, first order methods refer to the gradient-based methods, while the second order methods 
additionally involve the Hessian matrix in the computations. The gradient based quasi-Newton methods 
are overwhelmingly popular when it comes to implementation. We describe popular search methods 
below.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
135 
NNumerica l Optimizati on Method
7.3.1	
 The Steepest Descent Method
The steepest descent method, attributed to Cauchy, is the simplest of the gradient methods. The method 
involves choosing dk to locally move in the direction of maximum decrease in the function value, i.e., 
the direction opposite to the gradient vector at the current estimate point. 
Thus, the steepest descent method is characterized by: à¢Šà¯àµŒàµ†ß˜İ‚áˆºà¢à¯áˆ» which leads to the following 
update rule:
à¢à¯à¬¾à¬µàµŒà¢à¯àµ†ß™à¯Î® ß˜İ‚áˆºà¢à¯áˆ»
(7.12)
where the step ß™à¯W size to minimize İ‚áˆºà¢à¯à¬¾à¬µáˆ» can be analytically or numerically determined using 
methods described in Sec. 7.2. 
As an example, in the case of a quadratic function: İ‚áˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡­à¢àµ†à¢ˆà¯à¢Ç¡ ß˜İ‚àµŒà¡­à¢àµ†à¢ˆ the steepest 
descent method with exact line search results in the following update rule: 
à¢à¯à¬¾à¬µàµŒà¢à¯àµ†ß™Î® ß˜İ‚àµ«à¢à¯àµ¯Ç¢ ß™àµŒ×İ‚àµ«à¢à¯àµ¯
à¯×İ‚àµ«à¢à¯àµ¯
×İ‚áˆºà¢à¯áˆ»à¯Û¯×İ‚áˆºà¢à¯áˆ»
(7.13)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Fundamental Engineering Optimization 
Methods
136 
NNumerica l Optimizati on Method
The above update can be equivalently described in terms of a residual: à¢˜à¯àµŒà¢ˆàµ†à¡­à¢à¯àµŒàµ†ß˜İ‚áˆºà¢à¯áˆ»DV

à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¯à¢˜à¯Ç¢ß™à¯àµŒà¢˜à¯
à¯à¢˜à¯
à¢˜à¯
à¯Ü£à¢˜à¯

(7.14)
The steepest descent algorithm is given below.
Steepest Descent Algorithm:
Initialize: choose à¢à¬´
For İ‡àµŒÍ²Ç¡Í³Ç¡Í´Ç¡ Ç¥
1.	 Compute ß˜İ‚áˆºà¢à¯áˆ»
2.	 Check convergence: if à¸®ß˜İ‚áˆºà¢à¯áˆ»à¸®àµß³ stop.
3.	 Set à¢Šà¯àµŒàµ†ß˜İ‚áˆºà¢à¯áˆ»
4.	 Line search problem: Find ÂÂ‹Â
à°ˆà®¹à¬´İ‚áˆºà¢à¯àµ…ß™à¢Šà¯áˆ»
5.	 Set à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¢Šà¯ 
We note that a line search that minimizes İ‚áˆºß™áˆ» along the steepest-descent direction may not result in 
the lowest achievable function value over all search directions. This could happen, for example, when 
the current gradient ß˜İ‚àµ«à¢à¯àµ¯ points away from the local minimum, as is shown in the example presented 
at the end of the section.
A further weakness of the steepest descent method is that it becomes slow as the minimum is approached. 
This can be seen by examining the function derivative İ‚á‡±áˆºß™à¯áˆ» which is computed as follows:
İ€
İ€ß™à¯
İ‚àµ«à¢à¯àµ…ß™à¯à¢Šà¯àµ¯àµŒ×İ‚àµ«à¢à¯à¬¾à¬µàµ¯
à¯à¢Šà¯
(7.15)
The above result implies that the gradient ×İ‚àµ«à¢à¯à¬¾à¬µàµ¯ is normal to dk, i.e., in the case of steepest descent, 
normal to ×İ‚àµ«à¢à¯àµ¯ This implies a zigzag type progression towards the minimum that results in its slow 
progress. Due to its above weaknesses, the steepest descent method does not find much use in practice.
Rate of Convergence. The steepest-descent method displays linear convergence. In the case of quadratic 
functions, its rate constant is bounded by the following inequality (Griva, Nash & Sofer 2009, p. 406):
Ü¥àµŒİ‚àµ«à¢à¯à¬¾à¬µàµ¯àµ†İ‚áˆºà¢×›áˆ»
İ‚áˆºà¢à¯áˆ»àµ†İ‚áˆºà¢×›áˆ»àµ‘á‰†Ü¿İ‹İŠİ€áˆºà¡­áˆ»àµ†Í³
Ü¿İ‹İŠİ€áˆºà¡­áˆ»àµ…Í³á‰‡
à¬¶

(7.16)
The above result uses İ‚àµ«à¢à¯àµ¯àµ†İ‚áˆºà¢×›áˆ» which converges at the same rate as Và¸®à¢à¯àµ†à¢×›à¸®) Further, when 
using steepest-descent method with general nonlinear functions, the bound holds for à¡­àµŒ×à¬¶İ‚áˆºİ”×›áˆ»
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
137 
NNumerica l Optimizati on Method
Preconditioning. As with all gradient methods, preconditioning aimed at reducing the condition number 
of the Hessian matrix can be employed to aid convergence of the steepest-descent method. To illustrate 
this point, we consider the cost function: İ‚áˆºà¢áˆ»àµŒÍ²Ç¤Í³İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶àµŒà¢à¯à¡­à¢Ç¡ à¡­àµŒ diag (0,1,1), and define a 
linear transformation: à¢àµŒà¡¼à¢Ÿ where à¡¼àµŒİ€İ…Ü½İƒáˆºÎ¾Í³Í²Ç¡ Í³áˆ» Then, the objective function is transformed 
as: İ‚áˆºà¢áˆ»àµŒà¢Ÿà¯à¡¼à¯à¡­à¡¼à¢Ÿ where the matrix product 
İƒáˆº
áˆ»
à¡¼à¯à¡­à¡¼àµŒà¡µK has a condition number of unity, indicating 
that the steepest-descent method will now converge in a single iteration. 
An example of the steepest descent method is now presented.
Example 7.2: Steepest Descent
We consider minimizing İ‚áˆºà¢áˆ»àµŒÍ²Ç¤Í³İ”à¬µ
à¬¶àµ…İ”à¬¶
à¬¶ from an initial estimate à¢à¬´àµŒáˆºÍ·Ç¡Í³áˆ» The gradient of 
I İ‚áˆºà¢áˆ» is computed as , and ß˜İ‚áˆºà¢áˆ»àµŒàµ¤Í²Ç¤Í´İ”à¬µ
Í´İ”à¬¶àµ¨DQGß˜İ‚áˆºà¢à¬´áˆ»àµŒá‰‚Í³
Í´á‰ƒ Using the steepest-descent rule, the 
line search problem is given as: ÂÂ‹Â
à°ˆİ‚áˆºß™áˆ»àµŒÍ²Ç¤Í³áˆºÍ· àµ†ß™áˆ»à¬¶àµ…áˆºÍ³ àµ†Í´ß™áˆ»à¬¶7 The exact solution is found 
by setting İ‚á‡±áˆºß™áˆ»àµŒÍºÇ¤Í´ß™àµ†Í· àµŒÍ²RUß™àµŒÍ²Ç¤Í¸Í³ Therefore, à¢à¬µàµŒá‰‚Í¶Ç¤ÍµÍ»
àµ†Í²Ç¤Í´Í´á‰ƒDQGİ‚áˆºà¢à¬µáˆ»àµŒÍ³Next, 
we try an arbitrary search direction à¢Šà¬´àµŒá‰‚àµ†Í³
Í² á‰ƒ which gives İ‚áˆºß™áˆ»àµŒÍ²Ç¤Í³áˆºÍ· àµ†ß™áˆ»à¬¶ and a similar 
minimization results in İ‚á‡±áˆºß™áˆ»àµŒÍ²Ç¤Í´ß™àµ†Í³ àµŒÍ²RUß™àµŒÍ·IRUZKLFKà¢à¬µàµŒá‰‚Í²
Í³á‰ƒDQGİ‚áˆºà¢à¬µáˆ»àµŒÍ³which 
provides a better estimate of the actual minimum (0,0).
7.3.2	
 Conjugate-Gradient Methods
Conjugate-gradient (CG) methods employ conjugate vectors with respect to the Hessian matrix, as 
search directions in successive iterations; these directions hold the promise to minimize the function 
in İŠ steps. The CG methods are popular in practice due to their low memory requirements and strong 
local and global convergence properties. 
Let à¢Šà¬´Ç¡ à¢Šà¬¶Ç¡ Ç¥ Ç¡ à¢Šà¯¡à¬¿à¬µ where à¢Šà¯œà¯à¡­à¢Šà¯àµŒÍ²Ç¡ İ…àµİ†Ç¡ denote conjugate directions with respect to A matrix, 
and let à¢à¯ denote ×İ‚àµ«à¢à¯àµ¯Ç¤ Then, starting from the steepest descent direction, we can use the following 
procedure to generate A-conjugate directions: 

à¢Šà¬´àµŒàµ†à¢à¬´Ç¢à¢Šà¯à¬¾à¬µàµŒàµ†à¢à¯à¬¾à¬µàµ…ßšà¯à¢Šà¯İ‡àµ’Í²
(7.17)
Next, application of the conjugacy condition results in:

à¢Šà¯à¯à¡­à¢Šà¯à¬¾à¬µàµŒàµ†à¢Šà¯à¯à¡­İƒà¯à¬¾à¬µàµ…ßšà¯à¢Šà¯à¯à¡­à¢Šà¯àµŒÍ²Ç¡Â‘Â”ßšà¯àµŒà¢à¯à¬¾à¬µ
à¯
à¡­à¢Šà¯
à¢Šà¯à¯à¡­à¢Šà¯
(7.18)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
138 
NNumerica l Optimizati on Method
where we note that this expression can be further simplified if additional assumptions regarding 
the function and the line search algorithm are made. For example, since in the case of a quadratic 
function: 
à¢à¯à¬¾à¬µàµ†à¢à¯àµŒà¡­áˆºà¢à¯à¬¾à¬µàµ†à¢à¯áˆ»àµŒß™à¯à¡­à¢Šà¯ by substituting à¡­à¢Šà¯àµŒ
à¬µ
à°ˆà³–áˆºà¢à¯à¬¾à¬µàµ†à¢à¯áˆ» in 
(7.18), we obtain: ßšà¯àµŒ
à¢à³–à°¶à°­
à³…
áˆºà¢à³–à°¶à°­à¬¿à¢à³–áˆ»
à¢Šà³–à³…áˆºà¢à³–à°¶à°­à¬¿à¢à³–áˆ»(the Hestenes-Stiefel formula). Further, in the case of 
exact line search,İƒà¯à¬¾à¬µ
à¯
à¢Šà¯àµŒÍ² ßšà¯àµŒ
à¢à³–à°¶à°­
à³…
áˆºà¢à³–à°¶à°­à¬¿à¢à³–áˆ»
à¢à³–
à³…à¢à³–
 (the Polak-Ribiere formula). Finally, since 
à¢à¯à¬¾à¬µ
à¯
à¢Šà¯àµŒà¢à¯à¬¾à¬µ
à¯
àµ«àµ†à¢à¯àµ…ßšà¯à¬¿à¬µà¢Šà¯à¬¿à¬µàµ¯àµŒÍ²,whereas for quadratic functions, à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¯à¡­à¢Šà¯ 
therefore, by exact line search condition, à¢à¯à¬¾à¬µ
à¯
à¢à¯àµŒßšà¯à¬¿à¬µáˆºà¢à¯àµ…ß™à¯à¡­à¢Šà¯áˆ»à¯à¢Šà¯à¬¿à¬µàµŒÍ² resulting in 
ßšà¯àµŒ
à¢à³–à°¶à°­
à³…
à¢à³–à°¶à°­
à¢à³–
à³…à¢à³–
 (the Fletcher-Reeves formula). Other versions of ßšà¯ have also been proposed.
The significance of the conjugacy property is apparent if we formulate a solution as: İ•àµŒÏƒ
ß™à¯œà¢Šà¯œ
à¯¡
à¯œà­€à¬µ
 
which is composed of I İŠ conjugate vectors. Then, the minimization problem is decomposed into a set 
of one-dimensional problems given as:
ÂÂ‹Â
à¯¬İ‚áˆºà¢Ÿáˆ»àµŒà·
ÂÂ‹Â
à°ˆà³”àµ¬Í³
Í´ ß™à¯œ
à¬¶à¢Šà¯œà¯à¡­à¢Šà¯œàµ†ß™à¯œà¢‰à¯à¢Šà¯œàµ°
à¯¡
à¯œà­€à¬µ

(7.19)
Then, by setting the derivative with respect to ß™à¯œ equal to zero, we obtain: ß™à¯œà¢Šà¯œà¯à¡­à¢Šà¯œàµ†à¢‰à¯à¢Šà¯œàµŒÍ² leading 
to: ß™à¯œàµŒ
à¢‰à³…à¢Šà³”
à¢Šà³”à³…à¡­à¢Šà³” The CG method iteratively determines conjugate directions à¢Šà¯œ and their coefficients ß™à¯œ. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
139 
NNumerica l Optimizati on Method
A Conjugate-gradient algorithm that uses residuals: à¢˜à¯œàµŒà¢ˆàµ†à¡­à¢à¯œÇ¡ İ…àµŒÍ³Ç¡Í´Ç¡ Ç¥ Ç¡ İŠ is given below: 
Conjugate-Gradient Algorithm (Griva, Nash & Sofer, p454):
Init: Choose à¢à¬´àµŒà«™Ç¡ à¢˜à¬´àµŒà¢ˆÇ¡ à¢Šáˆºà¬¿à¬µáˆ»àµŒÍ²Ç¡ ßšà¬´àµŒÍ²Ç¤
For İ…àµŒÍ²Ç¡Í³Ç¡ Ç¥
1.	 Check convergence: if Ô¡à¢˜à¯œÔ¡ àµß³ stop.
2.	 If I İ…àµÍ²VHWßšà¯œàµŒ
à¢˜à³”
à³…à¢˜à³”
à¢˜à³”à°·à°­
à³…
à¢˜à³”à°·à°­  
3.	 Set à¢Šà¯œàµŒà¢˜à¯œàµ…ßšà¯œà¢Šà¯œà¬¿à¬µß™à¯œàµŒ
à¢˜à³”
à³…à¢˜à³”
à¢Šà³”à³…à¡­à¢Šà³”à¢à¯œà¬¾à¬µàµŒà¢à¯œàµ…ß™à¯œà¢Šà¯œà¢˜à¯œà¬¾à¬µàµŒà¢˜à¯œàµ†ß™à¯œà¡­à¢Šà¯œ
Preconditioning. In all gradient-based methods, the convergence rates improve when the Hessian matrix 
has a low condition number. Preconditioning, or scaling, aimed at reducing the condition number, 
therefore, helps to speed up the convergence rates. Preconditioning involves a linear transformation:
à¢àµŒà¡¼à¢Ÿ where P is invertible. 
In the case of CG method, as a result of preconditioning, the conjugate directions are modified as:
à¢Šà¬´àµŒàµ†à¡¼à¢à¬´Ç¢à¢Šà¯à¬¾à¬µàµŒàµ†à¡¼à¢à¯à¬¾à¬µàµ…ßšà¯à¢Šà¯İ‡àµ’Í²
(7.20)
The modified CG parameter (in the case of Fletcher-Reeves formula) is given as: ßšà¯àµŒ
à¢à³–à°¶à°­
à³…
à¡¼à¢à³–à°¶à°­
à¢à³–
à³…à¡¼à¢à³–
 Finally, 
the CG algorithm is modified to include preconditioning as follows:
Preconditioned Conjugate-Gradient Algorithm (Griva, Nash & Sofer, p. 475):
Initialize: Choose à¢à¬´àµŒà«™Ç¡ à¢˜à¬´àµŒà¢ˆÇ¡ à¢Šáˆºà¬¿à¬µáˆ»àµŒÍ²Ç¡ ßšà¬´àµŒÍ²Ç¤
For İ…àµŒÍ²Ç¡Í³Ç¡ Ç¥
1.	 Check convergence: if Ô¡à¢˜à¯œÔ¡ àµß³ stop.
2.	 Set Wà¢ à¯œàµŒà¡¼à¬¿à¬µà¢˜à¯œ,Iİ…àµÍ²VHWßšà¯œàµŒ
à¢˜à³”
à³…à¢ à³”
à¢˜à³”à°·à°­
à³…
à¢ à³”à°·à°­
3.	 Set à¢Šà¯œàµŒà¢ à¯œàµ…ßšà¯œà¢Šà¯œà¬¿à¬µß™à¯œàµŒ
à¢˜à³”
à³…à¢ à³”
à¢Šà³”à³…à¡­à¢Šà³”à¢à¯œà¬¾à¬µàµŒà¢à¯œàµ…ß™à¯œà¢Šà¯œà¢˜à¯œà¬¾à¬µàµŒà¢˜à¯œàµ†ß™à¯œà¡­à¢Šà¯œ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
140 
NNumerica l Optimizati on Method
Rate of Convergence. Conjugate gradient methods achieve superlinear convergence, which degenerates 
to linear convergence if the initial direction is not chosen as the steepest descent direction. In the case 
of quadratic functions, the minimum is reached exactly in İŠ iterations. For general nonlinear functions, 
convergence in Í´İŠ iterations is to be expected. Nonlinear CG methods typically have the lowest per 
iteration computational costs. 
An example of the CG method is given below.
Example 7.3: Conjugate-gradient method
We wish to solve the following minimization problem: ÂÂ‹Â
à¢İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ…Í²Ç¤Í·İ”à¬¶
à¬¶àµ†İ”à¬µİ”à¬¶Ç¡ where: 
×İ‚áˆºà¢áˆ»à¯àµŒáˆ¾Í´İ”à¬µàµ†İ”à¬¶Ç¡ İ”à¬¶àµ†İ”à¬µáˆ¿
Let İ”à¬´àµŒáˆºÍ³Ç¡Í³áˆ»Ç¡WKHQ×İ‚áˆºà¢à¬´áˆ»àµŒà¢‰à¬´àµŒáˆ¾Í³Ç¡ Í²áˆ¿à¯ and we set à¢Šà¬´àµŒàµ†à¢‰à¬´àµŒáˆ¾àµ†Í³Ç¡Í²áˆ¿à¯ which results in: à¢à¬µàµŒ
áˆ¾Í³ àµ†ß™Ç¡ Í³áˆ¿à¯DQGİ‚áˆºß™áˆ»àµŒáˆºÍ³ àµ†ß™áˆ»à¬¶àµ…ß™àµ†Í²Ç¤Í· Setting İ‚á‡±áˆºß™áˆ»àµŒÍ²Ç¡Z we obtain: ß™àµŒÍ²Ç¤Í· and the solution 
estimate is updated as à¢à¬µàµŒáˆ¾Í²Ç¤Í·Ç¡ Í³áˆ¿à¯
In the second iteration, we set à¢Šà¬µàµŒàµ†à¢‰à¬µàµ…ßšà¬´à¢Šà¬´ZKHUHà¢‰à¬µàµŒáˆ¾Í²Ç¡ Í²Ç¤Í·áˆ¿à¯Ç¡ ßšà¬´àµŒ
à¸®à¢‰à°­à¸®
Ô¡à¢‰à°¬Ô¡ àµŒÍ²Ç¤Í´Í·Ç¤ Accordingly, 
à¢Šà¬µàµŒáˆ¾àµ†Í²Ç¤Í´Í·Ç¡ àµ†Í²Ç¤Í·áˆ¿à¯Ç¡ à¢à¬¶àµŒáˆºÍ³ àµ†Í²Ç¤Í·ß™áˆ»áˆ¾Í²Ç¤Í·Ç¡ Í³áˆ¿à¯DQGİ‚áˆºß™áˆ»àµŒÍ²Ç¤Í´Í·áˆºÍ³ àµ†Í²Ç¤Í·ß™áˆ»à¬¶ Again, by setting 
İ‚á‡±áˆºß™áˆ»àµŒÍ²Ç¡ZHREWDLQß™àµŒÍ´ZKLFKJLYHVà¢à¬¶àµŒáˆ¾Í²Ç¡ Í²áˆ¿ We note that the minimum of a quadratic 
function of two variables is reached in two iterations.
7.3.3	
 Newtonâ€™s Method
Newtonâ€™s method for finding the zero of a nonlinear function was earlier introduced in Section 2.11. 
Here we apply Newtonâ€™s method to solve the nonlinear equation resulting from the application of FONC: 
×İ‚áˆºà¢áˆ»àµŒÍ² We use a linear approximation to ×İ‚áˆºà¢áˆ» to apply this condition as:
×İ‚àµ«à¢à¯àµ…à¢Šàµ¯Ø† ×İ‚àµ«à¢à¯àµ¯àµ…×à¬¶İ‚áˆºİ”à¯áˆ»à¢ŠàµŒà«™
(7.21)
Then, the direction vector is solved from a system of linear equations given as: 
×à¬¶İ‚áˆºà¢à¯áˆ»à¢ŠàµŒàµ†×İ‚áˆºà¢à¯áˆ»
(7.22)
which leads to the following update formula:
à¢à¯à¬¾à¬µàµŒà¢à¯àµ†àµ«×à¬¶İ‚áˆºà¢à¯áˆ»àµ¯
à¬¿à¬µ×İ‚áˆºà¢à¯áˆ»àµŒà¢à¯àµ†à¡´à¯
à¬¿à¬µà¢à¯
(7.23)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
141 
NNumerica l Optimizati on Method
We note that the above formula can also be obtained via second order Taylor series expansion of İ‚áˆºà¢áˆ» 
given as: 
İ‚àµ«à¢à¯àµ…à¢Šàµ¯àµŒİ‚áˆºà¢à¯áˆ»àµ…×İ‚áˆºà¢à¯áˆ»à¯à¢Šàµ…à¬µ
à¬¶à¢Šà¯à¡´à¯à¢ŠàµŒİà¯áˆºà¢Šáˆ»
(7.24)
The above expression implies that at every iteration Newtonâ€™s method approximates İ‚áˆºà¢áˆ» by a quadratic 
function: qk(d); it then solves the minimization problem: min
d qk(d) and updates the current estimate 
as: à¢à¯à¬¾à¬µàµŒà¢à¯àµ…à¢Š Further, the above solution assumes that qk(d) is convex, i.e., à¡´à¯àµŒ×à¬¶İ‚áˆºà¢à¯áˆ» is 
positive-definite. 
The application of Newtonâ€™s method relies on the positive-definite assumption for à¡´à¯àµŒ×à¬¶İ‚áˆºà¢à¯áˆ» If 
×à¬¶İ‚áˆºà¢à¯áˆ» is positive-definite, then a factorization of the form: ×à¬¶İ‚áˆºà¢à¯áˆ»àµŒà¡¸à¡°à¡¸à¯ where İ€à¯œà¯œàµÍ² can 
be used to solve for the resulting system of linear equations: áˆºà¡¸à¡°à¡¸à¯áˆ»à¢ŠàµŒàµ†×İ‚áˆºà¢à¯áˆ» If at any point D 
is found to have negative entries, i.e., if İ€à¯œà¯œàµ‘Í² then it should be replaced by a positive value, such as 
Èİ€à¯œà¯œÈ This correction amounts to adding a diagonal matrix E, such that ×à¬¶İ‚áˆºà¢à¯áˆ»àµ…à¡± is positive-definite. 
An algorithm for Newtonâ€™s method is given below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Fundamental Engineering Optimization 
Methods
142 
NNumerica l Optimizati on Method
Newtonâ€™s Method (Griva, Nash, & Sofer, p. 373): 
Initialize: Choose à¢à¬´ specify ß³
For İ‡àµŒÍ²Ç¡Í³Ç¡ Ç¥
1.	 Check convergence: If I Ô¡×İ‚áˆºà¢à¯áˆ»Ô¡ àµß³ stop
2.	 Factorize modified Hessian as 
à¯
S
×à¬¶İ‚áˆºà¢à¯áˆ»àµ…à¡±àµŒà¡¸à¡°à¡¸à¯DQGVROYHáˆºà¡¸à¡°à¡¸à¯áˆ»à¢ŠàµŒàµ†×İ‚áˆºà¢à¯áˆ»IRUà¢Š 
3.	 Perform line search to determine ß™à¯ and update the solution estimate as 
İ‚
à¯
à¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¯à¢Šà¯
Rate of Convergence. Newtonâ€™s method achieves quadratic rate of convergence in the close neighborhood 
of the optimal point, and superlinear rate of convergence otherwise. Moreover, due to its high 
computational and storage costs, classic Newtonâ€™s method is rarely used in practice. 
7.3.4	
Quasi-Newton Methods
Quasi-Newton methods that use low-cost approximations to the Hessian matrix are the among most 
widely used methods for nonlinear problems. These methods represent a generalization of one-
dimensional secant method, which approximates the second derivative as: İ‚á‡±á‡±áˆºİ”à¯áˆ»Ø†
à¯™á‡²áˆºà¯«à³–áˆ»à¬¿à¯™á‡²áˆºà¯«à³–à°·à°­áˆ»
à¯«à³–à¬¿à¯«à³–à°·à°­

In the multi-dimensional case, the secant method translates into the following: 
×à¬¶İ‚áˆºà¢à¯áˆ»áˆºà¢à¯àµ†à¢à¯à¬¿à¬µáˆ»Ø† ß˜İ‚áˆºà¢à¯áˆ»àµ†ß˜İ‚áˆºà¢à¯à¬¿à¬µáˆ»
(7.25)
Thus, if the Hessian is approximated by a positive-definite matrix  Hk, then Hk then is required to satisfy 
the following secant condition: 
à¡´à¯áˆºà¢à¯àµ†à¢à¯à¬¿à¬µáˆ»àµŒß˜İ‚áˆºà¢à¯áˆ»àµ†ß˜İ‚áˆºà¢à¯à¬¿à¬µáˆ»
(7.26)
Whereas, the above condition places İŠ constraints on the structure of Hk, further constraints may be 
added to completely specify Hk as well as to preserve its symmetry. 
The quasi-Newton methods aim to iteratively update Hk via: 
1.	 The direct update: à¡´à¯à¬¾à¬µàµŒà¡´à¯àµ…Î¿à¡´à¯Ç¡ à¡´à¬´àµŒà¡µRU
2.	 The inverse update: à¡²à¯à¬¾à¬µàµŒà¡²à¯àµ…Î¿à¡²à¯Ç¡à¡²àµŒà¡´à¬¿à¬µÇ¡ à¡²à¬´àµŒà¡µÇ¤
Once Hk is available, it can be employed to solve for the current search direction from: à¡´à¯à¢ŠàµŒàµ†×İ‚áˆºà¢à¯áˆ»
or from: à¢ŠàµŒàµ†à¡²à¯×İ‚áˆºà¢à¯áˆ» 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
143 
NNumerica l Optimizati on Method
To proceed further, let à¢™à¯àµŒà¢à¯àµ†à¢à¯à¬¿à¬µà¢Ÿà¯àµŒß˜İ‚áˆºà¢à¯áˆ»àµ†ß˜İ‚áˆºà¢à¯à¬¿à¬µáˆ»Ç¢ then, a symmetric rank-one update 
formula for Hk is given as (Griva, Nash & Sofer, p.414):
à¡´à¯à¬¾à¬µàµŒà¡´à¯àµ…
áˆºà¢Ÿà¯àµ†à¡´à¯à¢™à¯áˆ»áˆºà¢Ÿà¯àµ†à¡´à¯à¢™à¯áˆ»à¯
áˆºà¢Ÿà¯àµ†à¡´à¯à¢™à¯áˆ»à¯à¢™à¯

(7.27)
However, the above formula, while obeying the secant condition, à¡´à¯à¬¾à¬µà¢™à¯àµŒà¢Ÿà¯ does not ensure that Hk 
is positive-definite. Next, a class of symmetric rank-two update formulas that ensures positive-definiteness 
of Hk are defined by: 
à¡´à¯à¬¾à¬µàµŒà¡´à¯àµ†
áˆºà¡´à¯à¢™à¯áˆ»áˆºà¡´à¯à¢™à¯áˆ»à¯
à¢™à¯à¯à¡´à¯à¢™à¯
àµ…à¢Ÿà¯à¢Ÿà¯à¯
à¢Ÿà¯à¯à¢™à¯
àµ…ß¶áˆºà¢™à¯à¯à¡´à¯à¢™à¯áˆ»à¢œà¯à¢œà¯à¯
(7.28)
where à¢œà¯àµŒ
à¢Ÿà³–
à¢Ÿà³–à³…à¢™à³–àµ†
à¡´à³–à¢™à³–
à¢™à³–à³…à¡´à³–à¢™à³–DQGß¶× áˆ¾Í²Ç¡Í³áˆ» Two popular choices for ß¶DUHß¶àµŒÍ²DQGß¶àµŒÍ³ resulting 
in the well-known DFP (Davison, Fletcher, and Powell) and BGFS (Broyden, Fletcher, Goldfarb, and 
Shanno) update formulas. 
The former (DFP formula) results in the following inverse Hessian update:
à¡²à¯à¬¾à¬µàµŒà¡²à¯àµ†
áˆºà¡²à¯à¢Ÿà¯áˆ»áˆºà¡²à¯à¢Ÿà¯áˆ»à¯
à¢Ÿà¯à¯à¡²à¯à¢Ÿà¯
àµ…à¢™à¯à¢™à¯à¯
à¢Ÿà¯à¯à¢™à¯
 
(7.29)
The latter (BFGS formula) results in a direct Hessian update given as:
à¡´à¯à¬¾à¬µàµŒà¡´à¯àµ†
áˆºà¡´à¯à¢™à¯áˆ»áˆºà¡´à¯à¢™à¯áˆ»à¯
à¢™à¯à¯à¡´à¯à¢™à¯
àµ…à¢Ÿà¯à¢Ÿà¯à¯
à¢Ÿà¯à¯à¢™à¯

(7.30)
We note that the Hessian in the case of a quadratic function, İáˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ†à¢‰à¯à¢ obeys the secant 
condition, à¡½à¢™à¯àµŒà¢Ÿà¯, which shows that a symmetric positive-definite Hk in a quasi-Newton method 
locally approximates the quadratic behavior. 
The quasi-Newton algorithm is given below.
Quasi-Newton Algorithm (Griva, Nash & Sofer, p. 415):
Initialize: Choose à¢à¬´à¡´à¬´HJà¡´à¬´àµŒà¡µ specify ß 
For İ‡àµŒÍ²Ç¡Í³Ç¡ Ç¥
1.	 Check convergence: If Ô¡×İ‚áˆºà¢à¯áˆ»Ô¡ àµß stop
2.	 Solve à¡´à¯à¢ŠàµŒàµ†×İ‚áˆºà¢à¯áˆ»IRUà¢Šà¯ 
3.	 Solve for ÂÂ‹Â
à°ˆİ‚àµ«à¢à¯àµ…ß™à¢Šà¯àµ¯IRUß™à¯DQGXSGDWHà¢à¯à¬¾à¬µàµŒà¢à¯àµ…ß™à¯à¢Šà¯
4.	 Compute à¢™à¯Ç¡ à¢Ÿà¯ and update Hk as per (5.19) or (5.20)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
144 
NNumerica l Optimizati on Method
Rate of Convergence. Quasi-Newton methods achieve superlinear convergence, thus rivaling the second 
order methods for solving nonlinear programming (NP) problems.
Example 7.4: Quasi-Newton method
As an example, we consider the following NL problem: 
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒÍ´İ”à¬µ
à¬¶àµ†İ”à¬µİ”à¬¶àµ…İ”à¬¶
à¬¶
ZKHUH×İ‚àµŒàµ¤Í¶İ”à¬µàµ†İ”à¬¶
Í´İ”à¬¶àµ†İ”à¬µàµ¨Ç¡ ÜªàµŒá‰‚Í¶ àµ†Í³
àµ†Í³Í´á‰ƒ 
Let İ”à¬´àµŒá‰‚Í³
Í³á‰ƒÇ¡ Üªà¬´àµŒÜ«WKHQİ‚à¬´àµŒÍ´Ç¡ İ€à¬´àµŒàµ†×İ‚áˆºİ”à¬´áˆ»àµŒá‰‚àµ†Íµ
àµ†Í³á‰ƒ8VLQJİ‚áˆºß™áˆ»àµŒÍ´áˆºÍ³ àµ†Íµß™áˆ»à¬¶àµ…áˆºÍ³ àµ†ß™áˆ»à¬¶àµ†
áˆºÍ³ àµ†Íµß™áˆ»áˆºÍ³ àµ†ß™áˆ» and putting İ‚á‡±áˆºß™áˆ»àµŒÍ²JLYHVß™àµŒ
à¬¹
à¬µà¬º7KHQİà¬µàµŒß™İ€à¬´àµŒ
à¬¹
à¬µà¬ºá‰‚Í³
Í³á‰ƒÇ¡ İƒà¬µàµŒİ•à¬µÇ¡ à¢à¬µàµŒá‰€
à¬·
à¬¸Ç¡
à¬·
à¬¸á‰
For the Hessian update, we have: İ‚à¬µàµŒÍ²Ç¤Í·Í¸Í´Í·Ç¡ İƒà¬µàµŒàµ†Í²Ç¤Í³Í´Í·Ç¡ İƒà¬¶àµŒİƒà¬·àµŒàµ†Í²Ç¤Í¹Í·Ç¢à¢‰à¬µàµŒáˆ¾Í²Ç¤Í¹Í·Ç¡ Í²Ç¤Í¹Í·áˆ¿
à¬´
à¬´
à¬´
à¬´
 
and, for ß™àµŒÍ²Ç¤Í´Í·Ç¡à¢™à¬´àµŒáˆ¾àµ†Í²Ç¤Í´Í·Ç¡ àµ†Í²Ç¤Í´Í·áˆ¿àµŒà¢ à¬´àµŒà¢Ÿà¬´Ç¡ ß¦à¬µàµŒß¦à¬¶àµŒÍ²Ç¤Í³Í´Í·Ç¡ ß àµŒÍ³Ç¡ à¢à¬´àµŒà¢Ÿà¬´Ç¡ ß¦à¬·àµŒß¦à¬µ 
then, the Hessian update is computed as: à¡°à¬´àµŒÍº á‰‚Í³
Í³
Í³
Í³á‰ƒÇ¡ à¡±à¬´àµŒÍº á‰‚Í³
Í³
Í³
Í³á‰ƒÇ¡ à¡´à¬µàµŒà¡´à¬´Ç¤ 
We next proceed to discuss the trust-region methods of solving NP problems.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Fundamental Engineering Optimization 
Methods
145 
NNumerica l Optimizati on Method
7.3.5	
 Trust-Region Methods
Trust-region methods locally employ a quadratic approximation İà¯áˆºà¢à¯áˆ» to the nonlinear objective 
function; they were originally proposed to solve the nonlinear least-squares problems, but have since 
been adapted to solve more general optimization problems. 
The quadratic approximation is given as: İáˆºà¢áˆ»àµŒ
à¬µ
à¬¶à¢à¯à¡½à¢àµ†à¢‰à¯à¢Ç¡ and is valid in a limited neighborhood 
È³à¯àµŒáˆ¼à¢Ç£ Ô¡àª¡áˆºà¢àµ†à¢à¯áˆ»Ô¡ àµ‘Î¿à¯áˆ½RIà¢à¯ where àª¡ is a scaling parameter. The method then aims to find 
a à¢à¯à¬¾à¬µ× È³à¯Ç¡ which results in sufficient decrease in İ‚áˆºà¢áˆ» At each iteration k, trust-region algorithm 
solves a constrained optimization sub-problem defined by: 
ÂÂ‹Â
à¢Šİà¯áˆºà¢Šáˆ»àµŒİ‚áˆºà¢à¯áˆ»àµ…×İ‚áˆºà¢à¯áˆ»à¯à¢Šàµ…Í³
Í´ à¢Šà¯×à¬¶İ‚áˆºà¢à¯áˆ»à¢Š 
(7.31)
subject to Ô¡à¢ŠÔ¡ àµ‘Î¿à¯ 
Using a Lagrangian function approach the first order optimality conditions are given as:
áˆº×à¬¶İ‚áˆºà¢à¯áˆ»àµ…ß£à¡µáˆ»à¢Šà¯àµŒàµ†×İ‚áˆºà¢à¯áˆ»
(7.32)
where Hß£àµ’Í² is the Lagrange multiplier associated with the constraint, and áˆº×à¬¶İ‚áˆºà¢à¯áˆ»àµ…ß£à¡µáˆ» is a positive-
definite matrix. The quality of the quadratic approximation is estimated by: ß›à¯àµŒ
à¯™áˆºà¢à³–áˆ»à¬¿à¯™áˆºà¢à³–à°¶à°­áˆ»
à¯¤à³–áˆºà¢à³–áˆ»à¬¿à¯¤à³–áˆºà¢à³–à°¶à°­áˆ». If this 
ratio is close to unity, the trust region may be expanded in the next iteration. 
The resulting search direction dk is a function of Lagrange multiplier ß£à¢Šà¯àµŒà¢Šà¯áˆºß£áˆ» Thus, for ß£àµŒÍ² 
a sufficiently large Î¿à¯ and for a positive-definite ×à¬¶İ‚áˆºà¢à¯áˆ»à¢Šà¯áˆºÍ²áˆ» reduces to the Newtonâ€™s direction. 
Whereas, for Î¿à¯àµŒÍ²ß£Õœ Î»DQGà¢Šà¯áˆºß£áˆ» and aligns with the steepest-descent direction. 
The trust-region algorithm is given as follows:
Trust-Region Algorithm (Griva, Nash & Sofer, p.392):
Initialize: Choose 
J

S

à¢à¬´Î¿à¬´VSHFLI\ßÇ¡ Í² àµß¤àµßŸàµÍ³HJß¤àµŒ
à¬µ
à¬¸Ç¢ ßŸàµŒ
à¬·
à¬¸
For İ‡àµŒÍ²Ç¡Í³Ç¡ Ç¥
1.	 Check convergence: If Ô¡×İ‚áˆºà¢à¯áˆ»Ô¡ àµß stop
2.	 Solve ÂÂ‹Â
à¢Šİà¯áˆºà¢Šáˆ»VXEMHFWWRÔ¡à¢ŠÔ¡ àµ‘Î¿à¯
3.	 Compute ß›à¯ 
a)	 if ß›à¯àµß¤VHWà¢à¯à¬¾à¬µàµŒà¢à¯Ç¡ Î¿à¯à¬¾à¬µàµŒ
à¬µ
à¬¶Î¿à¯ 
b)	 else if ß›à¯àµßŸVHWà¢à¯à¬¾à¬µàµŒà¢à¯àµ…à¢Šà¯Ç¡ Î¿à¯à¬¾à¬µàµŒÎ¿à¯
c)	 else set à¢à¯à¬¾à¬µàµŒà¢à¯àµ…à¢Šà¯Ç¡ Î¿à¯à¬¾à¬µàµŒÍ´Î¿à¯
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
146 
NNumerica l Optimizati on Method
7.4	
Computer Methods for Solving the Constrained Problems
In this section, we describe the numerical methods devised for solving constrained nonlinear optimization 
problems. These methods fall into two broad categories: the first category includes penalty, barrier, and 
augmented Lagrangian methods that are an extension of the methods developed for unconstrained 
problems, and are collectively known as the transformation methods. The second category includes 
methods that iteratively approximate the nonlinear problem as a series of LP or QP problems and use 
the LP or QP methods to solve it.
In order to discuss these methods, we consider a general optimization problem described as:
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
Subject to á‰
İ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŒÇ¢
İƒà¯áˆºà¢áˆ»àµ‘Í²Ç¡ İ†àµŒİ…Ç¡ Ç¥ Ç¡ İ‰Ç¢
İ”à¯œà¯…àµ‘İ”à¯œàµ‘İ”à¯œà¯Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŠÇ¤

(7.33)
Prominent computer methods for solving constrained optimization problems are described in this and 
the following section.
7.4.1	
 Penalty and Barrier Methods
The Penalty and Barrier methods are extensions of the numerical methods developed for solving 
unconstrained optimization problems. Both methods employ a composite of objective and constraint 
functions where the constraints are assigned a high violation penalty. Once a composite function has been 
defined for a set of penalty parameters, it can be minimized using any of the unconstrained optimization 
techniques. The penalty parameters can then be adjusted in successive iterations. 
The Penalty and Barrier methods fall under sequential unconstrained minimization techniques (SUMTs). 
Because of their simplicity, SUMTs have been extensively developed and used in engineering design 
problems. The SUMTs generally employ a composite function of the following form (Arora, p. 477): 
È°áˆºà¢Ç¡ İáˆ»àµŒİ‚áˆºà¢áˆ»àµ…Ü²áˆºİƒáˆºà¢áˆ»Ç¡ İ„áˆºà¢áˆ»Ç¡ à¢˜áˆ»
(7.34)
where İƒáˆºà¢áˆ»DQGİ„áˆºà¢áˆ» are, respectively, the inequality and equality constraints, and r is a vector of penalty 
parameters. Depending on their region of iteration, these methods are further divided into Penalty or 
Barrier methods as described below: 
Penalty Function Method. A penalty function method that iterates through the infeasible region of 
space, employs a quadratic loss function of the following form:
Ü²áˆºİƒáˆºà¢áˆ»Ç¡ İ„áˆºà¢áˆ»Ç¡ à¢˜áˆ»àµŒİàµ¬à·àµ«İƒà¯œ
à¬¾áˆºà¢áˆ»àµ¯
à¬¶
à¯œ
àµ…à·àµ«İ„à¯œáˆºà¢áˆ»àµ¯
à¬¶
à¯œ
àµ°Ç¢İƒà¯œ
à¬¾áˆºà¢áˆ»àµŒÂÂƒÂšàµ«Í²Ç¡ İƒà¯œáˆºà¢áˆ»àµ¯Ç¡ İàµÍ² (7.35)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
147 
NNumerica l Optimizati on Method
Barrier Function Method. A barrier method that iterates through the feasible region of space, and is 
only applicable to inequality constrained problems, employs a log barrier function of the following form: 
Ü²áˆºİƒáˆºà¢áˆ»Ç¡ İ„áˆºà¢áˆ»Ç¡ à¢˜áˆ»àµŒÍ³
İà·ÂÂ‘Â‰àµ«àµ†İƒà¯œáˆºİ”áˆ»àµ¯
à¯œ

(7.36)
For both penalty and barrier methods, convergence implies that as İÕœ Î»à¢áˆºİáˆ»Õœ à¢×›ZKHUHà¢áˆºİáˆ» 
minimizes È°áˆºà¢Ç¡ İáˆ» To improve convergence, r may be replaced by a sequence áˆ¼İà¯áˆ½ We, however, note 
that since the Hessian of the unconstrained function becomes ill-conditioned for large r, both methods 
are ill-behaved near the constraint boundary.
7.4.2	
 The Augmented Lagrangian Method
As an alternative to the penalty and barrier methods described above, the augmented Lagrangian (AL) 
methods add a quadratic penalty term to the Lagrangian function that also includes multipliers for 
penalizing individual constraint violations. The resulting AL method is generally more effective than 
penalty and barrier methods, and is commonly employed to solve Finite Element Analysis problems.
The augmented Lagrangian method is introduced below using an equality constrained optimization 
problem where the problem is given as (Belegundu and Chandrupatla, p. 276):
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
6XEMHFWWRİ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ

(7.37)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
148 
NNumerica l Optimizati on Method
The augmented Lagrangian function for the problem is defined as: 
à£ªáˆºà¢Ç¡ à¢œÇ¡ İáˆ»àµŒİ‚áˆºà¢áˆ»àµ…à·á‰†İ’à¯İ„à¯áˆºà¢áˆ»àµ…Í³
Í´ İİ„à¯
à¬¶áˆºà¢áˆ»á‰‡
à¯

(7.38)
In the above, İ’à¯ are the Lagrange multipliers and the additional term defines an exterior penalty function 
with r as the penalty parameter. The gradient and Hessian of the AL are computed as:
×à£ªáˆºà¢Ç¡ à¢œÇ¡ İáˆ»àµŒ×İ‚áˆºà¢áˆ»àµ…à·á‰€İ’à¯àµ…İİ„à¯áˆºà¢áˆ»á‰
à¯
×İ„à¯áˆºà¢áˆ»
×à¬¶à£ªáˆºà¢Ç¡ à¢œÇ¡ İáˆ»àµŒ×à¬¶İ‚áˆºà¢áˆ»àµ…à·àµ¬á‰€İ’à¯àµ…İİ„à¯áˆºà¢áˆ»á‰×à¬¶İ„à¯áˆºà¢áˆ»àµ…İ×İ„à¯
à­˜×İ„à¯áˆºà¢áˆ»àµ°
à¯


(7.39)
While the Hessian of the Lagrangian may not be uniformly positive definite, a large enough value of r 
makes the Hessian of AL positive definite at x.
Next, since the AL is stationary at the optimum, then, paralleling the developments in the duality theory 
(Sec. 5.7), we can solve the above optimization problem via a min-max framework as follows: first, for 
a given r and v, we define a dual function via the following minimization problem:
ß°áˆºà¢œáˆ»àµŒÂÂ‹Â
à¢à£ªáˆºà¢Ç¡ à¢œÇ¡ İáˆ»àµŒİ‚áˆºà¢áˆ»àµ…à·àµ¬İ’à¯İ„à¯áˆºà¢áˆ»àµ…Í³
Í´ İá‰€İ„à¯áˆºà¢áˆ»á‰
à¬¶
àµ°
à¯

(7.40)
This step is then followed by a maximization problem defined as: ÂÂƒÂš
à¢œ
ß°áˆºà¢œáˆ» The derivative of the dual 
function is computed as: 
à¯—à°Ÿ
à¯—à¯©à³•àµŒİ„à¯áˆºà¢áˆ»àµ…×ß°à¯à¯—à¢
à¯—à¯©à³• where the latter term is zero, since ×ß°àµŒ×à£ªàµŒÍ² 
Further, an expression for the Hessian is given as: 
à¯—à°®à°Ÿ
à¯—à¯©à³”à¯—à¯©à³•àµŒ×İ„à¯œ
à¯à¯—à¢
à¯—à¯©à³•Ç¡ZKHUHWKH
à¯—à¢
à¯—à¯©à³• where the term 
can be obtained by differentiating ×ß°àµŒÍ² which gives: ×İ„à¯àµ…×à¬¶à£ªàµ¬
à¯—à¢
à¯—à¯©à³•àµ°àµŒÍ²RU×à¬¶à£ªàµ¬
à¯—à¢
à¯—à¯©à³•àµ°àµŒàµ†×İ„à¯ 
Therefore, the Hessian is computed as:
İ€à¬¶ß°
İ€İ’à¯œİ€İ’à¯
àµŒàµ†×İ„à¯œ
à¯áˆº×à¬¶à£ªáˆ»à¬¿à¬µ×İ„à¯
(7.41)
The AL method proceeds as follows: we choose a suitable áˆºİ’áˆ» and solve the minimization problem in 
(7.40) to define ß°áˆºİ’áˆ» We then solve the maximization problem to find the solution that minimizes the 
AL. The latter step can be done using gradient-based methods. For example, the Newton update for the 
maximization problem is given as:
à¢œà¯à¬¾à¬µàµŒà¢œà¯àµ†á‰†İ€à¬¶ß°
İ€İ’à¯œİ€İ’à¯
á‰‡
à¬¿à¬µ
à¢
(7.42)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
149 
NNumerica l Optimizati on Method
For large r, the update may be approximated as: İ’à¯
à¯à¬¾à¬µàµŒİ’à¯
à¯àµ…İà¯İ„à¯Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ (Belegundu and 
Chandrupatla, p. 278). For inequality constrained problems, the AL may be defined as (Arora, p. 480):
à£ªáˆºà¢Ç¡ à¢›Ç¡ İáˆ»àµŒİ‚áˆºà¢áˆ»àµ…à·àµ
İ‘à¯œİƒà¯œáˆºà¢áˆ»àµ…Í³
Í´ İİƒà¯œ
à¬¶áˆºà¢áˆ»Ç¡Â‹Âˆİƒà¯àµ…İ‘à¯
İàµ’Í²
àµ†Í³
Í´İİ‘à¯œ
à¬¶Ç¡Â‹Âˆİƒà¯àµ…İ‘à¯
İàµÍ²
à¯œ

(7.43)
The AL algorithm is given below.
The Augmented Lagrangian Algorithm (Arora, p. 480)
Initialize: estimate İ”à¬´Ç¡ İ‘à¬´àµ’Í²Ç¡ İ’à¬´Ç¡ İàµÍ²Ç¢FKRRVHß™àµÍ²Ç¡ ßšàµÍ³Ç¡ ß³àµÍ²Ç¡ ß¢àµÍ²Ç¡ Ü­àµŒÎ» 
For İ‡àµŒÍ³Ç¡Í´Ç¡ Ç¥
1.	 Solve à¢à¯àµŒÂÂ‹Â
à¢à£ªáˆºà¢Ç¡ à¢›Ç¡ à¢œÇ¡ İà¯áˆ»
2.	 Evaluate İ„à¯œàµ«à¢à¯àµ¯Ç¡ İ…àµŒÍ³Ç¡ Ç¤ Ç¤ Ç¡ İˆÇ¢ İƒà¯àµ«à¢à¯àµ¯Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¢
compute Ü­à´¥àµŒİ‰Ü½İ”á‰„Èİ„à¯œÈÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆÇ¢ ÂÂƒÂš á‰€İƒà¯Ç¡ àµ†
à¯¨à³•
à¯¥à³–á‰Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰á‰…
3.	 Check termination: If Ü­à´¥àµ‘ß¢DQGà¸®×à£ªàµ«à¢à¯àµ¯à¸®àµ‘ß³İ‰Ü½İ”àµ›Í³Ç¡ à¸®à¢à¯à¸®àµŸ quit
4.	 If Ü­à´¥àµÜ­ (i.e., constraint violations have improved), set Ü­àµŒÜ­à´¥
Set İ’à¯œ
à¯à¬¾à¬µàµŒİ’à¯œ
à¯àµ…İà¯İ„à¯œàµ«à¢à¯àµ¯Ç¢ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ6HWİ‘à¯
à¯à¬¾à¬µàµŒİ‘à¯
à¯àµ…İà¯İ‰Ü½İ”àµœİƒà¯àµ«à¢à¯àµ¯Ç¡ àµ†
à¯¨à³•
à³–
à¯¥à³–àµ Ç¢ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
If Ü­à´¥àµ
à¯„
à°ˆÇ¡(i.e., constraint violations did not improve by a factor ß™ ), set İà¯à¬¾à¬µàµŒßšİà¯
An example for the AL method is now presented.
Example 7.5: Design of cylindrical water tank (Belegundu and Chandrupatla, p. 278)
We consider the design of an open-top cylindrical water tank. We wish to maximize the volume of the 
tank for a given surface area Ü£à¬´ Let d be the diameter and h be the height; then, the optimization 
problem is formulated as:
ÂÂƒÂš
à¯—Ç¡à¯Ÿİ‚áˆºİ€Ç¡ İˆáˆ»àµŒß¨İ€à¬¶İˆ
Í¶ 
subject to İ„Ç£
à°—à¯—à°®
à¬¸àµ…ß¨İ€İˆàµ†Ü£à¬´àµŒÍ²
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
150 
NNumerica l Optimizati on Method
We drop the constant à°—
à¬¸Ç¡ convert to a minimization problem, assume 
à¬¸à®ºà°¬
à°—àµŒÍ³Ç¡ and redefine the problem 
as:
ÂÂ‹Â
à¯—Ç¡à¯Ÿİ‚Ò§áˆºİ€Ç¡ İˆáˆ»àµŒàµ†İ€à¬¶İˆ
subject to İ„Ç£İ€à¬¶àµ…Í¶İ€İˆàµ†Í³ àµŒÍ²
A Lagrangian function for the problem is formulated as: à£¦áˆºİ€Ç¡ İˆÇ¡ ß£áˆ»àµŒàµ†İ€à¬¶İˆàµ…ß£áˆºİ€à¬¶àµ…Í¶İ€İˆàµ†Í³áˆ»
The FONC for the problem are: àµ†Í´İ€İˆàµ…Í´ß£áˆºİ€àµ…Í´İˆáˆ»àµŒÍ²Ç¡ àµ†İ€à¬¶àµ…Í¶İ€ß£àµŒÍ²Ç¡ İ€à¬¶àµ…Í¶İ€İˆàµ†Í³ àµŒÍ²
Using FONC, the optimal solution is given as: İ€×› àµŒÍ´İˆ×› àµŒÍ¶ß£×› àµŒ
à¬µ
Î¾à¬·
The Hessian at the optimum point is given as: ×à¬¶à£¦áˆºİ€×›Ç¡ İˆ×›Ç¡ ß£×›áˆ»àµŒá‰‚àµ†Í´ß£
àµ†Í¶ß£
àµ†Í¶ß£
Í² á‰ƒ, It is evident that the 
Hessian is not positive definite. 
Next, the AL for the problem is formed as: à£ªáˆºİ€Ç¡ İˆÇ¡ ß£Ç¡ İáˆ»àµŒàµ†İ€à¬¶İˆàµ…ß£áˆºİ€à¬¶àµ…Í¶İ€İˆàµ†Í³áˆ»àµ…Í³
Í´ İáˆºİ€à¬¶àµ…Í¶İ€İˆàµ†Í³áˆ»à¬¶
The dual function is defined as: ß°áˆºß£áˆ»àµŒÂÂ‹Â
à¯—Ç¡à¯Ÿà£ªáˆºİ€Ç¡ İˆÇ¡ ß£Ç¡ İáˆ» 
The dual optimization problem is then formulated as: ÂÂƒÂš
à¯—Ç¡à¯Ÿß°áˆºß£áˆ» 
A plot of ß°áˆºß£áˆ»YVß£ vs. shows a concave function with ß£×› àµŒß£à¯ à¯”à¯«àµŒÍ²Ç¤Í³Í¶Í¶ 
The optimum values for the design variables are the same as above: İ€×› àµŒÍ´İˆ×› àµŒÍ²Ç¤Í·Í¹Í¹Ç¤
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Fundamental Engineering Optimization 
Methods
151 
NNumerica l Optimizati on Method
7.5	
Sequential Linear Programming
The sequential linear programming (SLP) method aims to sequentially solve the nonlinear optimization 
problem as a series of linear programs. In particular, we employ the first order Taylor series expansion 
to iteratively develop and solve a new LP subprogram to solve the KKT conditions associated with the 
NP problem. SLP methods are generally not robust, and have been mostly replaced by SQP methods.
To develop the SLP method, let xk denote the current estimate of design variables and let d denote the 
change in variable; then, we express the first order expansion of the objective and constraint functions 
in the neighborhood of xk as: 
İ‚àµ«à¢à¯àµ…à¢Šàµ¯àµŒİ‚àµ«à¢à¯àµ¯àµ…×İ‚àµ«à¢à¯àµ¯
à¯à¢Š
İƒà¯œàµ«à¢à¯àµ…à¢Šàµ¯àµŒİƒà¯œàµ«à¢à¯àµ¯àµ…×İƒà¯œàµ«à¢à¯àµ¯
à¯à¢ŠÇ¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰
İ„à¯àµ«à¢à¯àµ…à¢Šàµ¯àµŒİ„à¯àµ«à¢à¯àµ¯àµ…×İ„à¯àµ«à¢à¯àµ¯
à¯à¢ŠÇ¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ

(7.44)
To proceed further, let: İ‚à¯àµŒİ‚àµ«à¢à¯àµ¯Ç¡ İƒà¯œ
à¯àµŒİƒà¯œàµ«à¢à¯àµ¯Ç¡İ„à¯
à¯àµŒİ„à¯àµ«à¢à¯àµ¯ and define: Ü¾à¯œàµŒàµ†İƒà¯œ
à¯Ç¡ İà¯àµŒàµ†İ„à¯
à¯Ç¡  
àµ«
àµ¯
àµ«
àµ¯
à¯
à¯àµ«
àµ¯
à¢‰àµŒ×İ‚àµ«à¢à¯àµ¯Ç¡à¢‡à¯œàµŒ×İƒà¯œàµ«à¢à¯àµ¯Ç¡ à¢”à¯àµŒ×İ„à¯àµ«à¢à¯àµ¯Ç¡ à¡­àµŒáˆ¾à¢‡à¬µÇ¡ à¢‡à¬¶Ç¡ Ç¥ Ç¡ à¢‡à¯ áˆ¿Ç¡ à¡ºàµŒáˆ¾à¢”à¬µÇ¡ à¢”à¬¶Ç¡ Ç¥ Ç¡ à¢”à¯Ÿáˆ¿ Then, after 
dropping the constant term İ‚à¯ from the objective function, we define the following LP subprogram for 
the current iteration of the NP problem (Arora, p. 498):
ÂÂ‹Â
à¢Šİ‚Ò§ àµŒà¢‰à¯à¢Š
Subject to: à¡­à¯à¢Šàµ‘à¢ˆÇ¡ à¡ºà¯à¢ŠàµŒà¢‹
(7.45)
where İ‚Ò§ represents the linearized change in the original cost function and the columns of A and N 
represent, respectively, the gradients of inequality and equality constraints. Since the objective and 
constraint functions are now linear, the resulting LP subproblem can be converted to standard form and 
solved via the Simplex method. Problems with a small number of variables can also be solved graphically 
or by application of KKT conditions to the LP problem. 
The following points regarding the SLP method should be noted:
1.	 Since both positive and negative changes to design variables xk are allowed, the variables İ€à¯œ are 
unrestricted in sign and, therefore, must be replaced by İ€à¯œàµŒİ€à¯œ
à¬¾àµ†İ€à¯œ
à¬¿ in the Simplex algorithm.
2.	 In order to apply the simplex method to the problem, the rhs parameters Ü¾à¯œÇ¡ İà¯ are assumed 
non-negative, or else, the respective constraint must be multiplied with àµ†Í³ 
3.	 SLP methods require additional constraints of the form, àµ†Î¿à¯œà¯Ÿ
à¯àµ‘İ€à¯œ
à¯àµ‘Î¿à¯œà¯¨
à¯ termed as move 
limits, to bind the LP solution. These move limits represent the maximum allowed change in 
İ€à¯œ in the current iteration. They are generally selected as a percentage (1â€“100%) of the design 
variable values. They serve dual purpose of binding the LP solution and obviating the need 
for line search in the current iteration. Restrictive move limits tend to make the SLP problem 
infeasible.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
152 
NNumerica l Optimizati on Method
The SLP algorithm is presented below:
SLP Algorithm (Arora, p. 508): 
Initialize: choose à¢à¬´Ç¡ ßà¬µàµÍ²Ç¡ ßà¬¶àµÍ²
For İ‡àµŒÍ²Ç¡Í³Ç¡Í´Ç¡ Ç¥
1.	 Choose move limits Î¿à¯œà¯Ÿ
à¯Ç¡ Î¿à¯œà¯¨
à¯ as some fraction of current design xk
2.	 Compute İ‚à¯Ç¡ à¢‰Ç¡ İƒà¯œ
à¯Ç¡ İ„à¯
à¯Ç¡ Ü¾à¯œÇ¡ İà¯
3.	 Formulate and solve the LP subproblem for dk
4.	 If and İƒà¯œàµ‘ßà¬µÇ¢ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰Ç¢à¸«İ„à¯à¸«àµ‘ßà¬µÇ¢ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İŒÇ¢DQGà¸®à¢Šà¯à¸®àµ‘ßà¬¶ stop
5.	 Substitute à¢à¯à¬¾à¬µÕš à¢à¯àµ…ß™à¢Šà¯Ç¡İ‡Õš İ‡àµ…Í³
The SLP algorithm is simple to apply, but should be used with caution in engineering design problems 
as it can easily run into convergence problems. The selection of move limits is one of trial and error and 
can be best achieved in an interactive mode. 
An example is presented to explain the SLP method:
Example 7.6: Sequential Linear Programming
We perform one iteration of the SLP algorithm for the following NLP problem:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ†İ”à¬µİ”à¬¶àµ…İ”à¬¶
à¬¶
Subject to: Í³ àµ†İ”à¬µ
à¬¶àµ†İ”à¬¶
à¬¶àµ‘Í²Ç¢àµ†İ”à¬µàµ‘Í²Ç¡ àµ†İ”à¬¶àµ‘Í²
The NLP problem is convex and has a single minimum at à¢×› àµŒá‰€
à¬µ
Î¾à¬¶Ç¡
à¬µ
Î¾à¬¶á‰Ç¤ The objective and constraint 
gradients are: 
Î¾
Î¾
×İ‚à¯àµŒáˆ¾Í´İ”à¬µàµ†İ”à¬¶Ç¡ Í´İ”à¬¶àµ†İ”à¬µáˆ¿Ç¡ ×İƒà¬µ
à¯àµŒáˆ¾àµ†Í´İ”à¬µÇ¡ àµ†Í´İ”à¬¶áˆ¿Ç¡ ×İƒà¬¶
à¯àµŒáˆ¾àµ†Í³Ç¡Í²áˆ¿Ç¡ ×İƒà¬·
à¯àµŒáˆ¾Í²Ç¡ àµ†Í³áˆ¿ 
Let à¢à¬´àµŒáˆºÍ³Ç¡ Í³áˆ»Ç¡ so that İ‚à¬´àµŒÍ³Ç¡à¢‰à¯àµŒáˆ¾Í³Í³áˆ¿ further, let ßà¬µàµŒßà¬¶àµŒÍ²Ç¤Í²Í²Í³ then, using SLP method, 
the resulting LP problem at the current step is defined as: 
ÂÂ‹Â
à¯—à°­Ç¡à¯—à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ€à¬µàµ…İ€à¬¶
Subject to: àµ¥
àµ†Í´
àµ†Í´
àµ†Í³
Í²
Í²
àµ†Í³
àµ©àµ¤İ€à¬µ
İ€à¬¶àµ¨àµ‘àµ¥
Í³
Í³
Í³
àµ©
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
153 
NNumerica l Optimizati on Method
Since the LP problem is unbounded, we may use 50% move limits to bind the solution. The resulting 
update is given as: 
\
à¢Š×› àµŒá‰‚àµ†
à¬µ
à¬¶Ç¡ àµ†
à¬µ
à¬¶á‰ƒ
à¯
VRWKDWà¢à¬µàµŒá‰‚
à¬µ
à¬¶Ç¡
à¬µ
à¬¶á‰ƒ
à¯
 with resulting constraint violations given 
as: İƒà¯œàµŒá‰„
à¬µ
à¬¶Ç¡ Í²Ç¡ Í²á‰… We note that smaller move limits in this step could have avoided resulting constraint 
violation. 
The SLP algorithm is not robust as move limits need to be imposed to force a solution. In the following, 
a sequential quadratic problem that obviates the need for move limits is formulated and solved. 
7.6	
Sequential Quadratic Programming 
The sequential quadratic programming (SQP) method improves on the SLP method by discarding the 
move limits in favor of more robust ways of binding the solution. Specifically, SQP adds Ô¡à¢ŠÔ¡Ç¡ZKHUHà¢Š 
where represents the search direction, to the linear objective function (7.45) to define the resulting QP 
subproblem as follows (Arora, p. 514):
ÂÂ‹Â
à¢Šİ‚Ò§ àµŒà¢‰à¯à¢Šàµ…Í³
Í´ à¢Šà¯à¢Š
Subject to, à¡­à¯à¢Šàµ‘à¢ˆÇ¡ à¡ºà¯à¢ŠàµŒà¢‹
(7.46)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Fundamental Engineering Optimization 
Methods
154 
NNumerica l Optimizati on Method
Since the QP subproblem represents a convex programming problem, a unique global minimum, if one 
exists, can be obtained. We further make the following observations regarding the QP problem:
1.	 From a geometric perspective, İ‚Ò§ represents the equation of a hypersphere with its center 
È‚ à¢‰Ç¡ at and the search direction d points to the center of the hypersphere. 
2.	 When there are no active constraints, application of FONC: 
à°¡à¯™Ò§
à°¡à¢ŠàµŒà¢‰àµ…à¢ŠàµŒÍ² results in the 
search direction: à¢ŠàµŒÈ‚ à¢‰ which conforms to the steepest descent direction. 
3.	 When constraints are present, the QP solution amounts to projecting the steepest-descent 
direction onto the constraint hyperplane; the resulting search direction is termed as 
constrained steepest-descent (CSD) direction.
The QP subproblem can be analytically solved via the Lagrangian function approach. To do that, we add 
a slack variable à¢™ to the inequality constraint, and construct a Lagrangian function given as: 
à£¦áˆºà¢ŠÇ¡ à¢›Ç¡ à¢œáˆ»àµŒà¢‰à¯à¢Šàµ…à¬µ
à¬¶à¢Šà¯à¢Šàµ…à¢›à¯áˆºà¡­à¯à¢Šàµ†à¢ˆàµ…à¢™áˆ»àµ…à¢œà¯áˆºà¡ºà¯à¢Šàµ†à¢‹áˆ»
(7.47)
Then, the KKT conditions for a minimum are:

àª¸à£¦àµŒà¢‰àµ…à¢Šàµ…à¡­à¢›àµ…à¡ºà¢œàµŒà«™Ç¡
à¡­à¯à¢Šàµ…à¢™àµŒà¢ˆÇ¡
à¡ºà¯à¢ŠàµŒà¢‹Ç¡
à¢›à¯à¢™àµŒà«™Ç¡ à¢›àµ’à«™Ç¡ à¢™àµ’à«™ (7.48)
Further, by writing à¢œàµŒà¢Ÿàµ†à¢ Ç¡ à¢Ÿàµ’à«™Ç¡ à¢ àµ’à«™Ç¡ these conditions are expressed in matrix form as:
àµ¥
à¡µ
à¡­
à¡­à¯
à«™
à¡ºà¯
à«™

à«™
à¡µ
à«™

à¡º
àµ†à¡º
à«™
à«™
à«™
à«™
àµ©
Û
Û
Û
Û
Ûà¢Š
à¢›
à¢™
à¢Ÿ
à¢ Û’
Û‘
Û‘
Û‘
Û
àµŒá‰ˆ
àµ†à¢‰
à¢ˆ
à¢‹
á‰‰Ç¡RUà¡¼à¢„àµŒà¡½
(7.49)
where the complementary slackness conditions, à¢›à¯à¢™àµŒà«™Ç¡ translate as: à¢„à¯œà¢„à¯œà¬¾à¯ àµŒÍ²Ç¡ İ…àµŒİŠàµ…Í³Ç¡ Ú® Ç¡ İŠàµ…İ‰ 
We note that solution to the above problem can be obtained via LCP framework (Sec. 5.7.1).
Once a search direction d has been determined, a step-size along d needs to be computed by solving 
the line search problem. We next discuss the descent function approach that is used to resolve the line 
search step in the SQP solution process.
7.6.1	
 Descent Function Approach
In SQP methods, the line search solution is based on minimization of a descent function that penalizes 
constraint violations. The following descent function has been proposed in literature (Arora, p. 521):
È°áˆºà¢áˆ»àµŒİ‚áˆºà¢áˆ»àµ…Ü´Ü¸áˆºà¢áˆ»
(7.50)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
155 
NNumerica l Optimizati on Method
where İ‚áˆºà¢áˆ» represents the cost function value, Ü¸áˆºà¢áˆ» represents the maximum constraint violation, and 
Ü´àµÍ² is a penalty parameter. 
The descent function value at the current iteration is expressed as: 
È°à¯àµŒİ‚à¯àµ…Ü´Ü¸à¯Ü´àµŒÂÂƒÂšáˆ¼Ü´à¯Ç¡ İà¯áˆ½
(7.51)
where Ü´à¯ is the current value of the penalty parameter, İà¯ is the current sum of the Lagrange multipliers, 
and Ü¸à¯ is the maximum constraint violation in the current step. The latter parameters are computed as: 
İà¯àµŒÏƒ
İ‘à¯œ
à¯
à¯ 
à¯œà­€à¬µ
àµ…Ïƒ
à¸«İ’à¯
à¯à¸«
à¯£
à¯à­€à¬µ

Ü¸à¯àµŒÂÂƒÂšáˆ¼Í²Ç¢İƒà¯œÇ¡ İ…àµŒÍ³Ç¡ Ç¤ Ç¤ Ç¤ Ç¡ İ‰Ç¢à¸«İ„à¯à¸«Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŒáˆ½
(7.52)
where absolute values of the Lagrange multipliers and constraint violations for equality constraints are 
used. Next, the line search subproblem is defined as:
ÂÂ‹Â
à°ˆÈ°áˆºß™áˆ»àµŒÈ°àµ«à¢à¯àµ…ß™à¢Šà¯àµ¯
(7.53)
The above problem may be solved via the line search methods described in Sec. 7.2. 
An algorithm for solving the SQP problem is presented below:
SQP Algorithm (Arora, p. 526): 
Initialize: choose à¢à¬´Ç¡ Ü´à¬´àµŒÍ³Ç¡ ßà¬µàµÍ²Ç¡ ßà¬¶àµÍ²
For İ‡àµŒÍ²Ç¡Í³Ç¡Í´Ç¡ Ç¥
1.	 Compute İ‚à¯Ç¡ İƒà¯œ
à¯Ç¡ İ„à¯
à¯Ç¡ à¢‰Ç¡ Ü¾à¯œÇ¡ İà¯FRPSXWHÜ¸à¯
1.	 Formulate and solve the QP subproblem to obtain dk and the Lagrange multipliers 
à¢›à¯DQGà¢œà¯
2.	 If Ü¸à¯àµ‘ßà¬µDQGà¸®à¢Šà¯à¸®àµ‘ßà¬¶VWRS
3.	 Compute Ü´ formulate and solve line search subproblem to obtain ß™
4.	 Set à¢à¯à¬¾à¬µÕš à¢à¯àµ…ß™à¢Šà¯Ç¡ Ü´à¯à¬¾à¬µÕš Ü´Ç¡İ‡Õš İ‡àµ…Í³
It can be shown that the above algorithm is convergent, i.e., È°àµ«à¢à¯àµ¯àµ‘È°áˆºà¢à¬´áˆ»DQGWKDWà¢à¯ converges to 
the KKT point in the case of general constrained optimization problems (Arora, p. 525). 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
156 
NNumerica l Optimizati on Method
7.6.2	
 SQP with Approximate Line Search 
The above SQP algorithm can be used with approximate line search methods, similar to Arjimoâ€™s rule 
(Sec. 7.2.2) as follows: let İà¯Ç¡ İ†àµŒÍ²Ç¡Í³Ç¡ Ç¥ denote a trial step size, à¢à¯à¬¾à¬µÇ¡à¯ denote the trial design point, 
İ‚à¯à¬¾à¬µÇ¡à¯àµŒİ‚áˆºà¢à¯à¬¾à¬µÇ¡à¯áˆ» denote the function value at the trial solution, and È°à¯à¬¾à¬µÇ¡à¯àµŒİ‚à¯à¬¾à¬µÇ¡à¯àµ…Ü´Ü¸à¯à¬¾à¬µÇ¡à¯ 
denote the penalty function at the trial solution. The trial solution is required to satisfy the following 
descent condition: 
È°à¯à¬¾à¬µÇ¡à¯àµ…İà¯ß›à¸®à¢Šà¯à¸®
à¬¶àµ‘È°à¯Ç¡à¯Ç¡
Í² àµß›àµÍ³
(7.54)
where a common choice for Î³ is: ß›àµŒ
à¬µ
à¬¶ Further, İà¯àµŒß¤à¯Ç¡ ß¤àµŒ
à¬µ
à¬¶Ç¡ İ†àµŒÍ²Ç¡Í³Ç¡Í´Ç¡ Ç¥ The above descent 
condition ensures that the constraint violation decreases at each step of the method. The following 
example illustrates the application of approximate line search algorithm.
Example 7.7: Sequential Quadratic Programming with Approximate Line Search
We consider the above NL problem, given as:
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ†İ”à¬µİ”à¬¶àµ…İ”à¬¶
à¬¶
VXEMHFWWRİƒà¬µÇ£Í³ àµ†İ”à¬µ
à¬¶àµ†İ”à¬¶
à¬¶àµ‘Í²Ç¡ İƒà¬¶Ç£àµ†İ”à¬µàµ‘Í²Ç¡ İƒà¬·Ç£àµ†İ”à¬¶àµ‘Í²
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
157 
NNumerica l Optimizati on Method
where the gradient functions are computed as: ×İ‚à¯àµŒáˆ¾Í´İ”à¬µàµ†İ”à¬¶Ç¡ Í´İ”à¬¶àµ†İ”à¬µáˆ¿Ç¡ ×İƒà¬µ
à¯àµŒáˆ¾àµ†Í´İ”à¬µÇ¡ àµ†Í´İ”à¬¶áˆ¿Ç¡
J
×İƒà¬¶
à¯àµŒáˆ¾àµ†Í³Ç¡Í²áˆ¿Ç¡ ×İƒà¬·
à¯àµŒáˆ¾Í²Ç¡ àµ†Í³áˆ¿
Let İ”à¬´àµŒáˆºÍ³Ç¡ Í³áˆ» then, İ‚à¬´àµŒÍ³Ç¡ à¢‰àµŒáˆ¾Í³Ç¡ Í³áˆ¿à¯Ç¡ İƒà¬µáˆºÍ³Ç¡Í³áˆ»àµŒİƒà¬¶áˆºÍ³Ç¡Í³áˆ»àµŒİƒà¬·áˆºÍ³Ç¡Í³áˆ»àµŒàµ†Í³ Since, at this point, 
there are no active constraints, Ü¸à¬´àµŒÍ² the preferred search direction is: à¢ŠàµŒàµ†à¢‰àµŒáˆ¾àµ†Í³Ç¡ àµ†Í³áˆ¿à¯ the line 
search problem is defined as: ÂÂ‹Â
à°ˆÈ°áˆºß™áˆ»àµŒİ‚áˆºà¢à¬´àµ…ß™à¢Šà¬´áˆ»àµŒáˆºÍ³ àµ†ß™áˆ»à¬¶ This problem can be analytically 
solved by setting È°á‡±áˆºß™áˆ»àµŒÍ² with the solution: ß™àµŒÍ³ resulting in İ”à¬µàµŒáˆºÍ²Ç¡ Í²áˆ» however, this analytical 
solution results in a large constraint violation that is undesired.
Use of the approximate line search method for the problem results in the following computations: 
Let İà¬´àµŒÍ³Ç¡ Ü´à¬´àµŒÍ³Í²Ç¡ ß›àµŒß¤àµŒ
à¬µ
à¬¶WKHQà¢à¬µÇ¡à¬´àµŒáˆºÍ²Ç¡Í²áˆ»Ç¡ Ô¡à¢Šà¬´Ô¡à¬¶àµŒÍ´Ç¡ İ‚à¬µÇ¡à¬´àµŒÍ²Ç¡ Ü¸à¬µÇ¡à¬´àµŒÍ³Ç¡ È°à¬µÇ¡à¬´àµŒÍ³Í²Ç¡ and 
the descent condition È°à¬µÇ¡à¬´àµ…
à¬µ
à¬¶Ô¡à¢Šà¬´Ô¡à¬¶àµ‘È°à¬´àµŒÍ³ is not met. We then try İà¬µàµŒ
à¬µ
à¬¶WRREWDLQà¢à¬µÇ¡à¬µàµŒá‰€
à¬µ
à¬¶Ç¡
à¬µ
à¬¶á‰Ç¡  
à¬µÇ¡à¬µàµŒ
à¬µ
à¬¶Ç¡ È°à¬µÇ¡à¬µàµŒÍ·
à¬µ
à¬¸and the descent condition fails again; next, for İà¬¶àµŒ
à¬µ
à¬¸ZHJHWà¢à¬µÇ¡à¬¶àµŒá‰€
à¬·
à¬¸Ç¡
à¬·
à¬¸á‰Ç¡
à¬µÇ¡à¬¶àµŒÍ²Ç¡ È°à¬µÇ¡à¬¶àµŒ
à¬½
à¬µà¬º and the descent condition checks as: È°à¬µÇ¡à¬¶àµ…
à¬µ
à¬¼Ô¡à¢Šà¬´Ô¡à¬¶àµ‘È°à¬´ Therefore, we set: 
ß™àµŒİà¬¶àµŒ
à¬µ
à¬¸Ç¡ à¢à¬µàµŒà¢à¬µÇ¡à¬¶àµŒá‰€
à¬·
à¬¸Ç¡
à¬·
à¬¸á‰with no constraint violation. 
Next, we discuss some modifications to the SQP method that aid in solution to the QP subproblem.
7.6.3	
 The Active Set Strategy 
The computational cost of solving the QP subproblem can be substantially reduced by only including the 
active constraints in the subproblem. Accordingly, if the current design point à¢à¯× È³Ç¡ where Î© denotes 
the feasible region, then, for some small ßàµÍ²Ç¡ the set à££à¯àµŒàµ›İ…Ç£İƒà¯œ
à¯àµàµ†ßÇ¢ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰àµŸÚ‚áˆ¼İ†Ç£İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŒáˆ½ 
denotes the set of potentially active constraints. In the event à¢à¯×‘ È³Ç¡ let the current maximum constraint 
violation be given as: Ü¸à¯àµŒÂÂƒÂšáˆ¼Í²Ç¢İƒà¯œ
à¯Ç¡ İ…àµŒÍ³Ç¡ Ç¤ Ç¤ Ç¤ Ç¡ İ‰Ç¢à¸«İ„à¯
à¯à¸«Ç¡ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŒáˆ½ then, the active constraint set 
includes: à££à¯àµŒàµ›İ…Ç£İƒà¯œ
à¯àµÜ¸à¯àµ†ßÇ¢ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İ‰àµŸÚ‚àµ›İ†Ç£à¸«İ„à¯
à¯à¸«àµÜ¸à¯àµ†ßÇ¢ İ†àµŒÍ³Ç¡ Ç¥ Ç¡ İŒàµŸ 
We may note that an inequality constraint at the current design point can be characterized in the 
following ways: as active LIİƒà¯œ
à¯àµŒÍ² as ßDFWLYHLIİƒà¯œ
à¯àµàµ†ß as violated İ…İ‚İƒà¯œ
à¯àµÍ² or as inactive 
LIİƒà¯œ
à¯àµ‘àµ†ß whereas, an equality constraint is either active İ„à¯
à¯àµŒÍ² or violated İ„à¯
à¯àµÍ² 
The gradients of constraints not in à££à¯ do not need to be computed, however, the numerical algorithm 
using the potential constraint strategy must be proved to be convergent. Further, from a practical point 
of view, it is desirable to normalize all constraints with respect to their limit values, so that a uniform ß 
value can be used to check for a constraint condition at the design point.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
158 
NNumerica l Optimizati on Method
Using the active set strategy, the active inequality constraints being known, they can be treated as equality 
constraints. We, therefore, assume that only equality constraints are present in the active set, and define 
the QP subproblem as:
ÂÂ‹Â
à¢Šİ‚Ò§ àµŒà¢‰à¯à¢Šàµ…Í³
Í´ à¢Šà¯à¢Š
6XEMHFWWRà¡ºà´¥à¯à¢ŠàµŒà¢‹à´¤

(7.55)
Then, using the Lagrangian function approach, the optimality conditions are given as: à¡ºà´¥à¢œàµ…à¢‰àµ…à¢ŠàµŒà«™Ç¡ 
à¡ºà´¥à¯à¢Šàµ†à¢‹à´¤àµŒà«™ They can be simultaneously solved to eliminate the Lagrange multipliers as follows: from 
the optimality conditions we solve for à¢ŠDVà¢ŠàµŒàµ†à¢‰àµ†à¡ºà´¥à¢œÇ¡ and substitute it in the constraint equation 
to get: à¡ºà´¥à¯à¡ºà´¥à¢œàµŒàµ†à¡ºà´¥à¯áˆºà¢‰àµ…à¢Šáˆ» Next, we substitute à¢œ back in the optimality condition to get: 
à¢ŠàµŒàµ†áˆ¾à¡µàµ†à¡ºà´¥áˆºà¡ºà´¥à¯à¡ºà´¥áˆ»à¬¿à¬µà¡ºà´¥à¯áˆ¿à¢‰àµ…à¡ºà´¥áˆºà¡ºà´¥à¯à¡ºà´¥áˆ»à¬¿à¬µà¢‹
(7.56)
or, more compactly as: à¢ŠàµŒà¢Šà¬µàµ…à¢Šà¬¶ where à¢Šà¬µ in the above expression defines a matrix operator: P =
à¡µàµ†à¡ºà´¥áˆºà¡ºà´¥à¯à¡ºà´¥áˆ»à¬¿à¬µà¡ºà´¥à¯Ç¡ à¡¼à¡¼àµŒà¡¼Ç¡ that projects the gradient of the cost function onto the tangent hyperplane 
defined by: áˆ¼à¢ŠÇ£à¡ºà´¥à¯à¢ŠàµŒÍ²áˆ½Ç¡ which can also be obtained as a solution to the following minimization 
problem: ÂÂ‹Â
à¢ŠÔ¡à¢‰àµ†à¢ŠÔ¡à¬¶ subject to à¡ºà´¥à¯à¢ŠàµŒà«™ (Belegundu and Chandrupatla, p. 243). 
The second part of d defines a vector that points toward the feasible region. Further, these two components 
are orthogonal, i.e., à¢Šà¬µ
à¯à¢Šà¬¶àµŒÍ²Ç¤ Thus, we may interpret d as a combination of a cost reduction step à¢Šà¬µ and 
a constraint correction step à¢Šà¬¶Ç¤ Further, if there are no constraint violations, i.e., à¢‹à´¤àµŒà«™Ç¡WKHQà¢Šà¬¶àµŒà«™Ç¡ 
and d aligns with the projected steepest descent direction. 
7.6.4	
 SQP Update via Newtonâ€™s Update
We observe that, from a computational point of view, Newtonâ€™s method can be used to solve the SQP 
subproblem. In order to derive the SQP update via Newtonâ€™s method, we consider the following design 
optimization problem involving equality constraints (Arora, p. 554): 
ÂÂ‹Â
à¢İ‚áˆºà¢áˆ»
6XEMHFWWRİ„à¯œáˆºà¢áˆ»àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ

(7.57)
The Lagrangian function for the problem is constructed as: 
à£¦áˆºà¢Ç¡ à¢œáˆ»àµŒİ‚áˆºà¢áˆ»àµ…à¢œà¯à¢áˆºà¢áˆ»
(7.58)
The KKT conditions for a minimum are given as: 
×à£¦áˆºà¢Ç¡ à¢œáˆ»àµŒ×İ‚áˆºà¢áˆ»àµ…à¡ºà¢œàµŒà«™Ç¡ à¢áˆºà¢áˆ»àµŒà«™
(7.59)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
159 
NNumerica l Optimizati on Method
where à¡ºàµŒ×à¢à¯áˆºà¢áˆ» is the Jacobian matrix whose ith columns represents the gradient ×İ„à¯œ Next, Newtonâ€™s 
method is employed to compute the change in the design variables and Lagrange multipliers as follows: 
using first order Taylor series expansion for ×à£¦à¯à¬¾à¬µDQGà¢à¯à¬¾à¬µ we obtain:
àµ¤×à¬¶à£¦
à¡º
à¡ºà¯
Í²àµ¨
à¯
á‰‚Î¿à¢
Î¿à¢œá‰ƒ
à¯
àµŒàµ†á‰‚×à£¦
à¢á‰ƒ
à¯

(7.60)
The first equation above may be expanded as: ×à¬¶à£¦Î¿à¢à¯àµ…à¡ºàµ«à¢œà¯à¬¾à¬µàµ†à¢œà¯àµ¯àµŒàµ†àµ«×İ‚à¯áˆºà¢áˆ»àµ…à¡ºà¢œà¯àµ¯ and 
simplified as: ×à¬¶à£¦Î¿à¢à¯àµ…à¡ºà¢œà¯à¬¾à¬µàµŒàµ†×İ‚à¯áˆºà¢áˆ» resulting in the following Newton-Raphson iteration:
àµ¤×à¬¶à£¦
à¡º
à¡ºà¯
Í²àµ¨
à¯
àµ¤Î¿à¢à¯
Î¿à¢œà¯à¬¾à¬µàµ¨àµŒàµ†á‰‚×İ‚
à¢á‰ƒ
à¯

(7.61)
It is interesting to note that the above result can also be obtained via a QP problem defined in terms of 
incremental variables where the QP problem is defined as follows:
ÂÂ‹Â
Î¿à¢à«š
à«›Î¿à¢à¯×à¬¶à£¦Î¿à¢àµ…×İ‚à¯Î¿à¢
6XEMHFWWRİ„à¯œáˆºà¢áˆ»àµ…İŠà¯œ
à­˜Î¿à¢àµŒÍ²Ç¡ İ…àµŒÍ³Ç¡ Ç¥ Ç¡ İˆ

(7.62)
The Lagrangian function for the problem is formulated as:
à£¦áˆºÎ¿à¢Ç¡ à¢œáˆ»àµŒÍ³
Í´ Î¿à¢à¯×à¬¶à£¦Î¿à¢àµ…×İ‚à¯Î¿à¢àµ…à¢œà¯áˆºà¢àµ…à¡ºÎ¿à¢áˆ»
(7.63)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
160 
NNumerica l Optimizati on Method
The resulting KKT conditions for an optimum are given as: ×İ‚àµ…×à¬¶à£¦Î¿à¢àµ…à¡ºà¢œàµŒà«™Ç¡ à¢àµ…à¡ºÎ¿à¢àµŒà«™ In 
matrix form, these KKT conditions are similar to those used in the Newton-Raphson update.
7.6.5	
 SQP with Hessian Update 
The above Newtonâ€™s implementation of SQP algorithm uses Hessian of the Lagrangian function for the 
update. Since Hessian computation is relatively costly, an approximate to the Hessian may instead be 
used. Towards that end, let à¡´àµŒ×à¬¶à£¦ then the modified QP subproblem is defined as (Arora, p. 557):
ÂÂ‹Â
à¢Šİ‚Ò§ àµŒà¢‰à¯à¢Šàµ…Í³
Í´ à¢Šà¯à¡´à¢Š
6XEMHFWWRà¡­à¯à¢Šàµ‘à¢ˆÇ¡ à¡ºà¯à¢ŠàµŒà¢‹

(7.64)
We note that quasi-Newton methods (Sec. 7.3.4) solve the unconstrained minimization problem by solving 
a set of linear equations given as: à¡´à¯à¢Šà¯àµŒàµ†à¢‰à¯IRUà¢Šà¯ZKHUHà¡´à¯ where represents an approximation to 
the Hessian matrix. In particular, the popular BFGS method uses the following Hessian update: 
à¡´à¯à¬¾à¬µàµŒà¡´à¯àµ…à¡°à¯àµ…à¡±à¯
(7.65)
where à¡°à¯àµŒ
à¢Ÿà³–à¢Ÿà³–à³…
à¢Ÿà³–à³…à¢™à³–Ç¡ à¡±à¯àµŒ
à¢‰à³–à¢‰à³–à³…
à¢‰à³–à³…à¢Šà³–Ç¡ à¢™à¯àµŒß™à¯à¢Šà¯Ç¡ à¢Ÿà¯àµŒà¢‰à¯à¬¾à¬µàµ†à¢‰à¯Ç¡ à¢‰à¯àµŒ×İ‚àµ«à¢à¯àµ¯ 
Next, the BFGS Hessian update is modified to apply to the constrained optimization problems as 
follows: let à¢™à¯àµŒß™à¯à¢Šà¯Ç¡ à¢ à¯àµŒà¡´à¯à¢™à¯Ç¡ à¢Ÿà¯àµŒ×à£¦àµ«à¢à¯à¬¾à¬µàµ¯àµ†×à£¦àµ«à¢à¯àµ¯Ç¡ à¢™à¯à¯à¢Ÿà¯àµŒß¦à¬µÇ¡ à¢™à¯à¯à¢ à¯àµŒß¦à¬¶ further, 
define: à¢à¯àµŒß à¢Ÿà¯àµ…áˆºÍ³ àµ†ß áˆ»à¢ à¯ where ß àµŒÂÂ‹Â á‰„Í³Ç¡
à¬´Ç¤à¬¼à°•à°®
à°•à°®à¬¿à°•à°­á‰…à¢™à¯à¯à¢à¯àµŒß¦à¬·Ç¢ then, the Hessian update is given 
as: à¡´à¯à¬¾à¬µàµŒà¡´à¯àµ…à¡°à¯àµ†à¡±à¯Ç¡ à¡°à¯àµŒ
à¬µ
à°•à°¯à¢Ÿà¯à¢Ÿà¯à¯Ç¡ à¡±à¯àµŒ
à¬µ
à°•à°®à¢ à¯à¢ à¯à¯
The modified SQP algorithm is given as follows:
Modified SQP Algorithm (Arora, p. 558): 
Initialize: choose à¢à¬´Ç¡ Ü´à¬´àµŒÍ³Ç¡ à¡´à¬´àµŒÜ«Ç¢ßà¬µÇ¡ ßà¬¶àµÍ²
For İ‡àµŒÍ²Ç¡Í³Ç¡Í´Ç¡ Ç¥
1.	 Compute İ‚à¯Ç¡ İƒà¯œ
à¯Ç¡ İ„à¯
à¯Ç¡ à¢‰Ç¡ Ü¾à¯œÇ¡ İà¯Ç¡DQGÜ¸à¯,Iİ‡àµÍ²Ç¡ compute Hk
2.	 Formulate and solve the modified QP subproblem for search direction dk and the Lagrange 
multipliers à¢›à¯DQGà¢œà¯
3.	 If Ü¸à¯àµ‘ßà¬µDQGà¸®à¢Šà¯à¸®àµ‘ßà¬¶ stop.
4.	 Compute Ü´ formulate and solve line search subproblem to obtain Î±
5.	 Set à¢à¯à¬¾à¬µÕš à¢à¯àµ…ß™à¢Šà¯Ç¡ Ü´à¯à¬¾à¬µÕš Ü´Ç¡İ‡Õš İ‡àµ…Í³
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
161 
NNumerica l Optimizati on Method
An example for SQP with Hessian update is presented below.
Example 7.8: SQP with Hessian Update
As an example, we consider the above NL problem, given as: 
ÂÂ‹Â
à¯«à°­Ç¡à¯«à°®İ‚áˆºİ”à¬µÇ¡ İ”à¬¶áˆ»àµŒİ”à¬µ
à¬¶àµ†İ”à¬µİ”à¬¶àµ…İ”à¬¶
à¬¶
VXEMHFWWRİƒà¬µÇ£ Í³ àµ†İ”à¬µ
à¬¶àµ†İ”à¬¶
à¬¶àµ‘Í²Ç¢İƒà¬¶Ç£ àµ†İ”à¬µàµ‘Í²Ç¡ İƒà¬·Ç£ àµ†İ”à¬¶àµ‘Í²
The objective and constraint gradients for the problem are obtained as: 
×İ‚à¯àµŒáˆ¾Í´İ”à¬µàµ†İ”à¬¶Ç¡ Í´İ”à¬¶àµ†İ”à¬µáˆ¿Ç¡ ×İƒà¬µ
à¯àµŒáˆ¾àµ†Í´İ”à¬µÇ¡ àµ†Í´İ”à¬¶áˆ¿Ç¡ ×İƒà¬¶
à¯àµŒáˆ¾àµ†Í³Ç¡Í²áˆ¿Ç¡ ×İƒà¬·
à¯àµŒáˆ¾Í²Ç¡ àµ†Í³áˆ¿
To proceed, OHWİ”à¬´àµŒáˆºÍ³Ç¡Í³áˆ»Ç¡ so that, İ‚à¬´àµŒÍ³Ç¡ İƒà¬µáˆºÍ³Ç¡Í³áˆ»àµŒİƒà¬¶áˆºÍ³Ç¡Í³áˆ»àµŒİƒà¬·áˆºÍ³Ç¡Í³áˆ»àµŒàµ†Í³ since all constraints 
are initially inactive, the preferred search direction is: à¢ŠàµŒàµ†à¢‰àµŒáˆ¾àµ†Í³Ç¡ àµ†Í³áˆ¿à¯Ç¢ then, using approximate 
line search we obtain: ß™àµŒ
à¬µ
à¬¸Ç¡OHDGLQJWRà¢à¬µàµŒá‰€
à¬·
à¬¸Ç¡
à¬·
à¬¸á‰
For the Hessian update, we have: İ‚à¬µàµŒÍ²Ç¤Í·Í¸Í´Í·Ç¡ İƒà¬µàµŒàµ†Í²Ç¤Í³Í´Í·Ç¡ İƒà¬¶àµŒİƒà¬·àµŒàµ†Í²Ç¤Í¹Í·Ç¢à¢‰à¬µàµŒáˆ¾Í²Ç¤Í¹Í·Ç¡ Í²Ç¤Í¹Í·áˆ¿ and, for 
ß™àµŒÍ²Ç¤Í´Í·Ç¡à¢™à¬´àµŒáˆ¾àµ†Í²Ç¤Í´Í·Ç¡ àµ†Í²Ç¤Í´Í·áˆ¿àµŒà¢ à¬´àµŒà¢Ÿà¬´Ç¡ ß¦à¬µàµŒß¦à¬¶àµŒÍ²Ç¤Í³Í´Í·Ç¡ ß àµŒÍ³Ç¡ à¢à¬´àµŒà¢Ÿà¬´Ç¡ ß¦à¬·àµŒß¦à¬µ therefore, 
Hessian update is computed as: à¡°à¬´àµŒÍº á‰‚Í³
Í³
Í³
Í³á‰ƒÇ¡ à¡±à¬´àµŒÍº á‰‚Í³
Í³
Í³
Í³á‰ƒÇ¡ à¡´à¬µàµŒà¡´à¬´Ç¤
For the next step, the QP problem is defined as:
ÂÂ‹Â
à¯—à°­Ç¡à¯—à°®İ‚Ò§ àµŒ
à¬·
à¬¸áˆºİ€à¬µàµ…İ€à¬¶áˆ»àµ…
à¬µ
à¬¶áˆºİ€à¬µ
à¬¶àµ…İ€à¬¶
à¬¶áˆ»
6XEMHFWWRàµ†
à¬·
à¬¶áˆºİ€à¬µàµ…İ€à¬¶áˆ»àµ‘Í²Ç¡ àµ†İ€à¬µàµ‘Í²Ç¡ àµ†İ€à¬¶àµ‘Í²
Using a Lagrangian function approach, the solution is found from application of KKT conditions, which 
results in the following systems of equations: à¡¼à¢àµŒà¢—Ç¡ZKHUHà¢à¯àµŒáˆ¾İ€à¬µÇ¡ İ€à¬¶Ç¡ İ‘à¬µÇ¡ İ‘à¬¶Ç¡ İ‘à¬·Ç¡ İà¬µÇ¡ İà¬¶Ç¡ İà¬·áˆ¿ and, 
à¡¼àµŒ
Û
Û
Û
Û
Û

Í³
Í²
Í²
Í³
àµ†Í³Ç¤Í·
àµ†Í³Ç¤Í·
àµ†Í³
Í²
Í²
àµ†Í³

àµ†Í³Ç¤Í·
àµ†Í³
Í²
àµ†Í³Ç¤Í·
Í²
àµ†Í³
Í²
Í²
Í²
Í²
Í²
Í²
Í²
Í²
Í²

Í²
Í²
Í²
Í²
Í²
Í²
Í³
Í²
Í²
Í²
Í³
Í²
Í²
Í²
Í³Û’
Û‘
Û‘
Û‘
Û
Ç¡à¢—àµŒ
Û
Û
Û
Û
Ûàµ†Í²Ç¤Í¹Í·
àµ†Í²Ç¤Í¹Í·
Í²Ç¤Í³Í´Í·
Í²Ç¤Í¹Í·
Í²Ç¤Í¹Í· Û’
Û‘
Û‘
Û‘
Û

The complementary slackness conditions are given as: İ‘à¯œİà¯œàµŒÍ²İ…àµŒÍ³Ç¡Í´Ç¡Íµ The solution found from the 
simplex method is given as: à¢à¯àµŒáˆ¾Í²Ç¤Í³ÍºÍºÇ¡ Í²Ç¤Í³ÍºÍºÇ¡ Í²Ç¡ Í²Ç¡ Í²Ç¡Í²Ç¤Í³Í´Í·Ç¡ Í²Ç¤Í¹Í·Ç¡ Í²Ç¤Í¹Í·áˆ¿ We note that in this case as 
the number of variables is small, taking the complementarity conditions into account, there are eight basic 
solutions, only one of which is feasible and is given as: à¢„à¯àµŒáˆ¾Í²Ç¤Í³ÍºÍºÇ¡ Í²Ç¤Í³ÍºÍºÇ¡ Í²Ç¡ Í²Ç¡ Í²Ç¡Í²Ç¤Í³Í´Í·Ç¡ Í²Ç¤Í¹Í·Ç¡ Í²Ç¤Í¹Í·áˆ¿
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
162 
References
References
Arora, JS 2004, Introduction to Optimum Design, 2nd edn, Elsevier Academic Press, San Diego, CA.
Belegundu, AD and Chandrupatla TR 2012, Optimization Concepts and Applications in Engineering, 2nd 
edn (reprinted), Cambridge University Press, New York.
Boyd, S & Vandenberghe, L 2004, Convex Optimization, Cambridge University Press, New York.
Chong, EKP & Zak, SH 2013, An Introduction to Optimization, 4th edn. John Wiley & Sons, New Jersey.
Eisenbrand, F, Course notes for linear and discrete optimization, 
https://class.coursera.org/linearopt-001/lecture/index
Ferris, MC, Mangasarian, OL & Wright, SJ 2007, Linear Programming with Matlab, SIAM, Philadelphia, PA
Ganguli, R 2012, Engineering Optimiztion A Modern Approach, Universities Press, Hyderabad (India).
Griva, I, Nash, SG & Sofer, A 2009, Linear and Nonlinear Optimization, 2nd edn, SIAM, Philadelphia, PA.
Hager, WW & Zhang, H-C 2006, â€˜A survey of nonlinear conjugate gradient methodsâ€™, Pacific Journal of 
Optimization, vol. 2, pp. 35â€“58.
Hemmecke, R, Lecture notes on discrete optimization, 
https://www-m9.ma.tum.de/foswiki/pub/SS2011/DiscOpt/DiscOpt.pdf
Kelly, CT 1995, Iterative Methods for Linear and Nonlinear Equations, SIAM, Philadelphia, PA.
Luenberger, DG &Ye, Y 2008, Linear and Nonlinear Programming, 3rd edn, Springer, New York.
Pedregal, P 2004, Introduction to Optimization, Springer-Verlag, New York.
Sierksma, G 2002, Linear and Integer Programming: Theory and Practice, 2nd edn, Marcel Dekker, 
Monticello, NY.
Vanderbei, RJ 2007, Linear Programming: Foundations and Extensions, 3rd edn, Springer, New York.
Yang, X-S 2010, Engineering Optimization, John Wiley & Sons, New Jersey.
Download free eBooks at bookboon.com


Kamran Iqbal
Fundamental Engineering
Optimization Methods
Download free books at

2 
 
Kamran Iqbal
Fundamental Engineering Optimization 
Methods
Download free eBooks at bookboon.com

3 
 
Fundamental Engineering Optimization Methods
1st edition
© 2013 Kamran Iqbal & bookboon.com
ISBN 978-87-403-0489-3
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
4 
Contents
Contents
	
Preface	
8
1	
Engineering Design Optimization	
10
1.1	
Introduction 	
10
1.2	
Optimization Examples in Science and Engineering	
11
1.3	
Notation	
18
2	
Mathematical Preliminaries	
19
2.1	
Set Definitions 	
19
2.2	
Function Definitions	
20
2.3	
Taylor Series Approximation	
21
2.4	
Gradient Vector and Hessian Matrix	
23
2.5	
Convex Optimization Problems	
24
2.6	
Vector and Matrix Norms	
26
2.7	
Matrix Eigenvalues and Singular Values	
26
2.8	
Quadratic Function Forms	
27
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Fundamental Engineering Optimization 
Methods
5 
Contents
2.9	
Linear Systems of Equations	
28
2.10	
Linear Diophantine System of Equations	
30
2.11	
Condition Number and Convergence Rates	
30
2.12	
 Conjugate-Gradient Method for Linear Equations	
32
2.13	
Newton’s Method for Nonlinear Equations	
33
3	
Graphical Optimization	
34
3.1	
Functional Minimization in One-Dimension 	
35
3.2	
Graphical Optimization in Two-Dimensions	
36
4	
Mathematical Optimization	
43
4.1	
The Optimization Problem	
44
4.2	
Optimality criteria for the Unconstrained Problems	
45
4.3	
Optimality Criteria for the Constrained Problems	
48
4.4	
Optimality Criteria for General Optimization Problems	
54
4.5	
Postoptimality Analysis	
59
4.6	
Lagrangian Duality 	
60
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Fundamental Engineering Optimization 
Methods
6 
Contents
5	
Linear Programming Methods	
68
5.1	
The Standard LP Problem	
68
5.2	
The Basic Solution to the LP Problem	
70
5.3	
The Simplex Method	
72
5.4	
Postoptimality Analysis	
84
5.5	
Duality Theory for the LP Problems	
89
5.6	
Non-Simplex Methods for Solving LP Problems	
97
5.7	
Optimality Conditions for LP Problems	
101
5.8	
The Quadratic Programming Problem	
104
5.9	
The Linear Complementary Problem	
108
6	
Discrete Optimization 	
113
6.1	
Discrete Optimization Problems	
113
6.2	
Solution Approaches to Discrete Problems	
114
6.3	
Linear Programming Problems with Integral Coefficients	
115
6.5	
Integer Programming Problems	
119
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Fundamental Engineering Optimization 
Methods
7 
Contents
7	
Numerical Optimization Methods	
126
7.1	
The Iterative Method	
127
7.2	
Computer Methods for Solving the Line Search Problem	
128
7.3	
Computer Methods for Finding the Search Direction	
134
7.4	
Computer Methods for Solving the Constrained Problems	
146
7.5	
Sequential Linear Programming	
151
7.6	
Sequential Quadratic Programming 	
153
	
References	
162
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Fundamental Engineering Optimization 
Methods
8 
Preface
Preface
This book is addressed to students in fields of engineering and technology as well as practicing engineers. 
It covers the fundamentals of commonly used optimization methods used in engineering design. 
Optimization methods fall among the mathematical tools typically used to solve engineering problems. 
It is therefore desirable that graduating students and practicing engineers are equipped with these tools 
and are trained to apply them to specific problems encountered in engineering practice. 
Optimization is an integral part of the engineering design process. It focuses on discovering optimum 
solutions to a design problem through systematic consideration of alternatives, while satisfying resource 
and cost constraints. Many engineering problems are open-ended and complex. The overall design 
objective in these problems may be to minimize cost, to maximize profit, to streamline production, to 
increase process efficiency, etc. Finding an optimum solution requires a careful consideration of several 
alternatives that are often compared on multiple criteria. 
Mathematically, the engineering design optimization problem is formulated by identifying a cost function 
of several optimization variables whose optimal combination results in the minimal cost. The resource 
and other constraints are similarly translated into mathematical relations. Once the cost function and 
the constraints have been correctly formulated, analytical, computational, or graphical methods may 
be employed to find an optimum. The challenge in complex optimization problems is finding a global 
minimum, which may be elusive due to the complexity and nonlinearity of the problem.
This book covers the fundamentals of optimization methods for solving engineering problems. Written 
by an engineer, it introduces fundamentals of mathematical optimization methods in a manner that 
engineers can easily understand. The treatment of the topics presented here is both selective and concise. 
The material is presented roughly at senior undergraduate level. Readers are expected to have familiarity 
with linear algebra and multivariable calculus. Background material has been reviewed in Chapter 2.
The methods covered in this book include: a) analytical methods that are based on calculus of variations; 
b) graphical methods that are useful when minimizing functions involving a small number of variables; 
and c) iterative methods that are computer friendly, yet require a good understanding of the problem. 
Both linear and nonlinear methods are covered. Where necessary, engineering examples have been used 
to build an understanding of how these methods can be applied. Though not written as text, it may be 
used as text if supplemented by additional reading and exercise problems from the references.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
9 
Preface
There are many good references available on the topic of optimization methods. A short list of prominent 
books and internet resources appears in the reference section. The following references are main sources 
for this manuscript and the topics covered therein: Arora (2012); Belegundu and Chandrupatla (2012); 
Chong and Zak (2013); and, Griva, Nash & Sofer (2009). In addition, lecture notes of eminent professors 
who have regularly taught optimization classes are available on the internet. For details, the interested 
reader may refer to these references or other web resources on the topic. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Fundamental Engineering Optimization 
Methods
10 
Engineering  Desig n Optimizatio
1	 Engineering Design 
Optimization
This chapter introduces the topic of optimization through example problems that have been selected 
from various fields including mathematics, economics, computer science, and engineering.
Learning Objectives: The learning goal in this chapter is to develop an appreciation for the topic as 
well as the diversity and usefulness of the mathematical and computational optimization techniques.
1.1	
Introduction 
Engineering system design comprises selecting one or more variables to meet a set of objectives. A better 
design is obtained if an appropriate cost function can be reduced. The design is optimum when the cost 
is the lowest among all feasible designs. Almost always, the design choices are limited due to resource 
constraints, such as material and labor constraints, as well as physical and other restrictions. A feasible 
region in the design space is circumscribed by the constraint boundaries. More importantly, both the 
cost function and the constraints can be cast as mathematical functions involving design variables. The 
resulting mathematical optimization problem can then be solved using methods discussed in this book.
Engineering system design is an interdisciplinary process that necessitates cooperation among designers 
from various engineering fields. Engineering design can be a complex process. It requires assumptions to 
be made to develop models that can be subjected to analysis and verification by experiments. The design 
of a system begins by analyzing various options. For most applications the entire design project must be 
broken down into several subproblems which are then treated independently. Each of the subproblems 
can be posed as an optimum design problem to be solved via mathematical optimization.
A typical optimum engineering design problem may include the following steps: a descriptive problem 
statement, preliminary investigation and data collection as a prelude to problem formulation, identification 
of design variables, optimization criteria and constraints, mathematical formulation of the optimization 
problem, and finding a solution to the problem. This text discusses the last two steps in the design process, 
namely mathematical formulation and methods to solve the design optimization problem.
Engineering design optimization is an open-ended problem. Perhaps the most important step toward 
solving the problem involves correct mathematical formulation of the problem. Once the problem has 
been mathematically formulated, analytical and computer methods are available to find a solution. 
Numerical techniques to solve the mathematical optimization problems are collectively referred as 
mathematical programming framework. The framework provides a general and flexible formulation for 
solving engineering design problems. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
11 
Engineering  Desig n Optimizatio
Some mathematical optimization problems may not have a solution. This usually happens due to 
conflicting requirements of incorrect formulation of the optimization problem. For example, constraints 
may be restrictive so that no feasible region can be found, or the feasible region may be unbounded due 
to a missing constraint. In this text we will assume that the problem has been correctly formulated so 
that the feasible region is closed and bounded.
1.2	
Optimization Examples in Science and Engineering
We wish to introduce the topic of optimization with the help of examples. These examples have been 
selected from various STEM (science, technology, engineering, mathematics) fields. Each example 
requires finding the optimal values of a set of design variables in order to optimize (maximize or minimize) 
a generalized cost that may represent the manufacturing cost, profit, energy, power, distance, mean square 
error, and so on. The complexity of the design problem grows with number of variables involved. Each 
of the simpler problems, presented first, involves a limited number of design variables. The problems 
that follow are more complex in nature and may involve hundreds of design variables. Mathematical 
formulation of each problem is provided following the problem definition. While the simpler problems 
are relatively easy to solve by hand, the complex problems require the use of specialized optimization 
software in order to find a solution. 
Problem 1: Student diet problem
A student has a choice of breakfast menu (eggs, cereals, tarts) and a limited ($10) budget to 
fulfill his/her nutrition needs (1000 calories, 100 g protein) at minimum cost. Eggs provide 
500 calories and 50g protein and cost $3.50; cereals provide 500 calories and 40g protein and 
cost $3; tarts provide 600 calories and 20g protein and cost $2. How does he/she choose his/
her breakfast mix?
Formulation: Let ݔ்ൌሾݔଵǡ ݔଶǡ ݔଷሿ represent the quantities of eggs, cereals and tarts chosen 
for breakfast. Then, the optimization problem is mathematically formulated as follows:

௫భǡ௫మǡ௫య݂ൌ͵Ǥͷݔଵ൅͵ݔଶ൅ʹݔଷ
6XEMHFWWRͷͲͲሺݔଵ൅ݔଶሻ൅͸ͲͲݔଷ൒ͳͲͲͲǡ ͷͲݔଵ൅ͶͲݔଶ൅ʹͲݔଷ൒ͳͲͲǡ


͵Ǥͷݔଵ൅͵ݔଶ൅ʹݔଷ൑ͳͲǢݔଵǡ ݔଶǡ ݔଷא Ժ

(1.1)
Problem 2: Simplified manufacturing problem
A manufacturer produces two products: tables and chairs. Each table requires 10 kg of material 
and 5 units of labor, and earns $7.50 in profit. Each chair requires 5 kg of material and 12 units 
of labor, and earns $5 in profit. A total of 60 kg of material and 80 units of labor are available. 
Find the best production mix to earn maximum profit.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
12 
Engineering  Desig n Optimizatio
Formulation: Let /HWݔ்ൌሾݔଵǡ ݔଶሿ represent the quantities of tables and chairs to be 
manufactured. Then, the optimization problem is mathematically formulated as follows:

௫భǡ௫మ݂ൌ͹Ǥͷݔଵ൅ͷݔଶ
6XEMHFWWRͳͲݔଵ൅ͷݔଶ൑͸Ͳǡ ͷݔଵ൅ͳʹݔଶ൑ͺͲǢ ݔଵǡ ݔଶא Ժ
(1.2)
Problem 3: Shortest distance problem
Find the shortest distance from a given point ሺݔ଴ǡ ݕ଴ሻ to a given curve: ݕൌ݂ሺݔሻ
Formulation: The optimization problem is mathematically formulated to minimize the 
Euclidian distance from the given point to the curve:

௫ǡ௬݂ൌଵ
ଶሼሺݔെݔ଴ሻଶ൅ሺݕെݕ଴ሻଶሽ
6XEMHFWWRݕൌ݂ሺݔሻ

(1.3)
Problem 4: Data-fitting problem
Given a set of ܰ data points ሺݔ௜ǡ ݕ௜ሻǡ ݅ൌͳǡ ǥ ǡ ܰ fit a polynomial of degree to the data such 
that the mean square error σ
൫ݕ௜െ݂ሺݔ௜ሻ൯
ଶ
ே
௜ୀଵ
 is minimized.
Formulation: Let the polynomial be given as: ݕൌ݂ሺݔሻൌܽ଴൅ܽଵݔ൅ڮ ൅ܽ௠ݔ௠Ǣ then, the 
unconstrained optimization problem is formulated as:

௔బǡ௔భ݂ൌଵ
ଶ෍
ሺݕ௜െܽ଴െܽଵݔ௜െڮ െܽ௠ݔ௜
௠ሻଶ
ே
௜ୀଵ

(1.4)
Problem 5: Soda can design problem
Design a soda can (choose diameter d and height h) to hold a volume of 200 ml, such that the 
manufacturing cost (a function of surface area) is minimized and the constraint ݄൒ʹ݀ is obeyed.
Formulation: Let ்࢞ൌሾ݀ǡ ݈ሿ represent the diameter and length of the can. Then, the 
optimization problem is formulated to minimize the surface area of the can as:

ௗǡ௟݂ൌଵ
ସߨ݀ଶ൅ߨ݈݀
6XEMHFWWRభ
రߨ݀ଶ݈ൌʹͲͲǡ ʹ݀െ݄൑Ͳ
(1.5)
Problem 6: Open box problem
What is the largest volume for an open box that can be constructed from a given sheet of paper 
(8.5″×11″) by cutting out squares at the corners and folding the sides?
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
13 
Engineering  Desig n Optimizatio
Formulation: Let x represent the side of the squares to be cut; then, the unconstrained 
optimization problem is formulated as:

௫݂ൌݔሺͺǤͷ െʹݔሻሺͳͳ െʹݔሻ
(1.6)
Problem 7: Ladder placement problem
What are the dimensions (width, height) of the largest box that can be placed under a ladder 
of length l when the ladder rests against a vertical wall?
Formulation: Let [x, y] represent the dimensions of the box, and let ሺܽǡ Ͳሻ and ሺͲǡ ܾሻ represent 
the horizontal and vertical contact points of the ladder with the floor and the wall, respectively. 
Then, the optimization problem is mathematically formulated as:

௫ǡ௬݂ൌݔݕ
6XEMHFWWR
௫
௔൅
௬
௕൑ͳǡ ܽଶ൅ܾଶൌ݈
(1.7)
Problem 8: Logging problem
What are the dimensions of a rectangular beam of maximum dimensions (or volume) that can 
be cut from a log of given dimensions?
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Fundamental Engineering Optimization 
Methods
14 
Engineering  Desig n Optimizatio
Formulation: Let [x, y] represent the width and height of the beam to be cut, and let d represent 
the diameter of the log. Then, the optimization problem is formulated as:
max f = xy
  xy
Subject to: x2 + y2 – d2 ≤ 0

(1.8)
Problem 9: Knapsack problem
Given an assortment of n items, where each item i has a value ܿ௜൐Ͳ and a weight ݓ௜൐Ͳ 
fill a knapsack of given capacity (weight W) so as to maximize the value of the included items. 
Formulation: Without loss of generality, we assume that ܹൌͳ/HWݔ௜א ሼͲǡͳሽ . Let denote the 
event that item i is selected for inclusion in the sack; then, the knapsack problem is formulated as:

௫೔݂ൌ෍
ܿ௜ݔ௜
௡
௜ୀଵ

ǣ෍
ݓ௜ݔ௜
௡
௜ୀଵ
൑ͳ

(1.9)
Problem 10: Investment problem
Given the stock prices ݌௜ and anticipated rates of return ݎ௜ associated with a group of investments, 
choose a mix of securities to invest a sum of $1M in order to maximize return on investment.
Formulation: /HWݔ௜א ሼͲǡͳሽ express the inclusion of security i in the mix, then the investment 
problem is modeled as the knapsack problem (Problem 9).
Problem 11: Set covering problem
Given a set ܵൌሼ݁௜ǣ ݅ൌͳǡ ǥ ǡ ݉ሽ and a collection ࣭ൌ൛ܵ௝ǣ݆ൌͳǡ ǥ ǡ ݊ൟ of subsets of ܵ with 
associated costs ܿ௝ǡ find the smallest sub-collection ȭ of ࣭ that covers i.e., ܵǡLHǡ ڂ
ܵ௝ൌܵ
ௌೕאஊ

Formulation: /HWܽ௜௝א ሼͲǡͳሽ denote the condition that ݁௜א ܵ௝ǡDQGOHWݔ௝א ሼͲǡͳሽ and let 
denote the condition that ܵ௝א ȭǢ then, the set covering problem is formulated as:

௫ೕ݂ൌ෍
ܿ௝ݔ௝
௡
௝ୀଵ

ǣ෍
ܽ௜௝ݔ௜൒ͳ
௡
௝ୀଵ
ǡ ݅ൌͳǡ ǥ ǡ ݉Ǣ ݔ௝א ሼͲǡͳሽǡ ݆ൌͳǡ ǥ ǡ ݊
(1.10)
Problem 12: Airline scheduling problem
Given the fixed costs and operating costs per segment, design an optimum flight schedule to 
minimize total operating cost for given passenger demand on each segment over a network 
of routes to be serviced under given connectivity, compatibility, and resource (aircrafts, 
manpower) availability constraints.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
15 
Engineering  Desig n Optimizatio
Formulation: Let ܵൌሼ݁௜ǣ ݅ൌͳǡ ǥ ǡ ݉ሽ denote the set of flight segments required to be covered, 
and let each subset ܵ௝ك ܵ denote a set of connected flight segments that can be covered by an 
aircraft or a crew; then the least cost problem to cover the available routes can be formulated 
as a set covering problem (Problem 10).
Problem 13: Shortest path problem
Find the shortest path from node p to node q in a connected graph (V, E), where V denotes the 
vertices and denotes the edges.
Formulation: Let ݁௜௝ denote the edge incident to both nodes ݅ and ݆ǡ and let ݂ǣ ܧ՜ Թ 
represent a real-valued weight function; further, let ܲൌሺݒଵǡ ݒଶǡ ǥ ǡ ݒ௡ሻ denote a path, where 
ݒଵൌ݌ǡ ݒ௡ൌݍǢ then, the unconstrained single-pair shortest path problem is formulated as:

௡݂ൌ෍
݁௜ǡ௜ାଵ
௡ିଵ
௜ୀଵ

(1.11)
Alternatively, let ݔ௜௝ denote a variable associated with ݁௜௝Ǣ then, an integer programming 
formulation (Chapter 6) of the shortest path problem is given as:

௫೔ೕ݂ൌ෍݁௜௝ݔ௜௝
௜ǡ௝

ǣ෍ݔ௜௝
௝
െ෍ݔ௝௜
௝
ൌ൝
ͳ݅ൌ݌
െͳ݅ൌݍ
Ͳ


(1.12)
Note: the shortest path problem is a well-known problem in graph theory and algorithms, such 
as Dijkstra’s algorithm or Bellman-Ford algorithm, are available to solve variants of the problem.
Problem 14: Traveling salesman problem
A company requires a salesman to visit its N stores (say 50 stores) that are geographically 
distributed in different locations. Find the visiting sequence that will require the least amount 
of overall travel. 
Formulation: The traveling salesman problem is formulated as shortest path problem in an 
undirected weighted graph where the stores represent the vertices of the graph. The problem 
is then similar to Problem 10.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
16 
Engineering  Desig n Optimizatio
Problem 15: Transportation problem
Goods are to be shipped from m supply points with capacities: ݏଵǡ ݏଶǡ ǥ ǡ ݏ௠ to distribution 
points with demands: ݀ଵǡ ݀ଶǡ ǥ ǡ ݀௡ Given the transportation cost ܿ௜௝ for each of the network 
routes, find the optimum quantities, ݔ௜௝ǡ to be shipped along those routes to minimize total 
shipment cost.
Formulation: let ݔ௜௝ǡ denote the quantity to be shipped node i to node j; then, the optimization 
problem is formulated as:

௫೔ೕ݂ൌ෍ܿ௜௝ݔ௜௝
௜ǡ௝

ǣ෍ݔ௜௝
௝
ൌݏ௜ǡ݅ൌͳǡ ǥ ǡ ݉Ǣ෍ݔ௜௝
௜
ൌ݀௝ǡ݅ൌͳǡ ǥ ǡ ݊Ǣ ݔ௜௝൒Ͳ

(1.13)
Problem 16: Power grid estimation problem 
Given the measurements of active and reactive power flows ൫݌௜௝ǡ ݍ௜௝൯ between nodes i, j and the 
measurements ݒ௜ of the node voltages in an electric grid, obtain the best estimate of the state of 
the grid, i.e., solve for complex node voltages: ௜ൌݒ௜סߜ௜ǡ where ߜ௜ represents the phase angle. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
17 
Engineering  Desig n Optimizatio
Formulation: let ݒҧ௜ǡ ݌ҧ௜௝ǡ ݍത௜௝ represent the measured variables, and ݇௜
௩ǡ ݇௜௝
௣ǡ ݇௜௝
௤ǡ let respectively, 
represent the confidence in measurements of the node voltages and the power flows; further 
let ௜௝ൌݖ௜௝סߠ௜௝ represent the complex impedance between nodes ݅ǡ ݆Ǣ then, power grid state 
estimation problem is formulated as (Pedregal, p. 11):


௩೔ǡఋ೔݂ൌ෍݇௜
௩ሺݒ௜െݒҧ௜ሻଶ
௜
൅෍݇௜௝
௣൫݌௜௝െ݌ҧ௜௝൯
ଶ
௜ǡ௝
൅෍݇௜௝
௤൫ݍ௜௝െݍത௜௝൯
ଶ
௜ǡ௝

6XEMHFWWR൞
݌௜௝ൌ
௩೔
మ
௭೔ೕ ߠ௜௝െ
௩೔௩ೕ
௭೔ೕ൫ߠ௜௝൅ߜ௜െߜ௝൯
ݍ௜௝ൌ
௩೔
మ
௭೔ೕ ߠ௜௝െ
௩೔௩ೕ
௭೔ೕሺߠ௜௝൅ߜ௜െߜ௝ሻ


(1.14)
Problem 17 Classification problem
Given a set of data points: ࢞௜א Թ௡ǡ ݅ൌͳǡ ǥ ǡ ݊ǡ with two classification labels: ݕ௜א ሼͳǡ െͳሽ 
find the equation of a hyperplane separating data into classes with maximum inter-class distance.
Formulation: To simplify the problem, we assume that data points lie in a plane, i.e., ࢞௜א Թଶǡ 
and that they are linearly separable. We consider a hyperplane of the form: ்࢝࢞െܾൌͲǡ 
where w is a weight vector that is normal to the hyperplane. For separating given data points, 
we assume that ்࢝࢞௜െܾ൒ͳ for points labeled as 1, and ்࢝࢞௜െܾ൑െͳ for points labeled 
as –1. The two hyperplanes (lines) are separated by ଶ
ԡ࢝ԡ Thus, optimization problem is defined as:

࢝ଵ
ଶԡ࢝ԡଶ
6XEMHFWWRͳ െݕ௜ሺ்࢝࢞௜െܾሻ൑ͲǢ ݅ൌͳǡ ǥ ǡ ݊

(1.15)
Problem 18: Steady-state finite element analysis problem 
Find nodal displacements ݑ௜ that minimize the total potential energy associated with a set of 
point masses ݉௜ connected via springs of constants ݇௜௝ǡ while obeying structural and load 
constraints. 
Formulation: For simplicity we consider a one-dimensional version of the problem, where 
the nodal displacements are represented as: ݑଵǡ ݑଶǡ ǥ ǡ ݑேǤ Let ݂௜ represent an applied force at 
node ݅Ǣ then, the potential energy minimization problem is formulated as:

௨೔ς ൌଵ
ଶ෍݇௜௝ݑ௜ݑ௝
௜ǡ௝
൅෍ݑ௜݂௜
௜

(1.16)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
18 
Engineering  Desig n Optimizatio
Problem 19: Optimal control problem
Find an admissible control sequence ݑሺݐሻthat minimizes a quadratic cost function ܬሺݔǡ ݑǡ ݐሻǡ
while moving a dynamic system: ݔሶൌܣݔ൅ܤݑ between prescribed end points. The class of 
optimal control problems includes minimum energy and minimum time problems, among 
others.
Formulation: As a simplified problem, we consider the optimal control of an inertial system 
of unit mass modeled with position ݔሻ and velocity ݒ The system dynamics are given as: 
ݔሶൌݒǡ ݒሶൌݑǡ where ݑሺݐሻǡ ݐאሾͲǡ ܶሿ represents the input. We consider a quadratic cost that 
includes time integral of square of position and input variables. The resulting optimal control 
problem is formulated as:

௫೔݂ൌන
ଵ
ଶሺݔଶ൅ߩݑଶሻ݀ݐ
்
଴

6XEMHFWWRݔሶൌݒǡ ݒሶൌݑ

(1.17)
1.3	
Notation
The following notation is used throughout this book: Թ denotes the set of real numbers; Թ௡ denotes 
the set of real n-vectors; Թ௠ൈ௡ denotes the set of real ݉ൈ݊ matrices; ݂ǣ Թ௡՜ Թ௠ denotes an Թ௠ 
valued function defined over ; Թ௡Ժ denotes the set of integers, and Ժ௡ denotes integer vectors. In the 
text, small bold face letters such as ࢞ǡ ࢟ are used to represent vectors or points in Թ௡ capital bold face 
letters such ࡭ǡ ࡮ as are used to represent matrices; ࡭௤ represents qth column of A; and ࡵ represents an 
identity matrix.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
19 
Mathematical Preliminaries
2	 Mathematical Preliminaries
This chapter introduces essential mathematical concepts that are required to understand the material 
presented in later chapters. The treatment of the topics is concise and limited to presentation of key 
aspects of the topic. More details on these topics can be found in standard mathematical optimization 
texts. Interested readers should consult the references (e.g., Griva, Nash & Sofer, 2009) for details. 
Learning Objectives: The learning goal in this chapter is to understand the mathematical principles 
necessary for formulating and solving optimization problems, i.e., for understanding the optimization 
techniques presented in later chapters. 
2.1	
Set Definitions 
Closed Set. A set is closed if for any sequence of points ሼݔ௞ሽݔ௞א ܵ௞՜ஶݔ௞ൌݔ we have ݔא ܵ 
For example, the set ܵൌሼݔǣ ȁݔȁ ൑ܿሽ where c is a finite number, describes a closed set. 
Bounded Set. A set S is bounded if for every ݔא ܵǡ ԡݔԡ ൏ܿ where ԡήԡ represents a vector norm and 
c is a finite number. 
Compact set. A set S is compact if it is both closed and bounded.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Fundamental Engineering Optimization 
Methods
20 
Mathematical Preliminaries
Interior point. A point is 

௫భǡ௫మ݂ൌ͹Ǥͷݔଵ൅ͷݔଶ
6XEMHFWWRͳͲݔଵ൅ͷݔଶ൑͸Ͳǡ ͷݔଵ൅ͳʹݔଶ൑ͺͲǢ ݔଵǡ ݔଶא Ժ interior to the set if ሼݕǣ ԡݕെݔԡ ൏߳ሽؿ ܵ for some ߳൐Ͳ
Open Set. A set S is open if every 

௫భǡ௫మ݂ൌ͹Ǥͷݔଵ൅ͷݔଶ
6XEMHFWWRͳͲݔଵ൅ͷݔଶ൑͸Ͳǡ ͷݔଵ൅ͳʹݔଶ൑ͺͲǢ ݔଵǡ ݔଶא Ժ is an interior point of S. For example, the set ܵൌሼݔǣ ȁݔȁ ൏ܿሽ 
where c is a finite number, is an open set. 
Convex Set. A set S is convex if for each pair ݔǡ ݕא ܵ their convex combination ߙݔ൅ሺͳ െߙሻݕא ܵ
for Ͳ ൑ߙ൑ͳ Examples of convex sets include a single point, a line segment, a hyperplane, a halfspace, 
the set of real numbers ԹሻDQGԹ௡ 
Hyperplane. The set Wܵൌሼ࢞ǣ ࢇ்࢞ൌܾሽǡZ where a and b are constants defines a hyperplane. Note that 
in two dimensions a hyperplane is a line. Also, note that vector a is normal to the hyperplane.
Halfspace. The set ܵൌሼ࢞ǣ ࢇ்࢞൑ܾሽǡZ
I
$O
K
 where a and b are constants defines a halfspace. Note that vectora 
is normal to the halfspace. Also, note that a halfspace is convex.
Polyhedron. A polyhedron represents a finite intersection of hyperplanes and halfspaces. Note that a 
polyhedron is convex.
Convex Hull. The convex hull of a set S is the set of all convex combinations of points in S. Note that 
convex hull of S is the smallest convex set that contains S.
Extreme Point. A point ݔא ܵ is an extreme point (or vertex) of a convex S set if it cannot be expressed 
as ݔൌߙݕ൅ሺͳ െߙሻݖǡZLWKݕǡ ݖא ܵZKHUHݕǡ ݖ്ݔǡDQGͲ ൏ߙ൏ͳ
2.2	
Function Definitions
Function. A function ݂ሺ࢞ሻ describes a mapping from a set of points called domain to a set of points 
called range. Mathematically, ݂ǣ ࣞ՜ ࣬ where ࣞ denotes the domain and ࣬ the range of the function.
Continuous Function. A function ݂ሺ࢞ሻ is said to be continuous at a point ࢞଴ if lim࢞՜࢞బ݂ሺ࢞ሻൌ݂ሺ࢞଴ሻ 
Alternatively, if a sequence of points ሼ࢞௞ሽin the function domain ࣞሺ݂ሻconverges to࢞଴ then ݂ሺ࢞௞ሻmust 
converge to ݂ሺ࢞଴ሻ for a function to be continuous. Note, that for functions of single variable, this implies 
that left and right limits coincide.
Affine Function. A function of the form ݂ሺ࢞ሻൌࢇ்࢞൅ܾ of the form represents an affine function.
Quadratic Function. A function of the form ݂ሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞െ࢈்࢞ǡ where Q is symmetric, represents 
a quadratic function.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
21 
Mathematical Preliminaries
Level Sets. The level sets of a function are defined as ܵൌሼݔǣ ݂ሺ࢞ሻൌܿሽǤ For functions of a single 
variable, the level sets represent discrete points. For functions of two variables, level sets are contours 
plotted in the ݔݕ plane.
Stationary Point. From elementary calculus, a single-variable function ݂ሺ࢞ሻ has a stationary point at 
ݔ଴ if the derivative ݂Ԣሺݔሻ vanishes at Wݔ଴, i.e., ݂ᇱሺݔ଴ሻൌͲ Graphically, the slope of the function is zero 
at the stationary point, which may represent a minimum, a maximum, or a point of inflecion. 
Local Minimum. A multi-variable function, ݂ሺ࢞ሻ, has a local minimum at ࢞כLI݂ሺ࢞כሻ൑݂ሺ࢞ሻ in a 
small neighborhood around ࢞כǡ defined by ȁ࢞െ࢞כȁ ൏߳ 
Global Minimum. The multi-variable function ݂ሺ࢞ሻ has a global minimum at ࢞כLI݂ሺ࢞כሻ൑݂ሺ࢞ሻ for 
all ࢞ in a feasible region defined by the problem. 
Convex Functions. A function ݂ሺݔሻ defined on a convex set ܵ is convex if and only if for all ࢞ǡ ࢟א ܵǡ 
݂ሺሻ
݂ሺߙ࢞൅ሺͳ െߙሻ࢟ሻ൑ߙ݂ሺ࢞ሻ൅ሺͳ െߙሻ݂ሺ࢟ሻߙא ሾͲǡͳሿ Note that affine functions defined over convex 
sets are convex. Similarly, quadratic functions defined over convex sets are convex.
2.3	
Taylor Series Approximation
Taylor series approximates a differentiable function ݂ሺݔሻ in the vicinity of an operating point ݔ଴. Such 
approximation is helpful in several problems involving functions.
An infinite Taylor series expansion of ݂ሺݔሻ around ݔ଴ (where ݀ൌݔെݔ଴) is given as:
݂ሺݔ଴൅݀ሻൌ݂ሺݔ଴ሻ൅݂ᇱሺݔ଴ሻ݀൅ͳ
ʹǨ ݂ᇱᇱሺݔ଴ሻ݀ଶ൅ڮ
As an example, the Taylor series for sin and cosine functions around ݔ଴ൌͲ are given as: 
ݔൌݔെ
௫య
ଷǨ ൅
௫ఱ
ହǨ െڮ
 ݔൌͳ െ
௫మ
ଶǨ ൅
௫ర
ସǨ െڮ
These series are summed in the Euler formula:  ݔ൅݅ ݔൌ݁ି௜௫
The ݊th order Taylor series approximation of ݂ሺݔሻ is given as:
݂ሺݔ଴൅݀ሻ؆ ݂ሺݔ଴ሻ൅݂ᇱሺݔ଴ሻ݀൅ͳ
ʹǨ ݂ᇱᇱሺݔ଴ሻ݀ଶ൅ڮ ൅ͳ
݊Ǩ ݂ሺ௡ሻሺݔ଴ሻ݀௡
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
22 
Mathematical Preliminaries
We note that first or second order approximation often suffice in the close neighborhood of ݔ଴. As an 
example, the local behavior of a function is frequently approximated by a tangent line defined as:
݂ሺݔሻെ݂ሺݔ଴ሻ؆ ݂ᇱሺݔ଴ሻሺݔെݔ଴ሻ
Next, the Taylor series expansion of a function ݂ሺݔଵǡ ݔଶሻ of two variables at a point ሺݔଵ଴ǡ ݔଶ଴ሻ is given as:
݂ሺݔଵ൅݀ଵǡ ݔଶ൅݀ଶሻൌ݂ሺݔଵ଴ǡ ݔଶ଴ሻ൅߲݂
߲ݔଵ
݀ଵ൅߲݂
߲ݔଶ
݀ଶ൅ͳ
ʹ ቈ߲ଶ݂
߲ݔଵ
ଶ݀ଵ
ଶ൅
߲ଶ݂
߲ݔଵ߲ݔଶ
ଵଶ൅߲ଶ݂
߲ݔଶ
ଶ݀ଶ
ଶ቉൅ڮ
where ݀ଵൌݔଵെݔଵ଴ǡ ݀ଶൌݔଶെݔଶ଴ǡ and all partial derivatives are computed at the point: ሺݔଵ଴ǡ ݔଶ଴ሻ. 
Further, let ݖൌ݂ሺݔଵǡ ݔଶሻǢthen, the tangent plane of ݖ at ሺݔଵ଴ǡ ݔଶ଴ሻ is defined by the equation:
ݖൌ݂ሺݔଵ଴ǡ ݔଶ଴ሻ൅
డ௙
డ௫భቚ
ሺ௫భబǡ௫మబሻ
ሺݔଵെݔଵ଴ሻ൅
డ௙
డ௫మቚ
ሺ௫భబǡ௫మబሻ
ሺݔଶെݔଶ଴ሻ
Taylor series expansion in the case of a multi-variable function is given after defining gradient vector 
and Hessian matrix in Sec. 2.4. Finally, it is important to remember that Taylor series only approximates 
the local behavior of the function, and therefore should be used with caution. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Fundamental Engineering Optimization 
Methods
23 
Mathematical Preliminaries
2.4	
Gradient Vector and Hessian Matrix
The gradient vector and Hessian matrix play an important role in optimization. These concepts are 
introduced as such:
The Gradient Vector. Let ݂ሺ࢞ሻൌ݂ሺݔଵǡ ݔଶǡ ǥ ǡ ݔ௡ሻ be a real-valued function of variables with continuous 
partial derivatives, i.e., ݂א ׋ଵ. Then, the gradient of ݂ is a vector defined by:
׏݂ሺ࢞ሻ்ൌ൬߲݂
߲ݔଵ
ǡ ߲݂
߲ݔଶ
ǡ ǥ ǡ ߲݂
߲ݔ௡
൰
The gradient vector has several important properties. These include: 
1.	 The gradient points in the direction of maximum rate of increase in the function value 
at a given point. This can be seen by considering the directional derivative of ݂ሺ࢞ሻ 
along any direction ࢊ defined as:݂ࢊ
ᇱሺ࢞ሻൌ׏݂ሺ࢞ሻ்ࢊൌȁ׏݂ሺ࢞ሻȁȁࢊȁ  ߠ where ߠ is 
the angle between the two vectors. Then, the maximum rate of increase occurs when 
ߠൌͲLHDORQJ׏݂ሺ࢞ሻ
2.	 The magnitude of the gradient gives the maximum rate of increase in ݂ሺ࢞ሻ. Indeed, 
ԡௗԡୀଵ݂ௗ
ᇱሺݔሻൌԡ׏݂ሺ࢞ሻԡ
3.	 The gradient vector at a point ࢞כ is normal to the tangent hyperplane defined by ݂ሺ࢞ሻ 
constant. This can be shown as follows: let C be any curve in the tangent space passing 
through ࢞, and let ݏ be a parameter along C. Then, a unit tangent vector along ܥ is given as: 
డ࢞
డ௦ൌሺ
డ௫భ
డ௦ǡ
డ௫మ
డ௦ǡ ǥ ǡ
డ௫೙
డ௦ሻ Further, we note that 
ௗ௙
ௗ௦ൌ
డ௙
డ࢞
డ࢞
డ௦ൌ׏݂ሺ࢞ሻ்డ௫
డ௦ൌͲLH׏݂ሺ࢞ሻ is 
normal to డ࢞
డ௦.
The Hessian Matrix. The Hessian of ݂ is a ݊ൈ݊ matrix given by ׏ଶ݂ሺ࢞ሻ, where ሾ׏ଶ݂ሺ࢞ሻሿ௜௝ൌ
డమ௙
డ௫೔డ௫ೕ
Note that Hessian is a symmetric matrix, since 
డమ௙
డ௫೔డ௫ೕൌ
డమ௙
డ௫ೕడ௫೔.
As an example, we consider a quadratic function: ݂ሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞െ࢈்࢞ where Q is symmetric. Then 
its gradient and Hessian are given as: ; ׏݂ሺ࢞ሻൌࡽ࢞׏ଶ݂ሺ࢞ሻൌࡽ.
Composite functions. Gradient and Hessian in the case of composite functions are computed as follows: 
Let ݂ሺ࢞ሻൌ݃ሺ࢞ሻ݄ሺ࢞ሻ be a product of two functions; then, 
׏݂ሺ࢞ሻൌ׏݃ሺ࢞ሻ݄ሺ࢞ሻ൅݃ሺ࢞ሻ׏݄ሺ࢞ሻ
׏ଶ݂ሺ࢞ሻൌ׏ଶ݃ሺ࢞ሻ݄ሺ࢞ሻ൅݃ሺ࢞ሻ׏ଶ݄ሺ࢞ሻ൅׏݃ሺ࢞ሻ׏݄ሺ࢞ሻ்൅׏݃ሺ࢞ሻ׏݄ሺ࢞ሻ்
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
24 
Mathematical Preliminaries
For vector-valued functions, let સ݂ be a matrix defined by ሾ׏݂ሺ࢞ሻሿ௜௝ൌ
డ௙ೕሺ࢞ሻ
డ௫೔WKHQસ݂ሺ࢞ሻ், then defines 
the Jacobian of f at point ࢞. Further, let ݂ൌࢍ்ࢎ, where ࢍǡ ࢎǣ Թ௡՜ Թ௠ then: 
સ݂ൌሾસࢎሿࢍ൅ሾસࢍሿࢎ
Taylor series expansion for multi-variable functions. Taylor series expansion in the case of a multi-
variable function is given as:
݂ሺ࢞଴൅ࢊሻൌ݂ሺ࢞଴ሻ൅׏݂ሺ࢞଴ሻ்ࢊ൅ͳ
ʹǨ ࢊ்׏ଶ݂ሺ࢞଴ሻࢊ൅ڮ
where ׏݂ሺ࢞଴ሻ and ׏ଶ݂ሺ࢞଴ሻ are, respectively, the gradient and Hessian of ݂ computed at ࢞଴. In particular, 
a first-order change in ݂ሺ࢞ሻ at ࢞଴ along d is given as: ߜ݂ൌ׏݂ሺ࢞଴ሻ்ࢊ, where ׏݂ሺ࢞଴ሻ்ࢊ defines the 
directional derivative of ݂ሺ࢞ሻDW࢞଴ at along d.
2.5	
Convex Optimization Problems
Convex optimization problems are easier to solve due to the fact that convex functions have a unique global 
minimum. As defined in Sec. 2.2 above, a function ݂ሺݔሻ defined on a convex set is convex if and only if 
for all ࢞ǡ ࢟א ܵǡ ݂ሺߙ࢞൅ሺͳ െߙሻ࢟ሻ൑ߙ݂ሺ࢞ሻ൅ሺͳ െߙሻ݂ሺ࢟ሻǡ ߙא ሾͲǡͳሿ In general, this condition may 
be hard to verify and other conditions based on properties of convex functions have been developed. 
Convex functions have following important properties: 
1.	 If ݂ א ׋ଵ (i.e., is differentiable), then f is convex over a convex set S if and only if for all 
࢞ǡ ࢟א ܵǡ ݂ሺ࢟ሻ൒݂ሺ࢞ሻ൅׏݂ሺ࢞ሻ்ሺ࢟െ࢞ሻ Graphically, it means that a function is on or 
above the tangent hyperplane (line in two dimensions) passing through ࢞. 
2.	 If ݂א ׋ଶ (i.e., is twice differentiable), then f is convex over a convex set ܵ if and only if for 
all ࢞א ܵǡ ݂ԢԢሺ࢞ሻ൒Ͳ In the case of multivariable functions, f is convex over a convex set S 
if and only if its Hessian matrix is positive semi-definite everywhere in S, i.e., for all ࢞א ܵ 
and for all ࢊǡ ࢊ்સଶ݂ሺ࢞ሻࢊ൒Ͳ 
This can be seen by considering second order Taylor series expansion of ݂ሺ࢞ሻ at two points 
equidistant from a midpoint,࢞ഥ, given as: ݂ሺ࢞ഥേࢊሻ؆ ݂ሺ࢞ഥሻേ׏݂ሺ࢞ഥሻ்ࢊ൅
ଵ
ଶࢊ்׏ଶ݂ሺ࢞ഥሻࢊ
Adding these two points with ߙൌ
ଵ
ଶ and applying the definition of convex function gives: 
݂ሺ࢞ഥሻ൑݂ሺ࢞ഥሻ൅ࢊ்׏ଶ݂ሺ࢞ഥሻࢊRUࢊ்׏ଶ݂ሺ࢞ഥሻࢊ൒૙
3.	 If the Hessian is positive definite, i.e., for all ࢞א ܵ and for all ࢊǡ ࢊ்સଶ݂ሺ࢞ሻࢊ൐Ͳ then the 
function is strictly convex. This is, however, a sufficient but not necessary condition, and a 
strictly convex function may have only a positive semidefinite Hessian at some points. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
25 
Mathematical Preliminaries
4.	 If ݂ሺ࢞כሻ is a local minimum for a convex function f defined over a convex set S, then it is 
also a global minimum. This can be shown as follows: assume that ݂ሺ࢞כሻൌͲ and replace x 
with ࢞כ in property one above to get: ݂ሺ࢞ሻ൒݂ሺ࢞כሻǡ ࢞א ܵ Thus, for a convex function f, 
any point ࢞כ that satisfies the necessary condition: ׏݂ሺ࢞כሻൌͲ is a global minimum of f.
Due to the fact that convex functions have a unique global minimum, convexity plays an important role 
in optimization. For example, in numerical optimization convexity assures a global minimum to the 
problem. It is therefore important to first establish the convexity property when solving optimization 
problems. The following characterization of convexity applies to the solution spaces in such problems. 
Further ways of establishing convexity are discussed in (Boyd & Vandenberghe, Chaps. 2&3).
If a function ݃௜ሺ࢞ሻ is convex, then the set ݃௜ሺ࢞ሻ൑݁௜
ሽ
 is convex. Further, if functions ݃௜ሺ࢞ሻǡ ݅ൌͳǡ ǥ ǡ ݉ǡ 
are convex, then the set ሼ࢞ǣ݃௜ሺ࢞ሻ൑݁௜ǡ ݅ൌͳǡ ǥ ǡ ݉ሽ is convex. In general, finite intersection of convex 
sets (that include hyperplanes and halfspaces) is convex. 
For general optimization problems involving inequality constraints: ݃௜ሺ࢞ሻ൑݁௜ǡ ݅ൌͳǡ ǥ ǡ ݉DQG݈, and 
equality constraints: ݄௝ሺ࢞ሻൌܾ௝ǡ ݆ൌͳǡ ǥ ǡ ݈ǡW the feasible region for the problem is defined by the set: 
ܵൌሼ࢞ǣ݃௜ሺ࢞ሻ൑݁௜ǡ ݄௝ሺ࢞ሻൌܾ௝ሽ The feasible region is a convex set if the functions: ݃௜݅ൌͳǡ ǥ ǡ ݉ǡ are 
convex and the functions: ݄௝ǡ ݆ൌͳǡ ǥ ǡ ݈ǡ are linear. Note that these convexity conditions are sufficient 
but not necessary.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
26 
Mathematical Preliminaries
2.6	
Vector and Matrix Norms
Norms provide a measure for the size of a vector or matrix, similar to the notion of absolute value in the 
case of real numbers. A norm of a vector or matrix is a real-valued function with the following properties:
1.	 ԡ࢞ԡ ൒Ͳ for all ࢞
2.	 ԡ࢞ԡ ൌͲ if and only if ࢞ൌ૙
3.	 ԡߙ࢞ԡ ൌȁߙȁԡ࢞ԡ for all ߙא Թ
4.	 ԡ࢞൅࢟ԡ ൑ԡ࢞ԡ ൅ԡ࢟ԡ
Matrix norms additionally satisfy:
5.	 ||AB||≤||A|| ||B||
Vector Norms. Vector p-norms are defined by ԡ࢞ԡ௣ൌሺσ
ȁݔ௜ȁ
௡
௜ୀଵ
ሻ
భ
೛ǡ ݌൒ͳ They include the 1-norm 
ԡ࢞ԡଵൌσ
ȁݔ௜ȁ
௡
௜ୀଵ
 the Euclidean norm Pԡ࢞ԡଶൌඥσ
ȁݔ௜ȁଶ
௡
௜ୀଵ
 and the ∞-norm ||x||∞ = max|xi|.
Matrix Norms. Popular matrix norms are induced from vector norms, given as: ||A|| = max||Ax||
                     ||x||=1
 All 
induced norms satisfy ||Ax||≤||A|| ||x||. Examples of induced matrix norms are: 
1.	 ||A||1 = max Σn
i=1|Ai,j|
                    1≤j<n
 (the largest column sum of A) 
2.	 ||A||2 = √λmax(ATA), where denotes the maximum eigenvalue of the matrix 
3.	 ||A||∞ = max Σn
j=1|Ai,j|
                      1≤j<n
 (the largest row sum of A)
2.7	
Matrix Eigenvalues and Singular Values
Let A be an n × n matrix and assume that for some vector v and scalar λ, Av = λv; then λ is an eigenvalue 
and v is an eigenvector of A. The eigenvalues of A may be solved from:det (A – λI) = 0. The nth degree 
polynomial on the left-hand side of the equation is the characteristic polynomial of A whose roots are 
the eigenvalues of A. Let these roots be given as: λi,i = 1,…,n then their associated eigenvectors are 
solved from: (A – λiI) v = 0. 
A matrix with repeated eigenvalues may not have a full set of eigenvectors which, by definition, are 
linearly independent. This happens, for instance, when the nullity of (A – λiI) is less than the degree of 
repetition of λi. In such cases, generalized eigenvectors may be substituted to make up the count.
Spectral Decomposition of a Symmetric Matrix. If A is symmetric, it has real eigenvalues and a full 
set of eigenvectors. Labeling them ࢜ଵǡ ࢜ଶǡ ǥ ǡ ࢜௡ it is possible to choose them to be orthonormal, such 
that ࢜௜
்࢜௜ DQG࢜௜
்࢜௝ൌͲ്݆݅ By defining ࢂൌሺ࢜ଵǡ ࢜ଶǡ ǥ ǡ ࢜௡ሻDQG઩ൌ݀݅ܽ݃ሺߣଵǡ ߣଶǡ ǥ ǡ ߣ௡ሻ 
we have ࡭ࢂൌ઩ࢂRU࡭ൌࢂ઩ࢂ் This is referred to as spectral decomposition of A. 
1≤i≤n
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
27 
Mathematical Preliminaries
Singular Value Decomposition of a Non-square Matrix. For non-square ࡭א Թ௠ൈ௡, the singular value 
decomposition (SVD) of A is given as: ࡭ൌࢁ઱ࢂ்ൌσ
ߪ௜࢛௜࢜௜
்
௥
௜ୀଵ
 whereHݎൌUDQNሺ࡭ሻࢁא Թ௠ൈ௥ǡ 
ࢁ்ࢁൌࡵ௠ൈ௠Ǣࢂא Թ௡ൈ௥ǡ ࢂ்ࢂൌࡵ௡ൈ௡Ǣ ઱ൌሺɐଵǡ ɐଶǡ ǥ ǡ ɐ୰ሻ where 
ɐଵ൒ɐଶ൒ڮ ǡ ൒ɐ୰are 
termed as singular values of A.
2.8	
Quadratic Function Forms
The function ݂ሺ࢞ሻൌ்࢞ࡽ࢞ൌσ
σ
ܳ௜ǡ௝ݔ௜ݔ௝
௡
௝ୀଵ
௡
௜ୀଵ
 describes a quadratic form. Quadratic forms in one 
and two variables are, respectively, given as: ݂ሺݔሻൌݍݔଶDQG݂ሺݔଵǡ ݔଶሻൌܳଵǡଵݔଵ
ଶ൅ܳଶǡଶݔଶ
ଶ൅ʹܳଵǡଶݔଵݔଶ
We note that replacing matrix Q by its symmetric counterpart 
ଵ
ଶሺࡽ൅ࡽ்ሻ does not change ݂ሺ࢞ሻ. 
Therefore, in a quadratic form Q can always assumed to be symmetric. 
The quadratic form is classified as:
a)	 Positive definite if ்࢞ࡽ࢞൐Ͳ
b)	 Positive semidefinite if ்࢞ࡽ࢞൒Ͳ
c)	 Negative definite if I ்࢞ࡽ࢞൏Ͳ
d)	 Negative semidefinite if ்࢞ࡽ࢞൑Ͳ
e)	 Infinite otherwise
Let ߣ௠௜௡DQGߣ௠௔௫ and denote the minimum and maximum eigenvalues of Q; then the quadratic form 
obeys:
ߣ௠௜௡்࢞࢞൑்࢞ࡽ࢞൑ߣ௠௔௫்࢞࢞
Thus, positive definiteness of ்࢞ࡽ࢞ can be determined from the positivity of the eigenvalues of Q. In 
particular, let ߣ௜ǡ ݅ൌͳǡʹǡ ǥ ǡ ݊ be the eigenvalues of Q; then Q is:
a)	 Positive definite only if ߣ௜൐Ͳǡ ݅ൌͳǡʹǡ ǥ ǡ ݊ 
b)	 Positive semidefinite only if I ߣ௜൒Ͳǡ ݅ൌͳǡʹǡ ǥ ǡ ݊
c)	 Negative definite only if ߣ௜൏Ͳǡ ݅ൌͳǡʹǡ ǥ ǡ ݊
d)	 Negative semidefinite only if I ߣ௜൑Ͳǡ ݅ൌͳǡʹǡ ǥ ǡ ݊
e)	 Indefinite otherwise
Geometrically, the set ܵൌሼ࢞ǣ்࢞ࡽ࢞൑ܿሽ describes an ellipsoid in Թ௡ centered at 0 with its maximum 
eccentricity given by ඥߣ௠௔௫Ȁߣ௠௜௡ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
28 
Mathematical Preliminaries
2.9	
Linear Systems of Equations
Systems of linear equations arise in solving the linear programming problems (Chapter 5). In the 
following, we briefly discuss the existence of solutions in the case of such systems.
Consider a system of m (independent) linear equations in n unknowns described as: ࡭࢞ൌ࢈ Then, from 
linear algebra, the system has a unique solution if m = n; multiple solutions if m < n; and, the system is 
over-determined (and can be solved in the least-squares sense) if m > n.
For m = n, Gaussian elimination with partial pivoting results in a matrix decomposition ࡭ൌࡼࡸࢁZ where 
ࡼǡ ࡼ்ࡼൌࡵ is a permutation matrix; L is a lower triangular matrix with ones on the diagonal; and U 
is an upper triangular with eigenvalues of A on the main diagonal (Griva, Nash & Sofer, p.669). Then, 
using y, z as intermediate variables, the system can be solved in steps as: ࡼࢠൌ࢈ǡ ࡸ࢟ൌࢠǡࢁ࢞ൌ࢟ If 
A is symmetric and positive definite, then Gaussian elimination results in ࡭ൌࡸࢁൌࡸࡰࡸ் where D is 
a diagonal matrix of (positive) eigenvalues of A. In this case, the solution to the linear system is given 
as: ࢞ൌࡸࡰିଵࡸ்࢈ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Fundamental Engineering Optimization 
Methods
29 
Mathematical Preliminaries
Assume now that I ݉൏݊ and that matrix A has full row rank. Then, we can arbitrarily choose ሺ݊െ݉ሻ 
variables as independent (nonbasic) variables, and solve the remaining ሺ݉ሻ variables as dependent 
(basic) variables. The Gauss-Jordan elimination can be used to convert the system of equations into 
its canonical form given as: ࡵሺ௠ሻ࢞ሺ௠ሻ൅ࡽ࢞ሺ௡ି௠ሻൌ࢈ Then, the general solution to the linear system 
includes the independent variables: ࢞ሺ௡ି௠ሻ, and the dependent variables: ࢞ሺ௠ሻൌ࢈െࡽ࢞ሺ௡ି௠ሻ A 
particular solution to the linear system can be obtained by setting:࢞ሺ௡ି௠ሻൌ૙ and obtaining: ࢞ሺ௠ሻൌ࢈
For non-square matrices with ݉൐݊, Gram-Schmidt orthogonalization or Householder transformations 
can be applied to obtain ࡭ൌࡽࡾZKHUHࡽࡽ்ൌࡵDQGࡾ, where , and is upper triangular (QR 
factorization). The original system is equivalent to Rࡾ࢞ൌࡽ்࢈ which can then be solved via back-
substitution. Following are two examples of practical situations that result in linear least-squares problems 
involving over-determined systems of linear equations (݉൐݊). 
Linear Estimation Problem. Originally tackled by Carl Frederic Gauss, the linear estimation problem 
arises when estimating the state ࢞ of a linear system using a set of observations denoted as y. 
Consider a linear system of equations: ࡭࢞ൌ࢟ǡ ࡭א Թ௠ൈ௡ǡ ݉൐݊ where ࢟ is the observation vector. 
Let ࢘ൌ࡭࢞െ࢟ define a residual vector, and consider the unconstrained minimization problem: 
࢞ԡ࢘ԡଶൌሺ࡭࢞െ࢟ሻ்ሺ࡭࢞െ࢟ሻ 
Using derivatives, the problem is solved as: ௗ
ௗ࢞ሾ்࢞࡭்࡭࢞െ்࢟࡭࢞െ்࢞࡭࢟൅்࢟࢟ሿൌͲ which leads to: 
࡭்࡭࢞ൌ࡭்࢟ Thus, the solution to the least-squares problem is given as: ࢞ෝൌሺ࡭்࡭ሻିଵ࡭்࢟, where 
had denotes the estimated value of the variable. Further, let R describe the measurement covariance 
matrix: ࡾൌܧሾ்࢘࢘ሿǤ Then, the best linear estimator for x is given as: ࢞ෝൌሺ࡭்ࡾିଵ࡭ሻିଵ࡭்ࡾିଵ࢈.
Data Fitting Problem. The data-fitting problem involves fitting an ݊th degree polynomial given as: 
݌ሺݔሻൌ݌଴൅݌ଵݔ൅ڮ ൅݌௡ݔ௡ to a set of data points: where ሺݔ௜ǡ ݕ௜ሻǡ ݅ൌͳǡ ǥ ǡ ܰZKHUHܰ൐݊. 
To solve this problem, we similarly define a residual: ݎ௜ൌݕ௜െ݌ሺݔ௜ሻൌݕ௜െሺ݌଴൅݌ଵݔ௜൅ڮ ൅݌௡ݔ௜
௡ሻ
and define the following unconstrained minimization problem: min௣ೕσ
ݎ௜
ଶ
ே
௜ୀଵ
 where ݌௝ represents the 
coefficients of the polynomial. Then, by defining a coefficient vector: ࢞ൌሾ݌଴ǡ ݌ଵǡ ǥ ǡ ݌௡ሿ், and an 
ܰൈሺ݊൅ͳሻ matrix A whose rows are observation vectors of the form ሾͳǡ ݔ௜ǡ ݔ௜
ଶǡ ǥ ǡ ݔ௜
௡ሿ, we can 
solve for the coefficients using the linear least-squares framework. 
For example, in the linear case, ݌ሺݔሻൌ݌଴൅݌ଵݔDQG࡭LVDܰൈʹ matrix whose rows are ሾͳǡ ݔ௜ሿ 
vectors. The least-squares method then results in the following equations: 
ቆσ
ͳ
ே
௜ୀଵ
σ
ݔ௜
ே
௜ୀଵ
σ
ݔ௜
ே
௜ୀଵ
σ
ݔ௜
ଶ
ே
௜ୀଵ
ቇቀ݌଴
݌ଵቁൌቆσ
ݕ௜
ே
௜ୀଵ
σ
ݔ௜ݕ௜
ே
௜ୀଵ
ቇ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
30 
Mathematical Preliminaries
Using averages: ଵ
ேσ
ݔ௜
ே
௜ୀଵ
ൌݔҧǡ
ଵ
ேσ
ݕ௜
ே
௜ୀଵ
ൌݕതǡ the solution is given as: 

݌ଵൌσ
ሺݔ௜െݔҧሻሺݕ௜െݕതሻ
ே
௜ୀଵ
σ
ሺݔ௜െݔҧሻଶ
ே
௜ୀଵ
Ǣ݌଴ൌݕതെ݌ଵݔҧ
Finally, the above solution can also be obtained through application of optimality conditions (Chapter 3).
2.10	
Linear Diophantine System of Equations
A Linear Diophantine system of equations (LDSE) is represented as: ࡭࢞ൌ࢈ǡ ࢞א Ժ௡ The following 
algebra concepts are needed to formulate and solve problems involving a solution to LDSE.
Unimodular Matrices. Matrix ࡭א Ժ௡ൈ௡ is unimodular if det ሺ࡭ሻൌേͳǤ Further, if ࡭א Ժ௡ൈ௡ is 
unimodular, then Q࡭ିଵא Ժ௡ൈ௡0DWUL[࡭א Ժ௡ൈ௡ is totally unimodular if every square submatrix 
[࡯RI࡭ǡ has det ሺ࡯ሻא ሼͲǡ േͳሽǤ
Hermite Normal Form of a Matrix. /HW࡭א Ժ௠ൈ௡ǡ ݎܽ݊݇ሺ࡭ሻൌ݉Ǣ then, A has a unique hermite 
normal form given as: +1)ሺ࡭ሻൌሾࡰ૙ሿ where D is lower triangular with ݀௜௝൏݀௜௜ǡ ݆൏݅. Further, 
there exists a unimodular matrix U such that ࡭ࢁൌ+1)ሺ࡭ሻǡ where we note that post-multiplication by 
a unimodular matrix involves performing elementary column operations. Moreover, let ࢛ଵǡ ࢛ଶǡ ǥ ǡ ࢛௡ 
represent the U columns of then Qሼ࢛௠ାଵǡ ǥ ǡ ࢛௡ሽ form a basis for ker (A).
Solution to the LDSE. Assume that ࡭א Ժ௠ൈ௡ݎܽ݊݇ሺ࡭ሻൌ݉ǡ, and let ࡭ࢁൌ+1)ሺ࡭ሻǢ then, we 
may consider: ࢈ൌ࡭࢞ൌ࡭ࢁࢁିଵ࢞ൌࡴࡺࡲሺ࡭ሻ࢟ǡ ࢟ൌࢁିଵ࢞Ǥ Assume that we have a solution ࢟଴ to: 
+1)ሺ࡭ሻ࢟଴ൌ࢈Ǣ then, the general solution to the LDSE is given as:࢞ൌ࢞଴൅σ
ߙ௜࢞௜
௡ି௠
௜ୀଵ
ǡ where 
࢞଴ൌࢁ࢟଴ǡ ࢞௜א ሼ࢛௠ାଵǡ ǥ ǡ ࢛௡ሽ 
2.11	
Condition Number and Convergence Rates
The condition number of a matrix is defined as: cond ሺ࡭ሻൌԡ࡭ԡ ή ԡ࡭ିଵԡ Note that and cond ݀ሺ࡭ሻ൒ͳ
and cond ሺࡵሻൌͳǡ where I is an identity matrix. If A is symmetric with real eigenvalues, and 2-norm is 
used, then ሺ࡭ሻൌߣ௠௔௫ሺ࡭ሻȀߣ௠௜௡ሺ࡭ሻ
The condition number of the Hessian matrix affects the convergence rates of the optimization algorithms. 
Ill-conditioned matrices give rise to numerical errors in computations. In certain cases, it is possible 
to improve the condition number by scaling the variables. The convergence property implies that the 
generated sequence converges to the true solution in the limit. The rate of convergence dictates how 
quickly the approximate solutions approach the exact solution. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
31 
Mathematical Preliminaries
Assume that a sequence of points ሼݔ௞ሽ converges to a solution point ݔכ and define an error sequence: 
݁௞ൌݔ௞െݔכ Then, we say that the sequence ሼݔ௞ሽ converges to ݔכ with rate and rate constant ܥ 
if ௞՜ஶ
ԡ௘ೖశభԡ
ԡ௘ೖԡೝൌܥ Further, if uniform convergence is assumed, then ԡ݁௞ାଵԡ ൌܥԡ݁௞ԡ௥ holds for 
all ݇ Thus, convergence to the limit point is faster if ݎ is larger and ܥ is smaller. Specific cases for 
different choices of ݎ and ܥ are mentioned below. 
Linear convergence. For ݎൌͳDQGͲ ൏ܥ൏ͳԡ݁௞ାଵԡ ൌܥԡ݁௞ԡ signifying linear convergence. In 
this case the speed of convergence depends only on ܥ, which can be estimated as ܥൎ
௙൫௫ೖశభ൯ି௙ሺ௫כሻ
௙൫௫ೖ൯ି௙ሺ௫כሻ. 
Quadratic Convergence. For r = 2, the convergence is quadratic, i.e., ԡ݁௞ାଵԡ ൌܥԡ݁௞ԡଶ In this case 
if additionally C = 1, then the number of correct digits in double at every iteration.
Superlinear Convergence. For ͳ ൏ݎ൏ʹ, the convergence is superlinear. Superlinear convergence is 
achieved by numerical algorithms that only use the gradient (first derivative) of the cost function, and 
thus can qualitatively match quadratic convergence. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Fundamental Engineering Optimization 
Methods
32 
Mathematical Preliminaries
2.12	
 Conjugate-Gradient Method for Linear Equations
The conjugate-gradient method is an iterative method designed to solve a system of linear equations 
described as: ࡭࢞ൌ࢈ǡ where A is assumed normal, i.e., 
J
࡭்࡭ൌ࡭࡭் The method initializes with ࢞଴ൌ૙ǡ 
and uses an iterative process to obtain an approximate solution ࢞௡ in ݊ iterations. The solution is exact 
in the case of quadratic functions of the form: ݍሺ࢞ሻൌ
ଵ
ଶ்࢞࡭࢞െ࢈்࢞) For general nonlinear functions, 
convergence in 2݊ iterations is to be expected. The method is named so because ࡭࢞െ࢈ represents 
the gradient of the quadratic function. Solving a linear system of equations thus amounts to solving the 
minimization problem involving a quadratic function. 
The conjugate-gradient method generates a set of vectors ࢜ଵǡ ࢜ଶǡ ǥ ǡ ࢜௡ that are conjugate with respect 
to A matrix, i.e., ࢜௜
்࡭࢜௝ൌͲǡ ്݆݅/HW࢜ିଵൌ૙ǡ ߚ଴ൌͲ and define a residual ࢘௜ൌ࢈െ࡭࢞௜ Then, a 
set of conjugate vectors is iteratively generated as:
࢜௜ൌ࢘௜൅ߚ௜࢜௜ିଵǡߚ௜ൌ
࢜೔
೅࡭࢘೔
࢜೔
೅࡭࢜೔
We note that the set of conjugate vectors of a matrix is not unique. Further, nonzero conjugate vectors 
with respect to a positive-definite matrix are linearly independent. 
In conjugate-gradient and other iterative methods, scaling of variables, termed as preconditioning, helps 
reduce the condition number of the coefficient matrix, which aids in fast convergence of the algorithm. 
Towards that end, we consider a linear system of equations: ࡭࢞ൌ࢈ and use a linear transformation to 
formulate an equivalent system that is easier to solve. Let P be any nonsingular ݊ൈ݊ matrix, then an 
equivalent left-preconditioned system is formulated as: ࡼିଵ࡭࢞ൌࡼିଵ࢈, and a right-preconditioned 
system is given as: ࡭ࡼିଵࡼ࢞ൌ࢈. As the operator ࡼିଵ is applied at each step of the iterative solution, 
it helps to choose a simple ࡼିଵ with a small computational cost. An example of a simple preconditioner 
is the Jacobi preconditioner: ࡼൌ݀݅ܽ݃ሺ࡭ሻ 
Further, if A is symmetric and positive-definite, then ࡼିଵ should be chosen likewise. If both ࡼିଵ and ࡭
are positive-definite, then we can use the Cholesky decomposition of ࡼࡼൌ࡯்࡯, to write ࡯ିଵ࡯ି்࡭࢞ൌ
S

࡯ିଵ࡯ି்࢈RU࡯ି்࡭࡯ିଵ࢞ൌ࡯ି்࢈ Then, by defining ࡯ି்࡭࡯ିଵൌ࡭෡࡯ି்࢈ൌ࢈෡ we obtain ࡭෡࢞ൌ࢈෡ 
where ࡭෡ is positive-definite.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
33 
Mathematical Preliminaries
2.13	
Newton’s Method for Nonlinear Equations
Newton’s method, also known as Newton-Raphson method, iteratively solves a nonlinear equation: 
݂ሺݔሻൌͲǡstarting from an initial point ݔ଴ The method generates a series of solutions ሼݔ௞ሽ that are 
expected to converge to a fixed point ݔכ that represents a root of the equation. To develop the method, we 
assume that an estimate of the solution is available as ݔ௞ǡ and use first order Taylor series to approximate 
݂ሺݔሻ around ݔ௞ǡ i.e., let
݂ሺݔ௞൅ߜݔሻൌ݂ሺݔ௞ሻ൅݂ᇱሺݔ௞ሻߜݔ
Then, by setting ݂ሺݔ௞൅ߜݔሻൌͲ we can solve for the offset ߜݔ and use it to update our estimate ݔ௞ǡ as: 
ݔ௞ାଵൌݔ௞െ݂ሺݔ௞ሻȀ݂ᇱሺݔ௞ሻ
Next, Newton’s method can be extended to a system of nonlinear equations, given as: 
݂ଵሺݔଵǡ ݔଶǡ ǥ ǡ ݔ௡ሻൌͲ
݂ଶሺݔଵǡ ݔଶǡ ǥ ǡ ݔ௡ሻൌͲ
ڭ
݂௡ሺݔଵǡ ݔଶǡ ǥ ǡ ݔ௡ሻൌͲ
Let a gradient matrix ׏݂ሺ࢞ሻ be formed with columns: ׏݂ଵሺ࢞ሻǡ ׏݂ଶሺ࢞ሻǡ ǥ ǡ ׏݂௡ሺ࢞ሻǢ then, the transpose 
of the gradient matrix defines the Jacobian matrix given as: ܬሺ࢞ሻൌ׏݂ሺ࢞ሻ் Using the Jacobian matrix, 
the update rule in the ݊-dimensional case is given as:
࢞௞ାଵൌ࢞௞െ൫ܬሺ࢞௞ሻ൯
ିଵ݂ሺ࢞௞ሻ
Convergence Rate. We first note that Newton’s method requires a good initial guess for it to converge. 
Newton’s method, if it converges, exhibits quadratic rate of convergence near the solution point. The 
method can become unstable if ݂ሺݔכሻൎͲǤ Assuming ݂ᇱሺݔכሻ്Ͳ and ݔ௞ǡ is sufficiently close to ݔכ, 
we can use second order Taylor series to write:
ݔ௞ାଵെݔכ ൎͳ
ʹ ቆ݂ᇱᇱሺݔכሻ
݂ᇱሺݔכሻቇሺݔ௞െݔכሻଶ
which shows that Newton’s method has quadratic convergence with a rate constant: ܥൌ
ଵ
ଶቚ
௙ᇲᇲሺ௫כሻ
௙ᇲሺ௫כሻቚ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
34 
Graphical Optimization
3	 Graphical Optimization
We briefly discuss the graphical optimization concepts in this chapter before proceeding to formal 
mathematical optimization method in Chapter 4 and computational methods in Chapter 7. Graphical 
approach is recommended for problems of low dimensions, typically those involving one or two variables. 
Apart from being simple, the graphical method provides a valuable insight into the problem, which may 
not be forthcoming in the case of mathematical and computational optimization methods, particularly 
in the case of two-dimensional problems. 
The graphical method is applicable when the optimization problem is formulated with one or two 
variables. Graphical optimization helps enhance our understanding of the underlying problem and 
develop an appeal for the expected solution. The method involves plotting contours of the cost function 
over a feasible region enclosed by the constraint boundaries. In most cases, the desired optimum can 
be spotted by inspection. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
35 
Graphical Optimization
Software implementation of the graphical method uses a grid of paired values for the optimization 
variables to plot the objective function contours and the constraint boundaries. The minimum of the 
cost function can then be identified on the plot. Graphical minimization procedure thus involves the 
following steps:
1.	 Establishing the feasible region. This is done by plotting the constraint boundaries.
2.	 Plotting the level curves (or contours) of the cost function and identifying the minimum.
The graphical method is normally implemented in a computational software package such as Matlab © 
and Mathematica ©. Both packages include functions that aid the plotting and visualization of cost 
function contours and constraint boundaries. Code for Matlab implementation of graphical optimization 
examples considered in this chapter is provided in the Appedix. 
Learning Objectives: The learning goals in this chapter are:
1.	 Recognize the usefulness and applicability of the graphical method.
2.	 Learn how to apply graphical optimization techniques to problems of low dimensions.
3.1	
Functional Minimization in One-Dimension 
Graphical function minimization in one-dimension is performed by computing and plotting the function 
values at a set of discrete points and identifying its minimum value on the plot. We assume that the 
feasible region for the problem is a closed interval: ܵൌሾݔ௟ǡ ݔ௨ሿǢ then, the procedure can be summarized 
as follows:
1.	 Define a grid over the feasible region: let Wݔൌݔ௟൅݇ߜǡ ݇ൌͲǡͳǡʹǡ ǥZwhere ߜ defines the 
granularity of the grid. 
2.	 Compute and compare the function values over the grid points to find the minimum.
An illustrative example for one-dimensional minimization is provided below.
Example 3.1: Graphical function minimization in one-dimension
Let the problem be defined as: Minimize ݁௫ subject to ݔଶ൑ͳ Then, to find a solution, we define a 
grid over the feasible region as follows: OHWൌͲǤͲͳǡݔൌെͳǡ െͲǤͻͻǡ ǥ ǡ െͲǤͲͳǡͲǡͲǤͲͳǡǥ ǡͲǤͻͻǡͳ Then, 
݂ሺݔሻൌ݁ିଵǡ ݁ି଴Ǥଽଽǡ ǥ ǡ ݁ି଴Ǥ଴ଵǡ ͳǡ ݁଴Ǥ଴ଵǡ ǥ ǡ ݁଴Ǥଽଽǡ ݁ଵ By comparison, ݂௠௜௡ൌ݁ିଵDWݔൌെͳǤ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
36 
Graphical Optimization
3.2	
Graphical Optimization in Two-Dimensions
Graphical optimization is most useful for optimization problems involving functions of two variables. 
Graphical function minimization in two-dimensions is performed by plotting the contours of the 
objective function along with the constraint boundaries on a two-dimensional grid. In Matlab ©, the 
grid points can be generated with the help of ‘meshgrid’ function. Mathematica © also provide similar 
capabilities.
In the following we discuss three examples of applying graphical method in engineering design 
optimization problems where each problem contains two optimization variables. 
Example 3.2: Hollow cylindrical cantilever beam design (Arora, p. 85)
We consider the minimum-weight design of a cantilever beam of length L, with hollow circular cross-
section (outer radius ܴ௢ inner radius ܴ௜) subjected to a point load P. The maximum bending moment 
on the beam is given as PL, the maximum bending stress is given as:ߪ௔ൌ
௉௅ோ೚
ூǡ and the maximum shear 
stress is given as:߬ൌ
௉
ଷூ൫ܴ௢ଶ൅ܴ଴ܴ௜൅ܴ௜
ଶ൯ǡZKHUHܫൌ
గ
ସሺܴ௢ସെܴ௜
ସሻ is the moment of inertia of the 
cross-section. The maximum allowable bending and shear stresses are given as ߪ௔ and ߬௔, respectively. 
Let the design variables be selected as the outer radius ܴ௢ and the inner radius ܴ௜; then, the optimization 
problem is stated as follows: 
Minimize ݂ሺܴ଴ǡ ܴ௜ሻൌߨߩܮሺܴ଴
ଶെܴ௜
ଶሻ
Subject to: 
ఙ
ఙೌെͳ ൑Ͳǡ
ఛ
ఛೌെͳ ൑ͲǢܴ଴ǡ ܴ௜൑ͲǤʹ݉
The following data are provided for the problem: ܲൌͳͲ݇ܰǡ ܮൌͷ݉ǡ ߪ௔ൌʹͷͲܯܲܽǡ ߬௔ൌͻͲܯܲܽǡ  
ܧൌʹͳͲܩܲܽǡ ߩൌ͹ͺͷͲ݇݃Ȁ݉ଷ After substituting the values, and dropping the constant terms in f,
 the optimization problem is stated as: 
Minimize ݂ሺܴ଴ǡ ܴ௜ሻൌܴ଴
ଶെܴ௜
ଶ
ర
Subject to: ݃ͳǣ
଼ൈଵ଴షరோ೚
గሺோబ
రିோ೔
రሻെͳ ൑ͲǢ ݃ʹǣ
ସ൫ோ೚మାோబோ೔ାோ೔
మ൯
ଶ଻గ൫ோబ
రିோ೔
ర൯
െͳ ൑ͲǢܴ଴ǡ ܴ௜൑ʹͲܿ݉
The graphical solution to the problem, obtained from Matlab, is shown in Figure 3.1. The optimal solution 
is given as: ܴ௢ൌͲǤͳʹ݉ǡ ܴ௜ൌͲǤͳͳͷ݉ǡ ݂כ ൌͲǤͲͲͳͳ͹ͷ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
37 
Graphical Optimization
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.001
0.001
0.001
0.001
0.002
0.002
0.002
0.002
0.003
0.003
0.003
0.004
0.004
0.004
0.005
0.005
0.005
0.006
0.006
0.006
0.007
0.007
0.007
0.008
0.008
0.008
0.009
0.009
0.009
0.01
0.01
0.01
X= 0.12
Y= 0.115
Level= 0.001175
Ro
Ri
Hollow Cylindrical Cantilever Beam Design
Figure 3.1: Graphical solution to the minimum`-weight hollow cantilever beam design (Example 3.2)
Example 3.3: Symmetrical two-bar truss design (Arora, p. 59)
We wish to design a symmetrical two-bar truss to withstand a load ܹൌͷͲ݇ܰ The truss consists of 
two steel tubes pinned together at the top and supported on the ground at the other (figure). The truss 
has a fixed span ݏൌʹ݉ and a height ݄ൌξ݈ଶെͳ where l is the length of the tubes; both tubes have 
a cross-sectional area: ܣൌʹߨܴݐ where R is the radius of the tube and t is the thickness. The objective 
is to design a minimum-weight structure, where total weight is ʹߩ݈ܣ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Fundamental Engineering Optimization 
Methods
38 
Graphical Optimization
The truss design is subject to the following constraints: 
1.	 The height of the truss is to be limited as: ʹ ൑݄൑ͷǢ
2.	 The tube thickness is to be limited as: ܴ൑ͶͷݐǢ
3.	 The maximum allowable stress is given as: ߪ௔ൌʹͷͲܯܲܽǢ
4.	 To prevent buckling, tube loading should not exceed a critical value: ௐ௟
ଶ௛൑
௉೎ೝ
ிௌൌ
ଵ
ிௌ
గమாூ
ሺ௄௟ሻమ 
where ܭൌͲǤ͹ǡ ܧൌʹͳͲܩܲܽǡ the moment of inertia: ܫ؆ ߨܴଷݐǡ and ܨܵൌʹ denotes a 
safety factor.
Let the design variables be selected as: ݄ǡ ܴǡ ݐǢ then, the optimization problem is formulated as:
Minimize ݂ሺ݄ǡ ܴǡ ݐሻൌͶߨߩξ݄ଶ൅ͳܴݐ
Subject to: ݃ͳǣ
ௐξ௛మାଵ
ସగ௛ோ௧ఙೌെͳ ൑Ͳǡ݃ʹǣ
଴Ǥସଽௐ൫௛మାଵ൯
య
మǤ
గయா௛ோయ௧
െͳ ൑Ͳǡ ݃͵ǣܴെͶͷݐ൑Ͳǡ ݃Ͷǣʹ ൑݄൑ͷ
In the above formulation, there are three design variables: ݄ǡ ܴǡ ݐ. Consequently, we need to fix the value 
of one variable in order to perform the graphical design with two variables. We arbitrarily fix ݄ൌ͵݉ 
and graphically solve the resulting minimization problem stated, after dropping the constant terms in 
f , as follows:
Minimize ݂ሺܴǡ ݐሻൌܴݐ
Subject to: ݃ͳǣ
ଵǤ଺଻଻ൈଵ଴షఱ
ோ௧
െͳ ൑Ͳǡ݃ʹǣ
ଷǤଽ଺଺ൈଵ଴షఴ
ோయ௧
െͳ ൑Ͳǡ ݃͵ǣܴെͶͷݐ൑Ͳ
A graph of the objective function and the constraints for the problem is shown in the Figure 3.2. From 
the figure, the optimum values of the design variables are: ܴൌ͵Ǥ͹ܿ݉ǡ ݐൌͲǤͺ݉݉ǡ ݂כ ൌ͵ ൈͳͲିହ.
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
x 10
-3
5e-005
5e-005
5e-005
0.0001
0.0001
0.0001
0.00015
0.00015
0.0002
X= 0.037
Y= 0.0008
Level= 0.001
Two bar truss design
R(m)
t(m)
1e-005
1e-005
1e-005
1e-005
1e-005
2e-005
2e-005
2e-005
2e-005
3e-005
3e-005
3e-005
3e-005
4e-005
4e-005
4e-005
4e-005
Figure 3.2: Graphical solution to the minimum-weight symmetrical two-bar truss design (Example 3.3)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
39 
Graphical Optimization
Example 3.4: Symmetrical three-bar truss design (Arora, p. 46, 86)
We consider the minimum-weight design of a symmetric three-bar truss supported over-head. Members 
1 and 3 have the same cross-sectional area ܣଵ and the middle member 2 has cross-sectional area ܣଶ
Let l be the height of the truss, then the lengths of member 1 and 3 are ξʹ݈ and that of member 2 is l. 
A load P at the joint is applied at an angle ߠ so that the horizontal and vertical components of the applied 
load are given, respectively, as: ܲ௨ൌܲ ߠǡ ܲ௩ൌܲ ߠ The design variables for the problem are 
selected as ܣଵ and ܣଶ The design objective is to minimize the total massൌߩ݈ሺʹξʹܣଵ൅ܣଶሻ 
The constraints in the problem are formulated as follows: 
a)	 The stresses in members 1, 2 and 3, computed as: 
ߪଵൌ
ଵ
ξଶቂ
௉ೠ
஺భ൅
௉ೡ
ሺ஺భାξଶ஺మሻቃǢߪଶൌ
ξଶ௉ೡ
ሺ஺భାξଶ஺మሻǢߪଷൌ 
ଵ
ξଶቂെ
௉ೠ
஺భ൅
௉ೡ
ሺ஺భାξଶ஺మሻቃ, are to be limited 
by the allowable stress for the material. 
b)	 The axial force in members under compression, given as: ܨ௜ൌߪ௜ܣ௜, is limited by the 
buckling load, LHെܨ௜൑
గమாூ
௟೔
మRUെߪ௜൑
గమாఉ஺೔
௟೔
మ
൑ߪ௔ǡ or where the moment of inertia is 
estimated as: ܫ௜ൌߚܣ௜
ଶ ߚൌ constant. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Fundamental Engineering Optimization 
Methods
40 
Graphical Optimization
c)	 The horizontal and vertical deflections of the load point, given as: 
ݑൌξଶ௟௉ೠ
஺భாǡ ݒൌ
ξଶ௟௉ೡ
ሺ஺భାξଶ஺మሻாǡ are to be limited by ݑ൑ο௨ǡ ݒ൑ο௩
d)	 To avoid possible resonance, the lowest eigenvalue of the structure, given as: 
ߞൌ
ଷா஺భ
ఘ௟మሺସ஺భାξଶ஺మሻ where ߩ is the mass density should be higher than a specified frequency, 
i.e., ߞ൒ሺʹߨ߱଴ሻଶ
e)	 The design variables are required to be greater than some minimum value, i.e.,
ܣଵǡ ܣଶ൒ܣ௠௜௡ 
For a particular problem, let ݈ൌͳǤͲ݉ǡ ܲൌͳͲͲ݇ܰǡ ߠൌ͵Ͳιǡ ߩൌʹͺͲͲ
௞௚
௠యǡ ܧൌ͹Ͳܩܲܽǡ ߪ௔ൌ
ͳͶͲܯܲܽǡ ο௨ൌο௩ൌͲǤͷܿ݉ǡ ߱଴ൌͷͲܪݖǡ ߚൌͳǤͲǡܣ௠௜௡ൌʹܿ݉ଶ7KHQܲ௨ൌξଷ௉
ଶǡ ܲ௩ൌ
௉
ଶǢD 
and Then, and the resulting optimal design problem is formulated as:
Minimize ݂ሺܣଵǡ ܣଶሻൌʹξʹܣଵ൅ܣଶ
Subject to: 
݃ͳǣʹǤͷ ൈͳͲିସቈξ͵
ܣଵ
൅
ͳ
൫ܣଵ൅ξʹܣଶ൯
቉െͳ ൑Ͳǡ
݃ʹǣʹǤͷ ൈͳͲିସቈെξ͵
ܣଵ
൅
ͳ
൫ܣଵ൅ξʹܣଶ൯
቉െͳ ൑Ͳǡ
݃͵ǣ
ͷ ൈͳͲିସ
൫ܣଵ൅ξʹܣଶ൯
െͳ ൑Ͳǡ
݃ͶǣͳǤͲʹ ൈͳͲି଻ቈξ͵
ܣଵ
ଶെ
ͳ
ܣଵ൫ܣଵ൅ξʹܣଶ൯
቉െͳ ൑Ͳǡ
݃ͷǣ ͵Ǥͷ ൈͳͲିସ
ܣଵ
െͳ ൑Ͳǡ
݃͸ǣ ʹ ൈͳͲିସ
ܣଵ൅ξʹܣଶ
െͳ ൑Ͳǡ
݃͹ǣ ʹ ൈͳͲିସ
ܣଵ
െͳ ൑Ͳǡ
݃ͺǣ ʹ ൈͳͲିସ
ܣଶ
െͳ ൑Ͳǡ
݃ͻǣͳǤ͵ͳ͸ ൈͳͲିହ൫Ͷܣଵ൅ξʹܣଶ൯െͳ ൑Ͳ
݃ͳͲǣ ʹͶ͸͹ܣଵെͳ ൑Ͳ
The problem was graphically solved in Matlab (see Figure 3.3). The optimum solution is given as: 
ܣଵൌܣଷൌ͸ܿ݉ଶǡ ܣଶൌʹܿ݉ଶǡ ݂כ ൌͲǤͲͲͶͺ͸ܿ݉ଶ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
41 
Graphical Optimization
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x 10
-3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x 10
-3
0.0005
0.001
0.001
0.001
0.0015
0.0015
0.0015
0.002
0.002
0.002
0.0025
0.0025
0.0025
0.003
0.003
0.0035
X= 0.0006
Y= 0.0002
Level= 0.004864
A1
A2
Figure 3.3: Graphical solution to the minimum-weight symmetrical three-bar truss design (Example 3.4)
Appendix to Chapter 3: Matlab Code for Examples 3.2–3.4 
Example 3.2: Cantilever beam design (Arora, Prob. 2.23, p. 64)
% cantilever beam design, Prob. 2.23 (Arora)
ro=.01:.005:.2;
ri=.01:.005:.2;
[Ro,Ri]=meshgrid(ro,ri);
F=Ro.*Ro-Ri.*Ri;
G1=8e-4/pi*Ro./(Ro.^4-Ri.^4)-1;
G2=4/27/pi*(Ro.*Ro+Ro.*Ri+Ri.*Ri)./(Ro.^4-Ri.^4)-1;
figure, hold
contour(ro,ri,G1,[0 0])
contour(ro,ri,G2,[0 0])
[c,h]=contour(ro,ri, F, .001:.001:.01);
clabel(c,h);
Example 3.3: Two-bar truss design (Arora, Prob. 2.16, p. 61)
% two-bar trus; prob. 2.16 (Arora)
W=50e3;
r=0:.001:.05; 
t=0:.0001:.005;
[R,T]=meshgrid(r,t);
F=R.*T;
G1=sqrt(10)*W/12/pi./(250e6*R.*T)-1; 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
42 
Graphical Optimization
G2=4.9*sqrt(10)*W/3/pi/pi/pi./(210e9*R.^3.*T)-1;
G3=R-45*T;
figure, hold
contour(r,t,G1,[0 0]), pause
contour(r,t,G2,[0 0]), pause
contour(r,t,G3,[0 0]), pause
[c,h]=contour(r,t,F);
clabel(c,h)
Example 3.4: Symmetric three-bar truss (Arora, Prob. 3.29, p. 86)
%three-bar truss prob. 3.29 (Arora)
a1=0:1e-4:1e-3;
a2=0:1e-4:1e-3;
[A1,A2]=meshgrid(a1,a2);
F=2*sqrt(2)*A1+A2;
G1=2.5e-4*(sqrt(3)./A1+1./(A1+sqrt(2)*A2))-1;
G2=2.5e-4*(-sqrt(3)./A1+1./(A1+sqrt(2)*A2))-1;
G3=5e-4./(A1+sqrt(2)*A2)-1;
G4=1.02e-7*(sqrt(3)./A1./A1-1./A1./(A1+sqrt(2)*A2))-1;
G5=3.5e-4./A1-1;
G6=2e-4./(A1+sqrt(2)*A2)-1;
G7=2e-4./A1-1;
G8=2e-4./A2-1;
G9=1.316e-5*(A1+sqrt(2)*A2)-1;
G10=2467*A1-1;
figure, hold
contour(a1,a2, G1,[0 0]), pause
contour(a1,a2, G2,[0 0]), pause
contour(a1,a2, G3,[0 0]), pause
contour(a1,a2, G4,[0 0]), pause
contour(a1,a2, G5,[0 0]), pause
contour(a1,a2, G6,[0 0]), pause
contour(a1,a2, G7,[0 0]), pause
contour(a1,a2, G8,[0 0]), pause
contour(a1,a2, G9,[0 0]), pause
contour(a1,a2, G10,[0 0]), pause
[c,h]=contour(a1,a2,F); 
clabel(c,h)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
43 
Mathematical Optimization
4	 Mathematical Optimization
In this chapter we discuss the mathematical optimization problem, including its formulation and the 
techniques to solve it. The mathematical optimization problem involves minimization (or maximization) 
of a real-valued cost function by systematically choosing the values of a set of variables that are subject 
to inequality and/or equality constraints. Both cost and constraint functions are assumed analytical 
so that they can be locally approximated by Taylor series and their first and second derivatives can be 
computed. The analytical techniques used to solve the optimization problem include determination of 
first and second order necessary conditions that reveal a set of possible candidate points, which are then 
evaluated using sufficient conditions for an optimum. In convex optimization problems the feasible 
region, i.e., the set of points that satisfy the constraints, is a convex set and both object and constraint 
functions are also convex. In such problems, the existence of a single global minimum is assured.
Learning Objectives: the learning goals in this chapter are:
1.	 Understand formulation of unconstrained and constrained optimization problems
2.	 Learn the application of first and second order necessary conditions to solve optimization 
problems
3.	 Learn solution techniques used for convex optimization problems
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
44 
Mathematical Optimization
4.	 Understand the geometric viewpoint associated with optimization algorithms
5.	 Understand the concept of Lagrangian duality and how it helps toward finding a solution.
6.	 Learn the techniques used for post-optimality analysis for nonlinear problems
4.1	
The Optimization Problem
The general nonlinear optimization problem (the nonlinear programming problem) is defined as:

࢞݂ሺ࢞ሻ
6XEMHFWWRቐ
݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈Ǣ
݃௝ሺ࢞ሻ൑Ͳǡ ݆ൌ݅ǡ ǥ ǡ ݉Ǣ
ݔ௜௅൑ݔ௜൑ݔ௜௎ǡ ݅ൌͳǡ ǥ ǡ ݊


(4.1)
The above problem assumes minimization of a multi-variable scalar cost function ݂ሺ࢞ሻǡZKHUH࢞א Թ௡ 
்࢞ൌሾݔଵǡ ݔଶǡ ǥ ǡ ݔ௡ሿ that is subjected to ݈ equality constraints and ݉Ǣ inequality constraints. Additionally, 
lower and upper bounds on the optimization variables are considered, where these bounds may be 
grouped with the inequality constraints. 
Special cases involving variants of the general problem can also be considered. For example, the absence 
of both equality and inequality constraints specifies an unconstrained optimization problem; the problem 
may only involve a single type of constraints; the linearity of the objective and constraint functions 
specifies a linear programming problem (discussed in Chapter 5); and the restriction of optimization 
variables to a discrete set of values specifies a discrete optimization problem (discussed in Chapter 6).
We begin with defining the feasible region for the optimization problem and a discussion of the existence 
of points of minima or maxima of the objective function in that region.
Feasible Region. The set ȳ ൌ൛ݔǣ݄௜ሺ࢞ሻൌͲǡ ݃௝ሺ࢞ሻ൑Ͳǡ ݔ௜௅൑ݔ௜൑ݔ௜௎ൟ is termed as the feasible 
region for the problem. If the feasible region is convex, and additionally ݄௜ǡ ݃௝ are convex functions, 
then the problem is a convex optimization problem with some obvious advantages, e.g., f only has a 
single global minimum in Ω.
The Extreme Value Theorem in Calculus (attributed to Karl Weierstrass) provides sufficient conditions 
for the existence of minimum (or maximum) of a function defined over a complex domain. The theorem 
states: A continuous function ݂ሺ࢞ሻ defined over a closed and bounded set ߗك ܦሺ݂ሻ attains its maximum 
and minimum in Ω. 
Thus, according to this theorem, if the feasible region Ω of the problem is closed and bounded, a minimum 
for the problem exists. The rest of the book discusses various ways to find that minimum. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
45 
Mathematical Optimization
Finding the minimum is relatively easy in the case of linear programming problems, but could be 
considerably difficult in the case of nonlinear problems with an irregular of the constraint surface. As 
a consequence, numerical methods applied to a nonlinear problem may only return a local minimum. 
Stochastic methods, such as Simulated Annealing, have been developed to find a global minimum with 
some certainty in the case of nonlinear problems. These methods are, however, not covered in this text.
Finally, we note that the convexity property, if present, helps in finding a solution to the optimization 
problem. If convexity can be ascertained through application of appropriate techniques, then we are at 
least assured that any solution found in the process would be the global solution. 
4.2	
Optimality criteria for the Unconstrained Problems
We begin by reviewing the concept of local and global minima and a discussion of the necessary and 
sufficient conditions for existence of a solution. 
Local Minimum. A point x* is a local minimum of I ݂LI݂ሺ࢞כሻ൑݂ሺ࢞ሻ in a neighborhood defined by 
ȁ࢞െ࢞כȁ ൏ߜIRUVRPHߜ൐Ͳ 
Global Minimum. The point x* is a global minimum if I ݂ሺ࢞כሻ൑݂ሺ࢞ሻ࢞א ȳǡ where Ω is the feasible 
region. Further, the point x* is a strong global minimum if: ݂ሺ࢞כሻ൏݂ሺ࢞ሻ࢞א ȳ 
The local and global minima are synonymous in the case of convex optimization problems. In the 
remaining cases, a distinction between the two needs to be made. Further, local or global minimum in 
the case of non-convex optimization problems is not necessarily unique.
Necessary and Sufficient Conditions. The conditions that must be satisfied at the optimum point are 
termed as necessary conditions. However, the set of points that satisfies the necessary conditions further 
includes maxima and points of inflection. The sufficient conditions are then used to qualify the solution 
point as an optimum point. If a candidate point satisfies the sufficient conditions, then it is indeed the 
optimum point.
We now proceed to derive the first and second order conditions of optimality in the case of unconstrained 
optimization problems.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
46 
Mathematical Optimization
4.2.1	
 First Order Necessary Conditions (FONC)
We consider a multi-variable function ݂ሺ࢞ሻൌ݂ሺݔଵǡ ݔଶǡ ǥ ǡ ݔ௡ሻ and wish to investigate the behavior 
of a candidate point x*. By definition, the point x* is a local minimum of I ݂ሺ࢞ሻRQO\LI݂ሺ࢞כሻ൑݂ሺ࢞ሻ 
only if in the neighborhood of x*. To proceed, let ߜ࢞ൌ࢞െ࢞כ define a small neighborhood around 
x*, then we may use first-order Taylor series expansion of ݂ given as: 
J
݂ሺ࢞ሻൌ݂ሺ࢞כሻ൅׏݂ሺ࢞כሻ்ߜ࢞ to 
express the condition for local minimum as:
ߜ݂ൌ׏݂ሺ࢞כሻ்ߜ࢞൒Ͳ
(4.2)
We first note that the above condition is satisfied for ׏݂ሺ࢞כሻൌͲ Further, since ߜ࢞ is arbitrary, ׏݂ሺ࢞כሻ 
must be zero to satisfy the above non-negativity condition on Qߜ݂ Therefore, the first-order necessary 
condition (FONC) for optimality of I ݂ሺ࢞ሻ is stated as follows: 
FONC: If I ݂ሺ࢞ሻ has a local minimum at x*, then ׏݂ሺ࢞כሻൌͲ, or equivalently, 
డ௙ሺ࢞כሻ
డ௫ೕ
ൌͲǡ ݆ൌͳǡ ǥ ǡ ݊Ǥ 
The points that satisfy FONC are called stationary points of I ݂ሺ࢞ሻ. Besides minima, these points include 
maxima and the points of inflection. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
47 
Mathematical Optimization
4.2.2	
Second Order Conditions (SOC)
Assume now that FONC are satisfied, i.e., ׏݂ሺ࢞כሻൌͲ Then, we may use second-order Taylor series 
expansion of I ݂ሺ࢞ሻ to write the optimality condition as:

ߜ݂ൌߜ்࢞׏ଶ݂ሺ࢞כሻߜ࢞൒Ͳ
(4.3)
As ߜ࢞ are arbitrary, the above quadratic form is positive (semi)definite if and only if the Hessian matrix, 
׏ଶ݂ሺ࢞כሻ is positive (semi)definite. Therefore, the second order necessary condition (SONC) is stated as:
SONC: If ࢞כ is a local minimizer of I ݂ሺ࢞ሻ , then ߘଶ݂ሺ࢞כሻ൒Ͳ 
Whereas, a second order sufficient condition (SOSC) is stated as: 
SOSC: If ࢞כ satisfies ߘଶ݂ሺ࢞כሻ൐Ͳ , then ࢞כ is a local minimizer of I ݂ሺ࢞ሻ. 
Further, if ׏ଶ݂ሺ࢞כሻ is indefinite, then x* is an inflection point. In the event that ׏݂ሺ࢞כሻൌ׏ଶ݂ሺ࢞כሻൌͲ, 
the lowest nonzero derivative must be even-ordered for stationary points (necessary condition), and it 
must be positive for local minimum (sufficient condition).
Two examples of unconstrained optimization problems are now considered:
Example 4.1: Polynomial data-fitting
As an example of unconstrained optimization, we consider the polynomial data-fitting problem defined 
in Sec. 2.11. The problem is to fit an th degree polynomial:݌ሺݔሻൌσ
݌௝ݔ௝
௡
௝ୀ଴
W to a set of data points: 
ሺݔ௜ǡ ݕ௜ሻǡ ݅ൌͳǡ ǥ ǡ ܰ൐݊7 The objective is to minimize the mean square error (MSE, also termed as the 
variance of data points). The resulting unconstrained minimization problem is formulated as:

௣ೕ݂൫݌௝൯ൌͳ
ʹܰ෍
൫ݕ௜െሺ݌଴൅݌ଵݔ௜൅ڮ ൅݌௡ݔ௜
௡ሻ൯
ଶ
ே
௜ୀଵ

Then, the FONC for the problem are given as: 
߲݂
߲݌௝
ൌͳ
ܰ෍
൫ݕ௜െሺ݌଴൅݌ଵݔ௜൅ڮ ൅݌௡ݔ௜
௡ሻ൯ሺെݔ௜
௝ሻ
ே
௜ୀଵ
ൌͲ
For ݊ൌͳ they result in the following equations: 
 ቌ
ͳ
ଵ
ேσ ݔ௜
௜
ଵ
ேσ ݔ௜
௜
ଵ
ேσ ݔ௜
ଶ
௜
ቍቀ݌଴
݌ଵቁൌቌ
ଵ
ேσ ݕ௜
௜
ଵ
ேσ ݔ௜ݕ௜
௜
ቍ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
48 
Mathematical Optimization
As 
డమ௙
డ௣ೕ௣ೖൌ
ଵ
ேσ ݔ௜
௝ା௞
௜
 the SONC for the problem are evaluated as:
൮
ͳ
ڮ
ଵ
ேσ ݔ௜
௡
௜
ڭ
ڰ
ڭ
ଵ
ேσ ݔ௜
௡
௜
ڮ
ଵ
ேσ ݔ௜
ଶ௡
௜
൲൒Ͳ
 
For ݊ൌͳ the determinant of the Hessian evaluates as: ଵ
ேσ ݔ௜
ଶ
௜
െቀ
ଵ
ேσ ݔ௜
௜
ቁ
ଶ
 which defines the variance 
in the case of independent and identically distributed random variables. 
Finally, we note that since the data-fitting problem is convex, FONC are both necessary and sufficient 
for a minimum. 
Example 4.2: Open box problem
We wish to determine the dimensions of an open box of maximum volume that can be constructed 
form a sheet of paper (8.5 ×11 in) by cutting squares from the corners and folding the sides upwards. 
Let x denote the width of the paper that is folded up, then the problem is formulated as: 

௫
݂ሺݔሻൌሺͳͳ െʹݔሻሺͺǤͷ െʹݔሻݔ
The FONC for the problem evaluate as: ݂ᇱሺݔሻൌʹݔሺͳͻǤͷ െͶݔሻെሺͳͳ െʹݔሻሺͺǤͷ െʹݔሻൌͲ 
Using Matlab Symbolic toolbox ‘solve’ command, we obtain two candidate solutions: ݔכ ൌͳǤͷͺͷǡ ͶǤͻͳͷ
Application of SOC results in: ݂ᇱᇱሺݔሻൌെ͵ͻǤͻͷǡ͵ͻǤͻͷ respectively, indicating a maximum of I ݂ሺ࢞ሻ at 
ݔכ ൌͳǤͷͺͷZLWK݂ሺݔכሻൌ͸͸ǤͳͷFXLQ
4.3	
Optimality Criteria for the Constrained Problems
The majority of engineering design problems involve constraints (LE, GE, EQ) that are modeled as 
functions of optimization variables. In this section, we explore how constraints affect the optimality 
criteria. An important consideration when applying the optimality criteria to problems involving 
constraints is whether x* lies on a constraint boundary. This is implied in the case for problems involving 
only equality constraints, which are discussed first.
4.3.3	
Equality Constrained Problems
The optimality criteria for equality constrained problems involve the use of Lagrange multipliers. To 
develop this concept, we consider a problem with a single equality constraint, stated as:

࢞݂ሺ࢞ሻ
VXEMHFWWR݄ሺ࢞ሻൌͲ
(4.4)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
49 
Mathematical Optimization
We first note that the constraint equation can be used to solve for and substitute one of the variables (say 
ݔ௡) in the objective function, and hence develop an unconstrained optimization problem in variables. n-1 
This, however, depends on the form of ݄ሺ࢞ሻ and may not always be feasible. In order to develop more 
general optimality criteria, we follow Lagrange’s approach to the problem and consider the variation in 
the objective and constraint functions at a stationary point, given as:
݂݀ൌ߲݂
߲ݔଵ
݀ݔଵ൅ڮ ൅߲݂
߲ݔ௡
݀ݔ௡ൌͲ
(4.5)
݄݀ൌ߲݄
߲ݔଵ
݀ݔଵ൅ڮ ൅߲݄
߲ݔ௡
݀ݔ௡ൌͲ
We now combine these two conditions via a scalar weight (Lagrange multiplier, λ) to write:
෍
ቆ߲݂
߲ݔ௝
൅ߣ߲݄
߲ݔ௝
ቇ݀ݔ௝
௡
௝ୀଵ
ൌͲ
(4.6)
Since variations ݀ݔ௝ are independent, the above condition implies that: డ௙
డ௫ೕ൅ߣ
డ௛
డ௫ೕൌͲǡ ݆ൌͳǡ ǥ ǡ ݊. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Fundamental Engineering Optimization 
Methods
50 
Mathematical Optimization
We further note that application of FONC to a Lagrangian function defined as:ࣦሺ࢞ǡ ߣሻൌ݂ሺ࢞ሻ൅ߣ݄ሺ࢞ሻ 
also gives rise to the above condition. For multiple equality constraints, the Lagrangian function is 
similarly formulated as:
ࣦሺ࢞ǡ ࣅሻൌ݂ሺ࢞ሻ൅෍
ߣ௜݄௜ሺ࢞ሻ
௟
௜ୀଵ
 
(4.7)
Then, in order for ݂ሺ࢞ሻ to have a local minimum at x*, the following FONC must be satisfied:
߲ࣦ
߲ݔ௝
ൌ߲݂
߲ݔ௝
൅෍
ߣ௜
߲݄௜
߲ݔ௝
ݔ௝
௟
௜ୀଵ
ൌͲǢ ݆ൌͳǡ ǥ ǡ ݊
(4.8)
݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈
The above FONC can be equivalently stated as: 
׏࢞ࣦሺ࢞כǡ ࣅכሻൌͲǡ
׏ࣅࣦሺ࢞כǡ ࣅכሻൌͲ
These conditions suggest that ࣦሺ࢞כǡ ࣅכሻ is stationary with respect to both x and λ; therefore, minimization 
of ࣦሺ࢞ǡ ࣅሻ amounts to an unconstrained optimization problem. Further, the Lagrange Multiplier Theorem 
(Arora, p.135) states that if x* is a regular point (defined below) then the FONC result in a unique 
solution to Rߣ௜
כ
We note that the above FONC further imply: ׏݂ሺ࢞כሻൌെσ
ߣ௜׏݄௜ሺ࢞כሻ
௣
௜ୀଵ
 Algebraically, it means that 
the cost function gradient is a linear combination of the constraint gradients. Geometrically, it means 
that the negative of the cost function gradient lies in the convex cone spanned by the constraint normals 
(Sec. 4.3.5). 
SOSC for equality constrained problems are given as: ׏ଶࣦሺ࢞כǡ ࣅכሻൌ׏ଶ݂ሺ࢞כሻ൐Ͳ Further discussion 
on SOC for constrained optimization problems is delayed till Sec. 4.4.3. 
An example is now presented to explain the optimization process for equality constrained problems.
Example 4.3: We consider the following optimization problem:

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌെݔଵݔଶ
Subject to: ݄ሺݔଵǡ ݔଶሻൌݔଵ
ଶ൅ݔଶ
ଶെͳ ൌͲ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
51 
Mathematical Optimization
We first note that the equality constraint can be used to develop an unconstrained problem in one 
variable, given as: ௫భ݂ሺݔଵሻൌെݔଵඥͳ െݔଵ
ଶRU௫మ݂ሺݔଶሻൌെݔଶඥͳ െݔଶ
ଶ or . Instead, we follow 
the Lagrangian approach to solve the original problem below.
Next, we assess the objective function and observe that the origin is saddle point of the function. It is 
also instructive to review the problem from a graphical perspective (Figure 4.1). The figure shows the 
feasible region, i.e., the perimeter of a unit circle superimposed on the level sets of the objective function. 
Then, by inspection, the optimum can be located in the first and the third quadrant where the level 
curves are tangent to the circle.
The Lagrangian function for the problem is formulated as: ࣦሺݔଵǡ ݔଶǡ ߣሻൌെݔଵݔଶ൅ߣሺݔଵ
ଶ൅ݔଶ
ଶെͳሻ 
The FONC evaluate as: ʹߣݔଵെݔଶൌͲǡ ʹߣݔଶെݔଵൌͲǡ ݔଵ
ଶ൅ݔଶ
ଶെͳ ൌͲ 
Thus, there are four candidate solutions at: ሺݔଵ
כǡ ݔଶ
כሻൌቀേ
ଵ
ξଶǡ േ
ଵ
ξଶቁǡ ߣכ ൌേ
ଵ
ଶ 
The SONC for the problem evaluate as: ቂʹߣ
െͳ
െͳ
ʹߣቃ൒Ͳ Application of SONC reveals multiple minima 
at ሺݔଵ
כǡ ݔଶ
כሻൌቀ
ଵ
ξଶǡ
ଵ
ξଶቁ׫ ቀെ
ଵ
ξଶǡ െ
ଵ
ξଶቁZLWK݂ሺݔଵ
כǡ ݔଶ
כሻൌെ
ଵ
ଶ
Figure 4.1: Level sets of the objective function superimposed on the equality constraint.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
52 
Mathematical Optimization
The above example underscores some of the pitfalls in the case of nonlinear optimization problems: 
Application of FONC results in multiple nonlinear equations whose simultaneous solution reveals several 
candidate points, which pertain to maxima, minima and points of inflection. The minima may then be 
obtained via application of SOSC or via a comparison of function values at the individual points.
4.3.4	
 Inequality Constrained Problems
We next consider an optimization problem involving a single inequality constraint. The problem is 
stated as:

࢞݂ሺ࢞ሻ
VXEMHFWWR݃ሺ࢞ሻ൑Ͳ
(4.10)
We note that we can add a slack variable to the inequality constraint to turn it into equality. Further, to 
ensure constraint compliance, the slack variable is restricted to be non-negative. We therefore replace 
the inequality constraint with equality: ݃ሺ࢞ሻ൅ଶൌͲ A Lagrangian function for the problem is now 
developed as:
ࣦሺ࢞ǡ ߣǡ ݏሻൌ݂ሺ࢞ሻ൅ߣሺ݃ሺ࢞ሻ൅ଶሻ
(4.11)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Fundamental Engineering Optimization 
Methods
53 
Mathematical Optimization
The resulting FONC evaluate as: 
׏ࣦሺ࢞כǡ ߣכǡ ݏכሻൌ׏݂ሺ࢞כሻ൅ߣכ׏݃ሺ࢞כሻൌͲ
݃ሺ࢞כሻ൅כଶൌͲ
߲ࣦ
߲ݏൌʹߣכݏכ ൌͲ

(4.12)
The latter condition, known as the switching or complementarity condition, further evaluates as: ߣൌͲ 
(thus implying an inactive constraint) or ݏൌͲ (implying an active/binding constraint). Each of these 
cases is to be explored for feasible solutions, which can be checked for optimality via application of SOC. 
We note that by substituting: ଶൌെ݃ሺ࢞כሻ the FONC can be equivalently expressed as: ׏݂ሺ࢞כሻൌͲǡ
݃ሺ࢞כሻ൑Ͳǡ ߣכ݃ሺ࢞כሻൌͲwhich provides an equivalent characterization of FONC in the case of 
inequality constrained problems.
Finally, the above results can be extended to multiple inequality constraints as: 
ࣦሺ࢞ǡ ߣǡ ݏሻൌ݂ሺ࢞ሻ൅෍ߣ௜൫݃௜ሺ࢞ሻ൅ݏ௜
ଶ൯
௜

(4.13)
Then, in order for ݂ሺ࢞ሻ to have a local minimum at x*, the following FONC must be satisfied:
߲ࣦ
߲ݔ௝
ൌ߲݂
߲ݔ௝
൅෍
ߣ௜
߲݃௜
߲ݔ௝
ݔ௝
௠
௜ୀଵ
ൌͲǢ ݆ൌͳǡ ǥ ǡ ݊ 
(4.14)
݃௜ሺ࢞ሻ൅ݏ௜
ଶൌͲǡ ݅ൌͳǡ ǥ ǡ ݉
ߣ௜ݏ௜ൌͲǡ ݅ൌͳǡ ǥ ǡ ݉
where we note that for ݉ inequality constraints, application of the switching conditions results in cases, 
each of which needs to be explored for feasibility and optimality. 
Next, we present an example of the inequality constrained problem.
Example 4.4: We consider the following optimization problem:

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌെݔଵݔଶ
6XEMHFWWR݃ሺݔଵǡ ݔଶሻൌݔଵ
ଶ൅ݔଶ
ଶെͳ ൑Ͳ
The graphical consideration of the equality constrained problem was earlier presented in Fig. 4.1. From 
that figure, it is obvious that the inequality constrained problem will have a solution at the boundary of the 
constraint set, i.e., at the perimeter of the circle. This view is supported by the analysis presented below.
We first convert the inequality to equality constraint via: ݃ሺݔଵǡ ݔଶሻ൅ݏଶൌݔଵ
ଶ൅ݔଶ
ଶെͳ ൅ݏଶൌͲ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
54 
Mathematical Optimization
Then, the Lagrangian function is formulated as: ࣦሺݔଵǡ ݔଶǡ ߣǡ ݏሻൌെݔଵݔଶ൅ߣሺݔଵ
ଶ൅ݔଶ
ଶ൅ݏଶെͳሻ 
The resulting FONC evaluate as: ʹߣݔଵെݔଶൌͲǡ ʹߣݔଶെݔଵൌͲǡ ݔଵ
ଶ൅ݔଶ
ଶ൅ݏଶെͳ ൌͲǡ ߣݏൌͲ 
The switching condition further evaluates as: ߣכ ൌͲRUݏכ ൌͲ The former condition evaluates as: 
ሺݔଵ
כǡ ݔଶ
כሻൌሺͲǡͲሻǡ ݏכ ൌേͳ while latter condition evaluates as: ሺݔଵ
כǡ ݔଶ
כሻൌቀേ
ଵ
ξଶǡ േ
ଵ
ξଶቁǡ ߣכ ൌേ
ଵ
ଶ 
Function evaluation at the candidate points reveals multiple minima at ሺݔଵ
כǡ ݔଶ
כሻൌቀ
ଵ
ξଶǡ
ଵ
ξଶቁ׫ ቀെ
ଵ
ξଶǡ െ
ଵ
ξଶቁ 
with ݂ሺݔଵ
כǡ ݔଶ
כሻൌെ
ଵ
ଶ
4.4	
Optimality Criteria for General Optimization Problems
The general nonlinear optimization problem was defined in (4.1) above, where we can group the variable 
limits with the inequality constraints to state the problem as:

࢞݂ሺ࢞ሻ
(4.15)
6XEMHFWWR݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈Ǣ݃௝ሺ࢞ሻ൑Ͳǡ ݆ൌ݅ǡ ǥ ǡ ݉
The feasible region for the problem is given as: 
ȳ ൌ൛࢞ǣ݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈Ǣ݃௝ሺ࢞ሻ൑Ͳǡ ݆ൌͳǡ ǥ ǡ ݉ൟ
(4.16)
To solve the problem via the Lagrangian function approach, we first add slack variables to the inequality 
constraints; we then associate Lagrange multiplier vectors u and v with the inequality and equality 
constraints, respectively, and develop a Lagrangian function, which is given as:
ࣦሺ࢞ǡ ࢛ǡ ࢜ǡ ࢙ሻൌ݂ሺ࢞ሻ൅෍
ݒ௜݄௜ሺ࢞ሻ
௟
௜ୀଵ
൅෍
ݑ௝ሺ݃௝ሺ࢞ሻ൅ݏ௝
ଶ
௠
௝ୀଵ
ሻ 
(4.17)
The resulting FONC evaluate as:
1.	 Gradient conditions: డࣦ
డ௫ೖൌ
డ௙
డ௫ೖ൅σ
ݒ௜
כ డ௛೔
డ௫ೖ
௟
௜ୀଵ
൅σ
ݑ௝
כ డ௚ೕ
డ௫ೖ
௠
௝ୀଵ
ൌͲǢ ݇ൌͳǡ ǥ ǡ ݊ 
2.	 Switching conditions: ݑ௝
כݏ௝ൌͲǡ ݆ൌͳǡ ǥ ǡ ݉
3.	 Feasibility conditions: ݃௝ሺ࢞כሻ൑Ͳǡ ݆ൌͳǡ ǥ ǡ ݉Ǣ݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݌
4.	 Non-negativity condition: ݑ௝
כ ൒Ͳǡ ݆ൌͳǡ ǥ ǡ ݉
5.	 Regularity condition: for those ݑ௝
כǡ that satisfy \ݑ௝
כ ൐Ͳǡ ׏݃௝ሺ࢞כሻ are linearly independent
The above FONC are better known as the KKT (Krush-Kuhn-Tucker) conditions. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
55 
Mathematical Optimization
We note that ࢞ǡ ࢛ǡ ࢜ǡ ࢙ are, respectively, ݊݉݉ and ݈G dimensional vectors. Thus, the total number 
of variables in the problem is: ݊൅ʹ݉൅݈ meaning simultaneous nonlinear equations must be solved 
to obtain a candidate solution. Further, in accordance with the switching conditions, a total of ʹ௠ such 
solutions must be explored.
Further, since ݏ௝
ଶൌെ݃௝ሺ࢞ሻ non-negativity of ݏ௝
ଶ ensures feasibility of the inequality constraint. 
Therefore ݏ௝
ଶൌͲ, implies an active constraint, whereby \ݑ௝
כ ൐Ͳ and an inactive constraint is implied 
by: ݑ௝
כ ൌͲǡݏ௝
ଶ൐Ͳ We also note that for regular points the Lagrange Multiplier Theorem (Arora, p.135) 
ensures a unique solution to the Lagrange multipliers ݒ௜
כ and ݑ௝
כ
An example of general optimization problem is presented below.
Example 4.5: We consider adding a linear equality constraint to Example 4.4 above:


௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌെݔଵݔଶ
Subject to:݃ሺݔଵǡ ݔଶሻǣ ݔଵ
ଶ൅ݔଶ
ଶെͳ ൑ͲǢ ݄ሺݔଵǡ ݔଶሻǣݔଵ൅ݔଶെܿൌͲ
We first convert the inequality to equality constraint via: ݃ሺݔଵǡ ݔଶሻ൅ݏଶൌݔଵ
ଶ൅ݔଶ
ଶെͳ ൅ݏଶൌͲ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Fundamental Engineering Optimization 
Methods
56 
Mathematical Optimization
We then use Lagrange multipliers to formulate a Lagrangian function, which is given as:
ࣦሺݔଵǡ ݔଶǡ ݑǡ ݒǡ ݏሻൌെݔଵݔଶ൅ݑሺݔଵ
ଶ൅ݔଶ
ଶ൅ݏଶെͳሻ൅ݒሺݔଵ൅ݔଶെܿሻ
The resulting KKT conditions evaluate as: ʹݑݔଵ൅ݒെݔଶൌͲǡ ʹݑݔଶ൅ݒെݔଵൌͲǡ ݔଵ൅ݔଶെܿൌͲǡ
ݔଵ
ଶ൅ݔଶ
ଶ൅ݏଶെͳ ൌͲǡݑݏൌͲ From the switching condition: or ݑכ ൌͲRUݏכ ൌͲ
The former condition evaluates as: ሺݔଵ
כǡ ݔଶ
כሻൌቀ
௖
ଶǡ
௖
ଶቁǡ ݏכ ൌേටͳ െ
௖మ
ଶǡ ݒכ ൌ
௖
ଶ
The latter condition has no feasible solution. 
Function evaluation at the sole candidate points results in: ݂ሺݔଵ
כǡ ݔଶ
כሻൌെ
௖మ
ସ
4.4.1	
Optimality Criteria for Convex Optimization Problems
In this section we consider the class of optimization problems where the feasible region is a convex set 
and the objective and constraint functions are convex. The convexity property is desirable since it implies 
the existence of a global minimum to the optimization problem.
We consider the general optimization problem defined in (4.15) with the feasible region given by (4.16). 
Then Ω, is a convex set if functions ݄௜ are linear and ݃௝ are convex. If additionally ݂ሺ࢞ሻ is a convex 
function, then the optimization problem is convex.
We now assume that ݂ሺ࢞ሻ is a convex function defined over a convex set Ω. Then, if ݂ሺ࢞ሻ attains a local 
minimum at ࢞כ א ȳ then x* is also a global minimum over Ω. Furthermore, ݂ሺ࢞כሻ is a local/global 
minimum if and only if it satisfies the KKT conditions, i.e., the KKT conditions are both necessary and 
sufficient for a global minimum in the case of convex optimization problems. 
We, however, note that convexity is a sufficient but not necessary condition for a global minimum, i.e., 
nonexistence of convexity does not preclude the existence of a global minimum. An example of a convex 
optimization problem is presented below.
Example 4.6: We consider the following optimization problem:
௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌݔଵ
ଶ൅ݔଶ
ଶെݔଵݔଶ
subject to ݃ሺݔଵǡ ݔଶሻǣݔଵ
ଶ൅ݔଶ
ଶെͳ ൑ͲǢ ݄ሺݔଵǡ ݔଶሻǣݔଵ൅ݔଶെܿൌͲ
As was done in Example 4.5, we convert the inequality constraint to equality via: ݔଵ
ଶ൅ݔଶ
ଶെͳ ൅ݏଶൌͲ
We then use Lagrange multipliers to formulate a Lagrangian function given as:
ࣦሺݔଵǡ ݔଶǡ ݑǡ ݒǡ ݏሻൌݔଵ
ଶ൅ݔଶ
ଶെݔଵݔଶ൅ݑሺݔଵ
ଶ൅ݔଶ
ଶ൅ݏଶെͳሻ൅ݒሺݔଵ൅ݔଶെܿሻ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
57 
Mathematical Optimization
The resulting KKT conditions evaluate as: ሺʹݑ൅ͳሻݔଵ൅ݒെݔଶൌͲǡ ሺʹݑ൅ͳሻݔଶ൅ݒെݔଵൌͲǡ  
ݔଵ൅ݔଶെܿൌͲǡ ݔଵ
ଶ൅ݔଶ
ଶ൅ݏଶെͳ ൌͲǡ ݑݏൌͲ From the switching condition: ݑכ ൌͲRUݏכ ൌͲ
Similar to Example 4.5, the former condition evaluates as: ሺݔଵ
כǡ ݔଶ
כሻൌቀ
௖
ଶǡ
௖
ଶቁǡ ݏכ ൌേටͳ െ
௖మ
ଶǡ ݒכ ൌ
௖
ଶǢ  
while the latter condition has no feasible solution. Function evaluation at the sole candidate points results 
in:݂ሺݔଵ
כǡ ݔଶ
כሻൌെ
௖మ
ସ 
4.4.2	
A Geometric Viewpoint
The optimality criteria for constrained optimization problems have geometrical connotations. The 
following definitions help understand the geometrical viewpoint associated with the KKT conditions.
Active constraint set. The set of active constraints at x is defined as: ࣣൌ൛݅׫ ݆ǣ ݄௜ሺ࢞ሻൌͲǡ ݃௝ሺ࢞ሻൌͲൟ
The set of active constraint normals is given as: ࣭ൌሼ׏݄௜ሺ࢞ሻǡ ׏݃௝ሺ࢞ሻǡ ݆א ࣣሽ
Constraint tangent hyperplane. The constraint tangent hyperplane is defined by the set of vectors 
࣭ୄൌ൛ࢊǣ׏݄௜ሺ࢞ሻ்ࢊൌͲǡ ׏݃௝ሺ࢞ሻ்ࢊൌͲǡ ݆א ࣣൟ
Regular point. Assume x is a feasible point. Then, x is a regular point if the vectors in the active constraint 
set ࣭ are linearly independent. 
Feasible direction. Assume that x is a regular point. A vector d is a feasible direction if ݄௜ሺ࢞ሻ்ࢊൌͲǡ 
׏݃௝ሺ࢞ሻ்ࢊ൏Ͳǡ ݆א ࣣǢ where the feasibility condition for each active inequality constraint defines a half 
space. The intersection of those half spaces is a feasible cone within which a feasible vector d should lie.
Descent direction. A direction d is a descent direction if the directional derivative of f along d is negative, 
i.e., ׏݂ሺ࢞ሻ்ࢊ൏Ͳ
Extreme point. Assume x is a feasible point. Then, x is an extreme point if the active constraint set ࣣ 
at x is non-empty; otherwise it is an interior point.
Assume now that we are at an extreme point x of the feasible region. We seek a search direction which is 
both descent and feasible. If no such direction can be found then we have already reached the optimum. 
Geometrical categorization of the optimal point rests on the following lemma.
Farka’s Lemma. For ࡭א Թ௡ൈ௠ǡ ࢉא Թ௡ only one of the two problems has a solution:
1.	 ࡭்࢞൒૙ǡ ࢉ்࢞൏Ͳ
2.	 ࢉൌ࡭࢟ǡ ࢟൒૙
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
58 
Mathematical Optimization
Corollary. For any ࡭א Թ௡ൈ௠ǡ ࢉא Թ௡ZHKDYH࡭்࢞൒૙ǡ ࢉ்࢞൒Ͳ if and only if ࢉൌ࡭࢟ǡ ࢟൒૙
Farka’s lemma was used in the proof of Karush-Kuhn-Tucker (KKT) Theorem on NLP by Tucker. The 
lemma states that if a vector ࢉ does not lie in the convex cone: ࡯ൌሼ࡭࢟ǡ ࢟൒૙ሽǡ then there is a vector 
࢞ǡ ࡭்࢞൒૙ǡ that is normal to a hyperplane separating c from C. 
To apply this lemma we consider a matrix A whose columns represent the active constraint gradients at 
the optimum point x*, a vector that represents the objective function gradient ׏݂ሺ࢞כሻ and d represents 
a search direction. Then, there is no d satisfying the descent and feasibility conditions: ׏݂ሺ࢞כሻ்ࢊ൏Ͳ 
and ׏݃௝ሺ࢞ሻ்ࢊ൐Ͳǡ ݆א ࣣ if and only if the objective function gradient can be expressed as: െ׏݂ሺ࢞כሻൌ 
σ
ߣ௝׏݃௝ሺ࢞כሻ
௝אࣣ
ǡ ߣ௝൒Ͳ 
The above lemma also explains the non-negativity condition on Lagrange multipliers for inequality 
constraints in the KKT conditions.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Fundamental Engineering Optimization 
Methods
59 
Mathematical Optimization
4.4.3	
Second Order Conditions 
The second order necessary and sufficient conditions assume that x* satisfies the FONC (the KKT 
conditions) and use the Hessian of the Lagrangian function to investigate the behavior of the candidate 
point x* where we. The Hessian of the Lagrangian is defined as:
׏ଶࣦሺ࢞ǡ ࢛ǡ ࢜ǡ ࢙ሻൌ׏ଶ݂ሺ࢞ሻ൅෍
ݒ௜׏ଶ݄௜ሺ࢞ሻ
௟
௜ୀଵ
൅෍
ݑ௝׏ଶ݃௝ሺ࢞ሻ
௠
௝ୀଵ

(4.18)
Second Order Necessary Condition (SONC). Assume d is a feasible vector that lies in the constraint 
tangent hyperplane, i.e., let ൛ࢊǣ׏݄௜ሺ࢞ሻ்ࢊൌͲǡ ׏݃௝ሺ࢞ሻ்ࢊൌͲǡ ݆א ࣣൟ,I࢞כ is a local minimizer of f, 
then it satisfies the following SONC: 
ߜ݂ൌࢊ்׏ଶࣦሺ࢞כǡ ࢛כǡ ࢜כǡ ࢙כሻࢊ൒Ͳ
(4.19)
Second Order Sufficient Condition (SOSC). If for some ࢞כǡࢊ்׏ଶࣦሺ࢞כǡ ࢛כǡ ࢜כǡ ࢙כሻࢊ൐Ͳ for all 
൛ࢊǣ׏݄௜ሺ࢞כሻ்ࢊൌͲǡ ׏݃௝ሺ࢞כሻ்ࢊൌͲǡ ݑ௝
כ ൐Ͳൟ then x* is a local minimizer of ݂ሺ࢞ሻ 
Further, a stronger SOSC is given as: ׏ଶࣦሺ࢞כǡ ࢛כǡ ࢜כǡ ࢙כሻ൐Ͳ then ࢞כ is a local minimizer of ݂ሺ࢞ሻ
An example of SOC is now presented.
Example: Second order conditions
We consider the optimization problem in Example 4.5. The constraint tangent hyperplane for active 
constraints in Example 4.5 is computed as: ݀ଵെ݀ଶൌͲǤ  The Hessian of the Lagrangian at the optimum 
point ሺݔଵ
כǡ ݔଶ
כሻൌቀ
௖
ଶǡ
௖
ଶቁLVJLYHQDVቀͲ
െͳ
െͳ
Ͳ ቁTherefore, the SONC evaluate as: ࢊ்׏ଶࣦࢊൌʹ 
indicating that the candidate point is indeed an optimum point. Similar analysis applies to Example 4.6.
4.5	
Postoptimality Analysis
Postoptimality analysis refers to the study of the effects of parametric changes on the optimal solution. 
In particular, we are interested in the objective function variation resulting from relaxing the constraint 
limits. To study these changes, we consider the following perturbed optimization problem (Arora, p.153):
	

࢞݂ሺ࢞ሻ
(4.20)
Subject to ݄௜ሺ࢞ሻൌܾ௜ǡ ݅ൌͳǡ ǥ ǡ ݈Ǣ݃௝ሺ࢞ሻ൑݁௝ǡ ݆ൌ݅ǡ ǥ ǡ ݉
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
60 
Mathematical Optimization
where ܾ௜ and ݁௝ are small variations in the neighborhood of zero. Let the optimum point for the perturbed 
problem be expressed as: ࢞כሺ࢈ǡ ࢋሻǡ with the optimal cost given as: ݂כሺ࢈ǡ ࢋሻ Then, the implicit first 
order derivatives of the cost function are computed as: డ௙ሺ࢞כሻ
డ௕೔
ൌെݒ௜
כǡ
డ௙ሺ࢞כሻ
డ௘ೕ
ൌെݑ௝
כǢ and, the resulting 
cost function variation due to constraint relaxation is given as:
ߜ݂ሺ࢞כሻൌെ෍ݒ௜
כܾ௜
௜
െ෍ݑ௝
כ݁௝
௝

(4.21)
The above result implies that the non-zero Lagrange multipliers accompanying the active constraints 
determine the cost function sensitivity to constraint relaxation. Non-active constraints have zero Lagrange 
multipliers, and hence do not affect the solution. Further, if the Lagrange multipliers for active constraints 
were to take on negative values, then constraint relaxation would result in a reduction in the optimum 
cost function value, which is counter-intuitive.
The cost function variation resulting from changes to parameters embedded in the constraints, ݄௜ሺݏሻ 
and ݃௝ሺݏሻ can be similarly examined by considering how individual constraint variations affect the 
cost function, i.e., 
ߜ݂ሺ࢞כሻൌ෍ݒ௜
כߜ݄௜
௜
൅෍ݑ௝
כߜݒ௝
௝

(4.22)
where, once again, we observe that Lagrange multipliers for the individual constraints determine the 
sensitivity of I ߜ݂ሺ࢞כሻ to the parameter variations related to those constraints. 
4.6	
Lagrangian Duality 
Lagrangian duality in optimization problems stems from the fact that the Lagrangian function is 
stationary at the optimal point. The duality theory associates with every optimization problem (termed 
as primal) a dual problem that uses Lagrange multipliers as the optimization variables. 
To develop the duality concepts, we consider a general optimization problem (4.15), where a Lagrangian 
function for the problem was defined in (417). Equivalently, the Lagrangian function can be written 
without the slack variables, and in vector format the function and its derivatives are given as:
ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌ݂ሺ࢞ሻ൅்࢛ࢍ൅்࢜ࢎ
(4.23)
׏ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌ׏݂ሺ࢞ሻ൅ሾસࢍሿ࢛൅ሾસࢎሿ࢜
where ሾસࢍሿǡ ሾસࢎሿ are Jacobian matrices containing individual constraint derivatives as column vectors. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
61 
Mathematical Optimization
Next, let x* represent an optimal solution to the problem and let ሺ࢛כǡ ࢜כሻ be the associated Lagrange 
multipliers, then the Lagrangian function is stationary at the optimum point, i.e.׏ࣦሺ࢞כǡ ࢛כǡ ࢜כሻൌͲ
To proceed further, we assume that the Hessian of the Lagrangian is positive definite in the feasible 
region, i.e., ׏ଶࣦሺ࢞ǡ ࢛ǡ ࢜ሻ൒૙ǡ࢞א ȳ We can now state the following duality theorem (Belegundu and 
Chandrupatla, p. 269):
Duality theorem: The following are equivalent:
1.	 x* together with ሺ࢛כǡ ࢜כሻ solves the primal problem (4.15).
2.	 ሺ࢞כǡ ࢛כǡ ࢜כሻ is a saddle point of the Lagrangian function ࣦሺ࢞ǡ ࢛ǡ ࢜ሻ, i.e., 
ࣦሺ࢞כǡ ࢛ǡ ࢜ሻ൑ࣦሺ࢞כǡ ࢛כǡ ࢜כሻ൑ࣦሺ࢞ǡ ࢛כǡ ࢜כሻ
(4.24)
3.	 ሺ࢞כǡ ࢛כǡ ࢜כሻ solves the following dual problem:

࢞ǡ࢛ஹ૙ǡࣦ࢜ሺ࢞ǡ ࢛ǡ ࢜ሻ Subject to ׏ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌͲ
(4.25)
Further, the two extrema are equal, i.e., ࣦሺ࢞כǡ ࢛כǡ ࢜כሻൌ݂ሺ࢞כሻ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Fundamental Engineering Optimization 
Methods
62 
Mathematical Optimization
In (4.24) above,ࣦሺ࢞כǡ ࢛ǡ ࢜ሻൌ࢞אஐࣦሺ࢞ǡ ࢛ǡ ࢜ሻ represents a minimizer of ࣦ when ࢛൒૙ǡ ࢜ are 
fixed; similarly,ࣦሺ࢞ǡ ࢛כǡ ࢜כሻൌ࢛ஹ૙ǡࣦ࢜ሺ࢞ǡ ࢛ǡ ࢜ሻ is a maximizer of ࣦ when ࢞א ȳ is fixed. These 
two functions, respectively, lower and upper bound the Lagrangian at the optimum point. Hence,
݂ሺ࢞כሻ൑݂ሺ࢞ሻ for any x that is primal-feasible, andࣦሺ࢞ǡ ࢛ǡ ࢜ሻ൑݂ሺ࢞כሻ for any ࢞ǡ ࢛ǡ ࢜ that are dual 
feasible ׏ࣦൌͲǡ ࢛൒૙
Further,࢛ஹ૙ǡࣦ࢜ሺ࢞כǡ ࢛ǡ ࢜ሻ൑࢞אஐࣦሺ࢞ǡ ࢛כǡ ࢜כሻǡ which signifies weak duality. As a consequence of 
duality, if the dual objective is unbounded then primal problem has no feasible solution and vice versa.
We note that in nonlinear problems achieving strong duality (equality in 4.24) is not always possible, 
and, in general, a duality gap exists between the primal and dual solutions. Nevertheless, existence of 
strong duality is ensured in the case of convex optimization problems that satisfy the positive definite 
assumption on the Hessian matrix. Those problems are discussed following the dual function formulation.
4.6.1	
Formulation of the Dual Function
The definition of the dual problem in (4.24) assumes that ׏ଶࣦሺ࢞כǡ ࢛כǡ ࢜כሻ is positive definite. If this 
assumption is valid, then by Implicit Function Theorem there exists a neighborhood Gࣲ around ሺ࢞כǡ ࢛כǡ ࢜כሻ 
in which ࢞ൌ࢞ሺ࢛ǡ ࢜ሻ is a differentiable function, ׏ࣦሺ࢞ሺ࢛ǡ ࢜ሻǡ ࢛ǡ ࢜ሻൌͲDQG׏ଶࣦሺ࢞ሺ࢛ǡ ࢜ሻǡ ࢛ǡ ࢜ሻ, and 
is positive definite. Moreover, everyሺ࢛൒૙ǡ ࢜ሻFORVHWRሺ࢛כǡ ࢜כሻ close to has a unique corresponding x 
that is a global minimizer of ࣦሺ࢞ǡ ࢛ǡ ࢜ሻLQࣲ This allows us to define a dual function Q߮ሺ࢛ǡ ࢜ሻ as:
߮ሺ࢛ǡ ࢜ሻൌ
࢞אࣲࣦሺ࢞ǡ ࢛ǡ ࢜ሻ
(4.26)
where the minimum can be found from the application of FONC. In terms of the dual function the dual 
optimization problem is defined as:
ሺሻ
࢛ஹ૙ǡ࢜߮ሺ࢛ǡ ࢜ሻ
	

(4.27)
Let ሺ࢛כǡ ࢜כሻ be the optimal solution to the dual problem, then ߮ሺ࢛כǡ ࢜כሻൌ݂ሺ࢞כሻ 
The dual problem may be similarly solved via application of FONC. Towards this end, we use the chain 
rule to compute the derivative of the dual function as (Griva, Nash & Sofer, p. 537):
׏߮ሺ࢛ǡ ࢜ሻൌ׏࢛ǡ࢜࢞ሺ࢛ǡ ࢜ሻ׏௫߮ሺ࢛ǡ ࢜ሻ൅׏࢛ǡ࢜߮ሺ࢛ǡ ࢜ሻ
(4.28)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
63 
Mathematical Optimization
where, by definition,׏௫߮ሺ࢛ǡ ࢜ሻൌͲ Further, ׏࢛߮ሺ࢛ǡ ࢜ሻൌࢍ൫࢞ሺ࢛ǡ ࢜ሻ൯ǡ ׏࢜߮ሺ࢛ǡ ࢜ሻൌࢎ൫࢞ሺ࢛ǡ ࢜ሻ൯ǡ and 
׏࢛ǡ࢜߮ሺ࢛ǡ ࢜ሻൌ൤ࢍሺ࢞ሺ࢛ǡ ࢜ሻሻ
ࢎሺ࢞ሺ࢛ǡ ࢜ሻሻ൨ Next, we note that: ׏ଶ߮ሺ࢛ǡ ࢜ሻൌ׏࢞ሺ࢛ǡ ࢜ሻ൤સࢍ்
સࢎ்൨ǡ
સ
where ׏࢞ሺ࢛ǡ ࢜ሻ may 
be found from differentiating ׏௫߮ሺ࢛ǡ ࢜ሻൌ૙DV׏൫׏௫߮ሺ࢛ǡ ࢜ሻ൯ൌቂસࢍ
સࢎቃ൅׏࢞ሺ࢛ǡ ࢜ሻ׏௫௫߮ሺ࢛ǡ ࢜ሻൌ૙ 
Therefore, ׏࢞ሺ࢛ǡ ࢜ሻൌቂસࢍ
સࢎቃሾ׏௫௫
ଶ߮ሺ࢛ǡ ࢜ሻሿିଵ and we finally get:

׏ଶ߮ሺ࢛ǡ ࢜ሻൌસࢍሺ׏௫௫
ଶࣦሻିଵસࢍ்൅સࢎሺ׏௫௫
ଶࣦሻିଵસࢎ்
(4.29)
An example of the dual function is given in the next section under convex optimization problems.
4.6.2	
 Duality in Convex Optimization Problems
In the case of convex optimization problems, if x* is a regular point that solves the primal problem, and 
if ࢛כǡ ࢜כ are the associated Lagrange multipliers, then ሺ࢞כǡ ࢛כǡ ࢜כሻ is dual feasible and solves the dual 
problem. To develop these concepts, we consider the following quadratic programming (QP) problem:
	
Minimize ݍሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞
(4.30)
	
Subject to: ࡭࢞൒࢈
where Q is symmetric and positive definite. Then, using Lagrange multiplier vector λ for the inequality 
constraint, a Lagrangian function for the QP problem is given as: 
	
ࣦሺ࢞ǡ ࣅሻൌͳ
ʹ ்࢞ࡽ࢞൅ࢉ்࢞െࣅ்ሺ࡭࢞െ࢈ሻ
(4.31)
The associated dual QP problem is defined as:
	

࢞ǡࣅஹ૙ࣦሺ࢞ǡ ࣅሻൌͳ
ʹ ்࢞ࡽ࢞൅ࢉ்࢞െࣅ்ሺ࡭࢞െ࢈ሻ
(4.32)
	
Subject to ࡽ࢞൅ࢉെ࡭்ࣅൌ૙
To obtain a solution, we first solve the constraint equation for ࢞WRJHW࢞ሺࣅሻൌࡽିଵሺ࡭்ࣅെࢉሻ, and 
substitute it in the objective function to define the dual problem in terms of the dual function as:
	

ࣅஹ૙߮ሺࣅሻൌͳ
ʹ ࣅ்ሺ࡭ࡽିଵ࡭்ሻࣅ൅ሺ࡭ࡽିଵࢉ൅࢈ሻ்ࣅെͳ
ʹ ࢉ்ࡽିଵࢉ 
(4.33)
The gradient and Hessian of the dual function with respect to λ are computed as: 
	

׏߮ሺࣅሻൌሺ࡭ࡽିଵ࡭்ሻࣅ൅࡭ࡽିଵࢉ൅࢈ǡ
׏ଶ߮ሺࣅሻൌ࡭ࡽିଵ࡭்
(4.34)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
64 
Mathematical Optimization
If the optimal point is a regular point, then matrix A has full row rank. Then, from FONC, the solution 
to the dual QP problem is given as:
ࣅൌሺ࡭ࡽିଵ࡭்ሻିଵሺ࡭ࡽିଵࢉ൅࢈ሻ
(4.35)
where a non-negative solution ࣅ൒૙ has been assumed. 
As an example, we consider the following QP problem:
Example 4.6: quadratic optimization problem (Griva, Nash & Sofer, p.528)
Let the primal problem be defined as: ௫݂ሺݔሻൌݔଶVXEMHFWWRݔ൒ͳ Then, a solution to the primal 
problem is given as: ݔכ ൌͳ 
The Lagrangian function for the problem is formulated as: ࣦሺݔǡ ߣሻൌݔଶ൅ߣሺͳ െݔሻ The resulting dual 
optimization problem is defined as:
ఒஹ଴ࣦሺݔǡ ߣሻൌݔଶ൅ߣሺͳ െݔሻVXEMHFWWR׏ࣦሺݔǡ ߣሻൌʹݔ൅ߣൌͲ 
Eliminating the constraint redefines the dual problem as: 
ఒஹ଴߮ሺߣሻൌߣെ
ఒమ
ସ, with the solution: ߣכ ൌʹ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Fundamental Engineering Optimization 
Methods
65 
Mathematical Optimization
We may note that the saddle point condition is satisfied in this case, i.e.,
ఒஹ଴ࣦሺݔכǡ ߣሻൌ
ఒஹ଴ߣെ
ఒమ
ସ൑ࣦሺݔכǡ ߣכሻൌͳ ൑
௫ஹଵࣦሺݔǡ ߣכሻൌ
௫ஹଵݔଶ 
with equality satisfied on both sides.
4.6.3	
Local Duality Concepts
The existence of strong duality is only assured in the case of convex optimization problems. Nonetheless, 
using second order Taylor series expansion, any function can be locally approximated by a convex 
function. This prompts us to explore the possibility that the Lagrangian function is locally convex, and 
that strong duality may be locally achieved. 
Towards this end, we consider the general optimization problem (4.15) with the Lagrangian function 
given by (4.17). Let x* denote a solution to the optimization problem; if x* is a regular point, then there 
exist unique Lagrange multipliers ሺ࢛כǡ ࢜כሻ such that: ׏ࣦሺ࢞כǡ ࢛כǡ ࢜כሻൌͲ As discussed in 4.6.1 above, 
a local dual function for the problem can be defined via (4.26), and the dual optimization problem can 
be locally defined via (4.17). The local problem can be solved via application of FONC. Let ሺ࢛כǡ ࢜כሻ be 
the optimal solution to the dual problem, then ࣦכሺ࢛כǡ ࢜כሻൌ݂ሺ࢞כሻ
We note that local duality is applicable to both convex and non-convex problems. For example, we can 
apply local duality to the QP problem, to write: 
࢞ሺࣅሻൌࡽିଵሺ࡭்ࣅെࢉሻǡ
׏ࢍൌ࡭்ǡ
ࣦכሺࣅሻൌͳ
ʹ ࣅ்ሺ࡭ࡽିଵ࡭்ሻࣅ൅ሺ࡭ࡽିଵࢉ൅࢈ሻ்ࣅെͳ
ʹ ࢉ்ࡽିଵࢉǡ
(4.36)
׏ࣦכሺࣅሻൌെ࡭ࡽିଵ࡭்ࣅ൅࡭ࡽିଵࢉ൅࢈ǡ׏ଶࣦכሺࣅሻൌ࡭ࡽିଵ࡭்
As an example of a non-convex optimization problem we consider the following problem:
Example 4.7: Local duality
We consider the following optimization problem (Arora, p. 205):
௫݂ሺݔሻൌെݔଵݔଶǡVXEMHFWWRሺݔଵെ͵ሻଶ൅ݔଶ
ଶൌͷ
The Lagrangian function and its derivative are given as:
ࣦሺ࢞ǡ ࣅሻൌെݔଵݔଶ൅ߣሺሺݔଵെ͵ሻଶ൅ݔଶ
ଶെͷሻǡZKHUH
׏௫ࣦሺ࢞ǡ ࣅሻൌ൤െݔଶ൅ʹߣሺݔଵെ͵ሻ
െݔଵ൅ʹߣݔଶ
൨ൌቂʹߣ
െͳ
െͳ
ʹߣቃቂݔଵ
ݔଶቃെቂ͸ߣ
Ͳ ቃ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
66 
Mathematical Optimization
Solving the FONC together with the equality constraint, the optimum solution to the problem is given as:
࢞כ ൌሺͶǡʹሻǡ ߣכ ൌͳǡ ݂כ ൌെͺ
The Hessian at the optimum point is computed as: ׏ଶሺ࢞כǡ ߣכሻൌቂʹ
െͳ
െͳ
ʹ ቃ and is positive definite. 
Therefore, using local duality theory, ׏௫ࣦሺ࢞ǡ ࣅሻൌͲ is solved for x, which gives: ቂݔଵ
ݔଶቃൌ
ଵ
ସఒమିଵ൤ͳʹߣଶ
͸ߣ൨ 
Next, the dual problem defined by ࣅஹ૙ࣦሺ࢞ሺߣሻǡ ߣሻൌ
ସ൫ఒାఒయିଶ଴ఒఱ൯
ሺସఒమିଵሻమ
ǡ ߣ്േ
ଵ
ଶ which has a local 
solution at ߣכ ൌͳ with ࣦሺ࢞ሺߣכሻǡ ߣכሻൌെͺ which matches the primal solution.
4.6.4	
Separable Problems
Dual methods are particularly powerful when applied to separable problems that are structured as:

࢞݂ሺ࢞ሻൌ෍݂௜ሺݔ௜ሻ
௜

6XEMHFWWRσ ݃௜ሺݔ௜ሻ
௜
൑Ͳǡ σ ݄௜ሺݔ௜ሻൌͲ
௜

(4.37)
The dual function for the separable problem is formulated as:
߮ሺ࢛ǡ ࢜ሻൌ
࢞൭෍݂௜ሺݔ௜ሻ
௜
൅ݑ෍݃௜ሺݔ௜ሻ
௜
൅ݒ෍݄௜ሺݔ௜ሻ
௜
൱
which decomposes into m separate single-variable problems given as: ௫೔݂௜ሺݔ௜ሻ൅ݑ݃௜ሺݔ௜ሻ൅ݒ݄௜ሺݔ௜ሻ
EO
E
L
O
which can be relatively easy to solve. Thus, the formulation of a dual problem becomes simple.
The next example shows how local duality can be applied to engineering problems that are separable.
Example 4.8: truss design problem (Belegundu and Chandrupatla, p. 274) 
A truss contains a total of 16 elements of length ܮ௜ൌ͵Ͳǡ ݅ൌͳǡ ǥ ǡͳʹǢܮ௜ൌ͵Ͳξʹǡ ݅ൌͳʹǡ ǥ ǡͳ͸ 
and cross-sectional area Dݔ௜ (design variable). The truss bears a load ܲൌʹͷǡͲͲͲOE at the tip. The 
weight of the structure is to be minimized with a bound on tip deflection, ߜ൑ͳ in. The problem is 
formulated as:

࢞෍
ܮ௜ݔ௜
ଵ଺
௜ୀଵ

6XEMHFWWRσ
ቀ
௖೔
௫೔െߙቁ
ଵ଺
௜ୀଵ
൑Ͳݔ௜൒ݔ௜
௅ZKHUHܿ௜ൌ
௉௅೔௙೔
మ
ாఋೆǡ ߙൌ
ଵ
ଵ଺ǡ ݔ௜
௅ൌͳͲି଺LQ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
67 
Mathematical Optimization
The dual function is defined as: ߮ሺߤሻൌ௫೔ஹ௫೔
ಽσ
ܮ௜ݔ௜
ଵ଺
௜ୀଵ
൅ߤቀ
௖೔
௫೔െߙቁ
which leads to individual problems of the form: ௫೔ஹ௫೔
ಽ߰ൌܮ௜ݔ௜൅ߤቀ
௖೔
௫೔െߙቁ
Application of FONC gives: if and if ݔ௜
כ ൌට
ఓ௖೔
௅೔LIܿ௜൐ͲǡDQGݔ௜
כ ൌݔ௜
௅LIܿ௜ൌͲ 
The resulting dual maximization problem is defined as: ఓ߮ሺߤሻൌʹ σ
ඥܿ௜ܮ௜ξߤെߤ
ଵ଺
௜ୀଵ
൅ܿ
where c is a constant. Application of FONC then gives: ߤൌ൫σ ඥܿ௜ܮ௜
௜
൯
ଶ 
For the given data, a closed-form solution is obtained as: ߤכ ൌͳ͵ͷͺǤʹǡ ݂כ ൌͳ͵ͷͺǤʹଷǡ
࢞ൌሾͷǤ͸͹ͶǤʹͷͶǤʹͷʹǤͺͶʹǤͺͶͳǤͶʹͳǤͶʹͳͲି଺ͳǤͲ͸ͳǤͲ͸ͳǤͲ͸ͳͲି଺ͳǤ͹͹ͳǤ͹͹ͳǤ͹͹ͳǤ͹͹ሿ in.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Fundamental Engineering Optimization 
Methods
68 
Linear Programming Methods
5	 Linear Programming Methods
Linear programming (LP) problems form an important subclass of the optimization problems. The 
distinguishing feature of the LP problems is that the objective function and the constraints are linear 
functions of the optimization variables. LP problems occur in many real-life economic situations where 
profits are to be maximized or costs minimized with constraints on resources. Specialized procedures, 
such as the Simplex method, were developed to solve the LPP. The simplex method divides the variables 
into basic and nonbasic, the latter being zero, in order to develop a basic feasible solution (BFS). It then 
iteratively updates basic variables thus generating a series of BFS, each of which carries a lower objective 
function value than the previous. Each time, the reduced costs associated with nonbasic variables are 
inspected to check for optimality. An optimum is reached when all the reduced costs are non-negative.
Learning Objectives: The learning objectives in this chapter are:
1.	 Understand the general formulation of a linear programming (LP) problem 
2.	 Learn the Simplex method to solve LP problems and its matrix/tableau implementation 
3.	 Understand the fundamental duality properties associated with LP problems
4.	 Learn sensitivity analysis applied to the LP problems
5.	 Grasp the formulation of KKT conditions applied to linear and quadratic programming 
problems
6.	 Learn to formulate and solve the linear complementarity problem (LCP)
5.1	
The Standard LP Problem
The general LP problem is described in terms of minimization (or maximization) of a scalar objective 
function of n variables, that are subject to m constraints. These constraints may be specified as EQ 
(equality constraints), GE (greater than or equal to inequalities), or LE (less than or equal to inequalities). 
The variables themselves may be unrestricted in range, specified to be non-negative, or upper and/or 
lower bounded. 
Mathematically, the LP problem is expressed as:

௫ೕሺRU
௫ೕሻ ݖൌσ
ܿ௝ݔ௝
௡
௝ୀଵ

6XEMHFWWRσ
ܽ௜௝ݔ௝ሺ൑ǡ ൌǡ ൒ሻܾ௜
௡
௝ୀଵ
ǡ݅ൌͳǡʹǡ ǥ ǡ ݉

ݔ௝൒ݔ௝௅IRUVRPH݆ݔ௝൑ݔ௝௎IRUVRPH݆ݔ௝IUHHIRUVRPH݆
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
69 
Linear Programming Methods
While the general LP problem may be specified in different ways, the standard LP problem refers to 
a problem involving minimization of a scalar cost function, subject to only equality constraints and 
with optimization variables restricted to take on non-negative values. The inequality constraints can 
be converted to equality by adding (subtracting) slack (surplus) variables to LE (GE) type constraints. 
Further, the original variables can be replaced by new variables, which take on non-negative values. The 
standard LP problem is defined as:

௫೔ ݖൌ෍
ܿ௝ݔ௝
௡
௝ୀଵ

6XEMHFWWRσ
ܽ௜௝ݔ௝ൌܾ௜
௡
௝ୀଵ
ǡ ݔ௝൒ͲǢ ݅ൌͳǡʹǡ ǥ ǡ ݉

(5.1)
The standard LP problem thus has the following characteristics:
1.	 It involves minimization of a scalar cost function. 
2.	 The variables can only take on non-negative values, i.e., ݔ௜൒Ͳ
3.	 The r.h.s. is assumed to be non-negative, i.e., ܾ௝൒Ͳ
Additionally, 
1.	 The constraints are assumed to be linearly independent, which implies that ݎܽ݊݇ሺ࡭ሻൌ݉
2.	 The problem is assumed to be well-formulated, which implies that ࢞ࢉ்࢞൏λ
In the vector-matrix format, the standard LP problem is expressed as:

࢞ݖൌࢉ்࢞
(5.2)
	
   subject to ࡭࢞ൌ࢈ǡ ࢞൒૙
where ࡭א Թ௠ൈ௡Ǣ ࢞ǡ ࢉא Թ௡ǡ ࢈א Թ௠ 
When encountered, exceptions to the standard LP problem formulation are dealt as follows:
1.	 A maximization problem is changed to a minimization problem by taking negative of the 
cost function, i.e.,࢞ࢉ்࢞ؠ ࢞ሺെࢉ்࢞ሻ 
2.	 Any constant terms in z can be dropped.
3.	 Any ݔ௜א Թ (unrestricted in sign) is replaced by ݔ௜ൌݔ௜
ାെݔ௜
ିZKHUHݔ௜
ାǡ ݔ௜
ି൒Ͳ 
4.	 The inequality constraints are converted to equality constraints by the addition of slack 
variables (to LE constraint) or subtraction of surplus variables (from GE constraint).
5.	 If ܾ௝൏Ͳ, the constraint is first multiplied by –1, followed by the introduction of slack or 
surplus variables.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
70 
Linear Programming Methods
5.2	
The Basic Solution to the LP Problem
We first note that the feasible set defined by linear equalities (and inequalities) in an LP problem is 
convex. In addition, the cost function is linear, hence convex. Therefore, the LP problem represents a 
convex optimization problem and a single global minimum for the problem exists. 
Further, due to only equality constraints present in the problem, the optimum solution, if it exists, lies on 
the boundary of the feasible region. This is also true in the case of inequality constraints, since if none 
of the constraints is active, the application of first order optimality conditions to the problem would 
result in ܿ௜ൌͲǡ ݅ൌͳǡ ǥ ǡ ݊LH there will be no optimal solution.
Algebraically, the LP problem represents a linear system of m equations in n unknowns. Accordingly,
a)	 If ݉ ൌ݊ the solution may be obtained from the constraints only.
b)	 If ݉ ൐݊ some of the constraints may be redundant, or the system may be inconsistent.
c)	 If ݉ ൏݊ the LP problem has an optimum solution, and can be solved using methods 
described below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Fundamental Engineering Optimization 
Methods
71 
Linear Programming Methods
Next, we consider the ݉ ൏݊ case, and assume that matrix A has full row rank; then, we arbitrarily 
choose independent (nonbasic) variables, to solve for the remaining (m) dependent (basic) variables. Let 
the system be transformed into canonical form:ࡵሺ௠ሻ࢞ሺ௠ሻ൅ࡽ࢞ሺ௡ି௠ሻൌ࢈Ǣ then, the general solution 
includes ሺ݊െ݉ሻ independent variables: ࢞ሺ௡ି௠ሻ and (m) dependent variables: ࢞ሺ௠ሻൌ࢈െࡽ࢞ሺ௡ି௠ሻ$
particular solution to the linear system can be obtained by setting: ࢞ሺ௡ି௠ሻൌ૙, and obtaining: ࢞ሺ௠ሻൌ࢈
A basic solution x to a standard LP problem satisfies two conditions:
1.	 ࢞ is a solution to ࡭࢞ൌ࢈
2.	 The columns of A corresponding to the nonzero components of x are linearly independent.
Since A can have at the most m independent columns, it implies that A has at the most m nonzero 
components. When A has a full row rank, a basic solution is obtained by choosing ݊െ݉ variables as 
zero. The resulting solution, x, contains m basic variables, ࢞࡮, and ݊െ݉ nonbasic variables, ࢞ࡺ the 
latter taking on zero values. The columns of A corresponding to ࢞࡮ are termed as the basis vectors.
The set ࣭ൌሼ࢞ǣ ࡭࢞ൌ࢈ǡ ࢞൒૙ሽU represents the feasible region for the LP problem. We note that a 
basic solution, ࢞א ࣭ that is in the feasible region is termed as a basic feasible solution (BFS). Further, 
the feasible region is a polytope (polygon in n dimensions), and each BFS represents an extreme point 
(a vertex) of the polytope.
Let ்࢞ൌሾ࢞஻ǡ ࢞ேሿ where the basic variables occupy leading positions; we accordingly partition the cost 
function coefficients as: ࢉ்ൌሾࢉ஻ǡ ࢉேሿ and represent the constraint matrix as: ࡭ൌሾ࡮ǡ ࡺሿǡ where B is a 
݉ൈ݉ nonsingular matrix and N is a D݉ൈሺ݊െ݉ሻ then, the original LP problem is reformulated as: 

࢞ݖൌࢉ஻
்࢞஻൅ࢉே
்࢞ேǡ
6XEMHFWWR࡭࢞ൌሾ࡮ࡺሿቂ࢞஻
࢞ேቃൌ࢈ǡ ࢞஻൒૙ǡ ࢞ே൒૙
(5.3)
Then, a BFS corresponding to basis B is represented as: ்࢞ൌሾ࢞஻ǡ ૙ሿǡ ࢞࡮ൌ࡮ିଵ࢈൐૙ Since, by 
assumption, ݎܽ݊݇ሺ࡭ሻൌ݉࡮, can be selected from the various permutations of the columns of A. 
Further, since each basic solution has exactly m non-zero components, the total number of basic solutions 
is finite, and is given as: ቀ݊
݉ቁൌ
௡Ǩ
௠Ǩሺ௡ି௠ሻǨ The number of BFS is smaller than number of basic solutions 
and can be determined by comparing the objective function values at the various basic solutions.
The Basic Theorem of Linear Programming (e.g., Arora, p. 201) states that if there is a feasible solution 
to the LP problem, there is a BFS; and if there is an optimum feasible solution, there is an optimum 
BFS. The basic LP theorem implies that an optimal BFS must coincide with one of the vertices of the 
feasible region. This fact can be used to compare the objective function value at all BFSs, and find the 
optimum by comparison if the number of vertices is small. Finally, there can also be multiple optimums 
if an active constraint boundary is parallel to the level curves of the cost function.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
72 
Linear Programming Methods
5.3	
The Simplex Method
The simplex method iteratively solves the standard LP problem. It does so by starting from a known BFS 
and successively moving to an adjacent BFS that carries a lower objective function value. Each move 
involves replacing a single variable in the basis with a new variable, such that the objective function 
value decreases. The previously nonbasic variable entering the basis is termed as entering basic variable 
(EBV), and the one leaving it is termed as leaving basic variable (LBV). An optimum is reached when 
no neighboring BFS with a lower objective function value can be found.
5.3.1	
The Simplex Algorithm
In order to mathematically formulate the simplex algorithm, let ்࢞ൌሾ࢞࡮ǡ ࢞ࡺሿ represent a non-basic 
solution to the LP problem, and let the constraints be expressed as: ࡮࢞஻൅ࡺ࢞ேൌ࢈ Then, we can 
solve for ࢞஻DV࢞஻ൌ࡮ିଵሺ࢈െࡺ࢞ேሻǡ and substitute it in the objective function to obtain: 
ݖൌࢉ஻
்࡮ିଵ࢈൅ሺࢉே
்െࢉ஻
்࡮ିଵࡺሻ࢞ேൌ்࢟࢈൅ࢉොே
்࢞ேൌݖƸ ൅ࢉොே
்࢞ே
(5.4)
where ்࢟ൌࢉ஻
்࡮ିଵ defines a vector of simplex (Lagrange) multipliers, ࢉොே
்ൌࢉே
்െ்࢟ࡺ represents the 
reduced costs for the nonbasic variables (reduced costs are zero for the basic variables), and ݖƸ ൌ்࢟࢈ 
represents the objective function value corresponding to a basic solution, where ݕ௜൐Ͳ represents an 
active constraint.
The significance of the reduced costs is as follows: let ܿƸ௝אࢉොே
் then, we note that assigning a nonbasic 
variable ݔ௝ a nonzero value ߜ௝ will change the objective function by ܿƸ௝ߜ௝ Therefore, any ܿƸ௝൏Ͳ has 
the potential to decrease the value of I ݖ, and the corresponding ݔ௝ may be selected as the EBV. It is 
customary to select the variable ݔ௝ with the lowest ܿƸ௝ as the EBV.
To select an LBV, we examine the update to the basic solution from the introduction of EBV, given 
as: ࢞஻ൌ࢈෡െ࡭෡௤ݔ௤ZKHUH࢈෡ൌ࡮ିଵ࢈ǡ࡭෡௤ൌ࡮ିଵ࡭௤ǡ and ࡭௤ represents the ݍWK column of ࡭ Then 
ݔ௤, can be increased so long as ࢞஻൒૙Ǣ element wise considerations require that: ܾ෠௜െܣመ௜ǡ௤ݔ௤൐Ͳ 
Therefore, the maximum value of ݔ௤LVݔҧ௤ൌ௜൜
௕෠೔
஺෠೔ǡ೜ǣ ܣመ௜ǡ௤൐Ͳൠ is , and the variable corresponding 
to the lowest ratio from this ratio test is picked as LBV. 
The steps involved in the Simplex algorithm are summarized below.
The Simplex Algorithm (Griva, Nash & Sofer, p.131):
1.	 Initialize: Find an initial BFS to start the algorithm; accordingly, determine ࢞࡮ǡ ࢞ࡺǡ ࡮ǡ ࡺǡ  
்࢟ൌࢉ஻
்࡮ିଵǡݖƸ ൌ்࢟࢈
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
73 
Linear Programming Methods
2.	 Optimality test: Compute ࢈෡ൌ࡮ିଵ࢈ǡ ࢉොே
்ൌࢉே
்െ்࢟ࡺ7 Then, evaluate ࢉොே
் associated with 
current nonbasic variables. If all ܿƸ௝൐Ͳ the optimal has been reached. Otherwise, select a 
variable ݔ௤ with ܿƸ௤൏Ͳ as EBV.
3.	 Ratio test: Compute ࡭෡௤ൌ࡮ିଵ࡭௤ Determine: ௜൜
௕෠೔
஺෠೔ǡ೜ǣ ܣመ௜ǡ௤൐Ͳൠൌ
௕෠೛
஺෠೛ǡ೜6HWܣመ௣ǡ௤ Set as 
the pivot element. 
4.	 Update: Assign ݔ௤՚
௕෠೛
஺෠೛ǡ೜ǡ ࢞஻՚ ࢈෡െܣመ௤ݔ௤ǡ ݖƸ ՚ ݖƸ ൅ܿƸ௤ݔ௤ Update ࢞࡮ǡ ࢞ࡺǡ ࡮ǡ ࡺ
The following example illustrates the application of simplex algorithm to LP problems.
Example 5.1: The Simplex algorithm
We consider the following LP problem:


௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWRʹݔଵ൅ݔଶ൑ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸Ǣݔଵ൒Ͳǡ ݔଶ൒Ͳ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
74 
Linear Programming Methods
The problem is first converted to standard LP form by changing the sign of the objective function and 
adding slack variables ݏଵǡ ݏଶ to the LE constraints. The resulting optimization variables, cost coefficients, 
and constraint coefficients are given below:
்࢞ൌሾݔଵ
ݔଶ
ݏଵ
ݏଶሿǡ
ࢉ்ൌሾെ͵ െʹͲͲሿǡ
࡭ൌቂʹ
ʹͳ
͵ͳ
ͲͲ
ͳቃǡ
࢈ൌቂͳʹ
ͳ͸ቃ
The steps of the Simplex algorithm for the problem are shown below:
Step 1:
࢞஻ൌቂݏଵ
ݏଶቃൌቂͳʹ
ͳ͸ቃǡ ࢞ேൌቂݔଵ
ݔଶቃൌቂͲ
Ͳቃǡ ࢉ஻
்ൌሾͲǡͲሿǡ ࢉே
்ൌሾെ͵ǡ െʹሿǡ ࡮ൌࡵǡ ࢈෡ൌ࢈ൌቂͳʹ
ͳ͸ቃǡݖƸ ൌͲǡ
൜
ൠ
࢞஻ൌቂݏଵ
ݏଶቃൌቂͳʹ
ͳ͸ቃǡ ࢞ேൌቂݔଵ
ݔଶቃൌቂͲ
Ͳቃǡ ࢉ஻
்ൌሾͲǡͲሿǡ ࢉே
்ൌሾെ͵ǡ െʹሿǡ ࡮ൌࡵǡ ࢈෡ൌ࢈ൌቂͳʹ
ͳ͸ቃǡݖƸ ൌͲǡ
൜
ൠ
Update:ݔଵ՚ ͸ǡ ࢞஻՚ ቂͲ
Ͷቃǡ ݖƸ ՚ െͳͺ 
Step 2: 
S
࢞஻ൌቂݔଵ
ݏଶቃൌቂ͸
Ͷቃǡ ࢞ேൌቂݔଶ
ݏଵቃൌቂͲ
Ͳቃǡ ࢉ஻
்ൌሾെ͵ǡ Ͳሿǡ ࢉே
்ൌሾെʹǡ Ͳሿǡ ࡮ൌቂʹ
ʹͲ
ͳቃǡ ࡺൌቂͳ
͵ͳ
Ͳቃǡ ࢈෡ൌቂ͸
Ͷቃǡ
்࢟ൌሾെ͵Ȁʹǡ Ͳሿǡ ࢉොே
்ൌሾെͳȀʹǡ ͵Ȁʹሿǡ ݔ௤ൌݔଶǡ ܣመଵൌቂͳȀʹ
ʹ ቃǡ ൜
௕෠೔
஺෠೔ǡభǣ ܣመ௜ǡଵ൐Ͳൠൌሼͳʹǡ ʹሽǡ ܣመ௣ǡ௤ൌܣመଶǡଶ
 
Update: ݔଶ՚ ʹǡ ࢞஻՚ ቂͷ
Ͳቃǡ ݖƸ ՚ െͳͻ
Step 3: 
࢞஻ൌቂݔଵ
ݔଶቃൌቂͷ
ʹቃǡ ࢞ேൌቂݏଵ
ݏଶቃൌቂͲ
Ͳቃǡ ࢉ஻
்ൌሾെ͵ǡ െʹሿǡ ࢉே
்ൌሾͲǡ Ͳሿǡ ࡮ൌቂʹ
ʹͳ
͵ቃǡ ࡺൌࡵǡ
்࢟ൌሾെͷȀͶǡ െͳȀͶሿǡ ࢉොே
்ൌሾͷȀͶǡ ͳȀͶሿ
Since all ܿƸ௝൐Ͳ an optimal has been reached and ݖ௢௣௧ൌെͳͻ
5.3.2	
Tableau Implementation of the Simplex Algorithm
It is customary to use tableaus to capture the essential information about the LP problem. A tableau is an 
augmented matrix that includes the constraints matrix, the right-hand-side (rhs), and the coefficients of 
the cost function (represented in the last row). Each preceding row of the tableau represents a constraint 
equation, and each column represents a variable. The tableau method implements the simplex algorithm 
by iteratively computing the inverse of the basis (B) matrix. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
75 
Linear Programming Methods
We consider a standard linear program with n variables and m equality constraints, and assume that at the 
current iteration the vectors of basic and nonbasic variables are represented as ݔ஻ and ݔே, respectively. 
Then, the original linear program corresponds to the following tableau:
%DVLF
࢞࡮
࢞ࡺ
5KV
࢞࡮
࡮
ࡺ
࢈
െࢠ
ࢉ஻
்
ࢉே
்
Ͳ
In the above, basic variables are identified in the left-most column, the next columns m pertain to basis 
vectors and the right-most column represents the rhs. By pre-multiplying the tableau with the matrix: 
J
൤࡮ିଵ
Ͳ
െ்࢟
ͳ൨ǡ ்࢟ൌࢉ஻
்࡮ିଵǡ we obtain the tableau representation in the current basis:
%DVLF
࢞࡮
࢞ࡺ
5KV
࢞࡮
ࡵ
࡮ିଵࡺ
࡮ିଵ࢈
െࢠ
૙
ࢉே
்െ்࢟ࡺ
െ்࢟࢈

where ்࢟ represents the vector of Lagrange multipliers, ࢉே
்െ்࢟ࡺ represents the reduced costs, and 
்࢟࢈ represents the current objective function value. 
The simplex iteration begins with the optimality test: we inspect the reduced costs for the nonbasic 
variables (represented in the last row under nonbasic variables), and pick the variable with most negative 
reduced cost (or another variable with negative reduced cost) as EBV. Next, we use the ratio test to 
determine LBV from the current basis. Once the pivot element has been identified, Gaussian elimination 
steps (elementary row operations) are completed to reduce the EBV column to a unit vector, effectively 
replacing LBV with EBV in the current basis. This completes an iteration of the simplex method. The 
process is then repeated till all the reduced costs are non-negative, thus signifying that an optimum has 
been reached. 
We note that only the original matrices: ࡭ǡ ࢈ǡ ࢉ and the inverse of current basis matrix [࡮ିଵሻ are needed to 
compute the remaining entries in the current tableau. Further, only the EBV column among the nonbasic 
variable columns needs to be computed. These facts are useful when developing a computationally 
efficient implementation of the simplex method. Additionally, some mechanism is needed to update 
the representation of the inverse matrix for the next iteration. Computationally efficient ways of doing 
so are discussed in (Griva, Nash & Sofer, Ch. 7).
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
76 
Linear Programming Methods
Some abnormal terminations of the Simplex algorithm are described as follows: 
1.	 If the reduced cost for a nonbasic variable in the final tableau is zero, then there exists a 
possibility for multiple optimum solutions with equal cost function value. This happens 
when cost function contours (level curves) are parallel to one of the constraint boundaries.
2.	 If the reduced cost is negative but the pivot step cannot be completed due to all coefficients 
in the LBV column being negative, it reveals a situation where the cost function is 
unbounded below. 
3.	 If, at some point during Simplex iterations, a basic variable attains a zero value (i.e., the 
rhs has a zero), it is called degenerate variable and the corresponding BFS is termed as 
degenerate solution, as the degenerate row hence forth becomes the pivot row, with no 
improvement in the objective function. While it is theoretically possible for the algorithm to 
fail by cycling between two degenerate BFSs, this is not known to happen in practice.
An example for tableau implementation of the simplex method is presented below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Fundamental Engineering Optimization 
Methods
77 
Linear Programming Methods
Example 5.2: the Tableau method
As an example of the tableau method, we resolve example 5.1 using tableaus, where the optimization 
problem is stated as: 

௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWRʹݔଵ൅ݔଶ൑ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸Ǣݔଵ൒Ͳǡ ݔଶ൒Ͳ
The problem is first converted to the standard LP form. The constraints and cost function coefficients 
were entered in an initial simplex tableau, where the EBV, LBV, and the pivot element are identified 
underneath the tableau:

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ 5KV
࢙૚





࢙૛





ࢠ




Ͳ
(%9ݔଵ/%9ଵSLYRW
The subsequent simplex iterations result in the series of tableaus appearing below:
%DVLF ࢞૚
࢞૛
࢙૚
࢙૛ UKV
࢞૚





࢙૛





െࢠ

 


(%9ݔଶ/%9ଶSLYRW

%DVLF ࢞૚ ࢞૛
࢙૚
࢙૛
UKV
࢞૚


 

࢞૛





െࢠ





At this point, since all reduced costs are positive, an optimum has been reached with: 
ݔଵ
כ ൌͷǡ ݔଶ
כ ൌʹǡ ݖ௢௣௧ൌെͳͻ
5.3.1	
Final Tableau Properties
The final tableau from the simplex algorithm has certain fundamental properties that relate to the initial 
tableau. To reveal those properties, we consider the following optimization problem:

࢞
ݖൌࢉ்࢞
6XEMHFWWR࡭࢞൑࢈ǡ ࢞൒૙
(5.5)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
78 
Linear Programming Methods
Adding surplus variables to the constraints results in the following standard LP problem:

࢞ݖൌെࢉ்࢞
6XEMHFWWR࡭࢞൅ࡵ࢙ൌ࢈ǡ ࢞൒૙

(5.6)
An initial tableau for this problem is given as:
%DVLF
࢞
࢙
5KV
࢙
࡭
ࡵ
࢈
െࢠ
െࢉ்
૙
Ͳ
Assuming that the same order of the variables is maintained, then at the termination of the Simplex 
algorithm the final tableau is given as:
%DVLF
࢞
࢙
UKV
࢞࡮
࡭෩
ࡿ
࢈෩
െࢠ
ࢉො்
்࢟
ݖכ
We note that the coefficients in the final tableau are related to those in the initial tableau as follows: 

࡭෩ൌࡿ࡭ǡ࢈෩ൌࡿ࢈ǡ
ࢉො்ൌ்࢟࡭െࢉ்ǡ
ݖכ ൌ்࢟࢈
(5.7)
Thus, given the initial tableau ࡭ǡ ࢈ǡ ࢉ் and the final coefficients in the slack variable columns: ்࢟ǡࡿ 
we can reconstruct the final tableau as:
ሾܾܶܽሿ ൌ൤ࡿ
૙
்࢟
ͳ൨ሾܾܶܽሿ
(5.8)
Therefore, in a computer implementation of the Simplex algorithm, only the coefficients: ࡭ǡ ࢈ǡ ࢉ்ǡ ்࢟ǡ ࡿ 
need to be stored in order to recover the final tableau when the algorithm terminates. 
5.3.2	
Obtaining an Initial BFS
The starting point of the simplex algorithm is a valid BFS. This is trivial in the case of a maximization 
problems modeled with LE constraints (Example 5.1), where an obvious initial BFS is to choose the 
slack variables as the basic variables. Initial BFS is not so obvious when the problem involves GE or EQ 
constraints. It is so because the feasible region in the problem does not normally include the origin. Then, 
in order to initiate the simplex algorithm, we need to choose an initial B matrix, such that ࡮࢞஻ൌ࢈ 
yields a non-negative solution for ࢞஻ The two-phase Simplex method described below obtains an initial 
BFS by first solving an auxiliary LP problem.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
79 
Linear Programming Methods
The two-phase Simplex method works as follows: we add a set of ݉ auxiliary variables ࢞෥ǡ to the original 
optimization variables x, and define an auxiliary LP problem where the auxiliary objective function is 
selected to reduce the auxiliary variables. The auxiliary problem is defined as:

௫෤೔෍
ݔ෤௜
௠
௜ୀଵ

6XEMHFWWRሾ࡭ࡵሿቂ࢞
࢞෥ቃൌ࢈ǡ ࢞൒૙ǡ ࢞෥൒૙

(5.9)
The auxiliary problem is solved in Phase I of the Simplex algorithm. We note that ࢞෥ൌ࢈ is a valid BFS 
for the auxiliary problem and serves as a starting point for Phase I Simplex algorithm. Further, since 
only the GE and EQ constraints require auxiliary variables, their number can be accordingly chosen 
less than or equal to ݉. 
The starting tableau for the Phase I Simplex algorithm is given as:

%DVLF
࢞࡮
࢞ࡺ
࢞෥
5KV
࢞࡮
࡮
ࡺ
ࡵ
࢈
െࢠ
ࢉ஻
்
ࢉே
்
૙
૙
െࢠࢇ
૙
૙
૚ࢀ
૙
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Fundamental Engineering Optimization 
Methods
80 
Linear Programming Methods
where ૚்ൌሾͳǡ ǥ ǡͳሿ represents a unit vector. The first step in the Phase I Simplex is to make auxiliary 
variables the basic variables. This is done by row reductions aimed to generate unit vectors in the basis 
columns, which results in the following tableau:

%DVLF
࢞࡮
࢞ࡺ
࢞෥
5KV
࢞࡮
࡮
ࡺ
ࡵ
࢈
െࢠ
ࢉ஻
்
ࢉே
்
૙
Ͳ
െࢠࢇ
െ૚்࡮
െ૚்ࡺ
૙
െ૚்࢈
The Phase I Simplex algorithm continues till all the reduced costs in the auxiliary objective row become 
non-negative and the auxiliary objective function value reduces to zero, thus signaling the end of Phase I. 
If the auxiliary objective value at the end of Phase I does not equal zero, it indicates that no feasible 
solution to the original problem exists. 
Once the auxiliary problem has been solved, we turn to the original problem, and drop the auxiliary 
objective ݖ௔ሻ row and the auxiliary variable ࢞෥ columns from the current tableau (or ignore them). 
We then follow up with further Simplex iterations in Phase II of the algorithm till an optimum value 
for z is obtained. 
Two examples involving GE and EQ constraints are solved below to illustrate the implementation of the 
two-phase Simplex algorithm. 
Example 5.3: Two-phase Simplex algorithm for GE constraints
We consider the following LP problem:

௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWR͵ݔଵ൅ʹݔଶ൒ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸ǡ ݔଵ൒Ͳǡ ݔଶ൒Ͳ 
We first convert the problem to standard form by subtracting surplus variable (s1) from GE constraint 
and adding slack variable (s2) to LE constraint. The standard form LP problem is given as:

௫భǡ௫మݖൌെ͵ݔଵെʹݔଶ
6XEMHFWWR͵ݔଵ൅ʹݔଶെଵൌͳʹǡ ʹݔଵ൅͵ݔଶ൅ଶൌͳ͸Ǣݔଵǡ ݔଶǡ ݏଵǡ ݏଶ൒Ͳ
There is no obvious BFS to start the simplex algorithm. To solve the problem using two-phase simplex 
method, we add an auxiliary variable a1 to GE constraint and define the following auxiliary LP problem:

௫భǡ௫మݖ௔ൌܽଵ
6XEMHFWWR͵ݔଵ൅ʹݔଶെݏଵ൅ܽଵൌͳʹǡ ʹݔଵ൅͵ݔଶ൅ݏଶൌͳ͸Ǣݔଵǡ ݔଶǡ ݏଵǡ ݏଶǡ ܽଵ൒Ͳ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
81 
Linear Programming Methods
The starting tableau for the auxiliary problem (Phase I) is given as:
%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢇ૚ 5KV







࢙૛






െࢠ






െࢠࢇ






We first a1 bring into the bases by reducing the a1 column to a unit vector. 

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢇ૚ 5KV
࢙૚






࢙૛






െࢠ






െࢠࢇ






(%9ݔଵ/%9ݏଵSLYRW
The above step is followed by an additional Simplex iteration to reach the end of Phase I. The resulting 
final tableau for phase I is shown below:

%DVLF ࢞૚
࢞૛
࢙૚
࢙૛
ࢇ૚
5KV
࢞૚

 



࢙૛






െࢠ






െࢠࢇ






Since the auxiliary variable is now nonbasic and the auxiliary objective has a zero value, the auxiliary 
problem has been solved. To turn to the original problem, we drop the ݖ௔ row and the  a1 column from the 
tableau. This results in the tableau below, which represents a valid BFS:ݔଵൌͶǡ ݏଶൌͺW to start Phase II.

%DVLF ࢞૚
࢞૛
࢙૚
࢙૛ 5KV
࢞૚

 


࢙૛





െࢠ





(%9ଵ/%9ଶSLYRW
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
82 
Linear Programming Methods
Phase II: To continue, we perform another iteration of the Simplex algorithm leading to the final tableau:

%DVLF ࢞૚
࢞૛
࢙૚
࢙૛
5KV
࢞૚





࢙૚





െࢠ





At this point the original LP problem has been solved and the optimal solution is given as: 
ݔଵ
כ ൌͺǡ ݔଶ
כ ൌͲǡ ݖכ ൌെʹͶ
The second example of the two-phase simplex method involves equality constraints and bounds on the 
optimization variables. 
Example 5.4: Two-phase Simplex algorithm for EQ constraints
We consider the following LP problem:

௫భǡ௫మݖൌʹݔଵ൅ݔଶ
6XEMHFWWRݔଵ൅ݔଶൌ͵ǡ Ͳ ൑ݔଵ൑ʹǡ Ͳ ൑ݔଶ൑ʹ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
83 
Linear Programming Methods
We first add slack variables ݏଵǡ ݏଶ to the LE constraints. The resulting standard LP problem is given as:

௫భǡ௫మݖൌʹݔଵ൅ݔଶ
6XEMHFWWRݔଵ൅ݔଶൌ͵ǡ ݔଵ൅ଵൌʹǡ ݔଶ൅ଶൌʹǢݔଵǡ ݔଶǡ ݏଵǡ ݏଶ൒Ͳ
We note that no obvious BFS for the problem exists. In order to solve the problem via two-phase simplex 
method, we add an auxiliary variable a1 to the EQ constraint and define the following auxiliary problem:

௫భǡ௫మݖ௔ൌܽଵ
6XEMHFWWRݔଵ൅ݔଶ൅ܽଵൌ͵ǡ ݔଵ൅ݏଵൌʹǡ ݔଶ൅ݏଶൌʹǢݔଵǡ ݔଶǡ ݏଵǡ ݏଶǡ ܽଵ൒Ͳ
The starting tableau for the auxiliary problem is given below:

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢇ૚ 5KV







࢙૚






࢙૛






െࢠ






െࢠࢇ






First, the auxiliary variable is made basic by producing a unit vector in the column. This is followed by 
additional Simplex iterations to reach the Phase I solution as shown in the tableaus below:

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢇ૚ 5KV
ࢇ૚






࢙૚






࢙૛






െࢠ






െࢠࢇ






(%9ݔଵ/%9ݏଵSLYRW

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢇ૚ 5KV
ࢇ૚






࢞૚






࢙૛






െࢠ






െࢠࢇ






(%9ݔଶ/%9ܽଵSLYRW
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
84 
Linear Programming Methods

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢇ૚ 5KV
࢞૛






࢞૚






࢙૛






െࢠ






െࢠࢇ






At this point, since the reduced costs are non-negative and the auxiliary objective has a zero value, Phase I 
Simplex is completed with the initial BFS: ݔଵൌʹǡ ݔଶൌͳ After dropping the auxiliary variable column 
and the auxiliary objective row, the starting tableau for Phase II Simplex is given as:

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ 5KV
࢞૛





࢞૚





࢙૛





െࢠ





We follow the initial step with further simplex iterations. The optimum is reached in one iteration and 
the final tableau is given as:

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ 5KV
࢞૛





࢞૚





࢙૛





െࢠ





Since the reduced costs are non-negative, the current solution is optimal, i.e., ݔଵ
כ ൌͳǡ ݔଶ
כ ൌʹǡ ݖכ ൌͶ. 
Later, we show that this problem is more easily solved via dual Simplex method (Sec. 5.4.2). 
5.4	
Postoptimality Analysis
Postoptimality, or sensitivity analysis, aims to study how variations in the original problem parameters 
affect the optimum solution. Postoptimality analysis serves the following purposes:
1.	 To help in managerial decision making, regarding the potential effects of increase/decrease 
in resources or raising/lowering the prices. 
2.	 To analyze the effect of modeling errors, reflected in the uncertainty in parameter values in 
the coefficient matrices ࡭ǡ ࢈ǡ ࢉ்ሻ on the final LP solution. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
85 
Linear Programming Methods
In postoptimality analysis, we are interested to explore the effects of parametric changes in ܾ௜ǡ ܿ௝ǡ and ܣ௜௝
on the optimal solution. There are five basic parametric changes affecting the LP solution (Arora, p. 229):
1.	 Changes in cost function coefficients, ܿ௝Ǣ these changes affect the level curves of the 
function.
2.	 Changes in resource limitations, ܾ௜ǢWthese changes affect the set of active constraints.
3.	 Changes in constraint coefficients, ܽ௜௝Ǣ these changes affects the active constraint gradients.
4.	 The effect of including additional constraints
5.	 The effect of including additional variables
We note that we can use the final tableau to study the effects of parameter changes on the optimal solution. 
Toward that end, we recall, from Sec. 5.3.1, that the instantaneous cost function value in the Simplex 
algorithm is represented as: 

ݖൌ்࢟࢈൅ࢉොே
்࢞ேZKHUH்࢟ൌࢉ஻
்࡮ିଵDQGࢉොே
்ൌࢉே
்െ்࢟ࡺ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Fundamental Engineering Optimization 
Methods
86 
Linear Programming Methods
Then, writing ݖൌσ ݕ௜ܾ௜
௜
൅σ ܿƸ௝ݔ௝
௝
ǡ and taking the differentials with respect to ܾ௜ǡ ܿ௝ we obtain: 
ߜܿƸ௝ൌߜܿ௝െ்࢟ߜࡺ௝ǡ ߜݖൌσ ݕ௜ߜܾ௜
௜
൅σ ߜܿƸ௝ݔ௝
௝
 where ࡺ௝ represents the jth column of N. The above 
formulation may be used to analyze the effects of changes to ܾ௜ǡ ܿ௝ǡ and ࡺ௝ on z. Those results are 
summarized below (Belegundu & Chandrupatla, p. 167):
1.	 Changes to the resource constraints (rhs). A change in ܾ௜ has the effect of moving the 
associated constraint boundary. Then,
a)	 If the constraint is currently active ݕ௜൐Ͳ the change will affect the current basic 
solution: ݔ࡮ൌ࢈෩ǡ as well as ݖ௢௣௧ If the new ݔ࡮ is feasible, then ݖ௢௣௧ൌ்࢟࢈ is the 
new optimum value. If the new ݔ࡮ is infeasible, then dual Simplex steps may be used to 
restore feasibility (and hence optimality). 
b)	 If the constraint is currently non-active ݕ௜ൌͲ then ݖ௢௣௧and ݔ࡮ are not affected.
2.	 Changes to the objective function coefficients. Changes to ܿ௝ affect the level curves of z. 
Then,
a)	 If ܿ௝א ࢉ஻ then since the new ܿƸ௝്Ͳ Gauss-Jordan eliminations are needed to return ݔ௝
to the basis. If optimality is lost in the process (any ܿƸ௝൏Ͳሻ), further Simplex steps will 
be needed to restore optimality. If optimality is not affected, then ݖ௢௣௧ൌ்࢟࢈ is the new 
optimum.
b)	 If I ܿ௝א ࢉே though it does not affect ݖ, still ܿƸ௝ needs to be recomputed and checked for 
optimality.
3.	 Changes to the coefficient matrix. Changes to the coefficient matrix affect the constraint 
boundaries. For a change in ࡭௝ ݆WKFROXPQRI࡭
a)	 If ࡭௝א ࡮ corresponding to a basic variable, then Gauss-Jordan eliminations are 
needed to reduce ܣ௝ to a unit vector; then ܿƸ௝ needs to be recomputed and checked for 
optimality.
b)	 If I ࡭௝א ࡺǡ corresponding to a nonbasic variable, then the reduced cost ܿƸ௝ needs to be 
recomputed and checked for optimality. 
4.	 Adding Variables. If we add a new variable ݔ௡ାଵ to the problem, then the cost function 
is updated as: ݖൌࢉ்࢞൅ܿ௡ାଵݔ௡ାଵ In addition, a new column ܣ௡ାଵ is added to the 
constraint matrix. The reduced cost corresponding to the new column is computed as: 
ܿ௡ାଵെ்࢟ܣ௡ାଵ. Then, if this cost is positive, optimality is maintained; otherwise, further 
Simplex iterations are needed to recover optimality. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
87 
Linear Programming Methods
5.	 Adding inequality Constraints. Assume that we add an inequality constraint to the 
problem. Adding a constraint adds a row and the associated slack/surplus variable 
adds a column to the tableau. In this case, we need to check if adding a column 
to the basis affects the current optimum. We define an augmented B matrix as: 
࡮ൌ൤࡮
Ͳ
ࢇ஻
்
ͳ൨ǡZKHUH࡮ିଵൌ൤࡮ିଵ
Ͳ
ࢇ஻
்࡮ିଵ
ͳ൨ and write the augmented final tableau as:

%DVLF
࢞࡮
࢞ࡺ
5KV
࢞࡮
ࡵ
࡮ିଵࡺ
࡮ିଵ࢈
࢞࢔ା૚
ࡵ
ࢇ஻
்࡮ିଵࡺ
ࢇ஻
்࡮ିଵ࢈൅ܾ௡ାଵ
െࢠ
૙
ࢉே
்െ்࢟ࡺ
െ்࢟࢈
Since the reduced costs are not affected, if ࢇ஻
்࡮ିଵ࢈൅ܾ௡ାଵ൐Ͳ optimality is maintained. If 
not, we choose this row as the pivot row and apply dual Simplex steps (Sec. 5.5.2) to recover 
optimality. 
Further results on sensitivity analysis involve parametric linear programming, aimed at analyzing the 
range of parameters values in the perturbed solution that maintain feasibility and/or optimality. These 
ranges are normally reported by commercial analysis software. Interested readers may consult Sec. 6.5 
in (Griva, Nash & Sofer, 2009).
The following problem adopted from (Belegundu & Chandrupatla, p. 122) is used to illustrate the ideas 
presented in this section. 
Example 5.5: Postoptimality Analysis 
A vegetable farmer has the choice to grow tomatoes, green peppers, or cucumbers on his 200 acre farm. 
The man-days/per acre needed for growing the three vegetables are 6, 7 and 5, respectively. A total of 
500 man-hours are available. The yield/acre for the three vegetables are in the ratios: 4.5:3.6:4.0. We wish 
to determine the optimum crop combination that maximizes total yield.
The optimization problem was solved using the Simplex method. The initial and the final tableaus for 
the problem are reproduced below: 
,QLWLDO
%DVLF
࢞૚
࢞૛
࢞૜ ࢙૚ ࢙૛ 5KV
࢙૚






࢙૛






െࢠ
 




Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
88 
Linear Programming Methods
)LQDO
%DVLF
࢞૚
࢞૛
࢞૜ ࢙૚
࢙૛
5KV
࢙૚
 


 
࢞૜






െࢠ






From the final tableau, the optimum crop combination is given as: ݔଵ
כ ൌͲǡ ݔଶ
כ ൌͲǡ ݔଷ
כ ൌͳͲͲ with 
ݖכ ൌ 400. Further, the shadow prices for the slack variables are: 
ଷ
்࢟ൌሾͲǡ ͲǤͺሿZLWKݖכ ൌ்࢟࢈ൌͶͲͲ
Next, without re-solving the problem, we wish to answer the following questions:
a)	 If an additional 50 acres are added, what is the expected change in yield? The answer is 
found from: ݖכ ൌ்࢟ሺ࢈൅οሻZKHUHοൌሾͷͲǡͲሿ்ZLWKݖכ ൌͶͲͲ i.e., there is no expected 
change in yield. This also means that the land area constraint is not binding in the current 
optimum solution.
b)	 If an additional 50 man-days are added, what is the expected change in yield? The answer is 
found from: ݖכ ൌ்࢟ሺ࢈൅οሻZKHUHοൌሾͲǡ ͷͲሿ்ZLWKݖכ ൌͶͶͲ i.e., the yield increases by 
40 units. This also means that the man-days constraint is binding in the optimum solution.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Fundamental Engineering Optimization 
Methods
89 
Linear Programming Methods
c)	 If the yield/acre for tomatoes increases by 10%, how is the optimum affected? The answer 
is found by re-computing the reduced costs as: ࢉො்ൌ்࢟࡭െࢉ்ൌሾെͲǤͳͷǡ ʹǡ Ͳሿ Since a 
reduced cost is now negative, additional Simplex steps are needed to regain optimality. This 
is done and the new optimum is: ݔଵ
כ ൌͺ͵Ǥ͵͵ǡ ݔଶ
כ ൌͲǡ ݔଷ
כ ൌͲZLWKݖכ ൌͶͳʹǤͷ 
d)	 If the yield/acre for cucumbers drops by 10%, how is the optimum be affected? The answer 
is found by re-computing the reduced costs as: ࢉො்ൌ்࢟࡭െࢉ்ൌሾͲǤ͵ǡ ʹǡ ͲǤͶሿ The reduced 
costs are non-negative, but ࢞૜ is no more a basic variable. Regaining the basis results in 
reduced cost for ࢞ଵ becoming negative. Additional Simplex steps are performed to regain 
optimality, and the new optimum is: ݔଵ
כ ൌͺ͵Ǥ͵͵ǡ ݔଶ
כ ൌͲǡ ݔଷ
כ ൌͲZLWKݖכ ൌ͵͹ͷ 
e)	 If the man-hours needed to grow green peppers increase to 5/acre, how is the optimum 
affected? The answer is found by re-computing the reduced cost: ܿƸଶൌݕ்ܣଶെܿଶൌͲǤͶ 
Since ݔଶZ was non-basic and the revised reduced cost is non-negative, there is no change in 
the optimum solution.
5.5	
Duality Theory for the LP Problems
In this section we extend the Lagrangian duality (Sec. 4.5) to the LP problems. Duality theory applies to 
practical LP problems in engineering and economics. In engineering, for example, the primal problem in 
electric circuit theory may be posed in terms of electric potential, and its dual in terms of current flow. 
Similarly, an optimization problem in mechanics may be modeled with strains, and its dual modeled 
with stresses. In economics, if the primal problem seeks to optimize price per unit of product, its dual 
may seek to minimize cost per unit of resources. 
The LP duality is defined in the following terms: associated with every LP problem is a dual problem 
that is formulated in terms of dual variables, i.e., the Lagrange multipliers. In the symmetric form of 
duality, the primal (P) and the dual (D) LP problems are stated as:
3
࢞
ݖൌࢉ்࢞VXEMHFWWR࡭࢞൑࢈ǡ ࢞൒૙
'
࢟
ݓൌ்࢟࢈VXEMHFWWR்࢟࡭൒ࢉ்ǡ ࢟൒૙
(5.10)
where ࢞א Թ௡ denotes the primal variables and G࢟א Թ௠ denotes the dual variables. We note that, based 
on the definition of duality, the dual of the dual (D) is the same as primal (P). 
To define the dual of an arbitrary LP problem we first convert the problem into an equivalent problem 
of the primal form. The dual problem can then be formulated accordingly. For example, when (P) is 
given in the standard LP form, then (D) takes the following form: 
3
࢞ݖൌࢉ்࢞VXEMHFWWR࡭࢞ൌ࢈ǡ ࢞൒૙
'
࢟
ݓൌ்࢟࢈VXEMHFWWR்࢟࡭൑ࢉ்
(5.11)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
90 
Linear Programming Methods
where Lagrange multipliers y for the equality constraints in the dual formulation are unrestricted in sign. 
To obtain the above dual formulation, we use the following equivalence: ࡭࢞ൌ࢈
֞ ࡭࢞൒࢈ǡ െ࡭࢞൒െ࢈ǡ 
or: ሾ࡭ െ࡭ሿቂ࢞
࢞ቃ൒ቂ࢈
െ࢈ቃ We can then use the symmetric form of duality where the dual variable vector 
is given as: ሾ்࢛ǡ ்࢜ሿ We obtain the above result by designating dual variables as: ࢟ൌ࢛െ࢜Ǣ ࢛ǡ ࢜൒૙
where ࢟ is unrestricted in sign. 
The following example is used to explain LP duality.
Example 5.6: Duality in LP problems

$
%
&
'




To illustrate duality, we consider the problem of sending goods from node A to node D in a simplified 
network (Pedregal, p. 45). Assuming that the total quantity to be shipped equals 1, let ݔ௜௝ denote the 
fractional quantity to be shipped via link ݆݅ with associated transportation cost ܿ௜௝ (shown in the figure). 
Then, the primal objective is to minimize the transportation costs and the primal problem is formulated as: 

࢞ݖൌʹݔ஺஻൅͵ݔ஺஼൅ݔ஻஼൅Ͷݔ஻஽൅ʹݔ஼஽
Subject to: ݔ஺஻ൌݔ஻஼൅ݔ஻஽ǡ ݔ஺஼൅ݔ஻஼ൌݔ஼஽ǡ ݔ஻஽൅ݔ஼஽ൌͳ (equivalently, ݔ஺஻൅ݔ஺஼ൌͳሻǢ 
ݔ஺஻ǡ ݔ஻஼ǡ ݔ஺஼ǡ ݔ஻஽ǡ ݔ஼஽൒Ͳ
Alternatively, we may consider ݕூ to be the price of goods at node I, and ݕூെݕ஺ǡ as the profit to be 
made in transferring the goods from A to I. Then, the dual objective is to maximize the profit at node 
D. Then, if we arbitrarily assign: ݕ஺ൌͲ the dual formulation is given as: 

࢟
ݕ஽
Subject to: ݕ஻൑ʹǡ ݕ஼൑͵ǡ ݕ஼െݕ஻൑ͳǡ ݕ஽െݕ஻൑Ͷǡ ݕ஽െݕ஼൑ʹ
Finally, we note that both problems can be formulated in terms of following coefficient matrices:

ܣൌ൥
ͳ
Ͳ
Ͳ
ͳ
Ͳ
Ͳ

െͳ
െͳ
Ͳ
Ͳ
Ͳ
ͳ

Ͳ
െͳ
ͳ
൩ǡ ܾൌ൥
Ͳ
Ͳ
ͳ
൩ǡ ்ܿൌሾʹ͵ͳͶʹሿ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
91 
Linear Programming Methods
5.5.1	
 Fundamental Duality Properties
Duality theory confers fundamental properties on the optimization problem that relate the primal and 
dual linear programs. Specifically, these properties specify bounds on the two objectives and are useful 
in developing computational procedures to solve the primal and dual problems. These properties are 
stated below for the symmetric form of duality where (P) solves the maximization problem. 
Weak Duality. Let x denote a feasible solution to (P) and y a feasible solution to (D), then, 
்࢟࢈൒்࢟ۯ࢞൒ࢉ்࢞LHݓሺ࢟ሻ൒ݖሺ࢞ሻ Further, the difference between these two objective functions, 
࢈்࢟െࢉ்࢞ǡ is referred to as the duality gap. 
Optimality. If x is a feasible solution to (P) and a y feasible solution to (D), and, further, ࢉ்࢞ൌ࢈்࢟ 
then x is an optimal solution to (P), and y an optimal solution to (D). 
Unboundedness. If the primal (dual) problem is unbounded, then the dual (primal) problem is infeasible 
(i.e., the feasible region is empty). We note that this is a necessary consequence of weak duality.
Strong Duality. If the primal (dual) problem has a finite optimal solution, then so does the dual (primal) 
problem; further, these two optimums are equal, i.e., ݓ௢௣௧ൌ்࢟࢈ൌ்࢟ۯ࢞ൌࢉ்࢞ൌݖ௢௣௧ 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
92 
Linear Programming Methods
Further, x if is the optimal solution to (P), then ்࢟ൌࢉ஻
்࡮ିଵ is the optimal solution to (D), which can 
be seen from: ݓൌ்࢟࢈ൌࢉ஻
்࡮ିଵ࢈ൌࢉ஻
்ݔ࡮ൌࢉ்࢞ൌݖ 
We also note that the optimality of (P) implies the feasibility of (D), and vice versa. In particular, ݔ࡮൒૙ 
RU࢞൒૙ implies primal feasibility and dual optimality; whereas, ࢉොே
்൒૙RUࢉොൌࢉെ࡭்࢟൒૙ 
implies primal optimality and dual feasibility. 
Complementary Slackness. At the optimal point, we have: ்࢞ࢉൌ்࢞࡭்࢟ implying: ்࢞ሺࢉെ࡭்࢟ሻൌ
σ ݔ௝ሺܿെܣ்ݕሻ௝
࢐
ൌͲ which shows that it is not possible to have both ݔ௝൐Ͳ and ሺܣ்ݕሻ௝൏ܿ௝ at the 
optimum. 
Thus, if the jth primal variable is basic, i.e., ݔ௝ൌͲ then the th dual constraint is binding, i.e., ሺܣ்ݕሻ௝ = ܿ௝; 
and, if the jth primal variable is non-basic, i.e., ݔ௝ൌͲ then the jth dual constraint is non-binding, i.e., 
ሺܣ்ݕሻ௝൏ܿ௝
5.5.2	
 The Dual Simplex Method 
The dual simplex method involves application of the simplex method to the dual problem. Complementary 
to the regular simplex algorithm that initializes with a valid BFS and moves through primal feasible 
solutions, the dual simplex algorithm initializes with and moves through dual feasible (primal infeasible) 
solutions. The dual simplex algorithm thus iterates outsides of the feasible region. As such, the dual 
simplex method provides a convenient alternative to the two-phase simplex method in the event the 
optimization problem has no feasible solution (Sec. 5.3.2). 
To develop the dual simplex algorithm, we consider the minimization problem formulated with dual 
variables (5.10). We note that primal optimality ሺࢉො൒૙ሻ corresponds to dual feasibility ሺ்࢟࡭൒ࢉ்ሻ 
and primal feasibility ሺ࢞൒૙ሻ corresponds to dual optimality. We therefore assume that the objective 
function coefficients are positive and the rhs is partly negative VRPHܾ௜൏Ͳ The dual simplex algorithm 
then proceeds in a similar fashion to the primal algorithm except that:
1.	 The points generated during dual simplex iterations are primal infeasible as some basic 
variables have negative values. 
2.	 The solutions are always optimal in the sense that the reduced cost coefficients for nonbasic 
variables are non-negative.
3.	 An optimal is reached when a feasible solution with non-negative values for the basic 
variables has been found.
A tableau implementation of the dual Simplex algorithm proceeds as follows: after subtracting the surplus 
variables from GE constraints in (5.12) we multiply those constraints by –1 We then enter the constraints 
and the cost function coefficients in a tableau noting that the initial basic solution is infeasible. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
93 
Linear Programming Methods
At each iteration, the pivot element in the dual simplex method is determined as follows:
1.	 A pivot row ܣ௤் is selected as the row that has the basic variable with most negative value.
2.	 The ratio test to select the pivot column is conducted as: 
௜
൜
௖ೕ
ି஺೜ǡೕǣܿ௝൐Ͳǡ ܣ௤ǡ௝൏Ͳൠ
The dual simplex algorithm terminates when the rhs has become non-negative. 
5.5.3	
 Recovery of the Primal Solution
The final tableaus resulting from the application of simplex methods to the primal and dual problems 
are intimately related. In particular, the elements in the last row of the final dual tableau replicate the 
elements in the last column of the final primal tableau, and vice versa. This fact allows the recovery 
of primal solution from the final dual tableau. Let the dual problem be solved using standard simplex 
method, then the value of the ith primal variable equals the reduced cost coefficient of the slack or 
surplus variable associated with the ith dual constraint in the final dual tableau. In addition, if the dual 
variable is nonbasic, then its reduced cost coefficient equals the value of the slack or surplus variable for 
the corresponding primal constraint.
To reveal the above relationship, we re-consider the dual problem in (5.12), which, after subtracting 
surplus variables, is represented in the following equivalent form: 

࢟ݓൌ்࢟࢈
6XEMHFWWR்࢟࡭െࡵ࢙ൌࢉ்ǡ ࢟൒૙
An initial tableau for the dual problem, with s as the basic variables, is given as:
%DVLF
࢟
࢙
5KV
࢙
െ࡭்
ࡵ
െࢉ
െ࢝
࢈்
૙
Ͳ
Assuming that the same order of the variables is maintained, the final tableau at the termination of dual 
simplex algorithm may be given as:
%DVLF
࢞
࢙
5KV
࢟࡮
࡭෩
ࡿ
ࢉ෤
െ࢝
࢈෡்
்࢞
െݖכ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
94 
Linear Programming Methods
where we note that the primal variables appear in the last row under the slack/surplus variable columns. 
Further, the coefficients in the final tableau are related to those in the initial tableau as follows: 
࡭෩ൌെࡿ࡭்ǡࢉ෤ൌെࡿࢉǡ
࢈෡்ൌ࢈்െ்࢞࡭்ǡ
ݖכ ൌࢉ்࢞
(5.13)
The following examples illustrate the efficacy of the dual Simplex algorithm.
Example 5.7: Dual Simplex algorithm
We consider the dual of Example 5.1 where the original LP problem was defined as:

௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWRʹݔଵ൅ݔଶ൑ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸Ǣݔଵ൒Ͳǡ ݔଶ൒Ͳ
Using the symmetric form of duality, the dual optimization problem is defined as:

௬భǡ௬మݓൌͳʹݕଵ൅ͳ͸ݕଶ
6XEMHFWWRʹݕଵ൅ʹݕଶ൒͵ǡ ݕଵ൅͵ݕଶ൒ʹǢݕଵ൒Ͳǡ ݕଶ൒Ͳ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Fundamental Engineering Optimization 
Methods
95 
Linear Programming Methods
We subtract surplus variables from the GE constraints and multiply them with –1 before entering them 
in the initial tableau. We then follow with dual simplex iterations. The resulting series of tableaus is 
given below: 
%DVLF ࢟૚ ࢟૛ ࢙૚ ࢙૛ 5KV
࢙૚





࢙૛





െ࢝




Ͳ
(%9ݕଵ/%9ଵSLYRW

%DVLF ࢟૚ ࢟૛
࢙૚
࢙૛ 5KV
࢟૚





࢙૛





െ࢝





/%9ଶ(%9ݕଶSLYRW

%DVLF ࢟૚ ࢟૛
ܛ૚
ܛ૛
5KV
࢟૚





࢟૛




ó
െ࢝





At this point the dual LP problem is solved and the optimal solution is: ݕଵൌͳǤʹͷǡ ݕଶൌͲǤʹͷǡ ݓ௢௣௧ൌͳͻ 
We note that the first feasible solution obtained above is also the optimal solution. We further note that:
a)	 The optimal value of the objective function for (D) is the same as the optimal value for (P). 
b)	 The optimal values for the basic variables for (P) appear as reduced costs associated with 
non-basic variables in (D).
As an added advantage, the dual simplex method obviates the need for the two-phase simplex method to 
obtain a solution when an initial BFS is not readily available. This is illustrated by re-solving Example 5.3 
using the dual simplex algorithm. 
Example 5.8: Dual Simplex algorithm
We consider the dual problem of Example 5.3. The original LP problem is stated as:

௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWR͵ݔଵ൅ʹݔଶ൒ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸ǡ ݔଵ൒Ͳǡ ݔଶ൒Ͳ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
96 
Linear Programming Methods
The GE constraint in the problem is first multiplied by –1; the problem is then converted to dual problem 
using the symmetric form of duality. The dual optimization problem is given as:

௬భǡ௬మݖଵൌെͳʹݕଵ൅ͳ͸ݕଶ
6XEMHFWWRെ͵ݕଵ൅ʹݕଶ൒͵ǡ െʹݕଵ൅͵ݕଶ൒ͳǢݕଵ൒Ͳǡ ݕଶ൒Ͳ 
The series of tableaus leading to the optimal solution via the dual simplex method is given below:

%DVLF
࢟૚
࢟૛ ࢙૚ ࢙૛ 5KV
࢙૚





࢙૛





െ࢝
 


Ͳ
/%9ଵ(%9ݕଶSLYRW

%DVLF
࢟૚
࢟૛
࢙૚
࢙૛ 5KV
࢟૛





࢙૛





െ࢝





At this point the dual LP problem is solved with the optimal solution: ݕଵ
כ ൌͲǡ ݕଶ
כ ൌͳǤͷǡ ݓכ ൌʹͶ 
We note that this is the same solution obtained for Example 5.3. We further note that the reduced costs 
for nonbasic variables match with the optimal values of the primal basic variables. 
The final dual Simplex example involves a problem with equality constraints.
Example 5.9: Equality Constraints
We re-consider Example 5.4 where the optimization problem was given as:

௫భǡ௫మݖൌʹݔଵ൅ݔଶ
6XEMHFWWRݔଵ൅ݔଶൌ͵ǡ Ͳ ൑ݔଵǡ ݔଶ൑ʹ
In order to solve this problem via the dual Simplex method, we replace the equality constraint with twin 
inequality constraints: ሼݔଵ൅ݔଶൌ͵ሽ՞ ሼݔଵ൅ݔଶ൑͵ǡ ݔଵ൅ݔଶ൒͵ሽ Next, we multiply GE constraint 
with , and add slack variables to all inequalities. Finally, we identify: ݏଵݏଶݏଷݏସ as basic variables, 
and construct an initial tableau for the dual simplex method. This is followed by two iterations of the 
dual simplex algorithm leading to the optimum. The resulting tableaus for the problem are given below: 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
97 
Linear Programming Methods

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢙૜ ࢙૝ 5KV
࢙૚
ͳ
ͳ
ͳ
Ͳ
Ͳ
Ͳ
͵
࢙૛
Ǧͳ
Ǧͳ
Ͳ
ͳ
Ͳ
Ͳ
Ǧ͵
࢙૜







࢙૝







െࢠ






Ͳ
/%9ଶ(%9ݔଶSLYRW

%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢙૜ ࢙૝ 5KV
࢙૚
Ͳ
Ͳ
ͳ
ͳ
Ͳ
Ͳ
Ͳ
࢞૛
ͳ
ͳ
Ͳ
Ǧͳ
Ͳ
Ͳ
͵
࢙૜







࢙૝







െࢠ






െ͵
/%9ସ(%9ݔଵSLYRW
%DVLF ࢞૚ ࢞૛ ࢙૚ ࢙૛ ࢙૜ ࢙૝ 5KV
࢙૚
Ͳ
Ͳ
ͳ
ͳ
Ͳ
Ͳ
Ͳ
࢞૛
Ͳ
ͳ
Ͳ
Ͳ
Ͳ
ͳ
ʹ
࢙૜







࢞૚







െࢠ






െͶ
The dual Simplex algorithm terminates with ݖ௢௣௧ൌͶ 
5.6	
Non-Simplex Methods for Solving LP Problems
The non-simplex methods to solve LP problems include the interior-point methods that iterate through 
the interior of the feasible region, and attempt to decrease the duality gap between the primal and dual 
feasible solutions. These methods can have good theoretical efficiency and practical performance that 
is comparable with the simplex methods. In the following, we discuss the primal-dual interior-point 
method that has been particularly successful in the case of LP problems (Griva, Nash & Sofer, p. 321).
To introduce the primal-dual method, we consider asymmetrical form of duality where the primal and 
dual problems are described as:
3
࢞ݖൌࢉ்࢞
VXEMHFWWR࡭࢞ൌ࢈ǡ ࢞൒૙
(5.14)
'
࢞
ݓൌ࢈்࢟
VXEMHFWWR࡭்࢟൅࢙ൌ࢈ǡ ࢙൒૙
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
98 
Linear Programming Methods
We note that for x and y to be the feasible solutions to the primal and dual problems (at the optimum), 
they must satisfy the following complementary slackness condition: ݔ௝ݏ௝ൌͲǡ ݆ൌͳǡ ǥ ǡ ݊ The primal-
dual method begins with ݔ௝ݏ௝ൌߤǡ: for some ߤ൐Ͳǡ and iteratively reduces the values of ߤ, generating a 
series of vectors: ࢞ሺߤሻǡ ࢟ሺߤሻǡ ࢙ሺߤሻ along the way, in an effort to reduce the duality gap: ࢉ்࢞െ࢈்࢟ൌ݊ߤ.
To develop the primal-dual algorithm, let the updates to the current estimates: ࢞ǡ ࢟ǡ ࢙ǡ be given as: 
࢞൅ο࢞ǡ ࢟൅ο࢟ǡ ࢙൅ο࢙Ǣ then, these updates are required to satisfy the following feasibility and 
complementarity conditions: ࡭ሺ࢞൅ο࢞ሻൌ࢈ǡ ࡭்ሺ࢟൅ο࢟ሻሺ࢙൅ο࢙ሻൌࢉǡ ሺ࢞൅ο࢞ሻ்ሺ࢙൅ο࢙ሻൌ૙ 
Accordingly,
࡭ο࢞ൌ૙
࡭்ο࢟൅ο࢙ൌ૙
൫ݔ௝൅οݔ௝൯൫ݏ௝൅οݏ௝൯؆ ݔ௝ݏ௝൅ݔ௝οݏ௝൅ݏ௝οݔ௝ൌߤ

(5.15)
where the latter condition has been linearized for ease of implementation. To proceed further, we define: 
ࢄൌ݀݅ܽ݃ሺ࢞ሻǡ ࡿൌ݀݅ܽ݃ሺ࢙ሻǡ ࢋൌሾͳǡ ǥ ǡͳሿ்ǡ to express the complementarity condition as: ࢄࡿࢋൌߤࢋ 
Next, let ࡰൌࡿିଵࢄǡ ࢜ሺߤሻൌሺߤࡵെࢄࡿሻࢋǡWthen a solution to the linear system (5.16) is given as:
ο࢞ൌࡿିଵ࢜ሺߤሻെࡰο࢙
ο࢟ൌെሺ࡭ࡰ࡭்ሻିଵ࡭ࡿିଵ࢜ሺߤሻ
ο࢙ൌെ࡭்ο࢟

(5.16)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Fundamental Engineering Optimization 
Methods
99 
Linear Programming Methods
In practice, to ensure primal and dual feasibility, the following update rule for the solution vectors has 
been suggested (Griva, Nash & Sofer, p. 324):
࢞௞ାଵൌ࢞௞൅ߙ௞ο࢞௞ǡ
࢟௞ାଵൌ࢟௞൅ߙ௞ο࢟௞ǡ
࢙௞ାଵൌ࢙௞൅ߙ௞ο࢙௞
ߙ௞൏ሺߙ௉ǡ ߙ஽ሻǡ
ߙ௉ൌ
ο௫ೕழ଴െݔ௝
ȟݔ௝
ǡ
ߙ஽ൌ
ο௦ೕழ଴െݏ௝
ȟݏ௝

Finally, an initial estimate that satisfies (5.9) is needed to start the primal-dual method. To find that 
estimate, let the constraint equation for the primal problem (5.7) be written as: ࡵ࢞஻൅ࡽ࢞ேൌ࢈Ǣthen, 
for some ࢞଴ǡ ࢟଴ǡ a set of feasible vectors satisfying (5.9) is obtained as: ࢞ൌቂ
࢞଴
࢈െࡽ࢞଴ቃǡ ࢟ൌ࢟଴ǡ ࢙ൌ 
൤ࢉെ࡭்࢟଴
െ࢟଴
൨
Further, the bounding parameter μ is updated in successive iterations as: ߤ௞ାଵൌߛߤ௞ǡ Ͳ ൏ߛ൏ͳǡ
where ߛൌͲǤͳ is considered a reasonable choice. 
The primal-dual algorithm is given as follows:
Primal-Dual Algorithm:
Given ࡭ǡ ࢈ǡ ࢉ
Initialize: select: ߳൐Ͳǡ ߤ൐Ͳǡ Ͳ ൏ߛ൏ͳܰ (maximum number of iterations). 
Find initial ࢞ǡ ࢟ǡ ࢙൐૙ to satisfy (5.9).
For U݇ൌͳǡʹǡ ǥ
1.	 Check termination: if ்࢙࢞െ݊ߤ൏߳ǡRULI݇൐ܰǡ quit.
2.	 Use (5.16) to compute the updates vectors.
3.	 Use (5.17) to compute ߙ௞ and perform the update. 
4.	 Set ߤึߛߤ
An example of the primal-dual algorithm is presented below.
Example 5.10: Primal-Dual Algorithm
We re-consider Example 5.1 where the original optimization problem was given as:

௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWRʹݔଵ൅ݔଶ൑ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸Ǣݔଵ൒Ͳǡ ݔଶ൒Ͳ
The coefficient matrices for the problem are: ࡭ൌቂʹ
ͳ
ʹ
͵ͳ
Ͳ
Ͳ
ͳቃǡ ࢈ൌቂͳʹ
ͳ͸ቃǡ ࢉ்ൌሾെ͵
െʹሿ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
100 
Linear Programming Methods
To initialize the primal-dual algorithm, we select the following parameters: 
࢞଴ൌሾʹǡ ʹሿ்ǡ ࢟଴ൌሾെͳǡ െͳሿ்߳ൌͳͲି଺ǡ ߤൌͳͲǡ ߛൌͲǤͳܰൌͳͲ
Then, the initial estimates for the variables are: 
ߛ
ݔ்ൌሾʹǡ ʹǡ ͸ǡ ͸ሿǡ ݕ்ൌሾെͳǡ െͳሿǡ ݏ்ൌሾͳǡ ʹǡ ͳǡ ͳሿ
The variable updates for the first eight iterations are given below, where the last column contains the 
residual:
࢞૚
࢞૛
࢙૚
࢙૛
࢞ࢀ࢙െ࢔ࣆ








The optimum solution is given as: ݔଵ
כ ൌͷǤͲǡ ݔଶ
כ ൌʹǤͲǡ which agrees with the results of Example 5.1.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
101 
Linear Programming Methods
5.7	
Optimality Conditions for LP Problems
This section discusses the application of FONC to the LP problems. The first order optimality conditions 
in the case of general optimization problems are known as the KKT conditions. For convex optimization 
problems, the KKT conditions are both necessary and sufficient for optimality. 
5.7.1	
 KKT Conditions for LP Problems
To derive the KKT conditions for the LP problems, we consider a maximization problem proposed in 
(5.10) above. Using slack variables, the problem is converted into standard form as:

࢞ݖൌെࢉ்࢞
VXEMHFWWR࡭࢞െ࢈൅࢙ൌ૙ǡ ࢞൒૙
(5.18)
We now use Lagrange multiplier vectors ࢛ǡ ࢜ for the constraints to write a Lagrangian function as:
ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌെࢉ்࢞െ்࢛࢞൅்࢜ሺ࡭࢞െ࢈൅࢙ሻ
Then, the first order KKT conditions for the optimality of the solution vector are:
Feasibility: ࡭࢞െ࢈൅࢙ൌ૙
Optimality: ׏ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌ࡭்࢜െࢉെ࢛ൌ૙
Complementarity: ்࢛࢞൅்࢙࢜ൌ૙
Non-negativity: ࢞൒૙ǡ ࢙൒૙ǡ ࢛൒૙ǡ ࢜൒૙
The above equations need to be simultaneously solved for the unknowns: ࢞ǡ ࢙ǡ ࢛ǡ ࢜ to find the optimum. 
By substituting ࢙ǡ ࢛ from the first two equations into the third, the optimality conditions are reduced to:
்࢜ሺ࡭࢞െ࢈ሻൌ૙ǡ
்࢞ሺࢉെ࡭்࢜ሻൌ૙ǡ
࢞൒૙ǡ
࢜൒૙
Therefore, the following duality conditions are implied at the optimum point: 
a)	 Lagrange multipliers for the active (binding) constraints are positive, and 
b)	 Dual constraints associated with basic variables are binding. 
Alternatively, we can solve the optimality conditions by partitioning the problem into basic and 
nonbasic variables as: ்࢞ൌሾ࢞஻
்ǡ ࢞ே
்ሿǢ ࢉ்ൌሾࢉ஻
்ǡ ࢉே
்ሿǢ ࡭ൌሾ࡮ǡ ࡺሿǢ்࢛ൌሾ࢛஻
்ǡ ࢛ே
்ሿ Then, in terms 
of partitioned variables, the optimality conditions are given as:
ቂ࡮்
ࡺ்ቃ࢜െቂ࢛஻
࢛ேቃെቂࢉ஻
ࢉேቃൌቂ૙
૙ቃǡ
ሾ࢞஻࢞ேሿቂ࢛஻
࢛ேቃൌ૙
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
102 
Linear Programming Methods
Since ࢞஻്Ͳǡ࢛஻ൌͲThen, from the first equation, ்࢜ൌࢉ஻
்࡮ିଵǡ and from the second equation, 
࢛ே
்ൌࢉ஻
்࡮ିଵࡺെࢉே
்ൌࢉොே
் Thus, the reduced cost coefficients for nonbasic variables are the Lagrange 
multipliers, which are required to be non-negative at the optimum, i.e., ࢛ே൐ͲǤ
We can extend the optimality conditions to the dual problem formulated in (5.10). For the symmetric 
form of duality, the KKT conditions for the primal and dual problems are given as (Belegundu and 
Chandrapatla, p. 161):
	
Primal	
Dual
Feasibility: 	
	
࡭࢞൅࢙ൌ࢈ 	 	
	
࡭்࢜െ࢛ൌࢉ	
Optimality: 	
	
ࢉൌ࡭்࢜െ࢛	 	
	
࢈ൌ࡭࢞൅࢙
Complementarity: 	
	
்࢛࢞൅்࢙࢜ൌ૙	
Non-negativity: 	
	
࢞൒૙ǡ ࢙൒૙ǡ ࢛൒૙ǡ ࢜൒૙	
We note that the optimality condition for (P) is equivalent to the feasibility condition for (D) and vice 
versa, i.e., by interchanging the feasibility and optimality conditions, we may view the problem as primal 
or dual. It also shows that if (P) is unbounded, then (D) is infeasible, and vice versa. 
5.7.2	
 A Geometric Viewpoint
Further insight into the solution is obtained from geometrical consideration of the problem. Towards that 
end, let A be expressed in terms of row vectors as: ࡭்ൌሾࢇଵ
்ǡ ࢇଶ
்ǡ ǥ ǡ ࢇ௠
்ሿ where ࢇଵ
் represents a vector 
normal to the constraint: ࢇ௜
்࢞൅ݏ௜ൌܾ௜ Similarly, let െࢋ௝ denote a vector normal to the non-negativity 
constraint: െݔ௝൑Ͳ Then, the optimality requires that there exist real numbers, ݒ௜൒Ͳǡ ݅ൌͳǡ ǥ ǡ ݉
and ݑ௝൒Ͳǡ ݆ൌͳǡ ǥ ǡ ݊ such that the following conditions hold: 
ࢉൌσ ݒ௜
௜
ࢇ௜
்െσ ݑ௝
௝
ࢋ௝
σ ݒ௜ݏ௜
௜
൅σ ݑ௝ݔ௝
௝
ൌͲ

(5.21)
Let the Lagrange multipliers be grouped as: ߤ௜א ൛ݑ௜ǡ ݒ௝ൟ and let ܰ௜א ሼࢇ௜
்ǡ െࢋ௝ሽ denote the set of 
active constraint normals, then the complementarity condition is expressed as: ࢉൌെ׏ݖൌσ
ߤ௜
௜אࣣ
ܰ௜ 
where ࣣ denotes the set of active constraints. 
The above condition states that at the optimal point the negative of objective function gradient lies in the 
convex cone spanned by the active constraint normals. When this condition holds, the descent-feasible 
cone is empty, i.e., we cannot move in a direction that further decreases the objective function without 
leaving the feasible region. This result is consistent with Farkas Lemma, which for the LP problems is 
stated as follows:
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
103 
Linear Programming Methods
Farka’s Lemma (Belegundu and Chandrupatla, p. 204): Given a set of vectors, ࢇ௜ǡ ݅ൌͳǡ ǥ ǡ ݉ and a 
vector c, there is no vector d satisfying the conditions ࢉ்ࢊ൏Ͳ and ࢇ௜
்ࢊ൐Ͳǡ ݅ൌͳǡ ǥ ǡ ݉ if and only 
if c can be written as: ࢉൌσ
ߤ௜ࢇ௜
௠
௜ୀଵ
ǡ ߤ௜൒Ͳ
An illustrative example for the optimality conditions appears below:
Example 5.11: Optimality Conditions for the LP problem
We reconsider example 5.1 that was formulated as:

௫భǡ௫మݖൌ͵ݔଵ൅ʹݔଶ
6XEMHFWWR͵ݔଵ൅ʹݔଶ൒ͳʹǡ ʹݔଵ൅͵ݔଶ൑ͳ͸ǡ ݔଵ൒Ͳǡ ݔଶ൒Ͳ
Application of the optimality conditions results in the following equations:
ݔଵሺʹݒଵ൅ʹݒଶെʹሻ൅ݔଶሺݒଵ൅͵ݒଶെ͵ሻൌͲ
ݒଵሺʹݔଵ൅ݔଶെͳʹሻ൅ݒଶሺʹݔଵ൅͵ݔଶെͳ͸ሻൌͲ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
104 
Linear Programming Methods
We split these into four equations and use Matlab symbolic toolbox to solve them, which gives the 
following candidate solutions:
ሼݔଵǡ ݔଶǡ ݒଵǡ ݒଶሽൌሺͲǡͲǡͲǡͲሻǡ ሺ͸ǡͲǡͳǡͲሻǡ ሺͺǡͲǡͲǡͳሻǡ ሺͷǡʹǡͲǡͳሻǡ ሺͲǡͳʹǡ͵ǡͲሻǡ ሺͲǡͷǤ͵͵ǡͲǡͳሻǡ ቀͺ െ
ଷ௭
ଶǡ ݖǡ Ͳǡͳቁ
Then, it can be verified that the optimality conditions hold only in the case of: ሼݔଵǡ ݔଶǡ ݒଵǡ ݒଶሽൌሺͷǡʹǡͲͳሻǤ 
The optimum value of the objective function is: z* = 17.
5.8	
The Quadratic Programming Problem
Theory developed for the LP problems easily extends to quadratic programming (QP) problems. The 
QP problem arises frequently in convex programming when the energy associated with a problem is to 
be minimized. An example of that is the finite element analysis (FEA) problem in structures.
The QP problem involves minimization of a quadratic cost function subject to linear constraints, and 
is described as:
min ݍሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞ 
(5.22)
Subject to: ࡭࢞൒࢈ǡ ࢞൒૙
where Q is symmetric positive semidefinite. We first note that the feasible region for the QP problem 
is convex; further, for the given condition on ࡽǡ ݍሺ࢞ሻ is convex. Therefore, QP problem is a convex 
optimization problem, and the KKT conditions are both necessary and sufficient for a global solution. 
5.8.1	
 Optimality Conditions for QP Problems
To derive KKT conditions for the QP problem, we consider a Lagrangian function that includes Lagrange 
multipliers u, v for the non-negativity and inequality constraints. The Lagrangian function and its 
gradient are given as:
ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌ
ଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞െ்࢛࢞െ்࢜ሺ࡭࢞െ࢈െ࢙ሻ
(5.23)
׏ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌࡽ࢞൅ࢉെ࢛െ࡭்࢜
where s is a vector of slack variables. The resulting KKT conditions for the QP problem are given as:
Feasibility: ࡭࢞െ࢙ൌ࢈
Optimality: ࡽ࢞൅ࢉെ࢛െ࡭்࢜ൌ૙
Complementarity: ்࢛࢞൅்࢙࢜ൌ૙
Non-negativity: ࢞൒૙ǡ ࢙൒૙ǡ ࢛൒૙ǡ ࢜൒૙
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
105 
Linear Programming Methods
By eliminating variables s, u we can concisely express the KKT conditions as:
்࢞ሺࡽ࢞൅ࢉെ࡭்࢜ሻൌ૙ǡ
்࢜ሺ࡭࢞െ࢈ሻൌ૙ǡ࢞൒૙ǡ
࢜൒૙
(5.24)
Alternatively, we combine the optimality and feasibility conditions in matrix form as:
൤ࡽ
െ࡭்
࡭
૙൨ቂ࢞
࢜ቃ൅ቂࢉ
െ࢈ቃെቂ࢛
࢙ቃൌቂ૙
૙ቃ
Next, let: ࡹൌ൤ࡽ
െ࡭்
࡭
૙൨ǡ ࢠൌቂ࢞
࢜ቃǡ ࢝ൌቂ࢛
࢙ቃǡ ࢗൌቂࢉ
െ࢈ቃǢ then, the problem is transformed as: 
ࡹࢠ൅ࢗൌ࢝ǡ where the complementarity conditions are: ்࢝ࢠൌ૙ The resulting problem is known in 
linear algebra as the Linear Complementarity Problem (LCP) and is solved in Sec. 5.8 below. 
The above QP problem may additionally include linear equality constraints of the form: ࡯࢞ൌࢊǡ in 
which case, the problem is defined as:
min ݍሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞
(5.26)
subject to ࡭࢞൒࢈ǡ ࡯࢞ൌࢊǡ ࢞൒૙
We similarly add slack variables to the inequality constraint, and formulate the Lagrangian function as:
ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞െ்࢜ሺ࡭࢞െ࢈െ࢙ሻെ்࢛࢞൅்࢝ሺ࡯࢞െࢊሻ
The modified KKT conditions are given as: 
Feasibility: ࡭࢞െ࢈െ࢙ൌ૙ǡ ࡯࢞ൌࢊ
Optimality: ࡽ࢞൅ࢉെ࢛െ࡭்࢜൅࡯்࢝ൌ૙
Complementarity: ்࢛࢞൅்࢙࢜ൌ૙
Non-negativity: ࢞൒૙ǡ ࢙൒૙ǡ ࢛൒૙ǡ ࢜൒૙
where the Lagrange multipliers w for the equality constraints are not restricted in sign. By introducing: 
࢝ൌܡെܢǢ ܡǡ ܢ൒૙ we can represent the combined optimality and feasibility conditions as:
൥
ࡽ
െ࡭்
࡭
࡯
૙
૙
൩ቂ࢞
࢜ቃെ൥
ࡵ
૙
૙
ࡵ
૙
૙
൩ቂ࢛
࢙ቃ൅൥
࡯்
െ࡯்
૙
૙
૙
૙
൩ቂ࢟
ࢠቃ൅ቈ
ࢉ
െ࢈
െࢊ
቉ൌቈ
૙
૙
૙
቉ 
(5.28)
The above problem can be similarly solved via LCP framework, which is introduced in Sec. 5.8. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
106 
Linear Programming Methods
5.8.2	
 The Dual QP Problem
We reconsider the QP problem (5.22) and observe that the Lagrangian function (5.23) is stationary at 
the optimum with respect to x, u, v. Then, as per Lagrangian duality (Sec. 4.5), it can be used to define 
the following dual QP problem (called Wolfe’s dual):

࢞ǡ࢛ǡࣦ࢜ሺ࢞ǡ ࢛ǡ ࢜ሻൌଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞െ்࢛࢞൅்࢜ሺ࡭࢞െ࢈ሻ
Subject to: ׏ࣦሺ࢞ǡ ࢛ǡ ࢜ሻൌࡽ࢞൅ࢉെ࢛൅࡭்࢜ൌ૙ǡ ࢞൒૙ǡ ࢜൒૙
(5.29)
Further, by relaxing the non-negativity condition on the design variable x, we can eliminate u from the 
formulation, which results in a simpler dual problem defined as:

࢞ǡ࢜ஹ૙ࣦሺ࢞ǡ ࢜ሻൌଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞൅்࢜ሺ࡭࢞െ࢈ሻ
Subject to: ࡽ࢞൅ࢉ൅࡭்࢜ൌ૙
(5.30)
The implicit function theorem allows us to express the solution vector x in the vicinity of the optimum 
point as a function of the Lagrange multipliers ࢜DV࢞ൌ࢞ሺ࢜ሻ Next, the Lagrangian is expressed as 
an implicit function Ȱሺ࢜ሻ of the multipliers, termed as the dual function. Further, the dual function is 
obtained as a solution to the following minimization problem: 
Ȱሺ࢜ሻൌ
ࣦ࢞ሺ࢞ǡ ࢜ሻൌଵ
ଶ்࢞ࡽ࢞൅ࢉ்࢞൅்࢜ሺ࡭࢞െ࢈ሻ
(5.31)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENT…
     RUN FASTER.
          RUN LONGER..
                RUN EASIER…
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Fundamental Engineering Optimization 
Methods
107 
Linear Programming Methods
The solution is obtained by solving the FONC, the constraint in (5.30), for x as: 
࢞ሺ࢜ሻൌെࡽିଵሺ࡭்࢜൅ࢉሻ
(5.32)
and substituting it in the Lagrangian function to obtain:
Ȱሺ࢜ሻൌെଵ
ଶሺ࡭்࢜൅ࢉሻ்ࡽିଵሺ࡭்࢜൅ࢉሻെ்࢜࢈
ൌെభ
మ்࢜ሺ࡭ࡽିଵ࡭்ሻ࢜െሺࢉ்ࡽିଵ࡭்൅࢈்ሻ࢜െభ
మࢉ்ࡽିଵࢉ
(5.33)
In terms of the dual function, the dual QP problem is defined as: 

࢜ஹ૙Ȱሺ࢜ሻൌെଵ
ଶሺ࡭்࢜൅ࢉሻ்ࡽିଵሺ࡭்࢜൅ࢉሻെ்࢜࢈
(5.34)
The dual problem can also be solved by application of FONC, where the gradient and Hessian of Ȱሺ࢜ሻ
are given as:
׏Ȱ ൌെ࡭ࡽିଵሺ࡭்࢜൅ࢉሻെ࢈ǡ
׏ଶȰ ൌെ࡭ࡽିଵ࡭்
(5.35)
By solving ׏࢜Ȱ ൌͲǡ we obtain the solution to the Lagrange multipliers as:
࢜ൌെሺ࡭ࡽିଵ࡭்ሻିଵሺ࡭்ࡽିଵࢉ൅࢈ሻ
(5.36)
where the non-negativity of v is implied. Finally, the solution to the design variables is obtained from 
(5.32) as:
࢞ൌࡽିଵ࡭்ሺ࡭ࡽିଵ࡭்ሻିଵሺ࡭்ࡽିଵࢉ൅࢈ሻെࡽିଵࢉ
(5.37)
The dual methods have been successfully applied in structural mechanics. As an example of the dual QP 
problem, we consider a one-dimensional finite element analysis (FEA) problem involving two nodes.
Example 5.10: Finite Element Analysis (Belegundu and Chandrupatla, p. 187)
Let ݍଵǡ ݍଶrepresent nodal displacements in the simplified two node structure, and assume that a load P, 
where ܲൌ͸Ͳ݇ܰǡ is applied at node 1. The FEA problem is formulated as minimization of the potential 
energy function given as:

ࢗς ൌͳ
ʹ ்ࢗࡷࢗെ்ࢗࢌ
Subject to: ݍଶ൑ͳǤʹ
In the above problem, ்ࢗൌሾݍଵǡ ݍଶሿ represents the vector of nodal displacements. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
108 
Linear Programming Methods
The stiffness matrix K for the problem is given as: ࡷൌ
ଵ଴ఱ
ଷቂʹ
െͳ
െͳ
ͳ ቃ
ே
௠Ǥ
For this problem: ࡽൌࡷǡ ࢌൌሾܲǡ Ͳሿ்ǡ ࢉൌെࢌǡ࡭ൌሾͲ
ͳሿǡ ࢈ൌͳǤʹǤ
Further, ࡭ࡽିଵ࡭்ൌ͸ ൈͳͲିହǡ ࢉ்ࡽିଵ࡭்ൌെͳǤͺǡ ࢉ்ࡽିଵࢉൌͳǤͲͺ ൈͳͲିହǤ
We use (5.33) to obtain the dual function as: Ȱሺ࢜ሻൌെ͵ ൈͳͲିହݒଶെͲǤ͸ݒെͳǤͲͺ ൈͳͲିହ 
From (5.36) the solution to Lagrange multiplier is: ݒൌͳ ൈͳͲସ 
Then, from (5.37), the optimum solution to the design variables is: ݍଵൌͳǤͷ݉݉ǡ ݍଶൌͳǤʹ݉݉Ǥ
The optimum value of potential energy function is: ς ൌͳʹͻܰ݉Ǥ
Next, we proceed to define and solve the Linear Complementarity Problem.
5.9	
The Linear Complementary Problem
The application of optimality conditions to LP and QP problems leads to the Linear Complementary 
Problem (LCP), which can be solved using Simplex based methods. The LCP aims at finding vectors 
that satisfy linear equality, non-negativity, and complementarity conditions. When used in the context 
of optimization, the LCP simultaneously solves both primal and dual problems.
The general LCP problem is defined as follows: Given a real symmetric positive definite matrix M and 
a vector, q, find a vector ࢠ൒૙ such that: ࢝ൌࡹࢠ൅ࢗ൒૙ǡ ்࢝ࢠൌ૙ 
In the case of QP problem, we define: ࡹൌ൤ࡽ
െ࡭்
࡭
૙൨ǡ ࢠൌቂ࢞
࢜ቃǡ ࢝ൌቂ࢛
࢙ቃǡ ࢗൌቂࢉ
െ࢈ቃ to cast the 
problem into the LCP framework. Further, if Q is positive semidefinite, so is M, resulting in a convex 
LCP, which can be solved by Simplex methods, in particular, the Lemke’s algorithm. 
Toward finding a solution to the LCP, we observe that if all ݍ௜൒Ͳǡ then z = 0 Qࢠൌ૙solves the LCP. It 
is, therefore, assumed that one or more ݍ௜൏Ͳ Lemke’s algorithm introduces an artificial variable, z0, 
where ݖ଴ൌȁሺݍ௜ሻȁ to cast LCP into Phase I Simplex framework. The resulting problem is given as:
 ݖ଴
Subject to: ࡵ࢝െࡹࢠെࢋݖ଴ൌࢗǡ ்࢝ࢠൌ૙ǡ ࢝൒૙ǡ ࢠ൒૙
(5.38)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
109 
Linear Programming Methods
where ࢋൌሾͳͳ ڮ ͳሿ் and I is an identity matrix. The linear constraint is used to define the starting 
tableau for the Simplex method, where an initial BFS is given as: ࢝ൌࢗ൅ࢋݖ଴൒Ͳǡ ࢠൌͲ The 
algorithm starts with a pivot operation aimed to bring z0 into the basis. Thereafter, the EBV is selected as 
complement of the LBV in the previous iteration. Thus, if ݓ௥ leaves the basis, ݖ௥ enters the basis in the 
next tableau, or vice versa, which maintains the complementarity condition ݓ௥ݖ௥ൌͲǤ The algorithm 
terminates when z0 has become nonbasic.
Lemke’s Algorithm for solving LCP (Belegundu and Chandrupatla, p. 178): 
1.	 If all qi > 0, then LCP solution is: ݖ଴ൌͲǡ ࢝ൌࢗǡ ࢠൌ૙ No further steps are necessary. 
2.	 If some ݍ௜൏Ͳǡ select ݖ଴ൌȁሺݍ௜ሻȁ to construct the initial tableau. 
3.	 Choose the most negative ݍ௜ row and the z0 column to define the pivot element. In the first 
step z0 enters the basis, ݓ௜ corresponding to most negative ݍ௜ exits the basis. Henceforth, all 
qi ≥ 0.
4.	 If basic variable in column i last exited the basis, its complement in column j enters the 
basis. (At first iteration, ݓ௜ exits and ݖ௜ enters the basis). Perform the ratio test for column j 
to find the least among ݍ௜ /(positive row element i). The basic variable corresponding to row 
i now exits the basis. If there are no positive row elements, there is no solution to the LCP
5.	 If the last operation results in the exit of the basic variable z0, then the cycle is complete, 
stop. Otherwise go to step 3.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Fundamental Engineering Optimization 
Methods
110 
Linear Programming Methods
Two examples of Lemke’s algorithm are presented below:
Example 5.11: Lemke’s algorithm
We consider the following QP problem:

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌݔଵ
ଶ൅ݔଶ
ଶെݔଵݔଶെݔଵ൅ʹݔଶ
6XEMHFWWRݔଵ൅ݔଶ൑ͷǡ ݔଵ൅ʹݔଶ൑ͳͲǢ ݔଵǡ ݔଶ൒Ͳ
For the given problem: ࡽൌቂʹ
െͳ
െͳ
ʹቃǡ ࢉ்ൌሾെͳ
ʹሿǡ ࡭ൌቂͳ
ͳ
ͳ
ʹቃǡ ࢈ൌቂͷ
ͳͲቃǡ ݖ଴ൌെͳǤ
The resulting initial tableau for the problem is given as:

%DVLF ࢝૚ ࢝૛ ࢝૜ ࢝૝ ࢠ૚ ࢠ૛ ࢠ૜ ࢠ૝ ࢠ૙
ࢗ
࢝૚










࢝૛










࢝૜










࢝૝










SLYRW
We begin by a pivot step aimed at bringing ݖ଴ into the basis as represented by the following tableau:
%DVLF ࢝૚ ࢝૛ ࢝૜ ࢝૝ ࢠ૚ ࢠ૛ ࢠ૜ ࢠ૝ ࢠ૙ ࢗ
ࢠ૙










࢝૛










࢝૜










࢝૝










3LYRW
This is followed by further simplex iterations that maintain the complementarity conditions. The 
algorithm terminates when exits the basis. The resulting series of tableaus is given below:

%DVLF
࢝૚
࢝૛ ࢝૜ ࢝૝ ࢠ૚
ࢠ૛
ࢠ૜
ࢠ૝
ࢠ૙
ࢗ
ࢠ૚










࢝૛





    
࢝૜






   
࢝૝






   
The algorithm terminates after two steps as ݖ଴ has exited the basis. The basic variables are given as: 
ݖଵݓଶݓଷݓସ so that the complementarity conditions are satisfied, and the optimum solution is given 
as: ݔଵൌͲǤͷǡ ݔଶൌͲǡ ݂כ ൌെͲǤʹͷǤ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
111 
Linear Programming Methods
As the second LCP example, we reconsider the one-dimensional finite element analysis (FEA) problem 
that was solved earlier (Example 5.8).
Example 5.12: Finite Element Analysis (Belegundu and Chandrupatla, p. 187)
The problem is stated as:

ࢗς ൌͳ
ʹ ்ࢗࡷࢗെ்ࢗࢌ
Subject to: ݍଶ൑ͳǤʹ
In the above problem, ்ࢗൌሾݍଵǡ ݍଶሿ represents a vector of nodal displacements. A load ܲǡܲൌ͸Ͳ݇ܰǡ 
is applied at node 1, so that ࢌൌሾܲǡ Ͳሿ்ǤThe stiffness matrix K is given as: ࡷൌ
ଵ଴ఱ
ଷቂʹ
െͳ
െͳ
ͳ ቃ
ே
௠Ǥ
For this problem: ࡽൌࡷǡ ࢉൌെࢌǡ࡭ൌሾͲ
ͳሿǡ ࢈ൌͳǤʹǡ ݖ଴ൌെͳǤ
The initial and the subsequent tableaus leading to the solution of the problem are given below:

%DVLF ࢝૚ ࢝૛ ࢝૜
ࢠ૚
ࢠ૛
ࢠ૜ ࢠ૙
ࢗ
࢝૚








࢝૛








࢝૜








3LYRW

%DVLF ࢝૚ ࢝૛ ࢝૜
ࢠ૚
ࢠ૛
ࢠ૜ ࢠ૙
ࢗ
ࢠ૙








࢝૛



 



࢝૜








3LYRW

%DVLF
࢝૚
࢝૛
࢝૜ ࢠ૚
ࢠ૛
ࢠ૜
ࢠ૙
ࢗ
ࢠ૙








ࢠ૚
 






࢝૜








3LYRW
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
112 
Linear Programming Methods

%DVLF
࢝૚
࢝૛
࢝૜
ࢠ૚ ࢠ૛
ࢠ૜
ࢠ૙
ࢗ
ࢠ૙
  





ࢠ૚
 






ࢠ૛
 






3LYRW

%DVLF
࢝૚
࢝૛
࢝૜
ࢠ૚ ࢠ૛
ࢠ૜
ࢠ૙
ࢗ
ࢠ૜






 
ࢠ૚








ࢠ૛








The algorithm terminates when ݖ଴ has exited the basis. The final solution to the problem is given as:
ݖଵൌͳǤͷ݉݉ǡ ݖଶൌͳǤʹ݉݉ǡ ς ൌͳʹͻܰ݉Ǥ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Fundamental Engineering Optimization 
Methods
113 
Discrete Optimization
6	 Discrete Optimization 
This chapter is devoted to the study of solution approaches to discrete optimization problems that 
involve decision making, when the variables must be chosen from a discrete set. Many real world design 
problems fall in this category. For example, variables in optimization problems arising in production 
or transportation of goods represent discrete quantities and can only take on integer values. Further, 
scheduling and networking problems (e.g., assigning vehicles to transportation networks, frequency 
assignment in cellular phone networks, etc.) are often modeled with variables that can only take on 
binary values. The integer programming problem and binary integer programming problem are special 
cases of optimization problems where solution choices are limited to discrete sets.
Discrete optimization is closely related to combinatorial optimization that aims to search for the 
best object from a set of discrete objects. Classical combinatorial optimization problems include the 
econometric problems (knapsack problem, capital budgeting problem), scheduling problems (facility 
location problem, fleet assignment problem) and network and graph theoretic problems (traveling 
salesman problem, minimum spanning tree problem, vertex/edge coloring problem, etc.). Combinatorial 
optimization problems are NP-complete, meaning they are non-deterministic polynomial time problems, 
and finding a solution is not guaranteed in finite time. Heuristic search algorithms are, therefore, 
commonly employed to solve combinatorial optimization problems. Considerable research has also been 
devoted to finding computation methods that utilize polyhedral structure of integer programs. 
Learning Objectives. The learning aims in this chapter are:
1.	 Study the structure and formulation of a discrete optimization problem.
2.	 Learn common solution approaches to the discrete optimization problems.
3.	 Learn to use the branch-and-bound and cutting plane methods to solve the mixed integer 
programming problem.
6.1	
Discrete Optimization Problems
A discrete optimization problem may be formulated in one of the following ways:
1.	 An integer programming (IP) problem is formulated as: 

࢞
ݖൌࢉ்࢞
VXEMHFWWR࡭࢞൑࢈ǡ ࢞א Ժ௡ǡ ࢞൒૙ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
114 
Discrete Optimization
2.	 A binary integer programming (BIP) problem is formulated as:

࢞
ݖൌࢉ்࢞
VXEMHFWWR࡭࢞൑࢈ǡ ࢞א ሼͲǡͳሽ௡
 
3.	 A combinatorial optimization (CO) problem is formulated as:

࢞
ݖൌࢉ்࢞
VXEMHFWWR࡭࢞൑࢈ǡ ݔ௜א ሼͲǡͳሽሺ݅א ܤሻǡ ݔ௜א Ժሺ݅א ܫሻ 
4.	 A Mixed integer programming (MIP) problem is formulated as:

࢞
ݖൌࢉ்࢞
VXEMHFWWR࡭࢞൑࢈ǡ ݔ௜൒Ͳǡ ݔ௜א Ժǡ ݅ൌͳǡ ǥ ǡ ݊ௗǢݔ௜௅൑ݔ௜൑ݔ௜௎ǡ ݅ൌ݊ௗ൅ͳǡ ǥ ǡ ݊  
5.	 A general mixed variable design optimization problem is formulated as:

࢞݂ሺ࢞ሻ
VXEMHFWWR݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈Ǣ݃௝ሺݔሻ൑Ͳǡ ݆ൌ݅ǡ ǥ ǡ ݉Ǣݔ௜א ܦǡ ݅ൌͳǡ ǥ ǡ ݊ௗǢݔ௜௅൑ݔ௜൑ 
ݔ௜௎ǡ ݅ൌ݊ௗ൅ͳǡ ǥ ǡ ݊ 
In the following, we discuss solution approaches to linear discrete optimization problems (1–4 above). 
6.2	
Solution Approaches to Discrete Problems
We first note that the discrete optimization problems may be solved by enumeration, i.e., an ordered 
listing of all solutions. The number of combinations to be evaluated to solve the problem is given as:
ܰ௖ൌς
ݍ௜
௡೏
௜ୀଵ
 where ݊ௗ is the number of design variables and ݍ௜ represents the number of discrete 
values for the design variable ݔ௜ This approach is, however, not practically feasible as the ܰ௖ increases 
rapidly with increase in ݊ௗ and ݍ௜
Further, two common approaches to solve linear discrete optimization problems are: 
1.	 The branch and bound (BB) technique that divides the problem into a series of 
subprograms, where any solution to the original problem is contained in exactly one of the 
subprograms. 
2.	 The cutting plane method that iteratively refines the solution by adding additional linear 
inequality constraints (cuts) aimed at excluding non-integer solutions to the problem. 
These two approaches are discussed below. Besides, other approaches for solving discrete optimization 
problems include heuristic methods, such as tabu (neighborhood) search, hill climbing, simulated 
annealing, genetic algorithms, evolutionary programming, and particle swarm optimization. These topics 
are, however, not discussed here. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
115 
Discrete Optimization
In the following, we begin with the methods to solve an LP problem involving integral coefficients, 
followed by the BIP problems, and finally the IP/MIP problems. 
6.3	
Linear Programming Problems with Integral Coefficients
In this section, we consider an LP problem modeled with integral coefficients described as:

࢞ݖൌࢉ்࢞
6XEMHFWWR࡭࢞ൌ࢈ǡ ࢞൒૙ǡ ࡭א Ժ௠ൈ௡ǡ ࢈א Ժ௠ǡ ࢉא Ժ௡
(6.1)
We further assume that A is totally unimodular, i.e., every square submatrix C of A, has det ሺ࡯ሻא ሼͲǡ േͳሽǤ
In that case, every vertex of the feasible region, or equivalently, every BFS of the LP problem is integral. In 
particular, the optimal solution returned by the Simplex algorithm is integral. Thus, total unimodularity 
of A is a sufficient condition for integral solution to LP problems.
To show that an arbitrary BFS, x, to the problem is integral, let ࢞஻ represent the elements of x 
corresponding to the basis columns, then there is a square nonsingular submatrix B of A, such that ࡮࢞஻ൌ
b. Further, by unimodularity assumption, detሺ࡮ሻൌേͳǡDQG࡮ିଵൌേܣ݆݀࡮ǡZ
ଵ
 where Adj represents the 
adjoint matrix and is integral. Therefore, ࢞஻ൌ࡮ିଵ࢈ is integral.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360°
thinking.
© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Fundamental Engineering Optimization 
Methods
116 
Discrete Optimization
Further, if A is totally unimodular, so is ሾ࡭ࡵሿ This applies to problems involving inequality constraints:
࡭࢞൑࢈ǡ which, when converted to equality via addition of slack variable ࢙ are represented as: ࡭࢞൅ࡵ࢙ൌ
࢈ǡor ሾ࡭ࡵሿቂ࢞
࢙ቃൌ࢈7KHQLI࡭א Ժ௠ൈ௡ is totally unimodular and ࢈א Ժ௠ all BFSs to the problem have 
integral components.
We, however, note that total unimodularity of A is a sufficient but not necessary condition for an integral 
solution; integral BFS may still be obtained if A is not totally unimodular. An example would be a matrix 
with isolated individual elements that do not belong to the set: ሼെͳǡͲǡͳሽ. Indeed, a necessary condition 
for integral solutions to LP problem is that each ݉ൈ݉ basis submatrix B of A has determinant equal 
to േ.
An example of an LP problem with integral coefficients is considered below.
Example 6.1: Integer BFS
We consider the following LP problem with integer coefficients:

࢞
ݖൌʹݔଵ൅͵ݔଶ
6XEMHFWWRݔଵ൑͵ǡ ݔଶ൑ͷǡ ݔଵ൅ݔଶ൑͹ǡ࢞א Ժହǡ ࢞൒૙
Following the introduction of slack variables, the constraint matrix and the right hand side are given as: 
࡭ൌ൥
ͳ
Ͳ
ͳ

Ͳ
ͳ
ͳ

ͳ
Ͳ
Ͳ

Ͳ
ͳ
Ͳ

Ͳ
Ͳ
ͳ
൩࢈ൌ൥
͵
ͷ
͹
൩א ԺଷǡZ where we note that A is unimodular and ࢈א Ժଷ Then, using the 
simplex method, the optimal integral solution is obtained as: ்࢞ൌሾʹǡͷǡͳǡͲǡͲሿǡZLWKݖכ ൌͳͻ
6.4	
Binary Integer Programming Problems
In this section, we discuss solution of the BIP problem defined as:

࢞ݖൌࢉ்࢞
6XEMHFWWR࡭࢞൒࢈ǡ ݔ௜א ሼͲǡͳሽǡ ݅ൌͳǡ ǥ ǡ ݊
(6.2)
where we additionally assume that ࢉ൒૙ We note that this is not a restrictive assumption, as any variable 
ݔ௜ with negative ܿ௜ in the objective function can be replaced by: ݔ௜
ᇱൌͳ െݔ௜ 
Further, we note that under not-too-restrictive assumptions most LP problems can be reformulated in 
the BIP framework. For example, if the number of variables is small, and the bounds ݔ௠௜௡൏ݔ௜൏ݔ௠௔௫
on the design variables are known, then each ݔ௜ can be represented as a binary number using ݇ bits, 
where ʹ௞ାଵ൒ݔ௠௔௫െݔ௠௜௡ The resulting problem involves selection of the bits and is a BIP problem.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
117 
Discrete Optimization
The BIP problem can be solved by implicit enumeration. In implicit enumeration, obviously infeasible 
solutions are eliminated and the remaining ones are evaluated (i.e., enumerated) to find the optimum. The 
search starts from ݔ௜ൌͲǡ ݅ൌͳǡ ǥ ǡ ݊ǡ which is optimal. If this is not feasible, then we systematically 
adjust individual variable values till feasibility is attained. The implicit enumeration procedure is coded 
in the following algorithm that does not require an LP solver:
Binary Integer Programming Algorithm (Belegundu and Chandrupatla, p. 364): 
1.	 Initialize: set Wݔ௜ൌͲǡ ݅ൌͳǡ ǥ ǡ ݊ if this solution is feasible, we are done.
2.	 For some i set ݔ௜ൌͳ If the resulting solution is feasible, then record it if this is the first 
feasible solution, or if it improves upon a previously recorded feasible solution.
3.	 Backtrack VHWݔ௜ൌͲሻ if a feasible solution was reached in the previous step, or if feasibility 
appears impossible in this branch.
4.	 Choose another i and return to 2.
The progress of the algorithm is graphically recorded in a decision-tree, using nodes and arcs with node 
0 representing the initial solution ݔ௜ൌͲǡ ݅ൌͳǡ ǥ ǡ ݊ and node i representing a change in the value 
of variable ݔ௜. From node k, if we choose to raise variable ݔ௜ to one, then this is represented as an arc 
from node k to node i. At node i the following possibilities exist:
1.	 The resulting solution is feasible, meaning no further improvement in this branch is 
possible.
2.	 Feasibility is impossible from this branch.
3.	 The resulting solution is not feasible, but feasibility or further improvement are possible.
In the first two cases, the branch is said to have been fathomed. We then backtrack to node k, where 
variable ݔ௜ is returned to zero. We next seek another variable to be raised to one. The algorithm continues 
till all branches have been fathomed, and returns an optimum 0-1 solution.
We consider the following example of a BIP problem.
Example 6.2: Implicit enumeration (Belegundu and Chandrupatla, p. 367)
A machine shaft is to be cut at two locations to given dimensions 1, 2, using one of the two available 
processes, A and B. The following information on process cost and three-sigma standard deviation is 
available, where the combined maximum allowable tolerance is to be limited to 12mils:
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
118 
Discrete Optimization
3URFHVV?-RE
-RE
-RE
&RVW
6'
&RVW
6'
3URFHVV$

PLOV

PLOV
3URFHVV%

PLOV

PLOV
Let ݔ௜ǡ ݅ൌͳ െͶ denote the available processes for both jobs, and let ݐ௜ denote their associated 
tolerances. The BIP problem is formulated as:

࢞ݖൌ͸ͷݔଵ൅ͷ͹ݔଶ൅Ͷʹݔଷ൅ʹͲݔସ
6XEMHFWWRݔଵ൅ݔଶൌͳǡ ݔଷ൅ݔସൌͳǡ σ ݐ௜
௜
൑ͳʹǡ ݔ௜א ሼͲǡͳሽǡ ݅ൌͳǡ ǥ ǡͶǤ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Fundamental Engineering Optimization 
Methods
119 
Discrete Optimization
The problem is solved via implicit enumeration; the resulting decision-tree is represented below:
ͳ
Ͷ
͵
݂ൌͳͲ͹
1),
Ͳ
1),
ݔଷൌͳ
ݔସൌͳ
ʹ
Ͷ
͵
݂ൌͻͻ
2SWLPXP
ݔଷൌͳ
ݔସൌͳ
1),
ݔଵൌʹ
ݔଵൌͳ
Fig 6.1: The decision tree for Example 6.2 (NFI: No further improvement
6.5	
Integer Programming Problems
This section discusses the solution approaches to the IP problem formulated as:

࢞
ݖൌࢉ்࢞
6XEMHFWWR࡭࢞൑࢈ǡ ࢞א Ժ௡ǡ ࢞൒૙

(6.3)
To start with, the optimization problem that results when integrality constraint in the above problem 
is ignored is termed as LP relaxation of the IP problem. While a naïve solution to the IP problem may 
be to round off the non-integer LP relaxation solution, in general, this approach does not guarantee a 
satisfactory solution to IP problem. 
In the following, we discuss two popular methods for solving IP problems: the branch and bound 
method and the cutting plane method. Both methods begin by first solving the LP relaxation problem 
and subsequently using the LP solution to subsequently bind the IP solution. 
6.5.1	
 The Branch and Bound Method
The BB method is the most widely used method for solving IP problems. The method has its origins in 
computer science, where search over a large finite space is performed by using bounds on the objective 
function to prune the search tree. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
120 
Discrete Optimization
The BB method iteratively solves an IP problem as follows: it first obtains the LP relaxation solution; 
next, it introduces integrality constraints to define subprograms that effectively divide the feasible region 
into smaller subsets (branching); it then calculates objective function bounds for each subprogram 
(bounding); finally, it uses those bounds to discard non-promising solutions from further consideration 
(fathoming). The procedure ends when every branch has been fathomed and an optimum integer solution, 
if one exists, has been found. 
A decision tree is normally used to record the progress of the BB algorithm, where the LP relaxation 
solution is represented as node 0. Subsequently, at each node k, the algorithm sequences through the 
following phases:
1.	 Selection. If some variables in the simplex solution at node k have non-integer values, the 
algorithm selects the one with the lowest index (or the one with greatest economic impact) 
for branching.
2.	 Branching. The solution at node k is partitioned into two mutually exclusive subsets, each 
represented by a node in the decision tree and connected to node k by an arc. It involves 
imposition of two integer constraints ݔ௜൑ܫǡ ݔ௜൒ܫ൅ͳǡ ܫൌہݔ௜ۂ generating two new 
subprograms where each solution to the original IP problem is contained in exactly one of 
the subprograms. 
3.	 Bounding. In this phase, upper bounds on the optimal subproblem solutions are 
established. Solving a subprogram via LP solver results in one of the following possibilities: 
a)	 There is no feasible solution.
b)	 The solution does not improve upon an available IP solution.
c)	 An improved IP solution is returned and is recorded as current optimal.
d)	 A non-integer solution that is better than the current optimal is returned.
4.	 Fathoming. In the first three cases above the current branch is excluded from further 
consideration. The algorithm then backtracks to the most recently unbranched node in the 
tree and continues with examining the next node in a last in first out (LIFO) search strategy. 
Finally, the process ends when all branches have been fathomed, and an integer optimal solution to the 
problem, if one exists, has been found. 
Let NF denote the set of nodes not yet fathomed, F denote the feasible region for the original IP problem, 
ܨோ denote the feasible region for the LP relaxation, ܨ௞ denote the feasible region at node ݇ܵ௞ denote 
the subproblem defined as: 
࢞
ݖ௞ൌࢉ்࢞ǡ ࢞א ܨ௞ and let ݖ௅ denote the lower bound on the optimal 
solution. Then, the BB algorithm is given as follows:
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
121 
Discrete Optimization
Branch-and-bound Algorithm (Sierksma, p. 219): 
Initialize: set ܨ଴ൌܨோǡ ܰܨൌሼͲሽǡ ݖ௅ൌെλ.
While ܰܨ്׎ǡ
1.	 Select a label ݇א ܰܨ. 
2.	 Determine if there exists an optimal solution ሺݖ௞ǡ ࢞௞ሻWRܵ௞ǡHOVHVHWݖ௞ൌെλ
3.	 If ݖ௞൐ݖ௅ǡWKHQLI࢞௞א ܨǡVHWݖ௅ൌݖ௞
4.	 If ݖ௞൑ݖ௅ǡVHWܰܨൌܰܨ̳ሼ݇ሽ
5.	 If ݖ௞൐ݖ௅DQG࢞௞ב ܨǡ partition ܨ௞L into two or more subsets as follows: choose a variable 
ݔ௜א࢞௞with fractional value, ݔ௜ൌܫ൅ߜ௜ǡ ܫൌہݔ௜ۂǡ Ͳ ൏ߜ௜൏ͳǤ Define two new 
subprograms: ܨ௞భൌܨ௞ת ሼݔ௜൑ܫሽǡ ܨ௞మൌܨ௞మת ሼݔ௜൒ܫ൅ͳሽ6HWܰܨൌܰܨ׫ ሼ݇ଵǡ ݇ଶሽ
An example is now presented to illustrate the BB algorithm.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Fundamental Engineering Optimization 
Methods
122 
Discrete Optimization
Example 6.3: Branch and bound algorithm
We consider the following IP problem (Belegundu and Chandrupatla, p. 383): A tourist bus company 
having a budget of $10M is considering acquiring a fleet with a mix of three models: a 15-seat van costing 
$35,000, a 30-seat minibus costing $60,000, and a 60-seat bus costing $140,000. A total capacity of 2000 
seats is required. At least one third of the vehicles must be the big buses. If the estimated profits per 
seat per month for the three models are: $4, $3, and $2 respectively, determine the number of vehicles 
of each type to be acquired to maximize profit.
Let ݔଵǡ ݔଶǡ ݔଷ denote the quantities to be purchased for each of the van, minibus, and big bus; then, the 
optimization problem is formulated as:
0D[LPL]Hݖൌ͸Ͳݔଵ൅ͻͲݔଶ൅ͳʹͲݔଷ
6XEMHFWWRͷݔଵ൅͸Ͳݔଶ൅ͳͶͲݔଷ൑ͳͲͲͲǡ ͳͷݔଵ൅͵Ͳݔଶ൅͸Ͳݔଷ൒ʹͲͲͲǡ ݔଵ൅ݔଶെʹݔଷ൑ͲǢ
ݔଵǡ ݔଶǡ ݔଷ൒ͲDQGLQWHJHU
 
Following steps are taken to solve the problem. The progress is also shown in a decision tree in Fig. 6.2:
1.	 ܵ଴ǣ the LP relaxation problem ሺܨ଴ൌܨோሻ is first solved and produces an optimum solution: 
ݔଵ
כ ൌ Ͳǡ ݔଶ
כ ൌ͹Ǥ͸ͻǡ ݔଷ
כ ൌ͵Ǥͺͷǡ ݂כ ൌͳͳͷ͵Ǥͺ which serves as an upper bound for IP 
solution.
2.	 ܵଵǣ ܨ଴׫ ሼݔଷ൑͵ሽ is solved and produces an integer solution: 
ݔଵ
כ ൌͲǡ ݔଶ
כ ൌ͸ǡ ݔଷ
כ ൌ͵ǡ ݂כ ൌͻͲͲ This is recorded as current optimum. 
3.	 ܵଶǣ ܨ଴׫ ሼݔଷ൒Ͷሽ produces a non-integer solution: 
ݔଵ
כ ൌͳǤ͸ǡ ݔଶ
כ ൌ͸ǤͶǡ ݔଷ
כ ൌͶǡ ݂כ ൌͳͳͷʹ. 
4.	 ܵଷǣ ܨଶ׫ ሼݔଶ൑͸ሽ produces a non-integer solution: 
ݔଵ
כ ൌʹǤͳǡ ݔଶ
כ ൌ͸ǡ ݔଷ
כ ൌͶǤͲͷǡ ݂כ ൌͳͳͷͳǤͶ
5.	 ܵସǣ ܨଷ׫ ሼݔଷ൑Ͷሽ produces an integer solution: ݔଵ
כ ൌʹǡ ݔଶ
כ ൌ͸ǡ ݔଷ
כ ൌͶǡ ݂כ ൌͳͳͶͲ This 
is recorded as the new optimum and the branch is fathomed.
6.	 ܵହǣ ܨଷ׫ ሼݔଷ൒ͷሽ produces a non-integer solution: 
ݔଵ
כ ൌͺǤͷ͹ǡ ݔଶ
כ ൌͲǡ ݔଷ
כ ൌͷǡ ݂כ ൌͳͳͳͶǤ͵ which is lower than the current optimum, so the 
branch is fathomed. 
7.	 ܵ଺ǣ ܨଶ׫ ሼݔଶ൒͹ሽ produces a non-integer solution: ݔଵ
כ ൌͲǤͷ͹ǡ ݔଶ
כ ൌ͹ǡ ݔଷ
כ ൌͶǡ ݂כ ൌͳͳͶͶǤ͵ 
8.	 ܵ଻ǣ ܨ଺׫ ሼݔଵ൑Ͳሽ produces a non-integer solution: ݔଵ
כ ൌͲǡ ݔଶ
כ ൌ͹Ǥ͵͵ǡ ݔଷ
כ ൌͶǡ ݂כ ൌͳͳͶͲ 
which does not improve upon the current optimum. The branch is fathomed.
9.	 ଼ܵǣ ܨ଺׫ ሼݔଵ൒ͳሽ has no feasible solution. The branch is fathomed. 
10.	All branches having been fathomed, the optimal solution is: ݔכ ൌሺʹǡ͸ǡͶሻǡ ݂כ ൌͳͳͶͲ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
123 
Discrete Optimization
ܵ଴ǣ ܨ଴ൌܨோ
࢞כ ൌሺͲǡ ͹Ǥ͸ͻǡ ͵Ǥͺͷሻǡ ݂כ ൌͳͳͷ͵Ǥͺ
ܵଵǣ ܨ଴׫ ሼݔଷ൑͵ሽ
࢞כ ൌሺͲǡ ͸ǡ͵ሻǡ ݂כ ൌͻͲͲ
ܵଶǣ ܨ଴׫ ሼݔଷ൒Ͷሽ
࢞כ ൌሺͳǤ͸ǡ ͸ǤͶǡ Ͷሻǡ ݂כ ൌͳͳͷʹ
ܵଷǣ ܨଶ׫ ሼݔଶ൑͸ሽ
࢞כ ൌሺʹǤͳǡ ͸ǡ ͶǤͲͷሻǡ ݂כ ൌͳͳͷͳǤͶ
ܵ଺ǣ ܨଶ׫ ሼݔଶ൒͹ሽ
࢞כ ൌሺͲǤͷ͹ǡ ͹ǡ Ͷሻǡ ݂כ ൌͳͳͶͶǤ͵
ܵସǣ ܨଷ׫ ሼݔଷ൑Ͷሽ
࢞כ ൌሺʹǡ ͸ǡ Ͷሻǡ ݂כ ൌͳͳͶͲ
ܵହǣ ܨଷ׫ ሼݔଷ൒ͷሽ
࢞כ ൌሺͺǤͷ͹ǡ Ͳǡ ͷሻǡ ݂כ ൌͳͳͳͶǤ͵
ܵ଻ǣ ܨ଺׫ ሼݔଵ൑Ͳሽ
࢞כ ൌሺͲǡ ͹Ǥ͵͵ǡ Ͷሻǡ ݂כ ൌͳͳͶͲ
଼ܵǣ ܨ଺׫ ሼݔଵ൒ͳሽ
ܰܨܵ
 Fig. 6.2: The decision tree for Example 6.3.
6.5.2	
The Cutting Plane Method
Proposed by Gomory in 1958, the cutting plane method or Gomory’s method similarly begins with LP 
relaxation of the IP problem. It then trims the feasible region by successively adding linear constraints 
aimed to prune the non-integer solutions without losing any of the integer solutions. The new constraints 
are referred to as Gomory cuts. The process is repeated till an optimal integer solution has been obtained 
(Belegundu and Chandrupatla, p. 372; Chong and Zak, p. 438). 
To develop the cutting plan method, we assume that the partitioned constraint matrix for the LP relaxation 
problem is given in canonical form as:
ࡵ࢞஻൅࡭ே࢞ேൌ࢈
(6.4)
where ࢞஻ and ࢞ே refer to the basic and nonbasic variables. The corresponding BFS is given as: ࢞஻ൌ࢈ǡ
࢞ேൌ૙Ǥ Next, we consider the ith component of the solution: ݔ௜൅σ
ܽ௜௝ݔ௝
௡
௝ୀ௠ାଵ
ൌܾ௜ǡ and use the 
floor operator to separate it into integer and non-integer parts as:
ݔ௜൅
෍൫උܽ௜௝ඏ൅ߙ௜௝൯ݔ௝
௡
௝ୀ௠ାଵ
ൌہܾ௜ۂ ൅ߚ௜
(6.5)
Then, since උܽ௜௝ඏ൑ܽ௜௝ a feasible solution that satisfies (6.5) also satisfies:
ݔ௜൅
෍උܽ௜௝ඏݔ௝
௡
௝ୀ௠ାଵ
൑ܾ௜
(6.6)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
124 
Discrete Optimization
whereas, an integer feasible solution can be characterized by: 
ݔ௜൅
෍උܽ௜௝ඏݔ௝
௡
௝ୀ௠ାଵ
൑ہܾ௜ۂ
(6.7)
The integer feasible solution also satisfies the difference of the two inequalities, which is given as:
෍ߙ௜௝ݔ௝
௡
௝ୀ௠ାଵ
൒ߚ௜
(6.8)
The above inequality is referred to as the Gomory cut. We note that, since the left-hand-side equals 
zero, the optimal non-integer BFS does not satisfy this inequality. Thus, introduction of the inequality 
constraint (6.8) makes the current LP solution infeasible without losing any IP solutions. 
The constraint introduced by Gomory cut is first brought into standard form by subtracting a surplus 
variable. The resulting problem is solved using simplex method for a new optimal BFS, which is then 
inspected for non-integer components. The process is repeated till an integer BFS has been obtained.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Fundamental Engineering Optimization 
Methods
125 
Discrete Optimization
The cutting plane algorithm generates a family of polyhedra which satisfy: ȳ ـ ȳଵـ ȳଶـ ڮ ـ ȳ ת Ժ௡ǡ
where ȳ ൌሼݔא Թ௡ǣ ࡭࢞൑࢈ሽdenote the polyhedral associated with the LP relaxation problem. The 
cutting plane algorithm terminates in finite steps.
An example of the cutting plane method is presented below.
Example 6.4: Cutting Plane method
We consider the IP problem in Example 6.3 above where the LP relaxation solution was found as: ݔଵ
כ ൌ 
Ͳǡ ݔଶ
כ ൌ͹Ǥ͸ͻǡ ݔଷ
כ ൌ͵Ǥͺͷǡ ݂כ ൌͳͳͷ͵Ǥͺ The final tableau for the LP relaxation solution is given as: 
%DVLF
࢞૚
࢞૛
࢞૜
࢙૚
࢙૛
࢙૜
5KV
࢞૛







࢞૜







ܛ૜







െܢ







The following series of cuts then produces an integer optimum solution:
1R
&XW
2SWLPDOVROXWLRQ
Ć ͲǤͺͲͺݔଵ൅ͲǤͷ͵ͻݏଵ൅ͲǤͲ͵ͻݏଶെݏସൌͲǤ͸ͻʹ
ݔଵ
כ ൌͲǤͺͷ͹ǡ ݔଶ
כ ൌ͹ǡ ݔଷ
כ ൌ͵Ǥͻʹͻǡ ݂כ ൌͳͳͷʹǤͻ
Ć ͲǤͺ͵͵ݏଵ൅ͲǤͲʹͶݏଶ൅ͲǤͺͺͳݏସെݏହൌͲǤͻʹͻ ݔଵ
כ ൌʹǤͳ͸ʹǡ ݔଶ
כ ൌͷǤͻͶ͸ǡ ݔଷ
כ ൌͶǤͲͷͶǡ ݂כ ൌͳͳͷͳǤ͵
Ć ͲǤͲͷͶݏଵ൅ͲǤͻ͹͵ݏଶ൅ͲǤͳ͵ͷݏହെݏ଺ൌͲǤͻͶ͸ ݔଵ
כ ൌʹǤͲͺ͵ǡ ݔଶ
כ ൌͷǤͻ͹ʹǡ ݔଷ
כ ൌͶǤͲʹͺǡ ݂כ ൌͳͳͶͷǤͺ
Ć ͲǤͲͷ͸ݏଵ൅ͲǤͳ͵ͻݏହ൅ͲǤͻ͹ʹݏ଺െݏ଻ൌͲǤͻ͹ʹ
ݔଵ
כ ൌʹǡ ݔଶ
כ ൌ͸ǡ ݔଷ
כ ൌͶǡ ݂כ ൌͳͳͶͲ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
126 
NNumerica l Optimizati on Method
7	 Numerical Optimization 
Methods
This chapter describes the numerical methods used for solving both unconstrained and constrained 
optimization problems. These methods have been used to develop computational algorithms that form 
the basis of commercially available optimization software. The process of computationally solving the 
optimization problem is termed as mathematical programming and includes both linear and nonlinear 
programming. The basic numerical method to solve the nonlinear problem is the iterative solution 
method that starts from an initial guess, and iteratively refines it in an effort to reach the minimum (or 
maximum) of a multi-variable objective function. The iterative scheme is essentially a two-step process 
that seeks to determine: a) a search direction that does not violate the constraints and along which the 
objective function value decreases; and b) a step size that minimizes the function value along the chosen 
search direction. Normally, the algorithm terminates when either a minimum has been found, indicated 
by the function derivative being approximately zero, or when a certain maximum number of iterations 
has been exceeded indicating that there is no feasible solution to the problem.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
• STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
• PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
• STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Fundamental Engineering Optimization 
Methods
127 
NNumerica l Optimizati on Method
Learning Objectives: The learning objectives in this chapter are:
1.	 Understand numerical methods employed for solving optimization problems
2.	 Learn the approaches to numerically solve the line search problem in one-dimension
3.	 Learn the direction finding algorithms, including gradient and Hessian methods
4.	 Learn the sequential linear programming (SLP) and sequential quadratic programming 
(SQP) techniques
7.1	
The Iterative Method
The general numerical optimization method begins with an initial guess and iteratively refines it so as 
to asymptotically approach the optimum. To illustrate the iterative method of finding a solution, we 
consider an unconstrained nonlinear programming problem defined as:

࢞݂ሺ࢞ሻ
(7.1)
where x denotes the set of optimization variables. Let xk denote the current estimate of the minimum; 
then, the solution algorithm seeks an update, ࢞௞ାଵǡ that further reduces the function value, i.e., it results 
in: ݂൫࢞௞ାଵ൯൏݂൫࢞௞൯ also termed as the descent condition. 
In the general iterative scheme, the optimization variables are updated as per the following rule:
࢞௞ାଵൌ࢞௞൅ߙ௞ࢊ௞
(7.2)
In the above, dk represents any search direction and ߙ௞ is the step size along that direction. The iterative 
method is thus a two-step process:
1.	 Find the suitable search direction dk along which the function value locally decreases
2.	 Perform line search along dk to find ࢞௞ାଵ such that ݂൫࢞௞ାଵ൯ attains its minimum value 
We first consider the problem of finding a descent direction dk and note that it can be determined by 
checking the directional derivative of ݂൫࢞௞൯ along dk, which is given as the scalar product: ߘ݂൫࢞௞൯
்ࢊ௞
Since the scalar product is a function of the angle between the two vectors, the descent condition is 
satisfied if the angle between ߘ݂൫࢞௞൯ and ࢊ௞ is larger than 90°.
If the directional derivative of the function ݂൫࢞௞൯DORQJࢊ௞ is negative, then the descent condition 
is satisfied. Further, dk is a descent direction only if it satisfies: ߘ݂൫࢞௞൯
்ࢊ௞൏Ͳ,Iࢊ௞ If is a descent 
direction, then we are assured that at least for small positive values of ߙ௞݂൫࢞௞൅ߙ௞ࢊ௞൯൏݂ሺ࢞௞ሻ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
128 
NNumerica l Optimizati on Method
Next, assuming a suitable search direction dk has been determined, we next seek to determine a suitable 
step length ߙ௞, where an optimal value of ߙ௞ minimizes ݂൫࢞௞൯DORQJࢊ௞. Since both xk and dk are 
known, the projected function value along dk depends on ߙ௞ alone, and can be expressed as:

݂൫࢞௞൅ߙ௞ࢊ௞൯ൌ݂൫࢞௞൅ߙࢊ௞൯ൌ݂ሺߙሻ
(7.3)
The problem of choosing ߙ to minimize ݂൫࢞௞ାଵ൯ along dk  thus amounts to a single-variable functional 
minimization problem, also known as the line search problem, and is defined as:

ఈ݂ሺߙሻൌ݂൫࢞௞൅Ƚࢊ௞൯
(7.4)
Assuming that a solution exists, it is found at a point where the derivative of the function goes to zero. 
Thus, by setting ݂ԢሺߙሻൌͲ we can solve for the desired step size and update the current estimate xk. 
As an example of the line search problem, we consider minimizing a quadratic function: 
݂ሺ࢞ሻൌଵ
ଶ்࢞࡭࢞െ࢈்࢞ǡ ߘ݂ൌ࡭࢞െ࢈ 
(7.5)
where A is a symmetric positive definite matrix. Let d be a given descent direction; then, the line search 
problem reduces to the following minimization problem:

ఈ݂ሺߙሻൌ൫࢞௞൅ߙࢊ൯
்࡭൫࢞௞൅ߙࢊ൯െ࢈்൫࢞௞൅ߙࢊ൯
(7.6)
A solution is found by setting ݂ᇱሺߙሻൌࢊ்࡭൫࢞௞൅ߙࢊ൯െࢊ்࢈ൌͲ and is given as: 
ߙൌߘ݂ሺ࢞௞ሻ்ࢊ
ࢊ்࡭ࢊ
ൌࢊ்൫࡭࢞௞െ࢈൯
ࢊ்࡭ࢊ

(7.7)
An update then follows as: ࢞௞ାଵൌ࢞௞൅ߙࢊ
In the following, we first discuss numerical methods used to solve the line search problem in Sec. 7.2, 
followed by a discussion of the methods to solve the direction finding problem in Sec. 7.3.
7.2	
Computer Methods for Solving the Line Search Problem
In order to solve the line search problem, we assume that a suitable search direction dk has been 
determined, and wish to minimize the objective function: ݂൫࢞௞൅ߙࢊ௞൯ൌ݂ሺߙሻDORQJࢊ௞ We further 
assume dk that is a descent direction, i.e., it satisfies: ׏݂ሺ࢞௞ሻ்ࢊ௞൏Ͳ so that only positive values of 
ߙ need to be considered. Then, the line search problem reduces to finding a solution to (7.4) above. 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
129 
NNumerica l Optimizati on Method
In the following, we address the problem of finding the minimum of a function,݂ሺݔሻǡ ݔא Թǡ where 
we additionally assume that the function is unimodal, i.e., it has a single local minimum. Prominent 
computer methods for solving the line search problem are described below. 
7.2.1	
 Interval Reduction Methods
The interval reduction methods are commonly used to solve the line search problem. These methods 
find the minimum of a unimodal function in two steps: 
a)	 Bracketing the minimum to an interval 
b)	 Reducing the interval of uncertainty to desired accuracy 
The bracketing step aims to find a three-point pattern, such that for ݔଵǡ ݔଶǡ ݔଷ݂ሺݔଵሻ൑݂ሺݔଶሻ൐݂ሺݔଷሻ 
The bracketing algorithm can be started from any point in the domain of ݂ሺݔሻ though a good guess 
will reduce the number of steps involved. In the following description of the bracketing algorithm ݂௜ 
denotes ݂ሺݔሻ.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
130 
NNumerica l Optimizati on Method
Bracketing Algorithm (Belegundu & Chandrupatla p. 54): 
1.	 Initialize: choose ݔଵǡ οǡ ߛHJߛൌͳǤ͸ͳͺ
2.	 Set ݔଶൌݔଵ൅οHYDOXDWH݂ଵǡ ݂ଶ 
3.	 If ݂ଵ൏݂ଶVHWݔ଴՚ ݔଵǡ ݔଵ՚ ݔଶǡ ݔଶ՚ ݔ଴ǡ οൌെο 
4.	 Set Wοൌߛοݔଷൌݔଶ൅οHYDOXDWH݂ଷ
5.	 If ݂ଶ൒݂ଷVHW݂ଵ՚ ݂ଶǡ ݂ଶ՚ ݂ଷǡ ݔଵ՚ ݔଶǡ ݔଶ՚ ݔଷ, then go to step 3
6.	 Quit; points 1,2, and 3 satisfy ݂ଵ൒݂ଶ൏݂ଷ
Next, we assume that the minimum has been bracketed to a closed interval ሾݔ௟ǡ ݔ௨ሿ The interval 
reduction step aims to find the minimum in that interval. A common interval reduction approach is to 
use either the Fibonacci or the Golden Section methods; both methods are based on the golden ratio 
derived from Fibonacci’s sequence.
Fibonacci’s Method. The Fibonacci’s method uses Fibonacci numbers to achieve maximum interval 
reduction in a given number of steps. The Fibonacci number sequence is generated as: ܨ଴ൌܨଵൌͳǡ ܨ௜ൌ
ܨ௜ିଵ൅ܨ௜ିଶǡ ݅൒ʹ Fibonacci numbers have some interesting properties, among them:
1.	 The ratio ߬ൌ
௡՜ஶ
ி೙షభ
ி೙ൌξହିଵ
ଶ
ൌͲǤ͸ͳͺͲ͵Ͷ is known as the golden ratio. 
2.	 Using Fibonacci numbers, the number of interval reductions required to achieve a desired 
accuracy ߝ is the smallest n such that ͳȀܨ௡൏ߝ and can be specified in advance. 
3.	 For given l1 and ݊ we have ܫଶൌ
ி೙షభ
ி೙ܫଵǡ ܫଷൌܫଵെܫଶǡ ܫସൌܫଶെܫଷ etc. 
The Fibonacci algorithm is given as follows:
Fibonacci Algorithm (Belegundu & Chandrupatla p. 60):
Initialize: specify ݔଵǡ ݔସሺܫଵൌȁݔସെݔଵȁሻǡ ߝǡ ݊ǣ
ଵ
ி೙൏ߝ
Compute ߙଵൌ
ி೙షభ
ி೙ݔଶൌߙଵݔଵ൅ሺͳ െߙଵሻݔସHYDOXDWH݂ଶ
For ݅ൌͳǡ ǥ ǡ ݊െͳ 
1.	 Introduce ݔଷൌሺͳ െߙ௜ሻݔଵ൅ߙ௜ݔସǡHYDOXDWH݂ଷ 
2.	 If ݂ଶ൏݂ଷǡVHWݔସ՚ ݔଵǡ ݔଵ՚ ݔଷ
3.	 Else set ݔଵ՚ ݔଶǡ ݔଶ՚ ݔଷǡ ݂ଶ՚ ݂ଷ
4.	 Set ߙ௜ାଵൌ
ூ೙ష೔షభ
ூ೙ష೔
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
131 
NNumerica l Optimizati on Method
Golden Section Method. The golden section method uses the golden ratio ூ೔శభ
ூ೔ൌ߬ൌͲǤ͸ͳͺͲ͵Ͷfor 
interval reduction in the above Fibonacci algorithm. This results in uniform interval reduction strategy 
independent of the number of trials. Further, since the final interval ܫ௡ is related to the initial interval
ܫଵ as: ܫ௡ൌ߬௡ିଵܫଵ given I1 and a desired ܫ௡ǡ the number of interval reductions may be computed as:
݊ൌቔ
୪୬ூ೙ି୪୬ூభ
୪୬ఛ
൅
ଷ
ଶቕZKHUHہήۂ represents the floor function.
The golden section method can be integrated with the three-point bracketing algorithm by choosing 
ߛൌ
ଵ
ఛ and renaming ݔଷDVݔସ Stopping criteria for the golden section algorithm may be specified in 
terms of desired interval size, reduction in function value, or the number of interval reductions.
Next, the bracketing step can also be combined with the interval reduction step, and the integrated 
bracketing and interval reduction algorithm is given below.
Integrated Bracketing and Golden Section Algorithm (Belegundu & Chandrupatla p. 65):
Initialize: specify ݔଵǡ οǡ ߬ൌͲǤ͸ͳͺͲ͵Ͷǡ ߝ
1.	 Set ݔଶൌݔଵ൅οHYDOXDWH݂ଵǡ ݂ଶ
2.	 If ݂ଵ൏݂ଶVHWݔ଴՚ ݔଵǡ ݔଵ՚ ݔଶǡ ݔଶ՚ ݔ଴ǡ οൌെο
ο
3.	 Set οൌ
ο
ఛݔସൌݔଶ൅οHYDOXDWH݂ସ
4.	 If ݂ଶ൒݂ସVHW݂ଵ՚ ݂ଶǡ ݂ଶ՚ ݂ସǡ ݔଵ՚ ݔଶǡ ݔଶ՚ ݔସthen go to step 3
5.	 Introduce ݔଷൌሺͳ െ߬ሻݔଵ൅߬ݔସǡ evaluate ݂ଷ
6.	 If ݂ଶ൏݂ଷǡVHWݔସ՚ ݔଵǡ ݔଵ՚ ݔଷ
7.	 Else set ݔଵ՚ ݔଶǡ ݔଶ՚ ݔଷǡ ݂ଶ՚ ݂ଷ 
8.	 Check stopping criteria: ,Iȁݔଵെݔଷȁ ൏ߝ quit; else go to 5 
7.2.2	
 Approximate Search Algorithms
The calculations of the exact step size in the line search step are time consuming. In most cases, 
approximate function minimization suffices to advance to the next iteration. Since crude minimization 
methods may give rise to convergence issues, additional conditions on both dk and ߙ௞ are prescribed to 
ensure convergence of the numerical algorithm. These conditions include, for dk: a) sufficient descent 
condition, and b) gradient related condition; and for ߙ௞: a) sufficient decrease condition, and b) non 
trivial condition. They are described below.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
132 
NNumerica l Optimizati on Method
Sufficient Descent Condition. The sufficient descent condition, or the angle condition guards against dk 
becoming too close to ߘ݂൫࢞௞൯ The condition is normally stated as: െ
ఇ௙൫࢞ೖ൯
೅ࢊೖ
ฮఇ௙൫࢞ೖ൯ฮฮࢊೖฮ൒߳൐ͲIRUDVPDOO߳
Alternatively, the sufficient descent condition may be specified as:ߘ݂൫࢞௞൯
்ࢊ௞൏ܿฮߘ݂൫࢞௞൯ฮ
ଶǡܿ൐Ͳ 
Gradient Related Condition. The search direction is gradient related if ฮࢊ௞ฮ൒ܿฮߘ݂൫࢞௞൯ฮǡ ܿ൐Ͳ
This condition aids in convergence.
Sufficient Decrease Condition. The sufficient decrease condition on ߙ௞ ensures that a nontrivial 
reduction in the function value is obtained at each step. The condition is derived from Taylor series 
expansion of ݂൫࢞௞൅ߙ௞ࢊ௞൯ and is stated as: ݂൫࢞௞൅ߙ௞ࢊ௞൯െ݂൫࢞௞൯൑ߤߙ௞ߘ݂൫࢞௞൯
்ࢊ௞ǡ Ͳ ൏ߤ൏ͳ
Arjimo’s Rule. An alternative sufficient decrease condition, referred to as Arjimo’s rule, is given as: 
݂ሺߙሻ൑݂ሺͲሻ൅ߤߙ݂ᇱሺͲሻǡ
Ͳ ൏ߤ൏ͳ
(7.8)
Curvature Condition. A curvature condition is added to Arjimo’s rule to improve convergence. The 
curvature condition is given as:
ȁ݂ᇱሺߙሻȁ ൑ߟȁ݂ᇱሺͲሻȁǡ
Ͳ ൑ߟ൏ͳ 
(7.9)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
“The perfect start 
of a successful, 
international career.”
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Fundamental Engineering Optimization 
Methods
133 
NNumerica l Optimizati on Method
Further, the curvature condition implies that: ቚ׏݂൫࢞௞൅ߙ௞ࢊ௞൯
்ࢊ௞ቚ൑ߟቚ׏݂൫࢞௞൯
்ࢊ௞ቚǡ Ͳ ൑ߟ൏ͳ
Conditions (7.8) and (7.9) together with ߤ൑ߟ are known as Wolfe conditions, which are commonly 
used by all line search algorithms. A line search based on Wolfe conditions proceeds by bracketing the 
minimizer in an interval, followed by estimating it via polynomial approximation. These two steps are 
explained below:
Bracketing the Minimum. In the bracketing step we seek an interval ൣߙǡ ߙ൧ such that W݂ᇱ൫ߙ൯൏Ͳ and 
݂ᇱሺߙሻ൐Ͳ Since for any descent direction, ݂ᇱሺͲሻ൏Ͳ therefore, ߙൌͲ serves as initial lower bound 
on ߙ To find an upper bound, increasing ߙ values, e.g., ߙൌͳǡʹǡ ǥ are tried. Assume that for some 
ߙ௜൐Ͳ ݂ᇱሺߙ௜ሻ൏Ͳ݂ᇱሺߙ௜ାଵሻ൐Ͳ then, ߙ௜ serves as an upper bound. 
Estimating the Minimum. Once the minimum has been bracketed to a small interval, a quadratic or 
cubic polynomial approximation is used to find the minimizer. If the polynomial minimizer ߙො satisfies 
Wolfe’s condition for the desired ߟ value VD\ߟൌͲǤͷሻ and the sufficient decrease condition for the 
desired ߤ value (say ߤൌͲǤʹ), it is taken as the function minimizer, otherwise ߙො is used to replace one 
of the ߙߙ and the polynomial approximation step repeated.
Quadratic curve Fitting. Assuming that the interval ሾߙ௟ǡ ߙ௨ሿ contains the minimum of a unimodal 
function, ݂ሺߙሻ it can be approximated by a quadratic function: ݍሺߙሻൌܽ଴൅ܽଵߙ൅ܽଶߙଶ A quadratic 
approximation uses three points ሼߙ௟ǡ ߙ௠ǡ ߙ௨ሽ where the mid-point of the interval may be used for ߙ௠ 
The quadratic coefficients ሼܽ଴ǡ ܽଵǡ ܽଶሽ are solved from: ݂ሺߙ௜ሻൌܽ଴൅ܽଵߙ௜൅ܽଶߙ௜
ଶǡߙ௜߳ሼߙ௟ǡ ߙ௠ǡ ߙ௨ሽ 
which results in the following expressions:

ܽଶൌ
ͳ
ߙ௨െߙ௠
ቈ݂ሺߙ௨ሻെ݂ሺߙ௟ሻ
ߙ௨െߙ௟
െ݂ሺߙ௠ሻെ݂ሺߙ௟ሻ
ߙ௠െߙ௟
቉Ǣ
ܽଵൌ
ͳ
ߙ௠െߙ௟
൫݂ሺߙ௠ሻെ݂ሺߙ௟ሻ൯െܽଶሺߙ௟൅ߙ௠ሻǢ 
(7.10)
ܽ଴ൌ݂ሺߙ௟ሻെܽଵߙ௟െܽଶߙ௟
ଶ
The minimum for ݍሺߙሻ can be computed by setting ݍᇱሺߙሻൌͲǡ and is given as: ߙ௠௜௡ൌെ
௔భ
ଶ௔మ An 
explicit formula for ߙ௠௜௡ in terms of the three interval points can also be derived and is given as:
ߙ௠௜௡ൌߙ௠െͳ
ʹ
ሺߙ௠െߙ௟ሻଶሺ݂ሺߙ௠ሻെ݂ሺߙ௨ሻሻെሺߙ௠െߙ௨ሻଶሺ݂ሺߙ௠ሻെ݂ሺߙ௟ሻሻ
ሺߙ௠െߙ௟ሻሺ݂ሺߙ௠ሻെ݂ሺߙ௨ሻሻെሺߙ௠െߙ௨ሻሺ݂ሺߙ௠ሻെ݂ሺߙ௟ሻሻ 
(7.11)
An example of the approximate search algorithm is now presented.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
134 
NNumerica l Optimizati on Method
Example 7.1: Approximate search algorithm (Ganguli, p. 121)
We wish to approximately solve the following minimization problem: 
ఈ݂ሺߙሻൌ݁ିఈ൅ߙଶ We use 
Arjimo’s rule with: ߤൌͲǤʹ and ߙൌͲǤͳǡ ͲǤʹǡ ǥ to estimate the minimum. The Matlab commands 
used for this purpose and the corresponding results appear below:
>> f=inline(‘x.*x+exp(-x)’); mu=0.2; al=0:.1:1; 
>> feval(f,al)
1.0000	
0.9148	
0.8587	
0.8308	
0.8303	
0.8565	
0.9088	
0.9866	
1.0893	
1.2166	
1.3679
>> 1-mu*al
1.0000	
0.9800	
0.9600	
0.9400	
0.9200	
0.9000	
0.8800
0.8600	
0.8400	
0.8200	
0.8000
Then, according to Arjimo’s condition, an estimate of the minimum is given as: ߙൌͲǤͷǤ Further, 
since ݂ᇱሺͲሻ൏ͲDQG݂ᇱሺߙሻ൐ͲWand the minimum is bracketed by [0, 0.5]. We next use quadratic 
approximation of the function over ሼͲǡ
ఈ
ଶǡ ߙሽ to estimate the minimum as follows:
al=0; ai=0.25; au=0.5;
a2 = ((f(au)-f(al))/(au-al)-(f(ai)-f(al))/(ai-al))/(au-ai);
a1 = (f(ai)-f(al))/(ai-al)-a2*(al+ai);
xmin = -a1/a2/2 = 0.3531
An estimate of the minimum is given as: ߙොൌͲǤ͵ͷ͵ͳ We note that the exact solution is given as: 
ߙ௠௜௡ൌͲǤ͵ͷͳ͹.
Next, we describe the computer methods for finding the search direction. Our initial focus is on 
unconstrained problems. The constrained problems are discussed later in Sec. 7.4.
7.3	
Computer Methods for Finding the Search Direction
The computer methods for finding the search direction ࢊ௞ are normally grouped into first order and 
second order methods, where the order refers to the derivative order of the function approximation 
used. Thus, first order methods refer to the gradient-based methods, while the second order methods 
additionally involve the Hessian matrix in the computations. The gradient based quasi-Newton methods 
are overwhelmingly popular when it comes to implementation. We describe popular search methods 
below.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
135 
NNumerica l Optimizati on Method
7.3.1	
 The Steepest Descent Method
The steepest descent method, attributed to Cauchy, is the simplest of the gradient methods. The method 
involves choosing dk to locally move in the direction of maximum decrease in the function value, i.e., 
the direction opposite to the gradient vector at the current estimate point. 
Thus, the steepest descent method is characterized by: ࢊ௞ൌെߘ݂ሺ࢞௞ሻ which leads to the following 
update rule:
࢞௞ାଵൌ࢞௞െߙ௞ή ߘ݂ሺ࢞௞ሻ
(7.12)
where the step ߙ௞W size to minimize ݂ሺ࢞௞ାଵሻ can be analytically or numerically determined using 
methods described in Sec. 7.2. 
As an example, in the case of a quadratic function: ݂ሺ࢞ሻൌ
ଵ
ଶ்࢞࡭࢞െ࢈்࢞ǡ ߘ݂ൌ࡭࢞െ࢈ the steepest 
descent method with exact line search results in the following update rule: 
࢞௞ାଵൌ࢞௞െߙή ߘ݂൫࢞௞൯Ǣ ߙൌ׏݂൫࢞௞൯
்׏݂൫࢞௞൯
׏݂ሺ࢞௞ሻ்ۯ׏݂ሺ࢞௞ሻ
(7.13)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
▶▶enroll by September 30th, 2014 and 
▶▶save up to 16% on the tuition!
▶▶pay in 10 installments / 2 years
▶▶Interactive Online education
▶▶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Fundamental Engineering Optimization 
Methods
136 
NNumerica l Optimizati on Method
The above update can be equivalently described in terms of a residual: ࢘௞ൌ࢈െ࡭࢞௞ൌെߘ݂ሺ࢞௞ሻDV

࢞௞ାଵൌ࢞௞൅ߙ௞࢘௞Ǣߙ௞ൌ࢘௞
்࢘௞
࢘௞
்ܣ࢘௞

(7.14)
The steepest descent algorithm is given below.
Steepest Descent Algorithm:
Initialize: choose ࢞଴
For ݇ൌͲǡͳǡʹǡ ǥ
1.	 Compute ߘ݂ሺ࢞௞ሻ
2.	 Check convergence: if ฮߘ݂ሺ࢞௞ሻฮ൏߳ stop.
3.	 Set ࢊ௞ൌെߘ݂ሺ࢞௞ሻ
4.	 Line search problem: Find 
ఈஹ଴݂ሺ࢞௞൅ߙࢊ௞ሻ
5.	 Set ࢞௞ାଵൌ࢞௞൅ߙࢊ௞ 
We note that a line search that minimizes ݂ሺߙሻ along the steepest-descent direction may not result in 
the lowest achievable function value over all search directions. This could happen, for example, when 
the current gradient ߘ݂൫࢞௞൯ points away from the local minimum, as is shown in the example presented 
at the end of the section.
A further weakness of the steepest descent method is that it becomes slow as the minimum is approached. 
This can be seen by examining the function derivative ݂ᇱሺߙ௞ሻ which is computed as follows:
݀
݀ߙ௞
݂൫࢞௞൅ߙ௞ࢊ௞൯ൌ׏݂൫࢞௞ାଵ൯
்ࢊ௞
(7.15)
The above result implies that the gradient ׏݂൫࢞௞ାଵ൯ is normal to dk, i.e., in the case of steepest descent, 
normal to ׏݂൫࢞௞൯ This implies a zigzag type progression towards the minimum that results in its slow 
progress. Due to its above weaknesses, the steepest descent method does not find much use in practice.
Rate of Convergence. The steepest-descent method displays linear convergence. In the case of quadratic 
functions, its rate constant is bounded by the following inequality (Griva, Nash & Sofer 2009, p. 406):
ܥൌ݂൫࢞௞ାଵ൯െ݂ሺ࢞כሻ
݂ሺ࢞௞ሻെ݂ሺ࢞כሻ൑ቆܿ݋݊݀ሺ࡭ሻെͳ
ܿ݋݊݀ሺ࡭ሻ൅ͳቇ
ଶ

(7.16)
The above result uses ݂൫࢞௞൯െ݂ሺ࢞כሻ which converges at the same rate as Vฮ࢞௞െ࢞כฮ) Further, when 
using steepest-descent method with general nonlinear functions, the bound holds for ࡭ൌ׏ଶ݂ሺݔכሻ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
137 
NNumerica l Optimizati on Method
Preconditioning. As with all gradient methods, preconditioning aimed at reducing the condition number 
of the Hessian matrix can be employed to aid convergence of the steepest-descent method. To illustrate 
this point, we consider the cost function: ݂ሺ࢞ሻൌͲǤͳݔଵ
ଶ൅ݔଶ
ଶൌ்࢞࡭࢞ǡ ࡭ൌ diag (0,1,1), and define a 
linear transformation: ࢞ൌࡼ࢟ where ࡼൌ݀݅ܽ݃ሺξͳͲǡ ͳሻ Then, the objective function is transformed 
as: ݂ሺ࢞ሻൌ்࢟ࡼ்࡭ࡼ࢟ where the matrix product 
݃ሺ
ሻ
ࡼ்࡭ࡼൌࡵK has a condition number of unity, indicating 
that the steepest-descent method will now converge in a single iteration. 
An example of the steepest descent method is now presented.
Example 7.2: Steepest Descent
We consider minimizing ݂ሺ࢞ሻൌͲǤͳݔଵ
ଶ൅ݔଶ
ଶ from an initial estimate ࢞଴ൌሺͷǡͳሻ The gradient of 
I ݂ሺ࢞ሻ is computed as , and ߘ݂ሺ࢞ሻൌ൤ͲǤʹݔଵ
ʹݔଶ൨DQGߘ݂ሺ࢞଴ሻൌቂͳ
ʹቃ Using the steepest-descent rule, the 
line search problem is given as: 
ఈ݂ሺߙሻൌͲǤͳሺͷ െߙሻଶ൅ሺͳ െʹߙሻଶ7 The exact solution is found 
by setting ݂ᇱሺߙሻൌͺǤʹߙെͷ ൌͲRUߙൌͲǤ͸ͳ Therefore, ࢞ଵൌቂͶǤ͵ͻ
െͲǤʹʹቃDQG݂ሺ࢞ଵሻൌͳNext, 
we try an arbitrary search direction ࢊ଴ൌቂെͳ
Ͳ ቃ which gives ݂ሺߙሻൌͲǤͳሺͷ െߙሻଶ and a similar 
minimization results in ݂ᇱሺߙሻൌͲǤʹߙെͳ ൌͲRUߙൌͷIRUZKLFK࢞ଵൌቂͲ
ͳቃDQG݂ሺ࢞ଵሻൌͳwhich 
provides a better estimate of the actual minimum (0,0).
7.3.2	
 Conjugate-Gradient Methods
Conjugate-gradient (CG) methods employ conjugate vectors with respect to the Hessian matrix, as 
search directions in successive iterations; these directions hold the promise to minimize the function 
in ݊ steps. The CG methods are popular in practice due to their low memory requirements and strong 
local and global convergence properties. 
Let ࢊ଴ǡ ࢊଶǡ ǥ ǡ ࢊ௡ିଵ where ࢊ௜்࡭ࢊ௝ൌͲǡ ്݆݅ǡ denote conjugate directions with respect to A matrix, 
and let ࢍ௞ denote ׏݂൫࢞௞൯Ǥ Then, starting from the steepest descent direction, we can use the following 
procedure to generate A-conjugate directions: 

ࢊ଴ൌെࢍ଴Ǣࢊ௞ାଵൌെࢍ௞ାଵ൅ߚ௞ࢊ௞݇൒Ͳ
(7.17)
Next, application of the conjugacy condition results in:

ࢊ௞்࡭ࢊ௞ାଵൌെࢊ௞்࡭݃௞ାଵ൅ߚ௞ࢊ௞்࡭ࢊ௞ൌͲǡߚ௞ൌࢍ௞ାଵ
்
࡭ࢊ௞
ࢊ௞்࡭ࢊ௞
(7.18)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
138 
NNumerica l Optimizati on Method
where we note that this expression can be further simplified if additional assumptions regarding 
the function and the line search algorithm are made. For example, since in the case of a quadratic 
function: 
ࢍ௞ାଵെࢍ௞ൌ࡭ሺ࢞௞ାଵെ࢞௞ሻൌߙ௞࡭ࢊ௞ by substituting ࡭ࢊ௞ൌ
ଵ
ఈೖሺࢍ௞ାଵെࢍ௞ሻ in 
(7.18), we obtain: ߚ௞ൌ
ࢍೖశభ
೅
ሺࢍೖశభିࢍೖሻ
ࢊೖ೅ሺࢍೖశభିࢍೖሻ(the Hestenes-Stiefel formula). Further, in the case of 
exact line search,݃௞ାଵ
்
ࢊ௞ൌͲ ߚ௞ൌ
ࢍೖశభ
೅
ሺࢍೖశభିࢍೖሻ
ࢍೖ
೅ࢍೖ
 (the Polak-Ribiere formula). Finally, since 
ࢍ௞ାଵ
்
ࢊ௞ൌࢍ௞ାଵ
்
൫െࢍ௞൅ߚ௞ିଵࢊ௞ିଵ൯ൌͲ,whereas for quadratic functions, ࢍ௞ାଵൌࢍ௞൅ߙ௞࡭ࢊ௞ 
therefore, by exact line search condition, ࢍ௞ାଵ
்
ࢍ௞ൌߚ௞ିଵሺࢍ௞൅ߙ௞࡭ࢊ௞ሻ்ࢊ௞ିଵൌͲ resulting in 
ߚ௞ൌ
ࢍೖశభ
೅
ࢍೖశభ
ࢍೖ
೅ࢍೖ
 (the Fletcher-Reeves formula). Other versions of ߚ௞ have also been proposed.
The significance of the conjugacy property is apparent if we formulate a solution as: ݕൌσ
ߙ௜ࢊ௜
௡
௜ୀଵ
 
which is composed of I ݊ conjugate vectors. Then, the minimization problem is decomposed into a set 
of one-dimensional problems given as:

௬݂ሺ࢟ሻൌ෍

ఈ೔൬ͳ
ʹ ߙ௜
ଶࢊ௜்࡭ࢊ௜െߙ௜ࢉ்ࢊ௜൰
௡
௜ୀଵ

(7.19)
Then, by setting the derivative with respect to ߙ௜ equal to zero, we obtain: ߙ௜ࢊ௜்࡭ࢊ௜െࢉ்ࢊ௜ൌͲ leading 
to: ߙ௜ൌ
ࢉ೅ࢊ೔
ࢊ೔೅࡭ࢊ೔ The CG method iteratively determines conjugate directions ࢊ௜ and their coefficients ߙ௜. 
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
139 
NNumerica l Optimizati on Method
A Conjugate-gradient algorithm that uses residuals: ࢘௜ൌ࢈െ࡭࢞௜ǡ ݅ൌͳǡʹǡ ǥ ǡ ݊ is given below: 
Conjugate-Gradient Algorithm (Griva, Nash & Sofer, p454):
Init: Choose ࢞଴ൌ૙ǡ ࢘଴ൌ࢈ǡ ࢊሺିଵሻൌͲǡ ߚ଴ൌͲǤ
For ݅ൌͲǡͳǡ ǥ
1.	 Check convergence: if ԡ࢘௜ԡ ൏߳ stop.
2.	 If I ݅൐ͲVHWߚ௜ൌ
࢘೔
೅࢘೔
࢘೔షభ
೅
࢘೔షభ  
3.	 Set ࢊ௜ൌ࢘௜൅ߚ௜ࢊ௜ିଵߙ௜ൌ
࢘೔
೅࢘೔
ࢊ೔೅࡭ࢊ೔࢞௜ାଵൌ࢞௜൅ߙ௜ࢊ௜࢘௜ାଵൌ࢘௜െߙ௜࡭ࢊ௜
Preconditioning. In all gradient-based methods, the convergence rates improve when the Hessian matrix 
has a low condition number. Preconditioning, or scaling, aimed at reducing the condition number, 
therefore, helps to speed up the convergence rates. Preconditioning involves a linear transformation:
࢞ൌࡼ࢟ where P is invertible. 
In the case of CG method, as a result of preconditioning, the conjugate directions are modified as:
ࢊ଴ൌെࡼࢍ଴Ǣࢊ௞ାଵൌെࡼࢍ௞ାଵ൅ߚ௞ࢊ௞݇൒Ͳ
(7.20)
The modified CG parameter (in the case of Fletcher-Reeves formula) is given as: ߚ௞ൌ
ࢍೖశభ
೅
ࡼࢍೖశభ
ࢍೖ
೅ࡼࢍೖ
 Finally, 
the CG algorithm is modified to include preconditioning as follows:
Preconditioned Conjugate-Gradient Algorithm (Griva, Nash & Sofer, p. 475):
Initialize: Choose ࢞଴ൌ૙ǡ ࢘଴ൌ࢈ǡ ࢊሺିଵሻൌͲǡ ߚ଴ൌͲǤ
For ݅ൌͲǡͳǡ ǥ
1.	 Check convergence: if ԡ࢘௜ԡ ൏߳ stop.
2.	 Set Wࢠ௜ൌࡼିଵ࢘௜,I݅൐ͲVHWߚ௜ൌ
࢘೔
೅ࢠ೔
࢘೔షభ
೅
ࢠ೔షభ
3.	 Set ࢊ௜ൌࢠ௜൅ߚ௜ࢊ௜ିଵߙ௜ൌ
࢘೔
೅ࢠ೔
ࢊ೔೅࡭ࢊ೔࢞௜ାଵൌ࢞௜൅ߙ௜ࢊ௜࢘௜ାଵൌ࢘௜െߙ௜࡭ࢊ௜
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
140 
NNumerica l Optimizati on Method
Rate of Convergence. Conjugate gradient methods achieve superlinear convergence, which degenerates 
to linear convergence if the initial direction is not chosen as the steepest descent direction. In the case 
of quadratic functions, the minimum is reached exactly in ݊ iterations. For general nonlinear functions, 
convergence in ʹ݊ iterations is to be expected. Nonlinear CG methods typically have the lowest per 
iteration computational costs. 
An example of the CG method is given below.
Example 7.3: Conjugate-gradient method
We wish to solve the following minimization problem: 
݂࢞ሺݔଵǡ ݔଶሻൌݔଵ
ଶ൅ͲǤͷݔଶ
ଶെݔଵݔଶǡ where: 
׏݂ሺ࢞ሻ்ൌሾʹݔଵെݔଶǡ ݔଶെݔଵሿ
Let ݔ଴ൌሺͳǡͳሻǡWKHQ׏݂ሺ࢞଴ሻൌࢉ଴ൌሾͳǡ Ͳሿ் and we set ࢊ଴ൌെࢉ଴ൌሾെͳǡͲሿ் which results in: ࢞ଵൌ
ሾͳ െߙǡ ͳሿ்DQG݂ሺߙሻൌሺͳ െߙሻଶ൅ߙെͲǤͷ Setting ݂ᇱሺߙሻൌͲǡZ we obtain: ߙൌͲǤͷ and the solution 
estimate is updated as ࢞ଵൌሾͲǤͷǡ ͳሿ்
In the second iteration, we set ࢊଵൌെࢉଵ൅ߚ଴ࢊ଴ZKHUHࢉଵൌሾͲǡ ͲǤͷሿ்ǡ ߚ଴ൌ
ฮࢉభฮ
ԡࢉబԡ ൌͲǤʹͷǤ Accordingly, 
ࢊଵൌሾെͲǤʹͷǡ െͲǤͷሿ்ǡ ࢞ଶൌሺͳ െͲǤͷߙሻሾͲǤͷǡ ͳሿ்DQG݂ሺߙሻൌͲǤʹͷሺͳ െͲǤͷߙሻଶ Again, by setting 
݂ᇱሺߙሻൌͲǡZHREWDLQߙൌʹZKLFKJLYHV࢞ଶൌሾͲǡ Ͳሿ We note that the minimum of a quadratic 
function of two variables is reached in two iterations.
7.3.3	
 Newton’s Method
Newton’s method for finding the zero of a nonlinear function was earlier introduced in Section 2.11. 
Here we apply Newton’s method to solve the nonlinear equation resulting from the application of FONC: 
׏݂ሺ࢞ሻൌͲ We use a linear approximation to ׏݂ሺ࢞ሻ to apply this condition as:
׏݂൫࢞௞൅ࢊ൯؆ ׏݂൫࢞௞൯൅׏ଶ݂ሺݔ௞ሻࢊൌ૙
(7.21)
Then, the direction vector is solved from a system of linear equations given as: 
׏ଶ݂ሺ࢞௞ሻࢊൌെ׏݂ሺ࢞௞ሻ
(7.22)
which leads to the following update formula:
࢞௞ାଵൌ࢞௞െ൫׏ଶ݂ሺ࢞௞ሻ൯
ିଵ׏݂ሺ࢞௞ሻൌ࢞௞െࡴ௞
ିଵࢍ௞
(7.23)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
141 
NNumerica l Optimizati on Method
We note that the above formula can also be obtained via second order Taylor series expansion of ݂ሺ࢞ሻ 
given as: 
݂൫࢞௞൅ࢊ൯ൌ݂ሺ࢞௞ሻ൅׏݂ሺ࢞௞ሻ்ࢊ൅ଵ
ଶࢊ்ࡴ௞ࢊൌݍ௞ሺࢊሻ
(7.24)
The above expression implies that at every iteration Newton’s method approximates ݂ሺ࢞ሻ by a quadratic 
function: qk(d); it then solves the minimization problem: min
d qk(d) and updates the current estimate 
as: ࢞௞ାଵൌ࢞௞൅ࢊ Further, the above solution assumes that qk(d) is convex, i.e., ࡴ௞ൌ׏ଶ݂ሺ࢞௞ሻ is 
positive-definite. 
The application of Newton’s method relies on the positive-definite assumption for ࡴ௞ൌ׏ଶ݂ሺ࢞௞ሻ If 
׏ଶ݂ሺ࢞௞ሻ is positive-definite, then a factorization of the form: ׏ଶ݂ሺ࢞௞ሻൌࡸࡰࡸ் where ݀௜௜൐Ͳ can 
be used to solve for the resulting system of linear equations: ሺࡸࡰࡸ்ሻࢊൌെ׏݂ሺ࢞௞ሻ If at any point D 
is found to have negative entries, i.e., if ݀௜௜൑Ͳ then it should be replaced by a positive value, such as 
ȁ݀௜௜ȁ This correction amounts to adding a diagonal matrix E, such that ׏ଶ݂ሺ࢞௞ሻ൅ࡱ is positive-definite. 
An algorithm for Newton’s method is given below.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Fundamental Engineering Optimization 
Methods
142 
NNumerica l Optimizati on Method
Newton’s Method (Griva, Nash, & Sofer, p. 373): 
Initialize: Choose ࢞଴ specify ߳
For ݇ൌͲǡͳǡ ǥ
1.	 Check convergence: If I ԡ׏݂ሺ࢞௞ሻԡ ൏߳ stop
2.	 Factorize modified Hessian as 
௞
S
׏ଶ݂ሺ࢞௞ሻ൅ࡱൌࡸࡰࡸ்DQGVROYHሺࡸࡰࡸ்ሻࢊൌെ׏݂ሺ࢞௞ሻIRUࢊ 
3.	 Perform line search to determine ߙ௞ and update the solution estimate as 
݂
௞
࢞௞ାଵൌ࢞௞൅ߙ௞ࢊ௞
Rate of Convergence. Newton’s method achieves quadratic rate of convergence in the close neighborhood 
of the optimal point, and superlinear rate of convergence otherwise. Moreover, due to its high 
computational and storage costs, classic Newton’s method is rarely used in practice. 
7.3.4	
Quasi-Newton Methods
Quasi-Newton methods that use low-cost approximations to the Hessian matrix are the among most 
widely used methods for nonlinear problems. These methods represent a generalization of one-
dimensional secant method, which approximates the second derivative as: ݂ᇱᇱሺݔ௞ሻ؆
௙ᇲሺ௫ೖሻି௙ᇲሺ௫ೖషభሻ
௫ೖି௫ೖషభ

In the multi-dimensional case, the secant method translates into the following: 
׏ଶ݂ሺ࢞௞ሻሺ࢞௞െ࢞௞ିଵሻ؆ ߘ݂ሺ࢞௞ሻെߘ݂ሺ࢞௞ିଵሻ
(7.25)
Thus, if the Hessian is approximated by a positive-definite matrix  Hk, then Hk then is required to satisfy 
the following secant condition: 
ࡴ௞ሺ࢞௞െ࢞௞ିଵሻൌߘ݂ሺ࢞௞ሻെߘ݂ሺ࢞௞ିଵሻ
(7.26)
Whereas, the above condition places ݊ constraints on the structure of Hk, further constraints may be 
added to completely specify Hk as well as to preserve its symmetry. 
The quasi-Newton methods aim to iteratively update Hk via: 
1.	 The direct update: ࡴ௞ାଵൌࡴ௞൅οࡴ௞ǡ ࡴ଴ൌࡵRU
2.	 The inverse update: ࡲ௞ାଵൌࡲ௞൅οࡲ௞ǡࡲൌࡴିଵǡ ࡲ଴ൌࡵǤ
Once Hk is available, it can be employed to solve for the current search direction from: ࡴ௞ࢊൌെ׏݂ሺ࢞௞ሻ
or from: ࢊൌെࡲ௞׏݂ሺ࢞௞ሻ 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
143 
NNumerica l Optimizati on Method
To proceed further, let ࢙௞ൌ࢞௞െ࢞௞ିଵ࢟௞ൌߘ݂ሺ࢞௞ሻെߘ݂ሺ࢞௞ିଵሻǢ then, a symmetric rank-one update 
formula for Hk is given as (Griva, Nash & Sofer, p.414):
ࡴ௞ାଵൌࡴ௞൅
ሺ࢟௞െࡴ௞࢙௞ሻሺ࢟௞െࡴ௞࢙௞ሻ்
ሺ࢟௞െࡴ௞࢙௞ሻ்࢙௞

(7.27)
However, the above formula, while obeying the secant condition, ࡴ௞ାଵ࢙௞ൌ࢟௞ does not ensure that Hk 
is positive-definite. Next, a class of symmetric rank-two update formulas that ensures positive-definiteness 
of Hk are defined by: 
ࡴ௞ାଵൌࡴ௞െ
ሺࡴ௞࢙௞ሻሺࡴ௞࢙௞ሻ்
࢙௞்ࡴ௞࢙௞
൅࢟௞࢟௞்
࢟௞்࢙௞
൅߶ሺ࢙௞்ࡴ௞࢙௞ሻ࢜௞࢜௞்
(7.28)
where ࢜௞ൌ
࢟ೖ
࢟ೖ೅࢙ೖെ
ࡴೖ࢙ೖ
࢙ೖ೅ࡴೖ࢙ೖDQG߶א ሾͲǡͳሻ Two popular choices for ߶DUH߶ൌͲDQG߶ൌͳ resulting 
in the well-known DFP (Davison, Fletcher, and Powell) and BGFS (Broyden, Fletcher, Goldfarb, and 
Shanno) update formulas. 
The former (DFP formula) results in the following inverse Hessian update:
ࡲ௞ାଵൌࡲ௞െ
ሺࡲ௞࢟௞ሻሺࡲ௞࢟௞ሻ்
࢟௞்ࡲ௞࢟௞
൅࢙௞࢙௞்
࢟௞்࢙௞
 
(7.29)
The latter (BFGS formula) results in a direct Hessian update given as:
ࡴ௞ାଵൌࡴ௞െ
ሺࡴ௞࢙௞ሻሺࡴ௞࢙௞ሻ்
࢙௞்ࡴ௞࢙௞
൅࢟௞࢟௞்
࢟௞்࢙௞

(7.30)
We note that the Hessian in the case of a quadratic function, ݍሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞െࢉ்࢞ obeys the secant 
condition, ࡽ࢙௞ൌ࢟௞, which shows that a symmetric positive-definite Hk in a quasi-Newton method 
locally approximates the quadratic behavior. 
The quasi-Newton algorithm is given below.
Quasi-Newton Algorithm (Griva, Nash & Sofer, p. 415):
Initialize: Choose ࢞଴ࡴ଴HJࡴ଴ൌࡵ specify ߝ 
For ݇ൌͲǡͳǡ ǥ
1.	 Check convergence: If ԡ׏݂ሺ࢞௞ሻԡ ൏ߝ stop
2.	 Solve ࡴ௞ࢊൌെ׏݂ሺ࢞௞ሻIRUࢊ௞ 
3.	 Solve for 
ఈ݂൫࢞௞൅ߙࢊ௞൯IRUߙ௞DQGXSGDWH࢞௞ାଵൌ࢞௞൅ߙ௞ࢊ௞
4.	 Compute ࢙௞ǡ ࢟௞ and update Hk as per (5.19) or (5.20)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
144 
NNumerica l Optimizati on Method
Rate of Convergence. Quasi-Newton methods achieve superlinear convergence, thus rivaling the second 
order methods for solving nonlinear programming (NP) problems.
Example 7.4: Quasi-Newton method
As an example, we consider the following NL problem: 

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌʹݔଵ
ଶെݔଵݔଶ൅ݔଶ
ଶ
ZKHUH׏݂ൌ൤Ͷݔଵെݔଶ
ʹݔଶെݔଵ൨ǡ ܪൌቂͶ െͳ
െͳʹቃ 
Let ݔ଴ൌቂͳ
ͳቃǡ ܪ଴ൌܫWKHQ݂଴ൌʹǡ ݀଴ൌെ׏݂ሺݔ଴ሻൌቂെ͵
െͳቃ8VLQJ݂ሺߙሻൌʹሺͳ െ͵ߙሻଶ൅ሺͳ െߙሻଶെ
ሺͳ െ͵ߙሻሺͳ െߙሻ and putting ݂ᇱሺߙሻൌͲJLYHVߙൌ
ହ
ଵ଺7KHQݏଵൌߙ݀଴ൌ
ହ
ଵ଺ቂͳ
ͳቃǡ ݃ଵൌݕଵǡ ࢞ଵൌቀ
ଷ
ସǡ
ଷ
ସቁ
For the Hessian update, we have: ݂ଵൌͲǤͷ͸ʹͷǡ ݃ଵൌെͲǤͳʹͷǡ ݃ଶൌ݃ଷൌെͲǤ͹ͷǢࢉଵൌሾͲǤ͹ͷǡ ͲǤ͹ͷሿ
଴
଴
଴
଴
 
and, for ߙൌͲǤʹͷǡ࢙଴ൌሾെͲǤʹͷǡ െͲǤʹͷሿൌࢠ଴ൌ࢟଴ǡ ߦଵൌߦଶൌͲǤͳʹͷǡ ߠൌͳǡ ࢝଴ൌ࢟଴ǡ ߦଷൌߦଵ 
then, the Hessian update is computed as: ࡰ଴ൌͺ ቂͳ
ͳ
ͳ
ͳቃǡ ࡱ଴ൌͺ ቂͳ
ͳ
ͳ
ͳቃǡ ࡴଵൌࡴ଴Ǥ 
We next proceed to discuss the trust-region methods of solving NP problems.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Master’s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top master’s programmes
• 33rd place Financial Times worldwide ranking: MSc 
International Business
• 1st place: MSc International Business
• 1st place: MSc Financial Economics
• 2nd place: MSc Management of Learning
• 2nd place: MSc Economics
• 2nd place: MSc Econometrics and Operations Research
• 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier ‘Beste Studies’ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Fundamental Engineering Optimization 
Methods
145 
NNumerica l Optimizati on Method
7.3.5	
 Trust-Region Methods
Trust-region methods locally employ a quadratic approximation ݍ௞ሺ࢞௞ሻ to the nonlinear objective 
function; they were originally proposed to solve the nonlinear least-squares problems, but have since 
been adapted to solve more general optimization problems. 
The quadratic approximation is given as: ݍሺ࢞ሻൌ
ଵ
ଶ்࢞ࡽ࢞െࢉ்࢞ǡ and is valid in a limited neighborhood 
ȳ௞ൌሼ࢞ǣ ԡડሺ࢞െ࢞௞ሻԡ ൑ο௞ሽRI࢞௞ where ડ is a scaling parameter. The method then aims to find 
a ࢞௞ାଵא ȳ௞ǡ which results in sufficient decrease in ݂ሺ࢞ሻ At each iteration k, trust-region algorithm 
solves a constrained optimization sub-problem defined by: 

ࢊݍ௞ሺࢊሻൌ݂ሺ࢞௞ሻ൅׏݂ሺ࢞௞ሻ்ࢊ൅ͳ
ʹ ࢊ்׏ଶ݂ሺ࢞௞ሻࢊ 
(7.31)
subject to ԡࢊԡ ൑ο௞ 
Using a Lagrangian function approach the first order optimality conditions are given as:
ሺ׏ଶ݂ሺ࢞௞ሻ൅ߣࡵሻࢊ௞ൌെ׏݂ሺ࢞௞ሻ
(7.32)
where Hߣ൒Ͳ is the Lagrange multiplier associated with the constraint, and ሺ׏ଶ݂ሺ࢞௞ሻ൅ߣࡵሻ is a positive-
definite matrix. The quality of the quadratic approximation is estimated by: ߛ௞ൌ
௙ሺ࢞ೖሻି௙ሺ࢞ೖశభሻ
௤ೖሺ࢞ೖሻି௤ೖሺ࢞ೖశభሻ. If this 
ratio is close to unity, the trust region may be expanded in the next iteration. 
The resulting search direction dk is a function of Lagrange multiplier ߣࢊ௞ൌࢊ௞ሺߣሻ Thus, for ߣൌͲ 
a sufficiently large ο௞ and for a positive-definite ׏ଶ݂ሺ࢞௞ሻࢊ௞ሺͲሻ reduces to the Newton’s direction. 
Whereas, for ο௞ൌͲߣ՜ λDQGࢊ௞ሺߣሻ and aligns with the steepest-descent direction. 
The trust-region algorithm is given as follows:
Trust-Region Algorithm (Griva, Nash & Sofer, p.392):
Initialize: Choose 
J

S

࢞଴ο଴VSHFLI\ߝǡ Ͳ ൏ߤ൏ߟ൏ͳHJߤൌ
ଵ
ସǢ ߟൌ
ଷ
ସ
For ݇ൌͲǡͳǡ ǥ
1.	 Check convergence: If ԡ׏݂ሺ࢞௞ሻԡ ൏ߝ stop
2.	 Solve 
ࢊݍ௞ሺࢊሻVXEMHFWWRԡࢊԡ ൑ο௞
3.	 Compute ߛ௞ 
a)	 if ߛ௞൏ߤVHW࢞௞ାଵൌ࢞௞ǡ ο௞ାଵൌ
ଵ
ଶο௞ 
b)	 else if ߛ௞൏ߟVHW࢞௞ାଵൌ࢞௞൅ࢊ௞ǡ ο௞ାଵൌο௞
c)	 else set ࢞௞ାଵൌ࢞௞൅ࢊ௞ǡ ο௞ାଵൌʹο௞
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
146 
NNumerica l Optimizati on Method
7.4	
Computer Methods for Solving the Constrained Problems
In this section, we describe the numerical methods devised for solving constrained nonlinear optimization 
problems. These methods fall into two broad categories: the first category includes penalty, barrier, and 
augmented Lagrangian methods that are an extension of the methods developed for unconstrained 
problems, and are collectively known as the transformation methods. The second category includes 
methods that iteratively approximate the nonlinear problem as a series of LP or QP problems and use 
the LP or QP methods to solve it.
In order to discuss these methods, we consider a general optimization problem described as:

࢞݂ሺ࢞ሻ
Subject to ቐ
݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݌Ǣ
݃௝ሺ࢞ሻ൑Ͳǡ ݆ൌ݅ǡ ǥ ǡ ݉Ǣ
ݔ௜௅൑ݔ௜൑ݔ௜௎ǡ ݅ൌͳǡ ǥ ǡ ݊Ǥ

(7.33)
Prominent computer methods for solving constrained optimization problems are described in this and 
the following section.
7.4.1	
 Penalty and Barrier Methods
The Penalty and Barrier methods are extensions of the numerical methods developed for solving 
unconstrained optimization problems. Both methods employ a composite of objective and constraint 
functions where the constraints are assigned a high violation penalty. Once a composite function has been 
defined for a set of penalty parameters, it can be minimized using any of the unconstrained optimization 
techniques. The penalty parameters can then be adjusted in successive iterations. 
The Penalty and Barrier methods fall under sequential unconstrained minimization techniques (SUMTs). 
Because of their simplicity, SUMTs have been extensively developed and used in engineering design 
problems. The SUMTs generally employ a composite function of the following form (Arora, p. 477): 
Ȱሺ࢞ǡ ݎሻൌ݂ሺ࢞ሻ൅ܲሺ݃ሺ࢞ሻǡ ݄ሺ࢞ሻǡ ࢘ሻ
(7.34)
where ݃ሺ࢞ሻDQG݄ሺ࢞ሻ are, respectively, the inequality and equality constraints, and r is a vector of penalty 
parameters. Depending on their region of iteration, these methods are further divided into Penalty or 
Barrier methods as described below: 
Penalty Function Method. A penalty function method that iterates through the infeasible region of 
space, employs a quadratic loss function of the following form:
ܲሺ݃ሺ࢞ሻǡ ݄ሺ࢞ሻǡ ࢘ሻൌݎ൬෍൫݃௜
ାሺ࢞ሻ൯
ଶ
௜
൅෍൫݄௜ሺ࢞ሻ൯
ଶ
௜
൰Ǣ݃௜
ାሺ࢞ሻൌ൫Ͳǡ ݃௜ሺ࢞ሻ൯ǡ ݎ൐Ͳ (7.35)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
147 
NNumerica l Optimizati on Method
Barrier Function Method. A barrier method that iterates through the feasible region of space, and is 
only applicable to inequality constrained problems, employs a log barrier function of the following form: 
ܲሺ݃ሺ࢞ሻǡ ݄ሺ࢞ሻǡ ࢘ሻൌͳ
ݎ෍൫െ݃௜ሺݔሻ൯
௜

(7.36)
For both penalty and barrier methods, convergence implies that as ݎ՜ λ࢞ሺݎሻ՜ ࢞כZKHUH࢞ሺݎሻ 
minimizes Ȱሺ࢞ǡ ݎሻ To improve convergence, r may be replaced by a sequence ሼݎ௞ሽ We, however, note 
that since the Hessian of the unconstrained function becomes ill-conditioned for large r, both methods 
are ill-behaved near the constraint boundary.
7.4.2	
 The Augmented Lagrangian Method
As an alternative to the penalty and barrier methods described above, the augmented Lagrangian (AL) 
methods add a quadratic penalty term to the Lagrangian function that also includes multipliers for 
penalizing individual constraint violations. The resulting AL method is generally more effective than 
penalty and barrier methods, and is commonly employed to solve Finite Element Analysis problems.
The augmented Lagrangian method is introduced below using an equality constrained optimization 
problem where the problem is given as (Belegundu and Chandrupatla, p. 276):

࢞݂ሺ࢞ሻ
6XEMHFWWR݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈

(7.37)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
148 
NNumerica l Optimizati on Method
The augmented Lagrangian function for the problem is defined as: 
࣪ሺ࢞ǡ ࢜ǡ ݎሻൌ݂ሺ࢞ሻ൅෍ቆݒ௝݄௝ሺ࢞ሻ൅ͳ
ʹ ݎ݄௝
ଶሺ࢞ሻቇ
௝

(7.38)
In the above, ݒ௝ are the Lagrange multipliers and the additional term defines an exterior penalty function 
with r as the penalty parameter. The gradient and Hessian of the AL are computed as:
׏࣪ሺ࢞ǡ ࢜ǡ ݎሻൌ׏݂ሺ࢞ሻ൅෍ቀݒ௝൅ݎ݄௝ሺ࢞ሻቁ
௝
׏݄௝ሺ࢞ሻ
׏ଶ࣪ሺ࢞ǡ ࢜ǡ ݎሻൌ׏ଶ݂ሺ࢞ሻ൅෍൬ቀݒ௝൅ݎ݄௝ሺ࢞ሻቁ׏ଶ݄௝ሺ࢞ሻ൅ݎ׏݄௝
୘׏݄௝ሺ࢞ሻ൰
௝


(7.39)
While the Hessian of the Lagrangian may not be uniformly positive definite, a large enough value of r 
makes the Hessian of AL positive definite at x.
Next, since the AL is stationary at the optimum, then, paralleling the developments in the duality theory 
(Sec. 5.7), we can solve the above optimization problem via a min-max framework as follows: first, for 
a given r and v, we define a dual function via the following minimization problem:
߰ሺ࢜ሻൌ
࢞࣪ሺ࢞ǡ ࢜ǡ ݎሻൌ݂ሺ࢞ሻ൅෍൬ݒ௝݄௝ሺ࢞ሻ൅ͳ
ʹ ݎቀ݄௝ሺ࢞ሻቁ
ଶ
൰
௝

(7.40)
This step is then followed by a maximization problem defined as: 
࢜
߰ሺ࢜ሻ The derivative of the dual 
function is computed as: 
ௗట
ௗ௩ೕൌ݄௝ሺ࢞ሻ൅׏்߰ௗ࢞
ௗ௩ೕ where the latter term is zero, since ׏߰ൌ׏࣪ൌͲ 
Further, an expression for the Hessian is given as: 
ௗమట
ௗ௩೔ௗ௩ೕൌ׏݄௜
்ௗ࢞
ௗ௩ೕǡZKHUHWKH
ௗ࢞
ௗ௩ೕ where the term 
can be obtained by differentiating ׏߰ൌͲ which gives: ׏݄௝൅׏ଶ࣪൬
ௗ࢞
ௗ௩ೕ൰ൌͲRU׏ଶ࣪൬
ௗ࢞
ௗ௩ೕ൰ൌെ׏݄௝ 
Therefore, the Hessian is computed as:
݀ଶ߰
݀ݒ௜݀ݒ௝
ൌെ׏݄௜
்ሺ׏ଶ࣪ሻିଵ׏݄௝
(7.41)
The AL method proceeds as follows: we choose a suitable ሺݒሻ and solve the minimization problem in 
(7.40) to define ߰ሺݒሻ We then solve the maximization problem to find the solution that minimizes the 
AL. The latter step can be done using gradient-based methods. For example, the Newton update for the 
maximization problem is given as:
࢜௞ାଵൌ࢜௞െቆ݀ଶ߰
݀ݒ௜݀ݒ௝
ቇ
ିଵ
ࢎ
(7.42)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
149 
NNumerica l Optimizati on Method
For large r, the update may be approximated as: ݒ௝
௞ାଵൌݒ௝
௞൅ݎ௝݄௝ǡ ݆ൌͳǡ ǥ ǡ ݈ (Belegundu and 
Chandrupatla, p. 278). For inequality constrained problems, the AL may be defined as (Arora, p. 480):
࣪ሺ࢞ǡ ࢛ǡ ݎሻൌ݂ሺ࢞ሻ൅෍൞
ݑ௜݃௜ሺ࢞ሻ൅ͳ
ʹ ݎ݃௜
ଶሺ࢞ሻǡ݃௝൅ݑ௝
ݎ൒Ͳ
െͳ
ʹݎݑ௜
ଶǡ݃௝൅ݑ௝
ݎ൏Ͳ
௜

(7.43)
The AL algorithm is given below.
The Augmented Lagrangian Algorithm (Arora, p. 480)
Initialize: estimate ݔ଴ǡ ݑ଴൒Ͳǡ ݒ଴ǡ ݎ൐ͲǢFKRRVHߙ൐Ͳǡ ߚ൐ͳǡ ߳൐Ͳǡ ߢ൐Ͳǡ ܭൌλ 
For ݇ൌͳǡʹǡ ǥ
1.	 Solve ࢞௞ൌ
࢞࣪ሺ࢞ǡ ࢛ǡ ࢜ǡ ݎ௞ሻ
2.	 Evaluate ݄௜൫࢞௞൯ǡ ݅ൌͳǡ Ǥ Ǥ ǡ ݈Ǣ ݃௝൫࢞௞൯ǡ ݆ൌͳǡ ǥ ǡ ݉Ǣ
compute ܭഥൌ݉ܽݔቄȁ݄௜ȁǡ ݅ൌͳǡ ǥ ǡ ݈Ǣ  ቀ݃௝ǡ െ
௨ೕ
௥ೖቁǡ ݆ൌͳǡ ǥ ǡ ݉ቅ
3.	 Check termination: If ܭഥ൑ߢDQGฮ׏࣪൫࢞௞൯ฮ൑߳݉ܽݔ൛ͳǡ ฮ࢞௞ฮൟ quit
4.	 If ܭഥ൏ܭ (i.e., constraint violations have improved), set ܭൌܭഥ
Set ݒ௜
௞ାଵൌݒ௜
௞൅ݎ௞݄௜൫࢞௞൯Ǣ ݅ൌͳǡ ǥ ǡ ݈6HWݑ௝
௞ାଵൌݑ௝
௞൅ݎ௞݉ܽݔ൜݃௝൫࢞௞൯ǡ െ
௨ೕ
ೖ
௥ೖൠǢ ݆ൌͳǡ ǥ ǡ ݉
If ܭഥ൐
௄
ఈǡ(i.e., constraint violations did not improve by a factor ߙ ), set ݎ௞ାଵൌߚݎ௞
An example for the AL method is now presented.
Example 7.5: Design of cylindrical water tank (Belegundu and Chandrupatla, p. 278)
We consider the design of an open-top cylindrical water tank. We wish to maximize the volume of the 
tank for a given surface area ܣ଴ Let d be the diameter and h be the height; then, the optimization 
problem is formulated as:

ௗǡ௟݂ሺ݀ǡ ݈ሻൌߨ݀ଶ݈
Ͷ 
subject to ݄ǣ
గௗమ
ସ൅ߨ݈݀െܣ଴ൌͲ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
150 
NNumerica l Optimizati on Method
We drop the constant గ
ସǡ convert to a minimization problem, assume 
ସ஺బ
గൌͳǡ and redefine the problem 
as:

ௗǡ௟݂ҧሺ݀ǡ ݈ሻൌെ݀ଶ݈
subject to ݄ǣ݀ଶ൅Ͷ݈݀െͳ ൌͲ
A Lagrangian function for the problem is formulated as: ࣦሺ݀ǡ ݈ǡ ߣሻൌെ݀ଶ݈൅ߣሺ݀ଶ൅Ͷ݈݀െͳሻ
The FONC for the problem are: െʹ݈݀൅ʹߣሺ݀൅ʹ݈ሻൌͲǡ െ݀ଶ൅Ͷ݀ߣൌͲǡ ݀ଶ൅Ͷ݈݀െͳ ൌͲ
Using FONC, the optimal solution is given as: ݀כ ൌʹ݈כ ൌͶߣכ ൌ
ଵ
ξଷ
The Hessian at the optimum point is given as: ׏ଶࣦሺ݀כǡ ݈כǡ ߣכሻൌቂെʹߣ
െͶߣ
െͶߣ
Ͳ ቃ, It is evident that the 
Hessian is not positive definite. 
Next, the AL for the problem is formed as: ࣪ሺ݀ǡ ݈ǡ ߣǡ ݎሻൌെ݀ଶ݈൅ߣሺ݀ଶ൅Ͷ݈݀െͳሻ൅ͳ
ʹ ݎሺ݀ଶ൅Ͷ݈݀െͳሻଶ
The dual function is defined as: ߰ሺߣሻൌ
ௗǡ௟࣪ሺ݀ǡ ݈ǡ ߣǡ ݎሻ 
The dual optimization problem is then formulated as: 
ௗǡ௟߰ሺߣሻ 
A plot of ߰ሺߣሻYVߣ vs. shows a concave function with ߣכ ൌߣ௠௔௫ൌͲǤͳͶͶ 
The optimum values for the design variables are the same as above: ݀כ ൌʹ݈כ ൌͲǤͷ͹͹Ǥ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Fundamental Engineering Optimization 
Methods
151 
NNumerica l Optimizati on Method
7.5	
Sequential Linear Programming
The sequential linear programming (SLP) method aims to sequentially solve the nonlinear optimization 
problem as a series of linear programs. In particular, we employ the first order Taylor series expansion 
to iteratively develop and solve a new LP subprogram to solve the KKT conditions associated with the 
NP problem. SLP methods are generally not robust, and have been mostly replaced by SQP methods.
To develop the SLP method, let xk denote the current estimate of design variables and let d denote the 
change in variable; then, we express the first order expansion of the objective and constraint functions 
in the neighborhood of xk as: 
݂൫࢞௞൅ࢊ൯ൌ݂൫࢞௞൯൅׏݂൫࢞௞൯
்ࢊ
݃௜൫࢞௞൅ࢊ൯ൌ݃௜൫࢞௞൯൅׏݃௜൫࢞௞൯
்ࢊǡ ݅ൌͳǡ ǥ ǡ ݉
݄௝൫࢞௞൅ࢊ൯ൌ݄௝൫࢞௞൯൅׏݄௝൫࢞௞൯
்ࢊǡ ݆ൌͳǡ ǥ ǡ ݈

(7.44)
To proceed further, let: ݂௞ൌ݂൫࢞௞൯ǡ ݃௜
௞ൌ݃௜൫࢞௞൯ǡ݄௝
௞ൌ݄௝൫࢞௞൯ and define: ܾ௜ൌെ݃௜
௞ǡ ݁௝ൌെ݄௝
௞ǡ  
൫
൯
൫
൯
௝
௝൫
൯
ࢉൌ׏݂൫࢞௞൯ǡࢇ௜ൌ׏݃௜൫࢞௞൯ǡ ࢔௝ൌ׏݄௝൫࢞௞൯ǡ ࡭ൌሾࢇଵǡ ࢇଶǡ ǥ ǡ ࢇ௠ሿǡ ࡺൌሾ࢔ଵǡ ࢔ଶǡ ǥ ǡ ࢔௟ሿ Then, after 
dropping the constant term ݂௞ from the objective function, we define the following LP subprogram for 
the current iteration of the NP problem (Arora, p. 498):

ࢊ݂ҧ ൌࢉ்ࢊ
Subject to: ࡭்ࢊ൑࢈ǡ ࡺ்ࢊൌࢋ
(7.45)
where ݂ҧ represents the linearized change in the original cost function and the columns of A and N 
represent, respectively, the gradients of inequality and equality constraints. Since the objective and 
constraint functions are now linear, the resulting LP subproblem can be converted to standard form and 
solved via the Simplex method. Problems with a small number of variables can also be solved graphically 
or by application of KKT conditions to the LP problem. 
The following points regarding the SLP method should be noted:
1.	 Since both positive and negative changes to design variables xk are allowed, the variables ݀௜ are 
unrestricted in sign and, therefore, must be replaced by ݀௜ൌ݀௜
ାെ݀௜
ି in the Simplex algorithm.
2.	 In order to apply the simplex method to the problem, the rhs parameters ܾ௜ǡ ݁௝ are assumed 
non-negative, or else, the respective constraint must be multiplied with െͳ 
3.	 SLP methods require additional constraints of the form, െο௜௟
௞൑݀௜
௞൑ο௜௨
௞ termed as move 
limits, to bind the LP solution. These move limits represent the maximum allowed change in 
݀௜ in the current iteration. They are generally selected as a percentage (1–100%) of the design 
variable values. They serve dual purpose of binding the LP solution and obviating the need 
for line search in the current iteration. Restrictive move limits tend to make the SLP problem 
infeasible.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
152 
NNumerica l Optimizati on Method
The SLP algorithm is presented below:
SLP Algorithm (Arora, p. 508): 
Initialize: choose ࢞଴ǡ ߝଵ൐Ͳǡ ߝଶ൐Ͳ
For ݇ൌͲǡͳǡʹǡ ǥ
1.	 Choose move limits ο௜௟
௞ǡ ο௜௨
௞ as some fraction of current design xk
2.	 Compute ݂௞ǡ ࢉǡ ݃௜
௞ǡ ݄௝
௞ǡ ܾ௜ǡ ݁௝
3.	 Formulate and solve the LP subproblem for dk
4.	 If and ݃௜൑ߝଵǢ ݅ൌͳǡ ǥ ǡ ݉Ǣห݄௝ห൑ߝଵǢ ݅ൌͳǡ ǥ ǡ ݌ǢDQGฮࢊ௞ฮ൑ߝଶ stop
5.	 Substitute ࢞௞ାଵ՚ ࢞௞൅ߙࢊ௞ǡ݇՚ ݇൅ͳ
The SLP algorithm is simple to apply, but should be used with caution in engineering design problems 
as it can easily run into convergence problems. The selection of move limits is one of trial and error and 
can be best achieved in an interactive mode. 
An example is presented to explain the SLP method:
Example 7.6: Sequential Linear Programming
We perform one iteration of the SLP algorithm for the following NLP problem:

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌݔଵ
ଶെݔଵݔଶ൅ݔଶ
ଶ
Subject to: ͳ െݔଵ
ଶെݔଶ
ଶ൑ͲǢെݔଵ൑Ͳǡ െݔଶ൑Ͳ
The NLP problem is convex and has a single minimum at ࢞כ ൌቀ
ଵ
ξଶǡ
ଵ
ξଶቁǤ The objective and constraint 
gradients are: 
ξ
ξ
׏்݂ൌሾʹݔଵെݔଶǡ ʹݔଶെݔଵሿǡ ׏݃ଵ
்ൌሾെʹݔଵǡ െʹݔଶሿǡ ׏݃ଶ
்ൌሾെͳǡͲሿǡ ׏݃ଷ
்ൌሾͲǡ െͳሿ 
Let ࢞଴ൌሺͳǡ ͳሻǡ so that ݂଴ൌͳǡࢉ்ൌሾͳͳሿ further, let ߝଵൌߝଶൌͲǤͲͲͳ then, using SLP method, 
the resulting LP problem at the current step is defined as: 

ௗభǡௗమ݂ሺݔଵǡ ݔଶሻൌ݀ଵ൅݀ଶ
Subject to: ൥
െʹ
െʹ
െͳ
Ͳ
Ͳ
െͳ
൩൤݀ଵ
݀ଶ൨൑൥
ͳ
ͳ
ͳ
൩
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
153 
NNumerica l Optimizati on Method
Since the LP problem is unbounded, we may use 50% move limits to bind the solution. The resulting 
update is given as: 
\
ࢊכ ൌቂെ
ଵ
ଶǡ െ
ଵ
ଶቃ
்
VRWKDW࢞ଵൌቂ
ଵ
ଶǡ
ଵ
ଶቃ
்
 with resulting constraint violations given 
as: ݃௜ൌቄ
ଵ
ଶǡ Ͳǡ Ͳቅ We note that smaller move limits in this step could have avoided resulting constraint 
violation. 
The SLP algorithm is not robust as move limits need to be imposed to force a solution. In the following, 
a sequential quadratic problem that obviates the need for move limits is formulated and solved. 
7.6	
Sequential Quadratic Programming 
The sequential quadratic programming (SQP) method improves on the SLP method by discarding the 
move limits in favor of more robust ways of binding the solution. Specifically, SQP adds ԡࢊԡǡZKHUHࢊ 
where represents the search direction, to the linear objective function (7.45) to define the resulting QP 
subproblem as follows (Arora, p. 514):

ࢊ݂ҧ ൌࢉ்ࢊ൅ͳ
ʹ ࢊ்ࢊ
Subject to, ࡭்ࢊ൑࢈ǡ ࡺ்ࢊൌࢋ
(7.46)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planet’s 
electricity needs. Already today, SKF’s innovative know-
how is crucial to running a large proportion of the 
world’s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Fundamental Engineering Optimization 
Methods
154 
NNumerica l Optimizati on Method
Since the QP subproblem represents a convex programming problem, a unique global minimum, if one 
exists, can be obtained. We further make the following observations regarding the QP problem:
1.	 From a geometric perspective, ݂ҧ represents the equation of a hypersphere with its center 
Ȃ ࢉǡ at and the search direction d points to the center of the hypersphere. 
2.	 When there are no active constraints, application of FONC: 
డ௙ҧ
డࢊൌࢉ൅ࢊൌͲ results in the 
search direction: ࢊൌȂ ࢉ which conforms to the steepest descent direction. 
3.	 When constraints are present, the QP solution amounts to projecting the steepest-descent 
direction onto the constraint hyperplane; the resulting search direction is termed as 
constrained steepest-descent (CSD) direction.
The QP subproblem can be analytically solved via the Lagrangian function approach. To do that, we add 
a slack variable ࢙ to the inequality constraint, and construct a Lagrangian function given as: 
ࣦሺࢊǡ ࢛ǡ ࢜ሻൌࢉ்ࢊ൅ଵ
ଶࢊ்ࢊ൅்࢛ሺ࡭்ࢊെ࢈൅࢙ሻ൅்࢜ሺࡺ்ࢊെࢋሻ
(7.47)
Then, the KKT conditions for a minimum are:

સࣦൌࢉ൅ࢊ൅࡭࢛൅ࡺ࢜ൌ૙ǡ
࡭்ࢊ൅࢙ൌ࢈ǡ
ࡺ்ࢊൌࢋǡ
்࢛࢙ൌ૙ǡ ࢛൒૙ǡ ࢙൒૙ (7.48)
Further, by writing ࢜ൌ࢟െࢠǡ ࢟൒૙ǡ ࢠ൒૙ǡ these conditions are expressed in matrix form as:
൥
ࡵ
࡭
࡭்
૙
ࡺ்
૙

૙
ࡵ
૙

ࡺ
െࡺ
૙
૙
૙
૙
൩
ۏ
ێ
ێ
ێ
ۍࢊ
࢛
࢙
࢟
ࢠے
ۑ
ۑ
ۑ
ې
ൌቈ
െࢉ
࢈
ࢋ
቉ǡRUࡼࢄൌࡽ
(7.49)
where the complementary slackness conditions, ்࢛࢙ൌ૙ǡ translate as: ࢄ௜ࢄ௜ା௠ൌͲǡ ݅ൌ݊൅ͳǡ ڮ ǡ ݊൅݉ 
We note that solution to the above problem can be obtained via LCP framework (Sec. 5.7.1).
Once a search direction d has been determined, a step-size along d needs to be computed by solving 
the line search problem. We next discuss the descent function approach that is used to resolve the line 
search step in the SQP solution process.
7.6.1	
 Descent Function Approach
In SQP methods, the line search solution is based on minimization of a descent function that penalizes 
constraint violations. The following descent function has been proposed in literature (Arora, p. 521):
Ȱሺ࢞ሻൌ݂ሺ࢞ሻ൅ܴܸሺ࢞ሻ
(7.50)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
155 
NNumerica l Optimizati on Method
where ݂ሺ࢞ሻ represents the cost function value, ܸሺ࢞ሻ represents the maximum constraint violation, and 
ܴ൐Ͳ is a penalty parameter. 
The descent function value at the current iteration is expressed as: 
Ȱ௞ൌ݂௞൅ܴܸ௞ܴൌሼܴ௞ǡ ݎ௞ሽ
(7.51)
where ܴ௞ is the current value of the penalty parameter, ݎ௞ is the current sum of the Lagrange multipliers, 
and ܸ௞ is the maximum constraint violation in the current step. The latter parameters are computed as: 
ݎ௞ൌσ
ݑ௜
௞
௠
௜ୀଵ
൅σ
หݒ௝
௞ห
௣
௝ୀଵ

ܸ௞ൌሼͲǢ݃௜ǡ ݅ൌͳǡ Ǥ Ǥ Ǥ ǡ ݉Ǣห݄௝หǡ ݆ൌͳǡ ǥ ǡ ݌ሽ
(7.52)
where absolute values of the Lagrange multipliers and constraint violations for equality constraints are 
used. Next, the line search subproblem is defined as:

ఈȰሺߙሻൌȰ൫࢞௞൅ߙࢊ௞൯
(7.53)
The above problem may be solved via the line search methods described in Sec. 7.2. 
An algorithm for solving the SQP problem is presented below:
SQP Algorithm (Arora, p. 526): 
Initialize: choose ࢞଴ǡ ܴ଴ൌͳǡ ߝଵ൐Ͳǡ ߝଶ൐Ͳ
For ݇ൌͲǡͳǡʹǡ ǥ
1.	 Compute ݂௞ǡ ݃௜
௞ǡ ݄௝
௞ǡ ࢉǡ ܾ௜ǡ ݁௝FRPSXWHܸ௞
1.	 Formulate and solve the QP subproblem to obtain dk and the Lagrange multipliers 
࢛௞DQG࢜௞
2.	 If ܸ௞൑ߝଵDQGฮࢊ௞ฮ൑ߝଶVWRS
3.	 Compute ܴ formulate and solve line search subproblem to obtain ߙ
4.	 Set ࢞௞ାଵ՚ ࢞௞൅ߙࢊ௞ǡ ܴ௞ାଵ՚ ܴǡ݇՚ ݇൅ͳ
It can be shown that the above algorithm is convergent, i.e., Ȱ൫࢞௞൯൑Ȱሺ࢞଴ሻDQGWKDW࢞௞ converges to 
the KKT point in the case of general constrained optimization problems (Arora, p. 525). 
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
156 
NNumerica l Optimizati on Method
7.6.2	
 SQP with Approximate Line Search 
The above SQP algorithm can be used with approximate line search methods, similar to Arjimo’s rule 
(Sec. 7.2.2) as follows: let ݐ௝ǡ ݆ൌͲǡͳǡ ǥ denote a trial step size, ࢞௞ାଵǡ௝ denote the trial design point, 
݂௞ାଵǡ௝ൌ݂ሺ࢞௞ାଵǡ௝ሻ denote the function value at the trial solution, and Ȱ௞ାଵǡ௝ൌ݂௞ାଵǡ௝൅ܴܸ௞ାଵǡ௝ 
denote the penalty function at the trial solution. The trial solution is required to satisfy the following 
descent condition: 
Ȱ௞ାଵǡ௝൅ݐ௝ߛฮࢊ௞ฮ
ଶ൑Ȱ௞ǡ௝ǡ
Ͳ ൏ߛ൏ͳ
(7.54)
where a common choice for γ is: ߛൌ
ଵ
ଶ Further, ݐ௝ൌߤ௝ǡ ߤൌ
ଵ
ଶǡ ݆ൌͲǡͳǡʹǡ ǥ The above descent 
condition ensures that the constraint violation decreases at each step of the method. The following 
example illustrates the application of approximate line search algorithm.
Example 7.7: Sequential Quadratic Programming with Approximate Line Search
We consider the above NL problem, given as:

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌݔଵ
ଶെݔଵݔଶ൅ݔଶ
ଶ
VXEMHFWWR݃ଵǣͳ െݔଵ
ଶെݔଶ
ଶ൑Ͳǡ ݃ଶǣെݔଵ൑Ͳǡ ݃ଷǣെݔଶ൑Ͳ
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
157 
NNumerica l Optimizati on Method
where the gradient functions are computed as: ׏்݂ൌሾʹݔଵെݔଶǡ ʹݔଶെݔଵሿǡ ׏݃ଵ
்ൌሾെʹݔଵǡ െʹݔଶሿǡ
J
׏݃ଶ
்ൌሾെͳǡͲሿǡ ׏݃ଷ
்ൌሾͲǡ െͳሿ
Let ݔ଴ൌሺͳǡ ͳሻ then, ݂଴ൌͳǡ ࢉൌሾͳǡ ͳሿ்ǡ ݃ଵሺͳǡͳሻൌ݃ଶሺͳǡͳሻൌ݃ଷሺͳǡͳሻൌെͳ Since, at this point, 
there are no active constraints, ܸ଴ൌͲ the preferred search direction is: ࢊൌെࢉൌሾെͳǡ െͳሿ் the line 
search problem is defined as: 
ఈȰሺߙሻൌ݂ሺ࢞଴൅ߙࢊ଴ሻൌሺͳ െߙሻଶ This problem can be analytically 
solved by setting ȰᇱሺߙሻൌͲ with the solution: ߙൌͳ resulting in ݔଵൌሺͲǡ Ͳሻ however, this analytical 
solution results in a large constraint violation that is undesired.
Use of the approximate line search method for the problem results in the following computations: 
Let ݐ଴ൌͳǡ ܴ଴ൌͳͲǡ ߛൌߤൌ
ଵ
ଶWKHQ࢞ଵǡ଴ൌሺͲǡͲሻǡ ԡࢊ଴ԡଶൌʹǡ ݂ଵǡ଴ൌͲǡ ܸଵǡ଴ൌͳǡ Ȱଵǡ଴ൌͳͲǡ and 
the descent condition Ȱଵǡ଴൅
ଵ
ଶԡࢊ଴ԡଶ൑Ȱ଴ൌͳ is not met. We then try ݐଵൌ
ଵ
ଶWRREWDLQ࢞ଵǡଵൌቀ
ଵ
ଶǡ
ଵ
ଶቁǡ  
ଵǡଵൌ
ଵ
ଶǡ Ȱଵǡଵൌͷ
ଵ
ସand the descent condition fails again; next, for ݐଶൌ
ଵ
ସZHJHW࢞ଵǡଶൌቀ
ଷ
ସǡ
ଷ
ସቁǡ
ଵǡଶൌͲǡ Ȱଵǡଶൌ
ଽ
ଵ଺ and the descent condition checks as: Ȱଵǡଶ൅
ଵ
଼ԡࢊ଴ԡଶ൑Ȱ଴ Therefore, we set: 
ߙൌݐଶൌ
ଵ
ସǡ ࢞ଵൌ࢞ଵǡଶൌቀ
ଷ
ସǡ
ଷ
ସቁwith no constraint violation. 
Next, we discuss some modifications to the SQP method that aid in solution to the QP subproblem.
7.6.3	
 The Active Set Strategy 
The computational cost of solving the QP subproblem can be substantially reduced by only including the 
active constraints in the subproblem. Accordingly, if the current design point ࢞௞א ȳǡ where Ω denotes 
the feasible region, then, for some small ߝ൐Ͳǡ the set ࣣ௞ൌ൛݅ǣ݃௜
௞൐െߝǢ ݅ൌͳǡ ǥ ǡ ݉ൟڂሼ݆ǣ݆ൌͳǡ ǥ ǡ ݌ሽ 
denotes the set of potentially active constraints. In the event ࢞௞ב ȳǡ let the current maximum constraint 
violation be given as: ܸ௞ൌሼͲǢ݃௜
௞ǡ ݅ൌͳǡ Ǥ Ǥ Ǥ ǡ ݉Ǣห݄௝
௞หǡ ݆ൌͳǡ ǥ ǡ ݌ሽ then, the active constraint set 
includes: ࣣ௞ൌ൛݅ǣ݃௜
௞൐ܸ௞െߝǢ ݅ൌͳǡ ǥ ǡ ݉ൟڂ൛݆ǣห݄௝
௞ห൐ܸ௞െߝǢ ݆ൌͳǡ ǥ ǡ ݌ൟ 
We may note that an inequality constraint at the current design point can be characterized in the 
following ways: as active LI݃௜
௞ൌͲ as ߝDFWLYHLI݃௜
௞൐െߝ as violated ݂݅݃௜
௞൐Ͳ or as inactive 
LI݃௜
௞൑െߝ whereas, an equality constraint is either active ݄௝
௞ൌͲ or violated ݄௝
௞്Ͳ 
The gradients of constraints not in ࣣ௞ do not need to be computed, however, the numerical algorithm 
using the potential constraint strategy must be proved to be convergent. Further, from a practical point 
of view, it is desirable to normalize all constraints with respect to their limit values, so that a uniform ߝ 
value can be used to check for a constraint condition at the design point.
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
158 
NNumerica l Optimizati on Method
Using the active set strategy, the active inequality constraints being known, they can be treated as equality 
constraints. We, therefore, assume that only equality constraints are present in the active set, and define 
the QP subproblem as:

ࢊ݂ҧ ൌࢉ்ࢊ൅ͳ
ʹ ࢊ்ࢊ
6XEMHFWWRࡺഥ்ࢊൌࢋത

(7.55)
Then, using the Lagrangian function approach, the optimality conditions are given as: ࡺഥ࢜൅ࢉ൅ࢊൌ૙ǡ 
ࡺഥ்ࢊെࢋതൌ૙ They can be simultaneously solved to eliminate the Lagrange multipliers as follows: from 
the optimality conditions we solve for ࢊDVࢊൌെࢉെࡺഥ࢜ǡ and substitute it in the constraint equation 
to get: ࡺഥ்ࡺഥ࢜ൌെࡺഥ்ሺࢉ൅ࢊሻ Next, we substitute ࢜ back in the optimality condition to get: 
ࢊൌെሾࡵെࡺഥሺࡺഥ்ࡺഥሻିଵࡺഥ்ሿࢉ൅ࡺഥሺࡺഥ்ࡺഥሻିଵࢋ
(7.56)
or, more compactly as: ࢊൌࢊଵ൅ࢊଶ where ࢊଵ in the above expression defines a matrix operator: P =
ࡵെࡺഥሺࡺഥ்ࡺഥሻିଵࡺഥ்ǡ ࡼࡼൌࡼǡ that projects the gradient of the cost function onto the tangent hyperplane 
defined by: ሼࢊǣࡺഥ்ࢊൌͲሽǡ which can also be obtained as a solution to the following minimization 
problem: 
ࢊԡࢉെࢊԡଶ subject to ࡺഥ்ࢊൌ૙ (Belegundu and Chandrupatla, p. 243). 
The second part of d defines a vector that points toward the feasible region. Further, these two components 
are orthogonal, i.e., ࢊଵ
்ࢊଶൌͲǤ Thus, we may interpret d as a combination of a cost reduction step ࢊଵ and 
a constraint correction step ࢊଶǤ Further, if there are no constraint violations, i.e., ࢋതൌ૙ǡWKHQࢊଶൌ૙ǡ 
and d aligns with the projected steepest descent direction. 
7.6.4	
 SQP Update via Newton’s Update
We observe that, from a computational point of view, Newton’s method can be used to solve the SQP 
subproblem. In order to derive the SQP update via Newton’s method, we consider the following design 
optimization problem involving equality constraints (Arora, p. 554): 

࢞݂ሺ࢞ሻ
6XEMHFWWR݄௜ሺ࢞ሻൌͲǡ ݅ൌͳǡ ǥ ǡ ݈

(7.57)
The Lagrangian function for the problem is constructed as: 
ࣦሺ࢞ǡ ࢜ሻൌ݂ሺ࢞ሻ൅்࢜ࢎሺ࢞ሻ
(7.58)
The KKT conditions for a minimum are given as: 
׏ࣦሺ࢞ǡ ࢜ሻൌ׏݂ሺ࢞ሻ൅ࡺ࢜ൌ૙ǡ ࢎሺ࢞ሻൌ૙
(7.59)
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
159 
NNumerica l Optimizati on Method
where ࡺൌ׏ࢎ்ሺ࢞ሻ is the Jacobian matrix whose ith columns represents the gradient ׏݄௜ Next, Newton’s 
method is employed to compute the change in the design variables and Lagrange multipliers as follows: 
using first order Taylor series expansion for ׏ࣦ௞ାଵDQGࢎ௞ାଵ we obtain:
൤׏ଶࣦ
ࡺ
ࡺ்
Ͳ൨
௞
ቂο࢞
ο࢜ቃ
௞
ൌെቂ׏ࣦ
ࢎቃ
௞

(7.60)
The first equation above may be expanded as: ׏ଶࣦο࢞௞൅ࡺ൫࢜௞ାଵെ࢜௞൯ൌെ൫׏݂௞ሺ࢞ሻ൅ࡺ࢜௞൯ and 
simplified as: ׏ଶࣦο࢞௞൅ࡺ࢜௞ାଵൌെ׏݂௞ሺ࢞ሻ resulting in the following Newton-Raphson iteration:
൤׏ଶࣦ
ࡺ
ࡺ்
Ͳ൨
௞
൤ο࢞௞
ο࢜௞ାଵ൨ൌെቂ׏݂
ࢎቃ
௞

(7.61)
It is interesting to note that the above result can also be obtained via a QP problem defined in terms of 
incremental variables where the QP problem is defined as follows:

ο࢞૚
૛ο்࢞׏ଶࣦο࢞൅׏்݂ο࢞
6XEMHFWWR݄௜ሺ࢞ሻ൅݊௜
୘ο࢞ൌͲǡ ݅ൌͳǡ ǥ ǡ ݈

(7.62)
The Lagrangian function for the problem is formulated as:
ࣦሺο࢞ǡ ࢜ሻൌͳ
ʹ ο்࢞׏ଶࣦο࢞൅׏்݂ο࢞൅்࢜ሺࢎ൅ࡺο࢞ሻ
(7.63)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Fundamental Engineering Optimization 
Methods
160 
NNumerica l Optimizati on Method
The resulting KKT conditions for an optimum are given as: ׏݂൅׏ଶࣦο࢞൅ࡺ࢜ൌ૙ǡ ࢎ൅ࡺο࢞ൌ૙ In 
matrix form, these KKT conditions are similar to those used in the Newton-Raphson update.
7.6.5	
 SQP with Hessian Update 
The above Newton’s implementation of SQP algorithm uses Hessian of the Lagrangian function for the 
update. Since Hessian computation is relatively costly, an approximate to the Hessian may instead be 
used. Towards that end, let ࡴൌ׏ଶࣦ then the modified QP subproblem is defined as (Arora, p. 557):

ࢊ݂ҧ ൌࢉ்ࢊ൅ͳ
ʹ ࢊ்ࡴࢊ
6XEMHFWWR࡭்ࢊ൑࢈ǡ ࡺ்ࢊൌࢋ

(7.64)
We note that quasi-Newton methods (Sec. 7.3.4) solve the unconstrained minimization problem by solving 
a set of linear equations given as: ࡴ௞ࢊ௞ൌെࢉ௞IRUࢊ௞ZKHUHࡴ௞ where represents an approximation to 
the Hessian matrix. In particular, the popular BFGS method uses the following Hessian update: 
ࡴ௞ାଵൌࡴ௞൅ࡰ௞൅ࡱ௞
(7.65)
where ࡰ௞ൌ
࢟ೖ࢟ೖ೅
࢟ೖ೅࢙ೖǡ ࡱ௞ൌ
ࢉೖࢉೖ೅
ࢉೖ೅ࢊೖǡ ࢙௞ൌߙ௞ࢊ௞ǡ ࢟௞ൌࢉ௞ାଵെࢉ௞ǡ ࢉ௞ൌ׏݂൫࢞௞൯ 
Next, the BFGS Hessian update is modified to apply to the constrained optimization problems as 
follows: let ࢙௞ൌߙ௞ࢊ௞ǡ ࢠ௞ൌࡴ௞࢙௞ǡ ࢟௞ൌ׏ࣦ൫࢞௞ାଵ൯െ׏ࣦ൫࢞௞൯ǡ ࢙௞்࢟௞ൌߦଵǡ ࢙௞்ࢠ௞ൌߦଶ further, 
define: ࢝௞ൌߠ࢟௞൅ሺͳ െߠሻࢠ௞ where ߠൌ ቄͳǡ
଴Ǥ଼కమ
కమିకభቅ࢙௞்࢝௞ൌߦଷǢ then, the Hessian update is given 
as: ࡴ௞ାଵൌࡴ௞൅ࡰ௞െࡱ௞ǡ ࡰ௞ൌ
ଵ
కయ࢟௞࢟௞்ǡ ࡱ௞ൌ
ଵ
కమࢠ௞ࢠ௞்
The modified SQP algorithm is given as follows:
Modified SQP Algorithm (Arora, p. 558): 
Initialize: choose ࢞଴ǡ ܴ଴ൌͳǡ ࡴ଴ൌܫǢߝଵǡ ߝଶ൐Ͳ
For ݇ൌͲǡͳǡʹǡ ǥ
1.	 Compute ݂௞ǡ ݃௜
௞ǡ ݄௝
௞ǡ ࢉǡ ܾ௜ǡ ݁௝ǡDQGܸ௞,I݇൐Ͳǡ compute Hk
2.	 Formulate and solve the modified QP subproblem for search direction dk and the Lagrange 
multipliers ࢛௞DQG࢜௞
3.	 If ܸ௞൑ߝଵDQGฮࢊ௞ฮ൑ߝଶ stop.
4.	 Compute ܴ formulate and solve line search subproblem to obtain α
5.	 Set ࢞௞ାଵ՚ ࢞௞൅ߙࢊ௞ǡ ܴ௞ାଵ՚ ܴǡ݇՚ ݇൅ͳ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
161 
NNumerica l Optimizati on Method
An example for SQP with Hessian update is presented below.
Example 7.8: SQP with Hessian Update
As an example, we consider the above NL problem, given as: 

௫భǡ௫మ݂ሺݔଵǡ ݔଶሻൌݔଵ
ଶെݔଵݔଶ൅ݔଶ
ଶ
VXEMHFWWR݃ଵǣ ͳ െݔଵ
ଶെݔଶ
ଶ൑ͲǢ݃ଶǣ െݔଵ൑Ͳǡ ݃ଷǣ െݔଶ൑Ͳ
The objective and constraint gradients for the problem are obtained as: 
׏்݂ൌሾʹݔଵെݔଶǡ ʹݔଶെݔଵሿǡ ׏݃ଵ
்ൌሾെʹݔଵǡ െʹݔଶሿǡ ׏݃ଶ
்ൌሾെͳǡͲሿǡ ׏݃ଷ
்ൌሾͲǡ െͳሿ
To proceed, OHWݔ଴ൌሺͳǡͳሻǡ so that, ݂଴ൌͳǡ ݃ଵሺͳǡͳሻൌ݃ଶሺͳǡͳሻൌ݃ଷሺͳǡͳሻൌെͳ since all constraints 
are initially inactive, the preferred search direction is: ࢊൌെࢉൌሾെͳǡ െͳሿ்Ǣ then, using approximate 
line search we obtain: ߙൌ
ଵ
ସǡOHDGLQJWR࢞ଵൌቀ
ଷ
ସǡ
ଷ
ସቁ
For the Hessian update, we have: ݂ଵൌͲǤͷ͸ʹͷǡ ݃ଵൌെͲǤͳʹͷǡ ݃ଶൌ݃ଷൌെͲǤ͹ͷǢࢉଵൌሾͲǤ͹ͷǡ ͲǤ͹ͷሿ and, for 
ߙൌͲǤʹͷǡ࢙଴ൌሾെͲǤʹͷǡ െͲǤʹͷሿൌࢠ଴ൌ࢟଴ǡ ߦଵൌߦଶൌͲǤͳʹͷǡ ߠൌͳǡ ࢝଴ൌ࢟଴ǡ ߦଷൌߦଵ therefore, 
Hessian update is computed as: ࡰ଴ൌͺ ቂͳ
ͳ
ͳ
ͳቃǡ ࡱ଴ൌͺ ቂͳ
ͳ
ͳ
ͳቃǡ ࡴଵൌࡴ଴Ǥ
For the next step, the QP problem is defined as:

ௗభǡௗమ݂ҧ ൌ
ଷ
ସሺ݀ଵ൅݀ଶሻ൅
ଵ
ଶሺ݀ଵ
ଶ൅݀ଶ
ଶሻ
6XEMHFWWRെ
ଷ
ଶሺ݀ଵ൅݀ଶሻ൑Ͳǡ െ݀ଵ൑Ͳǡ െ݀ଶ൑Ͳ
Using a Lagrangian function approach, the solution is found from application of KKT conditions, which 
results in the following systems of equations: ࡼ࢞ൌࢗǡZKHUH்࢞ൌሾ݀ଵǡ ݀ଶǡ ݑଵǡ ݑଶǡ ݑଷǡ ݏଵǡ ݏଶǡ ݏଷሿ and, 
ࡼൌ
ۏ
ێ
ێ
ێ
ۍ

ͳ
Ͳ
Ͳ
ͳ
െͳǤͷ
െͳǤͷ
െͳ
Ͳ
Ͳ
െͳ

െͳǤͷ
െͳ
Ͳ
െͳǤͷ
Ͳ
െͳ
Ͳ
Ͳ
Ͳ
Ͳ
Ͳ
Ͳ
Ͳ
Ͳ
Ͳ

Ͳ
Ͳ
Ͳ
Ͳ
Ͳ
Ͳ
ͳ
Ͳ
Ͳ
Ͳ
ͳ
Ͳ
Ͳ
Ͳ
ͳے
ۑ
ۑ
ۑ
ې
ǡࢗൌ
ۏ
ێ
ێ
ێ
ۍെͲǤ͹ͷ
െͲǤ͹ͷ
ͲǤͳʹͷ
ͲǤ͹ͷ
ͲǤ͹ͷ ے
ۑ
ۑ
ۑ
ې

The complementary slackness conditions are given as: ݑ௜ݏ௜ൌͲ݅ൌͳǡʹǡ͵ The solution found from the 
simplex method is given as: ்࢞ൌሾͲǤͳͺͺǡ ͲǤͳͺͺǡ Ͳǡ Ͳǡ ͲǡͲǤͳʹͷǡ ͲǤ͹ͷǡ ͲǤ͹ͷሿ We note that in this case as 
the number of variables is small, taking the complementarity conditions into account, there are eight basic 
solutions, only one of which is feasible and is given as: ࢄ்ൌሾͲǤͳͺͺǡ ͲǤͳͺͺǡ Ͳǡ Ͳǡ ͲǡͲǤͳʹͷǡ ͲǤ͹ͷǡ ͲǤ͹ͷሿ
Download free eBooks at bookboon.com

Fundamental Engineering Optimization 
Methods
162 
References
References
Arora, JS 2004, Introduction to Optimum Design, 2nd edn, Elsevier Academic Press, San Diego, CA.
Belegundu, AD and Chandrupatla TR 2012, Optimization Concepts and Applications in Engineering, 2nd 
edn (reprinted), Cambridge University Press, New York.
Boyd, S & Vandenberghe, L 2004, Convex Optimization, Cambridge University Press, New York.
Chong, EKP & Zak, SH 2013, An Introduction to Optimization, 4th edn. John Wiley & Sons, New Jersey.
Eisenbrand, F, Course notes for linear and discrete optimization, 
https://class.coursera.org/linearopt-001/lecture/index
Ferris, MC, Mangasarian, OL & Wright, SJ 2007, Linear Programming with Matlab, SIAM, Philadelphia, PA
Ganguli, R 2012, Engineering Optimiztion A Modern Approach, Universities Press, Hyderabad (India).
Griva, I, Nash, SG & Sofer, A 2009, Linear and Nonlinear Optimization, 2nd edn, SIAM, Philadelphia, PA.
Hager, WW & Zhang, H-C 2006, ‘A survey of nonlinear conjugate gradient methods’, Pacific Journal of 
Optimization, vol. 2, pp. 35–58.
Hemmecke, R, Lecture notes on discrete optimization, 
https://www-m9.ma.tum.de/foswiki/pub/SS2011/DiscOpt/DiscOpt.pdf
Kelly, CT 1995, Iterative Methods for Linear and Nonlinear Equations, SIAM, Philadelphia, PA.
Luenberger, DG &Ye, Y 2008, Linear and Nonlinear Programming, 3rd edn, Springer, New York.
Pedregal, P 2004, Introduction to Optimization, Springer-Verlag, New York.
Sierksma, G 2002, Linear and Integer Programming: Theory and Practice, 2nd edn, Marcel Dekker, 
Monticello, NY.
Vanderbei, RJ 2007, Linear Programming: Foundations and Extensions, 3rd edn, Springer, New York.
Yang, X-S 2010, Engineering Optimization, John Wiley & Sons, New Jersey.
Download free eBooks at bookboon.com


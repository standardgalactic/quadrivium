Lecture Notes in Computer Science
2793
Edited by G. Goos, J. Hartmanis, and J. van Leeuwen

3
Berlin
Heidelberg
New York
Hong Kong
London
Milan
Paris
Tokyo

Roland Backhouse Jeremy Gibbons (Eds.)
Generic
Programming
Advanced Lectures
1 3

Series Editors
Gerhard Goos, Karlsruhe University, Germany
Juris Hartmanis, Cornell University, NY, USA
Jan van Leeuwen, Utrecht University, The Netherlands
Volume Editors
Roland Backhouse
The University of Nottingham
School of Computer Science and Information Technology
Jubilee Campus, Wollaton Road, Nottingham NG8 1BB, UK
E-mail: rcb@cs.nott.ac.uk
Jeremy Gibbons
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford OX1 3QD, UK
E-mail: Jeremy.Gibbons@comlab.ox.ac.uk
Cataloging-in-Publication Data applied for
A catalog record for this book is available from the Library of Congress.
Bibliographic information published by Die Deutsche Bibliothek
Die Deutsche Bibliothek lists this publication in the Deutsche Nationalbibliograﬁe;
detailed bibliographic data is available in the Internet at <http://dnb.ddb.de>.
CR Subject Classiﬁcation (1998): D.3, D.1, D.2, F.3, E.1
ISSN 0302-9743
ISBN 3-540-20194-7 Springer-Verlag Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer-Verlag. Violations are
liable for prosecution under the German Copyright Law.
Springer-Verlag Berlin Heidelberg New York
a member of BertelsmannSpringer Science+Business Media GmbH
http://www.springer.de
© Springer-Verlag Berlin Heidelberg 2003
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Olgun Computergraﬁk
Printed on acid-free paper
SPIN: 10931868
06/3142
5 4 3 2 1 0

Preface
Generic programming is about making programming more eﬀective by making
it more general. This volume is about a novel form of genericity in programs,
based on parameterizing programs by the structure of the data they manipulate.
The material is based on lectures presented at a summer school on Generic
Programming held at the University of Oxford in August 2002.
The lectures by Hinze and Jeuring introduced Generic Haskell, an extension
of the Haskell programming language that allows the programmer to deﬁne a
function by induction on the structure of types. The implementation of Generic
Haskell provided a valuable tool for students to experiment with applications of
this form of datatype genericity. The lecture material in this volume is divided
into two parts. The ﬁrst part (“practice and theory”) introduces Generic Haskell
and the theory that underlies its design. The second part (“applications”) dis-
cusses three advanced applications of Generic Haskell in some depth.
The value of generic programming is illusory unless the nature and extent of
the genericity can be described clearly and precisely. The lectures by Backhouse
and Crole delve deeper into the theoretical basis for datatype genericity. Back-
house reviews the notion of parametric polymorphism (a notion well known to
functional programmers) and then shows how this notion is extended to higher-
order notions of parametricity. These are used to characterize what it means
for a value to be stored in a datatype. Also, transformations on data structures
are given precise speciﬁcations in this way. Underlying this account are certain
basic notions of category theory and allegory theory. Crole presents the category
theory needed for a deeper understanding of mechanisms for deﬁning datatypes.
The ﬁnal chapter, by Fiadeiro, Lopes and Wermelinger applies the mathemat-
ical “technology” of parameterization to the larger-scale architectural structure
of programs. The description of a system is split into components and their in-
teractions; architectural connectors are parameterized by components, leading
to an overall system structure consisting of components and connector instances
establishing the interactions between the components.
Our thanks go to all those involved in making the school a success. We are
grateful to the technical support staﬀof the Oxford University Computing Lab-
oratory for providing computing facilities, to Yorck Hunke, David Lacey and
Silvija Seres of OUCL for assistance during the school, and to St. Anne’s Col-
lege for an amenable environment for study. Thanks also go to Peter Buneman
and Martin Odersky, who lectured at the school on Semi-structured Data and
on Object-Oriented and Functional Approaches to Compositional Programming,
respectively, but were unable to contribute to the proceedings.
June, 2003
Roland Backhouse
Jeremy Gibbons

Contributors
Roland Backhouse
School of Computer Science and Information Technology,
University of Nottingham, Nottingham, NG8 1BB, UK
rcb@cs.nott.ac.uk
http://www.cs.nott.ac.uk/˜rcb/
Roy Crole
Department of Mathematics and Computer Science,
University of Leicester, University Road, Leicester, LE1 7RH, UK
roy.crole@mcs.le.ac.uk
http://www.mcs.le.ac.uk/˜rcrole/
Jos´e Luiz Fiadeiro
Department of Computer Science, University of Leicester, University Road,
Leicester, LE1 7RH, UK
jose@fiadeiro.org
Ralf Hinze
Institut f¨ur Informatik III, Universit¨at Bonn, R¨omerstraße 164,
53117 Bonn, Germany
ralf@informatik.uni-bonn.de
http://www.informatik.uni-bonn.de/˜ralf/
Paul Hoogendijk
Philips Research, Prof. Holstlaan 4, 5655 AA Eindhoven, The Netherlands
Paul.Hoogendijk@philips.com
Johan Jeuring
Institute of Information and Computing Sciences, Utrecht University, P.O.
Box 80.089, 3508 TB Utrecht, The Netherlands
and
Open University, Heerlen, The Netherlands
johanj@cs.uu.nl
http://www.cs.uu.nl/˜johanj/
Ant´onia Lopes
Department of Informatics, Faculty of Sciences, University of Lisbon,
Campo Grande, 1749-016 Lisboa, Portugal
mal@di.fc.ul.pt
Michel Wermelinger
Dept. of Informatics, Faculty of Sciences and Technology, New University
of Lisbon, Quinta da Torre, 2829-516 Caparica, Portugal
mw@di.fct.unl.pt

Table of Contents
Chapter 1. Generic Haskell: Practice and Theory . . . . . . . . . . . . . . . . . . . . . . .
1
R. Hinze and J. Jeuring
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Type Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Haskell’s data Construct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Towards Generic Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Towards Generic Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.5
Stocktaking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.6
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2
Generic Haskell—Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.1
Mapping Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.2
Kind-Indexed Types and Type-Indexed Values . . . . . . . . . . . . . . . . .
25
2.3
Embedding-Projection Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4
Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.5
Pretty Printing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.6
Running Generic Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3
Generic Haskell—Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.1
The Simply Typed Lambda Calculus as a Type Language . . . . . . .
35
3.2
The Polymorphic Lambda Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.3
Specializing Type-Indexed Values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
3.4
Bridging the Gap. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
Chapter 2. Generic Haskell: Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
R. Hinze and J. Jeuring
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2
Generic Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
2.2
Signature. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.3
Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
2.4
Type-Indexed Tries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
2.5
Empty Tries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
2.6
Singleton Tries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.7
Look up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
2.8
Inserting and Merging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
2.9
Deleting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
2.10 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3
XComprez: A Generic XML Compressor . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.1
Implementing an XML Compressor as a Generic Program . . . . . . .
78
3.2
Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.3
Conclusions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84

VIII
Table of Contents
4
The Zipper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.1
The Basic Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.2
Data Types as Fixed Points of Pattern Functors . . . . . . . . . . . . . . . .
86
4.3
Type Indices of Higher Kinds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.4
Locations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
4.5
Navigation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Chapter 3. Generic Properties of Datatypes . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
R. Backhouse and P. Hoogendijk
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
2
Theorems for Free . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
2.1
Veriﬁable Genericity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3
Commuting Datatypes — Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
3.1
Structure Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
3.2
Broadcasts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
4
Allegories and Relators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.1
Allegories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
4.2
Relators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
4.3
Composition and Relators Are Parametric . . . . . . . . . . . . . . . . . . . . . 112
4.4
Division and Tabulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
4.5
Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5
Datatype = Relator + Membership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5.1
Pointwise Closure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
5.2
Regular Relators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.3
Natural Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
5.4
Membership and Fans. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
6
Commuting Datatypes — Formal Speciﬁcation . . . . . . . . . . . . . . . . . . . . . 125
6.1
Naturality Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
6.2
Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
6.3
Half Zips and Commuting Relators . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7
Consequences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
7.1
Shape Preservation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
7.2
All Regular Datatypes Commute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
8
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Chapter 4. Basic Category Theory for Models of Syntax . . . . . . . . . . . . . . . . 133
R.L. Crole
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
1.1
Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
1.2
The Aims . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
1.3
Learning Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
2
Syntax Deﬁned from Datatypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
2.1
An Example with Distinguished Variables and without Binding . . 135
2.2
An Example with Distinguished Variables and Binding . . . . . . . . . . 136
2.3
An Example with Arbitrary Variables and Binding . . . . . . . . . . . . . 137
2.4
An Example without Variables but with Binding . . . . . . . . . . . . . . . 138

Table of Contents
IX
3
Category Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
3.1
Categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
3.2
Functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
3.3
Natural Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
3.4
Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
3.5
Coproducts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
3.6
Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
3.7
The Functor 1 + (−): Set −→Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
3.8
The Functor A + (−): Set −→Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
3.9
The Functor 1 + (A × −): Set →Set . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4
Models of Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.1
A Model of Syntax with Distinguished Variables
and without Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
4.2
A Model of Syntax with Distinguished Variables
and with Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
4.3
A Model of Syntax with Arbitrary Variables and Binding . . . . . . . . 165
4.4
A Model of Syntax without Variables but with Binding . . . . . . . . . 168
4.5
Where to Now? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.1
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.2
Abstract Syntax Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.3
Inductively Deﬁned Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
5.4
Rule Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
5.5
Recursively Deﬁned Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
Chapter 5. A Mathematical Semantics for Architectural Connectors . . . . . . 178
J.L. Fiadeiro, A. Lopes and M. Wermelinger
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
2
System Conﬁguration in CommUnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
2.1
Component Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
2.2
Conﬁgurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
3
Architectural Description in CommUnity. . . . . . . . . . . . . . . . . . . . . . . . . . . 191
3.1
Architectural Connectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
3.2
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
4
An ADL-Independent Notion of Connector . . . . . . . . . . . . . . . . . . . . . . . . . 201
4.1
Architectural Schools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
4.2
Adding Abstraction to Architectural Connectors . . . . . . . . . . . . . . . 207
5
Towards an Algebra of Connectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
5.1
Role Reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
5.2
Role Encapsulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
5.3
Role Overlay. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
6
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

Generic Haskell: Practice and Theory
Ralf Hinze1 and Johan Jeuring2,3
1 Institut f¨ur Informatik III, Universit¨at Bonn
R¨omerstraße 164, 53117 Bonn, Germany
ralf@informatik.uni-bonn.de
http://www.informatik.uni-bonn.de/~ralf/
2 Institute of Information and Computing Sciences, Utrecht University
P.O.Box 80.089, 3508 TB Utrecht, The Netherlands
johanj@cs.uu.nl
http://www.cs.uu.nl/~johanj/
3 Open University, Heerlen, The Netherlands
Abstract. Generic Haskell is an extension of Haskell that supports the
construction of generic programs. These lecture notes describe the basic
constructs of Generic Haskell and highlight the underlying theory.
Generic programming aims at making programming more eﬀective by making
it more general. Generic programs often embody non-traditional kinds of poly-
morphism. Generic Haskell is an extension of Haskell [38] that supports the
construction of generic programs. Generic Haskell adds to Haskell the notion of
structural polymorphism, the ability to deﬁne a function (or a type) by induction
on the structure of types. Such a function is generic in the sense that it works not
only for a speciﬁc type but for a whole class of types. Typical examples include
equality, parsing and pretty printing, serialising, ordering, hashing, and so on.
The lecture notes on Generic Haskell are organized into two parts. This ﬁrst
part motivates the need for genericity, describes the basic constructs of Generic
Haskell, puts Generic Haskell into perspective, and highlights the underlying
theory. The second part entitled “Generic Haskell: applications” delves deeper
into the language discussing three non-trivial applications of Generic Haskell:
generic dictionaries, compressing XML documents, and a generic version of the
zipper data type.
The ﬁrst part is organized as follows. Section 1 provides some background
discussing type systems in general and the type system of Haskell in particular.
Furthermore, it motivates the basic constructs of Generic Haskell. Section 2
takes a closer look at generic deﬁnitions and shows how to deﬁne some popular
generic functions. Section 3 highlights the theory underlying Generic Haskell and
discusses its implementation. Section 4 concludes.
1
Introduction
This section motivates and introduces the basic constructs of Generic Haskell.
We start by looking at type systems.
R. Backhouse and J. Gibbons (Eds.): Generic Programming SS 2002, LNCS 2793, pp. 1–56, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

2
R. Hinze and J. Jeuring
A basic knowledge of Haskell is desirable, as all the examples are given either
in Haskell or in Generic Haskell.
1.1
Type Systems
Safe Languages. Most programmers probably agree that language safety is a
good thing. Language safety is quite a colorful term meaning diﬀerent things
to diﬀerent people. Here are a few deﬁnitions taken from Pierce’s excellent text
book “Types and Programming Languages” [40].
– A safe language is one that makes it impossible to shoot yourself in the foot
while programming.
– A safe language is one that protects its own abstractions.
– A safe language is one that that prevents untrapped errors at run time.
– A safe language is completely deﬁned by its programmer’s manual.
The deﬁnitions put emphasis on diﬀerent aspects of language safety. Quite
clearly, all of these are desirable properties of a programming language.
Now, language safety can be achieved by static type checking, by dynamic
type checking, or—and this is the most common case—by a combination of static
and dynamic checks. The language Haskell serves as an example of the latter
approach: passing an integer to a list-processing function is captured statically at
compile time while taking the ﬁrst element of the empty list results in a run-time
error.
Static and Dynamic Typing. It is widely accepted that static type systems
are indispensable for building large and reliable software systems. The most cited
beneﬁts of static typing include:
– Programming errors are detected at an early stage.
– Type systems enforce disciplined programming.
– Types promote abstraction (abstract data types, module systems).
– Types provide machine-checkable documentation.
However, type systems are always conservative: they must necessarily reject
programs that behave well at run time.
In a sense, generic programming is about extending the boundaries of static
type systems. This is the reason why these lecture notes have little to oﬀer for
addicts of dynamically typed languages. As we will see, most generic programs
can be readily implemented in a dynamically checked language. (Conceptually,
a dynamic language oﬀers one universal data type; programming a function that
works for a class of data types is consequently a non-issue.)
Polymorphic Type Systems. Polymorphism complements type security by
ﬂexibility. Polymorphic type systems like the Hindley-Milner system [33] allow

Generic Haskell: Practice and Theory
3
the deﬁnition of functions that behave uniformly over all types. A standard
example is the length function that computes the length of a list.
data List a
= Nil | Cons a (List a)
length
:: ∀a . List a →Int
length Nil
= 0
length (Cons a as) = 1 + length as
The ﬁrst line declares the list data type, which is parametric in the type of list
elements. The function length happens to be insensitive to the element type.
This is signalled by the universal quantiﬁer in length’s type signature (read:
List a →Int is a valid type of length for all types a). Though this is not Haskell 98
syntax, we will write polymorphic types always using explicit qualiﬁers. Most
readers probably know the universal quantiﬁer from predicate logic. Indeed,
there is a close correspondence between polymorphic type systems and systems
of higher-order logic, see [47]. In light of this correspondence we note that the
quantiﬁer in length’s type signature is second-order as it ranges over sets (if we
naively equate types with sets).
However, even polymorphic type systems are sometimes less ﬂexible than
one would wish. For instance, it is not possible to deﬁne a polymorphic equality
function that works for all types.
eq :: ∀a . a →a →Bool
-- does not work
The parametricity theorem [46] implies that a function of type ∀a . a →a →Bool
must necessarily be constant. As a consequence, the programmer is forced to
program a separate equality function for each type from scratch. This sounds
like a simple task but may, in fact, be arbitrarily involved. To illustrate some
of the diﬃculties we will go through a series of instances of equality (and other
generic functions). First, however, let us take a closer look at Haskell’s type
system, especially at the data construct.
1.2
Haskell’s data Construct
Haskell oﬀers one basic construct for deﬁning new types: a so-called data type
declaration. In general, a data declaration has the following form:
data B a1 . . . am = K1 t11 . . . t1m1 | · · · | Kn tn1 . . . tnmn.
This deﬁnition simultaneously introduces a new type constructor B and n data
or value constructors K1, . . . , Kn, whose types are given by
Kj :: ∀a1 . . . am . tj1 →· · · →tjmj →B a1 . . . am.
The type parameters a1, . . . , am must be distinct and may appear on the right-
hand side of the declaration. If m > 0, then B is called a parameterized type.
Data type declarations can be recursive, that is, B may also appear on the right-
hand side. In general, data types are deﬁned by a system of mutually recursive

4
R. Hinze and J. Jeuring
data type declarations. A Haskell data type is essentially a sum of products: the
components of the sum are labelled by constructor names; the arguments of a
constructor form a product.
The following sections provide several examples of data type declarations
organized in increasing order of diﬃculty.
Finite Types. Data type declarations subsume enumerated types. In this spe-
cial case, we only have nullary data constructors, that is, m1 = · · · = mn = 0.
The following declaration deﬁnes a simple enumerated type, the type of truth
values.
data Bool = False | True
Data type declarations also subsume record types. In this case, we have only
one value constructor, that is, n = 1.
data Fork a = Fork a a
An element of Fork a is a pair whose two components both have type a. This
example illustrates that we can use the same name for a type and for a data
constructor. In these notes we distinguish the two by using diﬀerent fonts: data
constructors are set in Roman and type constructors in Sans Serif.
Haskell assigns a kind to each type constructor. One can think of a kind as
the ‘type’ of a type constructor. The type constructor Fork deﬁned above has
kind ⋆→⋆. The ‘⋆’ kind represents nullary constructors like Char, Int or Bool.
The kind κ →ν represents type constructors that map type constructors of kind
κ to those of kind ν. Note that the term ‘type’ is sometimes reserved for nullary
type constructors.
The following type can be used to represent ‘optional values’.
data Maybe a = Nothing | Just a
An element of type Maybe a is an ‘optional a’: it is either of the form Nothing
or of the form Just a where a is of type a. The type constructor Maybe has kind
⋆→⋆.
Recursive Types. Data type declarations may be recursive or even mutually
recursive. A simple recursive data type is the type of natural numbers.
data Nat = Zero | Succ Nat
The number 6, for instance, is given by
Succ (Succ (Succ (Succ (Succ (Succ Zero))))).
Strings can also be represented by a recursive data type.
data String = NilS | ConsS Char String

Generic Haskell: Practice and Theory
5
The type String is a binary sum. The ﬁrst summand, Nil, is a nullary product
and the second summand, Cons, is a binary product. Here is an example element
of String:
ConsS ’F’ (ConsS ’l’ (ConsS ’o’ (ConsS ’r’
(ConsS ’i’ (ConsS ’a’ (ConsS ’n’ NilS)))))).
The most popular data type is without doubt the type of parametric lists; it
is obtained from String by abstracting over Char.
data List a = Nil | Cons a (List a)
The empty list is denoted Nil; Cons a x denotes the list whose ﬁrst element is
a and whose remaining elements are those of x. The list of the ﬁrst six prime
numbers, for instance, is given by
Cons 2 (Cons 3 (Cons 5 (Cons 7 (Cons 11 (Cons 13 Nil))))).
In Haskell, lists are predeﬁned with special syntax: List a is written [a], Nil is
replaced by [ ], and Cons a x by a :x. We will use both notations simultaneously.
The following deﬁnition introduces external binary search trees.
data Tree a b = Tip a | Node (Tree a b) b (Tree a b)
We distinguish between external nodes of the form Tip a and internal nodes of
the form Node l b r. The former are labelled with elements of type a while the
latter are labelled with elements of type b. Here is an example element of type
Tree Bool Int:
Node (Tip True) 7 (Node (Tip True) 9 (Tip False)).
The type Tree has kind ⋆→⋆→⋆. Perhaps surprisingly, binary type construc-
tors like Tree are curried in Haskell.
The following data type declaration captures multiway branching trees, also
known as rose trees [6].
data Rose a = Branch a (List (Rose a))
A node is labelled with an element of type a and has a list of subtrees. An
example element of type Rose Int is:
Branch 2 (Cons (Branch 3 Nil)
(Cons (Branch 5 Nil)
(Cons (Branch 7 (Cons (Branch 11 Nil)
(Cons (Branch 13 Nil) Nil))) Nil))).
The type Rose falls back on the type List. Instead, we may introduce Rose using
two mutually recursive data type declarations:
data Rose′ a = Branch′ a (Forest a)
data Forest a = NilF | ConsF (Rose′ a) (Forest a).
Now Rose′ depends on Forest and vice versa.

6
R. Hinze and J. Jeuring
The type parameters of a data type may range over type constructors of
arbitrary kinds. By contrast, Miranda (trademark of Research Software Ltd),
Standard ML, and previous versions of Haskell (1.2 and before) only have ﬁrst-
order kinded data types. The following generalization of rose trees, that abstracts
over the List data type, illustrates this feature.
data GRose f a = GBranch a (f (GRose f a))
A slight variant of this deﬁnition has been used by [37] to extend an implemen-
tation of priority queues with an eﬃcient merge operation. The type constructor
GRose has kind (⋆→⋆) →(⋆→⋆), that is, GRose has a so-called second-order
kind where the order of a kind is given by
order(⋆)
= 0
order(κ →ν) = max{1 + order(κ), order(ν)}.
Applying GRose to List yields the type of rose trees.
The following data type declaration introduces a ﬁxed point operator on the
level of types. This deﬁnition appears, for instance, in [32] where it is employed
to give a generic deﬁnition of so-called cata- and anamorphisms [30].
newtype Fix f
= In (f (Fix f))
data ListBase a b = NilL | ConsL a b
The kinds of these type constructors are Fix :: (⋆→⋆) →⋆and ListBase :: ⋆→
(⋆→⋆). Using Fix and ListBase the data type of parametric lists can alternatively
be deﬁned by
type List a = Fix (ListBase a).
Here is the list of the ﬁrst six prime numbers written as an element of type
Fix (ListBase Int):
In (ConsL 2 (In (ConsL 3 (In (ConsL 5
(In (ConsL 7 (In (ConsL 11 (In (ConsL 13 (In NilL))))))) ))))).
Nested Types. A regular or uniform data type is a recursive, parameterized
type whose deﬁnition does not involve a change of the type parameter(s). The
data types of the previous section are without exception regular types. This
section is concerned with non-regular or nested types [7]. Nested data types are
practically important since they can capture data-structural invariants in a way
that regular data types cannot. For instance, the following data type declaration
deﬁnes perfectly balanced, binary leaf trees [20]—perfect trees for short.
data Perfect a = ZeroP a | SuccP (Perfect (Fork a))
This equation can be seen as a bottom-up deﬁnition of perfect trees: a perfect
tree is either a singleton tree or a perfect tree that contains pairs of elements.
Here is a perfect tree of type Perfect Int:

Generic Haskell: Practice and Theory
7
SuccP (SuccP (SuccP (ZeroP (Fork (Fork (Fork 2 3)
(Fork 5 7))
(Fork (Fork 11 13)
(Fork 17 19)) )))).
Note that the height of the perfect tree is encoded in the preﬁx of SuccP and
ZeroP constructors.
The next data type provides an alternative to the ubiquitous list type if an
eﬃcient indexing operation is required: Okasaki’s binary random-access lists [37]
support logarithmic access to the elements of a list.
data Sequ a = EndS
| ZeroS (Sequ (Fork a))
| OneS a (Sequ (Fork a))
This deﬁnition captures the invariant that binary random-access lists are se-
quences of perfect trees stored in increasing order of height. Using this represen-
tation the sequence of the ﬁrst six prime numbers reads:
ZeroS (OneS (Fork 2 3) (OneS (Fork (Fork 5 7) (Fork 11 13)) EndS)).
The types Perfect and Sequ are examples of so-called linear nests: the param-
eters of the recursive calls do not themselves contain occurrences of the deﬁned
type. A non-linear nest is the following type taken from [7]:
data Bush a = NilB | ConsB a (Bush (Bush a)).
An element of type Bush a resembles an ordinary list except that the i-th element
has type Bushi a rather than a. Here is an example element of type Bush Int:
ConsB 1 (ConsB (ConsB 2 NilB)
(ConsB (ConsB (ConsB 3 NilB) NilB) NilB)).
Perhaps surprisingly, we will get to know a practical application of this data type
in the second part of these notes, which deals with so-called generalized tries.
Haskell’s data construct is surprisingly expressive. In fact, all primitive data
types such as characters or integers can, in principle, be deﬁned by a data dec-
laration. The only exceptions to this rule are the function space constructor and
Haskell’s IO data type. Now, the one-million-dollar question is, of course, how
can we deﬁne a function that works for all of these data types.
1.3
Towards Generic Programming
The basic idea of generic programming is to deﬁne a function such as equality by
induction on the structure of types. Thus, generic equality takes three arguments,
a type and two values of that type, and proceeds by case analysis on the type
argument. In other words, generic equality is a function that depends on a type.

8
R. Hinze and J. Jeuring
Deﬁning a function by induction on the structure of types sounds like a hard
nut to crack. We are trained to deﬁne functions by induction on the structure
of values. Types are used to guide this process, but we typically think of them
as separate entities. So, at ﬁrst sight, generic programming appears to add an
extra level of complication and abstraction to programming. However, we claim
that generic programming is in many cases actually simpler than conventional
programming. The fundamental reason is that genericity gives you ‘a lot of things
for free’—we will make this statement more precise in the course of these notes.
For the moment, let us support the claim by deﬁning two simple algorithms
both in a conventional and in a generic style (data compression and equality).
Of course, these are algorithms that make sense for a large class of data types.
Consequently, in the conventional style we have to provide an algorithm for each
instance of the class.
Towards Generic Data Compression. The ﬁrst problem we look at is to
encode elements of a given data type as bit streams implementing a simple form
of data compression [25]. For concreteness, we assume that bit streams are given
by the following data type (we use Haskell’s predeﬁned list data type here):
type Bin = [Bit]
data Bit = 0 | 1.
Thus, a bit stream is simply a list of bits. A real implementation might have a
more sophisticated representation for Bin but that is a separate matter.
We will implement binary encoders and decoders for three diﬀerent data
types. We consider these types in increasing level of diﬃculty. The ﬁrst exam-
ple type is String. Supposing that encodeChar :: Char →Bin is an encoder for
characters provided from somewhere, we can encode an element of type String
as follows:
encodeString
:: String →Bin
encodeString NilS
= 0 : [ ]
encodeString (ConsS c s) = 1 : encodeChar c ++ encodeString s.
We emit one bit to distinguish between the two constructors NilS and ConsS.
If the argument is a non-empty string of the form ConsS c s, we (recursively)
encode the components c and s and ﬁnally concatenate the resulting bit streams.
Given this scheme it is relatively simple to decode a bit stream produced
by encodeString. Again, we assume that a decoder for characters is provided
externally.
decodesString
:: Bin →(String, Bin)
decodesString [ ] = error "decodesString"
decodesString (0 : bin) = (NilS, bin)
decodesString (1 : bin) = let (c, bin1) = decodesChar bin
(s, bin2) = decodesString bin1
in (ConsS c s, bin2)

Generic Haskell: Practice and Theory
9
The decoder has type Bin →(String, Bin) rather than Bin →String to be able
to compose decoders in a modular fashion: decodesChar :: Bin →(Char, Bin),
for instance, consumes an initial part of the input bit stream and returns the
decoded character together with the rest of the input stream. Here are some
applications (we assume that characters are encoded in 8 bits).
encodeString (ConsS ’L’ (ConsS ’i’ (ConsS ’s’ (ConsS ’a’ NilS))))
=⇒1001100101100101101110011101100001100
decodesChar (tail 1001100101100101101110011101100001100)
=⇒(’L’, 1100101101110011101100001100)
decodesString 1001100101100101101110011101100001100
=⇒(ConsS ’L’ (ConsS ’i’ (ConsS ’s’ (ConsS ’a’ NilS))), [ ])
Note that a string of length n is encoded using n + 1 + 8 ∗n bits.
A string is a list of characters. We have seen that we obtain Haskell’s list
type by abstracting over the type of list elements. How can we encode a list of
something? We could insist that the elements of the input list have already been
encoded as bit streams. Then encodeListBin completes the task:
encodeListBin
:: List Bin →Bin
encodeListBin Nil
= 0 : [ ]
encodeListBin (Cons bin bins) = 1 : bin ++ encodeListBin bins.
For encoding the elements of a list the following function proves to be useful:
mapList
:: ∀a1 a2 . (a1 →a2) →(List a1 →List a2)
mapList mapa Nil
= Nil
mapList mapa (Cons a as) = Cons (mapa a) (mapList mapa as).
The function mapList is a so-called mapping function that applies a given func-
tion to each element of a given list (we will say a lot more about mapping
functions in these notes). Combining encodeListBin and mapList we can encode
a variety of lists:
encodeListBin (mapList encodeChar
(Cons ’A’ (Cons ’n’ (Cons ’j’ (Cons ’a’ Nil)))) )
=⇒1100000101011101101010101101100001100
encodeListBin (mapList encodeInt (Cons 47 (Cons 11 Nil)))
=⇒11111010000000000111010000000000000
(encodeListBin · mapList (encodeListBin · mapList encodeBool))
(Cons (Cons True (Cons False (Cons True Nil)))
(Cons (Cons False (Cons True (Cons False Nil)))
(Nil)))
=⇒11110110110111000.
Here, encodeInt and encodeBool are primitive encoders for integers and Boolean
values respectively (an integer occupies 16 bits whereas a Boolean value makes
do with one bit).

10
R. Hinze and J. Jeuring
How do we decode the bit streams thus produced? The ﬁrst bit tells whether
the original list was empty or not, but then we are stuck: we simply do not know
how many bits were spent on the ﬁrst list element. The only way out of this
dilemma is to use a decoder function, supplied as an additional argument, that
decodes the elements of the original list.
decodesList
:: ∀a . (Bin →(a, Bin)) →(Bin →(List a, Bin))
decodesList dea [ ]
= error "decodesList"
decodesList dea (0 : bin) = (Nil, bin)
decodesList dea (1 : bin) = let (a, bin1) = dea bin
(as, bin2) = decodesList dea bin1
in (Cons a as, bin2)
This deﬁnition generalizes decodesString deﬁned above; we have decodesString ∼=
decodesList decodesChar (corresponding to String ∼= List Char). In some sense,
the abstraction step that led from String to List is repeated here on the value
level. Of course, we can also generalize encodeString:
encodeList
:: ∀a . (a →Bin) →(List a →Bin)
encodeList ena Nil
= 0 : [ ]
encodeList ena (Cons a as) = 1 : ena a ++ encodeList ena as.
It is not hard to see that encodeList ena = encodeListBin · mapList ena. En-
coding and decoding lists is now fairly simple:
encodeList encodeChar (Cons ’A’ (Cons ’n’ (Cons ’j’ (Cons ’a’ Nil))))
=⇒1100000101011101101010101101100001100
encodeList encodeInt (Cons 47 (Cons 11 Nil))
=⇒11111010000000000111010000000000000
encodeList (encodeList encodeBool)
(Cons (Cons True (Cons False (Cons True Nil)))
(Cons (Cons False (Cons True (Cons False Nil)))
(Nil)))
=⇒11110110110111000.
The third data type we look at is Okasaki’s binary random-access list. Using
the recursion scheme of encodeList we can also program an encoder for binary
random-access lists.
encodeFork
:: ∀a . (a →Bin) →(Fork a →Bin)
encodeFork ena (Fork a1 a2) = ena a1 ++ ena a2
encodeSequ
:: ∀a . (a →Bin) →(Sequ a →Bin)
encodeSequ ena EndS
= 0 : [ ]
encodeSequ ena (ZeroS s)
= 1 : 0 : encodeSequ (encodeFork ena) s
encodeSequ ena (OneS a s) = 1 : 1 : ena a ++ encodeSequ (encodeFork ena) s
Consider the last equation which deals with arguments of the form OneS a s. We
emit two bits for the constructor and then (recursively) encode its components.

Generic Haskell: Practice and Theory
11
Since a has type a, we apply ena. Similarly, since s has type Sequ (Fork a),
we call encodeSequ (encodeFork ena). It is not hard to see that the type of
the component determines the function calls in a straightforward manner. As an
aside, note that encodeSequ requires a non-schematic form of recursion known as
polymorphic recursion [36]. The two recursive calls are at type (Fork a →Bin) →
(Sequ (Fork a) →Bin) which is a substitution instance of the declared type.
Functions operating on nested types are in general polymorphically recursive.
Haskell 98 allows polymorphic recursion only if an explicit type signature is
provided for the function. The rationale behind this restriction is that type
inference in the presence of polymorphic recursion is undecidable [17].
Though the Sequ data type is more complex than the list data type, encoding
binary random-access lists is not any more diﬃcult.
encodeSequ encodeChar (ZeroS (ZeroS (OneS
(Fork (Fork ’L’ ’i’) (Fork ’s’ ’a’)) EndS)))
=⇒101011001100101001011011001110100001100
encodeSequ encodeInt (ZeroS (OneS (Fork 47 11) EndS))
=⇒1011111101000000000011010000000000000
In general, a string of length n makes do with 2 ∗⌈lg (n + 1)⌉+ 1 + 8 ∗n bits.
Perhaps surprisingly, encoding a binary random-access list requires fewer bits
than encoding the corresponding list (if the list contains more than 8 elements).
To complete the picture here is the decoder for binary random-access lists.
decodesFork
:: ∀a . (Bin →(a, Bin)) →(Bin →(Fork a, Bin))
decodesFork dea bin
= let (a1, bin1) = dea bin
(a2, bin2) = dea bin1
in (Fork a1 a2, bin2)
decodesSequ
:: ∀a . (Bin →(a, Bin)) →(Bin →(Sequ a, Bin))
decodesSequ dea [ ]
= error "decodes"
decodesSequ dea (0 : bin) = (EndS, bin)
decodesSequ dea (1 : 0 : bin)
= let (s, bin′) = decodesSequ (decodesFork dea) bin
in (ZeroS s, bin′)
decodesSequ dea (1 : 1 : bin)
= let (a, bin1) = dea bin
(s, bin2) = decodesSequ (decodesFork dea) bin1
in (OneS a s, bin2)
Towards Generic Equality. As a second example, let us work towards imple-
menting a generic version of equality. Taking a look at several ad-hoc instances
of equality will improve our understanding when we consider the generic pro-
gramming extensions Generic Haskell oﬀers.
Let us start simple: here is equality of strings.

12
R. Hinze and J. Jeuring
eqString
:: String →String →Bool
eqString NilS NilS
= True
eqString NilS (ConsS c′ s′)
= False
eqString (ConsS c s) NilS
= False
eqString (ConsS c s) (ConsS c′ s′) = eqChar c c′ ∧eqString s s′
The function eqChar :: Char →Char →Bool is equality of characters. As usual,
we assume that this function is predeﬁned.
The type List is obtained from String by abstracting over Char. Likewise,
eqList is obtained from eqString by abstracting over eqChar.
eqList
:: ∀a . (a →a →Bool)
→(List a →List a →Bool)
eqList eqa Nil Nil
= True
eqList eqa Nil (Cons a′ x ′)
= False
eqList eqa (Cons a x) Nil
= False
eqList eqa (Cons a x) (Cons a′ x ′) = eqa a a′ ∧eqList eqa x x ′
Similarly, the type GRose of generalized rose trees is obtained from Rose by
abstracting over the list type constructor (which is of kind ⋆→⋆). Likewise,
eqGRose abstracts over list equality (which has a polymorphic type). Thus,
eqGRose takes a polymorphic function to a polymorphic function.
eqGRose :: ∀f . (∀a . (a →a →Bool) →(f a →f a →Bool))
→(∀a . (a →a →Bool)
→(GRose f a →GRose f a →Bool))
eqGRose eqf eqa (GBranch a f ) (GBranch a′ f ′)
= eqa a a′ ∧eqf (eqGRose eqf eqa) f f ′
The function eqGRose has a so-called rank-2 type. In general, the rank of a type
is given by
rank(C)
= 0
rank(∀a . t) = max{1, rank(t)}
rank(t →u) = max{inc (rank(t)), rank(u)},
where inc 0 = 0 and inc (n+1) = n+2. Most implementations of Haskell support
rank-2 types. The latest version of the Glasgow Haskell Compiler, GHC 5.04,
even supports general rank-n types.
As a ﬁnal example, consider deﬁning equality for the ﬁxed point operator on
the type level.
eqFix :: ∀f . (∀a . (a →a →Bool) →(f a →f a →Bool))
→(Fix f →Fix f →Bool)
eqFix eqf (In f ) (In f ′) = eqf (eqFix eqf ) f f ′
1.4
Towards Generic Haskell
In the previous section we have seen a bunch of ad-hoc instances of generic
functions. Looking at the type signatures of equality we see that the type of eqT

Generic Haskell: Practice and Theory
13
depends on the kind of T. Roughly speaking, the more complicated the kind of T,
the more complicated the type of eqT. To capture the type of generic functions,
Generic Haskell supports the deﬁnition of types that are deﬁned by induction
over the structure of kinds, so-called kind-indexed types.
Apart from the typings, it is crystal clear what the deﬁnition of eqT looks like.
We ﬁrst have to check whether the two arguments of equality are labelled by the
same constructor. If this is the case, then their arguments are recursively tested
for equality. Nonetheless, coding the equality function is boring and consequently
error-prone. Fortunately, Generic Haskell allows us to capture equality once and
for all.
To deﬁne generic equality and other generic functions it suﬃces to cover
three simple, non-recursive data types: binary sums, binary products and nullary
products (that is, the unit data type). Since these types are the building blocks of
data declarations, Generic Haskell is then able to generate instances of equality
for arbitrary user-deﬁned types. In other words, the generic equality function
works for all types of all kinds. Of course, if the user-deﬁned type falls back on
some primitive type, then the generic equality function must also supply code
for this type. Thus, generic equality will include cases for Char, Int, etc. On the
other hand, it will not include cases for function types or for the IO type since
we cannot decide equality of functions or IO actions.
We have already mentioned the slogan that generic programming gives the
programmer a lot of things for free. In our case, Generic Haskell automatically
takes care of type abstraction, type application and type recursion. And it does
so in a type-safe manner.
Kind-Indexed Types. The type of a generic function is captured by a kind-
indexed type which is deﬁned by induction on the structure of kinds. Here are
some examples.
type Encode{[⋆]} t
= t →Bin
type Encode{[κ →ν]} t = ∀a . Encode{[κ]} a →Encode{[ν]} (t a)
type Decodes{[⋆]} t
= Bin →(t, Bin)
type Decodes{[κ →ν]} t = ∀a . Decodes{[κ]} a →Decodes{[ν]} (t a)
type Eq{[⋆]} t
= t →t →Bool
type Eq{[κ →ν]} t
= ∀a . Eq{[κ]} a →Eq{[ν]} (t a)
The part enclosed in {[·]} is the kind index. In each case, the equation for kind ⋆
is the interesting one. For instance, t →t →Bool is the type of equality for
manifest types (nullary type constructors). Perhaps surprisingly, the equations
for function kinds always follow the same scheme, which we will encounter time
and again. We will see in Section 3.3 that this scheme is inevitable because of
the way type constructors of kind κ →ν are specialized.

14
R. Hinze and J. Jeuring
The type signatures we have seen in the previous section can be written more
succinctly using the kind-indexed types above.
encodeString :: Encode{[⋆]} String
encodeList
:: Encode{[⋆→⋆]} List
decodesString :: Decodes{[⋆]} String
decodesList
:: Decodes{[⋆→⋆]} List
eqString
:: Eq{[⋆]} String
eqList
:: Eq{[⋆→⋆]} List
eqGRose
:: Eq{[(⋆→⋆) →(⋆→⋆)]} GRose
eqFix
:: Eq{[(⋆→⋆) →⋆]} Fix
In general, the equality function for type t of kind κ has type Eq{[κ]} t.
Sums and Products. Recall that a Haskell data type is essentially a sum of
products. To cover data types the generic programmer only has to deﬁne the
generic function for binary sums and binary products (and nullary products).
To this end Generic Haskell provides the following data types.
data Unit
= Unit
data a :*: b = a :*: b
data a :+: b = Inl a | Inr b
Note that the operator ‘:*:’ is used both as a type constructor and as a data
constructor (pairing).
In a sense, the generic programmer views the data declaration
data List a = Nil | Cons a (List a)
as if it were given by the following type deﬁnition
type List a = Unit :+: a :*: List a,
which makes sums and products explicit (‘:*:’ binds more tightly than ‘:+:’).
The types Unit, ‘:*:’, and ‘:+:’ are isomorphic to the predeﬁned Haskell types
‘()’, ‘(, )’, and Either. The main reason for introducing new types is that it gives
the user the ability to provide special instances for ‘()’, ‘(, )’, and Either. As an
example, you may want to show elements of the pair data type in a special way
(by contrast, we have seen above that ‘:*:’ is used to represent the arguments of
a constructor).
Type-Indexed Values. Given these prerequisites, the deﬁnition of generic
functions is within reach. The generic programmer has to provide a type signa-
ture, which typically involves a kind-indexed type, and a set of equations, one
for each type constant, where a type constant is either a primitive type like Char,

Generic Haskell: Practice and Theory
15
Int, ‘→’ etc or one of the three types Unit, ‘:*:’, and ‘:+:’. As an example here is
the deﬁnition of the generic encoding function.
encode{|t :: κ|}
:: Encode{[κ]} t
encode{|Char|}
= encodeChar
encode{|Int|}
= encodeInt
encode{|Unit|} Unit
= [ ]
encode{|:+:|} ena enb (Inl a)
= 0 : ena a
encode{|:+:|} ena enb (Inr b) = 1 : enb b
encode{|:*:|} ena enb (a :*: b) = ena a ++ enb b
Generic functions are also called type-indexed values; the part enclosed in {|·|} is
the type index. Note that each equation is more or less inevitable. Characters and
integers are encoded using the primitive functions encodeChar and encodeInt.
To encode the single element of the unit type no bits are required. To encode an
element of a sum we emit one bit for the constructor followed by the encoding
of its argument. Finally, the encoding of a pair is given by the concatenation of
the component’s encodings. Since the types ‘:+:’ and ‘:*:’ have kind ⋆→⋆→⋆,
the generic instances take two additional arguments, ena and enb.
The deﬁnition of decode follows the same deﬁnitional pattern.
decodes{|t :: κ|}
:: Decodes{[κ]} t
decodes{|Char|}
= decodesChar
decodes{|Int|}
= decodesInt
decodes{|Unit|} bin
= (Unit, bin)
decodes{|:+:|} dea deb [ ]
= error "decodes"
decodes{|:+:|} dea deb (0 : bin) = let (a, bin′) = dea bin in (Inl a, bin′)
decodes{|:+:|} dea deb (1 : bin) = let (b, bin′) = deb bin in (Inr b, bin′)
decodes{|:*:|} dea deb bin
= let (a, bin1) = dea bin
(b, bin2) = deb bin1
in ((a :*: b), bin2)
Generic equality is equally straightforward.
eq{|t :: κ|}
:: Eq{[κ]} t
eq{|Char|}
= eqChar
eq{|Int|}
= eqInt
eq{|Unit|} Unit Unit
= True
eq{|:+:|} eqa eqb (Inl a) (Inl a′)
= eqa a a′
eq{|:+:|} eqa eqb (Inl a) (Inr b′)
= False
eq{|:+:|} eqa eqb (Inr b) (Inl a′)
= False
eq{|:+:|} eqa eqb (Inr b) (Inr b′)
= eqb b b′
eq{|:*:|} eqa eqb (a :*: b) (a′ :*: b′) = eqa a a′ ∧eqb b b′

16
R. Hinze and J. Jeuring
Generic Application. Given the deﬁnitions above we can encode and decode
elements of arbitrary user-deﬁned data types. The generic functions are invoked
by instantiating the type-index to a speciﬁc type, where the type can be any
closed type expression.
encode{|String|} (ConsS ’L’ (ConsS ’i’ (ConsS ’s’ (ConsS ’a’ NilS))))
=⇒1001100101100101101110011101100001100
decodes{|String|} 1001100101100101101110011101100001100
=⇒(ConsS ’L’ (ConsS ’i’ (ConsS ’s’ (ConsS ’a’ NilS))), [ ])
encode{|List Char|} (Cons ’A’ (Cons ’n’ (Cons ’j’ (Cons ’a’ Nil))))
=⇒1100000101011101101010101101100001100
encode{|List Int|} (Cons 47 (Cons 11 Nil))
=⇒11111010000000000111010000000000000
encode{|List (List Bool)|}
(Cons (Cons True (Cons False (Cons True Nil)))
(Cons (Cons False (Cons True (Cons False Nil)))
(Nil)))
=⇒11110110110111000.
In the examples above we call encode and decodes always for types of kind ⋆.
However, the generic functions are far more ﬂexible: we can call them at any
type of any kind. The following session illustrates a more general use (here with
generic equality).
eq{|List Char|} "hello" "Hello"
=⇒False
let sim c c′ = eqChar (toUpper c) (toUpper c′)
eq{|List|} sim "hello" "Hello"
=⇒True
If we instantiate the type index to a type constructor, then we have to pass an
‘equality function’ for the type argument as an additional parameter. Of course,
we can pass any function as long as it meets the typing requirements. In the
session above, we pass ‘an equality test’ that ignores case distinctions. Quite
clearly, this gives us an extra degree of ﬂexibility.
Generic Abstraction. Abstraction is at the heart of programming. Generic
Haskell also supports a simple form of type abstraction. Common usages of
generic functions can be captured using generic abstractions.
similar{|t :: ⋆→⋆|} :: t Char →t Char →Bool
similar{|t|}
= eq{|t|} sim
Note that similar is only applicable to type constructors of kind ⋆→⋆.

Generic Haskell: Practice and Theory
17
1.5
Stocktaking
A generic program is one that the programmer writes once, but which works
over many diﬀerent data types. Broadly speaking, generic programming aims at
relieving the programmer from repeatedly writing functions of similar function-
ality for diﬀerent user-deﬁned data types. Examples of generic functions include
equality, parsing and pretty printing, serialising, ordering, hashing, and so on.
A generic function such as a pretty printer or a parser is written once and for
all; its specialization to diﬀerent instances of data types happens without fur-
ther eﬀort from the user. This way generic programming greatly simpliﬁes the
construction and maintenance of software systems as it automatically adapts
functions to changes in the representation of data.
The basic idea of generic programming is to deﬁne a function such as equal-
ity by induction on the structure of types. Thus, generic equality takes three
kinds
types
values
arguments, a type and two values of that type, and proceeds
by case analysis on the type argument. In other words, generic
equality is a function that depends on a type. Consider the
structure of the language Haskell. If we ignore the module
system, Haskell has the three level structure depicted on the
right. The lowest level, that is, the level where computations
take place, consists of values. The second level, which imposes
structure on the value level, is inhabited by types. Finally, on
the third level, which imposes structure on the type level,
we have so-called kinds. Why is there a third level? We have
seen that Haskell allows the programmer to deﬁne parametric
types such as the popular data type of lists. The list type constructor can be
seen as a function on types and the kind system allows to specify this in a precise
way. Thus, a kind is simply the ‘type’ of a type constructor.
In ordinary programming we routinely deﬁne values that depend on values,
that is, functions and types that depend on types, that is, type constructors.
However, we can also imagine to have dependencies between adjacent levels. For
instance, a type might depend on a value or a type might depend on a kind. The
following table lists the possible combinations:
kinds depending on kinds parametric and kind-indexed kinds
kinds depending on types dependent kinds
types depending on kinds polymorphic and kind-indexed types
types depending on types parametric and type-indexed types
types depending on values dependent types
values depending on types polymorphic and type-indexed functions
values depending on values ordinary functions
.
If a higher level depends on a lower level we have so-called dependent types or
dependent kinds. Programming languages with dependent types are the subject
of intensive research, see, for instance, [4]. Dependent types will play little rˆole
in these notes as generic programming is concerned with the opposite direction,

18
R. Hinze and J. Jeuring
where a lower level depends on the same or a higher level. However, using de-
pendent types we can simulate generic programming, see Section 1.6. If a value
depends on a type we either have a polymorphic or a type-indexed function. In
both cases the function takes a type as an argument. What is the diﬀerence
between the two? Now, a polymorphic function stands for an algorithm that
happens to be insensitive to what type the values in some structure are. Take,
for example, the length function that calculates the length of a list. Since it
need not inspect the elements of a given list, it has type ∀a . List a →Int. By
contrast, a type-indexed function is deﬁned by induction on the structure of its
type argument. In some sense, the type argument guides the computation which
is performed on the value arguments.
A similar distinction applies to the type and to the kind level: a parametric
type does not inspect its type argument whereas a type-indexed type is deﬁned
by induction on the structure of its type argument and similarly for kinds. The
following table summarizes the interesting cases.
kinds deﬁned by induction on the structure of kinds kind-indexed kinds
types deﬁned by induction on the structure of kinds kind-indexed types
types deﬁned by induction on the structure of types type-indexed types
values deﬁned by induction on the structure of types type-indexed values
Amazingly, we will encounter examples of all sorts of parameterization in the
lecture notes (type-indexed types and kind-indexed kinds are covered in the
second part).
1.6
Related Work
This section puts Generic Haskell into a broader perspective discussing its lim-
itations, possible generalizations and variations and alternative approaches to
generic programming. To illustrate the underlying ideas we will use generic equal-
ity as a running example.
Generic Haskell. We have noted in the beginning of Section 1.1 that it is not
possible to deﬁne a polymorphic equality function that works uniformly for all
types.
eq :: ∀a . a →a →Bool
-- does not work
Consequently, Generic Haskell treats eq as a family of functions indexed by type.
Deviating from Generic Haskell’s syntax eq’s type could be written as follows.
eq :: {|a :: ⋆|} →a →a →Bool
A moment’s reﬂection reveals that this is really a dependent type: the second and
the third argument depend on the ﬁrst argument, which is a type. Of course,
since equality may be indexed by types of arbitrary kinds eq’s type signature is
slightly more complicated.
eq :: ∀κ . {|a :: κ|} →Eq{[κ]} a

Generic Haskell: Practice and Theory
19
The universal quantiﬁer, which ranges over kinds, makes explicit that eq works
for all kinds.
Though the construct {|a :: κ|} →t resembles a dependent type, type-indexed
functions are not ﬁrst-class citizens in Generic Haskell. For example, we can-
not deﬁne a higher-order generic function that takes type-indexed functions to
type-indexed functions. The reason for this restriction is that Generic Haskell
implements genericity by translating a type-indexed function into a family of
(higher-order) polymorphic functions, see Section 3.3.
Type Classes. Haskell’s major innovation is its support for overloading, based
on type classes. For example, the Haskell Prelude deﬁnes the class Eq (slightly
simpliﬁed):
class Eq a where
eq :: a →a →Bool
This class declaration deﬁnes an overloaded top-level function, called method,
whose type is
eq :: ∀a . (Eq a) ⇒a →a →Bool.
Before we can use eq on values of, say Int, we must explain how to take equality
over Int values:
instance Eq Int where
eq = eqInt.
This instance declaration makes Int an element of the type class Eq and says
‘the eq function at type Int is implemented by eqInt’. As a second example
consider equality of lists. Two lists are equal if they have the same length and
corresponding elements are equal. Hence, we require equality over the element
type:
instance (Eq a) ⇒Eq (List a) where
eq Nil Nil
= True
eq Nil (Cons a2 as2)
= False
eq (Cons a1 as1) Nil
= False
eq (Cons a1 as1) (Cons a2 as2) = eq a1 a2 ∧eq as1 as2.
This instance declaration says ‘if a is an instance of Eq, then List a is an instance
of Eq, as well’.
Though type classes bear a strong resemblance to generic deﬁnitions, they do
not support generic programming. A type class declaration corresponds roughly
to the type signature of a generic deﬁnition—or rather, to a collection of type
signatures. Instance declarations are related to the type cases of a generic def-
inition. The crucial diﬀerence is that a generic deﬁnition works for all types,
whereas instance declarations must be provided explicitly by the programmer
for each newly deﬁned data type. There is, however, one exception to this rule.
For a handful of built-in classes Haskell provides special support, the so-called
‘deriving’ mechanism. For instance, if you deﬁne
data List a = Nil | Cons a (List a) deriving (Eq)

20
R. Hinze and J. Jeuring
then Haskell generates the ‘obvious’ code for equality. What ‘obvious’ means is
speciﬁed informally in an Appendix of the language deﬁnition [38]. Of course, the
idea suggests itself to use generic deﬁnitions for specifying default methods so
that the programmer can deﬁne her own derivable classes. This idea is pursued
further in [23, 1].
Haskell translates type classes and instance declarations into a family of
polymorphic functions using the so-called dictionary passing translation, which
is quite similar to the implementation technique of Generic Haskell.
Intensional Type Analysis. The framework of intensional type analysis [16]
was originally developed as a means to improve the implementation of poly-
morphic functions. It was heavily employed in typed intermediate languages
not intended for programmers but for compiler writers. The central idea is to
pass types or representation of types at run time, which can be analysed and
dispatched upon. Thus, equality has the simple type
eq :: ∀a . a →a →Bool
-- non-parametric ∀.
As in Generic Haskell equality takes an additional type argument and does a
case analysis on this argument (using a typecase). The resulting code looks
quite similar to our deﬁnition of equality. The major diﬀerence is that the type
argument is interpreted at run time whereas Generic Haskell does the analysis
at compile time. On the other hand, type-indexed functions are second-class
citizens in Generic Haskell whereas in intensional type analysis they have ﬁrst-
class status. Originally, the framework was restricted to data types of kind ⋆, but
recent work [49] has lifted this restriction (the generalization is, in fact, inspired
by our work on Generic Haskell).
Type Representations. Typed intermediate languages based on intensional
type analysis are expressive but rather complex languages. Perhaps surprisingly,
dynamic type dispatch can be simulated in a much more modest setting: we
basically require a Hindley-Milner type system augmented with existential types.
The central idea is to pass type representations instead of types. As a ﬁrst try, if
Rep is the type of type representations, we could assign ‘eq’ the type ∀a . Rep →
a →a →Bool. This approach, however, does not work. The parametricity
theorem [46] implies that a function of this type must necessarily ignore its
second and its third argument. The trick is to use a parametric type for type
representations:
eq :: ∀a . Rep a →a →a →Bool.
Here Rep t is the type representation of t. In [9, 5] it is shown how to deﬁne a Rep
type in Haskell (augmented with existential types). This approach is, however,
restricted to types of one ﬁxed kind.
Dependent Types. We have noted above that the type Generic Haskell assigns
to equality resembles a dependent type. Thus, it comes as little surprise that we

Generic Haskell: Practice and Theory
21
can simulate Generic Haskell in a dependently typed language [2]. In such a lan-
guage we can deﬁne a simple, non-parametric type Rep of type representations.
The correspondence between a type and its representative is established by a
function Type :: Rep →⋆that maps a representation to its type. The signature
of equality is then given by
eq :: (a :: Rep) →Type a →Type a →Bool.
The code of eq is similar to what we have seen before; only the typechecking is
more involved as it requires reduction of type expressions.
Historical Notes. The concept of functional generic programming trades under
a variety of names: F. Ruehr refers to this concept as structural polymorphism
[42, 41], T. Sheard calls generic functions type parametric [44], C.B. Jay and
J.R.B. Cocket use the term shape polymorphism [27], R. Harper and G. Morrisett
[16] coined the phrase intensional polymorphism, and J. Jeuring invented the
word polytypism [28].
The mainstream of generic programming is based on the initial algebra se-
mantics of datatypes, see, for instance [15], and puts emphasis on general recur-
sion operators like mapping functions and catamorphisms (folds). In [43] several
variations of these operators are informally deﬁned and algorithms are given
that specialize these functions for given datatypes. The programming language
Charity [10] automatically provides mapping functions and catamorphisms for
each user-deﬁned datatype. Since general recursion is not available, Charity is
strongly normalizing. Functorial ML [26] has a similar functionality, but a diﬀer-
ent background. It is based on the theory of shape polymorphism, in which values
are separated into shape and contents. The polytypic programming language ex-
tension PolyP [24], a precursor of Generic Haskell, oﬀers a special construct for
deﬁning generic functions. The generic deﬁnitions are similar to ours (modulo
notation) except that the generic programmer must additionally consider cases
for type composition and for type recursion (see [19] for a more detailed com-
parison).
Most approaches are restricted to ﬁrst-order kinded, regular datatypes (or
even subsets of this class). One notable exception is the work of F. Ruehr [42],
who presents a higher-order language based on a type system related to ours.
Genericity is achieved through the use of type patterns which are interpreted
at run-time. By contrast, the implementation technique of Generic Haskell does
not require the passing of types or representations of types at run-time.
Exercise 1. Prove that a function of type ∀a . a →a →Bool is necessarily con-
stant.
Exercise 2. Deﬁne an instance of equality for random-access lists.
Exercise 3. Implement a generic version of Haskell’s compare function, which
determines the precise ordering of two elements. Start by deﬁning an appropriate
kind-indexed type and then give equations for each of the type constants Unit,
‘:+:’, and ‘:*:’.

22
R. Hinze and J. Jeuring
Exercise 4. Miranda oﬀers a primitive function force :: ∀a . a →a that fully eval-
uates its argument. Implement a generic version of force using Haskell’s seq
function (which is of type ∀a b . a →b →b).
Exercise 5 (Diﬃcult). Deﬁne a generic function that memoizes a given function.
Its kind-indexed type is given by
type Memo{[⋆]} t
= ∀v . (t →v) →(t →v)
type Memo{[κ →ν]} t = ∀a . Memo{[κ]} a →Memo{[ν]} (t a).
Note that Memo{[⋆]} t is a polymorphic type. Hint: memo{|t :: ⋆|} f should yield
a closure that does not dependent on the actual argument of f .
2
Generic Haskell—Practice
In this section we look at generic deﬁnitions in more detail explaining the var-
ious features of Generic Haskell. In particular, we show how to deﬁne mapping
functions, reductions, and pretty printers generically.
2.1
Mapping Functions
A mapping function for a type constructor F of kind ⋆→⋆lifts a given function
of type a →b to a function of type F a →F b. In the sequel we show how to
deﬁne mapping functions so that they work for all types of all kinds. Before we
tackle the generic deﬁnition, let us consider some instances ﬁrst. As an aside,
note that the combination of a type constructor and its mapping function is
often referred to as a functor.
Here is again the all-time favourite, the mapping function for lists.
mapList
:: ∀a1 a2 . (a1 →a2) →(List a1 →List a2)
mapList mapa Nil
= Nil
mapList mapa (Cons a as) = Cons (mapa a) (mapList mapa as).
The mapping function takes a function and applies it to each element of a given
list. It is perhaps unusual to call the argument function mapa. The reason for
this choice will become clear as we go along. For the moment it suﬃces to bear
in mind that the deﬁnition of mapList rigidly follows the structure of the data
type.
We have seen in Section 1.2 that List can alternatively be deﬁned using an
explicit ﬁxed point construction.
type List′ a = Fix (ListBase a).

Generic Haskell: Practice and Theory
23
How can we deﬁne the mapping function for lists thus deﬁned? For a start, we
deﬁne the mapping function for the base functor.
mapListBase
:: ∀a1 a2 . (a1 →a2) →∀b1 b2 . (b1 →b2)
→(ListBase a1 b1 →ListBase a2 b2)
mapListBase mapa mapb NilL = NilL
mapListBase mapa mapb (ConsL a b)
= ConsL (mapa a) (mapb b)
Since the base functor has two type arguments, its mapping function takes two
functions, mapa and mapb, and applies them to values of type a1 and b1, respec-
tively. Even more interesting is the mapping function for Fix
mapFix
:: ∀f1 f2 . (∀a1 a2 . (a1 →a2) →(f1 a1 →f2 a2))
→(Fix f1 →Fix f2)
mapFix mapf (In v) = In (mapf (mapFix mapf ) v),
which takes a polymorphic function as an argument. In other words, mapFix has
a rank-2 type. The argument function, mapf , has a more general type than one
would probably expect: it takes a function of type a1 →a2 to a function of type
f1 a1 →f2 a2. By contrast, the mapping function for List (which like f has kind
⋆→⋆) takes a1 →a2 to List a1 →List a2. The deﬁnition below demonstrates
that the extra generality is vital.
mapList′
:: ∀a1 a2 . (a1 →a2) →(List′ a1 →List′ a2)
mapList′ mapa = mapFix (mapListBase mapa)
The argument of mapFix has type ∀b1 b2 . (b1 →b2) →(ListBase a1 b1 →
ListBase a2 b2), that is, f1 is instantiated to ListBase a1 and f2 to ListBase a2.
Now, let us deﬁne a generic version of map. What is the type of the generic
mapping function? As a ﬁrst attempt, we might deﬁne
type Map{[⋆]} t
= t →t
-- WRONG
type Map{[κ →ν]} t = ∀a . Map{[κ]} a →Map{[ν]} (t a).
Alas, we have Map{[⋆→⋆]} List = ∀a . (a →a) →(List a →List a), which
is not general enough. The solution is to use a two-argument version of the
kind-indexed type Map.
type Map{[⋆]} t1 t2
= t1 →t2
type Map{[κ →ν]} t1 t2 = ∀a1 a2 . Map{[κ]} a1 a2 →Map{[ν]} (t1 a1) (t2 a2)
map{|t :: κ|}
:: Map{[κ]} t t
We obtain Map{[⋆→⋆]} List List = ∀a1 a2 . (a1 →a2) →(List a1 →List a2)
as desired. In the base case Map{[⋆]} t1 t2 equals the type of a conversion func-
tion. The inductive case has a very characteristic form, which we have already
encountered several times. It speciﬁes that a ‘conversion function’ between the

24
R. Hinze and J. Jeuring
type constructors t1 and t2 is a function that maps a conversion function be-
tween a1 and a2 to a conversion function between t1 a1 and t2 a2, for all possible
instances of a1 and a2. Roughly speaking, Map{[κ →ν]} t1 t2 is the type of a
‘conversion function’-transformer. It is not hard to see that the type signatures of
mapList, mapListBase, and mapFix are instances of this scheme. Furthermore,
from the inductive deﬁnition above we can easily conclude that the rank of the
type signature corresponds to the kind of the type index: for instance, the map
for a second-order kinded type has a rank-2 type signature.
The deﬁnition of map itself is straightforward.
map{|t :: κ|}
:: Map{[κ]} t t
map{|Char|} c
= c
map{|Int|} i
= i
map{|Unit|} Unit
= Unit
map{|:+:|} mapa mapb (Inl a)
= Inl (mapa a)
map{|:+:|} mapa mapb (Inr b) = Inr (mapb b)
map{|:*:|} mapa mapb (a :*: b) = mapa a :*: mapb b
This deﬁnition contains all the ingredients needed to derive maps for arbitrary
data types of arbitrary kinds. As an aside, note that we can deﬁne map even
more succinctly if we use a point-free style—as usual, the maps on sums and
products are denoted (+) and (∗).
map{|Char|}
= id
map{|Int|}
= id
map{|Unit|}
= id
map{|:+:|} mapa mapb = mapa + mapb
map{|:*:|} mapa mapb = mapa ∗mapb
Even more succinctly, we have map{|:+:|} = (+) and map{|:*:|} = (∗).
As usual, to apply a generic function we simply instantiate the type-index to
a closed type.
map{|List Char|} "hello world"
=⇒"hello world"
map{|List|} toUpper "hello world"
=⇒"HELLO WORLD"
We can also use map to deﬁne other generic functions.
distribute{|t :: ⋆→⋆|} :: ∀a b . t a →b →t (a, b)
distribute{|t|} x b
= map{|t|} (λa →(a, b)) x
The call distribute{|t|} x b pairs the value b with every element contained in the
structure x.

Generic Haskell: Practice and Theory
25
2.2
Kind-Indexed Types and Type-Indexed Values
In general, the deﬁnition of a type-indexed value consists of two parts: a type
signature, which typically involves a kind-indexed type, and a set of equations,
one for each type constant. A kind-indexed type is deﬁned as follows:
type Poly{[⋆]} t1 . . . tn
= . . .
type Poly{[κ →ν]} t1 . . . tn = ∀a1 . . . an . Poly{[κ]} a1 . . . an
→Poly{[ν]} (t1 a1) . . . (tn an).
The second clause is the same for all kind-indexed types so that the generic
programmer merely has to ﬁll out the right-hand side of the ﬁrst equation.
Actually, Generic Haskell oﬀers a slightly more general form (see Section 2.4),
which is the reason why we do not leave out the second clause.
Given a kind-indexed type, the deﬁnition of a type-indexed value takes on
the following schematic form.
poly{|t :: κ|}
:: Poly{[κ]} t . . . t
poly{|Char|}
= . . .
poly{|Int|}
= . . .
poly{|Unit|}
= . . .
poly{|:+:|} polya polyb = . . .
poly{|:*:|} polya polyb = . . .
We have one clause for each primitive type (Int, Char etc) and one clause for each
of the three type constructors Unit, ‘:*:’, and ‘:+:’. Again, the generic programmer
has to ﬁll out the right-hand sides. To be well-typed, the poly{|t::κ|} instance must
have type Poly{[κ]} t . . . t as stated in the type signature of poly. Actually, the
type signature can be more elaborate (we will see examples of this in Section 2.4).
The major insight of the mapping example is that a kind-indexed type can
have several type arguments. Recall in this respect the type of the generic equal-
ity function:
type Eq{[⋆]} t
= t →t →Bool
type Eq{[κ →ν]} t = ∀a . Eq{[κ]} a →Eq{[ν]} (t a).
Interestingly, we can generalize the type since the two arguments of equality
need not be of the same type.
type Eq{[⋆]} t1 t2
= t1 →t2 →Bool
type Eq{[κ →ν]} t1 t2 = ∀a1 a2 . Eq{[κ]} a1 a2 →Eq{[ν]} (t1 a1) (t2 a2)
We can assign eq{|t :: κ|} the more general type Eq{[κ]} t t. Though this gives
us a greater degree of ﬂexibility, the deﬁnition of eq itself is not aﬀected by
this change! As an example, we could pass eq{|List|} the ‘equality test’ match ::
Female →Male →Bool in order to check whether corresponding list entries
match.

26
R. Hinze and J. Jeuring
2.3
Embedding-Projection Maps
Most of the generic functions cannot sensibly be deﬁned for the function space.
For instance, equality of functions is not decidable. The mapping function map
cannot be deﬁned for function types since (→) is contravariant in its ﬁrst argu-
ment:
(→)
:: ∀a1 a2 . (a2 →a1) →
∀b1 b2 . (b1 →b2) →((a1 →b1) →(a2 →b2))
(f →g) h = g · h · f .
In the case of mapping functions we can remedy the situation by drawing from
the theory of embeddings and projections [13]. The central idea is to supply
a pair of functions, from and to, where to is the left-inverse of from, that is,
to · from = id. (If the functions additionally satisfy from · to ⊑id, then
they are called an embedding-projection pair.) We use the following data type
to represent embedding-projection pairs (the code below make use of Haskell’s
record syntax).
data EP a1 a2 = EP{from :: a1 →a2, to :: a2 →a1}
idE
:: ∀a . EP a a
idE
= EP{from = id, to = id }
(◦)
:: ∀a b c . EP b c →EP a b →EP a c
f ◦g
= EP{from = from f · from g, to = to g · to f }
Here, idE is the identity embedding-projection pair and ‘◦’ shows how to com-
pose two embedding-projection pairs (note that the composition is reversed for
the projection). In fact, idE and ‘◦’ give rise to the category Cpoe, the category
of complete partial orders and embedding-projection pairs. This category has
the interesting property that the function space can be turned into a covariant
functor.
(+E)
:: ∀a1 a2 . EP a1 a2 →∀b1 b2 . EP b1 b2 →EP (a1 :+: b1) (a2 :+: b2)
f +E g = EP{from = from f + from g, to = to f + to g }
(∗E)
:: ∀a1 a2 . EP a1 a2 →∀b1 b2 . EP b1 b2 →EP (a1 :*: b1) (a2 :*: b2)
f ∗E g
= EP{from = from f ∗from g, to = to f ∗to g }
(→E)
:: ∀a1 a2 . EP a1 a2 →∀b1 b2 . EP b1 b2 →EP (a1 →b1) (a2 →b2)
f →E g = EP{from = to f →from g, to = from f →to g }
Given these helper functions the generic embedding-projection map can be de-
ﬁned as follows.
type MapE{[⋆]} t1 t2
= EP t1 t2
type MapE{[κ →ν]} t1 t2 = ∀a1 a2 . MapE{[κ]} a1 a2 →MapE{[ν]} (t1 a1) (t2 a2)

Generic Haskell: Practice and Theory
27
mapE{|t :: κ|} :: MapE{[κ]} t t
mapE{|Char|} = idE
mapE{|Int|}
= idE
mapE{|Unit|} = idE
mapE{|:+:|}
= (+E)
mapE{|:*:|}
= (∗E)
mapE{|→|}
= (→E)
We will see in Section 3.4 that embedding-projection maps are useful for changing
the representation of data.
2.4
Reductions
The Haskell standard library deﬁnes a vast number of list processing functions.
We have among others:
sum, product
:: ∀a . (Num a) ⇒[a] →a
and, or
:: [Bool] →Bool
all, any
:: ∀a . (a →Bool) →([a] →Bool)
length
:: ∀a . [a] →Int
minimum, maximum :: ∀a . (Ord a) ⇒[a] →a
concat
:: ∀a . [[a]] →[a].
These are examples of so-called reductions. A reduction or a crush [31] is a
function that collapses a structure of values of type t (such a structure is also
known as a container) into a single value of type t. This section explains how to
deﬁne reductions that work for all types of all kinds. To illustrate the main idea
we ﬁrst discuss three motivating examples. Let us start with a generic function
that counts the number of values of type Int within a given structure of some
type.
Here is the type of the generic counter
type Count{[⋆]} t
= t →Int
type Count{[κ →ν]} t = ∀a . Count{[κ]} a →Count{[ν]} (t a)
and here is its deﬁnition.
count{|t :: κ|}
:: Count{[κ]} t
count{|Char|} c
= 0
count{|Int|} i
= 1
count{|Unit|} Unit
= 0
count{|:+:|} counta countb (Inl a)
= counta a
count{|:+:|} counta countb (Inr b) = countb b
count{|:*:|} counta countb (a :*: b) = counta a + countb b

28
R. Hinze and J. Jeuring
Next, let us consider a slight variation: the function sum{|t|} deﬁned below
is identical to count{|t|} except for t = Int, in which case sum also returns 0.
sum{|t :: κ|}
:: Count{[κ]} t
sum{|Char|} c
= 0
sum{|Int|} i
= 0
sum{|Unit|} Unit
= 0
sum{|:+:|} suma sumb (Inl a)
= suma a
sum{|:+:|} suma sumb (Inr b) = sumb b
sum{|:*:|} suma sumb (a :*: b) = suma a + sumb b
It is not hard to see that sum{|t|} x returns 0 for all types t of kind ⋆(well,
provided x is ﬁnite and fully deﬁned). So one might be led to conclude that sum
is not a very useful function. This conclusion is, however, too rash since sum
can also be parameterized by type constructors. For instance, for unary type
constructors sum has type
sum{|t :: ⋆→⋆|} :: ∀a . (a →Int) →(t a →Int)
If we pass the identity function to sum, we obtain a function that sums up a
structure of integers. Another viable choice is const 1; this yields a function of
type ∀a . t a →Int that counts the number of values of type a in a given structure
of type t a.
sum{|List Int|} [2, 7, 1965]
=⇒0
sum{|List|} id [2, 7, 1965]
=⇒1974
sum{|List|} (const 1) [2, 7, 1965]
=⇒3
As usual, we can use generic abstractions to capture these idioms.
fsum{|t :: ⋆→⋆|} :: t Int →Int
fsum{|t|}
= sum{|t|} id
fsize{|t :: ⋆→⋆|} :: ∀a . t a →Int
fsize{|t|}
= sum{|t|} (const 1)
Using a similar approach we can ﬂatten a structure into a list of elements.
The type of the generic ﬂattening function
type Flatten{[⋆]} t x
= t →[x]
type Flatten{[κ →ν]} t x = ∀a . Flatten{[κ]} a x →Flatten{[ν]} (t a) x
makes use of a simple extension: Flatten{[κ]} t x takes an additional type param-
eter, x, that is passed unchanged to the base case. One can safely think of x as

Generic Haskell: Practice and Theory
29
a type parameter that is global to the deﬁnition. The code for ﬂatten is similar
to the code for sum.
ﬂatten{|t :: κ|}
:: ∀x . Flatten{[κ]} t x
ﬂatten{|Char|} c
= [ ]
ﬂatten{|Int|} i
= [ ]
ﬂatten{|Unit|} Unit
= [ ]
ﬂatten{|:+:|} ﬂa ﬂb (Inl a)
= ﬂa a
ﬂatten{|:+:|} ﬂa ﬂb (Inr b) = ﬂb b
ﬂatten{|:*:|} ﬂa ﬂb (a :*: b) = ﬂa a ++ ﬂb b
The type signature of ﬂatten makes precise that its instances are parametric in
the type of list elements. We have, for instance,
ﬂatten{|Char|} :: ∀x . Char →[x]
ﬂatten{|Rose|} :: ∀x . ∀a . (a →[x]) →(Rose a →[x]).
Interestingly, the type dictates that ﬂatten{|Char|} = const [ ]. Like sum, the
ﬂatten function is pointless for types but useful for type constructors.
ﬄatten{|t :: ⋆→⋆|} :: ∀a . t a →[a]
ﬄatten{|t|}
= ﬂatten{|t|} wrap where wrap a = [a ]
The deﬁnitions of sum and ﬂatten exhibit a common pattern: the elements of
a base type are replaced by a constant (0 and [ ], respectively) and the pair con-
structor is replaced by a binary operator ((+) and (++), respectively). The generic
function reduce abstracts away from these particularities. Its kind-indexed type
is given by
type Reduce{[⋆]} t x
= x →(x →x →x) →t →x
type Reduce{[κ →ν]} t x = ∀a . Reduce{[κ]} a x →Reduce{[ν]} (t a) x.
Note that the type argument x is passed unchanged to the recursive calls.
reduce{|t :: κ|}
:: ∀x . Reduce{[κ]} t x
reduce{|Char|} e op c
= e
reduce{|Int|} e op i
= e
reduce{|Unit|} e op Unit
= e
reduce{|:+:|} reda redb e op (Inl a)
= reda e op a
reduce{|:+:|} reda redb e op (Inr b) = redb e op b
reduce{|:*:|} reda redb e op (a :*: b) = reda e op a ‘op‘ redb e op b
Using reduce we can ﬁnally give generic versions of Haskell’s list processing
functions listed in the beginning of this section.
freduce{|t :: ⋆→⋆|} :: ∀x . x →(x →x →x) →t x →x
freduce{|t|}
= reduce{|t|} (λe op a →a)

30
R. Hinze and J. Jeuring
fsum{|t|}
= freduce{|t|} 0 (+)
fproduct{|t|}
= freduce{|t|} 1 (∗)
fand{|t|}
= freduce{|t|} True (∧)
for{|t|}
= freduce{|t|} False (∨)
fall{|t|} f
= fand{|t|} · map{|t|} f
fany{|t|} f
= for{|t|} · map{|t|} f
fminimum{|t|} = freduce{|t|} maxBound min
fmaximum{|t|} = freduce{|t|} minBound max
ﬄatten{|t|}
= freduce{|t|} [ ] (++)
Typically, the two arguments of freduce form a monoid: the second argument is
associative and has the ﬁrst as its neutral element.
As an aside, note that the deﬁnition of ﬄatten has a quadratic running time.
Exercise 8 seeks to remedy this defect.
2.5
Pretty Printing
Generic functions are deﬁned by induction on the structure of types. Annoyingly,
this is not quite enough. Consider, for example, the method showsPrec of the
Haskell class Show. To be able to give a generic deﬁnition for showsPrec, the
names of the constructors, and their ﬁxities, must be made available.
To this end we provide one additional type case.
poly{|Con c|} polya = . . .
Roughly speaking, this case is invoked whenever we pass by a constructor. Quite
unusual, the variable c that appears in the type index is bound to a value of type
ConDescr and provides the required information about the name of a constructor,
its arity etc.
data ConDescr = ConDescr{conName :: String,
conType :: String,
conArity :: Int,
conLabels :: Bool,
conFixity :: Fixity}
data Fixity
= Nonﬁx
| Inﬁx{prec :: Int}
| Inﬁxl{prec :: Int}
| Inﬁxr{prec :: Int}
The Con data type itself is a simple wrapper type.
data Con a = Con a
Using conName and conArity we can implement a simple variant of Haskell’s
showsPrec function (ShowS, shows, showChar, and showString are predeﬁned in
Haskell).

Generic Haskell: Practice and Theory
31
type Shows{[⋆]} t
= t →ShowS
type Shows{[κ →ν]} t
= ∀a . Shows{[κ]} a →Shows{[ν]} (t a)
gshows{|t :: κ|}
:: Shows{[κ]} t
gshows{|:+:|} sa sb (Inl a)
= sa a
gshows{|:+:|} sa sb (Inr b) = sb b
gshows{|Con c|} sa (Con a)
| conArity c
0
= showString (conName c)
| otherwise
= showChar ’(’ · showString (conName c)
· showChar ’ ’ · sa a · showChar ’)’
gshows{|:*:|} sa sb (a :*: b) = sa a · showChar ’ ’ · sb b
gshows{|Unit|} Unit
= showString ""
gshows{|Char|}
= shows
gshows{|Int|}
= shows
The ﬁrst and the second equation discard the constructors Inl and Inr. They
are not required since the constructor names can be accessed via the type pat-
tern Con c. If the constructor is nullary, its string representation is emitted.
Otherwise, the constructor name is printed followed by a space followed by the
representation of its arguments. The fourth equation applies if a constructor has
more than one component. In this case the components are separated by a space.
In a nutshell, via ‘:+:’ we get to the constructors, Con signals that we hit a
constructor, and via ‘:*:’ we get to the arguments of a constructor. Or, to put it
diﬀerently, the generic programmer views, for instance, the list data type
data List a = Nil | Cons a (List a)
as if it were given by the following type deﬁnition.
type List a = (Con Unit) :+: (Con (a :*: List a))
As a simple example, the list Cons 1 Nil is represented by Inr (Con (1 :*:
Inl (Con Unit))).
It should be noted that descriptors of type ConDescr appear only in the
type index; they have no counterpart on the value level as value constructors
are encoded using Inl and Inr. If a generic deﬁnition does not include a case for
the type pattern Con c, then we tacitly assume that poly{|Con c|} polya = polya.
(Actually, the default deﬁnition is slightly more involved since a and Con a are
diﬀerent types: the data constructor Con must be wrapped and unwrapped at the
appropriate places. Section 3.4 explains how to accomplish this representation
change in a systematic manner.) Now, why does the type Con c incorporate
information about the constructor? One might suspect that it is suﬃcient to
supply this information on the value level. Doing so would work for show, but
would fail for a generic version of read, which converts a string to a value.
Consider the ‘Con’ case:
greads{|Con c|} ra s = [(x, s3) | (s1, s2) ←lex s,
s1
conName c,
(x, s3) ←ra s2].

32
R. Hinze and J. Jeuring
The important point is that greads produces (not consumes) the value, and yet
it requires access to the constructor name.
The gshows function generates one long string, which does not look pretty
at all when printed out. We can do better using Wadler’s pretty printing com-
binators [48].
data Doc
empty :: Doc
(3)
:: Doc →Doc →Doc
string :: String →Doc
nl
:: Doc
nest
:: Int →Doc →Doc
group :: Doc →Doc
render :: Int →Doc →String
The value empty represents the empty document, the operator ‘3’ catenates two
documents, and string converts a string to an atomic document. The document
nl denotes a potential line break. The function nest increases the indentation
for all line breaks within its document argument. The function group marks its
argument as a unit: it is printed out on a single line by converting all its potential
line breaks into single spaces if this is possible without exceeding a given line-
width limit. Finally, render w d converts a document to a string respecting the
line width w.
The generic function ppPrec{|t|} d x takes a precedence level d (a value
from 0 to 10), a value x of type t and returns a document of type Doc. The
function essentially follows the structure of gshows except that it replaces the
ShowS functions by pretty printing combinators.
type Pretty{[⋆]} t
= Int →t →Doc
type Pretty{[κ →ν]} t
= ∀a . Pretty{[κ]} a →Pretty{[ν]} (t a)
ppPrec{|t :: κ|}
:: Pretty{[κ]} t
ppPrec{|:+:|} ppa ppb d (Inl a)
= ppa d a
ppPrec{|:+:|} ppa ppb d (Inr b) = ppb d b
ppPrec{|Con c|} ppa d (Con a)
| conArity c
0
= string (conName c)
| otherwise
= group (nest 2 (ppParen (d > 9) doc))
where doc
= string (conName c) 3 nl 3 ppa 10 a
ppPrec{|:*:|} ppa ppb d (a :*: b) = ppa d a 3 nl 3 ppb d b
ppPrec{|Unit|} d Unit
= empty
ppPrec{|Int|} d i
= string (show i)
ppPrec{|Char|} d c = string (show c)
ppParen
:: Bool →Doc →Doc
ppParen False d
= d
ppParen True d
= string "(" 3 d 3 string ")"

Generic Haskell: Practice and Theory
33
The helper function ppParen encloses its second argument in parenthesis if its
ﬁrst evaluates to True.
2.6
Running Generic Haskell
This section explains how to use the Generic Haskell compiler called gh. It has
two basic modes of operation. If gh is called without arguments, the user is
prompted for a ﬁle name (relative to the working directory). The compiler then
processes the ﬁle and generates an output ﬁle with the same basename as the
input ﬁle, but the extension ‘.hs’. Generic Haskell source ﬁles typically have the
extension ‘.ghs’. Alternatively, input ﬁles can be speciﬁed on the command line.
A typical invocation is:
path_to_gh/bin/gh -L path_to_gh/lib/ your_file.ghs
A number of command line options are available:
Usage: gh [options...] files...
-v
--verbose
(number of v’s controls the verbosity)
-m
--make
follow dependencies
-V
--version
show version info
-h, -?
--help
show help
--cut=N
cut computations after N iterations
-C
--continue
continue after errors
-L DIR
--library=DIR
add DIR to search path
The ﬁrst level of verbosity (no -v ﬂag) produces only error messages. The second
level (-v) additionally provides diagnostic information and warnings. The third
level (-vv) produces debugging information.
The -m (or --make) option instructs the compiler to chase module dependen-
cies and to automatically process those modules which require compilation.
The option --cut=N stops the compilation after N iterations of the special-
ization mechanism (the default is to stop after 50 iterations). The ﬁrst level of
verbosity (-v) can be used to report the number of required iterations.
The -C (or --continue) option forces the compiler to continue compilation
even when an error is encountered. This can be used to generate further error
messages or to inspect the generated code.
The Generic Haskell compiler compiles ‘.ghs’ ﬁles and produces ‘.hs’ ﬁles
which can subsequently be compiled using a Haskell compiler. In addition, the
compiler produces ‘.ghi’ interface ﬁles, which will be used in subsequent compi-
lations to avoid unnecessary recompilation. As an example, Figure 1 displays the
source code for the generic version of Haskell’s shows function. Note that type
arguments are enclosed in {|·|} brackets, while {[·]} embraces kind arguments.
The Generic Haskell compiler generates ordinary Haskell code (Haskell 98
augmented with rank-2 types), which can be run or compiled using the Glasgow
Haskell Compiler, Hugs, or any other Haskell compiler. You only have to ensure

34
R. Hinze and J. Jeuring
type Shows {[ * ]} t
=
t -> ShowS
type Shows {[ k -> l ]} t
=
forall a . Shows {[ k ]} a -> Shows {[ l ]} (t a)
gshows {| t :: k |}
:: Shows {[ k ]} t
gshows {| :+: |} sa sb (Inl a)
=
sa a
gshows {| :+: |} sa sb (Inr b)
=
sb b
gshows {| Con c|} sa (Con a)
| conArity c == 0
=
showString (conName c)
| otherwise
=
showChar ’(’ . showString (conName c)
. showChar ’ ’ . sa a . showChar ’)’
gshows {| :*: |} sa sb (a :*: b)
=
sa a . showChar ’ ’ . sb b
gshows {| Unit |} Unit
=
showString ""
gshows {| Char |}
=
shows
gshows {| Int |}
=
shows
data Tree a b
=
Tip a | Node (Tree a b) b (Tree a b)
main
=
putStrLn (gshows {| Tree Int String |} ex "")
ex
:: Tree Int String
ex
=
Node (Tip 1) "hello"
(Node (Tip 2) "world" (Tip 3))
Fig. 1. A generic implementation of the shows function.
that the path to GHPrelude.hs (and to other Generic Haskell libraries), which
can be found in the lib subdirectory, is included in your compiler’s search path.
The Generic Haskell compiler is shipped with an extensive library, which
provides further examples of generic functions.
Exercise 6. Let M be a monad. A monadic mapping function for a type con-
structor F of kind ⋆→⋆lifts a given function of type a →M b to a function of
type F a →M (F b). Deﬁne a generic version of the monadic map. First deﬁne a
kind-indexed type and then give equations for each of the type constants.
The standard mapping function is essentially determined by its type (each
equation is more or less inevitable). Does this also hold for monadic maps?
Exercise 7. Closely related to mapping functions are zipping functions. A binary
zipping function takes two structures of the same shape and combines them into
a single structure. For instance, the list zip takes two lists of type List a1 and
List a2 and pairs corresponding elements producing a list of type List (a1, a2).
Deﬁne a generic version of zip that works for all types of all kinds. Hint: the
kind-indexed type of zip is essentially a three parameter variant of Map.
Exercise 8. The implementation of ﬄatten given in Section 2.4 has a quadratic
running time since the computation of x++y takes time proportional to the length

Generic Haskell: Practice and Theory
35
of x. Using the well-known technique of accumulation [6] one can improve the
running time to O(n). This technique can be captured using a generic function,
called a right reduction. Its kind-indexed type is given by
type Reducer{[⋆]} t x
= t →x →x
type Reducer{[κ →ν]} t x = ∀a . Reducer{[κ]} a x →Reducer{[ν]} (t a) x
The second argument of type x is the accumulator. Fill in the deﬁnition of reducer
and deﬁne ﬄatten in terms of reducer. Why is reducer called a right reduction?
Also deﬁne its dual, a left reduction.
Exercise 9. The generic function fsize computes the size of a container type.
Can you deﬁne a function that computes its height? Hint: you have to make use
of the constructor case Con.
Exercise 10 (this may take some time). In Section 1.3 we have implemented a
simple form of data compression. A more sophisticated scheme could use Huﬀ-
man coding, emitting variable-length codes for the constructors. Implement a
function that given a sample element of a data type counts the number of occur-
rences of each constructor. Hint: consider the type ConDescr. Which information
is useful for your task?
3
Generic Haskell—Theory
This section highlights the theory underlying Generic Haskell (most of the ma-
terial is taken from [22]).
We have already indicated that Generic Haskell takes a transformational
approach to generic programming: a generic function is translated into a family
of polymorphic functions. We will see in this section that this transformation
can be phrased as an interpretation of the simply typed lambda calculus (types
are simply typed lambda terms with kinds playing the rˆole of types). To make
this idea precise we switch from Haskell to the polymorphic lambda calculus.
The simply typed lambda calculus and the polymorphic lambda calculus are
introduced in Sections 3.1 and 3.2, respectively. Section 3.3 then shows how to
specialize generic functions to instances of data types.
The polymorphic lambda calculus is the language of choice for the theoretical
treatment of generic deﬁnitions. Though Haskell comes quite close to this ideal
language, there is one fundamental diﬀerence between Haskell and (our presen-
tation) of the polymorphic lambda calculus. In Haskell, each data declaration
introduces a new type; two types are equal iﬀthey have the same name. By con-
trast, in the polymorphic lambda calculus two types are equal iﬀthey have the
same structure. Section 3.4 explains how to adapt the technique of Section 3.3
to a type system based on name equivalence.
3.1
The Simply Typed Lambda Calculus as a Type Language
This section introduces the language of kinds and types that we will use in the
sequel. The type system is essentially that of Haskell smoothing away some of its

36
R. Hinze and J. Jeuring
irregularities. We have see in Section 1.2 that Haskell oﬀers one basic construct
for deﬁning new types: data type declarations.
data B a1 . . . am = K1 t11 . . . t1m1 | · · · | Kn tn1 . . . tnmn.
From a language design point of view the data construct is quite a beast as it
combines no less than four diﬀerent features: type abstraction, type recursion,
n-ary sums, and n-ary products. The types on the right-hand side are built from
type constants (that is, primitive type constructors), type variables, and type
application. Thus, Haskell’s type system essentially corresponds to the simply
typed lambda calculus with kinds playing the rˆole of types.
In the sequel we review the syntax and the semantics of the simply typed
lambda calculus. A basic knowledge of this material will prove useful when we
discuss the specialization of type-indexed values. Most of the deﬁnitions are
taken from the excellent textbook by J. Mitchell [34].
Syntax. The simply typed lambda calculus has a two-level structure: kinds and
types (since we will use the calculus to model Haskell’s type system we continue
to speak of kinds and types).
kind terms
κ, ν ∈Kind
type constants
C, D ∈Const
type variables
a, b ∈Var
type terms
t, u ∈Type
Note that we use Greek letters for kinds and Sans Serif letters for types.
Kind terms are formed according to the following grammar.
κ, ν ∈Kind ::= ⋆
kind of types
|
(κ →ν)
function kind
As usual, we assume that ‘→’ associates to the right.
Pseudo-type terms are built from type constants and type variables using
type abstraction and type application.
t, u ∈Type ::= C
type constant
|
a
type variable
|
(Λa :: ν . t)
type abstraction
|
(t u)
type application
We assume that type abstraction extends as far to the right as possible and that
type application associates to the left. For typographic simplicity, we will often
omit the kind annotation in Λa::ν . t (especially if ν = ⋆). Finally, we abbreviate
nested abstractions Λa1 . . . . Λam . t by Λa1 . . . am . t.
The choice of Const, the set of type constants, is more or less arbitrary.
However, in order to model Haskell’s data declarations we assume that Const

Generic Haskell: Practice and Theory
37
comprises at least the constants Char, Int, Unit, ‘:+:’, ‘:*:’, and a family of ﬁxed
point operators:
Const ⊇{Char :: ⋆, Int :: ⋆, Unit :: ⋆, (:*:) :: ⋆→⋆→⋆, (:+:) :: ⋆→⋆→⋆}
∪{Fixκ :: (κ →κ) →κ | κ ∈Kind}.
As usual, we write binary type constants inﬁx. We assume that ‘:*:’ and ‘:+:’
associate to the right and that ‘:*:’ binds more tightly than ‘:+:’. The set of
type constants includes a family of ﬁxed point operators indexed by kind. In the
examples, we will often omit the kind annotation in Fixκ.
In order to deﬁne ‘well-kinded’ type terms we need the notion of a context.
A context is a ﬁnite set of kind assumptions of the form a::κ. It is convenient to
view a context Γ as a ﬁnite map from type variables to kinds and write dom(Γ)
for its domain. Likewise, we view Const as a ﬁnite map from type constants to
kinds. A pseudo-type term t is called a type term if there is a context Γ and a
kind κ such that Γ ⊢t :: κ is derivable using the rules depicted in Figure 2.
Γ ⊢C :: Const(C) (t-const)
Γ ⊢a :: Γ(a) (t-var)
Γ, a :: ν ⊢t :: κ
Γ ⊢(Λa :: ν . t) :: (ν →κ) (t-→-intro)
Γ ⊢t :: (ν →κ)
Γ ⊢u :: ν
Γ ⊢(t u) :: κ
(t-→-elim)
Fig. 2. Kinding rules.
The equational proof system of the simply typed lambda calculus is given by
the rules in Figure 3. Let E be a possibly empty set of equations between type
terms. We write Γ ⊢E t1 = t2 :: κ to mean that the type equation t1 = t2 is
provable using the rules and the equations in E.
Γ ⊢((Λa :: ν . t) u = t[a := u]) :: κ (t-β)
a not free in t
Γ ⊢(Λa :: ν . t a = t) :: (ν →κ) (t-η)
Γ ⊢(Fixκ t = t (Fixκ t)) :: κ (t-fix)
Fig. 3. Equational proof rules (the usual ‘logical’ rules for reﬂexivity, symmetry, tran-
sitivity, and congruence are omitted).
The equational proof rules identify a recursive type Fixκ t and its unfolding
t (Fixκ t). In general, there are two ﬂavours of recursive types: equi-recursive

38
R. Hinze and J. Jeuring
types and iso-recursive types, see [12]. In the latter system Fixκ t and t (Fixκ t)
must only be isomorphic rather than equal. The subsequent development is
largely independent of this design choice. We use equi-recursive types because
they simplify the presentation somewhat.
Modelling Data Declarations. Using the simply typed lambda calculus as a
type language we can easily translate data type declarations into type terms. For
instance, the type B deﬁned by the schematic data declaration in the beginning
of this section is modelled by (we tacitly assume that the kinds of the type
variables have been inferred)
Fix (ΛB . Λa1 . . . am . (t11 :*: · · · :*: t1m1) :+: · · · :+: (tn1 :*: · · · :*: tnmn)),
where t1 :*: · · · :*: tk = Unit for k = 0. For simplicity, n-ary sums are reduced
to binary sums and n-ary products to binary products. For instance, the data
declaration
data List a = Nil | Cons a (List a)
is translated to
Fix (ΛList . Λa . Unit :+: a :*: List a).
Interestingly, the representation of regular types such as List can be improved
by applying a technique called lambda-dropping [11]: if Fix (Λf . Λa . t) is regular,
then it is equivalent to Λa . Fix (Λb . t[f a := b]) where t[t1 := t2] denotes the
type term, in which all occurrences of t1 are replaced by t2. For instance, the
λ-dropped version of Fix (ΛList . Λa . Unit :+: a :*: List a) is Λa . Fix (Λb . Unit :+:
a :*: b). The λ-dropped version employs the ﬁxed point operator at kind ⋆
whereas the original, the so-called λ-lifted version employs the ﬁxed point op-
erator at kind ⋆→⋆. Nested types such as Sequ are not amenable to this
transformation since the type argument of the nested type is changed in the
recursive call(s). As an aside, note that the λ-dropped and the λ-lifted version
correspond to two diﬀerent methods of modelling parameterized types: families
of ﬁrst-order ﬁxed points versus higher-order ﬁxed points, see, for instance, [8].
Environment Models. This section is concerned with the denotational se-
mantics of the simply typed lambda calculus. There are two general frameworks
for describing the semantics: environment models and models based on cartesian
closed categories. We will use environment models for the presentation since they
are somewhat easier to understand.
The deﬁnition of the semantics proceeds in three steps. First, we introduce
so-called applicative structures, and then we deﬁne two conditions that an ap-
plicative structure must satisfy to qualify as a model. An applicative structure
is similar to an algebra, in that we need a set of ‘values’ for each kind and an
interpretation for each type constant. Additionally, we have to give meaning to
type abstraction and type application.

Generic Haskell: Practice and Theory
39
Deﬁnition 1. An applicative structure A is a tuple (A, app, const) such that
– A = (Aκ | κ ∈Kind) is a family of sets,
– app = (appκ,ν : Aκ→ν →(Aκ →Aν) | κ, ν ∈Kind) is a family of maps, and
– const : Const →A is a mapping from type constants to values such that
const(C) ∈AConst(C) for every C ∈dom(Const).
The ﬁrst condition on models requires that equality between elements of function
kinds is standard equality on functions.
Deﬁnition 2. An applicative structure A = (A, app, const) is extensional, if
∀φ1, φ2 ∈Aκ→ν . (∀α ∈Aκ . app φ1 α = app φ2 α) ⊃φ1 = φ2.
A simple example for an applicative structure is the set of type terms itself: Let
H be an inﬁnite context that provides inﬁnitely many type variables of each kind.
An extensional applicative structure (Type, app, const) may then be deﬁned by
letting Typeκ = { t | Γ ⊢t :: κ for some ﬁnite Γ ⊆H }, app t u = (t u), and
const(C) = C.
The second condition on models ensures that the applicative structure has
enough points so that every type term containing type abstractions can be as-
signed a meaning in the structure. To formulate the condition we require the
notion of an environment. An environment η is a mapping from type variables
to values. If Γ is a context, then we say η satisﬁes Γ if η(a) ∈AΓ(a) for every
a ∈dom(Γ). If η is an environment, then η(a := α) is the environment mapping
a to α and b to η(b) for b diﬀerent from a.
Deﬁnition 3. An applicative structure A = (A, app, const) is an environment
model if it is extensional and if the clauses below deﬁne a total meaning function
on terms Γ ⊢t :: κ and environments such that η satisﬁes Γ.
AΓ ⊢C :: Const(C)η = const(C)
AΓ ⊢a :: Γ(a)η
= η(a)
AΓ ⊢(Λa :: ν . t) :: (ν →κ)η
= the unique φ ∈Aν→κ such that for all α ∈Aν
appν,κ φ α = AΓ, a :: ν ⊢t :: κη(a := α)
AΓ ⊢(t u) :: κη
= appν,κ (AΓ ⊢t :: (ν →κ)η) (AΓ ⊢u :: νη)
Note that extensionality guarantees the uniqueness of the element φ whose ex-
istence is postulated in the third clause.
The set of type terms can be turned into an environment model if we identify
type terms that are provably equal. Let E be a possibly empty set of equations
between type terms. Then we deﬁne the equivalence class of types [t] = { T ′ |
Γ ⊢E t = T ′ :: κ for some ﬁnite Γ ⊆H } and let Typeκ/E = {[t] | t ∈Typeκ},
(app/E) [t] [u] = [t u], and (const/E) (C) = [C]. Then the applicative structure
(Type/E, app/E, const/E) is an environment model.
The environment model condition is often diﬃcult to check. An equivalent,
but simpler condition is the combinatory model condition.

40
R. Hinze and J. Jeuring
Γ ⊢r :: ⋆
Γ ⊢s :: ⋆
Γ ⊢(r →s) :: ⋆
(t-fun)
Γ, a :: ν ⊢s :: ⋆
Γ ⊢(∀a :: ν . s) :: ⋆(t-all)
Fig. 4. Additional kinding rules for type schemes.
Deﬁnition 4. An applicative structure A = (A, app, const) satisﬁes the combi-
natory model condition if for all kinds κ, ν and µ there exist elements Kκ,ν ∈
Aκ→ν→κ and Sκ,ν,µ ∈A(κ→ν→µ)→(κ→ν)→κ→µ such that
app (app K a) b
= a
app (app (app S a) b) c = app (app a c) (app b c)
for all a, b and c of the appropriate kinds.
3.2
The Polymorphic Lambda Calculus
We have seen in Section 1.3 that instances of generic functions require ﬁrst-
class polymorphism. For instance, eqGRose takes a polymorphic argument to
a polymorphic result. To make the use of polymorphism explicit we will use a
variant of the polymorphic lambda calculus [14] (also known as Fω) both for
deﬁning and for specializing type-indexed values. This section provides a brief
introduction to the calculus. As an aside, note that a similar language is also
used as the internal language of the Glasgow Haskell Compiler [39].
The polymorphic lambda calculus has a three-level structure (kinds, type
schemes, and terms) incorporating the simply typed lambda calculus on the
type level.
type schemes
r, s ∈Scheme
individual constants
c, d ∈const
individual variables
a, b ∈var
terms
t, u ∈term
We use Roman letters for terms.
Pseudo-type schemes are formed according to the following grammar.
r, s ∈Scheme ::= t
type term
|
(r →s)
function type
|
(∀a :: ν . s)
polymorphic type
A pseudo-type scheme s is called a type scheme if there is a context Γ and a
kind κ such that Γ ⊢s :: κ is derivable using the rules listed in Figures 2 and 4.
Pseudo-terms are given by the grammar

Generic Haskell: Practice and Theory
41
Γ ⊢c :: const(c) (var)
Γ ⊢a :: Γ(a) (const)
Γ, a :: s ⊢t :: r
Γ ⊢(λa :: s . t) :: (s →r) (→-intro)
Γ ⊢t :: (s →r)
Γ ⊢u :: s
Γ ⊢(t u) :: r
(→-elim)
Γ, a :: ν ⊢t :: s
Γ ⊢(λa :: ν . t) :: (∀a :: ν . s) (∀-intro)
Γ ⊢t :: (∀a :: ν . s)
Γ ⊢r :: ν
Γ ⊢(t r) :: s[a := r]
(∀-elim)
Γ ⊢t :: r
Γ ⊢(r = s) :: ⋆
Γ ⊢t :: s
(conv)
Fig. 5. Typing rules.
t, u ∈term ::= c
constant
|
a
variable
|
(λa :: s . t)
abstraction
|
(t u)
application
|
(λa :: ν . t)
universal abstraction
|
(t r)
universal application.
Here, λa :: ν . t denotes universal abstraction (forming a polymorphic value) and
t r denotes universal application (instantiating a polymorphic value). Note that
we use the same syntax for value abstraction λa ::s . t (here a is a value variable)
and universal abstraction λa :: ν . t (here a is a type variable). We assume that
the set const of value constants includes at least the polymorphic ﬁxed point
operator
ﬁx :: ∀a . (a →a) →a
and suitable functions for each of the other type constants C in dom(Const) (such
as Unit for ‘Unit’, Inl, Inr, and case for ‘:+:’, and outl, outr, and (
:*:
) for
‘:*:’). To improve readability we will usually omit the type argument of ﬁx.
To give the typing rules we have to extend the notion of context. A context
is a ﬁnite set of kind assumptions a :: κ and type assumptions a :: s. We say a
context Γ is closed if Γ is either empty, or if Γ = Γ1, a :: κ with Γ1 closed, or if
Γ = Γ1, a :: s with Γ1 closed and free(s) ⊆dom(Γ1). In the following we assume
that contexts are closed. This restriction is necessary to prevent non-sensible
terms such as a :: a ⊢λa :: ⋆. a where the value variable a carries the type
variable a out of scope. A pseudo-term t is called a term if there is some context
Γ and some type scheme s such that Γ ⊢t :: s is derivable using the typing rules
depicted in Figure 5. Note that rule (conv) allows us to interchange provably
equal types.

42
R. Hinze and J. Jeuring
Γ ⊢((λa :: s . t) u = t[a := u ]) :: r (β)
a not free in t
Γ ⊢(λa :: s . t a = t) :: (s →r) (η)
Γ ⊢((λa :: ν . t) r = t[a := r]) :: s[a := r] (β)∀
a not free in t
Γ ⊢(λa :: ν . t a = t) :: (∀a :: ν . s) (η)∀
Γ ⊢(ﬁx r f = f (ﬁx r f )) :: r (fix)
Fig. 6. Equational proof rules (the usual ‘logical’ rules for reﬂexivity, symmetry, tran-
sitivity, and congruence are omitted).
The equational proof system of the polymorphic lambda calculus is given
by the rules in Figure 6. When we discuss the specialization of type-indexed
values, we will consider type schemes and terms modulo provable equality. Let
H be an inﬁnite context that provides type variables of each kind and vari-
ables of each type scheme and let E be a set of equations between type schemes
and/or between terms. Analogous to the entities Typeκ, [t] and Typeκ/E we
deﬁne Schemeκ = { s | Γ ⊢s :: κ for some ﬁnite Γ ⊆H }, the equivalence class
[s] = { s′ | Γ ⊢E s = s′ :: κ for some ﬁnite Γ ⊆H }, Schemeκ/E = {[s] | s ∈
Schemeκ}, and Terms = { t | Γ ⊢t :: s for some ﬁnite Γ ⊆H }, [t ] = { t′ |
Γ ⊢E t = t′ :: s for some ﬁnite Γ ⊆H }, and Terms/E = {[t ] | t ∈Terms}. Note
that [s1] = [s2] implies Terms1 = Terms2 because of rule (conv).
3.3
Specializing Type-Indexed Values
This section is concerned with the specialization of type-indexed values to con-
crete instances of data types. We have seen in Section 2.1 that the structure
of each instance of map{|t|} rigidly follows the structure of t. Perhaps surpris-
ingly, the intimate correspondence between the type and the value level holds
not only for map but for all type-indexed values. In fact, the process of special-
ization can be phrased as an interpretation of the simply typed lambda calculus.
The generic programmer speciﬁes the interpretation of type constants. Given
this information the meaning of a type term—that is, the specialization of a
type-indexed value—is ﬁxed: roughly speaking, type application is interpreted
by value application, type abstraction by value abstraction, and type recursion
by value recursion.
Before we discuss the formal deﬁnitions let us take a look at an example
ﬁrst. Consider specializing map for the type Matrix given by Λa . List (List a).
The instance mapMatrix is given by
mapMatrix :: ∀a1 a2 . (a1 →a2) →(Matrix a1 →Matrix a2)
mapMatrix = λa1 a2. λmapa :: (a1 →a2) . mapList (List a1) (List a2)
(mapList a1 a2 mapa).

Generic Haskell: Practice and Theory
43
The specialization of the type application t = List a is given by the lambda term
t = mapList a1 a2 mapa, which is a combination of universal application and
value application. Likewise, the specialization of the application List t is given by
mapList (List a1) (List a2) t. Consequently, if we aim at phrasing the specialization
of map as a model of the simply typed lambda calculus we must administer both
the actual instance of map and its type. This observation suggests to represent
instances as triples (s1, s2; maps) where s1 and s2 are type schemes of some kind,
say, κ and maps is a value term of type Map{[κ]} s1 s2. Of course, we have
to work with equivalence classes of type schemes and terms. Let E be a set
of equations specifying identities between type schemes and/or between terms.
Possible identities include outl (t, u) = t and outr (t, u) = u. The applicative
structure M = (M, app, const) is then given by
Mκ
= ([s1], [s2] ∈Schemeκ/E; TermMap{[κ]} s1 s2/E)
appκ,ν ([r1], [r2]; [t ]) ([s1], [s2]; [u ])
= ([r1 s1], [r2 s2]; [t s1 s2 u ])
const(C) = ([C], [C]; [map{|C|}]).
Note that the semantic application function app uses both the type and the
value component of its second argument. It is instructive to check that the
resulting term t s1 s2 u is indeed well-typed: t has type ∀a1 a2 . Map{[κ]} a1 a2 →
Map{[ν]} (r1 a1) (r2 a2), s1 and s2 have kind κ, and u has type Map{[κ]} s1 s2. It
is important to note that the deﬁnitions of Map{[κ →ν]} and app go hand in
hand. This explains, in particular, why the deﬁnition of Poly{[κ →ν]} is ﬁxed
for function kinds.
Now, does M also constitute a model? To this end we have to show that
M is extensional and that it satisﬁes the combinatorial model condition. The
ﬁrst condition is easy to check. To establish the second condition we deﬁne
combinators (omitting type and kind annotations)
Kκ,ν
= ([Kκ,ν ], [Kκ,ν ]; [λa1 a2 . λmapa . λb1 b2 . λmapb . mapa])
Sκ,ν,µ = ([Sκ,ν,µ], [Sκ,ν,µ];
[λa1 a2 . λmapa . λb1 b2 . λmapb . λc1 c2 . λmapc .
(mapa c1 c2 mapc) (b1 c1) (b2 c2) (mapb c1 c2 mapc)])
where K and S are given by
Kκ,ν
= Λa :: κ . Λb :: ν . a
Sκ,ν,µ = Λa :: (κ →ν →µ) . Λb :: (κ →ν) . Λc :: κ . (a c) (b c).
It is straightforward to prove that the combinatory laws are indeed satisﬁed.
It remains to provide interpretations for the ﬁxed point operators Fixκ. The
deﬁnition is essentially the same for all type-indexed values. This is why the
generic programmer need not supply instances for Fixκ by hand. Here is the
deﬁnition of map{|Fixκ|}.
map{|Fixκ|} = λf1 f2 . λmapf :: (Map{[κ →κ]} f1 f2) .
ﬁx (mapf (Fixκ f1) (Fixκ f2))

44
R. Hinze and J. Jeuring
Note that map{|Fixκ|} essentially equals ﬁx—if we ignore type abstractions and
type applications. Let us check that the deﬁnition of map{|Fixκ|} is well-typed.
The universal application mapf (Fixκ f1) (Fixκ f2) has type
Map{[κ]} (Fixκ f1) (Fixκ f2) →Map{[κ]} (f1 (Fixκ f1)) (f2 (Fixκ f2)).
Since we have Fixκ fi = fi (Fixκ fi), we can use rule (conv) to infer the
type Map{[κ]} (Fixκ f1) (Fixκ f2) →Map{[κ]} (Fixκ f1) (Fixκ f2). Consequently,
ﬁx (mapf (Fixκ f1) (Fixκ f2)) has type Map{[κ]} (Fixκ f1) (Fixκ f2) as desired.
Now, let us turn to the general case of generic functions. The deﬁnitions
for arbitrary type-indexed values are very similar to the ones for map. The
applicative structure P = (P, app, const) induced by the type-indexed value
poly{|t :: κ|} :: Poly{[κ]} t . . . t is given by
Pκ
= ([s1], . . . , [sn] ∈Schemeκ/E; TermPoly{[κ]} s1 ... sn/E)
appκ,ν ([r1], . . . , [rn]; [t ]) ([s1], . . . , [sn]; [u ])
= ([r1 s1], . . . , [rn sn]; [t s1 . . . sn u ])
const(c) = ([c], . . . , [c]; [poly{|c|}]).
where poly{|Fixκ|} is deﬁned
poly{|Fixκ|} = λf1 . . . fn . λpolyf :: (Poly{[κ →κ]} f1 . . . fn) .
ﬁx (polyf (Fixκ f1) . . . (Fixκ fn)).
Three remarks are in order. First, the value domain Pκ is a so-called dependent
product: the type of the last component depends on the ﬁrst n components. A
similar structure has also been used to give a semantics to Standard ML’s module
system, see [35]. Second, if t is a closed type term, then P∅⊢t::κη is of the form
([t], . . . , [t]; [poly{|t|}]) where poly{|t|} is the desired instance. As an aside, note
that this is in agreement with poly’s type signature poly{|t::κ|}::Poly{[κ]} t . . . t.
Third, a type-indexed value can be specialized to a type but not to a type scheme.
This restriction is, however, quite mild. Haskell, for instance, does not allow
universal quantiﬁers in data declarations. (Recall that we need type schemes to
be able to assign types to instances of generic functions.)
Let us conclude the section by noting a trivial consequence of the special-
ization. Since the structure of types is reﬂected on the value level, we have, for
instance,
poly{|Λa . f (g a)|} = λa1 . . . an . λpolya .
poly{|f|} (g a1) . . . (g an) (poly{|g|} a1 . . . an polya).
Writing type and function composition as usual this implies, in particular, that
map{|f · g|} = map{|f|} · map{|g|}. Perhaps surprisingly, this relationship holds for
all type-indexed values, not only for mapping functions. A similar observation is
that poly{|Λa . a|} = λa . λpolya . polya for all type-indexed values. Abbreviating
Λa . a by Id we have, in particular, that map{|Id|} = id. As an aside, note that
these generic identities are not to be confused with the familiar functorial laws
map{|f|} id = id and map{|f|} (ϕ·ψ) = map{|f|} ϕ·map{|f|} ψ, which are base-level
identities.

Generic Haskell: Practice and Theory
45
3.4
Bridging the Gap
The polymorphic lambda calculus is the language of choice for the theoretical
treatment of generic deﬁnitions as it oﬀers rank-n polymorphism, which is re-
quired for specializing higher-order kinded data types. We additionally equipped
it with a liberal notion of type equivalence so that we can interpret the type def-
inition List a = Unit :+: a :*: List a as an equality rather than as an isomorphism.
Haskell—or rather, extensions of Haskell come quite close to this ideal lan-
guage. The Glasgow Haskell Compiler, GHC, [45], the Haskell B. Compiler, HBC,
[3] and the Haskell interpreter Hugs [29] provide rank-2 type signatures. The lat-
est version of the Glasgow Haskell Compiler, GHC 5.04, even supports general
rank-n types. There is, however, one fundamental diﬀerence between Haskell and
(our presentation) of the polymorphic lambda calculus: Haskell’s notion of type
equivalence is based on name equivalence while the polymorphic lambda calculus
employs structural equivalence. In the sequel we explain the diﬀerence between
the two views and show how to adapt the specialization to a type system that
is based on name equivalence.
Generic Representation Types. Consider again the Haskell data type of
parametric lists:
data List a = Nil | Cons a (List a).
We have modelled this declaration (see Section 3.1) by the type term
Fix (ΛList . Λa . Unit :+: a :*: List a).
Since the equivalence of type terms is based on structural equivalence, we have,
in particular, that List a = Unit :+: a :*: List a (using (t-fix), see Figure 3). The
specialization of generic values described in the previous section makes essential
use of this fact: the List instance of poly given by (omitting type abstractions
and type applications)
ﬁx (λpoly List . λpolya . poly:+: polyUnit (poly:*: polya (polyList polya)))
only works under the proviso that List a = Unit :+: a :*: List a. Alas, in Haskell
List a is not equivalent to Unit :+: a :*: List a as each data declaration introduces
a new distinct type. Even the type Liste deﬁned by
data Liste a = Vide | Constructeur a (Liste a)
is not equivalent to List though only the names of the data constructors are diﬀer-
ent. Furthermore, Haskell’s data construct works with n-ary sums and products
whereas generic deﬁnitions operate on binary sums and products. The bottom
line of all this is that when generating instances we additionally have to intro-
duce conversion functions which perform the impedance-matching. Fortunately,
this can be done in a systematic way.

46
R. Hinze and J. Jeuring
To begin with we introduce so-called generic representation types, which me-
diate between the two representations. For instance, the generic representation
type for List, which we will call List◦, is given by
type List◦a = Unit :+: a :*: List a.
As to be expected our generic representation type constructors are just unit,
binary sum and binary product. In particular, there is no recursion operator.
We observe that List◦is a non-recursive type synonym: List (not List◦) appears
on the right-hand side. So List◦is not a recursive type; rather, it expresses the
‘top layer’ of a list structure.
The type constructor List◦is (more or less) isomorphic to List. To make the
isomorphism explicit, let us write functions that convert to and fro:
fromList
:: ∀a . List a →List◦a
fromList Nil
= Inl Unit
fromList (Cons x xs) = Inr (x :*: xs)
toList
:: ∀a . List◦a →List a
toList (Inl Unit)
= Nil
toList (Inr (x :*: xs)) = Cons x xs.
Though these are non-generic functions, it is not hard to generate them mechan-
ically. That is what we turn our attention to now.
Since the generic deﬁnitions work with binary sums and binary products,
algebraic data types with many constructors, each of which has many ﬁelds,
must be encoded as nested uses of sum and product. There are many possible
encodings. For concreteness, we use a simple linear encoding: for
data B a1 . . . am = K1 t11 . . . t1m1 | · · · | Kn tn1 . . . tnmn
we generate:
type B◦a1 . . . am = Σ (Π t11 . . . t1m1) · · · (Π tn1 . . . tnmn )
where ‘Σ’ and ‘Π’ are deﬁned
Σ t1 . . . tn =

t1
if n = 1
t1 :+: Σ t2 . . . tn if n > 1
Π t1 . . . tn =



Unit
if n = 0
t1
if n = 1
t1 :*: Π t2 . . . tn if n > 1.
Note that this encoding corresponds closely to the scheme introduced in Sec-
tion 3.1 except that here the argument types of the constructors are not recur-
sively encoded. The conversion functions fromB and toB are then given by

Generic Haskell: Practice and Theory
47
fromB
:: ∀a1 . . . am . B a1 . . . am →B◦a1 . . . am
fromB (K1 x11 . . . x1m1)
= inn
1 (tuple x11 . . . x1m1)
. . .
fromB (Kn xn1 . . . xnmn)
= inn
n (tuple xn1 . . . xnmn)
toB
:: ∀a1 . . . am . B◦a1 . . . am →B a1 . . . am
toB (inn
1 (tuple x11 . . . x1m1)) = K1 x11 . . . x1m1
. . .
toB (inn
n (tuple xn1 . . . xnmn)) = Kn xn1 . . . xnmn
where
inn
i t
=



t
if n = 1
Inl t
if n > 1 ∧i = 1
Inr (inn−1
i−1 t) if n > 1 ∧i > 1
tuple t1 . . . tn =



Unit
if n = 0
t1
if n = 1
(t1 :*: tuple t2 . . . tn) if n > 1.
An alternative encoding, which is based on a binary sub-division scheme, is given
in [18]. Most generic functions are insensitive to the translation of sums and
products. Two notable exceptions are encode and decodes, for which the binary
sub-division scheme is preferable (the linear encoding aggravates the compression
rate).
Specializing Generic Values. Assume for the sake of example that we want
to specialize the generic functions encode and decodes introduced in Section 1.3
to the List data type. Recall the types of the generic values (here expressed as
type synonyms):
type Encode a = a →Bin
type Decodes a = Bin →(a, Bin).
Since List◦involves only the type constants Unit, ‘:+:’ and ‘:*:’ (and the type
variables List and a), we can easily specialize encode and decodes to List◦a: the
instances have types Encode (List◦a) and Decodes (List◦a), respectively. How-
ever, we want to generate functions of type Encode (List a) and Decodes (List a).
Now, we already know how to convert between List◦a and List a. So it remains to
lift fromList and toList to functions of type Encode (List a) →Encode (List◦a) and
Encode (List◦a) →Encode (List a). But this lifting is exactly what a mapping
function does! In particular, since Encode and Decodes involve function types
and we have to convert to and fro, we can use the embedding-projection maps
of Section 2.3 for this purpose.
For mapE we have to package the two conversion functions into a single
value:
conv List :: ∀a . EP (List a) (List◦a)
conv List = EP{from = fromList, to = toList}.
Then encodeList and decodesList are given by

48
R. Hinze and J. Jeuring
encodeList ena = to (mapE{|Encode|} conv List) (encode{|List◦|} ena)
decodesList dea = to (mapE{|Decodes|} conv List) (decodes{|List◦|} dea).
Consider the deﬁnition of encodeList. The specialization encode{|List◦|} ena yields
a function of type Encode (List◦a); the call to (mapE{|Encode|} conv List) then
converts this function into a value of type Encode (List a) as desired.
In general, the translation proceeds as follows. For each generic deﬁnition we
generate the following.
– A type synonym Poly = Poly{[⋆]} for the type of the generic value.
– An embedding-projection map, mapE{|Poly|}.
– Generic instances for Unit, ‘:+:’, ‘:*:’ and possibly other primitive types.
For each data type declaration B we generate the following.
– A type synonym, B◦, for B’s generic representation type.
– An embedding-projection pair conv B that converts between B a1 . . . am and
B◦a1 . . . am.
conv B :: ∀a1 . . . am . EP (B a1 . . . am) (B◦a1 . . . am)
conv B = EP{from = fromB, to = toB}
The instance of poly for type B :: κ is then given by (using Haskell syntax)
poly B :: Poly{[κ]} B . . . B
polyB polya1 . . . polyam
= to (mapE{|Poly|} conv B . . . conv B) (poly{|B◦|} polya1 . . . polyam).
If Poly{[κ]} B . . . B has a rank of 2 or below, we can express polyB directly in
Haskell. Figures 7 and 8 show several examples of specializations all expressed
in Haskell.
Generating Embedding-Projection Maps. We are in a peculiar situation:
in order to specialize a generic value poly to some data type B, we have to spe-
cialize another generic value, namely, mapE to poly’s type Poly. This works ﬁne
if Poly like Encode only involves primitive types. So let us make this assumption
for the moment. Here is a version of mapE tailored to Haskell’s set of primitive
types:
mapE{|t :: κ|}
:: MapE{[κ]} t t
mapE{|Char|}
= idE
mapE{|Int|}
= idE
mapE{|→|} mapE a mapE b = mapE a →E mapE b
mapE{|IO|} mapE a
= EP{from = fmap (from mapE a),
to = fmap (to mapE a)}.
Note that in the last equation mapE falls back on the ‘ordinary’ mapping func-
tion fmap. (In Haskell, fmap, a method of the Functor class, is an overloaded
version of map, which is conﬁned to lists.) In fact, we can alternatively deﬁne
mapE{|IO|} = liftE
where

Generic Haskell: Practice and Theory
49
{- binary encoding -}
type Encode a
= a →Bin
encode Unit
:: Encode Unit
encode Unit
= λUnit →[ ]
encode :+:
:: ∀a . Encode a →(∀b . Encode b →Encode (a :+: b))
encode :+: encodea encodeb = λs →case s of {Inl a →0 : encodea a;
Inr b →1 : encodeb b}
encode :*:
:: ∀a . Encode a →(∀b . Encode b →Encode (a :*: b))
encode :*: encodea encodeb = λ(a :*: b) →encodea a ++ encodeb b
mapE Encode
:: ∀a b . EP a b →EP (Encode a) (Encode b)
mapE Encode m
= EP{from = λh →h · to m, to = λh →h · from m }
{- equality -}
type Equal a1 a2
= a1 →a2 →Bool
equal Unit
:: Equal Unit Unit
equal Unit
= λUnit Unit →True
equal :+:
:: ∀a1 a2 . Equal a1 a2 →(∀b1 b2 . Equal b1 b2
→Equal (a1 :+: b1) (a2 :+: b2))
equal :+: equal a equal b
= λs1 s2 →case (s1, s2) of
{(Inl a1, Inl a2) →equal a a1 a2;
(Inl a1, Inr b2) →False;
(Inr b1, Inl a2) →False;
(Inr b1, Inr b2) →equal b b1 b2}
equal :*:
:: ∀a1 a2 . Equal a1 a2 →(∀b1 b2 . Equal b1 b2
→Equal (a1 :*: b1) (a2 :*: b2))
equal :*: equal a equal b
= λ(a1 :*: b1) (a2 :*: b2) →equal a a1 a2 ∧equal b b1 b2
mapE Equal
:: ∀a1 b1 . EP a1 b1 →(∀a2 b2 . EP a2 b2
→EP (Equal a1 a2) (Equal b1 b2))
mapE Equal m1 m2
= EP{from = λh →λa1 a2 →h (to m1 a1) (to m2 a2),
to = λh →λb1 b2 →h (from m1 b1) (from m2 b2)}
{- generic representation types -}
type Maybe◦a
= Unit :+: a
fromMaybe
:: ∀a . Maybe a →Maybe◦a
fromMaybe Nothing
= Inl Unit
fromMaybe (Just a)
= Inr a
toMaybe
:: ∀a . Maybe◦a →Maybe a
toMaybe (Inl Unit)
= Nothing
toMaybe (Inr a)
= Just a
conv Maybe
:: ∀a . EP (Maybe a) (Maybe◦a)
conv Maybe
= EP{from = fromMaybe, to = toMaybe }
Fig. 7. Specializing generic values in Haskell (part 1).

50
R. Hinze and J. Jeuring
type List◦a
= Unit :+: a :*: List a
fromList
:: ∀a . List a →List◦a
fromList [ ]
= Inl Unit
fromList (a : as)
= Inr (a :*: as)
toList
:: ∀a . List◦a →List a
toList (Inl Unit)
= [ ]
toList (Inr (a :*: as))
= a : as
conv List
:: ∀a . EP (List a) (List◦a)
conv List
= EP{from = fromList, to = toList}
type GRose◦f a
= a :*: f (GRose f a)
fromGRose
:: ∀f a . GRose f a →GRose◦f a
fromGRose (GBranch a ts) = (a :*: ts)
toGRose
:: ∀f a . GRose◦f a →GRose f a
toGRose (a :*: ts)
= GBranch a ts
conv GRose
:: ∀f a . EP (GRose f a) (GRose◦f a)
conv GRose
= EP{from = fromGRose, to = toGRose }
{- specializing binary encoding -}
encode Maybe
:: ∀a . Encode a →Encode (Maybe a)
encode Maybe encodea = to (mapE Encode conv Maybe) (encode:+: encodeUnit encodea)
encode List
:: ∀a . Encode a →Encode (List a)
encode List encodea
= to (mapE Encode conv List) (
encode:+: encodeUnit (encode:*: encodea (encodeList encodea)))
encode GRose
:: ∀f . (∀b . Encode b →Encode (f b))
→(∀a . Encode a →Encode (GRose f a))
encode GRose encodef encode a
= to (mapE Encode conv GRose) (
encode:*: encodea (encodef (encodeGRose encodef encode a)))
{- specializing equality -}
equal Maybe
:: ∀a1 a2 . Equal a1 a2 →Equal (Maybe a1) (Maybe a2)
equal Maybe equal a
= to (mapE Equal conv Maybe conv Maybe) (equal :+: equal Unit equal a)
equal List
:: ∀a1 a2 . Equal a1 a2 →Equal (List a1) (List a2)
equal List equal a
= to (mapE Equal conv List conv List) (
equal :+: equal Unit (equal :*: equal a (equal List equal a)))
equal GRose
:: ∀f1 f2 . (∀b1 b2 . Equal b1 b2 →Equal (f1 b1) (f2 b2))
→(∀a1 a2 . Equal a1 a2
→Equal (GRose f1 a1) (GRose f2 a2))
equal GRose equal f equal a
= to (mapE Equal conv GRose conv GRose) (
equal :*: equal a (equal f (equal GRose equal f equal a)))
Fig. 8. Specializing generic values in Haskell (part 2).

Generic Haskell: Practice and Theory
51
liftE
:: ∀f . (Functor f) ⇒∀a a◦. EP a a◦→EP (f a) (f a◦)
liftE mapE a = EP{from = fmap (from mapE a), to = fmap (to mapE a)}.
Now, the Poly :: π instance of mapE is given by
mapE Poly
:: MapE{[π]} Poly Poly
mapE Poly mapE a1 . . . mapE ak = mapE{|Poly a1 . . . ak|} ρ.
where ρ = (a1 := mapE a1, . . . , ak := mapE ak) is an environment mapping type
variables to terms. We use for the ﬁrst time an explicit environment in order to
be able to extend the deﬁnition to polymorphic types. Recall that the specializa-
tion of generic values does not work for polymorphic types. However, we allow
polymorphic types to occur in the type signature of a generic value. Fortunately,
we can extend mapE so that it works for universal quantiﬁcation over types of
kind ⋆and kind ⋆→⋆.
mapE{|C|} ρ
= mapE{|C|}
mapE{|a|} ρ
= ρ(a)
mapE{|t u|} ρ
= (mapE{|t|} ρ) (mapE{|u|} ρ)
mapE{|∀a :: ⋆. t|} ρ
= mapE{|t|} ρ(a := idE)
mapE{|∀f :: ⋆→⋆. (Functor f) ⇒t|} ρ = mapE{|t|} ρ(f := liftE).
Two remarks are in order.
Haskell has neither type abstraction nor an explicit recursion operator, so
these cases can be omitted from the deﬁnition.
Unfortunately, we cannot deal with polymorphic types in general. Consider,
for instance, the type Poly a = ∀f . f a →f a. There is no mapping function that
works uniformly for all f. For that reason we have to restrict f to instances of
Functor so that we can use the overloaded liftE function. For polymorphic types
where the type variable ranges over types of kind ⋆things are simpler: since the
mapping function for a manifest type is always the identity, we can use id E.
Now, what happens if Poly involves a user-deﬁned data type, say B? In this
case we have to specialize mapE to B. It seems that we are trapped in a vicious
circle. One possibility to break the spell is to implement mapE for the B data
type ‘by hand’. Fortunately mapE is very well-behaved, so the code generation
is straightforward. The embedding-projection map for the data type B :: κ
data B a1 . . . am = K1 t11 . . . t1m1 | · · · | Kn tn1 . . . tnmn
is given by
mapE B
:: MapE{[κ]} B B
mapE B mapE a1 . . . mapE am
= EP{from = fromB, to = toB}
where
fromB (K1 x11 . . . x1m1) = K1 (from{|t11|} ρ x11) . . . (from{|t1m1|} ρ x1m1)
. . .
fromB (Kn xn1 . . . xnmn) = Kn (from{|tn1|} ρ xn1) . . . (from{|tnmn|} ρ xnmn)
toB (K1 x11 . . . x1m1)
= K1 (to{|t11|} ρ x11) . . . (to{|t1m1|} ρ x1m1)
. . .
toB (Kn xn1 . . . xnmn)
= Kn (to{|tn1|} ρ xn1) . . . (to{|tnmn|} ρ xnmn)

52
R. Hinze and J. Jeuring
where from{|t|} ρ = from (mapE{|t|} ρ), to{|t|} ρ = to (mapE{|t|} ρ), and ρ =
(a1 := mapE a1, . . . , am := mapE am). For example, for Encode and Decodes we
obtain
mapE Encode
:: ∀a a◦. EP a a◦→EP (Encode a) (Encode a◦)
mapE Encode mapE a = mapE →mapE a id E
mapE Decodes
:: ∀a a◦. EP a a◦→EP (Decodes a) (Decodes a◦)
mapE Decodes mapE a = mapE →id E (mapE (,) mapE a idE)
where the mapping function mapE (,) is generated according to the scheme above:
mapE (,)
:: ∀a a◦. EP a a◦→∀b b◦. EP b b◦→EP (a, b) (a◦, b◦)
mapE (,) mapE a mapE b = EP{from = from(,), to = to(,)}
where from(,) (a, b) = (from mapE a a, from mapE b b)
to(,) (a, b)
= (to mapE a a, to mapE b b).
Interestingly, the Generic Haskell compiler does not treat mapE in a special
way. Rather, it uses the same specialization mechanism also for mapE instances.
This is possible because mapE’s type involves only the type EP, for which we
can specialize mapE by hand.
4
Conclusion
We have presented Generic Haskell, an extension of Haskell that allows the def-
inition of generic programs. We have shown how to implement typical examples
of generic programs such as equality, pretty printing, mapping functions and
reductions. The central idea is to deﬁne a generic function by induction on the
structure of types. Haskell possesses a rich type system, which essentially cor-
responds to the simply typed lambda calculus (with kinds playing the rˆole of
types). This type system presents a real challenge: how can we deﬁne generic
functions and how can we assign types to these functions? It turns out that type-
indexed values possess kind-indexed types, types that are deﬁned by induction
on the structure of kinds.
Though generic programming adds an extra level of abstraction to program-
ming, it is in many cases simpler than conventional programming. The funda-
mental reason is that genericity gives you a lot of things for free. For instance, the
generic programmer only has to provide cases for primitive types and for binary
sums and products. Generic Haskell automatically takes care of type abstraction,
type application, and type recursion.
Generic Haskell takes a transformational approach to generic programming:
a generic function is translated into a family of polymorphic functions. We have
seen that this transformation can be phrased as an interpretation of the simply
typed lambda calculus. One of the beneﬁts of this approach—not mentioned in
these notes—is that it is possible to adapt one of the main tools for studying
typed lambda calculi, logical relations, to generic reasoning, see [22]. To prove a

Generic Haskell: Practice and Theory
53
generic property it suﬃces to prove the assertion for type constants. Everything
else is taken care of automatically.
Finally, we have shown how to adapt the technique of specialization to
Haskell, whose type system is based on name equivalence.
Acknowledgement
Thanks are due to Andres L¨oh for implementing the Generic Haskell compiler.
References
1. Artem Alimarine and Rinus Plasmeijer.
A generic programming extension for
Clean. In Th. Arts and M. Mohnen, editors, Proceedings of the 13th International
workshop on the Implementation of Functional Languages, IFL’01, pages 257–278,
lvsj, Sweden, September 2001.
2. Thorsten Altenkirch and Conor McBride.
Generic programming within depen-
dently typed programming. In Jeremy Gibbons and Jeuring Jeuring, editors, Pre-
Proceedings of IFIP TC2 Working Conf. on Generic Programming, WCGP’02,
Dagstuhl, 11–12 July 2002, 2002. (Final Proceedings to be published by Kluwer
Acad. Publ.).
3. Lennart Augustsson. The Haskell B. Compiler (HBC), 1998. Available from
http://www.cs.chalmers.se/~augustss/hbc/hbc.html.
4. Lennart Augustsson.
Cayenne – a language with dependent types.
SIGPLAN
Notices, 34(1):239–250, January 1999.
5. Arthur I. Baars and S. Doaitse Swierstra. Typing dynamic typing. In Simon Peyton
Jones, editor, Proceedings of the 2002 International Conference on Functional Pro-
gramming, Pittsburgh, PA, USA, October 4-6, 2002, pages 157–166. ACM Press,
October 2002.
6. Richard Bird. Introduction to Functional Programming using Haskell. Prentice
Hall Europe, London, 2nd edition, 1998.
7. Richard Bird and Lambert Meertens.
Nested datatypes.
In J. Jeuring, edi-
tor, Fourth International Conference on Mathematics of Program Construction,
MPC’98, Marstrand, Sweden, volume 1422 of Lecture Notes in Computer Science,
pages 52–67. Springer-Verlag, June 1998.
8. Richard Bird and Ross Paterson. Generalised folds for nested datatypes. Formal
Aspects of Computing, 11(2):200–222, 1999.
9. James Cheney and Ralf Hinze.
A lightweight implementation of generics and
dynamics.
In Manuel M.T. Chakravarty, editor, Proceedings of the 2002 ACM
SIGPLAN Haskell Workshop, pages 90–104. ACM Press, October 2002.
10. Robin Cockett and Tom Fukushima.
About Charity.
Yellow Series Report
92/480/18, Dept. of Computer Science, Univ. of Calgary, June 1992.
11. Olivier Danvy.
An extensional characterization of lambda-lifting and lambda-
dropping. In Aart Middeldorp and Taisuke Sato, editors, 4th Fuji International
Symposium on Functional and Logic Programming (FLOPS’99), Tsukuba, Japan,
volume 1722 of Lecture Notes in Computer Science, pages 241–250. Springer-
Verlag, November 1999.

54
R. Hinze and J. Jeuring
12. Vladimir Gapeyev, Michael Y. Levin, and Benjamin C. Pierce. Recursive subtyp-
ing revealed (functional pearl). In Proceedings of the ACM Sigplan International
Conference on Functional Programming (ICFP’00), volume 35 of ACM Sigplan
Notices, pages 221–232, New York, September 2000. ACM Press.
13. G. Gierz, K.H. Hofmann, K Keimel, J.D. Lawson, M. Mislove, and D.S. Scott. A
Compendium of Continuous Lattices. Springer-Verlag, 1980.
14. Jean-Yves Girard.
Interpr´etation fonctionelle et ´elimination des coupures dans
l’arithm´etique d’ordre sup´erieur. PhD thesis, Universit´e Paris VII, 1972.
15. T. Hagino. Category Theoretic Approach to Data Types. PhD thesis, University of
Edinburgh, 1987.
16. Robert Harper and Greg Morrisett. Compiling polymorphism using intensional
type analysis. In Conference record of the 22nd ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages (POPL’95), San Francisco, Cal-
ifornia, pages 130–141. ACM Press, 1995.
17. Fritz Henglein. Type inference with polymorphic recursion. ACM Transactions on
Programming Languages and Systems, 15(2):253–289, April 1993.
18. Ralf Hinze. A generic programming extension for Haskell. In Erik Meijer, editor,
Proceedings of the 3rd Haskell Workshop, Paris, France, September 1999.
The
proceedings appeared as a technical report of Universiteit Utrecht, UU-CS-1999-
28.
19. Ralf Hinze. Polytypic programming with ease (extended abstract). In Aart Middel-
dorp and Taisuke Sato, editors, 4th Fuji International Symposium on Functional
and Logic Programming (FLOPS’99), Tsukuba, Japan, volume 1722 of Lecture
Notes in Computer Science, pages 21–36. Springer-Verlag, November 1999.
20. Ralf Hinze. Functional Pearl: Perfect trees and bit-reversal permutations. Journal
of Functional Programming, 10(3):305–317, May 2000.
21. Ralf Hinze. A new approach to generic functional programming. In Thomas W.
Reps, editor, Proceedings of the 27th Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages (POPL’00), Boston, Massachusetts, Jan-
uary 19-21, pages 119–132, January 2000.
22. Ralf Hinze. Polytypic values possess polykinded types. Science of Computer Pro-
grammming, 43:129–159, 2002.
23. Ralf Hinze and Simon Peyton Jones. Derivable type classes. In Graham Hutton,
editor, Proceedings of the 2000 ACM SIGPLAN Haskell Workshop, volume 41.1 of
Electronic Notes in Theoretical Computer Science. Elsevier Science, August 2001.
The preliminary proceedings appeared as a University of Nottingham technical
report.
24. Patrik Jansson and Johan Jeuring.
PolyP—a polytypic programming language
extension. In Conference Record 24th ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages (POPL’97), Paris, France, pages 470–482.
ACM Press, January 1997.
25. Patrik Jansson and Johan Jeuring. Polytypic compact printing and parsing. In
S. Doaitse Swierstra, editor, Proceedings European Symposium on Programming,
ESOP’99, volume 1576 of Lecture Notes in Computer Science, pages 273–287,
Berlin, 1999. Springer-Verlag.
26. C.B. Jay, G. Bell`e, and E. Moggi. Functorial ML. Journal of Functional Program-
ming, 8(6):573–619, November 1998.
27. C.B. Jay and J.R.B. Cocket. Shapely types and shape polymorphism. In D. Sanella,
editor, Programming Languages and Systems — ESOP’94: 5th European Sympo-
sium on Programming, Edinburgh, UK, Proceedings, volume 788 of Lecture Notes
in Computer Science, pages 302–316, Berlin, 11–13 April 1994. Springer-Verlag.

Generic Haskell: Practice and Theory
55
28. Johan Jeuring and Patrik Jansson. Polytypic programming. In J. Launchbury,
E. Meijer, and T. Sheard, editors, Tutorial Text 2nd International School on Ad-
vanced Functional Programming, Olympia, WA, USA, volume 1129 of Lecture
Notes in Computer Science, pages 68–114. Springer-Verlag, 1996.
29. M.P. Jones and J.C. Peterson. Hugs 98 User Manual, May 1999. Available from
http://www.haskell.org/hugs.
30. G. Malcolm. Algebraic data types and program transformation. PhD thesis, Uni-
versity of Groningen, 1990.
31. Lambert Meertens. Calculate polytypically! In H. Kuchen and S.D. Swierstra, edi-
tors, Proceedings 8th International Symposium on Programming Languages: Imple-
mentations, Logics, and Programs, PLILP’96, Aachen, Germany, volume 1140 of
Lecture Notes in Computer Science, pages 1–16. Springer-Verlag, September 1996.
32. Erik Meijer and Graham Hutton. Bananas in space: Extending fold and unfold
to exponential types. In Conference Record 7th ACM SIGPLAN/SIGARCH and
IFIP WG 2.8 International Conference on Functional Programming Languages and
Computer Architecture, FPCA’95, La Jolla, San Diego, CA, USA, pages 324–333.
ACM Press, June 1995.
33. Robin Milner. A theory of type polymorphism in programming. Journal of Com-
puter and System Sciences, 17(3):348–375, 1978.
34. John C. Mitchell.
Foundations for Programming Languages.
The MIT Press,
Cambridge, MA, 1996.
35. Eugenio Moggi. A cateogry-theoretic account of program modules. Mathematical
Structures in Computer Science, 1(1):103–139, March 1991.
36. Alan Mycroft. Polymorphic type schemes and recursive deﬁnitions. In M. Paul
and B. Robinet, editors, Proceedings of the International Symposium on Program-
ming, 6th Colloquium, Toulouse, France, volume 167 of Lecture Notes in Computer
Science, pages 217–228, 1984.
37. Chris Okasaki. Purely Functional Data Structures. Cambridge University Press,
1998.
38. Simon Peyton Jones. Haskell 98 Language and Libraries. Cambridge University
Press, 2003.
39. Simon L. Peyton Jones. Compiling Haskell by program transformation: A report
from the trenches. In Hanne Riis Nielson, editor, Programming Languages and
Systems—ESOP’96, 6th European Symposium on Programming, Link¨oping, Swe-
den, 22–24 April, volume 1058 of Lecture Notes in Computer Science, pages 18–44.
Springer-Verlag, 1996.
40. Benjamin C. Pierce. Types and programming languages. MIT Press, Cambridge,
Mass., 2002.
41. Fritz Ruehr. Structural polymorphism. In Roland Backhouse and Tim Sheard,
editors, Informal Proceedings Workshop on Generic Programming, WGP’98,
Marstrand, Sweden, 18 June 1998. Dept. of Computing Science, Chalmers Univ.
of Techn. and G¨oteborg Univ., June 1998.
42. Karl Fritz Ruehr. Analytical and Structural Polymorphism Expressed using Pat-
terns over Types. PhD thesis, University of Michigan, 1992.
43. Tim Sheard. Automatic generation and use of abstract structure operators. ACM
Transactions on Programming Languages and Systems, 13(4):531–557, October
1991.
44. Tim Sheard.
Type parametric programming.
Technical Report CS/E 93-018,
Oregon Graduate Institute of Science and Technology, Department of Computer
Science and Engineering, Portland, OR, USA, November 1993.

56
R. Hinze and J. Jeuring
45. The GHC Team. The Glasgow Haskell Compiler User’s Guide, Version 5.04, 2003.
Available from http://www.haskell.org/ghc/documentation.html.
46. Philip Wadler.
Theorems for free!
In The Fourth International Conference on
Functional Programming Languages and Computer Architecture (FPCA’89), Lon-
don, UK, pages 347–359. Addison-Wesley Publishing Company, September 1989.
47. Philip Wadler. The Girard-Reynolds isomorphism. In N. Kobayashi and B. C.
Pierce, editors, Proc. of 4th Int. Symp. on Theoretical Aspects of Computer Science,
TACS 2001, Sendai, Japan, 29–31 Oct. 2001, volume 2215 of Lecture Notes in
Computer Science, pages 468–491. Springer-Verlag, Berlin, 2001.
48. Philip Wadler. A prettier printer. In Jeremy Gibbons and Oege de Moor, editors,
The Fun of Programming, Cornerstones of Computing, pages 223–243. Palgrave
Macmillan Publishers Ltd, March 2003.
49. Stephanie Weirich.
Higher-order intensional type analysis.
In D. Le M´etayer,
editor, Proceedings of the 11th European Symposium on Programming, ESOP 2002,
volume 2305 of Lecture Notes in Computer Science, pages 98–114, 2002.

Generic Haskell: Applications
Ralf Hinze1 and Johan Jeuring2,3
1 Institut f¨ur Informatik III, Universit¨at Bonn
R¨omerstraße 164, 53117 Bonn, Germany
ralf@informatik.uni-bonn.de
http://www.informatik.uni-bonn.de/~ralf/
2 Institute of Information and Computing Sciences, Utrecht University
P.O.Box 80.089, 3508 TB Utrecht, The Netherlands
johanj@cs.uu.nl
http://www.cs.uu.nl/~johanj/
3 Open University, Heerlen, The Netherlands
Abstract. Generic Haskell is an extension of Haskell that supports the
construction of generic programs. These lecture notes discuss three ad-
vanced generic programming applications: generic dictionaries, compress-
ing XML documents, and the zipper: a data structure used to represent
a tree together with a subtree that is the focus of attention, where that
focus may move left, right, up or down the tree. When describing and
implementing these examples, we will encounter some advanced features
of Generic Haskell, such as type-indexed data types, dependencies be-
tween and generic abstractions of generic functions, adjusting a generic
function using a default case, and generic functions with a special case
for a particular constructor.
1
Introduction
A generic (or polytypic, type-indexed) function is a function that can be instan-
tiated on many data types to obtain data type speciﬁc functionality. Examples
of generic functions are the functions that can be derived in Haskell [45], such
as show, read, and ‘ ’. In the ﬁrst part of these lecture notes, entitled Generic
Haskell: Practice and Theory [22], we have introduced generic functions, and we
have shown how to implement them in Generic Haskell [9]. We assume the reader
is familiar with the ﬁrst part of these notes. For an introduction to a related but
diﬀerent kind of generic programming, see Backhouse et al [2].
Why is generic programming important? Generic programming makes pro-
grams easier to write:
– Programs that could only be written in an untyped style can now be written
in a language with types.
– Some programs come for free.
– Some programs are simple adjustments of library functions, instead of com-
plicated traversal functions.
R. Backhouse and J. Gibbons (Eds.): Generic Programming SS 2002, LNCS 2793, pp. 57–96, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

58
R. Hinze and J. Jeuring
Of course not all programs become simpler when you write your programs in
a generic programming language, but, on the other hand, no programs become
more complicated. In this paper we will try to give you a feeling about where
and when generic programs are useful.
These lecture notes describe three advanced generic programming applica-
tions: generic dictionaries, compressing XML documents, and the zipper. The
applications are described in more detail below. In the examples, we will en-
counter several new generic programming concepts:
– Type-indexed data types. A type-indexed data type is constructed in a generic
way from an argument data type [23]. It is the equivalent of a type-indexed
function on the level of data types.
– Default cases. To deﬁne a generic function that is the same as another func-
tion except for a few cases we use a default case [10]. If the new deﬁnition
does not provide a certain case, then the default case applies and copies the
case from another function.
– Constructor cases. A constructor case of a generic program deals with a
constructor of a data type that requires special treatment [10]. Constructor
cases are especially useful when dealing with data types with a large num-
ber of constructors, and only a small number of constructors need special
treatment.
– Dependencies and generic abstractions. To write a generic function that uses
another generic function we can use a dependency or a generic abstrac-
tion [10].
We will introduce these concepts as we go along.
Example 1: Digital Searching. A digital search tree or trie is a search tree scheme
that employs the structure of search keys to organize information. Searching is
useful for various data types, so we would like to allow for keys and information
of any data type. This means that we have to construct a new kind of trie for
each key type. For example, consider the data type String deﬁned by
data String = Nil | Cons Char String.
We can represent string-indexed tries with associated values of type v as follows:
data FMap String v = Null String
| Node String (Maybe v)
(FMapChar (FMap String v)).
Such a trie for strings would typically be used for an index on texts. The construc-
tor Null String represents the empty trie. The ﬁrst component of the constructor
Node String contains the value associated with Nil. The second component of
Node String is derived from the constructor Cons :: Char →String →String.
We assume that a suitable data structure, FMapChar, and an associated look-
up function lookupChar :: ∀v . Char →FMapChar v →Maybe v for characters are

Generic Haskell: Applications
59
predeﬁned. Given these prerequisites we can deﬁne a look-up function for strings
as follows:
lookup String :: String →FMap String v →Maybe v
lookup String s Null String
= Nothing
lookup String Nil (Node String tn tc) = tn
lookup String (Cons c s) (Node String tn tc)
= (lookupChar c 3 lookup String s) tc.
To look up a non-empty string, Cons c s, we look up c in the FMapChar obtaining
a trie, which is then recursively searched for s. Since the look-up functions have
result type Maybe v, we use the reverse monadic composition of the Maybe
monad, denoted by ‘3’, to compose lookup String and lookupChar.
(3)
:: (a →Maybe b) →(b →Maybe c) →a →Maybe c
(f 3 g) a = case f a of {Nothing →Nothing; Just b →g b}
Consider now the data type Bush of binary trees with characters in the leaves:
data Bush = Leaf Char | Bin Bush Bush.
Bush-indexed tries can be represented by the following data type:
data FMap Bush v = Null Bush
| Node Bush (FMapChar v)
(FMap Bush (FMap Bush v)).
Again, we have two components, one to store values constructed by Leaf , and
one for values constructed by Bin. The corresponding look-up function is given
by
lookup Bush :: Bush →FMap Bush v →Maybe v
lookup Bush b Null Bush
= Nothing
lookup Bush (Leaf c) (Node Bush tl tf ) = lookupChar c tl
lookup Bush (Bin bl br) (Node Bush tl tf )
= (lookup Bush bl 3 lookup Bush br) tf .
One can easily recognize that not only the look-up functions, but also the data
types for the tries are instances of an underlying generic pattern. In the following
section we will show how to deﬁne a trie and associated functions generically for
arbitrary data types.
Example 2: Compressing XML Documents. The extensible markup language
XML [49] is used to mark up text with structure information. XML documents
may become (very) large because of the markup that is added to the content. A
good XML compressor can compress an XML document by quite a large factor.
An XML document is usually structured according to a DTD (Document
Type Deﬁnition), a speciﬁcation that describes which tags may be used in the
XML document, and in which positions and order they have to be. A DTD is,

60
R. Hinze and J. Jeuring
in a way, the type of an XML document. An XML document is called valid with
respect to a certain DTD if it follows the structure that is speciﬁed by that
DTD. An XML compressor can use information from the DTD to obtain better
compression. For example, consider the following small XML ﬁle:
<book lang="English">
<title>
Dead Famous </title>
<author> Ben Elton
</author>
<date>
2001
</date>
</book>.
This ﬁle may be compressed by separating the structure from the data, and
compressing the two parts separately. For compressing the structure we can
make good use of the DTD of the document.
<!ELEMENT book
(title,author,date,(chapter)*)>
<!ELEMENT title
(#PCDATA)>
<!ELEMENT author
(#PCDATA)>
<!ELEMENT date
(#PCDATA)>
<!ELEMENT chapter (#PCDATA)>
<!ATTLIST book lang (English | Dutch) #REQUIRED>
If we know how many elements and attributes, say n, appear in the DTD (the
DTD above document contains 6 elements), we can replace the markup of an
element in an XML ﬁle which is valid with respect to the DTD by a natural
number between 0 and n −1, or by log2 n bits. This is one of the main ideas
behind XMill [36]. We improve on XMill by only recording the choices made in an
XML document. In the above document, there is a choice for the language of the
book, and the number of chapters it has. All the other elements are not encoded,
since they can be inferred from the DTD. Section 3 describes a tool based on this
idea, which was ﬁrst described by Jansson and Jeuring in the context of data
conversion [26, 30]. We use HaXml [51] to translate a DTD to a data type, and
we construct generic functions for separating the contents (the strings) and the
shape (the constructors) of a value of a data type, and for encoding the shape
of a value of a data type using information about the (number of) constructors
of the data type.
XML compressors are just one class of XML tools that are easily implemented
as generic programs. Other XML tools that can be implemented as generic pro-
grams are XML editors, XML databases, and XML version management tools.
Example 3: Zipper. The zipper [24] is a data structure that is used to represent a
tree together with a subtree that is the focus of attention, where that focus may
move left, right, up, or down the tree. For example, the zipper corresponding to
the data type Bush, called Loc Bush, is deﬁned by
type Loc Bush
= (Bush, Context Bush)
data Context Bush = Top
| BinL Context Bush Bush
| BinR Bush Context Bush.

Generic Haskell: Applications
61
Using the type of locations we can eﬃciently navigate through a tree. For exam-
ple:
down Bush
:: Loc Bush →Loc Bush
down Bush (Leaf a, c)
= (Leaf a, c)
down Bush (Bin tl tr, c) = (tl, BinL c tr)
right Bush
:: Loc Bush →Loc Bush
right Bush (tl, BinL c tr) = (tr, BinR tl c)
right Bush m
= m.
The navigation function down Bush moves the focus of attention to the leftmost
subtree of the current node; right Bush moves the focus to its right sibling.
Huet [24] deﬁnes the zipper data structure for rose trees and for the data
type Bush, and gives the generic construction in words. In Section 4 we describe
the zipper in more detail and show how to deﬁne a zipper for an arbitrary data
type.
Other Applications of Generic Programming. Besides the applications mentioned
in the examples above, there are several application areas in which generic pro-
gramming can be used.
– Haskell’s deriving construct. Haskell’s deriving construct is used to gener-
ate code for for example the equality function, and for functions for reading
and showing values of data types. Only the classes Eq, Ord, Enum, Bounded,
Show and Read can be derived. The deﬁnitions of (equivalents of) the derived
functions can be found in the library of Generic Haskell.
– Compiler functions. Several functions that are used in compilers are generic
functions: garbage collectors, tracers, debuggers, test tools [7, 34], etc.
– Typed term processing. Functions like pattern matching, term rewriting and
uniﬁcation are generic functions, and have been implemented as generic func-
tions in [31, 28, 29].
The form and functionality of these applications is exactly determined by the
structure of the input data.
Maybe the most common applications of generic programming can be found
in functions that traverse data built from rich mutually-recursive data types with
many constructors, and which perform computations on a single (or a couple of)
constructor(s). For example, consider a function which traverses an abstract
syntax tree and returns the free variables in the tree. Only for the variable
constructor something special has to be done, in all other cases the variables
collected at the children have to be passed on to the parent. This function can be
deﬁned as an instance of a Generic Haskell library function crush [41], together
with a special constructor case for variables [10]. For more examples of generic
traversals, see L¨ammel and Peyton Jones [35].
The Generic Haskell code for the programs discussed in these lecture notes
can be downloaded from the applications page on
http://www.generic-haskell.org/.

62
R. Hinze and J. Jeuring
On Notation. To improve readability, the notation for generic functions we use
in these notes slightly diﬀers from the notation used in the ﬁrst part of these
notes [22]. For example, in the ﬁrst part of these lecture notes we write:
equal{|Char|}
= eqChar
equal{|Int|}
= eqInt
equal{|Unit|} Unit Unit
= True
equal{|:+:|} eqa eqb (Inl a) (Inl a′)
= eqa a a′
equal{|:+:|} eqa eqb (Inl a) (Inr b′)
= False
equal{|:+:|} eqa eqb (Inr b) (Inl a′)
= False
equal{|:+:|} eqa eqb (Inr b) (Inr b′)
= eqb b b′
equal{|:*:|} eqa eqb (a :*: b) (a′ :*: b′) = eqa a a′ ∧eqb b b′
equal{|Con c|} eqa (Con a) (Con b)
= eqa a b.
The function equal is a generic function which recurses over the type structure
of the argument type. The recursion is implicit in the arguments eqa and eqb in
the :+: and :*: cases. In this part of the lecture notes we will instead write:
equal{|Char|}
= eqChar
equal{|Int|}
= eqInt
equal{|Unit|} Unit Unit
= True
equal{|a :+: b|} (Inl a) (Inl a′)
= equal{|a|} a a′
equal{|a :+: b|} (Inl a) (Inr b′)
= False
equal{|a :+: b|} (Inr b) (Inl a′)
= False
equal{|a :+: b|} (Inr b) (Inr b′)
= equal{|b|} b b′
equal{|a :*: b|} (a :*:b) (a′ :*: b′) = equal{|a|} a a′ ∧equal{|b|} b b′
equal{|Con c a|} (Con a) (Con b)
= equal{|a|} a b.
Here the recursion over the type structure is explicit: equal{|a :*:b|} is expressed
in terms of equal{|a|} and equal{|b|}. We think this style is more readable, espe-
cially when a generic function depends on another generic function. Functions
written in the latter style can be translated to the former style and vice versa, so
no expressiveness is lost or gained; the only diﬀerence is readability. The Generic
Haskell compiler does not accept explicit recursive functions yet, but probably
will do so in the near future. A formal description of Generic Haskell with explicit
recursion is given by L¨oh et al. [37].
Organization. The rest of this paper is organized as follows. Section 2 introduces
generic dictionaries, and implements them in Generic Haskell. Section 3 describes
XComprez, a compressor for XML documents. Section 4 develops a generic
zipper data structure. Section 5 summarizes the main points and concludes.
These lecture notes contain exercises. Solutions to the exercises can be found
on the webpage for the Generic Haskell project: www.generic-haskell.org.
2
Generic Dictionaries
A trie is a search tree scheme that employs the structure of search keys to
organize information. Tries were originally devised as a means to represent a

Generic Haskell: Applications
63
collection of records indexed by strings over a ﬁxed alphabet. Based on work by
Wadsworth and others, Connelly et al. [11] generalized the concept to permit
indexing by elements built according to an arbitrary signature. In this section
we go one step further and deﬁne tries and operations on tries generically for
arbitrary data types of arbitrary kinds, including parameterized and nested data
types. The material in this section is largely taken from [18].
2.1
Introduction
The concept of a trie was introduced by Thue in 1912 as a means to represent
a set of strings, see [33]. In its simplest form a trie is a multiway branching tree
where each edge is labelled with a character. For example,
the set of strings {ear, earl, east, easy, eye } is represented by
the trie depicted on the right. Searching in a trie starts at the
root and proceeds by traversing the edge that matches the ﬁrst
character, then traversing the edge that matches the second
character, and so forth. The search key is a member of the
represented set if the search stops in a node that is marked—
marked nodes are drawn as ﬁlled circles on the right. Tries
can also be used to represent ﬁnite maps. In this case marked
nodes additionally contain values associated with the strings.
Interestingly, the move from sets to ﬁnite maps is not a mere
variation of the scheme. As we shall see it is essential for the
further development.
On a more abstract level a trie itself can be seen as a composition of ﬁnite
maps. Each collection of edges descending from the same node constitutes a
ﬁnite map sending a character to a trie. With this interpretation in mind it is
relatively straightforward to devise an implementation of string-indexed tries. If
strings are deﬁned by the following data type:
data String = Nil | Cons Char String,
we can represent string-indexed tries with associated values of type v as follows.
data FMap String v = Null String
| Node String (Maybe v)
(FMapChar (FMap String v))
Here, Null String represents the empty trie. The ﬁrst component of the con-
structor Node String contains the value associated with Nil. Its type is Maybe v
instead of v since Nil may not be in the domain of the ﬁnite map represented by
the trie. In this case the ﬁrst component equals Nothing. The second component
corresponds to the edge map. To keep the introductory example manageable we
implement FMapChar using ordered association lists.

64
R. Hinze and J. Jeuring
type FMapChar v = [(Char, v)]
lookupChar
:: ∀v . Char →FMapChar v →Maybe v
lookupChar c [ ]
= Nothing
lookupChar c ((c′, v) : x)
| c < c′
= Nothing
| c
c′
= Just v
| c > c′
= lookupChar c x
Note that lookupChar has result type Maybe v. If the key is not in the domain
of the ﬁnite map, Nothing is returned.
Building upon lookupChar we can deﬁne a look-up function for strings. To
look up the empty string we access the ﬁrst component of the trie. To look up a
non-empty string, say, Cons c s we look up c in the edge map obtaining a trie,
which is then recursively searched for s.
lookup String :: ∀v . String →FMap String v →Maybe v
lookup String s Null String
= Nothing
lookup String Nil (Node String tn tc)
= tn
lookup String (Cons c s) (Node String tn tc) =
(lookupChar c 3 lookup String s) tc
In the last equation we use monadic composition to take care of the error signal
Nothing.
Based on work by Wadsworth and others, Connelly et al. [11] have gener-
alized the concept of a trie to permit indexing by elements built according to
an arbitrary signature, that is, by elements of an arbitrary non-parameterized
data type. The deﬁnition of lookup String already gives a clue what a suitable
generalization might look like: the trie Node String tn tc contains a ﬁnite map
for each constructor of the data type String; to look up Cons c s the look-up
functions for the components, c and s, are composed. Generally, if we have a
data type with k constructors, the corresponding trie has k components. To look
up a constructor with n ﬁelds, we must select the corresponding ﬁnite map and
compose n look-up functions of the appropriate types. If a constructor has no
ﬁelds (such as Nil), we extract the associated value.
As a second example, consider the data type of external search trees:
data Dict = Tip String | Node Dict String Dict.
A trie for external search trees represents a ﬁnite map from Dict to some value
type v. It is an element of FMap Dict v given by
data FMap Dict v = Null Dict
| Node Dict (FMap String v)
(FMap Dict (FMap String (FMap Dict v))).
The data type FMap Dict is a nested data type, since the recursive call on the
right hand side, FMap Dict (FMap String (FMap Dict v)), is a substitution in-
stance of the left hand side. Consequently, the look-up function on external
search trees requires polymorphic recursion.

Generic Haskell: Applications
65
lookup Dict :: ∀v . Dict →FMap Dict v →Maybe v
lookup Dict d Null Dict
= Nothing
lookup Dict (Tip s) (Node Dict tl tn)
= lookup String s tl
lookup Dict (Node m s r) (Node Dict tl tn) =
(lookup Dict m 3 lookup String s 3 lookup Dict r) tn
Looking up a node involves two recursive calls. The ﬁrst, lookup Dict m, is of
type Dict →FMap Dict X →Maybe X where X = FMap String (FMap Dict v),
which is a substitution instance of the declared type.
Note that it is absolutely necessary that FMap Dict and lookup Dict are
parametric with respect to the codomain of the ﬁnite maps. If we restrict the
type of lookup Dict to Dict →FMap Dict T →Maybe T for some ﬁxed type
T, the deﬁnition no longer type-checks. This also explains why the construction
does not work for the ﬁnite set abstraction.
Generalized tries make a particularly interesting application of generic pro-
gramming. The central insight is that a trie can be considered as a type-indexed
data type. This renders it possible to deﬁne tries and operations on tries gener-
ically for arbitrary data types. We already have the necessary prerequisites at
hand: we know how to deﬁne tries for sums and for products. A trie for a sum
is essentially a product of tries and a trie for a product is a composition of tries.
The extension to arbitrary data types is then uniquely deﬁned. Mathematically
speaking, generalized tries are based on the following isomorphisms.
1 →ﬁn v
∼= v
(t1 + t2) →ﬁn v ∼= (t1 →ﬁn v) × (t2 →ﬁn v)
(t1 × t2) →ﬁn v ∼= t1 →ﬁn (t2 →ﬁn v)
Here, t →ﬁn v denotes the set of all ﬁnite maps from t to v. Note that t →ﬁn v
is sometimes written v[t], which explains why these equations are also known as
the ‘laws of exponentials’.
2.2
Signature
To put the above idea in concrete terms we will deﬁne a type-indexed data type
FMap, which has the following kind for types t of kind ⋆.
FMap{|t :: ⋆|} :: ⋆→⋆
So FMap assigns a type constructor of kind ⋆→⋆to each key type t of kind ⋆.
We will implement the following operations on tries.
empty{|t|}
:: ∀v . FMap{|t|} v
isempty{|t|} :: ∀v . FMap{|t|} v →Bool
single{|t|}
:: ∀v . (t, v) →FMap{|t|} v
lookup{|t|}
:: ∀v . t →FMap{|t|} v →Maybe v
insert{|t|}
:: ∀v . (v →v →v) →(t, v) →(FMap{|t|} v →FMap{|t|} v)
merge{|t|}
:: ∀v . (v →v →v) →(FMap{|t|} v →FMap{|t|} v →FMap{|t|} v)
delete{|t|}
:: ∀v . t →(FMap{|t|} v →FMap{|t|} v)

66
R. Hinze and J. Jeuring
The value empty{|t|} is the empty trie. The function isempty{|t|} takes a trie and
determines whether or not it is empty. The function single{|t|} (t, v) constructs a
trie that contains the binding (t, v) as its only element. The function lookup{|t|}
takes a key and a trie and looks up the value associated with the key. The
function insert{|t|} inserts a new binding into a trie, and merge{|t|} combines two
tries. The function delete{|t|} takes a key and a trie, and removes the binding
for the key from the trie. The two functions insert{|t|} and merge{|t|} take as
a ﬁrst argument a so-called combining function, which is applied whenever two
bindings have the same key. For instance, λnew old →new is used as the
combining function for insert{|t|} if the new binding is to override an old binding
with the same key. For ﬁnite maps of type FMap{|t|} Int addition may also be a
sensible choice. Interestingly, we will see that the combining function is not only
a convenient feature for the user; it is also necessary for deﬁning insert{|t|} and
merge{|t|} generically for all types!
2.3
Properties
Each of the operations speciﬁed in the previous section satisﬁes a number of laws,
which hold generically for all instances of t. These properties formally specify
parts of the informal descriptions of the operations given there, and can be proved
for the deﬁnitions given in the following sections using ﬁxed point induction. See
Hinze [19, 21] for examples of proofs of properties of generic functions.
lookup{|t|} k (empty{|t|}) = Nothing
lookup{|t|} k (single{|t|} (k1, v1)) = if k
k1 then Just v1 else Nothing
lookup{|t|} k (merge{|t|} c t1 t2) = combine c (lookup{|t|} k t1) (lookup{|t|} k t2),
where combine combines two values of type Maybe:
combine :: ∀v . (v →v →v) →(Maybe v →Maybe v →Maybe v)
combine c Nothing Nothing
= Nothing
combine c Nothing (Just v2) = Just v2
combine c (Just v1) Nothing = Just v1
combine c (Just v1) (Just v2) = Just (c v1 v2).
The last law, for instance, states that looking up a key in the merge of two tries
yields the same result as looking up the key in each trie separately and then
combining the results. If the combining form c is associative,
c v1 (c v2 v3) = c (c v1 v2) v3,
then merge{|t|} c is associative, as well. Furthermore, empty{|t|} is the left and
the right unit of merge{|t|} c:
merge{|t|} c (empty{|t|}) x = x
merge{|t|} c x (empty{|t|}) = x
merge{|t|} c x1 (merge{|t|} c x2 x3) = merge{|t|} c (merge{|t|} c x1 x2) x3.

Generic Haskell: Applications
67
The functions insert and delete satisfy the following laws: for all k, v, and c,
insert{|t|} c (k, v) (empty{|t|}) = single{|t|} (k, v)
delete{|t|} k (single{|t|} (k, v)) = empty{|t|}.
The operations satisfy many more laws, but we will omit them here.
2.4
Type-Indexed Tries
We hqe already noted that generalized tries are based on the laws of exponentials.
1 →ﬁn v
∼= v
(t1 + t2) →ﬁn v ∼= (t1 →ﬁn v) × (t2 →ﬁn v)
(t1 × t2) →ﬁn v ∼= t1 →ﬁn (t2 →ﬁn v)
In order to deﬁne the notion of ﬁnite map it is customary to assume that each
value type v contains a distinguished element or base point ⊥v, see [11]. A ﬁnite
map is then a function whose value is ⊥v for all but ﬁnitely many arguments. For
the implementation of tries it is, however, inconvenient to make such a strong
assumption (though one could use type classes for this purpose).
Instead, we explicitly add a base point when necessary motivating the fol-
lowing deﬁnition of FMap, our ﬁrst example of a type-indexed data type.
FMap{|t :: ⋆|}
:: ⋆→⋆
FMap{|Unit|} v
= Maybe v
FMap{|Int|} v
= Patricia.Dict v
FMap{|Char|} v
= FMapChar v
FMap{|t1 :+: t2|} v = FMap{|t1|} v ×• FMap{|t2|} v
FMap{|t1 :*: t2|} v = FMap{|t1|} (FMap{|t2|} v)
FMap{|Con t|} v
= FMap{|t|} v
Here, (×•) is the type of optional pairs.
data a ×• b = Null | Pair a b
Instead of optional pairs we could also use ordinary pairs in the deﬁnition of
FMap:
FMap{|t1 :+:t2|} v = FMap{|t1|} v :*: FMap{|t2|} v.
This representation has, however, two major drawbacks: (i) it relies in an essen-
tial way on lazy evaluation and (ii) it is ineﬃcient, see [18].
We assume there exists a suitable library implementing ﬁnite maps with
integer keys. Such a library could be based, for instance, on a data structure
known as a Patricia tree [44]. This data structure ﬁts particularly well in the
current setting since Patricia trees are a variety of tries. For clarity, we will use
qualiﬁed names when referring to entities deﬁned in the hypothetical module
Patricia.

68
R. Hinze and J. Jeuring
A few remarks are in order. FMap is a type-indexed data type [23]. The
only way to construct values of type FMap is by means of the functions in the
interface.
Furthermore, in contrast with type-indexed functions, the constructor index
Con doesn’t mention a constructor description anymore. This is because a type
cannot depend on a value, so the constructor description can never be used in
the deﬁnition of a type-indexed data type.
Since the trie for the unit type is given by Maybe v rather than v itself,
tries for isomorphic types are, in general, not isomorphic. We have, for in-
stance, Unit ∼= Unit :*: Unit (ignoring laziness) but FMap{|Unit|} v = Maybe v ̸∼=
Maybe (Maybe v) = FMap{|Unit :*:Unit|} v. The trie type Maybe (Maybe v) has
two diﬀerent representations of the empty trie: Nothing and Just Nothing. How-
ever, only the ﬁrst one will be used in our implementation. Similarly, Maybe v ×•
Maybe v has two elements, Null and Pair Nothing Nothing, that represent the
empty trie. Again, only the ﬁrst one will be used.
As mentioned in Section 2.2, the kind of FMap for types of kind ⋆is ⋆→⋆. For
type constructors with higher-order kinds, the kind of FMap looks surprisingly
similar to the type of type-indexed functions for higher-order kinds. A trie on
the type List a is a trie for the type List, applied to a trie for the type a:
FMap{|f :: ⋆→⋆|} :: (⋆→⋆) →(⋆→⋆).
The ‘type’ of a type-indexed type is a kind-indexed kind. In general, we have:
FMap{|f :: κ|}
:: FMAP{[κ]} f
FMAP{[κ :: 2]} :: 2
FMAP{[⋆]}
= ⋆→⋆
FMAP{[κ →ν]} = FMAP{[κ]} →FMAP{[ν]},
where the box 2 is the type of a kind, a so-called super-kind.
Example 1. Let us specialize FMap to the following data types.
data List a
= Nil | Cons a (List a)
data Tree a b = Tip a | Node (Tree a b) b (Tree a b)
data Fork a
= Fork a a
data Sequ a
= EndS | ZeroS (Sequ (Fork a)) | OneS a (Sequ (Fork a))
These types are represented by (see also section 3.1 in the ﬁrst part of these
lecture notes [22]):
List = Fix (ΛList . Λa . Unit :+: a :*:List a)
Tree = Fix (ΛTree . Λa b . a :+:Tree a b :*:b :*:Tree a b)
Fork = Λa . a :*:a
Sequ = Fix (ΛSequ . Λa . Unit :+: Sequ (Fork a) :+: a :*:Sequ (Fork a)).

Generic Haskell: Applications
69
Recall that (:*:) binds stronger than (:+:). Consequently, the corresponding
trie types are
FMap List = Fix (ΛFMap List . Λfa . Maybe ×• fa · FMap List fa)
FMap Tree = Fix (ΛFMap Tree . Λfa fb .
fa ×•
FMap Tree fa fb · fb · FMap Tree fa fb)
FMap Fork = Λfa . fa · fa
FMap Sequ = Fix (ΛFMap Sequ . Λfa .
Maybe ×•
FMap Sequ (FMap Fork fa) ×•
fa · FMap Sequ (FMap Fork fa)).
As an aside, note that we interpret a ×• b ×• c as the type of optional triples
and not as nested optional pairs:
data a ×• b ×• c = Null | Triple a b c.
Now, since Haskell permits the deﬁnition of higher-order kinded data types, the
second-order type constructors above can be directly coded as data types. All
we have to do is to bring the equations into an applicative form.
data FMap List fa v
= Null List
| Node List (Maybe v)
(fa (FMap List fa v))
data FMap Tree fa fb v = Null Tree
| Node Tree (fa v)
(FMap Tree fa fb
(fb (FMap Tree fa fb v)))
These types are the parametric variants of FMap String and FMap Dict deﬁned
in Section 2.1: we have FMap String ≈FMap List FMapChar (corresponding to
String ≈List Char) and FMap Dict ≈FMap Tree FMap String FMap String (cor-
responding to Dict ≈Tree String String). Things become interesting if we con-
sider nested data types.
data FMap Fork fa v = Node Fork (fa (fa v))
data FMap Sequ fa v = Null Sequ
| Node Sequ (Maybe v)
(FMap Sequ (FMap Fork fa) v)
(fa (FMap Sequ (FMap Fork fa) v))
The generalized trie of a nested data type is a second-order nested data type!
A nest is termed second-order, if a parameter that is instantiated in a recursive
call ranges over type constructors of ﬁrst-order kind. The trie FMap Sequ is
a second-order nest since the parameter fa of kind ⋆→⋆is changed in the
recursive calls. By contrast, FMap Tree is a ﬁrst-order nest since its instantiated

70
R. Hinze and J. Jeuring
parameter v has kind ⋆. It is quite easy to produce generalized tries that are
both ﬁrst- and second-order nests. If we swap the components of Sequ’s third
constructor—OneS a (Sequ (Fork a)) becomes OneS (Sequ (Fork a)) a—then the
third component of FMap Sequ has type FMap Sequ (FMap Fork fa) (fa v) and
since both fa and v are instantiated, FMap Sequ is consequently both a ﬁrst- and
a second-order nest.
2.5
Empty Tries
The empty trie is deﬁned as follows.
type Empty{[⋆]} t
= ∀v . FMap{|t|} v
type Empty{[κ →ν]} t = ∀a . Empty{[κ]} a →Empty{[ν]} (t a)
empty{|t :: κ|}
::
Empty{[κ]} t
empty{|Unit|}
= Nothing
empty{|Char|}
= [ ]
empty{|Int|}
= Patricia.empty
empty{|a :+:b|}
= Null
empty{|a :*:b|}
= empty{|a|}
empty{|Con c a|}
= empty{|a|}
The deﬁnition already illustrates several interesting aspects of programming with
generalized tries. First, the explicit polymorphic type of empty is necessary to
make the deﬁnition work. Consider the line empty{|a :*:b|}, which is of type
∀v . FMap{|a|} (FMap{|b|} v). It is deﬁned in terms of empty{|a|}, which is of type
∀v . FMap{|a|} v. That means that empty{|a|} is used polymorphically. In other
words, empty makes use of polymorphic recursion!
Suppose we want to deﬁne a function emptyI that is almost the same as the
function empty, but uses a diﬀerent value, say emptyIntTrie, for the empty trie
for integers. The deﬁnition of FMap says that emptyIntTrie has to be a Patricia
tree, but that might be changed in the deﬁnition of FMap. Then we can use a
default case [10] to deﬁne emptyI in terms of empty as follows:
emptyI {|t :: κ|} :: Empty (κ) t
emptyI {|Int|}
= emptyIntTrie
emptyI {|a|}
= empty{|a|}.
So the function emptyI is equal to the function empty in all cases except for the
Int case, where it uses a special empty trie.
Example 2. Let us specialize empty to lists and binary random-access lists.
empty List :: ∀fa . (∀w . fa w) →(∀v . FMap List fa v)
empty List ea = Null List
empty Fork :: ∀fa . (∀w . fa w) →(∀v . FMap Fork fa v)
empty Fork ea = Node Fork ea
empty Sequ :: ∀fa . (∀w . fa w) →(∀v . FMap Sequ fa v)
empty Sequ ea = Null Sequ

Generic Haskell: Applications
71
The second function, empty Fork, illustrates the polymorphic use of the param-
eter: ea has type ∀w . fa w but is used as an element of fa (fa w). The functions
empty List and empty Sequ show that the ‘mechanically’ generated deﬁnitions
can sometimes be slightly improved: the argument ea is not needed.
The function isempty{|t|} takes a trie and determines whether it is empty.
type IsEmpty{[⋆]} t
= ∀v . FMap{|t|} v →Bool
type IsEmpty{[κ →ν]} t = ∀a . IsEmpty{[κ]} a →IsEmpty{[ν]} (t a)
isempty{|t :: κ|}
:: IsEmpty{[κ]} t
isempty{|Unit|} v
= isNothing v
isempty{|Char|} m
= null m
isempty{|Int|} m
= Patricia.isempty m
isempty{|a :+:b|} Null = True
isempty{|a :+:b|} d
= False
isempty{|a :*:b|} d
= isempty{|a|} d
isempty{|Con c a|} d
= isempty{|a|} d
Function isempty assumes that tries are in ‘normal form’, so the empty trie is
always represented by Null, and not, for example, by Pair Null Null.
Example 3. Let us specialize isempty to lists and binary random-access lists.
isempty List
:: ∀fa . (∀w . fa w →Bool) →
(∀v . FMap List fa v →Bool)
isempty List iea Null List
= True
isempty List iea (Node List tn tc)
= False
isempty Fork
:: ∀fa . (∀w . fa w →Bool) →
(∀v . FMap Fork fa v →Bool)
isempty Fork iea (Node Fork tf )
= iea tf
isempty Sequ
:: ∀fa . (∀w . fa w →Bool) →
(∀v . FMap Sequ fa v →Bool)
isempty Sequ iea Null Sequ
= True
isempty Sequ iea (Node Sequ tv tf ts) = False
2.6
Singleton Tries
The function single{|t|} (t, v) constructs a trie that contains the binding (t, v)
as its only element. To construct a trie in the sum case, we have to return a
Pair, of which only one component is inhabited. The other component is the
empty trie. This means that single depends on empty. Generic Haskell supports
dependencies, so we can use both the empty trie and the single trie in the sum,
product, and constructor cases of function single. The dependency shows in the
type of the function single: on higher-order kinds, the type mentions the type of
empty.
type Single{[⋆]} t
= ∀v . (t, v) →FMap{|t|} v
type Single{[κ →ν]} t = ∀a . Empty{[κ]} a →Single{[κ]} a →Single{[ν]} (t a)

72
R. Hinze and J. Jeuring
Plain generic functions can be seen as catamorphisms [38, 42] over the structure
of data types. With dependencies, we also get the power of paramorphisms [40].
single{|t :: κ|}
:: Single{[κ]} t
single{|Unit|} (Unit, v)
= Just v
single{|Char|} (c, v)
= [(c, v)]
single{|Int|} (i, v)
= Patricia.single (i, v)
single{|a :+:b|} (Inl a, v)
= Pair (single{|a|} (a, v)) (empty{|b|})
single{|a :+:b|} (Inr b, v)
= Pair (empty{|a|}) (single{|b|} (b, v))
single{|a :*:b|} (a :*:b, v) = single{|a|} (a, single{|b|} (b, v))
single{|Con c a|} (Con b, v) = single{|a|} (b, v)
Example 4. Let us again specialize the generic function to lists and binary
random-access lists.
single List :: ∀k fa . (∀w . fa w) →(∀w . (k, w) →fa w)
→(∀v . (List k, v) →FMap List fa v)
single List ea sa (Nil, v)
= Node List (Just v) ea
single List ea sa (Cons k ks, v) =
Node List Nothing (sa (k, single List ea sa (ks, v)))
single Fork :: ∀k fa . (∀w . fa w) →(∀w . (k, w) →fa w)
→(∀v . (Fork k, v) →FMap Fork fa v)
single Fork ea sa (Fork k1 k2, v) = Node Fork (sa (k1, sa (k2, v)))
single Sequ :: ∀k fa . (∀w . fa w) →(∀w . (k, w) →fa w)
→(∀v . (Sequ k, v) →FMap Sequ fa v)
single Sequ ea sa (EndS, v)
= Node Sequ (Just v) Null Sequ ea
single Sequ ea sa (ZeroS s, v)
=
Node Sequ Nothing
(single Sequ (empty Fork ea) (single Fork ea sa) (s, v))
ea
single Sequ ea sa (OneS k s, v) =
Node Sequ Nothing
Null Sequ
(sa (k, single Sequ (empty Fork ea) (single Fork ea sa) (s, v)))
Again, we can simplify the ‘mechanically’ generated deﬁnitions: since the deﬁni-
tion of Fork does not involve sums, single Fork does not require its ﬁrst argument,
ea, which can be safely removed.
2.7
Look up
The look-up function implements the scheme discussed in Section 2.1.
type Lookup{[⋆]} t
= ∀v . t →FMap{|t|} v →Maybe v
type Lookup{[κ →ν]} t = ∀a . Lookup{[κ]} a →Lookup{[ν]} (t a)

Generic Haskell: Applications
73
lookup{|t :: κ|}
:: Lookup{[κ]} t
lookup{|Unit|} Unit fm
= fm
lookup{|Char|} c fm
= lookupChar c fm
lookup{|Int|} i fm
= Patricia.lookup i fm
lookup{|a :+:b|} t Null
= Nothing
lookup{|a :+:b|} (Inl a) (Pair fma fmb) = lookup{|a|} a fma
lookup{|a :+:b|} (Inr b) (Pair fma fmb) = lookup{|b|} b fmb
lookup{|a :*:b|} (a :*: b) fma
= (lookup{|a|} a 3 lookup{|b|} b) fma
lookup{|Con d a|} (Con b) fm
= lookup{|a|} b fm
On sums the look-up function selects the appropriate map; on products it ‘com-
poses’ the look-up functions for the components. Since lookup has result type
Maybe v, we use monadic composition.
Example 5. Specializing lookup{|t|} to concrete instances of t is by now probably
a matter of routine.
lookup List :: ∀k fa . (∀w . k →fa w →Maybe w)
→(∀v . List k →FMap List fa v →Maybe v)
lookup List la ks Null List
= Nothing
lookup List la Nil (Node List tn tc)
= tn
lookup List la (Cons k ks) (Node List tn tc)
= (la k 3 lookup List la ks) tc
lookup Fork :: ∀k fa . (∀w . k →fa w →Maybe w)
→(∀v . Fork k →FMap Fork fa v →Maybe v)
lookup Fork la (Fork k1 k2) (Node Fork tf )
= (la k1 3 la k2) tf
lookup Sequ :: ∀fa k . (∀w . k →fa w →Maybe w)
→(∀v . Sequ k →FMap Sequ fa v →Maybe v)
lookup Sequ la s Null Sequ
= Nothing
lookup Sequ la EndS (Node Sequ te tz to)
= te
lookup Sequ la (ZeroS s) (Node Sequ te tz to)
=
lookup Sequ (lookup Fork la) s tz
lookup Sequ la (OneS a s) (Node Sequ te tz to) =
(la a 3 lookup Sequ (lookup Fork la) s) to
The function lookup List generalizes lookup String deﬁned in Section 2.1; we
have lookup String ≈lookup List lookupChar.
2.8
Inserting and Merging
Insertion is deﬁned in terms of merge and single.
insert{|t :: ⋆|}
:: ∀v . (v →v →v) →(t, v) →FMap{|t|} v →FMap{|t|} v
insert{|t|} c (x, v) d = merge{|t|} c (single{|t|} (x, v)) d
Function insert is deﬁned as a generic abstraction. A generic abstraction is
a generic function that is deﬁned in terms of another generic function. The
abstracted type parameter is, however, restricted to types of a ﬁxed kind. In

74
R. Hinze and J. Jeuring
the above case, insert only works for types of kind ⋆. In the exercise at the end
of this section you will deﬁne insert as a generic function that works for type
constructors of all kinds.
Merging two tries is surprisingly simple. Given the function combine deﬁned
in section 2.3, and a function for merging two association lists
mergeChar
:: ∀v . (v →v →v) →
(FMapChar v →FMapChar v →FMapChar v)
mergeChar c [ ] x ′ = x ′
mergeChar c x [ ] = x
mergeChar c ((k, v) : x) ((k ′, v ′) : x ′)
| k < k′
= (k, v) : mergeChar c x ((k ′, v ′) : x ′)
| k
k ′
= (k, c v v ′) : mergeChar c x x ′
| k > k ′
= (k ′, v ′) : mergeChar c ((k, v) : x) x ′,
we can deﬁne merge as follows.
type Merge{[⋆]} t = ∀v .
(v →v →v) →FMap{|t|} v →FMap{|t|} v →FMap{|t|} v
type Merge{[κ →ν]} t = ∀a . Merge{[κ]} a →Merge{[ν]} (t a)
merge{|t :: κ|}
:: Merge{[κ]} t
merge{|Unit|} c v v ′
= combine c v v ′
merge{|Char|} c fm fm′ = mergeChar c fm′ fm
merge{|Int|} c fm fm′
= Patricia.merge c fm′ fm
merge{|Con d a|} c e e′ = merge{|a|} c e e′
For the sum case, we have to distinguish between empty and nonempty tries:
merge{|a :+:b|} c d Null
= d
merge{|a :+:b|} c Null d
= d
merge{|a :+:b|} c (Pair x y) (Pair v w) =
Pair (merge{|a|} c x v) (merge{|b|} c y w).
The most interesting equation is the product case. The tries d and d ′ are of
type FMap{|a|} (FMap{|b|} v). To merge them we can recursively call merge{|a|};
we must, however, supply a combining function of type ∀v . FMap{|b|} v →
FMap{|b|} v →FMap{|b|} v. A moment’s reﬂection reveals that merge{|b|} c
is the desired combining function.
merge{|a :*:b|} c d d′ = merge{|a|} (merge{|b|} c) d d′
The deﬁnition of merge shows that it is sometimes necessary to implement op-
erations more general than immediately needed. If Merge{[⋆]} t had been the
simpler type ∀v . FMap{|t|} v →FMap{|t|} v →FMap{|t|} v, then we would not
have been able to give a deﬁning equation for :*:.
Example 6. To complete the picture let us again specialize the merging operation
for lists and binary random-access lists. The diﬀerent instances of merge are
surprisingly concise (only the types look complicated).

Generic Haskell: Applications
75
merge List :: ∀fa . (∀w . (w →w →w) →(fa w →fa w →fa w))
→(∀v . (v →v →v)
→FMap List fa v →FMap List fa v →FMap List fa v)
merge List ma c Null List t
= t
merge List ma c t Null List
= t
merge List ma c (Node List tn tc) (Node List tn′ tc′)
=
Node List (combine c tn tn′)
(ma (merge List ma c) tc tc′)
merge Fork :: ∀fa . (∀w . (w →w →w) →(fa w →fa w →fa w))
→(∀v . (v →v →v)
→FMap Fork fa v →FMap Fork fa v →FMap Fork fa v)
merge Fork ma c (Node Fork tf ) (Node Fork tf ′)
=
Node Fork (ma (ma c) tf tf ′)
merge Sequ :: ∀fa . (∀w . (w →w →w) →(fa w →fa w →fa w))
→(∀v . (v →v →v)
→FMap Sequ fa v →FMap Sequ fa v →FMap Sequ fa v)
merge Sequ ma c Null Sequ t
= t
merge Sequ ma c t Null Sequ
= t
merge Sequ ma c (Node Sequ te tz to) (Node Sequ te′ tz ′ to′) =
Node Sequ (combine c te te′)
(merge Sequ (merge Fork ma) c tz tz ′)
(ma (merge Sequ (merge Fork ma) c) to to′)
2.9
Deleting
Function delete{|t|} takes a key and a trie, and removes the binding for the key
from the trie. For the Char case we need a help function that removes an element
from an association list:
deleteChar :: ∀v . Char →FMapChar v →FMapChar v,
and similarly for the Int case. Function delete is deﬁned as follows:
delete{|t :: κ|}
:: Delete{[κ]} t
delete{|Unit|} Unit fm = Nothing
delete{|Char|} c fm
= deleteChar c fm
delete{|Int|} i fm
= Patricia.delete i fm.
All cases except the product case are straightforward. In the product case, we
have to remove a binding for a product (a :*: b). We do this by using a to lookup
the trie d in which there is a binding for b. Then we remove the binding for b
in d, obtaining a trie d′. If d′ is empty, then we delete the complete binding
for a in d, otherwise we insert the binding (a, d′) in the original trie d. Here
we pass as a combining function λx y →x, which overwrites existing bindings
in a trie. From this description it follows that the function delete depends on
the functions lookup, insert (which depends on function empty), and isempty.

76
R. Hinze and J. Jeuring
Here we need the kind-indexed typed version of function insert, as deﬁned in
the exercise at the end of this section.
type Delete{[⋆]} t
= ∀v . t →FMap{|t|} v →FMap{|t|} v
type Delete{[κ →ν]} t = ∀a . Lookup{[κ]} a
→Insert{[κ]} a
→IsEmpty{[κ]} a
→Empty{[κ]} a
→Delete{[κ]} a
→Delete{[ν]} (t a)
delete{|a :+:b|} t Null
= Null
delete{|a :+:b|} (Inl a) (Pair x y) = Pair (delete{|a|} a x) y
delete{|a :+:b|} (Inr b) (Pair x y) = Pair x (delete{|b|} b y)
delete{|a :*:b|} (a :*: b) d
=
case (lookup{|a|} a 3 delete{|b|} b) d of
Nothing
→d
Just d′ | isempty{|b|} d′
→delete{|a|} a d
| otherwise
→insert{|a|} (λx y →x) (a, d′) d
delete{|Con c a|} (Con b) d
= delete{|a|} b d
Function delete should also maintain the invariant that the empty trie is repre-
sented by Null, and not by Pair Null Null, for example. It is easy to adapt the
above deﬁnition such that this invariant is maintained.
Since the type of delete is rather complex because of the dependencies, we
only give the instance of delete on List.
delete List :: ∀k fa . (∀w . k →fa w →Maybe w)
→(∀w . (w →w →w) →(k, w) →fa w →fa w)
→(∀w . fa w →Bool)
→(∀w . fa w)
→(∀w . (k →fa w →fa w))
→(∀v . List k →FMap List fa v →FMap List fa v)
delete List la ia iea ea da Nil Null List
=Null List
delete List la ia iea ea da Nil (Node List tn tc)
=Node List Nothing tc
delete List la ia iea ea da (Cons a as) Null List
=Null List
delete List la ia iea ea da (Cons a as) (Node List tn tc)=
case (la a 3 delete List la ia iea ea da as) tc of
Nothing →tc
Just tc′ | iea tc′ →da a tc
| otherwise →ia (λx y →x) (a, tc′) tc
2.10
Related Work
Knuth [33] attributes the idea of a trie to Thue who introduced it in a pa-
per about strings that do not contain adjacent repeated substrings [47]. De la

Generic Haskell: Applications
77
Briandais [13] recommended tries for computer searching. The generalization of
tries from strings to elements built according to an arbitrary signature was dis-
covered by Wadsworth [50] and others independently since. Connelly et al. [11]
formalized the concept of a trie in a categorical setting: they showed that a trie
is a functor and that the corresponding look-up function is a natural transfor-
mation.
The ﬁrst implementation of generalized tries was given by Okasaki in his
recent textbook on functional data structures [43]. Tries for parameterized types
like lists or binary trees are represented as Standard ML functors. While this
approach works for regular data types, it fails for nested data types such as Sequ.
In the latter case data types of second-order kind are indispensable.
Exercise 1. Deﬁne function insert as a generic function with a kind-indexed
kind. You can download the code for the functions described in this section from
http://www.generic-haskell.org, including the solution to this exercise. You
might want to avoid looking at the implementation of insert while solving this
exercise.
Exercise 2. Deﬁne a function update, which updates a binding in a trie.
update{|t|} :: ∀v . (t, v) →FMap{|t|} v →Maybe (FMap{|t|} v)
If there is no binding for the value of type t in trie of type FMap{|t|} v, update
returns Nothing.
3
XComprez: A Generic XML Compressor
The extensible markup language XML is a popular standard for describing doc-
uments with markup (or structure). XML documents may become (very) large
because of the markup that is added to the content. A lot of diskspace and
bandwidth is used to store and send XML documents. XML compressors reduce
the size of an XML document, sometimes by a considerable factor. This section
describes a generic XML compressor based on the ideas described in the context
of data conversion by Jansson and Jeuring [26, 30].
This section shows how an XML compressor is implemented as a generic
program, and it brieﬂy discusses which other classes of XML tools would proﬁt
from an implementation as a generic program. The example shows how Generic
Haskell can be used to implement XML tools whose behaviour depends on the
DTD or Schema of the input XML document. Example tools include XML edi-
tors, databases, and compressors.
Generic Haskell is ideally suited for implementing XML tools:
– Knowledge of the DTD can be used to provide more precise functionality,
such as manipulations of an XML document that preserve validity in an
XML editor, or better compression in an XML compressor.
– Generic Haskell programs are typed. Consequently, valid documents are
transformed to valid documents, possibly structured according to another
DTD. Thus Generic Haskell supports constructing type correct XML tools.

78
R. Hinze and J. Jeuring
– The generic features of Generic Haskell make XML tools easier to implement
in a surprisingly small amount of code.
– The Generic Haskell compiler may perform all kinds of advanced optimi-
sations on the code, such as partial evaluation or deforestation, which are
diﬃcult to conceive or implement by an XML tool developer.
3.1
Implementing an XML Compressor as a Generic Program
We have implemented an XML compressor, called XComprez, as a generic pro-
gram. XComprez separates structure from contents, compresses the structure
using knowledge about the DTD, and compresses the contents using the Unix
compress utility [52]. Thus we replace each element, or rather, the pair of open
and close keywords of the element, by the minimal number of bits required for
the element given the DTD. We distinguish four components in the tool:
– a component that translates a DTD to a data type,
– a component that separates a value of a data type into its structure and its
contents,
– a component that encodes the structure replacing constructors by bits,
– and a component for compressing the contents.
Of course, we have also implemented a decompressor, but since it is very similar
to the compressor, we omit its description. See the website for XComprez [32]
for the latest developments on XComprez. The Generic Haskell source code for
XComprez can be obtained from the website.
Translating a DTD to a Data Type. A DTD can be translated to one or more
Haskell data types. For example, the following DTD:
<!ELEMENT book
(title,author,date,(chapter)*)>
<!ELEMENT title
(#PCDATA)>
<!ELEMENT author
(#PCDATA)>
<!ELEMENT date
(#PCDATA)>
<!ELEMENT chapter (#PCDATA)>
<!ATTLIST book lang (English | Dutch) #REQUIRED> ,
can be translated to the following data types:
data Book
= Book Book Attrs Title Author Date [Chapter]
data Book Attrs
= Book Attrs{bookLang :: Lang}
data Lang
= English | Dutch
newtype Title
= Title String
newtype Author = Author String
newtype Date
= Date String
newtype Chapter = Chapter String.
We have used the Haskell library HaXml [51], in particular the functionality in
the module DtdToHaskell to obtain a data type from a DTD, together with func-
tions for reading (parsing) and writing (pretty printing) valid XML documents

Generic Haskell: Applications
79
to and from a value of the generated data type. For example, the following value
of the above DTD:
<book lang="English">
<title>
Dead Famous
</title>
<author> Ben Elton
</author>
<date>
2001
</date>
<chapter>Introduction </chapter>
<chapter>Preliminaries</chapter>
</book> ,
is translated to the following value of the data type Book:
Book Book Attrs { bookLang = English }
(Title "␣␣Dead␣Famous␣␣")
(Author "␣Ben␣Elton␣␣␣␣")
(Date "␣␣␣2001␣␣␣␣␣␣␣␣␣")
[Chapter "Introduction␣"
, Chapter "Preliminaries"
].
An element is translated to a value of a data type using just constructors and no
labelled ﬁelds. An attribute is translated to a value that contains a labelled ﬁeld
for the attribute. Thus we can use the Generic Haskell constructs Con and Label
to distinguish between elements and attributes in generic programs. We have
not introduced the Label construct in these lecture notes. It it used to represent
record labels in data types, and is very similar to the Con construct.
Separating Structure and Contents. The contents of an XML document is ob-
tained by extracting all PCData (Parsable Character Data: characters without
tags) and all CData (Character Data: characters with possibly tags, starting
with ‘<![CDATA[’ ending in ‘]]>’) from the document. In Generic Haskell, the
contents of a value of a data type is obtained by extracting all strings from the
value. For the above example value, we obtain the following result:
["␣␣Dead␣Famous␣␣"
, "␣Ben␣Elton␣␣␣␣"
, "␣␣␣2001␣␣␣␣␣␣␣␣␣"
, "Introduction␣"
, "Preliminaries"
].
The generic function extract, which extracts all strings from a value of a data
type, is deﬁned as follows:

80
R. Hinze and J. Jeuring
type Extract{[⋆]} t
= t →[String]
type Extract{[κ →ν]} t
= ∀a . Extract{[κ]} a →Extract{[ν]} (t a)
extract{|t :: κ|}
:: Extract{[κ]} t
extract{|Unit|} Unit
= [ ]
extract{|String|} s
= [s ]
extract{|a :+:b|} (Inl x)
= extract{|a|} x
extract{|a :+:b|} (Inr y)
= extract{|b|} y
extract{|a :*:b|} (x :*:y) = extract{|a|} x ++ extract{|b|} y
extract{|Con c a|} (Con b) = extract{|a|} b.
Note that it is possible to give special instances of a generic function on a partic-
ular type, as with extract{|String|} in the above deﬁnition. Furthermore, because
DtdToHaskell translates any DTD to a data type of kind ⋆, we could have de-
ﬁned extract just on data types of kind ⋆. However, higher-order kinds pose no
problems. Finally, the operator ++ in the product case is a source of ineﬃciency.
It can be removed using a standard transformation, see Exercise 8 in the ﬁrst
part of these lecture notes.
The structure from an XML document is obtained by removing all PCData
and CData from the document. In Generic Haskell, the structure, or shape, of
a value is obtained by replacing all strings by empty tuples. Thus we obtain a
value that has a diﬀerent type, in which occurrences of the type String have been
replaced by the type (). This is another example of a type-indexed data type [23].
For example, the type we obtain from the data type Book is isomorphic to the
following data type:
data ShapeBook
= ShapeBook ShapeBook Attrs
ShapeTitle
ShapeAuthor
ShapeDate
[ShapeChapter]
data ShapeBook Attrs
= ShapeBook Attrs{bookLang :: ShapeLang}
data ShapeLang
= SHAPEEnglish | SHAPEDutch
newtype ShapeTitle
= ShapeTitle ()
newtype ShapeAuthor = ShapeAuthor ()
newtype ShapeDate
= ShapeDate ()
newtype ShapeChapter = ShapeChapter (),
and the structure of the example value is
shapeBook = ShapeBook ( ShapeBook Attrs{bookLang = SHAPEEnglish })
( ShapeTitle ())
( ShapeAuthor ())
( ShapeDate ())
[ ShapeChapter ()
, ShapeChapter ()
] .

Generic Haskell: Applications
81
The type-indexed data type Shape replaces occurrences of String in a data type
by Unit.
Shape{|Unit|}
= Unit
Shape{|String|} = ()
Shape{|a :+:b|} = Shape{|a|} :+:Shape{|b|}
Shape{|a :*:b|} = Shape{|a|} :*:Shape{|b|}
Shape{|Con a|} = Con (Shape{|a|})
The generic function shape returns the shape of a value of any data type. It has
the following kind-indexed type.
type Shape{[⋆]} t
= t →Shape{|t|}
type Shape{[κ →ν]} t = ∀a . Shape{[κ]} a →Shape{[ν]} (t a)
Note that we use the same name both for the kind-indexed type of function
shape, as well as the type-indexed data type Shape. They can be distinguished
based on their index.
shape{|t :: κ|}
:: Shape{[κ]} t
shape{|Unit|} Unit
= Unit
shape{|String|} s
= ()
shape{|a :+: b|} (Inl a)
= Inl (shape{|a|} a)
shape{|a :+: b|} (Inr b)
= Inr (shape{|b|} b)
shape{|a :*: b|} (a :*: b) = (shape{|a|} a :*:shape{|b|} b)
shape{|Con c a|} (Con b) = Con (shape{|a|} b)
Given the shape and the contents (obtained by means of function extract) of a
value we obtain the original value by means of function insert:
insert{|t :: ⋆|} :: Shape{|t|} →[String] →t.
The generic deﬁnition (with a kind-indexed type) of insert is left as an exercise.
Encoding Constructors. The constructor of a value is encoded as follows. First
calculate the number n of constructors of the data type. Then calculate the
position of the constructor in the list of constructors of the data type. Finally,
replace the constructor by the bit representation of its position, using log2 n
bits. For example, in a data type with 6 constructors, the third constructor is
encoded by 010. We start counting with 0. Furthermore, a value of a data type
with a single constructor is represented using 0 bits. Consequently, the values
of all types except for String and Lang in the running example are represented
using 0 bits.
We assume there exists a function constructorPosition which given a con-
structor returns a pair of integers: its position in the list of constructors of the
data type, and the number of constructors of the data type.
constructorPosition :: ConDescr →(Int, Int)

82
R. Hinze and J. Jeuring
Function constructorPosition can be deﬁned by means of function constructors,
which returns the constructor descriptions of a data type. This function is deﬁned
in the module Collect, which can be found in the library of Generic Haskell.
constructors{|t :: ⋆|} :: [ConDescr]
Function constructors is deﬁned for arbitrary kinds in module Collect. We omit
the deﬁnitions of both function constructors and function constructorPosition.
The function encode takes a value, and encodes it as a value of type Bin, a
list of bits, deﬁned in the ﬁrst part of these lecture notes. The diﬀerence with
the function encode that is deﬁned in the ﬁrst part of these lecture notes is that
here we encode the constructors of the value, and not the choices made in the
sum. On average, the function encode given here compresses much better than
the function encode from the ﬁrst part of these lecture notes.
type Encode{[⋆]} t
= Shape{|t|} →Bin
type Encode{[κ →ν]} t = ∀a . Encode{[κ]} a →Encode{[ν]} (t a)
The interesting case in the deﬁnition of function encode is the constructor case.
We ﬁrst give the simple cases:
encode{|t :: κ|}
:: Encode{[κ]} t
encode{|Unit|}
= [ ]
encode{|String|}
= [ ]
encode{|a :*: b|} (a :*: b) = encode{|a|} a ++ encode{|b|} b
encode{|a :+: b|} (Inl a)
= encode{|a|} a
encode{|a :+: b|} (Inr b)
= encode{|b|} b.
For Unit and String there is nothing to encode. The product case encodes the
components of the product, and concatenates the results. The sum case strips
of the Inl or Inr constructor, and encodes the argument.
The encoding happens in the constructor case of function encode. We use
function intinrange2bits to calculate the bits for the position of the argument
constructor in the constructor list, given the number of constructors of the data
type currently in scope. The deﬁnition of intinrange2bits is omitted.
encode{|Con c a|} (Con a) =
intinrange2bits (constructorPosition c) ++ encode{|a|} a
intinrange2bits :: (Int, Int) →Bin
We omit the deﬁnition of the function to decode a list of bits into a value of a
data type. This function is the inverse of function encode deﬁned in this section,
and is very similar to the function decodes given in the ﬁrst part of these lecture
notes.
Compressing the Contents. Finally, it remains to compress the contents of an
XML document. At the moment we use the Unix compress utility [52] to com-
press the strings obtained from the document.

Generic Haskell: Applications
83
3.2
Analysis
How does XComprez perform, and how does it compare with other XML com-
pressors? The analysis given in this section is limited: XComprez is used as
an example of a generic program, and not as the latest development in XML
compression. Furthermore, we have not been able to obtain the executables or
the source code of most of the existing XML compressors.
Existing XML Compressors. Structure-speciﬁc compression methods give much
better compression results [4, 15, 14, 46] than conventional compression meth-
ods such as the Unix compress utility [52]. There exist many XML compres-
sors; we know of XMLZip [12], XMill [36], ICT’s XML-Xpress [25], Millau [16],
XMLPPM [6], XGrind [48], and lossy XML compression [5]. We will not perform
an exhaustive comparison between our compressor and these compressors, but
we will brieﬂy compare our compressor with XMill.
Compression Ratio. We have performed some initial tests comparing XCom-
prez and XMill. The tests are not representative, and it is impossible to draw
hard conclusions from the results. However, on our test examples XComprez is
40% to 50% better than XMill. We think this improvement in compression ratio
is considerable. When we replace HaXml by a tool that generates a data type
for a schema, we expect that we can achieve better compression ratios.
Code Size. With respect to code size, the diﬀerence between XMill and XCom-
prez is dramatic: XMill is written in almost 20k lines of C++. The main func-
tionality of XComprez is less than 300 lines of Generic Haskell code. Of course,
for a fair comparison we have to add some of the HaXml code (which is a library
distributed together with almost all compiler and interpreters for Haskell), the
code for handling bits, and the code for implementing the as yet unimplemented
features of XMill. We expect to be able implement all of XMill’s features in
about 20% of the code size of XMill.
Extensions of XComprez. A relatively simple way to improve XComprez it
is to analyze some source ﬁles that are valid with respect to the DTD, count
the number of occurrences of the diﬀerent elements (constructors), and apply
Huﬀman coding. We have implemented this rather simple extension [32].
XMill stores the strings in the elements in so-called containers. The standard
approach in XMill is to use diﬀerent containers for diﬀerent elements, so that,
for example, all authors are stored in the author container, all dates in the date
container, etc. Since the strings in for example the container for dates are very
similar, standard compression methods can compress the containers with a larger
factor than the single ﬁle obtained by storing all strings that appear in the XML
document. Again, it is easy to implement this feature as a generic program [32].
Finally, we have also used (Adaptive) Arithmetic Coding [3] to compress the
constructors.

84
R. Hinze and J. Jeuring
Specializing XComprez. XComprez can not only be used for compressing
XML documents, but also for compressing values of arbitrary data types that
have not necessarily been generated by DtdToHaskell.
Suppose we have a data type that contains a constructor Age, which takes
an integer as argument and denotes the age of a human being. Since 128 is
currently a safe upperbound for the age of a human being, it suﬃces to use 7
bits for the integer argument of Age. Suppose compressAge calculates these 7
bits from an age. Then we can reuse the deﬁnition of encode together with a
constructor case [10] to deﬁne a function specialEncode.
specialEncode{|t :: κ|}
:: Encode{[κ]} t
specialEncode{|case Age|} (Age i) = compressAge i
specialEncode{|a|}
= encode{|a|}
Function specialEncode is still a generic encoding function, but on values of the
form Age i it uses a diﬀerent compress function.
3.3
Conclusions
We have shown how to implement an XML compressor as a generic program.
XComprez compresses better than for example XMill because it uses the infor-
mation about an XML document present in a DTD.
There exist several other classes of XML tools that can be implemented as
generic programs, and that would beneﬁt from such an implementation. Exam-
ples of such tools are XML editors and XML databases [17]. The combination
of HaXml and generic programming as in Generic Haskell is very useful for
implementing the kind of XML tools for which DTDs play an important rˆole.
Using generic programming, such tools become easier to write, because a lot
of the code pertaining to DTD handling and optimisation is obtained from the
generic programming compiler, and the resulting tools are more eﬀective, because
they directly depend on the DTD. For example, a DTD-aware XML compres-
sor, such as XComprez described in this paper, compresses considerably better
than XML compressors that don’t take the DTD into account, such as XMill.
Furthermore, our compressor is much smaller than XMill.
Although we think Generic Haskell is very useful for developing DTD-aware
XML tools, there are some features of XML tools that are diﬃcult to express
in Generic Haskell. Some of the functionality in the DOM, such as the meth-
ods childNodes and firstChild in the Node interface, is hard to express in
a typed way. A ﬂexible extension of type-indexed data types [23] might oﬀer
a solution to this problem. We believe that fusing HaXml, or a tool based on
Schemas, with Generic Haskell, obtaining a ‘domain-speciﬁc’ language [8] for
generic programming on DTDs or Schemas is a promising approach.
For tools that do not depend on a DTD we can use the untyped approach
from HaXml to obtain a tool that works for any document. However, most of
the advantages of generic programming no longer apply.

Generic Haskell: Applications
85
Exercise 3. Adapt the function extract such that it returns a list of containers,
where a container is a list of strings. Return a (possibly empty) container for
every constructor name.
Exercise 4. There might be many empty containers when using the approach
from the previous exercise. Analyse a data type for occurrences of the type String
under constructors. Use this analysis to only return containers for constructor
names that might contain strings.
Exercise 5. Function insert takes the shape and the contents (a list of strings)
of a value, and inserts the strings at the right positions in the shape. Deﬁne
function insert as a a generic function with a kind-indexed type.
Exercise 6. Adapt the current version of XComprez such that it can use Huﬀ-
man coding instead of the standard constructor encoding used in this section.
Make sure other encodings can be used as well.
4
The Zipper
This section shows how to deﬁne a so-called zipper for an arbitrary data type.
This is an advanced example demonstrating the full power of a type-indexed
data type together with a number of generic functions working on it.
The zipper is a data structure that is used to represent a tree together with a
subtree that is the focus of attention, where that focus may move left, right, up
or down in the tree. The zipper is used in tools where a user interactively manip-
ulates trees, for instance, in editors for structured documents such as proofs or
programs. For the following it is important to note that the focus of the zipper
may only move to recursive components. Consider as an example the data type
Tree:
data Tree a b = Tip a | Node (Tree a b) b (Tree a b).
If the left subtree of a Node constructor is the current focus, moving right means
moving to the right tree, not to the b-label. This implies that recursive positions
in trees play an important rˆole in the deﬁnition of a generic zipper data structure.
To obtain access to these recursive positions, we have to be explicit about the
ﬁxed points in data type deﬁnitions. The zipper data structure is then deﬁned
by induction on the so-called pattern functor of a data type.
The tools in which the zipper is used, allow the user to repeatedly apply
navigation or edit commands, and to update the focus accordingly. In this section
we deﬁne a type-indexed data type for locations, which consist of a subtree (the
focus) together with a context, and we deﬁne several navigation functions on
locations.
4.1
The Basic Idea
The zipper is based on pointer reversal. If we follow a pointer to a subterm, the
pointer is reversed to point from the subterm to its parent so that we can go

86
R. Hinze and J. Jeuring
up again later. A location is a pair (t, c) consisting of the current subterm t
and a pointer c to its parent. The upward pointer corresponds to the context of
the subterm. It can be represented as follows. For each constructor K that has
m recursive subcomponents we introduce m context constructors K1, . . . , Km.
Now, consider the location (K t1 t2 . . . tm, c). If we go down to t1, we are left
with the context K • t2 . . . tm and the old context c. To represent the combined
context, we simply plug c into the hole to obtain K1 c t2 . . . tm. Thus, the new
location is (t1, K1 c t2 . . . tm). The following picture illustrates the idea (the
ﬁlled circle marks the current cursor position).
4.2
Data Types as Fixed Points of Pattern Functors
As mentioned above, in order to use the zipper, we have to be explicit about the
ﬁxed points in data type deﬁnitions. Therefore, we introduce the data type Fix,
which is used to deﬁne a data type as a ﬁxed point of a pattern functor. The
pattern functor makes the recursion explicit in a data type.
newtype Fix f = In{out :: f (Fix f)}
This is a labelled variant of the data type Fix deﬁned in Section 1.2 of the ﬁrst
part of these lecture notes. For example, the data types of natural numbers and
bushes can be deﬁned using explicit ﬁxed points as follows:
data NatF a
= ZeroF | SuccF a
type Nat
= Fix NatF
data BushF a = LeafF Char | BinF a a
type Bush
= Fix BushF.
It is easy to convert between data types deﬁned as ﬁxed points and the original
data type deﬁnitions of natural numbers and bushes. However, nested data types
and mutually recursive data types cannot be deﬁned in terms of this particular
deﬁnition of Fix.
4.3
Type Indices of Higher Kinds
The types that occur in the indices of a generic function have kind ⋆as their
base kind. For example, Int, Char and Unit are all of kind ⋆, and :+: and :*:

Generic Haskell: Applications
87
have kind ⋆→⋆→⋆. In this section we are going to deﬁne generic functions
which have ⋆→⋆as their base kind. We need slightly diﬀerent type indices for
generic functions operating on types of kind ⋆→⋆:
K t
= Λa . t
f1 :+: f2 = Λa . f1 a :+:f2 a
f1 :*: f2 = Λa . f1 a :*:f2 a
Con c f = Λa . Con c (f a)
Id
= Λa . a.
We have the constant functor K, which lifts a type of kind ⋆to kind ⋆→⋆. We
will need K Unit as well as K Char (or more general, K t for all primitive types).
We overload :+:, :*:, and Con, to be lifted versions of their previously deﬁned
counterparts. The only new type index in this set of indices of kind ⋆→⋆is the
identity functor Id. Hinze [20] shows that these types are the normal forms of
types of kind ⋆→⋆.
4.4
Locations
A location is a subtree, together with a context, which encodes the path from
the top of the original tree to the selected subtree. The type-indexed data type
Loc returns a type for locations given an argument pattern functor.
Loc{|f :: ⋆→⋆|}
::
⋆
Loc{|f|}
= (Fix f, Context{|f|} (Fix f))
Context{|f :: ⋆→⋆|} ::
⋆→⋆
Context{|f|} r
= Fix (LMaybe (Ctx{|f|} r))
data LMaybe f a
= LNothing | LJust (f a),
where LMaybe is the lifted version of Maybe. The type Loc is deﬁned in terms of
Context, which constructs the context parameterized by the original tree type.
The Context of a value is either empty (represented by LNothing in the LMaybe
type), or it is a path from the root down into the tree. Such a path is constructed
by means of the argument type of LMaybe: the type-indexed data type Ctx. The
type-indexed data type Ctx is deﬁned by induction on the pattern functor of the
original data type. It can be seen as the derivative (as in calculus) of the pattern
functor f [39, 1]. If the derivative of f is denoted by f′, we have
const′
= 0
(f + g)′ = f′ + g′
(f × g)′ = f′ × g + f × g′.
It follows that in the deﬁnition of Ctx we will also need access to the type
arguments themselves on the right-hand side of the deﬁnition.

88
R. Hinze and J. Jeuring
Ctx{|f :: ⋆→⋆|}
::
⋆→⋆→⋆
Ctx{|Id|} r c
= c
Ctx{|K Unit|} r c
= Void
Ctx{|K Char|} r c
= Void
Ctx{|f1 :+:f2|} r c = Ctx{|f1|} r c :+:Ctx{|f2|} r c
Ctx{|f1 :*:f2|} r c = (Ctx{|f1|} r c :*:f2 r) :+:(f1 r :*: Ctx{|f2|} r c)
This deﬁnition can be understood as follows. Since it is not possible to descend
into a constant, the constant cases do not contribute to the result type, which
is denoted by the ‘empty type’ Void, a type without values. The Id case denotes
a recursive component, in which it is possible to descend. Hence it may occur
in a context. Descending in a value of a sum type follows the structure of the
input value. Finally, there are two ways to descend in a product: descending left,
adding the contents to the right of the node to the context, or descending right,
adding the contents to the left of the node to the context.
For example, for natural numbers with pattern functor K Unit :+: Id, and
for trees of type Bush with pattern functor BushF, which can be represented by
K Char :+:(Id :*: Id) we obtain
Context{|K Unit :+: Id|} r
= Fix (LMaybe (NatC r))
Context{|K Char :+:Id :*: Id|} r = Fix (LMaybe (BushC r))
data NatC r c
= ZeroC Void | SuccC c
data BushC r c
= LeafC Void | BinCL (c, r) | BinCR (r, c).
The context of a natural number is isomorphic to a natural number (the context
of m in n is n −m), and the context of a Bush applied to the data type Bush
itself is isomorphic to the type Context Bush introduced in Section 1.
McBride [39, 1] also deﬁnes a type-indexed zipper data type. His zipper
slightly deviates from Huet’s and our zipper: the navigation functions on
McBride’s zipper are not constant time anymore. The observation that the
Context of a data type is its derivative (as in calculus) is due to McBride.
4.5
Navigation Functions
We deﬁne generic functions on the type-indexed data types Loc, Context, and
Ctx for navigating through a tree. All of these functions act on locations. These
are the basic functions for the zipper.
Function down. The function down is a generic function that moves down to
the leftmost recursive child of the current node, if such a child exists. Otherwise,
if the current node is a leaf node, then down returns the location unchanged.
down{|f :: ⋆→⋆|} :: Loc{|f|} →Loc{|f|}
The instantiation of down to the data type Bush has been given in Section 1.
The function down satisﬁes the following property:
∀m . down{|f|} m ̸= m
=⇒(up{|f|} · down{|f|}) m = m,

Generic Haskell: Applications
89
where the function up goes up in a tree. So ﬁrst going down the tree and then
up again is the identity function on locations in which it is possible to go down.
Since down moves down to the leftmost recursive child of the current node,
the inverse equality down{|f|} · up{|f|} = id does not hold in general. However,
there does exist a natural number n such that
∀m . up{|f|} m ̸= m
=⇒(right{|f|}n · down{|f|} · up{|f|}) m = m,
where the function right goes right in a tree. These properties do not completely
specify function down. The other properties it should satisfy are that the selected
subtree of down{|f|} m is the leftmost tree-child of the selected subtree of m, and
the context of down{|f|} m is the context of m extended with all but the leftmost
tree-child of m.
The function down is deﬁned as follows.
down{|f|} (t, c) = case ﬁrst{|f|} (out t) c of
Just (t′, c′) →(t′, In (LJust c′))
Nothing →(t, c)
To ﬁnd the leftmost recursive child, we have to pattern match on the pattern
functor f, and ﬁnd the ﬁrst occurrence of Id. The helper function ﬁrst is a generic
function that possibly returns the leftmost recursive child of a node, together
with the context (a value of type Ctx{|f|} c t) of the selected child. The function
down then turns this context into a value of type Context by inserting it in the
right (‘non-top’) component of a sum by means of LJust, and applying the ﬁxed
point constructor In to it.
ﬁrst{|f :: ⋆→⋆|}
:: ∀c t . f t →c →Maybe (t, Ctx{|f|} c t)
ﬁrst{|Id|} t c
= return (t, c)
ﬁrst{|K Unit|} t c
= Nothing
ﬁrst{|K Char|} t c
= Nothing
ﬁrst{|f1 :+:f2|} (Inl x) c
= do {(t, cx) ←ﬁrst{|f1|} x c; return (t, Inl cx)}
ﬁrst{|f1 :+:f2|} (Inr y) c
= do {(t, cy) ←ﬁrst{|f2|} y c; return (t, Inr cy)}
ﬁrst{|f1 :*:f2|} (x :*: y) c = do {(t, cx) ←ﬁrst{|f1|} x c
; return (t, Inl (cx, y))}
++ do {(t, cy) ←ﬁrst{|f2|} y c
; return (t, Inr (x, cy))}
Here, return is obtained from the Maybe monad, and the operator (++) is the
standard monadic plus, called mplus in Haskell, given by
(++)
:: ∀a . Maybe a →Maybe a →Maybe a
Nothing ++ m = m
Just a ++ m
= Just a.
The function ﬁrst returns the value and the context at the leftmost Id position.
So in the product case, it ﬁrst tries the left component, and only if it fails, it
tries the right component.

90
R. Hinze and J. Jeuring
The deﬁnitions of functions up, right and left are not as simple as the deﬁni-
tion of down, since they are deﬁned by pattern matching on the context instead
of on the tree itself. We will just deﬁne functions up and right, and leave function
left as an exercise.
Function up. The function up moves up to the parent of the current node, if the
current node is not the top node.
up{|f :: ⋆→⋆|} :: Loc{|f|} →Loc{|f|}
up{|f|} (t, c)
= case out c of
LNothing →(t, c)
LJust c′ →do {ft ←insert{|f|} c′ t;
c′′ ←extract{|f|} c′;
return (In ft, c′′)}
Remember that LNothing denotes the empty top context. The navigation func-
tion up uses two helper functions: insert and extract. The latter returns the
context of the parent of the current node. Each element of type Ctx{|f|} c t has
at most one c component (by an easy inductive argument), which marks the
context of the parent of the current node. The generic function extract extracts
this context.
extract{|f :: ⋆→⋆|}
:: ∀c t . Ctx{|f|} c t →Maybe c
extract{|Id|} c
= return c
extract{|K Unit|} c
= Nothing
extract{|K Char|} c
= Nothing
extract{|f1 :+: f2|} (Inl cx)
= extract{|f1|} cx
extract{|f1 :+: f2|} (Inr cy)
= extract{|f2|} cy
extract{|f1 :*: f2|} (Inl (cx, y)) = extract{|f1|} cx
extract{|f1 :*: f2|} (Inr (x, cy)) = extract{|f2|} cy
The function extract is polymorphic in c and in t.
Function insert takes a context and a tree, and inserts the tree in the current
focus of the context, eﬀectively turning a context into a tree.
insert{|f :: ⋆→⋆|}
:: ∀c t . Ctx{|f|} c t →t →Maybe (f t)
insert{|Id|} c t
= return t
insert{|K Unit|} c t
= Nothing
insert{|K Char|} c t
= Nothing
insert{|f1 :+: f2|} (Inl cx) t
= do {x ←insert{|f1|} cx t; return (Inl x)}
insert{|f1 :+: f2|} (Inr cy) t
= do {y ←insert{|f2|} cy t; return (Inr y)}
insert{|f1 :*: f2|} (Inl (cx, y)) t = do {x ←insert{|f1|} cx t; return (x, y)}
insert{|f1 :*: f2|} (Inr (x, cy)) t = do {y ←insert{|f2|} cy t; return (x, y)}
Note that the extraction and insertion is happening in the identity case Id; the
other cases only pass on the results.
Since up{|f|} · down{|f|} = id on locations in which it is possible to go down,
we expect similar equalities for the functions ﬁrst, extract, and insert. We have
that the following computation

Generic Haskell: Applications
91
do {(t, c′) ←ﬁrst{|f|} ft c;
c′′ ←extract{|f|} c′;
ft′ ←insert{|f|} c′ t
;
return (c
c′′ ∧ft
ft′ ) },
returns true on locations in which it is possible to go down.
Function right. The function right moves the focus to the next (right) sibling
in a tree, if it exists. The context is moved accordingly. The instance of right on
the data type Bush has been given in Section 1. The function right satisﬁes the
following property:
∀m . right{|f|} m ̸= m
=⇒
(left{|f|} · right{|f|}) m = m,
that is, ﬁrst going right in the tree and then left again is the identity function on
locations in which it is possible to go to the right. Of course, the dual equality
holds on locations in which it is possible to go to the left. Furthermore, the
selected subtree of right{|f|} m is the sibling to the right of the selected subtree
of m, and the context of right{|f|} m is the context of m in which the context is
replaced by the selected subtree of m, and the ﬁrst subtree to the right of the
context of m is replaced by the context of m.
Function right is deﬁned by pattern matching on the context. It is impossible
to go to the right at the top of a tree. Otherwise, we try to ﬁnd the right sibling
of the current focus.
right{|f :: ⋆→⋆|} ::
Loc{|f|} →Loc{|f|}
right{|f|} (t, c)
= case out c of
LNothing →(t, c)
LJust c′ →case next{|f|} t c′ of
Just (t′, c′′) →(t′, In (LJust c′′))
Nothing →(t, c)
The helper function next is a generic function that returns the ﬁrst location that
has the recursive value to the right of the selected value as its focus. Just as there
exists a function left such that left{|f|} · right{|f|} = id (on locations in which it
is possible to go to the right), there exists a function previous, such that
do {(t′, c′) ←next{|f|} t c ;
(t′′, c′′) ←previous{|f|} t′ c′;
return (c
c′′ ∧t
t′′)},
returns true (on locations in which it is possible to go to the right). We will
deﬁne function next, and omit the deﬁnition of function previous.

92
R. Hinze and J. Jeuring
next{|f :: ⋆→⋆|} :: ∀c t . t →Ctx{|f|} c t →Maybe (t, Ctx{|f|} c t)
next{|Id|} t c
= Nothing
next{|K Unit|} t c
= Nothing
next{|K Char|} t c
= Nothing
next{|f1 :+: f2|} t (Inl cx)
= do {(t′, cx ′) ←next{|f1|} t cx; return (t′, Inl cx ′)}
next{|f1 :+: f2|} t (Inr cy)
= do {(t′, cy′) ←next{|f2|} t cy; return (t′, Inr cy′)}
next{|f1 :*: f2|} t (Inl (cx, y) )
= do {(t′, cx ′) ←next{|f1|} t cx; return (t′, Inl (cx ′, y))}
++do {c ←extract{|f1|} cx;
x ←insert{|f1|} cx t;
(t′, cy) ←ﬁrst{|f2|} y c;
return (t′, Inr (x, cy))}
next{|f1 :*: f2|} t (Inr (x, cy))
= do {(t′, cy′) ←next{|f2|} t cy; return (t′, Inr (x, cy′))}
The ﬁrst three lines in this deﬁnition show that it is impossible to go to the
right in an identity or constant context. If the context argument is a value of a
sum, we select the next element in the appropriate component of the sum. The
product case is the most interesting one. If the context is in the right component
of a pair, next returns the next value of that context, properly combined with
the left component of the tuple. On the other hand, if the context is in the left
component of a pair, the next value may be either in that left component (the
context), or it may be in the right component (the value). If the next value is in
the left component, it is returned by the ﬁrst line in the deﬁnition of the product
case. If it is not, next extracts the context c (the context of the parent) from the
left context cx, it inserts the given value in the context cx giving a ‘tree’ value
x, and selects the ﬁrst component in the right component of the pair, using the
extracted context c for the new context. The new context that is thus obtained
is combined with x into a context for the selected tree.
Exercise 7. Deﬁne the function left:
left{|f :: ⋆→⋆|} :: Loc{|f|} →Loc{|f|}.
Exercise 8. If you don’t want to use the zipper, you can alternatively keep track
of the path to the current focus. Suppose we want to use the path to determine
the name of the top constructor of the current focus in a value of a data type.
The path determines which child of a value is selected. Since the products used in
our representations of data types are binary, a path has the following structure:
data Dir
= L | R
type Path = [Dir].
The to be deﬁned function selectCon takes a value of a data type and a path, and
returns the constructor name at the position denoted by the path. For example,

Generic Haskell: Applications
93
data List = Nil | Cons Char List
selectCon{|List|} (Cons 2 (Cons 3 (Cons 6 Nil))) [R, R, R]
=⇒"Nil"
data Tree = Leaf Int | Node Tree Int Tree
selectCon{|Tree|} (Node (Leaf 1) 3 (Node (Leaf 2) 1 (Leaf 5))) [R, R]
=⇒"Node"
selectCon{|Tree|} (Node (Leaf 1) 3 (Node (Leaf 2) 1 (Leaf 5))) [R, R, L]
=⇒"Leaf".
Deﬁne the generic function selectCon, together with its kind-indexed type.
Exercise 9. Deﬁne the function left, which takes a location, and returns the
location to the left of the argument location, if possible.
Exercise 10. For several applications we have to extend a data type such that
it is possible to represent a place holder. For example, from the data type Tree
deﬁned by
data Tree a b = Tip a | Node (Tree a b) b (Tree a b),
we would like to obtain a type isomorphic to the following type:
data HoleTree a b = Hole | Tip a | Node (HoleTree a b) b (HoleTree a b).
– Deﬁne a type-indexed data type Hole that takes a data type and returns a
data type in which also holes can be speciﬁed. Also give the kind-indexed
kind of this type-indexed data type. (The kind-indexed kind cannot and does
not have to be deﬁned in Generic Haskell though.)
– Deﬁne a generic function toHole which translates a value of a data type t
to a value of the data type Hole{|t|}, and a function fromHole that does the
inverse for values that do not contain holes anymore:
toHole{|t :: κ|}
:: ToHole{[κ]} t
fromHole{|t :: κ|} :: FromHole{[κ]} t
type ToHole{[⋆]} t
= t →Hole{|t|}
type FromHole{[⋆]} t = Hole{|t|} →t.
5
Conclusions
We have developed three advanced applications in Generic Haskell. In these
examples we use, besides generic functions with kind-indexed kinds, type-indexed
data types, dependencies between and generic abstractions of generic functions,
and default and constructor cases. Some of the latest developments of Generic
Haskell have been guided by requirements from these applications.
We hope to develop more applications using Generic Haskell in the future,
both to develop the theory and the language. Current candidate applications are
more XML tools and editors.

94
R. Hinze and J. Jeuring
Acknowledgements
Andres L¨oh implemented Generic Haskell and the zipper example, and con-
tributed to almost all other examples. Paul Hagg contributed to the implemen-
tation of the XML compressor. Dave Clarke, Andres L¨oh, Ralf L¨ammel, Doaitse
Swierstra and Jan de Wit commented on or contributed to (parts of) previous
versions of this paper.
References
1. Michael Abott, Thorsten Altenkirch, Neil Ghani, and Conor McBride. Derivatives
of containers. In Typed Lambda Calculi and Applications, TLCA 2003, 2003. To
appear.
2. R. Backhouse, P. Jansson, J. Jeuring, and L. Meertens. Generic programming: An
introduction. In S. Doaitse Swierstra, Pedro R. Henriques, and Jos´e N. Oliveira,
editors, Advanced Functional Programming, volume 1608 of LNCS, pages 28–115.
Springer-Verlag, 1999.
3. Richard Bird and Jeremy Gibbons. Arithmetic coding with folds and unfolds. In Jo-
han Jeuring and Simon Peyton Jones, editors, Advanced Functional Programming,
4th International Summer School, Oxford, UK, volume 2638 of LNCS. Springer-
Verlag, 2003. To appear.
4. Robert D. Cameron. Source encoding using syntactic information source models.
IEEE Transactions on Information Theory, 34(4):843–850, 1988.
5. Mario Cannataro, Gianluca Carelli, Andrea Pugliese, and Domenico Sacca. Seman-
tic lossy compression of XML data. In Knowledge Representation Meets Databases,
2001.
6. James Cheney. Compressing xml with multiplexed hierarchical models. In Pro-
ceedings of the 2001 IEEE Data Compression Conference, DCC’01, pages 163–172,
2001.
7. Koen Claessen, Colin Runciman, Olaf Chitil, John Hughes, and Malcolm Wallace.
Testing and tracing lazy functional programs using Quickcheck and Hat. In Johan
Jeuring and Simon Peyton Jones, editors, Advanced Functional programming, 4th
International Summer School, Oxford, UK, volume 2638 of LNCS. Springer-Verlag,
2003. To appear.
8. Dave Clarke. Towards GH(XML). Talk at the Generic Haskell meeting, see
http://www.generic-haskell.org/talks.html, 2001.
9. Dave Clarke, Ralf Hinze, Johan Jeuring, Andres L¨oh, and Jan de Wit. The Generic
Haskell user’s guide. Technical Report UU-CS-2001-26, Utrecht University, 2001.
Also available from http://www.generic-haskell.org/.
10. Dave Clarke and Andres L¨oh. Generic Haskell, speciﬁcally. In Jeremy Gibbons and
Johan Jeuring, editors, Generic Programming, volume 243 of IFIP, pages 21–48.
Kluwer Academic Publishers, January 2003.
11. Richard H. Connelly and F. Lockwood Morris. A generalization of the trie data
structure. Mathematical Structures in Computer Science, 5(3):381–418, September
1995.
12. XMLSolutions Corporation. XMLZip. Available from
http://www.xmlzip.com/, 1999.
13. Ren´e de la Briandais. File searching using variable length keys. In Proc. Western
Joint Computer Conference, volume 15, pages 295–298. AFIPS Press, 1959.

Generic Haskell: Applications
95
14. William S. Evans and Christopher W. Fraser. Bytecode compression via proﬁled
grammar rewriting. In SIGPLAN Conference on Programming Language Design
and Implementation, pages 148–155, 2001.
15. Michael Franz. Adaptive compression of syntax trees and iterative dynamic code
optimization: Two basic technologies for mobile object systems. In Mobile Object
Systems: Towards the Programmable Internet, pages 263–276. Springer-Verlag: Hei-
delberg, Germany, 1997.
16. Marc Girardot and Neel Sundaresan. Millau: an encoding format for eﬃcient rep-
resentation and exchange of XML over the Web. In IEEE International Conference
on Multimedia and Expo (I) 2000, pages 747–765, 2000.
17. Paul Hagg.
A framework for developing generic XML Tools.
Master’s thesis,
Department of Information and Computing Sciences, Utrecht University, 2002.
18. Ralf Hinze. Generalizing generalized tries. Journal of Functional Programming,
10(4):327–351, 2000.
19. Ralf Hinze. Generic Programs and Proofs. 2000. Habilitationsschrift, Bonn Uni-
versity.
20. Ralf Hinze. A new approach to generic functional programming. In Conference
Record of POPL ’00: The 27th ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages, pages 119–132. ACM Press, 2000.
21. Ralf Hinze. Polytypic values possess polykinded types. Science of Computer Pro-
gramming, 43(2-3):129–159, 2002.
22. Ralf Hinze and Johan Jeuring. Generic Haskell: practice and theory, 2003. To
appear.
23. Ralf Hinze, Johan Jeuring, and Andres L¨oh. Type-indexed data types. In Pro-
ceedings of the 6th Mathematics of Program Construction Conference, MPC’02,
volume 2386 of LNCS, pages 148–174, 2002.
24. G´erard Huet. The zipper. Journal of Functional Programming, 7(5):549–554, 1997.
25. INC Intelligent Compression Technologies.
XML-Xpress. Whitepaper available
from http://www.ictcompress.com/products_xmlxpress.html, 2001.
26. P. Jansson and J. Jeuring. Polytypic compact printing and parsing. In Doaitse
Swierstra, editor, ESOP’99, volume 1576 of LNCS, pages 273–287. Springer-Verlag,
1999.
27. Patrik Jansson. The WWW home page for polytypic programming. Available from
http://www.cs.chalmers.se/~patrikj/poly/, 2001.
28. Patrik Jansson and Johan Jeuring. Functional pearl: Polytypic uniﬁcation. Journal
of Functional Programming, 8(5):527–536, September 1998.
29. Patrik Jansson and Johan Jeuring.
A framework for polytypic programming
on terms, with an application to rewriting. In J. Jeuring, editor, Workshop on
Generic Programming 2000, Ponte de Lima, Portugal, July 2000, pages 33–45,
2000. Utrecht Technical Report UU-CS-2000-19.
30. Patrik Jansson and Johan Jeuring. Polytypic data conversion programs. Science
of Computer Programming, 43(1):35–75, 2002.
31. J. Jeuring.
Polytypic pattern matching.
In Conference Record of FPCA ’95,
SIGPLAN-SIGARCH-WG2.8 Conference on Functional Programming Languages
and Computer Architecture, pages 238–248. ACM Press, 1995.
32. Johan Jeuring and Paul Hagg. XComprez. Available from
http://www.generic-haskell.org/xmltools/XComprez/, 2002.
33. Donald E. Knuth. The Art of Computer Programming, Volume 3: Sorting and
Searching. Addison-Wesley Publishing Company, 2nd edition, 1998.
34. Pieter Koopman, Artem Alimarine, Jan Tretmans, and Rinus Plasmeijer. Gast:
Generic automated software testing. Submitted for publication, 2002.

96
R. Hinze and J. Jeuring
35. Ralf L¨ammel and Simon Peyton Jones. Scrap your boilerplate: a practical approach
to generic programming. In Proc ACM SIGPLAN Workshop on Types in Language
Design and Implementation (TLDI 2003), 2003.
36. Hartmut Liefke and Dan Suciu. XMill: an eﬃcient compressor for XML data. In
Proceedings of the 2000 ACM SIGMOD International Conference on Management
of Data, pages 153–164, 2000.
37. Andres L¨oh, Dave Clarke, and Johan Jeuring.
Generic Haskell, naturally: The
language and its type system. In preparation, 2003.
38. G. Malcolm. Data structures and program transformation. Science of Computer
Programming, 14:255–279, 1990.
39. Connor McBride. The derivative of a regular type is its type of one-hole contexts.
Unpublished manuscript, 2001.
40. L. Meertens. Paramorphisms. Formal Aspects of Computing, 4(5):413–425, 1992.
41. Lambert Meertens.
Functor pulling.
In Workshop on Generic Programming
(WGP’98), Marstrand, Sweden, June 1998.
42. E. Meijer, M.M. Fokkinga, and R. Paterson. Functional programming with ba-
nanas, lenses, envelopes, and barbed wire. In J. Hughes, editor, FPCA’91: Func-
tional Programming Languages and Computer Architecture, volume 523 of LNCS,
pages 124–144. Springer-Verlag, 1991.
43. Chris Okasaki. Purely Functional Data Structures. Cambridge University Press,
1998.
44. Chris Okasaki and Andy Gill. Fast mergeable integer maps. In The 1998 ACM
SIGPLAN Workshop on ML, Baltimore, Maryland, pages 77–86, 1998.
45. Simon Peyton Jones [editor], John Hughes [editor], Lennart Augustsson, Dave
Barton, Brian Boutel, Warren Burton, Simon Fraser, Joseph Fasel, Kevin Ham-
mond, Ralf Hinze, Paul Hudak, Thomas Johnsson, Mark Jones, John Launch-
bury, Erik Meijer, John Peterson, Alastair Reid, Colin Runciman, and Philip
Wadler. Haskell 98 — A non-strict, purely functional language. Available from
http://www.haskell.org/definition/, February 1999.
46. C.H. Stork, V. V. Haldar, and M. Franz. Generic adaptive syntax-directed com-
pression for mobile code. Technical Report 00-42, Department of Information and
Computer Science, University of California, Irvine, 2000.
47. Axel Thue. ¨Uber die gegenseitige lage gleicher teile gewisser zeichenreihen. Skrifter
udgivne af Videnskaps-Selskabet i Christiania, Mathematisk-Naturvidenskabelig
Klasse, 1:1–67, 1912. Reprinted in Thue’s “Selected Mathematical Papers” (Oslo:
Universitetsforlaget, 1977), 413–477.
48. Pankaj Tolani and Jayant R. Haritsa. XGRIND: A query-friendly XML compres-
sor. In ICDE, 2002.
49. W3C. XML 1.0. Available from http://www.w3.org/XML/, 1998.
50. C.P. Wadsworth. Recursive type operators which are more than type schemes.
Bulletin of the EATCS, 8:87–88, 1979. Abstract of a talk given at the 2nd In-
ternational Workshop on the Semantics of Programming Languages, Bad Honnef,
Germany, 19–23 March 1979.
51. Malcolm Wallace and Colin Runciman. Haskell and XML: Generic combinators or
type-based translation? In International Conference on Functional Programming,
pages 148–159, 1999.
52. J. Ziv and A. Lempel.
A universal algorithm for sequential data compression.
IEEE Transactions on Information Theory, 23(3):337–343, 1977.

Generic Properties of Datatypes
Roland Backhouse 1 and Paul Hoogendijk 2
1 School of Computer Science and Information Technology
University of Nottingham, Nottingham NG8 1BB, UK
rcb@cs.nott.ac.uk
2 Philips Research, Prof. Holstlaan 4, 5655 AA Eindhoven, The Netherlands
Paul.Hoogendijk@philips.com
Abstract. Generic programming adds a new dimension to the para-
metrisation of programs by allowing programs to be dependent on the
structure of the data that they manipulate. Apart from the practical
advantages of improved productivity that this oﬀers, a major potential
advantage is a substantial reduction in the burden of proof – by making
programs more general we can also make them more robust and more
reliable. These lectures discuss a theory of datatypes based on the alge-
bra of relations which forms a basis for understanding datatype-generic
programs. We review the notion of parametric polymorphism much ex-
ploited in conventional functional programming languages and show how
it is extended to the higher-order notions of polymorphism relevant to
generic programming.
1
Introduction
Imagine that you bought a processor designed to perform ﬂoating-point calcula-
tions with the utmost eﬃciency but later you discovered that it would not work
correctly for all input values. You would, of course, demand your money back —
even if the manufacturer tried to claim that the occasions on which the processor
would fail were very few and far between. Now imagine that you were oﬀered
some software that was described as “generic” but came with the caveat that it
had only been tried on a few instances and the extent of its “genericity” could
not be formulated precisely, let alone guaranteed. Would you buy it, or would
you prefer to wait and let others act as the guinea pigs?
“Generic programming” is important to software designers as a way of im-
proving programmer productivity. But “generic” programs can only have value if
the full extent of their “genericity” can be described clearly and precisely, and if
the programs themselves can be validated against their speciﬁcations throughout
the full range of their applicability.
These lectures are about a novel parameterisation mechanism that allows
programs to be dependent on the structure of the data that they manipulate,
so-called “datatype genericity”. The focus is on deﬁning clearly and precisely
what the word “generic” really means. The (potential) beneﬁt is a substantial
reduction in the burden of proof.
R. Backhouse and J. Gibbons (Eds.): Generic Programming SS 2002, LNCS 2793, pp. 97–132, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

98
R. Backhouse and P. Hoogendijk
In section 2, we review the notion of “parametric polymorphism” in the
context of functional programming languages. This is a form of genericity which
has long since proved its worth. For us, the important fact is that the notion can
be given a precise deﬁnition, thus allowing one to formally verify whether or not
a given function complies with the deﬁnition.
Section 3 introduces the notion of “commuting” two datatypes. This problem
helps to illustrate the concepts introduced in the later sections. We seek a non-
operational speciﬁcation based on fundamental building blocks for a generic
theory of datatypes.
Deﬁning parametric polymorphism involves extending the notion of “map-
ping” functions over a datatype to mapping relations over a datatype. (This is
unavoidably the case even in the context of functional programming.) Section 4
prepares the way for later sections by introducing the fundamentals of a rela-
tional theory of datatypes. For brevity and eﬀective, calculational reasoning we
use “point-free” relation algebra. The most important concept introduced in this
section is that of a “relator” — essentially a datatype. Other, more well-known
concepts like weakest liberal precondition, are formulated in a point-free style.
The “membership” relation and the associated “fan” of a relator are intro-
duced in section 5. We argue that a datatype is a relator with membership.
The introduction of a membership relation allows us to give greater insight into
what it means for a polymorphic function to be a natural transformation. It also
helps to distinguish between “proper” natural transformations and “lax” natural
transformation.
In sections 6 and 7, we return to the issue of specifying when datatypes
commute. This is formulated as the combination of two parametricity properties
(one of which is high-level) and a homomorphism property (with respect to
pointwise deﬁnition of relators).
2
Theorems for Free
In Pascal, it is necessary to deﬁne a diﬀerent length function for lists of diﬀerent
types. For example, two diﬀerent functions are needed to determine the length
of a list of integers and to determine the length of a list of characters. This
is a major inconvenience because, of course, the code for the two functions is
identical; only their declarations are diﬀerent. The justiﬁcation given, at the
time Pascal was designed, was that the increased redundancy facilitated type
checking, thus assisting with the early detection of programming errors. In typed
functional programming languages, beginning with ML [9, 10], only one so-called
“polymorphic” function is needed. Formally, the length function is a function
with type IN ←List.α for all instances of the type parameter α . That is,
whatever the type A , there is an instance of the function length that maps a
list of A ’s to a natural number.
A polymorphic function is said to be “parametric” if its behaviour does not
depend on the type at which it is instantiated [13]. That the length function on
lists is parametric can be expressed formally. Suppose a function is applied to
each element of a list —the function may, for example, map each character to a

Generic Properties of Datatypes
99
number— . Clearly, this will have no eﬀect on the length of the list. Formally1,
we have, for all types A and B and all functions f of type A ←B ,
lengthA ◦List.f
=
lengthB .
Note that this equation does not characterise the length function. Indeed, post-
composing length with any function g from natural numbers to natural num-
bers, we get a function that satisﬁes an equation of the same form. For example,
let sq denote the function that squares a number. Then
(sq ◦lengthA) ◦List.f
=
sq ◦lengthB .
Also precomposing the length function with any function that maps a list to
list, possibly duplicating or omitting elements of the list, but not performing
any computation dependent on the value of the list element, also satisﬁes an
equation of the same form. For example, suppose copycat is a (polymorphic)
function that maps a list to the list concatenated with itself. (So, for example,
copycat. [0,1,2] equals [0,1,2,0,1,2] .) Then
(lengthA ◦copycatA) ◦List.f
=
lengthB ◦copycatB .
What is common to the functions length , sq ◦length and length ◦copycat
is that they all have type IN ←List.α , for all instances of the type param-
eter α . Shortly, we deﬁne “parametric polymorphism” precisely. These three
functions exemplify one instance of the deﬁnition. Speciﬁcally, g has so-called
“polymorphic” type ⟨∀α :: IN ←List.α⟩if for each type A there is a function
gA of type IN ←List.A ; g is so-called “parametrically” polymorphic if it has
the property that, for all functions f of type A ←B ,
gA ◦List.f
=
gB .
In general, a function will be called “parametrically polymorphic” if we can
deduce a general equational property of the function from information about
the type of the function alone.
A yet simpler example of a parametrically polymorphic function is the func-
tion fst that takes a pair of values and returns the ﬁrst element of the pair. Using
α×β to denote the set of all pairs with ﬁrst component of type α and second
component of type β , the function fst has type ⟨∀α,β :: α ←α×β⟩. Now
1 Function composition is denoted by an inﬁx “ ◦” symbol. Function application is
denoted by an inﬁx “ . ” symbol. By deﬁnition, (f ◦g).x = f.(g.x) . This is often called
“backward composition” and “ ◦” is pronounced “after” (because f
is executed
“after” g ). Because of this choice of order of evaluation, it is preferable to use
a backward pointing arrow to indicate function types. The rule is that if f has
type A ←B and g has type B ←C then f ◦g has type A ←C . Functional
programmers know the function “ List ” better by the name “ map ”. As we shall see,
it is beneﬁcial to give the type constructor and the associated “ map ” function the
same name.

100
R. Backhouse and P. Hoogendijk
suppose f and g are functions of arbitrary type, and suppose f×g denotes
the function that maps a pair of values (x, y) (of appropriate type) to the pair
(f.x , g.y) . Then, clearly,
fst ◦f×g
=
f ◦fst .
(Strictly, we should add type information by way of subscripts on the two occur-
rences of fst , just as we did for length . Later this type information will prove
important, but for now we omit it from time to time.)
The simplest example of a parametrically polymorphic function is the identity
function. For each type A there is an identity function idA on A (the function
such that, for all x of type A , idA.x = x .) Clearly, for all functions f of type
A ←B ,
idA ◦f
=
f ◦idB .
What these examples illustrate is that a function is parametrically polymorphic
if the function enjoys an algebraic property in common with all functions having
the same type.
The key to a formal deﬁnition of “parametric polymorphism” is, in Wadler’s
words, “that types may be read as relations”. This involves extending each type
constructor —a function from types to types— to a relation constructor —a
function from relations to relations— . The relations may, in fact, have arbitrary
arity but we show how the extension is done just for binary relations.
For concreteness, we consider a language of types that comprises only func-
tion types and cartesian product. (In later sections we extend the language of
types to include dataypes like List .) Speciﬁcally, we consider the type expres-
sions deﬁned by the following grammar:
Exp
::=
Exp × Exp
|
Exp ←Exp
|
Const
|
V ar .
Here, Const denotes a set of constant types, like IN (the natural numbers) and
ZZ (the integers). V ar denotes a set of type variables. We use Greek letters to
denote type variables.
The syntactic ambiguity in the grammar is resolved by assuming that × has
precedence over ←, and both are left associative. For example,
IN ←IN ←α × α × β
means
(IN ←IN) ←((α × α) × β) .
The inclusion of variables means that type expressions denote functions from
types to types; we get a type for each instantiation of the type variables to types.
The notation T.A will be used to denote the type obtained by instantiating the
variables in type expression T by the types A . (Generally, A will be a vector
of types, one for each type variable in T .)
Type expressions are extended to denote functions from relations to relations
as follows. The constant type A is read as the identity relation idA on A . The
product R×S of binary relations R and S of types A ∼B and C ∼D is
deﬁned to be the relation of type A×C ∼B×D deﬁned by

Generic Properties of Datatypes
101
((a, c) , (b, d)) ∈R×S
≡
(a, b) ∈R
∧
(c, d) ∈S .
(1)
Finally, the function space constructor, “ ←”, is read as a mapping from a pair of
relations R and S of types A ∼B and C ∼D , respectively, to a binary re-
lation R ←S on functions f and g of type A ←C and B ←D , respectively.
Formally, suppose R and S are binary relations of type A ∼B and C ∼D ,
respectively. Then R ←S is the binary relation of type (A ←C) ∼(B ←D)
deﬁned by, for all functions f ∈A ←C and g ∈B ←D ,
(f, g) ∈R ←S
≡
⟨∀c,d
::
(f.c , g.d) ∈R
⇐
(c, d) ∈S⟩
.
(2)
In words, f and g construct R -related values from S -related values.
As an example, suppose ( A , ⊑) and ( B , ⪯) are partially ordered sets.
Then we can instantiate R to ⊑and S to ⪯getting a relation ⊑←⪯
between functions f and g of type A ←B . In particular, switching to the
usual inﬁx notation for membership of an ordering relation,
(f, f)
∈
⊑←⪯
≡
⟨∀u,v
::
f.u ⊑f.v ⇐u ⪯v⟩
.
So (f, f) ∈⊑←⪯is the statement that f
is a monotonic function. In
Wadler’s words, f maps ⪯-related values to ⊑-related values.
In this way, a type expression is extended to a function from relations to
relations. We use T.R to denote the relation obtained by instantiating the type
variables in type expression T by the (vector of) relations R . Note that the con-
struction we have given has the property that if relations R have type A ∼B
then T.R has type T.A ∼T.B.
Deﬁnition 1 (Parametric Polymorphism).
A term t is said to have
polymorphic type ⟨∀α :: T.α⟩, where T is a type expression parameterised by
type variables α , if t assigns to each type A a value tA of type T.A . A term
t of polymorphic type ⟨∀α :: T.α⟩is said to be parametrically polymorphic if,
for each instantiation of relations R to type variables, (tA , tB) ∈T.R , where
R has type A ∼B .
2
We sometimes abbreviate “parametrically polymorphic” to “parametric”.
Exercise 1.
The function fork creates a pair of values by simply copying its
input value. That is fork.x = (x, x) .
What is the type of fork ? Formulate the property that fork is parametrically
polymorphic and verify that this is indeed the case.
2
Exercise 2.
Function application has type ⟨∀α,β :: α
←
(α←β)×β⟩.
That is, for each pair of types A and B , each function f
of type A←B
and each b∈B , f.b ∈A . Formulate the statement that function application is
parametrically polymorphic and verify that this is indeed the case.
2
Exercise 3.
Currying has type ⟨∀α,β,γ :: α ←β ←γ ←(α
←
β×γ)⟩.
Speciﬁcally,
curry.f.x.y
=
f.(x, y) .

102
R. Backhouse and P. Hoogendijk
(Function application is assumed to be left-associative.) Formulate the statement
that currying is parametrically polymorphic and verify that this is indeed the
case.
2
As we have deﬁned them, the notions of polymorphic and parametrically
polymorphic are distinct. It is common for programming languages to permit
the deﬁnition of polymorphic functions without their being parametric. This
is known as ad hoc polymorphism [13]. For example, a language may have a
polymorphic binary operator that is called the “equality” operator. That is,
there is a binary operator, denoted say by == , such that for any pair of terms
s and t in the language, s == t denotes a boolean value. However, if the
operator is parametric, we deduce from its type — Bool ←α×α — that for
all relations R and all u , v , x and y ,
(u == v)
=
(x == y)
⇐
(u, x) ∈R
∧
(v, y) ∈R .
In particular, taking the relation R to be an arbitrary function f deﬁnable in
the programming language,
(f.x == f.y)
=
(x == y) .
In other words, if the “equality” operator is parametrically polymorphic and
truly tests for equality, all functions in the language are injective — which would
make the language a very restrictive language indeed. In practice, of course,
“equality” operators provided in programming languages are neither parametric
nor true implementations of equality on all types.
If a language is such that all polymorphic terms are parametrically polymor-
phic, then, by deﬁnition, if term t has type T , for each instantiation of type
variables to relations R of type A ∼B , (tA , tB) ∈T.R . Instances of the
property, for all relations R , (tA , tB) ∈T.R , are called free theorems. For
numerous examples of free theorems, see Wadler’s paper [14].
2.1
Veriﬁable Genericity
Polymorphism is a feature of programming languages that has many forms.
Overloading and other ad hoc forms of polymorphism are introduced for the
programmer’s convenience, but oﬀer little or no support for reasoning about
programs. Parametric polymorphism, on the other hand, entails a meaningful
and useful relationship between the diﬀerent instances of the polymorphic object.
Also, because the notion has a precise (and simple) formal deﬁnition, parametric
polymorphism is a veriﬁable form of genericity.
In the remaining sections, we introduce the idea of parameterising programs
by datatypes. The emphasis is on ensuring that the genericity that is introduced
is clearly and precisely speciﬁed so that all claims made about its use can be
formally veriﬁed.
3
Commuting Datatypes — Introduction
In this section, we introduce an example of “higher order” parametricity —
parametrisation with respect to type constructors (functions from types to types,

Generic Properties of Datatypes
103
like List ) rather than types (like Int ). Our purpose is to demonstrate the use-
fulness of making this sort of abstraction, and paving the way for the theoretical
development that is to follow.
We use the term “datatype” without deﬁnition for the moment. (Later it
will emerge that a datatype is a “relator with membership”.) Think, however, of
functions from type to types like pairing, and the formation of lists or arbitrary
tree structures.
The section contains a number of examples of “commuting” two datatypes.
This suggests a generic property of datatypes, namely that any two datatypes
can be “commuted”. We argue that one should deﬁne the notion of “commuting”
datatypes by a precise formulation of the properties we require of the operation
(by abstraction from a suitable collection of examples). In this way, reasoning
about speciﬁc programs that exploit the generic property is substantially easier.
The best known example of a commutativity property is the fact that two
lists of the same length can be mapped into a single list of pairs whereby
([a1 , a2 , . . . , an],[b1 ,b2 , . . . ,bn]) →[(a1 ,b1),(a2 , b2), . . . ,(an , bn)]
The function that performs this operation is known as the “zip” function to
functional programmers. Zip commutes a pair of lists (of the same length) into
a list of pairs.
Other speciﬁc examples of commutativity properties are easy to invent. For
instance, it is not diﬃcult to imagine generalising zip to a function that com-
mutes m lists each of length n into n lists each of length m . Indeed, this
latter function is also well known under the name matrix transposition. Another
example is the function that commutes a tree of lists all of the same length into
a list of trees all of the same shape. There is also a function that “broadcasts”
a value to all elements of a list —thus
(a,[b1 ,b2 , . . . ,bn]) →[(a,b1),(a,b2), . . . ,(a , bn)]
— . That is, the datatype ‘an element of type A paired with (a list of elements
of type B )’ is “commuted” to ‘a list of (element of type A paired with an
element of type B )’. More precisely, for each A , a broadcast is a paramet-
rically polymorphic function of type ⟨∀α :: List.(A×α) ←A×List.α⟩; the two
datatypes being “commuted” are thus ( A× ) and List . (Note that we use the
Greek letter α and the Latin letter A in order to distinguish the roles of the
two parameters.)
This list broadcast is itself an instance of a subfamily of the operations
that we discuss later. In general, a broadcast operation copies a given value
to all locations in a given data structure. That is, using F to denote an ar-
bitrary datatype and the inﬁx operator “ · ” to denote (backward) composi-
tion of datatypes, a broadcast is a parametrically polymorphic function of type
⟨∀α :: (F ·(A×)).α ←((A×)·F).α⟩.
A ﬁnal example of a generalised zip would be the (polymorphic) operation
that maps values of type (A+B)×(C+D) to values of type (A×C)+(B×D) ,
i.e. commutes a product of disjoint sums to a disjoint sum of products. A nec-
essary restriction is that the elements of the input pair of values have the same

104
R. Backhouse and P. Hoogendijk
“shape”, i.e. both be in the left component of the disjoint sum or both be in the
right component.
In general then, a zip operation transforms F -structures of G -structures
to G -structures of F -structures. (An F -structure is a value of type F.X for
some type X . So, a List -structure is just a list. Here, F could be, for example,
List and G could be Tree .) Typically, “zips” are partial since they are only
well-deﬁned on structures of the same shape. As we shall see, they may also be
non-deterministic; that is, a “zip” is a relation rather than a function. Finally,
the arity of the two datatypes, F and G , need not be the same; for example,
the classical zip function maps pairs of lists to lists of pairs, and pairing has two
arguments whereas list formation has just one. (This is a complication that we
will ignore here. See [6, 8] for full details.)
3.1
Structure Multiplication
A good example of the beauty of the “zip” generalisation is aﬀorded by what we
shall call “structure multiplication”. (This example we owe to D.J. Lillie [private
communication, December 1994].) A simple, concrete example of structure mul-
tiplication is the following. Given two lists [a1 ,a2 , . . .] and [b1 , b2 , . . .] form a
matrix in which the ( i,j )th element is the pair ( ai , bj ). We call this “structure
multiplication” because the input type is the product List.A×List.B for some
types A and B .
Given certain basic functions, this task may be completed in one of two ways.
The ﬁrst way has two steps. First, the list of a ’s is broadcast over the list of
b ’s to form the list
[([a1 ,a2 , . . . ,an],b1),([a1 ,a2 , . . . ,an], b2), . . .]
Then each b is broadcast over the list of a ’s. The second way is identical but
for an interchange of “ a ” and “ b ”.
Both methods return a list of lists, but the results are not identical. The
connection between the two results is that one is the transpose of the other. The
two methods and the connection between them are summarised in the following
diagram.
List.A×List.B


@@@@@
R
List.(List.A×B)
List.(A×List.B)
List.(List.(A×B))
?

- List.(List.(A×B))
?

Generic Properties of Datatypes
105
The point we want to make is that there is an obvious generalisation of
this procedure: replace List.A by F.A and List.B by G.B for some arbitrary
datatypes F and G . Doing so leads to the realisation that every step involves
commuting the order of a pair of datatypes.
This is made explicit in the diagram below where each arrow is labelled by
two datatypes separated by the symbol “ ↔”. For example, the middle, bottom
arrow is labelled “ F ↔G ”. This indicates a transformation that “commutes”
an “ F -structure” of “ G -structures” into a G -structure of F -structures. The
rightmost arrow is labelled “ A× ↔G ”. An ( A× )-structure is a pair of ele-
ments of which the ﬁrst is of type A . Ignoring the “ F. ” for the moment, the
transformation requires values of type A×G.B to be transformed into values of
type G.(A ×B) . That is, an ( A× )-structure of G -structures has to be trans-
formed into a G -structure of ( A× )-structures. Taking the “ F. ” into account
simply means that the transformation has to take place at each place in the
F -structure.
F.A×G.B


((F.A)×) ↔G
@@@@@
(×(G.B)) ↔F
R
G.(F.A×B)
F.(A×G.B)

					
F·(A×) ↔G
					
G.(F.(A×B))
×B ↔F
?

F ↔G
F.(G.(A×B))
A× ↔G
?
An additional edge has been added to the diagram to show the usefulness
of generalising the notion of commutativity beyond just broadcasting; this ad-
ditional inner edge shows how the commutativity of the diagram can be decom-
posed into smaller parts2.
Filling out the requirement in more detail, let us suppose that, for datatypes
F and G , zip.F.G is a family of relations indexed by types such that (zip.F.G)A
has type (G·F).A ∼(F·G).A . Then, assuming that, whenever R : U ∼V ,
F.R : F.U ∼F.V , the transformations that are used are as shown in the di-
agram below.
2 The additional edge together with the removal of the right-pointing edge in the
bottom line seem to make the diagram asymmetric. But, of course, there are sym-
metric edges. Corresponding to the added diagonal edge there is an edge connecting
G.(F.A × B) and F.(G.(A × B)) but only one of these edges is needed in the argu-
ment that follows.

106
R. Backhouse and P. Hoogendijk
F.A × G.B





(zip.((F.A)×).G)B
@
@
@
@
(zip.(×(G.B)).F)A
R
G.(F.A × B)
F.(A × G.B)

				
(zip.(F·(A×)).G)B
				
G.(F.(A × B))
G.(zip.(×B).F)A
?

(zip.F.G)A × B
F.(G.(A × B))
F.(zip.(A×).G)B
?
Now, in order to show that the whole diagram commutes (in the standard
categorical sense of commuting diagram) it suﬃces to show that the two smaller
diagrams commute. Speciﬁcally, the following two equalities must be established:
(zip.(F·(A×)).G)B = (zip.F.G)A×B ◦F.(zip.(A×).G)B
(3)
and
(zip.(F·(A×)).G)B ◦(zip.(×(G.B)).F)A
=
G.(zip.(×B).F)A ◦(zip.((F.A)×).G)B .
(4)
We shall in fact design our deﬁnition of “commuting datatypes” in such a way
that these two equations are satisﬁed (almost) by deﬁnition. In other words, our
notion of “commuting datatypes” is such that the commutativity of the above
diagram is automatically guaranteed.
3.2
Broadcasts
A family of functions bcst , where bcstA,B : F.(A×B) ←F.A×B , is said to
be a broadcast for datatype F iﬀit is parametrically polymorphic in the pa-
rameters A and B and bcstA,B behaves coherently with respect to product in
the following sense. First, the diagram
F.(A×11) 
bcstA,11
F.A×11
@@@@@
(F·rid)A
R


(rid·F)A
F.A

Generic Properties of Datatypes
107
(where ridA : A ←A×11 is the obvious natural isomorphism) commutes. Sec-
ond, the diagram
F.A×(B ×C) assF.A,B,C
(F.A×B)×C
F.(A×B)×C
bcstA,B ×idC
?
F.(A×(B ×C))
bcstA , B×C
?

F·assA,B,C
F.((A×B)×C)
bcstA×B , C
?
(where assA,B,C : A×(B ×C)←(A×B)×C is the obvious natural isomor-
phism) commutes as well.
The idea behind a “broadcast” is very simple. A broadcast for datatype F
is a way of duplicating a given value of type B across every location in an
F -structure of A ’s. The broadcasting operation is what Moggi [11] calls the
“strength” of a datatype.
The type of
bcstA,B
of datatype
F
is the same as the type of
(zip.(×B).F)A , namely F.(A×B) ∼F.A×B . We give a generic speciﬁcation
of a zip such that, if F and the family of datatypes ( ×A ) are included in a
class of commuting datatypes, then any relation satisfying the requirements of
(zip.(×B).F)A also satisﬁes the deﬁnition of bcstA,B (and is a total function).
Let us begin with an informal scrutiny of the deﬁnition of broadcast. In the in-
troduction to this section we remarked that a broadcast operation is an example
of a zip. Speciﬁcally, a broadcast operation is a zip of the form (zip.(×A).F)B .
That is, a broadcast commutes the datatypes ( ×A ) and F . Moreover, paying
due attention to the fact that the datatype F is a parameter of the deﬁnition,
we observe that all the transformations involved in the deﬁnition of a broadcast
are themselves special cases of a broadcast operation and thus of zips.
In the ﬁrst diagram there are two occurrences of the canonical isomorphism
rid . In general, we recognise a projection of type A←A×B as a broadcast
where the parameter F is instantiated to KA , the datatype that is constantly
A when applied to types. Thus ridA is (zip.(×11).KA)B for some arbitrary B .
In words, ridA commutes the datatypes ( ×11 ) and KA . Redrawing the ﬁrst
diagram above, using that all the arrows are broadcasts and thus zips, we get
the following diagram3.
3 To be perfectly correct we should instantiate each of the transformations at some
arbitrary B. We haven’t done so because the choice of which B in this case is truly
irrelevant.

108
R. Backhouse and P. Hoogendijk
F.(A×11) (zip.(×11).F)·KA
F.A×11
@@@@@
F.(zip.(×11).(KA))
R


zip.(×11).(KF.A)
F.A
Expressed as an equation, this is the requirement that
zip.(×11).(KF.A) = F.(zip.(×11).(KA)) ◦((zip.(×11).F)·KA)
(5)
Now we turn to the second diagram in the deﬁnition of boradcasts. Just
as we observed that rid is an instance of a broadcast and thus a zip, we
also observe that ass is a broadcast and thus a zip. Speciﬁcally, assA,B,C is
(zip.(×C).(A×))B . Once again, every edge in the diagram involves a zip op-
eration! That is not all. Yet more zips can be added to the diagram. For our
purposes it is crucial to observe that the bottom left and middle right nodes
—the nodes labelled F.(A×(B ×C)) and F.(A×B)×C — are connected by
the zip operation (zip.(×C).(F.(A×)))B .

 
  
 

      
   
  
  
 
 

 

  
 

 
     
 


 








     
  

  
 





  
  
 

      
 
 



 
      
   
 
   
 

 

     
 

  
This means that we can decompose the original coherence property into a
combination of two properties of zips. These are as follows. First, the lower
triangle:
(zip.(×C).(F·(A×)))B = F.(zip.(×C).(A×))B ◦(zip.(×C).F)A×B .
(6)
Second, the upper rectangle:
(zip.(×(B×C)).F)A◦(zip.(×C).((F.A)×))B
= (zip.(×C).(F·(A×)))B◦(zip.(×B).F)A×idC .
(7)
Note the strong similarity between (5) and (6). They are both instances of
one equation parameterised by three diﬀerent datatypes. There is also a sim-
ilarity between these two equations and (3); the latter is an instance of the

Generic Properties of Datatypes
109
same parameterised equation after taking the converse of both sides and as-
suming that zip.F.G= (zip.G.F)
∪. Less easy to spot is the similarity between
(4) and (7). As we shall see, however, both are instances of one equation pa-
rameterised again by three diﬀerent datatypes except that (7) is obtained by
applying the converse operator to both sides of the equation and again assuming
that zip.F.G= (zip.G.F)
∪.
4
Allegories and Relators
In this section, we begin the development of theory of datatypes in which the
primitive mechanisms for deﬁning programs are all parametrically polymorphic.
We use relation algebra (formally “allegory” theory [5]) as the basis for our
theory. There are many reasons why we choose relations as basis rather than
functions. The most compelling reason is that, for us, programming is about
constructing implementations of input-output relations. Program speciﬁcations
are thus relations; but, also, programs themselves may be relations rather than
functions — if one admits non-determinism, which we certainly wish to do.
Of more immediate relevance to generic programming is that, as we have seen,
the formal deﬁnition of parametric polymorphism necessitates an extension of the
deﬁnition of type constructors allowing them to map relations to relations. (The
extension to relations is unavoidable because, even for functions f and g , f ←g
is a relation on functions and not a function from functions to functions. Also, the
deﬁnition of commuting datatypes necessarily involves both nondeterminism and
partiality — in particular, when one of the datatypes is the constant datatype
KA , for some type A .
In this section, we build up a language of terms deﬁning relational speciﬁ-
cations and implementations. The primitive relations in this language are not
considered; our concern is with deﬁning a number of constructs that build com-
posite relations from given primitive relations and that preserve functionality
and are parametrically polymorphic.
4.1
Allegories
An allegory is a category with additional structure, the additional structure
capturing the most essential characteristics of relations. Being a category means,
of course, that for every object A there is an identity arrow idA , and that every
pair of arrows R : A ←B and S : B ←C , with matching source and target4,
can be composed: R◦S : A←C . Composition is associative and has id as a
unit.
The additional axioms are as follows. First of all, arrows of the same type
are ordered by the partial order ⊆and composition is monotonic with respect
to this order. That is,
S1◦T1 ⊆S2◦T2
⇐
S1 ⊆S2 ∧T1 ⊆T2 .
4 Note that we refer to the “source” and “target” of an arrow in a category in order
to avoid confusion with the domain and range of a relation introduced later.

110
R. Backhouse and P. Hoogendijk
Secondly, for every pair of arrows R ,S : A←B , their intersection (meet) R∩S
exists and is deﬁned by the following universal property, for each X : A←B ,
X ⊆R ∧X ⊆S
≡
X ⊆R ∩S .
Finally, for each arrow R : A←B its converse R
∪: B ←A exists. The converse
operator is deﬁned by the requirements that it is its own Galois adjoint, that is,
R
∪⊆S
≡
R ⊆S
∪,
and is contravariant with respect to composition,
(R◦S)
∪= S
∪◦R
∪.
All three operators of an allegory are connected by the modular law, also known
as Dedekind’s law [12]:
R◦S ∩T ⊆(R ∩T ◦S
∪)◦S .
The standard example of an allegory is Rel , the allegory with sets as objects
and relations as arrows. With this allegory in mind, we refer henceforth to the
arrows of an allegory as “relations”.
(Note that we no longer use the symbol “ ∼” in addition to the symbol “ ←”
as a means of distingushing relations from functions. From here on, relations are
our prime interest, unless we speciﬁcally say otherwise.)
4.2
Relators
Now that we have the deﬁnition of an allegory we can give the deﬁnition of a
relator.
Deﬁnition 2 (Relator).
A relator is a monotonic functor that commutes
with converse. That is, let A and B be allegories. Then the mapping F : A←B
is a relator iﬀ,
F.R : F.A←F.B
⇐
R : A←B ,
(8)
F.R ◦F.S = F.(R◦S)
for each R : A←B and S : B ←C
,
(9)
F.idA = idF.A
for each object A
,
(10)
F.R ⊆F.S
⇐R ⊆S
for each R : A←B and S : A←B
,
(11)
(F.R)
∪= F.(R
∪)
for each R : A←B
.
(12)
2
Two examples of relators have already been given. List is a unary relator,
and product is a binary relator. List is an example of an inductively-deﬁned
datatype; in [1] it was observed that all inductively-deﬁned datatypes are rela-
tors.
A design requirement which led Backhouse to the above deﬁnition of a relator
[1, 2] is that a relator should extend the notion of a functor but in such a way

Generic Properties of Datatypes
111
that it coincides with the latter notion when restricted to functions. Formally,
relation R : A ←B is total iﬀ
idB ⊆R
∪◦R ,
and relation R is single-valued or simple iﬀ
R ◦R
∪⊆idA .
A function is a relation that is both total and simple. It is easy to verify that total
and simple relations are closed under composition. Hence, functions are closed
under composition too. In other words, the functions form a sub-category. For an
allegory A , we denote the sub-category of functions by Map(A) . In particular,
Map(Rel) is the category having sets as objects and functions as arrows. Now
the desired property of relators is that relator F : A←B is a functor of type
Map(A)←Map(B) . It is easily shown that our deﬁnition of relator guarantees
this property. For instance, that relators preserve totality is proved as follows:
(F.R)
∪◦F.R
=
{
converse and relators commute: (12)
}
F.(R
∪) ◦F.R
=
{
relators distribute through composition: (9)
}
F.(R
∪◦R)
⊇
{
assume idB ⊆R
∪◦R ,
relators are monotonic: (11)
}
F.idB
=
{
relators preserve identities: (10)
}
idF.B .
The proof that relators preserve simplicity is similar.
Note how this little calculation uses all four requirements for F to be a
relator.
The following characterisation of functions proves to be very useful (for one
reason because it is a Galois connection).
Deﬁnition 3 (Function).
An arrow f : A←B is a function if, for all arrows,
R : C ←B and S : A←C ,
R ◦f
∪⊆S
≡R ⊆S ◦f .
2
Exercise 4.
Show that deﬁnition 3 is equivalent to the conjunction of the two
properties f ◦f
∪⊆idA and idB ⊆f
∪◦f .
2

112
R. Backhouse and P. Hoogendijk
4.3
Composition and Relators Are Parametric
As we go along, we introduce various building blocks for constructing speci-
ﬁcations and programs. As these are introduced, we check whether they are
parametric. Composition and relators are two such building blocks. So, in this
section, we formulate and verify their parametricity properties.
This is, in fact, very straightforward. The advantage of using point-free re-
lation algebra is that we can give a succinct deﬁnition of the arrow operator,
leading to more succinct formulations of its basic properties and, hence, easier
proofs.
There are several equivalent ways of deﬁning the arrow operator, all of which
are connected by deﬁnition 3. The one obtained directly from the pointwise
deﬁnition is:
(f, g) ∈R←S
≡
f
∪◦R ◦g ⊇S .
(13)
Another form, which we sometimes use, is:
(f, g) ∈R←S
≡R◦g ⊇f ◦S .
(14)
Now, a relator F has type ⟨∀α,β ::(F.α ←F.β) ←(α ←β)⟩. (The two occur-
rences “ F ” in “ F.α ” and “ F.β ” are maps from objects to objects, the ﬁrst
occurrence is a map from arrows to arrows.) Parametricity thus means that., for
all relations R and S , and all functions f and g ,
(F.f , F.g) ∈F.R ←F.S
⇐
(f, g) ∈R←S .
Here we have used the pointwise deﬁnition (2) of the arrow operator. Now using
the point-free deﬁnition (13), this reduces to:
(F.f)
∪◦F.R ◦F.g ⊇F.S
⇐
f
∪◦R ◦g ⊇S .
This we verify as follows:
(F.f)
∪
◦F.R
◦F.g
=
{
converse commutes with relators: (12)
}
F.(f
∪)
◦F.R
◦F.g
=
{
relators distribute through composition: (9)
}
F.(f
∪◦R ◦g)
⊇
{
assume f
∪◦R ◦g
⊇
S
relators are monotonic: (11)
}
F.S .
Note how this little calculation uses three of the requirements of F being a
relator. So, by design, relators preserve functionality and are parametric.
Exercise 5.
Formulate and verify the property that composition is parametri-
cally polymorphic.
2

Generic Properties of Datatypes
113
4.4
Division and Tabulation
The allegory Rel has more structure than we have captured so far with our
axioms. For instance, in Rel we can take arbitrary unions (joins) of relations.
There are also two “division” operators, and Rel is “tabulated”. In full, Rel is
a unitary, tabulated, locally complete, division allegory. For full discussion of
these concepts see [5] or [4]. Here we brieﬂy summarise the relevant deﬁnitions.
We say that an allegory is locally complete if for each set S of relations of
type A←B , the union ∪S : A←B exists and, furthermore, intersection and
composition distribute over arbitrary unions. The deﬁning property of union is
that, for all X : A ←B ,
∪S ⊆X ≡∀(S ∈S :: S ⊆X) .
We use the notation ⊥⊥A,B for the smallest relation of type A←B and ⊤⊤A,B
for the largest relation of the same type.
The existence of a largest relation for each pair of objects A and B is
guaranteed by the existence of a “unit” object, denoted by 11 . We say that
object 11 is a unit if id11 is the largest relation of its type and for every object
A there exists a total relation !A : 11 ←A . If an allegory has a unit then it is
said to be unitary.
The most crucial consequence of the distributivity of composition over union
is the existence of two so-called division operators “ \ ” and “/”. Speciﬁcally, we
have the following three Galois-connections. For all R : A←B , S : B ←C and
T : A←C ,
R◦S ⊆T
≡
S ⊆R\T
,
R◦S ⊆T
≡
R ⊆T/S ,
S ⊆R\T
≡R ⊆T/S ,
(where, of course, the third is just a combination of the ﬁrst two).
Note that R\T : B ←C and T/S : A←B . The interpretation of the factors
is
(b,c)∈R\T
≡
∀(a : (a,b)∈R : (a,c)∈T) ,
(a,b)∈T/S
≡
∀(c : (b,c)∈S : (a,c)∈T) .
The ﬁnal characteristic of Rel is that it is “tabular”. That is, each relation is
a set of ordered pairs. Formally, we say that an object C and a pair of functions
f : A ←C and g : B ←C is a tabulation of relation R : A←B if
R = f ◦g
∪
∧
f
∪◦f ∩g
∪◦g = idC .
An allegory is said to be tabular if every relation has a tabulation.
Allegory Rel is tabular. Given relation R : A ←B , deﬁne C to be the sub-
set of the cartesian product A×B containing the pairs of elements for which
(x,y)∈R . Then the pair of projection functions outl : A←C and outr : B ←C
is a tabulation of R .

114
R. Backhouse and P. Hoogendijk
If allegory B is tabular, a functor commutes with converse if it is monotonic.
(Bird and De Moor [4] claim an equivalence but their proof is ﬂawed.) So, if
we deﬁne a relator on a tabular allegory, one has to prove just requirement
(11), from which (12) can be deduced. Property (12) is, however, central to
many calculations and can usually be easily established without an appeal to
tabularity. The same holds for property (9) which —in a tabulated allegory— is
a consequence of (12) and the fact that relators are functors in the underlying
category of maps: the property is also central to many calculations and can
usually be easily established without an appeal to tabularity. For this reason we
prefer to retain the deﬁnition that we originally proposed.
4.5
Domains
In addition to the source and target of a relation it is useful to know their domain
and range. The domain of a relation R : A←B is that subset R> of idB deﬁned
by the Galois connection:
R ⊆⊤⊤A,B ◦X
≡R> ⊆X
for each X ⊆idB.
(15)
The range of R : A←B , which we denote by R<, is the domain of R
∪.
The interpretation of the domain of a relation is the set of all y such that
(x,y)∈R for some x . We use the names “domain” and “range” because we
usually interpret relations as transforming “input” y on the right to “output”
x on the left. The domain and range operators play an important role in a
relational theory of datatypes.
5
Datatype = Relator + Membership
This section deﬁnes a class of relators that we call the “regular” relators. This
class contains a number of primitive relators —the constant relators, one for
each type, product and coproduct— and a number of composite relators. We
also introduce the notion of the “membership” relation. The regular relators
correspond to the (non-nested) datatypes that one can deﬁne in a functional
programming language. They all have an associated membership relation; this
leads to the proposal, ﬁrst made by De Moor and Hoogendijk [7], that a datatype
is a relator with membership.
There are two basic means for forming composite relators, namely induction
and pointwise closure. We begin with a brief discussion of pointwise closure,
since we need to say something about it, but do not go into it here.
5.1
Pointwise Closure
For the purposes of this discussion, let us take for granted that List is an en-
dorelator and product (denoted by an inﬁx “ × ”) is a binary relator. By “endo”,
we mean that the source and target allegories of List are the same. By “binary”,

Generic Properties of Datatypes
115
we mean that, for some allegory A , product maps pairs of objects of A to an
object of A , and pairs of arrows of A to an arrow of A .
From these two relators, we can form new relators by so-called “pointwise clo-
sure”. One simple way is composition: deﬁning List·List by, for all X (whether
an arrow or an object), (List·List).X =List.(List.X) , it is easily checked that
List·List is a relator. Indeed, it is easy to verify that the functional composition
of two relators F : A ←B and G : B ←C , which we denote by F·G , is a rela-
tor. There is also an identity relator for each allegory A , which we denote by Id
leaving the speciﬁc allegory to be inferred from the context. The relators thus
form a category —a fact that we need to bear in mind later— .
Another way to form new relators is by tupling and projection. Tupling
permits the deﬁnition of relators that are multiple-valued. So far, all our exam-
ples of relators have been single-valued. Modern functional programming lan-
guages provide a syntax whereby relators (or, more precisely, the corresponding
functors) can de deﬁned as datatypes. Often datatypes are single-valued, but
in general they are not. Mutually-recursive datatypes are commonly occurring,
programmer-deﬁned datatypes that are not single-valued. But composite-valued
relators also occur in the deﬁnition of single-valued relators. For example, the
(single-valued) relator F deﬁned by F.R =R×R is the composition of the re-
lator × after the (double-valued) doubling relator. More complicated examples
like the binary relator ⊗that maps the pair (R, S) to R×S×S involve pro-
jection (of the pair (R, S) onto components R and S ) as well as repetition
(doubling), and product. The programmer is not usually aware of this because
the use of multiple-valued relators is camouﬂaged by the use of variables. For
our purposes, however, we need a variable-free mechanism for composing rela-
tors. This is achieved by making the arity of a relator explicit and introducing
mechanisms for tupling and projection.
In order to make all this precise, we need to consider a collection of allegories
created by closing some base allegory C under the formation of ﬁnite cartesian
products. (The cartesian product of two allegories, deﬁned in the usual point-
wise fashion, is clearly an allegory. Moreover, properties such as unitary, locally
complete etc. are preserved in the process.) An allegory in the collection is thus
Ck where k , the arity of the allegory is either a natural number or l∗m where
l is an arity and m is a number.
The arity of a relator F is k ←l if the target of F is Ck and its source is
Cl . We write F : k ←l rather than the strictly correct F : Ck ←Cl . A relator
with arity 1←1 is called an endorelator and a relator with arity 1←k , for
some k , is called single-valued.
To our collection of primitive relators, we need to add “duplicating” relators,
∆k , of arity k∗m ←k (that simply duplicate their arguments m times) and
projection relators, Projm , of arity k ←k∗m (that project vectors onto their
components).
By using variables, tupling and projection are made invisible; functions de-
ﬁned on the primitive components are implicitly extended “pointwise” to com-
posites formed by these relators.

116
R. Backhouse and P. Hoogendijk
Although the process of pointwise closure is seemingly straightforward, it
is a non-trivial exercise to formulate it precisely, and the inclusion of arity in-
formation complicates the discussion substantially. For this reason, we restrict
attention almost exculsively to endorelators. Occasionally, we illustrate how tu-
pling is handled using the case of a pair of relators. For this case, if F and G
are both endorelators , we use F∆G to denote the relator of arity 2←1 such
that (F∆G).R = (F.R , G.R) . Complete accounts can be found in [8, 6].
Deﬁnition 4 (Pointwise Closed).
A collection of relators is said to be
pointwise closed with base allegory C if each relator in the collection has type
Ck←Cl for some arities k and l , and the collection includes all projections, and
is closed under functional composition and tupling.
2
5.2
Regular Relators
The “regular relators” are those relators constructed from three primitive (classes
of) relators by pointwise closure and induction.
For each object
A
in an allegory, there is a relator
KA
deﬁned by
KA.R =idA . Such relators are called constant relators.
A coproduct of two objects consists of an object and two injection relations.
The object is denoted by A+B and the two relations by inlA,B : A+B ←A
and inrA,B : A+B ←B . For the injection relations we require that
inl
∪
A,B ◦inlA,B = idA
and
inr
∪
A,B ◦inrA,B = idB ,
(16)
inl
∪
A,B ◦inrA,B = ⊥⊥A,B ,
(17)
and
(inlA,B ◦inl
∪
A,B) ∪(inrA,B ◦inr
∪
A,B) = idA+B .
(18)
Having the functions inl and inr , we can deﬁne the junc operator: for all
R : C ←A and S : C ←B ,
R▽S
= (R ◦inl
∪
A,B) ∪(S ◦inr
∪
A,B) ,
(19)
and the coproduct relator: for all R : C ←A and S : D ←B
R+S
= (inlC,D ◦R) ▽(inrC,D ◦S) .
In the future, we adopt the convention that “ ▽” and “ + ” have equal prece-
dence, which is higher than the precedence of composition, which, in turn, is
higher than the precedence of set union. We will also often omit type informa-
tion (the subscripts attached to constants like inl ).
Exercise 6.
Prove the following properties of the junc operator:
R▽S ⊆X
≡R ⊆X◦inl ∧S ⊆X◦inr
,
the computation rules

Generic Properties of Datatypes
117
R▽S ◦inl = R
, and
R▽S ◦inr = S
,
and
X ⊆R▽S
≡X◦inl⊆R ∧X◦inr ⊆S .
Hence conclude the universal property
X =R▽S
≡X◦inl=R ∧X◦inr =S .
Prove, in addition, the fusion laws:
Q ◦R▽S = (Q◦R)▽(Q◦S) ,
R▽S ◦T+U = (R◦T)▽(S◦U) .
Formulate and verify the parametricity property of “ ▽” . Show also that “ + ”
is a relator.
2
A product of two objects consists of an object and two projection arrows.
The object is denoted by A×B and the two arrows by outlA,B : A←A×B
and outrA,B : B ←A×B . For the arrows we require them to be functions and
that
outlA,B ◦outr
∪
A,B = ⊤⊤A,B ,
(20)
and
outl
∪
A,B ◦outlA,B ∩outr
∪
A,B ◦outrA,B = idA×B .
(21)
Properties (20) and (21) are the same as saying that A×B , outlA,B
and
outrA,B , together, tabulate ⊤⊤A,B . Recalling that a unitary allegory is an al-
legory in which ⊤⊤A,B exists for each pair of objects, A , B , it follows that
products exist in a unitary, tabular allegory. (Note, however, that it is not nec-
essary for all arrows to have a tabulation for products to exist.)
Having the projection functions outl and outr , we can deﬁne the split op-
erator5 on relations: for all R : A←C and S : B ←C
R△S
=
outl
∪
A,B ◦R ∩outr
∪
A,B ◦S ,
(22)
and the product relator: for all for R : C ←A and S : D ←B ,
R×S
= (R ◦outlA,B) △(S ◦outrA,B) .
Exercise 7.
In category theory, coproduct and product are duals. Our def-
initions of product and coproduct are dual in the sense that compositions are
reversed (as in category theory), set union is replaced by set intersection, and the
5 The symbol “ ∆” is conventionally used in category theory to denote the doubling
functor. Its similarity with the symbol “ △”, which we use to denote the split oper-
ator, is by design and not coincidence. Category theoreticians, however, use ⟨R,S⟩
instead of R△S , thereby failing to highlight the correspondence. They also use [R,S]
where we use R▽S , thus obscuring the duality.

118
R. Backhouse and P. Hoogendijk
empty relation, ⊥⊥, is replaced by the universal relation, ⊤⊤. The properties
of the two are not completely dual, however, because composition distributes
through set union but does not distribute through set intersection. This makes
proofs about product and split more diﬃcult than proofs about coproduct and
junc. Nevertheless, crucial properties do dualise. In particular, product is a re-
lator, and split is parametric.
This exercise is about proving these properties omitting the harder parts.
For the full proofs see [1, 2, 8].
[Hard] Prove the computation rules
outl ◦R△S = R ◦S>
, and
outr ◦R△S = S ◦R>
.
(Hint: use the modular identity and the fact that outl is a function. You should
also use the identity R ◦S> = R ∩⊤⊤◦S .)
Assuming the split-cosplit rule
(R△S)
∪◦T △U = (R
∪◦T)∩(S
∪◦U) ,
prove the fusion laws
R×S ◦T △U = (R◦T)△(S◦U)
, and
R×S ◦T×U = (R◦T)×(S◦U) .
Fill in the remaining elements of the proof that “ × ” is a relator. Finally, formu-
late and verify the parametricity property of “ △” .
2
Tree relators are deﬁned as follows. Suppose that relation in : A←F.A is
an initial F -algebra. That is to say, suppose that for each relation R : B ←F.B
(thus each “ F -algebra”) there exists a unique F−homomorphism to R from
in . We denote this unique homomorphism by ([F ; R]) and call it a catamorphism.
Formally, ([F ; R]) and in are characterized by the universal property that, for
each relation X : B ←A and each relation R : B ←F.B ,
X = ([F ; R])
≡
X◦in = R ◦F.X .
(23)
Now, let ⊗be a binary relator and assume that, for each A ,
inA : T.A ←A⊗T.A
is an initial algebra of ( A⊗)6. Then the mapping
T
deﬁned by, for all
R : A←B ,
T.R = ([A⊗; inB ◦R⊗idT.B])
is a relator, the tree relator induced by ⊗.
(Characterization (23) can be weakened without loss of generality so that
the universal quantiﬁcations over relations X and R are restricted to universal
quantiﬁcations over functions X and R . This, in essence, is what Bird and De
Moor [4] refer to as the Eilenberg-Wright lemma.)
6 Here and elsewhere we use the section notation ( A⊗) for the relator ⊗(KA ∆Id) .

Generic Properties of Datatypes
119
Exercise 8.
The steps to show that T is a relator and catamorphisms are
parametric are as follows.
1. Show that in is an isomorphism. The trick is to construct R and S so
that ([F ; R])=idA and ([F ; S]) ◦in = ([F ; R]) . (For those familiar with initial
algebras in category theory, this is a standard construction.)
2. Now observe that ([F ; R]) is the unique solution of the equation
X::
X = R ◦F.X ◦in
∪.
By Knaster-Tarski, it is therefore the least solution of the equation
X::
R ◦F.X ◦in
∪⊆X
and the greatest solution of the equation
X::
X ⊆R ◦F.X ◦in
∪.
(This relies on the assumption that the allegory is locally complete.) Use
these two properties to derive (by mutual inclusion) the fusion property:
R ◦([F ; S]) = ([F ; T])
⇐
R ◦S = T ◦F.R .
3. Hence derive the fusion law
([A⊗; R]) ◦T.S = ([A⊗; R ◦S⊗id])
from which (and earlier properties) the fact that T is a relator and cata-
morphisms are parametric can be veriﬁed.
Complete this exercise.
2
5.3
Natural Transformations
Reynolds’ characterisation of parametric polymorphism predicts that certain
polymorphic functions are natural transformations. The point-free formulation
of the deﬁnition of the arrow operator (14) helps to see this.
Consider, for example, the reverse function on lists, denoted here by rev .
This has polymorphic type ⟨∀α :: List.α ←List.α⟩. So, it being parametric is the
statement
(revA, revB) ∈List.R ←List.R
for all relations R : A←B . That is,
List.R ◦revB ⊇revA ◦List.R
for all relations R. Similarly the function that makes a pair out of a single value,
here denoted by fork , has type ⟨∀α :: α×α ←α⟩. So, it being parametric is the
statement
R×R ◦forkB ⊇forkA ◦R
for all relations R : A←B .

120
R. Backhouse and P. Hoogendijk
The above properties of rev and fork are not natural transformation prop-
erties because they assert an inclusion and not an equality; they are sometimes
called “lax” natural transformation properties. It so happens that the inclusion
in the case of rev can be strengthened to an equality but this is certainly not
the case for fork . Nevertheless, in the functional programmer’s world being a
lax natural transformation between two relators is equivalent to being a natural
transformation between two functors as we shall now explain.
Since relators are by deﬁnition functors, the standard deﬁnition of a natural
transformation between relators makes sense. That is to say, we deﬁne a collec-
tion of relations θ indexed by objects (equivalently, a mapping θ of objects to
relations) to be a natural transformation of type F ←G , for relators F and G
iﬀ
F.R ◦θB = θA ◦G.R
for each R : A←B .
However, as illustrated by fork above, many collections of relations are not
natural with equality but with an inclusion. That is why we deﬁne two other
types of natural transformation denoted by F ← G and F →G , respectively.
We deﬁne:
θ : F ← G
=
F.R ◦θB ⊇θA ◦G.R
for each R : A←B
and
θ : F →G
=
F.R ◦θB ⊆θA ◦G.R
for each R : A←B
.
A relationship between naturality in the allegorical sense and in the categorical
sense is given by two lemmas. Recall that relators respect functions, i.e. relators
are functors on the sub-category Map . The ﬁrst lemma states that an allegorical
natural transformation is a categorical natural transformation:
(F.f ◦θB = θA ◦G.f
for each function f : A←B ) ⇐θ : F ← G .
The second lemma states the converse; the lemma is valid under the assumption
that the source allegory of the relators F and G is tabular:
θ : F ← G ⇐(F.f ◦θB = θA ◦G.f
for each function f : A←B ) .
In the case that all elements of the collection θ are functions we thus have:
θ : F ←G in A ≡θ : F ←G in Map(A)
where by “in X ” we mean that all quantiﬁcations in the deﬁnition of the type
of natural transformation range over the objects and arrows of X .
Exercise 9.
Prove the above lemmas.
2
Since natural transformations of type F ←G are the more common ones
and, as argued above, agree with the categorical notion of natural transformation
in the case that they are functions, we say that θ is a natural transformation if
θ : F ← G and we say that θ is a proper natural transformation if θ : F ←G .

Generic Properties of Datatypes
121
(As mentioned earlier, other authors use the term “lax natural transformation”
instead of our natural transformation.)
The natural transformations studied in the computing science literature are
predominantly (collections of) functions. In contrast, the natural transformations
discussed here are almost all non-functional either because they are partial or
because they are non-deterministic (or both).
The notion of arity is of course applicable to all functions deﬁned on prod-
uct allegories; in particular natural transformations have an arity. A natural
transformation of arity k ←l maps an l -tuple of objects to a k -tuple of rela-
tions. The governing rule is: if θ is a natural transformation to F from G (of
whatever type — proper or not) then the arities of F and G and θ must be
identical. Moreover, the composition θ◦κ of two natural transformations (de-
ﬁned by (θ◦κ)A = θA ◦κA ) is only valid if θ and κ have the same arity (since
the composition is componentwise composition in the product allegory).
5.4
Membership and Fans
Since our goal is to use naturality properties to specify relations it is useful
to be able to interpret what it means to be “natural”. All interpretations of
naturality that we know of assume either implicitly or explicitly that a datatype
is a way of structuring information and, thus, that one can always talk about
the information stored in an instance of the datatype. A natural transformation
is then interpreted as a transformation of one type of structure to another type
of structure that rearranges the stored information in some way but does no
actual computations on the stored information. Doing no computations on the
stored information guarantees that the transformation is independent of the
stored information and thus also of the representation used when storing the
information.
Hoogendijk and De Moor have made this precise [7]. Their argument, brieﬂy
summarised here, is based on the thesis that a datatype is a relator with a
membership relation.
Suppose F is an endorelator. The interpretation of F.R is a relation between
F -structures of the same shape such that corresponding values stored in the two
structures are related by R . For example, List.R is a relation between two lists
of the same length —the shape of a list is its length— such that the i th element
of the one list is related by R to the i th element of the other. Suppose A is
an object and suppose X ⊆idA . So X is a partial identity relation; in eﬀect X
selects a subset of A , those values standing in the relation X to themselves. By
the same token, F.X is the partial identity relation that selects all F -structures
in which all the stored values are members of the subset selected by X. This
informal reasoning is the basis of the deﬁnition of a membership relation for the
datatype F.
The precise speciﬁcation of membership for F is a collection of relations
mem
(indexed
by
objects
of
the
source
allegory
of
F )
such
that
memA : A←F.A and such that F.X is the largest subset Y of idF.A whose
“members” are elements of the set X . Formally, mem is required to satisfy the
property:

122
R. Backhouse and P. Hoogendijk
∀(X, Y : X ⊆idA ∧Y ⊆idF.A: F.X ⊇Y
≡(memA◦Y )< ⊆X)
(24)
Note that (24) is a Galois connection. A consequence is that a necessary condition
for relator F to have membership is that it preserve arbitrary intersections of
partial identities. In [7] an example due to P.J. Freyd is presented of a relator
that does not have this property. Thus, if one agrees that having membership
is an essential attribute of a datatype, the conclusion is that not all relators are
datatypes.
Property (24) doesn’t make sense in the case that F is not an endorelator
but the problem is easily rectiﬁed. The general case that we have to consider is
a relator of arity k ←l for some numbers k and l . We consider ﬁrst the case
that k is 1 ; for k >1 the essential idea is to split the relator into l component
relators each of arity 1←k . For illustrative purposes we outline the case that
l =2 .
The interpretation of a binary relator ⊗as a datatype-former is that a
structure of type A0⊗A1 , for objects A0 and A1 , contains data at two places:
the left and right argument. In other words, the membership relation for ⊗
has two components, mem0 : A0 ←A0⊗A1 and mem1 : A1 ←A0⊗A1 , one for
each argument. Just as in the endo case, for all ⊗-structures being elements
of the set X0⊗X1 , for partial identities X0 and X1 , the component for the
left argument should return all and only elements of X0 , the component for the
right argument all and only elements of X1 . Formally, we demand that, for all
partial identities X0 ⊆idA0 , X1 ⊆idA1 and Y ⊆idA0⊗A1 ,
X0⊗X1 ⊇Y
≡
(mem0◦Y )< ⊆X0 ∧(mem1◦Y )< ⊆X1
(25)
The rhs of (25) can be rewritten as
((mem0 ,mem1) ◦∆2Y )< ⊆(X0 , X1)
where ∆2 denotes the doubling relator: ∆2Y = (Y , Y ) . Now, writing mem in
place of ( mem0 ,mem1 ), A in place of ( A0 ,A1 ) and X in place of ( X0 , X1 ),
equation (25) becomes, for all partial identities X ⊆idA and Y ⊆id(⊗)A ,
(⊗)X ⊇Y
≡(mem ◦∆2Y )< ⊆X .
In fact, in [7] (24) is not used as the deﬁning property of membership. Instead the
following deﬁnition is used, and it is shown that (24) is a consequence thereof.
(As here, [7] only considers the case l = 1 .)
Arrow mem is a membership relation of endorelator F , if for object A
memA : A←F.A
and for each pair of objects A and B and each R : A←B ,
F.R ◦memB\idB = memA\R .
(26)
Properties (26) and (24) are equivalent under the assumption of extensionality
as shown by Hoogendijk [8].

Generic Properties of Datatypes
123
Property (26) gives a great deal of insight into the nature of natural trans-
formations. First, the property is easily generalised to:
F.R ◦memB\S = memA\(R◦S)
(27)
for all R : A←B and S : B ←C . Then, it is straightforward to show that the
membership, mem , of relator F : k ←l is a natural transformation. Indeed
mem : Id←F
,
(28)
and also
mem\id : F ←Id .
(29)
Having established these two properties, the —highly signiﬁcant— observation
that mem and mem\id are the largest natural transformations of their types
can be made. Finally, and most signiﬁcantly, suppose F and G are relators
with memberships mem.F and mem.G respectively. Then the largest natural
transformation of type F ←G is mem.F\mem.G .
Theorem 1.
A polymorphic relation mem is called the membership relation
of relator F if it has type ⟨∀α :: α ←F.α⟩and satisﬁes (26), for all R . The
relation mem\id is called the fan of F .
Assuming that id is the largest natural transformation of type Id← Id ,
mem\id is the largest natural transformation of type F ← Id .
Suppose F
and G are relators with memberships mem.F
and mem.G
respectively. Then, mem.F\mem.G is the largest natural transformation of type
F ← G . (In particular, mem.F is the largest natural transformation of type
Id← F .)
Proof. As mentioned, the proofs of (28) and (29) are left as exercises for the
reader. (See below.)
Now,
⟨∀A :: θA ⊆memA\idA⟩
=
{
factors
}
⟨∀A :: memA◦θA ⊆idA⟩
⇐
{
identiﬁcation axiom: id is the largest
natural transformation of type Id ← Id
}
mem ◦θ : Id ← Id
⇐
{
typing rule for natural transformations
}
mem : Id ← F ∧θ : F ← Id
=
{
(28)
}
θ : F ← Id .
We thus conclude that mem\id is the largest natural transformation of type
F ← Id .

124
R. Backhouse and P. Hoogendijk
Now we show that mem is the largest natural transformation of its type.
Suppose θ : Id ← F . Then
θ
⊆
{
factors
}
θ
◦mem\mem
=
{
(26)
}
θ
◦F.mem
◦mem\id
⊆
{
θ : Id ← F
}
mem
◦θ
◦mem\id
⊆
{
θ : Id ← F and mem\id : F ← Id .
So θ
◦mem\id : Id ← Id .
Identiﬁcation axiom.
}
mem .
Finally, we show that mem.F\mem.G is the largest natural transformation of
its type. Suppose θ : F ←G . Then,
mem.F\mem.G
⊇
θ
=
{
factors
}
mem.G
⊇
mem.F ◦θ
⇐
{
mem.G is the largest natural transformation
of its type
}
mem.F ◦θ : Id ← G
=
{
mem.F : Id ← F and θ : F ← G
composition of natrual transformations
}
true .
2
Exercise 10.
Verify (28) and (29).
2
The insight that these properties give is that natural transformations be-
tween datatypes can only rearrange values; computation on the stored values
or invention of new values is prohibited. To see this, let us consider each of the
properties in turn. A natural transformation of type Id← F constructs values
of type A given a structure of type F.A . The fact that the membership relation
for F is the largest natural transformation of type Id← F says that all val-
ues created by such a natural transformation must be members of the structure
F.A . Similarly, a natural transformation θ of type F ← G constructs values of

Generic Properties of Datatypes
125
type F.A given a structure of type G.A . The fact that mem.F\mem.G is the
largest natural transformation of type F ←G means that every member of the
F -structure created by θ is a member of the input G -structure. A proper nat-
ural transformation θ : F ←G has types F ←G and F →G . So θ has type
F ← G and θ
∪has type G ←F . Consequently, a proper natural transformation
copies values without loss or duplication.
The natural transformation mem\id , the largest natural transformation of
type F ← Id , is called the canonical fan of F . It transforms an arbitrary value
into an F -structure by non-deterministically creating an F -structure and then
copying the given value at all places in the structure. It plays a crucial role in the
sequel. (The name “fan” is chosen to suggest the hand-held device that was used
in olden times by digniﬁed ladies to cool themselves down.) Rules for computing
the canonical fan for all regular relators are as follows. (These are used later in
the construction of “zips”.)
fan.Proj = id
(30)
fan.(F∆G) = fan.F ∆fan.G
(31)
fan.(F·G) = F.(fan.G) ◦fan.F
(32)
fan.KA = ⊤⊤A,
(33)
fan.+ = (id▽id)
∪
(34)
fan.× = id△id
(35)
fan.T = ([id⊗; fan.⊗
∪])
∪
(36)
(where T is the tree relator induced by ⊗).
6
Commuting Datatypes — Formal Speciﬁcation
In this section we formulate precisely what we mean by two datatypes commut-
ing.
Looking again at the examples above, the ﬁrst step towards an abstract
problem speciﬁcation is clear enough. Replacing “list”, “tree” etc. by “datatype
F ” the problem is to specify an operation zip.F.G for given datatypes F and
G that maps F·G -structures into G·F -structures.
We consider only endo relators (relators of arity 1←1 ) here. For a full ac-
count, see [8, 6].
The ﬁrst step may be obvious enough, subsequent steps are less obvious. The
nature of our requirements is inﬂuenced by the relationship between parametric
polymorphism and naturality properties discussed earlier but takes place at a
higher level. We consider the datatype F to be ﬁxed and specify a collection
of operations zip.F.G indexed by the datatype G. (The fact that the index is
a datatype rather than a type is what we mean by “at a higher level”.) Such
a family forms what we call a collection of “half-zips”. The requirement is that
the collection be “parametric” in G . That is, the elements of the family zip.F

126
R. Backhouse and P. Hoogendijk
should be “logically related” to each other. The precise formulation of this idea
leads us to three requirements on “half-zips”. The symmetry between F and
G , lost in the process of ﬁxing F and varying G , is then restored by the simple
requirement that a zip is both a half-zip and the converse of a half-zip.
The division of our requirements into “half-zips” and “zips” corresponds to
the way that zips are constructed. Speciﬁcally, we construct a half-zip zip.F.G
for each datatype F in the class of regular datatypes and an arbitrary datatype
G . That is to say, for each datatype F we construct the function zip.F on
datatypes which, for an arbitrary datatype G , gives the corresponding zip op-
eration zip.F.G . The function is constructed to meet the requirement that it
deﬁne a collection of half-zips; subsequently we show that if the collection is
restricted to regular datatypes G then each half-zip is in fact a zip.
A further subdivision of the requirements is into naturality requirements and
requirements that guarantee that the algebraic structure of pointwise deﬁnition
of relators is respected (for example, the associativity of functional composition
of relators is respected). These we discuss in turn.
6.1
Naturality Requirements
Our ﬁrst requirement is that zip.F.G be natural. That is to say, its application to
an F·G -structure should not in any way depend on the values in that structure.
Suppose that F and G are relators. Then we demand that
zip.F.G : G·F ←F·G .
(37)
Thus a zip is a proper natural transformation. It transforms one structure to
another without loss or duplication of values.
Demanding naturality is not enough. Somehow we want to express that all the
members of the family zip.F of zip operations for diﬀerent datatypes G and
H are related. For instance, if we have a natural transformation θ : G ← H
then zip.F.G and zip.F.H should be “coherent” with the transformation θ .
That is to say, having both zips and θ , there are two ways of transforming
F·H -structures into G·F -structures; these should eﬀectively be the same.
One way is ﬁrst transforming an F·H -structure into an F·G -structure using
F·θ , (i.e. applying the transformation θ to each H -structure inside the F -
structure) and then commuting the F·G -structure into a G·F -structure using
zip.F.G .
Another way is ﬁrst commuting an F·H -structure into an H·F -structure
with zip.F.H and then transforming this H -structure into a G -structure (both
containing F -structures) using θ·F . So, we have the following diagram.
F·G F·θ
F·H
G·F
zip.F.G
?

θ·F
H·F
zip.F.H
?

Generic Properties of Datatypes
127
One might suppose that an equality is required, i.e.
(θ·F) ◦zip.F.H
=
zip.F.G ◦(F·θ)
(38)
for all natural transformations θ : G ←H . But this requirement is too severe
for two reasons.
The ﬁrst reason is that if θ is not functional, i.e. θ is a non-deterministic
transformation, the rhs of equation (38) may be more non-deterministic than the
lhs because of the possible multiple occurrence of θ . Take for instance F := List
and G = H := × , i.e. zip.F.G and zip.F.H are both the inverse of the zip
function on a pair of lists, and take θ := id∪swap , i.e. θ non-deterministically
swaps the elements of a pair or not. Then (θ·F) ◦zip.F.H unzips a list of pairs
into a pair of lists and swaps the lists or not. On the other hand, zip.F.G ◦(F·θ)
ﬁrst swaps some of the elements of a list of pairs and then unzips it into a pair
of lists.
The second reason is that, due to the partiality of zips, the domain of the
left side of (38) may be smaller than that of the right.
As a concrete example, suppose listify is a polymorphic function that con-
structs a list of the elements stored in a tree. The way that the tree is traversed
(inorder, preorder etc.) is immaterial; what is important is that listify is a natu-
ral transformation of type List←Tree . Now suppose we are given a list of trees.
Then it can be transformed to a list of lists by “listify”ing each tree in the list,
i.e. by applying the (appropriate instance of the) function List.listify . If all the
trees in the list have the same shape, a list of lists can also be obtained by ﬁrst
commuting the list of trees to a tree of lists (all of the same length) and then
“listify”ing the tree structure. That is, we apply the (appropriate instance of the)
function (listify·List) ◦zip.List.Tree . The two lists of lists will not be the same: if
the size of the original list is m and the size of each tree in the list is n then the
ﬁrst method will construct m lists each of length n whilst the second method
will construct n lists each of length m . However the two lists of lists are “zips”
of each other (“transposes” would be the more conventional terminology). This
is expressed by the commutativity of the following diagram in the case that the
input type List.(Tree.A) is restricted to lists of trees of the same shape.
List.(List.A) List.(listifyA)
List.(Tree.A)
List.(List.A)
(zip.List.List)A
?

listifyList.A
Tree.(List.A)
(zip.List.Tree)A
?
Note however that if we view both paths through the diagram as partial relations
of type List.(List.A)←List.(Tree.A) then the upper path (via List.(List.A) ) in-
cludes the lower path (via
Tree.(List.A) ). This is because the function
List.(listifyA) may construct a list of lists all of the same length (as required
by the subsequent zip operation) even though all the trees in the given list of

128
R. Backhouse and P. Hoogendijk
trees may not all have the same shape. The requirement on the trees is that they
all have the same size, which is weaker than their all having the same shape.
Both examples show that we have to relax requirement (38) using an inclusion
instead of equality. Having this inclusion, the requirement for θ can be relaxed
as well. So, the requirement becomes
(θ·F) ◦zip.F.H ⊆zip.F.G ◦(F·θ)
for all θ : G ← H
.
(39)
6.2
Composition
For our ﬁnal requirement we consider the monoid structure of relators under
composition. Fix relator F and consider the collection of zips, zip.F.G , indexed
by (endo)relator G . Since the (endo)relators form a monoid it is required that
the mapping zip.F is a monoid homomorphism.
In order to formulate this requirement precisely we let ourselves be driven by
type considerations. The requirement is that zip.F.(G·H) be some composition
of zip.F.G and zip.F.H of which zip.F.Id is the identity. But the type of
zip.F.(G·H) ,
zip.F.(G·H) : G·H·F ←F·G·H ,
demands that the datatype F has to be “pushed” through G·H leaving the
order of G and H unchanged. With zip.F.G we can swap the order of F and
G , with zip.F.H the order of F and H. Thus transforming F·G·H to G·H·F
can be achieved as shown below.
G·H·F

G · zip.F.H
G·F·H 
(zip.F.G) · H F·G·H
So, we demand that
zip.F.(G·H) = (G ·zip.F.H) ◦(zip.F.G·H) .
(40)
Moreover, in order to guarantee that zip.F.(G·Id) = zip.F.G = zip.F.(Id·G) we
require that
zip.F.Id = id·F
.
(41)
6.3
Half Zips and Commuting Relators
Apart from the very ﬁrst of our requirements ((37), the requirement that zip.F.G
be natural), all the other requirements have been requirements on the nature of
the mapping zip.F . Roughly speaking, (39) demands that it be parametric, and
(40) and (41) that it be functorial. We ﬁnd it useful to bundle the requirements
together into the deﬁnition of something that we call a “half zip”.
Deﬁnition 5 (Half Zip).
Consider a ﬁxed relator F and a pointwise closed
class of relators G . Then the members of the collection zip.F.G , where G ranges
over G , are called half-zips iﬀ

Generic Properties of Datatypes
129
(a) zip.F.G : G·F ←F·G , for each G ,
(b) zip.F.(G·H)
=
(G ·zip.F.H) ◦(zip.F.G·H) for all G and H ,
(c) (θ·F) ◦zip.F.H ⊆zip.F.G ◦(F·θ)
for each θ : G ← H .
(In addition, zips should respect the pointwise closure of the class G , but
this aspect of the deﬁnition is omitted here.)
2
Deﬁnition 6 (Commuting Relators).
The half-zip zip.F.G is said to be
a zip of ( F , G ) if there exists a half-zip zip.G.F such that
zip.F.G = (zip.G.F)
∪
We say that datatypes F and G commute if there exists a zip for ( F ,G ). 2
7
Consequences
In this section we address two concerns. First, it may be the case that our
requirement is so weak that it has many trivial solutions. We show that, on
the contrary, the requirement has a number of consequences that guarantee
that there are no trivial solutions. On the other hand, it could be that our
requirement for datatypes to commute is so strong that it is rarely satisﬁed.
Here we show that the requirement can be met for all regular datatypes. ( Recall
that the “regular” datatypes are the sort of datatypes that one can deﬁne in a
conventional functional programming language.) Moreover, we can even prove
the remarkable result that for the regular relators our requirement has a unique
solution.
7.1
Shape Preservation
Zips are partial operations: zip.F.G should map F –structures of ( G –structures
of the same shape) into G –structures of ( F –structures of the same shape).
This requirement is, however, not explicitly stated in our formalisation of being
a zip. In this subsection we show that it is nevertheless a consequence of that
formal requirement. In particular we show that a half zip always constructs G –
structures of ( F –structures of the same shape). We in fact show a more general
result that forms the basis of the uniqueness result for regular relators.
Let us ﬁrst recall how shape considerations are expressed. The function !A
is the function of type 11 ←A that replaces a value by the unique element of the
unit type, 11 . Also, for an arbitrary function f , F.f maps an F –structure to
an F –structure of the same shape, replacing each value in the input structure
by the result of applying f to that value. Thus F.!A maps an F –structure
(of A ’s) to an F –structure of the same shape in which each value in the input
structure has been replaced by the unique element of the unit type. We can say
that (F.!A).x is the shape of the F –structure x , and F.!A ◦f is the shape of
the result of applying function f .
Now, for a natural transformation θ of type F ←G , the shape characteris-
tics of α in general are determined by θ11 , since
F.!A ◦θA = θ11 ◦G.!A

130
R. Backhouse and P. Hoogendijk
That is, the shape of the result of applying θA is completely determined by
the behaviour of θ11 . The shape characteristics of zip.F.G , in particular, are
determined by (zip.F.G)11 since
(G·F).!A ◦(zip.F.G)A = (zip.F.G)11 ◦(F·G).!A
Our shape requirement is that a half zip maps an F – G –shape into a G – F –
shape in which all F –shapes equal the original F –shape. This we can express
by a single equation relating the behaviour of (zip.F.G)11 to that of fan.G .
Speciﬁcally, we note that (fan.G)F.11 generates from a given F -shape, x , an
arbitrary G -structure in which all elements equal x , and thus have the same
F –shape. On the other hand, F.((fan.G)11) , when applied to x , generates F -
structures with shape x containing arbitrary G -shapes. The shape requirement
(for endorelators) is thus satisﬁed if we can establish the property
(fan.G)F.11 = (zip.F.G)11 ◦F.((fan.G)11) .
(42)
This property is an immediate consequence of the following lemma.
Suppose F and G are datatypes. Then, if fan.G is the canonical fan of G ,
fan.G·F
= zip.F.G ◦(F ·fan.G) .
(43)
From equation (42) it also follows that the range of (zip.F.G)11 is the range of
(fan.G)F.11 , i.e. arbitrary G -structures of which all elements are the same, but
arbitrary, F -shape.
A more general version of (43) is obtained by considering the so-called fan
function. Recalling the characterising property of the membership relation (26),
we deﬁne the mapping ˆF (with the same arity as F , namely k ←l ) by
ˆF.R = F.R ◦mem\id ,
(44)
for all R : A←B . Then the generalisation of (43) is the following lemma.
Suppose F and G are datatypes. Then, if ˆG is the fan function of G ,
( ˆG·F).R = (zip.F.G)A ◦(F· ˆG).R ,
(45)
for all R : A←B .
It is (45) that often uniquely characterises zip.F.G . (In fact, our initial study
of zips [3] was where the notion of a fan was ﬁrst introduced, and (45) was
proposed as one of the deﬁning properties of a zip.)
7.2
All Regular Datatypes Commute
We conclude this discussion by showing that all regular relators commute. Mor-
ever, for each pair of regular relators F and G there is a unique natural trans-
formation zip.F.G satisfying our requirements.
The regular relators are constructed from the constant relators, product and
coproduct by pointwise extension and/or the construction of tree relators. The

Generic Properties of Datatypes
131
requirement that zip.F.G and zip.G.F be each other’s converse (modulo trans-
position) demands the following deﬁnitions:
zip.Id.G = id·G ,
(46)
zip.Proj.G = id ,
(47)
zip.(F ∆G).H = (zip.F.H , zip.G.H) ,
(48)
zip.(F·G).H = (zip.F.H ·G) ◦(F ·zip.G.H) .
(49)
For the constant relators and product and coproduct, the zip function is uniquely
characterised by (45). One obtains the following deﬁnitions, for all G :
zip.KA.G = fan.G·KA ,
(50)
zip.+.G = G.inl ▽G.inr ,
(51)
zip.×.G = (G.outl △G.outr)
∪.
(52)
Note that, in general, zip.KA.G and zip.×.G are not simple; moreover, the
latter is typically partial. That is the right domain of (zip.×.G)(A,B) is typi-
cally a proper subset of G.A×G.B . Zips of datatypes deﬁned in terms of these
datatypes will thus also be non-simple and/or partial. Nevertheless, broadcast
operations are always functional.
Tree relators are the last sort of relators in the class of regular relators. Let
T be the tree relator induced by ⊗as deﬁned in section 5.2. Then,
zip.T.G = ([idG⊗; G.in ◦(zip.⊗.G · (Id ∆T))]) .
(53)
8
Conclusion
The purpose of these lectures has been to introduce and study datatype-generic
programs, emphasising genericity of speciﬁcations and proof. A relational theory
of datatypes was introduced, motivated by the relational formulation of paramet-
ric polymorphism. Central to this theory is a (generic) formulation of the notion
of a relator with membership. A fundamental insight, formally expressible within
the theory, is that a natural transformation is a structural transformation that
rearranges information (but does not create or alter information) possibly repli-
cating or losing information in the process. A proper natural transformation
rearranges information without loss or replication.
The problem of “commuting” two datatypes (transforming an F -structure
of G -structures into a G -structure of F -structures) has been used to illustrate
the eﬀectiveness of a datatype-generic theory of programming. The speciﬁcation
of this problem is based on abstraction from the properties of brodacasting and
matrix transposition, and is entirely non-operational. In this way, very general
properties of the constructed programs are derived with a minimum of eﬀort.
The development of a constructive theory of datatype genericity is a chal-
lenging area of research. Almost all literature on generic programming is about
implementations with little or no regard to generic speciﬁcation or proof. But
ﬁnding the right abstractions to make the right building blocks demands that
speciﬁcation and proof be given as much consideration as implementation.

132
R. Backhouse and P. Hoogendijk
References
1. R.C. Backhouse, P. de Bruin, P. Hoogendijk, G. Malcolm, T.S. Voermans, and
J. van der Woude. Polynomial relators. In M. Nivat, C.S. Rattray, T. Rus, and
G. Scollo, editors, Proceedings of the 2nd Conference on Algebraic Methodology and
Software Technology, AMAST’91, pages 303–326. Springer-Verlag, Workshops in
Computing, 1992.
2. R.C. Backhouse, P. de Bruin, G. Malcolm, T.S. Voermans, and J. van der
Woude. Relational catamorphisms. In M¨oller B., editor, Proceedings of the IFIP
TC2/WG2.1 Working Conference on Constructing Programs from Speciﬁcations,
pages 287–318. Elsevier Science Publishers B.V., 1991.
3. R.C. Backhouse, H. Doornbos, and P. Hoogendijk. Commuting relators. Available
via World-Wide Web at http://www.cs.nott.ac.uk/˜rcb/MPC/papers, Septem-
ber 1992.
4. Richard S. Bird and Oege de Moor. Algebra of Programming. Prentice-Hall Inter-
national, 1996.
5. P.J. Freyd and A. ˇSˇcedrov. Categories, Allegories. North-Holland, 1990.
6. Paul Hoogendijk and Roland Backhouse. When do datatypes commute? In Eugenio
Moggi and Giuseppe Rosolini, editors, Category Theory and Computer Science, 7th
International Conference, volume 1290 of LNCS, pages 242–260. Springer-Verlag,
September 1997.
7. Paul Hoogendijk and Oege de Moor. Container types categorically. Journal of
Functional Programming, 10(2):191–225, 2000.
8. Paul Hoogendijk. A Generic Theory of Datatypes. PhD thesis, Department of
Mathematics and Computing Science, Eindhoven University of Technology, 1997.
9. R. Milner. A theory of type polymorphism in programming. J. Comp. Syst. Scs.,
17:348–375, 1977.
10. R. Milner. The standard ML core language. Polymorphism, II(2), October 1985.
11. E. Moggi. Notions of computation and monads. Information and Computation,
93(1):55–92, 1991.
12. J. Riguet. Relations binaires, fermetures, correspondances de Galois. Bulletin de
la Soci´et´e Math´ematique de France, 76:114–155, 1948.
13. C. Strachey. Fundamental concepts in programming languages. Lecture Notes,
International Summer School in Computer Programming, Copenhagen, August
1967.
14. P. Wadler. Theorems for free! In 4’th Symposium on Functional Programming
Languages and Computer Architecture, ACM, London, September 1989.

Basic Category Theory for Models of Syntax⋆
R.L. Crole
Department of Mathematics and Computer Science, University of Leicester,
Leicester, LE1 7RH, UK
R.Crole@mcs.le.ac.uk
Abstract. A preliminary version of these notes formed the basis of four
lectures given at the Summer School on Generic Programming, Oxford,
UK, which took place during August 2002. The aims of the notes are
to provide an introduction to very elementary category theory, and to
show how such category theory can be used to provide both abstract and
concrete mathematical models of syntax. Much of the material is now
standard, but some of the ideas which are used in the modeling of syntax
involving variable binding are quite new.
It is assumed that readers are familiar with elementary set theory and
discrete mathematics, and have met formal accounts of the kinds of syn-
tax as may be deﬁned using the (recursive) datatype declarations that
are common in modern functional programming languages. In particular,
we assume readers know the basics of λ-calculus.
A pedagogical feature of these notes is that we only introduce the cate-
gory theory required to present models of syntax, which are illustrated
by example rather than through a general theory of syntax.
1
Introduction
1.1
Prerequisites
We assume that readers already have a reasonable understanding of
– very basic (naive) set theory;
– simple discrete mathematics, such as relations, functions, preordered sets,
and equivalence relations;
– simple (naive) logic and the notion of a formal system;
– a programming language, preferably a functional one, and in particular of
recursive datatypes; language syntax presented as ﬁnite trees;
– inductively deﬁned sets; proof by mathematical and rule induction;
– the basics of λ-calculus, especially free and bound variables.
Some of these topics will be reviewed as we proceed. The appendix deﬁnes ab-
stract syntax trees, inductively deﬁned sets, and rule induction. We do not as-
sume any knowledge of category theory, introducing all that we need.
⋆This work was supported by EPSRC grant number GR/M98555.
R. Backhouse and J. Gibbons (Eds.): Generic Programming SS 2002, LNCS 2793, pp. 133–177, 2003.
c
⃝Springer-Verlag Berlin Heidelberg 2003

134
R.L. Crole
1.2
The Aims
Through these notes we aim to teach some of the very basics of category theory,
and to apply this material to the study of programming language syntax with
binding. We give formal deﬁnitions of the category theory we need, and some
concrete examples. We also give a few technical results which can be given very
abstractly, but for the purposes of these notes are given in a simpliﬁed and con-
crete form. We study syntax through particular examples, by giving categorical
models. We do not discuss the general theory of binding syntax. This is a current
research topic, but readers who study these notes should be well placed to read
(some of) the literature.
1.3
Learning Outcomes
By studying these notes, and completing at least some of the exercises, you
should
– know how simple examples of programming language syntax with binding
can be speciﬁed via (simple) inductively deﬁned formal systems;
– be able to deﬁne categories, functors, natural transformations, products and
coproducts, presheaves and algebras;
– understand a variety of simple examples from basic category theory;
– know, by example, how to compute initial algebras for polynomial functors
over sets;
– understand some of the current issues concerning how we model and imple-
ment syntax involving variable binding;
– understand some simple abstract models of syntax based on presheaves;
– know, by example, how to manufacture a categorical formulation of simple
formal systems (for syntax);
– be able to prove that the abstract model and categorical formal system are
essentially the same;
– know enough basic material to read some of the current research material
concerning the implementation and modeling of syntax.
2
Syntax Deﬁned from Datatypes
In this section we assume that readers are already acquainted with formal (pro-
gramming) languages. In particular, we assume knowledge of syntax trees; oc-
currences of, free, bound and binding variables in a syntax tree; inductive deﬁ-
nitions and proof by (rule) induction; formal systems for specifying judgements
about syntax; and elementary facts about recursive datatype declarations. Some
of these topics are discussed very brieﬂy in the Appendix, page 170. Mostly,
however, we prompt the reader by including deﬁnitions, but we do not include
extensive background explanations and intuition.
What is the motivation for the work described in these notes? Computer
Scientists sometimes want to use existing programming languages, and other

Basic Category Theory for Models of Syntax
135
tools such as automated theorem provers, to reason about (other) programming
languages. We sometimes say that the existing system is our metalanguage, (for
example Haskell) and the system to be reasoned about is our object language
(for example, the λ-calculus). Of course, the object language will (hopefully)
have a syntax, and we will need to encode, or specify, this object level language
within the metalanguage. It turns out that this is not so easy, even for fairly
simple object languages, if the object language has binders. This is true of the
λ-calculus, and is in fact true of almost any (object level) programming language.
We give four examples, each showing how to specify the syntax, or terms, of
a tiny fragment of a (object level) programming language. In each case we give a
datatype whose elements are called expressions. Such expressions denote abstract
syntax trees—see Appendix. The expressions form a superset of the set of terms
we are interested in. We then give further deﬁnitions which specify a subset of
the datatype which constitutes exactly the terms of interest. For example, in the
case of λ-calculus, the expressions are raw syntax trees, and the terms are such
trees quotiented up to α-equivalence. What we would like to be able to do is
Write down a datatype, as one can do in a functional programming
language, such that the expressions of the datatype are precisely
the terms of the object language we wish to encode.
Then, the “further deﬁnitions” alluded to above would not be needed. We would
have a very clean encoding of our object level language terms, given by an explicit
datatype and no other infrastructure (such as α-equivalence).
2.1
An Example with Distinguished Variables and without Binding
Take constructor symbols V, S and A
with arities one, one and two respec-
tively. Take also a set of variables V whose actual elements will be denoted
by vi where i ∈N. The set of expressions Exp is inductively deﬁned by the
grammar1
Exp ::= V V | S Exp | A Exp Exp
The metavariable e will range over expressions. It should be intuitively clear
what we mean by vi occurs in e, which we abbreviate by vi ∈e. We omit a
formal deﬁnition. The set of (free) variables of any e is denoted by fv(e).
Later in these notes, we will want to consider expressions e for which
fv(e) ⊂{ v0, . . . , vn−1 }
The idea is that when “constructing” an expression by using the grammar above,
we build it out of the initial segment (according to the indices) of variables. It
is convenient to give an inductive deﬁnition of such expressions. First we induc-
tively a set of judgements Γ n ⊢db e where n ≥1, Γ n def
= v0, . . . , vn−1 is a list,
1 This formal BNF grammar corresponds to a datatype declaration in Haskell.

136
R.L. Crole
0 ≤i < n
Γ n ⊢db V vi
Γ n ⊢db e
Γ n ⊢db S e
Γ n ⊢db e
Γ n ⊢db e′
Γ n ⊢db A e e′
Fig. 1. Expressions in Environment—Distinguished Variables, No Binding
0 ≤i < n
Γ n ⊢db V vi
Γ n+1 ⊢db e
Γ n ⊢db L vn e
Γ n ⊢db e
Γ n ⊢db e′
Γ n ⊢db E e e′
Fig. 2. Expressions in Environment—Distinguished Variables and Binding
and of course e is an expression. We refer to Γ n as an environment of variables.
The rules appear in Figure 1. Γ n ⊢db e is simply a notation for a binary rela-
tionship, written inﬁx style, with relation symbol ⊢db. Strictly speaking, we are
inductively deﬁning the set (of pairs) ⊢db. One can then prove by rule induction
that if Γ n ⊢db e then fv(e) ⊂Γ n. Check the simple details as an Exercise. Hint:
With the notation of the Appendix, we prove by Rule Induction
(∀(Γ n, e) ∈⊢db)
(fv(e) ⊂Γ n)
See Figure 7, page 173. One has to check property closure for each rule in Fig-
ure 1.
2.2
An Example with Distinguished Variables and Binding
We assume that readers are familiar with the syntax of the λ-calculus. One might
want to implement (encode) such syntax in a language such as Haskell. If L and
E
are new constructors then we might consider the grammar (datatype)
Exp ::= V V | L V Exp | E Exp Exp
We assume that readers are familiar with free, binding and bound variables,
in particular the idea that for a particular occurrence of a variable in a syntax
tree deﬁned by the grammar above, the occurrence is either free or bound. In
particular recall that fv(L vi e) = fv(e) \ { vi }; any occurrence of vi in L vi e
is bound, and in particular we call the occurrence vi binding. We will want to
consider expressions e for which fv(e) ⊂{ v0, . . . , vn−1 }, and we also give an
inductive deﬁnition of such expressions. More precisely we inductively deﬁne a
set of judgements Γ n ⊢db e where n ≥1. The rules appear in Figure 2. One can
then prove by rule induction that if Γ n ⊢db e then fv(e) ⊂Γ n. Check the simple
details as an Exercise. Notice that the rule for introducing abstractions L vn e
forces a distinguished choice of binding variable. This means that we lose the
usual fact that the name of the binding variable does not matter. However, the
pay-oﬀof what we call distinguished binding is that the expressions inductively

Basic Category Theory for Models of Syntax
137
x ∈∆
∆⊢ab V x
∆, x ⊢ab e
∆⊢ab L x e
∆⊢ab e
∆⊢ab e′
∆⊢ab E e e′
Fig. 3. Expressions in Environment—Arbitrary Variables and Binding
deﬁned in Figure 2 correspond exactly to the terms of the λ-calculus, without the
need to deﬁne α-equivalence. In essence, we are forced to pick a representative
of each α-equivalence class.
2.3
An Example with Arbitrary Variables and Binding
Expressions are still deﬁned by
Exp ::= V V | L V Exp | E Exp Exp
Now let ∆range over all non-empty ﬁnite lists of variables which have distinct
elements. Thus a typical non-empty ∆is v1, v8, v100, v2 ∈[ V ]. We will use
typical metavariables x, y, z which range over V. If ∆contains n ≥1 variables,
we may write ∆= x0, . . . , xn−1. Once again we inductively a set of judgements,
this time of the form ∆⊢ab e. The rules appear in Figure 3. One can then prove
by rule induction that if ∆⊢ab e then fv(e) ⊂∆. Check the simple details as an
Exercise.
It is convenient to introduce a notion of simultaneous substitution of variables
at this point. This will allow us to deﬁne the usual notion of α-equivalence of
expressions—yielding the terms of the λ-calculus. Such substitution will also be
used in our mathematical models, later on. Suppose that 0 ≤p ≤len(∆) −1.
Then elp(∆) is the pth element of ∆, with position 0 the “ﬁrst” element. We
write ϵ for the empty list. We will deﬁne by recursion over expressions e, new
expressions e{ϵ/ϵ} and e{∆′/∆}, where len(∆) = len(∆′). Informally, e{∆′/∆}
is the expression e in which any free occurrence of elp(∆) in e is replaced by
elp(∆′), with bound variables being changed to avoid capture of elp(∆′). For
example,
(L v8 (A v10 v2)){v3, v8/v8, v2} = L v11 (A v10 v8)
where the binding variable v8 is changed to v11. The deﬁnition is given in Fig-
ure 4.
We can inductively deﬁne the relation ∼α of α-equivalence
on the set
of all expressions with the single axiom2 (schema) L x e ∼α L x′ e{x′/x} where
x′ (̸= x) is any variable not in fv(e), and rules ensuring that ∼α is an equiv-
alence relation and a congruence for the constructors. Congruence for L, and
transitivity, are given by the rules (schemas)
e ∼α e′
e′ ∼α e′′
e ∼α e′′
e ∼α e′
L x e ∼α L x e′
2 Base rule.

138
R.L. Crole
We deﬁne
e{ϵ/ϵ}
def
= e
(V x){∆′/∆}
def
=











x
if
(∀p)(elp(∆) ̸= x)
elp(∆′)
if
(∃p)(elp(∆) = x)
(L x e){∆′/∆}
def
=













L x e{∆′/∆}
if
(∀p)(elp(∆′) ̸= x ∨elp(∆) ̸∈fv(e))
L x′ e{∆′, x′/∆, x}
if
(∃p)(elp(∆′) = x ∧elp(∆) ∈fv(e))
(E e e′){∆′/∆}
def
= E e{∆′/∆} e′{∆′/∆}
where
– ∆is ∆with x deleted (from position p, if it occurs) and, if x does occur, ∆′ is ∆′
with the element in position p deleted, and is otherwise ∆′; and
– x′ is the variable vw where w is 1 plus the maximum of the indices appearing in
∆′ and fv(e).
Fig. 4. Deﬁnition of Simultaneous Substitution
Write down all of the rules as an Exercise.
Note that the terms of the λ-calculus are given by the
[e]α
def
= {e′ | e′ ∼α e}
where e is any expression. One reason for also deﬁning the judgements ∆⊢ab e
is that they will be used in Section 4 to formulate a mathematical model.
2.4
An Example without Variables but with Binding
As mentioned, we assume familiarity with de Bruijn notation, and the notion of
index level. Note: You can follow the majority of these notes, without
knowing about de Bruijn terms. Just omit the sections on this topic.
Here is our grammar of raw de Bruijn expressions
Exp ::= V N | λ Exp | $ Exp Exp
Recall the basic idea is that the property of syntax which is captured by variable
binding, can also be embodied by node count in an expression. An example may
make this more transparent. Consider
e
def
= L v1 (L v5 L v4 (E v5v1 ))

Basic Category Theory for Models of Syntax
139
0 ≤j ≤n −1
n ⊢ib V j
n + 1 ⊢ib e
n ⊢ib λ e
n ⊢ib e
n ⊢ib e′
n ⊢ib $ e e′
Fig. 5. Expressions in Environment—Binding via Node Depth
The ﬁrst occurrence of v1 (binding) indicates that the second occurrence is
bound. However, if we draw out the ﬁnite tree, we can see that there is a single
path between the two variables, and the fact that the second v1 is bound by
the ﬁrst is “speciﬁed” by counting the number, 2, of nodes L strictly between
them. There is also one L between the two occurrences of v5. In de Bruijn no-
tation, e is rendered λ (λ (λ ($ 1 2))). For example, λ 0 corresponds to L v0 v0
and L v0 (L v1v0 ) to λ (λ 1). Now let n range over N. We will regard n as the
set { 0, . . . , n −1 } of natural numbers, with the elements treated as De Bruijn
indices. We inductively deﬁne a set of judgements, this time of the form n ⊢ib e.
The rules appear in Figure 5. One can then prove by rule induction that if
n ⊢ib e then e is a de Bruijn expression of level n. Check the simple details as
an Exercise.
We ﬁnish this section with an Exercise. Write down rules of the form ∆⊢
e with e corresponding to the datatype of Section 2.1, and ∆as deﬁned in
Section 2.3. Show that in this example, where variables are arbitrary and there
is no binding, the set of expressions e for which ∆⊢e is precisely the set of
elements of the datatype.
3
Category Theory
3.1
Categories
A category consists of two collections. Elements of one collection are called ob-
jects. Elements of the other collection are called morphisms. Speaking informally,
each morphism might be thought of as a “relation” or “connection” between two
objects. Here are some informal examples.
• The collection of all sets (each set is an object), together with the collection
of all set-theoretic functions (each function is morphism).
• The collection of all posets, together with all monotone functions.
• The set of real numbers R (in this case each object is just a real number r in
R), together with the order relation ≤on the set R (a morphism is a pair (r, r′)
in the set ≤).
It is important to note that the objects of a category do not have to be sets (in
the fourth example they are real numbers) and that the morphisms do not have
to be functions (in the fourth example they are elements of the order relation
≤). Of course, there are some precise rules which deﬁne exactly what a category
is, and we come to these shortly: the reader may care to glance ahead at the
deﬁnition given on page 140. We give a more complete example before coming
to the formal deﬁnition.

140
R.L. Crole
We can create an example of a category using the deﬁnitions of Section 2.1.
We illustrate carefully the general deﬁnition of a category using this example.
The collection of objects is N and the collection of morphisms is

m≥1
[ { e | Γ m ⊢db e } ]
Given any morphism es, there is a corresponding source and target. We deﬁne
src(es)
def
= max { i | vi ∈e ∧e ∈es } + 1
and
tar(es)
def
= case of

ϵ →0
[ e0, . . . , en−1 ] →n
We write es: m →n or m
es
→n to indicate the source and target of es. For
example, we have
2 [ A (A v0 v0) v1, A v1 v0, A v0 (S v0) ]
- 3
In the following situation
l
es′
- m
es - n
we say that es and es′ are composable. This means that there is a morphism
es ◦es′: l →n which can be deﬁned using es and es′, and is said to be their
composition. For example, if
1 [ S v0, A v0 v0 ]
- 2 [ A (A v0 v1) v1, A v1 v0, A v0 (S v1) ]
- 3
then the composition is
1 [ A (A (S v0) (A v0 v0)) (A v0 v0), A (A v0 v0) (S v0), A (S v0) (S (A v0 v0)) ]
- 3
Informally, the general deﬁnition of composition is that the element in position
p in es ◦es′ is the element in es in position p in which the m elements of es′
are substituted simultaneously for the free variables v0, . . . vm−1. We leave the
actual deﬁnition as an Exercise—do not forget what happens if either es or es′
is empty. Finally, for any object m there is an identity morphism,
m [ v0, . . . , vm−1 ]
- m
We now give the formal deﬁnition of a category. A category C is speciﬁed
by the following data:
• A collection ob C of entities called objects. An object will often be denoted
by a capital letter such as A, B, C . . .

Basic Category Theory for Models of Syntax
141
• A collection mor C of entities called morphisms. A morphism will often be
denoted by a small letter such as f, g, h . . .
• Two operations assigning to each morphism f its
source src(f) hich is an
object of C and its
target tar(f) lso an object of C. We write f: src(f) −→
tar(f) to indicate this, or perhaps f: A →B where A = src(f) and B = tar(f).
Sometimes we just say “let f: A →B be a morphism of C” to mean f is a
morphism of C with source A and target B.
• Morphisms f and g are composable if tar(f) = src(g). There is an operation
assigning to each pair of composable morphisms f and g their composition
which is a morphism denoted by g ◦f and such that src(g ◦f) = src(f) and
tar(g ◦f) = tar(g). So for example, if f: A →B and g: B →C, then there is a
morphism g ◦f: A →C. There is also an operation assigning to each object A
of C an
identity morphism idA: A →A. These operations are required to be
unitary
idtar(f) ◦f = f
f ◦idsrc(f) = f
and associative, that is given morphisms f: A →B, g: B →C and h: C →D
then
(h ◦g) ◦f = h ◦(g ◦f).
As an Exercise check that the operations from the previous example are unitary
and associative.
Here are some more examples of categories.
1. The category of sets and total functions, Set. The objects of the category
are sets and the morphisms are functions which are triples (A, f, B) where
A and B are sets and f ⊆A × B is a subset of the cartesian product of A
and B for which
(∀a ∈A)(∃!b ∈B)((a, b) ∈f)
We sometimes call f the graph of the function (A, f, B). The source and
target operations are deﬁned by src(A, f, B)
def
= A and tar(A, f, B)
def
= B.
Suppose that we have another morphism (B, g, C). Then tar(A, f, B) =
src(B, g, C), and the composition is given by
(B, g, C) ◦(A, f, B) = (A, g ◦f, C)
where g ◦f is the usual composition of the graphs f and g. Finally, if A is
any set, the identity morphism assigned to A is given by (A, idA, A) where
idA ⊆A × A is the identity graph. We leave the reader to check as an
Exercise that composition is an associative operation and that composition
by identities is unitary. Note: we now follow informal practice, and talk about
“functions” f as morphisms, even though f is strictly speaking the graph
component of the function (A, f, B).
2. The category of sets and partial functions, Part. The objects are sets and
the morphisms are partial functions. The deﬁnition of composition is the
expected one, namely given f: A →B, g: B →C, then for each element a of

142
R.L. Crole
A, g◦f(a) is deﬁned with value g(f(a)) if both f(a) and g(f(a)) are deﬁned,
and is otherwise not deﬁned.
3. Any preordered set (X, ≤) may be viewed as a category. Recall that a pre-
order ≤on a set X is a reﬂexive, transitive relation on X. The set of objects
is X. The set of morphisms is ≤. (X, ≤) forms a category with identity
morphisms (x, x) for each object x (because ≤is reﬂexive) and composition
(y, z)◦(x, y)
def
= (x, z) (because ≤is transitive). Note for x and y elements of
X, there is at most one morphism from x to y according to whether x ≤y
or not.
4. The category Preset has objects all preordered sets, and morphisms the
monotone functions. More precisely, a morphism with source (X, ≤X) and
target (Y, ≤Y ) is speciﬁed by giving a function f: X →Y such that if x ≤X x′
then f(x) ≤Y f(x′).
5. A discrete category is one for which the only morphisms are identities. So
a very simple example of a discrete category is given by regarding any set
as a category in which the objects are the elements of the set, there is an
identity morphism for each element, and there are no other morphisms.
6. The objects of the category F are the elements n ∈N, where we regard n
as the set { 0, . . . , n −1 } for n ≥1, and 0 is the empty set ∅. A morphism
ρ: n →n′ is any set-theoretic function.
7. Let C and D be categories. The product category C × D has as objects
all pairs (C, D) where C is an object of C and D is an object of D, and the
morphisms are the obvious pairs (f, g): (C, D) →(C′, D′).
It is an Exercise to work through the details of these examples.
In category theory, a notion which is pervasive is that of isomorphism. If two
objects are isomorphic, then they are very similar, but not necessarily identical.
In the case of Set, two sets X and Y are isomorphic just in case there is a bijection
between them—informally, they have the same number of elements. The usual
deﬁnition of bijection is that there is a function f: X →Y which is injective and
surjective. Equivalently, there are functions f: X →Y and g: Y →X which are
mutually inverse. We can use the idea that a pair of mutually inverse functions
in the category Set gives rise to bijective sets to deﬁne the notion of isomorphism
in an arbitrary category.
A morphism f: A →B in a category C is said to be an
isomorphism if
there is some g: B →A for which f ◦g = idB and g ◦f = idA. We say that g is
an inverse for f and that f is an inverse for g. Given objects A and B in C, we
say that A is isomorphic to B and write A ∼= B if such a mutually inverse
pair of morphisms exists, and we say that the pair of morphisms witnesses
the fact that A ∼= B. Note that there may be many such pairs. In the category
determined by a partially ordered set, the only isomorphisms are the identities,
and in a preorder X with x, y ∈X we have x ∼= y iﬀx ≤y and y ≤x. Note that
in this case there can be only one pair of mutually inverse morphisms witnessing
the fact that x ∼= y. Here are a couple of Exercises.

Basic Category Theory for Models of Syntax
143
(1) Let C be a category and let f: A →B and g, h: B →A be morphisms. If
f ◦h = idB and g ◦f = idA show that g = h. Deduce that any morphism f has
a unique inverse if such exists.
(2) Let C be a category and f: A →B and g: B →C be morphisms. If f and g
are isomorphisms, show that g ◦f is too. What is its inverse?
3.2
Functors
A function f: X →Y is a relation between two sets. We can think of the function
f as specifying an element of Y for each element of X; from this point of view,
f is rather like a program which outputs a value f(x) ∈Y for each x ∈X.
We might say that the element f(x) is assigned to x. A functor is rather like
a function between two categories. Roughly, a functor from a category C to a
category D is an assignment which sends each object of C to an object of D, and
each morphism of C to a morphism of D. This assignment has to satisfy some
rules. For example, the identity on an object A of C is sent to the identity in
D on the object FA, where the functor sends the object A in C to FA in D.
Further, if two morphisms in C compose, then their images under the functor
must compose in D. Very informally, we might think of the functor as “preserving
the structure” of C. Let us move to the formal deﬁnition of a functor.
A functor F between categories C and D, written F: C →D, is speciﬁed by
– an operation assigning objects FA in D to objects A in C, and
– an operation assigning morphisms Ff: FA →FB in D, to morphisms f: A →
B in C,
for which F(idA) = idF A, and whenever the composition of morphisms g ◦f is
deﬁned in C we have F(g ◦f) = Fg ◦Ff. Note that Fg ◦Ff is deﬁned in D
whenever g ◦f is deﬁned in C, that is, Ff and Fg are composable in D whenever
f and g are composable in C.
Sometimes we give the speciﬁcation of a functor F by writing the operation
on an object A as A 
→FA and the operation on a morphism f, where f: A →B,
as f: A →B 
→Ff: FA →FB. Provided that everything is clear, we sometimes
say “the functor f: C →D is deﬁned by an assignment
f: A −→B

→
Ff: FA −→FB
where f: A →B is any morphism of C.” We refer informally to C as the source
of the functor F, and to D as the target of F. Here are some examples.
1. Let C be a category. The identity functor
idC is deﬁned by idC(A)
def
= A
where A is any object of C and idC(f)
def
= f where f is any morphism of C.
2. We may deﬁne a functor F: Set →Set by taking the operation on objects to
be FA
def
= [ A ] and the operation on morphisms Ff
def
= map(f), where the
function map(f): [ A ] →[ B ] is deﬁned by
map(f)(as)
def
= case of

ϵ →ϵ
([a0, . . . , al−1]) = [f(a0), . . . , f(al−1)]

144
R.L. Crole
Being our ﬁrst example, we give explicit details of the veriﬁcation that F is
indeed a functor. To see that F(idA) = idF A note that on non-empty lists
F(idA)([a0, . . . , al−1])
def
= map(idA)([a0, . . . , al−1])
= [a0, . . . , al−1]
= id[ A ]([a0, . . . , al−1])
def
= idF A([a0, . . . , al−1]),
and to see that F(g ◦f) = Fg ◦Ff note that
F(g ◦f)([a0, . . . , al−1])
def
= map(g ◦f)([a0, . . . , al−1])
= [g(f(a0)), . . . , g(f(al−1))]
= map(g)([f(a0), . . . , f(al−1)])
= map(g)(map(f)([a0, . . . , al−1]))
= Fg ◦Ff([a0, . . . , al−1]).
3. Given categories C and D and an object D of D, the constant functor
˜D: C →D sends any object A of C to D and any morphism f: A →B of C
to idD: D →D.
4. Given a set A, recall that the powerset P(A) is the set of subsets of A. We
can deﬁne a functor P(f): Set →Set which is given by
f: A →B

→
P(f): P(A) →P(B),
where f: A →B is a function and P(f) is deﬁned by
P(f)(A′)
def
= {f(a′) | a′ ∈A′}
where A′ ∈P(A). We call P(f): Set →Set the covariant powerset func-
tor.
5. Given functors F: C →C′ and G: D →D′, the product functor
F × G: C × D →C′ × D′
assigns the morphism (Ff, Gg): (FC, FD) →(FC′, FD′) to any morphism
(f, g): (C, D) →(C′, D′) in C × D.
6. Any functor between two preorders A and B regarded as categories is pre-
cisely a monotone function from A to B.
It is an Exercise to check that the deﬁnitions deﬁne functors between categories.
3.3
Natural Transformations
Let C and D be categories and F, G: C →D be functors. Then a natural trans-
formation
α from F to G, written α: F →G, is speciﬁed by an operation

Basic Category Theory for Models of Syntax
145
which assigns to each object A in C a morphism αA: FA →GA in D, such that
for any morphism f: A →B in C, we have Gf ◦αA = αB ◦Ff. We denote this
equality by the following diagram
FA
αA- GA
FB
Ff
?
αB
- GB
Gf
?
which is said to commute. The morphism αA is called the component of the
natural transformation α at A. We also write α: F →G: C →D to indicate
that α is a natural transformation between the functors F, G: C →D. If we are
given such a natural transformation, we refer to the above commutative square
by saying “consider naturality of α at f: A →B.”
1. Recall the functor F: Set →Set deﬁned on page 143. We can deﬁne a natural
transformation rev: F →F which has components rev A: [ A ] →[ A ] deﬁned
by
rev A(as)
def
= case of

ϵ →ϵ
[ a0, . . . , al−1 ] →[al−1, . . . , a0]
where [ A ] is the set of ﬁnite lists over A (see Appendix). It is trivial to see
that this does deﬁne a natural transformation:
Ff ◦rev A([a0, . . . , al−1]) = [f(al−1), . . . , f(a0)] = rev B ◦Ff([a0, . . . , al−1]).
2. Let X and A be sets. Write X →A for the set of functions from X to A,
and let (X →A)×X be the usual cartesian product of sets. Deﬁne a functor
FX: Set →Set by setting FX(A)
def
= (X →A) × X where A is any set and
letting
FX(f): (X →A) × X −→(X →B) × X
be the function deﬁned by (g, x) 
→(f ◦g, x) where f: A →B is any function
and (g, x) ∈(X →A) × X. Then we can deﬁne a natural transformation
ev: FX →idSet by setting evA(g, x)
def
= g(x). To see that we have deﬁned
a natural transformation ev with components evA: (X →A) × X →A let
f: A →B be a set function, (g, x) ∈(X →A) × X, and note that
(idSet(f) ◦evA)(g, x) = f(evA(g, x))
= f(g(x))
= evB(f ◦g, x)
= evB(FX(f)(g, x))
= (evB ◦FX(f))(g, x).

146
R.L. Crole
It will be convenient to introduce some notation for dealing with methods of
“composing” functors and natural transformations. Let C and D be categories
and let F, G, H be functors from C to D. Also let α: F →G and β: G →H be
natural transformations. We can deﬁne a natural transformation β◦α: F →H by
setting the components to be (β◦α)A
def
= βA◦αA. This yields a category DC with
objects functors from C to D, morphisms natural transformations between such
functors, and composition as given above. DC is called the functor category
of C and D. As an Exercise, given categories C and D, verify that DC is indeed a
category. For example, one thing to check is that β ◦α as deﬁned above is indeed
natural.
An isomorphism in a functor category is referred to as a natural isomor-
phism. If there is a natural isomorphism between the functors F and G, then
we say that F and G are naturally isomorphic.
Lemma 1. Let α: F →G: C →D be a natural transformation. Then α is a
natural isomorphism just in case each component αC is an isomorphism in D.
More precisely, if we are given a natural isomorphism α in DC with inverse β,
then each βC is an inverse for αC in D; and if given a natural transformation
α in DC for which each component αC has an inverse (say βC) in D, then the
βC are the components of a natural transformation β which is the inverse of α
in DC.
Proof. Direct calculations from the deﬁnitions, left as an Exercise.
3.4
Products
The notion of “product of two objects in a category” can be viewed as an abstrac-
tion of the idea of a cartesian product of two sets. The deﬁnition of a cartesian
product of sets is an “internal” one; we specify the elements of the product in
terms of the elements of the sets from which the product is composed. However
the cartesian product has a particular property, namely
(Property Φ) Given any two sets A and B, then there is a set P and functions
π: P →A, π′: P →B such that the following condition holds: given any functions
f: C →A, g: C →B with C any set, then there is a unique function h: C →P
making the diagram
C

f
g
R
A 
π
P
h
?
π′- B
commute. End of deﬁnition of (Property Φ).
Let us investigate an instance of (Property Φ) in the case of two given sets A
and B. Suppose that A
def
= {a, b} and B
def
= {c, d, e}. Let us take P to be A×B
def
=
{(x, y) | x ∈A, y ∈B} and the functions π and π′ to be coordinate projection
to A and B respectively, and see if (P, π, π′) makes the instance of (Property Φ)

Basic Category Theory for Models of Syntax
147
for the given A and B hold. Let C be any other set and f: C →A and g: C →B
be any two functions. Deﬁne the function h: C →P by z 
→(f(z), g(z)). We
leave the reader to verify that indeed f = π ◦h and g = π′ ◦h, and that h is the
only function for which these equations hold with the given f and g. Now deﬁne
P ′ def
= {1, 2, 3, 4, 5, 6} along with functions p: P ′ →A and q: P ′ →B where
p(1),
p(2),
p(3) = a
q(1),
q(4) = c
p(4),
p(5),
p(6) = b
q(2),
q(5) = d
q(3),
q(6) = e
In fact (P ′, p, q) also makes the instance of (Property Φ) for the given A and B
hold true. To see this, one can check by enumerating six cases that there is a
unique function h: C →P ′ for which f = p ◦h and g = q ◦h (for example, if
x ∈C and f(x) = a and g(x) = d then we must have h(x) = 2, and this is one
case).
Now notice that there is a bijection between P (the cartesian product
{(a, c), (a, d), (a, e), (b, c), (b, d), (b, e)}
of A and B) and P ′. In fact any choices for the set P can be shown to be bijective.
It is very often useful to determine sets up to bijection rather than worry about
their elements or “internal make up,” so we might consider taking (Property Φ)
as a deﬁnition of cartesian product of two sets and think of the P and P ′ in the
example above as two implementations of the notion of cartesian product of the
sets A and B. Of course (Property Φ) only makes sense when talking about the
collection of sets and functions; we can give a deﬁnition of cartesian product for
an arbitrary category which is exactly (Property Φ) for the “category” of sets
and functions.
A binary product of objects A and B in a category C is speciﬁed by
• an object A × B of C, together with
• two projection
morphisms πA: A × B →A and πB: A × B →B,
for which given any object C and morphisms f: C →A, g: C →B, there is a
unique morphism ⟨f, g⟩: C →A×B for which πA ◦⟨f, g⟩= f and πB ◦⟨f, g⟩= g.
We refer simply to a binary product A×B instead of (A×B, πA, πB), without
explicit mention of the projection morphisms. The data for a binary product is
more readily understood as a commutative diagram, where we have written ∃!
to mean “there exists a unique”:
C

f
g
R
A 
πA
A × B
∃! ⟨f, g⟩
?
πB
- B
Given a binary product A × B and morphisms f: C →A and g: C →B, the
unique morphism ⟨f, g⟩: C →A × B (making the above diagram commute) is

148
R.L. Crole
called the mediating morphism for f and g. We sometimes refer to a property
which involves the “existence of a unique morphism” leading to a structure which
is determined up to isomorphism as a universal property. We also call ⟨f, g⟩
the pair of f and g. We say that the category C has binary products
if
there is a product in C of any two objects A and B, and that C has speciﬁed
binary products if there is a given canonical choice of binary product for each
pair of objects. For example, in Set we can specify binary products by setting
A × B
def
= { (a, b)
|
a ∈A, b ∈B } with projections given by the usual
set-theoretic projection functions. Talking of speciﬁed binary products is a
reasonable thing to do: given A and B, any binary product of A and B will be
isomorphic to the speciﬁed A × B.
Lemma 2. A binary product of A and B in a category C is unique up to iso-
morphism if it exists.
Proof. Suppose that (P, pA, pB) and (P ′, p′
A, p′
B) are two candidates for the bi-
nary product. Then we have ⟨pA, pB⟩: P →P ′ by applying the deﬁning prop-
erty of (P ′, p′
A, p′
B) to the morphisms pA: P →A, pB: P →B, and further
⟨p′
A, p′
B⟩: P ′ →P exists from a similar argument. So we have diagrams of the
form
P

pA
pB
R
A 
p′
A
P ′
⟨pA, pB⟩
?
p′
B
- B
P ′

p′
A
p′
B
R
A 
pA
P
⟨p′
A, p′
B⟩
?
pB
- B
But then f
def
= ⟨p′
A, p′
B⟩◦⟨pA, pB⟩: P →P and one can check that pA ◦f = pA
and that pB ◦f = pB, that is f is a mediating morphism for the binary product
(P, pA, pB); we can picture this as the following commutative diagram:
P

pA
pB
R
A 
pA
P
f
?
pB
- B
But it is trivial that idP is also such a mediating morphism, and so uniqueness
implies f = idP . Similarly one proves that ⟨pA, pB⟩◦⟨p′
A, p′
B⟩= idP ′, to deduce
P ∼= P ′ witnessed by the morphisms ⟨pA, pB⟩and ⟨p′
A, p′
B⟩.
Here are some examples

Basic Category Theory for Models of Syntax
149
1. The category Preset has binary products. Given objects A
def
= (X, ≤X) and
B
def
= (Y, ≤Y ), the product object A × B is given by (X × Y, ≤X×Y ) where
X × Y is cartesian product, and (x, y) ≤X×Y (x′, y′) just in case x ≤X x′
and y ≤Y y′. It is an Exercise to check the details.
2. The category Part has binary products. Given objects A and B, the binary
product object is deﬁned by
(A × B) ∪(A × { ∗A }) ∪(B × { ∗B })
where × is cartesian product, and ∗A and ∗B are distinct elements not in A
nor in B. The project πA is undeﬁned on B × { ∗B } and πB is undeﬁned on
A × { ∗A }. Of course πA(a, ∗A) = a for all a ∈A, and πB(b, ∗B) = b for all
b ∈B.
3. The category F has binary products. The product of n and m is written
n×m and is given by n∗m, that is, the set { 0, . . . , (n∗m)−1 }. We leave it
as an Exercise to formulate possible deﬁnitions of projection functions. Hint:
Think about the general illustration of products at the start of this section.
4. Consider a preorder (X, ≤) which has all binary meets x ∧y for x, y ∈X as
a category. It is an Exercise to verify that binary meets are binary products.
It will be useful to have a little additional notation which will be put to use
later on.
We can deﬁne the ternary product A × B × C for which there are three
projections, and any mediating morphism can be written ⟨f, g, h⟩for suitable f,
g and h. It is an Exercise to make all the deﬁnitions and details precise.
Let C be a category with ﬁnite products and take morphisms f: A →B and
f ′: A′ →B′. We write
f × f ′: A × A′ →B × B′
for the morphism ⟨f ◦π, f ′ ◦π′⟩where π: A × A′ →A and π′: A × A′ →A′. The
uniqueness condition (universal property) of mediating morphisms means that
in general one has
idA × idA′ = idA×A′
and
(g × g′) ◦(f × f ′) = g ◦f × g′ ◦f ′,
where g: B →C and g′: B′ →C′. Thus in fact we have a functor
×: C × C −→C
where C × C is a product of categories. Note that we sometimes write the image
of (A, A) or (f, f) under × as A2 or f 2.
1. Verify the equalities (uniqueness conditions) given above.
2. Let C be a category with ﬁnite products and let
l: X →A
f: A →B
g: A →C
h: B →D
k: C →E
be morphisms of C. Show that
(h × k) ◦⟨f, g⟩= ⟨h ◦f, k ◦g⟩
⟨f, g⟩◦l = ⟨f ◦l, g ◦l⟩

150
R.L. Crole
3.5
Coproducts
A coproduct is a dual notion of product. A binary coproduct of objects A
and B in a category C is speciﬁed by
• an object A + B of C, together with
• two insertion morphisms ιA: A →A + B and ιB: B →A + B,
such that for each pair of morphisms f: A →C, g: B →C there exists a unique
morphism [f, g]: A + B →C for which [f, g] ◦ιA = f and [f, g] ◦ιB = g. We can
picture this deﬁnition through the following commutative diagram:
A
ιA - A + B ιB
B
f
R

g
C
[f, g]
?
1. In the category Set, the binary coproduct of sets A and B is given by their
disjoint union together with the obvious insertion functions. We can deﬁne
the disjoint union A+B of A and B as the union (A×{1})∪(B ×{2}) with
the insertion functions
ιA : A →A + B ←B : ιB
where ιA is deﬁned by a 
→(a, 1) for all a ∈A, and ιB is deﬁned analogously.
Given functions f: A →C and g: B →C, then [f, g]: A + B →C is deﬁned
by
[f, g](ξ)
def
= case ξ of
ιA(ξA) = (ξA, 1) 
→f(ξA)
ιB(ξB) = (ξB, 2) 
→f(ξB)
We sometimes say that [f, g] is deﬁned by case analysis.
2. The category Preset has binary coproducts. Given objects A
def
= (X, ≤X)
and B
def
= (Y, ≤Y ), the product object A + B is given by (X + Y, ≤X+Y )
where X + Y is disjoint union, and ξ ≤X+Y ξ′ just in case ξ = (x, 1) and
ξ′ = (x′, 1) for some x, x′ such that x ≤X x′, or ξ = (y, 2) and ξ′ = (y′, 2)
for some y, y′ such that y ≤Y y′. It is an Exercise to check the details.
3. In F the coproduct object of n and m is n + m where we interpret + as
addition on N. It is an Exercise to work out choices of coproduct insertions.
What might we take as the canonical projections?
4. When does a preordered set have binary coproducts?
We end this section with notation that will be used later on.
One can deﬁne the ternary coproduct of three objects, which will have
three insertions, and mediating morphisms [f, g, h]. Fill in the details as an Ex-
ercise. Of course we can generalize to an n-ary coproduct, and call the mediating
morphism a cotupling.

Basic Category Theory for Models of Syntax
151
Let C be a category with ﬁnite coproducts and take morphisms f: A →B
and f ′: A′ →B′. We write
f + f ′: A + A′ →B + B′
for the morphism [ιB ◦f, ιB′ ◦f ′] where ιB: B →B + B′ and ιB′: B′ →B + B′.
The uniqueness condition of mediating morphisms means that one has
idA + idA′ = idA+A′
and
(g + g′) ◦(f + f ′) = g ◦f + g′ ◦f ′,
where g: B →C and g′: B′ →C′. So we also have a functor
+: C × C −→C
In a category with binary coproducts, then for any morphisms f, g, h, k, l with
certain source and target, the equalities
l ◦[f, g] = [l ◦f, l ◦g]
[f, g] ◦(h + k) = (f ◦h) + (g ◦k)
always hold due to the universal property of (binary) coproducts. What are the
sources and targets? Prove the equalities.
3.6
Algebras
Let F be an endofunctor on C, that is a functor F: C →C. An algebra for
the functor F is speciﬁed by a pair (A, σA)
where A is an object of C and
σA: FA →A is a morphism. An initial F-algebra (I, σI) is an algebra for which
given any other (A, σA), there is a unique morphism f: I →A such that
FI
σI - I
FA
Ff
?
σA
- A
f
?
commutes. As an Exercise, show that if σI: FI →I is initial, then so too is
σI′: FI′ →I′ where I′ is any object isomorphic to I, and σI′ is a morphism for
you to deﬁne.
One reason for deﬁning initial algebras is that certain datatypes can be mod-
eled as instances. Here is the rough idea. Each constructor of a datatype can
be thought of as a coproduct insertion3. Each constructor is applied to a ﬁnite
number (eg 2) of expressions, constituting a tuple of expressions. This can be
thought of as a product (eg binary). The datatype
Exp ::= V V | S Exp | A Exp Exp
3 Such insertions must be injective. There are categories in which insertions are not
injective! In all of the examples in these notes, however, insertions will be injective.

152
R.L. Crole
corresponds to (is modeled by) an object V + E + (E × E). The recursiveness
of the datatype declaration is modeled by requiring
E ∼= V + E + (E × E)
†
In these notes we shall see how to solve an equation such as † in the category
Set. In fact if we deﬁne a functor Σ: Set →Set by Σξ
def
= ˜V + ξ + (ξ × ξ), then it
will turn out that the solution we would construct using the methods below is an
initial algebra (σE, E). We now give a few examples, and illustrate the solution
method.
3.7
The Functor 1 + (−): Set −→Set
We write 1: Set →Set
for the functor which maps any function f: A →B to
id {∗}: { ∗} →{ ∗} where { ∗} is a one element set. Note that we will often also
write 1 for such a set. The functor maps f: A →B to id1 + f: 1 + A →1 + B.
The initial algebra is N up to isomorphism. We show how to construct an
initial algebra—the method will be applied in later sections, in adapted form, to
produce models of datatypes which represent syntax (see Section 2).
We set S0
def
= ∅and Sr+1
def
= 1 + Sr. Note that there is a coproduct inser-
tion ιSr: Sr →Sr+1. Note also that there is an inclusion4 function (morphism)
ir: Sr →Sr+1 where i0
def
= ∅: S0 →S1, and ir+1
def
= id1 + ir. The diﬀerence is an
elementary, but subtle, point! For example, we have i1, ιS1: S1 →S2 = 1+S1 for
which ιS1(∗, 1) = ((∗, 1), 2), where (∗, 1) ∈S1 = 1 + ∅. But i1 = id1 + i0 and so
i1(∗, 1) = [ι1 ◦id1, ιS1 ◦i0](∗, 1) = ι1 ◦id1(∗) = (∗, 1)
We also write i′
r: Sr →T where T
def
= ∪rSr, and claim that T is the object
part of an initial algebra for 1+(−). Note that as σT : 1+T →T, σT must be the
copair of two morphisms. We set σT
def
= [k, k′] where k: 1 →T and k′: T →T,
with k and k′ deﬁned as follows. Note there is a function
1
ι1
- 1 + ∅= S1
i′
1 - T
and we set k
def
= i′
1 ◦ι1. Note there are also functions
Sr
ιSr
- 1 + Sr = Sr+1
i′
r+1 - T
whose composition we call k′
r. It is an Exercise to check that k′
r+1 ◦ir = k′
r
by induction on r. Hence we can legitimately deﬁne the function k′: T →T by
setting k′(ξ)
def
= k′
r(ξ) for any r such that ξ ∈Sr.
4 That is, Sr ⊂Sr+1 for all r ≥0.

Basic Category Theory for Models of Syntax
153
We have to verify that σT : 1 + T →T is an initial algebra, namely, there is
exactly one commutative diagram
1 + T
σT - T
1 + A
id1 + f
?
f
- A
f
?
for any such given f. We deﬁne a family of functions f r: Sr →A by setting
f 0
def
= ∅: S0 →A, and recursively f r+1
def
= [f ◦ι1, f ◦ιA ◦f r]. It is an Exercise
to check that f r+1 ◦ir = f r. Hence we can legitimately deﬁne f: T →A by
f(ξ)
def
= f r(ξ) for any r where ξ ∈Sr.
To check that the diagram commutes, we have to prove that
f ◦[k, k′] = f ◦(id1 + f)
By the universal property of coproducts, this is equivalent to showing
[f ◦k, f ◦k′] = [f ◦ι1, f ◦ιA ◦f]
which we can do by checking that the respective components are equal. We give
details for f ◦k′ = f ◦ιA ◦f. Take any element ξ ∈T. Then we have
f(k′(ξ)) = f(ιSr(ξ))
= f r+1(ιSr(ξ))
= [f ◦ι1, f ◦ιA ◦f r](ιSr(ξ))
= f(ιA(f r(ξ)))
= f(ιA(f(ξ)))
The ﬁrst equality is by deﬁnition of k′ and k′
r; the second by deﬁnition of f; the
third by deﬁnition of f r+1.
A ﬁnal Exercise is to check that T ∼= N.
3.8
The Functor A + (−): Set −→Set
Let A be a set, + denote coproduct. Then the functor A + (−) has an initial
algebra (A × N, σA×N) where σA×N: A + (A × N) →A × N is deﬁned by
σA×N(ξ)
def
= case ξ of
ιA(a) 
→(a, 0)
ιA×N(a, n) 
→(a, n + 1)
where ιA and ιA×N are the left and right coproduct insertions, and n ≥0.

154
R.L. Crole
Then given any function f: A + S →S, we can deﬁne f where
A + (A × N) σA×N
- A × N
A + S
idA + f
?
f
- S
f
?
by setting
f(a, 0)
def
= f(ιA(a))
f(a, n + 1)
def
= f(ιA×N(f(a, n)))
It is an Exercise to verify that this does yield an initial algebra. It is also an
exercise to construct an initial algebra using the methodology of the previous
section, and to show that the two algebras are isomorphic.
3.9
The Functor 1 + (A × −): Set →Set
For k ≥1 we deﬁne the set Ak to be the collection of functions { 1, . . . , k } →A.
We identify A with A1; an element a ∈A is essentially a function 1 →A. If
a ∈A and l ∈Ak, then we deﬁne al ∈Ak+1 by al(1)
def
= a and al(r)
def
= l(r −1)
for r ≥2.
The functor 1 + (A × −) has an initial algebra (L, σL), where we set L
def
=
{ nil } ∪(
1≤k<ω Ak), and σL: 1 + (A × L) →L is deﬁned by
σL(ι1(∗))
def
= nil
σL(ιA×L(a, nil))
def
= a
σL(ιA×L(a, l))
def
= al
4
Models of Syntax
Recall that in Section 2 we deﬁned the syntax terms of an object level program-
ming language by making use of recursive datatypes. Note the phrase “making
use of”. Recall (pages 135 and 152) that we would like to have an ideal situation
(IS) in which
– we could write down a recursive datatype, whose elements are precisely the
terms which we are interested in; and
– we have a mathematical model of our syntax which is given as a solution to
a recursive equation derived directly from the datatype.

Basic Category Theory for Models of Syntax
155
Unfortunately, this is not so easily done when we are considering syntax involving
binding. It can be done (as is well known, and illustrated in these notes) for
simple syntax terms which involve algebraic constructors, such as the example
in Section 2.1. We can come close to (IS) for binding syntax, but we can’t achieve
it exactly using the traditional techniques described here.
In Section 2.1 we wrote down a datatype for expressions e, and then restricted
attention to expressions for which Γ n ⊢db e, that is, the variables occurring in
e must come from v0, . . . , vn−1. This does not conform to standard practice.
We would expect to deal with expressions speciﬁed using metavariables x which
would denote any of the actual variables vi. We could of course change the
judgement Γ n ⊢db e to ∆⊢db e (as in Section 2.3 and recall the exercise at the
end of Section 2) where ∆is any ﬁnite environment of metavariables x0, . . . , xn−1
and any xj denotes an actual variable. In this case, the expressions e such that
∆⊢db e correspond exactly with the expressions given by the datatype, and
moreover are the terms of the object syntax. Further, we have a mathematical
model described in the ﬁrst half of Section 4.1, and we manage to achieve (IS).
The reason for describing the judgements Γ n ⊢db e in this simple setting,
is that they illustrate a methodology which we must apply if we are to get
near to (IS) when dealing with binding syntax. In order to “avoid” the extra
infrastructure of α-equivalence, and deﬁne the terms of the λ-calculus induc-
tively (although not exactly as the elements of a datatype), we can restrict to
environments Γ n so that we can choose unique, explicit binding variables in
abstractions. Further, the use of environments Γ n with variables chosen sys-
tematically from a speciﬁed order will be used crucially when formulating the
mathematical models in presheaf categories. Here is the rough idea of how we
use presheaves over F to model expressions. A presheaf associates to every n the
set of expressions whose (free) variables appear in Γ n. And given any morphism
ρ: n →n′ in F, the presheaf associates to ρ a function which, roughly, maps any
expression e to
e{vρ(0), . . . , vρ(n−1)/v0, . . . , vn−1}
Thus ρ renames the (free) variables in e.
In this section we give some mathematical models of each system of syntax
from Section 2. In each case, we
Step 1. deﬁne an abstract endofunctor (over F
def
= SetF—see below) which
bears similarities to the datatype in question;
Step 2. construct an initial algebra T for the endofunctor;
Step 3. show that the datatype and system of syntax gives rise to a functor
Exp: F →Set, that is a presheaf in F;
Step 4. complete the picture by showing that T ∼= Exp so that T forms an
abstract mathematical model of the syntax.
In order to consider categorical models of syntax, we shall require some no-
tation and facts concerning a particular functor category. Recall the category F
deﬁned on page 142. The functor category which will be of primary concern is
SetF. We will write F for it.

156
R.L. Crole
A typical object F: F →Set is an example5 of a presheaf over F. A simple
example is given by the powerset functor P, deﬁned on page 144, restricted to
sets of the form n. Another (trivial) example is the empty presheaf ∅which
maps any ρ: n →m to the empty function with empty source and target.
The category F
def
= SetF has binary products. If F and F ′ are objects in F,
the product object F × F ′: F →Set is deﬁned on objects n in F by
(F × F ′)n
def
= (Fn) × (F ′n).
Now let ρ: n →n′ be a morphism in F. Hence (F × F ′)ρ should be a morphism
with source and target
(Fn) × (F ′n) −→(Fn′) × (F ′n′)
In fact we deﬁne (F × F ′)ρ
def
= (Fρ) × (F ′ρ) where we make use of the functor
×: Set × Set −→Set. The projection πF : F × F ′ →F is deﬁned by giving
components (πF )n
def
= πF n where n is an object of F, with πF ′ deﬁned similarly.
Check the details as an Exercise.
The category also has binary coproducts. Let ξ be any object or morphism
of F. We deﬁne F + F ′ by setting (F + F ′)ξ
def
= (Fξ) + (F ′ξ). The insertion
morphism (natural transformation) ιF : F + F ′ →F has components (ιF )n
def
=
ιF n: (Fn) + (F ′n) →Fn. ι′
F is deﬁned analogously. We leave it as an Exercise
to verify that F + F ′ is indeed a functor, that the insertions are natural, and
that the deﬁnitions above do give rise to binary coproducts.
Now let F and F ′ be two such objects (presheaves). Suppose that for any n
in F, F ′n ⊂Fn, and that the diagram
F ′n
⊂
Fn
F ′n′
F ′ρ
?
⊂
Fn′
Fρ
?
commutes for any ρ: n →n′. This gives rise to a natural transformation, which
we denote by i: F ′ →F.
We also need a functor δ: F →F. Suppose that F is an object in F. Then
δ F is deﬁned by assigning to any morphism ρ: n →n′ in F the function
(δ F)ρ
def
= F(ρ + id1): F(n + 1) −→F(n′ + 1)
If α: F →F ′ in F, then the components of δ α are given by (δ α)n
def
= αn+1. It
is an Exercise to verify all the details.
Lemma 3. Suppose that (Sr | r ≥0) is a family of presheaves in F, with
ir: Sr →Sr+1 for each r. Then there is a union presheaf T in F, such that
i′
r: Sr →T. We sometimes write ∪rSr for T.
5 In general, we call SetC the category of presheaves over C.

Basic Category Theory for Models of Syntax
157
Proof. Let ρ: n →n′ be any morphism in F. Then we deﬁne Tn
def
= 
r Srn, and
the function Tρ: Tn →Tn′ is deﬁned by (Tρ)(ξ)
def
= (Srρ)(ξ) where ξ ∈Tn,
and r is any index for which ξ ∈Sr(n). It is an Exercise to verify that we have
deﬁned a functor. Why is it well-deﬁned? Hint: prove that
(∀r, r′ ≥0)(r′ ≥r =⇒Sr →Sr′)
Lemma 4. Let (φr: Sr →A | r ≥0) be a family of natural transformations in
F with the Sr as in Lemma 3, and such that φr+1 ◦ir = φr. Then there is a
unique natural transformation φ: T →A, such that φ ◦i′
r = φr.
Proof. It is an Exercise to prove the lemma. The proof requires a simple calcula-
tion using the deﬁnitions. Hint: Note that there are functions φn: Tn →An where
we set φn(ξ)
def
= (φr)n(ξ) for ξ ∈Srn. The conditions of the lemma (trivially)
imply the existence and uniqueness of the φn, which are natural in n.
4.1
A Model of Syntax with Distinguished Variables
and without Binding
Step 1. We deﬁne a functor ΣVar
which “corresponds” to the signature of
Section 2.1. First, we deﬁne the functor Var : F →Set. Let ρ: m →n in F. Then
we set Var m
def
= { v0, . . . , vm−1 } and Var ρ(vi)
def
= vρi. It is trivial that Var is
a functor. Recall that SetF has ﬁnite products and coproducts, and moreover
that the operations + and × can be regarded as functors. Thus we can deﬁne a
functor ΣVar : SetF →SetF by setting ΣVar ξ
def
= Var + ξ + ξ2 where ξ is either
an object or a morphism.
Step 2. We now show that the functor ΣVar has an initial algebra, which we
denote by σT: ΣVar T →T. We deﬁne T as the union of a family of presheaves
(Sr | r ≥0) which satisfy the conditions of Lemma 3. We set S0
def
= ∅which is
the empty presheaf, and then set
Sr+1
def
= ΣVar Sr = Var + Sr + S2
r
We now check that the conditions of Lemma 3 hold, that is, ir: Sr →Sr+1 for
all r ≥0. We use induction over r. It is immediate that i0: S0 →S1 from the
deﬁnition of S0. Now suppose that for any r, ir: Sr →Sr+1. We are required to
show that ir+1: Sr+1 →Sr+2, that is, for any n in F,
Var n + Srn + (Srn)2
⊂
Var n + Sr+1n + (Sr+1n)2
Var n′ + Srn′ + (Srn′)2
Var ρ + Srρ + (Srρ)2
?
⊂Var n′ + Sr+1n′ + (Sr+1n′)2
Var ρ + Sr+1ρ + (Sr+1ρ)2
?

158
R.L. Crole
It follows from the induction hypothesis, Srn ⊂Sr+1n, and the deﬁnition of +
and × in Set, that we have subsets as indicated in the diagram. As an Exercise
make sure that you understand this—you need to examine the deﬁnitions of ×
and +. In fact the top inclusion is the component at n of the natural transfor-
mation ΣVar ir = idVar + ir + i2
r . Thus we have ir+1 = ΣVar ir. It is an Exercise
to check that the diagram commutes. Thus we can deﬁne T
def
= 
r Sr in F.
Next we consider the structure map σT. This natural transformation in F
has source and target Var + T + T 2 →T and so it is given by σT
def
= [κ, κ′, κ′′],
the cotupling of (insertion) natural transformations
κ : Var →T
κ′ : T →T
κ′′ : T 2 →T
For the ﬁrst morphism, note that Var ∼= S1, so that κ: Var ∼= S1 →T. It is a
simple Exercise to check that you understand the deﬁnition of κ; do not forget
that S1 = Var + ∅+ ∅2, and so S1n = Var n × { 1 }. We deﬁne κ′ by specifying
the family of morphisms
κ′
r: Sr
ιSr
- Var + Sr + S2
r = Sr+1 →T
and appealing to Lemma 4. Note that κ′
r is natural as it is the composition of
natural transformations. We must also check that κ′
r+1 ◦ir = κ′
r. To do this, we
have to check that the following diagram commutes
Sr+1
ιSr+1- Sr+2
→- T
Sr
ir
6
ιSr
- Sr+1
ir+1
6
→
- T

The right hand square commutes trivially. The left commutes by applying the
fact (deduced above) that ir+1 = idVar + ir + i2
r . We can also deﬁne κ′′ by
applying Lemma 4, but the deﬁnition requires a little care. Write S′
r
def
= S2
r.
Consider the family of morphisms
κ′′
r: S′
r = S2
r
ιS2
r
- Var + Sr + S2
r = Sr+1 →T
We can check that the κ′′
r satisfy the conditions of Lemma 4, and so they deﬁne
a morphism κ′′: U →T where U
def
= ∪rS′
r. But note that (why!?)
Un = ∪rS′
rn = ∪r(Srn)2 = (∪rSrn)2 = (Tn)2 = T 2n
and one can also check that Uρ = T 2ρ. Hence U = T 2, and we have our deﬁnition
of κ′′. Thus σT is deﬁned in F, as this category has ternary coproducts.

Basic Category Theory for Models of Syntax
159
We verify that σT: ΣVar T →T is an initial algebra. Consider
Var + T + T 2 σT - T
(∗)
Var + A + A2
Var + α + α2
?
α - A
α
?
To deﬁne α: T →A we specify a family of natural transformations αr: Sr →A
and appeal to Lemma 4. Note that α0: ∅→A and thus we deﬁne α0 to be the
natural transformation with components the empty functions ∅: ∅→An for
each n in F. Note that
αr+1: Sr+1 = Var + Sr + S2
r →A
and hence we can recursively deﬁne αr+1
def
= [α ◦ιVar , α ◦ιA ◦αr, α ◦ιA2 ◦α2
r]. It
follows very simply, by induction, that the αr are natural; if αr is natural, then
so too is αr+1, it being the cotupling of compositions of natural transformations.
It is an Exercise to verify by induction that the conditions of Lemma 4 hold,
that is, αr+1 ◦ir = αr for all r ≥0.
Using the universal property of ﬁnite coproducts, proving that the diagram
(∗) commutes is equivalent to proving
[α ◦κ, α ◦κ′, α ◦κ′] = [α ◦ιVar , α ◦ιA ◦α, α ◦ιA2 ◦α2]
which in turn is equivalent to proving that the respective components of the
cotuples are equal. We prove that α ◦κ′ = α ◦ιA ◦α: T →A. This amounts to
proving that αn ◦κ′
n = αn ◦ιAn ◦αn: Tn →An in Set for any n in F. Suppose
that ξ is an arbitrary element of Tn, where ξ ∈Srn. Then we have
αn(κ′
n(ξ)) = αn(ιSrn(ξ))
= (αr+1)n(ιSrn(ξ))
= [αn ◦ιVar n, αn ◦ιAn ◦(αr)n, αn ◦ι(An)2 ◦(αr)2
n](ιSrn(ξ))
= αn(ιAn((αr)n(ξ)))
= αn(ιAn(αn(ξ)))
Each step follows by applying an appropriate function deﬁnition.
Step 3. Suppose that ρ: n →n′ is any function. We deﬁne
Expdb n
def
= { e | Γ n ⊢db e }
For any expression e there is another expression denoted by (Expdb ρ)e which is,
informally, the expression e in which any occurrence of the variable vi is replaced
by vρ(i). We can deﬁne formally the expression (Expdb ρ)e by recursion over e,
by setting

160
R.L. Crole
– (Expdb ρ)(V vi)
def
= V ρi
– (Expdb ρ)(S e)
def
= S (Expdb ρ)e
– (Expdb ρ)(A e e′)
def
= A (Expdb ρ)e (Expdb ρ)e′
Further, one can show that if e ∈Expdb n, then (Expdb ρ)e ∈Expdb n′. Thus
we have a function Expdb ρ: Expdb n →Expdb n′ for any ρ: n →n′. It is an
Exercise to verify that these deﬁnitions yield a functor Expdb : F →Set. Further,
note that there are natural transformations (Exercise) S: Expdb
→Expdb
and
A: Expdb
2 →Expdb whose obvious deﬁnitions are omitted.
Step 4. We have constructed an initial algebra σT: ΣVar T →T in the category
F. We now show that the presheaf algebra T is isomorphic to the presheaf Expdb .
To do this we need natural transformations φ: T →Expdb and ψ: Expdb →T,
such that for any n in F, the functions φn and ψn give rise to a bijection between
Tn and Expdb n.
To specify φ: T →Expdb
we deﬁne a family of natural transformations
φr: Sr →Expdb , and appeal to Lemma 4.
– φ0: S0 = ∅→Expdb
has the empty function as components (φ0)n: ∅→
Expdb n
– Recursively we deﬁne
φr+1
def
= [V, S ◦φr, A ◦φ2
r]: Sr+1 = Var + Sr + S2
r →Expdb
Note that φ0 is obviously natural, and that φr+1 is natural if φr is, because F
has ternary coproducts. It is an Exercise to verify the conditions of the lemma.
To specify ψ: Expdb →T, for any n in F we deﬁne functions ψn: Expdb n →
Tn as follows. First note that Srn ⊂Tn for any r by deﬁnition of T. Then we
deﬁne
– ψn(V vi)
def
= (vi, 1) ∈S1n
– ψn(S e)
def
= ιSrn(ψn(e)) where r ≥1 is the height of the deduction of S e
– ψn(A e e′)
def
= ι(Srn)2((ψn(e), ψn(e′))) where r ≥1 is the height of the deduc-
tion of A e e′.
We next check that for any n in F,
Tn
φn-
∼=

ψn
Expdb n
We need the lemma
Lemma 5. For any r ≥0, and ξ ∈Sr+1n, the expression (φr+1)n(ξ) has a
deduction of height r.
Proof. By induction on r.

Basic Category Theory for Models of Syntax
161
Suppose that ξ ∈Srn ⊂Tn for some r. Then by deﬁnition, ψn(φn(ξ)) =
ψn((φr)n(ξ)). We show by induction that for all r ≥0, if ξ is any element of
Srn and n any object of F, then ψn((φr)n(ξ)) = ξ. For r = 0 the assertion
is vacuously true, as S0n is always empty. We assume the result holds for any
r ≥0. Let ξ ∈Sr+1n = Var n + Srn + Srn2. Then we have
ψn((φr+1)n(ξ)) = ψn([Vn, Sn ◦(φr)n, An ◦(φr)2
n](ξ))
We can complete the proof by case analysis. The situation r = 0 requires a little
care, but we leave it as an Exercise. We just consider the case when ξ = ιSrn(ξ′)
for some ξ′ ∈Srn (which implies that r ≥1). We have
ψn((φr+1)n(ξ)) = ψn((Sn ◦(φr)n)(ξ′))
(1)
= ψn(S (φr)n(ξ′))
(2)
= ιSrn(ψn((φr)n(ξ′)))
(3)
= ιSrn(ξ′)
(4)
= ξ
(5)
where equation 3 follows from Lemma 5, and equation 4 by induction. It is an
Exercise to show that φn is a left inverse for ψn. By appeal to Lemma 1 we are
done, and step 4 is complete.
However, by way of illustration, we can check directly by hand that ψ is
natural, that is for any ρ: n →n′ the diagram
Expdb n ψn - Tn
Expdb n′
Expdb ρ
?
ψ′
n
- Tn′
Tρ
?
commutes. We must prove that for all Γ n ⊢db e, we have
Tρ(ψn(e)) = ψn′((Expdb ρ)(e))
We consider only the inductive case of S e:
Tρ(ψn(S e)) = Tρ(ιSrn(ψn(e)))
= Sr+1ρ(ιSrn(ψn(e)))
= (Var ρ + Srρ + (Srρ)2)((ιSrn(ψn(e))))
= ιSrn((Srρ)(ψn(e)))
= ιSrn((Tρ)(ψn(e)))
= ιSrn′((ψ′
n(Expdb ρ(e))))
= ψn′(S (Expdb ρ)(e))
= ψn′(Expdb ρ(S e))
It is an Exercise to check this calculation; the sixth equality follows by induction.

162
R.L. Crole
4.2
A Model of Syntax with Distinguished Variables
and with Binding
Step 1. We deﬁne a functor which “corresponds” to the signature of Section 2.2.
This is ΣVar : F →F where ΣVar ξ
def
= Var + δ ξ + ξ2 and ξ is either an object
or a morphism. The functor δ: F →F was deﬁned on page 156.
Step 2. We can show that the functor ΣVar has an initial algebra, which we
denote by σT: ΣVar T →T, by adapting the methods given in Section 4.1.
In fact there is very little change in the details, so we just sketch the general
approach and leave the technicalities as an Exercise.
We deﬁne a family of presheaves (Sr | r ≥0), such that we may apply
Lemma 3. We set S0
def
= ∅which is the empty presheaf, and set Sr+1
def
= ΣVar Sr.
We must check that the conditions of Lemma 3 hold, that is, ir: Sr →Sr+1 for
all r ≥0. This can be done by induction over r, and is left as an exercise. We
can then deﬁne T
def
= 
r Sr in F.
Next we consider the structure map σT. This natural transformation in F
has source and target Var +δ T +T 2 →T and so it is given by σT
def
= [κ, κ′, κ′′],
the cotupling of (insertion) natural transformations
κ : Var →T
κ′ : δ T →T
κ′′ : T 2 →T
The deﬁnitions of κ and κ′′ are the same as in Section 4.1. Note that (δ T)n
def
=
T(n + 1) = 
r Sr(n + 1) = 
r(δ Sr)n = (
r δ Sr)n. In fact we can easily check
that δ T = ∪rδ Sr, as similar equalities hold if we replace n by any ρ. Hence we
can apply an instance of Lemma 4 to give a deﬁnition of κ′ by specifying the
family of morphisms κ′
r with the following deﬁnition
κ′
r: δ Sr
ιSr
- Var + δ Sr + S2
r = Sr+1 →T
Note that we must check to see that δ Sr →δ Sr+1 for all r. Use induction to
verify this as an Exercise; note that for any F ′ →F in F, we have δ F ′ →δ F.
We must verify that σT: ΣVar T →T is an initial algebra. Consider
Var + δ T + T 2 σT - T
(∗)
Var + δ A + A2
Var + δ α + α2
?
α
- A
α
?
To deﬁne α: T →A we specify a family of natural transformations αr: Sr →A
and appeal to Lemma 4. We deﬁne α0 to be the natural transformation with
components the empty functions ∅: ∅→An for each n in F, and

Basic Category Theory for Models of Syntax
163
αr+1
def
= [α ◦ιVar , α ◦ιA ◦δ αr, α ◦ιA2 ◦α2
r]: Sr+1 = Var + Sr + S2
r →A
The veriﬁcation that (∗) commutes is technically identical to the analogous sit-
uation in Section 4.1, and the details are left as an Exercise.
Step 3. Suppose that ρ: n →n′. We write Expdb n for the set { e | Γ n ⊢db e }
of expressions deduced using the rules in Figure 2. Let ρ{n′/n}: n + 1 →n′ + 1
be the function
ρ{n′/n}(j)
def
=

j
if
0 ≤j ≤n −1
n′
if
j = n
Consider the following (syntactic) deﬁnitions
– (Expdb ρ)(V vi)
def
= V vρi
– (Expdb ρ)(L vn e)
def
= L vn′ (Expdb ρ{n′/n})(e) and
– (Expdb ρ)(E e e′)
def
= E ((Expdb ρ)e) ((Expdb ρ)e′)
One can prove by rule induction that if Γ n ⊢db e and ρ: n →n′, then Γ n′ ⊢db
(Expdb ρ)e. In fact we have a functor Expdb
in F and the details are an Ex-
ercise. Note also that there are natural transformations L: δ Expdb →Exp and
E: Exp2 →Exp, provided certain assumptions are made about the category F!!
The latter’s deﬁnition is the obvious one. For the former, the components are
functions Ln: Expdb (n + 1) →Expdb n deﬁned by e 
→L vn e. Naturality is the
requirement that for any ρ: n →n′ in F, the diagram below commutes
(δ Expdb )n = Expdb (n + 1)
Ln- Expdb n
(δ Expdb )n′ = Expdb (n′ + 1)
(δ Expdb )ρ = Expdb (ρ + id1)
?
Ln′
- Expdb n′
Expdb ρ
?
Note that at the element e, this requires that
L vn′ (Expdb ρ{n′/n})e = L vn′ ((Expdb (ρ + id1))e)
and by considering when e is a variable, we conclude that this equality holds if
and only if
ρ{n′/n} = ρ + id1
which is true only if in F the coproduct insertion ι1: 1 →m + 1 maps ∗to m,
and ιm: m →m + 1 maps i to ρi for any i ∈m.

164
R.L. Crole
Step 4. We now show that the presheaf algebra T is isomorphic to the presheaf
Expdb . We have to show that there are natural transformations φ: T →Expdb
and ψ: Expdb →T, such that for any n in F, the functions φn and ψn give rise
to a bijection between Tn and Expdb n.
To specify φ: T →Expdb
we deﬁne a family of natural transformations
φr: Sr →Expdb , and appeal to Lemma 4.
– φ0: S0 = ∅→Expdb has as components the empty function, and
– recursively we deﬁne
φr+1
def
= [V, L ◦δ φr, E ◦φ2
r]: Sr+1 = Var + δ Sr + S2
r →Expdb
To specify ψ: Expdb →T, for any n in F we deﬁne functions ψn: Expdb n →
Tn as follows. First note that Srn ⊂Tn for any r by deﬁnition of T. Then we
deﬁne
– ψn(V vi)
def
= (vi, 1) ∈S1n.
– ψn(L vn e)
def
= ιSr(n+1)(ψn+1(e)) where r ≥0 is the height of the deduction
of L vn e.
– ψn(E e e′)
def
= ι(Srn)2((ψn(e), ψn(e′))) where r ≥0 is the height of the deduc-
tion of E e e′.
We check also that for any n in F,
Tn
φn-
∼=

ψn
Expdb n
Suppose that ξ ∈Srn ⊂Tn. Then by deﬁnition, ψn(φn(ξ)) = ψn((φr)n(ξ)).
We show by induction that for all r ≥0, if ξ is any element in level r and n
any object of F, then ψn((φr)n(ξ)) = ξ. For r = 0 the assertion is vacuously
true, as S0n is always empty. We assume the result holds for any r ≥0. Let
ξ ∈Sr+1n = Var n + Sr(n + 1) + Srn2. Then we have
ψn((φr+1)n(ξ)) = ψn([Vn, Ln ◦(φr)n+1, En ◦(φr)2
n](ξ))
We can complete the proof by analyzing the cases which arise depending on
which component ξ lives in. We just consider the case when ξ = ιSr(n+1)(ξ′) for
some ξ′ ∈Sr(n + 1). We have
ψn((φr+1)n(ξ)) = ψn((Ln ◦(φr)n+1)(ξ′))
(6)
= ψn(L vn (φr)n+1(ξ′))
(7)
= ιSr(n+1)n(ψn+1((φr)n+1(ξ′)))
(8)
= ιSr(n+1)n(ξ′)
(9)
= ξ
(10)
(11)

Basic Category Theory for Models of Syntax
165
where equation 8 follows from Lemma 5, and equation 9 by induction. It is an
Exercise to show that φn is a left inverse for ψn, and we are done appealing to
Lemma 1.
By way of illustration, we give a sample of the direct calculation that each
ψn is natural. We prove that for all Γ n ⊢db e, we have
Tρ(ψn(e)) = ψn′((Expdb ρ)(e))
We consider the case of L vn e; checking the details is an Exercise.
Tρ(ψn(L vn e)) = Tρ(ιSr(n+1)(ψn+1(e)))
= Sr+1ρ(ιSr(n+1)(ψn+1(e)))
= ((Var ρ + (δ Sr)(ρ) + (Srρ)2)ρ)(ιSr(n+1)(ψn+1(e)))
= ((Var ρ + Sr(ρ + id1) + (Srρ)2)ρ)(ιSr(n+1)(ψn+1(e)))
= ιSr(n′+1)((Sr(ρ + id1))(ψn+1(e)))
= ιSr(n′+1)((T(ρ + id1))(ψn+1(e)))
= ιSr(n′+1)((ψn′+1(Expdb (ρ + id1)(e)))
= ιSr(n′+1)(ψn′+1((Expdb ρ{n′/n})e))))
= ψn′(L vn′ (Expdb ρ{n′/n})(e))
= ψn′((Expdb ρ)(L vn e))
4.3
A Model of Syntax with Arbitrary Variables and Binding
We deﬁne a functor which “corresponds” to the signature of Section 2.3. The
functor ΣVar : F →F is deﬁned by setting ΣVar ξ
def
= Var + δ ξ + ξ2. As you
see, it is identical to the functor given at the start of Section 4.2, and thus has
an initial algebra σT: ΣVar T →T.
We show in this section that we can deﬁne a functor Expab
in F which
captures the essence of the inductive system of expressions given in Section 2.3
and is such that Expab ∼= T. We could prove this by proceeding (directly) as we
did in Section 4, undertaking the steps 2 to 4 of page 155. However, it is in fact
easier, and more instructive, to ﬁrst deﬁne Expab , step 3, and then prove that
Expab ∼= Expdb . Given previous results, this gives us steps 2 and 4.
We will need two lemmas which yield admissible rules (see Appendix). The
rules cannot be derived.
Lemma 6. Suppose that ∆⊢ab e, and that ∆′ is also an environment. Then
∆′ ⊢ab e{∆′/∆}.
Proof. We prove by rule induction
(∀∆⊢ab e)
(∀∆′) (∆′ ⊢ab e{∆′/∆})
We prove property closure for the rule introducing abstractions L x e. Suppose
that ∆⊢ab L x e. Then ∆, x ⊢ab e. Pick any ∆′. We try to prove that
∆′ ⊢ab (L x e){∆′/∆}

166
R.L. Crole
We consider only the case when x ∈∆′ at position p, and y
def
= elp(∆) ∈fv(e).
We must then prove
∆′ ⊢ab L x′ e{∆′, x′/∆, x}
∗
which is well-deﬁned as x ̸∈∆and x′ ̸∈∆′. By induction, we have ∆′, x′ ⊢ab
e{∆′, x′/∆, x}. Hence ∗follows.
Lemma 7. If ∆⊢ab e, and ∆is a sublist of an environment ∆1, then ∆1 ⊢ab e.
Proof. Rule Induction. Exercise.
Back to the task at hand. First we must deﬁne Expab . For n in F we set
Expab n
def
= { [e]α | Γ n ⊢ab e }
Now let ρ: n →n′. We deﬁne
(Expab ρ)([e]α)
def
= [e{vρ0, . . . , vρ(n−1)/v0, . . . , vn−1}]α
To check if this is a good deﬁnition, we need to show that if Γ n ⊢ab e then
Γ n′ ⊢ab e{vρ0, . . . , vρ(n−1)/v0, . . . , vn−1}
This follows from Lemma 6 and Lemma 7. We must also check that if there is
any e′ with e ∼α e′, then
e{vρ0, . . . , vρ(n−1)/v0, . . . , vn−1} ∼α e′{vρ0, . . . , vρ(n−1)/v0, . . . , vn−1}
This is proved by rule induction for ∼α, a tedious Exercise.
We now show that φ: Expab ∼= Expdb : ψ. The components of ψ are functions
ψn: Expdb n →Expab n given by ψn(e)
def
= [e]α. We consider the naturality of ψ at
a morphism ρ: n →n′, computed at an element ξ of Expdb n. We show naturality
for the case ξ = L vn e.
(Expab ρ) ◦ψn(ξ) = (Expab ρ)[L vn e]α
= [(L vn e){vρ0, . . . , vρ(n−1)/v0, . . . , vn−1}]α
def
= 2
Let us consider the case when renaming takes place. Suppose that there is a j
for which ρ(j) = n and vj ∈fv(e). Then6
(L vn e){vρ(0), . . . , vρ(n−1)/v0, . . . , vn−1} =
L vw e{vρ(0), . . . vρ(n−1), vw/v0, . . . , vn−1, vn}
where w is 1 plus the maximum of the indices occurring freely in e and the indices
ρ(0), . . . , ρ(n−1). Thus ρ(i) < w for all 0 ≤i ≤n−1. But the free variables in e
6 There is no deletion.

Basic Category Theory for Models of Syntax
167
must lie in v0, . . . , vn (why?) and moreover n = ρ(j) occurs in ρ(0), . . . , ρ(n−1).
Finally note that ρ(i) < n′, and so we must have w ≤n′. If w < n′, then
vn′ is not free in e{vρ(0), . . . vρ(n−1), vw/v0, . . . , vn−1, vn}. Otherwise (of course)
w = n′. Either way (why!?),
L vw e{vρ(0), . . . vρ(n−1), vw/v0, . . . , vn−1, vn}
∼α L vn′ e{vρ(0), . . . vρ(n−1), vn′/v0, . . . , vn−1, vn}
and so
2 = [L vn′ e{vρ0, . . . , vρ(n−1), vn′/v0, . . . , vn−1, vn}]α
= [L vn′ (Expdb ρ{n′/n})e]α
= ψn′ ◦(Expdb ρ)(ξ)
Next we deﬁne the functions φn: Expab n →Expdb n by setting φn([e]α)
def
=
Rn(e) where
– Rm(V x)
def
= V x
– Rm(L x e)
def
= L vm Rm+1(e{vm/x})
– Rm(E e e′)
def
= E Rm(e) Rm(e′)
To be well-deﬁned, we require Rm(e) = Rm(e′) for all e ∼α e′. We can prove this
by induction over ∼α. The only tricky case concerns the axiom for re-naming,
L x e ∼α L x′ e{x′/x} where x′ ̸∈fv(e). We have
Rm(L x′ e{x′/x}) = L vm Rm+1(e{x′/x}{vm/x′})
= L vm Rm+1(e{vm/x})
= Rm(L x e)
with the second equality holding as x′ ̸∈fv(e). Let e ∈Expdb n. We will have
φn(ψn(e)) = e provided that Rn(e) = e. We can prove this by rule induction,
showing
(∀Γ n ⊢db e)(Rn(e) = e)
The details are easy and left as an Exercise. Let e ∈Expab n. It remains to prove
that e = ψn(φn(e)). This will hold provided that Rn(e) ∼α e. We prove
(∀∆⊢ab e)
(∀Γ n) (∆= Γ n =⇒Rn(e) ∼α e)
We show property closure for the rule introducing abstractions. Suppose that
Γ n ⊢ab L x e. We must show that Rn(L x e) ∼α L x e. Now Γ n, x ⊢ab e and
so by a careful use of Lemma 6 we get Γ n+1 ⊢ab e{vn/x}. Hence by induction
Rn+1(e{vn/x}) ∼α e{vn/x}, and so
Rn(L x e)
def
= L vn Rn+1(e{vn/x}) ∼α L vn e{vn/x}
If x = vn we are done. If not, noting that x ̸∈Γ n by assumption, vn ̸∈fv(e).
Hence L vn e{vn/x} ∼α L x e.

168
R.L. Crole
4.4
A Model of Syntax without Variables but with Binding
Again, we show how some syntax, this time the de Bruijn expressions of Sec-
tion 2.4, can be rendered as a functor. We give only bare details, and leave most
of the working to the reader. We do assume that readers are already familiar
with the de Bruijn notation.
For any n in F we deﬁne Expib n
def
= { e | n ⊢ib e }. We deﬁne for ρ: n →n′
the function Expib ρ by recursion. Consider the following (syntactic) deﬁnitions
– (Expib ρ)(V i)
def
= V ρi
– (Expib ρ)(λ e)
def
= λ (Expib ρ∗)(e) and
– (Expib ρ)($ e e′)
def
= $ ((Expib ρ)e) ((Expib ρ)e′)
where ρ∗: n+1 →n′+1 and ρ∗(0)
def
= 0 and ρ∗(i+1)
def
= ρ(i)+1 for 0 ≤i ≤n−1.
One can prove by induction that if n ⊢ib e then n′ ⊢ib (Expib ρ)e, and thus Expib ρ
is well-deﬁned.
One can prove that Expib
∼= T by adapting the methods of Section 4.2,
establishing a natural isomorphism φ: T ∼= Expib : ψ. Such an isomorphism exists
only for a speciﬁc choice of coproducts in F.
To specify φ: T →Expib
we deﬁne a family of natural transformations
φr: Sr →Expib , and appeal to Lemma 4, as follows.
– φ0: S0 = ∅→Expib has as components the empty function, and
– recursively we deﬁne
φr+1
def
= [V, λ ◦δ φr, $ ◦φ2
r]: Sr+1 = Var + δ Sr + S2
r →Expib
where there are natural transformations λ: Expib
→Expib
and $: (Expib )2 →
Expib . Of course λn(e)
def
= λ e for any e in Expib n. This is natural only if the
coproduct insertion ι1: 1 →m + 1 maps ∗to 0, and ιm: m →m + 1 maps i + 1
to ρ(i) + 1 for any 0 ≤i ≤m −1.
We leave the deﬁnition of ψ, and all other calculations, as a long(ish) Exercise.
4.5
Where to Now?
There are a number of books which cover basic category theory. For a short and
gentle introduction, see [27]. For a longer ﬁrst text see [4]. Both of these books
are intended for computer scientists. The original and recommended general
reference for category theory is [19], which was written for mathematicians. A
very concise and fast paced introduction can be found in [10] which also covers
the theory of allegories (which, roughly, are to relations, what categories are to
functions). Again for the more advanced reader, try [31] which is an essential
read for anyone interested in categorical logic, and which has a lot of useful
background information. The Handbook of Logic in Computer Science has a
wealth of material which is related to categorical logic; there is a chapter [29]
on category theory. Equalities such as those that arise from universal properties

Basic Category Theory for Models of Syntax
169
can often be established using so-called calculational methods. For a general
introduction, and more references, see [2]. Finally we mention [32] which, apart
from being a very interesting introduction to category theory due to the many
and varied computing examples, has a short chapter devoted to distributive
categories.
The material in these notes has its origins in the paper [9]. You will ﬁnd
that these notes provide much of the detail which is omitted from the ﬁrst few
sections of this paper. In addition, you will ﬁnd an interesting abstract account
of substitution, and a detailed discussion of initial algebra semantics.
The material in these notes, and in [9], is perhaps rather closely allied to the
methodology of de Bruijn expressions. Indeed, in these notes, when we considered
λ-calculus, we either introduced α-equivalence, or we had a system of expressions
which forced a binding variable to be vn whenever the free variables of the
subexpression of an abstraction were in Γ n. We would like to be able to deﬁne a
datatype whose elements are the expressions of the λ-calculus, already identiﬁed
up to α-equivalence. This is achieved by Pitts and Gabbay [11], who undertake
a fundamental study of α-equivalence, and formulate a new set theory within
which this is possible. For further work see [28] and the references therein.
There is a wealth of literature on so called Higher Order Abstract Syntax.
This is another methodology for encoding syntax with binders. For an introduc-
tion, see [26, 25], although the ideas go back to Church. The paper [14] provides
links between Higher Order Abstract Syntax, and the presheaf models described
in these notes. For material on implementation, see [7, 8]. A more recent ap-
proach, which combines de Bruijn notation and ordinary λ-calculus in a hybrid
syntax, is described in [1].
If you are interested in direct implementations of α-equivalence, see [12, 13].
See [6] for the origins of de Bruijn notation.
The equation E ∼= V + E + (E × E) is a very simple example of a domain
equation. Such equations arise frequently in the study of the semantics of pro-
gramming languages. They do not always have solutions in Set. However, many
can be solved in other categories. See for example [30]. Readers should note that
the lemmas given in Section 3.3 can be presented in a more general, categorical
manner, which is described in loc. cit. In fact our so called union presheaf ∪rSr is
better described as a colimit (itself a generalization of coproduct) of the diagram
. . . Sr+1 −→Sr −→. . . −→S1 −→S0
. . . but this is another story.
Acknowledgements
Thanks to Simon Ambler for useful discussions. I should emphasize again that
these notes owe much to the ideas presented in Fiore, Plotkin and Turi’s paper
[9], and, more indirectly, to the recent work of Gabbay, Hofmann, and Pitts.

170
R.L. Crole
5
Appendix
5.1
Lists
We require the notion of a ﬁnite list. For the purposes of these notes, a (ﬁnite)
list over a set S is an element of the set
[ S ]
def
=

n<ω
Sn
where Sn is the set of n-tuples of S and S0 def
= { ϵ } where ϵ denotes the empty
list. We denote a typical non-empty element of [ S ] by [ s1, . . . , sn ] or sometimes
just s1, . . . , sn, and we write s ∈L to indicate that s occurs in the list (tuple)
L. We write len(l) for the length of any list l.
5.2
Abstract Syntax Trees
We adopt the following notation for ﬁnite trees: If T1, T2, T3 and so on to Tn is
a (ﬁnite) sequence of ﬁnite trees, then we write C T1 T2 T3 . . . Tn for the ﬁnite
tree which has the form
C
)
+

s
T1
T2
T3
. . .
Tn
Each Ti is itself of the form C′ T ′
1 T ′
2 T ′
3 . . . T ′
m. We call C a constructor and
say that C takes n arguments. Any constructor which takes 0 arguments is a
leaf node. We call C the root node of the tree. The roots of the trees Ti are
called the children of C. The constructors are labels for the nodes of the tree.
Each of the Ti above is a
subtree of the whole tree—in particular, any leaf
node is a subtree.
If we say that A is a constructor which takes two arguments, and S and V
constructors which takes one argument, then the tree in Figure 6 is denoted by
A (V v2) (A (V v2) S (V v8)). Note that in this (ﬁnite) tree, we regard each node
as a constructor. To do this, we can think of any vi as constructors which take
no arguments!!. These form the leaves of the tree. We call the root of the tree
the outermost constructor, and refer to trees of this kind as abstract syntax
trees. We often refer to an abstract syntax tree by its outermost constructor—the
tree above is an “A” expression.
5.3
Inductively Deﬁned Sets
In this section we introduce a method for deﬁning sets. Any such set will be
known as an inductively deﬁned set. Let us ﬁrst introduce some notation. We

Basic Category Theory for Models of Syntax
171
A

R
V
A

R
v2?
V
S
v3?
V
?
v8?
Fig. 6. An Abstract Syntax Tree
let U be any set. A rule R is a pair (H, c) where H ⊆U is any ﬁnite set, and
c ∈U is any element. Note that H might be ∅, in which case we say that R is a
base rule. Sometimes we refer to base rules as axioms. If H is non-empty we
say R is an inductive rule. In the case that H is non-empty we might write
H = { h1, . . . , hk } where 1 ≤k. We can write down a base rule R = (∅, c) using
the following notation
Base
(R)
c
and an inductive rule R = (H, c) = ({ h1, . . . , hk }, c) as
Inductive
h1
h2
. . .
hk (R)
c
Given a set U and a set R of rules based on U, a deduction is a ﬁnite tree
with nodes labelled by elements of U such that
• each leaf node label c arises as a base rule (∅, c) ∈R
• for any non-leaf node label c, if H is the set of children of c then (H, c) ∈R is
an inductive rule.
We then say that the set inductively deﬁned by R consists of those ele-
ments u ∈U which have a deduction with root node labelled by u.

172
R.L. Crole
Example 1.
1. Let U be the set { u1, u2, u3, u4, u5, u6 } and let R be the set of
rules
{ R1 = (∅, u1), R2 = (∅, u3), R3 = ({ u1, u3 }, u4), R4 = ({ u1, u3, u4 }, u5) }
Then a deduction for u5 is given by the tree
u5

R
u1
u3
?
u4

R
u1
u3
which is more normally written up-side down and in the following style
R1
u1
R2
u3
R1
u1
R2
u3
R3
u4
R4
u5
2. A set R of rules for deﬁning the set E ⊆N of even numbers is R = { R1, R2 }
where
(R1)
0
e
(R2)
e + 2
Note that rule R2 is, strictly speaking, a rule schema, that is e is acting as
a variable. There is a “rule” for each instantiation of e. A deduction of 6 is
given by
(R1)
0
(R2)
0 + 2
(R2)
2 + 2
(R2)
4 + 2
3. Let V be a set of propositional variables. The set of (ﬁrst order) propo-
sitions Prop is inductively deﬁned by the rules below. There are two dis-
tinguished (atomic) propositions true and false. Each proposition denotes
a ﬁnite tree. In fact true and false are constructors with zero arguments,
as is each p. The remaining logical connectives are constructors with two
arguments, and are written in a sugared (inﬁx) notation.
[v ∈V ]
v
false
true
φ
ψ
φ ∧ψ
φ
ψ
φ ∨ψ
φ
ψ
φ →ψ

Basic Category Theory for Models of Syntax
173
5.4
Rule Induction
In this section we see how inductive techniques of proof which the reader has
met before ﬁt into the framework of inductively deﬁned sets. We write φ(x) to
denote a proposition about x. For example, if φ(x)
def
= x ≥2, then φ(3) is true
and φ(0) is false. If φ(a) is true then we often say that φ(a) holds.
We present in ﬁg. 7 a useful principle called
Rule Induction. It will be
used throughout the remainder of these notes.
Let I be inductively deﬁned by a set of rules R. Suppose we wish to show that a
proposition φ(i) holds for all elements i ∈I, that is, we wish to prove
∀i ∈I.
φ(i) .
Then all we need to do is
– for every base rule b ∈R prove that φ(b) holds; and
– for every inductive rule h1...hk
c
∈R prove that whenever hi ∈I,
(φ(h1) ∧φ(h2) ∧. . . ∧φ(hk))
=⇒
φ(c)
We call the propositions φ(hj) inductive hypotheses. We refer to carrying out the
bulleted (•) tasks as “verifying property closure”.
Fig. 7. Rule Induction
The Principle of Mathematical Induction arises as a special case of Rule
Induction. We can regard the set N as inductively deﬁned by the rules
(zero)
0
n
(add1)
n + 1
Suppose we wish to show that φ(n) holds for all n ∈N, that is ∀n ∈N.φ(n).
According to Rule Induction, we need to verify
• property closure for zero, that is φ(0); and
• property closure for add1, that is for every natural number n, φ(n) implies
φ(n + 1), that is ∀n ∈N. (φ(n) =⇒φ(n + 1))
and this amounts to precisely what one needs to verify for Mathematical Induc-
tion.
1. Here is another example of abstract syntax trees deﬁned inductively. Let a
set of constructors be Z ∪{ +, −}. The integers will label leaf nodes, and +,
−will take two arguments written with an inﬁx notation. The set of abstract
syntax trees T inductively deﬁned by these constructors is given by
n
T1
T2
T1 + T2
T1
T2
T1 −T2

174
R.L. Crole
Note that the base rules correspond to leaf nodes. In the example tree
+

R
−
2

R
55
7
55 −7 is a subtree of (55 −7) + 2, as are the leaves 55, 7 and 2.
The principle of structural induction is deﬁned to be an instance of rule
induction when the inductive deﬁnition is of abstract syntax trees. Make
sure you understand that if T is an inductively deﬁned set of syntax trees,
to prove ∀T ∈T .φ(T) we have to prove:
– φ(L) for each leaf node L; and
– assuming φ(T1) and . . . and φ(Tn) prove φ(C(T1, . . . , Tn)) for each con-
structor C and all trees Ti ∈T .
These two points are precisely property closure for base and inductive rules.
Consider the proposition φ(T) given by L(T) = N(T) + 1 where L(T) is the
number of leaves in T, and N(T) is the number of +, −-nodes of T. We can
prove by structural induction
∀T ∈T .
L(T) = N(T) + 1
where the functions L, N: T →N are deﬁned recursively by
– L(n) = 1 and L(+(T1, T2)) = L(T1)+L(T2) and L(−(T1, T2)) = L(T1)+
L(T2)
– N(n) = 0 and N(+(T1, T2)) = N(T1) + N(T2) + 1 and N(−(T1, T2)) =
N(T1) + N(T2) + 1
This is left as an exercise.
Sometimes it is convenient to add a rule R to a set R, which does not alter
the resulting set I. We say that a rule
h1
. . .
hk R
c
is a derived rule of R if there is a deduction tree whose leaves are either the
conclusions of base rules or are instances of the hi, and the conclusion is c. The
rule R is called admissible if one can prove
(h1 ∈I) ∧. . . ∧(hk ∈I) =⇒(c ∈I)
Proposition 1. Let I be inductively deﬁned by R, and suppose that R is a
derived rule. Then the set I′ inductively deﬁned by R ∪{ R } is also I. Any
derived rule is admissible.
Proof. It is clear that I ⊂I′. It is an exercise in rule induction to prove that
I′ ⊂I. Verify property closure for each of the rules in R ∪{ R }, the property
φ(i)
def
= i ∈I. It is clear that derived rules are admissible.

Basic Category Theory for Models of Syntax
175
5.5
Recursively Deﬁned Functions
Let I be inductively deﬁned by a set of rules R, and A any set. A function
f: I →A can be deﬁned by
• specifying an element f(b) ∈A for every base rule b ∈R; and
• specifying f(c) ∈A in terms of f(h1) ∈A and f(h2) ∈A .... and f(hk) ∈A
for every inductive rule h1...,hk
c
∈R,
provided that each instance of a rule in R introduces a diﬀerent element of I—
why do we need this condition? When a function is deﬁned in this way, it is said
to be recursively deﬁned.
Example 2.
1. The factorial function F: N →N is usually deﬁned recursively.
We set
• F(0)
def
= 1 and
• ∀n ∈N.F(n + 1)
def
= (n + 1) ∗F(n).
Thus F(3) = (2 + 1) ∗F(2) = 3 ∗2 ∗F(1) = 3 ∗2 ∗1 ∗F(0) = 3 ∗2 ∗1 ∗1 = 6.
Are there are brackets missing from the previous calculation? If so, insert
them.
2. Consider the propositions deﬁned on page 172. Suppose that ψi and xi
are propositions and propositional variables for 1 ≤i ≤n. Then there
is a recursively deﬁned function Prop →Prop whose action is written
φ 
→φ{ψ1, . . . , ψn/x1, . . . , xn} which computes the simultaneous substitu-
tion of the φi for the xi where the xi are distinct. We set
• x{ψ1, . . . , ψn/x1, . . . , xn}
def
= ψj if x is xj;
• x{ψ1, . . . , ψn/x1, . . . , xn}
def
= x if x is none of the xi;
•
(φ ∧φ′){ψ1, . . . , ψn/x1, . . . , xn}
def
=
(φ{ψ1, . . . , ψn/x1, . . . , xn}) ∧(φ′{ψ1, . . . , ψn/x1, . . . , xn})
• The other clauses are similar.
References
1. S. J. Ambler and R. L. Crole and A. Momigliano. Combining Higher Order Ab-
stract Syntax with Tactical Theorem Proving and (Co)Induction. Accepted for the
15th International Conference on Theorem Proving in Higher Order Logics, 20-23
August, Virginia, U.S.A. Springer Verlag Lecture Notes in Computer Science 2410,
2002.
2. R. Backhouse and R. L. Crole and J. Gibbons. Algebraic and Coalgebraic Methods
in the Mathematics of Program Construction. Revised lectures from an Interna-
tional Summer School and Workshop, Oxford, UK, April 2000. Springer Verlag
Lecture Notes in Computer Science 2297, 2002.
3. M. Barr and C. Wells. Toposes, Triples and Theories. Springer-Verlag, 1985.
4. M. Barr and C. Wells.
Category Theory for Computing Science.
International
Series in Computer Science. Prentice Hall, 1990.

176
R.L. Crole
5. R. L. Crole. Categories for Types. Cambridge Mathematical Textbooks. Cambridge
University Press, 1993. xvii+335 pages, ISBN 0521450926HB, 0521457017PB.
6. N. de Bruijn. Lambda-calculus notation with nameless dummies: a tool for auto-
matic formula manipulation with application to the Church-Rosser theorem. Indag.
Math., 34(5):381–392, 1972.
7. J. Despeyroux, A. Felty, and A. Hirschowitz. Higher-order abstract syntax in Coq.
In M. Dezani-Ciancaglini and G. Plotkin, editors, Proceedings of the International
Conference on Typed Lambda Calculi and Applications, pages 124–138, Edinburgh,
Scotland, Apr. 1995. Springer-Verlag LNCS 902.
8. J. Despeyroux, F. Pfenning, and C. Sch¨urmann. Primitive recursion for higher-
order abstract syntax. In R. Hindley, editor, Proceedings of the Third International
Conference on Typed Lambda Calculus and Applications (TLCA’97), pages 147–
163, Nancy, France, Apr. 1997. Springer-Verlag LNCS.
9. M. Fiore and G. D. Plotkin and D. Turi. Abstract Syntax and Variable Binding. In
G. Longo, editor, Proceedings of the 14th Annual Symposium on Logic in Computer
Science (LICS’99), pages 193–202, Trento, Italy, 1999. IEEE Computer Society
Press.
10. P.J. Freyd and A. Scedrov. Categories, Allegories. Elsevier Science Publishers,
1990. Appears as Volume 39 of the North-Holland Mathematical Library.
11. M. Gabbay and A. Pitts. A new approach to abstract syntax involving binders. In
G. Longo, editor, Proceedings of the 14th Annual Symposium on Logic in Computer
Science (LICS’99), pages 214–224, Trento, Italy, 1999. IEEE Computer Society
Press.
12. A. Gordon. A mechanisation of name-carrying syntax up to alpha-conversion. In
J.J. Joyce and C.-J.H. Seger, editors, International Workshop on Higher Order
Logic Theorem Proving and its Applications, volume 780 of Lecture Notes in Com-
puter Science, pages 414–427, Vancouver, Canada, Aug. 1993. University of British
Columbia, Springer-Verlag, published 1994.
13. A. D. Gordon and T. Melham. Five axioms of alpha-conversion. In J. von Wright,
J. Grundy, and J. Harrison, editors, Proceedings of the 9th International Conference
on Theorem Proving in Higher Order Logics (TPHOLs’96), volume 1125 of Lecture
Notes in Computer Science, pages 173–190, Turku, Finland, August 1996. Springer-
Verlag.
14. M. Hofmann. Semantical analysis for higher-order abstract syntax. In G. Longo,
editor, Proceedings of the 14th Annual Symposium on Logic in Computer Science
(LICS’99), pages 204–213, Trento, Italy, July 1999. IEEE Computer Society Press.
15. F. Honsell, M. Miculan, and I. Scagnetto. An axiomatic approach to metareasoning
on systems in higher-order abstract syntax. In Proc. ICALP’01, number 2076 in
LNCS, pages 963–978. Springer-Verlag, 2001.
16. Bart Jacobs. Categorical Logic and Type Theory. Number 141 in Studies in Logic
and the Foundations of Mathematics. North Holland, Elsevier, 1999.
17. P.T. Johnstone. Topos Theory. Academic Press, 1977.
18. J. Lambek and P.J. Scott. Introduction to Higher Order Categorical Logic. Cam-
bridge Studies in Advanced Mathematics. Cambridge University Press, 1986.
19. S. Mac Lane. Categories for the Working Mathematician, volume 5 of Graduate
Texts in Mathematics. Springer-Verlag, 1971.
20. Saunders Mac Lane and Ieke Moerdijk. Topos theory. In M. Hazewinkel, editor,
Handbook of Algebra, Vol. 1, pages 501–528. North-Holland, Amsterdam, 1996.
21. Saunders Mac Lane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First
Introduction to Topos Theory. Universitext. Springer-Verlag, New York, 1992.

Basic Category Theory for Models of Syntax
177
22. R. McDowell and D. Miller.
Reasoning with higher-order abstract syntax in a
logical framework. ACM Transaction in Computational Logic, 2001. To appear.
23. J. McKinna and R. Pollack. Some Type Theory and Lambda Calculus Formalised.
To appear in Journal of Automated Reasoning, Special Issue on Formalised Math-
ematical Theories (F. Pfenning, Ed.),
24. C. McLarty.
Elementary Categories, Elementary Toposes, volume 21 of Oxford
Logic Guides. Oxford University Press, 1991.
25. F. Pfenning. Computation and deduction. Lecture notes, 277 pp. Revised 1994,
1996, to be published by Cambridge University Press.
26. F. Pfenning and C. Elliott. Higher-order abstract syntax. In Proceedings of the
ACM SIGPLAN ’88 Symposium on Language Design and Implementation, pages
199–208, Atlanta, Georgia, June 1988.
27. B.C. Pierce.
Basic Category Theory for Computer Scientists.
Foundations of
Computing Series. The MIT Press, 1991.
28. A. M. Pitts.
Nominal logic: A ﬁrst order theory of names and binding.
In
N. Kobayashi and B. C. Pierce, editors, Theoretical Aspects of Computer Software,
4th International Symposium, TACS 2001, Sendai, Japan, October 29-31, 2001,
Proceedings, volume 2215 of Lecture Notes in Computer Science, pages 219–242.
Springer-Verlag, Berlin, 2001.
29. A. Poign´e. Basic category theory. In Handbook of Logic in Computer Science,
Volume 1. Oxford University Press, 1992.
30. M.B. Smyth and G.D. Plotkin.
The Category Theoretic Solution of Recursive
Domain Equations. In SIAM Journal of Computing, 1982, volume 11(4), pages 761–
783.
31. Paul Taylor.
Practical Foundations of Mathematics.
Number 59 in Cambridge
Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 1999.
32. R. F. C. Walters. Categories and Computer Science. Number 28 in Cambridge
Computer Science Texts. Cambridge University Press, 1991.

R. Backhouse and J. Gibbons (Eds.): Generic Programming SS 2002, LNCS 2793, pp. 178–221, 2003. 
© Springer-Verlag Berlin Heidelberg 2003 
A Mathematical Semantics
for Architectural Connectors
J.L. Fiadeiro1, A. Lopes2, and M. Wermelinger3
1 Department of Computer Science, University of Leicester 
University Road, Leicester LE1 7RH, UK 
jose@fiadeiro.org
2 Department of Informatics, Faculty of Sciences, University of Lisbon 
Campo Grande, 1749-016 Lisboa, Portugal 
mal@di.fc.ul.pt
3 Dep. of Informatics, Faculty of Sciences and Technology, New University of Lisbon 
Quinta da Torre, 2829-516 Caparica, Portugal 
mw@di.fct.unl.pt
Abstract. A mathematical semantics is proposed for the notion of architec-
tural connector, in the style defined by Allen and Garlan, that builds on 
Goguen’s categorical approach to General Systems Theory and other algebraic 
approaches to specification, concurrency, and parallel program design.  This 
semantics is, essentially, ADL-independent, setting up criteria against which 
formalisms can be evaluated according to the support that they provide for ar-
chitectural design.  In particular, it clarifies the role that the separation between 
computation and coordination plays in supporting architecture-driven ap-
proaches to software construction and evolution.   It also leads to useful gener-
alisations of the notion of connector, namely through the use of multiple for-
malisms in the definition of the glue and the roles, and their instantiations with 
programs or system components that can be implemented in different languages 
or correspond to “real-world” components.
1 
Introduction 
Architectural connectors have emerged as a powerful tool for supporting the descrip-
tion of the overall organisation of systems in terms of components and their interac-
tions [6,27].  According to [1], an architectural connector (type) can be defined by a 
set of roles and a glue specification.  For instance, a typical client-server architecture 
can be captured by a connector type with two roles – client and server – which de-
scribe the expected behaviour of clients and servers, and a glue that describes how the 
activities of the roles are coordinated (e.g. asynchronous communication between the 
client and the server).  The roles of a connector type can be instantiated with specific 
components of the system under construction, which leads to an overall system struc-
ture consisting of components and connector instances establishing the interactions 
between the components. 
The similarities between architectural constructions as informally described above 
and parameterised programming [17] are rather striking and have been more recently 

A Mathematical Semantics for Architectural Connectors      179 
developed in [18].  The view of architectures that is captured by the principles and 
formalisms used in parameterised programming is reminiscent of Module Intercon-
nection Languages and Interface Definition Languages.  This perspective is some-
what different from the one followed in the work of Allen, Garlan and other research-
ers in Software Architectures.  Software Architectures in this more recent sense focus 
instead on the organisation of the behaviour of systems as compositions of compo-
nents ruled by protocols for communication and synchronisation.  As explained in 
[1], this kind of organisation is founded on interaction in the behavioural sense, 
which explains why process calculi are preferred to the functional flavour of equa-
tional specifications. 
In [10,12], we showed that the mathematical “technology” of parameterisation can 
also be used for the formalisation of architectural connectors in the interaction sense.  
More concretely, we used preliminary work on an algebraic approach to parallel 
program design [11], in the tradition of the categorical approach to General Systems 
Theory also developed by Goguen [16], in order to bring together architectural prin-
ciples and the categorical approach to system specification and design.  In this paper, 
we provide a more thorough account of the application of categorical principles to 
architectural design by bringing to bear the developments on an algebraic semantics 
for the coordination paradigm as expose in [13], techniques for associating mobility 
with architectural connectors [29], and the beginnings of a calculus for composing 
architectural connectors [30].
The mathematical framework that we propose for formalising architectural princi-
ples is not specific to any particular Architecture Description Language (ADL).  In 
fact, it will emerge from the examples that we shall provide that, contrarily to most 
other formalisations of Software Architecture concepts that we have seen, Category 
Theory is not another semantic domain for the formalisation of the description of 
components and connectors (like, say, the use of CSP in [1] or first-order logic in 
[26]).  Instead, it provides for the very semantics of “interconnection”, “configura-
tion”, “instantiation” and “composition”, i.e. the principles and design mechanisms 
that are related to the gross modularisation of complex systems.  Category Theory 
does this at a very abstract level because what it proposes is a toolbox that can be 
applied to whatever formalism is chosen for modelling the behaviour of systems as 
long as that formalism satisfies some structural properties.  It is precisely the struc-
tural properties that make a formalism suitable for supporting architectural design 
that we shall make our primary focus. 
Having this goal in mind, we start, in sections 2 and 3, by presenting a specific 
category corresponding to a specific program design language, and the way architec-
tural connectors can be formalised over that category.  This language – CommUnity – 
was developed precisely as a vehicle for illustrating the categorical approach to sys-
tem design in general, and the role of architectures in this process in particular.  Al-
though it borrows principles from other languages that can be found in the literature, 
it takes to an extreme the separation between the support that it provides for the com-
putations that are performed locally by components, and the mechanisms that are 
made available for coordinating the joint behaviour of components in a system.  
Hence, the purpose of introducing CommUnity at some length is to distil what we 
have found to be the minimal features that make up an architecture description lan-
guage.  In a companion paper [3], we show how these features have found their way 
into a full fledged object-oriented modelling language. 

180      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
We proceed in section 4 by abstracting from CommUnity the distilled categorical 
properties that make an architecture school.  We then show how the proposed 
mathematical framework leads to useful generalisations of the notion of architectural 
connector.  We show how to support the use of different formalisms for describing 
the roles and the glue of a connector, namely a declarative formalism for the roles 
and a procedural one for the glue.  We show how refinement mechanisms can interact 
with the application of architectural connectors as a means of supporting stepwise 
construction of systems.  By this we mean both the possibility of establishing an 
architecture at the earlier phases of design that can then be refined by replacing ab-
stract descriptions of the components involved by programs, and the support for a 
compositional evolution of the system through the addition, substitution or deletion 
of components or connectors.  Furthermore, we show how we can support heteroge-
neity of components as a means of supporting the reuse of legacy components and the 
description of systems that incorporate non-software components. 
Finally, in section 5, we show how to support several operations on connectors, 
giving the software architecture the ability to reuse connectors in the construction of 
the connectors that apply to a specific style.
The style of the paper is not purely “mathematical”.  It includes mathematical 
definitions of the main concepts, but we tried to make appeal more to the intuition 
than to rely on formality.  Familiarity with basic notions and constructions of Cate-
gory Theory is useful but there should be enough examples, and explanations around 
them, for readers acquainted with Software Architectures to make their way without 
much pain. 
2 
System Configuration in CommUnity 
To illustrate the categorical approach that we wish to put forward for formalising 
architectural connectors and architectural descriptions, we will use the program de-
sign language CommUnity.  CommUnity is a language similar to Unity [8] and Inter-
acting Processes [14] that was initially developed to show how programs fit into 
Goguen’s categorical approach to General Systems Theory [16]. Since then, the lan-
guage and the design framework have been extended to provide a formal platform for 
architectural design of open, reactive, reconfigurable systems. 
2.1 
Component Design 
One of the extensions that we have made to CommUnity since its original definition 
in [11] concerns the support for higher levels of design.  At such levels of design, the 
architecture of the system is given in terms of components that are not necessarily 
programs but abstractions of programs – called designs – that can be refined into 
programs in later stages of the development process.   Furthermore, some of these 
designs may account for components of the real-world with which the software com-
ponents will be interconnected.  Typically, such abstractions derive from require-
ments that have been specified in some logic or other mathematical models of the 
behaviour of real-world components.  In this paper, we shall not address the process 

A Mathematical Semantics for Architectural Connectors      181 
of deriving designs from specifications or other models.  We will concentrate on the 
mathematical support that can be given to the process of building an architecture of 
the system from given designs. 
The goal of supporting abstraction is not only to support a stepwise approach to 
software construction, but also an architectural design layer that is closer to the appli-
cation domain and, hence, can be used for driving the evolution of the system accord-
ing to the changes that occur in the domain.  An important part of this evolution may 
consist of changes in the nature of components, with real-world components being 
replaced or controlled by software components, or software components being repro-
grammed in another language, which again stresses the importance of supporting 
abstraction in architectural design. 
The support for abstraction in CommUnity is twofold.  On the one hand, designs 
account for what is usually called underspecification, i.e. they are structures that do 
not denote unique programs but collections of programs.  On the other hand, designs 
can be defined over a collection of data types that do not correspond necessarily to 
those that will be available in the final implementation platform.  Therefore, there are 
two refinement procedures that have to be accounted for in CommUnity.  On the one 
hand, the removal of underspecification from designs in order to define programs 
over the layer of abstraction defined by the data types that have been used.  On the 
other hand, the reification of the data types in order to bring programs into the target 
implementation environment.   
In order to address these two processes in reasonable depth, we would need more 
than one paper!  The choice of data types determines, essentially, the nature of the 
elementary computations that can be performed locally by the components, which are 
abstracted as operations on data elements.  Although such elementary computations 
also determine the granularity of the services that components can provide and, 
hence, the granularity of the interconnections that can be established at a given layer 
of abstraction, data refinement is more concerned with the computational aspects of 
systems than with coordination.  Because architectural design is more concerned with 
the coordination of system behaviour within a given layer of abstraction, we decided 
to give more emphasis to refinement of designs for a fixed choice of data types and 
omit any discussion on data refinement. 
Given this, we shall assume a collection of data types to be fixed.  In order to re-
main independent of any specific language for the definition of these data types, we 
take them in the form of a first-order algebraic specification.  That is to say, we as-
sume a data signature <S,Ω>, where S is a set (of sorts) and Ω is a S
*×S-indexed 
family of sets (of operations), to be given together with a collection Φ of first-order 
sentences specifying the functionality of the operations. 
A CommUnity component design over such a data type specification is of the form 
design P is
out
out(V)
in
  in(V) 
 
prv 
prv(V)
do
[]  
 
 
g[D(g)]: L(g), U(g) →  R(g) 
     
g sh(Γ)
 
 
 []
 
prv 
g[D(g)]: L(g), U(g) →  R(g) 
      
g prv(Γ)
where

182      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
• V is the set of channels.  Channels can be declared as input, output or private.
Input channels are used for reading data from the environment of the component.  
The component has no control on the values that are made available in such chan-
nels.  Moreover, reading a value from an input channel does not “consume” it: the 
value remains available until the environment decides to replace it.  Output and 
private channels are controlled locally by the component, i.e. the values that, at 
any given moment, are available on these channels cannot be modified by the en-
vironment.  Output channels allow the environment to read data produced by the 
component.  Private channels support internal activity that does not involve the 
environment in any way. We use loc(V) to denote the union prv(V)∪out(V), i.e. the 
set of local channels. Each channel v is typed with a sort sort(v)∈S.
• by Γ we denote  the set of action names.  The named actions can be declared either 
as private or shared (for simplicity, we only declare which actions are private). 
Private actions represent internal computations in the sense that their execution is 
uniquely under the control of the component.  Shared actions represent possible in-
teractions between the component and the environment, meaning that their execu-
tion is also under the control of the environment.  The significance of naming ac-
tions will become obvious below; the idea is to provide points of rendez-vous at 
which components can synchronise. 
• For each action name g, the following attributes are defined: 
- D(g) is a subset of loc(V) consisting of the local channels that can be effected 
by executions of the action named by g.  This is what is sometimes called the 
write frame of g.  For simplicity, we will omit the explicit reference to the write 
frame when R(g) is a conditional multiple assignment (see below), in which 
case D(g) can be inferred from the assignments.  Given a local channel v, we 
will also denote by D(v) the set of actions g such that v∈D(g).
- L(g) and U(g) are two conditions such that U(g)⊃L(g).  These conditions estab-
lish an interval in which the enabling condition of any guarded command that 
implements g must lie.  The condition L(g) is a lower bound for enabledness in 
the sense that it is implied by the enabling condition.  Therefore, its negation es-
tablishes a blocking condition.  On the other hand, U(g) is an upper bound in the 
sense that it implies the enabling condition, therefore establishing a progress
condition. Hence, the enabling condition is fully determined only if L(g) and 
U(g) are equivalent, in which case we write only one condition.   
- R(g) is a condition on V and D(g)’ where by D(g)’ we denote the set of primed 
local channels from the write frame of g. As usual, these primed channels ac-
count for references to the values that the channels take after the execution of 
the action.  These conditions are usually a conjunction of implications of the 
form pre ⊃ pos where pre does not involve primed channels. They correspond 
to pre/post-condition specifications in the sense of Hoare.  When R(g)  is such 
that the primed version of each local channel in the write frame of g is fully de-
termined, we obtain a conditional multiple assignment, in which case we use the 
notation that is normally found in programming languages.  When the write 
frame D(g) is empty,  R(g) is tautological, which we denote by skip.
Notice that CommUnity supports several mechanisms for underspecification – ac-
tions may be underspecified in the sense that their enabling conditions may not be 

A Mathematical Semantics for Architectural Connectors      183 
fully determined (subject to refinement by reducing the interval established by L and 
U) and their effects on the channels may also not be fully determined.   
When, for every g∈Γ, L(g) and U(g) coincide, and the relation R(g) defines a con-
ditional multiple assignment then the design is called a program.  Notice that a pro-
gram with a non-empty set of input channels is open in the sense that its execution is 
only meaningful in the context of a configuration in which these inputs have been 
instantiated with (local) channels of other components.  The notion of configuration, 
and the execution of an open program in a given configuration, will be discussed 
further below.  The behaviour of a closed program is as follows.  At each execution 
step, any of the actions whose enabling condition holds of the current state can be 
executed, in which case its assignments are performed atomically. Furthermore, pri-
vate actions that are infinitely often enabled are guaranteed to be selected infinitely 
often.  See [22] for a model-theoretic semantics of CommUnity. 
Designs can be parameterised by data elements (sorts and operations) indicated af-
ter the name of the component (see an example below).  These parameters are instan-
tiated at configuration time, i.e. when a specific component needs to be included in 
the configuration of the system being built, or as part of the reconfiguration of an 
existing system. 
As an example, consider the following parameterised design. 
design buffer [t:sort, bound:nat] is 
in   i:t 
out   o:t 
prv   rd: bool, b: list(t) 
do
  put: |b|<bound →  b:=b.i
 
[] prv next: |b|>0∧¬rd → o:=head(b)||b:=tail(b)||rd:=true
 
[]    get: rd → rd:=false   
 
This design is actually a (parameterised) program and the traditional notation of 
guarded commands was used accordingly.   Notice in particular that the reference to 
the write frame of the actions was omitted: it can be inferred from the multiple as-
signments that they perform.  This program models a buffer with a limited capacity 
and a FIFO discipline. It can store, through the action put, messages of sort t received 
from the environment through the input channel i, as long as there is space for them.  
The buffer can also discard stored messages, making them available to the environ-
ment through the output channel o and the action next.  Naturally, this activity is 
possible only when there are messages in store and the current message in o has al-
ready been read by the environment (which is modelled by the action get and the 
private channel rd).
In order to illustrate the ability of CommUnity to support higher-level component 
design, we present below the design of a typical sender of messages.  In this descrip-
tion, we are primarily concerned with the interaction between the sender and its envi-
ronment, ignoring details of internal computations such as the production of mes-
sages.
design sender[t:sort] is
out
o:t
 
prv 
rd: bool 
do
  prod[o,rd]: ¬rd,false→rd’
 
[]   send[rd]: rd,false→¬rd'

184      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
Notice that a sender cannot produce another message before the previous one has 
been processed.  After producing a message, a sender expects an acknowledgement 
(modelled through the execution of send) to produce a new message.
In order to leave unspecified when and how many messages a sender will send and 
in which situations it will produce a new message, the progress conditions of prod
and send are false (recall that the progress condition defines the upper bound for 
enabledness).  Furthermore, the discipline of production was also left completely 
unspecified: the action prod includes the channel o in its write frame but the design 
does not commit to any specific way of changing the value of this channel.  
From a mathematical point of view, (instantiated) CommUnity designs are struc-
tures defined as follows. 
A signature is a tuple <V,Γ,tv,ta,D> where 
• V is an S-indexed family of mutually disjoint finite sets, 
• Γ is a finite set, 
• tv: V→{out,in,prv} is a total function, 
• ta: Γ→{sh,prv} is a total function, 
• D: Γ→2
loc(V) is a total function. 
A design is a pair <θ,∆> where θ=<V,Γ,tv,ta,D> is a signature and ∆, the body of 
the design, is a tuple <R,L,U> where: 
• R assigns to every action g∈Γ, a proposition over V∪D(g)’ ,  
• L and U assign a proposition over V to every action g∈Γ.
The reader who is familiar with parallel program design languages or earlier ver-
sions of CommUnity will have probably noticed the absence of initialisation condi-
tions.  The reason they were not included in CommUnity designs is because they are 
part of the configuration language of CommUnity, not the parallel program design 
language.  That is to say, we take initialisation conditions as part of the mechanisms 
that relate to the building and management of configurations out of designs, not of the 
construction of designs themselves. 
2.2 
Configurations 
So far, we have presented the primitives for the design of individual components, 
which are another variation on guarded commands.  The features of these designs that 
are not shared with other parallel program design languages are those that concern 
design “in the large”, i.e. the ability to design large systems from simpler compo-
nents.  
The model of interaction between components in CommUnity is based on action 
synchronisation and the interconnection of input channels of a component with output 
channels of other components.  These are standard means of interconnecting software 
components.  What distinguishes CommUnity from other parallel program design 
languages is the fact that such interactions between components are required to be 
made explicit by providing the corresponding name bindings.  Indeed, parallel pro-
gram design languages normally leave such interactions implicit by relying on the use 
of the same names in different components.  In CommUnity, names are local to de-
signs.  This means that the use of the same name in different designs is treated as 

A Mathematical Semantics for Architectural Connectors      185 
being purely accidental, and, hence, expresses no relationship between the compo-
nents.  
In CommUnity, name bindings are established as relationships between the signa-
tures of the corresponding components, matching channels and actions of these com-
ponents.  These bindings are made explicit in configurations.  A configuration deter-
mines a diagram containing nodes labelled with the signatures of the components that 
are part of the configuration.  Name bindings are represented as additional nodes 
labelled with sets representing the actual interactions, and edges labelled with the 
projections that map each interaction to the corresponding  component signatures.
For instance, a configuration in which the messages from a sender component are 
sent (to a receiver) through a bounded buffer defines the following diagram:
  cable 
signature(sender) 
signature(buffer)
The node labelled cable is the representation of the set of bindings.  Because, as 
we have seen, channels and action names are typed and classified in different catego-
ries, not every pair of names is a valid binding.  To express the rules that determine 
valid bindings, it is convenient to structure cable as a signature itself.  Hence, in the 
case above, cable consists of an input channel to model the medium through which 
data is to be transmitted between the sender and the buffer, and a shared action for the 
two components to synchronise in order to transmit the data.  Because, as we have 
already mentioned, names in CommUnity are local, the identities of the shared input 
channel and the shared action in cable are not relevant: they are just placeholders for 
the projections to define the relevant bindings.  Hence, we normally do not bother to 
give them explicit names, and represent them through the symbol •.
The bindings themselves are established through the labels of the edges of the dia-
gram.  In the case above, the input channel of cable is mapped to the output channel o
of sender and to the input channel i of buffer. This establishes an i/o-interconnection 
between sender and buffer. On the other hand, the actions send of sender and put of 
buffer are mapped to the shared action of cable. This defines that sender and buffer
must synchronise each time either of them wants to perform the corresponding action.
The fact that the mappings on action names and on channels go in opposite directions 
will be discussed below. 
The arrows that we are using to define interconnections between components are 
also mathematical objects: they are examples of signature morphisms. 
A 
morphism 
σ:θ1→θ 2 
between 
signatures 
θ1=<V1,Γ1,tv1,ta1,D1> 
and 
θ2=<V2,Γ2,tv2,ta2,D2>  is a pair <σch,σac> where 
• σch: V1→V2 is a total function satisfying: 
- sort2(σch(v))=sort1(v)  for every v∈V1
- σch(o)∈out(V2)  for every o∈out(V1)
- σch(i)∈out(V2)∪in(V2)  for every i∈in(V1)
- σch(p)∈prv(V2)  for every p∈prv(V1)
o←•→i
send→•←put

186      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
• σac: Γ2→Γ1 is a partial mapping satisfying for every g∈Γ2 s.t. σac(g) is defined: 
- if g∈sh(Γ2) then σac(g)∈sh(Γ1)
- if g∈prv(Γ2) then σac(g)∈prv(Γ1)
- σch(D1(σac(g)))⊆D2(g)
- σac is total on D2(σch(v)) and σac(D2(σch(v)))⊆D1(v) for every v∈loc(V1)
Signature morphisms represent more than the projections that arise from name 
bindings as illustrated above.  A morphism σ from θ1 to θ2 is intended to support the 
identification of a way in which a component with signature θ1 is embedded in a 
larger system with signature θ2.  This justifies the various constructions and con-
straints in the definition.  
The function σch identifies for each channel of the component the corresponding 
channel of the system.  The partial mapping σac identifies the action of the component 
that is involved in each action of the system, if ever.  The fact that the two mappings 
go in opposite directions is justified as follows. Actions of the system constitute syn-
chronisation sets of actions of the components.  Because not every component is 
necessarily involved in every action of the system, the action mapping is partial.  On 
the other hand, because each action of the component may participate in more than 
one synchronisation set, but each synchronisation set cannot induce internal synchro-
nisations within the components, the relationship between the actions of the system 
and the actions of every component is functional from the former to the latter.   
Input/output communication within the system is not modelled in the same way as 
action synchronisation. Synchronisation sets reflect parallel composition whereas 
with i/o-interconnections we wish to unify communication channels of the compo-
nents.  This means that, in the system, channels should be identified rather than 
paired.  This is why mappings on channels and mappings on actions go in opposite 
directions.  We will see that, as a result, the mathematical semantics of configuration 
diagrams induces fibred products of actions (synchronisation sets) and amalgamated 
sums of channels (equivalence classes of connected channels).
The other constraints are concerned with typing.  Sorts of channels have to be pre-
served but, in terms of their classification, input channels of a component may be-
come output channels of the system.  This is because, in the absence of other con-
straints, the result of interconnecting an input channel of a component with an output 
channel of another component in the system is an output channel of the system.  
Mechanisms for internalising communication can be applied but they are not the 
default in a configuration.  The last two conditions on write frames implies that ac-
tions of the system in which a component is not involved cannot have local channels 
of the component in its write frame.  That is, change within a component is com-
pletely encapsulated in the structure of actions defined for the component.
The diagrams that we use for expressing configurations of interconnected compo-
nents are also mathematical objects.  Indeed, Category Theory goes a long way, as far 
as Mathematics in concerned, in providing a formal framework for configuration that 
is “graphical”.  Even so, the notation can be simplified and made more user-friendly 
by adopting some features that are typical of languages for configurable distributed 
systems like [25].  For instance, the interconnection defined before can be described 
as follows. 

A Mathematical Semantics for Architectural Connectors      187 
The interconnection, i.e. the name bindings, are still represented explicitly but, in-
stead of being depicted as a component, the cable is now represented, perhaps more 
intuitively, in terms of arcs that connect channels and actions directly.  We shall not 
provide a full definition of the graphical language that we defined for CommUnity 
because it is self-explanatory.  It is also easy to see that configurations in this notation 
are easily translated into categorical diagrams by transforming the interconnections 
into cables and morphisms. 
So far, we have explained how interconnections between components can be es-
tablished at the level of the signatures of their designs.  It remains to explain how the 
corresponding designs are interconnected, i.e. what is the semantics of the configura-
tion diagram once designs are taken into account.  For that purpose, we need to ex-
tend the notion of morphism from signatures to designs. 
A morphism σ:P1→P2 of designs P1= <θ1,∆1> and  P2= <θ2,∆2>, consists of a sig-
nature morphism σ:θ1→θ2  such that,  for every g∈Γ2 s.t. σac(g) is defined: 
1. Φ |– (R2(g)⊃σ(R1(σac(g))));
2. Φ |– (L2(g) ⊃σ(L1(σac(g))));
3. Φ |– (U2(g) ⊃σ(U1(σac(g))));
where Φ is the axiomatisation of the data type specification, |–  denotes validity in 
the first-order sense, and σ is the extension of σ to the language of expressions and 
conditions. Designs and their morphisms constitute a category c-DSGN. 
A morphism σ:P1→P2 identifies a way in which P1 is “augmented” to become P2
so that P2 can be considered as having been obtained from P1 through the superposi-
tion of additional behaviour, namely the interconnection of one or more components.  
The conditions on the actions require that the computations performed by the system 
reflect the interconnections established between its components.  Condition 1 reflects 
the fact that the effects of the actions of the components can only be preserved or 
made more deterministic in the system.  This is because the other components in the 
system cannot interfere with the transformations that the actions of a given compo-
nent make on its state, except possibly by removing some of the underspecification 
present in the component design. 
Conditions 2 and 3 allow the bounds that the component design specifies for the 
enabling of the action to be strengthened but not weakened. Strengthening of the 
lower bound reflects the fact that all the components that participate in the execution 
of a joint action have to give their permission for the action to occur. On the other 
hand, it is clear that progress for a joint action can only be guaranteed when all the 
designs of the components involved can locally guarantee so. 
The notion of morphism that we have just defined captures what in the literature 
on parallel program design is called “superposition” or “superimposition” [8,20].  See 
[11] for the categorical formalisation of different notions of superposition and their 
algebraic properties.
sender
o
send
buffer
i
put
get
prod
o

188      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
The semantics of configurations is given by a categorical construction: the colimit 
of the underlying diagrams.  Taking the colimit of a diagram collapses the configura-
tion into an object by internalising all the interconnections, thus delivering a design 
for the system as a whole.  Furthermore, the colimit provides a morphism σi from 
each component design Pi in the configuration into the new design (that of the sys-
tem).  Each such morphism is essential for identifying the corresponding component 
within the system because the construction of the new design typically requires that 
the features of the components be renamed in order to account for the interconnec-
tions.   
For instance, in the case of actions, the colimit represents every synchronisation 
set {g1,…,gn} of actions of the components, as defined through the interconnections, 
by a single action g1||…||gn whose occurrence captures the joint execution of the ac-
tions in the set.  The transformations performed by the joint action are specified by 
the conjunction of the specifications of the local effects of each of the synchronised 
actions, i.e. R(g1||…||gn)=σ1(R(g1))∧…∧σn(R(gn)) where the σi are the morphisms that 
connect the components to the system. The bounds on the guards of joint actions are 
also obtained through the conjunctions of the bounds specified by the components, 
i.e.L(g1||…||gn)=σ1(L(g1))∧…∧σn(L(gn)) and U(g1||…||gn)=σ1(U(g1))∧…∧σn(U(gn)).
At the level of the channels, instead of computing synchronisation sets, the colimit 
construction amalgamates the channels involved in each i/o-communication estab-
lished by the configuration.  Each such interaction is represented at the level of the 
system design as an output channel.  From a mathematical point of view, these chan-
nels represent the quotient sets of channels defined by the equivalence relation that 
results from the i/o-interconnections. 
Hence, colimits in CommUnity capture a generalised notion of parallel composi-
tion in which the designer makes explicit what interconnections are used between 
components.  We can see this operation as a generalisation of the notion of superim-
position as defined in [14]. 
An example of a more complex configuration is given below.  It models the inter-
connection between a user and a printer via a buffer. 
The user produces files that it stores in the private channel w.  It can then convert 
them either to postscript or pdf formats, after which it makes them available for print-
ing in the output channel p.
design user is
out
p:ps+pdf
 
prv 
s: 0..2, w: MSWord 
do
  work[w,s]: s=0,false → s’=1
 
[]   pr_ps: s=1,false → p:=ps(w)||s:=2
 
[]   pr_pdf: s=1,false → p:=pdf(w)||s:=2
 
[]   print: s=2 → s:=0 
 
The printer copies the files it downloads from the input channel rdoc into the pri-
vate channel pdoc, after which it prints them. 

A Mathematical Semantics for Architectural Connectors      189 
design printer is
in   rdoc:ps+pdf
 
prv 
busy: bool, pdoc: ps+pdf 
do
  rec: ¬busy → pdoc:=rdoc||busy:=true
 
[]
prv end_print: busy → busy:=false 
The configuration connects the user to the printer via a buffer as expected.  The 
user “prints” by placing the file in the buffer: this is achieved through the synchroni-
sation pair {print,put} and the i/o-interconnection {p,i}.  The printer downloads from 
the buffer the files that it prints: this is achieved through the synchronisation pair 
{get, rec}  and the i/o-interconnection {o,rdoc}.
The design of the system that results from the colimit of the configuration diagram 
contains two channels that account for the two i/o-interconnections {p,i} and 
{o,rdoc}, together with the private channels of the components.  At the level of its 
actions, it generates the following shared actions (synchronisation sets): 
• {print,put}, {get, rec} – these are required by the interconnections 
• {work}, {pr_ps}, {pr_pdf}, {work,get,rec}, {pr_ps,get,rec}, {pr_pdf,get,rec} – these 
reflect the concurrent executions that respect the interconnections. 
No other shared actions are possible because of the synchronisation requirements 
imposed on the components. 
Because actions of the system are synchronisation sets of actions of the compo-
nents, the evaluation of the guard of a chosen action can be performed in a distributed 
way by evaluating the guards of the component actions in the synchronisation set.  
The joint action will be executed iff all the local guards evaluate to true.  The execu-
tion of the multiple assignment associated with the joint action can also be performed 
in a distributed way by executing each of the local assignments.  What is important is 
that the atomicity of the execution must be guaranteed, i.e. the next system step 
should only start when all local executions have completed, and the 
i/o-communications should be implemented so that every local input channel is in-
stantiated with the correct value – that which holds of the local state before any exe-
cution starts (synchronicity).  Hence, the colimit of the configuration diagram should 
be seen as an abstraction of the actual distributed execution that is obtained by coor-
dinating the local executions according to the interconnections, rather than the pro-
gram that is going to be executed as a monolithic unit.   
The fact that the computational part, i.e. the one that is concerned with the execu-
tion of the actions on the state, can be separated from the coordination aspects is, 
therefore, an essential property for guaranteeing that the operational semantics is 
compositional on the structure of the system as given through its configuration dia-
gram.  We will return to this aspect later on and provide a formal characterisation of 
what we mean by separating computation from coordination. 
Not every diagram of designs reflects a meaningful configuration.  For instance, it 
does not make sense to interconnect components by connecting two output channels.  
Indeed, we cannot guarantee that every diagram admits a colimit, meaning that there 
are diagrams that have no “semantics” as configurations.  
The two following rules express the restrictions on the diagrams of designs that 
make them well-formed configurations. 
• An output channel of a component cannot be connected with output channels of 
the same or other components; 
• Private channels and private actions cannot be involved in the connections. 

190      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
It is important to notice that the second rule establishes the configuration semantics 
of private actions and channels. It supports the intuitive semantics we gave in section 
2.1, namely that private channels cannot be read by the environment and that the 
execution of shared actions is uniquely under the control of the component.   
Well-formed configurations are guaranteed to generate diagrams that admit colimits. 
In Category Theory, the colimit construction applies to diagrams in which all the 
nodes are labelled with objects of the same category and all the edges are labelled 
with morphisms of the same category as well.  However, in what concerns configura-
tions, we have expressed interconnections at the level of the signatures of the compo-
nent designs.  This is because we have explained interconnections in terms of syn-
chronisation of actions and i/o-communication, which are modelled at the level of 
signatures.  How can we bring into a single category all the elements that are used in 
a configuration to make sure that the mathematics work? 
In CommUnity, every signature θ defines a canonical design dsgn(θ): the one that 
is completely underspecified.  More precisely, to each action g of the signature we 
assign true to R(g), i.e. we make no commitments on the effects of the action on the 
channels, and true to L(g) and true to U(g), i.e. we make no commitments as to the 
bounds of the enabling condition in the sense that we do not restrict the designs to 
which the cable can be mapped.  It is trivial to check that, given a design P and a 
signature θ, every morphism σ:θ→sig(P) is also a morphism of designs dsgn(θ)→P.
Hence, we can replace every cable in a configuration diagram by the canonical design 
that it denotes, and take the colimit of the extended diagram as the semantics of the 
configuration.  Knowing this, and for simplicity, we will keep using signatures in 
diagrams. 
Although this solution satisfies our purposes in the sense that it provides the envis-
aged semantics for configurations, it also raises an interesting question.  Are we loos-
ing any expressive power by restricting interconnections to operate at the level of 
signatures?  Would we obtain a more expressive configuration language by allowing 
interconnections to be expressed directly as diagrams in the category of designs, i.e. 
using any kind of designs as cables?  After all, the application of Category Theory to 
General System Theory originally developed by Goguen does not distinguish be-
tween designs and their signatures… 
This question is concerned with the degree of separation that CommUnity provides 
between computation and coordination, an issue that is central to Software Architec-
tures.  More precisely, the question is about the mechanisms that in CommUnity are 
relevant for coordinating the behaviour of components.  If, indeed, extending con-
figuration diagrams to full designs does not increase the interconnections that are 
allowed in the language, then coordination does not require the features that are ab-
sent from signatures, i.e. the computational part of actions does not interfere with 
coordination.  If, on the contrary, we are able to show that we can establish more 
interconnections by using full designs as cables, then we will have proved that there 
is more to coordinating the interaction between components then synchronising ac-
tions and establishing i/o-communications. 
It turns out that signatures provide, indeed, all that is necessary for interconnecting 
designs.  To prove so, we have first to formulate the property at stake.  Because this 
is a discussion that can be held independently of the language that is being used, 
leading to a mathematical formalisation of coordination, we postpone the formulation 
of the property and its proof to section 3. 

A Mathematical Semantics for Architectural Connectors      191 
3 
Architectural Description in CommUnity 
3.1 
Architectural Connectors 
According to [1], an architectural connector (type) can be defined by a set of roles,
that can be instantiated with specific components of the system under construction, 
and a glue specification that describes how the activities of the role instances are to 
be coordinated.   Using the mechanisms that we have just described for configuration 
design in CommUnity, it is not difficult to come up with a formal notion of connector 
that has the same properties as those given in [1] for the language WRIGHT: 
• A connection consists of 
- two designs G and R, called the glue and the role of the connection, respec-
tively;
- a signature θ and two morphisms σ:dsgn(θ)→G,µ:dsgn(θ)→R connecting the 
glue and the role. 
• A connector is a finite set of connections with the same glue that, together, consti-
tute a well-formed configuration.
G
θ1
θi
θn
R1
  Ri
Rn
• The semantics of a connector is the colimit of the diagram formed by its connec-
tions. 
For instance, asynchronous communication through a bounded channel can be rep-
resented by a connector Async with two connections, as depicted below using the 
graphical notation that we have already introduced. 
The glue of Async is the bounded buffer with FIFO discipline presented before.  It 
prevents the sender from sending a new message when there is no space and prevents 
the receiver from reading a new message when there are no messages.   The two roles 
– sender and receiver – define the behaviour required of the components to which the 
connector can be applied.  For the sender, we require that no message be produced 
before the previous one has been processed.   Its design is the one given already in 
section 2.1. For the receiver, we simply require that it has an action that models the 
reception of a message.
design receiver [t:sort] is
in
i: t 
do rec: true,false→skip
sender[t]
o
send
buffer[t]
i
put
o
ge
receiver[t]
i
rec
prod

192      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
What we have described are connector types in the sense that they can be instanti-
ated. More concretely, the roles of a connector type can be instantiated with specific 
designs. In WRIGHT [1], role instantiation has to obey a compatibility requirement 
expressed via the refinement relation of CSP.  In CommUnity, the refinement relation 
is also defined over a category of designs but using a different notion of morphism. 
Indeed, the notion of morphism defined in the previous section does not capture a 
refinement relation in the sense that it does not ensure that any implementation of the 
target provides an implementation for the source.  For instance, it is easy to see that 
morphisms do not preserve the interval assigned to the guard of each action.  Given 
that the aim of the defined morphisms was to capture the relationship that exists be-
tween systems and their components, this is hardly surprising.  It is well known in 
languages such as CSP that the parallel composition of a collection of processes does 
not refine, necessarily, any of the individual processes.   
The notion of morphism that captures the refinement relation in CommUnity is the 
following: 
A refinement morphism σ:P1→P2 of designs P1= <θ1,∆1> and  P2= <θ2,∆2>  is a 
pair <σch,σac> where 
• σch:V1→Term(V2) is a total function mapping the channels of P1 to the class of 
terms built from the channels of P2 and the data type operations.  This mapping is 
required to satisfy, for every v∈V1, o∈out(V1), i∈in(V1), p∈prv(V1):
- sort2(σch(v))=sort1(v)
- σch(o)∈out(V2)
- σch(i)∈in(V2)
- σch(p)∈Term(loc(V2))
- σch↓(out(V1)∪in(V1)) is injective 
• σac: Γ2→Γ1 is a partial mapping satisfying for every g∈Γ2 s.t. σac(g) is defined: 
- if g∈sh(Γ2) then σac(g)∈sh(Γ1)
- if g∈prv(Γ2) then σac(g)∈prv(Γ1)
- if g∈sh(Γ1) then σ
1
ac
− (g)∅ 
- σch(D1(σac(g)))⊆D2(g)
- σac is total on D2(σch(v)) and σac(D2(σch(v)))⊆D1(v) for every v∈loc(V1)
and, furthermore, 
• 
for every g∈Γ2 s.t. σac(g) is defined: 
1. Φ |– (R2(g)⊃σ(R1(σac(g))));
2. Φ |– (L2(g) ⊃σ(L1(σac(g))));
• 
for every g1∈Γ , 
3. Φ |– (σ(U1(g1)) ⊃
2
1
(
)
ac g
g
σ
=
∆
U2(g2))
where  D is the extension of D to the language of expressions. Designs and their 
refinement  morphisms constitute a category r-DSGN.
A refinement morphism is intended to support the identification of a way in which 
a design P1 (its source) is refined by a more concrete design P2 (its target). 

A Mathematical Semantics for Architectural Connectors      193 
The function σch identifies for each input (resp. output) channel of P1 the corre-
sponding input (resp. output) channel of P2.  Notice that, contrarily to what happens 
with the component-of relationship, refinement does not change the border between 
the system and its environment and, hence, input channels can no longer be mapped 
to output channels.  Moreover, refinement morphisms allow each private channel of 
P1 to be expressed in terms of the local channels of P2 through an expression.  The 
evaluation of such an expression may involve some computation as captured through 
the use of operations from the underlying data types. Naturally it is required that the 
sorts of channels be preserved.  
The mapping σac identifies for each action g of P1, the set of actions of P2 that im-
plements g – given by σ
1
ac
−(g). This set is a menu of refinements for action g and can 
be empty for private actions. However, every action that models interaction between 
the component and its environment has to be implemented.  
The actions for which σac is left undefined (the new actions) and the channels 
which are not involved in σch(V1) (the new channels) introduce more detail in the 
description of the component.  
Conditions 2 and 3 require that the interval defined by the blocking and progress 
conditions of each action (in which the enabling condition of any guarded command 
that implements the action must lie) be preserved or reduced. This is intuitive because 
refinement, pointing in the direction of implementations, should reduce underspecifi-
cation. This is also the reason why the effects of the actions of the more abstract de-
sign are required to be preserved or made more deterministic.
For instance, sender is refined by user
via the refinement morphism 
η:sender→user defined by 
 
ηch(o)=p, ηch(rd)=(s=2)
ηac(pr_ps)=ηac(pr_pdf)=prod, ηac(print)=send.
In user, the production of messages (to be sent) is modelled by any of the actions 
pr_ps and pr_pdf and the messages are made available in the output channel p.  No-
tice that the production of messages, that was left unspecified in sender, is completely 
defined in user: it corresponds to the conversion of the files stored in w to ps or pdf 
formats.
In the graphical notation, refinement is represented through bold (thick) lines: 
Likewise, printer is a refinement of receiver via the refinement morphism 
κ:receiver→printer defined by 
sender
o
send
prod
user
work
prin
p
pr pd
pr p

194      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
κch(i)=rdoc
κac(rec)=rec
In printer, the reception of a message from the input channel (named rdoc) corre-
sponds to downloading it into the private channel pdoc.  This action is only enabled if 
the previous message has already been printed.  
An instantiation of a connector in CommUnity can now be defined as follows: 
• An instantiation of a connection with role R consists of a design P together with a 
refinement morphism φ:R→P.
• An instantiation of a connector consists of an instantiation for each of its connec-
tions. 
In order to define the semantics of such an instantiation, notice that each instantia-
tion φ:R→P of a connection can be composed with µ to define µ;φ:θ→sig(P).  This 
is because θ is according to the rules that define well-formed configurations and, 
hence, has no private channels, which means that the refinement morphism has the 
same properties as a composition morphism over θ.  As we have already argued, 
every such signature morphism can be lifted to a design morphism µ;φ:dsgn(θ)→P.
Hence, an instantiation of a connector defines a diagram in c-DSGN that connects the 
role instances to the glue. 
Moreover, because each connection is according to the rules set for well-formed 
configurations as detailed in the previous section, the diagram defined by the instan-
tiation is, indeed, a configuration and, hence, has a colimit. 
The semantics of a connector instantiation is the colimit of the diagram in 
c-DSGN formed as described above by composing the role morphism of each con-
nection with its instantiation. 
Because, as we have already argued, colimits in c-DSGN express parallel compo-
sition, this semantics agrees with the one provided in [1] for the language WRIGHT.  
In the next section, we shall take this analogy with WRIGHT one step further. 
Moreover, the categorical formalisation makes it possible to prove that the design 
that results from the semantics of the instantiation is a refinement of the semantics of 
the connector itself. 
As an example, let us consider, for simplicity, a connector with one role. 

A Mathematical Semantics for Architectural Connectors      195 
The meaning of the connector is given by the colimit of the pair <µ,σ> – 
<α:R→C,β:G→C>.  The instantiation of the role with the component P through the 
refinement morphism φ is given by the colimit of <µ;φ,σ> – <α':P→S,β':G→S>.
We can prove that there exists a refinement morphism φ':C→S, which establishes the 
“correctness” of the instantiation mechanism.  This is because all the different objects 
and morphisms involved can be brought into a more general category in which the 
universal properties of colimits guarantee the existence of the required refinement 
morphism.  A full proof of this property can be found in [21]. 
As an example, consider again the connector Async.  As we have already seen, its 
roles – sender and receiver – are refined by the designs user and printer, respectively.
Therefore, Async can be used for interconnecting these two components, giving rise 
to a configuration in which user sends the print requests to printer through a bounded 
channel.
The final configuration is obtained by calculating the composition of the signature 
morphisms that define the two connections of Async with the refinement morphisms 
η,κ. For instance, the channel p of user gets connected to the input channel i of buffer 
because η(o)=p and o is connected to i of buffer. The resulting configuration is ex-
actly the one we have already presented in section 2.2. 
Architectural connectors are used for systematising software development by of-
fering “standard” means for interconnecting components that can be reused from one 
application to another.  In this sense, the “typical” glue is a program that implements 

196      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
a well-established pattern of behaviour that can be superposed to existing components 
of a system through the instantiation of the roles of the connector.   
However, architectures also fulfil an important role in supporting a high-level de-
scription of the organisation of a system by identifying its main components and the 
way these components are interconnected.  An early identification of the architectural 
elements intended for a system will help to manage the subsequent design phases 
according to the organisation that they imply, identifying opportunities for reuse or 
the integration of third-party components.  From this point of view, it seems useful to 
allow for connectors to be based on glues that are not yet fully developed as programs 
but for which concrete commitments have already been made to determine the type of 
interconnection that they will ensure.  For instance, at an early stage of development, 
one may decide on adopting a client-server architecture without committing to a spe-
cific protocol of communication between the client and the server.  This is why, in 
the definition of connector in CommUnity, we left open the possibility for the glue 
not to be a program but a design in general.   
However, in this more general framework, we have to account for the possible re-
finements of the glue.  What happens if we refine the glue of a connector that has 
been instantiated to given components of a system? Is the resulting design a refine-
ment of the more abstract design from which we started?  More generally, how do 
connectors propagate through design, be it because the instances of the roles are re-
fined or the glue is refined?  One of the advantages of using Category Theory as a 
mathematical framework for formalising architectures is that answers to questions 
like these can be discussed at the right level of abstraction.  Another advantage is that 
the questions themselves can be formulated in terms that are independent of any spe-
cific ADL and answered by characterising the classes of ADLs that satisfy the given 
properties.  This is what we will do in later sections. 
3.2 
Examples 
We now present more examples of connectors, namely some that we will need in 
later sections for illustrating algebraic operations on connectors.  These examples 
relate to a case study on mobility [29]: One or more carts move continuously in the 
same direction on a U-units long circular track.  A cart advances one unit at each step.
Along the track there are stations.  There is at most one station per unit.  Each station 
corresponds to a check-in counter or to a gate.  Carts take bags from check-in stations 
to gate stations.  All bags from a given check-in go to the same gate.  A cart trans-
ports at most one bag at a time.  When it is empty, the cart picks a bag up from the 
nearest check-in.  Carts must not bump into each other.  Carts also keep a count  of 
how many laps they have done, starting at some initial location. 
The program that controls a cart is 
design cart is
 in  idest: 0..U - 1, ibag : int 
 out  obag, laps : int 
 prv  loc: 0..U - 1, dest: -1..U - 1, initloc : int 
 do  move: loc ≠ dest →  loc := loc +U 1 ||  laps := if(loc=initloc,laps+1,laps) 
 []  get: dest = -1 →  obag := ibag || dest := idest 
 []  put: loc = dest →  obag := 0 || dest := -1 
where +U is addition modulo U.

A Mathematical Semantics for Architectural Connectors      197 
Locations are represented by integers from zero to the track length minus one. 
Bags are represented by integers, the absence of a bag being denoted by zero. When-
ever the cart is empty, its destination is an  impossible location, so that the cart keeps 
moving until it gets a bag and a valid gate location through action get. When it 
reaches its destination, the cart unloads the bag through action put. Notice that since 
input channels may be changed arbitrarily by the environment, the cart must copy 
their values to output channels to make sure the correct bag is unloaded at the correct 
gate. 
A check-in counter manages a queue of bags that it loads one by one onto passing 
carts.
 
design check-in is
 
out  bag : int, dest : 0..U – 1, 
prv
loc : 0..U – 1, next : bool, q : list(int)
 
do  new: q≠[] ∧ next →  bag := head(q) || q := tail(q) || next := false 
 
[]   put: ¬next →  next := true 
Channel next is used to impose sequentiality among the actions. In a configuration 
in which a cart is loading at a gate, the put action must be synchronised with a cart’s 
get action and channels bag and dest must be shared with ibag and idest, respectively. 
A gate keeps a queue of bags and adds each new bag to the tail. 
 
design gate is
 
in
 bag : int 
 
prv  loc : 0..U - 1, q : list(int) 
 
do  get : q := q.bag 
In a configuration in which a cart is unloading at a gate, action get of the gate must 
be synchronised with the cart’s put action, and channel bag must be shared with 
obag.
Synchronisation
We begin with the connector that allows us to synchronise two actions of different 
components.  A cable would suffice for this purpose, but it is not able to capture the 
general case of transient synchronisation [29].  Having already a connector for the 
simpler case makes the presentation more uniform.  The synchronisation connector 
has a glue identical to its roles: 
with  
 
design action is
 
do  a: true,false → skip
Notice that the action has the least deterministic specification possible: its guard is 
given the widest possible interval and no commitments on its effects.  Hence, it can 
be refined by any action. 
action
action
action
a
a
a

198      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
According to the colimit semantics of connectors, when the two roles are instanti-
ated with particular actions a1 and a2 of particular components, the components have 
to synchronise with each other every time one of them wants to execute the corre-
sponding action: either both execute the joint action, or none executes. 
As an example of using this connector, if we wish to count how often a cart 
unloads, we can monitor its put action with a counter: 
 
design counter is
out  c:int 
do  inc: true → c := c + 1
 
[]   reset: true →  c := 0 
Notice that, because inc is always enabled, its synchronisation with put does not 
interfere with the behaviour of the cart.  Hence, we can say that the counter is, in-
deed, monitoring the cart. 
Subsumption
Intuitively, synchronisation is an “equivalence” between the occurrence of two ac-
tions: the occurrence of each of the actions “implies” the occurrence of the other.  In 
certain circumstances, we are interested in connecting two actions by only one of the 
implications.  For instance, to avoid a cart c1 colliding with the cart c2 right in front of 
it, we only need one implication: if c1 moves, so must c2.  The other implication is not 
necessary.  The analogy with implication also extends to the counter-positive: if c2
cannot move, for instance because it is (un)loading a bag, then neither can c1.  We call 
this “one-way” synchronisation action subsumption.
As for the connector, it simply adds to the synchronisation connector the ability to 
let the subsumed action occur freely.  This is only possible because our signature 
morphisms are contravariant on actions and, hence, allow an action to ramify into a 
set of actions.  In order to prevent carts from colliding, all that we have to do is to 
ramify the move action of the cart in front in two: one that accounts for the situation 
in which the cart behind moves, which needs to be performed synchronously, and the 
other to account for free move actions, i.e. moves that are not implied by the cart 
behind. 
action
action
action
cart
put
counter
inc
ibag
idest
obag
c
move
get
laps
a
a
a
re-

A Mathematical Semantics for Architectural Connectors      199 
The subsumption connector is as follows: 
where the glue is now given by 
design subsume is
do sync: true,false → skip
[] free: true,false → skip
Notice that although the two roles are equal, the binary connector is not symmetric 
because the connections treat the two actions differently: the right-hand one may be 
executed alone at any time, while the left-hand one must co-occur with the right-hand 
one, through action sync.  Indeed, if we instantiate the left-hand role with an action a1
and the right-hand role with an action a2, the semantics of the interconnection, as 
obtained through the colimit, is given by two synchronisation sets: {a1,sync,a2} and 
{free,a2}.   Notice that action a1 can only occur together with a2 but that a2 can occur 
without a1.
Hence, the left-hand action is the one that we want to connect to the move action of 
the cart  behind – c1, while the right-hand action is associated to the movement of c2 – 
the cart in front, as shown next.  
Ramification
A generalisation of the previous connectors is to allow an action to synchronise, in-
dependently, with two actions of two different programs.  The importance of this 
connector will become apparent in Section 5, where it is shown to be a primitive from 
which other connectors can be built. 
action
subsume
action
sync
free
a
a

200      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
 
design ramify is
 
do  branch1: true, false → skip
 
 
 branch2: true, false → skip
If we instantiate the left-hand role with an action a1 and the right-hand role with an 
action a2, and the middle role with an action b, the semantics of the interconnection, 
as obtained through the colimit, is given by two synchronisation sets: {a1,branch1,b}
and {a2,branch2,b}.   Notice that action b will always occur simultaneously with either 
a1 or a2 but not with both.  This connector can be generalised to any finite number of 
ramifications.  It allows for a server, e.g. a buffer, to be connected simultaneously, 
but independently, to a fixed maximum number of components. 
Inhibition
Another useful connector is the one that allows us to inhibit an action by making its 
guard false.  This is useful when, for some reason, we need to prevent an action from 
occurring but without having to reprogram the component.  Indeed, the mechanism of 
superposition that we have used as a semantics for the application of architectural 
connectors allow us to disable an action without changing the guard directly: it suf-
fices to synchronise the action with one that has a false guard. 
 
design inhibit is
 
do never: false →  skip
When the role is instantiated with an action with guard B, the result of the inter-
connection is the same action guarded by B∧false.  This connector can be generalised 
to arbitrary conditions with which one wants to strengthen the guards of given ac-
tions.  The inhibitor just has to be provided with the data that is necessary to compute 
the condition C that will strengthen the guard, for instance through the use of input 
channels through which we can select the sources of the information that will disable 
the action. 
 
design inhibit(C) is
 
in ...
 
do never: C →  skip
action
ramify
action
branch1
branch2
a
action
action
inhibit
never
a
a
a
a

A Mathematical Semantics for Architectural Connectors      201 
The result of instantiating the role with an action with guard B is the same action 
guarded by B∧C.
4 
An ADL-Independent Notion of Connector 
The notion of connector  that we presented for CommUnity can be generalised to 
other design formalisms. In this section, we shall discuss the properties that such 
formalisms need to satisfy for supporting the architectural concepts and mechanisms 
that we have illustrated for CommUnity.  
Before embarking on this discussion, we need to fix a framework in which de-
signs, configurations and relationships between designs, such as refinement, can be 
formally described.   Our past experience in formalising notions of structure in Com-
puting, building on previous work of J.Goguen on General Systems Theory, suggests 
that, as illustrated in section 2, Category Theory provides a convenient framework for 
our purpose.  More concretely, we shall consider that a formalism supporting system 
design includes : 
• a category c-DESC of component descriptions in which systems of interconnected 
components are modelled through diagrams; 
• for every set CD of component descriptions, a set Conf(CD) consisting of all 
well-formed configurations that can be built from the components in CD.  Each 
such configuration is a diagram in c-DESC that is guaranteed to have a colimit.  
Typically, Conf is given through a set of rules that govern the interconnection of 
components in the formalism.  
• a category r-DESC with the same objects as c-DESC, but in which morphisms 
model refinement, i.e. a morphism η:S→S’ in r-DESC expresses that S’ refines S,
identifying the design decisions that lead from S to S’. Because the description of a 
composite system is given by a colimit of a diagram in c-DESC and, hence, is de-
fined up to an isomorphism in c-DESC, refinement morphisms must be such that 
descriptions that are isomorphic in c-DESC refine, and are refined exactly by, the 
same descriptions. Hence, it is required that  
 
Isomorph(c-DESC)⊆Isomorph(r-DESC).
Summarising, all that we require is a notion of system description, a relationship 
between descriptions that captures components of systems, another relationship that 
captures refinement, and criteria for determining when a diagram of interconnected 
components is a well-formed configuration.  These requirements are discussed in 
more detail in [23]. 
4.1 
Architectural Schools 
In the context of this categorical framework, we shall now discuss the properties that 
are necessary for supporting Software Architectures.  

202      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
Coordination 
A key property of a formalism for supporting architectural design is that it provides a 
clear separation between the description of individual components and their interac-
tion in the overall system organisation.  In other words, the formalism must support 
the separation between what, in the description of a system, is responsible for its 
computational aspects and what is concerned with coordinating the interaction be-
tween its different components.  
In the case of CommUnity, as we have seen, only signatures are involved in inter-
connections.  The body of a component design describes its functionality and, hence, 
corresponds to the computational part of the design.   At the more general level that 
we are discussing, we shall take the separation between coordination and computation 
to be materialised through a functor sig: c-DESC→SIG mapping descriptions to 
signatures, forgetting their computational aspects.  The fact that the computational 
side does not play any role in the interconnection of systems can be captured by the 
following properties of this functor: 
• sig is faithful;
• sig lifts colimits of well-formed configurations;
• sig has discrete structures;
together with the following condition on the well-formed configuration criterion 
• given any pair of configuration diagrams dia1, dia2 s.t. dia1;sig=dia2;sig, either 
both are well-formed or both are ill-formed.
The fact that sig is faithful (i.e., injective over each hom-set) means that mor-
phisms of systems cannot induce more relationships than those that can be established 
between their underlying signatures.
The requirement that sig lifts colimits means that, given any well-formed configu-
ration expressed as a diagram dia:Ic-DESC of descriptions and colimit 
(sig(Si)→θ)i:I of the underlying diagram of signatures, i.e. of (dia;sig), there exists a 
colimit (Si→S)i:I of the diagram dia of descriptions whose signature part is the given 
colimit of signatures, i.e. sig(Si→S)=(sig(Si)→θ). This means that if we interconnect 
system components through a well-formed configuration, then any colimit of the 
underlying diagram of signatures establishes a signature for which a computational 
part exists that captures the joint behaviour of the interconnected components. 
The requirement that sig has discrete structures means that, for every signature 
θ:SIG, there exists a description s(θ):c-DESC such that, for every signature mor-
phism f:θ→sig(S), there is a morphism g:s(θ)→S in c-DESC such that sig(g)=f. That 
is to say, every signature θ has a “realisation” (a discrete lift) as a system component 
s(θ) in the sense that, using θ to interconnect a component S, which is achieved 
through a morphism f:θ→sig(S), is tantamount to using s(θ) through any g:s(θ)→S
such that sig(g)=f.  Notice that, because sig is faithful, there is only one such g, which 
means that f and g are, essentially, the same.  That is, sources of morphisms in dia-
grams of descriptions are, essentially, signatures. 
These properties constitute what we call a coordinated formalism.  More precisely, 
we say that c-DESC is coordinated over SIG through the functor sig.  In a coordi-
nated formalism, any interconnection of systems can be established via their signa-
tures, legitimating the use of signatures as cables in configuration diagrams.  By re-

A Mathematical Semantics for Architectural Connectors      203 
quiring that any two configuration diagrams that establish the same interconnections 
at the level of signatures be either both well-formed or both ill-formed, the fourth 
property ensures that the criteria for well-formed configurations do not rely on the 
computational parts of descriptions. 
Refinement and Compositionality 
Another crucial property for supporting architectural design is in the interplay be-
tween structuring systems in architectural terms and refinement.  We have already 
pointed out that one of the goals of Software Architectures is to support a view of the 
gross organisation of systems in terms of components and their interconnections that 
can be carried through the refinement steps that eventually lead to the implementation 
of all its components.  Hence, it is necessary that the application of architectural con-
nectors to abstract designs, as a means of making early decisions on the way certain 
components need to be coordinated, will not be jeopardised by subsequent refine-
ments of the component designs towards their final implementations.  Likewise, it is 
desirable that the application of a connector may be made on the basis of an abstract 
design of its glue as a means of determining main aspects of the required coordina-
tion without committing to the final mechanisms that will bring about that coordina-
tion. 
One of the advantages of the categorical framework that we have been proposing 
is that it makes the formulation of these properties relatively easy, leading to a char-
acterisation of the design formalisms that support them in terms of the structural 
properties that we have been discussing.  For instance, we have already seen that, in 
the situations in which refinement morphisms map directly to signature morphisms, 
we may simply put together, in a diagram of signatures, the morphisms that define 
the interactions and the morphisms that establish the refinement of the component 
descriptions. 
More precisely, in the situations in which there exists a forgetful functor r-sig:
r-DESC→SIG that agrees with the coordination functor sig on signatures, i.e. 
r-sig(S)=sig(S) for every S:c-DESC, given a well-formed configuration diagram dia
of a system with components S1,...,Sn and  refinement morphisms ηi:Si→S’i: i∈1..j,
dia
…
…
…
S1
…
 Si
…
Sn
…
η1
ηi
ηn
S'1
 S'i
S'n
we can obtain a new diagram in c-DESC and, hence, a new configuration, by com-
posing the morphisms r-sig(ηi) with those in dia that originate in cables (signatures) 
and have the Si as targets. 

204      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
However, as we have seen in the case of CommUnity, it may be possible to propa-
gate the interactions between the components of a system when their descriptions are 
replaced by more concrete ones even when refinement morphisms do not map to 
signature morphisms.  This more general situation can be characterised by the exis-
tence, for every well-formed configuration dia involving descriptions {S1,...,Sn} and
refinements morphisms {ηi:Si→S’i: i∈1..n}, of a well-formed configuration diagram 
dia+(ηi) that characterises the system obtained by replacing the Si by their refine-
ments.  The correctness criterion for this form of “configuration refinement” is that 
the colimit of dia+(ηi) provides a refinement for the colimit of dia.
dia
…
…
S1
…
 Si
…
Sn
…
S
η1
ηi
ηn
S'1
 S'i
S'n
dia+(ηi)
…
  
… S'1
…
 S'i
…
S'n
…
S'
We shall say that the formalisms that support such forms of correct configuration 
refinement are compositional.
When we consider the specific case of the configurations obtained by direct instan-
tiation of an architectural connector, this property reflects the compositionality of the 
connector as an operation on configurations.  Compositionality ensures that the se-
mantics of the connector is preserved (refined) by any system that results from its 
instantiation. For instance, in the case of a binary connector 

A Mathematical Semantics for Architectural Connectors      205 
G
θ1
θ2
dia
R1
  
R2
and given instantiations  η1:R1→P1 and η2:R2→P2, the description returned by the 
colimit of dia is refined by the description returned by the colimit of dia+(η1,η2).
G
θ1
θ2
dia+(η1,η2)
P1
  
P2
Likewise, compositionality guarantees that if a connector with an abstract glue G
is applied to given designs, and the glue is later on refined through a morphism 
η:G→G’, the description that is obtained through the colimit of dia+η is a refinement 
of the semantics of the original instantiation.  In fact, we can consider the refinement 
of the glue to be a special case of an operation on the connector that delivers another 
connector – a refinement of the original one in the case at hand.  Other operations 
will be analysed in section 5. 
Summary
In summary, a formalism F=<c-DESC,Conf,r-DESC> supports architectural de-
sign, and is called an architectural school,  if 
• c-DESC is coordinated over a category SIG through a functor sig: c-DESC→SIG 
• F is compositional  
GAMMA
We shall now illustrate the notion of architectural school with an example borrowed 
from coordination formalisms: the language Gamma [5] based on the chemical reac-
tion paradigm. 
A Gamma program P consists of  
• a signature Σ=<S,Ω,F>, where S is a set of sorts, Ω is a set of operation symbols 
and F is a set of function symbols, representing the data types that the program 
uses;
• a set of reactions, where a reaction R has the following structure: 
 
 
R 
≡  X, t1, ..., tn → t’1, ..., t’m ⇐ c 
 
where 

206      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
− 
X is a set (of variables); each variable is typed by a data sort in S; 
− 
t1, ..., tn → t’1, ..., t’m is the action of the reaction – a pair of sets of 
terms over X; 
− 
c is the reaction condition – a proposition over X. 
An example of a Gamma program is the following producer of burgers and salads 
from, respectively, meat and vegetables: 
PROD ≡ 
sorts  
meat, veg, burger, salad 
 
 
 
 
 
ops
vprod: veg→salad, mprod: meat→burger
 
 
 
reactions m:meat, m → mprod(m)  
 
 
 
 
 
 
 
v:veg, v →•vprod(v)
Gamma programs are always composable and their parallel composition, as de-
fined in [5], is a program consisting of all the reactions of the component programs – 
its behaviour is obtained by executing the reactions of the component programs in 
any order, possibly in parallel.  This leads us to the following notion of morphism.   
A morphism σ between Gamma programs P1 and P2 is a morphism between the 
underlying data signatures s.t. σ(P1)⊆P2, i.e., P2 has more reactions than P1.  Gamma 
programs and their morphisms constitute a category GAMMA.
This category is well behaved as far as interconnections are concerned, which 
means that every diagram is considered to be a well-formed configuration.  In order 
to illustrate system configuration in Gamma, consider that we want to interconnect 
the producer with the following consumer: 
CONS
≡ sorts  
food, waste  
 
ops
cons: food→waste,  
 
 
 
 
reactions f:food, f→cons(f)
The interconnection of the two programs is based on the identification of the food 
the consumer consumes, that is, the interconnection is established between their data 
types.  For instance, the coordination of the producer and the consumer based on 
meat is given by the following interconnection: 
 
 
 
 
 
 
  
sorts s
PROD         
 CONS
Given the simplicity of the mechanisms available in Gamma for interconnecting 
components, it is not difficult to conclude that Gamma is coordinated over the cate-
gory of data types: 
• the forgetful functor dt from Gamma programs to data types is faithful;  
• given any diagram in the category GAMMA, a colimit σi:(dt(Pi)→Σ)i:I of the cor-
responding diagram in the category of data types is lifted to the following colimit 
of programs σi:(Pi→<Σ,∪σj(Rj)>)i:I;
• the discrete lift of a data type is the program with the empty set of reactions. 
Notice, however, that we have extended the way in which Gamma programs are 
traditionally put together.  Gamma assumes a global data space whereas we have 
made it possible for Gamma programs to be developed separately and put together by 
meat←s→food

A Mathematical Semantics for Architectural Connectors      207 
matching the features that they are required to have in common.  This localisation 
further enhances the reusability of Gamma programs. 
In order to complete the picture, it remains to provide a notion of refinement. This 
is very simple because the morphisms of GAMMA may also be used for modelling 
the refinement relationship.  That is to say, a program P is refined by a program Q if
and only the reactions of P are also present in Q.  It is very easy to check that 
GAMMA defined in this way is compositional and, hence, defines an architectural 
school. 
Besides CommUnity and Gamma, other formalisms define architectural schools.  
For instance, the concurrency models that are formalised in [28] using categorical 
techniques satisfy the properties that we have laid down for architectural schools.  
Such formalisms fulfil a very important role in providing models of system behaviour 
that can be used as abstractions of non-software components in architectures.  The 
category of theories of any institution [19] also defines an architectural school, thus 
showing that another class of abstractions – capturing specifications of system behav-
iour – can also be used for architectural design.  Hence, the notion of architectural 
school that we put forward encompasses a large variety of formalisms.  In the next 
sections, we are going to show how we can capitalise on this variety. 
4.2 
Adding Abstraction to Architectural Connectors 
The mathematical framework that we presented in the previous sections provides not 
only an ADL-independent semantics for the principles and techniques that can be 
found in existing approaches to Software Architectures, but also a basis for extending 
the capabilities of existing ADLs.  Until the end of the paper, we will present and 
explore some of the avenues that this mathematical characterisation has opened, hop-
ing that the reader will want to explore them even further.  In this section, we will 
show how the roles of a connector can be formulated at a more declarative level, such 
as a specification logic, instead of a design language like CommUnity.    
As already mentioned, the purpose of the roles in a connector is to impose restric-
tions on the local behaviour of the components that are admissible as instances. In the 
approach to architectural design outlined in the previous sections, this is achieved 
through the notion of correct instantiation via refinement morphisms. As also seen 
above, roles do not play any part in the calculation of the resulting system. They are 
only used for defining what a correct instantiation is. This separation of concerns 
justifies the adoption of a more declarative formalism for the specification of roles, 
namely one in which it is easier to formulate the properties required of components to 
be admissible instances. 
In this section, we are going to place ourselves in the situation in which the glues 
are designs, the roles are specifications, and the instantiations of the roles are, again, 
designs.   We are going to consider that specifications are given as a category SPEC,
e.g. the category of theories of a logic formalised as an institution [19].  We take the 
relationship between specifications and designs to be captured through the following 
elements:
• a functor spec:SIGSPEC mapping signatures and their morphisms to specifica-
tions. 
• a satisfaction relation |= between design morphisms and specification morphisms 
satisfying the following properties:  

208      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
- If  π:P→P’ |= σ:S→S’, then idP |= idS and  idP’ |= idS’.
- If  π1:P1→P2 |= σ1:S1→S2 and π2:P2→P3 |= σ2:S2→S3 then π1;π2 |= σ1;σ2
- Let s:I→SPEC be a diagram of specifications and  p:I→c-DSGN a diagram 
of designs with the same shape such that, for every edge  f:i→j in I, pf:pi→pj
|= sf:si→sj.  We require that, if p admits a colimit πi:pi→P, then s admits a 
colimit σi:si→S such that, for every node i:I, πi |=σi.
- If idP |= idS and ρ:P→P’ is a refinement morphism, then idP’ |= idS
- For every signature θ, iddsgn(θ) |= idspec(θ)
The idea behind the functor spec is that, just like, through dsgn, signatures provide 
the means for interconnecting designs, they should also provide means for intercon-
necting specifications.  Hence, every signature generates a canonical specification – 
the specification of a cable.  However, we do not put as many constraints on spec as 
on dsgn because, for the purposes of this section, we are limiting the use of specifica-
tions to the definition of connector roles.  Naturally, if we wish to address architec-
ture building at the specification level, then we will have to require SPEC to satisfy 
the properties that we discussed in section 4.1. 
The satisfaction relation is defined directly on morphisms because our ultimate 
goal is to address interconnections, not just components.  Satisfaction of component 
specifications by designs is given through the identity morphisms.  The properties 
required of the satisfaction relation address its compatibility with the categorical 
constructions that we use, namely composition of morphisms and colimits.  The last 
two properties mean that refinement of component designs leaves the satisfaction 
relation invariant, and that the design (cable) generated by every signature satisfies 
the specification (cable) generated by the same signature.
Given such a setting, we generalise the notion of connector as follows: 
• A connection consists of 
- a design G and a specification R, called the glue and the role of the connec-
tion, respectively; 
- a signature θ and two morphisms µ:dsgn(θ)→G, σ:spec(θ)→R in c-DSGN
and SPEC, respectively,  connecting the glue and the role via the signature 
(cable). 
• A connector is a finite set of connections with the same glue. 
• An instantiation of a connection with signature θ and role morphism σ consists of 
a design P and a design morphism π:dsgn(θ)→P such that π |=σ.
• An instantiation of a connector consists of an instantiation for each of its connec-
tions.  An instantiation is said to be correct if the diagram defined by the instantia-
tion morphisms and the glue morphisms is a well-formed configuration. The 
colimit of this configuration defines the semantics of the instantiation, guaranteed 
to exist if the instantiation is correct.
Although the generalisation seems to be quite straightforward, we do not have an 
immediate generalisation for the semantics of connectors.  This is because the glue is 
a design and the role is a specification, which means that a connector does not pro-
vide us with a diagram like in the homogeneous case that we studied in section 3.  
However, if we are provided with a specification for the glue, we can provide seman-
tics for the connector at the specification level: 

A Mathematical Semantics for Architectural Connectors      209 
• A complete connection consists of 
- a design G and a specification R, called the glue and the role of the connec-
tion, respectively; 
- a signature θ and two morphismsµ:dsgn(θ)→G,σ:spec(θ)→R in c-DSGN and 
SPEC, respectively,  connecting the glue and the role via the signature (ca-
ble);
- a specification S and a morphism τ:spec(θ)→S such that µ |=τ.  Note that this 
means that the design G satisfies the specification S. 
• A complete connector is a finite set of complete connections with the same glue 
design and specification.  Its semantics is given by the colimit, if it exists, of the 
SPEC-diagram defined by the σi and the τi.
• The semantics of the instantiation of a complete connector satisfies the semantics 
of the connector.
An illustration of an abstract architectural school can be given in terms of a linear 
temporal logic. We adopt  the traditional notion of specification as a collection of 
sentences (a theory presentation) expressing the properties the component is required 
to satisfy. These properties are given in terms of the vocabulary of the component 
being specified – a set of input channels, a set of output channels and a set of action 
names.
We start by defining the language of the logic, for which the distinction between 
input and output channels is irrelevant, and then we define specifications. 
A temporal signature is a pair <V,Γ> where 
• V is an S-indexed family of mutually disjoint finite sets, 
• Γ is a finite set. 
The language of terms over a temporal signature tθ=<V,Γ>, with sort s, is 
Term(tθ)s defined by 
ts ::=v | c | (Xts) | f(ts1,…,tsn)   
for every v∈Vs, c∈Ω<>,s and f∈Ω<s1,…,sn>,s. The language of temporal proposi-
tions over a temporal signature tθ=<V,Γ> is Prop(tθ) defined by 
φ ::= (t1=st2) | g | (¬φ) | (φ⊃φ’) |  (Xφ) | (φUφ’) 
 
for every s∈S, g∈Γ, t1,t2∈ Term(tθ)s.
The temporal operators are X (next) and U (until). Intuitively,  
• Xφ holds in a state if φ holds in the next state; 
• φUφ’ holds when φ’ will hold sometime in the future and φ holds between now and 
then. 
As usual, we will consider that other temporal operators like G (always in the fu-
ture) and F (eventually) are defined as abbreviations: Gφ≡abv (φUfalse) and
Fφ≡abv (¬G(¬φ)).
A specification is a pair <<V,Γ,tv>,Φ> where <V,Γ> is a temporal signature,
tv:V→{in,out} is a total function (that identifies the type of channels)  and Φ is a set 
of propositions in Prop(<V,Γ>) . 
As an example, we present below the specifications of a typical sender and re-
ceiver of messages through a pipe.  

210      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
design sender[t:sort] is 
 
 
design receiver[t:sort] is
out   o:t, eof:bool  
 
in
o:t, eof:bool 
actions  
send 
 
 
 
 
out cl:bool 
axioms   
eof ⊃G(¬send ∧ eof)  
 
actions rec 
  
   
 
 
 
 
axioms  cl ⊃G(¬rec ∧ cl) 
  
   
 
 
 
 
 
((eof⊃Geof) ∧ (eof∧ cl)) ⊃ (¬recUcl)
The component sender interacts with its environment through the action send ac-
counting for the transmission of a message through the output channel o.  Further-
more, it signals the end of data through the output channel eof.  The axiom requires 
that eof be stable (remains true once it becomes true) and transmission of messages to 
cease once eof becomes true. 
The component receiver interacts with its environment through the action rec ac-
couting for the reception of a message through the input channel i.  The other means 
of interaction with the environment is concerned with the closure of communication.  
The component receives, through the input channel eof, a Boolean that indicates if 
transmission along i has ceased and signals the closure of communication through the 
output channel cl.  The first axiom requires that cl be stable and the reception of mes-
sages to cease once cl becomes true.  The second axiom expresses that, if the infor-
mation received through eof is stable, the receiver is obliged to close the communica-
tion as soon as it is informed that there will be no more data.  However, the receiver 
may decide to close the communication before that. 
Specification morphisms are taken as property preserving mappings that, as hap-
pens with the morphisms of design signatures, must map output channels into output 
channels.
 A morphism σ:tθ1→tθ2 of temporal signatures is a pair <σch,σac> where σch:V1→V2
is a total function satisfying sort2(σch(v))=sort1(v),  for every v∈V1, and σac: Γ2→Γ1 is a 
partial mapping.
A morphism σ:<<V1,Γ1,tv1>,Φ1>→<<V2,Γ2,tv2>,Φ2> of specifications is a morphism 
σ:<V1,Γ1>→<V2,Γ2> of temporal signatures s.t. 
1. if tv1(v)=out then tv2(σch(v))=out
2. Φ2 |– σ(Φ1)
where |–  is the usual consequence relation for linear temporal logic and σ is the 
extension of σ to the language of propositions. Specifications and their morphisms 
constitute a category SPEC.
It remains to capture the relationship between specifications as just introduced and 
designs in CommUnity. 
First we notice that every design signature θ can be mapped into a temporal signa-
ture by forgetting the different classes of channels and actions, as well as the write 
frames of actions.  Then, we notice that part of the semantics of CommUnity designs 
can be encoded in LTL.  More concretely, we may express in LTL the following facts 
already mentioned in section 2.1: 
• the negation of L(g) is a blocking condition of action g;
• for every local channel v, D(v) consists of the set of actions that can modify it; 
• the condition R(g) holds in every state in which g is executed;

A Mathematical Semantics for Architectural Connectors      211 
• private actions that are infinitely often enabled are guaranteed to be selected infi-
nitely often. 
A design P=<<V,Γ,tv,ta,D>,<R,L,U>> has the following properties: 
1. (g ⊃ L(g)) for every g∈Γ 
2. ∨,
g∈D(v)g ∨ (Xv=v)) for every v∈loc(V) 
3. (g ⊃τ(R(g)) for every g∈Γ, where τ is a translation that replaces any 
primed channel v’ by the term (Xv)
4. (GFU(g) ⊃GFg) for every g∈prv(Γ) 
The next step is to define a notion of refinement between specifications and de-
signs.  
A 
refinement 
of 
a 
specification 
S=<VS,ΓS,tvS>,ΦS> 
is 
a 
design 
P=<<VP,ΓP,tvP,ta,D>, ∆> and a morphism η:<VS,ΓS>→<VP,ΓP> of temporal signa-
tures s.t. 
1. tvS(v)=tvP(ηch(v))
2. ηch is injective 
3. Prop(P) |– η(ΦS)
That is to say, a design refines a specification iff the design has the properties  re-
quired by the specification.  The signature morphism identifies for each input (resp. 
output) channel of the specification the corresponding input (resp. output) channel of 
the design and , for each action g of the specification, the set of actions of the design 
that implements g.
Using this notion of refinement, we can introduce a notion of satisfaction relation 
between design morphisms and specification morphisms: 
Given a design morphism π:P→P’ in c-DSGN and a specifïcation morphism 
σ:S→S' in SPEC, π:P→P' |=σ:S→S' iff there exist refinements (P,η) of S and (P',η')
of S' s.t. η;π
*=σ
*;η', where π
*  and σ
* are the temporal signature morphisms induced 
by σ and π, respectively.
This satisfaction relation satisfies the conditions listed above. 
In order to illustrate the generalised notion of connector in this setting, we present 
below the connector cpipe,
where pipe is the design presented below.  
design pipe [t:sort, bound:nat] is
in   i:t, scl:bool 
out
o:t, eof:bool 
prv
rd: bool, b: list(t) 
do
put: true →  b:=b.i
 
[] prv
next: |b|>0∧¬rd → o:=head(b)||b:=tail(b)||rd:=true
 
[]    get: rd → rd:=false   
 
[] prv
sig: scl∧|b|=0 → eof:=true
sender
o
send
pipe
i
put
o
ge
receiver
i
rec
eof 
eof 
cl
eof
scl

212      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
This design, which is the glue of the connector, models a buffer with unlimited ca-
pacity and a FIFO discipline.  It signals the end of data to the consumer of messages 
as soon as the buffer gets empty and the sender of messages has already informed, 
through the input channel scl, that it will not send anymore messages. 
The two roles — the specifications sender and receiver introduced before— define 
the behaviour required of the components to which the connector cpipe can be ap-
plied.  It is interesting to notice that, due to the fact that LTL is more abstract than 
CommUnity, we were able to abstract away completely the production of messages in 
the role sender.  In the design of sender presented in 2.1 we had to consider an action 
modelling the production of messages.
We can now generalise these constructions even further by letting the connections 
use different specification formalisms and the instantiations to be performed over 
components designed in different design formalisms.  In this way, it will be possible 
to support the reuse of third-party components, namely legacy systems, as well as the 
integration of non-software components in systems, thus highlighting the role of 
architectures in promoting a structured and incremental approach to system construc-
tion and evolution. 
However, to be able to make sense of the interconnections, we have to admit that 
all the design formalisms are coordinated over the same category of signatures.  That 
is to say, we assume that the integration of heterogeneous components is made at the 
level of the coordination mechanisms, independently of the way each component 
brings about its computations. Hence, we will assume given
• a family {DSGNd}d:D of categories of designs, all of which are coordinated over the 
same category SIG via a family of functors  {dsgnd}d:D,
• a family {SPECc}c:C of categories of specifications together with a family 
{specc:SPECc→SIG}c:C of functors,  
• a family { |=r}r:R of satisfaction relations, each of which relates a design formalism 
d(r) and a specification formalism c(r).  We do not require R to be the cartesian 
product D×C, i.e. there may be pairs of design and specification formalisms for 
which no satisfaction relation is provided. 
Given such a setting, we generalise the notion of connector as follows: 
• A connection consists of 
- a design formalism DSGN and a specification formalism SPEC;
- a design G:DSGN and a specification R:SPEC, called the glue and the role of 
the connection, respectively; 
- a signature θ:SIG and two morphisms µ:dsgn(θ)→G,  σ:spec(θ)→R  in 
DSGN and SPEC, respectively, connecting the glue and the role via the sig-
nature (cable). 
• A connector is a finite set of connections with the same glue. 
• An instantiation of a connection with specification formalism SPEC, signature θ
and role morphism σ consists of 
- a design formalism DSGN’ and a satisfaction relation |= between  DSGN’ 
and SPEC, each from the corresponding family; 
- a design P and a design morphism π:dsgn(θ)→P in DSGN’ such that π |=σ.

A Mathematical Semantics for Architectural Connectors      213 
• An instantiation of a connector consists of an instantiation for each of its connec-
tions.
Given that we now have to deal with several design formalisms, providing a se-
mantics for the resulting configurations requires a homogeneous formalism to which 
all the design formalisms can be mapped.  Clearly, because we want the heterogene-
ity of formalisms to be carried through to the implementations in order to be able to 
support the integration of legacy code, third-party components and even non-software 
components, this common formalism cannot be at the same level as designs and the 
mapping cannot be a translation.  What seems to make more sense is to choose a 
behaviour model that can be used to provide a common semantics to all the design 
formalisms so that the integration is not performed at the level of the descriptions but 
of the behaviours that are generated from the descriptions.   Indeed, when we talk 
about the integration of heterogeneous components, our goal is to coordinate their 
individual behaviours.  An architecture should provide precisely the mechanisms 
through which this coordination is effected. 
We have already mentioned that concurrency models can be formalised in the 
categorical framework that we have described.  See [28] for several examples.  In 
particular, a very simple, trace-based behaviour model that is, in some sense, minimal 
can be found in [13].  This model should provide a common formalism for integra-
tion. 
5 
Towards an Algebra of Connectors 
It is not always possible to adapt components to work with the existing connectors.  
Even in those cases where it is feasible, a better alternative may be to modify the 
connectors because, usually, there are fewer connector types than components types.  
Moreover, most ADLs either provide a fixed set of connectors or only allow the crea-
tion of new ones from scratch, hence requiring from the designer a deep knowledge 
of the particular formalism and tools at hand.  Conceptually, operations on connectors 
allow one to factor out common properties for reuse and to better understand the 
relationships between different connector types.   
The notation and semantics of such connector operators are, of course, among the 
main issues to be dealt with.  Our purpose in this section is to show how typical op-
erators can be given an ADL-independent semantics by formalising them in the cate-
gorical framework that we presented in the previous sections.  An example of such an 
operator was already given in section 3.  We saw how, given a connector expressed 
through a configuration diagram dia
G
θ1
θ2
dia
R1
R2
and a refinement η:G→G’ of its glue, we can construct through  dia+η another 
connector that has the same roles as the original one, but whose glue is now G’.

214      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
G’
θ1
θ2
dia+η
R1
R2
A fundamental property of this construction, given by compositionality, is that the 
semantics of the original connector, as expressed by the colimit of its diagram, is 
preserved in the sense that it is refined by the semantics of the new connector.  This 
means that all instantiations of the new connector are refinements of instantiations of 
the old one.  This operation supports the definition of connectors at higher levels of 
abstraction by delaying decisions on concrete representations of the coordination 
mechanisms that they offer, thus providing for the definition of specialisation hierar-
chies of connector types. 
In the rest of this section we present three connector transformations that operate 
on the roles rather than the glue.  Transformations that, like above, operate at the 
level of the glue are more sensitive in that they interfere more directly with the se-
mantics of the connector to which they are applied.  Hence, they should be restricted 
to engineers who have the power, and ensuing responsabilities, to change the way 
connectors are implemented.  Operations on the roles are less critical and can be per-
formed more liberally by users who have no access to the implementation (glue). 
5.1 
Role Refinement 
To tailor general-purpose connectors for a specific application, it is necessary to re-
place the generic roles by specialised ones that can effectively act as “formal parame-
ters” for the application at hand.  Role replacement is done in the same way as apply-
ing a connector to components: there must be a refinement morphism from the 
generic role to the specialised one.  The old role is cancelled, and the new role mor-
phism is the composition of the old one with the refinement morphism in the sense 
discussed in section 3. 
Given an n-ary connector 
dia
G
θ1
θi
θn
R1
 Ri
Rn
and a refinement morphism ηi:Ri→ R’i for some 0 < i ≤ n, the role refinement opera-
tion yields the connector 

A Mathematical Semantics for Architectural Connectors      215 
dia +ηi
G
θ1
θi
θn
R1
 R’i
Rn
Notice that this operation can be applied to both abstract and heterogeneous con-
nectors in the sense of section 5. 
This operation preserves the semantics of the connectors to which it is applied in 
the sense that any instantiation of the refined connector is also an instantiation of the 
refined one.  This is because the refinement morphism that instantiates R’i can be 
composed with ηi to yield an instantiation of Ri.
As an example of role refinement, consider the asynchronous connector shown 
previously.  This connector is too general for our luggage distribution service because 
the sender and receiver roles do not impose any constraints on the admissible in-
stances.  We would like to refine these roles in order to prevent meaningless applica-
tions to our example like sending the location of the check-in as a bag to the cart. 
 
design cart-role is 
 
 
 
 
out  obag: int  
 
 
 
 
 
prv  dest : -1..U – 1  
 
 
 
do
 get[obag]: dest = -1 →  dest’ > -1 
 
 
 
 
[]   put: dest>-1,false →  obag := 0 || dest := -1 
with channel rd of the sender refined by the term dest ≠ –1. The resulting connector is 
Notice that the invalid combinations are not possible because cart-role cannot be 
refined with a gate or a check-in, nor is it possible to refine gate-role with a cart or a 
sender[int]
o
sen
buffer[int,1]
i
put
get
receiver[int]
i
rec
pro
cart-role
put
get
obag
o
cart-role
buffer[int,1]
i
put
get
receiver[int]
i
rec
put
get
obag
o

216      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
check-in. Moreover, the obag channel of cart-role  cannot be refined by channel laps
of cart. 
5.2 
Role Encapsulation 
To prevent a role from being further refined, the second operation we consider, when 
executed repeatedly, decreases the arity of a connector by encapsulating some of its 
roles, making the result part of the glue. This operation requires that the glue and the 
encapsulated role be in the same formalism.  
Given an n-ary connector 
G
θ1
θi
θn
R1
 Ri
Rn
the encapsulation of the i-th role is performed as follows: the pushout of the i-th con-
nection is calculated, and the other connections are changed by composing the mor-
phisms that connect the cables to the glue with the morphism that connects the glue 
with the apex of the pushout,  yielding a connector of arity n–1. 
G*
G
θ1
θi
θn
R1
 Ri
Rn
For instance, we can obtain the action subsumption connector from the action ramifi-
cation one  through encapsulation of the right-hand side action role: 
action
ramify
branch1
action
a
a
branch2

A Mathematical Semantics for Architectural Connectors      217 
We can also obtain a synchronisation connector through refinement of one role with 
the inhibit program 
and then encapsulating it 
 
design sync-2 is
 
do  branch1: true →•skip
 
[]  never2: false →  skip
Notice that the synchronisation connector obtained is not syntactically equal to the 
one presented in section 3.2, but it is “equivalent” because one of the ramifications of 
the bottom action a is never executed (because it is synchronized with never2) and 
thus the net effect of the connector is to synchronise both actions a through branch1.
If we now refine the left-hand branch also with inhibit
and then encapsulate it, we obtain a connector that is equivalent to inhibit.
action
ramify
branch2
action
branch1
action
inhibit
never
action
sync-2
never2
branch1
action
inhibit
action
sync-2
never2
branch1
action
never
a
a
a
a
a
a
a

218      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
design inhibit-2 is
 
do
never1: false →  skip
 
[]  never2: false →  skip
Hence ramification can be seen as a primitive connector. 
5.3 
Role Overlay 
The third operation allows combining several connectors into a single one if they 
have some roles in common, i.e., if there is an isomorphism between those roles.   
The construction is as follows. 
Consider an n-ary connector with roles Ri and glue G, and an m-ary connector 
with roles R’k and glue G’.  The glue of the new connector is calculated as follows.  
Consider the diagram that consists of all the connections that have isomorphic roles 
together with the isomorphisms, and calculate its colimit.  The apex of the colimit is 
the new glue.  Each of the pairs of connections involved gives rise to a connection of 
the new connector.  The role of this connection is one of the roles of the old connec-
tions; because they are isomorphic, it does not matter which one is chosen (in the 
figure, we chose R’k).  This role is connected directly to the new glue through one of 
the morphisms that results from the colimit; hence, its cable is the role signature. 
G
θj
θi
Rj
Ri
G*
R’k
θ’k
G’
inhbit-2
never2
never1
action
a

A Mathematical Semantics for Architectural Connectors      219 
Each of the connections of the original connectors that is not involved in the calcula-
tion of the new glue, which means that it does not share its role with a connection of 
the other connector, becomes a connection of the new connector by composing the 
old new glue morphism with the colimit morphism that connects the old glue to the 
new one.  This is exemplified in the figure with the connection with role Rj.
This operation provides a second way of showing that synchronisation is not a 
primitive connector in our catalogue.  We can indeed obtain full synchronisation of 
two actions by making each one subsume the other.  This is achieved by overlaying 
two copies of the subsumption connector in a symmetric way: the first (resp. second) 
role of one copy corresponds to the second (resp. first) role of the other copy. The 
diagram is 
and its colimit makes all actions collapse into a single one. Hence the glue of the 
resulting connector is isomorphic to the sync component. Moreover, each pair of 
overlaid roles results into a single one, and therefore there will be only two action
roles. In summary, the resulting connector is the synchronisation connector. 
6 
Concluding Remarks 
In this paper, we have shown how (elementary) concepts of Category Theory can be 
used to formalise key notions of Software Architecture independently of the language 
chosen for describing the behaviour of components, thus providing a semantic do-
main for Software Architectures that complements the choice of a computational 
model. As illustrated by the proposed category of CommUnity designs, the categor-
ical “style” favours the choice of formalisms in which the process of interconnecting 
components in configurations of complex systems is “separated” from the description 
of the internal computations that they perform.  Therefore, it supports an approach to 
software development that is very close to what has been proposed in the area of 
Coordination Languages and Models [15]. 
The independence that was achieved relative to the choice of specific Architecture 
Desciption Language was taken one step further.  We showed how the proposed 
mathematical framework supports generalisations of the notion of architectural con-
nector and corresponding instantiation mechanisms that address important issues like 
stepwise development and compositional evolution of systems driven by architec-
tures.  The wealth of possibilities that this formal approach has opened to Software 
Architectures is being exploited in a “real-world” environment, namely through the 
action
subsume
action
action
subsume
action
a
free
sync
free
a
a
a

220      J.L. Fiadeiro, A. Lopes, and M. Wermelinger 
extension of object-oriented and component-based modelling techniques.  This effort 
is being pursued as a response from industrial needs for added flexibility in the way 
we structure our systems to facilitate the reuse of legacy systems and the dynamic 
reconfiguration of systems in a way that is compositional with the evolution of the 
business domains.  The progress that has been made in this front is reported in a 
companion paper [3]. 
Further work is in progress at the more “theoretical level”.  These address  the  
support that is required for an architecture-driven process of reconfiguration in which 
connectors can be added, deleted or replaced in run-time.  Preliminary work in this 
direction is reported in [31,33].  We are also working on a notion of high-order con-
nector through which we can integrate non-functional aspects like security and 
fault-tolerance into functional architectures.  Work in this direction is reported in 
[24,32].  Finally, we intend to develop a logical calculus that supports the analysis of 
architectures, both in their static and dynamic aspects, including mechanisms for 
reasoning about possible interactions between connectors when applied simultane-
ously to a given system. 
Acknowledgments
We would like to thank Luís Andrade from ATX Software SA for the opportunity to 
test and revise our theory against the practice of architecture building in the 
“real-world”.   We would also like to thank Roland Backhouse and Jeremy Gibbons 
for the opportunity to lecture this material at the Summer School on Generic Pro-
gramming 2002, and the participants for much helpful feedback. 
References
1. R.Allen and D.Garlan, “A Formal Basis for Architectural Connectors”, ACM TOSEM, 6(3),
1997, 213-249. 
2. L.F.Andrade and J.L.Fiadeiro, “Interconnecting Objects via Contracts”, in R.France and 
B.Rumpe (eds), UML’99 – Beyond the Standard, LNCS 1723, Springer Verlag 1999, 
566-583.
3. L.Andrade and J.L.Fiadeiro, “Service-Oriented Business and System Specification: Beyond 
Object-orientation”, in H.Kilov and K. Baclwaski (eds), Practical Foundations of Business 
and System Specifications, Kluwer Academic Publishers 2003, 1-23.
4. L.F.Andrade, J.L.Fiadeiro, J.Gouveia, A.Lopes and M.Wermelinger, “Patterns for Coordi-
nation”, in COORDINATION’00, G.Catalin-Roman and A.Porto (eds), LNCS 1906, 
Springer-Verlag 2000, 317-322. 
5. J.P.Banâtre and D.Le Métayer, “Programming by Multiset Transformation”, Communica-
tions ACM 16(1), 1993, 55-77. 
6. L.Bass, P.Clements and R.Kasman, Software Architecture in Practice, Addison Wesley 
1998.
7. J.Bosch, “Superimposition: A Component Adaptation Technique”, Information and Soft-
ware Technology 1999. 
8. K.Chandy and J.Misra, Parallel Program Design - A Foundation, Addison-Wesley 1988. 
9. J.L.Fiadeiro and T.Maibaum, “Interconnecting Formalisms: supporting modularity, reuse 
and incrementality”, in G.E.Kaiser (ed), Proc. 3rd Symp. on Foundations of Software Engi-
neering, ACM Press 1995, 72-80. 

A Mathematical Semantics for Architectural Connectors      221 
10. J.L.Fiadeiro and T.Maibaum, “A Mathematical Toolbox for the Software Architect”, in 
J.Kramer and A.Wolf (eds), Proc. 8th International Workshop on Software Specification 
and Design, IEEE Computer Society Press 1996, 46-55. 
11. J.L.Fiadeiro and T.Maibaum, “Categorical Semantics of Parallel Program Design”, Science
of Computer Programming 28, 1997, 111-138. 
12. J.L.Fiadeiro and A.Lopes, “Semantics of Architectural Connectors”, in TAPSOFT’97,
LNCS 1214, Springer-Verlag 1997, 505-519. 
13. J.L.Fiadeiro and A.Lopes, “Algebraic Semantics of Coordination, or what is in a signa-
ture?”, in A.Haeberer (ed), AMAST’98, LNCS 1548, Springer-Verlag 1999. 
14. N.Francez and I.Forman, Interacting Processes, Addison-Wesley 1996. 
15. D.Gelernter and N.Carriero, “Coordination Languages and their Significance”, Communi-
cations ACM 35(2), 1992, 97-107. 
16. J.Goguen, “Categorical Foundations for General Systems Theory”, in F.Pichler and 
R.Trappl (eds), Advances in Cybernetics and Systems Research, Transcripta Books 1973, 
121-130.
17. J.Goguen, “Principles of Parametrised Programming”, in Biggerstaff and Perlis (eds), Soft-
ware Reusability, Addison-Wesley 1989, 159-225. 
18. J.Goguen, “Parametrised Programming and Software Architecture”, in Symposium on Soft-
ware Reusability, IEEE 1996. 
19. J.Goguen and R.Burstall, “Institutions: Abstract Model Theory for Specification and Pro-
gramming”, Journal of the ACM 39(1), 1992, 95-146. 
20. S.Katz, “A Superimposition Control Construct for Distributed Systems”, ACM TOPLAS 
15(2), 1993, 337-356. 
21. A.Lopes, “Não-determinismo e Composicionalidade na Especificação de Sistemas Reac-
tivos”, PhD Thesis (in Portuguese), Universidade de Lisboa, Jan. 1999. 
22. A.Lopes and J.L.Fiadeiro, ‘‘Using explicit state to describe architectures”, in E. Astesiano 
(ed), FASE’99, LNCS 1577, Springer-Verlag 1999, 144–160.
23. A.Lopes and J.L.Fiadeiro, “Revising the Categorical Approach to Systems”, in H.Kirchner 
and C.Ringeiseen (eds), AMAST’02, LNCS 2422, Springer-Verlag 2002, 426-440.
24. A.Lopes. M.Wermelinger and J.L.Fiadeiro, “Compositional Approach to Connector Con-
struction”, in M.Cerioli and G.Reggio (eds), Recent Trends in Algebraic Development 
Techniques, LNCS 2267, Springer-Verlag 2002, 201-220. 
25. J.Magee, J.Kramer and M.Sloman, “Constructing Distributed Systems in Conic”, IEEE
TOSE 15 (6), 1989. 
26. M.Moriconi and X.Qian, “Correctness and Composition of Software Architectures”, in 
Proc. Second Symposium on the Foundations of Software Engineering, ACM Press 1994, 
164-174.
27. D.Perry and A.Wolf, “Foundations for the Study of Software Architectures”, ACM
SIGSOFT Software Engineering Notes 17(4), 1992, 40-52. 
28. V.Sassone, M.Nielsen and G.Winskel, “A Classification of Models for Concurrency”, in 
E.Best (ed), CONCUR’93, LNCS 715, Springer-Verlag, 82-96. 
29. M.Wermelinger and J.L.Fiadeiro, “Connectors for Mobile Programs”, IEEE Transactions 
on Software Engineering 24(5), 1998, 331-341. 
30. M.Wermelinger and J.L.Fiadeiro, “Towards an Algebra of Architectural Connectors: a 
Case Study on Synchronisation for Mobility”, in Proc. 9th International Workshop on 
Software Specification and Design, IEEE Computer Society Press 1998, 135-142. 
31. M.Wermelinger and J.L.Fiadeiro, “Algebraic Software Architecture Reconfiguration”, in 
Software Engineering – ESEC/FSE'99, LNCS 1687, Springer-Verlag 1999, 393-409. 
32. M.Wermelinger, A.Lopes and J.L.Fiadeiro, “Superposing Connectors”, in Proc. 10th Inter-
national Workshop on Software Specification and Design, IEEE Computer Society Press 
2000, 87-94. 
33. M.Wermelinger, 
A.Lopes 
and 
J.L.Fiadeiro, 
“A 
Graph 
Based 
Architectural 
(Re)configuration Language”, in ESEC/FSE’01, V.Gruhn (ed), ACM Press 2001, 21-32. 

Author Index
Backhouse, R.
97
Crole, R.L.
133
Fiadeiro, J.L.
178
Hinze, R.
1, 57
Hoogendijk, P.
97
Jeuring, J.
1, 57
Lopes, A.
178
Wermelinger, M.
178


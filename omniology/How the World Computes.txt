
Lecture Notes in Computer Science
7318
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Alfred Kobsa
University of California, Irvine, CA, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Germany
Madhu Sudan
Microsoft Research, Cambridge, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbruecken, Germany

S. Barry Cooper Anuj Dawar
Benedikt Löwe (Eds.)
HowtheWorldComputes
Turing Centenary Conference and
8th Conference on Computability in Europe, CiE 2012
Cambridge, UK, June 18-23, 2012
Proceedings
1 3

Volume Editors
S. Barry Cooper
University of Leeds, School of Mathematics
Leeds LS2 9JT, UK
E-mail: s.b.cooper@leeds.ac.uk
Anuj Dawar
University of Cambridge, Computer Laboratory
William Gates Building, J.J. Thomson Avenue
Cambridge CB3 0FD, UK
E-mail: anuj.dawar@cl.cam.ac.uk
Benedikt Löwe
University of Amsterdam, Institute for Logic, Language and Computation
P.O. Box 94242, 1090 GE Amsterdam, The Netherlands
E-mail: b.loewe@uva.nl
ISSN 0302-9743
e-ISSN 1611-3349
ISBN 978-3-642-30869-7
e-ISBN 978-3-642-30870-3
DOI 10.1007/978-3-642-30870-3
Springer Heidelberg Dordrecht London New York
Library of Congress Control Number: 2012938860
CR Subject Classiﬁcation (1998): F.2, F.1, G.2, I.1
LNCS Sublibrary: SL 1 – Theoretical Computer Science and General Issues
© Springer-Verlag Berlin Heidelberg 2012
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
CiE 2012: How the World Computes, and the Alan Turing Centenary
In 2003, a group of researchers started their funding proposal for a Research
Training Network of the European Commission under the unfamiliar acronym
CiE with the words:
We can see now that the world changed in 1936, in a way quite unrelated
to the newspaper headlines of that year concerned with such things as
the civil war in Spain, economic recession, and the Berlin Olympics. The
end of that year saw the publication of a thirty-six page paper by a young
mathematician, Alan Turing, claiming to solve a long-standing problem
of the distinguished German mathematician David Hilbert.
The proposal (eventually unfunded) went on to describe how “computability
as a theory is a speciﬁcally twentieth-century development,” and how subse-
quently computability became “both a driving force in our daily lives” and a
concept one could talk about with a new precision.
The work and personality of Turing has had a key inﬂuence on the devel-
opment of CiE as an important player in the new multi-disciplinary landscape
of twenty-ﬁrst century computability theory. It has been said that Alan Turing
did not so much inhabit diﬀerent disciplines as investigate one discipline that
was fundamental. In recent times, CiE has sought to break down disciplinary
barriers, and to sponsor a return to this questioning approach of Turing to
“How the World Computes.”
Computability in Europe (CiE) is now a hugely successful conference series;
an association with close on a thousand members, and growing; has its own
journal Computability; and edits a high-proﬁle book series Theory and Appli-
cations of Computability. It was just under ﬁve years ago, toward the end of
2007, that the CiE Board set out to acknowledge its debt to Turing by mak-
ing the Alan Turing Year 2012 a unique and scientiﬁcally exciting year. Since

VI
Preface
then, CiE has formed an international Turing Centenary Advisory Committee,
which has played an important part in the development of the Turing Cente-
nary programme into a world-wide celebration. The CiE 2012 Turing Centenary
Conference will be remembered as a historic event in the continuing develop-
ment of the powerful explanatory role of computability across a wide spectrum
of research areas. We believe that the work presented at CiE 2012 represents the
best of current research in the area, and forms a ﬁtting tribute to the short but
brilliant trajectory of Alan Mathison Turing (June 23, 1912 – June 7, 1954).
Apart from being a celebration of the Turing Centenary, CiE 2012 was the
eighth meeting in our conference series. Both the conference series and the asso-
ciation promote the development of computability-related science, ranging over
mathematics, computer science and applications in various natural and engi-
neering sciences such as physics and biology, and also including the promotion
of related non-scientiﬁc ﬁelds such as philosophy and history of computing. This
conference was held at the University of Cambridge in England, linking natu-
rally to the semester-long research programme Semantics & Syntax at the Isaac
Newton Institute for Mathematical Sciences.
The ﬁrst seven of the CiE conferences were held at the University of Ams-
terdam in 2005, at the University of Wales Swansea in 2006, at the University
of Siena in 2007, at the University of Athens in 2008, at the University of Hei-
delberg in 2009, at the University of the Azores in 2010, and at Soﬁa University
in 2011. The proceedings of these meetings, edited in 2005 by S. Barry Cooper,
Benedikt L¨owe and Leen Torenvliet, in 2006 by Arnold Beckmann, Ulrich Berger,
Benedikt L¨owe and John V. Tucker, in 2007 by S. Barry Cooper, Benedikt L¨owe
and Andrea Sorbi, in 2008 by Arnold Beckmann, Costas Dimitracopoulos and
Benedikt L¨owe, in 2009 by Klaus Ambos-Spies, Benedikt L¨owe and Wolfgang
Merkle, in 2010 by Fernando Ferreira, Benedikt L¨owe, Elvira Mayordomo and
Lu´ıs Mendes Gomes, and in 2011 by Benedikt L¨owe, Dag Normann, Ivan Soskov,

Preface
VII
and Alexandra Soskova were published as Springer Lecture Notes in Computer
Science, Volumes 3526, 3988, 4497, 5028, 5365, 6158, and 6735, respectively.
The annual CiE conference has become a major event, and is the largest
international meeting focused on computability theoretic issues. The next meet-
ing in 2013 will be in Milan, Italy, and will be co-located with the conference
UCNC 2013 (Unconventional Computation and Natural Computation). The se-
ries is coordinated by the CiE Conference Series Steering Committee consisting of
Lu´ıs Antunes (Porto, Secretary), Arnold Beckmann (Swansea), S. Barry Cooper
(Leeds), Natasha Jonoska (Tampa FL), Viv Kendon (Leeds), Benedikt L¨owe
(Amsterdam and Hamburg, Chair), Dag Normann (Oslo), and Peter van Emde
Boas (Amsterdam).
The Programme Committee of CiE 2012 was responsible for the selection
of the invited speakers and special session organizers and consisted of Sam-
son Abramsky (Oxford), Pieter Adriaans (Amsterdam), Franz Baader (Dres-
den), Arnold Beckmann (Swansea), Mark Bishop (London), Paola Bonizzoni
(Milan), Douglas A. Cenzer (Gainesville FL), S. Barry Cooper (Leeds), Ann
Copestake (Cambridge), Anuj Dawar (Cambridge), Solomon Feferman (Stanford
CA), Bernold Fiedler (Berlin), Luciano Floridi (Oxford), Marcus Hutter (Can-
berra), Martin Hyland (Cambridge), Viv Kendon (Leeds), Stephan Kreutzer
(Berlin), Ming Li (Waterloo ON), Benedikt L¨owe (Amsterdam and Hamburg),
Angus Macintyre (Edinburgh), Philip Maini (Oxford), Larry Moss (Bloomington
IN), Amitabha Mukerjee (Kanpur), Damian Niwinski (Warsaw), Dag Normann
(Oslo), Prakash Panangaden (Montr´eal QC), JeﬀParis (Manchester), Brigitte
Pientka (Montr´eal QC), Helmut Schwichtenberg (Munich), Wilfried Sieg (Pitts-
burgh PA), Mariya Soskova (Soﬁa), Bettina Speckmann (Eindhoven), Christof
Teuscher (Portland OR), Peter Van Emde Boas (Amsterdam), Jan Van Leeuwen
(Utrecht), Rineke Verbrugge (Groningen).
Structure and Programme of the Conference
The conference had 12 invited plenary lectures, given by Dorit Aharonov (Jeru-
salem), Ver´onica Becher (Buenos Aires), Lenore Blum (Pittsburgh PA), Rodney
Downey (Wellington), Yuri Gurevich (Redmond WA), Juris Hartmanis (Ithaca
NY), Richard Jozsa (Cambridge), Stuart Kauﬀman (Santa Fe NM), James D.
Murray (Princeton NJ), Stuart Shieber (Cambridge MA), Paul Smolensky (Bal-
timore MD), and Leslie Valiant (Cambridge MA). Six of these plenary lectures
have abstracts in this volume. Blum’s lecture was the 2012 APAL Lecture funded
by Elsevier, Gurevich’s lecture was the EACSL Invited Lecture funded by the
European Association for Computer Science Logic, Murray’s lecture was the
Microsoft Research Cambridge Lecture funded by Microsoft Research, and the
lectures by Josza and Valiant were part of a joint event with King’s College,
Cambridge, on Alan Turing’s 100th birthday (June 23, 2012). In addition to
the plenary lectures, the conference had two public evening lectures delivered by
Andrew Hodges (Oxford) and Ian Stewart (Warwick).
The 2012 conference CiE had six special session on a range of topics. Speakers
in the special sessions were invited by the special session organizers and could

VIII
Preface
contribute a paper to this volume. Eighteen of the invited special session speakers
made use of this opportunity and their papers are included in these proceedings.
Cryptography, Complexity, and Randomness.
Organizers. Rod Downey (Wellington) and Jack Lutz (Ames IA).
Speakers. Eric Allender, Laurent Bienvenu, Lance Fortnow, Valentine Ka-
banets, Omer Reingold, and Alexander Shen.
The Turing Test and Thinking Machines.
Organizers. Mark Bishop (London) and Rineke Verbrugge (Groningen).
Speakers. Bruce Edmonds, John Preston, Susan Sterrett, Kevin Warwick,
and Jiˇr´ı Wiedermann.
Computational Models After Turing: The Church-Turing Thesis and
Beyond.
Organizers. Martin Davis (Berkeley CA) and Wilfried Sieg (Pittsburg PA).
Speakers. Giuseppe Longo, P´eter N´emeti, Stewart Shapiro, Matthew Szudzik,
Philip Welch, and Michiel van Lambalgen.
Morphogenesis/Emergence as a Computability Theoretic
Phenomenon.
Organizers. Philip Maini (Oxford) and Peter Sloot (Amsterdam).
Speakers. Jaap Kaandorp, Shigeru Kondo, Nick Monk, John Reinitz, James
Sharpe, and Jonathan Sherratt.
Open Problems in the Philosophy of Information.
Organizers. Pieter Adriaans (Amsterdam) and Benedikt L¨owe (Amsterdam
and Hamburg).
Speakers. Patrick Allo, Lu´ıs Antunes, Mark Finlayson, Amos Golan, and
Ruth Millikan.
The Universal Turing Machine, and History of the Computer.
Organizers. Jack Copeland (Canterbury) and John Tucker (Swansea).
Speakers. Steven Ericsson-Zenith, Ivor Grattan-Guinness, Mark Priestley,
and Robert I. Soare.
CiE 2012 received 178 regular submissions. These were refereed by the Pro-
gramme Committee and a long list of expert referees without whom the produc-
tion of this volume would have been impossible. Based on their reviews, 53 of
the submissions (29.8%) were accepted for publication in this volume. We should
like to thank the subreferees for their excellent work; their names are listed at
the end of this preface.
Organization and Acknowledgements
CiE 2012 was organized by Arnold Beckmann (Swansea), Luca Cardelli (Cam-
bridge), S. Barry Cooper (Leeds), Ann Copestake (Cambridge), Anuj Dawar
(Cambridge, Chair), Bjarki Holm (Cambridge), Martin Hyland (Cambridge),
Benedikt L¨owe (Amsterdam), Arno Pauly (Cambridge), Debbie Peterson (Cam-
bridge), Andrew Pitts (Cambridge), and Helen Scarborough (Cambridge).
At the conference, we were able to continue the programme“Women in Com-
putability” funded by the publisher Elsevier and organized by Mariya Soskova

Preface
IX
(Soﬁa). There were ﬁve co-located events: the workshop The Incomputable or-
ganized as part of the Isaac Newton Institute program Semantics & Syntax
at Chicheley Hall (June 12–15, 2012; organizers: S. Barry Cooper and Mariya
Soskova), the conference ACE 2012 at King’s College (June 15–16, 2012; or-
ganized by Jack Copeland and Mark Sprevak), the workshop Developments in
Computational Models (DCM 2012) at Corpus Christi College (17 June 2012;
organized by Benedikt L¨owe and Glynn Winskel), the CiE-IFCoLog student ses-
sion (June 18–23, 2012; organizers: Sandra Alves and Michael Gabbay), and the
conference Computability and Complexity in Analysis (CCA 2012) at the Com-
puter Lab of the University of Cambridge (June 24–27, 2012; organizers: Arno
Pauly and Klaus Weihrauch).
The organization of CiE 2012 would not have been possible without the
ﬁnancial and/or materials support of our sponsors (in alphabetic order): the As-
sociation for Symbolic Logic, Cambridge University Press, Elsevier B.V.,
the European Association for Computer Science Logic (EACSL), the Interna-
tional Federation for Computational Logic (IFCoLog), IOS Press, the Isaac New-
ton Institute for Mathematical Sciences, King’s College, Cambridge, Microsoft
Research Cambridge, Robinson College Cambridge, Science Magazine/AAAS,
Springer-Verlag, and the University of Cambridge.
We should also like to acknowledge the support of our non-ﬁnancial spon-
sors, the Association Computability in Europe and the European Association
for Theoretical Computer Science (EATCS).
We thank Andrej Voronkov for his EasyChair system which facilitated the
work of the Programme Committee and the editors considerably. The ﬁnal prepa-
ration of the ﬁles for this proceedings volume was done by Bjarki Holm, Steﬀen
L¨osch, and Nik Sultana.
April 2012
Anuj Dawar
S. Barry Cooper
Benedikt L¨owe

Reviewers
Accatoli, Beniamino
Al-Rifaie, Mohammad
Majid
Altenkirch, Thorsten
Ambainis, Andris
An, Hyung-Chan
Angione, Claudio
Arai, Toshiyasu
Arvind, Vikraman
Avigad, Jeremy
Bab, Sebastian
Barany, Vince
Barendregt, Henk
Barmpalias, Georgios
Barr, Katie
Beeson, Michael
Beklemishev, Lev
Bernardinello, Luca
Besozzi, Daniela
Bienvenu, Laurent
Binns, Stephen
Blass, Andreas
Bloem, Peter
Blokpoel, Mark
Bodlaender, Hans
Bradﬁeld, Julian
Brattka, Vasco
Braud, Laurent
Braverman, Mark
Bridges, Douglas
Briet, Jop
Carayol, Arnaud
Carlucci, Lorenzo
Cassaigne, Julien
Castiglione, Giuseppa
Cervesato, Iliano
Cook, Stephen
Coquand, Thierry
Daswani, Mayank
Davenport, James
Davis, Martin
Day, Adam
De Mol, Liesbeth
De Weerd, Harmen
Dechesne, Francien
Decker, Hendrik
Delhomme, Christian
Della Vedova, Gianluca
Dennunzio, Alberto
Diener, Hannes
Dimitracopoulos, Costas
Doty, David
Duparc, Jacques
Dziubi´nski, Marcin
Debowski, Lukasz
Endriss, Ulle
Evans, Roger
Everitt, Mark
Feige, Uriel
Ferreira, Fernando
Fici, Gabriele
Fokina, Ekaterina
Friis, Nicolai
Galliani, Pietro
Ganchev, Hristo
Ganesalingam, Mohan
Gasarch, William
Gentile, Claudio
Gherardi, Guido
Goncharov, Sergey
Goodman-Strauss,
Chaim
Gundersen, Tom
Gutin, Gregory
Hamkins, Joel David
Harizanov, Valentina
Hauser, Marcus
Herbelot, Aurelie
Hertling, Peter
Hindley, Roger
Hinman, Peter
Hinzen, Wolfram
Hirst, Jeﬀ
Holden, Sean
Horsman, Clare
Jockusch, Carl
Johannsen, Jan
Kach, Asher
Kahle, Reinhard
Kapron, Bruce
Kari, Jarkko
Kari, Lila
Khomskii, Yurii
Klin, Bartek
Kobayashi, Satoshi
Kolodziejczyk, Leszek
Kopczynski, Eryk
Kopecki, Steﬀen
Kracht, Marcus
Krajicek, Jan
Kristiansen, Lars
K¨uhnberger, Kai-Uwe
Kugel, Peter
Lasota, Slawomir
Lauria, Massimo
Leporati, Alberto
Lesnik, Davorin
Lewis, Andy
Le´on, Carlos
Li, Zhenhao
Lippmann, Marcel
Luttik, Bas
L¨oding, Christof
Malone, David
Manea, Florin
Marion, Jean-Yves
Martin, Andrew

XII
Reviewers
Meyerovitch, Tom
Michalewski, Henryk
Minari, Pierluigi
Minnes, Mia
Molina-Paris, Carmen
Montalban, Antonio
Moore, Cristopher
Morphett, Anthony
Mumma, John
Mummert, Carl
Murlak, Filip
Murray, Iain
Mycka, Jerzy
Nemoto, Takako
Ng, Selwyn
Novakovic, Novak
Oitavem, Isabel
Pattinson, Dirk
Pauly, Arno
Pin, Jean-Eric
Pradella, Matteo
Primiero, Giuseppe
Radoszewski, Jakub
Rampersad, Narad
Razborov, Alexander
Reimann, Jan
Richomme, Gw´ena¨el
Rimell, Laura
Rubin, Sasha
Ruemmer, Philipp
Salibra, Antonino
Sanders, Sam
Sato, Masahiko
Sattler, Ulrike
Schlage-Puchta,
Jan-Christoph
Schlimm, Dirk
Schneider, Thomas
Schraudner, Michael
Selivanov, Victor
Semukhin, Pavel
Seyﬀerth, Benjamin
Shafer, Paul
Shao, Wen
Simonsen, Jakob Grue
Simpson, Stephen
Skrzypczak, Michal
Smets, Sonja
Soloviev, Serguei
Sorbi, Andrea
Soskov, Ivan
Spandl, Christoph
Staiger, Ludwig
Stannett, Mike
Stephan, Frank
Straube, Ronny
Sturm, Monika
Sutskever, Ilya
Szudzik, Matthew
Szymanik, Jakub
Slomczy´nska, Katarzyna
Tait, William
Tao, Terence
Tazari, Siamak
Terwijn, Sebastiaan
Tesei, Luca
Teufel, Simone
Thorne, Camilo
Torenvliet, Leen
Toru´nczyk, Szymon
Toska, Ferit
Trautteur, Giuseppe
Trifonov, Trifon
Urquhart, Alasdair
van Kreveld, Marc
Vellino, Andre
Vencovska, Alena
Verlan, Sergey
Visser, Albert
Vogt, Paul
Vosgerau, Gottfried
V¨a¨an¨anen, Jouko
Wansing, Heinrich
Welch, Philip
Wiedermann, Jiˇri
Wiedijk, Freek
Wu, Guohua
Yang, Yue
Zdanowski, Konrad
Zeilberger, Noam

Table of Contents
Ordinal Analysis and the Inﬁnite Ramsey Theorem. . . . . . . . . . . . . . . . . . .
1
Bahareh Afshari and Michael Rathjen
Curiouser and Curiouser: The Link between Incompressibility and
Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
Eric Allender
Information and Logical Discrimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Patrick Allo
Robustness of Logical Depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
Lu´ıs Antunes, Andre Souto, and Andreia Teixeira
Turing’s Normal Numbers: Towards Randomness. . . . . . . . . . . . . . . . . . . . .
35
Ver´onica Becher
Logic of Ruler and Compass Constructions . . . . . . . . . . . . . . . . . . . . . . . . . .
46
Michael Beeson
On the Computational Content of the Brouwer Fixed Point Theorem . . .
56
Vasco Brattka, St´ephane Le Roux, and Arno Pauly
Square Roots and Powers in Constructive Banach Algebra Theory . . . . .
68
Douglas S. Bridges and Robin S. Havea
The Mate-in-n Problem of Inﬁnite Chess Is Decidable . . . . . . . . . . . . . . . .
78
Dan Brumleve, Joel David Hamkins, and Philipp Schlicht
A Note on Ramsey Theorems and Turing Jumps . . . . . . . . . . . . . . . . . . . . .
89
Lorenzo Carlucci and Konrad Zdanowski
Automatic Functions, Linear Time and Learning . . . . . . . . . . . . . . . . . . . . .
96
John Case, Sanjay Jain, Samuel Seah, and Frank Stephan
An Undecidable Nested Recurrence Relation. . . . . . . . . . . . . . . . . . . . . . . . .
107
Marcel Celaya and Frank Ruskey
Hard Instances of Algorithms and Proof Systems . . . . . . . . . . . . . . . . . . . . .
118
Yijia Chen, J¨org Flum, and Moritz M¨uller
On Mathias Generic Sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
Peter A. Cholak, Damir D. Dzhafarov, and Jeﬀry L. Hirst

XIV
Table of Contents
Complexity of Deep Inference via Atomic Flows. . . . . . . . . . . . . . . . . . . . . .
139
Anupam Das
Connecting Partial Words and Regular Languages. . . . . . . . . . . . . . . . . . . .
151
J¨urgen Dassow, Florin Manea, and Robert Merca¸s
Randomness, Computation and Mathematics . . . . . . . . . . . . . . . . . . . . . . . .
162
Rod Downey
Learning, Social Intelligence and the Turing Test: Why an
“Out-of-the-Box” Turing Machine Will Not Pass the Turing Test . . . . . . .
182
Bruce Edmonds and Carlos Gershenson
Conﬂuence in Data Reduction: Bridging Graph Transformation and
Kernelization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
Hartmut Ehrig, Claudia Ermel, Falk H¨uﬀner, Rolf Niedermeier, and
Olga Runge
Highness and Local Noncappability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
Chengling Fang, Wang Shenling, and Guohua Wu
Turing Progressions and Their Well-Orders . . . . . . . . . . . . . . . . . . . . . . . . . .
212
David Fern´andez Duque and Joost J. Joosten
A Short Note on Spector’s Proof of Consistency of Analysis . . . . . . . . . . .
222
Fernando Ferreira
Sets of Signals, Information Flow, and Folktales. . . . . . . . . . . . . . . . . . . . . .
228
Mark Alan Finlayson
On the Foundations and Philosophy of Info-metrics. . . . . . . . . . . . . . . . . . .
237
Amos Golan
On Mathematicians Who Liked Logic: The Case of Max Newman . . . . . .
245
Ivor Grattan-Guinness
Densities and Entropies in Cellular Automata . . . . . . . . . . . . . . . . . . . . . . .
253
Pierre Guillon and Charalampos Zinoviadis
Foundational Analyses of Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
Yuri Gurevich
Turing Machine-Inspired Computer Science Results . . . . . . . . . . . . . . . . . .
276
Juris Hartmanis
NP-Hardness and Fixed-Parameter Tractability of Realizing Degree
Sequences with Directed Acyclic Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
Sepp Hartung and Andr´e Nichterlein
A Direct Proof of Wiener’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
Matthew Hendtlass and Peter Schuster

Table of Contents
XV
Eﬀective Strong Nullness and Eﬀectively Closed Sets . . . . . . . . . . . . . . . . .
303
Kojiro Higuchi and Takayuki Kihara
Word Automaticity of Tree Automatic Scattered Linear Orderings Is
Decidable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
Martin Huschenbett
On the Relative Succinctness of Two Extensions by Deﬁnitions of
Multimodal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
Wiebe van der Hoek, Petar Iliev, and Barteld Kooi
On Immortal Conﬁgurations in Turing Machines . . . . . . . . . . . . . . . . . . . . .
334
Emmanuel Jeandel
A Slime Mold Solver for Linear Programming Problems . . . . . . . . . . . . . . .
344
Anders Johannson and James Zou
Multi-scale Modeling of Gene Regulation of Morphogenesis . . . . . . . . . . . .
355
Jaap A. Kaandorp, Daniel Botman, Carlos Tamulonis, and
Roland Dries
Tree-Automatic Well-Founded Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
Alexander Kartzow, Jiamou Liu, and Markus Lohrey
Inﬁnite Games and Transﬁnite Recursion of Multiple Inductive
Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
374
Keisuke Yoshii and Kazuyuki Tanaka
A Hierarchy of Immunity and Density for Sets of Reals . . . . . . . . . . . . . . .
384
Takayuki Kihara
How Much Randomness Is Needed for Statistics?. . . . . . . . . . . . . . . . . . . . .
395
Bjørn Kjos-Hanssen, Antoine Taveneaux, and Neil Thapen
Towards a Theory of Inﬁnite Time Blum-Shub-Smale Machines . . . . . . . .
405
Peter Koepke and Benjamin Seyﬀerth
Turing Pattern Formation without Diﬀusion . . . . . . . . . . . . . . . . . . . . . . . . .
416
Shigeru Kondo
Degrees of Total Algorithms versus Degrees of Honest Functions . . . . . . .
422
Lars Kristiansen
A 5n −o(n) Lower Bound on the Circuit Size over U2 of a Linear
Boolean Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
432
Alexander S. Kulikov, Olga Melanich, and Ivan Mihajlin
Local Induction and Provably Total Computable Functions:
A Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
440
Andr´es Cord´on–Franco and F. F´elix Lara–Mart´ın

XVI
Table of Contents
What is Turing’s Comparison between Mechanism and Writing
Worth? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
450
Jean Lass`egue and Giuseppe Longo
Substitutions and Strongly Deterministic Tilesets . . . . . . . . . . . . . . . . . . . .
462
Bastien Le Gloannec and Nicolas Ollinger
The Computing Spacetime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
472
Fotini Markopoulou
Uniﬁability and Admissibility in Finite Algebras . . . . . . . . . . . . . . . . . . . . .
485
George Metcalfe and Christoph R¨othlisberger
Natural Signs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
496
Ruth Garrett Millikan
Characteristics of Minimal Eﬀective Programming Systems . . . . . . . . . . . .
507
Samuel E. Moelius III
After Turing: Mathematical Modelling in the Biomedical and Social
Sciences: From Animal Coat Patterns to Brain Tumours to Saving
Marriages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
517
James D. Murray
Existence of Faster than Light Signals Implies Hypercomputation
already in Special Relativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
528
P´eter N´emeti and Gergely Sz´ekely
Turing Computable Embeddings and Coding Families of Sets . . . . . . . . . .
539
V´ıctor A. Ocasio-Gonz´alez
On the Behavior of Tile Assembly System at High Temperatures . . . . . . .
549
Shinnosuke Seki and Yasushi Okuno
Abstract Partial Cylindrical Algebraic Decomposition I: The Lifting
Phase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
560
Grant Olney Passmore and Paul B. Jackson
Multi-valued Functions in Computability Theory . . . . . . . . . . . . . . . . . . . . .
571
Arno Pauly
Relative Randomness for Martin-L¨of Random Sets . . . . . . . . . . . . . . . . . . .
581
NingNing Peng, Kojiro Higuchi, Takeshi Yamazaki, and
Kazuyuki Tanaka

Table of Contents
XVII
On the Tarski-Lindenbaum Algebra of the Class of all Strongly
Constructivizable Prime Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
589
Mikhail G. Peretyat’kin
Lower Bound on Weights of Large Degree Threshold Functions . . . . . . . .
599
Vladimir V. Podolskii
What Are Computers (If They’re not Thinking Things)? . . . . . . . . . . . . . .
609
John Preston
Compactness and the Eﬀectivity of Uniformization . . . . . . . . . . . . . . . . . . .
616
Robert Rettinger
On the Computability Power of Membrane Systems with Controlled
Mobility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
626
Shankara Narayanan Krishna, Bogdan Aman, and Gabriel Ciobanu
On Shift Spaces with Algebraic Structure . . . . . . . . . . . . . . . . . . . . . . . . . . .
636
Ville Salo and Ilkka T¨orm¨a
Finite State Veriﬁers with Constant Randomness. . . . . . . . . . . . . . . . . . . . .
646
A.C. Cem Say and Abuzer Yakaryılmaz
Game Arguments in Computability Theory and Algorithmic
Information Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
655
Alexander Shen
Turing Patterns in Deserts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
667
Jonathan A. Sherratt
Subsymbolic Computation Theory for the Human Intuitive Processor . . .
675
Paul Smolensky
A Correspondence Principle for Exact Constructive Dimension. . . . . . . . .
686
Ludwig Staiger
Lown Boolean Subalgebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
696
Rebecca M. Steiner
Bringing Up Turing’s ‘Child-Machine’ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
703
Susan G. Sterrett
Is Turing’s Thesis the Consequence of a More General Physical
Principle? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
714
Matthew P. Szudzik
Some Natural Zero One Laws for Ordinals Below ε0 . . . . . . . . . . . . . . . . . .
723
Andreas Weiermann and Alan R. Woods

XVIII
Table of Contents
On the Road to Thinking Machines: Insights and Ideas . . . . . . . . . . . . . . .
733
Jiˇr´ı Wiedermann
Making SolomonoﬀInduction Eﬀective: Or: You Can Learn What You
Can Bound. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
745
J¨org Zimmermann and Armin B. Cremers
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
755

Ordinal Analysis
and the Inﬁnite Ramsey Theorem
Bahareh Afshari1 and Michael Rathjen2
1 School of Informatics, University of Edinburgh, Edinburgh,
EH8 9AB, United Kingdom
bafshari@inf.ed.ac.uk
2 School of Mathematics, University of Leeds, Leeds, LS2 9JT, United Kingdom
rathjen@maths.leeds.ac.uk
Abstract. The inﬁnite Ramsey theorem is known to be equivalent to
the statement ‘for every set X and natural number n, the n-th Turing
jump of X exists’, over RCA0 due to results of Jockusch [5]. By subjecting
the theory RCA0 augmented by the latter statement to an ordinal anal-
ysis, we give a direct proof of the fact that the inﬁnite Ramsey theorem
has proof-theoretic strength εω. The upper bound is obtained by means
of cut elimination and the lower bound by extending the standard well-
ordering proofs for ACA0. There is a proof of this result due to McAloon
[6], using model-theoretic and combinatorial techniques. According to
[6], another proof appeared in an unpublished paper by J¨ager.
1
Introduction
Ramsey’s theorem for inﬁnite sets asserts that for every k ≥1 and colouring of
the k-element subsets of N with ﬁnitely many colours, there exists an inﬁnite
subset of N all of whose k-element subsets have the same colour [7]. We shall
denote the previous statement, when specialised to a ﬁxed k, by RT(k). It is
well known that ACA0 is equivalent to RT(k) for any (outer world) k ≥3
[11]. However, the general assertion of Ramsey’s theorem, ∀x RT(x) (abbreviated
henceforth by iRT), is stronger than ACA0.
For b ≥1 we write F : [A]n →b to signify that F maps the n-element subsets
of A into the set {0, . . . , b −1}. X ⊆A is said to be monochromatic for F if F
is constant on [X]n.
It is known from work of Jockusch [5, Theorem 5.7] that iRT is not provable
in ACA0. More precisely, for every n ≥0, there is a recursive F : [N]n+2 →2
such that the n-th Turing jump of ∅is recursive in any inﬁnite F-monochromatic
X ⊆N. On the other hand, it also follows from [5, Theorem 5.6] that for every
recursive F : [N]n →b and n ≥0 there exists an F-monochromatic X recursive
in the n-th Turing jump of ∅.
For X, Y ⊆N and n < ω, let jump(n, X, Y ) abbreviate the formula stating
that Y is the n-th Turing jump of X. By relativising the results of [5], we arrive
at the following theorem (cf. [6]).
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 1–10, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

2
B. Afshari and M. Rathjen
Theorem 1. ACA0 + ∀n∀X∃Y jump(n, X, Y ) and ACA0 + iRT prove the same
statements.
By [6], ACA0 +iRT has the same ﬁrst order consequences as the theory obtained
from PA by iterating the uniform reﬂection principle arbitrarily often. This will
also follow from Theorem 21, in light of the well-known fact that the latter theory
has proof-theoretic ordinal εω (see [8]). It is worth mentioning that the paper
[12] (whose results actually postdate those in the present paper) contains, among
other things, a characterization of the Π0
2 consequences of ACA0 augmented by
the inﬁnite Ramsey theorem.
We ﬁx a primitive recursive ordering ⪯on ω of order type Γ0. For α ⪯Γ0,
let TI(≺α) denote the scheme of transﬁnite induction on initial segments of α,
i.e.,
∀ξ(∀δ ≺ξA(δ) →A(ξ)) →∀ξ ≺¯βA(ξ),
for every β ≺α and every arithmetical formula A(x). Here ¯β denotes the numeral
corresponding to the ordinal β. The proof-theoretic ordinal of a theory T , is the
least α such that T is equivalent to PA + TI(≺α), in the sense that they prove
the same statements of arithmetic, and this fact can be established on the basis
of PA.
The theory ACA0 + ∀n∀X∃Y jump(n, X, Y ), commonly denoted by ACA′
0,
has previously been shown to have proof-theoretic ordinal εω in [6]. The latter pa-
per uses model-theoretic and combinatorial techniques but also draws on proof-
theoretic results in order to construct an instance of transﬁnite induction up to
εω that cannot be proven in ACA′
0, and thereby indicates that the strength of
the theory is bounded by εω. An unpublished proof using proof-theoretic means
is attributed to J¨ager [6]. However, to the authors’ knowledge no proof using cut
elimination is available in the published literature. This paper provides a simple
proof-theoretic ordinal analysis of the system ACA0 + ∀n∀X∃Y jump(n, X, Y ).
The upper bound is obtained by means of cut elimination and the lower bound
by extending the standard well-ordering proofs for ACA0 as in [9]. For the deﬁ-
nitions of systems of comprehension, ordinal notation and other basic deﬁnitions
we refer the reader to [1,11]. The results presented here form part of [1].
2
The Semi-formal System ACA∞
Let ACA∞be the inﬁnitary calculus corresponding to ACA0. Informally, the
system ACA∞is obtained from ACA0 by replacing the set induction axiom, i.e.,
∀X(0 ∈X ∧∀x(x ∈X →x + 1 ∈X) →∀n n ∈X), by the inﬁnitary ω-rule.
The language of ACA∞is the same as that of ACA0 but the notion of a
formula comes enriched with predicators. Formulae and predicators are deﬁned
simultaneously. Literals (atomic or negated atomic formulae) are formulae. Every
set variable is a predicator. If A(x) is a formula without set quantiﬁers, i.e.,
arithmetical, then {x | A(x)} is a predicator. If P is a predicator and t is a
numerical term, then t ∈P and t /∈P are formulae. The other formation rules
pertaining to ∧, ∨, ∀x, ∃x, ∀X, ∃X are as usual.

Ordinal Analysis and the Inﬁnite Ramsey Theorem
3
We shall be working in a Tait-style formalisation of the second order arith-
metic. By a sequent Γ we mean a ﬁnite set of formulae in the language of second
order arithmetic, L2. Due to the presence of the ω-rule we need only consider
formulae without free numerical variables.
The axioms of the system ACA∞are
– Γ, L where L is a true literal;
– Γ, s ∈X, t /∈X where s and t are terms of the same value.
The rules of ACA∞are
Γ, Ai for i < 2
(∨i)
Γ, A0 ∨A1
Γ, A0
Γ, A1
(∧)
Γ, A0 ∧A1
Γ, A(¯n) for all n
(ω)
Γ, ∀xA(x)
Γ, A(s)
(∃1) Γ, ∃xA(x)
Γ, A(X) X not free in Γ
(∀2)
Γ, ∀XA(X)
Γ, A({x | A0(x)})
(∃2)
Γ, ∃XA(X)
Γ, A(t)
(Pr1) Γ, t ∈{x | A(x)}
Γ, ¬A(t)
(Pr2) Γ, t /∈{x | A(x)}
Γ, A
Γ, ¬A
(Cut)
Γ
The rank of a formula A, denoted |A|, is deﬁned as follows.
– |L| = 0, if L is a literal.
– |s ∈P| = |s /∈P| = |A(¯0)| + 1, if P is the predicator {x | A(x)}.
– |A0 ∧A1| = |A0 ∨A1| = max{|A0|, |A1|} + 1.
– |∀xA(x)| = |∃xA(x)| = |A(x)| + 1.
– |∀XA(X)| = |∃XA(X)| = max{|A(X)| + 1, ω}.
For ordinals α, κ ≺Γ0, we write ACA∞
α
κ Γ
to convey that the sequent Γ
is deducible in ACA∞via a derivation of length ⪯α containing only cuts on
formulae of rank ≺κ. More precisely, this notion is deﬁned inductively as follows:
If Γ is an axiom of ACA∞then ACA∞
α
κ Γ holds for any α and κ. If αi < α and
ACA∞
αi
κ Γi hold for all premisses Γi of an ACA∞-inference with conclusion Γ,
then ACA∞
α
κ Γ , provided that in the case of (Cut) the cut-formulae also have
ranks ≺κ.
ACA∞corresponds to the system EA∗in [9] and can be interpreted into
the ﬁrst level of the semi-formal system of Ramiﬁed analysis, RA∗. The fact
that ACA∞enjoys cut elimination is folklore and the proof involves the stan-
dard techniques of predicative proof theory. For proofs of the following see, for
example, [1,9].

4
B. Afshari and M. Rathjen
Lemma 2
1. ACA∞
|A|·2
0
Γ, A, ¬A for every arithmetical formula A.
2. If Γ contains an axiom of ACA0, then ACA∞
ω+k
0
Γ for some k < ω.
Theorem 3 (First Cut Elimination Theorem for ACA∞). Let α, β ≺Γ0
and k < ω. If ACA∞
α
β+k Γ , then ACA∞
ωk(α)
β
Γ where ω0(α) = α and ωk+1(α)
= ωωk(α).
Theorem 4 (Second Cut Elimination Theorem for ACA∞). Let ω ⪯α ≺
Γ0. If ACA∞
α
ω Γ , then ACA∞
εα
0 Γ .
3
An Upper Bound for ACA0 + iRT
Let T denote the theory ACA0 + ∀n∀X∃Y jump(n, X, Y ). We shall obtain an
upper bound on the strength of this theory by a combination of embeddings and
cut elimination theorems. We ﬁrst embed T into an intermediate theory T ∗. The
semi-formal system T ∗has the same language as ACA∞. If A is a formula of
T , then we write A∗to refer to any formula obtained from A by substituting
all number variables by arbitrary closed terms. The system T ∗has as axioms
all sequences Γ, A∗such that A is a basic axiom of ACA0 or the set induction
axiom. Moreover, we have the following axioms in T ∗.
– Γ, A, ¬A
if A is arithmetical.
– Γ, jump(¯n, P, SP
n ) for every n and arithmetical predicator P.
In above, SP
n is the arithmetical predicator which deﬁnes the n-th Turing jump
of P. The predicator SP
n is formally deﬁned as follows.
SP
0 = {x | ∃u[x = ⟨0, u⟩∧u ∈X]},
SP
n+1 = {x | ∃u[x = ⟨n + 1, u⟩∧∃v {u}(SP
n )n(u) = v]},
where ⟨., .⟩is a primitive recursive pairing function, {u}Y represents the u-th
partial recursive function with oracle Y , and (Y )n denotes the n-slice of the
set Y , i.e., (Y )n = {y | ⟨n, y⟩∈Y }. The logical rules of T ∗are the same as
in ACA∞. The rank of a formula and notation T ∗α
κ Γ are deﬁned analogously.
From the choice of the axioms of T ∗and the fact that the rank of a formula is
always strictly less than ω + ω we can derive the following.
Theorem 5 (Embedding Theorem). Suppose T ⊢B. Then there exist nat-
ural numbers n and m such that T ∗n
ω+m B∗holds for all B∗.
We now perform partial cut elimination in T ∗. The following reduction lemma
goes through in the standard way.
Lemma 6 (Reduction Lemma). Let n0, n1 < ω and |A| = κ ⪰ω. If
T ∗n0
κ Γ, A and T ∗n1
κ Δ, ¬A , then T ∗n0+n1
κ
Γ, Δ .

Ordinal Analysis and the Inﬁnite Ramsey Theorem
5
Proof. By induction on the sum n0 + n1. We show the interesting case where
both A and ¬A are active in the derivations and A is derived via the (∀2)-rule.
Suppose A is of the form ∀Y A0(Y ) and we have
(1)
m0
κ
Γ, A0(X), [∀Y A0(Y )]
m0 < n0 (∀2)
(2)
n0
κ Γ, ∀Y A0(Y )
where X is not free in Γ ∪{∀Y A0(Y )}, and
(3)
m1
κ
Δ, ¬A0(P), [∃Y ¬A0(Y )]
m1 < n1 (∃2)
(4)
n1
κ Δ, ∃Y ¬A0(Y )
where P is an arithmetical predicator. In the above inferences, we write [B] to
emphasise that the formula B may or may not have appeared in the premise of
the original inference. Applying the induction hypothesis to (2) and (3) yields
n0+m1
κ
Γ, Δ, ¬A0(P) ,
(5)
and to (1) and (4) yields
m0+n1
κ
Γ, Δ, A0(X).
(6)
It is not hard to show that (6) implies
m0+n1
κ
Γ, Δ, A0(Q) for any arithmetical
predicator Q and in particular
m0+n1
κ
Γ, Δ, A0(P).
(7)
Since |A0(P)| ≺|∀Y A0(Y )| = κ, we may perform a cut on (5) and (7) to
conclude
n0+n1
κ
Γ, Δ as required. For full details see [1, §3].
Theorem 7 (Cut Elimination Theorem). If T ∗n
ω+m+1 Γ for some m, n <
ω, then T ∗2n
ω+m Γ .
Proof. By induction on n. If Γ is an axiom, then
0
0 Γ . Otherwise, there are two
cases to consider. Suppose Γ is of the form Γ ′, A and we have
ni
ω+m+1 Γ ′, Ai
i < ω, ni < n
(R)
n
ω+m+1 Γ ′, A
where R is any of the rules of T ∗except the cut rule. By applying the induction
hypothesis to the premise(s) of the above inference we obtain
2ni
ω+m Γ ′, Ai . Re-
applying the rule (R) yields
2n
ω+m Γ .
If the last inference was a cut, namely,
n0
ω+m+1 Γ, A
n1
ω+m+1 Γ, ¬A
(Cut)
n
ω+m+1 Γ

6
B. Afshari and M. Rathjen
where n0, n1 < n and |A| ⪯ω + m, by applying the induction hypothesis to the
premises of the above cut we obtain
2n0
ω+m Γ, A and
2n1
ω+m Γ, ¬A .
If |A| ≺ω + m, by a cut on A we derive
2n
ω+m Γ . Otherwise, the Reduction
Lemma yields
2n0+2n1
ω+m
Γ , thus by monotonicity we get the desired result.
Corollary 8. For n, m < ω, if T ∗n
ω+m Γ , then T ∗2n
m
ω
Γ
where 2n
0 = n and
2n
k+1 = 22n
k .
Finally, to analyse T we shall embed T ∗into ACA∞so that we can eliminate
the remaining cuts and read oﬀan upper bound. First we need the following
lemma, which can be veriﬁed by induction.
Lemma 9. There are primitive recursive functions f, g such that for each n,
f(n), g(n) < ω and ACA∞
f(n)
g(n) Γ, jump(¯n, P, SP
n ) .
Theorem 10. Let Γ be a ﬁnite set of arithmetical formulae and k < ω. Then
T ∗k
ω Γ implies ACA∞
εk
0 Γ .
Proof. By induction on k. If Γ is an axiom of T ∗we have the following three
cases to consider. If Γ is the sequent Γ ′, A∗where A is a basic axiom of ACA0 or
the set induction axiom, then we have, by Lemma 2, that ACA∞
ω+k
0
Γ ′, A∗for
some k < ω. If Γ is the sequent Γ ′, jump(¯n, P, SP
n ) where P is an arithmetical
predicator, Lemma 9 yields ACA∞
f(n)
g(n) Γ, jump(¯n, P, SP
n ) . By applying the First
Cut Elimination theorem for ACA∞we obtain
ACA∞
ωg(n)(f(n))
0
Γ, jump(¯n, P, SP
n ) ,
with ωk being deﬁned in Theorem 3. Since f(n), g(n) < ω, we have ωg(n)(f(n)) ≺
ε0, and hence may deduce ACA∞
ε0
0 Γ, jump(¯n, P, SP
n ) . Now let Γ be of the
form Γ ′, A, ¬A with A being arithmetical. As ACA∞
|A|·2
0
Γ, A, ¬A holds due to
Lemma 2, monotonicity provides the desired result.
Now suppose Γ is derived by an application of a logical rule in T ∗. If the last
inference is a cut, then we have
T ∗k0
ω Γ, A
T ∗k0
ω Γ, ¬A
(Cut)
T ∗k
ω Γ
where k0 < k and A is arithmetical. Applying the induction hypothesis to the
premises of the above cut yields ACA∞
εk0
0
Γ, A and ACA∞
εk0
0
Γ, ¬A . Ap-
plying a cut to A, we conclude that ACA∞
εk0 +1
m
Γ
for m = |A| + 1. Thus
ACA∞
ωm(εk0+1)
0
Γ and so we may deduce ACA∞
εk
0 Γ .
If Γ is derived via the (∀2)-rule, Γ must be of the form Γ ′, ∀Y A0(Y ) and we
have

Ordinal Analysis and the Inﬁnite Ramsey Theorem
7
T ∗k0
ω Γ ′, A0(X)
X is not free in Γ ′
and
k0 < k (∀2)
T ∗k
ω Γ ′, ∀Y A0(Y )
Applying the induction hypothesis to the premise of the above inference yields
ACA∞
εk0
0
Γ ′, A0(X) . Re-applying (∀2)-rule allows us to deduce ACA∞
εk
0 Γ .
The other cases are similar.
Corollary 11. Every arithmetical theorem of T without set variables is deriv-
able in PA + TI(≺εω).
Proof. Suppose A is an arithmetical sentence and T ⊢A. By the Embedding
Theorem 5, T ∗n
ω+m A holds for some n, m < ω. Cut elimination for T ∗, Theorem
7, yields that T ∗k
ω A holds for some k < ω. By embedding T ∗into ACA∞via
Theorem 10, we arrive at ACA∞
εk
0 A . This means that A is derivable in ACA∞
directly from the axioms and ﬁrst order rules, and, moreover, if A is of complexity
Π0
n for some n, then all formulae occurring in this cut-free derivation belong
to the same complexity class. By employing a partial truth predicate for Π0
n-
formulae and transﬁnite induction up to εk+1, one shows that PA+TI(≺εω) ⊢A
(cf. [10]).
4
A Lower Bound for ACA0 + iRT
To deduce that εω is also a lower bound for the strength of the theory T , we
shall show that T can prove the well-foundedness of all ordinals strictly less than
εω. Our method is to extend the standard well-ordering proofs for ACA0 as for
instance given in [9, Theorem 23.3]. Let us denote by Sp the operation deﬁned
by
Sp(V ) = {α | ∀ξ(ξ ⊂V →ξ + ωα ⊂V )},
where ξ ⊂V abbreviates ∀x(x ≺ξ →x ∈V ). For sets X and Y we write X ≤e Y
to convey that ∀x(KX(x) = {e}Y (x)), where KX denotes the characteristic
function of the set X. The aim of the next few lemmata is to establish that in
the theory T ﬁnite iterations of the Sp operator can be coded into a single set.
They are easy to verify using the Kleene T -predicate and S-m-n theorem [3].
Detailed proofs can be found in [1, §3]. In the following X(n) denotes the n-th
Turing jump of X, i.e., the set Y such that jump(n, X, Y ). We also use X′ and
X′′ respectively for X(1) and X(2).
Lemma 12. Let A(x, y, z, U) be a Δ0
0-formula (of the language of ACA0) with
all the free variables exhibited and U being a free set variable. Then there exists
a natural number e such that for every X ⊆N
{n | ∀x∃yA(x, y, n, X)} ≤e X′′.

8
B. Afshari and M. Rathjen
Proof. Since for every set X ⊆N the set {⟨x, n⟩: ∃yA(x, y, n, X)} is recursively
enumerable in X (uniformly in X), there exists an index e0 depending only on
the formula A(x, y, z, U) such that for all natural numbers n and sets X ⊆N,
∀x∃yA(x, y, n, X)
iﬀ
∀x⟨e0, ⟨x, n⟩⟩∈X′.
Likewise, there is some d0 such that for all sets Y ⊆N, {d0}Y is total and
∀x⟨e0, ⟨x, n⟩⟩∈Y
iﬀ
⟨d0, ⟨e0, n⟩⟩∈Y ′ .
It immediately follows from the S-m-n theorem that there is an index e such
that {n | ∀x∃yA(x, y, n, X)} ≤e X′′.
In particular, since Sp(X) is Π0
2 in X we can deduce the following.
Corollary 13. ACA0 proves the existence of a natural number e that satisﬁes
Sp(X) ≤e X′′ for all X ⊆N.
Lemma 14. There is a primitive recursive function ◦: ω ×ω →ω such that for
any sets X, Y , Z, if X ≤e Y and Y ≤f Z, then X ≤e◦f Z.
Lemma 15. There is a primitive recursive function N such that X′ ≤N(e) Y ′
whenever X ≤e Y .
Corollary 16. There exists a primitive recursive function g such that
Spn(X) ≤g(n) X(2n),
where Spn is inductively deﬁned as Sp0(X) = X and Spn+1(X) = Sp(Spn(X)).
Proof. We deﬁne g by induction on n. Suppose n = 0. Let g(0) be the index of
the identity function. Then Sp0(X) ≤g(0) X holds trivially. For the induction
step suppose n = k + 1. By Corollary 13 there is an e (independent of k) such
that
Sp(k+1)(X) = Sp(Sp(k)(X)) ≤e (Sp(k)(X))(2).
Using the induction hypothesis we may assume Sp(k)(X) ≤g(k) X(2k). Lemma
15 entails
(Sp(k)(X))(2) ≤N(N(g(k))) (X(2k))(2) = X(2k+2),
and Lemma 14 yields
Spk+1(X) ≤g(k+1) X(2k+2),
by setting g(k + 1) = e ◦N(N(g(k))). Since g is primitive recursive we are done.
Lemma 17. T ⊢∀x∀X∃Y A(x, X, Y ), where A is the formula deﬁned by (Y )0 =
X ∧∀n < x (Y )n+1 = Sp((Y )n).

Ordinal Analysis and the Inﬁnite Ramsey Theorem
9
Proof. We argue informally within T . Given x and X deﬁne
Y = {⟨n, z⟩| n ≤x ∧{g(n)}X(2n)(z) ≃0},
where g is the primitive recursive function given by Corollary 16. It is easy to
see that (Y )0 = X. Moreover, for n ≤x,
z ∈(Y )n
iﬀ
{g(n)}X(2n)(z) ≃0.
By Corollary 16 we have (Y )n = Sp(n)(X). Thus Sp(Sp(n)(X)) = Sp((Y )n), and
hence for n < x we can deduce (Y )n+1 = Sp((Y )n) as required.
Let Tran(≺) and LO(≺) be abbreviations for formulae stating ≺is transitive and
a linear order respectively. Fund(α, X) is the formula
Tran(≺) ∧(Prog≺(X) →∀ξ ≺α (ξ ∈X)),
where Prog≺(X) = ∀x(∀y(y ≺x →y ∈X) →x ∈X), and TI(α, X) is the
formula
LO(≺) ∧Fund(α, X).
The following lemma is well known. For a proof see [9, §21, Lemma 1].
Lemma 18. For every set X and α ≺Γ0,
ACA0 ⊢Fund(α, Sp(X)) →Fund(ωα, X).
Lemma 19. T ⊢Fund(ε0, X).
Proof. We argue informally within T . Fund(α, X) is progressive in α, therefore
it suﬃces to show ∀nFund(ωn(ω), X). By Lemma 17, A(n + 1, X, Y ) holds for
some Y . On the other hand as induction up to ω is available in T for every
set, Fund(ω, (Y )n) holds. Since (Y )n = Sp((Y )n−1), by using Lemma 18 and an
internal induction on n we obtain Fund(ωn(ω), (Y )0).
We can now show that T proves the well-foundedness of ordinals strictly less
than εω.
Theorem 20. For each k < ω, T ⊢Fund(ε¯k, X).
Proof. Since k is given externally, in the formal theory T it is named by ¯k, the
k-th numeral. Below, for ease of presentation, we shall identify k and ¯k. The
proof proceeds by external induction on k. For a ﬁxed k, by internal recursion
on n deﬁne αk
0 = εk−1 +1 and αk
n+1 = ωαk
n. This time start with Fund(αk
0, (Y )n)
and proceed as in Lemma 19 to derive ∀nFund(αk
n, X). Since supn αk
n = εk we
can deduce Fund(εk, X).

10
B. Afshari and M. Rathjen
5
Conclusion
We have shown that our upper bound for the proof-theoretic ordinal of the
theory T is indeed the least one. This allows us to determine the proof-theoretic
strength of T , and hence that of the inﬁnite Ramsey theorem.
Theorem 21. The theory ACA0 + iRT, i.e., ACA0 augmented by the inﬁnite
Ramsey theorem, proves the same arithmetical statements as PA + TI(≺εω).
Proof. Since ACA0+iRT is equivalent to ACA0+∀n∀X∃Y jump(n, X, Y ), Theo-
rem 20 implies ACA0 +iRT ⊢TI(ε¯k, X) for every k < ω, and thus ACA0 +iRT ⊢
TI(≺εω). Corollary 11 provides the other direction.
Acknowledgements. The ﬁrst author was supported by the Engineering and
Physical Sciences Research Council UK (grant number EP/G012962/1). The
second author was supported by a Royal Society International Joint Projects
award 2006/R3.
References
1. Afshari, B.: Relative computability and the proof-theoretic strength of some theo-
ries, Ph.D. Thesis, University of Leeds, U.K. (2008)
2. Friedman, H., McAloon, K., Simpson, S.: A ﬁnite combinatorial principle equivalent
to the 1-consistency of predicative analysis. In: Metakides, G. (ed.) Patras Logic
Symposion, pp. 197–230. North-Holland (1982)
3. Hinman, P.G.: Recursion-theoretic hierarchies. Springer, Heidelberg (1978)
4. J¨ager, G.: Theories for iterated jumps (1980) (unpublished notes)
5. Jockusch, C.G.: Ramsey’s theorem and recursion theory. Journal of Symbolic
Logic 37, 268–280 (1972)
6. McAloon, K.: Paris-Harrington incompleteness and progressions of theories. Pro-
ceedings of Symposia in Pure Mathematics 42, 447–460 (1985)
7. Ramsey, F.P.: On a problem of formal logic. Proceedings of the London Mathe-
matical Society 30(1), 264–286 (1930)
8. Schmerl, U.: A ﬁne structure generated by reﬂection formulas over primitive recur-
sive arithmetic. In: Boﬀa, M., McAloon, K., van Dalen, D. (eds.) Studies in Logic
and the Foundations of Mathematics, vol. 97, pp. 335–350. Elsevier (1979)
9. Sch¨utte, K.: Proof theory. Springer, Heidelberg (1977)
10. Schwichtenberg, H.: Proof theory: Some applications of cut-elimination. In: Bar-
wise, J. (ed.) Handbook of Mathematical Logic, pp. 867–895. North-Holland, Am-
sterdam (1977)
11. Simpson, S.G.: Subsystems of second order arithmetic. Springer, Heidelberg (1999)
12. De Smet, M., Weiermann, A.: A Miniaturisation of Ramsey’s Theorem. In: Ferreira,
F., L¨owe, B., Mayordomo, E., Mendes Gomes, L. (eds.) CiE 2010. LNCS, vol. 6158,
pp. 118–125. Springer, Heidelberg (2010)

Curiouser and Curiouser: The Link
between Incompressibility and Complexity
Eric Allender
Department of Computer Science, Rutgers University, Piscataway, NJ 08855,
United States of America
allender@cs.rutgers.edu
Abstract. This talk centers around some audacious conjectures that
attempt to forge ﬁrm links between computational complexity classes
and the study of Kolmogorov complexity.
More speciﬁcally, let R denote the set of Kolmogorov-random strings.
Let BPP denote the class of problems that can be solved with negligible er-
ror by probabilistic polynomial-time computations, and let NEXP denote
the class of problems solvable in nondeterministic exponential time.
Conjecture 1. NEXP = NPR.
Conjecture 2. BPP is the class of problems non-adaptively polynomial-
time reducible to R.
These conjectures are not only audacious; they are obviously false! R is
not a decidable set, and thus it is absurd to suggest that the class of
problems reducible to it constitutes a complexity class.
The absurdity fades if, for example, we interpret “NPR” to be “the
class of problems that are NP-Turing reducible to R, no matter which
universal machine we use in deﬁning Kolmogorov complexity”. The lec-
ture will survey the body of work (some of it quite recent) that suggests
that, when interpreted properly, the conjectures may actually be true.
1
Introduction
This is a story about mathematical notions that refuse to stay put in their
proper domain. Complexity theory is supposed to deal with decidable sets (and
preferably with sets that are very decidable – primitive recursive at least, and
ideally much lower in the complexity hierarchy than that). Undecidable sets
inhabit a very diﬀerent realm, and they look suspiciously out-of-place popping
up in a discussion of computational complexity classes.
And yet, the (undecidable) set of Kolmogorov-random strings persists in in-
truding into complexity-theoretic investigations. It has become much harder to
deny that there is a connection, although the precise nature of the relationship
remains unclear.
1.1
Cast of Characters
The primary focus of our attention will be a familiar list of deterministic and
nondeterministic time- and space-bounded complexity classes: P, NP, PSPACE,
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 11–16, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

12
E. Allender
NEXP, EXPSPACE, along with the class BPP of languages accepted in polyno-
mial time by probabilistic machines with negligible error, and the class P/poly
of problems with polynomial-size circuit complexity. Detailed deﬁnitions can be
found in a standard text such as [5].
Much of the action in our story revolves around the set of Kolmogorov-random
strings. Before this set can be introduced properly, some deﬁnitions are required.
Given a Turing machine M, the (plain) Kolmogorov complexity function
CM(x) is deﬁned to be the minimum element of the set {|d| : M(d) = x} (and
is undeﬁned if this set is empty). A machine U is said to be universal for this
measure, if
∀M∃c∀xCU(x) ≤CM(x) + c.
As usual in the study of Kolmogorov complexity (see, e.g., [9,8]), we pick one
such universal Turing machine and deﬁne C(x) to be CU(x). (There is something
rather arbitrary in the selection of U; we shall come back to this later.)
For some applications, a better-behaved Kolmogorov complexity measure is
the preﬁx-free measure KM(x) that has an identical deﬁnition, but where the
Turing machine M is restricted to be preﬁx-free (meaning that if M(x) halts,
then M does not halt on input xy for any non-empty string y). It turns out that
a universal preﬁx-free machine U exists such that, for all preﬁx-free machines
M there is a constant c such that for all x KU(x) ≤KM(x) + c, and we select
one such U and deﬁne K(x) to be KU(x). Again, consult [9,8] for details.
A string x is random (or incompressible) if there is no “description” d with
|d| < |x| such that U(d) = x. Depending on which notion of Kolmogorov com-
plexity we are using, this gives us two sets of random strings:
– RC = {x : C(x) ≥|x|}.
– RK = {x : K(x) ≥|x|}.
When it does not make any diﬀerence which of these two sets is meant, we shall
use the simpliﬁed notation “R”. (Similarly, if it is necessary to make explicit
mention of a universal machine U, we shall refer to RKU and RCU.)
2
Some Odd Inclusions
It has long been known [10] that R is Turing-equivalent to the halting problem.
However, it is much less clear what can be eﬃciently reduced to R. To date,
the only known proof that the halting problem can be Turing-reduced to R
via polynomial-size circuits relies on the arsenal of derandomization techniques
that were developed in the 1990s [2]. For eﬃcient “uniform” reductions (i.e.,
reductions computed by polynomial-time machines), it is not easy to see how to
make use of R as an oracle. (To illustrate this, we encourage the reader to spend
a minute trying to see how to reduce their favorite NP-complete problem to R.)
Thus the following theorem is of some interest.

The Link between Incompressibility and Complexity
13
Theorem 2.1. The following inclusions hold:
– BPP ⊆PR
tt [6].
– PSPACE ⊆PR [2].
– NEXP ⊆NPR [1].
Here, the notation PA
tt denotes the class of problems that are reducible to A
via polynomial-time truth-table reductions (also known as “non-adaptive” re-
ductions). These are reductions computed by an oracle Turing machine M that,
on input x, computes a list of queries y1, . . . , ym, and then asks the oracle about
each of these m queries, and then uses the oracle answers to decide whether
to accept or reject. (In a more general Turing reduction, the list of queries can
depend on the answers that the oracle gives.)
There is indeed something odd about Theorem 2.1. Is it interesting to study
eﬃcient reductions to an undecidable set? Since R is Turing-equivalent to the
halting problem, one ought to wonder whether every computably-enumerable set
is in PR
tt (in which case, Theorem 2.1 would not be very interesting).
In truth, it is still an open question whether the halting problem (and hence
every c.e. set) is in PRC
tt . In contrast, for the preﬁx-free measure K, the situation
is intriguing, as the next section will relate.
3
An Upper Bound on Complexity
The proofs of the inclusions in Theorem 2.1, such as NEXP ⊆NPR, make
use of no special properties of the universal Turing machine that deﬁnes Kol-
mogorov complexity. Thus it follows that we actually have the inclusion NEXP ⊆

U NPRKU , where the intersection is taken over all universal preﬁx-free Turing
machines. This might seem to be a trivial observation, but it is actually essen-
tial, if we want to obtain an upper bound on the complexity of classes such as
NPR. This is because there exist universal preﬁx-free Turing machines U such
that even P
RKU
tt
contains arbitrarily complex decidable sets.
However, a paper presented at the 2011 ICALP conference shows that, if
we consider only those problems that are reducible to RK regardless of which
universal Turing machine is used in deﬁning K-complexity, then we do indeed
obtain something that looks very much like a complexity class:
Theorem 3.1. [4]
– BPP ⊆Δ0
1 ∩
U P
RKU
tt
⊆PSPACE
– PSPACE ⊆Δ0
1 ∩
U PRKU
– NEXP ⊆Δ0
1 ∩
U NPRKU ⊆EXPSPACE.
Here, as usual, Δ0
1 denotes the class of decidable sets.
Theorem 3.1 is stated in terms of the preﬁx-free measure K. It seems reason-
able to conjecture that it holds also for the plain measure C, but there does not
seem to be an easy way to modify the proof of Theorem 3.1 to deal with the C
measure.

14
E. Allender
The proof of the inclusion Δ0
1 ∩
U NPRKU ⊆EXPSPACE proceeds by ﬁrst
observing that an NP-Turing reduction can be simulated by an exponential-
time truth-table reduction, and then noting that the PSPACE upper bound
on Δ0
1 ∩
U P
RKU
tt
translates into an EXPSPACE upper bound on the class
Δ0
1 ∩
U EXP
RKU
tt
. Since NP is widely conjectured to be a small subclass of
exponential time, it seems likely we are throwing away too much information
in the initial step in this argument (replacing an NP-Turing reduction by an
exponential-time truth-table reduction). That is, we suspect that the inclusion
Δ0
1 ∩
U EXP
RKU
tt
⊆EXPSPACE is not optimal. In fact, we suspect that the
inclusion NEXP ⊆NPR is tight, in the following sense:
Conjecture 3.2. NEXP = Δ0
1 ∩
U NPRKU .
Such a characterization of NEXP in terms of reducibility to RK would certainly
be unusual. Perhaps it would also be useful.
4
Towards a Characterization of BPP
There is more to report, regarding the inclusion Δ0
1 ∩
U P
RKU
tt
⊆PSPACE of
Theorem 3.1.
In a still-unpublished paper [3], it is argued that it is likely that the PSPACE
upper bound can be improved to PSPACE ∩P/poly. If it true, then this would
imply that BPP ⊆Δ0
1 ∩
U P
RKU
tt
⊆PSPACE ∩P/poly. Since there is a dearth
of interesting complexity classes between BPP and PSPACE ∩P/poly, this mo-
tivates the following:
Conjecture 4.1. BPP = Δ0
1 ∩
U P
RKU
tt
.
The evidence presented in [3] in support of the P/poly upper bound can be
summarized in this way: The authors present a true statement of the form
∀n∀jΨ(n, j) (provable in ZF), with the property that if, for each ﬁxed (n,j)
there is a proof in Peano Arithmetic of the statement ψ(n,j), then the P/poly
upper bound holds. (In fact, under this assumption, for each length n, it suﬃces
to restrict attention to truth-table reductions that make queries only of length
O(log n) and have as oracle a subset of R (possibly a diﬀerent subset for each
input length – which can be encoded as a circuit for inputs of length n).
Motivated largely by the results of [3], Buhrman and Loﬀ[7] have proved a
very recent result that can also be seen as supporting the P/poly upper bound.
For a polynomial-time reduction from a decidable set A to the undecidable set
R, it seems reasonable to hypothesize that the reduction would also work if one
used a very high time-complexity approximation to R, such as Rt(n)
K
for some
very rapidly-growing time bound t(n). Buhrman and Loﬀhave shown that, for
each decidable set A and polynomial-time truth-table reduction M, it is the
case that for every large-enough time bound t, if M reduces A to Rt(n)
K
, then
A ∈P/poly.

The Link between Incompressibility and Complexity
15
Interestingly, the techniques used by Buhrman and Loﬀalso allowed them
to show that the sentences ψ(n,j) considered in [3] are, in fact, independent of
Peano Arithmetic. Worse, they present a polynomial-time reduction with the
property that it can not be directly replaced by a reduction that makes queries
only of length O(log n), having as oracle a subset of R. Thus the general approach
discussed in [3] will need to be revised substantially, if it is to be used to obtain
a P/poly upper bound on this class.
5
Speculations
Since, to date, no interesting characterizations of complexity classes in terms of
eﬃcient reductions to R have been obtained, it may be premature to speculate
about the usefulness of such a characterization. Nonetheless, it is fun to engage
in such speculation. Could it be possible that linking the study of Kolmogorov-
random strings to the study of computational complexity classes could enable
the application of tools from one domain, to problems where these tools had
seemed inapplicable? It is an exciting prospect to contemplate.
The techniques of computability theory usually relativize, which might seem
like an impediment to the realization of this program. However, the inclusions
PSPACE ⊆PR and NEXP ⊆NPR each utilize techniques that do not relativize.
Perhaps there is room to explore new combinations of tools and techniques from
these ﬁelds.
The inclusion BPP ⊆PR
tt does relativize, in the following sense. The argument
of [6] shows that, for every decidable set A, every set in BPPA is PA-truth-table
reducible to R. Thus it is conceivable that a stronger version of Conjecture 4.1
holds, characterizing BPPA as the class of decidable sets that are PA-truth-
table reducible to R. Thus, any attempt to prove P = BPP (as many suspect)
by proving that P = Δ0
1 ∩
U P
RKU
tt
will require some new non-relativizing proof
techniques. (Note however that analogous equalities have been proved for some
limited classes of truth-table reductions [1].)
At the very least, results such as Theorem 3.1 provide motivation for some
questions in computability theory that have not received much attention. For
instance, is “Δ0
1∩” redundant in each line of Theorem 3.1? That is, if a set is in
NPRKU for each universal machine U, is it decidable?
Acknowledgments. The research of the author is supported in part by NSF
Grants CCF-0830133, CCF-0832787, and CCF-1064785.
References
1. Allender, E., Buhrman, H., Kouck´y, M.: What can be eﬃciently reduced to the
Kolmogorov-random strings? Annals of Pure and Applied Logic 138, 2–19 (2006)
2. Allender, E., Buhrman, H., Kouck´y, M., van Melkebeek, D., Ronneburger, D.:
Power from random strings. SIAM Journal on Computing 35, 1467–1493 (2006)

16
E. Allender
3. Allender, E., Davie, G., Friedman, L., Hopkins, S.B., Tzameret, I.: Kolmogorov
complexity, circuits, and the strength of formal theories of arithmetic. Tech. Rep.
TR12-028, Electronic Colloquium on Computational Complexity (submitted for
publication, 2012)
4. Allender, E., Friedman, L., Gasarch, W.: Limits on the Computational Power of
Random Strings. In: Aceto, L., Henzinger, M., Sgall, J. (eds.) ICALP 2011. LNCS,
vol. 6755, pp. 293–304. Springer, Heidelberg (2011)
5. Arora, S., Barak, B.: Computational Complexity, a modern approach. Cambridge
University Press (2009)
6. Buhrman, H., Fortnow, L., Kouck´y, M., Loﬀ, B.: Derandomizing from random
strings. In: 25th IEEE Conference on Computational Complexity (CCC), pp. 58–
63. IEEE Computer Society Press (2010)
7. Buhrman, H., Loﬀ, B.: Personal Communication (2012)
8. Downey, R., Hirschfeldt, D.: Algorithmic Randomness and Complexity. Springer
(2010)
9. Li, M., Vitanyi, P.: Introduction to Kolmogorov Complexity and its Applications,
3rd edn. Springer (2008)
10. Martin, D.A.: Completeness, the recursion theorem and eﬀectively simple sets.
Proceedings of the American Mathematical Society 17, 838–842 (1966)

Information and Logical Discrimination
Patrick Allo
Centre for Logic and Philosophy of Science, Vrije Universiteit Brussel, Pleinlaan 2,
B-1050 Brussels, Belgium
patrick.allo@vub.ac.be
Abstract. Allo & Mares [2] present an “informational” account of log-
ical consequence that is based on the content-nonexpansion platitude.
The core of this proposal is an inversion of the standard direction of ex-
planation: Informational content is not deﬁned relative to a pre-existing
logical space, but it is approached in terms of the level of abstraction at
which information is assessed.
In this paper I focus directly on one of the main ideas introduced
in that paper, namely the contrast between logical discrimination and
deductive strength, and use this contrast to (1) illustrate a number of
open problems for an informational conception of logical consequence,
(2) review its connection with the dynamic turn in logic, and (3) situate
it relative to the research agenda of the philosophy of information.
1
Background and Motivation
1.1
Conceptions of Logical Consequence
By a conception of logic, we primarily mean a conception of its core notion,
namely the concept of logical consequence (what follows from what). Tradition-
ally, logical consequence is analysed along two paths. The ﬁrst path relies on
the basic platitude that for A to follow from Γ, it should be impossible for A
to be false while all members of Γ are true. More exactly, according to this ﬁrst
path, the prima facie modal notion of logical consequence can be reduced to the
non-modal notion of truth-preservation. As emphasised by Jc Beall and Greg
Restall [6], this approach is summarised by the Generalised Tarski Thesis:
GTT. A conclusion A follows from premises Γ iﬀevery case where all the
premises in Γ are true is also a case where A is true.
When we provide a formal precisiﬁcation of the gtt, we usually do two things:
First, we give a formal description of what cases are; second, we inductively
deﬁne what it means for a formula to be true at a case (i.e., we state the truth-
conditions for the diﬀerent expressions of our language). By following this tra-
ditional approach, we give a model-theoretic formalisation of a truth-theoretic
conception of logical consequence: We use set-theoretical tools to explain what
cases are, and we explain the meaning of our logical vocabulary by stating the
truth-conditions for each type of expression that is in our language (we say, for
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 17–28, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

18
P. Allo
instance, that ⌜p or q⌝is true in a case iﬀ⌜p⌝is true in that case or ⌜q⌝is true
in that case). In accordance with our use of the gtt-label and its origin in [20],
we call this approach Tarskian.
The second path relies on the basic platitude that for A to follow from Γ,
there must be a derivation of A from Γ, where every step in that derivation
is motivated by a primitive rule of inference. Crucially, these primitive rules
are often seen as implicit deﬁnitions of the meaning of our logical vocabulary.
According to this inferential account, we rely on proof-theoretical methods to
determine the extension of “follows from,” and we use proof-conditions to explain
the meaning of the logical constants (we say, for instance, that to derive ⌜p or q⌝,
we must either derive ⌜p⌝or derive ⌜q⌝). Given its origin in a famous remark in
[12], we call this the Gentzian-approach.
In practice, inferentialism and proof-theoretical accounts of logical conse-
quence (and the meaning of logical constants) are often considered synonymous.
Similarly, truth-theoretical conceptions of logical consequence and the model-
theoretical means it relies on are traditional allies as well. This received view is
attractive when we consider the traditional truth-theoretical account of the con-
sequence relation of classical logic, or when we consider the equally traditional
inferentialist account of the consequence relation of intuitionist logic.1 Yet, it
becomes more problematic when we consider a wider variety of logical systems.
The standard model-theories of intuitionist or relevant logics,2 for instance, sug-
gest either that alternative logics change the meaning of “true,” or that these
models are mere formal tools that, from a philosophical point of view, miscon-
strue the purpose of developing an alternative to classical logics [1]. As such, the
rise of non-classical logics drives a wedge between truth-theoretical accounts of
logical consequence and their model-theoretical implementation, and—albeit to
a lesser extent—also between proof-theoretical characterisations of logical con-
sequence and their inferentialist philosophical underpinning. It is at this point
that informational conceptions of logical consequence become relevant.
Informational conceptions of logic were, perhaps most famously, developed as
a response to the charge that the usual model-theory for relevant logics (due
to Routley and Meyer)3 didn’t count as a proper semantics. As claimed by
Copeland [10, p. 406] the use of (a) possibly incomplete and/or inconsistent
cases together with (b) the Routley-star to enforce a non-classical behaviour
of its negation-connective, and (c) a ternary relation to enforce a non-classical
behaviour of its implication-connective, leads to a model-theory that gets the
intended extension of the relevant consequence relation right, but fails to shed a
light on the intended meaning of the logical connectives. In other words, we have
a pure semantics or model-theory without a corresponding applied semantics.
1 Intuitionist logics reject the classically valid rule of double-negation elimination, and
hence also the validity of indirect proof (to conclude A from the fact that we have
derived an absurdity from the negation of A), and of excluded middle.
2 Relevant logics avoid the so-called paradoxes of material and strict implication;
mainly: p →(q →p), q →(p ∨¬p), (p ∧¬p) →q, and q →(p →p).
3 The actual details can safely be ignored, but see [3] for an overview.

Information and Logical Discrimination
19
The informational conceptions of relevant consequence developed by Restall [18]
and Mares [15] provide an applied semantics based on Barwise’s and Perry’s
theory of situations [5]. The guiding insights of such approaches are, ﬁrst, that
the possibly incomplete and inconsistent cases should rather be understood in
terms of the information that is available in a situation rather than in terms of
what is true or false in that situation, second, that for ⌜not−p⌝to be available
in a situation is just for ⌜p⌝not to be available in all compatible situations,
and, third, that for ⌜p implies q⌝to be available in a situation is just to have
access to a regularity or constraint stating that if some situation contains the
information that ⌜p⌝there must also be a related situation that contains the
information that ⌜q⌝.
This speciﬁc formulation of an informational conception of logic shows that
model-theoretic characterisations of logical consequence need not be tied to
a narrow truth-conditional interpretation. The close connection between the
Routley-Meyer semantics and theories of information that are based on Bar-
wise and Perry’s theory of situation, however, does not imply that informa-
tional conceptions of logic should ignore proof-theoretical considerations. While
it is true that typical information-theoretical conceptions of logic rely on model-
theoretical characterisations,4 Mares [16] explicitly introduces proof-theoretical
considerations to motivate the information-conditions for certain expressions,
and Paoli [17] suggests an informational reading of sequents.
1.2
Informational Content and Logical Space
In a joint paper with Edwin Mares [2], I suggested that as a mere defence of the
Routley-Meyer semantics, the use of situations and information ﬂow between
situations falls short of being a full-ﬂedged informational account of logical con-
sequence. Hence, we proposed a more general ‘informational’ account of logical
consequence that is based on the content-nonexpansion platitude.
CN. A follows from Γ iﬀthe content of A does not exceed the combined content
of all members of Γ.
We then argued that, while classically there is no diﬀerence between an in-
formational and a truth-conditional approach to logic (the information acces-
sible at a world can straightforwardly be analysed as what is true in the set
of worlds that cannot be distinguished from the actual world), the contrast
between information-conditions and truth-conditions can be exploited in such
non-classical logics as relevant and intuitionistic logic.
Because an informational conception leads to an inversion of the standard di-
rection of explanation it allows for theoretical positions that are not readily avail-
able to its standard competitors (truth-conditional and inferential conceptions
4 The already mentioned proposals by Restall and Mares [18,15], but also an
information-theoretic characterisation of classical logic due to Corcoran [19] and
an informational interpretation of constructive substructural logics due to Wansing
[22,21].

20
P. Allo
of logical consequence). It is therefore particularly suited for both non-classical
and pluralistically inclined philosophies of logic.
The informational content of a piece of information is almost invariably asso-
ciated with a certain proportion of a given logical space (or space of possibilities);
namely the proportion of the space that is ruled out by that information. On
the standard order of explanation, the construction of this logical space is en-
tirely based on the identiﬁcation of possibility (true somewhere in the logical
space) with consistency, and hence of necessity (true everywhere in the logical
space) with inconsistency of negation. Concretely, the content associated with a
contradiction coincides with the totality of the logical space: Contradictions are
false across the whole space, and thus rule out the whole logical space. Analo-
gously, the content associated with a logical truth is empty: Logical truths are
true across the whole space (the negation of a logical truth is a contradiction),
and thus rule out the null-proportion of the logical space. These considerations
yield a logical space that only contains possibilities that are both consistent and
complete, and this is entirely in accordance with the standard truth-theoretic
principles that either ⌜A⌝is true or ⌜not −A⌝is true, but not both.
As long as we stick to this traditional order of explanation, it is hard to con-
struct a ﬁner logical space (a broader space of possibilities) without rejecting
these standard principles. By constructing a logical space that does not depend
on truth-theoretic considerations, but only on what can and cannot be distin-
guished, we keep logical and truth-theoretical considerations separate. The main
challenge is to explain how a logical space should be constructed if truth isn’t
the main criterion.
1.3
Dynamics of Information
The development of an informational conception of logical consequence is just
one way to connect logic with information. Another, more recent, trend in the
study of logic and information is oriented towards the dynamics of information.
Dynamic epistemic and doxastic logics (as well as many of their predecessors)
investigate the dynamics of information by looking speciﬁcally at, in the tradi-
tional sense, how the propositional attitudes of knowledge and belief change in
virtue of new information, and, in a more contemporary sense, how the distri-
bution of knowledge and belief in a multi-agent setting evolves in virtue of how
these agents exchange information. While extremely fruitful and inﬂuential, the
dynamic paradigm does not aim to provide an informational conception of logi-
cal consequence. This is so because, ﬁrstly, the dynamic turn in logic challenges
the centrality of the notion of logical consequence [8], and, secondly, because
it provides a logical model of informational dynamics, rather than an informa-
tional account of logical dynamics (though one may argue that it does both).
Yet, even if we grant the diﬀerence in focus, it still seems unfortunate that both
informational approaches to logic remain unrelated.

Information and Logical Discrimination
21
Overview
Rather than to reiterate the argumentation of this previous paper, I want to focus
directly on one of the main ideas introduced in that paper, namely the contrast
between logical discrimination and deductive strength (§2), and use this contrast
to review its connection with the dynamic turn in logic (§3.1); to illustrate a
number of open problems for an informational conception of logical consequence
(§3.2); and to situate it relative to the research agenda of the philosophy of
information (§4).
2
Information and Discrimination
The notion of logical discrimination captures what can be “told apart” in a given
logical system. We say, for instance, that intuitionist logic can discriminate be-
tween p and not−not−p, whereas classical logic cannot, or that paraconsistent
logics can tell diﬀerent inconsistent theories apart while from a classical view-
point there’s only one such theory, namely the trivial one. Still, even when it is
acknowledged that non-classical logics like intuitionist and paraconsistent logic
allow for ﬁner distinctions than classical logic, considerations of that kind are
not assumed to bear upon core issues in the philosophy of logic. The informa-
tional conception of logic we discuss in this paper introduces the notion of logical
discrimination as the main criterion for the construction of logical space.
2.1
Two Principles and the Crucial Inversion
As an introduction to how I want to exploit the contrast between logical dis-
crimination and deductive strength, consider the following two principles:5
IP. “Whenever there is an increase in available information there is a corre-
sponding decrease in possibilities, and vice versa.” [4, p. 491]
DD. “The more a logic proves, the fewer distinctions (or discriminations) it
registers” [14, p. 207]
While each of these principles is indiﬀerent with respect to the conceptual or-
der of its relata, (logical) orthodoxy often imposes such an order. The inverse
relationship principle IP is customarily read as a reduction of the notion of
information to that of possibility: What counts as informative depends on a pre-
existing space of possibilities. Furthermore, when dealing with logical possibility,
the relevant space of possibilities is often further reduced to the logical notions
of consistency and inconsistency. In practice, this comes down to a reduction of
information to truth-conditions.
5 The ﬁrst of these principles is usually referred to as the inverse relationship principle.
Since the second principle also expresses an inverse relation between two notions, I
try to avoid the traditional nomenclature.

22
P. Allo
Likewise, DD merely expresses an inverse relation between the deductive
strength and the discriminatory power of a logical system.6 Yet, the traditional
focus on what follows from what in logical theorising suggests that the discrim-
inatory power of a logical system is, if not a mere side-eﬀect of its deductive
strength, then at least a consequence of how contents are to be carved out to get
the extension of “follows from” right. This is a natural line of thought when one
thinks of logical consequence as truth-preservation, for in that case we should
make all distinctions that are needed to avoid stepping from truth to falsehood,
but no more than that.
Barwise [4] already questioned the standard reading of IP by proposing a more
pragmatic conception of possibilities and impossibilities (which is not restricted
to the notions of logical possibility and impossibility):
My main proposal here is that a good theory of possibility and informa-
tion should be consistent with [IP]. As I analyze things, impossibilities
are those states of the system under investigation that are ruled out by
(i.e., incompatible with) the currently available information about the
system. States that are not so ruled out are possibilities. Mathematical
inquiry, like any other form of successful inquiry decreases possibilities
when it increases the available information. (p. 491)
In the case of logical possibility, the available information consists of
the laws of logic. Just what the laws amount to, and how they relate the
metaphysical and mathematical laws, is a contentious question, however,
one we shall not attempt to answer. How one answers it will determine
what one counts as a logical possibility. (p. 497)
In [2] we followed a similar path by, in a ﬁrst move, proposing that how we carve
out contents by specifying a logical space is determined by the logical information
that is available, and, second, by analysing the notion of available information in
terms of how we access and use information in our environment. The emphasis on
access and use in the analysis of what counts as available information is rooted
in a relational conception of information; it depends on our environment as well
as on the kind of agents / reasoners we are. The following quote places this
relational conception in the broader context of the philosophy of information:
The point made was that data are never accessed and elaborated (by an
information agent) independently of a level of abstraction. Understood
as relational entities, data are constraining aﬀordances: they allow or
invite certain constructs (they are aﬀordances for the information agent
that can take advantage of them) and resist or impede some others (they
are constraints for the same agent), depending on the interaction with,
and the nature of, the information agent that processes them. [11, p. 87]
The way we access or fail to access certain information is tied to the distributed
nature of information (or data). We cannot access all information at once, and
6 As shown by Humberstone, this principle has exceptions, but here we can ignore this
complication.

Information and Logical Discrimination
23
because of that using information often means combining information that is
accessed in diﬀerent situations. By varying the ways in which we access and
combine information from diﬀerent situations, we can conceive of diﬀerent “rea-
soning styles” and relate these styles to diﬀerent logical systems [2, p. 9]. We
consider three such styles:
1. When we access information locally, and combine information from diﬀerent
situations without thereby always having access to the totality of the infor-
mation we used to reach our conclusions, the resulting reasoning-style can
be codiﬁed by a relevant or other substructural logic.
2. When we access information locally, but let the deductive reasoning pro-
cess be cumulative such as to retain access to the information we used, the
resulting reasoning-style can be captured by an intuitionistic logic.
3. When we access information globally (we access all accessible information at
once), the resulting reasoning-style is in accordance with classical logic.
This way of relating access and use to logical systems is very natural from the
perspective of the frame-semantics for relevant and intuitionist logics [7].
For an informational conception of logic, a logical space, but also a given de-
gree of logical discrimination, or a set of global constraints on a class of models
are all abstractions of how we access and use the information in our environ-
ment. Global constraints [2, p. 13] are much like the laws of logic mentioned in
the quotation by Barwise. Logical spaces, by contrast, can be related to the type
of environment we focus on—worlds vs. situations—, or to the perspective on
inference and the representation of partial information we adopt—worldly vs.
situated—see [2, p. 7], but also [6]. Yet, the notion of a degree of logical discrim-
ination is even more closely related to one of Barwise’s concerns; namely the
fact that a ‘given’ logical space is often too coarse to account for what we con-
sider genuine increases of available information. By questioning the usual reading
of DD and using considerations about which distinctions are worth retaining
directly for the construction of a logical space, we obtain an alternative perspec-
tive on the granularity problem. The main virtue of this type of approach is the
connection it establishes between two main concerns in logical (and other types
of formal) modelling: the ability to extract information from our model (infer-
ence), and the ability to distinguish between relevant properties of the model
(discrimination).
2.2
Logic and Granularity
With respect to the notion of granularity, the following view seems fairly un-
controversial: Propositions can be individuated more or less ﬁnely. What counts
as the correct level of abstraction (henceforth, LoA) or degree of granularity
can only be determined once we make clear what we’re after. It is, for instance,
standard to assume that if we’re interested in logical consequence, it suﬃces to
individuate propositions in terms of their truth-conditions to ﬁnd out whether
the content of the conclusion of an argument does not exceed the combined

24
P. Allo
content of its premises. On that account, two propositions should be treated
as (logically-) equivalent whenever their truth-conditions coincide. By contrast,
truth-conditions are often deemed too coarse to characterise the content of in-
tentional states. Thus—as exempliﬁed by the fact that logic requires us to make
fewer distinction than many other theoretical enterprises—diﬀerent theoretical
disciplines require diﬀerent degrees of granularity.
Yet, orthodoxy has it that while contents can be individuated more or less
ﬁnely, there’s only one “logical” way to discriminate propositional contents;
namely in terms of their truth-conditions. When supplemented with the (equally
orthodox) view that truth-conditions can only be assigned in accordance with
the classical truth-tables, this immediately motivates a monism about logical
consequence (there is only one correct consequence relation). An informational
conception of logical consequence avoids this second aspect of logical orthodoxy
by defending the view that some non-classical ways of carving out contents are as
logical as the classical way of doing so. This argument is backed by several con-
siderations; some of which are directly related to the functioning of non-classical
logics, others are related to how we choose a logical system. To a ﬁrst approxi-
mation, when we settle for a logical system to model or evaluate an argument,
the choice is between deductively strong classical consequence relations, and
deductively weaker sub-classical systems. When classical logic has unwelcome
consequences, we retreat to a weaker logic. This is the received view: Because
we end up with a crippled logic, the decision to use a sub-classical logic is a
genuine retreat.
With DD in mind, an alternative view is readily available: When we opt for a
deductively weaker consequence relation, we obtain some additional discrimina-
tory power in return. As a result, when we evaluate a logical system we need to
balance the opposite virtues of logical discrimination and deductive strength to
decide which logical system is the most appropriate for a given purpose. From
that perspective, the standard reading of DD as well as the received view on
logical revision is mistaken or at least incomplete. While paraconsistent logics
are often adopted as a means to avoid triviality in the face of contradiction, such
non-explosive logics can equally well be adopted with the intent to discriminate
between diﬀerent, yet classically equivalent, inconsistent theories. In fact, given
some minor assumptions, the avoidance of explosion and the ability to tell diﬀer-
ent contradictions apart are two sides of the same coin.7 This line of argument is
not readily available to the proponent of a truth-theoretical conception of logic.
7 Two expressions A and B are synonymous relative to ⊢(A ≡⊢B) iﬀwe have:
C1(A), . . . , Cn(A) ⊢Cn+1(A) iﬀC1(B), . . . , Cn(B) ⊢Cn+1(B)
Where Ci(B) is obtained from Ci(A) by replacing some (but not necessarily all)
occurrences of A in Ci(A) by B.
If, following Humberstone, we take synonymy as the formal explication of logi-
cal discrimination, then (assuming that consequence is reﬂexive and transitive, and
that simpliﬁcation is valid) “p and not-p” and “q and not-q” are synonymous for a
consequence relation iﬀthat consequence relation is explosive.

Information and Logical Discrimination
25
First, because if our main aim is to avoid stepping from truth to falsehood, it is
hard to motivate additional distinctions that do not serve this primary purpose
of truth-preservation (Why reject a classically valid argument if it is impossible
for the conclusion to be actually false whenever all the premises are actually
true?). Second, because ﬁner distinctions require a broader logical space, and
this can only be done by including possibly incomplete and/or inconsistent pos-
sibilities. This move isn’t straightforward if one wishes to preserve a classical
conception of truth.
3
Issues and Open Problems
In its standard presentation that is based on the content-nonexpansion platitude,
the informational conception of logic is a natural ally of logical pluralism. When
put in the words of van Benthem, we should rather say it is an ally of a “reasoning
styles” pluralism about logic. Indeed, despite the importance granted to the
notion of logical discrimination, the centrality of logical consequence (witness its
role in the deﬁnition of synonymy) still suggests that an informational conception
of logic is irrelevant to the concerns of the dynamic turn in logic. This type of
objection is implicit in the following fragments from van Benthem [8]:
In particular, I will argue that logical dynamics sets itself the more ambi-
tious diagnostic goal of explaining why sub-structural phenomena occur,
by ‘deconstructing’ them into classical logic plus an explicit account of
the relevant informational events. I see this as a still more challenging
departure from traditional logic. (. . . ) The view that logic is really only
about consequence relations may have been right at some historical stage
of the ﬁeld. (. . . ) Since the 1930s, modern core logic has been about at
least two topics: valid inference, yes—but on a par with that, deﬁnabil-
ity, language and expressive power. (. . . ) And to me, that deﬁnability
aspect has always been about describing the world, and once we can do
that, communicating to others what we know about it. (p. 182–3)
This type of objection raises two important issues for the position I defended
in the previous sections. First, it suggests that the informational conception of
logic may be too narrow to cover so-called logics for informational dynamics;
second, if so-called substructural phenomena can be elucidated in extensions of
classical logic, it implies that the sub-classical logics that arise from the need
to discriminate propositions more ﬁnely may not be the most appropriate or
desirable formalism to reach this goal. Below, I discuss each of these objections.
3.1
Information and the Dynamic Turn in Logic
The clue to see how the informational conception really engages with the dy-
namic turn in logic lies in the already mentioned identiﬁcation of inference with
the ability to extract information from a given formal model, and of discrimi-
nation with the ability to distinguish between diﬀerent features of that model.

26
P. Allo
Anyone familiar with the development of dynamic epistemic and doxastic logics
will immediately recognise these concerns. Extracting information from an epis-
temic or doxastic model is what we do when (a1) we assign beliefs or ascribe
knowledge to an agent, and (b1) we predict the eﬀect of certain actions on their
knowledge or beliefs. Distinguishing relevant features of a model is what we do
when (a2) we compare or contrast the epistemic states of diﬀerent agents, and
(b2) compare the eﬀect of diﬀerent types of actions.
The standard story can thus be extended as follows: If our underlying conse-
quence relation is powerful, our agents will seem highly knowledgeable according
to our model; if our underlying consequence relation is more discriminating, our
model will not collapse intuitively distinct knowledge or belief-states. While ob-
viously correct, this standard story is also overly reductive. The strength of
dynamic epistemic and doxastic logics lies precisely in the wide range of infor-
mational actions it can incorporate. In dynamic epistemic logic we can contrast
public with several types of private and semi-private announcements; in dynamic
doxastic logic we can contrast several belief-revision policies. Arguably, this is
an increase in discriminatory power that does not obviously correspond to a de-
crease in deductive power, and thus isn’t covered (in a non ad hoc way) by the
standard story. As such, the dynamic turn in logic clearly poses a challenge for
the informational conception of logic. I do not believe this is an insurmountable
challenge. The centrality of the content-nonexpansion platitude might have to be
reconsidered, and the standard story about how logical discrimination and de-
ductive strength are related will have to be generalised. But this seems feasible.
The notion of logical discrimination as well as the double concern of extracting
information from a model and the ability to make some distinctions in a model
while collapsing others can, therefore, remain unchallenged.
3.2
Guises of Logical Discrimination
Suggesting that logics that revise classical logic may be recaptured as extensions
of classical logic isn’t speciﬁc to the dynamic turn in logic, but is a standard
theme in the philosophy of non-classical logic [13,1], and in the discussion of the
notion of information in intuitionistic logic [9]. The guiding idea seems to be
this: We should not weaken the underlying consequence relation such as to be
able to carve out more ﬁne-grained contents, but we should make our languages
more expressive such as to both retain the deductive strength of classical logic,
and be able to make novel distinctions. The moral of the availability of these
two ways of changing our logic is, in my opinion, not that the conservative
approach of extending rather than revising our logic is always to be preferred.
It only reveals that the study of logical discrimination should not be reduced
to that of granularity. Expressivity is equally important! Again, the dynamic
turn (and more broadly the development of modal logics) only poses a challenge
to the development of an informational conception of logical consequence in
the sense that it calls for a further generalisation of its basic concepts (logical
discrimination isn’t just about the distinctions that are retained by a logic,
but also about the distinctions that are already potentially there in a formal

Information and Logical Discrimination
27
language). Such developments do not show that the project of placing logical
discrimination at the core of an informational conception of logic is not viable.
4
Informational Semantics and the Philosophy of
Information
To conclude, one could ask how this story about logical discrimination relates to
the core concerns of the philosophy of information. After all, the concerns about
the relation between extensions and revisions of classical logic are already a topic
in the philosophy of logic. Likewise, the question of how the content of intentional
states should be individuated if doing so on the basis of truth-conditions isn’t
viable, is a topic in formal semantics, philosophical logic, and the metaphysics
of modality. What do we win by regrouping these concerns under the heading
of the philosophy of information? There are two complementary answers to that
question.
The direct answer is that the inversion in the order of explanation leads to a
conception of logical consequence that takes information as its most basic notion.
As such, the development of an informational conception of logic falls squarely
within the scope of the philosophy of information.
A more indirect answer is related to the method of abstraction (the core
method in the philosophy of information [11, III]), and more precisely to the
fact that the relational conception of information can be used to motivate the
central role of logical discrimination. By adhering to the method of abstraction
we accept a form of pluralism that is based on making the relevant LoA explicit,
but we do not claim that anything goes. Indeed, because levels of abstraction can
be compared, we can require that one level of abstraction reﬁnes another level
of abstraction without making incompatible claims. This is exactly the situation
we have when we compare classical with non-classical logics: both relevant and
intuitionist logics provide a reﬁnement of classical logic, whereas classical logic
can be seen as the limiting case of each of them.
The interaction between the philosophy of logic and the philosophy of in-
formation also works in the opposite direction. Most standard presentations of
the method of abstraction suggest that diﬀerences in levels of abstraction are
best understood as diﬀerences in the non-logical vocabulary (e.g., the number of
non-logical predicates that are available), but the introduction of non-classical
logics in terms of additional logical distinctions suggests that diﬀerences in levels
of abstraction can equally well be understood as pure diﬀerences in logical dis-
crimination. As a result, the development of an informational conception of logic
challenges the standard presentation of non-classical logics by according more
importance to the notion of logical discrimination, and challenges the standard
presentation of the method of abstraction by making clear that even the under-
lying logic one adheres to is part of the LoA one assumes.
Acknowledgements. The author is a postdoctoral Fellow of the Research
Foundation – Flanders (FWO).

28
P. Allo
References
1. Aberdein, A., Read, S.: The philosophy of alternative logics. In: Haaparanta, L.
(ed.) The Development of Modern Logic, pp. 613–723. Oxford University Press,
Oxford (2009)
2. Allo, P., Mares, E.: Informational semantics as a third alternative? Erkenntnis,
1–19 (2011), http://dx.doi.org/10.1007/s10670-011-9356-1
3. Anderson, A.R., Belnap, N.D.: Entailment. The Logic of Relevance and Necessity,
vol. I. Princeton University Press, Princeton (1975)
4. Barwise, J.: Information and impossibilities. Notre Dame Journal of Formal
Logic 38(4), 488–515 (1997)
5. Barwise, J., Perry, J.: Situation and Attitudes. The David Hume Series of Philos-
ophy and Cognitive Science Reissues. CSLI Publications, Stanford (1999)
6. Beall, J.C., Restall, G.: Logical Pluralism. Oxford University Press, Oxford (2006)
7. Beall, J., Brady, R., Dunn, J., Hazen, A., Mares, E., Meyer, R., Priest, G., Restall,
G., Ripley, D., Slaney, J., Sylvan, R.: On the ternary relation and conditionality.
Journal of Philosophical Logic, 1–18 (May 2011) online First
8. van Benthem, J.: Logical dynamics meets logical pluralism? Australasian Journal
of Logic 6, 182–209 (2008)
9. van Benthem, J.: The information in intuitionistic logic. Synthese 167(2), 251–270
(2009)
10. Copeland, B.J.: On when a semantics is not a semantics: Some reasons for dis-
liking the Routley-Meyer semantics for relevance logic. Journal of Philosophical
Logic 8(1), 399–413 (1979), http://dx.doi.org/10.1007/BF00258440
11. Floridi, L.: The Philosophy of Information. Oxford University Press, Oxford (2011)
12. Gentzen, G.: Untersuchungen ¨uber das logische Schließen. I. Mathematische
Zeitschrift 39(1), 176–210 (1935)
13. Haack, S.: Deviant Logic. Some Philosophical Issues. Cambridge University Press,
Cambridge (1974)
14. Humberstone, I.L.: Logical discrimination. In: B´eziau, J.Y. (ed.) Logica Universalis,
pp. 207–228. Birkh¨auser Verlag, Basel (2005)
15. Mares, E.: Relevant logic and the theory of information. Synthese 109(3), 345–360
(1997)
16. Mares, E.: General information in relevant logic. Synthese 167(2), 343–362 (2009)
17. Paoli, F.: Substructural logics a primer. Trends in Logic: Studia Logica Library,
vol. 13. Kluwer Academic, Dordrecht (2002)
18. Restall, G.: Information ﬂow and relevant logics. In: Seligman, J., Westerst˚ahl,
D. (eds.) Logic, Language and Computation: The 1994 Moraga Proceedings, pp.
463–477. CSLI-Press, Stanford (1994)
19. Saguillo, J.M.: Methodological practice and complementary concepts of logical
consequence: Tarski’s model-theoretic consequence and Corcoran’s information-
theoretic consequence. History and Philosophy of Logic 30(1), 21–48 (2009)
20. Tarski, A.: On the concept of logical consequence. In: Tarski, A., Corcoran, J. (eds.)
Logic, Semantics, Meta-Matematics, 2nd edn., Hackett, Indianapolis (1983)
21. Wansing, H.: The Logic of Information Structures. LNCS (LNAI), vol. 681.
Springer, Berlin (1993)
22. Wansing, H.: Informational interpretation of substructural logics. Journal of Logic,
Language and Information 2, 285–308 (1993)

Robustness of Logical Depth
Lu´ıs Antunes1, Andre Souto2, and Andreia Teixeira1
1 Departamento de Ciˆencia de Computadores, Universidade do Porto,
R. Campo Alegre, 1021/1055, 4169-007 Porto, Portugal
2 Instituto de Telecomunica¸c˜oes, Instituto Superior T´ecnico, Universidade T´ecnica de
Lisboa, Torre Norte, Piso 10, Av. Rovisco Pais, 1 1049-001 Lisboa, Portugal
Abstract. Usually one can quantify the subjective notion of useful in-
formation in two diﬀerent perspectives: static resources – measuring the
amount of planing required to construct the object; or dynamic resources
– measuring the computational eﬀort required to produce the object. We
study the robustness of logical depth measuring dynamic resources, prov-
ing that small variations in the signiﬁcance level can cause large changes
in the logical depth.
1
Introduction
Philosophy and meaning of information has a long history, however recently
interest in this area has increased and researchers have tackled this from diﬀerent
approaches and perspectives, namely its meaning, quantiﬁcation and measures
of information and complexity. In this paper we are interested in measures of
meaningful or useful information. In the past there have been several proposals to
address this question: sophistication [8,9], logical depth [4], eﬀective complexity
[6], meaningful information [12], self-dissimilarity [13], computational depth [3],
facticity [1]. Pieter Adriaans [1] divided the several approaches to deﬁned a
string as interesting in: i) some amount of computation resources are required to
construct the object (Sophistication, Computational Depth). ii) exists a trade-
oﬀbetween the model and the data code under two part code optimization
(meaningful information, eﬀective complexity, facticity) and ﬁnally iii) it has
internal phase transitions (self-dissimilarity).
Solomonoﬀ[11], Kolmogorov [7] and Chaitin [5] independently deﬁned a rig-
orous measure of the information contained in an individual object x, as the
length of the shortest program that produces the object x. This measure is usu-
ally called Kolmogorov complexity of x and is denoted by K(x). A randomly
generated string, with high probability, has high Kolmogorov complexity, so con-
tains near maximum information. However by being randomly generated, makes
it unlikely to have useful information as we can obtain a similar one by ﬂipping
fair coins.
Usually one can quantify the subjective notion of useful information, as in item
i) previously deﬁned, in two diﬀerent perspectives: static resources – measuring
the amount of planning required to construct the object; or dynamic resources
– measuring the computational eﬀort required to produce the object.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 29–34, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

30
L. Antunes, A. Souto, and A. Teixeira
Regarding dynamic resources, the Kolmogorov complexity of a string x does
not take into account the time eﬀort necessary to produce the string from a de-
scription of length K(x). Bennett [4] called this eﬀort logical depth. Intuitively, a
computationally deep string takes a lot of computational eﬀort to be recovered
from its shortest description while shallow strings are trivially constructible from
their K(x), i.e., the shortest program for x does not require lots of computa-
tional power to produce it. After some attempts, Bennett [4] formally deﬁned
the s-signiﬁcant logical depth of an object x as the time required by a standard
universal Turing machine to generate x by a program p that is at most s bits
longer than its Kolmogorov complexity. Thus, an object is logically deep if it
takes a lot of time to be generated from any short description.
Bennett[4] claimed that the signiﬁcance level in logical depth is due to stability
reasons. In this paper we study its robustness, i.e., if small variations in the
signiﬁcance level can cause large changes in the logical depth. In this sense we
show that logical depth is not robust.
The rest of the paper is organized as follows: in the next section, we introduce
some notation, deﬁnitions and basic results needed for the comprehension of
the rest of the paper. In Section 3, we prove that logical depth is not a stable
measure.
2
Preliminaries
In this paper we use the binary alphabet Σ = {0, 1}, Σ∗= {0, 1}∗is the set
of all ﬁnite binary strings that are normally represented by x, y and z and Σn
and Σ≤n are the set of strings of length n and the set of strings of length at
most n, respectively. We denote the initial segment of length k of a string x with
length |x| by x[1:k] and its ith bit by xi. The function log will always mean the
logarithmic function of base 2. ⌊k⌋represents the largest integer smaller or equal
than k. All resource-bounds used in this paper are time constructible, i.e., there
is a Turing machine whose running time is exactly t(n) on every input of size
n, for some time t. Given a program p, we denote its running time by time(p).
Given two functions f and g, we say that f ∈O(g) if there is a constant c > 0,
such that f(n) ≤c · g(n), for almost all n ∈N.
2.1
Kolmogorov Complexity
We refer the reader to the book of Li and Vit´anyi [10] for a complete study on
Kolmogorov complexity.
Deﬁnition 1 (Kolmogorov complexity). Let U be a universal preﬁx-free
Turing machine. The preﬁx-free Kolmogorov complexity of x ∈Σ∗given y ∈Σ∗
is,
K(x|y) = min
p {|p| : U(p, y) = x}.
The t-time-bounded preﬁx-free Kolmogorov complexity of x ∈Σ∗given y ∈Σ∗
is,
Kt(x|y) = min
p {|p| : U(p, y) = x in at most t(|x|) steps}.

Robustness of Logical Depth
31
The default value for the axillary input y for the program p, is the empty string
ϵ and to avoid overloaded notation we usually drop this argument in those cases.
We choose as a reference universal Turing machine a machine that aﬀects the
running time of a program on any other machine by at most a logarithmic factor
and the program length by at most a constant number of extra bits.
Deﬁnition 2 (c-incompressible). A string x is c-incompressible if and only
if K(x) ≥|x| −c.
A simple counting argument can show the existence of c-incompressible strings.
In fact,
Theorem 1. There are at least 2n · (1 −2−c) + 1 strings x ∈Σn that are c-
incompressible.
Bennett [4] said that a string x is logically deep if it takes a lot of time to be
generated from any short description. The c-signiﬁcant logical depth of an object
x is the time that a universal Turing machine needs to generate x by a program
that is no more than c bits longer than the shortest description of x.
Deﬁnition 3 (Logical depth). Let x be a string, c a signiﬁcance level. A
string’s logical depth at signiﬁcance level c, is:
ldepthc(x) = min
p {time(p) : |p| ≤K(x) + c ∧U(p) = x} .
One can however, scale down the running time for program length by using a
Busy Beaver function, similar to the notion of Busy Beaver computational depth
introduced in [2].
Deﬁnition 4 (Busy Beaver function). The Busy Beaver function, BB, is
deﬁned by
BB : N →N
n →max
p:|p|≤n{running time of U(p) when deﬁned}
Deﬁnition 5 (Busy Beaver logical depth). The Busy Beaver logical depth,
with signiﬁcance level c, of x ∈Σn is deﬁned as:
ldepthBB
c
(x) = min
l
{∃p : |p| ≤K(x) + c and U(p) = x in time ≤BB(l)}
Notice that this is a rescaling of Deﬁnition 3, since BB−1(ldepthc(x)) =
ldepthBB
c
(x). From this new deﬁnition it is clear that ldepthBB
c
(x) ≤K(x) +
O(1). In fact, one can simulate any computation, keeping track of the amount of
computation steps and thus, K(time(p)) ≤|p|+O(1) which implies ldepthBB
c
(x)
≤K(x) + O(1).
Notice also that the Busy Beaver logical depth is a static measure based on
programs length.

32
L. Antunes, A. Souto, and A. Teixeira
3
Instability of Logical Depth
In this section we prove that even slightly changes of the constant on the signif-
icance level c of logical depth determines large variation of this measure.
Theorem 2. For every suﬃciently large n there are constants c, k1 and k2 and a
string x of length n such that ldepthk2(x) ≥2n and ldepth2c+k1(x) ≤O(n·log n).
Proof. Consider the following set:
A = {x ∈Σn : (∃p)|p| < n + K(n) −c ∧U(p) = x in time ≤2n}
Considering B = Σn −A, we know that B has at least 2n(1 −2−c) elements.
Let x ∈B such that n + K(n) −c −k1 ≤K(x) ≤n + K(n) −c −k2 for some
constants k1 and k2. We show that these strings exist in Lemma 3.1 bellow.
Thus,
– ldepthk2(x) ≥2n.
Assume that ldepthk2(x) < 2n, then, by deﬁnition of logical depth, there is a
program p of size at most K(x)+k2 ≤n+K(n)−c−k2+k2 = n+K(n)−c
such that U(p) = x in time < 2n, which implies that x would be an element
of A, which contradicts the choice of x.
– ldepth2c+k1(x) ≤O(n).
Since the signiﬁcance level is 2c+k1, then we can consider programs to deﬁne
its logical depth of length at least n+K(n)−c−k1+2c+k1 = n+K(n)+c (and
of course of length at most n+K(n)−c−k2+2c+k1 = n+K(n)+c+k1−k2).
So, if c is suﬃciently large to allow a preﬁx free version of the program print
to be one of the possible programs, then we conclude that ldepth2c+k1(x) is
at most the running time of print(x), which is at most O(n · log n).
Lemma 3.1. Let c be a constant and B be the set described in the last proof.
There are constants k1 and k2 and strings in B such that n −c −k1 ≤K(x) ≤
n −c −k2.
Proof. Consider the set S = {x ∈Σn : K(x) ≥n + K(n) −c −k1}.
It is easy to see that every element in S is in B. Let p be the program of size
≤n+ K(n)−c−a where a is a constant to be deﬁned later that has the longest
running time. Notice that K(p) ≥n + K(n) −c −a −l for some l. In fact, if
K(p) < n + K(n) −c −a −l for all l then we could consider the program that
runs p∗, the 1st program in the lexicographic order that produces p to obtain
p and then run it again and that would be a smaller program that would have
longer running time. More formally, consider the program q = RUN(·) where
RUN describes the universal Turing machine with some data and run it on U.
Since RUN is describable by a constant number of bit, say s, then, if the data
is p∗, |q| = |p∗| + s ≤n + K(n) −a −l + s ≤n + K(n) −a for suﬃciently
large l. Furthermore, timeU(q) ≥timeU(RUN(p∗)) = timeU(p∗) + timeU(p) ≥
timeU(p) which contradicts the choice of p.
Let t be the running time of p and let x be the ﬁrst string in the lexicographic
order such that Kt(x) ≥n + K(n). Thus,

Robustness of Logical Depth
33
– K(x) ≤K(p) + b ≤n + K(n) −c −a + b for some constant b since from p
we can compute t and then compute x.
– K(x) ≥n+K(n)−c−a. In fact, if K(x) < n+K(n)−c−a then considering
q the preﬁx-free program that witnesses the Kolmogorov complexity of x we
would have that |q| < n −c −a and then U(q) = x. Thus, by deﬁnition of
p we get time(q) < time(p) and hence Kt(x) ≤n + K(n) −a contradicting
the choice of x.
Just take a > b and also k1 = a and k2 = a −b.
Theorem 3. For every suﬃciently large there are constants c, k1 and k2 and a
string x of length n such that ldepthBB
k2 (x) ≥n and ldepthBB
2c+k1(x) ≤O(log n).
Proof. The idea is similar to Theorem 2. We rewrite the proof has the reasoning
of the conclusions changes a bit.
Consider the following set:
A = {x ∈Σn : (∃p)|p| < n + K(n) −c ∧U(p) = x in time ≤BB(n)}
Considering B = Σn −A, we know that B has at least 2n(1 −2−c) elements.
Let x ∈B such that n + K(n) −c −k1 ≤K(x) ≤n + K(n) −c −k2 for some
constants k1 and k2. Thus,
– ldepthBB
k2 (x) ≥n.
Assume that ldepthBB
k2 (x) < n, then, by deﬁnition of busy beaver logical
depth, there is a program p of size at most K(x)+k2 ≤n+K(n)−c−k2+k2 =
n + K(n) −c such that U(p) = x in time < BB(n), which implies that x
would be an element of A, contradicting the choice of x.
– ldepth2c+k1(x) ≤O(log n).
Since the signiﬁcance level is 2c+k1, then we can consider programs to deﬁne
its logical depth of length at least n+K(n)−c−k1+2c+k1 = n+K(n)+c (and
of course of length at most n+K(n)−c−k2+2c+k1 = n+K(n)+c+k1−k2).
So, if c is suﬃciently large to allow a preﬁx free version of the program print
to be one of the possible programs, then we conclude that ldepthBB
2c+k1(x) is at
most BB−1(time(print(x))) = BB−1(n log n) ≤O(log n) (since the busy
beaver function grows faster than any computable function, in particular
exponential).
We can adapt the argument presented above to prove that if we allow logarithmic
terms on the signiﬁcance level of logical depth we get a similar result.
Corollary 3.1. For every n and suﬃciently large there are constants c, k1
and k2 and a string x of length n such that ldepthBB
k2 log n(x)
≥
n and
ldepthBB
(2c+k1) log n(x) ≤O(log n).
Proof. The proof is equal to the previous one with the following adaptations:
A = {x ∈Σn : (∃p)|p| < n + K(n) −c log n ∧U(p) = x in time ≤BB(n)}

34
L. Antunes, A. Souto, and A. Teixeira
and with a similar reasoning to Lemma 3.1 we can show the existence of a
string in the complement of A satisfying n + K(n) −c log n −k1 log n ≤K(x) ≤
n + K(n) −c log n −k2 log n.
4
Conclusions
Our major contribution in this paper is the proof that the most commonly used
deﬁnition of logical depth in the literature is not stable, since small variations
in the signiﬁcance level can cause drastic changes in the value of Logical depth
even if we correct it with a Busy Beaver function.
Acknowledgments. We thanks Bruno Bauwens for helpful discussions and com-
ments. This work was supported by FCT projects PEst-OE/EEI/LA0008/2011
and PTDC/EIA-CCO/099951/2008.The authors are also supported by the grants
SFRH/BPD/76231/2011 and SFRH/BD/33234/2007 of FCT.
References
1. Adriaans, P.: Facticity as the amount of self-descriptive information in a data set
(2012)
2. Antunes, L., Fortnow, L.: Sophistication revisited. Theory of Computing Sys-
tems 45(1), 150–161 (2009)
3. Antunes, L., Fortnow, L., van Melkebeek, D., Vinodchandran, N.: Computational
depth: concept and applications. Theoretical Computer Science 354(3), 391–404
(2006)
4. Bennett, C.: Logical depth and physical complexity. In: A half-Century Survey on
the Universal Turing Machine, pp. 227–257. Oxford University Press, Inc., New
York (1988)
5. Chaitin, G.: On the length of programs for computing ﬁnite binary sequences.
Journal of ACM 13(4), 547–569 (1966)
6. Gell-Mann, M., Lloyd, S.: Information measures, eﬀective complexity, and total
information. Complexity 2(1), 44–52 (1996)
7. Kolmogorov, A.: Three approaches to the quantitative deﬁnition of information.
Problems of Information Transmission 1(1), 1–7 (1965)
8. Koppel, M.: Complexity, depth, and sophistication. Complex Systems 1, 1087–1091
(1987)
9. Koppel, M.: Structure. The Universal Turing Machine: A Half-Century Survey, 2nd
edn., pp. 403–419. Springer (1995)
10. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions. Springer (2008)
11. Solomonoﬀ, R.: A formal theory of inductive inference, Part I. Information and
Control 7(1), 1–22 (1964)
12. Vit´anyi, P.M.B.: Meaningful information. IEEE Transactions on Information The-
ory 52(10), 4617–4626 (2006)
13. Wolpert, D.H., Macready, W.: Using self-dissimilarity to quantify complexity. Com-
plexity 12(3), 77–85 (2007)

Turing’s Normal Numbers: Towards Randomness
Ver´onica Becher
Departamento de Computaci´on, Facultad de Ciencias Exactas y Naturales,
Universidad de Buenos Aires, Pabell´on I, Ciudad Universitaria, (1428) Buenos Aires,
Argentina
vbecher@dc.uba.ar
Abstract. In a manuscript entitled “A note on normal numbers” and
written presumably in 1938 Alan Turing gave an algorithm that produces
real numbers normal to every integer base. This proves, for the ﬁrst time,
the existence of computable normal numbers and it is the best solution
to date to Borel’s problem on giving examples of normal numbers. Fur-
thermore, Turing’s work is pioneering in the theory of randomness that
emerged 30 years after. These achievements of Turing are largely un-
known because his manuscript remained unpublished until its inclusion
in his Collected Works in 1992. The present note highlights Turing’s ideas
for the construction of normal numbers. Turing’s theorems are included
with a reconstruction of the original proofs.
1
On the Problem of Giving Instances of Normal
Numbers
The property of normality on real numbers, deﬁned by ´Emile Borel in 1909, is a
form of randomness. A real number is normal to a given integer base if its inﬁnite
expansion is seriously balanced: every block of digits of the same length must
occur with the same limit frequency in the expansion of the number expressed in
that base.1 For example, if a number is normal to base two, each of the digits ‘0’
and ‘1’ occur in the limit, half of the times; each of the blocks ‘00’, ‘01’, ‘10’ and ‘11’
occur one fourth of the times, and so on. A real number that is normal to every
integer base is called absolutely normal, or just normal. Borel proved that almost
all real numbers are normal (that is, the set of normal numbers has Lebesgue
measure 1), and he asked for an explicit example. Since then it has been easier
to conjecture results on normality than to prove them. In particular, it remains
unproved whether the fundamental mathematical constants such as π,
√
2 and e
are normal to some integer base. Although its has been proved that there exist
numbers that are normal to one base but not to another [9,26], no examples
have been given. There are already many particular constructions of numbers
1 An alternative characterization proves that a real number x is normal to a base b if,
and only if, the sequence (xbn)n≥1 is uniformly distributed modulo one [6]. Also, a
real number is normal to a base b if, and only if, its expansion is compressible by no
information lossless ﬁnite automaton (injective transducer) [22,18,7].
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 35–45, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

36
V. Becher
that are normal to a given base, but no explicit instance has been proved normal
to two multiplicatively independent bases; see [6] for up to date references.
It is fair to say that Borel’s question on providing an example of a normal
number (normal to every integer base) is still unresolved because the few known
instances are not completely satisfactory: it is desirable to show that a known
irrational number is normal, or, at least, to exhibit the number explicitly. We
would like an example with a simple mathematical deﬁnition and such that, in
addition of normality, some extra properties are proved. Considering that com-
putability is the acceptable notion of constructiveness since the 1930s, we would
also like that the number be easily computable. Let us recall that, as deﬁned
by Turing [24], the computable real numbers are those whose expansion in some
integer base can be generated by a mechanical (ﬁnitary) method, outputting
each of its digits, one after the other.
There is no evident reason for the normal numbers to have a non-empty in-
tersection with the computable numbers. A measure-theoretic argument is not
enough to see that these two sets intersect: the set of normal numbers in the unit
interval has Lebesgue measure one, but the computable numbers are just count-
able, hence they form a null set (Lebesgue measure 0). Indeed, there are com-
putable normal numbers, and this result should be attributed to Alan Turing.
His manuscript entitled “A note on normal numbers”, presumably written in
1938, presents the best answer to date to Borel’s question: an algorithm that
produces normal numbers. This early proof of existence of computable normal
numbers remained largely unknown because Turing’s manuscript was only pub-
lished in 1997 in his Collected Works, edited by J.L.Britton [25]. The editorial
notes say that the proof given by Turing is inadequate and speculate that the the-
orem could be false. In [1] we reconstructed and completed Turing’s manuscript,
trying to preserve his ideas as accurately as possible and correcting minor errors.
The very ﬁrst examples of normal numbers were independently given by Henri
Lebesgue and Waclaw Sierpi´nski2 in 1917 [16,23]. They also lead to computable
instances by giving a computable reformulation of the original constructions [2].
Together with Turing’s algorithm these are the only known constructions of com-
putable normal numbers. In his manuscript, Turing alerts the reader that the pro-
vided examples of normal numbers are not convenient and he explicitly says that
one would like that the expansion of such numbers be actually exhibited. From
his wording we suppose that he was aware of the problem that the n-th digit in
the expansion of a number output by his algorthm is deﬁned by exponentially
many operations in n. Actually, a literal reading of Turing’s algorithm yields that
at most simple-exponentially many operations suﬃce. Our reconstruction wors-
ens this amount to double-exponentially many, due to a modiﬁcation we had to
introduce in one expression that Turing wrote without a proof (see Section 2.2).
A theorem of Strauss [27] asserts that normal numbers computable in simple ex-
ponential time do exits, but this existential result yields no speciﬁc instances.
2 Both published their works in the same journal issue, but Lebesgue’s dates back
to 1909, immediately after Borel’s question.

Turing’s Normal Numbers: Towards Randomness
37
There are two other published constructions of normal numbers, one due
to W.M.Schmidt in 1962 [26], the other to M.B.Levin in 1979 [17], but it is
still unproved whether they yield computable numbers. Bugeaud in [5] demon-
strated the existence of Liouville numbers that are normal. It is an open problem
whether there are computable instances. Other non constructive examples of nor-
mal numbers follow from the theory of algorithmic randomness (recent reference
books are [10,20]; for an overview see [11] in this volume). Since randomness
implies normality, the particular real numbers that have been proved random
are, therefore, normal. For instance, Chaitin’s Omega numbers [8], the halting
probabilities of optimal Turing machines with preﬁx-free domain. But random
numbers are not computable, so Omega numbers are not the desired examples.3
2
Turing’s Construction of Normal Numbers
In his manuscript Turing proves two theorems. Here we discuss the main ideas
and include the proofs in accordance to our reconstruction in [1] of the original.
We intend our curent presentation to be simpler and more readable. Theorem
1 is a computable version of Borel’s fundamental theorem that establishes that
almost all real numbers, in the sense of Lebesgue measure, are normal [3]. The
theorem gives a construction of a set of real numbers as the limit of computably
deﬁnable ﬁnite approximations. This set has arbitrarily large measure and con-
sists only of normal numbers. This construction is valuable in its own right.
Turing’s Theorem 1. There is a computable function c(k, n) of two integer
variables with values consisting of ﬁnite sets of pairs of rational numbers such
that, for each k and n, if Ec(k,n) = (a1, b1)∪(a2, b2)∪...(am, bm) denotes the ﬁnite
union of the intervals whose rational endpoints are the pairs given by c(k, n), then
Ec(k,n) is included in Ec(k,n−1) and the measure of Ec(k,n) is greater than 1−1/k.
And for each k, E(k) = 
n Ec(k,n) has measure 1 −1/k and consists entirely of
normal numbers.
In Theorem 2 Turing gives an algorithm to output the expansion of a normal
number in base two. The proof relies on the construction in Theorem 1. The
algorithm is a computable functional: it receives an integer value that acts as a
parameter to control measure, and an inﬁnite sequence ν in base two to be used
as an oracle to possibly determine some digits of the output sequence. When ν
is a computable sequence (Turing puts the sequence of all zeros), the algorithm
yields a computable normal number. With this result Turing is the ﬁrst one to
prove the existence of computable normal numbers.
Turing’s Theorem 2. There is an algorithm that, given an integer k and an
inﬁnite sequence ν of zeros and ones, produces a normal number α(k, ν) in the
unit interval, expressed in base two, such that in order to write down the ﬁrst n
3 The family of Omega numbers coincides with the family of random real numbers that
can be approximated by a computable non-decreasing sequence of rationals [14].

38
V. Becher
digits of α(k, ν) the algorithm requires at most the ﬁrst n digits of ν. For a ﬁxed k
these numbers α(k, ν) form a set of measure at least 1 −2/k.
The algorithm can be adapted to intercalate the bits of the input sequence ν
at ﬁxed positions of the output sequence. Thus, one obtains non-computable
normal numbers in each Turing degree.
Notation. For an integer base b ≥2, a digit in base b is an element in {0, ..., b−
1}, and a block in base b a ﬁnite sequence of digits in base b. |u| is the length of
a block u, and u[i..i + r −1] is the inner block of r consecutive digits in a block
u starting at position i, for 1 ≤i ≤|u| −r + 1. A block w occurs in a block u
at position i if u[i...i + |w| −1] = w. The set of all blocks of length r in base
b is denoted by {0, ..., (b −1)}r. For each real number x in the unit interval we
consider the unique expansion in base b of the form x = ∞
i=1 aib−i, where the
integers 0 ≤ai < b, and ai < b −1 inﬁnitely many times. This last condition
over an is introduced to ensure a unique representation of every rational number.
When the base b is ﬁxed, we write x[i..i + r −1] to denote the inner block of
length r in the expansion of x in base b, starting at position i. We write μ to
denote Lebesgue measure.
Turing uses the following deﬁnition of normality, given by Borel in [4] as a
characterising property of normal numbers.
Deﬁnition 1 (Normality). For a real number x and an integer base b ≥2, the
number of occurrences of a given block w in the ﬁrst k digits of the expansion of
x in base b is S(x, b, w, k) = #{i : 1 ≤i ≤k−|w|+1 and x[i..i+|w|−1] = w}.
The number x is normal to base b if for every block w, limk→∞
S(x,b,w,k)
k
= b−|w|.
If x is normal to every base b ≥2 then we say x is normal.
2.1
Turings’s Theorem 1: A Construction via Finite Approximations
The main idea in Turing’s Theorem 1 is the construction of a set of normal numbers
of arbitrarily large measure, via ﬁnite approximations. This is done by pruning the
unit interval by stages such that, at the end, one obtains the desired set consist-
ing only of normal numbers. The construction is uniform on a parameter k, whose
only purpose is to establish the measure of the constructed set E(k) to be exactly
1 −1/k. At each stage n the construction is a ﬁnite set of intervals with rational
endpoints determined by a computable function c(k, n). At the initial stage 0, the
set Ec(k,0) is the whole unit interval. At stage n, the set Ec(k,n) is the ﬁnite approx-
imation to E(k) that results from removing from Ec(k,n−1) the points that are not
candidates to be normal, according to the inspection of an initial segment of their
expansions. At the end of this inﬁnite process all rational numbers are discarded,
because of their periodic structure. All irrational numbers with an unbalanced ex-
pansion are discarded. But also many normal numbers may be discarded, because
their initial segments remain unbalanced for too long.
The construction covers all initial segment sizes, all bases, and all blocks
by increasing computable functions of the stage n. And it has a decreasing

Turing’s Normal Numbers: Towards Randomness
39
bound on the acceptable discrepancy between the actual number of blocks in
the inspected initial segments and the perfect number of blocks expected by the
property of normality. These functions (initial segment size, base, block length
and discrepancy) are such that, at each stage n, the set of discarded numbers has
a small measure. The set E(k), obtained in the limit of the construction, is the
countable intersection of the sets Ec(k,n) and consists just of normal numbers.
The proof of Theorem 1 depends on a constructive version of the strong law
of large numbers: for each base there are a few blocks with too many or too few
occurrences of any given shorter block. The expected number of occurrences of
a given digit in a block of length k is k/b plus or minus a small fraction of k. An
upper bound for the number of blocks of length k having the expected occur-
rences of a given digit is proved in Hardy and Wright’s book4 [12], Theorem 148
(also in many books as [6,13,15]).
Deﬁnition 2. The number of blocks of length k in base b where a given block of
r digits occurs exactly i times is pb,r(k, i).
In particular, the number of blocks of length k with exactly i occurrences of a
given digit is pb,1(k, i) =
k
i

(b −1)k−i.
Lemma 1. Fix a base b ≥2 and a block length k > 6b. For every real number ε
such that 6/k ≤ε ≤1/b,

i: |i−k/b|≥εk
pb,1(k, i) < 2 bke−bε2k/6.
Turing extends this result to count occurrences of blocks instead of digits.
Lemma 2 corresponds to our reconstruction in [1] where we give the full proof.
The upper bound used by Turing in his manuscript is smaller but unproved.
Lemma 2. Let base b ≥2 and and let k and r be block lengths such that k > r.
For every real number ε such that 6/⌊k/r⌋≤ε ≤1/br,

i: |i−k/br|≥εk
pb,r(k, i) < 2 bk+2r−2r e−brε2k/6r.
Lemma 2 provides a lower bound for the measure of the set of real numbers that
are candidates to be normal based upon inspection of an initial segment of their
expansion in ﬁnitely bases. In the following we deﬁne A(ε, T, L, k) as the set of
real numbers such that their initial segment of size k in each base up to T has
a discrepancy of frequency below ε for each block of length up to L.
Deﬁnition 3. For a real value ε and integer values T , L and k, let
A(ε, T, L, k) =

2≤b≤T

1≤r≤L

w∈{0,...,b−1}r
{x ∈(0, 1) : |S(x, b, w, k) −k/br| < εk}.
Observe that A(ε, T, L, k) is a ﬁnite union of intervals with rational endpoints.
4 Since the ﬁrst edition of Introduction to the Theory of Numbers was in 1938 we
suppose the material was taught by G.H.Hardy in King’s College Cambridge at the
time Turing was a student.

40
V. Becher
Proposition 1. If 6/⌊k/L⌋≤ε ≤1/T L, μA(ε, T, L, k) ≥1−2L T 3L−1e−ε2k/3L.
Proof. By Deﬁnition 3, the complement of A(ε, T, L, k) in the unit interval is
A(ε, T, L, k)=

2≤b≤T

1≤r≤L

w∈{0,...,b−1}r
B(ε, b, w, k), where the set B(ε, b, w, k) =
{x ∈(0, 1) : |S(x, b, w, k) −k/br| ≥εk}. Observe that if a number x belongs to
B(ε, b, w, k) then so does each y such that x[1..k] = y[1..k]. Then, the interval
[0.x[1..k]000..., 0.x[1..k](b−1)(b−1)(b−1)...], which has measure b−k, is included
in B(ε, b, w, k). Recall that pb,r(k, i) (cf. Deﬁnition 2) is the number of diﬀerent
blocks of length k in which a given block of length r occurs exactly i times. Let-
ting the block length r = |w| we have μB(ε, b, w, k) ≤b−k

i: |i−k/br|≥εk
pb,r(i, k).
Applying Lemma 2, μB(ε, b, w, k) < 2 b2r−2r e−brε2k/6r. Since 1 ≤r ≤L,
2r/L ≤2 ≤br. Then, ε2k/3L ≤brε2k/6r. This gives a uniform upper bound
μB(ε, w, b, k) < 2 T 2L−2L e−ε2k/3L for all b, r, w such that 2 ≤b ≤T , 1 ≤r ≤L
and w ∈{0, ..., b−1}r. Thus, μA(ε, T, L, k) ≥

2≤b≤T

1≤r≤L

w∈{0,...,b−1}r
μB(ε, b, w, k).
In the third sum there are br many blocks w. Using 
2≤b≤T

1≤r≤L br =

2≤b≤T
bL+1−1
b−1
≤T L+1, conclude μA(ε, T, L, k) < 2L T 3L−1e−ε2k/3L. The proof
is completed by taking the complement.
Turing deﬁnes the sets Ak as particular instances of the sets A(ε, T, L, k) where
ε, T and L are computable functions of the initial segment size k such that
ε(k) goes to 0 as k increases, and T (k), L(k) are increasing in k. Turing chose
the base T (k) to grow sub-linearly in k, and the block length L(k) to grow
sub-logarithmically in k, which would yield the maximum discrepancy ε(k) (ac-
cording to the bound of Lemma 1). Other assignments are possible.
Deﬁnition 4. Let Ak = A(ε, T, L, k) for L =
√
ln k/4, T = eL and ε = 1/T L.
Proposition 2. There is k0 such that for all k ≥k0, μAk ≥1 −1/k(k −1).
Proof. By Deﬁnition 4, L =
√
ln k/4, T = eL and ε = 1/T L. Assume k ≥2.
Then, 6/⌊k/L⌋≤ε. By Proposition 1, μAk ≥1 −2L T 3L−1e−ε2k/3L. To obtain
μAk ≥1 −1/k(k −1) it suﬃces to show 2LT 3L−1k2 ≤eε2k/3L, which can be
proved to hold for any k ≥1.
From now on let k0 be the value established in Proposition 2. Turing recursively
deﬁnes the set Ec(k,n) as a subset of Ak with measure exactly 1−1/k+1/(k+n).
Deﬁnition 5. Let c(k, n) be the function of two integer variables with values
in ﬁnite sets of pairs of rational numbers such that, for each k and n, Ec(k,n) =
(a1, b1) ∪(a2, b2) ∪...(am, bm) denotes the ﬁnite union of the intervals whose
rational endpoints are given by the pairs in the set c(k, n). For any k ≥k0 let
Ec(k,0) = (0, 1) and Ec(k,n+1) = Ak+n+1 ∩Ec(k,n) ∩(βn, 1) where (βn, 1) is an
interval such that μEc(k,n+1) = 1 −1/k + 1/(k + n + 1).

Turing’s Normal Numbers: Towards Randomness
41
The βn above necessarily exists, it is unique, and it is a rational number com-
putable from the two other sets in the deﬁnition. Both are a union of ﬁnitely
many intervals with rational endpoints, so their respective measure are com-
putable, and they are big enough.
Proof of Turing’s Theorem 1. We ﬁrst prove that 
k≥k0 Ak contains only nor-
mal numbers. By way of contradiction assume x ∈
k≥k0 Ak and x is not normal
to base b. Then, limk→∞
S(x,b,w,k)
k
̸=
1
br for some block w of length r. So, there is
δ > 0 and there are inﬁnitely many values k such that |S(x, b, w, k) −k/br| > kδ.
Let T (k), L(k) and ε(k) be the assignments of Deﬁnition 4 and ﬁx k1 ≥k0 large
enough such that T (k1) ≥b, L(k1) ≥r and ε(k1) ≤δ. This is always possible be-
cause T (k) and L(k) are increasing in k, and ε(k) goes to 0 as k increases. Then,
for each k ≥k1, x ∈Ak and by Deﬁnition 3, |S(x, b, w, k) −k/br| < k ε(k) ≤kδ,
a contradiction. E(k) ⊆
i≥k Ai for k ≥k0; therefore, all real numbers in E(k)
are normal. Since μEc(k,n) = 1 −1/k + 1/(k + n), μE(k) = limn→∞μEc(k,n) =
1 −1/k. This completes the proof.
2.2
Turing’s Theorem 2: An Algorithm to Output Normal Numbers
Turing’s algorithm is uniform in the parameter k and it receives as input an
inﬁnite sequence ν of zeros and ones. The algorithm works by stages. The main
idea is to split the unit interval by halves, successively. It starts with the whole
unit interval and at each stage it chooses either the left half or the right half
of the current interval. The sequence α(k, ν) of zeros and ones output by the
algorithm is the trace of the left/right selection at each stage. The invariant
condition of the algorithm is that the intersection of the current interval with
the set E(k) of normal numbers of Theorem 1 has positive measure. Since Ec(k,n)
is the ﬁnite approximation of E(k) at stage n, the algorithm chooses the half of
the current interval whose intersection with Ec(k,n) reaches a minimum threshold
of measure which avoids running out of measure at any later stage. In case both
halves reach this minimum, the algorithm uses the n-th symbol of the input
sequence ν to decide. The chosen intervals at successive stages are nested and
their measures converge to zero; therefore, their intersection contains exactly one
number. This is the sequence α(k, ν) output by the algorithm. The algorithm is
correct if the number denoted by α(k, ν) is normal to base two. This is proved
by induction on the stage n, the only non obvious part is the veriﬁcation of the
invariant condition.
Each sequence output by the algorithm has an explicit convergence to nor-
mality: in the initial segment of length ℓin each base up to base T (ℓ), all blocks
of length up to L(ℓ) occur with the expected frequency plus or minus at most
ε(ℓ), where L(ℓ) =
√
ln ℓ/4, T (ℓ) = eL and ε(ℓ) = e−L2 = k−1/16.
The time complexity of the algorithm is the number of needed operations
to produce the n-th digit of the output sequence α(k, ν). This just requires to
compute, at each stage n, the measure of the intersection of the current interval
with the set Ec(k,n). Turing gives no hints on properties of the sets Ec(k,n)
that could allow for a fast calculation. The naive way does the combinatorial

42
V. Becher
construction of Ec(k,n) in a number of operations exponential in n. Turing’s
algorithm verbatim would have simple-exponential time complexity, but we have
been unable to verify its correctness. In our reconstruction in [1] the number of
intervals we consider in Ec(k,n) is exponentially larger than in Turing’s literal
formulation, so we end up with double-exponential time complexity.
Proof of Turing’s Theorem 2. Let k be the integer parameter and ν the input
inﬁnite sequence of zeros and ones. We write α to denote the output sequence,
α(i) for its digit in position i. Similarly for ν. Redeﬁne the computable function
c(k, n) of Theorem 1 as follows. Assuming k is big enough, let Ec(k,0) = (0, 1)
and for n > 0, Ec(k,n) = Ak22n+1 ∩Ec(k,n−1) ∩(βn, 1), where (βn, 1) is an interval
such that μEc(k,n) = 1 −1/k + 1/k22n+1. Here is the algorithm:
Start with I0 = (0, 1). At stage n > 0,
Split the interval In−1 = (an−1, bn−1) into two halves
I0
n = (an−1, an−1+bn−1
2
) and I1
n = ( an−1+bn−1
2
, bn−1).
If μ(Ec(k,n) ∩I0
n) > 1/k22n and μ(Ec(k,n) ∩I1
n) > 1/k22n then
let α(n) = ν(n) and In = Iν(n)
n
.
Else if μ(Ec(k,n) ∩I1
n) ≤1/k22n then
let In = I0
n and α(n) = 0.
Else, let In = I1
n and α(n) = 1.
To show that α is normal, we prove α ∈E(k) = 
n Ec(k,n) by induction on n.
For n = 0, Ec(k,0) = (0, 1); so, μ(Ec(k,n) ∩I0) = 1 > 1/k. For n > 0, assume the
inductive hypothesis μ(Ec(k,n) ∩In) >1/k22n. Since the sets Ec(k,n) are nested
Ec(k,n+1) ∩In =
	
Ec(k,n) ∩In

\
		
Ec(k,n) \ Ec(k,n+1)

∩In

.
So, μ(Ec(k,n+1) ∩In) = μ(Ec(k,n) ∩In) −μ(
	
Ec(k,n) \ Ec(k,n+1)

∩In). Then,
μ(Ec(k,n+1) ∩In) ≥μ(Ec(k,n) ∩In) −μ(Ec(k,n) \ Ec(k,n+1)). Using the equality
μ(Ec(k,n) \ Ec(k,n+1)) = 1/k22n+1 −1/k22(n+1)+1 and the inductive hypothesis,
we obtain μ(Ec(k,n+1) ∩In) > 1/k22n −(1/k22n+1 −1/k22n+3) > 2/k22(n+1).
It is impossible that both μ(Ec(k,n+1)∩I0
n+1) and μ(Ec(k,n+1)∩I1
n+1) be less than
or equal to 1/k22(n+1). At least one of the sets Ec(k,n+1) ∩Ii
n+1, for i ∈{0, 1},
has measure greater than 1/k22(n+1). The algorithm picks as In+1 the set Ii
n+1
which fulﬁlls this condition. In case both verify it, the oracle is used to choose
left or right. By construction, the expansion of each real number in Ec(k,n) ∩In
starts with α(0) α(1)...α(n).
We now prove that for a ﬁxed k, the set of output numbers α(k, ν) for all
possible inputs ν has measure at least 1 −2/k. Turing bounds the measure of
the unqualiﬁed intervals up to stage n, as the n ﬁrst bits of the sequence ν run
through all possibilities. Let Im =
	
m
2n+1 , m+1
2n+1

, for m = 0, 1, ..., 2n+1 −1. The
algorithm discards the interval Im when μ(Ec(k,n) ∩Im) ≤1/k22n. The set of
intervals that are not discarded is recursively deﬁned as follows. Let M(k, 0) =
(0, 1) and for n > 0, let M(k, n + 1) be the union of the intervals Im such that
Im ⊆M(k, n) and μ(Ec(k,n) ∩Im)>1/k22n. Then, μ(E(k) ∩M(k, n + 1)) equals
μ(E(k) ∩M(k, n)) −
2n−1

m=0
μ(E(k) ∩(M(k, n) \ M(k, n + 1)) ∩
 m
2n , m + 1
2n

).

Turing’s Normal Numbers: Towards Randomness
43
Each term in the sum is at most 1/k22n. Therefore, μ(E(k) ∩M(k, n + 1)) ≥
μ(E(k) ∩M(k, n)) −1/k2n. Applying this inequality recursively n times, we
get μ(E(k) ∩M(k, n + 1)) ≥μ(E(k) ∩M(k, 1)) −1/k n
i=1 1/2n. Finally, since
Ec(k,0) = (0, 1) and k ≥2, M(k, 1) = (0, 1
2) ∪( 1
2, 1); so, E(k) ∩M(k, 1) = E(k).
Then, μ(E(k) ∩
n M(k, n)) > μE(k) −1/k. Using that μE(k) = 1 −1/k,
conclude that E(k) ∩
n M(k, n) has measure at least 1 −2/k.
3
Towards the Theory of Algorithmic Randomness
Turing’s manuscript conveys the impression that he had the insight, ahead of
his time, that traditional mathematical concepts speciﬁed by ﬁnitely deﬁnable
approximations, such as measure or continuity, could be made computational.
This point of view has developed under the general name of eﬀective mathemat-
ics, a part of which is algorithmic randomness. From the modern perspective,
Turing’s construction of the set of normal numbers in Theorem 1, done via ﬁnite
approximations, is an instance of a fundamental entity in the theory of algorith-
mic randomness: a Martin-L¨of test 5 [19]. Intuitively, a real number is random
when when it exhibits the almost-everywhere behavior of all reals, for example
its expansion has no predictable regularities. A random real number must pass
every test of these properties. Martin-L¨of had the idea to focus just in properties
deﬁnable in terms of computability: a test for randomness is a uniformly com-
putably enumerable sequence of sets whose measure converges to zero. A real
number is random if it is covered by no such test. That is to say that it has
the almost-everywhere property of avoiding the measure-zero intersection. This
deﬁnition turned out to be equivalent to the deﬁnition of randomness in terms
of description complexity [8]. The equivalence between the two been taken as a
sign of robustness of the deﬁned notion of randomness.
Deﬁnition 6. 1.
A Martin-L¨of randomness test, hereafter ML-test, is a uni-
formly computably enumerable sequence (Vi)i≥0 of sets of intervals with rational
endpoints such that, for each i, μVi ≤2−i.
2. A real number x is random if for every ML-test (Vi)i≥0, x ̸∈
i≥0 Vi.
Turing’s set E(k) of Theorem 1 leads immediately to a ML-test6. Hence, it
provides a direct proof that randomness implies normality.
Corollary 1. The sequence (Vk)k≥0 = ((0, 1) \ E(2k))k≥0 is a ML-test.
Proof. By Theorem 1, E(k) = 
n≥1 Ec(k,n), where c(k, n) is computable and
for each k and n, Ec(k,n) is a ﬁnite set of intervals with rational endpoints.
So, the complement of each Ec(k,n) is also a ﬁnite set of intervals with rational
5 Martin-L¨of presented the test in terms of sequences of zeros and ones. We give here
an alternative formulation in terms of sets of intervals with rational endpoints.
6 In fact, Theorem 1 yields a Schnorr test [21]. This is a ML-test where µVi is com-
putable uniformly in i. The notion is unchanged if we, instead, let a Schnorr test be
a ML-test such that µVi = 2−i, for each i ≥0.

44
V. Becher
endpoints. Then, (0, 1) \ E(k) = 
n≥1(0, 1) \ Ec(k,n) is computably enumerable.
Since Turing’s construction is uniform in the parameter k, ((0, 1) \ E(k))k≥0 is
uniformly computably enumerable. Finally, since the measure of E(k) is 1−1/k,
μ((0, 1) \ E(k)) = 1/k. Thus, (Vk)k≥0 = ((0, 1) \ E(2k))k≥0 is a ML-test.
Corollary 2. Randomness implies normality.
Proof. If x is not normal then, by Theorem 1, x belongs to no set E(k), for
any k. So, x ∈
k≥0(0, 1) \ E(k). By Corollary 1, (Vk)k≥0 = ((0, 1) \ E(2k))k≥0
is a ML-test. Hence, x ∈
k≥0 Vk; therefore, x is not random.
References
1. Becher, V., Figueira, S., Picchi, R.: Turing’s unpublished algorithm for normal
numbers. Theoretical Computer Science 377, 126–138 (2007)
2. Becher, V., Figueira, S.: An example of a computable absolutely normal number.
Theoretical Computer Science 270, 947–958 (2002)
3. Borel, ´E.: Les probabilit´es d´enombrables et leurs applications arithm´etiques. Ren-
diconti del Circolo Matematico di Palermo 27, 247–271 (1909)
4. Borel, ´E.: Le¸cons sur la th`eorie des fonctions, 2nd edn., Gauthier Villars (1914)
5. Bugeaud, Y.: Nombres de Liouville et nombres normaux. Comptes Rendus de
l’Acad´emie des Sciences de Paris 335, 117–120 (2002)
6. Bugeaud, Y.: Distribution Modulo One and Diophantine Approximation. Cam-
bridge University Press (2012)
7. Bourke, C., Hitchcock, J., Vinodchandran, N.: Entropy rates and ﬁnite-state di-
mension. Theoretical Computer Science 349(3), 392–406 (2005)
8. Chaitin, G.: A theory of program size formally identical to information theory.
Journal ACM 22, 329–340 (1975)
9. Cassels, J.W.S.: On a paper of Niven and Zuckerman. Paciﬁc Journal of Mathe-
matics 2, 555–557 (1952)
10. Downey, R., Hirschfeldt, D.: Algorithmic Randomness and Complexity. Springer
(2010)
11. Downey, R.: Randomness, Computation and Mathematics. In: Cooper, S.B.,
Dawar, A., L¨owe, B. (eds.) CiE 2012. LNCS, vol. 7318, pp. 163–182. Springer,
Heidelberg (2012)
12. Hardy, G.H., Wright, E.M.: An Introduction to the Theory of Numbers, 1st edn.
Oxford University Press (1938)
13. Harman, G.: Metric Number Theory. Oxford University Press (1998)
14. Kuˇcera, A., Slaman, T.: Randomness and recursive enumerability. SIAM Journal
on Computing 31(1), 199–211 (2001)
15. Kuipers, L., Niederreiter, H.: Uniform Distribution of Sequences. Dover (2006)
16. Lebesgue, H.: Sur certaines d´emonstrations d’existence. Bulletin de la Soci´et´e
Math´ematique de France 45, 132–144 (1917)
17. Levin, M.B.: On absolutely normal numbers. English translation in Moscow Uni-
versity Mathematics Bulletin 34, 32–39 (1979)
18. Dai, L., Lutz, J., Mayordomo, E.: Finite-state dimension. Theoretical Computer
Science 310, 1–33 (2004)
19. Martin-L¨of, P.: The Deﬁnition of Random Sequences. Information and Con-
trol 9(6), 602–619 (1966)

Turing’s Normal Numbers: Towards Randomness
45
20. Nies, A.: Computability and Randomness. Oxford University Press (2009)
21. Schnorr, C.-P.: Zuf¨alligkeit und Wahrscheinlichkeit. In: Eine algorithmische
Begr¨undung
der Wahrscheinlichkeitstheorie.
Lecture Notes in
Mathematics,
vol. 218. Springer, Berlin (1971)
22. Schnorr, C.-P., Stimm, H.: Endliche Automaten und Zufallsfolgen. Acta Informat-
ica 1, 345–359 (1972)
23. Sierpi´nski, W.: D´emonstration ´el´ementaire du th´eor`eme de M. Borel sur les nom-
bres absolument normaux et d´etermination eﬀective d’un tel nombre. Bulletin de
la Soci´et´e Math´ematique de France 45, 127–132 (1917)
24. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society Series 2 42, 230–
265 (1936)
25. Turing, A.M.: A note on normal numbers. In: Britton, J.L. (ed.) Collected Works of
A.M. Turing: Pure Mathematics, pp. 263–265. North Holland, Amsterdam (1992);
with notes of the editor in 263–265
26. Schmidt, W.M.: On normal numbers. Paciﬁc Journal of Math. 10, 661–672 (1960)
27. Strauss, M.: Normal numbers and sources for BPP. Theoretical Computer Sci-
ence 178, 155–169 (1997)

Logic of Ruler and Compass Constructions
Michael Beeson
Department of Computer Science, San Jos´e State University, 208 MacQuarrie Hall,
San Jose, CA 95192-0249, United States of America
Abstract. We describe a theory ECG of “Euclidean constructive ge-
ometry”. Things that ECG proves to exist can be constructed with
ruler and compass. ECG permits us to make constructive distinctions
between diﬀerent forms of the parallel postulate. We show that Euclid’s
version, which says that under certain circumstances two lines meet (i.e.,
a point of intersection exists) is not constructively equivalent to the more
modern version, which makes no existence assertion but only says there
cannot be two parallels to a given line. Non-constructivity in geome-
try corresponds to case distinctions requiring diﬀerent constructions in
each case; constructivity requires continuous dependence on parameters.
We give continuous constructions where Euclid and Descartes did not
supply them, culminating in geometrical deﬁnitions of addition and mul-
tiplication that do not depend on case distinctions. This enables us to
reduce models of geometry to ordered ﬁeld theory, as is usual in non-
constructive geometry. The models of ECG include the set of pairs of
Turing’s constructible real numbers [7].
1
Introduction
Euclid’s geometry, written down about 300 BCE, has been extraordinarily in-
ﬂuential in the development of mathematics, and prior to the twentieth century
was regarded as a paradigmatic example of pure reasoning.
In this paper, we re-examine Euclidean geometry from the viewpoint of con-
structive mathematics. The phrase “constructive geometry” suggests, on the one
hand, that “constructive” refers to geometrical constructions with straightedge
and compass. On the other hand, the word “constructive” may suggest the use of
intuitionistic logic. We investigate the connections between these two meanings
of the word. Our method is to keep the focus on the body of mathematics in
Euclid’s Elements, and to examine what in Euclid is constructive, in the sense
of “constructive mathematics”. Our aim in the ﬁrst phase of this research was
to formulate a suitable formal theory that would be faithful to both the ideas
of Euclid and the constructive approach of Errett Bishop [4]. We achieved this
aim by formulating a theory ECG of “Euclidean constructive geometry”, ﬁrst
presented in [2], but improved in [3].
In constructive mathematics, if one proves something exists, one has to show
how to construct it. In Euclid’s geometry, the means of construction are not
arbitrary computer programs, but ruler and compass. Therefore it is natural
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 46–55, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Logic of Ruler and Compass Constructions
47
to look for a theory that has function symbols for the basic ruler-and-compass
constructions. The terms of such a theory correspond to ruler-and-compass con-
structions. Our ﬁrst main result is that when ECG proves that something exists,
that something can be constructed with ruler and compass.
In number theory, if one proves an existence theorem, then for a constructive
version, one has to show how to compute the desired number as a function of the
parameters. In analysis, if one proves an existence theorem, one has to be able
to compute approximations to the desired number from approximations to the
parameters. In particular, the solution will depend continuously on parameters,
at least locally. This feature of constructive analysis depends, in a way, on what
we think it means “to be given” a number x. Whatever that may mean, it surely
means that we have a way to get a rational approximation to x within any speci-
ﬁed limit of accuracy. Geometry is more like analysis than number theory, in the
sense that we do not want to assume in advance that points can be given to us all
at once in a completely determined location; points are given only approximately,
by dots on paper or a computer screen, or in Euclid’s case, by indentations in
sand (the Greeks drew their diagrams in sand). It might be doubtful whether
two such points coincide; in such a case one would have to ask the one who made
the diagram to reﬁne it. It follows that in constructive geometry, we should have
local continuous dependence of constructions on parameters. We can see that
dramatically in computer animations of Euclidean constructions, in which one
can select some of the original points and drag them, and the entire construction
“follows along.” One might formulate a program of “continuous geometry”, in
which one allows only constructions that depend continuously on parameters. It
turns out that this is just another way of viewing constructive geometry, since
theorems proved without non-constructive case distinctions will be implemented
by continuous ruler-and-compass constructions. One line of research has thus
been to identify and repair uses of non-constructive case distinctions. There are
several important places where repair is needed, but it is possible. Thus the “C”
in ECG could just as well be read as “continuous”, instead of “constructive.”
Once we have a good formal theory of constructive geometry, the possibil-
ity opens up to prove independence results. Our most striking results concern
the diﬀerent formulations of Euclid’s parallel postulate. Euclid’s original version
(Euclid 5) is not the same as the version more commonly used today (Playfair’s
axiom). The diﬀerence is that Euclid 5 says that under certain conditions, two
lines must meet, while Playfair’s axiom says that there cannot be two diﬀerent
parallels to line L through point P. Thus Euclid 5 makes an existence assertion,
but Playfair does not. We prove that Playfair does not imply Euclid 5 in ECG
minus the parallel axiom of ECG.
In classical (i.e., nonconstructive) geometry, there are theorems that show
that models of geometrical theories all have the form F 2, where F is an ordered
ﬁeld, and the geometrical relations of incidence and betweenness are deﬁned as
usual in analytic geometry. Diﬀerent geometric axioms correspond to diﬀerent
axioms in ordered ﬁeld theory. When the geometric axioms are those for ruler and
compass, we get Euclidean ﬁelds (those in which positive elements have square

48
M. Beeson
roots). We show that this paradigm extends to constructive geometry as well.
This is not trivial, because we need to give geometrical deﬁnitions of addition
and multiplication (of segments, or of points on a line) that are continuous in
parameters, in particular, do not require case distinctions about the sign to
construct the sum and product.
Once that is done, we move on to consider the models F 2, and we ﬁnd that
there are three diﬀerent possible deﬁnitions of “constructive Euclidean ﬁeld”.
The diﬀerence hinges on when the reciprocal 1/x is deﬁned: either positive ele-
ments have reciprocals, or nonzero elements have reciprocals, or elements without
reciprocals are zero. To prove two of these three versions equivalent requires ei-
ther proof by contradiction or non-constructive case distinctions. Each of these
versions corresponds to geometry, with a diﬀerent version of the parallel axiom.
We obtain our independence proofs by constructing models of one kind of
constructive ordered ﬁeld theory that do not satisfy the next stronger kind.
Of course these are not ﬁelds in the usual sense, because these ﬁeld theories
are non-constructively equivalent; they are what is known as Kripke models.
Their construction involves taking the ﬁeld elements to be certain real-valued
functions, corresponding to points whose location is not yet “pinned down.”
This short paper omits proofs, discussion of past axiomatizations of geometry,
philosophical discussions, and detailed discussions of the relations between this
work and the work of others. All these things can be found in [3]. Heartfelt thanks
to Marvin Greenberg and Freek Wiedijk for their comments and suggestions.
2
Is Euclid’s Reasoning Constructive?
Euclid’s reasoning is generally constructive; indeed the only irreparably non-
constructive proposition is Book I, Prop. 2, which shows that the rigid compass
can be simulated by a collapsible compass. We just take Euclid I.2 as an axiom,
thus requiring a rigid compass in ECG. Only one other repair is needed, in the
formulation of the parallel axiom, as we shall see below. Euclid did not deal with
disjunctions explicitly, and all his theorems are of the form: Given certain points
related in certain ways, we can construct (zero or more) other points related to
the given points and each other in certain ways. Euclid has been criticized (as far
back as Geminus and Proclus) for ignoring case distinctions in a proof, giving a
diagram and proof for only one case. Since case distinctions (on whether ab = cd
or not) are non-constructive, these omissions are prima facie non-constructive.
However, these non-constructive proof steps are eliminable, as we shall explain.
An example of such an argument in Euclid is Prop. I.6, whose proof begins
Let ABC be a triangle having the angle ABC equal to the angle ACB. I
say that the side AB is also equal to the side AC. For, if AB is unequal
to AC, one of them is greater. Let AB be greater, . . .
The same proof also uses an argument by contradiction in the form ¬x ̸= y →
x = y. This principle, the “stability of equality”, is an axiom of ECG, and is
universally regarded as constructively acceptable. The conclusion of I.6, however,

Logic of Ruler and Compass Constructions
49
is negative (has no ∃or ∨), so we can simply put double negations in front of
every step, and apply the stability of equality once at the end.
Prop. I.26 is another example of the use of the stability of equality: “. . . DE
is not unequal to AB, and is therefore equal to it.”
To put the matter more technically, in constructive logic we have P
→
¬¬P, and although generally we do not have ¬¬P
→
P, we do have it for
quantiﬁer-free, disjunction-free P. We can double-negate A∨¬A →B, obtaining
¬¬(A ∨¬A) →¬¬B, and then the hypothesis is provable, so we have ¬¬B,
and hence B since B is quantiﬁer-free and disjunction-free. The reason why
this works throughout Euclid is that the conclusions of Euclid’s theorems are
all quantiﬁer-free and disjunction-free. Euclid never even thought of stating a
theorem with an “or” in it. The bottom line is that Euclid is constructive as
it stands, except for Book I, Prop. 2, and the exact formulation of the parallel
postulate. These problems are remedied in ECG by taking Book I, Prop. 2
as an axiom and strengthening the parallel postulate as discussed below. We
also take as an axiom ¬¬B(x, y, z)
→
B(x, y, z), or “Markov’s principle for
betweenness”, enabling us to drop double negations on atomic sentences.
3
The Elementary Constructions
The Euclidean, or “elementary” constructions, are carried out by constructing
lines and circles and marking certain intersection points as newly constructed
points. The geometrical theory ECG given in [2] has terms to denote the geo-
metrical constructions. These terms can sometimes be “undeﬁned”, e.g., if two
lines are parallel, their intersection point is undeﬁned. Therefore ECG is based
on the logic of partial terms LPT [1, p. 97], in which there are atomic formulas
t ↓expressing that term t has a denotation (“is deﬁned”). A model of such a
theory can be regarded as a many-sorted algebra with partial functions repre-
senting the basic geometric constructions. Speciﬁcally, the sorts are Point , Line ,
and Circle. We have constants and variables of each sort.
ECG includes function symbols for the basic constructors and accessors, such
as Line (A, B) for the line through A and B and Circle (A, B) for the circle
through B with center A, and for the “elementary constructions” (each of which
has type Point ):
IntersectLines (Line K, Line L)
IntersectLineCircle1 (Line L, Circle C)
IntersectLineCircle2 (Line L, Circle C)
IntersectCircles1 (Circle C, Circle K)
IntersectCircles2 (Circle C, Circle K)
One can regard circles and lines as mere intermediaries; points are ultimately
constructed from other points. (This was proved in [2].)
There is a second constructor for circles, which we can describe for short
as “circle from center and radius”, as opposed to the ﬁrst constructor above,

50
M. Beeson
“circle from center and point.” Speciﬁcally Circle3 (A, B, C) constructs a circle
of radius BC and center A, provided B ̸= C. These two constructors for circles
correspond to a “collapsible compass” and a “rigid compass” respectively. The
compass of Euclid was a collapsible compass: you cannot use it to “hold” the
length BC while you move one point of the compass to A. You can only use it to
hold the radius AB while one point of the compass is ﬁxed at A, so in that sense
it corresponds to Circle (A, B). The second constructor Circle3 corresponds to a
rigid compass. The theory ECG includes Circle3 , and in [2] we gave reasons why
constructive geometry demands a rigid, rather than only a collapsible compass.
In short, without a rigid compass, one cannot project a point P onto a line L,
without making a case distinction between the case when P lies on L and the case
when it does not; and the ability to make such projections is crucial to deﬁning
a coordinate system and showing how to perform addition and multiplication on
segments.
There are three issues to decide:
– when there are two intersection points, which one is denoted by which term?
– In degenerate situations, such as Line (P, P), what do we do?
– When the indicated lines and/or circles do not intersect, what do we do
about the term(s) for their intersection point(s)?
Our answers to these questions are as follows. When the indicated lines or circles
do not intersect, then the term for their intersection is “undeﬁned”. This can
best be handled formally using the logic of partial terms, which we do in ECG;
it can also be handled in other more cumbersome ways without modifying ﬁrst-
order logic. We take Circle (P, P) to be deﬁned, i.e., we allow circles of zero
radius; that technicality makes the formal development smoother and seems
philosophically unobjectionable–we just allow the two points of the compass
to coincide. The point here is not so much that circles of zero radius are of
interest, but that we do not want to force a case distinction as to whether the
two points of the compass are, or are not, coincident. We take the two points of
intersection of a line Line (A, B) and a circle to occur in the same order as A
and B occur on L. That means that lines are treated as having direction. Not
only do they have direction, they “come equipped” with two points from which
they were constructed. There are function symbols to recover those points from
a line. Line (P, P) is undeﬁned, since having it deﬁned would destroy continuous
dependence of Line (P, Q) on P and Q.
The two intersection points p = IntersectCircles1 (C, K) and
q = IntersectCircles2 (C, K) are to be distinguished as follows: With a the center
of C and b the center of K we should have abp a right turn, and abq a left turn.
But can “right turn” and “left turn” be deﬁned? What we do is to deﬁne Right
and Left using equations involving IntersectCircles1 and IntersectCircles2 ; then
we give axioms about Right and Left , namely that if abc is a left turn, then c and
d are on the same side of Line (a, b) if and only if abd is a left turn, and c and d are
on opposite sides of Line (a, b) if and only if abd is a right turn. Note that neither
this issue nor its solution has to do with constructivity, but simply with the
introduction of function symbols corresponding to the elementary constructions.

Logic of Ruler and Compass Constructions
51
4
Models of Ruler-and-Compass Geometry
There are several interesting models of ECG, even with classical logic, of which
we now mention four. The standard plane is R2, with the usual interpretation of
points, lines, and planes. The Turing plane has for its points pairs of computable
real numbers [7]. The algebraic plane has for its points pairs of algebraic numbers.
The Tarski plane has for points just those points constructible with ruler and
compass; this is K2 where K is the least subﬁeld of the reals closed under square
roots of positive elements. The theory ECG uses the same primitive relations
as Tarski and Hilbert: betweenness B(a, b, c) and equidistance ab = cd. Hilbert
used strict betweenness and Tarski allowed B(x, x, x); we follow Hilbert.
5
Three Versions of the Parallel Postulate
Let P be a point not on line L. We consider lines through P that do not meet L
(i.e., are parallel to L). Playfair’s version of the parallel postulate says that two
parallels to L through P are equal. Recall that Euclid’s postulate 5 is
If a straight line falling on two straight lines make the interior angles
on the same side less than two right angles, the two straight lines, if
produced indeﬁnitely, meet on that side on which are the angles less than
the two right angles.
 p
 a
 q
 r
L
K
M
Fig. 1. Euclid 5: M and L must meet on the right side, provided B(q, a, r) and pq
makes alternate interior angles equal with K and L. The point at the open circle is
asserted to exist.
But this formulation of Euclid 5 makes use of the notion of “alternate interior
angles”, while angles are not directly treated in ECG, but instead are treated
as triples of points. A version of Euclid 5 that does not mention angles is given
in Fig. 2.
Although we have ﬁnally arrived at a satisfactory formulation of Euclid 5,
that formulation is satisfactory only in the sense that it accurately expresses
what Euclid said. It turns out that this axiom is not satisfactory as a parallel
postulate for ECG. The main reason is that it is inadequate to deﬁne division
geometrically. Here is why: As x gets nearer and nearer to 0, the number 1/x
requires a line of smaller and smaller slope to meet a certain horizontal line. If x
passes through zero, this intersection point “goes to inﬁnity”, then is undeﬁned

52
M. Beeson
 p
 a
 q
 s
 r
 t
L
K
M
Fig. 2. Euclid 5: M and L must meet on the right side, provided B(q, a, r) and pt = qt
and rt = st
when x = 0, but then “reappears on the other side”, coming in from minus
inﬁnity. Without knowing the sign of x, we shall not know on which side of
the transversal pq the two adjacent interior angles will make less than two right
angles. In other words, with Euclid 5, we shall only be able to divide by a number
whose sign we know; and the principle x ̸= 0 →x < 0 ∨x > 0 is not an axiom
(or theorem) of ECG. The conclusion is that if we want to divide by nonzero
numbers, we need to strengthen Euclid’s parallel axiom.
We make three changes in Euclid 5 to get the “strong parallel postulate”:
(i) We change the hypothesis B(q, a, r) to ¬on(a, K). In other words, we
require that the two adjacent interior angles do not make exactly two right
angles, instead of requiring that they make less than two right angles.
(ii) We change the conclusion to state only that M meets L, without specifying
on which side of the transversal pq the intersection lies.
(iii) We drop the hypothesis ¬ on (p, L).
 p
  a
 q
 s
 r
 t
L
K
M
Fig. 3. Strong Parallel Postulate: M and L must meet (somewhere) provided a is not
on K and pt = qt and rt = st
The strong parallel axiom diﬀers from Euclid’s version in that we are not
required to know in what direction M passes through P; but also the conclusion
is weaker, in that it does not specify where M must meet L. In other words,
the betweenness hypothesis of Euclid 5 is removed, and so is the betweenness
conclusion. Since both the hypothesis and conclusion have been changed, it is not
immediate whether this new postulate is stronger than Euclid 5, or equivalent,
or possibly even weaker, but it turns out to be stronger–hence the name.

Logic of Ruler and Compass Constructions
53
6
Constructive Geometry and Euclidean Fields
Classical Euclidean geometry has models K2 = K × K where K is a Euclidean
ﬁeld, i.e., an ordered ﬁeld in which nonnegative elements have square roots. We
take that deﬁnition also constructively, and we deﬁne a Euclidean ring to be an
ordered ring in which nonnegative elements have square roots. We use a language
with symbols + for addition and · for multiplication, and a unary predicate P(x)
for “x is positive”. A Euclidean ﬁeld is a Euclidean ring in which nonzero elements
have reciprocals. Constructively, we also need two weaker notions: Euclidean
rings in which positive elements have reciprocals, and Euclidean rings in which
elements without reciprocals are zero and if x is greater than a positive invertible
element, then x is invertible. These we call Playfair rings.
In order to show that the models of some geometrical theory T have the form
F 2, one has to deﬁne addition and multiplication (of segments or points on a
line) within T . This was ﬁrst done by Descartes in [5], and again (in a diﬀerent
way) by Hilbert in [6]. These constructions, however, involve a non-constructive
case distinction on the sign of the numbers being added or multiplied. To repair
this problem requires rather more elaborate constructions, and to make those
elaborate constructions work, one needs some more elementary case construc-
tions to also work without case distinctions, for example, constructing a line
through a point P perpendicular to a line L, without a case distinction as to
whether P is or is not on L. These problems are solved, and the (rather lengthy)
solutions are presented in detail, in [3].
We can prove that the models of ECG are of the form F 2, where F is a
Euclidean ﬁeld. More speciﬁcally, given such a ﬁeld, we can deﬁne betweenness,
incidence, and equidistance by analytic geometry and verify the axioms of ECG.
Conversely, and this is the hard part, we can deﬁne multiplication, addition, and
division of points on a line (having chosen one point as zero), in ECG. It turns
out that we need the strong parallel axiom to do that. If we replace the parallel
axiom of ECG by Euclid’s parallel postulate, we get instead models of the form
F 2, where F is a Euclidean ring in which nonzero elements have reciprocals, but
we cannot go the other way by deﬁning multiplication and addition geometrically
without the strong parallel axiom. (That is, if we only had Euclid 5, we would
need case distinctions, as Hilbert and Descartes did.)
We now work out the ﬁeld-theoretic version of Playfair’s axiom. Playfair says,
if P is not on L and K is parallel to L through P, that if line M through P
does not meet L then M = K. Since ¬¬M = K
→M = K, Playfair is just
the contrapositive of the parallel axiom of ECG, which says that if M ̸= K
then M meets L. Hence it corresponds to the contrapositive of x ̸= 0 →1/x ↓;
that contrapositive says that if x has no multiplicative inverse, then x = 0.
Thus Playfair geometries have models F 2 where F is a Playfair ring (as deﬁned
above). (We cannot prove the converse because we need the strong parallel axiom
to verify multiplication and addition).

54
M. Beeson
7
What ECG Proves to Exist, Can Be Constructed with
Ruler and Compass
In [2], we proved that if ECG proves an existential statement ∃yA(x, y), then
there is a term t of ECG such that ECG proves A(x, t(x)). In words: things that
ECG can prove to exist, can be constructed with ruler and compass. Of course,
the converse is immediate: things that can be constructed with ruler and compass
can be proved to exist in ECG. Hence the two meanings of “constructive”
coincide for ECG: it could mean “proved to exist with intuitionistic logic” or it
could mean “constructed with ruler and compass.”
The technique of the proof is to apply Gentzen’s cut-elimination theorem.
What makes it applicable is that the axiomatization of ECG has two important
properties: it is quantiﬁer-free, and it is disjunction-free. It was not diﬃcult to
axiomatize ECG in this way–we just followed Euclid. In [3] we draw on Tarski’s
approach to achieve a short elegant list of axioms, but that is not essential to
the analysis of the parallel axiom. There are many ways to axiomatize geometry.
8
Independence Results for the Parallel Axioms
The reduction of geometry to ﬁeld theory described above shows that (relative to
a base theory), the strong parallel axiom implies Euclid’s postulate 5 (since if re-
ciprocals of non-zero elements exist, then of course reciprocals of positive elements
exist). (A direct proof is in [3].) And Euclid 5 easily implies Playfair’s postulate.
Our main theorem is that neither of these two implications can be reversed.
Theorem 1. Euclid 5 does not imply the strong parallel axiom, and Playfair
does not imply Euclid 5, in ECG minus its parallel axiom.
Proof sketch of the ﬁrst claim (A detailed proof can be found in [3]). Since non-
constructively, the implications are reversible, we cannot hope to give counterex-
amples. In terms of ﬁeld theory, we won’t be able to construct a Euclidean ring
in which positive elements have reciprocals but nonzero elements do not. The
proof proceeds by constructing appropriate Kripke models. To show that Euclid
5 does not prove the strong parallel axiom, it suﬃces to prove the corresponding
result in ordered ﬁeld theory: the axiom that positive elements have reciprocals
does not imply that all nonzero elements have reciprocals. That does suﬃce, in
spite of the fact that we have full equivalence between geometry and ﬁeld the-
ory only for ECG and Euclidean ﬁelds, for if the weaker geometry proved the
strong parallel axiom SP, then the interpretation of SP in ﬁeld theory would be
provable, as that direction does work, and the interpretation of SP implies that
nonzero elements have reciprocals.
Then we need a Kripke model in which positive elements have reciprocals, but
nonzero elements do not necessarily have reciprocals. We construct such a Kripke
model whose “points” are functions from R to R. The function f is positive semidef-
inite if f(x) ≥0 for all real x. Let K be the least subﬁeld of the reals closed under
square roots of positive elements. Let A be the least ring of real-valued functions

Logic of Ruler and Compass Constructions
55
containing polynomials with coeﬃcients in K, and closed under reciprocals and
square roots of positive semideﬁnite functions. For example

1 + t2 +

1 + t4 +
1
1 + t2
is in A, but 1/t is not in A. We take A as the root of a Kripke model, interpreting
the positivity predicate P(x) to mean x is positive deﬁnite. We show (using
Pusieux series) that each member of A has ﬁnitely many zeroes and singularities
and that there is a countable set Ω including all zeroes and singularities, whose
complement is dense in R. For α ̸∈Ω, we deﬁne Aα by interpreting P(x) to
hold if and only if x(α) > 0. In our Kripke model, Aα lies immediately above
the root. Now t is a nonzero element without a reciprocal. But if x is positive,
then x(α) > 0 for all α ̸∈Ω, and since the complement of Ω is dense and x is
continuous, x is positive semideﬁnite, so 1/x exists in A.
9
Conclusions
Euclid needs only two modiﬁcations to be completely constructive: we have to
postulate a rigid compass, rather than relying on Prop. I.2 to simulate it, and we
have to take the strong parallel axiom instead of Euclid 5. With those changes
Euclid is entirely constructive, and ECG formalizes Euclid nicely. ECG has the
nice property that things it can prove to exist can be constructed with ruler
and compass, and it permits us to distinguish between versions of the parallel
axiom with diﬀerent constructive content, even though non-constructively they
are equivalent. The classical constructions used to give geometrical deﬁnitions of
addition and multiplication involve non-constructive case distinctions, but these
can be replaced by more elaborate constructions that are continuous (and con-
structive), so geometry can still be shown equivalent to the theory of Euclidean
ﬁelds, and diﬀerent versions of the parallel axiom correspond to weakenings of
the ﬁeld axiom about reciprocals. We can then use Kripke models whose points
are certain real-valued functions to establish formal independence results about
the diﬀerent versions of the parallel axiom.
References
1. Beeson, M.: Foundations of Constructive Mathematics. Springer, Heidelberg (1985)
2. Beeson, M.: Constructive geometry. In: Arai, T. (ed.) Proceedings of the Tenth Asian
Logic Colloquium, Kobe, Japan, pp. 19–84. World Scientiﬁc, Singapore (2009)
3. Beeson, M.: Foundations of Constructive Geometry (available on the author’s website)
4. Bishop, E.: Foundations of Constructive Analysis. McGraw-Hill, New York (1967)
5. Descartes, R.: The Geometry of Rene Descartes as an appendix to Discours de la
Methode, 1st edn. Dover, New York (1952); original title, La Geometrie. Facsimile
with English translation by Smith, D.E., Latham, M.L.
6. Hilbert, D.: Foundations of Geometry (Grundlagen der Geometrie). Open Court,
La Salle (1960); second English edition, translated from the tenth German edition
by Unger, L., Original publication date (1899)
7. Turing, A.: On computable numbers, with an application to the Entscheidungsprob-
lem. Proceedings of the London Mathematical Society, Series 2 42 (1936-1937)

On the Computational Content
of the Brouwer Fixed Point Theorem
Vasco Brattka1, St´ephane Le Roux2, and Arno Pauly3
1 Department of Mathematics and Applied Mathematics, University of Cape Town,
Private Bag, Rondebosch 7701, South Africa
Vasco.Brattka@uct.ac.za
2 Department of Mathematics, Technische Universit¨at Darmstadt,
Schlossgartenstraße 7, 64289 Darmstadt, Germany
leroux@mathematik.tu-darmstadt.de
3 Computer Laboratory, University of Cambridge, William Gates Building, 15 JJ
Thomson Avenue, Cambridge CB3 0FD, United Kingdom
Arno.Pauly@cl.cam.ac.uk
Abstract. We study the computational content of the Brouwer Fixed
Point Theorem in the Weihrauch lattice. One of our main results is that
for any ﬁxed dimension the Brouwer Fixed Point Theorem of that di-
mension is computably equivalent to connected choice of the Euclidean
unit cube of the same dimension. Connected choice is the operation that
ﬁnds a point in a non-empty connected closed set given by negative in-
formation. Another main result is that connected choice is complete for
dimension greater or equal to three in the sense that it is computably
equivalent to Weak K˝onig’s Lemma. In contrast to this, the connected
choice operations in dimensions zero, one and two form a strictly increas-
ing sequence of Weihrauch degrees, where connected choice of dimen-
sion one is known to be equivalent to the Intermediate Value Theorem.
Whether connected choice of dimension two is strictly below connected
choice of dimension three or equivalent to it is unknown, but we con-
jecture that the reduction is strict. As a side result we also prove that
ﬁnding a connectedness component in a closed subset of the Euclidean
unit cube of any dimension greater than or equal to one is equivalent to
Weak K˝onig’s Lemma.
1
Introduction
In this paper we continue with the programme to classify the computational con-
tent of mathematical theorems in the Weihrauch lattice (see [6,4,3,14,13,5,8]).
This lattice is induced by Weihrauch reducibility, which is a reducibility for par-
tial multi-valued functions f :⊆X ⇒Y on represented spaces X, Y . Intuitively,
f ≤W g reﬂects the fact that the function f can be realized with a single applica-
tion of the function g as an oracle. Hence, if two functions are equivalent in the
sense that they are mutually reducible to each other, then they are equivalent
as computational resources, as far as computability is concerned.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 56–67, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

On the Computational Content of the Brouwer Fixed Point Theorem
57
Many theorems in mathematics are actually of the logical form
(∀x ∈X)(∃y ∈Y ) P(x, y)
and such theorems can straightforwardly be represented by a multi-valued func-
tion f : X ⇒Y with f(x) := {y ∈Y : P(x, y)} (sometimes partial f are needed,
where the domain captures additional requirements that this input x has to
satisfy). In some sense the multi-valued function f directly reﬂects the compu-
tational task of the theorem to ﬁnd some suitable y for any x. Hence, in a very
natural way the classiﬁcation of a theorem can be achieved via a classiﬁcation
of the corresponding multi-valued function that represents the theorem. In this
paper we attempt to classify the Brouwer Fixed Point Theorem.
Theorem 1 (Brouwer Fixed Point Theorem 1911). Every continuous func-
tion f : [0, 1]n →[0, 1]n has a ﬁxed point x ∈[0, 1]n.
The fact that Brouwer’s Fixed Point Theorem cannot be proved constructively
has been conﬁrmed in many diﬀerent ways, most relevant for us is the counterex-
ample in Russian constructive analysis by Orevkov [12], which was transferred
into computable analysis by Baigger [1].
Constructions similar to those used for the above counterexamples have been
utilized in order to prove that the Brouwer Fixed Point Theorem is equivalent to
Weak K˝onig’s Lemma in reverse mathematics [17,16] and to analyze computabil-
ity properties of ﬁxable sets [11], but a careful analysis of these reductions reveals
that none of them can be straightforwardly transferred into a uniform reduction
in the sense that we are seeking here. The results cited above essentially charac-
terize the complexity of ﬁxed points themselves, whereas we want to characterize
the complexity of ﬁnding the ﬁxed point, given the function. This requires full
uniformity.
In the Weihrauch lattice the Brouwer Fixed Point Theorem of dimension n is
represented by the multi-valued function BFTn : C([0, 1]n, [0, 1]n) ⇒[0, 1]n that
maps any continuous function f : [0, 1]n →[0, 1]n to the set of its ﬁxed points
BFTn(f) ⊆[0, 1]n. The question now is where BFTn is located in the Weihrauch
lattice? It easily follows from a meta theorem presented in [3] that the Brouwer
Fixed Point Theorem BFTn is reducible to Weak K˝onig’s Lemma WKL for any
dimension n, i.e., BFTn ≤W WKL. However, for which dimensions n do we also
obtain the inverse reduction? Clearly not for n = 0, since BFT0 is computable,
and clearly not for n = 1, since BFT1 is equivalent to the Intermediate Value
Theorem IVT and hence not equivalent to WKL, as proved in [3].1
In order to approach this question for a general dimension n, we introduce a
choice principle CCn that we call connected choice and which is just the closed
choice operation restricted to connected subsets. That is, in the sense discussed
above, CCn is the multi-valued function that represents the following mathe-
matical statement: every non-empty connected closed set A ⊆[0, 1]n has a point
1 In constructive reverse mathematics the Intermediate Value Theorem is equivalent to
Weak K˝onig’s Lemma [9], since parallelization is freely available in this framework.

58
V. Brattka, S. Le Roux, and A. Pauly
x ∈[0, 1]n. Since closed sets are represented by negative information (i.e., by
an enumeration of open balls that exhaust the complement), the computational
task of CCn consists in ﬁnding a point in a closed set A ⊆[0, 1]n that is promised
to be non-empty and connected and that is given by negative information.
One of our main results, presented in Section 4, is that the Brouwer Fixed
Point Theorem is equivalent to connected choice for each ﬁxed dimension n, i.e.,
BFTn ≡W CCn. This result allows us to study the Brouwer Fixed Point Theorem
in terms of the function CCn, which is easier to handle since it involves neither
function spaces nor ﬁxed points. This is also another instance of the observation
that several important theorems are equivalent to certain choice principles (see
[3]) and many important classes of computable functions can be calibrated in
terms of choice (see [2]). For instance, closed choice on Cantor space C{0,1}N
and on the unit cube C[0,1]n are both easily seen to be equivalent to Weak
K˝onig’s Lemma WKL, i.e., WKL ≡W C{0,1}N ≡W C[0,1]n for any n ≥1. Studying
the Brouwer Fixed Point Theorem in the form of CCn now amounts to comparing
C[0,1]n with its restriction CCn.
Our second main result, given in Section 5, is that from dimension three on-
wards connected choice is equivalent to Weak K˝onig’s Lemma, i.e., CCn ≡W C[0,1]
for n ≥3. The backwards reduction is based on a geometrical construction that
seems to require at least dimension three in a crucial sense. It is easy to see that
connected choice operations for dimensions 0, 1 and 2 form a strictly increas-
ing sequence of Weihrauch degrees, i.e., CC0 <W CC1 <W CC2 ≤W CC3 ≡W C[0,1].
The status of connected choice CC2 of dimension two remains unresolved and
we conjecture that it is strictly weaker than choice of dimension three, i.e.,
CC2 <W CC3.
In order to prove our results, we use a representation of closed sets by trees of
so-called rational complexes, which we introduce in Section 3. It can be seen as a
generalization of the well-known representation of co-c.e. closed subsets of Cantor
space {0, 1}N by trees. As a side result we mention that ﬁnding a connectedness
component in a closed set for any ﬁxed dimension from one upwards is equivalent
to Weak K˝onig’s Lemma. This yields conclusions along the line of earlier studies
of connected components in [10]. In the following Section 2 we start with a short
summary of relevant deﬁnitions and results regarding the Weihrauch lattice.
This extended abstract does not contain any proofs.
2
The Weihrauch Lattice
In this section we brieﬂy recall some basic results and deﬁnitions regarding the
Weihrauch lattice. The original deﬁnition of Weihrauch reducibility is due to
Weihrauch and has been studied for many years (see [18,19,20,7]). Only re-
cently it has been noticed that a certain variant of this reducibility yields a
lattice that is very suitable for the classiﬁcation of mathematical theorems (see
[6,4,3,14,2,13,5]). The basic reference for all notions from computable analysis
is [21]. The Weihrauch lattice is a lattice of multi-valued functions on repre-
sented spaces. A representation δ of a set X is just a surjective partial map

On the Computational Content of the Brouwer Fixed Point Theorem
59
δ :⊆NN →X. In this situation we call (X, δ) a represented space. In general we
use the symbol “⊆” in order to indicate that a function is potentially partial.
Using represented spaces we can deﬁne the concept of a realizer. We denote the
composition of two (multi-valued) functions f and g either by f ◦g or by fg.
Deﬁnition 1 (Realizer). Let f :⊆(X, δX) ⇒(Y, δY ) be a multi-valued func-
tion on represented spaces. A function F :⊆NN →NN is called a realizer of f,
in symbols F ⊢f, if δY F(p) ∈fδX(p) for all p ∈dom(fδX).
Realizers allow us to transfer the notions of computability and continuity and
other notions available for Baire space to any represented space; a function
between represented spaces will be called computable, if it has a computable
realizer, etc. Now we can deﬁne Weihrauch reducibility.
Deﬁnition 2 (Weihrauch reducibility). Let f, g be multi-valued functions
on represented spaces. Then f is said to be Weihrauch reducible to g, in sym-
bols f ≤W g, if there are computable functions K, H :⊆NN →NN such that
K⟨id, GH⟩⊢f for all G ⊢g. Moreover, f is said to be strongly Weihrauch
reducible to g, in symbols f ≤sW g, if there are computable functions K, H such
that KGH ⊢f for all G ⊢g.
Here ⟨, ⟩denotes some standard pairing on Baire space. We note that the rela-
tions ≤W, ≤sW and ⊢implicitly refer to the underlying representations, which
we mention explicitly only when necessary. It is known that these relations only
depend on the underlying equivalence classes of representations, but not on the
speciﬁc representatives (see Lemma 2.11 in [4]). We use ≡W and ≡sW to denote
the respective equivalences regarding ≤W and ≤sW, and by <W and <sW we
denote strict reducibility.
A particularly useful multi-valued function in the Weihrauch lattice is closed
choice (see [6,4,3,2]) and it is known that many notions of computability can
be calibrated using the right version of choice. We shall focus on closed choice
for computable metric spaces, which are separable metric spaces such that the
distance function is computable on the given dense subset. We assume that
computable metric spaces are represented via their Cauchy representation (see
[21] for details).
By A−(X) we denote the set of closed subsets of a metric space X, where the
index “−” indicates that we work with negative information. This information is
given by a representation ψ−: NN →A−(X), deﬁned by ψ−(p) := X\∞
i=0 Bp(i),
where Bn is some standard enumeration of the open balls of X with center in the
dense subset and rational radius. The computable points in A−(X) are called
co-c.e. closed sets. We now deﬁne closed choice for the case of computable metric
spaces.
Deﬁnition 3 (Closed Choice). Let X be a computable metric space. Then the
closed choice operation of this space is deﬁned by CX :⊆A−(X) ⇒X, A →A
with dom(CX) := {A ∈A−(X) : A ̸= ∅}.
Intuitively, CX takes as input a non-empty closed set in negative representation
(i.e., given by ψ−) and it produces an arbitrary point of this set as output. Hence,

60
V. Brattka, S. Le Roux, and A. Pauly
A →A means that the multi-valued map CX maps the input A ∈A−(X) to the
set A ⊆X as a set of possible outputs.
3
Closed Sets and Trees of Rational Complexes
In this section we want to describe a representation of closed sets A ⊆[0, 1]n
that is useful for the study of connectedness. It is well-known that closed subsets
of Cantor space can be characterized exactly as sets of inﬁnite paths of trees.
We describe a similar representation of closed subsets of the Euclidean unit
cube [0, 1]n. While in the case of Cantor space clopen balls are associated to
each node of the tree, we now associate ﬁnite complexes of rational balls to each
node. While inﬁnite paths lead to points of the closed set in case of Cantor space,
they now lead to connectedness components.
This representation of closed subsets A ⊆[0, 1]n of the unit cube will enable us
to analyze the relation between connected choice and the Brouwer Fixed Point
Theorem in the next section. In this section we shall use this representation
in order to show that ﬁnding a connectedness component of a closed set A is
computably exactly as diﬃcult as Weak K˝onig’s Lemma.
We ﬁrst ﬁx some topological terminology. We work with the maximum norm
|| || on Rn. By B(x, r) := {y ∈Rn : ||x −y|| < r} we denote the open ball
with center x and radius r and by B[x, r] := {y ∈Rn : ||x −y|| ≤r} the
corresponding closed ball. Since we are using the maximum norm, all these balls
are open or closed cubes, respectively (if the radius is positive). By ∂A we denote
the topological boundary, by A the closure and by A◦the interior of a set A. If
the underlying space X is clear from the context, then Ac := X \ A denotes the
complement of A. We are now prepared to deﬁne rational complexes.
Deﬁnition 4 (Rational complex). We call a set R := {B[c1, r1], ..., B[ck, rk]}
of ﬁnitely many closed balls B[ci, ri] with rational center ci ∈Qn and positive
rational radius ri ∈Q an (n–dimensional) rational complex if  R is connected
and B1, B2 ∈R with B1 ̸= B2 implies B◦
1 ∩B◦
2 = ∅. By CQn we denote the set
of n–dimensional rational complexes.
Each rational complex R can be represented by a list of the corresponding ratio-
nal numbers c1, r1, ..., ck, rk and we implicitly assume in the following that this
representation is used for the set of rational complexes CQn.
In order to organize the rational complexes that are used to approximate sets
it is suitable to use trees. We recall that a tree is a set T ⊆N∗which is closed
under preﬁx, i.e., u ⊑v and v ∈T implies u ∈T . A function b : N →N is called
a bound of a tree T if w ∈T implies w(i) ≤b(i) for all i = 0, ..., |w|−1, where |w|
denotes the length of the word w. A tree is called ﬁnitely branching, if it has a
bound. A tree of rational complexes is understood to be a ﬁnitely branching tree
T (together with a bound) such that to each node of the tree a rational complex
is associated with the property that these complexes are compactly included in
each other if we proceed along paths of the tree and they are disjoint on any
particular level of the tree. We write A ⋐B for two sets A, B ⊆Rn if the closure

On the Computational Content of the Brouwer Fixed Point Theorem
61
A of A is included in the interior B◦of B and we say that A is compactly included
in B in this case.
Deﬁnition 5 (Tree of rational complexes). We call (T, f) a tree of rational
complexes if T ⊆N∗is a ﬁnitely branching tree and f : T →CQn is a function
such that for all u, v ∈T with u ̸= v
1. u ⊑v =⇒ f(v) ⋐ f(u),
2. |u| = |v| =⇒ f(u) ∩ f(v) = ∅.
In the following we assume that ﬁnitely branching trees T are represented as
a pair (χT , b), where χT : N∗→{0, 1} is the characteristic function of T and
b : N →N is a bound of T . Correspondingly, trees (T, f) of rational complexes
are then represented in a canonical way by (χT , b, f). We now deﬁne which set
A ⊆[0, 1]n is represented by such a tree (T, f) of rational complexes.
Deﬁnition 6 (Closed sets and trees of rational complexes). We say that
a closed set A ⊆Rn is represented by a tree (T, f) of n–dimensional rational
complexes if one obtains A = ∞
i=0

w∈T ∩Ni
 f(w).
It is clear that in this way any tree (T, f) of rational complexes actually rep-
resents a compact set A. This is because  f(w) is compact for each w ∈T
and since T is ﬁnitely branching, the set T ∩Ni is ﬁnite for each i, hence

w∈T ∩Ni
 f(w) is compact and hence A is compact too. Vice versa, every com-
pact set A ⊆Rn can be represented by a tree of n–dimensional rational com-
plexes. For [0, 1]n we mention the uniform result that even the map (T, f) →A
is computable and has a computable multi-valued right inverse. We assume that
trees of rational complexes are represented as speciﬁed above and closed sets A
are represented as points in A−([0, 1]n).
Proposition 1 (Closed sets and complexes). Let n ≥1. The map (T, f) →
A that maps every tree of n–dimensional rational complexes (T, f) to the closed
set A ⊆[0, 1]n represented by it, is computable and has a multi-valued computable
right inverse.
The representation of closed sets A ⊆[0, 1]n by trees of rational complexes also
has the advantage that connectedness components of A can easily be expressed
in terms of the tree structure. This is made precise by the following lemma. By
[T ] := {p ∈NN : (∀i) p|i ∈T } we denote the set of inﬁnite paths of T , which is
also called the body of T . Here p|i = p(0)...p(i −1) ∈N∗denotes the preﬁx of p
of length i for each i ∈N. We recall that a connectedness component of a set A
is a connected subset of A that is not included in any larger connected subset
of A. Any connectedness component of a subset A is closed in A. According to
the following lemma there is bijection between [T ] and the set of connectedness
components of a non-empty closed set A ⊆[0, 1]n.
Lemma 1 (Connectedness components). Let (T, f) be a tree of rational
complexes and let A ⊆[0, 1]n be the closed set represented by (T, f). Then the
sets Cp := ∞
i=0
 f(p|i) for p ∈[T ] are exactly all connectedness components of
A (without repetitions).

62
V. Brattka, S. Le Roux, and A. Pauly
As another interesting result we can deduce from Proposition 1 a classiﬁcation of
the operation that determines a connectedness component. We ﬁrst deﬁne this
operation. For short we use the notation An := {A ∈A−([0, 1]n) : A ̸= ∅} for
the space of non-empty closed subsets with representation ψ−.
Deﬁnition 7 (Connectedness components). By Conn : An ⇒An we de-
note the map with Conn(A) := {C : C is a connectedness component of A} for
every n ≥1.
The problem Conn of ﬁnding a connectedness component of a closed set has
the same strong Weihrauch degree as Weak K˝onig’s Lemma for every dimension
n ≥1.
Theorem 2 (Connectedness components). Conn ≡sW WKL for n ≥1.
4
Brouwer’s Fixed Point Theorem and Connected Choice
In this section we want to show that the Brouwer Fixed Point Theorem is com-
putably equivalent to connected choice for any ﬁxed dimension. We ﬁrst deﬁne
these two operations. By C(X, Y ) we denote the set of continuous functions
f : X →Y and for short we write Cn := C([0, 1]n, [0, 1]n).
Deﬁnition 8 (Brouwer Fixed Point Theorem). By BFTn : Cn ⇒[0, 1]n we
denote the operation deﬁned by BFTn(f) := {x ∈[0, 1]n : f(x) = x} for n ∈N.
We note that BFTn is well-deﬁned, i.e., BFTn(f) is non-empty for all f, since
by the Brouwer Fixed Point Theorem every f ∈Cn admits a ﬁxed point x, i.e.,
with f(x) = x. We now deﬁne connected choice.
Deﬁnition 9 (Connected choice). By CCn :⊆An ⇒[0, 1]n we denote the
operation deﬁned by CCn(A) := A for all non-empty connected closed A ⊆[0, 1]n
and n ∈N. We call CCn connected choice (of dimension n).
Hence, connected choice is just the restriction of closed choice C[0,1]n to connected
sets. We also use the following notation for the set of ﬁxed points of a functions
f ∈Cn.
Deﬁnition 10 (Set of ﬁxed points). By Fixn : Cn →An we denote the
function with Fixn(f) := {x ∈[0, 1]n : f(x) = x}.
It is easy to see that Fixn is computable, since Fixn(f) := (f −id)−1{0} and
it is well-known that closed sets in An can also be represented as zero sets of
continuous functions (see [21]). We note that the Brouwer Fixed Point Theorem
can be decomposed to BFTn = CCn ◦Conn ◦Fixn.
The main result of this section will be that the Brouwer Fixed Point Theo-
rem and connected choice are (strongly) equivalent for any ﬁxed dimension n
(see Theorem 3 below). An important tool for both directions of the proof is
the representation of closed sets by trees of rational complexes. The direction

On the Computational Content of the Brouwer Fixed Point Theorem
63
CCn ≤sW BFTn can be seen as a uniformization of an earlier construction of Baig-
ger [1] that is in turn built on results of Orevkov [12]. For the other direction
BFTn ≤sW CCn of the reduction we uniformize ideas of Joseph S. Miller [11] and
we use again the representation of closed sets by trees of rational complexes. We
also exploit the fact that each rational complex can easily be converted into a
simplicial complex. We recall that a proper n–dimensional rational simplex is the
convex hull of n + 1 geometrically independent rational points in [0, 1]n and a
proper rational simplicial complex is a set of ﬁnitely many proper simplexes such
that the interiors of distinct simplexes are disjoint. By SQn we denote the set
of all such proper rational simplicial complexes and we assume that each such
complex is represented by a speciﬁcation of a list of n + 1 geometrically inde-
pendent rational points for each simplex in the complex. Hence, it is clear that
there is a computable f : CQn →SQn with  f(R) =  R. That means that we
can easily translate each tree of rational complexes into a corresponding tree of
rational simplicial complexes (understood in the analogous way). We essentially
use Miller’s ideas to reduce the Brouwer Fixed Point Theorem uniformly to con-
nected choice. The ﬁrst observation is that the map Conn ◦Fixn is computable
(which might be surprising in light of Theorem 2).
Proposition 2. Conn ◦Fixn : Cn ⇒An is computable for all n ∈N.
Since BFTn ⊇CCn ◦Conn ◦Fixn we can directly conclude BFTn ≤sW CCn for all
n. Together with CCn ≤sW BFTn we obtain the following theorem.
Theorem 3 (Brouwer Fixed Point Theorem). BFTn ≡sW CCn for all n.
It is easy to see that in general the Brouwer Fixed Point Theorem and connected
choice are not independent of the dimension. In case of n = 0 the space [0, 1]n
is the one-point space {0} and hence BFT0 ≡sW CC0 are both computable. In
case of n = 1 connected choice was already studied in [3] and it was proved that
it is equivalent to the Intermediate Value Theorem IVT (see Deﬁnition 6.1 and
Theorem 6.2 in [3]).
Corollary 1 (Intermediate Value Theorem). IVT ≡sW BFT1 ≡sW CC1.
It is also easy to see that the Brouwer Fixed Point Theorem BFT2 in dimension
two is more complicated than in dimension one. For instance, it is known that
the Intermediate Value Theorem IVT always oﬀers a computable function value
for a computable input, whereas this is not the case for the Brouwer Fixed Point
Theorem BFT2 by Baigger’s counterexample [1]. We continue to discuss this
topic in Section 5.
Here we point out that Proposition 2 implies that the ﬁxed point set Fixn(f) of
every computable function f : [0, 1]n →[0, 1]n has a co-c.e. closed connectedness
component. Joseph S. Miller observed that also any co-c.e. closed superset of
such a set is the ﬁxed point set of some computable function and the following
result is a uniform version of this observation. We denote by (f, g) :⊆X ⇒Y ×Z
the juxtaposition of two functions f :⊆X ⇒Y and g :⊆X ⇒Z, deﬁned by
(f, g)(x) = (f(x), g(x)).

64
V. Brattka, S. Le Roux, and A. Pauly
Theorem 4 (Fixability). (Fixn, Conn ◦Fixn) is computable and has a multi-
valued computable right inverse for all n ∈N.
Roughly speaking a closed set A ∈An together with one of its connectedness
components is as good as a continuous function f ∈Cn with A as set of ﬁxed
points. As a non-uniform corollary we obtain immediately Miller’s original result.
Corollary 2 (Fixable sets, Miller 2002). A set A ⊆[0, 1]n is the set of ﬁxed
points of a computable function f : [0, 1]n →[0, 1]n if and only if it is non-empty
and co-c.e. closed and contains a co-c.e. closed connectedness component.
5
Aspects of Dimension
In this section we want to discuss aspects of dimension of connected choice and
the Brouwer Fixed Point Theorem. Our main result is that connected choice
is computably universal or complete from dimension three onwards in the sense
that it is strongly equivalent to Weak K˝onig’s Lemma, which is one of the degrees
of major importance. In order to prove this result, we use the following geometric
construction.
Proposition 3 (Twisted cube). The function T :⊆A−[0, 1] →A3 with
T (A) = (A × [0, 1] × {0}) ∪(A × A × [0, 1]) ∪([0, 1] × A × {1}) is computable
and maps non-empty closed sets A ⊆[0, 1] to non-empty connected closed sets
T (A) ⊆[0, 1]3.
Here tuples (x1, x2, x3) ∈T (A) have the property that at least one of the ﬁrst
two components provide a solution xi ∈A, but the third component provides
the additional information which one surely does. If x3 is close to 1, then surely
x2 ∈A and if x3 is close to 0, then surely x1 ∈A. If x3 is neither close to 0
nor 1, then both x1, x2 ∈A. Hence, there is a computable function H such that
C[0,1] = H ◦CC3 ◦T , which proves C[0,1] ≤sW CC3. Together with Theorem 3 we
obtain the following conclusion.
Theorem 5 (Completeness of three dimensions). For n ≥3 we obtain
CCn ≡sW BFTn ≡sW WKL ≡sW C[0,1].
We note that the reduction CCn ≤sW C[0,1]n holds for all n ∈N, since connected
choice is just a restriction of closed choice and C[0,1]n ≡sW C[0,1] ≡sW WKL is
known for all n ≥1 (see [2]).
In particular, we get the Baigger counterexample for dimension n ≥3 as a
consequence of Theorem 5. A superﬁcial reading of the results of Orevkov [12]
and Baigger [1] can lead to the wrong conclusion that they actually provide a
reduction of Weak K˝onig’s Lemma to the Brouwer Fixed Point Theorem BFTn
of any dimension n ≥2. However, this is only correct in a non-uniform way
and the corresponding uniform result is still open and does not follow from the
known constructions. The Orevkov-Baigger result is built on the following fact.

On the Computational Content of the Brouwer Fixed Point Theorem
65
Proposition 4 (Mixed cube). The function M :⊆A−[0, 1] →A2 with M(A) =
(A×[0, 1])∪([0, 1]×A) is computable and maps non-empty closed sets A ⊆[0, 1]
to non-empty connected closed sets M(A) ⊆[0, 1]2.
It follows straightforwardly from the deﬁnition that the pairs (x, y) ∈M(A) are
such that one out of two components x, y is actually in A. In order to express
the uniform content of this fact, we introduce the concept of a fraction.
Deﬁnition 11 (Fractions). Let f :⊆X ⇒Y be a multi-valued function and
0 < n ≤m ∈N. We deﬁne the fraction
n
mf :⊆X ⇒Y m such that
n
mf(x)
is the set of all (y1, ..., ym) ∈range(f)m with |{i : yi ∈f(x)}| ≥n for all
x ∈dom( n
mf) := dom(f).
The idea of a fraction n
mf is that it provides m potential answers for f, at least
n ≤m of which have to be correct. The uniform content of the Orevkov-Baigger
construction is then summarized in the following result.
Proposition 5 (Dimension two). 1
2C[0,1] ≤sW CC2 ≤sW C[0,1].
Proof. With Proposition 4 we obtain 1
2C[0,1] = CC2◦M and hence 1
2C[0,1] ≤sW CC2.
The other reduction follows from CC2 ≤sW C[0,1]2 ≡sW C[0,1].
That is, given a closed set A ⊆[0, 1] we can utilize connected choice CC2 of
dimension 2 in order to ﬁnd a pair of points (x, y) one of which is in A. This
result directly implies the counterexample of Baigger [1] because the fact that
there are non-empty co-c.e. closed sets A ⊆[0, 1] without computable point
immediately implies that
1
2C[0,1] is not non-uniformly computable (i.e., there
are computable inputs without computable outputs) and hence CC2 is also not
non-uniformly computable.
Corollary 3 (Orevkov 1963, Baigger 1985). There exists a computable
function f : [0, 1]2 →[0, 1]2 that has no computable ﬁxed point x ∈[0, 1]2. There
exists a non-empty connected co-c.e. closed subset A ⊆[0, 1]2 without computable
point.
We mention that Proposition 5 does not directly imply C[0,1] ≡sW CC2, since
1
2C[0,1] <W CC2. In the following result we summarize the known relations for
connected choice in dependency of the dimension.
Proposition 6. We obtain CC0 <W CC1 <W CC2 ≤W CCn ≡W C[0,1] for n ≥3.
Altogether, we are left with the major open problem whether C[0,1] ≤W CC2 holds
or not. We have a conjecture but currently no proof of it.
Conjecture 1 (Brouwer Fixed Point Theorem in dimension two). We conjecture
that CC2 <W C[0,1].
We mention that this conjecture is equivalent to the property that CC2 is not
parallelizable, i.e., to the property that 
CC2 ≡W CC2 does not hold. This is be-
cause 
CC2 ≡W C[0,1] follows from C{0,1} ≤sW CC2 and 
C{0,1} ≡sW C[0,1] and the
fact that parallelization is a closure operator, which are known results (see [3]).

66
V. Brattka, S. Le Roux, and A. Pauly
6
Conclusions
We have systematically studied the uniform computational content of the
Brouwer Fixed Point Theorem for any ﬁxed dimension and we have obtained
a systematic classiﬁcation that leaves only the status of the two-dimensional
case unresolved. Besides a solution of this open problem, one can proceed into
several diﬀerent directions.
For one, one could study generalizations of the Brouwer Fixed Point Theo-
rem, such as the Schauder Fixed Point Theorem or the Kakutani Fixed Point
Theorem. On the other hand, one could study results that are based on the
Brouwer Fixed Point Theorem, such as equilibrium existence theorems in com-
putable economics (see for instance [15]). Nash equilibria existence theorems
have been studied in [13] and they can be seen to be strictly simpler than the
general Brouwer Fixed Point Theorem (in fact they can be considered as linear
version of it). In this context the question arises of how the Brouwer Fixed Point
Theorem can be classiﬁed for other subclasses of continuous functions, such as
Lipschitz continuous functions?
Acknowledgements. This project has been supported by the National Re-
search Foundation of South Africa (NRF) and by the German Research Foun-
dation (DFG) through the German-South African project (DFG, 445 SUA-1
13/20/0).
References
1. Baigger, G.: Die Nichtkonstruktivit¨at des Brouwerschen Fixpunktsatzes. Arch.
Math. Logik Grundlag. 25, 183–188 (1985)
2. Brattka, V., de Brecht, M., Pauly, A.: Closed choice and a Uniform Low Basis
Theorem. Annals of Pure and Applied Logic 163(8), 986–1008 (2012)
3. Brattka, V., Gherardi, G.: Eﬀective choice and boundedness principles in com-
putable analysis. The Bulletin of Symbolic Logic 17(1), 73–117 (2011)
4. Brattka, V., Gherardi, G.: Weihrauch degrees, omniscience principles and weak
computability. The Journal of Symbolic Logic 76(1), 143–176 (2011)
5. Brattka, V., Gherardi, G., Marcone, A.: The Bolzano-Weierstrass theorem is the
jump of weak K˝onig’s lemma. Annals of Pure and Applied Logic 163, 623–655
(2012)
6. Gherardi, G., Marcone, A.: How incomputable is the separable Hahn-Banach the-
orem? Notre Dame Journal of Formal Logic 50, 393–425 (2009)
7. Hertling, P.: Unstetigkeitsgrade von Funktionen in der eﬀektiven Analysis. Infor-
matik Berichte 208, FernUniversit¨at Hagen, Hagen (November 1996)
8. Hoyrup, M., Rojas, C., Weihrauch, K.: Computability of the Radon-Nikodym
Derivative. In: L¨owe, B., Normann, D., Soskov, I., Soskova, A. (eds.) CiE 2011.
LNCS, vol. 6735, pp. 132–141. Springer, Heidelberg (2011)
9. Ishihara,
H.:
Reverse
mathematics
in
Bishop’s
constructive
mathematics.
Philosophia Scientiae, Cahier special 6, 43–59 (2006)
10. Le Roux, S., Ziegler, M.: Singular coverings and non-uniform notions of closed set
computability. Mathematical Logic Quarterly 54(5), 545–560 (2008)

On the Computational Content of the Brouwer Fixed Point Theorem
67
11. Miller, J.S.: Pi-0-1 Classes in Computable Analysis and Topology. Ph.D. thesis,
Cornell University, Ithaca, USA (2002)
12. Orevkov, V.: A constructive mapping of the square onto itself displacing every con-
structive point (Russian). Doklady Akademii Nauk 152, 55–58 (1963); translated
in: Soviet Math. - Dokl. 4, 1253–1256 (1963)
13. Pauly, A.: How incomputable is ﬁnding Nash equilibria? Journal of Universal Com-
puter Science 16(18), 2686–2710 (2010)
14. Pauly, A.: On the (semi)lattices induced by continuous reducibilities. Mathematical
Logic Quarterly 56(5), 488–502 (2010)
15. Richter, M.K., Wong, K.C.: Non-computability of competitive equilibrium. Eco-
nomic Theory 14(1), 1–27 (1999)
16. Shioji, N., Tanaka, K.: Fixed point theory in weak second-order arithmetic. Annals
of Pure and Applied Logic 47, 167–188 (1990)
17. Simpson, S.G.: Subsystems of Second Order Arithmetic. Perspectives in Mathe-
matical Logic. Springer, Berlin (1999)
18. von Stein, T.: Vergleich nicht konstruktiv l¨osbarer Probleme in der Analysis. Diplo-
marbeit, Fachbereich Informatik, FernUniversit¨at Hagen (1989)
19. Weihrauch, K.: The degrees of discontinuity of some translators between represen-
tations of the real numbers. Technical Report TR-92-050, International Computer
Science Institute, Berkeley (July 1992)
20. Weihrauch, K.: The TTE-interpretation of three hierarchies of omniscience princi-
ples. Informatik Berichte 130, FernUniversit¨at Hagen, Hagen (September 1992)
21. Weihrauch, K.: Computable Analysis. Springer, Berlin (2000)

Square Roots and Powers
in Constructive Banach Algebra Theory
Douglas S. Bridges1 and Robin S. Havea2
1 Department of Mathematics & Statistics, University of Canterbury, Private Bag
4800, Christchurch, New Zealand
d.bridges@math.canterbury.ac.nz
2 Department of Mathematics & Computing Science,
University of the South Paciﬁc, Suva, Fiji
robin.havea@usp.ac.fj
Abstract. Several new and improved results about positive integral
powers of hermitian elements, and square roots of positive elements, in
a Banach algebra are proved constructively.
1
Introduction
The purpose of this article is to extend our earlier constructive1 work on hermi-
tian and positive elements of a separable complex Banach algebra B with identity
e [6,5,9]. In particular, we provide conditions—one of which was, unfortunately,
lacking in Theorem 4.2 of [6] and the corresponding result in [9]—under which
we can prove constructively that positive integral powers of a hermitian element
are hermitian; also, we substantially generalise, by a relatively elementary proof,
the result in [5] that yields the existence and uniqueness of the square root of a
positive element of B.
Although we shall refer to the [6] for much of the background material needed
for this paper, for the reader’s convenience we here re-present some important
notions. First, let B′ denote the dual of B. In general, we cannot prove that every
f ∈B is normed in the sense that ∥f∥≡sup {|f(x)| : x ∈B, ∥x∥⩽1} exists.
However, even when f need not be normed, we adopt the shorthand ∥f∥⩽c to
signify that |f(x)| ⩽c for all x ∈B with ∥x∥⩽1. An element f of B′ is nonzero
if |f(x)| > 0 for some x ∈B. For each dense sequence (xn)n⩾1 in B we introduce
the corresponding double norm on B′, deﬁned by |||f||| ≡∞
n=1 2−n |f(xn)|.
The topology induced by this norm on the unit ball B′
1 ≡{f ∈B′ : ∥f∥⩽1}
of B′ is independent of the dense sequence relative to which the double norm is
deﬁned, and is, in fact, the weak∗topology on B′
1.
Now, we may not be able to prove constructively that the state space VB =
{f ∈B′ : f(e) = 1 = ∥f∥} of B is inhabited (that is, contains an element), let
alone weak∗compact as it is classically. For this reason we introduce, for each
t > 0, the approximation
1 We work entirely within the framework of Bishop-style constructive analysis
(BISH—for more on which, see [2,7,8]).
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 68–77, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Square Roots and Powers in Constructive Banach Algebra Theory
69
V t
B = {f ∈B′ : ∥f∥⩽1, |1 −f(e)| ⩽t}
to VB. The constructive Hahn-Banach theorem ([8], Theorem 5.3.3) is strong
enough for us to prove that V t
B is inhabited; moreover, it is weak∗compact for
all but countably many t > 0. We say that t > 0 is admissible if V t
B is weak∗
compact. We describe VB as ﬁrm if (i) it is weak∗compact and (ii) for each
ε > 0, there exists an admissible t > 0 such that for each f ∈V t
B, there exists
g ∈VB with |||f −g||| < ε. The following result is proved in [6] (Proposition
2.4).
Proposition 1. If B has ﬁrm state space, then so does each Banach subalgebra
of B.
We call an element x of B hermitian if for each ε > 0, there exists t > 0 such
that |Im f(x)| < ε for all f ∈V t
B; and positive—when we write x ⩾0—if for
each ε > 0, there exists t > 0 such that Re f(x) ⩾−ε and |Im f(x)| < ε for all
f ∈V t
B. These deﬁnitions of hermitian and positive are classically equivalent to
the standard classical ones found in [3], which are constructively too weak to be
of much use. The following appears as Lemma 4.1 of [6].
Lemma 1. Suppose that VB is ﬁrm. Then a ∈B is hermitian if and only if
f(a) ∈R for each f ∈VB, and a ⩾0 if and only if f(a) ⩾0 for each f ∈VB.
By a character of B we mean a nonzero multiplicative linear functional u :
B →C; such a mapping satisﬁes u(e) = 1 and is normed, with ∥u∥= 1. The
character space ΣB of B comprises all characters of B and is a subset of the
unit ball of B′. We cannot prove constructively that the character space of every
commutative Banach algebra B is inhabited, let alone that, as classically, it is
weak∗compact; see page 452 of [2]. However, as we shall see in Proposition 4,
we can construct elements of ΣB under certain conditions on B.
We say that an element x of B is strongly hermitian (resp. strongly pos-
itive) if it is hermitian (resp., positive) and the state space of the closed sub-
algebra A of B generated by {e, x} is the closed convex hull of ΣA. Classically,
the latter condition always holds (see [3], page 206, Lemma 3), so every hermi-
tian (resp. positive) x is strongly hermitian (resp. strongly positive). The main
results of this paper are the following.
Theorem 1. Let B be have ﬁrm state space, and let a be a strongly hermi-
tian element of B. Then an is hermitian, and a2n is positive, for each positive
integer n.
Theorem 2. Let B be have ﬁrm state space. Let a be a strongly positive element
of B, and A the Banach algebra generated by {e, a}. Then there exists a unique
positive element x of A such that x2 = a.
The ﬁrst of these is a corrected version of [6] (Theorem 4.2), in which we should
have had a hypothesis ensuring that the product of two positive elements of

70
D.S. Bridges and R.S. Havea
A is positive. Theorem 2 replaces the restrictive requirement that B be semi-
simple, used in [5] (Theorem 3), by the more widely applicable strong positivity
hypothesis on a.
2
Products of Hermitian/Positive Elements
When is the product of two hermitian/positive elements of a Banach algebra
hermitian/positive?
Proposition 2. Let B be commutative, with ﬁrm state space, and suppose that
VB is the weak∗-closed convex hull of ΣB.Then the product of two hermitian
elements of B is hermitian, and the product of two positive elements is positive.
Proof. Let x and y be hermitian elements of B. Given f ∈VB and ε > 0, pick
elements uk (1 ⩽k ⩽m) of ΣB, and corresponding nonnegative numbers λk,
such that m
k=1 λk = 1 and |f(xy) −m
k=1 λkuk (xy)| < ε. Since ΣB ⊂VB, we
have uk(x), uk(y) ∈R, by Lemma 1; whence
|Im f (xy)| ⩽

m

k=1
λk Im uk(xy)
 +
f(xy) −
m

k=1
λkuk (xy)
 < 0 + ε = ε.
But ε > 0 is arbitrary, so Im f(xy) = 0 and therefore f(xy) ∈R. Moreover, if
x ⩾0 and y ⩾0, then
Re f(xy) ⩾
m

k=1
λk Re (uk(x)uk(y)) −
f(xy) −
m

k=1
λkuk (xy)
 > 0 −ε = −ε.
Since ε > 0 is arbitrary, it follows from Lemma 1 that, when x, y are hermitian,
f(xy) ∈R, and when they are positive, f(xy) ⩾0.
We are now prepared for the Proof of Theorem 1. Under its hypotheses, let
A be the closed subalgebra of B generated by {e, a}. By Proposition 1, VA is
ﬁrm; since a is strongly hermitian, the hypotheses of Proposition 2 are satisﬁed,
and the desired conclusions follow almost immediately.
■
By a positive linear functional on B we mean an element f of B′ such that
f(x) ⩾0 for each positive element of B; we write f ⩾0 to signify that f is
positive. Every element of the state space VB is positive (see Section 3 of [6]).
Consider a convex subset K of B′
1. We say that f ∈K is a classical extreme
point of K if
∀g,h∈K

f = 1
2 (g + h) ⇒g = h = f

,
and an extreme point of K if
∀ε>0 ∃δ>0∀g,h∈K
f −1
2 (g + h)
 < δ ⇒|||g −h||| < ε

.
An extreme point is a classical extreme point, and the converse holds classi-
cally if K is also weak∗compact. If f is an extreme point of K relative to

Square Roots and Powers in Constructive Banach Algebra Theory
71
one double norm on B′, then it is an extreme point relative to any other dou-
ble norm on B′. Proposition 3.1 of [6] states that if the state space VB is
ﬁrm, then every extreme point of VB is an extreme point of the convex set
K0 = {f ∈B′ : f ⩾0, f(e) ⩽1}.
We omit the proof of our next result, which is very close to that on page 38
of [10].
Lemma 2. Suppose that B is commutative and that the product of two positive
elements of B is positive. Let f be a classical extreme point of K0. Then f(xy) =
f(x)f(y) for all x ∈B and all positive y ∈B.
Proposition 3. Suppose that B is generated by commuting positive elements,
and that the product of two positive elements of B is positive. Then every classical
extreme point of K0 is a multiplicative linear functional on B.
Proof. Clearly, B is commutative. Let f be a classical extreme point of K0,
and consider ﬁrst the case where f is nonzero. A simple induction based on
Lemma 2 shows that
f(xyn) = f(x)f(y)n
(x ∈B, y ∈B, y ⩾0) .
(1)
Given any x, y ∈B and ε > 0, pick positive elements a1, . . . , an and a com-
plex polynomial p(ζ1, . . . , ζn) such that ∥y −z∥< ε, where z ≡p(a1, . . . , an).
Since ﬁnite products of positive elements of B are positive, we see from (1) that
f(xz) = f(x)f(z); whence
|f(xy) −f(x)f(y)| ⩽|f(xy −xz)| + |f(x)f(z) −f(x)f(y)|
⩽∥x (y −z)∥+ |f(x)| |f(z −y)|
⩽2 ∥x∥∥y −z∥⩽2 ∥x∥ε.
Since ε > 0 is arbitrary, we conclude that f(xy) = f(x)f(y). Finally, to remove
the condition that f be nonzero, let x, y ∈B and suppose that f(xy) ̸= f(x)f(y).
Then, by the foregoing, f cannot be nonzero; so f = 0 and therefore f(xy) =
0 = f(x)f(y), a contradiction. Thus we have ¬ (f(xy) ̸= f(x)f(y)) and therefore
f(xy) = f(x)f(y).
Proposition 4. Suppose that B is generated by commuting positive elements
and has ﬁrm state space, and that the product of two positive elements of B is
positive. Let A be a unital Banach subalgebra of B. Then ΣA is inhabited, and
VA is the double-norm-closed convex hull of ΣA.
Proof. By Proposition 1, VA is ﬁrm and hence compact. Since VA is also convex,
it follows from the Krein-Milman theorem ([2], page 363, Theorem (7.5)) that
it has extreme points and is the double-norm-closed convex hull of the set of
those extreme points. By Proposition 3.1 of [6], every extreme point of VA is an
extreme point, and hence a classical extreme point, of K0. Since the elements of
VA are nonzero, the result now follows from Proposition 3.

72
D.S. Bridges and R.S. Havea
Proposition 4 readily yields a partial converse of Proposition 2:
Corollary 1. Let a be an element of B all of whose positive integer powers are
positive, and let A be the closed subalgebra of B generated by {e, a}. Then ΣA
is inhabited, and VA is the double-norm-closed convex hull of ΣA.
When—as in the Banach algebra C(X), where X is a compact metric space—is
a hermitian element expressible as a diﬀerence of positive elements? To answer
this, we need to say more about approximations to the character space.
For any dense sequence (xn)n⩾1 in B, we can ﬁnd a strictly decreasing se-
quence (tn)n⩾1 of positive numbers converging to 0 such that for each n the
set
Σtn
B ≡{u ∈B′
1 : |u (xjxk) −u(xj)u(xk)| ⩽tn (1 ⩽j, k ⩽n)
∧|1 −u(e)| ⩽tn}
is (inhabited and ) weak∗compact ([2], page 460, Proposition (2.7)). The inter-
section of these sets is the character space ΣB. For each x ∈B we deﬁne
∥x∥Σtn
B ≡sup

|u(x)| : u ∈Σtn
b

,
which exists since the function x ⇝|u(x)| is uniformly continuous on the double-
norm compact set Σtn
b .
We recall two important result from constructive Banach algebra theory.
Proposition 5.
Sinclair’s theorem: If x is a hermitian element of the Ba-
nach algebra B, then ∥xn∥1/n = ∥x∥for each positive integer n ([4], pages 293–
303).
Proposition 6. Let B be commutative, and let (tn)n⩾1 be a decreasing sequence
of positive numbers converging to 0 such that Σtn is compact for each n. Then
the sequences
	
∥xn∥1/n
n⩾1 and (∥x∥Σtn )n⩾1 are equiconvergent: that is, for
each term am of one sequence and each ε > 0, there exists N such that bn ⩽
am + ε whenever bn is a term of the other sequence with n ⩾N ([2], Chapter 9,
Proposition (2.9)).
Of particular importance for us is the following:
Corollary 2. Let B be commutative, and let (tn)n⩾1 be a decreasing sequence
of positive numbers converging to 0 such that Σtn
B is compact for each n. Let h
be a hermitian element of B. Then limn→∞∥h∥Σtn
B = ∥h∥.
Proof. By Sinclair’s theorem, ∥hn∥1/n = ∥h∥for each positive integer n. It fol-
lows from Proposition 6 that for each ε > 0, there exists N such that ∥h∥Σtn
B <
∥h∥+ ε for all n ⩾N. By that same proposition, for each n ⩾N, there exists
m such that ∥h∥= ∥hm∥1/m ⩽∥h∥Σtn
B + ε. Hence
∥h∥−∥h∥Σtn
B
 < ε for all
n ⩾N.

Square Roots and Powers in Constructive Banach Algebra Theory
73
Lemma 3. If x, y are commuting hermitian elements of B, then
max {∥x∥, ∥y∥} ⩽∥x + iy∥.
Proof. Replacing B by the closed subalgebra generated by {e, x, y}, let (tn)n⩾1
be a decreasing sequence of positive numbers converging to 0 such that Σtn is
compact for each n. Since Σtn
B ⊂V tn
B and x, y are hermitian, there exists N such
that min {|Im u(x)| , |Im u(y)|} < ε for each n ⩾N and each u ∈Σtn
B . For such
u we have
|u(x + iy)| ⩾|Re u(x + iy)| = |Re u(x) −Im u(y)| > |Re u(x)| −ε
and therefore |Re u(x)| < |u(x + iy)| + ε ⩽∥x + iy∥+ ε. But
|u(x)|2 = (Re u(x))2 + (Im u(x))2 < |Re u(x)|2 + ε2,
so |u(x)|2 ⩽(∥x + iy∥+ ε)2 + ε2. Since u ∈tn
B is arbitrary, we conclude that
∥x∥2
Σtn
B ⩽(∥x + iy∥+ ε)2 + ε2. Now, x is hermitian, so by Sinclair’s theorem,
∥xn∥1/n = ∥x∥for each positive integer n. It follows from Corollary 2 that
∥x∥2 = limn→∞∥x∥2
Σtn
B ⩽∥x + iy∥2, and hence that ∥x∥⩽∥x + iy∥. Finally,
replacing x, y by −y, x in the foregoing, we obtain ∥y∥⩽∥−y + ix∥= ∥x + iy∥.
Proposition 7. Suppose that B is generated by commuting positive elements,
and that the product of two positive elements of B is positive. Then for each
hermitian element x of B and each ε > 0, there exist positive a, b ∈B with
∥x −(a −b)∥< ε.
Proof. There exist commuting positive elements z1, . . . , zm of B with each ∥zk∥⩽
1, and a polynomial p (ζ1, . . . , ζm) over C, such that
∥x −p (z1, . . . , zm)∥< ε.
Write
p (ζ1, . . . , ζm) ≡
n

i1,...,im=1
α(i1, . . . , im)ζi1
1 · · · ζim
m
where each α (i1, . . . , im) ∈C. Note that each term zi1
1 · · · zim
m is positive. Per-
turbing each coeﬃcient α (i1, . . . , im) by a suﬃciently small amount, we can
arrange that Re α (i1, . . . , im) ̸= 0 and Im α (i1, . . . , im) ̸= 0 for each tuple
(i1, . . . , im). Let
P ≡{(i1, . . . , im) : Re α(i1, . . . , im) > 0} ,
Q ≡{(i1, . . . , im) : Re α(i1, . . . , im) < 0} ,
a ≡

(i1,...,im)∈P
Re α(i1, . . . , im)zi1
1 · · · zim
m ,
b ≡−

(i1,...,im)∈Q
Re α(i1, . . . , im)zi1
1 · · · zim
m .

74
D.S. Bridges and R.S. Havea
Then a ⩾0 and b ⩾0. Moreover,
ε > ∥x −p (z1, . . . , zm)∥
=

x −(a −b) −i
⎛
⎝
n

i1,...,im=1
(Im α(i1, . . . , im)) zi1
1 · · · zim
m
⎞
⎠

where both x −(a −b) and n
i1,...,im=1 (Im α(i1, . . . , im)) zi1
1 · · · zim
m are hermi-
tian; whence ∥x −(a −b)∥< ε, by the preceding lemma.
The approximation of hermitian elements by diﬀerences of two positive elements,
as in the preceding proposition, is related to classical work on V -algebras and
the Vidav-Palmer theorem (see, in particular, Lemma 8 in §38 of [3]). We intend
exploring that further in a future paper.
3
The Path to Theorem 2
Our proof of Theorem 2 requires yet more preliminaries, beginning with an
estimate that will lead to the continuity of positive square root extraction.
Lemma 4. Let p be a positive element of the Banach algebra B such that ∥p∥⩽
1, and let A be the Banach algebra generated by {e, p}. Let 0 < δ1, δ2 ⩽1, and
suppose that there exist positive elements b1, b2 of A such that b2
1 = e −δ1p and
b2
2 = e −δ2p. Then
∥b1 −b2∥2 ⩽68
3 |δ1 −δ2| (1 + ∥p∥) .
Proof. Given ε > 0, let
α =

1
3 (|δ1 −δ2| (1 + ∥p∥) + 2ε2).
Pick t0 > 0 such that: V t0
A and Σt0
A are compact,
u(b2
1) −u(b1)2 < α2 and
u(b2
2) −u(b2)2 < α2 for each u ∈Σt0
A , and
min {Re f(b1), Re f(b2)} ⩾−α and max {Im f(b1), f(b2)} ⩽α for each f ∈V t0
A
For each u ∈Σt0
A we have
|u (b1 −b2)| |u (b1 + b2)| =
u (b1)2 −u (b2)2
⩽
u

b2
1 −b2
2
 +
u(b2
1) −u(b1)2 +
u(b2
2) −u(b2)2
<
b2
1 −b2
2
 + 2ε2 = |δ1 −δ2| (1 + ∥p∥) + 2ε2 = 3α2.
Either |u (b1 −b2)| < 2α or |u (b1 −b2)| > α. In the latter case,
|Re u(b1) + Re u(b2)| ⩽|u(b1 + b2)| < 3α.

Square Roots and Powers in Constructive Banach Algebra Theory
75
Suppose that Re u(b1) > 4α. Then Re u(b2) < 3α −Re u(b1) < −α, which, since
u ∈V t0
A , contradicts our choice of t0. Hence −α ⩽Re u(b1) ⩽4α and therefore
|Re u(b1)| ⩽4α; so
u(b1)2 = (Re u(b1))2 + (Im u(b1))2 ⩽17α2
and therefore |u(b1)| ⩽
√
17α. Likewise, |u(b2)| ⩽
√
17α, so |u(b1 −b2)| ⩽
2
√
17α, an inequality that also holds in the case |u (b1 −b2)| < 2α. Since u ∈Σt0
A
is arbitrary, we now see that ∥b1 −b2∥2
Σt0
A < 68α2. But b1 −b2 is hermitian, so,
by Corollary 2,
∥b1 −b2∥2 ⩽∥b1 −b2∥2
Σt0
A < 68
3

|δ1 −δ2| (1 + ∥p∥) + 2ε2
.
Since ε > 0 is arbitrary, we now obtain the desired conclusion.
Proposition 8. Let B have ﬁrm state space, let a be a strongly positive element
of B such that ∥a∥< 1, and let A be the Banach algebra generated by {e, a}.
Then there exists a positive element s of A such that s2 = e −a.
Proof. Our proof is based on that of Bonsall and Duncan [3] (page 207, Lemma
7). Those authors use the Gelfand representation theorem and Dini’s theorem,
the latter lying outside the reach of BISH (see [1]). However, we can avoid
those two theorems altogether, as follows. First, we note that, by Lemma 5 of
[6], e −a ⩾0 and ∥e −a∥⩽1. Consider the special case where ∥a∥< 1. Let
x0 = 0 and, for each n,
xn+1 = 1
2(a + x2
n).
(2)
A simple induction shows that xn belongs to A. Noting that x1 = 1
2a, suppose
that ∥xn∥< ∥a∥; then
∥xn+1∥⩽1
2
	
∥a∥+ ∥xn∥2
< 1
2
	
∥a∥+ ∥a∥2
= 1 + ∥a∥
2
∥a∥< ∥a∥.
Thus ∥xn∥< ∥a∥for each n. Next, observe that, by Proposition 1, VA is ﬁrm;
it follows from Proposition 2 that the product of two positive elements of A is
positive. Thus if xn ⩾0, then x2
n ⩾0, so a + x2
n ⩾0 and therefore xn+1 ⩾0;
since x0 ⩾0, we conclude that xn ⩾0 for each n. In particular, x1−x0 = x1 ⩾0.
Now suppose that xn −xn−1 ⩾0. Then since xn, xn−1,and therefore xn + xn−1
are all positive elements of A,
xn+1 −xn = 1
2

x2
n −x2
n−1

= 1
2 (xn + xn−1) (xn −xn−1) ⩾0.
Moreover,
∥xn+1 −xn∥⩽1
2 (∥xn∥+ ∥xn−1∥) ∥xn −xn−1∥⩽∥a∥∥xn −xn−1∥,

76
D.S. Bridges and R.S. Havea
so, by another induction, ∥xn+1 −xn∥⩽∥a∥n ∥x1∥= 1
2 ∥a∥n+1. It follows that
if m > n ⩾1, then
∥xm −xn∥⩽
m−1

k=n
∥xk+1 −xk∥⩽
m−1

k=n
1
2 ∥a∥k+1
⩽1
2 ∥a∥n+1
∞

k=0
∥a∥k =
∥a∥n+1
2 (1 −∥a∥) →0 as n →∞.
Hence (xn)n⩾1 is a Cauchy sequence in the Banach algebra A and therefore
converges to a limit x ∈A. Clearly, x is positive, ∥x∥⩽∥a∥< 1, and x commutes
with a. Letting n →∞in (2), we obtain x =
1
2

a + x2
. Hence (e −x)2 =
e−2x+(2x−a) = e−a. Moreover, e−x ∈A and, by Lemma 5 of [6], e−x ⩾0.
Thus s ≡e −x is a positive square root of e −a in A.
Now consider the general case where ∥a∥⩽1. For each integer n ⩾2 set
rn = 1 −n−1. Then rna ⩾0 and ∥rna∥< 1. By the foregoing, there exists a
positive element sn of A with s2
n = e −rna. Taking p = e −a, δ1 =
1
m, and
δ2 = 1
n in Lemma 4 now yields
∥sm −sn∥2 ⩽68

1
m −1
n
 (1 + ∥a∥) ,
from which we see that (sn)n⩾1 is a Cauchy sequence in A. Since A is complete,
this sequence has a limit s ∈A. Clearly, s ⩾0 and s2 = e −a. Finally, taking
p = e −a and δ1 = δ2 = 1 in Lemma 3, we see that s is the unique positive
square root of e −a in A.
Finally, we have the Proof of Theorem 2. Under the hypotheses of that
theorem, if ∥a∥⩽1, then by Lemma 5 of [5], e −a ⩾0 and ∥e −a∥⩽1;
whence, by Proposition 8, there exists a unique positive element b of A such that
b2 = e −(e −a) = a. In the general case, compute δ > 0 such that ∥δa∥⩽1.
There exists a unique positive element p of A such that p2 = δa. Then δ−1/2p is
a positive element of A, and

δ−1/2p
2 = a. Moreover, if b is a positive square
root of a, then δ1/2b is a positive square root of δa, so δ1/2b = p and therefore
b = δ−1/2p. This establishes the uniqueness of the positive square root of a.
■
Acknowledgements. The authors thank the Department of Mathematics &
Statistics at the University of Canterbury for hosting Havea on several occasions
during this work, and the Faculty of Science, Technology and Environment at
the University of the South Paciﬁc, Suva, Fiji, for supporting him during those
visits.
References
1. Berger, J., Schuster, P.M.: Dini’s theorem in the light of reverse mathematics. In:
Lindstr¨om, S., Palmgren, E., Segerberg, K., Stoltenberg-Hansen, V. (eds.) Logi-
cism, Intuitionism, and Formalism—What has become of them? Synth`ese Library,
vol. 341, pp. 153–166. Springer, Dordrecht (2009)

Square Roots and Powers in Constructive Banach Algebra Theory
77
2. Bishop, E.A., Bridges, D.S.: Constructive Analysis. Grundlehren der Mathematis-
chen Wissenschaften, vol. 279. Springer, Berlin (1985)
3. Bonsall, F.F., Duncan, J.: Complete Normed Algebras. Ergebnisse der Mathematik
und ihrer Grenzgebiete, vol. 80. Springer, Berlin (1973)
4. Bridges, D.S., Havea, R.S.: Approximating the numerical range in a Banach al-
gebra. In: Crosilla, L., Schuster, P. (eds.) From Sets and Types to Topology and
Analysis. Oxford Logic Guides, pp. 293–303. Clarendon Press, Oxford (2005)
5. Bridges, D.S., Havea, R.S.: Constructing square roots in a Banach algebra. Sci.
Math. Japon. 70(3), 355–366 (2009)
6. Bridges, D.S., Havea, R.S.: Powers of a Hermitian element. New Zealand J.
Math. 36, 1–10 (2007)
7. Bridges, D.S., Richman, F.: Varieties of Constructive Mathematics. London Math.
Soc. Lecture Notes, vol. 97. Cambridge Univ. Press (1987)
8. Bridges, D.S., Vˆıt¸˘a, L.S.: Techniques of Constructive Analysis. Universitext.
Springer, New York (2006)
9. Havea, R.S.: On Firmness of the State Space and Positive Elements of a Banach
Algebra. J. UCS 11(12), 1963–1969 (2005)
10. Holmes, R.B.: Geometric Functional Analysis and its Applications. Graduate Texts
in Mathematics, vol. 24. Springer, New York (1975)

The Mate-in-n Problem
of Inﬁnite Chess Is Decidable
Dan Brumleve1, Joel David Hamkins2,3,4, and Philipp Schlicht5
1 Topsy Labs, Inc., 140 Second Street, 6th Floor, San Francisco, CA 94105,
United States of America
2 Department of Philosophy, New York University, 5 Washington Place, New York,
NY 10003, United States of America
3 Mathematics, CUNY Graduate Center, The City University of New York, 365 Fifth
Avenue, New York, NY 10016, United States of America
4 Mathematics, College of Staten Island of CUNY, Staten Island, NY 10314,
United States of America
jhamkins@gc.cuny.edu
5 Mathematisches Institut, Rheinische Friedrich-Wilhelms-Universit¨at Bonn,
Endenicher Allee 60, 53115 Bonn, Germany
schlicht@math.uni-bonn.de
Abstract. The mate-in-n problem of inﬁnite chess—chess played on an
inﬁnite edgeless board—is the problem of determining whether a des-
ignated player can force a win from a given ﬁnite position in at most
n moves. Although a straightforward formulation of this problem leads
to assertions of high arithmetic complexity, with 2n alternating quanti-
ﬁers, the main theorem of this article nevertheless conﬁrms a conjecture
of the second author and C. D. A. Evans by establishing that it is com-
putably decidable, uniformly in the position and in n. Furthermore, there
is a computable strategy for optimal play from such mate-in-n positions.
The proof proceeds by showing that the mate-in-n problem is expressible
in what we call the ﬁrst-order structure of chess Ch, which we prove (in
the relevant fragment) is an automatic structure, whose theory is there-
fore decidable. The structure is also deﬁnable in Presburger arithmetic.
Unfortunately, this resolution of the mate-in-n problem does not appear
to settle the decidability of the more general winning-position problem,
the problem of determining whether a designated player has a winning
strategy from a given position, since a position may admit a winning
strategy without any bound on the number of moves required. This issue
is connected with transﬁnite game values in inﬁnite chess, and the exact
value of the omega one of chess ωchess
1
is not known.
Inﬁnite chess is chess played on an inﬁnite edgeless chess board, arranged like the
integer lattice Z × Z. The familiar chess pieces—kings, queens, bishops, knights,
rooks and pawns—move about according to their usual chess rules, with bishops
on diagonals, rooks on ranks and ﬁles and so on, with each player striving to place
the opposing king into checkmate. There is no standard starting conﬁguration in
inﬁnite chess, but rather a game proceeds by setting up a particular position on
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 78–88, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

The Mate-in-n Problem of Inﬁnite Chess Is Decidable
79
the board and then playing from that position. In this article, we shall consider
only ﬁnite positions, with ﬁnitely many pieces; nevertheless, the game is sensi-
ble for positions with inﬁnitely many pieces. We came to the problem through
Richard Stanley’s question on mathoverﬂow.net [6].
The mate-in-n problem of inﬁnite chess is the problem of determining for
a given ﬁnite position whether a designated player can force a win from that
position in at most n moves, counting the moves only of the designated player.
For example, ﬁgure 1 exhibits an instance of the mate-in-12 problem, adapted
from a position appearing in [2]. We provide a solution at the article’s end.
Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z
0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0
Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z
0Z0Z0j0Z0Z0Z0Z0Z0Z0J0Z0
Z0Z0S0Z0Z0Z0Z0Z0Z0Z0Z0Z
0L0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0
Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z0Z
Fig. 1. White to move on an inﬁnite, edgeless board. Can white force mate in 12 moves?
A naive formulation of the mate-in-n problem yields arithmetical assertions
of high arithmetic complexity, with 2n alternating quantiﬁers: there is a white
move, such that for any black reply, there is a counter-play by white, and so on.
In such a formulation, the problem does not appear to be decidable. One cannot
expect to search an inﬁnitely branching game tree even to ﬁnite depth. Never-
theless, the second author of this paper and C. D. A. Evans conjectured that
it should be decidable anyway, after observing that this was the case for small
values of n, and the main theorem of this paper is to conﬁrm this conjecture.
Before proceeding, let us clarify a few of the rules as they relate to inﬁnite
chess. A position should have at most one king of each color. There is no rule
for pawn promotion, as there is no boundary for the pawns to attain. In inﬁnite
chess, we abandon as limiting the 50-move rule (asserting that 50 moves without
a capture or pawn movement is a draw). A play of the game with inﬁnitely
many moves is a draw. We may therefore abandon the three-fold repetition rule,
since any repetition could be repeated endlessly and thus attain a draw, if both
players desire this, and if not, then it needn’t have been repeated. This does
not aﬀect the answer to any instance of the mate-in-n problem, because if the
opposing player can avoid mate-in-n only because of a repetition, then this is
optimal play anyway. Since we have no oﬃcial starting rank of pawns, we also
abandon the usual initial two-step pawn movement rule and the accompanying
en passant rule. Similarly, there is no castling in inﬁnite chess.
Chess on arbitrarily large ﬁnite boards has been studied, for example in [3],
but the winning positions of inﬁnite chess are not simply those that win on
all suﬃciently large ﬁnite boards. For example, two connected white rooks can
checkmate a black king on any large ﬁnite board, but these pieces have no

80
D. Brumleve, J.D. Hamkins, and P. Schlicht
checkmate position on an inﬁnite board. The lack of edges in inﬁnite chess seems
to make it fundamentally diﬀerent from large ﬁnite chess, although there may
be a link between large ﬁnite chess and inﬁnite chess on a quarter-inﬁnite board.
Since inﬁnite chess proceeds from an arbitrary position, there are a few ad-
ditional weird boundary cases that do not arise in ordinary chess. For example,
we may consider positions in which one of the players has no king; such a player
could never herself be checkmated, but may still hope to checkmate her oppo-
nent, if her opponent should have a king. We shall not consider positions in
which a player has two or more kings, although one might adopt rules to cover
this case. In ordinary chess, one may construe the checkmate winning condition
in terms of the necessary possibility of capturing the king. That is, we may imag-
ine a version of chess in which the goal is simply to capture the opposing king,
with the additional rules that one must do so if possible and one must prevent
this possibility for the opponent on the next move, if this is possible. Such a
conception of the game explains why we regard it as illegal to leave one’s own
king in check and why one may still checkmate one’s opponent with pieces that
are pinned to one’s own king,1 and gives rise to exactly the same outcomes for
the positions of ordinary chess, with the extra king-capture move simply omit-
ted. This conception directs the resolution of certain other initial positions: for
example, a position with white to move and black in check is already won for
white, even if white is in checkmate.
The stalemate-in-n problem is the problem of determining, for a given ﬁnite
position whether a designated player can force a stalemate (or a win) in at
most n moves, meaning a position from which the player to move has no legal
moves. The draw-in-n-by-k-repetition problem is the problem of determining for
a given ﬁnite position whether a designated player can in at most n moves force
a win or force the opponent into a family of k positions, inside of which he may
force his opponent perpetually to remain. The winning-position problem is the
problem of determining whether a designated player has a winning strategy from
a given ﬁnite position. The drawn-position problem is the problem of determining
whether a designated player may force a draw or win from a given ﬁnite position.
Main Theorem 1. The mate-in-n problem of inﬁnite chess is decidable.
1. Moreover, there is a computable strategy for optimal play from a mate-in-n
position to achieve the win in the fewest number of moves.
2. Similarly, there is a computable strategy for optimal opposing play from a
mate-in-n position, to delay checkmate as long as possible. Indeed, there is
a computable strategy to enable any player to avoid checkmate for k moves
from a given position, if this is possible.
3. In addition, the stalemate-in-n and draw-in-n-by-k-repetition problems are
also decidable, with computable strategies giving optimal play.
1 Even in ordinary ﬁnite chess one might argue that a checkmate position with pinned
pieces should count merely as a draw, because once the opposing king is captured,
one’s own king will be killed immediately after; but we do not advance this argument.

The Mate-in-n Problem of Inﬁnite Chess Is Decidable
81
Allow us brieﬂy to outline our proof method. We shall describe a certain ﬁrst
order structure Ch, the structure of chess, whose objects consist of all the various
legal ﬁnite positions of inﬁnite chess, with various predicates to indicate whether
a position shows a king in check or whether one position is reachable from another
by a legal move. The mate-in-n problem is easily expressed as a Σ2n ∨Π2n
assertion in the ﬁrst-order language of this structure, the language of chess. In
general, of course, there is little reason to expect such complicated assertions
in a ﬁrst-order structure to be decidable, as generally even the Σ1 assertions
about an inﬁnite computable structure are merely computably enumerable and
not necessarily decidable. To surmount this fundamental diﬃculty, the key idea
of our argument is to prove that for any ﬁxed ﬁnite collection A of pieces,
the reduct substructure ChA of positions using only at most the pieces of A is
not only computable, but in the restricted language of chess is an automatic
structure, meaning that the domain of objects can be represented as strings
forming a regular language, with the predicates of the restricted language of
chess also being regular. Furthermore, the mate-in-n problem for a given position
has the same truth value in Ch as it does in ChA and is expressible in the
restricted language of chess. We may then appeal to the decidability theorem
of automatic structures [5,1], which asserts that the ﬁrst order theory of any
automatic structure is decidable, to conclude that the mate-in-n problem is
decidable. An alternative proof proceeds by arguing that ChA is interpretable
in Presburger arithmetic ⟨N, +⟩, and this also implies that the theory of chess
with pieces from A is decidable. The same argument methods apply to many
other decision problems of inﬁnite chess. Despite this, the method does not seem
to generalize to positions of inﬁnite game value—positions from which white
can force a win, but which have no uniform bound on the number of moves it
will take—and as a result, it remains open whether the general winning-position
problem of inﬁnite chess is decidable. Indeed, we do not know even whether the
winning-position problem is arithmetic or even whether it is hyperarithmetic.
One might begin to see that the mate-in-n problem is decidable by observing
that for mate-in-1, one needn’t search through all possible moves, because if a
distant move by a long-range piece gives checkmate, then all suﬃciently distant
similar moves by that piece also give checkmate. Ultimately, the key aspects of
chess on which our argument relies are: (i) no new pieces enter the board during
play, and (ii) the distance pieces—bishops, rooks and queens—move on straight
lines whose equations can be expressed using only addition. Thus, the structure
of chess is closer to Presburger than to Peano arithmetic, and this ultimately is
what allows the theory to be decidable.
Let us now describe the ﬁrst-order structure of chess Ch. Informally, the ob-
jects of this structure are all the various ﬁnite positions of inﬁnite chess, each
containing all the information necessary to set-up and commence play with an
instance of inﬁnite chess, namely, a ﬁnite listing of pieces, their types, their
locations and whether they are still in play or captured, and an indication of
whose turn it is. Speciﬁcally, deﬁne that a piece designation is a tuple ⟨i, j, x, y⟩,
where i ∈{K, k, Q, q, B, b, N, n, R, r, P, p} is the piece type, standing for king,

82
D. Brumleve, J.D. Hamkins, and P. Schlicht
queen, bishop, knight, rook or pawn, with upper case for white and lower case
for black; j is a binary indicator of whether the piece is in play or captured; and
(x, y) ∈Z × Z are the coordinates of the location of the piece on the chessboard
(or a default value when the piece is captured). A ﬁnite position is simply a
ﬁnite sequence of piece designations, containing at most one king of each color
and with no two live pieces occupying the same square on the board, together
with a binary indicator of whose turn it is to play next. One could easily deﬁne
an equivalence relation on the positions for when they correspond to the same
set-up on the board—for example, extra captured bishops do not matter, and
neither does permuting the piece designations—but we shall actually have no
need for that quotient. Let us denote by Ch the set of all ﬁnite positions of
inﬁnite chess. This is the domain of what we call the structure of chess Ch.
We shall next place predicates and relations on this structure in order to in-
dicate various chess features of the positions. For example, WhiteToPlay(p) holds
when position p indicates that it is white’s turn, and similarly for BlackToPlay(p).
The relation OneMove(p, q) holds when there is a legal move transforming posi-
tion p into position q. We adopt the pedantic formalism for this relation that the
representation of the pieces in p and q is respected: the order of listing the pieces
is preserved and captured pieces do not disappear, but are marked as captured.
The relation BlackInCheck(p) holds when p shows the black king to be in check,
and similarly for WhiteInCheck(p). We deﬁne BlackMated(p) to hold when it is
black’s turn to play in p, black is in check, but black has no legal move; the dual
relation WhiteMated(p) when it is white to play, white is in check, but has no
legal move. Similarly, BlackStalemated(p) holds when it is black’s turn to play,
black is not in check, but black has no legal move; and similarly for the dual
WhiteStalemated(p). The structure of chess Ch is the ﬁrst-order structure with
domain Ch and with all the relations we have mentioned here. The language is
partly redundant, in that several of the predicates are deﬁnable from the others,
so let us refer to the language with only the relations WhiteToPlay, OneMove,
BlackInCheck and WhiteInCheck as the restricted language of chess. Later, we
shall also consider expansions of the language.
Since the OneMove(p, q) relation respects the order in which the pieces are
enumerated, the structure of chess Ch naturally breaks into the disjoint compo-
nents ChA, consisting of those positions whose pieces come from a piece speciﬁ-
cation type A, that is, a ﬁnite list of chess-piece types. For example, ChKQQkb
consists of the chess positions corresponding to the white king and two queens
versus black king and one bishop problems, enumerated in the KQQkb order,
with perhaps some of these pieces already captured. Since there is no pawn pro-
motion in inﬁnite chess or any other way to introduce new pieces to the board
during play, any game of inﬁnite chess beginning from a position with piece spec-
iﬁcation A continues to have piece speciﬁcation A, and so chess questions about
a position p with type A are answerable in the substructure ChA. We consider
the structure ChA to have only the restricted language of chess, and so it is a
reduct substructure rather than a substructure of Ch.

The Mate-in-n Problem of Inﬁnite Chess Is Decidable
83
We claim that the mate-in-n problem of inﬁnite chess is expressible in the
structure of chess Ch. Speciﬁcally, for any ﬁnite list A of chess-piece types, there
are assertions WhiteWinsn(p) and BlackWinsn(p) in the restricted language of
chess, such that for any position p of type A, the assertions are true in ChA
if and only if that player has a strategy to force a win from position p in at
most n moves. This can be seen by a simple inductive argument. For the n = 0
case, and using the boundary-case conventions we mentioned earlier, we deﬁne
WhiteWins0(p) if it is black to play, black is in checkmate and white is not in
check, or it is white to play and black is in check. Next, we recursively deﬁne
WhiteWinsn+1(p) if either white can win in n moves, or it is white’s turn to
play and white can play to a position from which white can win in at most n
moves, or it is black’s turn to play and black indeed has a move (so it is not
stalemate), but no matter how black plays, white can play to a position from
which white can win in at most n moves. It is clear by induction that these
assertions exactly express the required winning conditions and have complexity
Σ2n ∨Π2n in the language of chess (since perhaps the position has the opposing
player going ﬁrst), or complexity Σ2n+1 ∨Π2n+1 in the restricted language of
chess, since to deﬁne the checkmate condition from the in-check relation adds
an additional quantiﬁer.
Lemma 1. For any ﬁnite list A of chess-piece types, the structure ChA is auto-
matic.
Proof. This crucial lemma is the heart of our argument. What it means for a
structure to be automatic is that it can be represented as a collection of strings
forming a regular language, and with the predicates also forming regular lan-
guages. The functions are replaced with their graphs and handled as predicates.
We refer the reader to [4] for further information about automatic structures.
We shall use the characterization of regular languages as those consisting of the
set of strings that are acceptable to a read-only Turing machine. We shall repre-
sent integers x and y with their signed binary representation, consisting of a sign
bit, followed by the binary representation of the absolute value of the integer.
Addition and subtraction are recognized by read-only Turing machines.
Let us discuss how we shall represent the positions as strings of symbols. Fix
the ﬁnite list A of chess-piece types, and consider all positions p of type A. Let
N be the length of A, that is, the number of pieces appearing in such positions p.
We shall represent the position p using 3N + 1 many strings, with end-of-string
markers and padding to make them have equal length. One of the strings shall
be used for the turn indicator. The remaining 3N strings shall represent each of
the pieces, using three strings for each piece. Recall that each piece designation
of p consists of a tuple ⟨i, j, x, y⟩, where i is the piece type, j is a binary indicator
for whether the piece is in play or captured and (x, y) are the coordinates of the
location of the piece in Z × Z. Since we have ﬁxed the list A of piece types,
we no longer need the indicator i, since the kth piece will have the type of the
kth symbol of A. We represent j with a string consisting of a single bit, and we
represent the integers x and y with their signed binary representation. Thus, the
position altogether consists of 3N+1 many strings. Oﬃcially, one pads the strings

84
D. Brumleve, J.D. Hamkins, and P. Schlicht
to the same length and interleaves them together into one long string, although
for regularity one may skip this interleaving step and simply work with the
strings on separate tapes of a multi-tape machine. Thus, every position p ∈ChA
is represented by a unique sequence of 3N + 1 many strings. We now argue
that the collection of such sequences of strings arising from positions is regular,
by recognizing with a read-only multi-tape Turing machine that they obey the
necessary requirements: the turn indicator and the alive/captured indicators are
just one bit long; the binary representation of the locations has the proper form,
with no leading zeros (except for the number 0 itself); the captured pieces display
the correct default location information; and no two live pieces occupy the same
square. If all these tests are passed, then the input does indeed represent a
position in Ch.
Next, we argue that the various relations in the language of chess are regular.
For this, it will be helpful to introduce a few more predicates on the collection of
strings. From the string representing a position p, we can directly extract from
it the string representing the location of the ith piece in that position. Similarly,
the coding of whose turn it is to play is explicitly given in the representation, so
the relations WhiteToPlay(p) and BlackToPlay(p) are regular languages.
Consider the relation Attacki(p, x, y), which holds when the ith piece of the
position represented by p is attacking the square located at (x, y), represented
with signed binary notation, where by attack we mean that piece i could move
so as to capture an opposing piece on that square, irrespective of whether there
is currently a friendly piece occupying that square or whether piece i could not
in fact legally move to that square because of a pin condition. We claim that
the Attack relation is a regular language. It suﬃces to consider the various piece
types in turn, once we know the piece is alive. If the ith piece is a king, we
check that its location in p is adjacent to (x, y), which is a regular language
because { (c, d) | |c −d| = 1 }, where c and d are binary sequences representing
signed integers, is regular. Similarly, the attack relation in the case of pawns and
knights is also easily recognizable by a read-only Turing machine. Note that for
bishops, rooks and queens, other pieces may obstruct an attack. Bishops move
on diagonals, and two locations (x0, y0) and (x1, y1) lie on the same diagonal if
and only if they have the same sum x0 + y0 = x1 + y1 or the same diﬀerence
x0−y0 = x1−y1, which is equivalent to x0+y1 = x1+y0, and since signed binary
addition is regular, this is a regular condition. When two locations (x0, y0) and
(x1, y1) lie on the same diagonal, then a third location (a, b) obstructs the line
connecting them if and only if it also has that sum a + b = x0 + y0 = x1 + y1 or
diﬀerence a −b = x0 −y0 = x1 −y1, and also has x0 < a < x1 or x1 < a < x0.
This is a regular condition on the six variables (x0, y0, x1, y1, a, b), because it
can be veriﬁed by a read-only Turing machine. The order relation x < y is a
regular requirement on pairs (x, y), because one can check it by looking at the
signs and the ﬁrst bit of diﬀerence. So the attack relation for bishops is regular.
Rooks move parallel to the coordinate axes, and so a rook at (x0, y0) attacks
the square at (x1, y1), if it is alive and these two squares are diﬀerent but lie
either on the same rank y0 = y1, or on the same ﬁle x0 = x1, and there is no

The Mate-in-n Problem of Inﬁnite Chess Is Decidable
85
obstructing piece. This is clearly a condition that can be checked by a read-only
Turing machine, and so it is a regular requirement. Finally, the attack relation
for queens is regular, since it reduces to the bishop and rook attack relations.
It follows now that the relation WhiteInCheck(p), which holds when the po-
sition shows the white king in check, is also regular. With a read-only Turing
machine we can clearly recognize whether the white king is indicated as alive in
p, and then we simply take the disjunction over each of the black pieces listed in
A, as to whether that piece attacks the location square of the white king. Since
the regular languages are closed under ﬁnite unions, it follows that this relation
is regular. Similarly the dual relation BlackInCheck(p) is regular.
Consider now the Movei(p, x, y) relation, similar to the attack relation, but
which holds when the ith piece in position p is alive and may legally move to the
square (x, y). It should be the correct player’s turn; there should be no obstruct-
ing pieces; the square at (x, y) should not be occupied by any friendly piece; and
the resulting position should not leave the moving player in check. Each of these
conditions can be veriﬁed as above. A minor complication is presented by the
case of pawn movement, since pawns move diﬀerently when capturing than when
not capturing, and so for pawns one must check whether there is an opposing
piece at (x, y) if the pawn should move via capture.
Consider next the relation OneMovei(p, q), which holds when position p is
transformed to position q by a legal move of piece i. With a read-only Turing
machine, we can easily check that position p indicates that it is the correct
player’s turn, and by the relations above, that piece i may legally move to its
location in q, that any opposing piece occupying that square in p is marked as
captured in q, and that none of the other pieces of p are moved or have their
capture status modiﬁed in q. Thus, this relation is a regular language.
Finally, we consider the relation OneMove(p, q) in ChA, which holds when
position p is transformed to position q by a single legal move. This is simply
the disjunction of OneMovei(p, q) for each piece i in A, and is therefore regular,
since the regular languages are closed under ﬁnite unions.
Thus, we have established that the domain of ChA is regular and all the
predicates in the restricted language of chess are regular, and so the structure
ChA is automatic, establishing the lemma.
We now complete the proof of the main theorem. Since the structure ChA is
automatic, it follows by the decidability theorem of automatic structures [5,1]
that the ﬁrst-order theory of this structure is uniformly decidable. In particular,
since the mate-in-n question is expressible in this structure—and we may freely
add constant parameters—it follows that the mate-in-n question is uniformly
decidable: there is a computable algorithm, which given a position p and a natu-
ral number n, determines yes-or-no whether a designated player can force a win
in at most n moves from position p. Furthermore, in the case that the position p
is mate-in-n for the designated player, then there is a computable strategy pro-
viding optimal play: the designated player need only ﬁrst ﬁnd the smallest value
of n for which the position is mate-in-n, and then search for any move leading

86
D. Brumleve, J.D. Hamkins, and P. Schlicht
to a mate-in-(n −1) position. This value-reducing strategy achieves victory in
the fewest possible number of moves from any ﬁnite-value position. Conversely,
there is a computable strategy for the losing player from a mate-in-n position
to avoid inadvertantly reducing the value on a given move, and thereby delay
the checkmate as long as possible. Indeed, if a given position p is not mate-in-n,
then we may computably ﬁnd moves for the opposing player that do not inadver-
tantly result in a mate-in-n position. Finally, we observe that the stalemate-in-n
and draw-in-n-by-k-repetition problems are similarly expressible in the structure
ChA, and so these are also decidable and admit computable strategies to carry
them out. This completes the proof of the main theorem.
An essentially similar argument shows that the structure of chess ChA, for any
piece speciﬁcation A, is deﬁnable in Presburger arithmetic ⟨N, +⟩. Speciﬁcally,
one codes a position with a ﬁxed length sequence of natural numbers, where
each piece is represented by a sequence of numbers indicating its type, whether
it is still in play, and its location (using extra numbers for the sign bits). The
point is that the details of our arguments in the proof of the main theorem show
that the attack relation and the other relations of the structure of chess are
each deﬁnable from this information in Presburger arithmetic. Since Presburger
arithmetic is decidable, it follows that the theory of ChA is also decidable.
We should like to emphasize that our main theorem does not appear to settle
the decidability of the winning-position problem, the problem of determining
whether a designated player has a winning strategy from a given position. The
point is that a player may have a winning strategy from a position, without there
being any ﬁnite bound on the number of moves required. Black might be able to
delay checkmate any desired ﬁnite amount, even if every play ends in his loss,
and there are positions known to be winning but not mate-in-n for any n. These
are precisely the positions with inﬁnite game value in the recursive assignment of
ordinal values to winning positions: already-won positions for white have value
0; if a position is white-to-play, the value is the minimum of the values of the
positions to which white may play, plus one; if it is black-to-play, and all legal
plays have a value, then the value is the supremum of these values. The winning
positions are precisely those with an ordinal value, and this value is a measure
of the distance to a win. A mate-in-n position, with n minimal, has value n. A
position with value ω has black to play, but any move by black will be mate-
in-n for white for some n, and these are unbounded. The omega one of chess,
denoted ωchess
1
, is deﬁned in [2] to be the supremum of the values of the ﬁnite
positions of inﬁnite chess. The exact value of this ordinal is an open question,
although an argument of Blass appearing in [2] establishes that ωchess
1
≤ωck
1 ,
as well as the accompanying fact that if a player can win from position p, then
there is a winning strategy of hyperarithmetic complexity. Although we have
proved that the mate-in-n problem is decidable, we conjecture that the general
winning-position problem is undecidable and indeed, not even arithmetic.
Consider brieﬂy the case of three-dimensional inﬁnite chess, as well as higher-
dimensional inﬁnite chess. Variants of three-dimensional (ﬁnite) chess arose in

The Mate-in-n Problem of Inﬁnite Chess Is Decidable
87
the late nineteenth century and have natural inﬁnitary analogues. Without elab-
orating on the details—there are various reasonable but incompatible rules for
piece movement—we remark that the method of proof of our main theorem
works equally well in higher dimensions.
Corollary 1. The mate-in-n problem for k-dimensional inﬁnite chess is
decidable.
Results in [2] establish that the omega one of inﬁnite positions in three-
dimensional inﬁnite chess is exactly true ω1; that is, every countable ordinal
arises as the game value of an inﬁnite position of three-dimensional inﬁnite chess.
We conclude the article with a solution to the chess problem we posed in
ﬁgure 1. With white Qe5+, the black king is forced to the right kg4, and then
white proceeds in combination Rg3+ kh4 Qg5+ ki4, rolling the black king to-
wards the white king, where checkmate is delivered on white’s 13th move. Alter-
natively, after Qe5+ kg4, white may instead play Ks4, moving his king to the
left, forcing the two kings together from that side, and this also leads to check-
mate by the queen on the 13th move. It is not possible for white to checkmate in
12 moves, because if the two kings do not share an adjacent square, there is no
checkmate position with a king, queen and rook versus a king. Thus, white must
force the two kings together, and this will take at least 12 moves, after which the
checkmate move can now be delivered, meaning at least 13 moves.2 However,
white can force a stalemate in 12 moves, by moving Qe5+, and then afterwards
moving only the white king towards the black king, achieving stalemate on the
12th move, as the black king is trapped on f4 with no legal move. White can
force a draw by repetition in 3 moves, by trapping the black king in a 4 × 4 box
with the white queen and rook at opposite corners, via Qe5+ kg4 Ri3 kh4 Qf6+,
which then constrains the black king to two squares.
Acknowledgements. The research of the second author has been supported
in part by grants from the National Science Foundation, the Simons Foundation
and the CUNY Research Foundation.
References
1. Blumensath, A., Gr¨adel, E.: Automatic structures. In: 15th Annual IEEE Sympo-
sium on Logic in Computer Science, Santa Barbara, CA, pp. 51–62. IEEE Comput.
Soc. Press, Los Alamitos (2000), http://dx.doi.org/10.1109/LICS.2000.855755
2. Evans, C.D., Hamkins, J.D., Woodin, W.H.: Transﬁnite game values in inﬁnite chess
(in preparation)
3. Fraenkel, A.S., Lichtenstein, D.: Computing a perfect strategy for n × n chess re-
quires time exponential in n. J. Combin. Theory Ser. A 31(2), 199–214 (1981),
http://dx.doi.org/10.1016/0097-3165(81)90016-9
2 C. D. A. Evans (US national master) conﬁrms this mate-in-13-but-not-12 analysis.

88
D. Brumleve, J.D. Hamkins, and P. Schlicht
4. Khoussainov, B., Minnes, M.: Three lectures on automatic structures. In: Logic
Colloquium 2007. Lect. Notes Log, vol. 35, pp. 132–176. Assoc. Symbol. Logic, La
Jolla (2010)
5. Khoussainov, B., Nerode, A.: Automatic Presentations of Structures. In: Leivant,
D. (ed.) LCC 1994. LNCS, vol. 960, pp. 367–392. Springer, Heidelberg (1995)
6. Stanley (mathoverﬂow.net/users/2807), R.: Decidability of chess on an inﬁnite
board. MathOverﬂow, http://mathoverflow.net/questions/27967 (version: July
20, 2010)

A Note on Ramsey Theorems and Turing Jumps
Lorenzo Carlucci1 and Konrad Zdanowski2
1 Dipartimento di Informatica, Universit`a di Roma Sapienza, Via Salaria 113,
00198 Roma, Italy
carlucci@di.uniroma1.it
2 Uniwersytet Kardynala Stefana Wyszy´nskiego w Warszawie, ul. Dewajtis 5,
01-815 Warszawa, Poland
k.zdanowski@uksw.edu.pl
Abstract. We give a new treatment of the relations between Ramsey’s
Theorem, ACA0 and ACA′
0. First we combine a result by Girard with
a colouring used by Loebl and Neˇsetril for the analysis of the Paris-
Harrington principle to obtain a short combinatorial proof of ACA0 from
Ramsey Theorem for triples. We then extend this approach to ACA′
0
using a characterization of this system in terms of preservation of well-
orderings due to Marcone and Montalb´an. We ﬁnally discuss how to
apply this method to ACA+
0 using an extension of Ramsey’s Theorem for
colouring relatively large sets due to Pudl`ak and R¨odl and independently
to Farmaki.
1
Introduction
The proof-theoretic and computability-theoretic strength of Ramsey Theorem
have been intensively studied. By Ramsey’s Theorem for n-tuples and c colours
we here mean the assertion that every colouring of the n-tuples of N in c colours
admits an inﬁnite monochromatic set. We refer to this principle by RTn
c . In the
context of Reverse Mathematics [12], a full characterization is known for the
Inﬁnite Ramsey Theorem for n-tuples with n ≥3. In particular, it is known that
for every n ≥3, RT3
2 is equivalent to ACA0 (i.e., to RCA0 augmented by the
assertion that the Turing jump of any set exists) and that ∀nRTn
2 is equivalent
to ACA′
0 (i.e., RCA0 augmented by the assertion that for all n the n-th Turing
jump of any set exists).
By a well-ordering preservation principle we mean an assertion of the form
∀X(WO(X) →WO(F(X))), where X is a linear order, WO(F) is the Π1
1 sentence
expressing that X is a well-ordering, and F is an operator from linear orders to
linear orders. An old result of Girard [4] shows that ACA0 is equivalent to the
well-ordering preservation principle ∀X(WO(X) →WO(2X )), or, equivalently,
to ∀X(WO(X) →WO(ωX )), where 2X and ωX have the expected meaning.
Well-ordering preservation principles have recently attracted new interest. Ana-
logues of Girard’s result for systems stronger that ACA0 have been recently ob-
tained by Afshari and Rathjen [1] and independently by Marcone and Montalb´an
[9] using proof-theoretic and computability-theoretic methods, respectively. The
systems ACA′
0, ACA+
0 , Π0
ωα-CA0 and ATR0 have been proved equivalent to
well-ordering preservation principles of increasing strength.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 89–95, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

90
L. Carlucci and K. Zdanowski
In this note we outline a combinatorial method for obtaining implications of
the form “This type of Ramsey Theorem implies closure under this type of Tur-
ing jump over RCA0”. In particular we combine Girard’s characterization of
ACA0 and Marcone-Montalb`an’s characterization of ACA′
0 with some parti-
tions deﬁned by Loebl and Neˇsetril in their beautiful combinatorial proof of the
independence of Paris-Harrington Theorem from Peano Arithmetic [8] to obtain
new proofs of the following results.
RCA0 ⊢RT4
2 →ACA0, and RCA0 ⊢∀nRTn
2 →ACA′
0.
Furthermore, we strongly conjecture that this approach can be extended to prove
that a Ramsey-type theorem for bicolourings of exactly large sets due to Pudl`ak-
R¨odl [11] and independently to Farmaki (see, e.g., [3]) implies ACA+
0 (i.e.,
equivalently, RCA0 augmented by the assertion that the ω Turing jump of any
set exists). The eﬀective and proof-theoretic content of this theorem have been
recently fully analyzed by the authors in [2]. We conjecture that the method
can be extended to relate the general version of the latter theorem [3] to the
systems Π0
ωα-CA0, for α ∈ωck
1 , using their characterization in [9] in terms of
the well-ordering preservation principles ∀X(WO(X) →WO(ϕ(α, X))).
2
RT3, ACA0, and ω-Exponentiation
We give a new proof of the fact that Ramsey Theorem for triples implies Arith-
metical Comprehension (over RCA0). We use Girard’s result [4,6,9] that
∀X(WO(X) →WO(ωX )) implies ACA0 over RCA0. We give a direct combina-
torial argument showing that Ramsey Theorem for triples implies ∀X(WO(X) →
WO(ωX)). Surprisingly, the needed colourings come from Loebl and Neˇsetril’s [8]
combinatorial proof of the unprovability of the Paris-Harrington principle from
Peano Arithmetic.
For linear orders we use the same notations as in [9]. In particular, we deﬁne
the following operator ωX from linear orders to linear orders. Given a linear
order X, ωX is the set of ﬁnite strings ⟨x0, x1, . . . , xk−1⟩of elements of X where
x0 ≥X x1 ≥X · · · ≥X xk−1. The intended meaning of ⟨x0, x1, . . . , xk−1⟩is ωx0 +
· · · + ωxk−1. The order ≤ωX on ωX is the lexicographic order. If α = ωα0n0 +
· · · + ωαknk in Cantor Normal Form, we denote ni by ci(α) and αi by ei(α). If
α > β we denote by Δ(α, β) the least index at which the Cantor Normal Forms
of α and β diﬀer.
Proposition 1. Over RCA0, RT3
3 implies ∀X(WO(X) →WO(ωX)).
Proof. Assume RT3
3. Suppose ¬WO(ωX ). We show ¬WO(X). We deﬁne a colour-
ing C(α) : [N]3 →3 with an explicit sequence parameter α of intended type
α : N →field(ωX). C(α) (with parameter α) is deﬁned by the following case
distinction.
C(α)(i, j, k) =
⎧
⎪
⎨
⎪
⎩
1
if Δ(αi, αj) > Δ(αj, αk),
2
if Δ(αi, αj) ≤Δ(αj, αk) ∧cΔ(αi,αj)(αi) > cΔ(αj,αk)(αj),
3
if Δ(αi, αj) ≤Δ(αj, αk) ∧cΔ(αi,αj)(αi) ≤cΔ(αj,αk)(αj).

A Note on Ramsey Theorems and Turing Jumps
91
Let α : N →field(ωX) be an inﬁnite descending sequence in ωX. Let H be
an inﬁnite C(α)-homogeneous set. Consider (βi)i∈N, where βi = αhi and H =
{h1, h2, . . . } in increasing order.
Case 1. The colour of C(α) on [H]3 is 1. Then
Δ(β1, β2) > Δ(β2, β3) > . . .
Contradiction, since Δ(βi, βi+1) ∈N.
Case 2. The colour of C(α) on [H]3 is 2. Then
cΔ(β1,β2) > cΔ(β2,β3) > . . .
Contradiction, since cΔ(βi,βi+1) ∈N.
Case 3. The colour of C(α) on [H]3 is 3. If Δ(βi, βj) = Δ(βj, βk), since
cΔ(βi,βj)(βi) ≤cΔ(βj,βk)(βj), and βi > βj, it must be the case that eΔ(βi,βj)(βi) >
eΔ(βj,βk)(βj).
If Δ(βi, βj) < Δ(βj, βk) then eΔ(βi,βj)(βi) > eΔ(βj,βk)(βj), since βi > βj > βk.
Thus, in any case
eΔ(β1,β2)(β1) > eΔ(β2,β3)(β2) > . . . .
In other words, α′ : N →X deﬁned by
i →eΔ(αhi ,αhi+1)(αhi),
is the desired inﬁnite descending sequence in X.
⊓⊔
Corollary 1. Over RCA0, RT3
3 implies ACA0.
Recall the following result of Jockusch’s (a similar observation is made at the
end of [8]).
Proposition 2 (Jockusch [7]). For every n, c, for every recursive C : [N]n →
c there exists a recursive C′ : [N]n+1 →2 such that the C′-homogeneous sets are
just the C-homogeneous sets.
Proof. For the proof it is suﬃcient to deﬁne C′ as follows. For S ∈[N]n+1,
C′(S) = 0 if [S]n is C-homogeneous and C′(S) = 1 otherwise.
⊓⊔
Thus, restriction to two colours is inessential for the study of the strength of
Ramsey Theorem.
Corollary 2. Over RCA0, RT4
2 implies ACA0.
3
∀nRTn, ACA′
0, and Iterated ω-Exponentiation
We give a new combinatorial proof of the fact that the full Ramsey Theorem
∀nRTn implies closure under all ﬁnite Turing jumps (over RCA0). This result
is originally due to McAloon [10].

92
L. Carlucci and K. Zdanowski
We
use Marcone and Montalb`an’s [9] result that ∀n∀X(WO(X)
→
WO(ω⟨n,X ⟩)) is equivalent to ACA′
0 over RCA0. We give a direct combina-
torial argument showing that Ramsey Theorem implies the latter well-ordering
preservation principle. Again, the needed colourings come from [8].
We deﬁne, as in [9], ω⟨0,X ⟩= X and ω⟨n+1,X ⟩= ωω⟨n,X⟩.
Proposition 3. (RCA0) ∀n∀cRTn
c implies ∀n∀X(WO(X) →WO(ω⟨n,X ⟩))).
Proof. We deﬁne a family of colourings C(α)
h
: [N]h+2 →d(h) for h ≥1, where d
is a primitive recursive function to be read oﬀfrom the proof. The deﬁnitions of
the colourings C(α)
h
feature an explicit inﬁnite sequence parameter α of intended
type α : N →field(ω⟨h,X ⟩). C(α)
1
is the colouring C(α) : [N]3 →3 deﬁned in the
proof of Proposition 1. We deﬁne C(α)
h
for h ≥2 as follows. Given i1, . . . , ih+2
let
v1 = (C(α)
1
(i1, i2, i3), C(α)
1
(i2, i3, i4), . . . , C(α)
1
(ih−1, ih, ih+1)),
and
v2 = (C(α)
1
(i2, i3, i4), C(α)
1
(i3, i4, i5), . . . , C(α)
1
(ih, ih+1, ih+2)).
C(α)
h
(i1, . . . , ih+2) =

(v1, v2)
if ¬(v1 = v2 = (3, . . . , 3)),
C(α[i1,...,ih+2])
h−1
(i1, . . . , ih+1)
otherwise.
where α[i1, . . . , ih+2] is the sequence
α1, . . . , αi1−1, eΔ(αi1,αi2 )(αi1), . . . , eΔ(αih+1 ,αih+2)(αih+1), αih+2, αih+2+1, . . . .
For well-deﬁnedness, we observe that in the second case the arguments are
strictly decreasing in ω⟨h−1,X ⟩.
When
considering C(α)
h
for
a
ﬁxed sequence α
we sometimes write
Ch(αi1, . . . , αih+2) for C(α)
h
(i1, . . . , ih+2).
Let h and α : N →field(ω⟨h,X ⟩) be given such that α is strictly descending.
Let H be an inﬁnite C(α)
h
-homogeneous set. For h = 1 we have already proved
the theorem. Let h ≥2. We show how to compute an X-descending sequence
given α and H. Let {s1, s2, . . . , } be an enumeration of H in increasing order.
We ﬁrst argue that for every j1 < · · · < jh+2, the corresponding values v1 and
v2 relative to αsj1 , . . . , αsjh+2 satisfy v1 = v2 = (3, . . . , 3).
To exclude the other cases we argue as follows. Let β1, . . . , βh+2 in the rest of
the argument be arbitrary αi1, . . . , αih+2 with {i1, . . . , ih+2}< ⊂H.
Case 1. The colour is (v1, v2) with v1 ̸= v2. This is easily seen to be impossible.
Let s be minimum such that v1 and v2 diﬀer at position s for the ﬁrst time. Then
C1(βi, βi+1, βi+2) = C1(βi+1, βi+2, βi+3), for i < s but C1(βs, βs+1, βs+2) ̸=
C1(βs+1, βs+2, βs+3). Now consider the (h+2)-tuple (β2, β3, . . . , βh+2, β∗) where
β∗is any choice of an element smaller than βh+2 in the inﬁnite homogeneous
set H. The ﬁrst component of the vector associated to this tuple is v2. But then
v1 = v2 should hold by homogeneity.
Case 2. The colour is (v, v) for some v and v ̸= (3, 3, . . . , 3).

A Note on Ramsey Theorems and Turing Jumps
93
Case 2.1. v ̸= (i, i, . . . , i) for i ∈{1, 2}. This is easily seen to be impossible. Let
s ∈[1, h−1] be minimum such that i=C1(βs, βs+1, βs+2) ̸= C1(βs+1, βs+2, βs+3)=
j. Then v1 = (i, i, . . . , i, j, . . . ). On the other hand i = C1(β2, β3, β4) = · · · =
C1(βs, βs+1, βs+2) and v2 = v1 imposes C1(βs+1, βs+2, βs+3) = i. Contradiction.
Case 2.2. If v = (1, . . . , 1) then the sequence (Δ(βi, βi+1))i∈N is strictly de-
scending in N.
Case 2.3. If v = (2, . . . , 2) then the sequence (cΔ(βi,βi+1))i∈N is strictly de-
scending in N.
Since Cases 1 and 2 cannot hold, we have the following.
(∃b)(∀j1, . . . , jh+2)[C
α[sj1 ,...,sjh+2 ]
h−1
(sj1, . . . , sjh+1) = b].
Therefore
(∀j1 < · · · < jh+2)[eΔ(αsj1 ,αsj2 )(αsj1 ) > · · · > eΔ(αsjh+1 ,αsjh+2 )(αsjh+1 )].
From this we can conclude that the sequence α′ deﬁned as follows
n →eΔ(αsn,αsn+1)(αsn)
is inﬁnite descending in ω⟨h−1,X ⟩. Furthermore we claim that C(α′)
h−1 is constant.
Let j1 < · · · < jh+1 and i1 < · · · < ih+1 be arbitrary. We show that
C(α′)
h−1(sj1, . . . , sjh+1) = C(α′)
h−1(si1, . . . , sih+1).
The following chain of identities holds by C(α)
h
-homogeneity of H and by deﬁni-
tion of C(α)
h
and of α′.
Cα′
h−1(j1, . . . , jh+1) =
Ch−1(eΔ(αsj1 ,αsj2 )(αsj1 ), . . . , eΔ(αsjh+1 ,αsjh+1+1)(αsjh+1 ))
=
C
α[sj1 ,...,sjh+1 ,sjh+1+1]
h−1
(sj1, . . . , sjh+1)
=
C(α)
h
(sj1, . . . , sjh+1, sjh+1+1)
=
C(α)
h
(si1, . . . , sih+1, sih+1+1)
=
C
α[si1 ,...,sih+1 ,sih+1+1]
h−1
(si1, . . . , sih+1)
=
Ch−1(eΔ(αsi1 ,αsi2 )(αsi1 ), . . . , eΔ(αsih+1 ,αsih+1+1)(αsih+1 ))
=
C(α′)
h−1(si1, . . . , sih+1)
Then, by iterating the above argument (h −1) times we obtain the desired
descending sequence in X, computably in α and H.
⊓⊔
Corollary 3. Over RCA0 ∀nRTn
2 implies ACA′
0.
Proof. We just need to replace each colouring C(α)
h
: [N]h+2 →d(h) used in the
proof of Proposition 3 by a colouring D(α)
h
: [N]h+3 →2 such that C(α)
h
and
D(α)
h
have the same homogeneous sets. This is possible by Proposition 2.
⊓⊔

94
L. Carlucci and K. Zdanowski
4
Large Sets, ACA+
0 , and the ε Function
We discuss how to apply the proof-technique from the previous sections to a
natural extension of Ramsey Theorem for colouring relatively large sets (in the
sense of Paris and Harrington [5]). A ﬁnite set X ⊆N is large if card(X) >
min(X) and is exactly large if card(X) = min(X) + 1. The principle of interest
is the following.
Theorem 1 (Pudl`ak-R¨odl [11] and Farmaki [3]). For every inﬁnite subset
M of N, for every colouring C of the exactly large subsets of N in two colours,
there exists an inﬁnite set L ⊆M such that every exactly large subset of L gets
the same colour by C.
We refer to the statement of the above Theorem as RT(!ω). The eﬀective and
proof-theoretic content of RT(!ω) has been recently characterized in [2]. In that
paper it is shown that RT(!ω) is equivalent to ACA+
0 over RCA0. We strongly
conjecture that the technique from the previous sections can be applied to give a
completely diﬀerent proof of the implication RCA0 ⊢RT(!ω) →ACA+
0 , using
Marcone-Montalb`an’s result that ∀X(WO(X) →WO(εX )) implies ACA+
0 over
RCA0 (Theorem 5.23 in [9]).
We use the following notation from [9]. An ordering εX is deﬁned from a
linear order X as follows. The elements of the new ordering are the formal
terms deﬁned inductively as follows. (1) 0, and εx for x ∈X, (2) If t1, t2 are
formal terms then t1 + t2 is a formal term. (3) If t is a formal term then ωt is
a formal term. By induction on terms we deﬁne the order relation ≤εX and a
normal form simultaneously as follow. A term t = t0 + · · ·+ tk is in normal form
if either t = 0 (i.e., k = 0 and t0 = 0) or the following points (A-B) hold. (A)
t0 ≥εX t1 ≥εX · · · ≥εX tk, and (B) each ti is either 0, εx with x ∈X or ωsi where
si is in normal form and si ̸= εx for any x. Every term t can be written in normal
form using the following points (i-iv). (i) + is associative, (ii) s + 0 = 0 + s = s,
(iii) If s <εX r then ωs + ωr = ωr, (iv) ωεx = εx. Given t = t0 + · · · + tn and
s = s0 + · · · + sm in normal form, t ≤εX s if and only if either (a) t = 0, or (b)
t = εx and εy occurs in s for some y ≥X x, or (c) t = ωt′, s0 = εy and t′ ≤εX εy,
or (d) t = ωt′, s0 = ωs′ and t′ ≤εX s′, or (e) k > 0 and t0 <εX s0, or (f) k > 0
and t0 = s0, m > 0 and t1 + · · · + tn ≤εX s1 + · · · + sm.
The next proposition follows from Theorem 9 in [2] and Theorem 3.4 in [9].
We believe that an alternative proof can be obtained using the techniques of
the previous sections, by taking the proof of Theorem 3.4 in [9] as a model.
We know how to reduce towers of exponentiation of arbitrary height starting
with homogeneous sets for the colourings from Proposition 3. The extraction
procedure in Proposition 3 uses the following computable operation on notations:
from a sequence (αi)i∈I the sequence (eΔ(αin,αin+1)(αin))n∈N is extracted, where
{i1, i2, . . . } is an enumeration of I in increasing order.
Proposition 4. (RCA0) RT(!ω) implies ∀X(WO(X) →WO(εX )).

A Note on Ramsey Theorems and Turing Jumps
95
References
1. Afshari, B., Rathjen, M.: Reverse Mathematics and well-ordering principles: a pilot
study. Ann. Pure Appl. Log. 160(3), 231–237 (2009)
2. Carlucci, L., Zdanowski, K.: The strength of Ramsey Theorem for colouring rela-
tively large sets, http://arxiv.org/abs/1204.1134
3. Farmaki, V., Negrepontis, S.: Schreier Sets in Ramsey Theory. Trans. Am. Math.
Soc. 360(2), 849–880 (2008)
4. Girard, J.-Y.: Proof Theory and Logical Complexity. Biblipolis, Naples (1987)
5. Harrington, L., Paris, J.: A mathematical incompleteness in Peano Arithmetic. In:
Barwise, J. (ed.) Handbook of Mathematical Logic, pp. 1133–1142. North-Holland
(1977)
6. Hirst, J.: Reverse Mathematics and ordinal exponentiation. Ann. Pure App.
Log. 66(1), 1–18 (1994)
7. Jockusch Jr., C.G.: Ramsey’s Theorem and Recursion Theory. J. Symb. Log. 37(2),
268–280 (1972)
8. Loebl, M., Neˇsetˇril, J.: An unprovable Ramsey-type theorem. Proc. Am. Math.
Soc. 116(3), 819–824 (1992)
9. Marcone, A., Montalb`an, A.: The Veblen function for computability theorists. J.
Symb. Log. 76(2), 575–602 (2011)
10. McAloon, K.: Paris-Harrington incompleteness and transﬁnite progressions of theo-
ries. In: Nerode, A., Shore, R.A. (eds.) Recursion Theory. Proceedings of Symposia
in Pure Mathematics, vol. 42, pp. 447–460. American Mathematical Society (1985)
11. Pudl`ak, P., R¨odl, V.: Partition theorems for systems of ﬁnite subsets of integers.
Discret. Math. 39(1), 67–73 (1982)
12. Simpson, S.G.: Subsystems of Second Order Arithmetic. Springer (1999)

Automatic Functions, Linear Time and Learning
John Case1, Sanjay Jain2, Samuel Seah3, and Frank Stephan2,3
1 Department of Computer and Information Sciences, University of Delaware,
Newark, DE 19716-2586, United States of America
case@cis.udel.edu
2 Department of Computer Science, National University of Singapore, Singapore
117417, Republic of Singapore.
sanjay@comp.nus.edu.sg
3 Department of Mathematics, National University of Singapore, Singapore 119076,
Republic of Singapore
a0030907@nus.edu.sg, fstephan@comp.nus.edu.sg
Abstract. The present work determines the exact nature of linear
time computable notions which characterise automatic functions (those
whose graphs are recognised by a ﬁnite automaton). The paper also de-
termines which type of linear time notions permit full learnability for
learning in the limit of automatic classes (families of languages which
are uniformly recognised by a ﬁnite automaton). In particular it is shown
that a function is automatic iﬀthere is a one-tape Turing machine with
a left end which computes the function in linear time where the input
before the computation and the output after the computation both start
at the left end. It is known that learners realised as automatic update
functions are restrictive for learning. In the present work it is shown
that one can overcome the problem by providing work-tapes additional
to a resource-bounded base tape while keeping the update-time to be
linear in the length of the largest datum seen so far. In this model, one
additional such worktape provides additional learning power over the au-
tomatic learner model and the two-work-tape model gives full learning
power.
1
Introduction
In inductive inference, automatic learners and linear time learners have played
an important role, as both are considered as valid notions to model severely
resource-bounded learners. On one hand, Pitt [18] observed that recursive learn-
ers can be made to be linear time learners by delaying; on the other hand, when
learners are formalised by using automata updating a memory in each cycle
with an automatic function, the corresponding learners are not as powerful as
non-automatic learners [10] and cannot overcome their weakness by delaying.
The relation between these two models is that automatic learners are indeed lin-
ear time learners [4] but not vice versa. This motivates to study the connection
between linear time and automaticity on a deeper level.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 96–106, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Automatic Functions, Linear Time and Learning
97
It is well known that a ﬁnite automaton recognises a regular language in lin-
ear time. One can generalise the notion of automaticity from sets to relations
and functions [3,9,13] and say that a relation or a function is automatic iﬀan
automaton recognises its graph, see Section 2. For automatic functions it is not
directly clear that they are in linear time, as recognising a graph and computing
the output of a string from the input are two diﬀerent tasks. Interestingly, in
Section 2 below, it is shown that automatic functions coincide with those com-
puted by linear time one-tape Turing machines which have the input and output
both starting at the left end of the tape. In other words, a function is automatic
iﬀit is linear-time computable with respect to the most restrictive variant of
this notion; increasing the number of tapes or not restricting the position of the
output on the tape results in a larger complexity class.
Section 3 is dedicated to the question on how powerful a linear time notion
must be in order to capture full learning power in inductive inference. In respect
to the automatic learners [4,10,11], it has been the practice to consider hypothe-
ses spaces whose membership relation is automatic (that is, uniformly regular)
and which are called automatic families. It turned out that certain automatic
families which are learnable by a recursive learner cannot be learnt by an au-
tomatic learner. One can simulate automatic learners by using one tape; in the
present work this tape (called base-tape) is restricted in length by the length of
the longest datum seen so far — this results in longest word size memory limited
automatic learners as studied in [10]. In each cycle, the learner reads one datum
about the set to be learnt and revises its memory and conjecture. The question
considered is how much extra power is added to the learner by permitting ad-
ditional work-tapes which do not have length-restrictions; in each learning cycle
the learner can, however, only work on these tapes in time linear in the length
of the longest example seen so far. It can be shown that by a clever archivation
technique, two additional work tapes can store all the data observed in a way
that a successful learner can be simulated. When having only one additional
work-tape, the current results are partial: One can simulate a learner when the
time-constraint of the computation is super-linear and all languages in the class
to be learnt are inﬁnite.
2
Automatic Functions and Linear Time
Informally, an automatic function is a function from strings to strings whose
graph is recognised by a ﬁnite automaton. More formally, this is based on
the notion of convolution. The convolution of two strings x = x1x2 . . . xm and
y = y1y2 . . . yn is deﬁned as follows: Let x′ = x′
1x′
2 . . . x′
r and y′ = y′
1y′
2 . . . y′
r,
where (i) r = max({m, n}); (ii) x′
i = xi, if i ≤m, x′
i = # otherwise; (iii)
y′
i = yi, if i ≤n, y′
i = # otherwise. Now, conv(x, y) = (x′
1, y′
1)(x′
2, y′
2) . . .
(x′
r, y′
r). One can deﬁne the convolution of a ﬁxed number of strings similarly in
order to deﬁne functions which have a ﬁxed number of inputs instead of one. One

98
J. Case et al.
can use convolutions also to deﬁne functions with several inputs computed by
one-tape Turing machines; therefore the exposition in this section just follows
the basic case of mapping strings to strings.
Now a function f is called automatic iﬀthere is a ﬁnite automaton which
recognises the convoluted input-output pairs; that is, given conv(x, y), the
automaton accepts iﬀx is in the domain of f and f(x) = y. The importance of
the concept of automatic functions and automatic relations is that every func-
tion or relation, which is ﬁrst-order deﬁnable from ﬁnite number of automatic
functions and relations, is automatic again and the corresponding automaton
can be computed eﬀectively from the other automata. This gives the second nice
fact that structures consisting of automatic functions and relations have a decid-
able ﬁrst-order theory [9,13]. The main result of this section is that the following
three models are equivalent:
– automatic functions;
– functions computed in deterministic linear time by a one-tape Turing ma-
chine where input and output start at the same position;
– functions computed in non-deterministic linear time by a one-tape Turing
machine where input and output start at the same position.
This equivalence is shown in the following two results, where the ﬁrst one gen-
eralises prior work [4, Remark 2].
Theorem 1. Let f be an automatic function. Then there is a deterministic
linear time one-tape Turing machine which replaces any legal input x on the
tape by f(x) starting at the same position as x before.
Proof. Assume that a deterministic automaton with c states (numbered 1 to
c, where 1 is the starting state) accepts a word of the form conv(x, y) · (#, #)
iﬀx is in the domain of f and y = f(x); the automaton rejects any other
sequence.
Suppose input is x = x1x2 . . . xr. Now one considers the following additional
work-tape symbols consisting of all tuples (a, s1, s2, . . . , sc): where a is # or one
of xk’s, and for d ∈{1, 2, . . ., c}, sd takes the values −, + or ∗. Now consider
the k-th cell: sd = −iﬀthere is no word of the form y1y2 . . . yk−1 such that the
automaton on input (x1, y1)(x2, y2) . . . (xk−1, yk−1) reaches the state d; sd = +
iﬀthere is exactly one such word; sd = ∗iﬀthere are at least two such words.
Here the xi and yi can also be # when a word has been exhausted.
Now the Turing machine simulating the automaton replaces the cell to the
left of the input by o, the cell containing x1 by (x1, +, −, . . . , −). Then, for
each new cell with entry xk (from the input or # if that has been exhausted)
the automaton replaces xk by (xk, s1, s2, . . . , sc) under the following conditions,
(where the entry in the previous cell was (xk−1, s′
1, s′
2, . . . , s′
c)):

Automatic Functions, Linear Time and Learning
99
– sd is + iﬀthere is exactly one (yk−1, d′) such that s′
d′ is + and the automaton
transfers on (xk−1, yk−1) from state d′ to d and there is no pair (yk−1, d′)
such that s′
d′ is ∗and the automaton transfers on (xk−1, yk−1) from d′ to d;
– sd is ∗iﬀthere are at least two pairs (yk−1, d′) such that s′
d′ is + and the
automaton transfers on (xk−1, yk−1) from state d′ to d or there is at least one
pair (yk−1, d′) such that s′
d′ is ∗and the automaton transfers on (xk−1, yk−1)
from d′ to d;
– sd is −iﬀfor all pairs (yk−1, d′) such that the automaton transfers on
(xk−1, yk−1) from d′ to d, it holds that s′
d′ is −.
Note that the third case applies iﬀthe ﬁrst two do not apply. The automaton
replaces each symbol in the input as above until it reaches the cell where the
intended symbol (a, s1, s2, . . . , sc) has sd = + for some accepting state d. If this
happens, the Turing machine turns around, memorises the state d, erases this
cell and goes backward.
When the Turing machine comes backward from the cell k + 1 to the cell k,
where the state memorised for the cell k + 1 is d′, then it determines the unique
(d, yk) such that sd = + (as stored in cell k) and the automaton transfers from
d to d′ on (xk, yk); now the Turing machine replaces the symbol on cell k by yk
(if yk ̸= #) and by the blank symbol (if yk = #). Then the automaton keeps
the state d in the memory and goes to the left and repeats this process until it
reaches the cell which has the symbol o on it. Once the Turing machine reaches
there, it replaces this symbol by the blank and terminates.
For the veriﬁcation, note that the output y = y1y2 . . . (with # appended)
satisﬁes that the automaton, after reading (x1, y1)(x2, y2) . . . (xk, yk), is always
in a state d with sd = + (as written in cell k + 1 in the algorithm above), as
the function value y is unique in x; thus, whenever the automaton ends up in
an accepting state d with sd = + then the input-output-pair conv(x, y) · (#, #)
has been completely processed and x ∈dom(f) ∧f(x) = y has been veriﬁed.
Therefore, the Turing machine can turn and follow the unique path, marked by
+ symbols, backwards in order to reconstruct the output from the input and the
markings. All superﬂuous symbols and markings are removed from the tape in
this process. As y depends uniquely on x, the automaton accepting conv(x, y)
can accept at most c symbols after the word x; hence the runtime of the Turing
machine is bounded by 2 · (|x| + c + 2), that is, the runtime is linear.
Note that this Turing machine makes two passes, one from the origin to the
end of the word and one back. These two passes are needed as the function
f(x1x2 . . . xk−1xk) = xk x2 . . . xk−1x1 shows, where the ﬁrst and last symbol are
exchanged and the others remain unchanged.
For the converse direction, assume that a function is computed by a non-
deterministic one-tape Turing machine in linear time in a way that input and
output starts at the same position on the tape. For an input x, any two non-
deterministic accepting computations have to produce the same output f(x).

100
J. Case et al.
Furthermore, the runtime of each computation has to follow the same linear
bound c · (|x| + 1), independent of whether the computation ends up in an
accepting state or not.
Theorem 2. Let f be a function computed by a non-deterministic one-tape Tur-
ing machine in linear time, with the input and output starting at the same posi-
tion. Then f is automatic.
Proof. The proof is based on crossing-sequence methods, see [8] and [16, Section
VIII.1]. Without loss of generality one can assume that there is a special symbol
o left of the input occurring only there and that the automaton each time turns
when it reaches this position. Furthermore, it starts there and returns to that
position at the end; a computation accepts only when the full computation has
been accomplished and the automaton has returned to its origin o. By a result of
Hartmanis [7] and Trakhtenbrot [19], there is a constant c′ such that an accepting
computation visits each cell of the Turing tape at most c′ times; otherwise the
function f would not be linear time computable. This permits to represent the
computation locally by storing for each visit to a cell — the direction from which
the Turing machine entered the cell, in which state it was, what activity it did
and in which direction it left the cell. This gives, for each cell, only a constant
amount of information — which can be stored in the cell using a suﬃciently
large alphabet.
Now a non-deterministic automaton can recognise the set
{conv(x, y) : x ∈dom(f) ∧y = f(x)}
by guessing on each cell, the local information of the visits of the Turing ma-
chine, and comparing it with the information from the previous cell and checking
whether it is consistent; furthermore, the automaton checks whether, on the k-th
cell, yk is written after all the guessed activity of the Turing machine and whether
this activity is consistent with the initial value xk. The automaton passes over
the full word and accepts conv(x, y) iﬀthe non-deterministic computation trans-
forms some input of the form ox#∗into some output of the form oy#∗. These
techniques are standard and the ﬁnal veriﬁcation is left to the reader.
Remark 3. One might ask whether the condition on the input and output
starting at the same position is really needed. The answer is “yes”. Assume by
way of contradiction that it would not be needed and that all functions lin-
ear time computable by a one-tape Turing machine without any restrictions
on output positions are automatic. Then one could consider the free monoid
over {0, 1}. For this monoid, the following function could be computed from
conv(x, y): The output is z = f(x, y) if y = xz; the output is # if such a z does
not exist. For this, the machine just compares x1 with y1 and erases (x1, y1),
x2 with y2 and erases (x2, y2) and so on, until it reaches (a) a pair of the form
(xm, ym) with xm ̸= ym or (b) a pair of the form (xm, #) or (c) a pair of the
form (#, ym) or (d) the end of the input. In cases (a) and (b) the output has to be

Automatic Functions, Linear Time and Learning
101
# and the machine just erases all remining input symbols and puts the special
symbol # to denote the special case; in case (c) the value z is just obtained
by changing all remaining input symbols (#, yk) to yk and the Turing machine
terminates. In case (d) the valid output is the empty string and the Turing
machine codes it adequately on the tape. Hence f would be automatic. But now
one could ﬁrst-order deﬁne concatenation g by letting g(x, z) be that y for which
f(x, y) = z; this would give that the concatenation is automatic, which is known
to be false. Hence the condition on the starting-positions cannot be dropped.
3
Linear Time Learners
In order to evaluate the complexity of a learner, the following assumptions are
made.
Deﬁnition 4. A learner M is a machine which maintains some memory and in
each cycle receives as input one word to be learnt, updates its memory and then
outputs an hypothesis. The machine is organised as follows:
– Tape 0 (base tape) contains convolution of the input, the output and some
information (memory) which is not longer than the longest word seen so
far (plus a constant). Input and output on tape 0 always start at a ﬁxed
position o.
– Tapes 1, 2, . . . , k are normal tapes, whose contents and head position are
not modiﬁed during change of cycle, which the Turing machine can use for
archiving information and doing calculations.
– The machine has in each cycle a time allowance linear in the length of the
largest example seen so far. Without loss of generality, tape 0 stores this
bound.
The learner is said to have k additional work-tapes iﬀit has in addition to tape
0 also the tapes 1, 2, . . . , k.
Note that if only tape 0 is present, the model is equivalent to an automatic
learner with the memory bounded by the size of the longest datum seen so far
(plus a constant) [4,10].
The class of languages to be learnt is represented by an automatic family
{Le : e ∈I}; automatic families [10,11] are the automata-theoretic counterpart
of indexed families [1,15] which were widely used in inductive inference to repre-
sent the class to be learnt. The basic model of inductive inference [1,2,6,12,17]
is that the learner M reads cycle by cycle a list w0, w1, . . . of all the words in
a language Le and at the same time M outputs a sequence e0, e1, . . . of indices,
in each cycle one of them, such that Lek = Le for almost all k. As the equiva-
lence of indices is automatic, one can take the hypothesis space I to be one-one
and therefore the criterion would indeed have that ek = e for almost all k. In
a one-one hypothesis space, the index e of a ﬁnite language Le has, up to an
additive constant, the same length as the longest word in Le; this follows from
[11, Theorem 3.5] using that drun(R) is the longest word in R, for a ﬁnite set R.

102
J. Case et al.
This observation is crucial as otherwise the time-constraint on the learner would
prevent the learner from eventually outputting the right index; for inﬁnite lan-
guages this is not a problem as the language must contain arbitrary long words.
Angluin [1] gave a criterion when a class is learnable in general. This crite-
rion, adjusted to automatic families, says that a class is learnable iﬀfor every
e ∈I there exists a ﬁnite set D such that there is no d ∈I with D ⊆Ld ⊂Le.
The main question of this section is which learnable classes can also be learnt
by a linear-time learner with k additional work tapes. For k = 0 this is in
general not possible, as automatic learners fail to learn various learnable clas-
ses [10], for example the class of all sets {0, 1}∗−{x} and the class of all sets
Le = {x ∈{0, 1}|e| : x ̸= e}.
Freivalds, Kinber and Smith [5] introduced limitations on the long term mem-
ory into inductive inference, Kinber and Stephan [14] transferred it to the ﬁeld of
language learning. Automatic learners have similar limitations and are therefore
not able to learn all learnable automatic classes [4,10]. The usage of additional
work-tapes for linear time learners permits to overcome these limitations, the
next results specify how many additional work-tapes are needed. Recall from
above that work-tapes are said to be additional iﬀthey are in addition to the
base tape.
Theorem 5. The automatic family consisting of Lε = {0, 1}∗, Lx0 = {0, 1}∗∪
{x2} −{x} and Lx1 = {0, 1}∗∪{x2} does not have an automatic learner but has
a linear-time learner using one additional work-tape.
Proof. An automatic learner cannot memorise all the data from {0, 1}∗it sees;
therefore one can show, see [10], that there are two ﬁnite sequences of words, one
containing x and one not containing x, such that the learner has the same long
term memory after having seen both sequences. If one presents to the learner
after one of these sequences all the elements of Lx0, then the automatic learner
has actually no way to ﬁnd out whether the overall language presented is Lx0 or
Lx1, therefore it cannot learn the class.
A linear time learner with one additional work-tape (called tape 1) initially
conjectures Lε and uses tape 1 to archive all the examples seen at the current
end of the written part of the tape. When the learner sees a word of the form
x2, it maintains a copy of it in the memory part of tape 0. In each subsequent
cycle, the learner scrolls back tape 1 by one word and compares the word there
as well as the current input with x2; if one of these two is x then the learner
changes its conjecture to Lx1, else it keeps its conjecture as Lx0. In the case that
the origin of tape 1 is reached, the learner from then onwards ignores tape 1 and
only compares the incoming input with x2.
Theorem 6. Every learnable automatic family has a linear-time learner using
two additional work-tapes.
Proof. Jain, Luo and Stephan [10] showed that for every learnable automatic
family {Le : e ∈I} there is an automatic learner with a memory bound of the
length of the longest example seen so far (plus a constant) which learns the

Automatic Functions, Linear Time and Learning
103
class from every fat text (a text in which every element of the language appears
inﬁnitely often). So the main idea is to use the two additional tapes in order to
simulate and feed the learner M on tape 0 with a fat text. In each cycle, tape
0 is updated from a pair conv(memk, wk) to conv(memk+1, ek), where memk is
the long-term memory of M before the k-th cycle and wk is the k-th input word
and ek is the conjecture issued in this cycle.
The update is now done in a way such that instead of one learning cycle, two
are done by ﬁrst mapping conv(memk, wk) to conv(mem′, e′) and then mapping
conv(mem′, t) to conv(memk+1, ek), where t is the word on tape 1 at the current
position (which can be accessed by scrolling the tape accordingly). Furthermore,
after reading the word t, tape 1 is scrolled to the starting position of the previous
word; when the tape is at the beginning, the direction of taking out the words is
altered until the end of the tape is reached. In each cycle, the current datum wk
is also appended at the end of tape 2. If at the end of a cycle, words are taken
out in a forward manner from tape 1 and the end of tape 1 is reached, then the
roles of tape 1 and tape 2 are exchanged, so that each word on tape 2 is then
given to M for learning, while tape 1 is used to archive the new words.
It is easy to see that in each cycle, the time spent is proportional to |memk|+
|xk| + |t| and thus linear in the length of the longest word seen so far (plus
a constant); note that mem′, e′, ek are also bounded by that length (plus a
constant). Furthermore, each input word is inﬁnitely often put through M as
each of the words observed gets archived on one of the two tapes. Hence M
learns the language. It follows that the given automatic family {Le : e ∈I} is
learnt by a linear time Turing machine with two additional work-tapes.
Open Problem 7. It is unknown whether one can learn every in principal
learnable automatic class using an automatic learner augmented by only one
work-tape.
The next result shows that, if one allows a bit more than just linear time, then
one can learn, using one work-tape, all learnable automatic classes of inﬁnite
languages. The result could even be transferred to families of arbitrary r.e. sets
as the simulated learner is an arbitrary recursive learner.
Theorem 8. Assume that {Le : e ∈I} is an automatic family where every Le
is inﬁnite and M is a recursive learner which learns this family. Furthermore,
assume that f, g are recursive functions with the property that f(n) ≥m when-
ever n ≥g(m) (so g is some type of inverse of f). Then there is a learner N
which learns the above family, using only one additional work tape, and satisﬁes
the following constraint: if n is the length of the longest example seen so far,
then only the cells 1, 2, . . ., n of tape 0 can be non-empty and the update time of
N in the current cycle is O(n · f(n)).
Further investigations deal with the question what happens if one does not add
further worktapes to the learner but uses other methods to store memory. In-
deed, the organisation in a tape is a bit arkward and using a queue solves some
problems. A queue is a tape where one reads at one end and writes at the op-
posite end, both the reading and writing heads are unidirectional and cannot

104
J. Case et al.
overtake each other. Tape 0 satisﬁes the same constraints as in the model of
additional work tapes and one also has the constraint that in each cycle only
linearly many symbols (measured in the length of the longest datum seen so far)
is stored in the queue and retrieved from it.
Theorem 9. Every learnable automatic family has a linear-time learner using
one additional queue as a data structure.
Proof. The learner simulates an automatic learner M using fat text, similarly as
in Theorem 6. Let M in the k-th step map (memk, wk) to (memk+1, ek) for M’s
memory memk. At the beginning of a cycle the learner has conv(vk, −, memk, −)
on Tape 0 where vk is the current datum, memk the archived memory of M and
“−” refers to irrelevant or empty content. In the k-th cycle, the learner scans
four times over Tape 0 from beginning to the end and each time afterwards
returns to the beginning of the tape:
1. Copy vk from Tape 0 to the write-end of the queue;
2. Read wk from the read-end of the queue and update Tape 0 to
conv(vk, wk, memk, −);
3. Copy wk from Tape 0 to the write-end of the queue;
4. Simulate M on Tape 0 in order to map (memk, wk) to (memk+1, ek)
and update Tape 0 to conv(vk, wk, memk+1, ek).
It can easily be veriﬁed that this algorithm permits to simulate M using the
data type of a queue and that each cycle takes only time linear in the length of
the longest datum seen so far.
A further data structure investigated is to the provision of additional stacks.
Tape 0 remains a tape in this model and has still to obey to the resource-bound
of not being longer than the longest word seen so far. Theorems 5 and 6 work also
with one and two stacks, respectively, as the additional work-tapes are actually
used like stacks.
Theorem 10. There are some automatic classes which can be learnt with one
additional stack but not by an automatic learner. Furthermore, every in principle
learnable automatic class can be learnt by a learner using two additional stacks.
Furthermore, the next result shows that in general one stack is not enough; so
one additional stack gives only intermediate learning power while two or more
additional stacks give the full learning power.
Theorem 11. The class of all Le = {x ∈{0, 1}|e| : x ̸= e} with e ∈{0, 1}∗∪
{2}∗cannot be learnt by a learner using one additional stack.
4
Conclusion
Automatic functions are shown to be the same as functions computed in linear
time by one-tape Turing machines with input and output starting at the left end

Automatic Functions, Linear Time and Learning
105
of the machine. Furthermore, linear time learner can be modelled by having a
base tape of the length of the longest datum seen so far plus additional structures
which can either be additional Turing machine work tapes, queues or stacks. In
each cycle the learner runs in time linear in the longest example seen so far,
updates the base tape and accesses the additional storage devices also only to
retrieve or store a linear number of symbols. It is shown that two additional work
tapes, two additional stacks or one additional queue give full learning power;
furthermore, the learning power of one additional stack is properly intermediate
and the learning power of one additional work tape is better than no additional
work tape. It is an open problem whether there is a diﬀerence in the learning
power of one and two additional work tapes.
Acknowledgements. The second author was supported in part by NUS grants
C252-000-087-001 and R252-000-420-112. The fourth author was supported in
part by NUS grant R252-000-420-112.
References
1. Angluin, D.: Inductive inference of formal languages from positive data. Informa-
tion and Control 45, 117–135 (1980)
2. Blum, L., Blum, M.: Toward a mathematical theory of inductive inference. Infor-
mation and Control 28, 125–155 (1975)
3. Blumensath, A., Gr¨adel, E.: Automatic structures. In: 15th Annual IEEE Sympo-
sium on Logic in Computer Science (LICS), pp. 51–62 (2000)
4. Case, J., Jain, S., Le, T.D., Ong, Y.S., Semukhin, P., Stephan, F.: Automatic
Learning of Subclasses of Pattern Languages. In: Dediu, A.-H., Inenaga, S., Mart´ın-
Vide, C. (eds.) LATA 2011. LNCS, vol. 6638, pp. 192–203. Springer, Heidelberg
(2011)
5. Freivalds, R., Kinber, E., Smith, C.H.: On the impact of forgetting on learning
machines. Journal of the ACM 42, 1146–1168 (1995)
6. Mark Gold, E.: Language identiﬁcation in the limit. Information and Control 10,
447–474 (1967)
7. Hartmanis, J.: Computational complexity of one-tape Turing machine computa-
tions. Journal of the Association of Computing Machinery 15, 411–418 (1968)
8. Hennie, F.C.: Crossing sequences and oﬀ-line Turing machine computations. In:
Sixth Annual Symposium on Switching Circuit Theory and Logical Design, pp.
168–172 (1965)
9. Hodgson, B.R.: D´ecidabilit´e par automate ﬁni. Annales des sciences math´ematiques
du Qu´ebec 7(1), 39–57 (1983)
10. Jain, S., Luo, Q., Stephan, F.: Learnability of Automatic Classes. In: Dediu,
A.-H., Fernau, H., Mart´ın-Vide, C. (eds.) LATA 2010. LNCS, vol. 6031, pp. 321–
332. Springer, Heidelberg (2010)
11. Jain, S., Ong, Y.S., Pu, S., Stephan, F.: On automatic families. In: Proceedings of
the eleventh Asian Logic Conference in Honour of Professor Chong Chitat on his
Sixtieth Birthday, pp. 94–113. World Scientiﬁc (2012)
12. Jain, S., Osherson, D.N., Royer, J.S., Sharma, A.: Systems That Learn, 2nd edn.
MIT Press (1999)

106
J. Case et al.
13. Khoussainov, B., Nerode, A.: Automatic Presentations of Structures. In: Leivant,
D. (ed.) LCC 1994. LNCS, vol. 960, pp. 367–392. Springer, Heidelberg (1995)
14. Kinber, E., Stephan, F.: Language learning from texts: mind changes, limited mem-
ory and monotonicity. Information and Computation 123, 224–241 (1995)
15. Lange, S., Zeugmann, T., Zilles, S.: Learning indexed families of recursive languages
from positive data: a survey. Theoretical Computer Science 397, 194–232 (2008)
16. Odifreddi, P.: Classical Recursion Theory. Studies in Logic and the Foundations of
Mathematics, vol. II, 143. Elsevier (1999)
17. Osherson, D., Stob, M., Weinstein, S.: Systems That Learn, An Introduction to
Learning Theory for Cognitive and Computer Scientists. Bradford — The MIT
Press, Cambridge, Massachusetts (1986)
18. Pitt, L.: Inductive inference, DFAs, and Computational Complexity. In: Jantke,
K.P. (ed.) AII 1989. LNCS (LNAI), vol. 397, pp. 18–44. Springer, Heidelberg (1989)
19. Trakhtenbrot, B.A.: Turing computations with logarithmic delay. Algebra i
Logika 3, 33–48 (1964)

An Undecidable Nested Recurrence Relation
Marcel Celaya and Frank Ruskey
Department of Computer Science, University of Victoria,
Victoria, BC, V8W 3P6, Canada
Abstract. Roughly speaking, a recurrence relation is nested if it con-
tains a subexpression of the form . . . A(. . . A(. . .) . . .). Many nested re-
currence relations occur in the literature, and determining their behavior
seems to be quite diﬃcult and highly dependent on their initial condi-
tions. A nested recurrence relation A(n) is said to be undecidable if the
following problem is undecidable: given a ﬁnite set of initial conditions
for A(n), is the recurrence relation calculable? Here calculable means
that for every n ≥0, either A(n) is an initial condition or the calculation
of A(n) involves only invocations of A on arguments in {0, 1, . . . , n −1}.
We show that the recurrence relation
A (n) = A (n −4 −A (A (n −4))) + 4A (A (n −4))
+ A (2A (n −4 −A (n −2)) + A (n −2))
is undecidable by showing how it can be used, together with carefully
chosen initial conditions, to simulate Post 2-tag systems, a known Turing
complete problem.
1
Introduction
In the deﬁning expression of a recurrence relation R (n), one ﬁnds at least one
application of R to some function of n. The Fibonacci numbers, for example,
satisfy the recurrence F (n) = F (n −1) + F (n −2) for n ≥2. A recurrence
relation R (n) is called nested when the deﬁning expression of R contains at
least two applications of R, one of which is contained in the argument of the
other.
Many sequences deﬁned in terms of nested recurrences have been studied over
the years. One famous example is Hofstadter’s Q sequence, which is deﬁned by
the recurrence
Q (n) = Q (n −Q (n −1)) + Q (n −Q (n −2)) ,
(1)
with initial conditions Q (1) = Q (2) = 1. This sequence is very chaotic, and a
plot of the sequence demonstrates seemingly unpredictable ﬂuctuation about the
line y = x/2. It remains an open question whether Q is deﬁned on all positive
integers, despite its introduction in [8] over 30 years ago. Indeed, if it happens
S.B. Cooper, A. Dawar, and B. Löwe (Eds.): CiE 2012, LNCS 7318, pp. 107–117, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

108
M. Celaya and F. Ruskey
that there exists some m such that m < Q (m −1) or m < Q (m −2), then
the calculation of Q (m) would require an application of Q to a negative integer
outside its domain. While little is known about the Q sequence, other initial
conditions that give rise to much better behaved sequences that also satisfy the
Q recurrence have been discovered [7], [12].
Another sequence deﬁned in terms of a nested recurrence is the Conway-
Hofstadter sequence
C (n) = C (C (n −1)) + C (n −C (n −1)) ,
(2)
with initial conditions C (1) = C (2) = 1. Unlike the Q sequence, this se-
quence is known to be well-deﬁned for n ≥1, and in fact Conway proved that
limn→∞C (n) /n = 1/2. Plotting the function C (n) −n/2 reveals a suprising,
fractal-like structure. This sequence is analyzed in depth in [9].
Another sequence whose structure is mainly understood, but is extraordinarily
complex, is the Hofstadter-Huber V sequence, deﬁned by
V (n) = V (n−V (n−1))+V (n−V (n−4)), with V (1) = V (2) = V (3) = V (4) = 1.
(3)
It was ﬁrst analyzed by Balamoham, Kuznetsov and Tanny [2] and recently
Allouche and Shallit showed that it is 2-automatic [1].
Some of these nested recurrences are well-behaved enough to have closed
forms. Hofstadter’s G sequence, for example, is deﬁned by
G (n) = n −G (G (n −1)) , with G(0) = 0.
(4)
This sequence has closed form G (n) = ⌊(n + 1) /φ⌋, where φ is the golden
ratio [6]. A sequence due to Golomb [7], deﬁned by G (1) = 1 and G (n) =
G (n −G (n −1)) + 1 when n > 1, is the unique increasing sequence in which
every n ≥1 appears n times, and has closed form G (n) =

(1 +
√
8n)/2

.
Despite their wide variation in behaviour, all of these recursions are deﬁned
in terms of only three simple operations: addition, subtraction, and recurrence
application. Of these, the latter operation makes them reminiscent of certain
discrete systems—particularly the cellular automaton. Consider, for instance,
the Q sequence deﬁned above. It is computed at any point by looking at the two
values immediately preceeding that point, and using them as “keys” for a pair
of “lookups” on a list of previously computed values, the results of which are
summed together as the next value. It is well-known that many cellular automata
are Turing complete; an explanation of how to simulate any Turing machine
in a suitably-deﬁned one-dimensional cellular automaton is given in [13]. With
respect to nested recurrences, therefore, two questions naturally arise. First, does
there exist, in some sense, a computationally universal nested recurrence deﬁned
only in terms of the aforementioned three operations? Second, given a nested
recurrence, is it capable of universal computation? In this paper we aim to clarify
the ﬁrst question and answer it in the positive. A related approach was taken by
Conway to show that a generalization of the Collatz problem is undecidable [4].

An Undecidable Nested Recurrence Relation
109
2
Tag Systems
The tag system, introduced by Emil Post in [11], is a very simple model of com-
putation. It has been used in many instances to prove that some mathematical
object is Turing complete. This was done, for example, with the one-dimensional
cellular automaton known as Rule 110; the proof uses a variant of the tag
system [5].
Such a system consists of a ﬁnite alphabet of symbols Σ, a set of production
rules Δ : Σ →Σ∗, and an initial word W0 from Σ∗. Computation begins with
the initial word W0, and at each step of the computation the running word
w1 . . . wk is transformed by the operation
w1 . . . wk ⊢w3 . . . wkΔ (w1) .
In other words, at each step, the word is truncated by two symbols on the left
but extended on the right according to the production rule of the ﬁrst truncated
symbol. In this paper, we adopt the convention that lowercase letters represent
individual symbols while uppercase letters represent words.
If the computation at some point yields a word of length 1, truncation of the
ﬁrst two symbols cannot occur; it is at this point the system is said to halt. The
halting problem for tag systems asks: given a tag system, does it halt? As is the
case for Turing machines, the halting problem for tag systems is undecidable
[10].
Although this deﬁnition of tag systems has two symbols deleted at each step,
there’s no reason why this number need be ﬁxed at two. In general, an m-tag
system is a tag system where at each step m symbols are removed from the
beginning of the running word. The number m is called the deletion number
of the tag system. It is known that m = 2 is the smallest number for which
m-tag systems are universal [3]. Thus, only 2-tag systems are considered in the
remainder of these notes.
Example 1. Two tag systems are depicted below, both of which share alphabet
Σ = {a, b, c} and production rules Δ given by a →abb, b →c, and c →a. The
initial word of the left tag system is abcb, while the initial word of the right tag
system is abab. Observe that one is periodic while the other halts.
abcb
cbabb
abba
baabb
abbc
bcabb
abbc
· · ·
abab
ababb
abbabb
babbabb
bbabbc
abbcc
bccabb
cabbc
bbca
cac
ca
a

110
M. Celaya and F. Ruskey
3
A Modiﬁed Tag System
The goal of this paper is to show that the recurrence given in the abstract can
simulate some universal model of computation. In particular, we wish to show
that if we encode the speciﬁcation of some abstract machine as initial conditions
for our recurrence, then the resulting sequence produced by the recurrence will
somehow encode every step of that machine’s computation. The tag system
model seems like a good candidate for this purpose, since the entire run of a
tag system can be represented by a single, possibly inﬁnite string we’ll call the
computation string. For example, the string corresponding to the tag system
above and to the right is ababbabbabbccabbcacaa. A specially-constructed nested
recurrence A would need only generate such a string on N = {0, 1, 2, . . .} to
simulate a tag system; each symbol would be suitably encoded as an integer.
Ideally, the sequence deﬁned by the nested recurrence can be calculated one
integer at a time using previously computed values. It would therefore make sense
to ﬁnd some tag-system-like model of computation capable of generating these
strings one symbol at a time. That way, the computation of the nth symbol
of a string in this new model can correspond to the calculation of A (n) (or,
more likely, some argument linear in n). With this motivation in mind, we now
introduce a modiﬁcation of the tag system model.
A reverse tag system consists of a ﬁnite set of symbols Σ, a set of production
rules δ : Σ2 →Σ, a function d : Σ →N, and an initial word W0 ∈Σ∗. While
an ordinary tag system modiﬁes a word by removing a ﬁxed number of symbols
from the beginning and adding a variable number of symbols to the end, the
situation is reversed in a reverse tag system.
A single computation step of a reverse tag system is described by the operation
w1 . . . wk ⊢wd(y)+1 . . . wky,
where y = δ (w1, wk). Given a word that starts with w1 and ends with wk, the
production rule for the pair (w1, wk) yields a symbol y which is appended to the
end of the word. Then, the ﬁrst d (y) symbols are removed from the beginning
of the word. The number d (s) we’ll call the deletion number of the symbol s. If
at some point the deletion number of y exceeds k, then the reverse tag system
halts.
Example 2. Let Σ = {a, b}, d (a) = 0, and
d (b) = 2. Deﬁne δ by
(a, a) →b
(a, b) →b
(b, a) →b
(b, b) →a.
It takes 12 steps before this reverse tag sys-
tem with initial word W0 = baaab becomes
periodic.
baaab
baaaba
aabab
babb
babba
bbab
bbaba
abab
abbbb
bbaabbbab· · ·

An Undecidable Nested Recurrence Relation
111
4
Simulating a Tag System with a Reverse Tag System
Consider a tag system T = (Σ, Δ, W0) such that each production rule of Δ yields
a nonempty string. The goal of this section is to construct a reverse tag system
R = (Σ′, δ, d, W ′
0) which simulates T .
This construction begins with Σ′. Some notation will be useful to represent
the elements that are to appear in Σ′. Let [s]j denote the symbol “sj”, where s
is a symbol in Σ and j is an integer.
For each si ∈Σ, write Δ (si) as si,ℓi . . . si,2si,1. For each symbol si,j in this
word, the symbol [si,j]j shall appear in Σ′. For example, if a →abc is a pro-
duction rule of Δ, then Σ′ contains the three symbols [a]3, [b]2, and [c]1. If
W0 = q1q2 . . . qm, the symbols [q1]1 , [q2]1 , . . . , [qm]1 are also included in Σ′.
Constructed this way, Σ′ contains no more symbols than the sum of the lengths
of the words in Δ (Σ) and the word W0.
The production rules of δ include the rules
δ([si]∗, [∗]1) = [si,ℓi]ℓi
δ([si]∗, [si,j]j) = [si,j−1]j−1
taken over all si ∈Σ, all j ∈{2, 3, . . . , ℓi}, and all possibilites for the ∗’s. Note
that this speciﬁcation of δ doesn’t necessarily exhaust all possible pairs of (Σ′)2,
however, any remaining pairs can be arbitrarily speciﬁed because they are never
used during the computation of R.
Finally, the deletion numbers are speciﬁed by
d

[s]j

=

0,
j > 1
2,
j = 1
for all [s]j ∈Σ′, and if W0 = q1q2 . . . qm, then W ′
0 = [q1]1 [q2]1 . . . [qm]1.
Example 3. This example demonstrates a simulation of the tag system T on the
left in Example 1 using a reverse tag system R = (Σ′, δ, d, W ′
0).
The production rules in Example 1 are
a →abb
b →c
c →a.
To properly simulate T , the three symbols a3, b2, b1 are needed for the ﬁrst rule,
the symbol c1 is needed for the second, and the symbol a1 is needed for the third.
The initial word for R is W ′
0 = a1b1c1b1. Taking all these symbols together, we
have Σ′ = {a1, b1, c1, b2, a3}.
If we take “∗” to mean “any symbol or subscript, as appropriate,” the pro-
duction rules δ can be written as follows:
(a∗, ∗1) →a3 (b∗, ∗1) →c1 (c∗, ∗1) →a1
(a∗, a3) →b2
(a∗, b2) →b1

112
M. Celaya and F. Ruskey
Finally, every symbol with a sub-
script of 1 gets a deletion number
of two, and zero otherwise:
d (a1) = d (b1) = d (c1) = 2
d (b2) = d (a3) = 0.
The output of R is depicted to the
right. Compare the marked rows
with the output of T in Example 1.
a1b1c1b1 ←
a1b1c1b1a3
a1b1c1b1a3b2
c1b1a3b2b1 ←
a3b2b1a1 ←
a3b2b1a1a3
a3b2b1a1a3b2
b1a1a3b2b1 ←
a3b2b1c1 ←
a3b2b1c1a3
a3b2b1c1a3b2
b1c1a3b2b1 ←
a3b2b1c1 ←
· · ·
One point worth mentioning is that if a reverse tag system halts while simulating
an ordinary tag system, then the simulated tag system must halt also. However,
the converse is not true! A reverse tag system might keep rolling once it has
completed the simulation of a halting tag system. The reverse tag system in
Example 2 is a good example of this; it can survive even when there’s only one
symbol, while ordinary tag systems always require at least two.
Theorem 1. Let T = (Σ, Δ, W0) be a tag system such that each production rule
of Δ yields a nonempty string, and let R be a reverse tag system constructed as
above in terms of T . Suppose k > 0, w1 . . . wk ∈Σ∗, and Δ (w1) = zℓzℓ−1 . . . z1.
If i1, i2, . . . , ik−1 are such that [wj]ij ∈Σ′, then
[w1]i1 . . . [wk−1]ik−1 [wk]1 ⊢∗[w3]i3 . . . [wk]1 [zℓ]ℓ. . . [z1]1
in R.
Proof. We have by construction of R that
[w1]i1 . . . [wk−1]ik−1 [wk]1 ⊢[w1]i1 . . . [wk−1]ik−1 [wk]1 [zℓ]ℓ
⊢[w1]i1 . . . [wk−1]ik−1 [wk]1 [zℓ]ℓ[zℓ−1]ℓ−1
...
...
⊢[w1]i1 . . . [wk−1]ik−1 [wk]1 [zℓ]ℓ[zℓ−1]ℓ−1 . . . [z2]2
⊢[w3]i3 . . . [wk−1]ik−1 [wk]1 [zℓ]ℓ[zℓ−1]ℓ−1 . . . [z2]2 [z1]1 .
⊓⊔
5
Simulating a Reverse Tag System with a Recurrence
While it’s possible to describe how the recurrence A simulates a reverse tag
system, a better approach is to introduce another, simpler recurrence B which
does this simulation, then show how A reduces to B. The simpler recurrence,
without initial conditions, is:

An Undecidable Nested Recurrence Relation
113
B (n) =

B (n −2) + 2B (B (n −1)) ,
if n is even
B (2B (n −2 −B (n −1)) + B (n −2)) ,
if n is odd.
Consider a reverse tag system R = (Σ, δ, d, W0). The simulation of R by B
necessitates encoding δ and d as initial conditions of B. In order to do this,
every symbol in Σ and every possible pair in Σ2 = Σ × Σ must be represented
by a unique integer. Then, invoking B on such an integer would correspond to
evaluating δ or d, whatever the case may be. In order to avoid conﬂicts doing
this, any integer representation of symbols and symbol pairs α : Σ ∪Σ2 →N
must be injective.
Assuming Σ = {s1, s2, . . . , st}, one such injection is deﬁned as follows:
α (si) = 4i+1 + 2 = 22i+2 + 2, and
α (si, sj) = 2α (si) + α (sj) = 22i+3 + 22j+2 + 6.
The fact that α is injective can be seen by considering the binary representation
of such numbers. Each of the bitstrings of α (s1) , . . . , α (st) are clearly distinct
from one another, and the bitstring of α (si, sj) for any i, j ∈{1, 2, . . . , t} “inter-
leaves” the bitstrings of α (si) and α (sj). The constant 2 term in the deﬁnition
of α is important in the next section, when the A recurrence is considered.
The initial conditions of B are constructed so that the encoding of d occurs
on α (Σ), and the encoding of δ occurs on α

Σ2
. For i, j ∈{1, 2, . . . , t}, the
encoding for d and δ is done respectively as follows:
B (α (si)) = 1 −d (si)
(5)
B (α (si, sj)) = α (δ (si, sj)) .
Is it worth noting that because of (5), B(n) can take on negative values.
The largest value attained by α is
α (st, st) = 3α (st) = 3 · 4t+1 + 6.
Let c0 = α (st, st) + 2. For the remainder of initial conditions that appear be-
fore c0 and don’t represent a symbol or symbol pair under α, B is assigned
zero. One observes that even though the number of initial conditions speciﬁed
is exponential in the size of Σ, only a polynomial number of these are actually
nonzero.
The way the B recurrence simulates R is that R’s computation string, as
represented under α, is recorded on the odd integers, while the length of the
running word is recorded on the even integers. Thus, for large enough n, the
pair (B (2n + 1) , B (2n + 2)) represents exactly one step of R’s computation.
The simulation begins with the initial word W0 = q1q2 . . . qm. Speciﬁcally, the m
integers α (q1) , . . . , α (qm) are placed on the ﬁrst m odd integers that come after

114
M. Celaya and F. Ruskey
Table 1. Illustration of initial conditions
B (c0 + k) α (q1) 0 α (q2) 0 . . . α (qm−1)
0
α (qm) 2m −2
k
1
2
3
4 . . . 2m −3 2m −2 2m −1
2m
c0. The value 2m −2 is then immediately placed after the last symbol of W0;
it is the last initial condition of B and signiﬁes the length of the initial word.
Beyond this point, the recurrence of B takes eﬀect. An illustration of these initial
conditions is given in Table 5.
We now formalize what is meant by “B simulates R.” As mentioned previously,
B will alternatingly output symbols and word lengths. We encode the symbols
and word lengths produced by B in the following manner: any symbol s ∈Σ
is encoded as the integer α (s), while the length k of some computed word is
recorded in the output of B as the value 2k −2.
Suppose that at the ith computation step of R, the word W = w1w2 . . . wk is
produced. We shall say that B computes the ith step of R at n if the following
equalities hold:
(B (n −2k + 1) , . . . , B (n −3) , B (n −1)) = (α (w1) , α (w2) , . . . , α (wk))
B (n) = 2k −2.
This terminology is justiﬁed, since if B computes the ith step of R at n, then
these equalities allow W to be reconstructed from the output of B near n. If
there exist constants r, s such that for all i ∈N, B computes the ith step of R
at ri + s whenever step i exists, then we shall say that B simulates R.
Theorem 2. With the above initial conditions, B simulates R = (Σ, δ, d, W0).
Proof. If we suppose that the 0th step of R yields the initial word W0, then by
Table 5 it is clear that B computes the 0th step of R at c0 + 2m.
Assume that B computes the ith step of R at 2n, where, again, we assume the
word produced at step i is w1w2 . . . wk. We would like to show that B computes
the (i + 1)th step of R at 2n + 2. Showing this, by induction, would prove the
theorem.
If y = δ (w1, wk), then the word produced by R at step i+1 is wd(y)+1 . . . wky.
The last symbol of this word is y and length of this word is k+1−d (y). Therefore,
to prove the theorem, we need only show that
B (2n + 1) = α (y)
B (2n + 2) = 2 (k + 1 −d (y)) −2
= 2 (k −d (y)) .

An Undecidable Nested Recurrence Relation
115
We ﬁrst consider the point 2n + 1. Since this point is odd, we have
B (2n + 1) = B (2B (2n −1 −B (2n)) + B (2n −1))
= B (2B (2n −1 −2 (k −1)) + B (2n −1 −2 (k −k)))
= B (2α (w1) + α (wk))
= B (α (w1, wk))
= α (δ (w1, wk))
= α (y) .
The point 2n + 2 is even, thus
B (2n + 2) = B (2n) + 2B (B (2n + 1))
= 2k −2 + 2B (α (y))
= 2k −2 + 2 (1 −d (y))
= 2 (k −d (y)) .
⊓⊔
The above theorem describes the behaviour of B when R does not halt. If R
halts at any point, then there exists some even n such that B (n) = −2. Then,
B (n + 1) = B (2B (n + 1) + B (n −1)), and so B is not calculable. Thus, B
with the prescribed initial conditions is calculable if and only if R does not halt.
6
Reducing A to B
It remains to show that the output of B is eﬀectively the same as the output of
the recurrence
A (n) =A (n −4 −A (A (n −4))) + 4A (A (n −2))
+ A (2A (n −4 −A (n −2)) + A (n −4)) ,
(6)
given the right initial conditions.
Once more, suppose we have a reverse tag system R = (Σ, δ, d, W0). One
restriction that will be made on R is that d (Σ) = {0, 2}. Section 3 demonstrated
how, despite this restriction, R can still simulate an ordinary tag system. The
goal at the beginning of these notes, to show that A is Turing complete, is
therefore still in reach.
Assume there are t symbols in Σ, and m symbols in the initial word W0. Let
c0 = α (st, st) + 2, as before. We now specify the initial conditions of A. For
n = 0, 1, . . ., c0, A and B will share the same initial conditions. Immediately
after, we’ll have, for 0 ≤n < m and 0 ≤j < 4,
A (c0 + 4n + j) =
⎧
⎪
⎨
⎪
⎩
0,
j = 0, 2
B (c0 + 2n + 1) ,
j = 1
2B (c0 + 2n + 2) ,
j = 3
(7)

116
M. Celaya and F. Ruskey
The next theorem, stated without proof for space reasons, demonstrates how to
obtain the sequence B from A. A proof of this theorem can be viewed online at
http://arxiv.org/abs/1203.0586.
Theorem 3. Using the given initial conditions for A and B, A is calculable if
and only if B is calculable. If B is calculable, then (7) holds for all n ≥0.
7
Concluding Remarks
In this paper, we have shown the existence of an undecidable nested recurrence
relation. Furthermore, like its more well known cousins (1), (2), (3) and (4), our
recurrence relation (6) is formed only from the operations of addition, subtrac-
tion, and recursion. Thus the result lends support to the idea that, in general,
it will be diﬃcult to prove broad results about nested recurrence relations. It
will be interesting to try to determine whether other nested recurrence relations,
such as (1), are decidable or not. If it is undecidable then it will certainly involve
extending the techniques that are presented here, since the form of the recursion
seems to prevent lookups in the manner we used.
Acknowledgements. The research was supported in part by an NSERC Dis-
covery Grant. The authors would like to thank the anonymous referees for their
suggestions and for providing reference [4].
References
1. Allouche, J.P., Shallit, J.: A variant of Hofstadter’s sequence and ﬁnite automata.
arXiv:1103.1133v2 (2011)
2. Balamohan, B., Kuznetsov, A., Tanny, S.: On the behavior of a variant of Hofs-
tadter’s Q-sequence. J. Integer Sequences 10, 29 pages (2007)
3. Cocke, J., Minsky, M.: Universality of tag systems with p = 2. J. ACM 11(1), 15–20
(1964)
4. Conway, J.H.: Unpredictable iterations. In: Proceedings of the 1972 Number The-
ory Confernence, pp. 49–52 (August 1972)
5. Cook, M.: Universality in elementary cellular automata. Complex Systems 15(1),
1–40 (2004)
6. Downey, P.J., Griswold, R.E.: On a family of nested recurrences. Fibonacci Quar-
terly 22(4), 310–317 (1984)
7. Golomb, S.: Discrete chaos: sequences satisfying “strange" recursions (1991)
(preprint)
8. Hofstadter, D.R.: Gödel, Escher, Bach: An Eternal Golden Braid. Basic Books
(1979)
9. Kubo, T., Vakil, R.: On Conway’s recursive sequence. Discrete Mathematics 152(1),
225–252 (1996)

An Undecidable Nested Recurrence Relation
117
10. Minsky, M.L.: Recursive unsolvability of Post’s problem of "Tag" and other topics
in theory of Turing machines. The Annals of Mathematics 74(3), 437–455 (1961)
11. Post, E.L.: Formal reductions of the general combinatorial decision problem. Amer-
ican Journal of Mathematics 65(2), 197–215 (1943)
12. Ruskey, F.: Fibonacci meets Hofstadter. Fibonacci Quarterly 49(3), 227–230 (2011)
13. Smith, A.R.: Simple computation-universal cellular spaces and self-reproduction.
In: IEEE Conference Record of the 9th Annual Symposium on Switching and
Automata Theory 1968, pp. 269–277 (October 1968)

Hard Instances of Algorithms and Proof Systems
Yijia Chen1, J¨org Flum2, and Moritz M¨uller3
1 Department of Computer Science and Engineering, Shanghai Jiao Tong University,
Dongchuan Road, No. 800, 200240 Shanghai, China
yijia.chen@cs.sjtu.edu.cn
2 Abteilung f¨ur mathematische Logik, Albert-Ludwigs-Universit¨at Freiburg,
Eckerstraße 1, 79104 Freiburg, Germany
joerg.flum@math.uni-freiburg.de
3 Kurt G¨odel Research Center for Mathematical Logic,
W¨ahringer Straße 25, 1090 Wien, Austria
moritz.mueller@univie.ac.at
Abstract. If the class Taut of tautologies of propositional logic has
no almost optimal algorithm, then every algorithm A deciding Taut
has a polynomial time computable sequence witnessing that A is not
almost optimal. We show that this result extends to every Πp
t -complete
problem with t ≥1; however, assuming the Measure Hypothesis, there
is a problem which has no almost optimal algorithm but is decided by
an algorithm without such a hard sequence. Assuming that a problem
Q has an almost optimal algorithm, we analyze whether every algorithm
deciding Q, which is not almost optimal algorithm, has a hard sequence.
1
Introduction
Let A be an algorithm deciding a problem Q. A sequence (xs)s∈N of strings
in Q is hard for A if it is computable in polynomial time and the sequence
(tA(xs)s∈N) is not polynomially bounded in s.1 Here, tA(x) denotes the number
of steps the algorithm A takes on input x. Clearly, if A is polynomial time, then
A has no hard sequences. Furthermore, an almost optimal algorithm for Q has
no hard sequences either. Recall that an algorithm A is almost optimal for Q
if for any other algorithm B deciding Q and all x ∈Q the running time tA(x)
is polynomially bounded in tB(x). In fact, if (xs)s∈N is a hard sequence for an
algorithm, then one can superpolynomially speed up it on {xs | s ∈N}, so it
cannot be almost optimal.
Central to this paper is the question: To what extent can we show that algo-
rithms which are not almost optimal have hard sequences? Our starting point is
the following result (more or less explicit in [3,11]):
If Taut, the class of tautologies of propositional logic, has no almost op-
timal algorithm, then every algorithm deciding Taut has hard sequences.
First we generalize this result from the Πp
1-complete problem Taut to all prob-
lems which are Πp
t -complete for some t ≥1:
1 All notions will be deﬁned in a precise manner later.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 118–128, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Hard Instances of Algorithms and Proof Systems
119
(∗) If a Πp
t -complete problem Q has no almost optimal algorithm, then every
algorithm deciding Q has hard sequences.
Apparently there are some limitations when trying to show (∗) for all problems
Q as we prove:
(+) If the Measure Hypothesis holds, then there is a problem which has no almost
optimal algorithm but is decided by an algorithm without hard sequences.
Perhaps one would expect that one can strengthen (∗) and show that even if a
Πp
t -complete problem Q has an almost optimal algorithm, then every algorithm,
which is not almost optimal and decides Q, has a hard sequence. However, we
show:
If the Measure Hypothesis holds, then every problem with padding and
with an almost optimal algorithm is decided by an algorithm which is not
almost optimal but has no hard sequences.
As an algorithm deciding a problem Q which is not almost optimal can be
polynomially speeded up on an inﬁnite subset of Q, by (+) we see that, at
least under the Measure Hypothesis, this notion of speeding up (e.g., considered
in [13]) is weaker than our notion of the existence of a hard sequence.
Assume that Q := Taut (or any Πp
t -complete Q) has no almost optimal
algorithm; thus, by (∗), every algorithm deciding Q has a hard sequence. Can
we even eﬀectively assign to every algorithm deciding Q a hard sequence? We
believe that under reasonable complexity-theoretic assumptions one should be
able to show that such an eﬀective procedure or at least a polynomial time
procedure does not exist, but we were not able to show it. However, recall that
by a result due to Stockmeyer [13] and rediscovered by Messner [10] we know:
For every EXP-hard problem Q there is a polynomial time eﬀective pro-
cedure assigning to every algorithm solving Q a sequence hard for it.
Hence, if EXP = Πp
t , then for every Πp
t -hard problem Q there is a polynomial
time eﬀective procedure assigning a hard sequence to every algorithm deciding Q.
Our proof of (∗) generalizes to nondeterministic algorithms. This “nondeter-
ministic statement” yields a version for Πp
t -complete problems of a result that
Kraj´ı˘cek derived for non-optimal propositional proof systems: If Taut has no
optimal proof system, then for every propositional proof system P there is a
polynomial time computable sequence (αs)s∈N of propositional tautologies αs
which only have superpolynomial P-proofs; moreover, he showed that the αs can
be chosen with s ≤|αs|. While it is well-known that for any problem Q nonde-
terministic algorithms deciding Q and proof systems for Q are more or less the
same, the relationship between deterministic algorithms and propositional proof
systems is more subtle. Nevertheless, we are able to use (∗) to derive a statement
on hard sequences for Πp
t -complete problems Q without a polynomially optimal
proof system.
As a byproduct, we obtain results in “classical terms” (that is, not referring
to hard sequences). For example, we get for t ≥1:

120
Y. Chen, J. Flum, and M. M¨uller
Let Q be Πp
t -complete. Then, Q has an almost optimal algorithm if and
only if Q has a polynomially optimal proof system.
If some Πp
t -complete has no almost optimal algorithm, then every Πp
t -
hard problem has no almost optimal algorithm.
It is still open whether there exist problems outside of NP with optimal proof
systems. We show their existence (in NE) assuming the Measure Hypothesis.
Kraj´ı˘cek and Pudl´ak [7] proved that NE = coNE implies that Taut has an
optimal proof system, a result later strengthened by [8,1].
If for an algorithm A deciding a problem Q we have a hard sequence (xs)s∈N
satisfying s ≤|xs|, then {xs | s ∈N} is a hard set for A, that is, a polynomial
time decidable subset of Q on which A is not polynomial time. Messner [10]
has shown for any Q with padding that all algorithms deciding Q have hard
sets if and only if Q has no polynomially optimal proof system. We show for
arbitrary Q that the existence of hard sets for all algorithms is equivalent to the
existence of an eﬀective enumeration of all polynomial time decidable subsets of
Q, a property which has turned out to be useful in various contexts (cf. [12,3,4]).
We analyze what Messner’s result means for proof systems.
The content of the sections is the following. In Section 2 we recall some con-
cepts. We deal with hard sequences for algorithms in Section 3 and for proof
systems in Section 4. Section 5 is devoted to hard sets and Section 6 contains
the results and the examples of problems with special properties obtained as-
suming that the Measure Hypothesis holds. Finally Section 7 gives an eﬀective
procedure yielding hard sequences for nondeterministic algorithms for coNEXP-
hard problems. Due to space limitations we defer almost all proofs to the full
version of this extended abstract.
2
Preliminaries
By nO(1) we denote the class of polynomially bounded functions on the natural
numbers. We let Σ be the alphabet {0, 1} and |x| the length of a string x ∈Σ∗.
We identify problems with subsets of Σ∗. In this paper we always assume that
Q denotes a decidable and nonempty problem.
We assume familiarity with the classes P (polynomial time), NP (nondeter-
ministic polynomial time) and the classes Πp
t for t ≥1 (the “universal” class of
the tth level of the polynomial hierarchy). In particular, Πp
1 = coNP.
The Measure Hypothesis [5] is the assumption “NP does not have measure 0
in E.”‘ For the corresponding notion of measure we refer to [9]. This hypothesis
is sometimes used in the theory of resource bounded measures.
A problem Q ⊆Σ∗has padding if there is a function pad : Σ∗× Σ∗→Σ∗
computable in logarithmic space having the following properties:
– For any x, y ∈Σ∗, |pad(x, y)| > |x| + |y| and

pad(x, y) ∈Q ⇐⇒x ∈Q

.
– There is a logspace algorithm which, given pad(x, y) recovers y.

Hard Instances of Algorithms and Proof Systems
121
By ⟨. . . , . . .⟩we denote some standard logspace computable tupling function with
logspace computable inverses.
If A is a deterministic or nondeterministic algorithm and A accepts the string
x, then we denote by tA(x) the minimum number of steps of an accepting run
of A on x; if A does not accept x, then tA(x) is not deﬁned. By L(A) we denote
the language accepted by A. We use deterministic and nondeterministic Turing
machines with Σ as alphabet as our basic computational model for algorithms
(and we often use the notions “algorithm” and “Turing machine” synonymously).
If necessary we shall not distinguish between a Turing machine and its code, a
string in Σ∗. By default, algorithms are deterministic. If an algorithm A on input
x eventually halts and outputs a value, we denote it by A(x).
3
Hard Sequences for Algorithms
In this section we derive the results concerning the existence of hard sequences
for Πp
t -complete problems.
Let Q ⊆Σ∗. A deterministic (nondeterministic) algorithm A deciding (accept-
ing) Q is almost optimal if for every deterministic (nondeterministic) algorithm
B deciding (accepting) Q we have
tA(x) ≤

tB(x) + |x|
O(1)
for all x ∈Q. Note that nothing is required for x /∈Q.
Clearly, every problem in P (NP) has an almost optimal (nondeterminis-
tic) algorithm. There are problems outside P with an almost optimal algorithm
(see Messner[10, Corollary 3.33]; we slightly improve his result in Theorem 22
of Section 6). However, it is not known whether there are problems outside
NP having an almost optimal nondeterministic algorithm and it is not known
whether there are problems with padding outside P having an almost optimal
algorithm. We show in Theorem 23 of Section 6 that the former is true if the
Measure Hypothesis holds.
Deﬁnition 1. Let Q ⊆Σ∗.
(1) Let A be a deterministic (nondeterministic) algorithm deciding (accepting)
Q. A sequence (xs)s∈N is hard for A if {xs | s ∈N} ⊆Q, the function 1s →xs
is computable in polynomial time, and tA(xs) is not polynomially bounded
in s.
(2) The problem Q has hard sequences for algorithms (for nondeterministic al-
gorithms) if every (nondeterministic) algorithm deciding Q has a hard se-
quence.
In the proof of the following lemma we show that an algorithm A can be super-
polynomially speeded up on {xs | s ∈N} if (xs)s∈N is hard for A.
Lemma 2. Let A be a deterministic (nondeterministic) algorithm deciding (ac-
cepting) Q. If A has a hard sequence, then A is not almost optimal.

122
Y. Chen, J. Flum, and M. M¨uller
As already remarked in the Introduction, part (b) of the next theorem, the main
result of this section, generalizes the corresponding result for Q = TAUT due to
Kraj´ı˘cek.
Theorem 3. Let Q be a Πp
t -complete problem for some t ≥1. Then:
(a) Q has no almost optimal algorithm
⇐⇒
Q has hard sequences for algo-
rithms.
(b) Q has no almost optimal nondeterministic algorithm
⇐⇒
Q has hard
sequences for nondeterministic algorithms.
Remark 4. For Q = Taut, part (a) is implicit in [3,11]. In fact, there it is shown
that a halting problem polynomially isomorphic to Taut has hard sequences for
algorithms if it has no almost optimal algorithm. In Remark 14 we show how this
can be extended to every coNP-complete problem using known results relating
almost optimal algorithms and proof systems.
Lemma 2 yields the implications from right to left in Theorem 3. The following
considerations will yield a proof of the converse direction. For a nondeterministic
algorithm A and s ∈N let As be the algorithm that rejects all x ∈Σ∗with
|x| > s. If |x| ≤s, then it simulates s steps of A on input x; if this simulation
halts and accepts, then As accepts; otherwise it rejects.
For Q ⊆Σ∗we consider the deterministic (nondeterministic) algorithm subset
problem Das(Q) (Nas(Q))
Das(Q) (Nas(Q))
Instance: A (nondeterministic) algorithm A and 1s with
s ∈N.
Question: L(As) ⊆Q ?
The following two lemmas relate the equivalent statements in Theorem 3 (a)
(in Theorem 3 (b)) to a statement concerning the complexity of Das(Q) (of
Nas(Q)).
Lemma 5. (a) If ⟨A, 1s⟩∈Das(Q) is solvable in time sf(A) for some function
f, then Q has an almost optimal algorithm.
(b) If there is a nondeterministic algorithm V accepting Nas(Q) such that for
all ⟨A, 1s⟩∈Nas(Q) we have tV(⟨A, 1s⟩) ≤sf(A) for some function f, then
Q has an almost optimal nondeterministic algorithm.
If Q is Πp
t -complete, then Nas(Q) and hence Das(Q) are in Πp
t , too (this is the
reason why 1s and not just s is part of the input of Nas(Q) and of Das(Q)).
Thus, together with Lemma 5 the following lemma yields the remaining claims
of Theorem 3.
Lemma 6. (a) Assume that Das(Q) ≤p Q, that is, that Das(Q) is polynomial
time reducible to Q. If ⟨A, 1s⟩∈Das(Q) is not solvable in time sf(A) for
some function f, then Q has hard sequences for algorithms.

Hard Instances of Algorithms and Proof Systems
123
(b) Assume that Nas(Q) ≤p Q. If there is no nondeterministic algorithm V
accepting Nas(Q) such that for all ⟨A, 1s⟩∈Nas(Q) we have tV(⟨A, 1s⟩) ≤
sf(A) for some function f, then Q has hard sequences for nondeterministic
algorithms.
Remark 7. In the proof of Theorem 3 we use the assumption that Q is Πp
t -
complete only to ensure that Nas(Q) ≤p Q (cf. Lemma 6). This condition is also
fulﬁlled for every Q complete, say, in one of the classes E or Pspace. Thus the
statements of Theorem 3 hold for such a Q.
Remark 8. Assume that Q is Πp
t -complete and has padding (for t = 1, the set
Taut is an example of such a Q). If Q has no almost optimal algorithm, then
every algorithm B deciding Q has a hard sequence (xs)s∈N with s ≤|xs|. Then,
in particular
{xs | s ∈N} ∈P
and
B is not polynomial time on {xs | s ∈N}.
In fact, it is well-known that for Q with padding we can replace any polynomial
time reduction to Q by a length-increasing one. An analysis of the proof of
Lemma 6 shows that then we can get hard sequences (xs)s∈N with s ≤|xs|.
In contrast to the last remark, for the validity of the next lemma it is important
that we do not require s ≤|xs| in our deﬁnition of hard sequence.
Lemma 9. Assume that S is a polynomial time reduction from Q to Q′ and let
B be a (nondeterministic) algorithm deciding (accepting) Q′. If (xs)s∈N is a hard
sequence for B ◦S, then (S(xs))s∈N is a hard sequence for B.
Therefore, if Q ≤p Q′ and Q has hard sequences for (nondeterministic) al-
gorithms then so does Q′.
We derive two consequences of our results:
Corollary 10. Assume t ≥1 and let Q and Q′ be Πp
t -complete. Then, Q has
an almost optimal algorithm if and only if Q′ has an almost optimal algorithm.
Corollary 11. Let t ≥1 and assume that the some Πp
t -complete problem has no
almost optimal algorithm. Then every Πp
t -hard problem has no almost optimal
algorithm.
4
Hard Sequences for Proof Systems
In this section we translate the results on hard sequences from algorithms to
proof systems. We ﬁrst recall some basic deﬁnitions.
A proof system for Q is a polynomial time algorithm P computing a function
from Σ∗onto Q. If P(w) = x, we say that w is a P-proof of x.
Let P and P′ be proof systems for Q. An algorithm T is a translation from P′
into P if P(T(w′)) = P′(w′) for every w′ ∈Σ∗. Note that translations always exist.
A translation is polynomial if it runs in polynomial time.

124
Y. Chen, J. Flum, and M. M¨uller
A proof system P for Q is p-optimal or polynomially optimal if for every proof
system P′ for Q there is a polynomial translation from P′ into P. A proof system
P for Q is optimal if for every proof system P′ for Q and all w′ ∈Σ∗there is a
w ∈Σ∗such that P(w) = P′(w′) and |w| ≤|w′|O(1). Clearly, any p-optimal proof
system is optimal.
We often will make use of the following relationship between the optimality
notions for algorithms and that for proof systems (see [7,10]).
Theorem 12. (1) For every Q we have (a) ⇒(b) and (b) ⇒(c); moreover (a),
(b), and (c) are all equivalent if Q has padding. Here
(a) Q has a p-optimal proof system.
(b) Q has an almost optimal algorithm.
(c) There is an algorithm that decides Q and runs in polynomial time on
every subset X of Q with X ∈P.
(2) For every Q we have (a) ⇐⇒(b),
(b) ⇒(c), and (c) ⇒(d); moreover
(a)–(d) are all equivalent if Q has padding. Here
(a) Q has an optimal proof system.
(b) Q has an almost optimal nondeterministic algorithm.
(c) There is a nondeterministic algorithm that accepts Q and runs in poly-
nomial time on every subset X of Q with X ∈NP.
(d) There is a nondeterministic algorithm that accepts Q and runs in poly-
nomial time on every subset X of Q with X ∈P.
We use our results of Section 3 to extend the equivalence between (a) and (b)
of part (1) of Theorem 12 to arbitrary Πp
t -complete problems:
Theorem 13. Let Q be a Πp
t -complete problem for some t ≥1. Then:
Q has a p-optimal proof system ⇐⇒Q has an almost optimal algorithm.
Remark 14. Using Theorem 12, for every coNP-complete Q we get a simple,
direct proof of
if Q has no almost optimal algorithm, then Q has hard sequences for
algorithms
using the result for Q = Taut (that we already knew by Remark 4). In fact,
assume that Q has no almost optimal algorithm. Then Taut has no almost opti-
mal algorithm; otherwise, Taut has a p-optimal proof system by the equivalence
of (a) and (b) in part (1) of Theorem 12 (Taut has padding!). As Q ≤p Taut,
then Q has a p-optimal proof system too (cf. [8, Lemma 1]) and hence, again
by Theorem 12, an almost optimal algorithm, a contradiction. Thus, Taut has
hard sequences for algorithms. As Taut ≤p Q, by Lemma 9 the problem Q has
hard sequences for algorithms, too.
We already mentioned that for every Q ⊆Σ∗there is a well-known and straight-
forward correspondence between proof systems and nondeterministic algorithms
preserving the optimality notions, so that the proof of the equivalence between
(a) and (b) in Theorem 12 (2) is immediate. Thus the translation of our results

Hard Instances of Algorithms and Proof Systems
125
for nondeterministic algorithms to proof systems is easy and we omit it here.
Moreover, the corresponding results are due to Kraj´ı˘cek [6] who proved them by
quite diﬀerent means.
Deﬁnition 15. (1) Let P be a proof systems for Q. A sequence (xs)s∈N is hard
for P if {xs | s ∈N} ⊆Q, the function 1s →xs is computable in polynomial
time, and there is no polynomial time algorithm W with P(W(1s)) = xs for
all s ∈N.
(2) The problem Q has hard sequences for proof systems if every proof system
for Q has a hard sequence.
For Q = Taut the following result is already known (cf., e.g., the survey [2,
Section 11]). We give a new proof that works for any, not necessarily paddable
Πp
t -complete problem Q.
Theorem 16. Let Q be a Πp
t -complete problem for some t ≥1. Then:
Q has no p-optimal proof system iﬀQ has hard sequences for proof systems.
Again an analysis of the proof of this theorem shows that for Q with padding,
we can require that the claimed hard sequence (xs)s∈N satisﬁes s ≤|xs|.
5
Hard Subsets
If for an algorithm A deciding a problem Q we have a hard sequence (xs)s∈N
satisfying s ≤|xs|, then {xs | s ∈N} is a polynomial time decidable subset of
Q on which A is not polynomial time. We then speak of a hard set for A even if
its elements cannot be generated in polynomial time. More precisely:
Deﬁnition 17. Let Q ⊆Σ∗.
(1) Let A be a deterministic or nondeterministic algorithm accepting Q. A subset
X of Q is hard for A if X ∈P and A is not polynomial time on X.
(2) The problem Q has hard sets for (nondeterministic) algorithms if every (non-
deterministic) algorithm deciding Q has a hard set.
Using these notions the equivalences (a) ⇔(c) and (a) ⇔(d) in Theorem 12 (1)
and (2), respectively, can be expressed in the following way:
Assume that Q has padding. Then
(1) Q has no almost optimal algorithm ⇐⇒Q has hard sets for algo-
rithms.
(2) Q has no almost optimal nondeterministic algorithm
⇐⇒
Q has
hard sets for nondeterministic algorithms.
Hence, we get (we leave the nondeterministic variant to the reader):

126
Y. Chen, J. Flum, and M. M¨uller
Corollary 18. Assume Q has padding.
(a) If Q has hard sequences for algorithms, then Q has hard sets for algorithms.
(b) If in addition Q is Πp
t -complete, then Q has hard sequences for algorithms if
and only if Q has hard sets for algorithms.
Assume that Q has an almost optimal algorithm. Then, in general, one cannot
show that every algorithm deciding Q, which is not almost optimal, has a hard
set. In fact, Messner [10, Corollary 3.33] has presented a P-immune Q0 with an
almost optimal algorithm. Of course, no algorithm deciding Q0 has a hard set.
For an arbitrary problem Q the existence of hard subsets is equivalent to a
(non-)listing property. We introduce this property.
Let C be the complexity class P or NP. A set X is a C-subset of Q if X ⊆Q
and X ∈C. We write List(C, Q) and say that there is a listing of the C-subsets
of Q by C-machines if there is an algorithm that, once having been started, lists
Turing machines M1, M2, . . . of type C such that {L(Mi) | i ≥1} = {X ⊆Q |
X ∈C}.
For Q with padding the equivalences in the following proposition were
known [12].
Theorem 19. (1) Q has hard sets for algorithms ⇐⇒not List(P, Q).
(2) Every nondeterministic algorithm A accepting Q is not polynomial on at
least one subset X of Q with X ∈NP ⇐⇒not List(NP, Q).
We close this section by introducing hard subsets for proof systems and stating
the corresponding result.
Deﬁnition 20. (1) Let P be a proof system for Q. A subset X of Q is hard
for P if X ∈P and there is no polynomial time algorithm W such that
P(W(x)) = x for all x ∈X.
(2) Q has hard sets for proof systems if every proof system for Q has a hard set.
The following result can be obtained along the lines of the proof of Theorem 16.
Theorem 21. Let Q be a problem with padding. Then:
Q has no p-optimal proof system if and only if Q has hard sets for proof systems.
6
Assuming the Measure Hypothesis
In this section we present some examples of problems with special properties,
some yield limitations to possible extensions of results mentioned in this paper.
Most are proven assuming the Measure Hypothesis.
Recall that an algorithm A deciding Q is optimal if for every algorithm B
deciding Q we have
tA(x) ≤(tB(x) + |x|)O(1)
for all x ∈Σ∗. Clearly, every problem in P has an optimal algorithm.

Hard Instances of Algorithms and Proof Systems
127
Theorem 22. (1) There exist problems in E \ P with optimal algorithms.
(2) If the Measure Hypothesis holds, then there exist problems in NP \ P with
optimal algorithms.
Here, E := 
d∈N Dtime(2d·n). Messner [10] showed the existence of problems
in E \ P with almost optimal algorithms. The question whether there are sets
outside of NP with optimal proof systems was stated by Kraj´ı˘cek and Pudl´ak [7]
and is still open. As already mentioned, they proved that Taut has an optimal
proof system if E = NE (:= 
d∈N Ntime(2d·n)). We are able to show:
Theorem 23. If the Measure Hypothesis holds, then there exist problems in
NE \ NP with optimal proof systems (or, equivalently, with almost optimal non-
deterministic algorithms).
Concerning algorithms which are not almost optimal but do not have hard se-
quences we derive the following results.
Theorem 24. Let Q be a problem with padding and with an almost optimal al-
gorithm. If the Measure Hypothesis holds, then there is an algorithm deciding Q,
which is not almost optimal and has hard sets but does not have hard sequences.
The following example shows that the padding hypothesis is necessary in
Theorem 24.
Example. Let Q := {1n | n ∈N}. As Q ∈P, it has an almost optimal algorithm.
However, the set Q itself is a hard set and (1s)s∈N a hard sequence for every
non-optimal (that is, for every superpolynomial) algorithm deciding Q.
Corollary 25. If the Measure Hypothesis holds, then the following are equiva-
lent for t ≥1:
(i) No Πp
t -complete problem has an almost optimal algorithm.
(ii) Every non-almost optimal algorithm deciding a Πp
t -complete problem has
hard sequences.
Theorem 26. If the Measure Hypothesis holds, there is a problem which has
hard sets for algorithms (and hence has no almost optimal algorithm) but has
algorithms without hard sequences.
7
Getting Hard Sequences in an Eﬀective Way
We have mentioned in the Introduction that Stockmeyer [13] has shown that
for every EXP-hard problem Q there is a polynomial time procedure assigning
to every algorithm deciding Q a hard sequence. Based on his proof we derive a
“nondeterministic” version.
Theorem 27. Let Q be a coNEXP-hard problem. Then there is a polynomial
time computable function g : Σ∗× {1}∗→Σ∗such that for every nondetermin-
istic algorithm A accepting Q the sequence

g(A, 1s)

s∈N is hard for A.

128
Y. Chen, J. Flum, and M. M¨uller
Acknowledgments. The authors thank the John Templeton Foundation for its
support through Grant #13152. Yijia Chen is aﬃliated with BASICS and MOE-
MS Key Laboratory for Intelligent Computing and Intelligent Systems which is
supported by National Nature Science Foundation of China (61033002). Moritz
M¨uller thanks the FWF (Austrian Research Fund) for its support through Grant
number P 23989 - N13.
References
1. Ben-David, S., Gringauze, A.: On the existence of optimal propositional proof
systems and oracle-relativized propositional logic. Electronic Colloquium on Com-
putational Complexity (ECCC), Technical Report TR98-021 (1998)
2. Beyersdorﬀ, O.: On the correspondence between arithmetic theories and propo-
sitional proof systems - a survey. Mathematical Logic Quarterly 55(2), 116–137
(2009)
3. Chen, Y., Flum, J.: On p-Optimal Proof Systems and Logics for PTIME. In:
Abramsky, S., Gavoille, C., Kirchner, C., Meyer auf der Heide, F., Spirakis, P.G.
(eds.) ICALP 2010, Part II. LNCS, vol. 6199, pp. 321–332. Springer, Heidelberg
(2010)
4. Chen, Y., Flum, J.: Listings and logics. In: Proceedings of the 26th Annual IEEE
Symposium on Logic in Computer Science (LICS 2011), pp. 165–174. IEEE Com-
puter Society (2011)
5. Hitchcock, J.M., Pavan, A.: Hardness hypotheses, derandomization, and circuit
complexity. In: Lodaya, K., Mahajan, M. (eds.) FSTTCS 2004. LNCS, vol. 3328,
pp. 336–347. Springer, Heidelberg (2004)
6. Kraj´ı˘cek, J.: Bounded arithmetic, propositional logic, and complexity theory. Cam-
bridge University Press (1995)
7. Kraj´ı˘cek, J., Pudl´ak, P.: Propositional proof systems, the consistency of ﬁrst order
theories and the complexity of computations. The Journal of Symbolic Logic 54,
1063–1088 (1989)
8. K¨obler, J., Messner, J.: Complete problems for promise classes by optimal proof
systems for test sets. In: Proceedings of the 13th IEEE Conference on Computa-
tional Complexity (CCC 1998), pp. 132–140 (1998)
9. Mayordomo, E.: Almost every set in exponential time is P-bi-immune. Theoretical
Computer Science 136(2), 487–506 (1994)
10. Messner, J.: On the Simulation Order of Proof Systems. PhD Thesis, Univ. Erlan-
gen (2000)
11. Monroe, H.: Speedup for natural problems and noncomputability. Theoretical Com-
puter Science 412(4-5), 478–481 (2011)
12. Sadowski, Z.: On an optimal propositional proof system and the structure of easy
subsets of TAUT. Theoretical Computer Science 288(1), 181–193 (2002)
13. Stockmeyer, L.: The Complexity of Decision Problems in Automata Theory. PhD.
Thesis, MIT (1974)

On Mathias Generic Sets
Peter A. Cholak1, Damir D. Dzhafarov1, and Jeﬀry L. Hirst2
1 Department of Mathematics, 255 Hurley Building, University of Notre Dame,
Notre Dame, IN 46556-4618, United States of America
cholak@nd.edu, ddzhafar@nd.edu
2 Department of Mathematical Sciences, 322 Walker Hall, Appalachian State
University, Boone, NC 28608-2091, United States of America
jlh@math.appstate.edu
Abstract. We present some results about generics for computable
Mathias forcing. The n-generics and weak n-generics in this setting form
a strict hierarchy as in the case of Cohen forcing. We analyze the com-
plexity of the Mathias forcing relation, and show that if G is any n-generic
with n ≥3 then it satisﬁes the jump property G(n−1) = G′ ⊕∅(n). We
prove that every such G has generalized high degree, and so cannot have
even Cohen 1-generic degree. On the other hand, we show that G, to-
gether with any bi-immune A ≤T ∅(n−1), computes a Cohen n-generic.
1
Introduction
Forcing has been a central technique in computability theory since it was intro-
duced (in the form we now call Cohen forcing) by Kleene and Post to exhibit a
degree strictly between 0 and 0′. The study of the algorithmic properties of Co-
hen generic sets, and of the structure of their degrees, has long been a rich source
of problems and results. In the present paper, we propose to undertake a similar
investigation of generic sets for (computable) Mathias forcing, and present some
of our initial results in this direction.
Mathias forcing was perhaps ﬁrst used in computability theory by Soare in
[11] to build an inﬁnite set with no subset of strictly higher degree. Subsequently,
it became a prominent tool for constructing inﬁnite homogeneous sets for com-
putable colorings of pairs of integers, as in Seetapun and Slaman [9], Cholak,
Jockusch, and Slaman [2], and Dzhafarov and Jockusch [4]. It has also found
applications in algorithmic randomness, in Binns, Kjos-Hanssen, Lerman, and
Solomon [1].
We show below that a number of results for Cohen generics hold also for Math-
ias generics, and that a number of others do not. The main point of distinction
is that neither the set of conditions, nor the forcing relation is computable, so
many usual techniques do not carry over. We begin with background in Section
2, and present some preliminary results in Section 3. In Section 4 we characterize
the complexity of the forcing relation, and in Section 5 we prove a number of
results about the degrees of Mathias generic sets, and about their relationship
to Cohen generic degrees. We indicate questions along the way we hope will be
addressed in future work.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 129–138, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

130
P.A. Cholak, D.D. Dzhafarov, and J.L. Hirst
2
Deﬁnitions
We assume familiarity with the terminology particular to Cohen forcing in com-
putability theory. (For background on computability theory, see [10]. For back-
ground on Cohen generic sets, see Section 1.24 of [3].) The deﬁnition of the
Mathias forcing partial order is standard, but its formalization in the setting
of computability theory requires some care. A slightly diﬀerent presentation is
given in [1, Section 6], over which ours has the beneﬁt of reducing the complexity
of the set of conditions from Σ0
3 to Π0
2.
Deﬁnition 1
1. A (computable Mathias) pre-condition is a pair (D, E) where D is a ﬁnite
set, E is a computable set, and max D < min E.
2. A (computable Mathias) condition is a pre-condition (D, E), such that E is
inﬁnite.
3. A pre-condition (D∗, E∗) extends a pre-condition (D, E), written (D∗, E∗) ≤
(D, E), if D ⊆D∗⊆D ∪E and E∗⊆E.
4. A set A satisﬁes a pre-condition (D, E) if D ⊆A ⊆D ∪E.
By an index for a pre-condition (D, E) we shall mean a pair (d, e) such that d is
the canonical index of D and E = {x : Φe(x) ↓= 1}. By adopting the convention
that for all x, if Φe(x) ↓then Φe(y) ↓∈{0, 1} for all y ≤x, it follows that Φe is
total if E is inﬁnite, i.e., if (D, E) is a condition. Of course, if E is ﬁnite then
Φe may only be deﬁned on a proper initial segment of ω.
The deﬁnition makes the set of all indices Π0
1. However, we can pass to a
computable subset containing an index for every pre-condition. Namely, deﬁne
a strictly increasing computable function g by
Φg(d,e)(x) =

0
if x ≤max Dd,
Φe(x)
otherwise.
Then the set of pairs of the form (d, g(d, e)) is computable, and each is an index
for a pre-condition. Moreover, if (d, e) is an index as well, then it and (d, g(d, e))
index the same pre-condition. Formally, all references to pre-conditions in the
sequel will be to indices from this set, and we shall treat D and E as numbers
when convenient.
Note that whether one pre-condition extends another is a Π0
2 question. By our
convention about partial computable functions, the same question for conditions
is seen to be Π0
1.
In what follows, a Σ0
n set of conditions refers to a Σ0
n-deﬁnable set of pre-
conditions, each of which is a condition. (Note that this is not the same as the set
of all conditions satisfying a given Σ0
n deﬁnition, as discussed further in the next
section.) We call such a set dense if it contains an extension of every condition,
and deﬁne what it means to meet or avoid such a set as usual.

On Mathias Generic Sets
131
Deﬁnition 2. Fix n ∈ω.
1. A set G is Mathias n-generic if it meets or avoids every Σ0
n set of conditions.
2. A set G is weakly Mathias n-generic if it meets every dense such set.
We call a degree generic if it contains a set that is n-generic for all n.
It is easy to see that for every n ≥2, there exists a Mathias n-generic G ≤T
∅(n) (indeed, even G′ ≤T ∅(n)). This is done just as in Cohen forcing (see [6,
Lemma 2.6]), but as there is no computable listing of Σ0
n sets of conditions, one
goes through the Σ0
n sets of pre-conditions and checks which of these consist of
conditions alone. We pass to some other basic properties of generics. We refer
to Mathias n-generics below simply as n-generics when no confusion is possible.
3
Basic Results
Note that the set of all conditions is Π0
2. Thus, the set of conditions satisfying
a given Σ0
n deﬁnition is Σ0
n if n ≥3, and Σ0
3 otherwise. For n < 3, we may thus
wish to consider the following stronger form of genericity, which has no analogue
in the case of Cohen forcing.
Deﬁnition 3. A set G is strongly n-generic if, for every Σ0
n-deﬁnable set of
pre-conditions P, either G satisﬁes some condition in P or G meets the set of
conditions not extended by any condition in P.
Proposition 1. For n ≥3, a set is strongly n-generic if and only if it is n-
generic. For n ≤2, a set is strongly n-generic if and only if it is 3-generic.
Proof. Evidently, every strongly n-generic set is n-generic. Now suppose P is a
Σ0
n set of pre-conditions, and let C consist of all the conditions in P. An inﬁnite
set meets or avoids P if and only if it meets or avoids C, so every max{n, 3}-
generic set meets or avoids P. For n ≥3, this means that every n-generic set is
strongly n-generic, and for n ≤2 that every 3-generic set is strongly n-generic.
It remains to show that every strongly 0-generic set is 3-generic. Let C be
a given Σ0
3 set of conditions, and let R be a computable relation such that
(D, E) belongs to C if and only if (∃a)(∀x)(∃y)R(D, E, a, x, y). Deﬁne a strictly
increasing computable function g by
Φg(D,E,a)(x) =

ΦE(x)
if (∃y)R(D, E, a, x, y) and ΦE(x) ↓,
↑
otherwise,
and let P be the computable set of all pre-conditions of the form (D, g(D, E, a)).
If (D, E) ∈C then ΦE is total and so there is an a such that Φg(D,E,a) = ΦE. If,
on the other hand, (D, E) is a pre-condition not in C then for each a there is an
x such that Φg(D,E,a)(x) ↑. Thus, the members of C are precisely the conditions
in P, so an inﬁnite set meets or avoids C if and only if it meets or avoids P. In
particular, every strongly 0-generic set meets or avoids C.

132
P.A. Cholak, D.D. Dzhafarov, and J.L. Hirst
As a consequence, we shall restrict ourselves to 3-genericity or higher from now
on, or at most weak 2-genericity. Without further qualiﬁcation, n below will
always be a number ≥3.
Proposition 2. Every n-generic set is weakly n-generic, and every weakly n-
generic set is (n −1)-generic.
Proof. The ﬁrst implication is clear. For the second, let a Σ0
n−1 set C of conditions
be given. Let D be the class of all conditions that are either in C or else have no
extension in C, which is clearly dense. If n ≥4, then D is easily seen to be Σ0
n
(actually Π0
n−1) as saying a condition (D, E) has no extension in C is written
∀(D∗, E∗)[[(D∗, E∗) is a condition ∧(D∗, E∗) ≤(D, E)] =⇒(D∗, E∗) /∈C].
If n = 3, this makes D appear to be Σ0
4 but since C is a set of conditions only,
we can re-write the antecedent of the above implication as
D ⊆D∗⊂D ∪E ∧(∀x)[ΦE∗(x) ↓= 1 ∧ΦE(x) ↓=⇒ΦE(x) = 1]
to obtain an equivalent Σ0
3 deﬁnition. In either case, then, a weakly n-generic
set must meet D, and hence must either meet or avoid C.
The proof of the following proposition is straightforward. (The ﬁrst half is proved
much like its analogue in the Cohen case. See, e.g., [8, Corollary 2.7].)
Proposition 3. Every weakly n-generic set G is hyperimmune relative to ∅(n−1).
If G is n-generic, then its degree forms a minimal pair with 0(n−1).
Corollary 1. Not every n-generic set is weakly (n + 1)-generic.
Proof. Take any n-generic G ≤T ∅(n). Then G is not hyperimmune relative to
∅(n+1), and so cannot be weakly (n + 1)-generic.
We shall separate weakly n-generic sets from n-generic sets in Section 5, thereby
obtaining a strictly increasing sequence of genericity notions
weakly 3-generic ⇐= 3-generic ⇐= weakly 4-generic ⇐= · · ·
as in the case of Cohen forcing. In many other respects, however, the two types
of genericity are very diﬀerent. For instance, as noted in [2, Section 4.1], every
Mathias generic G is cohesive, i.e., satisﬁes G ⊆∗W or G ⊆∗W for every
computably enumerable set W. In particular, if we write G = G0 ⊕G1 then one
of G0 or G1 is ﬁnite. This is false for Cohen generics, which, by an analogue of van
Lambalgen’s theorem due to Yu [12, Proposition 2.2], have relatively n-generic
halves. Thus, no Mathias generic can be even Cohen 1-generic.
Question 1. What form of van Lambalgen’s theorem holds for Mathias forcing?
Another basic fact is that every Mathias n-generic G is high, i.e., satisﬁes G′ ≥T
∅′′. (See [1, Corollary 6.7], or [2, Section 5.1] for a proof.) By contrast, it is a
well-known result of Jockusch [6, Lemma 2.6] that every Cohen n-generic set G
satisﬁes G(n) ≡T G ⊕∅(n). As no high G can satisfy G′′ ≤T G ⊕∅′′, it follows
that no Mathias generic can have even Cohen 2-generic degree. This does not
prevent a Mathias n-generic from having Cohen 1-generic degree, as there are
high 1-generic sets, but we show this does not happen either in Corollary 4.

On Mathias Generic Sets
133
4
The Forcing Relation
Much of the discrepancy between Mathias and Cohen genericity stems from the
fact that the complexity of forcing a formula, deﬁned below, does not agree with
the complexity of the formula. Our forcing language here is the typical one of
formal ﬁrst-order arithmetic plus a set variable, X, and the epsilon relation, ∈.
We regard every Σ0
0 (i.e., bounded quantiﬁer) formula ϕ with no free number
variables as being written in disjunctive normal form according to some ﬁxed
eﬀective procedure for doing so. Call a disjunct valid if the conjunction of all
the equalities and inequalities in it is true, which can be checked computably.
For each i (ranging over the number of valid disjuncts), let Pϕ,i be the set of all
n such that n ∈X is a conjunct of the ith valid disjunct, and Nϕ,i the set of all
n such that n /∈X is a conjunct of the ith valid disjunct. Canonical indices for
these sets can be determined uniformly eﬀectively from an index for ϕ.
Deﬁnition 4. Let (D, E) be a condition and let ϕ(X) be a formula with only the
set variable X free. If ϕ is Σ0
0, say (D, E) forces ϕ(G), written (D, E) ⊩ϕ(G),
if for some i, Pϕ,i ⊆D and Nϕ,i ⊆D ∪E. From here, extend the deﬁnition of
(D, E) ⊩ϕ(G) to arbitrary ϕ inductively according to the standard deﬁnition of
strong forcing (e.g., as in [3, p. 100, footnote 22, items (iii)–(v)]).
Remark 1. If ϕ(X) is Σ0
0 with only the set variable X free and A is a set then
ϕ(A) holds if and only if there is an i such that Pϕ,i ⊆A and Nϕ,i ⊆A. Hence,
(D, E) ⊩ϕ(G) if and only if ϕ(D ∪F) holds for all ﬁnite F ⊂E.
Lemma 1. Let (D, E) be a condition and let ϕ(X) be a formula in exactly one
free set variable.
1. If ϕ is Σ0
0 with no free number variables then the relation (D, E) ⊩ϕ(G) is
computable.
2. If ϕ is Π0
1, Σ0
1, or Σ0
2, then so is the relation (D, E) ⊩ϕ(G).
3. For n ≥2, if ϕ is Π0
n then the relation of (D, E) ⊩ϕ(G) is Π0
n+1.
4. For n ≥3, if ϕ is Σ0
n then the relation (D, E) ⊩ϕ(G) is Σ0
n+1.
Proof. We ﬁrst prove 1. If ϕ is as hypothesized and ϕ(D ∪F) does not hold
for some ﬁnite F ⊂E, then neither does ϕ(D ∪(F ∩(
i Pϕ,i ∪Nϕ,i))). So by
Remark 1, we have that (D, E) ⊩ϕ(G) if and only if ϕ(D ∪F) holds for all
ﬁnite F ⊂E ∩(
i Pϕ,i ∪Nϕ,i), which can be checked computably.
For 2, suppose that ϕ(X) ≡(∀x)θ(x, X), where θ is Σ0
0. We claim that (D, E)
forces ϕ(G) if and only if θ(a, D ∪F) holds for all a and all ﬁnite F ⊂E, which
makes the forcing relation Π0
1. The right to left implication is clear. For the
other, suppose there is an a and a ﬁnite F ⊂E such that θ(a, D ∪F) does not
hold. Writing θa(X) for the formula θ(a, X), let D∗= D ∪F and
E∗= {x ∈E : x > max D ∪F ∪

i
Pθa,i ∪Nθa,i},

134
P.A. Cholak, D.D. Dzhafarov, and J.L. Hirst
so that (D∗, E∗) is a condition extending (D, E). Then if (D∗∗, E∗∗) is any
extension of (D∗, E∗), we have that
D∗∗∩(

i
Pθa,i ∪Nθa,i)) = (D ∪F) ∩(

i
Pθa,i ∪Nθa,i)),
and so θ(a, D∗∗) cannot force θ(a, G). Thus (D, E) does not force ϕ(G). The rest
of 2 follows immediately, since forcing a formula that is Σ0
1 over another formula
is Σ0
1 over the complexity of forcing that formula.
We next prove 3 for n = 2. Suppose that ϕ(G) ≡(∀x)(∃y)θ(x, y, X) where
θ is Σ0
0. Our claim is that (D, E) ⊩ϕ(G) if and only if, for every a and every
condition (D∗, E∗) extending (D, E), there is a ﬁnite F ⊂E∗and a number
k > max F such that
(D∗∪F, {x ∈E∗: x > k}) ⊩(∃y)θ(a, y, G),
(1)
which is a Π0
3 deﬁnition. Since the condition on the left side of (1) extends
(D∗, E∗), this deﬁnition clearly implies forcing. For the opposite direction, sup-
pose (D, E) ⊩ϕ(G) and ﬁx any a and (D∗, E∗) ≤(D, E). Then by deﬁnition,
there is a b and a condition (D∗∗, E∗∗) extending (D∗, E∗) that forces θ(a, b, G).
Write θa,b(X) = θ(a, b, X), and let F ⊂E∗be such that D∗∗= D∗∪F. Since
θa,b(D∗∪F) holds, we must have Pθa,b,i ⊆D∗∪F and Nθa,b,i ∩(D∗∪F) = ∅
for some i. Thus, if we let k = max Nθa,b,i, we obtain (1).
To complete the proof, we prove 3 and 4 for n ≥3 by simultaneous induction
on n. Clearly, 3 for n −1 implies 4 for n, so we already have 4 for n = 3. Now
assume 4 for some n ≥3. The deﬁnition of forcing a Π0
n+1 statement is easily
seen to be Π0
2 over the relation of forcing a Σ0
n statement, and hence Π0
n+2 by
hypothesis. Thus, 3 holds for n + 1.
We shall see in Corollary 2 in the next section that the complexity bounds in
parts 3 and 4 of the lemma cannot be lowered to Σ0
n and Π0
n, respectively. As
a consequence, n-generics only decide all Σ0
n−1 formulas, and not necessarily all
Σ0
n formulas.
Proposition 4. Let G be n-generic, and for m ≤n let ϕ(X) be a Σ0
m or Π0
m
formula in exactly one free set variable. If (D, E) is any condition satisﬁed by
G that forces ϕ(G), then ϕ(G) holds.
Proof. If m = 0, then ϕ holds of any set satisfying (D, E), whether it is generic
or not. If m > 0 and the result holds for Π0
m−1 formulas, it also clearly holds
for Σ0
m formulas. Thus, we only need to show that if m > 0 and the result holds
for Σ0
m−1 formulas then it also holds for Π0
m formulas. To this end, suppose
ϕ(X) ≡(∀x)θ(x, X), where θ is Σ0
m−1. For each a, let Ca be the set of all
conditions forcing θ(a, X), which has complexity at most Σ0
n by Lemma 1. Hence,
G meets or avoids each Ca. But if G were to avoid some Ca, say via a condition
(D∗, E∗), then (D∗, E∗) would force ¬θ(a, G), and then (D, E) and (D∗, E∗)
would have a common extension forcing θ(a, G) and ¬θ(a, G). Thus, G meets
every Ca, so θ(a, G) holds for all a by hypothesis, meaning ϕ(G) holds.

On Mathias Generic Sets
135
Remark 2. It is not diﬃcult to see that if ϕ(G) is the negation of a Σ0
m formula
then any condition (D, E) forcing ϕ(G) forces an equivalent Π0
m formula. Thus,
if G is n-generic and satisﬁes such a condition, then ϕ(G) holds.
5
Degrees of Mathias Generics
We begin here with a jump property for Mathias generics similar to that of
Jockusch for Cohen generics. It follows that the degrees d satisfying d(n−1) =
d′ ∪0(n−1) yield a strict hierarchy of subclasses of the high degrees.
Theorem 1. For all n ≥2, if G is n-generic then G(n−1) ≡T G′ ⊕∅(n).
Proof. That G(n−1) ≥T G′ ⊕∅(n) follows from the fact that G is high, as dis-
cussed above. That G(n−1) ≤T G′⊕∅(n) is trivial for n = 2. To show it for n ≥3,
we wish to decide every Σ0,G
n−1 sentence using G′⊕∅(n). Let ϕ0(X), ϕ1(X), . . ., be
a computable enumeration of all Σ0
n−1 sentences in exactly one free set variable,
and for each i let Ci be the set of conditions forcing ϕi(G), and Di the set of
conditions forcing ¬ϕi(G). Then Di is the set of conditions with no extension in
Ci, so if G meets Ci it cannot also meet Di. On the other hand, if G avoids Ci
then it meets Di by deﬁnition. Now by Lemma 1, each Ci is Σ0
n since n ≥3, and
so it is met or avoided by G. Thus, for each i, either G meets Ci, in which case
ϕi(G) holds by Proposition 4, or else G meets Di, in which case ¬ϕi(G) holds
by Remark 2. To conclude the proof, we observe that G′ ⊕∅(n) can decide, uni-
formly in i, whether G meets Ci or Di. Indeed, from a given i, indices for Ci and
Di (as a Σ0
n set and a Π0
n set, respectively) can be found uniformly computably,
and then ∅(n) has only to produce these sets until a condition in one is found
that is satisﬁed by G, which can in turn be determined by G′.
Corollary 2. For every n ≥2 there is a Π0
n formula in exactly one free set
variable, the relation of forcing which is not Π0
n. For every n ≥3 there is a Σ0
n
formula in exactly one free set variable, the relation of forcing which is not Σ0
n.
Proof. It suﬃces to prove the second part, as it implies the ﬁrst by the proof of
Lemma 1. For consistency with Theorem 1, we ﬁx n ≥4 and prove the result for
n −1. If forcing every Σ0
n−1 formula were Σ0
n−1, then the proof of the theorem
could be carried out computably in G′ ⊕∅(n−1) instead of G′ ⊕∅(n). Hence, we
would have G(n−1) ≡T G′ ⊕∅(n−1), contradicting that G must be high.
The following result is the analogue of Theorem 2.3 of Kurtz [8] that every A >T
∅(n−1) hyperimmune relative to ∅(n−1) is Turing equivalent to the (n−1)st jump
of a weakly Cohen n-generic set. The proof, although mostly similar, requires
a few important modiﬁcations. The main problem is in coding A into G(n−2),
which, in the case of Cohen forcing, is done by appending long blocks of 1s to the
strings under construction. As the inﬁnite part of a Mathias condition can be
made very sparse, we cannot use the same idea here. We highlight the changes
below, and only sketch the rest of the details. Recall that a set is co-immune if
its complement has no inﬁnite computable subset.

136
P.A. Cholak, D.D. Dzhafarov, and J.L. Hirst
Proposition 5. If A >T ∅(n−1) is hyperimmune relative to ∅(n−1), then A ≡T
G(n−2) for some weakly n-generic set G.
Proof. Computably in A, we build a sequence (D0, E0) ≥(D1, E1) ≥· · · of
conditions, beginning with (D0, E0) = (∅, ω). Let C0, C1, . . . be a listing of all
Σ0
n sets of pre-conditions, and ﬁxing a ∅(n−1)-computable enumeration of each
Ci, let Ci,s be the set of all pre-conditions enumerated into Ci by stage pA(s). We
may assume that ⟨D, E⟩≤s for all (D, E) ∈Ci,s. Let B0, B1, . . . be a uniformly
∅(n−1)-computable sequence of pairwise disjoint co-immune sets. Say Ci requires
attention at stage s if there exists b ≤pA(s) in Bi ∩Es and a condition (D, E)
in Ci,s extending (Ds ∪{b}, {x ∈Es : x > b}).
At stage s, assume (Ds, Es) is given. If there is no i ≤s such that Ci requires
attention at stage s, set (Ds+1, Es+1) = (Ds, Es). Otherwise, ﬁx the least such
i. Choose the least corresponding b and earliest enumerated extension (D, E)
in Ci,s, and let (D∗, E∗) = (D, E). Then obtain (D∗∗, E∗∗) from (D∗, E∗) by
forcing the jump, in the usual manner. Finally, let k be the number of stages
t < s such that (Dt, Et) ̸= (Dt+1, Et+1), and let (D∗∗∗, E∗∗∗) = (D∗∗∪{b}, {x ∈
E∗∗: x > b}), where b is the least element of BA(k)∩E∗∗. If ⟨D∗∗∗, E∗∗∗⟩≤s+1,
set (Ds+1, Es+1) = (D∗∗∗, E∗∗∗), and otherwise set (Ds+1, Es+1) = (Ds, Es).
By deﬁnition, the Bi must intersect every computable set inﬁnitely often, and
so the entire construction is A-computable. That G = 
s Ds is weakly n-generic
can be veriﬁed much like in Kurtz’s proof, but using the ∅(n−1)-computable
function h where h(s) is the least t so that for each (D, E) with ⟨D, E⟩≤s
there exists b ≤t in Bi ∩E and (D∗, E∗) ∈Ci,t extending (D ∪{b}, {x ∈E :
x > b}). That G(n−2) ≤T A follows by Theorem 1 from G′ being forced during
the construction and thus being A-computable. Finally, to show A ≤T G(n−2),
let s0 < s1 < · · · be all the stages s > 0 such that (Ds−1, Es−1) ̸= (Ds, Es).
The sequence (Ds0, Es0) > (Ds1, Es1) · · · can be computed by G(n−2) as follows.
Given (Dsk, Esk), the least b ∈G−Dsk must belong to some Bi, and since G(n−2)
computes ∅(n−1) it can tell which Bi. Then G(n−2) can produce Ci until the ﬁrst
(D∗, E∗) extending (Dsk ∪{b}, {x ∈Esk : x > b}), and then obtain (D∗∗, E∗∗)
from (D∗, E∗) by forcing the jump. By construction, G satisﬁes (D∗∗, E∗∗) and
(Dsk+1, Esk+1) = (D∗∗∪{b}, {x ∈E∗∗: x > b}) for the least b ∈G−Dsk+1. And
this b is in B1 or B0 depending as k is or is not in B.
Corollary 3. Not every weakly n-generic set is n-generic.
Proof. By the previous proposition, ∅(n) ≡T G(n−2) for some weakly n-generic
set G. By Theorem 1, if G were n-generic we would have ∅(n+1) ≡T G(n−1) ≡T
G′ ⊕∅(n) ≡T ∅(n), which cannot be.
In spite of Theorem 1, we are still left with the possibility that some Mathias
n-generic set has Cohen 1-generic degree. We now show that this cannot happen.
Theorem 2. If G is n-generic then it has GH1 degree, i.e., G′ ≡T (G ⊕∅′)′.

On Mathias Generic Sets
137
Proof. A condition (D, E) forces i ∈(G ⊕∅′)′ if there is a σ ∈2<ω such that
that Φσ
i (i) ↓and for all x < |σ|,
σ(x) = 1 =⇒(D, E) ⊩x ∈G ⊕∅′;
σ(x) = 0 =⇒(D, E) ⊩x /∈G ⊕∅′.
This is thus a Σ0
2 relation, as forcing x ∈G⊕∅′ and x /∈G⊕∅′ are Σ0
1 and Π0
1,
respectively. We claim that (D, E) forcing i /∈(G ⊕∅′)′, i.e., ¬(i ∈(G ⊕∅′)′),
is equivalent to (D, E) having no ﬁnite extension that forces i ∈(G ⊕∅′)′, and
hence is Π0
2. That forcing implies this fact is clear. In the other direction, suppose
(D, E) does not force i /∈(G⊕∅′)′, and so has an extension (D∗, E∗) that forces
i /∈(G ⊕∅′)′. Let σ witness this fact, as above. Then if P and N consist of
the x < |σ| such that σ(2x) = 1 and σ(2x) = 0, respectively, σ witnesses that
(D ∪P, {x ∈E : x > max P ∪N}) also forces i ∈(G ⊕∅′)′.
We now show that G′ ≥T (G ⊕∅′)′. Let Ci be the set of conditions that force
i ∈(G ⊕∅′)′, and Di the set of conditions that force i /∈(G ⊕∅′)′. Then Ci is Σ0
3
and Di is Π0
2, and indices for them as such can be found uniformly from i. Each Ci
must be either met or avoided by G, and as in the proof of Theorem 1, G meets Ci
if and only if it does not meet Di. Which of the two is the case can be determined
by G′ since G′ ≥T ∅′′ and Ci and Di are both c.e. in ∅′′. By Proposition 4, G′ can
thus determine whether i ∈(G ⊕∅′)′, as desired.
Recall that a degree d is GLn if d(n) = (d ∪0′)(n−1), and that no such degree
can be GH1. It was shown by Jockusch and Posner [7, Corollary 7] that every
GL2 degree computes a Cohen 1-generic set. Hence, we obtain the following:
Corollary 4. Every Mathias n-generic set has GLm degree for all m ≥1.
Hence, it is not of Cohen 1-generic degree, but does compute a Cohen 1-generic.
We leave open the following question, which we have so far been unable to
answer. Partial answers are given in the subsequent results.
Question 2. Does every Mathias n-generic set compute a Cohen n-generic set?
Theorem 3. If G is Mathias n-generic, and A ≤T ∅(n−1) is bi-immune (i.e.,
A and A are each co-immune), then G ⊕A computes a Cohen n-generic.
Proof. For every set S = {s0 < s1 < · · · }, deﬁne SA = A(s0)A(s1) · · · , which is
a string in 2<ω if S is ﬁnite, and a sequence in 2ω otherwise. Now let C0, C1, . . .
be a listing of all Σ0
n subsets of 2<ω, together with ﬁxed ∅(n−1)-computable
enumerations. For each i, let Di be the set of all conditions (D, E) such that the
string DA belongs to Ci. Then Di is a Σ0
n set of conditions, and as such must be
met or avoided by G. If G meets Di then GA, viewed as an element of 2ω, meets
Ci. If G avoids Di, we claim that GA must avoid Ci. Indeed, suppose G avoids Di
via (D, E). Since A and A are co-immune, they intersect E inﬁnitely often, and
so if DA had an extension τ in Ci, we could make a ﬁnite extension (D∗, E∗) of
(D, E) so that D∗
A = τ. This extension would belong to Di, a contradiction.

138
P.A. Cholak, D.D. Dzhafarov, and J.L. Hirst
Thus, for example, the join of G with any non-computable A ≤T ∅′ computes a
Cohen n-generic, as every such A is bi-immune [5, Corollary 5 (iii)].
Proposition 6. If G is Mathias n-generic and H is Cohen n-generic then H
is not many-one reducible to G.
Proof. Seeking a contradiction, suppose f is a computable function such that
f(H) ⊆G and f(H) ⊆G. The set of conditions (D, E) with E ⊆ran(f) is
Σ0
3-deﬁnable, and must be met by G else G ∩ran(f) would be ﬁnite and H
would be computable. So ﬁx a condition (D, E) in this set satisﬁed by G. For
all a > max D, we then have that a ∈G if and only if a ∈E and f −1(a) ⊆H. It
follows that G ≤T H, and hence that G ≡T H, contradicting our observation at
the end of Section 3 that no Mathias n-generic can have Cohen n-generic degree.
Acknowledgements. The ﬁrst author was partially supported by NSF grant
DMS-0800198; the second author was partially supported by an NSF Postdoc-
toral Fellowship; the third author was partially supported by grant ID#20800
from the John Templeton Foundation. The opinions expressed in this publica-
tion are those of the authors and do not necessarily reﬂect the views of the John
Templeton Foundation. The authors are grateful to Christopher P. Porter and
the anonymous referees for helpful comments.
References
1. Binns, S., Kjos-Hanssen, B., Lerman, M., Solomon, R.: On a conjecture of Dobrinen
and Simpson concerning almost everywhere domination. J. Symbolic Logic 71, 119–
136 (2006)
2. Cholak, P.A., Jockusch Jr., C.G., Slaman, T.A.: On the strength of Ramsey’s
theorem for pairs. J. Symbolic Logic 66, 1–55 (2001)
3. Downey, R.G., Hirschfeldt, D.R.: Algorithmic randomness and complexity. Theory
and Applications of Computability. Springer, New York (2010)
4. Dzhafarov, D.D., Jockusch Jr., C.G.: Ramsey’s theorem and cone avoidance. J.
Symbolic Logic 74, 557–578 (2009)
5. Jockusch Jr., C.G.: The degrees of bi-immune sets. Z. Math. Logik Grundlagen
Math. 15, 135–140 (1969)
6. Jockusch Jr., C.G.: Degrees of generic sets. In: Drake, F.R., Wainer, S.S. (eds.)
Recursion Theory: its Generalisation and Applications. London Math. Soc. Lecture
Note Ser., vol. 45, pp. 110–139. Cambridge University Press, Cambridge (1980)
7. Jockusch Jr., C.G., Posner, D.B.: Double jumps of minimal degrees. J. Symbolic
Logic 43, 715–724 (1978)
8. Kurtz, S.A.: Notions of weak genericity. J. Symbolic Logic 48, 764–770 (1983)
9. Seetapun, D., Slaman, T.A.: On the strength of Ramsey’s theorem. Special Issue:
Models of arithmetic. Notre Dame J. Formal Logic 36, 570–582 (1995)
10. Soare, R.I.: Computability theory and applications. Theory and Applications of
Computability. Springer, New York (2012)
11. Soare, R.I.: Sets with no subset of higher degree. J. Symbolic Logic 34, 53–56 (1969)
12. Yu, L.: Lowness for genericity. Arch. Math. Logic 45, 233–238 (2006)

Complexity of Deep Inference via Atomic Flows
Anupam Das
Department of Computer Science, University of Bath, Claverton Down,
BA2 7AY, United Kingdom
a.das@bath.ac.uk
Abstract. We consider the fragment of deep inference free of compres-
sion mechanisms and compare its proof complexity to other systems,
utilising ‘atomic ﬂows’ to examine size of proofs. Results include a simu-
lation of Resolution and dag-like cut-free Gentzen, as well as a separation
from bounded-depth Frege.
1
Introduction
Deep inference diﬀers from other proof formalisms by allowing derivations them-
selves to be composed by logical connectives. There has recently been a lot of
activity in the proof complexity of deep inference [2], most notably that a cut-
free system, KS+, quasipolynomially simulates Frege systems [12] [3]. It is con-
jectured that this can be improved to a polynomial simulation, so ﬁnding lower
bounds for KS+ is probably as hard as ﬁnding one for Frege, which has escaped
proof complexity theorists for years.
However this quasipolynomial simulation relies crucially on the presence of
dag-like behaviour, manifested in deep inference by a particular rule, cocontrac-
tion:
A
−−−−−−
A ∧A. Without it we have a minimal complete system closed under deep
inference, KS. This system is free of compression mechanisms, in that a proof
of a conjunction can be ‘partitioned’ into proofs of each conjunct, unlike proofs
that are dag-like or contain cut.
It is conjectured that KS is unable to polynomially simulate KS+ [2], raising
the question of exactly where it ﬁts in the hierarchy of proof systems.
In this paper we focus on upper bounds and simulations to demonstrate the
relative strength of KS. Our arguments employ atomic ﬂows [10], diagrams that
track structural changes in a proof but forget logical information, to show that
cocontraction, and certain other steps, can be sometimes eliminated from a proof
in polynomial time. A comprehensive introduction to atomic ﬂows can be found
in [11].
Our main result is a polynomial simulation of dag-like cut-free Gentzen sys-
tems (dag-Gen−) in KS, improving on the simulation of the tree-like system in
[2]. This also places KS in the gap between dag-Gen−and a variation augmented
with elimination rules (Gen⋆), shown in [7] to simulate KS+, thereby quasipoly-
nomially simulating Frege by the aforementioned result. This is discussed further
in conclusion 7.2.
Fig. 1 summarises our results, and full proofs of results can be found in [8].
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 139–150, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

140
A. Das
bounded-depth Frege
Resolution
dag-Gen−
KS
KS+
Gen⋆
[7]
Frege
⊆
×
Cor. 34
×
⊆
×
?
?
Thm. 39
Thm. 47
×
×
⊆
[12]
quasi
⊆
Fig. 1. Relative complexity of systems after results in this paper
2
Deep Inference
We work in propositional logic over the basis {¯·, ∧, ∨} with formulae in negation
normal form. Syntactic equivalence of formulae is denoted ≡.
Deﬁnition 1 (Rules and Systems). An inference rule is a binary relation
on formulae decidable in polynomial time, and a system is a set of rules. We
deﬁne the rules we use below, and the systems KS = {ai↓, aw↓, ac↓, s, m}, KS+ =
KS ∪{aw↑, ac↑}, SKS = KS+ ∪{ai↑} and KS = {ai↑, aw↑, ac↑, s, m}.
We also have a logical rule = which allows us to apply laws of associativity,
commutativity and basic equations with units [2].
Atomic structural rules
Linear logical rules
t
ai↓−−−−−
a ∨¯a
f
aw↓−−
a
a ∨a
ac↓−−−−−
a
A ∧[B ∨C]
s −−−−−−−−−−−−−
(A ∧B) ∨C
identity
weakening
contraction
switch
a ∧¯a
ai↑−−−−−
f
a
aw↑−−
t
a
ac↑−−−−−
a ∧a
(A ∧B) ∨(C ∧D)
m −−−−−−−−−−−−−−−−−−−−−
[A ∨C] ∧[B ∨D]
cut
coweakening
cocontraction
medial
Deﬁnition 2 (Proofs and Derivations). We deﬁne derivations, and premiss
and conclusion functions (pr, cn resp.), inductively. Every formula A is a deriva-
tion with pr(A) ≡A ≡cn(A). For derivations Φ, Ψ: if ⋆∈{∧, ∨} then Φ ⋆Ψ is a
derivation with premiss pr(Φ) ⋆pr(Ψ) and conclusion cn(Φ) ⋆cn(Ψ); if
cn(Φ)
ρ −−−−−−
pr(Ψ) is
an instance of a rule ρ,
Φ
ρ −−
Ψ is a derivation with premiss pr(Φ) and conclusion
cn(Ψ).

Complexity of Deep Inference via Atomic Flows
141
If pr(Φ) ≡t then we call Φ a proof. If Φ is a derivation in a system S with
premiss A, conclusion B, we write
A
Φ
 S
B
. If A ≡t, i.e., Φ is a proof, we write
−
Φ
 S
B .
Proposition 3 ([1]). Each rule ρ below is derivable in {s, m, aρ}:
t
i↓−−−−−−
A ∨¯A
f
w↓−−
A
A ∨A
c↓−−−−−−
A
A ∧¯A
i↑−−−−−−
f
A
w↑−−
t
A
c↑−−−−−−
A ∧A
We shall use the above ‘generic’ rules as abbreviations for their derivations.
Deﬁnition 4 (Complexity). We deﬁne the size |Φ| of a derivation Φ to be the
number of atom occurrences in Φ. A system S p-simulates a system T if each
T -proof can be polynomially transformed into an S-proof of the same conclusion.
3
Atomic Flows
Deﬁnition 5 (Atomic Flows). For an SKS derivation Φ we deﬁne its atomic
ﬂow, fl(Φ), to be the diagram obtained by tracing the path of each atom through
the derivation, designating structural rules by the following corresponding nodes:
t
ai↓−−−−−
a ∨¯a
	
→
f
aw↓−−
a
	
→
a ∨a
ac↓−−−−−
a
	
→
a ∧¯a
ai↑−−−−−
f
	
→
a
aw↑−−
t
	
→
a
ac↑−−−−−
a ∧a
	
→
We consider ﬂows as graphs embedded in the plane with the six types of nodes
above. Note that edges may be pending at either end.
We deﬁne the size of a ﬂow φ, denoted |φ|, to be its number of edges.
Deﬁnition 6. We deﬁne a rewriting system norm on ﬂows in Fig. 2.
Proposition 7 ([10]). norm is terminating and conﬂuent.
Notation 8. If a ﬂow ψ is the normal form of a ﬂow φ under a terminating,
conﬂuent rewriting system r, then we write φ →
r ψ.
Deﬁnition 9. If R is a relation on atomic ﬂows we say that R lifts polynomially
to SKS if, whenever (fl(Φ), ψ) ∈R, we can construct a derivation Ψ in time
polynomial in |Φ| + |ψ| with the same premiss and conclusion as Φ and atomic
ﬂow ψ.
Theorem 10 ([10]). −→
norm lifts polynomially to SKS.

142
A. Das
w↓-c↓:
→
i↓-w↑:
→
c↑-w↑:
→
w↓-c↑:
→
w↓-w↑:
→
c↓-w↑:
→
c↓-c↑:
→
i↓-c↑:
→
Fig. 2. Local rewriting rules for the system norm
Corollary 11. If φ is the ﬂow of a KS+ proof, φ −→
norm ψ then ψ is the ﬂow of a
KS proof.
Example 12. In Fig. 3 we give a derivation, its ﬂow and a reduction under norm.
We consider atoms to be positive or negative, under some valid assignment of
polarity. We use the terms ‘upwards’ and ‘downwards’ with regard to derivations
and ﬂows, interpreted in the natural way, independently from the notion of
direction deﬁned below.
Deﬁnition 13 (Paths). To each edge we assign a direction: downwards if the
atom associated with it is positive, upwards if it is negative.
We deﬁne a path in a ﬂow to be a directed path between pending edges.
Example 14. We give the following ﬂow and all its paths:
⋆1
−
+
4
+
−
5
6
7
8
9
3
2
−
+
⋆
0
23679, 23678,
4578, 4579,
1.
where +, −indicate the polarity of the atom associated with an edge, under
some assignment, and ⋆indicates that either polarity may be correctly assigned.
Notice that the number of paths is invariant under valid assignments of
polarity.
The following results allow us to estimate the size of the normal form of a ﬂow,
under norm, without actually constructing it.
Observation 15. Reducing under norm conserves the number of paths in a ﬂow.

Complexity of Deep Inference via Atomic Flows
143
Recall that, in a proof, there are no assumptions, and so the ﬂow of a proof
can have no edge with upper end pending; it must be attached to an identity or
weakening node.
Let #(ρ, φ) be the number of ρ-nodes in a ﬂow φ, and ⌜φ⌝be its number of
paths.
Observation 16. If φ is the ﬂow of a KS proof, then ⌜φ⌝= #(ai↓, φ).
Theorem 17. If φ is the ﬂow of a KS+ proof, φ −→
norm ψ, then |ψ| = O(|φ|+⌜φ⌝).
Proof. Decompose ψ into its identity fragment ψ1 and weakening fragment ψ2.
Note that each rule involving w↓or w↑reduces the size of the ﬂow, so |ψ2| ≤|φ|.
Notice that |ψ1| = 2 · #(ai↓, ψ1) + #(ac↓, ψ1). However, clearly, the number
of contractions cannot outnumber the number of edges emanating from identity
steps, so we have |ψ1| ≤4 · #(ai↓, ψ1). By Obs. 16 we then have |ψ1| ≤4 · ⌜ψ⌝,
and by Obs. 15 that |ψ1| ≤4 · ⌜φ⌝, whence |ψ| = |ψ1| + |ψ2| ≤|φ| + 4 · ⌜φ⌝.
→
→
t
i↓−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
a ∨a
ac↓−−−−−
a
∨
¯a
aw↑−−
t
∧
¯a
ac↑−−−−−
¯a ∧¯a
= −−−−−−−−−−−−−−−−−−−
¯a ∧¯a
→
t
i↓−−−−−−−−−−−−−−−−−−−−−−−−−−
a ∨a
ac↓−−−−−
a
= −−−−−−−−−−−
a ∨
f
aw↓−−
a
ac↓−−−−−−−−−−−
a
∨(¯a ∧¯a)
→
t
i↓−−−−−−−−−−−−−−−−−−−−
a ∨a
ac↓−−−−−
a
∨(¯a ∧¯a)
Fig. 3. An example of a derivation, its ﬂow and a reduction under norm
Remark 18. The main contributor to an increase of ﬂow size reducing under
norm is the rule c↓-c↑. It can sometimes cause an exponential blowup [10].
The following proposition estimates the increase in size caused by c↓-c↑.
Proposition 19. If in every directed path of a ﬂow φ there are at most k alter-
nations of ac↑and ac↓nodes then ⌜φ⌝= |φ|O(k).
4
Truth Tables and Tableaux
Bruscoli and Guglielmi have proved that tree-like cut-free Gentzen cannot p-
simulate KS, by way of the Statman tautologies [2]. We oﬀer a new proof here,
via truth tables.

144
A. Das
Observation 20. A truth table proof for a formula A has size |A| · 2#A, where
#A is the number of distinct propositional variables in A.
Lemma 21. KS+ p-simulates truth tables.
Proof. Let τ be a tautology. For each partial assignment A, deﬁned on just those
atoms appearing in τ construct a derivation ΦA(τ) by structural induction on τ:
ΦA(a) ≡a
,
ΦA(A ∧B) ≡ΦA(A) ∧ΦA(B)
,
ΦA(A ∨B) ≡
ΦA(B)
= −−−−−−−−−−
f
w↓−−
A
∨B
where, in the last case, when τ is a disjunction, choose a disjunct B that is true
under A. It is clear that each ΦA(τ) has conclusion τ and premiss a conjunction
of literals; moreover this conjunction of literals is satisﬁed by A.
Let γA be the conjunction of all literals satisﬁed by A such that each literal
appears at most once. Then there is a derivation of the form:
γA
 {aw↑,ac↑}
pr(ΦA(τ))
.
By distributivity, derived on the left, we can construct a proof Ψ of 
A γA in
{ai↓, ac↑, s, m}, and then apply contractions to obtain the proof, on the right:
Distributivity:
A
c↑−−−−−−
A ∧A
∧[B ∨C]
= −−−−−−−−−−−−−−−−−−−−
A ∧
[B ∨C] ∧A
s −−−−−−−−−−−−−
B ∨(C ∧A)
= −−−−−−−−−−−−−−−−−−−−
A ∧[B ∨(A ∧C)]
s −−−−−−−−−−−−−−−−−−−−
(A ∧B) ∨(A ∧C)
,
−
Ψ
 {ai↓,ac↑,s,m}

A
⎡
⎢⎢⎢⎣
γA
 {aw↑,ac↑}
pr(ΦA)
ΦA
 {w↓}
τ
⎤
⎥⎥⎥⎦
 {c↓}
τ
Theorem 22. KS p-simulates truth tables.
Proof. In the above proofs all c↑steps are above all c↓steps, so by Prop. 19
the number of paths is polynomial in the size of the ﬂow. The result follows by
Thms. 17 and 10.
Notation 23. Let tree/dag-Gen−denote cut-free Gentzen with tree/dag proofs
resp.
Proposition 24 ([6]). Tree-Gen−cannot p-simulate truth tables.
Corollary 25. Tree-Gen−cannot p-simulate KS.
Proof. Immediate from Prop. 24 and Thm. 22.
5
Separations via the Functional Pigeonhole Principle
We show Gen−, Resolution and bounded-depth Frege systems cannot p-simulate
KS, by reducing under norm Jeˇr´abek’s KS+ proofs of the functional pigeonhole
principle.
Conversely we give a simulation of Resolution, and some extensions, in KS.

Complexity of Deep Inference via Atomic Flows
145
5.1
Polynomial-Size Proofs of the Functional Pigeonhole Principle
The functional pigeonhole principle is a class of propositional tautologies assert-
ing that there is no injective function from a set of size n + 1 to a set of size
n.
Deﬁnition 26 (Functional Pigeonhole Principle)
FPHPn ≡
n
i=0
n
j=1
¯aij ∨
n
i=0
n−1

j=1
n
j′=j+1
(aij ∧aij′) ∨
n
j=1
n−1

i=0
n

i′=i+1
(aij ∧ai′j)
Theorem 27 ([15][14]). Bounded-depth Frege has only exponential-size proofs
of FPHPn.
Corollary 28. Resolution and Gen−have only exponential-size proofs of FPHPn.
Theorem 29 ([4]). There are polynomial-size Frege proofs of FPHPn.
Proposition 30 ([2]). SKS is polynomially equivalent to Frege systems.
Lemma 31. Every SKS proof Φ of a formula A can be polynomially transformed
to a KS proof of A ∨
i(ai ∧¯ai), where ai are the distinct propositional variables
in A.
Lemma 32 ([12]). There are polynomial-size proofs Θn in KS+ of FPHPn.
Proof (Jeˇr´abek). By Thm. 29, Prop. 30 and Lemma 31 we can build KS proofs
Φn of FPHPn ∨
i,j(aij ∧¯aij) that have size polynomial in n. For each atom ast
we construct a derivation Ψ ast
n
in KS+ \ {ac↓} from ast ∧¯ast to FPHPn, on the
left below, and apply contractions to obtain the proof, on the right:
ast ∧¯ast
= −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
ast ∧¯ast ∧
t
i↓−−−−−−−−−−−−−−−−−−−−−−

j̸=t ¯asj ∨
j̸=t asj
2·s −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−

j
¯asj ∨
⎛
⎜
⎜
⎝
ast
n·ac↑−−−−−−−−−−

j̸=0 ast
∧
j̸=t
asj
 {s}

j̸=t ast ∧asj
⎞
⎟
⎟
⎠
w↓−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
FPHPn
,
−
Φn
 KS
⎡
⎢⎣FPHPn ∨
i,j
⎛
⎜
⎝
aij ∧¯aij
Ψ
aij
n
 KS+\{ac↓}
FPHPn
⎞
⎟
⎠
⎤
⎥⎦
 {c↓}
FPHPn
Theorem 33. There are polynomial-size proofs in KS of FPHPn.
Proof. In Θn above, there are 2 alternations between c↓and c↑nodes, so ⌜fl(Θn)⌝
= |fl(Θn)|O(2) by Prop. 19. The result follows by Thms. 17 and 10.
Corollary 34. Gen−, Resolution, bounded-depth Frege cannot p-simulate KS.
Proof. Immediate from Thm. 27, Cor. 28 and Thm. 33.

146
A. Das
5.2
A Polynomial Simulation of Resolution and Some Extensions
We give a p-simulation of resolution systems in KS, ﬁrst noticed in [9].
Deﬁnition 35. We deﬁne Resolution by the following CNF rewriting rules:
RES =

A ∧[B ∨a] ∧[¯a ∨C]
res −−−−−−−−−−−−−−−−−−−−−−−
A ∧[B ∨C]
,
A ∧[B ∨a ∨a]
ac↓−−−−−−−−−−−−−−−−
A ∧[B ∨a] ,
A ∧B
dag −−−−−−−−−−−
A ∧B ∧B,
A ∧B
aw↓−−−−−−−−−−−−
A ∧[B ∨b]

modulo associativity and commutativity. A RES refutation is a derivation
A
 RES
f
.
Proposition 36. Deﬁne w↓-i↑:
→
and w = {w↓-c↑, w↓-w↑, w↓-i↑, w↓-c↓}.
Then w is terminating, conﬂuent and →
w lifts polynomially to SKS. [10]
Lemma 37. RES refutations can be polynomially transformed to ones in KS.
Proof. We derive a generalisation res′ of res on the left, and eliminate ac↓-steps
by the translation on the right. Finally, aw↓steps are eliminated by Prop. 36.
[B ∨C] ∧[ ¯C ∨D]
2·s −−−−−−−−−−−−−−−−−−−−
B ∨
C ∧¯C
i↑−−−−−−−
f
∨D
= −−−−−−−−−−−−−−−−−−−
B ∨D
,

B ∨
a ∨a
ac↓−−−−−
a

∧[¯a ∨D]
res −−−−−−−−−−−−−−−−−−−−−−−−−−−−
B ∨D
→
[B ∨a ∨a] ∧

¯a
ac↑−−−−−
¯a ∧¯a
∨D

res′ −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
B ∨D
Lemma 38. We can transform a KS refutation of ¯A in linear time to a KS
proof of A.
Proof (Sketch). ‘Flip’ the refutation upside-down, replace every formula with its
negation and every ‘up’ rule with its corresponding ‘down’ rule.
Theorem 39. KS p-simulates Resolution systems.
Proof. Immediate from Lemmata 38 and 37.
Finally, the simulation above can be extended to some basic extensions of Res-
olution, introduced by Kraj´ıˇcek [13], where literals are replaced by conjunctions
of literals.
Deﬁnition 40. RES(f) consists of the rules of RES, with atomic variables vary-
ing over conjunctions of literals, and the rule
A ∧[B ∨
i ai] ∧[
j bj ∨C]
∧−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
A ∧[B ∨(
i ai ∧
j bj) ∨C].
Additionally, in a derivation Φ, no conjunction of literals may be larger than
f(|Φ|).
Proposition 41. KS p-simulates RES(f) for any function f.
Proof. ∧is derivable in {s}, and the old rules can be dealt with as before.

Complexity of Deep Inference via Atomic Flows
147
6
A Simulation of Dag-Like Cut-Free Gentzen
We consider Gen in its one-sided variation, e.g., GS1p in [16], and identify se-
quents with the disjunction of their formulae, as an abuse of notation.
We now give a translation of dag-like cut-free Gentzen proofs to KS+, and
then KS. Naively we could just apply a generic cocontraction to simulate each
dag-step, duplicating the entire sequent, but this may lead to an exponential
blowup reducing under norm by Rmk. 18.
Instead we notice that, when two branches of a dag step are brought together
by a ∧step, we only need to cocontract the formulae which are common ancestors
to the conjuncts of the ∧step. For example:
Γ, A, B, C
dag −−−−−−−−−−−−−−−−−−−−−−−−−−
Γ, A, B, C
Γ, A, B, C
∧−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Γ, Γ, [A ∨B] ∧[A ∨C], B, C
→
Γ
w↓−−−−−−
Γ ∨Γ
∨
A
c↑−−−−−−−−−−−−−−−−−−−−−−
A
w↓−−−−−−
A ∨B
∧
A
w↓−−−−−−
A ∨C
∨B ∨C
When there are other rules between the dag and ∧steps, we can translate them
into deep steps, inside the conjunction [A ∨B] ∧[A ∨C] above, for example.
Deﬁnition 42. For a sequent Γ and formula A occurring in a Gen−derivation
π, let AncΓ (A) denote the set of ancestors of A occurring in Γ.
Deﬁnition 43. A contraction loop in a ﬂow φ is a (c↑, c↓) pair of nodes (ν1, ν2)
in φ, with ν1 above ν2, where there are two (or more) disjoint paths between ν1
and ν2.
Lemma 44. There is a polynomial transformation T from dag-Gen−derivations
to KS+ satisfying the following properties:
T :
Γtree-Gen−
Δ
→
ΓKS
Δ
T :
Γ
π1

Δ
π2

Σ
→
Γ
T π1

Δ
T π2

Σ
,
T :
Γ
dag −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Γdag-Gen−
Δ, A
Γdag-Gen−
B, Σ
∧−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Δ, A ∧B, Σ
→
Γ
= −−−−−−−−−−−−−−−−−−−
Γ ′ ∨
X
c↑−−−−−−−
X ∧X
⋆
KS+
R ∨(A ∧B)
= −−−−−−−−−−−−−−−−−−−
Γ ′ ∨R
 KS
Δ ∨Σ
∨(A ∧B)
where X = AncΓ (A) ∩AncΓ (B) and Γ ′ = Γ \ X, R is some formula, and there
are no contraction loops in ⋆.
Observation 45. If π is a dag-Gen−derivation then, by the subformula prop-
erty and the properties in Lemma 44, there are no contraction loops in fl(T π).
Lemma 46. If there are no contraction loops in a ﬂow φ then ⌜φ⌝is polynomial
in |φ|.

148
A. Das
Proof. Deﬁne c:
→
. c is terminating, conﬂuent and if φ →
c
ψ then
|φ| = |ψ| and ⌜φ⌝≤⌜ψ⌝. If φ has no contraction loops then, in ψ, all c↓-nodes
are above all c↑-nodes. So ⌜φ⌝≤⌜ψ⌝= |ψ|O(3) = |φ|O(3) by Prop. 19.
Theorem 47. KS p-simulates dag-like cut-free Gentzen systems.
Proof. Immediate from Obs. 45, Lemmata 46, 44 and Thms. 17, 10.
7
Conclusions
We have seen that KS is a surprisingly powerful system, despite lacking any
mechanism to compress proofs. As well as the simulations of Resolution and
dag-like cut-free Gentzen, it cannot be p-simulated by bounded-depth Frege,
one of the strongest ‘weak’ systems, and also has polynomial-size proofs of the
functional pigeonhole principle.
7.1
Atomic Flows as a Tool for Complexity Analysis
Atomic ﬂows are a useful tool to analyse and manipulate derivations; often we
can avoid the possibly exponential blowup in eliminating cocontractions. Further
work could investigate whether we can always avoid this blowup, via a local or
global ﬂow reduction.
7.2
Dag-Like Cut-Free Gentzen Systems and Variations
Results in [7] show that the addition of elimination rules (below) to dag-Gen−
result in a system Gen⋆that is p-equivalent to KS+, and so quasipolynomially
simulates Frege.
Γ, A ∨B
∨-Elim −−−−−−−−−
Γ, A, B
,
Γ, A ∧B
∧-Elim-L −−−−−−−−−
Γ, A
,
Γ, A ∧B
∧-Elim-R −−−−−−−−−
Γ, B
On the other hand we showed that, without these modiﬁcations, Gen−cannot
even p-simulate KS, and in fact that KS ﬁts neatly between these two variations:
dag-Gen−< KS ≦? KS+ = Gen⋆≦? Frege
The restriction on proofs caused by the subformula property seems to be critical;
it would be interesting to investigate its eﬀects on proof complexity in general.
We regard KS to be an uncompressed system: every proof of a conjunction
A ∧B can be partitioned into a proof of A and a proof of B, with no sharing
between them, by substituting t for one of the conjuncts and reducing every line
in the proof by =.

Complexity of Deep Inference via Atomic Flows
149
Consequently, for any dag-Gen−proof of a conjunction A ∧B there are KS
proofs of A and B whose sizes sum to the size of the initial proof, for some
notion of size globally accurate up to a polynomial. We thus argue that the
sharing eﬀect of dagness in cut-free Gentzen systems serves solely to do some of
the work of deep inference, but not all of it due to the strict separation between
the two systems.
We notice that the separation of KS and tree-Gen−in [2] is in fact just a special
case of Thm. 47, since dag-like cut-free Gentzen has polynomial-size proofs of
the Statman tautologies [5].
7.3
Stronger Systems
We showed that bounded-depth Frege cannot p-simulate KS but did not con-
sider the other direction. We conjecture that they are incomparable, due to the
dissimilar ways they are deﬁned. Similar questions persist for other ‘stronger’
systems, e.g., Cutting Planes, although ongoing research suggests we may be
able to obtain a separation of KS from Cutting Planes.
Acknowledgements. The author would like to thank Alessio Guglielmi and
Tom Gundersen for all their time and eﬀort in discussing this paper, and the
anonymous referees for their useful comments.
References
1. Br¨unnler, K., Tiu, A.F.: A local system for classical logic (2001); preprint (WV-
01-02) (2001)
2. Bruscoli, P., Guglielmi, A.: On the proof complexity of deep inference. ACM Trans-
actions on Computational Logic 10(2), article 14, 1–34 (2009)
3. Bruscoli, P., Guglielmi, A., Gundersen, T., Parigot, M.: Quasipolynomial normali-
sation in deep inference via atomic ﬂows and threshold formulae (2009) (submitted)
4. Buss, S.R.: Polynomial size proofs of the propositional pigeonhole principle. Journal
of Symbolic Logic 52(4), 916–927 (1987)
5. Clote, P., Kranakis, E.: Boolean Functions and Computation Models. Springer
(2002)
6. D’Agostino, M.: Are tableaux an improvement on truth-tables? Journal of Logic,
Language and Information 1, 235–252 (1992)
7. Das, A.: On the Proof Complexity of Cut-Free Bounded Deep Inference. In:
Br¨unnler, K., Metcalfe, G. (eds.) TABLEAUX 2011. LNCS, vol. 6793, pp. 134–
148. Springer, Heidelberg (2011)
8. Das, A.: Complexity of deep inference via atomic ﬂows (2012)(preprint)
9. Guglielmi, A.: Resolution in the calculus of structures (2003) (preprint)
10. Guglielmi, A., Gundersen, T.: Normalisation control in deep inference via atomic
ﬂows II (2008) (preprint)
11. Gundersen, T.: A General View of Normalisation Through Atomic Flows. Ph.D.
thesis, University of Bath (2009)

150
A. Das
12. Jeˇr´abek, E.: Proof complexity of the cut-free calculus of structures. Journal of
Logic and Computation 19(2), 323–339 (2009)
13. Kraj´ıˇcek, J.: On the weak pigeonhole principle (2001)
14. Kraj´ıˇcek, J., Pudl´ak, P., Woods, A.: An exponential lower bound to the size of
bounded depth frege proofs of the pigeonhole principle. Random Structures &
Algorithms 7(1), 15–39 (1995)
15. Pitassi, T., Beame, P., Impagliazzo, R.: Exponential lower bounds for the pigeon-
hole principle. Computational Complexity 3, 97–140 (1993)
16. Troelstra, A., Schwichtenberg, H.: Basic Proof Theory. Cambridge Tracts in The-
oretical Computer Science, vol. 43. Cambridge University Press (1996)

Connecting Partial Words
and Regular Languages
J¨urgen Dassow1, Florin Manea2, and Robert Merca¸s1
1 Fakult¨at f¨ur Informatik, Otto-von-Guericke-Universit¨at Magdeburg, Postfach 4120,
39016 Magdeburg, Germany
dassow@iws.cs.uni-magdeburg.de, robertmercas@gmail.com
2 Institut f¨ur Informatik, Christian-Albrechts-Universit¨at zu Kiel, 24098 Kiel,
Germany
flm@informatik.uni-kiel.de
Abstract. We initiate a study of languages of partial words related to
regular languages of full words. First, we study the possibility of ex-
pressing a regular language of full words as the image of a partial-words-
language through a substitution that only replaces the hole symbols of
the partial words with a ﬁnite set of letters. Results regarding the struc-
ture, uniqueness and succinctness of such a representation, as well as
a series of related decidability and computational-hardness results, are
presented. Finally, we deﬁne a hierarchy of classes of languages of partial
words, by grouping together languages that can be connected in strong
ways to regular languages, and derive their closure properties.
1
Introduction
Two DNA strands attach one to the other, normally, in a complementary way
according to their nucleotides. That is, each purine, A or G, creates a hydrogen
bond with one complementary pyrimidine, T or C, respectively. But, sometimes,
it is the case that this process goes wrong, allowing G-T bonds. Starting from
this situation, and motivated by the need of having a way to recover (as much
as possible) and work with a correct DNA sequence, Berstel and Boasson [1]
suggested the usage of partial words as a suitable mathematical model. Partial
words are words that beside regular letters contain an extra “joker” symbol, also
called “hole” or “do-not-know” symbol, that matches all symbols of the original
alphabet, which were investigated already in the 1970s [3]. Going back to the
initial example, one could recover the information regarding a DNA sequence
from the badly bonded pair of DNA strands by associating actual letters to the
positions where the bonds were correct and holes to the positions where the
bonds were not correctly formed. Besides the above motivation, partial words
may ﬁnd applications in other ﬁelds, as well; for instance, they can be seen
as data sequences that are corrupted either by white noise or other external
factor, or even incomplete or insuﬃcient information, that one needs in some
process. In the last decade a lot of combinatorial and algorithmic properties
of partial words have been investigated (see the survey [2], and the references
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 151–161, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

152
J. Dassow, F. Manea, and R. Merca¸s
therein). Surprisingly, so far, no study of classes of languages of partial words
(or sets of partial words that have common features) was performed. The only
work on a connected topic, that we are aware of, is [4]. There, the concept
of restoration of punctured languages and several similarity measures between
full-words-languages, related to this concept, were investigated. More precisely,
puncturing a word means replacing some of its letters with holes; from a language
of punctured words, its restoration was obtained by taking all the languages that
can be punctured to obtain the respective language. The results of [4] regarded
classes of full-words-languages deﬁned by applying successively puncturing and
restoration operations to classes of languages from the Chomsky hierarchy.
The study of the class of regular languages, the most restrictive class of the
Chomsky-hierarchy, has been one of the central topics in theoretical computer
science. This class of languages, deﬁned usually either as the class of languages
accepted by ﬁnite automata or as the class of languages described by regular
expressions, was extensively studied throughout the last seventy years (starting
from the early 1940s) and, besides its impact in theory (mostly in language
theory, but also in complexity theory, for instance), it was shown to have a
wide range of applications. Regular languages, and the various mechanisms used
to specify them, were used, for instance, in compilers theory, circuit design,
text editing, pattern matching, formal veriﬁcation, DNA computing, or natural
language processing (see [7]).
In this work, we aim to establish a stronger connection between the attractive
notions mentioned above: partial words, on one side, and regular languages, on
the other side. First, we show how we can (non-trivially) represent every regular
language as the image of a regular language of partial words through a substi-
tution that deﬁnes the letters that may replace the hole (called ⋄-substitution,
in the following). Moreover, we show that such a representation can be useful:
for some regular languages, there exist deterministic ﬁnite automata accept-
ing languages of partial words that represent the full-word-language and are
exponentially more succinct than the minimal deterministic ﬁnite automaton
accepting that language. Unfortunately, it may also be the case when the min-
imal non-deterministic ﬁnite automaton accepting a language is exponentially
more succinct than any deterministic automaton accepting a language of partial
words representing the same language. Generally, automata accepting languages
of partial words representing a given full-words language can be seen as inter-
mediate between the deterministic ﬁnite automata and the non-deterministic
automata accepting that language. We also present a series of algorithmic and
complexity results regarding the possibility of representing a regular language
as the image of a language of partial words through a ⋄-substitution.
Motivated by the above results, that connect in a meaningful way languages of
partial words to regular languages of full words, and by the theoretical interest of
studying systematically such languages, we deﬁne a series of classes of languages
of partial words. Each of these classes contains languages that can be placed in
a particular strong relation with the regular languages. Further, we investigate

Connecting Partial Words and Regular Languages
153
these classes from a language theoretic point of view, show that they form a
hierarchy, and establish their closure properties.1
We begin the paper with a series of basic deﬁnitions. Let V be a non-empty
ﬁnite set of symbols called an alphabet. Each element a ∈V is called a letter. A
full word (or, simply, word) over V is a ﬁnite sequence of letters from V while a
partial word over V is a ﬁnite sequence of letters from V ∪{⋄}, the alphabet V
extended with the distinguished hole symbol ⋄. The length of a (partial) word u
is denoted by |u| and represents the total number of symbols in u; the number
of occurrences of a symbol a in a (partial) word w is denoted |w|a. The empty
(partial) word is the sequence of length zero and is denoted by λ. We denote
by V ∗(respectively, (V ∪{⋄})∗) the set of words (respectively, partial words)
over the alphabet V and by V + (respectively, (V ∪{⋄})+) the set of non-empty
words (respectively, non-empty partial words) over V . The catenation of two
(partial) words u and v is deﬁned as the (partial) word uv. Recall that V ∗(where
the alphabet V may include the ⋄symbol) is the free monoid generated by V ,
under the operation of catenation of words; the unit element in this monoid is
represented by the empty word λ. A language L of full words over an alphabet V
is a subset of V ∗; a language of partial words L over an alphabet V (that does not
contain the ⋄symbol) is a subset of (V ∪{⋄})∗. Given a language L we denote by
alph(L) (the alphabet of L) the set of all the letters that occur in the words of L;
for the precision of the exposure, we say that a language L of full (respectively,
partial) words is over V , with ⋄/∈V , if and only if alph(L) = V (respectively,
alph(L) = V ∪{⋄}). For instance, L = {abb, ab⋄} has alph(L) = {a, b, ⋄}, thus,
is a language of partial words over {a, b}. Note that the catenation operation
can be extended to languages; more precisely, if L1 and L2 are languages over
V , we deﬁne their catenation L1L2 = {w1w2 | w1 ∈L1, w2 ∈L2}.
Let u and v be two partial words of equal length. We say that u is contained
in v, denoted by u < v, if u[i] = v[i] for all u[i] ∈V ; moreover, u and v are
compatible, denoted by u ↑v, if there exists a word w such that u < w and
v < w. These notions can be extended to languages. Let L and L′ be two
languages of partial words with alph(L) ∪alph(L′) = V ∪{⋄} and ⋄/∈V . We
say that L is contained in L′, denoted L < L′, if, for every word w ∈L, there
exists a word w′ ∈L′ such that w < w′. We say that L is compatible to L′,
denoted L ↑L′, if, for each w ∈L, there exists w′ ∈L′ such that w ↑w′ and,
for each v′ ∈L′, there exists v ∈L such that v′ ↑v.
A substitution is a mapping h : V ∗→2U∗with h(xy) = h(x)h(y), for x, y ∈
V ∗, and h(λ) = {λ}; h is completely deﬁned by the values h(a) for all a ∈V . A
morphism is a particular type of a substitution for which h(a) contains exactly
one element for all a ∈V ; i.e., a morphism is a function h : V ∗→U ∗with
h(xy) = h(x)h(y) for x, y ∈V ∗. A ⋄-substitution over V is a substitution with
h(a) = {a}, for a ∈V , and h(⋄) ⊆V . Here we assume that ⋄can replace any
symbol of V .
1 A technical appendix containing full proofs of our results can be found at the web-
page: https://www.informatik.uni-kiel.de/zs/pwords

154
J. Dassow, F. Manea, and R. Merca¸s
In this paper, DFA stands for deterministic ﬁnite automaton and NFA for non-
deterministic ﬁnite automaton; the language accepted by a ﬁnite automaton M
is denoted L(M). Also, the set of all the regular languages is denoted by REG;
by REGfull, we denote the set of all the regular languages of full words. Further
deﬁnitions regarding ﬁnite automata and regular languages can be found in [7],
while partial words are surveyed in [2].
2
Deﬁnability by Substitutions
Let us begin our investigation by presenting several results regarding the way
regular languages can be expressed as the image of a language of partial words
through a substitution.
Lemma 1. Let L ⊆(V ∪{⋄})∗{⋄}(V ∪{⋄})∗be a language of partial words and
let σ be a ⋄-substitution over V . There exists a language L′ such that σ(L) =
σ(L′) and |w|⋄= 1 for all w ∈L′.
Lemma 2. Let L be a regular language over V and let σ be a ⋄-substitution
over V . Then there exists a maximal (with respect to set inclusion) language
L′ ⊆L which can be written as σ(L′′), where L′′ is a language of partial words
such that any word in L′′ has exactly one hole. Moreover, L′ and L′′ are regular
languages and, provided that L is given by a ﬁnite automaton accepting it, one
can algorithmically construct a ﬁnite automaton accepting L′ and L′′.
Proof. Let σ(⋄) = V ′.
We start by noting that a word w belongs to σ(L′′) for a language of partial
words L′′, whose elements contain at least one hole each, if and only if there
exist the words x and y such that w = xay, for some a ∈V ′ and {x}V ′{y} ⊆L.
Now let M = (Q, V, q0, F, δ) be a DFA accepting L.
Let q ∈Q. We deﬁne the language Rq as follows. A word w of L is in Rq if and
only if there exists the partial word x⋄y, compatible with w, with δ(q0, x) = q,
S = δ(q, V ′) ⊆Q, δ(S, y) ⊆F. Basically, Rq is the set of the words for which
there exists a compatible partial word x⋄y with exactly one hole, such that x
labels a path from q0 to q in A and any word from {x}V ′{y} is in L.
Clearly, Rq = {x | x ∈V ∗, δ(q0, x) = q}V ′{y | y ∈V ∗, δ(q′, y) ∈F for
all q′ ∈δ(q, V )}. It follows that Rq is regular and an automaton accepting
this language can be constructed starting from M. Moreover, Rq = σ(Hq), for
Hq = {x | x ∈V ∗, δ(q0, x) = q}{⋄}{y | y ∈V ∗, δ(q′, y) ∈F for all q′ ∈δ(q, V )}.
Take now L′ = ∪q∈QRq and L′′ = ∪q∈QHq. Then L′ is regular, as all the
languages Rq are regular, and L′ = σ(L′′). We only have to show that L′ is
maximal. If there exists L1 ⊂L and a language of partial words L2, whose
elements contain exactly one hole each, such that σ(L2) = L1, then for every
word w of L1 there exist the words x and y such that x⋄y ∈L2 and w ∈σ(x⋄y) =
{x}V ′{y} ⊆σ(L2) = L1. Thus, w ∈Rq for q = δ(q0, x). Therefore, L1 ⊆L′.
⊓⊔
Note that the sets Rq with q ∈Q are not a partition of L, as they are not
necessarily mutually disjoint.

Connecting Partial Words and Regular Languages
155
Next we introduce two relations connecting partial-words-languages and full-
words-languages.
Deﬁnition 1. Let L ⊆V ∗be a language and σ be a ⋄-substitution over V . We
say that L is σ-deﬁned by the language L′, where L′ ⊆(V ∪{⋄})∗is a partial-
words-language, if L = σ(L′). Moreover, we say that L is essentially σ-deﬁned
by L′, where L′ ⊆(V ′ ∪{⋄})∗, if L = σ(L′) and every word in L′ contains at
least a ⋄-symbol.
Obviously, for any regular language L over V , there is a regular language L′ of
partial words and a ⋄-substitution σ over V such that σ(L′) = L, i.e., L is σ-
deﬁned by L′. For instance, take the set L′ of the words obtained by replacing in
the words of L some occurrences of a symbol a ∈V by ⋄, and the ⋄-substitution σ
over V that maps ⋄to {a}. More relevant ways of deﬁning a regular language, in
the sense of Deﬁnition 1, are presented in this section. We begin by characterizing
the essentially deﬁnable languages.
Assume that the regular language L ⊆V ∗is essentially σ-deﬁnable for some
⋄-substitution σ over V . Then σ(L′) = L for some appropriate language L′ such
that any word of L′ contains at least a hole. By Lemma 1, we get that there is a
regular language L′′ of partial words such that σ(L′′) = σ(L′) and each word of
L′′ contains exactly one hole. Now by Lemma 2 and its proof we get immediately
the following characterisation of σ-deﬁnable languages.
Theorem 1. Let L be a regular language of full words over V and σ a ⋄-
substitution over V . Then L is essentially σ-deﬁnable if and only if L = 
q∈Q Rq
(where Rq is given in the proof of Lemma 2).
⊓⊔
We also easily get the following decidability results.
Theorem 2. i) Given a regular language L over V and a ⋄-substitution σ
over V , it is decidable whether L is essentially σ-deﬁnable.
ii) Given a regular language L over V , one can algorithmically identify all ⋄-
substitutions σ for which L is essentially σ-deﬁnable.
Proof. By the previous results, testing whether L is essentially σ-deﬁnable is
equivalent to testing whether L and L′ = ∪q∈QRq are equal. Because the equality
of two regular languages is decidable, the ﬁrst statement follows. The second
statement follows by an exhaustive search in the (ﬁnite) set of all ⋄-substitutions
σ over V for those that essentially deﬁne L.
⊓⊔
The following consequence of Lemma 2 is worth noting, as it provides a canonical
non-trivial representation of regular languages.
Theorem 3. Given a regular language L ⊆V ∗and a ⋄-substitution σ over V ,
there exists a unique regular language L⋄of partial words that fulﬁls the following
three conditions: (i) L = σ(L⋄), (ii) for any language L1 with σ(L1) = L we have
{w | w ∈L1, |w|⋄≥1} < {w | w ∈L⋄, |w|⋄≥1}, and (iii) (L⋄∩V ∗) ∩σ({w |
w ∈L⋄, |w|⋄≥1}) = ∅.

156
J. Dassow, F. Manea, and R. Merca¸s
Proof. Using the sets deﬁned in Lemma 2, take L⋄= L′′∪(L\L′). The conclusion
follows easily.
⊓⊔
Motivated by this last result, we now turn to the descriptional complexity of
representing a regular language of full words by regular languages of partial
words. We are interested in the question whether there is a regular language
L ⊆V ∗, a regular language L′ ⊆(V ∪{⋄})∗, and a ⋄-substitution σ over V with
σ(L′) = L such that the minimal DFA accepting L′ has a (strictly) lower number
of states than the minimal DFA accepting L? In other words, are there cases
when one can describe in a more succinct way a regular language via a language
of partial words and a substitution that deﬁne it? Moreover, can we decide
algorithmically whether for a given regular language L there exist a language of
partial words and a substitution providing a more succinct description of L?
Let L be a regular language of full words over V . We denote by minDFA(L) the
number of states of the (complete) minimal DFA accepting L. Furthermore, let
minNFA(L) denote the number of states of a minimal NFA accepting L. Moreover,
for a regular language L let min⋄
DFA(L) denote the minimum number of states
of a (complete) DFA accepting a regular language L′ ⊆(V ∪{⋄})∗(where ⋄is
considered as an input symbol) for which there exists a ⋄-substitution σ over V
such that σ(L′) = L.
We have the following relation between the deﬁned measures.
Theorem 4. i) For every regular language L we have
minDFA(L) ≥min⋄
DFA(L) ≥minNFA(L).
ii) There exist regular languages L such that
minDFA(L) > min⋄
DFA(L) > minNFA(L).
By the previous result one can see that, for certain substitutions σ, minimal
DFAs accepting languages of partial words that σ-deﬁne a given full-words-
regular language can be seen as intermediate between the minimal DFA and the
minimal NFA accepting that language: they provide a succinct representation of
that language, while having a limited non-determinism.
In fact, one can show that the diﬀerences minDFA(L) −min⋄
DFA(L) and
min⋄
DFA(L) −minNFA(L) can be arbitrarily large; more precisely, we may have
an exponential blow-up with respect to both relations.
Theorem 5. Let n be a natural number, n ≥3. There exist regular languages
L and L′ such that min⋄
DFA(L) ≤n + 1 and minDFA(L) = 2n −2n−2 and
minNFA(L′) ≤2n + 1 and min⋄
DFA(L′) ≥2n −2n−2.
The following remark provides an algorithmic side of the results stated above.
Remark 1. Given a DFA accepting a regular language L we can construct algo-
rithmically a DFA with min⋄
DFA(L) states, accepting a regular language of partial
words L′, and a ⋄-substitution σ over alph(L), such that L is σ-deﬁned by L′.
By exhaustive search, we take a DFA M with at most minDFA(L) states, whose
transitions are labelled with letters from an alphabet included in alph(L)∪{⋄},

Connecting Partial Words and Regular Languages
157
and a ⋄-substitution σ over alph(L). We transform M into an NFA accept-
ing σ(L(M)) by replacing the transitions labelled with ⋄by |σ(⋄)| transitions
labelled with the letters of σ(⋄), respectively. Next, we construct the DFA equiv-
alent to this NFA, and check whether it accepts L or not (that is, σ(L(M)) = L).
From all the initial DFAs we keep those with minimal number of states, since
they provide the answer to our question. It is an open problem whether such a
DFA can be obtained by a polynomial time deterministic algorithm; however,
we conjecture that the problem is computationally hard.
We conclude by showing the hardness of a problem related to deﬁnability.
Theorem 6. Consider the problem P: “Given a DFA accepting a language L of
full words, a DFA accepting a language L′ of partial words, and a ⋄-substitution
σ over alph(L), decide whether σ(L′) ̸= L.” This problem is NP-hard.
Proof. In [6], the following problem was shown to be NP-complete:
P ′: “Given a list of partial words S = {w1, w2, . . . , wk} over the alphabet V with
|V | ≥2, each partial word having the same length L, decide whether there exists
a word v ∈V L such that v is not compatible with any of the partial words in S.”
We show here how problem P ′ can be reduced in polynomial time by a many-
one reduction to problem P. Indeed, take an instance of P ′: a list of partial words
S = {w1, w2, . . . , wk} over the alphabet V with |V | ≥2, each having the same
length L. We can construct in polynomial time a DFA M accepting exactly the
language of partial words {w1, w2, . . . , wk}. Also, we can construct in linear time
a DFA M ′ accepting the language of full words V L. It is clear that for L(M) and
the substitution σ, mapping the letters of V to themselves and ⋄to V , we have
σ(L(M)) ̸= V L (that is, the answer to the input M, M ′ and σ of problem P is
positive) if and only if the answer to the given instance of P ′ is also positive.
Since solving P ′ is not easier than solving P, we conclude our proof.
⊓⊔
Theorem 6 provides a simple way to show the following well known result.
Corollary 1. The problem of deciding whether a DFA M and an NFA M ′ accept
diﬀerent languages is NP-hard.
3
Languages of Partial Words
While the results of the last section study the possibility and eﬃciency of deﬁning
a regular language as the image of a (regular) language of partial words, it seems
interesting to us to take an opposite point of view, and investigate the languages
of partial words whose images through a substitution (or all possible substitu-
tions) are regular. Also, languages of partial words compatible with at least one
regular language (or only with regular languages) seem worth investigating.
The deﬁnitions of the ﬁrst three classes considered in this section follow the
main lines of the previous section. We basically look at languages of partial words
that can be transformed, via substitutions, into regular languages.

158
J. Dassow, F. Manea, and R. Merca¸s
Deﬁnition 2. Let L be a language of partial words over V .
1. We say that L is (∀σ)-regular if σ(L) is regular for all the ⋄-substitutions σ
over alphabets that contain V and do not contain ⋄.
2. We say that L is max-regular if σ(L) is regular, where σ is a ⋄-substitution
over V ′ with σ(⋄) = V ′, and V ′ = V if V ̸= ∅, and V ′ is a singleton with ⋄/∈V ′,
otherwise.
3. We say that L is (∃σ)-regular if there exists a ⋄-substitution σ over a non-
empty alphabet V ′, that contains V and does not contain ⋄, such that σ(L) is
regular.
The classes of all (∀σ)-regular, max-regular, and (∃σ)-regular languages are
denoted by REG(∀σ), REGmax, and, respectively, REG(∃σ).
We consider, in the following, two classes of languages of partial words that are
deﬁned starting from the concept of compatibility.
Deﬁnition 3. Let L be a language of partial words over V .
4. We say that L is (∃)-regular if exists a regular language L′ of full words such
that L ↑L′.
5. We say that L is (∀)-regular if every language L′ of full words such that L ↑L′
is regular.
The class of all the (∃)-regular languages is denoted REG(∃), while that of (∀)-
regular languages by REG(∀).
According to the deﬁnitions from [4], the (∃)-regular languages are those whose
restoration contains at least a regular language, while (∀)-regular languages are
those whose restoration contains only regular languages.
We start with the following result.
Theorem 7. For every non-empty alphabet V with ⋄/∈V there exist an unde-
cidable language L of partial words over V , such that:
i) σ(L) ∈REG for all substitutions σ over V , and σ′(L) /∈REG for the ⋄-
substitution σ′ with σ′(⋄) = V ∪{c}, where c /∈V .
ii) every language L′ ⊆V ∗of full words, which is compatible with L, is regular
and there is an undecidable language L′′ ⊆(V ′)∗, where V ′ strictly extends V ,
which is compatible with L.
Proof. Let L1 ⊆V ∗be an undecidable language (for instance, L1 can be con-
structed by the classical diagonalization technique L1 = {an | the nth Turing
machine in an enumeration of the Turing machines with binary input does not ac-
cept the binary representation of n}, where a ∈V ) and L = V ∗∪{⋄w | w ∈L1}.
Clearly, for any ⋄-substitution σ over V , we have σ(L) = V ∗. However, if we
take a letter c /∈V and the ⋄-substitution σ′ which replaces ⋄by V ∪{c} we
obtain an undecidable language σ′(L). This concludes the proof of (i). To show
(ii) we just have to note that the only language contained in V ∗compatible with
L is V ∗, and, if we take a letter c /∈V and replace ⋄by c (or, in other words,
if we see ⋄as the conventional symbol c), we obtain an undecidable language
compatible with L.
⊓⊔
We can now show a ﬁrst result regarding the classes previously deﬁned.

Connecting Partial Words and Regular Languages
159
Theorem 8. REG = REG(∀σ) ⊂REGmax.
Proof. It is rather clear that REG(∀σ) ⊆REGmax.
Since REG is closed to substitutions it follows that REG ⊆REG(∀σ).
It is also not hard to see that REG(∀σ) ⊆REG (given a language L in
REG(∀σ), one can take the special substitution that replaces ⋄with a symbol
that does not occur in alph(L) and obtain a regular language; therefore L is a
regular language if ⋄is seen as a normal symbol).
By Theorem 7, REGmax contains an undecidable language; indeed, given an
non-empty alphabet V , the language L deﬁned in its proof for V is in REGmax
according to (i). The strictness of the inclusion REG ⊊REGmax follows.
⊓⊔
The next result gives some insight on the structure of the class REGmax.
Theorem 9. Let L ∈REGmax be a language of partial words over V ̸= ∅
and σ the ⋄-substitution used in the deﬁnition of REGmax. Then there exists a
maximal language (with respect to set inclusion) L0 ∈REGmax of partial words
over V such that σ(L0) = σ(L). Moreover, given an automaton accepting L, an
automaton accepting L0 can be constructed.
It is also not hard to see that any language from REGmax whose words contain
only holes is regular.
The following relation also holds:
Theorem 10. REGmax ⊂REG(∃σ) ⊂REG(∃).
Proof. The non-strict inclusions REGmax ⊆REG(∃σ) ⊆REG(∃) are immedi-
ate. We show now that each of the previous inclusions is strict.
Take L = {(ab)n⋄b(ab)n | n ∈N}. Considering σ a ⋄-substitution as in the
deﬁnition of REGmax, we have σ(L) ∩{w | w ∈{a, b}∗, w contains bb} is the
language {(ab)nbb(ab)n} which is not regular. Thus, σ(L) is not regular, and
L is not in REGmax. However, it is clearly in REG(∃σ), as when we take the
substitution σ(a) = {a}, σ(b) = {b}, and σ(⋄) = {a}, we have σ(L) = {(ab)2n+1 |
n ∈N}, which is a regular language. This shows that REGmax ⊂REG(∃σ).
Now, take L = {(ab)n⋄b(ab)n | n ∈N} ∪{(ab)na⋄(ab)n | n ∈N}. This
language is not in REG(∃σ) by arguments similar to above, but it is in REG(∃)
as it is compatible with {(ab)2n+1 | n ∈N}.
⊓⊔
As already shown, all the languages in REG(∀) are in REG = REG(∀σ); how-
ever, not all the languages in REG are in REG(∀). The following statement
characterizes exactly the regular languages that are in REG(∀).
Theorem 11. Let L be a regular partial-words-language over V . Then L ∈
REG(∀) if and only if the set {w | |w|⋄≥1, w ∈L} is ﬁnite.
The previous result provides a simple procedure for deciding whether a regular
partial-words-language is in REG(∀) or not. We simply check (taking as input
a DFA for that language) whether there are ﬁnitely many words that contain
⋄or not. If yes, we accept the input and conﬁrm that the given language is in
REG(∀); otherwise, we reject the input.
Theorem 11 has also the following consequence.

160
J. Dassow, F. Manea, and R. Merca¸s
Theorem 12. REG(∀) ⊂REG.
In many previous works (surveyed in [2]), partial words were deﬁned by replacing
speciﬁc symbols of full words by ⋄, in a procedure that resembles the puncturing
of [4]. Similarly, in [5], partial words were deﬁned by applying the ﬁnite trans-
duction deﬁned by a deterministic generalised sequential machine (DGSM) to
full words, such that ⋄appears in the output word. Accordingly, we can deﬁne
a new class of partial-words-languages, REGgsm, using this automata-theoretic
approach. Let L be a language of partial words over V , with ⋄∈alph(L); L
is gsm-regular, and is in REGgsm, if there exists a DGSM M and a regular
language L′ such that L is obtained by applying the ﬁnite transduction deﬁned
by M to L′. It is not hard to show that REGgsm = REG \ REGfull.
By the Theorems 8,10,11, and 12 we get the following hierarchies:
REGfull ⊂REG(∀) ⊂REG = REG(∀σ) ⊂REGmax ⊂REG(∃σ) ⊂REG(∃)
REG \ REGfull = REGgsm ⊂REG = REG(∀σ)
Finally, the closure properties of the deﬁned classes are summarized in the follow-
ing table. Note that y (respectively, n) at the intersection of the row associated
with the class C and the column associated with the operation ◦means that C is
closed (respectively, not closed) under operation ◦. A special case is the closure
of REGmax under union and concatenation: in general this class is not closed
under these operations, but when we apply them to languages of REGmax over
the same alphabet we get a language from the same class.
Class
∪∩∩REG alph(L)∗\ L ∗
·
φ φ−1 σ
REG(∀)
y
y
y
n
n n
n
n
n
REG = REG(∀σ)
y
y
y
y
y
y
y
y
y
REGmax
n/y n
n
n
y n/y n
n
n
REG(∃σ)
n
n
n
n
y
n
n
n
n
REG(∃)
y
n
n
y
y
y
n
n
n
Acknowledgements. The work of Florin Manea is supported by the DFG
grant 582014. The work of Robert Merca¸s is supported by the Alexander von
Humboldt Foundation.
References
1. Berstel, J., Boasson, L.: Partial words and a theorem of Fine and Wilf. Theoretical
Computer Science 218, 135–141 (1999)
2. Blanchet-Sadri, F.: Algorithmic Combinatorics on Partial Words. Chapman &
Hall/CRC Press (2008)
3. Fischer, M.J., Paterson, M.S.: String matching and other products. In: Complexity
of Computation, Proceedings of SIAM-AMS, vol. 7, pp. 113–125 (1974)
4. Lischke, G.: Restoration of punctured languages and similarity of languages. Math-
ematical Logic Quarterly 52(1), 20–28 (2006)

Connecting Partial Words and Regular Languages
161
5. Manea, F., Merca¸s, R.: Freeness of partial words. Theoretical Computer Sci-
ence 389(1-2), 265–277 (2007)
6. Manea, F., Tiseanu, C.: Hard Counting Problems for Partial Words. In: Dediu,
A.-H., Fernau, H., Mart´ın-Vide, C. (eds.) LATA 2010. LNCS, vol. 6031, pp. 426–
438. Springer, Heidelberg (2010)
7. Rozenberg, G., Salomaa, A.: Handbook of Formal Languages. Springer-Verlag New
York, Inc., Secaucus (1997)

Randomness, Computation and Mathematics
Rod Downey
1 School of Mathematics, Statistics and Operations Research, Victoria University,
P. O. Box 600, Wellington, New Zealand
2 Isaac Newton Institute for Mathematical Sciences, 20 Clarkson Road,
Cambridge CB3 0EH, United Kingdom
rod.downey@vuw.ac.nz
Abstract. This article examines some of the recent advances in our
understanding of algorithmic randomness. It also discusses connections
with various areas of mathematics, computer science and other areas of
science. Some questions and speculations will be discussed.
1
Introduction
The Copenhagen interpretation of quantum physics suggests to us that random-
ness is essential to our understanding of the universe. Mathematics has devel-
oped many tools to utilize randomness in the development of algorithms and in
combinatorial (and other) techniques. For instance, these include Markov Chain
Monte Carlo and the metropolis algorithms, methods central to modern science,
the probabilistic method is central to combinatorics. Computer science has its
own love aﬀair with randomness such as its uses in cryptography, fast algorithms
and proof techniques.
Nonetheless, it is not clear what each community means by “randomness”.
Moreover, even when we agree to try one of the formalizations of the notion
of randomness based on computation there is also no clear understanding on
how this should be interpreted and the extent to which the applications in the
disparate arenas can be reconciled.
In this article I will look at the long term programme of understanding the
meaning of randomness via an important part of Turing’s legacy, the theory of al-
gorithmic computation: algorithmic randomness. The last decade has seen some
quite dramatic advances in our understanding of algorithmic randomness. In
particular, we have seen signiﬁcant clariﬁcation as to the mathematical relation-
ship between algorithmic computational power of inﬁnite random sources and
algorithmic randomness. Much of this material has been reported in the short
surveys Downey [27], Nies [53] and long surveys [26,30] and long monographs
Downey and Hirschfeldt [29] and Nies [52]. Also the book edited by Hector Zenil
[78] has a lot of discussion of randomness of varying levels of technicality, many
aimed at the general audience.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 162–181, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Randomness, Computation and Mathematics
163
To my knowledge, Turing himself though that randomness was a physical
phenomenon, and certainly recognized the noncomputable nature of generating
random strings. For example, from Turing [71], we have the following quote1:
“ An interesting variant on the idea of a digital computer is a ”digital
computer with a random element.” These have instructions involving the
throwing of a die or some equivalent electronic process; one such instruc-
tion might for instance be, ”Throw the die and put the-resulting number
into store 1000.” Sometimes such a machine is described as having free
will (though I would not use this phrase myself).”
John von Neumann (e.g., [75]) also recognized the noncomputable nature of
generating randomness, and both seem to believe that physical procedures would
be necessary. Von Neumann’s quote is famous:
“Any one who considers arithmetical methods of producing random dig-
its is, of course, in a state of sin.”
Arguably this idea well predated any notion of computation, but the germ of
this can be seen in the following quotation of Joseph Bertrand [14] in 1889.
“How dare we speak of the laws of chance?
Is not chance the antithesis of all law?”
There has been a developing body of work seeking to understand not just the
theory of randomness but how it arizes in mathematics.
For example, we have also seen an initiative (whose roots go back to work
of Demuth [25]) towards using these ideas in the understanding of almost ev-
erywhere behaviour and diﬀerentiation in analysis (such as Brattka, Miller, Nies
[15]). Also halting probabilities are natural and turn up in places apparently
removed from such considerations. For instance they turned up naturally in
the study of subshifts of ﬁnite type (Hochman and Meyerovitch [39], Simpson
[66,68]), fractals (Braverman and Yampolsky [16,17]) (as we see later), we see
randomness giving insight into Ergodic theory such as Avigad [6], Bienvenu et
al. [13] and Franklin et al. [33].
Randomness has long been intertwined with computer science, (although some
regard this as a matter of debate such as Gregorieﬀand Ferbus [38]) being cen-
tral to things like polynomial identity testing, proofs like all known proofs of
Toda’s Theorem and the PCP theorem, as well as cryptographic security. A
nice programme of Allender and his co-workers (e.g., [4,3]) suggests that per-
haps complexity classes can be understood by understanding how they relate to
the collections of strings which are algorithmically random according to various
measures.
In this article I will try to give a brief outline of theses topics, and make some
tentative suggestions for lines of investigation.
1 I am indebted to Veronica Becher for discussions of Turing’s and von Neumann’s
thoughts on randomness.

164
R. Downey
My assumption of the reader of this paper is that they are not well versed
in the theory of algorithmic randomness. I will assume that they have a basic
training in computability theory to the level of a ﬁrst course in logic. If you are
at all excited by what you read I urge you to look at the surveys or the books
suggested above for fuller accounts.
2
Basics
I will refer to members of {0, 1}∗= 2<ω as strings, and inﬁnite binary sequences
(members of 2ω, Cantor space) as reals. 2ω is endowed with the tree topology,
which has as basic clopen sets
[σ] := {X ∈2ω : σ ≺X},
where σ ∈2<ω. The uniform or Lebesgue measure on 2ω is induced by giving
each basic open set [σ] measure μ([σ]) := 2−|σ|.
We identify an element X of 2ω with the set {n : X(n) = 1}. The space 2ω is
measure-theoretically identical with the real interval [0, 1], although the two are
not homeomorphic as topological spaces, so we can also think of elements of 2ω
as elements of [0, 1]. We shall let X ↾n denote the ﬁrst n bits of X.
The earliest work trying to give meaning to the randomness of an individual
source was that of von Mises who argued as follows. The real should certainly
have to obey the frequency laws like the law of large numbers. Thus
lim
n→∞
{m | m ≤n ∧X(m) = 1}
n
= 1
2.
This property is called normality and was studied by Borel and others. In fact,
any random real clearly should be absolutely normal, normal to any basis. It is
easy to construct such numbers computably (an interest of Turing discussed in
Veronica Becher’s article in this volume [8]). In fact any polynomial time random
real (in any reasonable sense) is absolutely normal.
von Mises’ idea was to consider any possible selection of a subsequence and
ask that it was normal: Let f : ω →ω be an increasing injection, a selection
function. Then a random X should satisfy the following.
lim
n→∞
{m | m ≤n ∧X(f(m)) = 1}
n
= 1
2.
von Mises work predated the work in the 30’s,culminating in the classic paper of
Turing [70], clarifying the notion of computable function. Thus von Mises had no
canonical choice for “acceptable selection rules”. However, Wald [76,77] showed
that for any countable collection of selection functions, there is a sequence that is
random in the sense of von Mises. Church [21] proposed restricting f to (partial)
computable increasing functions. This gives rise to notions now called computable
stochasticity, and partial computable stochasticity.
This was how matters stood until the work of Ville. [73] In the following,
S(α, n) is the number of 1’s in the ﬁrst n bits of α and similarly Sf for the
selected places.

Randomness, Computation and Mathematics
165
Theorem 1 (Ville’s Theorem [73]). Let E be any countable collection of
selection functions. Then there is a sequence α = α0α1 . . . such that the following
hold.
1. limn
S(α,n)
n
= 1
2.
2. For every f ∈E that selects inﬁnitely many bits of α, we have limn
Sf(α,n)
n
=
1
2.
3. For all n, we have S(α,n)
n
≤1
2.
The killer is item 3 which says that there are never situations with more 1’s than
0’s in the ﬁrst n bits of α. That is plainly non-random. Ville suggested adding
a further statistical law, the law of iterated logarithms, to von Mises’ deﬁnition.
However, we might well ask “How we can be sure that adding this law would
be enough?”. Why should we expect there not to be a further result like Ville’s
(which there is, see [29]) exhibiting a sequence that satisﬁes both the law of large
numbers and the law of iterated logarithms, yet clearly fails to have some other
basic property that we would naturally associate with randomness?
We could add more and more statistical laws to our collection of desiderata
for random sequences, but there is no reason to believe we would ever be done,
and we certainly do not want a deﬁnition of randomness that changes with
time, if we can avoid it. Martin-L¨of’s fundamental idea in [55] was to deﬁne an
abstract notion of a performable statistical test for randomness, and require that
a random sequence pass all such tests. He did so by eﬀectivizing the notion of a
set of measure 0. The way to think about Martin-L¨of’s deﬁnition below is that
as we eﬀectively shrink the measure of the open sets we regard as “tests”, we
are specifying reals satisfying them more and more.
In the below a Σ0
1 class is a computably enumerable collection {[σ] | σ ∈W}
for some c.e. set W of strings. Alternatively think of this as a c.e. set of intervals
in the interval [0, 1].
Deﬁnition 1 (Martin-L¨of [55])
1. A Martin-L¨of test is a sequence {Un}n∈ω of uniformly Σ0
1 classes such that
μ(Un) ≤2−n for all n.
2. A class C ⊂2ω is Martin-L¨of null if there is a Martin-L¨of test {Un}n∈ω
such that C ⊆
n Un.
3. A set A ∈2ω is Martin-L¨of random if {A} is not Martin-L¨of null.
Now there are three main views of algorithmic randomness. The above is called
the measure-theoretical paradigm.
We brieﬂy discuss the two other main paradigms in algorithmic randomness
as they are crucial to our story. The ﬁrst is the computational paradigm: Ran-
dom sequences are those whose initial segments are all hard to describe, or,
equivalently, hard to compress.
We think of Turing machines U with input τ giving a string σ. We regard τ as
a description of σ and the shortest such is regarded as the intrinsic information in
σ. The plain U-Kolmogorov complexity CU(σ) of σ is the length of the shortest

166
R. Downey
τ with U(τ) = σ. Turing machines can be enumerated U0, U1, . . . and hence we
can remove the machine dependence by deﬁning a new (universal) machine
U(0e1τ) = Ue(τ),
so that we can deﬁne for this machine M, C(σ) = CM(σ) and for all e, C(σ) ≤
CUe(σ) + e + 1. We shall use the notation ≤+ for constants and shall write
C(σ) ≤+ CUe(σ).
A simple counting argument due to Kolmogorov [44] shows that as C(σ) ≤+
|σ| (using the identity machine), there must be strings of length n with C(σ) ≥n.
We call such strings C-random.
We would like to deﬁne a real to be random by saying for all n, C(α ↾n) ≥+
n. Unfortunately, there are no such random reals due to a phenomenon called
complexity oscillations, which (in a quantative way) say that in very long strings
σ there must segments with C(σ ↾n) < n. This oscillation really due to the
fact that on input τ, we don’t just get the bits of τ as information but the
length of τ as well. Thus we are losing the intentional meaning that the bits of
τ are processed by U to produce σ. To get around this ﬁrst Levin [48,49] and
later Chaitin [19] suggested using preﬁx-free machines to capture this intentional
meaning.
Preﬁx free machines work like telephone numbers. If U(τ) ↓(i.e., halts) then
for all ˆτ comparable with τ, U(ˆτ) ↑.
Already we see a theme that there is not one but perhaps many notions of
computational compressibility of relevance to understanding randomness. In the
case of preﬁx free complexity, in some sense we know we are on the correct track,
due to the following theorem which can be interpreted as saying (for discrete
spaces) that Occam’s razor and Baye’s theorem give the same result (in that the
shortest description is essentially the probability that the string is output).
Theorem 2 (Coding Theorem-Levin [48,49], Chaitin [19]). For all σ,
K(σ) =+ −log(Q(σ)) where Q(σ) is μ({τ | U(τ) = σ}).
Using this notion, and noticing that the universal machine above would be preﬁx-
free if all the Ue were preﬁx free, we can deﬁne the preﬁx-free Kolmogorov
complexity K(σ).
Deﬁnition 2 (Levin [49], Chaitin [19]). A set A is 1-random if K(A ↾n) ≥+
n).
Theorem 3 (Schnorr). A real A is Martin-L¨of random iﬀit is 1-random.
Hence the two paradigms converge on a common intuition. It is easy to see that
since there are only countably many machines, a real is random with probability
1. The classic example of a 1-random real is Chaitin’s halting probability (for a
universal preﬁx-free machine U):
Ω =

{σ|U(σ)↓}
2−|σ|,

Randomness, Computation and Mathematics
167
the measure of the domain of U (which has meaning as the domain of U is a
preﬁx free set of strings).
It would seem that the deﬁnition of Ω is thoroughly machine independent
but in the same spirit as Myhill’s theorem, we can deﬁne a reducibility we call
Solovay reducibility, and show that there is only one Ω in this sense. To wit,
we observe that Ω = lims Ωs where Ωs = 
{σ|U(σ)[s]↓} 2−|σ|, (i.e., s steps of
computation), and hence Ω is what is called a left c.e.-real. We can deﬁne a
notion of reducibility on left c.e.-reals α ≤S β iﬀthere is a partial computable
function f and a constant c, such that for all rationals q (we assume all reals
are nonrational for uniformity), if q < α then f(q) ↓and |α −q| ≤c|β −f(q)|.
The culmination of a series of papers is the Kuˇcera-Slaman theorem which states
that there is really only one left-c.e. random real.
Theorem 4 (Kuˇcera-Slaman Theorem [46]). α is 1-random and left-c.e.
iﬀfor all left c.e.-reals β, β ≤S α.
The ﬁnal randomness paradigm is the one based on prediction. The unpredictabil-
ity paradigm is that we should not be able to predict the next bit of a random
sequences even if we know all preceding bits, in the same way that a coin toss is
unpredictable even given the results of previous coin tosses.
Deﬁnition 3 (Levy [50]). A function d : 2<ω →R≥0 is a martingale2 if for
all σ,
d(σ) = d(σ0) + d(σ1)
2
.
d is a supermartingale if for all σ,
d(σ) ≥d(σ0) + d(σ1)
2
.
A (super)martingale d succeeds on a set A if lim supn d(A ↾n) = ∞. The
collection of all sets on which d succeeds is called the success set of d, and is
denoted by S[d].
The idea is that a martingale d(σ) represents the capital that we have after
betting on the bits of σ while following a particular betting strategy (d(λ) being
our starting capital). The martingale condition d(σ) =
d(σ0)+d(σ1)
2
is a fair-
ness condition, ensuring that the expected value of our capital after a bet is
equal to our capital before the bet. Ville [73] proved that the success sets of
(super)martingales correspond precisely to the sets of measure 0.
Now again we shall need a notion of eﬀective betting strategy. We shall say
that the martingale is computable if d is a computable function (with range Q,
without loss of generality), and we shall say that d is c.e. iﬀd is given by an
eﬀective approximation d(σ) = lims ds(σ) where ds+1(σ) ≥ds(σ). This means
2 A more complex notion of martingale is used in probability theory. We shall discuss
this notion, and the connection between it and ours, in [29], where it is discussed
how computable martingale processes can be used to characterize 1-random reals.

168
R. Downey
that we are allowed to bet more as we become more conﬁdent of the fact that σ
is the more likely outcome in the betting, as time goes on.
Theorem 5 (Schnorr [64,65]). A set is 1-random iff no c.e. (super)martingale
succeeds on it.
These all seem basic theorems from long ago, but there remain a lot of things
we don’t understand even around these basic theorems. For example, here are
three questions around these theorems.
First, it seems strange that to deﬁne randomness we use c.e. martingales and
not computable ones. Based on this possible defect, Schnorr deﬁned two other
notions of randomness, computable randomness (where the martingales are all
computable) and Schnorr randomness (where we use the Martin-L¨of deﬁnition
but insist that μ(Uk) = 2−k rather than ≤2−k so we know precisely the [σ] in
Uk uniformly) meaning in each case that the randomness notion is a computable
rather and computably enumerable one. We know that Martin-L¨of randomness
implies computable randomness which implies Schnorr randomness, and none of
these implications are reversible. The ﬁrst question is:“Can we use some kind
of computable randomness to deﬁne 1-randomness?”. The suggested method to
do this is to use a computable but nonmonotonic notion of randomness, where
we have a betting strategy which bets on bits one at a time, but instead of
being increasing can bet in some arbitrary order, and may not bet on all bits.
The order is determined by what has happened so far. This gives a notion called
Kolmogorov-Loveland (or nonmonotonic) randomness and the following question
has been open for quite a while.
Question 1 (Muchnik, Semenov, and Uspensky [59]). Is every nonmonotonically
random sequence 1-random?
A discussion of known results can be found in [29].
The second and third questions actually stem from the proof where we show
that there is a translation of Martin-L¨of tests into c.e. supermartingales. There,
we start with a uniformly c.e. sequence R0, R1, . . . of preﬁx-free generators for a
Martin-L¨of test. We build a c.e. supermartingale d that bets evenly on σ0 and
σ1 until it ﬁnds that, say, σ0 ∈Rn, at which point it starts to favor σ0, to an
extent determined by n. If later d ﬁnds that σ1 ∈Rm, then what it does is
determined by the relationship between m and n. If m < n then d still favors
σ0, though to a lesser extent than before. If m = n then d again bets evenly on
σ0 and σ1. If m > n then d switches allegiance and favors σ1. This can happen
several times, as we ﬁnd more Ri to which σ0 or σ1 belong.
The computable enumerability of d is essential in the above. A computable
supermartingale (which we have seen we may assume is rational-valued with-
out loss of generality) has to decide which side to favor, if any, immediately.
Hitchcock has asked whether an intermediate notion, where we allow our super-
martingale to be c.e. but do not allow it to switch allegiances in the way described
above, is still powerful enough to capture 1-randomness. The purest version of
this question was suggested by Kastermans. A Kastergale is a pair consisting

Randomness, Computation and Mathematics
169
of a partial computable function g : 2<ω →{0, 1} and a c.e. supermartingale
d such that g(σ) ↓= i iff ∃s (ds(σi) > ds(σ(1 −i))) iff d(σi) > d(σ(1 −i)). A
set is Kastermans random if no Kastergale succeeds on it. A Hitchgale is the
same as a Kastergale, except that in addition the proportion ds(τj)
ds(τ) (where we
regard 0
0 as being 0) is a Σ0
1 function, so that if we ever decide to bet some
percentage of our capital on τj, then we are committed to betting at least that
percentage from that point on, even if our total capital on τ increases later. A
set is Hitchcock random if no Hitchgale succeeds on it. It is unknown if these
notions diﬀer from 1-randomness and the import is that is any bias allowed in
the deﬁnition of 1-randomness?
The message also is that there are many kinds of randomness and they each
give insight. Variations of the notion of randomness include Kurtz or weak ran-
domness, Demuth randomness, ﬁnite randomness, resource bounded randomness
(for analyzing complexity classes), etc. For instance, weak randomness asks that
X belongs to all Σ0
1 classes of measure 1. We refer mostly to [29,52] for more.
There are similarly many kinds of Kolmogorov complexities such as process and
monotone complexities (which solve the “C-” problem by asking that the action
of machines be continuous rather than preﬁx free). To wit, if U(σ) ↓and U(ν) ↓,
and σ ⪯ν, then U(σ) ⪯U(ν). There are various interpretations of this idea,
such as U being a multifunction (so that U is really a c.e. collection of pairs
of strings) called Km, monotone complexity, but for all of them, an analog of
Schnorr’s Theorem holds so that α is 1-random iﬀK(α ↾n) ≥+ n for all n. In
most cases, K(α ↾n) =+ n since the identity machine is monotone.
These ideas and associated probability measures have seen applications into
geometric measure theory such as Jan Reimann’s new proof of (classical) Frost-
mann’s Lemma using methods from eﬀective randomness ([62])3. These contin-
uous Kolmogorov complexities tend to be less well understood. Work of Adam
Day (see [29]) gives new methods for building machines. One hallmark is Day’s
remarkable improvement [23] of G´ac’s Theorem [35] that the Coding Theorem
fails for continuous spaces.
For the remainder of this paper we shall need some further (stronger) no-
tions of randomness. We can strengthen the idea of randomness by giving the
computational devices more compression power via oracles. Then if ∅(n) de-
notes the n-th iterate of the halting problem, we say that X is n + 1-random iﬀ
K∅(n)(X ↾n) ≥+ n for all n.
It is a surprising fact that for all n, n-randomness can be deﬁned purely in
terms of K with no oracle. This follows by the next result.
Theorem 6 (Bienvenu, Muchnik, Shen, and Vereschagin [12]). K∅′(σ) =
lim supm K(σ | m) ± O(1).
Hence A would be 2-random iﬀfor all n, lim supm K(A ↾n | m) ≥+ n. In some
cases, we know of natural deﬁnitions of n-randomness. For instance, we have
3 As we soon see, Simpson ([68]) has similar applications of eﬀective measure to derive
classical results in Hausdorﬀdimension.

170
R. Downey
seen that it is impossible for a real to have C(X ↾n) ≥+ n for all n, but Martin-
L¨of showed in his original paper that there are reals X with C(X ↾n) ≥+ n
for inﬁnitely many n. Joe Miller and later Nies, Stephan and Terwijn showed
that such randoms are precisely the 2-randoms, and later Miller showed that
the 2-randoms are exactly those that achieve maximal preﬁx-free complexity
(n + K(n)) inﬁnitely often. Also Becher and Gregorieﬀ[9] have a kind of index
set characterizations of higher notions of randomness. I know of no other natural
deﬁnitions, such as for the 3-randoms.
Another subtext in these investigations is to dispense with Kolmogorov
complexity altogether. The idea is to redo algorithmic randomness using total
machines.
Deﬁnition 4 (Bienvenu and Merkle [11]). A computable function f is a
Solovay function if 
n 2−f(n) < ∞and lim infn f(n) −K(n) < ∞(in other
words, there is a c such that f(n) ≤K(n) + c for inﬁnitely many n).
Solovay functions were ﬁrst constructed by Solovay, but any reasonable time
bounded version of preﬁx-free Kolmogorov complexity give rise to one. (An ob-
servation of H¨olzl, Kr¨aling, and Merkle [40].) Building on earlier work of G´acs,
and of Miller and Yu, recently Merkle, Miller and Nies have proven that a set A
is 1-random iff C(A ↾n) ≥n −g(n) −O(1) for any Solovay function g. In fact
by themselves, Solovay functions characterize 1-randomness.
Theorem 7 (Bienvenu and Downey [10]). Let f be a computable function.
The following are equivalent.
1. f is a Solovay function.
2. 
n 2−f(n) is a 1-random real.
Further extensions on this theme, generalizations and relationships with ran-
domness have been found. See [29], and the later material on K-triviality.
We have left out the vast amount of work on eﬀective dimensions. In the
same way as we eﬀectivize measure, we can eﬀectivize fractional measure. The-
orems include characterizations due to Mayordomo [56] that eﬀective Hausdorﬀ
dimension of X is equal to lim infn→∞
K(X↾n)
n
and the characterization of ef-
fective packing dimension by Athreya, Hitchcock, Lutz, and Mayordomo [5] as
lim supn→∞
K(X↾n)
n
(C can replace K in both cases). These concepts have been
shown to have fascinating interactions with computability, such as characteriz-
ing degree classes, and as we discuss later, have been used to give new proofs of
classical theorems. I don’t have space to discuss further, but see [29].
3
Randomness and Classical Computability
Interactions of measure, randomness and computability go way back to the early
years of the subject such as the paper de Leeuw et al. [24] where, amongst other
things, it is proven that a set X is enumerable from a set of oracles of positive

Randomness, Computation and Mathematics
171
measure iﬀX is computably enumerable. As a consequence, we get a result later
rediscovered by Sacks that if a real X is computable from a collection of sources
of positive measure, then X must be computable. Nevertheless, a classical result
is the following saying that random sources can have computational power.
Theorem 8 (Ku´cera [45], G´acs [36]). For every set X, there is a random
Y such that X ≤wtt Y , where ≤wtt is Turing reducibility with use bounded by a
computable function.
The above argues that 1-random reals are not random enough to correlate to
the thesis that random reals should have no computational power. This intuition
was clariﬁed by Stephan who proved the following4.
Theorem 9 (Stephan [69]). Suppose a random real is powerful enough to
compute a {0, 1}-valued function f such that for all n, f(n) ̸= ϕn(n) (i.e., a PA
degree). Then ∅′ ≤T X, so that it is a “false random.”
Thus we can wash away lots of computational power by raising the level of
randomness. For example, it can be shown that X is weakly 2-random (i.e., in
every Σ0
2 class of measure 1) iﬀX is 1-random and its degree forms a minimal pair
with ∅′. Hence no (weakly) 2-random real can bound a PA degree. A remarkable
theorem here is the following, demonstrating a deep relationship between PA
degrees and randomness.
Theorem 10 (Barmpalias, Lewis, and Ng [7]).
Every PA degree is the
join of two 1-random degrees.
There has been a huge amount of work concerning the interplay between things
like PA degrees and weakenings of the notion of ﬁxed point free functions (f(n) ̸=
ϕn(n)). For example, you can show that this ability corresponds to traceing, and
the speed of growth of the initial segment complexity of a real. As an illustration,
A is h-complex if C(A ↾n) ≥h(n) for all n. A is autocomplex if there is an A-
computable order h such that A is h-complex.
Theorem 11 (Kjos-Hanssen, Merkle, and Stephan [41]). A set is auto-
complex iff it is of DNC degree.
Another illustration of the interplay of notions of randomness and Turing degrees
is the theorem.
Theorem 12 (Nies, Stephan, and Terwijn [54]). If a nonhigh set (i.e.,
A′ ̸≥T ∅(2).) is Schnorr random then it is 1-random.
In fact it is possible to show that within the high degrees the separations be-
tween computable, Schnorr, and Martin-L¨of randomness always occur. In the
4 Interpreted by Hirschfeldt as saying that there are two methods of passing a stupidity
test. One is the be the genuine article. The other is to be like Ω is be so smart that
you know what a stupid person would say.

172
R. Downey
hyperimmune-free (or computably dominated a, meaning that for every f ≤a
there is a computable g with f(n) < g(n) for all n) degrees, weak randomness
coincides with all of these as well as weak 2-randomness.
There is a delicate interweaving of randomness notions and properties of Tur-
ing degrees. For example, Kurtz and Kautz long ago showed us that every 2-
random degree is hyperimmune (i.e., ∃f ≤a(∀g)(g computable →∃∞n(f(n) >
g(n))).) Moreover the “almost all” theory of degrees is decidable by another old
result of Stillwell. We refer to [29] for a lot more on this, and similar things
concerning eﬀective dimensions.
I cannot leave this part of the survey without mentioning the long sequences
of results about lowness notions. For any reasonable property P we say that X
is low for P if P X = P. For example, being low for the Turing jump means that
X′ ≡T ∅′. A set A is low for 1-randomness iﬀA does not make any 1-randoms
nonrandom. You can also have a notion of lowness for tests, meaning that every
(eﬀective nullset)A can be covered by an eﬀective nullset. In all cases the lowness
notion for randomness and for tests have turned out to coincide with a single
recent exception of “diﬀerence randomness” found by Diamondstone and Fanklin
(paper in preparation).
Now it is not altogether clear that noncomputable sets low for 1-randomness
should exist. But they do and form a remarkable class called the K-trivials. That
is, they coincide with the class of reals A such that forall n, K(A ↾n) ≤+ K(n).
(In fact Bienvenu and Downey [10] showed that it is enough to put a Solovay
function on the right side.) Many properties of this class have been shown, and
particularly Andre Nies proved the deep result that A is K-trivial iﬀA is low
for Martin-L¨of randomness iﬀA is useless as a compressor, KA =+ K. (Nies
[51]). A good account of this material can be found in Nies [52,53], but things
are constantly changing, with maybe 17 characterizations of this class. We also
refer to [29] for the situation up to mid-2010.
Other randomness notions give quite diﬀerent lowness notions. For example,
there are no noncomputable reals which are low for C and none low for com-
putable randomness. On the other hand, lowness for Schnorr and Kurtz random-
ness give interesting subclasses of the hyperimmune-free degrees characterized
by notions of being computably dominated, and ﬁxed point free functions in
the case of Kurtz. Work here is still ongoing and many results proven, but the
pattern remains very opaque. Even for a ﬁxed real like Ω (i.e., when does ΩX
remain random?) results are quite interesting. In the case of Ω, X is low for Ω
and X is computable from the halting problem, then X is K-trivial, whereas if
X is random, then it is 2-random. (Results of Joe Miller, see [29].)
These classes again relate to various reﬁnements of the jump and to “tra-
ceing” which means giving an eﬀective collection of possibilities for (partial)
functions computable from the degree at hand. Again this has taken on a life
of its own, and such methods have been used to solve questions from classical
computability theory. For instance, Downey and Greenberg’s [28] used “strong
jump traceability” to solve a longstanding question of Jockusch and Shore on

Randomness, Computation and Mathematics
173
pseudo-jump operators and cone avoidance. Strongly jump traceable reals have
their own agenda and form a fascinating class, cf., e.g., [20].
The ﬁnal material for this section is the deep results of Reimann and Slaman
who were looking at the question (ﬁrst discussed by Levin): given X ̸≡T ∅, is
there a measure relative to which X is random?
Clearly we can concentrate a measure on a real, but assuming that we are not
allowed to do this the answer is still that every noncomputable real can be made
random. On the other hand, if we ask that there are no atoms in the measure,
the situation is very diﬀerent. We get a class of never continuously n-random
reals. For each n this class is countable, but the proof of this requires magical
things like big fragments of Borel determinacy, provably. The reader should look
at Reimann and Slaman [63].
4
(Some) Applications
4.1
Left Out
I apologize to the workers who are using approximations to C like commercial
compression packages to apply Kolmogorov complexity to measure things like
common information5. As an illustration, I refer to the work of Vitanyi and his
co-workers who do phylogenetic analysis (in biology and music evolution, etc)
by replacing metrics like maximum parsimony by common information deﬁned
via Kolmogorov complexity. (Cf., e.g., [22,72].)
4.2
Ergodic Theory
A very important part of classical mathematics is concerned with recurrent ac-
tions of some process. For example, A d-dimensional shift of ﬁnite type is a
collection of colourings of Zd deﬁned by local rules and a shift action (basically
saying certain colourings are illegal). Its (Shannon) entropy is the asymptotic
growth in the number of legal colourings. More formally, consider G = (Nd, +)
or (Zd, +), and A a ﬁnite set of symbols. We give A the discrete topology and
AG the product topology. The shift action of G on AG is
(Sgx)(h) = x(h + g), for g, h ∈G ∧x ∈AG.
A subshift is X ⊆AG such that x ∈X implies Sgx ∈X (i.e., shift invariant).
The classical area of Symbolic Dynamics studies subshifts usually of “ﬁnite type.”
Such subshifts are well known to be connected to number theory, Ramsey theory
etc.
The following is a recent theorem showing that Ω occurs naturally in this
setting.
5 The earliest calssical application of Kolgorogov compexity I know of is an old one
by Schnorr and Fuchs [34] sharpening aspects of Markov Chain Monte Carlo.

174
R. Downey
Theorem 13 (Hochman and Meyerovitch, [39]). The values of entropies
of subshifts of ﬁnite type over Zd for d ≥2 are exactly the complements of halting
probabilities.
I remark that in the same way that Reimann proved Frostman’s Lemma us-
ing eﬀective methods, Simpson [68] has proven classical results using our ef-
fective methods. Simpson studies topological entropy for subshifts X and the
relationship with Hausdorﬀdimension. If X ⊂AG use the standard metric
ρ(x, y) = 2−|Fn| where n is as large as possible with x ↾Fn = u ↾Fn and
Fn = {−n, . . ., n}d. In discussions with co-workers, Simpson proved that the
classical dimension equals the entropy (generalizing a diﬃcult result of Fursten-
burg 1967) using eﬀective methods, which were much simpler.
Theorem 14 (Simpson [68]). If X is a subshift (closed and shift invariant),
then the eﬀective Hausdorﬀdimension of X is equal to the classical Hausdorﬀ
dimension of X is equal to the entropy, moreover there are calculable relation-
ships between the eﬀective and classical quantities. (See Simpson’s home page for
his recent talks and more precise details.)
Other types of Ergodic behaviour have been studied.
The general setting is the following. Let (X, μ) be a probability space, and
T : X →X measure preserving so that for measurable A ⊆X, μ(T −1A) = μ(A).
Such a map is invariant if T −1A = A except on a measure 0 set. Finally the map
is ergodic if every T -invariant subset is either null or co-null. The shift operator
above (say, on Cantor space so that T (a0a1 . . .) = a1a2 . . .) is an ergodic action
with the Bernoulli product measure.
A classic theorem of Poincar´e is that if T is ergodic on (X, μ), then for all
E ⊆X of positive measure and almost all x ∈X, T n(x) ∈E for inﬁnitely many
n. For a set of measurable subsets E of X, we call an x a Poincar´e point if
T n(x) ∈Q for all Q ∈E of positive measure. Long ago Kuˇcera [45] showed that
X is 1-random iﬀX is a Poincar´e point for the shift operator with respect to
the collection of eﬀectively closed subsets of 2ω.
Bienvenu et al. proved the following extension of this result.
Theorem 15 (Bienvenu, et al. [13]). Let T be computable ergodic on a com-
putable probability space (X, μ). Then x ∈X is 1-random iﬀx is a Poincar´e
point for all eﬀectively closed subsets of X.
We remark that the notion of a computable probability space is natural and along
the lines of the Pour-El Richards [61] version of computable metric space. There
are again a lot of results here. Franklin et al. [33] looked at the classic Birkhoﬀ
ergodic theorem for f ∈L1(X) (namely limn→∞1
n

i<n f(T i(x)) =  fdμ.)
and showed that 1-random points satisfy Birkhoﬀ’s ergodic theorem. For other
interpretations and stronger hypotheses (that the measure of the closed sets
is computable), G´acs, Hoyrup and Rojas [37], showed that the Birkhoﬀpoints
are precisely the Schnorr randoms. This is currently an area of intense activity,
and many of the classical ergodic theorems remain to be studied. For example,

Randomness, Computation and Mathematics
175
Furstenburg’s one with its applications to arithmetical progressions would seem
a natural candidate.
This is also related to metamathematical studies, and here we refer the reader
to Avigad [6].
Another interesting application of the ideas from algorithmic randomness is
to the area of Julia sets. Recall that this is described by z →z2 + αz, where
α = e2πiθ. Braverman and Yampolsky [16,17] showed that in general even for
computable θ, Julia sets can coincide with complements of Ω.
4.3
Diﬀerentiability Is the Same as Randomness
In his blog, Terry Tao remarks that Ergodic theorems and classical theorems
from analysis such as the Lebesgue theorem that functions of bounded variation
are diﬀerentiable almost everywhere are closely related. In Bishop’s book, they
have almost the same proof. It is thus not surprising that we see such theorems
giving rise to randomness notions. This is an idea going way back to the work of
Oswald Demuth, a constructivist from Prague. It is being actively pursued by
Brattka, Nies, Miller and others.
Recall that the Denjoy upper and lower derivatives for a function f are deﬁned
as follows.
Df(x) = lim sup
h→0
f(x) −f(x + h)
h
and Df(x) = lim inf
h→0
f(x) −f(x + h)
h
.
The Denjoy derivate exists iﬀboth of the above quantities exist and are ﬁnite.
The idea in this is that slopes like those in the deﬁnitions can be considered to
be martingales.
Using this for one direction, various notions of randomness can be character-
ized by (i) varying the strength of the notion of computable real valued function
(e.g., Markov computable, type 2 computable etc) (ii) varying the theorem.
For an illustration, we have the following.
Theorem 16 (Brattka, Miller and Nies [15]). z is computably random iﬀ
every computable (in the type two sense) increasing function f[0, 1] →R is
diﬀerentiable at x.
There are similar results relating 1-randomness of x to its diﬀerentiability of
functions of bounded variation. There is still a lot of activity here, and class
like Lipschitz functions and many other classical almost everywhere behaviour
in analysis are found to correlate to various notions of randomness. The paper
[15] is an excellent introduction to this material.
We might speculate that this could also be related to the general purpose
analog computer studied by Shannon, Martin-Pour-El, Ruebel and others last
century.

176
R. Downey
5
Relationships between Random Strings and Complexity
Classes
A very interesting programme is due to Allender and his co-workers (and others
such as Day). At ﬁrst glance it seems rather strange, but the idea is to look
at resource bounded reductions to highly noncomputable objects like clean ver-
sions of RC = {σ : C(σ) ≥|σ|
2 }, and similarly RQ for any other Kolmogorov
complexity Q. Long ago, Kummer [47] showed that RC is tt-complete. This
is by no means an obvious fact and the proof uses 0′′ nonuniformity to build
the reduction. It is not necessarily true for RK and depends on the choice of
universal machine, a fact established by Muchnik (in [58]), using a fascinating
game-theoretical argument (see [29] for details).
Kummer’s reduction was double exponential length increasing and one might
ask what does P RC look like. Clearly P RC has noncomputable sets of strings in
it, but the idea that this is an artifact of the choice of universal machine. The
correct class to look at is
∩UP RCU .
Sometimes it is suggested that this should be intersected with the computable
sets, but Allender conjectures that this makes no diﬀerence, ∩UP RCU ∩COMP =
∩UP RCU .
In [4] it is proven that P = ∩U{A : A ≤p
dtt RCU }∩COMP where the reductions
are restricted to polynomial time disjunctive truth table ones. Some of the results
so far, for any variants of the Kolmogorov complexity (so we drop the subscript)
are BPP ⊆{A : A ≤p
tt R} ∩COMP, PSPACE ⊆P R ∩COMP, and NEXP ⊆
NPR ∩COMP. Speciﬁcally it is open for these containments if we can drop the
∩COMP. The containments might actually be equality, and these are important
open questions. Recently, Allender, Friedman and Gasarch [2] have tightened
two of these for preﬁx-free complexity to BPP ⊆{A : A ≤p
tt RK} ∩COMP ⊆
PSPACE and NEXP ⊆NPRK ∩COMP ⊆EXPSPACE. Interestingly, these
proofs come from sharpening Muchnik’s game method, along with the fact that
the natural home for strategies is PSPACE.
The methods for some of these results use extractors. These are methods
of taking weak sources of randomness and producing pseudo-randomness from
them, and are particularly successful if you either have independent sources, or
some “true” randomness like a physical source assuming quantum assumptions.
those have found other uses in algorithmic randomness, such as Zimand’s proof
[79] that two sources of nonzero eﬀective Hausdorﬀdimension can together com-
pute a degree which has Hausdorﬀdimension 1. It is known that one source is
not enough as Miller [57] has shown that there is a Turing degree of fractional
eﬀective Hausdorﬀdimension. (See [29]. It is still open if a Turing degree can be
minimal and have eﬀective Hausdorﬀdimension 1.)
6
Physics
In this last section I will mention a few things of relevance. First, it is possible to
look at various natural phenomena which are regarded as random, such as, say,

Randomness, Computation and Mathematics
177
Brownian motion. Fouche [31,32], Kjos-Hanssen and Nerode [42] and B. Kjos-
Hanssen and T. Szabados [46] have a nice body of work here, showing that, for
instance, 1-randomness can be used to understand Brownian motion.
Another major area of randomness is quantum physics under the Copenhagen
interpretation. Some physicists claim that this produces true randomness. In the
same way that we don’t know if the universe can produce any incomputability,
it seems that we don’t know if it can even produce 1-randomness, say. In spite of
this, it seems that we can buy true randomness by Internet, via companies using
semi-transparent mirrors. One such company is Quantis: quantum mechanical
random number generator produced and sold by id Quantique of the University
of Geneva. They seem to pass reasonable practical statistical tests.
It seems that this is a hypothesis that might be analyzed. Assuming that the
universe is a (computable) manifold and assuming the Copenhagen interpreta-
tion, we could ask if we could produce initial segments of random reals. Calude,
Svozil and others are looking at this idea, e.g., [1,18]
7
Conclusion
This is my interpretation of a few themes and high points for the exciting area
of algorithmic randomness. Space considerations preclude me including more. I
do hope I have at least wetted your interest in this fascinating subject.
Acknowledgements. Research supported by the Marsden Fund of New Zealand.
This paper was written whilst the author was a visiting fellow at the Isaac New-
ton Institute for Mathematical Science, Cambridge, UK, as part of the Alan
Turing “Semantics and Syntax” programme, in 2012.
References
1. Abbott, A.A., Calude, C.S., Svozil, K.: Incomputability of quantum physics (in
preparation)
2. Allender, E., Friedman, L., Gasarch, W.: Limits on the Computational Power of
Random Strings. In: Aceto, L., Henzinger, M., Sgall, J. (eds.) ICALP 2011. LNCS,
vol. 6755, pp. 293–304. Springer, Heidelberg (2011)
3. Allender, E., Buhrman, H., Kouck´y, M., van Melkebeek, D., Ronneburger, D.:
Power from Random Strings. SIAM J. Comp. 35, 1467–1493 (2006)
4. Allender, E., Buhrman, H., Kouck´y, M.: What Can be Eﬃciently Reduced to the
Kolmogorov-Random Strings? Annals of Pure and Applied Logic 138, 2–19 (2006)
5. Athreya, K., Hitchcock, J., Lutz, J., Mayordomo, E.: Eﬀective strong dimension in
algorithmic information and computational complexity. SIAM Jour. Comput. 37,
671–705 (2007)
6. Avigad, J.: The metamathematics of ergodic theory. Annals of Pure and Applied
Logic 157, 64–76 (2009)

178
R. Downey
7. Barmpalias, G., Lewis, A., Ng, K.M.: The importance of Π0
1 classes in eﬀective
randomness. JSL 75(1), 387–400 (2010)
8. Becher, V.: Turing’s Normal Numbers: Towards Randomness. In: Cooper, S.B.,
Dawar, A., L¨owe, B. (eds.) CiE 2012. LNCS, vol. 7318, pp. 35–45. Springer, Hei-
delberg (2012)
9. Becher, V., Grigorieﬀ, S.: From index sets to randomness in ∅n, Random reals and
possibly inﬁnite computations. Journal of Symbolic Logic 74(1), 124–156 (2009)
10. Bienvenu, L., Downey, R.: Kolmogorov complexity and Solovay functions. In:
STACS 2009, pp. 147–158 (2009)
11. Bienvenu, L., Merkle, W.: Reconciling Data Compression and Kolmogorov Com-
plexity. In: Arge, L., Cachin, C., Jurdzi´nski, T., Tarlecki, A. (eds.) ICALP 2007.
LNCS, vol. 4596, pp. 643–654. Springer, Heidelberg (2007)
12. Bienvenu, L., Muchnik, A., Shen, A., Vereshchagin, N.: Limit complexities revis-
ited. In: STACS 2008 (2008)
13. Bienvenu, L., Day, A., Hoyrup, M., Mezhirov, I., Shen, A.: Ergodic-type character-
izations of algorithmic randomness. To appear in Information and Computation
14. Bertrand, J.: Calcul des Probabilit´es (1889)
15. Brattka, V., Miller, J., Nies, A.: Randomness and diﬀerentiability (to appear)
16. Braverman, M., Yampolsky, M.: Non-Computable Julia Sets. Journ. Amer. Math.
Soc. 19(3) (2006)
17. Braverman, M., Yampolsky, M.: Computability of Julia Sets. Springer (2008)
18. Calude, C., Svozil, K.: Quantum randomness and value indeﬁniteness. Advanced
Science Letters 1, 165–168 (2008)
19. Chaitin, G.: A theory of program size formally identical to information theory.
Journal of the ACM 22, 329–340 (1975)
20. Cholak, P., Downey, R., Greenberg, N.: Strong-jump traceablilty. I. The com-
putably enumerable case. Advances in Mathematics 217, 2045–2074 (2008)
21. Church, A.: On the concept of a random sequence. Bulletin of the American Math-
ematical Society 46, 130–135 (1940)
22. Cilibrasi, R., Vitanyi, P.M.B., de Wolf, R.: Algorithmic clustering of music based
on string compression. Computer Music J. 28(4), 49–67 (2004)
23. Day, A.: Increasing the gap between descriptional complexity and algorithmic prob-
ability. Transactions of the American Mathematical Society 363, 5577–5604 (2011)
24. de Leeuw, K., Moore, E.F., Shannon, C.E., Shapiro, N.: Computability by proba-
bilistic machines. In: Shannon, C.E., McCarthy, J. (eds.) Automata studies. Annals
of Mathematics Studies, vol. 34, pp. 183–212. Princeton University Press, Prince-
ton (1956)
25. Demuth, O.: The diﬀerentiability of constructive functions of weakly bounded vari-
ation on pseude-numbers. Comment. Math. Univ. Carolina. 16, 583–599 (1975)
26. Downey, R.: Five Lectures on Algorithmic Randomness. In: Chong, C., Feng, Q.,
Slaman, T.A., Woodin, W.H., Yang, Y. (eds.) Computational Prospects of Inﬁn-
ity, Part I: Tutorials. Lecture Notes Series, Institute for Mathematical Sciences,
National University of Singapore, vol. 14, pp. 3–82. World Scientiﬁc, Singapore
(2008)
27. Downey, R.: Algorithmic randomness and computability. In: Proceedings of the
2006 International Congress of Mathematicians, vol. 2, pp. 1–26. European Math-
ematical Society (2006)
28. Downey, R., Greenberg, N.: Pseudo-jump operators and SJTHard sets (submitted)
29. Downey, R., Hirschfeldt, D.: Algorithmic Randomness and Complexity. Springer
(2010)

Randomness, Computation and Mathematics
179
30. Downey, R., Hirschfeldt, D., Nies, A., Terwijn, S.: Calibrating randomness. Bulletin
Symbolic Logic 12, 411–491 (2006)
31. Fouche, W.: The descriptive complexity of Brownian motion. Advances in Mathe-
matics 155, 317–343 (2000)
32. Fouche, W.: Dynamics of a generic Brownian motion: Recursive aspects. Theoret-
ical Computer Science 394, 175–186 (2008)
33. Franklin, J., Greenberg, N., Miller, J., Ng, K.M.: Martin-Loef random points satisfy
Birkhoﬀ’s ergodic theorem for eﬀectively closed sets. Proc. Amer. Math. Soc. (to
appear)
34. Fuchs, H., Schnorr, C.: Monte Carlo methods and patternless sequences. In: Oper-
ations Research Verfahren, Symp., Heidelberg, vol. XXV, pp. 443–450 (1977)
35. G´acs, P.: On the relation between descriptional complexity and algorithmic prob-
ability. Theoretical Computer Science 22, 71–93 (1983)
36. G´acs, P.: Every set is reducible to a random one. Information and Control 70,
186–192 (1986)
37. G´acs, P., Hoyrup, M., Rojas, C.: Randomness on computable probability spaces,
a dynamical point of view. Theory of Computing Systems 48(3), 465–485 (2011)
38. Gregorieﬀ, S., Ferbus, M.: Is Randomness native to Computer Science? Ten years
after. In: [78], pp. 243–263 (2011)
39. Hochman, M., Meyerovitch, T.: A characterization of the entropies of multidimen-
sional shifts of ﬁnite type. Annals of Mathematics 171(3), 2011–2038 (2010)
40. H¨olzl, R., Kr¨aling, T., Merkle, W.: Time-Bounded Kolmogorov Complexity and
Solovay Functions. In: Kr´aloviˇc, R., Niwi´nski, D. (eds.) MFCS 2009. LNCS,
vol. 5734, pp. 392–402. Springer, Heidelberg (2009)
41. Fortnow, L., Lee, T., Vereshchagin, N.K.: Kolmogorov Complexity with Error.
In: Durand, B., Thomas, W. (eds.) STACS 2006. LNCS, vol. 3884, pp. 137–148.
Springer, Heidelberg (2006)
42. Kjos-Hanssen, B., Nerode, A.: Eﬀective dimension of points visited by Brownian
motion. Theoretical Computer Science 410(4-5), 347–354 (2009)
43. Kjos-Hanssen, B., Szabados, T.: Kolmogorov complexity and strong approximation
of Brownian motion. Proc. Amer. Math. Soc. 139(9), 3307–3316 (2011)
44. Kolmogorov, A.N.: Three approaches to the quantitative deﬁnition of information.
Problems of Information Transmission 1, 1–7 (1965)
45. Kuˇcera, A.: Measure, Π0
1 classes, and complete extensions of PA. In: Recursion
Theory Week, Oberwolfach. Lecture Notes in Mathematics, vol. 1141, pp. 245–
259. Springer, Berlin (1984-1985)
46. Kuˇcera, A., Slaman, T.: Randomness and recursive enumerability. SIAM J. on
Comp. 31, 199–211 (2001)
47. Kummer, M.: On the Complexity of Random Strings(Extended abstract). In:
Puech, C., Reischuk, R. (eds.) STACS 1996. LNCS, vol. 1046, pp. 25–36. Springer,
Heidelberg (1996)
48. Levin, L.: Some theorems on the algorithmic approach to probability theory and
information theory. Dissertation in Mathematics Moscow University (1971)
49. Levin, L.: Laws of information conservation (non-growth) and aspects of the foun-
dation of probability theory. Problems of Information Transmission 10, 206–210
(1974)
50. L´evy, P.: Th´eorie de l’Addition des Variables Al´eatoires. Gauthier-Villars (1937)
51. Nies, A.: Lowness properties and randomness. Advances in Mathematics 197(1),
274–305 (2005)

180
R. Downey
52. Nies, A.: Computability and Randomness. Oxford University Press (2009)
53. Nies, A.: Interactions of computability and randomness. In: Ragunathan, S. (ed.)
Proceedings of the International Congress of Mathematicians, pp. 30–57 (2010)
54. Nies, A., Stephan, F., Terwijn, S.A.: Randomness, relativization, and Turing de-
grees. JSL 70(2), 515–535 (2005)
55. Martin-L¨of, P.: The deﬁnition of random sequences. Information and Control 9,
602–619 (1966)
56. Mayordomo, E.: A Kolmogorov complexity characterization of constructive Haus-
dorﬀdimension. Infor. Proc. Lett. 84, 1–3 (2002)
57. Miller, J.: Extracting information is hard: a Turing degree of non-integral eﬀective
Hausdorﬀdimension. Advances in Mathematics 226(1), 373–384 (2011)
58. Muchnik, A.A., Positselsky, S.P.: Kolmogorov entropy in the context of computabil-
ity theory. Theor. Comp. Sci. 271, 15–35 (2002)
59. Muchnik, A.A., Semenov, A., Uspensky, V.: Mathematical metaphysics of random-
ness. Theor. Comp. Sci. 207(2), 263–317 (1998)
60. Nies, A., Miller, J.: Randomness and computability: Open questions. Bull. Symb.
Logic. 12(3), 390–410 (2006)
61. Poul-El, M., Richards, I.: Computability in Analysis and Physics. Springer (1989)
62. Reimann, J.: Eﬀectively closed classes of measures and randomness. Annals of Pure
and Applied Logic 156(1), 170–182 (2008)
63. Reimann, J., Slaman, T.: Randomness for continuous measures (to appear), draft
available from Reimann’s web site
64. Schnorr, C.P.: A uniﬁed approach to the deﬁnition of a random sequence. Mathe-
matical Systems Theory 5, 246–258 (1971)
65. Schnorr,
C.P.:
Zuf¨alligkeit
und
Wahrscheinlichkeit.
Eine
algorithmische
Begr¨undung
der Wahrscheinlichkeitstheorie.
Lecture Notes in
Mathematics,
vol. 218. Springer, Berlin (1971)
66. Simpson, S.: Medvedev Degrees of 2-Dimensional Subshifts of Finite Type. Ergodic
Theory and Dynamical Systems (to appear)
67. Simpson, S.: Mass Problems Associated with Eﬀectively Closed Sets. Tohoku Math-
ematical Journal 63(4), 489–517 (2011)
68. Simpson, S.: Symbolic Dynamics: Entropy = Dimension = Complexity (2011) (to
appear)
69. Stephan, F.: Martin-L¨of random sets and PA-complete sets. In: Logic Colloquium
2002. Lecture Notes in Logic, vol. 27, pp. 342–348. Association for Symbolic Logic
(2006)
70. Turing, A.: On computable numbers with an application to the Entscheidungsprob-
lem. Proceedings of the London Mathematical Society 42, 230–265 (1936); Correc-
tion in Proceedings of the London Mathematical Society 43, 544–546 (1937)
71. Turing, A.: Computing machinery and intelligence. Mind 59, 433–460 (1950)
72. Vitanyi, P.: Information distance in multiples. IEEE Trans. Inform. 57(4), 2451–
2456 (2011)
73. Ville, J.: ´Etude Critique de la Notion de Collectif. Gauthier-Villars (1939)
74. von Mises, R.: Grundlagen der Wahrscheinlichkeitsrechnung. Math. Z 5, 52–99
(1919)
75. von Neumann, J.: Various techniques used in connection with random digits. In:
Householder, A.S., Forsythe, G.E., Germond, H.H. (eds.) Monte Carlo Method. Na-
tional Bureau of Standards Applied Mathematics Series, vol. 12, pp. 36–38 (1951)

Randomness, Computation and Mathematics
181
76. Wald, A.: Sur le notion de collectif dans la calcul des probabiliti´es. Comptes Rendes
des Seances de l’Acad´emie des Sciences 202, 1080–1083 (1936)
77. Wald, A.: Die Weiderspruchsfreiheit des Kollektivbegriﬀes der Wahrscheinlichkeit-
srechnung. Ergebnisse eines mathematischen Kolloquiums 8, 38–72 (1937)
78. Zenil, H. (ed.): Randomness Through Computation: Some Answers, More Ques-
tions. World Scientiﬁc, Singapore (2011)
79. Zimand, M.: Two Sources Are Better Than One for Increasing the Kolmogorov
Complexity of Inﬁnite Sequences. In: Hirsch, E.A., Razborov, A.A., Semenov, A.,
Slissenko, A. (eds.) CSR 2008. LNCS, vol. 5010, pp. 326–338. Springer, Heidelberg
(2008)

Learning, Social Intelligence and the Turing Test
Why an “Out-of-the-Box” Turing Machine Will Not Pass
the Turing Test
Bruce Edmonds1 and Carlos Gershenson2
1 Centre for Policy Modelling, Manchester Metropolitan University, Aytoun
Building, Aytoun Street, Manchester M1 3GH, United Kingdom
bruce@edmonds.name
2 Departmento de Ciencias de la Computaci´on, Instituto de Investigaciones en
Matem´aticas Aplicadas y en Sistemas, Universidad Nacional Aut´onoma de M´exico,
Ciudad Universitaria, A.P. 20-726, 01000 M´exico D.F., M´exico
cgg@unam.mx
Abstract. The Turing Test checks for human intelligence, rather than
any putative general intelligence. It involves repeated interaction requir-
ing learning in the form of adaption to the human conversation partner.
It is a macro-level post-hoc test in contrast to the deﬁnition of a Turing
machine, which is a prior micro-level deﬁnition. This raises the question
of whether learning is just another computational process, i.e., can be
implemented as a Turing machine. Here we argue that learning or adap-
tion is fundamentally diﬀerent from computation, though it does involve
processes that can be seen as computations. To illustrate this diﬀerence
we compare (a) designing a Turing machine and (b) learning a Turing
machine, deﬁning them for the purpose of the argument. We show that
there is a well-deﬁned sequence of problems which are not eﬀectively
designable but are learnable, in the form of the bounded halting prob-
lem. Some characteristics of human intelligence are reviewed including
it’s: interactive nature, learning abilities, imitative tendencies, linguistic
ability and context-dependency. A story that explains some of these is
the Social Intelligence Hypothesis. If this is broadly correct, this points
to the necessity of a considerable period of acculturation (social learning
in context) if an artiﬁcial intelligence is to pass the Turing Test. Whilst
it is always possible to ‘compile’ the results of learning into a Turing
machine, this would not be a designed Turing machine and would not be
able to continually adapt (pass future Turing Tests). We conclude three
things, namely that: a purely “designed” Turing machine will never pass
the Turing Test; that there is no such thing as a general intelligence since
it necessarily involves learning; and that learning/adaption and compu-
tation should be clearly distinguished.
1
Introduction
The approaches in Turing’s two most famous papers contrast markedly. The
deﬁnition of a computation, in the form of a Turing Machine (Turing machine),
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 182–192, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Learning, Social Intelligence and the Turing Test
183
is a micro-level speciﬁcation of a device (its design and rules for operation) from
which computable functions can be deﬁned [24]. It speciﬁes what happens when
a Turing machine that has been built is set going. It is a formal deﬁnition that
deﬁnes the set of computable functions. The Turing Test (Turing Test) is a
macro-level test that is applied to an existing entity that is “running” [25]. It is
not formally deﬁned but a practical test, intended to be feasible to implement.
Here intelligence is not something to be proved but demonstrated.
As pointed out by French [10], the Turing Test is not a test of a putative
“general intelligence” but a test of a speciﬁc kind of intelligence – normal human
intelligence. There may well be intelligent entities that might not pass the Turing
Test, for example a human suﬀering from inﬂuenza or an alien whose language
we do not know. The point of the Turing Test is that if some entity passes it, then
it is hard to deny that this entity is intelligent – it short-cuts possible quibbling,
and thus opens up the possibility that an artiﬁcial entity could be judged as
intelligent.
The Turing Test consists of a conversation over a period of time between a
tester and the entity being tested. This requires an ability to learn or adapt
to what the tester has said, including: the topic of conversation, the style, the
detected context that the tester is coming from, and the importance given to
particular issues. Clearly the Turing Test is harder the longer it goes on for. It
is far easier to fool someone if one only talks to them for a limited period of
time (using, for example, rote learned scripts) than if there is time for topics to
be revisited with more testing questions, checking consistency with what went
before as well as common knowledge and assumptions. It is the interactive and
adaptive nature of the Turing Test that makes it so challenging, revealing shallow
strategies (such as simplistic syntactic approaches) as inadequate.1
The arguments in this paper rest upon the diﬀerence between computation
(deﬁned by a halting Turing machine) and adaption, which is an essential part
of the intelligence that is tested for by the Turing Test. Thus in section 2 we
argue that computation and adaptivity are diﬀerent kinds of things, giving an
example where they can be shown to diﬀer. We then brieﬂy review some of
the characteristics of human intelligence in section 3 and look at some of the
consequences in terms of passing the Turing Test using purely designed Turing
machines in section 4. We then consider the broader nature of intelligence and
its role in section 5 before concluding.
2
Learning vs. Computation
The relationship between adaptive processes, i.e., learning in the broadest sense,
and computation is not straightforward. Clearly learning involves processes that
can sensibly be characterized as a computation, as the ﬁeld of Machine Learning
1 Here we assume that the Turing Test is conducted over a suitably extended period
of time, since this tests intelligence as we know it, what is called the “Long Term
Turing Test” in [8].

184
B. Edmonds and C. Gershenson
amply demonstrates. Similarly, some computations – when acting upon some
internal data structure and outputting an updated data structure in response
to new information – might be sensibly thought of as a learning process. The
physical device of “a computer” can clearly be set up to do both learning and
computation.2 Thus the question arises whether they do, fundamentally, diﬀer.
In particular it is sometimes assumed that learning processes are just a particular
kind of computation. The core of our argument here is that the Turing machine
(or equivalent) is not an adequate model of learning processes, and hence misses
out a crucial aspect of intelligence, its adaptivity.
Many diﬀerent ways of deﬁning the computable functions turned out to be
the same, resulting in the “Church-Turing” thesis that these were the class
of eﬀective computations, including observed processes in the real world [5].
This meant that the details of the computation processes were not deemed as
crucial, but rather what was, and was not, ﬁnitely computable. Turing also
showed that there was a Universal Turing machine, one that could be given
a program number and then eﬀectively execute that program. Thus, in a deep
sense there is a universal characterization of computation. An observed, physical
process is meaningfully characterized as a computation if its inputs compared
to outputs can be eﬀectively predicted (at least at the micro, step-by-step level)
by a computable function (via a suitable mapping).
There is no such agreement on the deﬁnition of a learning process. Rather
there are many diﬀerent kinds of learning process, each with diﬀerent proper-
ties and assumptions. Indeed the “No Free Lunch” [28] theorems from Machine
Learning show (in an ultimate, abstract sense) that no learning algorithm is
better than any other. In other words, to ﬁnd a better learning algorithm you
need to use some prior knowledge about the class of situations within which
the adaption is taking place (in Machine Learning terms this is characterized
as the search problem), which means that the class of situations being learned
about is a proper subset of all possible situations. This indicates that there is
no universally best learning algorithm, however clever the learning strategy is
(e.g., using meta-reasoning/learning, or lots of special cases).
Thus, for our argument, we shall deﬁne a kind of learning process, an adequate
incremental learner, as follows:
• As a computational process (e.g., a Turing machine) plus a model, which is
a set of data;
• Where the data can be judged as to its truth or adequacy about something
exterior, which we shall call the learning “target” (for example via another
Turing machine that produces a prediction, or output, from the model with
respect to the target);
2 It is interesting that the ﬁrst use of the word “computers” referred to people – for
example the women that did calculations as part of the eﬀort to break codes during
WWII at Bletchley Park. The word seems to have been transferred to the devices
that took over this task when they were constructed [1].

Learning, Social Intelligence and the Turing Test
185
• Where the computation is iteratively provided with information from the
target such that during each iteration the data is modiﬁed by the Turing
machine using the information so that the updated model is at least as
adequate as before;3
• After a ﬁnite number of iterations the model becomes maximally adequate.
This is clearly not a universal deﬁnition of learning, since it excludes known
learning processes (e.g., ones that sometimes degrade the model). However, it is
also clearly a learning process and will thus do for our purposes. Here we use it
to show that there is something that can be learned (in the above sense) that
cannot be computed (by a Turing machine). The particular problem we shall use
for this demonstration is that of ﬁnding the lookup table (or Turing machine)
that solves the “Limited Halting Problem”.
The “Limited Halting Problem” is a sequence of increasingly diﬃcult problems,
indexed by integer, n, and deﬁned as follows. Let {P1, P2, P3...} be an eﬀective
enumeration of Turing machine’s. The limited halting problem is that of “given
n, does Pi(j) eventually halt where i, j ≤n”. Let us call this sequence of prob-
lems {H1, H2, H3, ...}, where Hn(i, j) is 1 if i, j ≤n and Pi(j) eventually halts, 0 if
i, j ≤n and Pi(j) does not halt, and undeﬁned otherwise. Each problem Hi is com-
putationally decidable, since it can be implemented as a simple 2D look-up table
with the rows {1, ...n} for possible program indices, Pi, and columns {0, ...n1} for
possible inputs, j, and entries 0 or 1 depending on whether Pi(j) eventually halts.
The problem is not the existence of this table but of ﬁnding the right entries for it.
The deﬁnition of computability is not constructive, it is suﬃcient that there exist
a program to compute a function, not that we can ﬁnd or implement this program.
The point is that there is an adequate incremental learner that can learn
the lookup table for Hn given any n but there is no Turing machine that can
implement this lookup table (or equivalent Turing machine) given any n. We
now give an informal proof of each part.
The following non-terminating algorithm establishes that, given an integer n
the lookup table that solves Hi can be learnt by an adequate incremental learner.
Build a nxn table with all entries 0
s := 1
Repeat for ever
For i from 1 to n
For j from 0 to n-1
Calculate Pi(j) for s steps
If it has terminated
then change entry at (I,j) to 1
Next j
Next i
3 One can see the Turing machine as presented with an index representing the set of
information: new information from the target, and the present model and outputting
an index representing the updated model (which would replace the old version of
the model).

186
B. Edmonds and C. Gershenson
Any Pi(j) that halts will do so after a ﬁnite period of time, so eventually the
table being adapted by the above algorithm will have the correct entries for
solving Hn although one might well not know when one gets to this point and
there is no eﬀective method for knowing whether one has (otherwise we could
solve the general halting problem). This algorithm fulﬁlls the deﬁnition for an
adequate incremental learner deﬁned above.
Now to show that there is no eﬀective method for ﬁnding the Turing machine
that implements the solution to Hn, given n. Suppose there were a computable
function f(x) such that Pf(n) computes Hn. In other words, of eﬀectively ﬁnding
the program index that implements the table for Hn, (which we know exists).
This is equivalent to having a general and eﬀective procedure for constructing
the Turing machine for Hn given n.
If there were such a computable function, f(x), then to decide whether Pi(j)
halts, we can calculate Pf(max(i,j))(i, j). This is deﬁned since i, j ≤max(i, j).
See if the answer is 1 or 0. Thus Pf(n) is computable via the universal Turing
machine [5]. Thus if f(x) were computable we could eﬀectively solve the general
halting problem, which we know is impossible [24]. Thus f(x) is not computable,
that is to say there is no Turing machine that computes it.
Thus there is a learning process whose “resulting” model is not computable by
a Turing machine. Learning is diﬀerent from computation. Of course, in a way,
this is obvious since a computation is, as deﬁned, not a process but a formally
deﬁned function (albeit possibly deﬁned using a process), whilst learning is an
on-going process. Learning can be seen as a kind of non-predeﬁned change in a
computational process [13]. A Turing machine cannot implement this since the
change is not predeﬁned.
Formalists may well be dissatisﬁed with the above demonstration since the
Turing machine has to halt whilst the adequate incremental learner does not.
However this diﬀerence is at the crux of the matter. Computation, as deﬁned
by a halting Turing machine, is not a process but the result of a process – the
Turing machine is merely a means of deﬁning which functions are computable.4
A possible confusion might arise if people conﬂate what we call a “computer”
(the physical object we use) and what is formally deﬁned as “computable” (using
a halting Turing machine or other deﬁnition). It is true that the intermediate
state of any process that implements a complete computation could, itself, be
seen as a computation of that intermediate state, but that intermediate state is
not part of the deﬁnition of the complete computation.5 This diﬀerence becomes
important when we are considering what sort of entity could pass the Turing
Test.
4 There are non-procedural ways of deﬁning computable functions, e.g., using lambda
calculus.
5 Each intermediate state is the result of a diﬀerent computation, but this is not the
same as the one an intermediate state is part of unless it happens to be the ﬁnal
state.

Learning, Social Intelligence and the Turing Test
187
3
Human Intelligence
We brieﬂy consider some of the characteristics of human intelligence, since the
Turing Test tests for an ability that results from human intelligence: the ability
to hold a recognisably “normal” conversation. These include being able to:
• continually react to social signals,
• imitate and learn from others,
• detect what is the appropriate social context,
• react in ways appropriate to the detected social context,
• imagine what it is like to be other people,
• use other people for ﬁltering relevant information from the environment,
• use other people to act on behalf of ourselves,
• make alliances and friendships, maintained by frequent interaction,
• acquire and eﬀectively use a large body of knowledge that is shared with
others,
• reason in ways which might be accepted by others using a shared, but im-
plicit, common knowledge,
• talk to others and make them understand our intention and meaning.
All of these characteristics (and many more) are explained by the “Social In-
telligence Hypothesis” (SIH) [19].6 The SIH seeks to explain the evolutionary
advantage provided by human brains. The explanation can be summarised as
follows: our brains give us the social ability to coordinate and develop a com-
monly held set of knowledge and behaviours; this allows groups of individuals to
inhabit specialized ecological niches (e.g., to live in the Tundra or Kalahari [22]);
this ability to collectively adapt to and successfully inhabit a variety of niches
gives us selective advantage. Under this view our brain is an adaptive organ to
give us social abilities, what might be called social intelligence, including the
characteristics listed above. However it also implies that these social abilities are
primary and classic displays of intelligence (including reasoning, and problem
solving) are by-products. Being able to solve a Rubrick’s Cube or play chess has
no selective advantage,7 but being part of a society that invents such puzzles
and games and talks about them is!
4
Design and the Turing Test
It is interesting that Turing chose a social test for intelligence years before the
social roots of intelligence were widely appreciated. However, these social roots
6 The related idea of “Machiavellian intelligence” had been around in primatology
(e.g., [7]) before it was taken up in anthropology, focussing on the cognitive ‘arms
race’ that might have occurred in terms of the competitive evolution in the social
ability to make alliances [2]. The SIH is more general in its formulation covering a
broad range of social abilities.
7 It is not even eﬀective as a display for attracting a potential mate, as those on the
back row of my undergraduate mathematics lectures demonstrated.

188
B. Edmonds and C. Gershenson
have implications for passing the Turing Test. In particular it indicates that being
part of the appropriate human culture is a key part of social intelligence, and
not just an ‘extra’ that needs to be added once individual intelligence is sorted
out. If this is the case, a considerable period of acculturation within a human
society is necessary to pass the Turing Test.8 A purely individual intelligence
without such training would not suﬃce.
If a signiﬁcant amount of learning is necessary to obtain a social intelligence
that might pass the Turing Test, then to a large extent, this intelligence is not
designed but developed in context. In other words, what one intentionally puts
into a Turing machine, i.e., by design, is not suﬃcient to pass the Turing Test, but
rather the huge body of context-speciﬁc information that is usually accumulated
by humans as they grow into a culture. This is what we mean when we say that
an “out-of-the-box” Turing machine will not pass the Turing Test.
Of course, once cultural information has been learnt by some entity, this
could be ‘compiled’ into a complex Turing machine, but we would not have
designed this in any meaningful sense. Such a compiled entity would be a static
entity that has stopped learning. Such a Turing machine might pass muster in
a one-shot and short Turing Test. However, if the test were extended in time
then the conversation itself would form part of the cultural information that
needs to be learnt about by the entity, so that in subsequent interactions it can
appropriately refer back to what has been said. The diﬀerence is made clear if
one has a conversation with a human with an impaired ability to lay down new
memories, but who retains all the long-term memories before a certain date.9
For the ﬁrst period of time such people seem normal, but over any period of
time their lack of memory becomes apparent.10
5
Intelligence in General
It should be obvious that the sort of intelligence that could pass the Turing Test
requires reasoning and learning, it cannot be reasoning alone.11 One consequence
of this, along with the “No Free Lunch” theorems mentioned above [28], is that
there is no such thing as general intelligence. That is to say that (unlike compu-
tation) intelligence cannot be general, rather that diﬀerent intelligences are each
suited to particular problems and/or environments. Clearly, if the SIH is true,
our intelligence is particularly suited to living in groups that develop a collective
body of knowledge, habits, norms, skills, stories etc. Whatever “clever” algo-
rithms we invent, using meta-reasoning, meta-learning, special cases, etc. will
8 Indeed, anecdotal accounts have it that Turing joked that such machines might be
teased whilst they attended human schools.
9 That such conditions exist and can be brought on by many diﬀerent causes can be
seen in many clinical accounts, e.g., [10].
10 Of course, it might be that such a person might pass the Turing Test if people with
such conditions were expected as a possible participant. However, they would be
clearly distinguishable from a healthy adult human.
11 Reasoning is roughly coincident to computation, since any formal reasoning can be
implemented as a computation and any computation as formal reasoning.

Learning, Social Intelligence and the Turing Test
189
have “blind spots” where its algorithms are not as eﬀective (w.r.t. its goals) as
another (and possibly simpler) algorithm. Any algorithmic elaboration will have
downsides as well as new abilities. In other words, intelligence is relative to the
goals and environment of an entity, it is not an ordinal quality, with humans
having the most.
Once one accepts that intelligence cannot have an ideal, we can distinguish
diﬀerent types of intelligence. For example, rats can be smarter than humans at
navigating mazes. Does this imply that rats are smarter than humans? Well, it
is better to clarify what is being tested. In the Turing Test where human social
interaction is being tested humans tend to do pretty well. But diﬀerent types of
tests allow us to explore diﬀerent types of intelligence, in animals and machines.
For example, social insects can be pretty eﬀective at collective decision making
[11]. Plants could also be said to be intelligent [23], as well as bacteria [2].12 We
could take a narrow deﬁnition of intelligence and apply it to humans, or embrace
a broad diversity of intelligences and understand human intelligence better by
relating it to other types. For example, Randall Beer deﬁnes intelligence as “the
ability to display adaptive behaviour” [1].
The same applies for artiﬁcial intelligence: we can take the narrow path of
attempting to program human intelligence, which we argue is not achievable
without learning/adaption; or we can take the broad path of exploiting all types
of biological and artiﬁcial intelligences for solving problems in the most diverse
areas. This broad approach seems more suitable for the parts of our complex
world that are not structured around human sociability. Given the fact that
problems are changing constantly in unpredictable ways due to their interactions
[13], systems we build to solve these problems must adapt and learn constantly,
matching the timescale at which problems change. We can predict that, as the
complexity of problems increase, artiﬁcial systems will focus more and more on
learning and adaption rather than on directly programming static solutions.
None of this invalidates the Turing Test that, if passed, would unequivocally
establish that a substantial intelligence, as impressive as human intelligence, had
been brought into being. We would be forced to recognise its legitimacy since
we impute intelligence in our fellow humans on a similar basis.
6
Understanding Ourselves
Understanding human intelligence is obviously hard, due to its complexity. How-
ever, there is another diﬃculty as well, a “touchy” subject for us humans. We
seek to understand ourselves using whatever cognitive means are at our disposal
and what we use for this purpose eﬀects our self-image.
The ﬁrst and obvious way of understanding ourselves is by observing and un-
derstanding others around us. Whilst we clearly understand others by imagining
how we would feel or think in their situation [6], it seems to also be true that,
12 Though it might be more accurate to attribute the intelligence in many cases to the
process of evolution (the underlying learning process) rather than the entity that
results.

190
B. Edmonds and C. Gershenson
during development, we use our observations of how others behave to help us un-
derstand ourselves [15]. The circular bootstrapping of our own identity produces
a very strong association between our perception of our own intelligence and
that of others. It is perhaps this that makes the Turing Test so compelling: we
cannot but consider the entities with whom we converse as similar to ourselves.
A second way of understanding ourselves is by using analogies with devices
around us. Whilst the Victorians might have conceived of the world and per-
sons in terms of intricate clockwork, we tend to use the computer. Thinking
of ourselves as computers is natural, since we have many things in common
with modern computational devices: they are interactive, responsive and can be
programmed to behave in human-like ways, for example by learning. Intensive
interaction with computers can lead to a strong association with such devices, to
the extent that we see ourselves as a kind of computer and impute many human
characteristics upon them [26]. Since the Turing machine is a formal model of a
computation, and being able to be predicted by such a model is what makes a
device a “computer”,13 this leads to an association of Turing machines and our
intelligence. However, as the above arguments show, this analogy captures only
some of the complete picture.
A third way of understanding ourselves, is as the pinnacle of evolution, as
possessing (essentially) a completely general intelligence. Somehow it is assumed
that the human brain, together with its creations: paper, maths, computers etc.,
is not limited in what it can understand. This is not a view of any individual, of
course, but rather that eventually humankind will be able to work anything out.
Surely, if people do indeed think this, this is nothing more than sheer hubris.
However forms of this thinking seem to be implicit in some of the thinking about
the Turing Test, e.g., French’s [10] denigration of ‘subcognitive’ aspects of human
intelligence and his criticism of the Turing Test as “only” being a test of human
intelligence.
We (along with others) wish to work towards a diﬀerent understanding of
intelligence, by looking to other parts of life, their ecology and the process of
evolution. This tries to relate the nature of intelligence to why it evolved. It is a
more interactive view that wishes to place individual entities within a broader
web of interactions, so that the interactions within an entity are just part of this
broader web. It is admitted that, currently, this view does not have formal models
of the strength of the Turing machine, but rather a plethora of simulations and
approaches.
7
Conclusions
There are good and general models of computation, going back to Turing’s orig-
inal paper but there is no equivalent for the interactive and continual process
13 It is sensible to think of these devices as “computers”, since one can understand
what they are doing since, with practice, we can perform simple calculations and
work out what small bits of computer code are doing. The micro-level model as a
kind of Turing machine predicts the device’s behaviour accurately.

Learning, Social Intelligence and the Turing Test
191
that we call learning, let alone the more general phenomena of adaptive inter-
action. The Turing Test nicely shows the diﬀerence between the two, being a
test that requires the latter. Perhaps Turing’s later paper will have the eﬀect of
moving on thinking about ways of formally representing intelligence, and free it
from the limited analogy of the Turing machine.
Acknowledgements. BE was partially supported by the Engineering and
Physical Sciences Research Council, grant number EP/H02171X/1. CG was par-
tially supported by SNI membership 47907 of CONACyT, Mexico. We thank the
referees for their further suggested reading, including [16], [21] and [27] which
were interesting but we did not feel clariﬁed the arguments here.
References
1. Beer, R.D.: Intelligence as adaptive behavior: an experiment in computational neu-
roethology. Academic Press (1990)
2. Ben-Jacob, E., Becker, I., Shapira, Y., Levine, H.: Bacterial linguistic communica-
tion and social intelligence. Trends in Microbiology 12(8), 366–372 (2004)
3. Byrne, Whiten, A.: Machiavellian intelligence. Oxford University Press (1988)
4. Copeland, B.J.: The Modern History of Computing. In: Edward, N. (ed.) The
Stanford Encyclopedia of Philosophy (fall 2008 edn.) (2008),
http://plato.stanford.edu/archives/fall2008/entries/computing-history
5. Cutland,
N.:
Computability,
an
introduction
to recursive
function
theory.
Cambridge University Press (1980)
6. Tomasello, M.: The cultural origins of human cognition. Harvard University Press
(1999)
7. De Waal, F.: Chimpanzee politics: power and sex among apes. John Hopkins Uni-
versity Press (1989)
8. Edmonds, B.: The constructability of artiﬁcial intelligence (as deﬁned by the Turing
Test). Journal of Logic Language and Information 9, 419–424 (2000)
9. Edmonds, B.: The social embedding of intelligence: how to build a machine that
could pass the Turing Test. In: Epstein, R., Roberts, G., Beber, G. (eds.) Parsing
the Turing Test, pp. 211–235. Springer (2008)
10. French, R.M.: Subcognition and the limits of the Turing Test. Mind 99, 53–64
(1989)
11. Garnier, S., Gautrais, J., Theraulaz, G.: The biological principles of swarm intelli-
gence. Swarm Intelligence 1(1), 3–31 (2007)
12. Gershenson, C.: Cognitive paradigms: Which one is the best? Cognitive Systems
Research 5(2), 135–156 (2004)
13. Gershenson, C.: Computing Networks: A General Framework to Contrast Neural
and Swarm Cognitions, Paladyn. Journal of Behavioral Robotics 1(2), 147–153
(2010)
14. Gershenson, C.: The implications of interactions for science and philosophy. Tech-
nical Report, 04, C3, UNAM, Mexico (2011)
15. Guimond, S., et al.: Social comparison, self-stereotyping, and gender diﬀerences in
self-construals. Journal of Personality and Social Psychology 90(2), 221–242 (2006)
16. Glymour, C.: Learning, prediction and causal Bayes nets. Trends in Cognitive
Sciences (1), 43–48 (2003)

192
B. Edmonds and C. Gershenson
17. Humphrey, N.K.: The social function of the intellect. In: Bateson, P.P.G., Hinde,
R.A. (eds.) Growing Points in Ethology, Cambridge University Press, Cambridge
(1976)
18. Kirshner, H.S.: Approaches to intellectual and memory impairments. In: Gradley,
W.G., et al. (eds.) Neurology in Clinical Practice, 5th edn., ch. 6, Butterworth-
Heinemann (2008)
19. Kummer, H., Daston, L., Gigerenzer, G., Silk, J.: The social intelligence hypoth-
esis. In: Weingart, et al. (ed.) Human by Nature: Between Biology and the Social
Sciences, pp. 157–179. Lawrence Erlbaum Associates (1997)
20. Lane, H.: The Wild Boy of Aveyron. Harvard University Press (1976)
21. Marr, D.: Vision: A Computational Approach. Freeman & Co., San Francisco
(1982)
22. Reader, J.: Man on Earth. Penguin Books (1990)
23. Trewavas, A.: Aspects of plant intelligence. Annals of Botany 92(1), 1–20 (2003)
24. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proc. of the London Mathematical Society 2 42, 230–265 (1936)
25. Turing, A.M.: Computing machinery and intelligence. Mind 59, 433–460 (1950)
26. Turkle, S.: The second self: computers and the human spirit. Simon and Schuster
(1984)
27. van Rooij, I.: Tractable Cognition: Complexity Theory in Cognitive Psychology.
PhD Thesis, Katholieke Universiteit Nijmegen, Netherlands (1998),
http://www.nici.ru.nl/~irisvr/PhDthesis.pdf
28. Wolpert, D.H.: The lack of a priori distinctions between learning algorithms. Neu-
ral Computation 8(7), 1341–1390 (1996)

Conﬂuence in Data Reduction: Bridging Graph
Transformation and Kernelization
Hartmut Ehrig, Claudia Ermel, Falk H¨uﬀner, Rolf Niedermeier,
and Olga Runge
Institut f¨ur Softwaretechnik und Theoretische Informatik, Technische Universit¨at
Berlin, Ernst-Reuter-Platz 7, 10587 Berlin, Germany
{hartmut.ehrig,claudia.ermel,falk.hueffner}@tu-berlin.de,
{rolf.niedermeier,olga.runge}@tu-berlin.de
Abstract Kernelization is a core tool of parameterized algorithmics for
coping with computationally intractable problems. A kernelization re-
duces in polynomial time an input instance to an equivalent instance
whose size is bounded by a function only depending on some problem-
speciﬁc parameter k; this new instance is called problem kernel. Typi-
cally, problem kernels are achieved by performing eﬃcient data reduction
rules. So far, there was little study in the literature concerning the mutual
interaction of data reduction rules, in particular whether data reduction
rules for a speciﬁc problem always lead to the same reduced instance, no
matter in which order the rules are applied. This corresponds to the con-
cept of conﬂuence from the theory of rewriting systems. We argue that
it is valuable to study whether a kernelization is conﬂuent, using the
NP-hard graph problems (Edge) Clique Cover and Partial Clique
Cover as running examples. We apply the concept of critical pair anal-
ysis from graph transformation theory, supported by the AGG software
tool. These results support the main goal of our work, namely, to es-
tablish a fruitful link between (parameterized) algorithmics and graph
transformation theory, two so far unrelated ﬁelds.
1
Introduction
Theoretical Computer Science is usually divided into algorithm-oriented research
and description-oriented research (as witnessed by the two volumes “Algorithms
and Complexity” and “Formal Methods and Semantics” of the Handbook of
Theoretical Computer Science [15]). Unfortunately, the corresponding research
communities typically work in two “parallel worlds” with relatively little inter-
action. In this work, we propose a new link between algorithmics and formal
methods that may lead to a fruitful “interdisciplinary” ﬁeld of research. More
speciﬁcally, we develop a connection between eﬃcient preprocessing of NP-hard
(graph) problems by kernelization [2,11] and the theory of graph transforma-
tions [6,20]: We employ the concept of conﬂuence of rewriting systems to show
“uniqueness results” for problem kernels. This leads to the natural concept of
conﬂuent data reduction rules, having a number of both theoretical and practical
beneﬁts as discussed in the following.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 193–202, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

194
H. Ehrig et al.
Conﬂuence in kernelization. Data reduction, also known as polynomial-time
preprocessing, is a classic approach for dealing with NP-hard combinatorial op-
timization problems (see [2,11] for surveys). The idea is to remove redundant
parts of the input, thereby obtaining a hard “core” of the instance. Costly al-
gorithms need then only be applied to this core. Data reduction is thus useful
in virtually any approach to solving computationally hard problems, whether
heuristic, approximative, or exact. Formally, we consider only decision prob-
lems, and a (data) reduction rule replaces in polynomial time a given problem
instance I by an instance I′ with |I′| < |I|. We say that the rule is correct when
I is a yes-instance iﬀI′ is a yes-instance. An instance to which none of a given
set of reduction rules applies is called reduced with respect to these rules.
While they are a standard technique for practitioners, only fairly recently
have data reduction rules been the subject of wider theoretical analyses, us-
ing the concept of a problem kernel [2,11]. This notion comes from the ﬁeld of
parameterized complexity [5,18,9], where performance of algorithms is analyzed
not just in terms of the problem size n, but also in terms of a parameter k, for
example the solution size. A kernelization is a data reduction that creates an
equivalent instance whose size depends only on the parameter k, and not on the
original input size n anymore (see Section 2 for a more formal deﬁnition).
We call a terminating set of data reduction rules conﬂuent if any order of
application of the rules yields a unique reduced instance, up to isomorphism.
Conﬂuence is a standard concept from graph transformation theory (see below).
There are a number of reasons why it seems useful to investigate whether data
reduction rules are conﬂuent: If they are, then the rules are robust in a sense;
we obtain a unique starting point for further processing after the data reduction
has been performed. In an implementation of the rules, we can apply the rules
in any order without worrying about the result, and can thus optimize for the
speed of their application. If the rules are not conﬂuent, this might indicate some
“slack” in the rules: some orders of application might lead to worse results, that
is, larger kernels. Investigating all this might lead to improved reduction rules.
Further, insight on the interaction of data reduction rules can lead to faster
kernelizations. Conﬂuence was also exploited in [14], who showed that for their
problem, one order of application of data reduction yields some desired property
of the reduced instance, and another order yields a diﬀerent desired property.
A proof of conﬂuence now shows that a reduced instance has both properties.
Finally, proving conﬂuence is also a good way to check for possible conﬂicts
between data reduction rules, since all possible interactions need to be taken
into account. It might also give an incentive to create “minimal” kernelization
rules in order to make conﬂuence proofs easier, which could give a sharper picture
of what exactly is needed to achieve a kernel.
If we allowed data reduction in kernelization with restriction of the rule ex-
ecution order, we can force conﬂuent kernelization in a trivial way by allowing
only one execution order. In this paper, we avoid this trivial case by allowing
any execution order.

Conﬂuence in Data Reduction
195
Conﬂuence of graph transformation systems. The theory of graph grammars
and graph transformation systems has been started in the early 1970s [8] as
a generalization of Chomsky grammars and term rewriting systems, which are
based on strings and trees, respectively. The main idea is the rule-based modi-
ﬁcation of graphs. Graph transformations are most suitable to model the oper-
ational semantics of visual languages and also to deﬁne model transformations
between diﬀerent kinds of models. Several approaches for graph transformations
are known [20], including logical and algebraic approaches. A graph transfor-
mation system consists of a set of graph rules, which are applied in a non-
deterministic way, leading to graph transformation steps G =⇒H and sequences
G
∗
=⇒H. A single rule consists of a left-hand side graph LHS, a right-hand side
graph RHS, and their intersection graph. To apply a rule, a match is sought, that
is, a subgraph in the input graph that is isomorphic to LHS. This subgraph with-
out the intersection graph is then deleted, resulting in a context graph, which
is glued together with RHS at the nodes and edges of the intersection graph. A
graph transformation system is called conﬂuent if for each pair of graph trans-
formation sequences G
∗
=⇒G1, G
∗
=⇒G2, there is a graph G3 together with
sequences G1
∗
=⇒G3 and G2
∗
=⇒G3.
There are numerous applications in software engineering, concurrency, and
distributed systems [7], where conﬂuence of graph transformations plays an im-
portant role. Conﬂuence together with termination, that is, non-existence of
inﬁnite transformation sequences, implies that any order of applying the rules as
long as possible yields a unique graph, up to isomorphism. Moreover, we obtain
for isomorphic input graphs isomorphic reduced graphs [6].
In order to show conﬂuence it is suﬃcient to show local conﬂuence and termi-
nation [13,17], where local conﬂuence means conﬂuence for the special case that
the given sequences from G to G1 and G2 are transformation steps G =⇒G1
and G =⇒G2, where in each step only one transformation rule is applied. Data
reduction rules for kernelization for graph problems deﬁne graph transformation
systems based on undirected graphs, such that the general concepts of (local)
conﬂuence and termination are applicable.
Structure of the Paper. After presenting basic concepts and deﬁnitions about
kernelization and critical pair analysis in Section 2, we present our two case
studies Clique Cover and Partial Clique Cover in Section 3 and 4, respectively,
by showing that the corresponding data reduction rule sets yield problem kernels
and are conﬂuent. Section 5 concludes with an outlook to future work. Due to
limited space, we defer some proofs and details to the full version of this paper.
2
Basic Concepts of Kernelization and Critical Pair
Analysis
Kernelizations. A parameterized problem can be deﬁned by a set of instances
(x, k), where k is called the parameter [5,9,18]. Let L be a parameterized problem.
A reduction to a problem kernel or kernelization is a transformation via data

196
H. Ehrig et al.
reduction rules of an instance (x, k) to an instance (x′, k′) (the problem kernel
of instance (x, k)), such that
– (x, k) ∈L ⇐⇒(x′, k′) ∈L,
– |x′| ≤g(k) for some arbitrary computable function g depending only on k,
– k′ ≤k, and
– the transformation runs in polynomial time.
We call g(k) the problem kernel of the parameterized problem L.
Critical Pair Analysis in Graph Transformation Theory. The algebraic theory of
graph transformations [6] provides a speciﬁc technique known from term rewrit-
ing systems [13], called critical pair analysis, which has been generalized to graph
transformation systems in [19]. Critical pair analysis supports the veriﬁcation of
local conﬂuence using the software system AGG [1]. The main idea is to show
local conﬂuence not for all pairs of (a possibly inﬁnite number of) transforma-
tion steps G =⇒G1 and G =⇒G2 via rules r1 resp. r2, but only for all critical
pairs. A pair of transformation steps is called a critical pair if it is conﬂicting
in a minimal context in the following sense: The pair G =⇒G1, G =⇒G2 via
r1, r2 is called parallel independent if there are transformation steps G1 =⇒G3,
G2 =⇒G3 via r2, r1 leading to the same G3. A pair is called conﬂicting if it
is not parallel independent, and it has minimal context if each vertex and edge
in G belongs to the match of r1 or r2 in G. For a graph transformation system
with a ﬁnite number of rules based on ﬁnite graphs, there is a ﬁnite number of
critical pairs. All of them can be computed automatically by the graph trans-
formation analysis tool AGG [1]. The Local Conﬂuence Theorem for algebraic
graph transformations [6] implies local conﬂuence of a graph transformation sys-
tem provided that all critical pairs are strictly conﬂuent, where “strictness” is an
additional technical condition for the transformations. The veriﬁcation of strict
conﬂuence for critical pairs can also be supported by AGG and is applied to
data reduction in Sections 3 and 4.
The application of critical pair analysis to data reduction rules, however, is not
yet fully automated. The ﬁrst reason is that the Local Conﬂuence Theorem [6]
based on critical pairs is valid for directed graphs (with parallel edges and loops)
and several other kinds of graphs, but not yet proved for undirected graphs as
considered for data reduction in this paper. The second reason is that data
reduction rules in general are rule schemes in the sense of graph transformation
theory, where rule schemes can be applied to an unbounded number of vertices,
and rules are applied to a constant-size subgraph. Each rule schema corresponds
to a—possibly inﬁnite—set of rules in the sense of [6]. For these reasons, we prove
conﬂuence directly in Sections 3 and 4; in the case of Partial Clique Cover,
the proof is quite complex, based on a large number of case distinctions. These
proofs depend strictly on the speciﬁc rules. It is an interesting challenge for future
work to extend the theory of graph transformations [6]—and the corresponding
tool AGG—to handle also data reduction in a more general way.

Conﬂuence in Data Reduction
197
3
Case Study Clique Cover
We use the well-known NP-hard Clique Cover problem for our ﬁrst case study.
Clique Cover
Instance: An undirected graph G = (V, E) and an integer k ≥0.
Question: Is there a set of at most k cliques in G such that each edge
in E has both its endpoints in at least one of the selected cliques?
For an instance (G, k), we call a set of at most k cliques that covers all edges
a solution. Choosing Clique Cover1 has several reasons: It is a conceptually
simple graph problem, and the best known (theoretical) data reduction rules so
far are easy to understand and also applied in practice [10]. Moreover, Clique
Cover has a kernelization with a size bound of 2k vertices [10,12], and it was re-
cently shown that under standard complexity-theoretic assumptions, this cannot
be improved to a polynomial bound [4].
Kernelization for Clique Cover. For the currently only known kernelization
for Clique Cover with parameter k, the following data reduction rules are
used [10,12].2
Rule 1. Remove isolated vertices, that is, vertices with no neighbors.
Rule 2. If there is an isolated edge, then delete it and decrease k by one.
Two vertices u, v ∈V are called twins if {u, v} ∈E and u and v have exactly
the same neighbors (except for v and u, respectively).
Rule 3. If {u, v} are twins and {u, v} is not an isolated edge, then delete u (that
is, remove it from the vertex set and all incident edges from the edge set).
Theorem 1 ([10,12]). Rules 1 to 3 are correct and yield a problem kernel for
Clique Cover with at most 2k vertices.
Note that for technical correctness of the kernel (as deﬁned in Section 2), we
need to check whether after exhaustive application of Rules 1 to 3 there are
more than 2k vertices left, and if so, the instance is replaced with a small “no”-
instance (for instance, k + 1 disjoint edges). We omit such trivial checks in the
following.
1 Note that in the literature sometimes also covering vertices instead of edges by
cliques is called Clique Cover.
2 We note that [10] uses diﬀerent rules involving “covered edges”, which are equivalent
to the rules presented here if the initial instance does not have covered edges (except
that Rule 3’ from [10] does not treat isolated edges correctly; as already noted in
[12], they require a special case.)

198
H. Ehrig et al.
Conﬂuence of Data Reduction for Clique Cover. We now show that the kernel-
ization rules from Theorem 1 are conﬂuent.
Theorem 2. The set of Rules 1 to 3 for Clique Cover is conﬂuent.
Proof. Clearly, the order of application for Rule 1 and Rule 2 with respect to
any of the three rules is not relevant, since their application does not aﬀect
the applicability of other rules. It remains to show that the relative order of
applications of Rule 3 does not matter.
If we consider two vertices as equivalent when they are twins, we obtain an
equivalence relation on the vertex set. Thus, we can partition the vertex set into
the equivalence classes of this relation, called twin classes. Note that every twin
class forms a clique in the graph. Let the twin graph3 of a graph be a graph with
the twin classes as vertices and an edge between two twin classes if there is an
edge between one vertex from one class and one vertex from the other class.
The twin graph does not change (up to isomorphism) when Rule 3 is applied,
since u and v must be from the same twin class and the rule thus always leaves
at least one vertex in any twin class. Further, Rule 3 is applicable until a twin
class contains exactly one vertex (if it is connected to vertices outside the twin
class) or two vertices (if it is an isolated clique). Since the twin graph and the
number of vertices per twin class uniquely represent a graph up to isomorphism,
we obtain conﬂuence.
⊓⊔
This proof also yields a shortcut to calculate the result of the kernelization,
whose naive calculation would require O(|E| · |V |2) time (The authors of [10]
only state the running time of O(|V |4) for Rules 1 to 3 plus another rule).
Corollary 1. A 2k-vertex kernel for Clique Cover can be found in linear
time.
Proof. From the proof of Theorem 2, we can see that it is suﬃcient to calculate
the twin graph, contract each twin class to a single vertex, and then delete
isolated vertices and edges. Finding the twin graph can be done in linear time [16,
Corollary 7.4], so the kernelization can be done in linear time, too.
⊓⊔
Conﬂuence via Critical Pair Analysis. As pointed out in Section 1, the standard
way to show conﬂuence of a rule set in graph transformation theory [6] is to
construct all critical pairs and to show for each critical pair that it is strictly
conﬂuent. The approach has been shown for directed graphs [6], and we are
conﬁdent that it can also be extended to undirected graphs as considered in
this paper, in particular to data reduction for Clique Cover and Partial
Clique Cover. Note that data reduction rules, like Rule 2, may also change
the parameter k, but this is not essential for conﬂuence and will be disregarded
in this section.
Actually, Rule 3 is a rule scheme in the sense of graph transformation theory,
which can be represented by the following family of rules R3.m for m ≥1:
3 Twin classes and the twin graph have been used before for data reduction under the
names critical cliques and critical clique graph (see e. g. [11]).

Conﬂuence in Data Reduction
199
LHS
u
qqq
...
...
NN
NN
x1
O
O
O
xm
nnn
v
R3.m(u,v)
RHS
x1
P
P
P
P
. . .
xm
mmmm
v
The rule describes the deletion of u. Applying the rule to a graph G means
to ﬁnd an occurrence of the left-hand side LHS in G satisfying N[u] = N[v] =
{u, v, x1, . . . , xm}, and to replace this occurrence by the right-hand side RHS.
For graphs with n vertices, we only have to consider rules Rule 1, Rule 2,
Rule 3.1, . . . , Rule 3.r with r = n −2, because rules with r > n −2 cannot
be applied. Fig. 1 shows the table computed by the AGG tool [1] giving the
number of critical pairs (CP) for each pair of rules and r = 3. Clicking on
an entry in the CP table table (e. g. the highlighted ﬁeld showing 12 minimal
conﬂicts for Rule 3.2 and Rule 3.3 where rule Rule 3.2 is applied ﬁrst), the 12
conﬂicting situations of these two rules are shown in detailed graphical views.
Vertices and edges in the rules (in the bottom of Fig. 1) are numbered to deﬁne
their conﬂicting overlapping situation. We can see one of the 12 conﬂicts in the
overlapping graph P in the upper right part of Fig. 1, where vertex 1 and edges
5, 6 and 8 shall be deleted by Rule 3.2, but vertex 1 and edges 6 and 8 are also
needed for the application of Rule 3.3 which is supposed to delete vertex 4 and
its incident edges.
Fig. 1. CP table for Clique Cover Rules 1 to 3.3, and one critical pair in detail
For each critical pair P1
r1
⇐= P
r2
=⇒P2 of the rule set in Fig. 1, we have shown
strict conﬂuence using AGG, essentially by applying the rules from the rule set
as long as possible to P1 and to P2, leading to reduced
graphs ¯P1 and ¯P2, and showing that they are isomor-
phic, as indicated in the diagram to the right.
P
r1
 jjjjjj
jjjjjj
r2 T
T
T
T
T
T
T
T
T
T
T
T
P1
∗SSSSSS
P2
∗ kkk
kkk
¯P1 ∼= ¯P2

200
H. Ehrig et al.
The critical pairs can be computed automatically, and the reduction sequences
P1
∗
=⇒¯P1, P2
∗
=⇒¯P2, and the isomorphism for ¯P1 and ¯P2 can be checked
interactively using the tool AGG.
4
Case Study Partial Clique Cover
We provide a second, more demanding case study: Partial Clique Cover, a
generalization of Clique Cover where some edges C are annotated as already
covered, and only uncovered edges need to be covered by cliques. Due to space
constraints, we can only sketch our results.
We generalize Rules 1 and 2 in a canonical way.
Rule 4 ([10, Rule 1]). Remove isolated vertices and vertices that are only
incident to covered edges.
Rule 5. If there is an isolated edge, then delete it and, if the edge was not
covered, decrease the parameter by one.
We then adapt Rule 3 as follows:
Rule 6. Let u, v be twins. Mark all edges incident to u as covered if the following
covering conditions hold:4
∀x ∈V \ {u, v} : {u, x} ∈C ⇐⇒{v, x} ∈C
(1)
{u, v} /∈C ⇒∃x ∈V \ {u, v} : {v, x} /∈C.
(2)
Unfortunately, the new rules do not yield a problem kernel for Partial Clique
Cover with respect to the parameter k. In fact, we can show that Partial
Clique Cover is already NP-hard for k = 3, and thus cannot have a problem
kernel unless P = NP. However, we can show a kernel for Partial Clique
Cover with respect to the combined parameter (k, c), where c = |C| is the
number of covered edges.
Theorem 3. Rules 4 to 6 yield a problem kernel for Partial Clique Cover
with at most 2k+c vertices.
The idea of the proof is to show that if a Partial Clique Cover instance has
more than 2k+c vertices, then we can construct a Clique Cover instance, ﬁnd
a data reduction opportunity there using Rules 1 to 3, and using this also ﬁnd
a data reduction for the Partial Clique Cover instance; in this way, we can
use the bounds from Theorem 1.
Next, we claim that the new rules are conﬂuent; the omitted proof uses local
conﬂuence and a somewhat involved case distinction.
Theorem 4. Rules 4 to 6 for Partial Clique Cover are conﬂuent.
The challenge in proving Theorem 4 is that we cannot use the twin graph any-
more as in Theorem 2, since it might be required for an optimal solution to cover
twins with diﬀerent cliques. In addition to a direct proof, for graphs of bounded
size Theorem 4 can be shown using critical pair analysis by AGG [1].
4 Note that if we drop either (1) or (2), then the rule is not correct.

Conﬂuence in Data Reduction
201
5
Discussion and Future Work
Seemingly for the ﬁrst time, our work establishes a fruitful link between graph
transformation theory and the theory of kernelization from parameterized algo-
rithmics. While considering (comparatively simple) kernelizations for edge clique
covering, already several theoretical and technical challenges popped up when
proving conﬂuent kernelizations. We believe that to analyze whether a set of data
reduction rules is conﬂuent is a well-motivated and natural theoretical question
of practical relevance with the potential for numerous opportunities for (inter-
disciplinary) future research between so far unrelated research communities.
As to research questions that are more rooted in graph transformation theory,
it is ﬁrst important to extend the theory of critical pair analysis to undirected
graphs. We are conﬁdent that this works not only for the examples in this paper.
Moreover, it is an important challenge to extend critical pair analysis from rules
considering a constant-size subgraph to so-called rule schemes with unbounded
number of vertices, that is, to transfer the “amalgamation” [3] of rewriting rules
to this new context.
As to research on conﬂuent kernelization rooted more in algorithmics, it ap-
pears to be of general interest to investigate how conﬂuent problem kernels
may help in deriving both upper and lower bounds for problem kernel sizes. In
addition, it remains to study how conﬂuence may contribute to speeding up ker-
nelization algorithms and how the knowledge of having a uniquely determined
problem kernel can help subsequent solution strategies that build on top of the
kernel. Finally, studying conﬂuence of data reduction and kernelization beyond
graph problems, for example for string or set problems, remains a future task.
Acknowledgments. We thank Jiong Guo (Universit¨at des Saarlandes) for
pointing to an NP-hardness proof for Partial Clique Cover already for k ≥3
covering cliques (see Section 4). The third author was supported by DFG project
PABI (NI 369/7-2).
References
1. AGG: Attributed Graph Grammar Tool. TU Berlin (2011),
http://tfs.cs.tu-berlin.de/agg
2. Bodlaender, H.L.: Kernelization: New Upper and Lower Bound Techniques. In:
Chen, J., Fomin, F.V. (eds.) IWPEC 2009. LNCS, vol. 5917, pp. 17–37. Springer,
Heidelberg (2009)
3. B¨ohm, P., Fonio, H.R., Habel, A.: Amalgamation of graph transformations: a syn-
chronization mechanism. Journal of Computer and System Sciences 34, 377–408
(1987)
4. Cygan, M., Kratsch, S., Pilipczuk, M., Pilipczuk, M., Wahlstr¨om, M.: Clique cover
and graph separation: New incompressibility results. Tech. Rep. arXiv:1111.0570v1
[cs.DS], arXiv (2011)
5. Downey, R.G., Fellows, M.R.: Parameterized Complexity. Springer (1999)

202
H. Ehrig et al.
6. Ehrig, H., Ehrig, K., Prange, U., Taentzer, G.: Fundamentals of Algebraic Graph
Transformation. EATCS Monographs in Theoretical Computer Science. Springer
(2006)
7. Ehrig, H., Kreowski, H.J., Montanari, U., Rozenberg, G. (eds.): Handbook of Graph
Grammars and Computing by Graph Transformation. Concurrency, Parallelism
and Distribution, vol. 3. World Scientiﬁc (1999)
8. Ehrig, H., Pfender, M., Schneider, H.: Graph grammars: an algebraic approach. In:
Proc. IEEE Symposium on Switching and Automata Theory, pp. 167–180. IEEE
(1973)
9. Flum, J., Grohe, M.: Parameterized Complexity Theory. Springer (2006)
10. Gramm, J., Guo, J., H¨uﬀner, F., Niedermeier, R.: Data reduction and exact algo-
rithms for clique cover. ACM Journal of Experimental Algorithmics 13, 2.2:1–2.2:15
(2008)
11. Guo, J., Niedermeier, R.: Invitation to data reduction and problem kernelization.
ACM SIGACT News 38(1), 31–45 (2007)
12. Gy´arf´as, A.: A simple lower bound on edge coverings by cliques. Discrete Mathe-
matics 85(1), 103–104 (1990)
13. Huet, G.: Conﬂuent reductions: Abstract properties and applications to term
rewriting systems. Journal of the ACM 27(4), 797–821 (1980)
14. Kneis, J., M¨olle, D., Richter, S., Rossmanith, P.: A bound on the pathwidth of
sparse graphs with applications to exact algorithms. SIAM Journal on Discrete
Mathematics 23(1), 407–427 (2009)
15. van Leeuwen, J. (ed.): Handbook of Theoretical Computer Science. MIT Press
(1990)
16. McConnell,
R.M.:
Linear-time
recognition
of
circular-arc
graphs.
Algorith-
mica 37(2), 93–147 (2003)
17. Newman, M.H.A.: On theories with a combinatorial deﬁnition of equivalence. An-
nals of Mathematics 43(2), 223–242 (1942)
18. Niedermeier, R.: Invitation to Fixed-Parameter Algorithms. Oxford Lecture Series
in Mathematics and Its Applications, vol. 31. Oxford University Press (2006)
19. Plump, D.: Conﬂuence of Graph Transformation Revisited. In: Middeldorp, A., van
Oostrom, V., van Raamsdonk, F., de Vrijer, R. (eds.) Processes, Terms and Cycles:
Steps on the Road to Inﬁnity. LNCS, vol. 3838, pp. 280–308. Springer, Heidelberg
(2005)
20. Rozenberg, G.: Handbook of Graph Grammars and Computing by Graph Trans-
formations: Foundations, vol. 1. World Scientiﬁc (1997)

Highness and Local Noncappability
Chengling Fang1, Wang Shenling1,2 and Guohua Wu1
1 Division of Mathematical Sciences, School of Physical and Mathematical Sciences
Nanyang Technological University, 21 Nanyang Link, Singapore 637371
fang0032@e.ntu.edu.sg, guohua@ntu.edu.sg
2 College of Information Science and Technology, Beijing Normal University
Haidian District, Beijing 100875, P.R. China
wang0362@e.ntu.edu.sg
Abstract. Let a be a nonzero incomplete c.e. degree. Say that a is
locally noncappable if there is a c.e. degree c above a such that no nonzero
c.e. degree below c can form a minimal pair with a, and c is a witness
of such a property of a. Seetapun proved that every nonzero incomplete
c.e. degree is locally noncappable, and Stephan and Wu proved recently
that such witnesses can always be chosen as high2 degrees. This latter
result is optimal as certain classes of c.e. degrees, such as nonbounding
degrees, plus-cupping degrees, etc., cannot have high witnesses. Here, a
c.e. degree is nonbounding if it bounds no minimal pairs, and is plus-
cupping if every nonzero c.e. degree below it is cuppable.
In this paper, we prove that for any nonzero incomplete c.e. degree
a, there exist two incomparable c.e. degrees c, d > a witnessing that
a is locally noncappable, and that c ∨d, the joint of c and d, is high.
This result implies that both classes of the plus-cuppping degrees and
the nonbounding c.e. degrees do not form an ideal, which was proved by
Li and Zhao by two separate constructions.
1
Introduction
Lachlan [4] and Yates [9] proved independently the existence of minimal pairs,
refuting a conjecture of Shoenﬁeld. A c.e. degree is cappable if it is either 0 or a
part of a minimal pair, and a c.e. degree is noncappable, if it is not cappable. The
existence of noncappable degrees was proved by Yates in [9]. In [1], Ambos-Spies,
Jockusch, Shore and Soare proved that all the cappable degrees form an ideal of
the class of c.e. degrees, while all the noncappable degrees form a strong ﬁlter
of the class of c.e. degrees. They also proved that a c.e. degree is noncappable if
and only if it is low-cuppable.
As an variant of noncappable degrees, Seetapun proposed in his thesis [7] the
notion of locally noncappable degrees. Here a c.e. degree a is locally noncappable if
there is a c.e. degree c above a such that no nonzero c.e. degree w below c forms
a minimal pair with a. We say that c witnesses that a is locally noncappable.
Seetapun proved in his thesis [7] that every nonzero incomplete c.e. degree is
locally noncappable. This result was later published in [3] by Giorgi, but with
one Σ3 outcome missing, so Giorgi’s construction is not complete. Recently,
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 203–211, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

204
C. Fang, W. Shenling, and G. Wu
Stephan and Wu [2] proved Seetapun’s result by showing that such witnesses
can always be chosen as high2 degrees. This is optimal as certain classes of c.e.
degrees, such as nonbounding degrees, plus-cupping degrees, etc., cannot have
high witnesses. In this paper, we prove that any nonzero incomplete c.e. degree
can have two incomparable c.e. degrees above it witnessing that it is locally
noncappable and the joint of these two degrees is high.
Theorem 1. For any nonzero incomplete c.e. degree a, there exist two incom-
parable c.e. degrees c, d > a witnessing that a is locally noncappable, and c ∨d,
the joint of c and d, is high.
Obviously, Theorem 1 implies both classes of the plus-cuppping degrees and the
nonbounding c.e. degrees do not form ideals, as c ∨d is high, and hence bounds
a minimal pair, and also noncuppable degrees. These two consequences were
proved by Li and Zhao in [6], by two separate constructions.
Theorem 1 also implies the the class of those degrees bounding no bases of
Slaman triples do not form an ideal, even though these degrees are downwards
closed. In [5], Leonhardi proved the existence of a high2 c.e. degree bounding no
bases of Slaman triples. here a nonzero c.e. degree a is called a base of a Slaman
triple if there are c.e. degrees b and c with c ̸
≤b such that for any w ≤a,
if w ̸
= 0, then c ≤b ∨w. Shore and Slaman proved in [8] that each high c.e.
degree bounds a Slaman triple, and hence a base of such a triple.
Our notation and terminology are quite standard and generally follow Soare
[10]. A parameter p is deﬁned to be fresh at a stage s if p > s and p is the least
number not mentioned so far in the construction.
2
Requirements
Let a be a nonzero and incomplete c.e. degree and A is a c.e. set in a. To
prove Theorem 1, we shall construct two c.e. sets C, D and a p.c. functional Λ
satisfying the following requirements:
He : Tot(e) = limx ΛA⊕C⊕D(e, x),
QC
e : C ̸
= ΦA⊕D
e
,
QD
e : D ̸
= ΦA⊕C
e
,
RC
e : We = ΦA⊕C
e
⇒We ≤T A or ∃Xe ≤T A, We(Xe c.e. and incomputable),
RD
e : Ve = Ψ A⊕D
e
⇒Ve ≤T A or ∃Ye ≤T A, Ve(Ye c.e. and incomputable).
Here {(We, Ve, Φe, Ψe) : e ∈ω} is an eﬀective enumeration of all quadruples
(W, V, Φ, Ψ) of c.e. sets W, V and p.c. functionals Φ, Ψ. The set Tot = {i :
ϕi is total} is a Π0
2-complete set. Let c be the degree of A ⊕C, and d be the
degree of A ⊕D. By the Q-requirements, c and d are two incomparable degrees
above a. The H-requirements ensure that c ∨d is high. By the R-requirements,
both c and d witness that a is locally noncappable.

Highness and Local Noncappability
205
2.1
A QC
e Strategy
A QC
e -strategy α is a standard Sacks coding strategy (here, a QD
e -strategy is
similar). That is, even though A is not in our control, we can still satisfy the
QC
e -requirement by the assumption that A is incomplete. α will run cycles j for
j ∈ω, and all cycles of α deﬁne a functional Ξα jointly. Each cycle j tries to
ﬁnd a number xj such that C(xj) ̸
= ΦA⊕D
e
(xj), and if cycle j fails to make it,
then this cycle will deﬁne ΞA
α (j) = K(j) successfully. As A is incomplete, some
cycle, j say, will ﬁnd an xj such that C(xj) ̸
= ΦA⊕D
e
(xj). Cycle j proceeds as
follows:
(1) Choose xj as a fresh number.
(2) Wait for ΦA⊕D
e
(xj) ↓= 0.
(3) Preserve D⌈ϕe(xj) from other strategies, and deﬁne ΞA
α (j) = K(j) with use
ξα(j) = ϕe(xj). Start cycle j + 1 simultaneously.
(4) Wait for A⌈ϕe(xj) or K(j) to change.
(a) If A⌈ϕe(xj) changes ﬁrst, then cancel all cycles j′ > j and drop the
D-restraint of cycle j to 0. Go back to step 2.
(b) If K(j) changes ﬁrst, then stop cycles j′ > j, and go to step 5.
(5) Put xj into C. Wait for A⌈ϕe(xj) to change.
(6) Deﬁne ΞA
α (j) = K(j) = 1 with use 0, and start cycle j + 1.
Cycle j has two outcomes:
(j, f) : Cycle j waits forever at step 2 or 5.
(The QC
e -strategy is satisﬁed via witness xj in an obvious way.)
(j, ∞) : Cycle j runs inﬁnitely often.
(Cycle j returns from step 4 to 2 inﬁnitely often, and hence ΦA⊕D
e
(xj) di-
verges, the QC
e -strategy is satisﬁed.)
Note that as A is assumed to be incomplete, it is impossible for α to run inﬁnitely
many cycles, with each cycle runs only ﬁnitely often, since otherwise, ΞA
α is
deﬁned as a total function and ΞA
α = K, a contradiction.
α has outcomes (j, ∞) and (j, f) with (j, ∞) < (j, f) and for any j1 < j2,
(j1, ∗) < (j2, †), where ∗, † ∈{∞, f}.
We introduce some notions for convenience when we talk about such cycles.
Say that cycle j acts if it chooses a fresh number xj as its attacker at step 1, or
it enumerates xj into C at step 5. Say that cycle j is active at stage s if at this
stage, when α is visited, α is running cycle j, except the situation that cycle j
is just started at stage s.
2.2
An RC
e Strategy
Assume that a strategy β works on RC
e , we deﬁne the length of agreement
function l(β, s) at stage s to be

206
C. Fang, W. Shenling, and G. Wu
l(β, s) = max{x < s : (∀y < x)[We(y)[s] = ΦA⊕C
e
(y)[s]]},
and the maximum length of agreement function at stage s is deﬁned to be
m(β, s) = max{l(β, t) : t < s and t is a β−stage}.
Say that a stage s is a β-expansionary stage if s = 0 or s is a β-stage with
l(β, s) > m(β, s).
At β-expansionary stages, we shall construct a c.e. set Xe and two p.c. func-
tionals Γβ and Δβ such that if We = ΦA⊕C
e
, then either We ≤T A or Xe is
incomputable with Xe = Γ A
β = ΔWe
β .
β has two outcomes ∞<L f. ∞denotes that β has inﬁnitely many expan-
sionary stages, and f for ﬁnitely many. Below outcome ∞, to ensure that Xe is
incomputable, we need to satisfy the following subrequirements
SC
e,i :
Xe ̸
= ϕi
for all i.
In the construction, we always deﬁne the use δβ(x) = x, which will ensure
that ΔWe
β
is totally deﬁned. When Γ A
β (x) is deﬁned at a β-expansionary stage s,
we shall deﬁne γβ(x)[s] > ϕe(x)[s], and as a consequence, when ϕe(x) diverges,
γβ(x) also diverges.
2.3
An SC
e,i Strategy
Assume that η is an SC
e,i-strategy below the ∞outcome of an RC
e -strategy β,
and η has two parameters xη and zη. The basic η-strategy is almost the same as
the one given in [2]:
(1) Pick xη as a fresh number.
(2) Wait for ϕi(xη) ↓= 0.
(3) Assign zη = xη and let xη be undeﬁned. Go back to step 1, and wait for A
to change below zη. (If such a change occurs, then Γ A
β (zη) is undeﬁned.)
(4) (Open a gap)
Create a link between β and η, wait for We to change below zη.
(5a) (Close a gap successfully)
If We has a change below zη, then η performs the diagonalization by
putting zη into Xe. Cancel the link to close the gap. In this case, η is
satisﬁed forever.
(5b) (Close a gap unsuccessfully)
If We does not have a change below zη, then cancel the link to close the
gap. Impose a restraint on C. Request that γβ(zη) be deﬁned greater than
ϕe(zη).
We shall deﬁne a p.c. functional Θη such that if η opens inﬁnitely many diﬀerent
gaps (each gap associated with diﬀerent value of zη), then ΘA
η is totally deﬁned
and computes We correctly, and hence η provides a global win for β.

Highness and Local Noncappability
207
In the construction, we deﬁne Θη in step (5b), i.e., when a gap for zη is closed
unsuccessfully, for all x ≤zη, if ΘA
η (x) ↑, then we deﬁne ΘA
η (x) = We(x) with
θη(x) = ϕe(x).
The period between a stage at which a gap is closed unsuccessfully and the
stage at which the next gap is open is called a cogap. Note that, when a gap is
closed unsuccessfully, a restraint is imposed on C to prevent We from changing
below zη. But, as A is given, A can change below ϕe(zη) during the cogaps, which
may change We below zη. In other words, during a cogap for zη, if We changes
on some number x ≤zη, then DeltaWe
β (zη) and Γ A
β (zη) are both undeﬁned and η
is allowed to enumerate zη into Xe to satisfy SC
e,i. In case that We does not have
such a change, then at the next β-expansionary stage, we shall deﬁne Γ A
β (zη)
again, with a big γ-use. Here comes a special feature of this strategy: a gap
can be closed and reopen, by changes of A, inﬁnitely many times, and this fea-
ture corresponds to a divergence outcome, as ΦA⊕C
e
diverges on some argument
below zη.
An η-strategy has four outcomes s <L g <L d <L w, the outcome s denotes
the case that a gap is closed successfully; the outcome g denotes the case that η
opens inﬁnitely many diﬀerent gaps (each gap associated with diﬀerent value of
zη, and is closed eventually), we can show that if We = ΦA⊕C
e
then We = ΘA
η ,
a global win for β; the outcome d denotes the case that η opens only ﬁnitely
many diﬀerent gaps and for the last gap, A changes below the corresponding
use ϕe(zη) inﬁnitely often, and hence ΦA⊕C
e
(zη) ↑, another global win for β; the
outcome w denotes the case that η waits for ϕi(xη) ↓= 0 forever for some xη.
We shall reconsider SC
e,i-strategies after the H-strategies are introduced.
An RD
e -strategy is similar to an RC
e -strategy and hence we have the following
subrequirements
SD
e,i : Ye ̸
= ϕi
for all i.
An SD
e,i-strategy is similar to an SC
e,i-strategy.
2.4
An He Strategy
Assume τ works on He. We deﬁne the length of convergence function l(τ, s) at
stage s to be
l(τ, s) = max{x < s : (∀y < x)[ϕe(y)[s] ↓]},
and the maximum length of convergence function at stage s is deﬁned to be
m(τ, s) = max{l(τ, t) : t < s and t is a τ-stage}.
Say that a stage s is a τ-expansionary stage if s = 0 or s is a τ-stage with
l(τ, s) > m(τ, s). τ has two outcomes ∞<L f. If τ has ∞outcome, i.e., ϕe is
total, then we shall deﬁne ΛA⊕C⊕D(e, x) = 1 for almost all x; if τ has f outcome,
i.e., ϕe is not total, then we shall deﬁne ΛA⊕C⊕D(e, x) = 0 for almost all x.

208
C. Fang, W. Shenling, and G. Wu
Note that Λ is a global p.c. functional built by us through the whole construc-
tion. ΛA⊕C⊕D(e, x) is undeﬁned automatically if some number ≤λ(e, x) is enu-
merated into C or D. As a consequence, λ(e, x) may be lifted when ΛA⊕C⊕D(e, x)
is redeﬁned later.
If we deﬁned ΛA⊕C⊕D(e, x) = 0 under the f outcome at a previous stage,
and now we see ϕe converges on more arguments, i.e., τ changes its outcome
from f to ∞, then we want to (re)deﬁne ΛA⊕C⊕D(e, x) = 1, but ﬁrst we need to
undeﬁne all the previous ΛA⊕C⊕D(e, x) = 0. So generally, at a τ-expansionary
stage, we shall put the λ(e, x) use into C to undeﬁne ΛA⊕C⊕D(e, x) = 0, and
we (re)deﬁne ΛA⊕C⊕D(e, x) = 1 with use λ(e, x) as −1 at τ-expansionary stages
since we never want to undeﬁne it later. This means that we only care about the
λ(e, x) use deﬁned under the f outcome.
Actually, in the construction, we need to consider the restraint imposed on τ,
so we shall deﬁne boundary bd(τ) of τ (playing a role of the restraint) as follows:
when τ is visited for the ﬁrst time, we deﬁne bd(τ) to be a fresh number, whenever
τ is initialized, we shall redeﬁne bd(τ) as a fresh number. At a τ-expansionary
stage s, we shall put the λ(e, x) use with bd(τ) < λ(e, x) ≤s into C to undeﬁne
ΛA⊕C⊕D(e, x) = 0.
One may ask whether it is true that the He-strategy τ only enumerates the
λ-uses only into C when it needs to undeﬁne ΛA⊕C⊕D(e, x) = 0. The answer
is “no”, as, for example, when A is of nonbounding degree or of plus-cupping
degree, then A⊕C cannot have high degree. Here we ﬁx C just for simplicity, and
in the actual construction, we also need to put the λ(e, x) use into D sometimes,
based on which gap is open. We shall explain below the idea of putting these
λ-uses into C and D alternatively, to make A ⊕C ⊕D of high degree.
2.5
Interaction between Strategies
Now we consider the interaction between the highness strategies and the gap-
cogap argument used in S-strategies.
If β is an RC
e -strategy working below the ∞outcome of an H-strategy τ,
i.e., τ⌢⟨∞⟩⊂β, then, in the deﬁnition of β-expansionary stages, we only use
β-believable computations, where a computation ΦA⊕C
e
(x)[s] is β-believable, if
for each n with bd(τ) < n ≤s, the λ(e′, n)[s] is already deﬁned as −1, where we
assume that τ is an He′-strategy.
A nontrivial case is when an He0-strategy τ works between β and a substrategy
η with
β⌢⟨∞⟩⊂τ ⊆τ ⌢⟨∞⟩⊂η.
Suppose that η opens a gap and a link between β and η is created at some β-
expansionary stage, s0 say. At the next β-expansionary stage s1 (β-believable com-
putations are considered), and suppose this gap is closed unsuccessfully, then the
link is cancelled, and during the cogap for η after stage s1, no number less than s1
can be put into C (to prevent We from changing). It is this restraint that forces us
to enumerate λ-uses into D. That is, when β closes the gap for η at stage s1, for all
the H-strategies τ with β⌢⟨∞⟩⊂τ ⊆τ ⌢⟨∞⟩⊂η, we shall put λ(e(τ), x) into D

Highness and Local Noncappability
209
to lift the λ-uses if needed. Thus, after stage s1, all the numbers enumerated into
C in the future by such H-strategies are greater than s1.
We now consider details of such interactions. Recall that we a gap is closed,
we do extend the deﬁnitions of Γ A
β and ΔWe
β , and if the gap is closed unsuccess-
fully, We does not change below zη, but A changes below ϕe(zη). Without loss of
generality, we only consider the case when a gap is closed (unsuccessfully) and
reopen inﬁnitely often. Than is, during each cogap, A changes on small numbers
and reopens the same gap. In this case, in [2], when a gap is reopen, η is visited
immediately with outcome d. It is this feature that is consistent with construc-
tion of a high2 degree. However, for our construction, we need to make some
modiﬁcations, as we want to make A ⊕C ⊕D high.
Here is the idea: After a gap for zη is closed unsuccessfully by β, at the next
β-expansionary stage, if A changes below ϕe(zη), but We does not change below
zη, we shall not let β reopen this gap immediately. Instead, if η is visited at this
stage, then we let η reopen this gap. That is, in the construction, at an η-stage,
if there is a gap which was closed unsuccessfully by β at a previous stage, and if
A changes below the corresponding use ϕe(zη) at this η-stage, then we shall let η
reopen this gap. Note that, in the construction, when a gap is reopened, it must
be reopened by η, not by β. This will avoid the problem above, since if a gap
can be closed unsuccessfully and reopened inﬁnitely often, then the H-strategies
between β and η must have outcome the same as the one seen at η.
Note that, in a cogap for η, it may happen that we see We change below zη
at a β-expansionary stage, but Γ A
β (zη) ↓at this stage. We now describe whey
this can happen in detail. Suppose that a gap for zη is closed unsuccessfully
by β at a β-expansionary stage s1, and we deﬁne ΔWe
β (zη) and Γ A
β (zη) at the
end of stage s1, with γβ(zη)[s1] > ϕe(zη)[s1]. At the next β-expansionary stage,
Γ A
β (zη) ↑, but ΔWe
β (zη) ↓. Assume that η is not visited at stage s2 (and hence
η cannot reopen this gap at stage s2), then we deﬁne Γ A
β (zη) at the end of
stage s2, with γβ(zη)[s2] > ϕe(zη)[s2]. Assume that ϕe(zη)[s2] > ϕe(zη)[s1], and
hence the H-strategies between β and η may enumerate some small λ-uses into
C, changing the computation ΦA⊕C
e
(zη)[s2]. Thus, at the next β-expansionary
stage s3, it may happen that ΔWe
β (zη) ↑, but Γ A
β (zη) ↓, i.e., We changes below
zη, but A does not change below ϕe(zη). In this case, we can not let η perform
the diagonalization at stage s3 even though We changes below zη.
On the other hand, at an η-stage, if a gap (which was closed unsuccessfully
by β at a previous stage) can not be reopened, i.e., A does not change below
ϕe(zη), then we shall consider a new value of zη (if any) for η and check whether
we can open a new gap. If we can open a new gap, then we say that the existing
gap is closed completely.
The same argument applies when β is an QD-strategy.
Finally, we consider the interaction between an RC-strategy and an RD-
strategy. Suppose that an RD
e -strategy β′ works between an RC
e′-strategy β and
an SC
e′,i′-strategy η with
β⌢⟨∞⟩⊂β′ ⊂β′⌢⟨∞⟩⊂η ⊂η⌢⟨O⟩⊂τ ⊂τ ⌢⟨∞⟩⊂η′,

210
C. Fang, W. Shenling, and G. Wu
where O ∈{g, d}, τ is an H-strategy and η′ is an SD
e,i-strategy and 0 ≤e′ ≤e.
Then we may have that a gap is open (or reopened) for η and creates a link
between β and η and η′ reopens a gap and creates a link between β′ and η′ at
the same stage, s0 say. That is, we have two crossed links (β, η) and (β′, η′) at
stage s0. At the next β-expansionary stage s1, suppose that β closes the gap for
η unsuccessfully and cancels the link (β, η), it requires that all the H-strategies
with ∞outcome between β and η enumerate the λ-uses into set D to lift λ-uses
if needed to protect C. And, η imposes a C-restraint after stage s1 till the stage,
s2 say, at which the next gap for η is open. But, during this cogap for η, we shall
travel the link (β′, η′) created at stage s0. Suppose that we travel the link (β′, η′)
at stage t (s1 < t < s2) say, and β′ closes the gap for η′ unsuccessfully, thus it
requires that all the H-strategies with ∞outcome between β′ and η′ enumerate
the λ-uses into set C to lift λ-uses if needed to protect D. In particular, suppose
that the H-strategy τ is required to enumerate the λ-uses into set C to lift λ-
uses at stage t, these λ-uses may be less than s1. But, η does not allow small
numbers (≤s1) to be enumerated into C before stage s2. So η (i.e., β) injures the
satisfaction of β′. Thus, such crossed links (β, η) and (β′, η′) should be avoided.
In the construction, we shall use a backup strategy to deal with this. That
is, we shall put a backup strategy ˆβ below η⌢⟨O⟩to try to satisfy the RD
e -
requirement. Therefore, in the construction, there will never be two crossed links
(β, η) and (β′, η′) as above at the same stage.
Actually, in a gap-cogap argument, the crossed links need to be avoided gener-
ally. So, in our construction, suppose that we have β⌢⟨∞⟩⊂β′ ⊂β′⌢⟨∞⟩⊂η,
where β can be an RC
e -strategy or RD
e -strategy, β′ can be an RC
e′-strategy or
RD
e′-strategy (e ≤e′), and η is a substrategy of β. Then, when η has outcome g
or d, we shall say that β′ becomes inactive or injured, and we need to arrange a
back-up strategy for β′ under this outcome. Moreover, it’s not hard to see that,
for a ﬁxed e, there are at most ﬁnitely many backup RC
e -strategies (RD
e -strategy
is similar) on any path of the construction tree and the longest RC
e -strategy is
responsible to satisfy the RC
e requirement.
Note that, we may have two nested links (β, η) and (β′, η′) at the same stage
in the construction, where β is an RC
e -strategy and β′ is an RD
e′-strategy, and
η is an SC
e,i-strategy and η′ is an SD
e′,i′-strategy with β⌢⟨∞⟩⊂β′ ⊂β′⌢⟨∞⟩⊂
η′ ⊂η′⌢⟨O⟩⊂η, where O ∈{g, d}. That is, we may have that a gap is open
(or reopened) for η′and creates a link between β′ and η′ and η reopens a gap
and creates a link between β and η at the same stage, s0 say. At the next β-
expansionary stage s1, suppose β closes the gap for η unsuccessfully and cancels
the link (β, η), so it requires that the H-strategies with ∞outcome between β
and η enumerate the λ-uses into set D to lift λ-uses if needed. (The enumeration
of numbers into D at stage s1 may have eﬀect on the computation involved in
the gap for η′, but this is allowed since the gap for η′ is open at stage s1.) η
will impose a C-restraint below s1 after stage s1 till the stage, s2 say, at which
the next gap for η is open. But, before stage s2, i.e., during this cogap for η, we
shall travel the link (β′, η′) created at stage s0 and β′ may close the gap for η′
unsuccessfully and so it requires that the H-strategies with ∞outcome between

Highness and Local Noncappability
211
β′ and η′ enumerate the λ-uses into set C to lift λ-uses if needed. Note that such
λ-uses are already lifted at stage s1 and hence must be greater than s1, so we
can enumerate them into C and there is no conﬂict.
This completes the description of our strategies for Theorem 1, and the pos-
sible interactions between them. A full construction can proceed on a priority
tree, and Theorem 1 is proved.
Acknowledgements. G. Wu was partially supported by NTU grant RG37/09,
M52110101 and grant MOE2011-T2-1-071 from MOE of Singapore.
References
1. Ambos-Spies, K., Jockusch Jr., C.G., Shore, R.A., Soare, R.I.: An algebraic de-
composition of the recursively enumerable degrees and the coincidence of several
degree classes with the promptly simple degrees. Trans. Amer. Math. Soc. 281,
109–128 (1984)
2. Stephan, F., Wu, G.: Highness, nonboundings and presentations (to appear)
3. Giorgi, M.B.: The computably enumerable degees are locally non-cappable. Arch.
Math. Logic 43, 121–139 (2004)
4. Lachlan, A.H.: Lower bounds for pairs of recursively enumerable degrees. Proc.
London Math. Soc. 16, 537–569 (1966)
5. Leonhardi, S.D.: Nonbounding and Slaman triples. Ann. Pure Appl. Logic 79, 139–
163 (1996)
6. Li, A., Zhao, Y.: Plus cupping degrees do not form an ideal. Science in China, Ser.
F. Information Sciences 47, 635–654 (2004)
7. Seetapun, D.: Contributions to Recursion Theory, Ph.D. thesis, Trinity College
(1991)
8. Shore, R.A., Slaman, T.A.: Working below a high recursively enumerable degree.
J. Symb. Logic 58, 824–859 (1993)
9. Yates, C.E.M.: A minimal pair of recursively enumerable degrees. J. Symbolic
Logic 31, 159–168 (1966)
10. Soare, R.I.: Recursively Enumerable Sets and Degrees. Perspectives in Mathemat-
ical Logic. Springer (1987)

Turing Progressions and Their Well-Orders
David Fern´andez Duque1 and Joost J. Joosten2
1 Departamento de Filosof´ıa y L´ogica y Filosof´ıa de la Ciencia, Universidad de
Sevilla, Calle Camilo Jos´e Cela, sin n´umero, Sevilla 41018, Spain
davidstofeles@gmail.com
2 Departamento de L´ogica, Hist´oria i Filosoﬁa de la Ci´encia, Universitat de
Barcelona, Montalegre, 6, 08001 Barcelona, Catalonia, Spain
jjoosten@ub.edu
Abstract. We see how Turing progressions are closely related to the
closed fragment of GLP, polymodal provability logic. Next we study nat-
ural well-orders in GLP that characterize certain aspects of these Turing
progressions.
1
Turing Progressions and Modal Logic
G¨odel’s Second Incompleteness Theorem tells us that any sound recursive the-
ory that is strong enough to code syntax will not prove its own consistency.
Thus, adding Con(T ) to such a theory T will yield a strictly stronger theory.
Turing took up this idea in his seminal paper [6] to consider recursive ordinal
progressions of some recursive sound base theory T :
T0
:= T ;
Tα+1 := Tα + Con(Tα);
Tλ
:= 
α<λ Tα
for limit λ.
As we shall see, poly-modal provability logics turn out to be suitably well
equipped to talk about Turing progressions. These logics have modalities [n]
that shall be interpreted as “provable in EA using all true Πn sentences” ab-
breviated [n]EA. By EA we denote Elementary Arithmetic which is a formal
arithmetic theory axiomatized by the recursive equations for successor, addition
and multiplication, by open induction together with an axiom stating the total-
ity of exponentiation. Often we shall not distinguish a modal formula from its
arithmetical interpretation.
We recall ([5]) that the provability logic of any Σ1-sound theory extending
EA is G¨odel L¨ob’s provability logic GL as deﬁned below. Various mathematical
statements can be expressed within GL like G¨odel’s Second Incompleteness The-
orem: 3⊤→32⊤. It is also not hard to see that ﬁnite Turing progressions are
deﬁnable in GL as Tn is provably equivalent to T + 3n
T ⊤. Transﬁnite progres-
sions are not expressible in the modal language with just one modal operator.
However, using stronger provability predicates provides a way out (see [3]):
Proposition 1. T + ⟨n + 1⟩T ⊤is a Πn+1 conservative extension of
T + {⟨n⟩k
T ⊤| k ∈ω}.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 212–221, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Turing Progressions and Their Well-Orders
213
The provability behavior of these mixed modalities is fully described by what
we call GLPω. We give a more general deﬁnition GLPΛ for any ordinal Λ.
Deﬁnition 1. For Λ an ordinal, the logic GLPΛ is the propositional normal
modal logic that has for each α < Λ a modality [α] and is axiomatized by the
following schemata:
[α](χ →ψ) →([α]χ →[α]ψ)
[α]([α]χ →χ) →[α]χ
⟨α⟩ψ →[β]⟨α⟩ψ
for α < β,
[α]ψ →[β]ψ
for α ≤β.
The rules of inference are Modus Ponens and necessitation for each modality:
ψ
[α]ψ. By GLP we denote the class-size logic that has a modality [α] for each
ordinal α and all the corresponding axioms and rules. GL refers to GLP1.
As suggested by Proposition 1, for the sake of Turing progressions a particular
interest lies in GLP0
Λ, the closed fragment – that is, the modal formulas that have
no propositional variables but rather just ⊥and ⊤– of these GLPΛ. We shall call
iterated consistency statements in this closed fragments worms in reference to
the heroic worm-battle, a variant of the Hydra battle (see [1]).
Deﬁnition 2 (Worms, S, Sα). By S we denote the set of worms of GLP which
is inductively deﬁned as ⊤∈S and A ∈S ⇒⟨α⟩A ∈S. Similarly, we inductively
deﬁne for each ordinal α the set of worms Sα where all ordinals are at least α
as ⊤∈Sα and A ∈Sα ∧β ≥α ⇒⟨β⟩A ∈S.
We shall denote worms by roman uppercase letters like A, B, . . . and often we
associate worms with the strings of ordinals occurring in them whence we can
concatenate worms. In denoting worms we might use any hybrid combination
between the formal deﬁnition and its associated string. For example, we might
equally well write ω0ω, as ⟨ω⟩0ω, or ⟨ω⟩⟨0⟩⟨ω⟩⊤.
The next easy lemma ([2], [4]) is the basis of most of our reasoning and we
shall use it often in the remainder of this paper without explicit mention.
Lemma 1
1. GLP ⊢AB →A
2. For a closed formula φ and a worm B, if β < α, then
GLP ⊢(⟨α⟩φ ∧⟨β⟩B) ↔⟨α⟩(φ ∧⟨β⟩B);
3. For a closed formula φ and a worm B, if β < α, then
GLP ⊢(⟨α⟩ϕ ∧[β]B) ↔⟨α⟩(ϕ ∧[β]B);
4. If A ∈Sα+1, then GLP ⊢A ∧⟨α⟩B ↔AαB;
5. If A, B ∈Sα and GLP ⊢A ↔B, then
GLP ⊢AαC ↔BαC.
Worms can be conceived as the backbone of GLP0. It is known that each closed
formula of GLP is equivalent a Boolean combination of worms. Moreover, a de-
cision procedure for GLP0 proceeds via a reduction to worms. Also, there are
various important generalizations of Proposition 1 in terms of worms. In partic-
ular (as in Prop. 1, there are some technical conditions on T , see [3] ):

214
D.F. Duque and J.J. Joosten
Proposition 2. For each ordinal α < ϵ0 there is some GLPω-worm A such that
T + A is Π1 equivalent to Tα.
To get generalizations of this lemma beyond ϵ0 one should consider more than ω
modalities. In particular [α] should be read as “provable in EA together with all
true hyperarithmetical sentences of level α”. This paper focusses on the modal
calculus involved in such generalizations.
2
Omega Sequences
We deﬁne an order <α on Sα by A <α B :⇔GLP ⊢B →⟨α⟩A. In [2] and [4] it is
shown that <α deﬁnes a well-order on Sα modulo provably equivalence. We can
consider the ordering <α also on the full class S. As we shall see <α is no longer
linear on S. However, we shall see that it is still well-founded. Anticipating this,
we can deﬁne Ωα(A), the generalized <α order-type of a worm A.
Deﬁnition 3. Given an ordinal ξ and a worm A, we deﬁne a new ordinal Ωξ(A)
by induction on <ξ by Ωξ(A) = supB<ξA(Ωξ(B)+). Likewise, we deﬁne oξ(A) =
sup{oξ(B) + 1 | B ∈Sξ ∧B <ξ A}.
With this, we can assign to each worm A a sequence of order-types Ω(A) for
the sequence (Ωξ(A))ξ∈On.
To link back to the Turing progressions we mention here that the Ω0(A)
corresponds to the α from Proposition 2 and the Ωξ(A) for ξ > 0 correspond
to further generalizations of Proposition 2. For worms that contain only natural
numbers this has been established as stated in Proposition 2 and it is conjectured
to hold also for worms containing larger ordinals. Thus, in the remainder of this
paper it is good to bear in mind that the omega sequences have important
proof-theoretic content.
In further sections we shall show how to calculate Ωξ(A) for given ξ and A.
In the current section we shall see how questions about Ωξ can be recursively
reduced to questions about oζ. For this reduction we need the syntactical deﬁ-
nitions of head and remainder.
Deﬁnition 4. Let A be a word. By hξ(A) we denote the ξ-head of A. Recur-
sively: hξ(ϵ) = ϵ, and hξ(ζ ⋆A) = ζ ⋆hξ(A) if ζ ≥ξ and hξ(ζ ⋆A) = ϵ if
ζ < ξ. Likewise, by rξ(A) we denote the ξ-remainder of A: rξ(ϵ) = ϵ, and
rξ(ζ ⋆A) = rξ(A) if ζ ≥ξ and rξ(ζ ⋆A) = ζ ⋆A if ζ < ξ.
We obviously have A = hξ(A) ⋆rξ(A) for all ξ and A and, clearly, over GLP,
hξ(A) ⋆rξ(A) and hξ(A) ∧rξ(A) are equivalent as the ﬁrst symbol of rξ(A) is
less than ξ and hξ(A) ∈Sξ (see Lemma 1).
Lemma 2. (A →⟨ξ⟩B) ⇔[(hξ(A) →⟨ξ⟩hξ(B)) ∧(A →rξ(B))].
Proof. “⇒” Clearly, B ↔hξ(B) ∧rξ(B) whence A →rξ(B). Likewise A ↔
hξ(A) ∧rξ(A). As hξ(A), hξ(B) ∈Sξ we know that either hξ(A) = hξ(B),

Turing Progressions and Their Well-Orders
215
hξ(B) →⟨ξ⟩hξ(A), or hξ(A) →⟨ξ⟩hξ(B). By assumption A →⟨ξ⟩B whence
A →⟨ξ⟩hξ(B) ∧rξ(B).
Suppose now hξ(A) = hξ(B). Then, hξ(A)∧rξ(A) →⟨ξ⟩hξ(A)∧rξ(A) whence
also
hξ(A) ∧rξ(A) →⟨ξ⟩(hξ(A) ∧rξ(A)).
The latter is equivalent to A →⟨ξ⟩A which contradicts the ireﬂexivity of <ξ.
Likewise, the assumption that hξ(B) →⟨ξ⟩hξ(A) contradicts the reﬂexivity
of <ξ and we conclude that hξ(A) →⟨ξ⟩hξ(B).
“⇐” This is the easier direction.
A ↔hξ(A) ∧rξ(A)
→⟨ξ⟩hξ(B) ∧rξ(B)
→⟨ξ⟩(hξ(B) ∧rξ(B))
→⟨ξ⟩B.
Note that this lemma indeed recursively reduces the general <ξ question between
to words, to the <ξ questions between words in Sξ. Note that <ξ is not tree-like.
For example, we see that both 011 <1 10111 <1 1111 and 011 <1 11011 <1 1111
while 10111 and 11011 are <1 incomparable.
3
Basic Properties of Omega Sequences
In these ﬁnal sections we give a full characterization of the sequences Ω(A). That
is, we shall determine for given A each of the values Ωξ(A) and shall classify at
what coordinates ξ the Ω(A) sequence changes value. Clearly, Ω(A) deﬁnes a
weakly decreasing sequence of ordinals.
Lemma 3. For ξ < ζ we have that Ωξ(A) ≥Ωζ(A).
Proof. In general we have for ξ < ζ that A →⟨ζ⟩B implies A →⟨ξ⟩B. Thus,
any <ζ sequence is automatically also a <ξ sequence.
Lemma 4. Ωξ(A) = oξ(hξ(A))
Proof. Suppose A0 <ξ A1 <ξ . . . <ξ A, then
hξ(A0) <ξ hξ(A1) <ξ . . . <ξ hξ(A)
by Lemma 2 whence Ωξ(A) ≤oξhξ(A).
On the other hand, if B <ξ hξ(A), then hξ(A) →⟨ξ⟩B. But as A ↔hξ(A) ∧
rξ(A) we also have A →⟨ξ⟩B. Consequently oξhξ(A) ≤Ωξ(A).
Corollary 1. The Ω(A) sequence has a maximal non-zero coordinate. In par-
ticular, the maximal non-zero coordinate is given by ΩFirst(A)(A), where First(A)
is the ﬁrst element of the non-empty word A.
Proof. Clearly, hFirst(A)(A) ̸= ϵ whence by Lemma 4, ΩFirst(A)(A) ̸= 0. On the
other hand, for ξ > First(A), clearly hξ(A) = ϵ whence Ωξ(A) = 0.

216
D.F. Duque and J.J. Joosten
It is good to have reduced Ωξ(A) to oξ(A) as in [2], and [4] a full calculus for
it is given. Let e0α := −1 + ωα and let eα denote the Veblen progression based
on e0. That is, each eα enumerates the ordinals which are simultaneous ﬁxed
points of all the eβ for β < α. We deﬁne e0α := α and eωξ+ζ = eξ ◦eζ whenever
ζ < ωξ + ζ. Further, we deﬁne ξ↑ζ := ξ + ζ and for ξ < ζ we deﬁne ξ↓ζ to be the
unique ordinal such that ξ↑(ξ↓ζ) = ζ. These operations are naturally extended
to worms by simultaneously applying them to all occurrences of ordinals.
Lemma 5
1. o(0n) = n;
2. If A = A10 . . . 0An, then o(A) = ωo1(An) + . . . + ωo1(A1);
3. For A ∈Sξ we have oξ(A) = o(ξ ↓A);
4. o(ξ ↑A) = eξo(A).
4
Successor Coordinates
First, let us compute Ωξ+1(A) in terms of Ωξ(A). By ℓα we denote the unique
β such that α = α′ + ωβ for α > 0. We deﬁne ℓ0 := 0.
Lemma 6. Given an ordinal ξ and a worm A,
oξ+1hξ+1hξ(A) = ℓoξhξ(A).
Proof. We write hξ(A) as A0ξ . . . ξAn. Clearly, hξ+1(hξ(A)) = A0. We shall now
see that ℓ(oξ(hξ(A))) = oξ+1(A0).
To this end, we observe that
oξhξ(A) = oξ(A0ξ . . . ξAn)
= o

(ξ↓A0)0 . . . 0(ξ↓An)

= ωo1(ξ↓An) + . . . + ωo1(ξ↓A0)
= ωoξ+1(An) + . . . + ωoξ+1(A0)
Consequently ℓoξhξ(A) = oξ+1(A0), as desired.
Now we are ready to describe the relation between successor coordinates of the
Ω(A) sequence.
Theorem 1. Ωξ+1(A) = ℓΩξ(A)
Proof.
Ωξ+1(A) = oξ+1hξ+1(A)
by Lemma 2
= oξ+1hξ+1hξ(A) by Lemma 6
= ℓoξhξ(A)
= ℓΩξ(A)
by Lemma 2.
Theorem 1 tells us what the relation between successor coordinates of Ω(A) is.
However, it does not directly tell us when successor coordinates are diﬀerent. If
Ωξ(A) is a ﬁxed point of ζ →ωζ then Ωξ(A) = Ωξ+1(A).

Turing Progressions and Their Well-Orders
217
5
Equal Coordinates
Theorem 2 below gives us a characterization of when diﬀerent coordinates attain
diﬀerent or equal values. Before we can state and prove this theorem we need
some notation and back-ground on Cantor Normal Forms (CNFs).
For α ∈On we deﬁne Nα and the syntactic operation CNF(α) := Nα
i=1 ωξi to
be the unique CNF expression of α. Next, we deﬁne for an ordinal α the set of
its Cantor Normal Form Approximation as the set of partial sums of CNF(α),
that is, if CNF(α) = Nα
i=1 ωξi, then
CNA(α) :=
 k

i=1
ωξi : 0 ≤k ≤Nα

.
We also deﬁne the Cantor Normal Form Projection of some ordinal ζ on another
ordinal ξ as follows:
CNP(ζ, ξ)
:=
max{ξ′∈CNA(ξ) | ξ′ ≤ζ}.
Note that CNP(ζ, ξ) is deﬁned for all ζ, ξ ∈On by setting max ∅= 0.
For α, β, γ ∈On we deﬁne
α ∼γ β
:⇔
CNP(α, γ) = CNP(β, γ).
In words, α ∼γ β whenever there is no partial sum of the CNF of γ that falls in
between α and β.
The just-deﬁned notions of CNA(ξ), CNP(ζ, ξ) and α ∼γ β are needed to
characterize the ξ↓ζ operation.
Lemma 7
1. ∀ζ≤ξ ζ↓ξ = CNP(ζ, ξ)↓ξ;
2. ∀ζ≤ξ∃!η∈CNA(ξ) ζ↓ξ = η↓ξ;
3. For ξ, ζ ≤η, we have
ξ↓η = ζ↓η ⇔ξ ∼η ζ.
Proof. (1.) We consider ζ ≤ξ. Now let η be shorthand for max{η′ ∈CNA(ξ) |
η′ ≤ζ} = CNP(ζ, ξ). The claim is that ζ↓ξ = η↓ξ. Let CNF(ξ) = Nξ
i=1 ωξi.
As η = k
i=1 ωξi for some k ≤Nξ, we see that
η↓ξ =
Nξ

i=k+1
ωξi
for k < Nξ and η↓ξ = 0 for k = Nξ. We now claim that ζ + (η↓ξ) = ξ so that
ζ↓ξ = η↓ξ follows from the fact that
∀ζ<ξ ∃!δ ζ + δ = ξ.

218
D.F. Duque and J.J. Joosten
We may assume ζ > η otherwise ζ + (η↓ξ) = ξ is trivial.
Thus,
η =
k

i=1
ωξi < ζ ≤
k+1

i=1
ωξi.
As by the deﬁnition of η we see that ζ ≤k+1
i=1 ωξi cannot be an equality whence
η =
k

i=1
ωξi < ζ <
k+1

i=1
ωξi.
Thus, η ∈CNA(ζ) and ζ + Nξ
i=k+1 ωξi = ξ, whence
Nξ

i=k+1
ωξi = ζ↓ξ =
k

i=1
ωξi = η↓ξ.
(2.) Follows from (1.) once we realize that for diﬀerent η and η′ both in CNA(ξ)
we have η↓ξ ̸= η′↓ξ.
(3). From the proof of (1.) we see that
ξ↓η = ζ↓η ⇔max{η′∈CNA(η) | η′ ≤ξ} = max{η′∈CNA(η) | η′ ≤ζ}
where the latter is precisely the deﬁnition of ξ ∼η ζ.
Once we have this lemma to characterize the ξ↓ζ operation, we are armed to
prove a characterization for when two coordinates in Ω(A) are equal. But ﬁrst
we need a deﬁnition of when a worm A is in Beklemishev Normal Form (BNF).
Recursively we say that the empty worm ϵ ∈BNF, and if Ai ∈Sξ+1∩BNF, with
Ai ≥ξ+1 Ai+1, then Anα . . . αA1 ∈BNF. It is easy to see that if a worm is in
BNF, then so are its head and remainder. From [2], and [4] we know that the
set S∩BNF is well-ordered by <0 and that o0 provides an isomorphism between
⟨S, <0⟩and ⟨On, <⟩.
Theorem 2. For A ∈BNF, the following ﬁve conditions are equivalent.
1. Ωξ(A) = Ωζ(A)
2. oξhξ(A) = oζhζ(A)
3. ξ↓hξ(A) = ζ↓hζ(A)
4. hξ(A) = hζ(A) and ξ↓hξ(A) = ζ↓hζ(A)
5. hξ(A) = hζ(A) and ∀η ∈hξ(A), ξ ∼η ζ
Proof. (1.) ⇔(2.) is just Lemma 4.
(2.) ⇔(3.): Observe that oξ(hξ(A)) = o(ξ↓hξ(A)) and oζ(hζ(A)) = o(ζ↓hζ(A)).
As o deﬁnes an isomorphism between S and On, we obtain
oξhξ(A) = oζhζ(A) ⇔ξ↓hξ(A) = ζ↓hζ(A).

Turing Progressions and Their Well-Orders
219
(3.) ⇔(4.): Suppose hξ(A) ̸= hζ(A). W.l.o.g. we may assume that ζ < ξ
whence
Length(hξ(A)) < Length(hζ(A))
and also
Length(hξ(A)) = Length(ξ↓hξ(A)) < Length(ζ↓hζ(A)) = Length(hζ(A)).
As A ∈BNF, also hξ(A) and hζ(A) are in BNF, whence also ξ↓hξ(A), ζ↓hζ(A)
are in BNF. We know that normal forms are graphically unique so that ξ↓hξ(A) ̸=
ζ↓hζ(A) whence o(ξ↓hξ(A)) ̸= o(ζ↓hζ(A)).
(4.) ⇔(5.):
hξ(A) = hζ(A) ∧ξ↓hξ(A) = ζ↓hζ(A)
⇔
hξ(A) = hζ(A) ∧∀η∈hξ(A) ξ↓η = ζ↓η ⇔by Lemma 7.3
hξ(A) = hζ(A) ∧∀η∈hξ(A) ξ ∼η ζ
6
Limit Coordinates
The results so far have already provided us with quite some insight about what
the sequences Ω(A) look like. By Lemma 3 we know that the set of values that
occur in Ω(A) is ﬁnite. Moreover, by Theorem 1 we know exactly the values at
successor coordinates. In particular, we know that if the value of Ω(A) at ξ is
the same as at the successor coordinate, then it remains the same for all further
successors.
The question remains what happens at limit ordinals coordinates. In this
section we shall determine at what limit ordinals a new value can be attained
and how the new value relates to previous values. Let us start out the analysis
by formulating a negative version of Theorem 2.
Lemma 8. For ξ < ζ we have that
Ωξ(A) > Ωζ(A) ⇔(∃η∈hξ(A) ξ≤η<ζ) ∨(∃η∈hξ(A) CNP(ξ, η)<CNP(ζ, η)).
Proof. By contraposing equivalence (1.) ⇔(5.) of Theorem 2 we get
Ωζ(A) ̸= Ωξ(A) ⇔hξ(A)̸=hζ(A) ∨∃η∈hζ(A) ξ ̸∼η ζ.
But, as ζ < ξ we see
hξ(A)̸=hζ(A) ⇔∃η∈hζ(A) ζ ≤η < ξ.
Likewise,
∃η∈hζ(A) ξ ̸∼η ζ ⇔∃η∈hζ(A) CNP(ζ, η)̸=CNP(ξ, η).
As ζ < ξ we have
CNP(ζ, η)̸=CNP(ξ, η) ⇔CNP(ζ, η)<CNP(ξ, η).

220
D.F. Duque and J.J. Joosten
The ﬁrst question to ask is at which limit coordinates the sequence Ω(A) can
change. Let us ﬁrst write precisely what it means for the sequence Ω(A) to
change at some coordinate ζ. We express this by the formula
Change(ζ, A)
:=
∃ξ<ζ (Ωξ(A)>Ωζ(A) ∧∀η (ξ≤η<ζ →Ωξ(A)=Ωζ(A))).
The next lemma gives an alternative characterization of Change(ζ, A).
Lemma 9. Change(ζ, A)
⇔
∀ξ<ζ Ωξ(A)>Ωζ(A)
Proof. For ζ ∈Succ this is clear. If ζ ∈Lim, then {Ωξ(A) | ξ < ζ} is a ﬁnite
set as all the Ωξ(A) ∈On and these are weakly decreasing. Thus, at some point
below ζ the sequence must stabilize.
We can now characterize at what limit ordinals the sequence Ω(A) can change.
Theorem 3. For ζ ∈Lim:
Change(ζ, A)
⇔
∃ξ∈hζ(A) ζ∈CNA(ξ)
Proof. For ζ ∈Lim we reason:
Change(ζ, A)
⇔By Lemma 9
∀ξ<ζ Ωξ(A)>Ωζ(A) ⇔By Lemma 8
∀ξ<ζ (∃η∈hξ(A) ξ≤η < ζ ∨∃η∈hξ(A) CNP(ξ, η)<CNP(ζ, η)) ⇔
∀ξ (ξ0<ξ<ζ →∃η∈hζ(A) CNP(ξ, η)<CNP(ζ, η))
where ξ0 := sup{ξ′ ∈A | ξ′ < ζ}. Note that for these ξ, indeed, we have hξ(A) =
hζ(A). We now claim that the latter is equivalent to ∃η∈hζ(A) ζ∈CNA(η).
Clearly, if ζ ∈CNA(η) for some η ∈hζ(A), then ξ↓η < ζ↓η for each ξ < ζ.
For the converse direction, suppose ζ /∈CNA(η) for all η ∈hζ(A). Then, for
all ξ′ with
sup{ξ | ∃η∈hζ(A) (ξ∈CNA(η) ∧ξ < ζ)} < ξ′ < ζ
we have ξ′ ∼η ζ for all z ∈hζ(A), whence by Theorem 2 Ωξ′(A) = Ωζ(A).
Now that we have fully determined at which limit coordinates a change can
occur the only thing left to establish is the size of the change. In other words,
if Change(ζ, A) for some ζ ∈Lim, how does Ωζ(A) relate to Ωξ(A) for ξ < ζ. In
order to answer this question, we need a generalization of Lemma 5.4.
Lemma 10. oξ(ζ↑A) = eζoξ(A) for A ∈Sξ.
Proof. We claim that for B ∈Sξ we have that ξ↓(ζ↑B) = ζ↑(ξ↓B). From this
claim the statement follows easily from Lemma 5.4.
oξ(ζ↑A) = o(ξ↓(ζ↑A)) = o(ζ↑(ξ↓A)) = eζo(ξ↓A) = eζoξ(A).
Thus, we only need to prove our claim. Clearly, it suﬃces to show the claim for
any ordinal η ≥ξ instead of for any word in Sξ. By deﬁnition, ξ↓(ζ↑z)=δ ⇔
ξ + δ = η + ζ. Likewise, ξ↓η = δ′ ⇔ξ + δ′ = η. As ζ↑(ξ↓η) = ξ↓η + ζ = δ′ + ζ
we obtain ξ + δ′ = η
⇒(ξ + δ′) + ζ = η + ζ and by associativity of ordinal
addition also ξ + (δ′ + ζ) = η + ζ. We conclude that δ′ + ζ = δ which translates
exactly to ζ↑(ξ↓η) = ξ↓(ζ↑η), quod erat demostrandum.

Turing Progressions and Their Well-Orders
221
With this technical lemma at hand we are ready to prove the concluding theorem
of this section.
Theorem 4. Let ζ∈Lim, and let ξ < ζ be such that, whenever ξ′ ∈[ξ, ζ), it
follows that Ωξ(A) = Ωξ′(A). Then,
Ωξ(A) = e−ξ+ζΩζ(A) = eℓζΩζ(A).
Proof. As the values of Ωξ′(A) do not change for ξ ≤ξ′ < ζ we know in particular
by Theorem 2 that
hξ(A) = hζ(A).
(1)
Likewise by Theorem 2 we see that −ξ + ζ = ωℓζ. Let δ = −ξ + ζ.
Then,
Ωζ(A) = oζhζ(A) = oξ+(ξ↓ζ)(hζ(A)) = oξ+δ(hζ(A)) = oξ(δ↓hζ(A)).
(2)
Thus,
Ωξ(A)
=
oξ(hξ(A))
= By (1)
oξ(hζ(A))
=
oξ(δ↑(δ↓hζ(A))) = By Lemma 10
eδoξ(δ↓hζ(A))
= By (2)
eδΩζ(A)
= By deﬁnition of eα
eℓζΩζ(A).
Note that this theorem establishes the size of limit coordinates both in case a
change does occur and in case no change occurs. The latter case can only be so
when Ωζ(A) is a ﬁxed point of eℓζ.
References
1. Beklemishev, L.D.: Provability algebras and proof-theoretic ordinals, I. Annals of
Pure and Applied Logic 128, 103–124 (2004)
2. Beklemishev, L.D.: Veblen hierarchy in the context of provability algebras. In: Pro-
ceedings of the Twelfth International Congress on Logic, Methodology and Philos-
ophy of Science. Kings College Publications (2005)
3. Beklemishev, L.D.: Reﬂection principles and provability algebras in formal arith-
metic. Russian Mathematical Surveys 60(2), 197–268 (2005)
4. Beklemishev, L.D., Fern´andez Duque, D., Joosten, J.J.: On transﬁnite provability
logic (under preparation, 2012)
5. Solovay, R.M.: Provability interpretations of modal logic. Israel Journal of Mathe-
matics 28, 33–71 (1976)
6. Turing, A.M.: Systems of logics based on ordinals. Proc. London Math. Soc. Series
2 45, 161–228 (1939)

A Short Note on Spector’s Proof
of Consistency of Analysis
Fernando Ferreira
Universidade de Lisboa, Campo Grande, Ed. C6, 1749-016 Lisboa, Portugal
fjferreira@fc.ul.pt
Abstract. In 1962, Cliﬀord Spector gave a consistency proof of anal-
ysis using so-called bar recursors. His paper extends an interpretation
of arithmetic given by Kurt G¨odel in 1958. Spector’s proof relies cru-
cially on the interpretation of the so-called numerical double negation
shift principle. The argument for the interpretation is ad hoc. On the
other hand, William Howard gave in 1968 a very natural interpretation
of bar induction by bar recursion. We show directly that, within the
framework of G¨odel’s interpretation, numerical double negation shift is
a consequence of bar induction.
The 1958 paper [4] of Kurt G¨odel presented an interpretation (now known as the
dialectica interpretation) of Heyting arithmetic HA into a quantiﬁer-free calculus
T of ﬁnite-type functionals. The terms of T denote certain computable function-
als of ﬁnite type (a primitive notion in G¨odel’s paper, as it were): the so-called
primitive recursive functionals in the sense of G¨odel. These terms can be rigor-
ously deﬁned and they include as primitives the combinators (a bureaucracy of
terms for dealing with the “logical” part of the calculus) and the arithmetical
constants: 0, the successor constant and, importantly, the recursors.1 The di-
alectica interpretation assigns to each formula A of the language of ﬁrst-order
arithmetic a (quantiﬁer-free) formula AD(x, y) of the language of T, and G¨odel
showed that if HA ⊢A, then there is a term t (in which y does not occur free) of
the language of T such that T ⊢AD(t, y).2 The combinators play a central role
in showing the preservation of the interpretation under (intuitionistic) logic and,
unsurprisingly, the recursors play an essential role in interpreting the induction
axioms.
It is convenient to extend the dialectica interpretation to Heyting arithmetic
in all ﬁnite types HAω.3 Within the language of this theory, one can formulate
the characteristic principles of the interpretation:
1 The reader can consult [11], [1] or [7] for a precise description of the calculus T
and of its terms in particular. These are good references for details concerning the
dialectica interpretation.
2 We are taking some liberties here (and will take in the sequel). Rigorously, either
one should speak of tuple of variables x := x1, . . . , xn and y := y1, . . . , ym or allow
convenient product types.
3 Some delicate issues concerning equality in higher types arise at this point (if not
before). See [1] for a discussion of these matters.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 222–227, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

A Short Note on Spector’s Proof of Consistency of Analysis
223
ACω: ∀x∃yB(x, y) →∃f∀xB(x, f(x)),
MPω: ¬∀xA(x) →∃x¬A(x),
IPω
∀: (∀xA(x) →∃yC(y)) →∃y(∀xA(x) →C(y)),
where x and y may be of any ﬁnite type, A is quantiﬁer-free and B, C are
arbitrary. The ﬁrst principle is a form of choice, the second is a ﬁnite-type form
of Markov’s principle, and the third is an independence of premises statement.
These principles arise in virtue of the very deﬁnition of the assignment A ⇝AD
and, accordingly, have trivial interpretations. They are suﬃcient to prove the
equivalence between a given formula A of the language of HAω and its dialectica
translation AD := ∃x∀yAD(x, y).4
The dialectica interpretation extends to (ﬁnite-type) Peano arithmetic PAω
by composing it with a negative translation A ⇝Ag. Therefore, if A is a conse-
quence of PAω then Ag is provable in HAω and, hence, there is a term t of the
language of T such that T ⊢(Ag)D(t, y).
In the last paragraph of his 1958 paper, G¨odel suggests the construction of
systems stronger than T. Presumably, calculi with more terms can interpret (via
the dialectica blueprint) stronger systems of arithmetic. In this note, we are
concerned with the system obtained from PAω by adding full numerical com-
prehension (obviously, this theory contains full second-order arithmetic, a.k.a.
analysis). The formulation of the numerical comprehension principle CA in ﬁnite
type arithmetic takes the form
∃f∀n(fn = 0 ↔A(n)),
for arbitrary formulae A (n is a number variable). Cliﬀord Spector isolated in
[10] the so-called principle of numerical double negation shift (principle F in
Spector’s own paper):
∀n¬¬P(n) →¬¬∀nP(n),
where n is a number variable and P is arbitrary. This principle is important
because of the following result:5
Theorem 1 (Kreisel). The negative translation of CA is provable in the theory
HAω + ACω together with the principle of numerical double negation shift.
Proof. The theory HAω proves ∀n¬¬(Ag(n) ∨¬Ag(n)). By numerical double
negation shift, ¬¬∀n(Ag(n)∨¬Ag(n)). Equivalently, ¬¬∀n∃k((k = 0 →Ag(n))∧
(k ̸
= 0 →¬Ag(n))). By ACω, ¬¬∃f∀n((fn = 0 →Ag(n))∧(fn ̸
= 0 →¬Ag(n))).
This formula is intuitionistically equivalent to the negative translation of CA.
⊓⊔
4 This result is due to Mariko Yasugi in [14]. There is a brief discussion in [3] clarifying
the role of the characteristic principles.
5 See [8] and note 4 of [10].

224
F. Ferreira
Spector’s paper [10] was published posthumously.6 The paper was submitted
by Kreisel on Spector’s behalf. The ﬁrst footnote (written by Kreisel, as were
all footnotes) and a postscript by G¨odel explain the origin of the paper. In the
paper, Spector generalizes Brouwer’s principle of bar induction to higher types.7
We work with the following version of bar induction: For any given formulas A
and B, if
(1) ∀f∃k A(f(k))
(2) ∀s, s′(A(s′) ∧s′ is a initial subsequence of s →A(s))
(3) ∀s(A(s) →B(s))
(4) ∀s (∀xB(sˆx) →B(s))
then B(⟨⟩). In the above, x is of an arbitrary given type, f is an inﬁnite se-
quence of elements of the type of x, and s, s′ are ﬁnite sequences of elements
of the type of x (also, f(k) is the ﬁnite sequence ⟨f0, . . . , f(k −1)⟩and sˆx is
the sequence s concatenated with the element x).8,9 The great novelty is that
Spector also introduces deﬁnitions by so-called bar recursion.10 There are now
new terms, arising from the bar recursors: an extension of G¨odel’s T calculus has
been produced. Spector points that “since our immediate objective is to obtain a
quantiﬁer-free interpretation of analysis, bar recursion rather than bar induction
is appropriate.” The goal is, of course, to witness (with the help of the new terms)
the dialectica translations of the instances of the numerical double negation shift
principle. By Kreisel’s result above, this would solve the problem of providing an
6 Spector died at the age of thirty in the summer of 1961 of acute leukemia, after
spending the academic year in the Institute for Advanced Study in Princeton, New
Jersey.
7 A statement of bar induction (or bar theorem) by Brouwer himself can be found in
[2]. Van Atten has a discussion of this principle in his book [12] on Brouwer.
8 The usual presentation of ﬁnite type arithmetic has no primitive types for ﬁnite
sequences (of a given ﬁxed type). However, there are manners of representing ﬁnite
sequences in ﬁnite type arithmetic. We will not worry about these issues here.
9 Bar induction in the sense of Brouwer demands that x be a number variable. Brouw-
erian bar induction is already suﬃcient to interpret Σ0
1 comprehension (and so, by a
well-known fact, arithmetical comprehension). This can be seen by analyzing care-
fully the proofs of the Kreisel result above and of the main result below. Spector
introduced bar induction for x of any given type.
10 We do not deﬁne bar recursion in this paper (for modern references and discussions,
see note 1). In contrast to recursion, bar recursion is not well deﬁned in the full
set theoretic structure of ﬁnite type functionals (it is not a classical principle). It
is nevertheless deﬁned in the structure of continuous functionals and in the strong
majorizability model. Bar induction is, on the other hand, a classical principle.

A Short Note on Spector’s Proof of Consistency of Analysis
225
interpretation for CA. In the crucial Section 10 of his paper, Spector produces
an ad hoc witness solution.11
Why does Spector introduce and discuss bar induction? After all, his interpre-
tation of analysis does not formally require it. He explains that “bar induction
(is discussed) primarily to point out the relationship between bar recursion and
the bar theorem” and candidly observes that “bar recursion is a principle of
deﬁnition and bar induction a corresponding principle of proof.” This is exactly
right. The situation is analogous with that of induction/recursion (pace the re-
mark in note 10). In eﬀect, William Howard proved in [6] that the principle of
bar induction is interpretable in the extended calculus, with bar recursors. The
proof is very natural and, if I may add, has a certain character of inexorability
about it.12
So, there is this very satisfying picture:
induction
——————
recursion
bar induction —————— bar recursion
The following result shows that the principle of double negation shift follows
from the characteristic principles and bar induction and, therefore (by Howard),
has a dialectica interpretation in the extended calculus.
Theorem 2. In the theory HAω together with the characteristic principles, the
principle of bar induction implies numerical double negation shift.
Proof. Under the hypothesis of the theorem, we must show that ∀n¬¬P(n) →
¬¬∀nP(n) is a consequence of bar induction (P arbitrary). By the characteristic
principles, the formula P(n) is equivalent to a formula of the form ∃x∀yQ(x, y, n),
with Q quantiﬁer-free. Let us assume ∀n¬¬P(n) and ¬∀nP(n) in order to derive
a contradiction. Consider A(s) :≡∃i < |s|∃y¬Q(si, y, i) and B(s) :≡A(s), where
s is of the type of ﬁnite sequences of elements of type x (|s| denotes the length
of s). It is clear that ¬B(⟨⟩). Therefore, if one proves the hypothesis (1) to
(4) of the principle of bar induction, we get a contradiction. Hypothesis (2)
and (3) hold trivially. Hypothesis (1) is a consequence of the assumption that
11 The heart of the matter lies in the solution of a system of equations arising from
the dialectica translation of the principle of double negation shift. Due the presence
of many negations, this system is rather cryptic. It is a kind of brute fact that,
somehow, one is able to solve it with bar recursive functionals. (This is not to say
that the proof in unmotivated, as Spector describes in his paper the motivation for
his proof.) The intuitionistic laws ¬¬A1 ∧. . . ∧¬¬An →¬¬(A1 ∧. . . ∧An) are
“miniaturizations” of the principle of double negation shift and, by G¨odel’s paper,
their dialectica translations have witnessing solutions in the T calculus. It is a non
trivial exercise to ﬁnd such solutions, even for n = 2. Paulo Oliva in [9] discusses these
solutions in detail and tries to motivate the use of bar recursion for the dialectica
interpretation of double negation shift in terms of a limit process when the number
of conjuncts goes to inﬁnity.
12 Avigad and Feferman say in [1] that “while the proof requires some eﬀort, the un-
derlying idea is straightforward.”

226
F. Ferreira
¬∀nP(n). In eﬀect, this assumption says that ¬∀n∃x∀yQ(x, y, n). By ACω and
intuitionistic logic, we get ∀f¬∀n, yQ(fn, y, n). Markov’s principle MPω, on the
other hand, entails ∀f∃n, y¬Q(fn, y, n). Therefore, ∀f∃k∃i < k∃y¬Q(fi, y, i),
i.e., ∀f∃kA(f(k)).
Let us argue (4). Given a ﬁnite sequence s, suppose ∀xB(sˆx), i.e.:
∀x(∃i < |s|∃y¬Q(si, y, i) ∨∃y¬Q(x, y, |s|)).
By the assumption ∀n¬¬P(n), we have ¬¬P(|s|). Using intuitionistic logic and
Markov’s principle MPω, we get ¬∀x∃y¬Q(x, y, |s|). We now appeal to the intu-
itionistic law
∀x(φ ∨ψ(x)), ¬∀x ψ(x) ⇒¬¬φ
(where x does not occur free in φ) to infer ¬¬∃i < |s|∃y¬Q(si, y, i). By Markov’s
principle MPω once again, we ﬁnally conclude B(s).
⊓⊔
Spector’s paper has an appendix in which he aims to “indicate how bar induction
can be used to obtain an interpretation (of analysis) in a system with quantiﬁers”
(cf. p. 8 of [10]). I read this statement as proposing to show that the translations
AD of instances A of (negative translations of) numerical comprehension some-
how follow from bar induction. Rather than presenting an x-witnessing term for
AD(x, y), Spector sets himself the goal of proving the statement ∃x∀yAD(x, y).
Section 12.1 of the appendix describes an informal proof of this result for the
particular case of comprehension for Σ0
1 predicates.13 Our result above can be
viewed as providing a formal argument for the general case.14,15
Acknowledgements. This work was partially ﬁnanced by Funda¸c˜ao para a
Ciˆencia e a Tecnologia, project reference PTDC/MAT/104716/2008.
References
1. Avigad, J., Feferman, S.: G¨odel’s functional (“Dialectica”) interpretation. In: Buss,
S.R. (ed.) Handbook of Proof Theory. Studies in Logic and the Foundations of
Mathematics, vol. 137, pp. 337–405. North Holland, Amsterdam (1998)
2. Brouwer, L.E.J.: ¨Uber Deﬁnitionsbereiche von funktionen. Mathematische An-
nalen 93, 60–75 (1927); English translation in [13], pp. 457–463
3. Ferreira, F.: A most artistic package of a jumble of ideas. Dialectica 62, 205–
222 (2008); Special Issue: G¨odel’s dialectica Interpretation. Guest editor: Thomas
Strahm
13 It is Spector himself who says that the proof is informal. It uses a Brouwerian
continuity principle and it is best seen as an argument in Brouwerian (intuitionistic)
analysis. Note that the proof only uses bar induction of numerical type (cf. note 9).
14 According to a letter of Spector to Kreisel mentioned in the ﬁrst footnote of [10],
this seemed to be the intent of Spector were he able to complete his paper.
15 The main theorem of this note can also be obtained from the work of Howard in [6].
Our proof has the advantage of being very direct.

A Short Note on Spector’s Proof of Consistency of Analysis
227
4. G¨odel, K.: ¨Uber eine bisher noch nicht ben¨utzte Erweiterung des ﬁniten Stand-
punktes. Dialectica 12, 280–287 (1958); Reprinted with an English translation in
[5], pp. 240–251
5. G¨odel, K., Feferman, S., et al. (eds.): Collected Works, vol. II. Oxford University
Press, Oxford (1990)
6. Howard, W.A.: Functional interpretation of bar induction by bar recursion. Com-
positio Mathematica 20, 107–124 (1968)
7. Kohlenbach, U.: Applied Proof Theory: Proof Interpretations and their Use in
Mathematics. Springer Monographs in Mathematics. Springer, Berlin (2008)
8. Kreisel, G.: Interpretation of analysis by means of constructive functionals of ﬁnite
types. In: Heyting, A. (ed.) Constructivity in Mathematics, pp. 101–128. North
Holland, Amsterdam (1959)
9. Oliva, P.: Understanding and Using Spector’s Bar Recursive Interpretation of Clas-
sical Analysis. In: Beckmann, A., Berger, U., L¨owe, B., Tucker, J.V. (eds.) CiE
2006. LNCS, vol. 3988, pp. 423–434. Springer, Heidelberg (2006)
10. Spector, C.: Provably recursive functionals of analysis: a consistency proof of anal-
ysis by an extension of principles in current intuitionistic mathematics. In: Dekker,
F.D.E. (ed.) Recursive Function Theory: Proceedings of Symposia in Pure Math-
ematics, vol. 5, pp. 1–27. American Mathematical Society, Providence (1962)
11. Troelstra, A.S. (ed.): Metamathematical Investigation of Intuitionistic Arithmetic
and Analysis. Lecture Notes in Mathematics, vol. 344. Springer, Berlin (1973)
12. van Atten, M.: On Brouwer. Wadsworth (2004)
13. van Heijenoort, J. (ed.): From Frege to G¨odel. Harvard University Press (1967)
14. Yasugi, M.: Intuitionistic analysis and G¨odel’s interpretation. Journal of the Math-
ematical Society of Japan 15, 101–112 (1963)

Sets of Signals, Information Flow, and Folktales
Mark Alan Finlayson
Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of
Technology, 32 Vassar Street, Cambridge MA 02139, United States of America
markaf@mit.edu
Abstract. I apply Barwise and Seligman’s theory of information ﬂow
to understand how sets of signals can carry information. More precisely I
focus on the case where the information of interest is not present in any
individual signal, but rather is carried by correlations between signals.
This focus has the virtue of highlighting an oft-neglected process, viz., the
diﬀerent methods that apply categories to raw signals. Diﬀerent methods
result in diﬀerent information, and the set of available methods provides
a way of characterizing relative degrees of intensionality. I illustrate my
points with the case of folktales and how they transmit cultural infor-
mation. Certain sorts of cultural information, such as a grammar of hero
stories, are not found in any individual tale but rather in a set of tales.
Taken together, these considerations lead to some comments regarding
the “information unit” of narratives and other complex signals.
1
A Theory of Information Flow
In their book “Information Flow: The Logic of Distributed Systems,” Barwise
and Seligman [1] present a mathematically sophisticated theory of how things
can carry information about other things. Barwise and Seligman started from
Dreske’s seminal work on information ﬂow [2], and expanded and formalized
his observations, integrating his approach with related approaches, resulting in
a more general formulation. (From here on out I will refer to this general for-
mulation as the “DBS” theory of information ﬂow, short for Dreske-Barwise-
Seligman). I observe that the DBS theory is, in fact, even more general than it
at ﬁrst appears, and it is my aim to illustrate how it can be used to frame and
describe several important facets of information ﬂow, knowledge, and belief that
were left unelaborated in both Barwise and Seligman’s and Dreske’s work. In
particular, I will show how the DBS theory, without modiﬁcation, can be used
to conceptualize two important items which Dreske touched upon only tantaliz-
ingly: learning and intensionality. I show how this conceptualization brings into
relief a part of information channels that is often taken for granted in philosoph-
ical analyses, namely, the process by which categories are applied to raw signals.
I will then apply these insights to make some comments on the information
content of cultural narratives (folktales).
I set the stage by reviewing in brief the relevant parts of the DBS theory.
The theory involves, at its core, classiﬁcations and infomorphisms. These two
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 228–236, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Sets of Signals, Information Flow, and Folktales
229
objects are used to model how information ﬂows across distributed systems, which
are systems that can be analyzed in terms of both a whole and constituent
parts. In Barwise and Seligman’s terminology, an information channel brings
classiﬁcations and infomorphisms together into a full model of the information
ﬂow of a particular system.
We shall lose no generality if we restrict ourselves to discussing a distributed
system W comprising only two parts, a proximal part P, to which we have
direct access and be thought of as the “receiving” end, and a distal part D, from
which information is ﬂowing. There are infomorphisms that map properties of
the classiﬁcations of the distal and proximal parts to the whole; call these d and
p, respectively. To provide a concrete example to discuss, let us take Barwise and
Seligman’s example of a nuclear reactor: in this case W is the whole reactor, D
will be the reactor core, and P will be a gauge in the reactor control room, and
d and p are the regularities that connect the core to the reactor to the gauge.
A classiﬁcation is similar to what one thinks of when considering the standard
classiﬁcation task in cognitive psychology: it is a set of labels or classes that may
be applied to some object or phenomenon. Classiﬁcations can be, for example,
mutually exclusive (e.g., {square,circle}), exhaustive (e.g., {true,false}),
or overlapping (e.g., {tall,fat}). They can also be none of those things. Im-
portantly, though, each part, as well as the whole, receives a classiﬁcation. For
our reactor example we might consider the reactor core D to be classiﬁed by the
exclusive types normal and overheating, the reactor status gauge can show
one of green or red, while the reactor overall can be in one of the four states
achieved by the cross product of these two classiﬁcations.
An infomorphism relates classiﬁcations on a part to classiﬁcations on the
whole. It is a way of describing how classiﬁcations are transformed as the in-
formation they carry moves through the distributed system, from one part to
another: they are models of the regularities that allow information ﬂow. In such
a system one infomorphism d may be applied to the distal part’s classiﬁcation
to obtain a classiﬁcation on the whole, and then another infomorphism p may
be applied in reverse to the classiﬁcation on the whole to obtain a classiﬁcation
on the proximal part. We need not say too much about infomorphisms except
that, as they are applied in the forward and reverse directions, the resulting
classiﬁcations loose some of their guarantees and internal relationships and are
downgraded to what Barwise and Seligman call local logics. In the reactor exam-
ple, the combined infomorphisms from reactor core to reactor whole, and then
from reactor whole to control room gauge, given a reactor in working order,
results in a display of green on the gauge when the core is normal, and a
display of red on the gauge when the core is overheating. Thus information
ﬂows from the distal part of the system (the reactor core) to the proximal part
of the system (the control room gauge).
The details are not critical to my argument, but there are two essential points
to take away from this description. First is that regularities across the sys-
tem, modeled by chains of infomorphisms, are what allow information to ﬂow
from one part of the system to another. (It is often helpful to think of these

230
M.A. Finlayson
regularities, like Dretske did, as lawful relationships, but one must remember
that not all regularities are lawful.) Second, classiﬁcations are the language by
which the information is communicated and information ﬂow is relative to the
classiﬁcations of the whole and its parts.
2
Information Flow via Sets of Signals
Information ﬂow in the DBS theory is intimately connected, whether explicitly or
implicitly, with signals. The examples in Dretske’s and Barwise and Seligman’s
work are all concerned with individual signals. A signal is not deﬁned precisely
in either work, but one gathers it follows the natural intuitions: signals carry
information and they are relatively localized in time and space. Signals ﬂow
across a distributed system from the distal part to the proximal part. They are
the messages that contain the information. A light ﬂashing SOS, reading the
symbols oﬀa map, a speech act: these are all signals.
I turn to an interesting and important case, that where the information of
interest is not present in any individual signal, but rather is carried by correla-
tions between signals. Regularities in a distributed system can result not only in
information carried in a single signal; certain types of regularities can also result
in correlations between signals.
How can this be so? Here is an example. Let us consider the reactor, and ask
a simple question: Is the reactor more often normal or overheating? Perhaps
not a very interesting question, and one whose answer is obvious to anyone who
knows much about nuclear reactors and how they are designed and run. But
imagine that you know very little at all about nuclear reactors. Then, certainly,
you would agree that if you were to learn the answer to this question about
a particular reactor, then you would be the recipient of information. How we
answer the question is straightforward: we simple observe the gauge periodically,
noting whether it the gauge is normal or overheating. Eventually, we stop
and count up our observations, and whichever type outnumbers the other, that
is our answer for this particular reactor.
How often we observe the gauge, for how long, doesn’t matter much for my
argument. What is important is that we cannot know, by observing any individ-
ual signal from gauge, whether the reactor is more often normal or not. The
information is not contained in any one signal, it is only contained in a collec-
tion of those signals. Now, one may object that this question is contrived and
uninteresting, and does not represent the sort of information we are interested
in studying. But, in fact, this is a common scientiﬁc question: “Is it more likely
that X or Y for some type of signal?” Doctors, for example, ask the question
of whether or not patients are more likely to die or be cured (or something in
between) when they use or don’t use a particular drug. Engineers ask whether a
building is more likely to fall down in an earthquake when designed this way or
that. Astronomers ask whether it is more likely for a type of star to go supernova
sooner rather than later, or not all, if it has this or that characteristic. All of
these examples are more complex than the reactor example, in that answering

Sets of Signals, Information Flow, and Folktales
231
these questions usually involves correlating signals from multiple parts of the
system at many diﬀerent times, using much more complex methods, but the ba-
sic principle is the same: one cannot obtain the information from any one signal.
The set of signals itself becomes an information channel.
How may this be analyzed within the DBS theory? The distributed system
becomes, not a single instance at a particular time of the system under con-
sideration, but a set of distributed systems, each one at diﬀerent times. Each
instance might be a speciﬁc time instance of a particular distributed system, they
might be diﬀerent instances of diﬀerent distributed systems (all similar in some
relevant way), or some mixture in between. The infomorphisms still reﬂect the
regularities that underlie the system, they now just describe regularities spread
across distributed system instances, and thus, time and space. The classiﬁcation
may be thought of as all the possible answers to the question — what those who
do statistical analyses might call the hypothesis space of the problem. When we
ﬁnally determine what is the actual answer, we have pinned a particular type
on the receiving end of our set-of-signals distributed system, and we are the
recipient of information.
This focus on the set of signals and the observation that scientists use sets
of signals to answer scientiﬁc questions highlights an important fact: the way
one correlates the signals in the set is key to the extraction of the information.
Diﬀerent methods result in diﬀerent information ﬂowing across the system. This
choice of method contains much of the contribution of science: how do we process
the raw data so as to uncover the information that we seek?
Naturally, the diﬀerences in information between methods may result from
diﬀerent classiﬁcations used or implied by each method. This is exactly the
same as in the individual signal case where diﬀerent information ﬂows when we
have a diﬀerent classiﬁcation for a part or the whole. But, there is an important
distinction I would like to highlight, namely, that diﬀerent methods might pro-
duce diﬀerent answers for the same classiﬁcation. For example, some correlation
techniques might give a wider or narrower range for an answer (on an ordinal
scale); on the other hand, a diﬀerent technique might give a completely diﬀerent
answer. Thus in the scientiﬁc literature much eﬀort is spent on justifying one’s
technique on principled grounds, and much is made of two diﬀerent techniques
converging on the same answer.
3
Learning and Intensionality
The above treatment shows that the DBS theory may be applied beyond exam-
ples containing a single signal. This allows us to frame two phenomena that are
left unelaborated in the DBS analysis.
The ﬁrst phenomenon is learning. Dretske noted that “Learning, the acquisi-
tion of concepts, is a process whereby we acquire the ability to extract . . . infor-
mation from the sensory representation.” [3, p. 61] Learning can be described in
the DBS theory by framing it as a set-of-signals information channel. We begin
with individual signals that are unclassiﬁed. Moving up to the set of signals level,

232
M.A. Finlayson
we apply a correlation method for identifying the type that applies to particular
signals in particular circumstances. Having learned this classiﬁcation we may
return to the single signal case, and apply the newly learned classiﬁcation. In
the reactor example, suppose we learn, via observations at multiple times, and
application of a particular correlation method, that certain gauges on the reactor
always move in synchrony. This is a classiﬁcation. When next presented with an
individual signal, where perhaps we can observe only a single gauge, we can infer
the state of the other gauges from a single observation. Similarly with what pre-
sumably happens when a child learns a new word. Daddy says “airplane!” and
points. This happens several times. Perhaps there are some near-misses that aid
learning (“No, honey, that’s a butterﬂy.”) Eventually, by correlations between
all these signals, the child learns the category, and now can say “airplane” herself
when seeing only a single signal. Learning has occurred.
The second phenomenon is degree of intensionality. Dretske said: “Our expe-
rience of the world is rich in information in a way that our consequent beliefs
are not. . . . The child’s experience of the world is (I rashly conjecture) as rich
and as variegated as that of the most knowledgable adult. What is lacking is a
capacity to exploit these experiences in the generation of reliable beliefs (knowl-
edge) about what the child sees. I, my daughter, and my dog can all see the
daisy. I see it as a daisy. My daughter sees it simple as a ﬂower. And who knows
about my dog?” [3, p. 60] Dreske describes these diﬀerences in the perceptions
as diﬀerences in intensionality. We can characterize this degree of intensionality
by equating it with the method for extracting the information from the signal.
The more sophisticated the correlation method, the more complex and varied
the proximal classiﬁcation, the more intensionality we assign to the agent in
question. (This observation might lead us to hope that we can provide a full or
partial order over intensionalities. Unfortunately this is not to be — see the next
section.)
There are a number additional observations that may be made on this topic.
For example, if we talk about information carried by sets of signals, why not talk
about information carried by sets of sets of signals? Or sets of sets of sets? This,
perhaps, is the same as talking about learning about learning, and so forth.
We might also explore how the scientiﬁc method in general, or speciﬁc ﬁelds
of inquiry, such as machine learning, are illuminated by these observations. We
could examine in more detail how the learning method intervenes between signal
and classiﬁcation. But rather than explore these interesting lines of inquiry, I turn
my attention to an application of these observations to a domain of particular
interest to me: cultural information as carried by sets of folktales.
4
Folktales and Narrative Structure
My switchings gears to the topic of cultural information as carried by sets of
folktales may seem like a non sequitur. I assert several reasons for the attention.
First, narratives are an excellent example of a complex signal which contains
myriad subtle sorts of information. Everyone is familiar with folktales, and so

Sets of Signals, Information Flow, and Folktales
233
they will serve as an eﬀective proxy for all sorts of complex signals with multiple
possible interpretations without the overhead of detailed setting of the ground.
I would like to use what I know about them to explore more this idea of varying
degrees of intensionality, and for this purpose they have the advantage of us not
yet knowing, scientiﬁcally speaking, what exactly is the information contained
in them, and therefore we need not suspend our disbelief to imagine that there
may be several ways of interpreting the information contained in folktales: we
have several proposals (I will consider two) and we don’t know which one (or
ones) is right. Second, these observations allow me to pose, and explore a bit,
some interesting questions about the nature of information carried in narratives.
Third, narratives and culture are of central importance my work, and I am the
one writing this paper. So bear with me.
Folktales speciﬁcally, and narratives in general, clearly communicate infor-
mation aside from any considerations of their properties across a set. They are
like any other text: they communicate information as individual objects. In a
folktale in particular, and narratives in general, we can learn things such as who
the characters are, what their plans and goals are, and what they are doing
and when. (Although, usually being about a ﬁctional world, it is an interesting
question whether this information translates into knowledge.) This sort of infor-
mation, the sort contained in an individual tale, is not the information we are
interested in here. I am interested, rather, in information that is communicated
across a set of tales.
There are numerous types of information communicated by a set of folktales.
My work so far has focused on a particular sort, that of narrative structure of
the plot [4]. This information corresponds to a grammar for plots, speciﬁc to the
culture in question. Much like a natural language, a folktale grammar provides
a set of symbols (plot pieces) and rules of combination that allow us to build
folktales in that culture. Much like the grammar of a natural language is not
captured in a single sentence, the grammar of the folktales is not captured in
any single tale. There have been many proposals for the form of these grammars,
proposals that span the range from universal theories across all stories, to highly
culturally-speciﬁc theories for certain genres of folktales. I will contrast two
examples, the ﬁrst being Vladimir Propp’s theory of the morphology of the
folktale [6].
Propp’s theory lays out a grammar in three levels, where the middle level, that
of the function, has 31 pieces that describe the major constituents of Russian
fairy tales. These pieces include such plot fragments as Villainy, Struggle (be-
tween the Hero and Villain), and Reward (of the Hero for defeating the Villain). I
demonstrated that these plot pieces and rules of combination, rather than being
ﬁgments of Propp’s imagination, can be learned by a computerized correlation
method from the actual tales [4]. I call the method Analogical Story Merging,
which is modiﬁcation of a machine learning technique called Bayesian Model
Merging that relies on correlations uncovered by a statistical process leveraging
Bayes’ rule. Key to the method is a bias function called the prior that tells the
method what similarities it should consider important when considering what

234
M.A. Finlayson
parts of the folktales may be patterns. In the case of learning Propp’s theory,
the prior focuses the method on three important features: the semantic character
of events; which characters are involved in those events; and where the events
occur in the timeline of the tale.
This is all well and good. We have a method that extracts higher-level plot
patterns from sets of folktales, where the patterns themselves cannot be seen by
examining just a single tale. We have identiﬁed information ﬂow from a set of
tales, and in contrast to other information in an individual folktale, there is a
fair chance that this information actually reﬂects something in reality (rather
than a ﬁctional world) — it likely reﬂects the ideas of participants in the culture
under examination, such as the sorts of bad things that can happen to people,
the proper conduct of a heroic person, and the rewards for heroic behavior. But
is this the only information transmitted by sets of folktales?
Consider a competing proposal for narrative structure, that of L´evi-Strauss [5].
In his structural analysis of myth, he identiﬁed units of analysis that he called
mythemes, where each mytheme was a set of semantic units uniﬁed by their
treatment of a common theme, such as death or familial relations. In contrast
to Propp’s so-called diachronic analysis of the tales, where each function is laid
out in the order it is encountered in the tale, L´evi-Strauss organized his analysis
synchronically, where mythemes are organized by theme regardless of their po-
sition in the tale. Moreover, L´evi-Strauss’s ‘grammar’ (if it may be called that)
is highly constrained, consisting of two paired binary oppositions arranged in a
speciﬁc relationship. While I don’t have an algorithm that demonstrates learning
L´evi-Strauss’s theory from stories, it is clear that the method I used for learning
Proppian structures would not be suﬃcient, merely from theoretical limitations
of grammatical inference.
Given both Propp’s and L´evi-Strauss’s analyses, what are we to say about
the information they contain, relative to one another? L´evi-Strauss’s theory is
not a specialization of Propp’s, or vice-versa — they are completely orthogonal.
One needs a completely diﬀerent method to learn L´evi-Strauss’s theory from the
stories. So which is the actual information carried across the set? The answer is
clear, in that it depends on the method one uses. Both theories, if underwrit-
ten by regularities in the distributed system (of people, culture and folktales)
describe equally valid information carried by the set. They are two completely
diﬀerent interpretations of what is going on.
This observation points the way toward understanding what is going on with
diﬀerent degrees of intensionality. Indeed, these examples show that intension-
ality is not a matter of degrees at all. We have an intuitive ranking of the
dog’s, child’s, and Dretske’s classiﬁcations of the daisy, but these are all rela-
tive to an implicit value judgement about merely one aspect of the classiﬁcation
method. Dretske’s classiﬁcation is a reﬁnement of the child’s, and the child’s a
reﬁnement of the dog’s (we suppose), and we have implicitly associated a more
reﬁned classiﬁcation with a higher degree of intensionality. But in the case of
narrative structure, there is no such reﬁnement relationship. The two theories
attend to quite diﬀerent patterns in the texts at hand. Thus we see clearly that

Sets of Signals, Information Flow, and Folktales
235
intensionality, in the general case, does not come with a clear intuitive ranking.
Intensionality is relative to some measure on the method we are using to ex-
tract our information. We may ﬁnd, for a particular situation, that complexity
of the method, or complexity of classiﬁcation, or utility for some purpose, are
the appropriate way to rank and order intensionalities. We have reﬁned the ques-
tion from what makes this processor of information more intensional than that
one? to what characteristics of the classiﬁcation method explain our intuitions
of degree of intensionality?
5
Information in Individual Narratives
Information can ﬂow from a set of narratives. What can we do with it? We can
of course go back and apply that knowledge to a single narrative. For example,
in a Proppian-style analysis, suppose we have learned from our set of narratives
that there is something we decide to call a Villainy in the culture in question,
and it takes certain speciﬁc forms. The method shows us what to pay attention
to for when looking for villainies, and so now we can (usually) pick out a villainy
in an individual story. This does not mean, of course, that the individual narra-
tive contains the information about the nature of villainies — we learned that
from the set of narratives. Our concept of villainy depended on analyzing the
whole set. It contains the information that there is (or is not) a villainy in that
particular narrative. This is just another way of emphasizing, as the DBS theory
does, that information ﬂow is relative to the receiver. Interestingly, for cultural
narratives, what information ﬂows at the narrative structure level is a function
not only of the method used (e.g., a Proppian-style analysis, or L´evi-Straussian
analysis, or something else), but also a function of the contents of the set itself.
Change the set of tales, to folktales from another culture, and you get diﬀerent
functions [4, §6.1.5]. This naturally leads to the questions of how we decide what
set of narratives to analyze? What principles should guide that selection? In my
work, and Propp’s, the principle was a representative selection of a particular
genre of folktale from a particular culture. For other purposes the principle could
be quite diﬀerent.
There is a second point of interest. Naturally, even if one keeps the selection
principle the same, the nature of the information extracted from the set of folk-
tales varies with the number of tales in the set [4, Fig. 5-3]. For smaller sets, we
learn fewer Proppian, and they are learned with less ﬁdelity. Thus, in a sense,
when fewer tales support the higher-level analysis, the information carried by
the individual tale is coarser, and the information “chunk size” is larger. One
would imagine, when doing a Proppian analysis on a set that properly contains
the set of tales analyzed by Propp, that one might ﬁnd more than 31 functions.
(Indeed, I noted a possible missing function of this sort [4, §5.5.4].) In this case,
with more tales, the information chunk is smaller, and the information carried
by the tale is ﬁner.
In a Proppian-style analysis, if Propp’s functions can be considered the “in-
formation unit” or “chunk size” of the plot of the narrative, relative to some

236
M.A. Finlayson
particular set of folktales, how do we know when we have the right sized chunk?
I cannot think of any philosophical reason that the chunks, in this particular
case, will be of one size rather than another. I imagine it will boil down to an ex-
perimental question, where one may ﬁnd, upon adding more and more folktales
to the original set, that the chunk size does not get any smaller. For another
style of analysis one may ﬁnd that there is no stable point, and the chunk size
always depends on the number of narratives added. This could potentially be a
discriminator between eﬀective and ineﬀective theories of narrative structure.
Acknowledgements. This work was funded by the Oﬃce of Naval Research
under award number N00014-09-1-0597. Any opinions, ﬁndings, and conclusions
or recommendations expressed here are those of the author and do not necessarily
reﬂect the views of the Oﬃce of Naval Research.
References
1. Barwise, J., Seligman, J.: Information Flow: The Logic of Distributed Systems.
Cambridge University Press, Cambridge (1997)
2. Dretske, F.I.: Knowledge and the Flow of Information. MIT Press, Cambridge (1981)
3. Dretske, F.I.: Pr´ecis of knowledge and the ﬂow of information. The Behavioral and
Brain Sciences 6(1), 55–90 (1983)
4. Finlayson, M.A.: Learning Narrative Structure from Annotated Folktales. Ph.D.
thesis, Department of Electrical Engineering and Computer Science. Massachusetts
Institute of Technology (2011)
5. L´evi-Strauss, C.: The structural study of myth. The Journal of American Folk-
lore 68(270), 428–444 (1955)
6. Propp, V.: Morphology of the Folktale. University of Texas Press, Austin (1968)

On the Foundations and Philosophy
of Info-metrics
Amos Golan
Info-Metrics Institute and Department of Economics, American University, 4400
Massachusetts Avenue, Washington, DC 20016, United States of America
agolan@american.edu
1
Background, Objective and Motivation
Among the set of open questions in philosophy of information posed by Floridi
[5,6] is the question of “What Is the Dynamics of Information?” For recent
discussion see Crnkovic and Hofkirchner [2]) and a complimentary summary of
open questions in the interconnection of philosophy of information and computa-
tion (Adriaans [1]). The broad deﬁnition of “dynamics of information” includes
the concept of “information processing.” This article concentrates on that con-
cept, redeﬁnes it as “info-metrics,” and discusses some open questions within
info-metrics.
Inference and processing of limited information is one of the most fascinating
universal problems. We live in the information age. Information is all around
us. But be it much or little information, perfect or blurry, complementary or
contradicting, the main task is always how to process this information such that
the inference – derivation of conclusions from given information or premises – is
optimal.
The emerging ﬁeld of info-metrics is the science and art of inference and quan-
titatively processing information. It crosses the boundaries of all sciences and
provides a mathematical and philosophical foundation for inference with ﬁnite,
noisy or incomplete information. Info-metrics is at the intersection of informa-
tion theory, statistical methods of inference, applied mathematics, statistics and
econometrics, complexity theory, decision analysis, modeling and the philosophy
of science. From mystery solving to the formulation of all theories – we must infer
with limited and blurry observable information. The study of info-metrics helps
in resolving a major challenge for all scientists and all decision makers of how
to reason under conditions of incomplete information. Though optimal inference
and eﬃcient information processing are at the heart of info-metrics, these issues
cannot be developed and studied without understanding information, entropy,
statistical inference, probability theory, information and complexity theory as
well as the meaning and value of information, data analysis and other related
concepts from across the sciences. Info-metrics is based on the notions of infor-
mation, probabilities and relative entropy. It provides a uniﬁed framework for
reasoning under conditions of incomplete information.
Though much progress has been made, there are still many deep philosoph-
ical and conceptual open questions in info-metrics: What is a correct inference
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 237–244, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

238
A. Golan
method? How should a new theory be developed? Is a uniﬁed approach to in-
ference, learning and modeling necessary? If so, does info-metrics provide that
uniﬁed framework? Or even simpler questions related to real data analyses and
correct processing of diﬀerent types and sizes of blurry data fall at the heart
of info-metrics. Simply stated, modeling ﬂaws and inference with imperfect in-
formation is a major challenge. Inconsistencies between theories and empirical
predictions are observed across all scientiﬁc disciplines. Info-metrics deals with
the study of that challenge. These issues – the fundamental problem, current
state, thoughts on a framework for potential solutions and open questions – are
the focus of that article.
The answer to the above questions demands a better understanding of both
information and information processing. That includes understanding the types
of information observed, connecting it to the fundamental – often unobserved –
entities of interest and then meshing it all together and processing it in a con-
sistent way that yields the best theories and the best predictions. Info-metrics
provides that mathematical and philosophical framework. It generalizes the Max-
imum Entropy (ME) principle (Jaynes [10,11]) which builds on the principle of
insuﬃcient reason. The ME principle states that in any inference problem, the
probabilities should be assigned by maximizing Shannon’s information measure
called entropy (Shannon [13]) subject to all available information. Under this
principle only the relevant information is used. All information enters as con-
straints in an optimization process: constraints on the probability distribution
representing our state of uncertainty. Maximizing the entropy subject to no
constraints yields the uniform distribution representing a state of complete un-
certainty. Introducing meaningful information as constraints in the optimization
takes the distribution away from uniformity. The more information there is, the
further away the resulting distribution is from uniformity or from a state of
complete uncertainty. (For detailed discussion on the philosophy of information,
as well as dynamic information, across the sciences see van Benthem, and Adri-
aans [4]; the resent text of van Benthem [3]; the proceedings of the Info-Metrics
Workshop on the Philosophy of Information [12].)
In this article, I will provide a summary of the state of info-metrics, the
universality of the problem and a solution framework. I will discuss some of
the open questions and provide a number of examples from across the scientiﬁc
spectrum throughout this article.
2
Examples and Framework
Much has been written on information. Much has been written on inference and
prediction. But the interdisciplinary combined study of information and eﬃcient
information processing is just starting. It is based on very sound foundations and
parts of it have long history within speciﬁc disciplines. It provides the needed
link to connect all sciences and decision processes. To demonstrate the basic
issues in info-metrics and the generality of the problem, consider the following
very simple examples.

On the Foundations and Philosophy of Info-metrics
239
Consider “betting” on the state of the economy as is conveyed by the data.
This is expressed nicely in a recent Washington Post editorial (August 9, 2011):
“Top oﬃcials of the Federal Reserve, and their staﬀ, assembled around a gigantic
conference table to decide what, if anything, they should do to help the ﬂagging
U.S. economy. Just about every member of this group, headed by Chairman
Ben S. Bernanke, was an expert in economics, banking, ﬁnance or law. Each
had access to the same data. And yet, after hours of discussion, they could not
agree....” What would you do? How would you interpret the data? Would you
follow the Chairman or would you construct your own analysis? There is not
enough information to ensure one solution. There are many stories that are fully
consistent with the available information and with the data in front of each one
of the Fed members. The Chairman’s solution is only one such story. The Fed
members face blurry and imperfect data. Simply phrased, the question is what
is the solution to X+Y = more-or-less 10. Unlike the previous case, even if you
have more information (say, X = more-or-less 3), there are still inﬁnitely many
solutions to this two blurry equations problem. Using information theory and
the tools of info-metrics yields the most conservative solution.
Now, rather than “betting” on the story the “data” represent, consider a
judge “betting” on the District Attorney’s story based on the observable evi-
dence, or a Justice “betting” on the correct interpretation of the constitution as
is nicely expressed by Justice Souter’s remarks at the 2011 Harvard Commence-
ment: “...The reasons that constitutional judging is not a mere combination of
fair reading and simple facts extend way beyond the recognition that constitu-
tions have to have a lot of general... . Another reason is that the Constitution
contains values that may well exist in tension with each other, not in harmony.
Yet another reason is that the facts that determine whether a constitutional pro-
vision applies may be very diﬀerent from facts like a person’s age or the amount
of the grocery bill; constitutional facts may require judges to understand the
meaning that the facts may bear before the judges can ﬁgure out what to make
of them. And this can be tricky...” The justices are faced with the same evidence
(information). But in addition to the hard evidence, the justices incorporate in
their decision process other information such as their own values, their own sub-
jective information, and their own interpretation of the written word. They end
up disagreeing even though they face the same hard evidence. Is there a trivial
way of solving it? No. But, there is a way to consistently incorporate the hard
evidence and the softer information such as prior beliefs, value judgment, and
imprecise meanings of words within the information-theoretic and info-metrics
framework mentioned earlier. Building on these examples, I discuss the univer-
sal problem of information processing and a framework for inference with the
available information.
3
Universality
Generally speaking, what is common to decisions made by Supreme Court jus-
tices, communication among individuals, scientiﬁc theories, literary reviews, art

240
A. Golan
critiques, image reconstruction, data analyses, and decision making? In each
case, inference is made and information is processed. This information may be
complete or incomplete, perfect or blurry, objective or subjective at the deci-
sion time or at the moment of analysis. The justices are faced with the same
evidence (information). But in addition to the evidence, each justice decides the
case using her/his own values and her/his own subjective information and inter-
pretation of the written word. They end up disagreeing even though they face
the same hard evidence. When communicating with one another, individuals
use their own interpretation and understanding of each word, phrase, gesture
and facial expression. Again, the subjective information is incorporated with the
hard evidence – the words. Scientiﬁc theories are based on observed information
(and observable data) together with sets of axioms and assumptions. Some of
these axioms and assumptions are unobservable and cannot be veriﬁed. They
reﬂect the very basic beliefs together with the minimally needed information the
scientist needs to deduce and infer a new theory. They are the fundamentals
necessary for building a theory but they are not always observed. Again, the
subjective information (beliefs, assumptions, axioms) is incorporated with the
hard evidence – the observed data. Literary and art reviewers are also faced with
the same evidence: the painting, the text, the dance, the poem. But, in addition
to evidence, they evaluate it based on their own subjective information. Image
and data processing involve observable, unobservable and subjective informa-
tion. Since all data processing involve noisy information, it is impossible to do
inference without incorporating subjective and other structural information. To
complicate things further, some of the observed or unobserved information may
incorporate contradicting pieces of information making it practically impossible
to reconcile it within a single, uniﬁed theory. Even worse: if the diﬀerent pieces
of information are complementary, how much value should each one receive? The
common thread across these examples is the need for coherent and eﬃcient infor-
mation processing and inference. The problem is universal across all disciplines
and across all decision makers.
4
The Starting Premise – The Observer
If one views the scientist as an observer (not a common view within the social
sciences) then the universal problem of inferring with limited information can
be resolved. That solution framework serves as a common thread across all sci-
ences. The idea is very simple. Regardless of the system or question studied,
the researcher observes only a certain amount of information or evidence. The
observer needs to process that information in a scientiﬁcally honest way. That
means that she has to process the observed information in conjunction with
the unobserved information coming from theoretical considerations, intuition or
subjective beliefs. That processing should be done within an optimal inference
framework while taking into account the relationship between the observable and
the unobservable; the relationship of the fundamental (often unobserved) entity
of interest and its relationship to the observable quantities; the connection be-
tween the micro state (usually unobserved) and the global macro state of the

On the Foundations and Philosophy of Info-metrics
241
system (usually observed); the value of the observed information; speciﬁcation
of the theoretical and other subjective information; and ﬁnally even if one has
all the answers and inference is made, it needs to be veriﬁed.
As an observer the scientist and the decision maker become detectives that
must combine all the types of information, process it all and make their inference.
When possible they validate their theory or decision and update their inference
accordingly. With that view we are back at the universal problem of inference
with limited information. But now we already acknowledge the advantage of
tackling that challenge as external observers. Since we deal with information, it
seems natural that the study of this problem takes us back to information theory
and the earlier work of Jaynes on the Maximum Entropy formalism.
5
Open Questions
Keeping in mind that all information is ﬁnite and that the observed information
is often very limited and blurry, all inferential problems are inherently underde-
termined (as discussed above). Further, the information observed is of diﬀerent
types: “hard” data, “soft” data and prior information. The “hard” data are just
the traditional observed quantities. The “soft” data represent our own subjective
beliefs and interpretation as well as axioms, assumptions and possible unveriﬁ-
able theories. The prior information represents our prior understanding of the
process being studied (often it is based on some veriﬁable data or on information
coming from previous studies or experiments).
I will start with a few examples of small and large data and then I will provide
a short list of open questions.
A number of simple representative examples capture some of the basic prob-
lems raised so far. Consider for example analyzing a game between two – or more
– individuals. Traditionally, one starts by assuming the players are rational or
maximize a certain objective function. Other structure (on the information set
each one of the players has) is also assumed. But in reality the researcher does
not observe the individual’s preferences (or objectives) but rather observes the
actions taken by these players. But similar to previous examples, this means that
the problem is inherently underdetermined: there are many stories/games that
are consistent with the observed actions. The info-metrics framework described
here allows one to analyze such problems. Bono and Golan (2011) formulate that
problem. They formulate a solution concept without making assumptions about
expected utility maximization, common knowledge or beliefs. Beliefs, strategies
and the degree to which players maximize expected utility are endogenously de-
termined as part of the solution. To achieve this, rather than solving the game
from the players’ point of view, they analyze the game as an “observer” who is
not engaged in the process of the game but observes the actions taken by the
players. They use an information-theoretic approach which is based on the Maxi-
mum Entropy principle. They also compare their solution concept with Bayesian
Nash equilibrium and provide a way to test and compare diﬀerent models and
diﬀerent modeling assumptions. They show that alternative uses of the observer’s

242
A. Golan
information lead to alternative interpretations of rationality. These alternative
interpretations of rationality may prove useful in the context of ex post arbi-
tration or the interpretation of experimental data because they indicate who is
motivating whom.
This example brings out some of the fundamental questions a researcher has
to deal with when constructing a theory and processing information in the be-
havioral or social sciences. For example, how can one connect the unobserved
preferences (and rationality) with the observed actions? Or, how much observed
information one needs in order to solve such a problem? Or what inference
method is the “correct” method to use? Or how can the theory be validated
with the observed information? All of these questions arise naturally when one
deals with small and noisy data within the social sciences.
Consider now a diﬀerent example dealing with decomposing mass spectra
of gas mixtures from noisy measurements (Toussaint, Golan and Dose [14]).
Given noisy observations the objective here is estimate the unknown cracking
patterns and the concentrations of the contributing molecules. Again, unless
more structure is imposed, the problem is underdetermined. The authors present
an information-theoretic inversion method called generalized maximum entropy
(GME) for decomposing mass spectra of gas mixtures from noisy measurements.
In this GME approach to the noisy, underdetermined inverse problem, the joint
entropies of concentration, cracking, and noise probabilities are maximized sub-
ject to the measured data. This provides a robust estimation for the unknown
cracking patterns and the concentrations of the contributing molecules. The
method is applied to mass spectroscopic data of hydrocarbons, and the esti-
mates are compared with those received from a Bayesian approach. The authors
also show that the GME method is eﬃcient and is computationally fast. Like the
previous example, an information-theoretic constrained optimization framework
is developed. Here as well some fundamental questions arise. For example, how
should one handle the noise if the exact underlying distribution is unknown?
How can one connect the observed noisy data with the basic entities of interest
(concentration and cracking in this case)?
Similar inferential problems exist also with big data. One trivial example is im-
age reconstruction or a balancing of a very large matrix. The ﬁrst problem is how
to reduce the data (or the dimensionality of the problem) so the reconstruction
will be computationally eﬃcient. Within the information-theoretic constrained
optimization (or inverse) framework discussed here one can solve that problem
as well. Generally speaking, the inverse problem is transformed into a general-
ized moment problem, which is then solved by an information theoretic method.
This estimation approach is robust for a whole class of distributions and allows
the use of prior information. The resulting method builds on the foundations
of information-theoretic methods, uses minimal distributional assumptions, per-
forms well and uses eﬃciently all the available information (hard and soft data).
This method is computationally eﬃcient. For more image reconstruction exam-
ples see Golan, Bhati and Buyuksahin [8]. For other examples within the natural

On the Foundations and Philosophy of Info-metrics
243
sciences see Golan and Volker [9]. For more derivations and examples (small and
large data, mostly within the social sciences) see Golan [7].
Below I provide a partial list of the fundamental open questions in info-
metrics. Naturally, some of these questions are not independent of one another,
but it is helpful to include each one of these questions separately.
1. What is information?
2. What information do we observe?
3. How can we connect the observed information to the basic unit (or entity)
of interest?
4. How should we process the information we observe while connecting it to
the basic unit of interest?
5. Can we quantify all types of information?
6. How can we handle contradicting evidence (or information)?
7. How can we handle complimentary evidence (or information)?
8. Can we deﬁne a concept of “useful” information?
9. Is there a way to assign value to information? If so is it an absolute or a
relative value?
10. How is the macro level information connected to the basic micro level infor-
mation?
11. How to do inference with ﬁnite information?
12. Is there a way for modelling (and developing theories) with ﬁnite/limited
information?
13. How can we validate our theories?
14. Is there a unique relationship between information and complexity?
15. What is a correct inference method? Is it universal to all inferential prob-
lems? (What are the mathematical foundations of that method?)
16. Is the same information processing and inference framework applies across
all sciences?
17. Is a uniﬁed approach to inference and learning necessary and useful?
The above list is not complete but it provides a window toward some pressing
issues within info-metrics that need more research. A more detailed discussion
and potential answers to some of these questions is part of a current and future
research agenda.
6
Summary
In this paper, I summarized some of the fundamental and philosophical principles
of info-metrics. In addition to presenting the basic ideas of info-metrics (including
its precise deﬁnition), I demonstrated some of the basic problems via examples
taken from across the disciplines. A short discussion of the diﬀerent types of
information is provided as well. These include the “hard” information (data),
“soft” information (possible theories, intuition, beliefs, conjectures, etc), and
priors. A generic framework for an information-theoretic inference is discussed.
The basic premise is that since all information is ﬁnite/limited (or since we need

244
A. Golan
to process information in ﬁnite time) we can construct all information processing
as a general constrained optimization problem (information-theoretic inversion
procedure). This framework builds on Shannon’s entropy (Shannon [13]), on
Bernoulli’s principle of insuﬃcient reason (published eight years after his death
in 1713) and on Jaynes principle of maximum Entropy (Jaynes [10,11]) and
further generalizations (e.g., Golan [7]).
The paper concludes with a partial list of open questions in info-metrics.
These questions are related directly to quantitative information processing and
inference and to the meaning of information. In future research these questions
will be tackled one at a time.
References
1. Adriaans, P.: Some open problems in the study of information and computation
(2011), http://staff.science.uva.nl/~pietera/open_problems.html
2. Crnkovic, G.D., Hofkirchner, W.: Floridi’s: Open Problems in Philosophy of Infor-
mation. Ten Years Later. Information 2, 327–359 (2011)
3. van Benthem, J.: Logical Dynamics of Information and Interaction, pp. 1–384.
Cambridge University Press, Cambridge (2011)
4. van Benthem, J., Adriaans, P.: Philosophy of Information. North Holland, Ams-
terdam (2008)
5. Floridi, L.: Open problems in the Philosophy of Information. Metaphilosophy 35,
554–582 (2004)
6. Floridi, L.: The Philosophy of Information, pp. 1–432. Oxford University Press,
Oxford (2011)
7. Golan, A.: Information and Entropy Econometrics – A Review and Synthesis. Foun-
dations and Trends in Econometric 2(1-2), 1–145 (2008); Also appeared as a book
- paperback
8. Golan, Bhati, A., Buyuksahin, B.: An Information-Theoretic Approach for Image
Reconstruction: The Black and White Case. In: Knuth, K. (ed.) 25th International
Workshop on Bayesian Inference and Maximum Entropy Methods in Science and
Engineering, MAXENT (2005)
9. Golan, A., Volker, D.: A Generalized Information Theoretical Approach to To-
mographic Reconstruction. J. of Physics A: Mathematical and General, 1271-1283
(2001)
10. Jaynes, E.T.: Information Theory and Statistical Mechanics. Physics Review 106,
620–630 (1957a)
11. Jaynes, E.T.: Information Theory and Statistical Mechanics II. Physics Review 108,
171–190 (1957b)
12. Proceedings of Info-Metrics Institute Workshop on the Philosophy of Information.
American University, Washington, DC (2011)
13. Shannon, C.E.: A Mathematical Theory of Communication. Bell System Technical
Journal 27, 379–423 (1948)
14. Toussaint, U.V., Golan, A., Dose, V.: Maximum Entropy Decomposition of
Quadruple Mass Spectra. Journal of Vacuum Science and Technology A 22(2),
401–406 (2004)

On Mathematicians Who Liked Logic
The Case of Max Newman
Ivor Grattan-Guinness
Middlesex University Business School, The Burroughs,
Hendon, London NW4 4BT, England
Abstract. The interaction between mathematicians and (formal) logi-
cians has always been much slighter than one might imagine. After a
brief review of examples of very partial contact in the period 1850-1930,
the case of Max Newman is reviewed in some detail. The rather surpris-
ing origins and development of his interest in logic are recorded; they
included a lecture course at Cambridge University, which was attended
in 1935 by Alan Turing.
1
Cleft
One might expect that the importance to many mathematicians of means of
proving theorems, and their desire in many contexts to improve the level of
rigour of proof, would motivate them to examine and reﬁne the logic that they
were using. However, inattention has long been common.
A very important source of maintaining the cleft during the 19th century
is the founding from the late 1810s onwards of the ‘mathematical analysis’ of
real variables, grounded upon an articulated theory of limits, by the French
mathematician A.-L. Cauchy. He and his followers extolled rigour, especially
careful nominal deﬁnitions of major concepts and detailed proofs of theorems.
From the 1850s onwards this aim was enriched by the German mathematician
Karl Weierstrass and his many followers, who brought in, for example, multiple
limit theory, deﬁnitions of irrational numbers, and an increasing use of symbols
– and from the early 1870s, Georg Cantor and his set theory. However, absent
from all these developments was explicit attention to any kind of logic.
This silence continued among the many set theorists who participated in the
inauguration of measure theory, functional analysis and integral equations1. Ar-
tur Schoenﬂies and Felix Hausdorﬀwere particularly hostile to logic, targeting
Bertrand Russell. Even the extensive dispute over the axiom of choice focussed
mostly on its legitimacy as an assumption in set theory and mathematics and
use of higher-order quantiﬁcation [Moore 1982]: its ability to state an inﬁnitude
of independent choices within ﬁnitary logic constituted a special diﬃculty for
logicists such as Russell.
1 The history of mathematical analysis is well covered; cf. especially [Rosenthal 1923;
Bottazzini 1986; Medvedev 1991; Jahnke 2003]. A similar story obtains for complex-
variable analysis.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 245–252, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

246
I. Grattan-Guinness
The creators of symbolic logics were exceptional among mathematicians in at-
tending to logic, but also they made little impact on their colleagues. The algebraic
tradition with George Boole, C. S. Peirce, Ernst Schr¨oder and others from the
mid 19th century was just a curiosity to most of their contemporaries. Similarly,
when mathematical logic developed from the late 1870s, especially with Giuseppe
Peano’s ‘logistic’ programme at Turin from around 1890, he gained many follow-
ers there [Roero and Luciano 2010] but few elsewhere. However, followers in the
1900s included the Britons Russell and A. N. Whitehead, who adopted logistic
(include Cantor’s set theory) and converted it into their ‘logicistic’ thesis that
all the “objects” of mathematics could be obtained; G. H. Hardy but not many
other mathematicians responded [Grattan-Guinness 2000; chs. 8-9]. From 1903
onwards Russell publicised the mathematical logic and arithmetic logicism put
forward from the late 1870s by Gottlob Frege, which had gained little attention
hitherto even from students of foundations and did not gain much more in the
following decades. In the late 1910s David Hilbert started the deﬁnitive phase of
his programme of metamathematics and attracted several followers at G¨ottingen
University and a few elsewhere; however, its impact among mathematicians was
limited even in Germany2.
The next generations of mathematicians include a few distinguished students
of foundations. For example, in the USA from around 1900 E. H. Moore studied
Peano and Hilbert and passed on an interest in logic and model theory to his
student Oswald Veblen, then to Veblen’s student Alonzo Church, and then to his
students Stephen Kleene and Barkley Rosser [Aspray 1991]. At Harvard Peirce
showed multiset theory to the Harvard philosopher Josiah Royce, who was led on
to study logic, and especially to supervise around 1910 C. I. Lewis, Henry Shef-
fer, Norbert Wiener, Morris Cohen and C. J. Ducasse, the last the main founder
of the Association of Symbolic Logic in the mid 1930s [Grattan-Guinness 2002].
In central Europe Johann von Neumann included metamathematics and ax-
iomatic set theory among his concerns [Hallett 1984; ch. 8], while in Poland a
distinguished group of logicians did not mesh with a distinguished group of math-
ematicians even though made both made much use of set theory [Kuratowski
1980]; for example, their joint journal Fundamenta mathematicae (1920+) rarely
carried logical articles.
But the normal attitude of mathematicians remained indiﬀerence. For in-
stance, around 1930 Alfred Tarski and others proved the ‘deduction theorem’
[Tarski 1941; 125-130]; [Kleene 1952; 90-98]; it gained the apathy of the mathe-
matical community, although it came to be noted by the Bourbaki group, who
normally were hostile to logics. (Maybe the reason was that their compatriot
Jacques Herbrand had proved versions of it; if so, it was his sole impact on
2 There does not seem to be an integrated social history of metamathematics, but
one can be pieced together from the temporally ordered trio [Peckhaus 1992; Sieg
1999; Menzler-Trott 2001]. In the early stages Hilbert based logic on the existence
of a thought-source, a rather peculiar notion already found in Dedekind; Zermelo
worked with truth-functions, but the place of logic in his set theory is modest. Com-
pare [Peckhaus 1994].

On Mathematicians Who Liked Logic
247
French mathematics.) Also, Kurt G¨odel’s theorems [G¨odel 1931] on the incom-
pletability of ﬁrst-order arithmetic were appreciated fairly quickly by students
of foundations, but the community did not become widely aware of them until
the mid 1950s [Grattan-Guinness 2011].
2
Newman’s Course at Cambridge
Turing’s own career provides a good example of the cleft; for when he submitted
his paper [Turing 1936] on computability to the London Mathematical Society
they could not referee it properly, because Max Newman was the only other
expert in Britain and he had been involved in its preparation [Hodges 1983, 109-
114]; they seem to have accepted it on Newman’s word. But this detail prompts
an historical question that has not been examined: why was the mathematician
Newman also a logician? An answer is given in the rest of this article; more
details are given in [Grattan-Guinness 2012a]
Although he did not publish much on logic, it is clear that Newman was famil-
iar with the technicalities of several parts of it. In particular, during his wartime
period at Bletchley Park he published three technical papers, one written jointly
with Turing [Newman 1942, 1943; Newman and Turing 1943]. At Cambridge in
the 1930s he had taught a course on ‘Foundations of mathematics’, which, as
is well known, was crucial for Turing when he attend it in 1935; for from it
learnt about recursion theory and G¨odel numbering from the only Briton who
was familiar with it.
Ready for the academic year 1933-1934, Newman ran the course only for the
two succeeding years before it was closed down, perhaps because of disaﬀection
among staﬀas well as among students. In particular, Hardy, despite his familiar-
ity with foundations, opined to Newman in 1937: ‘though “Foundations” is now
a highly respectable subject, and everybody ought to know something about
it, it is, (like dancing or “groups”) slightly dangerous for a bright young math-
ematician!’3. Somehow Newman continued to set questions for ﬁve of the six
years that he was to remain at Cambridge before moving to Bletchley Park in
1942;4 The questions for 1939 may have been set by Turing, who, presumably
in resistance against Hardy’s coolness, was invited to give a lecture course on
foundations in the Lent term of 1939. He was asked to repeat it in 1940; but by
then he also was at Bletchley Park5. How had Newman got involved in logic in
the ﬁrst place?
3 Newman’s archive, Saint John’s College Cambridge; thanks to David Anderson much
of it is available in digital form at http://www.cdpa.co.uk/Newman/. Individual
items are cited in the style ‘NA, [box] a- [folder] b- [document] c’; here 2-12-3.
4 A Mathematics Tripos course in ‘logic’ was launched in 1944 by S. W. P. Steen.
The Moral Sciences Tripos continued to oﬀer its long-running course on the more
traditional parts of ‘Logic’.
5 On Turing’s teaching, cf. [Hodges 1983; 153,177] and the Faculty Board minutes for
29 May 1939.

248
I. Grattan-Guinness
3
Newman’s Way in to Logic
Maxwell Hermann Alexander Neumann (1897-1984) was born in London to a
German father and an English mother. He gained a scholarship to St John’s
College Cambridge in 1915 and took Part I of the Mathematics Tripos in the
following year. But carrying the surname ‘Neumann’ in Britain in the Great War
was not a good idea; the family changed its surname to ‘Newman’, and Max had
to leave his college until 1919, when he returned and completed Part II of the
Tripos very well in 1921.
Then, very unusually, he spent much of the academic year 1922-1923 at Vienna
University. He went with two other members of his college.
One was Lionel Penrose; as a schoolboy he had been interested in Russell’s
mathematical logic, and he specialised in traditional versions of logic as taught
in the Moral Sciences Tripos. But he also examined mathematical logic, and
may well have at least have alerted his friend Newman to the subject, which was
absent from the Mathematics Tripos. He became interested also in psychology
(well represented in his own Tripos syllabus), especially its bearing upon logic,
and he wanted to meet Sigmund Freud and Karl B¨uhler and other psychologists
in Vienna. He seems to have initiated the visit to Vienna; his family was wealthy
enough to sustain it, especially as at that inﬂationary time British money would
last a long time in Vienna. His friendship with Newman was multi-faceted and
deep.
The other was Rolf Gardiner, who was later active in organic farming and folk
dancing, enthused for the Nazis [Moore-Colyer 2001], and was to be the father
of the conductor Sir John Eliot Gardiner. His younger sister Margaret came
along; she became an artist and a companion to the biologist Desmond Bernal.
She recalled ‘the still deeply impoverished town’ of Vienna, where Penrose and
Newman would walk side-by-side down the street playing a chess game in their
heads [Gardiner 1988; 61-68].
Of Newman’s contacts with the mathematicians in Vienna we have only a
welcoming letter of July 1922 from ordinary professor Wilhelm Wirtinger;6 but it
seems clear that his experience of Viennese mathematics was decisive in changing
the direction of his researches. His principal research interest was to become
topology, which was not a speciality of British mathematics. By contrast, in
Vienna some of Wirtinger’s own work related to the topology of surfaces; in 1922
the University recruited Kurt Reidemeister, who was to become a specialist in
combinatorial topology, like Newman himself; Leopold Vietoris was a junior staﬀ
member; and a student was Karl Menger (though rather ill at the time and away
from Vienna).
Most notably, ordinary professor Hans Hahn was not only a specialist in the
topology of curves, and in real-variable mathematical analysis; he also regarded
formal logic as both a research and as a teaching topic. In particular, while
Newman was there he ran a preparatory seminar on ‘algebra and logic’, and
in later years held two full seminars on Principia mathematica. In addition,
6 The Wirtinger letter is at NA, 2-1-2.

On Mathematicians Who Liked Logic
249
he supervised doctoral student Kurt G¨odel working on the completeness of the
ﬁrst-order functional calculus with identity, and as editor of the Monatshefte
f¨ur Mathematik und Physik published both that thesis and the sequel paper
[G¨odel 1931] on the incompletability of ﬁrst-order arithmetic (which was to be
registered as G¨odel’s higher doctorate).
Hahn also engaged in philosophical debates. When he had studied at Vienna
University from the mid 1890s to his higher doctorate in 1905 he had participated
in some of the discussion groups that surrounded certain chairs in the university.
After teaching elsewhere for several years, he returned to Vienna University as
a full professor of mathematics in 1921. During 1922 he led the move to appoint
to the chair of natural philosophy the German physicist and philosopher Moritz
Schlick; after arriving in 1923 Schlick created what was to be known as the
‘Vienna Circle’, with Hahn as a leading member7. Further, while the Circle had
no agreed philosophy among all its members, Schlick, Hahn and later Carnap
strongly advocated positivism and empiricism, acknowledging major inﬂuences
from Ernst Mach (who had held that chair in the 1890s) and Russell.
4
After Vienna
After his return Newman developed as a (pioneer) British topologist, with a
serious interest in logic and logic education and (as we shall soon see) a readiness
to engage with Russell’s philosophy; surely one sees heavy Viennese inﬂuences
here, especially from Hahn.
In 1923 Newman applied for a college fellowship. He submitted a paper
[Newman 1923a] on the avoiding the axioms of choice in developing the theory
of functions of a real variable that was published that year8, some unidentiﬁ-
able discussion of solutions of Laplace’s equation, and a long unpublished essay
[Newman 1923b] in the philosophy of science that was completed in August. Its
title, ‘The foundations of mathematics from the standpoint of physics’, could
well have originated in a Viennese chat. Maybe he wrote some of it there; un-
fortunately the 161 folios do not contain any watermarks.
In this essay Newman took the world of idealised objects that was customary
adopted in applied mathematics (smooth bodies, light strings, and so on) as ‘cer-
tain ideals, or abstractions [...] not applicable to those of real physical objects’,
and contrasted it with the world of real physical that one encounters and on
which he wished to philosophise. He distinguished between these two kinds of
philosophising by the diﬀerent logics that they used. The idealisers would draw
on the two-valued logic, for which he cited a recent metamathematical paper by
Hilbert [1922] as a source; but those interested in real life would go to construc-
tive logic, on which he cited papers by Brouwer [1918-1919] and Hermann Weyl
[1921].
We see here Newman’s notable readiness to admit logical pluralism, and to put
logics at the centre of the analysis of a philosophical problem; most unusual for
7 On the Vienna Circle, cf. [Stadler 2001]; on Hahn, cf. [Sigmund 1995].
8 On the context, cf. passim in [Rosenthal 1923] and [Medvedev 1991].

250
I. Grattan-Guinness
a mathematician, and far more Viennese than Cantabrigian. His college referees,
Ebenezer Cunningham and H. F. Baker, were not impressed by the essay but
recommended the award of the Fellowship. He neither revised the essay nor
seemed to seek its publication, although occasionally he alluded to its concerns;
and it must be at least a major source of his recognition of the importance of
logics.
This essay built upon the awareness of logic that he must have gained at Cam-
bridge from Penrose. That contact will have continued, for after Vienna Penrose
wrote several manuscripts on mathematical logic, especially the psychological as-
pects, in which he was inﬂuenced by Russell and also by Ludwig Wittgenstein’s
notion of tautology given in the recent Tractatus logico-philosophicus (1922). He
worked on a doctoral dissertation on the psychology of mathematics, but then
abandoned it9. From 1925 he studied for a degree in medicine at Cambridge and
London, and became a distinguished geneticist, psychiatrist and statistician, and
also father of the mathematicians Oliver and Sir Roger Penrose [Harris 1973].
An occasion for Newman to exercise his logical and philosophical talents arose
when he attended a set of philosophical lectures that Russell gave in Trinity
College Cambridge in 1926. They went into a book on ‘the analysis of matter’
[Russell 1927]. Newman helped Russell to write two chapters, and when the book
appeared he criticised its philosophical basis most acutely in [Newman 1928];
Russell accepted the criticisms, which stimulated Newman to write Russell two
long letters on logic and on topology [Grattan-Guinness 2012b].
Newman continued to pioneer both topology and logic at Cambridge. Doubt-
less with topology in mind, in 1936, a year before sinking the foundations course,
Hardy had proposed Newman as a Fellow of the Royal Society, with J. E. Lit-
tlewood as seconder, although Newman was no Hardy-Littlewood analyst; the
election was made in 1939. Newman used the Society to support his logical cause.
In 1950 he proposed Turing as a Fellow, seconded by Russell, the election being
accepted in 1951; ﬁve years later he wrote the obituary [Newman 1955] of Tur-
ing. In 1966 Newman proposed and Russell seconded G¨odel as Foreign Member,
duly gained two years later10. In 1970 he agreed to be the obituarist of Russell,
to be helped by the philosopher A. J. Ayer, but he was not well enough to oblige.
He died in 1984.
Among mathematicians who came to like logic, Newman is a very unusual
case. The (sparse) evidence suggests two sources: Penrose’s early interest; and
the unusual mixture of mathematics, logic and philosophy in Vienna, which drew
him also to topology. Thus he changed directions; had he stayed in Cambridge
in 1922-1923, he would have surely continued in the direction indicated by the
paper on avoiding the axioms of choice, namely, Hardy-Littlewood mathematical
analysis. But then his interest in logic could have waned (and in topology never
have ﬂowered), so that maybe no foundations course would have existed for
budding Hardy-Littlewood mathematical analyst Turing to take and thereby to
9 In the Penrose Papers, University College London Archives, cf. especially boxes 20-21
and 26-28.
10 Information comes from Royal Society Archives, and NA, 2-15-10 to -13.

On Mathematicians Who Liked Logic
251
learn of the subjects of recursive functions and undecidability. Then the story
of Bletchley Park and afterwards could have been very diﬀerent; neither he nor
this alternative Newman would have been the obvious choices to go there, nor
would they have been as eﬀective as they actually were. The way that things
turned out for Newman and Turing contained some strokes of luck!
References
Aspray, W.: Oswald Veblen and the Origins of Mathematical Logic at Princeton. In:
Drucker, T. (ed.) Perspectives on the History Of Mathematical Logic, pp. 54–70.
Birkh¨auser, Boston (1991)
Bottazzini, U.: The Higher Calculus. A History of Real and Complex Analysis from
Euler to Weierstrass. Springer, New York (1986)
Gardiner, M.: A Scatter of Memories. Free Association Books, London (1988)
G¨odel, K.: ¨Uber formal unentscheidbare S¨atze der Principia Mathematica und ver-
wandter Systeme. Monatshefte f¨ur Mathematik und Physik 38, 173–198 (1931);
Many reprs. and transs.
Grattan-Guinness, I.: The Search For Mathematical Roots, 1870-1940. Logics, Set The-
ories and the Foundations of Mathematics from Cantor through Russell to G¨odel.
Princeton University Press, Princeton (2000)
Grattan-Guinness, I.: Re-interpreting “
Y
”: Kempe on Multisets and Peirce on Graphs,
1886-1905. Transactions of the C. S. Peirce Society 38, 327–350 (2002)
Grattan-Guinness, I.: The Reception of G¨odel’s 1931 Incompletability Theorems by
Mathematicians, and Some Logicians, up to the Early 1960s. In: Baaz, M., Papadim-
itriou, C.H., Putnam, H.W., Scott, D.S., Harper, C.L. (eds.) Kurt G¨odel and the
Foundations of Mathematics. Horizons of Truth, pp. 55–74. Cambridge University
Press, Cambridge (2011)
Grattan-Guinness, I.: Discovering the Logician Max Newman (in preparation, 2012a)
Grattan-Guinness, I.: Logic, Topology and Physics: Max Newman to Bertrand Russell
(1928) (in preparation, 2012b)
Hallett, M.: Cantorian Set Theory and Limitation of Size. Clarendon Press, Oxford
(1984)
Harris, H.: Lionel Sharples Penrose. Biographical Memoirs of Fellows of the Royal
Society 19, 521–561 (1973); Repr. in Journal of Medical Genetics 11, 1–24 (1974)
Hilbert, D.: Die logischen Grundlagen der Mathematik. Mathematische Annalen 88,
151–165 (1922); Repr. in Gesammelte Abhandlungen, vol. 3, pp. 178-191. Springer,
Berlin (1935)
Hodges, A.: Alan Turing: the Enigma. Burnett Books and Hutchinson, London (1983)
Jahnke, N.H.: A History of Analysis. American Mathematical Society, Providence
(2003)
Kleene, S.C.: Introduction to Metamathematics. van Nostrand, Amsterdam (1952)
Kuratowski, K.: A Half Century of Polish Mathematics. Polish Scientiﬁc Publishers,
Oxford (1980)
Medvedev, F.A.: Scenes from the History of Real Functions. Birkh¨auser, Basel (1991);
translated by R. Cooke
Menzler-Trott, E.: Gentzens Problem. Birkh¨auser, Basel (2001); English ed.: Logic’s
Lost Genius: the Life of Gerhard Gentzen. American Mathematical Society and
London Mathematical Society, Providence (2007)
Moore, G.H.: Zermelo’s Axiom of Choice. Springer, New York (1982)

252
I. Grattan-Guinness
Moore-Colyer, R.J.: Rolf Gardiner, English Patriot and the Council for the Church and
Countryside. The Agricultural History Review 49, 187–209 (2001)
Newman, M.H.A.: On Approximate Continuity. Transactions of the Cambridge Philo-
sophical Society 23, 1–18 (1923a)
Newman, M.H.A.: The Foundations of Mathematics from the Standpoint of Physics
(1923b) manuscript, Saint John College Archives, item F 33.1
Newman, M.H.A.: Mr. Russell’s “Causal Theory of Perception”. Mind 37, 137–148
(1928)
Newman, M.H.A.: On Theories with a Combinatorial Deﬁnition of “Equivalence”. An-
nals of Mathematics 43, 223–243 (1942)
Newman, M.H.A.: Stratiﬁed Systems of Logic. Proceedings of the Cambridge Philo-
sophical Society 39, 69–83 (1943)
Newman, M.H.A.: Alan Mathison Turing. Biographical Memoirs of Fellows of the Royal
Society 1, 253–263 (1955)
Newman, M.H.A., Turing, A.: A Formal Theorem in Church’s Theory of Types. Journal
of Symbolic Logic 7, 28–33 (1943)
Peckhaus, V.: Hilbert, Zermelo und die Institutionalisierung der mathematischen Logik.
Deutschland. Berichte zur Wissenschaftsgeschichte 15, 27–38 (1992)
Peckhaus, V.: Logic in Transition: the Logical Calculi of Hilbert (1905) and Zermelo
(1908). In: Prawitz, D., Westerst˚ahl, D. (eds.) Logic and Philosophy of Science in
Uppsala, pp. 311–323. Kluwer, Dordrecht (1994)
Roero, C.S., Luciano, E.: La scuola di Giuseppe Peano. In: Roero (ed.) Peano e la sua
scuola, Fra matematica, logica e interlingua, Atti del Congresso internazionale di
studi, Torino, October 6-7, 2008, vol. xi–xviii, pp. 1–212. Deputazione Subalpina di
Storia Patria (2010)
Rosenthal, A.: Neuere Untersuchungen ¨uber Funktionen reeller Ver¨anderlichen. In: En-
cyklop¨adie der mathematischen Wissenschaften, vol. 2, pt. C, (article IIC9), pp.
851–1187. Teubner, Leipzig (1923)
Russell, B.A.W.: The Analysis of Matter. Kegan Paul, London (1927)
Sieg, W.: Hilbert Programs: 1917-1922. Bulletin of Symbolic Logic 5, 1–44 (1999)
Sigmund, K.: A Philosopher’s Mathematician: Hans Hahn and the Vienna Circle. The
Mathematical Intelligencer 17(4), 16–19 (1995)
Stadler, F.: The Vienna Circle. Springer, Vienna (2001)
Tarski, A.: Introduction to Logic and to the Methodology of the Deductive Sciences.
Oxford University Press, New York (1941); (1st edn., translated by O. Helmer)
Turing, A.M.: On Computable Numbers, with an Application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society 42(2), 230–265
(1936)
Weyl, C.H.H.:
¨Uber die neue Grundlagenkrise der Mathematik. Mathematische
Zeitschrift 10, 39–79 (1921); Repr. in Gesammelte Abhandlungen, vol. 2, pp. 143-180.
Springer, Berlin (1968)

Densities and Entropies in Cellular Automata
Pierre Guillon1,2 and Charalampos Zinoviadis1
1 Department of Mathematics, University of Turku, 20014 Turku, Finland
chzino@utu.fi
2 CNRS & Institut de Math´ematiques de Luminy, Campus de Luminy, Case 907,
13288 Marseille cedex 9, France
pguillon@math.cnrs.fr
Abstract. Following work by Hochman and Meyerovitch on multi-
dimensional SFT, we give computability-theoretic characterizations of
the real numbers that can appear as the topological entropies of one-
dimensional and two-dimensional cellular automata.
1
Introduction
Cellular automata are a widely-used model for complex systems or computation,
consisting in a network of cells each of whose is in one among a ﬁnite number of
states, that is updated synchronously in parallel as a function of the sates of its
neighbors. Their entropy is a measure of how complex or random the local long-
term behavior can look like. The entropy of cellular automata has been proven un-
computable in [1] (see also [13] for subshifts), but the question remained whether
the entropy of a single given cellular automaton could be an uncomputable num-
ber. Recently, M. Hochman and T. Meyerovitch have characterized the entropies
of 2-dimensional SFT [7] and 3-dimensional CA [5] as, respectively, the right-
computable numbers and the limits of computable increasing sequences of such
numbers. We prove here that these two classes still characterize the possible en-
tropies of, respectively, 1-dimensional and 2-dimensional CA. To do so, we adapt
their homogeneous encoding [7], J. Kari’s determinization signals [8] and P. G´acs’s
self-similar construction [3]. The result brings new equivalences between classes
that are equally natural in computability theory and dynamical systems; we also
believe that the construction in itself is promissing, and could help understand the
real computational power of these natural models.
In Section 2 we introduce the notions and a brief state of the art. In Section 3
we state our main results, and the following sections are devoted to sketching their
proofs. The algorithmic part, as well as entropy proofs and a sketch of the main
construction can be found in [4].
2
Preliminaries
2.1
Conﬁgurations
  will denote the set of natural numbers,
 1 the set
  \ {0} of positive natural
numbers and i, j the integer interval {i, . . . j}, for 0 ≤i ≤j.
+ is the set of
nonnegative real numbers.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 253–263, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

254
P. Guillon and Ch. Zinoviadis
Let A be a ﬁnite set called the alphabet and d ∈
 1 the dimension. Any
element x of A
 d is called a conﬁguration, and xi is called the state of cell i.
The set of conﬁgurations forms a compact topological space when endowed with
the product of the discrete topology.
For any q ∈A, ∞q∞denotes the q-uniform conﬁguration of A
 d, all of whose
cells are in state q. If U ⊂
d, x|U is the pattern representing the restriction of
x to U. For instance, we can deﬁne the central pattern x|B(r) of width r, where
B(r) = −r, rd.
2.2
Symbolic Dynamics
d acts on A
 d by the shift: to any k ∈
d we associate the homeomorphism
σk : A
 d →A
 d deﬁned by ∀x ∈A
 d, ∀i ∈
d, σk(x)i = xi+k. A (d-dimensional,
or dD) subshift is the set X =

x ∈A
 d ∀U ⊂
finite
d, k ∈
d, σk(x)|U /∈F

of
conﬁgurations that avoid some particular set F of ﬁnite patterns. Equivalently,
a subshift is a subset which is invariant by σk for any k ∈
d and topologically
closed. It is of ﬁnite type (SFT) if F can be chosen ﬁnite.
Let X ⊂A
 d be a subshift. The language of support U ⊂
d of X is
LU(X) =

x|U
 x ∈X

. Its complexity of support U is KU(X) = |LU(X)|.
The (topological) entropy of X is H(X) = limr→∞
log KB(r)(X)
|B(r)|
. This is always a
limit, but may be inﬁnite. Note that if Y ⊂B
 d is another subshift, then X × Y
can be essentially seen as a subshift of (A × B)
 d, and its entropy is the sum of
those of X and of Y .
A subshift Y ⊂B
 d is a letter factor of X ⊂A
 d if there exists some letter
projection π : A →B such that the corresponding global map Π : X →Y ,
deﬁned by the parallel application of π, is onto (we say that X letter-factors
onto Y ). A subshift is called soﬁc if it is a letter factor of some SFT.
The same deﬁnitions hold for (one-sided) subshifts over A
1.
The trace of X according to vector v ∈
d and width k is the (d−1)D subshift
τ k
v(X) =

(x|0,k×{0}+nv )n∈  x ∈X

over alphabet Ak. The directional en-
tropy according to vector v is the limit Hv (X) of the entropies of τ k
v (X), when k
goes to inﬁnity (see [9]). One can see that He2 (X) = limk→∞limr→∞
log Nk,r(X)
r
,
where Nk,r(X) = K0,k×0,r(X).
Let X and Y be 2D subshifts. We say that X simulates Y with parameters
B, T if there exists Z ⊂X such that X = 
0≤i<B,0≤j<T σ(i,j)(Z) and that
Z<B×T > =

(x|kB,(k+1)B×lT,(l+1)T  )k,l∈  (xi,j)i,j∈  ∈Z

is a subshift that
letter-factors onto Y , i.e., any conﬁguration of X can be divided into B × T
rectangles that project onto letters of Y . A simulation is an r-simulation if the
letters onto which an array of (2r + 1) horizontally consecutive rectangles of size
B × T project uniquely determine the central rectangle.
The following lemma will be useful in the sequel. (e1, e2) denotes the canonical
base for
2.

Densities and Entropies in Cellular Automata
255
Lemma 1 ([4]). Let X and Y be 2D subshifts such that X l-simulates Y with
parameters B, T. Then, He1 (X) ≤He1 (Y ) /B and He2 (X) ≤He2 (Y ) /T .
2.3
Cellular Automata and Determinism
A cellular automaton (CA) is a system F : A
 d →A
 d such that Fσk = σkF;
equivalently there is a radius r ∈
 1 and a local rule f : AB(r) →A such that
∀x ∈A
 d, ∀i ∈
d, F(x)i = f(x|i+V ). The entropy H(F) of F is the limit, when
r goes to inﬁnity, of the entropy of the subshift

(F t(x)|B(r) )t∈
 x ∈A
 d
.
We say that an SFT X ⊂A
 2 is south-deterministic if there is a map
F : τ 1
e1 →τ 1
e1 that maps any line of a valid tiling to a unique line that can
appear above, i.e., ∀x ∈X, j ∈
, F(x
 ×{j}) = x
 ×{j+1}. It is known that F
can actually be taken to be the restriction of a CA over alphabet A⊔{⊥}, where
⊥must be understood as “extension not deﬁned”; and the entropy of F is equal
to He1 (X) (intuitively, this comes from the fact that state ⊥will remain forever
and not contribute to the entropy). X is south-west-deterministic if there
is the same kind of CA on the diagonal, i.e., ∀x ∈X, j ∈
, F((xi,j)i=−j) =
(xi,j)i=1−j.
Let us say that a 2D subshift is S0-soﬁc if it is a letter-factor of some south-
deterministic SFT with null entropies, i.e., directional entropy 0 according to
any vector.
2.4
Eﬀectiveness
In A
 d, it is easy to enumerate computationally a base of open sets (consider the
sets of conﬁgurations sharing a given pattern as a central pattern). That way, we
can deﬁne an eﬀectively closed subset S ⊂A
 d as the complement of the union
of a computable sequence of open sets. It is an eﬀective subshift if, besides,
it is a subshift. For instance, trace of SFT are eﬀective subshifts. Eﬀectively
closed sets can also be deﬁned in other Cantor sets; in A
1 they correspond to
sets of conﬁgurations that are not ultimately rejected when scanned by some
given TM. An eﬀective system is an eﬀectively closed subset S ⊂(A
1)
 d
which is invariant by the
d-shift. Intuitively, it is a dynamical system where
the preimages of open sets can be computed.
A Π1 (or right-computable) number is the limit of a decreasing computable
sequence of rational numbers. A Σ2 number is the limit of an increasing com-
putable sequence of Π1 numbers. Equivalently, there exists an algorithm that on
input k outputs the code of another algorithm Mk such that Mk enumerates the
approximations of a Π1 number hk, the sequence hk is increasing and converges
to h. The set of Σ2 is strictly larger than the set of Π1 numbers, which, in turn,
is strictly larger than the set of computable (Δ1) numbers. We refer to [14] for
more on these classes of numbers (and many more).
Remark 1. The binary representations of real numbers from an interval [0, α]
form an eﬀectively closed subset of

 if and only if α is Π1.

256
P. Guillon and Ch. Zinoviadis
3
Results
Some evidence of the computing power of a given model can be given by studying
the class of numbers that can be realized as entropy. Elegant characterizations
have recently been achieved for multidimensional SFT.
Theorem 1 ([7,5]). For d ≥2, the class of entropies of d-dimensional SFT
(resp. d-dimensional soﬁc subshifts, eﬀective subshifts) is
+ ∩Π1.
In the broader case of eﬀective systems (and as a consequence for high-dimension
CA), the class of entropies that can be realized is larger.
Theorem 2 ([5]). For d ≥3, the class of entropies of d-dimensional CA (resp.
eﬀective systems) is
+ ∩Σ2 ∪{∞}.
The last two theorems have left open the case of entropies realized by 1D and
2D CA, that are both included in Σ2. The main purpose of the present article is
to solve these two remaining cases. The ﬁrst step of the answer is given by the
following result:
Theorem 3 ([11]). The entropy of a 1D CA is equal to the entropy of some
trace of the corresponding 2D SFT.
From the theorem above, the entropy of a 1D CA is thus Π1. We will actually
prove that the converse is also true.
Theorem 4. The class of entropies of 1D CA is
+ ∩Π1.
This class of numbers is thus strictly weaker than the possible entropies of 3D
CA, characterized in [5]. However, this is not true for the 2D case.
Theorem 5. The class of entropies of 2D CA is
+ ∩Σ2 ∪{∞}.
4
Construction
4.1
Density Encoding
This subsection is devoted to encoding data in the density of the conﬁgurations.
The most relevant is actually the binary case, which follows the construction in
[7]. A 1-net is a family (2n
 + kn)n∈1 of pairwise disjoint subsets of
 called
levels, where (kn)n∈1 ∈

1. It can be seen that for any 1-net, there is at
most one cell i ∈
 which does not belong to any level.
Let us denote |u|a the number of occurrences of letter a in word u. The
frequency of a letter a ∈A in some one-dimensional conﬁguration x ∈A
  is,
if ever it exists, the limit δa(x) = limr→∞
x|B(r)

a/|B(r)|.
If α, β ∈A
1, we note α ∼β if α = β or there exists i ∈
 1 such that
∀j < i, αj = βj, and ∀j > i, αi = βj and αj = βi. This is an equivalence
relation, for which all the classes have cardinal one or two. As an example,
two binary sequences are equivalent for ∼if and only if they represent binary

Densities and Entropies in Cellular Automata
257
expansions of the same real number in [0, 1[. Let 
A
1 be the quotient of A
1 by
this equivalence relation. It can be endowed with the induced topology from the
product topology. We will often confuse a sequence x and its equivalence class.
If α ∈A
1, we note Dα ⊂A
  the set of Tœplitz conﬁgurations which are
constantly equal to αn on level 2n
 + kn for some 1-net (2n
 + kn)n∈. If
S ⊂A
1, we note DS = 
α∈S Dα. These sets have interesting properties.
Remark 2.
1. For any nonempty closed set S ⊂A
1, DS is a nonempty subshift.
2. The frequency of any letter a ∈A in any conﬁguration x ∈Dα is 	
αi=a 2−i.
In particular if α is binary, then it is a binary expansion of δ1(α).
3. If α ∼β, then Dα = Dβ; otherwise, Dα ∩Dβ = ∅.
4. Let x ∈Dα, j ∈
, and i be an odd number. Then x|i +j is still in Dα.
Point 3 of the previous remark suggests that it is relevant to talk about Dα (resp.
DS) for an equivalence class α ∈
A
1, or for a real number α ∈[0, 1] (resp. a set
S ⊂
A
1 of classes).
Moreover, the sequence α encoded in the densities of the subshift can actually
(up to equivalence) be eﬀectively approximated by reading ﬁnite patterns.
Lemma 2 ([4]). There exists a TM M▷which, given a word u over alphabet
A, outputs a word v such that, if u = x|0,2n for some x ∈Dα and some n ∈
 ,
then v = β1,n for some β ∼α′ and α′
1,n = α1,n.
We say that a TM has input in 
A
1 if it reads sequences of A
1 as input, and
gives the same result for sequences in the same equivalence class. We can also
assume that, if α ∼α′, then this TM stops after the same number of steps for
α and α′.
Lemma 3 ([4]). For any TM
˜
M with input in 
A
1, there exists a TM M with
input in A
1 such that:
– If
˜
M halts over input α ∈A
1, then there exists k ∈
  such that for any
conﬁguration x ∈Dα, M halts over input x|0,k before time k;
– otherwise, M does not halt over any input x ∈Dα.
The following corollary is a direct application of Lemma 3 with a machine re-
jecting conﬁgurations outside some eﬀectively closed set.
Corollary 1. If S ⊂
A
1 is eﬀectively closed, then DS is an eﬀective subshift.
4.2
Checking Homogeneity
Our proof involves a deterministic SFT that is built layer by layer: the state of
each cell is in a product of alphabets that we deﬁne one after the other, each
layer having to respect some local constraints in how it can be superimposed
with the previous ones. For α ∈A
1 (resp. S ⊂A
1), let us note D∗
α (resp. D∗
S)

258
P. Guillon and Ch. Zinoviadis
the set of conﬁgurations x ⊂A
 2 which are constant vertically, and where each
row (xi,k)i∈  is in DS, for k ∈
.
The purpose of this subsection is to build an SFT which checks that some
layer is well homogeneous, in the sense of the following lemma; this follows [7,
Section 6], but contrary to this, keeping determinism and null entropies forces us
to go back to the actual SFT construction rather than directly invoke Mozes’s
theorem for 2 × 2-substitutions.
Lemma 4. D∗
A
 1 is S0-soﬁc.
We will only give a sketch of the proof. A 2-net is a family (In × Jn)n∈1 of
products of levels of two 1-nets (In)n∈1 and (Jn)n∈1. Each In × Jn itself is
called the level n of the net. The In (resp. Jn) being pairwise disjoint, it follows
that a horizontal (resp. vertical) line can intersect at most one level of the 2-net.
If i ∈In, then {i} ×
 is called a column of level n. By deﬁnition, columns of
level n appear with horizontal period 2n.
In [12], Robinson constructed an SFT R in which every conﬁguration is divided
regularly into squares of size 2n for every n. In particular, he mentions, in other
terms, the following property about the good repartition of a particular state
called a cross.
Lemma 5 ([12]). For every x ∈R, the set

i ∈
2 xi is a cross

is a 2-net.
Now, this SFT has been made deterministic in [8], by adding to it a layer with
signals that forbid some conﬁgurations that would share the same bottom-left
half as another one. The result can be restated as follows.
Lemma 6 ([8]). There exists a south-west-deterministic SFT −→
R that letter-
factors onto some nonempty subsystem of R.
Proof (of Lemma 4). Let us ﬁrst deﬁne a south-west-deterministic SFT ˜R′,
in which conﬁgurations are vertically constant and correspond horizontally to
D∗
A
 1 . ˜R′ ⊂R×A
 2 ×A
 2 is deﬁned with three layers: the ﬁrst one contains the
deterministic Robinson SFT −→
R; the second one is constant horizontally; the third
one is constant vertically. We additionally require that if the ﬁrst layer is a cross,
then the other two must coincide. ˜R′ is south-west-deterministic, since all three of
its layers are. Now it is not diﬃcult to turn this SFT into a south-deterministic
one, by simply considering ˜R =

(x(i,j−i))(i,j)∈ 2
 (xi,j)(i,j)∈ 2 ∈˜R′
, whose
columns correspond to columns of ˜R′, but lines correspond to north-west-to-
south-east diagonals of ˜R′.
Null entropies come from the substitutive nature of R, which is transmitted
to ˜R. More details about this can be found in [4].
⊓⊔
4.3
Checking the Density
In this section, we construct a south-deterministic SFT with null entropies which
letter-factors onto D∗
S. In the SFT, there is a special layer which consists exactly

Densities and Entropies in Cellular Automata
259
in D∗
S: from Lemma 4, we can a priori assume that all conﬁgurations of this
layers are in D∗
A
 1 , by implicitly having a layer in ˜R. We will now add a layer
whose purpose is to check that if x ∈D∗
α is read from this layer, with α ∈
A
1,
then α is really in the wanted set S, by simulating the application of a machine
M corresponding to the machine
˜
M that rejects any conﬁguration that is not
in S (see Lemma 3).
A naive simulation of the machine for an inﬁnite time would create invalid
limit conﬁgurations. A solution to this problem is to build the additional layer
in a self-similar way, in the fashion of [3,2,6]: we build a family of south-
deterministic SFT (Yn) such that Yn simulates the TM for n steps, and also
simulates Yn+1 with some parameters Bn, Tn. That way, if n was not enough to
ﬁgure out that the input had to be rejected, then a higher level will notice it.
More precisely, Yn will be able to apply the TM over the input x|Bn+10,Bn+j
for some j ∈0, Bn+1. The simulation of Yn+1, as deﬁned previously, consists in
dividing naturally every valid conﬁguration of Yn into rectangles of size Bn × Tn
called the Yn-macrotiles. An important feature is that this family admits a
uniform description: one single SFT is actually described. Each conﬁguration is
conscious of the level Yn it belongs to, and will check that it simulates a conﬁgu-
ration of the next one. The details of the construction ensuring these conditions
can be found in [4].
The following lemma applies machine M from Lemma 3 to ﬁnite conﬁgura-
tions composed of some arithmetic progressions in lines of the SFT, that are still
in Dα. Null entropies come from the self-simulation.
Lemma 7 ([4]). If S ⊂
A
1 is an eﬀectively closed set, then D∗
S is S0-soﬁc.
4.4
From Density to Entropy
Finally, let us see how Lemma 7 can be used to prove Theorem 4: it simply inde-
pendently splits each letter 1 into two letters, so that its density is transformed
into entropy.
Proof (of Theorem 4). One direction corresponds to Theorem 3. Let us prove the
converse. Should we make the product with the shift over 2⌊α⌋symbols, whose
entropy is ⌊α⌋, we can assume that α ∈[0, 1[.
Let F be the shift composed with the CA corresponding to the deterministic
SFT given by Lemma 7 for the eﬀectively closed set S consisting of binary
representations of real numbers from the interval [0, α], A its alphabet, and
π : A →
 be the corresponding letter projection. Let ˜F be the CA over alphabet
(A × {0}) ⊔(π−1(1) × {1}) such that the ﬁrst component performs F and the
second one performs the shift. , i.e., in the ﬁrst component we can see the 0-
entropy F and, in the second one the one-dimensional subshift:
D∇
S =

(yi)i∈  ∈

  ∃(xi)i∈  ∈DS, ∀i ∈
, if xi = 0, then yi = 0

.
It is known that the entropy of a product is the sum of the entropies, hence the
entropy of ˜F is that of D∇
S .

260
P. Guillon and Ch. Zinoviadis
KU(D∇
S ) = 	
u∈LU(DS) 2|u|1 can be bounded by KU(DS)2supu∈LU (DS )|u|1.
Hence, the entropy H(D∇
S ) is:
H(D∇
S ) = lim
r→∞
log KB(r)(D∇
S )
|B(r)|
≤H(DS) + lim
r→∞
sup
u∈LB(r)(DS)
|u|1
|B(r)|.
However, since H(DS) = 0, H(D∇
S ) is not more than the maximal density
α of conﬁgurations of DS. Conversely, if x ∈Dα ⊂DS, then LB(r)(D∇
S ) ⊃

(xi, yi)|i|<r
 ∀i ∈
, yi ∈{xi, 2xi}

; hence KB(r)(DS) ≥2|xB(r)|1 and H(D∇
S ) ≥
lim supr→∞
xB(r)

1 = α. Therefore, H( ˜F) = H(D∇
S ) = α.
⊓⊔
5
The Second Dimension
Let us now prove Theorem 5, dealing with 2D CA. The ﬁrst inclusion is direct
from Theorem 2. The idea here will be to realize, in each horizontal slice, some
right-computable number, as in the previous section. These slices will actually
be parameterized by some index encoded in its density, that is increased by
one between consecutive slices, and that will give a sequence approximating the
wanted Σ2. The trick is that the encoding has to be spare in order to prevent
limit conﬁgurations to achieve too much entropy; this has to be compensated by
having actual groups of consecutive slices hold the same parameter.
Let us denote by (w)4 the 4-ary representation of a natural number w ∈
  over
{0′, 1′, 2′, 3′}. Let (Sk)k∈1 be a computable sequence of eﬀectively closed subsets
of 
A
1, and S =

∗k(w)4y
 k ∈
 , 0 ≤w ≤4k −1, y ∈DSk

∪{∗∞, ♯∞} a set of
sequences over alphabet A = {∗, 0′, 1′, 2′, 3′, 0, 1, ♯}. Consider S′ = S1 ∪S2 ∪S3,
where:
S1 =

(z, z′) ∈S2 ∃k, y, w ∈

0, 4k −1

, z = ∗k(w)4y and z′ = ∗k(w + 1)4y

;
S2 =

(z, z′) ∈S2 ∃k, y, z = ∗k(4k −1)4y and z′ = ♯∞
;
S3 =

(z, z′) ∈S2 z = ∗∞and z′ = ∗∞or ∃k, y, z′ = ∗k(0)4y

.
Lemma 8 ([4]). S′ is an eﬀectively closed subset of

(A × A)
1.
We are now ready to characterize the entropies of 2D CA. Similarly to the one-
dimensional case, a 2D CA corresponds to a south-deterministic 3D SFT, up to
adding a spreading state, and its entropy can be seen as the directional one for
the south-to-north unitary vector.
Proof (of Theorem 5). Let αk be a computable sequence of Π1 numbers, Sk =
[0, αk], M the TM given by Lemma 8, Y the 2D SFT given by Lemma 7.
Consider now the following 3D SFT Y ′: each horizontal slice must satisfy the
conditions of Y . The only vertical local constraint we add is the following: the
second letter (in A) of the pair held by a tile is equal to the ﬁrst letter of the tile
on top of it. Intuitively, the way to think about this is that when a horizontal

Densities and Entropies in Cellular Automata
261
slice is considering whether it should accept or reject its input (the ﬁrst sequence
it holds), it can also read as input the sequence of the slice above it (the second
sequence).
Y ′ is south-deterministic. Indeed, every horizontal slice is an element of Y ,
which is a 2D south-deterministic SFT. Hence, if we know a slice x|  ×{n}×   ,
we can uniquely determine x|  ×{n+1}×   . Moreover, Y ′ has null entropies, as a
subshift of an inﬁnite product of 2D SFT with null entropies.
Let us now modify the SFT in order to get the wanted entropy. We need
to understand the structure of the conﬁgurations. From now on, we forget the
second sequence encoded in every horizontal slice and we work only with the ﬁrst
one. If zk is the sequence encoded in the kth horizontal slice, then the sequence
(zk)k∈  can only have one of the following forms:
– zk = ∗∞, for all k ∈
;
– there exist m ∈
, k ∈
 1 and y ∈D[0,αk] such that zi = ∗∞for i < m,
zi = ∗k(i −m)4y for m ≤i < m + 4k, and zi = ♯∞for i ≥m + 4k.
– zk = ♯∞, for all k ∈
;
This follows directly from the deﬁnition of S′. For k ∈
 1, let Y ′(k) ⊆Y ′ consist
of those conﬁgurations whose horizontal slices are either ∗∞, ♯∞, or contain
∗k(0)4y for some y ∈D[0,αk]. It is a subshift.
Let us allow splitting of the letter 1 into two (by adding a second, binary, layer,
as in the proof of Theorem 4), independently in every horizontal slice. Then, in
conﬁgurations of the subsystem Y ′(k) there are 4k slices where splitting is done
and each one contributes up to 4−kβ to the entropy, where β ∈[0, αk] is such
that y ∈β. This happens because in every slice, y is encoded in 2-net starting
from level 2k. Since splitting is done independently in 4k slices, the entropy of the
subsystem Y ′(k) is β. By the variational principle, and since the nonwandering
system of the CA is included in the disjoint union of the Y ′(k) and the trivial
subsystems, we have that the entropy of F in the vertical direction is:
H(F) = sup
k∈
sup
0≤β≤αk
β = sup
k∈
αk,
which is the wanted Σ2 number.
⊓⊔
6
Conclusion
We have reached a characterization of the entropies of CA in terms of com-
putability classes. This is inspired by what had been done over multidimensional
SFT, but the construction presents some intrinsically interesting points, such as
determinization widgets, self-similar construction, or a generalized encoding of
conﬁgurations into densities.
This problem helps us understand what kind of results on tilings could be
adapted to CA, that is when one of the dimensions of the system actually repre-
sents a deterministic temporal evolution. It could be interesting to try to adapt

262
P. Guillon and Ch. Zinoviadis
some more results from multidimensional symbolic dynamics, such as the substi-
tutions of [10], or the characterization of subactions in [5,2]. Nevertheless, when
translating into cellular automata, we will in general have to deal with wandering
points, which could be omitted here in the study of entropy but may sometimes
alter signiﬁcantly the results.
Among open problems, we could try to characterize the entropies of restricted
classes of CA: requiring transitivity constraints, or reversibility. The latter case
might be achieved by adapting our proof while requiring two-way determinism in
the underlying tilings (but again extending it to a full set of conﬁgurations may
be diﬃcult). We could also study the entropies of other computationally-inspired
dynamical systems, such as Turing machines with moving tapes.
Acknowledgements. This project was supported by the Academy of Finland
Grant 131558. The second author was also supported by the Finnish Academy
of Science and Letters and the Turku Center for Computer Science. The ﬁrst
author was supported by the ANR Projet Blanc “EMC”. Special thanks to Alexis
Ballier, Timo Jolivet, Jarkko Kari and Pascal Vanier for interesting discussion
around that matter.
References
1. ˇCulik II, K., Hurd, L.P., Kari, J.: The topological entropy of cellular automata is
uncomputable. Ergodic Theory & Dynamical Systems 12(2), 255–265 (1992)
2. Durand, B., Romashchenko, A., Shen, A.: Fixed-point tile sets and their applica-
tions (September 2010), draft
3. G´acs, P.: Reliable cellular automata with self-organization. Journal of Statistical
Physics 102(1-2), 45–267 (2001), http://www.cs.bu.edu/fac/gacs/recent-publ.
html
4. Guillon, P., Zinoviadis, C.: Densities and entropies in cellular automata (2012),
arXiv:1204.0949
5. Hochman, M.: On the dynamics and recursive properties of multidimensional
symbolic systems. Inventiones Mathematicæ 176(1), 131–167 (2009), http://www.
springerlink.com/content/h66428l759545081
6. Hochman, M.: Expansive directions for
 2 actions. Ergodic Theory & Dynamical
Systems 31(1), 91–112 (2011)
7. Hochman, M., Meyerovitch, T.: A characterization of the entropies of multidimen-
sional shifts of ﬁnite type. Annals of Mathematics 171(3), 2011–2038 (2010),
http://pjm.math.berkeley.edu/annals/ta/080814-Hochman/080814-Hochman-
v1.pdf
8. Kari, J.: The nilpotency problem of one-dimensional cellular automata. SIAM Jour-
nal on Computing 21(3), 571–586 (1992)
9. Milnor, J.: On the entropy geometry of cellular automata. Complex Systems 2(3),
357–385 (1988)
10. Mozes, S.: Tilings, substitution systems and dynamical systems generated by them.
Journal d’analyse math´ematique 53, 139–186 (1988)

Densities and Entropies in Cellular Automata
263
11. Park, K.K.: Entropy of a skew product with a ϝ2 -action. Paciﬁc Journal of Math-
ematics 172(1), 227–241 (1996), http://projecteuclid.org/euclid.pjm/110236
6193
12. Robinson, R.M.: Undecidability and nonperiodicity for tilings of the plane. Inven-
tiones Mathematicæ 12(3) (1971)
13. Simonsen, J.G.: On the computability of the topological entropy of subshifts. Dis-
crete Mathematics & Theoretical Computer Science 8, 83–96 (2006),
www.dmtcs.org/dmtcs-ojs/index.php/dmtcs/article/download/456/1602
14. Zheng, X., Weihrauch, K.: The Arithmetical Hierarchy of Real Numbers. In:
Kutylowski, M., Wierzbicki, T., Pacholski, L. (eds.) MFCS 1999. LNCS, vol. 1672,
pp. 23–33. Springer, Heidelberg (1999)

Foundational Analyses of Computation
Yuri Gurevich
Microsoft Research, One Microsoft Way, Redmond, WA 98052-6399,
United States of America
Give me a fulcrum, and I shall move the world.
—Archimedes
Abstract. How can one possibly analyze computation in general? The
task seems daunting if not impossible. There are too many diﬀerent kinds
of computation, and the notion of general computation seems too amor-
phous. As in quicksand, one needs a rescue point, a fulcrum. In compu-
tation analysis, a fulcrum is a particular viewpoint on computation that
clariﬁes and simpliﬁes things to the point that analysis becomes possible.
We review from that point of view the few foundational analyses of
general computation in the literature: Turing’s analysis of human com-
putations, Gandy’s analysis of mechanical computations, Kolmogorov’s
analysis of bit-level computation, and our own analysis of computation
on the arbitrary abstraction level.
1
Introduction
Algorithms and computations are closely related concepts. Syntactically algo-
rithms are programs (or recipes) but semantically they specify computations.
And the only computations that we consider here are algorithmic (also known
as mechanical). In this paper, we abstract from the syntax of algorithms, so that
analysis of algorithms and analysis of computation are one and the same.
Turing’s analysis of algorithms was provoked by the Entscheidungsproblem,
the problem whether the validity of ﬁrst-order formulas is computable. Logicians
have been interested in what functions are computable, and Turing’s analysis is
often seen from that point of view. But there may be much more to an algo-
rithm than its input-output behavior. In general algorithms perform tasks, and
computing functions is a rather special class of tasks.
Here we concentrate on foundational analyses of algorithms/computations,
not on what functions are computable.
2
Turing
Alan Turing analyzed computation in his 1936 paper “On Computable Num-
bers, with an Application to the Entscheidungsproblem” [21]. The validity re-
lation on ﬁrst-order formulas can be naturally represented as a real number,
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 264–275, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Foundational Analyses of Computation
265
and the Entscheidungsproblem becomes whether this particular real number is
computable. “Although the subject of this paper is ostensibly the computable
numbers, it is almost equally easy to deﬁne and investigate computable functions
of an integral variable or a real or computable variable, computable predicates,
and so forth. The fundamental problems involved are, however, the same in
each case, and I have chosen the computable numbers for explicit treatment as
involving the least cumbrous technique” [21, p. 230].
How could Turing analyze computation in such generality? The world of algo-
rithms is large and diverse. Explicitly or implicitly, he imposed some constraints
on the computations in consideration. And he found a fulcrum. We start with the
fulcrum. There were no computers in Turing’s time1 but that does not seem to
make Turing’s task much simpler. Humans are hard to analyze. Amazingly Tur-
ing found a way to do just that: Ignore how the algorithm is given, ignore what
human computers have in their minds, and concentrate on what the computers
do, what their observable behavior is. That is his fulcrum.
One may argue that Turing did not ignore the mind. He speaks about the
state of mind of the human computer explicitly and repeatedly. For example,
he says that “[t]he behaviour of the computer at any moment is determined by
the symbols which he is observing, and his ’state of mind’ at that moment” [21,
p. 250]. But Turing postulates that “the number of states of mind which need
be taken into account is ﬁnite.” The computer just remembers the current state
of mind, and even that is not necessary: “we avoid introducing the ’state of
mind’ by considering a more physical and deﬁnite counterpart of it. It is always
possible for the computer to break oﬀfrom his work, to go away and forget all
about it, and later to come back and go on with it. If he does this he must leave
a note of instructions (written in some standard form) explaining how the work
is to be continued. This note is the counterpart of the ’state of mind’.”
Turing introduced abstract computing machines that became known as Tur-
ing machines (and constructed a universal Turing machine). He deﬁned a real
number to be computable “if its decimal can be written down by a [Turing] ma-
chine” [21, p. 230]. His thesis was that Turing computable numbers “include all
numbers which could naturally be regarded as computable” (Turing [21, p. 230]).
He used the thesis to prove the undecidability of the Entscheidungsproblem. To
convince the reader of his thesis, Turing used three arguments.
Reasonableness: He gave examples of large classes of real numbers which are
[Turing] computable.
Robustness: He gave another explicit deﬁnition of computability and proved it
is equivalent to the original one “in case the new deﬁnition has a greater in-
tuitive appeal.” The robustness argument was strengthened in the appendix
where, after learning about Church’s explicit deﬁnition of computability [6],
he proved the equivalence of their deﬁnitions.
Appeal to Intuition: He analyzed computation appealing directly to intuition.
1 “Numerical calculation in 1936 was carried out by human beings; they used mechan-
ical aids for performing standard arithmetical operations, but these aids were not
programmable” (Gandy [8, p. 12]).

266
Y. Gurevich
The ﬁrst two arguments are important but insuﬃcient. There are other reason-
able and robust classes of computable real numbers, e.g., the class of primitive
recursive real numbers. The direct appeal to intuition is crucial.
While Turing’s analysis is very general, his algorithms are subject to some
constraints. Here are some of them.
Symbolic: Computation is symbolic (or digital, symbol-pushing).
Sequential Time: Computation splits into a sequence of steps.
Bounded Work: Only bounded work is performed at any one step.
Isolated Computation is self-contained. No oracle is consulted, and nobody in-
terferes with the computation either during a computation step or in between
steps. The whole computation of the algorithm is determined by the initial
state.
2.1
Discussion
Q2 Did Turing really impose the symbolic constraint?
A: Yes, he did. “Computing is normally done by writing certain symbols on
paper,” writes Turing [21, p. 249], and he analyses only such computations.
Q: Is this really a constraint?
A: These days we are so accustomed to digital computations that the symbolic
constraint may not look like a constraint. But it is. Non-symbolic computations
have been performed by humans from ancient times [13, §3].
Q: I came across a surprising remark of G¨odel that Turing’s argument “is sup-
posed to show that mental procedures cannot go beyond mechanical procedures”
[9]. I believe that Turing’s goal was to analyze mechanical procedures. Since such
procedures were executed by humans in his time, he had to analyze human ex-
ecution of mechanical procedures; there was no other way.
A: We may never know what goal was in Turing’s head; let’s hear G¨odel’s
argument.
Q: “What Turing disregards completely is the fact that mind, in its use, is not
static, but constantly developing, i.e., that we understand abstract terms more
and more precisely as we go on using them, and that more and more abstract
terms enter the sphere of our understanding. There may exist systematic meth-
ods of actualizing this development, which could form part of the procedure”
(G¨odel, [9]).
A: G¨odel raises a possibility that there exists a sophisticated decision procedure
for the Entscheidungsproblem that can be executed by gifted mathematicians.
Q: Hmm, if gifted mathematicians can reliably execute a procedure, they should
be able to ﬁgure out how to program it, and then the procedure is mechanical.
A: Well, it is hard to delimit human creativity. Certainly Turing did not do that.
Q: And didn’t intend to, I am sure. But let me change the topic. You said
nothing about Church’s arguments in favor of his deﬁnition of computability.
2 Q is my inquisitive friend Quisani, and A is the author.

Foundational Analyses of Computation
267
A: Church had strong arguments that his deﬁnition of reasonable and robust.
In particular, he and his student Kleene proved that a numerical function is
expressible in Church’s λ-calculus if and only if it is expressible in G¨odel’s re-
cursive calculus. Church’s thesis was that [G¨odel’s] recursive functions include
all numerical functions that are “eﬀectively calculable”.
Q: Here is another quote. “For the actual development of the (abstract) theory
of computation, where one must build up a stock of particular functions and
establish various closure conditions, both Church’s and Turing’s deﬁnition are
equally awkward and unwieldy. In this respect, general recursiveness is superior”
(Sol Feferman, [8, p. 6]). Do you buy that?
A: Indeed, the recursive approach has been dominant in mathematical logic,
but Turing’s approach dominates in computer science and it inﬂuenced the early
design of digital computers.
3
Kolmogorov
Andrei Kolmogorov analyzed computation in abstraction from the computer.
Kolmogorov’s fulcrum seems to be the idea that computations, independently
from the computer, satisfy nontrivial constraints. In a 1953 talk to the Moscow
Mathematical Society [14], he stipulated that every algorithmic process satisﬁes
the following constraints.
Sequentiality: An algorithmic process splits into steps whose complexity is
bounded in advance.
Elementary Steps: Each step consists of a direct and unmediated transfor-
mation of the current state S to the next state S∗.
Locality: Each state S has an active part of size bounded in advance. The direct
and unmediated transformation of S to S∗is based only on the information
about the active part of S and applies only to the active part.
These ideas gave rise to a new computation model developed by Kolmogorov
and his student Vladimir Uspensky [15]. Instead of a linear tape, a Kolmogorov
machine has a graph of bounded degree (so that there is a bound on the number
of edges attached to any vertex), with a ﬁxed number of the types of vertices
and a ﬁxed number of the types of edges. We speculated in [10] that “the thesis
of Kolmogorov and Uspensky is that every computation, performing only one
restricted local action at a time, can be viewed as (not only being simulated by,
but actually being) the computation of an appropriate KU machine.” Uspensky
agreed [22, p. 396].
We do not know what analysis, if any, allowed Kolmogorov and Uspensky to
arrive from the constraints above at the particular architecture of Kolmogorov
machines. “As Kolmogorov believed,” wrote Uspensky [22, p. 395], “each state of
every algorithmic process . . . is an entity of the following structure. This entity
consists of elements and connections; the total number of them is ﬁnite. Each
connection has a ﬁxed number of elements connected. Each element belongs to

268
Y. Gurevich
some type; each connection also belongs to some type. For every given algorithm
the total number of element types and the total number of connection types are
bounded.” In that approach, the number of nonisomorphic active zones is ﬁnite
(because of a bound on the size of the active zones), so that the state transition
can be described by a ﬁnite program.
Leonid Levin told us that Kolmogorov thought of computation as a physical
process developing in space and time, that the edges of Kolmogorov machine re-
ﬂect physical closeness of computation elements [16]. But then, as we mentioned
in [12], the dimensionality of the space may grow with the input size.
Kolmogorov’s analysis has not been well known. In this connection, let us
point out these references: [1,10,22,23].
4
Gandy
Gandy analyzed computation in his 1980 paper “Church’s Thesis and Principles
for Mechanisms” [7]. In this section, by default, quotations are from that paper.
Turing’s analysis of computation by a human being does not apply di-
rectly to mechanical devices . . . Our chief purpose is to analyze mechan-
ical processes and so to provide arguments for . . .
Thesis M. What can be calculated by a machine is computable.
Contrary to human computers, a machine can perform parallel actions. Kol-
mogorov machines are ﬁne “but at each step only a bounded portion of the
whole state [of a Kolmogorov machine] is changed.” Thesis M “must take par-
allel working into account.” A question arises what machines are.
(1) In the ﬁrst place I exclude from consideration devices which are
essentially analogue machines. . . . I shall distinguish between “mechan-
ical devices” and “physical devices” and consider only the former. The
only physical presuppositions made about mechanical devices . . . are that
there is a lower bound on the linear dimensions of every atomic part of
the device and that there is an upper bound (the velocity of light) on
the speed of propagation of changes.
(2) Secondly we suppose that the progress of calculation by a mechanical
device may be described in discrete terms, so that the devices considered
are, in a loose sense, digital computers.
(3) Lastly we suppose that the device is deterministic; that is, the sub-
sequent behaviour of the device is uniquely determined once a complete
description of its initial state is given.
After these clariﬁcations we can summarize our argument for a more
deﬁnite version of Thesis M in the following way.
Thesis P. A discrete deterministic mechanical device satisﬁes principles
I–IV below.
Later, discussing how to describe computation states, Gandy says that he wants
“the form of description to be suﬃciently abstract to apply uniformly to me-
chanical, electrical or merely notional devices.” After all the clariﬁcations, it is

Foundational Analyses of Computation
269
not clear what Gandy’s notion of machine is; see [18] in this connection. Gandy
does presume that machine computations are sequential-time and isolated; these
are two of the four constraints in §2. Sequential time parallelism is known as syn-
chronous.
Principles I-IV are precise though require too many deﬁnitions to be stated
precisely here. The four principles entail Gandy’s main theorem: “What can be
calculated by a machine is computable.”
Principle I asserts in particular that, for any machine, the states can be de-
scribed by hereditarily ﬁnite sets3and there is a transition function F such that,
if x describes an initial state, then Fx, F(Fx), . . . describe the subsequent states.
Principles II are III are technical restrictions on the state descriptions and the
transition function respectively. Principle IV generalizes Kolmogorov’s locality
constraint to parallel computations.
We now come to the most important of our principles. In Turing’s
analysis the requirement that the action depend only on a bounded por-
tion of the record was based on a human limitation. We replace this by a
physical limitation [Principle IV] which we call the principle of local cau-
sation. Its justiﬁcation lies in the ﬁnite velocity of propagation of eﬀects
and signals: contemporary physics rejects the possibility of instantaneous
action at a distance.
A preliminary version of Principle IV gives a good idea about the intentions
behind the principle.
Principle IV (Preliminary version). The next state, Fx, of a machine
can be reassembled from its restrictions to overlapping “regions” s and
these restrictions are locally caused. That is, for each region s of Fx
there is a causal neighborhood t ⊆TC(x) of bounded size such that
Fx ↾s [the restriction of Fx to s] depends only on x ↾t [the restriction
of x to t].
4.1
Comments
It isn’t clear to us what Gandy’s fulcrum was and even whether he had a fruitful
viewpoint on machine computations. We recently [13] criticized Gandy’s ap-
proach. Here we add just a few remarks.
The only parallelism that Gandy considers is synchronous. That is restrictive.
Nowadays asynchronous machine computations are common.
The principle of local causality does not apply to all synchronous parallel algo-
rithms. Gandy himself mentions one counterexample, namely Markov’s normal
algorithms [17]. The principle fails in the circuit model of parallel computation,
the oldest model of parallel computation in computer theory. The reason is that
the model allows gates to have unbounded fan-in. We illustrate this on the exam-
ple of a ﬁrst-order formula ∀xR(x) where R(x) is atomic. The formula gives rise
3 A set x is hereditarily ﬁnite if its transitive closure TC(x) is ﬁnite. Here TC(x) is
the least set t such that x ∈t and such that z ∈y ∈t implies z ∈t.

270
Y. Gurevich
to a collection of circuits Cn of depth 1. Circuit Cn has n input gates, and any
unary relation R on {1, . . . , n} provides an input for Cn. Circuit Cn computes
the truth value of the formula ∀xR(x) in one step, and the value depends on the
whole input. Ironically, it is easy to construct a Turing machine that simulates
sequentially these parallel computations.
Hereditarily ﬁnite sets are ﬁnite. The ﬁniteness constraint is understand-
able taking into account that Gandy’s goal was to conﬁrm Church’s thesis.
However, taking into account that Gandy’s machines are isolated (and thus
non-interactive), the ﬁniteness constraint excludes some useful algorithms. For
example it excludes a simple algorithm that consumes a stream of numbers keep-
ing track of the maximum of the numbers seen so far. The ﬁniteness constraint
is not necessarily satisﬁed by Turing machines. In particular, a Turing machine
can execute the stream algorithm above if the whole stream is written on its
initial tape.
We accept that computation states can be described in set theoretic terms. A
problem arises how to make the transition function work with such a description.
Many of Gandy’s technical problems are related to this problem, and indeed
describing algorithms in Gandy’s terms is rather challenging.
This said, let us emphasize that Gandy pioneered the axiomatic approach
in foundational analysis of algorithms. He bravely attacked the hard problem
of a general analysis of machine computations. Wilfried Sieg adopted Gandy’s
approach and reworked Gandy’s axioms, see [23] and references there, but he
did not clarify or justify Gandy’s fulcrum. The problem of a general analysis of
machine computations is wide open. In our view, the notion of machine com-
putation is evolving and will be evolving for the foreseeable future; think of
quantum computers for example. The notion has not matured enough to lend
itself to formal analysis.
5
Analyzing Computations on Their Native Levels
of Abstraction
5.1
Motivation
By the 1980s, there were plenty of computers and software. A problem arose
how to specify software. The most popular approaches to this problem were de-
notational semantics and algebraic speciﬁcations. Both approaches were proudly
declarative. The declarative character of speciﬁcations was supposed to be an
advantage. Indeed, declarative speciﬁcations tend to be more comprehensible,
higher-level (that is of higher level of abstraction) and cleaner than operational,
executable speciﬁcations, which is great. But executable speciﬁcations have their
own advantages. You can“play” with them: run them, test, debug. In principle,
you can verify properties of a declarative or executable spec mathematically, and
sometimes you have to, and there are better and better tools to do that. In prac-
tice though, mathematical veriﬁcation is out of the question in an overwhelming
majority of cases, and the possibility to test specs is indispensable. Declarative

Foundational Analyses of Computation
271
speciﬁcations are static while software evolves. In most cases, it is virtually im-
possible to keep a declarative spec and an implementation in sync. In the case
of an executable spec, you can test whether the implementation conforms to the
spec (or, if the spec was reverse-engineered from an implementation, whether
the spec is consistent with the implementation).
A question arises whether an executable speciﬁcation have to be low-level and
detailed? This leads to a theoretical, even foundational problem. Is there an ex-
ecutable speciﬁcation of any algorithm A on the level of abstraction of A itself?
For example, imagine that you conceived a wonderful algorithm. How would
you specify it succinctly in an executable way? A natural-language explanation
would not do as it is not executable. Besides, such an explanation may intro-
duce ambiguities and misunderstanding. You can program your algorithm in a
conventional programming language but this will surely introduce lower-level
details.
Turing and Kolmogorov machines are executable but low-level. Consider for
example Turing-machine implementations of these two versions of Euclid’s al-
gorithm for the greatest common divisor of two natural numbers: the original
version where you advance by means of diﬀerences, and a faster (and higher-
level) version where you advance by means of divisions. The chances are that
divisions were reduced to diﬀerences in the Turing machine implementation, and
the distinction of the abstraction levels disappeared.
Can we generalize Turing and Kolmogorov machines in order to solve the
foundational problem in question? The answer turns out to be positive, at least
for sequential algorithms [12], synchronous parallel algorithms [2], and interactive
algorithms [3,4].
Following Kolmogorov, we consider computation in abstraction from the com-
puter. Following Gandy, we use an axiomatic approach. The fulcrum for the
sequential case is this. Every algorithm A has its native level of abstraction.
On that level, the states can be faithfully represented by ﬁrst-order structures
of a ﬁxed vocabulary in such a way that state transitions become just sets of
assignments. The fulcrums for the parallel and interactive cases are built on this
fulcrum. Here we restrict attention to sequential algorithms and do not cover
parallel and interactive ones. Sequential algorithms are also known as classical
as they had been virtually the only algorithms from time immemorial to the
1950s. The three stipulations of Kolmogorov in §3 give a great informal descrip-
tion of sequential algorithms. In the rest of this section, algorithms are by default
sequential.
5.2
Constraints
Sequential Time: Any algorithm A is associated with a nonempty collection
S(A) of states, a subcollection I(A) ⊆S(A) of initial states and a (possibly
partial) state transition map τA : S(A) −→S(A).
Q: Your algorithm A is deterministic: τA(X) is determined by state X.
Why not to make τA multi-valued?

272
Y. Gurevich
A: In our view, this would involve intra-step (within a single step) inter-
action with the environment [12, §9]. Intra-step interactive algorithms are
analyzed in [3,4]. Note in this connection that we do not rule out inter-
step interaction with the environment. In other words, the environment
can intervene between the steps of the algorithm A. If the intervention
results in a legitimate state of A, the algorithm A continues to run. So,
in general, the steps of A are interleaved with those of the environment,
and thus the behavior of A is not necessarily determined by the initial
step.
Recall that a ﬁrst-order structure X is a nonempty set (the base set of X) with
relations and operations; the vocabulary of X consists of the names of those
relations and operations. For example, if the vocabulary of X consists of one
binary relation then X is a directed graph.
Abstract State: The states of an algorithm A can be faithfully represented by
ﬁrst-order structures of the same ﬁnite vocabulary, which we call the vocab-
ulary of A, in such a way that
– τA does not change the base set of a state,
– collections S(A) and I(A) are closed under isomorphisms, and
– any isomorphism from a state X to a state Y is also an isomorphism
from τA(X) to τA(Y ).
Q: You claim that ﬁrst-order structures are suﬃciently general to faith-
fully represent the states of any algorithm?
A: I have been making that claim from the 1980s. The collective expe-
rience of computer science seems to corroborate the claim.
Q: But maybe the notion of ﬁrst-order structure is too broad. Consider
for example, natural numbers with the usual arithmetic relations and
operations plus the unary relation T (n) that is true if and only if the
Turing machine number n halts on the empty tape. Starting with such
a structure, a simple algorithm solves the halting problem.
A: Suppose, more generally, that T is produced by some process, not
necessarily algorithmic. For example, T is the result of some measure-
ment or coin ﬂipping. How would you rule out your particular version of
T ?
Q: OK, but I have another question about the postulate. That base-set
preservation sounds restrictive. A graph algorithm may extend the graph
with new nodes.
A: And where will the algorithm take those nodes? From some reserve?
Make that reserve a part of your initial state.
Q: Now, why should the collection of states be closed under isomor-
phisms, and why should the state transition respect isomorphisms?
A: Every algorithm works at its native level of abstraction. Irrelevant
details should not matter. Consider a graph algorithm for example. In

Foundational Analyses of Computation
273
an implementation, nodes may be integer numbers, but the algorithm
cannot examine whether a node is even or odd or which of the nodes
is greater. These are implementation details irrelevant to the graph al-
gorithm. And if the algorithm does take advantage of the integer repre-
sentation of nodes then its vocabulary should reﬂect the relevant part of
arithmetic.
According to Kolmogorov’s informal deﬁnition of sequential algorithms, there is
a bound on the amount of work done during any one step. But how to measure
step complexity or the work done during one step? Fortunately the abstract state
constraint helps.
Note that, according to the sequential-time constraint, the next state τA(X)
of an algorithm A depends only on the current state X of A. The executor does
not need to remember any history (even the current position in the program);
all that is reﬂected in the state. If the executor is human and writes something
on scratch paper, that paper should be a part of the computation state.
In order to change the given state X into τA(X), the algorithm A explores
a portion of X and then performs the necessary changes of the values of the
predicates and operations of X. According to Kolmogorov’s informal deﬁnition,
the explored portion, the “active zone”, is bounded. And the change from X to
τA(X), let us call it ΔA(X), depends only on the results of exploration. Formally,
ΔA(X) can be deﬁned as the collection of equations F(¯a) = b where b is a new
value of a vocabulary function F at point ¯a.
But how does the algorithm know what to explore and what to change? That
information is normally supplied by the program, and it should be applicable
to all the states. In the light of the abstract state constraint, it should be given
symbolically, in terms of the vocabulary of A.
Bounded Exploration: There exists a ﬁnite set T of terms (or expressions)
in the vocabulary of algorithm A such that ΔA(X) = ΔA(Y ) whenever states
X, Y of A coincide over T .
5.3
Deﬁnition and the Representation Theorem
Now think of the sequential-time constraint as a postulate where S(A) is just
a nonempty collection of things. Think of the abstract-state and bounded-
exploration constraints as postulates that clarify/restrict what those things are
and how the map τA works.
Deﬁnition 1. A
(sequential)
algorithm
is
any
entity
that
satisﬁes the
sequential-time, abstract-state and bounded-exploration postulate.
Abstract state machines (ASMs) were deﬁned in [11]. Here we restrict attention
to sequential ASMs, which are undoubtedly algorithms.
Theorem 1 ([12]). For every algorithm A, there exists a sequential ASM with
the same states and the same state transition function.

274
Y. Gurevich
5.4
Deriving Church’s Thesis
Our postulates do not entail Church’s thesis. The reason is that initial states
of sequential algorithms may be uncomputable. The halting problem for Turing
machines may be encoded in an initial state. Think also of ruler-and-compass
algorithms or the Gauss elimination procedure; they satisfy our postulates but
cannot be simulated by Turing machines. An arithmetical-state postulate of [5]
asserts that only undeniably-computable operations are available in initial states;
see details in [5].
Theorem 2 ([5]). Church’s thesis follows from the sequential-time, abstract-
state, bounded-exploration and arithmetical-state postulates.
Acknowledgements. Many thanks to Andreas Blass and Oron Shagrir for
useful comments.
References
1. Blass, A., Gurevich, Y.: Algorithms: A quest for absolute deﬁnitions. In: Paun, G.,
et al. (eds.) Current Trends in Theoretical Computer Science, pp. 283–311. World
Scientiﬁc (2004); also in: Olszewski, A. (ed.): Church’s Thesis After 70 Years Ontos
Verlag, pp. 24–57. Ontos Verlag (2006)
2. Blass, A., Gurevich, Y.: Abstract state machines capture parallel algorithms. ACM
Trans. on Computational Logic 4(4), 578–651 (2003); Correction and Extension,
Same Journal 9(3), article 19 (2008)
3. Blass, A., Gurevich, Y.: Ordinary interactive small-step algorithms. ACM Trans.
Computational Logic 7(2), Part
I, 363–419 (2006); plus 8:3 , articles 15 and 16
(Parts II and III) (2007)
4. Blass, A., Gurevich, Y., Rosenzweig, D., Rossman, B.: Interactive small-step algo-
rithms. Logical Methods in Computer Science 3(4) (2007); papers 3 and 4 (Part I
and Part II)
5. Dershowitz, N., Gurevich, Y.: A natural axiomatization of computability and proof
of Church’s thesis. Bull. of Symbolic Logic 14(3), 299–350 (2008)
6. Church, A.: An unsolvable problem of elementary number theory. American Jour-
nal of Mathematics 58, 345–363 (1936)
7. Gandy, R.: Church’s thesis and principles for mechanisms. In: Barwise, J., et al.
(eds.) The Kleene Symposium, pp. 123–148. North-Holland (1980)
8. Gandy, R.O., Yates, C.E.M. (eds.): Collected works of A.M. Turing: Mathematical
logic. Elsevier (2001)
9. G¨oedel, K.: A philosophical error in Turing’s work. In: Feferman, S., et al. (eds.)
Kurt G¨odel: Collected Works, vol. II, p. 306. Oxford University Press (1990)
10. Gurevich, Y.: On Kolmogorov machines and related issues. Bull. of Euro. Assoc.
for Theor. Computer Science 35, 71–82 (1988)
11. Gurevich, Y.: Evolving algebra 1993: Lipari guide. In: B¨orger, E. (ed.) Speciﬁcation
and Validation Methods, pp. 9–36. Oxford Univ. Press (1995)
12. Gurevich, Y.: Sequential abstract state machines capture sequential algorithms.
ACM Trans. on Computational Logic 1(1), 77–111 (2000)
13. Gurevich, Y.: What Is an Algorithm? In: Bielikova, M., et al. (eds.) SOFSEM 2012.
LNCS, vol. 7147, pp. 31–42. Springer, Heidelberg (2012)

Foundational Analyses of Computation
275
14. Kolmogorov, A.N.: On the concept of algorithm. Uspekhi Mat. Nauk 8(4), 175–176
(1953) (Russian)
15. Kolmogorov, A.N., Uspensky, V.A.: On the deﬁnition of algorithm. Uspekhi Mat.
Nauk 13(4), 3–28 (1958) (Russian); English translation in AMS Translations 29,
217–245 (1963)
16. Levin, L.A.: Private communication (2003)
17. Markov, A.A.: Theory of algorithms. Trans. of the Steklov Institute of Mathemat-
ics 42 (1954) (Russian); English translation by the Israel Program for Scientiﬁc
Translations, 1962; also by Kluwer (2010)
18. Shagrir, O.: Eﬀective computation by humans and machines. Minds and Ma-
chines 12, 221–240 (2002)
19. Shagrir, O.: G¨oedel on Turing on computability. In: Olszewski, A., et al. (eds.)
Church’s Thesis After 70 Years, pp. 393–419. Ontos-Verlag (2006)
20. Sieg, W.: On computability. In: Irvine, A. (ed.) Handbook of the Philosophy of
Mathematics, pp. 535–630. Elsevier (2009)
21. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of London Mathematical Society 2(42), 230–265 (1936)
22. Uspensky, V.A.: Kolmogorov and mathematical logic. Journal of Symbolic
Logic 57(2), 385–412 (1992)
23. Uspensky, V.A., Semenov, A.L.: Theory of algorithms: main discoveries and appli-
cations, Nauka (1987) (Russian), Kluwer (2010) (English)

Turing Machine-Inspired Computer Science
Results
Juris Hartmanis
Department of Computer Science,
Cornell University, Ithaca, NY 14850, United States of America
jh@cs.cornell.edu
Abstract. This paper discusses how the Turing machine model directly
inspired and guided developments in theoretical computer science. In
particular, the Turing machine model was ideal for the creation of com-
putational complexity theory, which has grown into an essential part of
theoretical computer science and has found application in other disci-
plines. The machine operation count was used to deﬁne time-bounded
computations and the tape squares used deﬁned the tape or memory-
bounded computations. The deﬁnition and exploration of the correspond-
ing asymptotic complexity classes followed naturally.
I received my PhD from California Institute of Technology in 1955 with a disser-
tation in lattice theory. After two delightful years at Cornell I spent a summer in
the newly formed Information Studies Section of the General Electric Research
Lab in Schenectady, New York. This Section’s task was to build up a research
eﬀort to lay the scientiﬁc foundations for the emerging information and com-
puting technologies. For me this was an exciting introduction to a potentially
great new research area. By the end of the summer I had ﬁnished my ﬁrst paper
on linear coding networks and was fully committed to return to the Lab and
dedicate myself to computer science research. I did so a year later in 1958, after
fulﬁlling my earlier commitment to Ohio State University.
For computer science these were interesting and exciting times as academic
programs in computer science were created and research areas were deﬁned and
established. We got to know personally many of the scientists who explored com-
puter science. My early research at the GE Research Lab was concentrated on
ﬁnite automata and their decomposition into smaller automata from which they
could be realized. I was joined in this eﬀort by Dick Stearns who came to the
Information Studies Section after a summer job at the Lab and completion of
his PhD dissertation in game theory at Princeton University. We collaborated
intensively and explored whatever material we could ﬁnd on computer science. I
personally was very impressed by Shannon’s work on information theory. I was
surprised and very impressed that Shannon had captured precisely such vague
concepts as amount of information of a source and capacity of a noisy informa-
tion channel and could derive quantitative results about how much information
can be transmitted over such channels [14]. I wondered if an equivalent quan-
titative theory could be developed to measure the diﬃculty of computational
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 276–282, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Turing Machine-Inspired Computer Science Results
277
problems. My eﬀorts in this direction based on entropy did not succeed. I lacked
the right concepts or models for a quantitative theory of computing. My math-
ematical education had not exposed me to concepts of computability neither
via recursive function theory nor Turing’s work. I also recall that Dick Stearns
was not familiar with Turing’s work. Our exposure to Turing’s ideas was a dra-
matic event for us. We studied Turing’s paper [17] with excitement and were
delighted in the simplicity of the Turing machine model and the beauty of the
capture of computability via this model. The Turing machine was indeed a very
powerful intellectual tool. We read more and played intellectually with the new
concepts and models. We also read and were inﬂuenced by a tech report by Hisao
Yamada, later published as [18], which studied a special time-bounded class of
computations. All this led Dick and me to explore the Turing machine model as
a foundation for a quantitative theory of computing.
We quickly convinced ourselves that the Turing machine model was robust
under “reasonable” changes of the model. Turing had already observed that
adding tapes to the model did not change what it could compute. Our task was
to show that reasonable changes did not dramatically change the computation
time and to quantify the changes. We explored multi-tape machines, machines
with two and higher dimensional tapes as well as other modiﬁcations. In most
of these cases we could show that these modiﬁcations could not speed up the
computation by more than the square root of the one-tape machine time. We
deﬁned time-bounded computational complexity classes as consisting of all prob-
lems solvable in a number of steps bounded by some function of the input length
n. In other words, the computational complexity classes were deﬁned by how
fast the diﬃculty of the computation grew with the length of the input. We de-
rived hierarchy results by time-bounded diagonalization that showed that there
are computations with very sharp time bounds; a slight increase in the com-
putation time would yield a larger complexity class. Though these ﬁrst results
about computations with sharp time bounds were obtained by diagonalization
and were not about speciﬁc practical problems, they guaranteed the existence
of such problems and contributed to the interest in computational complexity
[7]. As we have learned later, proving lower computational bounds for speciﬁc
problems may be a very diﬃcult indeed. Think of the P
?= NP problem [6].
It is interesting to note that the machine model naturally suggested how to
bound the diagonalization time by adding a “clock” running in parallel (on
separate tapes) with the diagonalzation process and shutting it oﬀin the desired
time. Note that these clocks which bounded the complexity class runtime to T (n)
steps had to compute T (n) in T (n) steps to shut oﬀthe diagonalization in T (n)
steps. Originally we thought that these fast computable “clocks” were not an
essential part of the result and conjectured that they could be eliminated. A short
time later, my ﬁrst PhD student Alan Borodin proved the gap theorem in his
PhD dissertation [4]. The gap theorem showed that there exist arbitrarily large
gaps in the hierarchy of complexity classes. In essence, for any monotonically
growing recursive function, g(n) > n, there exists a recursive function T (n) > n
such that the complexity classes bounded by T (n) and g(T (n)) are identical.

278
J. Hartmanis
Clearly, T (n) had to be a function that required much more than T (n) steps to
compute it. Thus some sort of an “easy to compute clock” was essential for the
diagonalization to obtain computations with sharp time bounds.
Borodin’s gap theorem was proved in Blum’s axiomatic complexity theory, in-
dependent of any concrete machine models, and thus holding for all complexity
measures satisfying the Blum axioms, including time- and tape-bounded com-
putations. The gap theorem was discovered independently by B. Trakhtenbrot
in 1964 behind the “Iron Curtain” and by Borodin in 1969 and is known as the
Borodin-Trakhtenbrot theorem. We shall return to Blum axioms a bit later.
In the study of time bounded computations Dick and I also explored the com-
putation of real numbers. We showed that all algebraic numbers were computable
in n2 time. We also found transcendental numbers computable in real-time (the
successive digits of the number can each be printed by a Turing machine in a
ﬁxed amount of time). Our failure to show that there are real-time computable
irrational algebraic numbers led us to the following conjecture: The real-time
computable numbers are either rational or transcendental. That is, there are no
real-time computable irrational algebraic numbers. Should this be so, we would
have a proof for a large set of easily computable numbers that they are transcen-
dental. This would establish a very interesting contrast between the computa-
tional complexity of irrational algebraic numbers and transcendental numbers.
So far, we know of no real progress on this conjecture.
Dick Stearns and I met Manuel Blum at MIT while he was working on his
dissertation and we exchanged ideas about computational complexity and com-
pared our approaches. Dick and I were very impressed by the elegance Manuel’s
abstract axiomatic approach and the sweeping generality of his and the subse-
quent results formulated in terms of the Blum axioms [2]. We were particularly
impressed by Blum’s speedup theorem, which showed that for any complexity
measure satisfying the Blum axioms, there exist computations for which any
given algorithm can be “speeded up” by any prescribed arbitrarily large amount
[3]. For example, there exist computations such that for any given algorithm for
them there exists another algorithm which runs in loglog-time of the given algo-
rithm. These highly abstract axiomatic results nicely complemented our concrete
complexity results.
It is interesting to observe that Manuel, steeped in recursive function theory
at MIT, chose the abstract, axiomatic approach to complexity theory and that
Dick and I, at the GE Lab recently exposed to Turing’s work, chose the Tur-
ing machine model more directly to initiate the study of concrete computation
complexity theory.
After the early exploration of time bounded computations, Dick, I and Phil
Lewis turned to the investigation of tape- or memory-bounded computations [15].
This work progressed very rapidly and we derived sharp hierarchy theorems and
related results. A very fruitful modiﬁcation of the tape-bounded Turing machine
was the separation of a read-only input tape from the read-write work tape.
This was indeed a model-suggested modiﬁcation and it allowed us to investi-
gate the rich set of interesting complexity classes requiring little tape for their

Turing Machine-Inspired Computer Science Results
279
solution. We showed that there existed non-regular languages recognizable on
log log n tape and that no non-regular language could be recognized with less
tape. Thus there was a complexity gap between a constant amount of tape and
log log n tape. We had a similar gap for time-bounded computation of one-tape
machines between linear time (n) and n log n time. We also showed that there
existed non-regular context-free languages that could be recognized on log log n
tape (in essence, without the ability to count up to the length of the input). One
of our nicest results was that all context-free languages can be recognized on
(log n)2 tape. This was one of the ﬁrst computational complexity results about
a practical computer science problem. At the same time, our colleague Dan
Younger at the GE Lab showed that context-free languages can be recognized
in n3 time. This result is now known as the CYK algorithm, for Cocke-Younger-
Kasami, testifying to the rapid growth of work in complexity theory [5,11,19].
These early explorations were followed by a stream of interesting results by us
and other scientists, including Savitch’s result [13] that nondeterministic tape-
bounded computations can be performed on deterministic machines not using
more than the square of the amount of tape used by the nondeterministic com-
putation, and the Immerman–Szelepcs´enyi result that nondeterministic space
computations are closed under complementation [10,16].
Some of the unsolved concrete complexity class separation problems involve
tape bounded computations:
LOGTAPE
?= P, P
?= NP, NP
?= PTAPE, PTAPE
?= EXPTIME.
We know by diagonalization arguments that there is an inequality in the sequence
LOGTAPE
?= P, P
?= NP, NP
?= PTAPE,
we just do not know where the break occurs! The same holds for the sequence
P
?= NP, NP
?= PTAPE, PTAPE
?= EXPTIME.
Another interesting use of machine model interaction yielded a set of undecid-
ability results about context-free languages [8]. Context-free languages, CFLs,
are the languages accepted by nondeterministic push-down automata, PDAs,
or generated by context-free grammars, CFGs. For any Turing machine M, let
VAL-M designate the set of sequences of instantaneous descriptions of valid com-
putations by M in which every second instantaneous description is reversed. It
is easily seen that a nondeterministic pushdown automaton can recognize the set
of all invalid computations of M, INVAL-M , by nondeterministically selecting
a presumably valid instantaneous description of M’s computation and pushing
it onto the stack and pulling it up and comparing it to the following alleged
instantaneous description of M’s computation. If an error or mismatch between
the two allegedly consecutive instantaneous descriptions is found, the input is
accepted. This links context-free languages to Turing machine computations and
we see that the context-free language INVAL-M accepts all inputs if and only
if M accepts the empty set. Thus it is recursively undecidable if a PDA accepts

280
J. Hartmanis
all strings over the input alphabet or, equivalently, if a context-free grammar
generates all possible strings of its terminals. With a bit of cleverness it follows
that for context-free languages or context-free grammars the following problems
are recursively undecidable:
Is the complement of L empty?
Is L = L′?
Is L contained in L′?
Is complement of L ﬁnite?
Is complement of L regular?
Is complement of L a CFL?
Is the set of preﬁxes of the complement of L recursive?
Is the CFL L ambiguous?
Finally, these methods easily prove that no nontrivial property on the recursively
enumerable sets can be recursively decided for the quotients of two CFLs. Some
of these results were already known and proved by other methods [1].
Since the early explorations of computational complexity using Turing ma-
chine models, other models more directly modeling random access computers
have been proposed and explored yielding some interesting results.
A random access machine model, RAM , consists of an set of operations and an
set of registers R0, R1, R2, . . . capable of storing a nonnegative integer in binary
representation. The set of operations consist of assignment between registers,
indirect addressing, addition and proper subtraction of two register contents,
Boolean bitwise operations between two register contents, and conditional jump.
The computation is started with the input in R0 and all other registers set to
zero. The instructions of the program are executed in sequence until a conditional
jump is executed and the computation continues. The computation time is the
count of operations executed [9,12].
For this RAM model we get reassuring results:
RAM -POLYTIME = P, NRAM -POLYTIME = NP.
The surprise comes when we add multiplication of register contents as a one-step
operation to the RAM operation set; we call it an MRAM . The MRAM can
build, in n operations, exponentially long register contents on which it can per-
form parallel logical bit operations in one step. This power leads to the following
results:
MRAM -POLYTIME = PSPACE = NMRAM -POLYTIME.
Since we believe that P is not equal to NP nor NP to PSPACE, we must
conclude the multiplication made the RAM into an unrealistic model for com-
putational complexity theory. At the same time, it is reassuring that the poly-
nomial time RAM computations (with registers of unbounded length) yield the
classical polynomial time complexity classes, P and NP, respectively. These re-
sults give additional evidence for the adequacy of the Turing machine model for
computational complexity theory. At the same time, the more powerful MRAM

Turing Machine-Inspired Computer Science Results
281
model wipes out the distinctions between complexity classes of direct interest
in computer science. In general, the separation of complexity classes deﬁned by
diﬀerent concrete complexity measures are among the many unsolved problems
in complexity theory, including the famous P
?= NP problem.
Since these early days of computational complexity research, much of it in-
spired and guided by the Turing machine model, computational complexity has
grown into an essential and vital part of theoretical computer science with many
beautiful and important results as well as applications in computer science and
other disciplines.
References
1. Bar-Hillel, Y., Perlis, M., Shamir, E.: On formal properties of simple phrase struc-
ture grammars. Z. Phonetik, Sprachwissen. Komm. 14, 143–172 (1961)
2. Blum, M.: A machine-independent theory of the complexity of recursive functions.
J. Assoc. Comput. Mach. 14(2), 322–336 (1967)
3. Blum, M.: On eﬀective procedures for speeding up algorithms. J. Assoc. Comput.
Mach. 18(2), 290–305 (1971)
4. Borodin, A.B.: Computational complexity and the existence of complexity gaps. J.
Assoc. Comput. Mach. 19(1), 158–174 (1972)
5. Cocke, J., Schwartz, J.T.: Programming languages and their compilers: Preliminary
notes. Tech. rep., Courant Institute (1970)
6. Cook, S.A.: The complexity of theorem proving procedures. In: Proc. 3rd Symp.
Theory of Computing, pp. 151–158. ACM, New York (1971)
7. Hartmanis, J., Stearns, R.E.: On the computational complexity of algorithms.
Trans. Amer. Math. Soc. 117, 285–306 (1965)
8. Hartmanis, J.: Context-free languages and Turing machine computations. J. Sym-
bolic Logic 37(4), 759 (1972)
9. Hartmanis, J., Simon, J.: On the power of multiplciation in random-access ma-
chines. In: Proc. 15th Annu. IEEE Sympos. Switching Automata Theory, pp. 13–23
(1974)
10. Immerman, N.: Nondeterministic space is closed under complement. SIAM J. Com-
put. 17, 935–938 (1988)
11. Kasami, T.: An eﬃcient recognition and syntax algorithm for context-free lan-
guages. Tech. Rep. AFCRL-65-758, Air Force Cambridge Research Lab (1965)
12. Pratt, V.R., Rabin, M.O., Stockmeyer, L.J.: A characterization of the power of
vector machines. In: Proc. ACM Symp. Theory of Computation (STOC 1974), pp.
122–134 (1974)
13. Savitch, W.: Relationship between nondeterministic and deterministic tape com-
plexities. J. Comput. Syst. Sci. 4(2), 177–192 (1970)
14. Shannon, C.E.: A mathematical theory of communication. Bell System Technical
Journal 27, 379–423, 623–656 (1948)
15. Stearns, R., Hartmanis, J., Lewis, R.: Hierarchies of memory limited computations.
In: Proc. IEEE Conf. Switching Circuit Theory and Logical Design, pp. 179–190
(1965)

282
J. Hartmanis
16. Szelepcs´enyi, R.: The method of forcing for nondeterministic automata. Bull.
EATCS 33, 96–100 (1987)
17. Turing, A.M.: On computable numbers with an application to the Entschei-
dungsproblem. Proc. London Math. Soc. 42, 230–265 (1936); erratum: Ibid. 43,
544–546 (1937)
18. Yamada, H.: Real-time computation and recursive functions not real-time com-
putable. IEEE Trans. Electronic Computers EC-11(6), 753–760 (1962)
19. Younger, D.H.: Recognition and parsing of context-free languages in time n3.
Information and Control 10(2), 189–208 (1967)

NP-Hardness and Fixed-Parameter Tractability
of Realizing Degree Sequences
with Directed Acyclic Graphs
Sepp Hartung and Andr´e Nichterlein
Institut f¨ur Softwaretechnik und Theoretische Informatik, Technische Universit¨at
Berlin, TEL 12-4, Ernst-Reuter-Platz 7, 10587 Berlin, Germany
{sepp.hartung,andre.nichterlein}@tu-berlin.de
Abstract. In graph realization problems one is given a degree sequence
and the task is to decide whether there is a graph whose vertex de-
grees match the given sequence. This realization problem is known to
be polynomial-time solvable when the graph is directed or undirected.
In contrast, we show NP-completeness for the problem of realizing a
given sequence of pairs of positive integers (representing indegrees and
outdegrees) with a directed acyclic graph, answering an open question
of Berger and M¨uller-Hannemann [FCT 2011]. Furthermore, we classify
the problem as ﬁxed-parameter tractable with respect to the parameter
“maximum degree”.
Keywords: graph realization problems, combinatorial algorithms, pa-
rameterized complexity, realizing topological orderings.
1
Introduction
Berger and M¨uller-Hannemann introduced the following problem [1]:
DAG Realization
Input:
A multiset S =
a1
b1

, . . . ,
an
bn

of integer pairs with ai, bi ≥0.
Question: Is there a directed acyclic graph (without parallel arcs and self-
loops) that admits a labeling of its vertex set {v1, . . . , vn} such
that for all vi ∈V the indegree is ai and the outdegree is bi?
If the degree sequence S is a yes-instance, then S is called realizable and the
corresponding directed acyclic graph (DAG for short) D is called a realizing DAG
for S. Berger and M¨uller-Hannemann showed that this problem is polynomial-
time solvable for special types of degree sequences, but left the complexity of
the general problem as their main open question [1]. We answer this question by
showing that DAG Realization is NP-complete. Moreover, on the positive side
we classify DAG Realization as ﬁxed-parameter tractable with respect to the
parameter maximum degree Δ := max{a1, b1, . . . , an, bn}. The corresponding
algorithm actually constructs for yes-instances a realizing DAG.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 283–292, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

284
S. Hartung and A. Nichterlein
Related Work. It has been known for a long time that deciding whether
a given degree sequence (a multiset of positive integers) is realizable with an
undirected graph is polynomial-time solvable. There are characterizations for
realizable degree sequences due to [5] and algorithms by Havel and Hakimi [11,10].
In the case, where one asks whether there is a directed graph realizing the given
degree sequence (a multiset of positive integer pairs), has also been intensively
studied: Cf. [3,8,7,17] for characterizations of digraph realizations and [13] for
polynomial-time algorithms. The problem of realizing degree sequences has also
been studied in context of (loop-less) multigraphs, where the aim is to minimize
or maximize the number of multi-edges [12].
Preliminaries. We set N := {0, 1, 2, . . .}. A parameterized problem is ﬁxed-
parameter tractable if any instance (I, k), consisting of the “classical” problem
instance I and the parameter k ∈N, can be solved in f(k) · nc time. Thereby,
f is a computable function solely depending on k and c ∈N is a constant. For
a more detailed introduction to parameterized algorithmics and complexity we
refer to the monographs [4,6,15].
We denote directed graphs by D = (V, A) with vertex set V and arc set A ⊆
V × V . The indegree of v ∈V is denoted by d−(v) and the outdegree by d+(v).
Correspondingly, for a degree sequence S and an element s ∈S with s =
a
b

,
we set d−(s) := a and d+(s) := b. A directed graph D = (V, A) is a DAG if it
does not contain a cycle. A cycle is a vertex sequence v1, . . . , vl such that for all
1 ≤i < l : (vi, vi+1) ∈A and (vl, v1) ∈A. Each DAG D admits a topological
ordering , that is, an ordering of all its vertices v1, . . . , vn such that for all arcs
(vi, vj) ∈A it holds that i < j. Consequently, for a realizing DAG we call a
corresponding topological ordering a realizing topological ordering .
We use the opposed order ≤opp for the elements of a degree sequence S, as
introduced in [1]:
Deﬁnition 1.
a1
b1

≤opp
a2
b2

⇐⇒(a1 ≤a2 ∧b1 ≥b2)
Note that there might be elements in the degree sequence S that are not compa-
rable with respect to the opposed order. However, we can always assume that a
realization does not collide with the opposed order and thus DAG Realization
is polynomial-time solvable in case of all elements of S are comparable.
Lemma 1 ([1, Corollary 3]). Let S =
a1
b1

, . . . ,
an
bn

be a realizable degree
sequence. Then, there exists a realizing topological ordering φ such that for all
1 ≤i, j ≤n with si =
ai
bi

≤opp
aj
bj

= sj and si ̸
= sj, it holds that in φ the
position of the vertex that corresponds to si is smaller than the position of the
vertex that corresponds to sj.
Our paper is organized as follows: Section 2 contains the proof of the NP-
completeness and in Section 3 we show that DAG Realization is ﬁxed-
parameter tractable with respect to the parameter maximum degree Δ.
Due to the space constraints we omit most of the proofs in Section 2
and the detailed correctness proof of the algorithm that we describe in

NP-Hardness and Fixed-Parameter Tractability
285
Section 3. For the general public a full version of the paper is available
from http://arxiv.org/abs/1110.1510v2.
2
NP-Completeness
In this section we show the NP-hardness of DAG Realization by giving a
polynomial-time many-to-one reduction from the strongly NP-hard problem
3-Partition [9, SP15]:
3-Partition
Input:
A multiset A = {a1, . . . , a3m} of 3m positive integers and an
integer B with 3m
i=1 ai = mB and ∀i : B/4 < ai < B/2.
Question: Is there a partition of the 3m integers from A into m disjoint
triples such that in every triple the three elements add up to B?
This section is organized as follows: First we describe the construction of our
reduction and explain the idea of how it works. Then, we prove the correctness
in the remainder of the section.
Construction. Given an instance (A, B) of 3-Partition, we construct an
equivalent instance S of DAG Realization as follows:
S := X0, X1, . . . , Xm, α1, α2, . . . , α3m
where αi :=
ai
ai

, 1 ≤i ≤3m. The Xi, 0 ≤i ≤m, are subsequences which we
formally deﬁne after giving the idea of the construction. We call an element from
a subsequence Xi an x-element and the αj are called a-elements. In a realizing
DAG D the vertices realizing x-elements are called x-vertices and the vertices
realizing a-elements are called a-vertices.
The intuition of the construction is that a DAG D realizing S (if it exists)
looks as follows: The vertices realizing elements of a subsequence Xi, 0 ≤i ≤m,
form a “block” in a realizing topological ordering φ. These blocks are a skeletal
structure in any realizing topological ordering and there are m “gaps” between
them. The construction ensures that these gaps are ﬁlled with a-vertices and,
moreover, the indegree and outdegree of all the a-vertices in a gap sum up to B.
Hence, these m gaps require to partition the a-vertices into m sets where each
set has in total in- and outdegree B and, thus, correspond to a solution for the
3-Partition instance where we reduce from. In the reverse direction, for each
triple in a solution of a 3-Partition instance the corresponding a-vertices will
be used to ﬁll up one gap. See Figure 1 for an example of the construction.
To achieve the mentioned skeletal structure of the subsequences X0, . . . , Xm,
we require the corresponding x-vertices to form a transitive tournament. That is a
DAG with n vertices and
n
2

arcs that realizes the degree sequence {
 0
n−1

,

1
n−2

,
. . . ,
n−1
0

}. Observe that there is only one DAG realizing such a sequence and,
furthermore, such a transitive tournament admits only one topological ordering.
Now, we complete the reduction by deﬁning the subsequences X0, . . . , Xm.
As
indicated
in
Figure 1,
X0
and
Xm
contain
B
elements
and
the

286
S. Hartung and A. Nichterlein
X0
X1
X2
X3
X4
a1
a2
a3
a4
a5
a6
a7
a8
a9
a10
a11
a12
Fig. 1. A schematic representation of a DAG that realizes a degree sequence S that
is constructed from a 3-Partition instance with B = 12 and m = 4. There are ﬁve
blocks marked by the gray ellipses and four gaps between them. In each gap there are
three a-vertices, altogether having in- and outdegree B. The sets Xi, 1 ≤i ≤3, are
partitioned into two parts of size B. The vertices in the left part have B incoming arcs
from the a-vertices that ﬁll the gap between Xi−1 and Xi. Correspondingly, the vertices
in the right part have B outgoing arcs to the a-vertices that ﬁll the gap between Xi
and Xi+1. Consequently, the ﬁrst and the last block X0 and X4 are of size B. The in-
and outdegree of the a-vertices in each triple sum up to B.
other
subsequences
contain 2B
elements.
The
subsequence
X0
consists
of the elements x0
0, x1
0, . . . , xB−1
0
. This subsequence corresponds to the x-
vertices v0
0, . . . , vB−1
0
forming the ﬁrst block in a realizing DAG for S. Remember
that the x-vertices are supposed to form a transitive tournament. To achieve this,
vj
0 has (B−1−j) outgoing arcs to vj+1
0
, . . . , vB−1
0
and (m−1)2B+B = (2m−1)B
outgoing arcs to the x-vertices in the subsequent blocks. Furthermore, vj
0
has j incoming arcs from the x-vertices v0
0, . . . , vj−1
0
. Finally, each x-vertex
in v0
0, . . . , vB−1
0
has one outgoing arc to one of the three subsequent a-vertices.
Hence, the corresponding x-element of vj
0 is as follows:
xj
0 :=

j
(B −1 −j) + (2m −1)B + 1

=

j
2mB −j

.
Analogously, the subsequence Xm consists of B elements x0
m, x1
m, . . . , xB−1
m
with
xj
m :=
(2m −1)B + j + 1
B −1 −j

.
For 0 < i < m, the subsequence Xi consists of 2B elements x0
i , x1
i , . . . , x2B−1
i
.
Let v0
i , . . . , v2B−1
i
denote the corresponding x-vertices. Then, vj
i has (i −1)2B +
B = (2i −1)B incoming arcs from the x-vertices in the preceding blocks and
j incoming arcs from v0
i , . . . , vj−1
i
. Furthermore, vj
i has (m −i −1)2B + B =
(2m −2i −1)B outgoing arcs to the subsequent blocks and 2B −1 −j outgoing
arcs to vj+1
i
, . . . , vB−1
i
. Finally, if j < B, then vj
i has an incoming arc from one

NP-Hardness and Fixed-Parameter Tractability
287
of the three preceding a-vertices. Otherwise, if j ≥B, then vj
i has an outgoing
arc to one of the three subsequent a-vertices. Hence, the corresponding x-element
of vj
i is as follows:
xj
i :=

(2i −1)B + j + 1
(2m −2i + 1)B −1 −j

if j < B,
xj
i :=

(2i −1)B + j
(2m −2i + 1)B −j

if j ≥B.
Observe that the strong NP-hardness of 3-Partition is essential to prove the
polynomial running time of the reduction: The size of the constructed DAG
Realization instance is upper-bounded by a polynomial in the values of the
integers in A. Since 3-Partition is strongly NP-hard, it remains NP-hard when
the values of the integers in A are bounded by a polynomial in the input size.
Hence, the size of the DAG Realization instance is polynomially bounded in
the size of the 3-Partition instance. Clearly, the construction can be computed
in polynomial time.
Correctness. Next, we prove the correctness of the construction given above.
Therefore, throughout this subsection let (A, B) be an instance of 3-Partition
and let S be the constructed degree sequence.
Lemma 2. If (A, B) is a yes-instance of 3-Partition, then S is a yes-instance
of DAG Realization.
To show the reverse direction, we ﬁrst need two observations.
– Observation 1: In any DAG D realizing S, the a-vertices form an independent
set and the x-vertices form a transitive tournament.
– Observation 2: If S is realizable, then there is a realizing topological ordering
where the x-vertices are ordered as follows:
x0
0, x1
0, . . . , xB−1
0
, x0
1, . . . , x2B−1
1
, x0
2, . . . , x2B−1
2
, x0
3, . . . , x2B−1
m−1 , x0
m, . . . , xB−1
m
.
Lemma 3. If S is a yes-instance of DAG Realization, then (A, B) is a yes-
instance of 3-Partition.
Proof. Let D = (V, A) be the realization of S with a topological ordering φ.
Let vj
i be the x-vertex realizing xj
i and let ui be the a-vertex realizing ai. Fur-
thermore, posφ(v) denotes the position of v in the topological ordering φ. Since S
is a yes-instance, we can assume by Observation 2 that posφ(vj
i ) < posφ(vℓ
i)
for j < ℓand that posφ(vj
i ) < posφ(vℓ
k) for i < k.
From Observation 1 it follows that none of the x-vertices v0
0, v1
0, . . . , vB−1
0
has an incoming arc from an a-vertex, but each has one outgoing arc to an
a-vertex. Hence, we can assume that posφ(ui) > Φ(vB−1
0
) for all 1 ≤i ≤
3m. Observe that each x-vertex v0
1, v1
1, . . . , vB−1
1
has one incoming arc from
an a-vertex and no outgoing arc to an a-vertex. Hence, we can assume that

288
S. Hartung and A. Nichterlein
there are a-vertices ui1, ui2, . . . , uiℓwith posφ(vB−1
0
) < posφ(uij) < posφ(v0
1)
and ℓ
j=1 aij = B. Since B/4 < aj < B/2 for all 1 ≤j ≤3m, it follows
that ℓ= 3.
The vertices vB
1 , . . . , v2B−1
1
also have no incoming arc from an a-vertex but each
of them has an outgoing arc to an a-vertex. Also, each of the vertices v0
2, . . . , vB−1
2
needs one incoming arc from an a-vertex. Thus, we can assume that in the topo-
logical ordering φ of D there are three a-vertices between v2B−1
1
and v0
2 such that
their indegrees and also their outdegrees sum up to B. Analogously, it follows for
all 1 ≤i < m that there are three a-vertices uji
1, uji
2, uji
3 with posφ(v2B−1
i
) <
posφ(uji
1) < posφ(uji
2) < posφ(uji
3) < posφ(v0
i+1) and 3
ℓ=1 aji
ℓ= B. Thus,
(A, B) is a yes-instance of 3-Partition.
⊓⊔
Our construction together with Lemma 2 and Lemma 3 yields the NP-hardness
of DAG Realization. Containment in NP is easy to see: Guessing a n-vertex
DAG and checking whether or not it is a realization for S is clearly doable in
polynomial time. Hence, we arrive at the following theorem.
Theorem 1. DAG Realization is NP-complete.
Berger and M¨uller-Hannemann gave a polynomial-time algorithm for DAG Re-
alization if the degree sequence can be ordered with respect to the opposed
order [1]. Hence, one may search for other polynomial-time solvable special
cases. One way to identify such special cases is to have a closer look on NP-
hardness proofs and to check whether certain “quantities” need to be unbounded
in order to make the proof (many-to-one reduction) work [14,16]. In our NP-
hardness proof the maximum degree Δ is unbounded. We show in the next sec-
tion that DAG Realization is polynomial-time solvable for constant maximum
degree. Indeed, we can even show ﬁxed-parameter tractability with respect to the
parameter Δ.
3
Fixed-Parameter Tractability
Denoting the maximum degree in a degree sequence by Δ, in this section we
show that DAG Realization is ﬁxed-parameter tractable with respect to the
parameter Δ. To describe the basic idea that our ﬁxed-parameter algorithm is
based on, we need the following deﬁnition.
Deﬁnition 2. Let φ = v1, v2, . . . , vn be a topological ordering for a DAG D. For
all 1 ≤i ≤n, the potential at position i is a vector pφ
i ∈NΔ where pφ
i [l] for
1 ≤l ≤Δ is the number of vertices in the subsequence v1, . . . , vi that have in D
at least l neighbors in the subsequence vi+1, . . . , vn. The value of the potential pφ
i
is ω(pφ
i ) := Δ
l=1 pφ
i [l].
If the topological ordering φ is clear from the context, then we write p instead
of pφ. Observe that, for any potential pi ∈NΔ, it holds that pi[j] ≥pi[j + 1] for
all 1 ≤j < Δ.

NP-Hardness and Fixed-Parameter Tractability
289
Algorithm Outline. Our algorithm consists of two parts. First, if the degree se-
quence of a DAG Realization instance admits a DAG realization where at any
position the value of the potential is at least Δ2, then we shall ﬁnd such a “high-
potential” realization with the algorithm that is described in Subsection 3.2. Oth-
erwise, by exploiting the fact that the value of all potentials is upper-bounded, we
shall ﬁnd a “low potential” realization with the algorithm described in
Subsection 3.3.
3.1
General Terms and Observations
In this section we introduce some general notations and observations that will be
used in the algorithms to ﬁnd high potential as well as low potential realizations.
Notation: For a topological ordering φ = v1, . . . , vn and two indices 1 ≤i ≤j ≤
n, set φ[i, j] := vi, vi+1, . . . , vj. The set {vi, . . . , vj} is also denoted by φ[i, j].
Deﬁnition 3. Let S =
a1
b1

, . . . ,
an
bn

be a degree sequence. Two tuples
ai
bi

and
aj
bj

are of the same type if ai = aj and bi = bj. Furthermore, a type
ai
bi

is a good type if ai ≤bi and otherwise it is a bad type.
Note that there are at most (Δ + 1)2 diﬀerent types.
Well-Connected DAGs. Berger and M¨uller-Hannemann already observed
that, given a degree sequence S =
a1
b1

, . . . ,
an
bn

, one can check in polyno-
mial time whether S is realizable by a DAG with a corresponding topological
ordering v1, . . . , vn where d−(vi) = ai and d+(vi) = bi [2]. This implies that it is
suﬃcient to compute the correct ordering of the elements in S as they appear in
a topological ordering of a realizing DAG. To prove this, the main observation
is that for any topological ordering one can construct at least one corresponding
DAG by well-connecting consecutive vertices.
Deﬁnition 4. Let D be a DAG with a corresponding topological ordering φ =
v1, . . . , vn. The remaining outdegree at position j of vertex vi, 1 ≤i ≤j ≤n,
is the number of vi’s neighbors in the subsequence φ[j, n]. Furthermore, D is
well-connected if for all vertices vi ∈φ it holds that vi is connected to the d−(vi)
vertices in φ[1, i −1] that have the highest remaining outdegree at position i −1.
It is not hard to show that in a well-connected DAG the potential at position i
can be easily determined from the potential at position i −1.
Cut-Out Subsequences. The following lemma shows that if in a topological
ordering φ[1, n] there are two indices 1 ≤i < j ≤n with equal potential, then
we can cut out φ[i + 1, j] resulting in a topological ordering φ[1, i][j + 1, n].
One can show that one can reinsert φ[i + 1, j] at any position that fulﬁlls some
reasonable conditions. Thereby, the corresponding degree of each vertex do not
change. Cutting out subsequences and reinserting them appropriately is the main
operation that we perform in order to “restructure” a topological ordering such
that we can exploit the resulting regular structure in our algorithms.

290
S. Hartung and A. Nichterlein
Lemma 4. Let φ = v1, . . . , vn be a realizing topological ordering for the degree
sequence S =
a1
b1

, . . . ,
an
bn

. If there are two indices 1 ≤i < j ≤n such that
pφ
i = pφ
j , then the sequence φ′ = φ[1, i]φ[j+1, n] is a realizing topological ordering
for the degree sequence that results from S by deleting the degrees of the vertices
in φ[i + 1, j]. Moreover, the potential pφ′
i+l is equal to pφ
j+l for all 1 ≤l ≤n −j.
Lemma 4 shows that from a topological ordering φ we can cut out a subse-
quence φ[i + 1, j] whenever pφ
i = pφ
j . Informally speaking, one can show that the
subsequence φ[i + 1, j] can be inserted into any topological ordering φ′ at posi-
tion b whenever there is “enough potential” from the left part φ′[1, b] to satisfy
the indegrees of φ[i + 1, j]. Then, the structure of φ[i + 1, j] guarantees that the
“remaining potential” of φ′[1, b]φ[i + 1, j] is suﬃcient to satisfy the indegree of
φ′[b + 1, n] and thus φ′[1, b]φ[i + 1, j]φ′[b + 1, n] is a topological ordering.
3.2
High Potential Sequences
In this subsection we show that if a realizable sequence admits a realizing topo-
logical ordering where at some position the value of the potential is at least Δ2,
a so-called high potential realizing topological ordering , then there is also a re-
alizing topological ordering φ that is of the following “pattern”: The ordering φ
can be partitioned into four sub-sequences I ◦G◦B◦E (where ◦is the concatena-
tion). The sequence I is an initializing sequence that “establishes” a potential of
value at least Δ2, a so-called high potential. Correspondingly, at the end there is
a sequence E that reduces the value of the potential from a value that is greater
than Δ2 to zero. Furthermore, I and E are of length at most Δ2Δ and thus can
be guessed in O((Δ + 1)2)Δ2Δ) = O(Δ2Δ2Δ) time. The subsequence G, which
is of arbitrary length, only consists of good types and, correspondingly, B is of
arbitrary length but only consists of bad types.
Our strategy to prove that there is a high potential realizing topological order-
ing with the pattern I ◦G◦B◦E is as follows. Let φ = v1, . . . , vn be an arbitrary
high potential realizing topological ordering and let i be the minimum position
with high potential and, symmetrically, let j be the maximum position with high
potential. First, one can show that we can assume that i ≤Δ2Δ and j ≥n−Δ2Δ.
Towards this the main argument is that if i > Δ2Δ, since there are O(Δ2Δ) po-
tentials with value less than Δ2, there have to be two positions 1 ≤l1 < l2 < i
with pl1 = pl2. Then, by Lemma 4, we can cut out φ[l1 + 1, l2] from φ and one
can show that it can be reinserted right behind i, resulting in a realizing topolog-
ical ordering φ[1, l1]φ[l2 + 1, i]φ[l1 + 1, l2]φ[i + 1, n]. By iteratively applying this
operation, we end up with a realizing topological ordering where the minimum
position with high potential is at most Δ2Δ. A symmetric argument holds for
the maximum position j with high potential. Second, one can show that the
vertices in φ[i + 1, j] can be arbitrarily sorted under the constraint that at ﬁrst
vertices of good type occur, and then they are followed by the bad type vertices.
Altogether, this shows that in order to check whether there is a high potential
realizing topological ordering it is suﬃcient to branch into all possibilities to

NP-Hardness and Fixed-Parameter Tractability
291
choose I and E, insert the remaining vertices sorted by good and bad types in
between, and, ﬁnally, check whether this ordering is a topological ordering.
Theorem 2. If a DAG Realization instance admits a high potential realizing
topological ordering, then it can be solved in O(Δ4Δ2Δ · n) time.
3.3
Low Potential Sequences
In this section, we shall provide an algorithm that ﬁnds a low potential realization
(if one exists) for a DAG Realization instance. That is, a realization such that
in the corresponding topological ordering the value of all potentials is strictly
less than Δ2.
As in the high potential case, the main idea is to restrict the length of the
parts in a realizing topological ordering that have to be guessed by brute-force.
In the low potential case, we can exploit that there are at most Δ2Δ potentials
with value less than Δ2 and thus if the length of a realizing topological ordering
is greater than Δ2Δ, then there have to be two positions with equal potential. We
call subsequences with equal potential at their beginning and end super-types.
Then, by cutting-out super-types and reinserting them appropriately we can
bound the distance between two subsequent positions with the same potential,
implying a restricted number of diﬀerent super-types. Hence, if one removes in a
realizing topological ordering all repetitions of a super-type, then the length of
the resulting non-repeating ordering can be upper-bounded in a function of Δ.
The algorithm works as follows: In the ﬁrst step it branches into all possibilities
to choose a non-repeating ordering. In the second step, we check via solving
an ILP (integer linear program) whether this non-repeating ordering can be
extended by inserting repeating super-types to a realizing topological ordering
of the input degree sequence.
Theorem 3. If a degree sequence admits a low potential realizing topological
ordering, then it can be found in ΔΔΔO(Δ)
· n time.
Theorems 2 and 3 together lead to the main theorem of this section.
Theorem 4. DAG Realization is ﬁxed-parameter tractable with respect to
the parameter maximum degree Δ.
Note that Theorem 4 is a mere classiﬁcation result: The corresponding running
time is ΔΔΔO(Δ)
· n. It is dominated by the low potential case.
4
Conclusion and Open Questions
Answering an open question by Berger and M¨uller-Hannemann [1] we proved the
NP-completeness of DAG Realization. Following the spirit of deconstructing
intractability we ﬁgured out the necessity of large degrees in the NP-hardness
proof by showing ﬁxed-parameter tractability for DAG Realization with

292
S. Hartung and A. Nichterlein
respect to the maximum degree Δ. The natural questions whether DAG Real-
ization is solvable in single-exponential time and whether it admits a polynomial-
size problem kernel with respect to the parameter Δ arises. In our NP-hardness
reduction other parameters occur with unbounded values, for instance, the num-
ber of types. Investigating this parameter is an interesting task for future work.
Acknowledgements.
We thank Annabell Berger and Matthias M¨uller-
Hannemann for fruitful discussions about DAG Realization. Moreover, we are
grateful to Rolf Niedermeier for helpful comments improving the presentation.
References
1. Berger, A., M¨uller-Hannemann, M.: Dag Realizations of Directed Degree Sequences.
In: Owe, O., Steﬀen, M., Telle, J.A. (eds.) FCT 2011. LNCS, vol. 6914, pp. 264–275.
Springer, Heidelberg (2011)
2. Berger, A., M¨uller-Hannemann, M.: How to Attack the NP-Complete Dag Real-
ization Problem in Practice. In: Klasing, R. (ed.) SEA 2012. LNCS, vol. 7276, pp.
51–62. Springer, Heidelberg (2012)
3. Chen, W.K.: On the realization of a (p, s)-digraph with prescribed degrees. J.
Franklin Inst. 281(5), 406–422 (1966)
4. Downey, R.G., Fellows, M.R.: Parameterized Complexity. Springer (1999)
5. Erd˝os, P., Gallai, T.: Graphs with prescribed degrees of vertices. Math. Lapok 11,
264–274 (1960) (in Hungarian)
6. Flum, J., Grohe, M.: Parameterized Complexity Theory. Springer (2006)
7. Fulkerson, D.: Zero-one matrices with zero trace. Paciﬁc J. Math. 10(3), 831–836
(1960)
8. Gale, D.: A theorem on ﬂows in networks. Paciﬁc J. Math. 7, 1073–1082 (1957)
9. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. Freeman (1979)
10. Hakimi, S.: On realizability of a set of integers as degrees of the vertices of a linear
graph. i. J. SIAM 10(3), 496–506 (1962)
11. Havel, V.: A remark on the existence of ﬁnite graphs. Casopis Pest. Mat. 80, 477–
480 (1955)
12. Hulett, H., Will, T.G., Woeginger, G.J.: Multigraph realizations of degree se-
quences: Maximization is easy, minimization is hard. Oper. Res. Lett. 36(5), 594–
596 (2008)
13. Kleitman, D., Wang, D.: Algorithms for constructing graphs and digraphs with
given valences and factors. SIAM J. Discrete Math. 6(1), 79–88 (1973)
14. Komusiewicz, C., Niedermeier, R., Uhlmann, J.: Deconstructing intractability—a
multivariate complexity analysis of interval constrained coloring. J. Discrete Algo-
rithms 9(1), 137–151 (2011)
15. Niedermeier, R.: Invitation to Fixed-Parameter Algorithms. Oxford University
Press (2006)
16. Niedermeier, R.: Reﬂections on multivariate algorithmics and problem parameter-
ization. In: Proc. 27th STACS. LIPIcs, vol. 5, pp. 17–32. IBFI Dagstuhl, Germany
(2010)
17. Ryser, H.: Combinatorial properties of matrices of zeros and ones. Canadian J.
Math. 9, 371–377 (1957)

A Direct Proof of Wiener’s Theorem
Matthew Hendtlass and Peter Schuster
Pure Mathematics
University of Leeds
Leeds LS2 9JT, England
{mmmrh,pmtpsc}@leeds.ac.uk
Abstract. In functional analysis it is not uncommon for a proof to pro-
ceed by contradiction coupled with an invocation of Zorn’s lemma. Any
object produced by such an application of Zorn’s lemma does not in fact
exist, and it is likely that the use of Zorn’s lemma is artiﬁcial. It has
turned out that many proofs of this sort can be simpliﬁed, both in form
and complexity, with the principle of open induction isolated by Raoult
as a substitute for Zorn’s lemma. If moreover the theorem under consid-
eration is suﬃciently concrete, then a far weaker instance of induction
suﬃces and, with some massaging, one may obtain a fully constructive
proof. In the present note we apply this method to Gelfand’s proof of
Wiener’s theorem, producing ﬁrst a simple direct proof of Wiener’s the-
orem, and then an even simpler constructive proof. With this example in
mind we look toward developing a more generally applicable technique.
1
Introduction
This paper is the beginning of a programme to systematically constructivise
theorems from functional analysis which typically, in the classical setting, rely
on a proof by contradiction coupled with an invocation of Zorn’s lemma. Since
the ideal objects produced by an application of Zorn’s lemma in a proof by
contradiction do not exist (hence the contradiction), proofs of this form can
often be simpliﬁed by giving a direct proof using the principle of open induction
isolated by Raoult [12]. As pointed out in [13] for the setting of abstract algebra,
if a theorem proved with open induction has ﬁnite input data, then a ﬁnite
partial order carries the required instance of induction, giving an elementary
proof. In analysis we do not deal with ﬁnite inputs, but if a result is suﬃciently
concrete, then we can often make do with standard mathematical induction.
Proving concrete theorems directly with open induction removes unneccesary
appeal to the ideal objects of Zorn’s lemma, thus allowing us to work with
objects of concrete existence. This reduces the logical complexity, modulo open
induction, of our proofs and removes a major hurdle in constructivising proofs
of this form. Ultimately, we are interested in uniform techniques to translate
proofs which use Zorn’s lemma and contradiction into proofs via induction.
In this paper we consider Gelfand’s proof of Wiener’s theorem on the invert-
ibility of absolutely convergent Fourier series. We ﬁrst use open induction for a
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 293–302, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

294
M. Hendtlass and P. Schuster
simple direct variant of this proof , and then give an even simpler constructive
proof—motivated by the ﬁrst—which only requires a weak form of countable
choice.
Our aim is not to give either an elementary or a constructive proof of Wiener’s
theorem, tasks which have been performed by others. The purpose of the present
paper is to demonstrate the general approach this programme will take.
Functional analysis has already received plenty of attention from a constructive
point of view [4,2,7,8,6,3], including constructive proofs of Wiener’s theorem [7].1
We have in mind the following explicit statement of Wiener’s 1/f theorem.
Wiener’s Theorem. If (an)n∈Z is a sequence of complex numbers with
∞

n=−∞
|an| < ∞
such that the function f : [0, 2π] →C never vanishes which is deﬁned by
f(t) =
∞

n=−∞
aneint ,
then there exists a sequence (bn)n∈Z of complex numbers with
∞

n=−∞
|bn| < ∞
such that fg(t) = 1 for all t ∈[0, 2π] where
g(t) =
∞

n=−∞
bneint.
A constructive proof of this statement will embody an algorithm which takes as
input an absolutely summable sequence of complex numbers (an)n∈Z and outputs
another absolutely summable sequence of complex numbers (bn)n∈Z such that
g(t) = 1/f(t) for all t where f, g are as above.
We take as our base theory a suitable fragment of Aczel’s constructive Zermelo-
Fraenkel set theory (CZF) [1]. We indicate in brackets when our proofs require
more than that, e.g. the classical Zermelo-Fraenkel set theory with choice (ZFC).
2
A Direct Proof of Wiener’s Theorem
In this section we give a simple proof, using open induction and classical logic,
of Wiener’s theorem. This however requires some preparation.
1 Coquand and Spitters are preparing a treatise on constructive Banach algebra theory.

A Direct Proof of Wiener’s Theorem
295
Open induction [12] is a classical reformulation of Zorn’s lemma. Let P be a
chain complete partial order. A subset A of P is open if A intersects a chain C
in P whenever the supremum  C of C is in A; and A is progressive if
∀x∈P (∀y>xy ∈A →x ∈A) .
The most general form of open induction is:
OI.
If A is an open progressive subset of a chain complete poset P,
then A = P.
It is easy to show that OI is equivalent to Zorn’s lemma in ZF.
Our proof of Wiener’s theorem is a recasting of Gelfand’s short and elegant
proof using Gelfand theory. That such a proof is possible is not surprising since
Wiener’s theorem is a concrete statement about relatively simple objects; there
also have been elementary proofs of Wiener’s theorem before [11].
Let A be the set of all functions of the form
f(t) =
∞

n=−∞
aneint
where (an)n∈Z is a sequence of complex numbers such that
∞

n=−∞
|an| < ∞;
each f ∈A will be uniformly continuous with f(0) = f(2π). With addition and
multiplication on A given by the pointwise operations, and with the norm
∥f∥=
∞

n=−∞
|an|
where f is as above, A forms a commutative Banach algebra with unit 1 : x →1.
With this notation, Wiener’s theorem can be succinctly stated as
Theorem 1 (ZFC). If f ∈A is nonzero everywhere, then 1/f ∈A.
Gelfand’s proof uses the fact that the maximal ideals of A are of the form
Mt = {f ∈A : f(t) = 0}
with t ∈[0, 2π] (see e.g. [9, page 26-27]) and proceeds as follows. Let f ∈A be
such that f is nonzero everywhere and suppose that 1/f /∈A. Then the ideal
⟨f⟩generated by f is a proper ideal; by Zorn’s lemma, applied to the poset of
proper ideals of A ordered by inclusion, there exists a maximal ideal M which
contains ⟨f⟩. But by the above characterisation of maximal ideals, M = Mt for
some t ∈[0, 2π], which is absurd; whence 1/f ∈A.

296
M. Hendtlass and P. Schuster
Our proof, which apart from Gelfand’s method is motivated by the interplay
between varieties and ideals characteristic of algebraic geometry, applies open
induction on the—a fortiori, proper—ideals of A of the form
I(S) = {f ∈A : f(s) = 0 for all s ∈S} =

t∈S
Mt ,
where S is a nonempty subset of [0, 2π]. Our ﬁrst task is to show that the
collection P of these ideals, partially ordered by inclusion, is chain complete.
Before so doing we notice that I is antimonotone, and transforms a union of
subsets into the intersection of their ideals.
Lemma 1 (ZF). Let S, S′ be subsets of [0, 2π]. If I(S) ⊆I(S′), then S′ ⊆S.
In particular, I(S) = I(S′) if and only if S = S′.
Proof. Suppose x ∈S′ and d = ρ(x, S) > 0. Let f ∈A be a bump function on
[0, 2π] which is zero outside (x −d, x + d) ∩[0, 2π] and nonzero at x. Then f is
in I(S) but is not in I(S′)—a contradiction. Hence x ∈S.
We pause here to examine the structure of P; in particular to show that P is a
lattice in ZF. We ﬁrst observe that I([0, 2π]) = {0} is the least element of P,
and that the maximal elements of P are those of the form Mt with t ∈[0, 2π].
We now prove that P is a lattice. It is clear that the meet I(S)∧I(S′) of I(S)
and I(S′) is I(S) ∩I(S′) = I(S ∪S′). The join I(S) ∨I(S′) of I(S) and I(S′)
is I(S ∩S′): it is clear that I(S ∩S′) is an upper bound for I(S) and I(S′). Let
T ⊆[0, 2π] be such that I(T ) is also an upper bound for I(S) and I(S′). Then,
by Lemma 1, T ⊆S and T ⊆S′. Hence T ⊆S ∩S′, so I(S ∩S′) ⊆I(T ).
Although we have focused our attention on ideals of A, Lemma 1 tells us
that the poset P is anti-isomorphic to the poset Q of nonempty closed subsets
of [0, 2π]; whence the dual poset P ∼= Qop is isomorphic to the poset of proper
open subsets of [0, 2π], of course with the standard topology. The latter is chain
complete in ZF (in a compact space, the union of a chain of proper open subsets
is proper); whence the following is immediate.2
Proposition 1 (ZF). The poset P is chain complete.
We need to be more precise on this. The zero set of an ideal H of A is
Z (H) = {t ∈[0, 2π] : h (t) = 0 for all h ∈H} = {t ∈[0, 2π] : Mt ⊇H} .
This Z is antimonotone, and converts a sum of ideals into the intersection of their
zero sets. By Lemma 1, ZI (S) = S for every S ⊆[0, 2π]; in particular, ZI (S) =
S for every S ∈Q and IZ (F) = F for every F ∈P. So the aforementioned
anti-isomorphism between P and Q assigns F ∈P to Z (F) ∈Q and S ∈Q to
I (S) ∈P. Hence if C is a chain in P, then  C = IZ (H) where H =  C.
2 This has kindly been pointed out to us be one of the referees.

A Direct Proof of Wiener’s Theorem
297
We say that two sets D and E meet, denoted D ≬E, if D ∩E is nonempty.3
The Jacobson radical of an ideal H of A is
Jac (H) = {r ∈A : H ≬1 + ⟨1 −rs⟩for every s ∈A} .
This deﬁnition is classically equivalent to the more common one:
Lemma 2 (ZFC). For every ideal H of A we have
Jac (H) =

M⊇H
M
where M ranges over the maximal ideals of A.
While this lemma—which of course holds for an arbitrary commutative ring—is
usually proved by means of Zorn’s lemma (see for example [10, IX.1.1], also for
more discussion), a proof by open induction is possible. 4
Now, if H is an ideal of A, then IZ (H) = Jac (H), because t ∈Z (H) precisely
when Mt ⊇H. In all, if C is a chain in P, then  C = Jac (H) where H =  C.
Here ﬁnally is our proof of Theorem 1. To show that 1/f ∈A is tantamount
to show that 1 ∈⟨f⟩or, equivalently, that 1 + ⟨f⟩≬{0}. We will show this last
condition by an open induction in P. To this end let
A = {F ∈P : 1 + ⟨f⟩≬F}.
We ﬁrst show that A is open. Let C be a chain in P, and f ∈A. If Jac (H) ≬
1 + ⟨f⟩, say h = 1 −fg belongs to Jac (H), then H ≬1 + ⟨1 −h⟩= 1 + ⟨fg⟩.
Hence H ≬1 + ⟨f⟩as required.
To see that A is progressive, let F ∈P with F = I (S) where S ⊆[0, 2π]. We
have two cases, a base case and the induction case, depending on the size of S.
If |S| = 1, that is, S = {t} for some t ∈[0, 2π], then F = Mt is a maximal ideal
of A. Setting g = −f(t)−1, which actually belongs to C, we have that
(1 + fg)(t) = 1 + f(t)g(t) = 0,
so 1 + fg ∈F and 1 + ⟨f⟩≬F.
If |S| ⩾2, pick distinct elements t1, t2 of S and let δ = |t1 −t2|/2. By Lemma 1
and the induction hypothesis,
1 + ⟨f⟩≬I(Si)
(i = 1, 2)
where Si = S \ (ti −δ, ti + δ). Hence the multiplicative set 1 + ⟨f⟩meets
I(S1)I(S2) ⊆I(S1) ∩I(S2) = I(S1 ∪S2) = F.
Thus A is progressive. We can now apply open induction to conclude that A = P.
In particular, {0} ∈P, which is to say that 1+⟨f⟩≬{0} or, equivalently, 1/f ∈A.
3 To our knowledge this notation has been coined by Giovanni Sambin.
4 Simon Huber has kindly communicated to us a proof of this kind.

298
M. Hendtlass and P. Schuster
3
Constructing the Inverse
We are concerned not with giving a constructive proof of Theorem 1, this
has been done [7], but with the following question: Can we adapt the proof
of Theorem 1 to actually construct the element 1/f of A?
Let us look more closely at the induction in the proof of Theorem 1. We
suppose that 1 + ⟨f⟩≬I(S1) and 1 + ⟨f⟩≬I(S2); let −g1, −g2 ∈A be witnesses
of these statements: 1 −fgi ∈I(Si) (i = 1, 2). Then
(1 −fg1)(1 −fg2) = 1 −fg1 −fg2 + fg1fg2
= 1 −f(g1 + g2 −g1fg2)
is in
I(S1)I(S2) ⊆I(S1) ∩I(S2) = I(S1 ∪S2).
Given a well ordering (xα)α⩽2ℵ0 of [0, 2π] we can recast our proof of Wiener’s
theorem via open induction as a proof by transﬁnite induction. Using the above,
we can construct a sequence of elements (gα)α⩽2ℵ0 of A such that gα is zero on
{xβ : β ⩽α}. Then 1/f = g2ℵ0.
We want to restrict the induction in our proof of Wiener’s theorem to math-
ematical induction. Suppose we are given a sequence (qn)n∈N of elements of
[0, 2π]. Using the argument in the proof of Theorem 1 we construct, by primitive
recursion, functions gn ∈A (n ∈N) such that fgn(qm) = 1 for each m ⩽n. To
do so we set g1 = f(q1)−1, which again belongs to C, and
gn = f(qn)−1 + gn−1 −f(qn)−1gn−1f
for n > 1; moreover, by mathematical induction,
1 −gnf =
n

k=1
(1 −f/f(qk)) .
We would now like to pick a sequence (qn)n∈N such that
(i) (qn)n∈N is dense in [0, 2π] and
(ii) the sequence (gn)n∈N in A converges.
For then this limit must, by continuity, be the inverse of f. This, however, does
not work: for example, let f : t →eiπt. By construction, each gn is a polynomial
in f, but 1/f, t →e−iπt, cannot be approximated by a polynomial in eiπt. This
process, however, proves very straightforward when f is real valued, and with
some elementary geometry we can extend to the general case.
In the following we use, in addition to some small fragment of CZF, the weak
countable choice principle from [5]:

A Direct Proof of Wiener’s Theorem
299
WCC. Let (An)n⩾1 be a sequence of nonempty sets such that if n ̸= m,
then either An is a singleton or Am is a singleton. Then there exists a
sequence (an)n⩾1 with an ∈An for each n.
The principle WCC is valid in classical ZF but independent of Friedman’s IZF.5
We denote by ∥· ∥∞the supremum norm on A, and reserve ∥· ∥for the
Wiener norm
∥f∥=
∞

n=−∞
|an|;
hereafter we use A exclusively for A with the Wiener norm. We need the following
two results from elementary calculus, which we do not prove.
Lemma 3. If f ∈A, then
∥f∥⩽∥f∥∞+

d
dxf

∞
.
Lemma 4. Let f be a real valued function on [0, 2π]. If ∥f∥∞< 1/4, then

d
dxf 2

∞
< 1
2

d
dxf

∞
.
Theorem 2 (WCC). If f ∈A with inf |f| > 0, then 1/f ∈A.
Proof. For S ⊆C, deﬁne
A(S) = {f ∈A : ∀t∈[0,2π]f(t) ∈S}.
We ﬁrst consider the special case where f ∈A(0, ∞). We will construct a se-
quence (qn)n∈N in [0, 2π] such that the sequence (gn)n∈N, deﬁned above, con-
verges to the inverse of f. By construction
∥gnf −1∥∞=

n

k=1
(1 −f/f(qk))

∞
⩽
n

k=1
∥1 −f/f(qk)∥∞.
Let
r = 1 −inf f
sup f ,
and suppose that inf f < sup f. Pick q ∈[0, 2π] such that
f(q) ⩾sup f/(1 + r).
If we now set qn = q for each n ∈N, then
∥gnf −1∥∞=

n

k=1
(1 −f/f(qk))

∞
⩽∥1 −f/f(q)∥n
∞⩽rn
5 The latter can be shown by way of a sheaf model as provided e.g. in [14].

300
M. Hendtlass and P. Schuster
and thus ∥gnf −1∥∞→∞as n →∞. Now let n ∈N be such that
∥gnf −1∥∞< 1/4.
Then, by Lemma 4,

d
dx(1 −g2knf)

∞
=

d
dx(1 −f/f(q))2kn

∞
< 1
2k

d
dx(1 −f/f(q))n

∞
,
and hence
 d
dx(1 −g2knf)

∞→0 as k →∞. It now follows from Lemma 3 that
1 −g2knf →0 in A as k →∞. Since A is complete, 1/f = limk→∞g2kn is in A.
Now consider an arbitrary f ∈A (0, ∞). As in [5], by WCC we can choose an
increasing binary sequence (λn)n⩾1 such that
λn = 0 ⇒sup f −inf f < 1/n ;
λn = 1 ⇒sup f −inf f > 1/(n + 1) .
Without loss of generality we may assume that λ1 = 0. Pick any p ∈[0, 2π], and
let n ⩾1. If λn = 0, then we set gn = 1/f(p); in this case
∥gnf −1∥∞= ∥f/f(p) −1∥∞⩽max

1 −inf f
f(p) , sup f
f(p) −1
	
⩽

1 −inf f
f(p)

+

sup f
f(p) −1

= sup f −inf f
f(p)
<
1
n inf f .
If however λk = 1 −λk−1, then we follow the ﬁrst part of this proof with f ′ =
f/f (p) in place of f, construct a sequence (g′
n)n⩾1 in A such that ∥g′
nf ′−1∥∞→
0 as n →∞, and set gm = g′
m−k+1/f (p) for every m ⩾k. In this case,
∥gmf −1∥∞= ∥g′
m−k+1f ′ −1∥∞⩽rm−k+1
for all m ⩾k, simply because inf f ′/ sup f ′ = inf f/ supf.
In all, ∥gnf −1∥∞→0 as n →∞, from which we conclude as in the ﬁrst
part.
Now let f be an arbitrary element of A satisfying the hypothesis, and set
f ∗(t) =

n∈Z
ane−int.
Note that f ∗is in A. Since the map t →−t corresponds to a reﬂection in the
real axis, ff ∗is real valued; in fact, ff ∗= |f|2 ⩾inf |f|2 > 0. By the ﬁrst part
of this proof, there is g ∈A such that ff ∗g = 1; then 1/f = f ∗g is also in A.
4
A More General Result
In our classical proof of Theorem 1 we use not much that is speciﬁc to the algebra
of continuous periodic functions with absolutely summable Fourier coeﬃcients.
This observation allows us to abstract from that particular context, as follows.

A Direct Proof of Wiener’s Theorem
301
Let X be a topological space, and let A be a subalgebra of C(X). Consider the
partial order P(A) of—a fortiori, proper—ideals of the form
I(S) = {f ∈A : f(s) = 0 for all s ∈S},
where S is a nonempty subset of X.
We say that A isolates the points of X if for each x ∈X and each open
neighbourhood W of x there exists an f such that f(x) is nonzero and f is zero
on −W. For any such A the proof of Lemma 1 works equally well; whence in
particular the following counterpart of Proposition 1 holds:
Proposition 2 (ZF). Let X be a compact topological space. If a subalgebra A
of C(X) isolates the points of X, then P(A) is chain complete.
Also, the computation of the supremum of a chain can be carried over to P(A).
Theorem 3 (ZFC). Let A be a subalgebra of C(X) that isolates the points of
the compact Hausdorﬀspace X. If f ∈A is nonzero everywhere, then 1/f ∈A.
We will prove this theorem as an example of the proof principle from [13]. This
proof principle was in fact motivated by the proof of Theorem 1 given above.
Let U be a unary predicate on a partial order P with ﬁnite meets. An element
x ∈P is reducible if it can be written as the meet of two strictly larger elements
of P, i.e. there exist y, z ∈P such that y > x, z > x, and x = y ∧z. The
predicate U is said to be good if for every x ∈P, either x is reducible or U(x).
Further, U is meet closed if U(x) holds whenever x = y ∧z, U(y), and U(z).
Now the proof principle from [13] gives the following.
Theorem 4 (OI). Let U be an open predicate on a chain complete poset P. If
U is meet closed and good, then ∀x∈PU(x).
To prove Theorem 3, it suﬃces to show that the predicate
U(S) ≡1 + ⟨f⟩≬I(S)
on the chain complete poset P(A) is open, good, and meet closed, where the
meet operation is intersection. To do so we follow the proof of Theorem 1 given
above; in particular, the proof that U is open is completely analogous.
Suppose |S| = 1; then S = {t} for some t ∈X. Setting g = −f(t)−1 we have
(1 + fg)(t) = 1 + f(t)g(t) = 0,
so 1 + fg ∈I(S) and U(S). On the other hand, if |S| ⩾2, pick distinct elements
t1, t2 of S and let W1, W2 be disjoint open subsets of X such that ti ∈Wi. Then
(S \ W1) ∪(S \ W2) = S ;
whence I(S) is reducible in view of the counterpart of Lemma 1. In all, we have
proved that U is good.

302
M. Hendtlass and P. Schuster
It remains to show that U is meet closed. To this end, let I (S) = I (S1)∩I (S2),
and suppose that U(S1), U(S2); that is,
1 + ⟨f⟩≬I(Si)
(i = 1, 2).
Hence 1 + ⟨f⟩meets
I(S1)I(S2) ⊆I(S1) ∩I(S2) = I(S),
so U(S). Theorem 3 now follows from Theorem 4.
References
1. Aczel, P., Rathjen, M.: Notes on Constructive Set Theory. Report No. 40, Institut
Mittag-Leﬄer, Royal Swedish Academy of Sciences (2001)
2. de Bruijn, N.G., van der Meiden, W.: Notes on Gelfand’s theory. Indag. Math. 29,
467–474 (1967)
3. Bridges, D.S.: Constructive Functional Analysis. Research Notes in Mathematics,
vol. 28. Pitman, London (1979)
4. Bishop, E.A., Bridges, D.S.: Constructive Analysis. Grundlehren der Math. Wiss,
vol. 279. Springer, Heidelberg (1985)
5. Bridges, D., Richman, F., Schuster, P.: A weak countable choice principle.
Proc. Amer. Math. Soc. 128, 2749–2752 (2000)
6. Cohen, P.: A note on constructive methods in Banach algebras. Proc. Amer. Math.
Soc. 12, 159–163 (1961)
7. Coquand, T., Spitters, B.: Constructive theory of Banach algebras. Journal of Logic
and Analysis 2(11), 1–15 (2010)
8. Coquand, T., Stolzenberg, G.: The Wiener lemma and certain of its generalizations.
Bull. Amer. Math. Soc. (N.S.) 24, 1–9 (1991)
9. Gelfand, I.M., Raikov, D.A., Chilov, G.E.: Les anneaux norm´es commutatifs.
Gauthier-Villars, Paris (1964)
10. Lombardi, H., Quitt´e, C.: Alg`ebre commutative. M´ethodes constructives. Modules
projectifs de type ﬁni, Math´ematiques en devenir, vol. 107. Calvage & Mounet,
Paris (2012)
11. Newman, D.: A simple proof of Wiener’s 1/f theorem. Proc. Amer. Math. Soc. 48,
264–265 (1975)
12. Raoult, J.: Proving open properties by induction. Inform. Process. Letters 29, 19–23
(1988)
13. Schuster, P.: Induction in algebra: a ﬁrst case study. In: Twenty-Seventh Annual
ACM/IEEE Symposium on Logic in Computer Science, LICS 2012 (2012)
14. Troelstra, A.S., van Dalen, D.: Constructivism in mathematics: An introduc-
tion, Volume II. Studies in Logic and the Foundations of Mathematics, vol. 123.
North-Holland (1988)

Eﬀective Strong Nullness
and Eﬀectively Closed Sets
Kojiro Higuchi and Takayuki Kihara
Mathematical Institute, Tohoku University, Sendai 980-8578, Japan
sa7m24@math.tohoku.ac.jp
Abstract. The strongly null sets of reals have been widely studied in
the context of set theory of the real line. We introduce an eﬀectivization
of strong nullness. A set of reals is said to be eﬀectively strongly null if,
for any computable sequence {εn}n∈ω of positive rationals, a sequence
of intervals In of diameter εn covers the set. We show that for Π0
1 sub-
sets of 2ω eﬀective strong nullness is equivalent to another well studied
notion called diminutiveness: the property of not having a computably
perfect subset. In addition, we also investigate the Muchnik degrees of
eﬀectively strongly null Π0
1 subsets of 2ω. Let MLR and DNC be the sets
of all Martin-L¨of random reals and diagonally noncomputable functions,
respectively. We prove that neither the Muchnik degree of MLR nor that
of DNC is comparable with the Muchnik degree of a nonempty eﬀectively
strongly null Π0
1 subsets of 2ω with no computable element.
1
Introduction
1.1
Background
Miniaturization of set-theoretic notions is sometimes useful in computability
theory. For example, set-theoretic forcing is transformed into a notion called
arithmetical forcing and n-generic reals, and it has become a fundamental tool
in computability theory. There is another set-theoretical notion that we expect
its miniaturization to play an important role. The notion is known as strong
nullness which is introduced by Borel in 1919. Careful consideration on mea-
sure theoretic behavior of sets of reals has profound signiﬁcance in the study
on algorithmic randomness [8,13]. Binns [2,3,4] made a deep study on notions
stronger than being measure zero/Hausdorﬀdimension zero, and clariﬁed an in-
teresting connection among such measure theoretic smallness, Muchnik degrees,
and Kolmogorov complexity.
Inspired by these previous works, we introduce an eﬀectivization of strong
nullness. In contrast to Laver’s model [12] of ZFC in which all strongly mea-
sure zero sets are countable, we always have an eﬀectively strongly null set of
reals which is uncountable and Π0
1 deﬁnable. Indeed, we show that, for closed
subsets of reals, eﬀective strong nullness is equivalent to another well studied
notion called diminutiveness [4]. In addition, we also investigate the Muchnik
degrees of eﬀectively strongly null Π0
1 subsets of 2ω. Let MLR and DNC be the
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 303–312, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

304
K. Higuchi and T. Kihara
sets of all Martin-L¨of random reals and diagonally noncomputable functions, re-
spectively. We prove that neither the Muchnik degree of MLR nor that of DNC
is comparable with the Muchnik degree of a nonempty eﬀectively strongly null
Π0
1 subsets of 2ω with no computable element. Finally, we see some interactions
between measure theoretic smallness and Kolmogorov complexity. A real x ∈2ω
is K-trivial if K(x ↾n) ≤K(n) + O(1), and x is complex if K(x ↾f(n)) ≥n
for some computable function f, where K denotes the preﬁx-free Kolmogorov
complexity. By using a non-basis result for small Π0
1 sets, we construct a perfect
Π0
1 subset of 2ω whose elements are neither K-trivial nor complex.
1.2
Notation
Let ω = {0, 1, 2, · · ·} denote the set of all natural numbers; ωω = {f | f : ω →
ω}, Baire space; 2ω = {f | f : ω →{0, 1}}, Cantor space; ω<ω, the set of all
ﬁnite strings of natural numbers; and 2<ω, the set of all ﬁnite binary strings.
We deﬁne ω≤ω = ω<ω ∪ωω and 2≤ω = 2<ω ∪2ω. We use ∅to denote the empty
string or the empty set. For a set A, we use #A to denote the cardinal number
of A. For σ, τ ∈ω<ω and ρ, ρ′ ∈ω≤ω, we use σ ⊂ρ to mean that σ is an initial
segment of ρ, i.e., ρ extends σ; σ | τ to mean that σ and τ are incomparable,
i.e., neither σ ⊂τ nor σ ⊃τ; σρ or σ⌢ρ to denote the concatenation of σ and
ρ, i.e., the string σ followed by ρ; lh(ρ) and |ρ| to denote the length of ρ, i.e.,
the cardinal number of the domain of ρ; ρ ↾n to denote the initial segment of
ρ of the length n for any n ≤lh(ρ); ρ ∩ρ′ to denote the longest common initial
segment of ρ and ρ′; ρ ⊕ρ′ to denote the string ρ′′ with ρ′′(2n) = ρ(n) and
ρ′′(2n + 1) = ρ′(n), when lh(ρ) = lh(ρ′); [[σ]] to denote the set {f ∈ωω | σ ⊂f}
or the set {f ∈2ω | σ ⊂f} depending on the context. We often identify a natural
number n with the string ⟨n⟩of the length 1. Let A ⊂ω<ω and P, Q ⊂ωω. [[A]]
denotes the set 
σ∈A[[σ]]; [A], the set {f ∈ωω | (∀n ∈ω)[f ↾n ∈A]}; Ext(P),
the set {σ | [[σ]] ∩P ̸= ∅}; Br(P), the set {σ ∩τ | σ, τ ∈Ext(P) & σ | τ}; Brl(P),
the set {lh(σ) | σ ∈Br(P)}; Ext(A), Br(A) and Brl(A) denote the sets Ext([A]),
Br([A]) and Brl([A]), respectively; P ×Q denotes the set {f ⊕g | f ∈P & g ∈Q};
and P + Q, the set {0f, 1g | f ∈P & g ∈Q}. A set T ⊂ω<ω is called a tree if T
is closed under taking initial segments, i.e., τ ∈T if τ ⊂σ for some σ ∈T . For a
tree T , σ ∈T is an immediate successor of τ in T if τ ⊂σ and lh(σ) = lh(τ)+ 1;
T is ﬁnitely branching if every element in T has at most ﬁnitely many immediate
successors.
Let P, Q ⊂ωω. P is strongly reducible to Q, denoted by P ≤s Q, if there is a
computable function Φ : Q →P; P is strongly comparable with Q if P ≤s Q or
P ≥s Q; otherwise, strongly incomparable; P is strongly equivalent to Q, denoted
by P ≡s Q, if P ≤s Q and P ≥s Q. The strong degree of P is the equivalence
class of P under the equivalence relation ≡s. P is weakly reducible to Q, denoted
by P ≤w Q, if P ≤s {g} for all g ∈Q. Weak comparability, weak incomparability,
weak equivalence and weak degree are deﬁned in the same way. The arithmetical
hierarchy is introduced in the usual way.

Eﬀective Strong Nullness and Eﬀectively Closed Sets
305
2
Eﬀective Strong Nullness
2.1
Combinatorial Theorem
We ﬁrst show that a combinatorial theorem. While we use the theorem to char-
acterize eﬀective strong nullness we deﬁne later, the theorem itself is interesting.
Theorem 1. Let T ⊂ω<ω be a ﬁnitely branching tree. Suppose that [T ]\[[A]] ̸= ∅
holds for any A ⊂T \ {∅} such that
(∀n ∈ω)[#{σ ∈A | lh(σ) = n + 1} ≤#{τ ∈T | lh(τ) = n}].
Then there exists a length-preserving embedding from 2<ω into T .
Proof. Let ϕ(T ′) denote the condition that [T ′] \ [[A]] ̸= ∅holds for any A ⊂
T ′ \ {∅} such that
(∀n ∈ω)[#{σ ∈A | lh(σ) = n + 1} ≤#{τ ∈T ′ | lh(τ) = n}].
For σ ∈T , we deﬁne T (σ) = {τ ∈ω<ω | στ ∈T }. It suﬃces to show that for
any σ ∈T with ϕ(T (σ)) there exist at least two immediate successors σi, σj of
σ in T with ϕ(T (σi)) and ϕ(T (σj)).
Fix σ ∈T with ϕ(T (σ)). Let σk0, σk1, · · · , σkn ∈T be all immediate suc-
cessors of σ in T with k0 < k1 < · · · < kn. Suppose that there is at most one
immediate successor of σ in T with the property ϕ. Let i ≤n satisfy ϕ(σki) if
there is such a natural number. For any j ∈{0, 1, · · · , n} \ {i}, choose Aj wit-
nessing ¬ϕ(T (σkj)). It is easy to see that {ki} ∪
j̸
=i{kjτ | τ ∈Aj} witnesses
that ¬ϕ(T (σ)). We have a contradition. Thus there exist at least two immediate
successors of σ in T with the property ϕ.
⊓⊔
2.2
Eﬀective Strong Nullness and Diminutiveness
Now we turn to deﬁne eﬀective strong nullness and diminutiveness.
Deﬁnition 1. A set M ⊂2ω is called eﬀectively strongly null if for any com-
putable sequence {ki}i∈ω of natural numbers there exist ﬁnite strings σi, i ∈ω,
of length ≥ki such that {[[σi]]}i∈ω is an open cover of M, i.e., M ⊂
i∈ω[[σi]]
holds.
Proposition 1. Let C be an eﬀectively strongly null closed subset of 2ω. Given
any computable sequence {ki}i∈ω of naturals, there exist ﬁnitely many ﬁnite
strings σi, i ≤n, of the length ki such that {[[σi]]}i≤n is an open cover of C.
Proof. It is trivial by the eﬀective strong nullness and the compactness of C.
⊓⊔
Recall that a perfect subset of 2ω means a nonempty closed set with no isolated
point.

306
K. Higuchi and T. Kihara
Deﬁnition 2. A perfect set P ⊂2ω is said to be computably perfect if there
exists a computable function F : ω →ω such that
(∀n ∈ω)(∀f ∈P)(∃g ∈P)[n ≤lh(f ∩g) ≤F(n)].
A subset of 2ω is said to be diminutive if it contains no computably perfect subset.
Proposition 2 (Binns [4, Lemma 2.4]). If a Π0
1 subset of 2ω contains a
computably perfect subset, then it contains a computably perfect Π0
1 subset.
⊓⊔
Observe that a perfect set P is computably perfect if and only if there exists a
computable function F : ω →ω such that for any n ∈ω and any f ∈P there
exists an element g ∈P such that F(n) ≤lh(f ∩g) < F(n + 1) holds. Using this
equivalence, the following proposition is easily proved.
Proposition 3. Every computably perfect subset of 2ω is not eﬀectively strongly
null.
Proof. Let P be a computably perfect subset of 2ω. Choose a computable func-
tion F : ω →ω such that
(∀n ∈ω)(∀f ∈P)(∃g ∈P)[F(n) ≤lh(f ∩g) < F(n + 1)].
Now it is clear that {F(n+1)}n∈ω witnesses P is not eﬀectively strongly null.
⊓⊔
Corollary 1. Every eﬀectively strongly null subset of 2ω is diminutive.
⊓⊔
Using Theorem 1, we show the converse also holds for closed subsets of 2ω.
Theorem 2. A closed subset of 2ω is diminutive if and only if it is eﬀectively
strongly null.
Proof. Fix a closed set C ⊂2ω. Suppose that a computable sequence {ki}i∈ω
witnesses C is not eﬀectively strongly null. We show that C contains a com-
putably perfect subset. We may safely assume that ki < ki+1 for all i ∈ω.
Deﬁne F : ω →ω recursively by F(0) = 0 and F(n + 1) = F(n) + 2kF (n) for all
n ∈ω. Note that kF (n) ≥n and kF (n+1) −kF (n) ≥2kF (n) for all n ∈ω. Deﬁne
{Tn}n∈ω by T0 = {∅} and
Tn+1 = {σ ∈Ext(C) | lh(σ) = kF (n+1)}.
Let T = 
n∈ω Tn. Since C is closed, note that C = 
n∈ω[[Tn]]. The eﬀective
strong nullness of C implies that C \ [[A]] ̸= ∅holds if A ⊂T \ {∅} satisﬁes that
#{σ ∈A | lh(σ) = kF (n+1)} ≤#{τ ∈Ext(C) | lh(τ) = kF (n)} ≤2kF (n)
for all n ∈ω. Naturally, (T, ⊂) can be seen as a graph of a ﬁnitely branching
tree and can be embedding into ω<ω so that the image form a ﬁnitely branching
tree on ω with the assumption of Theorem 1. Thus 2<ω has a length-preserving
embedding into (T, ⊂) by Theorem 1. This implies that C has a computably
perfect subset witnessed via n →kF (n+1).
⊓⊔

Eﬀective Strong Nullness and Eﬀectively Closed Sets
307
2.3
Lattice Operators
Theorem 2 is very useful for investigating the properties of eﬀective strong null-
ness. We ﬁrst see relation between eﬀective strong nullness and lattice operators.
Proposition 4. Let P and Q be closed subsets of 2ω. Then P + Q is eﬀectively
strongly null if and only if so are P and Q.
Proof. This is because P +Q contains a computably perfect subset if and only if
one of P and Q contains a computably perfect subset by the deﬁnition of +.
⊓⊔
Proposition 5. Let P and Q be nonempty closed subsets of 2ω. If P × Q is
eﬀectively strongly null, then so are P and Q.
Proof. It is clear that if one of P and Q contains a computably perfect subset,
then so P × Q does.
⊓⊔
To show the converse of the above proposition, we need the following lemma.
Lemma 1. Let P be an eﬀectively strongly null Π0
1 subset of 2ω. Given a com-
putable sequence {ai}i∈ω of naturals, we can (uniformly) ﬁnd a computable in-
creasing function F : ω →ω and a computable sequence of ﬁnite strings σi,
i ∈ω, of the length ai such that [[σF (n)]], [[σF (n)+1]], · · · , [[σF (n+1)−1]] are an open
cover of P.
Proof. By Proposition 1, we know that for any k ∈ω there are ﬁnitely many
ﬁnite strings σk+i, i ≤m, of the length ak+i such that {[[σk+i]]}i≤m is an open
cover of P. Moreover, we can ﬁnd such sequences uniformly in a given k ∈ω
since P is Π0
1. From this, it is clear that the lemma holds.
⊓⊔
Theorem 3. If P and Q are eﬀectively strongly null Π0
1 subsets of 2ω, then
P × Q is also eﬀectively strongly null.
Proof. To show that P ×Q is eﬀectively strongly null, ﬁx a computable sequence
{bi}i∈ω of naturals. Let {ai}i∈ω be a strictly increasing computable sequence of
natural numbers such that bi ≤2ai for all i ∈ω and, applying Lemma 1 to P and
{ai}i∈ω, take a computable function F and a computable sequence {σi}i∈ω as in
Lemma 1. Here we can safely assume that F(0) = 0. Since Q is also eﬀectively
strongly null, there exist ﬁnite strings τn, n ∈ω, of the length aF (n+1) which
generate an open cover of Q. For each i ∈ω, deﬁne ρi = σi ⊕(τni ↾lh(σi)),
where ni is the unique natural number such that F(ni) ≤i < F(ni + 1). Since
lh(σ) = ai, we have lh(ρi) = 2ai for all i ∈ω.
It suﬃces to show that {ρi}i∈ω generates an open cover of P × Q. Fix f ⊕g ∈
P × Q. Since g ∈Q, there exists n ∈ω such that τn ⊂g. By the choice of ﬁnite
strings σi, F(n) ≤i < F(n + 1), there exists m with F(n) ≤m < F(n + 1) such
that σm ⊂f. We have ρm ⊂f ⊕g and, thus, P × Q ⊂
i∈ω[[ρi]].
⊓⊔
Corollary 2. For nonempty Π0
1 subsets of 2ω, the following three statements
are pairwise equivalent:
1. P and Q are eﬀectively strongly null.
2. P + Q is eﬀectively strongly null.
3. P × Q is eﬀectively strongly null.
⊓⊔

308
K. Higuchi and T. Kihara
2.4
Closure Properties
We shall see that eﬀective strong nullness is closed under taking subsets and is
closed under taking the images of computable functions for Π0
1 subsets of 2ω.
Proposition 6. Every subset of an eﬀectively strongly null subset of 2ω is again
eﬀectively strongly null.
⊓⊔
For a partial computable function Φ on 2ω, a ﬁnite binary string σ and a natural
number n, we use Φ(σ; n) to denote the computation of Φ with an oracle σ,
an input n and step lh(σ) and we use Φ(σ) to denote the ﬁnite string τ of the
maximum length such that Φ(σ; n) = τ(n) for all n < lh(τ).
Theorem 4. The image Φ(P) of an eﬀectively strongly null Π0
1 subset P of 2ω
under a computable function Φ : P →2ω is again eﬀectively strongly null.
Proof. Fix an eﬀectively strongly null Π0
1 subset P of 2ω and a computable func-
tion Φ : P →2ω, and assume, contrary to our theorem, Φ(P) is not eﬀectively
strongly null. Let a computable sequence {ki}i∈ω of naturals be a witness of
this assumption. Since P is Π0
1, we can ﬁnd a computable sequence {k′
i}i∈ω of
naturals such that lh(σ) ≥k′
i implies lh(Φ(σ)) ≥ki for all σ ∈Ext(P). Using
the eﬀective strong nullness of P, choose a sequence of ﬁnite strings σi of length
k′
i such that P ⊂
i∈ω[[σi]]. We have an open cover {[[Φ(σi)]]}i∈ω of Φ(P), con-
tradicting our assumption that {ki}i∈ω witnesses that Φ(P) is not eﬀectively
strongly null. Thus Φ(P) is eﬀectively strongly null.
⊓⊔
A set M of ωω is called computably bounded (c.b.) if there is a computable
function f : ω →ω with g(n) < f(n) for any g ∈M and n ∈ω. It is well-known
that every c.b. Π0
1 set is computably homeomorphic to a Π0
1 subset of 2ω.
Remark 1. We can extend Deﬁnition 1 and Deﬁnition 2 to c.b. closed subsets
of ωω in the straightforward way. Also, Theorem 2, Corollary 2, Proposition 6
and Theorem 4 can be easily extended to c.b. closed subsets of ωω. We shall use
these extended theorems later.
Theorem 5 (Simpson [14, Theorem 4.7]). Let P ⊂ωω be a nonempty c.b.
Π0
1 set and let Φ : P →ωω be a computable function. Then Φ(P) is a nonempty
c.b. Π0
1 subset of ωω.
⊓⊔
Theorem 6 (Simpson [14, Lemma 6.9]). Let M ⊂ωω and let P ⊂ωω be a
nonempty c.b. Π0
1 set. If M ≤w P, then P contains a nonempty c.b. Π0
1 subset
Q with M ≤s Q.
⊓⊔
We prove the following theorem using the technique of the proof of Corollary
2.16 in Binns [2].
Theorem 7. Let A be a set of nonempty c.b. Π0
1 subsets of ωω which is closed
under taking nonempty Π0
1 subset and taking the images of computable functions.
Let P ⊂ωω and let Q ∈A. If P ≤w Q, then some subset of P is in A.

Eﬀective Strong Nullness and Eﬀectively Closed Sets
309
Proof. Suppose that P ≤w Q. By Theorem 6, there exists a computable function
Φ : Q′ →P for some nonempty Π0
1 subset Q′ of Q. The image Φ(Q′) is a
nonempty c.b. Π0
1 subset of P by Theorem 5. We have Φ(Q′) ∈A by the closure
properties of A.
⊓⊔
Applying the theorem to A as the set of all nonempty Π0
1 eﬀectively strongly
null subsets of 2ω, we have the following corollary.
Corollary 3. If a subset P of 2ω is weakly reducible to a nonempty eﬀectively
strongly null Π0
1 subset of 2ω, then P contains a nonempty eﬀectively strongly
null Π0
1 subset.
⊓⊔
2.5
MLR and DNC
We denote MLR the set of all Martin-L¨of random elements of 2ω and denote DNC
the set of all diagonally non-computable elements of ωω. We refer to the texts by
Downey and Hirschfeldt [8], Nies [13] and Soare [15] for the deﬁnitions of these
notions. Simpson [14] proved that MLR is weakly incomparable with any perfect
thin Π0
1 subset of 2ω. We use the technique of his proof to show that MLR, DNC
are incomparable with any nonempty eﬀectively strongly null Π0
1 subset of 2ω
with no computable element. We use the facts that every nonempty Π0
1 subset
of MLR is weakly equivalent to MLR and that MLR contains a nonempty Π0
1
subset. See [14].
Theorem 8. Let P ⊂MLR be a nonempty Π0
1 set and let Φ : P →ωω be a
computable function. If Φ(P) contains no computable element, then Φ(P) ≡w P.
Proof. By Simpson [14, Corollary 4.9], we know that Φ(f) ≤tt f for all f ∈
P, where ≤tt refers to the truth-table reducibility. Additionally, by Demuth [6,
Lemma 30], we know that, for any f ∈P, Φ(f) is Turing equivalent to an element
of MLR. Thus MLR ≤w Φ(P) ≤w P ≡w MLR holds. Hence Φ(P) ≡w P.
⊓⊔
Theorem 9. Let A be a set of nonempty c.b. Π0
1 subsets of ωω which is closed
under taking nonempty Π0
1 subset and taking the images of computable functions.
Let P ⊂ωω be weakly reducible to MLR and let Q ∈A contain no computable
element. Suppose that every c.b. Π0
1 subset of P is not in A. Then P and Q are
weakly incomparable.
Proof. Since P ≤w Q implies that P contains a nonempty Π0
1 subset in A by
Theorem 7, we have P ̸≤w Q. Suppose that Q ≤w P. Since P ≤w MLR, we
have Q ≤w MLR. Choose a nonempty Π0
1 set R ⊂MLR and a computable
function Φ : R →Q. By Theorem 6 and Theorem 8, we have R ≡w Φ(R). By
the closure properties of A, we have Φ(R) ∈A. On the other hand, we have
P ≤w R ≡w Φ(R). A contradiction. Thus Q ̸≤w P.
⊓⊔
Proposition 7. Every nonempty Π0
1 subset of MLR contains a computably per-
fect subset.

310
K. Higuchi and T. Kihara
Proof. By Simpson [14, Lemma 8.9], every nonempty Π0
1 subset of MLR is of
positive measure. By Hertling [10, Proposition 8], we know that any closed sub-
set of 2ω of positive measure contains a computably perfect subset. Thus the
proposition holds.
⊓⊔
Applying Theorem to P = MLR and A as the set of all nonempty c.b. Π0
1
eﬀectively strongly null subsets of ωω, we have the following corollary.
Corollary 4. Let Q be an eﬀectively strongly null Π0
1 subset of 2ω with no
computable element. Then Q is weakly incomparable with MLR.
⊓⊔
Proposition 8. Every nonempty c.b. Π0
1 subset of DNC is computably perfect.
Proof. Let P ⊂DNC be a nonempty c.b. Π0
1 set. Using Recursion Theorem,
given a σ ∈2<ω, we can eﬀectively ﬁnd nσ such that {nσ}(nσ) is the unique
value of f(nσ) for some s ∈ω and some f ∈Ps ∩[[σ]] (if exist) such that,
for any f, g ∈Ps ∩[[σ]], f(nσ) = g(nσ), where Ps is a clopen set which is the
s-th approximation of P. Then the computable function m →max{nσ | σ ∈
2<ω & lh(σ) = m} witnesses that P is computably perfect.
⊓⊔
It is well-known that DNC ≤w MLR. See, for instance, Giusto/Simpson [9,
Lemma 6.18]. Thus applying the theorem to P = DNC and A as the set of all
nonempty c.b. Π0
1 eﬀectively strongly null subsets of ωω, we have the following
corollary.
Corollary 5. Let Q be an eﬀectively strongly null Π0
1 subset of 2ω with no
computable element. Then Q is weakly incomparable with DNC.
⊓⊔
Remark 2. Indeed, one direction of Corollary 5, i.e., DNC is weakly reducible to
no diminutive Π0
1 subset of 2ω, can be obtained easily using Theorem 2.12 and
Corollary 2.14 of Binns [4].
Remark 3. By Binns [4, Theorem 3.8, Theorem 3.9] and Binns [3], we have that
thinness or smallness imply diminutiveness. Thus Corollary 4 and Corollary 5
hold even when we replace “eﬀectively strongly null” with “thin” or “small”.
Here, Simpson [14, Theorem 9.15] showed that MLR is weakly incomarable with
any nonempty thin perfect Π0
1 subset of 2ω.
3
Kolmogorov Complexity, and K-Triviality
A real x ∈2ω is complex [11] if there is a computable function f : ω →ω such
that K(x ↾f(n)) ≥n for any n ∈ω, where K : 2<ω →ω denotes the preﬁx-free
Kolmogorov complexity. By combining Theorem 2 and a result from Binns [4,
Theorem 2.13], we have the following characterization:

Eﬀective Strong Nullness and Eﬀectively Closed Sets
311
Corollary 6. The following are pairwise equivalent for any Π0
1 set P ⊆2ω:
1. P is eﬀectively strongly null;
2. No element of P is complex.
3. There exists a real x ∈2ω such that no element of P wtt-computes x.
The previous works on measure theoretic smallness of Π0
1 sets implies the fol-
lowing non-basis theorem:
Theorem 10 (Small Non-Basis Theorem). For any Π0
1 set P ⊆2ω, if
Brl(P) is not dominated by any computable function, then every element of P
is neither complex nor computable in any 1-generic real.
Proof. Any such Π0
1 set is said to be small ([2]). By Binns [3,4], every small
Π0
1 set is diminutive. By Cenzer/Kihara/Weber/Wu [5], every small Π0
1 set is
immune, i.e., there exists no inﬁnite c.e. set of proper initial segments of elements
of the set. By Binns [4, Theorem 2.13], every element of a diminutive Π0
1 set is
non-complex. By Demuth/Kuˇcera [7], every 1-generic real computes no element
of an immune Π0
1 set.
⊓⊔
The Small Non-Basis Theorem 10 may have some applications. A real x ∈2ω
is K-trivial if there is a constant c ∈ω such that K(x ↾n) ≤K(n) + c for any
n ∈ω; A real x ∈2ω is i.o. K-trivial [1] if there is a constant c ∈ω such that
K(x ↾n) ≤K(n) + c for inﬁnitely many n ∈ω. Barmpalias/Vlek [1] showed
that, if a real is computable in a 1-generic, then it is i.o. K-trivial; and there is
a Π0
1 subset of 2ω consisting of i.o. K-trivial reals but which does not contain
any K-trivial reals.
Theorem 11. There is a Π0
1 set P ⊆2ω which satisﬁes the following:
1. Every element of P is i.o. K-trivial,
2. P contains inﬁnitely many complex reals,
3. No element of P is K-trivial.
4. No element of P is computable in a 1-generic real.
Proof (Sketch). Our idea is to add a strategy to the construction in Barmpalias-
Vlek [1, Theorem 2.10] to ensure immunity for Π0
1 sets.
⊓⊔
Theorem 12. There is a nonempty (perfect) Π0
1 subset of 2ω which satisﬁes
the following conditions:
1. No element of P is complex,
2. No element of P is K-trivial,
3. No element of P is computable in a 1-generic real.
Proof (Sketch). Our idea is to add a strategy to the construction in Barmpalias-
Vlek [1, Theorem 2.10] to ensure smallness for Π0
1 sets, via ﬁnite injury.

312
K. Higuchi and T. Kihara
Requirements. We need to construct a nonempty Π0
1 set P ⊆2ω satisfying the
following K-trivial avoiding requirements {Kc}c∈ω and smallness requirements
{Se}e∈ω:
Kc : (∃h⋆∈ω)(∀f ∈P) K(f ↾h⋆) > 2 log h⋆+ c,
Se : Φe total unbounded =⇒(∃n) [Φe(n), Φe(n + 1)] ∩Brl(P) = ∅.
Here, {Φe}e∈ω is an eﬀective enumeration of all partial computable function, and
[l, r] denotes the interval {m : l ≤m ≤r}.
The K-strategies ensure that no element of P is K-trivial; The S-strategies
ensure that P is small, by Binns [3, Theorem 2.10]. By Small Non-Basis
Theorem 10, every element of P is neither complex nor computable in a 1-
generic real, as desired.
⊓⊔
Acknowledgements. The authors were supported by JSPS Research Fellow-
ships.
References
1. Barmpalias, G., Vlek, C.S.: Kolmogorov complexity of initial segments of sequences
and arithmetical deﬁnability. Theor. Comput. Sci. 412(41), 5656–5667 (2011)
2. Binns, S.: Small Π0
1 classes. Arch. Math. Log. 45(4), 393–410 (2006)
3. Binns, S.: Hyperimmunity in 2N. Notre Dame Journal of Formal Logic 48(2), 293–
316 (2007)
4. Binns, S.: Π0
1 classes with complex elements. J. Symb. Log. 73(4), 1341–1353 (2008)
5. Cenzer, D., Kihara, T., Weber, R., Wu, G.: Immunity and non-cupping for closed
sets. Tbilisi Math. J. 2, 77–94 (2009)
6. Demuth, O.: A notion of semigenericity. Comment. Math. Univ. Carolinae 28, 71–
84 (1987)
7. Demuth, O., Kuˇcera, A.: Remarks on 1-genericity, semigenericity and related con-
cepts. Comment. Math. Univ. Carolinae 28, 85–94 (1987)
8. Downey, R.G., Hirschfeldt, D.R.: Algorithmic randomness and complexity. Theory
and Applications of Computability, 883 pages. Springer (2010)
9. Giusto, M., Simpson, S.G.: Located sets and reverse mathematics. J. Symb.
Log. 65(3), 1451–1480 (2000)
10. Hertling, P.: Surjective functions on computably growing Cantor sets. J. UCS 3(11),
1226–1240 (1997)
11. Kjos-Hanssen, B., Merkle, W., Stephan, F.: Kolmogorov complexity and the recur-
sion theorem. Trans. Amer. Math. Soc. 363(10), 5465–5480 (2011)
12. Laver, R.: On the consistency of Borel’s conjecture. Acta Math. 137(1), 151–169
(1976)
13. Nies, A.: Computability and Randomness. Oxford Logic Guides. Oxford University
Press (2009)
14. Simpson, S.G.: Mass problems and randomness. Bull. Symb. Log. 11(1), 1–27
(2005)
15. Soare, R.I.: Recursively Enumerable Sets and Degrees. Perspectives in Mathemat-
ical Logic. Springer, Heidelberg (1987)

Word Automaticity of Tree Automatic Scattered
Linear Orderings Is Decidable
Martin Huschenbett
Fakult¨at Informatik und Automatisierung, Fachgebiet Theoretische Informatik,
Technische Universit¨at Ilmenau, Postfach 100565, 98684 Ilmenau, Germany
martin.huschenbett@tu-ilmenau.de
Abstract A tree automatic structure is a structure whose domain can
be encoded by a regular tree language such that each relation is recog-
nisable by a ﬁnite automaton processing tuples of trees synchronously.
Words can be regarded as speciﬁc simple trees and a structure is word
automatic if it is encodable using only these trees. The question nat-
urally arises whether a given tree automatic structure is already word
automatic. We prove that this problem is decidable for tree automatic
scattered linear orderings. Moreover, we show that in case of a posit-
ive answer a word automatic presentation is computable from the tree
automatic presentation.
1
Introduction
The fundamental idea of automatic structures can be traced back to the 1960s
when B¨uchi, Elgot, Rabin, and others used ﬁnite automata to provide decision
procedures for the ﬁrst-order theory of Presburger arithmetic (N; +) and several
other logical problems. Hodgson generalised this idea to the concept of auto-
maton decidable ﬁrst-order theories. Independently of Hodgson and inspired by
the successful employment of ﬁnite automata and their methods in group theory,
Khoussainov and Nerode [4] initiated the systematic investigation of automatic
structures. Recalling the eﬀorts from the 1960s, Blumensath [2] extended this
concept notion beyond ﬁnite automata to ﬁnite automaton models recognising
inﬁnite words, ﬁnite trees, or inﬁnite trees.
Basically, a countable relational structure is tree automatic or tree automatic-
ally presentable if its elements can be encoded by ﬁnite trees in such a way that
its domain and its relations are recognisable by ﬁnite automata processing either
single trees or tuples of trees synchronously. A structure is word automatic if its
elements can be encoded using only speciﬁc simple trees which eﬀectively rep-
resent words. In contrast to the more general concept of computable structures
and based on the strong closure properties of recognisability, automatic struc-
tures provide pleasant algorithmic features. In particular, they possess decidable
ﬁrst-order theories.
Due to this latter fact, the concept of automatic structures gained a lot atten-
tion which led to noticeable progress (cf. [1,6]). Automatic presentations were
found for many structures, some structures where shown to be tree but not word
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 313–322, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

314
M. Huschenbett
automatic, for instance Skolem arithmetic (N; ×), whereas other structures, like
the random graph, were proven to be neither word nor tree automatic. For some
classes of structures it was even possible to characterise its automatic members,
for example an ordinal is word automatic respectively tree automatic precisely
if it is less than ωω respectively ωωω. Certain extensions of ﬁrst-order logic were
shown to preserve decidability of the corresponding theory. The question whether
two automatic structures are isomorphic turned out to be highly undecidable in
general as well as for some restricted classes of structures. At the same time, the
isomorphism problem for word automatic ordinals was proven to be decidable.
Last but not least, the diﬀerent classes of automatic structures was characterised
by means of interpretations in universal structures.
Due to the fact that word automaticity is a special case of tree automaticity,
the question naturally arises whether a given tree automatic structure is already
word automatic. As far as we know, this problem was neither solved in general
nor for any restricted class of structures. For that reason, we investigate the
respective question for scattered linear orderings in this paper. Actually, we prove
the corresponding problem to be decidable and our main result is as follows:
Theorem 1.1. Given a tree automatic presentation P of a scattered linear or-
dering L, it is decidable whether L is word automatic. In case L is word auto-
matic, one can compute a word automatic presentation of L from P.
Since every well-ordering is scattered, this result still holds if L is assumed to
be an ordinal. The proof of Theorem 1.1 splits into three parts. First, we intro-
duce the notion of slim tree languages and prove this property to be decidable
(Theorem 3.2). Second, we show that a slim domain is suﬃcient for a tree auto-
matic structure to be word automatic (Theorem 4.1). Last, we demonstrate that
this condition is also necessary in case of scattered linear orderings (Theorem 5.1).
Altogether, Theorem 1.1 follows from the three mentioned theorems.
2
Background
In this section we recall the necessary notions of logic, automatic structures
(cf. [1,6]), tree automata (cf. [3]), and linear orderings. We agree that the natural
numbers N include 0 and that [m, n] = {m, m + 1, . . . , n} ⊆N for all m, n ∈N.
Logic. A (relational) signature τ = (R, ar) is a ﬁnite set R of relation symbols
together with a map ar: R →N assigning to each R ∈R its arity ar(R) ≥1.
A τ-structure A =

A; (RA)R∈R

consists of a set A = dom(A), its domain, and
an ar(R)-ary relation RA ⊆Aar(R) for each R ∈R. First order logic FOτ over
τ is deﬁned as usual, including an equality predicate. A sentence is a formula
without free variables. Writing ϕ(¯x) means that all free variables of the formula
ϕ are among the entries of the tuple ¯x = (x1, . . . , xn). The set ϕA consists of all
¯a ∈An satisfying A |= ϕ(¯a), where the latter is deﬁned as usual.

Word Automaticity of Tree Automatic Scattered Linear Orderings
315
Automatic Structures. The set of all (ﬁnite) words over an alphabet Σ is Σ⋆, the
empty word is ε, and the length of w is |w|. Subsets of Σ⋆are called languages
and L ⊆Σ⋆is regular if it can be recognised by some (non-deterministic) ﬁnite
automaton.
Let  ̸∈Σ be a new symbol and Σ = Σ ∪{}. For n ≥1 consider an n-tuple
¯w = (w1, . . . , wn) ∈(Σ⋆)n of words with wi = ai,1ai,2 . . . ai,mi for all i ∈[1, n].
Let m = max{m1, . . . , mn} and ai,j =  for j ∈[mi + 1, m]. The convolution
of ¯w is the word ⊗¯w = ¯a1 . . . ¯am ∈(Σn
)⋆with ¯aj = (a1,j, . . . , an,j) ∈Σn

for all j ∈[1, m]. An n-ary relation R ⊆(Σ⋆)n is automatic if the language
⊗R ⊆(Σn
)⋆, which consists of all ⊗¯w with ¯w ∈R, is regular.
A τ-structure A with dom(A) ⊆Σ⋆is (word) automatic if dom(A) is regular
and RA is automatic for all R ∈R. A (word) automatic presentation of A is a
tuple

Adom; (AR)R∈R

of ﬁnite automata such that Adom recognises dom(A)
and AR recognises ⊗RA. Abusing notation, we call any structure B which is
isomorphic to some word automatic structure A also (word) automatic.
Tree Automata. A tree domain is a non-empty, ﬁnite, and preﬁx-closed subset
D ⊆{0, 1}⋆satisfying u0 ∈D iﬀu1 ∈D for all u ∈D. A tree over Σ is a map
t: D →Σ where dom(t) = D is a tree domain. The set of all trees is denoted by
TΣ and its subsets are called (tree) languages. For some t ∈TΣ and u ∈dom(t)
the subtree of t rooted at u is the tree t↾u ∈TΣ deﬁned by
dom(t↾u) = { v ∈{0, 1}⋆| uv ∈dom(t) }
and
(t↾u)(v) = t(uv) .
A (deterministic bottom-up) tree automaton A = (Q, ι, δ, F) over Σ consists of
a ﬁnite set Q of states, a start state function ι: Σ →Q, a transition function
δ: Σ × Q × Q →Q, and a set F ⊆Q of accepting states. For each t ∈TΣ a
state A(t) ∈Q is deﬁned recursively by A(t) = ι

t(ε)

if dom(t) = {ε} and
A(t) = δ

t(ε), A(t↾0), A(t↾1)

otherwise. The language recognised by A is the
set of all t ∈TΣ with A(t) ∈F. A language L ⊆TΣ is regular if it can be
recognised by some tree automaton.
The convolution of ¯t = (t1, . . . , tn) ∈(TΣ)n is the tree ⊗¯t ∈TΣn
 deﬁned
by dom(⊗¯t) = dom(t1) ∪· · · ∪dom(tn) and (⊗¯t)(u) =

t′
1(u), . . . , t′
n(u)

, where
t′
i(u) = ti(u) if u ∈dom(ti) and t′
i(u) =  otherwise. A relation R ⊆(TΣ)n is
automatic if the language ⊗R ⊆TΣn
 is regular.
Tree automatic structures and tree automatic presentations are deﬁned like
in the word automatic case, but based on trees and tree automata.
Linear Orderings. A linear ordering is a structure A =

A; <A
where <A is a
strict linear order relation on A. The ordering A is scattered if (Q; <) cannot be
embedded into A. Obviously, every well-ordering is scattered. For any two linear
orderings A and B we deﬁne another linear ordering A · B by dom(A · B) =
dom(A) × dom(B) and (a1, b1) <A·B (a2, b2) iﬀeither a1 <A a2 or a1 = a2 and
b1 <B b2. Finally, if A1 can be embedded into B1 and A2 into B2, then A1 · A2
can be embedded into B1 · B2.

316
M. Huschenbett
3
Slim and Fat Tree Languages
In this section, we introduce the notion of slim tree languages and show that it
is decidable whether the language recognised by a given tree automaton is slim.
Deﬁnition 3.1. The thickness (t) of a tree t ∈TΣ is the maximal number of
nodes on any level, i.e.,
(t) = max
 dom(t) ∩{0, 1}ℓ  ℓ≥0

∈N .
For every K ≥1 the set of all t ∈TΣ with (t) ≤K is denoted by TΣ,K.
A tree language L ⊆TΣ is slim if there exists some K ≥1 such that L ⊆TΣ,K,
otherwise L is fat.
A tree automaton A is reduced if for every state q of A there is a tree t ∈TΣ
with A(t) = q. For every tree automaton A one can compute a reduced tree
automaton which recognises the same language and has no more states than A.
Theorem 3.2. Given a reduced tree automaton A, it is decidable whether the
tree language L recognised by A is slim or fat. If L is slim, then L ⊆TΣ,2n−1,
where n is the number of states of A.
For the rest of this section we ﬁx a reduced tree automaton A = (Q, ι, δ, F). The
proof of Theorem 3.2 essentially depends on an inspection of the directed graph
GA = (Q, EA) with
(p, q) ∈EA
iﬀ
∃a ∈Σ, r ∈Q: δ(a, p, r) = q or δ(a, r, p) = q .
(1)
Clearly, this graph is computable from A. The lemma below is shown by applying
the idea of pumping to tree automata. Therein, the height h(t) of a tree t ∈TΣ
is the number
h(t) = max { |u| | u ∈dom(t) } ∈N .
Lemma 3.3. For every q ∈Q the following are equivalent:
(1) there are inﬁnitely many t ∈TΣ satisfying A(t) = q,
(2) there is a tree t ∈TΣ satisfying h(t) ≥n and A(t) = q, where n = |Q|,
(3) GA contains a cycle from which q is reachable.
An edge (p, q) ∈EA is special if in the deﬁnition of EA in Eq. (1) the state
r ∈Q can be chosen such that it satisﬁes the conditions of Lemma 3.3 (for r in
place of q). Since condition (3) is decidable, it is decidable whether an edge is
special. The key idea for proving Theorem 3.2 is stated by the following lemma:
Lemma 3.4. The following are equivalent:
(1) the tree language L recognised by A is fat,
(2) there is a tree t ∈L satisfying (t) > 2n−1, where n = |Q|,
(3) GA contains a cycle including a special edge and from which F is reachable.
The proof of this lemma works similar to the one of Lemma 3.3. Since condi-
tion (3) is decidable given A as input, Theorem 3.2 follows.

Word Automaticity of Tree Automatic Scattered Linear Orderings
317
4
Slim Tree Automatic Structures Are Word Automatic
This section is devoted to the proof of the following theorem:
Theorem 4.1. Let A be a tree automatic structure such that dom(A) is slim.
Then, A is already word automatic and one can compute a word automatic
presentation of A from a tree automatic presentation of A.
The idea of the proof is the following. Let K ≥1 be such that dom(A) ⊆TΣ,K.
We give an alphabet Σ and an injective map C : TΣ,K →Σ⋆, the encoding, such
that C(L) is regular for all regular L ⊆TΣ,K (Proposition 4.5) and C(R) is
automatic for all automatic relations R ⊆(TΣ,K)n (Proposition 4.6). Thus, the
structure C(A) is word automatic. A word automatic presentation of C(A) is
computable since both propositions are eﬀective and Theorem 3.2 allows for
computing a suitable K. Although it is possible to show both propositions using
automata, it is much more convenient to accomplish this by means of logic.
4.1
Monadic Second Order Logic
Monadic second order logic MSOτ extends FOτ by set variables, which range
over subsets of the domain and are denoted by capital letters, quantiﬁers for
these variables, and the formula “x ∈X” (cf. [7]). Let τ = (R, ar) and τ′ be two
signatures. An (MSO-)interpretation of a τ-structure A in a τ′-structure B is
a pair ⟨f, I⟩consisting of an injective map f : dom(A) →dom(B) and a tuple
I =

Δ; (ΦR)R∈R

of MSOτ ′-formulae with free FO-variables only such that
f

dom(A)

= ΔB and f

RA
= ΦB
R for each R ∈R. In fact, f induces an
isomorphism between A and I(B) =

ΔB; (ΦB
R )R∈R

. Replacing in an MSOτ-
formula ϕ(¯x) all symbols R ∈R with ΦR and relativising quantiﬁers to Δ yields
an MSOτ ′-formula ϕI(¯x) satisfying A |= ϕ(¯a) iﬀB |= ϕI
f(¯a)

for all ¯a ∈An.
For an alphabet Σ the signature WΣ consists of one binary relation symbol ≤
and a unary symbol Pa for each a ∈Σ. Every word w = a1a2 . . . a|w| ∈Σ⋆is
regarded as a WΣ-structure with domain dom(w) = {1, . . ., |w|}, ≤w being the
natural order on dom(w), and i ∈P w
a iﬀai = a. For ﬁxed numbers m, r ∈N,
relations like x = y + m and x ≡r (mod m) are expressible in MSOWΣ. The
language deﬁned by an MSOWΣ-sentence Φ is the set of all w ∈Σ⋆with w |= Φ.
The signature TΣ is similar to WΣ but contains two binary symbols S0 and
S1 instead of ≤. Each tree t ∈TΣ is considered as a TΣ-structure with domain
dom(t), (u, v) ∈St
d iﬀud = v (d = 0, 1), and u ∈P t
a iﬀt(u) = a. The language
deﬁned by some MSOTΣ-sentence Φ is the set of all t ∈TΣ with t |= Φ.
The following theorem holds for word languages as well as for tree languages:
Theorem 4.2 (cf. [7]). A language L is regular iﬀit is deﬁnable in MSO, and
both conversions, from automata to formulae and vice versa, are eﬀective.

318
M. Huschenbett
4.2
The Encoding and Preservation of Regularity
a
b
c
b
a c
c
b
a
Fig. 1. The tree tex
For the rest of this section ﬁx the K ≥1 from above. The
ﬁrst objective is to give the encoding C : TΣ,K →Σ⋆,
where $ is a new symbol and Σ = Σ × {0, 1} ∪{$}. For
a tree t ∈TΣ,K of height m = h(t) its encoding C(t) =
σ0σ1 . . . σm is made up of m + 1 blocks σ0, . . . , σm ∈ΣK
describing the individual levels of t. More speciﬁcally, σℓ
consists of the labels of the ℓ-th level from left to right,
each enriched by a bit stating whether the corresponding
node possesses children, and is padded up to length K by $ symbols. For ex-
ample, the tree tex ∈T{a,b,c} in Figure 1 on the right satisﬁes (tex) = 4 and is,
under the assumption K = 5, encoded by the word
C(tex) = ⟨a,1⟩$$$$ ⟨b,1⟩⟨c,1⟩$$$ ⟨c,0⟩⟨b,1⟩⟨b,0⟩⟨a,0⟩$ ⟨a,0⟩⟨c,0⟩$$$ .
Formally, for each ℓ∈[0, m] let uℓ,1, . . . , uℓ,sℓbe the lexicographic enumeration
(w.r.t. 0 < 1) of dom(t) ∩{0, 1}ℓ. For r ∈[1, sℓ] we let cℓ,r = 1 if uℓ,r is an inner
node, i.e., uℓ,r{0, 1} ⊆dom(t), and cℓ,r = 0 if uℓ,r is a leaf. Finally, we put
σℓ= ⟨t(uℓ,1), cℓ,1⟩⟨t(uℓ,2), cℓ,2⟩. . . ⟨t(uℓ,sℓ), cℓ,sℓ⟩$K−sℓ.
The main tool for studying the map C : TΣ,K →Σ⋆is the following lemma:
Lemma 4.3. For all t ∈TΣ,K there is an MSO-interpretation ⟨fC, IC⟩of t
in C(t) such that IC does not depend on t.
Proof. Observe that for each inner node u of t the children of u are the (2s−1)-th
and 2s-th node on the next level, where s is the number of inner nodes from left
up to u on its level. Formally, for an inner node uℓ,r we have uℓ,rd = uℓ+1,2s−1+d,
where d ∈{0, 1} and s = cℓ,1 + · · · + cℓ,r. Based on this observation, one can
give an interpretation ⟨fC, IC⟩of t in C(t) such that fC(uℓ,r) = ℓ· K + r.
⊓⊔
As a ﬁrst consequence, we obtain t ∼= IC(t) = IC(t′) ∼= t′, and hence t = t′, for
all t, t′ ∈TΣ,K with C(t) = C(t′), i.e., C is injective. The proof of the fact that
C preserves regularity is mainly based on Lemma 4.3 and Lemma 4.4 below.
Lemma 4.4. Let σ ∈Σ⋆. There exists a tree t ∈TΣ,K with C(t) = σ iﬀ
σ = σ0σ1 . . . σn for some n ≥0 and σ0, . . . , σn ∈ΣK satisfying (a) and (b):
(a) σℓ= αℓ,1 . . . αℓ,sℓ$K−sℓfor some sℓ≥1 and αℓ,1, . . . , αℓ,sℓ∈Σ × {0, 1} and
for each ℓ∈[0, n],
(b) s0 = 1, sℓ+1 = 2·(cℓ,1 +· · ·+cℓ,sℓ) for 0 ≤ℓ< n, and cm,1 +· · ·+cm,sm = 0,
where αℓ,r = ⟨aℓ,r, cℓ,r⟩.
Proof. To see that C(t) has the required shape, notice that (b) mainly reﬂects the
relationship between the numbers of nodes on two adjacent levels. Conversely, if
σ ∈Σ⋆is of the required shape, then there is a tree t ∈TΣ with t ∼= IC(σ) and
it turns out that (t) ≤K and C(t) = σ.

Word Automaticity of Tree Automatic Scattered Linear Orderings
319
Proposition 4.5. Let L ⊆TΣ,K be a regular language. Then, the language
C(L) ⊆Σ⋆is also regular and one can compute a ﬁnite automaton recognising
C(L) from a tree automaton recognising L.
Proof. Let ΓC be an MSOW 
Σ-sentence which expresses the requirement on the
shape of σ from Lemma 4.4. By Theorem 4.2, there is an MSOTΣ-sentence Φ
deﬁning L ⊆TΣ,K. Then, the MSOW 
Σ-sentence ΓC∧ΦIC deﬁnes C(L) and, again
by Theorem 4.2, this language is regular. Finally, all employed constructions are
eﬀective.
⊓⊔
4.3
Preservation of Automaticity
The purpose of this subsection is to complete the proof of Theorem 4.1.
Proposition 4.6. Let R ⊆(TΣ,K)n be an automatic relation. Then, the rela-
tion C(R) ⊆( Σ⋆)n is also automatic and one can compute a ﬁnite automaton
recognising ⊗C(R) from a tree automaton recognising ⊗R.
Basically, the key idea behind the proof is the same as for Proposition 4.5 though
it is more involved. Let ¯t = (t1, . . . , tn) ∈(TΣ,K)n. Due to cardinality reasons,
⊗¯t is commonly not directly interpretable in ⊗C(¯t) but only in an n-fold copy
of ⊗C(¯t). This is formalised by means of the injective monoid morphism
H : ( Σn
)⋆→( Σn
)⋆, ¯α1 . . . ¯αm →¯αn
1 . . . ¯αn
m .
The interpretation of ⊗¯t in H

⊗C(¯t)

embraces two aspects which are better
considered separately. Thus, we deﬁne an intermediate structure ⨿¯t which ex-
tends the disjoint union of the ti’s on domain dom(⨿¯t) = 
i∈[1,n]{i} × dom(ti)
by a binary relation L⨿¯t, relating all (i, u) and (j, v) with |u| = |v|, and unary
relations Q⨿¯t
i
= {i} × dom(ti) for each i ∈[1, n]. Altogether, we give several
interpretations whose formulae naturally do not depend on the speciﬁc choice
of ¯t. An overview of the whole setting is depicted in Figure 2.
⊗(t1, . . . , tn)
⨿(t1, . . . , ti, . . . , tn)
H

⊗

C(t1), . . . , C(tn)

C(ti)
⟨f⨿, I⨿⟩
⟨fH, IH⟩
⟨fC,i, IC⟩
⟨f⊗,i, I⊗,i⟩
Fig. 2. Interpretations involved in proving Proposition 4.6
The Interpretation ⟨f⨿, I⨿⟩. Standard techniques as the MSO-deﬁnability of
the transitive closure in combination with the relation L yield an MSO-formula
E(x, y) with ⨿¯t |= E

(i, u), (j, v)

iﬀu = v. Using this formula, one can construct
an interpretation ⟨f⨿, I⨿⟩of ⊗¯t in ⨿¯t such that f⨿(u) = (i, u), where i is minimal
with u ∈dom(ti).
The Interpretations ⟨f⊗,i, I⊗,i⟩. For all i ∈[1, n] and ¯w ∈( Σ⋆)n one can easily
give an interpretation ⟨f⊗,i, I⊗,i⟩of wi in H(⊗¯w) such that f⊗,i(p) = (p−1)·n+i.

320
M. Huschenbett
The Interpretation ⟨fH, IH⟩. For i ∈[1, n] let ⟨fC,i, IC⟩be the interpretation
of ti in C(ti) from Lemma 4.3. Then the map fH : dom(⨿¯t) →dom

H(⊗C(¯t))

with fH(i, u) = f⊗,i(fC,i(u)) is injective and one can construct formulae IH such
that ⟨fH, IH⟩is an interpretation of ⨿¯t in H

⊗C(¯t)

.
Proof (of Proposition 4.6). Let ΓH be an MSOW 
Σn
 -sentence deﬁning the lan-
guage H

⊗( Σ⋆)n
⊆( Σn
)⋆. If Φ deﬁnes ⊗R, then
ΓH ∧
	
i∈[1,n] Γ I⊗,i
C
∧(ΦI⨿)IH
deﬁnes H

⊗C(R)

. Since H is an injective monoid morphism, ⊗C(R) is regular
as well. Finally, all employed constructions are eﬀective.
⊓⊔
5
Fat Tree Automatic Ordinals Are Not Word Automatic
The goal of this section is to give the last missing piece for the proof of
Theorem 1.1, namely the following theorem:
Theorem 5.1. Let L be a tree automatic scattered linear ordering such that
dom(L) is fat. Then, L is not word automatic.1
The theorem below states the necessary condition on word automatic linear
orderings we use to show non-automaticity:
Theorem 5.2 (Khoussainov, Rubin, Stephan [5]). If L is a word automatic
linear ordering, then its FC-rank is ﬁnite.
Actually, we do not need any details on the FC-rank (ﬁnite condensation rank)
besides the fact that every scattered linear ordering L, having the property that
for each r ≥1 at least one linear ordering from
Nr =

A1 · A2 · · · Ar
 A1, . . . , Ar ∈

(N; <), (N; >)
 
can be embedded into L, has inﬁnite FC-rank. For any linear ordering A and all
a1, a2 ∈dom(A) we deﬁne cmpA(a1, a2) ∈{−1, 0, 1} to be −1 if a1 <A a2, 0 if
a1 = a2, and 1 if a2 <A a1. The main idea of the proof of Theorem 5.1 is given
by the following two lemmas:
Lemma 5.3. Let A and A1, . . . , Ar be inﬁnite linear orderings with dom(A) =
dom(A1) × · · · × dom(Ar) and satisfying the following two conditions:
(1) cmpA(¯a,¯b) is determined by cmpA1(a1, b1), . . . , cmpAr(ar, br) for all ¯a,¯b ∈A,
(2) if ¯a,¯b ∈A diﬀer only in the i-th component, then cmpA(¯a,¯b) = cmpAi(ai, bi).
Then, there exists a permutation π of {1, . . . , r} such that A is isomorphic to
Aπ(1) · Aπ(2) · · · Aπ(r).
1 This claim fails if we permit non-injective tree automatic presentations.

Word Automaticity of Tree Automatic Scattered Linear Orderings
321
Lemma 5.4. Let L = (L; <) be a tree automatic scattered linear ordering,
(A; A<) an automatic presentation of L, n the number of states of A, and r ≥1.
If there exists some tree t ∈L with (t) ≥r · 2n, then there are inﬁnite linear
orderings A1, . . . , Ar such that A1 · A2 · · · Ar can be embedded into L.
Proof (of Lemma 5.4). To simplify notation, we put s, t< = A<

⊗(s, t)

for all
s, t ∈TΣ. Moreover, we assume w.l.o.g. that from s, t< one can deduce whether
s = t holds true. Then, cmpL(s, t) is determined by s, t< for all s, t ∈L, i.e.,
there is a map f from the state set of A< to {−1, 0, 1} such that cmpL(s, t) =
f

s, t<

for all s, t ∈L.
Let T ∈L be a tree and ℓ≥n such that
dom(T) ∩{0, 1}ℓ ≥r · 2n. Thus,
there exist at least r mutually distinct u ∈dom(T)∩{0, 1}ℓ−n for which there is
a v ∈{0, 1}n with uv ∈dom(T), say u1, . . . , ur. For ¯t = (t1, . . . , tr) ∈(TΣ)r let
T[¯t] ∈TΣ be the tree obtained from T by replacing for each i ∈[1, r] the subtree
rooted at ui with ti. Then, A

T[¯t]

is determined by the r states A(t1), . . . , A(tr)
for all ¯t ∈(TΣ)r. Moreover, for ¯s ∈(TΣ)r the tree ⊗

T[¯s], T[¯t]

is obtained from
⊗(T, T) by replacing for each i ∈[1, r] the subtree rooted at ui with ⊗(si, ti).
Consequently,

T[¯s], T[¯t]

< is determined by the r states s1, t1<, . . . , sr, tr<
for all ¯s, ¯t ∈(TΣ)r.
Observe that the height h(T↾ui) is at least n for each i ∈[1, r]. Therefore,
by Lemma 3.3 and Ramsey’s theorem for inﬁnite, undirected, ﬁnitely coloured
graphs, there exists an inﬁnite set Ai ⊆TΣ of trees t ∈TΣ with A(t) = A(T↾ui)
such that
c(s, t) =

s, s<, t, t<, s, t<, t, s<

is the same set Qi for all distinct s, t ∈Ai. It turns out that Qi has exactly three
elements and s, s< = t, t< for all s, t ∈Ai.
Now, put A = A1 × · · · × Ar. For each ¯t ∈A we have A

T[¯t]

= A(T) and
hence T[¯t] ∈L. We deﬁne a linear ordering A =

A; <A
by ¯s <A ¯t iﬀT[¯s] < T[¯t].
By deﬁnition, A can be embedded into L.
For i ∈[1, r], ¯a ∈A, and t ∈Ai we let ¯ai/t ∈A be the tuple ¯a with
the i-th component replaced by t. Then, for all ¯a,¯b and s, t ∈Ai we obtain

T[¯ai/s], T[¯ai/t]

< =

T[¯bi/s], T[¯bi/t]

< and hence ai/s <A ai/t iﬀbi/s <A bi/t.
Thus, deﬁning a linear ordering Ai =

Ai; <Ai
by s <Ai t iﬀ¯ai/s <A ¯ai/t is
independent from the speciﬁc choice of ¯a ∈A. Clearly, cmpAi(s, t) is determined
by s, t< for all s, t ∈Ai. Since Qi contains exactly three elements, s, t< is
determined by cmpAi(s, t) for all s, t ∈Ai as well. Hence, the linear orderings
A and A1, . . . , Ar satisfy the condition of Lemma 5.3 below and consequently
Aπ(1) · · · Aπ(r) can be embedded into L.
⊓⊔
Finally, we are in a position to prove Theorem 5.1.
Proof (of Theorem 5.1). Let (A; A<) be an automatic presentation of L and n
the number of states of A. Since dom(L) is fat, for any r ≥1 there is a t ∈dom(L)
with (t) ≥r·2n. Let A1, . . . , Ar be the inﬁnite linear orderings from Lemma 5.4.
For each i ∈[1, r] some Bi ∈

(N; <), (N; >)

can be embedded into Ai. Then,
B1·B2 · · · Br ∈Nr can be embedded into A1·A2 · · · Ar and consequently into L.
Hence, L has inﬁnite FC-rank and is, by Theorem 5.2, not word automatic.
⊓⊔

322
M. Huschenbett
6
Conclusions
Altogether, we proved that is decidable whether a given tree automatic scattered
linear ordering is already word automatic. Taking a closer look at the proof
reveals that the problem is solvable nondeterministically in logarithmic space,
provided the tree automaton recognising the domain is reduced.
The restriction to scattered linear orderings naturally rises the question whether
this result holds true for general linear orderings. Unfortunately, this problem can-
not be solved by means of our technique since the ordering (Q; <) of the ration-
als admits a word automatic as well as a fat tree automatic presentation. As the
Boolean algebra of ﬁnite and co-ﬁnite subsets of N shares this feature, the same
pertains to an analogue of Theorem 1.1 for Boolean algebras. In spite of that, we
suggest trying to apply the technique to other classes of structures, such as groups
or trees, for which a necessary condition on its automatic members is known.
As a ﬁrst step, suppose that A is a tree automatic structure and ϕ a ﬁrst order
formula which deﬁnes a scattered linear order on A. If A is already word auto-
matic so is

dom(A); ϕA
and hence dom(A) is slim. Thus, A is word automatic
if, and only if, dom(A) is slim. Consequently, word automaticity is uniformly
decidable for the class of tree automatic structures which admit a ﬁrst order
deﬁnable scattered linear order. In particular, the decision procedure needs no
knowledge of the formula deﬁning the order.
Using similar arguments in combination with a technique and a result from [5],
one can further show that it is decidable whether a given tree automatic ﬁnitely
branching tree is already word automatic. However, for arbitrary, not necessarily
ﬁnitely branching trees this problem remains open.
Finally, Theorem 1.1 provides a decidable characterisation of all tree auto-
matic ordinals α ≥ωω. Finding such a characterisation for each ωωk with k ∈N
possibly turns out to be the main ingredient for showing that the isomorphism
problem for tree automatic ordinals is decidable.
References
1. B´ar´any, V., Gr¨adel, E., Rubin, S.: Automata-based presentations of inﬁnite struc-
tures. In: Esparza, J., Michaux, C., Steinhorn, C. (eds.) Finite and Algorithmic
Model Theory, pp. 1–76. Cambridge University Press (2011)
2. Blumensath, A.: Automatic structures. Diploma thesis, RWTH Aachen (1999)
3. G´ecseg, F., Steinby, M.: Tree languages. In: Rozenberg, G., Salomaa, A. (eds.) Hand-
book of Formal Languages, vol. 3, pp. 1–68. Springer, Heidelberg (1997)
4. Khoussainov, B., Nerode, A.: Automatic Presentations of Structures. In: Leivant,
D. (ed.) LCC 1994. LNCS, vol. 960, pp. 367–392. Springer, Heidelberg (1995)
5. Khoussainov, B., Rubin, S., Stephan, F.: Automatic linear orders and trees. ACM
Transactions on Computional Logic 6(4), 675–700 (2005)
6. Rubin, S.: Automata presenting structures: A survey of the ﬁnite string case. Bul-
letin of Symbolic Logic 14(2), 169–209 (2008)
7. Thomas, W.: Languages, automata, and logic. In: Rozenberg, G., Salomaa, A. (eds.)
Handbook of Formal Languages, vol. 3, pp. 384–455. Springer, Heidelberg (1997)

On the Relative Succinctness of Two Extensions
by Deﬁnitions of Multimodal Logic
Wiebe van der Hoek1, Petar Iliev1, and Barteld Kooi2
1 Department of Computer Science, University of Liverpool, Liverpool L69 7ZF,
United Kingdom
{wiebe,pvi}@liverpool.ac.uk
2 Department of Theoretical Philosophy, University of Groningen, Oude
Boteringestraat 52, 9712 GL Groningen, The Netherlands
B.P.Kooi@rug.nl
Abstract. The growing number of logics has lead to the question: How
do we compare two formalisms? A natural answer is: We can compare
their expressive power and computational properties. There is, however,
another way of comparing logics that has attracted attention recently,
namely in terms of representational succinctness, i.e., we can ask whether
one of the logics allows for a more “economical” encoding of information
than the other. Using extended-syntax trees that correspond to game
trees for the Addler-Immerman games, we prove that two well-known
abbreviations in multimodal logic lead to an exponential increase in
succinctness.
1
Introduction
Consider the following general question. Let L1 and L2 be two formalisms that
express the same class of “properties”. Is one of the formalisms representationally
more succinct (allowing for a more “economical” representation of information)
than the other and by how much? A famous instance of this general question is
whether there is a family of Boolean functions for which boolean circuits can be
exponentially more succinct than Boolean formulas. It is believed that the answer
to this question is beyond our current mathematical knowledge and techniques.
Indeed, it is convincingly argued in [5] that we are far from understanding rep-
resentational succinctness and we need to ﬁrst master some very basic problems
in order to develop our intuition and mathematical toolbox.
Here we prove that the multimodal logic [∧Γ]ML is exponentially more suc-
cinct than the logic [∨Γ]ML and vice versa. Hence, there are semantic properties
that are more economically expressed by using the [∧Γ] operator; similarly, there
are semantic properties that are more economically expressed by using the [∨Γ]
operator.
A formula of the form [∧Γ]ϕ is an abbreviation of the multimodal logic (ML)
formula 
a∈Γ [a]ϕ, where Γ is a ﬁnite set of relation indices. In the same way, a
formula of the form [∨Γ]ϕ is an abbreviation of the ML formula 
a∈Γ [a]ϕ. Read-
ers familiar with epistemic logic will notice that the [∧Γ] modality corresponds
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 323–333, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

324
W. van der Hoek, P. Iliev, and B. Kooi
to the “everybody knows” operator, while [∨Γ] corresponds to the “somebody
knows” operator. It is obvious that adding formulas of the form [∧Γ]ϕ to ML
and, thus, obtaining the logic [∧Γ]ML does not lead to an increase in expressive
power; what is more, the computational complexity of the satisﬁability problem
for [∧Γ]ML is the same as that for ML [6]. In the same way, [∨Γ]ML is not more
expressive than ML. However, it is known that both [∧Γ]ML and [∨Γ]ML are
exponentially more succinct than ML [3]. Proving that [∧Γ]ML is exponentially
more succinct than [∨Γ]ML and vice versa, is a further continuation of the line
of work started in [3]. Furthermore, it shows that formulas of the form [∨Γ]ϕ
and [∧Γ]ϕ introduce two diﬀerent types of “information” compression.
2
Preliminaries
2.1
Multimodal Logic
Deﬁnition 1 (Multimodal [∨Γ][∧Γ]MLm
n logic). The signature of the mul-
timodal logic [∨Γ][∧Γ]MLm
n is a pair S = {P, I }, where P = {p0, p1 . . . pn} is a
ﬁnite set of propostional symbols and I = {a1, . . . , am} is a ﬁnite set of indices.
Let PI be the set of nonempty subsets of I . The formulas of [∨Γ][∧Γ]MLm
n are
built according to the rule:
ψ := ⊥| ⊤| p ∈P | ¬ψ | ψ ∨ψ | ψ ∧ψ | [a]ψ | ⟨a⟩ψ | [∨Γ]ψ | [∧Γ]ψ,
where a ∈I and Γ ∈PI .
Deﬁnition 2 ([∧Γ]MLm
n , [∨Γ]MLm
n , and MLm
n ). The formulas of the logic
[∧Γ]MLm
n are the formulas of the logic [∨Γ][∧Γ]MLm
n with the exception of all
formulas of the form [∨Γ]ψ. Similarly, the formulas of the logic [∨Γ]MLm
n are
the formulas of the logic [∨Γ][∧Γ]MLm
n with the exception of all formulas of the
form [∧Γ]ψ. Finally, the formulas of multimodal logic MLm
n are the formulas of
[∨Γ][∧Γ]MLm
n logic with no formulas of the forms [∧Γ]ψ and [∨Γ]ψ allowed.
The semantics of [∨Γ][∧Γ]MLm
n is given as follows.
Deﬁnition 3 (Model). A model for the signature S = {P, I } is a triple M =
⟨W , R, V ⟩, where
– W is a set of points;
– R : I →2W ×W is a function that assigns a binary relation R(a) on W to
every a ∈I . We write wRav for (w, v) ∈R(a) and say that v can be reached
from w in one a-step.
– V : P →2W is a function that assigns a subset V (p) ⊆W to every p ∈P.
A model M = ⟨W , R, V ⟩is said to be ﬁnite if W is ﬁnite. Given a model
M = ⟨W , R, V ⟩, a pointed model is a pair (M , w), where w ∈W . Sets of
pointed models are denoted
 ,
,
 1,
1,
 2,
2, etc.

On the Relative Succinctness of Two Extensions
325
We deﬁne the notion “formula ϕ is true in a pointed model (M , w)” in the usual
way (see for example [2]). In particular:
(M , w) |= ⟨a⟩ψ
iﬀthere is a v ∈W such that wRav and (M , v) |= ψ;
(M , w) |= [a]ψ
iﬀ(M , v) |= ψ for all v ∈W such that wRav;
(M , w) |= [∧Γ]ψ iﬀ(M , w) |= 
a∈Γ [a]ψ;
(M , w) |= [∨Γ]ψ iﬀ(M , w) |= 
a∈Γ [a]ψ.
Given this semantics, it is obvious that [∧Γ]MLm
n , [∨Γ]MLm
n , and [∨Γ][∧Γ]MLm
n
are just extensions by deﬁnition of the logic MLm
n . In addition, if Γ = {a}, then
both [∧Γ]ψ and [∨Γ]ψ are the formula [a]ψ. Hence, in what follows, we assume
that the set Γ contains at least two indices. As usual, for every pointed model
(M , w) and every formula ⟨a⟩ψ,
(M , w) |= ⟨a⟩ψ ↔¬[a]¬ψ.
From now on, if (M , w) is a pointed model, where M = ⟨W , R, V ⟩, we write
v ∈M instead of v ∈W ; all models and all sets of pointed models are ﬁnite.
We write
  |= ϕ to mean that for all (M , w) ∈
 , (M , w) |= ϕ. Note that if
  = ∅, then for every [∨Γ][∧Γ]MLm
n formula ϕ, it is trivially true that
  |= ϕ.
We are going to use the well-known fact that if two pointed models (M , w) and
(N , v) are bisimilar, then for every [∨Γ][∧Γ]MLm
n formula ϕ, (M , w) |= ϕ if and
only if (N , v) |= ϕ (see [2]).
We deﬁne the following operations on pointed models and sets of pointed
models:
Deﬁnition 4. Let (M , w) and
  = {(M1, w1), . . . , (Mk, wk)} be a pointed model
and a set of pointed models for the signature S = {P, I }. Let a ∈I and Γ =
{ai, . . . , aj } be a subset of I . Then
– [a](M , w) = {(M , v) | v ∈M and wRav}. Intuitively, [a](M , w) is the set
of all pointed models that can be reached from w by making one a-step. Note
that if there is no point v ∈M such that wRav, then [a](M , w) = ∅.
– [a]  = [a](M1, w1) ∪. . . ∪[a](Mk, wk).
– [∧Γ]  = [ai] ∪. . . ∪[aj ] , i.e., [∧Γ]  is the union of [a]  for all a ∈Γ.
– Suppose that
  |= [∨Γ]ψ. Hence, for every (Mi, wi) ∈
 , there is a subset
Γi ⊆Γ such that for every a ∈Γi, (Mi, wi) |= [a]ψ. Therefore, we can
construct the set
[∨Γ(ψ)]  = 
a∈Γ1[a](M1, w1) ∪. . . ∪
a∈Γk [a](Mk, wk).
– Suppose that (M , w) |= ⟨a⟩ψ. Hence, there is at least one v ∈M such that
wRav and (M , v) |= ψ. We construct the nonempty set of all such pointed
models, i.e., ⟨a(ψ)⟩(M , w) = {(M , v) | v ∈M and wRav, and(M , v) |= ψ}.
– Suppose
  |= ⟨a⟩ψ. Therefore, we can form the nonempty set of pointed
models
⟨a(ψ)⟩  = ⟨a(ψ)⟩(M1, w1) ∪. . . ∪⟨a(ψ)⟩(Mk , wk).
– Suppose that
  |= ¬[∨Γ]ψ. Hence, for every (M , w) ∈
  and every a ∈Γ,
there is at least one v ∈M such that wRav and (M , v) |= ¬ψ. We form the
set ¬[∨Γ(ψ)]  = 
a∈Γ ⟨a(¬ψ)⟩(M1, w1) ∪. . . ∪
a∈Γ ⟨a(¬ψ)⟩(Mk , wk).

326
W. van der Hoek, P. Iliev, and B. Kooi
– Suppose that
  |= ¬[∧Γ]ψ. Hence, for every (Mi, wi) ∈
 , there is a
nonempty subset Γi ⊆Γ such that for every a ∈Γi there is at least one
v ∈Mi for which wiRav and (Mi, v) |= ¬ψ. Therefore, we can construct the
nonempty set
¬[∧Γ(ψ)]  = 
a∈Γ1⟨a(¬ψ)⟩(M1, w1) ∪. . . ∪
a∈Γk⟨a(¬ψ)⟩(Mk , wk).
2.2
Extended Syntax Trees
Every [∨Γ][∧Γ]MLm
n formula ϕ can be represented by a syntax tree in the usual
way, i.e., every leaf of the tree is labeled with an atomic symbol occurring in
ϕ and inner nodes are labeled with Boolean connectives or modal operators
occurring in ϕ.
The notion of extended syntax tree of a ﬁrst-order formula was introduced
in [5]. Extended syntax trees correspond to game trees for the Adler-Immerman
games deﬁned in [1]. The reader can think about these trees as normal syntax
trees in which every node has an additional semantic label that consists of two
sets of pointed models.
Deﬁnition 5 (Extended Syntax Tree). Let ϕ be a [∨Γ][∧Γ]MLm
n formula
and let
  and
 be two sets of pointed models such that
  |= ϕ and
 |= ¬ϕ.
The extended syntax tree T ⟨ ◦⟩
ϕ
is deﬁned inductively on the structure of ϕ as
follows:
(ϕ is a propositional symbol p ∈P): T ⟨ ◦⟩
p
consists of a single node t that
has a syntax label synl(t) := p and a semantic label seml(t) := ⟨  ◦
⟩.
(ϕ is ⊤): T ⟨ ◦⟩
⊤
consists of a single node t with synl(t) := ⊤and seml(t) :=
⟨  ◦
⟩.
Note that in this case
 = ∅.
(ϕ is ⊥): T ⟨ ◦⟩
⊥
consists of a single node t with synl(t) := ⊥and seml(t) :=
⟨  ◦
⟩.
Note that in this case
  = ∅.
(ϕ is ¬ψ): T ⟨ ◦⟩
¬ψ
has a root node t with synl(t) := ¬ and seml(t) := ⟨ ◦
⟩.
The unique child of t is the root of T ⟨◦ ⟩
ψ
. Note that
 |= ψ and
  |= ¬ψ.
(ϕ is ψ1 ∧ψ2): T ⟨ ◦⟩
ψ1∧ψ2
has a root node t with synl(t) := ∧and seml(t) :=
⟨ ◦
⟩. The ﬁrst child of t is the root of T ⟨ ◦1⟩
ψ1
. The second child of t is the
root of T ⟨ ◦2⟩
ψ2
, where, for i ∈{1, 2},
i = {(N , v) ∈
 | (N , v) |= ¬ψi}.
Note that
 =
1 ∪
2,
  |= ψ1 ∧ψ2, and
1 |= ¬ψ1,
2 |= ¬ψ2.
(ϕ is ψ1 ∨ψ2): T ⟨ ◦⟩
ψ1∨ψ2
has a root node t with synl(t) := ∨and seml(t) :=
⟨  ◦
⟩.
The ﬁrst child of t is the root of T ⟨ 1◦⟩
ψ1
. The second child of t is the root
of T ⟨ 2◦⟩
ψ2
, where, for i ∈{1, 2},
 i = {(M , v) ∈
  | (M , v) |= ψi}.
Note that
  =
 1 ∪
 2,
 1 |= ψ1, and
 2 |= ψ2,
 |= ¬(ψ1 ∨ψ2).
(ϕ is [a]ψ): T ⟨ ◦⟩
[a]ψ
has a root node t with synl(t) := [a] and seml(t) := ⟨  ◦
⟩.

On the Relative Succinctness of Two Extensions
327
The unique child of t is the root of T ⟨[a]  ◦⟨a(¬ψ)⟩⟩
ψ
. Note that [a]  |= ψ
and ⟨a(¬ψ)⟩ |= ¬ψ.
(ϕ is ⟨a⟩ψ): T ⟨ ◦⟩
⟨a⟩ψ
has a root node t with synl(t) := ⟨a⟩and seml(t) :=
⟨  ◦
⟩.
The unique child of t is the root of T ⟨⟨a(ψ)⟩ ◦[a] ⟩
ψ
. Note that ⟨a(ψ)⟩  |= ψ
and [a] |= ¬ψ.
(ϕ is [∧Γ]ψ): T ⟨ ◦⟩
[∧Γ ]ψ
has a root node t with synl(t) := [∧Γ] and seml(t) :=
⟨  ◦
⟩. The unique child of t is the root of T ⟨[∧Γ ]  ◦¬[∧Γ (ψ)] ⟩
ψ
. Note that
[∧Γ]  |= ψ and ¬[∧Γ(ψ)] |= ¬ψ.
(ϕ is [∨Γ]ψ): T ⟨ ◦⟩
[∨Γ ]ψ
has a root node t with synl(t) := [∨Γ] and seml(t) :=
⟨  ◦
⟩.
The unique child of t is the root of T ⟨[∨Γ ]  ◦¬[∨Γ (ψ)] ⟩
ψ
. Note that [∨Γ]  |= ψ
and ¬[∨Γ(ψ)] |= ¬ψ.
Then next proposition follows immediately from Deﬁnition 5.
Proposition 1. For any [∨Γ][∧Γ]MLm
n formula ϕ and any sets of pointed mod-
els
 ,
 such that
  |= ϕ and
 |= ¬ϕ, if the root of the extended syntax tree
T ⟨ ◦⟩
ϕ
has a child-node that is the root of the syntax tree T ⟨◦⟩
ψ
, then
 |= ψ
and
 |= ¬ψ.
Figure 1 shows the extended syntax tree T ⟨ ◦⟩
[∨Γ ]p∧⟨a⟩⟨b⟩⊤of the formula [∨Γ]p ∧
⟨a⟩⟨b⟩⊤, where
  = {(M1, s1), (M2, s2)} and
 = {(M3, s3), (M4, s4)}. The low-
ermost black square in a model Mi that is in the semantic label of the root of
the tree is the point si.
Deﬁnition 6 (Branches). A branch B in an extended-syntax tree is any path
leading from the root of the tree to a leaf. We denote by I (B) the word i1 . . . ik,
formed by the indices of all the nodes with syntactic labels of the form [i] or ⟨i⟩
occurring along a branch B when traversing the branch from the root to its leaf.
For instance, the branch B (right) of the game tree of Figure 1 satisfy I (B) =
ab. Note that given an extended syntax tree T ⟨ ◦⟩
ϕ
, the “shape” of the tree
depends solely on ϕ, e.g., if we disregard the semantic labels of the nodes, the
three extended syntax trees T ⟨ ◦⟩
⟨a⟩⟨b⟩⊤∧[∨Γ ]p, T ⟨∅◦⟩
⟨a⟩⟨b⟩⊤∧[∨Γ ]p, and T ⟨∅◦∅⟩
⟨a⟩⟨b⟩⊤∧[∨Γ ]p
are actually the usual syntax tree of the formula ⟨a⟩⟨b⟩⊤∧[∨Γ]p. Hence, the
following deﬁnition is unambiguous.
Deﬁnition 7 (Formula Size). Let ϕ be a [∨Γ][∧Γ]MLm
n formula and let
 and
 be two arbitrary sets of pointed models such that
  |= ϕ and
 |= ¬ϕ.
The size of ϕ (denoted ||ϕ||) is the number of verticies occurring in T ⟨ ◦⟩
ϕ
.
We enumerate the nodes of an extended syntax tree T ⟨ ◦⟩
ϕ
in increasing order
starting from the root of the tree; it’s children are enumerated from left to right,
etc., i.e., the nodes are enumerated as in a breadth-ﬁrst search algorithm.

328
W. van der Hoek, P. Iliev, and B. Kooi
M1
∧
M2
M4
[ Γ]
M1
M2
M4
M1
M2
M1
M2
p
⟨a⟩
M1
M2
M4
M1
M2
⟨b⟩
⊤
M3
M3
M3
M3
Fig. 1. The extended syntax tree T ⟨ ◦⟩
[∨Γ ]p∧⟨a⟩⟨b⟩⊤, where Γ = {a, b}. The current points
in the pointed models are denoted by a square, the rest of the points are denoted
by circles. Black squares and circles denote the points where the atom p is true. The
solid arrows in the models denote relation steps indexed with a, the dashed arrows are
indexed with b. The big white circles denote nodes in the syntax tree.
2.3
Succinctness
Deﬁnition 8. Let
 be a non-empty set of models and let L1 and L2 be two
logics. We say that L1 is at least as expressive as L2 on
, written L2 ≤expr

L1,
if for every ϕ2 ∈L2, there is a formula ϕ1 in L1 such that
 |= ϕ1 ↔ϕ2.
Following [5], our formal deﬁnition of succinctness is:

On the Relative Succinctness of Two Extensions
329
Deﬁnition 9 (Succinctness). Let L1, L2 be two logics. Let
 be a class of
models such that L1 ≤expr

L2. Let F be a class of functions f :
 →R. We say
that L1 is F-succinct in L2 on
, and write L1 ≤F
 L2, iﬀthere is a function
f ∈F such that for every L1-formula ϕ1, there is an L2-formula ϕ2 for which
the following is true:
–
 |= ϕ1 ↔ϕ2;
– ||ϕ2|| ≤f (||ϕ1||).
Intuitively, this means that F gives an upper bound on the size of L2 formulas
needed to express all of L1 on
. Therefore, by saying that L1 is exponentially
more succinct than L2, we mean L1 ≤expr

L2 and L1 ≰SUBEXP

L2, i.e., the size
of formulas of L2 expressing all of L1 on
 cannot be bounded from above by a
sub-exponential function.
In order to prove that L1 ≰SUBEXP

L2 for two logics L1 and L2, it is suﬃcient
to show that there are two inﬁnite sequences of formulas ϕ1, ϕ2, . . . in L1 and
χ1, χ2, . . . in L2, and rational numbers k and t such that
1. ||ϕn|| = kn + t;
2. ||χn|| ≥2i;
3. χn is the shortest formula in L2 such that
 |= ϕn ↔χn.
3
Main Results
For a natural number n ≥1, let [∧Γ]n stand for
n times [∧Γ ]



[∧Γ] . . . [∧Γ] and similarly for
[∨Γ]n.
Deﬁnition 10. Let S = {P, I } be a signature where P contains at least one
propositional symbol p and I contains at least two indices a and b and let Γ =
{a, b}. For every n ≥1, the [∧Γ]MLm
n formulas ϕn, the [∨Γ]MLm
n formulas θn,
and the MLm
n formulas ψn and χn are deﬁned as follows.
ϕn := ¬[∧Γ]n¬p,
ψ1 := ⟨a⟩p ∨⟨b⟩p;
ψn := ⟨a⟩ψn−1 ∨⟨b⟩ψn−1, n > 1.
θn := [∨Γ]np,
χ1 := [a]p ∨[b]p;
χn := [a]χn−1 ∨[b]χn−1, n > 1.
It is easily seen that the formulas in the right column are equivalent to the
formulas in the left column. It is obvious that the length of the latter is linear
in n while the length of the former is exponential in n.
Firstly, we would like to prove that there is no sequence of [∨Γ]MLm
n formu-
las δn such that ϕn is equivalent to δn and at the same time the length of δn is
subexponential in n. To this end, for every n ≥1, we deﬁne a set of models
 n
such that
 n |= ϕn. Then we ﬁnd a set of models
n such that
n |= ¬ϕn.

330
W. van der Hoek, P. Iliev, and B. Kooi
Finally, we prove that for every [∨Γ]MLm
n formula δ such that
 n ∪
n |=
ϕn ↔δ, the extended syntax tree T ⟨ n◦n⟩
δ
has at least 2n nodes. Intuitively,
it is clear that the main diﬃculty in such a proof stems from the “power” of
the [∨Γ] operator. While (M , w) |= [a]ϕ means that all points reachable from
w in one a-step satisfy the formula ϕ, (M , w) |= [∨Γ]ϕ means that there is at
least one index i ∈Γ such that all points reachable from w in one i-step satisfy
ϕ. Therefore, if we manage to deﬁne the models in
 n and
n in such a way
as to make the [∨Γ] operator “useless”, i.e., the possibility oﬀered by [∨Γ] to
non-deterministically chose relation steps is eliminated, our task will be easier.
Secondly, we would like to prove that there is no [∧Γ]MLm
n formula γn of
subexponential length such that θn is equivalent to γn. Again, the main problem
is the power of the [∧Γ] modality. Guided by the same intuition, for every n ≥1,
we deﬁne sets of models
n and
n where
n |= θn,
n |= ¬θn, and for every
[∧Γ]MLm
n formula γ such that
n ∪
n |= θn ↔γ, the syntax tree T ⟨n◦n⟩
γ
has at least 2n nodes. As for [∨Γ]MLm
n , we deﬁne the models in
n and
n so
that the operator [∧Γ] is of no “use”.
Deﬁnition 11 and items (b) and (c) from Lemma 1 are the formalization of
this idea.
Deﬁnition 11 (Tree models). Let the signature S = {P, I } be as in Deﬁnition
10. Figure 2 shows the sets of models
 n,
n,
n, and
n.
The tree-like models in
 1,
1,1,
1 are built as shown. The models in
n+1 and
n+1, for example, are deﬁned recursively by taking a model from
1, erasing the propositional symbols, and then using the leaves of the tree as
roots for the models from
n and
n as shown1. The same strategy is employed
in the construction of the models in
 n+1 and
n+1.
We denote the root of any of the models in
 n,
n,n,
n by r. For any
model M n+1
xk
, such that x ∈{a, b}, the pair (M n+1
xk
, M n
k ) stands for the pointed
model (M n+1
xk
, r), where r is the root of M n
k . Similarly for (M n+1
xk
, N n) and
(N n+1, N n).
Lemma 1. Let the sequences of formulas φn, θn, ψn, and χn be deﬁned as in
Deﬁnition 10 and let
 n, Nn,
n, and
n be as in Deﬁnition 11. For every n,
and all sets of pointed models
 and
:
(a)
 n |= ϕn,
n |= ¬ϕn,
n |= θn,
n |= ¬θn;
(b) For any (M n
w , r), there is neither a formula [∨Γ]ψ such that (M n
w , r) |=
[∨Γ]ψ and (N n, r) |= ¬[∨Γ]ψ nor a formula [∨Γ]ψ such that (N n, r) |=
[∨Γ]ψ and (M n
w , r) |= ¬[∨Γ]ψ.
(c) For any (On
w, r), there is neither a formula [∧Γ]ψ such that (On
w, r) |= [∧Γ]ψ
and (Pn
w, r) |= ¬[∨Γ]ψ nor a formula [∨Γ]ψ such that (Pn
w, r) |= [∧Γ]ψ and
(On
w, r) |= ¬[∨Γ]ψ.
1 Intuitively, the subscript in the name of the model On+1
ak
encodes a path (starting
with an a-step) leading from the root of the tree to a leaf satisfying the proposition
p. The same path in the model P n+1
ak
leads to a point that does not satisfy p. Apart
from this diﬀerence, the models On+1
ak
and P n+1
ak
look the same.

On the Relative Succinctness of Two Extensions
331
p
a
b
a
p
a
b
b
a
b
N 1
1
1
M 1
a
M 1
b
a
b
a
a
b
b
n+1
M n+1
ak
M n+1
bk
M n
k
M n
k
N n
N n
N n
N n
n+1
a
b
N n+1
N n
N n
p
a
b
p
a
b
a
b
a
b
1
1
n+1
n+1
On+1
ak
On+1
bk
O1
b
O1
a
P 1
a
b
b
a
a
P 1
b
p
p
a
b
b
p
a
b
a
p
On
k
On
k
P n
k
P n
k
On
k
On
k
a
b
b
P n+1
ak
a
b
a
P n+1
bk
P n
k
P n
k
P n
k
P n
k
On
k
On
k
Fig. 2. The sets of models
 n,
n,
n, and
n
(d) For any [∨Γ]MLm
n
formula ϕ such that
 ∪{(M n
w , r)} |= ϕ and
 ∪
{(N n, r)} |= ¬ϕ, the extended syntax tree T ⟨∪{(M n
w ,r)}◦{(N n,r)}∪⟩
ϕ
has a
branch B, where I (B) = w.
(e) For any [∧Γ]MLm
n formula ϕ such that
 ∪{(On
w, r)} |= ϕ and {(P n
w, r)} ∪
 |= ¬ϕ, the extended syntax tree T ⟨∪{(On
w,r)}◦{(Pn
w,r)}∪⟩
ϕ
has a branch B,
where I (B) = w.
Proof
(a) It is easily seen that
 n |= ψn,
n |= ¬ψn,
n |= χn,
n |= ¬χn.
(b) We prove only the case n > 1 and w = ak. The case w = bk or n = 1 is
analogous. Assume there is a formula [∨Γ]ψ such that (M n
ak, r) |= [∨Γ]ψ and
(N n, r) |= ¬[∨Γ]ψ. Consider the extended syntax tree T ⟨{(M n
ak ,r)}◦{(N n,r)}⟩
[∨Γ ]ψ
.
Using the last item from Deﬁnition 5, we see that there must be two points s
and t in N n such that rRas and (N n, s) |= ¬ψ and rRbt and (N n, t) |= ¬ψ.
There are only two such points and they are the roots of the models N n−1.

332
W. van der Hoek, P. Iliev, and B. Kooi
On the other hand, we must have at least one of the following (M n
ak, r) |= [a]ψ
or (M n
ak, r) |= [b]ψ. This however is impossible, since in the ﬁrst case there is
a z ∈M n
ak such that rRaz and z is the root of the tree N n−1; whereas, in the
second case, there is a u such that rRbu and u is the root of the model N n−1.
In either case, the root of T ⟨{(M n
ak ,r)}◦{(N n,r)}⟩
[∨Γ ]ψ
must have a child of the form
T ⟨∪{(M n
ak,N n−1)}◦{(N n,N n−1)}∪⟩
ψ
which is impossible since, using Proposition
1, we have two bisimilar models, namely (M n
ak, N n−1) and (N n, N n−1), for
which (M n
ak, N n−1) |= ψ and (N n, N n−1) |= ¬ψ. The assumption that there
is [∨Γ]ψ such that (N n, r) |= [∨Γ]ψ and (M n
w , r) |= ¬[∨Γ]ψ leads to a
contradiction in a similar way.
(c) The assumption that there is such a formula plus the rule for formulas of the
form [∧Γ]ψ from Deﬁnition 5, leads to a contradiction as in item(b) above.
(d) Let ϕ,
, and
 be as described and let T ⟨∪{(M n
w ,r)}◦{(N n,r)}∪⟩
ϕ
be the ex-
tended syntax tree. Suppose n > 12. We consider only the case w = ak. The
case w = bk is similar. Let rt denote the root of the tree. It is obvious that
the syntax label of rt cannot be one of the following “⊤”, “⊥”, “p”, and item
(b) implies that synl(rt) cannot be “[∨Γ]”. Therefore, synl(rt) is either “¬”,
“∨”, “∧”, “[i]” or “⟨i⟩” for i ∈{a, b}. If synl(rt) is one of the Boolean connec-
tives, then rt will have as a child the root of a tree T ⟨1∪{(M n
ak,r)}◦{(N n,r)}∪1⟩
ψ
or T ⟨1∪{(N n,r)}◦{(M n
ak,r)}∪1⟩
ψ
for some sub-formula ψ of ϕ. The reasoning
above shows that the root node rt1 of this tree cannot have a syntactic label
of the form “⊤”, “⊥”, “p”, or “[∨Γ]”. Hence, T ⟨∪{(M n
ak,r)}◦{(N n,r)}∪⟩
ϕ
must
contain at least one node that is the root of a tree T ⟨l∪{(M n
ak,r)}◦{(N n,r)}∪l⟩
ψ
or T ⟨l ∪{(N n,r)}◦{(M n
ak,r)}∪l⟩
ψ
for some sub-formula ψ of ϕ, that has one of
the forms [a]θ, ⟨a⟩θ, ⟨b⟩θ or [b]θ. Let n be the ﬁrst such node in the enumer-
ation of T ⟨∪{(M n
ak,r)}◦{(N n,r)}∪⟩
ϕ
. It is easy to see that for all formulas ⟨b⟩θ
and [b]θ, (M n
ak, r) |= ⟨b⟩θ if and only if (N n, r) |= ⟨b⟩θ, and (M n
ak, r) |= [b]θ
if and only if (N n, r) |= [b]θ. Hence, ψ is either of the form [a]θ or ⟨a⟩θ.
Consider the tree T ⟨l∪{(M n
ak,r)}◦{(N n,r)}∪l⟩
ψ
. In this case, ψ is not of the
form [a]θ since the root of the tree would contain a child with semantic label
l ∪{(M n
ak, N n−1)} ◦{(N n, N n−1)} ∪
l⟩. Given the fact that (M n
ak, N n−1)
and (N n, N n−1) are bisimilar, we arrive at a contradiction using Proposi-
tion 1. Hence ψ has the form ⟨a⟩θ; moreover, the bisimilarity of (M n
ak, N n−1)
and (N n, N n−1) implies that the root of the tree can only have a child with
semantic label ⟨l+1 ∪{(M n
ak, M n−1
k
)} ◦{(N n, N n−1)} ∪
l+1⟩. It is obvious
that this reasoning can be repeated until the branch B such that I (B) = ak
is constructed. In the case of T ⟨l ∪{(N n,r)}◦{(M n
ak,r)}∪l⟩
ψ
, similar considera-
tions show that ψ has the form [a]θ and that there is a branch B such that
I (B) = ak.
(e) The proof is analogous to the proof of item (d).
2 The proof of the case n = 1 will become clear from the proof for n > 1.

On the Relative Succinctness of Two Extensions
333
Theorem 1. Let
  denote the union of all
 n and
n models. Let
 denote
the union of all
n and
n models. Then
(1) [∧Γ]MLm
n ≰SUBEXP
 [∨Γ]MLm
n ;
(2) [∨Γ]MLm
n ≰SUBEXP

[∧Γ]MLm
n .
Proof. We prove (2). The proof of (1) is similar. Item (e) from Lemma 1 implies
that the syntax tree T ⟨n◦n⟩
δ
of any [∧Γ]MLm
n formula δ such that
n |= δ and
n |= ¬δ contains 2n diﬀerent branches. Hence, any [∧Γ]MLm
n formula δ such
that
 |= [∨Γ]np ↔δ has size at least 2n. Since [∨Γ]np is equivalent to the
formula χn from Deﬁnition 10, we see that [∨Γ]MLm
n is exactly exponentially
more succinct than [∧Γ]MLm
n .
4
Conclusion and Open Questions
As mentioned in the introduction, the results presented here are relevant to
epistemic logic. One disadvantage of such an interpretation is that all relations
in our models are neither reﬂexive, nor symmetric, nor transitive, whereas the
semantics of epistemic logic is usually given via models in which all relations
are relations of equivalence. We believe that our results can be extended to this
class of models as well, although we conjecture that the proofs will be technically
more challenging than ours.
On a more general note. The study of representational succinctness is a vastly
underdeveloped subject and many of the known results rely on unproven com-
putational complexity conjectures (see for example [4]). Improving the proofs of
at least some these results so that they are not dependent on such conjectures
will greatly increase our understanding of this phenomenon.
References
1. Adler, M., Immerman, N.: An n! lower bound on formula size. ACM Transactions
on Computational Logic 4(3), 296–314 (2003)
2. Blackburn, P., de Rijke, M., Venema, Y.: Modal logic. Cambridge University Press
(2001)
3. French, T., van der Hoek, W., Iliev, P., Kooi, B.: Succinctness of epistemic languages.
In: Proceedings of the 22nd International Joint Conference on Artiﬁcial Intelligence
(IJCAI), pp. 881–886 (2011)
4. Gogic, G., Papadimitriou, C., Selman, B., Kautz, H.: The comparative linguistics of
knowledge representation. In: Proceedings of the 14th International Joint Conference
on Artiﬁcial Intelligence (IJCAI), pp. 862–869 (1995)
5. Grohe, M., Schweikardt, N.: The succinctness of ﬁrst-order logic on linear orders.
In: Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science
(LICS), pp. 438–447 (2004)
6. Lutz, C.: Complexity and succinctness of public announcement logic. In: Proceedings
of the 5th International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS), pp. 137–143 (2006)

On Immortal Conﬁgurations in Turing Machines
Emmanuel Jeandel
Laboratoire d’Informatique, de Robotique et de Micro´electronique de Montpellier,
UMR 5506—CC 477, 161 rue Ada, 34095 Montpellier, Cedex 5, France
jeandel@lirmm.fr
Abstract. We investigate the immortality problem for Turing machines
and prove that there exists a Turing Machine that is immortal but halts
on every recursive conﬁguration. The result is obtained by combining
a new proof of Hooper’s theorem [11] with recent results on eﬀective
symbolic dynamics.
In this paper we investigate the behaviour of Turing machines seen as dynamical
systems. In this context, a Turing machine does not start from a speciﬁed initial
state and tape, but from any conﬁguration. In this context, we call a conﬁgur-
ation immortal if the machine runs forever starting from it. The seminal article
of Hooper [7] proves that we cannot decide whether a Turing machine has an
immortal conﬁguration (is immortal). However, the result does not say anything
about what the immortal conﬁgurations look like. In fact, in the construction by
Hooper, if the Turing machine has an immortal conﬁguration, then it has one
where the tape is almost completely empty.
A few results give some structure on the set of immortal conﬁgurations: Kurka
[12] asked whether there always exists a (temporally) periodic conﬁguration, and
was refuted by Blondel et al. [1]. Delvenne and Blondel [6] proved that the set of
immortal conﬁgurations, if nonempty, always contains temporally quasi-periodic
conﬁgurations.
In this article, we go further, and give some news results on the set of immortal
conﬁgurations. We prove in particular in Theorem 2 that there exists a Turing
machine for which the set of immortal conﬁgurations is nonempty and contains
no computable conﬁgurations.
The main ingredient is a new proof of Hooper’s theorem by Kari and Ollinger
[11] combined with an encoding into subshifts of an eﬀective set with no recursive
points. We ﬁrst deﬁne all relevant vocabulary in the next section, then proceed
to the proof.
1
The Immortality Problem
1.1
Deﬁnitions
We ﬁrst give some deﬁnitions and properties of Turing machines. Unless spe-
ciﬁed, our Turing machines will have only one biinﬁnite tape. We use the
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 334–343, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

On Immortal Conﬁgurations in Turing Machines
335
moving-tape convention: the head is always at position 0 and the tape, rather
than the head, is moving. In this context, the state of the system can be described
only by the state of the Turing machine and the content of the tape.
Let Σ be the alphabet of the tape, and S the set of states of the Turing
machine M. A conﬁguration (also called instantaneous description) is an element
of C = S × ΣZ. The Turing machine can now be seen as a partial function T (as
an halting conﬁguration has no image) on C.
A conﬁguration c ∈C is immortal for M if M runs forever starting from the
conﬁguration c. We denote by I(M) the immortal conﬁgurations (if they exist)
of M. I(M) is therefore the largest set on which the Turing machine map T is
total, and (I(M), T ) is therefore a dynamical system.
Note in particular that a conﬁguration is not only a tape, but the combination
of a tape and a state. As a consequence, one cannot restrict itself to the dynamics
of the Turing machine starting from some speciﬁc state, but must take into
account computations starting from any state.
The ﬁrst known result is given by Hooper:
Theorem 1 ([7]). There is no algorithm that decides, given a Turing Machine
M, whether I(M) is empty.
It is important to note that the proof of Hooper and subsequent proofs [11] do
not give any insights into the structure of I(M). Indeed, in these proofs, I(M),
if not empty, always contains a ﬁnite conﬁguration (ie every cell of the tape,
except a ﬁnite number, contains the same symbol). We shall prove here that
I(M) can be more intricate, and in particular might contain only nonrecursive
conﬁgurations.
1.2
Eﬀective Sets
To understand the computational properties of I(M), we need the following
notion:
Deﬁnition 1. A subset S ⊆{0, 1}N is an eﬀective set (also called a Π0
1-set) if
there exists an oracle Turing machine M so that S is exactly the set of oracles
on which the Turing machine, starting from the initial state and the empty tape,
runs forever.
There exist many equivalent deﬁnitions. For example, S is eﬀective if there exists
a recursive set of ﬁnite words L so that S is exactly the set of inﬁnite words
containing no preﬁx in L. There exists an extensive literature on eﬀective sets,
and we refer the reader to [3,4].
Now, up to a slight (recursive) encoding of the set of conﬁgurations C, I(M)
can be seen as a subset of {0, 1}N, and it is quite clear it is eﬀective: it is easy to
build a Turing machine that, using c ∈C as on oracle, simulates M on input c.

336
E. Jeandel
From the fact that I(M) is eﬀective, we already obtain many properties about
its structure, cf., e.g., the basis theorems in [4]. We also know [9] that there
exist (nonempty) eﬀective sets with no recursive points (A point w ∈{0, 1}N is
recursive if there is a Turing machine that outputs wn given n).
So now, the question is as follows: Are the immortal sets as rich as the eﬀective
sets? To answer this question, we shall look at the Turing degree of points of an
immortal set. If x and y are two inﬁnite words (or conﬁgurations), we say that
x ≤T y if there exists a Turing machine that outputs x given y as an oracle. The
Turing degree of a word is then its equivalence class for the relation ≤T ∩≥T .
The degree of recursive points is usually denoted 0.
Our ﬁrst observation is that there is no way to encode any eﬀective sets into
immortal sets, preserving, e.g., the Turing degrees, due to the following lemma:
Lemma 1 ([12]). If I(M) is nonempty, then one of the following is true:
– it contains a conﬁguration c ∈I(M) so that M, starting from c, never reads
the symbols in any position i < 0 of the tape of c.
– it contains a conﬁguration c ∈I(M) so that M, starting from c, never reads
the symbols in any position i > 0 of the tape of c.
A reformulation is that a Turing machine with moving tape is never positively
expansive, see [12]. As a consequence, take the conﬁguration c ∈I(M) given
by the lemma for which the Turing Machine, wlog, never reads any cell in any
position i < 0 of c. Then all conﬁgurations c′ ∈C identical with c on position
i ≥0 are also immortal conﬁgurations. Choosing the other bits of c′ wisely
proves we can ﬁnd in I(M) conﬁgurations of any Turing degree greater than the
degree of (the right part of) c.
Proposition 1. If I(M) is nonempty, there exists a Turing degree d so that
I(M) contains conﬁgurations of any Turing degree above d.
As there exist nonempty eﬀective sets where any two diﬀerent points have in-
comparable Turing degrees [9], there exist eﬀective sets that cannot be encoded
as immortal sets. So we need a weaker deﬁnition. The good notion is Muchnik
equivalence [17]:
Deﬁnition 2. Two subsets S1 and S2 of {0, 1}N are Muchnik equivalent if for
every x1 ∈S1, there is a point x2 ∈S2 computable with oracle x1, and conversely.
Intuitively, two sets S1 and S2 are Muchnik equivalent if they contain the same
“minimal” Turing degrees. In particular, S1 contains a recursive point if and only
if S2 contains a recursive point. It is important to note that the transform from x1
into x2 need not to be uniform in x1, that is the Turing machine transforming x1
into x2 may depend on x1. When the transform is uniform, we speak of Medvedev
equivalence. See [17] for more information on mass problems and Muchnik and
Medvedev equivalence.

On Immortal Conﬁgurations in Turing Machines
337
We now can state our result:
Theorem 2 (Main Result). For every eﬀective set S, there exists a Turing
machine M so that I(M), the set of immortal conﬁgurations of M, is Muchnik
equivalent to S.
Corollary 1. There exists a Turing machine M so that I(M) is nonempty and
contains only nonrecursive conﬁgurations.
1.3
Eﬀective Subshifts
Before going to the proof, we need another ingredient, coming from symbolic
dynamics. To introduce this notion, we look at traces of Turing machines: For
a conﬁguration c, the trace of c is the word u ∈(Σ × S)N where ui contains
the letter in position 0 of the tape and the state at the i-th step during the
execution of M on input c. The trace is well deﬁned only on conﬁgurations
c ∈I(M) (otherwise u would be ﬁnite). Let T (M) be the set of traces of M.
Now we look at the map from c to its trace u(c). First of all, it is clear that u(c)
is computable from c. Furthermore, we can reconstruct the cells of the tape of c
read by the Turing machine from u(c) (which, depending of c, might or not be all
the cells). In particular, there exists a conﬁguration d so that u(d) = u(c) that
is computable in u(c): Let I = [a, b] ⊆N (possibly with a = −∞or b = +∞) be
the set of cells of c that are visited during the computation of M, and reconstruct
the tape of d on I using the trace u(c) and take all other cells to be s for some
arbitrary symbol s ∈Σ. In particular, we just have proven that I(M) and T (M)
are Muchnik-equivalent. (It is important to note that the interval I is not always
computable given u(c): therefore the transformation from u(c) to d is not uniform
in u(c). (In technical terms, we obtain here only a Muchnik equivalence, and not
a Medvedev-equivalence).
T (M) has interesting properties. It is an eﬀective set: by a compactness ar-
gument, u ∈T (M) if for every length n > 0, there exists a conﬁguration c ∈C
so that u coincides with the (possibly ﬁnite) trace of c on positions i < n. T (M)
is also closed under shift: If u ∈T (M) then σ(u) deﬁned by σ(u)i = ui+1 is also
in T (M). This means T (M) is what is called an eﬀective subshift:
Deﬁnition 3. A subset S ⊆ΣN is an eﬀective (right-sided) subshift if it is
eﬀective and closed under shift.
An equivalent deﬁnition is as follows:
Deﬁnition 4. Let L ⊆Σ⋆. We denote by S(L), S+(L), S−(L) respectively the
set of biinﬁnite , right inﬁnite, left inﬁnite words over Σ that contain no word
in L as a factor (substring).
Then S ⊆ΣZ (resp. ΣN, ΣZ−) is an eﬀective twosided (resp. right-sided, left-
sided) subshift if S = S(L) (resp. S+(L), S−(L)) for some recursive language L.
Subshifts are an important tool to understand dynamical systems, cf., e.g., [13].
Eﬀective subshifts is the computable counterpart of subshifts, and has received

338
E. Jeandel
increasing attention in recent years [14,18,2]. In particular we obtained a result
similar to Proposition 1 for subshifts in [8].
Based on these properties, it is reasonable to try to encode an eﬀective sub-
shift, rather than an eﬀective set, as an immortal set. To be able to do this, we
need the following:
Theorem 3 ([14]). For every eﬀective set S, there exists a language L ⊆{0, 1}⋆
so that S(L) (resp. S+(L), S−(L)) is Muchnik-equivalent to S.
(Proposition 3.1 in [14] is given only for S+(L) but it is not hard to see it works
for left-sided and two-sided subshifts as well).
2
Proof of the Main Result
Now we are able to explain how the proof will work. We shall start from an
eﬀective subshift S(L) given by a recursive set L, and code it into a Turing
machine. The machine will have two tracks: The second one will be read-only
and contain a biinﬁnite word w, and the machine will try to prove that w ∈S(L).
However due to Lemma 1, the Turing machine might on some inputs prove only
that some inﬁnite preﬁx (resp. suﬃx) of w is in S−(L) (resp. S+(L)).
To do this, we shall start from the proof of the undecidability of the immor-
tality problem by Kari and Ollinger [11], and explain how to modify it for our
purposes. In particular, a thorough examination of [11] by the reader is encour-
aged. We shall try as much as possible to use the same notations.
2.1
Counter Machines
Usual proofs of the undecidability of the immortality problem usually start with
a counter machine. There will be no diﬀerence here. However note that we need
these machines to accept tapes, i.e., inﬁnite words. For this to make sense, we
shall consider oracle counter machines. To simplify the deﬁnition, we shall sup-
pose that the oracle is over the binary alphabet {0, 1}.
In an oracle counter machine, one of the counters (the ﬁrst here, for reasons
soon to be apparent) is used to represent the position inside the oracle. Inform-
ally, an oracle counter machine thus contains three types of instructions: lookup
instructions (test if a counter is nonzero), oracle lookup (test if the letter of the
oracle is nonzero) and modifying instructions (increase/decrease a counter).
We now deﬁne it formally. Let Φ = {−1, 0, 1} and Υ = {0, 1}.
An oracle k-counter machine is given by a tuple (S, k, T ) where S is a ﬁnite set
of states, k ∈N is the number of counters, and T ⊆S × {1, . . . , k, o} × Υ × Φ× S
the transition relation.
Let w be an inﬁnite word over {0, 1} that will be used as oracle for the
machine. A conﬁguration of a k-counter machine is an element of S × Nk. An
instruction (s, i, u, v, s′) can be applied only on conﬁgurations of the form (s, c)
and will lead to (s′, c′) (denoted by (s, c) ⊢(s′, c′)) with respect to the following
rules:

On Immortal Conﬁgurations in Turing Machines
339
– If i ̸
= o, the instruction can only be applied if u = min(1, ci). That is, u is the
result of the test whether the i-th counter is empty. If i = o, the instruction
can only be applied if wc1 = u, that is if the letter in position c1 of w is u.
– The instruction will then update the counter ci and the state depending
on v. If i = o, it does nothing (c′ = c). Otherwise c′
j = cj for j ̸
= i and
c′
i = ci + v. Note that it is not allowed to decrease the value of a counter
that is already zero.
Note that the ﬁrst counter plays a special role. A counter machine computes
by applying instructions. If at some point there is no instruction that can be
applied, then the machine halts. We shall only be interested in deterministic
counter machines (DCM), for which for every state s, there is at most two tuples
(s, i, u, v, s′) ∈T , and these tuples share the same i and diﬀer on u.
Let s0 be a special (initial) state of the counter machine. We say that a word
w ∈{0, 1}N is accepted by a counter machine if, starting from (s0, 0k), the
computation of the counter machine never halts. If we follow the usual encoding
of Turing machines into 3-counter machines [15], we can prove easily:
Lemma 2. For every eﬀective subset S ⊆{0, 1}N, there exists an oracle 4-
counter machine that accepts exactly S.
We shall need this machine to be reversible. Informally, a DCM M is reversible
(RCM) if there exists a DCM M ′ so that for every oracle w, (s, c) ⊢(s′, c′) by
M iﬀ(s′, c′) ⊢(s, c) by M ′, see [11] for a syntactical deﬁnition. Any DCM can
be extended into a RCM by using two additional counters to store the previous
instructions, so that we have
Lemma 3. For every eﬀective subset S ⊆{0, 1}N, there exists a oracle reversible
6-counter machine that accepts exactly S.
Finally we add to this machine a new counter, that increases every two in-
structions. This ensures that every inﬁnite computation of the counter machine,
regardless of the ﬁrst conﬁguration, will never be periodic. We thus obtain an
oracle reversible 7(!)-counter machine.
2.2
Turing Machines
We now explain how the clever construction of Kari and Ollinger works, starting
from an ordinary 2-counter machine. We shall in the next section explain how
to extend it for our purpose.
We begin by a generic simulation of a counter machine by Turing machines.
Let M be a 2-counter machine. Let Γ = {@, 0, x, y} be a set of 4 diﬀerent
symbols.
A naive but eﬀective way to encode these machines into Turing machines can
be described as below: The state of the Turing machine will contain the state of
the counter machine, and the tape will contain the word @0c1x0c2y where ci is
the value of the i-th counter.

340
E. Jeandel
@0000000000000x0000000000000y
^
Now the behaviour of the Turing machine is quite clear: When it is at the position
of @, it will scan the tape until x/y, deducing whether the i-th counter is zero,
and changing it if necessary (which might need to shift the counters of indices
greater than i), then coming back to @.
The main problem of this simulation is that it always has immortal uninterest-
ing conﬁgurations, corresponding to unbounded searches for one of the delimitor
symbols: If the symbol is not present in the conﬁguration, the search will be
inﬁnite.
To prevent this problem, the idea, originally from Hooper [7], is to use recurs-
ive calls. Suppose we are searching for the symbol a:
@000000000000x0000000000000y
^
?a
If the symbol a is not found in the next 3 cells of the tape, then we write @xy
over the next 3 symbols of the tape, then recursively call the Turing machine.
@@xy000000000x0000000000000y
^
s0
When (if) this nested simulation stops by reading the symbol a, the entire nested
simulation is erased by doing it in reverse. We then can continue the search for
a starting from the next symbol1
@000000000000x0000000000000y
^
?s
We refer the reader to [11] for more details. In particular we need to change the
ﬁrst symbol @ into many diﬀerent symbols to be able to keep into memory the
state of the Turing machine before the recursive call.
An important feature of this construction is the following. If the counter
machine has no periodic conﬁguration (which happens as soon as one counter
keeps increasing during the computation), then any computation of the Turing
machine will contain computations of the counter machine starting from (almost)
anywhere, in the following sense:
Proposition 2. Let c be an immortal conﬁguration of M. Then there exists an
inﬁnite interval I with the following property: for every i ∈I, and every n so
that i+ n ∈I, there exists a time during the computation of the Turing machine
when the cells from i to i + n contains the word @xy0n−2, the head is in position
i, and a recursive computation is started.
1 The construction in [11] actually starts the search from three symbols to the right
(as we already know there is no symbol s in the next two symbols). It is clear that
this does not change anything for their proof, but make Proposition 2 below true.

On Immortal Conﬁgurations in Turing Machines
341
2.3
The Main Construction
We now explain how the construction can be extended to work in our context.
As explained above, we may see the Turing machine as having two tracks (but
only one head): the ﬁrst one has the original construction, and the other one a
biinﬁnite word w over {0, 1}. When the counter machine starts a computation
from position i, it will try to accept the inﬁnite word u ∈{0, 1}N deﬁned by
uj = wi+j. Note that reading the letter of the oracle is quite easy: the position
of the letter x is always where we want to read the oracle (that’s why we chose
the ﬁrst counter to contain the position of the oracle)
Dealing with 7 counters instead of 2 requires no additional machinery, so we
are nearly done.
However one problem remains. Suppose we start from a RCM recognizing the
language S ⊆{0, 1}N. We now look at an inﬁnite run of the Turing machine and
we look at the interval I deﬁned by Proposition 2. There are two cases for I:
– I = [a, ∞[ (possibly with a = −∞) In this case, for all i ≥a, there exist
arbitrary large computation starting from the cell i. This means that for all
i ≥a, the word (uj)j≥i ∈S.
– I = [−∞, a[. In this case, we can say nothing: we cannot ﬁnd any position i
for which we are certain that the word (uj)j≥i is in S.
We thus have to use a last additional trick to make the whole thing work: during
the recursive call of the left searches, instead of using the same machinery, we
shall use another reversible counter machine, and run it in the opposite direc-
tion. That is, for the 2-counter machine, we shall start, e.g., from Y X@ and the
simulation will extend to the left, rather than the right.
Now this new construction starts from two reversible counter machines R1
and R2 and has the following property (the lemma is given for a oracle 2-counter
machine. The change for a 7-counter machine is obvious):
Lemma 4. Let c be an immortal conﬁguration for the Turing Machine M. Then
there exists an inﬁnite interval I = [a, b] (with a = −∞or b = ∞) with the
following property
– for every i ∈I, and every n ≥0 so that i + n ∈I, there exists a time during
the computation of the Turing machine when the cells of the ﬁrst track from
i to i + n contain the word @xy0n−2, and a computation from R1 is started
from i.
– for every i ∈I, and every n ≥0 so that i −n ∈I, there exists a time during
the computation of the Turing machine when the cells from i−n to i contain
the word 0n−2Y X@, and a computation from R2 is started from i.
We can now use all this reﬁned construction to ﬁnally prove our main result:
Theorem 2 (Main Result). For every eﬀective set S, there exists a Turing
machine M so that I(M), the set of immortal conﬁgurations of M, is Muchnik
equivalent to S.

342
E. Jeandel
Proof. We start from S, and use theorem 3 to obtain a recursive language L
so that the three subshifts deﬁned by L are Muchnik equivalent to S. Now let
R1 and R2 be two RCM that recognize respectively words with no factor in L
and words with no factor in the mirror of L, and M be the Turing machine
simulating R1 and R2 as above.
We now look at the immortal conﬁgurations for M.
– Let w be an biinﬁnite word avoiding L. Now it is clear that the conﬁguration
containing w on the second track and @xy on the ﬁrst track, with the head
aligned with @, is an immortal conﬁguration. Indeed, all factors of w (resp.
of its mirror) are not in L (resp. its mirror), so that all computations of R1
and R2 will not halt. Hence, for every w ∈S(L), there exists an immortal
conﬁguration c ∈I(M) so that c is computable in w. In particular for every
u ∈S, there exists c ∈I(M) so that c is computable from u.
– Let x be a immortal conﬁguration of the Turing machine, and I given by
Lemma 4. Let u be the second track of x
• If I = [a, ∞[, then the word w deﬁned by wj = ua+j avoids L, hence
there exists a right-inﬁnite word avoiding L that is Turing reducible to
x. By Muchnik equivalence of S+(L) and S, there exists u ∈S that is
computable from x.
• If I =] −∞, b[, then the word w deﬁned by wj = ub−j avoids the mirror
of L, hence there exists a left-inﬁnite word avoiding L that is Turing
reducible to x. By Muchnik equivalence of S−(L) and S, there exists
u ∈S that is computable from x.
In both cases, we have found a word u ∈S that is computable from x.
⊓⊔
It is important to note that we do not know, given a conﬁguration x, whether the
set I is left or right inﬁnite, as we would need to simulate the Turing machine
to do this. This means that the reduction from x to a right-inﬁnite word is
not uniform in x. As a side note, this means we have only proven a Muchnik
equivalence and not a Medvedev equivalence.
3
Conclusion
We ﬁnish with an application. There exists a well-known encoding of Turing
machines into aﬃne maps [5]. Using this encoding, we can prove:
Corollary 2. There exists a piecewise aﬃne map with rational coeﬃcients and
endpoints from [0, 1]2 to itself so that all computable points converge in ﬁnite time
to (0, 0). However, there exists a noncomputable point that does not converge in
ﬁnite time to (0, 0).
Using the encoding of piecewise aﬃne rational maps into tilings [10], we have in
particular an alternate proof of Myers’ theorem [16]: There exists a tiling system
that produces tilings, but no recursive tilings.

On Immortal Conﬁgurations in Turing Machines
343
Acknowledgements. The author was partially supported by ANR-09-BLAN-
0164.
References
1. Blondel, V.D., Cassaigne, J., Nichitiu, C.: On the presence of periodic conﬁgur-
ations in Turing machines and in counter machines. Theoretical Computer Sci-
ence 289(1), 573–590 (2002)
2. Cenzer, D., Dashti, A., King, J.L.F.: Computable symbolic dynamics. Mathemat-
ical Logic Quarterly 54(5), 460–469 (2008)
3. Cenzer, D., Remmel, J.: Π0
1 classes in mathematics. In: Handbook of Recursive
Mathematics - Volume 2: Recursive Algebra, Analysis and Combinatorics. Studies
in Logic and the Foundations of Mathematics, vol. 2, 139, ch. 13, pp. 623–821.
Elsevier (1998)
4. Cenzer, D., Remmel, J.: Eﬀectively Closed Sets. ASL Lecture Notes in Logic (2011)
(in preparation)
5. Collins, P., van Schuppen, J.H.: Observability of Hybrid Systems and Turing Ma-
chines. In: 43rd IEEE conference on Decision and Control, pp. 7–12 (2004)
6. Delvenne, J.C., Blondel, V.D.: Quasi-periodic conﬁgurations and undecidable dy-
namics for tilings, inﬁnite words and Turing machines. Theoretical Computer Sci-
ence 319, 127–143 (2004)
7. Hooper, P.K.: The Undecidability of the Turing Machine Immortality Problem.
Journal of Symbolic Logic 31(2), 219–234 (1966)
8. Jeandel, E., Vanier, P.: Turing degrees of multidimensional SFTs. submitted to
Theoretical Computer Science, arXiv:1108.1012v2
9. Jockusch, C.G., Soare, R.I.: Degrees of members of Π0
1
classes. Paciﬁc J.
Math. 40(3), 605–616 (1972)
10. Kari, J.: A small aperiodic set of Wang tiles. Discrete Mathematics 160, 259–264
(1996)
11. Kari, J., Ollinger, N.: Periodicity and Immortality in Reversible Computing. In:
Ochma´nski, E., Tyszkiewicz, J. (eds.) MFCS 2008. LNCS, vol. 5162, pp. 419–430.
Springer, Heidelberg (2008)
12. Kurka, P.: On topological dynamics of Turing machines. Theoretical Computer
Science 174, 203–216 (1997)
13. Lind, D.A., Marcus, B.: An Introduction to Symbolic Dynamics and Coding. Cam-
bridge University Press, New York (1995)
14. Miller, J.S.: Two Notes on subshifts. Proceedings of the American Mathematical
Society 140(5), 1617–1622 (2012)
15. Minsky, M.L.: Computation: Finite and Inﬁnite Machines. Prentice-Hall (1967)
16. Myers, D.: Non Recursive Tilings of the Plane II. Journal of Symbolic Logic 39(2),
286–294 (1974)
17. Simpson, S.G.: Mass problems associated with eﬀectively closed sets. Tohoku Math-
ematical Journal 63(4), 489–517 (2011)
18. Simpson, S.G.: Medvedev Degrees of 2-Dimensional Subshifts of Finite Type. Er-
godic Theory and Dynamical Systems (2011)

A Slime Mold Solver
for Linear Programming Problems
Anders Johannson1 and James Zou2
1 Department of Mathematics Uppsala University, P.O. Box 480 751 06 Uppsala,
Sweden
2 School of Engineering and Applied Sciences, Harvard University, Cambridge MA
02138, United States of America
Abstract. Physarum polycephalum (true slime mold) has recently
emerged as a fascinating example of biological computation through mor-
phogenesis. Despite being a single cell organism, experiments have ob-
served that through its growth process, the Physarum is able to solve
various minimum cost ﬂow problems. This paper analyzes a mathemat-
ical model of the Physarum growth dynamics. We show how to encode
general linear programming (LP) problems as instances of the Physarum.
We prove that under the growth dynamics, the Physarum is guaranteed
to converge to the optimal solution of the LP. We further derive an
eﬃcient discrete algorithm based on the Physarum model, and experi-
mentally verify its performance on assignment problems.
1
Introduction
How do biological systems process information and solve optimization problems?
There has been a growing interest to understand these questions within a compu-
tation framework. The agenda is two fold: ﬁrst, can we understand and analyze
biological systems in terms of algorithms; and second, can we design new al-
gorithms inspired by biology. Prominent examples of such “natural algorithms"
include ﬂocking algorithms motivated by the collective behavior of birds and
ﬁsh, and swarming algorithms motived by ant foraging behavior [1–3].
In this paper, we analyze the morphogenesis of Physarum polycephalum (true
slime mold) as a natural algorithm. The Physarum is a single cell organism with
multiple nuclei. Recent experiments have shown that it exhibits a surprising
ability to solve complex optimization problems [4, 5]. In one set of experiments,
researchers place food at various places akin to cities on a map [5]. As it grows,
the Physarum is able to process the input (food) in a de-centralized manner,
and converge to the distance minimizing network connecting these food sources1.
Mathematicians and biologists have proposed a dynamical systems model that
captures essential behaviors of Physarum growth [6]. Simulation and analysis of
this model give mechanistic insight of how such a simple organism can solve the
Shortest Path Problem [6–9].
1 An experiment from [4] is http://www.youtube.com/watch?v=BZUQQmcR5-g
S.B. Cooper, A. Dawar, and B. Löwe (Eds.): CiE 2012, LNCS 7318, pp. 344–354, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

A Slime Mold Solver for Linear Programming Problems
345
Here we show how a general Linear Programming Problem can be encoded as
an instance of a Physarum. We prove that under the Physarum growth model,
the appropriate quantity is guaranteed to converge to the optimal solution of
the LP. Our result draws on new analysis techniques inspired from the negative
cost cycle algorithm. In addition, we derive a discrete algorithm for solving LPs
from the dynamical systems. The algorithm is eﬃcient to implement. We apply
it to solve instances of the Assignment Problem, and report simulation results.
2
The Generalized Physarum Solver
A Model of Physarum Growth. A Physarum contains a network of veins
that it uses to transport nutrients across its body. The veins are modeled as a
graph G [6]. The vertices V are where multiple veins meet, and an edge e ∈E
is a segment of the vein. Each edge is a tube described by length ce and cross-
sectional area σe. Vertex i is associated with pressure pi. We assume that G is
directed. So edge ij goes from vertex i to j. The ﬂow on ij is xij = σij
pi−pj
cij . G
may contain multi-edges. In particular, a bidirectional edge can be modeled by
two edges: ij and ji, with distinct σij and σji.
The Physarum growth model describes the time evolution of σij and pi on G.
The expansion and contraction of a vein segment is governed by:
d
dtσij(t) = σij(t)pi −pj
cij
−σij(t).
(1)
The ﬁrst term on the left captures the positive feedback: the greater the ﬂow
on ij, the more the vein segment will expand. If pi < pj implying that xij < 0,
then σij shrinks. The second term reﬂect the natural decay of the Physarum.
The pressure pi is obtained from ﬂow conservation: the ﬂow into a vertex
(except for source or sink) equals the ﬂow out from it. Let A be the |V|x|E|
incidence matrix of G. The conservation equation is Ax(t) = b, where bi = 0
for all i except for bs = 1 and bu = −1. Vertices s and u are the source and
sink. The conservation equation can be rewriten as AW(t)ATp(t) = b, where
W(t) = diag(σe(t)/ce). If A has full row rank, then p(t) can be uniquely solved
for.
The Physarum growth dynamic thus has two coupled processes:
1. The vein sizes σij(t) grow according to Eqn. 1, which depends on p(t).
2. For a given set of σij(t), ﬂow conservation uniquely determines p(t).
Under this growth model, {xij(t)} evolves on G. As t →∞, the ﬂow x(t)
converges to 1 on edges in the shortest s-u path, and to 0 on all other edges [8].
LP Problems. A general linear programming (LP) problem is to min cTx
subject to the contraints Ax = b, x ≥0. Let Φ = {x : Ax = b} and Φ+ =
{x : Ax = b, x ≥0}. We work in cases where Φ+ forms a bounded polytope
and A has full row rank, which are common assumptions in LP applications. We
assume that the LP has a unique optimal solution. Any small perturbation to c
leads to unique optimum.

346
A. Johannson and J. Zou
Keeping the graph notations, we label columns of A by e ∈E. Each column
Ae is analogous to an edge and is associated with a “conductance" σe ≥0, which
is an auxiliary variable. Every constraint equation in Ax = b corresponds to a
vertex and is associated with a pressure pi. We deﬁne the generalized Physarum
Solver by the dynamics
d
dtσe = σe(ψe −1)
(2)
where ψe = (AT
e p)/ce is the “pressure gradient”and p satisﬁes Kirchhoﬀ’s Law
AWATp = b with W = diag(σe/ce).
(3)
As before, the “ﬂow" is given by xe(t)=σe(t)ψe(t). Note that Ax=AWATp=b,
though xe(t) could be negative if ψe < 0. So x(t) ∈Φ.
Duality. The pressure, pi, is the dual variable associated with each constraint.
If the dynamical system reaches a stationary point, then for all e, 0 =
d
dtσe =
σe( AT
e P
ce
−1). If xe > 0, then σe > 0 and stationarity implies AT
e P = ce. This
is precisely the complementary slackness condition. The stationary point of the
Physarum Solver is the optimal solution to the LP.
The main result of the paper is the following theorem.
Theorem 1. Suppose the LP problem has a unique optimal solution. Then from
any positive initial conﬁguration σ(0) > 0, the ﬂow of the Physarum Solver x(t)
converges exponentially fast to the optimal solution.
3
Proof of Convergence
3.1
LP with Positive Costs
We ﬁrst assume that ce > 0 ∀e. In the next section we shall show how to extend
the proof to general c. Here we give the outline of the proof; the technical details
are in the Appendix.
We multiply Eqn. 2 by et and integrate to obtain
σ(t) = (1 −e−t)˜x(t) + e−tσ(0)
(4)
with
˜x(t) .=
1
1 −e−t
 t
0
x(s)e−(t−s)ds.
(5)
Since ˜x(t) is a weighted average of x(s) for s ≤t and each x(s) satisﬁes Ax(s) =
b, we also have A˜x(t) = b.
Deﬁnition 1. A circulation is a |E| dimensional vector γ such that Aγ = 0.
There must exists e such that γe < 0, for otherwise the feasible region Φ+ would
be unbounded. Let γ−.= {e : γe < 0} denote the negative edges of γ. Similarly
deﬁne γ+ .= {e : γe > 0}. We say a circulation has negative cost if cTγ < 0.

A Slime Mold Solver for Linear Programming Problems
347
Lemma 1. In a bounded LP problem, x is an optimal solution if and only if
there is no negative cost circulation γ such that γ−⊆supp(x).
Our strategy is to prove that under Eqn. 2, the ﬂow on γ−becomes exponentially
small for any negative circulation γ. We start with the following Lemma.
Lemma 2. Let γ be a negative cost circulation. Then for all ϵ > 0, ∃e with
γe < 0 and t such that σe(t) < ϵ.
Proof. Let Zγ .= e−
e∈E γece log σe. Since 
e∈E γeceψe = pTAγ = 0,
d
dtZγ = Zγ

e∈E
γece(1 −ψe) = ZγcTγ < 0.
(6)
Therefore Zγ decays exponentially. On the other hand,
Zγ(t) = Πe∈γ−σe(t)|γe|ce
Πe∈γ+σe(t)γece ,
(7)
and by Lemma 4 (Appendix), all σe are bounded above for large t and hence the
denominator is bounded above as t increases. As the ratio decays exponentially,
the numerator must decay exponentially and at least one of the terms in its
product must be exponentially small.
The next lemma shows that we can clean up a solution x of Ax = b by essentially
setting xe to 0 if xe is suﬃciently small.
Lemma 3. Given x ∈Φ, and S ⊆E. There are constants M1 and K such that
if ∀e ∈S, |xe| < ϵ|S|+1 < M1/2m for some ϵ < min{1, M1
2 ,
1
2K } , then there
exists y ∈Φ satisfying:
• ||x −y||∞< Kϵ,
• supp(y) ⊆supp(x)\S,
• xeye ≥0 ∀e.
Combining the previous two lemmas, we prove that x(t) converges to the unique
optimal solution under the Physarum dynamics.
Proof of Theorem 1 for LP with Positive Costs
Consider the time-averaged solution ˜x(t) deﬁned above. Let S = {e : ˜xe(t) < 0}.
From Eqn. 4 and the fact that σ(t) is bounded for large t, it follows that
˜x(t) = σ(t) + O(e−k1t)
(8)
for some constant k1. Since σ(t) > 0, we have |˜xe(t)| ∼O(e−k1t), ∀e ∈S. For
suﬃciently large t so that the inequality requirements of Lemma 3 are satiﬁstied,
we obtain a y(t) such that
y(t) = ˜x(t) + O(e−k2t)
(9)

348
A. Johannson and J. Zou
for constant k2. Moreover y(t) ≥0 is feasible.
Let ˆy denote the optimal feasible solution of the LP. Let e ∈supp(y)/supp(ˆy).
Then by Lemma 6 (Appendix), there is a basic feasible solution τ such that
ye ≤k3 min{ye′ : e′ ∈supp(τ)}
(10)
for constant k3. Consider γ = ˆy−τ. This is a circulation since Aγ = 0, and it has
negative cost. By Lemma 2, ∃e′ ∈supp(τ) such that σe′(t) decays exponentially.
Eqns. 8 and 9 implies that ye′(t) ∼O(e−k4t) for some constant k4. Combined
with Eqn. 10, this implies that ye ∼O(e−k4t) for all e ∈supp(y)/supp(ˆy).
Applying Lemma 2 again to remove all such e, we ﬁnd ˜y ∈Φ+ such that
˜y = y(t) + O(e−k5t) = ˜x(t) + O(e−k2t) + O(e−k5t).
(11)
But ˜y has support contained in supp(ˆy), implies ˜y = ˆy since the optimal solution
is a unique basic feasible solution. This concludes that ˜x(t) = ˆy(t) + O(e−k6t).
As x(t) is continuous, the weighted average ˜x(t) converges to ˆy exponentially
implies x(t) also converges to ˆy exponentially.
⊓⊔
3.2
LP with General Costs
First we show how to deal with when ce = 0 for some e. The assumption that
Φ+ is a bounded polytope implies ∃M such that xe < M for all e ∈E and
x ∈Φ+. Given {A, b, c}, consider the LP {A, b,ˆc}, where ˆce = ce if ce > 0 and
ˆce = ϵ if ce = 0. This problem can be solved by the Physarum Solver since ˆc > 0.
Let x1 be the optimal solution of {A, b, c} and let x2 be its second lowest cost
solution. Deﬁne the gap δ = cTx2 −cTx1 and set ϵ <
δ
|E|M . It’s easy to see
that x1 is also the optimal solution of {A, b,ˆc}. Hence it suﬃces to apply the
Physarum Solver to solve {A, b,ˆc}.
Now consider if ce < 0 for some e ∈U ⊆E. For each such e add a new variable
ˆe and the constraint equation xe + xˆe = M. Modify the costs by setting c′
e = 0
and c′
ˆe = |ce| for e ∈U. The new LP problem {A′, b′, c′} can by solved by the
Physarum Solver. There is a one-to-one mapping between the feasible sets of
{A, b, c} and {A′, b′, c′} that maps the optimal solutions to each other.
3.3
Relations to Other LP Solvers
The Physarum dynamics does not maintain primal feasibility. Even though
Ax(t) = b always holds, the gradient AT
e p
ce
could be negative, resulting in xe(t) <
0 for some t. Though our proof shows that x(t) stays exponentially close to
the feasible polytope Φ+. Similarly, the potentials may violate dual feasibility:
AT
e p(t) > ce for some e.
In the Physarum solver, x(t) does not live on the boundary of Φ+ and in this
sense is more related to interior point methods than to the Simplex Algorithm.
Unlike interior point methods, the Physarum is not guaranteed to monotonically
reduce the cost cTx(t) or increase the dual objective pTb, and is not restricted
to be inside the polytope [10].

A Slime Mold Solver for Linear Programming Problems
349
The Physarum is conceptually most similar to the negative cost cycle algo-
rithm. In this algorithm, a negative cost cycle is identiﬁed and a ﬂow is pushed
on this cycle until one of the directed edges has ﬂow 0 [10]. As our proof shows,
this is also the Physarum’s strategy. Think of the graph case for intuition. If
there is a negative cost oriented cycle γ, such that all edges ij of γ are unsatu-
rated: pi −pj < cij. Then σij increase for all ij ∈γ, and an additional negative
cost ﬂow is pushed {xij(t + △) −xij(t) : ij ∈γ}.
4
The Discrete Physarum Solver
4.1
Finite Diﬀerence Approximation
The continuous dynamics of the generalized Physarum Solver makes it diﬃcult
to be implemented and analyzed as an algorithm. We use ﬁnite diﬀerence method
to discretize Eqn. 1
σe(t + △) ≈σe(t) +
AT
e p
ce
−1

σe(t)△.
(12)
By setting △= 1, we obtain a simple discrete algorithm.
Algorithm 1. Discrete Physarum Solver
INPUT: LP problem instance (A, b, c)
1: Initialize {σe(0) > 0, e ∈E}
2: while σ(t) not converged do
3:
Compute p(t) by solving AWATp = b, where W = diag( σe(t)
ce ).
4:
if ATp(t) > 0 then
5:
σe(t + 1) = σe(t)
AT
e p(t)
ce
.= x(t)
6:
else
7:
σe(t) = ϵ
8:
end if
9:
t = t + 1
10: end while
In the algorithm, ϵ ≪1 is a positive constant. We say σ has converged if
σ(t + 1) −σ(t) is small. We check for whether the algorithm has converged
to the optimal solution by testing for approximate complementary slackness:
| AT
e p
ce −1| < δ, ∀e for some small δ. The most computationally intensive step is to
solve Kirchhoﬀ’s Equation for p(t). Recent progress shows this can be done in
near-linear time in |E| [11]. Because of the large ﬁnite diﬀerence approximation
we have taken, the candidate solution x(t) is not guaranteed to converge to the
optimal. We show via simulation that the Discrete Physarum Solver does ﬁnd
the optimum for the special class of Assignment problems.

350
A. Johannson and J. Zou
4.2
The Assignment Problem
We test the discrete Physarum algorithm on a standard class of LP problems:
the Assignment Problem [10]. In the Assignment Problem, there are N workers
and N tasks. Every worker and task pair has an associated cost. The goal is to
assign each worker to a unique task such that the total cost incurred is minimal.
2
10
20
30
40
50
0
10
20
30
40
50
60
N
Number of iterations
 
 
25th percentile
median
75th percentile
Fig. 1. Number of iterations to ﬁnd the optimal solution versus the size of the Assign-
ment Problem. N is the number of tasks. For each N, plotted are the 25th, median,
and 75th percentile of the number of iterations.
This can be represented by a complete bipartite graph G with worker nodes
and task nodes. Edges are directed from worker nodes to task nodes. Edge ij
connecting worker i and task j is associated a cost cij. A is the incidence matrix,
and bi = 1 if i is a worker vertex; bj = −1 for task vertex. The update is
σij(t + 1) = σij(t) pi−pj
cij
= xij(t) if pi > pj; else σij(t + 1) = ϵ. To check for
optimality, we derive a hard assignment from x(t). For each i, identify ji =
arg maxj{xij(t)}. Set xiji = 1 and to 0 on all other edges. Then we test for
complementary slackness.
For each N from 2 to 50, we randomly generate 1000 weighted complete
bipartite graphs. The costs are iid samples from the uniform distribution. We
applied the Discrete Physarum Algorithm to each of these assignment problems.
After every iteration of the algorithm, we check for optimality. If the optimal
solution is found, the number of iterations so far is recorded; otherwise the
algorithm continues.
In all the assignment problem instances tested, the Discrete Physarum Solver
always converged to the optimal solution. For each N, we plot the 25th, median,
and 75th percentile of the number of iterations the Discrete Physarum Solver

A Slime Mold Solver for Linear Programming Problems
351
took before reaching the optimal solution. The median number of iterations
grows roughly linear in the size of the problem.
Acknowledgments. The authors would like to thank David Parkes, Devavrat
Shah, and David Sumpter for helpful discussions and feedback on the paper.
References
1. Chazelle, B.: Natural Algorithms. In: Proceedings of the 20th Symposium of Dis-
crete Algorithms (2009)
2. Sumpter, D.: Collective Animal Behavior. Princeton University Press (2010)
3. Camazine, S., Deneubourg, J., Franks, N., Sneyd, J., Theraulaz, G., Bonabeau, E.:
Self-Organization in Biological Systems. Princeton University Press (2003)
4. Tero, A., Takagi, S., Saigusa, T., Ito, K., Bebber, D., Fricker, M., Yumiki, K.,
Kobayashi, R., Nakagaki, T.: Rules for biological inspired adaptive network design.
Science 327(5964), 439–442 (2010)
5. Nakagaki, T., Yamada, H., Toth, A.: Maze-solving by an amoeboid organism. Na-
ture 407, 470 (2000)
6. Nakagaki, T., Tero, A., Kobayashi, R.: A mathematical model for adaptive trans-
port network in path ﬁnding by true slime mold. Journal of Theoretical Biol-
ogy 244(4), 553–564 (2007)
7. Miyaji, T., Ohnishi, I.: Physarum can solve the shortest path problem on Rieman-
nian surface mathematically rigorously. International Journal of Pure and Applied
Mathematics 47(3), 353–369 (2008)
8. Ito, K., Johansson, A., Nakagaki, T., Tero, A.: Convergence properties for the
Physarum solver. arXiv:1101.5249v1 (2011)
9. Bonifaci, V., Mehlhorn, K., Varma, G.: Physarum can compute shortest paths. In:
Proceedings of the 23th Symposium of Discrete Algorithms (2012)
10. Bertsimas, D., Tsitsiklis, J.: Introduction to Linear Optimization. Athena Scientiﬁc
(1997)
11. Christiano, P., Kelner, J., Madry, A., Spielman, D., Teng, S.: Electrical ﬂows,
Laplacian systems, and faster approximation of maximum ﬂow in undirected
graphs. In: Proceedings of the 43rd ACM Symposium on Theory of Computing
(2011)
A
Proofs of Technical Lemmas
Proof of Lemma 1
Suppose there exists a negative cost circulation γ. Let ϵ = maxe{ xe
|γe|}. Then
y = x + ϵγ satisﬁes y ≥0, Ay = b, and y has lower cost than x. This implies
x is not an optimal solution. Conversely, suppose x is not optimal. Let y be the
optimal solution and γ = y −x. Then γ is a negative cost circulation.
⊓⊔
Lemma 4. For all e, σe(t) is bounded for large t.

352
A. Johannson and J. Zou
Proof. Rearranging Eqn. 4, ˜x(t) > −e−tσ(0). Let S(t) = {e : ˜xe(t) < 0}. Then
for t suﬃciently large, the condition for Lemma 3 is satisﬁed and there exists y(t)
such that y(t) ≥0, Ay(t) = b, and ||˜x(t) −y(t)||∞< Ce−t for some constant
C. The vector y(t) lies in the feasible region of the LP, which we assume to be
a bounded polytopy, implies ||y(t)||∞is bounded. This implies that ||˜x(t)|| is
bounded and hence σ(t) is bounded.
Deﬁnition 2. Given A and x, a positive re-orientation of A with respect to
x is A′ and x′ such that
• x′
e = |xe| for all e
• A′
e = Ae for e where xe ≥0
• A′
e = −Ae for e where xe < 0
Deﬁnition 3. A positive cycle is a vector γ indexed by e ∈E such that
Aγ = 0, γe ≥0, ∀e, and supp(γ) is minimal. Since γ is ambigous up to a con-
stant, we set mine∈supp(γ){γe} = 1. The set of positive cycles of A is denoted by
pcyc(A).
Let bfs(A) denote the set of basic feasible solutions of the LP Ax = b, x ≥0.
Lemma 5. Given A, x ∈Φ and e ∈E such that |xe| < ϵ < M1/2. Then ∃
y ∈Φ such that:
• ||x −y||∞< 3ϵ M2
M1 ,
• supp(y) ⊆supp(x)\e,
• xeye ≥0 for all e,
Here
M1 = min
A′
min
τ∈bfs(A′)∪pcyc(A′){τe : e ∈E and τe > 0}
(13)
M2 = max
A′
max
τ∈bfs(A′)∪pcyc(A′){τe : e ∈E}
(14)
where min and max are over all re-orientation A′ of A.
Proof. After re-orientation, we can assume wlog that x ≥0. Then x lies in
the polyhedron deﬁned by A′x = b and x ≥0. We can express x as a con-
vex linear combination of basic feasible solutions and positive cycles [10]: x =

τ∈bfs(A′)∪pcyc(A′) cττ, such that cτ > 0 and 
τ∈bfs(A′) cτ = 1. We can split
the sum into two groups of τ depending on if τe = 0 or τe > 0:
x =

τ:τe=0
cττ +

τ:τe>0
cττ.
(15)
Let α = 
τ:τe>0 cτ. Then
αM1 ≤

τ:τe>0
cττe = xe < ϵ < M1
2
(16)

A Slime Mold Solver for Linear Programming Problems
353
implying α <
ϵ
M1 < 1
2. Therefore there is at least one bfs τ such that cτ > 0 and
τe = 0. Deﬁne
y =
1
1 −α

τ∈bfs(A′):τe=0
cττ +

τ∈pcyc(A′):τe=0
cττ.
(17)
Then y is the convex combination of at least one bfs and positive cycles, and
is also a feasible solution. In particular xeye ≥0 for all e. It’s also clear that
supp(y) ⊆supp(x). Finally,
||x −y||∞≤
α
1 −α||

τ∈bfs(A′):τe=0
cττ||∞+ ||

τ∈pcyc:τe>0
cττ||∞.
(18)
Since α < 1
2,
α
1−α < 2α. Moreover,
||

τ∈bfs(A′):τe=0
cττ||∞< M2

τ∈bfs(A′):τe=0
cτ < M2
(19)
, and || 
τ:τe>0 cττ||∞< M2α. We have
||x −y||∞≤3αM2 < 3ϵM2
M1
.
(20)
We use induction to remove multiple small edges.
Proof of Lemma 3
Let M1, M2 be the same as in the previous lemma, and set K = 3 M2
M1 . Let S =
{e1, e2, ..., e|S|}. It’s clear that properties 2 and 3 are satisﬁed during induction,
so we focus on 1. In the base case, we apply the previous lemma to e1 to construct
y1, such that ||x −y1||∞< ϵ|S|+1K. The induction claim that we shall prove is:
if ∃u that removes {e1, e2, ...el−1} from x, and ||x −u||∞< Kϵn, n ≤|S| + 1,
then we can construct y such that y removes {e1, ..., el} from x, and ||x −y||∞<
Kϵn−1. We can then iterate |S| times to construct y such that ||x −y||∞< Kϵ.
To prove the induction claim, observe that ||x −u||∞< Kϵn < ϵn−1
2 . Since
n ≤|S| + 1, xel < ϵ|S|+1 < ϵn and
uel < xel + ϵn−1
2
< ϵn−1 < M1
2 .
(21)
Apply Lemma to remove el from u and construct y such that ||y −u||∞<
Kϵn−1. Then
||x −y||∞< ||x −u||∞+ ||y −u||∞< Kϵn + Kϵn−1 < Kϵn−1.
(22)
⊓⊔
Let βmin = minτ∈bfs{τe : e ∈supp(τ)} and βmax = maxτ∈bfs{τe : e ∈supp(τ)}.

354
A. Johannson and J. Zou
Lemma 6. Let x ∈Φ+ and e ∈E be in the support of x. Then there is a basic
feasible solution ˆτ ∈Φ+ such that
min x(supp(ˆτ)) ≥βmin
βmax
xe
|supp(x)|
(23)
Proof. Since Φ+ is a bounded polytope, x is a convex linear combination of basic
feasible solutions. In particular, there exists τ ∈bfs such that supp(τ) ⊆supp(x).
Let cτ = min{ xe
τe : e ∈supp(τ)}. Then x −cττ ∈Φ+ and has support strictly
contained in the support of x. Iterating this shows that we can write x =  cττ,
where the number of positive cτ is at most |supp(x)|. Let ˆτ = argmaxτ{cττe},
then
cˆτβmax|supp(x)| ≥cˆτ ˆτe|supp(x)| ≥xe.
(24)
Furthermore
min x(supp(ˆτ)) ≥cˆτ min ˆτ(supp(ˆτ)) ≥cˆτbmin
(25)
and the desired inequality follows.

Multi-scale Modeling
of Gene Regulation of Morphogenesis
Jaap A. Kaandorp, Daniel Botman, Carlos Tamulonis, and Roland Dries
Faculty of Sciences, Universiteit van Amsterdam, Science Park 904, 1098 XH
Amsterdam, The Netherlands
Abstract. In this paper we demonstrate a spatio-temporal gene regu-
latory network for early gastrulation in the sea anemone Nematostella
vectensis. We measure gene expression during early gastrulation using a
gene expression quantiﬁcation tool. We measure gene expression during
early gastrulation when the embryo is more or less radial symmetrical
and where there is only one body axis (the aboral - oral body axis in the
case of Nematostella), allowing us, to use a one-dimensional model. The
shape changes are induced by mechanical forces that are deﬁned as rules
or explicitly coupled to genetic regulation. The morphological evolution
of the Nematostella embryo during gastrulation is described by a cell-
based model, which can potentially be coupled with the one-dimensional
gene regulatory network.
1
Introduction
A spectacular achievement in modern biology is the discovery of gene regula-
tory networks involved in pattern formation during embryo development [4]. An
important next step is to understand the complex regulatory structure and dy-
namics of these networks, which in turn are closely coupled with cell dynamics.
The network dynamics vary across space and time and are inﬂuenced by biome-
chanical events (e.g., formation of cell layers, cell migration, cell death, and cell
division). To provide insight into the dynamical behavior of such complex regula-
tory systems, detailed models are required, which can be tested against available
experimental observations. Several modelling formalisms have been proposed to
simulate the dynamics of developmental gene regulatory networks [5, 13, 22].
Here, we focus on spatio-temporal models of gene regulation, which capture
quantitative aspects of pattern formation during embryo development [12, 20].
However, obtaining such models is not straightforward. First of all, it is often not
clear which network-modelling formalism should be used. Due to the biochem-
ical complexity of eukaryotic transcription, it is currently impossible to derive
network models from ﬁrst principles [20]. Instead, many phenomenological for-
malisms have been used, from Boolean models to diﬀerential equation models
and stochastic formalisms [5]. There are currently no widely applicable guide-
lines for the choice of model, which depends on the speciﬁc problem under study,
as well as the availability and quality of gene expression data.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 355–362, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

356
J.A. Kaandorp et al.
Another important research problem is the determination of model parame-
ters. Even models of moderately sized networks contain a large number of param-
eters, which determine production, diﬀusion, decay rates, regulatory interaction
rates, as well as the regulatory topology of a gene network. These parameters are
often diﬃcult (if not impossible) to measure. Instead, they have to be inferred
by ﬁtting models to data using global, non-linear optimization. This type of
reverse-engineering approach poses a number of signiﬁcant challenges. One ma-
jor issue for parameter inference is that the observed dynamical behaviour of the
system can often be explained by multiple distinct regulatory mechanisms. This
generally happens because the optimization problem is ill-posed or insuﬃciently
constrained by data [2]. Alternatively, parameters can be diﬃcult to determine
due to correlations between them [2, 10]. Model validation based on additional
experimental evidence is required to decide which of the alternative mechanisms
is applicable to the real biological system. This is often time-consuming and
technically challenging. Therefore, it is essential to decrease the number of alter-
native predictions that need to be tested experimentally. One way of achieving
this is to take additional criteria into account in the identiﬁcation of the best
explanatory model. Usually, the accuracy with which a model reproduces ob-
served expression patterns is measured by a cost function based on the sum
of squared diﬀerences between model and data (single-objective optimisation).
Multi-objective optimization, however, can take other desirable properties into
account, such as model robustness and known information about the underlying
genetic regulatory network, to constrain the optimization procedure and produce
fewer but more realistic solutions.
The Drosophila (fruit ﬂy) gap gene interaction network has been derived in
quite some detail, partly through the study of mRNA hybridizations during early
embryogenesis [9, 12, 20]. The Drosophila case study is particularly convenient
for spatial gene expression analysis, because the embryos remain in the same
shape during their early development and gene expression is measured along a
nearly straight axis. Early embryogenesis in Drosophila occurs in a cigar-shaped
embryo-sac with quasi-radial symmetry that does not change shape and in which
there are no moving cells. Gene expression can therefore be quantiﬁed along the
main axis of symmetry (the anterior posterior axis) and the whole system can be
described with a one-dimensional spatio-temporal regulatory network. However,
nearly all other animals rapidly change their morphology by cell division, cell
movement, blastula formation and gastrulation. In order to quantify gene expres-
sion for comparison and analysis gene expression patterns should be mapped to
their tissue shape.
In this paper we demonstrate a spatio-temporal gene regulatory network for
early gastrulation in the sea anemone Nematostella vectensis. We measure gene
expression during early gastrulation using a gene expression quantiﬁcation tool
developed by [6]. We shall model formation of gene expression patterns in static
or changing geometries. Depending on the stage and symmetry of the gene ex-
pression patterns of interest this can be done in 1D, 2D or 3D. In this paper we
measure gene expression during early gastrulation when embryo is more or less

Multi-scale Modeling of Gene Regulation of Morphogenesis
357
radial symmetrical and where there is only one body axis (the aboral-oral body
axis in the case of Nematostella), allowing us, again, to use a one-dimensional
model. The shape changes are induced by mechanical forces that are deﬁned
as rules or explicitly coupled to genetic regulation. The morphological evolu-
tion of the Nematostella embryo during gastrulation is described by a cell-based
model [24], which can potentially be coupled with the one-dimensional gene
regulatory network.
2
Methods
Gene Expression Patterns in Nematostella Vectensis
For quantifying gene expression during gastrulation we have used a number of
published in situ hybridisations of Nematostella embryos (Fig. 1). Fig. 1A shows
Anthox1 expression [8] in the blastula stage. Anthox1 expression sets up the
ﬁrst body axis (the aboral- oral axis) in the system and breaks the symmetry of
the blastula stage. Fig 1B shows snail expression [16], snail is expressed exactly
opposite of Anthox1 expression around the future oral opening. In both pictures
the shape of the embryo is approximated with a spline [6] and the aboral site is
located at the left and the oral site at the right.
Fig. 1. Two examples of quantiﬁed in situ hybridisations of Nematostella vectensis. The
left picture shows Anthox1 expression (the original picture can be found in [8]. The
right picture shows snail expression (the original picture is described in [16]. In both
pictures the shape of the embryo is approximated with a spline and in both pictures
the aboral site is located at the left and the oral site at the right.

358
J.A. Kaandorp et al.
Modelling Gene Regulatory Networks
A number of diﬀerent modelling formalisms have been proposed to simulate the
dynamics of developmental gene regulatory networks [5]. Boolean network mod-
els [13] or generalised logical models (e.g., [7, 22] have been applied to model
pattern formation and cell-cycle regulation in a qualitative manner. Here we
focus on spatio-temporal models of gene regulation, which capture quantitative
aspects of pattern formation during embryo development. Here we use the con-
nectionist gene circuit model introduced by Reinitz and colleagues [12,17,19,23]:
dgia
dt
= Raφ{
Ng

b=1
W abgb
i + ha} −λga
i + Da∇2ga
i
(1)
where a and b denote gene products (targets and regulators respectively) and
i the cell (or nucleus) number (i.e., position in space). In Eq. 1 gene product
concentrations depend on three main factors: (1) The ﬁrst term describes the
regulation of gene product synthesis in each cell. The set of genes is assumed
to be identical in each cell. This is represented by the weight matrix W whose
elements W ab characterize the regulatory eﬀect of gene b on gene a. Ng denotes
the total number of genes in the model. φ is a sigmoid function with range
[0,1], Ra is the production rate, ha is a threshold parameter, which describes the
expression of gene product a in absence of any regulators b. (2)The second term
represents the decay of the gene products and (3)the third term represents the
exchange of diﬀusible products between neighbouring cells.
Parameter Estimation by Optimisation
In the connectionist model regulatory interactions are represented by a large
number of model parameters, such as kinetic constants, regulatory weights, Hill-
coeﬃcients and/or threshold parameters. The precise mathematical formulation
depends on the level of detail considered in each gene regulatory model, but they
all have in common that these parameter values are often diﬃcult or impossible
to measure. Instead, they have to be inferred by ﬁtting models to experimen-
tally measured gene expression data using global, non-linear optimization. The
standard optimisation approach minimizes a single cost function that represents
the quality of the data-ﬁt (single-objective optimisation). In most studies, pa-
rameters are estimated by minimizing the sum of squared diﬀerences between
model and data in which the residual function is given by:
E(θ) =

i,a,t
(ga
i (t, θ)model −ga
i (t)data)2
(2)
where the summation is performed over all cell, genes and time points for which
we have data. Box constraints and penalty functions are used to reduce the
search space and also to ensure that the parameter values are within biophysical
limits [9,12].

Multi-scale Modeling of Gene Regulation of Morphogenesis
359
Cell-Based Modelling
The model for gene regulation can be coupled with a physically based model of
cells. A great variety of cell-based modelling approaches have been published with
levels of detail varying from point cells to complex polygons [1,3,11,15,18,21].
Important aspects in early gastrulation are cell shape and cell membrane
properties that mediate cell-cell interactions in a complex manner where adhe-
sion strength may vary spatially. This cell polarity can also be utilized to direct
cell division and cell movement. To each cell or region a gene regulatory network
can be attributed and also signalling between cells, regions and the environ-
ment can be modelled. At early stages the number of cells is limited and cell
migration mainly occurs during gastrulation. For this stage, we use a cell-based
model, in which each cell is represented individually by a polygon (2D, [24]) or
a polyhedron (3D, unpublished data).
3
Results and Discussion
The in situ hybridisations (Fig. 1) are decomposed into segments that are located
along an approximately circular region. We have quantiﬁed gene expression in
every segment in Fig 1. If we start measuring gene expression at the aboral site,
we can transform the expression patterns shown in Fig 1 into the one-dimensional
plot shown in Fig. 2. The x-axis represents a spatial axis in the system and the
y-axis the quantiﬁcation of gene expression in every segment along the circular
decomposition constructed in the blastula stage. Fig. 2 summarizes the informa-
tion from several published in situ hybridizations during the early gastrulation
of Nematostella [8,14,16,25].
The quantiﬁed gene expression patterns shown in Fig. 2 can be ﬁt eﬃciently
to the connectionist gene expression model with a genetic algorithm-based op-
timisation method [9] in which the error function shown in Eq. 2 is minimized.
Fig 3 shows one (preliminary) approximation of the gene expression pattern in
Fig. 2. It is a dynamical solution: ﬁrst there is a maternal gradient beta-catenin,
followed by the formation of an Anthox1 gradient and ﬁnally a gradient of snail
expression around the oral pore.
Currently we are working on coupling the model of gene regulation with the
cell-based model of gastrulation. Fig. 4 shows an example of the 2D cell-based
model of gastrulation in Nematostella vectensis [24]. The simulation starts with a
series of wedge-shaped cells in a circular conﬁguration (top left picture in Fig. 4).
In the blue cells there is no snail expression, while in the red region snail is being
expressed. Snail expression radically changes the biomechanics of the cells: in the
red cells, cell adhesion decreases while the red cells remain attached at the apical
ends. This diﬀerent biomechanical behaviour results into the formation of bottle
cells and apical constriction [24]. Furthermore the red cells are becoming motile
and form ﬁlipodia which zipper with the rigid layer of blue cells. These biome-
chanical events - the apical constriction and the zipping up by the ﬁlopodia with
the blue cells - are suﬃcient to obtain a fully gastrulated structure (right bottom

360
J.A. Kaandorp et al.
Fig. 2. Quantiﬁed gene expression of Anthox1, beta-catenin, FoxB and snail in in situ
hybridisations of the blastula stage of Nematostella vectensis [8, 14, 16, 25]. Gene ex-
pression was measured in every segment along a circular decomposition at the blastula
stage (Fig. 1). The x-axis represents the spatial axis in the system and the measure-
ments start at the aboral segment in Fig. 1. The y-axis represents the normalized
quantiﬁcation of the gene expression in every segment along the decomposition.
Fig. 3. Simulated gene expression using the connectionist model (Eq. 1). This solution
is based on the data shown in Fig. 2. The picture shows simulated gene expression after
10 hours. The x-axis is a spatial axis in the system (similar like in Fig. 2).

Multi-scale Modeling of Gene Regulation of Morphogenesis
361
Fig. 4. 2D cell-based model [24] of gastrulation in Nematostella vectensis. The colours
indicate regions with diﬀerent simulated gene expression.
picture in Fig. 4) where two cell layers consisting of endodermal (red cells) and ec-
todermal cells are formed. In future work, we shall combine both the connectionist
gene regulatory model with the mechanical model of gastrulation to obtain a full
genetically regulated cell-based model of the developing Nematostella embryo.
Acknowledgements. Carlos Tamulonis was funded by the Funda¸c˜ao para a
Ciˆencia e a Tecnologia Portugal (grant SFRH/BD/29117/2006. Daniel Botman
was funded by the FP7 project BioPreDyn.
References
1. Armstrong, N.J., Painter, K.J., Sheratt, J.A.: A continuum approach to modelling
cell-cell adhesion. J. Theor. Biol. 243, 98–113 (2006)
2. Ashyraliyev, M., Fomekong Nanfack, Y., Kaandorp, J.A., Blom, J.G.: Systems
biology: Parameter estimation for biochemical models. FEBS Journal 276, 886–
902 (2009)
3. Dallon, J.C., Othmer, H.G.: How cellular movement determines the collective force
generated by the Dictyostelium discoideum slug. J. Theor. Biol. 231, 203–222
(2004)
4. Davidson, E.H.: The regulatory genome: gene regulatory networks in development
and evolution. Academic Press, London (2006)
5. de Jong, H.: Modeling and simulation of genetic regulatory systems: A literature
review. J. Comp. Biol. 9, 67–103 (2002)
6. de Jong, J.: Quantitative analysis of gene expression in Nematostella vectensis.
Master’s thesis, Faculty of Science, University of Amsterdam (2009)
7. Faur´e, A., Naldi, A., Chaouiya, C., Thieﬀry, D.: Dynamical analysis of a generic
Boolean model for the control of the mammalian cell cycle. Bioinformatics 22(14),
e124–e131 (2006)

362
J.A. Kaandorp et al.
8. Finnerty, J.R., Pang, K., Burton, P., Paulson, D., Martindale, M.Q.: Origins of
Bilateral Symmetry: Hox and Dpp expression in a Sea Anemone Science, vol. 304,
pp. 1335–1336 (2004)
9. Fomekong Nanfack, Y., Kaandorp, J.A., Blom, J.G.: Eﬃcient parameter estima-
tion for spatio-temporal models of pattern formation: Case study of Drosophila
melanogaster. Bioinformatics 23, 3356–3363 (2007)
10. Gutenkunst, R.N., Waterfall, J.J., Casey, F.P., Brown, K.S., Myers, C.R., et al.:
Universally Sloppy Parameter Sensitivities in Systems Biology Models. PLoS Com-
put. Biol. 3(10) (2007)
11. Honda, H., Tanemura, M., Nagai, T.: A three-dimensional vertex dynamics cell
model of space-ﬁlling polyhedra simulating cell behavior in a cell aggregate. J.
Theor. Biol. 226, 439–453 (2004)
12. Jaeger, J., Surkova, S., Blagov, M., Janssens, H., Kosman, D., Kozlov, K.N., Manu,
M.E., Vanario-Alonso, C.E., Samsonova, M., Sharp, D.H., Reinitz, J.: Dynamic
control of positional information in the early Drosophila blastoderm. Nature 430,
368–371 (2004)
13. Kauﬀman, S.A.: The Origins of Order: Self Organization and Selection in Evolu-
tion. Oxford University Press, Oxford (1993)
14. Magie, C.R., Pang, K., Martindale, M.Q.: Martindale Genomic inventory and ex-
pression of Sox and Fox genes in the cnidarian Nematostella vectensis. Dev. Genes
Evol. 215, 618–630 (2005)
15. Maree, A.F., Hogeweg, P.: Modelling Dictyostelium discoideum morphogenesis: the
culmination. Bull. Math. Biol. 64(2), 327–353 (2002)
16. Martindale, M.Q., Pang, K., Finnerty, J.R.: Investigating the origins of triploblasty:
mesodermal gene expression in a diploblastic animal, the sea anemone Nematostella
vectensis (phylum, Cnidaria; class, Anthozoa). Development 131, 2463–2474 (2004)
17. Mjolsness, E., Sharp, D.H., Reinitz, J.: A connectionist model of development. J.
Theor. Biol. 152, 429–453 (1991)
18. Odell, G., Oster, G., Burnside, B., Alberch, P.: A mechanical model for epithelial
morphogenesis. J. Math. Biol. 9, 291–295 (1980)
19. Reinitz, J., Sharp, D.H.: Mechanisms of eve stripe formation. Mechanisms of De-
velopment 49, 133–158 (1995)
20. Reinitz, J., Hou, S., Sharp, D.H.: Transcriptional Control in Drosophila. Com-
PlexUs 1, 54–64 (2003)
21. Schaller, G., Meyer-Hermann, M.: Multicellular tumor spheroid in an oﬀ-lattice
Voronoi-Delaunay cell model. Phys. Rev. E. Stat. Nonlin. Soft. Matter Phys. 71(5
pt. 1), 51910 (2005)
22. S´anchez, L., Thieﬀry, D.: A logical analysis of the Drosophila gap-gene system. J.
Theor. Biol. 211, 115–141 (2001)
23. Surkova, S., Kosman, D., Kozlov, K., Manu Myasnikova, E., et al.: Characteriza-
tion of the Drosophila segment determination morphome. Dev. Biol. 313, 844–862
(2008)
24. Tamulonis, C., Postma, M., Marlow, H., Magie, C., de Jong, J., Kaandorp, J.A.:
Morphometrics and Modeling of Nematostella vectensis Gastrulation. Developmen-
tal Biology 351, 217–228 (2011)
25. Wikramanayake, A.H., Hong, M., Lee, P.N., Pang, K., Byrum, C.A., Bince, J.M.,
Xu, R., Martindale, M.Q.: An ancient role for beta-catenin in the evolution of axial
polarity and germ layer segregation. Nature 426, 446–450 (2003)

Tree-Automatic Well-Founded Trees
Alexander Kartzow1,⋆, Jiamou Liu2, and Markus Lohrey1,⋆
1 Universit¨at Leipzig, Germany, Institut f¨ur Informatik, Johannisgasse 26, 04103
Leipzig, Germany
{kartzow,lohrey}@informatik.uni-leipzig.de
2 Department of Computer Science, University of Auckland, Private Bag 92019
Auckland, New Zealand
jiamou.liu@aut.ac.nz
Abstract. We investigate tree-automatic well-founded trees. For this,
we introduce a new ordinal measure for well-founded trees, called ∞-rank.
The ∞-rank of a well-founded tree is always bounded from above by
the ordinary (ordinal) rank of a tree. We also show that the ordinal
rank of a well-founded tree of ∞-rank α is smaller than ω · (α + 1). For
string-automatic well-founded trees, it follows from [16] that the ∞-rank
is always ﬁnite. Here, using Delhomm´e’s decomposition technique for
tree-automatic structures, we show that the ∞-rank of a tree-automatic
well-founded tree is strictly below ωω. As a corollary, we obtain that the
ordinal rank of a string-automatic (resp., tree-automatic) well-founded
tree is strictly below ω2 (resp., ωω). The result for the string-automatic
case nicely contrasts a result of Delhomm´e, saying that the ranks of
string-automatic well-founded partial orders reach all ordinals below ωω.
As a second application of the ∞-rank we show that the isomorphism
problem for tree-automatic well-founded trees is complete for level Δ0
ωω
of the hyperarithmetical hierarchy (under Turing-reductions). Full proofs
can be found in the arXiv-version [11] of this paper.
1
Introduction
Various classes of inﬁnite but ﬁnitely presented structures received a lot of atten-
tion in algorithmic model theory [2]. Among the most important such classes of
structures is the class of string-automatic structures [13]. A (relational) structure
is string-automatic if its universe is a regular set of words and all relations can
be recognized by synchronous multi-tape automata. During the past 15 years a
theory of string-automatic structures has emerged. This theory was developed
along two interrelated branches. The ﬁrst is a structural branch, which leads
to (partial) characterizations of particular classes of string-automatic structures
[6,12,14,15,18]. The second is an algorithmic branch, which leads to numerous
decidability and undecidability, as well as complexity results for important al-
gorithmic problems for string-automatic structures [4,14,17]. One of the most
fundamental results for string-automatic structures states that their ﬁrst-order
theories are uniformly decidable [13].
⋆The ﬁrst and third author are supported by the DFG research project GELO.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 363–373, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

364
A. Kartzow, J. Liu, and M. Lohrey
By replacing strings and string automata by trees and tree automata, Blumen-
sath [3] generalized string-automatic structures to tree-automatic structures and
proved that their ﬁrst-order theories are still uniformly decidable. However com-
pared to string-automatic structures, the theory of tree-automatic structures is
less developed. The only non-trivial characterization of a class of tree-automatic
structures we are aware of concerns ordinals. Delhomm´e proved in [6] that an
ordinal is tree-automatic if and only if it is strictly below ωωω. Some complexity
results for ﬁrst-order theories of tree-automatic structures are shown in [17]. Re-
cently, Huschenbett proved that it is decidable whether a given tree-automatic
scattered linear order is string-automatic [9].
In this paper, we study tree-automatic well-founded trees.1 Our main tool is
a an ordinal measure for well-founded trees called ∞-rank, which is related to
the classical (ordinal) rank of a well-founded tree. Consider a well-founded tree
T with root r. The rank of T is the smallest ordinal, which is strictly larger
than the ranks of the subtrees rooted in the children of r. In contrast to this, we
only require the ∞-rank of T to be (i) strictly larger than the ∞-ranks of those
subtrees that are rooted (up to isomorphism) in inﬁnitely many children of r
and (ii) to be at least as large as the ∞-ranks of those subtrees that are rooted
(up to isomorphism) in ﬁnitely many children of r. For instance, if a tree T has
ﬁnite depth, then ∞-rank(T) is the largest number i ∈N such that the tree N≤i
can be embedded into T.
Clearly, the ∞-rank of a well-founded tree is bounded from above by the
classical (ordinal) rank of a tree. We also show that the rank of a well-founded
tree of ∞-rank α is strictly bounded by ω · (α + 1). For string-automatic well-
founded trees, it follows from [16] that the ∞-rank is always ﬁnite. Here, using a
reﬁnement of Delhomm´e’s decomposition technique for tree-automatic structures
[6], we show that the ∞-rank of a tree-automatic well-founded tree is strictly
below ωω. As a corollary, we obtain that the rank of a string-automatic (resp.,
tree-automatic) well-founded tree is strictly below ω2 (resp., ωω). The result for
the string-automatic case nicely contrasts a result from [6,12], saying that the
ranks of string-automatic well-founded partial orders reach exactly all ordinals
below ωω.
Our second application of the ∞-rank concerns the isomorphism problem for
tree-automatic well-founded trees. In [16], it was shown that the isomorphism
problem for string-automatic well-founded trees is complete for level Δ0
ω of the
hyperarithmetical hierarchy. In other words, the isomorphism problem for string-
automatic well-founded trees is recursively equivalent to true arithmetic. We
show that the ∞-rank of well-founded computable trees determines the complex-
ity of the isomorphism problem in the following sense: The isomorphism problem
for well-founded computable trees of ∞-rank at most λ + k (where k ∈N and λ
is a computable limit ordinal) belongs to level Σ0
λ+3(k+1) of the hyperarithmeti-
cal hierarchy. Since we know that the ∞-rank of a tree-automatic well-founded
tree is strictly below ωω, we can use this fact and show that the isomorphism
1 In this paper tree always refers to an order tree T = (T, ≤) as opposed to a successor
tree, i.e., a tree is a partial order (without successor relation).

Tree-Automatic Well-Founded Trees
365
problem for tree-automatic well-founded trees belongs to level Δ0
ωω = Σ0
ωω ∩Π0
ωω
of the hyperarithmetical hierarchy. We also provide a corresponding lower bound
w.r.t. Turing-reductions. Thus, the isomorphism problem for tree-automatic well-
founded trees is Δ0
ωω-complete under Turing-reductions.
Let us remark that for non-well-founded order trees, the isomorphism problem
is complete for Σ1
1 (the ﬁrst existential level of the analytical hierarchy) already
in the string-automatic case [16], and this complexity is in a certain sense max-
imal, since the isomorphism problem for the class of all computable structures
is Σ1
1-complete as well [5,7]. Let us also emphasize that all our results only hold
for order trees, i.e., trees are seen as particular partial orders.
2
Preliminaries
A relational structure S consists of a domain D and atomic relations on the set
D. In this paper we shall only consider structures with countable domains. Let
A = (A, ≤) be a partial order. A subset B ⊆A is a chain if for all a, b ∈B, a ≤b
or b ≤a. A subset B ⊆A is an antichain if for all pairs of distinct a, b ∈B,
neither a ≤b nor b ≤a.
Trees and Forests. A forest is a partial order F = (F, ≤) where for every
a ∈F the set {b ∈F | b < a} is a ﬁnite chain. A tree is a forest which has a
smallest element, which is called the root of the tree. Thus, a forest is a disjoint
union of (an arbitrary number of) trees. For a given forest F, we denote by ⟨F⟩
the tree that results from adding a new root, i.e., a new smallest element, to F.
If F is the domain of F we denote by ⟨F⟩the domain of ⟨F⟩. For a node u in
F, let F(u) be the subtree of F at u, i.e., F(u) is the restriction of F to the set
{v ∈F | v ≥u}. We deﬁne the successor relation of F as
EF = {(x, y) ∈F × F | x < y, ¬∃z : x < z < y}.
For x ∈F the set of children of x in F is EF(x) = {y ∈F | (x, y) ∈EF}. A
forest F = (F, ≤) is well-founded if it does not contain an inﬁnite ascending chain
a1 < a2 < a3 < · · · .
Let us now deﬁne inductively the classical (ordinal) rank of a well-founded tree
as well as the new notion of ∞-rank. We use standard terminology concerning
ordinals; cf., e.g., [20]. For a set of ordinals M, let sup(M) be its supremum,
where sup(∅) = 0. Let T be a well-founded tree with root r. Thus, C = ET(r) is
the set of children of the root. We deﬁne the rank of T inductively as the ordinal
rank(T) = sup{rank(T(a)) + 1 | a ∈C}.
We deﬁne the ordinal ∞-rank(T) inductively using α = sup{∞-rank(T(a)) |
a ∈C}:
∞-rank(T) =

α
if {a ∈C | ∞-rank(T(a)) = α} is ﬁnite,
α + 1
otherwise.

366
A. Kartzow, J. Liu, and M. Lohrey
The ∞-rank of a forest F without smallest element is ∞-rank(F) = ∞-rank(⟨F⟩).
A simple application of K¨onig’s lemma shows that a well-founded forest F has
∞-rank 0 if and only if F is ﬁnite; in contrast, the rank of a ﬁnite tree can reach
any ﬁnite ordinal. More generally, ∞-rank(F) = n < ω if and only if there is
an embedding of the tree N≤n (the tree of height n where every non-leaf has
ℵ0 many children) into ⟨F⟩but no embedding of N≤n+1 into ⟨F⟩. The following
lemma is crucial for studying the ∞-rank:
Lemma 1. Let F = (F, ≤) be a well-founded forest. There are only ﬁnitely many
a ∈F with ∞-rank(F(a)) = ∞-rank(F).
Proof. Let α = ∞-rank(F). We show that D = {a ∈⟨F⟩| ∞-rank(⟨F⟩(a)) = α}
is ﬁnite. Note that D is a downward-closed subset of the tree ⟨F⟩. Assume that
this set is inﬁnite. Since ⟨F⟩is well-founded, K¨onig’s lemma implies that D
contains a node a which has inﬁnitely many children ai (i ∈N) that all belong
to D. But then α = ∞-rank(⟨F⟩(a)) ≥α + 1, which is a contradiction.
⊓⊔
It is obvious that ∞-rank(T) ≤rank(T) for every well-founded tree T. On the
other hand, we can also bound rank(T) in terms of ∞-rank(T) as follows.
Lemma 2. For a well-founded tree T we have rank(T) < ω · (∞-rank(T) + 1).
Proof. Let T = (T, ≤) We proceed by induction on ∞-rank(T). If T is ﬁnite,
then ∞-rank(T) = 0 and rank(T) ≤|T | < ω. Now assume that ∞-rank(T) = α
for some ordinal α > 0 such that the theorem holds for all trees of ∞-rank
strictly below α. By Lemma 1, Tα = {a ∈T | ∞-rank(T(a)) = α} is a ﬁnite and
downward-closed subset of T . Let Mα ⊆Tα be the set of ≤-maximal elements
of Tα and consider a tree T(a) for a ∈Mα. The deﬁnition of Mα implies the
following. If b ∈T with b > a, then ∞-rank(T(b)) = β for some ordinal β < α.
By the induction hypothesis it follows that rank(T(b)) < ω · (β + 1) ≤ω · α. In
particular, rank(T(b)) < ω · α for all children b of a. Thus, rank(T(a)) ≤ω · α.
Finally, since Tα is a ﬁnite set, we have
rank(T) ≤sup{rank(T(a)) | a ∈Mα} + |Tα| ≤ω · α + |Tα| < ω · (α + 1).
⊓⊔
Note that the upper bound of ω ·(∞-rank(T)+1) = ω ·∞-rank(T)+ω is optimal
as for any n < ω, the linear order of size n has ∞-rank 0 but rank ω · 0 + n.
Finite Labeled Trees. A ﬁnite binary tree is a preﬁx-closed ﬁnite subset T ⊆
{0, 1}∗, i.e., uv ∈T implies u ∈T . We denote the set of all ﬁnite binary trees
by T ﬁn
2 . Let ⪯be the preﬁx relation on {0, 1}∗. Clearly (T, ⪯) is a tree in the
above sense.
Let Σ be a ﬁnite alphabet. A ﬁnite Σ-labeled binary tree is a pair (T, λ),
where T ∈T ﬁn
2
and λ : T →Σ is a labeling function. By T ﬁn
2,Σ we denote the set
of all ﬁnite Σ-labeled binary trees. Elements of T ﬁn
2,Σ are denoted by lower case
letters (s, t, . . .).
Next, we deﬁne the convolution t1 ⊗· · · ⊗tn of t1, . . . , tn ∈T ﬁn
2,Σ as follows:
Let ti = (Ti, λi) where λi : Ti →Σ and ⋄/∈Σ. Let T = n
i=1 Ti and deﬁne

Tree-Automatic Well-Founded Trees
367
λ′
i : T →Σ ∪{⋄} by λ′
i(u) = λi(u) for u ∈Ti and λ′
i(u) = ⋄for u ∈T \ Ti.
Then t1 ⊗· · · ⊗tn is the ﬁnite ((Σ ∪{⋄})n \ {⋄}n)-labeled tree (T, λ) where λ is
deﬁned by λ(u) = (λ′
1(u), . . . , λ′
n(u)) for each u ∈T .
Tree Automata and Tree-Automatic Structures. For T ∈T ﬁn
2
let
cl(T ) = T ∪{ui | u ∈T, i ∈{0, 1}}
be its closure, which is again preﬁx-closed. Let Σ be a ﬁnite alphabet. A tree
automaton over Σ is a tuple A = (Q, Δ, QI, QF ), where Q is the ﬁnite set of
states, QI ⊆Q is the set of initial states, QF ⊆Q is the set of ﬁnal states, and
Δ ⊆(Q \ QF) × Σ × Q × Q is the transition relation. Given t = (T, λ) ∈T ﬁn
2,Σ,
a successful run of A on t is a mapping ρ : cl(T ) →Q such that (i) ρ(ε) ∈QI,
(ii) ρ(cl(T ) \ T ) ⊆QF , and (iii) for every d ∈T , (ρ(d), λ(d), ρ(d0), ρ(d1)) ∈Δ.
By L(A) we denote the set of all t ∈T ﬁn
2,Σ on which A has a successful run. A set
L ⊆T ﬁn
2,Σ is called regular if there is a tree automaton A over Σ with L = L(A).
An n-ary relation R ⊆(T ﬁn
2,Σ)n is called tree-automatic if there is a tree au-
tomaton AR over (Σ∪{⋄})n\{⋄}n such that L(AR) = {t1⊗· · ·⊗tn | (t1, . . . , tn) ∈
R}. A relational structure S is called tree-automatic over Σ if its domain is a
regular subset of T ﬁn
2,Σ and each of its atomic relations is tree-automatic; any
tuple P of automata that accepts the domain and the relations of S is called a
tree-automatic presentation of S. In this case, we write S(P) for S. If a tree-
automatic structure S is isomorphic to a structure S′, then S is called a tree-
automatic copy of S′ and S′ is tree-automatically presentable. In this paper we
sometimes abuse the terminology referring to S′ as simply tree-automatic and
calling a tree-automatic presentation of S also a tree-automatic presentation of
S′. We also simplify our statements by saying “given/compute a tree-automatic
structure S” for “given/compute a tree-automatic presentation P of a struc-
ture S(P)”. The structures (N, +) and (N, ×) are examples of tree-automatic
structures. We shall make use of the following simple lemma.
Lemma 3. For every tree-automatic structure there is an isomorphic tree-
automatic structure S over the alphabet {a}, i.e., the domain can be seen as
a subset of T ﬁn
2 .
Consider FO+∃∞+∃n,m+∃chain, i.e., ﬁrst-order logic extended by the quantiﬁers
∃∞(there exists inﬁnitely many), ∃n,m (there exists ﬁnitely many and the exact
number is congruent n modulo m, where m, n ∈N) and the chain-quantiﬁer
∃chain (if ϕ(x, y) is some formula, then ∃chainϕ(x, y) asserts that ϕ(x, y) deﬁnes
a partial order that contains an inﬁnite ascending chain). Results from [3,10,21]
show that the FO + ∃∞+ ∃n,m + ∃chain theory of any tree-automatic structure S
is (uniformly) decidable. Note that the property of being a tree is expressible in
FO + ∃∞and well-foundedness of a tree is expressible in FO + ∃chain. Hence, we
get:
Theorem 4. It is decidable whether a given tree-automatic structure is a well-
founded tree.

368
A. Kartzow, J. Liu, and M. Lohrey
Let K be a class of tree-automatic presentations. The isomorphism problem
Iso(K) is the set of pairs (P1, P2) ∈K × K of tree-automatic presentations with
S(P1) ∼= S(P2). If K is the class of tree-automatic presentations for a class C of
relational structures (e.g., trees), then we shall brieﬂy speak of the isomorphism
problem for (tree-automatic members of) C. The isomorphism problem for the
class of all tree-automatic structures is complete for Σ1
1, the ﬁrst level of the
analytical hierarchy; this holds already for (non well-founded) string-automatic
trees [14,16].
Hyperarithmetical Sets. We use standard terminology concerning recursion
theory; cf., e.g., [19]. We use the deﬁnition of the hyperarithmetical hierarchy
from Ash and Knight [1] (cf. [8]). We ﬁrst deﬁne inductively a set of ordinal
notations O ⊆N. Simultaneously we deﬁne a mapping a →|a|O from O into
ordinals and a strict partial order <O on O. The set O is the smallest subset of
N satisfying the following conditions:
– 1 ∈O and |1|O = 0, i.e., 1 is a notation for the ordinal 0.
– If a ∈O, then also 2a ∈O. We set |2a|O = |a|O + 1 and let b <O 2a if and
only if b = a or b <O a.
– If e ∈N is such that Φe (the eth partial computable function) is total,
Φe(n) ∈O for all n ∈N, and Φe(0) <O Φe(1)O <O Φe(2) <O · · · , then also
3 · 5e ∈O. We set |3 · 5e|O = sup{|Φe(n)|O | n ∈N} and let b <O 3 · 5e if and
only if there is an n ∈N with b <O Φe(n).
An ordinal α is computable if there is an a ∈O with |a|O = α. The smallest
non-computable ordinal is the Church-Kleene ordinal ωck
1 . If a ∈O then the
restriction of the partial order (O, <O) to Oa = {b ∈O | b <O a} is isomorphic
to the ordinal |a|O [1, Proposition 4.9]. Based on ordinal notations we deﬁne
the hyperarithmetical hierarchy. For this we deﬁne sets H(a) for each a ∈O as
follows:
– H(1) = ∅,
– H(2b) = H(b)′ (the Turing jump of H(b); cf., e.g., [19]),
– H(3·5e) = {⟨b, n⟩| b <O 3·5e, n ∈H(b)}; here ⟨·, ·⟩denotes some computable
pairing function.
Spector has shown that |a|O = |b|O implies that H(a) and H(b) are Turing equiv-
alent. The levels of the hyperarithmetical hierarchy can be deﬁned as follows,
where α is a computable ordinal.
– Σ0
α is the set of all subsets A ⊆N that are recursively enumerable in some
H(a) with |a|O = α (by Spector’s theorem, the concrete choice of a is irrel-
evant).
– Π0
α is the set of all complements of Σ0
α sets.
– Δ0
α = Σ0
α ∩Π0
α, i.e., Δ0
α is the set of all subsets A ⊆N that are Turing-
reducible to some H(a) with |a|O = α.
For any two computable ordinals α and β, α < β implies Σα ∪Πα ⊊Δβ. The
union of all classes Σ0
α where α < ωck
1 yields the class of all hyperarithmetical

Tree-Automatic Well-Founded Trees
369
sets. By a classical result of Kleene, the hyperarithmetical sets are exactly the
sets in Δ1
1 = Σ1
1 ∩Π1
1, where Σ1
1 is the ﬁrst existential level of the analytical
hierarchy, and Π1
1 is the set of all complements of Σ1
1-sets.
3
Bounding the ∞-rank of Tree-Automatic Well-Founded
Trees
The ﬁrst main result of this paper is:
Theorem 5. If T is a tree-automatic well-founded tree, then ∞-rank(T) < ωω.
Before we sketch a proof of this result, let us ﬁrst deduce a corollary:
Corollary 6. For T = (T, ≤) a string-automatic (tree-automatic, respectively)
well-founded tree we have rank(T) < ω2 (rank(T) < ωω, respectively).
Proof. For a string-automatic well-founded tree T, ∞-rank(T) is ﬁnite by [16]2.
With Lemma 2 we get rank(T) ≤ω · i < ω2 for some i ∈N. For a tree-automatic
well-founded tree T we have ∞-rank(T) < ωω by Theorem 5. Thus, there is some
i ∈N such that ∞-rank(T) ≤ωi. With Lemma 2 we get rank(T) < ω·ωi+ω < ωω.
⊓⊔
Note that Corollary 6 contrasts with results on the ranks of string-automatic
well-founded partial orders.3 By [6,12], the ordinal ranks of string-automatic
well-founded partial orders are the ordinals strictly below ωω. In fact, the result
still holds for partial orders without inﬁnite chains [11]. Moreover, Delhomm´e’s
characterization of tree-automatic ordinals yields tree-automatic well-founded
partial orders of rank α for each α < ωωω [6].
Let us sketch the proof of Theorem 5. The ﬁrst part relies on Delhomm´e’s
decomposition technique for tree-automatic structures from [6] where he proved
that the ordinal ωωω is not tree-automatic. Let us explain his decomposition
technique for a tree-automatic graph G = (V, E). Because of Lemma 3 we can
assume that V ⊆T ﬁn
2 . Consider a tree automaton A that accepts a subset of
V ⊗T ﬁn
2 , and for each s ∈T ﬁn
2
let Gs be the subgraph of G induced by the set
{t ∈V | t ⊗s ∈L(A)}.
Delhomm´e’s main proposition from [6] shows that every subgraph Gs can
be obtained from a ﬁnite set of subgraphs C by using the operations of box-
augmentation and sum-augmentation. Roughly speaking, a graph G is a sum-
augmentation of subgraphs G1, . . . , Gn if it is the disjoint union of G1, . . . , Gn
where we may add edges between diﬀerent Gi (but not within a single Gi).
G is a box-augmentation of the graphs G1, . . . , Gn with node sets V1, . . . , Vn
2 In [16], the notion of embedding rank of an arbitrary tree is deﬁned. Comparison of
the deﬁnitions shows that the embedding rank of a well-founded tree is ﬁnite iﬀits
∞-rank is ﬁnite.
3 The rank generalizes naturally to all well-founded partial orders (considering roots
as maximal elements of trees).

370
A. Kartzow, J. Liu, and M. Lohrey
if the node set of G is the product n
i=1 Vi and for every 1 ≤i ≤n and all
v1 ∈V1, . . . , vi−1 ∈Vi−1, vi+1 ∈Vi+1, . . . , vn ∈Vn, the subgraph of G induced
by the set {(v1, . . . , vi−1, v, vi+1, . . . , vn) | v ∈Vi} is isomorphic to Gi.
Now, let ν be a function that maps graphs to some set M such that isomor-
phic graphs are mapped to the same element. We say that m ∈M is ν-sum-
indecomposable (ν-box-indecomposable, resp.) if for all graphs G, G1, . . . , Gn
such that G is a sum-augmentation (box-augmentation, resp.) of G1, . . . , Gn the
following implication holds: If ν(G) = m then ν(Gi) = m for some 1 ≤i ≤n.
Delhomm´e’s decomposition result implies that the set {ν(Gs) | s ∈T ﬁn
2 } con-
tains only ﬁnitely many values that are both ν-sum-indecomposable and ν-box-
indecomposable.
In order to show that ωωω is not tree-automatic, Delhomm´e takes a tree-
automatic copy G = (V, ≤) of some ordinal and a tree automaton A for the
ﬁrst-order formula y < x. Hence, the substructures Gs are the initial segments
of G. Moreover let ν0 be the function that maps an initial segment of G to
the corresponding ordinal. Delhomm´e proves that every ordinal of the form ωωα
is both ν0-sum-indecomposable and ν0-box-indecomposable. Hence, G = (V, ≤)
can contain only ﬁnitely many initial segments of the form ωωα, which is not the
case for ωωω.
We follow a similar strategy. Heading for a contradiction to Theorem 5, take a
well-founded tree-automatic forest F = (F, ≤) with ∞-rank(F) = ωω. Let A be a
tree automaton for the ﬁrst-order formula x ≤y. Hence, the substructures Fs are
the subtrees F(v) of F for v ∈F. It is not diﬃcult to show that for every ordinal
α < ωω, F must contain a subtree of ∞-rank α. In particular, F contains a subtree
of ∞-rank ωi for every i ∈N. Now, let ν1 be the function that maps a subtree F(v)
to its ∞-rank. We would obtain a contradiction by proving that every ordinal
of the form ωα is both ν1-sum-indecomposable and ν1-box-indecomposable. In-
deed, we can prove that every ordinal of the form ωα is ν1-sum-indecomposable.
But there is a problem with ν1-box-indecomposability: The ordinals 0 and 1
are the only ν1-box-indecomposable ordinals. The problem is that any forest
can be embedded into the box-augmentation of two inﬁnite antichains. Hence,
box-augmentations of two inﬁnite antichains may have arbitrarily high ∞-rank.
To solve this problem, we observe that the box-augmentations that are used for
building up the subtrees Fs of F (s ∈T ﬁn
2 ) have a particular property that we
call tame colorability (this is joint work with Martin Huschenbett). If a graph
G = (V, E) is a box-augmentation of subgraphs Gi = (Vi, Ei) (1 ≤i ≤n), then
this box-augmentation is tamely colorable if for each 1 ≤i ≤n there is a ﬁnite col-
oring ci of Vi × Vi such that whether ((v1, . . . vn), (v′
1, . . . , v′
n)) ∈E only depends
on the colors ci(vi, v′
i) for 1 ≤i ≤n. A careful analysis of box-augmentations of
forests shows that ωα is also ν1-tamely-colorable-box-indecomposable (tamely-
colorable-box-indecomposability is deﬁned as box-indecomposability, but only
considering tamely colorable box-augmentations). As in Delhomm´e’s argument,
we conclude that a well-founded tree-automatic forest F only contains ﬁnitely
many subtrees of pairwise distinct ∞-ranks of the form ωi. Hence, ∞-rank(F) <
ωω and Theorem 5 follows.

Tree-Automatic Well-Founded Trees
371
4
The Isomorphism Problem for Well-Founded
Tree-Automatic Trees
It turns out that the ∞-rank for well-founded computable trees yields an up-
per bound on the recursion-theoretic complexity of the isomorphism problem.
Recall that we deﬁned trees and forests as particular partial orders. For the iso-
morphism problem, it is useful to assume that also the direct successor relation is
computable. When speaking of a computable forest in the following theorem, we
mean a forest F = (F, ≤) such that F ⊆N, ≤⊆N × N, and the direct successor
relation EF are all computable sets.4 Note that the direct successor relation of a
tree-automatic forest is still tree-automatic (and hence computable) because it
is ﬁrst-order deﬁnable.
Lemma 7. Let α be a computable ordinal and assume that α = λ + k, where
k ∈N and either λ = 0 or λ is a limit ordinal. The isomorphism problem for
well-founded computable trees of ∞-rank at most α belongs to level Σ0
λ+2(k+1) of
the hyperarithmetical hierarchy.
For the proof of Lemma 7 we use a characterization of the hyperarithmetical
levels by computable inﬁnitary formulas (cf. [1]). These are ﬁrst-order formulas
over the structure (N, +, ×), where countably inﬁnite conjunctions and disjunc-
tions are allowed. Computability of such an inﬁnitary formula means that for an
inﬁnite conjunction 
n∈N ϕn there is a computable function that maps n to a
representation of ϕn (note that ϕn may again contain inﬁnite conjunctions and
disjunctions), and similarly for inﬁnite disjunctions. Roughly speaking, such an
inﬁnitary formula can be encoded by a computable tree (the syntax tree of the
formula) and if that tree has rank α (a computable ordinal) then the relation
deﬁned by the formula belongs to level α of the hyperarithmetical hierarchy.
In order to prove Lemma 7, we construct for a computable ordinal α a com-
putable inﬁnitary formula isoα(x, y) that is satisﬁed in a well-founded com-
putable forest F if and only if F(x) and F(y) have ∞-rank at most α and
F(x) ∼= F(y). The construction is carried out inductively along the ordinal α,
similarly to the proof of Lemma 25 in [16].
From Theorem 5 and Lemma 7 it follows that the isomorphism problem for
well-founded tree-automatic trees belongs to Π0
ωω. Using similar formulas as
those constructed in our proof of Lemma 7, we can also show that the isomor-
phism problem for well-founded tree-automatic trees belongs to Σ0
ωω. Hence, we
get:
Corollary 8. The isomorphism problem for well-founded tree-automatic trees
belongs to Δ0
ωω.
4 On the other hand, if we would omit the requirement of a computable direct successor
relation in Lemma 7, then we would only have to replace the constant 2 in the lemma
by a larger value.

372
A. Kartzow, J. Liu, and M. Lohrey
Let us now turn to lower bounds. Our main technical result is:
Lemma 9. From a given i ∈N, one can compute a well-founded tree-automatic
tree Vi such that the following holds: From a given Π0
ωi-set P ⊆N (represented,
e.g., by a computable inﬁnitary formula) and n ∈N one can compute a well-
founded tree-automatic tree WP,n such that n ∈P if and only if Vi ∼= WP,n.
We prove Lemma 9 by a reduction from the isomorphism problem for well-founded
computable trees. We use a construction from [8]. Basically, [8, Proposition 3.2]
states Lemma 9 for well-founded computable trees (instead of well-founded tree-
automatic trees) and all computable ordinals (instead of ordinals ωi). It turns out
that the trees constructed in [8] for a certain ordinal α consist of computable sub-
trees of a “universal” well-founded computable tree Sα. In case α = ωi for i ∈N
we can moreover show that the tree Sωi is tree-automatic. Roughly speaking this
yields a weaker version of Lemma 9, where instead of well-founded tree-automatic
trees we have the (tree-automatic) trees Sωi enriched by a computable unary
predicate K on the node set of Sωi. In fact K can be assumed to be a sub-
set of the leaves of Sωi; it yields a computable subtree of the universal tree
Sωi by removing all leaves from K. Finally, to get rid of K we use a technique
from [16]. In Lemma 41 from [16] it was shown that there are non-isomorphic
(string-automatic) trees U0 and U1 with the following property: from an index
of a computable set of strings L ⊆{0, 1}∗one can compute a string-automatic
forest FL of height 3 such that: (i) the set of roots is {0, 1}∗, (ii) if x ∈L then
FL(x) ∼= U0, and (iii) if x /∈L then FL(x) ∼= U1. In [16], this statement was used
in order to reduce the isomorphism problem for (non-well-founded) computable
trees to the isomorphism problem for (non-well-founded) string-automatic trees.
Hence, the latter problem is Σ1
1-complete. In our situation, we ﬁrst prove a tree
version of [16, Lemma 41], where {0, 1}∗is replaced by T ﬁn
2 . Then, one can elimi-
nate the additional computable unary predicate K on the leaves of Sωi. For this,
every leaf from K is replaced by the height-3 tree U0, whereas all other leaves
are replaced by the height-3 tree U1. This yields a well-founded tree automatic
tree that encodes the pair (Sωi, K).
From Lemma 9 we can deduce that the isomorphism problem for well-founded
tree-automatic trees is Δ0
ωω-hard under Turing-reductions. With Corollary 8, we
ﬁnally obtain our main result for the isomorphism problem.
Theorem 10. The isomorphism problem for well-founded tree-automatic trees
is Δ0
ωω-complete under Turing-reductions.
Acknowledgment. We thank Martin Huschenbett for helpful discussions con-
cerning Delhomm´e’s decomposition result.

Tree-Automatic Well-Founded Trees
373
References
1. Ash, C.J., Knight, J.F.: Computable structures and the hyperarithmetical hierar-
chy. Studies in Logic and the Foundations of Mathematics, vol. 144. North-Holland
Publishing Co., Amsterdam (2000)
2. B´ar´any, V., Gr¨adel, E., Rubin, S.: Automata-based presentations of inﬁnite struc-
tures. In: Finite and Algorithmic Model Theory. London Mathematical Society
Lecture Notes Series, vol. 379, pp. 1–76. Cambridge University Press (2011)
3. Blumensath, A.: Automatic structures. Diploma thesis, RWTH Aachen (1999)
4. Blumensath, A., Gr¨adel, E.: Finite presentations of inﬁnite structures: Automata
and interpretations. Theory Comput. Syst. 37, 642–674 (2004)
5. Calvert, W., Knight, J.F.: Classiﬁcation from a computable viewpoint. Bull. Sym-
bolic Logic 12(2), 191–218 (2006)
6. Delhomm´e, C.: Automaticit´e des ordinaux et des graphes homog`enes. C.R. Acad.
Sci. Paris Ser. I 339, 5–10 (2004)
7. Goncharov, S.S., Knight, J.F.: Computable structure and antistructure theorems.
Algebra Logika 41(6), 639–681 (2002)
8. Hirschfeldt, D.R., White, W.M.: Realizing levels of the hyperarithmetic hierarchy
as degree spectra of relations on computable structures. Notre Dame J. Form.
Log. 43(1), 51–64 (2002)
9. Huschenbett, M.: Word automaticity of tree automatic scattered linear orderings is
decidable. Technical report, arXiv.org (2012), http://arxiv.org/abs/1201.5070
10. Kartzow, A.: First-Order Model Checking On Generalisations of Pushdown
Graphs. PhD thesis, TU Darmstadt (2011)
11. Kartzow, A., Lohrey, M., Liu, J.: Tree-automatic well-founded trees. Technical
report, arXiv.org (2012), http://arxiv.org/abs/1201.5495
12. Khoussainov, B., Minnes, M.: Model theoretic complexity of automatic structures.
Ann. Pure Appl. Logic 161(3), 416–426 (2009)
13. Khoussainov, B., Nerode, A.: Automatic Presentations of Structures. In: Leivant,
D. (ed.) LCC 1994. LNCS, vol. 960, pp. 367–392. Springer, Heidelberg (1995)
14. Khoussainov, B., Nies, A., Rubin, S., Stephan, F.: Automatic structures: richness
and limitations. Log. Methods Comput. Sci. 3(2):2:2, 18 (2007)
15. Khoussainov, B., Rubin, S., Stephan, F.: Automatic linear orders and trees. ACM
Trans. Comput. Log. 6(4), 675–700 (2005)
16. Kuske, D., Liu, J., Lohrey, M.: The isomorphism problem on classes of automatic
structures with transitive relations. To appear in Trans. Amer. Math. Soc. (2012)
17. Kuske, D., Lohrey, M.: Automatic structures of bounded degree revisited. J. Sym-
bolic Logic 76(4), 1352–1380 (2011)
18. Oliver, G.P., Thomas, R.M.: Automatic Presentations for Finitely Generated
Groups. In: Diekert, V., Durand, B. (eds.) STACS 2005. LNCS, vol. 3404, pp.
693–704. Springer, Heidelberg (2005)
19. Rogers, H.: Theory of Recursive Functions and Eﬀective Computability. McGraw-
Hill (1968)
20. Rosenstein, J.: Linear Ordering. Academic Press (1982)
21. To, A.W., Libkin, L.: Recurrent Reachability Analysis in Regular Model Check-
ing. In: Cervesato, I., Veith, H., Voronkov, A. (eds.) LPAR 2008. LNCS (LNAI),
vol. 5330, pp. 198–213. Springer, Heidelberg (2008)

Inﬁnite Games and Transﬁnite Recursion
of Multiple Inductive Deﬁnitions
Keisuke Yoshii and Kazuyuki Tanaka
Mathematical Institute, Tohoku University, Sendai 980-8578, Japan
keisuke.yoshii@gmail.com, tanaka@math.tohoku.ac.jp
Abstract. The purpose of this research is to investigate the logical
strength of weak determinacy of Gale-Stewart games from the stand-
point of reverse mathematics. It is known that the determinacy of Σ0
1
sets (open sets) is equivalent to system ATR0 and that of Σ0
2 corre-
sponds to the axiom of Σ1
1 inductive deﬁnitions. Recently, much eﬀort
has been made to characterize the determinacy of game classes above
Σ0
2 within second order arithmetic. In this paper, we show that for any
k ∈ω, the determinacy of Δ((Σ0
2)k+1) sets is equivalent to the axiom of
transﬁnite recursion of Σ1
1 inductive deﬁnitions with k operators, denote
[Σ1
1]k-IDTR. Here, (Σ0
2)k+1 is the diﬀerence class of k + 1 Σ0
2 sets and
Δ((Σ0
2)k+1) is the conjunction of (Σ0
2)k+1 and co-(Σ0
2)k+1.
1
Introduction
In 1953, Gale and Stewart introduced the following inﬁnite games. Two players I
and II alternatively choose an element from N and construct an inﬁnite sequence
f ∈NN. Player I is said to win the game if the resulting sequence f belongs to
a given set A, and player II wins otherwise. We denote this game GA, and call
A the winning set. If A is an open set (a closed set, or a Borel set), GA is called
an open game (a closed game, or a Borel game, respectively). A strategy for
player I (for player II) is a function σ from Neven to N (a function τ from Nodd
to N), where Neven (Nodd) is a set of ﬁnite sequences of natural numbers with
even (odd) length. A game GA is said to be determinate if one of the players has
a winning strategy in GA, i.e., a strategy with which the player always wins.
Researches on the determinacy of inﬁnite games have been conducted in de-
scriptive set theory. It is provable in ZFC that a Borel game is determinate, but
the same thing does not hold for an analytic game. These facts simply represent
that the strength of the determinacy of GA varies depending on the complexity
of the winning set A.
Aside from descriptive set theory, researches classifying the strength of deter-
minacy within second order arithmetic have been started by J. Steel and Tanaka
along the Reverse Mathematics program. The goal of the program is to answer
the following questions: What set existence axioms are necessary and suﬃcient
to prove the theorems of ordinary mathematics? See Simpson [11] for the major
results.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 374–383, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Inﬁnite Games and Transﬁnite Recursion of Multiple Inductive Deﬁnitions
375
In [12], it is already shown that Δ0
2-determinacy and Δ1
2-CA are incompa-
rable, which means that the determinacy strength can not be characterized by
a standard comprehension axiom. In fact, even Δ1
1-determinacy can not imply
Δ1
2-CA ([5]). Thus, diﬀerent kinds of set existence axioms such as inductive def-
initions and transﬁnite recursions are needed to characterize the determinacy
statements.
In [13], it is shown that Σ0
2-determinacy is equivalent to the axiom Σ1
1-ID of Σ1
1
inductive deﬁnitions. Then, in [6], to pin down Δ0
3-determinacy, we introduced
a new axiom of multiple inductive deﬁnitions as well as the diﬀerent hierarchy
(Σ0
2)α, 0 < α < ω1. (see also [5]). In this paper, we treat the determinacy of the
game in the Wadge classes Δ((Σ0
2)k+1) which is between (Σ0
2)k and (Σ0
2)k+1,
and prove that it is equivalent to the axiom of iterated inductive deﬁnition with
multiple operators, [Σ1
1]k-IDTR. This more or less answers Open Question 29 in
A. Montalb´an’s survey paper [8]. See section 4.
2
Preliminaries
In this section, we recall some basic deﬁnitions and facts about second order
arithmetic. The language L2 of second-order arithmetic is a two-sorted language
with number variables x, y, z, . . . and unary function variables f, g, h, . . ., con-
sisting of constant symbols 0, 1, +, ·, =, < . We also use set variables X, Y, Z, . . .,
intending to range over the {0, 1}-valued functions, that is, the characteristic
functions of sets.
The formulas can be classiﬁed as follows:
– ϕ is bounded (Π0
0) if it is built up from atomic formulas by using propositional
connectives and bounded number quantiﬁers (∀x < t), (∃x < t), where t does
not contain x.
– ϕ is Π1
0 if it does not contain any function quantiﬁer. Π1
0 formulas are called
arithmetical formulas.
– ¬ϕ is Σi
n if ϕ is a Πi
n-formula (i ∈{0, 1}, n ∈ω).
– ∀x1 · · · ∀xkϕ is Π0
n+1 if ϕ is a Σ0
n-formula (n ∈ω),
– ∀f1 · · · ∀fkϕ is Π1
n+1 if ϕ is a Σ1
n-formula (n ∈ω).
We loosely say that a formula is Σi
n (resp. Πi
n) if it is equivalent over a base
theory (such as ACA0) to a ψ ∈Σi
n (resp. Πi
n).
We now deﬁne some popular axiom schemata of second order arithmetic.
Deﬁnition 1. Let C be a set of L2 formulas.
(1) C-IND:
(ϕ(0) ∧∀x(ϕ(x) →ϕ(x + 1))) →∀xϕ(x),
where ϕ(x) belongs to C.
(2) C-TI:
for any well-ordering <X, (∀x(∀y <X x ϕ(y) →ϕ(x))) →∀xϕ(x),
where ϕ(x) belongs to C.
(3) C-CA :
∃X∀x(x ∈X ↔ϕ(x)),
where ϕ(x) belongs to C and X does not occur freely in ϕ(x).

376
K. Yoshii and K. Tanaka
(4) C ∩C−-CA :
∀x(ϕ(x) ↔ψ(x)) →∃X∀x(x ∈X ↔ϕ(x)),
where ϕ(x) and ¬ψ(x) belong to C and X does not occur freely in ϕ(x).
(5) C-AC :
∀x∃Xϕ(x, X) →∃X∀xϕ(x, Xx),
where ϕ(x, X) belongs to C and Xx = {y : (x, y) ∈X}.
The system ACA0 consists of the ordered semiring axioms for (ω, +, ·, 0, 1, <),
Σ0
1-CA and Σ0
1-IND. For a set Λ of sentences, Λ0 denotes the system consisting
of ACA0 plus Λ.
By Δi
n-CA, we denote Σi
n ∩(Σi
n)−-CA. We can easily show that for any k ≥0,
Δ1
k-CA0 ⊂Σ1
k-AC0.
Moreover, if k = 2, the above two axioms are known to be equivalent to each
other.
For a formula ϕ with a distinct variable f ranging over NN, we associate
a two-person game Gϕ (or simply denote ϕ) as follows: player I and player II
alternately choose natural numbers (starting with I) to form an inﬁnite sequence
f ∈NN and I (resp. II) wins iﬀϕ(f) (resp. ¬ϕ(f) ). We say that ϕ is determinate
if one of the players has a winning strategy σ : N<N →N in the game ϕ. For
a class C of formulas, C-Det is the axiom which states that any game in C is
determinate.
3
Weak Determinacy of Games and Inductive Deﬁnitions
3.1
Inductive Deﬁnitions
We start by formalizing the axiom of inductive deﬁnition. An operator Γ :
P(N) →P(N) belongs to a class C of formulas iﬀits graph {(x, X) : x ∈Γ(X)}
belongs to C. Γ is said to be monotone iﬀΓ(X) ⊂Γ(Y ) whenever X ⊂Y . By
mon-C, we shall denote the class of monotone operators in C.
A relation W is a pre-ordering iﬀit is reﬂexive, connected and transitive. W
is a pre-wellordering iﬀit is a well-founded pre-ordering. The ﬁeld of W is the
set F = {x : ∃y (x, y) ∈W ∨(y, x) ∈W}. An axiom of inductive deﬁnition
asserts the existence of a pre-wellordering constructed by iterative application
of a given operator.
Deﬁnition 2. Let C be a set of L2 formulas. C-ID asserts that for any operator
Γ ∈C, there exists a set W ⊂N × N such that
1. W is a pre-wellordering on its ﬁeld F,
2. ∀x ∈F
Wx = Γ(W<x) ∪W<x,
3. Γ(F) ⊂F,
where Wx = {y ∈F : (y, x) ∈W} and W<x = {y ∈F : (y, x) ∈W and
(x, y) /∈W}.

Inﬁnite Games and Transﬁnite Recursion of Multiple Inductive Deﬁnitions
377
We write C-MI to denote mon-C-ID. We note that for a monotone operator Γ,
the second condition of the above deﬁnition can be replaced by
∀x ∈F
Wx = Γ(W<x).
It is also easy to see that for any class C, C-MI0 implies C-CA0.
Finally, we have
Theorem 3 (Tanaka [13]). Over RCA0, Σ0
2-Det, Σ1
1-MI, and Σ1
1-ID are equiv-
alent.
3.2
Diﬀerence Sets
Now, we consider more complex games than Σ0
2.
Deﬁnition 4. Let C and C′ be classes of formulas. We denote the classes of
formulas in the form φ ∧ψ (φ ∈C, ψ ∈C′) as C ∧C′, and ¬ψ (ψ ∈C) as ¬C.
Deﬁnition 5. For all n, k ≥1, we deﬁne the classes of the formulas (Σ0
n)k, (Π0
n)k
as follows.
– (Σ0
n)1 = Σ0
n,
(Σ0
n)k = Σ0
n ∧(Π0
n)k−1 if k > 1,
– (Π0
n)k = ¬(Σ0
n)k.
Lemma 6. For any n, k ≥1, the following hold.
– (Σ0
n)k = Π0
n ∧(Σ0
n)k−1 if k is even.
– (Σ0
n)k = Σ0
n ∨(Σ0
n)k−1 if k is odd.
Deﬁnition 7. Let C and C′be a class of formulas. If there exists a C-formula
ψ, a ¬C′-formula η, and a C′-formula η′ such that for all f ∈XN(ϕ(f) ↔
((ψ(f)∧η(f))∨(¬ψ(f)∧η′(f)))), then we call a formula ϕ a Sep(C, C′)-formula.
The above theorem can be easily extended as follows.
Theorem 8 (Mashiko et al. [4]). Over RCA0, Σ1
1-ID0, Sep(Δ0
1, Σ0
2)-Det, and
Sep(Σ0
1, Σ0
2)-Det are equivalent.
Deﬁnition 9. Let C be a class of formulas. If there exists a ¬C-formula ϕ′ such
that for all f ∈NN(ϕ(f) ↔ϕ′(f)), we call a C-formula ϕ a Δ(C)-formula.
Our objective of this paper is to pin down the strength of Δ((Σ0
2)k)-determinacy.
For this purpose, we use the following fact.
Theorem 10. Over RCA0, the class Δ((Σ0
n)k+1) is equivalent to the class
Sep(Δ0
n, (Σ0
n)k).
The following lemma can be easily shown by induction.

378
K. Yoshii and K. Tanaka
Lemma 11. For any n, k ≥1, any (Σ0
n)k formula ϕ can be expressed as follows:
ϕ ↔

(ϕ1 ∧¬ϕ2) ∨· · · ∨(ϕk−1 ∧¬ϕk)
if k even
(ϕ1 ∧¬ϕ2) ∨· · · ∨(ϕk−1 ∧¬ϕk) ∨ϕk
if k odd
(1)
where ϕi’s are Σ0
n formulas and for each i < k, ϕi+1 →ϕi holds. Conversely, if
a formula ϕ can be expressed as above, it is (Σ0
n)k. Moreover, a (Π0
n)k formula
can be expressed analogously.
Then the theorem can be proved as follows. First, to show Sep(Δ0
n, (Σ0
n)k) ⊆
Δ((Σ0
n)k+1), we need to prove the next lemma.
Lemma 12. Fix any k, n ≥1. For a Σ0
n formula ψ0(f), a Π0
n formula ψ1(f), a
(Π0
n)k formula η(f), and a (Σ0
n)k formula η′(f), there exist a (Σ0
n)k+1 formula
ζ0(f) and a (Π0
n)k+1 formula ζ1(f) such that
∀f(ψ0(f) ↔ψ1(f))
→∀f

(ζ0(f) ↔ζ1(f)) ∧

ζ0(f) ↔

(ψ0(f) ∧η(f)) ∨(¬ψ0(f) ∧η′(f))

.
Proof. Assume that k is even. Take a Σ0
n formula ψ0(f), a Π0
n formula ψ1(f),
a (Π0
n)k formula η(f) and a (Σ0
n)k formula η′(f) such that ∀f(ψ0(f) ↔ψ1(f))
holds. Since ψ(f) ∧η(f) is (Σ0
n)k+1 formula, by the lemma 11, there exist Σ0
n
formulas η1(f), · · · , ηk+1(f) such that the following hold: ηi+1(f) →ηi(f) (1 ≤
i < k) and (ψ(f)∧η(f)) ↔

(η1(f)∧¬η2(f))∨(η3(f)∧¬η4(f))∨· · ·∨(ηk−1(f)∧
¬ηk(f)) ∨ηk+1(f)

. Also, since ¬ψ(f) ∧η′(f) is a (Σ0
n)k formula, by lemma 11,
there exist Σ0
n formulas η′
1(f), · · · , η′
k(f) such that the following hold: η′
i+1(f) →
η′
i(f) (1 ≤i < k) and (¬ψ(f) ∧η′(f)) ↔

(η′
1(f) ∧¬η′
2(f)) ∨(η′
3(f) ∧¬η′
4(f)) ∨
· · · ∨(η′
k−1(f) ∧¬η′
k(f))

.
Now, for 1 ≤i ≤k, if we let φi(f) ≡ηi(f) ∨η′
i(f), φk+1(f) ≡ηk+1(f),
we have φi+1(f) →φi(f) and ((ψ(f) ∧η(f)) ∨(¬ψ(f) ∧η′(f))) ↔((φ1(f) ∧
¬φ2(f)) ∨· · · ∨(φk−1(f) ∧¬φk(f)) ∨φk+1(f)). Thus, by lemma 11, there exists
a (Σ0
n)k+1 formula ζ0(f) such that ζ0(f) ↔((ψ(f) ∧η(f)) ∨(¬ψ(f) ∧η′(f)))
holds. By the same argument, there exists a (Σ0
n)k+1 formula ζ1(f) such that
ζ1(f) ↔((ψ(f) ∧¬η(f)) ∨(¬ψ(f) ∧¬η′(f))) holds.
Since ζ0(f) ↔¬ζ1(f) and ¬ζ1(f) is a (Π0
n)k+1 formula, the statement holds.
Clearly, we can prove it in the same way when k is odd.
⊓⊔
The reverse inclusion Sep(Δ0
n, (Σ0
n)k) ⊇Δ((Σ0
n)k+1) can be shown by a similar
argument. Thus, we obtain the theorem 10.
3.3
Tranﬁnite Recursion of Inductive Deﬁnitions
Following [4], we introduce an axiom C-IDTR which asserts the existence of sets
constructed by transﬁnite recursion of the inductive deﬁnitions.

Inﬁnite Games and Transﬁnite Recursion of Multiple Inductive Deﬁnitions
379
Deﬁnition 13. The formal system C-IDTR0 consists of ACA0 and the following
axiom scheme (C-IDTR): for any well-ordering ⪯and C-operator Γ, there exists
a sequence ⟨V r : r ∈ﬁeld(⪯)⟩satisfying the following conditions.
1. V r is pre-well ordering on its ﬁeld F r = ﬁeld(V r).
2. ∀x ∈F r(V r
x = Γ F ≺r(V r
<x) ∪V r
<x).
3. Γ F ≺r(F r) ⊂F r.
where V r
x = {y ∈F r : y ≤V r x}, V r
<x = {y ∈F r : y <V r x}, F ≺r = {F r′ :
r′ ≺r}.
Especially, [mon-C]-IDTR0 is also written as C-MITR0. For C-MITR0, the sec-
ond condition of the deﬁnition may be replaced by ∀x ∈F r (V r
x = Γ F ≺r(V r
<x)).
As mentioned above, in C-IDTR0 the inductive deﬁnitions with C operator Γ
are iterated transﬁnitely many times. More precisely, we apply the inductive
deﬁnition and obtain a pre-well ordering V r0 on its ﬁeld F r0. Then, by taking
F r0 as an oracle, we again apply the inductive deﬁnition with operator Γ F r0 .
Then, we obtain a pre-well ordering V r1 on its ﬁeld F r1. We iterate this procedure
transﬁnitely many times along the given well-ordering ⪯, and then we obtain
the sequence of pre-well orderings ⟨V r : r ∈ﬁeld(⪯)⟩.
In [4], it is already stated that Σ1
1-IDTR0 and Δ((Σ0
2)2)-determinacy are equiv-
alent. We here sketch the proof.
Theorem 14. Over RCA0, the following are equivalent.
(i) Δ((Σ0
2)2)-Det.
(ii) Σ1
1-IDTR0.
(iii) Sep(Δ0
2, Σ0
2)-Det.
Proof. By theorem 10, it is suﬃcient to prove the equivalence of (ii) and (iii).
(ii) →(iii). First, we recall that Δ0
2-Det ↔Π1
1-TR0, and Σ0
2-Det ↔Σ1
1-ID
([12], [13]). Then, to construct a winning strategy of a Sep(Δ0
2, Σ0
2) game, we
may match up winning strategies for Δ0
2 games and Σ0
2 games. Thus, it is not
so hard to see that Sep(Δ0
2, Σ0
2)-Det is proved from Σ1
1-IDTR0 by a similar way
as in the proof of theorem 5.1 of [12].
(iii)→(ii). We construct a following Sep(Δ0
2, Σ0
2)-game. The proof technique
is a reﬁnement of previous researches. In particular, we count on the proof of
theorem 3.1. in [13]. A play of the game starts with player I’s choosing a pair
(y∗, r∗) and arising a question “y∗∈F r∗?”.
For this question, player II answers 1, which means “Yes”, if II thinks y∗∈
F r∗. Then, II constructs a sequece of pre-well orderings ⟨V r : r ⪯r∗⟩so that
y∗∈F r∗
1 . In this game the player who constructs the pre-well orderings is
called Pro. Conversely, the other player, who tries to point out mistakes in the
construction of Pro, is called Con.
Pro wins the game if Pro can construct the sequence of pre-wellorderings
satisfying the all conditions of Σ1
1-IDTR0 with y∗∈F r∗. Con wins otherwise.
Now we assume that Con has a winning strategy in this game. (This means
that Con knows “the correct construction” of the sequence of pre-wellorderings.)

380
K. Yoshii and K. Tanaka
As Pro constructs the pre-well orderings, since Con knows what the right
elements are for those sets, Con can points out (if any) unsuitable elements in
the Pro’s construction. (We deﬁne a term challenge to Pro’s construction as the
same meaning of pointing out.) There are two crucial techniques of this proof
in the ways of challenges, which are not used in the proof of [13]. The ﬁrst one
is that the switches of the players’ roles, Pro and Con, occur during the game.
For example, assume that when Pro insists that x ̸∈F r in the game, Con thinks
x ∈F r. Then, Con will make a challenge to that Pro’s assertion x ̸∈F r, and
Con becomes Pro and constructs V r
x on the ﬁeld(F r) such that x ∈F r.
The other technique is that we let Con point out unsuitable elements until
Con exhausts all challenges in the Pro’s construction. We consider the following
example. In the proof, we deﬁne a rule of challenge which requires Con to make
challenge in descending order. For examples, once Con made a challenge to
x ̸≤V r y then Con can not do it in V r′ for any r′ ≻r or F r \ V r
y after that
challenge.
From this rule, if Con challenges without violating the rules, Con wins in the
following two cases:
1. Con can point out the mistakes in Pro’s construction inﬁnitely many times.
2. Con can show that the last challenged element is actually not the suitable
one, if Con points out only ﬁnitely many elements.
For the ﬁrst case, if Con can challenge inﬁnitely many times, it means there is
an inﬁnite descending sequence in Pro’s construction. The second case is the
crucial point of the proof. If Con points out only ﬁnitely many times it means
that Con agrees that there is no inappropriate elements any more in the Pro’s
construction. In other words, Pro and Con agree on the initial segments of V r∗.
For example, suppose that the last challenge occurs to Pro’s assertion x ̸≤V r y
for any x, y, r. Then, Con and Pro agree on the parts V r
<y and ⟨V r′ : r′ ≺r⟩.
Con will use that agreed-part of the two players to show that y ≤V r x.
⊓⊔
3.4
Multiple Inductive Deﬁnitions and Determinacy of Games
In [6], inductive deﬁnitions with multiple operators are introduced. By com-
bining transﬁnite recursion with such inductive deﬁnitions, we introduce a new
axiom scheme [Σ1
1]k-IDTR0, and we prove that it is equivalent to Δ((Σ0
2)k+1)-
determinacy. For the simplicity, we here consider only the case k = 2.
Deﬁnition 15. The formal deﬁnition of [S0, S1]-IDTR0 consists of ACA0 and
the following axiom scheme: Let S0 and S1 are collections of operators. The
axiom scheme [S0, S1]-IDTR0 asserts the following. For any well-ordering ⪯and
any Γ0 ∈S0, Γ1 ∈S1, there exist ⟨W r : r ∈ﬁeld(⪯)⟩, ⟨V r,x : r ∈ﬁeld(⪯), x ∈
F r
1 ⟩and ⟨V r,∞: r ∈ﬁeld(⪯)⟩such that the following are all satisﬁed.
1. W r is pre-wellordering on its ﬁeld F r
1 .
2. ∀x ∈F r
1 ∪{∞}
– V r,x is pre-wellordering on its ﬁeld F r,x
0
.

Inﬁnite Games and Transﬁnite Recursion of Multiple Inductive Deﬁnitions
381
– V r,x
y
= Γ
F ≺r
1
⊕W r
<x
0
(V r,x
<y ) ∪V r,x
<y for all y ∈F r,x
0
.
– W r
x = Γ F ≺r
1
1
(F r,x
0
) ∪W r
<x.
– Γ
F ≺r
1
⊕W r
<x
0
(F r,x
0
) ⊂F r,x
0
.
3. W r
∞= W r
<∞= F r
1 .
where F ≺r
1
= ⊕{F ri
1 : ri ≺r}. Note also that X ⊕Y = {2x : x ∈X} ∪{2y + 1 :
y ∈Y }.
We see how the sequence of pre-well orderings are constructed. We assume that
W ≺r has been constructed and consider the rth construction of [C]2-IDTR0.
Γ F ≺r
1
0
-operator is applied until we get the ﬁxed point F r,x0
0
. Then, S1-operator
with oracle F ≺r
1
, which is Γ F ≺r
1
1
, is applied to F r,x0
0
, and Γ F ≺r
1
1
(F r,x0
0
) = W r
x0 is
constructed. Next, this W r
x0 is joined to F ≺r
1
, written as F ≺r
1
⊕W r
x0, and then
S0-operator becomes Γ
F ≺r
1
⊕W r
x0
0
. This procedure is repeated until F r
1 becomes
the common ﬁxed point of those two operators. Note that the pre-well orderings
constructed by [C]2-ID0 is equal to the r0th (i.e., the ﬁrst) construction by [C]2-
IDTR0, where r0 is the ⪯-least element of ﬁeld(⪯).
Then, we obtain the following theorem.
Theorem 16. The following is provable over RCA0. For any k ≥1,
Δ((Σ0
2)k+1)-Det ↔[Σ1
1]k-IDTR0.
Proof. The proof becomes complicated, but we can use the similar ideas which
are shown in the sketch of the proof of theorem 14 to prove this theorem.
⊓⊔
4
Conclusion and Future Studies
In this paper, we introduced the axiom of transﬁnite recursion of Σ1
1 inductive
deﬁnitions with k operators, denote [Σ1
1]k-IDTR0, and showed that it is equiva-
lent to the determinacy of Δ((Σ0
2)k+1) sets. A key fact used in the proof is that
a Δ((Σ0
2)k+1) set is expressed as a Sep(Δ0
2, (Σ0
2)k) set, namely a Δ0
2-separated
union of a (Σ0
2)k set and (Π0
2)k set. By virtue of this fact, we can utilize a dif-
ference hierarchy for a Δ0
2 set (cf. [12], [6]) to construct a winning strategy for a
Δ((Σ0
2)k+1) game.
In [6], the exact determinacy strength of Δ0
3 sets has been pinned down in
terms of transﬁnte combinations of Σ1
1 inductive deﬁnitions. We should notice
that their axiom for transﬁnte combinations of Σ1
1 inductive deﬁnitions is much
stronger than our [Σ1
1]k-IDTR here. However, it is worth studying such an axiom
as [Σ1
1]α-IDTR, where α is an ordinal, to reﬁne their result on Δ0
3 games.
Montalb´an ans Shore [7] show that for any m ≥1, Π1
m+2-CA proves the
determinacy of (Σ0
3)m sets, but Δ1
m+2-CA does not. Thus, (Σ0
3)ω-determinacy
is not provable over Z2. Then, Montalb´an [8] raises Question 28 to classify the
precise strength of (Σ0
3)m-determinacy.

382
K. Yoshii and K. Tanaka
In [1], Bradﬁeld has shown that the sets of Player I’s winning positions of a
(Σ0
2)k-game are exactly the same as the (k + 1)-level of μ-calculus alternation
hierarchy Σμ
k+1. Then, Bradﬁeld [2] claims that the hierarchy ⟨Σμ
n, n ∈ω⟩is
strict, that is, for any k in ω, we have Σμ
k  Σμ
k+1. This result easily follows
from the previous result on multiple inductive deﬁnitions ([6]) together with ob-
servation that for any k in ω, Π1
2-CA0 proves the consistency of Δ1
2-CA0 +(Σ0
2)k-
determinacy, while it does not prove the consistency of (Σ0
2)<ω-determinacy. (cf.
Heinatsch and M¨ollerfeld [3])
From the main result of this paper, we shall also obtain the following re-
ﬁnement. First of all, the hierarchy ⟨Πμ
n, n ∈ω⟩is naturally deﬁned and so is
⟨Δμ
n, n ∈ω⟩. Then, by the argument of this paper, we can associate a Δμ
n+1
formula with transﬁnite recursion of a Σμ
k formula. Moreover, for any k in ω, we
have Σμ
k  Δμ
k+1  Σμ
k+1 by a similar observation as above. Details will appear
in the future literature.
The research to characterize the determinacy strength in Cantor space has
been conducted by Nemoto, MedSalem, and Tanaka. See [10] and table 2 of
[9]. See also Question 29 in A. Montalb´an’s survey paper [8]. To investigate
the relationships between the determinacies strength in Cantor space and Baire
space, a technique which translates the games in Baire space into the games
in Cantor space was introduced in [10]. Translate here means that, for a game
φ(f) in Baire space, the game φ∗(f) in Cantor space is constructed so that
the same player with the winning strategy in φ(f) has a winning strategy. By
constructing this king of game φ∗(f), if we assume that φ ∈C and φ∗∈C′, then
we can prove over appropriate system that C′-Det∗is provable from C-Det. By
applying this method, we obtain that, for any k > 0, [Σ1
1]k-IDTR0 is equivalent
to Δ((Σ0
2)k+2)-Det∗.
Subsystem of SOA Determinacy in Baire space Determinacy in Cantor Space
ATR0
Δ0
1, Σ0
1
Δ0
2, Σ0
2
Π1
1-CA0
Δ((Σ0
1)2), (Σ0
1)2
Sep(Σ0
1, Σ0
2)
Π1
1-TR0
Δ0
2
Δ((Σ0
2)2)
Σ1
1-ID0
Σ0
2
(Σ0
2)2
Sep(Σ0
1, Σ0
2)
Σ1
1-IDTR0
Δ((Σ0
2)2)
Δ((Σ0
2)3)
...
...
...
[Σ1
1]k-ID0
(Σ0
2)k
(Σ0
2)k+1
[Σ1
1]k-IDTR0
Δ((Σ0
2)k+1)
Δ((Σ0
2)k+2)
...
...
...
[Σ1
1]ω-ID0
(Σ0
2)ω
(Σ0
2)ω
[Σ1
1]TR-ID0
Δ0
3
Δ0
3

Inﬁnite Games and Transﬁnite Recursion of Multiple Inductive Deﬁnitions
383
The above diagram shows the results on determinacy strength of inﬁnite games
in second order arithmetic. The left column contains subsystems of second order
arithmetic from weaker to stronger. The center and the right most columns
contain classes of the games in Baire space and Cantor space, respectively. Each
row represents that a certain axiom is equivalent to the determinacy of the
corresponding games over appropriate systems (RCA0, but with Π1
3-TI for the
last row).
Acknowledgements. The ﬁrst author was partially supported by Global COE
program. This research was partially supported by KAKENHI 23340020.
References
1. Bradﬁeld, J.C.: Fixpoints, games and the diﬀerence hierarchy. Theor. Inform.
Appl. 37, 1–15 (2003)
2. Bradﬁeld, J.C.: The modal μ-calculus alternation hierarchy is strict. Theor. Com-
put. Sci. 195, 133–153 (1998)
3. Heinatsch, C., M¨ollerfeld, M.: The determinacy strength of Π1
2-comprehension.
Ann. Pure Appl. Logic 161, 1462–1470 (2010)
4. Mashiko, K., Tanaka, K., Yoshii, K.: Determinacy of the Inﬁnite Games and In-
ductive Deﬁnition in Second Order Arithmetic. In: RIMS Kokyuroku, vol. 1729,
pp. 167–177 (2011)
5. MedSalem, M.O., Tanaka, K.: Δ0
3-determinacy, comprehension and induction.
Journal of Symbolic Logic 72, 452–462 (2007)
6. MedSalem, M.O., Tanaka, K.: Weak determinacy and iterations of inductive def-
initions. Lect. Notes Ser. Inst. Math. Sci. Natl. Univ. Singap., vol. 15. World Sci.
Publ., Hackensack (2008)
7. Montalb´an, A., Shore, R.A.: The Limits of determinacy in second order arithmetic
(preprint)
8. Montalb´an, A.: Open Questions in Reverse Mathematics. Bulletin of Symbolic
Logic 17, 431–454 (2011)
9. Nemoto, T.: Determinacy of Wadge classses and subsystems of second order arith-
metic. Math. Log. Quart. 55(2), 154–176 (2009)
10. Nemoto, T., MedSalem, M.O., Tanaka, K.: Inﬁnite games in the Cantor space and
subsystems of second order arithmetic. Math. Log. Quart. 53, 226–236 (2007)
11. Simpson, S.G.: Subsystems of Second Order Arithmetic. Springer (1999)
12. Tanaka, K.: Weak axioms of determinacy and subsystems of analysis I (Δ0
2 games).
Z. Math. Logik Grundlag. Math. 36, 481–491 (1990)
13. Tanaka, K.: Weak axioms of determinacy and subsystems of analysis II (Σ0
2 games).
Ann. Pure Appl. Logic 52, 181–193 (1991)

A Hierarchy of Immunity and Density
for Sets of Reals
Takayuki Kihara
Mathematical Institute, Tohoku University, Sendai 980-8578, Japan
kihara.takayuki.logic@gmail.com
Abstract. The notion of immunity is useful to classify degrees of non-
computability. Meanwhile, the notion of immunity for topological spaces
can be thought of as an opposite notion of density. Based on this view-
point, we introduce a new degree-theoretic invariant called layer density
which assigns a value n to each subset of Cantor space. Armed with
this invariant, we shed light on an interaction between a hierarchy of
density/immunity and a mechanism of type-two computability.
1
Introduction
1.1
Summary
The study of immunity was initiated essentially by Post in 1944. Demuth-Kuˇcera
[5] studied the notion of immunity for closed sets in Baire space. Immunity for
a closed set indicates that it is “far from dense”. They showed that any 1-
generic real computes no element of any immune co-c.e. closed set, and hence no
1-generic real computes a Martin-L¨of random real. Binns [1] introduced many
notions of hyperimmunity for closed sets to classify degrees of diﬃculty of co-
c.e. closed sets. Cenzer-Kihara-Weber-Wu [4] started the systematic study on
immunity for closed sets. Higuchi-Kihara [6] clariﬁed that such notions indicat-
ing being “nearly/far from dense” are extremely useful to study a hierarchy of
nonuniform computability on sets of reals. We investigate a hierarchy of prop-
erties that are “nearly dense”, by introducing a new degree-theoretic invariant
called layer density which assigns a value n to each subset of any computable
metric space. In this way, we shed light on an interaction between a hierarchy of
density and a mechanism of type-two computability. We also continue the work
[6] on the structure inside the Turing upward closure of any co-c.e. closed set.
1.2
Notation and Convention
Much of our notation in this paper follows that in [6]. For basic terminology on
Computability Theory and Computable Analysis, see [3,8,9]. For any sets X and
Y , f is said to be a function from X to Y if dom(f) ⊇X and range(f) ⊆Y
hold. We use the symbol ⌢for concatenation. For σ ∈ω<ω, we let |σ| denote the
length of σ. Moreover, f ↾n denotes the unique initial segment of f of length
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 384–394, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

A Hierarchy of Immunity and Density for Sets of Reals
385
n. We also deﬁne [σ] = {f ∈ωω : f ⊃σ}. For a tree T ⊆ω<ω, let [T ] denote
the set of all inﬁnite paths through T . For a subset A of a space X, cl(A), and
ext(A) denote the closure, and the exterior of A, respectively. A representation
ρ of a space X is a surjection ρ :⊆ωω →X. Let A−(X) denote the hyperspace
consisting of closed subsets of X represented by ψ−: α →X \ 
n βα(n). Here,
{βn}n∈ω is a ﬁxed countable base of X. A computable element of A−(X) (i.e.,
ψ−(α) for some computable α ∈ωω) is called a co-c.e. closed set or a Π0
1 class.
2
Computability with Layers
2.1
Density and Immunity
Let X be a topological space, and B be a collection of open sets in X. A subset
S ⊆X is said to be B-dense if it intersects with all nonempty open sets contained
in B. By restricting B, one may introduce various “pre-dense” properties. For
instance, immunity [4] and hyperimmunity [1] can be introduced in this way.
A variety of interactions are known between density/immunity and degrees of
diﬃculty [4,5,7]. To introduce nice B-density notion, we consider the following
eﬀective notion for open sets: An open set S ⊆X is bi-c.e. open if both S and
ext(S) are c.e. open. We ﬁx X = 2ω. A sequence {Bn} of open rational balls
is nontrivial if it contains no empty set, and lim infn diam(Bn) = 0; computable
if it is uniformly computable (hence, 
n Bn is c.e. open); and decidable if it is
computable, and 
n Bn is bi-c.e. open. Let P ⊆2ω be a closed set, and let
T ext
P
denote the tree {σ ∈2<ω : P ∩[σ] ̸
= ∅}. Cenzer et al. [4] introduced the
following notion: P is immune if T ext
P
contains no inﬁnite computable subset. P
is tree-immune if T ext
P
contains no inﬁnite computable subtree.
Proposition 1. Let P ⊆2ω be a closed set with no computable element. Then,
P is not immune if and only if it is B-dense for some nontrivial computable
sequence B of open balls; P is not tree-immune if and only if it is B-dense for
some nontrivial decidable sequence B of pairwise disjoint open balls.
Proof. Assume that P is B-dense via an inﬁnite computable sequence B of open
balls. For each B ∈B, we choose the smallest clopen set [σ] including B, and
enumerate [σ] into another sequence B∗. As lim infB∈B diam(B) = 0, the se-
quence B∗is inﬁnite. It is easy to see that P is also B∗-dense. Therefore, P is
not immune. Another direction is obvious.
Assume that P is not tree-immune via an inﬁnite computable tree V ⊆T ext
P
.
As P has no computable element, V has inﬁnitely many leaves, i.e., L = {σ ∈
V : (∀i < 2) σ⌢i ̸
∈V } is inﬁnite. Then, we deﬁne B = {[σ] : σ ∈L}. To
enumerate the exterior of  B, for each σ ∈2<ω, we deﬁne (σ⌢i)∗= σ⌢(1 −i)
for each i < 2. Then, the exterior of  B is generated by the computable set
{σ ∈2<ω \ V : σ∗∈V }, since [V ] has no interior. Hence,  B is bi-c.e. open.
Conversely, assume that P is B-dense for a decidable sequence B = {[σn]}n∈ω
of open balls. Then, there is a computable enumeration of all strings σ that are
comparable with σn for some n ∈ω, since B = {σn}n∈ω is computable. Moreover,

386
T. Kihara
[σ] ⊆ext( B) if and only if there is no n ∈ω such that σ is comparable with
σn. Hence, the set U consisting of all strings σ ∈2<ω which are comparable
with some σn is computable, since ext( B) is c.e. open. Then, we can compute
the tree V = {σ ∈2<ω : (∃n ∈ω) σ ⊆σn} as follows: If σ ̸
∈U, then declare
σ ̸
∈V . If σ ∈U, then σ must be comparable with some σn. Wait for the least
such n ∈ω, and if σ ⊆σn, then declare σ ∈V . Otherwise, declare σ ̸
∈V .
This algorithm correctly computes V , since the sequence {σn}n∈ω is pairwise
incomparable. Then, for each σ ⊆σn, the open ball [σ] ⊇[σn] intersects with P,
by B-density of P.
⊓⊔
By considering layers {Bj}j∈ω, {Bj,k}j,k∈ω, {Bj,k,l}j,k,l∈ω, . . . of open balls hit-
ting a set P ⊆2ω, we may strengthen the notion of B-density. Here, it is required
that P is {Bj}j∈ω-dense; P ∩Bj is {Bj,k}k∈ω-dense for each j ∈ω; P ∩Bj ∩Bj,k
is {Bj,k,l}l∈ω-dense for each j, k ∈ω, . . .
Deﬁnition 1. Let Y be a subset of X = 2ω.
1. A sequence {Bn,m}(n,m)∈I×J of open balls is an J-reﬁnement of {An}n∈I in
Y if it is pairwise disjoint, and Bn,m ⊆An for any (n, m) ∈I × J.
2. A sequence {Bk}k<n (resp. {Bk}k∈ω) of decidable sequences of nonempty
open rational balls is an n-layer in Y (resp. an ∞-layer) if Bk+1 = {Bk+1
i,j }i,j
is an ω-reﬁnement of Bk = {Bk
i }i in Y , and {Bk+1
i,j }j∈ω is decidable uni-
formly in i, for any k < n −1 (resp. for any k ∈ω).
3. For n ∈ω ∪{∞}, a set P ⊆X is n-layered if there is an n-layer B = {Bk}
in P such that P is  B-dense, where B0 = {X}.
4. The layer density of a set P ⊆X is deﬁned as follows:
density(P) = sup{n ∈ω ∪{∞} : P is n-layered }.
Here, the ordering on ω ∪{ω, ∞} is deﬁned as n < ω < ∞for any n ∈ω.
Proposition 2. Let P be a subset of 2ω. Then, P is empty if and only if
density(P) = 0; If Q ⊆P, then density(Q) ≤density(P); If P is dense, then P
is ∞-layered.
⊓⊔
Proposition 3. Let P ⊆2ω be a closed set with no computable element. Then,
P ⊆2ω is n-layered if and only if there is a sequence {Ti}i<n of inﬁnite com-
putable trees such that [Tn] ⊆P for any i < n, and Ti ⊆T ext
i+1 for any i < n −1.
Proof. Assume that P ⊆2ω has such a sequence {Ti}i≤n of inﬁnite computable
trees. We eﬀectively enumerate all leaves {σi
k}k∈ω of the tree Ti, for each i < n.
Then, as Proposition 1, {2ω, {[σ0
k]}k∈ω, . . . , {[σn−1
k
]}k∈ω} forms an n-layer of P.
Conversely, assume that P ⊆2ω is n-layered via {Bi}i≤n. As in the proof
of Proposition 1, without loss of generality, we may assume Bi is of the form
{[σi
k]}k∈ω, for each i ≤n. Then, we deﬁne Ti = {σ ∈2<ω : (∃k ∈ω) σ ⊆
σi+1
k
}. We can see that Ti is computable for each i < n, as Proposition 1. Then,
{T0, T1, . . . , Tn−1, TP } is the desired sequence.
⊓⊔

A Hierarchy of Immunity and Density for Sets of Reals
387
Example 1. Let P be a co-c.e. closed subset of 2ω. Then, for a ﬁxed computable
tree TP with P = [TP ], we have the computable set {ρn}n∈ω of all leaves of TP .
The concatenation P ⌢P is deﬁned by 
n ρn⌢P. Consider P (1) = P; P (n+1) =
P ⌢P (n); P (ω) = 
n ρn⌢P (n); and P (∞) = 
n P (n). Then, density(P (n)) ≥n;
density(P (ω)) ≥ω; and density(P (∞)) = ∞. See also Higuchi-Kihara [6].
2.2
Learnability on Topological Spaces
When we try to extract eﬀective content in classical mathematics, we sometimes
encounter the notion of nonuniform computability [2,10]. The deep structures of
subnotions of nonuniformly computability have been studied [6].
Deﬁnition 2 (Learnability). Let X be a topological space with a representa-
tion θ :⊆ωω →X, and ﬁx a new symbol ? ̸
∈X.
1. The representation θ? of the space X? = X ∪{?} is deﬁned as θ?(⟨0⟩⌢α) =
θ(α), and θ?(⟨1⟩⌢α) =?, for any α ∈ωω.
2. A sequence {fn}n∈ω of partial functions fn :⊆Y →X? is ?-good if ? ∈
{fn(α), fn+1(α)} whenever fn(α) ̸
= fn+1(α).
3. The discrete limit of a ?-good sequence {fn}n∈ω of partial functions fn :⊆
Y →X? is a partial function limn fn :⊆Y →X deﬁned as follows.
lim
n fn(α) =

ft(α),
if (∀s ≥t) fs(α) ̸
=?,
undeﬁned,
if (∃∞s) fs(α) =?.
4. A function f :⊆Y →X is learnable if it is the discrete limit of a computable
?-good sequence {fn}n∈ω of partial functions fn :⊆Y →X?.
5. An anti-Popperian point of a ?-good sequence {fn}n∈ω is a point α ∈ωω
such that fn(α) =? at most ﬁnitely many n ∈ω, but limn fn(α) is undeﬁned.
6. A function f : Y →X is eventually Popperian learnable (abbreviated as
e.P. learnable) if it is the discrete limit of a computable ?-good sequence
{fn}n∈ω of partial functions fn :⊆Y →X? with no anti-Popperian points.
Lemma 1 (Blum-Blum Locking). Let (X, d) be a Polish space with a repre-
sentation, and Q be a closed set in X. For every learnable function Γ : Q →P,
there is an open set U ⊆X such that Q ∩U ̸
= ∅, and the restriction Γ|U :
Q ∩U →P is computable.
Proof. Suppose not. Fix a learnable function Γ = lims Γs : Q →P witnessing
the falsity of the assertion. Then, for any open set U ∗
0 and every s0 ∈ω, there is
s1 ≥s0 such that the open set U1 = Γ −1
s1 {?} has a nonempty intersection with
Q. Then U1 contains an open ball {p ∈X : d(p, q) < ε} with q ∈Q and ε > 0.
Pick U ∗
1 = {p ∈X : d(p, q) < min{ε/2, 2−n}} ⊆U1. By iterating this procedure,
we can get a decreasing sequence {U ∗
n}n∈ω. Choose xn ∈U ∗
n ∩Q. Then, {xn}n∈ω
converges to an element x ∈Q ∩
n cl(U ∗
n). By our choice of {U ∗
n}n∈ω, we see
that Γs(x) =? for inﬁnitely many s ∈ω. Consequently, Γ(x) = lims Γs(x) is
undeﬁned, i.e., dom(Γ) ̸
⊇Q.
⊓⊔

388
T. Kihara
3
Degrees of Diﬃculty
3.1
Layer Density as a Degree-Theoretic Invariant
Theorem 1. Let P, Q ⊆2ω be co-c.e. closed sets with no computable element.
If a computable function exists from P to Q, then density(P) ≤density(Q).
Proof. A sequence {Tm}m<n of inﬁnite computable trees is said to be an n-layer
if T ext
m
⊆Tm+1 for each m < n −1. This deﬁnition is essentially equivalent to
the deﬁnition of n-layers of open balls, by Proposition 3. Let P be an n-layered
co-c.e. closed set with an n-layer {Tm}m<n, and Q be a co-c.e. closed set. Let
Φ be a computable function from P to Q. As P is co-c.e. closed, we may safely
assume that Φ is total. It suﬃces to show that the sequence {Φ(Tm)}m<n of
images of Tm’s under Φ forms an n-layer of Q. Note that Φ(Tm) is computable
for any m ≤n, by totality of Φ. Fix m < n−1. For each leaf ρ of Φ(Tm), we must
have a leaf ρ∗of Tm with Φ(ρ∗) = ρ. As Tm ⊆T ext
m+1, there are inﬁnitely many
nodes of Tm+1 extending ρ∗. By weak K¨onig’s lemma, Tm+1 has an inﬁnite path
g extending ρ∗, and then g belongs to P, since [Tm+1] ⊆P. Therefore, Φ(g) ∈Q
by our assumption that dom(Φ) includes P. Then, Φ(Tm+1) has a path Φ(g) ∈Q
extending Φ(ρ∗) = ρ, i.e., ρ ∈Φ(Tm) is extendible in Φ(Tm+1). Hence, we have
Φ(Tm) ⊆(Φ(Tm+1))ext, as desired.
⊓⊔
Deﬁnition 3. Fix P ⊆X. The layer density of a point α ∈X on P is deﬁned
as densityP (α) = inf{density(P ∩O) : α ∈O ∈Σ0
1(X)}. For n ∈ω ∪{ω, ∞} a
point α ∈X is an n-layered accumulation point of P if densityP (α) ≥n.
Theorem 2. Let P, Q ⊆2ω be co-c.e. closed sets with no computable element. If
a learnable function exists from P to Q, then density(P) ≤max{ω, density(Q)}.
Proof. Fix an ∞-layered co-c.e. closed set P ⊆2ω and a computable function
F : P →2ω. By Blum-Blum Locking Lemma 1, there is a string σ extendible in
P ♥= {α ∈P : densityP (α) = density(P)} such that F ↾[σ] is computable, since
P ♥is nonempty and closed. Moreover, density(P ♥) = density(P) = ∞. The
image of an ∞-layer by a computable function is again an ∞-layer. Therefore,
F(P) is ∞-layered.
⊓⊔
For elements a, b of a lattice L, we say that a cups to b if a is one-half of a
witness of join-reducibility of b. For a bounded lattice L and a ∈L, we also
say that a is cuppable in L if a cups to max L. We deﬁne preorders ≤1
1 and ≤1
ω
on P(ωω) as follows: P ≤1
1 Q (resp. P ≤1
ω Q) if there is a partial computable
(resp. learnable) function F on ωω such that dom(F) ⊇P and F(P) ⊆Q. The
structures P(ωω)/ ≡1
1 and P(ωω)/ ≡1
ω form lattices, where the supremum in
these lattices are given by P ⊗Q = {p ⊕q : (p, q) ∈P × Q}. The former lattice
is called the Medvedev lattice, and the latter lattice is said to be the degrees of
nonlearnability [6].
Theorem 3. For each n ∈ω ∪{∞}, let LDn denote the set of all Medvedev
degrees of n-layered co-c.e. closed sets in 2ω. Then, the set LDn is a princi-
pal prime ideal in LD1, and every element of LDn+1 is noncuppable in LDn.

A Hierarchy of Immunity and Density for Sets of Reals
389
Moreover, LD∞is a principal prime ideal in the degrees of nonlearnability of
nonempty co-c.e. closed sets.
Proof. See Cenzer et al. [4, Corollary 4.13]. Indeed, the top element of LDn is the
Medvedev degree of PA(n), where PA denotes the set of all consistent complete
theories extending Peano Arithmetic. For principality, by Higuchi-Kihara [6],
PA(n+1) is noncuppable in LDn, i.e., PA(n+1) does not cup to PA(n).
⊓⊔
Fix a countable base O of Cantor space 2ω. A set P ⊆2ω is totally ∞-layered
if it is ∞-layered, and there exists a computable function B : O × ω →(Oω)<ω
such that B(U, n) forms an n-layer of P ∩U, whenever P ∩U is ∞-layered.
Example 2. Fix a co-c.e. closed set P = [TP ] ⊆2ω. Then P ▼denotes the set of all
inﬁnite paths through the tree consisting of strings of the form ρ0⌢τ(0)⌢ρ1⌢τ(1)
⌢ρ2⌢. . . ⌢ρ|τ|−1⌢τ(|τ|−1)⌢σ, where σ, τ ∈TP and each ρi is a leaf of TP . Then,
P ▼is totally ∞-layered, and (P ▼)♥= {α ∈P ▼: densityP ▼(α) = density(P ▼)}
is co-c.e. closed.
Theorem 4. If a totally ∞-layered set P has a co-c.e. closed subset P ⋆con-
sisting of ∞-layered accumulation points, then P is noncuppable in the degrees
of nonlearnability of co-c.e. closed subsets of 2ω.
Lemma 2. Let C(X) denote the space of all continuous functions on X. There
exists a computable function Ξ : C(ωω) × A−(2)ω × (2<ω)ω × ωω →ωω such
that, for any (f, H, (σi)i∈ω, α) ∈C(ωω) × A−(2)ω × (2<ω)ω × ωω, if the im-
age of f|[σi]⊗{α} intersects with the product set H ⊆2ω for every i ∈ω, then
Ξ(f, H, (σi)i∈ω, α) is contained in H.
Proof. Indeed, the proof of Cenzer et al. [4, Theorem 5.2] is uniform, where
their theorem states that, if a co-c.e. closed set P is B-dense for some inﬁnite
computable sequence B = {[σi]}i∈ω of intervals (i.e., P is not immune), then it
does not cup to any separating class H ∈A−(2)ω. In other words, if a computable
function f : P ⊗R →H exists, then we have a computable function Ξ : ωω →ωω
such that Ξ(α) ∈H for any α ∈R.
⊓⊔
Proof (Theorem 4). Fix a learnable function F = lims Fs : P ⊗R →PA. Note
that P ⋆⊗{g} is closed for any g ∈R. Therefore, by Blum-Blum Locking Lemma
1, there must exist an extendible string ρ in P ⋆such that Gρ = F|(P ⋆∩[ρ])⊗{g}
is computable. Then, we can ﬁnd a sequence {σρ
i }i∈ω extending ρ such that
P ⋆∩[σρ
i ] ̸
= ∅, since P is totally ∞-layered. Therefore, Ξ(Gρ, PA, (σρ
i )i∈ω, g) is
contained in PA, where Ξ is a computable function in Lemma 2. From an input
g ∈R, one can learn a ρg such that ρg ∈P ⋆and Γs|ρg⊗{g} = Γ|ρg||ρg⊗{g} for
any s ≥|ρg|, since the assertion Γs|Y = Γt|Y is equivalent to the following: for
any clopen set [σ] and any u ∈[t, s], such that Γ −1
t
({?}) ∩Y ̸
= ∅. Here recall
that {?} is a clopen set in (ωω)?, and hence, Γ −1
t
({?}) is c.e. open. Therefore,
there is a Π0
1(g) statement characterizing ρg, uniformly in g ∈R. Then, we have
a learnable function h = lims hs : R →2ω which maps g to such ρg. Deﬁne
Δs(g) =? if hs(g) =?, and Δs(g) = Ξ(Ghs(g), PA, (σhs(g)
i
)i∈ω, g) otherwise. It is
easy to see that the learnable function Δ = lims Δs maps R into PA.
⊓⊔

390
T. Kihara
3.2
Topological Games and Popperian Learnability
By Lewis-Shore-Sorbi [7], the initial segment (0, d] below the Medvedev degree
d of a dense set in ωω has no co-c.e. closed set. There are other density-like
properties making co-c.e.-free initial segments:
For a set S ⊆X, the two-players game GS is deﬁned as follows: Each play
is a decreasing sequence {Un}n∈ω of open sets with S ∩Un ̸
= ∅. For a play
p = {Un}n∈ω, Player II wins on p if S ∩
n Un ̸
= ∅. Otherwise, Player I wins. If
Player II has a winning strategy for the game GS, then S is called Choquet.
Player I:
U0
U2
U4
. . .
⊇
⊇
⊇
⊇
⊇
⊇
Player II:
U1
U3
U5
. . .
Theorem 5. Assume that a set P ⊆2ω contains a Choquet subset C ⊆P
whose closure has a dense subset of computable points. For any co-c.e. closed
set Q ⊆2ω, if an e.P. learnable function exists from P to Q, then Q contains a
computable element.
Proof. Let F :⊆ωω →ωω be a partial learnable function. A partial computable
function f :⊆ω<ω →ω<ω ∪{?} is said to be an approximation of F if:
– (?-goodness) f(σ−) ̸
⊆f(σ) occurs only when ? ∈{f(σ−), f(σ)};
– (Convergence) F(x) = lims f(x ↾s), for any x ∈dom(F).
Fix a winning strategy ψII for Player II on the Choquet game GC, a co-c.e. closed
set Q ⊆2ω with no computable element, and suppose that an e.P. learnable
function F : P →Q exists. Fix also an approximation f : ω<ω →ω<ω ∪{?}
of F. Choose any string τi with [τi] ∩C ̸
= ∅. Since cl(C) has a dense subset of
computable points, C is dense at a computable point βi ⊃τi. Note that, if f(βi ↾
n) ̸
=? for any n ≥|τi|, then [f(σ)]∩Q = ∅for some σ ⊂βi. Otherwise, since F is
e.P., we have limn f(βi ↾n) ∈Q. However, monotonicity of {f(βi ↾n)}|τi|≤n∈ω
implies that limn f(βi ↾n) is computable. This contradicts our assumption that
Q contains no computable element. If f(σ) ̸
∈TQ happens for some σ ⊂βi
extending τi, for any α ∈C extending σ, we have f(σ) ̸
⊆lims f(α ↾s) ∈Q,
since F(C) ⊆Q. Therefore, f(σ∗) =? must occur for some σ∗with τi ⊂σ∗⊂α.
Then, deﬁne ψI(τi) = σ∗. Player II extend it to τi+1 = ψII(ψI(τi)). Eventually
an inﬁnite increasing sequence {τi}i∈ω is constructed, and then h = limi τi ∈C
by the property of ψII. However, limn f(h ↾n) does not converge. Therefore,
h ̸
∈dom(F).
⊓⊔
Deﬁnition 4. Fix a set S ⊆X, and we consider the Choquet game GS.
1. A function ψ is a strategy if, for a given open set bn in X, ψ(bn) is an open
subset of bn, and S ∩ψ(bn) ̸
= ∅whenever S ∩bn ̸
= ∅.
2. A function ψ is a prestrategy if, for a given previous move aθ, ψ(aθ) is a
pair ⟨bθ0, bθ1⟩of open sets with bθ0 ∪bθ1 ⊆aθ, or ψ(aθ) = Resign, where
we declare that S ∩Resign = ∅.
3. For a strategy ψI and a prestrategy ψII, the preplay ψI ⊗ψII produced by ψI
and ψII is a collection ⟨a⟨⟩, bθj, aθj⟩θ∈2<ω,j<2, where a⟨⟩= ψI(⟨⟩), ψII(aθ) =
⟨bθ0, bθ1⟩, and aθj = ψI(bθj) for any θ ∈2<ω, and j < 2.

A Hierarchy of Immunity and Density for Sets of Reals
391
a⟨⟩
Resign
Resign
Resign
b0
b1
a0
a1
Resign
Resign
I
II
I
II
I
b00
b01
b10
a00
a01
a10
b001
b011
b100
b101
II
I
II
I
. . .
The play along 0110 . . .
b0110
Fig. 1. A preplay on a given Choquet game
4. For a preplay p = ⟨a⟨⟩, bθj, aθj⟩θ∈2<ω,j<2, the play of p along h ∈2ω is
deﬁned by the inﬁnite sequence p|h = ⟨a⟨⟩, bθ, aθ⟩θ⊂h.
5. The play tree Play(ψI ⊗ψII) of a preplay ψI ⊗ψII = ⟨a⟨⟩, bθj, aθj⟩θ∈2<ω,j<2 is
deﬁned by Play(ψI⊗ψII) = {θ : (∀η ⊆θ) bη ̸
= Resign}. For a partial preplay
π ⊂ψI ⊗ψII, the play tree Play(π) is also deﬁned in the same manner.
6. A prestrategy ψII for Player II is winning if, for every strategy ψI for Player
I, Player II wins on the play of ψI ⊗ψII along any inﬁnite path h through
Play(ψI ⊗ψII), i.e., S ∩
n(ψI ⊗ψII|h)(n) ̸
= ∅for any h ∈[Play(ψI ⊗ψII)].
7. A function ψ is a playful strategy if it is a prestrategy, and the play tree
Play(φ ⊗ψ) has an inﬁnite path for any strategy φ.
8. If Player II has a computable winning playful strategy for the game GS, then
S is called PA-Choquet.
A partial computable function β : ω<ω →ωω is a dense choice of computable
points in C if C ∩[σ] is dense at the point β(σ), whenever C ∩[σ] is nonempty.
Theorem 6. Assume that a set P ⊆2ω contains a PA-Choquet subset C ⊆P
whose closure has a dense choice of computable points. For any co-c.e. closed set
Q ⊆2ω and any R ⊆ωω, if an e.P. learnable function exists from P ⊗R to Q,
then an e.P. learnable function exists from R to Q.
Proof. Fix a computable winning playful strategy ψII for the player II on the
Choquet game GC, a co-c.e. closed set Q ⊆2ω, and an e.P. learnable function
F : P ⊗R →Q with an approximation f : ω<ω →ω<ω ∪{?}. Let β be a dense
choice of computable points in C. Fix g ∈R.
Strategies Sg
θ. We introduce a strategy Sg
θ for each θ ∈2<ω. There are four
states for strategies, Active, Changed, Refuted, and Resigned. First we
declare the root strategy Sg
⟨⟩to be Active. Assume that, on a partial play on
the Choquet game GC, the θ-th move τ g
θ of ψII is given, Sg
θ is Active, and there
is no Active strategy Sg
κ for κ ⊊θ. We determine the state of the θ-th strategy
Sg
θ as follows:
– Sg
θ is Changed if f(σ ⊕g) =? for some τg
θ ⊂σ ⊂β(τ g
θ ).
– Sg
θ is Refuted if f(σ ⊕g) ̸
∈TQ for some τg
θ ⊂σ ⊂β(τ g
θ ).
– Sg
θ is Resigned when we ﬁnd that θ does not extend to an inﬁnite path
through the play tree Play(ψI ⊗ψII) of the winning strategy ψII.

392
T. Kihara
If Sg
θ is declared to be Changed, or Resigned, then we withdraw the previous
declaration that Sg
θ is Active, and close the strategy Sg
θ.
Play on Choquet Game GC. Now we determine the next move of Player I, i.e.,
deﬁne ψI(τ g
θ ). If Sg
θ is Refuted or Resigned, then Player I takes no action. If
Sg
θ is Changed, then Player I chooses the least σ such that Sg
θ is refuted at σ,
and put ψI(τ g
θ ) = σ. Then, by using the winning strategy ψII, Player II chooses
the (θ0)-th move τ g
θ0 and the (θ1)-th move τg
θ1, from the partial play ψI(τ g
θ ), i.e.,
ψII(ψI(τ g
θ )) = ⟨τ g
θ0, τg
θ1⟩, and declare that the strategies Sg
θ0 and Sg
θ1 are Active.
Note that τ g
θ and the state of Sg
θ at each stage are partial computable uniformly
in θ and g, since ψII and β are computable.
Observation. For any g ∈R, consider the following binary tree V g consisting
of all binary strings θ ∈2<ω such that Sg
θ is declared to be Active at some
stage. Claim that V g has no inﬁnite path. If V g has an inﬁnite path h, then f
must outputs ? inﬁnitely often along ph = 
θ⊂h τθ. However, ph is constructed
along the winning strategy ψII, and ph is an inﬁnite path through the play
tree Play(ψI ⊗ψII), since no substring of h is Resigned. As ψII is winning, ph
must belong to C ⊆P. It implies that F(ph ⊕g) = lims f(ph ⊕g ↾s) does
not converges, and note that ph ⊕g ∈C ⊗{g} ⊆P ⊗R, This contradicts our
assumption that the domain of F includes P ⊗R.
Thus, at some stage, all declarations of strategies on V g are determined.
Moreover, each leaf of V g which is not assigned Resign by ψII must be declared
to be Active at almost all stages. Because C is dense at β(τ g
ρ ) for each leaf
ρ ∈V g which is not declared to be Resigned, and then lims f(β(τ g
ρ ) ⊕g ↾s) is
total, since F = lim f is e.P., and each leaf ρ ∈V g must not be declared to be
Changed. In particular, lims f(β(τ g
ρ ) ⊕g ↾s) ∈Q.
Learning Procedure. We construct an e.P. learnable function G : R →Q. The
learner G(g) tries to ﬁnd an Active leaf ρ of V g at each stage s, and set
G(g) = F(β(τg
θ ) ⊕g). Each time his guess on an eventually Active leaf of V g
is changed, an approximation of G returns ?. If g is contained in R, then by
ﬁniteness of V g, an approximation of G(g) eventually ﬁnds an Active leaf of
V g. If g ̸
∈R, then G(g) may yet fail to ﬁnd an Active leaf of V g. But then
its approximation returns ? inﬁnitely often. Otherwise, G(g) is deﬁned to be
F(β(τ g
θ )⊕g), and then it is e.P., since F is e.P. By the previous observation, the
e.P. learnable function G maps R into Q as desired.
⊓⊔
Deﬁnition 5 (Higuchi-Kihara [6]). Fix σ ∈ω<ω, and i ∈ω. Then the i-th
projection of σ is inductively deﬁned as follows.
pri(⟨⟩) = ⟨⟩,
pri(σ) =

pri(σ−)⌢n, if σ = σ−⌢⟨i, n⟩,
pri(σ−), otherwise.
Furthermore, the projection of x ∈ωω is deﬁned to be pri(x) = limn pri(x ↾n).
Theorem 7. For every co-c.e. closed set P ⊆2ω, for each k ≥2, the set
TeamkLearning(P) = {x ∈ωω : (∃i < k) pri(x) ∈P (∞)} is a Σ0
3 subset

A Hierarchy of Immunity and Density for Sets of Reals
393
of 2ω which has the same Turing upward closure as P, and has a PA-Choquet
subset whose closure has a dense choice of computable points.
Proof. Set S = Team2Learning(P). Straightforwardly, we can check that S is
Σ0
3, and it has the same Turing upward closure as P. Consider the following set:
C = {x ∈ωω : pr0(x) ∈P (∞) & (∀n ∈ω) pr1(x ↾n) ∈T ext
P
}.
Clearly, C is a subset of S. To construct a dense choice β of computable points
in the closure of C, we ﬁx a leaf of TP . Given σ, if it has a nonempty intersection
with C, then pr0(σ) must be of the form ρ0⌢ρ1⌢. . . ⌢ρn⌢τ, where ρi is a leaf of
TP for each i ≤n, and τ is a node of TP . By a uniformly computable way, we
can calculate the position of a leaf τ⌢η of TP . Then, deﬁne β(σ) as follows:
β(σ) = σ⌢(0|η| ⊕η)⌢(0|ρ| ⊕ρ)⌢(0|ρ| ⊕ρ)⌢. . . ⌢(0|ρ| ⊕ρ)⌢(0|ρ| ⊕ρ)⌢. . .
Here, 0|α| ⊕α denotes the string ⟨0, α(0), 0, α(1), . . . , 0, α(|α|−1)⟩. Clearly, β(σ)
is contained in the closure of C.
Now we construct a strategy ψ for Player II on Choquet game GC as follows:
Given aθ ∈ω<ω, the θ-th move of Player I, ﬁrst check whether pr1(aθ) has an
extension in TP of length max{|pr1(aθ)|, |θ|} or not. If not (it is possible because
of the past moves by Player II), Player II resigns the game GC, i.e., ψII(aθ) =
Resign. Otherwise, when |pr1(aθ)| > |θ|, Player II does not act, i.e., ψII(aθ) =
⟨aθ, aθ⟩. If pr1(aθ) ≤|θ|, then Player II returns ψII(aθ) = ⟨aθ⌢⟨1, 0⟩, aθ⌢⟨1, 1⟩⟩.
By our construction of the strategy ψII, for every ψI ⊗ψII|h along any inﬁnite
path h through the play tree Play(ψI ⊗ψII), the 1-st projection of 
n ψI ⊗ψII|h
must be contained in P. Therefore, 
n ψI ⊗ψII|h is contained in C. Moreover,
P is equal to the set of all inﬁnite paths through Play(ψI ⊗ψII). Consequently,
ψII is a winning playful strategy of Player II.
⊓⊔
Acknowledgements. This work was supported by a Grant-in-Aid for JSPS
fellows.
References
1. Binns, S.: Hyperimmunity in 2N. Notre Dame Journal of Formal Logic 48(2), 293–
316 (2007)
2. Brattka, V., Gherardi, G.: Eﬀective choice and boundedness principles in com-
putable analysis. Bulletin of Symbolic Logic 17(1), 73–117 (2011)
3. Brattka, V., Presser, G.: Computability on subsets of metric spaces. Theor. Com-
put. Sci. 305(1-3), 43–76 (2003)
4. Cenzer, D., Kihara, T., Weber, R., Wu, G.: Immunity and non-cupping for closed
sets. Tbilisi Math. J. 2, 77–94 (2009)
5. Demuth, O., Kuˇcera, A.: Remarks on 1-genericity, semigenericity and related con-
cepts. Comment. Math. Univ. Carolinae 28, 85–94 (1987)
6. Higuchi, K., Kihara, T.: Inside the Muchnik degrees: Discontinuity, learnability,
and constructivism (preprint)

394
T. Kihara
7. Lewis, A.E.M., Shore, R.A., Sorbi, A.: Topological aspects of the Medvedev lattice.
Arch. Math. Log. 50(3-4), 319–340 (2011)
8. Soare, R.I.: Recursively Enumerable Sets and Degrees. Perspectives in Mathemat-
ical Logic, xVIII+437 pages. Springer, Heidelberg (1987)
9. Weihrauch, K.: Computable Analysis: An Introduction. Texts in Theoretical Com-
puter Science, 285 pages. Springer (2000)
10. Ziegler, M.: Real computation with least discrete advice: A complexity theory of
nonuniform computability with applications to eﬀective linear algebra. Annals of
Pure and Applied Logic 163(8), 1108–1139 (2012),
http://www.sciencedirect.com/science/article/pii/S016800721100203X

How Much Randomness Is Needed for Statistics?
Bjørn Kjos-Hanssen1, Antoine Taveneaux2, and Neil Thapen3,4
1 University of Hawai‘i at M¯anoa, Honolulu, HI 96822, United States of America
bjoern@math.hawaii.edu
2 Laboratoire d’Informatique Algorithmique: Fondements et Applications (LIAFA),
Universit´e Paris Diderot—Paris 7, 75205 Paris Cedex 13, France
taveneaux@liafa.jussieu.fr,
3 Academy of Sciences of the Czech Republic, 115 67 Praha 1, Czech Republic
thapen@math.cas.cz
4 Isaac Newton Institute for Mathematical Sciences, 20 Clarkson Road, Cambridge
CB3 0EH, United Kingdom
Abstract. In algorithmic randomness, when one wants to deﬁne a ran-
domness notion with respect to some non-computable measure λ, a choice
needs to be made. One approach is to allow randomness tests to access
the measure λ as an oracle (which we call the “classical approach”). The
other approach is the opposite one, where the randomness tests are com-
pletely eﬀective and do not have access to the information contained in λ
(we call this approach “Hippocratic”). While the Hippocratic approach
is in general much more restrictive, there are cases where the two coin-
cide. The ﬁrst author showed in 2010 that in the particular case where
the notion of randomness considered is Martin-L¨of randomness and the
measure λ is a Bernoulli measure, classical randomness and Hippocratic
randomness coincide. In this paper, we prove that this result no longer
holds for other notions of randomness, namely computable randomness
and stochasticity.
1
Introduction
In algorithmic randomness theory we are interested in which almost sure prop-
erties of an inﬁnite sequence of bits are eﬀective or computable in some sense.
Martin-L¨of deﬁned randomness with respect to the uniform fair-coin measure μ
on 2ω as follows.
A sequence X ∈2ω is Martin-L¨of random if we have X ̸
∈
n∈N Un for
every sequence of uniformly Σ0
1 (or eﬀectively open) subsets of 2ω such
that μ(Un) ≤2−n.
Now if we wish to consider Martin-L¨of randomness for a Bernoulli measure μp
(i.e. a measure such that the ith bit is the result of a Bernoulli trial with param-
eter p) we have have two ways to extend the previous deﬁnition.
The ﬁrst option is to consider p as an oracle (with an oracle p we can
compute μp) and relativize everything to this oracle. Then X is μp-Martin-
L¨of random if for every sequence (Un)n∈N of uniformly Σ0
1[p] sets such that
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 395–404, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

396
B. Kjos-Hanssen, A. Taveneaux, and N. Thapen
μp(Un) ≤2−n we have X ̸
∈
n∈N Un. We will call this approach the classical
notion of Martin-L¨of randomness relative to μp.
Another option is to keep the measure μp “hidden” from the process which
describes the sequence (Un). One can merely replace μ by μp in Martin-L¨of’s
deﬁnition but still require (Un) to be uniformly Σ0
1 in the unrelativized sense.
This notion of randomness was introduced by Kjos-Hanssen [4] who called it
Hippocratic randomness; Bienvenu, Doty and Stephan [1] used the term blind
randomness.
Kjos-Hanssen showed that for Bernoulli measures, Hippocratic and classical
randomness coincide in the case of Martin-L¨of randomness. Bienvenu, G´acs,
Hoyrup, Rojas and Shen [2] extended Kjos-Hanssen’s result to other classes of
measures. Here we go in a diﬀerent direction and consider weaker randomness
notions, such as computable randomness and stochasticity. We discover the con-
tours of a dividing line for the type of betting strategy that is needed in order
to render the probability distribution superﬂuous as a computational resource.
We view statistics as the discipline concerned with determining the underlying
probability distribution μp by looking at the bits of a random sequence. In the
case of Martin-L¨of randomness it is possible to determine p ([4]), and therefore
Hippocratic randomness and classical randomness coincide. In this sense, Martin-
L¨of randomness is suﬃcient for statistics to be possible, and it is natural to ask
whether smaller amounts of randomness, such as computable randomness, are
also suﬃcient.
Notation. Our notation generally follows Nies’ monograph [5]. We write 2n for
{0, 1}n, and for sequences σ ∈2≤ω we will also use σ to denote the real with
binary expansion 0.σ, that is, the real ∞
i=1 σ(i)2−i. We use ε to denote the
empty word, σ(n) for the nth element of a sequence and σ ↾n for the sequence
formed by the ﬁrst n elements. For sequences ρ, σ we write σ ≺ρ if σ is a
proper preﬁx of ρ and denote the concatenation of σ and ρ by σ.ρ or simply σρ.
Throughout the paper we set n′ = n(n −1)/2.
1.1
Hippocratic Martingales
Formally a martingale is a function M : 2<ω →R≥0 satisfying
M(σ) = M(σ0) + M(σ1)
2
.
Intuitively, such a function arises from a betting strategy for a fair game played
with an unbiased coin (a sequence of Bernoulli trials with parameter 1/2). In
each round of the game we can choose our stake, that is, how much of our capital
we will bet, and whether we bet on heads (1) or tails (0). A coin is tossed, and
if we bet correctly we win back twice our stake.
Suppose that our betting strategy is given by some ﬁxed function S of the
history σ of the game up to that point. Then it is easy to see that the function
M(σ) giving our capital after a play σ satisﬁes the above equation. On the

How Much Randomness Is Needed for Statistics?
397
other hand, from any M satisfying the equation we can recover a corresponding
strategy S.
More generally, consider a biased coin which comes up heads with probability
p ∈(0, 1). In a fair game played with this coin, we would expect to win back 1/p
times our stake if we bet correctly on heads, and 1/(1 −p) times our stake if we
bet correctly on tails. Hence we deﬁne a p-martingale to be a function satisfying
M(σ) = pM(σ1) + (1 −p)M(σ0).
We can generalize this further, and for any probability measure μ on 2ω deﬁne
a μ-martingale to be a function satisfying
μ(σ)M(σ) = μ(σ1)M(σ1) + μ(σ0)M(σ0).
For the Bernoulli measure with parameter p, we say that a sequence X ∈2ω
is p-computably random if for every total, p-computable p-martingale M, the
sequence (M(X ↾n))n is bounded.
This is the classical approach to p-computable randomness. Under the Hip-
pocratic approach, the bits of the parameter p should not be available as a
computational resource. The obvious change to the deﬁnition would be to re-
strict to p-martingales M that are computable without an oracle for p. However
this does not give a useful deﬁnition, as p can easily be recovered from any
non-trivial p-martingale. Instead we will deﬁne p-Hippocratic computable mar-
tingales in terms of their stake function (or strategy) S.
We formalize S as a function 2<ω →[−1, 1] ∩Q. The absolute value |S(σ)|
gives the fraction of our capital we put up as our stake, and we bet on 1 if
S(σ) ≥0 and on 0 if S(σ) < 0. Given α ∈(0, 1), the α-martingale Mα arising
from S is then deﬁned inductively by
Mα(ε) = 1
Mα(σ1) = Mα(σ)

1 −|S(σ)| + |S(σ)|
α
1{S(σ)≥0}

Mα(σ0) = Mα(σ)

1 −|S(σ)| + |S(σ)|
1 −α 1{S(σ)<0}

where, for a formula T , we use the notation 1{T } to mean the function which
takes the value 1 if T is true and 0 if T is false.
We deﬁne a p-Hippocratic computable martingale to be a p-martingale Mp
arising from some total computable (without access to p) stake function S. We
say that a sequence X ∈2ω is p-Hippocratic computably random if for every p-
Hippocratic computable martingale M, the sequence (M(X ↾n))n is bounded.
In Section 2 below we show that for all p ∈MLR the set of p-Hippocratic
computably random sequences is strictly bigger than the set of p-computable
random sequences. More precisely, we show that we can compute a sequence
Q ∈2ω from p such that Q is p-Hippocratic computably random. In a nutshell,
the proof works as follows. We use the number p in two ways. To compute the
ith bit of Q, the ﬁrst i bits of p are treated as a parameter r = 0.p0 . . . pi, and

398
B. Kjos-Hanssen, A. Taveneaux, and N. Thapen
we pick the ith bit of Q to look like it has been chosen at random in a Bernoulli
trial with bias r. To do this, we use some fresh bits of p (which have not been
used so far in the construction of Q) and compare them to r, to simulate the
trial. Since these bits of p were never used before, if we know only the ﬁrst i −1
bits of Q they appear random, and thus the ith bit of Q indeed appears to be
chosen at random with bias r. Since r = 0.p1p2 . . . pi converges quickly to p
(this convergence is faster than the deviations created by statistical noise in a
real sequence of Bernoulli trials with parameter p), we are able to show that Q
overall looks p-random as long as we do not have access to p, in other words,
that Q is p-Hippocratic computably random.
1.2
Hippocratic Stochasticity and KL Randomness
In Section 3 we consider another approach to algorithmic randomness, known as
stochasticity. It is reasonable to require that a random sequence satisﬁes the law
of large numbers, that is, that the proportion of 1s in the sequence converges to
the bias p. But, for an unbiased coin, the string
010101010 . . .
satisﬁes this law but is clearly not random. Following this idea, we say that
a sequence X is p-Kolmogorov-Loveland (or KL) stochastic if there is no p-
computable way to select inﬁnitely many bits from X, where we are not allowed
to know the value of a bit before we select it, without the selected sequence
satisfying the law of large numbers (see Deﬁnition 1 for a formal approach).
For this paradigm the Hippocratic approach is clear: we consider only
selection functions which are computable without an oracle for p. We show
in Theorem 2 that for p ∈Δ0
2 ∩MLR there exists a sequence Q which is p-
Hippocratic KL stochastic but not p-KL stochastic. Again we use p as a random
bit generator and create a sequence Q that appears random for a sequence of
Bernoulli trials, where the bias of the ith trial is qi for a certain sequence (qi)i
converging to p. Intuitively, the convergence is so slow that it is impossible to
do (computable) statistics with Q to recover p, and we are able to show that
without access to p the sequence Q is p-KL stochastic.
At the end of Section 3 we consider another notion, Kolmogorov-Loveland
randomness. We give a simple argument to show that if we can compute p from
every p-Hippocratic KL random sequence, then the p-Hippocratic KL random
sequences and the p-KL random sequences are the same (and vice versa).
Some technical deﬁnitions and lemmas follow in the appendix.
2
Computable Randomness
In this section we show that for any Martin-L¨of random bias p, p-computable
randomness is a stronger notion than p-Hippocratic computable randomness.
Theorem 1. Let α ∈MLR. There exists a sequence Q ∈2ω, computable in
polynomial time from α, such that Q is α-Hippocratic computably random.

How Much Randomness Is Needed for Statistics?
399
Before giving the proof, we remark that a stronger version of the theorem is true:
the sequence Q is in fact α-Hippocratic partial computably random (meaning
that we allow the martingale to be a partial computable function, see Deﬁnition
7.4.3 in [3]).
Also, a sceptic could (reasonably) complain that it is not really natural for
us to make bets without any idea about our current capital. However if we
add an oracle to give the integer part of our capital at each step (or even an
approximation with accuracy 2−n when we bet on the nth bit), Theorem 1
remains true and the proof is the same. In the same spirit we could object that
it is more natural to have a stake function giving the amount of our bet (to
be placed only if we have a capital large enough) and not the proportion of our
capital. For this deﬁnition of a Hippocratic computable martingale, similarly the
theorem remains true and the proof is the same.
Proof. Let α ∈MLR. Then α is not rational and cannot be represented by
a ﬁnite binary sequence and we can suppose that 0 < α < 1/2. Recall that
n′ = n(n−1)/2 and that we freely identify a sequence X (ﬁnite or inﬁnite) with
the real number with the binary expansion 0.X.
The proof has the following structure. First, we describe an algorithm to
compute a sequence Q from α. To compute each bit Qn of Q we will use a
ﬁnite initial segment of α as an approximation of α, and we will compare this
with some other fresh bits of α which we treat as though they are produced
by a random bit generator. In this way Qn will approximate the outcome of a
Bernoulli trial with bias α.
Second, we will suppose for a contradiction that there is an α-Hippocratic
computable martingale (that is, a martingale that arises from a stake function
computable without α) such that the capital of this martingale is not bounded
on Q. We will show that we can use this stake function to construct a Martin
L¨of test (Un)n such that α does not pass this test.
So let Q = Q1Q2 . . . be deﬁned by the condition that:
Qn =

0 if 0.αn′+1 . . . αn′+n ≥0.α1 . . . αn,
1 otherwise.
We can compute Q in polynomial time from α, as we can compute each bit Qn
in time O(n2).
Now let S : 2<ω →Q ∩[−1, 1] be a computable stake function. We will write
MX for the X-martingale arising from S. Suppose for a contradiction that
lim sup
n→∞Mα(Q ↾n) = ∞.
Our goal is to use this to deﬁne a Martin-L¨of test which α fails. The classical
argument (see Theorem 6.3.4 in [3]) would be to consider the sequence of sets
Vj = {X ∈2ω|∃n Mα(X ↾n) > 2j},
but without oracle access to α this is not Σ0
1, and does not deﬁne a Martin-L¨of
test. However it turns out that we can use a similar sequence of sets, based on

400
B. Kjos-Hanssen, A. Taveneaux, and N. Thapen
the idea that, although we cannot compute Mα precisely, we can approximate
it using the approximation α1 . . . αn′ of α. For this we will use the following
lemma, proved in the appendix.
Lemma 1. For α ∈MLR, there exists m ∈N such that if σ ≽(α ↾m′) and
τ ≽(α ↾m′) then for all η ∈2<ω and all n ≥m we have:
if 0 < τ −σ < 2−n′ and |η| ≤n + 1 then |Mσ(η) −Mτ(η)| ≤2−n.
Let m be given by Lemma 1 and let ρ be α ↾m′. Without loss of generality
we may assume 2−m < ρ. Let Γ : 2≤ω →2≤ω be the operator which converts
α1 . . . αn′ into Q1 . . . Qn. That is, Γ(α1 . . . αk) = Q1 . . . Qn where n is the biggest
integer such that n′ ≤k. This notation naturally extends to inﬁnite sequences
so we may write Γ(α) = Q. We consider the uniform sequence of Σ0
1 sets
U ′
j = {X1 . . . Xk′|ρ ≼X1 . . . Xk′ and MX1...Xk′ (Γ(X1 . . . Xk′)) > 2j}.
We let Uj denote the set of inﬁnite sequences with a preﬁx in U ′
j. By Lemma 1,
|Mα(Γ(α1 . . . αk′)) −Mα1...αk′ (Γ(α1 . . . αk′))| < 2−k ≤1
for all suﬃciently large k. Since Mα increases unboundedly on Q = Γ(α) it
follows that α ∈Uj for all j.
To show that (Uj) is a Martin-L¨of test, it remains to show that the measure
of Uj is small. Since σ →Mσ(σ) is almost a α-martingale, where σ runs over
the preﬁxes of α, we will use a lemma similar to the Kolmogorov inequality (see
Theorem 6.3.3 in [3]). We postpone the proof to the appendix.
Lemma 2. For any number n ≥m, any extension σ ≽ρ of length n′ and any
preﬁx-free set Z ⊆
k∈N{0, 1}k′ of extensions of σ, we have

τ∈Z
2−|τ|Mτ(Γ(τ)) ≤2−|σ|e2 [1 + Mσ(Γ(σ))] .
Now ﬁx j and let Wj be a preﬁx-free subset of U ′
j with the property that the set
of inﬁnite sequences with a preﬁx in Wj is exactly Uj. Then by the deﬁnition of
U ′
j, if τ ∈Wj then Mτ(Γ(τ)) ≥2j. Hence by Lemma 2 we have:
μ(Uj) =

τ∈Wj
2−|τ| ≤

τ∈Wj
Mτ(Γ(τ))
2j
2−|τ| ≤2−|ρ|e2 (1 + Mρ(Γ(ρ)))
2j
.
Since 2−|ρ| (1 + Mρ(Γ(ρ))) is constant, this shows that (Uj) is a Martin-L¨of test.
As α ∈
j Uj it follows that α ̸
∈MLR. This is a contradiction.
Notice that this proof makes use of the fact that in our betting strategy we
have to proceed monotonically from left to right through the string, making a
decision for each bit in turn as we come to it. This is why our construction is
able to use α as a random bit generator, because at each step it can use bits
that were not used to compute the previous bits of Q. Following this idea the
question naturally arises: if we are allowed to use a non-monotone strategy, then
are the classical and Hippocratic random sequences the same? We explore this
question in the next section.

How Much Randomness Is Needed for Statistics?
401
3
Kolmogorov-Loveland Stochasticity and Randomness
We deﬁne Kolmogorov-Loveland stochasticity and show that, in this setting,
the Hippocratic and classical approaches give diﬀerent sets. We also consider
whether this is true for Kolmogorov-Loveland randomness, and relate this to a
statistical question.
3.1
Deﬁnitions
For a ﬁnite string σ ∈{0, 1}n, we write #0(σ) for |{k < n|σ(k) = 0}| and #1(σ)
for |{k < n|σ(k) = 1}|. We write Φ(σ) for #1(σ)/n, the frequency of 1s in σ.
Deﬁnition 1 (Selection Function). A KL selection function is a partial
function
f : 2<ω →{scan, select} × N.
We write f(σ) as a pair (s(σ), n(σ)) and in this paper we insist that for all σ
and ρ ≻σ we have n(ρ) ̸
= n(σ), so that each bit is read at most once.
Given input X, we write (V X
f ) for the sequence of strings seen (with bits either
scanned or selected) by f, so that
V X
f (0) = X(n(ε))
V X
f (k + 1) = V X
f (k).X(n(V X
f (k))).
We write U X
f
for the subsequence of bits selected by f. Formally U X
f
is the limit
of the monotone sequence of strings (T X
f ) where
T X
f (0) = ε
T X
f (k + 1) =
T X
f (k)
if s(V X
f (k)) = scan
T X
f (k).n(V X
f (k))
if s(V X
f (k)) = select.
Informally, the function is used to select bits from X in a non-monotone way. If
V is the string of bits we have read so far, n(V ) gives the location of the next
bit of X to be read. Then “s(V ) = scan” means that we will just read this bit,
whereas “s(V ) = select” means that we will add it to our string T of selected
bits.
Deﬁnition 2 (p-KL stochastic sequence). A sequence X is p-KL stochastic
if for all p-computable KL selection functions f (notice that f can be a partial
function) such that the limit U X
f
of (T X
f ) is inﬁnite, we have
lim
k→∞Φ(T X
f (k)) = p.
A sequence X is p-Hippocratic KL stochastic if for all KL selection functions f,
computable without an oracle p, such that U X
f
is inﬁnite, we have
lim
k→∞Φ(T X
f (k)) = p.

402
B. Kjos-Hanssen, A. Taveneaux, and N. Thapen
For technical reasons we informally introduce two more deﬁnitions (formal deﬁni-
tions are given in section C.1 in the Appendix). A generalized Bernoulli measure
λ is a probability measure such that each bit is seen as an independent Bernoulli
trial with parameter bλ
i ∈(0, 1). A KL martingale is equipped with a KL style
selection function which allows it to bet on bits in a non-monotone way. A se-
quence X is KL random for a measure if every KL martingale using the measure
is bounded on X.
3.2
Hippocratic Stochasticity Is Not Stochasticity
We will show that, despite the fact that we now allow non-monotone strategies,
once again there exist sequences computable from α which are α-Hippocratic KL
stochastic, for α ∈MLR∩Δ0
2 (recall that Chaitin’s constant Ω is the prototypical
example of such an α).
We remark that our proof shows also that for α ∈MLR ∩Δ0
2 the Hippocratic
and classical versions of MWC-stochasticity are diﬀerent (see Deﬁnition 7.4.1 in
[3] for a formal deﬁnition).
Theorem 2. Let α ∈MLR ∩Δ0
2. There exists a sequence Q ∈2ω, computable
from α, such that Q is α-Hippocratic KL stochastic.
Proof. We will ﬁrst deﬁne the sequence Q, and then show that Q is λ-KL random
for a certain generalized Bernoulli measure λ for which the parameters (bλ
i ) con-
verge to α. Finally we will show that it follows that Q is actually α-Hippocratic
KL stochastic.
Since α ∈Δ2
0, by Shoenﬁeld’s limit lemma α is the limit of a computable
sequence of real numbers (although the convergence must be extremely slow,
since α is not computable). In particular there exists a computable sequence of
ﬁnite strings (βk) such that βk ∈{0, 1}k and
lim
k→∞βk = α.
We deﬁne Qk by
Qk =
1
if 0.βk ≥0.αk′+1 . . . αk′+k
0
otherwise.
We set Q = Q1Q2 . . . . Intuitively, as in the proof of Theorem 1, we are using
α as a random bit generator to simulate a sequence of Bernoulli trials with
parameter βk.
Notice that the transformation mapping α to Q is a total computable function.
We know that in general if g is total computable, and X is (Martin-L¨of) random
for the uniform measure μ, then g(X) is random for the measure μ ◦g−1 (see
[8] for a proof of this fact). Since α ∈MLR, in our case this tell us that Q is
random for exactly the generalized Bernoulli measure λ given by bλ
i = βi.
Lemma 3 ([7] and in [3] p.311). If X is Martin-L¨of random for a computable
generalized Bernoulli measure λ, then X is λ-KL random.

How Much Randomness Is Needed for Statistics?
403
The proof is essentially the same as for the uniform measure on 2ω. We postpone
it to the appendix. It follows that Q is λ-KL random and by the next lemma, also
proved in the appendix, we can conclude that Q is α-Hippocratic KL stochastic,
completing the argument.
Lemma 4. Let λ be a computable generalized Bernoulli measure and suppose
lim
i→∞bλ
i = p.
Then every λ-KL random sequence is p-Hippocratic KL stochastic.
3.3
Kolmogorov-Loveland Randomness
We have shown that for computability randomness and non-monotone stochas-
ticity, whether a string is random can depend on whether or not we have ac-
cess to the actual bias of the coin. It is natural to ask if this remains true for
Kolmogorov-Loveland randomness.
Lemma 5 ([6]). For sequences X, Y ∈2ω, X ⊕Y is p-KL random if and only
if both X is p-KLY -random and Y is p-KLX-random, and this remains true in
the Hippocratic setting (that is, where the KL martingales do not have oracle
access to p).
The proof is a straightforward adaptation of the proof given in [6] (Proposition
11). Using this lemma we can show an equivalence between our question and a
statistical question.
Theorem 3. The following two sentences are equivalent.
1. The p-Hippocratic KL random and p-KL random sequences are the same.
2. From every p-Hippocratic KL random sequence X, we can compute p.
Proof. For (1) ⇒(2), we know that if X is p-KL random then it must satisfy
the law of the iterated logarithm (see [9] for this result). Hence we know how
quickly Φ(X ↾k) converges to p and using this we can (non-uniformly) compute
p from X.
For (2) ⇒(1), suppose that X is p-Hippocratic KL random but not p-KL
random. Let X = Y ⊕Z. Then by Lemma 5, Y (say) is not p-KLZ random,
meaning that there is a KL martingale (M, f) which, given oracle access to p
and Z, wins on Y . On the other hand, both Y and Z remain p-Hippocratic KL
random, so in particular by (2) if we have oracle access to Z then we can compute
p. But this means that we can easily convert (M, f) into a p-Hippocratic KL
martingale which wins on X, since to answer oracle queries to either Z or p it
is enough to scan Z and do some computation.

404
B. Kjos-Hanssen, A. Taveneaux, and N. Thapen
Acknowledgements. The authors would like to thank Samuel Buss for his
invitation to University of California, San Diego. Without his help and the uni-
versity’s support this paper would never exist. Taveneaux’s research has been
helped by a travel grant of the “Fondation Sciences Math´ematiques de Paris”.
Kjos-Hanssen’s research was partially supported by NSF (USA) grant no. DMS-
0901020. Thapen’s research was partially supported by grant IAA100190902 of
GA AV ˇCR and by Center of Excellence CE-ITI under grant P202/12/G061 of
GA CR and RVO: 67985840.
References
1. Bienvenu, L., Doty, D., Stephan, F.: Constructive dimension and Turing degrees.
Theory of Computing Systems 45(4), 740–755 (2009)
2. Bienvenu, L., G´acs, P., Hoyrup, M., Rojas, C., Shen, A.: Algorithmic tests and
randomness with respect to a class of measures. Proceedings of the Steklov Institute
of Mathematics 271, 41–102 (2011)
3. Downey, R.G., Hirschfeldt, D.: Algorithmic Randomness and Complexity. Springer
(2010)
4. Kjos-Hanssen, B.: The probability distribution as a computational resource for ran-
domness testing. J. Log. Anal. 2 (2010)
5. Nies, A.: Computability and Randomness. Oxford University Press (2009)
6. Merkle, W., Miller, J.S., Nies, A., Reimann, J., Stephan, F.: Kolmogorov-Loveland
randomness and stochasticity. Ann. Pure Appl. Logic 138, 183–210 (2006)
7. Muchnik, A.A., Semenov, A.L., Uspensky, V.A.: Mathematical metaphysics of ran-
domness. Theoret. Comput. Sci. 207, 263–317 (1998)
8. Shen, A.: One more deﬁnition of random sequence with respect to computable mea-
sure. In: First World Congress of the Bernoulli Society on Math. Statistics and
Probability Theory, Tashkent (1986)
9. Wang, Y.: Resource bounded randomness and computational complexity. Theoret-
ical Computer Science 237, 33–55 (2000)

Towards a Theory of Inﬁnite Time
Blum-Shub-Smale Machines
Peter Koepke and Benjamin Seyﬀerth
Mathematisches Institut, Rheinische Friedrich-Wilhelms-Universit¨at Bonn,
Endenicher Allee 60, 53115 Bonn, Germany
{koepke,seyfferth}@math.uni-bonn.de
Abstract. We introduce a generalization of Blum-Shub-Smale machines
on the standard real numbers R that is allowed to run for a transﬁnite
ordinal number of steps before terminating. At limit times, register con-
tents are set to the ordinary limit of previous register contents in R. It
is shown that each such machine halts before time ωω or diverges. We
undertake ﬁrst steps towards estimating the computational strength of
these new machines.
1
Introduction
In the spirit of ordinal computability — the study of classical models of compu-
tations generalized to transﬁnite ordinal numbers — we study a variation of the
Blum-Shub-Smale (BSS) machine introduced in [1]. In contrast to established
models of ordinal computability, such as Hamkins’ and Lewis’ inﬁnite time Tur-
ing machines (ITTMs) [4] and Koepke’s ordinal Turing machines (OTMs) [5],
these machines employ real numbers in the classical continuum R as opposed
to elements of Baire space ωω or Cantor space ω2. The topological diﬀerences
matter as soon as we consider limits (see below). Variations thereof, be it in
allowing inﬁnitely many registers or changes in the limit behavior, might very
well change the computational strength. In this paper, we aim for the “weak-
est” possible generalization of BSS machines into ordinal time. We believe that
already this restricted model shows interesting properties.
Our machines have a ﬁnite number n of registers, each containing a real
number. Generalizations to other ﬁelds and rings are possible but shall not
be of concern to this paper. The computation is steered by a ﬁnite program
P ⊆ω × {f | f : Rn
rational
−−−−−→Rn} × {0, 1} × ω × ω, containing commands of the
form (i, φ, j, k, l), where i is the index of the command at hand, φ is a rational
map (with rational coeﬃcients), and j tells us if the command represents a com-
putation node or a branch node. In case j = 0, we are at a computation node, the
register content x ∈Rn is replaced by φ(x) and the next command (index i + 1)
is carried out next. The values of k and l are ignored in this case. Otherwise,
j = 1 and we are at a branching node. This means that the register content is
left unchanged and, depending on whether φ(x) > 0, the next command will
be the one with index k. If on the other hand φ(x) ≤0, command number l is
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 405–415, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

406
P. Koepke and B. Seyﬀerth
carried out next. We can assume the indices of a given program’s commands to
form an initial segment of the natural numbers and that no index appears twice.
In case a command index is called for which no command in the program exists,
the computation halts.
Note 1. As a minor technical detail we would like to note that, as in the original
paper [1], we avoid discontinuity points of rational functions by putting decision
nodes before each computation or decision node to check whether the denomina-
tor of the rational function to be evaluated is 0. If not, we continue as planned,
if yes, an inﬁnite loop is entered and the computation diverges.
So far, we have outlined a standard BSS machine with the additional restriction
that the rational functions present in computation and branching nodes do not
allow for arbitrary real coeﬃcients. We add irrational coeﬃcients in form of pa-
rameters later on. We now make our machines access the transﬁnite: In order for
the machine to run for inﬁnitely many steps, we have to deﬁne the register con-
tent at limit times. In the established theories of inﬁnite time or ordinal Turing
and register machines, often an inferior or superior limit is used for this purpose.
Instead, we want to restrict ourselves here to ordinary limits of sequences of real
numbers. This immediately implies that there will be situations where an inﬁnite
time BSS machine will, e.g., be properly deﬁned at any ﬁnite time but not at the
ﬁrst limit time ω because the register contents do not converge. We can imagine
the machine to “crash” in such a case and say that for such a combination of
program and input no valid computations exists. In case of converging register
contents we also have to come up with a command that is carried out at a limit
time. For this we shall indeed use the inferior limit, i.e., the command with the
least index that was used coﬁnally often below the limit. Note that we do not
introduce a dedicated limit state.
Let us put things together in the following deﬁnition.
Deﬁnition 1. Let n ∈ω be a number of registers. Let k < n and let P be a n + k-
register BSS program. The inﬁnite time BSS machine computation (ITBM compu-
tation) with parameters p1, p2, . . . , pk ∈R by P on some input x ∈Rn is deﬁned
as the transﬁnite sequence (Ct)t∈θ = (R(t), I(t))t∈θ ∈θ(Rn+k × ω) where:
(a) θ ∈Ord or θ = Ord
(b) R(0) = (x, p1, p2, . . . , pk) and I(0) = 0
(c) (computation node) If t < θ and I(t) = i let (i, φ, 0, k, l) be the command of
P with index i. Then R(t + 1) = φ(R(t)) and I(t + 1) = i + 1.
(d) (branching node) If t < θ and I(t) = i let (i, φ, 1, k, l) be the command of
P with index i. Then R(t + 1) = R(t) and if furthermore φ(R(t)) > 0 then
I(t + 1) = k; if on the other hand φ(R(t)) ≤0, then I(t + 1) = l.
(e) If t < θ is a limit and y = lims→tR(s), then R(t) = y and I(t) =
lim infs→t I(s).
(f) If θ < Ord, then θ is a successor ordinal and I(θ−1) calls a command index
that is not in P (the machine halts (in θ-many steps)).

Towards a Theory of Inﬁnite Time Blum-Shub-Smale Machines
407
We deﬁne ITBM computable functions on the reals:
Deﬁnition 2. A function f : domf →Y where domf, Y ⊆R is called ITBM
computable in parameters p1, p2, . . . , pk if there is an at least k + 1-register ITBM
program P s.t. for every x ∈domf the computation by P on input (x, 0, 0, . . . , 0, p1,
p2, . . . , pk) halts and the ﬁnal register content is of the form (f(x), ·, ·, . . . , ·). On
input x /∈domf the computation is required to diverge. We call such a function
ITBM computable if k = 0, i.e., no real parameters are necessary.
The use of one limit step enables us to compute the classical elementary functions
that are deﬁned by power series as illustrated by the following examples. While
such functions as the exponential function can be computed in classical recursive
analysis, they are not computable in the standard BSS model [2].
Example 1. The exponential function e : R →R, x →ex = ω
k=0
xk
k! is ITBM
computable: Deﬁne a 5-register program that computes the desired function if
|x| < 1. Later we shall describe the modiﬁcations necessary to work for any x.
Algorithm 1
input R1 = x;
set R2 := R3 := R4 := R5 := 1; (initialize)
call loop;
loop:
if R2 = 0 then set R1 := R5 and halt, else continue;
set R4 :=
R4
R4+1; (store
1
i , where i is the current iteration)
set R3 := R3 ∗R4; (store
1
i!)
set R2 := R2 ∗R1; (store xi)
set R5 := R5 +
1
R2
R3 ; (store
i!
xi )
call loop;
If |x| < 1, all register contents converge and R5 contains the desired output at
time ω, which is correctly recognized when R2 = 0. We can adapt the algorithm
for |x| ≥1 by adding a case distinction in the beginning and, in case |x| ≥1, save
1
xi in register three. Then register three converges also at limit times. Of course,
the command that updates R5 inside the loop has to be changed accordingly.
Example 2. The sine function sin : R →R, x →ω
k=0(−1)k x(2k+1)
(2k+1)! and the
cosine function cos : R →R, x →ω
k=0(−1)k x2k
(2k)! are ITBM computable. This
is proven by the previous example and the fact that (−1)k can be recovered
from (−1
2)k and ( 1
2)k, both of the latter which can be convergently stored and
updated in separate registers.
Note 2. Common to this type of examples is that some tricks are necessary to
make all registers converge at limits. Auxiliary registers used for scratch work
often do not contain converging content. If their content is bounded, however,

408
P. Koepke and B. Seyﬀerth
one can simply divide the register content coﬁnally often by a ﬁxed number
and keep track of how often this division has occurred. Unbounded contents are
best stored as their multiplicative inverse. Both approaches can be imagined as
pushing the relevant data contained in a register into increasingly later places
in their decimal/binary expansion. Compound limits like ω · ω are an additional
problem, as scratch registers cannot be set to arbitrary values after limit times
without sacriﬁcing convergence at the compound limit. However, in every limit
stage towards a compound limit the register content will be bounded if treated
like above. So, in order to ensure convergence at the compound limit, these
bounds themselves need to converge. See the next chapter for an example.
2
Clockable Ordinals
Since Hamkins’ and Lewis’ paper on ITTMs [4], determining those ordinals that
appear as halting times on empty inputs has proved to be important for the
study of machine models. Since our machines do not halt at limit times, we are
interested in machines that run for some limit number α many steps and halt in
the next step:
Deﬁnition 3. An ordinal α is called ITBM clockable if there is an n ∈ω and
an n-register ITBM program that halts on input 0 ∈Rn in exactly α + 1 many
steps.
The algorithms above prove that ω is clockable, but let us establish this anew
with an algorithm that uses only one register.
Lemma 1. The ﬁrst limit ordinal ω is ITBM clockable.
Proof (by algorithm).
Algorithm 2
set R1 := 1;
call loop;
loop :
if R1 = 0 then halt else continue;
set R1 := R1
2 ;
call loop;
We can clock ω · n by having n loops in separate lines of code, where loop1 calls
loop2 and loop i calls loop i + 1 instead of the halting command, each time
resetting R1 to 1. Instead, we could also use a separate register to perform a
countdown from n. When trying to extend this approach trivially to clock ω · ω,
we run into a problem that is connected to the fact that ω · ω is a compound
limit, i.e., a limit of limits: At time ω · ω, R1 will have coﬁnally often been set
from 0 to 1, so convergence or R1 fails. While this is easily ﬁxed as seen below, it
hints at the limitations imposed by the strict limit rule and why the supremum
of ITBM clockable ordinals might be quite low in the ordinals.

Towards a Theory of Inﬁnite Time Blum-Shub-Smale Machines
409
Lemma 2. The ﬁrst compound limit ordinal ω · ω is ITBM clockable.
Proof (by algorithm)
Algorithm 3
set R1 := 1;
set R2 := 1;
call loop;
loop:
if R2 = 0 then halt else continue;
if R1 = 0 then set R2 :=
R2
2
and set R1 := R2 and continue,
else continue;
set R1 := R1
2 ;
call loop;
In this algorithm, R1 is halved repeatedly to detect limits. Once a limit time is
reached (R1 = 0), R2 is halved and R1 is reset not to 1 but to the current value
of R2. Once R2 hits 0, we have found the compound limit ω · ω. At every limit,
every register content converges.
As before, it is easy to clock ﬁnite multiples of the form ω · ω · n. Also, if we
extend the algorithm to use extra registers R3, R4, . . . , Rn in the same manner
as we extended the ω-algorithm with R2, we can in fact clock any ﬁnite power
ωn. However, this is as far as we can get, as ωω turns out to be the supremum
of the ITBM clockable ordinals.
First observe that at any limit time, the register contents are a ﬁxed point for
every command that has been carried out coﬁnally often:
Lemma 3. Let (Ct)t<θ be an ITBM computation by some program P and let α <
θ be a limit ordinal. Then the ﬁrst command in the computation that alters the
register contents after time α has not been carried out coﬁnally often below α.
Proof. Let (c, φ, 0, ·, ·) ∈P be a computation node which is called coﬁnally often
below α. Let c be called at some time β > α, where for all α < γ < β the register
content has not been changed yet, i.e., R(α) = R(γ) = R(β). Since φ may be
assumed as locally continuous (cf. Note 1), we get:
R(β + 1) = φ(R(β))
= φ(R(α))
= φ(lim
t→α R(t))
=
lim
t→α∧I(t)=c φ(R(t)
=
lim
t→α∧I(t)=c R(t + 1)
= R(α)

410
P. Koepke and B. Seyﬀerth
So the ﬁrst computation node that changes the register content after time α
cannot have been called coﬁnally below α.
Theorem 1. Let P be a program with k computation nodes. Then in any com-
putation (Ct)t<θ according to P, the register contents stabilize before ωk+1.
Proof. If k = 0 then the computation halts after ﬁnitely many steps or diverges
since the program contains only ﬁnitely many branch nodes: The computation
may run through these nodes in a ﬁnite sequence or in an inﬁnite loop. Every
branch node may trigger halting depending on its rational function evaluated on
the unchanged input. After ﬁnitely many steps, every node in the sequence or
loop has been visited once. If the computation didn’t halt up to this point, the
program will go on forever as the register content is never changed.
So let the hypothesis hold for k. Let P be a program with (k + 1)-many
computation nodes. Suppose the register contents change after ωk+2. The ﬁrst
computation node c responsible for a new register content is not used coﬁnally
often below ωk+2. Let α be the supremum over the times < ωk+2 when c was car-
ried out nontrivially. We can view (Ct)α≤t<ωk+2 as the computation by P \ {c}
on input R(α). Inductively, the register content of this computation stabilizes in
ωk+1-many steps. Since α + ωk+1 < ωk+2 this means that the original computa-
tion stabilizes before ωk+2. But a computation that stabilizes before a limit can
never change its register content again.
Once the register contents have stabilized, an ITBM computation diverges or
may run for an additional ﬁnitely many steps before halting. So we get:
Theorem 2. The supremum of ITBM clockable ordinals is ωω.
The above argument is in fact independent of the input:
Corollary 1. Every ITBM computation halts before ωω-many steps or diverges.
Corollary 2. If an ITBM computation diverges, the register content at time
ωω is not changed any more and can be considered the pseudo-output of the
diverging computation.
3
Connections to Other Models of Computation
From a computability perspective, reals provide ample possibilities as codes for
complex objects. We can code and decode into reals with our ITBMs by inter-
preting the binary expansion of a real in the interval [0, 1] as an element of the
Cantor space ω2, i.e., an ω-long sequence of 0’s and 1’s. The binary expansion
of a real is not necessarily unique, so two binary strings representing the same
real will appear to our machines as equivalent.
Let us give an algorithm to retrieve the n-th binary digit bn of an x =
0.b0b1b2 · · · ∈[0, 1).

Towards a Theory of Inﬁnite Time Blum-Shub-Smale Machines
411
Algorithm 4
input R1 = x;
input R2 = n;
set R3 = 1
2; ( = 20)
set R4 = 0; (the current approximation to x)
call loop;
loop:
if R2 := 0 then call lastloop else continue; if R4 + R3 > R1
then continue (do not add R3 to the approximation R4 if the
result would exceed x)
else set R4
:=
R4
+
R3 and continue; (add R3 to the
approximation)
set R3 := R3
2 ;
set R2 := R2 −1; (= #remaining iterations)
call loop;
lastloop:
if R4 + R3 > x then set R1 := 0 and halt else set R1 := 1 and
halt;
With this algorithm we can also do local changes to the binary bits of x in
the fashion of a Turing machine. So ﬁnite Turing computations can obviously
be implemented on BSS/ITBMs. Also, the halting problem for Turing machines
becomes ITBM computable:
Lemma 4. The classical halting problem is ITBM computable.
Proof. Since standard Turing computations are ITBM computable, we can gen-
erate Turing programs successively. So, in iteration n carry out the ﬁrst n Turing
programs for n many steps on empty input, using a dedicated simulation register.
In step n, use only the n-th and later binary digits for the Turing simulation, so
at time ω, this register will have converged to 0 (cf. Note 2). Once the algorithm
ﬁnds that some program (say, the i-th) halts on its input, change the i-th binary
digit of an initially zero output register to 1. Since there is a ﬁnite time when all
halting computations of programs with index < n will have halted, this register
converges to the halting problem.
So our machines have more computing strength than Turing machines or classical
BSS machines. Also, the type-2 Turing machines of computable analysis (see [8])
can easily be simulated by ITBMs: The output tape of a type-2 Turing machine,
when modeled as an ITBM register, converges by deﬁnition. Input tapes do not
change their content and the ﬁnitely many work tapes can be made convergent
like in Note 2.
Using the above coding, ITBMs can also operate on functions. A continuous
function f : R →R may be input to an ITBM as its restriction to the rationals
f Q = f ↾Q : Q →R coded into a real pf ∈[0, 1]. This requires a ﬁxed
enumeration of rational numbers q : ω →Q which may be chosen as ITBM

412
P. Koepke and B. Seyﬀerth
computable and an ITBM computable bijective pairing function ⟨·, ·⟩: ω × ω →
ω. Then f Q(x) = y may be expressed as “the ⟨q−1(x), i⟩-th binary digit of pf is
exactly the i-th digit of y”. By computing a convergent sequence of rationals for
a given real (nested intervals) we can compute the function value at this real.
This takes ω-many steps: Produce, for all n < ω, the approximations of x up
to n binary digits in a similar way to Algorithm 4 as a sequence of rationals
converging to x. For every such approximation, decode from pf the function
value up to n digits. This deﬁnes a sequence of rationals that converge to f(x).
Example 3. The derivative of a diﬀerentiable function is ITBM computable.
Proof. Given a point x, have an ITBM evaluate the diﬀerential quotient in x
using only f(x) itself and rational approximations to f(x) .
An upper bound on strength of ITBMs is given by the strength of ITTMs:
Lemma 5. Let P be an n-register BSS program. There is an ITTM program Q
and a map f : Rn →ω2 s.t. for every x ∈R the ITTM computation by Q on
input f(x) halts and returns the information that either no computation by P
on x exists, or that P halts on x with output y ∈ω2, or that the P diverges on
x with pseudo-output y ∈ω2, where f −1(y) is the ﬁnal register content of the
ITBM computation by P on x.
Proof. We assume that the ITTM we are working with has a ﬁnite number of
read-write tapes, which can be accomplished of interlacing these tapes onto the
single scratch tape in the deﬁnition of ITTMs in [4]. Due to Corollary 2, we
know that after time ωω also a diverging ITBM computation does not change
its register content anymore. Since it is easy for an ITTM to construct a well
order of ω of length ωω on one tape, it is possible for an ITTM to code the
complete ITBM computation (Rt, It)t<ωω up to time ωω on the output tape.
So let us begin with deﬁning a map f ′ : R →ω2. First, imagine the value
f ′(x) to be contained in three elements of ω2 (i.e., three Turing tapes) where
the ﬁrst tape contains only a 0 or 1 in the ﬁrst cell to specify the sign of x,
the second contains the maximal exponent n s.t. 2n ≤x, and the third contains
the binary expansion of
x
2n , ignoring the decimal (binary) point and normalized
in the following way: Binary expansions that end on an inﬁnite trail of 1s are
replaced by the unique binary expansion of the same number that ends on a trail
of 0s. Then, use an ITTM computable pairing function to interlace these three
tapes into one. The function f ′ can easily be extended to a function f : Rn →ω2.
It is easy to see that an ITTM is perfectly capable — albeit with an enormous
time consumption — of computing addition, subtraction, multiplication and
division on elements of ω2 and of normalizing the result in the way described
above. So, given the input, the program Q will start carrying out the sequence
of ITBM commands in P and writing the results (and the program instructions
used) one after another in the respective cells of the output tape. At limit times,
the output tape contains all the information of the previous times, so it is easy
for the Q to check whether everything converges and compute the limit if one

Towards a Theory of Inﬁnite Time Blum-Shub-Smale Machines
413
exists. If not, output that there is no valid computation by P on x. If yes, Q
will continue its simulation of P up to time ωω. At time ωω, it can replace the
output tape content with f(y) where y is P’s output or pseudo-output.
It turns out that ITTMs are much stronger than needed, as will be clear from
the following. G¨odel’s constructible model L of set theory is closely related to
inﬁnite time computations. We shall use Ronald Jensen’s Jα-hierarchy to study
deﬁnability in L. The Jα[−→x ]-hierarchy relativized to the real parameters −→x
is deﬁned by the following recursion on the ordinals: J0[−→x ] = ∅; Jα+1[−→x ] is
the closure of Jα[−→x ] ∪{Jα[−→x ]} under all rudimentary functions, using also the
parameters −→x . The rudimentary functions are simple set theoretic functions
which include the formation of ordered pairs. Also ﬁrst-order deﬁnitions over
structures can be computed rudimentarily. Note that the ordinal height of Jα[−→x ]
is Jα[−→x ] ∩Ord = ω · α. In the sequel, the level Jωω[−→x ] will play a special role.
It is also a member of the standard L-hierarchy, and indeed Jωω[−→x ] = Lωω[−→x ]
is the least level beyond ω where the two hierarchies coincide.
In the following we assume that real numbers are coded by their binary ex-
pansions. Then a real number a will be a function from ω into the set {0, 1, ., −},
where . denotes the binary dot and −is the minus sign. The real a can be con-
sidered a subset of Hω = J1[−→x ]. We show that an ITBM computation can be
uniformly deﬁned along the Jα[−→x ]-hierarchy.
Lemma 6. Let (Ct)t∈θ be an ITBM computation according to a program P on
input −→x ∈Rn. Then for all α > 0 the following hold:
(i) If t < ω · α then C ↾t + 1 ∈J1+α[−→x ].
(ii) C ↾ω ·α is uniformly Σ1(J1+α[−→x ]) deﬁnable in the parameters −→x and Hω.
Proof. By the set theoretic recursion theorem, Deﬁnition 1 yields a deﬁnition of
(Ct)t∈θ of the form
y = Ct ↔∃f : t + 1 →V [f(0) = G0(−→x )
∧∀u < t(f(u + 1) = G1(f(u), Hω))
∧∀u < t(limit(u) →f(u) = G2(f ↾u, Hω))
∧y = f(t)].
(1)
The functions G0, G1, G2 are rudimentary: G0 produces the initial conﬁgura-
tion C0 from the input −→x . This amounts to assembling (R(0), I(0)) from the
components of −→x . This operation is certainly rudimentary.
The function G1 is the transition function from the conﬁguration at time u
to the conﬁguration at time u+1. The transition involves some case distinctions
and the application of rational functions to register contents. The complexity
of real arithmetic is indeed arithmetical in the arguments: for reals a and b the
relations a = b, a < b and the reals a + b, a · b, a −b, and a
b (in case b ̸= 0) are
ﬁrst-order deﬁnable over the structure (Hω, a, b). Since Hω = J1[−→x ] ∈J2[−→x ],
real arithmetic is rudimentary, using the extra parameter Hω.

414
P. Koepke and B. Seyﬀerth
The function G2 performs the ITBM limit operation at limit times u < θ.
Given f ↾u, the ordinary limit in the binary reals can be obtained by ﬁrst-order
quantiﬁcation over the multisorted structure (Hω, u, f ↾u). So G2 is rudimen-
tary.
This means that deﬁnition (1) is a Σ1-deﬁnition of y = Ct whose kernel is
rudimentary. Rudimentary predicates are absolute with respect to any level of
the Jα[.]-hierarchy. Note that the unique witness for the existential quantiﬁer in
(1) is C ↾(t + 1).
We now show the lemma by simultaneous induction on α > 0. By our consid-
erations so far, (i) for α implies (ii) for α.
Case α = 1. If t < ω · α = ω then C ↾t + 1 is built from −→x and Hω by ﬁnitely
many applications of the rudimentary functions G0 and G1. Since J1+1[−→x ] is
closed under rudimentary functions, C ↾t + 1 ∈J1+1[−→x ].
Case α = β + 1, where the lemma holds for β. Then C ↾ω · β is Σ1(J1+β[−→x ])
deﬁnable in parameters, and hence C ↾β ∈J1+α[−→x ]. For t ∈[ω · β, ω · α),
C ↾t+1 can be built from C ↾β by ﬁnitely many applications of the rudimentary
functions G0 and G1. Hence C ↾t + 1 ∈J1+α[−→x ].
For α being a limit ordinal, property (i) for all β < α immediately implies
property (i) at α.
Corollary 3. Every ITBM computable real is an element of Lωω = Jωω.
A set A ⊆R is ITBM decidable if there is an ITBM program that outputs 0 on
inputs x ∈A and 1 otherwise. A set B ⊂R is ITBM semi-decidable if there is a
program P that halts only on the x ∈B.
Corollary 4. All ITBM (semi-)decidable sets of reals lie in Lωω[R].
4
Remarks and Open Questions
As with other models of ordinal computability, we have established a connection
between computability and G¨odel’s constructibility. A natural conjecture is that
Corollary 3 can be reversed. Can the constructible hierarchy up to Lωn be ”sim-
ulated” by an ITBM? This will require ITBMs to be able to do simple syntactic
operations and inductions up to ωn, for every natural number n.
ITBMs form ”pointwise” limits at every limit time. This should lead to con-
nections with the Baire hierarchy of functions.
Relaxing the limit rules by going to lim inf’s, e.g., will allow computations
to go on beyond time ωω and will lead to stronger notions of computability. In
[6] resp. [7], [3] we studied machines with ﬁnitely many registers that contain
natural numbers. At limit times register contents are the lim inf’s of previous
register contents. The weaker machines in [6] crash when one of these lim inf’s
is ∞whereas in [7], [3] that register is reset to 0. One can use similar limit
rules for inﬁnite time generalizations of Blum-Shub-Smale machines. Those gen-
eralizations obviously are able to simulate the machines of [6] resp. [7], [3] and
should thus allow to compute all hyperarithmetic reals resp. ﬁnitely iterated
hyperjumps.

Towards a Theory of Inﬁnite Time Blum-Shub-Smale Machines
415
References
1. Blum, L., Shub, M., Smale, S.: On a theory of computation and complexity over the
real numbers: NP-completeness, recursive functions and universal machines. Bulletin
of the American Mathematical Society 21(1), 1–46 (1989)
2. Brattka, V.: The emperors new recursiveness: The epigraph of the exponential funci-
ton in two models of computability. In: Ito, M., Imaoka, T. (eds.) Words, Languages
and Combinatorics III, pp. 63–72. World Scientiﬁc Publishing, Singapore (2003)
3. Carl, M., Fischbach, T., Koepke, P., Miller, R., Nasﬁ, M., Weckbecker, G.: The basic
theory of inﬁnite time register machines. Archive for Mathematical Logic 49(2), 249–
273 (2010)
4. Hamkins, J.D., Lewis, A.: Inﬁnite time Turing machines. The Journal of Symbolic
Logic 65(2), 567–604 (2000)
5. Koepke, P.: Turing computations on ordinals. The Bulletin of Symbolic Logic 11,
377–397 (2005)
6. Koepke, P.: Inﬁnite Time Register Machines. In: Beckmann, A., Berger, U., L¨owe,
B., Tucker, J.V. (eds.) CiE 2006. LNCS, vol. 3988, pp. 257–266. Springer, Heidelberg
(2006)
7. Koepke, P., Miller, R.: An Enhanced Theory of Inﬁnite Time Register Machines.
In: Beckmann, A., Dimitracopoulos, C., L¨owe, B. (eds.) CiE 2008. LNCS, vol. 5028,
pp. 306–315. Springer, Heidelberg (2008)
8. Weihrauch, K.: Computable analysis. An Introduction. Springer, Heidelberg (2000)

Turing Pattern Formation without Diﬀusion
Shigeru Kondo
Graduate School of Frontier Biosciences, Osaka University, 1-3 Yamadaoka, Suita,
Osaka 565-0871, Japan
skondo@fbs.osaka-u.ac.jp
Abstract. Using the pigmentation pattern of zebraﬁsh as the experi-
mental system, we have been studying the mechanism of skin pattern
formation. Recent ﬁndings of the cellular interactions among the two
types of pigment cells, melanophores and xanthophores, are uncovering
the cellular and molecular mechanisms. With these data, we now can an-
swer the crucial question, “Is this a Turing mechanism or not?” We have
identiﬁed the molecular basis of three interactions between the pigment
cells. All of them are transferred at the tip of the dendrites of cells. In
spite of the expectation of many theoretical biologists, there is no dif-
fusion of the chemicals involved in the patterning mechanism. However,
we also found that the lengths of the dendrites are diﬀerent among the
interactions, which makes it possible to generate the conditions of Turing
pattern formation, “local positive feedback and long range negative feed-
back”. Although it does not contain “diﬀusion”, it may be appropriate
to be called as a Turing mechanism.
1
Introduction
Over the past three decades, studies at the molecular level have revealed that
a wide range of physiological phenomena are regulated by complex networks of
cellular or molecular interactions [1]. The complexity of such networks gives rise
to new problems, however, as the behavior of such systems often deﬁes immedi-
ate or intuitive understanding. Mathematical approaches can help to facilitate
the understanding of complex systems, and to date these have taken two pri-
mary forms. The ﬁrst of these involves analyzing every element of a network
quantitatively and simulating all interactions by computation [1]. This strategy
is eﬀective in relatively simple systems, for example, the metabolic pathway in a
single cell, and is extensively explored in the ﬁeld of systems biology. However,
for more complex systems in which spatiotemporal parameters take on impor-
tance, it becomes almost impossible to make a meaningful prediction. In such
cases, a second strategy involving simple mathematical modeling from which the
details of the system are omitted can be more eﬀective in extracting the nature
of the complex system [4]. The reaction-diﬀusion (RD) model [6] proposed by
Alan Turing is a masterpiece of this sort of mathematical modeling, one that is
capable of explaining how spatial patterns develop autonomously.
In the RD model, Turing used a simple system of “two diﬀusible substances
interacting with each other” to represent patterning mechanisms in the em-
bryo, and found that such systems have the ability to generate spatial patterns
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 416–421, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Turing Pattern Formation without Diﬀusion
417
autonomously. Unfortunately Turing died soon after publishing this legendary
paper, but simulation studies of the model have shown that this system can
replicate most biological spatial patterns [3,5,2]. Later, a number of mathemati-
cal models [3] have been proposed, but in most of them, Turings basic idea that
“the mutual interaction of elements results in spontaneous pattern formation” is
followed. The RD model is now recognized as a standard among mathematical
theories that deal with biological pattern formation.
However, this model has yet to gain widespread acceptance among exper-
imental biologists. One of the major causes for this is the gap between the
mathematical simplicity of the model and the complexity of the real world.
The hypothetical molecules in the original RD model have been so idealized
for the purposes of mathematical analysis that it seems nearly impossible to
adapt the model directly to the complexity of real biological systems. However
this is a misunderstanding to which experimental researchers tend to succumb.
We can understand the logic of pattern formation using even simple models,
and by adapting this logic to very complex biological phenomena, it becomes
easier to extract the essence of the underlying mechanisms. Genomic data and
new analytic technologies have caused a shift in the target of research in de-
velopmental biology from the identiﬁcation of molecules to understanding the
behavior of complex networks, making the RD model more important as a tool
for theoretical analysis.
2
Finding Turing Patterns in Real Systems
During embryogenesis, a great variety of periodic structures develop from various
non-periodic cell or tissue sources, suggesting that waves of the sort generated
by Turing or related mechanisms may underlie a wide range of developmental
processes. Using modern genetic and molecular techniques, it is entirely possible
to identify putative elements of interactive networks that fulﬁll the criteria of
short-range positive feedback and long-range negative feedback, but ﬁnding the
network alone is not enough. Skeptics rightly point out that just because there
is water, it does not mean there are waves. No matter how vividly or faithfully a
mathematical simulation might replicate an actual biological pattern, this alone
does not constitute proof that the simulated state reﬂects the reality. This has
certainly been another major hurdle in identifying compelling examples of Tur-
ing patterns in living systems. The solution, however, is not so complicated; in
order to show that a wave exists, we need to identify the dynamic properties of
the pattern that is predicted by the computer simulation. The ﬁrst experimental
demonstrations have focused on pattern formation in the skin, because the spe-
ciﬁc characteristics of Turing patterns are more evident in two dimensions than
in one.
3
Turing Patterns in Vertebrate Skin
The ﬁrst observation of the dynamic properties of Turing patterns in nature was
made by Kondo and Asai in a study of horizontal stripes in the tropical ﬁsh,

418
S. Kondo
Pomacanthus imperator. Recently their work has shown that this dynamic na-
ture is shared by many ﬁsh species, including the well-established model organ-
ism, zebraﬁsh. While zebraﬁsh stripes may appear to be stationary, experimental
perturbation of the pattern triggers slow changes. As can be seen in Figure 1,
following laser ablation of pigment cells in a pair of black horizontal stripes,
the lower line shifts upward before stabilizing in a Bell-like curve. As a result,
the spatial interval between the lines is maintained, even when their direction
changes. This striking behavior is entirely predicted by simulation of the Turing
mechanism.
Fig. 1.
4
Zebraﬁsh
Fortunately, the zebraﬁsh is amenable to a variety of experimental approaches,
and it is hoped that this will lead to the identiﬁcation of the circuit of interactions
that generates these patterns. Work to date has shown that the skin patterns
of this ﬁsh are set up and maintained by interactions between pigmented cells.
Nakamasu [5] worked out the interaction network among the pigment cells (cf.
Figure 2). Although the shape of the network is diﬀerent from that of the orig-
inal Turing model, it ﬁts the short-range positive, long-range negative feedback
description.

Turing Pattern Formation without Diﬀusion
419
Fig. 2.
5
Identiﬁcation of the Network at the Cellular and
Molecular Level
The network among the pigment cells that Nakamasu [5] had determined was
a very rough estimation of the whole system deduced from cell survival and
migration. To clarify the mechanism of pattern formation, we needed to identify
each interaction at the level of genes and molecules. To complete this project,
we have isolated the mutant genes that cause the abnormal skin pattern, and
analyzed the function of the genes with a newly developed procedure to maintain
the pigment cells in vitro. With this in vitro system, we were able to observe the
cell-cell interaction directly without any disturbance of other cells.
6
Short Range Repulsion of the Pigment Cells Is
Correspond to the Local Positive Feedback [2]
The in vitro observation of cells has shown that melanophores and xanthophores
interact directly with each other via the tip of xanthophore dendrites. Xan-
thophores extend the dendrites toward melanophores actively. Melanophores
touched by xanthophores move to escape from the touch, but the xanthophore
moves to follow the melanophores. It is obvious that mutual signal transduction
occurs at the xanthophore dendrites. This repulsive behavior of the pigment cells
is theoretically possible to induce the complete segregation of the two types of
pigment cells that are seen in the real zebraﬁsh. As this eﬀect enhances the local
dominance of one cell type, it can be recognized as local positive feedback. One
of the genes involved in the repulsive behavior is connexin418, a gap junction
gene. Artiﬁcially changing the activity of this gene gives rise to various skin
patterns in zebraﬁsh.

420
S. Kondo
7
Long Range Survival Help by the Locally Competing
Cell Is Correspond to the Long Negative Feedback
(Unpublished Data)
In vivo experiments have suggested that the existence of xanthophores is nec-
essary for the survival of melanophores, and the functional distance of this in-
teraction is assumed to be “long range”. After a systematic search with modern
molecular-genetic technologies, we recently identiﬁed a membrane bound signal-
ing molecule as being responsible for this eﬀect. Although these molecules are
bound to the membrane, melanophores spread long “legs” (pseudopodia) that
are as long as a half width of the zebraﬁsh stripe. Therefore, the membrane
molecule can transfer the signal to cells located far apart. As melanophores are
competing with xanthophores locally, this relay (melanophore > (negative) >
xanthophore > (positive) > melanophore) of the signal acts as the “long range
negative feedback loop”.
8
No Diﬀusion in the Identiﬁed Network
The discovered network satisﬁes the conditions for Turing pattern formation,
“local positive feedback and long range negative feedback”, and computer sim-
ulation of this mechanism can generate the stripe patterns. Currently we are
continuing experiments to clarify the molecular details of pigment pattern for-
mation. One of the interesting outcomes of the study is that it is now possible
to generate a wide variety (nearly all kinds of skin pattern seen in the animal
world) of patterns on the zebraﬁsh skin by modulating the nucleotide sequence
of a single gene, connexin418. Hence we are sure that the network of interaction
we discovered is the core of the pattern forming mechanism in zebraﬁsh, and
provably in many other animals. One thing that we, and most other researchers
did not expected is that “there is no diﬀusion” in the system. Therefore, the
identiﬁed mechanism is not a RD-mechanism. However, the speciﬁc functional
distance of each signal is determined by the length of dendrites and pseudopodia.
In the classic RD mechanism, the width of the stripes is determined by the
diﬀusion constants and the reaction (production and decay) ratios of the hypo-
thetical molecules. All such values are determined, in the real body of animals, by
the activity of the cells and tissues. The problem is that such cellular activity is
not stable. Production and release of the hypothetical “activator and inhibitor”
might change drastically depending on environmental conditions. This should
make the resulting pattern extremely unstable. Another problem of diﬀusion is
“3D diﬀusion”. Skin pigmentation occurs in a 2D ﬁeld. But if the signal were
transferred by diﬀusion, we should account for diﬀusion towards the inside and
outside of the body. Diﬀusion of the signaling molecule to the outside of the plane
should destroy the Turing pattern. These diﬃculties have been noticed by exper-
imental biologists, making them skeptical of the Turing mechanism. However,
we now know that the signals are transferred by the dendrites and pseudopodia.
These cellular structures are far more stable than diﬀusion, and the directionality

Turing Pattern Formation without Diﬀusion
421
is obvious. So, we expect that the skeptical feeling among experimental biolo-
gists we disappear soon. We expect they will accept more actively the theoretical
models of biological pattern formation.
9
Does the Discovered Mechanism Belong to “Turing
Mechanism”?
As the identiﬁed mechanism does not contain “diﬀusion”, it is not a RD mecha-
nism. If the deﬁnition of a Turing mechanism is identical to an RD mechanism,
then the identiﬁed mechanism cannot be called a Turing mechanism. If I have
some right to name it as the person who found it, I still would like to call it as a
Turing mechanism. One of the reasons is that the essence of the pattern forming
mechanism does not depend on the device of signal transfer. The most important
and innovative idea of Turing was that the diﬀerence of functional distance can
give rise to the periodic pattern. So any kind of signaling mechanism can give
rise to similar pattern. The reason why Turing used “diﬀusion” might be that
the details of cell biology were not yet clear at that stage. As the discovered
mechanism still retains the principle of the idea of Turing, I would like to call it
as a Turing mechanism.
10
Future Directions
Interestingly, most of the genes involved in pigment pattern formation are very
common genes that function during morphogenesis of the animal embryo. This
fact implies a possibility that the identiﬁed molecular mechanism is not only
governing the pigment pattern formation in the skin but also controlling the
structure of the animal body. To test this interesting possibility, we are now
examining the eﬀect of these genes on various morphological phenomena. It
has been questioned for long whether a Turing mechanism governs many of the
pattern formation of animals or not. The answer may come up in a few years.
References
1. Alon, U.: An introduction to systems biology. CRC Press, Taylor and Francis Group,
London (2006)
2. Inaba, M., Yamanaka, H., Kondo, S.: Pigment pattern formation by contact-
dependent depolarization. Science 335(677) (2012)
3. Kondo, S., Asai, R.: A reaction-diﬀusion wave on the skin of the marine angelﬁsh
Pomacanthus. Nature 376(765) (1995)
4. May, R.M.: Simple mathematical models with very complicated dynamics. Na-
ture 261(459) (1976)
5. Nakamasu, A., Takahashi, M., Kondo, S.: Interactions between zebraﬁsh pigment
cells responsible for the generation of Turing patterns. Proc. Natl. Acad. Sci.
U.S.A. 106(8429) (2009)
6. Turing, A.M.: The chemical basis of morphogenesis. Bull. Math. Biol. 52(153) (1990)

Degrees of Total Algorithms versus Degrees
of Honest Functions
Lars Kristiansen
Department of Mathematics, P.O. Box 1053, Blindern, 0316 Oslo, Norway
larsk@math.uio.no
Abstract. We prove a few theorems elucidating the relationship be-
tween to diﬀerent approaches to subrecursive degree theory. One ap-
proach has its roots in the theory of algorithms and Turing degrees. The
other approach has its roots in subrecursive hierarchies of fast-growing
functions.
1
Introduction
Two obviously diﬀerent, but also apparently related, degree structures have re-
cently been studied in the literature.
Cai [3,4] studies degrees of algorithms computing total functions. The algo-
rithms are given by an enumeration of, e.g., Turing machines. The reducibility
relation i ≤p j holds if T +tot(Φj) ⊢tot(Φi) where T is a suﬃciently strong ﬁrst-
order number theory and tot(Φe) is a Π0
2-sentence stating that the algorithm e
computes a total function. In this paper we shall work with Peano Arithmetic
(PA) and we deﬁne i ≤p j by PA + tot(Φj) ⊢tot(Φi). The PA-provability degrees
are the equivalence classes induced on the algorithms computing total functions
by the reducibility relation ≤p.
Kristiansen et al. [12] studies degrees of honest functions. Honest functions
are number-theoretic functions with certain properties. Typical examples of such
functions are the fast-growing functions found in a spine sequence for a subrecur-
sive hierarchy. The reducibility relation f ≤αE g holds if the function f can be
deﬁned from the function g by (Kalmar) elementary operations and ordinal re-
cursion up to the ordinal α. The honest α-elementary degrees are the equivalence
classes induced on the honest functions by ≤αE. Now, ϵ0 is the proof-theoretic or-
dinal of PA, and the zero degree in the structure of honest ϵ0-elementary degrees
consists of all honest functions being provably total in PA.
So, we are dealing with two degree structures: The PA-provability degrees are
degrees of algorithms, and two algorithms that compute the same function will
not necessarily be of the same PA-provability degree – indeed, there will be an
algorithm computing a trivial function like 2x in any PA-provability degree. The
honest ϵ0-elementary degrees are degrees of functions, and a trivial function like
2x belongs to the zero degree. The aim of this paper is to state a few theorems
elucidating relationship between these two structures.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 422–431, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Degrees of Total Algorithms versus Degrees of Honest Functions
423
2
Preliminaries
A function φ is elementary if φ can be generated from the initial functions 2x,
max, 0, S (successor), In
i (projections) by composition and bounded primitive
recursion. (Recall that f is generated by bounded primitive recursion over g,
h and j when f(⃗x, 0) = g(⃗x) and f(⃗x, y + 1) = h(⃗x, y, f(⃗x, y)) and f(⃗x, y) ≤
j(⃗x, y).) It is easy to see that for any elementary function φ, we have φ(⃗x) ≤
2max(⃗x)
k
where k is a ﬁxed number and 2x
0 = x and 2x
k+1 = 22x
k. Moreover, the
elementary predicates are closed under propositional operations and bounded
quantiﬁers, and uniform systems for coding ﬁnite sequences of natural numbers
are available inside the class of elementary functions. We are quite informal and
indicate the use of coding functions with the notations ⟨. . .⟩and (x)i where
(⟨x0, . . . , xi, . . . , xn⟩)i = xi. For more on the elementary functions, cf., e.g., [20].
The reader should have some knowledge of degree theory, and we assume fa-
miliarity with the computable functions, indexes for computable functions, com-
putation trees and other well-known concepts of computability theory. For more
on basic computability theory, cf., e.g., [18] or [19].
Furthermore, we assume the reader is familiar with formalised ﬁrst-order
arithmetic, in particular, we assume familiarity with PA. We shall work with
a version of PA where we have function symbols and deﬁning equations for all
the elementary functions, and we use N to denote the standard model for this
ﬁrst-order theory. If we simply say that a PA-statement is true, we mean that
the statement is true in N. We may display the free variables of a PA-statement
A by using the standard notation A(x1, . . . , xn). If we use this notation, then
we shall display all the free variables in the statement. For more on PA, cf., e.g.,
[13] or [5].
The reader is also required to know a little bit about ordinals and proof theory.
3
Cai’s Degree Structure
Let U be a function such that U(⟨x1, . . . , xm⟩) = xm, that is, a function giving
the last coordinate of a sequence number. Let T be the Kleene predicate, that
is, the predicate T (e, ⟨x1, . . . , xn⟩, t) holds iﬀt is a computation tree for the
eth computable function on the inputs x1, . . . , xn. The predicate T and the
function U are elementary, and for each total computable function φ we have
φ(x1, . . . , xn) = U(μz[T (e, ⟨x1, . . . , xn⟩, z)]) when e is a computable index for φ.
Let φi(x) = U(μt[T (i, x, t)]), and let Φi(x1, x2, x3) be a Δ0-statement (in the
language of PA) such that
N |= Φi(x1, x2, x3)

sx1,x2,x3
a,b,t

⇔
T (i, a, t) ∧U(t) = b .
(It is trivial that such a Δ0-statement exists since we are working with a version
of PA that contains function symbols for all the elementary functions.) Now,
φ0, φ1, φ2, . . . is an enumeration of (all) the unary computable functions, whereas

424
L. Kristiansen
Φ0, Φ1, Φ2, . . . is an enumeration of (some) Δ0-statements of PA. Moreover, we
have
N |= ∃zΦi(x1, x2, z)

sx1,x2
a,b

⇔
φi(a) = b .
Let tot(Φi) ≡∀x∃yzΦi(x, y, z). The index i of the statement Φi describes an
algorithm (that computes the function φi). If the statement tot(Φi) is true, the
algorithm given by i is total (the algorithm yields an output for any input). We
deﬁne the set of total algorithms, written Atot, by Atot = {i | N |= tot(Φi)} and
we deﬁne the reducibility relation ≤p over the total algorithms by
i ≤p j
⇔
PA + tot(Φj) ⊢tot(Φi) .
The PA-provability degrees of (total) algorithms are the equivalence classes in-
duced on Atot by ≤p.
Cai [3,4] investigates the PA-provability degrees of total algorithms. He proves
that the structure is a distributive lattice; that any degree strictly above the zero
degree, has an incomparable degree; that a minimal pair exists. He also stud-
ies a jump operator. Cai’s proofs are based on classical computability-theoretic
constructions. For more results and details, see [3,4].
4
The Honest Elementary Degrees
A function f : N →N is honest if it is monotone, that is, f(x) ≤f(x + 1);
dominates 2x, that is, f(x) ≥2x; and has elementary graph, that is, the relation
f(x) = y is elementary. A function φ is elementary in a function ψ, written φ ≤E
ψ, if φ can be generated from the initial functions ψ, 2x, max, 0, S (successor),
In
i (projections) by composition and bounded primitive recursion. The honest
elementary degrees are the equivalence classes induced on the honest functions
by the reducibility relation ≤E. We use f ≤g to denote ∀x[f(x) ≤g(x)], and f k
denotes k iterations of the function f, that is, f 0(x) = x and f k+1(x) = f(f k(x)).
Honest degree theory has its roots in subrecursion theory developed in the
1970s. Some relevant papers are those by Meyer & Ritchie [17] and Machtey
[14,15,16]. Inﬂuenced by this work, Kristiansen introduced the honest elementary
degrees about 20 years later. The ﬁrst published proof of the next theorem can
be found in Kristiansen [6].
Theorem 1 (Growth Theorem). Let f and g be honest functions. Then, we
have
g ≤E f
⇔
g ≤f k for some ﬁxed k .
In the 1990s, Kristiansen used The Growth Theorem to investigate honest ele-
mentary degrees and related subjects. This research is published in a thesis [9]
and four papers [6,7,8,10]. The structure of honest elementary degrees is a compa-
rable to a classical computability-theoretic degree structure, e.g., the structure of
Turing degrees, but the Growth Theorem makes it possible to abandon classical
computability-theoretic proof methods and investigate this structure by asymp-
totic analysis and methods of number-theoretic nature. To prove that g ≤E f,

Degrees of Total Algorithms versus Degrees of Honest Functions
425
it is suﬃcient to provide a ﬁxed k such that g ≤f k; to prove that g ̸≤E f, it
is suﬃcient to prove that such a k does not exist. Thus, there is no need for
the standard computability-theoretic machinery involving enumerations, diago-
nalisations and constructions with requirements to be satisﬁed. This makes the
proofs concise and transparent.
We know a lot about the structure of honest elementary degrees: The structure
is a distributive lattice with very strong density proprieties. We have capability
and cupability results. We have studied a jump operator, and lown and highn
degrees exist (for any n ∈N). For more results and details, see [12,11].
5
The Honest ϵ0-Elementary Degrees
The honest α-elementary degrees, where α is some ordinal less than or equal to
ϵ0, were recently introduced by Kristiansen, Weiermann & Schlage-Puchta [12].
An α-elementary degree is a generalisation of an elementary degree.
Our approach to ordinals and ordinal recursion is based on work by Weier-
mann, and others, cf., e.g., [2]. The reader should recall that any ordinal strictly
less than ϵ0 can be written in Cantor normal form, that is, in the form ωα1 +
. . . + ωαn + 0 where α1 ≥. . . ≥αn and each αi is in Cantor normal form. In
particular, any k ∈N can be written in the form k = ω0 + . . . + ω0



k copies
+ 0.
For any ordinal α < ϵ0, we deﬁne the norm of α, written N(α), by induction
over the structure of α’s Cantor normal form: N(0) = 0; N(β+γ) = N(β)+N(γ);
and N(ωβ) = 1 + N(β). We deﬁne the α-iterate of the unary function φ, written
φα, by φ0(x) = φ(x) and
φα(x) = max{φβφβ(x) | β < α ∧N(β) ≤N(α) + x}
for any α such that 0 < α < ϵ0. (The set {β | β < α ∧N(β) ≤N(α) + x} is
ﬁnite.)
Let α ≤ϵ0. A function φ is α-elementary in a function ψ, written φ ≤αE ψ,
if φ can be generated from the initial functions ψ, 2x, max, 0, S (successor), In
i
(projections) by composition, bounded primitive recursion and β-iteration where
β < α. Let SLim denote the class of all inﬁnite additive principal numbers ≤ϵ0,
that is, SLim = {ωβ | ϵ0 ≥β > 0}.
Fix α ∈SLim. It is proved in [12] that the reducibility relation ≤αE induce
a degree structure on the honest functions. The honest α-elementary degrees
are the equivalence classes induced on the honest functions by ≤αE. The next
theorem is also proved in [12].
Theorem 2 (Generalised Growth Theorem). Let f and g be honest func-
tions, and let α ∈SLim. Then, we have
g ≤αE f
⇔
g ≤fβ for some ﬁxed β < α .
For any α ∈SLim, the structure of honest α-elementary degrees seems to be
very similar to the structure of honest elementary degrees. When the Gener-
alised Growth Theorem is available, we can easily transform proofs of theorems

426
L. Kristiansen
on elementary degrees into proofs of corresponding theorems on α-elementary
degrees. Hence, structure of honest α-elementary degrees is a distributive lattice
with strong density properties; we have a jump operator; for any n, there exists
lown and highn degrees; and so on. It is an open problem whether the struc-
ture of honest α-elementary degrees is isomorphic to the structure of honest
β-elementary degrees for any α, β ≤ϵ0.
We need a few more deﬁnitions before we can state our next theorem: Δ0-
statements and Σ1-statements of the form A(x1, . . . , xn, y) will be called rep-
resentations. A representation A(x1, . . . , xn, y) is a representation of a function
φ(x1, . . . , xn) when
N |= A

sx1,...,xn,y
a1,...,an,b

⇔
φ(a1, . . . , an) = b
(*)
holds for any assignment s. If (*) holds and A is a Δ0-statement, we shall say
that A is an honest representation of φ. (Any honest function has an honest
representation.) For any representation A(⃗x, y), let tot(A) denote the statement
∀⃗x∃yA.
Now we are ready to state a theorem relating ≤ϵ0E-reducibility to PA-
provability of Π2-statements in extensions of PA. A proof of this theorem can be
found in [12].
Theorem 3. Let f and g be honest functions. Then, we have f ≤ϵ0E g if, and
only if, for any representation A of g there exists a representation B of f such
that PA + tot(A) ⊢tot(B).
Algorithms and representations are two sides of the same coin. When we have
an algorithm i (computing the function φi), then we also have a representation
of the function φi: the statement ∃tΦi(x, y, t) is a representation of φi. When we
have a representation ∃zC(x, y, z) of a function ξ, then we also have an algorithm
i computing ξ such that PA + tot(C) ⊢tot(Φi). (Let i be the algorithm that (i)
takes input x; (ii) searches for the least number ⟨y, z⟩such that C(x, y, z) holds;
and (iii) gives output y.) Hence, the next theorem relating ≤p-reducibility and
≤ϵ0E-reducibility, is a straightforward consequence of Theorem 3.
Theorem 4. Let f and g be honest functions. Then, we have f ≤ϵ0E g if, and
only if, for any algorithm i computing g there exists an algorithm j computing
f such that j ≤p i.
6
Honest Associates and PA+ Degrees
The ﬁrst-order theory PA+ is PA extended with all true Π1-statements. We deﬁne
the reducibility relation ≤p+ over the total algorithms by
i ≤p+ j
⇔
PA+ + tot(Φj) ⊢tot(Φi) .
The PA+-provability degrees of (total) algorithms are the equivalence classes
induced on Atot by ≤p+.

Degrees of Total Algorithms versus Degrees of Honest Functions
427
Now, i ≤p j implies i ≤p+ j, but i ≤p+ j does not imply i ≤p j. Hence
the structure of PA+-provability degrees is coarser than the structure of PA-
provability degrees, and inside each PA+-provability degrees there is an inﬁnite
structure of PA-provability degrees.
Next we shall deﬁne a function ψi called the honest associate of the algorithm
i. Intuitively, ψi(x) yields the computation trees for the algorithm i on all inputs
≤x. After deﬁning ψi, we shall prove the one of the main theorems of this paper:
Theorem 5 (Growth Theorem for Algorithms). For any i, j ∈Atot, we
have
i ≤p+ j
⇔
ψi ≤(ψj)α for some α < ϵ0 .
We deﬁne honest associate of the algorithm e, written ψe, by
ψe(x) = ⟨t0, t1, . . . , tx⟩
where tı = μz[T (e, ı, z)] (for ı = 0, . . . , x). Let |t| = x be a Δ0-statement of PA
being true iﬀt encodes a sequence of length x; let (t)ı = z be a Δ0-statement of
PA being true iﬀthe ıth component in the sequence encoded by t equals z; let
T (e, ı, z) be a Δ0-statement of PA deﬁning the Kleene predicate T (e, ı, z); and
ﬁnally, let
Ψe(x, t)
≡
|t| = x ∧∀ı ≤x∃z ≤t [ (t)ı = z ∧T (e, ı, z) ] .
Note the following:
– ψ0, ψ1, ψ2 . . . is an enumeration of computable functions
– if ψi is a total function, then ψi is an honest functions
– ψi is total iﬀφi is total
– Ψ0, Ψ1, Ψ2 . . . is an enumeration of Δ0-statements, and we have
N |= Ψi(x1, x2)

sx1,x2
a,b

⇔
ψi(a) = b
and thus, Ψi is an honest representation of ψi.
Lemma 1. For any i, j ∈Atot, we have
i ≤p+ j
⇔
PA+ + tot(Ψj) ⊢tot(Ψi) .
Proof. It should be obvious that we have PA ⊢tot(Ψk) ↔tot(Φk) (for any
k ∈N). Since PA+ is an extension of PA, we also have
PA+ ⊢tot(Ψk) ↔tot(Φk) .
(*)
By the deﬁnition of ≤p+ and (*), we have i ≤p+ j iﬀPA+ + tot(Φj) ⊢tot(Φi) iﬀ
PA+ + tot(Ψj) ⊢tot(Ψi).
⊓⊔
Lemma 2. Let α < ϵ0, and let f be an honest function. Then, fα is an honest
function. Moreover, for any honest representation F of f, there exists an honest
representation Fα of fα such that PA + tot(F) ⊢tot(Fα).

428
L. Kristiansen
Lemma 2 holds since transﬁnite induction up to ϵ0 can be carried out in PA.
The proof is long and technical, but there should be no surprise that the lemma
holds. (Proofs of related results can be found in [12]; see Lemma 17 and Theorem
16.) The next lemma is the crucial lemma needed for the proof of the Growth
Theorem for Algorithms. Lemma 2 is used to prove the right-left implication of
the next lemma.
Lemma 3. For any i, j ∈Atot, we have
PA+ + tot(Ψj) ⊢tot(Ψi)
⇔
ψi(x) ≤(ψj)α(x) for some α < ϵ0 .
Proof. The left-right implication is a consequence of the following claim.
(Claim) Let h be an honest function, and let Ah be any honest repre-
sentation of h. Furthermore, let Bφ be a representation of the function
φ. If PA+ + tot(Ah) ⊢tot(Bφ), then we have φ ≤hγ for some γ < ϵ0.
This claim can be veriﬁed by making minor modiﬁcations and extensions to
the proofs found in Blankertz & Weiermann [1]. A derivation in a Tait-style
calculus of a set of PA-formulas can be embedded in a calculus based on so-
called F-controlled derivations. F-controlled derivations are similar to the usual
derivations in a ﬁrst-order Tait-style calculus, but an inﬁnitary ω-rule replaces
the standard ∀-rule, and moreover, the F-controlled derivations embody some
explicit information about ∃-witnesses, derivation lengths and complexity of cut
formulas. Let Γ denote a set of PA-formulas. The Embedding Lemma in [1]
roughly states that if PA ⊢Γ, then we have a derivation Fγ ⊢α
r Γ in the F-
controlled calculus where γ, α < ϵ0; r < ω; and F is an elementary honest
function. The ordinal α gives information about the height of the derivation;
the natural number r gives an upper bound on the complexity of the cut for-
mulas involved in the derivation; and the function Fγ yields upper bounds on
∃-witnesses. The Embedding Lemma can be extended to the following lemma.
(Extended Embedding Lemma) Let h be an honest function, and let
Ah be an honest representation of h. If PA+ + tot(Ah) ⊢Γ, then there
exists γ, α < ϵ0 and r < ω such that hγ ⊢α
r Γ.
The proof of this lemma is very similar to the proof of the Embedding Lemma
in [1] except that we now also have to assure that
(i) everything works well when we are working with PA+ in place of PA
(ii) we have hγ ⊢α
0 tot(Ah) for some γ, α < ϵ0.
It is easy to prove that both (i) and (ii) go through. (Indeed, the very proof
given in [1] works for the theory PA+.) When the Extended Embedding Lemma
is proved, we can proceed as in [1] and carry out cut-elimination in the F-
controlled calculus, and then, we can use the existence of cut-free proofs to
prove that PA+ + tot(Ah) ⊢tot(Bφ) entails that there exists γ < ϵ0 such that
φ ≤hγ. This completes a sketch of a possible way to prove (Claim). It should

Degrees of Total Algorithms versus Degrees of Honest Functions
429
also be possible to prove this claim by interpreting derivations (in the theory
PA+ + tot(Ah)) in G¨odel’s system T .
We turn to the proof of the right-left implication. The function ψj is honest,
and Ψj is an honest representation of ψj. By Lemma 2, we have an honest
representation (Ψj)α of (ψj)α such that PA + tot(Ψj) ⊢tot((Ψj)α). Now, assume
ψi(x) ≤(ψj)α(x). Then
∀xy [ (Ψj)α(x, y) →∃z ≤yΨi(x, z) ]
is a true Π1-statement, and thus, this statement is an axiom of PA+. (The
statement (Ψj)α is a Δ0-statement as (Ψj)α is an honest representation.) Hence,
we have
– PA + tot(Ψj) ⊢tot((Ψj)α)
– PA+ ⊢∀xy [ (Ψj)α(x, y) →∃z ≤yΨi(x, z) ]
and then we also have PA+ + tot(Ψj) ⊢tot(Ψi).
⊓⊔
The Growth Theorem for Algorithms (Theorem 5) follows straightforwardly from
Lemma 1 and Lemma 3.
Corollary 1. Let i, j ∈Atot and recall that ψi and ψj are the honest associates
of respectively i and j. The following statements are equivalent: (i) i ≤p+ j. (ii)
ψi ≤(ψj)α for some α < ϵ0. (iii) ψi ≤ϵ0E ψj. (iv) For any representation A of
ψj there exists a representation B of ψi such that PA + tot(A) ⊢tot(B). (v) For
any algorithm j′ computing ψj there exists an algorithm i′ computing ψi such
that i′ ≤p j′.
Proof. (i) and (ii) are equivalent by the Growth Theorem for Algorithms; (ii) and
(iii) are equivalent by Theorem 2; (iii) and (iv) are equivalent by Theorem 3; and
(iii) and (v) are equivalent by Theorem 4.
⊓⊔
7
The Isomorphism Theorem
We use ≡r to denote the equivalence relation induced by the reducibility relation
≤r.
Lemma 4. Let f be an honest function. Then, there exists ψi such that f ≡E ψi
(and thus f ≡ϵ0E ψi).
Proof. We need the following claim.
(Claim) Let ξ be an elementary function. Then there exists an index i
and a ﬁxed k ∈N such that ξ(x1, . . . , xn) = φi(⟨x1, . . . , xn⟩) and
T (i, ⟨x1, . . . , xn⟩, t) ⇒t ≤2⟨x1,...,xn⟩
k
.

430
L. Kristiansen
This claim is proved by induction over the build-up of ξ from the initial functions
2x, max, 0, S, In
i by composition and bounded primitive recursion.
Let f be an arbitrary honest function. We shall prove that there exists i ∈Atot
such that f ≡E ψi.
Let χf(x, y) = 0 if f(x) = y; otherwise, let χf(x, y) = 1. Now, χf is elementary
as f is honest. Thus, by the claim, there exists j ∈Atot and a ﬁxed k ∈N such
that
φj(⟨x, y⟩) = χf(x, y)
and
T (j, ⟨x, y⟩, t) ⇒t ≤2⟨x,y⟩
k
.
This entails that there exists i ∈Atot and a ﬁxed ℓ∈N such that
φi(x) = μy[χf(x, y) = 0] = f(x)
and
T (i, x, t) ⇒t ≤2f(x)
ℓ
.
We shall now consider ψi, that is, the honest associate of φi. We have ψi(x) =
⟨t0, . . . , tx⟩where tr = μz ≤2f(r)
ℓ
[T (i, r, z)] (for r = 0, . . . x). It follows that
there exists a ﬁxed m ∈N such that ψi(x) ≤2f(x)
m
, and thus, we also have
ψi ≤f m+1, and thus, by the Growth Theorem, ψi ≤E f. Moreover, as f ≤ψi,
the Growth Theorem also yields f ≤E ψi.
⊓⊔
Theorem 6 (Isomorphism). The structure of PA+-degrees of algorithms is
isomorphic to the structure of honest ϵ0-elementary degrees.
Proof. Any function in the sequence ψ0, ψ1, ψ2, . . . is honest, and Lemma 4 as-
sures that any honest function is ≡ϵ0E-equivalent to some function in the se-
quence ψ0, ψ1, ψ2, . . .. The Generalised Growth Theorem assures that we have
g ≤ϵ0E f
⇔
g ≤fα for some ﬁxed α < ϵ0
for any honest functions f and g. The Growth Theorem for Algorithms
(Theorem 5 above) assures that we have
i ≤p+ j
⇔
ψi ≤(ψj)α for some ﬁxed α < ϵ0
for any i, j ∈Atot. It follows that the degree structure induced on the honest
function by ≤ϵ0E is isomorphic to the structure induced on Atot by ≤p+.
⊓⊔
Proof methods based on computability-theoretic constructions work for PA-
provability degrees, but they do not work for PA+-provability degrees. Proof
methods based on growth of honest functions work for PA+-provability degrees,
but they do not for PA-provability degrees. The proof methods developed by
Cai are needed to investigate the structure of PA-provability degrees. Still, some
results on PA+-provability degrees have corollaries for PA-provability degrees as
i ̸≤p+ j implies i ̸≤p j. One example of such a corollary is that there exists an
inﬁnite anti-chain of PA-provability degrees. This follows from Theorem 6 since
there exists inﬁnite anti-chains in the structure of honest ϵ0-elementary degrees.
(It is proved in [9] that any countable partial ordering can be embedded in the
structure of elementary degrees. A similar proof will yield the same result for the
structure of honest ϵ0-elementary degrees. Thus, we can ﬁnd inﬁnite anti-chains
in the structure of honest ϵ0-elementary degrees.)

Degrees of Total Algorithms versus Degrees of Honest Functions
431
Acknowledgements.
The author wants to thank Andreas Weiermann,
Mingzhong Cai and many others for enlightening discussions and advice. In
particular, he wants to thank Albert Visser for making him aware of the nice
properties of the theory PA+. An anonymous referee also deserves some thanks.
References
1. Blankertz, B., Weiermann, A.: How to Characterize Provably Total Functions. In:
Hajek (ed.) G¨odel 1996. LNL, vol. 6, pp. 205–213. Springer, Heidelberg (1996)
2. Buchholz, W., Cichon, A., Weiermann, A.: A Uniform Approach to Fundamental
Sequences and Hierarchies. Mathematical Logic Quarterly 40, 273–286 (1994)
3. Cai, M.: Degrees of Relative Provability. Accepted for publication in the Notre
Dame Journal of Formal Logic (manuscript)
4. Cai, M.: Elements of Classical Recursion Theory: Degree-Theoretic Properties and
Combinatorial Properties. PhD Thesis, Department of Mathematics, Cornell Uni-
versity (2011)
5. Kaye, R.: Models of Peano Arithmetic. Clarendon Press, Oxford (1991)
6. Kristiansen, L.: Information Content and Computational Complexity of Recursive
Sets. In: Hajek (ed.) G¨odel 1996. LNL, vol. 6, pp. 235–246. Springer, Heidelberg
(1996)
7. Kristiansen, L.: A Jump Operator on Honest Subrecursive Degrees. Archive for
Mathematical Logic 37, 105–125 (1998)
8. Kristiansen, L.: Subrecursive Degrees and Fragments of Peano Arithmetic. Archive
for Mathematical Logic 40, 365–397 (2001)
9. Kristiansen, L.: Papers on Subrecursion Theory. Dr Scient Thesis, Department of
Informatics, University of Oslo (1996) ISBN 82-7368-130-0
10. Kristiansen, L.: Lown, Highn, and Intermediate Subrecursive Degrees. In: Calude,
Dinneen (eds.) Combinatorics, Computation and Logic, pp. 286–300. Springer,
Singapore (1999)
11. Kristiansen, L., Lubarsky, R.S., Weiermann, A., Schlage-Puchta, J.-C.: On the
Structure of Honest Elementary Degrees. In: Friedman, Koerwien, M¨uller (eds.)
Accepted for Publication in the Proceedings of the Inﬁnity Project
12. Kristiansen, L., Weiermann, A., Schlage-Puchta, J.-C.: Streamlined Subrecursive
Degree Theory. Annals of Pure and Applied Logic 163, 698–716 (2012)
13. Lindstr¨om, P.: Aspects of Incompleteness. LNL, vol. 10. Springer, Berlin (1997)
14. Machtey, M.: Augmented Loop Languages and Classes of Computable Functions.
Journal of Computer and System Sciences 6, 603–624 (1972)
15. Machtey, M.: The Honest Subrecursive Classes are a Lattice. Information and
Control 24, 247–263 (1974)
16. Machtey, M.: On the Density of Honest Subrecursive Classes. Journal of Computer
and System Sciences 10, 183–199 (1975)
17. Meyer, A.R., Ritchie, D.M.: A Classiﬁcation of the Recursive Functions. Zeitschr.
f. Math. Logik und Grundlagen d. Math. Bd. 18, 71–82 (1972)
18. Odifreddi, P.: Classical Recursion Theory. North-Holland (1989)
19. Rogers, H.: Theory of Recursive Functions and Eﬀective Computability. McGraw
Hill (1967)
20. Rose, H.E.: Subrecursion. Functions and Hierarchies. Clarendon Press, Oxford
(1984)

A 5n −o(n) Lower Bound on the Circuit Size
over U2 of a Linear Boolean Function
Alexander S. Kulikov1,2, Olga Melanich1, and Ivan Mihajlin3
1 Laboratory of Mathematical Logic, Steklov Institute of Mathematics
at St. Petersburg, 27 Fontanka, St. Petersburg 191023, Russia
2 Algorithmic Biology Laboratory, St. Petersburg Academic University, Khlopina
8/3, St. Petersburg, 194021, Russia
3 St. Petersburg State Polytechnical University, Polytechnicheskaya, 29,
St. Petersburg, 195251, Russia
Abstract. We give a simple proof of a 5n −o(n) lower bound on the
circuit size over U2 of a linear function f(x) = Ax where A ∈{0, 1}log n×n
(here, U2 is the set of all Boolean binary functions except for parity and
its complement).
1
Introduction
Proving lower bounds on the circuit complexity of explicit Boolean functions is a
central problem of theoretical computer science. Despite of many eﬀorts currently
we can only prove small linear lower bounds: 3n −o(n) for circuits over the full
binary basis B2 [1,2] and 5n −o(n) for the basis U2 = B2 \ {⊕, ≡} [3]. These
lower bounds are proved for single-output Boolean functions. However, even less
is known for multi-output functions. Though intuitively it seems that computing
several functions must be harder than computing just one of them no stronger
lower bounds are known for multi-output functions. E.g., if instead of one output
we consider o(n) outputs then the strongest lower bounds over B2 and U2 are
still 3n −o(n) and 5n −o(n), respectively.
In this note, we prove a 5n −o(n) lower bound on the circuit size over U2
of a linear Boolean function f(x) = Ax where all the columns of the matrix
A ∈{0, 1}log n×n are pairwise diﬀerent and non-zero. In fact, we prove a wider
result:
CU2(Ax ⊕b) ≥5(n −m) ,
where A ∈{0, 1}m×n is a matrix consisting of n diﬀerent columns and b ∈
{0, 1}m is any vector.
The advantage of the proof is that it contains almost no case analysis though
is based on the standard gate elimination method. First, we show that an optimal
circuit for such a function does not contain out-degree one variables. For out-
degree two variables, we show that either the considered circuit is not optimal
or by appropriate substitution at least ﬁve gates can be eliminated. Finally, if a
circuit contains a variable of degree three then it is straightforward to eliminate
ﬁve gates.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 432–439, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

A 5n −o(n) Lower Bound on the Circuit Size over U2
433
2
Deﬁnitions
By Bn,m we denote the set of all Boolean functions f: {0, 1}n →{0, 1}m. Bn,1
is denoted just by Bn. For f ∈Bn,m, by fi we denote the i-th component of f
(thus, fi ∈Bn).
A circuit over a basis Ω ⊆B2 is a DAG where each vertex has in-degree either
0 or 2 and unbounded out-degree. Vertices of in-degree 0 are called input gates
and are labeled by variables x1, . . . , xn. All other vertices are called gates and
are labeled by binary functions from Ω. Some of the gates are also marked as
output gates. Edges connecting gates are usually called wires. The out-degree of
a variable is the number of gates fed by this variable.
Such a circuit computes in a natural way a function from Bn,m where m is the
number of output gates. The size of a circuit is its number of gates not including
the input gates. The two widely studied bases Ω are B2 and U2 = B2 \ {⊕, ≡}.
The 16 binary functions f(x1, x2) from B2 are usually classiﬁed as follows:
– two constants: 0, 1;
– four degenerate functions: x1, x1 ⊕1, x2, x2 ⊕1;
– eight AND-type functions: (x1 ⊕a)(x2 ⊕b) ⊕c, where a, b, c ∈{0, 1};
– two XOR-type functions: x1 ⊕x2 ⊕a, where a ∈{0, 1}.
It is easy to see that gates labeled by functions from the ﬁrst two classes can
be easily removed from a circuit. Also, circuits over U2 do not contain gates
computing XOR-functions. In the following, we assume without loss of generality
that a circuit over U2 consists of AND-type gates only.
For a function f ∈Bn,m, by CΩ(f) we denote the minimal size of a circuit
over Ω computing f.
3
Known Lower Bounds
3.1
Single-Output Functions
By a counting argument, Shannon [4] showed that the circuit complexity (over
both B2 and U2) of almost all functions from Bn is Θ(2n/n). For explicit func-
tions, the following lower bounds are known.
For the basis B2, Schnorr [5] proved a 2n −Θ(1) lower bound for a wide
class of functions satisfying some natural property. Paul [6] proved a 2n −o(n)
lower bound for the storage access function and a 2.5n −o(n) lower bound for a
modiﬁcation of the storage access function. Stockmeyer [7] proved a 2.5n−Θ(1)
lower bound for many symmetric functions. Blum [1] generalized Paul’s proof to
get a 3n −o(n) lower bound. Kojevnikov and Kulikov [8] proved a 7n/3 −Θ(1)
lower bound for functions of high degree. Demenkov and Kulikov [2] proved a
3n −o(n) lower bound for aﬃne dispersers.
For the basis U2, Schnorr [5] proved that the circuit complexity of parity is
3n−3. Zwick [9] proved a 4n−Θ(1) lower bound for certain symmetric functions.
Lachish and Raz [10] proved a 4.5n−o(n) lower bound for strongly two-dependent
functions. Iwama and Morizumi [3] improved the bound to 5n −o(n).

434
A.S. Kulikov, O. Melanich, and I. Mihajlin
3.2
Multi-output Functions
By counting one can show that almost all functions from Bn,n have circuit
complexity Θ(2n).
Several lower bounds on complexity of multi-output functions are discussed
in Hiltgen’s thesis [11] (section 4.3, “Lower Bounds on the Complexity of Vector
Functions”).
Lamagne and Savage [12] proved the following result: if, for f ∈Bn,m, all fi’s
are diﬀerent, then
CΩ(f) ≥min
1≤i≤m CΩ(fi) + m −1 .
In other words, the result says that if one needs to compute m diﬀerent functions
instead of just one then at least m −1 additional gates are needed. Using this
simple method one can prove 4n−o(n) and 6n−o(n) lower bounds on the circuit
complexity over B2 and U2, respectively, for a function from Bn,n. E.g., we can
take a function g ∈Bn with CB2(g) ≥3n −o(n) and deﬁne f = (f1, . . . , fn)
where fi(x1, . . . , xn) = g(x1, . . . , xn) ⊕xi.
Blum and Seysen [13] proved that CB2(AND, NAND) = 2n −2 and more-
over any optimal circuit for (AND, NAND) consists of two independent trees.
Red’kin [14] proved that the circuit complexity over B2 of a binary adder is
2.5n −3. The binary adder is a function from Bn,n/2+1 that outputs the sum
of two input n/2-bit numbers. Interlando et al. [15] studied the circuit com-
plexity over {⊕} of the same function studied in this note. They proved that
C{⊕}(Ax) = 2n−o(n). Chashkin [16] proved that CB2(Ax) = 2n−o(n) and also
constructed a function from Bn,n with circuit complexity (over B2) 3n −o(n).
Hiltgen [11] studied so-called feebly one-way functions, i.e., permutations from
Bn,n such that C(f) < C(f −1). He constructed examples of such f’s with
CB2(f) = n −Θ(1) and CB2(f −1) = 2n −Θ(1).
4
A 5n −o(n) Lower Bound
In this section, we consider functions f ∈Bn,m of the form f(x) = Ax⊕b, where
A ∈{0, 1}m×n is a matrix with n diﬀerent non-zero columns and b ∈{0, 1}m is
any vector. First, note that after assigning a Boolean value to any variable one
gets a function of the same type (the corresponding column of A is eliminated
and some bits of b are changed; all the columns of A are still diﬀerent and non-
zero). This allows us to prove a lower bound by induction. Next, we prove a few
elementary lemmas for circuits computing such functions.
Recall that in this section we consider only circuits over U2. One can assume
without loss of generality that such a circuit consists of AND-type gates only.
The main property of such a gate is the following: if a variable feeds an AND-type
gate then one can assign a constant to this variable so that the gate becomes
constant.
Lemma 1. If some input variable xi of a circuit computing f feeds exactly one
gate then this circuit is not optimal.

A 5n −o(n) Lower Bound on the Circuit Size over U2
435
Proof. Assume for the sake of contradiction that xi has out-degree 1.
If all outputs either depend only on xi or does not depend on xi at all then
xi does not need to feed any gate. In this case we can feed a constant instead of
xi into the gate that is fed by xi.
Otherwise at least one of the outputs depends on xi and at least one other
variable. Let G be the other input of the gate that is fed by xi. Note that G
does not depend on xi. The considered gate computes a function of the form
(xi ⊕a)(g ⊕b) ⊕c. Assume that there is an assignment to all variables but xi
under which G is equal to b. Then the considered gate trivializes to the constant
c and the resulting circuit does not depend on xi. This contradicts the fact
that even under this substitution the considered output still depends on xi. This
means that under all possible substitutions G is equal to b⊕1, i.e., G is constant.
This, in turn, means that the circuit is not optimal.
Lemma 2. Let P and Q be gates fed by a variable xi and let Q be a direct
successor of P. Then one can reconstruct the circuit without increasing its size
so that P and Q are not connected by a wire.
Proof. Let G be the other successor of P. Then Q depends on G and xi. Note
that Q cannot compute a XOR-type function of xi and G since to compute the
XOR of two variables in U2 one needs three gates. Thus, we can change the
binary function computed in Q and put a wire to Q not from P, but directly
from G. The transformation is illustrated in Fig. 1.
xi
G
P
Q
xi
G
P
Q
Fig. 1. A transformation described in Lemma 2
Theorem 1
CU2(f) ≥5(n −m) .
Proof. The proof is by induction on n −m. Without loss of generality we may
assume that A does not contain all-zero columns.
There are two cases when the statement holds by trivial reasons: n ≤m and
m = 1 (if m = 1 then n = 1 since all the columns must be diﬀerent).
Assume now that n > m and m > 1. If there is a row containing exactly one
1 (say, at i-th column), then the corresponding output depends only on xi. We
may then assign xi = 0. This removes the i-th column from the matrix as well as

436
A.S. Kulikov, O. Melanich, and I. Mihajlin
the corresponding row. It is easy to see that the resulting circuit still computes
the resulting function. It is also easy to see that all the columns in the resulting
matrix are diﬀerent.
Let now all the rows A contain at least two 1’s and consider an optimal
circuit computing f. None of the input variables is an output gate. Note also the
following property of the function f: even if we assign all variables but xi there
is at least one output that still depends on xi. Also, by Lemma 1 all variables
feed at least two gates.
Consider a top gate P, i.e., a gate fed by two variables xi and xj. As the
circuit is optimal these variables are diﬀerent and each of them feeds at least
one other gate.
We now consider several cases. In each of the cases we show that it is possible
to assign some variable a constant so that at least 5 gates are eliminated. Note
that all the gates that become constant are not output gates as all output gates
depend on at least two variables. This means that each such gate has at least
one successor. Note also that Lemma 2 allows us to assume that if two gates are
fed by the same variable then neither of them is fed by the other one.
Case 1. One of xi and xj (say, xi) feeds at least three gates (call them P, Q, R).
Then there exists a constant c ∈{0, 1} such that the substitution xi = c makes
at least two of these gates constant (say, P and Q). Hence, this substitution
eliminates P, Q, R and all the successors of P and Q (Fig. 2).
xi
P
Q
R
Case 1.1
xi
P
Q
R
Case 1.2
xi
xj
P
Q
Case 2.1
Fig. 2. Cases 1.1, 1.2, and 2.1
Case 1.1. If the total number of successors of P and Q is at least 2, then at
least ﬁve gates are eliminated.
Case 1.2. If P and Q have a single successor then it also becomes constant and
its successor is also eliminated (and if R happens to be this last successor then
R becomes constant and also its successor is eliminated).
Case 2. Both xi and xj feed exactly two gates. Denote the other successors of
xi and xj by R and Q, respectively.

A 5n −o(n) Lower Bound on the Circuit Size over U2
437
Case 2.1. Q = R (Fig. 2). We show that this case is just impossible. Indeed,
since P and Q are AND-type gates,
P = (xi ⊕a1)(xj ⊕b1) ⊕c1 ,
Q = (xi ⊕a2)(xj ⊕b2) ⊕c2 ,
for some constants a1, b1, c1, a2, b2, c2 ∈{0, 1}. Note that a1 ̸
= a2. Otherwise by
a substitution xi = a1 we would make the circuit independent of xj. By exactly
the same reason b1 ̸
= b2. But then the circuit does not distinguish between the
assignments {xi = a1, xj = b1 ⊕1} and {xi = a1 ⊕1, xj = b1} (under both these
substitutions P = c1 and Q = c2). This cannot be the case since all the columns
of A are diﬀerent and hence there is at least one output that either depends on
xi and does not depend on xj or vice versa.
Case 2.2. Q ̸
= R. Since the circuit is a DAG the gates can be topologically
sorted. Fix some topological order and assume that R precedes Q in it. This, in
particular, means that R does not depend on Q.
We would like to show that there is a constant c ∈{0, 1} such that the
substitution xj = c makes both gates P and R constant. Let G be the other
input of R (Fig. 3). As P and R are AND-type gates there exist constants
a1, b1, c1, a2, b2, c2 ∈{0, 1} such that
P = (xj ⊕a1)(xi ⊕b1) ⊕c1 ,
R = (xj ⊕a2)(g ⊕b2) ⊕c2 .
Note that under the substitution xi = b1 the gate P trivializes to the constant
c1. Then if for some substitution to all other variables except for xj the gate G
has value b2, then the gate R also trivializes to the constant c2 and the circuit
becomes independent of the variable xj while this cannot be the case. Thus,
xi
xj
G
P
Q
R
S
xi
xj
P
Q
R
G
T
Case 2.2.1
Case 2.2.2
Fig. 3. All subcases of case 2.2

438
A.S. Kulikov, O. Melanich, and I. Mihajlin
xi = b1 implies G = b2 ⊕1. A crucial observation is that then xj = a1 also
implies G = b2 ⊕1. Indeed there is no path in the circuit from Q to G (by the
assumption that R precedes Q), hence G can only depend on xi through P. And
we know that if P = c1 then G = b2 ⊕1.
Altogether, if we assign xj = a1 then P and G trivialize to constant, R also
becomes a constant as it is now fed by two constants. All the successors of
P, G, R are also eliminated. Also, in the resulting circuit xi has out-degree 1
hence by Lemma 1 at least one additional gate can be eliminated. To show that
in all possible cases at least 5 gates are eliminated we consider two subcases
depending on the successors of P (the two subcases are shown at Fig. 3). Denote
by S some successor of P. Note that Lemma 2 guarantees that S ̸
= Q and
S ̸
= R.
Case 2.2.1. S ̸
= G. Then the substitution xj = a1 kills the gates P, R, G, S.
After that at least one additional gate can be eliminated due to Lemma 1.
Case 2.2.2. S = G. Note that if any of P, G, R has out-degree more than 1
we again remove at leats 5 gates. Assume now that all of them have out-degree
exactly 1. Denote the only successor of R by T and consider the other input of
T . It is not a constant and it does not depend on xj. Thus, by an appropriate
substitution to all variables but xj we can trivialize the gate T and make the
circuit independent of xj, a contradiction.
Acknowledgements. Research is partially supported by Russian Foundation
for Basic Research (11-01-00760-a and 11-01-12135-oﬁ-m-2011), RAS Program
for Fundamental Research, Grant of the President of Russian Federation (NSh-
3229.2012.1), and Computer Science Club scholarship.
References
1. Blum, N.: A Boolean function requiring 3n network size. Theoretical Computer
Science 28, 337–345 (1984)
2. Demenkov, E., Kulikov, A.S.: An Elementary Proof of a 3n −o(n) Lower Bound
on the Circuit Complexity of Aﬃne Dispersers. In: Murlak, F., Sankowski, P. (eds.)
MFCS 2011. LNCS, vol. 6907, pp. 256–265. Springer, Heidelberg (2011)
3. Iwama, K., Morizumi, H.: An Explicit Lower Bound of 5n-o(n) for Boolean Circuits.
In: Diks, K., Rytter, W. (eds.) MFCS 2002. LNCS, vol. 2420, pp. 353–364. Springer,
Heidelberg (2002)
4. Shannon, C.E.: The synthesis of two-terminal switching circuits. Bell System Tech-
nical Journal 28, 59–98 (1949)
5. Schnorr, C.P.: Zwei lineare untere Schranken f¨ur die Komplexit¨at Boolescher Funk-
tionen. Computing 13, 155–171 (1974)
6. Paul, W.J.: A 2.5n-lower bound on the combinational complexity of Boolean func-
tions. SIAM Journal of Computing 6(3), 427–433 (1977)
7. Stockmeyer, L.J.: On the combinational complexity of certain symmetric Boolean
functions. Mathematical Systems Theory 10, 323–336 (1977)
8. Kojevnikov, A., Kulikov, A.S.: Circuit Complexity and Multiplicative Complexity
of Boolean Functions. In: Ferreira, F., L¨owe, B., Mayordomo, E., Mendes Gomes,
L. (eds.) CiE 2010. LNCS, vol. 6158, pp. 239–245. Springer, Heidelberg (2010)

A 5n −o(n) Lower Bound on the Circuit Size over U2
439
9. Zwick, U.: A 4n lower bound on the combinational complexity of certain symmetric
boolean functions over the basis of unate dyadic Boolean functions. SIAM Journal
on Computing 20, 499–505 (1991)
10. Lachish, O., Raz, R.: Explicit lower bound of 4.5n −o(n) for boolean circuits.
In: Proceedings of the Annual Symposium on Theory of Computing (STOC), pp.
399–408 (2001)
11. Hiltgen, A.P.L.: Cryptographically Relevant Contributions to Combinational Com-
plexity Theory. ETH Series in Information Processing, vol. 3 (1994)
12. Lamagna, E.A., Savage, J.E.: On the logical complexity of symmeric switching
functions in monotone and and complete bases. Technical report, Brown University
(1973)
13. Blum, N., Seysen, M.: Characterization of all optimal networks for a simultaneous
computation of AND and NOR. Acta Informatica 21(2), 171–181 (1984)
14. Red’kin, N.P.: Minimal realization of a binary adder. Problemy kibernetiki 38,
181–216 (1981) (in Russian)
15. Interlando, J.C., Byrne, E., Rosenthal, J.: The Gate Complexity of Syndrome De-
coding of Hamming Codes. Applications of Computer Algebra (ACA), 1–5 (2004)
16. Chashkin, A.V.: On complexity of Boolean matrices, graphs and corresponding
Boolean matrices. Diskretnaya Matematika 6(2), 43–73 (1994) (in Russian)

Local Induction and Provably Total Computable
Functions: A Case Study
Andr´es Cord´on–Franco and F. F´elix Lara–Mart´ın
Departamento Ciencias de la Computaci´on e Inteligencia Artiﬁcial, Facultad de
Matem´aticas. Universidad de Sevilla, C/ Tarﬁa, s/n, 41012 Sevilla, Spain
{acordon,fflara}@us.es
Abstract. Let IΠ−
2 denote the fragment of Peano Arithmetic obtained
by restricting the induction scheme to parameter free Π2 formulas. An-
swering a question of R. Kaye, L. Beklemishev showed that the provably
total computable functions (p.t.c.f.) of IΠ−
2 are, precisely, the primitive
recursive ones. In this work we give a new proof of this fact through
an analysis of the p.t.c.f. of certain local versions of induction principles
closely related to IΠ−
2 . This analysis is essentially based on the equiva-
lence between local induction rules and restricted forms of iteration. In
this way, we obtain a more direct answer to Kaye’s question, avoiding the
metamathematical machinery (reﬂection principles, provability logic,...)
needed for Beklemishev’s original proof.
1
Introduction
An important notion in studying the computational content of a fragment of
Arithmetic is that of its provably total computable functions. A number–theoretic
computable function f : INk →IN is said to be a provably total computable
function (p.t.c.f.) of a theory T , written f ∈R(T ), if there is a Σ1 formula
ϕ(⃗x, y) such that:
1. ϕ deﬁnes the graph of f in the standard model of Arithmetic IN; and
2. T ⊢∀⃗x ∃!y ϕ(⃗x, y).
Observe that condition 1. amounts to the computability of f, whereas condition
2. yields an implicit measure of the complexity of f attending to the logical
principles needed to prove that a Σ1–deﬁnition of f deﬁnes a total function. Since
it was introduced by G. Kreisel in the 1950s this notion has been widely studied,
and nice recursion–theoretic and computational complexity characterizations of
the sets R(T ) have been obtained for a good number of theories T . For instance,
by a classical result due independently to G. Mints, C. Parsons and G. Takeuti,
the class of p.t.c.f. of the scheme of induction for Σ1–formulas IΣ1 equals to the
class of the primitive recursive functions PR. Indeed, all classes R(IΣn), n ≥1,
can be characterized in terms of the Fast Growing Hierarchy up to the ordinal
ε0. As for weak fragments below IΣ1, their p.t.c.f. have been characterized in
terms of subrecursive operators (bounded recursion, bounded minimization, ...)
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 440–449, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Local Induction and Provably Total Computable Functions: A Case Study
441
as well as in terms of computational complexity classes. In fact, their classes
of p.t.c.f. have been intensively investigated in connection with important open
problems in Complexity Theory, mainly in the context of Bounded Arithmetic.
In spite of the wide range of the theories considered, a number of uniform
methods for characterizing the p.t.c.f. of an arithmetic theory are available. E.g.
Herbrand analyses as developed by W. Sieg in [9], S. Buss’ witnessing method [5]
or, in general, proof–theoretic techniques using Cut elimination theorem. How-
ever, for some particular fragments of Peano Arithmetic none of these standard
methods seems to be applicable. Of special interest is the case of the scheme of
parameter free Π2–induction, IΠ−
2 , given by the induction scheme
Iϕ :
ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)) →∀x ϕ(x) ,
restricted to ϕ(x) ∈Π−
2 (as usual, we write ϕ(x) ∈Γ −to mean that ϕ is in Γ
and contains no other free variables than x). Since IΣ−
1 ⊆IΠ−
2 and IΣ1 is Σ3–
conservative over IΣ−
1 [8], it follows that every primitive recursive function is
provably total in IΠ−
2 ; and R. Kaye asked whether the p.t.c.f. of IΠ−
2 are exactly
the primitive recursive ones. This question remained elusive until [4], where L.
Beklemishev gave a positive answer using modal provability logic techniques.
Although quite elegant, Beklemishev’s answer only provides an indirect solution.
Firstly, he reformulated IΠ−
2 in terms of local reﬂection principles (reﬂection
principles in Arithmetic are axiom schemes expressing the statement that “if a
formula ϕ is provable in a theory T then ϕ is valid”). Secondly, he derived the
result as an application of a conservation theorem for local reﬂection principles
whose proof leans upon properties of G¨odel–L¨ob provability logic GL.
In this work we obtain a more direct answer to Kaye’s question, avoiding the
metamathematical machinery needed for Beklemishev’s proof. In fact, our proof
that R(IΠ−
2 ) = PR will follow the lines of standard arguments for characterizing
classes R(T ). Let us consider, for instance, a proof that R(IΣ1) = PR. Such a
proof typically proceeds in two steps.
– Step 1: IΣ1 is Π2–conservative over the inference rule version of the principle
of Σ1–induction Σ1–IR. So, R(IΣ1) = R(Σ1–IR).
– Step 2: Applications of Σ1–IR correspond to applications of the primitive
recursion operator.
The main obstacle to apply this argument to IΠ−
2 is that there is no simple,
direct argument to reduce IΠ−
2 to an inference rule version of it. Here we solve
this problem by showing that IΠ−
2 is equivalent to I(Σ−
2 , K2), a certain version
of the parameter free Σ2–induction scheme where the elements x for which the
induction axiom claims ϕ(x) to hold are restricted to be Σ2–deﬁnable elements.
Equipped with this result, it is easy to obtain that IΠ−
2
is Π2 (in fact, Π3)
conservative over the corresponding inference rule version (Σ2, K2)–IR. Then,
we show that applications of (Σ2, K2)–IR correspond to (restricted forms) of the
iteration operator and thus all functions in R(IΠ−
2 ) are primitive recursive.
Our analysis also yields a new conservation theorem for fragments of Peano
Arithmetic, which is of independent interest. Namely, we prove that IΠ−
2 is Π3–
conservative over IΣ1. This improves on a previous result by Beklemishev in [4],

442
A. Cord´on–Franco and F.F. Lara–Mart´ın
where conservativity between these theories with respect to boolean combina-
tions of Σ2–sentences was established.
We close this section by giving a precise deﬁnition of the auxiliary scheme
that will be central in our analysis of the class of p.t.c.f. of IΠ−
2 .
Let L = {0, S, +, ·, <} denote the language of ﬁrst order Arithmetic. If Γ is
a set of formulas of L, then IΓ is the theory axiomatized over Robinson’s Q by
the induction scheme, Iϕ, restricted to formulas ϕ(x) ∈Γ. If free variables other
that x are not allowed, we write ϕ(x) ∈Γ −and, accordingly, IΓ −denotes the
theory axiomatized over Q by the axioms Iϕ, for ϕ(x) ∈Γ −.
Deﬁnition 1. I(Σ2, K2) is the theory given by IΣ−
1 together with the scheme
ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)) →
→∀x1, x2 (δ(x1) ∧δ(x2) →x1 = x2) →∀x (δ(x) →ϕ(x))
where ϕ(x) ∈Σ2 and δ(x) ∈Σ−
2 . The natural inference rule associated to this
scheme, denoted (Σ2, K2)–IR, is given by:
ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1))
∀x1, x2 (δ(x1) ∧δ(x2) →x1 = x2) →∀x (δ(x) →ϕ(x))
where δ(x) ∈Σ−
2 and ϕ(x) ∈Σ2. Finally, if we restrict the scheme to ϕ(x) ∈
Σ−
2 , we obtain the parameter free counterpart of I(Σ2, K2), denoted I(Σ−
2 , K2).
Remark 1. Firstly, let us recall that, given a model A, K2(A) denotes the set
of elements of A that are deﬁnable in A by a formula δ(x) ∈Σ2. This explains
why K2 appears in our notation for these theories. Secondly, if A |= IΣ−
1 , then
K2(A) ≺2 A (i.e., K2(A) is a Π2–elementary substructure of A). This property
plays an important role in what follows and it is because of it that I(Σ2, K2) is
axiomatized over IΣ−
1 instead of over a weaker system (such as Q or IΔ0).
A key fact is that I(Σ−
2 , K2) provides an alternative formulation of IΠ−
2 :
Lemma 2. IΠ−
2 ≡I(Σ−
2 , K2).
Proof. We only prove that I(Σ−
2 , K2) extends IΠ−
2 . The converse is similar. Let
A |= I(Σ−
2 , K2) and ϕ(x) ∈Π−
2 such that A |= ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)).
Assume A |= ∃x ¬ϕ(x). Since A |= IΣ−
1 , K2(A) ≺2 A and there is a ∈K2(A)
such that A |= ¬ϕ(a). Let δ(v) be a Σ2 formula deﬁning the element a and let
θ(x) be ∃v (δ(v) ∧¬ϕ(v −x)). Clearly, A |= θ(0) ∧∀x (θ(x) →θ(x + 1)). By
I(Σ−
2 , K2), A |= θ(a) and so A |= ¬ϕ(0), which is a contradiction.
Given a theory T and an inference rule R, we denote by [T, R] the closure of T
under ﬁrst order logic and unnested applications of R. We denote by T + R the
closure of T under ﬁrst order logic and (nested) applications of R. Therefore,
T + R = 
k∈ω[T, R]k, where [T, R]0 = T and [T, R]k+1 = [[T, R]k, R].
The ﬁrst step in the analysis of IΠ−
2 is a suitable reduction of I(Σ2, K2) to a
fragment deﬁned by the rule (Σ2, K2)–IR. Indeed, we have:

Local Induction and Provably Total Computable Functions: A Case Study
443
Proposition 3. I(Σ2, K2) is Π3–conservative over IΣ−
1 + (Σ2, K2)–IR.
Very conveniently, this reduction can be carried out by the same tools used to
derive the reduction of IΣ1 to Σ1–IR (e.g., by adapting the cut–elimination
argument used in [3] to derive a similar reduction for the Collection scheme).
Alternatively, in [6], lemma 3.6, we gave a model–theoretic proof of this result
using the notion of a Σn+1–closed model, following the methods introduced by
Avigad in [1].
2
Local Induction and Restricted Iteration
Next step in our analysis is to show that applications of (Σ2, K2)–IR correspond
to (a restricted form of) the iteration operator. To this end, we shall consider
extensions of L obtained by adding a ﬁnite set of unary function symbols, F =
{f1, . . . , fn}, and a (ﬁnite or countable) set of new constant symbols, C. Through
this section we consider a ﬁxed set of constants, C, and we shall denote by LF
the language L+{f1, . . . , fn}+C. If g is a new unary function symbol then LF,g
will denote the language L{f1,...,fn,g}.
Deﬁnition 4. Let f ∈F a unary function symbol and T an LF–theory. We say
that f is an iterable non decreasing function over T if the theory T proves:
∀x1, x2 (x1 ≤x2 →f(x1) ≤f(x2)), and ∀x (x2 < f(x))
Let ΣF
0 be the class of bounded formulas of LF. Classes ΣF
n+1 and ΠF
n+1 are
deﬁned as usual. The theory IΣF
0 is the LF–theory axiomatized over Q by
– The induction axiom Iϕ for each formula ϕ ∈ΣF
0 , and
– Axioms for each f ∈F:
∀x1, x2 (x1 ≤x2 →f(x1) ≤f(x2)), and ∀x (x2 < f(x))
This is a basic theory to deal with the iteration of f and to guarantee the usual
properties of the iteration of a nondecreasing function with a ΠF
0 –deﬁnable
graph. The basic facts provable in this theory were stated in [6]. Next result
collects together the facts that we shall need in the present context.
Proposition 5. For each f ∈F there exists a formula ITf(z, x, y) ∈ΣF
0 such
that the following formulas are theorems of IΣF
0 :
1. ITf(z, x, y1) ∧ITf(z, x, y2) →y1 = y2.
2. (ITf(0, x, y) ↔x = y) ∧(ITf(1, x, y) ↔f(x) = y).
3. ITf(z + 1, x, y) ↔∃y0 ≤y (ITf(z, x, y0) ∧f(y0) = y).
4. ITf(z, x, y) →∀z0 < z ∃y0 < y ITf(z0, x, y0).
5. z ≥1 ∧ITf(z, x, y) →x2 < y ∧z ≤y.
6. z ≥1 ∧x1 ≤x2 ∧ITf(z, x1, y1) ∧ITf(z, x2, y2) →y1 ≤y2.
7. ITf(z1, x, y0) ∧ITf(z2, y0, y) →ITf(z1 + z2, x, y).
In what follows we use a more suggestive notation and write f z(x) = y instead
of ITf(z, x, y).

444
A. Cord´on–Franco and F.F. Lara–Mart´ın
Deﬁnition 6. We say that f ∈F is a dominating function over T if, for any
term t(x) of LF, there exists k ∈ω such that T proves
∀x (t(x) ≤f k(x + σ(t)))
where σ(t) = c1 + · · ·+ cm and c1, . . . , cm are all the constants occurring in t(x).
Lemma 7. Let T be an extension of IΣF
0
and let f ∈F a (iterable nonde-
creasing) dominating function over T . Then, for each term t(x1, . . . , xm) of LF
whose variables are among x1, . . . , xm, there exists k ∈ω such that
T ⊢t(x1, . . . , xm) < f k(x1 + · · · + xm + σ(t)).
Remark 2. Languages LF and the notion of a dominating function are tailored
to deal with the following situation. Assume Γ = {θ1, . . . , θm} is a ﬁnite set of
Σ0–formulas with only two free variables, say x and y, and for each j = 1, . . . , m,
¯θj(x, y) denotes the formula ∀u ≤x∃v ≤y θj(u, v). Let F = {f1, . . . , fm, f} be
a set of unary function symbols and let T be the extension of IΣF
0 with the
following additional axioms:
– For each j = 1, . . . , m, ∀x (fj(x) = y ↔∃y0 ≤y (¯θj(x, y0)∧y = (x+1)2+y0).
– ∀x (f(x) = (x + 1)2 + f1(x) + · · · + fm(x)).
Then, every h ∈F is an iterable nondecreasing function over T and f is a
dominating function over T . This last fact can be proved by induction on terms.
The most interesting case occurs when t(x) is a product of two terms, t1(x)·t2(x).
By induction hypothesis, t1(x) ≤f k(x + σ(t1)) and t2(x) ≤f l(x + σ(t2)), for
some k ≥max(l, 2) (so, for every u, f k(u) ≥k ≥2.) Then,
t(x) ≤(t1(x) + t2(x))2 ≤f(t1(x) + t2(x)) ≤f(f k(x + σ(t1)) + f l(x + σ(t2)))
≤f(2 · f k(x + σ(t))) ≤f((f k(x + σ(t)))2) ≤f k+2(x + σ(t))
and we conclude that t(x) ≤f k+2(x + σ(t)). The remaining cases are similar.
As a ﬁnal step in the analysis of (Σ2, K2)–IR and due to technical reasons,
it will be convenient to denote the Σ2–deﬁnable elements by closed terms of
an extended language. This motivates the introduction of the following local
induction rules.
Deﬁnition 8. For each set of formulas Γ and each set of closed terms Λ0 of
LF we consider the rules (where ϕ(x) ∈Γ and t ∈Λ0):
(Γ, Λ0)–IR :
ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1))
ϕ(t)
(Γ, Λ0)–IR0 :
∀x (ϕ(x) →ϕ(x + 1))
ϕ(0) →ϕ(t)
Deﬁnition 9. We say that Λ0 is exponentially closed over T if for every t, s ∈
Λ0 there exists t′ ∈Λ0 such that [T, (ΣF
1 , Λ0)–IR] ⊢∃y ≤t′ (st = y).

Local Induction and Provably Total Computable Functions: A Case Study
445
These rules were intensively studied in [6], where the following results were
obtained. From now on, we assume that T is a ﬁxed extension of IΣF
0 obtained
by adding a set of ΠF
1 sentences, and Λ0 denotes the set of all closed terms of
a sublanguage of LF extending L and containing the set of constants C (and so
Λ0 is closed under sum and product).
Remark 3. Let us note that under these assumptions T satisﬁes a natural version
of Parikh’s theorem (see [7], chapter 5, theorem 1.4). This fact will be used
extensively without further comments.
In addition, we assume that there is f ∈LF a dominating function over T and
Λ0 is exponentially closed over T . Then, we have (see lemma 4.8, lemma 4.10
and theorem 4.14 of [6]):
Proposition 10. T + (ΠF
2 , Λ0)–IR ≡T + {∀x ∃y (f t(x) = y) : t ∈Λ0}.
Theorem 11. T + (ΠF
2 , Λ0)–IR0 is ΠF
2 –conservative over T + (ΠF
2 , Λ0)–IR.
Here we extend our work in [6] and obtain a new theorem on these local induction
systems that will be crucial to derive our main result. The ideas involved are
similar to the ones used in [6] to obtain Proposition 10 and Theorem 11.
Theorem 12. T + IΣF
1 extends T + (ΣF
2 , Λ0)–IR.
Proof. The arguments used in [2], proposition 2.1, can be easily adapted to yield
that for every k ∈ω, [T, (ΣF
2 , Λ0)–IR]k ≡[T, (ΠF
2 , Λ0)–IR0]k. So it is enough to
prove that for every k ∈ω, T + IΣF
1 extends [T, (ΠF
2 , Λ0)–IR0]k. We proceed
by induction on k ∈ω:
Case k = 0 is trivial; so, let us assume that T +IΣF
1 extends [T, (ΠF
2 , Λ0)–IR0]k.
Let t ∈Λ0 and ϕ(u, v) ∈ΠF
2 such that
(†)
[T, (ΠF
2 , Λ0)–IR]k ⊢∀u (ϕ(u, v) →ϕ(u + 1, v)).
We must prove that T + IΣF
1 ⊢ϕ(0, v) →ϕ(t, v).
Without loss of generality, we can assume that ϕ(u, v) ≡∀x ∃y ϕ0(u, x, y, v),
with ϕ0(u, x, y, v) ∈ΣF
0 . Let g be a new unary function symbol and T g the
extension of T + IΣF,g
0
obtained by adding the sentences:
∀x1, x2 (x1 ≤x2 →g(x1) ≤g(x2)),
∀x (x2 < g(x))
and
∀x (f(x) ≤g(x)).
Thus, g is a dominating (iterable nondecreasing) function over T g. By (†), it
follows that [T g, (ΠF,g
2
, Λ0)–IR]k ⊢ϕg, where ϕg is the following sentence:
∀u (∀x ∃y ≤g(x + u + v) ϕ0(u, x, y, v) →∀x ∃y ϕ0(u + 1, x, y, v)).
Claim. There is a closed term τ0 ∈Λ0 such that T g + ∀x ∃y (gτ0(x) = y) proves
∀u (∀x ∃y ≤g(x+ u + v) ϕ0(u, x, y, v) →∀x ∃y ≤gτ0(u + x+ v) ϕ0(u + 1, x, y, v))

446
A. Cord´on–Franco and F.F. Lara–Mart´ın
Proof of Claim: We distinguish two cases:
Case 1: k = 0. Then T g ⊢ϕg. Hence, by Parikh’s theorem, there exists a term
s(u, x, v) of LF,g such that
T g ⊢∀u (∀x ∃y ≤g(x+u+v) ϕ0(u, x, y, v) →∀x ∃y ≤s(u, x, v) ϕ0(u+1, x, y, v))
By Lemma 7, there is m ∈ω such that T g ⊢s(u, x, v) < gm(u + x + v + σ(s)).
By induction on z it can be proved that
T g ⊢gu(x + z) = y1 ∧gu+z(x) = y2 →y1 ≤y2
and, thus, if τ0 = m + σ(s) then τ0 ∈Λ0 and the result follows.
Case 2: k ≥1. Since [T g, (ΠF,g
2
, Λ0)–IR]k ⊢ϕg and ϕg is a ΠF,g
2
–formula, by
Theorem 11, T g + (ΠF,g
2
, Λ0)–IR also proves ϕg. It follows from Proposition 10
that there exist t1, . . . , tn ∈Λ0 such that
T g + {∀x ∃y (gtj(x) = y) : j = 1, . . . , n} ⊢ϕg.
Let r = t1 + · · · + tn. Then, by part (4) of Proposition 5, T g + ∀x ∃y (gr(x) = y)
extends T g + {gtj is total :
j = 1, . . . , n}. Let h be a new unary function
symbol and let T h be the extension of T g obtained by adding to T g the axiom
∀x (gr(x) = h(x)). Then T h ⊢ϕg and T h is conservative over T g.
By Proposition 5, h is an iterable nondecreasing function over T h and T h ⊢
∀x (g(x) ≤h(x)). Therefore, h is a dominating function over T h and T h extends
IΣF,g,h
0
. By Parikh’s theorem, there is a term s(u, x, v) of LF,g,h such that
T h ⊢∀u (∀x ∃y ≤g(x+u+v) ϕ0(u, x, y, v) →∀x ∃y ≤s(u, x, v) ϕ0(u+1, x, y, v))
and, by Lemma 7, there is m ∈ω such that T h ⊢s(u, x, v) < hm(u+x+v+σ(s)).
Recall that T h ⊢hu(x + z) = y1 ∧hu+z(x) = y2 →y1 ≤y2 and, thus, if
σ0 = m + σ(s) then σ0 ∈Λ0 and T h + ∀x ∃y (hσ0(x) = y) proves
∀u (∀x ∃y ≤g(x+u+v) ϕ0(u, x, y, v) →∀x ∃y ≤hτ0(u+x+v) ϕ0(u+1, x, y, v))
Using part (7) of Proposition 5, we can prove, by ΣF,g,h
0
–induction, that
T h ⊢hz(x) = y ↔gr·z(x) = y
As a consequence, T h + ∀x ∃y (hσ0(x) = y) proves
∀u (∀x ∃y ≤g(x+u+v) ϕ0(u, x, y, v) →∀x ∃y ≤gr·σ0(u+x+v) ϕ0(u+1, x, y, v))
Hence, putting τ0 = r ·σ0 ∈Λ0, the result follows concluding the proof of Claim.
Let A |= T + IΣF
1 and c ∈A such that A |= ϕ(0, c). We shall show that
A |= ϕ(t, c). Let ψ(x, y, c) ∈ΣF
0 the formula
∀z ≤x∃w ≤y (ϕ0(0, z, w, c) ∧y = w + f(x)).

Local Induction and Provably Total Computable Functions: A Case Study
447
Then A |= ∀x∃yψ(x, y, c) and the formula ψ(x, y, c) ∧∀z < y¬ψ(x, z, c) deﬁnes a
total nondecreasing function H : A →A. There is a ΣF
0 formula, that we denote
by Hz(x) = y, deﬁning the iteration of H and, since A |= IΣF
1 , we have
A |= ∀x ∀z ∃y (Hz(x) = y).
Let θ(u, v) be the following ΠF
1 formula:
u > t ∨∀x ∀y1

Hτ u
0 (x + u + v) = y1 →∃y ≤y1 ϕ0(u, x, y, v)

.
Since A |= ∀x ∃y (H(x) = y), by deﬁnition of θ(u, v) we have A |= θ(0, v). Let us
show that A |= ∀u (θ(u, v) →θ(u + 1, v)).
Pick a, b ∈A such that A |= a ≤t ∧θ(a, b). Then, the formula Hτ a
0 (x) = y
deﬁnes a total nondecreasing function in A and we can use it to get an expansion
of A to a model Ag of T g such that Ag |= ∀x ∃y ≤g(x + a + b) ϕ0(a, x, y, b). By
part (7) of Proposition 5, we can prove by ΣF,g
0
–induction on z that
Ag |= ∀z ≤τ0 [gz(x + a + b) = Hτ a
0 ·z(x + a + b)]
In particular, Ag |= ∀x (gτ0(x+a+b) = Hτ a
0 ·τ0(x+a+b)) and, as a consequence,
Ag |= T g + ∀x ∃y (gτ0(x) = y). Hence, by the Claim, we conclude that Ag |=
∀x ∃y ≤gτ0(x + a + b) ϕ0(a + 1, x, y, b) and, therefore, A |= θ(a + 1, b).
We have shown that A |= θ(0, v) ∧∀u (θ(u, v) →θ(u + 1, v)), and we know
that A |= IΠF
1 (because IΣF
1 ≡IΠF
1 ), so, A |= ∀u θ(u, b). In particular, since
A |= θ(t, v) →∀x ∃y ≤Hτ t
0(t + x + v) ϕ0(t, x, y, v),
we conclude A |= ϕ(t, v).
3
Main Result
We are now ready to obtain the main results. Firstly, we need a version of
Theorem 12 in the language of ﬁrst–order Arithmetic.
Lemma 13. IΣ1 extends IΔ0 + (Σ2, K2)–IR.
Proof. Let A |= IΣ1 and ϕ(x) ∈Σ2 such that
(•)
IΔ0 + (Σ2, K2)–IR ⊢ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)).
We must show that for every δ(u) ∈Σ−
2 ,
(⋆)
A |= ∀x1 ∀x2(δ(x1) ∧δ(x2) →x1 = x2) →∀x (δ(x) →ϕ(x)).
By (•) there exist formulas ϕ1(x), . . . , ϕr(x) ∈Σ2 and δ1(x), . . . , δr(x) ∈Σ−
2
such that IΔ0 plus the sentences
αj :
∀x1 ∀x2(δj(x1) ∧δj(x2) →x1 = x2) →∀x (δj(x) →ϕj(x))

448
A. Cord´on–Franco and F.F. Lara–Mart´ın
(j = 1 . . . , r) proves ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)). More precisely for each j ≤r,
IΔ0 +

1≤i<j
αi ⊢ϕj(0) ∧∀x (ϕj(x) →ϕj(x + 1)),
and IΔ0 + r
i=1 αi ⊢ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)).
Let E = {j : 1 ≤j ≤r, A |= ¬∃xδj(x)} and, for each j ∈E, let θj(x, y) ∈Π0
such that ¬∃x δj(x) is equivalent to ∀x∃y θj(x, y). Let m the cardinal of E and
let F = {f1, . . . , fm, f} a set of new unary function symbols. From the set of
Σ0 formulas Γ = {θj(x, y) :
j ∈E}, we deﬁne a theory T as in Remark 2.
Let L(A) denote the language obtained by adding to L a constant symbol a, for
each a ∈A. Put T ′ = T + DΠ1(A), where DΠ1(A) is the Π1–diagram of A. Let
Λ0 be the set of closed terms of L(A) containing only constants of the form a
for a ∈K2(A). Then A has a natural expansion AF to the language LF ∪L(A)
such that AF |= T ′ + IΣF
1 . By Proposition 12, AF |= T ′ + (ΣF
2 , Λ0)–IR. Given
δ(x) ∈Σ−
2 , we can distinguish several cases:
If A |= ¬∃x δ(x) then (⋆) obviously holds. On the other hand, if A |= ¬
∀x1 ∀x2(δ(x1) ∧δ(x2) →x1 = x2), since this is a Σ2–sentence and T ′ extends
DΠ1(A), we have that T ′ ⊢¬∀x1 ∀x2(δ(x1) ∧δ(x2) →x1 = x2). So,
T ′ ⊢∀x1 ∀x2(δ(x1) ∧δ(x2) →x1 = x2) →∀x (δ(x) →ϕ(x)).
In that way (⋆) holds again. We must deal with a last case: A |= ∃!x δ(x).
Then there exists d ∈K2(A) such that A |= δ(d) and d ∈Λ0. In order to
verify (⋆) it is enough to show that T ′ + (ΣF
2 , Λ0)–IR ⊢ϕ(d).
We prove, by induction on j, that for all j = 1, . . . , r, T ′ + (ΣF
2 , Λ0)–IR ⊢αj.
Let j ≤r, and assume that T ′ + (ΣF
2 , Λ0)–IR ⊢
1≤i<j αi. Then
(•)j
T ′ + (ΣF
2 , Λ0)–IR ⊢ϕj(0) ∧∀x (ϕj(x) →ϕj(x + 1)).
If j ∈E or A |= ¬∀x1 ∀x2(δj(x1) ∧δj(x2) →x1 = x2) then, reasoning as
in previous cases, we conclude that T ′ ⊢αj. If A |= ∃!x δj(x), then there exists
b ∈K2(A) such that A |= δj(b) and b ∈Λ0. Using (•)j we get T ′+(ΣF
2 , Λ0)–IR ⊢
ϕj(b). As a consequence, T ′ + (ΣF
2 , Λ0)–IR ⊢∃x (δ(x) ∧ϕj(x)), and it follows
that T ′ + (ΣF
2 , Λ0)–IR ⊢αj, as required.
We have proved that T ′ + (ΣF
2 , Λ0)–IR ⊢r
j=1 αj; hence
T ′ + (ΣF
2 , Λ0)–IR ⊢ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1))
It follows that T ′ + (ΣF
2 , Λ0)–IR ⊢ϕ(d) and, as a consequence, (⋆) holds.
Our last theorem extends a previous conservation result obtained in [4] and, as
a direct corollary, yields the characterization of the p.t.c.f. of IΠ−
2 .
Theorem 14. IΠ−
2 is Π3–conservative over IΣ1.
Proof. Let θ be a Π3 sentence provable in IΠ−
2 . Then I(Σ2, K2) ⊢θ by
Lemma 2 and IΣ−
1 + (Σ2, K2)–IR
⊢
θ by Proposition 3. We need the
following fact:

Local Induction and Provably Total Computable Functions: A Case Study
449
Claim. IΣ−
1 + (Σ2, K2)–IR ≡IΣ−
1 + (IΔ0 + (Σ2, K2)–IR)
Proof of Claim: Each axiom of IΣ−
1 is a Σ3 sentence, so it is enough to prove
that for every σ0(u) ∈Π2,
[IΔ0, (Σ2, K2)–IR] + ∃u σ0(u) extends [IΔ0 + ∃u σ0(u), (Σ2, K2)–IR].
Assume IΔ0 + ∃u σ0(u) ⊢ϕ(0) ∧∀x (ϕ(x) →ϕ(x + 1)), with ϕ(x) ∈Σ2, and let
ψ(x, u) ∈Σ2 be σ0(u) →ϕ(x). Then, IΔ0 ⊢ψ(0, u)∧∀x (ψ(x, u) →ψ(x+1, u)),
and, therefore, [IΔ0, (Σ2, K2)–IR] ⊢Uδ →∀x (δ(x) →ψ(x, u)), where δ(x) is in
Σ−
2 and Uδ denotes the sentence ∀x1 ∀x2(δ(x1) ∧δ(x2) →x1 = x2). Then it
holds that [IΔ0, (Σ2, K2)–IR] also proves
∃u σ0(u) →(Uδ →∀x (δ(x) →ϕ(x)))
and so [IΔ0, (Σ2, K2)–IR] + ∃u σ0(u) ⊢Uδ →∀x (δ(x) →ϕ(x)), as required.
It follows from this Claim and Lemma 13 that IΣ1 extends IΣ−
1 +(Σ2, K2)–IR
and, therefore, IΣ1 ⊢θ.
Corollary 15. The class of provably total computable functions of IΠ−
2 is the
class of primitive recursive functions.
Acknowledgements. This work was partially supported by grant MTM2008-
06435 of Ministerio de Ciencia e Innovaci´on, Spain.
References
1. Avigad, J.: Saturated models of universal theories. Annals of Pure and Applied
Logic 118, 219–234 (2002)
2. Beklemishev, L.D.: Induction rules, reﬂection principles and provably recursive func-
tions. Annals of Pure and Applied Logic 85(3), 193–242 (1997)
3. Beklemishev, L.D.: A proof–theoretic analysis of collection. Archive for Mathemat-
ical Logic 37(5-6), 275–296 (1998)
4. Beklemishev, L.D.: Parameter free induction and provably total computable func-
tions. Theoretical Computer Science 224, 13–33 (1999)
5. Buss, S.: The Witness Function Method and Provably Recursive Functions of Peano
Arithmetic. In: Westertahl, D., Prawitz, D., Skyrms, B. (eds.) Proceedings of the
9th International Congress on Logic, Methodology and Philosophy of Science, pp.
29–68. Elsevier, North–Holland, Amsterdam (1994)
6. Cord´on–Franco, A., Fern´andez–Margarit, A., Lara–Mart´ın, F.F.: On conservation
result for parameter–free Πn–induction. In: C´egielski, P. (ed.) Studies in Weak
Arithmetics, pp. 49–97. CSLI Publications, Stanford (2010)
7. H´ajek, P., Pudl´ak, P.: Metamathematics of First–Order Arithmetic. Perspectives in
Mathematical Logic. Springer (1993)
8. Kaye, R., Paris, J., Dimitracopoulos, C.: On parameter free induction schemas. The
Journal of Symbolic Logic 53(4), 1082–1097 (1988)
9. Sieg, W.: Herbrand Analyses. Archive for Mathematical Logic 30, 409–441 (1991)

What is Turing’s Comparison
between Mechanism and Writing Worth?
Jean Lass`egue1 and Giuseppe Longo2
1 Centre de Recherche en ´Epist´emologie Appliqu´ee (CREA), ´Ecole Polytechnique,
32, boulevard Victor, 75015 Paris, France
{jean.lassegue,giuseppe.longo}@polytechnique.edu
2 Centre International de Recherches en Philosophie, Lettres, Savoirs (CIRPHLES),
D´epartement de philosophie, ´Ecole Normale Sup´erieure, 45 rue dUlm,
75005 Paris, France
longo@ens.fr
Abstract. In one of the many and fundamental side-remarks made by
Turing in his 1950 paper (The Imitation Game paper), an analogy is
made between Mechanism and Writing. Turing is aware that his Machine
is a writing/re-writing mechanism, but he doesn’t go deeper into the
comparison. Striding along the history of writing, we shall hint here at
the nature and the role of alphabetic writing in the invention of Turing’s
(and today’s) notion of computability. We shall stress that computing is
a matter of alphabetic sequence checking and replacement, far away from
the physical world, yet related to it once the role of physical measurement
is taken into account. Turing Morphogenesis paper, 1952, provides the
guidelines for the modern analysis of “continuous dynamics” at the core
Turing’s late and innovative approach to bio-physical processes.
1
Introduction
In his 1950 philosophical article, Turing rather oﬀhandedly used a comparison
between mechanism and writing he didn’t take time to develop: “(Mechanism
and writing are from our point of view almost synonymous)” [28, p. 456]. The
relationship between the two terms is far from being straightforward though.
Mechanism seems to be understood by Turing as a branch of physics and, in
a broader perspective, as the ideal deterministic world-view natural sciences
should pursue. On the other hand, even if writing could be stretched to ﬁt into
linguistics it is neither a type of knowledge nor a scientiﬁc paradigm but rather
a technology used for recording data in many diﬀerent areas. Therefore how
could mechanism and writing be compared? Turing’s remark could of course be
considered as a simple digression in the course of his article and the sentence
just quoted is indeed put into brackets. But it is worth trying to stick to this
comparison and follow it as far as it can lead us to. This is the goal of the next
few pages. First of all, we should reﬁne the comparison between the two terms
to make it clearer.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 450–461, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

What is Turing’s Comparison between Mechanism and Writing Worth?
451
2
Mechanism as a Scientiﬁc Paradigm
Resting upon the intellectual development of Turing himself, we know ﬁrstly
that he extended Hilbert’s formalist program as far as to come up against one
of its inner limitations: the negative result of the halting problem in 1936 is a
paradigmatic example of a limitation in decidability as well as the birth certiﬁ-
cate of computer science. Now, as hinted below, there is a deep and non-trivial
relationship between (computational) undecidability and (deterministic) unpre-
dictability in mathematical physics. Secondly, Turing managed to work out a
new kind of non-predictive determinism in his 1952 article by studying symme-
try breaking and non-linear processes in morphogenesis. In this article, Turing
puts forward the notion of an “exponential drift” - what is nowadays called
“sensitivity to initial conditions”. Because of this “drift”, a ﬂuctuation / per-
turbation below measurement, may be ampliﬁed over time and may induce an
unpredictable, yet major (i.e., observable) change in the genesis of forms. As is
well known since Poincar´e, but not much studied since then (with a few ex-
ceptions: Hadamard, Pontyagrin, Birkhoﬀ...), unpredictability pops out as soon
as non-linearity expresses dynamical interactions between components of a sys-
tem, such as in the action/reaction/diﬀusion system Turing studies in 1952 (of
course, technically, he works at the solutions given by the linear approximation
of the system, yet he is aware of, and greatly interested by, the key properties of
non-linearity). Of course, unpredictability is at the interface between the math-
ematical determination, e.g., equations, evolution functions..., and the physical
processes which is being modeled by these written equations, functions etc..
In reference to this unpredictability, Turing makes the following observation in
his 1950 paper, concerning his Discrete State Machine (DSM, as he then calls his
Logical Computing Machine), “...[in a DSM], it is always possible to predict all
future states... This is reminiscent of Laplace’s view... The prediction which we
are considering is, however, rather nearer to practicability than that considered
by Laplace” [28, p. 440].
In fact, he explains, the Universe and its processes are subject to the exponen-
tial drift and gives the following example: “The displacement of a single electron
by a billionth of a centimetre at one moment might make the diﬀerence between
a man being killed by an avalanche a year later, or escaping.”. On the contrary,
and here lies the greatest eﬀectiveness of his approach, “...It is an essential prop-
erty of... [DSMs] that this phenomenon does not occur. Even when we consider
the actual physical machines instead of the idealized machines...”, prediction is
possible, [28, p. 440]. Of course, stresses Turing, there may be a program which
is so long that it is hard (practically impossible) to predict its behavior; yet,
this is a practical issue, a very diﬀerent one from the core theoretical property,
that is deterministic unpredictability in non-linear dynamical systems, due in
particular to the exponential drift.
In view of its developments, which originated from Turing’s DSM, computer
science appears to be predictive determinism made real, and no one should be
allowed to belittle what an incredible feat it is to have made it possible since
many (and perhaps most) physical processes precisely do not belong to predictive

452
J. Lass`egue and G. Longo
determinism but to the non-predictive one Turing explored in his last years. From
this point of view, computer science came not so much as a surprise but as a
kind of miracle, even if it became more and more clear that mechanism cannot
be extended to most physical processes in nature (the “frictionless” simple pen-
dulum and pulsars are some of the known exceptions), as was established as early
as Poincar´e’s famous example in celestial mechanics concerning The Three Body
Problem: the sun and one planet give a predictable Keplerian orbit, one more
planet and the non-linear interactions let the system go, at equilibrium, along un-
stable orbits (almost always). Therefore, what should be kept in mind in Turing’s
comparison is not “mechanism” as such but the opposition between two paradigms
in science: predictive (mechanistic) determinism on the one hand and non-
predictive determinism on the other, the two of which Turing explored with close
scrutiny and in which he produced fundamental results.
In the case of writing, a similar remark should be done. It is not writing proper
that the attention should be focused on, but the opposition between explicit and
implicit processes in the recording of data. It will therefore be argued below
that computer science inherits a very speciﬁc writing structure which gives a
compelling rationale to the comparison put forward by Turing: explicit processes
in mechanism and writing are of the same nature. But in order to reach this
conclusion, we have to hark back to the history of writing.
3
The Long History of Writing
3.1
Writing Numbers and Languages
Turing does not specify which kind of writing should be compared to mechanism
and we are left only with conjectures. Let us therefore start with the most
primitive written system in the West as described in [24]. If we go back to this
primitive use of writing, we ﬁnd that it was ﬁrst used in Mesopotamia around
-3100 B.C. to count speciﬁc goods to be stored and that the recording of natural
languages came only in second, at a later stage. The counting system itself which
was utilized was much older.
Prehistoric Counting System. In order to record quantities of goods which were
presumably stocked for conservation and redistribution, Mesopotamian accoun-
tants made use of a pre-historical system of tokens that can be traced as far
back as 7500 B.C. This system of tokens had three remarkable features. Firstly,
tokens were geometrical forms, that bore no resemblance to the good which they
referred to. Secondly, these tokens were used to refer to speciﬁc goods: each kind
of geometrical token would refer to a single kind of good. Therefore, the system
implied a great variety of tokens, in fact as many as the kinds of goods to be
accounted for. Thirdly, tally was made possible through sheer repetition: using
as many tokens as the number of pieces would indicate the quantity of the good
in stock. By its geometrical form, each token was bestowed a qualitative meaning
referring to the type of good and by its repetition, a quantitative one, referring
to the number of pieces under consideration. It should be emphasized that this

What is Turing’s Comparison between Mechanism and Writing Worth?
453
system is purely graphical and does not presuppose any use of natural language,
even if it was presumably accompanied by verbal expression.
Involvement of Language. Later on, the tally system evolved and verbal lan-
guage became necessary to keep track of the name of the donators. The written
recording of words in natural language capable of referring to names would fol-
low the numerical system which was purely graphical. But the transformation
of the writing system had a consequence on the writing of numbers in return.
It was not thanks to repetition that the tokens were now capable of referring
to numbers: it was through their names in natural language that they could
play the role of predicates1. This meant a fantastic economy of means since from
then on, no repetition of tokens was needed to express plurality and a single
token interpreted as a quantity could now avoid the tedious and often mistaken
repetition of the same token. But in return, it would also induce the use of nat-
ural language as the foundation for the whole counting system: writing was not
entirely graphical anymore and would then oscillate between a visible, graphical
part and an invisible, verbal one (for example the graphical token “˚AA” would
mean ﬁve units), the relationship between the two being subject to technical
innovations all along the history of writing.
3.2
Three Technical Innovations in the History of Verbal Writing
Three of these innovations are worth mentioning in order to specify in what
sense mechanism and writing can be considered as synonyms.
Phonograms. As we just noticed, as soon as it became necessary to go beyond
graphical subitizing in order to keep a numerical track of what was in stock,
speech was used in the up to then seemingly silent and only graphical process of
recording. Phonograms (addition of syllables to form a new word like in a rebus)
were devised and the way tokens standing for numbers and those standing for
goods was therefore reorganized.
A phonogram uses the ﬁrst syllable of a word chosen in advance (let us call is
a “master-word”) as a part of its own composition.2 In order to compose a new
phonogram from “master-words”, the ﬁrst syllable must therefore be recognized
as such and be used at some place (the ﬁrst or any other one ﬁxed in advance)
in the new word. This process follows therefore a numerical (ordinal) pattern in
which the places (ﬁrst, second and so on) taken by the syllables are crucial. The
disassociation of a syllable (recognized as the ﬁrst one in the master-word used as
1 Cf. [25, pp. 162–167]: “Remarkably, no new signs were created to express abstract
numbers. Instead, the signs referring to measures of grain simply acquired another
meaning: the wedge standing for a small unit of grain became ‘1’, and the circle
representing a large unit of grain became ‘10’.”
2 E.g., [25, pp. 162–167] uses as an example the composition of the modern phonogram
’Lucas’ by the composition of the token for “Lu” (retrieved from the ﬁrst syllable of
the Sumerian word for mouth “Lu”) and of the token for “Cas” (retrieved from the
ﬁrst syllable of the Sumerian word for man “Ka”).

454
J. Lass`egue and G. Longo
blueprint) plays the same role as a numerical predicate which is ﬁrst dissociated
and then placed next to a word to compose a sentence (like in the expression ’3
jars’). This writing system implies that one has to know in advance which are
the “master-words” new phonograms can be composed from: any phonographic
composition presupposes that the speaker has a knowledge of the set of syllables
used in the “master-words” of the language.
Alphabets.
Contrary to phonograms the construction of which depends on
“master-words” used as blueprints for syllables, alphabets seem at ﬁrst sight
to form a ﬁnite list of tokens that record the sounds used for the formation of all
the words of a given lexicon. Graphically speaking though, two diﬀerent types
of alphabets must be distinguished for they do not record the same phonological
realities: a consonantal alphabet or “abjad”3 tends to record the syllables of a
given language and therefore carries the semantic value attached to its syllables
whereas a vocalic-consonantal alphabet or “alphabet”4” in the strict sense of the
term tends to record the phonemes of a given language, apart from any semantic
consideration. Let us make this diﬀerence clear.
Consonantal Alphabet. A consonantal alphabet or “abjad”, is a type of writing
system where each symbol stands for a consonant, leaving the reader to supply
the appropriate vowel and complete the syllable. It is particularly ﬁt for lan-
guages (like Hebrew or Arabic) that have only a few vowels, for the semantic
indetermination regarding the completion of the syllable is rather limited and
easily supplemented by a reader, provided that he or she is knowledgeable in the
language which is spoken. In this type of writing, the knowledge of the language
is therefore a mandatory prerequisite, for no word can be read if the reader is
not able to supply the vowels.
Vocalic-consonantal Alphabet. Alphabets in the strict sense of the word are sys-
tems of writing that do not record syllables but phonemes, i.e., sounds that are
considered as having the same function in speech but do not have any meaning in
themselves. From this point of view, abjads and alphabets, even though they are
sometimes both called alphabets, are not based on the same principle (cf. [10])
and they diﬀer on at least three features. Firstly, alphabets are not restricted to
languages the structure of which possesses only a few vowels: any language can
be written with an alphabet because what is recorded is just phonemes, that is
phonological realities and not syllables. Secondly, no previous knowledge of the
language is required: anybody who can read alphabetically can read a foreign
language which is alphabetically written, even without understanding a word of
what is written (try with Turk or Vietnamese if you are not familiar with these
languages or just read something when you are tired until you eventually realize
you have been reading without understanding a word of what was written... This
is just impossible in Arabic or Hebrew). Thirdly, and as a consequence, no inter-
vention is required from the reader, which means that the process of alphabetical
3 Standing for the ﬁrst four letters in Arabic writing: ’Alif, B¯a’, ˇG¯im, D¯al.
4 Standing for the ﬁrst two letters in Greek writing: Alpha, Bˆeta.

What is Turing’s Comparison between Mechanism and Writing Worth?
455
reading can be transferred to a machine. This is the true reason for the compar-
ison Turing put forward in his 1950 article: mechanism and alphabetic writing
only involve entirely explicit processes of pattern recognition of characters that
have an objective value in the sense that they do not depend either on the type
of language (natural or artiﬁcial language as long as it is written) or on the type
of reader (human or machine).
The oldest example of an alphabet is very well known: it is the Greek one,
which appeared in the 8th century B.C. and from which all other alphabets
derive. Its linguistic originality has been commented upon through and through
but what is most striking from our point of view is just one fact: alphabetical
reading is potentially mechanisable. That is why the Hilbertian formalist program
as well as the works by Sch¨onﬁnkel and Curry (see below) and all formalisms
in the 30’s inherit a Greek alphabetical-mechanistic structure which is the core
of computability, after a long history we cannot fully describe in details here.
This alphabetical-mechanistic structure is not, so to say, the alpha and omega
of deterministic science, as Turing was well aware in his 1952 article, both in
mathematical physics and in linguistics. For it is true that explicit processes
deprived of any semantic structure lack what is most fundamental in spoken
languages as well as in mathematical (and biological) structures: their plasticity,
their versatility and, nonetheless, their incredible stability. But one has to make
clear what this deterministic structure exactly is to be able to go beyond.
4
The Alphabetic Combinators
In the 1920’s, Sch¨onﬁnkel and Curry independently proposed an algebra of signs
for expressing and computing logical predicates (cf. [8,9]) for a technical intro-
duction and an historical account). Curry’s formalism, which is still a reference,
is based on two “combinators”, K and S, that act on (combine) signs according
to the following rules:
(KX)Y > X
read: “KXY reduces to X”
SXY Z > XZ(Y Z) read: “SXY Z reduces to XZ(Y Z)”
By convention, association is intended to the left: XY Z ≡(XY )Z (the ﬁrst
sequence is identical to the second).
Surprisingly enough, by combining these “manipulators of signs” in a type-
free way (there are no restrictions on what can be applied to what), one can
compute a large class of functions from sequences of signs to sequences of signs.
For example, I ≡SKK computes the identity function:
IX ≡SKKX > KX(KX) > X,
while SIIX > XX, or (S(K(SI))(S(KK)I))XY > Y X, where, of course, X, Y,
Z . . . are arbitrary signs or sequences of signs (within a parenthesis!). Truth
values may also be encoded: T ≡K, F ≡KI. And, by a smart coding of numbers
as combinators (take 0 ≡KI and... keep going in a non-obvious way, see the

456
J. Lass`egue and G. Longo
references or below), one can compute all the Turing computable functions. Let’s
stress that Curry’s Combinatory Logic, at the origin of the formal computability
of the ’30s, is a pure calculus of alphabetic signs. Uninterpreted, or meaningless
(and this is crucial), sequences of alphabetic signs are formally copied, erased,
iterated in a ... potentially mechanisable way, following Hilbert’s request for
formal systems. There is even no intended mathematical meaning, as there was
no “interpretation” of this calculus on algebraic or geometric structures at the
time. The point of course is to understand what “interpretation” means. Yet, this
calculus is where the conception of the modern computing machine began: by a
“meaningless” action of signs over signs, formally codiﬁed by axioms and/or rules
(the rules for K and S above). All what is needed is a “pattern matching of signs”
(one should better say: sequence matching, i.e., checking that two sequences are
identical) and sequence replacement (replace a sequence by another, according
to the rules).
Shortly later, Church, also in Princeton, invented the λ-calculus [6]. This
calculus has a more “mathematical ﬂavor”, since it provides a purely formal
theory of functions, based on “functional abstraction” over variables (λy.X below
is the functional analogous of {y/X} in Set Theory). Consider a string of signs,
X, possibly containing a (free) variable y, then write λy.X as the function that
operationally behaves as follows, by “reduction” again, “>”:
(λy.X)Z > [Z/y]X
where [Z/y] is short for Z replacing y in all free occurrences of y in X (a variable
y is bound or “not free” when it occurs in the ﬁeld of a λy.(...), cf. [3] for details).
No more needs to be said: by borrowing for functions the notion of “abstraction”
used for sets, one obtains an amazingly powerful calculus with just one axiom
(and a few obvious rules for reduction, “>”). Integer numbers, for example, are
easily coded by 0 ≡λx.λy.y and n ≡λx.λy.x(x...(xy)...), where x occurs n
times. Curry’s and Church’s alphabetic calculi are equivalent modulo a simple
translation (yet, some subtleties are required, cf. [12,3]).
When these calculi and Turing’s computable functions, jointly with Kleene’s
system [13], were proved equivalent as for their number-theoretic expressiveness
(they compute the same class of functions), λ-calculus played a pivotal role: the
proofs were given under the form of “equivalent λ-expressiveness”. The Turing-
Church Thesis was then justiﬁed by these rather surprising equivalences: it was
fair to assume that all formal systems a la Hilbert could at most deﬁne the same
class of functions, the Turing computable ones. The meaningless manipulation
of alphabetic strings is at the core of these formalisms, following Hilbert’s notion
of a formal system. Type Theory [7,11] made the link between propositions of
Formal Logic and types that one can associate to typable terms of Curry-Church
calculi.
Are these purely formal-alphabetic systems “in the (physical) world”? They
are not. As we said earlier on, they are grounded on one of the most fantastic
(and earliest) abstraction invented by humans: alphabetic writing. They stress
the core idea of it: signs have no and must not have meaning. In the alphabetic

What is Turing’s Comparison between Mechanism and Writing Worth?
457
systems of writing, meaning pops out phonetically: the sound of voice gives
meaning to the sequences of letters.
However, let’s make a distinction. In the formal systems for computation, a
form of meaning is possible: the operational one given by the rules and axioms
for manipulating them. Consider, say, the KXY > X rule mentioned above. One
may say that it gives a “meaning” to K as an operator on signs. No meaning
yet, in no ordinary, nor mathematical sense, just an operational deﬁnition. The
non obvious mathematical structures “interpreting” these calculi were invented
in the late ’60, [26] and were later given a general set-theoretic frame, see ([18,3])
for the type free calculus, and a categorical one, for both the type-free and the
typed calculi [27,19,14,1].
The diﬀerence of these forms of “meaning” should be clear. The ﬁrst is the
purely mechanical processing of uninterpreted signs: (re-)writing as a mecha-
nism. As explained above, it is the direct result of the amazing abstraction
proposed by the Greek invention of an uninterpreted alphabet, at the origin of
our modern form of writing. These signs are so meaning independent that they
may be used for a game of signs, a combinatorial mechanics which is at the true
origin of modern computing: a machine can operate according to the rules for
the K and S operators. “Mechanism and writing are from our point of view al-
most synonymous”, says Turing. The so-called operational semantics is just the
speciﬁcation of rules to operate mechanically on signs. Indeed, once Turing made
explicit the distinction between software and hardware, a vast area of program-
ming styles were directly based on Lambda Calculus and Combinatory Logic:
the functional languages (LISP, Edinburg ML...), the Haskell style’s languages
(from Haskell B. Curry), which are also functional, but with a more combina-
torial ﬂavor. Theory of Programming is largely indebted to “term-rewriting”,
[5], the discipline that generalized Curry’s and Church’s calculi by the general
analysis of alphabetic “sequence checking and sequence replacement systems of
mechanical rules”, at the core of Turing’s intuition.
The mathematical semantics instead, deals with meaning in a rather diﬀerent
way. One has to interpret those signs and their operations over independently es-
tablished mathematical structures, deriving their “meaning”, in principle, from
radically diﬀerent conceptual constructions. It is like a “translation” into a dif-
ferent language that per se bears meaning by diﬀerent or totally independent
conceptual experiences and contexts. The challenge is not obvious. Let’s see why.
Any sequence of signs can be applied to any other. So XX is well formed, say. A
way to understand that X may operate on X is to consider X as a function act-
ing on ... X. Typically, in λ-calculus, (λx.M)N works even when N ≡(λx.M),
whose mathematical meaning is that of a function applied to itself. How to con-
struct a non-trivial mathematical universe where this is possible? The universe
must be non-trivial as it must contain all integer numbers and, as we know from
Turing’s equivalences, all computable functions on them. Some non obvious cat-
egorical properties allow this, in particular the existence of “reﬂexive objects”,
i.e., spaces D such that all the morphisms (functions) on D, call it D →D, may
be isomorphically embedded into D. This is set-theoretically impossible, unless

458
J. Lass`egue and G. Longo
D = {1}, since D →D is always provably larger than D. The construction
of suitable categories is given in the references: they must have enough mor-
phisms to interpret all computable functions on integers, but must not contain
all set-theoretic functions. Here, we just want to stress that that categorical
interpretation may be really considered as providing “mathematical meaning”:
one has to construct, by totally independent tools from the operatorial signs, a
mathematical structure (a category, in its precise technical sense), where, typi-
cally, this challenging self-application is understood by an embedding of a space
of morphisms (functions), D →D, into its own domain of deﬁnition, D.
But why should these categories “add meaning”? First, as we said, a trans-
lation in another language does add meaning, per se. Second, categories are
deﬁned in terms of diagrams, that is of visual objects. Categorical diagrams may
be always described by sets of equations, yet only their visual display makes
the symmetries manifest, while they are just implicit in the equations. Duali-
ties (like adjunctions, a deep concept in Category Theory) are symmetries and
play a major role in the mathematics of categories. So, in the end, meaning is
added to the mechanical signs’ pushing, by their interpretation into geometric
structures, including diagrams and their gestaltic power to make us understand.
Symmetries are meaningful to us: they organize space, correlate structures and
visualized concepts.
Of course, by adding structural meaning we lose eﬀectiveness. In Combina-
tory logic and λ-calculus, the reduction operation, “>”, is decidable and its
transitive and symmetric closure, “=”, is semi-decidable. That is, a machine can
perform them and, given two terms, M and N, it may decide whether M > N
or semi-decide whether M = N. Instead, in all proper set-theoretic and cat-
egorical models, the interpretation of “>” and “=” are identiﬁed and are not
semi-decidable. No machine can (semi-)decide the equality of inﬁnite objects,
like the functions or morphisms in the categories needed for the semantics of
these type-free calculi. In a sense, one goes from the Laplacian universe where
dynamics and computations are predictable (one may “decide” the future), as
observed by Turing, to richer universes where meaning is grounded on man-
ifolded conceptual experiences (mathematical structures, gestalts...), far away
from the purely mechanical notion of “decidability” by an algorithm.
5
The Undecidable
There exist deep and non-obvious correlations between G¨odel-Turing undecidabil-
ity and Poincar´e’s deterministic unpredictability, at the core of modern non-linear
dynamics, cf. [21]. One thing should be clear though: our systems of equations gen-
erally yield computable solutions, if any. Writing and solving the most complex
systems of (non-)linear equations is a matter of writing and rewriting, that is one
has to write them and apply an algorithm that we invented to solve them, if any.
This is entirely eﬀective, including the result (see below for some exceptions),
unless one is crazy enough as to put a non-computable real, Chaitin’s Ω, say,
as a coeﬃcient or an exponent in the equations. Some computationalists derive

What is Turing’s Comparison between Mechanism and Writing Worth?
459
from this that “the world is computable” (against Turing’s claim: “The displace-
ment of a single electron by a billionth of a centimetre... ” may induce major
unpredictable/incomputable changes, [28, p. 440]. As he saw perfectly well, it
is the question of physical measurement that changes everything, i.e., the re-
lation between mathematics and the physical world by way of a measurement
(the “small error... about the size”, [28, p. 451]. It is our alphabetic writing of
equations and solutions that is computable. And most of the time mathemat-
ics “misses the world”: unpredictability means that the solution of the intended
equational/functional system for a physical process diverges from that process,
i.e., very soon it does not describe it anymore, or “prediction becomes impossi-
ble”, as observed since Poincar´e. The key issue is “how to associate a number to
a process”, whether it is an integer or a computable real, that is, the problem is
measurement, the only way by which we access to the physical processes.
Take a physical double pendulum: the two equations determining it, if imple-
mented on a digital computer (they have computable solutions, of course), do
not allow to predict this chaotic physical process, in view of the inevitable choice
of an input number and of (Turing’s) “exponential drift”: two (computable real)
numbers within the best measurement interval may very soon yield radically dif-
ferent trajectories. In [16], unpredictability is proved for the planetary system
(many more equations), at a reasonable astronomical time scale (about 1 mil-
lion years). There is no way to compute and predict with arbitrary precision
processes that are enough sensitive to initial conditions. The approximation in
(classical!) measurement has lower bounds of various sorts: thermal ﬂuctuation,
gravitational eﬀects, elasticity... Thus most physical processes, far from “comput-
ing” non-computable functions, just do not (well) deﬁne input-output functions,
[20,21]: starting on the same input as an interval provided by measurement, a
physical process the mathematical representation of which is chaotic, will soon
lead to diverging trajectories, thus to very diﬀerent measurable outputs. As for
actual implementations, the Shadowing Lemmas, [22], at most guarantee (and
only for hyperbolic non-linear systems), that, for any discrete space-time tra-
jectory, there exists a continuous one approximating it (and not the converse!).
So far, we dealt with the unpredictability of physical processes modeled by com-
putable functions.
There exist two remarkable exceptions to the computability of the mathemat-
ical solutions of interesting mathematical systems. One is given in [4], the other
in [23]. These eﬀectively written systems yield only non-computable solutions.
Yet, by looking closely at their proofs, one sees that this doesn’t say much, per
se, about the physical world, but only about the “mechanisms of writing” used.
The ﬁrst proof reduces, by a complex argument, to Turing’s halting problem,
thus to a purely diagonal argument within the formalism. As for the second, the is-
sue is more delicate and it has been extensively discussed, see [31]. The very elegant
system used for computability over real numbers, [30], (there are many and they
are not equivalent, a challenge for the extension of Church Thesis to computable
“continua”), uses as input values the computable reals. One then shows that com-
putability implies continuity (over the Cantor-set topology). Then the proof goes

460
J. Lass`egue and G. Longo
by proving the non-continuity of the unique solution, under computable initial
conditions. This non-continuity is clearly a relevant physico-mathematical fact, to
be discussed ﬁrst. But... what is exactly its physical sense? Poincar´e, from a purely
mathematical analysis to the non-analyticity (almost everywhere) of the solution
of the Three (planetary) Body Problem, gave a physico-mathematical meaning to
the existence of diverging coeﬃcients in the approximating series: minor ﬂuctu-
ations in the initial conditions (below measurement) could mathematically yield
divergent trajectories. Predicting is a matter of “pre-dicere” (to say or, better,
to write in advance), that is to measure ﬁrstly, then compute by (re-)writing and
compare written numerical results and measurement obtained at the end of the
process.
We lack, so far, a close analysis of the physical relevance of the incomputability
(due to a discontinuity) of the wave equations (by the way, are these as sound,
for actual waves, as Newton-Laplace equations for planets?). This should be
discussed in reference to actual physical measurement as the only way we have
to “extract” numbers from a natural process. We can only say, so far, that the
computations lead to a “mathematical singularity” (a discontinuity), an issue
within the mechanism of term (re-)writing and solving equations.
And so we are back to our founding father. Alphabetic writing and its formal
re-writing are eﬀective (they are “synonymous to mechanism”): incomputabil-
ity is their internal challenge. Each time, the relations between mathematical
writing and the world must be closely analyzed, as Poincar´e did in reference to
physical measurement: there is no way we can deduce the existence of physi-
cal super-computers from internal (yet very deep) mathematical games of signs.
The fact that the world does not compute does not need to mean that it “super-
computes”, i.e., it computes non-computable functions, as some wrongly claim.
As we said, it may simply mean that it doesn’t even (well) deﬁne a function.
This is so for a double pendulum, the planets or any system to be mathemati-
cally described as sensitive to initial conditions. It may also be the case that our
mathematical system badly models the world; typically, Navier-Stokes and wave
equations badly apply close to borders - what is the relevance of this fact, as for
measurement? More generally, what is the relationship between (eﬀective) math-
ematical writing and physical processes which may be only related by means of
physical measurement?
References
1. Asperti, A., Longo, G.: Categories, Types and Structures; Category Theory for the
working computer scientist. M.I.T. Press (1991)
2. Bailly G, Longo G.: Mathematics and Natural Sciences: the Physical Singularity
of Life. Imperial Coll. Press, London (2011)
3. Barendregt, H.: The Lambda Calculus: its syntax and semantics. North Holland,
Amsterdam (1984)
4. Braverman, M., Yampolski, M.: Non-computable Julia sets. Journ. Amer. Math.
Soc. 19, 551–578 (2006)
5. Bezem, M., Klop, J.W., Roel de Vrijer, R.: Term Rewriting Systems. Cambridge
Univ. Press, Cambridge (2003)

What is Turing’s Comparison between Mechanism and Writing Worth?
461
6. Church, A.: A set of postulates for the foundation of Logic. Annals of Math. 33(2),
346–366, 34, 37–54 (1932-1933)
7. Church, A.: A formalisation of the simple theory of types. JSL 5, 56–58 (1940)
8. Curry, H.B., Feys, E.: Combinatory Logic, vol. I. North-Holland, Amsterdam
(1968)
9. Curry, H.B., Hindley, J.R., Seldin, J.: Combinatory Logic, vol. II. North-Holland,
Amsterdam (1972)
10. Herrenschmidt, C.: Les trois ´ecritures; langue, nombre, code. Gallimard, Paris
(2007)
11. Hindley, R.: Basic Simple Type Theory. Cambridge University Press, Cambridge
(1997)
12. Hindley, R., Longo, G.: Lambda-calculus models and extensionality. Zeit. f¨ur Math-
emathische Logik und Grundlagen der Mathematik 26(2), 289–310 (1980)
13. Kleene, S.C.: Lambda deﬁnability and recursiveness. Duke Math. J. 2, 340–353
(1936)
14. Lambek, J., Scott, P.J.: Introduction to higher order Categorial Logic. Cambridge
Univ. Press, Cambridge (1986)
15. Lass`egue’s page (downloadable articles), http://www.lassegue.net
16. Laskar, J.: Large scale chaos in the Solar System. Astron. Astrophysics 287, L9–L12
(1994)
17. Longo’s page (downloadable articles), http://www.di.ens.fr/users/longo
18. Longo, G.: Set-theoretical models of lambda-calculus: Theories, expansions, iso-
morphisms. Annals of Pure and Applied Logic 24, 153–188 (1983)
19. Longo, G., Moggi, E.: A category-theoretic characterization of functional complete-
ness. Theoretical Computer Science 70(2), 193–211 (1990)
20. Longo, G.: Critique of Computational Reason in the Natural Sciences. In: Gelenbe,
E., et al. (eds.) Fundamental Concepts in Computer Science. Imperial College
Press, London (2008)
21. Longo G.: Incomputability in Physics and Biology. To appear in MSCS (2012) (see
Longo’s web page); Preliminary version: Ferreira, F., L¨owe, B., Mayordomo, E.,
Mendes Gomes, L. (eds.): CiE 2010. LNCS, vol. 6158. Springer, Heidelberg (2010)
22. Yu, P.S.: Shadowing in dynamical systems. Springer (1999)
23. Pour-El, M.B., Richards, J.I.: Computability in analysis and physics. Perspectives
in Mathematical Logic. Springer, Berlin (1989)
24. Schmandt-Besserat, D.: How Writing Came About. University of Texas Press,
Austin (1992)
25. Schmandt-Besserat, D.: From Tokens to Writing: the Pursuit of Abstraction. Bull.
Georg. Natl. Acad. Sci. 175(3), 162–167 (2007)
26. Scott, D.: Continuous lattices. In: Lawvere (ed.) Toposes, Algebraic Geometry and
Logic. SLNM, vol. 274, pp. 97–136. Springer, Berlin (1972)
27. Scott, D.: Data types as lattices. SIAM Journal of Computing 5, 522–587 (1976)
28. Turing, A.M.: Computing Machinery and Intelligence. Mind LIX, 433–460 (1950)
29. Turing, A.M.: The Chemical Basis of Morphogenesis. Philo. Trans. Royal Soc.
B 237, 37–72 (1952)
30. Weihrauch, K.: Computable analysis. Springer, Berlin (2000)
31. Weihrauch, K., Zhong, N.: Is wave equation computable or computers can beat the
Turing Machine? Proc. London Math. Soc. 85(3), 312–332 (2002)

Substitutions and
Strongly Deterministic Tilesets
Bastien Le Gloannec and Nicolas Ollinger
Laboratoire d’Informatique Fondamentale d’Orl´eans (LIFO), Universit´e d’Orl´eans,
BP 6759, 45067 Orl´eans Cedex 2, France
{Bastien.Le-Gloannec,Nicolas.Ollinger}@univ-orleans.fr
Abstract. Substitutions generate hierarchical colorings of the plane. De-
spite the non-locality of substitution rules, one can extend them by adding
compatible local matching rules to obtain locally checkable colorings as
the set of tilings of ﬁnite tileset. We show that for 2 × 2 substitutions
the resulting tileset can furthermore be chosen strongly deterministic, a
tile being uniquely determined by any two adjacent edges. A tiling by a
strongly deterministic tileset can be locally reconstructed starting from
any inﬁnite path that cross every line and column of the tiling.
A strongly deterministic tileset is a ﬁnite set of Wang tiles, square tiles with
colored edges, having the property that, for any two adjacent edges, no two tiles
share the same pair of colors. This generalization of Kari’s NW-deterministic
tilesets [1], introduced to study dynamical properties of cellular automata, was
introduced by Kari and Papasoglu [2] who constructed a strongly deterministic
aperiodic tileset by enrichment of Robinson’s aperiodic tileset [3]. The result was
more recently extended by Lukkarila [4] who proved that the Domino Problem
remains undecidable for strongly deterministic tilesets. Strong determinism adds
to the locally checkable property of tilings the ability to recover uniquely from
ﬁnite holes, the complete description of the whole tiling being encoded into every
8-connected inﬁnite path that cross every line and column of the tiling. Moreover,
the tiling can be reconstructed, that is recomputed, by iteratively applying local
rules. Notice that this notion is diﬀerent from the robustness to error introduced
by Durand et al. [5], albeit strong determinism can be used as a tool to built
robust tilesets. See Lukkarila [4] for links with self-healing in self-assembly.
A substitution rule associates a ﬁnite pattern of letters to every letter. Bigger
and bigger patterns are obtained by iterating the rule, leading to a notion of limit
set: colorings of the plane generated by the substitution. Substitutions provide
a convenient tool to organize areas in space, for example to built the skeleton
of a computation scheme. Indeed, a large amount of constructions on tilings, for
example to construct aperiodic tilings [3, 5–7], involve the enforcement of the
limit set of some substitution using local matching rules. General constructions
have been provided in the literature [7–10] to enforce limit sets of diﬀerent kinds
of substitutions by encoding locally checkable encodings inside tilings.
In this paper, we provide an eﬀective method to associate to every two-by-two
substitution s a strongly deterministic tileset τ(s) such that the limit set Λs of
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 462–471, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Substitutions and Strongly Deterministic Tilesets
463
the substitution is equal to the letter by letter projection π

Xτ(s)

of the set of
tilings Xτ(s). Our approach is based on geometric constructions. After introduc-
ing the necessary deﬁnitions, the 104 tiles aperiodic tileset from [7] is extended
as a strongly aperiodic tileset isomorphic to the tileset of Kari and Papasoglu [2]
(section 2). Given a substitution, this tileset is then decorated to obtain a tile-
set deterministic in one direction that enforces the limit set (section 3). Finally,
four copies of the previous tileset are synchronized to obtain the wanted strongly
deterministic tileset (section 4). As a consequence, techniques from [7] can be
combined to Lukkarila [4] to simplify the embedding of a Turing machine com-
putation inside strongly deterministic tilesets to prove the undecidability of the
Domino Problem and transfer more general results on tilings to the strongly
deterministic case.
1
Deﬁnitions
A tileset is a triple (τ, H, V) where τ is a ﬁnite alphabet of tiles and H, V ⊆τ 2
are ﬁnite sets of couples representing respectively the compatible horizontal and
vertical neighbors. A tiling T is a map T : Z2 →τ associating a tile to every cell
of Z2 in such a way that every tile is compatible with its neighbors relatively to
H and V: ∀(x, y) ∈Z2, (T (x, y), T (x + 1, y)) ∈H and (T (x, y), T (x, y + 1)) ∈V.
The compatibility rules represented by H and V are denoted as the local rules of
the tileset. Implicitely associating the local rules to the tiles, we usually denote
the tileset (τ, H, V) simply as τ. The set of all tilings by a tileset τ is denoted as
Xτ ⊆τ Z2. A tiling T ∈τ Z2 is periodic if there exists a translation vector p ∈Z2
such that ∀x ∈Z2, T (x+p) = T (x). A tileset τ is aperiodic if it can tile the plane
(i.e., Xτ ̸= ∅) but never in a periodic way. We use the abbreviations NE, NW,
SE, SW to denote the directions north-east, north-west, south-east, south-west
respectively. A tileset τ is NE-deterministic if for all couples of tiles (tw, ts) ∈τ2,
there exists at most one tile tne ∈τ simultaneously compatible to the east with
tw and to the north with ts: (tw, tne) ∈H and (ts, tne) ∈V. {NW,SE,SW}-
determinism is deﬁned the same way. A tileset is strongly deterministic if it is
simultaneously NE, NW, SE and SW-deterministic.
Given a ﬁnite alphabet Σ, a Σ-coloring c of the discrete plane Z2 by Σ is a
map c : Z2 →Σ. The translation of a coloring c by a vector u ∈Z2 is the map
u · c ∈ΣZ2 verifying ∀x ∈Z2, u · c(x) = c(x −u). Endowed with the product
topology over Z2 of the discrete topology over Σ, ΣZ2 is a compact topological
space. A subshift Y ⊆ΣZ2 is a topologically closed and translation invariant
subset of ΣZ2. E.g. the set of tilings Xτ of a tileset τ is a subshift of τZ2. A
subshift Y ⊆ΣZ2 is soﬁc if it can be recognized by local constraints in the
following sense: there exist a tileset τ and an alphabetical projection π : τ →Σ,
naturally extended to τ-colorings π : τ Z2 →ΣZ2, such that π(Xτ) = Y.
Let us now introduce a strengthened version of the soﬁcity that we refer to
as directional soﬁcity. A subshift Y ⊆ΣZ2 is NE-soﬁc if there exist a NE-
deterministic tileset τ and an alphabetical projection π : τ →Σ such that
π(Xτ) = Y. {NW,SW,SE}-soﬁcity is deﬁned the same way. A subshift Y ⊆ΣZ2

464
B. Le Gloannec and N. Ollinger
is 4-way soﬁc if there exist a strongly deterministic tileset τ and an alphabetical
projection π : τ →Σ such that π(Xτ) = Y.
Let ⊞denote the ﬁnite set {0, 1}×{0, 1}. A 2×2 substitution over an alphabet
Σ is a map s : Σ →Σ⊞. s is naturally extended to its global map S : ΣZ2 →ΣZ2
verifying: ∀c ∈ΣZ2, ∀x ∈Zd, ∀u ∈⊞, S(c)(2x + u) = s(c(x))(u). Following a
dynamical systems point of view, we deﬁne the set of colorings of the plane
generated by a substitution s as its limit set Λs = 
n≥0

u · Sn 
ΣZ2
u∈Z2
which is a subshift of ΣZ2. Let us conclude by stating a useful characterization
of the limit set for our construction. Given a substitution s over Σ, a Σ-coloring
c ∈ΣZ2 admits a history if there exists a sequence (un, cn)n≥0, with, ∀n ≥0,
un ∈⊞and cn ∈ΣZ2, such that c0 = c and ∀n ≥0, cn = un · S(cn+1). Then Λs
is exactly the set of colorings admitting a history.
2
A Strongly Deterministic Aperiodic Tileset
In this section, we build a strongly deterministic aperiodic tileset by enriching
the aperiodic tileset τ of 104 tiles introduced in [7].
The Aperiodic Tileset τ of 104 Tiles. The tileset τ can be deﬁned as the
smallest ﬁxed point of a 2 × 2 substitution scheme. It is indeed self-simulating
for a certain 2 × 2 substitution s on τ depicted on ﬁgure 1: both tilesets τ and
s(τ) (which tiles are 2 × 2 macro-tiles on τ) are isomorphic and every tiling by
τ can be uniquely decomposed into a tiling by s(τ). The substitution s is non-
ambiguous: every coloring of its limit set Λs admits a unique pre-image by S.
This forces in particular Λs to contain only non-periodic colorings. As Xτ ⊆Λs,
τ is aperiodic. For a detailed presentation of this construction, the reader is
invited to refer to [7].
Let us now ﬁx some useful notations. Let ■□
□□, □■
□□, □□
■□, □□
□■(i.e., SW, SE, NW,
NE) denote the four colors of the layer 1 (parity), respectively represented in
dark blue, blue, dark red, red on the ﬁgures; and X, H, V the three general
types: cross, horizontal bridge and vertical bridge respectively, of decorations of
the layer 2. Paths of information on the layer 2 are called cables. We ﬁnally use
the notations ⌝, ⌜, ⌟, ⌞to indicate the cables of positions SW, SE, NW, NE
respectively of a tile with layer 2 of type cross X.
Remark 1. Observe that s forces the layer 2 (cables) of any tiling to describe
an inﬁnite stacking of parity grids (where square colors are alternating parity in
both directions) which nth level is a grid of step 2n + 1. To establish the soﬁcity
of a limit set, one could code a coloring of the history on each level. Also note
that s forces each level of the parity grid to be translated of 1/2 step of the
immediately inferior level grid in the SW direction.
The 4-Way Deterministic Enriched Tileset τ ′. The tileset τ is not deter-
ministic in any direction: the only reason is that is it not possible to determine

Substitutions and Strongly Deterministic Tilesets
465
−→
−→
−→
Fig. 1. Substitution s and pattern of s3(τ)
whether the type of layer 2 H or V must be chosen when producing a tile of
parity ■□
□□.
Let us deﬁne an alphabet of labels L = {X, H, V} and label the cables of the
layer 2 by elements of ⊞× L (instead of ⊞). The local rule is extended so that
the labels of L are preserved (as the colors of ⊞) all along a same cable. Observe
that any tile t of parity ■□
□□is directly encircled by a cable on the layer 2 of
its eight neighbors. Let us enrich the local rules in respect to the four directly
neighboring tiles so that this label corresponds to the type of the tile t: if t is a
cross, the label must be X; if t is an horizontal bridge, the label must be H; if
t is a vertical bridge, the label must be V. Let us observe that the substitutive
structure of the tilings already forces every tile of parity ■□
□□directly encircled
by a cable of color □■
□□, □□
■□, □□
□■to be a horizontal bridge, vertical bridge, cross
respectively. Hence the added labeling is only necessary for cables of color ■□
□□as
the other couples color-label appearing in a tiling must always be (□■
□□, H), (□□
■□, V)
and (□□
□■, X).
Those new labels and local rules force the choice between types H and V on
the layer 2 of tiles of parity ■□
□□. However the production of the labels on cables
of color ■□
□□is not deterministic: e.g., when the considered determinism direction
is NE, the label on the cable at position ⌞of a tile of type X is not determined
when this cable is of color ■□
□□. That piece of information can only be obtained
back in the history of the coloring coded by the tiling. For this purpose, we
add some wires as depicted on the ﬁgure 2(a), carrying a label of L. Those new
wires must start on every corner of a square of color ■□
□□(ﬁgure 2(b)) and follow
cables of color □□
□■(ﬁgures 2(d) and 2(e)) until they reach an orthogonal cable of
superior level (ﬁgures 2(c) and 2(e)). Tiles receiving those new decorations are
hence perfectly identiﬁed. Associated local rules are: the label carried by a wire
must be preserved all along; on both ends of a wire, the label must corresponds
to the ones held by the cables to which it is connected.
The proof of the following result is mainly an exhaustive veriﬁcation.
Theorem 1. τ′ is an aperiodic strongly deterministic tileset.

466
B. Le Gloannec and N. Ollinger
(a)
(b) start
(c) end
(d) wire
(e) wire & end
Fig. 2. Pattern of a tiling by τ ′ (a) and tiles holding new wires
3
NE-Soﬁcity of Substitutions Limit Sets
Enforcing the Limit Set of a Substitution: The Enriched Tileset τ ′(s′).
Given a 2 × 2 substitution s′ over an alphabet Σ, we use the quaternary tree
structure constituted by the cables of color □□
□■and contained in every tiling, to
carry the history of a conﬁguration of the limit set of s′. For that purpose, every
cable of color □□
□■must carry a letter of Σ: those cables are hence labeled by
elements of ⊞× L × Σ (instead of ⊞× L). The added labels must verify certain
rules to enforce the tree to hold the hierarchy imposed by the substitution s′:
on V tiles where two cables of color □□
□■cross, the letters a ∈Σ carried by the
superior vertical cable and b ∈Σ carried by the inferior horizontal cable must
verify b = s′(a)(x, y) where x = 0 (resp. x = 1) if the superior □□
□■cable is on
right (resp. left) position and y = 0 (resp. y = 1) if the inferior □□
□■cable is on
bottom (resp. top) position; symmetrically, on H tiles where two cables of color
□□
□■cross, the letters a carried by the superior horizontal cable and b carried by
the inferior vertical cable must verify b = s′(a)(x, y) where y = 0 (resp. y = 1) if
the superior □□
□■cable is on top (resp. bottom) position and x = 0 (resp. x = 1)
if the inferior □□
□■cable is on left (resp. right) position; ﬁnally, on every cross X in
□□
□■position, the letters a carried by the □□
□■cable of the layer 2 and b carried by
the layer 1 must verify b = s′(a)(u) where u = ■□
□□(resp. □■
□□,□□
■□,□□
□■) if the □□
□■cable
is in position ⌞(resp. ⌟,⌜,⌝) in the cross. We also enrich the local rules to force
the four tiles of a same 2 × 2 parity block to share the same letter of Σ. For all
tiles t ∈τ′(s′) with layer 1 (u, a) ∈⊞× Σ, let us deﬁne π(t) = s′(a)(u).
Theorem 2 ([7]). π

Xτ ′(s′)

= Λs′, hence limit sets of 2 × 2 substitutions are
soﬁc.

Substitutions and Strongly Deterministic Tilesets
467
So that in every level of a tiling all cables symmetrically carry the hierarchical
information, we ultimately enrich τ′(s′) by adding letters of Σ on cables of colors
■□
□□, □■
□□and □□
■□and forcing every □□
□■cable to share its letter with the three other
cables of its parity block: on every tile (X included) where a cable of color ■□
□□
(resp. □□
■□) appears at the left of a □■
□□(resp. □□
□■) cable, they must share the same
letter of Σ; similarly, on every tile where a cable of color □□
■□(resp. □□
□■) appears
at the top of a ■□
□□(resp. □■
□□) cable, they must share the same letter of Σ. We
refer to these rules as sharing rules. That way, every level of the parity grid of
every tiling t by τ′(s′) codes a coloring of the history of π(t) where each letter
is carried by a parity block.
The NE-Deterministic Tileset τNE(s′). The tileset τ ′(s′) is not determin-
istic in any direction. The reason is that the letter of Σ carried by the corner
of a cable on a X tile cannot always be deterministically determined when its
position corresponds to the considered direction of determinism and no sharing
rule force this letter: e.g., for the determinism direction NE, the letters of ■□
□□
cables of position ⌞on X tiles are not determined. Note that this problem also
arises in the particular case of the level 0: e.g., for the determinism direction NE,
the letter of Σ held by the parity layer of any tile of parity ■□
□□cannot be obtained
deterministically. That piece of information can be obtained in the superior level
of the history which appears, according to the remark 1, to be translated in the
SW direction. In the following, we build a NE-deterministic tileset based on that
observation.
For that purpose, we must add a color of ⊞and a letter of Σ on some of
the wires added in the previous section: those that link the SW corner of a ■□
□□
cable to the closest superior level cable in the south or west direction, e.g., the
vertical wires running to the south depicted on the ﬁgure 3(a) (or we could have
chosen equivalently the corresponding horizontal wires running to the west).
Those wires are then labeled by elements of L × ⊞× Σ (instead of L). Tiles
holding a wire are represented on ﬁgure 3. On tiles carrying the end (ﬁgure 3(c))
of such a wire, the color and letter of the wire must be the same as the ones of
the cable to which it is connected. On tiles carrying the start (ﬁgure 3(b)) of
such a wire, the color u ∈⊞and letter a ∈Σ carried by the wire and the letter
b ∈Σ carried by the ■□
□□cable must verify b = s′(a)(u). Associated local rules
are: the labels carried by a wire must be preserved all along its propagation.
Solving the same problem at level 0, i.e., predicting the letter of the parity
layer of ■□
□□tiles, is easier. Remember that every ■□
□□tile is directly encircled by a
cable on its eight neighbors. Then simply add the following local rules: the letters
b ∈Σ of the parity layer of any ■□
□□tile and a ∈Σ of the direct encirclement
cable, of color u ∈⊞, in any of its four direct neighbors, must verify b = s′(a)(u).
τne(s′) is a NE-deterministic tileset. The proof of this result is an exhaustive
veriﬁcation. It should be quite clear as the enrichments were done in purpose.
The soﬁcity result is obviously still valid (for a naturally extended projection
π simply removing all additional decorations): we have π

Xτne(s′)

= Λs′. This
lead to the following result.
Theorem 3. Limit sets of 2 × 2 substitutions are NE-soﬁc.

468
B. Le Gloannec and N. Ollinger
(a)
(b) start
(c) end
(d) wire
Fig. 3. Pattern of a tiling by τne(s′) (a) and tiles holding wires
4
4-Way Soﬁcity of Substitutions Limit Sets
In this section, we enrich the NE-deterministic tileset τne(s′) into a strongly
deterministic tileset τ4w(s′).
The 4-Way Deterministic Tileset τ4w(s′). We have underlined in remark 1
the fact that the substitution s translates superior levels of the parity grid to-
wards the SW direction. Considering the three other symmetrical possible choices
for this substitution represented on the ﬁgure 4, we can similarly build tilesets
τnw(s′), τse(s′) τsw(s′) that are NW, SE and SW-deterministic respectively. To
build a strongly deterministic tileset τ4w(s′), let us consider the tileset consti-
tuted by the cartesian product τne(s′) × τnw(s′) × τse(s′) × τsw(s′) for which we
require the local rules to be veriﬁed on each of the components of the product.
For any tile t = (t1, t2, t3, t4), we moreover require the four components t1,. . . , t4
to share the same layer 1, i.e., same parity and same letter of Σ. That way, the
coloring coded by each of the components of a tiling T = (T1, T2, T3, T4) is the
same: π1(T1) = π2(T2) = π3(T3) = π4(T4) with πi the associated projections.
Each of the components of the product tileset is deterministic in one direction.
The idea is to use this component to make the three others deterministic in its
direction. We must then synchronize the histories coded by the four components
of a tiling so that they code the same coloring at every level. The synchronized
tileset should then be strongly deterministic.
Recall that the obstacle to determinism is that the letter of Σ carried by
the corner of a cable on a tile X cannot always be deterministically determined
when its position corresponds to the considered direction of determinism and no
sharing rule force this letter.
Analyzing the Case of Two Opposite Directions. Let us examine the case
of two components of opposite determinism directions. Without loss of gener-
ality, let us consider SW and NE directions and, in this paragraph only, the

Substitutions and Strongly Deterministic Tilesets
469
−→
(a) sne = s
−→
(b) snw
−→
(c) sse
−→
(d) ssw
Fig. 4. Four symmetrical substitutions
τne(s′) × τsw(s′) associated product (i.e., components 1 and 4 of the previously
introduced product) for which a tiling pattern is depicted of ﬁgure 5(a) (where
on each tile, the ﬁrst and second components are represented in the SW and NE
corners respectively, and colors of the second components are dark green, green,
orange, yellow for ■□
□□,□■
□□,□□
■□,□□
□■respectively). In any tiling, only the parity layers
of both components are actually synchronized so that they both code the same
coloring at level 0. But let us assume one moment that we have synchronized
both components on every level so that both components code exactly the same
history of that coloring. Let us now pick the determinism direction SW, and
analyze how the ﬁrst component can be made SW-deterministic using the sec-
ond component. By symmetry, the same analyze would make sense for the other
component in the opposite determinism direction. In our choice, for the deter-
minism direction SW on the ﬁrst component, the letters of □□
□■cables of position
⌝on X tiles are not determined. As illustrated by the scattered representations
of ﬁgures 5(c) and 5(d), we claim, when the tiling is fully synchronized, that the
letters of these cables are precisely those carried by the corresponding □□
□■cables
(in yellow) of the second component and same hierarchical level that appear on
neighboring tiles pointed by a vector (−1, −1), i.e., one diagonal shift away in
the SW direction. It is not convenient that this piece of information is available
one shift away in the considered determinism direction: it precisely arrives one
step later the position we need it. Nevertheless, observe that, at every level, that
shift is constant (−1, −1).
The case of two components of orthogonal determinism directions (e.g.,
τne(s′)×τnw(s′) depicted on ﬁgure 5(b)) is similar (required information available
one shift away).
Grouping Tiles. The analyses lead before show that the required pieces of
information to synchronize the histories of the four components and by this
way make the tileset strongly deterministic always are available one shift away,
unfortunately in the considered determinism direction, of the tile to predict.
A simple solution to this constant shift problem is a 3 × 3 grouping of tiles:
let τ0 be the previously deﬁned product tileset, let
denote the set {0, 1, 2}2,
τ1 ⊆τ0 denote the set of valid 3 × 3 patterns (in respect to τ0 local rules) over
τ0 and consider the grouped tileset (τ1, H1, V1) verifying, ∀(t, t′) ∈τ 2
1 , (t, t′) ∈
H1 ⇔∀y ∈{0, 1, 2}, t2,y = t′
0,y and (t, t′) ∈V1 ⇔∀x ∈{0, 1, 2}, tx,2 = t′
x,0.
We then obtain τ4w(s′) from τ1 by adding some synchronization requirements
on the tiles. Let us get back to the example used in the case of two opposite

470
B. Le Gloannec and N. Ollinger
(a) τne(s′) × τsw(s′)
(b) τne(s′) × τnw(s′)
(c) Cables of level 1 of τne(s′) × τsw(s′)
(d) Cables of level 2 of τne(s′) × τsw(s′)
Fig. 5. Scattered view of the cables of the two-components tilesets and views level-by-
level of the cables of τne(s′) × τsw(s′)
directions: on each tile t for which the central sub-tile t1,1 has a ﬁrst component
of type X with a □□
□■cable in ⌝position labeled by a letter a ∈Σ, there must be,
as t forms a 3 × 3 valid pattern for τ0, a yellow □□
□■cable going along the fourth
components of the sub-tiles t0,2, t0,1, t0,0, t1,0, t2,0 and, still for validity reasons,
carrying a same letter b ∈Σ. We require in that case, based on the previous
analysis, to have a = b. Considering all the symmetrical requirements and the
analog requirements for the orthogonal case, we obtain the τ4w(s′) tileset.
Results. τ4w(s′) is a strongly deterministic tileset. The proof of this result is an
exhaustive veriﬁcation. The cases examined before should convince the reader of

Substitutions and Strongly Deterministic Tilesets
471
its correctness. Again, the soﬁcity result is obviously still valid (for a naturally
extended projection π simply removing all additional decorations): we still have
π

Xτ4w(s′)

= Λs′. This lead to the ﬁnal result.
Theorem 4. Limit sets of 2 × 2 substitutions are 4-way soﬁc.
References
1. Kari, J.: The nilpotency problem of one-dimensional cellular automata. SIAM J.
Comput. 21(3), 571–586 (1992)
2. Kari, J., Papasoglu, P.: Deterministic aperiodic tile sets. Geometric and Functional
Analysis 9, 353–369 (1999)
3. Robinson, R.: Undecidability and nonperiodicity for tilings of the plane. Inven-
tiones Mathematicae 12(3), 177–209 (1971)
4. Lukkarila, V.: The 4-way deterministic tiling problem is undecidable. Theor. Com-
put. Sci. 410(16), 1516–1533 (2009)
5. Durand, B., Romashchenko, A., Shen, A.: Fixed Point and Aperiodic Tilings. In:
Ito, M., Toyama, M. (eds.) DLT 2008. LNCS, vol. 5257, pp. 276–288. Springer,
Heidelberg (2008)
6. Berger, R.: The undecidability of the domino problem. Memoirs American Math-
ematical Society 66 (1966)
7. Ollinger, N.: Two-by-Two Substitution Systems and the Undecidability of the
Domino Problem. In: Beckmann, A., Dimitracopoulos, C., L¨owe, B. (eds.) CiE
2008. LNCS, vol. 5028, pp. 476–485. Springer, Heidelberg (2008)
8. Mozes, S.: Tilings, substitution systems and dynamical systems generated by them.
Journal d’Analyse Math´ematique 53(1), 139–186 (1989)
9. Goodman-Strauss, C.: Matching rules and substitution tilings. Annals of Mathe-
matics 147(1), 181–223 (1998)
10. Fernique, T., Ollinger, N., TUCS: Combinatorial Substitutions and Soﬁc Tilings.
In: Proceedings of JAC 2010 Journ´ees Automates Cellulaires 2010, Turku Finland,
pp. 100–110 (2010)

The Computing Spacetime
Fotini Markopoulou
Perimeter Institute for Theoretical Physics, 31 Caroline St. N., Waterloo Ontario
N2L 2Y5, Canada
fotinimk@gmail.com
Abstract. The idea that the Universe is a program in a giant quantum
computer is both fascinating and suﬀers from various problems. Nonethe-
less, it can provide a uniﬁed picture of physics and this is very useful for
the problem of Quantum Gravity where such a uniﬁcation is necessary.
We give an introduction to the idea of the universe as a quantum com-
putation, the problem of Quantum Gravity, and Quantum Graphity, a
simple way to model a dynamical spacetime as a quantum computation.
1
Introduction
That the Universe can be thought of as a giant computation is a straightforward
corollary of the existence of a universal Turing machine. The basic idea (nicely
summarized by Deutsch in [9]) goes as follows. The laws of physics allow for
a machine, the universal Turing machine, such that its possible motions corre-
spond to all possible motions of all possible physical objects. That is, a universal
quantum computer can simulate every physical entity and its behavior. This
means that physics, the study of all possible physical systems, is isomorphic to
the study of all programs that could run on a universal quantum computer. We
can think of our universe as software running on a universal computer.
Should this logical inference aﬀect our understanding of physics, or even
change the way we do science? Several diﬀerent lines of thought say yes, an
idea most concretely articulated in the ﬁeld of cellular automata and quantum
information theory. In 1969, Konrad Zuse, in his book Calculating Space, pro-
posed that the physical laws of the universe are fundamentally discrete, and
that the entire universe is the output of a deterministic computation on a giant
cellular automaton [30]. Cellular automata (CA) are regular grids of cells, each
cell in one of a ﬁnite number of states, usually just black/white. An initial state
of the CA is updated in global discrete time steps, in which each cell’s new state
changes as a function of its old state and that of a small number of neighbors.
A concrete example of Zuse’s vision is Conway’s Game of Life. The rules are
simple: If a cell has 2 black neighbors, it stays the same; if it has 3 black neigh-
bors, it becomes black; otherwise it becomes white. The result is remarkably rich
behavior on the border between randomness and order. A striking feature is the
occurrence of gliders, small groups of cells that move like independent emergent
entities. We can arrange the automaton so that the gliders interact to perform
computations, and it can be shown that the Game of Life is a universal Turing
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 472–484, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

The Computing Spacetime
473
machine [3]. It is simple to see how this evokes the possibility that we live in
a giant CA [8]: In our CA Universe, what we think of as elementary particles
may just be emergent gliders. Since CAs exist that are Turing machines, it is
in principle possible to have any kind of glider behavior generated by a CA, in-
cluding gliders observing the laws of elementary particle physics. We don’t know
how these are generated because we have no access to the microscopic cells, so
we make physical theories about particle-like objects, but, in reality, we live in
a CA.
Quantum information theory has given a new and interesting twist on the
Universe as a Computation. Many practitioners in this ﬁeld argue that every-
thing fundamentally is information, an old idea that can be traced at least back
to Wheeler’s inﬂuential it from bit [29]. In that view, all interactions between
physical systems in the universe are instances of information processing. The
information involved in those processes is more primary than the physical sys-
tems themselves. Instead of thinking of particles as colliding, we should think
of the information content of the particles being involved in a computation. By
simple interpolation, the entire universe is nothing but a giant computation. As
Lloyd puts it in [21], the universe computes “its own dynamical evolution; as the
computation proceeds, reality unfolds”.
These are fascinating ideas when loosely interpreted, but with obvious prob-
lems, including:
1. What does it mean that information is more fundamental than its physical
instantiation?
2. Since any observation we can make, and anything physics describes, is just
the program, there is no way to know the hardware that runs that program.
The program can perhaps give us some hints as to what machine could eﬃ-
ciently run it, but at the end of the day this scenario assumes a fundamentally
unknowable machine.
3. Is that machine running just one program, our universe? If yes, how is that
“mother computation” chosen? If no, we need a meta-program that runs
multiple programs, a computer version of the multiverse idea [27]. By one
more iteration, multiple computers, each running multiple programs, are a
logical possibility, leading to an extreme form of a multiverse. Or are we
secretly assuming a Programmer?
4. The idea requires that all of physics is computable.
5. The CA Universe, in addition, advocates that the universe is fundamentally
discrete. Fundamental discreteness is a very old and attractive idea but it
remains to be seen whether it can be reconciled with observable physics, and,
in particular, with quantum mechanics and Lorentz invariance. Quantum
mechanics makes essential use of the complex numbers, a continuous ﬁeld.
It is, of course, logically possible to push fundamental discreteness to an
extremely small scale, perhaps the Planck scale, and claim that the world
appears continuous only by approximation, because we have no access to
that small scale. This is where Lorentz invariance comes in. The Lorentz
transformations form a non-compact group, meaning that by boosting an

474
F. Markopoulou
observer suﬃciently, we can blow up any tiny amount of discreteness to
arbitrarily large size. Depending on the details of the physics, even scales
smaller than the Planck scale can thus become observationally accessible.
Reconciling observational constraints on Lorentz invariance violations and
fundamental discreteness is a very active subject of research in quantum
gravity and quantum gravity phenomenology.1
At the end of the day, the Universe as a Computation idea may just reﬂect the
current way we understand and bring order to our surroundings. Maybe it just
shifts us a little from the “Blind Watchmaker” to the “Blind Programmer”. I
ﬁnd it very likely that the Universe as Computation is a culturally determined
and temporary idea. Fun as it may be to speculate about the universe being a
computer, it is rather sterile to do so in the abstract. The interesting question is
whether this scenario can be put to good uses: Does it give us useful new tools
and methods with which we can solve problems we couldn’t solve before? Does
it raise new interesting questions? The purpose of this article is to argue that
it does, and that the relevant area of physics to use the idea of the Universe
as a Computation is the ﬁeld of Quantum Gravity and Quantum Cosmology.
Quantum Gravity needs to unify quantum ﬁeld theory, the physics of matter,
with general relativity, the physics of spacetime, into a single consistent theory.
The universe as a Computation suggests a new kind of uniﬁcation: physical
systems and their dynamics can be represented in terms of their information
content and their dynamics is the processing of that information.
We shall illustrate this view with an example. In [16,12], we initiated a study
of quantum gravity using spin systems as toy models for emergent geometry
and gravity. These quantum graphity models are based on quantum networks
with no a priori geometric notions. We have repeatedly found the quantum
information perspective to be useful, both as a tool chest (for example, the Lieb-
Robinson speed of information propagation can be used to derive the speed of
light [20,23], or error correction to deﬁne conserved quantities [4]) and as an aid
to conceptual clarity: the information theoretic language allows us to do physics
without reference to a background geometry.
The purpose of this introductory article is to illustrate these ideas in a brief
and self-contained format and invite discussion and exchange of ideas between
the ﬁelds of quantum gravity and computer science. Technical details can be
found in the suggested references. In the next Section, we state the problem of
Quantum Gravity in terms of the breakdown of classical spacetime at Planck
scale and the problem of time. In Section 3, we summarize the basics of Quantum
Graphity, the representation of pre-geometry as qubits of adjacency, a sketch
of the derivation of the speed of light from the fundamental dynamics, the toy
trapped surfaces that arise in these systems, and the mechanism by which matter
inside that world sees an emergent curved geometry. We brieﬂy summarize our
conclusions in Section 4.
1 Cf., e.g., [14] and references therein.

The Computing Spacetime
475
2
The Problem of Quantum Gravity
The ﬁeld of Quantum Gravity is the attempt to unify General Relativity and
Quantum Theory. In spite of their impressive successes, the two theories leave us
with a gap in situations in which the quantum eﬀects of the gravitational ﬁeld
become important. This hampers our understanding of some of the most fasci-
nating modern physics, such as the physics of black holes [26], Hawking radiation
[28], and the very early universe,2 or leads to absurdities such as the cosmologi-
cal constant problem [7]. In all these situations, the curvature of spacetime is so
high we are not conﬁdent in the reliability of quantum ﬁeld theory.
The length scale where quantum gravitational eﬀects become signiﬁcant is,
by dimensional analysis, the Planck length, lPl =

GN¯h
c3 , the combination of
Newton’s constant GN, Planck’s constant ¯h, and the speed of light c. This is
incredibly small, lPl ∼10−35m, corresponding to energy scales of 1019GeV . At
Planck scale, the concepts of size and distance break down. Any microscopic
probe energetic enough to precisely measure a Planck-sized object needs to be
so energetic (to measure lP l, its Compton wavelength must be ∼lP l) that it
would completely distort the region of space it was supposed to measure. In this
sense, the notion of a classical spacetime manifold breaks down at Planck scale.
A quantum theory of gravity that reconciles general relativity and quantum
theory, or replaces them, is required to understand physics, including spacetime,
at that scale.
In spite of decades of research, a satisfactory quantum theory of gravity still
eludes us. Much of the diﬃculty comes from the fundamentally diﬀerent assump-
tions that these theories make on how the universe works. General relativity
describes spacetime as a manifold M with a dynamical metric ﬁeld gμν, and
gravity as the curvature that spacetime. Quantum ﬁeld theory describes particle
ﬁelds on a ﬂat and ﬁxed spacetime. Naive quantization of gravity, treating it as
another quantum ﬁeld, leads to nonsense as gravity is non-renormalizable. The
diﬀerence between the two theories can be phrased in terms of the way each
treats time. A fundamental lesson of general relativity is that there is no ﬁxed
spacetime background spacetime: geometry tells matter where to go and matter
tells geometry how to curve. The spacetime geometry is a dynamical ﬁeld. In
addition, physical quantities are invariant under diﬀeomorphisms of M. This
means (roughly) that general relativity is a relational theory, i.e., the only phys-
ically relevant information is the relationship between events in spacetime [2].
On the other hand, quantum theory requires a ﬁxed background spacetime, ei-
ther a Newtonian one (quantum mechanics), or a ﬁxed Minkowski spacetime
(quantum ﬁeld theory). Time in quantum theory is not a dynamical ﬁeld, it is
a background parameter.
Turning spacetime geometry into a quantum ﬁeld is possible and the task
of conservative approaches to quantum gravity such as Loop Quantum Gravity
[1]. The result, however, of such quantizations is peculiar. We obtain a so-called
wavefunction of the universe |ΨU⟩, i.e., the diﬀeomorphism invariant quantization
2 Cf., e.g., [5].

476
F. Markopoulou
of the metric gμν projected on a spatial slice of M. Instead of a Schr¨odiger
equation, the evolution of |ΨU⟩is governed by the Wheeler-deWitt equation:
H|ΨU⟩= 0,
(1)
where H is the quantization of the “projection” of the Einstein equations in the
direction normal to the spatial slice (for the actual details of this procedure, see,
for example, [25]). The Wheeler-deWitt equation is peculiar on two (related)
counts: it describes the evolution of the entire universe, not just a localized
system as in the Schr¨odinger equation, and the right hand side is zero (not time
evolution). That zero can be traced to the diﬀeomorphism invariance of general
relativity and the fact that the Einstein equations describe the dynamics of the
entire universe. The diﬀeomorphism symmetry gets mixed up with evolution in
ways that are very diﬃcult to untangle.3
Much more can be said about this, but the purpose of the present note is to
point out that, since quantum gravity needs to unify quantum theory and general
relativity, a uniﬁcation of the corresponding descriptions of the physical world is
required, and that quantum information theory can provide this. Reducing both
quantum ﬁelds and diﬀerential manifolds to their information theoretic content
can provide a common framework. The Universe as a Computation can, in that
sense, be seen as a useful and practical tool to solve a long-standing problem.
Note that we do not need to resolve whether information precedes its physical
instantiation, or answer most of the problems listed above in order to put this
notion to useful work. All we need is that an information theoretic description
is possible, both for the physics of matter and for the physics of space-time.
We have been pursuing this idea in the Quantum Graphity models for quantum
gravity and we shall give a concrete example of such a model in the next Section.
3
Quantum Graphity
Quantum Graphity models [16,12] are toy models for emergent geometry and
gravity. They are based on graphs whose adjacency is quantum and dynamical:
the edges can be on (connected), oﬀ(disconnected), or in a superposition of on
and oﬀ. We interpret the graph as pregeometry (the graph connectivity tells us
who is neighbouring whom). A graphity model is given by such graph states
evolving under a local Hamiltonian. The model of [12] which describe in the
rest of this section, is a toy model for interacting matter and geometry, a Bose-
Hubbard model where the interactions, or adjacencies, are quantum variables.
3.1
Qubits of Adjacency
Let us assume a universe consisting of N fundamental constituents, systems la-
beled by i = 1, ..., N. These are quantum mechanical, so we have {Hi}; i = 1, ..., N
3 For a classic review of the longstanding eﬀort to ﬁnd gravity’s true degrees of freedom
(metrics modulo diﬀeomorphisms), cf. [15].

The Computing Spacetime
477
Hibert spaces. Let KN denote the complete graph that has these N systems as its
vertices, a graph with N(N−1)
2
links e ≡(i, j). To every such e we associate a
Hilbert space He ≃C2 . Basis states on He can be labeled by |1⟩, |0⟩, and we
choose to interpret |1⟩as the link e being on, or present, and |0⟩as the link be-
ing oﬀ, or missing. The total Hilbert space corresponding to KN then is Hgraph =
N(N−1)/2
e=1
He.
Our choice of basis in He means that every basis state in Hgraph corresponds to
a subgraph of KN. A generic state |Ψgraph⟩∈Hgraph is a quantum superposition
of subgraphs of KN. For N very large, the state space contains superpositions
of all possible ﬁnite graphs. By analogy with the adjacency matrix of a graph,
we call He a qubit of adjacency. States in Hgraph then provide a simple discrete
precursor to quantum geometry. Note, however, that since we cannot assume a
pre-existing spacetime on which our N systems live, we cannot interpret the N
vertices of KN as points in that spacetime. That is, we do not have a discretiza-
tion of a geometry, the geometry corresponding to a state is to be inferred from
the behavior of matter interacting with |Ψgraph⟩.
To see how this works, let us next deﬁne a simple form of matter.
3.2
Interacting Matter and Geometry
We shall assign simple matter degrees of freedom to the vertices of KN by
assigning the Hilbert space Hi of a harmonic oscillator to each vertex i. We de-
note its creation and annihilation operators by ˆb†
i,ˆbi respectively, where ˆb†
i|0⟩i =
|1⟩i,ˆbi|1⟩i = |0⟩i, satisfying the usual bosonic relations, [ˆb†,ˆb†]= 0 =[ˆb,ˆb], [ˆb,ˆb†] =
1. Our N physical systems then are N bosonic particles and the total Hilbert
space for these bosons is given by Hbosons = N
i=1 Hi.
The total Hilbert space of the theory is the state space of the combined matter
and connectivity degrees of freedom, H = Hbosons ⊗Hgraph. A basis state in H
has the form |Ψ⟩≡|Ψbosons⟩⊗|Ψgraph⟩≡|n1, ..., nN⟩⊗|e1, ..., e N(N−1)
2
⟩. The
ﬁrst factor tells us how many bosons there are at every site i, while the second
factor tells us which pairs e interact. This is an unusual bosonic system, as the
structure of interactions is now promoted to a quantum degree of freedom.
This is interesting as a generic state can be a quantum superposition of “in-
teractions”. For example, for i and j in the state |φij⟩= (|10⟩⊗|1⟩ij + |10⟩⊗
|0⟩ij)/
√
2, means a particle in i and no particle in j, and a quantum superposi-
tion between i and j interacting or not. The state, |φij⟩= (|00⟩⊗|1⟩ij + |11⟩⊗
|0⟩ij)/
√
2, represents a diﬀerent superposition, in which the bosonic degrees of
freedom and the graph degrees of freedom are entangled. It is a signiﬁcant feature
of the model that matter can be entangled with geometry.
In [12], we proposed a simple Hamiltonian for the dynamics of the matter-
geometry interaction. If the bosons are not interacting, their total Hamiltonian is
trivial, ˆHv = N
i=1 ˆHi = −
i μˆb†
iˆbi. An interesting interaction term is hopping,
the physical process in which a quantum i is destroyed at i and one is created at
j. We shall require that hopping is possible only if there is an on edge between
i and j. Such dynamics is described by a Hamiltonian of the form

478
F. Markopoulou
ˆHhop = −Ehop

(i,j)
ˆPij ⊗(ˆb†
iˆbj + ˆbiˆb†
j),
(2)
where ˆPij = |1⟩⟨1|(i,j) is the projector on the edge (i, j) being in the on state.
This projector is important, it means that it is the dynamics of the particles
described by ˆHhop that gives to the link degrees of freedom the meaning of
geometry: the state of the graph determines where the matter is allowed to go.
In the spirit of “geometry tells matter where to go and matter tells geometry
how to curve”, we need interacting graph and matter. To avoid interpretational
problems, this interaction should be unitary. The simplest unitary interaction is
ˆHex = k

(i,j)
|0⟩⟨1|(i,j) ⊗(ˆb†
iˆb†
j)R + |1⟩⟨0|(i,j) ⊗(ˆbiˆbj)R.
(3)
This destroys an edge (i, j) and create R quanta at i and R quanta at j, or,
vice-versa, destroys R quanta at i and R quanta at j to convert them into an
edge. An example is shown in Figure 1. Of course, we need dynamics also for
the graph degrees of freedom alone. The simplest choice is simply to assign some
energy to every edge, ˆHlink = −U 
(i,j) σz
(i,j).
Fig. 1. Graph-matter dynamics: A link is is exchanged for two particles at its vertices;
the particles hop on existing links; two particles are destroyed and a link is created
between the corresponding vertices
This ﬁnal step brings us to the total Hamiltonian for the model proposed in
[12]:
ˆH = ˆHlink + ˆHv + ˆHex + ˆHhop.
(4)
It is possible to design such systems in the laboratory. For instance, one can use
arrays of Josephson junctions whose interaction is mediated by a quantum dot
with two levels.
This is a peculiar system in that who interacts with whom is a quantum degree
of freedom, but otherwise it is an extremely simple system. Does it lead to any
interesting behavior? Yes, more than one would expect, as we shall see next.
3.3
Calculating the Speed of Light as Propagation of Information
ˆHhop tells us that it takes a ﬁnite amount of time to go from i to j. If the graph
was a chain, it would take a ﬁnite amount of time (modulo exponential decaying
terms) for a particle to go from one end of the chain to another. This results

The Computing Spacetime
479
to a “spacetime” picture (the evolution of the adjacency graph in time) with a
ﬁnite lightcone structure. We can calculate this speed of light from the speed
with which information propagates using methods from quantum information
theory. From a local Hamiltonian, that is, a Hamiltonian that is the product of
local terms, H = 
⟨ij⟩hij, we can deﬁne the Lieb-Robinson speed of informa-
tion propagation [20,23] as follows. Consider two points P and Q on a lattice,
distance dP Q apart. A disturbance at P is felt at Q a time t later with strength
∥[OP (0), OQ(t)]∥, where OP (0) and OQ(t) are operators at P at time 0 and Q
at time t respectively. It is shown in [20,23] that this signal strength is bound by
∥[OP (0), OQ(t)]∥≤2∥OP ∥∥OQ∥

n
(2|t|hmax)2
n!
NP Q(n),
(5)
where hmax is the maximum coupling strength in the Hamiltonian and NP Q is
the number of paths of length n in the lattice that connect P and Q. This can
be rewritten as
∥[OP (0), OQ(t)]∥≤2∥OP ∥∥OQ∥C exp [−a (dP Q −vt)] .
(6)
Saturating the above inequality deﬁnes the maximum speed v of information
propagation in this system. Combining the two bounds allows us to calculate
this speed in terms of the couplings in the Hamiltonian and the connectivity of
the lattice.
In [11], we tested that v above can be the speed of light, by showing that in
string net condensation, a spin system whose emergent excitations are photons
[17,18,19], v agrees with the speed of light in the emergent Maxwell equations.4
This is an interesting result as it allows us to reconcile emergent ﬁnite light cones
and non-relativistic quantum mechanics. The underlying physics is, of course,
quantum mechanics, but, within the bounds deﬁned above, the system appears
to have a ﬁnite light cone. A signal is possible outside the light cone, but it
is exponentially suppressed. In fact, recent results show that in the continuum
limit the ﬁnite light cone becomes exact as that signal vanishes in the continuum
limit. Further work in [24] and results in [10] indicate that the emergence of a
Minkowski metric is a behavior that can be extended to inﬁnite-dimensional
systems, i.e., the physics we are studying is not limited to spin systems.
Note that this speed v increases with the number NP Q of paths connecting
P and Q, and therefore it is a function of the connectivity of the lattice. Higher
connectivity (vertex degree) means higher speed of light. This is used in what
follows to derive the eﬀective curved geometry matter sees.
3.4
Analogue Black Holes
One of the features of the above Hamiltonian acting on states which are not
degree-regular graphs, observed in [12], was the trapping of bosons into regions
4 Note, however, that this derivation does not assure us that this maximum speed is
universal for all species of matter.

480
F. Markopoulou
of high degree (cf. Fig. 2). The basic idea is the following: consider two sets of
nodes, A and B, separated by a set of points C on the boundary, and let the
vertices in A be of much higher degree than the vertices in B, dA ≫dB . If the
number of edges departing from the set C and going to the set A is much higher
than the number of edges going from C to B, then the hopping particles have a
high probability of being “trapped” in the region A.
Fig. 2. Toy black hole conﬁguration
A way to see the trapping is to study the Lieb-Robinson speed of particle
propagation. Since the speed of propagation of particle probability is degree
dependent, in the two regions we have two diﬀerent speeds. Their ratio depends
uniquely on the ratio of the degree of the two regions. Then, using the laws of
optics, probability is reﬂected at the boundary due to Snell’s law: for dB
dA ∼1
N →
0, and the region A acts as a trap.
This heuristic argument can be made precise. In [6], it was shown that matter
propagating on the graph state shown in Fig. 2 has a unique ground state which
is protected by a gap which increases linearly with the size N of the completely
connected region. That is, high connectivity conﬁgurations are spin-system ana-
logues of trapped surfaces.
3.5
Geometry: What the Matter Sees
While we can assign a metric to the graph itself, for the purposes of an analogue
model for quantum gravity, the relevant geometry is the one the metric sees. This
metric can be aﬀected both by the graph state and by the matter Hamiltonian.
Solving this problem is not easy. Our model may be simple conceptually, but
it is a Hubbard model on a dynamical irregular lattice and, as is well-known,
the Hubbard model beyond 1d quickly becomes intractable. Luckily, it turns out
that a large and interesting sector of our model can be reduced to an eﬀective
1d Hubbard model with modiﬁed couplings.
In order to do this, we restrict the time-dependent Schr¨odinger equation to
the manifold formed by the classical states, i.e., single-particle states with a well-
deﬁned but unknown position. The equation of motion obtained corresponds to
the equation of propagation of light in inhomogeneous media, similarly to black
hole analogue systems. Once we have such a wave equation, we can extract the

The Computing Spacetime
481
corresponding metric. This is a one-dimensional Hubbard model on a lattice with
variable vertex degree and multiple edges between the same two vertices. The
probability density for the matter obeys a (discrete) diﬀerential equation closed
in the classical regime. This is a wave equation in which the vertex degree is
related to the local speed of propagation of probability. This allows an interpre-
tation of the probability density of particles similar to that in analogue gravity
systems: the matter sees a curved spacetime. This establishes the desired rela-
tion between the connectivity of the graph and the curvature of its continuous
analogue geometry.
4
Discussion
We argued that the idea of the Universe as a Computation is useful because it
provides a route to a uniﬁcation of matter and geometry. In the above, however,
the Computation idea is a tool, not necessarily a fundamental ontology, and
our results do not imply or require it from bit. It simply helps to talk about
information as the primary object. It can be tempting to interpret this phrase as
meaning that bit pre-exists it. However, I must admit that I do not understand
what we can mean by information as decoupled from its physical realization. I
prefer to simply take advantage of the fact that it allows us to study a system
without having to specify the details of unknown physics.
In summary, it is possible to formulate Planck scale physics as a quantum
information processing system, as we demonstrated in previous work (Quantum
Causal Histories [22,13]). It is useful to formulate Planck scale physics as a quan-
tum information processing system because: 1. Quantum information provides
an unambiguous description of physics before geometry. 2. It is suitable for emer-
gence problems, just as classical information is useful for statistical physics. 3. It
provides a new toolbox well-adapted to background independent problems and
lets us import methods from statistical physics to a background independent
context. Information is useful because it allows us to study a system without
committing to a particular ontology, necessary when the ontology is ambiguous,
as is the case in emergence approaches to quantum gravity.
Even though I am not a believer in the full-blown Universe as a Computa-
tion philosophy, it is fun to explore some of the questions it raises from the
rather concrete viewpoint described above. We shall end with a sample of such
questions.
Information as uniﬁcation. The old version of uniﬁcation is the picture of
group theory and symmetry breaking and the convergence of fundamental cou-
plings. While this is now outdated, some level of uniﬁcation is needed for quan-
tum matter to interact with dynamical spacetime, as the language clash between
diﬀerentiable manifolds and quantum ﬁelds on a ﬁxed background has long been
an obstacle to quantum gravity. The idea that information underlies everything
allows a new path: express both gravity and matter in information theoretic
terms. Quantum graphity models are a ﬁrst step in that direction. It is a long
way to go but we are catching a promising glimpse of a novel form of uniﬁcation.

482
F. Markopoulou
Why is the universe so stable? If the universe is a computer program, how
come it doesn’t crash, or at least it hasn’t crashed yet? This sounds like a joke,
but it is a relevant question in cosmology: is our universe a stable attractor, and if
so why? It is interesting to look for potential commonalities between mechanisms
for stability in computers and in physics. In computers, stability comes from some
kind of built-in redundancy that provides error correction. In physics, certain
symmetries can be seen as a kind of error correction. Elsewhere, we noted that
the notion of decoherence-free subsystems used in quantum computing to protect
against noise and errors is very similar to the notion of conserved quantities,
something we used in [4] to ﬁnd a large class of conserved quantities in Loop
Quantum Gravity. I believe these results are only just scratching the surface.
How are the physics laws/computer program selected? Why our universe is
what it is is a perennial problem in quantum gravity and cosmology. In the
Universe as a Computation scenario it directly translates to the question of
how the Program is selected, and this new viewpoint brings new possibilities.
There are four commonly given answers: 1. Anthropic arguments: by construc-
tion, the universe we observe has to be compatible with the conscious life that
observes it, hence it is unremarkable that the fundamental constants happen to
fall within the narrow range that allows life. This is currently a very popular
idea, supported by logic and possibly inﬂation and string theory, but also widely
criticized as unscientiﬁc and non-explanatory. 2. Our laws have evolved through
the history of the universe. This generally leads to meta-laws selecting the laws
and a resulting circular argument. 3. Multiverse: our universe is one of many
physically realized universes. The many can be arranged in various ways which
have been thoroughly classiﬁed by Tegmark [27]. Unlike the anthropic argument,
this scenario is wider, and, in some forms, in principle testable. But there is a
huge proliferation of potential universes, not just those we can generate by allow-
ing the fundamental constants to take other values (the usual multiverse), but
also all possible laws or programs. 4. Ideas of self-organized criticality (SOC):
our universe is a stable attractor. One may think that this should be the most
promising direction, however, such ideas have hardly been explored. To a great
extend, there is a serious technical obstacle. SOC is typically observed in non-
equilibrium systems, while all of fundamental physics uses equilibrium quantum
ﬁeld theory. Properly introducing SOC ideas in cosmology requires a departure
from the standard framework. Since many of the results in this area are already
expressed in algorithmic terms, a description of the Universe as a computation
can make it easier to introduce SOC ideas to a (quantum) cosmological setting.
It will, of course, be necessary to study quantum systems that exhibit SOC ﬁrst.
This is a fascinating long-term direction for this kind of work.
Acknowledgements. This work was supported by the Alexander von Hum-
boldt and the Templeton Foundations.

The Computing Spacetime
483
References
1. Ashtekar, A., Lewandowski, J.: Background independent quantum gravity: A status
report, Class. Quant. Grav. 21, R53 (2004)
2. Ashtekar, A., Stachel, J.: Conceptual Problems of Quantum Gravity. Springer
(1991)
3. Berlekamp, E.R., Conway, J.H., Guy, R.K.: Winning Ways for your Mathematical
Plays. AK Peters Ltd. (2001)
4. Bilson-Thompson, S.O., Markopoulou, F., Smolin, L.: Quantum Gravity and the
Standard Model. Class. Quant. Grav. 24, 3975 (2005)
5. Brandenberger,
R.:
Inﬂationary
Cosmology:
Progress
and
Problems,
hep-
ph/9910410
6. Caravelli, F., Hamma, A., Markopoulou, F., Riera, A.: Trapped surfaces and emer-
gent curved space in the Bose-Hubbard model. Phys. Rev. D, arxiv:1108 (to appear,
2013)
7. Carroll, S.: The Cosmological Constant, astro-ph/0004075
8. Dennett, D.C.: Consciousness explained. Back Bay Books, Boston (2001)
9. Deutsch, D.: Physics, Philosophy and Quantum Technology. In: Shapiro, J.H.,
Hirota, O. (eds.) The Sixth International Conference on Quantum Communica-
tion, Measurement and Computing. Rinton Press, Princeton (2003)
10. Eisert, J., Osborne, T.J.: General Entanglement Scaling Laws from Time Evolu-
tion. Phys. Rev. Lett. 97, 150404 (2006)
11. Hamma, A., Markopoulou, F., Premont-Schwarz, I., Severini, S.: Lieb-Robinson
bounds and the speed of light from topological order. Phys. Rev. Lett. 102, 017204,
arXiv:0808.2495v2 [quant-ph]
12. Hamma, A., Markopoulou, F., Lloyd, S., Caravelli, F., Severini, S., Markstrom, K.:
A quantum Bose-Hubbard model with evolving graph as toy model for emergent
spacetime. Phys. Rev. D 81, 104032 (2010)
13. Hawkins, E., Markopoulou, F., Sahlmann, H.: Algebraic Causal Histories. Class.
Q. Grav. 20, 3839 (2003)
14. Hossenfelder, S.: Experimental Search for Quantum Gravity, arxiv:1010.3420
15. Isham, C.J.: Prima Facie Questions in Quantum Gravity, gr-qc/9310031
16. Konopka, T., Markopoulou, F., Severini, S.: Quantum Graphity: a model of emer-
gent locality. Phys. Rev. D 77, 104029 (2008)
17. Levin, M., Wen, X.G.: Fermions, strings, and gauge ﬁelds in lattice spin models.
Phys. Rev. B 67, 245316 (2003)
18. Levin, M.A., Wen, X.G.: String-net condensation: A physical mechanism for topo-
logical phases. Phys. Rev. B 71, 045110 (2005)
19. Levin, M., Wen, X.G.: Quantum ether: Photons and electrons from a rotor model.
arXiv:hep-th/0507118
20. Lieb, E.H., Robinson, D.W.: The ﬁnite group velocity of quantum spin systems.
Commun. Math. Phys. 28, 251–257 (1972)
21. Lloyd, S.: Programming the Universe: A Quantum Computer Scientist Takes On
the Cosmos, Knopf (2006)
22. Markopoulou, F.: Quantum Causal Histories. Class. Q. Grav. 17, 2059 (2000)
23. Nachtergaele, B., Sims, R.: Lieb-Robinson Bounds in Quantum Many-Body
Physics. In: Sims, R., Ueltschi, D. (eds.) Entropy and the Quantum. Contemporary
Mathematics, vol. 529, pp. 141–176. American Mathematical Society (2010)
24. Pr´emont-Schwarz,
I.,
Hamma,
A.,
Klich,
I.,
Markopoulou-Kalamara,
F.:
Lieb-Robinson bounds for commutator-bounded operators, arXiv:0912.4544v1
[quant-ph]

484
F. Markopoulou
25. Rovelli, C.: Quantum Gravity. Cambridge U. Press, New York (2004)
26. Susskind, L.: The Black Hole War, Little, Brown (2008)
27. Tegmark, M.: The Multiverse Hierarchy, arxiv:0905.1283
28. Wald, R.M.: The thermodynamics of black holes, gr-qc/9912119
29. Wheeler, J.A., Ford, K.: Geons, black holes and quantum foam: a life in physics.
W.W. Norton Company, Inc., New York (1998)
30. Zuse, K.: Rechnender Raum. Elektronische Datenverarbeitung 8, 336–344 (1967)

Uniﬁability and Admissibility in Finite Algebras
George Metcalfe and Christoph R¨othlisberger
Mathematics Institute, University of Bern, Sidlerstrasse 5, 3012 Bern, Switzerland
{george.metcalfe,christoph.roethlisberger}@math.unibe.ch
Abstract. Uniﬁability of ﬁnite sets of equations and admissibility of
quasiequations in ﬁnite algebras are decidable problems, but a naive ap-
proach is computationally unfeasible even for small algebras. Algorithms
are given here for obtaining more eﬃcient proof systems deciding these
problems, and some applications of the algorithms are described.
1
Introduction
Checking validity in ﬁnite algebras (similarly, derivability in ﬁnite-valued logics)
has been studied extensively in the literature, and may be considered a “solved
problem” in the sense that there exist both general methods for obtaining proof
systems for checking validity (tableaux, resolution, multisequents, etc.) and stan-
dard optimization techniques for such systems (lemma generation, indexing, etc.)
(cf., e.g., [14, 26, 3]). In this paper, we consider the related problems of checking
the uniﬁability of ﬁnite sets of equations and admissibility of quasiequations in
ﬁnite algebras. These problems are decidable (cf. [22]); however, a naive approach
leads to computationally unfeasible procedures even for small algebras.
Let us ﬁx a class of algebras K in the same language, noting that we will
usually be concerned here with the case where K contains one ﬁnite algebra A.
A uniﬁer in K for a ﬁnite set of equations Σ (in the same language) is a substi-
tution σ such that for each ϕ ≈ψ in Σ, the equation σ(ϕ) ≈σ(ψ) holds in all
members of K. The (equational) uniﬁability problem for K consists of checking
whether or not a given ﬁnite set of equations Σ has a uniﬁer in K. A more chal-
lenging problem, not considered in this work, is to ﬁnd “most general bases” for
uniﬁers in K (i.e., such that each uniﬁer of Σ in K is obtained from a member of
the basis by applying a further substitution), and to determine the uniﬁcation
type (unary, ﬁnitary, inﬁnitary, or nullary) of K (cf., e.g., [2, 9, 10]). Uniﬁability
allowing additional free constants may also be investigated (cf., e.g., [2, 24, 1]).
If the language of K contains constants, then checking uniﬁability in K amounts
to checking satisﬁability in the corresponding ground algebra: namely, FK(0), the
free algebra of K on zero generators. If this algebra is ﬁnite (e.g., if K generates
a locally ﬁnite variety), then it can be used to check uniﬁability, employing tools
such as MUltlog/MUltseq [23, 12] or 3TAP [5] to obtain a corresponding proof
system. If the language of K lacks a constant, then uniﬁability in K amounts to
satisﬁability in any subalgebra of FK(1), the free algebra of K on one generator.
Again, if FK(1) is ﬁnite (guaranteed if K generates a locally ﬁnite variety), then
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 485–495, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

486
G. Metcalfe and C. R¨othlisberger
using a tool such as the Algebra Workbench [25], the smallest such subalgebra
can be found and a corresponding proof system obtained.
Checking the admissibility of a rule or quasiequation is more challenging.
Intuitively, a rule is admissible in a logical system if it can be added to the system
without producing any new theorems. Algebraically, a quasiequation Σ ⇒ϕ ≈ψ
is admissible in K if every uniﬁer of Σ in K is also a uniﬁer of ϕ ≈ψ in K.
Equivalently, Σ ⇒ϕ ≈ψ is admissible if and only if (henceforth iﬀ) it holds
in FK(ω), the free algebra on countably inﬁnitely many generators of K. Note
that Σ is uniﬁable in K iﬀΣ ⇒p ≈q is not admissible in K where p, q are any
distinct variables not occurring in Σ. Admissibility (in tandem with uniﬁcation)
has been studied intensively in the context of intermediate and transitive modal
logics and their algebras [22, 15, 9, 10, 17, 8], leading also to proof systems
for checking admissibility [11, 16, 4], and certain many-valued logics and their
algebras [22, 7, 18–20], but a general theory for this latter case has so far been
lacking.
If K contains a single algebra A with n elements, then admissibility amounts
to holding in the ﬁnite free algebra FA(n) and is decidable [22]. However, even
for small n, the size of FA(n) may be prohibitively large. This is particularly
striking when compared with the fact that derivability and admissibility in K
may coincide; that is, K may be structurally complete (cf., e.g., [22, 7]). In other
cases, admissibility of a quasiequation corresponds to holding in other, often
quite small, algebras (cf., e.g., [20]). An algorithm is provided in this paper for
discovering such algebras. Namely, we show that admissibility in A corresponds
to holding in any subalgebra B of FA(n) where A is a homomorphic image of B.
Since tools such as the Algebra Workbench [25] allow us to calculate subalgebras
and check homomorphic images, we obtain a procedure that is feasible even
when the size of the free algebras is relatively large. We illustrate our procedure
using a selection of well-known ﬁnite algebras, conﬁrming some known results
from the literature, and establishing new ones. We also investigate a situation
where the smallest algebra for checking admissibility may not be the best choice,
larger algebras with the structure of a product sometimes being more suitable
for automated reasoning methods. Finally, we address, via an example, the issue
that subalgebras of the free algebra FA(n) may not always give the best result,
since there may exist smaller suitable subalgebras of a product of FA(n).
Uniﬁability and admissibility play a fundamental meta-level role in describing
“hidden properties” of (classes of) algebras and logics. In particular, establishing
the completeness of a logic or proof system with respect to some restricted class
of algebras (perhaps just one standard algebra) often involves showing that a
certain rule or quasiequation is admissible. Similarly, deciding uniﬁability of
concepts can be a useful tool for database redundancy checking in description
logics [1]. Finally, a longer term goal of the current work is to automatically
obtain admissible rules for ﬁnite algebras and logics that can be used to simplify
reasoning steps or to speed up derivations for checking validity.

Uniﬁability and Admissibility in Finite Algebras
487
2
Preliminaries
Let us ﬁrst recall some ideas from universal algebra, referring to [6, 13] for further
details. For a language L, we denote the formula algebra over countably inﬁnitely
many variables by FmL and let ϕ, ψ stand for arbitrary members of the universe
FmL called L-formulas. An L-equation is a pair of L-formulas, written ϕ ≈ψ.
An L-clause is an ordered pair of ﬁnite sets of L-equations, written Σ ⇒Δ,
called an L-quasiequation if |Δ| = 1 and an L-negative clause if Δ = ∅. As usual,
if the language is clear from the context we may omit the preﬁx L.
Let us ﬁx K to be a class of L-algebras, noting that in this paper K will often
consist of a single ﬁnite L-algebra A, which we typically write without brackets.
A set Σ of L-equations is satisﬁable in an L-algebra A if Σ ⊆ker h for some
homomorphism h: FmL →A. We write Σ |=K Δ and say, if Σ ∪Δ is ﬁnite,
that the L-clause Σ ⇒Δ “holds in K” if for every A ∈K and homomorphism
h: FmL →A, Σ ⊆ker h implies Δ ∩ker h ̸= ∅.
K is said to be axiomatized by a set of L-clauses Λ if K is the class of L-algebras
A such that all L-clauses in Λ hold in A. K is called a variety, quasivariety, or
antivariety if it is axiomatized by a set of L-equations, L-quasiequations, or L-
negative clauses, respectively. The variety V(K), quasivariety Q(K), and antiva-
riety V−(K) generated by K are the smallest variety, quasivariety, and antivariety
containing K, respectively. Let H, I, S, P, PU, P∗
U, and H−1 be, respectively, the
class operators of taking homomorphic images, isomorphic images, subalgebras,
products, ultraproducts, non-empty ultraproducts, and homomorphic preim-
ages. Then V(K) = HSP(K), Q(K) = ISPPU(K), and V−(K) = H−1SP∗
U(K);
for a ﬁnite algebra A these latter equivalences reﬁne to Q(A) = ISP(A) and
V−(A) = H−1S(A) (cf. [6, 13] for details).
For a cardinal κ, let FK(κ) denote the free algebra of K with κ generators,
recalling that V(K) = V(FK(ω)) and that V(K1) = V(K2) implies FK1(ω) =
FK2(ω) (cf. [6]). Note that when considering uniﬁability and admissibility, it
can be helpful to view the elements of FK(κ) for κ ≤ω as equivalence classes
[ϕ] of formulas ϕ containing at most κ variables, deﬁned with respect to the
congruence relating ϕ and ψ whenever |=K ϕ ≈ψ.
3
Uniﬁability
As a warm-up for the harder case of admissibility, let us ﬁrst consider the problem
of checking whether a set of L-equations Σ is uniﬁable in a class K of L-algebras.
Formally, a homomorphism (substitution) σ: FmL →FmL is called a K-uniﬁer
for Σ if |=K σ(ϕ) ≈σ(ψ) for every (ϕ ≈ψ) ∈Σ. If such a homomorphism exists,
then Σ is called K-uniﬁable. In this paper, we shall be particularly interested
in the case where Σ is ﬁnite and K consists of just one ﬁnite algebra (or is
at least a locally ﬁnite variety). Our goal will be to ﬁnd for an arbitrary ﬁnite
algebra A, another algebra B such that checking uniﬁability in A corresponds to
checking satisﬁability in B. Standard tools for ﬁnite algebras and logics such as
MUltlog/MUltseq [23, 12] or 3TAP [5] may then be used to automatically obtain
a proof system for this latter task.

488
G. Metcalfe and C. R¨othlisberger
Let us ﬁx K to be a class of L-algebras and Σ to be a set of L-equations.
Observe ﬁrst that Σ is K-uniﬁable iﬀthere is a K-uniﬁer for Σ mapping each
formula to a formula with at most one variable. Simply note that if σ is a K-
uniﬁer for Σ, then also ˜σ ◦σ is a K-uniﬁer for Σ where ˜σ maps every variable
to a ﬁxed variable p. In fact ˜σ ◦σ is a K-uniﬁer for Σ for any substitution ˜σ.
In particular, if L contains at least one constant, then we can map formulas
to ground formulas. Building on this simple observation, the following lemmas
establish that checking K-uniﬁability amounts to checking satisﬁability in any
subalgebra of FK(1), and, moreover, that smallest ﬁnite subalgebras of FK(1)
are the smallest algebras for which K-uniﬁability corresponds to satisﬁability.
Lemma 1. The following are equivalent for any B ∈IS(FK(1)):
(1) Σ is K-uniﬁable.
(2) Σ is satisﬁable in FK(1).
(3) Σ is satisﬁable in B.
Proof. (1) ⇒(2). If Σ is K-uniﬁable, then there is a K-uniﬁer σ for Σ mapping
each formula to a formula with at most one variable. It follows that Σ is satisﬁed
in FK(1) by the unique homomorphism h: FmL →FK(1) mapping each variable
p to the equivalence class of σ(p) in FK(1).
(2) ⇒(3). Suppose that Σ ⊆ker h for some h: FmL →FK(1). Deﬁne a
homomorphism k: FK(1) →B by k(x) = a for some a ∈B where x is the single
generator of FK(1). Then Σ ⊆ker(k ◦h) as required.
(3) ⇒(1). If Σ ⊆ker h for some h: FmL →B and e embeds B into FK(1),
then any substitution mapping p to a member of the equivalence class e(h(p))
is a K-uniﬁer of Σ.
⊓⊔
Proposition 1. The following are equivalent for any B ∈K:
(1) Σ is K-uniﬁable iﬀΣ is satisﬁable in B.
(2) V−(FK(1)) = V−(B).
Moreover, if C is a smallest ﬁnite subalgebra of FK(1), then |C| ≤|B| for any
B ∈K satisfying (1) and (2).
Proof. Note ﬁrst that V−(FK(1)) = V−(B) iﬀthe same negative clauses Σ ⇒∅
hold in FK(1) and B. But Σ ⇒∅holds in FK(1) iﬀΣ is not satisﬁable in
FK(1) iﬀ, by Lemma 1, Σ is not K-uniﬁable. Moreover, Σ ⇒∅holds in B
iﬀΣ is not satisﬁable in B. Hence V−(FK(1)) = V−(B) iﬀwhenever Σ is K-
uniﬁable, then Σ is satisﬁable in B, and vice versa. Suppose now that C is
a smallest ﬁnite subalgebra of FK(1). By Lemma 1, C satisﬁes (1) and hence
(2); i.e., V−(FK(1)) = V−(C). So for any B ∈K satisfying (1) and (2), B ∈
V−(FK(1)) = V−(C) = H−1S(C) = H−1(C), and it follows that |C| ≤|B|.
⊓⊔
Hence we obtain an (in some sense) optimal proof system for checking uniﬁa-
bility in a given ﬁnite algebra by (i) calculating the ﬁnite algebra FA(1), and
(ii) calculating a smallest subalgebra B of FA(1) (e.g., using the Algebra Work-
bench [25]), and then (iii) deriving a proof system for checking satisﬁability in
B (e.g., using MUltlog/MUltseq [23, 12] or 3TAP [5]).

Uniﬁability and Admissibility in Finite Algebras
489
Example 1. Consider the 4-element algebra D4 = ⟨{⊥, a, b, ⊤}, ∧, ∨, ¬, ⊥, ⊤⟩
(noting that Q(D4) is the variety of De Morgan algebras) consisting of a dis-
tributive bounded lattice with an involutive negation deﬁned as shown below:
    ⊥
a
b
⊤
Since there are constants in the language of D4, the smallest algebra for checking
uniﬁability is the ground algebra FD4(0) with elements associated to ⊥and ⊤.
That is, checking uniﬁability amounts to checking classical satisﬁability. E.g.,
p∧¬p ≈p∨¬p is not uniﬁable in D4, since ⊤∧¬⊤̸= ⊤∨¬⊤and ⊥∧¬⊥̸= ⊥∨¬⊥.
Moreover, although the constant-free case of DL
4 = ⟨{⊥, a, b, ⊤}, ∧, ∨, ¬⟩(which
generates the variety of De Morgan lattices) is not so immediate, there exists a
smallest subalgebra of FDL
4 (1) with elements corresponding to p∧¬p and p∨¬p.
So checking uniﬁability in DL
4 amounts again to checking classical satisﬁability.
K need not consist of just one or ﬁnitely many ﬁnite algebras. It is enough that
the FK(0) algebra, if it exists, or otherwise the FK(1) algebra, is ﬁnite. This is
the case for any locally ﬁnite (i.e., ﬁnitely generated algebras are ﬁnite) variety.
Example 2. Consider the (locally ﬁnite) variety of bounded positive Sugihara
monoids studied in [21]: algebras ⟨A, ∧, ∨, ·, →, t, ⊥, ⊤⟩where ⟨A, ∧, ∨, ⊥, ⊤⟩is
a bounded lattice, ⟨A, ·, t⟩is a commutative idempotent monoid, and →is the
residual of · (i.e., x ≤y →z iﬀx · y ≤z) satisfying t ≤(x →y) ∨(y →x) and
[(((x →y) →y) →x) →z] · [(((y →x) →x) →y) →z] ≤z. The free algebra
of this variety on zero generators has 6 elements: the equivalence classes of t,
⊤, ⊥, ⊤→t, (⊤→t) →⊥, and ((⊤→t) →⊥) ∧t. Hence uniﬁcation in the
variety can be decided using a proof system for satisﬁability in this algebra.
Example 3. Even if the free algebra on zero generators is inﬁnite, it may have
a manageable structure. Consider the variety of bounded distributive lattices
with a unary operator □satisfying □(x ∧y) ≈□x ∧□y and □⊤≈⊤. The free
algebra on zero generators may be represented as a chain ⊥< □⊥< □□⊥<
. . . < ⊤isomorphic to ⟨N ∪{∞}, min, max, s⟩where s(x) = x + 1. Satisﬁability
in this algebra and hence also checking uniﬁability in the variety is decidable.
Note, however, that adding Boolean negation to the language gives algebras for
the modal logic K; decidability of uniﬁcation in this variety is one of the most
challenging open problems in the area (cf., e.g., [17, 1]).
4
Admissibility
An L-quasiequation Σ ⇒ϕ ≈ψ is said to be admissible in a class of L-algebras K
if every K-uniﬁer of Σ is a K-uniﬁer of ϕ ≈ψ. K is called structurally complete

490
G. Metcalfe and C. R¨othlisberger
if admissibility and validity coincide: i.e., Σ ⇒ϕ ≈ψ is admissible in K iﬀ
Σ |=K ϕ ≈ψ. Admissibility generalizes uniﬁability in the following sense: a
ﬁnite set of L-equations Σ is uniﬁable in a non-trivial class K of L-algebras iﬀ
the quasiequation Σ ⇒p ≈q with p and q not occurring in Σ is not admissible.
Quasiequations admissible in K are, equivalently, quasiequations that hold in
FK(ω), the free algebra on countably inﬁnitely many generators of K.
Lemma 2 ([20]). For a class of L-algebras K:
(a) Σ ⇒ϕ ≈ψ is admissible in K iﬀΣ |=FK(ω) ϕ ≈ψ.
(b) K is structurally complete iﬀQ(K) = Q(FK(ω)).
Decidability of admissibility in ﬁnite algebras is guaranteed by the following:
Lemma 3 ([22]). Let A be an n-element algebra. Then FA(m) is ﬁnite for all
m ∈N and Q(FA(ω)) = Q(FA(n)).
However, even free algebras on a small number of generators can be very large.
E.g., the free algebra FD4(2) (cf. Example 1) has 168 elements. We therefore
seek smaller algebras that also generate Q(FA(ω)) as a quasivariety.
Proposition 2. Let A, B be L-algebras such that B ∈Q(FA(ω)), and A ∈V(B)
(in particular, if B ∈S(FA(ω)) and A ∈H(B)). Then
(a) Q(B) = Q(FA(ω)).
(b) Σ ⇒ϕ ≈ψ is admissible in A iﬀΣ |=B ϕ ≈ψ.
Proof. (a) Since B ∈Q(FA(ω)), immediately Q(B) ⊆Q(FA(ω)). For the other
direction, note that V(A) ⊆V(B) ⊆V(Q(FA(ω))) = V(FA(ω)) = V(A); i.e.
V(A) = V(B). Hence FA(ω) = FB(ω) ∈Q(B) and Q(FA(ω)) ⊆Q(B).
(b) follows directly from (a) using Lemma 2.
⊓⊔
This suggests the following procedure when A is ﬁnite:
(i) Find the smallest free algebra FA(m) (m ≤|A|) such that A ∈H(FA(m)).
(ii) Compute the set Sub(FA(m)) of subalgebras of FA(m).
(iii) Construct the set Adm(A) of all B ∈Sub(FA(m)) such that A ∈H(B).
(iv) Derive a proof system for checking satisﬁability in a smallest B ∈Adm(A).
Steps (i)-(iii) of the procedure have been implemented using macros implemented
for the Algebra Workbench [25]. Step (iv) can be implemented directly making
use of a system such as MUltlog/MUltseq [23, 12] or 3TAP [5].
Example 4. Consider the algebra S→¬
3
= ⟨{−1, 0, 1}, →, ¬⟩with operations
→
-1
0
1
¬
-1
1
1
1
-1
1
0
-1
0
1
0
0
1
-1
-1
1
1
-1
noting that an equation of the form ϕ ≈ϕ →ϕ holds in S→¬
3
iﬀϕ is a theorem of
the {→, ¬}-fragment of the logic RM. Now, following our procedure, we obtain:

Uniﬁability and Admissibility in Finite Algebras
491
(i) S→¬
3
̸∈
H(FS→¬
3
(1)), but S→¬
3
∈
H(FS→¬
3
(2)), so Q(FS→¬
3
(ω))
=
Q(FS→¬
3
(2)).
(ii) FS→¬
3
(2) has 264 elements and Sub(FS→¬
3
(2)) contains its 5134 subalgebras.
(iii) Adm(S→¬
3
) contains 989 subalgebras B of FS→¬
3
(2) such that S→¬
3
∈H(B).
(iv) The two smallest algebras in Adm(S→¬
3
) have 6 elements; e.g.,
B = ⟨{[p →p], [¬(p →p)], [(p →q) →(p →q)], [(q →q) →(p →p)],
[¬((p →q) →(p →q))], [¬((q →q) →(p →p))]}, →, ¬⟩.
Example 5. In some cases, it is possible to establish structural completeness
results for the algebra A (equivalently, the quasivariety generated by A) using
the described procedure. The smallest B ∈Adm(A) may be an isomorphic copy
of A itself; i.e., A can be embedded into FA(m) for some m. In particular, known
structural completeness results have been conﬁrmed for
L→
3 = ⟨{0, 1
2, 1}, →L⟩
the 3-element Komori C-algebra
B1 = ⟨{0, 1
2, 1}, min, max, ¬G⟩
the 3-element Stone algebra
G3 = ⟨{0, 1
2, 1}, min, max, →G⟩
the 3-element positive G¨odel algebra
S→
3 = ⟨{−1, 0, 1}, →S⟩
the 3-element implicational Sugihara monoid
where x →L y = min(1, 1 −x + y), x →G y is y if x > y, otherwise 1,
¬Gx = x →G 0, and →S is the operation →of S→¬
3
from Example 4. A new
structural completeness result has also been established for the pseudocomple-
mented distributive lattice B2 obtained by adding a top element to the 4-element
Boolean algebra. Note, however, that the procedure timed out for the case of
the 9-element algebra B3.
Example 6. Recall from Example 1 that the algebras D4 and DL
4 generate the
varieties of De Morgan algebras and De Morgan lattices, respectively. In both
cases, the algebras D4 and DL
4 are homomorphic images of the corresponding
free algebras on two generators (with 168 and 166 elements, respectively) but
not on one generator. The smallest algebra found automatically in Adm(D4) is
isomorphic to the product D4 × 2 where 2 is the 2-element Boolean algebra,
while the smallest algebra in Adm(DL
4) is isomorphic to D4 × 2 with extra top
and bottom elements. The fact that these smallest algebras are themselves struc-
turally complete and generate the varieties of De Morgan algebras and lattices
was established in [20]. Our procedure here conﬁrms these results automatically.
Similar results were also obtained in [20] for Kleene algebras and lattices
(subvarieties of De Morgan algebras and lattices) generated by the 3-element
chains C3 = ⟨{⊤, a, ⊥}, ∧, ∨, ¬, ⊥, ⊤⟩and CL
3 = ⟨{⊤, a, ⊥}, ∧, ∨, ¬⟩where ¬
swaps ⊥and ⊤and leaves a ﬁxed. In both cases the smallest algebra in Adm(C3)
and Adm(CL
3), found automatically by our procedure, is a 4-element chain.
For certain classes of algebras K, structural completeness fails but a quasiequa-
tion Σ ⇒ϕ ≈ψ is admissible iﬀΣ |=K ϕ ≈ψ or Σ is not uniﬁable in K.
Moreover, following the previous section, the task of checking uniﬁability in K
corresponds to checking satisﬁability in some subalgebra of FK(1). Below we
characterize this situation in the case where K contains just one algebra.

492
G. Metcalfe and C. R¨othlisberger
Lemma 4. For an L-algebra A and A×B ∈Q(A), the following are equivalent:
(1) Σ |=A×B ϕ ≈ψ.
(2) Σ |=A ϕ ≈ψ or Σ is not satisﬁable in B.
Proof. (1) ⇒(2). If Σ is satisﬁable in B, then there exists a homomorphism
h: FmL →B with Σ ⊆ker h. For any homomorphism k: FmL →A with
Σ ⊆ker k, deﬁne e: FmL →A × B by e(u) = (k(u), h(u)). Then Σ ⊆ker e so,
using (1), e(ϕ) = e(ψ) and k(ϕ) = k(ψ). I.e., Σ |=A ϕ ≈ψ.
(2) ⇒(1). If Σ |=A ϕ ≈ψ, then Σ |=A×B ϕ ≈ψ, since A×B ∈Q(A). If Σ is
not satisﬁable in B, then Σ is not satisﬁable in A × B, so Σ |=A×B ϕ ≈ψ.
⊓⊔
Corollary 1. If A is an L-algebra and Q(FA(ω)) = Q(A × B), then the fol-
lowing are equivalent:
(1) Σ ⇒ϕ ≈ψ is admissible in A.
(2) Σ |=A ϕ ≈ψ or Σ is not satisﬁable in B.
Example 7. Consider the 3-valued Lukasiewicz algebra L3 = ⟨{0, 1
2, 1}, →, ¬⟩
with x →y = min(1, 1 −x + y) and ¬x = 1 −x. Following our procedure:
(i) L3 ̸∈H(FL3(0)), but L3 ∈H(FL3(1)), so Q(FL3(ω)) = Q(FL3(1)).
(ii) FL3(1) has 12 elements and Sub(FL3(1)) contains its 8 subalgebras.
(iii) Adm(L3) contains just one proper subalgebra B of FL3(1) with L3 ∈H(B);
B is isomorphic to L3 × 2 (where 2 = ⟨{0, 1}, →, ¬⟩), so by Corollary 1,
Σ ⇒ϕ ≈ψ is admissible iﬀΣ |=L3 ϕ ≈ψ or Σ is not classically satisﬁable.
Note ﬁnally that, unlike the situation for uniﬁcation, our procedure may not
ﬁnd the smallest algebra that generates Q(FA(ω)). There may be a subalgebra
of a product of copies of FA(m) which also generates this quasivariety, is not a
subalgebra of FA(ω), and has fewer elements than any member of Adm(A).
Example 8. Consider the algebra P = ⟨{a, b, c, d}, ⋆⟩where the unary operation
⋆and the free algebras FP(n) are described by the following diagrams:
a
P
b
c
d
FP(n)
x1
⋆(x1)
⋆(⋆(x1))
   xn
⋆(xn)
⋆(⋆(xn))
Following our procedure:
(i) P ̸∈H(FP(1)), but P ∈H(FP(2)), so Q(FP(ω)) = Q(FP(2)).
(ii) FP(2) has 6 elements and Sub(FP(2)) contains its 8 subalgebras.
(iii) Adm(P) contains just one algebra, FP(2) itself.
However, P can also be embedded into FP(1) × FP(1); that is, P ∈Q(FP(1)).
Hence Q(P) = Q(FP(ω)) and, by Lemma 2, P is structurally complete.

Uniﬁability and Admissibility in Finite Algebras
493
Table 1. Algebras for checking admissibility
A
|A|
Quasivariety Q(A)
Free algebra
|Output Algebra|
L3
3
algebras for L3 (Ex. 7)
|FA(1)| = 12
6
B1
3
Stone algebras (Ex. 5)
|FA(1)| = 6
3
C3
3
Kleene algebras (Ex. 6)
|FA(1)| = 6
4
L→
3
3
algebras for L→
3 (Ex. 5)
|FA(2)| = 40
3
CL
3
3
Kleene lattices (Ex. 6)
|FA(2)| = 82
4
S→¬
3
3
algebras for RM→¬ (Ex. 4)
|FA(2)| = 264
6
S→
3
3
algebras for RM→(Ex. 5)
|FA(2)| = 60
3
G3
3
algebras for G3 (Ex. 5)
|FA(2)| = 18
3
DL
4
4
De Morgan lattices (Exs. 1,6)
|FA(2)| = 166
8
D4
4
De Morgan algebras (Exs. 1,6)
|FA(2)| = 168
10
P
4
Q(P) (Ex. 8)
|FA(2)| = 6
6
B2
5
Q(B2) (Ex. 5)
|FA(1)| = 7
5
A partial solution to this last issue when V(A) = Q(A) would be to represent
A as a subdirect product of subdirectly irreducible algebras A1, . . . , An in V(A).
Then V(A) = V({A1, . . . , An}) and we can ﬁnd C1, . . . , Cn such that Ci is the
smallest subalgebra of some FA(mi) with Ai ∈H(Ci). In particular, if V(A) is
structurally complete, then each Ci will be an isomorphic copy of Ai.
5
Concluding Remarks
In this work we have introduced a basic framework for checking uniﬁability and
admissibility in ﬁnite algebras, obtaining concrete algorithms that derive poten-
tially eﬃcient proof systems. Sample results on admissibility obtained by the
procedure for 3, 4, and 5 element algebras have been described, and are sum-
marized in Table 1. This approach may also be extended to cope with arbitrary
ﬁnite-valued logics, making some minor modiﬁcations to deal with designated
values. Note, however, that in order to improve the speed of the procedure and
to cope with algebras of higher cardinality, which may (or may not) have very
large free algebras even on small numbers of generators, more eﬃcient strategies
should be implemented. In particular, rather than calculating all the subalge-
bras of a free algebra and then checking which are homomorphic preimages of the
original algebra, an interlaced procedure should be used which stores the current
most suitable subalgebra and rules out larger candidates. For particularly large
cases, we might also consider iteratively bounding the size of the subalgebras
considered. Finally, we recall that although the procedure described in Section 3
obtains the smallest possible algebra for checking uniﬁability, this is not the case
for the admissibility procedure described in Section 4 since the smallest suitable
algebra could be a subalgebra of a product of free algebras. Tackling this issue,

494
G. Metcalfe and C. R¨othlisberger
and the question of whether larger algebras (e.g., those with a product structure,
cf. Corollary 1) might be better suited for checking admissibility will be the
subject of future work.
Acknowledgements. Supported by Swiss National Science Foundation grant
20002 129507.
References
1. Baader, F., Morawska, B.: Uniﬁcation in the description logic EL. Logical Methods
in Computer Science 6(3) (2010)
2. Baader, F., Snyder, W.: Uniﬁcation theory. In: Handbook of Automated Reasoning,
vol. I, ch. 8, pp. 447–533. Elsevier (2001)
3. Baaz, M., Ferm¨uller, C.G., Salzer, G.: Automated deduction for many-valued log-
ics. In: Handbook of Automated Reasoning, vol. II, ch.20, pp. 1355–1402. Elsevier
Science B.V. (2001)
4. Babenyshev, S., Rybakov, V., Schmidt, R.A., Tishkovsky, D.: A tableau method for
checking rule admissibility in S4. In: Proceedings of UNIF 2009. ENTCS, vol. 262,
pp. 17–32 (2010)
5. Beckert, B., H¨ahnle, R., Oel, P., Sulzmann, M.: The Tableau-Based Theorem
Prover 3T AP, Version 4.0. In: McRobbie, M.A., Slaney, J.K. (eds.) CADE 1996.
LNCS, vol. 1104, pp. 303–307. Springer, Heidelberg (1996)
6. Burris, S., Sankappanavar, H.P.: A Course in Universal Algebra. Graduate Texts
in Mathematics, vol. 78. Springer, New York (1981)
7. Cintula, P., Metcalfe, G.: Structural completeness in fuzzy logics. Notre Dame
Journal of Formal Logic 50(2), 153–183 (2009)
8. Cintula, P., Metcalfe, G.: Admissible rules in the implication-negation fragment of
intuitionistic logic. Annals of Pure and Applied Logic 162(10), 162–171 (2010)
9. Ghilardi, S.: Uniﬁcation in intuitionistic logic. Journal of Symbolic Logic 64(2),
859–880 (1999)
10. Ghilardi, S.: Best
solving modal equations. Annals of Pure and Applied
Logic 102(3), 184–198 (2000)
11. Ghilardi, S.: A resolution/tableaux algorithm for projective approximations in IPC.
Logic Journal of the IGPL 10(3), 227–241 (2002)
12. Gil, A.J., Salzer, G.: Homepage of MUltseq, http://www.logic.at/multseq
13. Gorbunov, V.A.: Algebraic Theory of Quasivarieties. Springer (1998)
14. H¨ahnle, R.: Automated Deduction in Multiple-Valued Logics. OUP (1993)
15. Iemhoﬀ, R.: On the admissible rules of intuitionistic propositional logic. Journal
of Symbolic Logic 66(1), 281–294 (2001)
16. Iemhoﬀ, R., Metcalfe, G.: Proof theory for admissible rules. Annals of Pure and
Applied Logic 159(1-2), 171–186 (2009)
17. Jeˇr´abek, E.: Admissible rules of modal logics. Journal of Logic and Computation 15,
411–431 (2005)
18. Jeˇr´abek, E.: Admissible rules of Lukasiewicz logic. Journal of Logic and Compu-
tation 20(2), 425–447 (2010)
19. Jeˇr´abek, E.: Bases of admissible rules of Lukasiewicz logic. Journal of Logic and
Computation 20(6), 1149–1163 (2010)
20. Metcalfe, G., R¨othlisberger, C.: Admissibility in De Morgan algebras. Soft Com-
puting (February 2012)

Uniﬁability and Admissibility in Finite Algebras
495
21. Olson, J.S., Raftery, J.G.: Positive Sugihara monoids. Algebra Universalis 57, 75–
99 (2007)
22. Rybakov, V.: Admissibility of Logical Inference Rules. Studies in Logic and the
Foundations of Mathematics, vol. 136. Elsevier, Amsterdam (1997)
23. Salzer, G.: Homepage of MUltlog, http://www.logic.at/multlog
24. Sofronie-Stokkermans, V.: On uniﬁcation for bounded distributive lattices. ACM
Transactions on Computational Logic 8(2) (2007)
25. Sprenger, M.: Algebra Workbench. Homepage,
http://www.algebraworkbench.net
26. Zach, R.: Proof theory of ﬁnite-valued logics. Master’s thesis, Technische Univer-
sit¨at Wien (1993)

Natural Signs
Ruth Garrett Millikan
Department of Philosophy, University of Connecticut, Storrs,
CT 06269-2054, United States of America
Abstract. A description of natural signs and natural information is
proposed that interprets the presence of natural information as an aﬀor-
dance for the particular animal or species that would interpret it. This
solves the reference class problem that undercuts earlier correlational ac-
counts. It explains how there can be natural signs of individuals and also
various non-correlational signs. The eﬀect of superimposition of natural
signs is then described.
1
Introduction
A fundamental problem in the philosophy of information on which little recent
progress seems to have been made is the problem what it is for a signal or sign
to carry information with a certain content. It is my belief that this question can
be given a useful answer that is univocal across signs of all kinds, for example,
natural signs, conventional signs and the signals with which animals commu-
nicate. Today there is time to say some words only about natural information
and natural signs, but the description I will give of them can be generalized.
I believe, to all signs that genuinely carry information, for example, to animal
signals and natural language sentences that are non-accidentally true.
Natural information, as I will use the term, is that resource in the proximal
environment that can supply an animal with knowledge of its more distal envi-
ronment if the animal possesses appropriate cognitive tools. Natural information
is carried by natural signs. Natural signs are what constitute the basic materi-
als in nature that can be used to support perception and cognition in animals
including humans. If we want to understand how perception and cognition are
possible, we ﬁrst need to understand what natural signs are and the various
forms they can take.
Here are some examples of natural signs. Despite ﬁrst appearances, I will
argue, they are all fundamentally alike.
1. That the water is boiling is a sign that it has reached 212 degrees Fahrenheit.
2. Black clouds are often a sign of immanent rain; fever is often a sign of
infection, sometimes a sign of measles, other times of ﬂu, and so forth.
3. Given a wooden frame made of four straight boards paired in length and
properly nailed end to end in a closed ﬁgure, that the frame has equal diag-
onals is a sign, often used by carpenters, that the sides are parallel and the
corners at right angles.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 496–506, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Natural Signs
497
4. That the head is like that of an elephant is likely to be a sign that the tail
is like that of an elephant.
5. ‘O say can you see’ sung to a certain tune is likely to be a sign that ‘by the
dawn’s early light’ is coming next.
6. Bluster, who made the bomb I am to carry to Muggins, said that if by any
chance I should hear it ﬁzzing I should drop it dead and run, for that would
be a sign that it was about to go oﬀprematurely.
7. The direction of the North Star is a sign of the direction of geographic north;
the pull on the southern pole of the magnetosome in a northern hemisphere
marine bacterium is a sign of the direction of lesser oxygen.
8. That Jim has gone to the party is a good sign that that is where Jane is
also.
9. This certain quality of voice is a sign for me that it is Aino, my daughter,
who is speaking.
10. Traveling north from Rt. 89 on Wormwood Hill Road, the pond on the right
is a sign that our house is coming up next.
11. Suzy’s mitten lying on the walk to the side door, given that it wasn’t there
earlier today, is a sign that Suzy is already home from school.
12. The pointed tracks in the snow leading from the tattered azaleas on the
north of our yard to the stripped rhododendrons on the east are a sign that
it was the same hungry deer that nibbled on both.
The above examples are chosen in part to challenge assumptions that seem
frequently to be made about the nature of natural signs.
One such assumption is that natural signs are always related by causal laws
to what they signify. But in (3) the equal diagonals are not causes of the parallel
sides and in (7) geographic north is not causally related to the direction of the
North Star nor is the direction of lesser oxygen causally related to the pull of
the magnetosome. In (10) the pond on Wormwood Hill Road is not causally
connected with my house
not a cause of it, not an eﬀect of it, not an eﬀect
of a common cause. And in examples (9) through (12), the signs are signs of
individuals, but there are no causal laws that concern individuals as such. In (12)
the sign is of an identity relation, which is also an odd candidate for participation
in causal laws.
A second suggestion has been that natural signs, signs carrying natural in-
formation, correspond to their signiﬁeds with certainty, with ‘a probability of 1’
[1]. The idea is that a natural sign is always such qua being of a some uniform
physical type and that this type must be such that every token of it necessarily
corresponds to a real signiﬁed of a another uniform type. If that this water is
boiling is a natural sign that it is at 212 degrees, then if any water boils (in this
kind of situation) it must be at 212 degrees. If you know how to read a certain
kind of natural sign then, unlike knowing how to read a conventional sign, you
can’t go wrong in taking every token just like it to signify something actual of
the same kind as the other tokens do. What a wonderful support for knowledge
that would be, as Dretske [1] tried to show.
Dretske then qualiﬁed his claim, however, with a reference to ‘channel con-
ditions,’ conditions that had to be assumed to be in place, mediating between

498
R.G. Millikan
sign and signiﬁed, to make the probability be 1. But unless presence of the right
channel conditions is to be considered part of the natural sign itself, this wholly
undercuts the assumption of certainty. That the water is boiling will not be a
sign that it has reached 212 if it is on top of Mt. Everest or on the moon. Know-
ing how to read a certain kind of sign by assuming that the channel conditions
are right cannot yield certainty. Certainty about the channel conditions would
have to be added.
2
Correlational Signs
Indeed, the kind of signs on which perception and cognition actually rest virtually
never have forms/shapes/intrinsic properties that correspond unfailingly to the
same sort of signiﬁed, nor are channel conditions that might produce unfailing
correspondence (should they be known) univocally signed to senses like ours
unless still further channel conditions which are not necessarily univocally signed
are assumed. Actual animal cognition is not supported by infallible indicators.
Said another way, tokens of signs that support actual perception/cognition do
not carry their signifying types on their sleeves. A token fever might be a token
of any of many signifying types; it might mean measles, or rather ﬂu, or even
allergy, and so forth. Physical twins of natural sign tokens that don’t mean
the same abound. Compare homonyms, which sound the same but are diﬀerent
words.
The capacities of humans and other animals to perceive and to know
sys-
tematically to acquire accurate perceptions and true beliefs often appear to rest
heavily on mere correlations in nature rather than necessities. In line with this,
it has been suggested that we recognize a kind of natural information Nicholas
Shea [7] calls it ‘correlational information’ that is produced when there is a non-
accidentally continuing correlation (typically assumed to be underwritten by a
causal connection, but we can admit other kinds of non-accidental correlation
too) between one kind of thing and another, such that the probability of the one
is raised given the other [2,6,7]. Lloyd [2], at least, is clear that where A types
carry this kind of information about B types, an A token does not carry this
information unless there actually exists a corresponding B token (p. 64). For
example, no matter how high the non-accidental correlation between fever and
measles is in the area, if Johnny’s fever is caused by ﬂu then his particular fever
is not a correlational sign of measles. Considering correlational signs as candi-
dates for a kind of natural sign signs that support perception and cognition we
would need Lloyd’s restriction. Signs carrying misinformation will not, as such,
support cognition.
An obvious diﬃculty for the above description of correlational signs concerns
the strength of correlation to be required. But more basic, I think, is that cor-
relations and conditional probabilities are deﬁned relative to reference classes.
It could of course be that given all space-time as the reference class, the boiling
of water does raise the probability that it is at 212 degrees. But boiling is likely
to raise the probability more that water is at one or another of various other

Natural Signs
499
Fig. 1. The Reference Class Problem (With permission from Cambridge University
Press) The diagram represents a space-time continuum collapsed to two dimensions.
The dots are centers of strong positive correlation of A with B, fading in strength
outwards. The triangles are centers of strong negative correlation of A with B. Outside
the dots and triangles there is little or no correlation of A with B. Assume that the
bug is currently encountering some speciﬁc token A that is coupled with B. Which
circumscribed area determines the reference class relative to which this speciﬁc token
A is or is not, objectively, a correlational sign of B? Suggestion: the relevant area would
be one that precisely encloses the past and ongoing path of the bug (or its species).
temperatures, there being no reason to think that one earth-atmosphere is an
especially common pressure.
Similarly, we have no evidence that a elephant-like head at one end raises
the probability of a elephant-like tail at the other throughout all space-time,
and it is quite certain that the direction of the North Star does not raise the
probability of that direction being geographic north universe wide. If natural
signs were correlational signs, clearly the reference classes for these correlations
would have to be restricted in some way. How then would we decide, for any
candidate natural sign token, what the boundaries should be for the restricted
class by reference to which it would be required to be an instance of a correlation?
How, for example, would we determine that, given this little bacterium right here
now, the current direction of magnetic north is a natural sign of lesser oxygen,
considering that the relevant correlation holds (probably) neither universe wide
nor (certainly) earth wide?
Whispering in our ear may be the reply that the relevant reference class
is the earth’s northern oceans because, well, ‘that’s where the correlation is’.
But the correlation is lots of other areas the bacterium is in as well
myriad
smaller areas in some of which it may be higher, and also myriad larger areas,
ones, for example, that overlap with or include the northern hemisphere. And the
correlation fails to hold in various other areas the bacterium is equally in, perhaps
in some smaller areas such as the very square yard right around the bacterium

500
R.G. Millikan
which could be aﬀected by a bar magnet someone has dropped underneath or
by bubbles coming up from a diver. And it certainly fails in various larger areas
that include where the bacterium is along with the southern seas. (For a visual
image of this kind of problem, see ﬁgure 1) That there exists a correlation (of
some positive strength or other) within some arbitrarily chosen reference class
which includes this item surely cannot deﬁne for us what a natural sign is given
our purposes. It is not facts of this anemic kind that underlie the possibility of
perception and cognition. Besides, we should remember examples of signs such
as the ﬁzzing bomb in (6) and Suzy’s mitten in (11). These are unique cases, the
coinciding of sign with signiﬁed occurring only once.
3
A Reference Class for Determining Natural
Correlational Signs
Clearly no cognizing creature samples space and time randomly, or samples
very much of it at all. If correlations or conditional probabilities are relevant
to understanding what in nature supports cognition, I suggest that the only
relevant or non-arbitrary reference class is the class of (candidate) signs that
occur within the path of the animal that would read them, or if it is a species
that learns to read them, within the various paths of that species’ members. The
correlation must exist over the reference class that is this path, or over a portion
or portions of this path that the animal must be able to distinguish, or over
a long enough period of the animal’s life for it to learn it. I am not suggesting
that the reference class should merely include the animal’s or animals’ path(s) or
relevant parts of it. That would raise exactly the same problem over again. The
idea is that the relevant non-arbitrary reference class consists in the very samples
sampled by the animal or species that uses a natural sign, or of such samples as
are further discriminable by it in some way as relevant. Useful correlations have
to occur within the experience of the animals using them, or within portions
of this experience the animal can learn to discriminate as such. Or they have
to occur among things prior signs of which occur within the experience of the
animal using them, and so forth.
This changes the game quite radically, however, from the one played by tradi-
tional theorists of natural signs, who have hoped to ﬁnd natural signs and natural
information as an ‘objective commodity’ [1] out in the world, as structures that
God could have seen long before he fashioned creatures to harvest them. Natural
signs will instead be aﬀordances like food or like shelter, things that are what
they are only relative to an animal who would use them, in this case relative, in
the ﬁrst instance, to the actual location of the animal, not merely to its capac-
ities. Nothing will be a natural sign of anything else absolutely. Rather, there
will be things that can serve as signs or do serve as signs for a given animal or
species. Notice, however, that from this perspective the question how strong a
correlation must be to create a natural sign is not a problem. The correlation
must be strong enough and of the right kind for the particular animal to detect
it, and for it to be possibly useful, given the animal’s purposes, to bet on it. Or,

Natural Signs
501
if the reference to possible use for a given animal is too vague, we can always
fall back on the underlying idea that serving as a natural sign is what is basic
rather than being a natural sign.
Strong correlations between things throughout all space and time are chieﬂy
restricted to what follows from universal natural laws, these universal laws, as
science knows, being hard to discern. But the space-time paths of individual
animals and species often seem to be reference classes of very strong correlation
between fairly numerous and fairly evident kinds of things. Suppose that we
pause to wonder why this is so. Why are the correlations among samples an
individual animal or a species happens to encounter along one section of its
own idiosyncratic space-time path likely to continue to hold for the samples in
another? Why are strong correlations in an animal’s sampling past often good
signs that these correlations will remain strong in its sampling future, regardless
of the strength of these correlations universe wide? This, it seems to me, is the
fundamental question we should be trying to answer about correlational natural
signs, about how they can help to make cognition possible. Moreover I will argue
that the answer to this question generalizes beyond correlational signs. It can foot
an understanding of the nature of other natural signs that are not correlational,
as in examples (6) and (10) through (12).
4
Answering the Fundamental Question
We need to superimpose two familiar facts. First, the path of an animal through
space and time is continuous. It does not skip from one distant time and place
to another. Similarly, a species spreads out over space and time on continuous
individual paths that branch from one another, conﬁned by natural selection
within geographical areas with friendly properties. Second, the usual reason for
the presence of local correlations is that things persist through time, staying in
the same or connected locales, or that they self-maintain or cycle in the same or
in connected locales or, in post-archeozoic times on earth, that they reproduce
or are reproduced (artifacts, animal signals, linguistic forms) in the same or
in connected locales. Whether conditions, events or entities, much that is near
the animal today will be near it tomorrow, or something much like it will be.
Channel conditions remain in place and events that manifest themselves through
these channels recur. For these reasons, where there are good correlations in one
portion of an animal’s space-time path, frequently these correlations extend to
other portions.
Mountains, valleys, rivers, oceans, the earth’s atmosphere, persist. Individual
trees, rocks, houses, roads, paths persist in deﬁnite places. Individual animals
and plants and also species of these self-maintain hence persist, remaining in
roughly the same or connected areas. The earth persists and rotates causing
cyclical daily patterns, and it circles around the persisting sun causing cyclical
yearly patterns, cyclical weather patterns and cyclical patterns in plant and
animal behaviors. (It is not a natural law that black clouds tend to burst. Under
diﬀerent conditions they too would persist.) People cycle from home to work or

502
R.G. Millikan
school and back home again at fairly regular hours. Animal and plant species
reproduce, stabilized by homeostasis in their gene pools and by natural selection
in a continuing environment. Artifacts that work well or that please, songs and
stories, red and green Christmas decorations and steeples on churches are all
numerously reproduced. Where local sampling shows correlations, typically the
causes of the correlations are being actively preserved or cycled or reproduced
and will continue to be so for some time through neighboring times and places.
An animal’s path typically persists among, crosses or overlaps the single or
branching paths of many such persisting, cycling or reproducing items.
Traveling north up Wormwood Hill Road, passing the pond on the right is
repeatedly followed by passing our house because the relation between our house
and the pond persists (10). In the circles I walk in the peculiar quality of my
daughter Aino’s voice is repeatedly reproduced by my daughter’s larynx which
persists in areas that frequently overlap mine (9). The correlation between lesser
oxygen and the direction southern poles of magnetosomes of northern hemisphere
bacteria point persists because the relation between magnetic north and the
surface of the ocean persists as does the scarcity of other sources of magnetic
ﬁelds on the earth (7). The correlation between a elephant-like head at one
end with a elephant-like tail at the other persists on earth because elephants
reproduce (4). The correlation between ‘O say can you see’ and ‘by the dawn’s
early light’ persists because the U.S. national anthem is copied over and over
by people who teach it to one another (5). The correlation of fever with measles
persists because the measles virus reproduces in people and people reproduce
thus reproducing the conditions that lead from the measles virus to the fever
(2). The correlation between where Jim is to be found in the evening and where
Jane is to be found persists because Jim and Jane and a certain bond between
them also persist often bringing them together in the evenings (8).
Understanding why a correlation persists, what kind of endurings or cyclings
or reproducings are accounting for it, can of course be a huge help in trying to
project the more exact path or paths that a correlation will take. Knowing how
a disease spreads, for example, helps in deciding whether certain symptoms are
likely to be signs of it or rather of something else. Knowing that Jim and Jane
have split, I will no longer expect Jim’s location to be a sign of Jane’s. But it is
not always necessary to know reasons. I understand that only edible mushrooms
grow on dead hardwood trees, but I have no idea why. Most fundamental, how-
ever, is that many animals are born, live and die such that their life paths taken
entire mark out reference classes in which many of the correlations they depend
on actively persist. There is no need for them to recognize boundaries.
5
Generalizing to Non-correlational Signs
Viewing a correlation merely as a repeated pattern, and viewing natural sign
reading simply as pattern completion, it will be possible to generalize this picture
in several ways to include many more natural signs than those that depend upon
correlations.

Natural Signs
503
A continuing correlation can be viewed simply as a very small recurring pat-
tern, a repeated pattern of an A state of aﬀairs being so-related to a B state of
aﬀairs. What repeats is a black cloud being close overhead at a place and time
with rain being at that place shortly after that time, or a elephant-like head
being at one end of a thing and an elephant-like tail at the other, or child having
a rash and fever when the child harbors the measles virus. What repeats in a
sign-signed relation is a relation between a certain kind of sign and a certain
kind of signed; reading the sign is completing the pattern.
In the simplest cases, the relation that determines the part of the pattern that
is signed from the part that is the sign consists in the identity of an aspect of
the state of aﬀairs that is signed with an aspect of the state of aﬀairs that is the
sign. The same thing that has an elephant head at one end has an elephant tail
at the other. The same child that has a rash and fever also harbors the measels
virus. In the same place that the black clouds appear, the rain comes down soon
after. In the same car and at the same time, that the gas guage needle is at the
top of the dial, the gas tank is full. Or, generalizing this, In the same car and
at the same time that the guage needle is a certain rough proportion of the way
between the bottom and the top lines on the dial, the gas in the tank is that
same rough proportion of the way from empty to full. But the relation between
sign and signed may rest on a function other than identity. What is needed is
only that the repeating pattern involve a determining relation between sign and
signed, what is signed being determined as a function of the sign. Notice, for
example, that in the case of black clouds and rain, the time of the rain is only a
function of the time of the clouds, namely, just after. Similarly for lightning and
thunder, of course.
What repeats in the case of basic natural laws often involves patterns of this
more general kind. The temperature at which water boils is not 212 degrees but
a function of the pressure; the basic pattern is a recurring relation between (the
same portion of) water’s temperature, its pressure and its boiling. For a very
diﬀerent kind of example, although American alligators range from nine inches
long to more than ﬁfteen feet long, the length of the alligator’s snout is always
the same rough proportion of the length of its tail. There is a recurring relation
between snout length and tail length, a recurring pattern in which the one is
determined as a function of the other. And so, of course, for every other pair
of the alligator’s parts. Relations between signs and what they sign of this kind
may be called “projection relations” and we can call pattern repetitions of this
more abstract sort repetitions by projection.
6
Natural Signs Based on a Single Repetition
Signiﬁcant correlatons between one thing and another require many repetitions
of the one given the other. But single recurrences of extremely complex patterns
may also yield natural signs. When a very detailed complex pattern repeats
even once in a nearby location, it is likely that the repetition is no accident,
the chance of accident going down sharply with the detail and complexity of

504
R.G. Millikan
the pattern. Rather, the pattern has probably endured (it’s the same individual
again) or been repeated as a result of the persistence of conditions along with
the cycling of events and/or some process involving reproduction or copying.
If you encounter two detailed paintings that are exactly alike it is more than
likely either that one has been reproduced from the other or both from some
third or perhaps it is the same painting again. Moreover if you ﬁnd that one half
of a detailed painting is exactly like one half of another, thus making it likely
that the one has been reproduced from the other or from some third, or that
it is the same one over again, then it is very likely that the other halves match
as well. Suddenly noticing the passing surroundings in my town from the back
seat of the car and seeing that they exactly match the road and yard in front
of Alice’s house and that the house is alike as well, it is very likely that all the
other features of Alice’s house are to be found there also, for it is likely that it
IS Alice’s house, which has continued to endure since I last saw it.
Besides recurrence of the size relation between snout and tail of the alligator,
the shape, composition and structure of the whole alligator, all its parts and
all their relations to one another a concrete and very detailed pattern keeps
recurring. Similarly, the patterns that are whole elephants and whole daisies
repeat, and also whole symphonies and whole Protestant wedding ceremonies
and whole Toyota Camrys and whole Gothic churches and whole chairs and whole
books (copies of them) and whole McDonalds restaurants. Locally repeating
patterns of these kinds tend to be much less abstract, much more detailed and
richer, than are, say, universal lawlike patterns, and they are usually much easier
to notice. Mentally completing complex patterns to match those that have been
encountered in one’s past in this way can often supply knowledge on a base of
only one or a very few prior samples. Possibly I have only seen one elephant or
one alligator before, or been at Alice’s house only once before.
The notion of a correlation has dropped far into the background here, a much
broader notion of local pattern repetition and pattern completion emerging as
the base principle explaining natural signs and the reading of natural informa-
tion. It seems best to claim then that the general form that natural signs take is
that of patterns that repeat, either directly or by projection, within an animal’s
experience and that reading natural signs is just pattern completion. Natural
sign reading may be based, at the one extreme, on observation of many repeti-
tions of a very simple pattern, at the other extreme, on a single prior observation
of a very complex pattern. Having had it rain many times when black clouds
form I am prepared for rain the next time I see black clouds. Having seen only
one whole elephant, I am prepared for the tail on the next.
It is often possible to recognize a complex repeated pattern from any of many
diﬀerent samplings of its various features. An elephant may be recognized from
the front or the back or the side, by the trunk, by the head, by the legs, by
the tusks, by the tail, by the trumpet. Each of these may serve as a sign of an
elephant and as a sign of each other elephant feature. Similarly, of course, an
automobile or a premises may be recognized from the front or the back or the
side and so forth. A song may be recognized by hearing the beginning or the end

Natural Signs
505
or only a line in the middle. There are many ways to recognize a baseball game
or a restaurant or a piano, also many ways to test for a chemical substance,
and so forth. Thus the completion of complex patterns is a doubly versatile way
that nature aﬀords of reading natural signs. Surely it is not just correlations of
simple pairs of features but repeated clumpings of multiple features that supply
the bulk of natural signs supporting cognition.
7
Natural Signs of Individuals
An immediate and notable implication of this description of root signs is that
there can be natural root signs of individual objects. There can be natural in-
formation that concerns individual objects. (This is a result that has not, to my
knowledge, been obtained by a theory of information before) We are considering
information to be a kind of aﬀordance relative to an animal that can appreciate
it, in particular, relative to the reference class of actual samplings of the animal
over time. These samplings are taken along the animal’s space-time path, which
crosses the paths of other locally enduring individual objects, sometimes trac-
ing them for a period of time, often re-crossing them at later times. Many of
these objects will have simple features or combinations of features that remain
unique in the experience of the animal. These features are natural signs for the
animal, carrying natural information for it about individual objects. Nor is the
animal dependent only on features of individuals that are unique in the ani-
mal’s experience. I remarked earlier that although many of the correlations that
animals depend on may actively persist throughout their lives, in other cases
the animal may recognize rough boundaries for a domain of correlation, learning
ways to recognize or track this domain. Individual objects persist through single,
continuous space-time paths, diﬀerent kinds of individuals displaying diﬀerent
characteristic patterns of spatial displacement over time. Big rocks, trees and
houses usually say in the same place; squirrels have certain characteristic ways
of moving about, automobiles another. Experience with these various patterns
of rest or movement enables fallible tracking of the sign domains for various
individuals of these kinds over longer or shorter periods of time.
8
Ovelapping Natural-Sign Patterns
I have presented natural signs as resulting from endurances and repetitions of
patterns along paths that cris-cross and interweave with the paths of the animals
for whom these signs are aﬀordances. Such patterns often cris-cross or interweave
with one another as well. Repeated patterns may be superimposed on one an-
other. If part of one pattern is recognized as such, then recognized as also being a
part of a second superimposed pattern, pattern completion may reveal relations
between various other parts of these patterns as well. Think of employing two
maps that overlap in content to determine relations between places shown just
on one with those shown just on the other. Compare recognizing the repeated
pattern that is small stray items lying outdoors where the people who own them

506
R.G. Millikan
have recently been, as superimposed over the repeated pattern that is Suzy,
whose stray mitten one recognizes, entering from school by the side door about
this time of day. This superpositioning yields Suzy having already returned from
school (11). Bluster, who made the bomb I am to carry to Muggins, has mentally
superimposed and chained together many small repeating patterns involving one
simple kind of part that his bomb contains interacting with another simple part,
which completed chain would determine it to go oﬀas planned. But he knows of
other sometimes repeated patterns that, superimposed, would include a hissing
sound and immediate premature detonation, and it is this latter pattern that he
is warning me about (6).
Acknowledgements. Portions of this paper are a revised form from [5], used
with permission.
References
1. Dretske, F.: Knowledge and the Flow of Information. The MIT Press, Cambridge
(1981)
2. Lloyd, D.: Simple Minds. The MIT Press, Cambridge (1989)
3. Millikan, R.G.: Varieties of Meaning. The MIT Press, Cambridge (2004)
4. Millikan, R.G.: On knowing the meaning; with a coda on swampman. Mind 119(473),
43–81 (2010)
5. Millikan, R.G.: Natural Information, Intentional Signs and Animal Communication.
In: Stegmann, U. (ed.) Animal Communication Theory: Information and Inﬂuence.
Cambridge University Press (forthcoming)
6. Price, C.: Functions in Mind; A theory of Intentional Content. Oxford University
Press, Oxford (2001)
7. Shea, N.: Consumers need information: Supplementing teleosemantics with an input
condition. Philosophy and Phenomenological Research 75(2), 404–435 (2007)

Characteristics of Minimal Eﬀective
Programming Systems
Samuel E. Moelius III
IDA Center for Computing Sciences, 17100 Science Drive, Bowie, MD 20715-4300,
United States of America
semoeli@super.org
Abstract. The Rogers semilattice of eﬀective programming systems
(epses) is the collection of all eﬀective numberings of the partial com-
putable functions ordered such that θ ≤ψ whenever θ-programs can be
algorithmically translated into ψ-programs. Herein, it is shown that an
eps ψ is minimal in this ordering if and only if, for each translation func-
tion t into ψ, there exists a computably enumerable equivalence relation
(ceer) R such that (i) R is a subrelation of ψ’s program equivalence re-
lation, and (ii) R equates each ψ-program to some program in the range
of t. It is also shown that there exists a minimal eps for which no single
such R does the work for all such t. In fact, there exists a minimal eps ψ
such that, for each ceer R, either R contradicts ψ’s program equivalence
relation, or there exists a translation function t into ψ such that the
range of t fails to intersect inﬁnitely many of R’s equivalence classes.
1
Introduction
Let N be the set of natural numbers, i.e., {0, 1, 2, ...}. An eﬀective programming
systems (eps) is a partial computable function λp, x ψp(x) mapping N2 to N, and
having the following property. For each partial computable function ζ mapping N
to N, there exists a p such that ψp = ζ. Eﬀective programming systems abstract
the notion of programming language in the following sense. One can think of p
as a program, and of ψp as the partial computable function denoted by p within
some programming language corresponding to ψ.
Rogers [19] introduced the following ordering on epses. For epses θ and ψ,
θ ≤ψ iﬀthere exists a computable function t : N →N such that, for each
p, θp = ψt(p). Intuitively, θ ≤ψ whenever θ-programs can be algorithmically
translated into ψ-programs. Moreover, an eps ψ is minimal in this ordering iﬀ
having the ability to algorithmically translate θ-programs into ψ-programs im-
plies having the ability to algorithmically translate ψ-programs into θ-programs,
for each eps θ.
Arguably, the most well studied collection of minimal epses is that of the
Friedberg numberings [3, 11]. Recall that a Friedberg numbering is an eps that
is 1-1, i.e., for each p and q, ψp = ψq implies p = q. Examples of works that
make use of this concept include [12, 16, 18, 2, 22, 21, 10, 23, 5, 6, 7].
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 507–516, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

508
S.E. Moelius III
In [17], Pour-El asked whether every minimal eps is equivalent to some Fried-
berg numbering. Ershov [1, §5] showed that there exists a minimal eﬀective
numbering of the computably enumerable sets that is not equivalent to any 1-1
numbering. Shortly thereafter, his student, Khutoretskii, established the anal-
ogous result for the partial computable functions, thereby answering Pour-El’s
question.
Theorem 1 (Khutoretskii [8, Ex. 1 and Cor. 4]). There exists a minimal
eps that is not equivalent to any Friedberg numbering.
For the purposes of this paper, Theorem 1 is best viewed through the following
folklore theorem.
Theorem 2 (Folklore). For each eps ψ, ψ is equivalent to a Friedberg num-
bering iﬀψ’s program equivalence relation is computable.
Thus, Theorem 1 could be restated as: there exists a minimal eps whose program
equivalence relation is not computable. On the other hand, as noted in the proof
of Theorem 1, the constructed eps’s program equivalence relation is computably
enumerable. (In particular, exactly one such equivalence class is a simple set [20,
§8.1], and all others a singletons.) Thus, one has the following.
Theorem 3 (Khutoretskii, corollary of Thm. 2 and proof of Thm. 1).
There exists an eps whose program equivalence relation is computably enumer-
able, but not computable.
Subsequent to the above, Khutoretskii showed the following.
Theorem 4 (Khutoretskii, corollary of [9, Thm. 1]). There exists a min-
imal eps whose program equivalence relation is not computably enumerable.
Clearly, Theorems 3 and 4 can be viewed as a sharpening of Theorem 1. Herein,
we sharpen Khutoretskii’s results even further.
To facilitate the statement of our results, we ﬁrst give a few deﬁnitions. Sup-
pose that ψ is an eps. For each t : N →N, we say that t is a translation function
into ψ iﬀthere exists an eps θ such that t witnesses θ ≤ψ. The following deﬁ-
nition is equivalent. For each t : N →N, t is a translation function into ψ iﬀt is
computable and the partial function λp, x ψt(p)(x) is an eps.
Deﬁnition 5. Suppose that ψ is an eps, and that t is a translation function into
ψ. Then, for each equivalence relation R, (a) and (b) below.
(a) R strongly ties t into ψ iﬀR satisﬁes (i) and (ii) just below.1
(i) R is a subrelation of ψ’s program equivalence relation.
(ii) The range of t intersects each of R’s equivalence classes.
(b) R weakly ties t into ψ iﬀR satisﬁes (i) just above and (ii∗) just below.2
(ii∗) The range of t intersects all but ﬁnitely many of R’s equivalence classes.
1 In some places, we omit the phrase “into ψ” when it is clear from context.
2 See footnote 1.

Characteristics of Minimal Eﬀective Programming Systems
509
Thus, if equivalence relation R strongly ties translation function t into eps ψ,
then R equates each ψ-program to some program in the range of t. If R merely
weakly ties t into ψ, then there may be inﬁnitely many ψ-programs that R does
not equate to any program in the range of t. However, those inﬁnitely many such
ψ-programs will form only ﬁnitely many equivalence classes.
Our ﬁrst main result is that the minimal epses may be characterized as follows.
Theorem 6. For each eps ψ, (a)-(c) below are equivalent.
(a) ψ is minimal.
(b) For each translation function t into ψ, there exists a computably enumerable
equivalence relation (ceer)3 that strongly ties t into ψ.
(c) For each translation function t into ψ, there exists a ceer that weakly ties t
into ψ.
Note that Theorem 4 is about a single equivalence relation, i.e., the program
equivalence relation of a certain eps, whereas Theorem 6 is about one equivalence
relation per translation function into any given eps. Thus, one might ask: if ψ
is a minimal eps, then might there always exist a single ceer that strongly ties
each translation function into ψ? The answer, as it turns out, is no. In fact, as
Theorem 7 below states, there need not even exist a single ceer that weakly ties
each translation function into ψ.
Theorem 7. There exists an eps ψ satisfying (a) and (b) below.
(a) ψ is minimal.
(b) For each ceer R, there exists a translation function t into ψ such that R does
not weakly tie t into ψ.
Continuing with this line of thought, one ﬁnds that the strong and weak notions
of Deﬁnition 5 separate when one considers single equivalence relations.
Theorem 8. There exists an eps ψ and a ceer R satisfying (a) and (b) below.
(a) For each translation function t into ψ, R weakly ties t into ψ.
(b) For each ceer R′, there exists a translation function t into ψ such that R′
does not strongly tie t into ψ.
Clearly, if ψ is an eps, and ψ’s program equivalence relation is computably enu-
merable, then there exists a single ceer R that strongly ties each translation
function into ψ, i.e., R is ψ’s program equivalence relation. Thus, one might ask:
does the converse hold? Theorem 9, just below, establishes that it does not.
Theorem 9. There exists an eps ψ and a ceer R satisfying (a) and (b) below.
3 We pronounce ceer like the ﬁrst syllable of “series”. Computably enumerable equiva-
lence relations are of interest in their own right. Gao and Gerdes [4] give an excellent
survey.

510
S.E. Moelius III
P0 ⇐⇒
6
Thm. 2
(Folklore)
P ′
0 =⇒
̸
⇐=
6
Thm. 3
(Khutoretskii)
P1 =⇒
̸
⇐=
6
Thm. 9
P2 =⇒
̸
⇐=
6
Thm. 8
P3 =⇒
̸
⇐=
6
Thm. 7
P4 ⇐⇒P ′
4 ⇐⇒
AAK

Thm. 6
P ′′
4 .
– P0(ψ) ⇔ψ is equivalent to a Friedberg numbering.
– P ′
0(ψ) ⇔ψ’s program equivalence relation is computable.
– P1(ψ) ⇔ψ’s program equivalence relation is computably enumerable.
– P2(ψ) ⇔there exists a ceer R that strongly ties each translation function into ψ.
– P3(ψ) ⇔there exists a ceer R that weakly ties each translation function into ψ.
– P4(ψ) ⇔for each translation function t into ψ, there exists a ceer that strongly ties
t into ψ.
– P ′
4(ψ) ⇔for each translation function t into ψ, there exists a ceer that weakly ties
t into ψ.
– P ′′
4 (ψ) ⇔ψ is minimal.
Fig. 1. A summary of the results mentioned in Section 1. In addition to the above:
Mal’cev [13, 14] showed that P1 ⇒P ′′
4 , and Khutoretskii [9] showed that P1 ̸⇐P ′′
4 (see
Theorem 4).
(a) For each translation function t into ψ, R strongly ties t into ψ.
(b) ψ’s program equivalence relation is not computably enumerable.
Figure 1 summarizes the results mentioned in this section. The remainder of this
paper is organized as follows. Section 2 covers preliminaries. Section 3 gives the
proof of Theorem 6, and a sketch of the proof of Theorem 7. Complete proofs of
Theorems 7 through 9 can be found in the expanded version of this paper [15].
2
Preliminaries
Computability-theoretic concepts not covered below are treated in [20].
Lowercase math-italic letters (e.g., i, p, x), with or without decorations, range
over elements of N, unless stated otherwise. Uppercase math-italic letters (e.g.,
I, P, X), with or without decorations, range over subsets of N, unless stated
otherwise. For each non-empty X, min X denotes the minimum element of X.
min ∅def
= ∞. For each non-empty, ﬁnite X, max X denotes the maximum element
of X. max ∅def
= −1. Fin denotes the collection of all ﬁnite subsets of N.
⟨·, ·⟩denotes any ﬁxed pairing function, i.e., a 1-1, onto, computable function
of type N2 →N [20, page 64]. For each x, y, and z, ⟨x, y, z⟩def
=

x, ⟨y, z⟩

. For
each X and Y , X × Y def
= {⟨x, y⟩| x ∈X ∧y ∈Y }.
Every partial function considered herein maps N to N, unless stated otherwise.
For each partial function ζ, and each x, ζ(x)↓denotes that ζ(x) converges;
whereas, ζ(x)↑denotes that ζ(x) diverges. We use ↑to denote the value of a

Characteristics of Minimal Eﬀective Programming Systems
511
divergent computation. For the sake of some subsequent proofs, it is convenient
to have the following notation. For each i and n,
i<n def
= λx

i, if x < n;
↑, otherwise.
(1)
Thus, i<n is the partial function that maps each value less than n to i, and that
diverges everywhere else. For each partial function ζ, rng(ζ) denotes the range
of ζ, i.e., rng(ζ) def
= {y | (∃x)[ζ(x) = y]}. PartComp denotes the set of all partial
computable functions (mapping N to N).
ϕ denotes any ﬁxed acceptable (i.e., maximal) eps [19, 20, 16, 18, 21]. For
each p, Wp def
= {x | ϕp(x)↓}. For each p and s, the following.
ϕs
p
def
= λx
ϕp(x), if x < s and ϕp(x) converges in fewer than s steps;
↑,
otherwise.
(2)
W s
p
def
= {x | ϕs
p(x)↓}.
(3)
For each eps ψ, Equiv(ψ) denotes ψ’s program equivalence relation, i.e.,
Equiv(ψ) def
= {⟨p, q⟩| ψp = ψq}.
(4)
For each equivalence relation R, Classes(R) denotes the set of R’s equivalence
classes, i.e., Classes(R) is the set of exactly those E satisfying (a)-(c) below.
(a) E ̸= ∅.
(b) (∀p, q ∈E)[⟨p, q⟩∈R].
(c) (∀p ∈E)(∀q ̸∈E)[⟨p, q⟩̸∈R].
3
Results
This section gives the proof of Theorem 6, and a sketch of the proof of Theorem 7.
Complete proofs of Theorems 7 through 9 can be found in the expanded version
of this paper [15].
Our ﬁrst main result is that the minimal epses may be characterized as per
Theorem 6, restated just below. Recall from Deﬁnition 5 that if equivalence rela-
tion R strongly ties translation function t into eps ψ, then (i) R is a subrelation
of ψ’s program equivalence relation, and (ii) the range of t intersects each of R’s
equivalence classes. On the other hand, if R merely weakly ties t into ψ, then the
range of t need only intersect all but ﬁnitely many of R’s equivalence classes.
Theorem 6. For each eps ψ, (a)-(c) below are equivalent.
(a) ψ is minimal.
(b) For each translation function t into ψ, there exists a ceer that strongly ties
t into ψ.
(c) For each translation function t into ψ, there exists a ceer that weakly ties t
into ψ.

512
S.E. Moelius III
Proof. Let ψ be given.
(a) ⇒(b): Suppose that ψ is minimal. Let t be any translation function into
ψ, and let θ be such that t witnesses θ ≤ψ. Since ψ is minimal, there exists
a t′ : N →N witnessing ψ ≤θ. Let R be the reﬂexive, symmetric, transitive
closure of
{⟨p, (t ◦t′)(p)⟩| p ∈N}.
(5)
Clearly, R is a ceer and R ⊆Equiv(ψ). It remains to show that, for each E ∈
Classes(R), rng(t) ∩E ̸= ∅. So, let E ∈Classes(R) be given, and let p ∈E be
arbitrary. Then, clearly, (t ◦t′)(p) ∈rng(t) ∩E.
(b) ⇒(c): Immediate.
(c) ⇒(a): Suppose (c). Further suppose that θ is an eps, and that t : N →N
witnesses θ ≤ψ. Then, by (c), there exists a ceer R ⊆Equiv(ψ) such that, for
all but ﬁnitely many E ∈Classes(R), rng(t) ∩E ̸= ∅. Let n be the number of
elements of Classes(R) that do not intersect rng(t), and let E0, ..., En−1 be those
elements. Choose q0, ..., qn−1 such that, for each i < n and p ∈Ei, θqi = ψp.
Note that, for each p, either R equates p to some element of rng(t), or p ∈Ei, for
some i < n. It follows that the function t′ : N →N, deﬁned next, is computable.
t′ = λp
⎧
⎨
⎩
q, where q is ﬁrst found such that ⟨p, t(q)⟩∈R,
if such a q exists;
qi, otherwise, where i is such that p ∈Ei.
(6)
It is straightforward to verify that t′ witnesses ψ ≤θ.
□(Theorem 6)
Theorem 7, restated just below, is our second main result. It establishes that
there there exists a minimal eps ψ such that, for each ceer R, either R contradicts
ψ’s program equivalence relation, or there exists a translation function t into ψ
such that the range of t fails to intersect inﬁnitely many of R’s equivalence
classes.
Theorem 7. There exists an eps ψ satisfying (a) and (b) below.
(a) ψ is minimal.
(b) For each ceer R, there exists a translation function t into ψ such that R does
not weakly tie t into ψ.
Proof (Sketch). The eps ψ is constructed below, following some necessary deﬁni-
tions. Let Aux ⊆PartComp be such that
Aux = PartComp \ {⟨i, j⟩<k+1 | i, j ∈N ∧k < 2i}.
(7)
It is straightforward to show that Aux is 1-1, computably enumerable. So, let
(αℓ)ℓ∈N be a 1-1, eﬀective numbering of Aux .
As is common, ψ is constructed in stages, i.e., ψ is the union of ψ0 ⊆ψ1 ⊆· · · .
In conjunction with ψ, four computable predicates are constructed: λi, s [i ∈
R-ﬂagss], λi, j, ℓ, s [⟨i, j, ℓ⟩∈t-ﬂagss], λℓ, s [ℓ∈Srcs], and λp, s [p ∈Dsts]. The
purposes of these predicates are as follows.

Characteristics of Minimal Eﬀective Programming Systems
513
– The R-ﬂags predicate keeps track of which i are such that Wi contradicts
ψ’s program equivalence relation. More precisely, for each i, if there exists
an s such that i ∈R-ﬂagss, then Wi ̸⊆Equiv(ψ).
– The t-ﬂags predicate helps to keep track of which ℓmay be such that ϕℓis
a translation function into ψ. It will turn out that: if i and ℓare such that
Wi ⊆Equiv(ψ) and ϕℓis a translation function into ψ, then, for each j, and
all but ﬁnitely many s, ⟨i, j, ℓ⟩∈t-ﬂagss.
– The Src predicate keeps track of which ℓare such that αℓhas not yet been
assigned to any ψ-program. In particular, if ℓand s are such that ℓ∈Srcs
and αℓ̸= λx ↑, then, for each p, ψs
p ̸= αℓ.
– The Dst predicate keeps track of which ψ-programs have not yet been used.
More precisely, if p and s are such that p ∈Dsts, then ψs
p = λx ↑.
For each i and s, i ∈R-ﬂagss+1 iﬀi ∈R-ﬂagss, unless stated otherwise. Anal-
ogous statements apply to the t-ﬂags, Src, and Dst predicates, as well. The
following will be clear from the construction of ψ, for each s.
R-ﬂagss ⊆R-ﬂagss+1.
(8)
t-ﬂagss ⊆t-ﬂagss+1.
(9)
Srcs ⊇Srcs+1.
(10)
Dsts ⊇Dsts+1.
(11)
Let height : N3 →N be such that, for each i, j, and s,
heights
i,j = |{ℓ| ⟨i, j, ℓ⟩∈t-ﬂagss}|.
(12)
It will be clear from the construction of ψ that, for each i, j, ℓ, and s,
⟨i, j, ℓ⟩∈t-ﬂagss ⇒ℓ< i.
(13)
Thus, for each i, j, and s,
heights
i,j ≤i.
(14)
Let num : N3 →N be such that, for each i, j, and s,
nums
i,j = 2i−h, where h = heights
i,j.
(15)
Let f : N3 →N be such that, for each i, j, and k,
fi,j(k) = 2⟨i, j · 2i+1 + k⟩.
(16)
For each i, j, s, and k < nums
i,j, let Es
i,j,k ∈Fin and ¯Es
i,j,k ∈Fin be as follows,
with h = heights
i,j.
Es
i,j,k =

fi,j
	
k · 2h+1

, ..., fi,j
	
k
· 2h+1 + 2h −1


.
(17)
¯Es
i,j,k =

fi,j
	
k · 2h+1 + 2h
, ..., fi,j
	
(k + 1) · 2h+1
−1


.
(18)
Suppose that i, j, and s are such that heights+1
i,j
= h + 1, where h = heights
i,j.
Then, it can be shown that, for each k < nums+1
i,j , the following.
Es+1
i,j,k = Es
i,j,2k
∪¯Es
i,j,2k
.
(19)
¯Es+1
i,j,k = Es
i,j,2k+1 ∪¯Es
i,j,2k+1.
(20)

514
S.E. Moelius III
– Stage s = −1. Do the following.
• Set R-ﬂags0 = ∅.
• Set t-ﬂags0 = ∅.
• Set Src0 = N.
• Set Dst0 = 2N + 1.
• For each i, j, and k < 2i, set ψ0
fi,j(2k) = ψ0
fi,j(2k+1) = ⟨i, j⟩<k+1.
• For each p ∈2N + 1, set ψ0
p = λx ↑.
– Stage s = ⟨0, ℓ⟩. If ℓ∈Srcs, then do the following.
• Set Srcs+1 = Srcs \ {ℓ}.
• Set Dsts+1 = Dsts \ {min Dsts}.
• Set ψs+1
min Dsts = αℓ.
– Stage s = ⟨i + 1, 0, −⟩. Determine whether there exist j and k satisfying condi-
tions (a)-(c) just below.
(a) i ̸∈R-ﬂagss.
(b) k < nums
i,j.
(c) W s
i ∩(Es
i,j,k × ¯Es
i,j,k) ̸= ∅.
If such j and k exist, then do the following.
• Set R-ﬂagss+1 = R-ﬂagss ∪{i}.
• Choose any ℓ, m ∈Srcs such that ℓ̸= m and ⟨i, j⟩<2i ⊆αℓ∩αm.
• Let d : N →N be any 1-1, computable function such that rng(d) is computable,
rng(d) ⊆Dsts, and Dsts \ rng(d) is inﬁnite.
• Set Srcs+1 = Srcs \ {ℓ, m}.
• Set Dsts+1 = Dsts \ rng(d).
• For each j, each k < nums
i,j, and each p ∈Ei,j,k, set ψs+1
p
= αℓ.
• For each j, each k < nums
i,j, and each q ∈¯Ei,j,k, set ψs+1
q
= αm.
• For each j and k < nums
i,j, set ψs+1
d(n+k) = ⟨i, j⟩<(k+1)·2h, where
n = 
ˆj<j nums
i,ˆj and h = heights
i,j.
– Stage s = ⟨i + 1, j + 1, ℓ, −⟩. Let h = heights
i,j. Determine whether conditions (i)-
(iv) just below are satisﬁed.
(i) ℓ< i.
(ii) i ̸∈R-ﬂagss.
(iii) ⟨i, j, ℓ⟩̸∈t-ﬂagss.
(iv) For each k < nums
i,j, rng(ϕs
ℓ) ∩(Es
i,j,k ∪¯Es
i,j,k) ̸= ∅.
If so, then do the following.
• Set t-ﬂagss+1 = t-ﬂagss ∪{⟨i, j, ℓ⟩}. (Note that this implies heights+1
i,j
=
heights
i,j + 1.)
• Let n = nums+1
i,j . (Note that, by the just previous step, n = nums
i,j/2.)
• Let {q0 < q1 < · · · < qn−1} be the n least elements of Dsts.
• Set Dsts+1 = Dsts \ {q0, q1, ..., qn−1}.
• For each k < n and p ∈(Es+1
i,j,k ∪¯Es+1
i,j,k), set ψs+1
p
= ⟨i, j⟩(2k+2)·2h.
• For each k < n, set ψs+1
qk
= ⟨i, j⟩<(2k+1)·2h.
Fig. 2. The construction of ψ in the proof of Theorem 7. The symbols height, num, f,
E, and ¯E are deﬁned in (12), (15), (16), (17), and (18), respectively.

Characteristics of Minimal Eﬀective Programming Systems
515
E3,j,0
Stage s + 1 = 4,j + 1,',− + 1.
Stage s' + 1 = 4,j + 1,'',− + 1.

Stage s'' + 1 = 4,0,− + 1.
m
3,j<1
E3,j,0 E3,j,1
3,j<2
E3,j,1 E3,j,2
3,j<3
E3,j,2 E3,j,3
3,j<4
E3,j,3 E3,j,4
3,j<5
E3,j,4 E3,j,5
3,j<6
E3,j,5 E3,j,6
3,j<7
E3,j,6 E3,j,7
3,j<8
E3,j,7
3,j<2
3,j<2
3,j<4
3,j<4
3,j<6
3,j<6
3,j<8
3,j<8
E3,j,0
E3,j,0
E3,j,1
E3,j,1
E3,j,2
E3,j,2
E3,j,3
E3,j,3
3,j<4
3,j<4
3,j<8
3,j<8
E3,j,0
E3,j,0
E3,j,1
E3,j,1

m
E3,j,0
E3,j,0
E3,j,1
Stage 0.
E3,j,1
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
 
0 
     
s + 1
     
s + 1
     
s + 1
     
s + 1
     
s + 1
     
s + 1
     
s + 1
     
s + 1
      
s' + 1
      
s' + 1
      
s' + 1
      
s' + 1
       
s'' + 1
       
s'' + 1
       
s'' + 1
       
s'' + 1
Fig. 3. A depiction of what could happen in the proof of Theorem 7 with respect to
the ψ-programs of the form f3,j(k), where j is arbitrary and k < 16 (see text)
The partial function ψ is constructed in Figure 2. To help to give some of
the intuition behind the construction, Figure 3 depicts what could happen with
respect to the ψ-programs of the form f3,j(k), where j is arbitrary and k < 16.
In stage 0, the programs will form eight pairs of equivalence classes, where the
kth pair computes ⟨3, j⟩<k+1 (the ﬁrst such pair being the 0th). If, subsequently,
the conditions of some stage s of the form ⟨4, j + 1, ℓ′, −⟩are satisﬁed, then, in
stage s + 1, the programs will form four pairs of equivalence classes, where the
kth pair computes ⟨3, j⟩<2k+2. If, similarly, the conditions of some stage s′ of
the form ⟨4, j + 1, ℓ′′, −⟩are satisﬁed (where ℓ′ ̸= ℓ′′), then, in stage s′ + 1, the
programs will form two pairs of equivalence classes, where the kth pair computes
⟨3, j⟩<4k+4. If, ﬁnally, the conditions of some stage s′′ of the form ⟨4, 0, −⟩are
satisﬁed, then, in stage s′′+1, the equivalence classes will alternate in computing
αℓand αm, for some distinct ℓand m.
≈□(Theorem 7)
Acknowledgements. The author would like to thank James S. Royer for many
helpful conversations relevant to this paper.

516
S.E. Moelius III
References
1. Ershov, Y.L.: On computable enumerations. Algebra and Logic 7(5), 330–346
(1968)
2. Freivalds, R., Kinber, E.B., Wiehagen, R.: Inductive inference and computable one-
one numberings. Zeitschrift f¨ur Mathematische Logik und Grundlagen der Mathe-
matik 28(27-32), 463–479 (1982)
3. Friedberg, R.M.: Three theorems on recursive enumeration. I. Decomposition.
II. Maximal Set. III. Enumeration without duplication. Journal of Symbolic
Logic 23(3), 309–316 (1958)
4. Gao, S., Gerdes, P.: Computably enumerable equivalence relations. Studia Log-
ica 67(1), 27–59 (2001)
5. Goncharov, S., Yakhnis, A., Yakhnis, V.: Some eﬀectively inﬁnite classes of enu-
merations. Annals of Pure and Applied Logic 60(3), 207–235 (1993)
6. Herrmann, E., Kummer, M.: Diagonals and D-maximal sets. Journal of Symbolic
Logic 59(1), 60–72 (1994)
7. Jain, S., Stephan, F., Teutsch, J.: Index sets and universal numberings. Journal of
Computer and System Sciences 77(4), 760–773 (2011)
8. Khutoretskii, A.B.: On the reducibility of computable numerations. Algebra and
Logic 8(2), 145–151 (1969)
9. Khutoretskii, A.B.: Two existence theorems for computable numerations. Algebra
and Logic 8(4), 277–282 (1969)
10. Kummer, M.: A note on direct sums of Friedbergnumberings. Journal of Symbolic
Logic 54(3), 1009–1010 (1989)
11. Kummer, M.: An easy priority-free proof of a theorem of Friedberg. Theoretical
Computer Science 74(2), 249–251 (1990)
12. Lavrov, I.A.: Computable numberings. In: Logic, Foundations of Mathematics and
Computability Theory, pp. 195–206 (1977)
13. Mal’cev, A.I.: Positive and negative numerations. Proceedings of the USSR
Academy of Sciences 160(2), 278–280 (1965)
14. Mal’cev, A.I.: Positive and negative numberings. In: The Metamathematics of Al-
gebraic Systems. Studies in Logic and the Foundations of Mathematics, vol. 66,
pp. 379–383. Elsevier (1971); Translated by B.F. Wells III
15. Moelius III, S.E.: Characteristics of minimal eﬀective programming systems. Sub-
mitted to arXiv (2012)
16. Machtey, M., Winklmann, K., Young, P.: Simple G¨odel numberings, isomorphisms,
and programming properties. SIAM Journal on Computing 7(1), 39–60 (1978)
17. Pour-El, M.B.: G¨odel numberings versus Friedberg numberings. Proceedings of the
American Mathematical Society 15(2), 252–256 (1964)
18. Riccardi, G.A.: The independence of control structures in abstract programming
systems. Journal of Computer and System Sciences 22(2), 107–143 (1981)
19. Rogers Jr., H.: G¨odel numberings of partial recursive functions. Journal of Symbolic
Logic 23(3), 331–341 (1958)
20. Rogers Jr., H.: Theory of Recursive Functions and Eﬀective Computability. Mc-
Graw Hill (1987); Reprinted. MIT Press (1967)
21. Royer, J.S.: A Connotational Theory of Program Structure. LNCS, vol. 273.
Springer, Heidelberg (1987)
22. Schinzel, B.: On decomposition of G¨odelnumberings into Friedbergnumberings.
Journal of Symbolic Logic 47(2), 267–274 (1982)
23. Spreen, D.: Computable one-to-one enumerations of eﬀective domains. Information
and Computation 84(1), 26–46 (1990)

After Turing: Mathematical Modelling
in the Biomedical and Social Sciences
From Animal Coat Patterns to Brain Tumours
to Saving Marriages
James D. Murray
1 Centre for Mathematical Biology, Mathematical Institute, 24-29 St Giles’, Oxford,
OX1 3LB, England
2 Applied & Computational Mathematics, Ecology & Evolutionary Biology,
Princeton University Princeton NJ 085440-1003, United States of America
Abstract. Turing’s 1952 paper on reaction diﬀusion models for spatial
pattern formation was important in the early development of the appli-
cation of mathematical modelling in biology and medicine. We describe
here three very diﬀerent problems which have been studied in depth and
which have proved informative and useful in understanding speciﬁc phe-
nomena. We describe an early study of a reaction diﬀusion model which
helped explain the diverse coat patterns observed on animal coats. We
then describe a basic, but surprisingly informative and accurate model,
currently used medically, for quantifying the growth of gliomablastoma
brain tumours. It enhances imaging techniques beyond any brain scan-
ning procedure currently available and is used to estimate patient life
expectancy and explain some current patient brain tumour anomalies.
Finally we describe a modelling example from the social sciences, which
quantiﬁes marital interaction which was used to predict divorce with sur-
prising accuracy and has helped design a new scientiﬁc marital therapy
which is currently used.
1
Introduction
The rediscovery of Turing’s 1952 paper in the late 1960’s had a major inﬂu-
ence in the development of mathematical biology which grew rapidly from the
mid-1970’s. Mathematical modelling is now used in practically every ﬁeld in the
biomedical sciences with the involvement in the social sciences clearly an impor-
tant growth ﬁeld of the near future. Much of the research in the late 20th century
was on biological pattern formation. With the explosion of genetic studies the
belief that genetics would solve all these developmental problems has certainly
not been borne out. The underlying mechanisms involved in developmental biol-
ogy and medicine are still largely unknown and is a major interdisciplinary area
of mathematics in the biological sciences.
A survey of just some of the early work in this important interdisciplinary
ﬁeld, now known as mathematical biology, or theoretical biology, or systems
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 517–527, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

518
J.D. Murray
biology, and its remarkable growth since the early late 1960’s and 1970’s is given
in an increasing number of books necessarily becoming ever more specialised. By
the mid-1980’s it was becoming more widely accepted that any real contribution
to the biological sciences from modelling must be genuinely interdisciplinary and
hence related to real biology.
2
How the Leopard Gets Its Spots
Turing’s contribution to mathematical biology is The Chemical Basis of Mor-
phogenesis [32] which has been seminal in several areas of spatial patterning
modelling in development, ecology and other biological areas since its rediscov-
ery in the late 1960’s. He did not apply his model to any speciﬁc biological
problem.
Turing showed how a system of reacting chemicals which can also diﬀuse,
could generate a steady state heterogeneous spatial pattern of chemical con-
centrations. He called these chemicals morphogens. He hypothesized that these
morphogenetic prepatterns could cue cell diﬀerentiation and result in observed
spatial patterns. His model is encapsulated in the coupled system of reaction
diﬀusion equations, of the general form,
∂u
∂t = γf(u, v) + Du∇2u
∂v
∂t = γg(u, v) + Dv∇2v
(1)
where the functions f(u, v) and g(u, v) denote the reaction kinetics associated
with the chemicals u, called the activator, and v, the inhibitor with Du and Dv
the diﬀusion coeﬃcients of u and v respectively. The parameter, γ, arises when
the system is written in this nondimensional form: it is a measure of domain
scale. A stability analysis of the steady states of the kinetics shows that to
generate spatial patterns in u and v it is necessary, among other things, that the
inhibitor has a higher diﬀusion rate than the activator, that is Dv > Du (cf.,
e.g., [15, 16]). A review article by [11] is speciﬁcally devoted to Turing’s theory.
A speciﬁc and the ﬁrst experimental reaction diﬀusion mechanism [30] was
used to study how animal coat patterns and butterﬂy wing patterns might be
formed [12–14]. The [30] reaction terms used in (1) were
f(u, v) = a −u −h(u, v)
g(u, v) = α(b −v) −h(u, v)
h(u, v) =
ρuv
1+u+Ku2
(2)
where a, b, α, ρ and K are constants the values of which were chosen which
result in steady state spatially heterogeneous solutions. These parameters were
kept ﬁxed for all the calculations. Only the scale and geometry of the domain
were varied in the analysis. For a given domain size and geometry each set of
initial conditions gave a qualitatively similar but unique pattern, a fact reﬂected
in nature.
It was shown [12, 14] that a single pre-patterning mechanism was capable of
generating the typical geometry of mammalian coat patterns, from the mouse to

After Turing: Mathematical Modelling in the Biomedical and Social Sciences
519
the elephant and almost everything in between, with the ﬁnal pattern governed
simply by the size and shape of the embryo at the time the pattern forma-
tion process was initiated. In solving the reaction diﬀusion partial diﬀerential
equation systems the domain size and shape is crucial in inﬂuencing the spatial
patterns obtained. For a given mechanism if you try and simulate solutions in a
very small domain it is not possible to obtain steady state spatial patterns. A
minimum size is needed to drive any sustainable spatial pattern. As the size of
the domain is increased, a series of increasingly complex spatial patterns emerge.
The concept behind the model is that the simulated spatial patterns solu-
tions of a reaction diﬀusion mechanism reﬂect the ﬁnal morphogen melanin pat-
terning observed on animal coats. With this scenario the cells react to a given
level in morphogen concentration, thus producing melanin (or rather becoming
melanocytes - cells which produce melanin). In Figures 1(a) and 1(c) the black
regions represent levels of melanin concentration higher than the uniform steady
state. It should be emphasized that this model is a hypothetical one which has
not been veriﬁed experimentally biologically but certainly circumstantially. Such
an approach has also been used in many patterning processes such as in butterﬂy
wing patterns [13, 15, 19] the last of which presents experimental conﬁrmation
of their theoretical predictions.
The solutions of the reaction diﬀusion system (1) and (2) in domains shown in
Figure 1(a) were ﬁrst computed as an example of how the geometry constrains
the possible pattern modes. When the domain is very narrow, only simple, es-
sentially one-dimensional, modes can exist. Two-dimensional patterns require
the domain to have enough two dimensionality. With a tapering cylinder as in
Figure 1(a) if the radius at one end is large enough, two-dimensional patterns
can exist on the surface. So, a tapering cylinder can exhibit a gradation from a
two-dimensional pattern to simple stripes as illustrated in Figure 1(a).
This shows that the conical domain mandates that it is not possible to have a
tail with spots at its tip and stripes at its base, but only the converse: Figure 1(a)
shows some examples of speciﬁc animal tails. This phenomenon is a genuine
example of a developmental constraint. Cheetahs, of which a photo of one is
shown in Figure 1(b), are prime examples of this, as well as other spotted animals
such as genets. If the threshold level of morphogen is diﬀerent, a diﬀerent but
related pattern can develop. In this way unique, but globally similar, patterns
can be formed and could be the explanation for the diﬀerent types of patterns on
diﬀerent species of the same animal genre, such as the giraﬀe. Because the initial
conditions for an individual animal involve a limited randomness it implies the
uniqueness of each animal pattern in the same species.
The interpretation of Figure 1(c) is that if the animal embryo is too small
when the patterning mechanism is activated, as in the mouse, or too large, as
in the hippopotamus and elephant for example, then no clear pattern will be
observed and these animals are essentially uniform in color.
There have been numerous developments and an increased understanding of
how coat patterns on animals, ﬁsh and butterﬂies, for example, are formed with
the addition and combination of other pattern forming mechanisms, such as

520
J.D. Murray
(a)
(b)
(c)
(d)
Fig. 1. (a) Examples of a developmental constraint. Spotted animals can have striped
tails but not the other way round. From left to right are typical of the tail of the leopard,
the cheetah and the genet together with the solutions from a reaction diﬀusion system
which can generate steady state spatial patterns. The geometry and scale when the
pattern mechanism is activated play crucial roles in the resulting coat patterns. Dark
regions represent areas of high morphogen concentration. (Tail art work reproduced
from [14] with permission of Patricia Wynne). (b) A cheetah (Acinonyx jubatus) which
is an example of the developmental constraint described in (a). (Photograph courtesy of
Professor Andrew Dobson). (c) Numerical simulations of the reaction diﬀusion model
analysis for the generation of general coat markings on an illustrative domain. The
model parameters were also the same; only the scale parameter, γ, was varied. The
domain sizes have been reduced to ﬁt in a single ﬁgure but in the simulations there
was a scale factor of 1,000 between the smallest and the largest ﬁgure. (d) Belted
Galloway cows are examples of the second bifurcation (Photograph courtesy of Allan
Wright). Cf. [14–16] for numerous other examples.
chemotaxis whereby there is movement of cells up chemical gradients. There are
numerous review articles and books, for example, [21] who consider ﬁsh stripes,
[10, 25, 9] who discuss evolving ﬁsh patterns among other patterned species.
By the end of the 1970s it was becoming clear that a major problem with
reaction diﬀusion models of pattern formation was the paucity of any real bio-
logical system which conﬁrmed or otherwise the chemical basis of morphogenesis.

After Turing: Mathematical Modelling in the Biomedical and Social Sciences
521
This gave rise in the early 1980s to the new experimentally veriﬁable approach
to modeling morphogenesis, namely the mechanical theory in [20] and which is
discussed at length with numerous applications, the predictions of which were
conﬁrmed by experiment (cf. [15, 16] for a detailed survey).
3
Brain Tumours: Enhancing Imaging Techniques,
Quantifying Therapy Eﬃcacy and Estimating Patient
Life Expectancy
High grade glioblastoma brain tumours are the most aggressive brain tumours
and are always fatal with an approximate median life expectancy of 9-12 months.
In spite of increasing accuracy of imaging techniques they still cannot detect can-
cer cell densities suﬃciently accurately. A practical model, which encompasses
the two key elements in the growth of glioblastoma brain tumours, namely the
invasive diﬀusive properties of the cancer cells and their growth rate, is given by
∂c
∂t = ∇.D(x)∇c + ρc
(3)
where c(x, t) is the tumour cell density, cells/mm3, at position, x, in the brain
at time, t (months), and D(x) is diﬀusion (mm2/month) which quantiﬁes the
invasiveness of the cancer cells at position x in the brain. ρ is the proliferation
rate (/month) of the cancer cells which gives the turnover time as (log2)/ρ
(months). Diﬀusion in grey matter is smaller than in white matter.
Solutions of (3) are unbounded in time because of the form of the growth term
which implies exponential growth but in the time scale relevant to glioma growth
and patient survival time it does not contribute signiﬁcantly to the solutions
relevant to patients.
In the original model [5] the brain was considered to be homogeneous mat-
ter bounded by the ventricles and skull. Even with such a simple anatomical
model the tumour growth predictions of the analysis were broadly in line with
patient observation of both low and high grade brain tumours. The limitations
of current imaging techniques were clear. The model was used to mimic various
accepted medical treatments, speciﬁcally radiation, surgical resection (removal)
[33, 29] and chemotherapy [31, 26, 23]. A three dimensional model was proposed
and studied in [1] who were the ﬁrst to demonstrate that cancer cell diﬀusion,
mainly ignored up to that time, is a major component of glioma growth. They
showed that only those tumours with a low diﬀusion rate could beneﬁt from
wide surgical resection although eventually there will be multifocal recurrence
as found clinically [24]: cf. [16] for a full discussion and review which encompasses
anatomically correct brains.
4
Virtual Gliomas for Anatomically Correct Brains:
Enhanced Imaging and Current Limitations
A major advance in the practical application of the model (3) was the availability
of the brain web atlas [3]. This allowed the model to be applied to anatomically

522
J.D. Murray
correct brains [27, 26, 29, 28, 16, 17]. Among other things it made it possible to
reﬁne the gross anatomic boundaries and to vary the degree of motility of glioma
cells depending on whether it was in grey or white matter. With the BrainWeb
Atlas it was possible to solve equation (3) in a three dimensional anatomically
correct brain in which the grey and white matter are clearly delineated.
The patient speciﬁc procedure is to evaluate the tumour size from brain scans
and, what is essential, estimate the parameter values for each patient [27] to
obtain the average diﬀusion coeﬃcient and the average growth rate. There is a
threshold of detection of cancer cells with all imaging techniques, whether by
imaging or microscopic studies. To use the predictive potential of the mathe-
matical model (3) for preliminary predictions, serial imaging of the tumour was
used to calculate its volume which was then taken as the volume of an equivalent
sphere with radius r, namely 4πr3
3 . We then consider the model to be radially
symmetric with a constant diﬀusion coeﬃcient, based on averaging the values
from imaging. Equation (3) then becomes
∂c
∂t = D
∂2c
∂r2 + 2∂c
r∂r

+ ρc
(4)
We consider that at time t = 0 there is a concentrated number of cancer cells,
N cells/mm3, at r = 0 in which case the analytical solution of (4) is given by
c(r, t) = Nexp(ρt −
r2
4Dt)
8(πDt)
3
2
(5)
If the smallest level of image detection is denoted by c1 cells/mm3, then the
radius, r, of the tumour for this cell density is obtained from (5) which asymp-
totically gives
r ∼2t

Dρ ⇒v = r/t ∼2

Dρ
(6)
where v is the velocity of tumour growth. That is, the equivalent radial growth
of the tumour is linear in time a ﬁnding conﬁrmed by patient data: cf., e.g., [16].
5
Approximate in vivo Patient Survival Time
Based on accepted medical practice, we consider detection is when the spherical
equivalent tumour volume is of radius 15mm and that death occurs when the ra-
dius is 30mm. The approximate survival time, tsurvival (months), from detection,
in the absence of any treatments, is, from (6),
tsurvival = tr=30 −tr=15 =
7.5
√Dρ
(7)
Typical growth rates vary widely, approximately from 1-5 /month and diﬀusion
rates from 1-8 mm2/month. The medians for 9 patients in the study in [23] are

After Turing: Mathematical Modelling in the Biomedical and Social Sciences
523
D = 0.9mm2/month and ρ = 1.16/month which gives a median survival time of
7.34 months.
Simulations for an anatomically correct brain highlights the problems with
current imaging limitations. Figure 2 is a computed solution of (3) in a three
dimensional anatomically correct brain with the diﬀusion coeﬃcient spatially
dependent on whether or not it is in grey or white matter. It shows the detectable
tumour at death and the spread of the tumour cells beyond what can be detected
by the most accurate current CT or MRI, or any other, imaging techniques.
Fig. 2. Computed solutions of equation (3) in a three dimensional anatomically accu-
rate brain with typical patient parameter values. These show the horizontal section of
the virtual human brain through the site of the original tumour (+ in (a), * in (b)). The
left image in each is the tumour at original detection while the right image is the same
tumour at time of death. The thick black contour deﬁnes the edge of the tumour that
can be detected by enhanced computer tomography (CT). The blue contours outside
this black line represent the same cancer cell densities peripheral to the imaging limits.
(a) Tumour in grey matter: the time from diagnosis to death is approximately 256
days. (b) Tumour in white matter: the time from diagnosis to death is approximately
158 days. (Figures extracted from [27]).
The model described here has been used to quantify the eﬀect of diﬀerent
treatment eﬃcacies for individual patients prior to their use, such as [31, 26, 33].
The last of these predicted patient survival rates which compared surprisingly
accurately with the extant data at the time and recently published in [22]. A
full discussion of how such treatments are incorporated in the model and their
comparison with patient data is given in [16]. The model has been used to
determine when tumours start [17, 18].
6
Marital Interaction and Divorce Prediction
The rise in divorce rates in developed countries is widespread, important, but a
poorly understood phenomenon. What makes some marriages happy but others

524
J.D. Murray
miserable? Previous attempts at predicting marriage dissolution tended to be
based on mismatches in the couple’s personality or areas of disagreement: these
have not been too successful. The original model to quantify the interaction of
a couple discussing a problem of contention, which they chose, was proposed in
[4]: cf. also further developments in [6, 8, 7].
In [4] is developed, a simple, but surprisingly accurate predictive, mathemati-
cal model based on only a few variables descriptive of speciﬁc marital interaction
patterns, such as the diﬀerence between positivity and negativity and how each
partner is inﬂuenced by the other during a 15 minute (ﬁlmed) discussion of a
subject of ongoing contention [4]. These topics could be on any topic, for ex-
ample, money, in-laws, housing, sex, food or politics; the couple chose the topic
they discussed. A major part of the longitudinal study on which the model was
tested involved several hundred newly married couples. With the data, and the
model analysis of it, it was possible to identify patterns predictive as to whether
the couple would divorce or stay married happily or unhappily.
Based on the hypothesis that without a theoretical understanding of the pro-
cesses related to marital, or relationship, stability and dissolution, it is diﬃcult
to design and evaluate eﬀective relationship therapies. Using a psychological cod-
ing system to “score” each partner in a conﬂict conversation, the resultant time
series can visually describe the ebb and ﬂow of the interaction for a relation-
ship conversation. The scoring system used is SPAFF (Speciﬁc Aﬀect Scoring
system); cf., e.g., a full description in [2]. The system consists of scoring each
comment according to the speciﬁc scoring system. During the couple’s discus-
sion, of the topic of contention, an integer number between +4 and -4 is given
according to the sentiment expressed. For example, positive numbers are given
for aﬀection (+4), humour (+4), interest (+2), while negative numbers are given
for sadness (-1), anger (-1), contempt (-4), whining (-1), belligerence (-2) dis-
gust (-3), stonewalling (-2) and so on. There is thus a continual scoring of the
conversation as a function of time.
The model equations are:
Wt+1 = a + r1Wt + IHW (Ht)
Ht+1 = b + r2Ht + IWH(Wt)
(8)
where Wt and Ht are the scores of the wife and husband obtained from what
they said at time t. Consider the ﬁrst equation, the wife’s. The inﬂuence function
IHW (Ht) is the inﬂuence the husband has on the wife as a function of his score
when he speaks at time t and it reﬂects the inﬂuence he has when the wife
replies, namely at time t + 1. The parameter r1, is the inertia parameter, which
quantiﬁes how easy, or willing, it is for the wife to change her attitude reﬂected
in her score the last time she spoke, namely Wt, at time t. Here 0 ≤r1, r2 < 1.
For example, if r1 is small the wife is not ﬁxed on what she last said since this
term is small and is more willing to change her view. The parameter, a, reﬂects
how the wife feels about the marriage when the husband is not inﬂuencing her,
that is when IHW (Ht) = 0.
Although each couple has a unique relationship quantiﬁed by their own char-
acteristic model parameters, an analysis of the longitudinal study of couples only

After Turing: Mathematical Modelling in the Biomedical and Social Sciences
525
ﬁve dominant couple’s styles were found [4]. These are primarily based on the
inﬂuence functions, namely (i) validating, (ii) volatile, (iii) conﬂict-avoiding, (v)
hostile and (v) hostile-detached. Only 2 of these styles, (i) and (iii), are stable,
typically resulting in long-term happy relationships.
The ability to predict the longitudinal course of marital relationships using
this modelling approach, with an average of 94% accuracy based on several
hundred couples, has now been reported in the laboratories in four separate
longitudinal studies: cf. the book [7]. Here the model has been extended to
incorporate the eﬀect of a baby on marriages and also to committed cohabiting
gay male and lesbian relationships.
References
1. Burgess, P., Kulesa, P., Murray, J., Alvord, E.: The interaction of growth rates
and diﬀusion coeﬃcients in a three-dimensional mathematical model of gliomas. J.
Neuropathol. Exp. Neurol. 56, 704–713 (1997)
2. Coan, J., Gottman, J.: The Speciﬁc Aﬀect (SPAFF) coding system. In: Coan, J.,
Allen, J. (eds.) Handbook of Emotion Elicitation and Assessment, pp. 106–123.
Oxford University Press, New York (2007)
3. Cocosco, C., Kollokian, V., Kwan, R.K.-S., Evans, A.: Brain Web: Online Inter-
face to a 3D MRI Simulated Brain Database. In: Proceedings of 3rd International
Conference on Functional Mapping of the Human Brain, vol. 5 (1997)
4. Cook, J., Tyson, R., White, K., Rushe, R., Gottman, J., Murray, J.: Mathematics
of marital conﬂict: Qualitative dynamic mathematical modeling of marital inter-
action. J. Family Psychology 9, 110–130 (1995)
5. Cruywagen, G., Woodward, D., Tracqui, P., Bartoo, G., Murray, J., Alvord, E.:
The modelling of diﬀusive tumors. J. Biol. Systems 3, 937–945 (1995)
6. Gottman, J., Guralnick, M., Wilson, B., Swanson, C., Swanson, K., Murray, J.:
What should be the focus of emotion regulation in children? A nonlinear dynamic
mathematical model of children’s peer interaction in groups. Development & Psy-
chopathology 9(2), 421–452 (1997)
7. Gottman, J., Murray, J., Swanson, C., Tyson, R., Swanson, K.: The Mathematics
of Marriage: Dynamic Nonlinear Models. MIT Press, Cambridge (2002)
8. Gottman, J., Swanson, K., Murray, J.: The mathematics of marital conﬂict: dy-
namic mathematical nonlinear modeling of newlywed marital interaction. J. Family
Psychol. 13, 1–17 (1999)
9. Kondo, S., Iwashita, M., Yamaguchi, M.: How animals get their skin patterns: ﬁsh
pigment pattern as a live Turing wave. Inst. J. Dev. Biol. 53, 851–856 (2009)
10. Maini, P.: How the mouse got its stripes. Proc. Nat. Acad. Sci. 100, 9656–9657
(2003)
11. Maini, P.: Using mathematical models to help understand biological pattern for-
mation. C. R. Biologies 327, 225–234 (2004)
12. Murray, J.: A pre-pattern formation mechanism for animal coat markings. J. Theor.
Biol. 88, 161–199 (1981)
13. Murray, J.: On pattern formation mechanisms for lepidopteran wing patterns and
mammalian coat markings. Phil. Trans. Roy. Soc. (Lond.) B 295, 473–496 (1981)

526
J.D. Murray
14. Murray, J.: Mammalian coat patterns: How the leopard gets its spots. Scientiﬁc
American 256, 80–87 (1988)
15. Murray, J.: Mathematical Biology. Springer, Heidelberg (1989)
16. Murray, J.: Mathematical Biology: II. Spatial Models and Biomedical Applications,
3rd edn., vol. 2. Springer, New York (2003)
17. Murray, J.: On the Growth of Brain Tumours: enhancing imaging techniques, high-
lighting limitations of current imaging, quantifying therapy eﬃcacy and estimating
patient life expectancy. In: Lenaerts, T., Giacobini, M., Bersini, H., Bourgigne, P.,
Dorigo, M., Doursat, R. (eds.) Proceedings of the Eleventh European Conference
on the Synthesis and Simulation of Living Systems, Advances in Artiﬁcial Life,
ECAL 2011, pp. 23–26. MIT Press (2011)
18. Murray, J.: Glioblastoma brain tumours: Estimating the time from brain tumour
initiation and resolution of a patient survival anomaly after similar treatment pro-
tocols. J. Biol. Dyn. (in press, 2012)
19. Nijhout, N., Maini, P., Madzvamuse, A., Wathen, A., Sekimura, T.: Pigmentation
pattern formation in butterﬂies: experiments and models. C. R. Biologies 326,
717–727 (2003)
20. Oster, G., Murray, J., Harris, A.: Mechanical aspects of mesenchymal morphogen-
esis. J. Embryol. Exp. Morph. 78, 83–125 (1983)
21. Painter, K., Maini, P., Othmer, H.: Stripe formation in juvenile Pomacanthus ex-
plained by a generalized Turing mechanism with chemotaxis. Proc. Nat. Acad.
Sci. 96, 5549–5554 (1999)
22. Ramakrishna, R., Barber, J., Kennedy, G., Win, R.R., Ojemann, G., Berger, M.,
Spence, A., Rostomily, R.: Imaging features of invasion and preoperative and post-
operative tumor burden in previously untreated gliomablastomas: Correlation with
survival. Surg. Neurol. Int. 1, 40–51 (2010)
23. Rockne, R., Rockhill, J., Mrugala, M., Spence, A., Kalet, I.K., Hendrickson, K.,
Cloughesy, A.L., Alvord, E., Swanson, K.: Prediciitng the eﬃcacy of radiotherapy
in individual glioblastoma patients in viv: a mathematical modelling approach.
Phys. Med. Biol. 55, 3271–3285 (2010)
24. Silbergeld, D., Rostomily, R., Alvord, E.: The cause of death in patients with
glioblastomas is multifocal: Clinical factors and autopsy ﬁndings in 117 cases of
supratentorial glioblastomas in adults. J. Neuro-Oncol. 10, 179–185 (1991)
25. Suzuki, N., Hirata, M., Kondo, S.: Traveling stripes on the skin of a mutant mouse.
Proc. Nat. Acad. Sci. USA 100, 9680–9685 (2003)
26. Swanson, K., Alvord, E., Murray, J.: Quantifying eﬃcacy of chemotherapy of brain
tumors (gliomas) with homogeneous and heterogeneous drug delivery therapy. Acta
Biotheoretica 50(6), 223–237 (2002)
27. Swanson, K., Alvord, E., Murray, J.: Virtual brain tumours (gliomas) enhance the
reality of medical imaging and highlight inadequacies of current therapy. British J.
Cancer 86, 14–18 (2002); Abstracted and featured in the Year Book of the Institute
of Oncology Elsevier Science (2003)
28. Swanson, K., Alvord, E., Murray, J.: Virtual and real brain tumors: using math-
ematical modelling to quantify glioma growth and invasion. J. Neurological Sci-
ences 216(3), 1–10 (2003)
29. Swanson, K., Alvord, E., Murray, J.: Virtual resection of gliomas: eﬀect of extent of
resection on recurrence. Mathematical and Computer Modelling 37(11), 1177–1190
(2003)

After Turing: Mathematical Modelling in the Biomedical and Social Sciences
527
30. Thomas, D.: Artiﬁcial enzyme membranes, transport, memory, and oscillatory phe-
nomena. In: Thomas, D., Kernevez, J.P. (eds.) Analysis and Control of Immobilized
Enzyme Systems, pp. 115–150. Springer, Heidelberg (1975)
31. Tracqui, P., Cruywagen, G., Woodward, D., Bartoo, G., Murray, J., Alvord, E.:
A mathematical model of glioma growth: the eﬀect of chemotherapy on spatial-
temporal growth. Cell Prolif. 28, 17–31 (1995)
32. Turing, A.: The chemical basis of morphogenesis. Phil. Trans. Roy. Soc. B 237,
37–72 (1952)
33. Woodward, D., Cook, J., Tracqui, P., Cruywagen, G., Murray, J., Alvord, E.: A
mathematical model of glioma growth: the eﬀect of extent of surgical resection.
Cell Prolif. 29, 269–288 (1996)

Existence of Faster than Light Signals Implies
Hypercomputation already in Special Relativity
P´eter N´emeti and Gergely Sz´ekely
Alfr´ed R´enyi Institute of Mathematics, Hungarian Academy of Sciences, POB 127,
1364 Budapest, Hungary
{nemeti.peter,szekely.gergely}@renyi.mta.hu
Abstract. Within an axiomatic framework, we investigate the possibil-
ity of hypercomputation in special relativity via faster than light signals.
We formally show that hypercomputation is theoretically possible in spe-
cial relativity if and only if there are faster than light signals.
1
Introduction
The theory of relativistic hypercomputation (i.e., the investigation of relativ-
ity theory based physical computational scenarios which are able to solve non-
Turing-computable problems) has an extensive literature and it is investigated
by several researchers in the past decades, cf., e.g., [3, 4, 6, 7, 9, 12]. For an
overview of diﬀerent approaches to hypercomputation, cf., e.g., [24].
It is well-known that hypercomputation is not possible in special relativity in
the usual sense (i.e., the sense of Malament–Hogarth spacetimes), cf., e.g., [9]. In
this paper, we show that it is possible to perform relativistic hypercomputation
via ordinary computers (Turing machines) in special relativity if there are faster
than light (FTL) signals, e.g., particles. We shall also show that there have to
be FTL signals if relativistic hypercomputation is possible in special relativity
(via Turing machines), cf. Thm.2.
It is interesting in and of itself to investigate the (logical) consequences of
the assumption that FTL objects exist, independently of the question whether
they really exist or not in our actual physical universe. Logic based axiomatic
investigations typically aim for describing all the theoretically possible universes
and not just our actual one. Moreover, so far we have not excluded the possibility
of the existence of FTL entities in our actual universe; and from time to time
there appear theories and experimental results suggesting the existence of FTL
objects. Recently, the OPERA experiment, cf. [17], raised the interest in the
possibility of FTL particles.
Contrary to the common belief, the existence of FTL particles does not lead
to a logical contradiction within special relativity. For a formal axiomatic proof
of this fact, cf. [26]. However, it is interesting to note that, in contrast with this
result, the impossibility of the existence of FTL inertial observers follows from
special relativity, cf., e.g., [1].
The investigation of FTL motion in relativity theory goes back (at least) to
Tolman, cf., e.g., [28, p.54-55]. Since then a great many works dealing with FTL
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 528–538, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Hypercomputation in SR
529
motion have appeared in the literature, cf., e.g., [13, 15, 19–21, 23, 29] to mention
only a few.
2
Hypercomputation in SR
It is well-known that we can send information back to the past if there are FTL
particles, cf., e.g., [26] and [28, p.54-55]. It is natural to try using this possibil-
ity to design computers with greater computational power. We shall show that
uniformly accelerated relativistic computers can compute beyond the Church–
Turing barrier via using FTL signals. In this section, we show this fact informally.
In Sect.5, we reconstruct our informal ideas of this section within an axiomatic
theory of special relativity extended with accelerated observers.
Our ﬁrst observation is that if we can send out an FTL signal with a certain
speed, we also have to be able to send out arbitrarily fast signals, by the principle
of relativity. Prop.1 is a formal statement of this observation. To informally
justify this statement, let us assume that we can send out an FTL signal by a
certain experiment, say with speed 1.01c. According to special relativity, for any
FTL speed, say 1010c, there is a inertial reference frame (moving relative to our
frame) according to which our signal moves with this speed. By the principle of
relativity, inertial frames are experimentally indistinguishable, cf. [8, §5, pp.149-
159] and [27, pp.176-178]. So the experiment which is conﬁgured in our reference
frame as our original experiment is seen by this moving inertial frame as yielding
an FTL signal moving with speed 1010c in our frame. Therefore, in our (or any
other inertial) reference frame, it is possible to send out an FTL signal with any
speed.
Let us see the construction of our special relativistic hypercomputer. Let the
computer be accelerated uniformly with respect to an inertial observer, cf. Fig.1.1
There is an event O with the following property: any event E on the worldline
of our uniformly accelerated computer is simultaneous with O, according to the
inertial observer comoving with the computer at E, cf., e.g., [14, Fig.6.4, p.173],
and [18, Fig.5.13, p.152].
Now let us show that this conﬁguration can be used to decide non-Turing-
computable questions if there are FTL signals. Let us set the computer to work
on some recursively enumerable but non-Turing-computable problem, say the
decision problem for the consistency of ZF set theory; the computer enumerates
one by one all the consequences of ZF. Let us ﬁx an event M on the worldline of
the programmer which is later than O according to him. Now, if the computer
ﬁnds a contradiction, let it send out a fast enough signal which reaches the
programmer before event M. Such signal exists since, by our ﬁrst observation,
the computer can send out a signal which is arbitrarily fast with respect to his
coordinate system (i.e., any half line in the “upper” half space determined by the
comoving observer’s simultaneity can be the worldline of the signal). Therefore,
if the programmer receives a signal between events O and M, he knows that ZF
1 In relativity theory, uniform acceleration means motion along a hyperbola (according
to inertial observers), cf., e.g., [5, §3.8, pp.37-38], [14, §6], and [22, §12.4, pp.267-272].

530
P. N´emeti and G. Sz´ekely
FTL signal
Computer
Programmer
O
M
Comoving observer at 2
Simultaneity of the comoving
observer at 2
Simultaneity of the comoving
observer at 1
0
1
2
3
Fig. 1. Illustration of hypercomputation via FTL particles
is inconsistent; and if there is no signal between M and O, he knows that the
computer has not found any contradiction, so after event M the programmer
can conclude that there is no contradiction in ZF set theory. The same way, by
this thought experiment using FTL signals, we can decide (experimentally) any
recursively enumerable set of numbers.
If there are no FTL signals, then the whole computation has to happen in the
causal past of the event when the programmer learns the result of computation.
However, in special relativity, the computer remaining within the causal past of
any event has only ﬁnite time to compute by the twin paradox theorem. That is
why hypercomputation is not possible in special relativity without FTL signals.
This argument is also the basis of proving that Minkowski spacetime is not a
Malament–Hogarth spacetime.
3
The Language of Our Axiom Systems
To formalize the result of Sect.2, we need an axiomatic theory of special relativity
extended with accelerated observers. Here we shall use the following two-sorted
language of ﬁrst-order logic parametrized by a natural number d ≥2 representing
the dimension of spacetime:
{ B, Q ; Ob, IOb, Ph, +, ·, ≤, W },
where B (bodies) and Q (quantities) are the two sorts, Ob (observers), IOb
(inertial observers) and Ph (light signals) are one-place relation symbols of sort
B, + and · are two-place function symbols of sort Q, ≤is a two-place relation
symbol of sort Q, and W (the worldview relation) is a d+2-place relation symbol
the ﬁrst two arguments of which are of sort B and the rest are of sort Q.

Hypercomputation in SR
531
Relations Ob(o), IOb(m) and Ph(p) are translated as “o is an observer,” “m is
an inertial observer,” and “p is a light signal,” respectively. To speak about co-
ordinatization, we translate W(k, b, x1, x2, . . . , xd) as “body k coordinatizes body
b at space-time location ⟨x1, x2, . . . , xd⟩,” (i.e., at space location ⟨x2, . . . , xd⟩and
instant x1).
To make them easier to read, we omit the outermost universal quantiﬁers
from the formalizations of our axioms, i.e., all the free variables are universally
quantiﬁed.
We use the notation Qd for the set of all d-tuples of elements of Q. If ¯x ∈Qd,
we assume that ¯x = ⟨x1, . . . , xd⟩, i.e., xi denotes the i-th component of the d-
tuple ¯x. Specially, we write W(m, b, ¯x) in place of W(m, b, x1, . . . , xd), and we
write ∀¯x in place of ∀x1 . . . ∀xd, etc.
4
Axioms of Special Relativity
Let us recall some of our axioms for special relativity. Our ﬁrst axiom states some
basic properties of addition, multiplication and ordering true for real numbers.
AxOField: The quantity part ⟨Q, +, ·, ≤⟩is an ordered ﬁeld.
In the next axiom, we shall use the concepts of time diﬀerence and spatial dis-
tance. The time diﬀerence of coordinate points ¯x, ¯y ∈Qd is deﬁned as:
time(¯x, ¯y) := x1 −y1.
To speak about the spatial distance of any two coordinate points, we have to
use squared distance since it is possible that the distance of two points is not
amongst the quantities, e.g., the distance of points ⟨0, 0⟩and ⟨1, 1⟩is
√
2. So in
the ﬁeld of rational numbers, ⟨0, 0⟩and ⟨1, 1⟩do not have distance just squared
distance. Therefore, we deﬁne the squared spatial distance of ¯x, ¯y ∈Qd as:
space2(¯x, ¯y) := (x2 −y2)2 + . . . + (xd −yd)2.
Our next axiom is the key axiom of our axiom system of special relativity. This
axiom is the outcome of the Michelson–Morley experiment, and it has been
continuously tested ever since then. Nowadays it is tested by GPS technology.
AxPh : For any inertial observer, the speed of light is the same everywhere and
in every direction (and it is ﬁnite). Furthermore, it is possible to send out a
light signal in any direction everywhere:
IOb(m) →∃cm

cm > 0 ∧∀¯x¯y

space2(¯x, ¯y) = c2
m · time(¯x, ¯y)2
↔∃p

Ph(p) ∧W(m, p, ¯x) ∧W(m, p, ¯y)

.
Let us note here that AxPh does not require (by itself) that the speed of light
is the same for every inertial observer. It requires only that the speed of light

532
P. N´emeti and G. Sz´ekely
according to a ﬁxed inertial observer is a positive quantity which does not depend
on the direction or the location. However, by AxPh, we can deﬁne the speed of
light according to inertial observer m as the following binary relation:
c(m, v)
def
⇐⇒v > 0 ∧∀¯x¯y

∃p

Ph(p) ∧W(m, p, ¯x) ∧W(m, p, ¯y)

→space2(¯x, ¯y) = v2 · time(¯x, ¯y)2
.
By AxPh, there is one and only one speed v for every inertial observer m such
that c(m, v) holds. From now on, we shall denote this unique speed by cm.
Our next axiom connects the worldviews of diﬀerent inertial observers by
saying that they coordinatize the same “external” reality (the same set of events).
By the event occurring for observer m at coordinate point ¯x, we mean the set
of bodies m coordinatizes at ¯x:
evm(¯x) := {b : W(m, b, ¯x)}.
AxEv: All inertial observers coordinatize the same set of events:
IOb(m) ∧IOb(k) →∃¯y ∀b

W(m, b, ¯x) ↔W(k, b, ¯y)

.
From now on, we shall abbreviate the subformula ∀b

W(m, b, ¯x) ↔W(k, b, ¯y)

of AxEv to evm(¯x) = evk(¯y). The next two axioms are only simplifying ones.
AxSelf: Any inertial observer is stationary relative to himself:
IOb(m) →∀¯x

W(m, m, ¯x) ↔x2 = . . . = xd = 0

.
AxSymD: Any two inertial observers agree as to the spatial distance between two
events if these two events are simultaneous for both of them. Furthermore,
the speed of light is 1 for all observers:
IOb(m) ∧IOb(k) ∧x1 = y1 ∧x′
1 = y′
1 ∧evm(¯x) = evk(¯x′)
∧evm(¯y) = evk(¯y′) →space2(¯x, ¯y) = space2(¯x′, ¯y′), and
IOb(m) →∃p

Ph(p) ∧W(m, p, 0, . . . , 0) ∧W(m, p, 1, 1, 0, . . ., 0)

.
Our axiom system SpecRel is the collection of the ﬁve simple axioms above:
SpecRel := {AxOField, AxPh, AxEv, AxSelf, AxSymD}.
To show that SpecRel captures the kinematics of special relativity, let us intro-
duce the worldview transformation between observers m and k (in symbols,
wmk) as the binary relation on Qd connecting the coordinate points where m
and k coordinatize the same (nonempty) events:
wmk(¯x, ¯y)
def
⇐⇒evm(¯x) = evk(¯y) ̸= ∅.

Hypercomputation in SR
533
Map P : Qd →Qd is called a Poincar´e transformation iﬀit is an aﬃne bijection
such that, for all ¯x, ¯y, ¯x′, ¯y′ ∈Qd for which P(¯x) = ¯x′ and P(¯y) = ¯y′,
time(¯x, ¯y)2 −space2(¯x, ¯y) = time(¯x′, ¯y′)2 −space2(¯x′, ¯y′).
Thm.1 shows that our streamlined axiom system SpecRel perfectly captures the
kinematics of special relativity since it implies that the worldview transforma-
tions between inertial observers are the same as in the standard non-axiomatic
approaches. For the proof of Thm.1, cf. [2].
Theorem 1. Let d ≥3. Assume SpecRel. Then wmk is a Poincar´e transforma-
tion if m and k are inertial observers.
The so-called worldline of body b according to observer m is deﬁned as:
wlm(b) := {¯x : W(m, b, ¯x)}.
Corollary 1. Let d ≥3. Assume SpecRel. The wlm(k) is a straight line if m
and k are inertial observers.
To extend SpecRel to accelerated observers, we need further axioms. We connect
the worldviews of accelerated and inertial observers by the next axiom.
AxCmv: At each moment of its world-line, each observer coordinatizes the nearby
world for a short while as an inertial observer does.
Axiom AxCmv is captured by formalizing the following statement: at each point
of the worldline of an observer there is an inertial comoving observer such that
the derivative of the worldview transformation between them is the identity
map, cf., e.g., [1] [25, §6] for details. We shall also use the generalized (localized)
versions of axioms AxEv and AxSelf of SpecRel assumed for every observer.
AxEv−: Observers coordinatize all the events in which they participate:
Ob(k) ∧W(m, k, ¯x) →∃¯y evm(¯x) = evk(¯y).
AxSelf−: In his own worldview, the worldline of any observer is an interval of
the time axis containing all the coordinate points of the time axis where the
observer coordinatizes something:

W(m, m, ¯x) →x2 = . . . = xd = 0

∧

W(m, m, ¯y) ∧W(m, m, ¯z) ∧y1 < t < z1 →W(m, m, t, 0, . . . , 0)

∧
∃b

W(m, b, t, 0, . . . , 0) →W(m, m, t, 0, . . . , 0)

.
Let us add these three axioms to SpecRel to get a theory of accelerated observers:
AccRel0 := SpecRel ∪{AxCmv, AxEv−, AxSelf−}.

534
P. N´emeti and G. Sz´ekely
Since AxCmv ties the behavior of accelerated observers to the inertial ones and
SpecRel captures the kinematics of special relativity perfectly by Thm.1, it is quite
natural to think that AccRel0 is a theory strong enough to prove the most fun-
damental theorems about accelerated observers. However, AccRel0 does not even
imply the most basic predictions of relativity theory about accelerated observers,
such as the twin paradox. Moreover, it can be proved that even if we add the whole
ﬁrst-order logic theory of real numbers to AccRel0 is not enough to get a theory
that implies (predicts) the twin paradox, cf., e.g., [11] and [25, §7].
In the models of AccRel0 in which the twin paradox is not true, there are some
deﬁnable gaps in W. Our next assumption excludes these gaps.
CONT: Every parametrically deﬁnable, bounded and nonempty subset of Q has
a supremum (i.e., least upper bound) with respect to ≤.
In CONT, “deﬁnable” means “deﬁnable in the language of AccRel, parametri-
cally.” For a precise formulation of CONT, cf. [11, p.692] or [25, §10.1]. When Q
is the ordered ﬁeld of real numbers, CONT is automatically true.
Let us extend AccRel0 with axiom schema CONT:
AccRel := AccRel0 ∪CONT.
It can be proved that AccRel implies the twin paradox, cf. [11] and [25, §7.2].
Let us now introduce some auxiliary axioms we shall use here but not listed so
far. To do so, let us call a linear bijection of Qd trivial transformation if leaves
the time components (i.e., ﬁrst coordinates) of coordinate points unchanged and
it ﬁxes the points of the time axis, i.e., the set of trivial transformation is:
Triv := { T : T is a linear bijection of Qd,
T (¯y)1 = ¯y1 and T(¯x) = ¯x if ¯xs = ¯o },
where ¯o denotes the origin, i.e., coordinate point ⟨0, . . . , 0⟩.
AxThExp#: Inertial observers can move with any speed less than the speed of
light and new inertial reference frames can be constructed from other iner-
tial reference frames by transforming them by trivial transformations and
translations along the time axis:
∃h IOb(h) ∧

IOb(m) ∧space2(¯x, ¯y) < c2
m · time(¯x, ¯y)2
∧T ∈Triv →∃km′
IOb(k) ∧IOb(m′) ∧W(m, k, ¯x) ∧W(m, k, ¯y)
∧evm(¯x) = evk(¯o) ∧wmm′ = T

.
The following axiom is a consequence of the principle of relativity. Cf. [10, 26]
for a formalization of the principle of relativity in our ﬁrst-order logic language.
AxVel: If one observer can send out a body with a certain speed in a certain
direction, then any other inertial observer can send out a body with this
speed in this direction.

Hypercomputation in SR
535
IOb(m) ∧IOb(k) →

∃b

W(m, b, ¯x) ∧W(m, b, ¯y)

↔∃b

W(k, b, ¯x) ∧W(k, b, ¯y)

.
We call body b inertial body iﬀthere is an inertial observer m according to
who b moves with uniform rectilinear motion:
IB(b)
def
⇐⇒∃m¯x¯y

IOb(m) ∧¯x ̸= ¯y ∧W(m, b, ¯x) ∧W(m, b, ¯y)∧
∀¯z

W(m, b, ¯z) ↔∃λ

¯z = ¯x + λ(¯y −¯x)
	
.
Let us now formulate the possibility of the existence of FTL inertial bodies.
∃FTLBody: There is an inertial observer who can send out an FTL inertial body:
∃mb¯x¯y

IB(b) ∧IOb(m) ∧W(m, b, ¯x) ∧W(m, b, ¯y)∧
space2(¯x, ¯y) > c2
m · time(¯x, ¯y)2
.
∃FTLBody implies that inertial observers can send out a body with arbitrary
large speed in any direction if SpecRel, AxThExp#, CONT and AxVel are assumed:
Proposition 1. Let d ≥3. Assume SpecRel, AxThExp#, CONT, AxVel and
∃FTLBody. Then any inertial observer can send out a body with any speed in
any direction:
IOb(m) →∃b

W(m, b, ¯x) ∧W(m, b, ¯y)

.
For the proof of Prop.1, cf. [16, §6].
5
Hypercomputation in AccRel
In this section, we formulate our statement on the logical equivalence between
the existence of FTL signals and the possibility of hypercomputation in special
relativity as a theorem in our ﬁrst-order logic language. To formulate the pos-
sibility of hypercomputation as a formula of our ﬁrst-order logic language, let
us deﬁne the life-curve lcm(k) of observer k according to observer m as the
world-line of k according to m parametrized by the time measured by k, formally:
lcm(k) := { ⟨t, ¯x⟩∈Q × Qd : ∃¯y k ∈evk(¯y) = evm(¯x) ∧y1 = t }.
The range and domain of a binary relation R, is deﬁned as:
Ran R := { y : ∃x R(x, y) }
and
Dom R := { x : ∃y R(x, y) }.
The following formula of our language captures the possibility of relativistic
hypercomputation in the sense used in the theory of relativistic computation.

536
P. N´emeti and G. Sz´ekely
HypComp: There are two observers a programmer p and a computer c and an
instant τ in the programmer’s worldline such that the computer has inﬁnite
time to compute, and during its computation the computer can send a signal
st to the programmer which reaches the programmer before the ﬁxed instant:
∃pcτ

Ob(p) ∧Ob(c) ∧∀mx

IOb(m) ∧x ≥0 →x ∈Dom lcm(c)∧
∀t

t > 0 →∃t′st

0 < t′ < τ ∧st ∈evm

lcm(c)(t)
	
∩evm

lcm(p)(t′)
	
.
The following axiom ensures the existence of uniformly accelerated observers.
Ax∃UnifOb: It is possible to accelerate an observer uniformly:
IOb(m) →∃k

Ob(k) ∧Dom lcm(k) = Q
∧∀¯x

¯x ∈Ran lcm(k) ↔x2
2 −x2
1 = a2 ∧x3 = . . . = xd = 0

.
Now we can state our theorem on the logical equivalence between the existence
of FTL signals and the possibility of hypercomputation in special relativity:
Theorem 2. Let d ≥3. Then
{AccRel, AxThExp#, Ax∃Unifob, AxVel} |= ∃FTLBody ↔HypComp.
For the proof of Thm.2, cf. [16, §6].
6
Concluding Remarks
We have shown that, in special relativity, the possibility of hypercomputation is
equivalent to the existence of FTL signals. A natural continuation is to investi-
gate the question concerning the limits of the possibility of using FTL particles
in hypercomputation in special and general relativity theories. For example, is
there a natural assumption on spacetime which does not forbid the existence of
FTL particles, but makes it impossible to use them for hypercomputation?
Of course our construction contains several engineering diﬃculties. For exam-
ple, the larger the distance the more diﬃcult to aim with a signal. Therefore, the
computer has to calculate the speed of the FTL signal more and more accurately
to ensure that the signal arrives to the programmer between events O and M,
cf. Fig.1. Thus the computer has to be able to aim with the FTL signal with
arbitrary precision.
Acknowledgments. This research is supported by the Hungarian Scientiﬁc
Research Fund for basic research grants No. T81188 and No. PD84093.

Hypercomputation in SR
537
References
1. Andr´eka, H., Madar´asz, J.X., N´emeti, I., Sz´ekely, G.: A logic road from special
relativity to general relativity. Synthese, 1–17 (2011) (online-ﬁrst)
2. Andr´eka, H., Madar´asz, J.X., N´emeti, I., Sz´ekely, G.: What are the numbers in
which spacetime? (2012), arXiv:1204.1350
3. Andr´eka, H., N´emeti, I., N´emeti, P.: General relativistic hypercomputing and foun-
dation of mathematics. Nat. Comput. 8(3), 499–516 (2009)
4. D´avid, G., N´emeti, I.: Relativistic computers and the Turing barrier. Appl. Math.
Comput. 178(1), 118–142 (2006)
5. d’Inverno, R.: Introducing Einstein’s relativity. Oxford University Press, New York
(1992)
6. Earman, J., Norton, J.D.: Forever is a day: supertasks in Pitowsky and Malament–
Hogarth spacetimes. Philos. Sci. 60(1), 22–42 (1993)
7. Etesi, G., N´emeti, I.: Non-Turing computations via Malament–Hogarth space-
times. Internat. J. Theoret. Phys. 41(2), 341–370 (2002)
8. Friedman, M.: Foundations of Space-Time Theories. Relativistic Physics and Phi-
losophy of Science. Princeton University Press, Princeton (1983)
9. Hogarth, M.L.: Does general relativity allow an observer to view an eternity in a
ﬁnite time? Found. Phys. Lett. 5(2), 173–181 (1992)
10. Madar´asz, J.X.: Logic and Relativity (in the light of deﬁnability theory). Ph.D.
thesis, E¨otv¨os Lor´and Univ., Budapest (2002),
http://www.math-inst.hu/pub/algebraic-logic/Contents.html
11. Madar´asz, J.X., N´emeti, I., Sz´ekely, G.: Twin paradox and the logical foundation
of relativity theory. Found. Phys. 36(5), 681–714 (2006)
12. Manchak, J.B.: On the possibility of supertasks in general relativity. Found.
Phys. 40(3), 276–288 (2010)
13. Matolcsi, T., Rodrigues Jr., W.A.: The geometry of space-time with superluminal
phenomena. Algebras Groups Geom. 14(1), 1–16 (1997)
14. Misner, C.W., Thorne, K.S., Wheeler, J.A.: Gravitation. W. H. Freeman and Co.,
San Francisco (1973)
15. Mittelstaedt, P.: What if there are superluminal signals? The European Physical
Journal B - Condensed Matter and Complex Systems 13, 353–355 (2000)
16. N´emeti, P., Sz´ekely, G.: Special relativistic hypercomputation is possible if there
are faster than light signals (2012) (preprint version); arXiv:1204.1773
17. OPERA collaboration: Measurement of the neutrino velocity with the OPERA
detector in the CNGS beam (2011), arXiv:1109.4897
18. Petkov, V.: Relativity and the nature of spacetime, 2nd edn. Frontiers Collection.
Springer, Berlin (2009)
19. Recami, E.: Tachyon kinematics and causality: a systematic thorough analysis of
the tachyon causal paradoxes. Found. Phys. 17(3), 239–296 (1987)
20. Recami, E.: Superluminal motions? A bird’s-eye view of the experimental situation.
Found. Phys. 31, 1119–1135 (2001)
21. Recami, E., Fontana, F., Garavaglia, R.: Special relativity and superluminal mo-
tions: a discussion of some recent experiments. Internat. J. Modern Phys. A 15(18),
2793–2812 (2000)
22. Rindler, W.: Relativity. Special, general, and cosmological, 2nd edn. Oxford Uni-
versity Press, New York (2006)
23. Selleri, F.: Superluminal signals and the resolution of the causal paradox. Found.
Phys. 36, 443–463 (2006)

538
P. N´emeti and G. Sz´ekely
24. Stannett, M.: The case for hypercomputation. Appl. Math. Comput. 178(1), 8–24
(2006)
25. Sz´ekely, G.: First-Order Logic Investigation of Relativity Theory with an Emphasis
on Accelerated Observers. Ph.D. thesis, E¨otv¨os Lor´and Univ., Budapest (2009)
26. Sz´ekely, G.: The existence of superluminal particles is consistent with the kinemat-
ics of Einstein’s special theory of relativity (2012), arXiv:1202.5790
27. Taylor, E.F., Wheeler, J.A.: Spacetime Physics. W. H. Freeman and Company,
New York (1997)
28. Tolman, R.C.: The Theory of the Relativity of Motion. University of California,
Berkely (1917)
29. Weinstein, S.: Super luminal signaling and relativity. Synthese 148, 381–399 (2006)

Turing Computable Embeddings
and Coding Families of Sets
V´ıctor A. Ocasio-Gonz´alez
Department of Mathematics, University of Notre Dame, Notre Dame IN 46617, USA
vocasiog@nd.edu
Abstract. In [7] the notion of Turing computable embeddings is in-
troduced as an eﬀective counterpart for Borel embeddings. The former
allows for the study of classes of structures with universe a subset of
ω. It also allows for ﬁner distinctions, in particular, among classes with
ℵ0 isomorphism types. The hierarchy of eﬀective cardinalities that arises
from TC embeddings has been studied, among other places, in [7] and
[2]. In this work, we prove that the special class of ‘daisy graphs’, a sub-
class of undirected graphs used to code families of sets, has the same
eﬀective cardinality as the class of archimedian real closed ﬁelds. As a
consequence, the class of abelian p-groups and the class of archimedian
real closed ﬁelds are TC incomparable.
1
Introduction
An interesting problem in mathematics is that of determining when two struc-
tures of the same class are isomorphic. A way to approach this problem, within
a class, is to determine what properties must a structure share with another so
that an isomorphism can be constructed. Once this has been determined for two
classes, a natural question to ask is how do the complexity of the ‘isomorphism
problems’ for the same classes of structures compare with each other. Various
notions of reducibility exist for comparing the classes of structures. Among them
is the notion of Turing computable embeddings, introduced in [7], which pro-
vides an eﬀective version of a Borel embedding, see [3]. It is based on the Turing
reduction of atomic diagrams of structures, done in such a way as to preserve
isomorphism types.
Deﬁnition 1. A Turing computable embedding (TC embedding) of a class of
structures K in another class K′ is a Turing operator Φ = ϕe such that:
– for each A ∈K, there is a B ∈K′ such that ϕD(A)
e
= χD(B),
– if A, A′ correspond, respectively, to B, B′ ∈K′, then A ∼= A′ iﬀB ∼= B′
We say that K ≤tc K′ iﬀthere is a TC embedding from K into K′. Thus
≤tc provides a partial ordering on classes of structures. Also, we say K and K′
have the same eﬀective cardinality if K ≡tc K′, i.e if K ≤tc K′ and K′ ≤tc K.
Note that Turing computable embeddings are uniform computable procedures.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 539–548, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

540
V.A. Ocasio-Gonz´alez
A drawback of Borel embeddings is that they are unable to distinguish between
classes with ℵ0 isomorphism types. Furthermore, structures are required to have
universe all of ω, while for TC embeddings we only require that the universe is a
subset of ω. Exploiting this feature, the authors, in [7], were able to prove, among
other things, that the class of ﬁnite prime ﬁelds lies strictly below (in the sense
of TC-embeddings) the class of Q vector spaces. In those cases where we have
a TC embedding from one class of structures to another, and both classes have
no structure which is ﬁnite, then we can see that if K ≤tc K′ then K ≤B K′,
so that ≤tc is a reﬁnement of Borel embeddings on classes with computable
languages and whose structures have inﬁnite countable universe. More precisely,
whenever we have a structure with inﬁnite countable universe we can always
modify it so that the universe is all of ω. So, that any TC embedding between two
classes which have no ﬁnite structures can be thought of as a Borel embedding
between said classes after we make the change in the universe of the structures.
Evidence for this reﬁnement is seen when comparing the work in [2] and work
from Friedman and Stanley. In Borel embeddings, it was known that the class of
abelian p-groups (ApG) is not reducible to the class of families of ‘daisy graphs’
DG (which we can think of as families of subsets of ω) and vice versa, we say
these classes are Borel incomparable. In [2], among other things, these results
are translated to the realm of TC embeddings using (morally) the same ideas as
Friedman and Stanley and it is proven that these classes are incomparable from
the point of view of ≤tc.
The main body of work in this paper consists in describing the Turing oper-
ators which provide the embedding of DG into ARCF, the class of archimedean
real closed ﬁelds, and vice versa. From this it follows that ARCF and ApG are
TC incomparable. It is known, from work in [1], that incomparable classes of
structures exist in the hierarchy of ≤tc. This work provides an example of two
‘natural’ classes that are incomparable.
1.1
Facts about Theory of Real Closed Fields
Throughout this paper, when talking about a real closed ﬁeld, we assume we are
working in the language of ordered rings, L = {+, ·, 0, 1, <}. It is known that
the theory of real closed ﬁelds in this language is O-minimal, complete, model
complete and decidable. Further, it admits quantiﬁer elimination, and hence the
deﬁnable sets are exactly the sets deﬁned by (ﬁnite) boolean combinations of quan-
tiﬁer free formulas. These facts are freely used throughout this work. For detailed
proofs and explanations the author recommends [9]. In proving Theorem 4 and
Theorem 5, we shall use the following fact for O-minimal structures, whose proof
can be found in [8]:
Theorem 1. Suppose ϕ(¯a, ¯x) is true of ¯b, a tuple algebraically independent over
¯a. There there is an open box, B, around ¯b such that for all ¯x ∈B, ϕ(¯a, ¯x) holds.
The statement above is not explicitly stated as a theorem in [8], but rather it is
a result within the proof of one of the main theorems of the paper. We shall also
need the following result of Van den Dries which can be found in [10] and [9]:

Turing Computable Embeddings and Coding Families of Sets
541
Theorem 2. Let R be a real closed ﬁeld, then R has deﬁnable Skolem functions.
In fact, many schemes exists for skolemizing the structures in RCF but in this
paper we use the one do to Van den Driess in [10]. A proof on how this skolemiza-
tion works on subsets of higher dimensions can be found in [9]. The skolemization
explicitly deﬁnes what is the value of the new function symbols introduced to
the language. Because we can do this uniformly for all structures in RCF, we
shall call RCF+ the theory of real closed ﬁelds in the language of ordered rings
augmented by the functions symbols that witness the skolemization. It follows
that RCF+ is also complete, decidable, O-minimal and admits quantiﬁer elimi-
nation. It should also be noted that R regarded as a structure of RCF+ has no
new deﬁnable subsets. In Theorem 5, during the process of deﬁning the alge-
braic closure of an element, we shall use the built-in deﬁnable Skolem functions
of RCF+. This way we do not have to worry too much about ﬁnding the roots of
polynomials, a crucial step in enumerating the algebraic closure of elements, and
as a consequence we might just end up with multiple names for the elements.
2
Results
Throughout the paper, all languages considered are computable and structures
have universe a subset of ω.
2.1
Construction of Embeddings
We deﬁne a daisy graph as an undirected graph, G, with a distinguished vertex,
say x0 which we call a center, and a set of edges E, such that every other vertex
in G is part of a unique loop containing x0. Since any subset of ω can be coded
as a daisy graph, without loss of generality, for S ⊂ω, we can think of a daisy
graph as having a loop of size 2n+3 if n ∈S or a loop of size 2n+4 if n /∈S. The
class DG is composed of all (countable) collections of daisy graphs, i.e coding
(countable) families of subsets of ω, such that there is at most one daisy coding
each set. It follows that structures A, B ∈DG are isomorphic iﬀthey code the
same subsets of ω. This characterization provides a means for embedding ARCF
into DG. Recall that a real number r is uniquely determined by its Dedekind cut,
i.e., {q ∈Q : q ≤r}. So, ﬁx an enumeration of the rational numbers (qn)n<ω.
We can eﬀectively construct a structure A ∈DG for each structure R ∈ARCF
by associating each r ∈R with the daisy that codes the set Sr = {n : qn < r}.
The construction of the daisies in A is done in stages. Further, multiple daisies
must be constructed simultaneously, since at any stage, we have seen only ﬁnitely
many elements of R and know ﬁnitely much about each. Our main strategy, at
any stage s, will consist of taking a real r ∈R, which we know it is truly a
real number since the ﬁeld is archimedian, and determining for r < s whether
qn < r. If qn < r, then we enumerate sentences into the diagram of A, enough to
construct a loop of size 2n + 3 if qn ≤r. Otherwise, we construct a loop of size
2n + 4. This procedure is uniform on structures in ARCF and it can be shown
that isomorphic structures in ARCF give rise to isomorphic structures in DG.
Hence, we have proved the following:

542
V.A. Ocasio-Gonz´alez
Theorem 3. ARCF ≤tc DG
Our goal now is to prove that the converse of Theorem 3 is also true. For this we
need to construct a computable Turing operator Φ which uniformly in A (using
as oracle D(A)) computes the atomic diagram of a structure R in ARCF. This is,
in practice, more complex. Two ARCF’s are isomorphic iﬀthey have the same
reals. Thus, doing the above construction ‘backwards’ fails to provide a Turing
computable embedding, since by taking the algebraic closure of the reals deﬁned
by the Dedekind cuts we might end up with two non-isomorphic A, A′ ∈DG
mapping to the same R ∈ARCF.
Our strategy will be to construct Φ as a composition of Turing operators.
First, we identify each node in 2<ω with a closed interval with rational end-
points [q, q′] ⊆[0, 1] so that every path through 2<ω corresponds to a countable
intersection of (nested) closed intervals. We do this with a computable operator T
in such a way, that paths through 2<ω represent algebraically independent reals.
Thus, embedding any structure A ∈DG into 2<ω provides a way to transform
information about whether or not a number n is in a given set, to information
about whether or not a real is in a given interval. The latter is, of course, more
useful when deciding facts about the ARCF we ultimately wish to construct.
Finally, we use the image of A inside 2<ω to output the atomic diagram of a
structure in ARCF.
In working towards constructing T , as described above, we make precise those
properties it must have. The following notation will allow a better way to express
the conditions we wish to impose to T .
Deﬁnition 2. Suppose T : 2<ω →Q × Q is a computable operator mapping
σ 
→(q, q′). We deﬁne IT
σ = [q, q′] to be the closed interval with rational endpoints
q and q′.
Once we have ﬁxed such a T , we shall identify σ ∈2<ω with IT
σ . Furthermore,
we shall write Iσ for IT
σ when there is no ambiguity about the operator T . From
now on, we shall identify an operator T deﬁned as above with the tree it deﬁnes
in 2ω and we shall denote them both by T . Recall that a binary tree is perfect
if for every σ in the tree there is a τ extending σ with τ∧0 and τ ∧1 also in the
tree. Our goal for now will be to prove the following theorem.
Theorem 4. There is a perfect, computable, binary tree T whose continuum
many paths represent algebraically independent reals over Q.
In order to prove the Theorem above we shall prove that there exists a com-
putable operator T : 2<ω →Q × Q satisfying the following conditions:
1. T (∅) = [0, 1]
2. If σ ⪯τ, then Iτ ⊆Iσ.
3. If length(σ) = n, then diameter(Iσ) ≤2−n, where diameter(a, b), for an
interval (a, b), is deﬁned to be b −a.
4. If σ, τ are incomparable and both of length n, Iσ ∩Iτ = ∅

Turing Computable Embeddings and Coding Families of Sets
543
5. For f ∈2ω, let rf be the unique real in 
σ⊆f Iσ. Then for distinct f1, . . . , fn ∈
2ω, rf1, . . . , rfn are algebraically independent.
Observe that properties (1)-(4) are not hard to realize. The ﬁrst one is a deﬁnition
while the other three can be simultaneously made true by subdividing a given
interval I into appropiate pieces. Further, note that by property (4) and the fact
that T has 2<ω as its domain, the tree T will be perfect. Most of the work will
be proving property (5). It is done by a classic diagonalization argument and
recurrent applications of the following lemma whose proof we omit but can be
seen to be true by the O-minimality of RCF.
Lemma 1. Given disjoint intervals I1, . . . , In ⊂[0, 1] ⊂IR with known rational
endpoints and given a non-trivial p(x1, . . . , xn) ∈ZZ[x1, . . . , xn], we can eﬀec-
tively ﬁnd disjoint intervals with rational endpoints J1, . . . , Jn with Ji
⊆
Ii,
such that for all reals a1 ∈J1, . . . , an ∈Jn p(a1, . . . , an) ̸= 0.
Our main strategy for proving Theorem 4 will be to apply the above lemma (sev-
eral times if necessary) at each stage. Note that once we have ﬁnished searching
for the rational endpoints of the Ji’s, we can pick, for each i, J′
i ⊆Ji, also with
rational endpoints, in such a way, that properties (2)-(4) are satisﬁed. For exam-
ple, if Ji = [q1, q2], pick J′
i = [q′
1, q′
2] such that q′
1 = 1
3(q1+q2) and q′
2 = 2
3(q1+q2).
This will be enough to satisfy properties (2)-(4). In fact, there is no need to sub-
divide the intervals in pieces with the same diameter, nor is it important that
there are 3 pieces. Any partition of Ji is acceptable as long as we satisfy the
conditions. With this in mind let’s prove Theorem 4.
Proof (4)
Fix an enumeration of all polynomials with integer coeﬃcients in arbitrarily
long (but ﬁnite) tuples of variables. Recall that n reals are algebraically inde-
pendent (over Q) if they satisfy no formula of the form p(x1, . . . , xn) = 0, if
p(x1, . . . , xn) ∈Q[x1, . . . , xn] (equivalently ZZ[x1 . . . , xn]). So at each stage, we
satisfy a requirement of the form:
R<σ1,...,σn,k> ≡There exists Iτ1 ⊆Iσ1, . . . , Iτn ⊆Iσn such that for all reals
r1 ∈Iτ1, . . . , rn ∈Iτn pk(r1, . . . , rn) ̸= 0
Each requirement is said to require attention when the image under T of all
σ1, . . . , σn has been deﬁned. We construct T : 2<ω →Q × Q in stages:
-At stage s = 0; T0(∅) = [0, 1]
-At stage s + 1;
Let Ts be the (partial) function at stage s and let Rσ1,...,σn,k be the least require-
ment needing attention. Let Endσi be the set of all end points of Ts extending
Iσi, i.e Iγ ∈Endσi iﬀσi ≺γ and Ts(γ∧0), Ts(γ∧1) are undeﬁned. Note that for
i ̸= j, Endσi ∩Endσj will not necessarily be empty. In fact, Endσi and Endσj
could be equal. But since we are satisfying property (4) at each stage, then for
Iγ ∈Endσi and Iγ′ ∈Endσj with γ ̸= γ′, hence incomparable since they are end
points, Iγ ∩Iγ′ = ∅. Further, if we take a tuple Iγ1, . . . , Iγn with Iγi ∈Endσi, and

544
V.A. Ocasio-Gonz´alez
there is some Iγi = Iγj then we do nothing since Lemma 1 does not apply. This,
we argue, does not aﬀect the conclusion of the Theorem since if two reals r1, r2
correspond to two distinct paths through T (2<ω), then they can be separated
by disjoint intervals by property (4). So from now on in this proof we assume
that whenever we take a tuple as above, it is a tuple with no repetitions.
Take a tuple Iγ1, . . . , Iγn with Iγi ∈Endσi and apply Lemma 1 to get J1, . . . ,
Jn. By construction, Ji ⊆Iγi for all i. Pick another tuple as above (if any are
available) and apply Lemma 1 again but this time for all Iγi that appear again
we use the corresponding Ji instead of Iγi when applying the lemma. The idea is
to reﬁne each Iγi as much as possible so the J′
i ⊆Iγi that we end up with works
with any other reﬁnement of the Iγj, j ̸= i. So, as before, pick tuples until all
possibilities are exhausted, which they will since Endσi is ﬁnite for all i. Now,
for all Jj ⊆Iγj, Iγj ∈Endσi, i < n, subdivide as in the comment below Lemma
1 Jj into J0
j and J1
j . Deﬁne Ts+1 = Ts ∪

(γ∧
j 0, J0
j ) : Iγj ∈Endσi for some i

∪

(γ∧
j 1, J1
j ) : Iγj ∈Endσi for some i

.
Observe that the fact that T is perfect comes both from the fact that 2<ω is
perfect and that for any σ we are mapping σ∧0 and σ∧1 to disjoint intervals.
Now that we have T , the next step towards proving the embeddability of the
class DG into ARCF is to show that given A ∈DG, we can get, through a uni-
formly computable process, a subtree of 2<ω, TA, whose labeled paths represent
the sets coded by daisies in A. To deﬁne the notion of labeled path we ﬁrst need
to say what is a label or mark on a tree. We introduce to the language (of trees)
a relation M (for marks), and a set of constants {xi : i < ω}, and we say that
a path f ∈[TA] is a labeled path if there exists one (and only one) constant
xi such that for inﬁnitely many σ ≺f, (σ, xi) ∈M. The tree TA,s will have
the feature that every node will have attached to it countably (possibly ﬁnitely)
many ‘marks’. This type of bookkeping is unnecesary for the construction of TA,s
itself but attaching to the tree these marks will allow us, in the construction in
Theorem 5, to guarantee that we can associate to each daisy in A a ‘name’ or
constant, with only ﬁnitely much information, and allow it to keep the name.
Further, it is possible that TA contains paths that codes no daisy in A, hence
labeling the paths permits us to identify which paths do. The tree TA may be
perfect, but in any case, its labeled paths will denote algebraically independent
reals, in the sense of Theorem 4. There will be no terminal nodes. This follows
from the fact that at any stage, say s, the terminal nodes of TA,s correspond to
distinct daisies. How we extend these terminal nodes (in TA,s), that is choosing
left or right, will be determined by what information is coded in the correspond-
ing daisy. Since daisies give both positive and negative information about a set,
all terminal nodes are (eventually) extended to elements of 2<ω.
Lemma 2. There is a computable operator H : DG →2<ω that, uniformly in A,
computes a tree TA whose labeled paths correspond to algebraically independent
reals.

Turing Computable Embeddings and Coding Families of Sets
545
Proof (2)
Let D(A) be the atomic diagram of A and let T be as in Theorem 4. Let C be
the set of all elements of A that are centers of daisies. Let X = {xi : i < ω} of
new constants and let M ⊆T × X be a binary relation symbol. We construct
TA, M, and C in stages:
-At stage 0; TA,0 = ∅, i.e the tree consists of only the empty node; C = ∅; M = ∅.
-At stage s + 1;
At this stage, TA,s has at most s endpoints, C = {c0, . . . , ck} for k ≤s , and there
are at most s tuples in M. In particular, any σ ∈TA,s has at most s < ∞marks.
We allow ourselves to look only at the ﬁrst s sentences of D(A), call this set of
sentences D(A)s. Using these, identify elements in A that appear to be centers of
daisy graphs. We can identify the centers since they are the only elements that
are connected by an edge to more than 2 elements. Further, D(A)s tells us that
they are distinct. If we see a new center, say ck+1, then we put ck+1 ∈C and
identify it with a σck+1 which starts at the empty node of T , as explained below.
We introduce a set of tuples of the form (ct, i), called Check, where ct ∈C and
i < ω. This set keeps track of questions we have asked about centers of daisy
graphs. Now, ﬁx a c ∈C and verify if D(A)s has enough information to say the
following for all i ≤j ≤s, where i is the largest such that (c, i) ∈Check (if no
tuple involving c is in Check we let i = 0):
1. The element c is in a loop of size 2j + 3.
2. The element c is in a loop of size 2j + 4.
Of course, there is no sentence saying (1) or (2) above, since this is an existential
statement, but we regard it as true if we have enough information in D(A)s to
construct a loop of the appropiate size. Note that by our convention on daisy
graphs only one must be true but we might at this stage not see which. Start by
checking if (1) or (2) is true for j = i. If (1) is true then let τ ∈T be such that
σ∧
c 0 ⪯τ, τ∧0 ∈T , τ∧1 ∈T , and for all τ ′ ≻σc, τ ′ ≻τ. That is, τ extends σc
by going to the left and does so in such a way that τ stops at the next splitting.
This can be done since T is perfect. If (2) is true then ﬁnd τ ∈T with same
properties except this time τ extends σc from the right. Either way, add (c, j)
to Check. If j = c (recall the universe of A is a subset of ω) or (σc, xc) ∈M,
add (τ, xc) ∈M and (σ, xc) ∈M for all σ ≺τ. The ﬁrst condition guarantees
all daisies (and hence paths) are eventually associated with a mark. The second
provides continuity, so that once we have a mark we can follow that mark on
the tree forever. Finally, let σc = τ and repeat the process for the remaining
j. If at any point we get a false answer for both (1) and (2), or j = s + 1,
then we stop asking questions about c and pick another c′ ∈C. Now deﬁne
TA,s+1 = Ts ∪{σc : c ∈C}. Finally, to conclude that TA = 
s<ω TA,s has the
desired properties we still need to prove the following. If f is a labeled path
through TA, then there exists a unique xc such that for inﬁnitely many σ ≺f,
(σ, xc) ∈M.
To see this is so, note ﬁrst that labeled paths through TA correspond in a one-
to-one fashion with the centers of daisies in A. Furthermore, if f corresponds

546
V.A. Ocasio-Gonz´alez
to a daisy whose center is c then it will be labeled, i.e., (σ, xc) ∈M for some
σ ≺f, when we can answer whether or not there is a loop of size 2c+ 3 or 2c+ 4
in said daisy. Once this initial labeling has been done, for every τ ≻σ there is
(τ, xc) ∈M. To see that xc is unique note that if we think of daisies as subsets
of ω, then for any two daisies, say S1, S2, there is a least n such that n ∈S1 and
n /∈S2. So that if f is the path associated, without loss of generality, with S1
and σ is the initial segment of f we have at the stage where we witness there is
a loop if size 2n + 3 in S1, then the τ extending σ will recieve a mark for S1 and
will never recieve a mark for S2.
We are now ready to construct the second part of the computable operator Φ.
The ﬁrst part was given by Lemma 2, the second by the theorem below. Instead
of working in RCF, the theory of real closed ﬁelds, we shall be working on the
skolemization of RCF, RCF+, as deﬁned in Section 1.1. We also add constants
naming all terms in the language of RCF+, with extra constants {ci : i < ω} for
possible centers of daisies. This simpliﬁes the process of ﬁnding the algebraic clo-
sure of an element. No longer do we need to deﬁne a constant to be (interpreted
as) a root of a particular polynomial in the structure we are constructing. We
know the constant is already ‘picked out’ by a Skolem function. Thus we shall
eventually ﬁnd the root of a polynomial if we check the constants one by one.
This will all be made precise in the proof of Theorem 5.
Theorem 5. DG ≤tc ARCF
Proof (5)
We show how to convert a family of daisy graphs, A, to an ARCF, R. By Lemma 2
we freely identify A with TA for the rest of the proof. The ﬁeld R will contain
one real for each path in TA, all algebraically independent from each other. We
shall enumerate the diagram of R, D(R), in stages, assuming we have the atomic
diagram of A, D(A).
Let L+ be the language of RCF+ and we augment it with the following two
sets of constants, C = {ci : i ∈ω} and R = {ri : i < ω; ri = t(¯c)}, where t(¯x) is
a term in L+∪C. The set C will serve as an auxiliary set since it will help us keep
track of how many independent reals we have at any given stage. We identify
each mark, xci, of TA,s, the tree at stage s, with the constant ci to ensure we get
the appropiate independent reals in R. By identifying a constant c with a mark
we mean to add to D(R) sentences saying that c lies in the rational interval at
the node marked in TA,s. We denote this interval as Ic. Note we shall use D(TA)
to reconstruct TA but we shall be more focused on ﬁnding the marks on TA since
these will allow us to say new facts about the constants. On the other hand, the
set R, or in some cases a subset of R, will serve as the universe of the structure
R we shall construct. We might worry that if we use only elements from the set
R we might not be adding the reals coded by the daisies in A but notice that for
every ci there is some rj such that rj = ci, just by how we deﬁned the elements
in R.
Now, recall that by quantiﬁer elimination any ﬁrst-order formula in L+ ∪R
is equivalent to a ﬁnite boolean combination of equalities and inequalities of

Turing Computable Embeddings and Coding Families of Sets
547
polynomials. Let {αn(¯z)}n∈ω be an eﬀective enumeration of all ﬁnite boolean
combinations of equalities and inequalities in L+ ∪R with parameters ¯z which
are ﬁnite but arbitrary in length.
From the previous discussion it follows that we must satisfy the following
requirement for all i: Decide whether αi(¯z) ∈D(R) or ¬αi(¯z) ∈D(R), where ¯z
is a tuple (of meaningful) elements of R. By a meaningful element of R we mean
an element r whose deﬁnition as a term relies on constants of C that we have
already identiﬁed with a mark of TA.
Each stage will be divided in two steps. First we check if we can extend our
knowledge of the constants c. Then we shall enumerate into D(R) all facts we
can decide with the amount of information available.
Construction
-At stage s = −1; D(R) = ∅, and no marks have been identiﬁed in TA.
-At stage s + 1;
Let TA,s be the part of TA we can construct using the ﬁrst s sentences of D(A).
Here we are thinking of running the construction described in Lemma 2 as far
as we can using the information given about A. So at this stage we have k ≤s
constants in C identiﬁed with marks of TA,s. Now construct TA,s+1 and do, of
the following, what applies: If no new mark appears, do nothing. If a new mark
xc appears, that is, we see a new tuple (σ, xc) ∈M, identify the constant c
with it. Note that if this is not the ﬁrst identiﬁcation of c, we could be adding
a sentence to D(R) that adds no new (useful) information about c. This might
happen if we see (σ, xc) ∈M now but we had already seen that (τ, xc) ∈M
for some τ ≻σ. Because of this, we deﬁne Ic to be the interval with smallest
diameter where we know c lies.
Let R′
s+1 be the ﬁrst s+1 constants r ∈R which are meaningful (as described
above). Then for each i ≤s+1 we make tuples of elements in R′
s+1 of appropriate
length, where appropriate is determined by the length of ¯z in αi(¯z). Notice
that since every r ∈R′
s+1 can be deﬁned in terms of c’s, then in fact, αi(¯r)
is equivalent to some β(ci1, . . . , cin), where β(¯z) is a sentence in L+ ∪C. This
β(¯z) can be found by just replacing each r by its deﬁnition in terms of the c’s.
Further, by Theorem 1, if β(¯c) is true then there exists J1, . . . , Jn with cij ∈Jj,
Jj ⊆Icij , such that ∀a1 ∈J1 · · · ∀an ∈Jn β(a1, . . . , an) is also true. Similarly,
if ¬β(¯c) is true.
So in order (according to your list), pick i ≤s + 1 and an (appropriate) tuple
¯r. Find the corresponding β(¯c) and ask which of the following is true:
∀a1 ∈Ici1 · · · ∀an ∈Icin β(a1, . . . , an)
∀a1 ∈Ici1 · · · ∀an ∈Icin ¬β(a1, . . . , an)
Recall that by quantiﬁer elimination the above sentences are equivalent to quan-
tiﬁer free sentences and by decidability we can check their truth value. At this
stage we might think both are false, but by completeness of RCF, and hence
RCF+, one must be true. So if we think both are false we do nothing. If we
can tell which one is true then enumerate αi(¯r) into D(R) if the ﬁrst is true or

548
V.A. Ocasio-Gonz´alez
¬αi(¯r) if the second is true. When we have gone through all of our list, of both
tuples and αi’s, we pass to the next stage.
We claim that the above procedure suﬃces, so we need to prove that all
sentences are decided at some stage. But this follows from Theorem 1 and the
fact that a path through TA corresponds to a nested sequence of intervals.
To conclude, notice that for any two non-isomorphic daisy graphs, A and B,
there exists a daisy S coded in A which is not coded in B or vice versa. Thus the
ARCF’s, Φ(A) and Φ(B), to which they are mapped respectively will contain at
least one real which does not appear in the other since the constants c in the
construction are all algebraically independent.
References
1. Calvert, W., Cummins, D., Knight, J.F., Miller, S.: Comparison of classes of Finite
structures. Algebra and Logic 43(6), 374–392 (2004)
2. Fokina, E., Knight, J.F., Melnikov, A., Quinn, S.M., Safranski, C.: Classes of
ulm type and coding rank-homogeneous trees in other structures. J. Symbolic
Logic 76(3), 846–869 (2011)
3. Friedman, H., Stanley, L.: A Borel reducibility theory for classes of countable struc-
tures. J. Symbolic Logic 54(3), 894–914 (1989)
4. Goncharov, S., Harizanov, V., Knight, J., McCoy, C., Miller, R., Solomon, R.: Enu-
merations in computable structure theory. Annals of Pure and Applied Logic 136,
219–246 (2005)
5. Chisholm, J., Fokina, E., Goncharov, S., Harizanov, V., Knight, J., Quinn, S.:
Intrinsic bounds on complexity and deﬁnability at limit levels. J. Symbolic
Logic 74(3), 1047–1060 (2009)
6. Hirschfeldt, D., Khoussainov, B., Slinko, A., Shore, R.: Degree spectra and com-
putable dimensions in algebraic structures. Annals of Pure and Appl. Logic 115,
71–113 (2002)
7. Knight, J.F., Miller, S., Vanden Boom, M.: Turing computable embeddings. J.
Symbolic Logic 72(3), 901–918 (2007)
8. Knight, J.F., Pillay, A., Steinhorn, C.: Deﬁnable sets in ordered structures II.
Trans. Amer. Math. Soc. 295(2), 593–605 (1986)
9. Marker, D.: Model theory: an introduction. Graduate Texts in Mathematics,
vol. 217. Springer (2000)
10. Van den Dries, L.: Algebraic theories with deﬁnable skolem functions. J. Symbolic
Logic 49(2), 625–629 (1984)

On the Behavior of Tile Assembly System
at High Temperatures
Shinnosuke Seki and Yasushi Okuno
1 Department of Information and Computer Science, Aalto University,
P.O. Box 15400, 00076, Aalto, Finland
shinnosuke.seki@aalto.fi
2 Department of Systems Bioscience for Drug Discovery, Kyoto University, 46-29,
Yoshida-Shimo-Adachi-cho, Sakyo-ku, Kyoto, 606-8501, Japan
okuno@pharm.kyoto-u.ac.jp
Abstract. Behaviors of Winfree’s tile assembly systems (TASs) at high
temperatures are investigated in combination with integer programming
of a speciﬁc form called threshold programming. First, we propose a
way to build bridges from the Boolean satisﬁability problem (SAT) to
threshold programming, and further to TAS’s behavior, in order to prove
the NP-hardness of optimizing temperatures of TASs that behave in a
way given as input. These bridges will take us further to two important
results on the behavior of TASs at high temperatures. The ﬁrst says that
arbitrarily high temperatures are required to assemble some shape by a
TAS of “reasonable” size. The second is that for any temperature τ ≥4
given as a parameter, it is NP-hard to ﬁnd the minimum size TAS that
self-assembles a given shape and works at a temperature below τ.
1
Introduction
The abstract Tile Assembly Model (aTAM), which has been introduced by Win-
free [13] based on a dynamic version of Wang tiling [12], is a model of “pro-
grammable crystal growth” with algorithmically-rich theoretical background and
results. Self-assembling (square) tiles have been experimentally implemented in
1982 by Seeman [11] as monomers with sticky ends that are designed so inge-
niously that they are guaranteed to bind deterministically into a single target
shape (a commonly-used implementation of such tiles is based on DNA double-
crossover molecules [7]). The last three decades have seen drastic advancements
on the reliability of DNA tile assembly; in fact, the error rate of 10% per tile in
2004 was improved down to 0.13% in 2009 [8].
In the aTAM, sticky ends are abstracted to be a glue label, and their strengths
are assigned by a strength function g. A square tile adheres stably to an aggregate
of tiles whenever the sum of the strengths of neighboring sides with matching
labels according to g exceeds a threshold τ, which is a system parameter called
temperature. A certain “behavior” of tile assembly systems (TASs) was proved to
require a very large gap between binding strengths as well as ones exponentially
small in terms of the larger gaps [5] (for a formal deﬁnition of TAS’s behavior as
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 549–559, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

550
S. Seki and Y. Okuno
strength-free TAS, see Sect. 3). “Temperature” in aTAM is rather metaphorical
than actually describing the physical temperature of the experimental environ-
ment. It nonetheless can be interpreted as a physical metric for the granularity
with which diﬀerent energy levels must be distinguished in order for TASs to
behave as expected. One can quantitize the minimum distinction and rescale so
that this quantity is normalized to be 1. As such, aTAM assumes that the glue
strengths and temperature are integers.
The stability of tile attachment is determined by the strengths that at most 4
cooperative binding sites oﬀer, relative to τ. This motivates us to study a system
of inequalities whose left-hand side consists of at most 4 terms (non-negative
integers or constants) and whose right-hand side is τ. We call such an inequality
a τ-inequality of at most 4 terms; we use this term often with the replacement of
τ by either ≥τ (at least τ) or <τ (strictly less than τ) to specify its sign. Then,
we can say that τ-inequalities of at most 4 terms dominate the behavior of a
TAS at the micro, or local, level, that is, every attachment event. Optimizing
(minimizing) τ under τ-inequalities is a speciﬁc type of integer programming we
call threshold programming (TP). We prove that TP is NP-hard even under the
condition that all ≥τ-inequalities involved be of at most 4 terms and all <τ-
inequalities involved be of at most 3 terms (Lemma 2). This condition enables
us to reduce TP to the local behavior of a TAS, and this implies that optimizing
the temperature of TASs that behave in a way speciﬁed as input cannot be done
in a polynomial time, unless P = NP (Theorem 2).
The TP instances obtained in this reduction will lead us further to the study
of terminal behaviors of TASs. For any temperature τ, one of the instances is
converted into a shape such that TASs of “reasonable size” can self-assemble the
shape only at the temperatures above τ (Theorem 3). They are also converted
diﬀerently into a shape to prove that, for any τ ≥4, it is NP-hard to compute
the minimum number of tile types required for TASs at a temperature at most
τ to self-assemble the shape (Theorem 4).
Current laboratory techniques allow us to handle only at most 2 distinct en-
ergy levels, that is, temperature 2 (even making a distinction between two energy
levels is diﬃcult) [3,6]. Therefore, as of this date, we cannot help but interpret
our results as computational infeasibility of determining whether behaviors of
TAS can be implemented physically.
2
Abstract Tile Assembly Model
Let N0 denote the set of non-negative integers, and N = N0 \ {0}. A tile type
t is a unit square with four sides, each having a glue label (often represented
as a ﬁnite string taken from a label set Λ). In this paper, we list glue labels
in the counter-clockwise order starting from north (N) in order to specify a tile
type. For each direction d ∈{N, W, S, E}, let t(d) be the glue label at the d side
of t. We assume a ﬁnite set T of tile types, but an inﬁnite number of copies of
each tile type, each copy referred to as a tile. An assembly (a.k.a., supertile) is
a positioning of tiles on (part of) the integer lattice Z2; i.e., a partial function

On the Behavior of Tile Assembly System at High Temperatures
551
Z2  T . A strength function is a function g : Λ →N0 indicating, for each glue
label ℓ, the strength g(ℓ) with which it binds. Two adjacent tiles in an assembly
interact if the glue labels on their abutting sides are the same and have positive
strength according to g. Each assembly induces a binding graph, a grid graph
whose vertices are tiles, with an edge between two tiles if they interact. The
assembly is τ-stable if every cut of its binding graph has strength at least τ,
where the weight of an edge is the strength of the glue it represents. That is, the
assembly is τ-stable if at least energy τ is required to separate it into two parts.
A tile assembly system (TAS) is a quadruple T = (T, σ, g, τ), where T and
g are as stated above, σ : Z2  T is a ﬁnite τ-stable seed assembly, and
τ ∈N is a parameter called temperature. We assume that all seed assemblies
σ consist of a single tile (i.e., |dom σ| = 1). An assembly α is producible by T
if either α = σ, or β is a producible assembly and α can be obtained from β
by placing a single tile on an empty space such that the resulting assembly α
is τ-stable. An assembly is terminal if no tile can be stably attached to it. Let
A[T ] be the set of producible assemblies of T , and let A□[T ] ⊆A[T ] be the
set of producible terminal assemblies of T . A TAS T is directed if |A□[T ]| = 1.
Given a (connected) shape S ⊆Z2, a TAS T strictly self-assembles S if every
terminal assembly produced by T has shape S.
A directed TAS that strictly self-assembles a shape S can be regarded as a
“program” to output S. Hence, a descriptional complexity of S that captures
the notion of how concisely one can describe such a TAS has been investigated.
Rothemund and Winfree introduced the temperature-2 directed tile complexity of
S [9], which is deﬁned as the minimum number of tile types required for a directed
TAS at the temperature 2 to strictly self-assemble S. This complexity measure
is naturally extended for an arbitrary temperature τ as directed tile complexity
of S at the temperature τ; its deﬁnition must follow in a straightforward manner.
We denote it by Cdtilec(τ)(S). Let Cdtilec(<τ)(S) = min{Cdtilec(i)(S) | 1 ≤i < τ}
and Cdtilec(≤τ)(S) = min{Cdtilec(<τ)(S), Cdtilec(τ)(S)}.
Proposition 1. For any τ ∈N, TASs at the temperature τ can be simulated at
any temperature that is a multiple of τ.
Proof. This simulation is simply done by multiplying the strengths and τ of a
given TAS T1 = (T, σ, g, τ) by a proper constant c. The TAS thus obtained is
T2 = (T, σ, g′, cτ) with g′(ℓ) = cg(ℓ) for each glue label ℓin T .
Let us consider the T1 and T2 in this proof with the assumption that they are
directed. It goes without saying that T1 and T2 reach the same terminal assembly
(i.e., A□[T1] = A□[T2]), they exhibit exactly the same behaviors not only at the
macro level as such but also at the micro, or local, level, that is, A[T1] = A[T2].
Let us formalize this behavioral equivalence at the local level next.
3
Behavioral Equivalences among TASs
The “behavior” of a TAS T = (T, σ, g, τ) is determined fully by its strength
function g and temperature τ. We can rephrase this as: g and τ determine the

552
S. Seki and Y. Okuno
global behavior of T by specifying the local behavior of each tile type t ∈T as
cooperation set of t with respect to g and τ [5], which is deﬁned as:
Dg,τ(t) =

D ⊆{N, W, S, E}
 
d∈D g(t(d)) ≥τ

.
This is the collection of subsets of sides of t whose glues have suﬃcient strengths
to bind cooperatively. As such, a cooperation set is closed under superset oper-
ation. A tile type with empty cooperation set can be rid from T because a tile
of that type never attaches. Then, for any t ∈T , {N, W, S, E} ∈Dg,τ(t).
Cooperation sets provides a behavioral equivalence among TASs. Given TASs
T1 = (T, σ, g1, τ1) and T2 = (T, σ, g2, τ2) that share the tile set T and seed σ, if
Dg1,τ1(t) = Dg2,τ2(t) for each tile type t ∈T , then the local behaviors of T1 and T2
are exactly the same. Then these TASs are said to be locally equivalent (written
as T1 ∼T2). Note that T1 ∼T2 implies A[T1] = A[T2] and A□[T1] = A□[T2].
Thus, given locally equivalent TASs, one is directed and strictly self-assembles
a shape if and only if so is and does the other.
By the local equivalence ∼, the set of all TASs is divided into the equivalence
classes, and all the TASs in an equivalence class behave exactly in the same way
locally. Hence, we can choose a TAS T = (T, σ, g, τ) arbitrarily from the class,
and let it describe the behavior of this class by the pair (T, {Dg,τ(t) | t ∈T }).
This suggests a way to deﬁne a variant of TAS by assigning each t ∈T with a set
of subsets of {N, W, S, E} as a cooperation set. This variant is free from strength
function or temperature, and hence, called the strength-free TAS [5]. Formally,
a strength-free TAS is a triple (T, σ, D), where T and σ are the same as those for
TAS, while D : T →P(P({N, W, S, E})) is a function from a tile type t ∈T to a set
of subsets of {N, W, S, E} that is closed under superset operation, where P means
the power set. Given a strength-free TAS Tsf = (T, σ, D) and a (conventional)
TAS (T, σ, g, τ), they are locally equivalent if D(t) = Dg,τ(t) for each t ∈T . For
an equivalent class, if Tsf is locally equivalent to its element, then so is it to all
of its elements. Hence, we consider that Tsf represents the local behavior of this
class. Of particular note is that such a Tsf is unique. It must be also noted that
there exists a strength-free TAS that is locally equivalent to no TAS, and this
implementability can be checked in a polynomial time [5].
The strength-free TAS was introduced as a technical tool to solve an open
problem posed by Adleman et al. [2]. In the paper, they proposed an algorithm
to ﬁnd a minimum size directed TAS T = (T, σ, g, 2) that strictly self-assembles
the n× n square Sqn, subject to the constraint that the TAS’s temperature is 2.
This algorithm enumerates all temperature-2 TASs with at most Cdtilec(2)(Sqn)
tile types, and checks whether each of them is directed and strictly self-assembles
Sqn (this is proved to be polynomial-time checkable in n). The temperature of
a system to be checked need not be 2, but rather the temperature-2 restriction1
is utilized to upper-bound the number of all candidates to be thus checked by a
polynomial in n, and as a result, this algorithm runs in a polynomial time in n.
Chen, Doty, and Seki improved this algorithm by removing the temperature
constraint
[5].
Though
being
based
on
the
above-mentioned
idea
by
1 This can be replaced with the temperature-c restriction for any constant c ≥1.

On the Behavior of Tile Assembly System at High Temperatures
553
Adleman et al., their algorithm enumerates rather all strength-free TASs with at
most Cdtilec(2)(Sqn) tile types, and commits extra check for the implementabil-
ity. A problem arises of whether this algorithm can be modiﬁed so as to output
among all the minimum size directed TASs for Sqn the one(s) working at the
lowest temperature. This motivates us to study the following optimization:
FINDOPTIMALSTRENGTH
INPUT :
a strength-free TAS Tsf
OUTPUT : a TAS of minimal temperature that is locally equivalent to Tsf, if any.
We shall prove that this is NP-hard (Theorem 2), which was left open in [5].
3.1
Threshold Programming
In order to prove the NP-hardness of FINDOPTIMALSTRENGTH, let us introduce a
subclass of integer programming (IP) that aims at optimizing τ subject to only
≥τ-inequalities and <τ-inequalities. We call an IP a threshold programming (TP).
This is formalized as: for given integer matrices C1, C2, minimize τ on condition
that C1x ≥τ1 and C2x < τ1, where x is a vector of non-negative integer
variables and 1 = (1, 1, . . ., 1). FINDOPTIMALSTRENGTH is a TP any of whose
constraints is either a ≥τ-inequality of at most 4 terms or a <τ-inequality of at
most 3 terms. Such a TP is denoted by TP(4, 3). Its decision variant, denoted by
τ-THRESHOLDPROGRAMMING(4, 3) or simply τ-TP(4, 3), is of interest in which
τ is regarded rather as a constant, and one is asked to decide whether x exists.
Our arguments below mainly consist of designing systems of τ-inequalities for
various purposes. As a tool, we introduce a sub-system that will be embedded
into these systems and force their variables to assume at least or exactly some
speciﬁc value. It is built on the following pair of τ-inequalities:
x1 + xa ≥τ and xa < τ.
(1)
This pair implies x1 ≥1. Once (1) being embedded into a τ-inequality system,
the variable x1 cannot help but assume a positive value itself (xa is assumed to
be an auxiliary variable occurring only in (1)). In the rest, when we say that a
system has a positive variable x, we assume that its positiveness is thus forced.
Let us build a sub-system called 2i+1-adder (to the lower bound of a vari-
able)2. From a variable x with a lower bound n, i.e., x ≥n, it creates a
variable with a lower bound n + 2i+1. It has 5i+5 positive integer variables
z1, z2, x0, xb, xc, Ak, A′
k, A′′
k, B′
k, B′′
k(1 ≤k ≤i) and is designed as follows: for
2 ≤j ≤i,
A′
1 + B′
1 + x0,
A′′
1 + B′′
1 + x0
≥τ,
A1 + B′
1 + x0,
A′
1 + B′′
1 + x0
< τ,
Aj−1 + B′
j + A′
j, Aj−1 + B′′
j + A′′
j ≥τ,
A′′
j−1 + B′
j + Aj, A′
j + B′′
j + A′′
j−1 < τ,
A′′
i + xb,
z1 + xc
< τ,
z1 + xb,
z2 + xc
≥τ.
(2)
2 This is a modiﬁcation of a system of inequalities proposed in [5].

554
S. Seki and Y. Okuno
Solving (2) gives A′′
i ≥Ai + 2i+1 −2 for 1 ≤k ≤i. The four inequalities in
the last two lines of (2) yield z1 ≥A′′
i + 1 and z2 ≥z1 + 1 (in fact, these four
inequalities implement a 21-adder). As a result, z2 ≥Ai + 2i+1.
Assume that Ai ≥n; then z2 ≥n + 2i+1. Note that under this assumption
the new system has a solution for any τ ≥n + 2i+1 + 1. With the fact that any
positive number can be written as a sum of powers of 2, this property makes
possible to combine multiple copies of 2i+1-adders inductively in order to provide
a variable with an arbitrarily large lower bound.
In Sect. 4, the 2i+1-adder and what we shall build based on it in Sect. 3.2 will
be transformed into shapes in order to prove two of our main results (Theorems 3
and 4). To this end, it must be noted that these systems of τ-inequalities are
quadripartite, that is, we can quarter their variable set such that each inequality
contains at most one variable from each of the four resulting variable subsets.
3.2
FINDOPTIMALSTRENGTH is NP-hard
Let us prove the NP-hardness of FINDOPTIMALSTRENGTH. For the reduction, we
employ a variant of 3-SAT called (monotone) 1-IN-3-SAT introduced by Schaefer
[10], in which no literal is negated and one is required to ﬁnd a truth assignment
such that each clause has exactly one true literal. He proved its NP-completeness.
We propose its restricted variant called quadripartite 1-IN-3-SAT, whose instance
consists of a variable set that is a union of four pairwise disjoint sets U1, U2, U3, U4
and clauses that contain at most one variable from each of these four subsets.
Lemma 1. Quadripartite 1-IN-3-SAT is NP-complete.
Theorem 1. For any τ ≥4, τ-THRESHOLDPROGRAMMING(4, 3) is NP-complete.
Proof. A proof for τ = 4 comes ﬁrst. Due to Lemma 1, we convert an instance
of quadripartite 1-IN-3-SAT, given as a pair of a set of Boolean variables U =
{u1, . . . , un} and that of clauses C = {c1, . . . , cm}, into an instance of τ-TP(4, 3).
The quadripartite property is not needed here, but will be so in Sect. 4.
Let us convert this SAT instance into a system S of τ-inequalities with pos-
itive integer variables v1, v2, . . . , vn (needless to say, (1) is used here for their
positiveness), which correspond to the SAT variables in U, such that the SAT
instance is satisﬁable if and only if the system is solvable. In S, the j-th clause
of C, cj = {uj1, uj2, uj3} with 1 ≤j1, j2, j3 ≤n, is represented as
vj1 + vj2 + vj3 ≥4, vj1 + vj2 < 4, vj1 + vj3 < 4, and vj2 + vj3 < 4,
(3)
which is equivalent to the equation vj1 + vj2 + vj3 = 4 due to the assumption
that vj1, vj2, vj3 ≥1. Its solution must be that exactly one of the three variables
is 2 and the others are 1. Therefore, if S is solvable, then by interpreting those in
v1, . . . , vn with value 2 be positive and the other (that is, with value 1) negative,
we can retrieve a way to satisfy the 1-IN-3-SAT instance; and vice versa. Thus,
4-TP(4, 3) is NP-complete (in fact, we proved that even 4-TP(3, 2) is so).
Now the result is generalized for an arbitrary τ ≥4. As presented in Sect. 3.1,
we can design τ-inequalities of at most 3 terms that provide auxiliary variables

On the Behavior of Tile Assembly System at High Temperatures
555
x1, x2, xτ−4 with lower bounds 1, 2, and τ−4, respectively. Combining them with
an inequality x1 + x2 + xτ−4 < τ implies that in any solution of S, xτ−4 must
assume the value τ−4. We add this “constant” to the inequalities in (3) as
vj1 + vj2 + vj3 + xτ−4 ≥τ,
vj1 + vj2 + xτ−4 < τ
(4)
and the analogs of this <τ-inequality for vj1+vj3 and vj2+vj3. Since xτ−4 = τ−4,
these four τ-inequalities are equivalent to vj1 + vj2 + vj3 = 4.
We conclude this proof by noting that any τ-inequality used is either a ≥τ-
inequality of at most 4 terms or a <τ-inequality of at most 3 terms.
⊓⊔
Theorem 1 leads us to the NP-hardness of TP. Deleting the constant term xτ−4
from the τ-inequalities x1 + x2 + xτ−4 < τ and (4) yields x1 + x2 < τ and
vj1 + vj2 + vj3 ≥τ, vj1 + vj2 < τ, vj1 + vj3 < τ, and vj2 + vj3 < τ,
(5)
and consider optimizing τ. Due to x1 +x2 < τ, the minimal possible value of τ is
4. Then its optimal value is 4 if and only if the 1-IN-3-SAT instance is satisﬁable.
Lemma 2. THRESHOLDPROGRAMMING(4, 3) is NP-hard.
Using this instance, now we can prove that FINDOPTIMALSTRENGTH is NP-hard.
Making use of the fact that all of its τ-inequalities contain at most 4 terms, we
transform them into cooperation sets of a strength-free TAS. The inequalities
(5), which are for the clause cj, are encoded as tcj = (vj1, vj2, vj3, xτ) with
D(tcj) = P({N, W, S, E})\{{N}, {W}, {S}, {N, W}, {N, S}, {W, S}}, where an auxiliary
variable xτ is introduced whose glue is so strong that its attachment needs no
cooperation. The other inequalities are encoded similarly.
Theorem 2. FINDOPTIMALSTRENGTH is NP-hard.
One observation of technical importance is that all the systems built in this
section can be solved such that the values of all variables are strictly less than
τ. Its check is left to the reader.
4
Tile Complexity at High Temperatures
In Sect. 3, bridges from 1-IN-3-SAT to TP(4, 3) and further to the local, or mi-
cro, behavior of a TAS have been established. Since the study of TAS aims at
facilitating the design of nano-scale structures, we should shift our focus onto
their global (or macro, terminal) behavior; how a given shape is built by TASs.
Problems of interest include: for any temperature τ given as parameter,
1. Is there a shape Sτ that prefers the temperatures above τ to the lower ones
in terms of tile complexity; that is, Cdtilec(<τ)(Sτ) > Cdtilec(τ)(Sτ)? If so,
then can we design an algorithm to construct Sτ?
2. Can we compute Cdtilec(≤τ)(S) of a shape S in a polynomial time?

556
S. Seki and Y. Okuno
Fig. 1.
A logical component for a ≥τ-inequality of 4 terms. Positions p1, p2, p3, p4
(speciﬁed by their subscripts) are at the tips of the four trees of color red, green, blue,
and yellow from the left. These trees are associated with its four variables. The left is a
gadget consisting of a single gray tile at the position p5 supported by four pillars that
are of the same shape as the four respective variable trees.
For τ = 2, these problems have been studied intensively. Adleman et al. proved
that Cdtilec(2)(Sqn) = O(
log n
log log n) for any n × n square Sqn [1]. In contrast,
Cdtilec(1)(Sqn) ≤2n−1 and conjectured to be tight [2], which is highly probable.
Thus, Cdtilec(<2)(Sqn) ≫Cdtilec(2)(Sqn), provided the conjecture is true. As for
the second problem, computing Cdtilec(≤2)(S) of a given shape S is NP-hard [2].
We shall work on these problems without any constraint on temperature, and
answer them as follows.
Theorem 3. For any τ
≥2, there is a shape Sτ with Cdtilec(<τ)(Sτ) >
Cdtilec(τ)(Sτ).
Theorem 4. For any τ ≥4, it is NP-hard to compute the directed tile complexity
of a shape at the temperatures below τ.
These theorems will be proved by choosing a proper system of τ-inequalities
among those built in Sect. 3 (recall that they are quadripartite) and transform-
ing it into a shape S and a constant c. Let V = {v1, v2, . . . , vn} be the set of
all variables occurring in the system, which can be divided into four subsets
VN, VW, VS, VE due to the quadripartite property. In addition, as mentioned at the
end of Sect. 3, we can assume that vi < τ for all 1 ≤i ≤n; this is only for
simplifying the design of S and our explanation below.
Consider a directed TAS T = (T, σ, g, τ) that self-assembles S, whatever it
is. First of all, we convert the variables in V into trees of height h (a parameter
adjustable for our convenience) including a special position called tip. In Fig. 1,
the positions p1, . . . , p4 are the tip of trees including them respectively. Trees
for the variables in VN (VW, VS, and VE) are designed so as to stick out their tip
from north (resp., west, south, and east) as the red (yellow, green, and blue)
tree in Fig. 1. We say that these trees are of north, west, south, and east type,
respectively. Trees of the same directional type are of the identical shape except
their identiﬁcation “bit patterns” of length ⌈log n⌉. These n trees will be a part of
S, and T actually needs nh + 
1≤i≤n ci tile types (see [2] for details) no matter

On the Behavior of Tile Assembly System at High Temperatures
557
Fig. 2. A logical component for <τ-inequalities. The variable trees are written con-
cisely, but only due to the space limit; they should be of shape as illustrated in Fig. 1.
what its temperature is (tree’s tile complexity is temperature-independent); h+ci
tile types are exclusively for the tree for vi, where ci is a constant. We adjust h
such that h ≫max{ci | 1 ≤i ≤n}. By such an adjustment, one can also force
T to put its seed somewhere on the scaﬀold (this technique is used in [2,4]).
Let us convert one ≥τ-inequality of 4 terms in the system into a shape. The
system’s quadripartite property guarantees that its four variables are represented
as trees of four diﬀerent directional types. Let T assemble the whole shape in
Fig. 1. It must put tiles of distinct types t1, t2, t3, t4 at p1, p2, p3, p4, respectively.
As seen in Fig. 1, these trees can be bundled closer into a gadget so that glues on
their tips can cooperate to attach the grey tile at the position p5. This actually
suggests a way for T to accomplish the assembly process using strictly less than
5h tile types; that is, reusing the four variable trees for the gadget and letting
the tip glues t1(S), t2(E), t3(N), and t4(W) cooperate to attach the grey tile. For
this cooperation to happen, g must assign these glues with strengths such that
g(t1(S)) + g(t2(E)) + g(t3(N)) + g(t4(W)) ≥τ.
(6)
We can interpret this as; T assigns the variables that are represented by the
trees, or more precisely, by their tip glues with values so that their sum is at
least τ. This means that posing a proper upper bound on the number of available
tile types turns T into a mechanism to solve the given ≥τ-inequality.
There must be an objection that T could be designed so as to assemble the
shape in a diﬀerent manner. For example, another green tree could be made
for the gadget from scratch and T lets it singly-support the grey tile, which in
turn needs no interaction with the others any more. Then, the component gets
irrelevant to the ≥τ-inequality (6). However, this “cheating” costs T extra O(h)
tile types for the new green tree. Without any proof, we claim that unless 5h tile
types were available, T could not help but adopt our original way of assembly,
and hence, it assigns the tip glues (variables) with strengths (values) so as to
satisfy (6).
Due to the lack of space, a logical component for <τ-inequalities of at most 3
terms can be only illustrated in Fig. 2. One thing to be emphasized is that this
cannot be done simply by modifying the design for ≥τ-inequalities by leaving the
position p5 empty. This is because no attachment may not mean the insuﬃcient
strength but label unmatching. We claim that unless 5h tile types are available
for T , the strengths assigned to the tip glues of red, yellow, and blue trees must
be all positive and their sum must be strictly less than τ.

558
S. Seki and Y. Okuno
Every τ-inequality is thus converted into a component (note that the above-
mentioned component designs can be easily modiﬁed for τ-inequalities of less
terms). When a variable is common in inequalities, the resulting components
should share the corresponding tree. The quadripartite property makes this shar-
ing possible. Being mounted on a scaﬀold, these components amount to S.
The constant c should be set to (nh + 
1≤i≤n ci) + h + c′, where the extra
h is for the auxiliary (white) tree in the component for <τ-inequalities, and
c′ is the number of tiles for the scaﬀold. Setting h > 
1≤i≤n ci + c′ secures
c < (n + 2)h. For any k ≥2, now we can transform the system of τ-inequalities
designed in Sect. 3.1 that forces τ be at least k into a shape Sk such that
Cdtilec(<k)(Sk) > Cdtilec(k)(Sk) in a straightforward way. This proves Theorem 3.
Thus converting the TP instance built for Theorem 1 instead brings Theorem 4.
Acknowledgements. The authors thank anonymous referees for their valuable
comments and suggestions on an earlier version and David Doty for fruitful and
insightful discussions. This work was carried out under the ﬁnancial support
from Funding Program for Next Generation World-Leading Researchers (NEXT
program) to Y. O. and from Kyoto University Start-up Grant-in-Aid for Young
Scientists No. 021530 to S. S. Part of this work was also ﬁnancially supported
by the Department of Information and Computer Science of Aalto University.
References
1. Adleman, L.M., Cheng, Q., Goel, A., Huang, M.D.: Running time and program
size for self-assembled squares. In: Proc. of STOC 2001, pp. 740–748. ACM (2001)
2. Adleman, L.M., Cheng, Q., Goel, A., Huang, M.D., Kempe, D., de Espan´es, P.M.,
Rothemund, P.W.K.: Combinatorial optimization problems in self-assembly. In:
Proc. of STOC 2002, pp. 23–32 (2002)
3. Barish, R.D., Schulman, R., Rothemund, P.W., Winfree, E.: An information-
bearing seed for nucleating algorithmic self-assembly. Proceedings of the National
Academy of Sciences 106(15), 6054–6059 (2009)
4. Bryans, N., Chiniforooshan, E., Doty, D., Kari, L., Seki, S.: The power of nonde-
terminism in self-assembly. In: Proc. of SODA 2011, pp. 590–602 (2011)
5. Chen, H.L., Doty, D., Seki, S.: Program Size and Temperature in Self-Assembly. In:
Asano, T., Nakano, S.-I., Okamoto, Y., Watanabe, O. (eds.) ISAAC 2011. LNCS,
vol. 7074, pp. 445–453. Springer, Heidelberg (2011)
6. Chen, H.L., Schulman, R., Goel, A., Winfree, E.: Reducing facet nucleation during
algorithmic self-assembly. Nano Letters 7(9), 2913–2919 (2007)
7. Fu, T.J., Seeman, N.C.: DNA double-crossover molecules. Biochemistry 32, 3211–
3220 (1993)
8. Fujibayashi, K., Hariadi, R., Park, S.H., Winfree, E., Murata, S.: Toward reliable
algorithmic self-assembly of DNA tiles: A ﬁxed-width cellular automaton pattern.
Nano Letters 8(7), 1791–1797 (2009)
9. Rothemund, P.W.K., Winfree, E.: The program-size complexity of self-assembled
squares (extended abstract). In: Proc. of STOC 2000, pp. 459–468 (2000)
10. Schaefer, T.J.: The complexity of satisﬁability problems. In: Proc. of STOC 1978,
pp. 216–226 (1978)

On the Behavior of Tile Assembly System at High Temperatures
559
11. Seeman, N.C.: Nucleic-acid junctions and lattices. Journal of Theoretical Biol-
ogy 99, 237–247 (1982)
12. Wang, H.: Proving theorems by pattern recognition - II. The Bell System Technical
Journal XL (1), 1–41 (1961)
13. Winfree, E.: Algorithmic Self-Assembly of DNA. Ph.D. thesis, California Institute
of Technology (June 1998)

Abstract Partial Cylindrical Algebraic
Decomposition I: The Lifting Phase
Grant Olney Passmore1,2 and Paul B. Jackson2
1 Clare Hall, University of Cambridge, Herschel Road,
Cambridge CB3 9AL, United Kingdom
grant.passmore@cl.cam.ac.uk
2 Laboratory for Foundations of Computer Science, School of Informatics,
University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, Scotland
pbj@inf.ed.ac.uk
Abstract. Though decidable, the theory of real closed ﬁelds (RCF) is
fundamentally infeasible. This is unfortunate, as automatic proof meth-
ods for nonlinear real arithmetic are crucially needed in both formalised
mathematics and the veriﬁcation of real-world cyber-physical systems.
Consequently, many researchers have proposed fast, sound but incom-
plete RCF proof procedures which are useful in various practical appli-
cations. We show how such practically useful, sound but incomplete RCF
proof methods may be systematically utilised in the context of a com-
plete RCF proof method without sacriﬁcing its completeness. In partic-
ular, we present an extension of the RCF quantiﬁer elimination method
Partial CAD (P-CAD) which uses incomplete ∃RCF proof procedures
to “short-circuit” expensive computations during the lifting phase of P-
CAD. We present the theoretical framework and preliminary experiments
arising from an implementation in our RCF proof tool RAHD.
1
Introduction
Tarski’s theorem that the elementary theory of real closed ﬁelds (RCF) admits
eﬀective elimination of quantiﬁers is one of the longstanding hallmarks of math-
ematical logic [13]. From this result, the decidability of elementary algebra and
geometry readily follow, and a most tantalising situation arises: In principle, ev-
ery elementary arithmetical conjecture over ﬁnite-dimensional real and complex
spaces may be decided simply by formalising the conjecture and asking a com-
puter of its truth. So why then do we still not know how many unit hyperspheres
may kiss1 in ﬁve dimensions? Is it 41? 42?
The issue is one of complexity. Though decidable, RCF is fundamentally in-
feasible. Due to Davenport-Heintz [5], it is known that there exist families of
n-dimensional RCF formulas of length O(n) whose only quantiﬁer-free equiva-
lences must contain polynomials of degree 22Ω(n)
and of length 22Ω(n).
1 See, e.g., [11] for background on the kissing problem for n-dimensional hyperspheres.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 560–570, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Abstract Partial Cylindrical Algebraic Decomposition I: The Lifting Phase
561
Nevertheless, there are countless examples of diﬃcult, high-dimensional RCF
problems solved in mathematical and engineering practice. What is the discon-
nect? (1) RCF problems solved in practice are most often solved using an ad
hoc combination of methods, not by a general decision method. (2) RCF prob-
lems arising in practice commonly have structural properties dictated by the
application domain from which they originated. Such structural properties can
often be exploited making such problems more amenable to analysis and pushing
them within the reaches of restricted, more eﬃcient variants of known decision
methods.
With this in mind, many researchers have proposed fast, sound but incom-
plete RCF proof procedures, many of them being of substantial practical use
[1,7,14,10,12,6,4]. This is especially true for formal methods, where improved
automated RCF proof methods are needed in the formal veriﬁcation of cyber-
physical systems. In these cases, as the RCF problems to be analysed are usually
machine-generated (and incomprehensibly large), incomplete proof procedures
can go a long way. For example, there is no denying the fact that applying a
full quantiﬁer elimination algorithm to decide the falsity of a formula such as
∃x1, . . . , x100 ∈R (x1 ∗x1 + . . . + x100 ∗x100 < 0) is an obvious misappropriation
of resources. While such an example may seem contrived, consider the fact that
when an RCF proof method is used in formal veriﬁcation eﬀorts, it is often fed
huge collections of machine-generated formulas which may be (un)satisﬁable for
extremely simple reasons. Ideally, one would like to be able to use fast, sound but
incomplete proof procedures as much as possible, falling back on the far more
computationally expensive complete methods only when necessary. It would be
desirable to have a principled manner in which incomplete proof methods could
be used to improve the performance of a complete method without sacriﬁcing
its completeness.
We present Abstract Partial Cylindrical Algebraic Decomposition (AP-CAD),
an extension of the RCF quantiﬁer elimination procedure partial CAD. In AP-
CAD, arbitrary sound but possibly incomplete ∃RCF proof procedures may be
used to “short-circuit” certain expensive computations during CAD construc-
tion. This is done in such a way that the completeness of the combined proof
method is guaranteed. We restrict our AP-CAD presentation to the practically
useful case of ∃RCF. We have implemented AP-CAD within our RCF proof
tool RAHD [9] for the case of full-dimensional cell decompositions and present
experiments. RAHD contains many RCF proof methods and allows users to
combine them into their own heuristic RCF proof procedures through a proof
strategy language. This is ideal for AP-CAD, as the proof procedure parameters
used by AP-CAD can be formally realised as RAHD proof strategies.
2
CAD Preliminaries
For a detailed account of CAD, we refer the reader to [2]. We present only
the background on (P-)CAD required to understand AP-CAD for ∃RCF. P-
CAD is currently the most eﬃcient known general quantiﬁer elimination method

562
G.O. Passmore and P.B. Jackson
for RCF2. An important fact is that the complexity of the (P-)CAD decision
algorithm is doubly exponential in the dimension (number of variables) of its
input formula. Generally, the most expensive phase of the (P-)CAD algorithm
is the so-called “lifting phase.” Let us ﬁx some notation.
A semialgebraic set is a subset of Rn deﬁnable by a quantiﬁer-free formula in
the language of ordered rings. A region of Rn is a connected component of Rn.
An algebraic decomposition of Rn is a decomposition of Rn into ﬁnitely many
semialgebraic regions. A cylindrical algebraic decomposition is a special type of
algebraic decomposition whose regions are in a sense “well-behaved” with respect
to projections onto lower dimensions. A cell is a region of a CAD.
Before delving into technical details, let us discuss how we can use a CAD to
make ∃RCF decisions. By “the polynomials of (an ∃RCF formula) ϕ,” we shall
mean the collection of polynomials obtained by zeroing the RHS of every atom
in ϕ through subtracting the RHS from both sides. We assume each such ∃RCF
formula is in prenex normal form, so that it is an ∃-closed boolean combination
of sign conditions, i.e., of atoms of the form (p ⊙0) with p ∈Z[x1, . . . , xn],
⊙∈{<, ≤, =, ≥, >}. We use QF(ϕ) to mean the quantiﬁer-free matrix of ϕ.
The key point is that if we have in hand a suitable CAD C = {c1, . . . , cm} ⊂
2Rn derived from an ∃RCF formula ϕ, we can decide the truth of ϕ from
the CAD directly. The reason is simple: C will have the property that every
polynomial of ϕ has constant sign on each ci, i.e., given p a polynomial of ϕ
and a ci a cell, it shall hold that ∀r ∈ci(p(r) = 0) ∨∀r ∈ci(p(r) > 0) ∨
∀r ∈ci(p(r) < 0). Consequently, QF(ϕ) has constant truth value at every point
in a given cell. Thus, to decide ϕ, we simply substitute a single sample point
from each ci into QF(ϕ) and see if it ever evaluates to true. It will evaluate to
true on at least one sample point if and only if ϕ is true over Rn.
We shall deﬁne CAD by induction on dimension3. A CAD of R is a decom-
position of R into ﬁnitely many cells ci ⊆R s.t. each ci is of the form (i) {α1},
or (ii) ]α1, α2[, or (iii) ]-∞, α1[ or ]α1, +∞[ for algebraic real numbers αi. Let A
be a region of Ri. We call A × R the cylinder over A and denote it by Z(A).
Deﬁnition 1 (Stack). Let f1, . . . , fk ∈C(A, R). That is, fj is a continuous
function from A to R. Furthermore, suppose that the images of the fj are ordered
over A s.t. ∀α ∈A (fj(α) < fj+1(α)). Then, f1, . . . , fk induce a stack S over
A, where S is a decomposition of Z(A) into 2k+1 regions of the following form:
– r1 = {⟨α, x⟩| α ∈A, x < f1(α)},
r3 = {⟨α, x⟩| α ∈A, f1(α) < x < f2(α)},
...
r2k−1 = {⟨α, x⟩| α ∈A, fk−1(α) < x < fk(α)},
r2k+1 = {⟨α, x⟩| α ∈A, fk(α) < x},
2 See [8] for an explanation as to why P-CAD is also currently the best known general
decision method for practical ∃RCF problems, despite the fact that ∃RCF has a
theoretical exponential speed-up over RCF.
3 We shall speak freely of the symbolic manipulation and arithmetic of (irrational)
real algebraic numbers. See, e.g., [2] for an algorithmic account.

Abstract Partial Cylindrical Algebraic Decomposition I: The Lifting Phase
563
– r2 = {⟨α, x⟩| α ∈A, x = f1(α)},
...
r2k = {⟨α, x⟩| α ∈A, x = fk(α)}.
A CAD of Ri+1 will be obtained from a CAD C of Ri by constructing a stack
over every cell in C.
Deﬁnition 2 (CAD in Ri+1). An algebraic decomposition Ci+1 of Ri+1 is a
CAD iﬀCi+1 is a union of stacks Ci+1 = k
j=1 wj, s.t. the stack wj is constructed
over cell cj in a CAD Ci = {c1, . . . , ck} of Ri.
The P-invariance property will allow us to use CADs to make ∃RCF decisions.
Deﬁnition 3 (P-invariance). Let P = {p1, . . . , pk} ⊂Z[x1, . . . , xn] and A
be a region of Rn. Then, we say A is P-invariant iﬀevery member of P has
constant sign on A. That is given any pi ∈P,
∀r ∈A(pi(r) = 0)
∨
∀r ∈A(pi(r) > 0)
∨
∀r ∈A(pi(r) < 0).
Given a CAD C, we say C is P-invariant iﬀevery cell of C is P-invariant.
2.1
CAD Construction and Evaluation for ∃RCF
The use of CADs for deciding ∃RCF sentences will take place in four steps. In
what follows, ϕ is an ∃RCF sentence and P = {p1, . . . , pk} ⊂Z[x1, . . . , xn] is
the collection of polynomials of ϕ.
Projection. The projection phase will begin with P and iteratively apply a
projection operator Proji of the form Proji : 2Z[x1,...,xi+1] →2Z[x1,...,xi] until
a set of polynomials is obtained over Z[x1]. This process will consist of levels,
one for each dimension, s.t. at each level i we shall have what is called a level-
i projection set, Pi ⊂Z[x1, . . . , xi]. These level-i projection sets will have a
special property: If we have a Pi-invariant CAD of Ri, then we can use this
CAD to construct a Pi+1-invariant CAD of Ri+1.
Base. The base phase consists of computing a P1-invariant CAD of R1, implic-
itly described as a sequence of sample points, one for each cell in the CAD.
This can be done by univariate real root isolation and basic machinery for
arithmetic with real algebraic numbers. Let us suppose we have done this
and our sequence of sample points is s1 < s2 < . . . < s2m+1.
Lifting. The lifting phase will take an implicit description of a P1-invariant
CAD of R1 and progressively transform it into an implicit description of Pn-
invariant CAD of Rn. Let C = {c1, . . . , cm} be the Pi-invariant CAD for Ri
which we shall lift to a Pi+1-invariant CAD of Ri+1. Let S = {s1, . . . , sm}
be our set of sample points, one from each cell in C. Then, for each cell cj,
we shall use the sample point sj ∈cj to construct a set of sample points in
Ri+1 corresponding to a stack over cj:

564
G.O. Passmore and P.B. Jackson
1. As sj ∈Ri, we have that sj = ⟨r1, . . . , ri⟩for some r1, . . . , ri ∈R.
2. Let Pi+1[sj] denote Pi+1[x1 →r1, x2 →r2, . . . , xi →ri]. Then Pi+1[sj] ⊂
Z[xi+1] is a univariate family of polynomials.
3. Using the same process as we did in the base phase, compute a Pi+1[sj]-
invariant CAD of R1. Let this CAD be represented by a sequence of
sample points t1 < t2 < . . . < t2v+1 ∈R.
4. Then, the stack over cj will be represented by the set of 2v + 1 sample
points obtained by appending each tj to the lower-dimensional sample
point sj. That is, our stack over cj will be represented by the following
sequence of sample points z1, . . . , z2v+1 in Ri+1: z1 = ⟨r1, . . . , ri, t1⟩,
z2 = ⟨r1, . . . , ri, t2⟩, . . ., z2v+1 = ⟨r1, . . . , ri, t2v+1⟩.
In the above construction, we call the cell cj (or the sample point representing
it, sj) the parent of the stack {z1, . . . , z2v+1}.
Evaluation. Let S = {s1, . . . , sm} ⊂Rn be our ﬁnal set of sample points.
Return the boolean value 
r∈S QF(ϕ)[r].
2.2
Partial CAD
Let us now sketch the idea of partial CAD, due to Collins and Hong [3]. As it
stands, the CAD construction algorithm will build a P-invariant CAD induced
by the polynomials P of an ∃RCF formula ϕ without paying any attention
to the logical content of the formula itself. But, when performing lifting, i.e.,
constructing a stack of regions of Ri+1 over a lower-dimensional cell cj ⊂Ri,
we may be easily able to see — simply by substitution and evaluation — that
the formula QF(ϕ) could never be satisﬁed over cj. For instance, let QF(ϕ) =

(x4
4 + x3x3
2 + 3x1 > 2x4
1) ∧(x2
1 > x2 + x3)

. If cj is a cell in a P3-invariant
CAD of R3 represented by the sample point sj = ⟨0, 1, 5⟩, then we can see
QF(ϕ) will never be satisﬁed over a cell in a stack which is a child of cj. Thus,
we need not lift over cj and can eliminate it.
This is the idea behind partial CAD when applied to ∃RCF formulas:
Before lifting over a cell in a CAD of Ri, check if there are any atoms in
your formula only involving the variables x1, . . . , xi. If so, then perform par-
tial evaluation of your formula by evaluating those atoms upon your sample
point in Ri, and then use simple propositional reasoning to try to deduce the
truth of your formula. This can also allow us to ﬁnd a satisfying assignment
for the variables in QF(ϕ) without constructing a whole CAD. For instance,
let QF(ϕ) =

(x4
4 + x3x3
2 + 3x1 > 2x4
1) ∨(x2
1 < x2)

. If cj is a cell in a P2-
invariant CAD of R2 represented by the sample point sj = ⟨−1, 2⟩, then we can
see immediately by substitution that QF(ϕ) is satisﬁable over R4. As a witness
to this satisﬁability, we may return ⟨−1, 2, r3, r4⟩where r3, r4 ∈R are arbitrary.
3
Abstract Partial CAD
From a high level of abstraction, we can see partial CAD to be normal CAD
augmented with three pieces of algorithmic data:

Abstract Partial Cylindrical Algebraic Decomposition I: The Lifting Phase
565
1. A strategy for selecting lower-dimensional cells to use for evaluating lower-
dimensional atoms in our input formula,
2. An algorithm which when given a cell cj will construct a formula F(cj)
which, if it both has a truth value and is decided, can be used to tell (i) if
the cell cj can be thrown away (i.e., F(cj) is decided to be false), or (ii)
if a satisfying assignment for our formula can be extracted already from a
lower-dimensional cell (i.e., F(cj) is decided to be true),
3. A proof procedure which will be used to decide the formulas F(cj) generated
by the algorithm above.
In fact, in their original paper on partial CAD, Collins and Hong make the point
that diﬀerent cell selection strategies could be used and even implement and
experiment with a number of them4. For partial CAD restricted to ∃RCF,
these three pieces of algorithmic data described above would be:
1. Select cells ci ∈C in some speciﬁed enumeration order (speciﬁed by s):
cs(1), cs(2), cs(3), . . . .
2. Given a cell cj represented by a sample point sj = ⟨r1, . . . , ri⟩∈Ri, the
formula F(cj) will be constructed from our original ∃RCF formula ϕ by
the following process:
(a) Let ϕ′ be QF(ϕ) augmented by instantiating x1 with r1, x2 with r2, . . ..
(b) Evaluate all variable-free atoms in ϕ′ to obtain a new formula ϕ′′.
(c) Replace all (unique) variable-containing atoms in ϕ′′ with fresh propo-
sitional variables to obtain a new formula F(cj).
3. Use a propositional logic proof procedure to attempt to decide F(cj).
If F(cj) is false (i.e., unsatisﬁable), cell cj can be abandoned and we need not
lift over it. If F(cj) is true (i.e., tautologous), then we can extract a witness to
the truth of ϕ from the sample point sj. Otherwise, we lift over cj. These three
pieces of data give us the widely-used partial CAD of Collins and Hong. But,
from this point of view, we see that there are many other choices we could make.
3.1
Stages, Theatres and Lifting
The fundamental notion of AP-CAD will be that of an stage5. Let L∃OR be the
fragment of the language of ordered rings consisting of purely ∃prenex sentences.
Deﬁnition 4 (Stage). A stage A = ⟨⟨S, w⟩, F, P⟩will be given by three pieces
of algorithmic data. We describe a stage by how it acts in the context of a ﬁxed
(but arbitrary) i-dimensional space Ri. These data are as follows:
4 For Collins and Hong, a cell selection strategy selects single cells in some speciﬁed
order. In Abstract Partial CAD, cell selection strategies will select sets of cells in
some speciﬁed order and ∃RCF proof procedures will be applied to see if every cell
in a selected set of cells may be eliminated.
5 The intended connotation is of a stage in a theatre.

566
G.O. Passmore and P.B. Jackson
1. A cell selection strategy for selecting subsets of Ci for analysis (we call such
a subset a “selection of cells”),
2. A formula construction strategy for constructing an ∃RCF formula whose
truth value will correspond to the relevance of a selection of cells (we call
such a formula a “cell selection relevance formula”),
3. An ∃RCF proof procedure used to (attempt to) decide the truth or falsity
of a cell selection relevance formula.
In the context of CAD construction, sample points will be eliminated when their
corresponding cells are deemed to be irrelevant to the ∃RCF formula inducing
the CAD. This removal might then result in a set of sample points for which the
cell selection function behaves diﬀerently than it did initially. This motivates the
containment axiom for covering width functions, so that these dynamics do not
result in a non-terminating CAD-based decision algorithm employing the stage
machinery. In what follows, let Ri = {s ⊂Ri | |s| < ω}.
1. A cell selection strategy consists of two components:
(a) A covering width function w : Ri →N,
(b) A cell selection function S : Ri × N →Ri obeying for all Si ∈Ri and all
j ∈{1, . . ., w(Si)} the containment axiom: S(Si, j) ⊂Si.
2. A formula construction strategy is a function F : L∃OR × Ri →L∃OR
obeying certain relevance judgment axioms. To describe these axioms, we
need the context of a ﬁxed (but arbitrary) ∃RCF formula and an associ-
ated Pi-invariant CAD of Ri. Let ϕ be an ∃RCF formula with polynomials
P ⊂Z[x1, . . . , xn] and let Pn, . . . , P1 be a sequence of level-(n, . . . , 1) pro-
jection sets rooted in P (recall Pn = P). Let Ci = {c1, . . . , cm} be a Pi-
invariant CAD of Ri with Si a set of sample points drawn from a subset of
the cells in Ci. If we are given a set of sample points {sa1, . . . sav} ⊆Si,
then △({sa1, . . . sav}) will denote the set of cells from which the sample
points saj are drawn. Then, for each set of sample points Si and each
j ∈{1, . . ., w(Si)} the following relevance judgment axioms must hold:
[(RCF |= ¬F(ϕ, S(Si, j))) =⇒N(ϕ, S(Si, j))] , and [(RCF |= F(ϕ, S(Si, j)))
=⇒RCF |= ϕ], where N(ϕ, {sa1, . . . sav}) means that no child (at any an-
cestral depth, i.e., in a Pi+1-invariant CAD of Ri+1, in a Pi+2-invariant
CAD of Ri+2, . . . , in a Pn-invariant CAD of Rn) of any cell in the set
Δ({sa1, . . . , sav}) will satisfy QF(ϕ).
3. An ∃RCF proof procedure is a function
P : L∃OR →
⎛
⎝{true, false, unknown} ∪

j∈N+
Rj
⎞
⎠
obeying the soundness axioms: ((P(ψ) = true)
=⇒
RCF |= ψ), ((P(ψ) =
false)
=⇒
RCF |= ¬ψ), ((P(ψ) ∈Rj)
=⇒
RCF |= QF(ψ)[P(ψ)])
for arbitrary ψ ∈L∃OR and with QF(ψ)[P(ψ)] in the ﬁnal axiom being the
substitution of the j-vector P(ψ) (or an arbitrary extension of it to the dimen-
sion of the polynomials appearing in ψ) into ψ, resulting in a variable-free
formula. In this case, we call P(ψ) a witness to the truth of ψ.

Abstract Partial Cylindrical Algebraic Decomposition I: The Lifting Phase
567
We shall want to have the freedom to give our AP-CAD algorithm a sequence
of stages, one for each dimension 1, . . . , n. The intuition is as follows: Stages are
introduced so that one can present a strategy to an underlying CAD decision
algorithm which will prescribe a method for the algorithm to recognise when it
can short-circuit certain expensive computations. If we can abandon a cell at
a low dimension, for instance at the base phase or when beginning to lift over
cells of R2, then this can potentially give us hyper-exponential savings down the
line. Thus, it makes sense to arrange stages A1, A2, . . . An so that stage A1 works
hardest to make relevance judgments about cells. For if A1 causes us to throw
away cell cj ⊂R1, then this could lead to huge savings later. Then, A2 might
still work hard but a bit less hard, and so on. This collection of stages gives rise
to the notion of an n-theatre. In what follows, let Θ be the set of all stages.
Deﬁnition 5 (Theatre). An n-theatre T is a function T : {1, . . . , n} →Θ.
Stage i in a theatre will be used to make judgments about cells in a Pi-invariant
(partial) CAD of Ri (i.e., at level i). Let us describe a decision method we shall
use for deciding ∃RCF sentences in the framework of AP-CAD.
Algorithm 1 (AP-CAD with Theatrical Lifting). Suppose we are given an
∃RCF sentence ϕ with polynomials P ⊂Z[x1, . . . , xn], and an n-theatre T.
1. Projection. As with standard P-CAD, compute a sequence of projection sets
P1, . . . , Pn.
2. Base. As with standard P-CAD, compute a P1-invariant CAD of R1, C1 =
{c1, . . . , c2m+1} represented by sample points S1 = {s1, . . . , s2m+1}. Set the
current dimension i := 1.
3. Lifting. Let T(i) = Ai = ⟨⟨Si, wi⟩, Fi, Pi⟩be the stage for dimension i, and
Si the set of sample points for the Pi-invariant (partial) CAD of Ri over
which we need to lift.
(a) Let U := wi(Si) and let j := 1.
(b) While j ≤U do
i. Let {sa1, . . . , sav} := Si(Si, j).
ii. Let χ := Pi(Fi({sa1, . . . , sav})).
iii. If χ = true, then return true.
iv. If χ = ⟨x1, . . . , xw⟩∈Rw for some w ≤n, then
A. Fix an n-dim. extension of χ, e.g., r = ⟨x1, . . . , xw, 0⟩∈Rn.
B. Evaluate QF(ψ)[r] and set R ∈{true, false} to this result.
C. If R = true, then return r as a witness to the truth of ϕ.
D. If R = false, then return true6.
v. If χ = false, then set S′
i := Si \ {sa1, . . . , sav}, else set S′
i := Si.
6 This is perhaps the one counter-intuitive part of the algorithm. Note, however, that
this is actually correct: By the combination of the second relevance judgment axiom
for Fi and the soundness axioms for Pi, the fact that RCF |= Fi(Si(Si, j)) means
that ϕ is true. It’s just that the witness Pi computed for the truth of Fi(Si(Si, j))
might fail to be a witness for ϕ. In this case, we simply know ϕ is true without
knowing a witness for it.

568
G.O. Passmore and P.B. Jackson
vi. If S′
i = ∅then return false.
vii. If S′
i = Si then set j := j + 1.
viii. If S′
i ⊂Si then
A. Set Si := S′
i.
B. Set U := wi(Si).
C. Set j := 1.
(c) Now, Si = {t1, . . . , tu} contains sample points corresponding to the cells
we have not deemed to be irrelevant. We need to lift over them.
i. Let Si+1 := ∅.
ii. For j from 1 to u do
A. Substitute the components of tj in for the variables x1, . . . , xi in
Pi+1 to obtain a univariate family Pi+1[tj] ⊂Z[xi+1].
B. Compute a Pi+1[tj]-invariant CAD of R1, represented by sample
points Kj.
C. Set Si+1 := Si+1 ∪Kj.
(d) Increase the current dimension by setting i := i + 1.
(e) If i = n then lifting is done and we may proceed to the evaluation phase.
(f) If i < n then we loop and begin the lifting process again, but now with
the set of sample points Si+1.
4. Evaluation. Return the boolean value 
r∈Sn QF(ϕ)[r].
Theorem 2. Algorithm 1 is a sound and complete ∃RCF proof procedure.
Proof. By induction on dimension. (See the extended version of this paper.)
4
Experimental Results
As an experiment (explicated in the extended version of this paper), we built a
concrete AP-CAD theatre combining interval constraint reasoning with standard
partial CAD [9]. As CAD performance is strongly dependent on the number of
cells retained at each dimension, we compared this for three CAD variants: (i)
CAD, (ii) standard P-CAD, and (iii) AP-CAD, w.r.t. an ∃RCF sentence ϕ s.t.
QF(ϕ) =

(x1x4 + x2x4 + x3x2 < 0) ∧(x2 > 0) ∧(x3 > 0) ∧(x4 > 0)
∧(x3x4 −x2
4 + x2
3 + 1 < 0)

.
As QF(ϕ) involves only strict inequalities, we appeal to a theorem of McCal-
lum allowing us to only consider full-dimensional cells (selecting rational sample
points), and compare the methods w.r.t. this CAD variant [9]. We observe that
the AP-CAD method retains less cells than the other methods. This is supported
by experiments we have done with other ∃RCF formulas. In all cases, the cost
of AP-CAD theatre execution measured < 0.01% of the total CPU time, indi-
cating that there is much positive impact to be made by using incomplete RCF
proof procedures to enhance the performance of CAD-based decision methods.
The cell retainment counts are as follows:

Abstract Partial Cylindrical Algebraic Decomposition I: The Lifting Phase
569
CAD P-CAD AP-CAD
Q1
2
2
1
Q2
14
7
5
Q3
40
10
7
Q4
200
50
35
5
Conclusion
AP-CAD allows strategic algorithmic data to be used to “short-circuit” expen-
sive computations during the lifting phase of a CAD-based decision algorithm.
This provides a principled approach for utilising fast, sound but possibly incom-
plete ∃RCF proof procedures to enhance a complete decision method without
threatening its completeness. We see many ways this work might be extended.
It would be very interesting to work out similar machinery to be used during the
projection phase of P-CAD. For this work to bear serious practical fruit, many
more AP-CAD stages should be constructed and experimented with heavily.
Acknowledgements. This paper reports on work presented in Chapters 7 and
8 of the ﬁrst author’s 2011 University of Edinburgh Ph.D. thesis [9], supervised
by the second author. This research was supported by the EPSRC [grant num-
bers EP/I011005/1 and EP/I010335/1]. We thank the referees for their helpful
comments and suggestions.
References
1. Avigad, J., Friedman, H.: Combining Decision Procedures for the Reals. In: Logical
Methods in Computer Science (2006)
2. Basu, S., Pollack, R., Roy, M.F.: Algorithms in Real Algebraic Geometry. Springer,
USA (2006)
3. Collins, G.E., Hong, H.: Partial Cylindrical Algebraic Decomposition for Quantiﬁer
Elimination. J. Sym. Comp. 12(3), 299–328 (1991)
4. Daumas, M., Lester, D., Mu˜noz, C.: Veriﬁed Real Number Calculations: A Library
for Interval Arithmetic. IEEE Trans. Comp. 58(2), 226–237 (2009)
5. Davenport, J.H., Heintz, J.: Real Quantiﬁer Elimination is Doubly Exponential. J.
Symb. Comput. 5, 29–35 (1988),
http://dx.doi.org/10.1016/S0747-71718880004-X
6. Gao, S., Ganai, M., Ivancic, F., Gupta, A., Sankaranarayanan, S., Clarke, E.: Inte-
grating ICP and LRA solvers for deciding nonlinear real arithmetic problems. In:
FMCAD 2010, pp. 81–89 (2010)
7. Harrison, J.: Verifying Nonlinear Real Formulas via Sums of Squares. In: Schnei-
der, K., Brandt, J. (eds.) TPHOLs 2007. LNCS, vol. 4732, pp. 102–118. Springer,
Heidelberg (2007), http://portal.acm.org/citation.cfm?id=1792233.1792242
8. Hong, H.: Comparison of Several Decision Algorithms for the Existential Theory
of the Reals. Tech. rep., RISC (1991)
9. Passmore, G.O.: Combined Decision Procedures for Nonlinear Arithmetics, Real
and Complex. Ph.D. thesis, University of Edinburgh (2011)

570
G.O. Passmore and P.B. Jackson
10. Passmore, G.O., Jackson, P.B.: Combined Decision Techniques for the Existential
Theory of the Reals. In: Carette, J., Dixon, L., Coen, C.S., Watt, S.M. (eds.) MKM
2009, Held as Part of CICM 2009. LNCS (LNAI), vol. 5625, pp. 122–137. Springer,
Heidelberg (2009)
11. Pfender, F., Ziegler, G.M.: Kissing Numbers, Sphere Packings, and Some Unex-
pected Proofs. Notices of the A.M.S. 51, 873–883 (2004)
12. Platzer, A., Quesel, J.-D., R¨ummer, P.: Real World Veriﬁcation. In: Schmidt, R.A.
(ed.) CADE-22. LNCS, vol. 5663, pp. 485–501. Springer, Heidelberg (2009)
13. Tarski, A.: A Decision Method for Elementary Algebra and Geometry. RAND
Corporation (1948)
14. Tiwari, A.: An Algebraic Approach for the Unsatisﬁability of Nonlinear Con-
straints. In: Ong, L. (ed.) CSL 2005. LNCS, vol. 3634, pp. 248–262. Springer,
Heidelberg (2005)

Multi-valued Functions in Computability Theory
Arno Pauly
Clare College, University of Cambridge, Trinity Lane,
Cambridge CB2 1TL, United Kingdom
Arno.Pauly@cl.cam.ac.uk
Abstract. Multi-valued functions are common in computable analysis
(built upon the Type 2 Theory of Eﬀectivity), and have made an appear-
ance in complexity theory under the moniker search problems leading to
complexity classes such as PPAD and PLS being studied. However, a
systematic investigation of the resulting degree structures has only been
initiated in the former situation so far (the Weihrauch-degrees).
A more general understanding is possible, if the category-theoretic
properties of multi-valued functions are taken into account. In the present
paper, the category-theoretic framework is established, and it is demon-
strated that many-one degrees of multi-valued functions form a dis-
tributive lattice under very general conditions, regardless of the actual
reducibility notions used (e.g., Cook, Karp, Weihrauch).
Beyond this, an abundance of open questions arises. Some classic
results for reductions between functions carry over to multi-valued func-
tions, but others do not. The basic theme here again depends on category-
theoretic diﬀerences between functions and multi-valued functions.
Keywords: Multi-valued functions, many-one reduction, Weihrauch re-
ducibility, category theory, degree structure.
1
Introduction
What Are Multi-valued Functions? A (partial) multi-valued function f :⊆
A ⇒B is just a set f ⊆A × B—i.e., a relation. However, the category of
multi-valued functions is not the category of relations! We write f(a) for {b ∈
B | (a, b) ∈f} and dom(f) = {a ∈A | ∃b ∈f(a)}. Then the composition of
multi-valued functions f :⊆A ⇒B, g :⊆B ⇒C is deﬁned via c ∈(g ◦f)(a)
iﬀf(a) ⊆dom(g) and ∃b ∈f(a) s.t. c ∈g(a). In the usual deﬁnition of the
composition for relations, the former condition is absent!
The intended interpretation of a multi-valued function f :⊆A ⇒B is that it
links problem instances to solutions. This draws interest to the following partial
order:
f ⪯g ⇔dom(f) ⊆dom(g) ∧g| dom(f) ⊆f
We can read f ⪯g as f is easier as g: There may be fewer instances for f than
for g, and a solution to a problem instance in g is a solution for it in f, too,
where applicable. This has the consequence that any procedure solving g also
solves f.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 571–580, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

572
A. Pauly
For any two multi-valued functions f, g :⊆A ⇒B there exists a hardest
multi-valued function easier than both, i.e., there are binary inﬁma w.r.t. ⪯.
These are given by f ∧g = (f ∪g)| dom(f)∩dom(g).
Why Use Them? First, multi-valued functions are natural: From elimination
orders on graphs over Nash equilibria in games to ﬁxed points of continuous
mappings, there are plenty of problems without a natural way to specify the
desired solution uniquely. In fact, if one accepts their formulation as multi-valued
functions, one can even prove that the latter two are non-equivalent to any
function!
Then, they are well-behaved under realizability: It is a common situation
in computability and complexity theory that we have an algorithmic notion for
some functions on some special sets X, Y which we intend to lift to more general
sets A, B. We do this by ﬁxing surjective encodings δA :⊆X →A, δB :⊆Y →B,
and then calling, e.g., a function f : A →B computable, iﬀthere is a computable
function F :⊆X →Y such that the following diagram commutes:
X
F
−−−−→Y
⏐⏐δA
⏐⏐δB
A
f
−−−−→B
In general (depending on δA, δB), there will be algorithms (i.e., functions F :⊆
X →Y ) which do not compute any function f : A →B, which leads to the
canonization problem: The desire to ﬁnd an algorithm CA :⊆X →X with the
properties CA(x) = CA(y) whenever δA(x) = δA(y), and δA(CA(x)) = δA(x).
On the other hand, every algorithm computes a multi-valued function, hence,
the canonization problem is relegated to a far less fundamental position.
Algorithms lacking semantics as a function can nonetheless be very mean-
ingful. A common example for this is the multi-valued function χ : R →{0, 1}
with 0 ∈χ(x) iﬀx ≤1 and 1 ∈χ(x) iﬀx ≥0. χ is computable – but the only
computable functions from R to {0, 1} are the constant ones! Hence, when work-
ing with real numbers, tests will have to be non-deterministic, i.e., multi-valued
functions.
Finally, as will be demonstrated in this paper, the properties of multi-valued
functions have a nice impact on the degree-structure of many-one reductions:
One always obtains a distributive lattice here.
Due to lack of space, proofs are omitted here. They can be found in [17,18].
2
Background
Many-one reductions between multi-valued functions have been studied in com-
plexity theory for several decades now, with the complexity classes PPAD [14]
and PLS [12] garnering a lot of attention. Both have a number of very interesting

Multi-valued Functions in Computability Theory
573
complete problems, we just mention ﬁnding Nash equilibria in ﬁnite two player
games with integer payoﬀs as a complete problem for PPAD [7].
There also are a several problems which are known to be in both PPAD and
PLS, but where this is the best classiﬁcation available. Deciding the winner in
parity or discounted payoﬀgames is a typical example here. Despite this strong
motivation to study PPAD∩PLS, only recently it was noticed (in a publication)
that this class actually has complete problems [8]—a fact that is an obvious
consequence of the degree structure being a distributive lattice (which we show
here). A systematic investigation of the degree structure seems to be missing so
far.
In another setting for many-one reductions between multi-valued functions is
the programme to classify the computational content of mathematical theorems
initiated in [3,10]. Here a mathematical theorem of the form
∀x ∈X (x ∈D ⇒∃y ∈Y T (x, y))
is read as a multi-valued function T :⊆X ⇒Y with dom(T ) = D which has to
ﬁnd a witness y ∈Y given some x ∈X. The tool for classiﬁcation is Weihrauch-
reducibility, a form of many-one reducibility introduced originally in [20,21].
Various theorems been classiﬁed in this framework: e.g., the Hahn-Banach
theorem [10], Weak K¨onig’s Lemma, the Intermediate Value theorem [4], Nash’s
theorem on the existence of equilibria [15], Bolzano-Weierstrass [5], Brouwer’s
Fixed Point theorem [6].
Accompanying the investigation of speciﬁc degrees, also the overall degree
structure has been studied. The Weihrauch degrees form a distributive lattice
[4,16], and can be turned into a Kleene algebra when equipped with additional
natural operations ×, ∗[11]. While some additional results in this area do depend
on speciﬁc properties of Weihrauch reducibility, the fundamental ones only use
generic properties of many-one reductions and multi-valued functions—and as
such would also apply to the study of PPAD, PLS, etc.!
In the present paper we outline how the notion of generic properties of many-
one reductions between multi-valued functions can be formalized, and show the
fundamental structural results derivable from them. Then we introduce some
properties that do depend on speciﬁc reducibilities, and both present some results
and open questions.
3
The Category of Multi-valued Functions
It is easy to see that composition of multi-valued functions is associative, so they
form a category Mult. One can lift disjoint unions and cartesian products from
sets to multi-valued functions in the straight-forward way, we shall denote the
results by f + g and f × g. The disjoint union retains its rˆole as the coproduct,
however, the cartesian product is not the categorical product!
This situation is reminiscent of categories of partial functions, and indeed we
can borrow the following:

574
A. Pauly
Deﬁnition 1 (Robinson and Rosolini [19]). A p-category is a category C
together with a naturally associative and naturally commutative bifunctor × :
C × C →C (the (cartesian) product), a natural transformation Δ (the diagonal)
between the identity functor and the derived functor X →X × X, and two fam-
ilies of natural transformations (πA
1 )A∈Ob(C) and (πB
2 )B∈Ob(C) (the projections)
where πA
1 is between the derived functor X →X × A and the identity, while
πB
2 is between the derived functor X →B × X and the identity, such that the
following properties are given:
πX
1 (X) ◦Δ(X)=πX
2 (X) ◦Δ(X) = idX (πY
1 (X) × πX
2 (Y )) ◦Δ(X × Y )=idX×Y
πY
1 (X) ◦(idX × πZ
1 (Y )) = π(Y ×Z)
1
(X) πZ
1 (X) ◦(idX × πY
2 (Z)) = π(Y ×Z)
1
(X)
πX
2 (Y ) ◦(πY
1 (X) × idZ) = π(X×Y )
2
(Z) πX
2 (Z) ◦(πX
2 (Y ) × idZ) = π(X×Y )
2
(Z)
For easier reading, we shall write πX,Y
1
instead of πY
1 (X), πX,Y
2
for πX
2 (Y ) and
ﬁnally ΔX in place of Δ(X). If the superscripts are obvious from the context,
they may be dropped.
The treatment of partial maps in a categorical framework causes the concept
of the domain of a map to split into two separate ones. With Dom(f) we denote
the object A, if f : A →B is a morphism (and likewise CDom denotes B
here). Following DiPaola and Heller [9], we write dom(f) for the morphism
πA,B
1
◦(idA ×f)◦ΔA, where π1 is the ﬁrst projection of the product X ×Y . One
can interpret dom(f) : A →A as the partial identity on that part of A where
the partial map f is actually deﬁned. If dom(f) = idCDom(f), we call f total.
Additionally we shall assume that the categories underlying our p-categories
have coproducts, and that the functor × distributes over the coproducts.
We already mentioned the fundamental partial order ⪯. As it is compati-
ble with the composition of multi-valued functions, as well as with the carte-
sian product and the coproduct, we introduce the notion of a poset enriched
p-category for such structures. If also binary inﬁma exists, and are compatible
with composition, cartesian products and coproducts, we have a meet-semilattice
enriched p-category. These concepts come with a natural concept of a substruc-
ture, which we shall use.
Moreover, we need two minor properties: A sub poset enriched p-category
is called wide, if it contains all objects of the superstructure. A poset-enriched
p-category is totally connected, if for any two objects A, B there is a morphisms
cA,B : A →B. With these notions available, we can now provide the setting we
need to introduce many-one reductions:
Deﬁnition 2. A many-one category extension (moce) shall be a meet-semilattice
enriched p-category P with coproducts, together with a wide and totally connected
sub-poset enriched p-category S, where × distributes over the coproducts.
The intuition behind the preceding deﬁnition is that P is the category of prob-
lems one wishes to structure by reductions, whereas S is the subcategory of
simple multi-valued functions that serve as reduction witnesses. Typical choices
for S would be computable or polynomial-time computable functions (or multi-
valued functions).

Multi-valued Functions in Computability Theory
575
4
The Lattice of Many-One Degrees
There are two deﬁnitions of many-one reductions commonly found in the lit-
erature on (multi-valued) functions, which diﬀer in the question whether the
post-processing of the oracle answer still has access to the input. Forgetting the
input leads to a simpler deﬁnition, and may make proofs of non-reducibility eas-
ier, while retaining it yields the nicer degree structure and allows to formulate
stronger and more meaningful separation statements. We shall speak of strong
many-one reductions if the original input is forgotten, and of many-one reduc-
tions otherwise.
Throughout this subsection, we assume that some moce (P, S, ×, ⪯) is given,
and refrain from mentioning it explicitly where this is unnecessary.
Deﬁnition 3 (Strong many-one reductions). Let f ≤sm g hold for f, g ∈P,
if there are H, K ∈S with f ⪯H ◦g ◦K.
Deﬁnition 4 (Many-one reductions). Let f ≤m g hold for f, g ∈P, if there
are H, K ∈S with f ⪯H ◦(idDom(f) × (g ◦K)) ◦ΔDom(f).
Proposition 1. Both ≤sm and ≤m deﬁne preorders on P. For any, f, g ∈P,
f ≤sm g implies f ≤m g.
By D we shall denote the partially ordered class of ≤m-equivalence classes in
P. Both the coproduct + and the cartesian product × in P can be lifted to
operations on D, which we shall denote by +, × again. We need a third operation
to be lifted from P to D. The coproduct injections shall be ιX,Y
1
: X →X + Y
and ιX,Y
2
: Y →X + Y , and we denote the inﬁmum regarding ⪯via ∧. Now for
f : X →Y , g : A →B deﬁne f ⊕g : X × A →Y + B via:
f ⊕g = [(ιY,B
1
◦πY,B
1
) ∧(ιY,B
2
◦πY,B
2
)] ◦(f × g)
Informally, f ⊕g receives a problem instance to each of f and g, and has to
produce a solution to one of them. Unlike +, ×, it is clear that ⊕lacks a coun-
terpart for functions. Its degree-theoretic relevance follows from the following
main result:
Theorem 1. D is a distributive lattice, with ⊕as inﬁmum and + as supremum.
The presence of certain distinguished objects in P translates into the existence
of special degrees in D. As usual, we call an object I ∈Ob(P) initial, iﬀfor
any object A ∈Ob(P) there is exactly one morphism f : I →A. The concept is
extended to domains in p-categories by calling dom i initial, iﬀfor any A ∈Ob(P)
there is exactly one morphism f with CDom(f) = A and f = f ◦dom i.
Our notion of emptiness for objects in p-categories does not amount to empti-
ness in the underlying category. Instead, we call E ∈Ob(P) empty, iﬀfor any
total morphism g : A →E we ﬁnd A to be initial. Likewise, a domain dom e
is empty, iﬀfor any total morphism g with (dom e) ◦g = g, we ﬁnd Dom(g) to
be initial. Note that empty implies initial. Objects are initial (empty), iﬀthe
corresponding identity is initial (empty) as a domain.

576
A. Pauly
An object F ∈Ob(P) is called ﬁnal, iﬀfor any object A ∈Ob(P) there is
exactly one total morphism g : A →F. Likewise, a domain dom f is ﬁnal, iﬀfor
any A ∈Ob(P) there is exactly one total morphism g with Dom(g) = A and
dom f ◦g = g. Objects are initial (empty, ﬁnal), iﬀthe corresponding identity is
initial (empty, ﬁnal) as a domain.
Proposition 2. Let dom i be initial in both S and P. Then its degree (denoted
by 0) is the bottom element in D.
In particular, this shows that all initial domains are equivalent w.r.t. ≤m. The
same holds for ﬁnal domains, whose degree (if present) we denote by 1. Then we
ﬁnd:
Theorem 2. If P and S share an empty domain and a ﬁnal domain, then D
with the operations ×, + and the constants 0, 1 is an idempotent commutative
semiring, i.e., the following equations hold for all a, b, c ∈D:
1. a + a = a, a + b = b + a, a + (b + c) = (a + b) + c
2. a × b = b × a, a × (b × c) = (a × b) × c
3. a × (b + c) = (a × b) + (a × c)
4. 0 + a = a, 0 × a = 0, 1 × a = a
We remark that in a similar fashion, an operation ∗can be introduced (albeit
requiring slightly more assumption) turning D into a Kleene-algebra. As a con-
sequence, a deﬁnition of wtt-reductions can be derived from our deﬁnition of
many-one reductions.
5
Some Examples
In this section, we shall exhibit two basic examples for our framework, namely
the adapted versions of Karp and Cook reducibilities to multi-valued functions.
Another prime example is Weihrauch reducibility. The structural investigation
of Weihrauch degrees served as inspiration for the present work, and we refer to
the original literature for the details [4,16,2,11].
These examples do not exhaust the range of applicability, though: Medvedev-
reducibility and many-one reductions between parameterized search problems
are omitted due to limited space; resource-bounded variants of Weihrauch re-
ducibility also satisfy the requirements. Beyond computability, also continuity
may be used as the decisive property of reduction witnesses.
5.1
Computable Many-One Reductions
Here we consider the category of multi-valued functions from {0, 1}∗to {0, 1}∗in
the rˆole of P, while the category of reduction witnesses S is given by the category
C1 of restrictions of partial computable functions. These categories satisfy our
conditions, with f +g, f ×g deﬁned via (f +g)(0x) = 0f(x), (f +g)(1x) = 1g(x),
(f × g)(⟨x, y⟩) = ⟨f(x), g(x)⟩.

Multi-valued Functions in Computability Theory
577
Deﬁnition 5 (special case of Deﬁnition 4). For two multi-valued functions
f, g :⊆{0, 1}∗⇒{0, 1}∗, deﬁne f ≤m g, if there are computable functions
H, K :⊆{0, 1}∗→{0, 1}∗with H⟨x, y⟩∈f(x) whenever y ∈g(K(x)).
We use C1 to denote the set of degrees in this setting.
Corollary 1 (of Theorem 1). (C1, ⊕, +) is a distributive lattice.
In C1, there exists both an empty domain and ﬁnal domains, namely the no-
where deﬁned multi-valued function ∅⊂{0, 1}∗× {0, 1}∗and any {(x, x)} ⊆
{0, 1}∗× {0, 1}∗. The corresponding degrees shall be denoted by 0, 1 ∈C1.
Proposition 3. 1 is the least element in C1 \ {0} and contains exactly those
multi-valued functions admitting a computable choice function.
We do point out that decision problems cannot be considered as a special case of
multi-valued functions in the straight-forward way, as our deﬁnition of many-one
reductions allows modiﬁcations of the output; in particular, the characteristic
function of a set is trivially equivalent to the characteristic function of its com-
plement. However, many results proven for many-one reductions between search
problems hold—with identical proofs—also for Turing reductions with the num-
ber of oracle queries limited to 1, which corresponds to the notion employed
here.
For example, Yates’ result [22] regarding the existence of minimal pairs ap-
plies here as follows:
Proposition 4 (Yates [22]). There are a, b ∈C1 \ {0, 1} with total represen-
tatives such that for any c ≤m (a ⊕b) that has a representative f ∈c of the
type f : {0, 1}∗→{0, 1}, we ﬁnd c = 1.
However, the cumbersome restriction to degrees admitting a function represen-
tative is necessary, as minimal pairs for multi-valued functions do not exist in
the computable case:
Proposition 5. If a, b ∈C1 have total representatives, then a ⊕b = 1 implies
a = 1 or b = 1.
The proof of the preceding proposition is based on the following technical lemma:
Lemma 1. There are Turing functionals Ψ, Φ, such that for all total multi-
valued functions f, g : {0, 1}∗⇒{0, 1}∗and for any choice function I of (f ⊕g),
either Ψ I is a choice function of f or ΦI is a choice function of g.
5.2
Polynomial-Time Many Reductions
Proceeding as above, but taking the category of polynomial-time computable
functions as the category S of reduction witnesses, we again obtain a degree
structure w.r.t. many-one reductions in the generic way, which we shall denote
by P1, and the reducibility by ≤p
m.

578
A. Pauly
Corollary 2 (of Theorem 1). (P1, ⊕, +) is a distributive lattice.
Again, 0 and 1 exist and are the bottom and second-least element respectively.
0 ∈P1 contains only the no-where deﬁned multi-valued function, whereas 1
contains exactly those multi-valued functions with non-empty domain admitting
a polynomial-time computable choice function.
Again, some results for functions or decision problems can be transferred. As
a demonstration, we extend Ladner’s density result [13, Theorem 2] to multi-
valued functions. For this, note that two notions coinciding for single-valued
functions diﬀer for multi-valued functions, namely the existence of a computable
choice function and the decidability of the graph. We call those multi-valued
functions satisfying the former condition computable. The latter condition has
the disadvantage of not being preserved downwards by many-one reductions.
However, a decidable graph is the condition needed for the following theorem.
Its proof closely resembles the one of [13, Theorem 2].
Theorem 3. Let a, b ∈P1 admit representatives with decidable graphs and
satisfy b ≰p
m a. Then there are b0, b1 ∈P1 with b = b0 + b1, bi ≰p
m a and
b ≰p
m a + bi for both i ∈{0, 1}.
Corollary 3. The degrees in P1 admitting decidable graphs are dense (in them-
selves).
A question that has received a lot of attention regarding (polynomial-time)
many-one reductions between decision problems is about the existence and na-
ture of minimal pairs. In terms of lattice theory, this asks whether the degree 1 is
meet-irreducible, and if not, what kind of pairs can satisfy a ⊕b = 1. Following
the initial result by Ladner that minimal pairs for polynomial-time many-one
reductions between decision problems exist [13], Ambos-Spies could prove that
every computable super-polynomial degree is part of a minimal pair [1].
For search problems, however, the question remains open:
Question 1. Is 1 ∈P1 meet-irreducible?
The techniques used to construct a minimal pair in [13,1] diagonalize against
pairs of reductions Re, Rf trying to prevent Re(a) = Rf(b) for the constructed
representatives a, b. If the equality cannot be prevented, then one can prove that
the resulting set is already polynomial-time decidable using a constant preﬁx
of b, hence, polynomial-time decidable. However, for search problems any pair
of reductions to a pair of search problems produces a search problem, namely
Re(a) ∪Rf(b).
A non-computable minimal pair for Type-2 search problems was constructed
in [11] by Higuchi and Pauly. There, the crucial part is the identiﬁability of
hard and easy instances, which is not available in a Type-1 setting. The negative
answer we obtained for computable many-one reductions in Subsection 5.1 relied
on Lemma 1, which again cannot be transferred to the time-bounded case: There
are polynomial-time decidable relations R such that neither R nor its inverse ¬R†
admit a polynomial-time choice function, even if P = NP should hold.

Multi-valued Functions in Computability Theory
579
6
Outlook
Hopefully we have made the case that investigating the degree structures of
many-one reductions between multi-valued functions is both intrinsically and
extrinsically interesting. The basic results follow from our generic results in Sec-
tion 4, but beyond that the various kinds require speciﬁc attention. To some
extent proof concepts can be extended from the traditional setting of many-one
reductions between functions, but beyond that novel techniques are called for.
Acknowledgements. This paper is based on the second chapter of the authors
thesis [18]. An extended version including proofs is available at the arXiv, see
[17].
References
1. Ambos-Spies, K.: Minimal pairs for polynomial time reducibilities. In: B¨orger, E.
(ed.) Computation Theory and Logic. LNCS, vol. 270, pp. 1–13. Springer, Heidel-
berg (1987)
2. Brattka, V., de Brecht, M., Pauly, A.: Closed choice and a uniform low basis the-
orem. Annals of Pure and Applied Logic (2012)
3. Brattka, V., Gherardi, G.: Eﬀective choice and boundedness principles in com-
putable analysis. Journal of Symbolic Logic 76, 143–176 (2011) arXiv:0905.4685
4. Brattka, V., Gherardi, G.: Weihrauch degrees, omniscience principles and weak
computability. Bulletin of Symbolic Logic 17, 73–117 (2011) arXiv:0905.4679
5. Brattka, V., Gherardi, G., Marcone, A.: The Bolzano-Weierstrass Theorem is the
jump of Weak K¨onig’s Lemma. Annals of Pure and Applied Logic 163(6), 623–655
(2012)
6. Brattka, V., le Roux, S., Pauly, A.: On the Computational Content of the Brouwer
Fixed Point Theorem. In: Cooper, S.B., Dawar, A., L¨owe, B. (eds.) CiE 2012.
LNCS, vol. 7318, pp. 57–68. Springer, Heidelberg (2012)
7. Chen, X., Deng, X.: Settling the complexity of 2-player Nash-equilibrium. Tech.
Rep. 134, Electronic Colloquium on Computational Complexity (2005)
8. Daskalakis, C., Papadimitriou, C.: Continuous local search. In: Proceedings of
SODA (2011)
9. Di Paola, R., Heller, A.: Dominical categories: Recursion theory without elements.
Journal of Symbolic Logic 52, 594–635 (1987)
10. Gherardi, G., Marcone, A.: How incomputable is the separable Hahn-Banach the-
orem? Notre Dame Journal of Formal Logic 50(4), 393–425 (2009)
11. Higuchi, K., Pauly, A.: The degree-structure of Weihrauch-reducibility. arXiv
1101.0112 (2011)
12. Johnson, D.S., Papadimtriou, C.H., Yannakakis, M.: How easy is local search?
Journal of Computer and System Sciences 37(1), 79–100 (1988)
13. Ladner, R.E.: On the structure of polynomial time reducibility. Journal of the
ACM 22(1), 155–171 (1975)
14. Papadimitriou, C.H.: On the complexity of the parity argument and other ineﬃ-
cient proofs of existence. Journal of Computer and Systems Science 48(3), 498–532
(1994)

580
A. Pauly
15. Pauly, A.: How incomputable is ﬁnding Nash equilibria? Journal of Universal Com-
puter Science 16(18), 2686–2710 (2010)
16. Pauly, A.: On the (semi)lattices induced by continuous reducibilities. Mathematical
Logic Quarterly 56(5), 488–502 (2010)
17. Pauly, A.: Many-one reductions between search problems. arXiv 1102.3151 (2011)
18. Pauly, A.: Computable Metamathematics and its Application to Game Theory.
Ph.D. thesis, University of Cambridge (2012)
19. Robinson, E., Rosolini, G.: Categories of partial maps. Information and Computa-
tion 79(2), 95–130 (1988)
20. Weihrauch, K.: The degrees of discontinuity of some translators between repre-
sentations of the real numbers. Informatik Berichte 129, FernUniversit¨at Hagen,
Hagen (July 1992)
21. Weihrauch, K.: The TTE-interpretation of three hierarchies of omniscience princi-
ples. Informatik Berichte 130, FernUniversit¨at Hagen, Hagen (September 1992)
22. Yates, C.E.M.: A minimal pair of recursively enumerable degrees. The Journal of
Symbolic Logic 31(2), 159–168 (1966)

Relative Randomness
for Martin-L¨of Random Sets
NingNing Peng, Kojiro Higuchi, Takeshi Yamazaki, and Kazuyuki Tanaka
Mathematical Institute, Tohoku University, Sendai 980-8578, Japan
{sa8m42,sa7m24,yamazaki,tanaka}@math.tohoku.ac.jp
Abstract. Let Γ be a set of functions on the natural numbers. We
introduce a new randomness notion called semi Γ-randomness, which is
associated with a Γ-indexed test. Fix a computable sequence {Gn}n∈ω
of all c.e. open sets. For any f ∈Γ, {Gf(n)}n∈ω is called a Γ-indexed test
if μ(Gf(n)) ≤2−n for all n. We prove that weak n-randomness is strictly
stronger than semi Δ0
n-randomness, for n > 2. Moreover, we investigate
the relationships between various deﬁnitions of randomness.
1
Introduction
The present paper is concerned with the algorithmic notion of randomness such
as the one originally introduced by P. Martin-L¨of [6] in 1966. A Martin-L¨of test,
or ML-test for short, is a uniformly c.e. sequence (Gm)m∈ω of open sets such
that μ(Gm) ≤2−m for all m ∈ω. A set Z ⊆ω fails the test if Z ∈
m Gm,
otherwise Z passes the test. A set Z is Martin-L¨of random if Z passes each
ML-test.
There are many diﬀerent approaches to randomness, most of which deﬁne
a notion stronger than Martin-L¨of randomness. One approach is to generalize
the Martin-L¨of test by specifying the m-th component (a c.e. set of measure
at most 2−m) via a function in some function class Γ. For example, the notion
of Demuth randomness, introduced and studied by Demuth [1,2], is deﬁned in
this way. Thus, a Demuth test is a sequence {Gn}n∈ω of c.e open sets such that
Gn = Wf(n) for all n ∈ω and μ(Gn) ≤2−n where f is ω-c.e..
A main purpose of this paper is to give a general framework for such random-
ness notions. To do this, we introduce the notion of semi Γ-randomness. Note
that our deﬁnition of passing a test is diﬀerent from Demuth randomness, where
a set Z fails a Demuth test if Z is in inﬁnitely many Gm’s, but fails a Γ-indexed
test if it is in all of the Gm, see Section 3.
Γ-randomness was ﬁrst studied by the ﬁrst author [10], and is strongly con-
nected with Yu’s research [11]. The Γ-randomness notion could sometimes
produce alternative proofs of existing results. For instance, some properties of
∅′-Schnorr randomness are proved more easily by the characterization due to L-
randomness than the usual methods. In Section 2, we prove a new result about
L-randomness. Another motivation of studying semi Γ-randomness is to con-
sider weak versions of Γ-randomness. Last of all, the main results of this paper
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 581–588, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

582
N. Peng et al.
appear in section 3. We show that for n > 2, weak n-randomness is strictly
stronger than semi Δ0
n-randomness from a general property of semi Γ- random-
ness. Moreover, we prove that weak 2-randomness, which is clearly equivalent to
semi Δ0
2-randomness, is strictly stronger than semi L-randomness.
2
Preliminaries
The collection of binary strings is regarded as the set 2<ω of all functions from
{0, . . ., n −1} to {0, 1} for some n ∈ω. We use σ, τ, · · · to denote the elements
of 2<ω. Let 2ω denote the set of inﬁnite binary sequences. Subsets of ω can be
identiﬁed with elements of 2ω.
For σ ∈2<ω, we write lh(σ) for the length of σ. The empty string is denoted
by λ. For strings σ and τ, let σ ⪯τ denote that σ is a preﬁx string of τ,
i.e., dom(σ) ⊆dom(τ) and σ(m) = τ(m) holds for each m ∈dom(σ). The
concatenation of two strings σ and τ is denoted by στ. For a set A, A ↾n is
the preﬁx of A of length n. A topology on 2ω is induced by the basic open sets
[σ] = {X ∈2ω : X ⪰σ} for all strings σ ∈2<ω. So each open set of 2ω is
generated by a subset of 2<ω, that is [S]≺= {X ∈2ω : ∃σ ∈S (σ ⪯X)}. With
this topology, 2ω is called Cantor space.
The Lebesgue measure μ on 2ω is induced by letting the measure be μ([σ]) :=
2−|σ| for each basic open set [σ]. If a class G ⊆2ω is open then μ(G) =

σ∈B 2−|σ| where B is a preﬁx-free set of strings such that G = 
σ∈B[σ].
A class C ⊆2ω is called null if μ(C) = 0. If 2ω −C is null, we say that C is conull.
Martin-L¨of randomness is a central notion of algorithmic randomness for sub-
sets of ω, which deﬁned in the following way.
Deﬁnition 1 (Martin-L¨of [6])
(i) A Martin-L¨of test, or ML-test for short, is a uniformly c.e. sequence
{Gm}m∈ω of open sets such that μ(Gm) ≤2−m for all m ∈ω.
(ii) A set Z ⊆ω fails the test if Z ∈
m Gm, otherwise Z passes the test.
(iii) Z is Martin-L¨of random, or ML-random, if Z passes each ML-test.
In this way, we are presenting the Martin-L¨of random sets as the sets that pass
all reasonable statistical tests in the form of eﬀectively presented null sets. The
randomness notions which are stronger than Martin-L¨of randomness have been
studied. 2-randomness is Martin-L¨of randomness relative to ∅′. It was ﬁrst stud-
ied by Kurtz [4] in 1981. He also considered weak 2-randomness, an interesting
notion lying strictly between Martin-L¨of randomness and 2-randomness. A set
is weakly random (or Kurtz random) if it is not a member of any null Π0
1 class.
Similarly, it is weakly 2-random if it is not a member of any null Π0
2 class.
Deﬁnition 2 (Kurtz [4])
(i) A generalized ML-test is a uniformly c.e. sequence {Gm}m∈ω of open sets
such that μ(
m Gm) = 0.
(ii) Z is weakly 2-random if it passes every generalized ML-test.

Relative Randomness for Martin-L¨of Random Sets
583
Fact 1. (i) 2-randomness ⇒weak 2-randomness ⇒Martin-L¨of randomness.
(ii) The reverse implication fails (Kurtz, Kautz).
We recall some notions from [10]. Let ωω be the set of all functions from ω to ω.
Deﬁnition 3. Let Γ ⊂ωω. A set Z is Γ-random if Z is ML-random relative to
f for all f ∈Γ. Any ML-test relative to f ∈Γ is called a Γ-test.
For f ∈ωω, we say f-random and f-test instead of {f}-random and {f}-test,
respectively.
Theorem 1 (Miller/Yu [7]). Let X, Y, Z ∈2ω. If X is ML-random, Y is
Z-random and X ≤T Y , then X is Z-random.
Proof. See [7, Theorem 4.3].
Corollary 1. Let X be ML-random and Γ ⊂ωω. If X ≤T Y and Y is Γ-
random, then X is Γ-random.
Let L denote the set of all low functions, where a function is low if its Turing jump
is Turing reducible to the Turing jump of a computable function. We investigate
some properties of L-randomness. Note that there exists a low ML-random set.
Clearly, any low set is not L-random. Thus, L-randomness is strictly stronger
than ML-randomness. In fact, it turned out that L-randomness is equivalent to
∅′-Schnorr randomness.
Theorem 2 (Yu [11]). L-randomness is equivalent to ∅′-Schnorr randomness.
Proof. See [11, Theorem 4.1].
L-randomness can be also given by subsets of L. Let G denote the set of all
1-generic functions, where a function f is called a 1-generic if for any c.e. set
W ⊂ω<ω, there exists σ ≺f such that σ ∈W or σ is incomparable with any
element in W. Modifying the proof of Theorem 2, one can show that L ∩G-
randomness is equivalent to ∅′-Schnorr randomness.
We would like to introduce another characterization of L-randomness. Then,
the next lemma is useful. Let f, g ∈ωω. We say that f is LR-reducible to g,
abbreviated f ≤LR g, if g-randomness implies f-randomness.
Lemma 1. Let Γ, Γ ′ ⊂ωω such that for any f ∈Γ there is a function g ∈Γ ∩Γ ′
with f ≤LR g. Then, Γ-randomness is equivalent to Γ ∩Γ ′-randomness.
Proof. It suﬃces to show that every set which is not Γ-random is not Γ ∩Γ ′-
random. Fix A ∈2ω such that A is not Γ-random. There exists f ∈Γ such
that A is not f-random. By the assumption, there exists g ∈Γ ∩Γ ′ such that
f ≤LR g. By the deﬁnition of LR-reducibility, A is not g-random. Thus A is not
Γ ∩Γ ′-random.
Let PA denote the set of all functions of PA degrees.

584
N. Peng et al.
Proposition 1. L-randomness is equivalent to L ∩PA-randomness.
Proof. This is obtained from lemma 1, using the fact that for any low f, there
exists a low g such that f ≤T g and g is of PA degree relative to f by Low Basis
Theorem.
Let MLR denote the set of all ML-random sets.
Conjecture 1. L-randomness is equivalent to L ∩MLR-randomness.
The proof of Proposition 1 does not apply to this Conjecture since any c.e. set
which is not low for random cannot be computed by a low random.
3
Semi Γ -Randomness
In this section, we investigate a new randomness notion weaker than Γ-random.
We concentrate on an index function for the componets of a ML-test.
Deﬁnition 4. Let Γ ⊆ωω.
(i) We say that a sequence {Gn}n∈ω of c.e open sets is a Γ-indexed test if
there exists f ∈Γ such that Gn = Wf(n) for all n ∈ω and μ(Gn) ≤2−n.
(ii) A set fails the test if it is in 
n Gn, otherwise the set passes the test.
(iii) A set is semi Γ-random if it passes every Γ-indexed test.
Note that, there is no semi ωω-random set. It is straightforward from the deﬁ-
nition that Martin-L¨of randomness is equivalent to semi Δ0
1-randomness. In [5],
independently with us, Kuˇcera and Nies deﬁne a semi Δ0
2-random set neverthe-
less they call it limit randomness.
Theorem 3. A set is semi Δ0
2-random if and only if it is weakly 2-random.
Proof. Let {Wf(n)}n∈ω be such that μ(
n∈ω Wf(n)) = 0, where f is computable.
Since μ(
m<l Wf(m)) ≤2−n is Π0
1, there is a function g ≤T ∅′ such that
μ(
m<g(n) Wf(m)) ≤2−n for all n ∈ω. Also, there is a function h ≤T ∅′ such
that Wh(n) = 
m<g(n) Wf(m) for any n. So, for any generalized ML-test {Wf(n)},
there exists a semi Δ0
2-test {Wh(n)} that passes the same subset of 2ω.
Let {Wf(n)}n∈ω be a Δ0
2-indexed test with f ≤T ∅′. Then Z ∈
n∈ω Wf(n) is
Π0
2. Therefore there exists uniformly c.e. sequences {Un}n∈ω such that 
n∈ω Un =

n∈ω Wf(n). Since μ(
n∈ω Wf(n)) = 0, {Un}n∈ω is a generalized ML-test se-
quence which passes the same subset of 2ω.
Proposition 2. Every L-random set is semi L-random.
Proof. This follows from a simple fact that every semi L-test is a L-test.
Deﬁnition 5. For f, g ∈ωω, g is said to be a tail of f if there exists a string σ
such that σg = f.

Relative Randomness for Martin-L¨of Random Sets
585
Theorem 4. If Γ ⊂ωω is closed under ≡T , then there is no universal Γ-indexed
test unless ML-randomness is equivalent to semi Γ-randomness.
Proof. Let Γ be a subset of ωω closed under ≡T . Choose a ML-random set X
which is not semi Γ-random. Note that no tail of X is semi Γ-random by the
assumption on Γ. Now, ﬁx a Γ-indexed test {Wg(i)}i∈ω. Since the complement
of Wg(1) in 2ω has at least measure 1/2, there exists a tail Y of X such that
Y ̸∈Wg(1). Thus, {Wg(i)}i∈ω is not a universal Γ-indexed test since Y is not
semi Γ random and Y ̸∈
i∈ω Wg(i).
The following lemma is used for separating between weak randomness and semi
Γ-randomness.
Lemma 2. For any Γ, Γ ′ ⊂ωω such that Γ-randomness is not equivalent to
ML-randomness and Γ ′ is countable, there exists a semi Γ ′-random set which is
not Γ-random.
Proof. Choose a ML-random set X ∈
i∈ω Vi, where {Vi}i∈ω is a universal
f-test for some f ∈Γ. Let {{Ugi(j)}j∈ω}i∈ω be a sequence of all Γ ′-indexed
tests. We construct a function h and a ⊂-increasing sequence {σi}i∈ω such that
[σi] ⊂Vi and (limj→∞σj) ̸∈Ugi(h(i)) for any i ∈ω.
We shall describe the construction of h and {σi}i∈ω at stage s. Let σ =

i<s σi. As the inductive hypothesis, we may suppose that [σ]\
i<s Ugi(h(i)) has
positive measure. Deﬁne h(s) by the least number x such that [σ]\(
i<s Ugi(h(i))∪
Ugs(x)) has positive measure. Choose a tail Y of X such that
σY ∈[σ] \

i≤s
Ugi(h(i))
Note that σY ∈
i∈ω Vi since X ∈
i∈ω Vi. Thus, we can choose σs ⊋σ such
that σs ⊂σY and [σs] ⊂Vs. This completes the construction at stage s.
It is clear that h and {σi}i∈ω have the desired properties. Hence limi→∞σi is
a semi Γ ′-random set which is not Γ-random.
Theorem 5. If n > 2, then weak n-randomness is strictly stronger than semi
Δ0
n-randomness.
Proof. By Lemma 2, there exists a semi Δ0
n-random set which is not n −1-
random. Since weak n-randomness implies n −1-randomness and semi Δ0
n-
randomness, weak n-randomness is strictly stronger than semi Δ0
n-randomness.
Finally, we consider the case n = 2.
Theorem 6. Let Γ be the set of all functions of Turing degrees below degT(∅′).
There exists a Π0
1(∅′) null set P containing a semi Γ-random set.
Proof. Let f be a ∅′-computable increasing function such that f(0) > 0 and
{x}(x) ↓implies {x}f(x)(x) ↓for all x ∈ω. Note that any function which

586
N. Peng et al.
dominates f computes ∅′. Thus no element of Γ dominates f. Deﬁne F ⊂ω<ω,
H : ω →ω and M : ω →ω by
F = {σ ∈ω<ω | (∀i < lh(σ))[σ(i) < f(i)]},
H(x) = (least y)[f(x) · 2−y ≤2−1]
and M(0) = 0 and
M(x + 1) = M(x) + H(x) + 1
for all x ∈ω. Let Pﬁn(ω) be the set of all ﬁnite subsets of ω. Deﬁne O : F →
Pﬁn(ω) by O(λ) = ∅and
O(σi) =

O(σ) ∪{i}
if μ(Wi) ≤2−M(lh(σ))−1,
O(σ)
otherwise.
Deﬁne T : F →2<ω by recursively as follows: let T (λ) = λ; and let T (σi) be a
ﬁnite binary string τ of length (lh(T (σ)) + H(lh(σ))) such that τ ⪰T (σ) and
μ([τ] \

j∈O(σi)
Wj) ≥2−M(lh(σi)).
We show that T is total by induction on σ ∈F. Clearly, T (λ) is deﬁned and
μ([T (λ)] \ 
j∈O(λ) Wj) ≥2−M(lh(λ)) holds. Let σi ∈F. Suppose that μ([T (σ)] \

j∈O(σ) Wj) ≥2−M(lh(σ)) holds as an inductive hypothesis. We only consider
the case that μ(Wi) ≤2−M(lh(σ))−1. In this case we have that O(σi) = O(σ)∪{i}
and, therefore,
μ([T (σ)] \

j∈O(σi)
Wj) ≥2−M(lh(σ))−1.
Since M(lh(σi)) = M(lh(σ)) + H(lh(σ)) + 1 holds by the deﬁnition of M, T (σi)
is deﬁned and satisﬁes that μ([T (σi)] \ 
j∈O(σi) Wj) ≥2−M(lh(σi)).
Let P = {X ∈2ω | (∀x)(∃σ ∈F ∩ωx)[T (σ) ⊂X]}. It is easy to see that P is
Π0
1(∅′) since f, F, H, M, O and T are ∅′-computable.
We show that P is null. Every σ ∈F has exactly f(lh(σ))-many immediate
successors in F. Thus, by the deﬁnition of H and T , we have
μ([{T (σ) | σ ∈F ∩ωx+1}]≺) ≤2−1 · μ([{T (σ) | σ ∈F ∩ωx+1}]≺)
for all x ∈ω. Notice that T (σ) has length 
k<lh(σ) H(k) by the deﬁnition of
T , and {T (σ) | σ ∈F ∩ωx} is the set of all extendible elements of P of length

k<x H(k) for all x ∈ω. Hence P is null.
Next, we show that P has a semi Γ-random set X. For a path p through
F, we deﬁne T (p) = 
x∈ω T (p ↾x) and O(p) = 
x∈ω O(p ↾x). Note that
T (p) ̸∈
i∈O(p) Wi by the deﬁnition of T . Let {{Wgi(j)}j∈ω}i∈ω be a sequence of
all Γ-indexed tests. By Padding lemma, we can safely assume that gi is strictly
increasing.

Relative Randomness for Martin-L¨of Random Sets
587
We shall construct a strict ⊂-increasing sequence {σi}i∈ω of elements of F
such that for any i ∈ω, there exists j ∈ω with gi(j) ∈O(σi+1). Let σ0 = λ. Fix
i ∈ω. Suppose that for any i′ < i, there exists j ∈ω with gi′(j) ∈O(σi). We
want to ﬁnd σi+1 ⊃σi in F such that gi(j) ∈O(σi+1) for some j ∈ω.
To ﬁnd such σi and j, we ﬁrst construct gi-computable function q : ω →
ω recursively. q(0) = max{f(lh(σi)), gi(M(lh(σ)) + 1)}. Suppose that q(x) is
deﬁned. Let y0 = (least y)[q(x) · 2−y ≤2−1] and let y1 = q(x) + y0 + 1. Deﬁne
q(x + 1) = gi(y1 + 1). Since q <T ∅′, we can choose the least natural number
x ∈ω such that q(x + 1) < f(lh(σi) + x + 1). Let σi+1 = σi0xq(x + 1). By the
choice of x, one can easily prove that
(∀y ≤x)[f(lh(σi) + y), M(lh(σi) + y) ≤q(y)]
by induction on y. Choose y1 ∈ω such that q(x + 1) = gi(y1 + 1). Then
μ(Wgi(y1+1)) = μ(Wgi(y1+1)) ≤2−gi(y1+1) and gi(y1 +1) = q(x+1) < f(lh(σi)+
x + 1) hold. Thus, we have M(lh(σi) + x) ≤q(x) < y1 < gi(y1 + 1) = q(x + 1)
by the deﬁnition of q(x+1). Hence μ(Wq(x+1)) ≤2−M(lh(σi)+x) holds. Therefore
gi(y1 + 1) = q(x + 1) ∈O(σi+1).
Now it is clear that p = 
i∈ω T (σi) is an element of P such that p is not
Γ-random.
Corollary 2. Weak 2-randomness is strictly stronger than semi L-randomness.
Proof. By Theorem 3, we know that weak 2-randomness is stronger than semi
L-randomness. By Theorem 6, there exists a semi L-random set which is not
∅′-Kurtz random. This implies that weak 2-randomness is strictly stronger than
semi L-randomness since any weak 2-randomness is ∅′-Kurtz random.
Proposition 3. 2-randomness does not imply semi Δ0
3-randomness.
Proof. Since there is a Δ0
3 2-random set, we show that there is no Δ0
3 semi Δ0
3-
random set. Let X be a Δ0
3 set, then there is a Δ0
3 function f such that Wf(n) =
{X ↾n}. Note that Wf(n) is a set containing only X ↾n and μ(Wf(n)) ≤2−n.
So, {Wf(n)}n∈ω is a semi Δ0
3-test. But A ∈
n∈ω Wf(n). Hence, A is not semi
Δ0
3-random.
4
Conclusions and Future Research
In this paper, we introduced semi Γ-randomness for a set of number functions. We
showed that weak n-randomness is strictly stronger than semi Δ0
n-randomness, if
n > 2, although weak 2-randomness is equivalent to semi Δ0
2-randomness. On the
other hand, we showed that weak 2-randomness is strictly stronger than semi L-
randomness where L is the set of low sets. In the future literature, we shall investi-
gate a characterization of semi L-randomness in terms of Kolmogorov complexity
or martingales.

588
N. Peng et al.
Acknowledgments. The ﬁrst author was partially supported by a Global COE
Program and scientiﬁc research plan project of the education department of
Shaanxi province (Grant: 2010JK697). The second author was partially sup-
ported by JSPS Research Fellowship for Young Scientists. This research was
partially supported by KAKENHI 23340020.
We would like to thank Takayuki Kihara, Masahiro Kumabe, Steve Simp-
son and ChenGuang Liu for their valuable comments and discussions. We also
express our heartfelt thanks to the anonymous reviewers for their constructive
comments.
References
1. Demuth, O.: Some classes of arithmetical real numbers. Commentations Mathe-
maticae Universitatis Carolinae 23(3), 453–465 (1982) (Russian)
2. Demuth, O.: Remarks on the structure of tt-degrees based on constructive mea-
sure theory. Commentations Mathematicae Universitatis Carolinae 29(2), 233–247
(1988)
3. Downey, R.G., Hirshfeldt, D.R.: Hirshfeldt: Algorithmic Randomness and Com-
plexity. Springer, Berlin (2010)
4. Kurtz, S.A.: Randomness and genericity in the degrees of unsolvability. Ph.D.
Dissertation, University of Illinois, Urbana (1981)
5. Kuˇcera, A., Nies, A.: Demuth randomness and computational complexity. Annals
of Pure and Applied Logic 162(7), 504–513 (2011)
6. Martin-L¨of, P.: The deﬁnition of random sequences. Information and Control 9(6),
602–619 (1966)
7. Miller, J.S., Yu, L.: On initial segment complexity and degrees of randomness.
Transactions of the American Mathematical Society 360(6), 3193–3210 (2008)
8. Nies, A.: Computability and Randomness. Oxford University Press (2009)
9. Peng, N.N.: Algorithmic randomness and lowness notions. Master Thesis, Mathe-
matical Institute, Tohoku University, Sendai, Japan (2010)
10. Peng, N.N.: The notions between Martin L¨of randomness and 2-randomness. RIMS
Kˆokyˆuroku (1792), 117–122 (2010)
11. Yu, L.: Characterizing strong randomness via Martin-L¨of randomness. Annals of
Pure and Applied Logic 163(3), 214–224 (2012)

On the Tarski-Lindenbaum Algebra of the Class
of all Strongly Constructivizable Prime Models
Mikhail G. Peretyat’kin
Institute of Mathematics, 125 Pushkin Street, 050010 Almaty, Kazakhstan
m.g.peretyatkin@predicate-logic.org
Abstract. We study the class Ps.c of all strongly constructivizable prime
models of a ﬁnite rich signature σ. It is proven that the Tarski-Lindenba-
um algebra L(Ps.c) considered together with a G¨odel numbering γ of
the sentences is a Boolean Π0
4-algebra whose computable ultraﬁlters
form a dense set in the set of all ultraﬁlters; moreover, the numer-
ated Boolean algebra (L(Ps.c), γ) is universal relative to the class of all
Boolean Σ0
3-algebras. This gives an important characterization of the
Tarski-Lindenbaum algebra L(Ps.c) of the semantic class Ps.c.
Theories in ﬁrst-order predicate logic with equality are considered. General con-
cepts of model theory, algorithm theory, Boolean algebras, and constructive
models can be found in Hodges [5], Rogers [9], Goncharov and Ershov [2], and
Goncharov [1].
1
Preliminaries
A ﬁnite signature is called rich, if it contains at least one n-ary predicate or
function symbol for n>1, or two unary function symbols. The following notations
are used: FL(σ) is the set of all formulas of signature σ, SL(σ) is the set of all
sentences (i.e., closed formulas) of signature σ. In the work, we use a ﬁnite rich
signature σ, and consider a ﬁxed G¨odel numbering Φi, i ∈N, of the set SL(σ). A
theory F is called ﬁnitely axiomatizable if it is deﬁned by a ﬁnite set of axioms
and its signature is ﬁnite. Generally, incomplete theories are considered.
Let T be a theory of signature σ. On the set of sentences SL(σ), an equivalence
relation ∼T is deﬁned by the rule Φ ∼T Ψ
⇔
T ⊢(Φ ↔Ψ). The logical
connectives ∨, ∧, and ¬ generate Boolean operations ∪, ∩, and −on the quotient
set SL(σ)/∼T ; One can easily check that, these operations are well-deﬁned on
the ∼T -classes. Thereby, we obtain an algebra of the form
L(T ) =

SL(σ)/∼T ; ∪, ∩, −, 0, 1

,
that, in fact, is a Boolean algebra. It is called the Tarski-Lindenbaum algebra of
the theory T . By L(T ), we denote the Tarski-Lindenbaum algebra L(T ) consid-
ered together with a G¨odel numbering γ; thereby, the concept of a computable
isomorphism is applicable to such objects. For a class of models M, we write
brieﬂy L(M) instead of L

Th(M)

.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 589–598, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

590
M.G. Peretyat’kin
The set of all ﬁnite tuples α of the form α = ⟨α0, α1, . . ., αn⟩, αi ∈{0, 1}, is
denoted by 2<ω. The empty tuple is denoted by ∅. The canonical (G¨odel) index
of a ﬁnite tuple of zeros and ones of the form ε = ⟨ε0, ε1, . . . , εn−1⟩, εi ∈{0, 1},
is the number Nom(ε)
= 2n + ε02n−1 + ε12n−2 + . . . + εn−1 −1. We often
write shortly ⟨ε⟩instead of Nom(ε). We consider Boolean algebras in signature
σBA = {∪, ∩, −, 0, 1}. Besides, we consider two following binary relations, which
are ﬁrst-order deﬁnable in the theory of Boolean algebras by formulas a ⊆b ⇔
(a∩b = a), a ⊇b ⇔(a∩b = b). Let B be a Boolean algebra, and a ∈B. By B[a],
we denote the restriction of the Boolean algebra B on the set of all subelements
of the element a ∈B counting that 1 = a and −x is deﬁned as a∖x in B[a]. If
b is an element of a Boolean algebra and α ∈{0, 1}, then bα means b for α = 1
and −b for α = 0. Similarly, if Φ is a formula and α ∈{0, 1}, then Φα means Φ
for α = 1 and ¬Φ for α = 0. We use notation P(A) for the power-set of A, and
Card(A) for cardinality of the set A.
Deﬁnition of the concept of a binary tree can be found in [7, Sec, 2.1]. In the
work, we use a more specialized term compact binary trees for them. An element
n of a compact binary tree D such that L(n) ̸∈D is called a dead end of the
tree D. The set of all dead-end elements of a tree D is denoted by Dend(D). A
tree is called atomic if above each of its elements, there is at least one dead-end
element. A chain is a set π ⊆N for which two following conditions are satisﬁed:
m ≼n ∧n ∈π ⇒m ∈π, for all m, n ∈N,
m, n ∈π ⇒m ≼n ∨n ≼m, for all m, n ∈N.
If D is a compact binary tree, we denote by Π(D) the set of all maximal chains,
while Πﬁn(D) denotes the set of all maximal ﬁnite chains of the tree D. We also
use the following technical notations: by Pn, we denote the table condition with
the G¨odel number n, n ∈N; A |= Pi means that the table condition is satisﬁed
in the set A, A ⊆N;
Ω(m) = {A ⊆N | (∀i ∈Wm) A |= Pi},
is the parametric Stone space with index m; Dn is the closure of the set Wn up
to a compact binary tree; and DX
n is the closure of the set W X
n up to a compact
binary tree, where Wn denotes the computably enumerable set with c.e index n,
while W X
n denotes the computably enumerable set with c.e. index n relative to
computability with an oracle X ⊆N, cf. [9]. It can be easily checked that the
tree Dn is computably enumerable, while the tree DX
n is computably enumerable
with oracle X; moreover, each c.e. tree is presented in the sequence Dn, n ∈N,
and each c.e. tree in computation with oracle X is presented in the sequence
DX
n , n ∈N. In accordance with [7, Sec.2.3], the number n plays the role of a c.e.
index for the tree Dn; furthermore, n plays the role of a c.e. index for the tree
DX
n in computability with an oracle X.
Finally, mention that, the main statement of the canonical construction of
ﬁnitely axiomatizable theories is found in [7, Theorem 3.1.1.] (the old title
“intermediate construction” is considered as obsolete).

The Tarski-Lindenbaum Algebra of the Class of SC Prime Models
591
2
Main Claim
Hereafter, we ﬁx a ﬁnite rich signature σ. We denote by P(σ) the class of all prime
models of signature σ, and by SC(σ), the class of all strongly constructivizable
models of signature σ. Intersection of these classes Ps.c(σ) = P(σ)∩SC(σ) is the
main object of our further study.
Theorem 1. The following assertions hold:
(a) L(Ps.c(σ)) is a Boolean Π0
4-algebra,
(b) computable ultraﬁlters of L(Ps.c(σ)) represent a dense set among arbitrary
ultraﬁlters in the algebra,
(c) for an arbitrary Boolean Σ0
3-algebra (B, ν) there is a sentence Φ of signature
σ, such that (B, ν) ∼= (L

Th(Mod(Φ) ∩Ps.c(σ))

, γ), where γ is a G¨odel
numbering of the sentences of signature σ.
Proof. By criterion of Goncharov and Harrington [3,4], a prime model N of
a complete decidable theory T is strongly constructivizable if and only if the
family of principal types realized in N is computable. From this we obtain that
a sentence Ψ has a strongly constructivizable prime model if and only if there is a
complete decidable theory T whose principal types are dense in the family of all
types, and the family of its principal types is computable; moreover, Ψ ∈T . An
immediate calculation gives the preﬁx ∃∀∃∀for this condition. Finally, sentences
Φ and Ψ are equivalent on the class Ps.c(σ) of all strongly constructivizable prime
models if and only if (Φ ∧¬Ψ) ∨(Ψ ∧¬Φ) does not have a model in this class.
This gives the required preﬁx ∀∃∀∃for (a).
Let T be an arbitrary complete theory extending Th(Ps.c(σ)), and Ψ ∈T .
Obviously, Ψ has a model N ∈Ps.c(σ). From this we have that complete decidable
theory T ′ = Th(N) presenting a computable ultraﬁlter in St(Th(Ps.c(σ))), is
found in the neighborhood Ψ of the given arbitrary ultraﬁlter T of this Stone
space. This gives the required density property posed in (b).
Proof of Part (c) is considered in Sections 3–4.
3
A Technical Statement
In this section, we use the concept of a compact binary tree (cf. preliminaries).
Lemma 2. There exists a total computable functional Θ : P(N) →P(N), that
satisﬁes the following properties:
(a) D = Θ(A) is an atomic tree, for all A ⊆N,
(b) D = Θ(A) is a computably enumerable tree, if the set A ⊆N is computable,
(c) for any computable A ⊆N, in the tree D = Θ(A), the family Πﬁn(D) is
computable if and only if (∀k)

k ∈A ⇒Wk is ﬁnite

.
Proof. We describe an eﬀective process of constructing a computably enumerable
tree D(A), which depends on an input parameter A ⊆N. Following Rogers [9],
we use notation Wn for nth computably enumerable set in Post’s numbering of

592
M.G. Peretyat’kin
the family of all c.e. sets. Denote by W t
n a ﬁnite part of the set Wn that can be
computed in t steps. Introduce the following function dependent on A together
with a pair of integer parameters:
h(A, k, t) = Card
 
i∈A ∩{0,...,k}W t
i

.
It is obvious that for any A ⊆N the function h(A, k, t) is monotone in both k
and t, and the following property holds for an arbitrary A ⊆N:
(∃ωk)

lim
t→∞h(A, k, t) = ∞

⇔(∃j ∈A)[ Wj is inﬁnite ].
(3.1)
There exists a strong sequence of ﬁnite sets πt
n,i such that
(1) πt
n,i is a chain for all n, i, t (cf. preliminaries),
(2) πt
n,i ⊆πt+1
n,i for all n, i, t,
(3) any computable family of chains coincides with one of the families of the
form Πn = {πn,i|i ∈N}, n ∈N, where πn,i = ∪{πt
n,i|t ∈N}.
We now pass to construction of the tree D(A). For the sake of brevity, we
denote it by D. By Dt we denote a part of D, which is already constructed in
the moment t. We shall use the markers □k
n, n < k. At some moment, the marker
□k
n may be placed on a chain of family Πn passing through the element:
qk = 2k+2 −3 = L RR...R
  	
k
(0),
(3.2)
cf. [7, Fig.2.1.2(c)]. After that the marker is not moved. Some of the markers
may be unused. At each step, we consider a pair (n, k), n < k; moreover, each
such pair should be considered inﬁnitely many times.
Construction.
Step t = 0. Let us set
D0 = {0} ∪{1, 2, 5, 6, . . ., 2k+1 −3, 2k+1 −2, . . . ; k ∈N}.
The set D0 is an inﬁnite tree having the last right chain, as in [7, Fig. 2.1.2(c)].
None of the markers are used at the initial moment.
Step t > 0. We consider a pair (n, k).
Case 1: the marker □k
n, is not yet placed. In this case, we verify whether there
exists j < t such that qk ∈πt
n,j. In the case of a positive answer, we put the
marker □k
n on one of the chains πn,j subject to this condition. In addition, we
set Dt = Dt−1.
Case 2: the marker □k
n is already placed, and it marks the chain πn,j. Let e
be a maximal element of the set πt
n,j ∩Dt−1. We put
Dt =

Dt−1 ∪{L(e), R(e)}, if e < h(A, k, t),
Dt−1 otherwise.
The step t is complete.

The Tarski-Lindenbaum Algebra of the Class of SC Prime Models
593
Assume that π is an inﬁnite chain of the tree D = ∪{Dt|t ∈N}. We also as-
sume that π is not the last right chain. Then, π passes through one of the elements
(3.2). By construction, π must coincide with some chain πn,j marked by one of
the markers □k
n. Moreover, it is necessary that the condition limt→∞h(A, k, t) =
∞is satisﬁed. The above reasoning implies that at most k diﬀerent inﬁnite chains
can pass through the element qk, but only in the case of growth of the function
h(A, k, t) as t increases. Moreover, all these chains are computably enumerable.
Consequently, they are computable. Thus, in any case, the tree D is superatomic;
this implies that the target tree D is atomic.
We now prove the statement in Part (c) of Lemma 2.
First, suppose that (∀j)[ j ∈A ⇒Wj is ﬁnite]. By (3.1), we have
(∃k0)(∀k > k0) lim
t→∞h(A, k, t) < ∞.
In this case, the tree D can contain only a ﬁnite set of inﬁnite chains, and each
of these chains is computable. Hence, the family Πﬁn(D) is computable.
Now, suppose that (∃j ∈A)[Wj is inﬁnite]. Consider a family Πn containing
only ﬁnite chains. It is required to prove that Πn ̸= Πﬁn(D). To this end, we
take k > n such that
lim
t→∞h(A, k, t) = ∞.
In the case when the marker □k
n is not used, we obtain that none of the chains of
the family Πn passes through qk; therefore, Πn cannot coincide with the family
of all ﬁnite chains of the tree D. Consider another case when the marker □k
n is
placed on πn,j ∈Πn at some moment. Let e be its maximal element. Since the
function (λt)h(A, k, t) increases unboundedly, the elements L(e) and R(e) will
be included in Dt. As a result, the chain πn,j is not a maximal chain of the tree
D = D(A). In this case, we also obtain Πn ̸= Πﬁn(D).
These relations yield the necessary statement for Part (c) of Lemma 2
4
Proof to Part (c) of Theorem 1
Given a numerated Boolean Σ0
3-algebra (B, ν). By [8, Theorem 2.1], there is a
numeration ν′ of B such that (B, ν′) is a Boolean Σ0
3-algebra whose computable
ultraﬁlters represent a dense set in the set of all ultraﬁlters. For the sake of
simplicity, we shall assume that the source numerated algebra (B, ν) has such
properties, that is:
computable ultraﬁlters of (B, ν) form a dense set in St(B).
(4.0)
We assume, that B is a nontrivial algebra. By deﬁnition, signature operations ∪,
∩and −in B are presentable by computable functions on ν-numbers, and the
equality relation is a Σ0
3-relation in numeration ν:
ν(x)=ν(y) ⇔H(x, y),
H ∈Σ0
3.

594
M.G. Peretyat’kin
Consequently, there exists a unary relation H∗in this class Σ0
3 such that for any
ﬁnite tuple of zeros and ones α=⟨α0, α1, . . . , αn⟩, we have
ν(0)α0 ∩ν(1)α1 ∩. . . ∩ν(n)αn=0 ⇔⟨α0,α1, . . .,αn⟩∈H∗,
H∗∈Σ0
3.
For our construction, we shall use the following m-complete in class Σ0
3 set, cf.
[9, Sec. 14.8,Th. XV]:
E3 =

n | (∃k)

k ∈Wn ∧Wk is inﬁnite
 
.
(4.1)
Therefore, its complement is a set that is m-complete in class Π0
3:
A3 =

n | (∀k)

k ∈Wn ⇒Wk is ﬁnite
 
.
(4.2)
Particularly, we have n ∈A3 ⇔n ̸∈E3 for all n ∈N.
Since any Σ0
3-set is m-reducible to E3, there is a general computable function
f(x) such that for an arbitrary tuple α ∈2<ω, α = ⟨α0, ..., αn⟩, we have
ν(0)α0 ∩ν(1)α1 ∩. . . ∩ν(n)αn=0 ⇔(∃k)

k ∈Wf(α) ∧Wk is inﬁnite

, (4.3)
or equivalently,
ν(0)α0 ∩ν(1)α1 ∩. . . ∩ν(n)αn̸=0 ⇔(∀k)

k ∈Wf(α) ⇒Wk is ﬁnite

.
(4.4)
Lemma 3. The following assertions are true:
(a) f(∅) ∈A3,
(b) for any α in 2<ω, f(α) ∈A3 ⇔f(α0) ∈A3 or f(α1) ∈A3,
(c) for any α in 2<ω, f(α) ∈E3 ⇔f(α0) ∈E3 and f(α1) ∈E3.
Proof. The property (a) is a consequence of the fact that algebra B is nontrivial.
The property (b) follows from the deﬁnition of the function f(x) and relation
H∗(x), representing an ideal in the free Boolean algebra. The property (c) is a
corollary from (b).
Now, our goal is to choose some pair (m, s) of integer parameters.
Choice of m. We choose m such that Ω(m) = P(N) (cf. preliminaries). For
this purpose, it is enough to get m such that Wm = ∅.
Choice of s. For this purpose, we describe a computable functional Ψ from
P(N) to P(N), actually, yielding compact binary trees.
Given a set A ⊆N. Let α = ⟨α0, α1, ..., αk, ...⟩be the characteristic sequence
for A, i.e., the following is satisﬁed:
αk =

1,
if k ∈A,
0,
if k ̸∈A.
(4.5)
Taking A as an input parameter, let us construct the following set
QA = Wf(∅) ∪Wf(⟨α0⟩) ∪. . . ∪Wf(⟨α0,...,αn⟩) ∪. . . .
(4.6)

The Tarski-Lindenbaum Algebra of the Class of SC Prime Models
595
It can easily be checked that, in the case when A is computable, the set QA
is computably enumerable. Use this set QA as an input parameter for the con-
struction X →Θ(X) described in Lemma 2. As a result, we obtain a subset
D of N, which actually is a tree by Lemma 2 (a). Thereby, the transformation
A →Ψ(A) is presented by the following rule:
A →QA →Θ(QA) = D = Ψ(A).
(4.7)
End of description of the operator Ψ.
It can easily be checked that the described transformation A →Ψ(A) is
realized by an algorithm using the set A ⊆N as its input parameter. Thereby, the
transformation A →Ψ(A) can be considered as a computation by an algorithm
M with an oracle A. Let s be a G¨odel number of the algorithm M. In accordance
with the basic deﬁnitions of the theory of algorithms, we obtain the following
form of the operator Ψ:
Ψ(A) = DA
s .
(4.8)
Choice of the pair of parameters (m, s) is ﬁnished.
Study main properties of the transformation Ψ : A →DA
s .
Lemma 4. The following assertions hold:
(a) For any A ⊆N, DA
s is an atomic tree,
(b) For any computable A ⊆N, DA
s is a computably enumerable tree.
Proof. Statement of Part (a) is provided by Lemma 2 (a), while Part (b) is
provided by Lemma 2 (b).
Now we immediately pass to the proof of Part (c) of Theorem 1.
First of all, we have to point out a sentence Φ of the given ﬁnite rich signa-
ture σ, as it is stated in Part (c) of Theorem 1. For this, we use the canonical
construction, cf. [7, Ch. 3, Th. 3.1.1]. Apply this construction to the pair (m, s)
specifying also signature σ. As a result, we obtain a ﬁnitely axiomatizable the-
ory F = FC(m, s, σ) of signature σ. As Φ, we get a conjunction of axioms of the
theory F. After that, our principal aim is to show that the sentence Φ satisﬁes
all the requirements listed in Theorem 1 (c).
By the main statement of the canonical construction, there exists an eﬀective
sequence of sentences θn, n ∈N, of signature σ such that the family of extensions
of F deﬁned for each A ⊆N by
F[A] = F ∪{θi|i ∈A} ∪{¬θj|j ∈N∖A},
(4.9)
satisﬁes the following properties:
C1 For any A ⊆N, the theory F[A] is either complete or contradictory;
C2 The theory F[A], A ⊆N, is consistent if and only if A ∈Ω(m);
C3 For an arbitrary A ∈Ω(m), the following statements are true :
(a) theory F[A] has a prime model if and only if the tree DA
s is atomic,

596
M.G. Peretyat’kin
(b) a prime model of the theory F[A], if it exists, is strongly constructivizable
if and only if the set A is computable and the family of chains Πﬁn(DA
s )
is computable.
Consider an arbitrary ﬁnite tuple of zeros and ones α = ⟨α0, ..., αk⟩. Construct
an elementary intersection of the elements in B by the rule
bα = ν(0)α0 ∩ν(1)α1 ∩... ∩ν(k)αk,
(4.10)
as well as an elementary conjunction of corresponding sentences by the rule
βα = θα0
0
∧θα1
1
∧... ∧θαk
k
.
(4.11)
The main idea of the construction is to provide the following relation:
Lemma 5. For any tuple α ∈2<ω, bα ̸= 0 if and only if Φ ∧βα has a strongly
constructivizable prime model.
Proof. Assume that bα ̸= 0. By (4.0), computable ultraﬁlters form a dense set
among arbitrary ultraﬁlters in the Boolean algebra (B, ν). From this we obtain
that there is an inﬁnite sequence α∗= ⟨αi | i < ω⟩extending α such that the
set A related to α∗by (4.5) is computable, and
ν(0)α0 ∩... ∩ν(i)αi ̸= 0,
for all i ∈N.
(4.12)
By (4.4), we obtain that each set Wf(⟨α0,...,αi⟩), i ∈N, contains only indices of ﬁ-
nite sets; therefore, the set (4.6) also satisﬁes this property. Thereby, by
Lemma 2, the tree Ψ(A) = DA
s deﬁned in (4.7) and (4.8) is an atomic computable
tree with computable family Πﬁn(DA
s ). By C1, C2, and C3(a,b), theory F[A] is
consistent, complete, and has a prime model N, which is strongly constructiviz-
able. This ensures that the formula Φ ∧βα is satisﬁed in the strongly construc-
tivizable model N since it is provable from F[A].
Now, we assume that the sentence Φ∧βα has strongly constructivizable prime
model N. Consider the set
A = {θi | N |= θi},
(4.13)
which is obviously computable. Build an inﬁnite sequence α∗= ⟨αi | i < ω⟩
related to A by (4.5). Since A ∈Ω(m), by C1 and C2, the theory F[A] is
consistent and complete. Moreover, this theory is decidable by Janiczak theorem
since it is computably axiomatizable. By (4.13), all axioms of F[A] are satisﬁed
on the model N. Thereby, we have that A is computable and F[A] has a strongly
constructivizable prime model. By C3(a,b), we conclude that the tree DA
s is
atomic and the family Πﬁn(DA
s ) is computable. In accordance with rules (4.7)
and (4.8), we have DA
s = Θ(QA), where QA is deﬁned by (4.6). By Lemma 2, QA
must contain only indices of ﬁnite sets. Applying (4.4), we ﬁnally obtain bα ̸= 0.
Lemma 5 is proven.

The Tarski-Lindenbaum Algebra of the Class of SC Prime Models
597
Let us map elements ν(i), i ∈N, of Boolean algebra B to sentences θi, i ∈N, of
signature σ by the rule:
λ∗(ν(k)) = θk,
k ∈N.
(4.14)
Now, we shall extend the partial mapping (4.14) up to a computable isomorphism
of the algebras under consideration. Deﬁne a mapping
λ : B →L

Th

Mod(Φ) ∩Ps.c(σ)

by the following rule: for an arbitrary ﬁnite sequence of ﬁnite binary tuples
α0, α1, . . . αn ∈2<ω,
we put
λ(bα0 ∪bα1 ∪... ∪bαn) = βα0 ∪βα1 ∪... ∪βαn.
(4.15)
The mapping λ is deﬁned on all elements of B since the set of expressions involved
in (4.15) embraces all elements of this algebra; moreover, μ is a homomorphic
embedding of B into L

Th(Mod(Φ) ∩Ps.c(σ)). Based on notation (4.9) together
with property C1 for the canonical construction, by [7, Lem.0.3.2], we obtain
that the set {θi | i ∈N} is generating in L(Φ), and thus, in the Boolean algebra
L

Th(Mod(Φ) ∩Ps.c(σ)), ensuring that μ is a homomorphism ”onto”. Taking
into consideration the principal property stated in Lemma 5, we obtain that μ
is a one-to-one mapping. At last, Boolean operations above unions of elements
bα of the form (4.10) and disjunctions of elementary conjunctions βα of the form
(4.11), after omitting of zero terms, are produced by same rules. Therefore, λ is
an isomorphism. Obviously, it is computable in numerations ν and γ obtaining
ﬁnally the required computable isomorphism
λ : (B, ν) →

L

Th(Mod(Φ) ∩Ps.c(σ))

, γ

.
Thereby, Part (a) of Theorem 1 is completely proven.
5
Final Remarks
The statement of Theorem 1 characterizes the Tarski-Lindenbaum algebra of the
class of all strongly constructivizable prime models. It demonstrates a possibility
to include ”computable Brute Force” into a ﬁnitely axiomatizable theory. As a
complement to Theorem 1, we can point out an earlier result, [7, Th.8.2.5(c)],
that elementary theory of the class of all strongly constructivizable prime mod-
els is a Π0
4-complete set. As it is proven in [6, Sec.2, Th.1, Th.2], any Boolean
Π0
4-algebra admits a Σ0
3-numeration. Thus, Theorem 1 represents a signiﬁcant
characterization of the Tarski-Lindenbaum algebra L(Ps.c.(σ)).

598
M.G. Peretyat’kin
References
1. Goncharov, S.S.: Countable Boolean Algebras and Decidability. Plenum, New York
(1997)
2. Goncharov, S.S., Ershov, Y.L.: Constructive models. Plenum, New York (1999)
3. Goncharov, S.S., Nurtazin, A.T.: Constructive models of complete decidable theo-
ries. Algebra Logika 12(2), 67–77 (1973)
4. Harrington, L.: Recursively presented prime models. J. Symbolic Logic 39(2), 305–
309 (1974)
5. Hodges, W.: A shorter model theory. Cambridge University Press, Cambridge (1997)
6. Odintsov, S.P., Selivanov, V.L.: Arithmetical hierarchy and ideals of numerated
Boolean algebras. Siberian Math. Journal 30(6), 140–149 (1989) (Russian)
7. Peretyat’kin, M.G.: Finitely axiomatizable theories. Plenum, New York (1997)
8. Peretyat’kin, M.G.: On the numerated Boolean algebras with a dense set of com-
putable ultraﬁlters. Siberian Electronic Mathematical Reports (SEMR), 6 pp. (in
publication, 2012)
9. Rogers, H.J.: Theory of Recursive Functions and Eﬀective Computability. McGraw-
Hill Book Co., New York (1967)

Lower Bound on Weights
of Large Degree Threshold Functions
Vladimir V. Podolskii
Steklov Mathematical Institute, Gubkina str. 8, 119991, Moscow, Russia
podolskii@mi.ras.ru
Abstract. An integer polynomial p of n variables is called a threshold
gate for the Boolean function f of n variables if for all x ∈{0, 1}n
f(x) = 1 if and only if p(x) ⩾0. The weight of a threshold gate is the
sum of its absolute values.
In this paper we study how large weight might be needed if we ﬁx some
function and some threshold degree. We prove 2Ω(22n/5) lower bound on
this value. The best previous bound was 2Ω(2n/8) [12].
In addition we present substantially simpler proof of the weaker
2Ω(2n/4) lower bound. This proof is conceptually similar to other proofs
of the bounds on weights of nonlinear threshold gates, but avoids a lot
of technical details arising in other proofs. We hope that this proof will
help to show the ideas behind the construction used to prove these lower
bounds.
1
Introduction
Let f : {0, 1}n →{0, 1} be a Boolean function. A threshold gate for the Boolean
function f is an integer polynomial p(x) of n variables x = (x1, . . . , xn) such that
for any x ∈{0, 1}n we have f(x) = 1 if and only if p(x) ⩾0. In other words,
for all x ∈{0, 1}n it is true that f(x) = sgn p(x), where we adopt the following
deﬁnition of the sign function: sgn(t) = 1 if t ⩾0 and sgn(t) = 0 otherwise.
Thus, threshold gates are just representations of Boolean functions as the signs
of polynomials. The formal study of such representations started in 1968 with
the seminal monograph of Minsky and Papert [7]. Since then representations of
this form found a lot of applications in circuit complexity, structural complexity,
learning theory and communication complexity (see, for example [14,5,2,16]).
Two key complexity measures of threshold gates are the degree of the thresh-
old gate and its weight. The degree deg p of a threshold gate p is just the degree
of the polynomial. The weight W(p) of a threshold gate p is the sum of absolute
values of all its coeﬃcients.
The complexity measures of a Boolean function f related to these complexity
measures of threshold gates are the minimal threshold degree of a threshold gate
for f which we denote by deg± f and call the threshold degree and the minimal
weight of a threshold gate for f. Both of these complexity measures play an
important role in theoretical computer science (see the references above). In this
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 599–608, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

600
V.V. Podolskii
paper we are interested in the minimal possible value of the weight of a threshold
gate for some function f when the degree of the threshold gate is bounded. It
is convenient to denote by W(f, d) the minimal weight of a threshold gate of
degree at most d for f. Note that this value is deﬁned only if d ⩾deg± f. It is
also not hard to see that for all f we have deg± f ⩽n and W(f, n) ⩽2O(n) (just
consider the polynomial p such that p(x) = f(x) for all x).
The ﬁrst results on the value of W(f, d) were proven for d = 1. In [9] (see
also [8] and [4]) it was proven that for all f with deg± f = 1 it is true that
W(f, 1) = nO(n). For a long time only lower bounds of the form W(f, 1) =
2Ω(n) were known (see [10] for one of early results). Tight lower bound was
proven in [4], that is the function f with deg± f = 1 was constructed such that
W(f, 1) = nΩ(n).
Concerning higher degree d, upper bound can be easily extended from the
case d = 1. Namely for all f with deg± f ⩽d it is true (and easy to see) that
W(f, d) = nO(dnd) (see [15,3,12]). Note that this upper bound is much worse
than for the case d = 1. As for the lower bound, the one which is better than
for the linear case (and also tight for constant d) appeared in [12]. For any
constant d the function f of threshold degree d was constructed there such that
W(f, d) = nΩ(nd) (constant in Ω here depends on d). It is implicit in that paper
though that the argument works for nonconstant d also and the resulting lower
bound (with the dependence on d) is
n
d
 1
2 ( n
2d )d−o(( n
d )d)
.
(1)
For this result another proof was given in [1].
Thus it turns out that the required weight grows with the growth of degree
d. In this paper we are interested how large it might grow (note that for d = n
the weight is small again: W(f, n) ⩽2O(n)). That is we study the value
W = max
d
max
f : deg±(f)⩾d W(f, d).
The lower bound (1) works even for d linearly depending on n and so gives doubly
exponential lower bound on this value. But it works only for d ⩽(n −c)/32,
where c is some constant, so the best lower bound we get from [12] is 2Ω(2n/8).
In this paper we prove the following bound.
Theorem 1. W ⩾2Ω(22n/5).
We note that the best upper bound known is simple 2O(n2n) (this can be deduced
from the upper bound for the case d = 1 and the fact that there are at most 2n
monomials).
To prove our lower bound we adopt the strategy of [12] and provide a uniﬁed
treatment of the argument of that paper. In short, the proof strategy is as follows.
Starting from some function of threshold degree 1 (with some additional proper-
ties) that requires large weight when represented by degree-1 threshold gates we

Lower Bound on Weights of Large Degree Threshold Functions
601
construct its “d-dimensional” generalization in a very speciﬁc way. For this gener-
alization we are able to prove lower bounds for degree-d threshold gates and due
to the speciﬁc features of our generalization we can prove strong lower bound.
In the paper [12] the construction of the function starts with the function
constructed by H˚astad in [4] to prove the optimal lower bound for the case
d = 1. This helps to get n in the base of the exponent in the lower bound and
thus to prove strong lower bound for the case of constant d. On the other hand,
H˚astad’s function is very complicated and have desired properties only for large
enough number of variables (16 variables). This does not allow us to prove lower
bound for d close to n. In this paper we start with a much simpler functions
having required properties starting from just 3 variables. With this function we
cannot get n in the base of the exponent, but on the other hand we are able now
to prove bounds for much larger d and thus get better lower bound on W.
We start exposition of our result by giving a simpler proof of the weaker bound
of 2Ω(2n/4) . In this proof we are able to avoid a lot of technical complications
arising in the proof of [12] and make the function for which we prove the bound
much simpler (here we use as a starting function of threshold degree 1 well known
“greater than” function). We hope that this makes the proof easier to read and
helps to show the ideas behind the construction which were not very clear in [12].
After that we deﬁne another starting function and explain how to change the
proof to get W ⩾2Ω(22n/5) lower bound. The idea here is not only that we can
prove the bound for larger d, but also that, roughly speaking, choosing the good
function we can remove the constant 2 from the denominator of the term
 n
2d

in the exponent in the bound (1).
Besides representation of Boolean functions as f : {0, 1}n →{0, 1}, also rep-
resentation of the form f : {−1, +1}n →{−1, +1} turns out to be useful in
complexity theory. For this representation we can also consider threshold gates
and also deﬁne corresponding measures of the functions. Note that we can switch
from one representation to another one by a simple linear transform. Thus the
threshold degree of the function does not depend on the representation and the
threshold weight may change only by 2n multiplicative factor (see [6] for more
information on relations between threshold weights in these two settings). Since
this factor is very small compared to our lower bound, our result is true for both
representations and in the proof we can choose the one of two presentations of
Boolean functions which is more convenient to us. For the proof of weaker bound
we shall use {0, 1} variables and for the stronger one — {−1, 1} variables.
We note in the conclusion that if we have the lower bound S on the minimal
weight for the function of n variables and for degree d, it is easy to translate it to
exactly the same bound S for n′ = n + c and d′ = d + c for any c, even depending
on n (see, for example, [13], Corollary 1). This observation allows us to deduce
strong lower bounds on weights of threshold gates of degree close to n.
Theorem 2. For any ϵ > 0 and d ⩽(1 −ϵ)n there is an explicit function f
such that W(f, d) = 22Ω(n). For any d ⩽n −2(1 + ϵ) log n there is an explicit
function f such that W(f, d) = 2Ω(n1+ϵ).

602
V.V. Podolskii
The rest of the paper is devoted to the formulation and the proof of our results.
In Sections 2 and 3 we give a simple proof for the weaker bound: in the former
we construct the function for which in the latter we prove the lower bound. In
Section 4 we explain how to change the proof to give the stronger bound.
2
Construction of the Function
In this section we present the construction of the function for which we prove a
weaker form of our bound. Our function is the generalization of GT function.
Deﬁnition 1. For Boolean x, y ∈{0, 1}n let GT(x, y) = 1 iﬀx ⩾y, where
x = (x1, . . . , xn) and y = (y1, . . . , yn) are considered as binary representations
of integers with xn and yn being the most signiﬁcant bits.
Our function will depend on m = 2n variables (x, y) = (x1, . . . , xn, y1, . . . , yn) ∈
{0, 1}m. Let us ﬁx some k1, . . . , kd such that d
i=1 ki = m and partition the
input variables x and y in d groups of size k1, . . . , kd, that is
(x, y) = (x1, x2, . . . , xd, y1, y2, . . . , yd),
where xi, yi ∈{0, 1}ki for all i.
Let us denote by [k] the set {1, . . ., k}. Let us denote by <1 the following
ordering of the set [k]: 1, 2, 3, . . ., k −1, k, and by <0 the reverse ordering: k, k −
1, k −2, . . . , 2, 1. We shall use these orders on sets [k1], . . . , [kd]. It will always be
clear from the context which set we consider.
Let us denote by numil the ordinal number of l ∈[k] w.r.t. the order <i.
To deﬁne our function we need to deﬁne a speciﬁc order on the set K =
[k1] × . . . × [kd]. The construction below is essentially the same as in [12]. Our
order will be similar to the lexicographic one, that is to compare two tuples from
K we shall compare their components one by one until we ﬁnd the diﬀerence.
But as opposed to the lexicographic order, where each component of the tuples
is compared w.r.t. the same ordering, in our order of the tuples components
might be compared w.r.t. diﬀerent orderings. Moreover, the ordering in which
we compare the current component depends not only on the ordinal number of
the component but on the values of previous components of the tuples.
Formally, suppose we want to compare tuples α = (α1, . . . , αd) ∈K and
β = (β1, . . . , βd) ∈K. First we compare α1 and β1 w.r.t. the ordering <1. If
they are not equal then we have already compared the tuples: the larger the ﬁrst
component is the larger the tuple is. If they are equal we proceed to the second
components. To compare them we use the following recursive rule to choose the
next order.
Assume that the order <il to compare the lth components of the tuples is
already determined and it happens that αl = βl. The order to compare (l + 1)st
components is determined by the ordinal number of αl (which coincides with βl
by the assumption) w.r.t. the order <il. Namely,
il+1 = numilαl (mod 2).
(2)

Lower Bound on Weights of Large Degree Threshold Functions
603
In other words, we compare the (l+1)st coordinates w.r.t. the order <0 if αl has
even ordinal number w.r.t. the order <il and we compare (l + 1)st coordinates
w.r.t. the order <1 otherwise.
To say it the other way, we associate with coordinates of any tuple α =
(α1, . . . , αd) ∈K orders <i1, . . . , <id according to the rule (2). We use these
orders to compare coordinates of α with coordinates of other tuples. Note that
for two tuples α = (α1, . . . , αd) and β = (β1, . . . , βd) the orders corresponding
to their components coincides until we meet the ﬁrst diﬀerence. After the ﬁrst
diﬀerence the orders corresponding to components might be diﬀerent in α and
β but we do not need to compare coordinates any further. Let us denote by
numα,lαl the ordinal number of the lth component of α w.r.t. the corresponding
order.
Now we can deﬁne our function.
Deﬁnition 2. For given (x, y) = (x1, x2, . . . , xd, y1, y2, . . . , yd), where xi =
(xi
1, . . . , xi
ki), yi = (yi
1, . . . , yi
ki) ∈{0, 1}ki let α = (α1, . . . , αd) ∈K be the largest
tuple w.r.t. the introduced order such that d
i=1(xi
αi −yi
αi) ̸= 0. Then let
f(x1, . . . , xd, y1, . . . , yd) = sgn
d

i=1
(xi
αi −yi
αi).
If there is no such α let f(x1, . . . , xd, y1, . . . , yd) = 1.
Note that if d = 1 our function is exactly the GT function.
3
2Ω(2n/4) Lower Bound
First we note that our function is computable by a degree d threshold gate.
Lemma 1. deg±(f) ⩽d.
The proof of this lemma is rather simple. One just have to order monomials
d
i=1(xi
αi −yi
αi) for all α ∈K in the increasing order and sum them up with
suitable increasing coeﬃcients. The details of the proof are omitted.
Now we proceed to the main result of this section.
Theorem 3. Let ki ⩾2 be even for i < d and kd ⩾3. Then
deg±(f) = d
W(f, d) ⩾2
(kd−2)
d−1

i=1
ki−d
Remark 1. We can state analogous theorem for arbitrary ki ⩾2 for i < d and
not only for even. However, with this assumption the proof and the bound are
cleaner and at the same time the theorem still gives 2Ω(2n/4) bound.
The 2Ω(2n/4) lower bound follows easily from Theorem 3.

604
V.V. Podolskii
3.1
Proof of Theorem 3
Let us consider an arbitrary threshold gate p for f of degree at most d. That is,
for any x, y ∈{0, 1}n we have f(x, y) = sgn(p(x, y)).
It will be convenient for us to work in variables
ui
j = xi
j −yi
j,
vi
j = xi
j + yi
j.
(3)
So after substituting xi
j = (ui
j + vi
j)/2 and yi
j = (ui
j −vi
j)/2 and multiplying the
polynomial by 2d to make the coeﬃcients integer we obtain the polynomial p′ in
variables ui
j, vi
j that sign-represents f. That is
f(x, y) = sgn(p′(x −y, x + y)).
It is easy to see that the weight of the new polynomial is almost the same as the
weight of p (compared to the value of our bound). Namely, we have the following
bound.
Lemma 2. W(p′) ⩽2dW(p).
The proof of this lemma is simple and is omitted.
Remark 2. A similar bound holds in the other direction too, but we do not need it.
Now we have to prove that W(p′) ⩾2(kd−2) d−1
i=1 ki.
First we shall prove that we can assume that p′ has a nice structure. Lemmas
similar to the next one appeared in [4,12] (see [13] for a more general version).
Lemma 3. If we substitute by 0 all coeﬃcients of the monomials of p′ in which
variables from one of the groups u1, . . . , ud are not presented, the resulting poly-
nomial q will also sign-represent f.
The proof of this lemma is based on the fact that the function f is anti-symmetric
with respect to variables xi and yi for any i, that is if we permute variables xi
and yi then the function will change the value (except for some singular inputs).
From this it is easy to see that all monomials that are even in some variables ui
can be eliminated from p′. The details of the proof are omitted.
As a byproduct of the proof of this lemma we have the following corollary.
Corollary 1. deg±(f) = d.
Since W(p′) ⩾W(q) it is enough to prove that W(q) ⩾2(kd−2) d−1
i=1 ki.
Now we need a lemma concerning degree 1 threshold gates for GT. The ar-
gument is quite standard (see [10,11]) and is omitted.
Lemma 4. Let p = k
i=1 wiui be a degree 1 threshold gate for GT(x, y) where
x, y ∈{0, 1}k. Then for j ⩾2
wj ⩾2j−2w1 > 0
(4)
and
wj ⩾wj−1.
(5)

Lower Bound on Weights of Large Degree Threshold Functions
605
It will be convenient for us to consider two variants of the GT function: we
denote by GT1 the usual GT function and by GT0 the analogous function but
now on the reversed input. That is, GT0(x, y) = 1 if and only if x ⩾y, where
x = (x1, . . . , xk), y = (y1, . . . , yk) are considered as binary representations of
integer numbers where the most signiﬁcant bits are x1, y1. It is easy to see that
if p = k
i=1 wiui is a threshold gate for GT0 then we have wj−1 ⩾wj and
wn−j+1 ⩾2j−2wn > 0, where j = 2, . . . n.
Now we can prove the main lemma.
Lemma 5. For all l ⩽d if α ∈K is such that numα,iαi = 1 for all i ⩾l and
β = (α1, . . . , kl −αl + 1, . . . , αd). Then
wβ ⩾wα2(kd−2) d−1
i=l ki.
The idea is the following: we ﬁx variables in all groups u1, . . . , ud except one.
Then the function f becomes essentially the GT function and we can apply
Lemma 4. Repeating this trick we can accumulate the large factor due to the
speciﬁc construction of our order. More speciﬁcally the proof goes by induction
on decreasing l. For the base of induction l = d we ﬁx all variables except ud and
applying inequality (4) immediately obtain the desired result. For the induction
step we ﬁrst apply the induction hypothesis to l+1 and then apply inequality (5)
to the lth coordinate. Then we can again apply induction hypothesis to l + 1
and so forth. In this way we can apply induction hypothesis kl times and obtain
the desired result. The proof details are omitted.
It is easy to prove Theorem 3 now. Applying Lemma 5 with l = 1 we get
wβ ⩾wα2(kd−2) d−1
i=1 ki.
Now, it is easy to see that wα > 0 (just substitute u1
α1 = −1, ui
αi = 1 for all
i ̸= 1 and ui
j = 0 for all i and all j ̸= αi). We conclude that wα ⩾1 and
wβ ⩾2(kd−2) d−1
i=1 ki.
4
Improved Lower Bound
In this section we improve the argument of the previous sections to obtain the
better lower bound. More precisely we prove
W ⩾2Ω(22n/5).
We shall work with Boolean variables {−1, +1}, so we change the deﬁnition of
sgn-function: sgn(x) = 1 if x ⩾0 and sgn(x) = −1 otherwise.
The idea is to use another function instead of GT as a building block in our
construction.
Deﬁnition 3. For x = (x1, . . . , xk) ∈{−1, +1}k let g(x1, . . . , xk) be equal to
−xk if bits x1, . . . , xk are not all equal and let it be xk if they are all equal.

606
V.V. Podolskii
This function can be easily represented by a linear threshold gate:
g(x1, . . . , xk) = sgn(
k−1
	
i=1
xi −(k −2)xk).
We need an alternative deﬁnition of g. Consider k linear form: Li(x) = xi −xi+1
for i = 1, . . . , k −1 and L0(x) = x1 + xk (Li will play a role of ui in the previous
proof). The alternative (equivalent) deﬁnition of g is that g(x) is equal to the
sign of the last nonzero in the sequence L0(x), L1(x), L2(x), . . . , Lk−1(x). Note
that now it is more convenient for us to start numeration of Li from 0. The
reason for this is that the actual beneﬁt we shall get only from linear forms
L1, . . . , Lk−1. The form L0 is needed only for technical reasons (the same role
was previously played by variables vi).
Note that in the proof of weaker bound we needed actually not one base
function, but two of them. They were very similar though: GT0 and GT1. In
case of stronger bound two functions will diﬀer more substantially. Again, g1 is
just the function g we deﬁned above. As for g0 we let g0(x) be x1 if not all bits
of the input are equal and g0(x) = −x1 if all bits of input are equal. That is
now we not only reverse the order of variables but also multiply the value of the
function by −1. Note that g0(x) is equal to the sign of the last nonzero in the
sequence −L0(x), Lk−1(x), Lk−2(x), . . . , L1(x).
Now we can apply the previous proof scheme with the functions g1 and g0 on
ﬁrst d −1 components and with GT on the last component. We denote the new
function by f again. Below we state what changes in the proof.
For the new function the construction of the ordering is the same except that
we use diﬀerent orderings <
′
0 and <
′
1 on the ﬁrst d −1 coordinates, namely we
let <′
1 to be 0, 1, 2, . . ., k −2, k −1 and <′
0 to be 0, k −1, k −2, . . . , 2, 1, that is,
0 is always the smallest element. The orderings on the last coordinate remains
the same as before (as well as the rule (2) deﬁning the ordering on each next
coordinate).
In the Deﬁnition 2 we now have only variables x1, x2, . . . , xd, yd and we let
f(x) = sgn((−1)c1+...+cd−1Lα1(x1)Lα2(x2) . . . Lαd−1(xd−1)(xd
αd −yd
αd))
for the largest α for which the expression is nonzero, where ci = 1 if αi = 0 and
the order corresponding to the i-th coordinate of α is <′
0 and ci = 0 otherwise. If
there is no such α (which can happen only if xd = yd) we let f(x) to be 1. Note
that the number of variables n = d−1
i=1 ki + 2kd is almost twice less than before.
The theorem we prove has the following form.
Theorem 4. Let ki ⩾3 be odd for i < d and kd ⩾3. Then
deg±(f) = d
W(f, d) ⩾2(kd−2) d−1
i=1 (ki−1)−d log n
Instead of the variables (3) for the ﬁrst d−1 coordinates we now let ui
j = Lj(xi)
for j = 0, . . . , ki −1. We do not have now variables vi for i ⩽d −1.

Lower Bound on Weights of Large Degree Threshold Functions
607
The proof of Lemma 2 remains the same, but now the factor appearing in it
can be upper bounded by nd. The proof strategy of Lemma 3 remains essentially
the same, only this time instead of permutation of variables on ﬁrst d −1 coor-
dinates we multiply variables by −1. This shows the oddity of the polynomial
in variables ui for i ⩽d and the rest part is the same.
We present now the analog of Lemma 4.
Lemma 6. Let p = k−1
i=0 wiui be a degree 1 threshold gate for g1(x) where x ∈
{−1, +1}k. Then for j = 0, 1, . . ., k −1 we have wj > 0 and for j = 2, . . . , k −1
we have wj > wj−1.
For the function g0 analogous statement is true.
Lemma 7. Let p = k−1
i=0 wiui be a degree 1 threshold gate for g0(x) where
x ∈{−1, +1}k. Then we have w0 < 0, for j = 1, . . . , k −1 we have wj > 0 and
for j = 2, . . . , k −1 we have wj−1 > wj.
Proofs of these lemmas are omitted.
The analog of Lemma 5 is very similar to the previous version, but becomes
a little bit clumsy since we distinguish cases of l = d and l < d.
Lemma 8. For all l ⩽d if α ∈K is such that numα,iαi = 1 for all i ⩾l and
β = (α1, . . . , kl−αl+δl,d, . . . , αd), where δl,d is a Kronecker delta (that is δij = 1
if i = j and δij = 0 otherwise) then
wβ ⩾wα2(kd−2) d−1
i=l (ki−1).
Concerning the proof of the lemma, the base of induction remains completely
the same (note, that the statement is the same also). As for the induction step,
it also remains the same but now we can apply the induction hypothesis kl −1
times instead of kl times in the previous proof.
To conclude the proof of our lower bound we have to choose the values of ki
to maximize the lower bound we have. It is not hard to see that the optimal way
is to take kd = 3 as before and to take k1 = k2 = . . . = kd−1, let us denote the
value of them by k. Then the exponent of our bound will be about (k −1)n/k.
Simple analysis shows that the maximum (over integers) is attained when k = 5.
Thus we have a lower bound of 222(n−6)/5−n.
To prove the ﬁrst part of Theorem 2 we can just let m = 5
4ϵn and consider
the function from the previous paragraph with m variables. Then we have lower
bound 2Ω(22m/5) for degree m/5 threshold gates and applying the observation
preceding Theorem 2 we get the desired bound. For the second part of the
theorem let m = 5
2(1 + ϵ) log n.
Finally we note that the result of [12] can also be reproved by the same argu-
ment and with better constants if we use H˚astad’s function in the last coordinate
and g function in other coordinates.
Acknowledgements. The work is supported by the Russian Foundation for Ba-
sic Research and the programme “Leading Scientiﬁc Schools” (grant no.
NSh-5593.2012.1).

608
V.V. Podolskii
References
1. Babai, L., Hansen, K.A., Podolskii, V.V., Sun, X.: Weights of exact threshold
functions. In: Hlinˇen´y, P., Kuˇcera, A. (eds.) MFCS 2010. LNCS, vol. 6281, pp.
66–77. Springer, Heidelberg (2010)
2. Beigel, R.: Perceptrons, PP, and the polynomial hierarchy. Computational Com-
plexity 4, 339–349 (1994)
3. Buhrman, H., Vereshchagin, N.K., de Wolf, R.: On computation and communica-
tion with small bias. In: Proc. of the 22nd Conf. on Computational Complexity
(CCC), pp. 24–32 (2007)
4. H˚astad, J.: On the size of weights for threshold gates. SIAM J. Discret. Math. 7(3),
484–492 (1994)
5. Klivans, A.R., Servedio, R.A.: Learning DNF in time 2
˜
O(n1/3). J. Comput. Syst.
Sci. 68(2), 303–318 (2004)
6. Krause, M., Pudl´ak, P.: Computing Boolean functions by polynomials and thresh-
old circuits. Comput. Complex. 7(4), 346–370 (1998)
7. Minsky, M.L., Papert, S.A.: Perceptrons: Expanded edition. MIT Press, Cambridge
(1988)
8. Muroga, S.: Threshold logic and its applications. Wiley Interscience, Chichester
(1971)
9. Muroga, S., Toda, I., Takasu, S.: Theory of majority decision elements. Journal of
the Franklin Institute 271(5), 376–418 (1961)
10. Myhill, J., Kautz, W.H.: On the size of weights required for linear-input switching
functions. IRE Trans. on Electronic Computers 10(2), 288–290 (1961)
11. Parberry, I.: Circuit complexity and neural networks. MIT Press, Cambridge (1994)
12. Podolskii, V.V.: Perceptrons of large weight. Probl. Inf. Transm. 45, 46–53 (2009)
13. Podolskii, V.V., Sherstov, A.A.: A small decrease in the degree of a polynomial
with a given sign function can exponentially increase its weight and length. Math-
ematical Notes 87, 860–873 (2010)
14. Razborov, A.A.: On small depth threshold circuits. In: Proceedings of the Third
Scandinavian Workshop on Algorithm Theory, pp. 42–52. Springer, London (1992)
15. Saks, M.E.: Slicing the hypercube. Surveys in Combinatorics, pp. 211–255 (1993)
16. Sherstov, A.A.: Communication lower bounds using dual polynomials. Bulletin of
the EATCS 95, 59–93 (2008)

What Are Computers
(If They’re not Thinking Things)?
John Preston
Department of Philosophy, The University of Reading, Whiteknights,
P.O. Box 217, Reading, Berkshire, RG6 6AH, United Kingdom
Many of us now imagine that in the future humans either will, or at least could,
‘in theory’, construct an electronic digital computer which would really be a
thinking thing. Alan Turing was one of the ﬁrst and surely the most notable
exponent of this view, and a signiﬁcant proportion of his published work was
devoted to arguing for it.
However, even if one accepts this ‘computationalist’ view, the question ‘What
are computers?’ is still worth asking, since almost no-one thinks of past and
existing computers, or even of most foreseeable computers, as thinking things.
We still need an account of what these devices that now surround us are, even
if computationalists are right to think of them as proto-thinkers, as it were. In
fact, I think that when one sees the answer to this question, the temptation to
think that even more sophisticated computers really would be thinking things
evaporates.
1
Our Technologies
The electronic digital computer is the most prominent among a proliferation of
devices whose operations we regularly and quite naturally describe in some terms
that can also be used to characterise human actions: mousetraps catch mice,
washing machines wash clothes, dishwashers wash the crockery, thermostats turn
the central heating on and oﬀ, pocket calculators calculate, guided missiles seek
the exhaust heat of aircraft in order to destroy them, etc, etc.
Sometimes the underlying idea is still that these devices are things that we
use in order to carry out these tasks, activities or functions. (I caught the mouse
(using the mousetrap), you washed the clothes (by putting them in the washing
machine), etc.). In the case of some of these devices, though, we have some-
how learned to take seriously talk of the device itself carrying out the activity.
Usually, no misunderstanding results. But the computer falls into this group,
since we now think and talk of digital electronic computers as if they perform
computations, calculate, search for data, store information, execute instructions,
etc. This is entirely contingent, incidentally: when Turing was writing, the term
‘computer’ meant person who performs computations, and it could easily have
kept that meaning. If someone had invented a handy term or acronym for what
we now call computers (e.g., ‘EDMs’, for ‘electronic digital machines’), ‘com-
puter’ could have kept its original meaning, and we would easily and naturally
have thought of EDMs as ‘the devices that computers use’ to do these things.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 609–615, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

610
J. Preston
When we attribute such achievements there are three aspects to the attribu-
tion: the task gets done, it gets done correctly, and it’s the device that does it. To
take all this at face value would be to conclude that these devices can correctly
be said to be actually performing that function or carrying out that activity
which would otherwise have to be performed or carried out by a person, using
the activity-term in the same sense that it has when applied to humans. So we
can speak of computers not only as ‘searching for’ data, ‘storing’ information,
‘executing’ instructions, but also as playing chess, scheduling tasks, controlling
the operation of other machines, etc.
I’d like to suggest, though, that this way of talking when applied to computers
encourages us to conceive of them in the wrong way, that it involves getting their
entire relation to us wrong. In the case of computers, it’s quite remarkable that
hardly anyone stands up for this alternative and, I think, commonsense view of
them and their activities. A relatively superﬁcial feature of the language we use
to talk about such devices is allowing computationalists to set the terms of the
debate about ‘machine intelligence’.1
2
Replacing-Technologies
We human beings aren’t the only tool-using animals, but by any standard our use
of tools is the most widespread and the most impressive. We use tools in situa-
tions where we want something done but don’t want to use (only) our bodies to
do it. Our reasons for not using only our bodies can be various: not wanting to
get hurt, not being able to ﬁt into or reach a particular space, not wanting to get
dirty, performing some task (like driving in a screw, or cutting a piece of wood) for
which no part of the human body will do, as well as considerations of speed, cost,
eﬃciency, etc. Alongside using tools (in this core sense) that relieve us of tasks
that are too dangerous, dirty, or physically unmanageable for our bodies, we now
also use technologies that relieve us of tasks that are boring, complicated, time-
consuming or expensive. This is often where computers come in. Of course, we
also use computers to help us control machines of other kinds - cars, telephones,
industrial machines, etc..
Computationalists are right to think that there’s something importantly dif-
ferent about the computer. The computer is a new kind of technology. But it’s
a kind that’s nevertheless contiguous with other kinds of technologies, and to
understand it properly as a technology is to understand both this contiguity and
the way in which it goes beyond previous technologies.
For each technology there’s a crucial distinction between the process or ac-
tivity involved and the product that results. What I call ‘replacing-technologies’
are machines and devices that replace human activities. (Not all our machines
1 One small group of philosophers who do resist the computationalist current is in-
spired by certain remarks that Ludwig Wittgenstein made about computation, partly
in response to Turing, and I in turn have taken inspiration from them in this article.
Their works in question here are [3], [4], and [6]. None of these people are to be
blamed for any problematic developments of their ideas that I make here, though.

What Are Computers
611
are replacing-technologies: no number of human beings, unaided by technology,
could ever produce certain electromagnetic phenomena, or atomic power, or a
nuclear explosion). We use replacing-technologies to ensure that the product or
end-state of some activity that could only formerly be done by humans is brought
about, even though the process or activity itself hasn’t taken place. Of course,
some process has taken place, but this process isn’t the one that we formerly
used, unaided, to produce that result. The device in question replaces a stretch
of human activity, but does so in such a way that the arrangement which would
otherwise have been the end-state of that activity does result.
Our pre-computational technology mostly involved tools and implements for
the performance of tasks that used to require a signiﬁcant amount of human
physical eﬀort. Computers, though, mostly relieve us of a diﬀerent kind of ac-
tivity, the kind of activity that used to require thinking, reasoning, inferring,
calculating, remembering, and certain other psychological skills. Because the
activities they replace are of this kind, intellectual rather than manual, and be-
cause these activities are often quite distinctive to human beings (they can’t
be performed by other creatures), computers are a very special, and especially
impressive, kind of replacement-technology, a technology capable of being espe-
cially close to our minds (and thus perhaps to our hearts, as it were).
This distinctiveness is of course down to Alan Turing, who showed not only
that a certain kind of mathematical function can be encoded in a way that
makes it ideally suited to be mechanically implemented (in electrical circuitry),
but also that what we now call ‘Turing machines’ are devices with a certain kind
of universality (the kind speciﬁed in [7, § 6]). Since almost all our programmed
electronic devices can be thought of as (horrendously complicated) Turing ma-
chines, our stored-program digital computers are general-purpose technologies
in a speciﬁc way, a way in which no previous technology has ever been. Their
ever-increasing importance to us derives at least partly from this fact.
3
Intentional Actions and Non-intentional Operations
Because the main philosophical debate has been about machine intelligence, com-
putationalists tend to think that if one could show that intelligent operations can
be ‘broken down into’ unintelligent ones, and then show that Turing machines op-
erate entirely on the basis of millions of operations of just this unintelligent kind,
one would have successfully opened the door to the idea of an intelligent ma-
chine. There are problems with this strategy, but regardless of whether it works,
I suggest instead that the key issue is really not intelligence but another, more
fundamental feature of certain psychological concepts, the one that philosophers
call intentionality. Intentionality is a feature of certain psychological phenomena
crucially, the same psychological phenomena that computationalists are centrally
concerned with, such as thoughts, beliefs, and desires. Roughly, a psychological
verb ψ is intentional if it can be true to say of a person that he or she ψ’s that
p, on a particular occasion, despite the fact that p isn’t the case on that occa-
sion. So, for example, I can want Usain Bolt to set a new world record in the

612
J. Preston
100 metres at the London Olympics even if it turns out that he doesn’t, I can
think or believe that he’s Trinidadian even though he isn’t, I can even regret that
he didn’t win the 200 metres at the 2009 Berlin World Championships despite
the fact that (unbeknownst to me at that time, of course) he did. Intentionality
means that these psychological phenomena can still be correctly attributed to
people even when the ‘object’ of those phenomena doesn’t exist, or doesn’t come
to pass. Its exact technical deﬁnition and history aren’t vital here but, the thing
to note is that whether or not the high-level operations of computers are rightly
described in ‘intentional’ terms (as computationalists would like, and as some of
our ordinary and some of our technical talk about them suggests), those opera-
tions can always be broken down into sub-operations that are non-intentional.
At the lowest level, as it were (think of this as the level of the Turing machine),
the operations involved are clearly and purely mechanical. They are operations
such as: scanning a square on the machine-tape, registering the contents of that
square, erasing the symbol in the square, writing another such symbol, etc. That
the operations of almost all our computers can be broken down into such steps
is one of the things that Turing’s ‘On Computable Numbers’ absolutely guaran-
tees. But the fact that these operations are purely mechanical is a sure sign that
they’re not genuinely intentional.
For the fact is that neither human actions nor human psychological skills
can be thus ‘broken down’. Complex actions and skills can be thought of as
composed of simple ones, of course, but these simple ones are still intentional.2
This is a logical feature of intentionality, not merely a contingent one, and it
marks a fundamental, categorical divide between the way in which computers
work and the nature of human beings. However complex our computers become,
they will only involve ‘more of the same’ (i.e., more such purely mechanical
operations), and this can’t yield thought or intelligence, just because it can’t
yield intentionality.
4
The Intentional Stance?
Isn’t it the case, though, as Daniel Dennett has suggested,3 that we take the
intentional stance with respect to computers? That is, that we (at least on
occasion) think of them in intentional terms, ascribing intentional properties to
them?
On a very superﬁcial level it’s true that most of us do occasionally and casually
say of a computer that it ‘wants’ or is ‘waiting for’ an instruction, that it’s
‘recalling’ certain information, or even (when it’s taking a long time processing)
that it’s ‘thinking about’ an issue. And when things go wrong we sometimes
think of the computer we’re using as frustrating our plans, perhaps even as
conspiring to do so.
On a deeper level, as I’ve already stressed, although adult human beings don’t
often think of non-computational devices in such psychological terms, we do
2 Here I am indebted to Tony Palmer’s important but neglected paper [5].
3 Cf. several of the essays reprinted in [1,2].

What Are Computers
613
describe computers as if they achieve the psychological tasks they in fact replace
(computing, calculating, storing information, executing instructions, etc.).
But to conclude from either of these habits that we do or can take the ‘in-
tentional stance’ towards them would be a wild exaggeration. The range of in-
tentional terms that we apply to computers is radically restricted and thus a
very weak analogue of the full range of intentional terms that can be applied to
humans. There’s simply no application for most of the associated psychological
terms we predicate of other humans, such as hoping, fearing, expecting, aspiring,
anticipating, despairing, regretting, considering, reconsidering, contemplating,
pondering, reﬂecting, etc. This is a consequence of what philosophers call the
‘holism’ of the intentional: the serious and literal application of an intentional
term brings with it the actuality, or at least the possibility, of applying other
intentional terms to that same person. What’s more, even when we consider
those few intentional terms that we do apply to computers, we should remember
that humans are of course quite capable of applying them to non-computational
devices, and even to artifacts which aren’t devices at all (such as dolls and pup-
pets). So although computers (machines generally) can be described using a
very weak image or shadow of one range of our psychological vocabulary, it does
no harm whatsoever to think that our applications of such terms to machines,
including computers, are always ‘in scare-quotes’, and not to be taken seriously.
Nevertheless, to talk of past, present or, I would argue, future computers as
achieving the tasks they replace, or even occasionally as performing intelligent
activities, such as thought, does at least have two very good rationales. The
ﬁrst is the fact that what their activities replace are indeed usually stretches of
thought or intellectual activity. Where this clearly isn’t the case, as for example
in the case of a car’s engine management system, which is also computational,
there’s very little temptation to think of the device in intentional terms. So
where we humans, unaided, would have to think in order to attain some result,
a temptation to conceive also of the computer which can be made to achieve
that result (perhaps much faster and more reliably than we can) as a thinking
thing is quite naturally felt.
The second rationale for talking of computers using a very limited range of
intentional terms is that most of us simply don’t know how computers operate.
Their operation is recondite, that is, hidden from our inspection, and its full
mechanical description is simply unwieldy. It would be quite wrong though, to
suggest, as Dennett sometimes does, that the only way of coming to understand
‘what the computer is doing’ is from the ‘intentional stance’. That is, we all
know that there is a purely mechanical description of their activity, although
most of us couldn’t say what it is. Nevertheless, taking the ‘intentional stance’
towards computers (where this is understood in a very limited way, as a matter
of predicting and explaining their operations and products in terms of goals,
searches, decisions, etc.) is quite natural when one knows of no other way of
explaining their operations, or even when this other way is simply too long-
winded (which is almost always the case with computers, simply due to the
number of operations they are capable of performing at great speed). This also

614
J. Preston
explains why computer science texts usually explain the operation of computers
in terms of analogies with human rule-following activities (such as games, recipes,
knitting patterns, etc.).
Both these rationales, however, are very clearly pragmatic in nature. They can
do nothing to support the serious philosophical conclusion that such machines are
really performing intentional operations. It’s surely ironic that serious thinkers
who would otherwise scout such considerations have allowed them to prevail
in this context, since elsewhere such rationales would normally be regarded as
hopelessly insuﬃcient for a philosophical conclusion.
5
Conclusion
In sum, computers are the latest kind of labour-saving technologies. The labour
they save isn’t that of the ‘workers by hand’, though, but that of the ‘workers
by brain’. However sophisticated, computers aren’t things which compute, but
things that we use to replace the human activity of computation. Perhaps they
might thus be better described as things that people compute with, or things
that people use to compute. But even these descriptions have to be taken with a
pinch of salt, since in cases where I use a computational device to generate for me
the result of a calculation, for example, no actual calculation (or computation)
has taken place.
Among those working in the ﬁeld of artiﬁcial intelligence, it is perhaps the
designers of ‘expert systems’ who have come closest to realising the replacement
status of computer technologies, since they’re quite explicitly aware that they
are (in their terms) trying to ‘mechanise’ the boring bits of experts’ jobs. How
ironic that the least interesting bit of artiﬁcial intelligence (from the point of
view of philosophy) should have the clearest conception, not merely of the goals
of the subject, but also of the nature of the devices it produces!
Where does all this leave Alan Turing himself? Turing manifestly still counts
as a great thinker, one whose mathematical work made possible the development
of devices that are eﬀecting an enormous and unprecedented transformation in
human work- (and leisure-) activity. He is thus the herald of a new machine-
age (for better or worse, of course). But if we cease to think of him as the
herald of thinking or intelligent machines, even though he would himself have
protested and in some circles his reputation will be thought to suﬀer, his genuine
achievements will in no way be underestimated.
References
1. Dennett, D.C.: Brainstorms: Philosophical Essays on Mind and Psychology. Har-
vester Press, Sussex (1981)
2. Dennett, D.C.: The Intentional Stance. MIT Press, Cambridge (1987)
3. Hacker, P.M.S.: Men, Minds and Machines. In: Wittgenstein: Meaning and Mind,
pp. 147–170. Blackwell, Oxford (1990)

What Are Computers
615
4. Hyman, J.: Introduction to Investigating Psychology: Sciences of the Mind after
Wittgenstein, pp. 1–24. Routledge, London (1991)
5. Palmer, A.: The Limits of AI: Thought Experiments and Conceptual Investigations.
In: Torrance, S. (ed.) The Mind and the Machine: Philosophical Aspects of Artiﬁcial
Intelligence, pp. 43–50. Ellis Horwood, Chichester (1984)
6. Shanker, S.: Wittgenstein’s Remarks on the Foundations of AI. Routledge, London
(1998)
7. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proc. Lond. Math. Soc., II. Ser. 42, 230–265 (1936)

Compactness and the Eﬀectivity
of Uniformization
Robert Rettinger
Fakult¨at f¨ur Mathematik und Informatik, FernUniversit¨at in Hagen,
Universit¨atsstraße 1, 58097 Hagen, Germany
Abstract. We give new proofs of eﬀective versions of the Riemann map-
ping theorem, its extension to multiply connected domains and the uni-
formization on Riemann surfaces. Astonishingly, in the presented proofs
we need barely more than computational compactness and the classical
results.
1
Introduction
The Riemann mapping theorem is probably one of the most fundamental theo-
rems in complex analysis. Thus, not surprisingly, eﬀectivization of this theorem
was on the agenda ever since the theorem was proven. The ﬁrst general con-
structive solution for Riemann mappings were given by P. Koebe in 1910 using
the osculation method (cf. [5]). More constructivity results were added over the
years based on diﬀerent methods like potential theoretic methods, circle pack-
ings and random walks. Using the osculation method, P. Hertling [6] gave the
ﬁrst exact characterization of the eﬀectiveness of the Riemann mapping based on
the type-2-theory of eﬀectivity. Similar results were recently given for multiply
connected domains (cf. [7]), whereas eﬀectivity results for the uniformization of
Riemann surfaces were not known before.
Despite the belief that the classical proof of the Riemann mapping theorem is
highly non-constructive, we will show that we have to add only a few things to
make it constructive. More precisely, our main ingredient will be computational
compactness of a certain class of formal power series. This property will even
show eﬀectiveness of other (non-constructively proven) results such as conformal
mappings for multiply connected domains and the uniformization of Riemann
surfaces. To be more precise, we list the classical results which we are going to
eﬀectivize in the next sections below.
Let Dε(z0) denote the disc {z ∈C | |z −z0| < ε}. Furthermore we use Dε
and D instead of Dε(0) and D1(0), respectively. The classical Riemann theorem
guarantees the existence of conformal mappings for simply connected domains
(cf., e.g., [1]):
Theorem 1. Let U be a proper and simply connected open subset of C, 0 ∈U.
Then there exists a unique bi-holomorphic mapping f : U →D with f(0) = 0
and f ′(u) > 0.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 616–625, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Compactness and the Eﬀectivity of Uniformization
617
A similar result holds for multiply connected domains. There is a small techni-
cality concerning this case: For k > 1 there does not exist a single k-connected
domain D so that all k-connected domains can be bi-holomorphically mapped
onto D, that is there does not exist a canonical k-connected domain. Here we
use circles with concentric circular slits but any of the families found in liter-
ature (cf., e.g., [9]) would work as well. To deﬁne the class Dk of k-connected
circles with concentric circular slits let ι : ∂D × [0; 1] × [0; 1] →2D be the map-
ping ι(γ, a, b) = {b · γ′ | ∃0 ≤α ≤a.γ′ = γ · e2πiα}. Then a set D is in
Dk iﬀD = D \ (ι(γ1, a1, b1) ∪... ∪ι(γk−1, ak−1, bk−1)) for pairwise disjoint sets
ι(γ1, a1, b1), ...,ι(γk−1, ak−1, bk−1) with γi ∈∂D, ai ∈[0; 1) and bi ∈(0; 1).
Theorem 2. Let k > 0 be a natural number, U be a k-connected, proper subset
of C and K be a connected component of the boundary ∂U of U, 0 ∈U. Then
there exists a unique domain D ∈Dk and a unique bi-holomorphic mapping
f : U →D so that f(z) = 0, f ′(0) > 0 and f(K) ⊆∂D.
A Riemann surface is a topological space T together with an atlas τ, i.e., τ is
a family τ = (τi)i∈N of topological mappings τi : D →T so that T = 
i τi(D)
and τ−1
i
◦τj is bi-holomorphic on τ−1
j
(τi(D)) for all i, j ∈N. Thus the maps of
the atlas, i.e., the τi, deﬁne a complex structure on T . Furthermore, a mapping
f : T →T ′ of two Riemann surfaces (T, τ) and (T ′, τ′) is called meromorphic iﬀ
for all i, j ∈N the mapping τ−1
j
◦f ◦τi is holomorphic.
Theorem 3. Let (T, τ) be a simply connected Riemann surface. Then T can be
bi-meromorphically mapped onto exactly one of the following spaces
(1) D (hyperbolic case),
(2) C (spherical case) or (3) C∞(Euclidean case),
where C∞denotes the complex sphere. More precisely, let z0, z1, z∞∈T and R
either D, C, C∞. Then there exists a unique bi-holomorphic mapping
(1) fz0 : T →D with fz0(z0) = 0 and f ′
z0(z0) > 0 in the hyperbolic case,
(2) fz0 : T →C with fz0(z0) = 0 and f ′
z0(z0) = 1 in the spherical case and
(3) fz0,z1,z∞
: T
→C∞with fz0,z1,z∞(z0) = 0, fz1,z1,z∞(z1) = 1 and
fz0,z1,z∞(z∞) = ∞in the Euclidean case.
2
Type-2-Theory, Enumerability and Computational
Compactness
Most of the following notions and more details can be found, e.g., in the book
[12]. We will start with some basic, well established concepts of type-2-theory:
relative computability and representations. Afterwards we will introduce two less
frequently used notions: r.e. open sets and computationally compact spaces.
Let Σω denote the set of inﬁnite sequences α0α1... of elements of the ﬁnite
alphabet Σ and Preﬁx(ω) denote the set of ﬁnite preﬁxes of the sequence ω ∈Σω.
To introduce computability on Σω we use the classical Turing-machine: A partial

618
R. Rettinger
function f :⊆Σω →Σω is called computable iﬀthere exists a computable
function fe :⊆Σ∗→Σ∗so that
ω ∈dom(f) iﬀPreﬁx(ω) ⊆dom(fe) and fe(Preﬁx(ω)) = Preﬁx(f(ω)).
Given two representations ρ, ρ′ of the sets M, M ′, i.e., surjective functions ρ :⊆
Σω →M and ρ′ :⊆Σω →M ′, we call a function f :⊆M →M ′ (ρ, ρ′)-
computable, iﬀthere exists a computable realization g of f, i.e., a computable
function g :⊆Σω →Σω so that f ◦ρ(ω) = ρ′ ◦g(ω) for all ω ∈dom(f ◦ρ). Thus
computability on M and M ′ depends on the chosen representation ρ and ρ′ up
to equivalence, i.e., computable translations between these representations. In
the same way we can introduce computability on ﬁnite products and sets given
by numberings ν : N →M. If the representations are obvious by the context or
if we use so called standard representations of spaces then we will simply say
that a function f is computable instead of f is (ρ, ρ′)-computable. Notice that
all the following (standard) representations are admissible with respect to the
usual topology on these spaces.
Using a standard tupling function in a straight forward way, we can immedi-
ately deﬁne, for given numbering ν of M and representations ρ, ρ′ of sets T, T ′, re-
spectively, representations ν∗,  ν, ν×ρ, ρ×ρ′,  ρ and ρ∗of M ∗, ∞
i=0 M, M ×
T , T × T ′, ∞
i=0 T and T ∗, respectively, where T ∗= 
k T k,  ν(0i010i11...) =
(ν(i0), ν(i1), ...),  ρ(α0α1...) = (ρ(α⟨0,0⟩α⟨0,1⟩...), ρ(α⟨1,0⟩α⟨1,1⟩...), ...) and the
other products can be determined in the same way.
Let νQ denote a standard numbering of Q then the standard representation
ρR of R is deﬁned to be the restriction of  νQ to fast converging sequences,
i.e., sequences (q0, q1, ...) of rational numbers so that |qi −qj| < 2−min(i,j). The
standard representations of C, Rn and Cn for arbitrary n is then deﬁned by
ﬁnite products of ρR. Furthermore, the standard representation ρU of the open
subsets of C is deﬁned via the inﬁnite product (ν2
Q × νQ), where a sequence
((q0, q′
0), q′′
0 ), ((q1, q′
1), q′′
1 ), ... represents the set 
i Dq′′
i (qi + i · q′
i).
Using the standard representation ρC of the complex numbers, we get a stan-
dard representation ρF =  ρC of the set FP of formal power series over C. We
will use the usual notation f = 
i ai · zi instead of (a0, a1, ...) in the sequel.
Furthermore, if A is some domain with 0 ∈A, a formal power series deﬁnes a
holomorphic function on a neighborhood of 0 iﬀit converges on this neighbor-
hood. We will denote this function by f, too.
Let (T, τ) be some Riemann surface. Then we identify T with the set of
functions τ −1
i
◦τj. We will do this as follows: Let ρR be the restriction of  ρC ×
 ρU ×  ρF to those triples ((z0, z1, ...), (U0, U1, ...), (f0, f1, ...)) so that zi ∈D,
Ui ⊆D and there exists a Riemann surface (T, τ) so that dom(τ −1
i
◦τj) =
U⟨i,j⟩and f⟨i,j⟩= τ−1
i
◦τj on U⟨i,j⟩. Furthermore, we call a Riemann surface
(T, τ) ﬁnitely generated iﬀthere exist i so that T = i
j=0 τj(D). In this case
we can deﬁne a representation ρF R of these Riemann surfaces as a restriction of
ρ∗
C × ρ∗
U × ρ∗
F in the same way. For C∞we ﬁx the standard representations given
by the maps τ0, τ1, so that τ0(z) = 2 · z and τ1(z) = 1/z for all z ∈D.

Compactness and the Eﬀectivity of Uniformization
619
Finally we use the following standard representation of the Sierpinski space
S = {0, 1}: Let ρS(0ω) = 0 and ρS(w) = 1 for all w ∈{0, 1}ω \ {0ω}. Notice that
this is a standard representation with respect to the usual topology ∅, {1}, {0, 1}
on S. Using the Sierpinski space, we say that a subset O of a space A with a
standard representation ν of A is r.e. open iﬀthere exists a computable (total)
function f : A →S so that O = f −1(1). Similarly, for a family g :⊆B →2A with
index set B we call g uniformly r.e. open, iﬀthere exists a computable function
f : B×A →S so that for all b ∈dom(g) we have g(b) = {w | (b, w) ∈f −t(1)}. To
simplify matters we will use in most cases non-uniform recursive enumerability.
However, all the results mentioned below do indeed hold even uniformly for
computable families g. Finally, we deﬁne relative r.e. openness for subsets O ⊂A
of a represented space A as follows: O is r.e. open in B ⊆A iﬀthere exists a
computable function f :⊆A →S so that B ⊆dom(f) and O = f −1(1). Notice
that A is always r.e. open in A.
For subsets of C r.e. openness is equivalent to the enumerablility of all rational
closed balls inside the set ( A denotes the topological closure of A):
Lemma 1. Let O be a subset of C. Then O is r.e. open iﬀthere exists an r.e.
set R ⊆Q × Q[i] so that O = 
(q,q′)∈R Dq(q′).
Furthermore it is well known, that several constructions preserve r.e. openness.
Lemma 2. Let T, T ′ be r.e. open sets and f : T →T ′ be computable. Then
T × T ′,
T ∩T ′,
T ∪T ′ and f −1(T ′)
are r.e. open.
Given two sequences (qi)i and (q′
i)i and some j it is easy to decide whether
(qi −q′
i) > 2−i+2, which means that the set {(x, x′) ∈R2 | x > x′} is r.e. open
and we get the following result by Lemma 2.
Corollary 1. Let f, g : U →R be computable functions and U be r.e. open.
Then the sets
{z | f(z) > g(z)} and {z | f(z) < g(z)}
are r.e. open.
To deﬁne computable compactness let T be a separable T0 space and (Bi)i be an
enumeration of a basis of T . Then we call K ⊆T computably compact iﬀK is
a compact Hausdorﬀspace and there exists an r.e. set R ⊆N∗so that the set
{(Bi0, ...., Bin) | (i0, ..., in) ∈R} is the set of all ﬁnite coverings of K by basis
elements. Similar to the r.e. open case, we deﬁne a uniform version of computable
compactness and, although we give in most cases only the non-uniform results,
all the results mentioned below hold even in the uniform setting. Let g :⊆B →
2T be a family of closed subsets of T with index set B. We call g uniformly
computably compact iﬀthere exists a computable function f : B →(N∗)ω so
that, for all b ∈dom(g), we have that g(b) is a compact Hausdorﬀspace and

620
R. Rettinger
{(Bi0, ...., Bin) | (i0, ..., in) ∈f(b)} is the set of all ﬁnite coverings of g(b) by basis
elements. For the spaces used in this paper we choose a canonical enumeration of
basis elements (which are, anyway, already chosen by saying that our standard
representations are admissible).
Simple examples of computationally compact spaces are the compact inter-
vals [p; q] for computable p and q or ∂D. The following lemma gives us all the
computationally compact spaces which we will need throughout this paper.
Lemma 3. Let T , T ′ be computationally compact spaces and f : T →T ′ be
computable. Then
T × T ′,
∞

i=0
T and f(T )
are computationally compact.
The result on ﬁnite products is already shown in [13]. The inﬁnite product,
i.e., the eﬀective version of the Theorem of Tychonoﬀ, can be easily proven by
the very same methods used to prove the ﬁnite case. An even more general
eﬀective version of Tychonoﬀ’s Theorem can be found in [11]. As an immediate
consequence of Lemma 3 we get that the space of bounded formal power series
are computationally compact:
Corollary 2. Let (ri)i be computable sequence of computable numbers. Then
the class FP[(ri)i] := {f = 
i aizi | ∀i.|ai| ≤ri} is computably compact.
Having all these notions at hand the following ”computability by compactness”
argument is quite simple to prove. This principle is common place but neverthe-
less quite powerful as we will see in the next section. (For a diﬀerent application
of this principle, cf., e.g., [4].)
Let T now be some computationally compact space with an enumeration ν
of a basis. Furthermore let t ∈T be some element so that T \ {t} is r.e. open.
Then t is computable with respect to the standard admissible representation:
We have to enumerate all (indexes of) basis elements A so that t ∈A. To do so,
let (Bi)i and (Ci)i be enumerations of all ﬁnite coverings (ﬁnite set of indexes
of basis elements) of T and an enumeration of all indexes of basis elements so
that 
i ν(Ci) = T \ {t}, respectively. We can easily enumerate all indexes i of
basis elements of T so that {i} ∪n
j=0{Cj} ⊃Bj for some i, j, n ∈N. If t ∈ν(i)
then there exists some n and j so that the previous equality is fulﬁlled because
ν(i) ∪(T \ {t}) = T , T is compact, i.e., there exists some ﬁnite sub-covering of
the open covering ν(i), ν(C0), ν(C1), ... of T and (Bi)i is an enumeration of all
(ﬁnite sets of indexes of) ﬁnite coverings of T . If, on the other hand, t /∈ν(i)
then ν(i) ∪(T \ {t}) = T \ {t} and the above equality is never fulﬁlled, as (Bi)i
contains only sets of indexes of coverings of T . We can state this argument as
follows:
Lemma 4. Let T be a computationally compact space (with respect to an ad-
missible representation) and t ∈T be given so that T \ {t} is r.e. open. Then t
is computable.

Compactness and the Eﬀectivity of Uniformization
621
3
Conformal Mappings for Simply and Multiply
Connected Domains
In this section we will apply Lemma 4 to re-prove eﬀective versions of Theorem 1
and Theorem 2. For simplicity reasons we will restrict ourselves to bounded do-
mains and discuss the case of unbounded domains at the end of this section.
Given an open computable function f : T →T ′, r.e. openness of T does not
necessarily imply r.e. openness of T ′. For holomorphic functions, however, this
can be easily shown (cf. [6]). Given an ϵ > 0 and an r.e. open domain A ⊆C we
denote the set of formal power series which determine non-constant holomorphic
functions f on A with f(A) ⊆Dε by H[A, Dε]. Notice that we will assume that
A is a domain with 0 ∈A without further mentioning. For non-connected A the
following reformulation of the Eﬀective Open Mapping Theorem in [6] hold, if
one replaces A by the connected component of A which contains 0.
Lemma 5. Let ε > 0 and A with 0 ∈A be r.e. open. Then the family h :⊆
FP →2C, h(f) = f(A) for all f ∈H[A, Dε], is r.e. open in H[A, Dε].
We will see below that the above lemma will help to eliminate all those holomor-
phic functions which cannot be Riemann mappings because their images contain
points which they actually shouldn’t. However, to do this, we have to eliminate
all formal power series, which do not deﬁne a holomorphic function on A, ﬁrst.
Lemma 6. Let A be a r.e. open domain A ⊆C and an ε > 0 be given. Then the
set of all f ∈FP which do not determine a function f : A →Dε is r.e. open.
Proof. Let A0, A1, ... be an enumeration of rational balls so that A = 
i Ai and
let f ∈FP be given. We would like to apply standard continuation techniques
(cf., e.g., [8] or [10]) in connection with the Kreiskettenverfahren. However, to
apply this techniques we need to guarantee that f is indeed a function in contrast
to the fact that we want to enumerate all those f which are not functions.
The solution to this problem is quite simple. Let’s assume that f is indeed a
function f : A →Dε, ﬁrst. Using the Riemann inequality we get a bound on the
coeﬃcients of the Taylor series in each point of each Ai. On the other hand, it is
easy to verify that a formal power series whose coeﬃcients are bounded in this
way do indeed deﬁne a function f : A →D2ε. Thus we have simply to assume
that f determines a function f : A →D2ε and apply the Kreiskettenverfahren.
In this way we will eventually realize that f does not deﬁne a suitable function
by testing the size of the coeﬃcients and values. For simply connected domains
A this is already enough to prove the statement of the lemma. For multiply
connected domains, f need not be a function although the above construction
would say that it is a function. This is because f can depend on the chosen
path we use the Kreiskettenverfahren on, i.e., the path along which we continue
f. Notice that we don’t have to consider paths but only ﬁnite sequences of
Ai0 = A0, Ai1, ..., Ais so that Ain ∩Ain+1 ̸= ∅, because such a sequence deﬁnes
uniquely the continuation of the function. Using continuation along these paths
we can enumerate those f which are not functions and prove the lemma even in
the multiply connected case.

622
R. Rettinger
Now, given a computable, r.e. open family g : B →2A, A ⊆C and a computable
sequence (zi)i of points in C, it is easy to see that the set {b | ∃i.zi ∈g(b)} is r.e.
open. Furthermore by testing whether f(z) ̸= f(z′) for diﬀerent z, z′ ∈A one
can guarantee that constant f will never be enumerated. Combining this with
Lemma 5 and Lemma 6 we get the following corollary:
Corollary 3. Let A, B be domains in C with 0 ∈A, B so that
(i) A is r.e. open and
(ii) there exists a computable sequence (zi)i of computable complex numbers
which is dense in the boundary ∂B of B.
Then the class of formal power series f ∈FP so that f does not determine a
holomorphic function f : A →B, is r.e. open.
This is already enough to prove an eﬀective version of Theorem 1. As we are
only considering bounded domains, we can restrict ourselves to a set of suitably
bounded formal power series which we will again denote by FP in the sequel.
Theorem 4. Let U be some bounded, simply connected and r.e. open subset of
C, 0 ∈U, so that there exists a computable sequence of points on the boundary
of U which is dense for this boundary. Then the Riemann mapping f : U →D
of U with f ′(0) > 0 and f(0) = 0 is computable.
Proof. Let NF and NF−denote the sets
NF = {f ∈FP | f ̸∈H(U, D)} and NF−= {f ∈FP | f ̸∈H(D, U)},
respectively. By Corollary 3 these sets are r.e. open. By uniform versions of
Corollary 1 the sets
INV = {(f, g) ∈FP2 | f ∈H(U, D), g ∈H(D, U), f ◦g ̸= id},
INV−= {(f, g) ∈FP2 | f ∈H(U, D), g ∈H(D, U), g ◦f ̸= id},
NN = {f ∈FP | f ∈H(U, D), f(0) ̸= 0 ∨f ′(0) ̸> 0}
are r.e. open in H(U, D) × H(D, U) and H(U, D), respectively. Furthermore
FP2 \ ((NF × NF−) ∪INV ∪INV−∪(NN × FP)) = {(f, f −1)}
where f is the uniquely determined function of Theorem 1. Thus the statement
of the theorem follows by Lemma 4, where we get in addition the inverse of the
Riemann mapping for free.
This theorem even characterize the class of domains for which the Riemann
mapping is computable (cf. [6]).
To get the eﬀective analogue of Theorem 2 we have to determine in addition
the circular slit domain. This seems to be a problem, as the class of these domains
is not even compact. However, relaxing the conditions on this class slightly gives
us a computably compact space. The Riemann mapping will then take care of
the right domain itself:

Compactness and the Eﬀectivity of Uniformization
623
Theorem 5. Let U be some bounded, k-connected and r.e. open subset of C with
0 ∈U so that there exists a computable sequence of points on the boundary of U
which is dense for this boundary. Furthermore let K be the connected component
of ∂U determined by the unbounded connected component C of C \ U.
Then there exists a unique computable set D ∈Dk and a computable bi-
holomorphic mapping f : U →D with f ′(0) > 0, f(0) = 0 and f(K) = ∂D.
Proof. Similar to Dk we deﬁne ˆDk to be the class of sets
D = D \ (ι(γ1, a1, b1) ∪... ∪ι(γk−1, ak−1, bk−1))
with γi ∈∂D, ai ∈[0; 1] and bi ∈[0; 1]. Notice that Dk ⊆ˆDk but there exists
sets in ˆDk with several connected components. However, if we restrict the above
construction to k-connected sets in ˆDk we get essentially Dk. Furthermore, the
class (∂D × [0; 1] × [0; 1])k−1 is computably compact and r.e. open by Lemma 1.
By a uniform version of Corollary 3 and Lemma 3 the family
gNF(D) = {(f, g) | f ̸∈H(U, D) ∨g ̸∈H(D, U)}
is r.e. open. Similarly the families gINV, gINV−and gNNx deﬁned by
gINV(D) = {(f, g) | f ∈H(U, D) ∨g ∈H(D, U), f ◦g ̸= id},
gINV−(D) = {(f, g) | f ∈H(U, D) ∨g ∈H(D, U), g ◦f ̸= id},
gNNx(D) = {(f, g) | f ∈H(U, D), f(0) ̸= 0 ∨f ′(0) ̸> 0}
are open relative to a suitable superset and thus
C =

D × (gNF(D) ∪gINV(D) ∪gINV−(D) ∪gNNx(D))
is r.e. open as well. Then (D, f, f −1) with
{(D, f, f −1)} = (∂D × [0; 1] × [0; 1])k−1 × FP × FP \ C
is computable which proves the theorem. Notice, that D is indeed a domain in
Dk because f is a bi-holomorphic function which preserves k-connectivity.
Finally, the eﬀective versions of Theorem 1 and Theorem 2 can be proven by
well known reductions to the bounded cases given above by using the square
root trick (cf., e.g., [5]). An alternative proof could follow our lines, however,
it seems to be unavoidable to use some deeper results from complex analysis,
e.g., Koebe’s 1/4 Theorem, in this case. Theorem 5 gives in its uniform version
essentially the same result as the main result in [7]. However, the above proof is
slightly more general as we can adopt it to show that we actually do not need
the exact value of the connectedness of the domain.

624
R. Rettinger
4
Uniformization
We will present eﬀective versions of Theorem 3 in this section. We omit detailed
proofs, which are essentially analog to the proofs of Corollary 3 and Theorem 4,
Theorem 5 in the previous section.
Given a ﬁnite sequence τ0, ... τn of maps of a Riemann surface T and a formal
power series f, we can proceed exactly as we have done in the prove of Lemma
6 to continue f to T and exclude the case, that f is not a bi-meromorphic map
onto D. Notice, that we get a diﬀerent mapping fi for each map i. The only thing
we have to test in addition to Lemma 6 is that f indeed deﬁnes a mapping on T ,
i.e., we have to exclude those f so that fi diﬀers on some z from fj at τ −1
j
◦τi(z).
But this can be done exactly as we have done with the other properties. Thus,
in the case that T is deﬁned by ﬁnitely many maps and is equivalent to D the
uniformization mapping of Theorem 3 is computable. In a similar way one can
show computability in the other cases where T is deﬁned by ﬁnitely many maps.
Theorem 6. Let (T, τ) be a ﬁnitely generated, simply connected and computable
Riemann surface. Then there exists computable bi-meromorphic uniformization
functions f : T →D, f : T →C or f : T →C∞.
As a consequence we get the following general result for compact Riemann
surfaces:
Corollary 4. For any computable and compact, simply connected Riemann sur-
face (T, τ) there exists a bi-holomorphic computable function f : T →C.
Such generalizations of Theorem 6 are not true in the hyperbolic or Euclidean case.
However, we can give a characterization similar to the one given in Theorem 4
and 5. To this end let dT denote the hyperbolic metric on T .
Theorem 7. Let (T, τ) be a computable, hyperbolic and simply connected Rie-
mann surface, z0 be a computable element in T and (zi,j)i,j be a computable
sequence in T so that
1. d(zi,j, z0) > j for all i, j ∈N and
2. ∀z ∈T , n ∈N.d(z, z0) > n ⇒∃i, j.j ≥n ∧d(z, zi,j) < 1/n.
Then there exists a computable bi-meromorphic mapping f : T →D.
5
Discussion
We have presented examples for the very powerful and simple principle given
in Lemma 4. Although problems in complex analysis are well suited for this
principle it is likely that this principle is of use even in other ﬁelds.
Furthermore the results given are purely computability results. Using the same
method it seems, however, possible to achieve at least certain space bounds. As
for all three eﬀective versions of conformal mappings there is a lower complexity

Compactness and the Eﬀectivity of Uniformization
625
bound ♯P (cf., e.g., [2]), thus an upper bound PSPACE would be indeed almost
giving the exact complexity. Thus it seems to be worth to investigate even the
complexity aspects of the principle given in Lemma 4.
Finally, most of the presented results can be given in a uniform way. This is
obvious for simply and multiply connected sub-domains of C or, for Riemann
surfaces, in the hyperbolic case. In the spherical and Euclidean case, however,
this seems impossible with the information given by the representations of this
paper.
References
1. Ahlfors, L.: Complex Analysis, 3rd edn. McGraw-Hill Science/Engineering/Math.
(1979)
2. Binder, I., Braverman, M., Yampolsky, M.: On computational complexity of Rie-
mann mapping. Arkiv for Matematik 45(2) (2007)
3. Brattka, V., Presser, G.: Computability on subsets of metric spaces. Theoretical
Computer Science 305, 43–76 (2003)
4. Galatolo,
S., Hoyrup, M., Rojas, C.: Dynamics and abstract
computabil-
ity:computing invariant measures. Discrete Contin. Dyn. Sys. 29(1), 193–212 (2011)
5. Henrici, P.: Applied and computational complex analysis. Wiley Classics Library
Series, vol. 3 (1986)
6. Hertling, P.: An eﬀective Riemann mapping theorem. Theoretical Computer Sci-
ence 219(1-2), 225–265 (1999)
7. Andreev, V., McNicholl, T.: Computing Conformal Maps onto Canonical Slit Do-
mains. In: 6th Int’l Conf. on Computability and Complexity in Analysis. Schloss
Dagstuhl - Leibniz-Zentrum fuer Informatik, Germany (2009)
8. M¨uller, N.: Uniform Computational Complexity of Taylor series. In: Ottmann, T.
(ed.) ICALP 1987. LNCS, vol. 267, pp. 435–444. Springer, Heidelberg (1987)
9. Nehari, Z.: Conformal mapping. McGraw-Hill, New York (1952)
10. Rettinger, R.: Lower Bounds on the Continuation of Holomorphic Functions.
Electr. Notes Theor. Comput. Sci 221, 207–217 (2008)
11. Rettinger, R., Weihrauch, K.: Tychonoﬀfor compact sets is computable (submit-
ted)
12. Weihrauch, K.: Computable Analysis. Springer, Berlin (2000)
13. Weihrauch, K., Grubba, T.: Elementary Computational Topology. Journal of Uni-
versal Computer Science 15(6), 1381–1422 (2009)

On the Computability Power
of Membrane Systems with Controlled Mobility
Shankara Narayanan Krishna1, Bogdan Aman2,3, and Gabriel Ciobanu2,3
1 Department of Computer Science and Engineering, IIT Bombay, Powai, Mumbai
400076, India
krishnas@cse.iitb.ac.in
2 Institute of Computer Science, Romanian Academy, Str. T. Codrescu nr.2,
cod 700481, Iasi, Romania
baman@iit.tuiasi.ro
3 “A.I.Cuza” University of Ia¸si, Bulevardul Carol I, Nr.11, 700506, Iasi, Romania
gabriel@info.uaic.ro
Abstract. In a previous paper we have shown that membrane systems
with controlled mobility are able to solve a ΠP
2 complete problem. Then,
an enriched model with forced endocytosis and forced exocytosis enables
us to move to the fourth level in the polynomial hierarchy, the model
having ΣP
4 ∪ΠP
4 as lower bound. In this paper we study the computabil-
ity power of this model (using forced endocytosis and forced exocyto-
sis), and determine the border condition for achieving computational
completeness: 4 membranes provide Turing completeness, while 3 mem-
branes do not. Moreover, we show that the restricted division operation
(which is crucial in achieving the ΣP
4 ∪ΠP
4 lower bound) does not pro-
vide computational completeness. However, Turing completeness can be
achieved with pairs of operations (exocytosis, inhibitive endocytosis) and
(inhibitive exocytosis, endocytosis) by using 4 membranes. Finally, we
present some computability results expressing that membrane systems
which use the operations of restricted division, restricted exocytosis and
inhibitive endocytosis cannot yield computational completeness.
1
Introduction
The main data structure investigated in membrane computing [12] is the multi-
set, a set of objects with multiplicities associated with its elements. One of the
basic operations on multisets, also corresponding to bio-chemical reactions tak-
ing place in a cell, is multiset rewriting. The evolution of P systems was deﬁned
mainly by multiset rewriting rules over the objects present in the compartments
of the membrane structure, while the structure remained the same during the
evolution/computation [12]. However, from a mathematical and biological point
of view, this is a too restricted case. It is natural to also allow the membrane
structure to evolve by division rules, endocytosis rules (taking a membrane that
is outside a neighbouring membrane and moving it inside) and exocytosis rules
(moving a membrane outside the membrane that contains it).
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 626–635, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

On the Computability Power of Membrane Systems with Controlled Mobility
627
When membrane systems are seen as computing devices, two main research
directions are usually considered: computational power in comparison with the
classical notion of Turing computability, and eﬃciency in algorithmically solv-
ing hard problems (e.g., NP-problems) in polynomial time [12]. In this respect,
membrane systems deﬁne classes of computing devices which are both powerful,
usually equivalent to Turing machines, and eﬃcient.
Polynomial-time solutions to NP-complete problems using P systems with ac-
tive membranes, through the generation of an exponential space, are presented
comprehensively in [13]. Some solutions using systems of mobile membranes
have been proposed recently in [1,8,5]. This paper is a continuation of the at-
tempts made to answer essentially an open question posed in [12]: Is it possible
to eﬃciently solve PSPACE-complete problems . . . without using any of the
operations of non-elementary membrane division . . .? In [8], we solved a ΠP
2
complete problem using P systems with mobile membranes by using elementary
membrane division, endocytosis, exocytosis and inhibitive endocytosis. Recently
we enriched the model in [8], and showed that a restricted use of the operations
endocytosis (endo), exocytosis (exo), forced endocytosis (fendo), forced exocytosis
(fexo), inhibitive endocytosis (iendo) and elementary membrane division (div) can
give a semi-uniform solution of a ΠP
4 complete problem in polynomial time [5].
This is the ﬁrst known result that connects the complexity class ΠP
4 within the P
area. It is not clear whether this model can eﬃciently solve PSPACE-complete
problems, but the fact that it does not use any of the powerful operations men-
tioned in the open question, and is capable of reaching level 4 of the polynomial
hierarchy seems encouraging.
Here, we analyse the computational power of the various operations used in
our model. The interplay between endo, exo, fendo, fexo is quite powerful, and
the computational power of a Turing machine is obtained using twelve mem-
branes [6]. The computability aspects of the diﬀerent forms of mobility in en-
hanced mobile membranes are investigated. Several combinations of operations
that provide Turing completeness are obtained: ten membranes along with fendo,
fexo; nine membranes with endo, exo, pendo; eight membranes with fendo, fexo,
pendo and twelve membranes with endo, exo, fendo give computational complete-
ness. A form of restricted mobility described by some new rules does not lead to
computational completeness.
We conjecture that the operations used to reach level four of the polynomial
hierarchy are not “powerful enough” to yield computational completeness. Other
important results obtained in this paper characterize tight bounds for obtaining
computational completeness. We observe that the results obtained in this paper
use three, four or ﬁve membranes, a major improvement with respect to the
results obtained in [7] where eight, nine or ten membranes are used. It is worth
noting that three is the smallest number of membranes when eﬀectively using
the movement of membranes given by endocytosis and exocytosis.

628
S.N. Krishna, B. Aman, and G. Ciobanu
2
Enhanced Mobile Membranes with Controlled Mobility
In what follows, the set of natural numbers is denoted by N, and V denotes a
ﬁnite alphabet. The free monoid generated by V under the operation of concate-
nation and the empty string denoted by λ, as unit element, is denoted by V ∗.
We assume that the reader is familiar with membrane computing; for the
state of the art, see [12]. The class of mobile membrane systems used here is
an extension of P systems with mobile membranes and controlled mobility [8],
which is a construct Π = (V, H, μ, w1, . . . , wn, R, I), where: n ≥1 (the initial
degree of the system); V is an alphabet (its elements are called objects); H
is a ﬁnite set of labels for membranes; μ is a membrane structure, consisting
of n membranes, labelled with elements of H; w1, w2, . . . , wn are strings over V ,
describing the initial multisets of objects placed in the n membranes of μ, I is
the set of elementary membranes of μ, representing the output membranes of the
system, and R is a ﬁnite set of developmental rules of the following forms, where
membrane h is elementary and membrane m is not necessarily elementary:
(a) [ a]h[ ]m →[[ w] h]m, for h, m ∈H, a ∈V, w ∈V ∗
endocytosis
a membrane labelled h enters the adjacent membrane labelled m under the
control of object a; the labels h and m remain unchanged during this process;
however, the object a is modiﬁed to w during the operation.
(b) [ [a]h] m →[w] h[ ]m, for h, m ∈H, a ∈V, w ∈V ∗
exocytosis
a membrane labelled h is sent out of a membrane labelled m under the control
of object a; the labels of the two membranes remain unchanged; however the
object a from membrane h is modiﬁed during this operation;
(c) [ ] h[a]m →[[ ] hw] m, for h, m ∈H, a ∈V, w ∈V ∗
forced endocytosis
a membrane labelled h enters the adjacent membrane labelled m under the
control of object a of m; the labels h and m remain unchanged during this
process; however, the object a is modiﬁed to w during the operation.
(d) [ a[ ]h]m →[ ]h[ w] m, for h, m ∈H, a ∈V, w ∈V ∗
forced exocytosis
a membrane labelled h is sent out of a membrane labelled m under the control
of object a of m; the labels of the two membranes remain unchanged; however
the object a of membrane m is modiﬁed to w during this operation.
(e) [ a]h[] m/¬S →[[ a]h]m, for h, m ∈H, a ∈V
inhibitive endocytosis
a membrane labelled h containing a can enter m provided m does not contain
any object from S; the object a and the labels h and m of the membranes also
remain unchanged. The objects of S are inhibitors that prevent membrane h
from entering membrane m whenever h contains the object a.
(f) [ a[]h] m/¬S →[]m[a]h, for h, m ∈H, a ∈V
inhibitive exocytosis
a membrane labelled h containing a can exit m provided m does not contain
any object from S; the object a does not evolve in the process; the labels h
and m of the membranes also remain unchanged.
(g) [ a]h →[u]h[ v]h, for h ∈H, a ∈V, u, v ∈V ∗
elementary division
in reaction with an object a, the membrane labelled h is divided into two
membranes labelled h, with the object a replaced in the two new membranes
by possibly new objects; the other objects remain unchanged.

On the Computability Power of Membrane Systems with Controlled Mobility
629
The paper [8] did not consider the operations of forced endocytosis and forced
exocytosis. The rules are applied according to the following principles:
1. All rules are applied in parallel, non-deterministically choosing the rules,
membranes, and objects, but in such a way that the parallelism is maximal
(in each step we apply a multiset of rules such that no further rule can be
added, no further membranes and objects can evolve at the same time).
2. Membrane m is said to be passive, while membrane h is said to be active. In
a computation step, any object or any active membrane can be involved in at
most one rule, but the passive membranes are not considered involved (hence
they can be used by several rules at the same time as passive membranes).
3. The evolution of objects and membranes takes place in a bottom-up manner.
After having a (maximal) multiset of rules chosen, they are applied starting
from the innermost membranes, level by level, up to the skin membrane (all
these sub-steps form a unique evolution step, called a transition step).
4. When a membrane is moved across another membrane by rules (a)-(f), its
whole contents (its objects) are moved.
5. All objects and membranes which do not evolve at a given step, are passed
unchanged to the next conﬁguration of the system.
By using the rules in this way, we get transitions among the conﬁgurations
of the system. A computation is a sequence of transitions that starts from an
initial conﬁguration, such that each two transitions are connected by a common
conﬁguration. A computation is successful if it halts (it reaches a conﬁguration
where no rule can be applied).
At the end of a halting computation, the number of objects in the mem-
branes I is considered to be the result of the computation. A non-halting com-
putation provides no output. The family of all sets of numbers N(Π) which
are obtained as a result of a halting computation by a P system Π with en-
hanced mobile membranes and controlled mobility of degree at most n using
rules α ⊆{exo, endo, fendo, fexo, iendo, rexo, div}, is denoted by NEMCMn(α).
Here iendo, rexo represent inhibitive endocytosis and inhibitive exocytosis, and
div denotes division. When the number of membranes is ﬁnite, and not ﬁxed as
any particular n, we denote it as ∗.
An alternate notion of obtaining a result of a computation is to look at the
language generated by the system. Given I, let fi, i ∈I be the multiset content
of membrane i at the end of a halting computation. Then L(Π) = {⊎i∈Ifi} is the
language associated with Π, namely the union of the multisets in all membranes.
The family of all languages L(Π) which are obtained as a result of a halting com-
putation by a P system Π with enhanced mobile membranes and controlled mo-
bility of degree at most n using rules α ⊆{exo, endo, fendo, fexo, iendo, rexo, div},
is denoted by LEMCMn(α).
When |w| = 1 in rules (a)–(d), we call the operations “restricted”, and use
rendo, rexo, rfendo, rfexo to denote them. Here, r stands for restricted. Likewise,
when |u| = |v| = 1 in rule (g), we denote it by rdiv.
A special case of the use of iendo (rexo) rules is when the inhibitor set is
S = ∅. Then, there are no symbols whose absence is mandatory for the mobility.

630
S.N. Krishna, B. Aman, and G. Ciobanu
However, the thing to note here is that there is no evolution of objects. Thus,
when S = ∅, an iendo rule is written as [a] i[ ] j →[[ a]i]j. Similar for rexo.
3
Computability Power of Controlled Mobility
The language and automata approach is fundamental in introducing comput-
ing devices and investigating their computing power, compared with the power
of Turing machines and other classical models of computation. NRE, NCS,
NOλ, NMAT, and NFIN denote the family of recursively enumerable sets,
context-sensitive sets, ordered sets, sets of matrix grammars without appear-
ance checking and ﬁnite sets of natural numbers, respectively. It is known that
NMATλ, NMATac, NCS, NOλ ⊂NRE [3].
In this section we explore the computability power of the class consisting of
operations rendo, rexo, rfendo, rfexo, iendo, rexo, rdiv. The complexity of this class
can be found in [5], where the ΣP
4 ∪ΠP
4 lower bound is proved.
The next result deﬁnes the border for computational completeness while using
the operations endo, exo, fendo and fexo. Here, we show that four membranes are
suﬃcient for computational completeness by using the fact that each recursively
enumerable language can be generated by a matrix grammar in the strong bi-
nary normal form [4]. This result is tight from the known result in [6]: three
membranes do not give computational completeness.
First we deﬁne the notions used in the proof: a matrix grammar with appear-
ance checking is a tuple G = (N, T, M, S, F) where N, T are sets of non-terminals
and terminals respectively, S is the start symbol, M is a ﬁnite set of matrices
of the form (r1, . . . , rm), m ≥1, with context-free rewriting rules ri : αi →βi,
αi ∈N, βi ∈(N ∪T )∗, and F is a subset of the totality of occurrences of rules
in the matrices of M. For any two strings x, y we say that x ⇒ac y iﬀthere
are strings x0, . . . , xn and a matrix (r1, . . . , rm) ∈M such that x0 = x, xn = y,
and xi−1 = x′
i−1αix′′
i−1, xi = x′
i−1βix′′
i−1 for some x′
i−1, x′′
i−1 ∈(N ∪T )∗, for
all 1 ≤i ≤n −1, or else, the rule αi →βi ∈F, αi is not a subword of
xi−1, and xi−1 = xi. In other words, a direct derivation in G corresponds to
applying the rules of a matrix, in order, skipping the rules of F. We denote by
MATλ
ac the families of languages generated. In case all rules are λ-free, we remove
the superscript λ from the notation. G is in the strong binary normal form, if
N = N1 ∪N2 ∪{S, †}, with these three sets mutually disjoint, two distinguished
symbols B(1), B(2) ∈N2, and the matrices in M of one of the following forms:
1. (S →XA), with X ∈N1, A ∈N2,
2. (X →Y, A →x), with X, Y ∈N1, A ∈N2, x ∈(N2 ∪T )∗, |x| ≤2,
3. (X →Y, B(j) →†), with X, Y ∈N1, j = 1, 2,
4. (X →λ, A →x), with X ∈N1, A ∈N2, x ∈T ∗, |x| ≤2.
There is only one matrix of type 1, and F consists of the rules B(j) →† with
j = 1, 2 appearing in matrices of type 3 († is a trap-symbol, once introduced it
is never removed). A matrix of type 4 is used only once, in the last step of a
derivation. It is proved in [4] that each recursively enumerable language can be
generated by a matrix grammar in the strong binary normal form.

On the Computability Power of Membrane Systems with Controlled Mobility
631
Theorem 1. NEMCM4(endo, exo, fendo, fexo) = NRE.
Proof. We simulate a matrix grammar with appearance checking G
=
(N, T, M, S, F) in the strong binary normal form. We construct the P system
Π = (V, {0, 1, 2, 3}, [ [ ]1 [ ]2 [ ]3]0, ∅, {XA}, ∅, ∅, R, {1, 2, 3}). The symbols
X, A correspond to the initial matrix (S →XA). Let there be n1 matrices of
types 2,4 labelled 1, . . . , n1 and n2 matrices of type-3 labelled n1+1, . . ., n1+n2.
We replace matrices of type-4 with (X →Z, A →x) where Z is a new spe-
cial symbol. V = N ∪T ∪{Yj, Aj | Y ∈N1, A ∈N2, 1 ≤j ≤n1 + n2} ∪
{Y ′, Y (1), Y (2), Y 1, Y 2 | Y ∈N1} ∪{Z, †} ∪{⟨x⟩| x ∈(N2 ∪T )∗, |x| ≤2}. The
rules are:
Simulation of a type-2 matrix mi : (X →Y, A →x)
1. [ X]1[ ]2 →[ [Yi] 1]2, [ [A] 1] 2 →[ Aj]1[ ]2, X ∈N1, A ∈N2
(endo, exo)
provided matrices mi, mj have rules for X, A respectively
2. [ Yk]1[ ] 3 →[ [Yk−1] 1]3, [ [Ak] 1]3 →[Ak−1] 1[ ] 3, k ≥2
(endo, exo)
3. [ Y1]1[ ]3 →[[ Y ′] 1]3, [[ A1] 1]3 →[⟨x⟩] 1[ ]3
(endo, exo)
4. [ [⟨x⟩] 1]3 →[ † ]1[ ]3, [[ Yj]1]3 →[ † ]1[ ]3
(exo, exo)
5. [ Y ′] 1[ ] 3 →[ Y [ ] 3]1, [Aj[ ] 3]1 →[ † ]1[ ]3, j ≥1
(fendo, fexo)
6. [ ⟨x⟩[ ]3] 1 →[ x]1[ ]3
(fexo)
Simulation of a type-3 matrix mi : (X →Y, B(j) →†)
1. [ X]1[ ]2 →[ Y (1)[ ]2] 1 if (X →Y, B(1) →†) is a type-3 matrix
(fendo)
2. [ X]1[ ]3 →[ Y (2)[ ]3] 1 if (X →Y, B(2) →†) is a type-3 matrix
(fendo)
3. [ B(1)[ ]2]1 →[ † ]1[ ] 2, [B(2)[ ]3] 1 →[ † ] 1[ ] 3
(fexo)
4. [ Y (1)] 1[ ]3 →[Y 1Y 2[ ] 3]1, [Y (2)]1[ ]2 →[ Y 1Y 2[ ]2] 1
(fendo)
5. [ Y 1[ ]2] 1 →[λ] 1[ ]2, [ Y 2[ ]3] 1 →[ Y ] 1[ ]3
(fexo)
Termination
1. [ Z]1[ ]2 →[[ λ] 1]2, [ Aj]1[ ]3 →[ † [ ]3]1, A ∈N2
2. [ † ] 1[ ]i →[ [ † ]1]i, [[ † ] 1]i →[ † ] 1[ ] i, i = 2, 3
(endo, exo)
The initial conﬁguration is [[ XA]1[ ]2[ ]3] 0. Assume there is a type-2 matrix
mi : (X →Y, A →x). Then the following rules are used: membrane 1 enters 2
replacing X with Yi, and membrane 1 comes out replacing some A ∈N2 with Aj.
Membrane 1 keeps moving in and out of membrane 3 reducing the indices i, j.
When we have Y1, membrane 1 enters membrane 3, replacing Y1 with Y ′; if
simultaneously we have A1, it is replaced with ⟨x⟩. Consider the case j < i. We
have ⟨x⟩before obtaining Y1. Then, ⟨x⟩is replaced with † when membrane 1
comes out of membrane 3. Suppose j > i. We have Y ′Al in membrane 1, l ≥1.
Then a fendo rule is used, by which membrane 3 enters membrane 1, replacing Y ′
with Y ; Al is replaced with †. The case when X is replaced with Xi, and there
are no symbols A ∈N2 is handled by the rule [[ Yj]1]3 →[ † ]1[ ] 3.
The simulation of a type-3 matrix proceeds as follows: we have [ X]1 in mem-
brane 1. If X corresponds to a type-3 matrix, mi : (X →Y, B(1) →†), then

632
S.N. Krishna, B. Aman, and G. Ciobanu
membrane 2 enters membrane 1 using a fendo rule, replacing X with Y (1). If
there is a B(1) present in membrane 1, then membrane 2 replaces this with a †
triggering an inﬁnite computation; in parallel, membrane 3 enters membrane 1
replacing Y (1) with Y 1Y 2. Consider the two possibilities : (1) we have the conﬁg-
uration [ [Y 1Y 2[ ] 2[ ]3] 1]0 (hence, B(1) is absent). In this case, membranes 2, 3
leave membrane 1: Y 1 is erased, while Y 2 is replaced with Y . (2) we have the
conﬁguration [[ Y 1Y 2 † [ ] 3]1[ ]2] 0 (hence, B(1) was present). In this case, mem-
brane 3 leaves membrane 1 replacing Y 2 with Y , but this does not matter, since
we are anyway getting into an inﬁnite computation. The case when X is part of
the matrix (X →Y, B(2) →†) is symmetric; membranes 2, 3 exchange roles.
To terminate, we ﬁrst simulate a type-4 matrix, at the end of which we obtain
a Z in membrane 1. This Z is erased when membrane 1 enters membrane 2.
Note that if there are symbols of N2 present, then membrane 1 will come out
of membrane 2 replacing A ∈N2 with Aj. In this case, membrane 3 enters
membrane 1 replacing Aj with †. Note that this is the only applicable rule in
this case. In the case when membrane 1 has no symbols of N2, it remains trapped
inside membrane 2, the contents of membrane 1 giving the output.
⊓⊔
We shall now show that using inhibitive endocytosis and exocytosis, it is possible
to obtain computational completeness with 4 membranes.
Theorem 2.
1. NEMCM4(iendo, exo) = NRE,
2. NEMCM4(endo, rexo) = NRE.
Remark 1 Note that the semantics of the rules iendo, exo allow a membrane i to
enter membrane j using an iendo rule [a] i[ ]j\¬S →[ [a]i]j, and then membrane i
exits membrane j using a rule [[ b]i]j →[ w] i[ ]j, b ̸= a.
In what follows we consider a restricted use of the rules iendo, exo: if a mem-
brane i enters membrane j using an iendo rule [ a]i[ ] j\¬S →[[ a]i] j in a step
k ≥1, then during step k +1, membrane i comes out of membrane j replacing the
same symbol a with some w. We call this a restricted semantics of iendo, exo. It is
clear that by enforcing this restricted semantics, Theorem 2 still remains valid.
We denote by NEMCMr
n(iendo, exo) the families of numbers N(Π) computed
by systems Π under the restricted semantics of iendo, exo. The next theorem
says that such systems with 3 membranes are strictly less than RE, by using
ordered grammars that are known to be less than RE. An ordered grammar is a
quadruple G = (N, T, S, P) where N is ﬁnite set of non-terminals, T is a ﬁnite
set of terminals, S ∈N is the start symbol and P is a ﬁnite partially ordered
set of context-free production rules of the form A →x, A ∈N, x ∈(N ∪T )∗.
For x, y ∈(N ∪T )∗, we say that x directly derives y, written x ⇒y iﬀthere is
a production p : A →w ∈P such that x = x′Ax′′, y = x′wx′′, and there is no
production q : B →v ∈P such that p ≺q and B occurs in x. The generated
language is L(G) = {w | w ∈T ∗, S ⇒∗w}. The family of languages generated
is denoted Oλ. When λ-free rules are used, we remove λ from the notation.
Theorem 3.
1. NEMCMr
4(iendo, exo) = NRE,
2. NEMCMr
3(iendo, exo) ⊆NOλ ⊂NRE.

On the Computability Power of Membrane Systems with Controlled Mobility
633
Proof.
1. Follows from Theorem 2 and Remark 1 above.
2. Given a P system with 3 membranes Π = (V, {1, 2, 3}, μ, w1, w2, w3, R, I),
we construct an ordered grammar G that simulates Π such that N(Π) =
N(L(G)). Without loss of generality, assume that μ = [ [[ ]1]2] 3. Deﬁne
G = (N, T, S, P) with N = V ∪V1 ∪V2 ∪V ′ ∪V 1 ∪V2 ∪{f in
r,ai | i = 1, 2, r ∈
R} ∪{†}, T = V1. Here, Vi = {ai | a ∈V }, V i = {ai | a ∈V }, i = 1, 2 and
V ′ = {a′ | a ∈V }. Let the rules of R be labelled r1, . . . , rn. The production
rules of P are as follows:
(a) p1 : S →a′x1w2, with w1 = ax1, and exists an exo rule [ [a] 1]2 →[y]1[ ] 2
(b) p2 : a′ →y1
(c) p3 : S →w1 if none of the symbols of w1 have exo rules in R
(d) pr,a,i : ai →f in
r,ai, such that r : [a]i[ ] j\¬X →[ [a] i] j ∈R, i ∈{1, 2}
(e) p′
b,r,j : bj →† for b ∈X, given r ∈R as above
(f) p′′
a,i : f in
r,ai →yi if there is an exo rule [ [a] i] j →[y]i[ ] j, i, j ∈{1, 2}
(g) pa,i : ai →ai if there is no iendo rule [a] i[ ]j\¬X →[ [a] i] j ∈R, i ∈{1, 2}.
(h) pt,a,2 : a2 →λ
with order p′
b,r,j ≻pr,a,i, pr,a,i ≻pa,i, p′′
a,i ≻pr,b,j for i, j ∈{1, 2} and a, b ∈V .
Let w1, w2, w3 be the contents of membranes 1, 2 and 3. Since the initial conﬁg-
uration is [[ [w1] 1w2] 2w3]3, the computation begins with an exo rule, simulated
with the rules p1, p2. Any of the symbols having an exo rule are picked up in
membrane 1, primed and replaced using p2. Here on, the restrictive semantics
comes into play. The conﬁguration after the ﬁrst exo rule is [ [ y1x1] 1[w2]2w3] 3.
One of the membranes 1, 2 can enter the other using an iendo rule. If a mem-
brane (assume membrane 1) enters the other (membrane 2) using an iendo rule r
involving a, then in the next step, a is replaced using an exo rule due to the re-
strictive semantics of iendo, exo. Rule pr,a,1 models the entry of membrane 1 into
membrane 2. Note that if the inhibitor set X in rule r has symbols b1, . . . , bk, then
the rules p′
bl
2,r,2 all have a greater priority than pr,a,1; 1 ≤l ≤k. That is, if any of
the symbols b1, . . . , bk are present in membrane 2, then the iendo rule cannot be
used. Note that when pr,a,1 is used, then certainly, none of the symbols b1, . . . , bk
are present in membrane 2, and a1 is replaced with f in
r,a1. This denotes the fact
that membrane 1 is “inside” membrane 2, using rule r, involving symbol a. The
only next applicable rule is p′′
a,1 simulating the exo rule replacing this symbol a.
Note the order which enforces this: in the presence of a symbol f in
r,ai, no other f in
q,bj
can be introduced before f in
r,ai is rewritten. This is continued until we reach a con-
ﬁguration which is the halting conﬁguration.
Note that by the deﬁnition of restrictive semantics, if we are in a conﬁguration
[[ [yi]ixj] jw3] 3 that is not the initial conﬁguration, then this cannot be halting
(since we achieved this conﬁguration using an iendo rule in the previous step).
Thus, the halting conﬁguration can only be [ [y1] 1[ x2] 2w3]3. To simulate halting,
we use rules pa,i for i = 1, 2. Note the order pr,a,i ≻pa,i prevents replacing
symbols ai with ai if an iendo rule is applicable. Thus, we obtain a string over
V 1∪V 2 only when we reach a halting conﬁguration. This is followed by rule pt,a,2
which erases all symbols of membrane 2 at the end of a halting conﬁguration.

634
S.N. Krishna, B. Aman, and G. Ciobanu
In case the initial conﬁguration is [ [w1]1[ w2]2w3]3, then the initial rule will
be S →f in
r,a1x1w2 with w1 = ax1 or S →f in
r,a2x2w1 with w2 = ax2. The erasing
rule pt,a,2 will not be there in this case.
⊓⊔
Open problem It is known that NEMCMr
3(iendo, exo) ⊆NEMCM3(iendo, exo) ⊆
NEMCMr
4(iendo, exo) = NRE. Which inclusions are strict? What is the power
of EMCM3(iendo, exo)? An interesting class is EMCM3(iendo, exo)—on the one
hand, it seems very diﬃcult to simulate a matrix grammar with appearance
checking/ two counter machine, since we have just two membranes to play with;
on the other hand, the inherent appearance checking in the iendo rules, cou-
pled with the general (erasing) nature of the exo rules make it diﬃcult to ﬁnd
mechanisms less than RE to simulate these.
A related question is the power of NEMCM∗(iendo, exo−λ) where exo−λ stands
for exocytosis rules where a ∈V is replaced with w ∈V +. If we assume the ini-
tial membrane structure μ = [[ ]1[ ]2 . . . [ ]n−1] n, then it can be shown that
NEMCM∗(iendo, exo−λ) ⊆NMATac ⊂NRE. The intuition behind this state-
ment is: keep a ﬂag to remember the current membrane structure, have a matrix
of rules, using appearance checking to simulate iendo rules. No erasing is needed,
since I = {1, . . . , n−1}. A quotient of the computed language with respect to the
ﬂag completes the simulation. However, for a general μ, the computational power
is not clear, since I could be a proper subset of {1, . . . , n}, and the contents of
membranes in {1, . . . , n}\I have to be erased at the end.
We show that restrictive mobility is not powerful without rdiv (Theorem 4),
while rdiv oﬀers limited power in the absence of restrictive mobility (Theorem 5).
Theorem 4. NEMCM∗(rendo, rexo, rfendo, rfexo, iendo, rexo) ⊆NFIN.
Theorem 5. NEMCM∗(rdiv) ⊆NCS.
The
operations
used
to
establish
the
Σp
4
∪Πp
4
lower
bound
are
{rdiv, rendo, rexo, rfendo, rfexo, iendo}. Using Theorems 4 and 5, we conjecture
that NEMCM∗(rdiv, rendo, rexo, rfendo, rfexo, iendo, rexo) ⊂NRE. Looking at
the proof of Theorem 2, one could think of replacing exo rules using rdiv, while
considering matrix grammars (with or without appearance checking) to simulate
the rules A →x, (|x| ≤2 helps). However, this creates extra copies of symbols,
which have to be done away with, and the lack of erasing rules poses a problem.
The next theorem puts together the restricted division with restricted mobility,
and compare them with NMATλ and ET0L.
An E0L system is a context-free pure grammar with parallel derivations: G =
(V, T, ω, R) where V is an alphabet, T ⊆V is a terminal alphabet, ω ∈V ∗is an
axiom, and R = {a →v | a ∈V and v ∈V ∗} is a ﬁnite set of rules such that
for each a ∈V there is at least one rule a →v in R. For w1, w2 ∈V ∗, we say
that w1 ⇒w2 if w1 = a1 . . . an, w2 = v1 . . . vn for ai →vi ∈R, 1 ≤i ≤n. An
ET0L system is a construct G = (V, T, ω, R1, . . . Rn) such that each quadruple
(V, T, ω, Ri) is an E0L system. The generated language is deﬁned as L(G) = {x ∈
T ∗| ω ⇒Rj1 w1 ⇒Rj2 . . . ⇒Rjm wm = x}, where m ≥0, 1 ≤ji ≤n, 1 ≤i ≤m.
Theorem 6.
1. NEMCM4(rendo, rexo, rdiv) −NMATλ ̸= ∅,
2. LEMCM5(rendo, rexo, rdiv) −ET0L ̸= ∅.

On the Computability Power of Membrane Systems with Controlled Mobility
635
4
Conclusion
The results presented in this paper use a possibly minimal number of membranes
(four) to achieve Turing completeness with pairs of operations (exocytosis, in-
hibitive endocytosis) and (inhibitive exocytosis, endocytosis). This is a major
improvement with respect to the results obtained in [7] where nine membrane
are used. We also present some computability results expressing that membrane
systems using the operations of restricted division, restricted exocytosis and in-
hibitive endocytosis cannot yield computational completeness, deﬁning the fron-
tier between RE and non-RE for controlled mobility in membrane computing.
Acknowledgements. The work of Bogdan Aman and Gabriel Ciobanu was
supported by a grant of the Romanian National Authority for Scientiﬁc Research,
CNCS-UEFISCDI, project number PN-II-ID-PCE-2011-3-0919.
References
1. Aman, B., Ciobanu, G.: Solving a Weak NP-complete Problem in Polynomial Time
by Using Mutual Mobile Membrane Systems. Acta Informatica 48(7-8), 409–415
(2011)
2. Aman, B., Ciobanu, G.: Mobility in Process Calculi and Natural Computing.
Springer (2011)
3. Dassow, J., P˘aun, G.: Regulated Rewriting in Formal Language Theory (1989)
4. Freund, R., P˘aun, G.: On the Number of Non-terminal Symbols in Graph-
Controlled, Programmed and Matrix Grammars. In: Margenstern, M., Rogozhin,
Y. (eds.) MCU 2001. LNCS, vol. 2055, pp. 214–225. Springer, Heidelberg (2001)
5. Krishna, S.N., Aman, B., Ciobanu, G.: Solving the 4QBF Problem in Polynomial
Time by Using the Biological-Inspired Mobility (preprint)
6. Krishna, S.N., Ciobanu, G.: On the Computational Power of Enhanced Mobile
Membranes. In: Beckmann, A., Dimitracopoulos, C., L¨owe, B. (eds.) CiE 2008.
LNCS, vol. 5028, pp. 326–335. Springer, Heidelberg (2008)
7. Krishna, S.N., Ciobanu, G.: Computability Power of Mobility in Enhanced Mobile
Membranes. In: L¨owe, B., Normann, D., Soskov, I., Soskova, A. (eds.) CiE 2011.
LNCS, vol. 6735, pp. 160–170. Springer, Heidelberg (2011)
8. Krishna, S.N., Ciobanu, G.: A ΣP
2 ∪ΠP
2 Lower Bound Using Mobile Membranes. In:
Holzer, M. (ed.) DCFS 2011. LNCS, vol. 6808, pp. 275–288. Springer, Heidelberg
(2011)
9. Krishna, S.N., P˘aun, G.: P Systems with Mobile Membranes. Natural Computing 4,
255–274 (2005)
10. Minsky, M.L.: Computation: Finite and Inﬁnite Machines. Prentice-Hall (1967)
11. P˘aun, G.: P Systems with Active Membranes: Attacking NP-Complete Problems.
Journal of Automata, Languages and Combinatorics 6(1), 75–90 (2001)
12. P˘aun, G., Rozenberg, G., Salomaa, A. (eds.): The Oxford Handbook of Membrane
Computing. Oxford University Press (2010)
13. P´erez-Jim´enez,
M.J.,
Riscos-N´u˜nez,
A.,
Romero-Jim´enez,
A.,
Woods,
D.:
Complexity-Membrane Division, Membrane Creation. In: [12], pp. 302–336
14. Rozenberg, G., Salomaa, A. (eds.): The Mathematical Theory of L Systems. Aca-
demic Press (1980)

On Shift Spaces with Algebraic Structure
Ville Salo and Ilkka T¨orm¨a
Department of Mathematics, Turun Yliopisto, 20014 Turku, Finland
{vosalo,iatorm}@utu.fi
Abstract. We investigate subshifts with a general algebraic structure
and cellular automata on them, with an emphasis on (order-theoretic)
lattices. Our main results concern the characterization of Boolean al-
gebraic subshifts, conditions for algebraic subshifts to be recoded into
cellwise algebras and the limit dynamics of homomorphic cellular au-
tomata on lattice subshifts.
Keywords: subshifts, algebraic structure, lattices, cellular automata.
1
Introduction
There has been considerable interest in the connections between groups and
symbolic dynamics in the literature. Linear cellular automata have been inves-
tigated extensively [5] [3] [10] [11] [6], and some authors [12] [7] [1] have looked
into subshifts with a group structure. Since the study of linear automata and
group subshifts has proven fruitful, it seems natural to consider whether other
algebraic structures produce interesting behavior in subshifts and CA. This pa-
per is, as far as we are aware, the ﬁrst foray in this direction, with an emphasis
on order-theoretic lattices.
In the ﬁrst section, we look at the case of giving a subshift the structure of
an order-theoretic lattice, with the operations deﬁned cellwise. We characterize
all such subshifts in terms of the local rules that deﬁne them. In particular,
we ﬁnd that a result of Kitchens [7] for group subshifts also holds for Boolean
algebras, namely that they are, up to conjugacy, products of a full shift and a
ﬁnite subshift. In our case, the conjugacy can even be made algebraic.
In the second section, we ﬁnd necessary and suﬃcient conditions for when
an algebra operation can be recoded to one deﬁned cellwise. The conditions are
stated in terms of the aﬃne maps of the algebra, that is, functions built from the
algebra operations. We prove that the recoding can be done when all the maps
are ‘local’, in the sense that their radius is bounded as cellular automata. Many
natural classes of algebras, in particular groups and Boolean algebras, always
satisfy this condition.
In the third section, we consider cellular automata that are also algebra ho-
momorphisms, in the case that the algebra has the so-called congruence-product
property (lattices exhibit this property). We prove that all such CA are stable,
and the limit set is conjugate to a product of full shifts and ﬁnite shifts.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 636–645, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

On Shift Spaces with Algebraic Structure
637
2
Deﬁnitions
If S is a ﬁnite set, the alphabet, the space S
  is called the full shift over S. If x ∈
S
 , we denote the ith coordinate of x with xi, and abbreviate xixi+1 · · · xi+n−1
by x[i,i+n−1]. For a word w ∈Sn, we say that w occurs in x, if there exists i
such that w = x[i,i+n−1]. On S
  we deﬁne the shift map σS (or simply σ, if S is
clear from the context) by σS(x)i = xi+1 for all i.
We assume a basic acquaintance with symbolic dynamics, in particular the
notions of subshift, forbidden word, SFT, soﬁc shift, mixingness, block map and
cellular automaton. A clear exposition can be found in [9]. The set of words of
length n occurring in a subshift X is denoted by Bn(X).
Let T be a set of pairs (f, n), where f is a symbol and n ∈
 , with the
property that (f, m), (f, n) ∈T ⇒m = n. Here, n is called the arity of f. An
algebra of type T is a pair (S, F), where S is a set and for each (f, n) ∈T we
have a function f ′ : Sn →S in the set F. In this case, we identify f ′ with f, and
the functions f are called algebra operations. We usually identify (S, F) with S,
if F is clear from the context.
A variety of type T deﬁned by the set of identities I is the class of algebras
of type T that satisfy the identities in I. We do not deﬁne identities rigorously,
but are satisﬁed with an intuitive presentation (see the examples below). If F
is a variety, (S, F) ∈F and R ⊂S is closed under the operations of S, then we
call (R, F) a subalgebra of S. The set of subalgebras of S is denoted Sub(S). A
function g : S →R between two algebras in F is called a homomorphism or an F-
morphism if g(f(s1, . . . , sn)) = f(g(s1), . . . , g(sn)) for each n-ary operation f. If
g is bijective, it is called an isomorphism. The direct product of an indexed family
(Si)i∈I of algebras is the algebra 
i∈I Si, where the operations are deﬁned
cellwise (f(s1, . . . , sn)i = f(s1
i , . . . , sn
i )). An algebra is directly indecomposable,
if it is not isomorphic to a product of two nontrivial algebras. All ﬁnite algebras
are isomorphic to a ﬁnite product of directly indecomposable ﬁnite algebras. All
varieties are closed under subalgebras, homomorphic images and products [2].
If ∼⊂S × S is an equivalence relation that satisﬁes
s1 ∼t1, . . . , sn ∼tn =⇒f(s1, . . . , sn) ∼f(t1, . . . , tn)
for all si, ti ∈S and all n-ary operations f, we say that ∼is a congruence on S.
The set of congruences on an algebra S is denoted Con(S). A natural algebraic
structure is induced by the algebra operations on the set of equivalence classes
S/ ∼, the kernel ker(g) = {(s, t) ∈S × S | g(s) = g(t)} of a homomorphism g
is always a congruence, and S/ ker(g) is isomorphic to g(S) [2] (the last claim is
known as the Homomorphism Theorem).
The variety of lattices has type {(∧, 2), (∨, 2)} and is deﬁned by the identities
x ∧x ≈x, x ∧y ≈y ∧x ,
(x ∧y) ∧z ≈x ∧(y ∧z), x ∧(x ∨y) ≈x ,
and the same identities with ∧and ∨interchanged (their dual versions). The
operations ∧and ∨are called meet and join, respectively. It is known that the

638
V. Salo and I. T¨orm¨a
variety of lattices coincides with the class of partially ordered sets where all
pairs of elements have suprema and inﬁma, where the correspondence is given
by x ∧y = inf{x, y} and x ∨y = sup{x, y}. If S is a lattice and a, b ∈S, then we
denote [a, b] = {c ∈S | a ≤c ≤b}, where ≤is the partial order of S. A lattice S
is modular if it satisﬁes the additional constraint that whenever a, b, c ∈S and
a ≤b, then a ∨(b ∧c) = b ∧(a ∨c). The variety of distributive lattices satisﬁes
the lattice identities and the additional identity
x ∨(y ∧z) = (x ∨y) ∧(x ∨z)
and its dual version. All distributive lattices are modular [2]. Of particular in-
terest is the binary lattice
 containing the elements {0, 1} with their usual
numerical order.
The variety of Boolean algebras has type {(∧, 2), (∨, 2), (¯·, 1), (1, 0), (0, 0)}. A
Boolean algebra is a distributive lattice w.r.t. ∧and ∨, and also satisﬁes
x ∧0 ≈0, x ∨1 ≈1 ,
x ∧¯x ≈0, x ∨¯x ≈1 .
It is known that every ﬁnite Boolean algebra is isomorphic to the algebra of
subsets 2T of some set T where the ordering is given by set inclusion [2].
Let F be a variety of algebras. We call X ⊂S
  an F-subshift, if it is a
subshift and has an algebra structure in F whose operations are block maps.
In particular, every ﬁnite S ∈F induces a natural cellwise algebra structure
on S
  by f(x1, . . . , xn)i = f(x1
i , . . . , xn
i ) for all i (that is, S
  is taken to be
a direct product), and in this case, if X ⊂S
  is a subshift in Sub(S
 ), it is
called a cellwise F-subshift. A conjucacy that is also an F-morphism is called
algebraic. A cellular automaton with state set S that is also an F-morphism is
called F-linear.
3
Cellwise Lattice Subshifts
In this section, let S be a ﬁnite lattice ordered by ≤.
We begin by giving a characterization for the cellwise lattice subshifts.
Deﬁnition 1. Let X ⊂S
  be a subshift and a ∈S. For such X we deﬁne ma
and M a by
∀i ∈
 : ma
i =

x∈X
x0≥a
xi, M a
i =

x∈X
x0≤a
xi .
It is easy to see that a ≤b implies ma ≤mb and M a ≤M b.
Lemma 1. Let X ⊂S
  be a subshift with B1(X) = S. The following are
equivalent:
– X is a cellwise lattice subshift.
– For all x ∈S
 , we have x ∈X iﬀx ≥σ−i(mxi) holds for all i.
– For all x ∈S
 , we have x ∈X iﬀx ≤σ−i(M xi) holds for all i.

On Shift Spaces with Algebraic Structure
639
Proof. Assume ﬁrst that X is a cellwise lattice subshift. Let a ∈S, and for all
i ∈
, let xi ∈X such that xi
i = ma
i . Such xi exist, since X ∈Sub(S
 ) and has
alphabet S. Then the sequence n
i=−n xi approaches ma as n grows, so ma ∈X.
The second condition is then necessary by the deﬁnition of the mxi, and it is
suﬃcient since the sequence n
i=−n σ−i(mxi) approaches x as n grows.
Assume then that X /∈Sub(S
 ). Suppose ﬁrst that there exist x, y ∈X
with x ∧y /∈X, and let ai = xi ∧yi. Now, for all i, we have that x ∧y ≥
σ−i(mxi) ∧σ−i(myi) ≥σ−i(mai), so that the second condition does not hold.
Suppose then that x ∨y /∈X, and let ai = xi ∨yi. Then we have x ∨y ≥
σ−i(mxi) and x ∨y ≥σ−i(myi), so that x ∨y ≥σ−i(mai), and again the second
condition fails.
The third condition is dual to the second, and is proven similarly.
⊓⊔
Remark 1. It can be proven that a cellwise lattice subshift X is soﬁc if and only
if all the ma are eventually periodic in both directions (and the dual claim for
M a holds as well).
Lemma 2. A nontrivial subshift X ⊂

  is a cellwise lattice subshift if and only
if one of the following holds:
– X = S
 .
– X = {x ∈S
  | x = σn(x)} for some n.
– X = {x ∈S
  | ∀i ∈
, p ∈P : xi = 1 ⇒xi+p = 1} for a ﬁnite set P ⊂
 .
– X = {x ∈S
  | ∀i ∈
, p ∈P : xi = 1 ⇒xi−p = 1} for a ﬁnite set P ⊂
 .
Proof. Let X be a cellwise lattice subshift, and let K = {i ∈
 | m1
i = 1}. If
K = {0}, then X = S
 , so suppose that K ̸= {0}. From Lemma 1 and the fact
that m1 ∈X it follows that K is a subsemigroup of
, i.e.,
{a1n1 + · · · + aknk | ai ∈
 , ni ∈K} ⊂K .
A standard pigeonhole argument then shows that K is generated by some ﬁnite
set P ⊂
. The third and fourth items correspond to the cases P ⊂
  and
−P ⊂
 , respectively.
If P contains both positive and negative elements, let n = gcd P. An appli-
cation of Bezout’s identity then shows that K = {pn | p ∈
}, and the second
case follows.
⊓⊔
Corollary 1. Every cellwise lattice subshift of

  is an SFT.
Corollary 2. If X is also closed under complementation in Lemma 2, only the
ﬁrst two cases can occur.
Proof. Assume the contrary, say case 3. Then X contains the point x = ∞01∞.
But now ¯x = ∞10∞∈X, and thus m1 = ∞010∞.
⊓⊔
We have now completely characterized all binary Boolean algebraic subshifts.

640
V. Salo and I. T¨orm¨a
Deﬁnition 2. Let S = 2T be a ﬁnite Boolean algebra and X ⊂S
  a Boolean
algebraic subshift. For each t ∈T , deﬁne the block map πt : X →{0, 1}
  by
πt(x)i =

1, if t ∈xi
0, if t /∈xi
.
If R ⊂T , we also deﬁne πR : X →({0, 1}R)
  by πR(x) = (πt(x))t∈R.
Note that each πt and πR above is also a homomorphism of Boolean algebras,
and that πT is an injective block map from X to ({0, 1}T)
 .
Deﬁnition 3. Let S = 2T be a ﬁnite Boolean algebra. A subshift X ⊂S
  is
called simple if there exists a set R ⊂T such that the following conditions hold.
– For every t ∈T , we have either πt(X) = {x ∈

  | x = σn(x)} for some
n ∈
 , or πt(X) =

 .
– For all t ∈T , there exists r ∈R and k ∈
 with πt = σk ◦πr.
– The elements of R are independent in the sense that if r ∈R, x ∈πr(X) and
y ∈πR−{r}(X), then there exists z ∈X with πr(z) = x and πR−{r}(z) = y.
Informally, a subshift is simple if it is essentially a product of full shifts and
periodic subshifts. Note that in particular, such a subshift is conjugate to a
product of a full shift and a ﬁnite shift. Clearly, all simple shifts are cellwise
Boolean algebraic subshifts. The converse also holds:
Theorem 1. Let S = 2T be a ﬁnite Boolean algebra and X ⊂S
  a subshift
with B1(X) = S. If X ∈Sub(S
 ), then X is simple.
Proof. Since each πt(X) is a cellwise Boolean algebraic subshift over
, the ﬁrst
condition follows from Corollary 2.
Deﬁne an equivalence relation ∼on T by t ∼s if πt = σk ◦πs for some k ∈
.
Let R be a set of representatives for the equivalence classes of ∼. Then for this
R, the second condition holds.
Consider then the conﬁguration m{r} for some r ∈R. If t ∈m{r}
i
for any t ∈T
and i ∈
, then πt(x) ≥σ−i(πr(x)), and by complementing, πt(x) ≤σ−i(πr(x)),
holds for all x ∈X. The converse is also true, which means that R ∩m{r}
i
⊂{r}
for all i. The third condition then follows from Lemma 1, since X is also a
cellwise lattice subshift.
⊓⊔
Example 1. Let S be a lattice with at least 3 elements, and let a ̸= 1 be a least
successor of 0. Consider the rule ‘if xi = 1, then xi±2j ≥a for all j ∈
 ’. By
Lemma 1, the subshift generated by this rule forms a lattice. It is easy to see
that this subshift is mixing, but is not even soﬁc.
4
General Algebraic Subshifts
In this section, F is a variety of algebras. We now consider F-subshifts X that
are not necessarily cellwise. One way to deﬁne such general shifts is to use a
cellwise F-subshift Y and a conjugacy φ : X →Y , and deﬁne
f(x1, . . . , xn) = φ−1(f(φ(x1), . . . , φ(xn)))

On Shift Spaces with Algebraic Structure
641
for all n-ary algebra operations f of F. Clearly, φ now becomes an algebraic con-
jugacy. The main theorems in this section address the issue of deciding whether
a given F-subshift is algebraically conjugate to some cellwise F-subshift.
Deﬁnition 4. An aﬃne map of an algebra S is inductively deﬁned as either
t(ξ) = ξ (the identity map), t(ξ) = a for some a ∈S (the constant map) or
t(ξ) = f(a1, . . . , an), where f is an n-ary operation, one of the ai is an aﬃne
map and the rest are constants aj ∈S. Here, ξ /∈S is used as a variable. To each
aﬃne map t we also associate a function Eval(t) : S →S by replacing ξ with
the function argument and evaluating the resulting expression. We may denote
Eval(t)(a) by t(a) if a ∈S. The set of aﬃne maps of S is denoted by Af(S).
For example, in the ring
, the term t(ξ) = 2 · (3 + (ξ · (−4))) is an aﬃne map,
and Eval(t)(i) = −8i + 6 for all i ∈
.
The following is a dynamical characterization of F-subshifts that are cellwise
up to algebraic conjugacy. The proof uses a common recoding technique found,
for example, in the Recoding Construction 4.3.1 of [8].
Theorem 2. Let X ⊂S
  be an F-subshift. Then there exists a cellwise F-
subshift Y and an algebraic conjugacy φ : X →Y if and only if there is an r
such that for all t ∈Af(X), the block map Eval(t) has radius at most r.
Proof. Suppose ﬁrst that such an r exists. Then, we can also meaningfully apply
an aﬃne map t ∈Af(X) to a word w ∈B2r+1(X) rooted at the origin by
extending it arbitrarily to a conﬁguration x ∈X, and taking the center cell of
t(x). We deﬁne the following equivalence relation on B2r+1(X):
∀v, w ∈B2r+1(X) : v ∼w ⇐⇒∀t ∈Af(X) : t(v)0 = t(w)0 .
Note that, in particular, v ∼w =⇒v0 = w0. We deﬁne an injective block map
ψ : X →(B2r+1(X)/ ∼)
  by ψ(x)i = x[i−r,i+r]/ ∼, and denote Y = ψ(X). We
denote the obtained conjugacy by φ : X →Y .
In order to make the algebra operations commute with φ, we deﬁne
f(y1, . . . , yn) = φ(f(φ−1(y1), . . . , φ−1(yn)))
for all n-ary algebra operations f, which is obviously well-deﬁned. Now φ extends
to a bijection between Af(X) and Af(Y ) in a natural way. Let us show that every
algebra operation f is then deﬁned cellwise in Y . Consider two points y, y′ ∈Y
with y0 = y′
0. We need to show that φ(t)(y)0 = φ(t)(y′)0 for all φ(t) ∈Af(Y ).
Assume the contrary, that φ(t)(y)0 ̸= φ(t)(y′)0 for some φ(t) ∈Af(Y ). Let
x = φ−1(y) and x′ = φ−1(y′). Then also t(x)[−r,r] = v ̸∼w = t(x′)[−r,r], and
thus there exists t′ ∈Af(X) such that t′(v) ̸= t′(w). But by the assumption on
aﬃne maps, t′′ = t′ ◦t has radius r. Now we have t′′(x)0 ̸= t′′(x′)0, which is a
contradiction, since x[−r,r] ∼x′
[−r,r].
For the converse, note that if X is algebraically conjugate to a cellwise F-
subshift Y via the conjugacy φ, then the radius of every translate is at most the
sum of the radii of φ and φ−1.
⊓⊔

642
V. Salo and I. T¨orm¨a
We also obtain a suﬃcient algebraic condition for algebraic conjugacy with a
cellwise F-subshift. In the special case of the full shift, this becomes a charac-
terization. We start with a deﬁnition.
Deﬁnition 5. We deﬁne the depth of an aﬃne map as the number of nested
algebra operations in it. We say an algebra S is k-shallow if for every t ∈Af(S)
there exists t′ ∈Af(S) of depth at most k such that Eval(t) = Eval(t′).
Theorem 3. Let X ⊂S
  be an F-subshift. If X is k-shallow, then it is cellwise
up to algebraic conjugacy. If X is algebraically conjugate to R
  where R ∈F,
then X is k-shallow for some k.
Proof. If X is k-shallow, then clearly all aﬃne maps have uniformly bounded
radii, and Theorem 2 gives the result.
For the other claim, it suﬃces to show that R
  is k-shallow for some k. Since R
is ﬁnite, the set Γ = {Eval(t) | t ∈Af(R)} is ﬁnite. For an aﬃne map t ∈Af(R
 )
we denote by ti the aﬃne map in Af(R) that t computes in coordinate i. For
each aﬃne map t ∈Af(R
 ) we deﬁne Δt = {Eval(ti) | i ∈
}. Let n = |Γ| and
note that since Rn is ﬁnite, it is k-shallow for some k.
Let t ∈Af(R
 ) and let j1, . . . , jn be coordinates such that for all h ∈Δt we
have Eval(tji) = h for some i. Construct an aﬃne map s ∈Af(Rn) by si = tji.
Since Rn is k-shallow we ﬁnd some aﬃne map s′ ∈Af(Rn) of depth at most k
with Eval(s) = Eval(s′). We may now deﬁne an aﬃne map t′ ∈Af(R
 ) by t′
i =
s′
j′
i for all i, where j′
i is such that Eval(ti) = Eval(sj′
i). Since Eval(sj) = Eval(s′
j)
for all j, we have Eval(t) = Eval(t′).
⊓⊔
Corollary 3. Up to algebraic conjugacy, every distributive lattice, Boolean al-
gebra, ring, semigroup, monoid and group subshift is deﬁned cellwise.
Proof. By ﬁnding suitable normal forms, one easily sees that distributive lattices,
semigroups and monoids are 2-shallow, while Boolean algebras, groups and rings
are 3-shallow. Note that aﬃne maps have at most one unknown variable.
⊓⊔
Corollary 4. Every Boolean algebraic subshift is algebraically conjugate to a
product of a full shift and a ﬁnite shift.
Proof. This follows from the previous corollary and Theorem 1.
⊓⊔
The previous corollary is an analogue of a result of Kitchens [7] for group sub-
shifts.
Example 2. In the proof of Theorem 3, for the claim that algebraic conjugacy im-
plies k-shallowness, the use of a full shift is crucial: the claim is false even for the
mixing one-step SFT X ⊂{0, 1, 2, ⊥}
  with the forbidden pairs {10, 11, 20, 21}
equipped with the cellwise binary operation
· 0 1 2 ⊥
0 0 0 0 ⊥
1 1 2 1 ⊥
2 2 2 2 ⊥
⊥⊥⊥⊥⊥

On Shift Spaces with Algebraic Structure
643
Since lattices are not shallow in general, Theorem 2 and Theorem 3 do not tell
us much about lattice subshifts. In fact, the following example gives a lattice
subshift on a mixing SFT which is not cellwise up to algebraic conjugacy.
Example 3. Let S = {1+, 1−, 0+, 0−} have the lattice structure 1+ > 0+ > 0−
and 1+ > 1−> 0−, and deﬁne X ⊂S
  as the SFT with forbidden words
0−1+ and a−b−cδ for all (a, b, c) ̸= (0, 0, 0) and δ ∈{+, −}. Deﬁne X to have a
lattice structure where ∨is deﬁned cellwise, and ∧is deﬁned as ﬁrst applying the
cellwise meet of S
 , and then rewriting instances of 0−1+ to 0−0+ and instances
of a−b−cδ to 0−0−0δ. An easy calculation conﬁrms that the operations are well-
deﬁned and X indeed forms a lattice subshift.
We now show that X has aﬃne maps of arbitrary radius by giving two right
asymptotic points x, y ∈X with x0 ̸= y0, and then constructing aﬃne maps
tk ∈Af(X) for all k ∈
  such that tk(x)k ̸= tk(y)k. Let x = ∞1+.0+1+∞and
y = ∞1+∞. Deﬁne zk = σ−k(∞1+.1−1+∞) and z′ = ∞0+∞, and let
tk(ξ) = ((· · · (((ξ ∧z0) ∨z′) ∧z1) ∨z′ · · · ) ∧zk) ∨z′ .
It is easy to see that tk(x) = ∞1+.(0+)k+21+∞, and on the other hand, tk(y) = y.
We are not aware of a modular lattice which is not shallow. This raises the
natural question whether every modular lattice subshift deﬁned cellwise up to
algebraic conjugacy.
5
F-Linear Cellular Automata
Let F be again a variety, and S a ﬁnite member of F. We only consider cellwise
deﬁned algebraic structures of S
  in this section. We begin with the following
useful lemma, characterizing all F-linear cellular automata.
Lemma 3. A CA G with neighborhood radius r is F-linear if and only if its
local rule g is an F-morphism from S2r+1 to S.
Deﬁnition 6. The variety F has the congruence-product property, if for all
ﬁnite families (Si)i∈[1,n] of algebras in F we have that
Con(
n

i=1
Si) =
n

i=1
Con(Si) .
A proof of the following can be found, for example, in [4].
Lemma 4. The variety of lattices has the congruence-product property.
In the remainder of this section, we show that if F has the congruence-product
property, then the F-linear cellular automata have very simple limit sets and
limit dynamics. In particular, by the above lemma, our results hold for lattice-
linear automata.

644
V. Salo and I. T¨orm¨a
Lemma 5. Let F be a variety with the congruence-product property and S ∈F
ﬁnite. Let G be an F-linear CA on S
  with radius r and local function g. Denote
by R ⊂S the alphabet of the limit set of G (which is clearly a subalgebra),
and denote by π′
k the canonical projections R2r+1 →R. Let m
i=1 Ri be the
decomposition of R into directly indecomposable algebras, and denote by πi the
canonical projections R →Ri. Then for each i ∈[1, m] there exist ji, ki and a
surjective F-morphism hi : Rji →Ri such that πi ◦g|R2r+1 = hi ◦πji ◦π′
ki.
Since the proof of Lemma 4 can also be carried out for Boolean algebras, we
have the following.
Corollary 5. If S = 2T is a Boolean algebra and G a Boolean-linear CA on
S
 , then for each t ∈T , either πt(G(S
 )) is trivial, or we have i ∈
 and t′ ∈T
such that πt ◦G = πt′ ◦σi.
Theorem 4. Let F be a variety with the congruence-product property and S ∈F
ﬁnite. The limit set X of an F-linear cellular automaton G on S
  is algebraically
conjugate to a product of full shifts, and G is stable. Furthermore, there exists
p ∈
  such that Gp|X is a product of powers of shift maps.
Proof. Let m
i=1 Si be the decomposition of S into directly indecomposable al-
gebras. Deﬁne H = ({S1, . . . , Sm}, E) as the directed graph where (Si, Sj) ∈E
iﬀthe domain of the surjective map hj given by Lemma 5 is Si. Since each Si
has exactly one incoming arrow, every strongly connected component of H is a
cycle or a single vertex. Let Si be in a cycle, say Si →Si1 →· · · →Sip′−1 →Si.
Since Si is ﬁnite, the map fi = hi ◦hip′−1 ◦· · · ◦hi1 is an automorphism of Si,
and there exists pi ∈
  such that f pi
i
is the identity map of Si. This in turn
implies that for all x ∈S
 , we have
πi(Gpi(x)) = πi(σki(x))
for some ki ∈
. That is, Gpi simply shifts the Si-components of points by a
constant amount. Let I be the set of indices i such that Si occurs in a cycle,
and let p = lcmi∈I(pi). Clearly, G has a natural reversible restriction on the full
shift S
 I , where SI = 
i∈I Si.
Consider then Sj for some j ∈J = [1, m] −I. By following the incoming
arrows we necessarily ﬁnd an i(j) ∈I and a path of the form
Si(j) →Si1 →· · · →Sip′−1 →Si(j) →Sj1 →· · · →Sjq′−1 →Sj ,
where jk ∈J for all k. Denote by q(j) the length q′ of the path from Si(j) to
Sj, and let q = maxj∈J q(j).
Clearly, if y = Gq(x) and j ∈J , then πj(y) is a function of πi(j)(x), which
in turn is a function of some πi(y) with i ∈I. But this means that the J -
components of y are uniquely determined by its I-components. This and the
fact that G is reversible on S
 I imply that X = Gq(S
 ), X is algebraically
conjugate to S
 I , and Gp|X is a product of powers of shift maps.
⊓⊔

On Shift Spaces with Algebraic Structure
645
6
Future Work
In this paper we have only considered limit sets in the case when the cellular
automaton starts from the full shift. It would be interesting to study limit sets
of F-linear automata starting from more complicated shifts. We do not know
if our approach generalizes to, say, mixing F-subshifts of ﬁnite type, assuming
the congruence-product property. Future work might also involve studying the
connections between other properties of the variety F and the F-linear CA.
Acknowledgements. This research was supported by the Academy of Finland
Grant 131558.
References
1. Boyle, M., Schraudner, M.:
 d group shifts and Bernoulli factors. Ergodic Theory
Dynam. Systems 28(2), 367–387 (2008),
http://dx.doi.org/10.1017/S0143385707000697
2. Burris, S., Sankappanavar, H.P.: A course in universal algebra. Graduate Texts in
Mathematics, vol. 78. Springer, New York (1981)
3. Cattaneo, G., Formenti, E., Manzini, G., Margara, L.: Ergodicity, transitivity, and
regularity for linear cellular automata over
 m. Theoret. Comput. Sci. 233(1-2),
147–164 (2000), http://dx.doi.org/10.1016/S0304-3975(98)00005-X
4. Gr¨atzer, G.: Lattice theory. First concepts and distributive lattices. W. H. Freeman
and Co., San Francisco (1971)
5. Itˆo, M., ˆOsato, N., Nasu, M.: Linear cellular automata over
 m. J. Comput. System
Sci. 27(1), 125–140 (1983), http://dx.doi.org/10.1016/0022-0000(83)90033-8
6. Kari, J.: Linear Cellular Automata with Multiple State Variables. In: Reichel, H.,
Tison, S. (eds.) STACS 2000. LNCS, vol. 1770, pp. 110–121. Springer, Heidelberg
(2000)
7. Kitchens, B.P.: Expansive dynamics on zero-dimensional groups. Ergodic Theory
Dyn. Syst. 7, 249–261 (1987)
8. Kitchens, B.P.: Symbolic dynamics – One-sided, two-sided and countable state
Markov shifts. Universitext. Springer, Berlin (1998)
9. Lind, D., Marcus, B.: An introduction to symbolic dynamics and coding. Cam-
bridge University Press, Cambridge (1995),
http://dx.doi.org/10.1017/CBO9780511626302
10. Manzini, G., Margara, L.: Invertible linear cellular automata over
 m: algorithmic
and dynamical aspects. J. Comput. System Sci. 56(1), 60–67 (1998),
http://dx.doi.org/10.1006/jcss.1997.1535
11. Sato, T.: Ergodicity of linear cellular automata over
 m. Inform. Process.
Lett. 61(3), 169–172 (1997),
http://dx.doi.org/10.1016/S0020-0190(96)00206-2
12. Schmidt, K.: Dynamical systems of algebraic origin. Progress in Mathematics,
vol. 128. Birkh¨auser Verlag, Basel (1995)

Finite State Veriﬁers with Constant Randomness
A.C. Cem Say1 and Abuzer Yakaryılmaz2
1 Bo˘gazi¸ci University, Department of Computer Engineering,
Bebek 34342 ˙Istanbul, Turkey
say@boun.edu.tr
2 University of Latvia, Faculty of Computing, Raina bulv. 19, R¯ıga, 1586, Latvia
abuzer@lu.lv
Abstract. We give a new characterization of NL as the class of lan-
guages whose members have certiﬁcates that can be veriﬁed with small
error in polynomial time by ﬁnite state machines that use a constant
number of random bits, as opposed to its conventional description in
terms of deterministic logarithmic-space veriﬁers. It turns out that al-
lowing two-way interaction with the prover does not change the class of
veriﬁable languages, and that no polynomially bounded amount of ran-
domness is useful for constant-memory computers when used as language
recognizers, or public-coin veriﬁers.
1
Introduction
It is known that allowing constant-memory computers to use random bits and
to commit small amounts of error increases their power, both as language rec-
ognizers [9], and as veriﬁers of membership proofs [6]. In this paper, we exam-
ine the eﬀects of restricting such probabilistic machines (2pfa’s) to use only a
constant number of random bits, independent of the length of the input. We
prove that such constant-randomness 2pfa’s are able to verify membership in
precisely the languages in NL. This is an interesting addition to the facts that
NL has deterministic logspace veriﬁers, and NP is the class of languages that
has logspace veriﬁers that use logarithmically many random bits [5]. We obtain
this result by demonstrating that such veriﬁers are equivalent to multihead ﬁnite
automata. Allowing these constant-coin veriﬁers to use logarithmic space, and
to have two-way interaction with the prover, does not augment the class of veri-
ﬁable languages. No nonregular language has such an interactive proof system if
the veriﬁer is restricted to use public coins. We also show that, when used as rec-
ognizers, no amount of polynomially-bounded randomness gives standard 2pfa’s
any power beyond their deterministic versions. The rest of this paper is struc-
tured as follows: Section 2 provides the necessary background. Our results on the
new characterization of NL in terms of ﬁnite state veriﬁers, and the public-coin
case, are presented in Section 3. Section 4 is a conclusion.
2
Preliminaries
For background on interactive proof systems with bounds on the usage of space
and/or randomness, the reader is referred to [4].
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 646–654, 2012.
  Springer-Verlag Berlin Heidelberg 2012

Finite State Veriﬁers with Constant Randomness
647
The main model of veriﬁer that we shall use is a Turing machine with a read-
only input tape and a single read/write work tape. The input tape holds the
input string between two occurrences of the end-marker symbol
 , and we as-
sume that the machine’s transition function never attempts to move the input
head beyond the end-markers. The input tape head is on the left end-marker
at the start of the process. The veriﬁer exchanges information with a prover by
writing and reading one symbol at a time from the communication alphabet Γ
in a communication cell. Using this information channel, the prover attempts to
prove the membership of the input string in the language under consideration.
Of course, one should not trust this blindly, and we even allow the possibility
that the prover sends an inﬁnite sequence of symbols, a contingency that could
cause careless veriﬁers to run forever. The machine also has access to a source
of random bits. The state set of the veriﬁer TM is Q = R ∪D ∪{qa, qr}, where
R is the set of coin-tossing states, D is the set of deterministic states, and qa
(accept) and qr (reject) are the halting states. Associated with each state q ∈Q,
there is a communication symbol γq ∈Γ. The special “null symbol” ϵ is guar-
anteed to be a member of Γ. One of the non-halting states is designated as the
start state. When any state q ∈R ∪D with γq ̸= ϵ is entered, γq is written
in the communication cell. The prover can be modeled as a prover transition
function ρ, which determines the symbol γ ∈Γ to be written in response, based
on the input string and the entire communication that has taken place so far.
Let ♦= {−1, 0, +1} denote the set of possible head movement directions. When
the veriﬁer reads the response of the prover, it behaves according to the veriﬁer
transition function δ as follows: For q ∈R, δ(q, σ, θ, γ, b) = (q′, θ′, di, dw) indi-
cates that the machine will switch to state q′, write θ′ on the work tape, move
the input head in direction di ∈♦, and the work tape head in direction dw ∈♦,
if it is originally in state q, scanning the symbols σ, θ, and γ in the input and
work tapes, and the communication cell, respectively, and seeing the random bit
b as a result of the coin toss. For q ∈D, δ(q, σ, θ, γ) = (q′, θ′, di, dw) has a similar
meaning, but without the randomness. If γq = ϵ, the veriﬁer transition function
described above is applied directly, without any communication.
We say that language L has a (private-coin) interactive proof system (IPS)
with error probability ε if there exists a prover P and a veriﬁer V such that
1. for every x ∈L, the interaction of P and V on input w results in acceptance
with probability at least 1 −ε, and,
2. for every x /∈L, and for any prover P ∗, the interaction of P ∗and V on input
x results in rejection with probability at least 1 −ε.
Interactive proof systems where the veriﬁer accepts every member of the lan-
guage with probability 1 are said to have perfect completeness.
IP(t,s,r) is the class of languages that have interactive proof systems with
veriﬁers whose expected runtime is t(n) steps, and which use s(n) space (i.e.,
work tape cells) and r(n) random bits on any input of length n. We use the
notations cons, log, poly, and exp to stand for functions that are O(1), O(log n),
O(nk), and 2O(nk) for any constant k, respectively. For veriﬁers with no bound
on a speciﬁc parameter, we shall use the symbol ∞for that parameter.

648
A.C. Cem Say and A. Yakaryılmaz
Replacing condition 2 in the deﬁnition of interactive proof systems with the
weaker condition
2
′. for every x /∈L, and for any prover P ∗, the interaction of P ∗and V on
input x results in acceptance with probability at most ε
leads to our deﬁnition of IPw(t,s,r), the counterpart of IP(t,s,r) with these al-
ternative kinds of veriﬁers that do not have to halt with high probability for
all inputs. It should be noted that the time bound of t has a diﬀerent meaning
in the deﬁnition of IPw, and indicates only that P is able to convince V on all
inputs which are members of the language L with high probability in time t.
A one-way interactive proof system [3] is an IPS where the prover is restricted
so that it maps the set of input strings to the set of sequences from the commu-
nication alphabet. For input string w, the prover writes the ith symbol of the
corresponding sequence in the communication cell at the ith time the veriﬁer
enters a state q with γq ̸= ϵ. This ensures that the communication between the
prover and the veriﬁer is one-way. The corresponding language classes are named
by preﬁxing the class names mentioned above with the designation “oneway-”.
Note that one can deﬁne a one-way IPS without mentioning a prover at all, as
in the conventional “certiﬁcate” deﬁnitions of nondeterministic classes like NP.
The following equalities are trivial:
NP = oneway-IPw(poly,poly,0)
(1)
NL = oneway-IPw(poly,log,0)
(2)
Note that specifying 0 as the randomness complexity of the veriﬁer is just a way
of saying that it is deterministic.
Allowing logarithmic amounts of randomness yields the characterization [5]
NP = oneway-IPw(poly,log,log) = IPw(poly,log,log),
(3)
with an improvement in the space bound. Relaxing the randomness bound fur-
ther does not help on its own, since [3]
oneway-IPw(poly,log,poly) = NP,
(4)
but allowing interaction as well famously yields [2,16]
IPw(poly,log,poly) = PSPACE.
(5)
A public-coin IPS, also known as an Arthur-Merlin game, is an IPS where the
veriﬁer sends precisely its state, tape and head updates at every step to the
prover, thereby ensuring that the prover always knows the veriﬁer’s conﬁgu-
ration. The public-coin version of IPw(t,s,r) is named AMw(t,s,r). It is known
[1,10,16] that
AMw(exp,log,exp) = P,
(6)
and
AMw(poly,poly,poly) = PSPACE.
(7)

Finite State Veriﬁers with Constant Randomness
649
The relationships in Equations 1-7 remain true for the strong deﬁnition of IPSs,
since logarithmically bounded space is suﬃcient to cut oﬀunacceptably long
computational paths. When one considers ﬁnite state veriﬁers, [8] which use
only a constant amount of cells on the work tape,1 the diﬀerence between the
weak and strong deﬁnitions of becomes evident.
With no limits on the runtime, or the number of random bits to be used,
weak ﬁnite state veriﬁers are very powerful; oneway-IPw(∞,cons,∞) contains
every recursively enumerable language, whereas IP(∞,cons,∞) is contained in
SPACE(22O(n)) [6]. It has been proven [8] that Arthur-Merlin games with ﬁnite
state veriﬁers exist for languages outside the class 2PFA of languages recogniz-
able by “stand-alone” 2pfa’s, and that some languages have linear-time ﬁnite
state veriﬁers only if the public-coin restriction is not enforced, in contrast to
Equations 5 and 7.
We shall focus on veriﬁers which use a constant number of random bits for
any input.
We start our examination of the eﬀects of limiting the number of random bits
by noting that machines that are not helped by a prover about their input are
very weak when restricted to work with constant workspace, and polynomially
bounded randomness.
Theorem 1. For any polynomial p, every 2pfa whose expected number of coin
tosses on halting computational branches is O(p(n)) for input strings of length
n recognizes a regular language with bounded error.
Proof. This is a straightforward modiﬁcation of the proof (in [7]) of the following
fact [7,14]:
For any polynomial p, 2pfa’s with expected runtime O(p(n)) recognize only
the regular languages with bounded error.
We omit the details due to space constraints.
In the next section, we shall demonstrate an interesting relationship between
constant-space, constant-randomness veriﬁers and multihead ﬁnite automata. A
k-head ﬁnite automaton (2nfa(k)) is simply a nondeterministic ﬁnite-state ma-
chine with k two-way heads that it can direct on a read-only tape containing
the input string, ﬂanked by two end-markers. A conﬁguration of a 2nfa(k) is a
tuple consisting of its current state and head positions. The classes of languages
recognized by these machine families will be denoted as 2NFA(k). Deterministic
single-head versions of multihead automata are denoted as 2dfa’s. Detailed in-
formation about 2nfa(k)’s can be found in [12]. We note the following important
facts that will be used in our proofs.
1 It is easy to see that such machines can be simulated by machines with longer pro-
grams which have no work tape at all, namely, two-way probabilistic ﬁnite automata
(2pfa’s) [9].

650
A.C. Cem Say and A. Yakaryılmaz
Fact 1. ∪k≥12NFA(k) = NL [11].
Fact 2. Every 2nfa(k) has an equivalent 2nfa(2k) that halts in O(nk) time on
every computational branch.
3
2pfa Veriﬁers with Constant Randomness and 2nfa(k)’s
Our new characterization of NL is demonstrated by the following two lemmas.
Lemma 1. NL ⊆oneway-IPw(poly,cons,cons).
Proof. Let L be any language in NL. By Fact 1, L is recognized by a 2nfa(k) M.
We show how to construct a weak one-way IPS with the required properties that
recognizes L with perfect completeness for any desired error probability ε < 1
2.
As mentioned above, this is equivalent to demonstrating how every member of L
has a membership certiﬁcate that can be checked with such a veriﬁer. We start
by building a veriﬁer V that simulates one run of M, by consulting the certiﬁcate
for choosing among the nondeterministic branches of M. V uses just r = ⌈log k⌉
random bits to branch to k computation paths (each path has probability at
least 2−r) while scanning the left input end-marker. Each such path will use its
head to track the position of the corresponding head of M. For every step of the
simulation of M, the certiﬁcate contains a symbol conveying the list of k symbols
that would be scanned by M’s heads at this step, together with an indication
of which nondeterministic choice should now be taken by M to eventually reach
the accept state. The ith path of V rejects immediately if it sees that the present
certiﬁcate symbol is inconsistent with what the ith head is currently scanning,
and updates its state and head position according to M’s program and the
information given by the certiﬁcate otherwise.
If the input string is accepted by M, the certiﬁcate will lead all paths of V to
acceptance, by giving correct information about what the heads are seeing and
the nondeterministic choice at every step, yielding a total acceptance probability
of 1. Otherwise, any certiﬁcate must “lie” about at least one head in order to
make some paths accept, causing the path responsible for that head to reject,
so the acceptance probability in that case is at most 1 −2−r. To reduce the
unacceptably high error bound for nonmembers, we chain several copies of V
to run one after another,2 on a correspondingly long certiﬁcate, and accept if
and only if all copies accept, rejecting otherwise. It is easy to see that a chain
of m copies of V involves an error of (1 −2−r)m, and therefore m ≥
log ε
log(1−2−r)
iterations are suﬃcient to obtain an error of ε, where the total number of random
bits used by the resulting veriﬁer would be O(k log k log 1
ε). Note that a 2nfa(k)
with state set Q has at most |Q|(n+2)k distinct reachable conﬁgurations on any
input of length n, and therefore V runs in polynomial time for correct proofs of
membership.
2 Note that the certiﬁcate guides the paths of V to position their heads back on the
left end-marker and to start the next round of coin-ﬂipping simultaneously.

Finite State Veriﬁers with Constant Randomness
651
The reason why our construction does not yield an IPS according to the strong
deﬁnition is that an evil prover can supply an inﬁnitely long fake certiﬁcate that
makes some paths of the veriﬁer enter inﬁnite loops by lying3 about a head that
those paths cannot see, at the cost of being rejected by the path responsible for
that head.
The proof of Lemma 1 shows that 2NFA(2), which contains nonregular lan-
guages, has veriﬁers with two random bits. This is the minimum number of
useful random bits for 2pfa veriﬁers. A single coin toss would create just two
computational paths with equal probability. Since a probabilistic machine that
always responds correctly can be replaced by its deterministic counterpart, we
must have the veriﬁer err for at least one input string. But the probability of
such an error is at least 1
2 in a machine that tosses its coin only once, which
would violate our bounded error condition.
The reader should also note that the IPSs of Lemma 1 are strictly stronger than
2pfa’s unaided by a prover, even when the latter are allowed to use an unbounded
number of fair coins, since it is known [13,15] that the class of languages recogniz-
able by such stand-alone 2pfa’s is properly contained in the class L.
We shall now show that two-way interaction with the prover does not aug-
ment the power of constant-randomness veriﬁers, even if they are allowed to use
logarithmic space.
Lemma 2. IPw(poly,log,cons) ⊆NL.
Proof. Suppose that a language L has an IPS with prover P and logspace veriﬁer
V , that use the communication alphabet Γ. V always uses at most r random bits,
and has state set Q = C∪N, where C = {q ∈Q | γq ̸= ϵ} is the set of states which
communicate with the prover, and N is the set of “noncommunicating” states.
Let t(n) denote the polynomial time bound of V . We start by transforming V
to a set S={M1, M2, . . . , M2r} of deterministic logspace veriﬁers, each of which
simulates a version of V with a diﬀerent assignment to the r-bit random string.
We build a one-way IPS with a deterministic logspace veriﬁer M for L. The
purported membership certiﬁcate that M will check consists of 2r tracks, each
with alphabet T = Γ ∪{⃝, ∞}. The ith track is supposed to contain a transcript
of Mi’s communications with the prover about the input x. The ith track square
of the jth certiﬁcate symbol contains
– γ, if Mi receives the prover response γ in its jth communication step,
– ⃝, if Mi performs a halting computation with fewer than j communications,
and,
– ∞otherwise, that is, if Mi enters a nonhalting path of noncommunicating
states after performing fewer than j communications.
To process the jth certiﬁcate symbol, M simulates all the Mis that are indicated
to be on a halting path on the input x until they reach their jth communication
3 We can assume that the simulated multihead automaton has the desirable property
mentioned in Fact 2. Any prover that causes a long runtime must therefore be lying.

652
A.C. Cem Say and A. Yakaryılmaz
step, or terminate. M rejects if it detects a mismatch between the track content
and the actual computation of Mi.
Note that some members of S can have the same communication transcript,
that is, they can send precisely the same sequence of symbols to the prover,
and therefore receive the same sequence of responses, during their execution on
x. Partition S into blocks, each of which correspond to a diﬀerent communica-
tion transcript. M discovers this partition as it goes through the certiﬁcate. At
the start, it considers all the Mis as in the same block in the initial partition.
Whenever it scans a new certiﬁcate symbol, M reﬁnes the partition to separate
the Mis that send diﬀerent symbols, or perform no communication, and rejects
if the certiﬁcate is claiming that diﬀerent prover messages are being received
by two veriﬁers in the same block of the new partition. If any track contains a
communication symbol after the appearance of a ⃝on an ∞, M rejects. If it
detects that it has been running for more than n · t(n) steps, M rejects. M ac-
cepts if the certiﬁcate survives these tests, and a majority of the Mis are veriﬁed
to terminate with acceptance.
Clearly, a majority of the members of S accept as a result of their interaction
with P on the input x if and only if x ∈L. If the input is not in L, there is no
prover that can fool V for more than half of its possible coin strings to cause
acceptance together, and no certiﬁcate can make M accept this input.
We have proven that
Theorem 2. oneway-IPw(poly,cons,cons) = IPw(poly,log,cons) = NL.
Let REG denote the set of regular languages. We also have the following to say
about the public-coin versions of these veriﬁers.
Theorem 3. AMw(∞,cons,cons) = REG.
Proof. One direction is obvious. For the other direction, we adapt the proof of
Lemma 2 to this simpler case. The members of the set S of 2r deterministic
veriﬁers (the Mis) used in that proof are now 2dfa’s. Since the prover is now
free to send diﬀerent messages to each of these veriﬁers, we do not have to worry
about checking for consistency among the supplied communication transcripts
of those machines. We can therefore simulate them sequentially, rather than in
parallel, requiring the certiﬁcate to just present the transcripts of the Mis one
after another. But such a certiﬁcate can be checked by a deterministic ﬁnite
state veriﬁer with just one head, meaning that the machine we construct is in
fact a 2nfa, and therefore can recognize only regular languages.
4
Open Questions
We have been able to represent the relationship between NL and NP in the form
NL = oneway-IPw(poly,cons,cons) ⊆oneway-IPw(poly,log,log) = NP.
Further examination of other classes like oneway-IPw(poly,cons,log) would be
interesting.

Finite State Veriﬁers with Constant Randomness
653
Is the inclusion
oneway-IP(poly,cons,cons) ⊆oneway-IPw(poly,cons,cons)
proper or not? Every language that can be veriﬁed by a constant-randomness
2pfa that halts with probability 1 is recognized in linear time by a 2nfa(k) for
some k. Does there exist a language in NL which cannot be recognized in linear
time by any 2nfa(k)?
Acknowledgements. Say was partially supported by T¨UB˙ITAK with grant
108E142; Yakaryılmaz was partially supported by T¨UB˙ITAK with grant 108E142
and FP7 FET-Open project QCS.
We are grateful to Martin Kutrib, who helped us immensely with our questions
about nfa(k)’s. We also thank Taylan Cemgil and Richard Lipton for their helpful
answers.
References
1. Condon, A.: Computational Models of Games. MIT Press (1989)
2. Condon,
A.:
Space-bounded
probabilistic
game
automata.
Journal
of
the
ACM 38(2), 472–494 (1991)
3. Condon, A.: The complexity of the max word problem and the power of one-way
interactive proof systems. Computational Complexity 3(3), 292–305 (1993)
4. Condon, A.: The complexity of space bounded interactive proof systems. In:
Complexity Theory: Current Research, pp. 147–190. Cambridge University Press
(1993)
5. Condon, A., Ladner, R.: Interactive proof systems with polynomially bounded
strategies. Journal of Computer and System Sciences 50(3), 506–518 (1995)
6. Condon, A., Lipton, R.J.: On the complexity of space bounded interactive proofs.
In: Proceedings of the 30th Annual Symposium on Foundations of Computer Sci-
ence. pp. 462–467 (1989),
http://portal.acm.org/citation.cfm?id=1398514.1398732
7. Dwork, C., Stockmeyer, L.: A time complexity gap for two-way probabilistic ﬁnite-
state automata. SIAM Journal on Computing 19(6), 1011–1123 (1990)
8. Dwork, C., Stockmeyer, L.: Finite state veriﬁers I: The power of interaction. Journal
of the ACM 39(4), 800–828 (1992)
9. Freivalds, R.: Probabilistic two-way machines. In: Proceedings of the International
Symposium on Mathematical Foundations of Computer Science, pp. 33–45 (1981)
10. Goldwasser, S., Sipser, M.: Private coins versus public coins in interactive proof
systems. In: Proceedings of the 18th Annual ACM Symposium on Theory of Com-
puting (STOC 1986), pp. 59–68 (1986)
11. Hartmanis, J.: On non-determinancy in simple computing devices. Acta Informat-
ica 1, 336–344 (1972)
12. Holzer, M., Kutrib, M., Malcher, A.: Complexity of multi-head ﬁnite automata:
Origins and directions. Theoretical Computer Science 412, 83–96 (2011)

654
A.C. Cem Say and A. Yakaryılmaz
13. Ka¸neps, J.: Stochasticity of the languages acceptable by two-way ﬁnite probabilistic
automata. Diskretnaya Matematika 1, 63–67 (1989) (Russian)
14. Ka¸neps, J., Freivalds, R.: Running Time to Recognize Nonregular Languages by 2-
Way Probabilistic Automata. In: Leach Albert, J., Monien, B., Rodr´ıguez-Artalejo,
M. (eds.) ICALP 1991. LNCS, vol. 510, pp. 174–185. Springer, Heidelberg (1991)
15. Macarie, I.I.: Space-eﬃcient deterministic simulation of probabilistic automata.
SIAM Journal on Computing 27(2), 448–465 (1998)
16. Shamir, A.: IP = PSPACE. Journal of the ACM 39(4), 869–877 (1992)

Game Arguments in Computability Theory
and Algorithmic Information Theory
Alexander Shen1,2
1 Laboratoire d’Informatique, de Robotique et de Micro´electronique de Montpellier
(LIRMM), Universit´e Montpellier 2, UMR 5506—CC477, 161 rue Ada, 34095
Montpellier Cedex 5, France
alexander.shen@lirmm.fr
2 On leave from Institute for Information Transmission Problems,
Russian Academy of Sciences, Bolshoy Karetny per. 19, Moscow, 127994, Russia
sasha.shen@gmail.com
Abstract. We provide some examples showing how game-theoretic ar-
guments can be used in computability theory and algorithmic informa-
tion theory: unique numbering theorem (Friedberg), the gap between
conditional complexity and total conditional complexity, Epstein–Levin
theorem and some (yet unpublished) result of Muchnik and Vyugin.
1
Friedberg’s Unique Numbering
It often happens that some result in computability theory or algorithmic infor-
mation theory is essentially about the existence of a winning strategy in some
game. This statement can be elaborated in a more formal way [5,6,7], but in this
paper we illustrate this approach by several examples.
Our ﬁrst example is a classical result of R. Friedberg [4]: the existence of
unique numberings.
Theorem 1 (Friedberg). There exists a partial computable function F(·, ·) of
two natural variables such that:
1. F is universal, i.e., every computable function f(·) of one variable appears
among the functions Fn : x →F(n, x);
2. all the functions Fn are diﬀerent.
Proof. The proof can be decomposed in two parts. First, we describe some game
and explain why the existence of a (computable) winning strategy for one of the
players makes the statement of Friedberg’s theorem true.
In the second part of the proof we construct a winning strategy and therefore
ﬁnish the proof.
Game
The game is inﬁnite and is played on two boards. Each board is a table with
an inﬁnite number of columns (numbered 0, 1, 2 . . . from left to right) and rows
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 655–666, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

656
A. Shen
(numbered 0, 1, 2, . . . starting from the top). Each player (we call them Alice and
Bob, as usual) plays on its own board. The players alternate. At each move player
can ﬁll some (ﬁnitely many) cells at her/his choice with any natural numbers
(s)he wishes. Once a cell is ﬁlled, it keeps this number forever (it cannot be
erased).
The game is inﬁnite, so in the limit we have two tables A (ﬁlled by Alice) and
B (ﬁlled by Bob). Some cells in the limit tables may remain empty. The winner
is determined by the following rule: Bob wins if
– for each row in A-table there exists an identical row in B-table;
– all the rows in B-table are diﬀerent.
Lemma 1. Assume that Bob has a computable winning strategy in this game.
Then the statement of Theorem 1 is true.
Proof. A table represents a partial function of two arguments in a natural way:
the number in ith row and jth column is the value of the function on (i, j); if
the cell is not ﬁlled, the value is undeﬁned.
Let Alice ﬁll A-table with the values of some universal function (so the jth
cell in the ith row is the output of ith program on input j). Alice does this at
her own pace simulating in parallel all the programs (and ignoring Bob’s moves).
Let Bob apply his computable winning strategy against the described strategy
of Alice. Then his table also corresponds to some computable function B (since
the entire process is algorithmic). This function satisﬁes both requirements of
Theorem 1: since A-function is universal, every computable function appears in
some row of A-table and therefore (due to the winning condition) also in some
row of B-table. So B is universal. On the other hand, all Bn are diﬀerent since
the rows of B-table (containing Bn) are diﬀerent.
Remark 1. If Alice had a computable winning strategy in our game, the state-
ment of Theorem 1 would be false. Indeed, let Bob ﬁll his table with the values
of a universal function that satisﬁes the requirements of the theorem (ignoring
Alice’s moves). Then Alice ﬁlls her table in a computable way and wins. This
means that some row of Alice’s table does not appear in Bob’s table (so his
function is not universal) or two rows in Bob’s table coincide (so his function
does not satisfy the uniqueness requirement).
So we can try this game approach even not knowing for sure who wins in the
game; ﬁnding out who wins in the game would tell us whether the statement of
the theorem is true or false (assuming the strategy is computable).
Winning Strategy
Lemma 2. Bob has a computable winning strategy in the game described.
Proving this lemma we may completely forget about computability and just
describe the winning strategy explicitly (this is the main advantage of the game
approach). We do this in two steps: ﬁrst we consider a simpliﬁed version of the

Game Arguments in Computability Theory
657
game and explain how Bob can win. Then we explain what he should do in the
full version of the game.
In the simpliﬁed version of the game Bob, except for ﬁlling B-table, could kill
some rows in it. The rows that were killed are not taken into account when the
winner is determined. So Bob wins if the ﬁnal (limit) contents of the tables sat-
isﬁes two requirements: (1) for each row in A-table there exists an identical valid
(non-killed) row in B-table, and (2) all the valid rows in B-table are diﬀerent.
(According to this deﬁnition, after the row is killed its content does not matter.)
To win the game, Bob hires a countable number of assistants and makes ith
assistant responsible for ith row in A-table. The assistants start their work one by
one; let us agree that ith assistant starts working at move i, so at every moment
only ﬁnitely many assistants are active. Assistant starts her work by reserving
some row in B-table not reserved by other assistants, and then continues by
copying the current contents of ith row of A-table (for which she is responsible)
into this reserved row. Also at some point the assistant may decide to kill the
current row reserved by her, reserve a new row, and start copying the current
content of ith row into the new reserved row. Later in the game she may kill the
reserved row again, etc.
The instructions for the assistant say when the reserved row is killed. They
should guarantee that
– if ith row in the ﬁnal (limit) state of A-table coincides with some previous
row, then ith assistant kills her reserved row inﬁnitely many times (so none
of the reserved rows remain active);
– if it is not the case, i.e., if ith row is diﬀerent from all previous rows in the
ﬁnal A-table, then ith assistant kills her row only ﬁnitely many times (and
after that faithfully copies ith row of A-table into the reserved row).
If this is arranged, the valid rows of B-table correspond to the ﬁrst occurences
of rows with given contents in A-table, so they are all diﬀerent, and contain all
the rows of A-table.
The instruction for the assistant number i: keep track of the number of rows
that you have already killed in some counter k; if in the current state of A-
table the ﬁrst k positions in ith row are identical to the ﬁrst k positions of some
previous row, kill the current reserved row in B-table (and increase the counter);
if not, continue copying ith row into the current row.
Let us see why these instructions indeed have the required properties. Imagine
that in the limit state of A-table the row i is the ﬁrst row with given content,
i.e., is diﬀerent from all the previous rows. For each of the previous rows let us
select and ﬁx some position (column) where the rows diﬀer, and consider the
moment T when these positions reach the ﬁnal state. Let N be the maximal
number of the selected columns. After step T the ith row in A-table diﬀers from
all previous rows in one of the ﬁrst N positions, so if the counter of killed rows
exceeds N, no more killings are possible (for this assistant).
On the other hand, assume that ith assistant kills her row ﬁnitely many times
and N is the maximal value of her counter. After N is reached, the contents of
ith row in A-table is always diﬀerent from the previous rows in one of the ﬁrst

658
A. Shen
N positions, and the same is true in the limit (since this rectangle reaches its
limit state at some moment).
So Bob can win in the simpliﬁed game, and to ﬁnish the proof of Lemma 2
we need to explain how Bob can refrain from killing and still win the game.
Let us say that a row is odd if it contains a ﬁnite odd number of non-empty
cells. Bob will now ignore odd rows of A-table and at the same time guarantee
that all possible odd rows (there are countably many possibilities) appear in
B-table exactly once. We may assume now without loss of generality that odd
rows never appear in A-table: if she adds some element in a row making this row
odd, this element is ignored by Bob until Alice wants to add another element
in this row, and then the pair is added. This makes the A-table that Bob sees
slightly diﬀerent from what Alice actually does, but all the rows in the limit
A-table that are not odd (i.e., are inﬁnite or have even number of ﬁlled cells)
will get through — and Bob separately takes care of odd rows.
Now the instructions for assistants change: instead of killing some row, she
should ﬁll some cells in this row making it odd, and ensure that this odd row is
new (diﬀerent from all other odd rows of the current B-table). After that, this
row is considered like if it were killed (no more changes). This guarantees that
all non-odd rows of A-table appear in B-table exactly once.
Also Bob hires an additional assistant who ensures that all possible odd rows
appear in B-table: she looks at all the possibilities one by one; if some odd
row has not appeared yet, she reserves some row and puts the desired content
there. (Unlike other assistants, she reserves more and more rows.) This behavior
guarantees that all possible odd rows appear in B-table exactly once. (Recall
that other assistants also avoid repetitions among odd rows.) Lemma 2 and
Theorem 1 are proven.
2
Total Conditional Complexity
In this section we switch from the general computability theory to the algo-
rithmic information theory and consider a simple example. We compare the
conditional complexity C(x|y) and the minimal length of the program of a total
function that maps y to x. It turns out that the second quantity can be much
bigger (see, e.g., [1].) But let us recall ﬁrst the deﬁnitions.
The conditional complexity of a binary string x relative to a binary string y (a
condition) is deﬁned as the length of the shortest program that maps y to x. The
deﬁnition depends on the choice of the programming language, and one should
select an optimal one that makes the complexity minimal (up to O(1) additive
term). When the condition y is empty, we get (unconditional plain) complexity
of x. See, e.g., [9] for more details. The conditional complexity of x relative to y
is denoted by C(x|y); the unconditional complexity of x is denoted by C(x).
It is easy to see that C(x|y) can also be deﬁned as the minimal complexity of
a program that maps y to x. (This deﬁnition coincides with the previous one up
to O(1) additive term.) But in some applications (e.g., in algorithmic statistics,
see [10]) we are interested in total programs, i.e. programs that are guaranteed

Game Arguments in Computability Theory
659
to terminate for every input. Let us deﬁne CT(x|y) as the minimal complexity
of a total program that maps y to x. In general, this restriction could increase
complexity, but how signiﬁcant could be this increase? It turns out that these
two quantities may diﬀer drastically, as the following simple theorem shows:
Theorem 2. For every n there exist two strings xn and yn of length n such that
C(xn|yn) = O(1) but CT(xn|yn) ≥n.
Proof. To prove this theorem, consider a game Gn (for each n). In this game
Alice constructs a partial function A from Bn to Bn, i.e., a function deﬁned on
(some) n-bit strings, whose values are also n-bit strings. Bob constructs a list
B1, . . . , Bk of total functions of type Bn →Bn. (Here B = {0, 1}.)
The players alternate; at each move Alice can add several strings to the domain
of A and choose some values for A on these strings; the existing values cannot
be changed. Bob can add some total functions to the list, but the total length
of the list should remain less than 2n. The players can also leave their data
unchanged; the game, though inﬁnite by deﬁnition, is essentially ﬁnite since
only ﬁnite number of nontrivial moves is possible. The winner is determined as
follows: Alice wins if there exists a n-bit string y such that A(y) is deﬁned and
is diﬀerent from B1(y), . . . , Bk(y).
Lemma 3. Alice has a computable (uniformly in n) winning strategy in this
game.1
Before proving this lemma, let us explain how it helps to prove Theorem 2. Let
(for every n) Alice play against the following strategy of Bob: he just enumerates
all the total functions of type Bn →Bn who have complexity less than n, and
adds them to the list when they appear. (As in the previous section, Bob does not
really care about Alice’s moves.) The behavior of Alice is then also computable
since she plays a computable strategy againt a computable opponent. Let yn be
the string where Alice wins, and let xn be equal to A(yn) where A is the function
constructed by Alice.
It is easy to see that C(xn|yn) = O(1); indeed, knowing yn, we know n, can
simulate the game, and ﬁnd xn during this simulation. On the other hand, if
there were a total function of complexity less than n that maps yn to xn, then
this function would be in the list and Bob would win.
So it remains to prove the lemma by showing the strategy for Alice. This
strategy is straightforward: ﬁrst Alice selects some y and says that A(y) is equal
to some x. (This choice can be done in arbitrary way, if Bob has not selected
any functions yet; we may always assume it is the case by postponing the ﬁrst
move of Bob; the timing is not important in this game.) Then Alice waits until
one of Bob’s functions maps y to x. This may never happen; in this case Alice
does nothing else and wins with x and y. But if this happens, Alice selects
another y and chooses x that is diﬀerent from B1(y), . . . , Bk(y) for all total
1 Since the game is eﬀectively ﬁnite, in fact the existence of a winning strategy implies
the existence of a computable one. But it is easy to describe the computable strategy
explicitly.

660
A. Shen
functions B1, . . . , Bk that are currently in Bob’s list. Since there are less than
2n total functions in the list, it is always possible. Also, since Bob can make at
most 2n −1 nontrivial moves, Alice will not run out of strings y. Lemma 3 and
theorem 2 are proven.
3
Epstein–Levin Theorem
In this section we discuss a game-theoretic interpretation of an important recent
result of Epstein and Levin [3]. This result can be considered as an extension of
some previous observations made by Vereshchagin (see [10]). Let us ﬁrst recall
some notions from the algorithmic information theory.
For a ﬁnite object x one may consider two quantities. The ﬁrst one, the
complexity of x, shows how many bits we need to describe x (using an optimal
description method). The second one, a priori probability of x, measures how
probable is the appearance of x in a (universal) algorithmic random process.
The ﬁrst approach goes back to Kolmogorov while the second one was suggested
earlier by Solomonoﬀ.2 The relation between these two notions in a most clean
form was established by Levin and later by Chaitin (see [2] for more details).
For that purpose Levin modiﬁed the notion of complexity and introduced preﬁx
complexity K(x) where programs (descriptions) satisfy an additional property:
if p is a program that outputs x, then every extension of p (every string having
preﬁx p) also outputs x. (Chaitin used another restriction: the set of programs
should be preﬁx-free, i.e., none of the programs is a preﬁx of another one; though
it is a signiﬁcantly diﬀerent restriction, it leads to the same notion of complexity
up to O(1) additive term.) The notion of a priori probability can be formally
deﬁned in the following way: consider a randomized algorithm M without input
that outputs some natural number and then terminates. The output number
depends on the outcomes of the internal random bit generator (fair coin), so
for every x there is some probability mx to get x as output. (The sum  mx
does not exceed 1; it can be less if the machine M performs a non-terminating
computation with positive probability.) There exists a universal machine M of
this type, i.e., the machine for which function x →mx is maximal up to a
constant factor. (For example, M can start by choosing a random machine in
such a way that every choice has positive probability, and then simulate this
machine.) We ﬁx some universal machine M and call the probability mx to get
x on its output a priori probability of x.
The relation between preﬁx complexity and a priory probability is quite close:
Levin and Chaitin have shown that K(x) = −log2 mx + O(1). However, the
situation changes if we extend preﬁx complexity and a priori probability to sets.
Let X be a set of natural numbers. Then we can consider two quantities that
measure the diﬃculty of a task “produce some element of X”: complexity of
X, deﬁned as the minimal length of a program that produces some element in
X, and a priori probability of X, the probability to get some element of X as
2 Solomonoﬀalso mentioned the optimal description method as a technical tool some-
where in his paper.

Game Arguments in Computability Theory
661
an output of the universal machine M. For an arbitrary set of integers these
two quantities are not related in the same way as before: complexity can diﬀer
signiﬁcantly from the minus logarithm of a priori probability. In other words, for
an arbitrary set X the quantities
max
x∈X mx
and

x∈X
mx
(the ﬁrst one corresponds to the complexity, the second one corresponds to
a priori probability) could be very diﬀerent. For example, if X is the set of
strings of length n that have complexity close to n, the ﬁrst quantity is rather
small (since all mx are close to 2−n) while the second one is quite big (a string
chosen randomly with respect to the uniform distribution on n-bit strings, has
complexity close to n with high probability).
Epstein–Levin theorem says that such a big diﬀerence is not possible if the
set X is stochastic. The notion of a stochastic object was introduced in the al-
gorithmic statistics. A ﬁnite object X is called stochastic if there exist a ﬁnite
distribution (a probability distribution with ﬁnite domain and rational proba-
bilities) P that has small complexity, and the randomness deﬁciency of X with
respect to P, deﬁned as −log P(X) −K(X|P), is small.
We do not go into the further details about Epstein–Levin theorem. Instead
we just present a game that corresponds to it and prove that one of the players
has a computable winning strategy; interested readers can check that indeed the
existence of such a strategy implies the statement of Epstein–Levin theorem.
(See also the comments after the deﬁnition of the game.)
The Epstein–Levin game happens on a ﬁnite bipartite graph E ⊂L × R with
left part L and right part R. A probability distribution ρ on R with rational
values is ﬁxed, together with three parameters: some natural number k, some
natural number l and some positive rational number δ. After all these objects
are ﬁxed, we consider the following game.
Alice assigns some rational weights to vertices in L. Initially all the weights
are zeros, but Alice can increase them during the game. The total weight of L
(the sum of weights) should never exceed 1. Bob can mark some vertices on the
left and some vertices on the right. After a vertex is marked, it remains marked
forever. The restrictions for Bob: he can mark at most l vertices on the left, and
the total ρ-probability of marked vertices on the right should be at most δ. The
winner is determined as follows: Bob wins if every vertex y on the right for which
the (limit) total weight of all its L-neighbors exceeds 2−k, either is marked itself
(at some point), or has a marked (at some point) neighbor.
Evidently, the task of Bob becomes harder if l or δ decrease (he has less
freedom in marking vertices), and becomes easier if k decreases (he needs to
take care of less vertices). So the greater k and the smaller δ is, the bigger l is
needed by Bob to win. The following lemma gives a bound (with some absolute
constant in O(1)):
Lemma 4. For l = 2k(log(1/δ) + O(1)) Bob has a computable winning strategy
in the described game.

662
A. Shen
Before we prove this lemma, let us explain the connection between this game and
the statement of Epstein–Levin theorem. Vertices in R are ﬁnite sets of integers;
vertices in L are integers, and the edges correspond to ∈-relation. Alice weights
are a priori probabilities of integers (more precisely, increasing approximations
to them). The distribution P on R is a simple distribution (on a ﬁnite family of
ﬁnite sets) that is assumed to make X (from Levin–Epstein theorem) stochastic.
Bob can mark X making it non-random with respect to P (the marked vertices
form a P-small subset and therefore all have big randomness deﬁciency), so
Epstein and Levin do not need to care about it any more. If X is not marked
and has big total weight (= the total a priori probability), X is guaranteed
to have a marked neighbor. This means that some element of X is marked and
therefore has small complexity (since there are only few marked elements); this is
what Epstein–Levin theorem requires. (Of course, one needs to use some speciﬁc
bounds instead of “small” and “large” etc., but that is all.)
Proof. To prove the existence of a winning strategy for Bob, we use the following
(quite unusual — I’ve seen only one other example) type of argument: we exhibit
a simple probabilistic strategy for Bob that guarantees some positive probability
of winning against any strategy of Alice. Since the game is essentially a ﬁnite
game with full information (see the comments at the end of the proof about how
to make it really ﬁnite), either Alice or Bob have a winning strategy. And if Alice
had one, no probabilistic strategy for Bob could have a positive probability of
winning.
Let us describe this strategy for Bob. It is rather simple: if Alice increases
weight of some vertex x in L by an additional ε > 0, Bob in his turn marks x
with probability c2kε, while c > 1 is some constant to be chosen later. If c2kε > 1
(this always happens if ε is 2−k or more), Bob marks this vertex for sure, and
this will satisfy all its R-neighbors. Note that without loss of generality we may
assume that Alice increases weights one at a time, since we can split her move
into a sequence of moves.
We have explained how Bob marks L-vertices; if at some point this does not
help, i.e., if there is a R-vertex that currently has total weight at least 2−k but
no marked neighbors, Bob marks this R-vertex.
The probabilistic strategy for Bob is described, and we need to show that the
probability of winning the game for Bob (for suitable c, see below) is positive.
It is true even for a stronger deﬁnition of winning: let us require that for each
y in R whose total weight reaches the threshold after some Alice’s move, the
requirement is satisﬁed immediately (after the next move of Bob). Fix some y.
Assume that the weights of neighbors of y were increased by ε1, . . . , εu during
the game, and  εi ≥2−k. After each increase the corresponding neighbor of y
was marked with probability c2kεi, so the probability that the requirement for
y is still false (and y is marked), does not exceed
(1 −c2kε1) · . . . · (1 −c2kεu) ≤e−c2k(ε1+...+εu) ≤e−c
(recall that (1 −t) ≤e−t and that  εi ≥2−k). Therefore the expected ρ-
measure of marked vertices on the right (the weighted average of numbers not

Game Arguments in Computability Theory
663
exceeding e−c) does not exceed e−c, and the probability to exceed 2e−c is at
most 1/2. So it is enough to let c be log2(1/δ) + O(1).
It remains to make two comments. First, the estimate for probability should
be done more accurately, since the values of ε1, . . . , εu may depend on Bob’s
moves. However, the coin tossing used to decide whether to mark some vertex
on the left, is independent of what happened before, and this makes the estimate
valid. (Formally, we need a backward induction in the tree of possibilities.)
Second, to switch from a probabilistic strategy to a deterministic one, we
need to make the game ﬁnite. One may assume that current weights of vertices
on the left all have the form 2−m for some integer m (replacing weights by
approximations from below, we can compensate for an additional factor of 2 by
changing k by 1). Still the game is not ﬁnite, since Alice can start with very
small weights. However, this is not important: the graph is ﬁnite, and all very
small weights can be replaced by some 2−m. If 2−m · #L < 1, then the sum of
weights still does not exceed 2, and this again is a constant factor.
4
Muchnik–Vyugin Theorem
Consider the following problem. Let m be some constant. Given a string x0 and
integer n, we want to ﬁnd strings x1, . . . , xm such that C(xi|xj) = n + O(1) for
all pairs of diﬀerent i, j in the range 0, . . . , m. (Note that both i and j can be
equal to 0). This is possible only if x0 has high enough complexity, at least n,
since C(x0|xj) is bounded by C(x0). It turns out that such x1, . . . , xm indeed
exist if C(x0) is high enough (though the required complexity of x0 is greater
than n), and the constant hidden in O(1)-notation does not depend on n (but
depends on m).
This statement is non-trivial even for n = 1: it says that for every n and for
every string x of high enough complexity there exists a string y such that both
C(x|y) and C(y|x) are equal to n + O(1).
Here is the exact statement (it speciﬁes also the dependence of O(1)-constant
on m):
Theorem 3 (An.A. Muchnik, M. Vyugin). For every m and n and for
every binary string x0 such that
C(x0) > n(m2 + m + 1) + O(log m)
there exist strings x1, . . . , xm such that
n ≤C(xi|xj) ≤n + O(log m)
for every two diﬀerent i, j ∈{0, . . ., n}.
Note that the high precision is what makes this theorem non-trivial (if an addi-
tional term O(log C(x0)) were allowed, one could take the shortest program for
x0 and replace ﬁrst n bits in it by m independent random strings).

664
A. Shen
Let us explain the game that corresponds to this statement. It is played on
graph with (m+1) parts X0, . . . , Xm (there are countably many vertices in each
part Xi representing possible values of xi). As usual, there are two players: Alice
and Bob. Alice may connect vertices from diﬀerent parts by undirected edges,
while Bob can connect them by directed edges. Alice and Bob make alternating
moves; at each move they can add any ﬁnite set of edges. Alice can also mark
vertices x0 in X0. The restrictions are:
– Alice may mark at most m2n+1+nm(m+1) vertices (in X0);
– for each vertex xi ∈Xi and for each j ̸= i, Alice may draw at most m(m +
1)2n undirected edges connecting xi with vertices in Xj;
– for each vertex xi ∈Xi and for each j ̸= i, Bob may draw at most 2n
outgoing edges from xi to vertices in Xj.
The game is inﬁnite. Alice wins if (in the limit) for every non-marked vertex
x0 ∈X0 there exist vertices x1, . . . , xm from X1, . . . , Xm such that each two
vertices xi, xj (where i ̸= j) are connected by an undirected (Alice) edge, but
not connected by a directed (Bob) edge.
Lemma 5. Alice has a computable winning strategy in this game.
It is easy to see how this lemma can be used to prove the statement. Imagine
that Bob draws an edge xi →xj when he discovers that C(xj|xi) < n. Then he
never violates the restriction. Alice can computably win against this strategy;
every marked vertex then has small complexity, since a marked vertex can be
described by its ordinal number in the enumeration order. For every non-marked
vertex x0 there exist x1, . . . , xm that satisfy the winning conditions. For them
C(xj|xi) ≥n (otherwise Bob would connect them by a directed edge), and
C(xj|xi) ≤n + O(log m), since xj can be obtained from xi if we know i, j, and
the ordinal number of undirected edge xi–xj (among all the edges that connect
xi to Xj).
So it remains to prove the lemma. Alice does the following. First of all, it
groups all vertices into disjoint cliques of size m+1 that contain vertices from all
m+1 parts; each vertex is included in one of the cliques. (Formally Alice cannot
create inﬁnitely many edges at a time, but this can be artiﬁcially postponed.)
A clique becomes invalid when Bob creates an edge between two vertices in this
clique. Then the clique is destroyed and in some cases a new one is created (see
below). We may assume without loss of generality that Bob creates edges only
between vertices of some currently active clique, and only one edge is created at
each move (the creation of other edges can be postponed).
When a clique becomes invalid, all the vertices record this fact in their “in-
ternal memory”. Each vertex remembers m(m + 1) non-negative integers cor-
responding to ordered pairs (i, j). This tuple is called an “index” of a vertex.
When the clique becomes invalid because of Bob’s edge going from Xi to Xj,
the component of index corresponding to (i, j) is incremented by 1 in all the
vertices of the clique, the clique is disbanded, and all its vertices become free.
Alice then tries to form a new clique involving x0 from the disbanded one,
in such a way that all vertices of this clique (from all m + 1 parts) have equal

Game Arguments in Computability Theory
665
indices (that coincide with the updated index of x0), are not yet connected to
each other by Bob’s edges, and the other vertices of the clique (except x0) are
free, i.e., are not included in any current clique. If this is not possible, no clique
is created, and the vertex x0 is marked.
This strategy keeps the following invariant relations:
– for each clique, all its vertices have the same index;
– for every clique and for every pair i, j (where i ̸= j) the (i →j)th component
of indices of clique vertices equals to the number of Bob’s edges going from
xi to Xj, as well as the number of Bob’s edges going from Xi to xj (where
xi and xj are clique vertices in Xi and Xj); in particular, all components of
the index are less than 2n (since they are equal to the number of outgoing
edges from xi to Xj); here we use that Bob at each step draws only one
edge, and this edge is parallel to some edge of some active clique;
– at every step only ﬁnitely many vertices have non-zero indices, and for each
value of the index we have the same number of vertices of that index in all
the parts Xi.
– the number of free vertices in all the parts is the same; in X0 free vertices
are marked; the number of free vertices of a given index is also the same in
all parts;
To ﬁnish the proof, it remains to prove the bound for the number of marked
vertices. For that we estimate the number of marked vertices of each index (the
number of possible indices is bounded by 2nm(m+1) since its components are
bounded by 2n). The idea here is simple: if we have many (more than 2m2n)
free vertices of some index, we can always ﬁnd a clique (made of them) for
every vertex x0 ∈X0 of that index that has lost its old clique: indeed, we do it
sequentially in X1, . . . , Xm, and at every step we can ﬁnd a vertex that is not
connected to already selected vertices, since the number of outgoing edges, as
well as the number of incoming edges, is bounded by n2m.
Acknowledgments. Supported in part by NAFIT EMER-008-01 grant and
RFBR 09-01-00709-a grant. The author is grateful to Leonid Levin, Peter G´acs,
the participants of Kolmogorov seminar (Moscow) and his colleagues in LIRMM
(Montpellier) and LIAFA (Paris). M. Vyugin kindly allowed me to include the
unpublished result of An. Muchnik (1958–2007) and himself in Section 4.
References
1. Bauwens, B.: Computability in statistical hypotheses testing, and characterizations
of independence and directed inﬂuences in time series using Kolmogorov complex-
ity. PhD thesis, Ugent (May 2010)
2. Bienvenu,
L.,
Shen,
A.:
Algorithmic
information
theory
and
martingales.
arXiv:0906.2614v1 (2009) (preprint)
3. Epstein, S., Levin, L.A.: Sets Have Simple Members. arXiv:1107.1458v5 (2011)
(preprint)

666
A. Shen
4. Friedberg, R.: Three theorems on recursive numberings. J. of Symbolic Logic 23,
309–316 (1958)
5. Muchnik, A.A.: On the basic structures of the descriptive theory of algorithms.
Soviet Math. Dokl. 32, 671–674 (1985)
6. Muchnik, A.A., Mezhirov, I., Shen, A., Vereshchagin, N.: Game interpretation of
Kolmogorov complexity. arXiv:1003.4712v1 (2010) (preprint)
7. Vereshchagin, N.: Kolmogorov complexity and Games. Bulletin of the European
Association for Theoretical Computer Science 94, 51–83 (2008)
8. Muchnik, A.A., Vyugin, M.: Information distance for complex strings (2007) (un-
published work)
9. Shen, A.: Algorithmic Information Theory and Kolmogorov Complexity, Technical
Report, Uppsala University, TR2000-034,
http://www.it.uu.se/research/publications/reports/2000-034/
10. Vereshchagin, N.K., Vitanyi, P.M.B.: Rate Distortion and Denoising of Individ-
ual Data Using Kolmogorov Complexity. IEEE Transactions on Information The-
ory 56(7), 3438–3454 (2010)

Turing Patterns in Deserts
Jonathan A. Sherratt
Department of Mathematics and Maxwell Institute for Mathematical Sciences,
Heriot-Watt University, Edinburgh EH14 4AS, United Kingdom
jas@ma.hw.ac.uk
Abstract. Self-organised patterns of vegetation are a characteristic fea-
ture of many semi-arid regions. In particular, banded vegetation is typ-
ical on hillsides. Mathematical modelling is widely used to study these
banded patterns, because there are no laboratory replicates. I will de-
scribe the development of spatial patterns in an established model for
banded vegetation via a Turing bifurcation. I will discuss numerical sim-
ulations of the phenomenon, and I will summarise nonlinear analysis on
the existence and form of spatial patterns as a function of the model
parameter that corresponds to mean annual rainfall.
1
Introduction
Self-organised patterns of vegetation are a characteristic feature of semi-deserts.
The most striking and best studied example is striped patterns on gentle slopes
(see [1, 2] for review). These occur in many parts of the world, and are partic-
ularly well documented in Australia [3, 4], Mexico/South-Western USA [5, 6]
and sub-Saharan Africa [7–9] Bands of grass, shrubs or trees run along contours,
separated by bare ground; wavelengths of about 1km are typical for trees and
shrubs, with shorter wavelengths observed for grasses.
There are no laboratory replicates of banded vegetation, so that empirical
study is limited to observation of existing patterns. Because the timescale of
pattern evolution is very slow (decades), such observational data is ineﬀective
as a basis for assessing the implications of changes in environmental parameters
such as rainfall. Therefore theoretical models are an important and widely used
tool for studying these patterns [10]. This paper is concerned with pattern for-
mation in one model for banded vegetation, due originally to Klausmeier [11].
It comprises coupled partial diﬀerential equations for plant and water densities,
and is the basic model for patterning due to water redistribution. Many exten-
sions of the Klausmeier model have been proposed over the last decade. Most of
these involve separate variables for soil and surface water [12–16]. Some authors
have also incorporated features such as rainfall variability [17–19] and a herbi-
vore population [20]; see [21, 22] for other recent extensions. Note also that the
Klausmeier model and its extensions are not the only theoretical explanation for
vegetation stripes. Lejeune and coworkers [23–26] have studied in detail a model
based on the combination of short-range activation and long-range inhibition
between neighbouring plants. Here the activation is due to shading of one plant
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 667–674, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

668
J.A. Sherratt
by another, while competition for water results in inhibition; the diﬀerence in
length scales of these processes is due to the root system within the soil being
much more extensive than the parts of the plants above ground. In this model,
slope acts as a selector rather than an initiator of spatial patterning.
This paper is concerned with the original Klausmeier model [11]. I will present
a detailed discussion of pattern solutions of this model, which arise via a Turing
bifurcation in the model partial diﬀerential equations. I will show that studying
these pattern solutions can provide valuable new ecological insights into the
formation and maintainance of vegetation patterns in semi-deserts.
2
Model Equations
The dimensionless form of the Klausmeier model is:
∂u/∂t =
plant
growth

wu2 −
plant
loss

Bu +
plant
dispersal



∂2u/∂x2
(1a)
∂w/∂t =
A

rain-
fall
−
w

evap-
oration
−
wu2

uptake
by plants
+ ν∂w/∂x



ﬂow
downhill
.
(1b)
Here u(x, t) is plant density, w(x, t) is water density, t is time and x is space in
a one-dimensional domain of constant slope, with the positive direction being
uphill. The (dimensionless) parameters A, B and ν reﬂect rainfall, plant loss
and slope gradient respectively. For full details of the dimensional model and
nondimensionalisation, see Klausmeier (1999), Sherratt (2005) or Sherratt &
Lord (2007).
For all parameter values, (1) has a stable trivial steady state u = 0, w = A,
corresponding to bare ground, without vegetation. When A ≥2B, there are also
two other homogeneous steady states which arise from a saddle node bifurcation:
u = u1 ≡
2B
A −
√
A2 −4B2 ,
w = w1 ≡A −
√
A2 −4B2
2
(2)
and
u = u2 ≡
2B
A +
√
A2 −4B2 ,
w = w2 ≡A +
√
A2 −4B2
2
.
(3)
The ﬁrst of these (2) is always unstable to homogeneous perturbations; the
second is the key equilibrium from which patterns develop. This steady state is
linearly stable to homogeneous perturbations whenever B < 2. For larger values
of B and small A, (3) can become unstable, giving complicated local dynamics
including a limit cycle, but realistic parameter values for semi-arid environments
imply that B < 2.
For large values of the rainfall parameter A, (3) is also stable to inhomoge-
neous perturbations, so that the model predicts the spatially uniform vegetation
that characterises temperate parts of the world. However as A is decreased a Tur-
ing bifurcation occurs: (3) becomes unstable to some spatially inhomogeneous

Turing Patterns in Deserts
669
perturbations, and spatial patterns develop. The patterns consist of periodically
repeating peaks and troughs of vegetation (Figure 1), and as the rainfall pa-
rameter A is decreased further, these solutions gradually increase in amplitude,
resembling more closely the empirically observed patterns. In the prototypical
Turing system of two coupled reaction-diﬀusion equations, the patterns arising
from a Turing bifurcation are stationary. However the advection term in (1a)
causes the patterns to move, in the positive x direction (uphill). There has been
a long-running debate in the ecological literature about this uphill migration,
with some ﬁeld studies reporting stationary patterns (e.g., [3]). However, the
majority of data sets spanning a time period suﬃcient to address this issue do
indicate uphill migration, with speeds in the range 0.2–1 m year−1 (see Table 5
of [1]). A recent and very detailed study using photographic data from satellites
[27, Chapter 10] conﬁrms migration, with speeds in this range, for three out
of six geographical locations. The ecological cause of uphill migration is that
moisture levels are higher on the uphill edge of the bands than on their downhill
edge, leading to reduced plant death and greater seedling density [28, 29].
3
Travelling Wave Solutions
Mathematically, patterns moving with constant shape and speed can be studied
via the ansatz u(x, t) = U(z) and w(x, t) = W(z), where z = x −ct with c
being the migration speed. Substituting these solution forms into (1) gives the
travelling wave equations
d2U/dz2 + c dU/dz + WU 2 −BU = 0
(4a)
(ν + c)dW/dz + A −W −WU 2 = 0 .
(4b)
Patterned solutions correspond to periodic solutions of (4). In [30], Gabriel Lord
and I used numerical bifurcation analysis to study these periodic solutions. We
showed that for a given value of the migration speed c, patterns occur for a
range of rainfall parameter values A. For most values of c, this range is bounded
by a Hopf bifurcation point for (3,4) and a homoclinic solution of (4). However
for some values of c there is a fold in the branch of periodic travelling wave
solutions, and this then constitutes on end of the rainfall range for patterns
[31, 32]. A typical result is illustrated in Figure 2, which shows the loci of the
Hopf bifurcation point and the homoclinic solution in the A–c parameter plane,
for ﬁxed values of B and ν.
Analytical study of (1) is made more complicated by the advective term in the
u-equation. For example, linear stability analysis of (3) to investigate the Turing
bifurcation is signiﬁcantly more complicated in (1) than for a system of two
reaction-diﬀusion equations [34], and indeed one cannot obtain an exact closed-
form expression for the value of A at which the bifurcation occurs. However,
the slope parameter ν is much larger than A and B: Klausmeier [11] estimated
ν = 182.5, A = 0.1–3.0 and B=0.05–2.0. This large value is not due to the slope
itself being steep: banded vegetation is restricted to slopes of a few percent, and

670
J.A. Sherratt
Fig. 1. An illustration of a typical vegetation pattern, as predicted by the Klausmeier
model (1). There is a periodic pattern of peaks in vegetation density u, separated by
regions in which vegetation is almost absent. The surface water density w also has
a periodic form; it is largest on the uphill side of a vegetation stripe, and gradually
decreases with distance uphill to the next stripe. The pattern moves slowly uphill;
in this case the (dimensionless) migration speed is approximately 0.9. The parameter
values are A = 2.5, B = 0.45, ν = 182.5, which are in the range of Klausmeier’s
(1999) parameter estimates for grass. The equations were solved numerically using a
ﬁnite diﬀerence scheme (see [30] for details) on the domain 0 < x < 125 with periodic
boundary conditions.

Turing Patterns in Deserts
671
Fig. 2. A typical example of the part of the A–c parameter plane in which there are
patterned solutions of (1), which corresponds to limit cycles in (4). I plot the loci
of Hopf bifurcation points (
) and homoclinic solutions (
) in (4),
which bound the pattern region. The other parameters are B = 0.45 and ν = 182.5.
The plot is truncated at c ≈20: patterns actually exist for values of c up to about
50. The numerical solutions were performed using auto [33]. The loci of homoclinic
orbits are approximations; they are in fact the loci of solutions of a ﬁxed but very long
wavelength (3000). Further details of the numerical continuation approach are given in
[30].
on steeper slopes, diﬀerent processes occur because rainwater generates gullies.
Rather, ν is large because the plant diﬀusion coeﬃcient is small compared to
the advection rate of water, and it is the relative values of these quantities that
determines the nondimensional parameter ν [11, 34]. By exploiting this large
value of ν it is possible to obtain leading order approximations for various key
points in the A–c parameter plane. In particular:
– The Turing bifurcation occurs at A = (
√
2−1)1/2ν1/2B5/4, c = A2/(2B2ν)+
B3ν/(2A2) [32].
– The maximum migration speed for patterns is c = νB/(2 −B) [31].
– The
base
of
the
“tusk-shaped”
region
(see
Figure
2)
occurs
at
c = 0.881B3/4ν1/2 [35, 36].
– Pattern solutions exist for arbitrarily small values of the migration speed c
[32].

672
J.A. Sherratt
4
Conclusion
Ecological pattern formation at the level of whole ecosystems is a new, exciting,
and rapidly growing research area, whose study is inﬂuenced strongly by Turing’s
ideas [37]. Vegetation patterns in semi-arid regions represent one example of
such patterns, but there are many others, including regular isolated spots of
trees and shrubs in savanna grasslands [38, 39], patterns of open-water pools in
peatlands [40, 41], labyrinthine patterns in mussel beds [42, 43], striped patterns
of tree lines (“ribbon forests”) in the Rocky Mountains [44, 45]. Mathematical
modelling is an important tool for the study of landscape patterns, and the
Klausmeier model (1) is one of the most generic models: as well as semi-arid
vegetation, it has been used to model fog-dependent plant ecosystems [46] and
(with a slight modiﬁcation) mussel beds in river estuaries [47]. I have outlined the
mathematical analysis of pattern formation in the Klausmeier model, showing
how such analysis can make clear and quantitative predictions concerning critical
levels of the rainfall level and wave speed.
References
1. Valentin, C., d’Herb`es, J.M., Poesen, J.: Soil and water components of banded
vegetation patterns. Catena 37, 1–24 (1999)
2. Rietkerk, M., Dekker, S.C., de Ruiter, P.C., van de Koppel, J.: Self–organized
patchiness and catastrophic shifts in ecosystems. Science 305, 1926–1929 (2004)
3. Dunkerley, D.L., Brown, K.J.: Oblique vegetation banding in the Australian arid
zone: implications for theories of pattern evolution and maintenance. J. Arid En-
viron. 52, 163–181 (2002)
4. Berg, S.S., Dunkerley, D.L.: Patterned mulga near Alice Springs, central Australia,
and the potential threat of ﬁrewood collection on this vegetation community. J.
Arid Environ. 59, 313–350 (2004)
5. Monta˜na, C.: The colonization of bare areas in two–phase mosaics of an arid ecosys-
tem. J. Ecol. 80, 315–327 (1992)
6. McDonald, A.K., Kinucan, R.J., Loomis, L.E.: Ecohydrological interactions within
banded vegetation in the northeastern Chihuahuan Desert, USA. Ecohydrology 2,
66–71 (2009)
7. MacFadyen, W.: Vegetation patterns in the semi–desert plains of British Soma-
liland. Geographical J. 115, 199–211 (1950)
8. Valentin, C., d’Herb`es, J.M.: Niger tiger bush as a natural water harvesting system.
Catena 37, 231–256 (1999)
9. Couteron, P., Mahamane, A., Ouedraogo, P., Seghieri, J.: Diﬀerences between
banded thickets (tiger bush) at two sites in West Africa. J. Veg. Sci. 11, 321–328
(2000)
10. Borgogno, F., D’Odorico, P., Laio, F., Ridolﬁ, L.: Mathematical models of vegeta-
tion pattern formation in ecohydrology. Rev. Geophys. 47, art. no. RG1005 (2009)
11. Klausmeier, C.A.: Regular and irregular patterns in semiarid vegetation. Sci-
ence 284, 1826–1828 (1999)
12. HilleRisLambers, R., Rietkerk, M., van de Bosch, F., Prins, H.H.T., de Kroon,
H.: Vegetation pattern formation in semi–arid grazing systems. Ecology 82, 50–61
(2001)

Turing Patterns in Deserts
673
13. Rietkerk, M., Boerlijst, M.C., van Langevelde, F., HilleRisLambers, R., van de
Koppel, J., Prins, H.H.T., de Roos, A.: Self–organisation of vegetation in arid
ecosystems, Am. Am. Nat. 160, 524–530 (2002)
14. Gilad, E., von Hardenberg, J., Provenzale, A., Shachak, M., Meron, E.: A math-
ematical model of plants as ecosystem engineers, J. J. Theor. Biol. 244, 680–691
(2007)
15. Ursino, N.: Modeling banded vegetation patterns in semiarid regions: inter–
dependence between biomass growth rate and relevant hydrological processes. Wa-
ter Resour. Res. 43, W04412 (2007)
16. Ursino, N.: Above and below ground biomass patterns in arid lands. Ecological
Modelling 220, 1411–1418 (2009)
17. Ursino, N., Contarini, S.: Stability of banded vegetation patterns under seasonal
rainfall and limited soil moisture storage capacity. Adv. Water Resour. 29, 1556–
1564 (2006)
18. Guttal, V., Jayaprakash, C.: Self–organisation and productivity in semi–arid
ecosystems: implications of seasonality in rainfall. J. Theor Biol. 248, 290–500
(2007)
19. Kletter, A.Y., von Hardenberg, J., Meron, E., Provenzale, A.: Patterned vegetation
and rainfall intermittency. J. Theor. Biol. 256, 574–583 (2009)
20. van de Koppel, J., Rietkerk, M., van Langevelde, F., Kumar, L., Klausmeier, C.A.,
Fryxell, J.M., Hearne, J.W., van Andel, J., de Ridder, N., Skidmore, M.A., Stroos-
nijder, L., Prins, H.H.T.: Spatial heterogeneity and irreversible vegetation change
in semiarid grazing systems. Am. Nat. 159, 209–218 (2002)
21. Pueyo, Y., Keﬁ, S., Alados, C.L., Rietkerk, M.: Dispersal strategies and spatial
organization of vegetation in arid ecosystems. Oikos 117, 1522–1532 (2008)
22. Keﬁ, S., Rietkerk, M., Katul, G.G.: Vegetation pattern shift as a result of rising
atmospheric CO2 in arid ecosystems. Theor. Pop. Biol. 74, 332–344 (2008)
23. Lefever, R., Lejeune, O.: On the origin of tiger bush. Bull. Math. Biol. 59, 263–294
(1997)
24. Couteron, P., Lejeune, O.: Periodic spotted patterns in semi–arid vegetation ex-
plained by a propagation–inhibition model. J. Ecol. 89, 616–628 (2001)
25. Barbier, N., Couteron, P., Lefever, R., Deblauwe, V., Lejeune, O.: Spatial de-
coupling of facilitation, competition at the origin of gapped vegetation patterns.
Ecology 89, 1521–1531 (2008)
26. Lefever, R., Barbier, N., Couteron, P., Lejeune, O.: Deeply gapped vegetation pat-
terns: on crown/root allometry, criticality and desertiﬁcation. J. Theor. Biol. 261,
194–209 (2009)
27. Deblauwe, V.: Modulation des structures de v´eg´etation auto–organis´ees en milieu
aride / Self–organized vegetation pattern modulation in arid climates. PhD thesis,
Universit´e Libre de Bruxelles (2010)
28. Tongway, D.J., Ludwig, J.A.: Theories on the origins, maintainance, dynamics,
and functioning of banded landscapes. In: Tongway, D.J., Valentin, C., Seghieri,
J. (eds.) Banded Vegetation Patterning in Arid and Semi–Arid Environments, pp.
20–31. Springer, New York (2001)
29. Monta˜na, C., Seghieri, J., Cornet, A.: Vegetation dynamics: recruitment, regen-
eration in two–phase mosaics. In: Tongway, D.J., Valentin, C., Seghieri, J. (eds.)
Banded Vegetation Patterning in Arid and Semi–Arid Environments, pp. 132–145.
Springer, New York (2001)
30. Sherratt, J.A., Lord, G.J.: Nonlinear dynamics, pattern bifurcations in a model for
vegetation stripes in semi–arid environments. Theor. Pop. Biol. 71, 1–11 (2007)

674
J.A. Sherratt
31. Sherratt, J.A.: Pattern solutions of the Klausmeier model for banded vegetation in
semi–arid environments II: patterns with the largest possible propagation speeds.
Proc. R. Soc. Lond. A 467, 3272–3294 (2011)
32. Sherratt, J.A.: Pattern solutions of the Klausmeier model for banded vegetation
semi–arid environments IV: slowly moving patterns and their stability (submitted)
33. Doedel, E.J.: AUTO, a program for the automatic bifurcation analysis of au-
tonomous systems. Cong. Numer. 30, 265–384 (1981)
34. Sherratt, J.A.: An analysis of vegetation stripe formation in semi–arid landscapes.
J. Math. Biol. 51, 183–197 (2005)
35. Sherratt, J.A.: Pattern solutions of the Klausmeier model for banded vegetation
in semi–arid environments I. Nonlinearity 23, 2657–2675 (2010)
36. Sherratt, J.A.: Pattern solutions of the Klausmeier model for banded vegetation
in semi–arid environments III: the transition between homoclinic solutions (sub-
mitted)
37. Turing, A.M.: The chemical basis of morphogenesis. Phil. Trans. R. Soc. Lond.
B 237, 37–72 (1952)
38. Lejeune, O., Tlidi, M., Couteron, P.: Localized vegetation patches: a self–organized
response to resource scarcity. Phys. Rev. E 66, 010901 (2002)
39. Ben Wu, X., Archer, S.R.: Scale–dependent inﬂuence of topography–based hydro-
logic features on patterns of woody plant encroachment in savanna landscapes.
Landscape Ecol. 20, 733–742 (2005)
40. Belyea, L.R.: Climatic and topographic limits to the abundance of bog pools.
Hydrological Processes 21, 675–687 (2007)
41. Eppinga, M.B., de Ruiter, P.C., Wassen, M.J., Rietkerk, M.: Nutrients and hydrol-
ogy indicate the driving mechanisms of peatland surface patterning. Am. Nat. 173,
803–818 (2009)
42. van de Koppel, J., Rietkerk, M., Dankers, N., Herman, P.M.J.: Scale–dependent
feedback and regular spatial patterns in young mussel beds. Am. Nat. 165, E66–E77
(2005)
43. van de Koppel, J., Gascoigne, J.C., Theraulaz, G., Rietkerk, M., Mooij, W.M., Her-
man, P.M.J.: Experimental evidence for spatial self–organization and its emergent
eﬀects in mussel bed ecosystems. Science 322, 739–742 (2008)
44. Hiemstra, C.A., Liston, G.E., Reiners, W.A.: Observing, modelling, and validat-
ing snow redistribution by wind in a Wyoming upper treeline landscape. Ecol.
Modelling 197, 35–51 (2006)
45. Bekker, M.F., Clark, J.T., Jackson, M.W.: Landscape metrics indicate diﬀerences
in patterns and dominant controls of ribbon forests in the Rocky Mountains, USA.
Applied Vegetation Science 12, 237–249 (2009)
46. Borthagaray, A.I., Fuentes, M.A., Marquet, P.A.: Vegetation pattern formation in
a fog–dependent ecosystem. J. Theor. Biol. 265, 18–26 (2010)
47. Wang, R.H., Liu, Q.X., Sun, G.Q., Jin, Z., van de Koppel, J.: Nonlinear dynamic
and pattern bifurcations in a model for spatial patterns in young mussel beds. J.
R. Soc. Interface 6, 705–718 (2009)

Subsymbolic Computation Theory
for the Human Intuitive Processor
Paul Smolensky
Department of Cognitive Science, Johns Hopkins University, 237 Krieger Hall, 3400
North Charles Street, Baltimore, Maryland 21218, United States of America
smolensky@jhu.edu
Abstract. The classic theory of computation initiated by Turing and
his contemporaries provides a theory of eﬀective procedures—algorithms
that can be executed by the human mind, deploying cognitive processes
constituting the conscious rule interpreter. The cognitive processes con-
stituting the human intuitive processor potentially call for a diﬀerent
theory of computation. Assuming that important functions computed by
the intuitive processor can be described abstractly as symbolic recursive
functions and symbolic grammars, we ask which symbolic functions can
be computed by the human intuitive processor, and how those functions
are best speciﬁed—given that these functions must be computed using
neural computation. Characterizing the automata of neural computa-
tion, we begin the construction of a class of recursive symbolic functions
computable by these automata, and the construction of a class of neural
networks that embody the grammars deﬁning formal languages.
1
Eﬀective Procedures vs. Intuitive Cognitive Processes
What is the set of functions computable by human intuition? Intuitive cognitive
processes contrast with conscious rule interpretation [10], the mental processes
enabling a clerk to take a list of English instructions and carry out a complex
mathematical calculation by consciously interpreting those instructions one after
another. Less mundanely, the Entscheidungsproblem also concerns the capabili-
ties of conscious rule interpretation: Turing’s 1936 paper [15] and other classic
works provide a theory of the set of functions computable by human conscious
rule interpretation, i.e., by eﬀective procedures. Other notions of computation
yielding diﬀerent sets of computable functions derive from alternative concep-
tions of ‘machine’ in the sense of physical artifact (e.g., diﬀerential analyzer,
optical or quantum computer).
But it is a natural biological system, the brain, that motivates the conception
of computation of interest here. Within one stream of cognitive science [11],
models of the brain and models of mental processes converged around 30 years
ago upon a class of computational systems for modeling intuition, a class known
variously as ‘connectionist’, ‘parallel distributed processing’, or ‘abstract neural
network’ computational architectures [8].
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 675–685, 2012.
  Springer-Verlag Berlin Heidelberg 2012

676
P. Smolensky
Since the dawn of modern cognitive science in the middle of the 20th cen-
tury, many cognitive scientists have treated intuition as formally identical with
conscious rule interpretation, but inaccessible to consciousness [11]. Within this
conception, all higher mental processes, and not just conscious rule interpreta-
tion, can be modeled as eﬀective procedures using classical computation. Newell
[5], for example, famously identiﬁed the architecture of the human mind with
that of a universal classical computer: what he called a ‘physical symbol system’.
This mainstream approach can be called the symbolic paradigm: such compu-
tation centers on symbols which individually serve syntactically as the tokens
of manipulation and semantically as the elements interpretable in terms of the
contents of consciousness.
The connectionist view, in contrast, models intuitive cognition within a dif-
ferent class of computational system, which we call subsymbolic [10] (after [1]).
Section 2 characterizes subsymbolic computation via a class of automata.
Sections 4 and 5 consider classes of subsymbolically-computable functions re-
spectively derived from recursive equations and from formal languages.
The research program we consider (formally synopsized in [12]) is called
subsymbolic rather than nonsymbolic because, unlike other research explor-
ing connectionist computation (e.g. [3]), the working hypothesis is that—at a
less ﬁne-grained, more abstract level of description—the functions computed by
intuitive processes are often well approximated by symbolic descriptions. The
intuitive mental processes that actually compute these functions do not however
admit a symbolic description: these processes require subsymbolic, connection-
ist descriptions. A central question then is: which symbolic functions can the
human intuitive processor compute? The formal foundation for pursuing this
question is laid in Section 3 and revisited in Section 6.
We return brieﬂy in Section 7 to implications for cognitive science of some of
the results we present, but until then we put aside the psychological motivation
for connectionist computation—from theories of intuitive cognitive processes—
and focus on the biological motivation. Speciﬁcally, we consider the formal ca-
pabilities of (a certain conception of) neural computation.
An important part of the mind-body problem is to connect the mental to
the physical brain, and computational reduction oﬀers the ﬁrst prospect for
carrying this out rigorously. This requires, however, that the computational ar-
chitecture deployed has primitive operations, data, and combinators that a brain
can provide. And this is what the connectionist architectures employed in the
subsymbolic paradigm achieve, according to our current best understanding of
the appropriate level of neural organization. To summarize the subsequent dis-
cussion: a mental concept is encoded in the activity of many neurons, not a
single one; the activity of a neuron is a continuously varying quantity at the rel-
evant level of analysis; combination of information from multiple neurons has an
approximately linear character, while the consequences of this combination for
neural activation has a non-linear character that imposes minimum and max-
imum levels. The primitives provided by the continuous architectures of the
subsymbolic paradigm are within the capabilities of the brain—that the brain

Subsymbolic Computation Theory for the Human Intuitive Processor
677
has greater complexity than assumed in these architectures does not compromise
the subsymbolic paradigm’s reduction of mental to neural computation.
While brain theory is far from achieving a settled state, this conception of
continuous neural processing has generally displaced earlier notions according to
which the relevant level of analysis was taken to be one where neural activations
are binary (ﬁring/not-ﬁring), as assumed by the early discrete-computational
network descriptions of the brain developed by Turing [16] as well as McCulloch
and Pitts [4]. Similarly, early on, the search for the meaning of neural activation
targeted individual cells, but recent years have seen an explosion of research in
which neural meaning is sought by recording ‘population codes’ over hundreds
of neurons, or patterns of aggregated activity over hundreds of thousands of
neurons (using functional Magnetic Resonance Imaging).
2
Subsymbolic Automata
From an automata-theoretic perspective, computation is characterized by four
fundamental properties [8, and references therein].
1. The tokens manipulated by primitive operations are ‘subsymbolic’: they
reside at a level lower than that of the symbols of the symbolic paradigm. Each
token is called the ‘activation value’ of a ‘unit’ (or ‘neuron’). Tokens are related
many-to-many to consciously accessible concepts: a single concept is realized by
a particular combination of many subsymbolic tokens—a ‘pattern of activity’—
and a single token is simultaneously part of the realization of multiple concepts.
This crucial property is called distributed representation of concepts.
2. The tokens are real numbers, of which a given automaton has a ﬁxed num-
ber n: a state is an element of Rn. The primitive operations are those of contin-
uous numerical (as opposed to discrete symbolic) computation. These primitives
include multiplication and exponentiation, but not determination of whether two
tokens are of the same type, nor binding a value to a variable.
3. Each automaton is a continuously evolving dynamical system: diﬀerential
equations play the role of algorithms. The primitive operations combine not
sequentially, but in parallel, as all activation values simultaneously evolve in
time according to the dynamical equations.
4. The dynamical equations are quasi-linear: if ak(t) denotes the kth activation
value at time t, then the dynamics is given by dak/dt = σ([W · a(t)]k) −ak(t)
where a(t) is the collection of all activation values at time t, {ak(t)}n
k=1. The
state space of all possible a is assumed to be vector space, and W is a matrix
of values {Wkj}n
j,k=1 called ‘connection weights’ (or ‘synaptic strengths’): Wkj
is the weight of the ‘connection’ from unit j to unit k. W · a is the matrix-
vector product: [W · a(t)]k ≡
j Wkjaj(t) is called the ‘net input to unit k’.
The possibly non-linear function σ : R →R can serve to keep activation values
within certain limits; a common choice is the logistic function σ(z) = 1/[1+e−z].

678
P. Smolensky
The space of dynamical equations available to subsymbolic models is of course
greater than that given here; what is most crucial for our purposes is that the
representational states of the computational system form a vector space.1
Such a subsymbolic system is often depicted as a network, a graph with the
units as nodes (labelled by activation values), and the connections as links (la-
belled by weights).
An important special case is the linear associator. First, σ(z) ≡z; then the
equilibrium state satisﬁes W · a = a. Second, the network is two-layer, feed-
forward: the units split into a set of input units (with activation vector i) and a
set of output units (o), with non-zero weights only from input to output units.
Then the equilibrium state satisﬁes ˆ
W · i = o and the network is equivalent to a
linear transformation (given by the matrix ˆ
W of input-to-output weights) from
the vector space of input vectors i to the vector space of output vectors o.
The implications of distributed representation—of taking vectors in Rn as the
basic data type of the computational architecture—are many [14]. The basic op-
erations of continuous mathematics provided by vector space theory now do the
fundamental computational work. Consider, for example, language processing,
a system of intuitive processes that is critical for cognitive science. To preview
the remainder of the article: Instead of stringing together two conceptual-level
symbols to form Frodosubject livespredicate, we add together two vectors, the
ﬁrst encoding ‘Frodo as subject’ and the second ‘lives as predicate’. The vec-
tor encoding ‘Frodo as subject’ results from taking a vector encoding Frodo
and a vector encoding ‘subject’ and combining them with a vector operation,
the tensor product. The basic mapping operation of vector space theory, linear
transformation, provides the core of mental processing. Vector similarity, deﬁned
using the vector inner product, serves to model conceptual similarity, a central
notion of psychological theory. And a notion from dynamical systems theory
proves to have major implications as well: the diﬀerential equations governing
many subsymbolic models have the property that as time progresses, a quantity
called the Harmony steadily increases. Harmony can be interpreted as a nu-
merical measure of the well-formedness of the network’s activation vector with
respect to the weights. The objective of network computation is to produce the
representation (vector) that is maximally well-formed—optimal. And Harmony
turns out to bear a deep relation to well-formedness as deﬁned by a grammar.
We now examine some of the details behind this summary, and proceed to
consider some of the implications for intuitive computation in the human mind.
3
From Symbolic to Vectorial Representations
At the foundation of the subsymbolic (but not the nonsymbolic) connectionist
program is the reduction of symbolic conceptual-level mental representations to
1 A ﬁfth property—learning—is key to much connectionist modeling; the connection
Wkj is typically ‘learned’ from statistical inference concerning the co-activation of
units k and j during ‘training’, in which example input-output pairs of a target func-
tion are presented. However our concern here is not learnability but computability.

Subsymbolic Computation Theory for the Human Intuitive Processor
679
more ﬁne-grained vectorial subconceptual representations. The vectorial embed-
ding of a set of symbol structures S proceeds as follows [14, ch. 5]. Throughout,
we use the example of binary trees with node labels from an alphabet A.
First, each symbol structure is mapped to a set of (symbolic) ﬁller/role bind-
ings via a ﬁller-role decomposition. For example, a particular role rx might be
identiﬁed with a node position x (e.g., left child of right child of root, denoted
by the bit string x = 01); among the bindings for a particular tree s ∈S, this
role would be bound to a particular symbol f ∈A just in case the symbol f
labels the node position x in the tree s; this binding is written f/rx and the set
of all bindings of s is β(s).
Next, a vector space VF called the ﬁller (vector) space is selected, along with
an injection ψF from the set of symbolic ﬁllers to VF . Similarly, a role (vector)
space VR is selected, along with an injection ψR from the set of symbolic roles
to VR. Then VF and VR are combined by the tensor product to form the actual
representational (vector) space for trees VS ≡VF ⊗VR.2
Finally, the ﬁller-role decomposition and vectorial realizations are combined:
the vector realization of s, ψS(s) ∈VS, is the sum of the vector realizations
of the ﬁller-role bindings of s, each of which uses the tensor product to bind
together the encodings of the ﬁller and the role:
ψS(s) =

f/r∈β(s)
ψF (f) ⊗ψR(r)
Henceforth, ψS will be abbreviated ψ.
For binary trees, we use a role realization obeying the recursive requirements
ψR(r0x) = ψR(r0) ⊗ψR(rx),
ψR(r1x) = ψR(r1) ⊗ψR(rx)
where 0x denotes the concatenation of the bit 0 with the bit-string x. Thus,
e.g., ψR(r010) = ψR(r0) ⊗ψR(r1) ⊗ψR(r0); ψR is entirely determined by ψR(r0)
and ψR(r1), the vectors in V (1)
R
realizing the roles ‘left-’ and ‘right-child of root’.
These two vectors need to be linearly independent, so dim(V (1)
R ) ≥2; for con-
creteness, we assume it equals 2. Roles for positions at tree depth d are realized
with d-fold tensor products; the total vector space VR is the direct sum of spaces
V (d)
R , d = 0, . . . , ∞for all tree depths: VR = ∞
d=0 V (d)
R , with dim(V (d)
R ) = 2d.
Likewise, VS = ∞
d=0 V (d)
S
= ∞
d=0

VF ⊗V (d)
R

= VF ⊗VR. In the case d = 0,
V (0)
R
is a 1-dimensional vector space so V (0)
S
≡VF ⊗V (0)
R
is isomorphic to VF .
2 Some deﬁnitions: Given bases {ˆfu}n
u=1 for VF and {ˆrv}m
v=1 for VR, {ˆfu ⊗ˆrv} is a basis
for the n × m-dimensional space VS. The tensor product of vectors (f, r) →f ⊗r
mapping VF × VR to VS is bilinear, i.e., linear in each of f and r independently. So
if {fu} and {rv} are respectively the elements of f ∈VF and r ∈VR with respect
to bases {ˆfu} and {ˆru}, then the elements of f ⊗r ∈VS with respect to the basis
{ˆfu ⊗ˆrv} are {furv}. The direct sum of vector spaces U and V , U ⊕V , is U × V
with the obvious linear operations: α(u, v) + β(u′, v′) ≡(αu + βu′, αv + βv′),
[A ⊕B] · (u, v) ≡(A · u, B · v); dim(U ⊕V ) = dim(U) + dim(V ).

680
P. Smolensky
A tree s of depth d = 0 is simply a symbol in A; in this case atom(s) = T
(otherwise atom(s) = F). The identity transformations on inﬁnite-dimensional
VS and VR, 2-dimensional V (1)
R , and 1-dimensional V (0)
R
are respectively denoted
 S,
 R, 1R and 1R. The identity transformation on ﬁnite-dimensional VF is 1F .
4
Linearly Computable Functions by Recursion
We shall say that a function f : S →S is linearly computable (i.e., computable
by a linear associator network) if there is a linear transformation Wf : VS →VS
such that ψS ◦f = Wf ◦ψS, i.e., the following diagram commutes:
S
f
−−−−→S
ψ
⏐⏐
⏐⏐ψ
VS
Wf
−−−−→VS
This section discusses work that begins to characterize the set of linearly com-
putable symbolic functions over binary trees (cf. [2]). We proceed from the base
case of functions over depth-0 trees (the symbols in A), to the closure under com-
position of the primitive tree-role construction and decomposition functions, to
functions deﬁned by a kind of primitive recursion.
We start then with the set B of functions fg that map every symbol A ∈A
in a tree into the symbol g(A), without changing which node it labels, where
g : A →A is some (partial) function.
The functions in B are linearly computable. In the case of interest, the vectors
in VF realizing the symbols {ψ(a)|a ∈A} are linearly independent, hence form
a basis for VF (adding additional vectors if necessary); let its dual basis be
{ψ+(a)|a ∈A} (i.e., [ψ+(a)]T ψ(a′) is 1 if a = a′ otherwise 0). Then fg ∈B is
realized by Wf ≡
a∈A ψ(g(a))[ψ+(a)]T ⊗
 R.
We next form F, the smallest set of functions that (i) includes the functions
in B, (ii) is closed under composition with functions in B, and (iii) is closed
under composition with the primitive tree functions: cons, which constructs a
new binary tree cons(p, q) ≡s with left sub-tree p and right sub-tree q; ex0 and
ex1, which extract the left sub-tree ex0(s) = p and right sub-tree ex1(s) = q
from s = cons(p, q).
The primitive tree functions are all linearly computable [14, ch. 8]. There exist
linear transformations Wf on VS satisfying, for i = 0, 1:
ψ ◦cons(p, q) = Wcons0 · ψ(p) + Wcons1 · ψ(q)
ψ ◦exi(s) = Wexi · ψ(s)
In fact any function f in F is linearly computable, by a linear transformation of
the form
Wf =
  ⊗Wf
(e.g, Wcons0 =
  ⊗Wcons0)

Subsymbolic Computation Theory for the Human Intuitive Processor
681
where
  satisﬁes the recursion relation
  = 1R +
  ⊗1R [14, ch. 8]. The solution
is
 R, which can be written (deﬁning the sum-of-powers operator ℘):
 R = ℘(1R) ≡
∞
	
d=1
(1R)⊗d ≡1R ⊕1R ⊕(1R ⊗1R) ⊕(1R ⊗1R ⊗1R) ⊕· · ·
Wf is a transformation dependent on f; the compositional map f 
→Wf is
recursively deﬁned by requirements such as
f = cons(f ′, f ′′) ⇒Wf = Wcons0 · Wf ′ + Wcons1 · Wf ′′
An example of a simple function not in F is reverse, which reverses the left
and right children of every node. This function is deﬁned by the recursion
reverse(s) =

s
if atom(s)
cons(reverse(ex1(s)), reverse(ex0(s)))
otherwise
reverse is linearly computable by a transformation 1F ⊗P where P satisﬁes the
recursion
P = 1R ⊕Wcons0 · P · Wex1 + Wcons1 · P · Wex0
A solution is
P = ℘(Wreverse)
where Wreverse ≡Wcons0 · Wex1 + Wcons1 · Wex0
More generally, consider the recursion
f(s) =

g(s)
if atom(s)
h(f(ex0(s)), f(ex1(s)))
otherwise
Suppose g and h are linearly computable:
ψ ◦g(s) = G · ψ(s) and ψ ◦h(p, q) = H0 · ψ(p) + H1 · ψ(q)
Then f will be linearly computable by a linear transformation Wf if a solution
exists to the following recursion:
Wf = G ⊕H0 · Wf · Wex0 + H1 · Wf · Wex1
A solution is
Wf = G ⊛
∞

d=0
M⊛d where M ≡H0 ⊛Wex0 + H1 ⊛Wex1
Here ⊛is a contracted tensor product operator the deﬁnition of which requires
more detail concerning the direct-sum structure of VS than is possible here.

682
P. Smolensky
5
Grammars and Optimization
Alongside recursive function theory is another classic approach to specifying
functions: via formal languages, deﬁned by grammars consisting of string rewrit-
ing rules (or productions). This approach has achieved considerable success in
characterizing intuitive mental processing of natural language (including that
underlying human performance in Turing’s Imitation Game [17]); this work cul-
minates in the theory of ‘universal grammar’, which formally characterizes what
the grammatical systems of human languages share and exactly how they may
diﬀer. (We shall discuss universal grammar in Section 7.)
The relation to subsymbolic computation arises because grammatical well-
formedness can be realized as a kind of connectionist well-formedness called
Harmony (or negative ‘energy’) [14, ch. 9, and references therein]. Harmony
arises as a quantity that is steadily increased by the dynamical processing within
an important class of network which includes those quasi-linear networks hav-
ing both a symmetric connection matrix (Wkj = Wjk) and a monotonically
increasing function σ.3 Network dynamics can be interpreted as computing a
maximal-Harmony representation—locally maximal, that is: at an equilibrium
of the dynamics, no inﬁnitesimal state change can produce higher Harmony. To
compute global Harmony maxima, stochastic diﬀerential equations can be used
to deﬁne the network dynamics (achieving probabilistic convergence as t →∞).
Can the notion of well-formedness provided by grammars that are deﬁned
by string rewriting rules be realized as network Harmony? To investigate this
question, a mediating notion—Harmonic Grammar—proves useful [14, ch. 6]. A
Harmonic Grammar HG assigns to any tree s a numerical well-formedness value
HG(s); the language speciﬁed by HG is the set of trees with maximal Harmony:
LHG ≡argmaxs∈SHG(s).
A Harmonic Grammar HG is realized in a network N iﬀfor all s ∈S, HG(s) =
HN (ψ(s)), where HN is the connectionist Harmony of N. It turns out that
the language LG generated by any rewrite-rule grammar G can be speciﬁed by
a Harmonic Grammar HG (that is, LG = LHG), and HG can be constructed
directly from the rules of G [14, ch. 10].
Further, when G is a context-free grammar in Chomsky Normal Form (hence
with binary derivation trees), our vectorial realization of binary trees can be used
to construct a network NG that realizes HG [14, ch. 8]. This HG has the form
HG(s) = 
b,b′∈β(s) Hb,b′ where for each pair of ﬁller/role bindings (b, b′) of s, the
number Hb,b′ encodes the grammatical well-formedness of b and b′ co-existing
in s. Typically in linguistics Hb,b′ is non-positive, with |Hb,b′| interpreted as the
strength in the grammar HG of the constraint: ‘do not combine b and b′ in the
same structure’.4
3 The Harmony of the state vector a of a network N with weight matrix W is HN(a) ≡
 
kj akWkjaj + 
k h(ak), h(a) ≡−
 a
0 σ−1(a′)da′.
4 E.g., if b is a singular subject (like ‘Frodo’) and b′ a plural verb form (like ‘live’),
hagree = Hb,b′ < 0 is a grammatical penalty for a verb failing to agree in number with
its subject. Adding hagreeψ(b)[ψ(b′)+]T to the weight matrix W causes the Harmony
of any structure containing b and b′ (e.g., ‘Frodo live’) to decrease by |hagree|.

Subsymbolic Computation Theory for the Human Intuitive Processor
683
When LG = LHG, any tree s0 generated by G is realized by a vector ψ(s0)
that has maximal network Harmony HNG(ψ(s0)) among the vectors {ψ(s)|s ∈S}
realizing trees (this discrete subset of VS, the image of S under ψ, will be called
“the grid”).
Within VS there are, however, vectors v* oﬀthe grid which have higher Har-
mony than those grid states realizing grammatical trees: such a vector v* does
not realize a tree; rather it realizes a pseudo-tree with weighted blends of symbols
bound to tree positions (e.g., instead of A/rx there might be [0.5A −0.1B]/rx).
Such non-grid states are optimal within the vector space because conﬂicts be-
tween the constraints encoded in H entail that optima constitute compromises
that interpolate between the alternative discrete states favored by the conﬂicting
constraints [13].
6
Quantization
In order for the network to produce bona ﬁde trees as output, another com-
ponent can be added to the dynamics of NG. This (deterministic) quantization
dynamics produces an attractor at all and only the vectors on the grid. As
the computation proceeds, this quantization dynamics comes to overpower the
stochastic Harmony-optimization dynamics, so that the ﬁnal output will be an
attractor—a grid state [13]. In a range of (very simple) applications to modeling
human language processing, simulations show these networks to be capable of
reliably computing the vectorial realizations of symbolic states that are globally-
optimal among grid states. Further, when forced to terminate too quickly, these
simulations show that the probability of outputting some error E appropriately
declines with its Harmony H(E): not only do the global optima of the Harmonic
Grammar model correct linguistic competence, the Harmonic Grammar’s evalu-
ation of sub-optimal states models the errors characteristic of human linguistic
performance.
7
Optimization and Universal Grammar
The shift from a rewrite-rule grammar G to a Harmonic Grammar HG for spec-
ifying a language may seem minor, but in fact it constitutes a fairly radical
reconception: from grammars as generators to grammars as evaluators, from
grammatical as generated to grammatical as optimal. Does this aﬀord a bet-
ter formal system for capturing the functions computed by human linguistic
intuition? There is reason to believe that it does.
Soon after the appearance of Harmonic Grammar came the empirical dis-
covery that in human grammars, it is often the case that the strength of each
constraint exceeds the combined strengths of all weaker constraints. The con-
straints can then be considered to form a strict-domination hierarchy, with each
constraint having absolute priority over all weaker constraints; while numerical
strengths are needed for the reduction to neural computation, only the prior-
ity ranking is needed to determine optimality. This is Optimality Theory [6,7],

684
P. Smolensky
which has proved to make a signiﬁcant contribution to the theory of universal
grammar via its central principle: the grammars of all human languages consist
in the same constraints; grammars may diﬀer only in how the constraints are
ranked into a strict-domination hierarchy. This means that all languages share
the same well-formedness criteria: they diﬀer only in which criteria take priority
in cases of conﬂict. The empirical successes are cases when what is universally
shared by human languages are preferences that outputs of the grammar should
respect, as opposed to processes that generate those outputs; since rewrite-rules
characterize grammatical knowledge as generative procedures, for the purposes
of universal grammar, it proves advantageous that Optimality Theory charac-
terizes grammatical knowledge as preferences over outputs [9]. In the case of
language, at least, specifying computations via optimization has been shown to
provide insight into the nature of the functions computed by human intuition.
References
1. Hofstadter, D.R.: Waking up from the Boolean dream, or, subcognition as compu-
tation. In: Hofstadter, D.R. (ed.) Metamagical Themas: Questing for the Essence
of Mind and Pattern, pp. 631–665. Bantam Books (1986)
2. Kimoto, M., Takahashi, M.: On Computable Tree Functions. In: He, J., Sato, M.
(eds.) ASIAN 2000. LNCS, vol. 1961, pp. 273–289. Springer, Heidelberg (2000)
3. McClelland, J., Botvinick, M., Noelle, D., Plaut, D., Rogers, T., Seidenberg, M.,
Smith, L.: Letting structure emerge: Connectionist and dynamical systems ap-
proaches to cognition. Trends in Cognitive Sciences 14(8), 348–356 (2010)
4. McCulloch, W., Pitts, W.: A logical calculus of the ideas immanent in nervous
activity. Bulletin of Mathematical Biology 5(4), 115–133 (1943)
5. Newell, A.: Physical symbol systems. Cognitive Science 4(2), 135–183 (1980)
6. Prince, A., Smolensky, P.: Optimality Theory: Constraint interaction in generative
grammar. Blackwell (1993/2004)
7. Prince, A., Smolensky, P.: Optimality: From neural networks to universal grammar.
Science 275(5306), 1604–1610 (1997)
8. Rumelhart, D., McClelland, J., The PDP Research Group: Parallel distributed
processing: Explorations in the microstructure of cognition. Foundations, vol. 1.
MIT Press, Cambridge (1986)
9. Rutgers Optimality Archive, http://roa.rutgers.edu
10. Smolensky, P.: On the proper treatment of connectionism. Behavioral and Brain
Sciences 11(01), 1–23 (1988)
11. Smolensky, P.: Cognition: Discrete or continuous computation? In: Cooper, S., van
Leeuwen, J. (eds.) Alan Turing — His Work and Impact. Elsevier (2012)
12. Smolensky, P.: Symbolic functions from neural computation. Philosophical Trans-
actions of the Royal Society – A: Mathematical, Physical and Engineering Sciences
(in press, 2012)
13. Smolensky, P., Goldrick, M., Mathis, D.: Optimization and quantization in gradient
symbol systems: A framework for integrating the continuous and the discrete in
cognition. Cognitive Science (in press, 2012)

Subsymbolic Computation Theory for the Human Intuitive Processor
685
14. Smolensky, P., Legendre, G.: The harmonic mind: From neural computation to
Optimality-Theoretic grammar, vol. 1: Cognitive architecture, vol. 2: Linguistic
and philosophical implications. MIT Press, Cambridge (2006)
15. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society 42, 230–265
(1936)
16. Turing, A.M.: Intelligent machinery: A report by Turing, A.M. National Physical
Laboratory (1948)
17. Turing, A.M.: Computing machinery and intelligence. Mind 59(236), 433–460
(1950)

A Correspondence Principle
for Exact Constructive Dimension
Ludwig Staiger
Institut f¨ur Informatik, Martin-Luther-Universit¨at Halle-Wittenberg,
von-Seckendorﬀ-Platz 1, D-06099 Halle, Germany
staiger@informatik.uni-halle.de
Abstract. Exact constructive dimension as a generalisation of Lutz’s
[10,11] approach to constructive dimension was recently introduced in
[19]. It was shown that it is in the same way closely related to a priori
complexity, a variant of Kolmogorov complexity, of inﬁnite sequences as
their constructive dimension is related to asymptotic Kolmogorov com-
plexity.
The aim of the present paper is to extend this to the results of [8,9,18]
(see also [2, Section 13.6]) where it is shown that the asymptotic Kol-
mogorov complexity of inﬁnite sequences in Σ0
2-deﬁnable sets is bounded
by their Hausdorﬀdimension.
Using Hausdorﬀ’s original deﬁnition one obtains upper bounds on the
a priori complexity functions of inﬁnite sequences in Σ0
2-deﬁnable sets
via the exact dimension of the sets.
Lutz’s [10,11] eﬀectivisation of classical Hausdorﬀdimension led to the deﬁnition
of constructive and computable dimensions of sets of inﬁnite sequences. He put
also the question of whether there is a correspondence principle stating that the
constructive (or computable) dimension of suﬃciently simple sets coincides with
their Hausdorﬀdimension (cf. [9]). A ﬁrst positive answer for classical dimensions
and sets deﬁnable by ﬁnite automata follows from the results of [17,12], and for
Σ0
2-deﬁnable sets positive answers were given in [8,9] and [18].
In a recent paper [19] the above mentioned results by Lutz and results by
Ryabko were generalised from the case of ‘usual’ (classical) constructive and
Hausdorﬀdimension to the case of exact dimension [5,7]. This concerns Lutz’s
martingale characterisation of Hausdorﬀdimension and Ryabko’s [15] (see also
[1]) determining of the dimension of the level sets of the constructive dimension
(or asymptotic Kolmogorov complexity) of sets of inﬁnite sequences.
Usually, the Hausdorﬀdimension (here also called classical Hausdorﬀdimen-
sion) of a set of reals is a real number α characterising a certain density or
measure property of this set (see the textbooks [3,4] or [12]). If one looks to
Hausdorﬀ’s original paper [7], however, one ﬁnds that he deﬁned the Hausdorﬀ
dimension to be a non-decreasing, right continuous function h : (0, ∞) →(0, ∞),
nowadays called a gauge function [5].
The paper [19] provided a generalisation of the martingale characterisation of
Hausdorﬀdimension and the determining of the dimension of the level sets to
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 686–695, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

A Correspondence Principle for Exact Constructive Dimension
687
the case of exact dimension and to Kolmogorov complexity functions of inﬁnite
sequences.
In the papers [8,9] and [18] (see also [2, Section 13.6]) a tight bound on the
maximum asymptotic Kolmogorov complexity of sequences in Σ0
2-sets by its
‘usual’ Hausdorﬀdimension was presented and computable martingales success-
ful on Σ0
2-sets with an exponent close to the Hausdorﬀdimension were con-
structed.
The purpose of the present paper is to generalise these results to a corre-
spondence principle for the case of exact dimensions. This results also in a more
precise bound on the maximum Kolmogorov complexity of sequences in Σ0
2-sets
than the mere asymptotics given in the above mentioned papers.
The paper is organised as follows. After introducing some notation and some
preliminaries on gauge functions and Hausdorﬀ’s original approach we present
in Section 2 necessary results, mainly from [19] on exact Hausdorﬀdimension,
martingales and their eﬀectivisation. Then Sections 3.1 and 3.2 show that the
correspondence principles for constructive and computable dimensions hold for
Σ0
2-deﬁnable sets of sequences and gauge functions satisfying some computability
constraints. The proofs follow mainly the line of the proofs given in [18]. Due to
space limitations they had to be omitted in this paper.
1
Notation and Preliminaries
In this section we introduce the notation used throughout the paper. By IN =
{0, 1, 2, . . .} we denote the set of natural numbers and by Q the set of rational
numbers. Let X = {0, 1, . . ., r −1} be an alphabet of cardinality |X| = r ≥2.
By X∗we denote the set of ﬁnite words on X, including the empty word e, and
Xω is the set of inﬁnite strings (ω-words) over X. Subsets of X∗will be referred
to as languages and subsets of Xω as ω-languages.
For w ∈X∗and η ∈X∗∪Xω let w·η be their concatenation. This concatena-
tion product extends in an obvious way to subsets W ⊆X∗and B ⊆X∗∪Xω.
We denote by |w| the length of the word w ∈X∗and pref(B) is the set of all
ﬁnite preﬁxes of strings in B ⊆X∗∪Xω. We shall abbreviate w ∈pref(η) (η ∈
X∗∪Xω) by w ⊑η, and η ↾n is the n-length preﬁx of η provided |η| ≥n. The δ-
limit of a language V ⊆X∗is the ω-language V δ := {ξ : ξ ∈Xω∧|pref(ξ)∩V | =
∞}. A language W ⊆X∗is referred to as preﬁx-free if w ⊑v and w, v ∈W
imply w = v.
For a computable domain D, such as IN, Q or X∗, we refer to a function
f : D →IR as left computable (or approximable from below) provided the set
{(d, q) : d ∈D ∧q ∈Q ∧q < f(d)} is computably enumerable. Accordingly,
a function f : D →IR is called right computable (or approximable from above)
if the set {(d, q) : d ∈D ∧q ∈Q ∧q > f(d)} is computably enumerable,
and f is computable if f is right and left computable. In contrast to this we
refer to a function f : D →Q as computable provided f returns the exact value
f(d) ∈Q. Accordingly, a real number α ∈IR is left computable, right computable
or computable provided the constant function cα(t) = α is left computable, right
computable or computable, respectively.

688
L. Staiger
A super-martingale is a function V : X∗→[0, ∞) which satisﬁes V(e) ≤1
and the super-martingale inequality
r · V(w) ≥
x∈X V(wx) for all w ∈X∗.
(1)
If Eq. (1) is satisﬁed with equality V is called a martingale. Closely related with
(super-)martingales are continuous (or cylindrical) (semi-)measures μ : X∗→
[0, 1] where μ(e) ≤1 and μ(w) ≥
x∈X μ(wx) for all w ∈X∗.
1.1
Gauge Functions and Hausdorﬀ’s Original Approach
A function h : (0, ∞) →(0, ∞) is referred to as a gauge function provided h is
right continuous and non-decreasing.1 If not stated otherwise, we shall always
assume that limt→0 h(t) = 0.
The h-dimensional outer measure of F on the space Xω is given by
Hh(F) := lim
n→∞inf
 
v∈V
h(r−|v|) : V ⊆X∗∧F ⊆V · Xω ∧min
v∈V |v| ≥n

.
(2)
If limt→0 h(t) > 0 then Hh(F) < ∞if and only if F is ﬁnite.
The usual α-dimensional Hausdorﬀmeasure Hα is deﬁned by gauge functions
hα(t) = tα, α ∈[0, 1], that is, Hα = Hhα.
In this case the (usual or classical) Hausdorﬀdimension of a set F ⊆Xω is
deﬁned as
dimH F := sup{α : α = 0 ∨Hα(F) = ∞} = inf{α : α ≥0 ∧Hα(F) = 0} .
(3)
As we see from Eq. (2) for our purposes the behaviour of gauge function is
of interest only in a small vicinity of 0. Moreover, in many cases we are not
interested in the exact value of Hh(F) when 0 < Hh(F) < ∞. Thus we can
often make use of scaling a gauge function and altering it in a range (ε, ∞)
apart from 0.
The following properties of gauge functions h and the related measure Hh are
proved in the standard way.
Property 1. Let h, h′ be gauge functions.
1. If c · h(r−n) ≤h′(r−n) for some c > 0, then c · Hh(F) ≤Hh′(F).
2. If lim
n→∞
h(r−n)
h′(r−n) = 0 then Hh′(F) < ∞implies Hh(F) = 0, and Hh(F) > 0
implies Hh′(F) = ∞.
Here the ﬁrst property implies a certain equivalence of gauge functions. In fact,
if c · h ≤h′ and c · h′ ≤h in the sense of Property 1.1 then for all F ⊆Xω the
measures Hh(F) and Hh′(F) are both zero, ﬁnite or inﬁnite.
In the same way the second property gives a partial pre-order of gauge func-
tions (see [6, Section 9.1]). By analogy to the change-over-point dimH F for
Hα(F) this partial pre-order yields a suitable notion of Hausdorﬀdimension in
the range of arbitrary gauge functions.
1 In fact, since we are only interested in the values h(r−n), n ∈IN, the requirement
of right continuity is just to conform with the usual meaning (cf. [5]).

A Correspondence Principle for Exact Constructive Dimension
689
Deﬁnition 1. We refer to a gauge function h as an exact Hausdorﬀdimension
function for F ⊆Xω provided
Hh′(F) =
⎧
⎨
⎩
∞, if
lim
n→∞
h(r−n)
h′(r−n) = 0 , and
0 ,
if
lim
n→∞
h′(r−n)
h(r−n) = 0 .
In fact, Hausdorﬀ[7] deﬁned the dimension of a set F as an equivalence class
of gauge functions [ h ] such that 0 < Hh(F) < ∞. Property 1 shows that our
deﬁnition covers this case.
Deﬁnition 1 is not as simple as the one of the classical Hausdorﬀdimension
in Eq. (3), and it seems to be much more diﬃcult to ﬁnd the exact borderline, if
it exists, between gauge functions with Hh(F) = 0 and such with Hh(F) = ∞.
2
Previous Results
2.1
Exact HausdorﬀDimension and Martingales
In this section we show a generalisation of Lutz’s martingale characterisation of
Hausdorﬀdimension to exact dimension.
Let Sc,h[V] :=

ξ : ξ ∈Xω ∧lim supn→∞
V(ξ[0..n])
rn·h(r−n) ≥c

, for a super-
martingale V : X∗→[0, ∞), a gauge function h and a value c ∈(0, ∞]. In
particular, S∞,h[V] is the set of all ω-words on which the super-martingale V is
successful w.r.t. the order function f(n) = rn · h(r−n) in the sense of Schnorr
[16]. S∞,h[V] is also referred to as the success set of the super-martingale V w.r.t.
the order function f(n) = rn · h(r−n).
Observe that Sc,h[V] ⊆Sc′,h′[V] whenever c, c′ ∈(0, ∞] and lim
n→∞
h′(r−n)
h(r−n) = 0.
Now we can generalise Lutz’s result.
Theorem 1 ([19, Theorem 1]). Let F ⊆Xω. Then a gauge function h is an
exact Hausdorﬀdimension function for F if and only if
1. for all gauge functions h′ with lim
n→∞
h′(r−n)
h(r−n) = 0 there is a super-martingale
V such that F ⊆S∞,h′[V], and
2. for all gauge functions h′′ with lim
n→∞
h(r−n)
h′′(r−n) = 0 and all super-martingales
V it holds F ̸⊆S∞,h′′[V].
2.2
Eﬀectivisation of Exact HausdorﬀDimension
The constructive dimension is a variant of dimension deﬁned analogously to
Theorem 1 using only left computable super-martingales. For the usual family
of gauge functions hα(t) = tα it was introduced by Lutz [10,11] and resulted,
similarly to dimH in a real number assigned to a subset F ⊆Xω. In the case of left
computable super-martingales the situation turned out to be even simpler than
in the case of arbitrary super-martingales because the results of Levin [21] and

690
L. Staiger
Schnorr [16] show that there is an optimal left computable super-martingale U,
that is, every other left computable super-martingale V satisﬁes V(w) ≤cV ·U(w)
for all w ∈X∗and some constant cV > 0 not depending on w. Thus we may
deﬁne (cf. [19])
Deﬁnition 2. Let F ⊆Xω. We refer to h : R →R as an exact constructive
dimension function for F provided F ⊆S∞,h′[U] for all h′, lim
t→0
h(t)
h′(t) = 0, and
F ̸⊆S∞,h′′[U] for all h′′, lim
t→0
h′′(t)
h(t) = 0.
Originally, Levin [21] showed that there is an optimal left computable contin-
uous semi-measure M on X∗. As usual, we call a function μ : X∗→[0, ∞) a
continuous (or cylindrical) semi-measure on X∗provided μ(e) ≤1 and μ(w) ≥

x∈X μ(wx) for all w ∈X∗. One easily veriﬁes that μ is a continuous semi-
measure if and only if V(w) := r|w| · μ(w) is a super-martingale. Thus we
might use UM with UM(w) := r|w| · M(w) as our optimal left computable super-
martingale.
Closely related to Levin’s optimal left computable semi-measure is the a priori
entropy (or complexity) KA : X∗→N deﬁned by2
KA(w) := ⌊−logrM(w)⌋
(4)
The requirement KA(w) ≥0 is one reason why we assumed M(e) ≤1.
The following theorem derives a bound for the set of sequences whose KA-
complexity function is bounded.
Theorem 2 ([19, Theorem 4]). Let −∞< c < ∞and let h be a gauge
function. Then there is a c′ > 0 such that
{ξ : KA(ξ[0..n]) ≤i.o. −logrh(r−n) + c} ⊆Sc′,h[U].
Conversely, if ξ ∈Sc,h[U], c < ∞, then from Eq. (4) one easily calculates
KA(ξ[0..n]) ≤i.o. −logrh(r−n) + c′′ for some c′′ ∈(0, ∞). Thus we obtain a
complexity characterisation of the success sets of the universal super-martingale
U.

c>0
{ξ : KA(ξ[0..n]) ≤i.o. −logrh(r−n) + c} =

c>0
Sc,h[U]
(5)
For gauge functions h′ tending faster to 0 than h the following relations follow
from Sc,h[U] ⊆S∞,h′[U].
Corollary 1. Let h, h′ be gauge functions such that limt→0
h′(t)
h(t) = 0. Then
1. {ξ : ∃c(KA(ξ[0..n]) ≤i.o. −logrh(r−n) + c)} ⊆S∞,h′[U], and
2. Hh′	
{ξ : ∃c(KA(ξ[0..n]) ≤i.o.−logrh(r−n) + c)}

= 0.
2 Here we follow the notation of [20], in [2] a priori complexity was denoted by KM .

A Correspondence Principle for Exact Constructive Dimension
691
3
The Results
In [8,9,18] the correspondence principle could be stated for arbitrary (real) val-
ues of classical dimension. In the case of gauge functions the situation is more
complicated. On the one hand because of the involved Deﬁnition 1, and, on the
other hand, for the following reason (cf. also [19, Remark 2]). Unlike the classical
case where the computable (even the rational) numbers are dense in the reals,
for gauge functions it holds that, if α ∈(0, 1) is not a computable real, there is
no computable function between hα(t) = tα and hα(t) = tα + logr
1
t .
First we mention the following general lower bound to the complexity function
KA from [13] together with Eq. (5) yields a tight estimate for gauge functions
satisfying F ̸⊆Sc,h′′[U] for arbitrary F ⊆Xω (cf. Deﬁnition 2).
Theorem 3 ([13]). Let F ⊆Xω, h be a gauge function and Hh(F) > 0.
Then for every c > 0 with Hh(F) > c · U(e) there is a ξ ∈F such that
KA(ξ[0..n]) ≥a.e. −logrh(r−n) −logr c.
In order to obtain the announced upper bound, in view of Eq. (5) in the following
two parts we show that for Σ0
2-deﬁnable subsets F ⊆Xω and gauge functions
h satisfying some computability constraints there are left-computable super-
martingales or computable martingales V, respectively, such that F ⊆S∞,h′[V]
whenever limt→0
h′(t)
h(t) = 0 and Hh(F) = 0.
3.1
Constructive Dimension
As in [18] we ask now for an estimate of the condition F ⊆Sc,h′[U] of Deﬁnition 2.
The results use the following construction.
We start with an auxiliary lemma characterising subsets F ⊆Xω having null
measure.
Lemma 1 ([14]). Let F ⊆Xω and h be a gauge function. Then Hh(F) = 0 if
and only if there is a language V ⊆X∗such that F ⊆V δ and 
v∈V h(r−|v|) <
∞.
The following theorem gives a constructive version of Lemma 1.
Theorem 4. If F ⊆Xω is a Σ2-deﬁnable ω-language and h is a right com-
putable gauge function such that Hh(F) = 0 then there are a computable non-
decreasing function ¯h : {r−i : i ∈N} →Q and a computable language V ⊆X∗
satisfying
1. ¯h(r−n) ≥h(r−n) for all n ∈N,
2. F ⊆V δ and 
v∈V ¯h(r−n) < ∞.
Interpolating the computable function ¯h we obtain the following consequence.
Corollary 2. If F ⊆Xω is a Σ2-deﬁnable ω-language and h is a right com-
putable gauge function such that Hh(F) = 0 then there is a computable non-
decreasing function ¯h : Q →Q satisfying H¯h(F) = 0 and ¯h(t) ≥h(t) for
t ∈Q ∩(0, 1).

692
L. Staiger
Our Theorem 4 yields the required upper bound for the preﬁx complexity KP,
and hence also of the a priori complexity KA of an ω-word in F.
To this end we use the characterisation of KP via discrete semi-measures (cf.
[2,20]).3
A mapping ν : X∗→R is referred to as a discrete semi-measure provided

w∈X∗ν(w) < ∞. It is known that there is an optimal left computable discrete
semi-measure, that is, a left computable discrete semi-measure m such that for
every left computable discrete semi-measure ν there is a constant cν such that
∀w(w ∈X∗→ν(w) ≤cν ·m(w)). This measure m deﬁnes the preﬁx complexity
(similarly as M deﬁnes the a priory complexity KA) KP(w) := ⌊−logrm(w)⌋.
If V ⊆X∗is computably enumerable and ¯h : {r−n > n ∈N} →R is a left
computable function such that 
v∈V ¯h(r−|v|) < ∞then
ν(w) :=
¯h(r−|w|), if w ∈V , and
0,
otherwise
(6)
deﬁnes a left computable discrete semi-measure. Thus Theorem 4 implies the
following upper bound on the complexity functions of ω-words.
Lemma 2. If F ⊆Xω is a Σ2-deﬁnable ω-language and h is a right computable
gauge function such that Hh(F) = 0 then
KP(ξ[0..n]) ≤i.o. −logrh(r−n) + O(1) for all ξ ∈F, and
KA(ξ[0..n]) ≤i.o. −logrh(r−n) + O(1) for all ξ ∈F.
The latter inequality follows from the former and KA(w) ≤KP(w) + O(1) (see,
e.g., [2,20]).
Finally, Lemma 2, Eq. (5) and Corollary 1 prove the following.
Theorem 5. If F ⊆Xω is a union of Σ2-deﬁnable sets and h is a right com-
putable gauge function such that Hh(F) = 0 then F ⊆S∞,h′[U] for every gauge
function h′ such that limt→0
h′(t)
h(t) = 0.
3.2
Computable Dimension
Computable dimension is based on computable super-martingales as constructive
dimension was based on left computable super-martingales. In contrast to the
latter, for the former there is no universal computable super-martingale (cf.
[2,16]). Thus we deﬁne analogously to Theorem 1
Deﬁnition 3. We refer to a gauge function h as an exact computable dimension
function for F ⊆Xω provided
1. for all gauge functions h′ with lim
n→∞
h′(r−n)
h(r−n) = 0 there is a computable super-
martingale V such that F ⊆S∞,h′[V], and
2. for all gauge functions h′′ with lim
n→∞
h(r−n)
h′′(r−n) = 0 and all computable super-
martingales V it holds F ̸⊆S∞,h′′[V].
3 Here we follow also the notation of [20], in [2] preﬁx complexity was denoted by K.

A Correspondence Principle for Exact Constructive Dimension
693
As for the constructive case the second item is fulﬁlled provided Hh(F) > 0.
For Item 1 we prove that for computable gauge functions h and Σ0
2-deﬁnable
sets F ⊆Xω with Hh(F) = 0 there is a computable martingale V such that
F ⊆
c>0 Sc,h[V].
In order to achieve our goal we introduce families of covering codes as in [18].
For a preﬁx code C ⊆X∗we deﬁne its minimal complementary code as
C := (X ∪pref(C) · X) \ pref(C) .
If C = ∅we have C = X, and if C ̸= ∅the set C consists of all words
w · x ̸∈pref(C) where w ∈pref(C) and x ∈X. It is readily seen that C ∪C is
a maximal preﬁx code, C ∩C = ∅, and pref(C ∪C) = {e} ∪pref(C) ∪C.
We call C := (Cw)w∈X∗a family of covering codes provided each Cw is a
ﬁnite preﬁx code. Since then the set Cw ∪Cw is a ﬁnite maximal preﬁx code,
every word u ∈X∗has a uniquely speciﬁed C-factorisation u = u1 · · · un · u′
where ui+1 ∈Cu1···ui ∪Cu1···ui for i = 0, . . . , n −1 (u1 · · · ui = e, if i = 0) and
u′ ∈pref(Cu1···un ∪Cu1···un). Analogously, every ξ ∈Xω has a uniquely speciﬁed
C-factorisation ξ = u1 · · · ui · · · where ui+1 ∈Cu1···ui ∪Cu1···ui for i = 1, . . . .
In what follows we use martingales derived from preﬁx codes in the following
manner.
Lemma 3. Let h : R →R a gauge function and ∅̸= C ⊆X∗be a preﬁx code
satisfying 
v∈C h(r−|v|) < ∞. Then there is a martingale V(h)
C
: X∗→[0, ∞)
such that
V(h)
C (w) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
r|w| · h(r(−|w|)

v∈C h(r(−|v|) + 
u∈
C r−|u|
, for w ∈C , and
1

v∈C h(r(−|v|) + 
u∈
C r−|u|
, for w ∈C .
(7)
Remark 1. If C is a ﬁnite preﬁx code and h : Q →Q is computable then V(h)
C
is
a computable martingale.
For a gauge function h : R →R let hw(t) := h(r−|w|·t)
h(t)
and let C := (Cw)w∈X∗
be a family of covering codes.
Using the martingales V(hw)
Cw
we deﬁne a new martingale VC as follows:
For u ∈X∗consider the C-factorisation u1 · · · un · u′, and put
V(h)
C (u) :=
n−1

i=0
V
(hu1···ui )
Cu1···ui (ui+1)

· V
(hu1···un)
Cu1···un (u′) ,
that is, V(h)
C
is in some sense the concatenation of the martingales V(hw)
Cw . Observe
that V(h)
C
is computable if only h : R →R is a computable function, the codes
Cw are ﬁnite and the function which assigns to every w the corresponding code
Cw is computable.
We have the following.

694
L. Staiger
Lemma 4. Let h : N →Q be a gauge function and let C = (Cw)w∈X∗be a
family of covering codes such that 
v∈Cw
h(r−|wv|)
h(r−|v|) ≤r−|w| for all w ∈X∗.
If the ω-word ξ ∈Xω has a C-factorisation ξ = u1 · · · ui · · · such that for
some nξ ∈N and all i ≥nξ the factors ui+1 belong to Cu1···ui. Then there is a
constant cξ > 0 not depending on i for which
VC(u1 · · · ui) ≥cξ · r|u1···ui| · h(r−|u1···ui|) .
Now we derive the announced result.
Theorem 6. For every Σ2-deﬁnable ω-language F ⊆Xω and every computable
gauge function h : Q →R such that Hh(F) = 0 there is a computable martingale
V such that F ⊆
c>0 Sc,h[V].
References
1. Cai, J.Y., Hartmanis, J.: On Hausdorﬀand topological dimensions of the Kol-
mogorov complexity of the real line. J. Comput. System Sci. 49(3), 605–619 (1994)
2. Downey, R.G., Hirschfeldt, D.R.: Algorithmic Randomness and Complexity. The-
ory and Applications of Computability. Springer, New York (2010)
3. Edgar, G.: Measure, topology, and fractal geometry, 2nd edn. Undergraduate Texts
in Mathematics. Springer, New York (2008)
4. Falconer, K.: Fractal geometry. John Wiley & Sons Ltd., Chichester (1990)
5. Graf, S., Mauldin, R.D., Williams, S.C.: The exact Hausdorﬀdimension in random
recursive constructions. Mem. Amer. Math. Soc. 71(381), x+121 (1988)
6. Graham, R.L., Knuth, D.E., Patashnik, O.: Concrete mathematics, 2nd edn.
Addison-Wesley Publishing Company, Reading (1994)
7. Hausdorﬀ, F.: Dimension und ¨außeres Maß. Math. Ann. 79(1-2), 157–179 (1918)
8. Hitchcock, J.M.: Correspondence Principles for Eﬀective Dimensions. In: Wid-
mayer, P., Triguero, F., Morales, R., Hennessy, M., Eidenbenz, S., Conejo, R. (eds.)
ICALP 2002. LNCS, vol. 2380, pp. 561–571. Springer, Heidelberg (2002)
9. Hitchcock, J.M.: Correspondence principles for eﬀective dimensions. Theory Com-
put. Syst. 38(5), 559–571 (2005)
10. Lutz, J.H.: Gales and the Constructive Dimension of Individual Sequences. In:
Welzl, E., Montanari, U., Rolim, J.D.P. (eds.) ICALP 2000. LNCS, vol. 1853, pp.
902–913. Springer, Heidelberg (2000)
11. Lutz, J.H.: The dimensions of individual strings and sequences. Inform. and Com-
put. 187(1), 49–79 (2003)
12. Merzenich, W., Staiger, L.: Fractals, dimension, and formal languages. RAIRO
Inform. Th´eor. Appl. 28(3-4), 361–386 (1994)
13. Mielke, J.: Reﬁned bounds on Kolmogorov complexity for ω-languages. Electr.
Notes Theor. Comput. Sci. 221, 181–189 (2008)
14. Reimann, J.: Computability and Fractal Dimension. Ph.D. thesis, Ruprecht-Karls-
Universit¨at Heidelberg (2004)
15. Ryabko, B.Y.: Coding of combinatorial sources and Hausdorﬀdimension. Dokl.
Akad. Nauk SSSR 277(5), 1066–1070 (1984)
16. Schnorr, C.P.: Zuf¨alligkeit und Wahrscheinlichkeit. In: Eine Algorithmische
Begr¨undung
der Wahrscheinlichkeitstheorie.
Lecture Notes in
Mathematics,
vol. 218. Springer, Berlin (1971)

A Correspondence Principle for Exact Constructive Dimension
695
17. Staiger, L.: Kolmogorov complexity and Hausdorﬀdimension. Inform. and Com-
put. 103(2), 159–194 (1993)
18. Staiger, L.: A tight upper bound on Kolmogorov complexity and uniformly optimal
prediction. Theory Comput. Syst. 31(3), 215–229 (1998)
19. Staiger, L.: Constructive Dimension and HausdorﬀDimension: The Case of Exact
Dimension. In: Owe, O., Steﬀen, M., Telle, J.A. (eds.) FCT 2011. LNCS, vol. 6914,
pp. 252–263. Springer, Heidelberg (2011)
20. Uspensky, V.A., Shen, A.: Relations between varieties of Kolmogorov complexities.
Math. Systems Theory 29(3), 271–292 (1996)
21. Zvonkin, A.K., Levin, L.A.: The complexity of ﬁnite objects and the basing of the
concepts of information and randomness on the theory of algorithms. Uspehi Mat.
Nauk 25(6(156)), 85–127 (1970)

Lown Boolean Subalgebras
Rebecca M. Steiner
Graduate Center of the City University of New York, 365 Fifth Avenue, New York,
NY 10016, United States of America
rsteiner@gc.cuny.edu
Abstract. Every lown Boolean algebra, for 1 ≤n ≤4, is isomorphic to
a computable Boolean algebra. It is not yet known whether the same is
true for n > 4. However, it is known that there exists a low5 subalgebra
of the computable atomless Boolean algebra which, when viewed as a
relation on the computable atomless Boolean algebra, does not have
a computable copy. We adapt the proof of this recent result to show
that there exists a low4 subalgebra of the computable atomless Boolean
algebra which, when viewed as a relation on the computable atomless
Boolean algebra, has no computable copy. This result provides a sharp
contrast with the one which shows that every low4 Boolean algebra has
a computable copy. That is, the spectrum of the subalgebra as a unary
relation can contain a low4 degree without containing the degree 0, even
though no spectrum of a Boolean algebra (viewed as a structure) can do
the same.
1
Introduction
Researchers in computability theory have been coding (or attempting to code)
lown sets into Boolean algebras for at least the past twenty-ﬁve years. The
question which has been the focus of much of this work is: For which n is it
true that every lown Boolean algebra is isomorphic to a computable Boolean
algebra? The spectrum of a structure A is the set Spec(A) of Turing degrees of
structures isomorphic to A: Spec(A) = {deg(D) : D ∼= A}. With this deﬁnition,
we can rephrase the above question:
Question 1. For which n is it true that the spectrum of a lown Boolean algebra
must contain the degree 0?
Question 1 has been settled for n = 1, 2, 3, and 4, all in the aﬃrmative [2,11,6].
That is, if the spectrum of a Boolean algebra contains a low, low2, low3, or low4
degree, it must also contain the degree 0.
The goal of this chapter is not to contribute to the solution of Question 1, but
rather to contribute to the solution of a diﬀerent (but related) question: If B is
the computable atomless Boolean algebra and A is a subalgebra which is lown
within B, must there be a computable subalgebra D such that (B, A) ∼= (B, D)?
The spectrum of a relation R on a computable structure M is the set DgSpM(R)
of Turing degrees of all images of R under isomorphisms from M onto other
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 696–702, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Lown Boolean Subalgebras
697
computable structures: DgSpM(R) = {deg(S) : (∃B ≤T ∅) [(B, S) ∼= (M, R)]}.
With this deﬁnition, it is equivalent to ask:
Question 2. For which n is it true that the spectrum of a lown Boolean algebra,
viewed as a unary relation on some ﬁxed copy of B, must contain the degree 0?
Question 2 has recently been answered in the negative for n = 5 [7], which is a
case still unsettled for the former question.
A negative answer to Question 2 for n = 1, 2, 3, or 4 would be signiﬁcant
because it would provide us with an example of a spectrum of a subalgebra as
a unary relation on the computable atomless Boolean algebra B which cannot
possibly be the spectrum of a Boolean algebra viewed as a structure in its own
right – the spectrum of the subalgebra would contain a lown degree for n = 1,
2, 3, or 4 without containing the degree 0, and we know that no spectrum of a
Boolean algebra as a structure has this property [6].
In this article, we adapt the proof of the result that the spectrum of a low5
subalgebra of B, viewed as a relation on B, need not contain the degree 0 to show
that the spectrum of a low4 subalgebra of B, viewed as a relation on B, need not
contain the degree 0. Thus we provide a negative answer to the n = 4 case for the
spectrum of a subalgebra. We also point out that this implies another spectral
diﬀerence between Boolean algebras as structures and Boolean subalgebras as
relations on B. A theorem of Jockusch and Soare in [5] says that if a Boolean
algebra has n-th jump degree d (deﬁned below in Deﬁnition 4), where n < ω,
then d = 0(n). In other words, no Boolean algebra as a structure can have n-th
jump degree larger than 0(n). We show below in Corollary 11 that a Boolean
subalgebra of B, considered as a relation on B, can have n-th jump degree strictly
larger than 0(n).
2
The History
The ﬁrst result on lown Boolean algebras and computable copies was the result
of Downey and Jockusch for n = 1 which appeared in 1994.
Theorem 1. [2, Thm. 1] Every low Boolean algebra is isomorphic to a com-
putable one.
They converted an argument about Boolean algebras to an argument about
linear orderings, and then applied a theorem of Remmel [8, Thm. 2.1] which
gives suﬃcient conditions for two linear orderings to have isomorphic interval
algebras.
The result for n = 2 was the work of Thurber, and it appeared very soon
after the n = 1 result. Thurber used the same sort of linear ordering argument
Downey and Jockusch used.
Theorem 2. [11, Thm. 1] Every low2 Boolean algebra is isomorphic to a com-
putable one.

698
R.M. Steiner
Six years after the original result, Knight and Stob came out with the results
for n = 3 and n = 4.
Theorem 3. [6, Cor. 3.3] Every low3 Boolean algebra is isomorphic to a com-
putable one.
Theorem 4. [6, Cor. 5.3] Every low4 Boolean algebra is isomorphic to a com-
putable one.
Knight and Stob, unlike Downey and Jockusch and Thurber, argued in terms of
Boolean algebras directly.
In an eﬀort to attempt a result for n = 5, Miller, in 2011, instead ended
up with a result just as interesting about low5 subalgebras of the computable
atomless Boolean algebra B when viewed as a relation on B.
Deﬁnition 1. The spectrum of a structure A is the set Spec(A) of Turing de-
grees of structures isomorphic to A:
Spec(A) = {deg(D) : D ∼= A} .
The spectrum of a relation R on a computable structure M is the set DgSpM(R)
of Turing degrees of all images of R under isomorphisms from M onto other
computable structures:
DgSpM(R) = {deg(S) : (∃B ≤T ∅) [(B, S) ∼= (M, R)]} .
Theorem 5. [7, Thm. 2.4] Let c be any Turing degree which is not low4. Then
there exists a Boolean subalgebra A of the computable atomless Boolean algebra
B for which DgSpB(A) contains c but does not contain 0. In particular, there
are low5 degrees for which this works.
Proof. We present a sketch of this proof.
Let B be a Boolean algebra and let A be a subalgebra of B. An element x ∈B
is called an A-supremum if x is the least upper bound in B of an inﬁnite set of
A-atoms. Such an x is called a single A-supremum (also called a 1-atom of A
in the literature) if x is not the union of two disjoint A-suprema, and a k-fold
A-supremum if x is the union of k disjoint single A-suprema. The property of
being a k-fold A-supremum is ΣA
4 uniformly in k.
Given an arbitrary nonlow4 degree c and an arbitrary C ∈c, Miller uses a
C-oracle to construct a subalgebra A of B so that c ∈DgSpB(A). He builds A
to satisfy the following:
n ∈C(4) ⇐⇒∃x ∈A [ x is a 2n-fold A-supremum ].
(For the detailed constructions of the subalgebra A, see Miller’s proof in [7].)
The statement
∃x ∈A [ x is a 2n-fold A-supremum ]

Lown Boolean Subalgebras
699
is Σ0
4 in A, and hence computable in A(4). And because this statement is equiv-
alent to n being in C(4), we have C(4) ≤T A(4).
Now, suppose 0 ∈DgSpB(A). In other words, suppose there is a computable
subalgebra D of B such that (B, D) ∼= (B, A). Then the uniform ΣA
4 deﬁnition of
k-fold A-suprema would convert to a uniform ΣD
4 deﬁnition of k-fold D-suprema
on (B, D), which of course means just a Σ0
4 deﬁnition, since D is computable.
Thus it is Σ0
4 whether D contains a 2n-fold D-supremum, and this in turn implies
that C(4) ≤T ∅(4). This is a contradiction because we chose C to be nonlow4.
⊓⊔
3
The Main Result
Theorem 6. Let c be any Turing degree which is not low3. Then there exists
a Boolean subalgebra A of the computable atomless Boolean algebra B for which
DgSpB(A) contains c but does not contain 0.
Proof. The construction of a D for which (B, D) ∼= (B, A) and deg(D) = c
follows exactly as in the proof of Theorem 5 and [7], even though we are dealing
with a nonlow3 degree here, rather than a nonlow4 degree. This is still a C(4)-
construction.
It is actually a small (yet powerful) fact from a classic result known as the
“Jump Theorem” which allows us to use a low3 argument with a low4 con-
struction; we shall use this fact to show that DgSpB(A) does not contain the
degree 0.
Fact 7. [9, “Jump Theorem” 3.4.3(v)] Let A ⊆ω and B ⊆ω. Then A ≤T B if
and only if A′ ≤1 B′.
The reason this works is that the Turing reduction C(4) ≤T A(4) is actually
much stronger than just a Turing reduction. Recall that according to Miller’s
construction in [7], we have
n ∈C(4) ⇐⇒∃x ∈A [ x is a 2n-fold A-supremum ].
(1)
It turns out that we actually have a 1-reduction from C(4) to A(4): Recall that
∃x ∈A [ x is a 2n-fold A-supremum ]
is a ΣA
4 property, and since all ΣA
4 sets 1-reduce to A(4), there is a total com-
putable function f such that for all n,
f(n) ∈A(4) ⇐⇒A contains a 2n-fold A-supremum.
(2)
Then by equivalences (1) and (2), n ∈C(4) iﬀf(n) ∈A(4). Thus C(4) ≤1 A(4)
via this f.
For any D for which (B, D) ∼= (B, A), if A has a k-fold A-supremum, then D
also has a k-fold D-supremum: limits must map to limits under an isomorphism
(same for limits of limits, etc.), and so k-fold suprema are isomorphism-invariant.
Thus n ∈C(4) iﬀf(n) ∈D(4). So C(4) ≤1 D(4).

700
R.M. Steiner
If there were a computable such D, then we would have C(4) ≤1 ∅(4), and
then by Fact 7, we would have C(3) ≤T ∅(3), which is a contradiction since we
chose C to be nonlow3.
⊓⊔
Corollary 8. There is a low4 degree c and a Boolean subalgebra A of B for
which DgSpB(A) contains c but does not contain 0.
Proof. Choose c to be low4 but not low3, and then apply Theorem 6.
⊓⊔
Corollary 9. The spectrum of a Boolean subalgebra of B can fail to be the spec-
trum of any Boolean algebra (as a structure).
Proof. The spectrum of the Boolean subalgebra of B which we constructed in
Theorem 6 contains a low4 degree but not the degree 0. The spectrum of a
Boolean algebra (as a structure in its own right) cannot contain a low4 degree
without containing the degree 0, as we stated in Theorem 4.
Deﬁnition 2. A structure D is called ultrahomogeneous if every isomorphism
between ﬁnitely-generated substructures of D extends to an automorphism
of D.
Deﬁnition 3. Let K be a class of ﬁnitely-generated structures. A structure D
is the Fra¨ıss´e limit of K if D is countable, ultrahomogeneous, and K is the class
of all ﬁnitely-generated substructures of D.
Theorem 10. If A is a Boolean subalgebra of the computable atomless Boolean
algebra B and A is not intrinsically computable, then DgSpB(A) is closed upwards
in the Turing degrees.
Proof. In [1], the authors proved that if F is the Fra¨ıss´e limit of a class K of ﬁnite
structures over a ﬁnite language L where ThL(K) is computably axiomatizable
and locally ﬁnite, then if R is a unary relation on F which is not intrinsically
computable, DgSpF(R) is upward closed under Turing reducibility [1, Cor. 5.2].
These hypotheses all hold for the atomless Boolean algebra B, and so if A is the
unary relation on B, we have exactly the result we want.
⊓⊔
When Miller wrote up his proof of Theorem 5, he neglected to describe the
spectrum of the Boolean algebra he built. So we describe it here.
Corollary 11. The spectrum of the Boolean subalgebra constructed in [7] and
used here in Theorem 6 is the set of all Turing degrees d such that
c′′′ ≤d′′′
where c is the nonlow4 or nonlow3 degree in the theorem.
Proof. Let D be a copy of the A we built, where deg(D) = d. We know from the
construction of A that n ∈C(4)
⇐⇒∃x ∈A [ x is a 2n-fold A-supremum ].
Since D ∼= A and suprema are isomorphism-invariant, we have n ∈C(4)
⇐⇒

Lown Boolean Subalgebras
701
∃x ∈D [ x is a 2n-fold D-supremum ]. In the proof of Theorem 6 we deﬁned f
for which f(n) ∈A(4) means A contains a 2n-fold A-supremum. So C(4) ≤1 D(4)
via this f, and by Fact 7, we have C′′′ ≤T D′′′.
Now let D be a set of degree d, and suppose C′′′ ≤T D′′′. By Fact 7, we then
have C(4) ≤1 D(4). Just as in Miller’s construction in [7], we use a D-oracle to
build a Boolean subalgebra R of B with (B, A) ∼= (B, R). This gives us R ≤T D,
and now that we have Theorem 10 to give us the upward closure of DgSpB(A),
we can say that d ∈DgSpB(A).
⊓⊔
Last but not least, we recall a well-known theorem of Jockusch and Soare about
Boolean algebras as structures which we now know does not hold for Boolean
subalgebras as relations on B.
Deﬁnition 4. For an ordinal α, the α-th jump degree of a structure M is the
least degree among the degrees of α-th jumps of structures isomorphic to M, if
such a least degree exists.
Theorem 12. (Jockusch & Soare) [5, Thm. 1.3] Let n < ω. If a Boolean algebra
has nth-jump degree d, then d = 0(n).
This theorem, along with Corollary 11, diﬀerentiates the jump degree spectrum
of a Boolean algebra and the jump degree spectrum of a Boolean subalgebra
of B. Theorem 12 states that the n-th jump degree of a Boolean algebra (as a
structure) can’t be any larger than 0(n), while Corollary 11, via n = 3, shows
that the n-th jump degree of a Boolean subalgebra of B, considered as a relation
on B, can be larger than 0(n).
Acknowledgements. This research was partially supported by grant #DMS-
1001306 from the National Science Foundation.
References
1. Csima, B.F., Harizanov, V.S., Miller, R.G., Montalb´an, A.: Computability of
Fra¨ıss´e Limits. Journal of Symbolic Logic 76, 66–93 (2011)
2. Downey, R., Jockusch, C.G.: Every low Boolean algebra is isomorphic to a recursive
one. Proceedings of the American Mathematical Society 122, 871–880 (1994)
3. Frolov, A., Harizanov, V.S., Kalimullin, I., Kudinov, O., Miller, R.G.: Degree spec-
tra of highn and nonlown degrees. To appear in the Journal of Logic and Compu-
tation
4. Harizanov, V.S., Miller, R.G.: Spectra of structures and relations. Journal of Sym-
bolic Logic 72, 324–348 (2007)
5. Jockusch, C.G., Soare, R.I.: Boolean algebras, stone spaces, and the iterated Turing
jump. Journal of Symbolic Logic 59, 1121–1138 (1994)
6. Knight, J.F., Stob, M.: Computable Boolean algebras. Journal of Symbolic
Logic 65, 1605–1623 (2000)
7. Miller, R.G.: Low5 Boolean subalgebras and computable copies. Journal of Sym-
bolic Logic 76, 1061–1074 (2011)

702
R.M. Steiner
8. Remmel, J.B.: Recursive isomorphism types of recursive Boolean algebras. Journal
of Symbolic Logic 46, 572–594 (1981)
9. Soare, R.I.: Recursively Enumerable Sets and Degrees. Springer, Berlin (1987)
10. Steiner, R.M.: Eﬀective Algebraicity (submitted for publication)
11. Thurber, J.J.: Every low2 Boolean algebra has a recursive copy. Proceedings of the
American Mathematical Society 123, 3859–3866 (1995)

Bringing Up Turing’s ‘Child-Machine’
Susan G. Sterrett
Department of Philosophy, Carnegie Mellon University, Pittsburgh,
PA 15213, United States of America
susangsterrett@gmail.com
Abstract. Turing wrote that the “guiding principle” of his investigation
into the possibility of intelligent machinery was “The analogy [of machin-
ery that might be made to show intelligent behavior] with the human
brain.” [10] In his discussion of the investigations that Turing said were
guided by this analogy, however, he employs a more far-reaching anal-
ogy: he eventually expands the analogy from the human brain out to
“the human community as a whole.” Along the way, he takes note of an
obvious fact in the bigger scheme of things regarding human intelligence:
grownups were once children; this leads him to imagine what a machine
analogue of childhood might be. In this paper, I’ll discuss Turing’s child-
machine, what he said about diﬀerent ways of educating it, and what
impact the “bringing up” of a child-machine has on its ability to behave
in ways that might be taken for intelligent. I’ll also discuss how some of
the various games he suggested humans might play with machines are
related to this approach.
1
A ‘Guiding Principle’
In his writings on intelligence and machinery, Turing often employs analogies.
One analogy he states explicitly and calls the “guiding principle” of his investi-
gation into “possible ways in which machinery might be made to show intelligent
behavior” is “the analogy with the human brain.” [10]
The analogy that Turing employs in the discussions that follow is not a sim-
ple analogy between machine and brain; it’s more speciﬁc, and less physically-
oriented, than the brief description of an analogy between computing machinery
and the human brain quoted above might at ﬁrst suggest. Turing says his investi-
gation is mainly concerned with the analogy between the ways in which a human
[with a human brain] is educated such that the potentialities for human intel-
ligence are realized, and “an analogous teaching process applied to machines.”
[10].
That is, his investigation concerns identifying and evaluating proposals for
ﬁlling in the part of the analogy that answers: if we want a machine to ful-
ﬁl its potentialities for intelligence, how should it be “educated”? In his 1950
“Computing Machinery and Intelligence”, he spoke of dividing the problem of
building a machine that can imitate the human mind into two parts: “The child
program and the education process.” He mentions yet a third component, on
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 703–713, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

704
S.G. Sterrett
this approach: “Other experience, not to be described as education, to which it
[the machine] has been subjected.” That is, there is a distinction between “the
education process” and “other experience.” But what is it that distinguishes the
education process?
2
Intelligent Behavior versus Completely Disciplined
Behavior
This analogy—between a machine that has undergone an education process and
a human student who has been educated by a teacher—provides Turing with
the means to respond to one of the most common objections raised against the
possibility that a machine could be regarded as exhibiting intelligent behavior.
This objection (to the possibility of intelligent machinery) is, in Turing’s words,
the view that “[i]nsofar as a machine can show intelligence this is to be regarded
as nothing but a reﬂection of the intelligence of its creator.” That view, he says,
is much like the view that “the credit for the discoveries of a pupil should be
given to his teacher”, which can be rebutted as follows:
“ In such a case the teacher would be pleased with the success of his
methods of education, but would not claim the results themselves unless
he had actually communicated them to his pupil. He would certainly
have envisaged in very broad outline the sort of thing his pupil might be
expected to do, but would not expect to forsee any sort of detail. ” [10,
p. 2]
Turing contrasts “intelligent behavior” of a machine with “completely disciplined
behavior.” Both are exceptional sorts of behavior for a machine; he says that
“Most of the programmes which we can put into the machine will result in it
doing something that we cannot make sense of at all, or which we regard as
completely random behavior.”
In both intelligent behavior and completely disciplined behavior, we are able
to make sense of the machine’s behavior. But the kind of sense we make of it
diﬀers. When a machine is carrying out computations, the machine’s behavior
is “completely disciplined” and what we strive for is to have “a clear mental
picture of the state of the machine at each moment in the computation.” [9,
p. 459] When a teacher is educating a machine with an intent to produce an
intelligent machine, the goal of the education process entails that some of the
machine’s rules of behavior will be undergoing change. The teacher will be able
“to some extent to predict the pupil’s behavior”, but, in contrast to the case of
programming it to carry out computations, won’t have a clear picture of what is
going on within the machine being educated. Intelligent behavior is not a large
departure from completely disciplined behavior, but it does diﬀer qualitatively
from completely disciplined behavior: the sense we make of it is distinctively
diﬀerent. Intelligent behavior escapes the predictability of completely disciplined
machine behavior without veering oﬀinto random behavior.

Bringing Up Turing’s ‘Child-Machine’
705
One qualitative diﬀerence between completely disciplined behavior and intel-
ligent behavior is the presence of initiative. When describing a universal machine
with no special programming but able to carry out whatever program is put into
it, Turing remarks that after carrying out the actions speciﬁed by the program,
it would sink into inactivity until another action is required. It would lack ini-
tiative. This is one reason that the universal computer, even if produced from
a child-machine by some machine analogue of an education process to produce
completely disciplined behavior, is not a good candidate for a machine analogue
of a human — even though the actions that it does take would be faultless.
Turing thinks that an intelligent machine will be fallible, and that the feature of
fallibility can be (or might be an indication of something that is) an important
advantage. He also thinks that it might be required to have some sort of ran-
dom element in a machine in order to produce a machine that is amenable to
undergoing the kind of process that is analogous to the education of a human.
So, randomness probably has a part to play in producing a machine that might
possibly be said to exhibit intelligence. Yet, intelligent machine behavior is not
random behavior. The part that randomness plays in intelligent machinery is in
the generation of possibilities among which some search process is then employed.
[9, p. 459] Turing writes of using a random element to generate forms of behavior
at one point [9, p. 459]; at another point he speaks of using a random element to
generate diﬀerent child-machines among which one then selects the best ones. [9,
p. 456]. However, in both places he indicates that random generation alone does
not seem very eﬃcient, and that he would expect to supplement the generation
of alternatives or the search among alternatives with some more directed, more
informed process. Of the process of “ﬁnding” an appropriate child-machine, he
writes:
“One may hope, however, that this process will be more expeditious
than evolution. The survival of the ﬁttest is a slow method for measuring
advantages. The experimenter, by the exercise of intelligence, should be
able to speed it up. Equally important is the fact that he is not restricted
to random mutations. If he can trace a cause for some of the weakness
he can probably think of the kind of mutation which will improve it.”
[9, p. 456]
And, of the education process, which aims to ﬁnd the appropriate behavior:
“The systematic method [of trying out diﬀerent possibilities in the
search for a solution] has the disadvantage that there may be an enor-
mous block without any solutions in the region which has to be investi-
gated ﬁrst. Now the learning process may be regarded as a search for a
form of behaviour which will satisfy the teacher (or some other criterion).
Since there is probably a very large number of satisfactory solutions the
random method seems to be better than the systematic.” [9, p. 459]
The education process is a matter of “intervening” on the machine. Just as
the behavior of the early machines could be changed by using a screwdriver to

706
S.G. Sterrett
change the machine’s physical conﬁguration by physical means, so the behavior
of digital computers can be changed by using communication with it to change its
rules of operation in some way. These two kinds of intervention are referred to as
“screwdriver intervention” and “paper intervention”, respectively. In “Intelligent
Machinery”, the guiding principle (the analogy mentioned earlier) is employed
here, too—with some qualiﬁcations. Turing notes that human life is such that
“interference is the rule rather than the exception.” He identiﬁes which part
of human life he means to compare to a machine that might be regarded as
exhibiting intelligence:
“ [A human] is in frequent communication with other [humans], and
is continually receiving visual and other stimuli which themselves consti-
tute a form of interference. It will only be when the [human] is’concentrating’
with a view to eliminating these stimuli or ‘distractions’ that he approx-
imates a machine without interference.” [10, p. 8; emphasis added]
The human behavior during a time period when the human approximates a
machine without interference, though, “is largely determined by the way he has
been conditioned by previous interference.”
Since, as he says, humans are constantly undergoing interference, how is the
analogy between humans and machines supposed to go here? What is the dif-
ference between undergoing an education process and being intervened upon in
other ways? Well, he seems to think of education as a special kind of interference:
it involves a teacher who intentionally tries to aﬀect the behavior of the machine.
It’s interference directed towards some goal. So, even though humans undergo
interference as a rule as they go about their daily lives (except for the times
when they withdraw and concentrate on something), we still want to distinguish
the kind of interference that is education from other kinds of interference.
The analogy may not be precise, but I think it is pretty clear: humans undergo
education processes for a portion of their lives (which Turing estimates at about
the ﬁrst twenty years of their lives), and their behavior after that is very much
aﬀected by the education they have received, even though they still receive other
interference—most of the time, in fact. The point is to approximate the human
process of education with some analogous process suitable for machines. The ma-
jor points of his proposal are that, on analogy with a human’s life, we plan for
these three stages of a machine: ﬁrst, there is the infant stage of a machine, which
is a machine that has not been educated and is at least partly unorganized. It
need not be a blank slate, but it is important that large amounts of its behavior
are undetermined. This is followed by the child-machine stage, during which the
machine is educated. The ﬁrst stage of education is to get the machine to a point
where “it could be relied on to produce deﬁnite reactions to certain commands.”
[10, p. 118] Education involves a teacher who is intentionally trying to teach
or modify the machine’s behavior to eﬀect some speciﬁc kinds of behavior. The
machine’s behavior is in ﬂux during this time. Even if the machine is given the
means to educate itself using some kind of program during the child-machine
stage, there is still oversight and monitoring by a teacher of sorts who checks

Bringing Up Turing’s ‘Child-Machine’
707
up on its progress and intervenes if necessary. At some point the education can
be ended, and the machine that results when education is ended is supposed
to behave in a way that can be predicted “in very broad outline” by someone
familiar with how it has been educated — but its behavior might not, in fact
probably will not, be fully predictable. Finally, there is the adult-machine, which
is still capable of learning, but is also capable of quite complex behavior without
additional intervention.
What about a process that would start with an unorganized machine, which
we would then ‘organize’ by suitable interference to be a universal machine (e.g.,
a digital computer capable of being programmed)? Turing says that researchers
should be interested in understanding the process that begins with an unorga-
nized machine and results in a universal machine, but he doesn’t regard such a
process as the appropriate “analogous process” of human education: a universal
machine isn’t really the behavioral analogue of an adult. One of the diﬀerences
between a human adult and a universal machine is the point mentioned above
regarding the lack of initiative. There are other reasons, too: such an adult-
machine would “obey orders given in an appropriate language, even if they were
very complicated; he would have no common sense, and would obey the most
ridiculous orders unﬂinchingly.” [10, p. 116]
Turing describes an experiment in “educating” machines he carried out. It
involved a process meant to be analogous to administering punishments and
rewards; of giving the machine something analogous to pain and pleasure. The
machine to be educated in his experiment was one whose description was incom-
plete, as he put it, meaning that its actions were not yet fully speciﬁed; thus, the
machine’s operation would give rise to speciﬁc cases where the action called for is
not determined. When such a speciﬁc case arises, the following is done: an action
is selected randomly and applied tentatively, by making the appropriate entry
in the machine’s description. This is the point where the teacher “educates” the
machine.
The general idea of employing pleasure and pain he has in mind is revealed
in his discussion of “pleasure-pain systems.” We can get the general idea with-
out getting into the details too much. He is considering unorganized machines
whose states are described using two expressions, one of which he calls “charac-
ter”: “Pleasure interference tends to ﬁx the character, i.e., towards preventing
it changing, whereas pain stimuli tend to disrupt the character, causing features
which had become ﬁxed to change, or to become again subject to random vari-
ation.” When he describes the particular experiment he carried out, though,
which he refers to as a “particular type of pain-pleasure system”, the analogy
seems to employ the brain-machine analogy quite directly: “When a pain stimu-
lus occurs all tentative entries are cancelled, and when a pleasure stimulus occurs
they are all made permanent.” [10, p. 118] At the time, he found it took too
much work to pursue this means of educating a machine much farther than the
rather simpliﬁed version of it he had carried out.

708
S.G. Sterrett
As his friend and colleague Donald Michie put it, they were waiting for hard-
ware. Michie recounts a story about one of Turing’s plans to program the “Manch-
ester Baby” to investigate what would happen when two diﬀerent programs for
playing chess were pit against each other: “[Turing] was thwarted (rightly) by .
. . the guardian of its scarce resources, Tom Kilburn.” According to Michie, in
the years leading up to Turing’s 1948 and 1950 papers on intelligent machinery,
Michie, Turing, and Jack Good “formed a sort of discussion club focused around
Turing’s astonishing ‘child machine’ concept.1 His proposal was to use our knowl-
edge of how the brain acquires its intelligence as a model for designing a teachable
intelligent machine.” [1] The idea that the source of learning might be sought in
some random elements of neural physiology was well-known in psychology; decades
earlier, in his Principles of Psychology, William James had concluded a discussion
on the formation of pathways in the brain: “ All this is vague to the last degree,
and amounts to little more than saying that a new path may be formed by the
sort of chances that in nervous material are likely to occur.” [3, p. 104] The dis-
cussion club worked on developing the analogy for machines; one might say that
what they were doing in that discussion club was developing the basic ideas of
what has since become known as reinforcement learning. Michie later showed that
reinforcement learning could indeed be successfully carried out in machines, prov-
ing sceptics wrong. [4] In 1948, though, there was still a “wait for hardware”, and
having to wait for the hardware to be available to test their ideas must have been
frustrating.
Turing also outlined other approaches he would have liked to try: one might
program one’s “teaching policies” into the machine, and let it run for awhile,
modifying its own programs, and periodically check to see how much progress
it has made in its education. He regarded the problem of building a machine
that would display “initiative” as well as “discipline” (as he put it) as crucial.
Achieving discipline in a machine: that we can see how to do. What initiative
adds to discipline in a machine: this is a matter of comparing humans and
machines with complete discipline, and asking what humans that are able to
communicate had in addition to discipline. Then, one could address what it was
that should be copied in the machine. A question remained as to what process
to use that achieves ending up with a machine that had both. In particular, in
what order should these two be instilled in the machine: ﬁrst, discipline, then
initiative, or somehow both together?
3
Teachers, Singular and Plural
In most of Turing’s examples of a teacher educating a machine, it seems he
is thinking of one or at most a few individual teachers. The kind of machine
under consideration is a universal (i.e., programmable) machine, speciﬁcally, a
digital computer, equipped with a means of “at most, organs of sight, speech,
1 Turing actively sought out discussion with colleagues. Another such colleague was
the philosopher Ludwig Wittgenstein, whose seminar he attended. For Turing’s ‘con-
structive uses’ of their discussions, cf. [2].

Bringing Up Turing’s ‘Child-Machine’
709
and hearing.” His investigations are, as a result, biased towards activities that
“require little contact with the outside world.” [10, p. 117] There are other
obstacles, too: even if a machine were equipped with the ability to navigate
physically, there are limitations on its abilities to be socialized. More than once,
he mentions the advantage that human learners have in that they are able to
beneﬁt from interactions with other humans.2
It is interesting that a major piece of research in cultural anthropology on
cross-cultural features of child-rearing appeals to neural processes very much
in line with Turing’s “pain-pleasure systems”, except that it is evaluations of
goodness and badness, rather than physical pleasure and physical pain, that are
administered. What is interesting is that this work [5] provides a model of what
the education process of a human child would be on the “it takes a village to
raise a child” view: “Cultural models of child rearing, thus, exploit the neural
capacities of the children so reared, to achieve a result, adulthood, that could
not be accomplished by the human brain alone.” One can see the kind of issues
this might raise: what if diﬀerent members of the community contradict each
other? This is exactly the issue (“constancy of [the child’s] experience”) that the
cultural anthropologists in [5] discovered was universally deemed important.
Turing does not talk about this kind of education—education by a community,
or education constrained and informed by cultural norms—of a child-machine,
but he does talk about the role of the human community in the intellectual
activity of humans. It occurs in his discussion of initiative. In that discussion,
by the time he got to talking about human community, he had already treated
the issues of discipline and initiative separately, and the education of the child-
machine had been limited to the instillation of discipline into the machine. Yet,
given that he did, albeit very brieﬂy, indicate that the analogy between brain and
machine might involve how the human community is involved in the education
process of a human, the question of an analogue for a community as teacher
during the education process of a machine arises quite naturally.
We might ask, what kind of community? Turing considered digital comput-
ers that have “organs” for sight, speech, and hearing. The newest cellphones
(e.g., equipped with Siri) have such “organs”, and they incorporate some ma-
chine learning capabilities, including learning the preferences and habits of their
owners. These kinds of machines (state of the art cellphones) generally interact
with and “learn from” a single human owner. Yet, they interact with other vir-
tual agents who communicate with them. Now that we have these possibilities
not available to Turing, we might consider following through in more detail on
remarks that Turing made about “intellectual search” by humans immersed in
a human community, and consider what the analogous processes for a machine
might be.
2 Harry Collins’ research on imitation games extends this observation about the
value of interactions with others. He also ties the kind of knowledge gained in
this way with the kind of knowledge that can be exhibited using imitation games.
http://www.cardiff.ac.uk/socsi/contactsandpeople/harrycollins/expertise-
project/imitationgameresearch.html

710
S.G. Sterrett
4
Imitation and Intelligence
Turing concludes his 1948 paper “Intelligent Machinery” with a section on the
concept of intelligence, which ends in the description of an experiment. The
experiment involves three people and the game of chess.
“It is not diﬃcult to devise a paper machine which will play a not
very bad game of chess. Now get three [humans] as subjects for the
experiment A, B, C. A and C are to be rather poor chess players, B is
the operator who works the paper machine. . . . Two rooms are used
with some arrangement for communicating moves, and a game is played
between C and either A or the paper machine. C may ﬁnd it diﬃcult to
tell which he is playing.” [10, p. 127]
By ‘paper machine’, Turing means creating “the eﬀect of a computing machine
by writing down a set of rules of procedure and asking a man to carry them out.
” [10, p. 113] So, the human who operates the paper machine, in conjunction
with the written rules of procedure, is imitating a machine. Now, although it
is meant to be straightforward to imitate a machine by this method, Turing
does not consider it trivially easy, for he advises using someone who is both a
mathematician and a chess player to work the paper machine.
Now, this experimental setup is most assuredly not intended to be an objective
measure of intelligence of the paper machine. To prevent any charge of interpre-
tive license, let me quote Turing from this last section of the paper, which bears
the heading “Intelligence as an emotional concept”: “The extent to which we
regard something as behaving in an intelligent manner is determined as much
by our own state of mind and training as by the properties of the object under
consideration.”
Upon what, then, does regarding something as intelligent depend? His answer
is given in terms of what it is that would rule out regarding something as in-
telligent: “If we are able to explain and predict its behaviour or if there seems
to be little underlying plan, we have little temptation to imagine intelligence.”
Diﬀerent people bring diﬀerent skills with respect to explaining and predicting
the behavior of something: “With the same object therefore it is possible that
one man would consider it as intelligent and another would not; the second man
would have found out the rules of its behavior.”
The experiment is set up as a comparison: between A, a “rather poor” chess
player, and B, a paper machine. The experiment is not in terms of whether
B can beat A at chess—the way the experiment is set up, B, which is a man
imitating the behavior of a machine by implementing rules that could be carried
out by a machine, but which are written by a human and intended to be read
by a human, will likely win some rounds. The comparison is not between chess-
playing abilities, but between how transparent it is to C that B’s behavior is
being produced by following a set of written rules capable of being carried out
by a machine.
While “Intelligent Machinery” closed with the description of a three person
game about telling the diﬀerence between a performance generated by ‘rules

Bringing Up Turing’s ‘Child-Machine’
711
of behavior’ and one by a human, “Computing Machinery and Intelligence”
opened with such a three person game. The three persons in the game (called an
imitation game) were named A, B, and C, too, and C was to distinguish between
A and B. There was a diﬀerence, though: the moves being communicated were
not positions in a game of chess, but taking one’s turn in a conversation. The
distinction was not a matter of distinguishing between which player was a person
and which a machine, but between which conversationalist was a man and which
was a woman.
The experimental setup in Turing’s 1950 paper that I dubbed “The Original
Imitation Game Test” is very like a TV game show that premiered six years
later, in 1956, called “To Tell The Truth.” It was played as follows:
“ Three challengers are introduced, all claiming to be the central
character.
[. . .] the host reads aloud a signed aﬀadavit about the central char-
acter.
The panelists are each given a period of time to question the chal-
lengers. Questions are directed to the challengers by number (Number
One, Number Two and Number Three), with the central character sworn
to give truthful answers, and the impostors permitted to lie and pretend
to be the central character.
After questioning is complete, each member of the panel votes on
which of the challengers they believe to be the central character, [. . .]
Once the votes are cast, the host asks, “Will the real [person’s name]
please stand up?” The central character then stands, [. . .] Prize money
is awarded to the challengers based on the number of incorrect votes the
impostors draw.” [11]
Being a convincing imposter can be diﬃcult. In previously published work, I
have argued that the OIG Test is a better game than the one currently referred
to as “the Turing Test.” [8,7,6]. One reason I gave for my view was that the task
given the machine in the OIG Test is the same as the task set for the human in
the OIG Test: to imitate something that it is not.3 The concept of a machine
being set the task of imitation should not seem at all foreign here—in fact, the
term “imitation” is used by Turing in describing a universal machine; he speaks
of the ability of a universal machine to imitate other machines. Isn’t imitation
a straightforward task for a computer, then, you may ask?
No, I don’t think it is. For an uneducated machine (such as an uneducated
universal machine) to imitate is one thing—it amounts to implementing a pro-
gram. For an educated machine to imitate is quite another. In fact, I argued,
what is called for is not really imitation, but ﬁguring out and carrying out what
3 In [8] I also show that the two tests in [9] give diﬀerent quantitative, as well as
qualitative, results. I consider it a major contribution of [8] to give what amounts to
a proof that the two tests are unequivocably diﬀerent on signiﬁcant points, and that
the OIG Test need not be set up around gender diﬀerences. Secondary literature
citing [8] has not always recognized these two major points.

712
S.G. Sterrett
it takes to be a convincing imposter. While the central character in “To Tell
the Truth” may give an answer to a question without any fear of being led to
another question that he or she cannot answer, the imposter has to think how
to keep the conversation from turning to topics that might present problems for
an imposter. An imposter has to constantly be on guard to override tendencies
to respond in ways that have by now become habitual, but which are inappro-
priate while posing as an imposter. Talk of overriding habits is, I believe, no
longer fanciful talk, as reading about work done by robotics researchers on the
diﬃculties faced in applying imitation learning in robotics will reveal. If IBM is
looking for suggestions for its next Grand Challenge, let me suggest “To Tell the
Truth.”4
I shall not repeat all the points I made in those earlier works on Turing and
tests for intelligence. Rather, my point in this talk about the OIG Test concerns
a question germane to the education of Turing’s Child-Machines. I suggest that
reﬂecting on the question of how machines produced using diﬀerent methods for
educating machines fare on the OIG Test leads to useful ways of thinking about
machine intelligence.
Acknowledgements. I should like to thank an anonymous referee for some
helpful remarks, and the audience at the Fourth Regional Wittgenstein Work-
shop held at Washington and Lee University on March 11th, 2012 for discussion
on this paper.
References
1. BCS Computer Conservation Society. Recollections of early AI in Britain: 1942 -
1965 (2002); (video for the BCS Computer Conservation Society’s October 2002
Conference on the history of AI in Britain) transcript (downloaded on March 25,
2012)
2. Floyd, J.: Turing, Wittgenstein, and Types: Philosophical Aspects of Turing’s ‘The
Reform of Mathematical Notation and Phraseology’ (1944-1945). In: Barry Cooper,
S., van Leeuwen, J. (eds.) Alan Turing - His Work and Impact. The Collected Works
of A. M. Turing, revised edn. North-Holland, Elsevier (2001) (to appear)
3. James, W.: The Principles of Psychology, vol. I. Henry Holt and Company, New
York (1890)
4. Michie, D.: Trial and Error. Science Survey, part 2, 129–145(1961)
5. Quinn, N.: Cultural Selves. Annals of the New York Academy of Sciences 1001,
145–176 (2003)
6. Sterrett, S.G.: Too Many Instincts: Contrasting Philosophical Views on Intelligence
in Humans and Non-Humans. JETAI Journal of Experimental and Theoretical
Artiﬁcial Intelligence 14(1), 39–60 (2002b); reprinted in: Ford, K., Glymour, C.,
Hayes, P. (eds.) Thinking About Android Epistemology. MIT Press (March 2006)
4 IBM’s Deep Blue took on the challenge of a machine playing chess at the Grand-
master level. IBM’s Watson (with DeepQA technology) took on the challenge of
competing in the game show Jeopardy!

Bringing Up Turing’s ‘Child-Machine’
713
7. Sterrett, S.G.: Nested Algorithms and The Original Imitation Game Test: A Reply
to James Moor. Minds and Machines 12, 131–136 (2002a)
8. Sterrett, S.G.: Turing’s Two Tests for Intelligence. Minds and Machines 10, 541–
559 (2000); reprinted in: Moor, J.H. (ed.) The Turing Test: The Elusive Standard
of Artiﬁcial Intelligence. Kluwer Academic (2003)
9. Turing, A.M.: Computing machinery and intelligence. Mind 59, 433–460 (1950)
10. Turing, A.M.: Intelligent Machinery. In: Turing, A.M., Ince, D.C. (ed.) Mechanical
Intelligence, Collected Works, pp. 107–127. North Holland (1948/1992)
11. Wikipedia contributors. To Tell the Truth. Wikipedia, The Free Encyclopedia (Jan-
uary 27, 2012), Web (January 27, 2012)

Is Turing’s Thesis the Consequence
of a More General Physical Principle?
Matthew P. Szudzik
Carnegie Mellon University, Education City, PO Box 24866, Doha, Qatar
mszudzik@cmu.edu
Abstract. We discuss historical attempts to formulate a physical hy-
pothesis from which Turing’s thesis may be derived, and also discuss
some related attempts to establish the computability of mathematical
models in physics. We show that these attempts are all related to a
single, uniﬁed hypothesis.
1
Introduction
Alan Turing [14] proposed the concept of a computer—that is, the concept
of a mechanical device that can be programmed to perform any conceivable
calculation—after studying the processes that humans use to perform calcula-
tions. In particular, he claimed that any function of non-negative integers which
can be eﬀectively calculated by humans is a function that can be calculated by a
Turing machine. This claim, known as Turing’s thesis, is an empirical principle
that has withstood many tests to its validity.1 But why does Turing’s thesis
seem to be true? Can it be derived, for example, from a principle of contempo-
rary sociology, from a principle of human biology, or perhaps from a principle of
fundamental physics?
We shall be concerned with the last of these questions, namely the question
of whether Turing’s thesis is the consequence of a physical principle. But before
discussing some of the historically important attempts to answer this question,
let us introduce the following formalism. Given a physical system, deﬁne a de-
terministic physical model for the system to be
1. a set S of states
together with
2. a set A of functions from S to the real numbers, and
3. for each non-negative real number t, a function Φt from S to S.
If the system begins in state s, then we understand Φt (s) to be the state of
the system after t units of time. Furthermore, we identify each member α of
1 Indeed, every time a software developer attempts to write a computer program to
implement an explicitly-described procedure, Turing’s thesis is tested.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 714–722, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

Is Turing’s Thesis the Consequence of a More General Physical Principle?
715
A with an observable quantity of the system,2 and we consider α (s) to be the
value predicted for the observable quantity when the system is in state s. For
example, the following is a deterministic physical model for a particle that is
moving uniformly along a straight line with a velocity of 3 meters per second.
Model 1. Let S be the set of all real numbers and deﬁne Φt (x) = x + 3t for
all states x in S, where t is the time measured in seconds. The position of the
particle on the line, measured in meters, is given by the function α (x) = x.
We say that a deterministic physical model is faithful if and only if there is a
state s0 such that, for each time t, the actual values of the observable quantities
at that time agree with the values that are predicted when the system is in state
Φt (s0). Deterministic physical models are commonly studied in the theory of
dynamical systems.
Now, the ﬁrst notable attempt to derive Turing’s thesis from a principle of
physics appears to have been made by Robert Rosen [12]. Rosen hypothesized3
that every physical system has a faithful deterministic physical model where
1. the set S is the set of all non-negative integers,
2. each α in A is a total recursive function,4 and
3. when restricted to non-negative integer times t, Φt (s) is a total recursive
function of s and t.
Note that since recursive functions are functions from non-negative integers to
non-negative integers, Condition 2 implies that the values of all observable quan-
tities are non-negative integers. This hypothesis is justiﬁed by the fact that actual
physical measurements have only ﬁnitely many digits of precision. For example,
a distance measured with a meterstick is a non-negative integer multiple m of
the length of the smallest division on the meterstick. Therefore, without loss of
generality, we can consider m to be the value of the measurement. Nevertheless,
Condition 2 does not forbid time from being measured with a non-negative real
number, since time is not regarded as an observable quantity in deterministic
physical models. For example, it is consistent with Rosen’s hypotheses to specify
that
Φt (s) = Φ⌊t⌋(s)
(1)
for all non-negative real numbers t and for all states s, where ⌊t⌋denotes the
largest integer less than or equal to t.
Now, Turing’s thesis can be derived from Rosen’s hypotheses as follows. First,
note that by deﬁnition, if a function ψ is eﬀectively calculable then there is a
2 To ensure that the predictions of the model are unambiguous, it is common practice
in physics to deﬁne each observable quantity operationally [1].
3 Rosen did not fully formalize his hypotheses. The hypotheses given here are reason-
able formal interpretations of Rosen’s “Hypothesis I” [12].
4 We shall use terminology from the theory of recursive functions [11] throughout this
paper. It follows from Turing’s work [15] that the set of all functions from non-
negative integers to non-negative integers that are computable by Turing machines
is identical to the set of recursive functions.

716
M.P. Szudzik
physical system that can reliably be used to calculate ψ (x) for every non-negative
integer x. This means that there must be a system where the input can be
observed, where the output can be observed, and where it is possible to observe
that the system is ﬁnished with the calculation. In the context of deterministic
physical models this means that the system has observable quantities α, β, and
γ, respectively, such that for each non-negative integer x
1. there exists a state s and a time u such that α (s) = x and γ (Φt (s)) = 1 for
all t ≥u, and
2. if α (s) = x for any state s then β (Φt (s)) = ψ (x) where t is any time such
that γ (Φt (s)) = 1.
It then immediately follows from Rosen’s hypotheses that every eﬀectively cal-
culable function is recursive, whether it is calculated by a human being or by
any other physical system. That is, Turing’s thesis is a consequence of Rosen’s
hypotheses.
Informally, Rosen’s hypotheses can be understood as stating that the uni-
verse is discrete, deterministic, and computable. It is important then to ask
whether we are living in a discrete, deterministic, computable universe. Un-
fortunately, according to the currently-understood laws of physics, the answer
appears to be “No.” The universe, as we currently understand it, does not seem
to satisfy Rosen’s hypotheses. Nevertheless, it may be possible to reformulate
the currently-understood physical laws so that Rosen’s hypotheses are satisﬁed.
This sort of reformulation was ﬁrst attempted by Konrad Zuse [18,19] in the
1960’s. Since then, increasingly sophisticated attempts have been made by Ed-
ward Fredkin [3] and Stephen Wolfram [17]. In contrast, Roger Penrose [9,10]
has speculated that the universe might not be computable, but eﬀorts to ﬁnd
experimental evidence for this assertion have not succeeded. There is, in fact,
very little that can currently be said. The question of whether Turing’s thesis
is the consequence of a valid physical principle is too diﬃcult to be answered
conclusively at this time.
2
Physical Models
Although deterministic physical models are widely used in the study of dynam-
ical systems and have important real-world applications, they are not the only
sort of model that is useful in physics. The sorts of models used in quantum elec-
trodynamics [2], for example, are necessarily non-deterministic. And it is diﬃcult
to reconcile the way that time is modeled in general relativity [4] with the way
that it is treated as a linear quantity external to the states in a deterministic
physical model. Anyone who has attempted to reformulate the laws of physics
so that Rosen’s hypotheses are satisﬁed has had to grapple with these obstacles,
and no one has had complete success.
For this reason, we have proposed [13] a more general sort of model which
we simply call a physical model. A physical model for a system is a set S of
states together with a set A of functions from S to the real numbers. Each

Is Turing’s Thesis the Consequence of a More General Physical Principle?
717
member α of A is identiﬁed with an observable quantity of the system,5 and
α (s) is the value predicted for that observable quantity when the system is in
state s. Deterministic physical models are special sorts of physical models. For
example, the deterministic physical model that was described in the introduction
(Model 1) can be expressed as the following physical model.
Model 2. Let S be the set of all triples (x, x0, t) of real numbers such that
t > 0 and x = x0 + 3t. The position of the particle on the line, measured
in meters, is given by the function α (x, x0, t) = x. The initial position of the
particle (for example, recorded in the observer’s notebook) is given by the func-
tion β (x, x0, t) = x0. The time, measured in seconds, is given by the function
γ (x, x0, t) = t.
In contrast to Model 1, time is treated as an observable quantity in this model, as
is the initial position. The inclusion of these observable quantities in the model
can be justiﬁed physically by noting that if a researcher were to test Model 1 in
a laboratory experiment, he would be required to measure the initial position
x0 of the particle and the position x of the particle at some later time t. That
is, besides the position x, observations of both the time and the initial position
are fundamental to the system.
Physical models can also be used to describe non-deterministic systems. For
example, suppose that one atom of the radioactive isotope nitrogen-13 is placed
inside a detector at time t = 0. We say that the detector has status 1 at time t
if the decay of the isotope was detected at any earlier time, and we say that the
detector has status 0 otherwise. We use a non-negative integer to represent the
history of the detector. In particular, if bi is the status of the detector at time
t = i, then the history of the detector at time t = n is h = n
i=1 bi2n−i. Note
that when h is written as an n-bit binary number, the ith bit (counting from
left to right) is bi. For example, if the isotope decays sometime between t = 2
and t = 3, then the history of the detector at time t = 5 is 7 because the binary
representation of 7 is (00111)2. Now, if the history of the detector is displayed
on a computer screen, then the following is a physical model for the history of
the detector as observed by a researcher looking at the screen.
Model 3. Let S be the set of all triples

t, 2d −1, k

where d, t, and k are non-
negative integers such that d ≤t, t ̸= 0, and 2k ≤2d −1. The history of the
detector is given by the function α (t, h, k) = h, and the time, measured in units
of the half-life of the isotope, is given by the function β (t, h, k) = t.
In a deterministic model such as Model 2, the state of the system at a particular
time is uniquely determined from its initial state. But this is not the case for
Model 3, which is a non-deterministic physical model. For example, if we are
given as an initial condition that the detector had status 0 at time t = 1, then
there are four diﬀerent states at time t = 3 which are consistent with that initial
condition.
(3, (000)2 , 0)
(3, (001)2 , 0)
(3, (011)2 , 0)
(3, (011)2 , 1)
5 As before, we require that each observable quantity be deﬁned operationally.

718
M.P. Szudzik
In addition, Model 3 has the property that if these states are considered to be
equally likely, then the corresponding probability of observing a given history
at time t = 3 matches the probability predicted by the conventional theory
of radioactive decay. For example, since the history (011)2 is observed in half
of the four states, there is a 1
2 probability that the detector’s history will be
(011)2 at time t = 3 if the detector had status 0 at time t = 1. See [13] for
a more complex example of a non-deterministic physical model that involves
incompatible quantum measurements.
3
Computable Physical Models
Deﬁne a computable physical model to be a physical model where S is a recursive
set of non-negative integers and where each α in A is a total recursive function.6
Of course, deterministic physical models that satisfy Rosen’s hypotheses can all
be expressed as computable physical models. Non-deterministic models, such as
the model for radioactive decay (Model 3), can also be expressed as computable
physical models. Now, we assert the following hypothesis.
Computable Universe Hypothesis. The laws of physics can be expressed as
a computable physical model.
To show that Turing’s thesis is a consequence of the computable universe hy-
pothesis, ﬁrst recall that if a function ψ is eﬀectively calculable then there is
a physical system that can reliably be used to calculate ψ (x) for every non-
negative integer x. This means that in some state of the universe it must be
possible to observe a record of the the function’s input, to observe the function’s
output, and to observe that the calculation has ﬁnished. In the context of a
physical model for the universe, this means that there are observable quantities
α, β, and γ, respectively, such that for each non-negative integer x
1. there exists a state s such that α (s) = x and γ (s) = 1, and
2. if α (s) = x and γ (s) = 1 for any state s, then β (s) = ψ (x).
It then immediately follows from the computable universe hypothesis that every
eﬀectively calculable function is recursive, whether it is calculated by a human
being or by any other system that is governed by physical law. Hence, Turing’s
thesis is a consequence of the computable universe hypothesis.
Of course, as was the case with Rosen’s hypotheses, it is not known whether
the computable universe hypothesis is true. But since the computable universe
hypothesis allows for non-determinism and for more complex temporal relation-
ships, it may be somewhat easier to reformulate the currently-understood phys-
ical laws so as to satisfy this less-restrictive hypothesis.
We conclude this section by considering the following example that helps
to clarify certain features of our deﬁnition of eﬀective calculability in the con-
text of non-deterministic computable physical models. First, let ⟨x, y⟩denote a
6 See the isomorphism theorems in [13] for other, equivalent deﬁnitions of a computable
physical model.

Is Turing’s Thesis the Consequence of a More General Physical Principle?
719
non-negative integer that encodes the pair (x, y) of non-negative integers. For
example, using Cantor’s pairing function, we could deﬁne
⟨x, y⟩= 1
2

x2 + 2xy + y2 + 3x + y

.
(2)
Triples (x, y, z) of non-negative integers can then be encoded as ⟨⟨x, y⟩, z⟩. We
shall use ⟨x, y, z⟩as an abbreviation for ⟨⟨x, y⟩, z⟩. Also deﬁne x mod 2 to be the
rightmost bit in the binary representation of the non-negative integer x. Now
consider the following computable physical model for the history of a ‘noisy’
detector.
Model 4. Let S be the set of all pairs ⟨t, h⟩where t and h are non-negative
integers such that t ̸= 0 and h ≤2t −1. The time is given by the function
α ⟨t, h⟩= t, the current status of the detector is given by the function β ⟨t, h⟩=
h mod 2, a trivial observable quantity is given by the function γ ⟨t, h⟩= 1, and
the history of the detector is given by the function δ ⟨t, h⟩= h.
Although superﬁcially similar to Model 3, all histories are possible in Model 4.
That is, there are no restrictions on the status of the detector. At any point in
time and regardless of the detector’s past history, the status of the detector can
be 1 or 0. Now imagine forming a tree by taking each state ⟨t, h⟩of Model 4 as
a node of the tree, and by taking the children of this node to be those states
of the form ⟨t + 1, h′⟩where the ﬁrst t bits of h′ agree with the ﬁrst t bits of
h. We call this the tree of histories for Model 4, and every branch on this tree
corresponds to an alternate sequence of histories for the detector. Moreover, we
can think of the status of the detector as deﬁning a function along each branch
of the tree. For each state s on the branch, the input of the function is α (s),
the corresponding output is β (s), and the fact that the calculation has ﬁnished
is indicated by γ (s). But since all histories are possible for this detector, every
possible function ψ from the positive integers to the set {0, 1} is calculated by
the detector along some branch, including functions ψ which are not recursive
(that is, not computable by a Turing machine). Therefore, Model 4 is an example
of a computable physical model where non-recursive functions may be calculated
along certain branches of the tree of histories. It is important to note, though,
that according to our deﬁnition of eﬀective calculability, these non-recursive
functions are not eﬀectively calculable, since for each state s of Model 4 there
is another state s′ such that α (s) = α (s′) and γ (s) = γ (s′), but β (s) ̸= β (s′).
In other words, the detector cannot reliably be used to calculate ψ because the
detector is behaving non-deterministically.
4
Continuous Models
Although Turing’s thesis holds in all discrete, deterministic, computable uni-
verses, Turing himself did not believe that the universe is discrete. In particular,
Turing [16] stated that

720
M.P. Szudzik
digital computers . . . may be classiﬁed amongst the ‘discrete state ma-
chines’. These are the machines which move by sudden jumps or clicks
from one quite deﬁnite state to another. These states are suﬃciently
diﬀerent for the possibility of confusion between them to be ignored.
Strictly speaking there are no such machines. Everything really moves
continuously.
But discreteness is not a prerequisite for computability. In fact, Georg Kreisel [5]
has hypothesized that the universe may be continuous and computable.
Before we turn to a discussion of Kreisel’s hypothesis, deﬁne a functional
physical model for a system to be a set D of ﬁnitely many real-valued functions,
each of which takes k real numbers as input, for some non-negative integer k.
We identify each member δ of D with an observable quantity of the system, and
also identify each of the k inputs with an observable quantity. The observable
quantities that are identiﬁed with the inputs are said to be the given quantities
for the model,7 and the observable quantities that are identiﬁed with the mem-
bers of D are said to be the predicted quantities of the model. We say that a
functional physical model is faithful if and only if the values of the predicted
quantities in the model match the values that are actually measured whenever
the values of the given quantities in the model match the values that are actually
measured.8
For example, the deterministic physical model that was described in the intro-
duction (Model 1) can be expressed as the following functional physical model.
Model 5. Let the position of the particle on the line, measured in meters, be
given by the function δ (x0, t) = x0 + 3t. The initial position of the particle is
x0. The time, measured in seconds, is t.
In this case, the position of the particle is the only predicted quantity. The
particle’s initial position and the time are the given quantities.
Now, note that every integer i can be encoded as a non-negative integer ζ (i),
where ζ (i) = 2i if i ≥0 and where ζ (i) = −2i−1 if i < 0. Each rational number
a
b in lowest-terms with b > 0 can be encoded as a non-negative integer ρ
 a
b

where
ρ
a
b

= ζ

(sgn a) 2ζ(a1−b1)3ζ(a2−b2)5ζ(a3−b3)7ζ(a4−b4)11ζ(a5−b5) · · ·

(3)
and where
a = (sgn a) 2a13a25a37a411a5 · · ·
(4)
7 To ensure that each function δ in D is deﬁned for all real number inputs, the scale of
each given quantity should be adjusted so that it ranges over all real numbers. For
example, if a given quantity is a temperature and if δ is undeﬁned for temperatures
below 0 degrees Kelvin, then a logarithmic temperature scale should be used instead
of the Kelvin scale.
8 In practice, of course, measurement errors will prevent us from knowing these values
exactly.

Is Turing’s Thesis the Consequence of a More General Physical Principle?
721
and
b = 2b13b25b37b411b5 · · ·
(5)
are the prime factorizations of a and b, respectively. For each pair of rational
numbers q and r, deﬁne (q ; r) to be the non-negative integer ⟨ρ (q) , ρ (r)⟩. We
shall use (q ; r) to represent the open interval with endpoints q and r.
Next, let IN denote the set of non-negative integers and let IR denote the
set of real numbers. Note that every real number x can be represented as a
nested sequence of open intervals whose intersection is x. We say that a function
φ : IN →IN is a nested oracle for x ∈IR if and only if φ (0) = (a0 ; b0), φ (1) =
(a1 ; b1), φ (2) = (a2 ; b2), . . . is a sequence of nested intervals whose intersection
is x. A function δ : IR →IR is said to be computable (in the sense deﬁned by
Lacombe [6,7,8]) if and only if there is a total recursive function ξ such that if
φ is a nested oracle for x ∈IR, then λm [ ξ (φ (m))] is a nested oracle for δ(x).
This deﬁnition can naturally be extended to functions from IRk to IR, for any
k ∈IN. Finally, we say that a functional physical model is computable if and
only if every member of the set D is computable in the sense that we have just
described.
Let us now return to a discussion of Kreisel’s hypothesis. Kreisel hypothe-
sized that every faithful functional physical model is computable. For example,
Model 5 is computable because in that model the function δ : IR2 →IR is
computed by the total recursive function ξ that is deﬁned so that
ξ ((a ; b) , (c ; d)) = (a + 3c ; b + 3d)
(6)
for all rational numbers a, b, c, and d. Moreover, every computable functional
physical model can be expressed as a computable physical model. For example,
Model 5 can be expressed as the following computable physical model.
Model 6. Let S be the set of all triples ⟨(a ; b) , (c ; d) , (a + 3c ; b + 3d)⟩where
a, b, c, and d are rational numbers such that a < b and c < d. The initial
position of the particle on the line, represented as a range of positions measured
in meters, is given by the function α ⟨(a ; b) , (c ; d) , (e ; f)⟩= (a ; b). The time
interval, measured in seconds, is given by the function β ⟨(a ; b) , (c ; d) , (e ; f)⟩=
(c ; d). The position of the particle, represented as a range of positions measured
in meters, is given by the function γ ⟨(a ; b) , (c ; d) , (e ; f)⟩= (e ; f).
Note that the states of Model 6 are all of the form
⟨(a ; b) , (c ; d) , ξ ((a ; b) , (c ; d))⟩,
(7)
and this guarantees that if φ is a nested oracle for some initial position x0 and
if ψ is a nested oracle for some time t, then there is a unique sequence of states
s0, s1, s2, . . . in S such that α (sm) = φ (m) and β (sm) = ψ (m) for all m ∈IN.
Therefore,
γ (sm) = ξ (φ (m) , ψ (m))
(8)
for all m ∈IN, and λm [γ (sm)] is a nested oracle for the position δ (x0, t) that is
predicted by Model 5. Thus, there is a direct correspondence between the func-
tional physical model (Model 5) and the computable physical model (Model 6).
See [13] for more information regarding this correspondence.

722
M.P. Szudzik
In summary, the computable physical models comprise a very general class of
models, capable of expressing discrete deterministic models such as those studied
by Rosen, non-deterministic models such as the model for radioactive decay, and
continuous models such as those studied by Kreisel. Furthermore, Turing’s thesis
is a consequence of the hypothesis that the laws of physics can be expressed as
a computable physical model. It is very tempting, therefore, to wonder whether
this hypothesis might be true.
References
1. Bridgman, P.W.: The Logic of Modern Physics. Macmillan (1927)
2. Feynman, R.P.: QED: The Strange Theory of Light and Matter. Princeton Uni-
versity Press (1985)
3. Fredkin, E.: Digital mechanics. Physica D 45, 254–270 (1990)
4. G¨odel, K.: An example of a new type of cosmological solutions of Einstein’s ﬁeld
equations of gravitation. Reviews of Modern Physics 21, 447–450 (1949)
5. Kreisel, G.: A notion of mechanistic theory. Synthese 29, 11–26 (1974)
6. Lacombe, D.: Extension de la notion de fonction r´ecursive aux fonctions d’une
ou plusieurs variables r´eelles I. Comptes Rendus Hebdomadaires des S´eances de
l’Acad´emie des Sciences 240, 2478–2480 (1955)
7. Lacombe, D.: Extension de la notion de fonction r´ecursive aux fonctions d’une
ou plusieurs variables r´eelles II. Comptes Rendus Hebdomadaires des S´eances de
l’Acad´emie des Sciences 241, 13–14 (1955)
8. Lacombe, D.: Extension de la notion de fonction r´ecursive aux fonctions d’une
ou plusieurs variables r´eelles III. Comptes Rendus Hebdomadaires des S´eances de
l’Acad´emie des Sciences 241, 151–153 (1955)
9. Penrose, R.: The Emperor’s New Mind. Oxford University Press (1989)
10. Penrose, R.: Shadows of the Mind. Oxford University Press (1994)
11. Rogers, Jr., H.: Theory of Recursive Functions and Eﬀective Computability.
McGraw-Hill (1967)
12. Rosen, R.: Church’s thesis and its relation to the concept of realizability in biology
and physics. Bulletin of Mathematical Biophysics 24, 375–393 (1962)
13. Szudzik, M.P.: The computable universe hypothesis. In: Zenil, H. (ed.) A Com-
putable Universe. World Scientiﬁc (forthcoming 2012)
14. Turing, A.M.: On computable numbers, with an application to the Entschei-
dungsproblem. Proceedings of the London Mathematical Society 42, 230–265
(1937)
15. Turing, A.M.: Computability and λ-deﬁnability. The Journal of Symbolic Logic 2,
153–163 (1937)
16. Turing, A.M.: Computing machinery and intelligence. Mind 59, 433–460 (1950)
17. Wolfram, S.: A New Kind of Science. Wolfram Media (2002)
18. Zuse, K.: Rechnender raum. Elektronische Datenverarbeitung 8, 336–344 (1967)
19. Zuse, K.: Rechnender Raum. Friedrich Vieweg & Sohn (1969), see [20] for an En-
glish translation
20. Zuse, K.: Calculating space. Tech. Rep. AZT-70-164-GEMIT, Massachusetts Insti-
tute of Technology, Project MAC (1970)

Some Natural Zero One Laws
for Ordinals Below ε0
Andreas Weiermann and Alan R. Woods
Vakgroep Wiskunde, Universiteit Gent, Krijgslaan 281 S22, 9000 Ghent, Belgium
Andreas.Weiermann@UGent.be
Abstract. We are going to prove that every ordinal α with ε0 > α ≥ωω
satisﬁes a natural zero one law in the following sense. For α < ε0 let
Nα be the number of occurences of ω in the Cantor normal form of
α. (Nα is then the number of edges in the unordered tree which can
canonically be associated with α.) We prove that for any α with ωω ≤
α < ε0 and any sentence ϕ in the language of linear orders the limit
δϕ(α) = limn→∞
#{β<α:(β,∈)|=ϕ ∧Nβ=n}
#{β<α:Nβ=n}
exists and that δϕ(α) ∈{0, 1}.
We further show that for any such sentence ϕ the limit δϕ(ε0) exists
although this limit is in general in between 0 and 1. We also investigate
corresponding asymptotic densities for ordinals below ωω.
1
Introduction
This paper concerns logical limit laws for inﬁnite ordinals. It is based on methods
and techniques from the theory of logical limit laws for classes of ﬁnite struc-
tures and some ingredients from the theory of linear orders. We heavily use the
machinery (which to a large extent goes back to pioneering work of Compton)
developed in the book by Burris [2].
In 2001 the ﬁrst author (after having read [2] and having recognized that or-
dinals below ε0 form a natural additive number system) discussed the possibility
of limit laws for ordinals with Compton at an AOFA-workshop in Tatihoo and
Compton suggested among other things to contact the second author because
of his results on limit laws for trees. This led to a fruitful interaction over the
years. Tragically, the second author passed away at the age of 58 unexpectedly in
December 2011 and the ﬁrst author feels responsible to make publically available
the beautiful results which so far have been achieved (partially funded by DFG
and the John Templeton Foundation).
Technically this article is based on a mixture of results from [4] (which when
compared with the results from this article have preliminary nature) and [7].
Also some basic techniques from [2,3] (and implicitly [1]) are used.
More elaborate results of the authors (concerning larger ordinal segments and
more general languages, like the second order monadic ones) on limit laws for
ordinals are sketched at the end and will hopefully be treated at later occasions.
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 723–732, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

724
A. Weiermann and A.R. Woods
2
Some Basic Results
For two linear orders A and B let Gn(A, B) be the following two person game,
also well known as Ehrenfeucht Fra¨ısse game, between, lets say, Bob and Alice. A
play consists of an ordered sequence of n repetitions of the following: Bob chooses
an element of A or B and Alice chooses an element of the other. The element of
A selected at the t-th turn is denoted at and the element of B selected at the
t-th turn is denoted bt. We say that Alice has won the game if for each s, t ≤n
the assertion as < at (mod A) is equivalent with bs < bt (mod B). Otherwise we
say that Bob has won. As usual we deﬁne what a winning strategy for Alice is
and we say A ∼n B iﬀAlice has a winning strategy for Gn(A, B). (We took this
standard exposition from page 99 in Rosenstein’s classical text book [3] on linear
orders.) Let L be the usual ﬁrst order language of linear orders. Then α ∼n β
yields that α and β model exactly the same L-sentences ϕ of quantiﬁer rank not
exceeding n.
Lemma 1. Fix a natural number n ≥1. Let (ai)i≤n and (a′
i)i≤n be sequences
of natural numbers such that for all i ≤n: (ai ≥2n
⇐⇒
a′
i ≥2n) and
(ai < 2n ⇒ai = a′
i).
Then we have
ωn · an + · · · + ω0 · a0 ∼n ωn · a′
n + · · · + ω0 · a′
0.
(1)
Moreover for such sequences (ai)i≤n and (a′
i)i≤n and for any pair of non zero
ordinals α and β we have
ωn+1 · α + ωn · an + · · · + ω0 · a0 ∼n ωn+1 · β + ωn · a′
n + · · · + ω0 · a′
0.
(2)
Proof. We have only to show assertion (1) (but we still need assertion (2) for
applying the induction hypothesis). For, we have ωn+1 · α ∼2n+2 ωn+1 and
ωn+1 · β ∼2n+2 ωn+1 by assertion (1) of Theorem 6.18 in [3]. Assertion (1)
above and closure of ∼n under sums (i.e., assertion (1) of Lemma 6.5 of [3])
together with downward conservativity (cf., e.g., Lemma 6.4 of [3]) then yield
ωn+1 · α + ωn · an + · · · + ω0 · a0 ∼n ωn+1 · β + ωn · a′
n + · · · + ω0 · a′
0. For proving
assertion (1) we proceed by induction on n. The induction starts with n = 1 for
which the assertion holds. Now consider
γ = ωn · an + · · · + ω0 · a0
and
δ = ωn · a′
n + · · · + ω0 · a′
0.
We are going to apply Theorem 6.6 in [3] i.e., we verify the Fra¨ısse conditions for
games on linear orderings. So we have to show that for every splitting of γ (δ)
into an initial and ﬁnal segment there is a splitting of δ (γ) into corresponding
initial and ﬁnal segments for which the second player wins the corresponding
games on initial and ﬁnal segments separately with n −1 moves.

Some Natural Zero One Laws for Ordinals Below ε0
725
Assume without loss of generality that Player one picks ξ < γ. (In case that
he picks ξ < δ the argument is symmetrical.) Assume that
ξ = ωn · an + · · · + ωi+1 · ai+1 + ωi · bi + · · · + ω0 · b0
and bi < ai.
Assume ﬁrst that ai ≥2n.
In this case also a′
i ≥2n. If bi < 2n−1 let b′
i := bi. If If bi ≥2n−1 and
ai −bi < 2n−1 let b′
i := a′
i −(ai −bi) ensuring b′
i ≥2n−1. If bi ≥2n−1 and
ai −bi ≥2n−1 then let b′
i := 2n−1. Let
ξ′ := ωn · a′
n + · · · + ωi+1 · a′
i+1 + ωi · b′
i + · · · + ω0 · b0
and let Player two play ξ′. By induction hypothesis applied to (2) and assertion
(1) of Lemma 6.5 in [3] we obtain ξ ∼n−1 ξ′. So the initial segments determined
by ξ and ξ′ are ∼n equivalent and it remains that the ﬁnal segments in γ and δ
are also ∼n equivalent. To prove this let
X := {η : ξ < η < γ}
and
Y := {η : ξ′ < η < δ}.
For a set X of ordinals let as usual otype(X) denote its order type, i.e., the
ordinal which is order isomorphic to it. Then
otype(X) = ωi · (ai −bi) + ωi−1 · ai−1 + · · · + ω0 · a0.
If bi < 2n−1 then
otype(Y ) = ωi · (a′
i −bi) + ωi−1 · a′
i−1 + · · · + ω0 · a′
0
where ai −bi ≥2n−1 since a′
i ≥2n.
If bi ≥2n−1 and ai −bi < 2n−1 then
otype(Y ) = ωi · (a′
i −(a′
i −(ai −bi)) + ωi−1 · a′
i−1 + · · · + ω0 · a′
0
= ωi · (ai −bi) + ωi−1 · a′
i−1 + · · · + ω0 · a′
0.
If bi ≥2n−1 and ai −bi ≥2n−1 then
otype(Y ) = ωi · (a′
i −2n−1) + ωi−1 · a′
i−1 + · · · + ω0 · a′
0
where a′
i −2n−1 ≥2n−1 since a′
i ≥2n. By induction hypothesis applied to (2)
and closure of ∼n−1 under sum (assertion (1) of Lemma 6.5 in [3]) we obtain
X ∼n−1 Y.
Assume for the second case that ai < 2n. In this case by assumption on the
ai and a′
i we have necessarily a′
i = ai. Let
ξ′ := ωn · a′
n + · · · + ωi+1 · a′
i+1 + ωi · bi + · · · + ω0 · b0

726
A. Weiermann and A.R. Woods
be the response of Player two. To apply Theorem 6.6 of [3] we split as in the ﬁrst
case γ and δ into initial and ﬁnal segments. By induction hypothesis applied to
(2) and assertion (1) of Lemma 6.5 in [3] we obtain ξ ∼n−1 ξ′. As before let
X := {η : ξ < η < γ}
and
Y := {η : ξ′ < η < δ}.
otype(X) = ωi · (ai −bi) + ωi−1 · ai−1 + · · · + ω0 · a0
and
otype(Y ) = ωi · (a′
i −bi) + ωi−1 · a′
i−1 + · · · + ω0 · a′
0.
By induction hypothesis applied to (2) and closure of ∼n−1 under sum we obtain
X ∼n−1 Y. The assertion now follows from Theorem 6.6 of [3].
Let us deﬁne for each α < ε0 the norm of α, Nα, as follows by recursion on
α. N0 := 0 and Nα := n + Nα1 + · · · + Nαn if α has Cantor normal form
α = ωα1 + · · · + ωαn. Then Nα is the number of occurrences of ω in the Cantor
normal form of α. If we associate with α an unordered tree in the canonical
way (see the proof of Theorem 2) then Nα is the number of edges in the tree
representation of α and so N is in fact a canonical norm function on the ordinals
below ε0. For a given α < ε0 and n < ω there will only be ﬁnitely many ξ < α
with Nξ = n. For a ﬁnite set M we denote its cardinality by #M. The ordinal
ε0 is as usual the least ordinal ξ such that ξ = ωξ.
For α < ε0 we therefore may then deﬁne
cα(n) = #{ξ < α : Nξ = n}
and for an L-sentence ϕ we may further deﬁne
cϕ
α(n) = #{ξ < α : ξ |= ϕ ∧Nξ = n}.
We further deﬁne
δϕ(α) := lim
n→∞
cϕ
α(n)
cα(n)
if this limit exists. We say that α satisﬁes an L-limit law if δϕ(α) exists for all ϕ
and we say that α satisﬁes an L zero one law if δϕ(α) exists for all ϕ and if δϕ(α)
is either 0 or 1. In the sequel we write α |= ϕ as an abbreviation for (α, ∈) |= ϕ.
Theorem 1. Let ε0 > α ≥ω. Then ωα satisﬁes an L zero one law.
Proof. Let ϕ be a sentence of the language of linear orders. Assume that n is
the quantiﬁer rank of ϕ. Let I be the set {{0}, {1}, . . ., {2n −1}, {m : m ≥2n}}.
Let A0, . . . , An be a sequence of elements of I. Let
Pα(A0, . . . , An) := {ωn+1·β+ωn·an+· · ·+ω0·a0 < ωα : β > 0 ∧(∀i ≤n)[ai ∈Ai]}

Some Natural Zero One Laws for Ordinals Below ε0
727
and
Qα(A0, . . . , An) := {ωn · an + · · · + ω0 · a0 < ωα : (∀i ≤n)[ai ∈Ai]}.
Then the union of Pα’s and Qα’s taken over all (ﬁnitely many) sequences of
elements in I gives the set of all ordinals below ωα. By the Lemma 1 we have γ ∼n
δ for any pair of elements of each of the sets Pα(A0, . . . , An) and Qα(A0, . . . , An).
The ﬁnite collection of the Pα(A0, . . . , An) and Qα(A0, . . . , An) yields an eﬀective
(ﬁnite) description of the ﬁnitely many equivalence classes for ∼n. Then
{π < ωα : π |= ϕ} =

A0,...,An∈I:(∃iAi̸
={∅})∧∃ξ∈Pα(A0,...,An):ξ|=ϕ
Pα(A0, . . . , An) ∪

A0,...,An∈I:(∃iAi̸
={∅})∧∃ξ∈Qα(A0,...,An):ξ|=ϕ
Qα(A0, . . . , An)
and we only have to show that the asymptotic density of each Pα(A0, . . . , An) and
Qα(A0, . . . , An) is either 0 or 1. Of course the value 1 can only be distributed
once. Consider a set Pα(A0, . . . , An) where some Ai is a singleton containing
exactly ai. Let Pn+1 := {ωη : α > η ≥n + 1}. Let Pi := {ωi} for i ≤n. Then
Pα(A0, . . . , An) is a partition set in the sense of Burris [2] and can be written as
P(A0, . . . , An) = (≥0)Pn+1 + · · · + ai · Pi · · · .
This means that elements of P(A0, . . . , An) can be written as sums of arbitrary
many members of Pn+1 and ai members of each Pi for i ≤n. We have that the
local count function cωα(n) for ωα is in RT1 by [4] and according to Compton’s
Theorem 4.1 in [2] we obtain that the asymptotic density of Pα(A0, . . . , An) is
zero since ai is a small index (following the terminology of Deﬁnition 3.23 in
[2]. If all Ai are non singleton elements then Theorem 4.1 in [2] shows that the
asymptotic density of Pα(A0, . . . , An) is one. Finally since Q(A0, . . . , An) can be
written following Deﬁnition 3.25 of [2] as 0 ·Pn+1 + · · · Theorem 4.1 in [2] yields
that the asymptotic density of Q(A0, . . . , An) is zero since 0 is a small index.
Theorem 2. For any L sentence ϕ the limit δϕ(ε0) exists.
Proof. The proof starts as the proof of Theorem 1 and continues with Woods’
tree theorem.
Let ϕ be a sentence of the language of linear orders. Assume that n is the
quantiﬁer rank of ϕ. Let I be the set {{0}, {1}, . . ., {2n −1}, {m : m ≥2n}}.
Let A0, . . . , An be a sequence of elements of I. Let
P(A0, . . . , An) := {ωn+1·β+ωn·an+· · ·+ω0·a0 < ε0 : β > 0 ∧(∀i ≤n)[ai ∈Ai]}
and
Q(A0, . . . , An) := {ωn · an + · · · + ω0 · a0 < ε0 : (∀i ≤n)[ai ∈Ai]}.
Then the union of P’s and Q’s taken over all (ﬁnitely many) sequences of ele-
ments in I gives the set of all ordinals below ε0. By Lemma 1 we have γ ∼n δ for

728
A. Weiermann and A.R. Woods
any pair of elements of ﬁxed set P or a ﬁxed set Q. Thus we have an eﬀective
description of the ﬁnitely many equivalence classes for ∼n. Then
{π < ε0 : π |= ϕ} =

A0,...,An∈I:(∃iAi̸
={∅})∧∃ξ∈P (A0,...,An):ξ|=ϕ
Pα(A0, . . . , An) ∪

A0,...,An∈I:(∃iAi̸
={∅})∧∃ξ∈Q(A0,...,An):ξ|=ϕ
Q(A0, . . . , An)
and we only have to show we only have to show that the asymptotic density of each
P(A0, . . . , An) and Q(A0, . . . , An) exists. Now consider a ﬁxed P(A0, . . . , An) (or
Q(A0, . . . , An)). To each ordinal α less than ε0 we may associate canonically a
ﬁnite rooted non planar tree T (α) as follows. (Non planar refers in contrast to
planar to the property that there is no canonical ordering assumed for the im-
mediate subtrees of a given tree.) To 0 we associate the tree consisting of a root.
If α has a normal form ωα1 + · · · + ωαn then we may assume that we have as-
sociated trees T (αi) to the ordinals αi for 1 ≤i ≤n. Now let T (α) be the tree
consisting of a root and immediate subtrees T (α1), . . . , T (αn). Then |T (α)|, the
number of nodes of T (α), is 1 + N(α). By the ﬁnitary character of the descrip-
tion of P(A0, . . . , An) we may ﬁnd a sentence ψ in the language of trees such that
P(A0, . . . , An) = {α < ε0 : T (α) |= ψ}. This follows from Lemma 1 and the fact
that one can describe trees representing ordinals of the form ωk by a ﬁrst order
formula in the language of trees. Theorem 1.1 of the second author from [7] yields
that for any monadic second order property ψ the limiting distribution proba-
bility of the fraction of unlabelled rooted trees with n vertices and which satisfy
ψ exists. Therefore
lim
n→∞
#{T : T |= ψ ∧|T | = n + 1}
#{T : |T | = n + 1}
exists. But this limit is equal to
lim
n→∞
#{α < ε0 : T (α) |= ψ ∧Nα = n}
#{α < ε0 : Nα = n}
which is the asymptotic density of P(A0, . . . , An).
It is clear that we cannot expect a zero one law for ϵ0. Being a successor is a ﬁrst
order property which has in this case a limiting distribution probability strictly
inbetween 0 and 1 as shown in [4].
3
Reﬁnements
We now investigate limit laws for not necessarily additive principal ordinals
below ε0.

Some Natural Zero One Laws for Ordinals Below ε0
729
Lemma 2. Let α = ωβ · m + γ where m > 0 and γ < ωβ < ε0. Let ϕ be an
L-sentence. Assume that
δϕ(ωβ · m) = lim
n→∞
cϕ
ωβ·m(n)
cωβ·m(n)
exists. Then
lim
n→∞
cϕ
α(n)
cα(n) = δϕ(ωβ · m).
Proof. This Lemma is an easy corollary of the proof of Lemma 3.1 in [4].
cϕ
α(n)
cα(n) ≥cϕ
ωβ·m(n)
cα(n)
= cωβ·m(n)
cα(n)
· cϕ
ωβ·m(n)
cωβ·m(n) →n→∞1 · δϕ(ωβ · m)
by equation (2) in the proof of Lemma 3.1 of [4]. Moreover
cϕ
α(n)
cα(n) ≤cϕ
ωβ·m(n)
cωβ·m(n) + #{ξ : ωβ · m ≤ξ < α ∧Nξ = n}
cωβ·m(n)
→n→∞δϕ(ωβ · m) + 0
by equation (2) in the proof of Lemma 3.1 of of [4].
Lemma 3. Assume that α < ωω where α = ωk ·mk +· · ·+ω0 ·m0 with mk > 0.
Then for each ℓ∈{0, . . ., mk} there is a sentence ϕ ∈L such that
lim
n→∞
cϕ
α(n)
cα(n) =
ℓ
mk
.
Proof. Let ϕ describe that there exist exactly (mk −ℓ) k-limit points. (Recall
that 1-limit points are just limit points, i.e., ordinals of the form ω˙(1 + β) and
recall that k-limit points are limits of (k −1)-limit points. So k-limit points
have the form ωk˙(1 + β). Obviously being a k-limit point is ﬁrst order deﬁnable
property of an ordinal under consideration.)
We are going to prove that
lim
n→∞
cϕ
ωk·mk(n)
cωk·mk(n) =
ℓ
mk
.
This yields the claim by Lemma 2. We ﬁrst see that
{δ < ωk · mk : δ |= ϕ ∧Nδ = n} =
{δ < ωk · mk : δ ≥ωk · (mk −ℓ) ∧Nδ = n} =
{ξ < ωk · l : Nξ = n −N(ωk · (mk −ℓ))}
Thus cϕ
ωk·mk(n) = cωk·ℓ(n −N(ωk · (mk −ℓ))) ∼cωk(n) · ℓas shown in the last
line of the proof of Lemma 3.1 of [4] of [4]. Moreover cωk·mk(n) ∼m·cωk(n) and
the assertion follows.

730
A. Weiermann and A.R. Woods
Lemma 4. If γ ≥ω > m > 0 then
lim
n→∞
cϕ
ωγ·m(n)
cωγ·m(n) = lim
n→∞
cϕ
ωγ(n)
cωγ(n) ∈{0, 1}.
Proof. Fix ϕ. Assume that the quantiﬁer rank of ϕ does not exceed the natural
number r. For 2 ≤i ≤m let
M 1
i (n) := {δ < ωγ · i : δ |= ϕ ∧Nδ = n ∧δ ≥ωγ · (i −1) + ωr+1}
and
M 2
i (n) := {δ < ωγ · i : δ |= ϕ ∧Nδ = n ∧ωγ · (i −1) ≤δ < ωγ · (i −1) + ωr+1}
Then
cϕ
ωγ·m(n) ≥
#{δ < ωγ · m : δ |= ϕ ∧Nδ = n ∧δ ≥ωr}+
#M 1
2 (n) + #M 2
2 (n) + . . . + #M 1
m(n) + #M 2
m(n).
By Assertion (1) of Theorem 6.18 in [3] we know that ωr ∼2·r ωr ·β for all β > 0.
If δ = ωγ · i + ξ where ξ < ωγ and i > 0 then, since γ ≥ω, δ = ωr · ωγ · i + ξ
and we have the equivalence: δ |= ϕ ⇐⇒ωr + ξ |= ϕ. Thus δ →ωr + ξ gives
a projection into ωγ which preserves the validity (invalidity) of ϕ. Assume ﬁrst
that limn→∞
cϕ
ωγ (n)
cωγ (n) = 1. The remaining case limn→∞
cϕ
ωγ (n)
cωγ (n) = 0 can be treated
similarly. Now we have
#M 1
i (n) = #{ξ < ωγ : ξ |= ϕ ∧Nξ = n −(Nγ + 1) · i}
hence
#M 1
i (n)
m · cωγ(n)
converges to
1
m. We have
#M 2
i (n) = #{ξ < ωr+1 : ξ |= ϕ ∧Nξ = n −(Nγ + 1) · i + r + 1}
hence
#M 2
i (n)
m · cωγ(n)
converges to 0. We see that lim infn→∞
cϕ
ωγ ·m(n)
cωγ ·m(n) ≥1 and the assertion follows.
Corollary 1. If ε0 > γ ≥ωω then γ satisﬁes an L zero one law.
Theorem 3. Assume that α < ωω where α = ωk·mk+· · ·+ω0·m0 with mk > 0.
Then for each sentence ϕ ∈L there is an ℓ∈{0, . . . , mk} such that
lim
n→∞
cϕ
α(n)
cα(n) =
ℓ
mk
.
(3)

Some Natural Zero One Laws for Ordinals Below ε0
731
Proof. By Lemma 2 we may assume without loss of generality that α = ωk · m.
Let ϕ be given and assume that the rank of ϕ is r. Let Pi := {ωi} for i ≤k.
Then {δ < α : δ |= ϕ} can be written as a disjoint union of partition sets (in the
sense of Deﬁnition 3.25 of [2]) in the form 
A0,...,Ak AkPk + · · · + A0P0 where
Ai is an index in {0, 1, . . ., 2r −1, (≥2r)}. Now Ak has to be ﬁnite by the choice
of α. If one Ai is ﬁnite for some i < k then Ak−1Pk−1 + . . .+ A0P0 is a partition
set in ωk such that by Theorem 4.1 of [2] we have
lim
n→∞#{δ ∈Ak−1Pk−1 + . . . + A0P0 : Nδ = n}
cωk(n)
= 0.
Thus we may concentrate on AkPk + (≥2r)Pk−1 + · · · + (≥2r)P0. This set has
the same asymptotic density as AkPk and this set has asymptotic density in the
desired set according to the shape of Ak. Moreover the resulting ﬁnite sums of
these densities are also in the desired set of values.
One referee of this paper suggested to report about possible extensions of the
results of this paper. We give a short list but proofs will be reported elsewhere.
1. Let (pi)i≥1 be an enumeration of the prime numbers and for α < ε0 with
Cantor normal form α = ωβ + γ let Gα := pGβ · Gγ where G0 := 1. (This
G¨odel numbering shows up in Sch¨utte’s proof theory book and is also known
as Matula-coding.) Let
Δϕ(α) := lim
n→∞
#{β < α : β |= ϕ ∧Gβ ≤n}
#{β < α : Gβ ≤n}
.
Then Δϕ(α) ∈{0, 1} for ωω ≤α < ε0 and ϕ ∈L. This follows by adapting
the proofs of this paper to the new situation since we are working with
multiplicative numbers systems (in the sense of [2]) and we know from [4]
that we have for α < ε0 that
n →#{β < α : Gβ ≤n} ∈RV0.
2. Let (pi)i≥1 again be the enumeration of the prime numbers and for α < ε0
with Cantor normal form α = ωα1 + · · · + ωαn let G′α := pG′α1
1
· . . . · pG′αn
n
where G′0 := 1. (Such a G¨odel numbering shows up in G¨odel’s work).) Let
Δ′
ϕ(α) := lim
n→∞
#{β < α : β |= ϕ ∧G′β ≤n}
#{β < α : G′β ≤n}
.
Then we conjecture that Δ′
ϕ(α) ∈{0, 1} for ωω ≤α < ε0 and ϕ ∈L. We
expect that this follows by adapting the proofs of this paper to the new
situation. To carry this out it seems reasonable to investigate whether we
have for α < ε0 that
n →#{β < α : G′β ≤n} ∈RV0.
A remaining problem is that with respect to G′ we do not have a multiplica-
tive number system in the sense of Burris [2].

732
A. Weiermann and A.R. Woods
3. Let us now come back to the additive situation which is based on the norm
function N. If ϕ is in the monadic second order language of linear orders
then a limit law (but in general no zero one law) can be proved for all α
with ωω ≤α ≤ε0. A proof for this result has been obtained by applying as
new ingredient Shelah’s ‘additive colouring’ technique.
4. If ϕ is in the weak monadic second order language of linear orders with +
then a limit law can be proved for ε0 but no algorithm can separate formulas
having limiting probability 0 from formulas having limiting probability 1.
5. If ϕ is in the weak monadic second order language of linear orders with +
and · then a limit law can be proved for the thinned out domain of structures
A = {ωα : α < ε0}.
6. One referee of this paper asked whether limit laws are aﬀected by the choice
of notation. Using a formula by Lagrange 1775 (cf. Lemma 3.3 in [5]) it
can easily be shown that an L zero one law breaks down for many ordinals
between ωω and ε0 when they are represented by the lexicographic path
order over a signature with a binary function symbol and a constant (cf.,
e.g., the deﬁnition of < in [5] p.6). But the expectation is that for any
system of ordinal notations published in the literature at least limit laws
will hold with respect to L and canonically extended norm functions. For
more expressive languages one would expect Cesaro limit laws to hold in
many situations covered by ‘natural wellorderings’ (cf., e.g., [6]).
References
1. Bell, J.P., Burris, S.N.: Asymptotics for logical limit laws: when the growth of the
components is in an RT class. Trans. Amer. Math. Soc. 355(9), 3777–3794 (2003)
2. Burris, S.N.: Number Theoretic Density and Logical Limit Laws. Mathematical
Surveys and Monographs, vol. 86. American Mathematical Society (2001)
3. Rosenstein, J.: Linear Orderings. Academic Press (1982)
4. Weiermann, A.: A zero one law characterization of ϵ0. Mathematics and Computer
Science II. In: Chauvin, B., Flajolet, P., Gardy, D., Mokkadem, A. (eds.) Proceedings
of the Colloquium on Algorithms, Trees, Combinatorics and Probabilities, pp. 527–
539. Birkh¨auser (2002)
5. Weiermann, A.: Phase transition thresholds for some Friedman-style independence
results. Mathematical Logic Quarterly 53(1), 4–18 (2007)
6. Weiermann, A.: Analytic combinatorics, proof-theoretic ordinals, and phase transi-
tions for independence results. Annals of Pure and Applied Logic 136(1-2), 189–218
(2005)
7. Woods, A.R.: Coloring rules for ﬁnite trees, and probabilities of monadic second
order sentences. Random Structures Algorithms 10(4), 453–485 (1997)

On the Road to Thinking Machines:
Insights and Ideas
Jiˇr´ı Wiedermann
Institute of Computer Science, Academy of Sciences of the Czech Republic,
Pod Vod´arenskou vˇeˇz´ı 2, 182 07 Prague 8, Czech Republic
jiri.wiedermann@cs.cas.cz
Abstract. The quest for understanding the working of artiﬁcial minds
attaining a human-like cognition is culminating. While still inspired by
the functionality of biological brains, the realization of thinking machines
need not slavishly copy the principles used by their living pendants.
Achieving a higher-level artiﬁcial intelligence no longer seems to be a
matter of a fundamental scientiﬁc breakthrough but rather a matter of
exploiting our best algorithmic theories of thinking machines supported
by our most advanced robotic and real time data processing technologies.
We review recent examples of such theories, ideas and machines which
could pave the road towards building interesting artiﬁcial brains.
1
Introduction
Recent media coverage of the IBM Corp.’s computer named Watson defeating
the two most successful contestants in the history of the game show “Jeopardy!”
in a three day competition in February 2011 illustrates quite nicely the following
fact. Nowadays, people not only accept that machines can think, people even
seem to accept that machines can and will rival human intellectual abilities.
Watson the Computer has in fact showcased the current state of the artiﬁcial
general intelligence. In contrast to Watson the Computer whose ability to per-
form intelligent action is restricted to a (narrow) mental domain there exist less
media celebrated anthropomorphic bi-pedal robots (mostly of Japanese and Ko-
rean provenance) displaying impressive intelligent action in the domain of their
human-like body motion. Do thinking machines `a la Watson the Computer on
one hand, and walking robots on the other hand, represent two independent
“evolutionary lines”, or is there anything in common to both kind of machines?
Watson the Computer and various kinds of humanoid robots represent two
approaches to building human-like thinking machines. While robots rely on their
embodiment and build their (initially prevailing motor) intelligence so to speak
in a bottom-up manner, a very restricted expert intelligence of Watson the Com-
puter has been given to it in a top-down manner, as though via a Nuremberg
funnel. (According to German literary tradition, the Nuremberg funnel method
enables teaching everything to even the most stupid student without any ef-
fort on his or her side, as though by “pouring” the required knowledge into the
student’s head with the help of a funnel.)
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 733–744, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

734
J. Wiedermann
We shall argue that the ﬁrst, bottom-up approach starting at the embodiment
level, is an approach leading to a development at the end of which we may expect
a humanoid artifact with human-like intellectual abilities, language acquisition,
understanding, thinking, and certain forms of consciousness included. This de-
velopment can eventually be complemented by the Nuremberg funnel method.
In this way intelligence operating over the entire knowledge base currently avail-
able to mankind will be achieved. Such intelligence can continuously enlarge
itself by its own discoveries using similar principles as existing human-like intel-
ligence. Eventually, the underlying robot or robots could again be de-embodied
to give rise to a thinking, knowledgeable non-robotic artifact. This artifact will
possess the highest degree od artiﬁcial general intelligence upper-bounded by
known computability limits (cf. [32]). Such state can be viewed as the singular-
ity point—i.e., a state when machine intelligence will reach the level of human
intelligence, considered, e.g., in [16]. Note that this ﬁnal state will still be like
a state of knowledge or wisdom that in principle would also be attainable by
a single human. This is because the underlying operating principles will be the
same. The diﬀerence will be that this new artiﬁcial intelligence will exploit com-
putational, perceptory and knowledge resources by many order of magnitudes
exceeding those available to a single person.
In the main Section 2 we describe a series of the most important recent
achievements and trends in the theory of artiﬁcial thinking machines that will,
at least to our mind, shape the ﬁeld in the near future. These achievements will
be presented in an order enabling insights into the present ideas of how think-
ing machines should be constructed. The most important among these ideas is
a departure from biologism, automatically built internal world models, use of
mirror neurons forming the basis for imitation learning, development of habits,
understanding, thinking, realization of both phenomenal and functional con-
sciousness, global workspace theory, exploitation of episodic memories, and real
time massive data processing.
2
The Principles
During the past two decades an interesting and promising body of new knowledge
has accumulated in the theory. When properly screened, selected and ordered,
this body of knowledge has a potential to oﬀer more or less coherent ideas about
algorithmic principles on which computational cognitive systems could be based.
The respective knowledge has been scattered in the literature, the emerging
trends are often not formulated explicitly and important contributions seem to
penetrate only slowly into the general awareness of people working in the ﬁeld.
Next, we shall highlight the main reasons for believing that we already have
enough knowledge to glimpse the algorithmic principles behind the working of
interesting artiﬁcial minds attaining a high-level cognition. In what follows we
shall list and comment on the main ideas, trends and important theoretical
achievements we have in mind.

On the Road to Thinking Machines
735
2.1
Escaping the Turing Test
In theory, instead of considering thinking machines that can pass the Turing
test, often a more general notion of humanoid cognitive systems is considered.
Humanoid cognitive systems are cognitive systems endowed with human-like
intelligence, not necessarily with the intelligence that would be indistinguishable
by the Turing test [26]. Namely, the Turing test is explicitly, and unnecessarily,
anthropomorphic. If our ultimate goal is to create machines that could help
people in an intellectual domain, then it does not make sense to insist that the
behavior of such machines must closely resemble that of people.
When speaking about humanoid cognitive systems, nowadays we usually have
in mind humanoid robotic systems. This is a substantial deviation from Turing’s
original ideas [26] when he had in mind only “disembodied” computers, with no
sensors and eﬀectors, communicating with people only via a terminal.
In humanoid robotic systems, the adjective “humanoid” concerns both the
form and the contents of such robotic systems. Physically, such systems should
take the form of a human body, with as much sensors and actuators, mirroring
perception and motion of the human body, as possible. As we shall see later,
this is of utmost importance since practically all cognitive functions, higher-level
function included, are derived from the sensorimotor interaction of a robot with
its environment. Should the cognitive functions under development in a robot be
of human-like nature, then the sensorimotor interaction of that robot, inclusively
its environment, should be of similar nature as in the case of humans. We say
that a humanoid robotic cognitive system should be embodied in a human-like
body, and situated, via its sensors and eﬀectors, in a human-like environment
(cf. [21,22] for more details).
As far as the “content” of a humanoid robotic system, i.e., its control system
is concerned, it appears there is no need to mirror the architecture of the human
brain, only its functionality — see the next item.
2.2
Escaping Biologism
The next idea is escaping from anthropomorphism, or biologism in the design of
control part of cognitive systems. It is amazing how many pictures and schemes of
the human brain we see during a conference devoted to cognitive system design.
Compare that to a similar situation in a conference devoted to the aircraft design,
plagued by pictures of birds and their anatomy.
Nowadays, in addition to human brain, we see many examples of possible
architectures for cognition in the nature. For instance, the organization of the
nervous system of an octopus, which is known to be capable of performing ex-
traordinary intellectual feats, is totally diﬀerent from the organization support-
ing higher level cognitive activities in the vertebrate brain [17]. Therefore, when
thinking about artiﬁcial minds it only seems natural to concentrate on solutions
permitted, and enabled by our technologies while, of course, being inspired by
nature.

736
J. Wiedermann
2.3
Internal World Models
If a humanoid cognitive system has to communicate “intelligently” with people,
it should obviously have information how the people’s world looks like, what
could be the abilities of people under various circumstance, etc. In short, such a
system should possess a kind of internal model of the external world (inclusively
that of the self), represented in whatever useful way.
Nowadays, it is generally believed that in order to open the road towards
higher brain functions in humanoid cognitive systems we need automatic com-
putational mechanisms that will augment the semantic knowledge acquired in
the interaction of the system with its environment. These mechanisms often
make use of internal world models. Presently, prevailing trends seem to prefer
representations of the internal worlds in the form of neural nets (cf. the mirror
net and formation of concepts described in Parts 2.4. and 2.5) rather than in
the form of rule-based symbol manipulation systems. For an overview of the re-
cent state-of-the-art and a discussion on internal world models, cf. [13] or [6]. A
cognitive system architecture exploiting the idea of a world model can be found,
e.g., in [31] or [33].
2.4
Mirror Neurons
Mirror neurons were discovered during the 1990s (cf. [24]). Roughly speaking,
the mirror neurons are neurons that ﬁre if their owner performs a certain action
as well if their owner observes the same species performing the same action. This
can be interpreted as mirror neurons being a mechanism for “mind reading” of
other subjects. Far-reaching conjectures on the importance of mirror neurons for
understanding the intentions of other people, empathy, imitation learning and
even for language readiness (cf. [1,14,23]) have been developed since 1990s. In
[29] it was shown that mirror neurons can serve as a mechanism synthesizing
the multimodal (i.e., motor, perceptional and proprioceptive) information and
completing it, if necessary, by virtue of associativity so that an agent can remain
situated even when parts of the multimodal information are missing. Such a
mechanism forms a basis on which plausible explanation of the development of
a host of mental abilities has been founded. These abilities range from imitation
learning, communication via a sign language up to the dawn of thinking and con-
sciousness (cf. Part 2.7). The respective results have built a bridge between the
theory of embodied cognition and mirror neurons. These results have also justi-
ﬁed the above mentioned hopes laid on the discovery of mirror neurons, indeed.
The basic model from [29] has later been elaborated in a series of subsequent
papers (cf. [30,31,33]).
Note that the net of mirror neurons can be seen as a speciﬁc kind of an internal
static world model. In this model, the world is represented in the way as it is
cognized by an agent’s sensory and motor actions, i.e., by an agent’s interaction
with its environment. It can be termed as a sensorimotor model describing the
“syntax” of the world. In the mirror net, the combinations of the exteroceptory
and proprioceptory inputs are stored jointly with motor actions ﬁtting together,

On the Road to Thinking Machines
737
which “make sense” for the agent. Note that since the proprioceptory information
is always a part of multimodal information, also elements of an agent’s own model
are in fact available in the mirror net.
2.5
Algebra of Thoughts
Each occurrence of multimodal information in the net of mirror neurons repre-
sents an embodied concept. Moreover, new, so-called abstract concepts are formed
from the existing (mainly embodied) concepts in the control unit of a cognitive
agent by the following four principles: contiguity in space, contiguity in time,
similarity, and abstraction. (The ﬁrst three principles have already been identi-
ﬁed by the 18th century Scottish philosopher D. Hume [15].) The concepts are
connected via associations of various strengths. The concepts and the associa-
tions among them form the agent’s memory. At each time, some concepts in it
are in active state. These concepts represent the current “mental state” of the
agent. When new multimodal information enters the control unit it activates a
new set of concepts. Based on the current mental state and the set of newly
activated concepts, a new set of concepts is activated. This set represents the
new mental state of the agent and determines the next motor action of the unit.
In [28,29,30,31,33] it has been shown that, based on the above mentioned
principles, the control unit is capable of solving simple cognitive tasks: learning
simultaneous occurrence of concepts (by contiguity in space), their sequence, so-
called simple conditioning (by contiguity in time), similarity based behavior and
computing their abstractions. In fact, these are the unit’s basic operations.
The control unit represents a speciﬁc model of the world capturing the “se-
mantics” of the world. In this model, the relations among concepts are stored.
Obviously, these relations correspond to real relationships among real objects
and phenomena observed or generated by the agent during its existence. Similar
relations are also maintained among the representations of these objects and
phenomena. All this information represents a kind of a dynamic internal world
model. One can also see this model as a depository of the “patterns of behavior
which make sense in a given situation.”
2.6
Habits
If one wants to go further in the realization of the cognitive tasks, one should
consider special concepts called aﬀects. The aﬀects come in two forms: positive
and negative ones. The basic aﬀects are activated directly from the sensors.
The ones corresponding to the positive feelings are positive whereas the ones
corresponding to the negative feelings are negative. The excitatory (inhibitory)
associations arise among positive (negative) aﬀects and concepts. The role of the
aﬀects is to modulate the excitation mechanism. With the help of aﬀects, one can
simulate reinforcement learning (also called operant conditioning) and delayed
reinforcement learning. Pavlovian conditioning (cf. [27], p. 217), reinforcement
learning and delayed reinforcement learning seem to represent a set of minimal

738
J. Wiedermann
tests, which a cognitive system aspiring to produce a non-trivial behavior should
pass.
Aﬀects by themselves do not correspond to what O’Regan [20] calls “raw
feelings”, but they create an important ingredient of phenomenal consciousness
under fulﬁllment of other requirements described in Part 2.10.
In a stimulating environment during an agent’s interaction with its environ-
ment concepts within the control unit start to self–organize, via property of
similarity, into clusters whose centers are formed by abstract concepts. More-
over, by properties of time contiguity, chains of concepts, called habits, linked
by associations start to emerge. The habits correspond to often performed ac-
tivities. The behavior of agents governed by habits starts to prevail. In most
cases, such a behavior unfolds eﬀortlessly since habits are triggered indepen-
dently from the agent’s goals or intentions, by environmental cues (cf. [18]).
Only at the “crossings” of some habits an additional piece of multimodal infor-
mation from the mirror net is required directing the subsequent behavior. For
more details concerning the work and cognitive abilities of the control unit, see
the author’s earlier papers where the control unit under the name “cogitoid” has
been described.
2.7
Understanding of Understanding
The mechanism of imitation learning is a starting point for higher mental abil-
ities, cf. [1,14]. Imagine the following situation: agent A observes agent B per-
forming a certain well distinguishable task. If A has in its repository of behavioral
units (i.e., in its internal syntactic world model) multimodal information which
matches well the situation mediated by its sensors (which, by itself, is a dif-
ﬁcult robotic task), then A’s mirror net will identify the entire corresponding
multimodal information (by virtue of associativity). At the same time, it will
complement it by the ﬂag saying “this is not my own experience” (because the
proprioceptive part of information must have been ﬁlled in when completing the
entire multimodal information) and deliver it to the central unit where it will be
processed adequately. This can be seen as a germ of the self concept.
At that moment, A has at its disposal information on what B is about to
do, and hence, it can “forecast” the future actions of B. “Forecasting” is done
by following the associations in the control unit starting in the current mental
state. Agent A can even reconstruct the “feelings” of B (via aﬀects) since they
are parts of the retrieved multimodal information. This might be called empathy
in our model. Moreover, if we endow our agent with the ability to memorize short
recent sequences of its mental states, then A can repeat the observed actions of
B. This, of course, is called imitation.
The same mechanism helps to form a more detailed model of self. Namely,
observing the activities of a similar agent from a distance helps the observer to
“ﬁll in” the gaps in its own dynamic internal world model (i.e., in the control
unit), since from the beginning an observer only knows “what it feels like” if
it perceives its own part of the body while doing the actions at hand. At this
stage, we are close to primitive communication done with the help of gestures.

On the Road to Thinking Machines
739
Indicating some action via a characteristic gesture, an agent “broadcasts” visual
information that is completed by the observer’s associative memory mechanism
to the complete multimodal information. That is, with the help of a single gesture
complex information can be mediated. A gesture acts like an element of a higher-
level (proto)language. For articulating agents, it is possible to complement and
subsequently even substitute gestures by articulated sounds. It is the birth of
a spoken language. At about this time the process of stratiﬁcation of abstract
concepts from the embodied ones begins thanks to the abstraction potential of
the control unit (cf. Part 2.5). Namely, the agents “understand” their gestures
(language), deﬁned in terms of abstract concepts, via empathy in terms of their
embodiment or grounding, in the same sensorimotorics and proprioception (i.e.,
in the same embodied concepts) [10,11], and in a more involved case, in the
same patterns of behavior (habits). Without having a body an agent could not
understand its communication [22].
2.8
Thinking
Having communication ability, an agent is close to thinking: thinking means com-
munication with oneself, similarly as in cf. [7]. By communicating with oneself,
an agent triggers the mechanism of discriminating between the external stimuli
(I listen to what I am saying) and the internal ones. This mechanism may be
termed as self-awareness in our model. By a small modiﬁcation (from the view-
point of the agent’s designer), one can achieve that the still self-communication
can be arranged without the involvement of speaking organs at all. In this case,
the respective motor instructions issued by the control unit will not reach these
organs. However, these instructions still proceed to the internal world model.
Here they will invoke the same multimodal information as in the case when an
agent directly hears spoken language or perceives its gestures via proprioception.
Obviously, while thinking an agent “switches oﬀ” any interaction with the ex-
ternal world (i.e., both perception and motor actions). In such a case, the same
processes go on, but this time they are only based on the virtual information
stored in the internal world model. One can say that in the thinking mode an
agent works “oﬀ-line”, while in the “standard” mode it works “on-line”. More
details about the development of higher mental abilities can be found, e.g., in
[33].
2.9
Global Workspace Theory
Global workspace theory (GWT) is a simplistic, very high-level cognitive archi-
tecture that has been developed by Baars by the end of the last century [2,3]
to explain emergence of a conscious process from large sets of unconscious pro-
cesses in the human brain. Central to the theory is a model of information ﬂow
in which multiple, parallel, asynchronous specialist processes (corresponding to
unconscious processes) compete and co-operate in an arena for access to a global
workspace. A winning process then corresponds to a conscious process. This pro-
cess is promoted to the global workspace and is allowed to broadcast information

740
J. Wiedermann
back to the arena. Based on this, the specialist processes invoke another set of
unconscious processes and the whole cycle repeats itself. Note that the processes
in the global workspace appear in a serial manner, one after the other, while
each of them is the integrated product of parallel processing. The GWT can
successfully model a number of characteristics of consciousness, such as its role
in handling novel situations, its limited capacity, its sequential nature, and its
ability to trigger a vast range of unconscious brain processes. Unfortunately, the
GWT neither does explain the mechanism how an originally unconscious process
becomes a conscious one nor the mechanism of process competition.
The GWT has been incorporated into a number of computational models
(cf. [9]). The operation of a cogitoid, mentioned in Part 2.6, can be seen as a
speciﬁc implementation of the GWT. It is perhaps interesting to observe that
one “question/answer processing cycle” (cf. [8]) of Watson the Computer works,
in fact, according to the GWT.
2.10
(Dis)solving the Hard Problem of Consciousness
For the past decade or two, the modern theory of consciousness has been stig-
matized by the dichotomy between so-called functional (or access) conscious-
ness and the so-called phenomenal consciousness (or qualia). These two notions
were famously introduced by American philosopher of the mind, Ned Block [4]
and subsequently also adopted by other important protagonists (e.g., David
Chalmers [5]) in the ﬁeld. Functional consciousness consists of that information
globally available in the cognitive system for the purposes of reasoning, speech
and high-level action control. Phenomenal consciousness consists of subjective
phenomenal experience and feelings. Nowadays, we have relatively good ideas
how to implement functional consciousness (cf. [31]). On the other hand, phe-
nomenal consciousness seemed to present a nut hard to crack. The problem of
explaining how and why we have qualitative phenomenal experiences (of form
“what is it like”) presents so-called hard problem of consciousness [5]. Neverthe-
less, recently theories pointing to a common evolutionary [12] and sensorimotor
basis (cf. [20] and other works by this author) for both phenomenal and func-
tional consciousness have appeared. According to O’Regan [20], in order to have
a “raw feel” (qualia) it will suﬃce for a robot already possessing functional
consciousness to engage in an embodied (sensorimotor) interaction with its en-
vironment. More speciﬁcally, qualia can be seen as an engagement in exercising
a “ﬁxed sensorimotor skill” accompanied by (functional) conscious attendance
to that engagement and the skill’s quality. The respective real-world interac-
tion has to possess the properties of richness, bodiliness, insubordinateness and
grabbiness. Richness characterizes abundance in details. Bodiliness or corporality
requires that voluntary motions of a body systematically aﬀect sensory inputs.
Insubordinateness means that the world has its own dynamic that we can aﬀect
only partially (if at all) causing that bodiliness is never complete. And ﬁnally,
grabbiness means that the perceptual stimuli have the alerting capacity—they
can peremptorily interfere with cognitive processing (e.g., they can cause an
interrupt).

On the Road to Thinking Machines
741
2.11
Episodic Memory
Episodic memory is what people “remember”, i.e., the contextualized informa-
tion about autobiographical events (times, places, associated emotions), and
other contextual knowledge that can be explicitly stated. Multimodal informa-
tion (mentioned in Part 2.4) can serve as the simplest example of a “unitary”
episodic memory. It is obvious that such memory is important for an agent to
know about its past. Therefore, as noted in [19], it is surprising that the vast
majority of integrated intelligent systems have ignored episodic memory, which
often dooms them to what can be achieved by people with amnesia, which is
demonstrably limited. Nowadays we frequently witness “add-ons” to the exist-
ing models of cognitive systems architecture to account for episodic memory
(cf. [19]). In [19] it is argued that episodic memory systems can support a vast
number of cognitive capabilities which are mostly based on inspecting memories
from the past that are “similar” to the present situation. Among these capa-
bilities there is noticing novel situations, detecting repetitions, virtual sensing
(reminded by some recall), future action modeling, planing ahead (cf. [34]), en-
vironment modelling, predicting success/failure, managing long term goals, etc.
Incorporating episodic memories and their eﬃcient management and especially
their retrieval is a non-trivial matter and it is here where current massive data
processing technologies can ﬁnd their good use (cf. Part 2.12). In the simplest
case, in the case of simple agents (like animals), the retrieval from episodic mem-
ory can be based directly on the associative ability of the mirror net (Part 2.4).
Phenomenal consciousness seems to play the role of a tagging system for episodic
memories. More involved cases can be based on exploiting more complex tags
attached to stored episodes, in analogy with retrieval from picture databases.
Tags can be seen as short natural language expressions pertinent to the stored
episode. Tagging (and storing) of current multimodal information is performed
each time when the subject is conscious of that information. Thus, an ability to
describe what an agent perceives at a given moment, and a conscious awareness
of it, seems to be a prerequisite of eﬃcient episodic memory management and
their exploitation. (By the way, the role of language and consciousness in episodic
memory management seems to have escaped the attention of researchers.)
2.12
Real Time Massive Data Processing
In a sense, Watson the Computer can be seen as a crippled cognitive system
specialized in doing eﬃcient contextual retrieval invoked by clues over its prepro-
cessed episodic memory. Its success was possible thanks to technological progress
enabling maintenance of supercritical volumes of data and their searching and
retrieval by supercritical speed. Could this be the case that we are witnessing
the birth of a new paradigm? This paradigm states that intelligence is not only
a matter of suitable algorithms, but also, and mainly so, of the ability to accu-
mulate (e.g., via learning and episodic memories storing), organize, and exploit
large data volumes representing knowledge, at a speed matching the timescale of
the environmental requirements. In the case of a robot, its sensorimotor interac-
tions must also possess this quality, i.e., they must involve real time processing.

742
J. Wiedermann
Watson the Computer seems to be the ﬁrst case where the real time aspect
has boldly entered the game, enforcing a massively parallel solution which has
become the main factor in Watson’s victory.
2.13
De-embodiment of Robotic Humanoid Cognitive Systems
As mentioned earlier, the right starting point for constructing human-like think-
ing machines has been development of systems possessing a “complete” body.
Having a body has been a necessary condition for the evolution of higher mental
abilities. However, once having such a robot, for practical purposes (let us say for
conversation or expert advice) we might want to de-embody it, leaving it with its
basic communication abilities only. The result would then be something similar
to the brain in the vat. The resulting cognitive system will still be operational
capable of thinking and of further mental development thanks to its elaborated
internal world model and to model’s update mechanisms. Such systems will be
relatively easy to replicate and each one can later be specialized (via “Nurem-
berg funnel technique”) to become an expert (or just an intellectual companion)
in some speciﬁc domain according to the needs of its user.
3
Conclusion
Should a breakthrough occur in building thinking machines, then this break-
through will be a kind of engineering breakthrough involving integration of our
best theories of mind with the most advanced robotic and real time data process-
ing technologies. The man/year eﬀort spent in the case of Watson the Computer
shows clearly that this is going to be a formidable engineering task since only
a negligible part of the thinking machine research agenda has been veriﬁed so
far. It is unlikely that thinking machines will be developed by purely academic
research since it is beyond its power to concentrate the necessary amount of
manpower and technology. This cannot be accomplished by large international
research programs either since a dedicated long-term open-ended eﬀort of many
researchers concentrated on a single practically non-decomposable task is needed.
It seems to be a unique strategic opportunity for giant IT corporations. The road
towards thinking machines glimpses ahead of us and it only is a matter of money
whether we set oﬀfor a journey along this road.
Acknowledgements. This research was partially supported by RVO 67985807
and GA ˇCR grant No. P202/10/1333.
References
1. Arbib, M.A.: The mirror system hypothesis: how did protolanguage evolve? In:
Tallerman, M. (ed.) Language Origins: Perspectives on Evolution. Oxford Univer-
sity Press (2005)

On the Road to Thinking Machines
743
2. Baars, B.J.: A cognitive theory of consciousness. Cambridge University Press, Cam-
bridge (1988)
3. Baars, B.J.: In the theater of consciousness: The workspace of the mind. Oxford
University Press, Oxford (1997)
4. Block, N.: On a Confusion About a Function of Consciousness. Brain and Behav-
ioral Sciences 18, 227–247 (1995)
5. Chalmers, D.: Facing Up To the Problem of Consciousness. Journal of Conscious-
ness Studies 2(3), 200–219 (1995)
6. Cruse, H.: The evolution of cognition—a hypothesis. Cognitive Science 27(1) (2003)
7. Dennett, D.: Consciousness Explained. The Penguin Press (1991)
8. Ferrucci, D., et al.: Building Watson: An Overview DeepQA Project. AI Magazine,
200–214 (Fall 2010)
9. Franklin, S.: IDA: A conscious artifact? J. of Consciousness Studies 10(4-5), 47–66
(2003)
10. Feldman, J.: From Molecule to Metaphor. MIT Press, Cambridge (2006)
11. Harnad, S.: The symbol grounding problem. Physica D (42), 335–346 (1990)
12. Harvey, I.: Evolving Robot Consciousness: The Easy Problems and the Rest. In:
Fetzer, J.H. (ed.) Evolving Consciousness. Advances in Consciousness Research
Series, pp. 205–219. John Benjamins, Amsterdam (2002)
13. Holland, O., Goodman, R.: Robots with internal models: a route to machine con-
sciousness? Journal of Consciousness Studies 10(4-5) (2003)
14. Hurford, J.R.: Language beyond our grasp: what mirror neurons can, and cannot,
do for language evolution. In: Kimbrough, O., Griebel, U., Plunkett, K. (eds.) The
Evolution of Communication systems: A Comparative Approach. The Viennna
Series in Theoretical Biology. MIT Press, Cambridge (2002)
15. Hume, D.: Enquiry concerning human understanding. In: Selby-Bigge, L.A. (ed.)
Enquiries Concerning Human Understanding and Concerning the Principles of
Morals, 3rd edn. Clarendon Press, Oxford (2003), revised by Nidditch, P.H.
16. Kurzweil, R.: The Singularity is Near, p. 652. Viking Books (2005)
17. Llin`as, R.: I of the Vortex: From Neurons to Self. MIT Press (2001)
18. Neal, D.: The Pull of the Past: When Do Habits Persist Despite Conﬂict With
Motives? Pers. Soc. Psychol. Bull. 37, 1428–1437 (2011)
19. Nuxoll, A.M., Laird, J.E.: Extending Cognitive Architecture with Episodic Mem-
ory. In: Proceedings of the Twenty-Second Conference on Artiﬁcial Intelligence.
AAAI Press, Vancouver (2007)
20. Kevin O’Regan, J.: How to Build Consciousness into a Robot: The Sensorimotor
Approach. In: Lungarella, M., Iida, F., Bongard, J.C., Pfeifer, R. (eds.) 50 Years of
Aritﬁcial Intelligence. LNCS (LNAI), vol. 4850, pp. 332–346. Springer, Heidelberg
(2007)
21. Pfeifer, R., Scheier, C.: Understanding Intelligence. The MIT Press, Cambridge
(1999)
22. Pfeifer, R., Bongard, J.: How the body shapes the way we think: a new view of
intelligence. MIT Press (2006)
23. Ramachandran, V.S.: Mirror neurons and imitation as the driving force behind
‘the great leap forward’ in human evolution. EDGE: The third culture (2000),
http://www.edge.org/3rd_culture/ramachandran/ramachandran_p1.html
24. Rizzolatti, G., Fadiga, L., Gallese, V., Fogassi, I.: Premotor cortex and the recog-
nition of motor actions. Cognitive Brain Research 3, 131–141 (1996)
25. Shanahan, M.P.: Consciousness, emotion, and imagination: a brain-inspired ar-
chitecture for cognitive robotics. In: Proceedings AISB 2005 Symposium on Next
Generation Approaches to Machine Consciousness, pp. 26–35 (2005)

744
J. Wiedermann
26. Turing, A.: Computing Machinery and Intelligence. Mind 59(236), 433–460 (1950)
27. Valiant, L.G.: Circuits of the mind. Oxford University Press, New York (1994)
28. Wiedermann, J.: Towards Algorithmic Explanation of Mind Evolution and Func-
tioning (Extended Abstract). In: Brim, L., Gruska, J., Zlatuˇska, J. (eds.) MFCS
1998. LNCS, vol. 1450, pp. 152–166. Springer, Heidelberg (1998)
29. Wiedermann, J.: Mirror neurons, embodied cognitive agents and imitation learning.
Computing and Informatics 22(6), 545–559 (2003)
30. Wiedermann, J.: HUGO: A Cognitive Architecture with an Incorporated World
Model. In: Proc. of the European Conference on Complex Systems ECCS 2006,
Said Business School. Oxford University (2006)
31. Wiedermann, J.: A high level model of a conscious embodied agent. In: Proc.
of the 8th IEEE International Conference on Cognitive Informatics, pp. 448–456
(2010); expanded version appeared in: International Journal of Software Science
and Computational Intelligence (IJSSCI) 2(3), 62–78 (2010)
32. Wiedermann, J.: A Computability Argument Against Superintelligence. In: Cog-
nitive Computation, February 18. Springer (2012)
33. Wiedermann, J.: Towards Computational Models of Artiﬁcial Cognitive Systems
That Can, in Principle, Pass the Turing Test∗. In: Bielikov´a, M., Friedrich, G.,
Gottlob, G., Katzenbeisser, S., Tur´an, G. (eds.) SOFSEM 2012. LNCS, vol. 7147,
pp. 44–63. Springer, Heidelberg (2012)
34. Zimmer, C.: The Brain: Memories Are Crucial for Looking Into the Future.
DISCOVER Magazine (April 2011)

Making SolomonoﬀInduction Eﬀective
Or: You Can Learn What You Can Bound
J¨org Zimmermann and Armin B. Cremers
Institute of Computer Science, Rheinische Friedrich-Wilhelms-Universit¨at Bonn,
R¨omerstr. 164, 53117 Bonn, Germany
{jz,abc}@iai.uni-bonn.de
Abstract. The notion of eﬀective learnability is analyzed by relating it
to the proof-theoretic strength of an axiom system which is used to derive
totality proofs for recursive functions. The main result, the generator-
predictor theorem, states that an inﬁnite sequence of bits is learnable
if the axiom system proves the totality of a recursive function which
dominates the time function of the bit sequence generating process.
This result establishes a tight connection between learnability and
provability, thus reducing the question of what can be eﬀectively learned
to the foundational questions of mathematics with regard to set existence
axioms. Results of reverse mathematics are used to illustrate the impli-
cations of the generator-predictor theorem by connecting a hierarchy of
axiom systems with increasing logical strength to fast growing functions.
Our results are discussed in the context of the probabilistic universal
induction framework pioneered by Solomonoﬀ, showing how the inte-
gration of a proof system into the learning process leads to naturally
deﬁned eﬀective instances of Solomonoﬀinduction. Finally, we analyze
the problem of eﬀective learning in a framework where the time scales
of the generator and the predictor are coupled, leading to a surprising
conclusion.
1
Introduction
An eﬀective learning system is a system which can be fully speciﬁed by a program
on a universal Turing machine. In its most general form, this program transforms
a stream of percepts generated by an environment (later called the generator)
into a stream of actions possibly changing this environment. Via this senso-
motoric loop the system is embedded into its environment, about which a priori
nothing is known [7]. A more speciﬁc notion of learning is deﬁned as the process
which translates the stream of percepts into predictions for future percepts.
These predictions can then be used for choosing actions. The analysis of the
“design space” for eﬀective learning systems leads to three major questions:
1. How should a learning system represent and process uncertainty, or, what is
the proper inductive logic?
2. What set of possible models of the environment should the system consider?
3. How to relate the explanatory power of a model to its complexity?
S.B. Cooper, A. Dawar, and B. L¨owe (Eds.): CiE 2012, LNCS 7318, pp. 745–754, 2012.
c
⃝Springer-Verlag Berlin Heidelberg 2012

746
J. Zimmermann and A.B. Cremers
In the long run, the learning system should be able to detect as many regular-
ities in its percept stream as possible, while dealing sensibly with the inherent
uncertainty of predictions based on a ﬁnite amount of data. The ﬁrst question
regarding the representation and processing of uncertainty is in itself a current
area of research, where a plethora of diﬀerent approaches are discussed [5]. An
attempt to ﬁnd a unifying perspective on these approaches has been made in
[15]. However, this question is not the focus of this contribution, so we shall
discuss only a probabilistic learning framework, which is essentially a dynamic
extension of Bayesian inference. Here we focus on the second and third question.
The deﬁnition of proof-driven learning systems, combining search in proof and
program space, was inspired by an algorithm solving all well-deﬁned problems
as fast as possible (save for a factor of 5 and additive terms) introduced by M.
Hutter in [6]. We apply this idea in the context of learning in the limit and dis-
cuss in more detail the dependence of concepts like well-deﬁnedness of programs
or eﬀective learnability on the proof-theoretic strength of the employed back-
ground theory, thus establishing an explicit link between eﬀective learnability
and the foundational mathematical questions treated in proof theory and ordi-
nal analysis [10]. Additionally, our results emphasize the fact that provability
is not an absolute concept. The theoretical limits of eﬀective learnability and
computable approximations to Solomonoﬀinduction are intensively discussed in
various publications investigating diﬀerent aspects of this topic, see, for example,
[8] and the references therein, but to our knowledge none has made the connec-
tion between proof-theoretic strength, time complexity and eﬀective learnability
as explicit as it is stated in our main result, the generator-predictor theorem.
The ﬁrst learning in the limit framework, wich does not consider the uncertainty
of predictions, was introduced by E. M. Gold [3]. A discussion of our results in
the context of this framework can be found in [16].
2
The Generator Space
A clariﬁcation of the concept of learnability has to consider the following ques-
tion: What is the set of possible generators for observed events? To answer this
question, we shall explore the notion of “all possible generators” from a math-
ematical and computational point of view, and discuss the question of eﬀective
learnability in the context of such generic model spaces. The last sentence uses
intentionally the plural form of model space, because we shall see that the notion
of “all possible models” cannot be deﬁned in an absolute sense, but only with re-
gard to a reference proof system. This dependence will lead to the establishment
of a relationship between the time complexity of the percept-generating environ-
ment and the logical complexity of an eﬀective learning system, thus shedding
new light on the undecidability of a general approach to induction developed by
Solomonoﬀin the 1960s [12,13,9].

Making SolomonoﬀInduction Eﬀective
747
2.1
Algorithmic Ontology: Programs as Generators
Ontology is a part of philosophy which tries to deﬁne what exists, or, more
speciﬁcally, what possibly could exist. In the realm of mathematics, this question
leads to the set existence problem, which is (partially) answered by various set
theories, most commonly by using the axiom system ZFC, Zermelo-Fraenkel set
theory with the axiom of choice. But in the realm of computer science, existence
has to be eﬀective existence, i.e., the domain of interest and its operations must
have eﬀective representations.
For this reason the objects we consider are programs executed by a ﬁxed
universal Turing machine U having a one-way read-only input tape, some work
tapes, and a one-way write-only output tape (such Turing machines are called
monotone), which will be the reference machine for all what follows. The choice
of the speciﬁc universal Turing machine has only a constant factor as eﬀect on
the space complexity of a program and at most a logarithmic factor as eﬀect on
the time complexity [1]. These eﬀects are small enough to be neglected in the
following foundational considerations, but in the context of alternative models of
computation, like cellular automata on inﬁnite grids, the choice of the reference
machine may become an important issue. The program strings are chosen to be
preﬁx-free, i.e., no program string is the preﬁx of another program string. This is
advantageous from a coding point of view (enabling, for example, the application
of Kraft’s inequality 
p 2−length(p) ≤1), and does not restrict universality [9].
A program p (represented as ﬁnite binary string) is a generator of a possible
world, if it outputs an inﬁnite stream of bits when executed by U. Unfortu-
nately, it is not decidable whether a given program p has this well-deﬁnedness
property. This is the reason why the general approach to induction introduced
by Solomonoﬀis incomputable: the inference process uses the whole set of pro-
grams (program space) as possible generators, even the programs which are not
well-deﬁned in the above sense.
This results in the following dilemma: either one restricts the model space to
a decidable set of well-deﬁned programs, which leads to an eﬀective inference
process but ignores possibly meaningful programs, or one keeps all well-deﬁned
programs, but at the price of necessarily keeping ill-deﬁned programs as well,
risking the incomputability of the inference process. However, in the following
we propose an approach which tries to mitigate this dilemma by reducing the
question of learnability to the question of provability.
3
Learning Systems
Here we introduce the notion of a probabilistic learning system, which takes
a ﬁnite string of observed bits (the percept string) as input and produces a
probabilistic prediction for the next bit as output:
Deﬁnition 1. A probabilistic learning system is a function
Λ : {0, 1}∗× {0, 1} →[0, 1]Q,
with Λ(x, 0) + Λ(x, 1) = 1 for all x ∈{0, 1}∗.

748
J. Zimmermann and A.B. Cremers
Λ is an eﬀective probabilistic learning system if Λ is a total recursive function.
We use rational numbers as probability values, because real numbers cannot
be used directly in a context of computability questions, but have to be dealt
with by eﬀective approximations [14]. This would increase the complexity of our
deﬁnitions signiﬁcantly, and is not necessary for a ﬁrst understanding of the fun-
damental relationship between learnability and provability. Also we treat only
deterministic generators leading to one deﬁnite observable bit sequence. A gen-
eralization of our results to real valued probabilities and probabilistic generators
should be possible, but is open to future research.
Next we extend the prediction horizon of Λ by feeding it with its own predic-
tions. This leads to a learning system Λ(k) which makes probabilistic predictions
for the next k bits (xy is the concatenation of string x and y):
Λ(1) = Λ,
Λ(k+1)(x, y1) = Λ(k)(x, y) · Λ(xy, 1),
x ∈{0, 1}∗, y ∈{0, 1}k,
Λ(k+1)(x, y0) = Λ(k)(x, y) · Λ(xy, 0).
Finally, we deﬁne the learnability of an inﬁnite bit sequence s (si:j is the subse-
quence of s starting with bit i and ending with bit j):
Deﬁnition 2. An inﬁnite bit sequence s is learnable in the limit by the proba-
bilistic learning system Λ, if for all ϵ > 0 there is an n0 so that for all n ≥n0
and all k ≥1:
Λ(k)(s1:n, sn+1:n+k) > 1 −ϵ.
4
Σ-Driven Learning Systems
We now introduce the learning systems we shall use to investigate the notion of
eﬀective learnability. But ﬁrst we need the following deﬁnitions specifying the
nature of background theories available to these learning systems.
Deﬁnition 3. A logic frame is a 5-tuple Σ = (S, F, |=, ⊢, Φ), where elements of
S are called the structures of Σ, elements of F are called the sentences of Σ, the
relation |= ⊆S × F is called the satisfaction relation of Σ, ⊢: 2F →2F is the
deduction system of Σ, and Φ ⊆F is the core axiom system of Σ.
A deduction system is sound if for all Ψ ⊆F it holds: if Ψ ⊢ψ, then for all
M ∈S: if M |= Ψ, then M |= ψ. Next we deﬁne admissibility for logic frames:
Deﬁnition 4. Let Σ = (S, F, |=, ⊢, Φ) be a logic frame. Σ is admissible if ⊢is
sound, Φ is enumerable, ⊢eﬀectively maps enumerable sets of axioms to enu-
merable sets of logical consequences, and for all recursive functions f there is a
sentence φtot(f) ∈F with the following property: if Φ ⊢φtot(f), then f is a total
recursive function.
Note that this deﬁnition only ﬁxes the meaning of φtot(f) when it is derivable
from Φ. A pathological deﬁnition like φtot(f) = FALSE for all f is consistent
with the above deﬁnition of a logic frame, but would imply that such a logic
frame could not prove the totality of any recursive function f.

Making SolomonoﬀInduction Eﬀective
749
Let φ1, . . . , φn be the ﬁrst n sentences enumerated by the deduction system
of an admissible logic frame Σ, then the set Totn(Σ) is deﬁned as follows:
f ∈Totn(Σ)
iﬀ
φtot(f) ∈{φ1, . . . , φn}.
So Totn(Σ) contains the recursive functions which admit a totality proof within
the ﬁrst n sentences enumerated by the deduction system of Σ.
Now let gn(m) = max({f(m)|f ∈Totn(Σ)}) and g(n) = gn(n). g is called
the guard function w.r.t. Σ. The maximum over an empty set is deﬁned as 0.
Note that the maximum is taken over a ﬁnite set of total recursive functions, so
the guard function is a total recursive function, too. The guard function g will
play a central role as a scheduler ensuring the eﬀectiveness of learning systems.
Let Σ be an admissible logic frame, then the Σ-driven probabilistic learning
system Λ(Σ) is deﬁned as follows: Λ(Σ) uses a dynamic model space and dy-
namic prior probabilities and considers at inference step n only the part of the
program space given by the programs with length at most n. Because n grows
unboundedly, eventually every program will be part of the model space, but at
every instant of time the model space is ﬁnite. This assumption implies that
the posterior distributions and the probabilistic predictions can be computed
exactly and all involved probabilities are rational numbers. Λ(Σ) also manages
three labels for programs in addition to their current probabilities, in contrast to
classical Baysian inference: “candidate”, “suspended”, and “discarded”. A pro-
gram which is added to the model space initially gets the label “candidate”, all
other programs keep their label from the previous inference step. After the nth
bit has been observed, Λ(Σ) executes the following steps:
Input: the nth observed bit.
Output: a probabilistic prediction for the next bit.
1. Derive φn using the deduction system of Σ.
2. Compute g(n).
3. Initialize μ∗= 0. This variable accumulates preliminary posterior probability
mass and is needed to normalize the posterior distribution.
4. Start the enumeration of all programs p with length(p) ≤n.
5. If a program p is labeled “discarded”, it has already probability 0 and nothing
has to be done.
6. If a program p is labeled “suspended” or “candidate”, evaluate program p
till it outputs n bits or its step function reaches g(n).
7. If p has generated a bit and it is the observed one, then label p as “candi-
date”. Assign p a preliminary posterior probability of 2−(length(p)+switch(p,n)),
where switch(p, n) counts the number of switches p has experienced from
“suspended” status back to “candidate” status up to now. A higher number
of switches is translated retroactively into a lower prior probability. Add the
preliminary posterior probability to μ∗
8. If the generated bit is not the observed one, label p as “discarded” and set
its posterior probability to 0.

750
J. Zimmermann and A.B. Cremers
9. If p has reached the time limit speciﬁed by the guard function g, then label
p as “suspended” and set its posterior probability to 0.
10. Continue the enumeration of programs.
11. If the enumeration of all programs p with length(p) ≤n is completed and no
program has the label “candidate”, return ( 1
2, 1
2) as probabilistic prediction
for the next bit. Exit this inference step.
12. Rescale the preliminary posterior probabilities of all models labeled “can-
didate” by 1/μ∗, resulting in a normalized posterior distribution on the
currently considered model space. Use the guarded versions p ↾g of the
programs p labeled “candidate” for computing the probabilistic prediction
for the next bit. (p ↾g, p guarded by g, is the time limited version of p: if
the step function (the function which just counts the transitions made by
p) of p(m) exceeds g(m), then the computation of p is terminated and 0 is
returned, else p(m) is returned).
This ﬁnishes our deﬁnition of Σ-driven probabilistic learning systems. Next we
shall formulate and prove a theorem characterizing their learning capabilities.
5
Generator-Predictor Theorem
If the generator functions (see below) of all programs p generating a bit sequence
s grow so fast that they are not dominated by a provably total recursive function,
then it is indistinguishable, even in the limit, from a non-eﬀective bit sequence.
So the quest for making Solomonoﬀinduction eﬀective can be reduced to the
concept of provably total recursive functions. But which functions are provably
total recursive and which are not? The answer is: it depends. It depends on
the background theory or logic frame which is accepted for the construction of
totality proofs. At this point we have reduced the problem of universal induction
to a logical parameter, the logic frame Σ to which the learning system can
refer. The next step is to relate the proof strength of a logic frame Σ to the
time complexity of a program p which generates the bit stream observed by our
learning system. This will yield a natural characterization of the learnable bit
sequences relative to Σ.
Before we can state this relationship as a theorem, we need the notion of the
generator time function, generator function for short, of a program p:
Deﬁnition 5. The generator time function G(U)
p
: N →N ∪{∞} of a program
p w.r.t. the universal reference machine U assigns every n ∈N the number
of transitions needed to generate the ﬁrst n bits by the reference machine U
executing p. If n0 is the smallest number for which p does not generate a new
bit, then G(U)
p
(n) = ∞for all n ≥n0.
In the following we shall drop the superscript (U) because we are working only
with one reference machine.
In general there are several programs generating the same observable bit se-
quence. So one can not hope to learn exactly the program p which generates the

Making SolomonoﬀInduction Eﬀective
751
observed bit sequence, but only a program p′ which is observation equivalent.
The equivalence class of programs corresponding to an inﬁnite bit sequence s
we shall denote by [s]. Now we have introduced all the notions and concepts we
need in order to state our main result:
Generator-Predictor Theorem: Let Σ be an admissible logic frame and s an
inﬁnite bit sequence. s is learnable by the eﬀective probabilistic learning system
Λ(Σ), if Σ proves the totality of a recursive dominator of a generator function
Gp for at least one program p ∈[s].
Proof: Let LΣ(s) = {p|p ∈[s] and Σ proves the totality of a recursive domina-
tor of the generator function of p}. So LΣ(s) contains the programs which can
be eventually used by Λ(Σ) as perfect predictors. If LΣ(s) is empty, the theo-
rem makes no statement about the learnability of s, so lets assume that LΣ(s)
contains at least one element. We shall show that the sum of the posterior prob-
abilities of the programs in LΣ(s) will converge to 1 as the number of observed
bits increases. This will be conducted in two steps. Let α(p, n) be the preliminary
posterior probability (i.e., the posterior probability assigned to p before normal-
ization) of program p in the nth inference step (programs not contained in the
nth model space are considered as having a preliminary posterior probability of
0), α(n) be the sum of the preliminary posterior probabilities of all programs
in LΣ(s) and ¯α(n) be the sum of the preliminary posterior probabilities of all
programs not in LΣ(s). Note that α(n)+ ¯α(n) ̸= 1, because preliminary and not
ﬁnal posterior probabilities are considered. First we shall show the existence of
a number n1 so that α(n) ≥c for some constant c > 0 and for all n ≥n1. And
second, we shall see that ¯α(n) converges to 0 as the number of inference steps n
goes to inﬁnity. This implies that the normalized value of α(n) has to go to 1 as
n goes to inﬁnity:
αnorm(n) =
α(n)
α(n) + ¯α(n) =
1
1 + ¯α(n)
α(n)
−→
n→∞
1
So both steps together will establish the generator-predictor theorem.
If p ∈LΣ(s), then there is a number n0 so that the guard function g majorizes
Gp for all n ≥n0. Let n1 = max(n0, length(p)). Then p is part of the model space
for all n ≥n1 and it is not discarded and not suspended. Its preliminary posterior
probability is 2−(length(p)+switch(p,n1)). This number does not change anymore for
n ≥n1 (no new switches), thus c = 2−(length(p)+switch(p,n1)) > 0 is a lower bound
for α(n) for all n ≥n1. This completes the ﬁrst step.
Kraft’s inequality for preﬁx codes implies 
p̸∈LΣ(s) 2−length(p) ≤1. Thus for
all ϵ > 0 there is a k0 with 
p̸∈LΣ(s),length(p)>k 2−length(p) < ϵ for all k ≥k0,
because for a convergent sum the partial sums 
p̸∈LΣ(s),length(p)≤k 2−length(p)
converge to the limit. Now choose n2 so that 
p̸∈LΣ(s),length(p)>n 2−length(p) <
ϵ/2 for all n ≥n2, and n3 so that 
p̸∈LΣ(s),length(p)≤n2 α(p, n) < ϵ/2 for all
n ≥n3. n3 exists, because for a ﬁxed n2 there are only ﬁnitely many summands
α(p, n) contributing to the sum, each dropping to 0 (discarded or suspended

752
J. Zimmermann and A.B. Cremers
forever) or converging to 0 (number of switches increases unboundedly) as n
goes to inﬁnity. Then for all n ≥n3 we have:
¯α(n) =

p̸∈LΣ(s)
α(p, n) =

p̸∈LΣ(s)
length(p)≤n2
α(p, n)
+

p̸∈LΣ(s)
length(p)>n2
α(p, n)
≤

p̸∈LΣ(s)
length(p)≤n2
α(p, n)
+

p̸∈LΣ(s)
length(p)>n2
2−length(p)
<
ϵ/2 + ϵ/2
=
ϵ
This ﬁnishes the second step and thus the proof of the generator-predictor the-
orem.
⊓⊔
For example, if ΣPA = (FOL, PA) (First order logic Peano Arithmetic), then every
program with a generator function which is dominated by a provably total re-
cursive function w.r.t. PA can be learned by Λ(ΣPA). As the Ackermann-function
is provably total in PA, this is already a pretty large set, and PA allows totality
proofs of functions which grow much faster than the Ackermann-function.
The Generator-Predictor theorem states that for learning a bit sequence s it
is enough to prove the totality of a recursive dominator of the generator function
of a program p ∈[s], it is not necessary to prove the totality of p itself. However,
one can show that whenever there is a p ∈[s] so that the logic frame Σ can
prove the totality of a recursive dominator of Gp, then there is a q ∈[s] for
which Σ can prove totality, provided the logic frame satisﬁes some weak closure
conditions [16].
6
Learnability and Reverse Mathematics
Reverse mathematics is a program in mathematical logic that seeks to deter-
mine which set existence axioms are required to prove theorems of mathematics
in order to classify these theorems according to the strength of existence axioms
necessary to derive them. The program was founded in the 1970s by H. Friedman
[2]. Maybe the most remarkable fact of reverse mathematics is that many theo-
rems of classical mathematics fall into ﬁve large equivalence classes, consisting of
provably equivalent theorems. This leads to ﬁve standard axiom systems, which
are linearly ordered according to their proof-theoretic strength. S. G. Simpson
has named them the “Big Five” [11], starting with a system called RCA0 (Recur-
sive Comprehension Axiom), followed by WKL0 (Weak K¨onig’s Lemma), ACA0
(Arithmetical Comprehension Axiom), ATR0 (Arithmetical Transﬁnite Recur-
sion), and Π1
1-CA0 (Π1
1-Comprehension Axiom).
One can show that the functions which admit totality proofs by RCA0 are ex-
actly the primitive recursive functions. This implies that a bit stream sAckermann
which has a generating program with a generator function growing like the
Ackermann-function—a function known not to be primitive recursive—but no

Making SolomonoﬀInduction Eﬀective
753
generating program with a generator function dominated by a primitive recur-
sive function, can not be learned by Λ(RCA0). In terms of totality proofs WKL0
adds no additional proof-strength to RCA0, but the next system, ACA0, can
prove the totality of the Ackermann-function, so Λ(ACA0) can learn sAckermann
and is therefore a stronger learning system than Λ(RCA0). But there are total
recursive functions which have no totality proof in ACA0 (which is equivalent to
Peano Arithmetic in this regard), for example the Goodstein-function [4]. This
can be continued even beyond Π1
1-CA0, leading to the foundational questions
within mathematics concerning the introduction of ever stronger set existence
axioms. Here it suﬃces to note that these examples are a good illustration of
the fact that increased proof-theoretic strength translates into stronger learning
systems.
7
Synchronous Learning Frameworks
A closer look on real world incremental learning situations, where both, the
environment and the learning system, are not suspended while the other one is
performing its transitions resp. computations, leads to the following notion of
synchrony of a bit sequence s:
Deﬁnition 6. s is synchronous if lim sup
n→∞
Gp(n)
n
< ∞for at least one p ∈[s].
Synchrony entails that the time scales of the learning system and the environ-
ment are coupled, that they cannot ultimately drift apart. As long as one not
assumes a malicious environment, synchrony seems to be a natural property.
Such a setting for learning could be called a synchronous learning framework, in
contrast to the above considered learning frameworks, which could be classiﬁed
as asynchronous.
In order to learn in the case of synchrony, it suﬃces to prove that n2 is total,
because n2 is a dominator of every generator function satisfying the synchrony
condition. Thus, a much weaker background theory than, e.g., Peano Arithmetic
would suﬃce for an eﬀective learning system to learn all synchronously generated
bit sequences. In fact, because RCA0—the weakest of the ﬁve standard axiom
systems considered in reverse mathematics—proves the totality of all primitive
recursive functions, Λ(RCA0) is a perfect learning system in a synchronous world.
8
Conclusion
We argue that the generator-predictor theorem establishes a natural perspective
on the eﬀective core of Solomonoﬀinduction by shedding new light on the cause
of the incomputability of the non-relativized Solomonoﬀinduction, instead of di-
rectly introducing speciﬁc resource constraints in order to achieve computability,
like this is done, for example, in [7] for the AIξ learning system. This shifts the
questions related to learnability to questions related to provability, and therefore
into the realm of the foundations of mathematics.

754
J. Zimmermann and A.B. Cremers
The problem of universal induction in the synchronous learning framework,
however, is intrinsically eﬀective, and the focus of future research in a syn-
chronous framework can be on eﬃciency questions. In fact, the source of incom-
putability in the asynchronous learning framework can be traced back to the fact
that the learning system does not know how much time the generator process has
“invested” in order to produce the next bit. An extension of a bit sequence s by
inserting “clock signals” (coding a clock signal by “00” and output bits by “10”
and “11”) marking the passing of time would transform every sequence s into a
synchronous one, thus eliminating the incomputability of Solomonoﬀinduction.
So the synchronous learning framework seems to be perfectly suited for studying
the problem of universal induction from a computational point of view.
References
1. Arora, S., Barak, B.: Complexity Theory: A Modern Approach. Cambridge Uni-
versity Press (2009)
2. Friedman, H.: Some systems of second order arithmetic and their use. In: Proceed-
ings of the International Congress of Mathematicians, Vancouver, B.C, vol. 1, pp.
235–242. Canad. Math. Congress, Montreal (1974)
3. Gold, E.M.: Language identiﬁcation in the limit. Information and Control 10(5),
447–474 (1967)
4. Goodstein, R.: On the restricted ordinal theorem. Journal of Symbolic Logic 9,
33–41 (1944)
5. Huber, F., Schmidt-Petri, C. (eds.): Degrees of Belief. Springer (2009)
6. Hutter, M.: The fastest and shortest algorithm for all well-deﬁned problems. Inter-
national Journal of Foundations of Computer Science 13(3), 431–443 (2002)
7. Hutter, M.: Universal Artiﬁcial Intelligence: Sequential Decisions based on Algo-
rithmic Probability. Springer (2005)
8. Legg, S.: Is There an Elegant Universal Theory of Prediction? In: Balc´azar, J.L.,
Long, P.M., Stephan, F. (eds.) ALT 2006. LNCS (LNAI), vol. 4264, pp. 274–287.
Springer, Heidelberg (2006)
9. Li, M., Vit´anyi, P.M.B.: An introduction to Kolmogorov complexity and its appli-
cations, 3rd edn. Graduate Texts in Computer Science. Springer (2008)
10. Rathjen, M.: The art of ordinal analysis. In: Proceedings of the International
Congress of Mathematicians, pp. 45–69. Eur. Math. Soc. (2006)
11. Simpson, S.G.: Subsystems of Second Order Arithmetic, 2nd edn. Cambridge Uni-
versity Press (2009)
12. Solomonoﬀ, R.: A formal theory of inductive inference, part I. Information and
Control 7(1), 1–22 (1964)
13. Solomonoﬀ, R.: A formal theory of inductive inference, part II. Information and
Control 7(2), 224–254 (1964)
14. Weihrauch, K.: Computable analysis. Springer (2000)
15. Zimmermann, J., Cremers, A.B.: The Quest for Uncertainty. In: Calude, C.S.,
Rozenberg, G., Salomaa, A. (eds.) Maurer Festschrift. LNCS, vol. 6570, pp. 270–
283. Springer, Heidelberg (2011)
16. Zimmermann, J., Cremers, A.B.: Proof-driven learning systems (2012) (preprint)

Author Index
Afshari, Bahareh
1
Allender, Eric
11
Allo, Patrick
17
Aman, Bogdan
626
Antunes, Lu´ıs
29
Becher, Ver´onica
35
Beeson, Michael
46
Botman, Daniel
355
Brattka, Vasco
56
Bridges, Douglas S.
68
Brumleve, Dan
78
Carlucci, Lorenzo
89
Case, John
96
Celaya, Marcel
107
Chen, Yijia
118
Cholak, Peter A.
129
Ciobanu, Gabriel
626
Cord´on–Franco, Andr´es
440
Cremers, Armin B.
745
Das, Anupam
139
Dassow, J¨urgen
151
Downey, Rod
162
Dries, Roland
355
Dzhafarov, Damir D.
129
Edmonds, Bruce
182
Ehrig, Hartmut
193
Ermel, Claudia
193
Fang, Chengling
203
Fern´andez Duque, David
212
Ferreira, Fernando
222
Finlayson, Mark Alan
228
Flum, J¨org
118
Gershenson, Carlos
182
Golan, Amos
237
Grattan-Guinness, Ivor
245
Guillon, Pierre
253
Gurevich, Yuri
264
Hamkins, Joel David
78
Hartmanis, Juris
276
Hartung, Sepp
283
Havea, Robin S.
68
Hendtlass, Matthew
293
Higuchi, Kojiro
303, 581
Hirst, Jeﬀry L.
129
H¨uﬀner, Falk
193
Huschenbett, Martin
313
Iliev, Petar
323
Jackson, Paul B.
560
Jain, Sanjay
96
Jeandel, Emmanuel
334
Johannson, Anders
344
Joosten, Joost J.
212
Kaandorp, Jaap A.
355
Kartzow, Alexander
363
Kihara, Takayuki
303, 384
Kjos-Hanssen, Bjørn
395
Koepke, Peter
405
Kondo, Shigeru
416
Kooi, Barteld
323
Krishna, Shankara Narayanan
626
Kristiansen, Lars
422
Kulikov, Alexander S.
432
Lara–Mart´ın, F. F´elix
440
Lass`egue, Jean
450
Le Gloannec, Bastien
462
Le Roux, St´ephane
56
Liu, Jiamou
363
Lohrey, Markus
363
Longo, Giuseppe
450
Manea, Florin
151
Markopoulou, Fotini
472
Melanich, Olga
432
Merca¸s, Robert
151
Metcalfe, George
485
Mihajlin, Ivan
432
Millikan, Ruth Garrett
496
Moelius III, Samuel E.
507
M¨uller, Moritz
118
Murray, James D.
517

756
Author Index
N´emeti, P´eter
528
Nichterlein, Andr´e
283
Niedermeier, Rolf
193
Ocasio-Gonz´alez, V´ıctor A.
539
Okuno, Yasushi
549
Ollinger, Nicolas
462
Passmore, Grant Olney
560
Pauly, Arno
56, 571
Peng, NingNing
581
Peretyat’kin, Mikhail G.
589
Podolskii, Vladimir V.
599
Preston, John
609
Rathjen, Michael
1
Rettinger, Robert
616
R¨othlisberger, Christoph
485
Runge, Olga
193
Ruskey, Frank
107
Salo, Ville
636
Say, A.C. Cem
646
Schlicht, Philipp
78
Schuster, Peter
293
Seah, Samuel
96
Seki, Shinnosuke
549
Seyﬀerth, Benjamin
405
Shen, Alexander
655
Shenling, Wang
203
Sherratt, Jonathan A.
667
Smolensky, Paul
675
Souto, Andre
29
Staiger, Ludwig
686
Steiner, Rebecca M.
696
Stephan, Frank
96
Sterrett, Susan G.
703
Sz´ekely, Gergely
528
Szudzik, Matthew P.
714
Tamulonis, Carlos
355
Tanaka, Kazuyuki
374, 581
Taveneaux, Antoine
395
Teixeira, Andreia
29
Thapen, Neil
395
T¨orm¨a, Ilkka
636
van der Hoek, Wiebe
323
Weiermann, Andreas
723
Wiedermann, Jiˇr´ı
733
Woods, Alan R.
723
Wu, Guohua
203
Yakaryılmaz, Abuzer
646
Yamazaki, Takeshi
581
Yoshii, Keisuke
374
Zdanowski, Konrad
89
Zimmermann, J¨org
745
Zinoviadis, Charalampos
253
Zou, James
344


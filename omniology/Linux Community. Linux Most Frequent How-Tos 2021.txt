Linux 
Community
Collation of most 
sought after 
Linux Technical 
Articles
Linux
Most
Frequent 
How-Tos

Linux Tutorials
Contents
How to convert xlsx files to xls or csv format on Linux
How to back up a MySQL server database
How to share a directory with Samba on Fedora or CentOS
How to reduce line spacing in LaTex bibliography
How to configure Access Control Lists (ACLs) on Linux
How to install VMware Player on Ubuntu desktop
How to watch live streaming video from the command line on Linux
How to use yum to download a RPM package without installing it
How to create a video DVD on Linux desktop
What are things to do after installing Ubuntu 13.10
How to install Node.js on Linux How to start VMware server remote console 
without web interface
How to use on-screen virtual keyboard on Linux
How to upgrade the kernel on CentOS
How to scan Linux for rootkits with rkhunter
How to install dummynet on CentOS
How to install pip on Linux
How to improve the security of Magento e-commerce website
How to download webcomics from the command line on Linux
How to manage your personal expenses from the command line
How to install and configure tinc VPN on Linux
How to use local storage for CloudStack VMs
How to configure a network printer and scanner on Ubuntu desktop
How to install Xen hypervisor on unused old hardware
How to remove trailing whitespaces in a file on Linux How to compress PNG 
files on Linux
How to fix "lineno.sty not found" Latex error on Linux
How to connect your LAN to Amazon Virtual Private Cloud
How to install tcpping on Linux
How to convert image, audio and video formats on Ubuntu
What are some obscure but useful Vim commands
How to configure peer-to-peer VPN on Linux

How to log in to system VMs in CloudStack
How to make spreadsheets in a Linux terminal
How to check NetworkManager version on Ubuntu
How to configure rsyslog client for remote logging on CentOS
How to disable Apport internal error reporting on Ubuntu
How to convert List to Array in Java
How to change a message at the login console on Linux How to disable 
entering password for default keyring to unlock on Ubuntu desktop
How to print "optimized out" value in gdb
How to use Git behind proxy on Ubuntu
How to close an open DNS resolver
How to mount Google Drive on Linux
How to open a port in the firewall on CentOS or RHEL
How to convert a text file to PDF format on Linux
How to turn your CentOS box into an OSPF router using Quagga
What is good stock portfolio management software on Linux
How to enable and configure desktop sharing on Linux Mint Cinnamon 
desktop
How to access a NAT guest from host with VirtualBox
How to remove Amazon ads on Ubuntu
How to uninstall and remove Apache2 on Ubuntu or Debian How to allow
remote access to MySQL server
How to configure network interfaces in CentOS
How to install Unity Tweak Tool on Ubuntu desktop
How to parse JSON string via command line on Linux
How to find the default gateway on Linux
How to set up MailScanner, Clam Antivirus and SpamAssassin in CentOS 
mail server
How to set up Squid as a transparent web proxy on CentOS or RHEL
How to regenerate /etc/mtab file on Linux
How to set up networking between Docker containers
What are good web server benchmarking tools for Linux
How to configure PCI-passthrough on virt-manager
How to find the number of CPU cores on Linux
How to run program or process on specific CPU cores on Linux How to

check the changelog of a package on Linux
How to make a static web site for free via Google App Engine
How to install OpenSim on Ubuntu
How to install VMware Tools on Ubuntu or Debian VM
How to set a static MAC address on VMware ESXi virtual machine
How to build a RPM or DEB package from the source with CheckInstall
How to clone or copy a virtual machine on VirtualBox
How to install an old version of Firefox on Linux
How to set up a transparent HTTPS filtering proxy on CentOS
How to check what libraries are used by a program or process on Linux
How to turn off server signature on Apache web server
How to set up a secure SFTP server in Linux
How to monitor a Linux server and desktop remotely from web browser
How to create a GRE tunnel on Linux How to skip existing files when 
copying with scp
How to create an encrypted disk partition on Linux
How to convert PS/EPS file into JPG image on Linux
How to monitor Linux servers with SNMP and Cacti
How to install FFmpeg on Linux
How to convert MP3 ID3 tag encodings on Linux
How to find which jar file a Java class belongs to
How to enable PowerTools on CentOS 8
How to boot on an ISO image from GRUB
How to fix "404 Not Found" error with "apt-get update" on old Ubuntu
How to install Java runtime on Linux
How to fix "Your profile could not be opened correctly" on Google Chrome
How to configure fail2ban to protect Apache HTTP server
How to install and configure Conky on Linux How to install Ubuntu desktop 
behind a proxy
How to copy or transfer multiple files to Android devices
How to cut, split or edit MP3 file on Linux
How to convert TIFF files to PDF format on Linux
How to set up a media center with Raspberry Pi
How to install RubyGems on Linux
How to deduplicate files on Linux with dupeGuru
How to checkout a specific version of git repository
How to run an Octave script from command line

How to fix "network 'default' is not active" error in libvirt
How to block traffic by country on Linux
How to count lines of source code in Linux
What are the alternatives to Skype on Linux
How to check glibc version on Linux How to limit network bandwidth on 
Linux
How to create a site-to-site IPsec VPN tunnel using Openswan in Linux
How to fix "ImportError: No module named wxversion" on Linux
What is a good text editor on Linux
How to use matplotlib for scientific plotting on Linux
How to encrypt files and directories with eCryptFS on Linux
How to check firmware version on Linux
How to run SQL queries against Apache log files on Linux
How to add a static route permanently on Linux
How to set up Clam Antivirus, SpamAssassin and MailScanner on Ubuntu 
mail server
How to monitor system temperature on Linux
How to change camera resolution programmatically in Android
How to secure SSH login with one-time passwords on Linux How to monitor
DHCP traffic from the command line on Linux
How to install non-free packages on Debian
How to install Perl modules from CPAN
How to fix "fatal error: jsoncpp/json/json.h: No such file or directory"
How to set up WireGuard VPN server on Ubuntu 20.04
How to look up the geographic location of an IP address from the command 
line
How to run a command on multiple servers at once
How to enable VNC remote console in VMware Player
How to browse and search API documentation offline on Linux
How to set up a transparent proxy on Linux
What is good reference management software on Linux
How to fix "X11 forwarding request failed on channel 0"
How to disable Flash plugin on Google Chrome browser How to assign
multiple IP addresses to one network interface on CentOS
How to create .deb Debian package for Java web application
How to list all files contained in a Debian package
How to browse the web anonymously with Google Chrome

How to apply image effects to pictures on Raspberry Pi
How to build a kernel module with DKMS on Linux
What are the best plugins to increase productivity on Emacs
How to set up QoS bandwidth rate limit on Vyatta router
How to enable Nux Dextop repository on CentOS or RHEL
How to set up a Subversion (SVN) server on CentOS or Fedora
How to set up Samba as a Primary Domain Controller
How to reload config.boot in Vyatta
How to set up a secure Apache webserver on Ubuntu
How to add an additional vif to VM in XenServer What are useful online 
tools for Linux
How to filter, split or merge pcap files on Linux
How to get the process ID (PID) of a shell script
How to perform BGP traffic engineering using Quagga on Linux
How to install XenCenter on Linux
How to automatically run a script when logging into Ubuntu Desktop 
How to set JAVA_HOME environment variable automatically on Linux 
How to compress and minify CSS and JavaScript from the command line 
What are useful Bash aliases and functions
How to check SSH protocol version on Linux
What are useful netcat examples on Linux
How to remove overlay scrollbars in Ubuntu
What is a good subtitle editor on Linux
How to fix Android SDK Content Loader stuck at 0% in Eclipse How to 
install Apache Tomcat on Ubuntu or Debian
How to install dos2unix on Linux
How to record screen activities on Android
How to create a Linux LVM partition
How to download GOG games from the command line on Linux
How to compare two version numbers in a shell script
How to set up server monitoring system with Monit
Linux vs. Windows device driver model: architecture, APIs and build 
environment comparison
How to apply XenServer update patch
How to find and kill misbehaving MySQL queries
How to install httptunnel on Linux
How to set up Ubuntu Desktop VM on Amazon EC2

How to mount HDFS using FUSE How to backup files to a remote FTP 
server using lftp
How to force password change at the next login on Linux
How to install Wine on Linux
How to compile and install Nginx web server from source on Linux
How to make a file immutable on Linux
How to enable SSL for MySQL server and client
How to resume a large SCP file transfer on Linux
How to install Maven on CentOS
How to disable HTTP redirect in wget
How to suppress warning or error messages in find command
How to copy and paste between VMware Player VM and host desktop
How to set up a lightweight web server on Raspberry Pi How to use LVM in 
Linux
Linux Tutorials
How to convert xlsx files to xls or 
csv format on Linux
The xlsx file name extension is a new file format based on XML, first used 
by Microsoft Office Excel 2007 spreadsheet application. The letter x 
appended to the existing xls extension used by earlier versions of Excel, 
signifies that the format is based on XML. If you would like to open or edit 
an xlsx file created by Excel 2007 or later, on an earlier version of Excel or 
any other spreadsheet application, you need to convert xlsx files first.
There are four different ways you can convert xlsx files on Linux.
Method One: Gnumeric
The first method is to use Gnumeric. Gnumeric is a free spreadsheet program 
which can import or export data in different file formats such as csv, xls and 
xlsx. Gnumeric comes with a command-line utility called ssconvert which can

convert between different spreadsheet file formats. In order to convert xls files 
to xlsx or csv format by using Gnumeric, first install Gnumeric. Once 
Gnumeric is installed, you can use ssconvert.
term
Ilk_ $ sudo apt-get install gnumeric
_ l^lNote that ssconvert accepts arguments in the following format.
term
$ ssconvert --export-type=ID [input file] [output file]
In the above, --export-type is optional, and without --export-type, ssconvert can infer 
export type from the extension of output file. If you
want to explicitly specify export type, ID can be one of the following.
Gnumeric_Excel:excel_dsf : Microsoft Excel 97/2000/XP, 5.0/95 format
Gnumeric_Excel:xlsx : Microsoft Excel 2007
Gnumeric_pdf:pdf_assistant : Portable Document Format (PDF) 
Gnumeric_stf:stf_csv: Comma Separated Values (CSV)
So if you would like to convert xlsx files to csv format using ssconvert:
$ ssconvert input.xlsx output.csv
$ ssconvert --export-type=Gnumeric_stf:stf_csv input.xls output.txt
Method Two: xlsx2csv
The drawback of the Gnumeric-based method is that you need to install 
Gnumeric which may be too bloated software to install just for file 
conversion. A more lightweight way is to use xlsx2csv which is a python tool 
for xlsx to csv conversion.
term

$ git clone https://github.com/dilshod/xlsx2csv.git $ cd xlsx2csv 
$ ./xlsx2csv.py input.xlsx output.csv
Method Three: OpenOffice
Besides these methods, you can also use OpenOffice (if you already have it 
installed) to perform xlsx format conversions. OpenOffice comes with a 
command-line utility called unoconv which can convert xlsx files.
If you do not have it installed, you can install it with:
xterm
$ sudo apt-get install unoconv
$ unoconv -f csv input.xlsx
__^^HNote that once you install unoconv using apt-get, it will install
OpenOffice package as well.
Method Four: Google Docs
The final method of converting xlsx files is to use Google Spreadsheets. 
Google Spreadsheets can import Excel 2007 files. Thus, all you have to do is 
to import your xlsx file on to Google Docs Spreadsheet, click on Download as 
menu, and choose an appropriate file format: Excel (xls), PDF, CSV, Text. 
Note that document files which can be uploaded to Google Docs for 
conversion cannot be larger than 2mb.
Linux Tutorials
How to back up a MySQL server 
database

When you are running a MySQL server with critical information, you 
probably want to set up a protection mechanism such as replication or backup 
to prevent loss of data in the server due to unforseen events. While there are 
number of ways to back up MySQL databases, the simpliest solution is Linux 
command line. In particular, a Linux command-line tool called mysqldump 
allows you to back up MySQL databases without needing to shut down a 
MySQL server. mysqldump generates as output a text file containing a series of 
MySQL commands that represent a current snapshot of MySQL databases 
being backed up. The mysqldump output file can easily be compressed and/or 
encrypted as needed.
In this tutorial, I will describe how to back up a MySQL server with 
mysqldump.
The mysqldump program is contained in MySQL client package. So you will 
need to install MySQL client package first.
Install mysqldump on Linux
Install mysqldump on Ubuntu or Debian
term
T $ sudo apt-get install mysql-client
Install mysqldump on CentOS, Fedora or RHEL
term
T $ sudo yum install mysql
In order to generate an online snapshot for a live MySQL server, you need to 
prevent any update to its databases while backup is being processed. How to 
achieve that depends on what storage engine you are using for the MySQL 
tables inside. So first find out which storage engine (e.g., MyISAM, Innodb) 
you are using. This guide will tell you how.

Backup InnoDB Databases
If all your MySQL tables use InnoDB, you can use --single-transaction option 
with mysqidump to make an online backup:
xterm
$ mysqldump -h[server-ip-address] -u[username] p[password] --all-databases
--single-transaction > backup.sql
Note that in the above mysqldump command, there is no space between 
argument options (e.g., -h, -u, -p) and argument values themselves (e.g., 
server-ip-address, username, password).
Backup MyISAM or InnoDB/MyISAM Databases
The above command, however, does not work if you are using MyISAM 
tables or a mix of InnoDB/MyISAM tables, since MyISAM does not support 
transactions. In that case, you need to explicitly lock all the tables while the 
backup is being done as follows.
term
_ ^^Kterm
mysql> flush tables with read lock;
The MySQL flush statement in the above closes all open tables in the 
MySQL server, and obtains read locks to all the tables in all existing 
databases, thereby preventing any write to the databases. This ensures a 
consistent snapshot of a running system. Now proceed to perform the backup.
xterm
T__ $ mysqldump -h[server-ip-address] -u[username] p[password] --all­
databases > backup.sql

^^■Finally, release the global lock to the databases.
term
mysql> unlock tables;
Linux Tutorials
How to share a directory with 
Samba on Fedora or CentOS
Nowadays sharing data across different computers is not something new at 
home or many work places. Riding on this trend, modern operating systems 
make it easy to share and exchange data transparently across computers via 
network file systems. If your work environment involves a mix of Microsoft 
Windows and Linux computers, one way to share files and folders among 
them is via SMB/CIFS, a cross-platform network file sharing protocol. 
Windows Microsoft natively supports SMB/CIFS, while Linux offers free 
software implementation of SMB/CIFS network protocol in Samba.
In this article, we will demonstrate how to share a directory using Samba. 
The Linux platform we will use is Fedora or CentOS. This article is dividied 
into four parts. First, we will install Samba under Fedora/CentOS 
environment. Next, we discuss how to adjust SELinux and firewall 
configurations to allow file sharing with Samba. Finally, we cover how to 
enable Samba to share a directory.
Step One: Install Samba on Fedora or CentOS
First thing first. Let's install Samba and configure basic settings.
Check whether Samba application is already installed on your system by 
running:
xterm
Hb_ $ rpm -q samba samba-common samba-client

that Samba is not installed. In that case, install Samba using the command 
below.
xterm
lb_ $ sudo yum install samba samba-common samba-client
Next, creates a local directory which will share data over network. This 
directory will be exported to remote users as a Samba share. In this tutorial, 
we will create this directory in the top-level directory /shared, so make sure that 
you have the privileges to do it.
term
__ $ sudo mkdir /shared
If you want to create a shared directory inside your home directory (e.g., 
$home/shared), you must activate Samba home directory sharing in the 
SELinux options, which will be described below in more detail.
After creating /shared directory, set the privileges of the directory so other 
users can access it. 
xterm
$ sudo chmod o+rw /shared
__ ^Blf you don't want other users to be able to have write to the 
directory, just remove the w option in chmod command as follows.
xterm
lb_ $ sudo chmod o+r /shared
__ I^BNext, create one empty file as a test. This file will be used to verify 
that he Samba share is mounted properly.
I______^^Ixterm
T__ $ sudo touch /shared/filel
Step Two: Configure SELinux for Samba

Next, we need to re-configure SELinux which is enabled by default in Fedora 
and CentOS distributions. SELinux allows Samba to read and modify files or 
directories only when they have the right security context (e.g., labeled with 
the samba_share_t attribute).
The following command adds the necessary label to file-context 
configuration:
xterm
T $ sudo semanage fcontext -a -t samba_share_t " <directory>(/.
*)?"
 
__ ^^^Replace the <directory> with the local directory we created earlier for 
Samba share (e.g., /shared):
xterm
T__ $ sudo semanage fcontext -a -t samba_share_t
"/shared(/.
*)?"
_J^®To activate the label change, we then must run the restorecon 
command like below.
T__ $ sudo restorecon -R -v /shared
To share a directory inside our home directory via Samba, we must enable 
sharing home directory option in SELinux because it is disabled by default. 
The following command achieves the desired effect. Skip this step if you are 
not sharing your home directory.
term

$ sudo setsebool -P samba_enable_home_dirs 1
Step Three: Configure Firewall for Samba
The next step is to open necessary TCP/UDP ports in the firewall settings for 
Samba to operate.
If you are using firewalld (e.g., on Fedora or CentOS 7 or later), the following 
command will take care of permanent firewall rule change for Samba service.
xterm
Hl_ $ sudo firewall-cmd --permanent --add-service=samba
__ ^Bf you are using iptabies for your firewall (e.g., CentOS 6 or earlier), 
use the following commands to open up necessary Samba ports to the world.
xterm
lb_ $ sudo vi /etc/sysconfig/iptables
- A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 445
- j ACCEPT
- A RH-Firewall-1-INPUT -m state --state NEW -m udp -p udp --dport
445 -j ACCEPT
- A RH-Firewall-1-INPUT -m state --state NEW -m udp -p udp --dport
137 -j ACCEPT
- A RH-Firewall-1-INPUT -m state --state NEW -m udp -p udp --dport
138 -j ACCEPT
- A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 139
- j ACCEPT

xterm
Bl_ $ sudo service iptables restart
Step Four: Change Samba Configuration
The last step is to configure Samba to export a created local directory as a 
Samba-share.
Open the Samba configuration file with a text editor, and add the following 
lines at the bottom of the file.
xterm
T__ $ sudo nano /etc/samba/smb.conf 
[myshare]
comment=my shared files

path=/shared 
public=yes 
writeable=yes
In the above the text inside a pair of brackets (e.g., myshare) is the name of the 
Samba-shared resource, which will be used to access the Samba share from a 
remote host.
Create a Samba user account which is required to mount and export the 
Samba file system. To create a Samba user, use the smbpasswd tool. Note that 
the Samba user account must be the same as any existing Linux user. If you 
try to add a non-existing user with smbpasswd, it will give an error message.
If you don't want to use any existing Linux user as a Samba user, you can 
create a new dedicated user in your system. For safety, set the new user's 
login shell to /sbin/nologin, and do not create its home directory.
In this example, we are creating a new user named sambaguest as follows.
term
T__ $ sudo useradd -M -s /sbin/nologin sambaguest
$ sudo passwd sambaguest

After creating a new user, add the user as a Samba user using smbpasswd 
command. When this command asks a password, you can type a different 
password than the user's password.
xterm
$ sudo smbpasswd -a sambaguest
___^^H4. Activate the Samba service, and check whether the Samba 
service is running or not.
xterm
$ sudo systemctl enable smb.service
$ sudo systemctl start smb.service
$ sudo systemctl is-active smb

To see the list of shared directories in Samba, type the following command. 
xterm
lb_ $ smbclient -U sambaguest -L localhost
Terminal - kris@galungung:/home/kris 
f . □ X
File Edit View Terminal Tabs Help
[kris@galungung ~]$ smbclient -U sambaguest -L localhost
Enter sambaguest's password:
Domain=[MYGROUP] 0S=[Unix] Server=[Samba 4.1.17]
Domain=[MYGROUP] 0S=[Unix] Server=[Samba 4.1.17]
Sharename
Type
Comment
myshare
Disk
my shared files
IPC$
IPC
IPC Service [Samba Server Version 4.1.17)
sambaguest
Disk
Home Directories
Server 
Comment
Workgroup 
Master
[kris@galungung ~]$ |
The following is a screenshot of accessing the Samba-shared directory on

Thunar file manager, and doing copy-paste of file1. Note that the Samba 
share is accessible via smb://<samba-server-ip-address>/myshare address on Thunar.
Linux Tutorials
How to reduce line spacing in
LaTex bibliography
Question: I would like to save some space in a LaTex document by reducing 
the space between references in bibliography section. How can I reduce line 
spacing of bibliography in LaTex?

You can use the natbib package of LaTex. This package supports various 
citation formats (e.g., author-year and numerical citations). It also allows you 
to adjust the space between references.
To reduce line spacing in bibliography, simply put the following lines in the 
preamble of your LaTex document. This will shrink the line spacing in 
bibliography to 0pt.
usepackage{natbib} 
setlength{bibsep} {0.0pt}
Note that using the natbib package may change your citation format.
In that case, you can define the citation format explicitly using natbib options 
as follows. This will produce citations with square bracket, sorted, and 
comma-separated styles.
usepackage[square,sort,comma]{natbib}
setlength{bibsep}{0.0pt}
If you encounter "Package natbib Error: Bibliography not compatible with 
author-year citations" message, add numbers option to get around that.
usepackage[square,sort,comma,numbers]{natbib} 
setlength{bibsep}{0.0pt}

Linux Tutorials
How to configure Access Control 
Lists (ACLs) on Linux
Working with permissions on Linux is rather a simple task. You can define 
permissions for users, groups or others. This works really well when you 
work on a desktop PC or a virtual Linux instance which typically doesn't 
have a lot of users, or when users don't share files among themselves. 
However, what if you are a big organization where you operate NFS or 
Samba servers for diverse users. Then you will need to be nitpicky and set up 
more complex configurations and permissions to meet the requirements of 
your organization.
Linux (and other Unixes, that are POSIX compliant) has so-called Access 
Control Lists (ACLs), which are a way to assign permissions beyond the 
common paradigm. For example, by default you apply three permission 
groups: owner, group, and others. With ACLs, you can add permissions for 
other users or groups that are not simple "others" or any other group that the 
owner is not part of it. You can allow particular users A, B and C to have 
write permissions without letting their whole group to have writing 
permission.
ACLs are available for a variety of Linux filesystems including EXT2, 
EXT3, EXT4, XFS, Btfrs, etc. If you are not sure if the filesystem you are 
using supports ACLs, just read the documentation.
Install ACL Tools on Linux

First of all, we need to install the tools to manage ACLs.
On Ubuntu/Debian:
term
$ sudo apt-get install acl
On CentOS/Fedora/RHEL:
term
# yum -y install acl
On Arch Linux:
term
# pacman -S acl
Enable ACLs on your Filesystem
For demonstration purpose, I will use Ubuntu server, but other distributions 
should work the same way.
After installing ACL tools, it is necessary to enable ACL feature on our disk 
partitions so that we can start using it.
First, we can check if ACL feature is already enabled: As you noticed, my 
root partition has the ACL attribute enabled. In case yours doesn't, you need 
to edit your /etc/fstab file. Add acl flag in front of your options for the partition 
you want to enable ACL.
term
__ $ mount

f reeiiserflf reexploit :4 rant
/dev/mapper/freexploit--vg-root on I type ext4 (rw,acl,errors=remount-ro)
proc on /proc type proc (rw,noexec,nosuid,nodev)
sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)
none on /sys/fs/cgroup type tmpfs (rw)
none on /sys/fs/fuse/connections type fusectl (rw)
none on /sys/kernel/debug type debugfs (rw)
none on /sys/kernel/security type securityfs (rw)
udev on /dev type devtmpfs (rw,mode=0755)
devpts on /dev/pts type devpts (rw,noexec,nosuid,gid-5,ide-0620)
tmpfs on /run type tmpfs (rw,noexec,nosuid,size=ie%,mode=0755)
none on /run/lock type tmpfs (rw,noexec,nosuid,nodev,size=5242880)
none on /run/shm type tmpfs (rw,nosuid,nodev)
none on /run/user type tmpfs (rN,noexec,nosuid,nodev,size40«00,»de=0755)
none on /sys/fs/pstore type pstore (rw)
/dev/sdal on /boot type ext2 (rw)
systemd on /sys/fs/cgroup/systemd type cgroup (rw,noexec,nosuid,nodev,none,name=systemd) 
freeuser@freexploit!'$ |
default ]
1
freeuser|?,0,0,233 t 7h 28m 14S 0.8 0,6 0,5 o 2014-98-17 C 20:58 d



Now we need to re-mount the partition (I prefer to reboot completely, 
because I don't like losing data). If you enabled ACL for any other partitions, 
you have to remount them as well.
U _______^^■xtenii
$ sudo mount / -o remount
__ Awesome! Now that we have enable ACL in our system, let's start 
to work with it.
ACL Exaiples
Basically ACLs are managed by two commands: setfacl which is used to add 
or modify ACLs, and getfacl which shows assigned ACLs. Let's do some 
testing.
I created a directory /shared owned by a hypothetical user named freeuser.
xtenm
rti_$ ls -lh /

freeusergfreexploit:/? Is -lh
total 85K 
drwxr-xr-x 2 root
root
4.0K Aug 16 10:44 bin
drwxr-xr-x 4 root
root
l.flKAug 1610:27 boot
drwxr-xr-x 15 root
root
4.1K Aug 16 11:56 dev
drwxr-xr-x 90 root
root
4.0K Aug 16 10:45 etc
drwxr-xr-x 5 root
root
4.0K Aug 16 10:42 home
Irwxrwxrwx Iroot
root
33 Aug 1610:22 initrd.img •> boot/initrd.img-3.13.0-32-generic
drwxr-xr-x 21 root
root
4.0K Aug 16 10:25 lib
drwxr-xr-x 2 root
root
4.0K Aug 16 10:21 lib64
drwx. . . . . 2 root
root
16K Aug 16 10:21 lost+found
drwxr-xr-x 3 root
root
4.9K Aug 1610:21 media
drwxr-xr-x 2 root
root
4.0K Apr 1016:12 mt
drwxr-xr-x 2 root
root
4.0K Jul 22 16:48 opt
dr-xr-xr-x 84 root
root
fl Aug 16 10:36 proc
drwx. . . . . 2 root
root
4.9K Aug 1610:21 root
drwxr-xr-x 18 root
root
649 Aug 16 11:56 run
drwxr-xr-x 2 root
root
12K Aug 16 10:29 shin
drwxr-xr-x 3 freeuser freeuser 4.9K Aug 16 10:53 shared
drwxr-xr-x 2 root
root
4.9K JU1 22 16:48 srv
dr-xr-xr-x 13 root
root
fl Aug 16 10:36 sys
drwxrwxrwt 2 root
root
4.0K Aug 16 11:17 B
drwxr-xr-x 10 root
root
4.0K Aug 1610:21 usr
drwxr-xr-x 12 root
root
4.0K Aug 16 10:26 var
Irwxrwxrwx iroot 
freeuser@freexplolt:/$
root
1
30 Aug 1619:22 vnlinuz •> boot/vmlinuz-3.13.9-32-generic
freeuser@7.0.0.233 I 7h 43^ 24S 0.5 0,4 0,5 0 2014-08-17 0 21:14®
|W

I want to share this directory with two other users test and test2, one with full 
permissions and the other with just read permission.
First, to set ACLs for user test:
xterm
$ sudo setfacl -m u:test:rwx /shared
__ ^^■iNow user test can create directories, files, and access anything under
/shared directory.

freeuserOfreexploit:/$ Is -1 /shared/
total 4
drwxr-xr-x 2 test test 4096 Aug 1610:53 own_by_test
freeuser|freexploit:/$ |
default ]
freeuser@7.0,0,233 j 8h 4H129S 0.7 0.5 0,5 0 2014-08-17 i 21:35 liS!

Now we will add read-only permission for user test2:
xterm
T $ sudo setfacl -m u:test2:rx /shared
_J^HNote that execution permission is necessary so test2 can read 
directories.

freeuser|freexploit:/shared! sudo setfacl -m u:test2:rx /shared 
freeuser@freexploit:/shared$ sudo -ii test2 -H Is -1
total 4
drwxr-xr-x 2 test test 4096 Aug 1610:53 own_by_test
freeoser|f reexploit: /shared! |
default ]
1
freeuser@7.0.0.233 t 7h 46H116S 0.5 0,4 0,5 0 2014-08-17 5 21'16 0)1

Let me explain the syntax of setfacl command:
-m means modify ACL. You can add new, or modify existing ACLs. u: means 
user. You can use g to set group permissions.
test is the name of the user.
:rwx represents permissions you want to set.
Now let me show you how to read ACLs.
xterm
lb_ $ ls -lh /shared

freeuseif reexploit:/$ Is -1 shared/
total 4
drwxr-xr-x 1 test test 4096 Aug 1610:53 own_by_test
freeuser^freexploit:/$ Is -1 I
total 85
drwxr-xr-x 
2 root 
root 
4996 Aug 16 19:44 bin
drwxr-xr-x 
4 root 
root 
1924 Aug 16 10:27 boot
drwxr-xr-x 
15 root 
root 
4199 Aug 16 11:56 dev
drwxr-xr-x 
90 root 
root 
4096 Aug 16 10:45 etc
drwxr-xr-x 
5 root 
root 
4996 Aug 16 19:42 home
Irwxrwxrwx 
1 root 
root 
33 Aug 16 10:22 initrd.img ■> boot/initrd,ioig-3,13,0-32-generic
drwxr-xr-x 
21 root 
root 
4996 Aug 16 10:25 lib
drwxr-xr-x 
2 root 
root 
4096 Aug 1610:21 lib64
drwx. . . . .  2 root 
root 
16384 Aug 1619:21 lost+found
drwxr-xr-x 
3 root 
root 
4996 Aug 16 10:21 media
drwxr-xr-x 2root /root 
4996 Apr 1916:12unit
drwxr-xr-x 
2 root/ root 
4096 Jul 22 16:48 opt
dr-xr-xr-x 
84 ro;t 
root 
0 Aug 16 19:36 proc
drwx. . . . .  2/oot root 
4996 Aug 1619:21 root
drwxr-xr-x 
18 root 
root 
649 Aug 16 11:56 run
drwxr-xr-x 
2 root 
root 
12288 Aug 16 10:29 sbin
drwxrwxr-x+ 3 freeuser freeuser 4996 Aug 1619:53 shared
drwxr-xr-x 
2 root 
root 
4996 Jul 22 16:48 srv
dr-xr-xr-x 
13 root 
root 
0 Aug 16 10:36 sys
drwxrwxrwt 
2 root 
root 
4096 Aug 16 12:17 J
drwxr-xr-x 
19 root 
root 
4096 Aug 16 19:21 usr
drwxr-xr-x 12 root 
root 
4996 Aug 16 10:26 var
Irwxrwxrwx 
1 root 
root 
39 Aug 16 19:22 vmlinuz •> boot/vmlinuz-3,13,9-32-generic
freeuser^freexploit:/$ |
mil 1 
freeuser§7,0,0,233 t 71148m 6S 0,8 0,5 0.5 J 2014-98-17 6 ?1'1R
BhTSli

As you noticed, there is a + (plus) sign after normal permissions. It means that 
there are ACLs set up. To actually read ACLs, we need to run:
xterm
T__ $ sudo getfacl /shared

freeuser@freexploit:/$ getfacl /shared
getfacl: Removing leading 7' from absolute path names
I. file: shared
# owner: freeuser
# group: freeuser 
user::rwx
user:test:rwx
user:test2:r-x
group::r-x
mask:™
other::r-x
freeuser|freexploit:/$ |
default ]
1
freeuser@7.0.0.233 t 7h 49m 42S 0.8 0.6 0,5 ® 2014-88-17

Finally if you want to remove ACL:
xterm
Bf__ $ sudo setfacl -x u:test /shared

freeuser@freexploit :/$ sudo setfacl -x u:test /shared 
freeuser|freexploit :/$ getfacl /shared
getfacl: Removing leading 7' from absolute path names
# file: shared
# owner: freeuser
# group: freeuser 
user::rwx
user:test2:r-x
group::r-x 
mask::r-x
other::r-x
freeuser|freexploit:/$ |
default ]
1
freeuser®?.0.0.233 
t ?h 51m 17s 0.5 0.5 0.5 02014-08-17 5 ?1-?1

If you want to wipe out all ACL entries at once:
xterm
T_ $ sudo setfacl -b /shared

freeuser@freexploit :/$ sudo setfacl -b /shared
freeuserjif reexploit :/$ getfacl /shared
getfacl: Removing leading 7' from absolute path names
# file: shared
# owner: freeuser
# group: freeuser
user::rwx
group::r-x
other::r-x
freeuser|freexploit:/$ |
default ]
1
freeuser®/.0.0.233 t 7h 51m 58s 0.6 0.5 0.5 $2914-88-17 fl 21:22 OM

One last thing. The commands cp and mv can change their behavior when they 
work over files or directories with ACLs. In the case of cp, you need to add 
the -p parameter to copy ACLs. If this is not posible, it will show you a 
warning. mv will always move the ACLs, and also if it is not posible, it will 
show you a warning.
Conclusion
Using ACLs gives you a tremendous power and control over files you want 
to share, especially on NFS/Samba servers. Moreover, if you administer 
shared hosting, this tool is a must have.
Linux Tutorials
How to install VMware Player on 
Ubuntu desktop
VMware Player is a closed-source multi-platform virtualization software that 
is free for personal use. VMware Player has a convenient graphical user 
interface for virtual machine (VM) management, and with built-in 
NAT/bridged networking, there is no hassle in configuring VM networking, 
unlike other hypervisors such as https://www.google.com/kvm.html">KVM 
and Xen.
It is straightforward to install VMware Player on Ubuntu desktop, except 
that there are some caveats to take care of, which will be described in the 
following.
To install VMware Player from scratch, you can follow this one-minute step 
on most Ubuntu Desktop releases.
Download VMware Player for 32-bit or 64-bit Linux systems appropriately. 
As of this writing, the 32-bit and 64-bit VMware Player binaries are named 

vmware-player-5.0.0-812388.i386.bundle, and vmware-player5.0.0-812388.x86_64.bundle, 
respectively.
Once download is completed, it will be saved as
Once download is completed, it will be saved as 812388.x86_64.txt locally (for 64-bit 
system). Install the downloaded file simply as follows.
xterm
H__ $ sudo sh ./VMware-Player-5.0.0-812388.x86_64.txt
__ I^Bon most Ubuntu desktop releases, the above step is all it takes to 
install VMware Player, and you can start deploying VMs right away.
Patch for Ubuntu Desktop 11.10
However, I noticed that on Ubuntu desktop 11.10, creating a new VM on 
freshly installed VMware Player results in kernel panic instantly. The kernel 
panic is apparently due to incompatibility between vmmon kernel module used 
by VMware Player and Linux kernel 3.5 used by Ubuntu 12.10. If you 
experience the kernel panic, you can apply this VMware kernel module patch 
to avoid it. The patch recompiles vmmon kernel module, and so you need to 
have build environment ready (e.g., by running "sudo apt-get install build-essential") 
prior to running the patch. The patch was originally written by Artem S. 
Tashkinov, and modified by me (to handle the case when there are other 
VMware products are installed). The patch is known to work for VMware 
Player 5 as well as VMware Workstation 9.
To apply the patch:
xterm
$ tar xvfvz vmware9_kernel3.5_patch.tgz
$ cd vmware9_kernel3.5_patch
$ ./patch-modules_3.5.0.sh

Disable KVM
Another recommendation when using VMware Player on Ubuntu desktop is 
to disable KVM hypervisor if you are using it. Ubuntu uses KVM as the 
default virtualization technology. As such, if your host has 64-bit processors 
with hardware virtualization support (e.g., Intel VT or AMD-V), a fresh 
installation of Ubuntu desktop has KVM installed and enabled. VMware 
Player 4 would not install on a system with KVM enabled. While VMware 
Player 5 does not have such restriction, it is recommended you disable KVM 
when using VMware Player.
To check whether KVM is enabled or not on your Ubuntu desktop, do the 
following:
term
lb_ $ sudo lsmod|grep kvm
__ ^Hkvm intel 132759 0 
kvm 414070 1 kvm_intel
If you see the above kernel modules enabled, it means that KVM is enabled 
on your system.
In order to permanently disable KVM on Ubuntu, add kvm-intel to 
/etc/modprobe.d/blacklist.conf as follows.
term
lb_ $ sudo vi /etc/modprobe.d/blacklist.conf 
__B^^B# To use VMware Player 
blacklist kvm-intel

After rebooting, verify that KVM is diabled. Now you are ready to use 
VMware Player on Ubuntu desktop!
Linux Tutorials
How to watch live streaming video 
from the command line on Linux
There are quite a few popular live streaming services on the web (e.g., 
Dailymotion, YouTube Live, UStream, etc.). Most of these streaming 
services are accessed from a web browser via flash plugin. If you heavily use 
some of those services on Linux, you may notice that your web browser 
sometimes becomes unresponsive or otherwise consumes a lot of CPU 
resource and even leaks memory. That is because of the 
misbehaving/malfunctioning flash plugin.
If you hate the flash plugin as much as I do, you can consider another way to 
enjoy streamed contents from those services.
Enter Livestreamer.
Livestreamer is a command line interface (CLI) client which, upon given a 
streaming URL, retrieves live streaming video from the URL, and pipes it 
into a native video player running on local host. So with Livestreamer, you 
can enjoy live streaming from various sources via a much stable and 
lightweight video player such as VLC or mplayer, without opening a web 
browser.
Currently, Livestreamer supports streaming from over 60 different web sites 
including Dailymotion, YouTube Live, Twitch/Justin.tv, Livestream and 
UStream.
In this tutorial, I will describe how to access live streaming from the 
command line with Livestreamer.
Install Livestreamer on Linux

For Ubuntu, Debian or Linux Mint:
xterm
lb_ $ sudo apt-get install python-pip
$ sudo pip install livestreamer
For Fedora, CentOS or RHEL:
xterm
lb_ $ sudo yum install python-pip
$ sudo pip install livestreamer
___^^Hbv default, Livestreamer attempts to pipe streaming video into VLC 
player. Thus you also need to install VLC player on your Linux system.
Watch Live Streaming from the Command Line 
with Livestreamer
To watch any live streaming channel/program, first obtain its corresponding 
URL from its official site.
For example, let's say the URL for the live streaming content that you want to 
watch is http://www.youtube.com/watch?v=u4pw3offwgs
To check available streaming quality (i.e., bitrate) of the given content, 
simply run livestreamer command with the URL:
I___^Ixterm
$ livestreamer http://www.youtube.com/watch?
v=U4Pw3ofFW gs 
[cli][info] Found matching plugin youtube for URL
http://www.youtube.com/watch?v=U4Pw3ofFWgs
Available streams: 240p, 360p, 480p (best), 72p (worst)

To watch a live stream in the best available quality, run livestreamer command 
in the following format. This will automatically launch VLC player, and 
streaming will start on VLC.
xterm
lb_ $ livestreamer http://www.youtube.com/watch?
v=U4Pw3ofFWgs best
dev@dev:~$
dev@dev:~$ livestreamer http://www.youtube.com/watch?v=U4Pw3ofFWgs best
[cli][info] Found matching plugin youtube for URL http://www.youtube.com/watch7v
=U4Pw3ofFWgs
[cli][info] Opening stream: 480p
[cli][info] Starting player: /usr/bin/vlc

To watch a live stream in specific streaming quality (e.g., 360p), run livestreamer 
as follows.
xterm
Bi_ $ livestreamer http://www.youtube.com/watch?
v=U4Pw3ofFWgs 360p
___^^BiI' you have your own favorite streaming player, you can let 
Livestreamer launch the player instead of default VLC, as follows.
xterm
Hl_ $ livestreamer --player=mplayer 
http://www.youtube.com/watch?v=U4Pw3ofFWgs 360p
Livestreamer supports an external configuration file, where you can specify 
frequently used options for Livestreamer. The configuration file is useful 

especially when you are trying to access content in the subscription channels 
which require authentication. In such cases, you can specify whatever 
required access credentials (e.g., password, OAuth token, cookie, etc) in the 
configuration files.
Livestreamer configuration file can be created at either ~/.config/livestreamer/config 
or ~/.livestreamerrc.
The configuration file should contain one option per line in the format: 
option[=value].
For example: 
http-proxy=http://hostname:port/ 
https-proxy=https://hostname:port/ 
player=mplayer -cache 2048
Linux Tutorials
How to use yum to download a 
RPM package without installing it
Question: I want to download a RPM package from Red Hat's standard 
repositories. Can I use yum command to download a RPM package without 
installing it?
yum is the default package manager for Red Hat based systems, such as 
CentOS, Fedora or RHEL. Using yum, you can install or update a RPM 
package while resolving its package dependencies automatically. What if you 
want to download a RPM package without installing it on the system? For 
example, you may want to archive some RPM packages for later use or to

install them on another machine.
Here is how to download an RPM package from yum repositories.
Method One: yum
The yum command itself can be used to download a RPM package. The 
standard yum command offers --downloadonly option for this purpose.
xterm
T $ sudo yum install --downioadoniy <package-name>
__ ^^HBy default, a downloaded RPM package will be saved in:
/var/cache/yum/x86_64/[centos/fedora-version]/[repository]/packages
In the above, [repository] is the name of the repository (e.g., base, fedora, 
updates) from which the package is downloaded.
If you want to download a package to a specific directory (e.g., /tmp):
xterm
T__ $ sudo yum install --downloadonly --downloaddir=/tmp <package­
name>
__ I^^Note that if a package to download has any unmet dependencies, 
yum will download all dependent packages as well. None of them will be 
installed.
One important thing is that on CentOS/RHEL 6 or earlier, you will need to 
install a separate yum plugin (called yum-plugin-downloadonly) to be able to use -- 
downloadonly command option:
xterm
Hi_ $ sudo yum install yum-plugin-downloadonly
__ I^Bwithout this plugin, you will get the following error with yum:

Command line error: no such option: --downloadonly
[dev@localhost -]$ sudo yum install --downloadonly tcpdump
Loaded plugins: langpacks, retresh-packagekit
Resolving Dependencies
- -> Running transaction check
- --> Package tcpdump.x86 64 14:4.4.0-l.fcl9 will be updated
- --> Package tcpdump.x86_64 14:4.4.0-4.fcl9 will be an update
- -> Finished Dependency Resolution
Dependencies Resolved
Package Arch Version 
Repository Size
Updating:
tcpdump x86 64 
14:4.4.0-4.fcl9 updates 369 k
Transaction Summary
Upgrade 1 Package
Total size: 369 k
Background downloading packages, then exiting: 
exiting because "Download Only" specified 
[dev@localhost -]$
Method Two: yumdownloader
Another method to download an RPM package is via a dedicated package 
downloader tool called yumdownloader. This tool is part of yum-utils package 
which contains a suite of helper tools for yum package manager.

term
$ sudo yum install yum-utils
^^BTo download an RPM package:
term
$ sudo yumdownloader <package-name>
The downloaded package will be saved in the current directory. You need to 
use root privilege because yumdownloader will update package index files during 
downloading. Unlike yum command above, none of the dependent package(s) 
will be downloaded.
Linux Tutorials
How to create a video DVD on
Linux desktop
Question: I want to create a video DVD from MP4 movie files. Is there an 
open-source DVD authoring tool that I can use to create a video DVD in the 
Linux desktop environment?
DeVeDe is an open-source (GPLv3) DVD authoring software that allows you 
to create video DVD, VCD, SVCD or DivX from any number of video files. 
DeVeDe relies on other software such as Mplayer, FFMpeg, MEncoder, 
DVDAuthor, VCDImager and MKisofs for format conversion. As such, it 
supports a variety of popular input video/audio formats (e.g., .mp4, .avi, 
.mpg, .mkv). You can choose between PAL and NTSC video formats, and 
can add menus or subtitles.
DeVeDe can come in handy if you want to preserve or give away a copy of 
your vacation, family or entertainment videos.
Installing Devede on Linux

For Debian, Ubuntu and Their Derivatives:
xterm
lb_ # aptitude install devede
For RHEL or CentOS:
Enable EPEL and Nux Dextop repositories, and then run: 
xterm
lb_ # yum install devede
For Fedora:
Enable RPM Fusion repository, and then run: 
xterm
■l_ # yum install devede
Creating a Video DVD with DeVeDe
Launch DeVeDe and choose Video DVD from the menu.

VCD
sVCD
CVD
Disc type selection
Choose the disc type you want to create with DeVeDe
Super VideoCD
Creates a VideoCD with better picture quality
China VideoDisc
Another kind of Super VideoCD
Video DVD
Creates a video DVD suitable for all DVD heme players
VideoCD
Creates a VideoCD, with a picture quality equivalent to VHS
DivX/ MPEG-4
Creates files compliant with DivX home players
DVD
DivX ,
On the next screen, a title is automatically added to the collection by 
DeVeDe. You can edit its caption (1), add a file (2), and then add another 
title. Repeat this process as many times as needed.

File info
Original size (pixels):
Final size (pixels):
Length (seconds):
Frames per second:
Video rate (Kbits/sec):
Audio rate (Kbits/sec):
Size of chapters:
Output aspect ratio:
Estimated length (MBytes):
Disc usage
Media size:
4.7 GB DVD J
0%
Adjust disc usage
Default format
(0 PAL NTSC
Menus
|d Create a menu with the titles
Menu options Preview menu
- Advanced options
Action
Only convert film files to compliant MPEG files
Create disc structure
Create an ISO or BIN/CUE image, ready to burn to a disc
[J Quit
For
The " Title properties" box allows you to choose between six actions to be 
performed when the current title ends. Click on ok when you're done:

The " File properties" box is used to add a file to the current title. Click on the 
folder icon (upper right corner), and browse your computer for a desired file. 
You can also add a subtitle file by clicking on Add button right next to the 
subtitles box. Once you have selected a video file (and alternatively a subtitle 
file as well), click on ok at the bottom right corner.

You can add as many files as needed (only limited by the DVD capacity).
Next, edit the menu options. You can choose a background image and audio 
file to play while the main menu is displayed (which is as soon as the DVD is 
inserted into the player).

Finally, click on OK in the Menu options box to go back to the main disk
structure, where you will need to click Forward in order to begin creating an 
.iso image with a video DVD structure.

DeVeDe is done creating a video DVD, we will be presented with the 
following box:

Now we can burn
the .iso file (which contains a video DVD) to a disk, using growisofs or another 
DVD burning tool.
Linux Tutorials
What are things to do after 
installing Ubuntu 13.10
Ubuntu 13.10 (code-named "Saucy Salamander") was launched on 17 
October 2013, with all the latest goodies inside. Once you download and 
install it on your computer, what are the things to do next? In other words, 
what are the essential or useful apps to install?
In this article, I will share my experience with the latest Saucy Salamander 
(more specifically Kubuntu 64-bit) installed on my laptop. I am going to 
share information about useful Ubuntu apps I installed for my daily 
computing.
Proprietary Codecs
By default, the proprietary codecs for restricted formats and their apps (e.g., 
Flash Player) are not included in Ubuntu 13.10. To install the codecs and 
Flash Player, run the following commands.

$ sudo apt-get install ubuntu-restricted-extras $ sudo apt-get install 
libavformat-extra-53 libavcodecextra-53
Java Run-Time and Browser Plugin
You may also need Java RTE installed in your system to use Java browser 
plugin. Before the installation begins, you must know which Java package 
you need. Most user will choose to install OpenJDK, but it is also possible to 
use Oracle Java. For me, Oracle Java is my favorite for all things related to 
Java. Follow this command to install OpenJDK or Oracle Java.
For OpenJDK:
term
$ sudo apt-get install openjdk-7-jre icedtea-7-plugin 
_J^Hpor Oracle Java:
term
$ sudo add-apt-repository ppa:webupd8team/java $ sudo apt-get update 
$ sudo apt-get install oracle-java7-installer
For Orable Java, you will be asked to approve the end user license
during the installation process. So don't forget to click yes or ok button.
Browser (Google Chrome and Opera)
A web browser is one of essential tools in any Linux desktop. By default, 
Ubuntu comes with Mozilla Firefox pre-installed. If you are a Google 
Chrome fan as I am, use the following command to install Google Chrome 
browser.

For 64 bit:
xterm
T__ $ wget https://dl.google.com/linux/direct/googlechrome-
stable_current_amd64.deb
__ ^^Bpor 32 bit:
xterm
lb_ $ wget https://dl.google.com/linux/direct/googlechrome-
stable_current_i386.deb
L [After the download process is finished, execute this command. 
xterm
lb_ $ sudo dpkg -i google-chrome
*.deb
Some people report that Google Chrome won't install properly because some 
dependencies are not met. On my system, I only needed to install libcurl3. Here 
are other pointers related to Google Chrome problems on Ubuntu 13.10, in 
case you have an issue with Google Chrome installation.
http://askubuntu.com/questions/361090/installing-google-chrome-inubuntu- 
13-10-error-how-to
http://askubuntu.com/questions/359530/google-chrome-update-wontinstall- 
due-to-unmet-dependencies
If your favorite browser is Opera, download its deb package from its official 
source. Choose a right package according to your system architecture (32 bit 
or 64 bit). Then use dpkg command to install Opera web browser.
term
$ sudo dpkg -i opera
*.deb
Instant Messenger (Pidgin and Skype)
The best instant messenger for me is Pidgin. However, Kubuntu 13.10 (with 
KDE 4.11) comes with KDE Telepathy as a default instant messenger. 
Telepathy is integrated perfectly with KDE, where you are notified when

messages come, and you can manage all your instant messenger accounts 
through KDE System Settings.
Account 
Details
Shortcuts 
and Gestures
Personal 
Information
Application Application
Appearance and System 
Notifications
Applications
Search
Behavior
Behavior
Effects Appearance
Hardware

If you are a Pidgin fan like me, follow the command below to install Pidgin 
on your system. It will not interfere with Telepathy settings. However, note 
that Pidgin and Telepathy cannot run at the same time because instant 
messenger servers will block your connection as spam activity.
term
$ sudo apt-get install pidgin
Another choice for instant messenger is Skype, which is one of the most 
popular instant messengers with built-in video conference capability. To 
install Skype on your system, follow these instructions.

term
$ sudo sh -c "echo 'deb
http://archive.canonical.com/ubuntu/ saucy partner' >>
/etc/apt/sources.list.d/canonical_partner.list" $ sudo apt-get update 
$ sudo apt-get install skype
One thing to note is that Skype needs a 32-bit version of QtWebKit libraries 
when run on 64 bit systems. Even when you download the Skype package 
from its website, you still need 32 bit libraries. By adding or activating 
Canonical partner repository, all necessary 32 bit libraries will be installed 
automatically.
Video and Music Player
VLC is one of the most popular open-source video players. The repository of 
Ubuntu 13.10 ships VLC version 2.0.8, but if you want the newest version of 
VLC, follow this command to install it from its official repository.
4 ■__xterm
$ sudo add-apt-repository ppa:videolan/stable-daily $ sudo apt-get update 
$ sudo apt-get install vlc
you prefer using the stable version provided by Ubuntu's base 
repository, just execute this command, without adding VLC repository.
xterm
$ sudo apt-get install vlc
As for music players, I prefer Clementine to Amarok, while the latter is 
provided as a default music player in Kubuntu 13.10. For me, Clementine 
gives all the nice features I need for my music consumption. To install 
Clementine, execute this command.
term

Here is a snapshot of my Clementine after installation.
$ sudo apt-get install clementine
Cloud File Storage (Dropbox and Copy.com)
One important application I often use is cloud file storage such as Dropbox, 
Copy.com, or Google Drive. I usually use Dropbox for my cloud file storage. 
Follow this instruction to install Dropbox on your system.

First, download Dropbox deb package from their website. Choose the right 
type (32/64 bit) of Dropbox application according to your system. Then 
install the downloaded package.
xterm
Bl__ $ sudo dpkg -i dropbox
*.deb
After the installation process, you will get a notification saying that some 
program wants to run. Just click the notify sign, and the Dropbox installation 
will continue. Then your Dropbox user account and password will be needed. 
Also, configure the directory which will be used to keep the files in your 
system. If you already have a Dropbox account, you can select which 
directory or files to be synchronized. By default, Dropbox will be set to 
autostart, so if you log in into your desktop, it will be running automatically 
without your intervention.
Another good cloud file storage is Copy.com. The free version of its service 
gives you 15GB space at no charge. Copy.com also provides their 
synchronization tools for Linux. If you have a Copy.com user account, follow 
this instruction.
First, download Copy.com client from their website. The client is one single 
package for all types of Linux systems.
Extract the package into the directory that you want. I usually use bin folder 
under my home directory to put all executables which are not related to Linux 
system. You will have two directories in the Copy.com directory which 
correspond to two different system type: x86 and x86_64.
Enter either of those directories according to your system architecture, and 
search for an executable called copyagent. Run that executable, and it will 
guide you to connect your computer to Copy.com server and start 
synchronizing. You will be asked to provide your user account and password 
during the sync process.
Here I am sharing a screenshot of my Kubuntu 13.10 desktop. You can see 
Skype, Dropbox, CopyAgent and Clementine running smoothly.


Hope this article will help you set up your desktop with the latest Ubuntu 
13.10. If you have any other customization to recommend, feel free to chime 
in!
Linux Tutorials
How to install Node.js on Linux
Question: How can I install Node.js on [insert your Linux distro]?
Node.js is a server-side software platform built on Google's V8 JavaScript 
engine. Node.js has become a popular choice for building high-performance 
server-side applications all in JavaScript. What makes Node.js even more 
attractive for backend server development is the huge ecosystem of Node.js 
libraries and applications. Node.js comes with a command line utility called
npm which allows you to easily install, version-control, and manage 
dependencies of Node.js libraries and applications from the vast npm online 
repository.
In this tutorial, I will describe how to install Node.js on major Linux 
distros including Debian, Ubuntu, Fedora and CentOS.
Node.js is available as a pre-built package on some distros (e.g., Fedora or 
Ubuntu), while you need to install it from its source on other distros. As 
Node.js is fast evolving, it is recommended to install the latest Node.js from 
its source, instead of installing an outdated pre-built package. The lasted 
Node.js comes with npm (Node.js package manager) bundled, allowing you to 
install external Node.js modules easily.
Install Node.js on Debian
Starting from Debian 8 (Jessie), Node.js is available in the official 
repositories. Thus you can install it with:
xterm

$ sudo apt-get install npm
_ I^Mon Debian 7 (Wheezy) or earlier, you can install Node.js from its 
source as follows.
xterm
$ sudo apt-get install python g++ make
$ wget http://nodejs.org/dist/node-latest.tar.gz $ tar xvfvz node-latest.tar.gz
$ cd node-v0.10.21 (replace a version with your own) $ ./configure
$ make
$ sudo make install
Install Node.js on Ubuntu or Linux Mint
Node.js is included in Ubuntu (13.04 and higher). Thus installation is 
straightforward. The following will install Node.js and npm.
xterm
lb_ $ sudo apt-get install npm
$ sudo ln -s /usr/bin/nodejs /usr/bin/node
__ l^®While stock Ubuntu ships Node.js, you can install a more recent 
version from its PPA.
term
$ sudo apt-get install python-software-properties python g++ make
$ sudo add-apt-repository -y ppa:chris-lea/node.js $ sudo apt-get update 
$ sudo apt-get install npm
Install Node.js on Fedora

Node.js is included in the base repository of Fedora. Therefore you can use 
yum to install Node.js on Fedora.
xterm
Hb_ $ sudo yum install npm
__ ^Bf you want to install the latest version of Node.js, you can build it 
from its source as follows.
xterm
$ sudo yum groupinstall 'Development Tools'
$ wget http://nodejs.org/dist/node-latest.tar.gz $ tar xvfvz node-latest.tar.gz
$ cd node-v0.10.21 (replace a version with your own) $ ./configure
$ make
$ sudo make install
Install Node.js on CentOS or RHEL
To install Node.js with yum package manager on CentOS, first enable EPEL 
repository, and then run:
xterm
Hb_ $ sudo yum install npm
___^^BiI' you want to build the latest Node.js on CentOS, follow the same 
procedure as in Fedora.
Install Node.js on Arch Linux
Node.js is available in the Arch Linux community repository. Thus 
installation is as simple as running:
xterm
Hb_ $ sudo pacman -S nodejs npm
Check the Version of Node.js

Once you have installed Node.js, you can check Node.js version as follows.
xterm
T $ node --version
Linux Tutorials
How to start VMware server remote 
console without web interface
In VMware Server, console access for virtual machines (VMs) is obtained by 
using VM remote console plug-in of web browsers. However, as VMware 
Server is no longer supported by VMware, its remote console plug-in has not 
been usable in the latest web browsers. For example, VM remote console 
plug-in is broken for Firefox version 3.6 and higher. When you attempt to 
launch VM console window on the latest Firefox via console plugin, you will 
get an error saying:
Cannot access virtual machine console. The request timed out.
Given this situation, if you would like to have console access to your VM 
created by VMware Server, there are two ways to do it. The first option is to 
install an old version (e.g., 3.5.x) of Firefox to be able to use VM remote 
console plug-in on it. The other method is to run a remote console access 
program, standalone, without going through web browser. In the 
following, I'll describe the second method.
First, connect to a VMware Server host via SSH with X11 tunneling option. 
This is to launch VM console access program on the VMware Server host 
remotely. Note that the remote host must have X11 forwarding enabled first 
(i.e., "x11forwarding yes" in /etc/ssh/sshd_config).

term
Hb_ $ ssh -X [email protected]<vmware-server-IP-address>
Now, check the names of VMs located on the VMware Server host, using 
vmrun. You need to know the name of your VM in order to launch console 
access program for the VM. In this example, there are a total of three VMs on 
the host as follows.
H__ $ vmrun -T server -h https://127.0.0.1:8333/sdk -u [login_user] -p
[login_pass] list
Total running VMs: 3 
[standard] vm1/vm1.vmx
[standard] vm2/vm2.vmx
[standard] vm3/vm3.vmx
In the above, [login_user] and [login_pass] are login credentials for VMware 
Server. Now, extract remote console program from local VMware Server 
installation as follows.
$ mkdir vmx
$ cd vmx
$ unzip /usr/lib/vmware/webAccess/tomcat/apache-
tomcat6.0.16/webapps/ui/plugins/vmware-vmrc-linux-x64.xpi

__B^^Bln the above, I assume that VMware Server is installed on 64-bit 
machine. For 32-bit system, use vmware-vmrc-linux-x86.xpi instead.
xterm
rti_$ ls
__ ^^Bcomponents install.js install.rdf plugins
Once you unzip vmware-vmrc-linux-x64.xpi, you will see plugins directory, in which 
remote console program (i.e., vmware-vmrc) exists. Run the remote console 
program as follows.
term
$ cd plugins
$ ./vmware-vmrc -h 127.0.0.1:8333 -u [login_user] -p [login_pass] "
[standard] vm1/vm1.vmx"
)
Note that the last argument ("[standard] vm1/vm1.vmx
is the name of
VM that you would like to access, which you find out earlier.
Linux Tutorials
How to use on-screen virtual 
keyboard on Linux
On-screen virtual keyboard is an alternative input method that can replace a 
real hardware keyboard. Virtual keyboard may be a necessity in various 
cases. For example, your hardware keyboard is just broken; you do not have 
enough keyboards for extra machines; your hardware does not have an 
available port left to connect a keyboard; you are a disabled person with 
difficulty in typing on a real keyboard; or you are building a touchscreen­

based web kiosk.
On-screen keyboard can also be a protection mechanism against a hardware 
keylogger which silently records your keystrokes for sensitive information 
such as passwords. Some online banking sites actually force you to use a 
virtual keyboard for security-enhanced transactions.
In Linux environment, there are a couple of open-source virtual keyboard 
software available, e.g., GOK (GNOME Onscreen Keyboard), kvkbd, 
onboard, Florence.
In this tutorial, I am going to focus on Florence and show you how to set up 
a virtual keyboard with Florence. Florence comes with a number of nice 
features such as flexible layout, multiple input methods, auto-hide, etc. As 
part of the tutorial, I will also demonstrate how to use Ubuntu desktop with 
a mouse only.
Install Florence Virtual Keyboard on Linux
Fortunately, Florence is available on base repositories of most Linux distros.
Ubuntu, Debian or Linux Mint:
term
$ sudo apt-get install florence
Fedora, CentOS or RHEL:
On CentOS/RHEL-based system, EPEL repo is required. 
xterm
Hl_ $ sudo yum install florence
Mandriva or Mageia:

term
$ sudo urpmi florence
Arch Linux:
For Arch Linux users, the package is available in AUR.
Configure and Launch Virtual Keyboard
Once you install Florence, you can launch virtual keyboard simply by typing: 
xterm
Hl_ $ florence
__^^Hbv default, the virtual keyboard is always on top of other windows, 
allowing you to type on any active window easily.
To change default settings of Florence, click on tool key on the left side of 
the keyboard.

In style menu of Florence settings, you can customize keyboard style, and 
enable/disable sound effect.
In window menu, you can adjust keyboard background transparency and key 
opacity, as well as control keyboard ratio, taskbar, resizability and always- 
ontop features. Transparency and opacity adjustment can be useful if your 
screen resolution is not high enough, so the virtual keyboard is blocking other 
windows. In this example, I switch to transparent keyboard, and set opacity to 
50%.

In behaviour menu, you can change an input method. Florence supports several 
different input methods: mouse, touch screen, timer and ramble. The default 
input is mouse method. The last two methods do not require button clicks. 
With timer method, key press is triggered by locating a pointer at the key for 
a certain amount of time. The ramble method works similar to timer input, 
but with dexterity and training, can type much faster than timer method.

In layout menu, you can change the keyboard layout. For example, you can 
extend the keyboard layout to include navigation keys, numeric keys, and 
function keys.

Use Ubuntu Desktop with Mouse Only
I am going to demonstrate how to integrate Florence with Ubuntu desktop, so 
that we can access the desktop without a hardware keyboard. While this 
tutorial is specific to Ubuntu desktop with LightDM (Ubuntu's default display 
manager), a similar environment can be set up for other desktop 
environments.
The initial setup requires a hardware keyboard, but once the setup is 
completed, you only need a mouse, but not the keyboard.
When you boot up Ubuntu desktop, the boot procedure ends with launch of a 
display manager (or login manager) with Greeter interface, where you type in 
your login info. By default, Ubuntu desktop uses LightDM with Unity 
Greeter interface. Without a hardware keyboard, you cannot enter username 
and password at the login screen.
To be able to launch a virtual keyboard at the login screen, install GTK+ 
Greeter, which comes with on-screen keyboard support.
xterm
$ sudo apt-get install lightdm-gtk-greeter

Then, open a Greeter configuration file ( /etc/lightdm/lightdm-gtkgreeter.conf) with a 
text editor, and specify Florence as an on-screen keyboard to use. Instead of 
Florence, you could also use onboard, Ubuntu's default on-screen keyboard.
term
$ sudo vi /etc/lightdm/lightdm-gtk-greeter.conf 
[greeter] 
keyboard=florence --no-gnome --focus &
loreeter1
keyboard=florence --no-gnome -focus 6
background=/us r/share/backg rounds/wa rty- final-ubuntu.png
theme -name=Ambiance
icon-theme-name=Login!cons
font-name=Ub<mtu 11
xft-antialias=t rue
xft-dpi-96
xft-hintstyle=slight
xft-rgba=rgb
show-language-selector=true
Let's reboot Ubuntu desktop, and verify whether you can use virtual keyboard 
at the login screen.
When you see the GTK+ Greeter's login screen after boot, click on a human 
symbol icon on the top right corner. You will see On Screen Keyboard menu 
option as follows.

Click on this option, and a virtual keyboard will pop up on the login screen. 
Now you should be able to log in by tapping on the on-screen keyboard.

For those GDM2/GDM3 users, the Florence official site offers 
documentation on using virtual keyboard at GDM2/GDM3 screen.
The last step to make our Ubuntu desktop fully keyboard-less is to have 
virtual keyboard auto-start upon login, so that we can use our desktop without 
a hardware keyboard even after logging in. For that, create the following 
desktop file.
xterm
T__ $ mkdir -p ~/.config/autostart
$ vi ~/.config/autostart/florence.desktop 
[Desktop Entry]
Type=Application
Name=Virtual Keyboard
Comment=Auto-start virtual keyboard
Exec=florence --no-gnome

O 
1] 
Qi ® I
This will make virtual keyboard appear as soon as you log in to the desktop.
Hope this tutorial is useful to you. As you can see, Florence is quite powerful 
virtual keyboard which can be used for different purposes. Let me know if 
you have any use case for virtual keyboard.

Linux Tutorials
How to upgrade the kernel on
CentOS
You may want to upgrade the Linux kernel for various reasons. For example, 
you want to try a new kernel module or device driver which requires the 
latest kernel feature. Or, a new vulnerability has been uncovered in an 
existing Linux kernel. In other times, your Linux server may be unreliable 
with frequent system crashes. Whatever the reason is, upgrading the Linux 
kernel is an essential maintenance job for every Linux user.
In this tutorial, I will describe how to upgrade the Linux kernel on CentOS 
system. Here I will be upgrading the Linux kernel 2.6.32, which is the stock 
kernel in CentOS 6.4, to the Linux kernel 3.2.48 LTS.
First, verify the current kernel version:
xterm
Ilk_ $ uname -r
__ ^H2.6.32-358.el6.x86 64
Before you start, install all necessary prerequisite software for building a 
kernel:
$
term
sudo yum groupinstall "Development Tools"
$ sudo yum install ncurses-devel
L —IaIso, install any existing updates on your system:
I_____ ^^Ixterm
$ sudo yum update
__ ^^■iNow you are ready to upgrade the kernel from 2.6.32 to 3.2.48 LTS.
Download the new kernel source from kernel.org, and install it on your

system:
term
$ wget
https://www.kernel.org/pub/linux/kernel/v3.x/linux3.2.48.tar.xz
$ sudo tar xvfvJ linux-3.2.48.tar.xz -C /usr/src $ cd /usr/src/linux-3.2.48
_J^HBefore compiling a new kernel, you need to generate a kernel 
configuration.
If you want to generate a kernel configuration via graphical user interface, 
run:
term
lb_ $ sudo make menuconfig

Alternatively, if you want to use the same kernel configuration as the 
currently running kernel, run the following command instead. You still need 
to set any newly added configuration options by manually answering 
questions. If you do not know the meaning of each option, just press enter to 
accept a default answer.
term
$ sudo make oldconfig

Support for AMD/Fujttsu/Spansion flash chips (MTD_CFI_AMDSTD) [M/?] n
Support for ST (Advanced Architecture) flash chips (MTD_CFI_STAA) [M/n/?] n
Support for RAM chips in bus napping (MTD_RAM) [M/n/y/?] pi
Support for ROM chips in bus mapping (MTD_ROM) [M/n/y/?] pi
Support for absent chips in bus mapping (MTD_ABSENT) [M/n/y/?] pi 
a
* Mapping drivers for chip access
Support non-linear mappings of flash chips (MTD_COMPLEX_MAPPINGS) [Y/n/?] y 
Flash device in physical memory map (MTD PHYSMAP) [N/m/?] n
CFI Flash device mapped on AMD SC520 CDP (MTD_SC520CDP) [M/n/?] pi
CFI Flash device mapped on AMD NetSc520 (MTD_NETSC520) [M/n/?] pi
JEDEC Flash device mapped on Technologic Systems TS-5500 (MTD_TS5500) [M/n/y/?
] pi
CFI Flash device mapped on Arcon SBC-GXx boards (MTDSBCGXX) [N/m/?] n
BIOS flash chip on AMD76X southbridge (MTD_AMD76XROM) [N/m/?] n
BIOS flash chip on Intel Controller Hub 2/3/4/S (MTD.ICHXROM) [N/m/?] n
BIOS flash chip on Intel ESB Controller Hub 2 (MTD_ESB2R0M) [M/n/?] pi
BIOS flash chip on Nvidia CK804 (MTD_CK8D4XROM) [M/n/?] pi
BIOS flash chip on Intel SCB2 boards (MTD_SCB2_FLASH) [M/n/?] pi 
CFI flash device on SnapGear/SecureEdge (MTD_NETtel) [N/m/?] n 
BIOS flash chip on Intel L440GX boards (MTD_L440GX) [N/m/?] n 
PCI MTD driver (MTD_PCI) [M/n/y/?] pi
PCMCIA MTD driver (MTD_PCMCIA) [N/m/y/?] (NEW)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
If you want to use the same old kernel configuration, and automatically 
accept default answers to every question, run the following command instead:
xterm
lb_ $ sudo sh -c 'yes "" | make oldconfig'
__ ^^^After kernel configuration is done, go ahead and compile a new 
kernel:
xterm
lb_ $ sudo make
The above step will take 30 minutes or more, depending on your CPU speed. 
After compilation is completed, go ahead and install the new kernel (and all 
kernel modules) as follows.

xterm
T $ sudo make modules_install install
___I^Bto use the newly built kernel in your system, you need to choose
the new kernel on grub menu upon boot-up.__________________________
GNU GRUB uersion 0.97 (636K lower / 1046400H upper memory)
| CentOS (3.2.48)
CentOS (2.6.32-358.14.1.e16.x86_64)
CentOS (2.6.32-358.el6.x86_64)
Use the t and 4 keys to select which entry is highlighted. 
Press enter to boot the selected OS, ’e’ to edit the 
commands before booting, ’a’ to modify the kernel arguments 
before booting, or ’ c’ for a command-line.
Alternatively, edit /boot/grub/grub.conf to specify the new kernel as the default 
kernel to load. In /boot/grub/grub.conf, change the "default" number to whichever 
position your new kernel information is in the kernel list (counting starts at 0).
term
__ $ sudo vi /boot/grub/grub.conf

default=1
Linux Tutorials
How to scan Linux for rootkits with 
rkhunter
A rootkit is a malicious software designed to obtain root-level access to a 
computer while hiding its presence or identity from antivirus software. 
Common ways for rootkits to get installed on your system are through trojan 
horses contained in drive-by downloads, known system vulnerabilities, 
suspicious email attachments, web surfing, or simply by password cracking.
On Linux, there are several rootkit scanner tools that help project against 
known or potential rootkits. One such rootkit detection tool is called Rootkit 
Hunter (rkhunter). Here I will describe how to scan a Linux system for 
rootkits with rkhunter.
Install rkhunter on Linux
To install rkhunter or Ubuntu, Debian or Linux Mint: 
xterm
T $ sudo apt-get install rkhunter
I Ito install rkhunter on Fedora: 
xterm
HI_ $ sudo yum install rkhunter
__ ^^B'lo install rkhunter on CentOS or RHEL, first set up Repoforge 
repository on your system, and then use yum command.
term

$ sudo yum install rkhunter
Scan a Linux System for Rootkits
To perform rootkit scanning on your system, simply run the following. 
xterm
Ilk_ $ sudo rkhunter -c
__ I^BOnce rkhunter is initiated, it will go ahead and run a series of tests as 
follows.
Compare SHA-1 hashes of system binaries against known good values 
maintained in the database.
Check for known rootkit files and directories, as well as rootkit strings.
Perform malware detection, including checking for login backdoors, sniffer 
log files, and other suspicious directories.
Perform trojan specific checks such as examining enabled xinetd services.
Perform checks on network ports and interfaces.
Perform system boot checks.
Perform group and account checks.
Perform system configuration file checks.
Perform filesystem checks.
The following screenshots shows Rootkit Hunter in action.

xmodulo(3)ubuntu32: *
Vole Rootkit
Xzibit Rootkit 
X-Org SunOS Rootkit 
zaRwT.KiT Rootkit 
ZK Rootkit
[ Not found ] 
[ Not found J 
[ Not found ] 
[ Not found ] 
[ Not found ]
[ OK ]
[ OK ]
Performing additional rootkit checks
Suckit Rookit additional checks
[ OK ]
Checking for possible rootkit files and directories
[ None found ]
Checking for possible rootkit strings
[ None found ]
Performing malware checks
Checking running processes for suspicious files
[ None found ]
Checking for login backdoors
[ None found ]
Checking for suspicious directories
[ None found ]
Checking for sniffer log files
[ None found ]
Checking for Apache backdoor
[ Not found ]
Performing Linux specific checks 
Checking loaded kernel modules 
Checking kernel module names
Press <ENTER> to continue]

Once scanning is completed, rkhunter stores the result in
/var/iog/rkhunter.iog. You can check for any warning as follows.
I_____ ^^Ixterm
T $ sudo grep Warning /var/log/rkhunter.log
[11:09:12] /usr/bin/unhide.rb [ Warning ]
[11:09:12] Warning: The command '/usr/bin/unhide.rb' has been replaced by 
a script: /usr/bin/unhide.rb: Ruby script, ASCII text [11:10:53] Checking if 
SSH root access is allowed [ Warning ] [11:10:53] Warning: The SSH and 
rkhunter configuration options should be the same:
[11:10:54] Checking for hidden files and directories [ Warning ] [11:10:54]

Warning: Hidden directory found: /etc/.java
[11:10:54] Warning: Hidden directory found: /dev/.udev
[11:10:54] Warning: Hidden file found: /dev/.initramfs: symbolic link to
'/run/initramfs'
Rootkit Hunter relies on a set of database files to detect rootkits. If you would 
like to check if the database files are up-to-date, simply run rkhunter with 
--update option. If there is a newer version of the database files, it will 
automatically fetch up-to-date database files using wget.
xterm
Bl__ $ sudo rkhunter --update

rkhunter can be run as a cron job with --cronjob option, in which case rkhunter will 
perform scanning in non-interactive mode, and store scanning result in 
/var/log/rkhunter.log for offline inspection.
As a rootkit scanner tool, rkhunter can only detect rookits, but not remove 
them. Then what should you do if rkhunter reports the presence of a rootkit, or 
throws any kind of warnings? First, you need to check whether or not those 
cases are false-positives. Warnings could be triggered simply due to ongoing 
software upgrades, custom system configurations, or other legitimate binary 
changes. If you are not sure, seeking help from sources such as rkhunter user 
mailing list can be an option.

If your system is indeed infected with a rootkit, trying to remove the rootkit 
yourself may not be the best course of action, unless you are a security expert 
who is capable of diagnosing the full mechanism, attack vector and 
penetration path of the particular rootkit.
When a rootkit is discovered on your system, the best way to deal with the 
situation is probably taking the compromised system offline first, and then 
moving all your data away from the system. While doing so, do not back up 
any binary that you cannot confirm is clean.
Linux Tutorials
How to install dummynet on 
CentOS
Dummynet is an open-source network emulation tool which allows one to 
emulate various networking properties of a physical link, such as bandwidth 
capacity, packet losses, delays and queue length. Dummynet can be used to 
test experimental network protocols in an emulated network environment.
While dummynet had originally been developed for FreeBSD platform, it 
was later ported to Linux systems as an external kernel module. In this guide, 
I will describe how to compile and install dummynet kernel module on 
CentOS. This setup was tested on CentOS 6.2.
To start out, install all necessary prerequisites for building dummynet kernel 
module.
xterm
T__ $ sudo yum -y groupinstall "Development Tools"
The next step is to install a matching kernel source on CentOS. You need to 
use a matching kernel source in order to be able to load dummynet in your 
kernel. CentOS 6.2 has the following kernel.

term
$ sudo uname -r 
kJ2.6.32-220.17.1.el6.x86 64
Install the matching kernel source, and prepare for building dummynet 
against the kernel source.
I______^^Ixterm
$ mkdir -p
/tmp/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} $ echo 
"%_topdir /tmp/rpmbuild" > ~/.rpmmacros $ wget
http://vault.centos.org/6.2/os/Source/SPackages/kernel2.6.32-220.el6.src.rpm
$ rpm -i kernel-2.6.32-220.el6.src.rpm 2>&1 | grep -v 
mockb 
mockb
220.el6.tar.bz2 -C /usr/src/kernels/
$ cd /usr/src/kernels/2.6.32-220.17.1.el6.x86_64 $ sudo make oldconfig
$ sudo make prepare
$ sudo make scripts
__ ^^BDownload the latest dummynet source code, and compile it against 
the matching kernel source.
xterm
20120119.tar.gz
$ tar xvjvz ipfw3-20120119.tar.gz
$ cd ipfw3-20120119
$ cd ipfw3-20120119 
220.17.1.el6.x86_64

If build is successful, you will have dummynet kernel module called
ipfw_mod.ko created in ipfw3-20120119/dummynet2 directory. Try loading the 
module.
xterm
$ sudo insmod ipfw_mod.ko
___^^^^^^llf successful, finally go ahead and install the dummynet 
module. Optionally, you can make it automatically loaded upon boot as 
follows.
xterm
220.el6.x86_64/kernel/net/netfilter
$ sudo depmod
$ sudo sh -c 'echo modprobe ipfw_mod >> 
/etc/rc.modules'
$ sudo chmod +x /etc/rc.modules
Linux Tutorials
How to install pip on Linux
Question: I want to use pip to install Python packages. However, when I run 
pip, it says "pip: command not found". How can I install pip on [insert your 
Linux distro]?
There are various ways to install and manage Python packages. One of the 
easiest way is to use pip (or pip3 for Python3) command line tool. Using pip (or 
pip3), you can install/update/uninstall a Python package, as well as list all 
installed (or outdated) packages from the command line.
Install pip on Ubuntu, Debian or Linux Mint

T__ $ sudo apt-get install python-pip # for Python2 $ sudo apt-get install
python3-pip # for Python3
Note that on Ubuntu 20.04 or later, python-pip is no longer included in their 
repositories. Thus if you want to install pip on newer Ubuntu systems, you 
need to install it from the official source, as described at the end of this 
tutorial.
Install pip on Fedora or CentOS/RHEL 7 or Later
No third-party repository is needed to install pip amd pip3.
xterm
Hi_ $ sudo yum install python-pip # for Python2
$ sudo yum install python3-pip # for Python3__ L-J
Install pip on CentOS or RHEL 6 or Earlier
To install pip on old CentOS/RHEL system, first enable EPEL repository, and 
then run:
term
Hl_ $ sudo yum install python-pip # for Python2
Install pip on Arch Linux or Manjaro
term
T__ $ sudo pacman -S python2-pip # for Python2
$ sudo pacman -S python-pip # for Python3
Install pip on OpenSUSE

xterm
T__ $ sudo zypper install python-pip # for Python2 $ sudo zypper install
python3-pip # for Python3__ L-J
Install pip from the Official Source
If you want to install pip or pip3 from the official source directly, you can use 
the following commands.
For pip:
xterm
Bl_ $ wget https://bootstrap.pypa.io/get-pip.py
$ sudo python2 get-pip.py
L Bpor pip3:
xterm
T__ $ wget https://bootstrap.pypa.io/get-pip.py
$ sudo python3 get-pip.py
Linux Tutorials
How to improve the security of 
Magento e-commerce website
Magento is an e-commerce software platform used by small businesses and 
leading brands, and its community edition is a freely available open-source 
program. Thanks to the huge collection of third-party developed plugins and 
themes which extend the default functionality and design, Magento is 
evolving into the WordPress of the e-commerce industry.
However, the growing popularity of Magento gives hackers a good enough 
reason to target this e-commerce platform. Especially when you are operating 
an e-shop using Magento, you are dealing with potentially important and 
sensitive information about your customers, which is incredibly valuable 

commodity to hackers. Thus it is imperative that you perform thorough due 
diligence and follow best practices to secure every part of your Magento 
deployment.
In this tutorial we will cover best practices for Magento security and 
demonstrate some examples on how to secure your Magento website.
1. Choose a strong and secure password for the Magento 
administrator account
The most obvious first step to keep your Magento installation safe and secure 
is to have a strong password for your Magento administrator account. Make 
sure that the administrator's password is at least ten random characters long 
and include numbers, upper and lower case letters and special characters 
(e.g., "q&t:A;-Q&4'?>#6"). Delete any other accounts that you're not using. 
The less accounts you have, the less opportunities hackers will have.
2. Use an encrypted connection (SSL/HTTPS) for all Magento 
web pages
SSL is a secure protocol which encrypts the sensitive information, such as 
login credentials, personal information etc. sent from the web browser to the 
web server. To implement SSL on your Magento website, first obtain an SSL 
certificate (e.g., from Let's Encrypt). With the obtained certificate, configure 
the web server to listen on port 443 and enable SSL through the Magento 
administration back-end.
For example, if you use Apache on a Debian based server, create a new 
virtual host:
xterm
# vi /etc/apache2/sites-available/your-domain.comssl.conf
__ ^^■acIcI the following lines to it: 
<IfModule mod_ssl.c>
<VirtualHost *:443>
ServerName your-comain.com

DocumentRoot /var/www/html/magento/
SSLEngine on
#Specify the path to your SSL certificate file: 
SSLCertificateFile /etc/apache2/ssl/your-domain.crt
#Specify the path to private key file:
SSLCertificateKeyFile /etc/apache2/ssl/your-domain.key
#Specify the path to CA certificate:
SSLCACertificateFile /etc/apache2/ssl/ca-bundle.crt
<Directory /var/www/html/magento>
Options -Indexes +FollowSymLinks +MultiViews
Order allow,deny
AllowOverride All
</Directory>
ErrorLog /var/log/apache2/ssl_error.log
CustomLog /var/log/apache2/ssl_access.log combined
</VirtualHost>

</IfModule>
HI_ # a2enmod ssl
Disable the default SSL virtual host and enable the your-
domain.comssl.conf virtual host:
term
__# a2dissite default-ssl
# a2ensite your-domain.com-ssl.conf
I [Finally restart Apache for the changes to take effect:
xterm
T # /etc/init.d/apache2 restart
Log in to the Magento administration back-end, then go to System >> 
Configuration. From the menu on the left, under the General selection 
list, click Web. On the Secure tab change http to https, and set "Use 
secure urls in Frontend" and "Use secure urls in Admin" to Yes.
Do not forget to clear the Magento cache and your web browser's cache, then 
try to access your website with https.

GENERAL
Secure
General
Web
Base URL
http ://magento-ce.local/
[STORE VIEW]
Make sure that base URL ends with 7 (slash), eg.
Design
http://yourdonnain/magento/
Currency Setup
Base Link URL
{{secure base uri}}
[STORE VIEW]
Store Email Addresses
Contacts
Make sure that base URL ends with 7 (slash), e.g. 
http://yourdomain/magento/
Reports
Base Skin URL
{{secure base_url}}skin/
[STORE VIEW]
Content Management
Base Media URL
{{secure base_url}}media/
[STORE VIEW]
CATALOG
Base JavaScript URL
{{secure base_ui1}}js/
[STORE VIEW]
Warning! When using CDN, in some cases JavaScript
Catalog
may not run properly if CDN is not in your subdomain
Inventory
Use Secure URLs in Frontend
I Yes 
i
[STORE VIEW]
Google Sitemap
Use Secure URLs in Admin
[ Yes 
:
[GLOBAL]
RSS Feeds
Email to a Friend
Offloader header
SSL„OFFLOADED
[GLOBAL]
If some of your visitors bookmarked your website using http://yourdomain.com 
and you want to make sure that all requests to http are being redirected to 
https, you may add the following lines to the .htaccess file 
located in the document root of your Magento website (e.g.
/var/www/html/magento/.htaccess):
RewriteCond %{HTTPS} off
RewriteRule A(.*)$
 https://your-domain.com/$1 [R=301,L]
3. Create a custom path for the administrator back-end
The default admin URL for Magento (e.g., yourstore.com/store/admin) can become 
an easy target for hackers trying to break into the Magento backend using 
brute-force attacks. To prevent that, you can customize the default admin 
path as follows.
Log in to your server via SSH as root. Then open the local.xml file located in 
the /app/etc directory of your Magento installation (e.g.
/var/www/html/magento/app/etc/local.xml), and locate the following line:

<![CDATA[admin]]>
Replace admin to something more complex, like mymagento123 or whatever you 
want the URL of the administrator back-end to be. This will change the 
admin URL to yourstore.com/store/mymagento123.
4. Whitelist only approved IP addresses for admin access
Besides hiding the admin URL like above, you can restrict the access to the 
admin URL at the network level. In other words, restrict the admin access to 
only a few IP addresses or CIDR blocks that you explicitly approved. To set 
up an IP whitelist for Apache, add the following locationmatch directive inside 
your virtual host configuration. Note that mymagento123 is the custom admin 
login page that you define earlier.
<LocationMatch "MyMagento123">
Order Deny,Allow
Deny from All
Allow from 111.111.111.0/24
</LocationMatch>
Don't forget to restart Apache after updating the whitelist. Now you will be 
able to access the custom admin login page 
(yourstore.com/store/mymagento123) only from the specified IP address block.

5. Harden your Magento website's file permissions
Following the principle of least privilege, make sure that the Magento 
website files and directories are not writable by any one else except the web 
server. For this, the web server user (www-data on Debian, or apache on CentOS) 
needs to have access to Magento website files and directories under the 
Magento website document root, so run the following command to 
accomplish that (on Debian):
xterm
# chown www-data:www-data -R /var/www/html/magento
Set " chmod 750" on directories and 640 on all website files, except app/etc, media 
and var directories, which need 770 permissions (The 770 permissions give full 
control to the owner and the group, and no 
permissions to anyone else):
xterm
# find /var/www/html/magento -type f -print0 | xargs r0 chmod 640
# find /var/www/html/magento -type d -print0 | xargs r0 chmod 750
# chmod -R g+w
/var/www/html/magento/{app/etc,media,var}
6. Use two-factor authentication for admin access
There is no such thing as a silver bullet as far as security is concerned. In that 
regard, an additional layer of protection you can add to the admin access is 
two-factor authentication. With two-factor authentication extension enabled, 
you will be required to present a one-time security code to gain access to the 
Magento backend. One-time security code is generated by Google 
Authenticator app installed on your smartphone which a malicious hacker 
would not possess.

7. Use Magento extensions only from trusted sources
The security of any software system is as strong as its weakest link. While the 
huge ecosystem of third-party Magento extensions is certainly a boon to 
Magento users, each of these extensions could potentially introduce a new 
attack vector. Even with all other security protections in place, it takes only a 
single vulnerability of one poorly written extension to break down your 
Magento system. Before integrating any third-party extension to your 
Magento store, always do your due diligence checking out the reputation of 
the developer and customer reviews from independent communities such as 
Magento community. Use only those extensions from reputable sources with 
a good track record, and those which are regularly updated and maintained.
8. Update Magento on regular basis
Keep the Magento installation including all extensions and themes up to date 
by upgrading whenever a new security patch or version comes out. It's a good 
idea to create a backup of your Magento website files and its database before 
you start updating Magento.
To find out which Magento version is currently installed on your website, log 
in to your server via SSH as root, navigate to the document root directory of 
your Magento based website (e.g. /var/www/html/magento) and run the following 
commands:
term
# cd /var/www/html/magento
# php -r "include 'app/Mage.php'; echo 'Your Magento version is: ', 
Mage::getVersion(); " ; echo "";
Conclusion
In this tutorial, we discuss best security practices for Magento-powered 

ecommerce websites. Following these guidelines, you will substantially 
improve the security of your Magento website. There are other techniques 
you can use outside your Magento configuration to improve the general 
security of your website, like blocking unwanted IP addresses or installing an 
IDS, so be sure to do everything you can. Generally, it's better to host an e­
shop on a VPS or a dedicated server rather than on a shared hosting. You will 
have better performances and you will have more control over your host if 
you are using a VPS or dedicated hosting. Whatever you do, be careful when 
configuring your Magento. Remember that one small mistake can break your 
whole Magento installation.
Linux Tutorials
How to download webcomics from
the command line on Linux
Do you never miss a new strip from xkcd? Read webcomics regularly? Or 
would you like to back up all the strips of your favorite website? Hopefully, 
the open source community has the solution: a command line program to 
download all your favorite webcomics from your terminal.
Before we begin, remember that you should keep these downloaded strips for 
your personal use, and not broadcast them without permission. If you really 
like an author's work, support the comic by donating or buying some of the 
merchandise.
Install Dosage on Linux
The open source program to download webcomics is called Dosage. There 
are a couple of ways to install this webcomic downloader on your machine 
since it is written in Python. Today we will go with an easy way.
First, you will need to install pip3. Also, make sure that you have at least 
Python 3.5 installed. Then use pip3 to install dosage as follows.

xterm
T $ sudo pip3 install dosage
__ ^Hlf pip3 cannot somehow find the package, use the following 
command instead.
I_____ ^^Ixterm
$ sudo pip3 install
https://github.com/webcomics/dosage/releases/download/2 .17/dosage-
2.17.tar.gz
___^^Mdosage will automatically create a new folder called Comics in your 
home directory.
Basic Usage of dosage
dosage 's basic usage can be described as follows. Using dosage, you can find 
webcomics in the database that you are interested in reading, download the 
strips, and easily fetch the latest strips as they come out. In a sense, you more 
or less subscribe to a webcomic, and dosage will take care of making sure that 
you never miss any unread strips.
To start downloading and reading offline your webcomics, begin by listing 
them with the command:
xterm
T $ dosage -l
__^^Bkight now, dosage has over 2000 comics in its database. My personal 
tip is if you are looking for a particular webcomic, use the syntax:
xterm
$ dosage -l | grep [keyword]
__^^Hil will then return all the comics with title containing [keyword].
Once you decided which comic you wanted to subscribe to from the list, use 
this command to subsribe to the comic:
xterm
T $ dosage [name of the webcomic]

File Edit View Terminal Go Help
adrien@xubuntu-virtual:~$ dosage captainSNES
CaptainSNES> Retrieving 1 strip
CaptainSNES> Saved Comics/CaptainSNES/2014-04-11b.png (226.42KB).
CaptainSNES> Saved Comics/CaptainSNES/2014-04-11 a.png (243.47KB). 
adrien@xubiintii-virtual :~$ |
Subscribing to a comic will automatically create a folder in the Comics 
directory, and download the latest strip of that webcomic.
If instead of downloading just the latest strip, you are interested in all the 
issues, use this command:
xterm
T $ dosage -a [name of the comic]

Finally, once you subscribed to a couple of webcomics, you can easily 
download the latest strip of all of them in one shot with the simple command 
below:
xterm
Bl__ $ dosage @
—■■if you never want to miss your daily comics for example, you 
should run this command every day.
Advanced Usage of dosage
Past the first day playing around with dosage, you might want to get the most 
out of it. It entails knowing a bit more about the command's syntax and 
shortcuts.
If you tried to download some xkcd strips, you might have noticed that dosage 
refuses with the message:
use the --adult option to confirm your age

Because by default dosage will ignore any webcomic flagged for people over 
18 (and for some reason xkcd is one of them). To bypass that, just do as it 
says:
xterm
T__ $ dosage --adult xkcd
From a previous example, you may have noticed that the argument
@ is used to refer to all downloaded comics. A continuation is @@ for all

comics in dosage database.
xterm
T $ dosage @@
_J^®The above command will download the latest strip of every comic 
that dosage knows about.
If you want to fetch the strips from the beginning of the series up to a 
particular day, you can do:
xterm
lb_ $ dosage -a [name of the comic]:[year-month-day]
_J^HFor example, to see all of Calvin and Hobbes' strips from 2014 until 
its creation, run:
xterm
T $ dosage -a calvinandhobbes:2014-01-01
Finally for all the developers out there who would like to do something of 
these strips for your personal use, dosage integrates the possibility of 
generating rss, json, and html log files while downloading strips:
term
$ dosage -o [type] [name of the comic]
In the above command, [type] is either rss, json, or html, and [name of the comic] 
can also be just @. For example, the html argument will create a nice HTML 
code to see all the strips downloaded:
The command below will download all the strips for Calvin and Hobbes, and 
then spit out an HTML code to view in your web browser all the strips in a 
nice webpage format.
term
$ dosage -o html -a calvinandhobbes

To conclude, I invite you to go visit the official site for more information.

dosage is a really neat tool, and I know that it will be of great use to any fans 
of webcomics out there. I'm very curious to know what can come out of the 
fancier options like creating a json file out of downloaded strips.
Do you have an alternative to dosage? Or are you actually a fan of the latter 
and use it regularly? Let us know in the comments.
Linux Tutorials
How to manage your personal 
expenses from the command line
The Linux command line can be used for many things, like making 
spreadsheets, playing music, or accessing Facebook or Twitter. And to pursue 
our dream of ultimate graphic-less knowledge, I propose you today with a 
tool to manage your personal expenses from the command line. The name is 
gnu Pem, for "Personal Expense Manager".
Installation of GNU Pem
For Debian or Ubuntu:
I could not find a PPA for recent versions of Ubuntu, so the simplest way is 
still to download the package from the official source, extract it, and then run 
the traditional:
term
$ ./configure
$ make
$ sudo make install

For Arch Linux:
For Arch Linux users, you can check out the AUR.
For Fedora or CentOS/RHEL:
xterm
$ sudo yum install pem
For CentOS/RHEL, you need to enable EPEL repository first.
Usage of GNU Pem
Pem uses your system clock to record your transactions. So before using it, 
make sure that the time on your computer is correct.
Pem is very easy to understand. The basic usage to record an expense is:
xterm
lb_ $ pem [tag] [amount]
__ I^Bwhere [tag] is anything that will remind you of your expense.
So for example:
xterm
T $ pem candy 10
__ ^^Hwill remind me that I spent $10 in candy today. If I want to use 
spaces in my tag, I like to use quotation marks:
xterm
lb_ $ pem "magic cards" 40
I Ito record an income, use the flag -e:
xterm
lb_ $ pem -e babysitting 50
-L^JFinally, to see your expenses for the last n days, use the command: 
xterm
T $ pem -s [n]
__ I^^Notice that if you leave n blank, it will show you all your expenses 
for the day nicely formatted.

■ -(-)------------------------------------------- 0
' - pem -s
Date
Description
Earned
Spent
nov.-23 14 candy 
babysitting 
magic cards
0.00
50.00
0.00
JQ.00 
0.00 
40.00
nov.-23 14|dimanche (Total)
| 
|Balance
| 
50.00
1
f' 50.00|
0,00|
|Average expense/day
HF
It is possible to have a monthly report with a similar structure:
xterm
lb_ $ pem -m [n]
___^^■bliL a really helpful feature is the ability to set categories. Using the 
flag -c you can specify a category for your expense.
xterm
lb_ $ pem -c [name] [tag] [expense]
_L~Jah example would be:
xterm
lb_ $ pem -c games Pac-Man 2
__ I^Bfor spending $2 buying a Pac-Man game.
And you can see all the categories with:
xterm
lb_ $ pem -C

Behind the scenes, Pem will create by default a .pem folder in your home 
directory in which it will store your transaction using the directory structure: 
.pem/[last two digit of year]/[Month]
A transaction will be recorded in the month file using the format: 
[code],[tag], [income], [expense]
If you wish to write your expenses to a specific file, you can use the -f flag: 
xterm
Il_ $ pem -f myexpenses.txt candy 10
__ ^^H'lhe details are not that interesting, unless you want to export the 
data later. The programmers suggest using a spreadsheet software to do so.
To conclude, I really enjoy using Pem. It is a nice little piece of sotfware that 
does one job and does it well. It is possible to imagine combining it with 
some sort of script to automate the whole process. I could probably use it to 
record all the money I spent buying Steam games on sale. I encourage you to 
visit the official page, and read the manual entry to learn more about it.
What do you think of Pem? Would you consider using it or is it too nerdy? 
Let us know in the comments.

Linux Tutorials
How to install and configure tinc
VPN on Linux
tinc is an open-source VPN software with a number of powerful features not 
found in other VPN solutions. For example, tinc VPN allows peers behind 
NAT to communicate with one another via VPN directly, not through a third 
party server. This makes tinc a type of peer-to-peer VPN solution. Other 
features include full IPv6 support and path MTU discovery.
In this tinc example, I will show you how to set up a VPN connection between 
two hosts via tinc. Let's call these hosts alice and bob respectively. Note that 
these are just symbolic names used by tinc, not necessarily hostnames. In this 
example, I assume that host bob will initiate a VPN connection to host alice.
Install tinc on Linux
First, install tinc on both hosts.
For CentOS system, first set up RepoForge repository, and then do the 
following.
xterm
Bl_ $ sudo yum install tinc -y
_J^HFor Debian/Ubuntu system:
l_^lxterm
_ $ sudo apt-get install tinc
Configure tinc
Now, let's go ahead and configure tinc VPN on both hosts as follows.
On host alice, do the following.
term

$ sudo mkdir -p /etc/tinc/myvpn/hosts
__ B^®Then create a tinc configuration file called tinc.conf, and host 
configuration file(s) as follows.
xterm
Il_ $ sudo vi /etc/tinc/myvpn/tinc.conf
Name = alice 
AddressFamily = ipv4 
Interface = tun0
In the above example, the directory myvpn under /etc/tinc is the name of the 
VPN network to be established between alice and bob. VPN name can be any 
alphanumeric name without containing -. In tinc.conf, the
Name field indicates the name of tinc-running local host, which doesn't have to 
be actual hostname. You can choose any generic name.
Next, create host configuration files which contain host-specific information.
xterm
lb_ $ sudo vi /etc/tinc/myvpn/hosts/alice
Address = 1.2.3.4
Subnet = 10.0.0.1/32
The name of host configuration file (e.g., alice) should be the same as the one

you defined in tinc.conf. The Address field indicates a globally routable public IP 
address associated with alice. This field is required for at least one host in a 
given VPN network so that other hosts can initiate VPN connections to it. In 
this example, alice will serve as the bootstrapping server, and so has a public 
IP address (e.g., 1.2.3.4). The Subnet field indicates the VPN IP address to be 
assigned to alice.
The next step is to generate public/private keys.
term
Hb_ $ sudo tincd -n myvpn -K4096
The above command will generate 4096 bit public/private keys for host alice.
The private key will be stored as /etc/tinc/myvpn/rsa_key.priv, 
and the public key will be appended to /etc/tinc/myvpn/hosts/alice.
Next, configure the scripts that will be run right after tinc daemon gets started, 
as well as right before tinc daemon is terminated.
term
lb_ $ sudo vi /etc/tinc/myvpn/tinc-up
_ ^H#! /bin/sh
ifconfig $INTERFACE 10.0.0.1 netmask 255.255.255.0
term
lb_ $ sudo vi /etc/tinc/myvpn/tinc-down
_ ^H#! /bin/sh
ifconfig $INTERFACE down

xterm
T $ sudo chmod 755 /etc/tinc/myvpn/tinc-
*
_J^Hnow tinc configuration for host alice is done. Similar to alice, 
configure tinc on host bob as follows.
xterm
T__ $ sudo mkdir -p /etc/tinc/myvpn
$ sudo vi /etc/tinc/myvpn/tinc.conf
Name = bob 
AddressFamily = ipv4 
Interface = tun0 
ConnectTo = alice
In the above, note that unlike host alice, I place connectto field in bob's tinc 
configuration, since host bob will initiate a VPN connection to host alice when 
tinc daemon on host bob is up.
xterm
Bl_ $ sudo vi /etc/tinc/myvpn/hosts/bob
__I^BSubnet = 10.0.0.2/32
I
xterm
lb_ $ sudo tincd -n myvpn -K4096
Similarly, the bob's private key will be stored as

/etc/tinc/myvpn/rsa_key.priv, and its public key will be added to /etc/tinc/myvpn/hosts/bob.
lb_ $ sudo vi /etc/tinc/myvpn/tinc-up
ifconfig $INTERFACE 10.0.0.2 netmask 255.255.255.0
each host's public key file onto the other host: 
On host alice:
xterm
lb_ $ scp /etc/tinc/myvpn/hosts/alice
[email protected]:/etc/tinc/myvpn/hosts/
L lOn host bob:
xterm
T__ $ scp /etc/tinc/myvpn/hosts/bob
[email protected]:/etc/tinc/myvpn/hosts/
Finally, start tinc daemon on them as follows. Since host bob initiates a VPN 
connection, you will need to start tinc daemon on host alice first, and then host 
bob.
term
__ $ sudo tincd -n myvpn

Two hosts should now be able to talk to each other via VPN IP
addresses assigned to them.
Linux Tutorials
How to use local storage for
CloudStack VMs
When CloudStack is deploying a VM (including guest VMs and system 
VMs), by default, the VM is set to use NFS-mounted primary storage as their 
root partition. The NFS-mounted primary storage is shared among all 
compute nodes located in the same CloudStack cluster. One of the reasons for 
using shared primary storage is VM migration. It is easy to migrate a VM 
from one compute node to another if the VM is backed by the same shared 
primary storage. If you don't need to migrate VMs, and compute nodes have 
plenty of local storage, you can configure CloudStack such that deployed 
VMs use compute node's local storage.
In order to use local storage for CloudStack VMs, log in to CloudStack's 
management interface as admin, and do the following.
Go to "Global Settings", and change the following attributes to true: 
system.vm.use.local.storage 
use.local.storage
Note that these changes won't take effect until CloudStack's management 
server gets restarted. So make sure to restart CloudStack management server 
afterward:
xterm
lb_ $ sudo service cloud-management restart
Next, you must create a new Compute Offering which uses local storage for 
VM storage. To do so, go to "Service Offerings" in CloudStack's management 
interface, and choose "Compute Offerings". Then add a new Compute Offering 

with "Storage type" set to Local.
Now when you deploy a new VM, you need to choose the Compute Offering 
that you just created, in order to use local storage.
Linux Tutorials
How to configure a network printer 
and scanner on Ubuntu desktop
In the previous article, we discussed how to install several kinds of printers 
(and also a network scanner) in a Linux server. Today we will deal with the 
other end of the line: how to access the network printer/scanner devices from 
a desktop client.
Network Environment
For this setup, our server's (Debian Wheezy 7.2) IP address is
192.168.0.10, and our client's (Ubuntu 12.04) IP address is
192.168.0.105. Note that both boxes are on the same network (192.168.0.0/24). If we 
want to allow printing from other networks, we need to modify the following 
section in the cupsd.conf file on the sever:
<Location />
Order allow,deny
Allow localhost
Allow from XXX.YYY.ZZZ.
*
</Location>

(in the above example, we grant access to the printer from localhost and from 
any system whose IPv4 address starts with xxx.yyy.zzz)
To verify which printers are available on our server, we can either use lpstat 
command on the server, or browse to the
https://192.168.0.10:631/prmters page.
xterm
lb_ [email protected]:~# Ipstat -a
EPSON_Stylus_CX3900 accepting requests since Mon 18 Aug 2014 
10:49:33 AM WARST
PDF accepting requests since Mon 06 May 2013 04:46:11 PM WARST 
SamsungML1640Series accepting requests since Wed 13 Aug 2014 10:13:47 
PM WARST

Installing Network Printers in Ubuntu Desktop
In our Ubuntu 12.04 client, we will open the Printing menu (Dash -^ Printing). 
Note that in other distributions the name may differ a little (such 
as Printers or Print & Fax, for example):
No printers have been added to our Ubuntu client yet:

Here
are the steps to install a network printer on Ubuntu desktop client.
1) The Add button will fire up the New Printer menu. We will choose Network 
printer -^ Find Network Printer and enter the IP address of our server, then click
Find:
2) At the bottom we will see the names of the available printers. Let's choose 
the Samsung printer and press Forward:
3) We will be asked to fill in some information about our printer. When we're 
done, we'll click on Apply:

4) We will then be asked whether we want to print a test page. Let’s click on
Print test page:

The print job was created with local id 2:
5) Using our server's CUPS web interface, we can observe that the print job 
has been submitted successfully (Printers -^ SamsungML1640Series -^
Show completed jobs):

Search in SamsungML1640Series:
Show Active Jobs Show All Jobs
Showing Sol 3 completed jobs,
Name User Size Pages State
Search Clear
Control
SamsungML1640Sertes-29 Unknown Withheld lit 1 completed at 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Mon18Atjg2014 11:46:05AMWARST
SamsungML1640Series-28 
Unknown 
Withheld 
lit 
I 
completed at
Mon18Aug20U1128S8AMWARST
SamsungML1640Series-27 
Unknown 
Withheld 
lit 
1 
completed at
Wed13Aug201410:15:42PMWARST
We can also display this same information by running the following 
command on the printer server:
T__ [email protected]:~# cat /var/log/cups/page_log | grep
- i samsung
SamsungML1640Series root 27 [13/Aug/2014:22:15:34 -0300] 1 1 - 
localhost Test Page -
SamsungML1640Series gacanepa 28 [18/Aug/2014:11:28:50 -0300] 1 1
- 192.168.0.105 Test Page -
SamsungML1640Series gacanepa 29 [18/Aug/2014:11:45:57 -0300] 1 1
- 192.168.0.105 Test Page -

The page_log log file shows every page that has been printed, along with the 
user who sent the print job, the date & time, and the client's IPv4 address.
To install the Epson inkjet and PDF printers, we need to repeat steps 1 
through 5, and choose the right print queue each time. For example, in the 
image below we are selecting the PDF printer:
that according to the CUPS-PDF documentation, by default:
PDF files will be placed in subdirectories named after the owner of the print 
job. In case the owner cannot be identified (i.e. does not exist on the server) 
the output is placed in the directory for anonymous operation (if not disabled 
in cups-pdf.conf - defaults to

/var/spool/cups-pdf/anonymous/).
These default directories can be modified by changing the value of the Out 
and anondirname variables in the /etc/cups/cups-pdf.conf file. Here, ${home} is 
expanded to the user's home directory:
Out ${HOME}/PDF
AnonDirName /var/spool/cups-pdf/ANONYMOUS
Network Printing Examples
Example #1
Printing from Ubuntu 12.04, logged on locally as gacanepa (an account with the 
same name exists on the printer server).

After printing to the PDF printer, let's check the contents of the 
/home/gacanepa/PDF directory on the printer server:
term

[email protected]:~# Is -l /home/gacanepa/PDF 
total 368
-rw------- 1 gacanepa gacanepa 279176 Aug 18 13:49 Test_Page.pdf
-rw------- 1 gacanepa gacanepa 7994 Aug 18 13:50 Untitled1.pdf
-rw------- 1 gacanepa gacanepa 74911 Aug 18 14:36
Welcome_to_Conference_-_Thomas_S__Monson.pdf
The PDF files are created with permissions set to 600 (-rw-------), which
means that only the owner (gacanepa in this case) can have access to them. We 
can change this behavior by editing the value of the userumask variable in the 
/etc/cups/cups-pdf.conf file. For example, a umask of 0033 will cause the PDF 
printer to create files with all permissions for the owner, but read-only 
privileges to all others.
xterm
T [email protected]:~# grep -i UserUMask /etc/cups/cupspdf.conf
__ Key: UserUMask
UserUMask 0033
For those unfamiliar with umask (aka user file-creation mode mask), it acts as 
a set of permissions that can be used to control the default file permissions 

that are set for new files when they are created. Given a certain umask, the 
final file permissions are calculated by performing a bitwise boolean AND 
operation between the file base permissions (0666) and the unary bitwise 
complement of the umask. Thus, for a umask set to 0033, the default permissions 
for new files will be NOT (0033) AND 0666 = 644 (read / write / execute 
privileges for the owner, read-only for all others.
Example #2
Printing from Ubuntu 12.04, logged on locally as jdoe (an account with the 
same name doesn't exist on the server).

term
__[email protected]:~# Is -l /var/spool/cups
pdf/ANONYMOUS

total 5428
-rw-rw-rw- 1 nobody nogroup 5543070 Aug 18 15:57
Linux__Wikipedia__the_free_encyclopedia.pdf
The PDF files are created with permissions set to 666 (-rw-rw-rw-), which 
means that everyone has access to them. We can change this behavior by 
editing the value of the anonumask variable in the /etc/cups/cupspdf.conf file.
At this point, you may be wondering about this: Why bother to install a 
network PDF printer when most (if not all) current Linux desktop 
distributions come with a built-in "Print to file" utility that allows users to 
create PDF files on-the-fly?
There are a couple of benefits of using a network PDF printer:
A network printer (of whatever kind) lets you print directly from the 
command line without having to open the file first.
In a network with other operating system installed on the clients, a PDF 
network printer spares the system administrator from having to install a PDF 
creator utility on each individual machine (and also the danger of allowing 
end-users to install such tools).
The network PDF printer allows to print directly to a network share with 
configurable permissions, as we have seen.
Installing a Network Scanner in Ubuntu Desktop
Here are the steps to installing and accessing a network scanner from Ubuntu 
desktop client. It is assumed that the network scanner server is already up and 
running as described here.

1) Let us first check whether there is a scanner available on our Ubuntu client 
host. Without any prior setup, you will see the message saying that "No 
scanners were identified."
term
__ $ scanimage -L
No scanners detectec
which came with this software (README, FAQ, manpages). 
gacanepa@ubuntuOS:~$ I
jase check your scanner is cc
gacanepa@ubuntuOS:~
gacanepa@ubuntuOS:~$ scanimage -L
No scanners were identified. If you were expecting something different,
check that the scanner is plugged in, turned on and detected by the
sane-find-scanner tool (if appropriate). Please read the documentation
2) Now we need to enable saned daemon which comes pre-installed on Ubuntu 
desktop. To enable it, we need to edit the /etc/default/saned file, and set the RUN 
variable to yes:
xterm
lb_ $ sudo vim /etc/default/saned
_ ^H# Set to yes to start saned
RUN=yes

3) Let's edit the /etc/sane.d/net.conf file, and add the IP address of the server 
where the scanner is installed:
4) Restart saned:
term
$ sudo service saned restart
fc) Let's see if the scanner is available now:
gacanepa@ubuntuOS:~$ scanimage -L
device 'net:192.168.0.10:epson2:ltbusb:0O4:O02l is a Epson CX4000 flatbed scanner 
gacanepa@ubuntuOS:~$ |
Now we can open Simple Scan (or other scanning utility) and start scanning 
documents. We can rotate, crop, and save the resulting image:

Summary
Having one or more network printers and scanner is a nice convenience in 
any office or home network, and offers several advantages at the same time. 
To name a few:
Multiple users (connecting from different platforms / places) are able to send 
print jobs to the printer's queue.
Cost and maintenance savings can be achieved due to hardware sharing.
I hope this article helps you make use of those advantages.
Linux Tutorials
How to install Xen hypervisor on 
unused old hardware

Xen is a bare metal hypervisor, meaning that you must prepare a bare 
machine to install and run Xen. KVM is a little different - you can add it to 
any machine already running Linux. This tutorial describes how to install and 
configure Xen hypervisor on unused hardware.
This procedure uses Debian Jessie (their testing distribution) as the host OS 
(also known as Dom0). Jessie is not the only choice - Xen support is built 
into the Linux kernel, and plenty of Linux distributions include one of these 
Xenenabled kernels.
Find Unused Hardware
As a start, find a suitable workstation which can be wiped out, such as an old 
laptop or desktop. Older hardware may not be good for gaming, but it is good 
enough for a host OS and a couple of guests. A PC with these specifications 
works fine.
1 CPU with 2 cores (64-bit)
4GB memory
80GB hard disk
ability to boot from CD, DVD or USB a network interface
Note that the CPU must be a 64-bit processor since Debian dropped support 
for 32-bit Xen packages. If you don't have spare hardware, you could invest 
in an old machine. 2010's $1000 flagship laptop is today's $100 bargain. A 
second-hand laptop from eBay and a memory upgrade will do fine.
Burn a Bootable CD/USB
Download the ISO image for Debian Jessie. The small netinst image available 
from the official Debian website works fine.
xterm
$ wget
http://cdimage.debian.org/cdimage/jessie_di_beta_2/amd6 4/iso-cd/debian- 
jessie-DI-b2-amd64-netinst.iso

__■^■Next, identify the device name assigned to your CD/DVD or USB 
drive (e.g., /dev/sdc).
Burn the downloaded ISO image into a bootable CD or a USB using dd 
command. Replace /dev/sdc with the device name you identified above.
xterm
lb_ $ sudo dd if=debian-jessie-DI-b2-amd64-netinst.iso of=/dev/sdc
Start the Installation
To start the installation, boot with the Debian installer CD/USB.
It's a good idea to use a wired connection, not WiFi. If the WiFi won't 
connect because firmware or driver software is missing, you won't get very 
far.

Debian GHIJ/L
menu
debi a ii
GNU/Linud
Press ENTER to boot or TAB to edit a
InstaJ 1
Graphical instaii
Advanced options
Help ™
Install with speech synthesis
menu entry
Partition the Disk
This setup uses four primary disk partitions. Automatic OS installers usually 
set up an extended partition that contains logical partitions. Set up the four 
partitions like this.
sda1 : mount on /boot, 200MB
sda2: mount on /, 20GB, Ubuntu uses 4GB
sda3: swap, 6GB (4GB of memory x 1.5 = 6)
sda4: reserved for LVM, not mounted, all the rest of the disk space

Install the Base System
It's a good idea to make the install as simple and short as possible. A basic 
working system can always be added to later. Debian's APT (Advanced 
Package Tool) makes adding software easy. Installing Debian on a 
workstation can cause pretty obscure time-wasting issues. Perhaps a graphics 
driver does not agree with the kernel or maybe the old CD-ROM drive only 
works intermittently.
When it comes to choosing what to install, do install an SSH server and don't 
install a desktop like Gnome.

A graphical desktop requires hundreds of package installs - it's a lot of extra 
work that can be done later. If you run into problems, waiting for that desktop 
install is a waste of time. Also, without desktop component, the system boot 
will be much quicker - seconds rather than minutes. This procedure requires a 
few reboots, so that's a handy time-saver.

An SSH server lets you configure the workstation from another computer. 
This allows you to avoid some of the problems with old hardware - perhaps 
the old machine's keyboard is missing keys, the LCD screen has dead pixels 
or the trackpad is unresponsive etc.
Add LVM (Logical Volume Manager)
Install the LVM tools as the root.
xterm
lb_ # apt-get update
# apt-get install lvm2
i^Jpick a physical volume to work with. 
xterm
T # pvcreate /dev/sda4 
i^Jcreate a volume group.
xterm
T # vgcreate vg0 /dev/sda4
__ ^^Bvou don't need to create a logical volume. If you want to test LVM 
works, create a volume then delete it.
xterm
lb_ # lvcreate -nmytempvol -L10G vg0
# lvremove /dev/vg0/mytempvol
__ I^Bcheck LVM status.
term
# pvs (to view information about physical volumes) # vgs (to view 
information about volume groups) # lvs (to view information about logical 
volumes)
Add a Linux Ethernet Bridge
We are going to set up a Linux bridge so that all Xen's guest domains can be 

connected to, and communicate through the bridge.
Install the bridge tools.
xterm
T # apt-get install bridge-utils 
USee what interfaces are configured.
xterm
lb_ # ip addr
rootixenhost|ip addrj
1: lo: (LOOPBACK,UP,LOReR_UP> mtu 65536 qdisc noqueue state UNKNOWN group defaul 
t
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
inet6 ::1/128 scope host 
valid_lft forever preferredjft forever
2: etho:| (BROADCAST,MULTICAST,UP,LOHER_UP> mtu 1500 qdisc pfifo.fast state UP gr 
oup default qlen 1000
link/ether 08:00:27:36:4f:41 brd ff:ff:ff:ff:ff:ff
inet 192.168.1.7/24 brd 192.168.1.255 scope global ethO
valid.lft forever preferredjft forever
inet6 fe80::aOO:27ff:fe36:4f41/64 scope link 
valid.lft forever preferredjft forever 
rootOxenhost: J
In this example, we have one primary interface assigned eth0. We are going to 
add eth0 to the Linux bridge by editing the network configuration file 
(/etc/network/interfaces).
Before making any change, back up the network configuration file to keep 
the original working configuration safe.
B__xterm
# cd /etc/network/
# cp interfaces interfaces.backup 
# vi /etc/network/interfaces
The file contents look something like this. 
auto lo 
iface lo inet loopback

allow-hotplug eth0
iface eth0 inet dhcp
Change the file to this. 
auto lo
iface lo inet loopback 
auto eth0
iface eth0 inet manual
auto xenbr0
iface xenbr0 inet dhcp 
bridge_ports eth0

Activate the network configuration change:
lb_ # systemctl restart networking
Verify Networking Settings
Verify that a Linux bridge xenbro is created successfully.
I___ xterm
■f—# ip addr show xenbr0
Also check that the primary interface eth0 is successfully added to the 
bridge. 
xterm
T_ # brctl show

root@xenhost: [ip addr show xenbro]
3: xenbrO: (BROADCAST,MULTICAST,UP,LOHER_UP> mtn 1500 qdisc noqueue state UP gro 
up default
link/ether 08:1^7:36:4f:41 bhd ff:ff:ff:ff:ff:ff
|inet 192.168.1.7/24 brd 192.168.1.255 scope global xenbro] 
vaTid_ltt forever preferred_ltt forever
inet6 fe80::aOO:27ff:fe36:4f41/64 scope link
valid.lft forever preferred_lft forever 
root@xenhost
rootSxenhost:brctl show
bridge name 
bridge id 
STP enabled interfaces
IxenbrO 
8000.080027364f 41 
no 
ethO|
rooTidxennosT:"#
You now have a working machine with Jessie installed. Xen is not yet 
installed at this point. Let's proceed to install Xen next.
Install the Xen Hypervisor
Install Xen and QEMU packages, and update the GRUB bootloader.
xterm
Bl_ # apt-get install xen-linux-system
__ l^lReboot.
When the GRUB screen appears, you can see extra booting options listed.

GNU GRUB version 2.02~beta2-15
Debian GNU/Linux
Advanced options for Debian GNU/Linux
^Debian GNU/Linux, with Xen hypervisor
Advanced options for Debian GNU/Linux (frith Xen hypervisor)
Choose this option to boot into Xen DomO
Use the 1 and I keys to select which entry is highlighted. 
Press enter to boot the selected OS, 'e' to edit the commands 
before booting or 'c' for a command-line.
The first option will boot automatically in five seconds (see the 
grub_timeout line in /etc/default/grub), so this is not the time to get a coffee.
Press the down arrow to highlight the option Debian gnu/linux, with Xen hypervisor, 
and press return. Many lines of information appear, followed by the usual 
login screen.
Check Xen Works
Xen hypervisor comes with Xen management command-line tool called xl, 
which can be used to create and manage Xen guest domains. Let's use xl 
command to check if Xen is successfully installed.

Log in as root, and run:
xterm
Hl_ # xl info
hich will display various information about Xen host.
root@xenhost:~# xl info
host
xenhost
release
3.16.0-4-amd64
version
#1 SMP Debian 3.16.7-2 (2014-11-06)
machine
x86 64
nrcpus
1
max_cpu_id
0
nrnodes
1
cores per socket
1
threads_per_core
1
cpumhz
2084
hwcaps
078bfbff:28100800:00000000:00000100:00000209:00000000:0
0000001:00000000
virtcaps
total memory
1023
freememory
63
sharing_freed_memory
0
sharing used memory
©
outstandingclaims
0
f reecpus
0
xenjnajor
4
xenminor
4
xenextra
.1
xenversion
4.4.1
xencaps
xen-3.0-x86 64 xen-3.0-x86_32p
xenscheduler
credit
xenpagesize
4096
platform_params
virt start=0xffff8O0000000O0O
xen_changeset
xencommandline
placeholder
To see a list of existing Xen domains:
xterm
Hl_ # xl list 

root@xenhost:~# xl list
Name 
ID Mem VCPUs State Time(s)
Domain-0 
0 945 
1 r......  
11.2
root@xenhost:~# 
root@xenhost:~#
A little table of domains appears. Without any Xen guest domain created, the 
only entry should be Domain-0, your Debian installation.
Change the Boot Order
When you reach this point, the Xen install is complete. There is one more 
thing to fix - the default boot will not load Xen. GRUB chooses the first item 
in the boot menu (Debian GNU/Linux), not the third (Debian GNU/Linux, 
with Xen hypervisor).
The default option in the boot menu is defined in the GRUB configuration 
file /boot/grub/grub.cfg. To change the default option, don't edit that file, 
but edit /etc/default/grub instead. A little helper program called grubmkconfig reads 
in this default configuration file and all the templates in /etc/grub.d/, then writes 
the grub.cfg file.
Edit Debian's configuration file for grub-mkconfig. 
xterm
T # vi /etc/default/grub
_^^Bchange the line:
GRUB DEFAULT=0
to
GRUB_DEFAULT='Debian GNU/Linux, with Xen hypervisor'

Then update the GRUB configuration file.
xterm
T # grub-mkconfig -o /boot/grub/grub.cfg
Finally reboot. After a few seconds, the grub boot menu appears. Check that 
the third option Debian gnu/linux, with Xen hypervisor is highlighted automatically.
Final Note
If you use this machine as your hands-on workstation, install a graphical 
desktop. The Debian library includes a few desktop environments. If you 
want a graphical desktop that includes everything and the kitchen sink, go for 
Gnome. If graphics just get in your way, try Awesome.
Note that the Debian Jessie default environment Gnome comes with a huge 
amount of extra applications including the productivity suite LibreOffice, the 
Iceweasel web browser and the Rhythmbox music player. The install 
command apt-get install gnome adds 1,000 packages and takes up nearly 2GB of 
disk space. Running this heavyweight desktop takes up 1GB of memory.
Linux Tutorials
How to remove trailing whitespaces 
in a file on Linux
Question: I have a text file in which I need to remove all trailing 
whitespsaces (e.g., spaces and tabs) in each line for formatting purpose. Is 
there a quick and easy Linux command line tool I can use for this?
When you are writing code for your program, you must understand that there 
are standard coding styles to follow. For example, "trailing whitespaces" are 
typically considered evil because when they get into a code repository for 
revision control, they can cause a lot of problems and confusion (e.g., "false 

diffs"). Many IDEs and text editors are capable of highlighting and 
automatically trimming trailing whitepsaces at the end of each line.
Here are a few ways to remove trailing whitespaces in Linux command­
line environment.
Method One: sed
A simple command line approach to remove unwanted whitespaces is via sed. 
The following command deletes all spaces and tabs at the end of each line in 
input.java.
xterm
lb__ $ sed -i 's/[[:space:]]
*$//'
 input.java
If there are multiple files that need trailing whitespaces removed, you can use 
a combination of find and sed commands. For example, the following 
command deletes trailing whitespaces in all *.java  files recursively found in 
the current directory as well as all its sub-directories.
xterm
T__ $ find . -name "*.java"  -type f -printO | xargs -0 sed
-i 's/[[:space:]]
*$//'
Method Two: Vim
Vim text editor is able to highlight and trim whitespaces in a file as well. 
To highlight all trailing whitespaces in a file, open the file with Vim editor 
and enable text highlighting by typing the following in Vim command line 
mode.
:set hlsearch

Then search for trailing whitespaces by typing: 
/s+$
This will show all trailing spaces and tabs found throughout the file.
* Get an unsigned byte from the current position of the ByteBuffer 
*
* @param bb ByteBuffer to get the byte from
* @return an unsigned byte contained in a short
*/
public static short getllnsignedByte(ByteBuffer bb) {|
return ((short) (bb.getO & (short) ));
Highlight trailing whitespaces in Vim
* Get an unsigned byte from the specified offset in the ByteBuffer 
*
* @param bb ByteBuffer to get the byte from
* @param offset the offset to get the byte from
* (areturn an unsigned byte contained in a short
*/
public static short getUnsignedByte(ByteBuffer bb, int offset) {| 
return ((short) (bb.get(offset) & (short) ));
}
* Put an unsigned byte into the specified ByteBuffer at the current
* position
18,3 
8%
Then to clean up all trailing whitespaces in a file with Vim, type the 
following Vim command.
:%s/s+$//
This command means substituting all whitespace characters found at the end 
of the line (s+$) with no character.

Linux Tutorials
How to compress PNG files on
Linux
PNG (short for "Portable Network Graphics") is a raster image format 
designed to replace limitations of GIF image format in terms of data 
compression and color precision. The size of a PNG image file can vary 
significantly based on several factors including color depth, interlacing, 
precompression filter, compressor used, etc.
In this tutorial, I will describe how to compress a PNG image file losslessly 
on Linux. If you want to compress JPEG images, refer to this tutorial 
instead.
optipng is a Linux command-line utility that performs various optimizations on 
PNG image files, including size compression, metadata recovery and 
integrity checks, etc. Using optipng, you can compress PNG files losslessly.
Install optipng on Linux
To install optipng on Ubuntu or Debian:
xterm
$ sudo apt-get install optipng
___^^Hto install optipng on CentOS or RHEL, first set up Repoforge 
repository on your system, and then run:
xterm
lb_ $ sudo yum install optipng
L Ito install optipng on Fedora, simply run:
xterm
T $ sudo yum install optipng

Compress PNG Image with optipng
Once you have installed optipng, you can compress a PNG image file as 
follows.
xterm
T__ $ optipng -o 7 input.png
optipng -o 7 input.png
OptiPNG 0.6.4: Advanced PNG optimizer. Copyright (C) 2001-2010 Cosmin 
Truta.
** Processing: input.png
1600x1200 pixels, 4x8 bits/pixel, RGB+alpha Reducing image to 3x8
bits/pixel, RGB Input IDAT size = 608523 bytes
Input file size = 609539 bytes
Trying:
zc = 9 zm = 9 zs = 0 f = 0 IDAT size = 523671
zc = 9 zm = 8 zs = 0 f = 0 IDAT size = 523639
zc = 9 zm = 9 zs = 0 f = 4 IDAT size = 523579
Selecting parameters:
zc = 9 zm = 9 zs = 0 f = 4 IDAT size = 523579

Output IDAT size = 523579 bytes (84944 bytes decrease)
Output file size = 523707 bytes (85832 bytes = 14.08% decrease)
In the above, -o option specifies the optimization level (0 to 7). The higher the 
level is, the more trials optipng will run through, resulting in a smaller image.
Linux Tutorials

How to fix "lineno.sty not found" 
Latex error on Linux
Question: When I tried to compile a LaTex file on Linux, I am getting 
"LaTeX Error: File 'lineno.sty' not found" error. How can I fix this error? 
The LaTex package lineno.sty provides LaTex styles for producing line 
numbers on paragraphs. To fix this LaTex error, you need to install the 
lineno.sty package, which is part of the Tex Live Humanity packages. They 
contain LaTex styles for law, linguistics, social sciens and humanities. 
Follow the instructions below to install Tex Live Humanity packages on 
Linux.
For Ubuntu, Debian or Linux Min:
term
lb_ $ sudo apt-get install texlive-humanities
On Fedora:
term
lb_ $ sudo yum install texlive-collection-humanities
Linux Tutorials
How to connect your LAN to
Amazon Virtual Private Cloud
If you have a lot of your services hosted in the Amazon AWS public cloud, 
and are looking for ways to access your leased AWS resources in a secure 
way, this article is for you. Initially you started out accessing Amazon AWS

over the public Internet. Over time, you have migrated more and more 
services to your AWS Virtual Private Cloud (VPC) using the architecture of 
your choice - multi-tiered servers, load balancers, auto-scaled, managed 
database, hosting services internal to your organization, and so on. Now you 
realize that routing all your traffic to and from AWS over the public Internet 
is no longer viable because of security concerns and/or company policies. 
Moreover, it makes more sense to access services internal to your company 
using private IP addresses.
Amazon AWS provides multiple options if you want to access your Amazon 
VPC's private IP address space directly from your LAN. All these options are 
secure as the traffic either goes over an encrypted tunnel or over an isolated 
dedicated connection.
In a nutshell, you have the following two options to seamlessly and securely 
connect your LAN to Amazon VPC.
1. Virtual Private Network (VPN)
2. AWS Direct Connect
We will be exploring both options in details in this article.
Option 1: VPN Tunnels
In most cases, the easiest option would be to connect your LAN to your 
Amazon VPC via a site-to-site VPN tunnel, which would carry encrypted 
traffic between your LAN and Amazon VPC. Personally, I consider VPN 
tunnels as one of the safest methods in routing traffic over public networks as 
there are multiple measures in place against eavesdropping and spoofing, 
such as IPSec, SSL, periodic renegotiation of keys, peer verification, message 
integrity verification, periodic re-establishment of the tunnel via new keys, 
and so on.
In Amazon AWS, there are several ways to hook up your LAN with Amazon 
VPC via VPN tunnels.
Option 1.1: Using a VPN Capable Instance in AWS
You can easily set up an instance within your Amazon VPC, that can be 

configured as a VPN endpoint. On-premise VPN endpoint 
(firewall/router/server) on your LAN will then connect to this instance via a 
site-to-site VPN tunnel. When preparing a VPN capable instance, you can 
choose either of the following options.
Open source VPN software: You can launch a regular Linux instance and 
turn it into a VPN server by settting up open-source VPN software on it. 
Popular choices of VPN software include Openswan, OpenVPN and 
StrongSwan.
Virtual router/firewall appliance: Alternatively you can create a 
VPNcapable virtual router appliance with necessary VPN capabilities. After 
launching a virtual router, you can configure it as a VPN endpoint within 
your Amazon VPC. Example of such virtual appliances are Cisco CSR or 
Cisco ASA which come with enterprise-class VPN capabilities. Similar 
virtual appliances from Checkpoint, Sonicwall and Sophos are also available.
As far as cost is concerned, the only change that you would incur is the 
standard instance usage charge and data transfer charge. Please keep in mind 
that some instance types have some additional charges for running software, 
e.g., RHEL, Cisco (other than BYOL).

A VPN tunnel can be set up in either of the following two modes.
Policy-based VPN: A tunneling policy defines what kind of traffic will go 
over the tunnel. Typically the traffic between your LAN's subnet and 
Amazon VPC's subnet is sent via the tunnel. In policy based VPN, if the 
tunnel remains idle over an extended period of time due to the absence of 
traffic matched with the policy, the tunnel might not renegotiate keys 
automatically and get torn down. It is thus recommended to have a 
router/gateway send keepalive packets or probe packets periodically over the 
tunnel.
Route-based VPN: In a route based VPN, both VPN endpoints in the LAN 
and VPC have virtual tunnel interfaces configured (e.g., tunnel0, tunnel1), 
and assigned IP addresses from the same subnet. The tunnel then remains up 
as long as the the IKE and IPSec negotiations are successful, whether or not 
there is traffic over the tunnel. VPN traffic is treated as regular routed traffic. 
For example, you can add static routes for the VPN traffic, or even configure 
something dynamic like OSPF or BGP.
There are a couple of things to note when operating a VPN-capable instance.
Depending on your country, your ISP may be filtering VPN traffic. Always 
use an "elastic" IP address with your instance. This ensures that the IP 
address will not change due to a stop/start of an instance. Depending on your 
configuration, the port used by the tunnel needs to be opened in your AWS 
security group and VPC ACL. Common VPN ports are:
Description
ESP (Encapsulated Security Protocol)
AH (Authentication Header)
Key negotiation and traffic In case NAT-T is configured 
Protocol Port IP (Neither TCP or50UDP) 
IP (Neither TCP or51UDP)
UDP 500 UDP 4500
Option 1.2: Using AWS Managed VPN Gateway

The second option for connecting your LAN to Amazon VPC via a VPN 
tunnel is to leverage AWS managed VPN gateways. If you choose this 
option, you no longer need to maintain VPN endpoints in your Amazon VPC, 
and hence there is no need to run a VPN-capable instance.
The AWS VPN gateway is officially called a Virtual Private Gateway 
(VGW). AWS will provide two public IP addresses for each VGW, and you 
can use these IP addresses to set up two separate VPN tunnels for your 
Amazon VPC. It is up to you how you can use these tunnels (e.g., for load 
balancing and high availability).
The VGW is then connected with one or more routers on your premises, 
called Customer Gateway (CGW). For example, you may want to connect 
two or more branch offices to the same Amazon VPC. Alternatively, you can 
connect one customer router to multiple VGWs from different Amazon 
VPCs. For example, you may want to connect your LAN to five different 
VPCs.
If you are considering using AWS Managed VPN Gateway, there are a 
couple of things you should keep in mind.
Each Amazon VPC supports only one VGW.
You can connect more than one customer router (your own router) to a single 
VGW.
You can connect one customer router with multiple VGWs. VGW supports 
NAT traversal, multiple encryption options and DiffieHellman key exchange 
scheme.
VGW supports both policy based and route based VPN.
VGW settings cannot be changed once the VGW has been activated. 
However, you can replace a VGW anytime when you want to make a change. 
In that case, be ready to reconfigure the IP addresses at your router as the IP 
addresses of the VGW will change.
AWS supports a pair (one inbound and one outbound) of security 
associations per tunnel. This becomes especially important for firewalls (e.g., 
Cisco ASA) that create a security association for each LAN subnet.

Option 2: AWS Direct Connect
The second option for connecting your LAN to Amazon VPC is using AWS 
Direct Connect. In this case, you would connect your LAN to AWS data 
center over dedicated fiber optic cables. This option is particularly popular 
for enterprises who have their own infrastructure located in close proximity 
to AWS data centers. Some enterprises choose to lease fiber optic backbone 
from service providers to connect to AWS via AWS Direct Connect.
In case you are considering AWS Direct Connect, you should keep the 
following factors in mind.
You will be connecting to your Amazon VPC using dedicated network 
connections (e.g., leased or dedicated fiber optic cables), not via VPN tunnels 
created over the public Internet. For some enterprises, such nonshared 
dedicated connections are a security requirement.
The throughput of your direct connect link solely depends on owned/leased 
fiber optic connectivity and its bandwidth.
AWS does not provide physical connectivity as a service. The customer is 

responsible for arranging and maintaining fiber optic connectivity to AWS 
'Meet Me Room' (MMR) in the data center.
1000BASE-LX or 10GBASE-LR connections over singlemode fiber is 
supported. The endpoint on your premises should support BGP and 802.1q 
VLANs.
AWS Direct Connect Types: Private and Public
You can choose to use a private or public direct connect. A private direct 
connect is used when both your VPC and local LAN are using private IP 
addresses. You can use any private AS number along with private IP address 
space for BGP peering. The private direct connect link would advertise only 
the VPC's private IP address range.
If you have a pool of public IP addresses in your LAN which you want to 
hook up with your VPC over the direct connect link, you would need to use 
public direct connect peering. For example, you could be a telecom provider 
with a

/8 public IP space, or have a content delivery network that has a /21 public IP 
prefix. The AS number that you would use during BGP peering must be a 
public AS number allocated by the Internet Registry with your public IP 
prefix.
Procedure for Ordering AWS Direct Connect
When you order AWS Direct Connect, you go through the following steps. 
Step One: Physical Connectivity
AWS issues a Letter of Authorization (LOA) and Connecting Facility 
Assignment (CFA) to you, containing the port details so you know where to 
connect the fiber. The LOA-CFA is valid for 90 days. In case it expires, you 
have to request for the LOA-CFA to be reissued. You need the LOA when 
you/your partner completes the connectivity at the AWS MMR. Typically, a 
physical connection is established via fiber optic cables between AWS 
designated ports and the router interfaces on your premises.
Step Two: Virtual Interfaces
Once the physical link is complete, you can create one or more virtual 
interfaces in your AWS Direct Connect Console. When creating a direct 
connect virtual interface, you would have to provide the following 
information.
/30 peering IP block, typically provided by the customer. Depending on the 
use case, AWS might also provide a peering IP block.
VLAN ID for the virtual interface.
Based on the provided information, a virtual interface is created. You can 
now access the downloadable configuration template that you will find 
helpful in configuring the virtual interface at your own router. Ideally, you 
would configure a virtual interface with an IP address in the /30 range and the 
VLAN of the subinterface.
Step Three: BGP Peering
Once the point to point connectivity using the virtual interface has been 

created, you can move forward with the BGP peering. Based on whether you 
have chosen public or private direct connect, you would use a private or a 
public AS number for BGP peering.
Once the BGP peering is up, you can start advertising your LAN prefixes 
towards AWS. You can advertise public prefixes as long as you have 
obtained the prefix from an RIR. You cannot advertise private prefixes on a 
public BGP peering.
In case you have a private BGP peering over the direct connect link, you can 
advertise your private prefixes towards AWS. Traffic towards public prefixes 
will always go via the Internet gateway, even if you advertise those prefixes 
over the private direct connect link.
In a public peering, AWS will advertise the AWS public prefixes of that 
region through BGP. In case of private peering, AWS will advertise only the 
private CIDR of the Amazon VPC.
There are a couple of things to note when setting up AWS Direct Connect.
The physical interface of your router should not have any IP address. The IP 
addresses are to be assigned to virtual sub-interfaces.
In case you are using dual core fiber and your physical link is not coming up 
because of too much loss in the optical signal, try swapping the TX and RX 
cables to your SFP.
BGP needs TCP port 179 open to establish the session.
You might run into asymmetric routing in case you advertise identical 
prefixes over both BGP peering.
Adding Routes to VPC Route Tables
To route traffic between your LAN and your Amazon VPC, necessary routes 
need to be added to the VPC's route table. This applies to both VPN and 
AWS Direct Connect cases. Here are two options to populate the VPC route 
tables.
Routing

Static routing
Typical Route Table Entry Your LAN subnets are statically routed with the 
next hop set to
software VPN instance, the VGW or the AWS Direct Connect link. 
Characteristics
Easy to configure when you have a few subnets in your LAN. Harder to 
manage when you have a lot of LAN subnets.
Dynamic routing Your CGW has BGP peering with the VPN 
endpoints/Direct Connect router. Routes are exchanged over BGP peering. 
Highly scalable, adaptable and less prone to human error.
A VGW can be set to propagate any routes that it learns from the CGW 
throughout the VPC. This comes in very handy in case you have many 
subnets in your VPC that have their own route tables.
Conclusion
To summarize, you have many options when you want to connect your LAN 
or servers to Amazon AWS. The cost and complexity of the connection 
depends on how you want your traffic to be routed and how you plan for 
disaster recovery. If you choose a VPN tunnel option, you can either maintain 
a VPN-capable router instance within Amazon VPC for a single VPN tunnel, 
or have multiple VPN tunnels with AWS-managed VPN gateway. If the cost 
is not limitation, you can choose high performance dedicated direct connect 
links. In short, the flexibility in designing and implementing a connectivity 
that matches your specific needs is one of the primary reasons why many of 
us love working with AWS.
Linux Tutorials
How to install tcpping on Linux
A common way to measure network latency to a remote host is by using ping 

utility which uses ICMP echo request and reply packets. In some cases, 
however, ICMP traffic is blocked by firewalls, which renders ping utility 
useless with hosts behind restrictive firewalls. In such case, you will need to 
rely on layer-3 measurement tools that use TCP/UDP packets since these 
layer-3 packets are more likely to bypass common firewall rules.
One such layer-3 measurement tool is tcpping. To measure latency, tcpping takes 
advantage of so-called half-open connection technique, based 
on TCP three-way handshake. That is, it sends a TCP SYN packet to a 
remote
host on a port number (80 by default). If the remote host is listening on the 
port, it will respond with TCP ACK packet. Otherwise, it will respond with 
TCP RST packet. Either way, tcpping can measure round-trip-time (RTT) 
delay of a remote host, by timing outgoing SYN packet and incoming ACK 
(or
RST) packet.
The same half-open connection technique is already implemented by 
tcptraceroute tool. So tcpping simply relies on tcptraceroute to perform latency 
measurement.
Install tcpping on Linux
tcpping is implemented as a shell script, and this script replies on external tools 
to perform and report RTT measurements. Thus, in order to install 
tcpping, you first need to install these prerequisites first.
Prerequisite #1: tcptraceroute
To install tcptraceroute on Ubuntu or Debian:
xterm
$ sudo apt-get install tcptraceroute
_J^Hto install tcptraceroute on CentOS or RHEL, first set up RepoForge on 
your system, and then run:
xterm
T $ sudo yum install tcptraceroute

Prerequisite #2: bc
Another tool used by tcpping is GNU bc, which comes pre-installed on all 
major Linux distributions. However, if you are running tcpping in a minimal 
Linux runtime environment (e.g., Docker container, AWS minimal image 
AMI),
bc may not be pre-installed. In such case, you need to install bc yourself.
To install bc on Debian based Linux:
xterm
T $ sudo apt-get install bc
_J^®To install bc on Red Hat based Linux:
term
$ sudo yum install bc
Installation of tcpping
After installing these prerequisite tools, finally go ahead and download tcpping 
from the official source.
I_____ ^^Ixterm
$ cd /usr/bin
$ sudo wget http://www.vdberg.org/~richard/tcpping $ sudo chmod 755 
tcpping
Use tcpping to Measure Latency
To measure network latency by using tcpping, you can use the following 
format.

tcpping [-d] [-c] [-r sec] [-x count] ipaddress [port]
- d : print timestamp before each result.
- c: use columned output for easy parsing.
- r: interval in seconds between consecutive probes (1 second by
default).
- x: repeat n times (unlimited by default).
[port]: target port (80 by default).
Note that you need root privilege to run tcpping as it needs to invoke the 
privileged tcptraceroute command.
For any target web server where port 80 is open, you can measure its RTT 
delay with tcpping as follows.
B__xterm
$ sudo tcpping www.cnn.com
seq 0: tcp response from 157.166.240.13 [open] 82.544 ms
seq 1: tcp response from 157.166.241.10 [open] 80.771 ms
seq 2: tcp response from 157.166.241.11 [open] 80.838 
ms
seq 3: tcp response from 157.166.241.10 [open] 80.145 ms
seq 4: tcp response from 157.166.240.11 [open] 86.253 ms
For any arbitrary remote host, you need to make sure port 80 (or any other 
port) is open before running tcpping. To check if a remote TCP port is open, 
you can use nc command as follows.
term
__ $ nc -vn <ip-address> <port-number>

Linux Tutorials
How to convert image, audio and 
video formats on Ubuntu
If you need to work with a variety of image, audio and video files encoded in 
all sorts of different formats, you are probably using more than one tools to 
convert among all those heterogeneous media formats. If there is a versatile 
all-in-one media conversion tool that is capable of dealing with all different 
image/audio/video formats, that will be awesome.
Format Junkie is one such all-in-one media conversion tool with an extremely 
user-friendly GUI. Better yet, it is free software! With Format Junkie, you 
can convert image, audio, video and archive files of pretty much all the 
popular formats simply with a few mouse clicks.
Install Format Junkie on Ubuntu 12.04, 12.10 and 
13.04
Format Junkie is available for installation via Ubuntu PPA format-junkie- 
team. This PPA supports Ubuntu 12.04, 12.10 and 13.04. To install Format 
Junkie on one of those Ubuntu releases, simply run the following.
term
$ sudo add-apt-repository ppa:format-junkie 
team/release
$ sudo apt-get update
$ sudo apt-get install formatjunkie
$ sudo ln -s

/opt/extras.ubuntu.com/formatjunkie/formatjunkie /usr/bin/formatjunkie
Install Format Junkie on Ubuntu 13.10
If you are running Ubuntu 13.10 (Saucy Salamander), you can download and 
install .deb package for Ubuntu 13.04 as follows. Since the .deb package for 
Format Junkie requires quite a few dependent packages, install it using
gdebi deb installer.
On 32-bit Ubuntu 13.10:
term
$ wget https://launchpad.net/~format-junkie 
$ wget https://launchpad.net/~format-junkie
1~raring0.2_i386.deb
$ sudo gdebi formatjunkie_1.07-1~raring0.2_i386.deb $ sudo ln -s
/opt/extras.ubuntu.com/formatjunkie/formatjunkie /usr/bin/formatjunkie
On 64-bit Ubuntu 13.10:
term
$ wget https://launchpad.net/~format-junkie 
$ wget https://launchpad.net/~format-junkie
1~raring0.2_amd64.deb
$ sudo gdebi formatjunkie_1.07-1~raring0.2_amd64.deb $ sudo ln -s
/opt/extras.ubuntu.com/formatjunkie/formatjunkie /usr/bin/formatjunkie
Install Format Junkie on Ubuntu 14.04 or Later

The currently available official Format Junkie .deb file requires libavcodecextra- 
53 which has become obsolete starting from Ubuntu 14.04. Thus if you want 
to install Format Junkie on Ubuntu 14.04 or later, you can use the following 
third-party PPA repositories instead.
$ sudo add-apt-repository ppa:jon-severinsson/ffmpeg $ sudo add-apt- 
repository ppa:noobslab/apps
$ sudo apt-get update
$ sudo apt-get install formatjunkie
File Conversion with Format Junkie
To start Format Junkie after installation, simply run:
term
Hl_ $ formatjunkie
1. Convert audio, video, image and archive formats with 
Format Junkie
The user interface of Format Junkie is pretty simple and intuitive, as shown 
below. To choose among audio, video, image and iso media, click on one of 
four tabs at the top. You can add as many files as you want for batch 
conversion. After you add files, and select output format, simply click on
Start Converting button to convert.

Format Junkie supports conversion among the following media formats:
Audio: mp3, wav, ogg, wma, flac, m4r, aac, m4a, mp2.
Video: avi, ogv, vob, mp4, 3gp, wmv, mkv, mpg, mov, flv, webm. Image: 
jpg, png, ico, bmp, svg, tif, pcx, pdf, tga, pnm.
Archive: iso, cso.
2. Subtitle encoding with Format Junkie

Besides media conversion, Format Junkie also provides GUI for subtitle 
encoding. Actual subtitle encoding is done by mencoder. In order to do subtitle 
encoding via Format Junkie interface, first you need to install MEencoder.
xterm
Bl_ $ sudo apt-get install mencoder
__B^^B'rhen click on Advanced tab on Format Junkie. Choose AVI/subtitle 
files to use for encoding, as shown below.

Overall, Format Junkie is an extremely easy-to-use and versatile media 
conversion tool. One drawback, though, is that it does not allow any sort of 
customization during conversion (e.g., bitrate, fps, sampling frequency, 
image quality, size). So this tool is recommended for newbies who are 
looking for an easy-to-use simple media conversion tool.

Linux Tutorials
What are some obscure but useful
Vim commands
If my latest post on the topic did not tip you off, I am a Vim fan. So before 
some of you start stoning me, let me present you a list of obscure Vim 
commands. What I mean by that is: a collection of commands that you might 
have not encountered before, but that might be useful to you. As a second 
disclaimer, I do not know which commands you might know and which one 
you find useful. So this list really is a collection of relatively less known Vim 
commands, but which can still probably be useful.
Saving a File and Exiting
I am a bit ashamed of myself for that one, but I only recently learned that the 
command
:x 
is equivalent to:
:wq 
which is saving and quitting the current file.
Basic Calculator
While in insert mode, you can press Ctrl+r then type = followed by a simple 
calculation. Press enter, and the result will be inserted in the document. For

example, try:
Ctrl+r '=2+2' ENTER
inserted in the document.
And 4 will be
Finding Duplicate Consecutive Words
When you type something quickly, it happens that you write a word twice in 
a row. Just like this this. This kind of error can fool anyone, even when 
rereading yourself. Hopefully, there is a simple regular expression to prevent 
this. Use the search (/ by default) and type:
()_S*1
This should display all the duplicate words. And for maximum effect, don't 
forget to place: 
set hlsearch

in your ~/.vimrc file to highlight all search hits.
Abbreviations
Probably one of the most impressive tricks, you can define abbreviations in 
Vim, which will replace what you type with somethig else in real time. The 
syntax is the following:
:ab [abbreviation] [what to replace it with]
The generic example is: 
:ab asap as soon as possible
Which will replace asap with "as soon as possible" as you write.

Save a file that you forgot to open as root
This is maybe an all time favorite in the forums. Whenever you open a file 
that you do not have permission to write to (say a system configuration file 
for example) and make some changes, Vim will not save them with the 
normal command: :w
Instead of redoing the changes after opening it again as root, simply run: :w 
!sudo tee %
Which will save it as root directly.
Crypt Your Text on the Go
If you do not want someone to be able to read whatever is on your screen, 
Vim has the built in option to ROT13-encode your text with the following 
command: 
ggVGg?

gg for moving the cursor to the first line of the Vim buffer, V for entering 
visual mode, and G for moving the cursor to the last line of the buffer. So 
ggvg will make the visual mode cover the entire buffer. Finally g? applies 
ROT13 encoding to the selected region.
Notice that this should be mapped to a key for maximum efficiency. It also 
works best with alphabetical characters. And to undo it, the best is simply to 
use the undo command: u
Auto-completion
Another one to be ashamed of, but I see a lot of people around me not 
knowing it. Vim has by default an auto-completion features. Yes it is very 
basic, and can be enhanced by plugins, but it can still help you. The process 
is simple. Vim can try to guess the end of your word based on the word you 
wrote earlier. If you are typing the word "compiler" for the second time in the 
same file for example, just start typing "com" and still in insertion mode, 
press
Ctrl+n to see Vim finish your word for you. Simple but handy.
Look at the Diff Between Two Files

Probably a lot of you know about vimdiff command, which allows you to open
Vim in split mode and compare two files with the syntax:
term
Hl_ $ vimdiff [filel] [file2]
But the same result is achievable with the Vim command:
:diffthis
First open your initial file in Vim. Then open the second one in split mode 
with: 
:vsp [file2]
Finally launch: 
:diffthis 
in the first buffer, switch buffer with Ctrl+w and type: 
:diffthis 
again.
The two files will then be highlighted with focus on their differences. 
To turn the diff off, simply use: :diffoff
Revert the Document in Time

Vim keeps track of the changes you make to a file, and can easily revert it to 
what it was earlier in time. The command is quite intuitive. For example: 
:earlier 1m 
will revert the document to what it was a minute ago.
Note that you can inverse this with the command: :later
Delete Inside Markers
Something that I always wanted to be comfortable doing when I started using 
Vim: easily delete text between brackets or parenthesis. Go to the first marker 
and simply use the syntax:
di[marker]
So for example, deleting between parenthesis would be: 
di( 
once your cursor is on the first parenthesis. For brackets or quotation marks, 
it would be:
di{ 
and

di"
Delete Until a Specific Maker
A bit similar to deleting inside a marker but for different purpose, the 
command:
dt[marker]
will delete everything in between your cursor and the marker (leaving it safe) 
if the marker is found on the same line. For example: 
dt.
will delete the end of your sentence, leaving the . intact.
Turn Vim into a Hex Editor
This is not my favorite trick, but some might find it interesting. You can 
chain Vim and the xxd utility to convert the text into hexadecimal with the 
command:
:%!xxd

And similarly, you can revert this with: 
:%!xxd -r
Place the Text Under Your Cursor in the Middle of 
the Screen
Everything is in the title. If you want to force the screen to scroll and place 
whatever is under your cursor in the middle, use the command:
zz
in visual mode.
Jump to Previous/Next Position
When editing a very big file, it is frequent to make changes somewhere, and 
jump to another place right after. If you wish to jump back simply, use: 
Ctrl+o

to go back to where you were.
And similarly:
Ctrl+i
will revert such jump back.
Render the Current File as a Web Page
This will generate an HTML page displaying your text, and show the code in 
a split screen: 
:%Tohtml

Very basic but so fancy.
To conclude, this list was assembled after reading some various forum 
threads and the Vim Tips wiki, which I really recommend if you want to 
boost your knowledge about the editor.
If you know any Vim command that you find useful and that you think most 
people do not know about, feel free to share it in the comments. As said in 

the introduction, an "obscure but useful" command is very subjective, but 
sharing is always good.
Linux Tutorials
How to configure peer-to-peer VPN 
on Linux
A traditional VPN (e.g., OpenVPN, OpenSwan, PPTP) is composed of a 
VPN server and one or more VPN clients connected to the server. When any 
two VPN clients talk to each other, the VPN server needs to relay VPN traffic 
between them. The problem of such a hub-and-spoke type of VPN topology 
is that the VPN server can easily become a performance bottleneck as the 
number of connected clients increases. The centralized VPN server is also a 
single point of failure in a sense that if the VPN server goes down, the entire 
VPN is no longer accessible to any VPN client.
Peer-to-peer VPN (or P2P VPN) is an alternative VPN model that addresses 
these problems of the traditional server-client based VPN. In a P2P VPN, 
there is no longer a centralized VPN server. Any node with a public IP 
address can bootstrap other nodes into a VPN. Once connected to a VPN, 
each node can communicate with any other node in the VPN directly, without 
going through an intermediary server node. When any one node goes down, 
the rest of nodes in the VPN are not affected. Inter-node latency/bandwidth 
and VPN scalability naturally improve in such a setting, which is desirable if 
you want to use a VPN for multi-player gaming or file sharing among many 
friends.
There are several open-source implementations of P2P VPN, such as Tinc, 
peerVPN, and n2n. In this tutorial, I am going to demonstrate how to 
configure a peer-to-peer VPN using n2n on Linux.
n2n is an open-source (GPLv3) software allowing you to construct an 
encrypted layer-2/3 peer-to-peer VPN among users. The VPN created by 
n2n is NAT-friendly, which means that two users behind different NAT routers 

can directly talk to each other over the VPN. n2n supports symmetric NAT 
type which is the most restrictive form of NAT. For that, the VPN traffic of 
n2n is encapsulated by UDP.
A n2n VPN is composed of two kinds of nodes: edge node and super node. An 
edge node is a computer which is connected to a VPN, potentially from 
behind a NAT router. A super node is a computer with a publicly reachable 
IP address, which assists with initial signaling for NATed edges. To create a 
P2P VPN among users, we need at least one super node.
Preparation
In this tutorial, I am going to set up a P2P VPN using three nodes: one super 
node, and two edge nodes. The only requirement is that edge nodes be able to 
ping the IP address of the super node. It does not matter whether the edge 
nodes are behind NAT routers or not.
Install n2n on Linux

To construct a P2P VPN using n2n, you need to install n2n on every edge node 
as well as super node.
Due to its minimal dependency requirements, n2n can be built easily on most 
Linux platforms.
For Debian, Ubuntu or Linux Mint:
To install n2n on Debian-based system: 
xterm
$ sudo apt-get install subversion build-essential libssl-dev
$ svn co https://svn.ntop.org/svn/ntop/trunk/n2n $ cd n2n/n2n_v2
$ make
$ sudo make install
For Fedora, CentOS or RHEL:
To install n2n on Red Hat-based system:
term
$ sudo yum install subversion gcc-c++ openssl-devel $ svn co 
https://svn.ntop.org/svn/ntop/trunk/n2n $ cd n2n/n2n_v2
$ make
$ sudo make install
Configure a P2P VPN with n2n
As mentioned before, we need to set up at least one super node which acts as 
an initial bootstraping server. We assume that the IP address of the super 
node is 1.1.1.1.

Configure Super Node
On a computer which acts as a super node, run the following command. The 
"-l <port>" specifies the listening port of the super node. No root privilege is 
required to run supernode.
term
$ supernode -l 5000
Configure Edge Nodes
On each edge node, use the following command to connect to a P2P VPN.
The edge daemon will be running in the background.
Edge node #1:
xterm
$ sudo edge -d edge0 -a 10.0.0.10 -c mynetwork -u 1000
-g 1000 -k password -l 1.1.1.1:5000 -m
ae:e0:4f:e7:47:5b
I l.dge node #2:
xterm
$ sudo edge -d edge0 -a 10.0.0.11 -c mynetwork -u 1000
-g 1000 -k password -l 1.1.1.1:5000 -m
ae:e0:4f:e7:47:5c
Here are some explanations on the command-line.
The " -d <name>" option specifies the name of a TAP interface being created by 
edge command.
The "-a <ip-address>" option defines (statically) the VPN IP address to be 
assigned to the TAP interface. If you want to use DHCP, you need to set up a 
DHCP server on one of edge nodes, and use "-a dhcp:0.0.0.0" option instead.

The "-c <community-name>" option specifies the name of a VPN group (with a 
length of up to 16 bytes). This option is used to create multiple VPNs among 
the same group of nodes.
The -u and -g options are used to drop root priviledge after creating a TAP 
interface. The edge daemon will run as the specified user/group ID. The "-k 
<key-string>" option specifies a twofish encryption key string to be used. If you 
want to hide a key-string from the command-line, you can define the key in 
n2n_key environment variable.
The "-l <ip-address:port>" option specifies super node's listening IP address and 
port number. For redundancy, you can specify up to two different super 
nodes (e.g., "-l <supernode A>" "-l <supernode B>").
The "-m <mac-address>" assigns a static MAC address to a TAP interface. 
Without this, edge command will randomly generate a MAC address. In fact, 
hardcoding a static MAC address for a VPN interface is highly 
recommended. Otherwise, in case you restart edge daemon on a node, ARP 
cache of other peers will be polluted due to a newly generated MAC addess, 
and they will not send traffic to the node until the polluted ARP entry is 
evicted.

[dev@centos7 ~]$ ifconfig
edge©: flaqs=4163<UP,BROADCAST,RUNNING, MULTICAST mtu 1400 
inet 10.0.0.10 netmask 255.255.255.0 broadcast 10.0.0.255 
inet6 fe80::ac58:b6ff:fea5:529e prefixlen 64 scopeid 0x20<link> 
ether ae:58:b6:a5:52:9e txqueuelen 500 (Ethernet) 
RX packets 87 bytes 10705 (10.4 KiB) 
RX errors 0 dropped 0 overruns 0 frame 0 
TX packets 101 bytes 13336 (13.0 KiB)
TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
enp0s3: flags=4163<UP,BROADCAST,RUNNING, MULTICAST mtu 1500 
inet 10.0.2.15 netmask 255.255.255.0 broadcast 10.0.2.255 
inet6 fe80::a0O:27ff:feca:4ed4 prefixlen 64 scopeid 0x20<link> 
ether 08:00:27:ca:4e:d4 txqueuelen 1000 (Ethernet) 
RX packets 75023 bytes 61626532 (58.7 MiB) 
RX errors 0 dropped 0 overruns 0 frame 0 
TX packets 18380 bytes 1133169 (1.0 MiB)
TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
At this point, you should be able to ping from one edge node to the other 
using their VPN IP addresses.
Troubleshooting
1. You are getting the following error while invoking edge daemon.
n2n[4405]: ERROR: ioctl() [Operation not permitted][-1]
Be aware that edge daemon requires superuser privilege when creating a TAP 
interface. Thus make sure to use root privilege or set SUID for edge command. 
You can always use -u and -g option to drop root privilege afterwards.
Conclusion 

n2n can be a quite practical free VPN solution for you. You can easily 
configure a super node from your own home network or by grabbing a 
publicly addressable VPS instance from cloud hosting. Instead of placing 
sensitive credentials and encryption keys in the hands of a third-party VPN 
provider, you can use n2n to set up your own low-latency, high bandwidth, 
scalable P2P VPN among your friends.
What is your thought on n2n? Share your opinion in the comment.
Linux Tutorials
How to log in to system VMs in 
CloudStack
In CloudStack, there are two system VMs (Console Proxy VM and 
Secondary Storage VM) that are automatically created by CloudStack for 
each zone. Console Proxy VM (CPVM) provides console access to tenant's 
guest VMs, while Secondary Storage VM (SSVM) downloads and maintains 
VM templates used by tenants. While setting up CloudStack, you may 
experience problems with either of these system VMs.
For example, for any reason SSVM cannot reach the Internet, or cannot 
download a default system template from http://download.cloud.com. In that case, 
logging in to SSVM helps diagnose the problem.
You can log in to a system VM via SSH from within the compute node that 
runs the VM, as follows.
In the system VMs created by CloudStatck, sshd listens on the link-local 
address (in 169.254.0.0/16 subnet) only, and no other interface. Therefore, you 
first need to find out the link-local address assigned to a system VM in 
question. To find out the link-local address information of the system VM, go 
to Infrastructure menu of CloudStack's web-based management interface.
Then, SSH to the system VM on port number 3922 from within their 

underlying compute node as follows.
I___ xterm
1$ ssh -i /root/.ssh/id_rsa.cloud -p 3922 
_ [email protected]
Linux Tutorials
How to make spreadsheets in a
Linux terminal
If you are on a quest to get rid of your desktop environment and do 
everything from the console, well good luck with that. However, if you are a 
bit more serious, and would like to learn to do a new thing from the terminal, 
what about mastering spreadsheets?
Sure, LibreOffice or any office suite of your choice may do it very well in the 
first place, but sometimes using a pick up truck to go to the kitchen is not the 
most appropriate path. What I mean by that is that it happens that you may 
need to create a quick spreadsheet with just a few simple formulas, but 
nothing so complicated that you need all the power of a traditional office 
suite. Plus, if your system is a bit old, or if you are running a lot of 
applications in the background, you may have to wait a few dozens of 
seconds watching that splash screen load. On the other hand, a spreadsheet 
application in the console is fast, simple, and efficient. Among the magical 
software that will allow you to do that, I propose you sc, anagram for 
spreadsheet calculator.
Install sc on Linux
The sc package is available in the universe repo for Debian-based distros such
as Debian, Ubuntu or Linux Mint, so installation on such systems is easy: 
xterm
lb_ $ sudo apt-get install sc

On Arch Linux, check out the AUR. For other distros, you can
download its source, and build it.
Basic Usage ofsc
As usual, the first screen is always the scariest. But good news: a lot of the 
shortcuts for sc are inspired by those of vim (or I guess it's bad news if you 
only know Emacs). So you can move around with the traditional hjkl keys:
h for left
j for down
k for up
l for left
Or just use the arrow keys if you are not afraid of being burn for heresy. 
The second thing you might want to do in a spreadsheet is to enter some 
numerical value. I was a little thrown off at first because the insertion mode is 
a bit different here. To enter a numerical value in the selected cell press =, 

then your value.
To enter some text instead, type <, then your text. 
If you need to save your work at any time, use: 
P[name].sc 
to dump your work into a .sc file. 
Conversely, 
G[name] 
will retrieve your work.
If you haven't guessed by yourself, the command q serves to quit the program. 
But what we all want is to create some formulas. Just like in any traditional 
spreadsheet, start with = sign and refer to the other cells by their coordinate.
For example, to do the sum of the first two cells of the first row, do: 
=A0+B0

Bl (10 2 0) [A0+B0]
To copy the content of a cell, get the cursor over it and press ma. Then go to 
an empty cell and do ca.
Finally, using the last technique and our earlier example, we can extend the 
formula to add the values of two adjacent cells with:
[#]ca
where [#] is just the number of cells we want to extend the formula to. As you

may have noticed, this is just like in vim, where you do [#][command] to repeat a 
command a certain number of times. In our previous example:
3ca
will extend the addition formula to the next 3 cells in the row, giving them 
the values of bo+co, co+do, and do+eo respectively.
Advance Usage of sc

If you know your vim shortcuts, you will be able to make the most out of sc 
after a few experiments:
Use c to refer to a column. So dc and yc to delete and yank the current column 
respectively.
Use r to refer to a row. Same idea here, dr and yr to delete and yank the current 
row.
The command g to move to a particular line in vim also works in sc with the 
syntax:
g[coordinate of a cell]
For example, gC22 to go straight to the cell C22.
I said earlier that ma was to mark a cell. To be exact, it stores the content of 
the cell into the variable a, that you can then copy with ca. Alternatively, you 
can use any other letter to refer to a different variable: mb, mc, md, etc.
Check out more by typing ? at anytime from the interface.

Which Screen? [a-p, q]|
Overview:
A: 
This overview
B: 
Toggle Options
C: 
Set Options
D: 
Cursor movement commands
E: 
Cell entry and editing commands
F: 
Line Editing
G: 
File commands
H: 
Row and column commands
I: 
Range commands
J: 
Miscellaneous commands
K: 
Variable names/Expressions
L: 
Range functions
M: 
Numeric functions
N: 
String functions
0: 
Financial functions
P: 
Time and date functions
Q: 
Return to main spreadsheet
To conclude, sc is another one of these programs which will make people 
debate over its utility. Is a console spreadsheet calculator obsolete, now that 
our computers are powerful enough to run LibreOffice or whatever? Or on 
the contrary, should we stick to the basics for simple tasks and not waste 
resources?
I also heard good things about oleo, which does sensibly the same thing, but 
never tried it myself. Is it a viable alternative to sc?
Anyway, let us know what you think in the comments.
Linux Tutorials

How to check NetworkManager 
version on Ubuntu
Question: I am trying to set up a VPN tunnel using NetworkManager on my 
Ubuntu Linux, and want to check whether the current version of my 
NetworkManager has native support for this VPN tunnel. How can I check 
the version of NetworkManager installed on my Ubuntu, and how to find out 
NetworkManager versions available on other Ubuntu releases?
NetworkManager is a standard network configuration daemon for Linux, 
whose primary goal is to keep network connectivity active at all time. Unlike 
stationary server environment, typical Linux laptop/desktop can connect to 
the network via more than one physical (wired/wireless) or virtual (VPN, 
VLAN, VxLAN) network interfaces, and can switch from one interface to 
another dynamically, depending on locations and user preferences. That is 
when NetworkManager really makes your life easy by enabling automatic 
connection setup and migration.
To support a variety of network configurations and emerging new VPN 
software, NetworkManager has been regularly updated with new releases 
every now and then. If you want to use NetworkManager to configure a 
custom VPN tunnel, you need to check the NetworkManager version and see 
if NetworkManager provides native support for it.
The following describes two different ways to check NetworkManager 
version on Ubuntu.
Method One
The easiest way to check the version of currently installed NetworkManager 
is to run NetworkManager itself.
For example, to check NetworkManager version on Ubuntu 18.04:
xterm
Hi_ /usr/sbin/NetworkManager --version

1.10.6
Another shortcut is to use nmcli, a command-line-based front-end for 
NetworkManager. nmcli is enclosed in network-manager package, and nmcli version 
is matched with that of NetworkManager.
term
Il_ $ nmcli --version
__ ^^Hnmc.li tool, version 1.10.6
Method Two: rmadison
Sometimes you may want to find out what version of NetworkManager is 
offered by different Ubuntu releases. For example, you are trying to 
configure WireGuard VPN on NetworkManager, and this requires 
NetworkManager version 1.16 or higher. You want to find out which Ubuntu 
releases come with NetworkManager version 1.16+.
For that, you can use a command-line tool called rmadison. This tool queries 
the Debian archieve database remotely and finds out which package version 
is registered for different releases and archtiectures. So by using rmadison, you 
can find out exactly what version of NetworkManager is available on 
different Ubuntu versions.
term
lb_ $ rmadison network-manager

For example, the above output indicates that NetworkManager version 1.16+
is available starting from Ubuntu 20.04 (focal).
Linux Tutorials
How to configure rsyslog client for 
remote logging on CentOS

rsyslog is an open source utility widely used on Linux systems to forward or 
receive log messages via TCP/UDP protocols. rsyslog daemon can be 
configured in two scenarios. Configured as a log collector server, rsyslog 
daemon can gather log data from all other hosts in the network, which are 
configured to send their internal logs to the server. In another role, rsyslog 
daemon can be configured as a client which filters and sends internal log 
messages to either a local folder (e.g. /var/log) or a remote rsyslog server based 
on routing facility.
Assuming that you already have a rsyslog server up and running on your 
network, this guide will show you how to set up a CentOS system to route its 
internal log messages to a remote rsyslog server. This will greatly improve 
your system's disk usage, especially if you don't have a separate large 
partition dedicated for /var directory.
Step One: Install rsyslog on CentOS
On CentOS 6 and 7, rsyslog daemon comes preinstalled. To verify that rsyslog is 
installed on your CentOS system, issue the following command:
xterm
lb_ # rpm -qa | grep rsyslog
# rsyslogd -v

[dev@centos7 -]$ rpm -qa | grep rsyslog 
.-mmjsonparse-7.4.7-6.el7.x86_64
rsyslo -7.4.7-6.el7.x86_64 
[dev@centos7 ~]$ 
[dev@centos7 ~]$ rsyslogd -v 
rsyslogd 7.4.7, compiled with:
FEATUREREGEXP: 
Yes
FEATURELARGEFILE: 
No
GSSAPI Kerberos 5 support: 
Yes
FEATURE DEBUG (debug build, slow code): No 
32bit Atomic operations supported: 
Yes
64bit Atomic operations supported: 
Yes
Runtime Instrumentation (slow code): 
No
uuid support: 
Yes
See http://www.rsyslog.com for more information. 
[dev@centos7 -]$
If for some reason rsyslog daemon is missing on your system, issue the 
following command to install it:
xterm
HI__# yum install rsyslog
Step Two: Configure rsyslog as a Syslog Client
The next step is to transform your CentOS machine into a rsyslog client which 
sends all of its internal log messages to the central remote log server.
To do so, open the main rsyslog configuration file located in /etc path with your 
favorite text editor:
xterm
lb__# nano /etc/rsyslog.conf
After the file is opened for editing, you need to add the following statement at 
the bottom of the file. Replace the IP address with your remote rsyslog server's

IP address.
*.*  @192.168.1.25:514
The above statement tells rsyslog daemon to route every log message from 
every facility on the system to the remote rsyslog server (192.168.1.25) on UDP 
port 514.
If for some reasons you need a more reliable protocol like TCP, and the rsyslog 
server is configured to listen for TCP connections, you must add an 
extra @ character in front of the remote host's IP address as in the below 
excerpt:
*.* @@192.168.1.25:514
Note that you can also replace the IP address of the rsyslog server with its DNS 
name (FQDN).
If you want to forward log messages from a specific facility only, let's say 
kernel facility, then you can use the following statement in your rsyslog 
configuration file.
kern.
*
 @192.168.1.25:514
r ”
Once you have modified the configuration, you need to restart the daemon to 
activate the change:
On CentOS 7 or later:
term

# systemctl restart rsyslog.service
On CentOS 6 or earlier:
term
# service rsyslog restart
In another scenario, let's assume that you have installed an application named 
foobar on your machine, which generates logs to /var/log/foobar.log
file. Now you want to direct only its logs to a remote rsyslog server. This 
can be achieved by loading imfile module in the rsyslog configuration as 
follows.
First load the imfile module. This must be done just once.
module(load="imfile" PollingInterval="5")
Then specify the path to the log file that the imfile module should monitor:
input(type="imfile"
File="/var/log/foobar.log"
Tag="foobar"
Severity="error"
Facility="local7")

Finally, direct local7 facility to the remote rsyslog server: 
local7.
*
 @192.168.1.25:514
Don't forget to restart rsyslog daemon.
Step Three: Enable rsyslog to Auto-start
To automatically start rsyslog client after every system reboot, run the 
following command to enable it system-wide:
On CentOS 7 or later:
_____^^Ixterin
# systemctl enable rsyslog.service
On CentOS 6 or earlier:
xterm
lb_ # chkconfig rsyslog on

Summary
In this tutorial I demonstrated how to turn a CentOS system into rsyslog client 
to force it to send its log messages to a remote rsyslog server. Here I assume 
that the connection between a rsyslog client and rsyslog server is secure (e.g., 
within corporate network protected by a firewall). Under any circumstances 
do not configure a rsyslog client to forward log messages over insecure 
networks or, especially, over the Internet as the syslog protocol is a clear-text 
protocol. For secure transmission, consider encrypting syslog messages using 
TLS/SSL.
Linux Tutorials
How to disable Apport internal 
error reporting on Ubuntu
Question: On Ubuntu desktop, I often encounter a popup window, alerting 
that Ubuntu has experienced an internal error, and asking me to send an error 
report. This is bothering me as it keeps popping up for every application 
crash. How can I turn off the error reporting feature?
Ubuntu desktop comes with Apport pre-installed, which is a system that 
catches applications crashes, unhandled exceptions or any non-crash 
application bugs, and automatically generates a crash report for debugging 
purposes. When an application crash or bug is detected, Apport alerts user of 
the event by showing a popup window and asking the user to submit a crash 
report. You will see messages like the following.
"Sorry, the application XXXX has closed unexpectedly." "Sorry, Ubuntu 
XX.XX has experienced an internal error." "System program problem 
detected."

Sorry, Ubuntu 13.10 has experienced an internal error.
If you notice further problems, try restarting the computer
Q Send an error report to help fix this problem
Ignore future problems of this type
Show Details
Continue
If application crashes are recurring, frequent error reporting alerts can be 
disturbing. Or you may be worried that Apport can collect and upload any 
sensitive information of your Ubuntu system. Whatever the reason is, you 
may want to disable Apport's error reporting feature.
Disable Apport Error Reporting Temporarily
If you want to disable Apport temporarily, use this command:
For Ubuntu 14.10 and Earlier:
term
lb_ $ sudo service apport stop
For Ubuntu 15.04 or Later:
term
$ sudo systemctl stop apport
Note that Apport will be enabled back after you boot your Ubuntu
system.
Disable Apport Error Reporting Permanently

To turn off Apport permanently, edit /etc/default/apport with a text editor, and 
change the content to the following.
enabled=0
Now if you reboot your Ubuntu system, Apport will automatically be 
disabled.
If you think you will never use Apport, another method is to simply remove it 
altogether.
I__xterm
■f_ $ sudo apt-get purge apport
Linux Tutorials
How to convert List to Array in
Java
Suppose you are maintaining a list of objects using List (i.e., list<object>), and 
want to convert it into an array of objects (i.e., Object[]). How would you 
convert List to Array in Java?
To do that, you can in general take advantage of list.toarray(), which returns 
Object[], an array of all the elements in the list.
Here is a code example of converting List to Array with list.toarray().
List<String> strList = new ArrayList<String> ();
strList.add("apple");
strList.add("orange");
strList.add("mango");

String[] strArray = strList.toArray(new String[strList.size()]);
However, one caveat of this approach is that list.toarray() does not work with 
primitive object types such as int, float, etc.
In order to convert List of primitive types to a corresponding array, you can 
leverage arrayutils.toprimitive(object[] array), which converts an array of non­
primitive Objects to primitives. arrayutils is available in Apache Commons 
Lang library.
Here is a code example of converting list<integer> to int[] with 
arrayutils.toprimitive().
import org.apache.commons.lang.ArrayUtils;
List<Integer> intList;
intList.add(1);
intList.add(5);
intList.add(7);

int[] intArray = ArrayUtils.toPrimitive(intList.toArray(new 
Integer[intList.size()]));
Linux Tutorials
How to change a message at the 
login console on Linux
Question: I would like to show a custom message at the login console. How 
can I change a pre-login message which appears at the login screen?
The pre-login message you see at the console screen is useful in multi-user 
Linux environment for various purposes. It can contain important messages 
for users to be aware of when they are logging in. In Linux, the pre-login 
message to be shown at the console screen is specified in /etc/issue file.
/etc/issue is a plain text file which can optionally contain escape sequences for 

various built-in information.
The following is an example of /etc/issue file.
This is a test message.
Current date: d
Current time: t
System name: s
Architecture: m
OS build info: v
Hostname: n
Kernel: r
Number of logged-in users: u
Once you edit /etc/issue as above, you will see the following message at the 
login console.

This is a test message.
Current date: Z014-03-Z8
Current time: 16:35:35
System name: Linux
Architecture: x86_64
OS build info: #1 SMF Wed Oct 16 18:3?:1Z UTC Z013
Hostname: centos.domain
Kernel : Z .6.3Z-358.Z3.Z.e16.x86_64
Number of logged-in users: 0
centos login:
Linux Tutorials
How to disable entering password 
for default keyring to unlock on 
Ubuntu desktop
Question: When I boot up my Ubuntu desktop, a pop up dialog appears, 
asking me to enter a password to unlock default keyring. How can I disable 
this "unlock default keyring" pop up window, and automatically unlock my 
keyring?
A keyring is thought of as a local database that stores your login information 
in encrypted forms. Various desktop applications (e.g., browsers, email 
clients) use a keyring to store and manage your login credentials, secrets, 
passwords, certificates, or keys securely. For those applications to retrieve the 
information stored in a keyring, the keyring needs to be unlocked.
GNOME keyring used by Ubuntu desktop is integrated with desktop login, 
and the keyring is automatically unlocked when you authenticate into your 

desktop. But your default keyring can remain "locked" if you set up 
automatic desktop login or wake up from hibernation. In this case, you will 
be prompted: 
"Enter password for keyring 'Default keyring' to unlock. An application 
wants to access to the keyring 'Default keyring,' but it is locked."
If you want to avoid typing a password to unlock your default keyring every 
time such a pop-up dialog appears, the following describes how you can 
disable the password prompt.
Before proceeding, please understand the implication of disabling the 
password prompt. By automatically unlocking the default keyring, you will 
make your keyring (and any information stored in the keyring) accessible to 
anyone who uses your desktop, without them having to know your password.
Disable Password for Unlocking Default Keyring
Open up Dash, and type password to launch Passwords and Keys app.

Alternatively, use the seahorse command to launch the GUI from the command 
line.I______^^Ixterm
T $ seahorse
_J^HOn the left side panel, right-click on the Default keyring, and choose
Change Password.

Passwords and Keys
Passwords
Filter
IB Login
Certificates
iiGnomeZKeyS...
HUser Key Storag 
jSystem Trust
PGP Keys
C GnuPG keys
Secure Shell
iAi OpenSSH keys
Set as default
Change Password
Lock
Delete 
Properties
odulo.com/wp-login.php
ks.com/wp-login.php
m/members/login.php
A https://accounts.google.com/ServiceLogin 
|W+tl I
A https://accounts.google.com/ServiceLogin
A https://accounts.google.com/ServiceLogin
A https://accounts.google.com/ServiceLogin
A https://accounts.google.com/ServiceLogin 
l+HWI I
https://accounts.google.com/5erviceLogin
A https://accounts.qooqle.com/ServiceLoqin 
Type your current login password.

Leave a new password for the Default keyring as blank.
Click on Continue button to confirm to store passwords unencrypted.
That's it. From now on, you won't be prompted to unlock the default keyring.
Linux Tutorials
How to print "optimized out" value 
in gdb
Question: I was debugging a program with gdb debugger. When I tried to 
print the value of a variable while tracing a function call, gdb says <value 
optimized out>, not displaying the value. How can I show the value of an 
<optimized out> variable in gdb?
Modern compilers such as gcc are able to perform various optimizations based 
on syntax and semantic analysis of the code at compile time. The goal of such 
optimizations is to improve program execution speed and/or reduce binary 

size, and they typically come with the cost of extra compilation time or 
reduced debuggability of the program.
The <value optimized out> message in gdb is one symptom of such compiler 
optimizations. To view the optimized-out value of a variable during 
debugging, you need to turn off gcc compiler optimization, either on a 
pervariable basis, or program-wide, as described below.
(gdb) p quantity
$8 = <optimized out>
Solution One: Turn-off Compiler Optimization on a 
Per-variable Basis
If you are interested in a particular variable in gdb, you can declare the 
variable as "volatile" and recompile the code. This will make the compiler 
turn off compiler optimization for that variable.
volatile int quantity = 0;
If you declare a variable as volatile, it means that the variable can be 
modified externally (e.g., by the operating system, a signal handler, or 
hardware interrupt, etc). This essentially tells the compiler that the variable's 
value in memory can change at any time. Thus the compiler must not perform 
any optimization on the variable, which makes the variable accessible to gdb 
debugger as well.

Solution Two: Turn-off Compiler Optimization for 
the Entire Program
Another option to see all <optimized out> variables in gdb is of course disabling 
gcc optimization altogether.
Look for compilation flags (e.g., in cflags) in your Makefile. You will find 
something like -O1, -O2 or -O3, which defines various levels of gcc 
optimization. Remove this flag, or change it to -O0.
For example, change:
CFLAGS = -g -O2 -DSF_VISIBILITY -fvisibility=hidden -fno-strict-aliasing
-Wall
to:
CFLAGS = -g -O0 -DSF_VISIBILITY -fvisibility=hidden -fno-strict-aliasing 
-Wall
This will reduce compilation time, and allows gdb to inspect the program 
properly, but at the cost of possibly reduced run-time program performance.
Linux Tutorials
How to use Git behind proxy on

Ubuntu
If you would like to use Git behind proxy, you can configure proxy settings 
via a following Git-specific way. Note that setting http_proxy/https_proxy 
environment variables alone is not sufficient since the variables are ignored 
by Git. You need to store proxy information in a Git configuration file by 
using the following command.
term
$ export http_proxy=http://myproxy.domain.com:1234 $ git config --global 
http.proxy $http_proxy
$ git config --global https.proxy $http_proxy
Then, a user-specific Git configuration file (~/.gitconfig) will be
created and updated as follows.
[http]
proxy = http://myproxy.domain.com:1234
[https] 
proxy = http://myproxy.domain.com:1234
For Ubuntu 11.04 or 11.10
If you are using Ubuntu 11.04 or 11.10, you may be getting the following 
error when trying to clone any repository via https from github:

remote HEAD refers to nonexistent ref, unable to checkout
The error is due to a bug in the libcurl version installed in the particular version 
of Ubuntu you are using. To solve the problem, manually install the latest 
libcurl3-gnutls and two dependencies (libp11-kit0 and
libgnutls26).
xterm
3ubuntu2_amd64.deb
Linux Tutorials
How to close an open DNS resolver
The DNS server that we have created in the previous tutorial is an open DNS 
resolver. An open resolver does not filter any incoming requests, and accepts 
queries from any source IP address.
Unfortunately, an open resolver can become an easy target to attackers. For 
example, attackers can initiate a Denial of Service (DoS) or even worse, a 
Distributed Denial of Service (DDoS) attack on the open DNS server. These 
attacks can also be combined with IP spoofing, where all the reply packets 
will be directed to a victim’s spoofed IP address. In another attack scenario 

called DNS amplification attacks, an open DNS server can actively 
participate in the attacks.
According to openresolverproject.org, it is not advisable to run an open 
resolver unless necessary. Most companies keep their DNS servers accessible 
to only their customers. This tutorial will focus on how to configure a DNS 
server so that it stops being an open resolver and responds only to valid 
customers.
Tuning Firewall
As DNS runs on UDP port 53, system admins may attempt to allow port 53 for 
client IP addresses only, and block the port from the rest of the Internet.
Though this will work, there are going to be some problems. Since the 
communication between the root servers and the DNS servers use port 53 as 
well, we have to make sure that the IP addresses of the root servers are also 
allowed on UDP port 53 in the firewall.
A sample firewall script is provided below. For production servers, make sure 
that the rules match your requirements and also comply with company 
security policies.
xterm
Ilk_ # vim firewall-script
_!■## existing rules are flushed to start with a new set of rules ## 
iptables -F
iptables -A INPUT -s A.A.A.A/X -p udp --dport 53 -j ACCEPT iptables -A 
INPUT -s B.B.B.B/Y -p udp --dport 53 -j ACCEPT iptables -A INPUT -s 
C.C.C.C/Z -p udp --dport 53 -j ACCEPT 
iptables -A INPUT -p udp --dport 53 -j DROP
## making the rules persistent ## 
service iptables save

Make the script executable and run it.
HI_ # chmod +x firewall-script
# ./firewall-script
Blocking Recursive Queries
DNS queries can be primarily categorized as recursive and iterative queries. 
For a recursive query, the server responds to the client with either the answer 
or an error message. If the answer is not available in the server cache, the 
server communicates with the root servers to obtain authoritative name 
servers. The servers keeps looking up until it gets an answer, or until the 
query times out. For an iterative query, on the other hand, the server simply 
refers the client to another server who would be able to process, thus leading 

to less processing on the server itself.
We can control the IP addresses that are allowed for recursive queries. We 
modify the configuration file /etc/named.conf and add/modify the following 
parameters.
term
__# vim /etc/named.conf
## we define ACLs to specify the source address/es ## acl customer-a{ 
A.A.A.A/X; };
acl customer-b { B.B.B.B/Y; C.C.C.C/Z; };
## we call the ACLs under options directive ##
options {
directory "/var/named";
allow-recursion { customer-a; customer-b; };
};
Tuning Firewall for Open Resolver
If you must run an open resolver, it is recommended that you tune the 

firewall properly so that your server cannot be exploited. smurfmonitor 
repository provides a powerful set of iptables rules that can be used in open 
resolvers, such as blocking requests for domains involved in DNS 
amplification attacks. The repository is updated periodically, and it is highly 
recommended for DNS server admins.
To sum up, attacks on open DNS resolvers are common, especially for DNS 
servers without proper security. This tutorial demonstrated how to disable an 
open DNS server. We have also seen how iptables can be used to add an 
additional layer of security to an open DNS server.
Hope this helps.
Linux Tutorials
How to mount Google Drive on 
Linux
In the past, close to 30K people signed up for a online petition, desperately 
wanting to have an official native Linux client for Google Drive, and yet their 
voice is still being ignored by Google. Perhaps when it comes to boosting 
their bottom line, Linux desktop market is not a priority for Google.
They can ignore Linux desktop market all they want, but they cannot ignore 
the power of FOSS. Faced with the frustration, the open-source community 
respondded, producing unofficial Google Drive clients such as Grive or 
SyncDrive. These clients are file synchronization tools which sync files and 
folders between local file system and remote Google Drive. As such, you 
cannot mount Google Drive using these tools.
If you want to mount Google Drive on Linux, you can try google- 
driveocamlfuse (gdfuse), which is a FUSE-based file system backed by Google 
Drive. Using this user-space file system, you can mount your Google Drive 
account on Linux, and have full read/write access to files/folders stored in 
your Google Drive as if they were local files/folders.

In this tutorial, I will describe how to mount Google Drive on Linux with 
google-drive-ocamlfuse. google-drive-ocamlfuse is written in OCaml, and 
you can use OPAM (OCaml package manager) to install google-drive- 
ocamlfuse and its dependencies. The following guide shows distro-specific 
instructions to install google-drive-ocamlfuse.
Install google-drive-ocamlfuse on Debian and 
Ubuntu
These instructions are tested on Debian 10 (buster) and Ubuntu 20.04 
(focal). If you have success on other versions of Debian/Ubuntu, let me 
know.
The first step is to install OPAM. Luckily OPAM is available in the base 
repository of Debian 10 and Ubuntu 20.04. So simply install it with:
xterm
T $ sudo apt install opam
__ I^^Next, start OPAM as follows. It will ask you a series of questions 
for OPAM initialization. You can press enter each time to choose default 
answers.
xterm
lb_ $ opam init
_L~_JNext, update OPAM package repositories.
term
$ opam update
OPAM has a useful tool called depext which can resolve external dependencies 
of OPAM packages. depext automatically detects and installs external 
dependencies of google-drive-ocamlfuse using apt-get package manager.
xterm
T $ opam install depext
___I^Bnow go ahead and install external dependencies of google-drive- 
ocamlfuse with depext. As shown below, several DEB packages (e.g., libfuse-dev 
libsqlite3-dev, m4) are installed by depext.

T_ $ opam depext google-drive-ocamlfuse
term
Finally, install google-drive-ocamlfuse with opam.
xterm
T $ opam install google-drive-ocamlfuse

*32
danJdebian:-$ opan install google-drive-ocanlfuse
The following actions will be performed:
* install conf-grip
3
[required by conf-gnp-pown-sec, zarith]
* install conf-n4
1
[required by ocanlfind]
* install ocanl-secondary-conpiler 4,08,1-1
[required by ocanlfind-secondary]
* install canlidl
1.09
[required by google-drive-ocanlfuse]
* install conf-perl
1
[required by zarith]
* install conf-pkg-config
1.3
[required by conf-libfuse, conf-zlib, conf-sqlite3]
* install ocanlbuild
0,14.0
[required by ocanlnet]
* install conf-gnp-pown-sec
3
[required by cryptokit]
I install ocanlfind
1.8.1
[required by extlib]
* install conf-zlib
1
[required by cryptokit]
* install conf-sqlite3
1
[required by sqlite3]
* install conf-libfuse
1
[required by ocanlfuse]
* install conf-libcurl
1
[required by ocurl]
* install zarith
1.11
[required by cryptokit]
* install ocanlfind-secondary
1.8.1
[required by dune]
* install base-bytes
base
[required by extlib]
1 install ocurl
0.9.1
[required by gapi-ocanl]
* install dune
2.7.1
[required by google-drive-ocanlfuse]
* install ocanlnet
4.1.8
[required by gapi-ocanl]
* install result
1.5
[required by dune-configurator]
* install easy-fornat
1.3.2
[required by yojson]
* install cppo
1.6.7
[required by extlib]
I install csexp
1,3.2
[required by dune-configurator]
* install biniou
1.2.1
[required by yojson]
I install extlib
1.7.7
[required by google-drive-ocanlfuse]
* install dune-configurator
2.7.1
[required by ocanlfuse, cryptokit, sqlite3]
* install yojson
1,7.0
[required by gapi-ocanl]
* install sqlitel
5.0.1
[required by google-drive-ocanlfuse]
* install ocanlfuse
2.7.1-cvs7 [required by google-drive-ocanlfuse]
* install cryptokit
1.16.1
[required by google-drive-ocanlfuse]
* install gapi-ocaml
0,4,1
[required by google-drive-ocanlfuse]
I install google-drive-ocanlfuse
0.7.24
Do you want to continue? [Y/n;

After successful build, the google-drive-ocamlfuse binary will be found in 
~/.opam/defauit/bin. Add this path to your path environment variable.
term
T_ $ vi ~/.bashrc
__ ^^HpATH=="$PATH:$HOME/.opam/default/bin"
term
T_ $ source ~/.bashrc
Install google-drive-ocamlfuse on Ubuntu via PPA
The author of google-drive-ocamlfuse maintains the ppA repository for 
google-drive-ocamlfuse. So you can install it on Ubuntu and its derivatives 
such as Linux Mint more easily.
$ sudo add-apt-repository ppa:alessandro-strada/ppa $ sudo apt update 
$ sudo apt install google-drive-ocamlfuse
Install google-drive-ocamlfuse on Fedora
These instructions are tested on Fedora 33. If you have success on other 
versions of Fedora, let me know.
The procedure starts with installing OpAM, which makes installation of 
google-drive-ocamlfuse painless. OpAM is included in the base repository of 
Fedora 33.

$ sudo yum install opam
_J^HNext, initialize OPAM as follows. It will ask you a series of 
questions for OPAM initialization. You can press enter each time to accept 
default answers.
xterm
$ opam init
I iNext. update OPAM package repositories with: 
xterm
$ opam update
Next, install an OPAM package called depext, which can install external 
dependencies of google-drive-ocamlfuse using Fedora's yum package 
manager.
xterm
lb_ $ opam install depext
___I^Bgo ahead and install external dependencies of google-drive- 
ocamlfuse. As shown below, several RPM packages (e.g., fuse-devel gmp-devel, 
sqlite-devel) are installed with yum.
xterm
$ opam depext google-drive-ocamlfuse
# The following new 05 packages need to be installed: fuse-devel gnp-devel libcurl-devel perl-Pod-Htnl pkgconfig 
sqlite-devel zlib-devel
The following connand needs to be run through "sudo":_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
I yun install fuse-devel gmp-devel libcurl-devel perl-Pod-Htnl pkgconfig sqlite-devel zlib-develI
Finally, install google-drive-ocamlfuse with opam.
xterm
lb_ $ opam install google-drive-ocamlfuse

[danfifedora ']$ iopam install google-drive-ocanlfuse 
The following actions will be performed:
♦ install dune
2.7.1
[required by google-drive-ocanlfuse]
♦install conf-gnp
3
[required by conf-gnp-pown-sec, zarith]
♦ install conf-iM
1
[required by ocamlfind]
♦install canlidl
1.69
[required by google-drive-ocanlfuse]
♦ install conf-perl
1
[required by zarith]
♦ install conf-pkg-config
1.3
[required by conf-libfuse, conf-zlib, conf-sqlite3]
♦ install ocanlbuild
0.14.0
[required by ocamlnet]
♦install result
1.5
[required by dune-configurator]
♦ install easy-fornat
1.3.2
[required by yojson]
♦ install cppo
1.6.7
[required by extlib]
♦ install conf-gnp-pown-sec
3
[required by cryptokit]
♦ install ocanlfind
1.8.1
[required by extlib]
♦ install conf-zlib
1
[required by cryptokit]
♦ install conf-sqliteJ
1
[required by sqlite3]
♦ install conf-libfuse
1
[required by ocamlfuse]
♦install conf-libcurl
1
[required by ocurl]
♦ install csexp
1.3.2
[required by dune-configurator]
♦ install biniou
1.2,1
[required by yojson]
♦ install zarith
1.11
[required by cryptokit]
♦ install base-bytes
base
[required by extlib]
♦ install ocurl
6.9.1
[required by gapi-ocanl]
♦ install dune-configurator
2.7.1
[required by ocamlfuse, cryptokit, sqlite3]
♦ install yojson
1.7.6
[required by gapi-ocanl]
♦install ocanlnet
4.1.8
[required by gapi-ocanl]
♦install extlib
1.7.7
[required by google-drive-ocanlfuse]
♦ install sqliteJ
5,6.2
[required by google-drive-ocanlfuse]
♦ install ocanlfuse
2.7.1-cvs7 [required by google-drive-ocanlfuse]
♦ install cryptokit
1.16.1
[required by google-drive-ocanlfuse]
♦ install gapi-ocanl
6.4.1
[required by google-drive-ocanlfuse]
I install google-drive-ocanlfuse 0.7.24
=“ * 30
Do you want to continue’ [Y/n

After successful build, the google-drive-ocamlfuse executable will be found 
in ~/.opam/defauit/bin. Add this path to your path environment variable.
term
T $ vi ~/.bashrc
__»ATH=" $PATH:$HOME/.opam/default/bin"
term
T__ $ source ~/.bashrc
Install google-drive-ocamlfuse on Other Linux 
Systems
If you want to install google-drive-ocamlfuse on other Linux distros, the 
easiest way is to first install OPAM. Most modern Linux distributions 
provide OPAM in their repositories. Once OPAM is installed, the rest of the 
steps should be the same as Debian, Ubuntu and Fedora described above.
Arch Linux: https://archlinux.org/packages/community/x86_64/opam/ 
OpenSUSE: https://software.opensuse.org/package/opam Manjaro: 
https://discover.manjaro.org/packages/opam
Mount Google Drive with google-drive-ocamlfuse
The first step to configure google-drive-ocamlfuse is to run it in your home 
directory without any argument:
term
$ google-drive-ocamlfuse
This will open up a web browser window, asking you to log in to your 
Google account. After logging in, you will see the following screen, 
requesting for permission. Click on Allow.

Next you have to grant gdfuse access to your Google Drive. Click on
Allow.

3 Sign in-Coogle Accoun X
(?) 4 C ft 10 A https://accounts.google.com/signin/oauth/consent?authu5 ••• 0 ft
lll\ 0 
=
gdfuse wants to access your 
Google Account
This will allow gdfuse to:
See, edit, create, and delete all of your Google (J 
Drive files
Make sure you trust gdfuse
You may be sharing sensitive info with this site or app. 
Learn about how gdfuse will handle your data by reviewing 
its privacy policies. You can always see or remove access 
in your Google Account
Learn about the risks
Cancel
Subsequently, the browser window will say:
The application was successfully granted access. Please wait for the client to 
retrieve the authorization tokens

At this point, if you go back to the termain window where google- 
driveocamlfuse was launched, you will see the message: 
Access token retrieved correctly.
If, for whatever reason, you see the following error, simply restart 
googledrive-ocamlfuse from the terminal and go through Google 
authorization steps again.
Cannot get token. Quitting.
At this point, the initial configuration is completed. You should see a new 
directory named ~/.gdfuse/default created, which contains the configuration file 
config and file cache.
Now proceed to create a local mount point for your Google Drive and mount 
Google Drive as follows.
xterm
T__ $ mkdir ~/googledrive
$ google-drive-ocamlfuse ~/googledrive
__K this point, your Google Drive should be accessible via
~/googledrive. You can use the mount command to verify that.
xterm
lb_ $ mount | grep google
__ ^^Byou can also check how much space is left on your Google Drive 
account by using df command.

term
__ $ df ~/googledrive
To unmount Google Drive from your file system, simply run: 
I_ xterm
dan@ubuntu:
*S
 nount grep google
-drive-ocanlfuse on /hone/dan/ drive type fuse. -drive-ocanlfuse (rw.nosuid.nodev.relatipo
ser uMOOl .group id=1691
dan@ubuntu:-$ df googlednve
google-drive-ocanlfuse 15728640 1286852 14441788 /bone/dan/googledrive
$ fusermount -u ~/googledrive
Mount Multiple Google Drive Accounts 
Simultaneously
If you have more than one Google Drive account, you can mount them 
simultaneously. In this case, use -label option to distinguish between them as 
follows.
xterm
T $ google-drive-ocamlfuse -label [label] [mountpoint]
_J^HNote that the original Google Drive you mounted without any label 
in the above is assigned the label called default.
Every time you configure google-drive-ocamlfuse with a new label, you will 
need to go through the same Google authentication procedure described 
above, but for a different Google account. After that, ~/.gdfuse/[label] will be 

created to store configuration data for the different Google account.
Auto-mount Google Drive upon Boot
If you want to have Google Drive mounted automatically upon boot, you can 
use systemd to do that. Create the following systemd unit file. In this example, 
the user account being used is dan, and the paths for google-driveocamlfuse 
and the mount point are defined accordingly. So adjust them based on your 
environment.
term
__ $ sudo vi /etc/systemd/system/google-drive.service 
[Unit]
Description=FUSE filesystem over Google Drive After=network.target
[Service]
User=dan
Group=dan
ExecStart=/home/dan/.opam/default/bin/google-drive-ocamlfuse -label 
default /home/dan/googledrive
ExecStop=fusermount -u /home/dan/googledrive
Restart=always
Type=forking
[Install]
WantedBy=multi-user.target
Alias=google-drive.service

Now reload systemd and test-run it with: 
xterm
Bl_ $ sudo systemctl daemon-reload
$ sudo systemctl start google-drive
.L^Jverify that the status of the Google Drive systemd service is active: 
xterm
T_ $ sudo systemctl status google-drive

__ B^BNow if you reboot your system, your Google Drive account will be 
automatically mounted and ready for use by systemd.
If you don't want to use systemd, you can also use /etc/fstab or pam_mount to auto­
mount Google Drive. Refer to the official document for 
details.
Linux Tutorials
How to open a port in the firewall 
on CentOS or RHEL
Question: I am running a web/file server on my CentOS box, and to access 
the server remotely, I need to modify a firewall to allow access to a TCP port 
on the box. What is a proper way to open a TCP/UDP port in the firewall of 
CentOS/RHEL?
Out of the box, enterprise Linux distributions such as CentOS or RHEL come 
with a powerful firewall built-in, and their default firewall rules are pretty 
restrictive. Thus if you install any custom services (e.g., web server, NFS, 
Samba), chances are their traffic will be blocked by the firewall rules. You 
need to open up necessary ports on the firewall to allow their traffic.
On CentOS/RHEL 6 or earlier, the iptables service allows users to interact with 
netfilter kernel modules to configure firewall rules in the user space. Starting 
with CentOS/RHEL 7, however, a new userland interface called
firewalld has been introduced to replace iptables service.
To check the current firewall rules, use this command:
xterm
lb_ $ sudo iptables -L

[dev@centos7 -]$ sudo iptables -L
Chain INPUT (policy ACCEPT)
target prot opt source
destination
ACCEPT all •• anywhere
anywhere
ctstate RELATED,ESTABLISHED
ACCEPT all -- anywhere
anywhere
INPUT direct all -- anywhere
anywhere
INPUT ZONES SOURCE all - anywhere
anywhere
INPUT”ZONES' all - anywhere
anywhere
ACCEPT icmp •- anywhere
anywhere
REJECT all •• anywhere
anywhere
reject-with icmp-host-prohibited
Chain FORWARD (policy ACCEPT)
target prot opt source
destination
ACCEPT all -- anywhere
anywhere
ctstate RELATED,ESTABLISHED
ACCEPT all •• anywhere
anywhere
FORWARD direct all -- anywhere
anywhere
FORWARD IN ZONES SOURCE all •• anywhere anywhere
FORWARD IN ZONES all •• anywhere anywhere
FORWARDJ)UT_ZONES_SOURCE all - anywhere anywhere
FORWARD OUT_ZONES all •• anywhere anywhere
ACCEPT 
icmp •• anywhere 
anywhere
REJECT 
all •• anywhere 
anywhere 
reject-with icmp-host-prohibited
Now let's see how we can update the firewall to open a port on 
CentOS/RHEL.
Open a Port on CentOS/RHEL 7 or Later
Starting with CentOS and RHEL 7, firewall rule settings are managed by 
firewalld service daemon. A command-line client called firewall-cmd 
can talk to this daemon to update firewall rules permanently.
To open up a new port (e.g., tcp/80) permanently, use these commands. 
xterm
$ sudo firewall-cmd --zone=public --add-port=80/tcp -permanent

$ sudo firewall-cmd --reload
ithout --permanent flag, the firewall rule would not persist across
reboots.
Check the updated rules with:
term
Hl_ $ firewall-cmd --list-all
Open a Port on CentOS/RHEL 6 or Earlier
On CentOS/RHEL 6 or earlier, the iptables service is responsible for 
maintaining firewall rules.
Use iptables command to open up a new TCP/UDP port in the firewall. To save 
the updated rule permanently, you need the second command.
$ sudo iptables -I INPUT -p tcp -m tcp --dport 80 -j ACCEPT
$ sudo service iptables save
__ Another way to open up a port on CentOS/RHEL 6 is to use a 
terminal-user interface (TUI) firewall client, named system-config-firewall-tui.
term
lb_ $ sudo system-config-firewall-tui
Choose Customize button in the middle and press ENTER.

system-config-firewall
----------- 1 Firewall Configuration |-----------
A firewall protects against unauthorized 
network intrusions. Enabling a firewall blocks 
all incoming connections. Disabling a firewall 
allows all connections and is not recommended.
Firewall:
<Tab>/<Alt-Tab> between elements | <Space> selects | <F12> next screen
If you are trying to update the firewall for any well-known service (e.g., web 
server), you can easily enable the firewall for the service here, and close the 
tool. If you are trying to open up any arbitrary TCP/UDP port, choose
Forward button and go to a next window.

Add a new rule by choosing Add button.

Specify a port (e.g., 80) or port range (e.g., 3000-3030), and protocol (e.g., tcp or 
udp).

Finally, save the updated configuration, and close the tool. At this point, the 
firewall will be saved permanently.

system-config-firewall
- - - - - - - - - 1 Warning |- - - - - - - - -
Clicking the 'Yes' button will override 
any existing firewall configuration. Are 
you sure that you want to do this?
Please remember to check if the services 
iptables and ipGtables are enabled.
<Tab>/<Alt-Tab> between elements | <Space> selects | <F12> next screen
Linux Tutorials
How to convert a text file to PDF
format on Linux
Question: I want to convert a plain text file into a PDF document. Is there an 
easy way to convert a text file to a PDF file from the command line on 
Linux?

When you have a bunch of text documents to maintain, there are advantages 
in converting them into PDF format. For example, PDF is good for printing 
because PDF documents have pre-defined layout. Besides, with PDF format, 
there is less risk hat the documents are accidentally modified.
To convert a text file to PDF format, you can follow two-step procedures.
Prerequisites
First, you need to install two prerequisite packages.
For Debian, Ubuntu or Linux Mint:
term
$ sudo apt-get install enscript ghostscript
For Fedora, CentOS or RHEL:
term
$ sudo yum install enscript ghostscript
For Arch Linux:
term
$ sudo pacman -S enscript ghostscript
Convert a Text File to PDF Format
Once all prerequisites are installed, follow these two steps to generate a PDF 
file from a text file.
First, convert a text file to Postscript format by using enscript commandline 
tool.

term
$ enscript -p output.ps input.txt
Finally convert the generated postscript file to a PDF file.
term
$ ps2pdf output.ps output.pdf
Linux Tutorials
How to turn your CentOS box into 
an OSPF router using Quagga
Quagga is an open source routing software suite that can be used to turn your 
Linux box into a fully-fledged router that supports major routing protocols 
like RIP, OSPF, BGP or ISIS router. It has full provisions for IPv4 and IPv6, 
and supports route/prefix filtering. Quagga can be a life saver in case your 
production router is down, and you don't have a spare one at your disposal, so 
are waiting for a replacement. With proper configurations, Quagga can even 
be provisioned as a production router.
In this tutorial, we will connect two hypothetical branch office networks (e.g., 
192.168.1.0/24 and 172.17.1.0/24) that have a dedicated link between 
them.

Our CentOS boxes are located at both ends of the dedicated link. The 
hostnames of the two boxes are set as site-a-rtr and site-b-rtr respectively. IP 
address details are provided below.
SiteA: 192.168.1.0/24
SiteB: 172.17.1.0/24
Peering between 2 Linux boxes: 10.10.10.0/30
The Quagga package consists of several daemons that work together. In this 
tutorial, we will focus on setting up the following daemons.
1. Zebra: a core daemon, responsible for kernel interfaces and static routes.
2. Ospfd: an IPv4 OSPF daemon.
Install Quagga on CentOS
We start the process by installing Quagga using yum.
xterm
Bl_ # yum install quagga
On CentOS 7 or later, SELinux prevents /usr/sbin/zebra from writing to its 
configuration directory by default. This SELinux policy interferes with the 
setup procedure we are going to describe, so we want to disable this policy. 
For that, either turn off SELinux (which is not recommended), or enable the
;bra_write_config boolean as follows. Skip this step if you are using CentOS 6. 
xterm
# setsebool -P zebra_write_config 1
__ Without this change, we will see the following error when 
attempting to save Zebra configuration from inside Quagga's command shell. 
Can't open configuration file /etc/quagga/zebra.conf.OS1Uu5.
After Quagga is installed, we configure necessary peering IP addresses, and 
update OSPF settings. Quagga comes with a command line shell called vtysh. 
The Quagga commands used inside vtysh are similar to those of major router 

vendors such as Cisco or Juniper.
Phase 1: Configuring Zebra
We start by creating a Zebra configuration file, and launching Zebra daemon.
term
# cp /usr/share/doc/quagga-XXXXX/zebra.conf.sample 
/etc/quagga/zebra.conf
# service zebra start
# chkconfig zebra on
—^^■Launch vtysh command shell: 
xterm
lb_ # vtysh
_L~_JtIu' prompt will be changed to:
xterm
■l _site-A-RTR#
_J^Hwhich indicates that you are inside vtysh shell.
First, we configure the log file for Zebra. For that, enter the global 
configuration mode in vtysh by typing:
xterm
Hb_ site-A-RTR# configure terminal
_ ^^Hand specify log file location, then exit the mode:
___ xterm
site-A-RTR(config)# log file /var/log/quagga/quagga.log site-A-
RTR(config)# exit
Save configuration permanently:
xterm
Hb_ site-A-RTR# write
_J^HNext, we identify available interfaces and configure their IP 
addresses as necessary.
xterm

Hl_ site-A-RTR# show interface
Interface eth0 is up, line protocol detection is disabled
Interface eth1 is up, line protocol detection is disabled
site-A-RTR# configure terminal
site-A-RTR(config)# interface eth0
site-A-RTR(config-if)# ip address 10.10.10.1/30 site-A-RTR(config-if)#
description to-site-B
site-A-RTR(config-if)# no shutdown 
Go ahead and configure eth1 parameters:
site-A-RTR(config)# interface eth1
site-A-RTR(config-if)# ip address 192.168.1.1/24 
site-A-RTR(config-if)# description to-site-A-LAN site-A-RTR(config-if)# no 
shutdown

Now verify configuration:
Hl_ site-A-RTR(config-if)# do show interface
Interface eth0 is up, line protocol detection is disabled 
inet 10.10.10.1/30 broadcast 10.10.10.3
Interface eth1 is up, line protocol detection is disabled
inet 192.168.1.1/24 broadcast 192.168.1.255
term
lb_ site-A-RTR(config-if)# do show interface description
Interface Status Protocol Description 
eth0 up unknown to-site-B 
eth1 up unknown to-site-A-LAN

Save configuration permanently, and quit interface configuration mode.
term
site-A-RTR(config-if)# do write 
site-A-RTR(config-if)# exit 
site-A-RTR(config)# exit 
site-A-RTR#
Quit vtysh shell to come back to Linux shell.
xterm
■l _site-A-RTR# exit
_J^HNext, enable IP forwarding so that traffic can be forwarded between 
etho and eth1 interfaces.
xterm
T__ # echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf # sysctl -p
/etc/sysctl.conf
__ ^^^Repeat the IP address configuration and IP forwarding enabling 
steps on siteBserver as well.
If all goes well, you should be able to ping siteB's peering IP 10.10.10.2 from 
siteA server.
Note that once Zebra daemon has started, any change made with vtysh's 
command line interface takes effect immediately. There is no need to restart 
Zebra daemon after configuration change.
Phase 2: Configuring OSPF
We start by creating an OSPF configuration file, and starting the OSPF 

daemon:
term
# cp /usr/share/doc/quagga-XXXXX/ospfd.conf.sample 
/etc/quagga/ospfd.conf
# service ospfd start
# chkconfig ospfd on 
_^^MNow launch vtysh shell to continue with OSPF configuration:
xterm
lb_ # vtysh
I fcnter router configuration mode:
xterm
lb_ site-A-RTR# configure terminal
site-A-RTR(config)# router ospf
iw-Joptionally, set the router-id manually:
xterm
H__ site-A-RTR(config-router)# router-id 10.10.10.1
.L^JacIcI the networks that will participate in OSPF:
xterm
site-A-RTR(config-router)# network 10.10.10.0/30 area 0
site-A-RTR(config-router)# network 192.168.1.0/24 area 0
Save configuration permanently:
xterm
Hb_ site-A-RTR(config-router)# do write
_ ^^^Repeat the similar OSPF configuration on siteB as well:
I___^Ixterm
site-B-RTR(config-router)# network 10.10.10.0/30 area 0 site-B-
RTR(config-router)# network 172.17.1.0/24 area 0

site-B-RTR(config-router)# do write
The OSPF neighbors should come up now. As long as ospfd is running, any 
OSPF related configuration change made via vtysh shell takes effect 
immediately without having to restart ospfd.
In the next section, we are going to verify our Quagga setup.
Verification
1. Test with ping
To begin with, you should be able to ping the LAN subnet of siteB from siteA. 
Make sure that your firewall does not block ping traffic.
term
[[email protected] ~]# ping 172.17.1.1 -c 2
2. Check routing tables
Necessary routes should be present in both kernel and Quagga routing tables.
term
[[email protected] ~]# ip route 
10.10.10.0/30 dev eth0 proto kernel scope link src 10.10.10.1
172.17.1.0/30 via 10.10.10.2 dev eth0 proto zebra metric 20
192.168.1.0/24 dev eth1 proto kernel scope link src 192.168.1.1

xterm
T__ [[email protected] ~]# vtysh
site-A-RTR# show ip route
__ ^^Bcodes: K - kernel route, C - connected, S - static, R - RIP, O - 
OSPF, I - ISIS, B - BGP, > - selected route, * - FIB route
O 10.10.10.0/30 [110/10] is directly connected, eth0, 00:14:29 C>
*
10.10.10.0/30 is directly connected, eth0
C>
*
 127.0.0.0/8 is directly connected, lo
O>
*
 172.17.1.0/30 [110/20] via 10.10.10.2, eth0, 00:14:14
C>
*
 192.168.1.0/24 is directly connected, eth1
3. Verifying OSPF neighbors and routes
Inside vtysh shell, you can check if necessary neighbors are up, and proper 
routes are being learnt.
term

Bl_ [[email protected] ~]# vtysh
site-A-RTR# show ip ospf neighbor
site-A-RTR# show ip ospf neighbor
Neighbor ID Pri State Dead Time Address Interface 
RXmtL RqstL DBsmL
10.10.10.2 
1 Full/DR 39.268s 10.10.10.2 
eth0:10.10.10.1 
0 
0 
0
site-A-RTR# 
site-A-RTR#
site-A-RTR# show ip ospf route
====== OSPF network routing table ======
N 
10.10.10.0/30 
[19] area: 0.8.0.0
directly attached to eth0
N 
172.16.1.0/30 
[20] area: 0.0.0.0
via 10.10.10.2, etho
= OSPF router routing table ======
—————— OSPF external routing table ——————
In this tutorial, we focused on configuring basic OSPF using Quagga. In 
general, Quagga allows us to easily configure a regular Linux box to speak 
dynamic routing protocols such as OSPF, RIP or BGP. Quagga-enabled 
boxes will be able to communicate and exchange routes with any other router 
that you may have in your network. Since it supports major open standard 
routing protocols, it may be a preferred choice in many scenarios. Better yet, 
Quagga's command line interface is almost identical to that of major router 
vendors like Cisco or Juniper, which makes deploying and maintaining 
Quagga boxes very easy.
Hope this helps.
Linux Tutorials
What is good stock portfolio 
management software on Linux

If you are investing in the stock market, you probably understand the 
importance of a sound portfolio management plan. The goal of portfolio 
management is to come up with the best investment plan tailored for you, 
considering your risk tolerance, time horizon and financial goals. Given its 
importance, no wonder there are no shortage of commercial portfolio 
management apps and stock market monitoring software, each touting 
various sophisticated portfolio performance tracking and reporting 
capabilities.
For those of you Linux aficionados who are looking for a good open-source 
portfolio management tool to manage and track your stock portfolio on 
Linux, I would highly recommend a Java-based portfolio manager called 
JStock. If you are not a big Java fan, you might be turned off by the fact that 
JStock runs on a heavyweight JVM. At the same time I am sure many people 
will appreciate the fact that JStock is instantly accessible on every Linux 
platform with JRE installed. No hoops to jump through to make it work on 
your Linux environment.
The day is gone when "open-source" means "cheap" or "subpar". Considering 
that JStock is just a one-man job, JStock is impressively packed with many 
useful features as a portfolio management tool, and all that credit goes to Yan 
Cheng Cheok! For example, JStock supports price monitoring via watchlists, 
multiple portfolios, custom/built-in stock indicators and scanners, support for 
27 different stock markets and cross-platform cloud backup/restore. JStock is 
available on multiple platforms (Linux, OS X, Android and Windows), and 
you can save and restore your JStock portfolios seamlessly across different 
platforms via cloud backup/restore.
Sounds pretty neat, huh? Now I am going to show you how to install and use 
JStock in more detail.
Install JStock on Linux
Since JStock is written in Java, you must install Java runtime to run it. Note 
that JStock requires JRE 1.8 or higher. If your JRE version does not meet this 
requirement, JStock will fail with the following error.

Exception in thread "main" java.lang.UnsupportedClassVersionError: 
org/yccheok/jstock/gui/JStock : Unsupported major.minor version 51.0
Once you install JRE on your Linux, download the latest JStock release from
the official website, and launch it as follows. 
xterm
$ wget
https://github.com/yccheok/jstock/releases/download/rel ease_1-0-7- 
13/jstock-1.0.7.13-bin.zip
$ unzip jstock-1.0.7.13-bin.zip
$ cd jstock
$ chmod +x jstock.sh
$ ./jstock.sh
In the rest of the tutorial, let me demonstrate several useful features
of JStock.
Monitor Stock Price Movements via Watchlist on
JStock
On JStock you can monitor stock price movement and automatically get 
notified by creating one or more watchlists. In each watchlist, you can add 
multiple stocks you are interested in. Then add your alert thresholds under 
"Fall Below" and "Rise Above" columns, which correspond to minimum and 
maximum stock prices you want to set, respectively.

For example, if you set minimum/maximum prices of AAPL stock to $102 
and $115.50, you will be alerted via desktop notifications if the stock price 
goes below $102 or moves higher than $115.50 at any time.
You can also enable email alert option, so that you will instead receive email 
notifications for such price events. To enable email alerts, go to Options menu. 
Under Alert tab, turn on "Send message to email(s)" box, and enter your Gmail 
account. Once you go through Gmail authorization steps, JStock will start 
sending email alerts to that Gmail account (and optionally CC to any third- 
party email address).

Manage Multiple Stock Portfolios with JStock
JStock allows you to manage multiple portfolios. This feature is useful if you 
are using multiple stock brokers. You can create a separate portfolio for each 
broker and manage your buy/sell/dividend transactions on a per-broker basis. 
You can switch different portfolios by choosing a particular portfolio under 
"Portfolio" menu. The following screenshot shows a hypothetical portfolio.

Optionally you can enable broker fee option, so that you can enter any broker 
fees, stamp duty and clearing fees for each buy/sell transaction. If you are 
lazy, you can enable fee auto-calculation and enter fee schedules for each 
brokering firm from the option menu beforehand. Then JStock will 
automatically calculate and enter fees when you add transactions to your 
portfolio.

Screen Stocks with Built-in/Custom Indicators
If you are doing any technical analysis on stocks, you may want to screen 
stocks based on various criteria (so-called "stock indicators"). For stock 
screening, JStock offers several pre-built technical indicators that capture 
upward/downward/reversal trends of individual stocks. The following is a list 
of available indicators.
Moving Average Convergence Divergence (MACD)
Relative Strength Index (RSI)
Money Flow Index (MFI)
Commodity Channel Index (CCI)
Doji
Golden Cross, Death Cross

Top Gainers/Losers
To install any pre-built indicator, go to " Stock Indicator Editor" tab on JStock. 
Then click on Install button in the right-side panel. Choose "Install from jstock 
server" option, and then install any indicator(s) you want.
Once one or more indicators are installed, you can scan stocks using them. 
Go to "Stock Indicator Scanner" tab, click on Scan button at the bottom, and choose 
any indicator.

Once you select the stocks to scan (e.g., NYSE, NASDAQ), JStock will 
perform scan, and show a list of stocks captured by the indicator.

oe® JStock - Free Stock Market Software
File Edit Country Language Database Watchlist Portfolio Options Look n Feel Help Android
DOW JONES: 17,425.03 (-178.84) NASDAQ : 5,007.41 (-58.44) S&P500 : 2,043.94 (-19.42)
Stock Watchlist $ Stock Indicator Editor ft ( Stock Indicator Scanner Portfolio Management 0
Indicator Scan Result
(Indicator 
Code [Symbol 
||Prev 
Last [[High * [Low 
Vol 
Chg 
Chg(%) j[LVol ||Buy
B
MACD Up Trend Signal
AEB
AEGON N.V. Perp.
24.28
24.13
24.35
24.02
0
-0.15
-0.61
0
0.00
MACD Up Trend Signal
ATU
Actuant Corporati
24.32
23.96
24.38
23.93
0
-0.36
-1.48
0
0.00
MACD Up Trend Signal
AGO-F
Assured Guaranty
24.89
24.91
24.92
24.86
0
0.02
0.08
0
0.00
MACD Up Trend Signal
AHT-D
Ashford Hospitali
25.21
25.19
25.28
25.13
0
-0.02
-0.08
0
0.00
MACD Up Trend Signal
AHT-A
Ashford Hospitali
25.27
25.25
25.50
25.20
0
-0.02
-0.09
0
0.00
MACD Up Trend Signal
AHT-E
Ashford Hospitali
25.51
25.35
25.52
25.28
0
-0.16
-0.63
0
0.00
MACD Up Trend Signal
AGO-B
Assured Guaranty
25.73
25.70
25.70
25.70
0
-0.03
-0.11
0
0.00
MACD Up Trend Signal
AEH
AEGON N.V. Perp.
25.79
25.80
25.81
25.76
0
0.01
0.04
0
0.00
MACD Up Trend Signal
AFA
American Financial...
25.98
25.86
25.87
25.83
4180
-0.12
-0.48
100
25.83
MACD Up Trend Signal
BAC-D
Bank of America C
25.88
25.83
25.91
25.82
0
-0.05
-0.19
0
0.00
MACD Up Trend Signal
BCS-A
Barclays PLC ADS
26.00
25.97
25.97
25.94
0
-0.03
-0.12
0
0.00
MACD Up Trend Signal
BGE-B
Bge Cap Trust II
25.97
26.00
26.03
25.94
0
0.03
0.10
0
0.00
MACD Up Trend Signal
ALL-A
Allstate Corporat
25.97
25.95
26.17
25.90
0
-0.02
-0.08
0
0.00
MACD Up Trend Signal
ARE-E
Alexandria Real E
25.85
25.70
26.21
25.70
0
-0.15
-0.58
0
0.00
MACD Up Trend Signal
BCS-C
Barclays PLC Amer
26.45
26.35
26.44
26.32
0
-0.10
-0.38
0
0.00
MACD Up Trend Signal
BCS-D
Barclays PLC Amer
26.60
26.53
26.61
26.52
0
-0.07
-0.26
0
0.00
MACD Up Trend Signal
AIR
AAR Corp.
26.29
26.29
26.68
25.84
0
0.00
0.00
0
0.00
MACD Up Trend Signal
AGO
Assured Guaranty
26.78
26.43
26.80
26.43
0
-0.35
-1.31
0
0.00
MACD Up Trend Signal
AXS-C
Axis Capital Hold
26.82
26.82
26.84
26.75
0
0.00
0.00
0
0.00
MACD Up Trend Signal
BDN-E
Brandywine Realty
26.54
26.76
26.87
26.58
0
0.22
0.83
0
0.00
MACD Up Trend Signal
ARH-C
Arch Capital Grou
26.89
26.89
26.89
26.78
0
0.00
0.00
0
0.00
MACD Up Trend Signal
BAC-I
Bank Amer Corp De
26.80
26.76
26.89
26.72
0
-0.04
-0.15
0
0.00
MACD Up Trend Signal
BAC-W
Bank of America C
26.83
26.74
26.90
26.71
0
-0.09
-0.34
0
0.00
MACD Up Trend Signal
BGCA
BGC Partners Inc. 8...
26.91
26.91
26.98
26.82
2304
0.00
0.00
160
26.92
MACD Ud Trend Sianal
AFK__
Aeaon NV 8.00% N...
27.53
27.54
___ 27.56
27.47
29641
___ 0.01
___ 0.04
146
___ 27.54
___________  "________  ______
► Scan,.. Q Stop]
Indicator scanner is scanning China Eastern Air.,.(12% completed)
Besides pre-built indicators, you can also define custom indicator(s) on your 
own with a GUI-based indicator editor. The following example screens for 
stocks whose current price is less than or equal to its 60-day average price.

Cloud Backup and Restore between Linux and
Android JStock
Another nice feature of JStock is cloud backup and restore. JStock allows you 
to save and restore your portfolios/watchlists via Google Drive, and this 
feature works seamlessly across different platforms. Such cross-platform 
backup/restore can be useful if you are using JStock back and forth on two 
different platforms. I tested it on Linux desktop and Android mobile, and it 
worked beautifully. I saved my JStock portfolios to Google Drive on 
Android, and was able to restore them on Linux version of JStock. Ideally it 
would be nice if JStock automatically synced to the cloud, so that I wouldn't 
have to trigger cloud backup/restore manually. Still thumbs up to this nice 
feature.

If you don't see your portfolios/watchlists after restoring from Google Drive, 
make sure that your country is correctly set under Country menu.
JStock Android free version is available from Google Play store. You will 
need to upgrade to premium version for one-time payment if you want to use 
its full features (e.g., cloud backup, alerts, charts). I think the premium 
version is definitely worth it.

As a final note, I should mention that its creator, Yan Cheng Cheok, is pretty 
active in JStock development, and quite responsive in addressing any bugs. 
Kudos to him!
What do you think of JStock as portfolio tracking software?
Linux Tutorials
How to enable and configure 
desktop sharing on Linux Mint 
Cinnamon desktop
Question: I was trying to enable desktop sharing via Vino VNC server (vino- 
server) on Linux Mint 17 Cinnamon desktop. However, I notice that vino- 
preferences tool which allows us to configure vino-server (e.g., sharing option, 
security, notification on/off) no longer exists. Also, I cannot find desktop 
sharing menu on Cinnamon desktop. How can I configure desktop sharing 
via vino-server on the latest Linux Mint 17 Cinnamon desktop?
The latest Linux Mint Cinnamon desktop comes with vino-server preinstalled 
for VNC desktop sharing, but it's reported that desktop sharing configuration 

menu is missing.
An alternative way to configure vino-server and enable desktop sharing is to use 
dconf-editor's graphical interface, as described in the following.
First install dconf-editor:
xterm
T $ sudo apt-get install dconf-editor 
_J^HLaunch dconf-editor.
I_^lxterm
$ dconf-editor
Navigate to org->gnome->desktop->remote-access on the left panel of dconf-editor.
Then you will see various desktop sharing options.

dconf Editor
+ x
privacy
screensaver
Type:
Default:
Schema:
Summary Enable remote access to the desktop
Description: If true, allows remote access to the desktop via the RFB 
protocol. Users on remote machines may then connect to the 
desktop using a VNC viewer.
Boolean
false
dconf Editor
► apps
► ca
► com
► desktop
▼ org
► cinnamon
► freedesktop
▼ gnome 
GWeather
► Totem
► brasero 
calculator 
caribou
► charmap
▼ desktop
► ally
► applications 
background 
input-sources 
interface 
lockdown 
media-handlpa 
notifications
Set to Default 
-------------- j
org.gnome.Vino
Name
Value
alternative-port
5900
authentication-methods ['vnc'l 
/
disable-background
□ /
disable-xdamage
*
1________
icon-visibility
client
lock-screen-on-disconnect □
mailto
network-interface
notify-on-connect
prompt-enabled
require-encryption
n
use-alternative-port
liil
use-upnp
□
view-only
□
vnc-password
cGFzc3dvcmQ=
Most importantly, click on enabled to activate desktop remote access. Besides
this, you can customize other options.
For example, you can enable VNC password authentication by changing the 
following fields:
authentication-methods: set it to ['vnc']
vnc-password: change it to Base64-encoded string of a preferred password.

In this example, we choose VNC password as passwordpassword encoded string 
is cgfzc3dvcmqk.
term
$ echo "password" | base64 
cGFzc3dvcmQK
Optionally, you can enable other options:
notify-on-connect: shows a desktop notification when vino-server receives a
connection request.
prompt-enabled: a remote user is not allowed to access a desktop via VNC 
until the VNC request is approved by the desktop owner.
Troubleshoot
1. I am getting the following error when attempting to start vino-server.
** (vino-server:4280): WARNING **
:
 The desktop sharing service is not 
enabled, so it should not be run.
To enable desktop sharing service, use dconf-editor as described above. 
Alternatively, run the following command.
term
T # gsettings set org.gnome.Vino enabled true
Linux Tutorials
How to access a NAT guest from

host with VirtualBox
Question : I have a guest VM running on VirtualBox, which uses NAT 
networking. So the guest VM is getting a private IP address (10.x.x.x) 
assigned by VirtualBox. If I want to SSH to the guest VM from the host 
machine, how can I do that?
VirtualBox supports several networking options for guest VMs, one of them 
being NAT networking. When NAT networking is enabled for a guest VM, 
VirtualBox automatically performs network address translation between the 
guest VM and host's network stack, so that you do not have to configure 
anything on the host machine and local network for the guest VM's 
networking to work. The implication of such NAT, however, is that the guest 
VM is not reachable or visible from external networks as well as from the 
local host itself. This is a problem if you want to access the guest VM from 
the host machine for some reason (e.g., SSH).
If you want to access a NAT guest from the host on VirtualBox, you can 
enable port forwarding for VirtualBox NAT, either from the GUI or from the 
command line. This tutorial demonstrates how to SSH a NAT guest from 
the host by enabling port forwarding for port 22. If you want to access HTTP 
of a NAT guest instead, replace port 22 with port 80.
Configure VirtualBox Port Forwarding from the 
GUI
On VirtualBox, choose the guest VM you want to access, and open Settings 
window of the VM. Click on Network menu on the left, click on Advanced to 
show additional network adapter options.

Click on a button labeled Port Forwarding.

You will see a window where you can configure port forwarding rules. Click 
on Add icon in the upper right corner.

Add a new port forwarding rule with the following detail. 
Name: SSH (any arbitrary unique name)
Protocol: TCP
Host IP: 127.0.0.1
Host Port: 2222 (any unused port higher than 1024)
Guest IP: IP address of the guest VM
Guest Port: 22 (SSH port)

Port forwarding configured for the guest VM will be enabled automatically 
when you power on the guest VM. For verification, check that port 2222 is 
opened by VirtualBox after you launch the guest VM:
Now that port forwarding is in place, you can SSH to the guest VM bs 
follows. 
xterm
■l _$ ssh -p 2222 <login>@127.0.0.1
__ ^^Hah SSH login request sent to 127.0.0.1:2222 will automatically be 
translated into 10.0.2.15:22 by VirtualBox, allowing you to SSH to the guest 
VM.
Configure VirtualBox Port Forwarding from the

Command Line
VirtualBox comes with a command-line management interface called 
vboxmanage. Using this command-line tool, you can also set up port 
forwarding for your guest VM.
The following command creates a port forwarding rule for guest VM named 
centos7 with IP address 10.0.2.15 and SSH port 22, mapped to local 
host at port 2222. The name of the rule (ssh in this example) must be 
unique.
$
term
VBoxManage modifyvm "centos7" --natpf1
"SSH,tcp,127.0.0.1,2222,10.0.2.15,22"
_J^HOnce the rule is created, you can verify that by using the command 
below.
xterm
Hl_ $ VBoxManage showvminfo "centos7" | grep NIC
$ VBoxManage showvminfo "centos?" I grep NIC
NIC 1: 
MAC: O80027CA4ED4, Attachment: NAT, Cable connected: on, Trace:
off (file: none), Type: 82540EM, Reported speed: 0 Mbps, Boot priority: 0, Prom
isc Policy: deny, Bandwidth group: none
NIC 1 Settings: MTU: 0, Socket (send: 64, receive: 64), TCP Window (send:64, re
ceive: 64)_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
NIC 1 Rule(0): name = SSH, protocol = tcp, host ip = 127.0.0.1, host port = 22 
22, guest ip = 10.0.2.15, guest port = 22
NIC 2: 
disabled
NIC 3: 
disabled
NIC 4: 
disabled
NIC 5: 
disabled
NIC 6: 
disabled
NIC 7: 
disabled
NIC 8: 
disabled

Linux Tutorials
How to remove Amazon ads on
Ubuntu
Question: I don't want to see Amazon ads when I am searching on Ubuntu 
Dash. How can I disable Amazon ads on Ubuntu?
Since Ubuntu 12.10 (Quantal Quetzal), Unity Dash started to show shopping 
suggestions from Amazon as well as online search results, which are tailored 
to your searches. This feature ticks off many Ubuntu users.
©< G
® < [® mP3 player
Hi Applications
St tj 4>)) 4:38 PM O
Rhythmbox Music 
Player
m More suggestions
SanDisk Sansa Clip* 4 
GB MP3 Player (Black)
See 6 more results ►
Silver Metal Clip MP3 
...up to 2GB 4GB 8GB
Black mini Clip Mp3 
player
Sony Walkman 
...B MP3 Player Black
If you want to disable Amazon search results on Ubuntu Dash, go to 
"System Settings" -^ Privacy (or "Security & Privacy"). Then turn off 
"Include online search results" under Search tab. This will also disable showing other 
online search results on Ubuntu Dash.

Record Activity
Search Results Recent Items Files Applications Diagnostics
When searching in the Dash: 
Include online search results.
*' Privacy
All Settings Privacy
Another way to remove Amazon shopping suggestions from Ubuntu Dash 
is to remove "Unity Shopping Lens" as follows. This works for Ubuntu 12.10, 
Ubuntu 13.04.
term
__ $ sudo apt-get remove unity-lens-shopping

After removal, you need to log out, and log back in to finalize
deactivation.
Linux Tutorials
How to uninstall and remove
Apache2 on Ubuntu or Debian
It turns out that uninstalling Apache2 web server is not as straightforward as 
installing it. You will need to remove Apache2 package itself, as well as 
other dependent packages and its associated configurations. Here is how you 
remove Apache2 cleanly on Ubuntu and Debian.
The first step is to stop any running instance of Apache2, because Apache2 
will not be properly removed while it is running.
xterm
Bl_ $ sudo service apache2 stop
Then uninstall Apache2 and its dependent packages. Use purge option instead 
of remove option with apt-get command. The former option will remove 
dependent packages, as well as any configuration files created by them. In 
addition, use autoremove option as well, to remove any other dependencies that 
were installed with Apache2, but are no longer used by any other package.
term
__bin apache2-common
$ sudo apt-get autoremove
__ ^^HFinally, check if there is any configuration files or manual pages 
belonging to Apache2, which are still not removed.
I_____ ^^Ixterm

$ whereis apache2 
apache2: /etc/apache2
In this example, /etc/apache2 directory still exists. Since this directory (as well 
as any configuration files in it) is no longer necessary, go ahead and remove 
it manually.
Linux Tutorials
How to allow remote access to
MySQL server
If you have installed MySQL server fresh, the MySQL server binds on local 
loopback address (i.e., 127.0.0.1) by default. This means that you can access a 
MySQL server only from the local host, but not from any remote host. You 
can verify this by running netstat as follows.
term
Bl_ $ sudo netstat -nap | grep mysql 
tcp 0 0 127.0.0.1:3306 0.0.0.0:
*
 LISTEN 23799/mysqld
unix 2 [ ACC ] STREAM LISTENING 180374 23799/mysqld
/var/run/mysqld/mysqld.sock

The above netstat output means that mysqld (MySQL server daemon) is listening 
on the loopback address 127.0.0.1 for any incoming MySQL connection. If a 
remote MySQL client residing outside the server attempts to access the 
server, its access request will receive a timeout error.
If you would like the local MySQL server to be accessed from any external 
host, you can follow the steps below to allow remote access to MySQL 
server.
Step One: Edit MySQL Bind Address
The first step is to edit the MySQL server's bind address which is currently 
set to loopback. For this, open the MySQL server's configuration file, and 
modify it as follows.
T $ sudo vi /etc/mysql/my.conf
xterm
# bind-address = 127.0.0.1 # comment this line out
If you cannot locate bind-address in my.conf, you can add the following line to
my.conf.
bind-address = 0.0.0.0

Then, restart MySQL server:
xterm
$ sudo /etc/init.d/mysql restart (Debian/Ubuntu) $ sudo systemctl restart 
mysql (Debian/Ubuntu with systemd)
$ sudo systemctl restart mysqld (Fedora or CentOS/RHEL 7 or later)
$ sudo /etc/init.d/mysqld restart (CentOS/RHEL 6 or earlier)
Now if you re-run the above netstat command, you will see that mysqld is 
listening on 0.0.0.0:3306, or <public-ip-address-of
your_machine>:3306. This means that you have successfully changed the MySQL 
binding address for remote access.
Step Two: Grant Remote Access to MySQL User
The next step is to grant remote access to the MySQL user who needs remote 
access. For this, you need to log in to the MySQL server and issue 
appropriate commands.
Log in to the MySQL server from the command line as follows.
xterm
T $ sudo mysql -u root -p
I [After login, run the following command at the MySQL prompt: 
xterm
■l _mysql> GRANT ALL PRIVILEGES ON *.*  TO
[email protected]'%' IDENTIFIED BY "alice_password";
_^^Brhe above command grants a privilege to the MySQL user alice to 
allow her to access all MySQL databases from any remote host.
Alternatively, you can selectively grant remote access privilege as follows.
xterm
mysql> GRANT ALL PRIVILEGES ON mydb.
*
 TO 
[email protected]'1.2.3.4' IDENTIFIED BY

'bob_password';
__ ^^Mln the above case, it allows the MySQL user bob to access a database 
called mydb from a remote host with IP address 1.2.3.4.
Finally, run the following command to enable the privilege change to take 
effect.
■l _mysql> FLUSH PRIVILEGES;
As a final note, be aware that opening up your MySQL server to the world 
like this can be a source of security vulnerabilities. A more secure way to 
access MySQL server remotely is via an SSH tunnel. See this post for more 
detail.
Linux Tutorials
How to configure network 
interfaces in CentOS
In CentOS, Fedora or RHEL, if you have disabled advanced network 
management tools such as Network Manager, you can configure network 
interfaces by using configuration files located in /etc/sysconfig/networkscripts. For 
each network interface (e.g., eth0), there should be a corresponding 
configuration file (e.g., ifcfg-eth0) in the directory. If you would like to 
configure a network interface, do the following.
Configure a Static IP Address
If you would like to assign a static IP address to the interface eth0:
term
T $ sudo vi /etc/sysconfig/network-scripts/ifcfg-ethG

DEVICE=eth0
BOOTPROTO=static
HWADDR=00:05:29:E0:4F:3D
IPADDR=10.0.1.10
NETMASK=255.255.255.0
GATEWAY=10.0.1.1
ONBOOT=yes
TYPE=Ethernet
Configure a DHCP IP Address
If the interface etho has IP address assigned by DHCP:
xterm
lb $ sudo vi /etc/sysconfig/network-scripts/ifcfg-ethG
DEVICE=eth0
BOOTPROTO=dhcp
HWADDR=00:05:29:E0:4F:3D
ONBOOT=yes
TYPE=Ethernet

Once network configuration has been created/updated, you can activate a 
network interface by running:
xterm
T__ $ sudo service network restart
Note that if you are using Desktop version of CentOS, where network 
interfaces are configured with Network Manager by default, you will need to 
disable NetworkManager first, before configuring interfaces as described in 
this post.
Linux Tutorials
How to install Unity Tweak Tool on 
Ubuntu desktop
Question: I am trying to customize fresh Ubuntu desktop that I just installed. 
For that, I want to use Unity Tweak Tool. How can I install Unity Tweak 
Tool on Ubuntu?
Unity Tweak Tool is a popular Unity desktop customization tool. As the 
name implies, this tool is applicable to Ubuntu's default desktop environment 
only (i.e., Unity-based GNOME desktop). This versatile tool allows you to 
customize a wide variety of Unity desktop features and configurations via 

extremely intuitive and easy-to-use GUI. Its menu looks similar to Ubuntu's 
official System Settings, but its desktop customization capabilities far exceed 
those of the default System Settings. Using Unity Tweak Tool, you can 
customize appearance, behaviors and configurations for a plethora of desktop 
components, such as workspace, windows, icons, themes, cursors, fonts, 
scrolling, hot corners, etc. If you are a Unity desktop user, Unity Tweak Tool 
is definitely a must-have app.
While Unity Tweak Tool is an essential tool for Ubuntu desktop, it does not 
come pre-installed on stock Ubuntu desktop. For those of you to customize 

Unity desktop, here is how to install Unity Tweak Tool on Ubuntu desktop.
Install Unity Tweak Tool on Ubuntu 13.04 and 
higher
Starting with Ubuntu 13.04, Unity Tweak Tool is available in the Ubuntu's 
base repositories. So its installation is as straightforward as:
I______^^Ixterm
T $ sudo apt-get install unity-tweak-tool
I Ito launch Unity Tweak Tool:
xterm
lb_ $ unity-tweak-tool
__ ^Blf you want to try the latest version of Unity Tweak Tool, you can 
install it from its daily PPA (as describe below).
Install Unity Tweak Tool on Ubuntu 12.10
Note that Unity Tweak Tool works on Ubuntu 12.10 and higher. if you want 
to install it on Ubuntu 12.10, you can use its PPA repository as follows. This 
PPA repository is also useful when you want to test the latest developments 
of the tool.
term
$ sudo add-apt-repository ppa:freyja-dev/unity-tweaktool-daily
$ sudo apt-get update
$ sudo apt-get install unity-tweak-tool
Linux Tutorials
How to parse JSON string via

command line on Linux
If you often deal with JSON-formatted texts from the command line or in 
shell scripts, you may wonder if there is any command-line utility which can 
parse JSON string. A command-line JSON parser can be handy when you test 
or debug JSON web services. You can feed JSON-formatted responses from 
web services into the command-line JSON parser, thereby easily inspecting 
otherwise hard-to-read JSON responses or extracting individual objects from 
them.
In this tutorial, I will describe how to parse JSON string from the 
command line.
On Linux, there is a command-line JSON processor called jq which does 
exactly that. Using jq, you can parse, filter, map, and transform 
JSONstructured data effortlessly.
Install jq on Linux
Install jq on Ubuntu, Debian or Linux Mint
term
T $ sudo apt-get install jq
Install jq on CentOS or RHEL
First, enable EPEL repository and run:
term
T $ sudo yum install jq
Instll jq on any other Linux
You can simply download its binary (available for 32-bit and 64-bit system 

separately) as follows.
On 64-bit system:
term
$ wget
https://github.com/stedolan/jq/releases/download/jq1.6/jq-linux64
$ chmod +x ./jq-linux64
$ sudo cp jq-linux64 /usr/bin/jq
__ ^^HOn 32-bit system, replace the above wget command with the 
following.
term
$ wget
https://github.com/stedolan/jq/releases/download/jq1.6/jq-linux32
___^^HtIu' jq binary is also available for Windows and OS X platforms, 
and its full source code is released under the MIT license.
Parse JSON String with jq
The following examples illustrate how to parse JSON-structured data with jq. 
An example JSON Schema:
term
lb_ $ cat json.txt
{
name": "Google", 
location": 
{
street": "1600 Amphitheatre Parkway", 
city": "Mountain View", 
state": "California",

"country": "US"
},
"employees":
[
{
"name": "Michael", 
"division": "Engineering" 
}, 
{
"name": "Laura", 
"division": "HR" 
}, 
{
"name": "Elise",
"division": "Marketing"
}
]
}


To parse a JSON object:
term
xterm
$ cat json.txt | jq '.name'
"Google"
To parse a nested JSON object:
lb_ $ cat json.txt | jq '.location.city'
__ ^^■"Mountain View"
To parse a JSON array:
term
Bl_ $ cat json.txt | jq '.employees[0].name'
Michael
To extract specific fields from a JSON object:
Bl_ $ cat json.txt | jq '.location | {street, city}' 
{
"city": "Mountain View",
"street": "1600 Amphitheatre Parkway" 
}

If you want to parse JSON string inside your program (e.g., written in Python 
or Perl), you can refer to these Python and Perl tutorials. If what you are 
interested in is to check if a given JSON string conforms to JSON schema, 
this tutorial may be useful.
Linux Tutorials
How to find the default gateway on 
Linux
In TCP/IP networking, a default gateway provides a default route for a host 
to use in order to send traffic to remote networks. Here "default" implies that 
a host will always rely on this gateway to send traffic to unless it knows how 
to route the traffic.
To find out what is the default gateway used on Linux, you can refer to a 
local routing table.
The ip command shows a local routing table.
xterm
T__ $ ip route show
default via 192.168.1.1 dev wlan0 proto static
169.254.0.0/16 dev wlan0 scope link metric 1000
172.16.199.0/24 dev vmnet1 proto kernel scope link src 172.16.199.1
192.168.1.0/24 dev wlan0 proto kernel scope link src 192.168.1.4 metric 9
192.168.233.0/24 dev vmnet8 proto kernel scope link src

192.168.233.1
In the routing table, the row starting with " default via" shows information about 
the default route configured on your Linux system. The information includes 
the IP address of the default gateway, and the network interface reachable to 
this gateway.
In this example, 192.168.1.1 is the default gateway, and it is reachable via wlan0 
network interface.
If you are writing a shell script where you need to obtain the IP address of the 
default gateway programmatically, the following command can be useful. It 
will print out the IP address of the default gateway.
xterm
lb_ $ ip route show | grep 'default' | awk '{print $3}'
__ ^■192.168.1.1
Thus in your script, you can easily store the IP address of the default gateway 
to a variable as follows.
#!/bin/sh

default_gateway_ip='ip route show | grep 'default' | awk '{print $3}''
Linux Tutorials
How to set up MailScanner, Clam 
Antivirus and SpamAssassin in 
CentOS mail server
In the world of mail servers, MailScanner is one of the best open source 
software for virus scanning and spam detection. MailScanner relies on 
preinstalled anti-virus and anti-spam software to check incoming and 
outgoing emails for malicious content or patterns of spamming. This makes 
sure that the mail server does not participate in the distribution of malware 
and unsolicited spam emails. It also helps preventing the mail server IP from 
becoming blacklisted, keeping the mail server records clean.
This tutorial will focus on setting up MailScanner along with Clam Antivirus 
and SpamAssassin in a CentOS system. The procedure should work on 
RHEL as well. If you are interested in setting up this system on Ubuntu, refer 
to this tutorial instead.
Installing MailScanner is a lengthy process, but going forward step by step 
should make the deployment process easy.
Preparing the System

Before we start doing anything, it should be mentioned that SELinux is 
disabled on CentOS. Configuring SELinux for MailScanner is beyond the 
scope of this tutorial. It is also necessary to add Repoforge repository on 
CentOS.
Installing Dependencies
yum is used to install packages that are required for MailScanner. The list is 
long, but fortunately yum can resolve all the dependencies.
xterm
# yum install -y yum-utils gcc cpp perl bzip2 zip unrar make patch automake 
rpm-build perl-Archive-Zip perlFilesys-Df perl-OLE-Storage_Lite perl-Sys- 
Hostname-Long perl-Sys-SigAction perl-Net-CIDR perl-DBI perl­
MIMEtools perl-DBD-SQLite binutils glibc-devel perlFilesys-Df zlib zlib- 
devel wget mlocate
Installing ClamAV and SpamAssassin
yum can be used to install ClamAV and SpamAssassin as well. The following 
few steps cover how to install and prepare them.
xterm
Hl_ # yum install clamav spamassassin
i^-Jupdate ClamAV.
xterm
HI_ # freshclam -v
_ ^^Bupdate and start SpamAssassin.
I___ xterm
# sa-update
# service spamassassin start 
# chkconfig spamassassin on
Fix a path to MailScanner by creating a symbolic link.

term
HI_ # ln -s /usr/bin/freshclam /usr/local/bin/freshclam
Configuring Postfix
Postfix is stopped and disabled on start-up. Postfix should not auto-start 
because the MailScanner service will be responsible for invoking Postfix 
whenever necessary.
xterm
T # service postfix stop
# chkconfig postfix off
__^^Bpostfix header_checks is used to hold any incoming email that Postfix 
receives. MailScanner performs checks on the emails held in a queue.
xterm
# vim /etc/postfix/main.cf
__^B## This line is added ##
eader_checks = regexp:/etc/postfix/header_checks 
xterm
# vim /etc/postfix/header_checks 
__^B## This line is added ## 
/AReceived:/ HOLD
Preparing MailScanner

MailScanner is not yet available in CentOS or Repoforge repositories. We 
will download packages from the official MailScanner site and install it.
xterm
# wget
http://www.mailscanner.info/files/4/rpm/MailScanner4.84.6-1.rpm.tar.gz
__ B^MNow we will extract and install the packages. The installation will 
take some time, so you can take a break if you want.
xterm
# tar zxvf MailScanner-4.84.6-1.rpm.tar.gz
# cd MailScanner-4.84.6-1
# ./install
__ ^^MAfter installation, the directories necessary for SpamAssassin are 
created and permissions are modified.
xterm
# mkdir /var/spool/MailScanner/spamassassin
# chown postfix /var/spool/MailScanner/spamassassin # chown postfix
/var/spool/MailScanner/incoming/
*
 
__■^■Next, the configuration file for MailScanner is backed up and then 
modified.
term
__# vim /etc/MailScanner/MailScanner.conf
%org-name% = test CentOS Mail Server %org-long-name% =
ORGFULLNAME %web-site% = ORG WEBSITE
Run As User = postfix Run As Group = postfix MTA = postfix
Incoming Queue Dir = /var/spool/postfix/hold Outgoing Queue Dir =

/var/spool/postfix/incoming
Virus Scanners = clamav
# # please check /etc/MailScanner/spam.lists.conf for more details ## Spam 
List = SBL+XBL
# # the directory created earlier ##

SpamAssassin User State Dir = /var/spool/MailScanner/spamassassin 
At this point, MailScanner is ready. We can initialize the service. 
Debug MailScanner stats before firing up.

term
__# MailScanner -lint
__H^^^^^^^Hxterm
HI_ # service MailScanner start
# chkconfig MailScanner on
Verifying MailScanner Operation
After MailScanner has been deployed, the events that take place behind the 
scenes can be viewed in /var/log/maillog. The following log snippet shows the 
sample activities while a mail is processed by Postfix.
xterm
T # tailf /var/log/maillog
Mar 8 03:12:15 centos postfix/pickup[15865]: 79F6D1391: uid=0 from= 
Mar 8 03:12:15 centos postfix/cleanup[15871]: 79F6D1391: hold: header 
Received: by mail.example.tst (Postfix, from userid 0)??id 79F6D1391; Sat, 
8 Mar 2014 03:12:15 +0600 (BDT) from local; from= to=
Mar 8 03:12:15 centos postfix/cleanup[15871]: 79F6D1391: messageid=
<[email protected]>
Mar 8 03:12:16 centos MailScanner[15832]: New Batch: Scanning 1 
messages, 668 bytes
Mar 8 03:12:16 centos MailScanner[15832]: Virus and Content Scanning: 
Starting
Mar 8 03:12:22 centos MailScanner[15832]: Requeue: 
79F6D1391.AA526 to 0FA2E139C
Mar 8 03:12:22 centos MailScanner[15832]: Uninfected: Delivered 1 
messages
Mar 8 03:12:22 centos postfix/qmgr[15866]: 0FA2E139C: from=, size=442, 
nrcpt=1 (queue active)
Mar 8 03:12:22 centos MailScanner[15832]: Deleted 1 messages from 
processing-database
Mar 8 03:12:22 centos postfix/local[15897]: 0FA2E139C: to=, relay=local, 

delay=6.8, delays=6.7/0.01/0/0.07, dsn=2.0.0, status=sent (delivered to 
mailbox)
Mar 8 03:12:22 centos postfix/qmgr[15866]: 0FA2E139C: removed
The above process can be summarized as:

1. As instructed, Postfix holds the mail upon receipt.
2. MailScanner swoops in and scans the email in queue.
3. MailScanner re queues the email and hands it over back to Postfix.
4. Postfix processes the email as necessary and delivers the mail to recipient.
On a finishing note, MailScanner is a very powerful tool for providing 
necessary security to a mail server. It can protect the mail server from 
malware for both incoming and outgoing mails. It is a must for any email 
server deployed in production environment.
This tutorial covered setting up MailScanner with basic configuration. The 
parameters of MailScanner as well as SpamAssassin and ClamAV can be 
customized to meet the requirements of the production environment.
Hope this helps.
Linux Tutorials
How to set up Squid as a 
transparent web proxy on CentOS 
or RHEL
In the previous tutorial, we have seen the method of creating a gateway using 
iptables. This tutorial will focus on turning the gateway into a transparent 
proxy server. A proxy is called transparent when clients are not aware that 
their requests are processed through the proxy.
There are several benefits of using a transparent proxy. First of all, for end 
users, a transparent proxy can enhance their web browsing performance by 
caching frequently accessed web content, while introducing minimal 
configuration overhead for them. For administrators, it can be used to enforce 
various administrative policies such as content/URL/IP filtering, rate limiting, 
etc.

A proxy server acts as an intermediary between a client and a destination 
server. The client sends requests to the proxy server which then evaluates the 
requests and takes necessary actions. In this tutorial, we will be setting up a 
web proxy server using Squid, which is a robust, customizable and stable 
proxy server. Personally, I had administered a Squid server with 400+ client 
workstations for about a year. Although I had to restart the service about once 
a month in average, CPU and storage utilization, throughput and client 
response time were all great.
We will be configuring Squid for the following topology. The CentOS/RHEL 
box has one NIC (eth0) connected to the private LAN, and the other one (eth1) 
connected to the Internet.
Squid Installation
To set up a transparent proxy with Squid, we start by adding necessary iptables 
rules. These rules should help you get started, but please make 
sure that they do not conflict with any of the existing configuration.
term
# iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE # iptables -t 
nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 3128
The first rule will cause all outbound packets from eth1 (WAN interface) to 
have the source IP address of eth1 (i.e., enable NAT). The second rule will 
redirect all incoming HTTP packets (destined to TCP 80) from eth0 (LAN 
interface) to Squid listening ort (TCP 3128), instead of forwarding it out to 
WAN interface right away.

We start Squid installation by using yum.
term
Hl_ # yum install squid
Now we will modify Squid configuration to turn it into a transparent proxy. 
We define our LAN subnet (e.g., 10.10.10.0/24) as a valid client network. Any 
traffic not originating from the LAN subnet will be denied access.
term
# vim /etc/squid/squid.conf
__ ^^B^^Hvisible_hostname proxy.example.tst http_port 3128 
transparent
# # Define our network ##
acl our_network src 10.10.10.0/24
# # make sure that our network is allowed ## http_access allow our_network
# # finally deny everything else ##
http_access deny all

Now we start Squid service and make sure it is added to startup. 
xterm
lb_ # service squid start
# chkconfig squid on
Now that Squid is up and running, we can test its functionality by monitoring 
Squid log. Visit any URL from a computer connected to the LAN, and you 
should see something like the following in the log.
term
# tailf /var/log/squid/access.log 
1402987348.816 1048 10.10.10.10 TCP_MISS/302 752 GET
http://www.google.com/ - DIRECT/173.194.39.178 text/html

1402987349.416 445 10.10.10.10 TCP_MISS/302 762 GET 
http://www.google.com.bd/? - DIRECT/173.194.78.94 text/html
According to the log file, the machine with IP 10.10.10.10 tried to access 
google.com and Squid processed the request.
The most basic form of Squid proxy server is now ready. In the rest of the 
tutorial, we will be tuning some parameters of Squid to control outbound 
traffic. Note that this is for demonstration only. Actual policies should be 
customized to meet your requirements.
Preliminary
Before starting the configuration, we clarify a few points.
Squid Configuration Parsing
While reading a configuration file, Squid parses the file in a top-down 
fashion. Rules are parsed top-down until a match is found. Whenever a match 
is found, that rule is executed, and any other rule below it is ignored. So, the 
best practice for adding filtering rules is to specify rules in the following 
sequence.
explicit allow
explicit deny 
allow entire LAN 
deny all

Squid restart vs. Squid reconfigure
Whenever Squid configuration is modified, Squid service needs to be 
restarted. Depending on the number of active connections, restarting the 
service may take a a while, sometimes several minutes. LAN users will not 
be able to access the Internet during this time. To avoid such service 
interruption, we can use the following command instead of service squid restart.
4 
______ ^^Ixterin
lb_ # squid -k reconfigure
__ ^^H'rhis command will allow Squid to run with updated parameters 
without restarting itself.
Filtering LAN Hosts by IP Address
In this demonstration, we want to set up Squid such that hosts with IP address 
10.10.10.24 and 10.10.10.25 are prevented from accessing the Internet.
We create a text file denied-ip-file containing the IP addresses of all 
denied hosts, and add that file in Squid configuration.
xterm
~M # vim /etc/squid/denied-ip-file
_IB10.10.10.24 
10.10.10.25

## then we apply the ACL ##
http_access deny denied-ip-list ## explicit deny ## 
http_access allow our_network ## allow LAN ## 
http_access deny all ## deny all ##
Now we need to restart Squid service. Squid will no longer honor requests 
from these IP addresses. If we check the squid log, we will find tcp_denied 
for requests originated from these hosts.
Filtering Websites in a Blacklist
This method will work for HTTP only. Assuming that we want to block 
badsite.com and denysite.com, we add the addresses to a file and add 
the reference to squid.conf.

T__ # vim /etc/squid/badsite-file
denysite
xterm
lb_ # vim /etc/squid/squid.conf
_■■## ACL definition ## 
acl badsite-list url_regex "/etc/squid/badsite-file"
## ACL application ##
http_access deny badsite-list
http_access deny denied-ip-list ## previously set, no effect here ##
http_access allow our_network
http_access deny all
Please note that we have used the ACL type url_regex, which will match the 
words badsite and denysite in requested URLs. That is, any request whose URL 
contains badsite or denysite (e.g., badsite.org, newdenysite.com, otherbadsite.net) will be 
blocked.

Combining Multiple ACLs
We will create an access list to block clients with IP addresses
10.10.10.200 and 10.10.10.201 from accessing custom-blocksite.com. Any other clients 
would be able to access the site. To do this, we will create an access list to 
isolate the IP addresses first, and then create another access list to isolate the 
required website. Finally, we will use both access lists simultaneously to 
meet the requirement.
xterm
lb_ # vim /etc/squid/custom-denied-list-file
__ ^■10.10.10.200
10.10.10.201
term
# vim /etc/squid/custom-block-website-file
Jcustom-block-site
xterm
T # vim /etc/squid/squid.conf
__ ^^Hacl custom-denied-list src "/etc/squid/custom-denied-list-file" acl 
custom-block-site url_regex "/etc/squid/custom-block-website-file" 
## ACL application ##

http_access deny custom-denied-list custom-block-site
http_access deny badsite-list ## previously set, no effect here ## http_access 
deny denied-ip-list ## previously set, no effect here ## http_access allow 
our_network
http_access deny all
lb_ # squid -k reconfigure
The blocked hosts should not be able to access the mentioned site now. The 
log file /var/log/squid/access.log should contain tcp_denied for corresponding 
requests.
Setting Maximum Download Size
Squid can be used to control the maximum downloadable file size. We want 
to restrict maximum download size to 50mb for hosts 10.10.10.200 and
10.10.10.201 . We have already created the ACL custom-denied-list previously to

isolate the traffic from these sources. Now we will use the same access list to 
restrict download size.
term
lb_ # vim /etc/squid/squid.conf
reply_body_max_size 50 MB custom-denied-list
term
Hb_ # squid -k reconfigure
Setting up Squid Caching Hierarchy
Squid supports caching by storing frequently accessed files in the local 
storage. Imagine hundreds of users within your LAN are accessing 
google.com. Without caching, the logo or doodle for the page needs to be 
fetched individually for each request. Squid can store the logo or doodle in its 
cache to serve them from its cache. This results in improved user perceived 
performance as well as reduced bandwidth usage. A win-win if you will.
To enable caching, we modify the configuration file squid.conf.
term
# vim /etc/squid/squid.conf
_^^Bcache_dir ufs /var/spool/squid 100 16 256
The numbers 100, 16 and 256 have the following meaning.
100mb storage is allocated for Squid cache. You may increase the allocated 
space if you want.
16 directories, each containing 256 subdirectories will be used to store cache

files. This parameter should not be modified.
We can verify whether Squid cache is enabled from the log file 
/var/log/squid/access.log. For successful cache hits, we should see 
entries with tcp_hit.
To sum up, Squid is a powerful, industry standard web proxy server that is 
used widely by system admins worldwide. Squid provides easy access control 
that can be used to administer traffic originating from the LAN. It can be 
deployed in small companies as well as large enterprise networks. This 
tutorial covered only a subset of all Squid features. Refer to the official 
documentation for a complete feature list.
Hope this helps.
Linux Tutorials
How to regenerate /etc/mtab file on 
Linux
Question: I accidentally messed up (or removed) /etc/mtab file. How can I 
recreate the original /etc/mtab file which reflects the current mount 
information?
In case /etc/mtab file is removed or corrupted by accident, and you want to 
recover the original /etc/mtab file, you can use /proc/mounts, which shows an up- 
to-date list of all mounts currently used by Linux system.
To reset /etc/mtab file to reflect the current mount information, run the 
following command.
xterm
T__ $ sudo sh -c 'grep -v rootfs /proc/mounts > /etc/mtab'__

de v@caill ou: -$
dev@caillou:~$ grep -v rootfs /proc/mounts
sysfs /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
udev /dev devtmpfs rw,relatime,size=40G8764k,nr_inodes=1002191,mode=755 O 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0
tmpfs /run tmpfs rw,nosuid,relatime,size=1607344k,mode=755 0 0
/dev/mapper/ubuntu-root / ext4 rw,relatime,errors=remount-ro,data=ordered 0 0
cgroup /sys/fs/cgroup tmpfs rw,relatime,mode=755 0 0
none /sys/fs/fuse/connections fusectl rw,relatime 0 0
none /sys/kernel/debug debugfs rw,relatime 0 O
none /sys/kernel/security securityfs rw,relatime 0 0
none /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0
none /run/shm tmpfs rw,nosuid,nodev,relatime G 0
none /run/user tmpfs rw,nosuid,nodev,noexec,relatime,size=lG2400k,mode=755 0 0
cgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0
cgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0
cgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0
cgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0
cgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0
cgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0
cgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0
cgroup /sys/fs/cgroup/perf event cgroup rw, relatime,perf event 0 0
/dev/sdal /boot ext2 rw,relatime,errors=continue 0 0
binfmtmisc /proc/sys/fs/binfmt_misc binfmtmisc rw,nosuid,nodev,noexec,relatime 
0 0
Linux Tutorials
How to set up networking between
Docker containers
As you may be well aware, Docker container technology has emerged as a 
viable lightweight alternative to full-blown virtualization. There are a 
growing number of use cases of Docker that the industry adopted in different 

contexts, for example, enabling rapid build environment, simplifying 
configuration of your infrastructure, isolating applications in multi-tenant 
environment, and so on. While you can certainly deploy an application 
sandbox in a standalone Docker container, many real-world use cases of 
Docker in production environments may involve deploying a complex 
multitier application in an ensemble of multiple containers, where each 
container plays a specific role (e.g., load balancer, LAMP stack, database, 
UI).
There comes the problem of Docker container networking: How can we 
interconnect different Docker containers spawned potentially across different 
hosts when we do not know beforehand on which host each container will be 
created?
One pretty neat open-source solution for this is weave. This tool makes 
interconnecting multiple Docker containers pretty much hassle-free. When I 
say this, I really mean it.
In this tutorial, I am going to demonstrate how to set up Docker networking 
across different hosts using weave.
How Weave Works
Let's first see how weave works. Weave creates a network of peers, where 
each peer is a virtual router container called weave router residing on a distinct 
host. The weave routers on different hosts maintain TCP connections among 
themselves to exchange topology information. They also establish UDP 
connections among themselves to carry inter-container traffic. A weave 

router on each host is then connected via a bridge to all other Docker 
containers created on the host. When two containers on different hosts want 
to exchange traffic, a weave router on each host captures their traffic via a 
bridge, encapsulates the traffic with UDP, and forwards it to the other router 
over a UDP connection.
Each weave router maintains up-to-date weave router topology information, 
as well as container's MAC address information (similar to switch's MAC 
learning), so that it can make forwarding decision on container traffic. Weave 
is able to route traffic between containers created on hosts which are not 
directly reachable, as long as two hosts are interconnected via an intermediate 
weave router on weave topology. Optionally, weave routers can be set to 
encrypt both TCP control data and UDP data traffic based on public key 
cryptography.
Prerequisite
Before using weave on Linux, of course you need to set up Docker 
environment on each host where you want to run Docker containers. Check 
out these tutorials on how to create Docker containers on Ubuntu or 
CentOS/Fedora.
Once Docker environment is set up, install weave on Linux as follows. 
xterm
$ wget
https://github.com/zettio/weave/releases/download/lates t_release/weave 
$ chmod a+x weave
$ sudo cp weave /usr/local/bin
Make sure that /usr/local/bin is include in your path variable by
appending the following in /etc/profile.
export PATH="$PATH:/usr/local/bin" 

Repeat weave installation on every host where Docker containers will be 
deployed.
Weave uses TCP/UDP 6783 port. If you are using firewall, make sure that 
these port numbers are not blocked by the firewall.
Launch Weave Router on Each Host
When you want to interconnect Docker containers across multiple hosts, the 
first step is to launch a weave router on every host.
On the first host, run the following command, which will create and start a 
weave router container.
term
lb_ $ sudo weave launch
The first time you run this command, it will take a couple of minutes to 
download a weave image before launching a router container. On successful 
launch, it will print the ID of a launched weave router.
To check the status of the router, use this command:
term
T__ $ sudo weave status

dev@alice:~$ sudo weave launch 
e444bbf20847e9a21c4b77eedeeeef27335c016a051e8e8aca6a9OdOdac0ed64 
dev@alice:~$ 
dev@alice:~$ 
dev@alice:~$ sudo weave status 
weave router 0.9.0 
Encryption off 
Our name is 7a:cb:45:4a:7e:e2
Sniffing traffic on &{9 65535 ethwe 7e:aa:25:65:d3:43 up|broadcast|multicast} 
MACs: 
7a:cb:45:4a:7e:e2 -> 7a:cb:45:4a:7e:e2 (2015-03-19 03:15:59.227190155 +0000 UTC) 
7e:aa:25:65:d3:43 -> 7a:cb:45:4a:7e:e2 (2015-03-19 03:15:58.902470533 +0000 UTC) 
06:fe:cl:e4:c9:8e > 7a:cb:45:4a:7e:e2 (2015-03-19 03:15:59.179437142 +0000 UTC) 
Peers: 
Peer 7a:cb:45:4a:7e:e2 (v0) (UID 17597211240893378141)
Routes: 
unicast: 
\
7a:cb:45:4a:7e:e2 -> 00:00:00:00:00:00 
\
broadcast: 
7a:cb:45:4a:7e:e2 > [] 
Reconnects: 
dev@alice:~$
Since this is the first weave router launched, there will be only one peer in the 
peer list.
You can also verify the launch of a weave router by using docker command.
xterm
T $ docker ps

dev@alice:~$ docker ps
CONTAINER ID 
INAGE 
COMMAND 
CREATED STATUS
PORTS 
NAMES
e444bbf20847 
zettio/weave:0.9.0 
"/home/weave/weaver 
15 minutes ago Up 15 minutes
0.0.0.0:6783->6783/tcp, 0.0.0.0:6783->6783/udp weave
dev@alice:~$
On the second host, run the following command, where we specify the IP 
address of the first host as a peer to join.
xterm
lb_ $ sudo weave launch <first-host-IP-address>
__ ^^Bwhen you check the status of the router, you will see two peers: the 
current host and the first host.

dev@bob:~$ sudo weave launch 172.16.253.155
46705b671ba6fd5a4d74f9390acl6f5ad77acl2dd887e8bdade634b782a2703a
dev@bob:~$ 
\
dev@bob:~$ sudo weave status
weave router 0.9.0
Encryption off
Our name is 7a:45:cc:5c:3e:54
Sniffing traffic on &{11 65535 ethwe 0e:6a:92:7f:80:If up|broadcast|multicast}
MACs:
0e:6a:92:7f:80:If -> 7a:45:cc:5c:3e:54 (2015-03-19 03:20:30.779001375 +0000 UTC) 
12:49:d2:06:a9:27 -> 7a:45:cc:5c:3e:54 (2015-03-19 03:20:31.341274424 +0000 UTC) 
7a:45:cc:5c:3e:54 -> 7a:45:cc:5c:3e:54 (2015-03-19 03:20:31.453149845 +0000 UTC)
Peers:__________________________________________
Peer 7a:45:cc:5c:3e:54 (v2) (UID 7833608821879461754)
-> 7a:cb:45:4a:7e:e2 [172.16.253.155:6783]
Peer 7a:cb:45:4a:7e:e2 (v2) (UID 17597211240893378141)
-> 7a:45:cc:5c:3e:54 [172.16.253.157:58824]
Routes:
unicast:
7a:45:cc:5c:3e:54 -> 00:00:00:00:00:00
7a:cb:45:4a:7e:e2 -> 7a:cb:45:4a:7e:e2
broadcast: 
twuq neers
*
 self (bob) and slice
7a:45:cc:5c:3e:54 -> [7a:cb:45:4a:7e:e2] 
’
7a:cb:45:4a:7e:e2 -> []
Reconnects:
As you launch more routers on subsequent hosts, the peer list will grow 
accordingly. When launching a router, just make sure that you specify any 
previously launched peer's IP address.
At this point, you should have a weave network up and running, which 
consists of multiple weave routers across different hosts.

Interconnect Docker Containers across Multiple 
Hosts
Now it is time to launch Docker containers on different hosts, and 
interconnect them on a virtual network.
Let's say we want to create a private network 10.0.0.0/24, to interconnect two 
Docker containers. We will assign random IP addressses from this subnet to 
the containers.
When you create a Docker container to deploy on a weave network, you need 
to use weave command, not docker command. Internally, the weave command 
uses docker command to create a container, and then sets up Docker 
networking on it.
Here is how to create a Ubuntu container on hostA, and attach the container to 
10.0.0.0/24 subnet with an IP addresss 10.0.0.1.
xterm
Bi_ hostA:~$ sudo weave run 10.0.0.1/24 -t -i ubuntu
__ ^^Bon successful run, it will print the ID of a created container. You 
can use this ID to attach to the running container and access its console as 
follows.
xterm
Hl_ hostA:~$ docker attach <container-id>
__ ^^BlMove to hostB, and let's create another container. Attach it to the 
same subnet (10.0.0.0/24) with a different IP address 10.0.0.2.
xterm
Bi_ hostB:~$ sudo weave run 10.0.0.2/24 -t -i ubuntu
__ ^^^Let's attach to the second container's console as well:
xterm 
__________ xterm
hostB:~$ docker attach <container-id>
_ l^lAt this point, those two containers should be able to ping each other 
via the other's IP address. Verify that from each container's console.

dev@alice:-$ sudo weave run 10.0.0.1/24 -t -i ubuntu
285af7488997bdcca412fe59ffa2c78475eeb75e0ed8b5952bd2b6b901ae9ef0
dev@alice:-$
dev@alice:-$ docker attach 285af7488997bdcca412fe59ffa2c78475eeb75e0ed8b5952bd2b6b901ae9ef0
root@285af7488997:/#
:root@285af7488997:/#
:root(8285af7488997:/# ping 10.0.0.2
PING 10.0.0.2 (10.0.0.2) 56(84) bytes of data.
64 bytes from 10.0.0.2: icmp_seq=l ttl=64 time=45.6 ms
64 bytes from 10.0.0.2: icmp_seq=2 ttl=64 time=2.02 ms
64 bytes from 10.0.0.2: icmp_seq=3 ttl=64 time=1.83 ms
64 bytes from 10.0.0.2: icmp_seq=4 ttl=64 time=1.51 ms
64 bytes from 10.0.0.2: icmp_seq=5 ttl=64 time=1.83 ms
AC 
’
— 10.0.0.2 ping statistics —
5 packets transmitted, 5 received, 0% packet loss, time 4010ms
rtt min/avg/max/mdev = 1.516/10.572/45.655/17.542 ms
root@285af7488997:/#
If you check the interfaces of each container, you will see an interface named 
ethwe which is assigned an IP address (e.g., 10.0.0.1 and 10.0.0.2) you 
specified.

root@285af7488997:/# ifconfig
eth0 Link encap:Ethernet HWaddr 6e:62:ef:01:50:34
inet addr:172.17.0.4 Beast:0.0.0.0 Mask:255.255.0.0 
inet6 addr: fe80::6c62:efff:fe01:5034/64 Scope:Link 
UP BROADCAST RUNNING MTU:1500 Metric:1
RX packets:8 errors:© dropped:0 overruns:© frame:0
TX packets:8 errors:© dropped:© overruns:© carrier:© 
collisions:© txqueuelen:l©0©
RX bytes:648 (648.0 B) TX bytes:648 (648.0 B)
ethwe Link encap:Ethernet HWaddr f2:8b:©d:52:be:21
inet addr:10.0.0.1 Beast:0.0.0.0 Mask:255.255.255.0 
inet6 addr: fe80::f08b:dff:fe52:be21/64 Scope:Link 
UP BROADCAST RUNNING MULTICAST MTU:65535 Metric:1 
RX packets:23 errors:© dropped:© overruns:© frame:© 
TX packets:20 errors:© dropped:© overruns:© carrier:© 
collisions:© txqueuelen:l©0©
RX bytes:187© (1.8 KB) TX bytes:1544 (1.5 KB)
Other Advanced Usages of Weave
Weave offers a number of pretty neat features. Let me briefly cover a few 
here.
Application Isolation
Using weave, you can create multiple virtual networks and dedicate each 
network to a distinct application. For example, create 10.0.0.0/24 for one group 
of containers, and 10.10.0.0/24 for another group of containers, and so on.
Weave automatically takes care of provisioning these networks, and isolating 
container traffic on each network. Going further, you can flexibly detach a 
container from one network, and attach it to another network without 
restarting containers. For example:
First launch a container on 10.0.0.0/24:
term
$ sudo weave run 10.0.0.2/24 -t -i ubuntu
Detach the container from 10.0.0.0/24:

term
$ sudo weave detach 10.0.0.2/24 <container-id>
Re-attach the container to another network 10.10.0.0/24:
term
__ $ sudo weave attach 10.10.0.2/24 <container-id>

dev(3bob:~$ sudo weave run 10.0.0.2/24 -t -i ubuntu
76aecOff5c5Of30727eca8ddc0522b7abff26b7f775a549becc0045Odd6876b4
dev(dbob:~$_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
dev@bob:~$ sudo weave detach 10.0.0.2/24 76aec0ff5c50f30727eca8ddc0522b7abff26b7f7 
75a549becc00450dd6876b4
dev@bob:~$
dev@bob:~$ sudo weave attach 10.10.0.2/24 76aec0ff5c50f30727eca8ddc0522b7abff26b7f
775a549becc00450dd6876b4
dev@bob:~$ 
w
dev@bob:~$ docker attach 76aec0ff5c50f30727eca8ddc0522b7abff26b7f775a549beccO0450d
d6876b4 
\ Detach from one network
root@76aec0ff5c50:/# 
' and attach to another network
root@76aec0ff5c50:/# ifconfig
eth0 Link encap:Ethernet HWaddr 02:38:90:d5:5f:63
inet addr:172.17.0.4 Bcast:0.O.0.O Mask:255.255.0.0
inet6 addr: fe80::38:90ff:fed5:5f63/64 Scope:Link
UP BROADCAST RUNNING MTU:1500 Metric:l
RX packets:? errors:0 dropped:2 overruns:0 frame:0
TX packets:8 errors:© dropped:© overruns:0 carrier:© 
collisions:0 txqueuelen:1000
RX bytes:558 (558.0 B) TX bytes:648 (648.0 B) < Updated!
ethwe Link encap:Ethernet HWaddr b2:a5:b6:79:0e:7c
inet addr:10.10.0.2 Bcast:0.0.0.0 Mask:255.255.255.0
inet6 addr: fe80::b0a5:b6ff:fe79:e7c/64 Scope:Link
UP BROADCAST RUNNING MULTICAST MTU:65535 Metric:1
RX packets:8 errors:© dropped:0 overruns:0 frame:©
Now this container should be able to communicate with other containers on 
10.10.0.0/24. This is a pretty useful feature when network information is 
not available at the time you create a container.

Integrate Weave Networks with Host Network
Sometimes you may need to allow containers on a virtual weave network to 
access physical host network. Conversely, hosts may want to access 
containers on a weave network. To support this requirement, weave allows 
weave networks to be integrated with host network.
For example, on hostA where a container is running on network 10.0.0.0/24, run 
the following command.
term
T hostA:~$ sudo weave expose 10.0.0.100/24
This will assign IP address 10.0.0.100 to hostA, so that hostA itself is also 
connected to 10.0.0.0/24 network. Obviously, you need to choose an IP address 
which is not used by any other containers on the network.
At this point, hostA should be able to access any containers on 10.0.0.0/24, 
whether or not the containers are residing on hostA. Pretty 
neat!
Conclusion
As you can see, weave is a pretty useful Docker networking tool. This tutorial 
only covers a glimpse of its powerful features. If you are more ambitious, you 
can try its multi-hop routing, which can be pretty useful in multi-cloud 
environment, dynamic re-routing, which is a neat fault-tolerance feature, or 
even its distributed DNS service which allows you to name containers on 
weave networks. If you decide to use this gem in your environment, feel free 
to share your use case!
Linux Tutorials
What are good web server

benchmarking tools for Linux
As far as web server performance is concerned, there are many different 
factors at play, e.g., front-end application design, network latency/bandwidth, 
web server configuration, server-side in-memory cache, raw hardware 
capability, server load of shared hosting, etc. To compare and optimize web 
server performance under such a wide array of factors, we often perform load 
test (or stress test) using a web server micro-benchmark tool. A typical 
benchmark tool injects synthetic workloads or replays real-world traces to a 
web server, and measures web server performance and scalability in terms of 
varying metrics (e.g., response time, throughput, number of requests per 
second, CPU load, etc).
For those of you who want to find out how your web server or web service 
will measure up under different workload conditions, here are a list of web 
server benchmark tools available on Linux platforms.
1. ApacheBench
ApacheBench (ab) is a standard command-line web server benchmark tool 
bundled with Apache HTTP server. It can send an arbitrary list of 
(concurrent) web requests. Support for POST/PUT/GET requests, as well as 
basic password authentication is available. Testing results include requests 
per second, time per request, transfer rate, connection time statistics (min, 
max, median, mean), etc. License: Apache v2.0.

dev@devhost:ab -c2 -nl00 http://mit.edu/
This is ApacheBench, Version 2.3 <$Revision: 1604373 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/
Benchmarking mit.edu (be patient).... done
Server Software: 
AkamaiGHost
Server Hostname: 
mit.edu
Server Port: 
80
Document Path: 
/
Document Length: 
0 bytes
Concurrency Level: 
2
Time taken for tests: 
1.832 seconds
Complete requests: 
100
Failed requests: 
0
Non-2xx responses: 
100
Total transferred: 
16100 bytes
HTML transferred: 
0 bytes
Requests per second: 
54.58 [#/sec] (mean)
Time per request: 
36.642 [ms] (mean)
Time per request: 
18.321 [ms] (mean, across all concurrent requests)
Transfer rate:
8.58 [ Kbytes/sec]
received
Connection Times
(ms)
min
mean [+/-sd]
median max
Connect:
15
18
1.6
17
23
Processing : 
16
19
1.3
19
23
Waiting:
16
18
0.9
18
21
Total:
33
36
2.2
36
42
2. Apache JMeter
Apache JMeter is a cross-platform Java-based GUI program designed to 
stress test any web application. It can be used to test the performance of 
webserver backends powered by server-side languages (e.g., PHP, Java, 
ASP.NET) or databases (e.g., JDBC, LDAP, MongoDB). It provides highly 
pluggable testing architecture via extensible data visualization GUI. License: 
Apache v2.0.

3. curl-loader
curl-loader is a command-line application workload generator which can 
simulate multiple HTTP/HTTPS FTP/FTPS clients. Simulated clients can 
conduct various tasks, such as authenticated login (POST or GET/POST), 
GET/POST/PUT requests from batch configuration with probabilistic 
distribution, FTP passive/active operations, HTTP logoff (POST, 
GET/POST, GET with cookie), etc. Per-client status and statistics are logged 
to a file. License: GPLv2.
4. FunkLoad
FunkLoad is a web server load testing tool written Python. It can perform 
functional unit testing, as well as stress and longevity testing. Features 
include GET/POST/PUT/DELETE requests, basic authentication, cookie, 

HTTPS with SSL/TLS, browser cache emulation, and CSS/image/JavaScript 
fetching. License: GNU GPL.
5. Gatling
Gatling is an open-source protocol-agnostic load testing tool primarily used 
to benchmark HTTP servers and web services. Using a lightweight 
asynchronous testing engine, it can easily simulate thousands of concurrent 
users whose web browsing behaviors and scenarios (e.g., login, browse 
product listings, add a product to cart, check out) are independently scripted. 
It supports realtime reports via the Graphite protocol, and can be integrated 
via extensions with other third-party building tools such as Maven, Jenkins, 
SBT. License: Apache v2.0.

6. Httperf
Httperf is an HTTP workload generator command-line tool which can 
generate a number of different types of HTTP traffic, including 
GET/HEAD/PUT/POST requests, HTTP pipelining, SSL traffic, stateful 
sessions with cookie, etc. Output includes connection rate, connection time 
statistics (min, max, median, stddev), request/reply rate, and network 
throughput. License: GNU GPLv2.
7. Pylot
Pylot is a Python-based performance and scalability testing tool for web 
services. It generates multi-agent workload scenarios based on test cases 
defined in an XML file, and displays stats and error reporting results in 
realtime. It supports HTTPS/SSL, cookie handling, regular expression based 
response verification, multi-threading, console/GUI modes. License: GNU 
GPLv3.
8. Siege
Siege is an HTTP load testing and benchmarking tool for terminal 

environment. Support for basic password authentication, cookies, HTTPS 
with SSL is available. License: GNU GPL.
9. The Grinder
The Grinder is a Java-based multi-threaded test framework which can 
perform load test and functional test of various application and network 
protocols written in Java APIs, including as HTTP servers, SOAP, XML- 
RPC, REST web services, JMS, JDBC, RMI, and POP3/SMTP/LDAP. It 
supports dynamic loading and monitoring of test scripts written in Jython and 
Clojure languages, and allows injecting load from multiple machines in the 
distributed fashion. Its HTTP support includes cookie handling, SSL, 
connection ratelimiting, trace record and replay, proxy, etc.
10. Tsung
Tsung is an open-source multi-protocol stress test tool which can generate 
different types of workloads for HTTP, SSL, WebDAV, SOAP, 
PostgresSQL, MySQL, LDAP, XMPP servers. With HTTP server testing, it 
supports basic requests (GET/POST/PUT/DELETE/HEAD), cookies, 
authentication with password or oAuth, SOAP, graph visualization and 
HTML report, multiple IP addresses via IP aliasing, etc. License: GNU 
GPLv2.

11. Web Polygraph
Web Polygraph is a workload generator tool that can simulate HTTP, FTP, 
SSL traffic for benchmarking. It comes with HTTP client and server which, 
together, can stress test caching proxies, web server accelerators, content 
filters, etc. Support for LDAP credentials, basic/NTLM/Kerberos 
authentication is available. License: Apache v2.0.
12. Wrk

wrk is a scalable HTTP benchmarking tool which leverages lightweight event 
notifications like epoll and kqueue. Support for LuaJIT-scripted workloads, 
HTTP pipelining, authentication token, dynamic requests, and customizable 
report is available. License: Apache v2.0.
13. Locust
Locust is an open-source load testing tool for web services. You can define 
user behaviors in your load test in Python code and easily deploy the load test 
across multiple servers to simulate a large number of concurrent users 
accessing your web service. Locust provides a web-based interface to 
monitor various performance statistics (min, max, median, 90-percentile 
latencies, request rate, sucess/failure ratio, etc). License: MIT.
Statistics Charts Failures Exceptions Download Data
HOST 
STATUS 
RPS 
FAILURES I
LOCUST 
mipv/localhost:^ RUNNING g 0%
Edit
Type
Name
# Requests
# Fails
Median 
(ms)
90%lle 
(ms)
Average 
(ms)
Mln 
(ms)
Max 
(ms)
Average size
(bytes)
Current 
RPS
Current 
Fallures/s
GET
/hello
10
0
4
5
4
3
5
176
0.6
0
GET
/item
24
0
4
5
4
3
5
183
0.7
0
POST
/login
10
0
7
18
8
5
18
269
0
0
GET
/world
10
0
4
5
4
3
5
176
0.6
0
Aggregated
54
0
4
7
5
3
18
196
1.9
0
About
Linux Tutorials
How to configure PCI-passthrough 
on virt-manager
Question: I would like to dedicate a physical network interface card to one of 
my guest VMs created by KVM. For that, I am trying to enable PCI 
passthrough of the NIC for the VM. How can I add a PCI device to a guest

VM with PCI passthrough on virt-manager?
Modern hypervisors such as KVM or Xen enable efficient resource sharing 
among multiple guest operating systems by virtualizing and emulating 
hardware resources. However, such virtualized resource sharing may not 
always be desirable, or even should be avoided when VM performance is a 
great concern, or when a VM requires full DMA control of a hardware 
device. One technique used in this case is so-called PCI passthrough, where a 
guest VM is granted an exclusive access to a PCI device (e.g., 
network/sound/video card). Essentially, PCI passthrough bypasses the 
virtualization layer, and directly exposes a PCI device to a VM. No other VM 
can access the PCI device.
Requirement for Enabling PCI Passthrough
If you want to enable PCI passthrough for an HVM guest (e.g., a 
fullyvirtualized VM created by KVM), your system (both CPU and 
motherboard) must meet the following requirement. If your VM is 
paravirtualized (created by Xen), you can skip this step.
In order to enable PCI passthrough for an HVM guest VM, your system must 
support vt-d (for Intel processors) or amd-vi (for AMD processors). Intel's 
VT-d ("Intel Virtualization Technology for Directed I/O") is available on 
most high-end Nehalem processors and its successors (e.g., Westmere, Sandy 
Bridge, Ivy Bridge). Note that VT-d and VT-x are two independent features. 
A list of Intel/AMD processors with VT-d/AMD-Vi capability can be found 
here.
After you verify that your host hardware supports VT-d/AMD-Vi, you then 
need to do two things on your system. First, make sure that vt-d/amd-vi is 
enabled in system BIOS. Second, enable iommu on your kernel during 
booting. The IOMMU service, which is provided by VT-d,/AMD-Vi, protects 
host memory access by a guest VM, and is a requirement for PCI passthrough 
for fully-virtualized guest VMs.
To enable IOMMU on the kernel for Intel processors, pass intel_iommu=on boot 
parameter on your Linux. Check out this tutorial to find out how to add this 

boot parameter via GRUB.
After configuring the boot parameter, reboot your host.
Add a PCI Device to a VM on Virt-Manager
Now we are ready to enable PCI passthrough. In fact, assigning a PCI device 
to a guest VM is straightforward on virt-manager.
Open the VM's settings on virt-manager, and click on Add Hardware button on the 
left sidebar.
Choose a PCI device to assign from a PCI device list, and click on Finish 
button.
Finally, power on the guest. At this point, the host PCI device should be 
directly visible inside the guest VM.

Troubleshooting
If you see either of the following errors while powering on a guest VM, the 
error may be because VT-d (or IOMMU) is not enabled on your host. 
Error starting domain: unsupported configuration: host doesn't support 
passthrough of host PCI devices
Error starting domain: Unable to read from monitor: Connection reset by peer
Make sure that intel_iommu=on boot parameter is passed to the kernel during 
boot as described above.
Linux Tutorials
How to find the number of CPU
cores on Linux
Multi-core CPU processors are common nowadays, including dual-core 
processors (e.g., Intel Core Duo), quad-core processors (e.g., Intel Core i5), 
and hexa-core processors (e.g., AMD Phenom II X6). Also, many server­
grade physical machines are equipped with more than one CPU processor. In 
order to find the number CPUs, and the number of cores per CPU, you can 
refer to
/proc/cpuinfo.

A sample /proc/cpuinfo of HP Proliant DL 380 G7 server is as follows. The HP 
Proliant server is equipped with two Intel Xeon 5600 series processors.
processor : 0
vendor_id : GenuineIntel
cpu family : 6
model : 44
model name : Intel(R) Xeon(R) CPU E5620 @ 2.40GHz stepping : 2
cpu MHz : 2399.316
cache size : 12288 KB
physical id : 0
siblings : 8
core id : 0
cpu cores : 4
apicid : 0
initial apicid : 0
fpu : yes

fpu_exception : yes
cpuid level : 11
wp : yes
flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat 
pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb 
rdtscp lm constant_tsc arch_perfmon pebs bts rep_good pni monitor ds_cpl 
vmx smx est tm2 ssse3 cx16 xtpr dca sse4_1 sse4_2 popcnt lahf_lm ida 
bogomips : 4802.28
clflush size : 64
cache_alignment : 64
address sizes : 40 bits physical, 48 bits virtual
power management:

Find the Number of Physical CPUs
The following command will show the number of physical CPU processors 
on board.
xterm
lb_ $ cat /proc/cpuinfo | grep "Aphysical id" | sort | uniq | wc -l
_ ^M2
Find the Number of Cores per CPU
If you want to know how many cores each physical CPU processor has, use 
the following command.
I_____ ^^Ixterm

$ cat /proc/cpuinfo | grep "Acpu cores" | uniq
cpu cores : 4
Find the Total Number of Processors
The total number of processors (or cores) available is the number of physical 
CPUs multiplied by the number of cores per CPU.
term
T $ cat /proc/cpuinfo | grep "Aprocessor" | wc -l
_ ^Ml6
Note that Intel Xeon 5600 series processors have Intel Hyper-Threading 
capability. So each core shows up as two processors in Linux. In this case, 
they are referred to as logical processors. Thus the total processor count seen 
by Linux is 16 (= 2 x 4 x 2).
Linux Tutorials
How to run program or process on 
specific CPU cores on Linux
As multi-core CPUs become increasingly popular on server-grade hardware 
as well as end-user desktop PCs or laptops, there have been growing efforts 
in the community (e.g., in terms of programming models, compiler or 
operating system support) towards developing applications optimized for 
multi-core architecture.

One operating system (OS) support often exploited to run performancecritical 
applications on multi-core processors is so-called processor affinity or CPU 
pinning. This is an OS-specific feature that binds a running process or 
program to particular CPU core(s).
Binding a program to specific CPU cores can be beneficial in several 
scenarios. For example, when an application with highly cache-bound 
workload runs together with other CPU-intensive jobs, pinning the 
application to a specific CPU would reduce CPU cache misses. Also, when 
two processes communicate via shared memory intensively, scheduling both 
processes on the cores in the same NUMA domain would speed up their 
performance.
This tutorial describes how to run a program or process on specific CPU 
cores on Linux.
To assign particular CPU cores to a program or process, you can use taskset, a 
command line tool for retrieving or setting a process' CPU 
affinity on Linux.
Install taskset on Linux
The taskset tool is part of util-linux package in Linux, and most Linux distros 
come with the package pre-installed by default. If taskset is not available on 
your Linux system (like in minimal Docker containers), install it as follows.
Install taskset on Debian, Ubuntu or Linux Mint
xterm
$ sudo apt-get install util-linux
Install taskset on CentOS, Fedora or RHEL

term
Hb_ $ sudo yum install util-linux
View the CPU Affinity of a Running Process
To retrieve the CPU affinity information of a process, use the following 
command. Note that taskset returns the current CPU affinity in a hexadecimal 
bitmask format.
■_$ taskset -p <PID>
__^^Hl;or example, to check the CPU affinity of a process with PID 2915:
term
$ taskset -p 2915 
pid 2915's current affinity mask: ff
In this example, the returned affinity (represented in a hexadecimal bitmask) 
corresponds to 11111111 in binary format, which means the process can run on 
any of eight different CPU cores (from 0 to 7).
The lowest bit in a hexadecimal bitmask corresponds to core ID 0, the second 
lowest bit from the right to core ID 1, the third lowest bit to core ID 2, etc. So 
for example, a CPU affinity 0x11 represents CPU core 0 and 4.
taskset can show CPU affinity as a list of processors instead of a bitmask, 
which is easier to read. To use this format, run taskset with -c option. For 
example:
term
T $ taskset -cp 2915

pid 2915's current affinity list: 0-7
Pin a Running Process to Particular CPU Core(s)
Using taskset, you can pin (or assign) a running process to particular CPU 
core(s). For that, use the following command.
xterm
■l _$ taskset -p <COREMASK> <PID>
$ taskset -cp <CORE-LIST> <PID>
___^^Bfop example, to assign a process to CPU core 0 and 4, do the 
following.
xterm
lb_ $ taskset -p 0x11 9030
__ ^^^^^^Bpid 9030's current affinity mask: ff
pid 9030's new affinity mask: 11
Or equivalently:
xterm
lb_ $ taskset -cp 0,4 9030
__ ^^■With -c option, you can specify a list of numeric CPU core IDs 
separated by commas, or even include ranges (e.g., 0,2,5,6-10).
Note that in order to be able to change the CPU affinity of a process, a user 
must have cap_sys_nice capability. Any user can view the affinity mask of a 
process.
Launch a Program on Specific CPU Cores

taskset also allows you to launch a new program as pinned to specific CPU 
cores. For that, use the following command.
xterm
■l _$ taskset <COREMASK> <EXECUTABLE>
___^^Hfop example, to launch vlc program on a CPU core 0, use the 
following command.
xterm
lb_ $ taskset 0x1 vlc
Dedicate a Whole CPU Core to a Particular
Program
While taskset allows a particular program to be assigned to certain CPUs, that 
does not mean that no other programs or processes will be scheduled on those 
CPUs. If you want to prevent this and dedicate a whole CPU core to a 
particular program, you can use isolcpus kernel parameter, which allows you to 
reserve the CPU core during boot.
Add the kernel parameter isolcpus=<cpu_id> to the boot loader during boot or 
GRUB configuration file. Then the Linux scheduler will not schedule any 
regular process on the reserved CPU core(s), unless specifically requested 
with taskset. For example, to reserve CPU cores 0 and 1, add
isolcpus=0,1 kernel parameter. Upon boot, then use taskset to safely assign the 
reserved CPU cores to your program.
Linux Tutorials
How to check the changelog of a 
package on Linux
Question: When I install or upgrade a package on Linux, I want to find out 

what changes (e.g., new features or bug fixes) have been made in the package 
before or after installing it. How can I view the changelog of a (Deb/RPM) 
package on [Debian, Ubuntu, CentOS, Fedora]?
When a program or a library is packaged as a Deb or RPM package for 
distribution, several metadata files are included in the package. One of them 
is a changelog file, which records in reverse chronological order what changes 
occurred each time the package has been updated.
So if you want to find out what changes are made to the package that you are 
about to install or already installed, you can view the package's changelog. Here 
is how to check the changelog on Debian-based or Red-Hat based Linux.
View the Changelog of a Deb Package
There are several ways to view the changelog of an (installed or uninstalled) 
Deb package on Debian based Linux.
Method One: apt-get or aptitude
The latest apt-get allows you to check the changelog of a package, whether or 
not it is installed on your system.
I_____ ^^Ixterm
lb_ $ apt-get changelog <package-name> | more

dan@poweredge:-$[apt-get changelog iftop | more]
Get:l Changelog for iftop (http://changelogs.ubuntu.com/changelogs/pool/universe 
/i/iftop/iftop_1.0~pre4-2/changelog) [12.9 kB] 
iftop (1.0~pre4-2) unstable; urgency=medium
* Upload to unstable.
* Add MAC-address-format.patch which restores the MAC address formatting to 
its former behaviour.
Thanks Xiaoguang Sun who reported this issue on iftop's mailing list.
-- Markus Koschany <apo@gambaru.de> Tue, 11 Mar 2014 13:21:55 +0100
iftop (1.0~pre4-l) experimental; urgency=medium
* Imported Upstream version 1.0~pre4.
* Drop all patches. Merged upstream.
* Update debian/copyright for new release.
-- Markus Koschany <apo@gambaru.de> Tue, 25 Feb 2014 22:48:37 +0100
iftop (1.0~pre2-5) unstable; urgency=low
* [76d626d] Drop NEWS file. It is obsolete.
aptitude , another command-line package manager, comes with the same option 
as apt-get to show the changelog of a package. aptitude comes pre-installed on all 
Debian-based distros except for Ubuntu desktop.
xterm
$ aptitude changelog <package-name> | more
___Ha nice thing about aptitude is that it can be supplemented with ncurses- 
based user interface.

To open ncurses-enhanced aptitude:
xterm
T $ aptitude-curses
__ Ko search for a particular package on aptitude-curses, press / and type
its name. On the description page of a package, press C to see its changelog.
Actions Undo |
C-T: Menu ?: Hr 
Packag
Package Resolver Search Options Views Help 
ownload/Install/Remove Pkgs 
fo
iftop (1.0~pre2
_ Install 
+
3 Remove 
-
iftop changes
Purge
Keep
Hold
Mark Auto
Mark Manual
Forbid Version
Information 
enter
Cycle Package Information i
Changelog 
C
It.
latest upstream release, 
reconf instead of autotools-dev 
uild time and to ensure that 
s to Graham Inggs for the report.
yright format 1.0.
*
*
*
*
*
L
*
*
[76d626d] 0 
[fdf86bf] 0 
■ Source f 
[e2651cf] F 
[2f7ea27]
to recreate 
iftop can b 
(Closes: #6 
[92cec5f] U 
• Add miss
[e8e73e0] Add man-page.patch.
• Fix errors in man page and lintian warring hyphen-used-as-minus-sign. 
Bump Standards-Version to 3.9.5, no changes.
Markus Koschany <apo@gambaru.de> Thu, 31 Oct 2313 20:06:59 +0100
iftop (1.0~pre2-4) unstable; urgency=low
* [488690e] New Maintainer. (Closes: #726549)
Method Two: Synaptic

If you are a desktop user on Debian/Ubuntu-based system, an additional 
option to view a package's changelog is via synaptic, a graphical package 
management tool for Deb packages.
You can install it on Debian-based systems with:
xterm
T $ sudo apt-get install synaptic
_J^BOnce you launch synaptic, you can easily check the changelog by 
clicking on Get Changelog button on any package description page.

Synaptic Package Manager
File Edit Package Settings Help
iftop Changelog
& J illI ®
Quick filter
Q Search
v 
or
Reload Mark All Upgrades
Apply Properties
iftop
All
S Package
Installed Version
Latest Version
Size Description
LP-PPA-mjblenner-ppa-hal/now
1 D ODA miklannor nm k-d/t-r.irh
1 iftop
1.0-pre2-5
1.0~pre2-5
112 kB displays bandwidl
Local
Local/main(apt.insync 
Local/mainfsecurity.ub 
Local/main(us.archive. 
Local/multiverse (us.ar 
Local/universe (us.arch 
stable/main (dl.google 
trusty/main (archives 
trusty/main(us.archive
Complete changelog of the latest version: 
iftop (1.0~pre2-5) unstable; urgency=low
*[76d626d] Drop NEWS file. It is obsolete.
*[fdf86bf] Drop README.source.
-Source format 3.0 uses quilt by default.
* 
rfl Fly unlrh fil? and d pfpr f rhp ht
* 
[2f7ea27] Build with -parallel and autoreconf instead of autotools-dev 
to recreate the whole build system at build time and to ensure that 
iftop can be built twice in a row. Thanks to Graham Inggs for the report.
t
Sections
Status
Origin
* [92cec5f] Update d ebian/copyright to copyright format 1.0.
-Add missing BSDlicenses.
*[e8e73e0] Addman-page.patch.
■ Fix errors in man page and lintian warning hyphen-used-as-minus-sign.
Custom Filte
Search Result
Close
Architecture^^^ 
2 packages listed, 3529 installed, 0 broken. 0 to install/upgrade, 0 to remove

Method Three (Ubuntu-specific): Software-Updater
Another GUI method, which is specific to Ubuntu desktop, is via Software 
Updater. This GUI tool alerts you of any new Ubuntu software updates, and 
installs them on your command. You can use Software Updater to check the 
changes made in any to-be-installed software. Note that Software Updater 
cannot show the changelog of any arbitrary package as all the other methods do.
Once it is launched with:
xterm
T $ update-manager
__ ^Hlt can show the changelog of packages to-be installed (but not 
downloaded yet).

oe® Software Updater
Updated software is available For this computer. Do you want to install it now?
’ Details of updates
Install
Download
8 B GNU C Library: detached debugging symbols
8 B Office productivity suite-English american help
8u Public headers and libs for Xen
8u Public libs for Xen
8 B Transitional English language pack forThunderbird
8 B Transitional package to ensure multiarch compatibility
8 U Xenstore communications library for Xen
► 8® Ubuntu base 
Change Log
3 Other updates
3.5 MB
2.6 MB
521 kB
323 kB
11 kB
8 kB
19 kB
200.7 MB
13.1MB
Changes for rhythmbox versions: 
Installed version: 3.2.1-1ubuntu3 
Available version: 3.2.l-1ubuntu3.1
' 12 Rhythmbox Music Player
1.4 MB
► 8 B Software JiUpdates^X^
2.5 MB
► 8 ® Ubuntu baseX^
9.2 MB
TTechnica 
Changes
desojptltfn
Description
Version 3.2.1-1ubuntu3.1:
*debian/patches/restore-traditional-menubar.patch:
- use correct keybinding label for play/pause (Ip: S1274726)
* debian/oatches/ait svnc ootion.oatch:
339.2 MB will be downloaded.
Settings...
Remind Me Later Install Now

Method Four: /usr/share/doc
If you want to check the changelog of any already-installed package, you can 
simply read changelog files installed on your system as follows.
$ zless /usr/share/doc/<package
name>/changelog.Debian.gz
$ zless /usr/share/doc/<package-name>/changelog.gz
View the Changelog of an RPM Package
There are several ways to view the changelog of an (installed or uninstalled) 
RPM package on CentOS/RHEL/Fedora-based Linux.
Method One: rpm
If you want to check the changelog of any installed package, you can use rpm 
command as follows.
term
lb_ $ rpm -q --changelog <package-name> | more

[dev@centos ~]$ rpm -q --changelog tcpdump | more
* Thu Jun 18 2015 Michal Sekletar <msefcleta@re31iat.com> ■ 14:4.5.1-3
• add support for nano second timestamps (#1151406)
• fix cdp dissector, allow zero-length data frames (#1231246)
* Fri Jan 24 2014 Daniel Mach <dmach@redhat.com> - 14:4.5.1-2 
• Mass rebuild 2014-01-24
* Wed Jan 15 2014 Michal Sekletar <
> - 14:4.5.1-1
msekleta@redhat.com
- update to 4.5.1
* Fri Dec 27 2013 Daniel Mach <dmach@redhat.com> - 14:4.5.0-2.20131108gitb07944a
- Mass rebuild 2013-12-27
* Fri Nov 08 2013 Michal Sekletar <msekleta@redhat.com> - 14:4.5.0-1.20131108gitb07944a 
- update to snaphot gitb07944a (#1026855)
-don't try to change ownership of stdout (#1015767)
* Thu Jun 06 2013 Michal Sekletar <msekleta@redhat.com> - 14:4.4.0-1
- update to 4.4.0
* Fri Feb 15 2013 Fedora Release Engineering <rel-eng@lists.fedoraproject.org> - 14:4.3 
.0-3
Method Two: repoquery
If you want to check the changelog of a package which is not installed on your 
system, you cannot use rpm command. Instead, you can use the repoquery 
command which will work whether or not a package is installed. You can 
install repoquery with:
xterm
Bl_ $ sudo yum install yum-utils

To see the changelog of any arbitrary package using repoquery:
term
$ repoquery --changelog <package-name> | more
Method Three: yum-changelog
Another way to view a package's changelog is via yum's changelog plugin. Install 
the plugin as follows.
xterm
$ sudo yum install yum-changelog
__ ^^■iNow you can use yum command to see the changelog of individual 
packages before/after installing them.
xterm
lb_ $ yum changelog <package-name>
__ l^lThe changelog plugin has additional options to customize the 
changelog view. For example:
To view the 5 most recent changelogs of a package:
xterm
T $ yum changelog 5 <package-name>
LJ view all changelogs of a package since 2015/06/01:
term
$ yum changelog "2015-06-01" <package-name>

[dev@centos ~]$,yum changelog "2016-01-25" kernel
Loaded plugins: changelog 77astestmirror" 
Loading mirror speeds from cached hostfile , 
* base: mirror.lug.udel.edu 
\
* elrepo: ftp.osuosl.org
* extras: 
 
OA4C/1/OC
mirror.net.cen.ct.gov
* updates: 
 Kefflfil UpdaiCS SIHC6 2016/1/25
mirror.vcu.edu
Listing changelogs since 2016-01-25
===== installed Packages ============ 
kernel-3.10.0-327.10.1.el7.x86_64 
installed
* Tue Feb 16 07:00:00 2016 CentOS Sources <bugs@centos.org> - 3.10.0-327.10.1.el 
7
- Apply debranding changes
ke rnel-3.10.0-327.4.5.el7.x86_64 
installed
* Mon Jan 25 07:00:00 2016 CentOS Sources <bugs@centos.org> - 3.10.0-327.4.5.el7 
- Apply debranding changes
changelog stats. 3 pkgs, 3 source pkgs, 2 changelogs 
[dev@centos -]$
Method Four (Fedora-specific): dnf
All three above methods still work on Fedora, except that you need to install 
yum if you have switched to dnf as a default package manager.
As of this writing, dnf does not yet provide an option to check the changelog of 
individual packages. One thing you can do with dnf is to 
show update advisories of any critical bug fixes, security patches and

enhancements (similar to Ubuntu's Software Updater). To see the changelog 
of such critical updates, run:
xterm
lb_ $ dnf updateinfo info | more
[dan@devhost ~]$|dnf updateinfo info | more|
Last metadata expiration check performed 0:03:03 ago on Fri Feb 26 21:31:22 2016
GeoIP-GeoLite-data-2015.12-l.fc23
Update ID : FEDORA-2015-a4adlb7446 
Type : enhancement
Updated : 2015-12-14 02:42:34
Description : Periodic database update.
LibRaw-0.16.2-3.fc23
Update ID : FEDORA-2015-d2fc3321O8
Type : security
Updated : 2015-12-07 12:40:03
Bugs : 1287057 • CVE-2015-8366 LibRaw: Index overflow in smal decode segm 
ent [fedora-all]
: 1287077 - CVE-2015-8367 LibRaw: Memory objects are not intialized 
properly [fedora-all]
Description : Patch for CVE-2015-8366, CVE-2015-8367
Linux Tutorials

How to make a static web site for 
free via Google App Engine
Are you planning to make or build your own homepage? For that, you may 
consider Google App Engine as one option, especially if the homepage will 
attract a small amount of daily traffic.
Google App Engine (GAE) is a platform as a service (PaaS) cloud computing 
platform for hosting web applications in Google's data centers available 
across the world. You can deploy any web application written in Java or 
Python on App Engine. GAE provides on-demand pricing model where you 
will be paying nothing as long as your application stays within daily quota (in 
terms of instance hours, datastore operations, outgoing bandwidth, etc).
In this post, I will show you how to create and host a static web site on 
Google App Engine. To follow this guide, you don't need any knowledge on 
Java or Python language.
First, sign up at http://appengine.google.com. If you already have a Google 
account, it is just a one-click process.
Log in to Google App Engine. Create a new application that will be powering 
your website. The name of your application will be part of the URL of your 
website (i.e., [your_app_name].appspot.com). So the name must be unique across all 
existing App Engine applications. You can create up to 10 applications under 
any one App Engine account.
In this example, I choose the application name: this-is-my-testapplication.

Google app engine
Create an Application
You have 8 applications remaining.
Application Identifier:
this-is-my-test-application .appspot.com Check Availability Yes- "this-is-my-test-application" is available!
All Google account names and certain offensive or trademarked names may not be used as Application Identifiers. 
You can map this application to your own domain later Learn more
Application Title:
|This is my test application
Displayed when users access your application.
Authentication Options (Advanced): Learn more
Google App Engine provides an API for authenticating your users, including Google Accounts Google Apps. and OpenlD. Ifyou choose to use this feature for 
youll need to specify now what type of users can sign in to your application:
' Open to all Google Accounts users (default)
If your application uses authentication, anyone with a valid Google Account may sign in.
Restricted to the following Google Apps domain:
e.g. Ioo.com
If your application uses authentication, only members of this Google Apps domain may sign in. If your organization uses Google Apps. use this option to 
an HR tracking tool) that is only accessible to accounts on your Google Apps domain. This option cannot be changed once it has been set
(Experimental) Open to all users with an OpenlD Provider
If your application uses authentication, anyone who has an account with an OpenlD Provider may sign in.
Create Application Cancel
Once you have created an application, the next step is to install Google App

Engine SDK for Python, which allows you to create, test and deploy your 
App Engine application.
term
$ wget
http://googleappengine.googlecode.com/files/google_appe ngine_1.7.4.zip
$ unzip google_appengine_1.7.4.zip
The unzipped App Engine SDK will be located in 
/home/youraccount/google_appengine directory, which will be your App 
Engine SDK installation path.
Now create your application locally, as follows. The application is a simple 
single page website that displays "Hello there".
I_____ ^^Ixterm
Hl__ $ mkdir ~/myapp
$ vi ~/myapp/app.yaml 
application: this-is-my-test-application version: 1 
runtime: python
api_version: 1 
#default_expiration: "30d" 
handlers:
- url: /
static_files: www/index.html
upload: www/index.html
- url: /
static_dir: www

Bl__ $ mkdir ~/myapp/www
T__ $ vi ~/myapp/www/index.html
<html>
<body>

Hello there
</body>
</html>
Once you have created your application, the next step is to test and deploy it 
by using Google App Engine (GAE) Launcher.
Install GAE Launcher as follows.
$ svn checkout http://google-appengine-wx
launcher.googlecode.com/svn/trunk/ google-appengine-wxlauncher
__ ^^■'I’hen install any necessary prerequisite for GAE Launcher.
For Debian/Ubuntu:
lb_ $ sudo apt-get install python-wxversion python-wxglade
__ ^Ml;or CentOS/RHEL/Fedora:
term
lb_ sudo yum install wxPython wxGlade
Finally start GAE Launcher.
term
T__ $ cd google-appengine-wx-launcher
$ python GoogleAppEngineLauncher.py
The first time you run GAE Launcher, it will throw out a warning saying

App Engine SDK must be installed" as shown below.
This is because GAE launcher does not know yet where the App Engine SDK 
is installed on your system. To configure the location of App Engine SDK, go 
to Edit -^ Preferences menu on GAE Launcher, and fill in the info as shown 
below. You can leave the other fields empty.

Restart GAE Launcher.
Now add a new application on GAE Launcher by going to File -^ "Add Existing 
Application".
Set " Application Path" to the path of your application that you have created 
earlier (e.g., /home/youraccount/myapp), and choose application port number (e.g., 
8080).

Click on Run to test-run your application on your local computer.

Now go to http://127.0.0.1:8080 on your browser, and verify if the web site looks 
okay.
If you are satisfied, click on Deploy to deploy your application on Google App 
Engine. While attempting to deploy your application to Google, GAE 
Launcher will ask your email address and password associated with your App 
Engine account. Type your info to proceed.

Once your application is successfully deployed, you can access your website 
via http://this-is-my-test-application.appspot.com
As clarified at the beginning, an GAE-powered application can run for free as 
long as it does not incur resources beyond daily free quota (in terms of 
compute, datastore, bandwidth etc). You can check out resource usage of 
your application via Google App Engine administration console, as shown 
below.

Assuming that a simple static website would not involve much of compute or 
database resources, you may want to watch out for bandwidth quota (set to 
1Gbytes transfer per day).
Linux Tutorials
How to install OpenSim on Ubuntu
OpenSimulator (or just OpenSim) is an open-source 3D virtual world 
simulation server platform. It allows multiple users to custom-build a virtual 
environment, and to interact with each other in real-time, much like in 
Second Life. The virtual world simulated by OpenSim can be accessed by 
multiplatform viewer clients. This guide shows you how to set up a single 
node OpenSim server, and how to use OpenSim viewer client to access the 
virtual world simulated by the server.

OpenSim Server Installation
The following set-up has been tested on 64-bit Ubuntu 12.10 Server machine. 
Hardware requirements for OpenSim server vary depending on the number of 
avatars and complexity of virtual worlds, but in general multi-core hardware 
is recommended. Check OpenSim website for detailed hardware 
requirements.
First, install prerequisites for OpenSim. 
xterm
$ sudo apt-get install nant 
$ sudo apt-get install libmono-microsoft8.0-cil
_J^BDownload the latest OpenSim binaries, and start the OpenSim 
server.
xterm

$ wget http://opensimulator.org/dist/opensim 
0.7.4.tar.gz
$ tar xvfvz opensim-0.7.4.tar.gz
$ cd opensim-0.7.4/bin
$ ./opensim-ode.sh
The first time you run OpenSim server, it will ask you various questions 
setting up regions, etc. Make a note of "First name", "Last name" and "Password" 
you entered. Once it successfully goes through initial configuration, you will 
get OpenSim Prompt:
term
__ Region (your_region_name) #
OpenSim Client installation
The following OpenSim viewer client set-up has been tested on 32-bit 
Ubuntu Desktop 12.04.
The official Second Life viewer which is provided by Linden Lab no longer 
works with OpenSim, as they dropped support for OpenSim recently. There 
are several alternative OpenSim viewers you can use. In this example, I am 
going to use Imprudence OpenSim viewer.
Download Imprudence viewer binary from their website, and run it as 
follows.
xterm
$ tar xvfvj Imprudence-1.3.2b-Linux-x86.tar.bz2 $ cd Imprudence-1.3.2b- 
Linux-x86
$ ./imprudence
If you are getting "iibjpeg.so.62 not found" error while running

Imprudence, install the following package.
xterm
T__ $ sudo apt-get install libjpeg62
Once Imprudence viewer is successfully run, you will see " First Name", "Last 
Name", "Password" blanks toward the bottom of Imprudence GUI. Before 
proceeding, you need to configure the grid information of your OpenSim 
server. For that, click on "Grid Manager" button at the bottom of the screen. It 
will open up Grid Manager window, as shown below.
openjrr
Lt.;in UR1:
’tftLrldJMo' OeSSlnR
G;idIriD. JSLs:
I.-:.aJr!
Support:
iirn.ail:
Password
t7«b Seamen:
First name:
"asswo-rd:
Gnd:
Last name
cpcoli
Q Feuizii.lie' 
.
Star i cal on'
Kokua Bota 3.4.3.254 73
ll^pe- up':
t^Dsice:
Grin i '<in?.ger 
Sfllect.a Grid:
G.'.rr ■ lame:
0® Imprudence
File Edll Helu Advanced
Lnoert Lab's iLLl Co mmunlta... .■:j; ■ Kokua-3.4.42563 3
r 
nfrjui 1,-uu iiu ife-J
-EiUPfeiL, -a^i- ■ 
imp-usenoe 1,32
hLLjjy/192.168. L 10C:9DCDr
|hrip://t92. IfiB ,S .lODiBCOOPmeihad^itnjjn
Nanni
xmoditlo
ll -Yj Lasi Lcca-icn
k.caHus.1 
meta?
3>±Mk 
crap 
g-ancird 
tipper id
ra: Hnivrid 
ulepla.isorlos 
icencesim 
iecondlfe 
sercndifrt^la 
lIiei'ewx'jilLl/hd 
A.orldsimt*"a 
/ouraltemaiivel Io
Click on " Add new grid" menu on the left, and fill in the following information.
Grid name: opensim
Login URI: http://192.168.233.138:9000/

Login page: http://192.168.233.138:9000/?method=login
Replace 192.168.233.138 with the IP address of your OpenSim server.
Finally, to log in, choose opensim grid that you just set up, and type in 
First/Last name and password.
Linux Tutorials
How to install VMware Tools on
Ubuntu or Debian VM
The following post will walk you through how to isntall VMware Tools on 
Ubuntu or Debian virtual machine (VM). I assume that you have created 
your VM on VMware Player.
You first need to power on the VM on which you want to install VMware 
Tools. Once the VM is on, check if CD-ROM is connected to your VM, and 
if so, disconnect it from the VM. Then, go to "Virtual Machine" -^ "Install VMware 
Tools" on VMware Player menu interface. VMware Player will then download 
an iso image of VMware tools, and make it mounted to the VM as its CD- 
ROM. Now reboot the VM.
I recommend doing the rest of the installation procedure as root, rather than 
using sudo. Using sudo, installation procedure may fail as environment 
variables may not be set up properly.
Installing VMware Tools involves building new kernel modules, and 
therefore you need to prepare build environment for kernel module 
compilation.
xterm
lb_ $ apt-get install build-essential
$ apt-get install linux-headers-'uname -r'
Now extract the source code of VMware Tools from the CD-ROM's mount 

point. Note that the default path for mounted CD-ROM might be different 
across different distros. On Ubuntu 12.04, it's as follows.
xterm
743747.tar.gz
_J^HFinally, simply run vmware-instaii.pl contained in VMware Tools 
distribution.
If you would like to install VMware Tools on a CentOS-based VM, refer to 
this tutorial instead.
term
$ cd vmware-tools-distrib 
$ ./vmware-install.pl
hile running, the installation script will ask you a series of
questions. If you are not sure, you can accept all default answers.
Once all necessary configurations and builds are completed, a new initrd 
image will be created. Reboot the VM to finalize VMware Tools installation.
Linux Tutorials
How to set a static MAC address on
VMware ESXi virtual machine
Question: I want to assign a static MAC address to a virtual machine (VM) 
on VMware ESXi. However, when I attempt to start a VM with a static MAC 
address, the VM fails to start and throws an error "00:0c:29:1f:4a:ab is not an 
allowed static Ethernet address. It conflicts with VMware reserved MACs". 
How can I set a static MAC address on VMware ESXi VMs?
When you create a VM on VMware ESXi, each network interface of the VM 
is assigned a dynamically generated MAC address. If you want to change this 
default behavior and assign a static MAC address to your VM, here is how to 
do it.

As you can see above, VMware's vSphere GUI client already has a menu for 
setting a static MAC address for a VM. However, this GUI-based method 
only allows you to choose a static MAC address from 00:50:56:xx:xx:xx, which is 
VMware-reserved MAC address range. If you attempt to set any arbitrary 
MAC address outside this MAC range, you will fail to launch the VM, and 
get the following error.

Then what if I want to assign any arbitrary MAC address to a VM?
Fortunately, there is a workaround to this limitation. The solution is, instead 
of using vSphere GUI client, editing .vmx file of your VM directly, after 
logging in to the ESXi host.
First, turn off the VM to which you want to assign a static MAC address.
Enable SSH access to your ESXi host if you haven't done it already. Then log 
in to the ESXi host via SSH.
Move to the directory where your VM's .vmx file is located:
xterm
T__ # cd vmfs/volumes/datastore1/[name-of-vm]
Open .vmx file with a text editor, and add the following fields. Replace the

MAC address field with your own.
ethernet0.addressType = "static" 
ethernet0.checkMACAddress = "false" 
ethernet0.address = "00:0c:29:1f:4b:ac"
Now you should be able to launch a VM with the static MAC address you 
defined in .vmx file.
Linux Tutorials
How to build a RPM or DEB 
package from the source with 
CheckInstall
Question: I would like to install a software program by building it from the 
source. Is there a way to build and install a package from the source, instead 
of running make install? That way, I could uninstall the program easily later if I 
want to.
If you have installed a Linux program from its source by running make install, it 
becomes really tricky to remove it completely, unless the author of the 
program provides an uninstall target in the Makefile. You will have to compare 
the complete list of files in your system before and after installing the 
program from source, and manually remove all the files that were added 
during the installation.

That is when checkinstall can come in handy. CheckInstall keeps track of all the 
files created or modified by an install command line (e.g., make install, make 
install_modules, etc.), and builds a standard binary package, giving you the 
ability to install or uninstall it with your distribution's standard package 
management system (e.g., yum for CentOS/RHEL or aptget for 
Ubuntu/Debian). It has been also known to work with Slackware, SuSe, 
Mandrake and Gentoo as well, as per the official documentation.
In this post, I will only focus on Red Hat and Debian based distributions, and 
show how to build a RPM or DEB package from the source using 
CheckInstall.
Installing CheckInstall on Linux
To install Checkinstall on Ubuntu and other Debian derivatives: 
xterm
Bb_ # aptitude install checkinstall
To install CheckInstall on Red Hat-based distributions such as CentOS, you 
will need to download a pre-built RPM file of CheckInstall (e.g., searchable 
from http://rpm.pbone.net), as it has been removed from the Repoforge 
repository. The RPM package for CentOS 6 works in CentOS 7 as well.
term
# wget
ftp://ftp.pbone.net/mirror/ftp5.gwdg.de/pub/opensuse/re
positories/home:/ikoinoba/CentOS_CentOS
6/x86_64/checkinstall-1.6.2-3.el6.1.x86_64.rpm # yum install checkinstall-
1.6.2-3.el6.1.x86_64.rpm
Once CheckInstall is installed, you can use the following command 
to build a package for particular software.
xterm
Bb_ # checkinstall <install-command>

—(^■Without install-command argument, the default install command make 
install will be used.
Build an RPM or DEB Pacakge with CheckInstall
In this example, I will build a package for htop, an interactive text-mode 
process viewer for Linux (like top on steroids).
First, let's download the source code from the official website of the project.
As a best practice, we will store the tarball in /usr/local/src, and untar it.
I_____ ^^Ixterm
# cd /usr/local/src
# wget http://hisham.hm/htop/releases/1.0.3/htop1.0.3.tar.gz
# tar xzf htop-1.0.3.tar.gz
# cd htop-1.0.3
Let's find out the install command for htop, so that we can invoke CheckInstall 
with checkinstall command. As shown below, htop is installed with make install 
command.
xterm
T__ # ./configure
# make install
Therefore, to build a htop package, we can run checkinstall command without 
any argument, which will then use make install command to build a package. 
Along the process, the checkinstall command will ask you a series of questions.
In short, here are the commands to build a package for htop:
xterm
lb_ # ./configure
# checkinstall
__ Answer y to "Should I create a default set of package docs?":

checkinstall 1.6.2, Copyright 2009 Felipe Eduardo Sanchez Diaz Duran 
This software is released under the GNU GPL.
The package documentation directory ./doc-pak does not exist. 
Should I create a default set of package docs? [y]: y|
You can enter a brief description of the package, then press Enter twice:
Please write a description for the package.
End your description with an empty line or EOF.
>> htop is an interactive text-mode process viewer for Linux|
Enter a number to modify any of the following values or Enter to proceed:
This package will be built according to these values:
j - Maintainer: [ rooWdebian ]
'I 
- Summary: 
[ htop is an interactive text-mode process viewer for Linux ]
2 
- Name: 
[ htop ]
1 
- Version: 
[ 1.0.3 ]
4 
- Release: 
[ 1 ]
5 
- License: 
[ GPL ]
5 - Group: [ checkinstall ]
7 - Architecture: [ 1386 ]
B - 
Source location: ( htop-1.0.3 
]
9 - 
Alternate source location: [ 
]
10 - Requires: [ ]
11 - Provides: [ htop ]
12 - Conflicts: [ ]
13 - Replaces: [ ]
Enter a number to change any of them or press ENTER to continue: fl
Then CheckInstall will create a .rpm or a .deb package automatically, 
depending on what your Linux system is:
On CentOS 7:

Done. The new package has been installed and saved to
/root/rpmbuild/RPMS/x86_64/htop-1 .0.3-1 .xB6_&4.rpm
You can remove it from your system anytime using:
rpm -e htop-1.0.3-1
On Debian 7:
Linux Tutorials
How to clone or copy a virtual 
machine on VirtualBox
If you need to copy or clone a virtual machine (VM) often, VirtualBox is 
probably the best free virtualization software for you. VirtualBox has a built- 
in support for virtual machine cloning, so cloning a VM is just a matter of a 
few mouse clicks.
In order to clone a VM on Virtual Box, you first need to save the machine 
state of a given VM, or completely power off the VM. You cannot clone an 
actively running VM.
Once a VM is powered off or saved, you can clone the VM by following this 
guide.
Go to Machine -+ Clone on VirtualBox GUI menu. It will open up VM clone 
wizard as follows. Type in a new machine name.
On the next screen, you will get to choose "Clone type". If you want to create an 
independent copy of your VM, choose "Full clone".

If there is any snapshot of an original VM, you can choose between copying 
the current machine state only, or copying the current machine state, plus all 
available snapshots.
Finally, click on "Clone button" to proceed with cloning. Once cloning is done, 
you can power on the cloned VM.

Solve Networking Problems of Cloned Linux VM
When cloning a Linux VM, additional steps may be necessary to make the 
networking of its cloned VM work.
For example, for Ubuntu/Debian VM, you may notice that there is no active 
network interface, upon powering on the cloned VM. The problem is due to 
the fact that the Linux system of the cloned VM stores the original VM's 
MAC address(es) for its network interface(s) in its udev configuration file, 
even though VirtualBox has generated new MAC address(es) for the cloned 
VM.

Since Linux 2.6 kernel, udev device manager system has been introduced to 
handle devices including networking devices, hard drives, USB devices, etc. 
The udev of the original VM assigns device name eth0 to the VM's MAC 
address, and this (eth0, MAC address) mapping rule is stored in
/etc/udev/rules.d/
*-persistent-net.rules
.
The cloned VM will use the same udev rule to attempt to assign device names. 
However, since VirtualBox has generated a different MAC address for the 
cloned VM, it cannot assign eth0 to the cloned VM's primary interface, and 
ends up assigning a different name (e.g., eth1) to the interface. Of course, 
because there is no network configuration for eth1, networking cannot work.
You can verify this problem by checking udev rule of the cloned VM as 
follows.
xterm
T__ $ sudo vi /etc/udev/rules.d/
*-persistent-net.rules
# PCI device 0x8086:/sys/devices/pci0000:00/0000:00:03.0 (e1000) 
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?
",
 
ATTR{address}=="08:00:27:d9:c8:cc", ATTR{dev_id}=="0x0", 
ATTR{type}=="1", KERNEL=="eth
",
 NAME="eth0"
*
*
# PCI device 0x8086:/sys/devices/pci0000:00/0000:00:03.0 (e1000) 
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?
",
 
ATTR{address}=="08:00:27:2d:49:3e", ATTR{dev_id}=="0x0", 
ATTR{type}=="1", KERNEL=="eth
",
 NAME="eth1"
*
*

In this example, the first line is a copy from the original VM, and the second 
line was appended by the cloned VM. 08:00:27:d9:c8:cc is the MAC address 
assigned to the original VM's interface, which is named eth0, while
08:00:27:2d:49:3e is the MAC address assigned to the cloned VM's interface, 
which is assigned eth1.
To fix this problem, remove the first line, and replace eth1 with eth0 in the 
second line. Thus, a correct udev rule for the cloned VM looks like:
# PCI device 0x8086:/sys/devices/pci0000:00/0000:00:03.0 (e1000)
SUBSYSTEM=="net", ACTION=="add", DRIVERS=="?
*",
 
ATTR{address}=="08:00:27:2d:49:3e", ATTR{dev_id}=="0x0", 
ATTR{type}=="1", KERNEL=="eth
*",
 NAME="eth0"
Finally, reboot the cloned VM.

One more thing: make sure to chance hostname of the cloned VM in 
/etc/hostname and /etc/hosts, since the cloned VM has the same 
hostname of the original VM.
Linux Tutorials
How to install an old version of
Firefox on Linux
There could be several reasons why you want to stick with a specific (old) 
version of Firefox web browser, instead of updating it to the latest version. 
Some of your favorite add-ons could no longer work after Firefox update; 
you may want to try out an old Firefox extension which has not been updated 
for too long a time to be compatible with the latest Firefox; some web 
application's user interface may no longer be accessible after you have 
upgraded your Firefox, etc.
While it is recommended that for security reasons, you always stay up-to-date 
with the latest release of Firefox, you may still want to go back to an old 
version of Firefox if it is absolutely necessary. It is not very complicated to 
install an old version of Firefox on Linux. The following is 5-minute guide 
to do it.
First, download the version of Firefox that you want from Mozilla FTP 
archive.
In the above archive, there are two binaries available for each version. 32-bit 
Firefox binary is contained in "linux-i686" folder, and 64-bit counterpart is in 
"linux-x86_64" folder. 64-bit binaries are available only for version 4.0 and 
higher.
Once you have downloaded a Firefox tar file from the archive, untar it in 
your system.
B__xterm
$ sudo tar xvfvj ~/Downloads/firefox-3.5.9.tar.bz2 -C _/opt

If your system already has another version of Firefox installed (e.g.,
/usr/bin/firefox), create a backup as follows.
xterm
lb_ $ cd /usr/bin 
$ sudo mv firefox firefox-backup
__ (^"Finally, create a symbolic link pointing to the Firefox you just 
installed.
xterm
I!_ $ sudo ln -s /opt/firefox/firefox /usr/bin/firefox $ sudo chmod 755
/usr/bin/firefox
__ ^^Hnow an old Firefox is ready to be used!
Linux Tutorials
How to set up a transparent HTTPS 
filtering proxy on CentOS
HTTPS protocol is used more and more in today's web. While this may be 
good for privacy, it leaves modern network administrator without any means 
to prevent questionable or adult contents from entering his/her network. 
Previously it was assumed that this problem does not have a decent solution. 
Our how-to guide will try to prove otherwise.
This guide will tell you how to set up Squid on CentOS / RedHat Linux for 
transparent filtering of HTTP and HTTPS traffic with help of Diladele Web 
Safety ICAP server, which is a commercial solution for Linux, BSD and 
MacOS. The Linux installer of Diladele Web Safety used in this tutorial 
contains fully featured keys which remain valid for 3 month period, so you 
can test its full features during this trial period.
Assumptions and Requirements
In this tutorial, I will assume the following. You have a network with IP 

addresses from 192.168.1.0 subnet, network mask is 255.255.255.0, and all 
workstations are set to use 192.168.1.1 as default gateway. On this default 
gateway, you have two NICs - one facing LAN with IP address
192.168.1.1 , the other is plugged in into ISP network and gets its public Internet 
address through DHCP. It is also assumed your gateway has CentOS or 
RedHat Linux up and running.
Step 1. Update and Upgrade
Before going further, run the following script to upgrade your system to the 
most recent state.
# !/bin/bash set -e
# update should be done as root
if [[ $EUID -ne 0 ]]; then
echo "This script must be run as root" 1>&2
exit 1
fi
# update and upgrade
yum update && yum upgrade
# disable selinux

sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config 
# and reboot
reboot

Step 2. Install Apache Web Server
Diladele Web Safety has sophisticated a web administrator console to easily 
manage filtering settings and policies. This Web UI is built using Python 
Django web framework, and requires Apache web server to function 
correctly. Run the following script to install them.
#!/bin/bash 
set -e
# all web packages are installed as root if [[ $EUID -ne 0 ]]; then
echo "This script must be run as root" 1>&2 exit 1 
fi
# install python libs
yum install python-setuptools python-ldap
# install python django for web ui easy_install django==1.5
# install apache web server to run web ui yum install httpd php mod_wsgi
# make apache autostart on reboot chkconfig httpd on
# this fixes some apache errors when working with python-django wsgi echo 
"WSGISocketPrefix /var/run/wsgi" >> /etc/httpd/conf.d/wsgi.conf
# and restart apache
service httpd restart


Step 3. Install Diladele Web Safety
Download and install the latest version of Diladele Web Safety using the 
following script.
#!/bin/bash
# all packages are installed as root
if [[ $EUID -ne 0 ]]; then
echo "This script must be run as root" 1>&2 exit 1 
fi
# detect current architecture (default assumes x86_64) ARCH_1='uname -m' 
ARCH_2="amd64"
if [[ $ARCH_1 == 'i686' ]]; then
ARCH_1="i386"
ARCH_2="i386"
fi
# bail out on any error set -e
# get latest qlproxy
curl
http://updates.diladele.com/qlproxy/binaries/3.2.0.4CAF/$ARCH_2/rele
http://updates.diladele.com/qlproxy/binaries/3.2.0.4CAF/$ARCH_2/rele
4CAF.$ARCH_1.rpm
# install it
yum -y --nogpgcheck localinstall qlproxy-3.2.0-4CAF.$ARCH_1.rpm
# qlproxy installed everything needed for apache, so just restart service httpd 
restart


Step 4. Install Required Build Tools
To be able to perform HTTP/HTTPS transparent filtering, we need to get the 
latest version of Squid (the one that comes with CentOS / RedHat by default 
is too outdated), and rebuild it from source. The following script installs all 
build tools required.
#!/bin/bash
# install all build tools
if [[ $EUID -ne 0 ]]; then
echo "This script must be run as root" 1>&2 exit 1 
fi
# install development packages required
yum install -y gcc-c++ pam-devel db4-devel expat-devel libxml2-devel 
libcap-devel libtool redhat-rpm-config rpm-build openldap-devel 
openssldevel krb5-devel
# squid needs perl and needs additional perl modules not present by default in 
CentOS 6
default in CentOS 6
8.noarch.rpm > epel-release-6-8.noarch.rpm
rpm -Uvh epel-release-6
*.rpm
yum install -y perl-Crypt-OpenSSL-X509

Step 5. Build Squid from Source
Rebuild the Squid RPM by running the following script.
#!/bin/bash
# stop on any error

set -e
# rpm build MUST be run as normal user
if [[ $EUID -eq 0 ]]; then
echo "This script must NOT be run as root" 1>&2 exit 1 
fi
# get squid sources
pushd rpmbuild/SOURCES
curl http://www.squid-cache.org/Versions/v3/3.4/squid-3.4.4.tar.xz &gt;
squid-3.4.4.tar.xz
curl http://www.squid-cache.org/Versions/v3/3.4/squid-3.4.4.tar.xz.asc &gt;
squid-3.4.4.tar.xz.asc
popd
# build the binaries RPMs out of sources
pushd rpmbuild/SPECS
rpmbuild -v -bb squid.spec
popd

Step 6. Install Squid
After build finishes, install Squid. It is advisable to uncomment the lines 
which generate your own root certification authority. Default installation of 
Diladele Web Safety does have its own ca, but trusting it may pose serious 
security risk if your devices are used by users outside of your network.
#!/bin/bash
# stop on every error

set -e
# install RPMs as root
if [[ $EUID -ne 0 ]]; then
echo "This script must be run as root" 1>&2 exit 1 
fi
# detect current architecture (default assumes x86_64) ARCH_1='uname -m' 
ARCH_2="amd64"
ARCH_3="lib64"
if [[ $ARCH_1 == 'i686' ]]; then ARCH_2="i386"
ARCH_3="lib"
fi
pushd rpmbuild/RPMS/$ARCH_1
yum localinstall -y squid-3.4.4-0.el6.$ARCH_1.rpm popd
# set up the ssl_crtd daemon if [ -f /bin/ssl_crtd ]; then rm -f /bin/ssl_crtd 
fi
ln -s /usr/$ARCH_3/squid/ssl_crtd /bin/ssl_crtd /bin/ssl_crtd -c -s 
/var/spool/squid_ssldb chown -R squid:squid /var/spool/squid_ssldb
# uncomment to regenerate certificates for SSL bumping if you do not like 
defaults


# openssl req -new -newkey rsa:1024 -days 1365 -nodes -x509 -keyout 
myca.pem -out myca.pem
# openssl x509 -in myca.pem -outform DER -out myca.der
# then copy certificates
# cp myca.pem /etc/opt/quintolabs/qlproxy/
# cp myca.der /etc/opt/quintolabs/qlproxy/
# make squid autostart after reboot 
chkconfig squid on
Step 7. Integrate Squid with Diladele Web Safety
Integrate Squid and Diladele Web Safety by running the following script. 
#!/bin/bash
# stop on any error set -e
# integration should be done as root if [[ $EUID -ne 0 ]]; then
echo "This script must be run as root" 1>&2 exit 1
fi
# allow web ui read-only access to squid configuration file chmod o+r

/etc/squid/squid.conf
# perform integration by replacing squid.conf file
mv /etc/squid/squid.conf /etc/squid/squid.conf.original &amp;&amp; mv 
squid.conf /etc/squid/squid.conf

# parse the resulting config just to be sure 
/usr/sbin/squid -k parse

# restart squid to load all config
/sbin/service squid restart
Step 8. Transparently Redirect HTTPS Traffic to 
Squid
Transparent filter for HTTP and HTTPS traffic will be implemented by 
redirecting traffic to ports 80 and 443 to Squid using iptables. This implies that 
the box with Squid acts as default gateway for your LAN. Please note this is 
only one way to implementing transparent filtering. Other possible solutions 
are described in Squid’s Wiki.
#!/bin/bash
# firewall setup should be done as root if [[ $EUID -ne 0 ]]; then 
echo "This script must be run as root" 1>&2 exit 1 
fi
# check kernel forwarding is enabled
enabled=' cat /proc/sys/net/ipv4/ip_forward'
if [[ $enabled -ne 1 ]]; then
echo "Kernel forwarding seems to be disabled, enable it in /etc/sysctl.conf, 
reboot and rerun this script" 1>&2 
exit 1 
fi

# set the default policy to accept first (not to lock ourselves out from remote 
machine)
iptables -P INPUT ACCEPT
# flush all current rules from iptables iptables -F
# allow pings from eth0 and eth1 for debugging purposes iptables -A INPUT
- p icmp -j ACCEPT
# allow access for localhost iptables -A INPUT -i lo -j ACCEPT
# accept packets belonging to established and related connections iptables -A 
INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
# allow ssh connections to tcp port 22 from eth0 and eth1 iptables -A INPUT
- p tcp --dport 22 -j ACCEPT
# allow connection from LAN to ports 3126, 3127 and 3128 squid is running

on
iptables -A INPUT -i eth0 -p tcp --dport 3126 -j ACCEPT iptables -A INPUT
- i eth0 -p tcp --dport 3127 -j ACCEPT iptables -A INPUT -i eth0 -p tcp -- 
dport 3128 -j ACCEPT
# redirect all HTTP(tcp:80) traffic coming in through eth0 to 3126 iptables -t 
nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 80 -j REDIRECT --to­
ports 3126
# redirect all HTTPS(tcp:443) traffic coming in through eth0 to 3127 iptables
- t nat -A PREROUTING -i eth0 -p tcp -m tcp --dport 443 -j REDIRECT --to­
ports 3127
# configure forwarding rules


iptables -A FORWARD -i eth0 -o eth1 -p tcp --dport 22 -j ACCEPT iptables 
-A FORWARD -i eth1 -o eth0 -p tcp --sport 22 -j ACCEPT iptables -A 
FORWARD -p icmp -j ACCEPT
iptables -A FORWARD -i eth0 -o eth1 -p tcp --dport 80 -j ACCEPT iptables 
-A FORWARD -i eth1 -o eth0 -p tcp --sport 80 -j ACCEPT iptables -A 
FORWARD -i eth0 -o eth1 -p tcp --dport 53 -j ACCEPT iptables -A 
FORWARD -i eth0 -o eth1 -p udp --dport 53 -j ACCEPT iptables -A 
FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT 
iptables -A FORWARD -j REJECT --reject-with icmp-host-prohibited
# enable NAT for clients within LAN
iptables -t nat -A POSTROUTING -o eth1 -j MASQUERADE
# set default policies for INPUT, FORWARD (drop) and OUTPUT (accept) 
chains
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT
# list created rules iptables -L -v
# save the rules so that after reboot they are automatically restored 
/sbin/service iptables save
# enable the firewall
chkconfig iptables on
# and reboot machine
reboot


Check if HTTPS is Transparently Filtered
Please note, in order for HTTPS filtering to function correctly, we must 
install the proxy certificate from /etc/opt/quintolabs/qlproxy/myca.der into Trusted 
Root Certification on all workstations in our network. The following 
screenshots show that HTTPS requests were decrypted and filtered 
transparently.

0 settings
Opera] Q Surfing Now Monitoring X 0 Google
ft Settings
+
□ I 101

/ \ Secure connection
www.google.com
The connection is secure.
Hide dezsils
First visited: No previous visits recorded
Certificate: ww.qQOqle.COrri
DiladeleB.V.
Connection: TLS1.2AES_128.GCM RSA
Google
+You Gmail Images ;;;
Sign in
Google Search I'm Feeling Lucky
Advertising Business About
Install Google Chrome
X
A faster way to browse the web
Privacy & Terms Settings Google.nl

Browsing to Google and searching for an adult term (e.g. NSFW), we get the 
HTTPS request filtered and blocked transparently.
Resume
We now have the default gateway in our network capable of transparently 
filtering HTTP and HTTPS traffic. All workstations in our network trust the 
root certificate from proxy, and thus get their HTTPS request decrypted and 
filtered. Browsing environment in our network became much safer. Refer to 
the documentation of Diladele Web Safety for more details.
Linux Tutorials

How to check what libraries are 
used by a program or process on 
Linux
Question: I would like to know which shared libraries are loaded at runtime 
when I invoke a particular executable. Is there any way to identify shared 
library dependencies of a program executable or a running process on Linux?
You can use the following methods to identiy which shared librariies a given 
program executable (e.g., /path/to/program) or a given running process (e.g., PID 
1149) depends on.
Check Shared Library Dependencies of a Program 
Executable
To find out what libraries a particular executable depends on, you can use ldd 
command. This command invokes dynamic linker to find out library 
dependencies of an executable.
term
$ Idd /path/to/program

$ Ldd /usr/bin/ssh
linux-vdso.so.1 => (0xOO0O7ffff83feOO0)
libselinux.so. 1 => /Ub/x86 64-Unux-gnu/Ubselinux.so.l (0x00007f74f3904000) 
libcrypto.so.I.0.0 => /lib/x86 64-linux-gnu/libcrypto.so.1.0.0 (0x00007f74f352a000) 
libdl.S0.2 => /Ub/x86_64-linux-gnu/Ubdl.so.2 (Ox00007f74f3325000)
libz.so.l => /Ub/x86_64-linux-gnu/Ubz.so.l (Ox00007f74f31OcO0O)
libresolv.so.2 => /Ub/x86_64-Unux-gnu/Ubresolv.so.2 (0x00007f74f2ef 1000)
libgssapi_krb5.so.2 => /usr/lib/x86_644inux-gnu/Ubgssapi_krb5.so.2 (0x68607f74f2caa 
000)
libc.so.6 => /Ub/x86_64-linux-gnu/Ubc.so.6 (Ox00007f74f28e4O0O)
libpcre.so.3 => /lib/x86 64-linux-gnu/libpcre.so.3 (0x00007f74f26a6000)
/Ub64/ld-linux-x86-64.so.2 (Ox00007f74f3df1000)
Ubkrb5.so.3 => /usr/Ub/x86_64-Unux-gnu/Ubkrb5.so.3 (0x00007f74f23da000) 
libkScrypto.so.3 => /usr/lib/x86_64-Unux-gnu/libk5crypto.so.3 (0x00007f74f21ab000) 
libcom err.so.2 => /Ub/x86_64-linux-gnu/Ubcom_err.so.2 (0x00007f74flfa7000) 
libkrb5support.so.0 => /usr/lib/x86_644inux-gnu/Ubkrb5support.so.0 (0x00007f74fld9b 
000)
libkeyutils.so.1 => /Ub/x86_64-linux-gnu/libkeyutils.so.l (0x00007f74flb97000) 
libpthread.so.© => /lib/x86_64-linux-gnu/libpthread.so.0 (0x0O0O7f74f1978000)
Note that it is not recommended to run ldd with any untrusted third-party 
executable because some versions of ldd may directly invoke the executable to 
identify its library dependencies, which can be security risk.
Instead, a safer way to show library dependencies of an unknown application 
binary is to use the following command.
xterm
lb_ $ objdump -p /path/to/program | grep NEEDED

$
$ obj dump -p /usr/bin/git | grep NEEDED
NEEDED
libpcre.so.3
NEEDED
libz. so. 1
NEEDED
libresolv.so.2
NEEDED
libpth read.so.0
NEEDED
libc.so.6
$
A_
Check Shared Library Dependencies of a Running 
Process
If you want to find out what shared libraries are loaded by a running process, 
you can use pldd command, which shows all shared objects loaded into a 
process at run-time.,/p>
^_^_^^Bxtenn
$ sudo pldd <PID>
Note that you need root privilege to run pldd command.

$
$ sudo pldd 1149
1149: 7usr7sEin/sshd
Linux-vdso.so.1
/Lib/x86_64-linux-gnu/libwrap .so.B
/Lib/x86_64-linux-gnu/libpam.so.O
/Lib/x86_64 linux-gnu/libselinux,so.1
/usr/lib/x86_64- Linux -gnu/libc k-connec tor.so,0 
/Lib/x86_64-linux-gnu/libdbus-1.so.3
/Lib/x86_64-linux-gnu/libcrypto. so.1,0,0 
/lib/x86_64-linux-gnu/libutil.so, 1
/Lib/x86_64 linux-gnu/libz.so,1
/Lib/x86_64-linux-gnu/libcrypt.so. 1
/usr/lib/x86_64-Linux-gnu/libgssapi_krb5.so,2
/usr/lib/x86_64-linux-gnu/libkrbS.so. 3 
/Lib/x86_64-linux-gnu/libconi_err. so, 2 
/Lib/x86_64-Linux-gnu/libc.so. 6
/Lib/x86_64-linux-gnu/libnsl.so.1
/Lib/x86 64-linux-gnu/libaudit.so.1
Alternatively, a command line utility called pmap, which reports memory map 
of a process, can also show shared library dependencies of a running process.
xterm
T $ sudo pmap <PID>

5
$ sudo pmap 146©
fsshd -D
146Q: 
/usr/sbin.
BBBB7flb69e63BBB
44K r-x- - libnss files-2. 19. so
SGB67flb69e6eBBB
2B44K — libnss files-2. 19. so
BBW7flb6aB6dMB
4K r-----
libnss_ files-2. 19. so
Bf>0fl7flb6aB®e00fl
4K rw— libnss fites-2. 19. so
000B7flb6a06fB0B
44K r-x- - libnss nis-2.19 , so
68B87flbBaS7a8B8
2B44K — libnss nis-2.19 . so
BBBB7flb5a279BBB
4K r-----
libnss nis-2.19 . so
B6007flb6a27afl00
4K rw— libnss nis-2.19 . so
00007flb6a27b0fle
36K r-x- - libnss compat’2 ,19.so
Bee07flb6a284Bfl0
2044K — libnss compat-2 . 19.so
BBW7flb6a483BBB
4K r-----
libnss compat-2 .19.so
000fl7flb6a4840Be
4K rw— libnss compat-2 .19.so
B8BB7flb6a485BBB
92K r-x- - libresolv-2.19. so
Linux Tutorials
How to turn off server signature on 
Apache web server
Question: Whenever Apache2 web server returns error pages (e.g., "404 not 
found", "403 access forbidden pages"), it shows web server signature (e.g., 
Apache version number and operating system info) at the bottom of the 
pages. Also, when Apache2 web server serves any PHP pages, it reveals PHP 
version info. How can I turn off these web server signatures in Apache2 web 
server?
Revealing web server signature with server/PHP version info can be a 
security risk as you are essentially telling attackers known vulnerabilities of 
your system. Thus it is recommended you disable all web server signatures as 
part of server hardening process.

< C 0 192.168.1.7/home.html
Not Found
The requested L'RL /home.html was not found on this server.
Apache/2.2.22 (Debian) Server at 192.168.1.7 Port 80
Disable Apache Web Server Signature
Disabling Apache web server signature can be achieved by editing Apache 
config file.
On Ubuntu, Debian or Linux Mint:
xterm
lb_ $ sudo vi /etc/apache2/apache2.conf
On CentOS, Fedora, RHEL or Arch Linux:
xterm
$ sudo vi /etc/httpd/conf/httpd.conf
__ ^^■acIcI the following two lines at the end of Apache config file.
ServerSignature Off
ServerTokens Prod

Then restart web server to activate the change:
term
$ sudo service apache2 restart (Ubuntu, Debian or Linux Mint)
$ sudo service httpd restart (CentOS/RHEL 6)
$ sudo systemctl restart httpd.service (Fedora, CentOS/RHEL 7, Arch Linux)
The first line that says serversignature Off makes Apache2 web server hide 
Apache version info on any error pages.
However, without the second line servertokens Prod, Apache server will still 
include a detailed server token in HTTP response headers, which reveals 
Apache version number.

$ curl -v http://192.168.1.7/home.html
* Hostname was NOT found in DNS cache
* Trying 192.168.1.7...
* Connected to 192.168,1.7 (192.168.1.7) port 80 (#0)
> GET /home.html HTTP/1.1
> User-Agent: curl/7.35.0
> Host: 192.168.1.7
> Accept: /
 
>
*
< HTTP/1.1 404 Not Found
< Date: Wed, 13 Aug 2014 92:85:11 GMT
* Server Aoache/2.2.22 (Debian not blacklisted
< Server: Apache/2.2.22 (Debian)
< Vary: Accept-Encoding
< Content-Length: 284
< Content-Type: text/html; charset=iso-8859-1 
<
<ID0CTYPE HTML PUBLIC "-//IETF//DTD HTML 2,0//EN'>
<htmlxhead>
<title>404 Not Found</title>
</headxbody>
<hl>Not Found</hl>
What the second line servertokens Prod does is to suppress a server token in 
HTTP response headers to a bare minimal.
So with both lines in place, Apache will not reveal Apache version info in 
either web pages or HTTP response headers.
Hide PHP Version
Another potential security threat is PHP version info leak in HTTP response 
headers. By default, Apache web server includes PHP version info via 
xpowered-by field in HTTP response headers. If you want to hide PHP version 
in HTTP headers, open php.ini file with a text editor, look for expose_php = On, 
and change it to expose_php = Off.

> GET /info.php HTTP/1,1
> User-Agent: curl/7.35.0
> Host: 192.168.1.7
> Accept: /
 
>
*
< HTTP/1,1 288 OK
< Date: Wed, 13 Aug 2014 03:05:45 GMT
* Server Apache is not blacklisted
< Server: Anache
< X-Powered-By: PHP/5.4.4-14+deb7ul2
< Vary: Accept - Encoding
< Transfer-Encoding: chunked
< Content-Type: text/html 
<
On Ubuntu, Debian or Linux Mint:
term
$ sudo vi /etc/php5/apache2/php.ini
On CentOS, Fedora, RHEL or Arch Linux:
term
$ sudo vi /etc/php.ini 
^-Jexpose_php = Off
Finally, restart Apache2 web server to reload updated PHP config file. Now 
you will no longer see x-powered-by field in HTTP response headers.
Linux Tutorials
How to set up a secure SFTP server

in Linux
SFTP service provides secure file access and transfer mechanisms over SSH 
tunnels. If you are setting up an SFTP server accessed by multiple users, you 
need to enforce security protection, not only in terms of protecting SFTP 
users from external intruders, but also in terms of protecting the SFTP server 
from (potentially malicious) SFTP users, and providing isolation among 
individual SFTP users.
In this tutorial, I will describe how to set up a secure SFTP server in 
Linux, by properly protecting the SFTP server from SFTP users, and 
isolating individual SFTP users from one another. There can be many 
different ways to achieve this goal, but I will describe mysecureshell based 
approach here.
is openssh based SFTP server, featuring a number ofmysecureshell 
security features:
Limit per-connection download/upload bandwidth
Limit the number of concurrent connections per account
Hide file and directory owner/group/rights
Hide files and directories which user has no access to
Limit the life time of a connection
Chroot SFTP user into his/her home directory
Deny upload of files and directories that match regular expressions
Install MySecureShell on Linux
To use mysecureshell on Linux, you first need to install the following 
prerequisites.
To install prerequisites on Ubuntu or Debian:
xterm
T $ sudo apt-get install libssl0.9.8 ssh openssh-server gcc make 
I Ito install prerequisites on CentOS, RHEL or Fedora:
xterm

$ sudo yum install openssl-devel openssh-server gcc make 
Once all prerequisites are installed, you can build and install
mysecureshell on Linux as follows.
term
$ wget
http://mysecureshell.free.fr/repository/index.php/sourc 
e/mysecureshell_1.31.tar.gz
$ tar xvfvz mysecureshell_1.31.tar.gz
$ cd mysecureshell_1.31
$ ./configure
$ make
$ sudo ./install.sh en
#########################################
# MySecureShell #
#########################################
Welcome to the MySecureShell installation script !
Detecting needed files for installation: Existing file MySecureShell [ OK ]
Existing file sftp_config [ OK ]
Do you want to test MySecureShell (check libraries requirement) ? (Y/n) Test 
MySecureShell...
Test ending

This script will made a few operations:
- Install MySecureShell in /bin
- Make a configuration file in /etc/ssh/sftp_config
- Introduce if which MySecureShell as a valid shell
- Install utilities in /usr/bin
WARNING: The server will shutdown and all sftp connected clients will be 
killed !
- Do you want to continue installation ? (Y/n)
MySecureShell Installation
MySecureShell file created [ OK ] MySecureShell file created [ OK ]
Do you want MySecureShell shell to be add like valid shell on your system 
(Y/n)
MySecureShell shell added like a valid shell [ OK ]
Installation of tool sftp-who [ OK ]
Installation of tool sftp-kill [ OK ]
Installation of tool sftp-state [ OK ]
Installation of tool sftp-admin [ OK ]
Installation of tool sftp-verif [ OK ]


Installation of tool sftp-user [ OK ]
Do you want to automatically rotate MySecureShell logs ? (Y/n) Initialisation 
of MySecureShell rotation logs [ OK ]
cp: target '/share/man/fr/man8' is not a directory
Installation of Manuals [ OK ]
Installation Finished !
Configure MySecureShell
After installation, verify where MySecureSheii is installed.
term
lb_ $ whereis MySecureShell
Usr/bin/MySecureShell
In order to manage SFTP users with mysecureshell, first create a Linux group 
that SFTP users will belong to. Let's say the group is called sftp.

xterm
T $ sudo groupadd sftp
__ l^®Then configure an existing SFTP user (e.g., alice) so that the user 
belongs to sftp group, and uses MySecureShell shell upon login.
xterm
$ sudo usermod -s /usr/bin/MySecureShell -g sftp alice
__ Ml you are creating a new SFTP user from scratch, then run the 
following command instead.
xterm
lb_ $ sudo useradd -m -s /usr/bin/MySecureShell -g sftp bob
To customize the default settings of mysecureshell, edit its configuration file 
located at /etc/ssh/sftp_config. in the configuration file, you can define various 
per-group security settings. For example, for Linux group
sftp:
xterm
T $ sudo vi /etc/ssh/sftp_config 
<Group sftp>
Download 50k # limit download speed for each connection
Upload 0 # unlimit upload speed for each connection StayAtHome true # 
limit user to his/her home directory VirtualChroot true # fake a chroot to the 
home account LimitConnectionByUser 1 # max connection for each account 
LimitConnectionByiP 1 # max connection by iP for each account 
idleTimeOut 300 # disconnect user if idle too long time (in sec) 
HideNoAccess true # hide file/directory which user has no access 
</Group>

Once the configuration file has bee edited, make sure to restart sshd as 
follows.
To restart sshd on Ubuntu or Debian:
______ ^^■xtenn
T $ sudo service ssh restart
I Ito restart sshd on CentOS, RHEL or Fedora:
xtenm
lb_ $ sudo service sshd restart

Access and Manage SFTP server
On client-side, you can log in to the SFTP server as follows. The user is 
chrooted to his own home directory, and no other directory on the server is 
visible to the user.
term
__ $ sftp [email protected]_host.com 
[email protected] 's password: 
Connected to 192.168.233.141.
sftp> pwd
Remote working directory: / 
sftp>
On SFTP server-side, you can manage SFTP server and its users as follows.
To monitor SFTP users who are connected currently:
xterm
lb_ $ sftp-who
--- 1 / 10 clients --

Global used bandwith : 0 bytes/s / 0 bytes/s 
PID: 24377 Name: bob IP: 192.168.10.55
Home: /home/bob
Status: idle Path: /
File:
Connected: 2013/05/28 20:57:42 [since 01mins 05s]
Speed: Download: 0 bytes/s [50.00 kbytes/s] Upload: 0 bytes/s [unlimited]
Total: Download: 1002 bytes Upload: 82 bytes
To disconnect a particular SFTP user forcefully:
term
T $ sudo sftp-kill bob
Linux Tutorials
How to monitor a Linux server and

desktop remotely from web browser
When it comes to monitoring a Linux box, there are more than enough 
options to choose from. While there are many production-quality monitoring 
solutions (e.g., Nagios, Zabbix, Zenoss), boasting of fancy UI, monitoring 
scalability, comprehensive reporting capabilities, etc., these solutions are 
probably an overkill for most of us end users. If all you need is to check the 
basic status (e.g., CPU load, memory usage, active processes, disk usage) of a 
remote Linux server or desktop, consider linux-dash.
is a web-based lightweight monitoring dashboard for Linux linux-dash 
machines, which can display, in real-time, various system properties, such as 
CPU load, RAM usage, disk usage, Internet speed, network connections, 
RX/TX bandwidth, logged-in users, running processes etc. linux-dash does 
not come with any backend database for storing long-term statistics. Simply 
drop in linux-dash app in an existing web server (e.g., Apache, Nginx), and 
you are good to go. It is a quick and easy way to set up remote monitoring for 
personal projects.
In this tutorial, I am going to describe how to set up linux-dash in Nginx 
web server on Linux. Nginx is preferred over Apache web server due to its 
lightweight engine.
Set up linux-dash on Debian, Ubuntu or Linux Mint
First, install Nginx web server with php-fpm. xterm 
_______ xterm
$ sudo apt-get install git nginx php5-json php5-fpm php5-curl
__ ^^HConfigure Nginx for linux-dash app by creating
/etc/nginx/conf.d/linuxdash.conf as follows. In this example, we are going to use port
8080.
xterm
lb_ $ sudo vi /etc/nginx/conf.d/linuxdash.conf

server {
server_name $domain_name;
listen 8080;
root /var/www;
index index.html index.php; access_log /var/log/nginx/access.log; error_log 
/var/log/nginx/error.log;
location ~*  .
(?:xml|ogg|mp3|mp4|ogv|svg|svgz|eot|otf|woff|ttf|css|js|jpg|jpeg|gif|png|ic o)$ 
{
try_files $uri =404;
expires max;
access_log off;
add_header Pragma public;
add_header Cache-Control "public, must-revalidate, proxyrevalidate";
} 
location /linux-dash { 
index index.html index.php; 
}

# PHP-FPM via sockets
location ~ .php(/|$) {
fastcgi_param SCRIPT_FILENAME 
$document_root$fastcgi_script_name;
fastcgi_split_path_info a(.+? .php)(/. *)$;
fastcgi_pass unix:/var/run/php5-fpm.sock;
if (!-f $document_root$fastcgi_script_name) { 
return 404;

}
try_files $uri $uri/ /index.php?$args;
include fastcgi_params;
}
lb_ $ sudo rm /etc/nginx/sites-enabled/default

Configure php-fpm by editing /etc/php5/fpm/pool.d/www.conf. Make sure to edit user, 
group and listen directives as shown below. You can keep the rest of the 
configuration unchanged.
term
$ sudo vi /etc/php5/fpm/pool.d/www.conf 
user = www-data
group = www-data
listen = /var/run/php5-fpm.sock
Proceed to download and install linux-dash.
term
$ git clone https://github.com/afaqurk/linux-dash.git $ sudo cp -r linux-dash/ 
/var/www/
$ sudo chown -R www-data:www-data /var/www
Restart Nginx web server as well as php5-fpm to finalize installation.
xterm
Bl_ $ sudo service php5-fpm restart
$ sudo service nginx restart

Set up linux-dash on CentOS, Fedora or RHEL
On CentOS, it is necessary to enable EPEL repository first. 
Install Nginx web server and php-fpm component.
xterm
$ sudo yum install git nginx php-common php-fpm 
__ ^^B'lo configure Nginx for linux-dash app, create 
/etc/nginx/conf.d/linuxdash.conf as follows.
xterm
lb_ $ sudo vi /etc/nginx/conf.d/linuxdash.conf 
server {
server_name $domain_name;
listen 8080;
root /var/www;
index index.html index.php; access_log /var/log/nginx/access.log; error_log 
/var/log/nginx/error.log;
location ~*  .
(?:xml|ogg|mp3|mp4|ogv|svg|svgz|eot|otf|woff|ttf|css|js|jpg|jpeg|gif|png|ic o)$
{
try_files $uri =404;
expires max;
access_log off;
add_header Pragma public;
add_header Cache-Control "public, must-revalidate, proxyrevalidate";
}

location /linux-dash {
index index.html index.php; } 
# PHP-FPM via sockets
location ~ .php(/|$) {
fastcgi_param SCRIPT_FILENAME 
$document_root$fastcgi_script_name;
fastcgi_split_path_info a(.+? .php)(/. *)$;
fastcgi_pass unix:/var/run/php-fpm.sock;
if (!-f $document_root$fastcgi_script_name) { 
return 404;
}
try_files $uri $uri/ /index.php?$args;
include fastcgi_params;
}
}


Next, configure php-fpm by editing /etc/php-fpm.d/www.conf. In this file, make sure 
to set listen, user and group fields as below. You can leave the rest of the 
configuration unchanged.
term
$ sudo vi /etc/php-fpm.d/www.conf 
listen = /var/run/php-fpm.sock 
user = nginx
group = nginx
Download and install linux-dash under /var/www.
term
$ git clone https://github.com/afaqurk/linux-dash.git $ sudo cp -r linux-dash/ 
/var/www/
$ sudo chown -R nginx:nginx /var/www
__ ■^■Finally, restart Nginx web server as well as php-fpm, and set them to 
autostart upon boot.
xterm

$ sudo service php-fpm restart
$ sudo service nginx restart
$ sudo chkconfig nginx on
$ sudo chkconfig php-fpm on 
__B^^Bln this example, we have configured linux-dash to use TCP port 8080.
So make sure that the firewall is not blocking TCP port 8080.
Monitor a Linux Machine with linux-dash
To access linux-dash from a web browser, simply go to http://<linuxip- 
address>:8080/linux-dash/ on your web browser.
Below are the screenshots of linux-dash. The web dashboard consists of 
several widgets, each of which displays particular system properties. You can 
customize the look of the web dashboard by rearranging and/or closing some 
of the widgets. Check out some of the screenshots of linux-dash.

Inux-dash
flContributeonGitHub
t
Users
V
A simple web dashboard to monitor your server,
0 General Info 0
X
OS: Ubuntu 13.10 3,1 LO-12-generic
Uptime: 18 minutes
Sen/erTlme:SatApr510:1721CST 
2014
Hostname: ubuntu
fl Load average 0 X
Number of cores: 2
1 min
5 min
15 min
50*
32*
14*
LOO
0.64
0,28
b mm
cj xj
Total
Used
Free
1,995m«
203 ms
1,791mb
10*
90*
= Diskusage
0] x
= Software 0 X
FILESYSTEM
SIZE
USED
AVAIL
MOUNTED
SOME
IN5TAL1ATION
/dev/sdal
w
L8G
16G
1096
1
make
/usr/bin/make
none
4.0K
0
4,OK
096
/sys/fs/cgroup
mysql
/usr/bin/mysql
none
5.0M
0
5.0M
0%
/run/lock
openssl
/usr/bin/openssl
none
998M
0
998M
096
/run/shm
python
/usr/bin/python
none
W
0
TOOM
096
/run/user
nginx
/usr/sbln/nginx
I_
■_

Inux-dash
flContributeonGitHub
General
g
Disk
1
CPU
KAM
*
Users
-p
Network
0
Closed Widgets
*
0
PHPInfo’
i= DHCPLeases 0 X
EXPIRESAT
MAC ADDRESS
IPADDRESS
HOSTNAME
No data available In table
IP
3C
INTERFACE
IP
etho
192.168233.203/24
ethl
192.168.1.7/24
external ip
71.187.158.8
Io
127.0.0.1/8
Io
::l/128
Previous Next
Internet Speed
@897 KB/s
« X
V *
fi Network Statistics
X  
C 3
NUMBER OF CONNECTIONS
IPADDRESS
9
192.168.233.1
2 
192.168.233.2
First Previous 1 Next Last
S Ping
X
HOST
TIMEONMS)
wikipedia.org
116.128
gnti.org
78.079
github.com
100.903
Erst Previous 1 Next Last
3 Bandwidth
3 x
mt 1,980,9131 re 39,979
S Users 3
X
TYPE USER
HOME
S online
S x
S Last Login
3 x
WHO FROM
LOGIN AT IDLE
WHO FROM
WHEN

III
linux-dash 
DContributeonGitHub
General
I
Disk
i
CRJ
B 
KAM
Users
Network
0
Closed Widgets
*
0
PHPinfo’
£ users C
X
S Online
A 
V
X
® Last Login
S x
TYPE
USER
HOME
WHO
FROM
LOGIN AT
IDLE
WHO
FROM
WHEN
user
nobody
/nonexistent
xmodulo
192,168,233,1
10:12
4:37
moduli 192.168,233,1
SatApr 510:12:44 
+0800
user
dev
/home/dev
dev
09:59
16:33
0.16s
user
dan
/home/dan
dev
192.168,233.1
10:01
1:05
dan
192.168,233,1
SatApr 510:12:33
+0800
user 
xmodulo 
/home/xmodulo
system 
root 
/root
First Previous 1 13 4 5 Next Last
dan
First
10:12
ext Last
4:48
192.168.233,1
Previous 1N
dev
F
192.168.233,1
rst Previous 1
Sat Apr 510:01:02 
+0800
to Last
Processes
X
search,.,
USER
PID
%CPU
%MEM
V5Z
RS5
m
STAT START
TIME
COMMAND
102
484
0,0
0.0
30496
1264
7
Ss
09:58
0:00
dbus-daemon
dan
2649
0,0
0,0
94848
1776
1
5
10:12
0:00
sshd:
dan
2650
0,0
0.0
4440
628
pts/2
S;
10:12
0:00
•sh
dev
1536
0,0
0,1
22500
3720
tlyl
5+
09:59
0:00
•bash
dev
1791
0.0
0,0
94848
1776
?
5
L1L
0:00
sshd:
dev
1792
0,0
0,1
22564
3928
pts/1
5s+
0:00
•bash
dev
2632
80,0
0,0
20460
1736
pts/1
5
10:12
4:03
/usr/bin/perl

Linux Tutorials
How to create a GRE tunnel on
Linux
Question: I want to connect to remote networks by using a GRE tunnel. How 
can I create a GRE tunnel between two end points on Linux?
GRE tunnels are IP-over-IP tunnels which can encapsulate IPv4/IPv6 and 
unicast/multicast traffic. To create a GRE tunnel on Linux, you need ip_gre 
kernel module, which is GRE over IPv4 tunneling driver.
o first make sure that ip_gre is loaded.
xterm
T__ $ sudo modprobe ip_gre
$ Ismod | grep gre
__ I^Bip_gre 22432 0
gre 12989 1 ip_gre
Here, we assume that you want to create a GRE tunnel between two 
interfaces with the following IP addresses.
Host A: 192.168.233.204
Host B: 172.168.10.25
On host A, run the following command.
B__xterm
$ sudo ip tunnel add gre0 mode gre remote 172.168.10.25 local
192.168.233.204 ttl 255

$ sudo ip link set gre0 up
$ sudo ip addr add 10.10.10.1/24 dev gre0
In the above, we create a GRE-type tunnel device called gre0, and set its 
remote address to 172.168.10.25. Tunneling packets will be originating from 
192.168.233.204 (local IP address), and their TTL field will be set to
255. The tunnel device is assigned IP address 10.10.10.1 with netmask 
255.255.255.0.
Now verify that route for the GRE tunnel is set up correctly:
xterm
T $ ip route show
__ I^Bdefault via 135.112.29.1 dev eth0 proto static 
10.10.10.0/24 dev gre0 proto kernel scope link src 10.10.10.1
• — — - ,-------- — — - -  ------------ - ---- --- - —  ------------- - - ------,---- --- - - -
On host B, run similar commands as follows.
term
$ sudo ip tunnel add gre0 mode gre remote 
192.168.233.204 local 172.168.10.25 ttl 255
$ sudo ip link set gre0 up
$ sudo ip addr add 10.10.10.2/24 dev gre0
__ I^HAt this point, a GRE tunnel should be established between host A 
and host B.To verify that, from one tunneling end point, ping the other end 
point.
On host A, run:
xterm
■l _$ ping 10.10.10.2

PING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.
64 bytes from 10.10.10.2: icmp_req=1 ttl=64 time=0.619 ms
64 bytes from 10.10.10.2: icmp_req=2 ttl=64 time=0.496 ms
64 bytes from 10.10.10.2: icmp_req=3 ttl=64 time=0.587 ms
If you want to tear down the existing GRE tunnel, run the following 
command from either end.
xterm
lb__ $ sudo ip link set gre0 down
$ sudo ip tunnel del greO
Linux Tutorials
How to skip existing files when 
copying with scp
Question: I want to download (or upload) files from (or to) a remote server 
using the scp command. In this case, I want to skip existing files, so that they 
will not get overwritten by scp. But the scp command would blindly overwrite 
existing files if the same name files exist at either host. How can I copy files 
over without overwriting existing files, so that only new files are downloaded 
(or uploaded) by scp?

Suppose you have a list of files on a remote host, some of which already exist 
locally. What you want is to transfer only those files that are not found 
locally. If you blindly run scp with wildcard, it would fetch all remote files 
(existing as well as non-existing files), and overwrite existing local files. You 
want to avoid this.
In another similar situation, you may want to upload local files to a remote 
site, but without replacing any remote files.
Here are a few ways to skip existing files when transferring files with scp.
Method One: rsync
If the local and remote hosts have rsync installed, using rsync will be the easiest 
way to copy only new files over, since rsync is designed for 
incremental/differential backups.
In this case, you need to explicitly tell rsync to skip any existing files during 
sync. Otherwise, rsync will try to use file modification time to sync two hosts, 
which is not what you want.
To download all remote files (over SSH) while skipping existing local files: 
xterm
$ rsync -av --ignore-existing
[email protected]_host:/path/to/remote/directory/
*
 /path/to/local/directory/ 
-■^■Similarly, to upload all local files (over SSH) without overwriting 
any duplicate remote files:
xterm
$ rsync -av --ignore-existing
/path/to/local/directory/
*
[email protected]_host:/path/to/remote/directory/

Method Two: getfacl/setfacl
Another way to scp only new files over to a destination is by leveraging file 
permissions. More specifically, what you can do is to make all destination 
files "read-only" before scp transfer. This will prevent any existing destination 
files from being overwritten by scp. After scp transfer is completed, restore the 
file permissions to the original state. The ACL command-line tools (getfacl and 
setfacl) come in handy when you temporarily change file permissions and 
restore them.
Here is how to scp files without replacing existing files using ACL tools.
To download all remote files (over SSH) while skiping existing local files:
xterm
$ cd /path/to/local/directory

$ getfacl -R . > permissions.txt
$ chmod -R a-w .
$ scp -r
[email protected]_host:/path/to/remote/directory/
*
 . $ setfacl --
restore=permissions.txt
Similarly, to upload all local files without replacing any remote file, first back 
up the file permissions of the remote destination folder. Then remove 
writepermission from all files in the remote destination folder. Finally, upload 
all local files, and then restore the saved file permissions.
Linux Tutorials
How to create an encrypted disk 
partition on Linux
Suppose you have a portable USB drive to use with your Linux system. If 
you are security conscious, you may want to encrypt your USB drive, so that 
no one else tamper with content in your USB drive. There are many options 
to protect the drive with encryption. For example, you can encrypt particular 
files or directories using eCryptFS. You can also turn to full disk encryption. 
In particular, you can use dm-crypt and LUKS, which together provide 
transparent encryption of block devices based on device mapper subsystem.
In this tutorial, I will describe how to set up a disk partition encrypted by 
dmcrypt+LUKS on Linux.
Install dm-crypt on Linux
To encrypt a partition using dm-crypt+LUKS on Linux, install the following.
On Ubuntu, Mint or Debian:

term
T $ sudo apt-get install cryptsetup
On CentOS, Fedora or RHEL:
term
Bl_ $ sudo yum install cryptsetup
Create an Encrypted Partition
Using fdisk, create a new partition to encrypt as follows. In this example, I 
assume that /dev/sdb is mapped to your hard drive to encrypt.

00 © dev@localhost:~
[dev@localhost »]$ sudo fdisk /dev/sdb
Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklab 
el
Building a new DOS disklabel with disk identifier 0x70b505ac.
Changes will remain in memory only, until you decide to write them.
After that, of course, the previous content won't be recoverable.
Warning: invalid flag OxOOOO of partition table 4 will be corrected by w(rite)
WARNING: DOS-compatible mode is deprecated. It's strongly recommended to 
switch off the mode (command 'c') and change display units to 
sectors (command 'u').
Command (m for help): n
Command action
e extended
p primary partition (1-4)
P
Partition number (1-4): 1
First cylinder (1-130, default 1): 1
Last cylinder, +cylinders or +size{K,M,G] (1-130, default 130): 130
Command (m for help): w
The partition table has been altered!
Calling ioctl() to re-read partition table.
Syncing disks.
[dev@localhost ~]$
A newly created partition to use with LUKS is mapped to /dev/sdb1. Initialize 
this partition by using cryptsetup command. This command will overwrite the 
partition with random data, and prompt you for an initial passphrase to use.
term
$ sudo cryptsetup --verbose --verify-passphrase luksFormat /dev/sdbl

QB© dev@localhost:~
[dev@localhost »]$ sudo cryptsetup --verbose --verify-passphrase luksFomat /dev 
/sdbl
WARNING!
This will overwrite data on /dev/sdbl irrevocably.
Are you sure? (Type uppercase yes): YES
Enter LUKS passphrase:
Verify passphrase: 
Command successful. 
[dev@localhost ~]$
[dev@localhost »]$
You can check LUKS configuration of the partition by running the following 
command, which will dump LUKS header information.
xterm
lb__ $ sudo cryptsetup luksDump /dev/sdb1
___^^HlUKS header information for /dev/sdb1
Version: 1
Cipher name: aes
Cipher mode: cbc-essiv:sha256
Hash spec: sha1
Payload offset: 4096
MK bits: 256
MK digest: 18 1d 6d 3e e9 44 2a fe bf 67 78 8f aa 02 7f 91 2a f4 f2 17
MK salt: 26 cc 29 9f 0b 7d ea ff 44 9f fe 34 91 40 6e 9b
af 1e bd 8f d0 d2 1c 3a 70 30 35 5f 2d 49 9a 95
MK iterations: 222875
UUID: 5acc17e0-80be-40ba-beae-626e47b57379
Key Slot 0: ENABLED

Salt: 26 20 29 39 a5 1d 02 7b ca 8c bd 18 bc 29 64 7e 28 dc 06 65 78 0e 16
95 1a 67 14 66 12 2d a3 c1 Key material offset: 8
AF stripes: 4000
Key Slot 1: DISABLED
Key Slot 2: DISABLED
Key Slot 3: DISABLED
Key Slot 4: DISABLED
Key Slot 5: DISABLED
Key Slot 6: DISABLED
Key Slot 7: DISABLED

Next, open the LUKS partition as follows.
xterm
$ sudo cryptsetup luksOpen /dev/sdb1 sdb1
The above command will ask you to enter a passphrase. Once the LUKS 
partition is successfully opened with a correct passphrase, the encrypted 
partition will be mapped to /dev/mapper/sdb1. To check if this block device is 
created successfully, use this command:
term
sudo fdisk -l
$
Disk /dev/mapper/sdb1: 1067 MB, 1067156992 bytes

255 heads, 63 sectors/track, 129 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes 
Sector size (logical/physical): 512 bytes / 512 bytes 
I/O size (minimum/optimal): 512 bytes / 512 bytes 
Disk identifier: 0x7b0402f6
Finally, you can create a new filesystem on /dev/mapper/sdb1, and mount it on 
your Linux system:
term
Ilk_ $ sudo mkfs.ext3 /dev/mapper/sdbl
$ sudo mount /dev/mapper/sdbl /mnt
Mount LUKS-Encrypted Hard Drive Automatically 
on Boot
If you want to have your LUKS-encrypted partition mounted automatically 
upon boot, follow this procedure.
First, create a randomly generated key file used to open the encrypted 
partition during boot sequence. Make this key file readable by the root only.
$ sudo dd if=/dev/urandom of=/root/key.sdb1 bs=1024 count=4 
$ sudo chmod 400 /root/key.sdb1

Add the key file to LUKS configuration:
term
$ sudo cryptsetup luksAddKey /dev/sdb1 /root/key.sdb1
J Verify that the key file has been successfully added:
term
$ sudo cryptsetup luksDump /dev/sdb1
Key Slot 0: ENABLED
Salt: 26 20 29 39 a5 1d 02 7b ca 8c bd 18 bc 29 64 7e 28 dc 06 65 78 0e 16
95 1a 67 14 66 12 2d a3 c1 Key material offset: 8
AF stripes: 4000
Key Slot 1: ENABLED
Iterations: 404242
Salt: 9d b9 05 d4 06 be 8c db 74 bd cb 59 de 9a 95 8a 91 8c 09 5d 91 5f 0a e6
b5 86 3c 81 73 22 e1 db Key material offset: 264
AF stripes: 4000

Next, obtain the UUID of the encrypted block device.
xterm
T $ sudo cryptsetup luksUUID /dev/sdb1
__ ^■5acc17e0-80be-40ba-beae-626e47b57379
Now, edit /etc/crypttab to add the following entry.
xterm
T $ sudo vi /etc/crypttab
__ I^Bsdb1 /dev/disk/by-uuid/5acc17e0-80be-40ba-beae-626e47b57379
/root/key.sdb1 luks

The format of the entry in /etc/crypttab is as follows.
<name of encrypted block device> /dev/disk/by-uuid/<UUID of block 
device> <location of key file> luks
Finally, create a mount point, and edit /etc/fstab to add mount point 
information:
term
lb__ $ sudo mkdir /mnt_sdb1
$ sudo vi /etc/fstab
dev/mapper/sdb1 /mnt_sdb1 ext3
Reboot now. The encrypted partition should be auto-mounted upon boot up.
Linux Tutorials
How to convert PS/EPS file into
JPG image on Linux
If you want to convert a PostScript (PS) or an Encapsulated PostScript (EPS) 
file into JPG format on Linux, you can use ghostscript.
To install ghostscript on Ubuntu or Debian:

xterm
lb__ $ sudo apt-get install ghostscript
I Bio install ghostscript on CentOS, RHEL or Fedora:
xterm
lb__ $ sudo yum install ghostscript
__ ^^B'lo convert PS or EPS file to JPG format using ghostscript, run the 
following command.
xterm
■l _$ gs -sDEVICE=jpeg -dJPEGQ=100 -dNOPAUSE -dBATCH
dSAFER -r300 -sOutputFile=output.jpg input.eps
If you want to resize or rotate the generated a JPG image file, you can use 
imagemagick. A command-line utility called mogrify contained in imagemagick 
package can do necessary image transformation.
To install imageMagick on Ubuntu or Debian: 
xterm
xterm
$ sudo apt-get install imagemagick
I Ito install imageMagick on CentOS, RHEL or Fedora:
xterm
Hb__ $ sudo yum install ImageMagick
__ ^^Bonce imageMagick is installed, run mogrify command to resize and 
rotate an image.
xterm
H_ $ mogrify -trim -resize 800x600 -rotate 90 myfile.jpg__ L-J
Linux Tutorials
How to monitor Linux servers with
SNMP and Cacti

SNMP (or Simple Network Management Protocol) is used to gather data on 
what is going on within a device, such as load, hard disk states, bandwidth. 
These data are used by network monitoring tools such as Cacti to generate 
graphs for monitoring purposes.
In a typical deployment of Cacti and SNMP, there will be one or more 
SNMPenabled devices, and a separate monitoring server where Cacti collects 
SNMP feeds from those devices. Please keep in mind that all the devices that 
need to be monitored must be SNMP enabled. In this tutorial, we will be 
configuring Cacti and SNMP on the same Linux server for demonstration 
purpose.
Configure SNMP on Debian or Ubuntu
To install SNMP agent (snmpd) on a Debian-based system, run the following 
command.
xterm
T [email protected]:~# apt-get install snmpd
L [Then edit its configuration like the following.
xterm
T [email protected]:~# vim /etc/snmp/snmpd.conf
__ ^^B# this will make snmpd listen on all interfaces agentAddress 
udp:161 
# a read only community 'myCommunity' and the source network is defined 
rocommunity myCommunity 172.17.1.0/24 
sysLocation Earth 
sysContact [email protected]

After editing the config file, restart snmpd. 
xterm
lb__[email protected]:~# service snmpd restart
Configure SNMP on CentOS or RHEL
To install SNMP tools and libraries, run the following command. 
xterm
lb__[email protected]:~# sudo yum install net-snmp
L iTben edit an SNMP config file like the following. 
xterm
■t_ [email protected]:~# vim /etc/snmp/snmpd.conf 
# A user 'myUser' is being defined with the community string 
'myCommunity' and source network 172.17.1.0/24
com2sec myUser 172.17.1.0/24 myCommunity

# myUser is added into the group 'myGroup' and the permission of the group 
is defined
group myGroup v1 myUser
group myGroup v2c myUser
view all included .1
access myGroup "" any noauth exact all all none
term
lb__[email protected]:~# service snmpd restart
[email protected]:~# chkconfig snmpd on
Restart snmpd service, and add it to startup service list.
Testing SNMP
SNMP can be tested by running the snmpwalk command. If SNMP has been 
successfully configured, this command will generate a ton of output.
term
Hl_ [email protected]:~# snmpwalk -c myCommunity
172.17.1.44 -v1

iso.3.6.1.2.1.1.1.0 = STRING: "Linux mrtg 3.5.0-17-generic #28-Ubuntu
SMP Tue Oct 9 19:31:23 UTC 2012 x86_64"
iso.3.6.1.2.1.1.2.0 = OID: iso.3.6.1.4.1.8072.3.2.10
iso.3.6.1.2.1.1.3.0 = Timeticks: (2097) 0:00:20.97
~~ OUTPUT TRUNCATED ~~ 
iso.3.6.1.2.1.92.1.1.2.0 = Gauge32: 1440
iso.3.6.1.2.1.92.1.2.1.0 = Counter32: 1 
iso.3.6.1.2.1.92.1.2.2.0 = Counter32: 0
iso.3.6.1.2.1.92.1.3.1.1.2.7.100.101.102.97.117.108.116.1 = Timeticks: (1) 
0:00:00.01
iso.3.6.1.2.1.92.1.3.1.1.3.7.100.101.102.97.117.108.116.1 = HexSTRING: 07 
DD 0B 12 00 39 27 00 2B 06 00

Configure Cacti with SNMP
In this tutorial, we are setting up both Cacti and SNMP on the same Linux 
server. So go ahead and install Cacti on your Linux server on which SNMP 
was just configured.
After installation, Cacti web interface can be accessed at 
http://172.17.1.44/cacti, of course, in your case, the IP address of your 
server.

Cacti Installation Guide
Thanks for taking the time to download and install cacti,, the complete graphing 
solution for your network. Before you can start making cool graphs, there are a 
few pieces of data that cacti needs to know.
Make sure you have read and followed the required steps needed to install cacti 
before continuing, Install information can be found for Unix and ,Vir.32-based 
operating systems.
Also, if this is an upgrade, be sure to reading the Upgrade information file.
Cacti is licensed under the GNU General Public License, you must agree to its 
provisions before continuing:
This p Lcgrair. is free software; ycu can redistribute it and/or modify 
it under the terms of the GNU General Public Ltcenze as published by 
the Free Software Fcundaticn; either version 2 of the License, br |at 
your option) any later versicn.
This prcgraiT: is distributed in the hcpe that it will be useful, but 
WITHOUT ANY WARRANTY; without even the implied warranty of 
MERCHANTABILITY cr FITNESS FZ'R A PARTICULAR PURPOSE. See the C-NU 
General Public License for mare details.
Next >>
Cacti Installation Guide
Please select the type of installation
New Install
The following information has been determined from Cacti's configuration file. If it 
is not correct, please edit 'include/config.php' before continuing.
jatdbs.'.e User: cacti 
Gatabase Hcatname: local host 
□atabsae: zee Li 
Server Operating System Type: uriix
Next > >

The paths during Cacti installation are usually correct. But they can be double 
checked if necessary.

Cacti Installation Guide
Make sure al I of these values are correct before continuing.
[FOUND] RRDTool Binary Path: The path to the rrdtool binary,
/usr/bin/rrdtool
[OK; FILE FOUND]
[FOUND] PHP Binary Path: The path to your PHP binary file [may require a 
php recompile to get this file].
7usr/bn/php
[OK: FILE FOUND]
[FOUND] snmpwalk Binary Path: The path to your snmpwalk binary,
/usr/bin/snmpwalk
[OK: FILE FOUND]
[FOUND] snmpget Binary Path: The path to your snmpget binary.
/usr/bir./sr mpget
[OK: FILE FOUND]
[FOUND] snmpbulkwalk Binary Path: The path to your snmpbulkwalk 
binary.
/□sr/bin/snmpbijlkwalk
[OK: FILE FOUND]
[FOUND] snmpgetnext Binary Path: The path to your snmpgetnext binary.
/□ s r/b tn/s n mp g etn ex t
[OK: FILE FOUND]
[FOUND] Cacti Log File Path: The path to your Cacti log file.
'/va r/1 o g/cactc/ cacti, to g
[OK FILE FOUND]
SNMP Utility Version: The type of SNMP you have installed. Required if you 
are using SNMP v2c dt don't have embedded SNMP support in PHP
NET-SNMPS.x
RRDTool Utility Version. The version of RRDTool that you have installed
[RRDT00l 1.4.X :|
NOTE: Once you click "Finish", all of your settings will oe saved and your 
database will be upgraded if this is an upgrade. You can change any of the 
settings on this screen at a later time by going to "Cacti Settings" from within 
Cacti.
Finish

During the first-time installation, the default username and password for 
Cacti are admin and admin. You will be forced to change the password after first 
login.
Add and Manage Devices to Cacti
Cacti will poll devices based on SNMP string that was configured earlier. In 
this tutorial, we will add only the local server where SNMP is already 
enabled.
To add devices, we login as admin and go to console in the Cacti admin 
panel. We click Console > Devices.

Logged in a
Logout;
Nev< Graphs
Management
You are now logged into Cacti. You can follow these basic 
steps to get started.
• Create devices for network
• Create graphs for your new devices
• View your new graphs
Version
0.8.8a
Graph Management
Data Sources
Graph Trees
■ Devices ■
3
| Collectian Methods
Data Queries
Data Input Methods
There may already be a device named localhost. We do not need it as we will 
create fresh graphs. We can delete this device from the list. We add a new 
device by using the add button.
Next, we set the device parameters.

Now that the device has been added, we specify the graph templates that we 
want to create. This section can be found in the bottom section of the page. 
And then we proceed to creating the graphs.

Logged in as admin (Logout)
I Create Graphs^oTThl^HosF"
Data Source List
Graph List
Here, we create graphs for load average, RAM and hard disk, processor.
Graph Templates
Graph Template Name
Create: Unix - Load Average 
[7
Create: {Select a graph type to create) ▼
Data Query [SNMP - Get Mounted Partitions]
0
Index
Description
Storage Allocation Units
a
1
Physical memory
1024
3
Virtual memory
1024
s
6
Memory buffers
1024
n
?
Cached memory
1024
n
8
Shared memory
1024
n
to
Swap space
1024
a
31
/
4096
32
/sys/fe/fuse/connections
409S
a
33
/dev
4096
□
Data Query [SNMP - Get Processor Information]
Processor Index Number
Interface Graphs and 64-bit Counters

By default, Cacti uses 32-bit counters in SNMP queries. 32-bit counters are 
sufficient for most bandwidth graphs, but they do not work correctly for 
graphs greater than 100 Mbps. If it is known that the bandwidth will exceed 
Mbps. If it is known that the bandwidth will exceed bit counters is not hard at 
all.
Note: It takes around 15 minutes for Cacti to populate new graphs. There are 
not alternatives to being patient.
Creating Graph Trees
These snapshots illustrate how to create graph trees and how to add graph to 
those trees.
Console ■> Graph Trees 
Logged in as admin [Logout)
Graph Trees 
Add
Name
Graph Management
Graph Trees
Data Cnnrrcc
We can verify the graph in the graph tree.

User Management
Finally, we create a user with view permission to only the graph that we have 
created. Cacti has built in user management system, and it is highly 
customizable.

Logged in as admin (Lagcut;
Ccnscle -> User Management
Create
User Management
Add
Nev,1 Graphs
Search; 
Go Clear 
2
1 Management
Graph Management
Graph Trees
Data Sources
« Previous 
Showing Rows 1 to 2 of 2 [1] 
Next»1
User Name1* Full Name 
Enabled Realm Default Graph Polity Last Login
a
Devices
admin 
Administrator 
Yes 
Local 
ALLOW 
Wednesday, November 20,2013 20:16:41
guest 
Guest Account 
Yes 
Local 
ALLOW 
N/A
CdlactKn Methods
Data Queries
Data Input Methods
« Previous 
Showing Rows 1 to 2 of 2 [1] 
Next»
L 
—-----------
Choose an action- Delete ▼
Go
1 Templates
Graph Templates
Host Templates
Data Templates
| Import/Export
ImpaitTemplates
Expert Templates
Confignratmn
Settings
Utilities
System Utilities
User Management
Logout User

User Management [new]
User Name
The login name far this user.
Full Name
A more descriptive name far this user, that can include spaces or special 
characters,
Sample User 1
Password
••••••
Enter the password for this user twice, Remember that passwords are —
case sensitive!
••••••
Enabled
Determines if user is able to login,
-*]  Enabled!
Account Options
User Must Change Password at Next Login
Set any user account-specific options here,
V Allow this User to Keep Custom Graph Settings
2) User Has Rights to Tree View
Graph Options
PT! . 
।
Set any graph-specific options here,
I’Ll User Has Rights to List View
1*1  User Has Rights to Preview View
0 Show the page that user pointed their browser to.
Login Options
What to do when this user logs in,
Show the default console screen,
Show the default graph screen,
Only used if you have LDAP or Web Basic Authentication enabled,
Local ▼
Changing this to an non-enabled realm will effectively disable the user,
Realm permissions control which sections of Cacti this user will have access to.

Realm Permissions
Graph Permissions
Graph Settings
Graph policies will be evaluated in the order shown until a match is found.
Graph Permissions (By Graph)
Ul U^/ll ■ XJI ■ I 
W ■ 
\ y UlUJJliy
Default Policy
The default allow/deny graph policy for this user.
Deny ▼
X
1) This Server-Traffic-ethO 1
Add Graph: ▼
Add
Graph Permissions (By Device)
Default Policy
The default allow/deny graph policy for this user,
No Devices
Deny ▼
. 
_____  .. . 
All permissions should be
Add Host: This Server (172.17.1.23) ▼ 
settodeny
Add
Graph Permissions (By Graph Template)
Default Policy
The default allow/deny graph policy for this user.
No Graph Templates
Deny ▼
Add Graph Template: Cisco - CPU Usage 
▼
Add
Tree Permissions
Default Policy
The default allow/deny graph policy for this user,
Deny ▼
1) Sample Tree |
X
Add Tree: ▼
Add
After completing these steps, we can log in with the user user1 and verify that 
only this user is able to view the graph.

User Login
Please enter your Cacti user name and password below;
User Name: userl
Password: 
«*«««•
Login

In this tutorial, we have deployed a Cacti server in the network monitoring 
system, which then collects SNMP feeds from SNMP-enabled devices. Cacti 
servers are stable, and can deal with a variety of graphs. However, here the 
SNMP data feed is not protected with authentication and encryption. For 
stronger security, you should consider using SNMP v3 which comes with 
advanced security features.
Hope this helps.
Linux Tutorials

How to install FFmpeg on Linux
FFmpeg is cross-platform free software that can record, transcode, filter and 
stream audio and video data. FFmpeg builds are available for Linux, 
Windows and MacOS X platforms. While FFmpeg itself is a versatile 
multimedia tool for end-users, it also provides audio/video codec library with 
which many multimedia applications are built.
If you want to install FFmpeg on Linux, there are distro-specific ways to do 
it. In this post, I will describe how to install FFmpeg on various Linux 
distros.
Install FFmpeg on Ubuntu, Debian or Linux Mint
To install FFmpeg on Ubuntu, Debian or Linux Mint, you can compile it 
yourself. Follow this guide to build FFmpeg from the source.
Install FFmpeg on CentOS or RHEL
To install FFmpeg on CentOS or RHEL, first enable Repoforge on your 
system, and then run the following.
xterm
lb__ $ sudo yum install ffmpeg
Install FFmpeg on Fedora
To install FFmpeg on Fedora, first set up RPM Fusion on your system, and 
then use the following command.
xterm
lb__ $ sudo yum install ffmpeg
—■■If you want to compile FFmpeg from the source on a 
CentOS/Fedora-based system, follow this guide instead.

Find a List of Codecs Supported by FFmpeg
If you want to know what codecs are supported by FFmpeg, you can use the 
following commands.
To show available video codecs in FFmpeg:
xterm
T__ $ ffmpeg -codecs | grep "A.{3}V"
DEVSD flv Flash Video (FLV) / Sorenson Spark / Sorenson H.263 D V D 
fraps Fraps
DEV D gif GIF (Graphics Interchange Format)
DEV D h261 H.261
DEVSDT h263 H.263 / H.263-1996
D VSD h263i Intel H.263
EV h263p H.263+ / H.263-1998 / H.263 version 2
D V D h264 H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 D V D 
h264_vdpau H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10 (VDPAU 
acceleration)

To show available audio codecs in FFmpeg:
xterm
lb__ $ ffmpeg -codecs | grep "A.{3}A"
D A D 8svx_exp 8SVX exponential
D A D 8svx_fib 8SVX fibonacci
DEA D aac Advanced Audio Coding
D A D aac_latm AAC LATM (Advanced Audio Codec LATM syntax) DEA
D ac3 ATSC A/52A (AC-3)
EA ac3_fixed ATSC A/52A (AC-3)
D A D adpcm_4xm ADPCM 4X Movie
DEA D adpcm_adx SEGA CRI ADX ADPCM
D A D adpcm_ct ADPCM Creative Technology
D A D adpcm_ea ADPCM Electronic Arts
D A D adpcm_ea_maxis_xa ADPCM Electronic Arts Maxis CDROM XA . .

Linux Tutorials
How to convert MP3 ID3 tag 
encodings on Linux
Question: When I load MP3 files on Rhythmbox music player, song titles 
and artist names appear as gibberish and unreadable characters. I suspect this 
is an ID3 tag's character encoding problem. How can I fix this problem?

If your music player does not display some MP3 tracks correctly (e.g., with 
garbled symbols), it is most likely that the character encoding of ID3 tags is 
not supported on your Linux system. You can often encounter this problem 
when song titles or artist names are written in Cyrillic, Greek, Chinese, 
Japanese or Korean characters.
To solve this problem, you need to convert character encoding of ID3 tags to 
UTF-8/Unicode. The very tool that can do such conversion is mid3iconv 
command-line tool. Using mid3iconv, you can convert ID3 tags from any 
arbitrary character encoding to Unicode.
Install mid3iconv on Linux
The mid3iconv tool is part of python-mutagen package, which is universally 
available on major Linux platforms.
For Debian, Ubuntu or Linux Mint:
term
__ $ sudo apt-get install python-mutagen

For CentOS, Fedora or RHEL:
term
Bl_ $ sudo yum install python-mutagen
Convert ID3 Character Encodings with mid3iconv
The typical command-line usage of mid3iconv is as follows.
term
T $ mid3iconv -e <source-encoding> -d input.mp3
The above command converts the character encoding of input MP3 file's ID3 
tag from <source-encoding> to Unicode. If you want to "dry-run" the conversion 
without actually modifying a MP3 file, you can add -p option.
For example, to convert ID3's character encoding from cp1252 to utf
8/Unicode:
lb__ $ mid3iconv -e CP1251 -d input.mp3
__ ^^Bwithout -p option, the converted ID3 tag will be overwritten to an 
input MP3 file.
Fix Broken Characters in MP3 Song Titles and 
Artist Names
Now let's try to fix broken characters in MP3 ID3 tags with mid3iconv.
The first thing you need to do is to identify the character encoding used by 
your MP3 files. You may be able to guess their character encoding if you 
know where they are from. For example, if MP3 files are Japanese songs, the 
encoding may be shift-jis or euc-jp, while Chinese songs may be encoded with 

gb 2312 or gbk, etc. If you are not sure, you can use
mid3iconv's "dry-run" option to find it out.
That is, choose any encoding from:
xterm
Bl_ $ iconv --list
-LJand apply conversion (with "dry-run" option):
xterm
T $ mid3iconv -p -d e <source-encoding> input.mp3
__ ^Hlf the output is readable, that means you guessed a source encoding 
correctly.
Once you know what ID3 encoding your MP3 files are using, use the 
following command to batch-convert ID3 character encoding for all your 
MP3 files.
xterm
Bl_ $ find . -name "*mp3"  -printO | xargs -0 mid3iconv -e <source­
encoding> -d
_J^HFor example, if ID tags of your MP3 songs are encoded with euc-kr 
(Korean characters), use the below command to fix broken characters.
xterm
Bb_ $ find . -name "*mp3"  -printO | xargs -0 mid3iconv -e EUR-KR -d
__ Verify that MP3 files are displayed correctly.

Linux Tutorials
How to find which jar file a Java

class belongs to
Suppose you have multiple jar files, and want to know which jar file contains 
a given Java class definition. This kind of situation can arise when you are 
dealing with noclassdeffounderror in your Java program.
The JAR (Java ARchive) format is built on ZIP file format, so in order to 
check what Java class definitions are available in a given jar file, you need to 
check the jar file in an uncompress form. Is there any convenient way to 
inspect all jar files in search for Java class definition?
Method One
In fact, it is not that difficult to find out a list of Java classes contained in a 
given jar file. With tvf option, jar command can show you a table of contents 
in jar file. For example:
xterm
lb__ $ jar tvf gson-2.2.2.jar 
0 Mon Jul 02 18:04:46 EDT 2012 META-INF/
705 Mon Jul 02 18:04:44 EDT 2012 META-INF/MANIFEST.MF
0 Mon Jul 02 18:04:40 EDT 2012 com/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/stream/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/internal/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/internal/bind/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/annotations/
0 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/reflect/
732 Mon Jul 02 18:04:40 EDT 2012
com/google/gson/JsonParseException.class
1001 Mon Jul 02 18:04:40 EDT 2012 com/google/gson/Gson$1.class
413 Mon Jul 02 18:04:40 EDT 2012

com/google/gson/JsonDeserializationContext.class 
207 Mon Jul 02 18:04:40 EDT 2012
com/google/gson/FieldNamingStrategy.class
4762 Mon Jul 02 18:04:40 EDT 2012
com/google/gson/DefaultDateTypeAdapter.class
Leveraging this feature of jar command, you can run the following to find the 
jar file that contains a specific Java class definition. In this example, I 
want to know from what jar file com.google.gson.jsonparser.class is loaded. Run this 
command in the directory where jar files are located.
$
term
find . -name "*.jar"  | while read JAR; do jar tvf $JAR | grep
JsonParser && echo $JAR ; done
2594 Mon Jul 02 18:04:40 EDT 2012
com/google/gson/JsonParser.class ./gson-2.2.2.jar

Method Two
Another approach is to use a GUI program called jar-explorer, which allows you 
to explore the content of individual jar files, as well as recursively search for 
Java classes in all jar files in a directory. To install and run jarexplorer, do the 
following.
$ wget https://github.com/javalite/jar 
explorer/archive/v0.7.tar.gz
$ tar -xf v0.7.tar.gz
$ cd jar-explorer-0.7/bin
$ java -jar jarexplorer-0.7.jar
Once you have launched jar-explorer, go to File -^ "Locate Root Directory or Jar File" 
menu. Select the directory where jar files (and any subdirectories containing 
jar files) are located. Under "Jar File List" upper window, you will then see a list 
of all existing jar files. Enter the name of Java class you are searching for 
(e.g., jsonparser), in the search blank at the top.
As you can see in the screenshot above, there is one jar file (e.g., gson2.2.2.jar) 
that contains jsonparser.
Using jar-explorer, you can also explore the content of each Java class 
definition, simply by clicking on the class name.

Linux Tutorials
How to enable PowerTools on
CentOS 8
The PowerTools repository, which is available on CentOS/RHEL 8, provides 

developer related tools and libraries. Some of common EPEL packages 
depend on packages available from PowerTools. Thus if you have set up the 
EPEL repository on your CentOS system, it is recommended that you enable 
PowerTools as well.
To enable PowerTools repository on CentOS/RHEL 8, you can use the config­
manager plugin for DNF, which manages various DNF configuration 
options, including adding/removing or enabling/disabling repositories.
Prereqsuite: DNF conf-manager plugin
To install config-manager plugin, run the following command:
xterm
lb__ $ sudo yum install dnf-plugins-core
Enable PowerTools with DNF config-manager
CentOS/RHEL 8 comes with PowerTools repository already added. Thus, 
you simply need to enable it as follows.
B__xterm
Bf_ $ sudo dnf config-manager --set-enabled powertools
U Verify that PowerTools is successfully enabled:
xterm
$ dnf repolist

To see a list packages available from PowerTools repository:
xterm
T__ $ dnf repo-pkgs powertools list

Linux Tutorials
How to boot on an ISO image from
GRUB

If you need to use multiple Linux distributions, you do not have so many 
options. You can either install it, on your machine or in a virtual one, or you 
can boot on it from an ISO file in live mode. The second option, if less needy 
in hard drive space, is bothersome as you will need a USB stick or CD to 
hold the ISO file and boot from. However, there is a third alternative which is 
a bit of a compromise: have the ISO image on your hard drive, and boot into 
it in live mode at startup. Less needy in disk space than a full install, and 
fully functional, this seems to be a good alternative to a slow virtual machine. 
I will explain how to do this using the popular bootloader GRUB.
What Do You Need?
Obviously you will need to be using GRUB, which is the case on pretty much 
all modern Linux distributions. You will also need to have an ISO file of the 
distribution of your choice, downloaded on your hard drive. Finally, you 
should know where your boot partition is, and how to express it in GRUB 
language. For this, launch:
xterm
Hl_ # fdisk -l
_J^Mand the boot partition is the one with the *. For me it's /dev/sdai, 
which is written (hdo,i) in GRUB language.
Device
/dev/sdal
Boot 
Start
* 
2048
End
821247
Sectors
819200
Size Id Type
400M 27 Hidden NTFS WinRE
/dev/sda2
821248
367943679
367122432 175,1G 7 HPFS/NTFS/exFAT
/dev/sda3
367943680
368730864
787185 384,4M 83 Linux 
,
/dev/sda4
368730865 1465149167 1096418303 522,8G 83 Linux
For reference, /dev/sda2 would be (hd0,2), /dev/sdbi would be (hdi,i), etc. (You got 
the point).
What Do We Edit?
First, open /etc/default/grub and check that the following line:
#GRUB_HIDDEN_TIMEOUT=o

is indeed commented with a # in front.
Save it and then go open /etc/grub.d/40_custom.
This file will be where you put the arguments to boot on an ISO. The 
structure is of the form:
menuentry "[Entry's title in the grub screen]" {
set isofile="[path to ISO file]"
loopback loop [boot partition in Grub language]$isofile
[some specific] arguments
}
For example, if you wish to boot Ubuntu from an ISO file, you want to 
append this to your 40_custom file:
menuentry "Ubuntu 14.04 (LTS) Live Desktop amd64" { 
set isofile="/boot/ubuntu-14.04-desktop-amd64.iso" 
loopback loop (hd0,1)$isofile

linux (loop)/casper/vmlinuz.efi boot=casper iso-scan/filename=${isofile} 
quiet splash
initrd (loop)/casper/initrd.lz
}
If you would like to launch Gparted instead:
menuentry "GParted Live amd64" {
set isofile="/boot/gparted-live-0.18.0-2-amd64.iso"
loopback loop (hd0,1)$isofile
loopback loop $isofile

linux (loop)/live/vmlinuz boot=live config union=aufs noswap noprompt 
ip=frommedia toram=filesystem.squashfs findiso=${isofile}
initrd (loop)/live/initrd.img
}
Or even Fedora:
menuentry "Fedora 20 Live Desktop x86_64" {
set isofile="/boot/Fedora-Live-Desktop-x86_64-20-1.iso"
loopback loop (hd0,1)$isofile
loopback loop $isofile
linux (loop)/isolinux/vmlinuz0 root=live:CDLABEL=Fedora-Live-

Desktopx86_64-20-1 rootfstype=auto ro rd.live.image quiet rhgb rd.luks=0
rd.md=0 rd.dm=0 iso-scan/filename=${isofile}
initrd (loop)/isolinux/initrd0.img
}
Note that the arguments will change depending on the distribution. 
Hopefully, there are a few places where you can start looking. I like this one, 
but there are plenty of others. Also, take into consideration where you place 
your ISO files. If your home directory is encrypted or out of reach, you might 
prefer placing the files in your boot partition like in the example. But make 
sure that you have enough room first.
inally, don't forget to save 40_custom and update GRUB with: 
xterm
# sudo update-grub
__ ^Hto see your changes appear at the next boot.

GNU GRUB version 2.02~beta2-9ubuntul
kubuntu
Advanced options for Ubuntu
Memory test (memtest86+)
Memory test (memtest86+, serial console 115200)
Ubuntu 14.04 (LTS) Live Desktop amd64
Use the J and I keys to select which entry is highlighted. 
Press enter to boot the selected OS, 'e' to edit the commands 
before booting or 'c' for a command-line.
What Next?
Want more? Well by playing a bit with the arguments, you can boot on an 
ISO and have it do something instantly. For example, if you are completely 
paranoid and wish to have an option to erase your hard drive quickly, it is 
possible to set up something using DBAN. Now be really careful as this will 
wipe your disk without recovery option on startup: 
menuentry "Darik's Boot and Nuke" { 
set isofile="/boot/dban.iso"

loopback loop (hd0,1)$isofile
linux (loop)/dban.bzi nuke="dwipe" silent 
}
Alternatively, 
menuentry "Darik's Boot and Nuke" { 
set isofile="/boot/dban.iso" 
loopback loop (hd0,1)$isofile 
linux (loop)/dban.bzi 
}
will show you the DBAN's options and let you choose to wipe your drive. Be 
careful as it is still very dangerous.
To conclude, there are plenty of things to do with ISO files and Grub: 
whether you want a quick live session or prefer to destroy everything with the 
tip of your finger. The next step would be to launch some privacy focused 
distribution like Tails for example.

What do you think of booting on an ISO from GRUB? Is it something you 
would consider doing? And why? Let us know in the comments.
Linux Tutorials
How to fix "404 Not Found" error 
with "apt-get update" on old 
Ubuntu
Question: I have old Ubuntu 13.04 (Raring Ringtail) installation on my PC. 
When I run "sudo apt-get update" on it, it throws a bunch of "404 Not Found" errors, 
and I cannot install or update any package using aptget or aptitude. Due to this 
error I cannot even upgrade it to a newer release. How can I fix this problem?
W: Failed to fetch http://us.archive.ubuntu.com/ubuntu/dists/raring-backports/ 
universe/binary-i386/Packages 404 Not Found [IP: 91.189.91.13 80]
W: Failed to fetch http://us.archive.ubuntu.com/ubuntu/dists/raring-backports/ 
multiverse/binary-i386/Packages 404 Not Found [IP: 91.189.91.13 80]
W: Failed to fetch http://extras.ubuntu.com/ubuntu/dists/raring/main/source/So 
urces 404 Not Found
W: Failed to fetch http://extras.ubuntu.com/ubuntu/dists/raring/main/binary-am 
d64/Packages 404 Not Found
W: Failed to fetch http://extras.ubuntu.com/ubuntu/dists/raring/main/binary-i3 
86/Packages 404 Not Found
E: Some index files failed to download. They have been ignored, or old ones us 
ed instead.
-i —.1....- -l_. ,______ db__n___________________________________________________________________________________________________________________________
Every Ubuntu release has its end-of-life (EOL) time; regular Ubuntu releases 
are supported for 18 months, while LTS (Long Term Support) versions are 
supported up to 3 years (server edition) and 5 years (desktop edition). Once a 
Ubuntu release has reached EOL, its repositories will no longer be accessible, 
and you won't get any maintenance updates and security patches from

Canonical. As of this writing, Ubuntu 13.04 (Raring Ringtail) has already 
reached EOL.
If the Ubuntu system you are using is EOL-ed, you will get the following 404 
errors from apt-get or aptitude as its repositories have been deprecated.
W: Failed to fetch
http://us.archive.ubuntu.com/ubuntu/dists/raringbackports/multiverse/binary- 
i386/Packages 404 Not Found [IP: 91.189.91.13 80]
W: Failed to fetch
http://extras.ubuntu.com/ubuntu/dists/raring/main/binaryamd64/Packages 404 
Not Found
W: Failed to fetch
http://security.ubuntu.com/ubuntu/dists/raringsecurity/universe/binary- 
i386/Packages 404 Not Found [IP: 91.189.88.149 80]
E: Some index files failed to download. They have been ignored, or old ones 
used instead

W: Failed to fetch
http://security.ubuntu.com/ubuntu/dists/utopicsecurity/main/source/Sources 
404 Not Found
W: Failed to fetch
http://security.ubuntu.com/ubuntu/dists/utopicsecurity/restricted/source/Sourc 
404 Not Found
W: Failed to fetch
http://security.ubuntu.com/ubuntu/dists/utopicsecurity/universe/source/Source 
404 Not Found
W: Failed to fetch

http://security.ubuntu.com/ubuntu/dists/utopicsecurity/multiverse/source/Sour 
404 Not Found
W: Failed to fetch
http://security.ubuntu.com/ubuntu/dists/utopicsecurity/main/binary- 
amd64/Packages 404 Not Found
For those users who are using old versions of Ubuntu, Canonical maintains 
http://old-releases.ubuntu.com, which is an archive of EOL-ed repositories. 
Thus, when Canonical's support for your Ubuntu installation ends, you need 
to switch to repositories maintained at http://old-releases.ubuntu.com (unless 

you want to upgrade it before EOL).
Here is a quick way to fix "404 Not Found" errors on old Ubuntu by switching to 
old-releases repositories.
First, replace main/security repositories with old-releases versions as follows.
xterm
$ sudo sed -i -r 's/([a-z]{2}.)?archive.ubuntu.com/oldreleases.ubuntu.com/g'
/etc/apt/sources.list
$ sudo sed -i -r 's/security.ubuntu.com/old
releases.ubuntu.com/g' /etc/apt/sources.list
Then open /etc/apt/sources.list with a text editor, and look for extras.ubuntu.com. This 
repository is also no longer supported for EOL-ed
releases. So you need to comment out extras.ubuntu.com by prepending # sign.
#deb http://extras.ubuntu.com/ubuntu raring main
#deb-src http://extras.ubuntu.com/ubuntu raring main
Now you should be able to install or update packages on an old unsupported 
Ubuntu release.
Linux Tutorials
How to install Java runtime on
Linux
Question: I want to run a Java application on my Linux. For that, I need to

set up Java Runtime Environment (JRE). How can I install Java Runtime on 
[insert your Linux distro]?
Java Runtime Environment (JRE) or Java Runtime is a minimal set of Java 
SDK, which allows one to run Java applications. JRE consists of Java Virtual 
Machine (JVM), libraries, and other supporting components. If you want to 
run a complied Java application, you do not have to install a full-featured 
Java development kit, but just need JRE installed.
There are two choices for setting up JRE on Linux: use either Oracle JRE or 
OpenJDK JRE (open source implementation of the Java Platform). Choose 
either one you prefer, as described in the following.
Install Oracle JRE on Linux
Go to Oracle website, and click on jre Download link.

« JDK: (Java Development Kit) For Java Developers. Includes a complete JRE plus tools for 
developing, debugging, and monitoring Java applications.
• Server JRE: (Server Java Runtime Environment} For deploying Java applications on servers. 
Includes tools for JVM monitoring and tools com mon lyjequi red for server applications, but does not 
include browser integration (the Java plug-in), auto-updbte. noran installer. Learn more *
• JRE: (Java Runtime Environment). Covers most end-users naeds. Contains everything required to 
run Java applications on your system. 
'v
JDK 
Server JRE 
JRE
JDK 7 Docs
Server JRE 7 Docs
JRE7 Docs
■ Installation Instructions
■ Installation Instructions
- Installation Instructions
- ReadMe
■ ReadMe
■ ReadMe
■ Release Notes
■ Release Notes
■ Release Notes
■ Oracle License
- Oracle License
- Oracle License
■ Java SE Products
■ Java SE Products
- Java SE Products
- Third Party Licenses
■ Third Parti'Licenses
■ Third Party Licenses
■ Certified System
* Certified System
» Certified System
Configurations
Configurations
Configurations
On the next page, accept Oracle license agreement. Then download a file 
according to your Linux system:

Non-RPM based Linux (e.g., Debian, Ubuntu, Mint): jre-7u45-linuxi586.tar.gz 
(32-bit) or jre-7u45-linux-x64.tar.gz (64-bit) RPM-based Linux (e.g., Fedora, 
CentOS, RHEL): jre-7u45-linuxi586.rpm (32-bit) or jre-7u45-linux-x64.rpm (64-bit) 
To install Oracle JRE on RPM-based Linux:
$ sudo rpm -ivh jre1.7.0_45-linux-<arch>.rpm
I Ito install Oracle JRE on non RPM-based Linux:
xterm
$ sudo mkdir -p /usr/java
$ sudo tar xvfvz jre1.7.0_45-linux-<arch>.tar.gz -C /usr/java 
__B^MAfter installation, set the following environment variables in ~/.bashrc
JAVA_HOME=/usr/java/jre1.7.0_45
PATH=$PATH:$HOME/bin:$JAVA_HOME/bin

Install OpenJDK Java Runtime on Linux
To install OpenJDK Java Runtime on Debian, Ubuntu or Linux Mint:
term
__ $ sudo apt-get install openjdk-7-jre
To install OpenJDK Java Runtime on Fedora, CentOS or RHEL:
term
lb_ $ sudo yum install java-1.7.0-openjdk
_J^HAfter installation, set JAVA_HOME environment variable in
~/.bashrc
JAV A_HOME=/etc/alternative/j re
If you cannot find /etc/alternative/jre folder, set java_home to an alternative path 
(e.g., /usr/lib/jvm/default-java).
Linux Tutorials
How to fix "Your profile could not 
be opened correctly" on Google 
Chrome
Question: When I open Google Chrome web browser on my Linux box, I 
have several pop-up messages with:

Your profile could not be opened correctly.
This error happens every time I open Google Chrome. How can I solve this 
error?
When you see an error message saying "Your profile could not be opened 
correctly" on your Google Chrome web browser," that may be because 
somehow your profile data on Google Chrome got corrupted. This can 
happen while you upgrade your Google Chrome browser manually on Linux.
I
Your profile could not be opened correctly.
Some features may be unavailable. Please 
check that the profile exists and you have 
permission to read and write its contents.
OK
Depending on
exactly which file got corrupted, you can try one of these methods.
Method One
Close all your Chrome browser windows/tabs.
Go to ~/.config/google-chrome/Default, and remove/rename "Web Data" file as below.
term
T__ $ cd ~/.config/google-chrome/Default
$ rm "Web Data"
Re-open Google Chrome browser.
Method Two

Close all your Chrome browser windows/tabs.
Go to ~/.config/googie-chrome/"Profiie 1", and rename History file as below.
xterm
T__ $ cd ~/.config/google-chrome/"Profile 1"
$ mv History History.bak
__ ^^^Re-open Google Chrome browser.
Method Three
If the problem still persists, you can remove the Default profile folder
( ~/.config/google-chrome/Default) altogether. Note that by doing so, you will lose all 
previously opened Google tabs, imported bookmarks, browsing history, sign­
in data, etc.
Before removing it, first close all your Chrome browser windows/tabs.
I______^^Ixterm
T $ rm -rf ~/.config/google-chrome/Default
__ After restarting Google Chrome, the folder ~/.config/googlechrome/Default 
will automatically be re-generated.
Method Four
If none of the above solutions fixes the problem, it may not be due to profile 
data corruption. If you are running Red Hat based Linux (e.g., CentOS, 
Fedora, RHEL), another potential cause for this problem is SELinux. Try the 
following command to see if it helps.
$
term
sudo setsebool -P
unconfined_chrome_sandbox_transition off
Linux Tutorials

How to configure fail2ban to protect 
Apache HTTP server
Apache web server in production environments can be under attack in various 
different ways. Attackers may attempt to gain access to unauthorized or 
forbidden directories by using brute-force attacks or executing evil scripts. 
Some malicious bots may scan your websites for any security vulnerability, 
or collect email addresses or web forms to send spams to.
Apache HTTP server comes with comprehensive logging capabilities 
capturing various abnormal events indicative of such attacks. However, it is 
still nontrivial to systematically parse detailed Apache logs and react to 
potential attacks quickly (e.g., ban/unban offending IP addresses) as they are 
perpetrated in the wild. That is when fail2ban comes to the rescue, making a 
sysadmin's life easier.
fail2ban is an open-source intrusion prevention tool which detects various 
attacks based on system logs and automatically initiates prevention actions 
e.g., banning IP addresses with iptables, blocking connections via
/etc/hosts.deny , or notifying the events via emails. fail2ban comes with a set of 
predefined jails which use application-specific log filters to detect common 
attacks. You can also write custom jails to deter any specific attack on an 
arbitrary application.
In this tutorial, I am going to demonstrate how you can configure fail2ban to 
protect your Apache HTTP server. I assume that you have Apache HTTP 
server and fail2ban already installed. Refer to another tutorial for
fail2ban installation.
What is a Fail2ban Jail?
Let me go over more detail on fail2ban jails. A jail defines an 
applicationspecific policy under which fail2ban triggers an action to protect a 

given application. fail2ban comes with several jails pre-defined in
/etc/fail2ban/jail.conf , for popular applications such as Apache, Dovecot, 
Lighttpd, MySQL, Postfix, SSH, etc. Each jail relies on applicationspecific 
log filters (found in /etc/fail2ban/fileter.d) to detect common attacks. Let's check 
out one example jail: ssh jail.
[ssh]
enabled = true
port = ssh
filter = sshd
logpath = /var/log/auth.log
maxretry = 6
banaction = iptables-multiport
This SSH jail configuration is defined with several parameters:
[ssh]: the name of a jail with square brackets.
enabled: whether the jail is activated or not.
port: a port number to protect (either numeric number of well-known 
name).
filter: a log parsing rule to detect attacks with. 

logpath: a log file to examine.
maxretry: maximum number of failures before banning.
banaction: a banning action.
Any parameter defined in a jail configuration will override a corresponding 
fail2ban-wide default parameter. Conversely, any parameter missing will 
be assgined a default value defined in [default] section.
Predefined log filters are found in /etc/fail2ban/filter.d, and available actions are in
/etc/fail2ban/action.d.
Built-in log filters
dropbear.conf 
exim.conf 
gssftpd.conf 
lighttpd-auth.conf 
lighttpd-fastcgi.conf 
mysqld-auth.conf 
named-refused.conf 
pam-generic.conf 
php-url-fopen.conf 
postfix.conf 
proftpd.conf 
pure-ftpd.conf 
qmail.conf
recidive.conf
roundcube-auth.conf 
sasl.conf
sieve.conf 
sogo-auth.conf 
sshd.conf
sshd-ddos.conf 
vsftpd.conf 
webmin-auth.conf 
wuftpd.conf 
xinetd-fail.conf
$
$
$ Is /etc/fail2ban/filter.d
apache-auth.conf 
apache-badbots.conf 
apache-common.conf 
apache-nohome.conf 
apache-noscript.conf 
apache-overflows.conf 
assp.conf 
asterisk.conf 
common.conf 
courierlogin.conf 
couriersmtp.conf 
cyrus-imap.conf 
dovecot.conf
$ 
S
bsd-ipfw.conf
iptables- new.conf
complain.conf
iptables-xtrecent-echo.conf
dshield.conf
mail-buffered.conf
dummy.conf
mail.conf
hostsdeny.conf
mail-whois.conf
ipfilter.conf
mail-whois-lines.conf
ipfw.conf
mynetwatchman.conf
iptables-allports.conf
pf.conf
iptables-blocktype.conf
route.conf
iptables.conf
sendmail-buffered.conf
iptables-ipset-proto4.conf
sendmail.conf
iptables-ipset-protoG.conf
sendmail-whois.conf
iptables-multiport.conf
sendmail-whois-lines.conf
iptables-multiport-log.conf
shorewall.conf
If you want to overwrite fail2ban defaults or define any custom jail, you can do 
so by creating /etc/fail2ban/jail.local file. In this tutorial, I am going to use 
/etc/fail2ban/jail.local.

Enable Predefined Apache Jails
Default installation of fail2ban offers several predefined jails and filters for 
Apache HTTP server. I am going to enable those built-in Apache jails. Due to 
slight differences between Debian and Red Hat configurations, let me provide
fail2ban jail configurations for them separately.
Enable Apache Jails on Debian or Ubuntu
To enable predefined Apache jails on a Debian-based system, create 
/etc/fail2ban/jail.local as follows.
xterm
T__ $ sudo vi /etc/fail2ban/jail.local
# detect password authentication failures [apache]
enabled = true
port = http,https
filter = apache-auth
logpath = /var/log/apache
*/*error.log
 maxretry = 6
# detect potential search for exploits and php vulnerabilities [apache-noscript] 
enabled = true
port = http,https
filter = apache-noscript
logpath = /var/log/apache
*/*error.log
maxretry = 6
# detect Apache overflow attempts
[apache-overflows]
enabled = true
port = http,https
filter = apache-overflows
logpath = /var/log/apache
*/*error.log
maxretry = 2

# detect failures to find a home directory on a server 
[apache-nohome]
enabled = true
port = http,https

filter = apache-nohome
logpath = /var/log/apache
*/*error.log
maxretry = 2
Since none of the jails above specifies an action, all of these jails will 
perform a default action when triggered. To find out the default action, look 
for banaction under [default] section in /etc/fail2ban/jail.conf. banaction = iptables- 
multiport
In this case, the default action is iptables-multiport (defined in 
/etc/fail2ban/action.d/iptables-multiport.conf). This action bans an IP address using 
iptables with multiport module.
After enabling jails, you must restart faii2ban to load the jails.
term
T__ $ sudo service fail2ban restart
Enable Apache Jails on CentOS/RHEL or Fedora
To enable predefined Apache jails on a Red Hat based system, create 
/etc/fail2ban/jail.local as follows.
term
T__ $ sudo vi /etc/fail2ban/jail.local

# detect password authentication failures 
[apache]
enabled = true
filter = apache-auth
logpath = /var/log/httpd/
*error_log
 maxretry = 6
# detect spammer robots crawling email addresses [apache-badbots]
enabled = true
port = http,https
filter = apache-badbots
logpath = /var/log/httpd/
*access_log
bantime = 172800
maxretry = 1
# detect potential search for exploits and php vulnerabilities [apache-noscript] 
enabled = true
port = http,https
filter = apache-noscript
logpath = /var/log/httpd/
*error_log
maxretry = 6
# detect Apache overflow attempts [apache-overflows]
enabled = true
port = http,https
filter = apache-overflows
logpath = /var/log/httpd/
*error_log
 maxretry = 2
# detect failures to find a home directory on a server 
[apache-nohome]

enabled = true


filter = apache-nohome
logpath = /var/log/httpd/
*error_log
 maxretry = 2
# detect failures to execute non-existing scripts that
# are associated with several popular web services
# e.g. webmail, phpMyAdmin, WordPress
port = http,https
filter = apache-botsearch
logpath = /var/log/httpd/
*error_log
maxretry = 2
Note that the default action for all these jails is iptables-multiport (defined as 
banaction under [default] in /etc/fail2ban/jail.conf). This action bans an IP address 
using iptables with multiport module.

After enabling jails, you must restart fail2ban to load the jails in fail2ban.
On Fedora or CentOS/RHEL 7:
xterm
T $ sudo systemctl restart fail2ban
On CentOS/RHEL 6:
xterm
T $ sudo service fail2ban restart
Check and Manage fail2ban Banning Status
Once jails are activated, you can monitor current banning status with fail2ban- 
client command-line tool.
To see a list of active jails:
xterm
lb_ $ sudo fail2ban-client status
L-Jlo see the status of a particular jail (including banned IP list):
term
$ sudo fail2ban-client status [name-of-jail]

[dev@centos7 -]$ sudo fail2ban-client status 
Status
|- Number of jail; . 5 ,
Jail list: apache, apache-badbots, apache-nohome, apache-noscript, apache-overflows 
[dev@centos/ -]$ 
[dev@centos7 -]$ 
[dev@centos7 sudo fail2ban-client status apache 
Status for the jail: apache
I- Filter 
Active railzban jails
- Currently failed: 9
• Total failed: 12 
Journal matches:
'• Actions
- Currently banned: 2
- Total banned: 
2 
V
Banned IP list: 192.168.1.190 192.168.1.129
[dev@centos7 -]$
You can also manually ban or unban IP addresses.
To ban an IP address with a particular jail:
xterm
lb_ $ sudo fail2ban-client set [name-of-jail] banip [ipaddress]
I Ito unban an IP address blocked by a particular jail:
xterm
T_ $ sudo fail2ban-client set [name-of-jail] unbanip [ipaddress]
Summary
This tutorial explains how a fail2ban jail works and how to protect an Apache 
HTTP server using built-in Apache jails. Depending on your environments 
and types of web services you need to protect, you may need to adapt existing 
jails, or write custom jails and log filters. Check outfail2ban's official Github 
page for more up-to-date examples of jails and filters.
Are you using fail2ban in any production environment? Share your experience.

Linux Tutorials
How to install and configure Conky 
on Linux
Conky is a light-weight system monitoring tool combined with a 
fullycustomized desktop theme, which can completely change your Linux 
desktop experience. Using Conky, you will get a fully personalized desktop 
theme, populated with an eye-catching smart clock, current date and time, as 
well as the current status of your Linux system such as used/free HDD space, 
RAM and CPU utilization. Conky interface can also keep you updated with 
up/down time of the system and uploaded/downloaded data traffic. The point 
is, Conky is fully configurable and scriptable, being able to collect and 
display almost any type of information directly on an X window.
Conky works pretty much the same way on different Linux distros. This 
tutorial will guide you to install and configure Conky on Linux. While 
Conky was tested in Ubuntu environment in this tutorial, you can configure 
Conky on other distros using the same instructions.
Let's assume that your Linux system is connected to the Internet.
Install Conky on Linux
For Ubuntu, Debian or Linux Mint:
To install conky on Ubuntu, Debian, use apt-get: 
xterm
T__ $ sudo apt-get install conky-all
For Fedora:
To install Conky on Fedora, simply run:
term

Ilk_ $ sudo yum install conky
For CentOS or RHEL:
To install Conky on CentOS or RHEL, first set up EPEL repository, and then 
run:
xterm
Hi_ $ sudo yum install conky
Configure Conky on Linux
Conky's configuration is defined with the Lua programming language. There 
are plenty of Conky themes you can download. In this tutorial, let's download 
and try one Conky example.

view add^rtwort my artwork search content
ARTWORK
All 
Wallpapers 
GTKl.x 
GTKZ.k 
GTK3.k 
Metacity 
GNOME Shell 
Compiz 
Beryl 
Cinnamon Themes 
Icons 
GDM Themes 
Colour Schemes 
Splash Screens 
DeskJets
Screenlets 
XMM5 Themes 
M Player Themes 
Xine Themes 
VLC Themes 
GnoMenu Themes 
Nautilus Pattern 
Backgrounds 
Screenshots 
Fonts
Cliparts 
Systemsounds 
XI1 Mouse Themes 
Topaz Brainstorm 
Cairo-Clock Themes 
Screensavers 
Nautilus Scripts 
Maemo 
Other
Search
J__________________  
ll
Content Fans [w]
Conkylua 2011
Other GNOME Stuff
/'Zoorii
Downloads. 110608 
Submitted' Feb 212011
Updated: May 3 2011
Description:
conkyrc:
change the line: ${font caviar dreams:size=12}${color FFFFFF}${alignr}${weather
http://weather.noaa.gov/pub/data/observation5/metar/stations/ LQBK
LQBK my city, find the code for your city: http://weather.noaa.gov
clock.ringsJua copy to: /home/username/.lua/scripts
linuxmint_main_logo.png, new-ubuntu-logo.png, fedora-logo.png, open-suse-logo.png copy to:
/home/username/.conky
NEW: http://gnome-look.org/content/show,php?content= 141411
Changelog:
26.02.2011:
added Conky Mint-lua v2
Thanks 5N75
24.02.2011:
corrected minor errors
added "conky Debian-lua" 
Thanks Meho R.
corrected minor errors
License:
You can do whatever you want with this, feel free to change it and fool around with the settings.
Click here and 
download
©Send to a friend
g Subscribe
0Other Artwork from despot?/
0Report inappropriate content
goto page: gprev 1 2 3 4 5
Linux42.org
Sony NEXT AVCHD to 
Apple Aperture 3 on OS X 
Mountain Lion
The biggest cone crusher 
supplier?
What is Cancer Horoscope - 
Cancer Astrology 2
Stop being a fan of 
removed content
http://kde-look.org/ 
Artwork upload fail 
Q ask question 
igmore
NEWS
database problems
Peppermint Wallpaper
Challenge
4 weeks and 2 contests
Contest deadline extended
Ijjmore
Popular Groups
Desktop Screenshots
•SPAM
51ackwarQ
I love KDE 3.5x
KILL voting system
FlulDE (New Computer 
Interfaces Discussion)
QEIectroTech
KoolChart

The Conky Lua will be downloaded as a compressed archive, named as 
139024-Conky-iuai.tar.gz. Extract files from the compressed tarball.
xterm
H_ $ tar xvfvz 139024-Conky-luai.tar.gz
$ cd Conky-lua
You will find several compressed folders inside the uncompressed archive, 
each containing Conky configuration files for different Linux distros. 
Currently it contains Conky config files for Debian, Ubuntu, Mint, Fedora, 
and openSUSE.
In this tutorial, we will configure Conky on Ubuntu. Hence, we extract a 
folder called Conky ubuntu-lua.tar.gz.
xterm
T_ $ tar xvfvz Conky ubuntu-lua.tar.gz
$ cd Conky ubuntu-lua
__I^M^^Hln the extracted content, you will find three files, namely: 
clock_rings.lua, conkyrc and new-ubuntu-logo.png.
Now, rename the file conkyrc to .conkyrc, and copy it to your home directory.
xterm
Bi_ $ mv conkyrc .conkyrc
$ cp .conkyrc ~
_J^HNext, create a new folder named .conky in your home directory, and 
opy the other two files (clock_rings.lua and new-ubuntu-logo.png) to ~/.conky folder.
xterm
Hl_ $ mkdir ~/.conky
$ cp clock_rings.lua new-ubuntu-logo.png ~/.conky
__ ^^Bnow you need to edit the .conkyrc file that you put in your home 
directory.
In ~/.conkyrc, find the line: lua_load
/~.lua/scripts/clock_rings.lua and change
/~.lua/scripts/clock_rings.lua to ~/.conky/clock_rings.lua.
xterm
T $ vi ~/.conkyrc

# Lua Load #
lua_load ~/.conky/clock_rings.lua
lua_draw_hook_pre clock_rings
After editing ~/.conkyrc, you are ready to launch Conky.
Go to the terminal and type the following command to launch Conky as a 
daemon.
term
$ conky -d
Now you are done with the installation and configuration of Conky. If you 
follow everything accordingly, then you must have a transparent desktop 
utility theme like this one:

Files
0*  Documents
Devices
8 J! C8 Volume ±
rHome Documents
Qseardi
Bookmarks
flwebsite
Book-2013-03-15
html
manuals
Computer
iHome
c Desktop
fc Documents
fl Downloads 
JjMusic 
l| Pictures 
flvideos 
.File System 
jTrash
Network
flBrowseNetwork
whitepapers
9fcfd5087e51faf3f
papers
slides
*
Uptime: 12h 16m 
Processes: 261 
Running:!
dev.domain
Ubuntu 12.10 x86_64
Kernel: 3.5.0-18-generic

If you want to stop Conky, just go to the terminal and type the following. 
xterm
Hl_ $ killall conky
__ I^BOnce you verify that Conky works correctly, you can set up Conky 
so that it auto-starts when you log in to your desktop.
Linux Tutorials
How to install Ubuntu desktop 
behind a proxy
Question: My computer is connected to a corporate network sitting behind 
an HTTP proxy. When I try to install Ubuntu desktop 14.04 (Trust Tahr) on 
the computer from a CD-ROM drive, the installation hangs and never 
finishes while trying to retrieve files, which is presumably due to the proxy. 
However, the problem is that Ubuntu installer never asks me to configure 
proxy during installation procedure. Then how can I install Ubuntu desktop 
behind a proxy?
Unlike Ubuntu server, installation of Ubuntu desktop is pretty much 
autopilot, not leaving much room for customization, such as custom disk 
partitioning, manual network settings, package selection, etc. While such 
simple, one-shot installation is considered user-friendly, it leaves much to be 
desired for those users looking for "advanced installation mode" to customize 
their Ubuntu desktop installation.
In addition, one big problem of the default installer for Ubuntu desktop, at 
least for 14.04 (Trusty Tahr), is the absense of proxy settings. If your 
computer is connected behind a proxy, you will notice that Ubuntu 
installation gets stuck while preparing to download files.

This post describes how to get around the limitation of Ubuntu installer and 
install Ubuntu desktop when you are behind a proxy.
The basic idea is as follows. Instead of starting with Ubuntu installer directly, 
boot into live Ubuntu desktop first, configure proxy settings, and finally 
launch Ubuntu installer manually from live desktop. The following is the step 
by step procedure.
After booting from Ubuntu desktop CD/DVD or USB, click on "Try Ubuntu" on 
the first welcome screen.

Once you boot into live Ubuntu desktop, click on Settings icon in the left.

Go to Network menu.
Configure proxy settings manually.

System Settings 
t| | <i)'. 7:35 PM 1}
Next, open a terminal.
Enter a root session by typing the following:
xterm
T $ sudo su
U Finally, type the following command as the root.
xterm
# ubiquity gtk_ui
__ ^^B'Fhis will launch GUI-based Ubuntu installer as follows.

Proceed with the rest of installation.

Linux Tutorials

How to copy or transfer multiple 
files to Android devices
ADB (Android Debug Bridge) is a command-line utility that allows you to 
communicate with Android devices from your computer. Among other 
things, you can use ADB to copy file(s) to and from Android devices. As of 
this writing, ADB does not support wildcards to push or transfer multiple 
files (e.g., *.jpg)  to Android devices.
So for example, the following will not work. 
xterm
T $ adb push *.jpg  /sdcard/my_photo
—■■if you would like to copy or transfer multiple files to an Android 
device, use the following trick.
First create a temporary directory, and copy to that directory all files that you 
want to transfer to your Android device. Then using ADB, push that entire 
temporary directory to the Android device. ADB will then push all individual 
files in the directory to the destination location of your Android device. For 
example:
term
$ mkdir tmp_dir
$ cp *jpg  tmp_dir
$ adb push tmp_dir /sdcard/my_photo 
push: tmp_dir/001.jpg -> /sdcard/my_photo/001.jpg
push: tmp_dir/002.jpg -> /sdcard/my_photo/002.jpg
push: tmp_dir/003.jpg -> /sdcard/my_photo/003.jpg
push: tmp_dir/004.jpg -> /sdcard/my_photo/004.jpg

Linux Tutorials
How to cut, split or edit MP3 file on 
Linux
If you are a music enthusiast, you may often want to edit MP3 files for 
various reasons. For example, you may want to trim or chop MP3 files to get 
rid of silence at the beginnings and ends. Or you may want to split a big MP3 
file into smaller segments of certain lengths.
In Linux, there are many free MP3 editor software tools at your disposal. In 
this tutorial, I will explain how to cut, split or edit MP3 files by using a 
Linux tool called Audacity.
Audacity is one of the best known open-source software for recording and 
editing audio data. It is available for Linux, Windows and Mac OS.
Install Audacity on Linux
Audacity is included in the base or third-party repositories on major Linux 
distros. Thus you can easily install it from an appropriate distro-specific 
repository.
For Ubuntu or Debian:

To install Audacity on Ubuntu or Debian, simply run:
xterm
T_ $ sudo apt-get install audacity
For CentOS or RHEL 6 or Later:
To install Audacity on CentOS/RHEL 6 or later, first enable EPEL 
repository, and then run:
xterm
T_ $ sudo yum install audacity
For Fedora:
To install Audacity on Fedora, simply run: 
xterm
T_ $ sudo yum install audacity
Edit an MP3 File with Audacity
Audacity allows you to cut, copy, split, or merge audio data. To edit an MP3 
file, open it with Audacity as follows.
xterm
$ audacity input.mp3
__ ^^BT'he Audacity interface looks like the following.

If you want, zoom in the file view area by pressing Ctrl+1 key.
Then select the region to edit by mouse clicking and dragging left-selection 
boundary.

Once the region to edit is selected and highlighted as above, you can cut, 
copy or paste the selected region by using ctrl+x, ctrl+c, and ctrl+v 
respectively.
After finishing editing, you can export the edited content to a separate MP3 
file. During file export, you can edit the metadata of the exported MP3 file by 
using Audacity's built-in MP3 metadata editor, as shown below.

Note that running two instances of Audacity simultaneously on your system 
may cause data loss or cause your system to crash. To work on multiple MP3 
files, use Open or New menu in the current instance of Audacity instead.
Linux Tutorials

How to convert TIFF files to PDF 
format on Linux
TIFF (Tagged Image File Format) is a popular image file format. Unlike 
other types of image files, a single TIFF file can contain multiple images 
(e.g., multipage document scan) in it. TIFF editor software such as GIMP is 
able to support such multi-page TIFF files. If you would like to convert a 
TIFF file to PDF format on Linux, you can follow the instructions below.
A simple command-line approach to convert a TIFF image to a PDF 
document is to use a command-line utility called tiff2pdf.
Install tiff2pdf on Linux
To install tiff2pdf on Debian or Ubuntu:
xterm
$ sudo apt-get install ghostscript libtiff-tools
I Ito install tiff2pdf on CentOS, RHEL
xterm
lb_ $ sudo yum install ghostscript libtiff
_ ^^B'lo install tiff2pdf on Fedora:
I___^Ixterm
$ sudo yum install ghostscript libtiff-tools
Convert a TIFF Image to PDF Format with tiff2pdf
If you have installed tiff2pdf, you can convert a TIFF image to a PDF file as 
follows.
xterm
lb_ $ tiff2pdf -o output.pdf input.tiff

If you would like to extract specific pages from a TIFF image and convert 
them into a PDF file, what you can do is to convert the whole TIFF file into a 
PDF using the above method, and then edit the PDF file to your needs.
Linux Tutorials
How to set up a media center with
Raspberry Pi
'One of the most popular and useful projects with Raspberry Pi (RPi) board is 
turning RPi into a media center box. All you need to do is to download a 
media center image for RPi, and write it to your SD card. There are plenty of 
media center distributions for Raspberry Pi, such as OpenELEC, RaspBMC, 
GeeXboX, RaspyFi, etc.
In this article, I will discuss how to set up a media center on Raspberry Pi 
using two of the most popular media center images: OpenELEC and 
RaspBMC. Both images use XBMC as built-in media player software.
Set up OpenELEC Media Center on Raspberry Pi
To set up an XBMC media center with OpenELEC on Raspberry Pi, follow 
instructions below.
First, download the latest OpenELEC image for RPi from the official 
OpenELEC site. As of this writing, the latest build is 3.2.4.
Extract the image, and go to the extracted OpenELEC directory.
xterm
lb_ $ tar xvf OpenELEC-RPi.arm-3.2.4.tar
$ cd OpenELEC-RPi.arm-3.2.4
Put your SD card into a card reader.
We need to find out what device name is assigned to the SD card. To do so, 
run this command.

term
T $ dmesg | tail
[ 3999.329472] sd 6:0:0:0: [sdb] Cache data unavailable
[ 3999.329476] sd 6:0:0:0: [sdb] Assuming drive cache: write through [
3999.330486] sd 6:0:0:0: [sdb] Cache data unavailable
[ 3999.330490] sd 6:0:0:0: [sdb] Assuming drive cache: write through [
3999.332054] sdb: sdb1
In this example, the SD card is assigned /dev/sdb. Note that sdb1 is a partition 
name. When you write an image to the SD card, you need to use /dev/sdb, not 
/dev/sdb1.
Make sure that no partition in the SD card is mounted since the card will be 
partitioned/formatted subsequently.
term
T__ $ umount /dev/sdb1
Run the script called create_sdcard with the device name of your SD card (e.g., 
/dev/sdb). The script is found inside the extracted OpenELEC image directory. 
The script will write the OpenELEC image into your SD card (/dev/sdb).
term
T $ sudo ./create_sdcard /dev/sdb
__ ^^BWait for a while until it is finished. Once it's done, run the 

following command, which will flush file system buffers. After that, eject the 
SD card.
xterm
T $ sudo sync
__ ^■Plug the SD card to your RPi, and turn the power on to start 
enjoying your OpenELEC media center. The default username and password 
of OpenELEC is root and openelec.
Set up RaspBMC Media Center on Raspberry Pi
To set up an XBMC media center with RaspBMC on Raspberry Pi, here are 
the steps to follow.
First, download the "standalone" image of RaspBMC from the official 
RaspBMC website, which contains complete RaspBMC packages, and 
automatically expands the RaspBMC image to use the full size of your card 
on bootup.
Extract the downloaded RaspBMC image file.
xterm
lb_ $ gunzip raspbmc-final.img.gz
__ ^^^Insert your SD card into a card reader.
Using the same steps described in OpenELEC case above, find out the device 
name assigned to the inserted SD card (e.g., /dev/sdb).
Make sure that none of the partition(s) (if any) in the SD card is mounted.
xterm
T $ umount /dev/sdb1
LJ\ w go ahead and write the image to the SD card as follows.
xterm
lb_ $ sudo dd bs=4M if=/path/to/raspbmc-final.img of=/dev/sdb
__ ^^^After it's completed, plug the SD card into your Raspberry Pi, and 
start enjoying the RaspBMC system.
Here I show some pictures of RaspBMC system running on my RPi. I use 
Raspberry Pi B model with 1 TB Toshiba ext HDD for movie storage, and 

Acer AL1916W as my main display. For interactions with RaspBMC system, 
I use mini USB keyboard.
First time boot of RaspBMC.


Initial appearance after booting.


External HDD is mounted automatically after booting.

BUT
WJW
2:33 PM
HER PICTURES VIDEOS MUSIC PRO
fid 
AM-OT
jA UMMbiirMnnuailblunUiua 
I ■\ mWJIIICu ICllivraUK lUUQullf C 
\1/TOSHIBA EXT 
|
■ ________________.

The complete appearance of my RaspBMC system.


List of movies stored in an external HDD.

aar

System summary of my RaspBMC system.


Linux Tutorials
How to install RubyGems on Linux
Question: I am trying to install a Ruby application package from RubyGems, 
but I'm getting the following error when attempting to install a Ruby package. 
How can I install RubyGems on [insert your Linux distro]? "bash: gem: 
command not found"
RubyGems is a package manager for building, uploading, downloading and 
installing Ruby packages (also called Gems). When you install a Gem 
package, RubyGems automatically downloads and installs the Gem and all its 
dependencies/libraries.
Here is an instruction on how to install RubyGems on various Linux distros. 
When you install RubyGems, you also want to install Ruby development 
environment as well, which is needed to build Ruby extensions which are 
typically written in C language.
Install RubyGems on Ubuntu, Debian or Linux 
Mint
term
lb_ $ sudo apt-get install rubygems ruby-dev
Install RubyGems on CentOS, Fedora or RHEL
term
lb_ $ sudo yum install rubygems ruby-devel
Install RubyGems on Arch Linux

On Arch Linux, RubyGems is included in the ruby package. So install ruby 
package from AUR. Then install development tools as well:
xterm
T__ $ sudo pacman -S base-devel
Linux Tutorials
How to deduplicate files on Linux 
with dupeGuru
Recently, I was given the task to clean up my father's files and folders. What 
made it difficult was the abnormal amount of duplicate files with incorrect 
names. By keeping a backup on an external drive, simultaneously editing 
multiple versions of the same file, or even changing the directory structure, 
the same file can get copied many times, change names, change locations, 
and just clog disk space. Hunting down every single one of them can become 
a problem of gigantic proportions. Hopefully, there exists nice little software 
that can save your precious hours by finding and removing duplicate files on 
your system: dupeGuru. Written in Python, this file deduplication software 
switched to a GPLv3 license a few hours ago. So time to apply your new 
year's resolutions and clean up your stuff!
Install dupeGuru on Linux
On Ubuntu, you can add the Hardcoded Software PPA:
xterm
Bl_ $ sudo apt-add-repository ppa:hsoft/ppa
$ sudo apt-get update
I lAnd then install with:
xterm
T $ sudo apt-get install dupeguru-se
__ ^^B(')n Arch Linux, the package is present in the AUR. If you prefer 

compiling it yourself, the sources are on GitHub.
Basic Usage of dupeGuru
dupeGuru is conceived to be fast and safe. Which means that the program is 
not going to run berserk on your system. It has a very low risk of deleting 
stuff that you did not intend to delete. However, as we are still talking about 
file deletion, it is always a good idea to stay vigilant and cautious: a good 
backup is always necessary.
Once you took your precautions, you can launch dupeGuru via the command: 
xterm
T $ dupeguru_se
__^^Byoli should be greeted by the folder selection screen, where you can 
add folders to scan for deduplication.
Once you 
selected your directories and launched the scan, dupeGuru will show its

results by grouping duplicate files together in a list.
Note that by default dupeGuru matches files based on their content, and not 
their name. To be sure that you do not accidentally delete something 
important, the match column shows you the accuracy of the matching 
algorithm. From there, you can select the duplicate files that you want to take 
action on, and click on Actions button to see available actions.

The choice of actions is quite extensive. In short, you can delete the 
duplicates, move them to another location, ignore them, open them, rename 
them, or even invoke a custom command on them. If you choose to delete a 
duplicate, you might get as pleasantly surprised as I was by available deletion 
options.

You can not only send the duplicate files to the trash or delete them 
permanently, but you can also choose to leave a link to the original file 
(either using a symlink or a hardlink). In oher words, the duplicates will be 
erased, and a link to the original will be left instead, saving a lot of disk 
space. This can be particularly useful if you imported those files into a 
workspace, or have dependencies based on them.
Another fancy option: you can export the results to a HTML or CSV file. Not 
really sure why you would do that, but I suppose that it can be useful if you 
prefer keeping track of duplicates rather than use any of dupeGuru's actions 
on them.
Finally, last but not least, the preferences menu will make all your dream 
about duplicate busting come true.

There you can select the criterion for the scan, either content based or name 
based, and a threshold for duplicates to control the number of results. It is 
also possible to define the custom command that you can select in the 
actions. Among the myriad of other little options, it is good to notice that by 
default, dupeGuru ignores files less than 10KB.

For more information, I suggest that you go check out the official website, 
which is filled with documention and other goodies.
To conclude, dupeGuru is my go-to software whenever I have to prepare a 
backup or to free some space. I find it powerful enough for advanced users, 
and yet intuitive to use for newcomers. Cherry on the cake: dupeGuru is cross 
platform, which means that you can also use it for your Mac or Windows PC. 
If you have specific needs, and want to clean up music or image files, there 
exists two variations: dupeguru-me and dupeguru-pe, which respectively find 
duplicate audio tracks and pictures. The main difference from the regular 
version is that it compares beyond file formats and takes into account specific 
media meta-data like quality and bit-rate.
What do you think of dupeGuru? Would you consider using it? Or do you 
have any alternative deduplication software to suggest? Let us know in the 
comments.
Linux Tutorials
How to checkout a specific version 
of git repository
A public git repository is often shared by multiple developers who constantly 
contribute new code and fixes, and users who check out code to try it. Along 
with code itself, a git repository contains full commit history of the code, as 
well as revision tracking information. As such, you can easily checkout a 
particular version of specific files from git repository if you want. In order to 
checkout a specific version of git repository, see the following examples.
I assume that you have already cloned a git repository as follows. 
xterm
T__ $ git clone <url of git repository>
$ cd <cloned-directory>
__ ^^^Before checking out a specific version, you probably want to 

examine the change history first.
To view the commit history of the current directory:
term
$ git log
To view the commit history of a specific file:
xterm
T $ git log -p src/import.c
__B^^B'rhe sample commit history looks as follows.
commit 7a51831cb4dc6b2ae56ad24400ba1fdfed064528
Merge: 69f8529 6fd2611
Author: Peter
Date: Wed Nov 14 21:58:47 2012 +0000
The history shows you, for each commit entry, detailed information about the

commit, as well as 40-character checksum hash (e.g., 7a518xxxxxx). The 
checksum hash is essentially a version number you need to remember in 
order to checkout next.
To checkout a specific version of the current directory:
term
__ $ git checkout <checksum_hash>
To checkout a specific version of a file:
term
__ $ git checkout <checksum_hash> <file>
Linux Tutorials
How to run an Octave script from 
command line
GNU Octave is an open-source alternative to MATLAB, a proprietary 
software and programming framework for conducting numerical mathematics 
and data analysis. If you are an Octave user, you are probably familiar with 
its interactive shell environment.
However, if you want to perform Octave-based analytics non-interactively in 
a batch processing environment, you need to be able to run Octave scripts 
from the command line.
Here is a simple Octave script example which you can use. The script 
accepts command line arguments.
xterm
lb_ $ vi hello.m

__ ■^■#!/usr/bin/octave -qf
# example octave script 
arg_list = argv ();
num = str2int(arg_list{1});
printf ("Name of Octave script: ", program_name ());
tic(); for i=1:num 
a(i) = i;
endfor
elapsed = toc();

printf("Elapsed time: %.4f seconds", elapsed);
Don't forget to make the script executable: 
xterm
lb_ $ chmod 755 hello.m
_L~Jal this point you can simply run the script as an executable. 
xterm
T__ $ ./hello.m 10000
Linux Tutorials
How to fix "network 'default' is not 
active" error in libvirt
Question: When I am trying to start a guest VM on QEMU/KVM via 
virtmanager, it fails to start with the error:
"Error starting domain: Requested operation is not valid: network 'default' is 
not active"

How can I fix this error?

When you install QEMU/KVM in Linux, the default NAT network is created 
automatically by libvirtd daemon. The properties of the default network are 
defined in the libvirt's default network template (located at 
/etc/libvirt/qemu/networks/default.xml). This default network is used to interconnect

guest VMs's virtual NICs in NAT mode.
If, for whatever reason, the default network is deactivated, you won't be able to 
start any guest VMs which are configured to use the network.
When you check the state of existing networks, you will see inactive state for
term
$ sudo virsh net-list --all
the default network.
Here we show how to solve the default network stuck in inactive state.
Solution One
Your first attempt can be simply trying to start the network with virsh. 
xterm
lb_ $ sudo virsh net-start default
If you encounter the following error, go to the next solution.
internal error: Network is already in use by interface virbr0
Solution Two
Identify the name of the bridge associated with the default network. You can 
find out the bridge name in the default network template 
(/etc/libvirt/qemu/networks/default.xml). In most cases, the bridge name is virbr0.
Remove that bridge as follows.
term

$ sudo ifconfig virbrO down
$ sudo brctl delbr virbr0
Now start the default network using virsh command.
$ sudo virsh net-start default
This will automatically re-create the virbrQ bridge.
term
virbrO automatically recreated
dan(apoweredqe:~l
Link encap:Ethernet Hkladdr 52:54:O0:a5:19:aa
4|iet addr:192.168.122.1 Beast:192.168.122.255 Mask:255.255.255.0
UP'BRSADCAST MULTICAST MTU:1500 Metric:l
RX packeO..errors:0 dropped:0 overruns:© frame:©
IX packets:0 erlw^O dropped:© overruns:© carrier:©
collisions:© txqueuetM
dangpoweredge>$ ifconfig virbrO
VirbrO
dan<apoweredqe>$|virsh net-start default! 
Setwork default started]
default inactive yes
- Start 'default' network
dan@poweredge:*$ 
dan@poweredge:~$ 
dan@poweredge:~$ 
dan@poweredge:~$ 
dan@poweredge>S 
dan@poweredge:~$ virsh net-list --all
Name 
State Autostart Persistent
Destroy virbrO
sudo ifconfig virbrO down
sudo brctl delbr virbrO
Verify the state of the default network.
B__xterm
■f_ $ sudo virsh net-list --all

Linux Tutorials
How to block traffic by country on
Linux
As a system admin who maintains production Linux servers, there are 
circumstances where you need to selectively block or allow network traffic 
based on geographic locations. For example, you are experiencing denial- 
ofservice attacks mostly originating from IP addresses registered with a 
particular country. In other cases, you want to block SSH logins from 
unknown foreign countries for security reasons. Or your company has a 
distribution right to online videos, which allows it to legally stream to 
particular countries only. Or you need to prevent any local host from 
uploading documents to any non-US remote cloud storage due to 
georestriction company policies.
All these scenarios require an ability to set up a firewall which does 
countrybased traffic filtering. There are a couple of ways to do that. For 
one, you can use TCP wrappers to set up conditional blocking for individual 
applications (e.g., SSH, NFS, httpd). The downside is that the application you 
want to protect must be built with TCP wrappers support. Besides, TCP 
wrappers are not universally available across different platforms (e.g., Arch 
Linux dropped its support). An alternative approach is to set up ipset with 
country-based GeoIP information and apply it to iptables rules. The latter 
approach is more promising as the iptables-based filtering is application­
agnostic and easy to set up.

In this tutorial, I am going to present iptables-based GeoIP filtering which is 
implemented with xtables-addons. For those unfamiliar with it, xtables-addons is a 
suite of extensions for netfilter/iptables.
Included in xtables-addons is a module called xt_geoip which extends the 
netfilter/iptables to filter, NAT or mangle packets based on source/destination 
countries. For you to use xt_geoip, you don't need to recompile the kernel or 
iptables, but only need to build xtables-addons as modules, using the current kernel 
build environment
(/lib/moduies/'uname -r'/build). Reboot is not required either. As soon as you build 
and install xtables-addons, xt_geoip is immediately usable with iptables.
As for the comparison between xt_geoip and ipset, the official source of xtables- 
addons mentions that xt_geoip is superior to ipset in terms of
memory footprint. But in terms of matching speed, hash-based ipset might 
have an edge.
In the rest of the tutorial, I am going to show how to use iptables/xt_geoip to 
block network traffic based on its 
source/destination countries.
Install xtables-addons on Linux
Here is how you can compile and install xtables-addons on various Linux 
platforms.
To build xtables-addons, you need to install a couple of dependent packages first.
Install Dependencies on Ubuntu, Debian or Linux Mint
xterm
T__ $ sudo apt-get install iptables-dev xtables-addonscommon libtext-csv-
xs-perl pkg-config
Install Dependencies on CentOS, RHEL or Fedora

CentOS/RHEL 6 requires EPEL repository be set up first (for perl-textcsv_xs). 
Then run:
xterm
Hl_$ sudo yum install gcc-c++ make automake kernel-devel'uname -r'
wget unzip iptables-devel perl-Text-CSV_XS__ L-J
Compile and Install xtables-addons
Download the latest xtables-addons source code from the official site, and 
build/install it as follows.
term
$ wget https://inai.de/files/xtables-addons/xtablesaddons-2.10.tar.xz
$ tar xf xtables-addons-2.10.tar.xz
$ cd xtables-addons-2.10
$ ./configure
$ make
$ sudo make install
Note that for Red Hat based systems (CentOS, RHEL, Fedora) which have 
SELinux enabled by default, it is necessary to adjust SELinux policy as 
follows. Otherwise, SELinux will prevent iptables from loading xt_geoip module.
term
$ sudo chcon -vR --user=system_u /lib/modules/$(uname r)/extra/
*.ko
 
$ sudo chcon -vR --type=lib_t /lib64/xtables/
*.so
Install GeoIP Database for Xtables-addons
The next step is to install GeoIP database which will be used by xt_geoip for

IP-to-country mapping. Conveniently, the xtables-addons source package comes 
with two helper scripts for downloading GeoIP database from MaxMind and 
converting it into a binary form recognized by xt_geoip. These scripts are found 
in geoip folder inside the source package. Follow the instructions below to 
build and install GeoIP database on your system.
term
$ cd geoip
$ ./xt_geoip_dl
$ ./xt_geoip_build GeoIPCountryWhois.csv
$ sudo mkdir -p /usr/share/xt_geoip
$ sudo cp -r {BE,LE} /usr/share/xt_geoip
According to MaxMind, their GeoIP database is 99.8% accurate on a 
countrylevel, and the database is updated every month. To keep the locally 
installed GeoIP database up-to-date, you want to set up a monthly cron job to 
refresh the local GeoIP database as often.
Block Network Traffic Originating from or 
Destined to a Country
Once xt_geoip module and GeoIP database are installed, you can immediately 
use the geoip match options in iptabies command.
xterm
T__ $ sudo iptables -m geoip --src-cc country[,country...]
--dst-cc country[,country...]
Countries you want to block are specified using two-letter ISO3166 code 
(e.g., us (United States), cn (China), in (India), fr (France)).
For example, if you want to block incoming traffic from Yemen (ye) and 
Zambia (zm), the following iptables command will do.

xterm
Hi_ $ sudo iptables -I INPUT -m geoip --src-cc YE,ZM -j DROP
__ ^Hlf you want to block outgoing traffic destined to China (cn), run the 
following command.
xterm
H__ $ sudo iptables -A OUTPUT -m geoip --dst-cc CN -j DROP
__ ^^Blhe matching condition can also be negated by prepending ! to --src- 
cc or --dst-cc. For example:
If you want to block all incoming non-US traffic on your server, run this:
xterm
T $ sudo iptables -I INPUT -m geoip ! --src-cc US -j DROP__L—.
dev@devhost:-$ 
'dev@devhost:~$
idev@devhost:~$[sudo iptables -I INPUT -m geoip -src-cc YF,ZM -j DROP
dev@devhost:~$ sudo iptables -I INPUT -m geoip --dst-cc CN -j DROP
dev@devhost:~$
ldev@devhost:~$
devtMevhost :-$ sudo iptables L Country-specific traffic blocking rules
Chain INPUT (policy ACCEPT)
target 
prot opt source 
destination
DROP 
all - anywhere 
anywhere 
-m geoip -destination-country CN
DROP 
all - anywhere 
anywhere 
-m geoip -source-country YE,ZM
Chain FORWARD (policy ACCEPT)
target 
prot opt source 
destination
Chain OUTPUT (policy ACCEPT)
target 
prot opt source 
destination
dev@devhost:~$
dev@devhost:~$

For Firewall-cmd Users
Some distros such as CentOS/RHEL 7 or Fedora have replaced iptables with 
firewalld as the default firewall service. On such systems, you can use firewall- 
cmd to block traffic using xt_geoip similarly. The above three examples can be 
rewritten with firewall-cmd as follows.
term
$ sudo firewall-cmd --direct --add-rule ipv4 filter INPUT 0 -m geoip --src-cc 
YE,ZM -j DROP
$ sudo firewall-cmd --direct --add-rule ipv4 filter OUTPUT 0 -m geoip --dst- 
cc CN -j DROP
$ sudo firewall-cmd --direct --add-rule ipv4 filter INPUT 0 -m geoip ! --src- 
cc US -j DROP
Conclusion
In this tutorial, I presented iptables/xt_geoip which is an easy way to filter 
network packets based on their source/destination countries. This can be a 
useful arsenal to deploy in your firewall system if needed. As a final word of 
caution, I should mention that GeoIP-based traffic filtering is not a foolproof 
way to ban certain countries on your server. GeoIP database is by nature 
inaccurate/incomplete, and source/destination geography can easily be 
spoofed using VPN, Tor or any compromised relay hosts. Geography-based 
filtering can even block legitimate traffic that should not be banned. 
Understand this limitation before you decide to deploy it in your production 
environment.
Linux Tutorials
How to count lines of source code in

Linux
For various reasons you may want to know in how many lines of code given 
open-source software is implemented. For example, you want to estimate the 
effort devoted to developing a particular open-source program. Or you want 
to gauge the size and complexity of a program before trying it. There is some 
controversy as to using source lines of code (SLOC) as a metric to determine 
the size of a software program, since existing programming languages differ 
greatly in terms of clarify and brevity.
In any rate, if you would like to count the number of source code lines 
quickly and accurately, you can use a command-line tool called cloc (short for 
"Count Lines Of Code"). cloc is a Perl program that is dedicated to counting 
the number of lines of code. To estimate the size of codebase accurately,
cloc automatically detects different types of programming/scripting languages, 
and discounts comment lines and blank lines based on the type appropriately.
Install cloc on Linux
For Ubuntu, Debian or Linux Mint:
xterm
T $ sudo apt install cloc
For CentOS or RHEL:
To install cloc on CentOS/RHEL, first enable EPEL repository and then run: 
xterm
T__ $ sudo yum install cloc
For Fedora:

cioc is available in the base repository, so simply run: 
xterm
T_ $ sudo dnf install cloc
Check the Number of Lines of Code with cloc
The basic usage of cloc is as follows.
xterm
T_ $ cloc .
cloc will then look for source code files in the current directory and all its sub­
directories recursively, detects the type of language used in each file, and 
counts the number of lines of code. As shown below, the final summary 
shows the breakdown of number of lines of code for different programming 
languages.

3065 text files.
1430 unique files.
2649 files ignored.
$|cloc .|
Analyze the number of lines 
of code in current directory
github.com/AlDanial/cloc v 1.74 T=9.65 s (44.7 files/s. 11540.6 lines/s)
Language
files
blank
comment
code
css
15
5023
2224
23312
Visual Basic
3
1475
0
13890
Python
128
3022
2199
12116
TeX
50
1442
1505
10436
C
88
2330
4881
9194
HTML
15
1561
227
5847
'C/C++ Header
16
344
835
1786
Perl
12
217
329
1327
.Bourne Shell
36
242
772
1143
Markdown
8
252
25
563
,YAML
2
121
12
526
CMake
7
105
253
432
Dockerfile
8
39
37
277
JavaScript
10
48
98
172
PHP
7
65
11
152
XML
2
18
115
127
Bourne Again Shell
1
20
42
87
make
4
18
0
51
JSON
18
0
0
18
SQL
1
2
0
3
SUM:
431
16344
13565
81459
If you want to count the total number of lines of code in a particular set of 
files (e.g., *.py ), you can run the following command.
term
$ find . -name "*.py"  | xargs cloc

$ find . -name "*.py"  | xargs cloc 
146 text files.
128 unique files.
18 files ignored.
github.con/AlDanial/cloc v 1.74 T=0.10 s (1324.0 files/s, 179329.0 lines/s)
Language
files
blank
connent
code
Python
128
3022
2199
12116
SUM:
128
3022
2199
12116
In the above example, we consider only Python source code and count the 
number of lines of Python code.
Linux Tutorials
What are the alternatives to Skype 
on Linux
It is pretty much acknowledged by now that Skype is evil. Maybe not as evil 
as a DRM on a brand new game, but very close. To summarize the events, 
Skype has been bought by Microsoft, has been spied on by the NSA, is now 
quitting its peer-to-peer protocol for a centralized system, and on top of that, 
is proprietary software. The worst of it is that just like a DRM on a game, we 
put up with all of this for the product. It is true that Skype at first did help 
users go into the VoIP realm. Its interface is intuitive, and its setup is simple. 
However, it is time to move on. For this, here is a list of six software to 
replace Skype with on Linux.

1. Blink
Presented as a "state-of-the-art, easy to use SIP client," Blink is distributed 
under GPL license with a free version for Linux and Windows, and an 
optional pro version for Mac OS. All the basic features are present: instant 

messaging, video calls, file transfer, conferences, and screen sharing. It's 
written in Python and Cocoa, but most importantly, I love the URL of the 
official website.
2. Ekiga

Formerly known as GnomeMeeting, Ekiga comes with a lot of advanced 
features. Beyond the basic video call and chat, it comes packaged with a 
bunch of audio and video codecs, and integrates well with Gnome. But what I 

like the most is the interface. Simple yet beautiful, it really does the trick to 
replace Skype's sexiness.
3. Empathy

Probably one of the most famous of the lot, Empathy comes by default with a 
lot of GNOME environments. And for good reasons, it integrates well with 
the desktop and does a bunch of things. Beyond SIP VoIP, it also allows you 

to use your favorite protocols, including but not limited to Facebook IM and 
Google Talk (because if you are leaving Skype for spying on you, you can go 
straight to Facebook and Google instead). And as its most uncommon 
features, it includes geolocation of contacts and desktop remote controlling.
4. Jitsi

Also famous, Jitsi is a multiplatform SIP VoIP software written in Java. What 
differentiates it from the others is a particularly trendy interface and a focus 
on encryption and privacy. If you are leaving Skype because of your lack of 
trust, Jitsi is probably the way to go: it proposes both messaging and voice 

encryption using OTR and ZRTP protocols. Besides that, you will find the 
same conference/call recording/instant messaging features as elsewhere.
5. Linphone
Linphone does not have an interface that convinces me that much, as it tries 
to show everything at once, but remains one of the pillars in terms of VoIP 
software. Besides running on every possible computer OS, it also presents 
itself as an application for Blackberry, Android, iOS, and Windows Phone. 
The software is sober but efficient in terms of features, and includes TLS 
encryption. Hence, it looks like a good alternative for the Skype mobile app.
6. Pidgin

Finally one of my favorite messaging software, Pidgin, includes a plugin to 
support SIP protocol. Called SIPE, it transforms the application that you love 
to support the protocol that you want. If you already have your habits with 
Pidgin (like a billion plugins installed for it, personalized emoticons, and a 
sick profile picture), you will be delighted to just add another account to the 
software. In the new account window, select the Office Communicator option.
Bonus

As a bonus, let me introduce Tox. This recently launched software does not 
include voice and video calls yet, but has the ambition to be the officious, 
completely secure, clone of Skype. With its design, entirely decentralized, 
and encrypted from end to end, Tox reminds me a lot of TorChat, but with a 
lot more ambitions. So this is software to support and keep an eye on.
To conclude, Skype is strong, Skype is big, but it isn't the only one out there. 
If you want privacy, you can always find it. But just like the best intentions in 
the world, you will need your contacts and your friends to follow you in order 
for all of this to make sense. As Utopian as it sounds, I hope that one day we 
can all move on.

Linux Tutorials
How to check glibc version on Linux
Question: I need to find out the version of the GNU C library (glibc) that I 
have on my Linux system. How can I check glibc version on Linux?
The GNU C library ( glibc) is the GNU implementation of the standard C 
library. glibc is a critical component of the GNU toolchain, which is used 
along with binutils and compiler to generate userspace application binaries 
for a target architecture.
When built from source, some Linux programs may be required to link 
against a particular version of glibc. In that case, you may want to check out 
the information about installed glibc to see if dependencies are met.
Here are simple ways to check glibc version on Linux.
Method One
A simple command-line to check the version of the GNU C library is as 
follows.
xterm
lb_ $ ldd --version
alice@caillou:~$ ldd --version
ldd (Ubuntu EGLIBC 2.19-0ubuntu6) 2.19
Copyright (C) 2G14 Free Software Foundation, Inc.
This is free software; see the source for copying conditions. There is NO 
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. 
Written by Roland McGrath and Ulrich Drepper. 
alice@caillou:~$
In this example, the version of glibc is 2.19.

Method Two
Another method is to "type" the glibc library (i.e., libc.so.6) from the command 
line as if it were a command.
The output will show more detailed information about glibc library, including 
the version of glibc and the GNU compiler used, as well as available glibc 
extensions. The location of glibc varies depending on distros and processor 
architectures.
On 64-bit Debian based system:
xterm
M__$ /lib/x86_64-linux- gnu/libc .so.6
_L~Jon 32-bit Debian based system:
xterm
H__$ /lib/i386-linux- gnu/libc .so.6
_L™J(')n 64-bit Red Hat based system:
xterm
H__ $ /lib64/libc.so.6
kJ()n 32-bit Red Hat based systems:
xterm
lb_ $ /lib/libc.so.6
__ ^^Hhere is the sample output of typing glibc library.

alice@caillou:~$ /lib/x86 64-Unux-qnu/libc.so.6
GNU C Library (Ubuntu EGLIBC 2.19-0ubuntu6) stable release version 2.19, by Roland McGrath et al.
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.
There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE.
Compiled by GNU CC version 4.8.2.
Compiled on a Linux 3.13.9 system on 2014-04-12.
Available extensions:
crypt add-on version 2.1 by Michael Glad and others
GNU Libidn by Simon Josefsson
Native POSIX Threads Library by Ulrich Drepper et al
BIND-8.2.3-T5B
libc ABIs: UNIQUE IFUNC
For bug reporting instructions, please see:
<https://bug s.launchpad.net/ubuntu/+source/eglibc/+bug s>.
alice@caillou:~$
Linux Tutorials
How to limit network bandwidth on
Linux
If you often run multiple networking applications on your Linux desktop, or 
share bandwidth among multiple computers at home, you will want to have a 
better control over bandwidth usage. Otherwise, when you are downloading a 
big file with a downloader, your interactive SSH session may become 
sluggish to the point where it's unusable. Or when you sync a big folder over 
Dropbox, your roommate may complain that video streaming at her computer 
gets choppy.
In this tutorial, I am going to describe two different ways to rate limit 
network traffic on Linux.

Rate Limit a Particular Application on Linux
One way to rate limit network traffic is to restrict the bandwidth consumed by 
a particular network application. This can be achieved via a command-line 
tool called trickle. The trickle command allows you to shape the traffic of a 
given program by pre-loading a rate-limited socket library at run-time. A nice 
thing about trickle is that it runs purely in user-space, meaning you don't need 
root privilege to restrict the bandwidth usage of a program. To be compatible 
with trickle, the program must use socket interface with no statically linked 
library. trickle can be handy when you want to rate limit a program which does 
not have a built-in bandwidth control functionality.
To install trickle on Ubuntu, Debian and their derivatives: 
xterm
xterm
$ sudo apt-get install trickle
__ ^^B'lo install trickle on Fedora or CentOS/RHEL (with EPEL 
repository):
xterm
Bb_ $ sudo yum install trickle
__ l^lBasic usage of trickle is as follows. Simply put, you prepend trickle
(with rate) in front of the command you are trying to run.
xterm
Bl_ $ trickle -d <download-rate> -u <upload-rate> <command>
__ ^^BT'his will limit the download and upload rate of <command> to 
specified values (in KBytes/s).
For example, set the maximum upload bandwidth of your scp session to 100
KB/s:
xterm
Bl_ $ trickle -u 100 scp backup.tgz
[email protected]_host.com:
If you want, you can set the maximum download speed (e.g., 300 kb/s) of your 
Firefox browser by creating a custom launcher with the following command.

trickle -d 300 firefox %u
Finally, trickle can run in a daemon mode, where it can restrict the aggregate 
bandwidth usage of all running programs launched via trickle. To launch trickle 
as a daemon (i.e., trickled):
$ sudo trickled -d 1000
Once the trickled daemon is running in the background, you can launch other 
programs via trickle. If you launch one program with trickle, its maximum 
download rate is 1000 kb/s. If you launch another program with
trickle, each of them will be rate limited to 500 kb/s, etc.
Rate Limit a Network Interface on Linux
Another way to control your bandwidth resource is to enforce bandwidth 
limit on a per-interface basis. This is useful when you are sharing your 
upstream Internet connection with someone else. Like anything else, Linux 
has a tool for you. A command-line tool named wondershaper exactly does that: 
rate-limit a network interface.
wondershaper is in fact a shell script which uses tc to define traffic shaping and 
QoS for a specific network interface. Outgoing traffic is shaped by being 
placed in queues with different priorities, while incoming traffic is rate­
limited by packet dropping.
In fact, the stated goal of wondershaper is much more than just adding bandwidth 
cap to an interface. wondershaper tries to maintain low latency for interactive 

sessions such as SSH while bulk download or upload is going on. Also, it 
makes sure that bulk upload (e.g., Dropbox sync) does not suffocate 
download, and vice versa.
To install wondershaper on Ubuntu, Debian and their derivatives:
xterm
T $ sudo apt-get install wondershaper
_J^®To install wondershaper on Fedora or CentOS/RHEL (with EPEL 
repository):
I_____ ^^Ixterm
$ sudo yum install wondershaper
I [Basic usage of wondershaper is as follows.
xterm
lb_ $ sudo wondershaper <interface> <download-rate> <upload-rate>
__ ^^Bl'or example, to set the maximum download/upload bandwidth for 
eth0 to ioooKbit/s and 500Kbit/s, respectively:
xterm
M__ $ sudo wondershaper ethO 1000 500
I Byoh can remove the rate limit by running:
xterm
T $ sudo wondershaper clear eth0
__ ^Hlf you are interested in how wondershaper works, you can read its shell 
script (located in /sbin/wondershaper).
Conclusion
ln this tutorial, l introduced two different ways to control your bandwidth 
usages on Linux desktop, on per-application or per-interface basis. Both tools 
are extremely user-friendly, offering you a quick and easy way to shape 
otherwise unconstrained traffic. For those of you who want to know more 
about rate control on Linux, refer to the Linux bible.
Linux Tutorials

How to create a site-to-site IPsec 
VPN tunnel using Openswan in 
Linux
A virtual private network (VPN) tunnel is used to securely interconnect two 
physically separate networks through a tunnel over the Internet. Tunneling is 
needed when the separate networks are private LAN subnets with globally 
non-routable private IP addresses, which are not reachable to each other via 
traditional routing over the Internet. For example, VPN tunnels are often 
deployed to connect different NATed branch office networks belonging to 
the same institution.
Sometimes VPN tunneling may be used simply for its security benefit as 
well. Service providers or private companies may design their networks in 
such a way that vital servers (e.g., database, VoIP, banking servers) are 
placed in a subnet that is accessible to trusted personnel through a VPN 
tunnel only. When a secure VPN tunnel is required, IPsec is often a preferred 
choice because an IPsec VPN tunnel is secured with multiple layers of 
security.
This tutorial will show how we can easily create a site-to-site VPN tunnel 
using Openswan in Linux.
Test Topologies
This tutorial will focus on the following topologies for creating an IPsec 
tunnel.

Site-A 
Site-A 
Internet 
Site-B Site-8
Private Subnet VPN Server 
VPN Server Private Subnet
Topology 1-Typical Site to Site VPN
Site-A 
Internet 
Site-B Site-B
VPN Server 
VPN Server Private Subnet
Topology 2 - VPN without NAT (Direct Routing)

Installing Packages and Preparing VPN Servers
Usually, you will be managing site-A only, but based on the requirements, you 
could be managing both site-A and site-B. We start the process by installing 
Openswan.
On Red Hat based Systems (CentOS, Fedora or RHEL):
term
__# yum install openswan Isof
On Debian based Systems (Debian, Ubuntu or Linux Mint):
# apt-get install openswan
commands:
term
Now we disable VPN redirects, if any, in the server using these

# for vpn in /proc/sys/net/ipv4/conf/
;*
# do echo 0 > $vpn/accept_redirects;
# echo 0 > $vpn/send_redirects;
# done
Next, we modify the kernel parameters to allow IP forwarding and
disable redirects permanently.
term
# vim /etc/sysctl.conf 
net.ipv4.ip_forward = 1
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.all.send_redirects = 0
Reload /etc/sysctl.conf:
term
T # sysctl -p
__ I^Bwe allow necessary ports in the firewall. Please make sure that the 
rules are not conflicting with existing firewall rules.
# iptables -A INPUT -p udp --dport 500 -j ACCEPT # iptables -A INPUT -p 
tcp --dport 4500 -j ACCEPT # iptables -A INPUT -p udp --dport 4500 -j 
ACCEPT
inally, we create firewall rules for NAT.

term
# iptables -t nat -A POSTROUTING -s site-Aprivate-subnet -d site-B- 
private-subnet -j SNAT -to site-A-Public-IP
Please make sure that the firewall rules are persistent.
Note:
You could use masquerade instead of snat. Logically it should work, but it 
caused me to have issues with virtual private servers (VPS) in the past. So I 
would use snat if I were you.
If you are managing site-B as well, create similar rules in site-B server.
Direct routing does not need snat.
Preparing Configuration Files
The first configuration file that we will work with is ipsec.conf. Regardless of 
which server you are configuring, always consider your site as left and 
remote site as right. The following configuration is done in siteA's VPN 
server.
xterm
# vim /etc/ipsec.conf
__^B## general configuration parameters ##
config setup 
plutodebug=all 

plutostderrlog=/var/log/pluto.log
protostack=netkey
nat_traversal=yes
virtual_private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12 ## 
disable opportunistic encryption in Red Hat ##
oe=off
# # disable opportunistic encryption in Debian ## ## Note: this is a separate 
declaration statement ## include /etc/ipsec.d/examples/no_oe.conf
# # connection definition in Red Hat ##
conn demo-connection-redhat
authby=secret
auto=start
ike=3des-md5
# # phase 1 ##
keyexchange=ike
# # phase 2 ##
phase2=esp
phase2alg=3des-md5
compress=no
pfs=yes
type=tunnel
left=<siteA-public-IP>
leftsourceip=<siteA-public-IP>
leftsubnet=<siteA-private-subnet>/netmask
# # for direct routing ##
leftsubnet=<siteA-public-IP>/32
leftnexthop=%defaultroute
right=<siteB-public-IP>


rightsubnet=<siteB-private-subnet>/netmask
# # connection definition in Debian ## 
conn demo-connection-debian 
authby=secret
auto=start
# # phase 1 ## 
keyexchange=ike
# # phase 2 ##
esp=3des-md5
pfs=yes
type=tunnel
left=<siteA-public-IP>
leftsourceip=<siteA-public-IP>
leftsubnet=<siteA-private-subnet>/netmask
# # for direct routing ##
leftsubnet=<siteA-public-IP>/32 
leftnexthop=%defaultroute 
right=<siteB-public-IP>
rightsubnet=<siteB-private-subnet>/netmask


Authentication can be done in several different ways. This tutorial will cover 
the use of pre-shared key, which is added to the file /etc/ipsec.secrets.
term
T # vim /etc/ipsec.secrets 
siteA-public-IP siteB-public-IP: PSK "pre-shared-key"
## in case of multiple sites ##
siteA-public-IP siteC-public-IP: PSK "corresponding-pre-shared-key"
Starting the Service and Troubleshooting
The server should now be ready to create a site-to-site VPN tunnel. If you are 
managing site-B as well, please make sure that you have configured the site-B's 
server with necessary parameters. For Red Hat based systems,
please make sure that you add the service into startup using chkconfig 
command.
term
# /etc/init.d/ipsec restart
If there are no errors in both end servers, the tunnel should be up now. Taking 
the following into consideration, you can test the tunnel with ping command.
1. The site-B's private subnet should not be reachable from site-A. I.e., ping 
should not work if the tunnel is not up.
2. After the tunnel is up, try ping to site-B's private subnet from siteA. This 
should work.
Also, the routes to the destination's private subnet should appear in the

server's routing table.
term
T # ip route
_J^H[site-B's private-subnet] via [siteA-gateway] dev eth0 src [siteA- 
public-IP] default via [site-A's gateway] dev eth0
Additionally, we can check the status of the tunnel using the following useful 
commands.
xterm
T # service ipsec status
IPsec running - pluto pid: 20754 
pluto pid 20754
1 tunnels up
some eroutes exist
term
lb_ # ipsec auto --status
# # output truncated ##
000 "demo-connection-debian": myip=<siteA-public-IP>; hisip=unset;

000 "demo-connection-debian": ike_life: 3600s; ipsec_life: 28800s;
rekey_margin: 540s; rekey_fuzz: 100%; keyingtries: 0; nat_keepalive: yes
000 "demo-connection-debian": policy:
PSK+ENCRYPT+TUNNEL+PFS+UP+IKEv2ALLOW+SAREFTRACK+lK 
+rKOD; prio: 32,28; interface: eth0;
# # output truncated ##
000 #184: "demo-connection-debian":500 STATE_QUICK_R2 (IPsec SA 
established); EVENT_SA_REPLACE in 1653s; newest IPSEC; eroute 
owner; isakmp#183; idle; import:not set
# # output truncated ##
000 #183: "demo-connection-debian":500 STATE_MAIN_I4 (ISAKMP SA 
established); EVENT_SA_REPLACE in 1093s; newest ISAKMP;
lastdpd=-1s(seq in:0 out:0); idle; import:not set

The log file /var/log/pluto.log should also contain useful information regarding 
authentication, key exchanges and information on different phases of the 
tunnel. If your tunnel doesn't come up, you could check there as well. If you 
are sure that all the configuration is correct, and if your tunnel is still not 
coming up, you should check the following things.

1. Many ISPs filter IPsec ports. Make sure that udp/500, tcp/udp/4500 ports are 
allowed by your ISP. You could try connecting to your server IPsec ports 
from a remote location by telnet.
2. Make sure that necessary ports are allowed in the firewall of the servers.
3. Make sure that the pre-shared keys are identical in both end servers.
4. The left and right parameters should be properly configured on both end 
servers.
5. If you are facing problems with NAT, try using snat instead of
masquerading.
To sum up, this tutorial focused on the procedure of creating a site-to-site 
IPSec VPN tunnel in Linux using Openswan. VPN tunnels are very useful in 
enhancing security as they allow admins to make critical resources available 
only through the tunnels. Also VPN tunnels ensure that the data in transit is 
secured from eavesdropping or interception.
Hope this helps. Let me know what you think.
Linux Tutorials
How to fix "ImportError: No 
module named wxversion" on Linux
Question: I was trying to run a Python application on [insert your Linux 
distro], but I got the following error:
ImportError: No module named wxversion.
How can I solve this error in the Python program?
Looking for python... 2.7.9 - Traceback (most recent call last): File 
"/home/dev/playonlinux/python/check_python.py", line 1, in import os, 

wxversion
ImportError: No module named wxversion 
failed tests
This error indicates that your Python application is GUI-based, relying on a 
missing Python module called wxPython. wxPython is a Python extension 
module for the wxWidgets GUI library, popularly used by C++ programmers 
to design GUI applications. The wxPython extension allows Python 
developers to easily design and integrate GUI within any Python application.
To solve this import error, you need to install wxPython on your Linux, as 
described below.
Install wxPython on Debian, Ubuntu or Linux Mint
xterm
$ sudo apt-get install python-wxgtk2.8
Install wxPython on Fedora
term
lb_ $ sudo yum install wxPython
Install wxPython on CentOS or RHEL 

wxPython is available on the EPEL repository of CentOS/RHEL, not on base 
repositories. Thus, first enable EPEL repository on your system, and then use 
yum command.
xterm
HI_ $ sudo yum install wxPython
Install wxPython on Arch Linux
term
$ sudo pacman -S wxpython
Install wxPython on Gentoo
term
$ emerge wxPython
Linux Tutorials
What is a good text editor on Linux
Want to launch a heated debate among Linux aficionados? Ask them what is 
their favorite text editor. In the open source community, choosing a software 
to write text with, and potentially to write code with, is such an important 
decision that choosing a football team or a game console is less dangerous. 
But anyone new to Linux should not be apprehensive of the plethora of 
advice and the variety of trolls on the topic, and instead try to get familiar 
with a bunch of different text editors in the first place. So today I shall 
propose you a non-exhaustive thematic list of what you can find to write text 
with on Linux. This list will (try to) exclude the full-fledge IDEs, which are 
only made for programming, and the editors made specifically for LaTex.

1. Vim & Emacs
Let's go straight to the big guys. When someone asks about text editors on 
Linux in a room, one person will immediately respond Vim and another will 
say Emacs. And there are very good reasons for that. They are both very 
powerful editors, with a lot of features, a lot of plugins, and a very strong 
community ready to support anyone. If you are not familiar with them at all, 
it would be a bit hard to describe the extent of their capabilities. But put 
simply, they allow you to move very quickly within the text, make huge edits 
very simply, record macros, and do basically any crazy things you might 
want to do with text. The downside to both of them is that the learning 
process to control what you are doing is unavoidable and takes time. Past this 
point, I will not sink into the debate of declaring one better than another, but 
really recommend that everyone learn at least one of the two.

2. Sublime Text & Lime & Atom

Another text editor which has been on the rise for these past years is Sublime 
Text. Some may see it as a friendlier version of Vim or Emacs, clearly 
designed for programming. Indeed, it retains some similarities like the batch 
edition and the goto function which will remind some of Emacs or an 
energized Vim. However, it remains more visual and accessible. Also, the 
large number of plugins will seduce those inclined to customization.
The only gray point to Sublime Text is its license: if you only use open­
source, go away. Hopefully, to deal with that, an ambitious clone called Lime 
appeared recently. It is still under heavy development, but the spirit is here: a 
similar experience to Sublime Text, with the after taste of open source. 
Nothing more to say except admiring the effort.
Even more recently, Atom, the official competition to Sublime Text coming 
from GitHub was released also as open source. Wanting to provide a 
fullfeatured out of the box editor, Atom comes packaged with all you need to 
jump around files, use code snippets, etc. However, its strength remains in its 

easy customization process based on HTML and CSS, and its Node.js 
integration. This is as far as we will go in this list before actually reaching the 
definition of an IDE.
3. Gedit & Kate & Mousepad & Leafpad

// initializes the instance variables: number and valid
// NOTE: This class assumes that the String n
// is formatted correctly.
public PhoneNumber(String n){
numbe r=n;
valid=t rue;
Ouvrir
R
PhoneNumber.java
-/workspace 
“
0
X
// This 
// area 
private
method checks to see if the area code is a Manhattan 
code
void checkl(){
int a reaCode=Int ege r.pa rseInt(numbe r.subst ring(0,3));
if (I(areaCode==212 || areaCode==917 || areaCode==646)) 
valid=false;
// This 
// is a 
private
method checks to see if the first number of the prefix 
valid number (2-9)
void check2(){
int first=lnteger.parselnt(number.substrings,5)) ;
if (first==0 || first==l)
valid=false;
public method calls the two support methods to check
// this
// to see if the number is valid 
public void check(){
checklO
check2();
1// This is an accessor method for the IV valid 
public boolean isValid(){
return valid;
java v Largeur des tabulations : 8 sz
Lig 8. Col 46
INS
}
}
If we leave the realm of the super-powered editors, we can turn to what I see 
as the "desktop environment classics." These editors are more traditional in 
the sense that some can be enhanced with plugins, but that their focus is 
simply to write text. If you have an idea in mind and want to jolt down 
something before you forget about it (I blame video games for my short 
attentions span), you are not going to learn or even need the shortcuts in Vim 
or Sublime Text. You just want to some blank space. The good thing about 
these editors is that they are more or less well integrated with your desktop 
environment. In this category, Gedit and Kate, for Gnome and KDE 
respectively, mirror their environment, and are customizable via plugins, 
which makes it easier to write LaTeX, for example. Mousepad and Leafpad 
are more appropriate for lightweight desktops like Xfce and LXDE. They are 
in a way close to Windows' notepad. So if you are looking for flexibility and 
accessibility, pick here.

4. Nano & Qute

Another popular "family" of text editors is the distraction-free editors. If you 
like to have Facebook or Twitter opened constantly in the background, or 
receive an email every five minutes, you know how difficult it can be to 
focus on that essay due tomorrow, or this code not compiling. In this case, 
you need an editor that will take the whole space on the screen, and block 
everything else.
Probably the least favorite of this kind is Nano. If you want to block any 
distraction, go without X server. It is simple and straightforward. In fact, 
George R. R. Martin, the author of Game of Thrones, recently revealed in an 
interview that he uses a similar DOS Word processor to write his novels.
If you prefer something a bit more keen on the eye, you could like my 
personal favorite: Qute. No fancy features, maybe a bit of LaTeX to look 
nice, but the weight is really on the interface. It proposes a beautiful 
navigation and edition experience. If you are uneasy because of the terminal, 
Qute is a good alternative.

5. LibreOffice & Calligra & Abiword

Finally, an office suite is also a text editor. I doubt you can code easily on 
that, but it will certainly be more appropriate for plain text and more quick to 
pick up than LaTeX. In this category, it would be impossible to avoid 
LibreOffice and Calligra, the two behemoth in term of featues and fame. I 
like both of them, but a lot of people have a clear preference for the former 
over the latter. If you miss Microsoft Word, you will be in your realm. A bit 
of the underdog, Abiword is a lightweight alternative to those. If you goal is 
just to edit a word document, and you do not care about spreadsheets or 
databases, Abiword will do the trick while preserving the features.

To conclude briefly, if I had one advice to give, it would be to pick the editor 
that people around you use. If for some crazy reason everyone you know uses 
LibreOffice to code in C, or Sublime Text to write a novel, and that you have 
to do something similar, you should follow the trend. The reason being that if 
you encounter a problem and need some help, it will be easier for them to 
bring it to you.
What is your favorite text editor? What do you use it for? Please let us know 
in the comments.
Linux Tutorials
How to use matplotlib for scientific 
plotting on Linux
If you want an efficient, automatable solution for producing high-quality 
scientific plots in Linux, then consider using matplotlib. Matplotlib is a

Pythonbased open-source scientific plotting package with a license based on 
the Python Software Foundation license. The extensive documentation and 
examples, integration with Python and the NumPy scientific computing 
package, and automation capability are just a few reasons why this package is 
a solid choice for scientific plotting in a Linux environment. This tutorial will 
provide several example plots created with matplotlib.
Features
Numerous plot types (bar, box, contour, histogram, scatter, line plots...) 
Python-based syntax
Integration with the NumPy scientific computing package
Source data can be Python lists, Python tuples, or NumPy arrays 
Customizable plot format (axes scales, tick positions, tick labels...) 
Customizable text (font, size, position...)
TeX formatting (equations, symbols, Greek characters...)
Compatible with IPython (allows interactive plotting from a Python shell) 
Automation - use Python loops to iteratively create plots
Save plots to image files (png, pdf, ps, eps, and svg format)
The Python-based syntax of matplotlib serves as the foundation for many of 
its features and enables an efficient workflow. There are many scientific 
plotting packages that can produce quality plots, but do they allow you to do 
it directly from within your Python code? On top of that, do they allow you to 
create automated routines for iterative creation of plots that can be saved as 
image files? Matplotlib allows you to accomplish all of these tasks. You can 
now look forward to saving time that would have otherwise been spent 
manually creating multiple plots.
Installation of matplotlib
Installation of Python and the NumPy package is a prerequisite for use of 
matplotlib. Instructions for installing NumPy can be found here.
To install matplotlib in Debian or Ubuntu, run the following command: 
xterm
$ sudo apt-get install python-matplotlib

To install matplotlib in Fedora or CentOS/RHEL, run the following
command:
xterm
$ sudo yum install python-matplotlib
Matplotlib Examples
This tutorial will provide several plotting examples that demonstrate how to 
use matplotlib:
Scatter and line plot Histogram plot
Pie chart
In these examples we will use Python scripts to execute matplotlib 
commands. Note that the numpy and matplotlib modules must be imported 
from within the scripts via the import command. np is specified as a reference 
to the
numpy module and plt is specified as a reference to the
matplotlib.pyplot namespace:
import numpy as np
import matplotlib.pyplot as plt
Example 1: Scatter and Line Plot
The first script, script1.py completes the following tasks:
Creates three data sets ( xData, yData1, and yData2)
Creates a new figure (assigned number 1) with a width and height of 8 inches 
and 6 inches, respectively

Sets the plot title, x-axis label, and y-axis label (all with font size of 14) Plots 
the first data set, yData1, as a function of the xData dataset as a dotted blue line 
with circular markers and a label of "y1 data" Plots the second data set, yData2, 
as a function of the xData dataset as a solid red line with no markers and a 
label of "y2 data".
Positions the legend in the upper left-hand corner of the plot Saves the figure 
as a PNG file
Contents of script1.py:
import numpy as np
import matplotlib.pyplot as plt
xData = np.arange(0, 10, 1)
yData1 = xData.__pow__(2.0)
yData2 = np.arange(15, 61, 5)
plt.figure(num=1, figsize=(8, 6))
plt.title('Plot 1', size=14)
plt.xlabel('x-axis', size=14)
plt.ylabel('y-axis', size=14)
plt.plot(xData, yData1, color='b', linestyle='--', marker='o', label='y1 data')
plt.plot(xData, yData2, color='r', linestyle='-', label='y2 data')
plt.legend(loc='upper left')
plt.savefig('images/plot1.png', format='png')

The resulting plot is shown below:

Plot 1
x-axis
■ — y2 data
Example 2: Histogram Plot
The second script, script2.py completes the following tasks: 

Creates a data set containing 1000 random samples from a Normal 
distribution
Creates a new figure (assigned number 1) with a width and height of 8 inches 
and 6 inches, respectively
Sets the plot title, x-axis label, and y-axis label (all with font size of 14) Plots 
the data set, samples, as a histogram with 40 bins and an upper and lower 
bound of -10 and 10, respectively
Adds text to the plot and uses TeX formatting to display the Greek letters
mu and sigma (font size of 16)
Saves the figure as a PNG file
Contents of script2.py:
import numpy as np
import matplotlib.pyplot as plt
mu = 0.0
sigma = 2.0
samples = np.random.normal(loc=mu, scale=sigma, size=1000)
plt.figure(num=1, figsize=(8, 6))
plt.title('Plot 2', size=14)
plt.xlabel('value', size=14)
plt.ylabel('counts', size=14)
plt.hist(samples, bins=40, range=(-10, 10))
plt.text(-9, 100, r'$mu$ = 0.0, $sigma$ = 2.0', size=16)
plt.savefig('images/plot2.png', format='png')


value
Example 3: Pie Chart
The third script, script3.py completes the following tasks:

Creates data set containing five integers
Creates a new figure (assigned number 1) with a width and height of 6 inches 
and 6 inches, respectively
Adds an axes to the figure with an aspect ratio of 1
Sets the plot title (font size of 14)
Plots the data set, data, as a pie chart with labels included Saves the figure as a
PNG file
Contents of script3.py: 
import numpy as np
import matplotlib.pyplot as plt
data = [33, 25, 20, 12, 10]
plt.figure(num=1, figsize=(6, 6))
plt.axes(aspect=1)
plt.title('Plot 3', size=14)
plt.pie(data, labels=('Group 1', 'Group 2', 'Group 3', 'Group 4', 'Group 5'))
plt.savefig('images/plot3.png', format='png')

The resulting plot is shown below:

Plot 3
Summary
This tutorial provides several examples of plots that can be created with the

matplotlib scientific plotting package. Matplotlib is a great solution for 
scientific plotting in a Linux environment given its natural integration with 
Python and NumPy, its ability to be automated, and its production of a wide 
variety of customizable high quality plots. Documentation and examples for 
the matplotlib package can be found at the official site.
Linux Tutorials
How to encrypt files and directories 
with eCryptFS on Linux
You do not have to be a criminal or work for the CIA to use encryption. You 
simply don't want anybody to spy on your financial data, family pictures, 
unpublished manuscripts, or secret notes where you have jotted down startup 
ideas which you think can make you super rich.
I have heard people telling me "I'm not important enough to be spied on" or 
"I don't hide anything to care about." Well, my opinion is that even if I don't 
have anything to hide, or I can publish a picture of my kids with my dog, I 
have the right to not do it and want to protect my privacy.
Types of Encryption
We have largely two different ways to encrypt files and directories. One 
method is filesystem-level encryption, where only certain files or directories 
(e.g., /home/alice) are encrypted selectively. To me, this is a perfect way to start. 
You don't need to re-install everything to enable or test encryption. 
Filesystem-level encryption has some disadvantages, though. For example, 
many modern applications cache (part of) files in unencrypted portions of 
your hard drive, such as swap partition, /tmp and /var folders, which can result 
in privacy leaks.
The other way is so-called full-disk encryption, which means that the entire 
disk is encrypted (possibly except for a master boot record). Full disk

encryption works at the physical disk level; every bit written to the disk is 
encrypted, and anything read from the disk is automatically decrypted on the 
fly. This will prevent any potential unauthorized access to unencrypted data, 
and ensure that everything in the entire filesystem is encrypted, including 
swap partition or any temporarily cached data.
Available Encryption Tools
There are several options to implement encryption in Linux. In this tutorial, I 
am going to describe one option: eCryptFS a stacked cryptographic 
filesystem tool. For your reference, here is a roundup of available Linux 
encryption tools.
Filesystem-level encryption
EncFS : one of the easiest ways to try encryption. EncFS works as a stacked 
filesystem, so you just create an encrypted folder and mount it to a folder to 
work with.
eCryptFS: a POSIX compliant cryptographic filesystem, eCryptFS works in 
the same way as EncFS, so you have to mount it.
Full-disk encryption
Loop-AES : the oldest disk encryption method. It is really fast and works on 
old system (e.g., kernel 2.0 branch).
DMCrypt: the most common disk encryption scheme supported by the 
modern Linux kernel.
CipherShed: an open-source fork of the discontinued TrueCrypt disk 
encryption program.
Basics of eCryptFS
eCryptFS is a stacked cryptographic filesystem, which has been natively 
supported by the Linux kernel since 2.6.19 (as ecryptfs module). An eCryptFS- 
encrypted pseudo filesystem is mounted on top of your current filesystem. It 
works perfectly on EXT filesystem family and others like JFS, XFS,

ReiserFS, Btrfs, even NFS/CIFS shares. Ubuntu uses eCryptFS as its default 
method to encrypt home directory, and so does ChromeOS. Underneath it, 
eCryptFS uses AES algorithm by default, but it supports others algorithms, 
such as blowfish, des3, cast5, cast6. You will be able to choose among them 
in case you create a manual setup of eCryptFS.
Like I said, Ubuntu lets us choose whether to encrypt our /home directory 
during installation. Well, this is the easiest way to use eCryptFS.

@ Install
Who are you?
Your name: Your name
I
14 0 ® «» $
Pick a username:
Choose a password:
Confirm your password:
Your computer’s name:

Ubuntu provides a set of user-friendly tools that make our life easier with 
eCryptFS, but enabling eCryptFS during Ubuntu installation only creates a 
specific pre-configured setup. So in case the default setup doesn't fit your 
needs, you will need to perform a manual setup. In this tutorial, I will 
describe how to set up eCryptFS manually on major Linux distros.
Installation of eCryptFS
On Ubuntu, Debian or its derivatives:
xterm
Bl_ $ sudo apt-get install ecryptfs-utils
__ I^^Note that if you chose to encrypt your home directory during 
Ubuntu installation, eCryptFS should be already installed.
On CentOS, RHEL or Fedora:
term
# yum install ecryptfs-utils
On Arch Linux:
xterm
T $ sudo pacman -S ecryptfs-utils
__ I^^After installing the package, it is a good practice to load the 
eCryptFS kernel module just to be sure:
xterm
lb__ $ sudo modprobe ecryptfs
Configure eCryptFS
Now let's start encrypting some directory by running eCryptFS configuration

tool:
_____^^Bxterm
$ ecryptfs-setup-private

f reeuse r@freexploit:ecryptfs-setup■private
Enter your login passphrase [freeuser];
Enter your mount passphrase [leave blank to generate one]:
******* nt ********************************* ************ *****************
you SHOULD RECORD YOUR MOUNT PASSPHRASE AND STORE IT IN A SAFE LOCATION.
ecryptf swap-passphrase V.ecryptfs/wrapped-passphrase
THIS WILL BE REQUIRED IF YOU NEED TO RECOVER YOUR DATA AT A LATER TIME,
************************************************************************
Done configuring.
Testing mount/write/umount/read...
Inserted auth tok with sig [bdadc9d2735357b4
Inserted auth tok with sig [9d0a4549ef6f9f2b
Inserted auth tok with sig [bdadc9d273535M 
Inserted auth tok with sig [9dDa4549ef6f9f2b 
Testing succeeded.
into the user session keyring 
into the user session keyring 
into the user session keyring 
into the user session keyring
Logout, and log hack in to begin using your encrypted directory,
freeuser@freexploit!'$ Is
Documents Private 
freeuser@freexploit :-■$ |
default ]
1 
f»er(M,0,233
i 16h 53n42s IMI
0 2014-08-23 0 22:05
It will ask for a login passphrase and a mount passphrase. The login

passphrase is the same as your normal login password. The mount passphrase 
is used to derive a file encryption master key. Leave it blank to generate one 
as it's safer. Log out and log back in.
You will notice that eCryptFS created two directories by default: Private and 
.Private in your home directory. The ~/.Private directory contains encrypted data, 
while you can access corresponding decrypted data in the
~/Private directory. At the time you log in, the ~/.Private directory is 
automatically decrypted and mapped to the ~/Private directory, so you can 
access it. When you log out, the ~/Private directory is automatically unmounted 
and the content in the ~/Private directory is encrypted back into the ~/.Private 
directory.
The way eCryptFS knows that you own the ~/.Private directory, and 
automatically decrypts it into the ~/Private directory without needing you to 
type a password is through an eCryptFS pam module which does the trick for 
you.
In case you don't want to have the ~/Private directory automatically mounted 
upon login, just add the --noautomount option when running
ecryptfs-setup-private tool. Similarly, if you do not want the
~/Private directory to be automatically unmounted after logout, specify - 
noautoumount option. But then, you will have to mount or unmount 
~/Private directory manually by yourself:
xterm
lb__ $ ecryptfs-mount-private ~/.Private ~/Private
$ ecryptfs-umount-private ~/Private
L Byoh can verify that .Private folder is mounted by running:
xterm
Hb__ $ mount

Now we can start putting any sensitive files in ~/Private folder, and they will

automatically be encrypted and locked down in ~/.Private folder when we log 
out.
All this seems pretty magical. Basically, the ecryptfs-setup-private tool makes 
everything easy to set up. If you want to play a little more and set up specific 
aspects of eCryptFS, go to the official documentation.
Conclusion
To conclude, if you care a great deal about your privacy, the best setup I 
recommend is to combine eCryptFS-based filesystem-level encryption with 
full-disk encryption. Always remember though, file encryption alone does not 
guarantee your privacy.
Linux Tutorials
How to check firmware version on
Linux
Question: I would like to upgrade the BIOS firmware of my Linux computer. 
But first I want to check the firmware version. Is there any Linux command 
for that?
To find out the firmware version of a Linux computer, you can use a 
command line tool called dmidecode. This tool dumps the content of a 
computer's system BIOS table in a human-readable format. Displayed 
information includes various hardware properties such as firmware, 
motherboard, CPU, cache, memory controller, PCI slots, etc.
Run the following command, and look for firmware version under "bios 
information".
xterm
T__ $ sudo dmidecode | more

$ sudo dmidecode I more
# dmidecode 2,12
SMBIOS 2,8 present,
180 structures occupying 6066 bytes. 
Table at OxBFBDBOOO.
is upgradeable 
shadowing is allowed 
support is available 
from CD is supported
Handle OxOQOO, DMT type 0, 24 bytes 
BIOS Information 
Vendor: HP 
Version: P71 
Release Date: 12/26/2013 
Address: OxFOOQO 
Runtime Size: 64 kB 
ROM Size: 8192 kB 
Characteristics:
PCI is supported 
PNP is supported 
BIOS 
BIOS 
ESCD 
Boot
Selectable boot is supported 
EDD is supported
5.257360 kB floppy services are supported (int 13h) 
5.2571-2 MB floppy services are supported (int 13h) 
3,57720 kB floppy services are supported (int 13h) 
Print screen service is supported (int 5h)
8042 keyboard services are supported (int 9h) 
Serial services are supported (int 14h) 
Printer services are supported (int 17h)
CGA/mono video services are supported (int lOh) 
ACPI is supported 
USB legacy is supported
BIOS boot specification is supported
Function key-initiated network boot is supported 
Targeted content distribution is supported
Firmware Revision: 1.40

Linux Tutorials
How to run SQL queries against 
Apache log files on Linux
One of the distinguishing features of Linux is that, under normal 
circumstances, you should be able to know what is happening and has 
happened on your system by analyzing one or more system logs. Indeed, 
system logs are the first resource a system administrator tends to look to 
while troubleshooting system or application issues. In this article, we will 
focus on the Apache access log files generated by Apache HTTP server. We 
will explore an alternative way of analyzing Apache access logs using asql, an 
open-source tool that allows one to run SQL queries against the logs in order 
to view the same information in a more friendly format.
Background on Apache Logs
There are two kinds of Apache logs:
Access log: Found at /var/log/apache2/access.log (for Debian) or /var/log/httpd/access_log 
(for Red Hat). Contains records of every 
request served by an Apache web server.
Error log: Found at /var/log/apache2/error.log (for Debian) or /var/log/httpd/error_log 
(for Red Hat). Contains records of all error
conditions reported by an Apache web server. Error conditions include, 
but are not limited to, 403 (Forbidden, usually returned after a valid 
request missing access credentials or insufficient read permissions), and 404 
(Not found, returned when the requested resource does not exist). Although 
the verbosity of Apache access log file can be customized through Apache's 
configuration files, we will assume the default format in this article, which is 
as follows:
Remote IP - Request date - Request type - Response code - Requested 
resource - Remote browser (may also include operating system)

So a typical Apache log entry looks like:
192.168.0.101 - - [22/Aug/2014:12:03:36 -0300] "GET
/icons/unknown.gif HTTP/1.1" 200 519
"http://192.168.0.10/test/projects/read_json/" "Mozilla/5.0 (X11; Ubuntu; 
Linux x86_64; rv:30.0) Gecko/20100101 Firefox/30.0"
But what about Apache error log? Since error log entries dealing with 
particular requests have corresponding entries in the access log (which you 
can customize), you can use the access log file to obtain more information 
about error conditions (refer to example 5 for more details).
That being said, please note that access log is a system-wide log file. To find 
the log files of virtual hosts, you may also need to check their corresponding 
configuration files (e.g., within /etc/apache2/sites-available/[virtual host name] on 
Debian).
Installing asql on Linux
asql is written in Perl, and requires two Perl modules: a DBI driver for SQLite 
and GNU readline.
Install asql on Debian, Ubuntu or their derivatives

asql and its dependencies will automatically be installed with aptitude on 
Debian-based distributions.
term
T # aptitude install asql
Install asql on Fedora, CentOS or RHEL
On CentOS or RHEL, you will need to enable EPEL repository first, and then 
run the commands below. On Fedora, proceed to these commands directly.
# sudo yum install perl-DBD-SQLite perl-Term-ReadLineGnu
# wget http://www.steve.org.uk/Software/asql/asql1.7.tar.gz
# tar xvfvz asql-1.7.tar.gz
# cd asql
# make install
How Does asql Work?
As you can guess from the dependencies listed above, asql converts 
unstructured plain-text Apache log files into a structured SQLite database, 
which can be queried using standard SQL commands. This database can be 
populated with the contents of current and past log files - including 
compressed rotated logs such as access.log.X.gz or access_log.old.
First, launch asql from the command line with the following command
term
# asql
ou will be entering asql's built-in shell interface.

root@debianasql 
asql vl.6 - type help' 
asql> |
for help.
Let's type help to list the
available commands in the asql shell:
asql> help
asql vl.6
The following commands are available within this shell:
alias - Define, or view, persistent aliases.
alter - Run an ALTER query against the database.
create - Run a CREATE query against the database.
delete - Run a DELETE query against the database.
drop - Run a DROP query against the database.
exit - Exit the shell.
help - Show general, or command-specific, help information, 
insert - Run an INSERT query against the database.
load - Load an Apache logfile.
quit - Exit this shell.
restore - Load a previously save'd temporary database.
save - Save the temporary database.
select - Run a SELECT query against the database.
show - Show the structure of the database.
update - Run an UPDATE query against the database.
For command-specific help run "help command".
asql> |
We will begin by loading all the access logs in asql, which can be done with:
xterm
asql> load </path/to/apache-access-logs>
__^^Hln case of Debian, the following command will do:
xlterm

asql> load /var/log/apache2/access.
*
In case of CentOS/RHEL, use this command instead:
_____^^Ixterm
asql> load /var/log/httpd/access_log
*
When asql finishes loading access logs, we can start querying the database. 
Note that the database created after loading is temporary, meaning that if you 
exit the asql shell, the database will be lost. If you want to preserve the 
database, you have to save it to a file first. We will see how to do that later 
(refer to examples 3 and 4).
asql> load /var/log/apache2/access.* 
Loading: /var/log/apache2/access.log 
Loading: /var/log/apache2/access.log.l 
Loading: /var/log/apache2/access.log.2.gz 
asql> |
he database
contains a table named logs. The available fields in the logs table can be 
displayed using the show command:

asql> show;
The table ’log’ table has the following columns:
id - ID of the request
source - IP, or hostname, which made the request.
request - The HTTP request
status - The HTTP status-code returned
size - The size of the response served, in bytes, 
method - The HTTP method invoked (GET, PUT, POST etc), 
referer - The HTTP referer (sic).
agent - The User-Agent which made the request.
version - The HTTP version used by this client.
date - The date and time at which the request was made, 
label - Any label applied when the logfile was read, 
user - The remote (authenticated) user, if any.
The .asql hidden file, which is stored in each user's home directory, records the 
history of the commands that were typed by the user in an asql shell. Thus, 
you can browse through it using the arrow keys, and repeat previous 
commands by just pressing enter when you find the right one.
SQL Query Examples with asql
Here a few examples of running SQL queries against Apache log files with 
asql.
Example 1:
Listing the request sources / dates and HTTP status codes returned during the 
month of October 2014.
month of October 2014.
01T00:00:00' ORDER BY source;

asql> SELECT source, date, status FROM logs WHERE date >= '2014-10-01100:00:00' ORDER BY source; 
115.239.248.46 2014-10-09118:09:35 404
192.168.0.104 2014-10-28122:00:49 404
192.168.0.104 2014-10-28122:00:50 404
asql> |
Example 2:
Displaying the total size (in bytes) of requests served per client in descending 
order.
SELECT source,SUM(size) AS Number FROM logs GROUP BY source 
ORDER BY Number DESC;
asql>_SELECT source,SUH(size) AS Number FROM logs GROUP BY source ORDER BY Number DESC;
|.28 2008472
1.29 1890138
192.168.0.100 1749037
|156 948694
192.ibb.0’.108 566512
192.168.0.105 221525
tt.22 185341
192.168.0.101 161158
192.168.0.102 75937
J.185 49062
197 fl 1fl4 AMSR

Example 3:
Saving the database to [filename] in the current working directory. 
save [filename]
asql> load /var/log/apache2/access.*  
Loading: /var/log/apache2/access.log 
Loading: Ivar/log/apache2/access.log.1 
Loading: /var/log/apache2/access.log.2.gz 
asql> save accessDB2014-l0-28 
Saving to : accessDB2014-10-28 
asql>
This allows us
to avoid the need for waiting while the log parsing is performed with the load 
command as shown earlier.
Example 4:
Restoring the database in a new asql session after exiting the current one. 
restore [filename]

root@debian:~# Is -Ih | grep access
-rw-r--r-- 1 root root 3.0M Oct 29 Oft:16 accessDB2014-10-28 
root@debian:asql --quiet
asql> SELECT * FROM logs' 
=
No files loaded yet! f temporary data oas s Hemp .
asql> restore accessDB2014-10-28 । 
asql> SELECT COUNT(id) FROM logs; 
10047
Afterthe database is restoredfrom 
the access DB2014-10-28 file, the 
database can be queried.
Example 5:
Returning error conditions logged in the access file. In this example, we will 
display all the requests that returned a 403 (access forbidden) HTTP code. 
SELECT source,date,status,request FROM logs WHERE status='403' 
ORDER BY date

asqb SELECT sourcejdate^tatus, request FROM logs WHERE status^!1 ORDER EY date
192.168.0.100 2013-94-24100:19:48 403 /heraldo/mysql/
192.168.0.100 2010-04-24100:19:55 400 /heraldo/mysql/dos.html
192.168.0.101 2014-02-11120:41:12 490 /Pearson.CompTIA.NetworkPlus.N10-005.Authorized.Exam.Cram.4th.Edition.Nov.2011.pdf
192.168.0.101 2014-02-11723:41:36 400 /Pearson.CompTIA.NetworkPlus.N10-005.Authorized.Exam.Cram.4th.Edition.Nov.2011.pdf
192.168.0.101 2014-02-11123:41:48 493 /Pearson.CompTIA.NetworkPlus.N10-005.Authorized.Exam.Cram.4th.Edition.Nov.2011.pdf
192.168.0.101 2014-08-18713:23:50 403 /motd.pdf
192.168.0.101 2014-08-18710:24:25 403 /test/motd.pdf
192.168.0.104 2014-10-29701:34:51 400 /blocked.htd
192.168.0.1O4 2014-10-29701:34:54 403 /blocked.html
192.168.0.104 2014-10-29701:34:56 403 /blocked.html
asql> I
This goes to show that although asql only analyzes access logs, we can use the 
status field of a request to display requests with error conditions.
Summary
We have seen how asql can help us analyze Apache logs and present the 
results in a user friendly output format. Although you could obtain similar 
results by using command line utilities such as cat in conjunction with
grep , uniq, sort, and wc (to name a few), in comparison asql represents a Swiss 
army knife due to the fact that it allows us to use standard SQL syntax to 
filter the logs according to our needs.
Feel free to leave your questions or comments below. Hope it helps.
Linux Tutorials
How to add a static route
permanently on Linux

A static route is defined by statically configuring a next-hop router IP address 
for a specific destination network. You can add a static route by using route add 
or ip route add command. However, any static route so added is not persistent 
across reboots. If you would like to configure a permanent static route on 
Linux, do the following.
In this example, it is assumed that you want to configure a static route for 
network 10.10.0.0/16 that is reachable via next hop 10.10.5.5 on eth0.
Add a static route permanently on
CentOS/RHEL/Fedora
xterm
■l _$ sudo echo "10.10.0.0/16 via 10.10.5.5" >>
/etc/sysconfig/network-scripts/route-ethO
Add a static route permanently on Ubuntu/Debian
xterm
H__ $ sudo echo "up route add -net 10.10.0.0/16 gw 10.10.5.5 dev eth0" >>
/etc/network/interfaces__ L-J
Linux Tutorials
How to set up Clam Antivirus, 
SpamAssassin and MailScanner on 
Ubuntu mail server
Antivirus and anti-spam protection are the among the most important security 
features for a mail server. Unix/Linux based mail servers are typically 
invulnerable to malware and viruses, and there is a very slim chance that the 

server itself may get infected. On the other hand, the operating system of an 
end user device may not always be so secured. We certainly do not want our 
mail server to accept or distribute malware embedded emails. So setting up 
antivirus software on a mail server is a must.
Anti spam filters will inspect every incoming and outgoing mail for patterns 
of spamming. For example, spam mails usually contain a large number of 
recipients. Also, reverse DNS query for the domain in a spam mail does not 
always provide proper answers. If the spam filter software finds any mail that 
could be spam, it blocks the mail. This helps retaining the reputation of the 
mail server, as well as prevents the IP address of the mail server from being 
blacklisted.
In this tutorial, we will be looking at how to secure our mail server on 
Ubuntu by setting up:
Clam Antivirus : open-source antivirus engine.
SpamAssassin: e-mail spam filtering engine.
MailScanner [version_4.74.16-1]: uses antivirus and anti-spam engines to 
scan inbound and outbound emails.
This tutorial is version specific. As of this writing, MailScanner is not 
available in the Ubuntu repository. So we will be using the MailScanner .deb 
package instead. Unfortunately, the dependency packages required for the 
latest version of MailScanner [4.79.11-2.2] are not available in the Ubuntu 
repository either. However, the dependency packages for version 4.74.16-1 
are available. Thus, we will be using MailScanner [4.79.16-1] .deb package 
in this tutorial. Ubuntu 12.04 is used for testing.
For those of you who are interested in setting it up on CentOS, refer to this 
tutorial instead.
Installing Dependencies on Ubuntu
Before starting doing anything on Ubuntu, the first thing to do is be to install 
all the necessary dependencies. The list of dependencies is long, but luckily it 
can be done using one command.

term
# apt-get install gcc g++ cpp zlib1g-dev libgmp3-dev perl bzip2 zip make 
patch automake libhtml-templateperl linux-headers-'uname -r' build-essential 
libnewtdev libusb-dev libconvert-tnef-perl libdbd-sqlite3-perl libfilesys-df- 
perl libmailtools-perl libmime-tools-perl libmime-perl libnet-cidr-perl libsys- 
syslog-perl libiostringy-perl libfile-temp-perl libole-storage-lite-perl 
libarchive-zip-perl libole-storage-lite-perl libdigestsha-perl
Installing Clam Antivirus and SpamAssassin
Now that the dependencies are installed, Clam Antivirus and SpamAssassin 
can be installed using apt-get.
xterm
lb_ # apt-get install clamav clamav-daemon spamassassin
USpamAssassin has to be enabled, and then started:
xterm
lb_ # vim /etc/default/spamassassin
_I^Benabled=i
xterm
lb_ # service spamassassin restart
__ ^^^After the packages are installed, they can be updated using the 
following commands.
xterm
T # freshclam ; sa-update
Installing MailScanner

After all the software that MailScanner depends on has been installed, we 
will download the .deb package for MailScanner version 4.74 and install it.
xterm
# wget
http://mirrors.kernel.org/ubuntu/pool/universe/m/mailsc
anner/mailscanner_4.74.16-1_all.deb
# dpkg -i mailscanner_4.74.16-1_all.deb
Configuring MailScanner
Now it is time to adjust the parameters of MailScanner.
First of all, the directory for SpamAssassin is created and permission for that 
directory is adjusted.
xterm
Bi_# mkdir /var/spool/MailScanner/spamassassin
# chown postfix /var/spool/MailScanner/spamassassin
__B^^B'rhe configuration file /etc/MailScanner/MailScanner.conf is backed up, and 
then modified as followed.
xterm
Hl_# vim /etc/MailScanner/MailScanner.conf 
%org-name% = test Ubuntu mail server
%org-long-name% = Your Organization Name Here %web-site% =
www.your-organisation.com
Run As User = postfix Run As Group = postfix 
Incoming Queue Dir = /var/spool/postfix/hold 
Outgoing Queue Dir = /var/spool/postfix/incoming 
MTA = postfix
Virus Scanners = clamav
Spam List = SBL+XBL

# # please check /etc/MailScanner/spam.lists.conf for more details ## 
SpamAssassin User State Dir = /var/spool/MailScanner/spamassassin ## the 
directory created earlier ##

More information about the configuration file parameters can be found in the 
official documentation.
Postfix configuration file is modified as well. We will configure Postfix to 
hold off any mails. MailScanner will swoop in, and check those emails. Then 
the mails will be handed over to Postfix again for delivery. Here is how the 
configurations are modified.
term
T # vi /etc/postfix/header_checks 
__ ^^■/AReceived:/ HOLD
term
~M # vim /etc/postfix/main.cf
__ ^^Bheader_checks = regexp:/etc/postfix/header_checks
MailScanner is enabled by un-commenting the following line.
term
A__# vim /etc/default/mailscanner
Irun mailscanner=1

Finally, Postfix and MailScanner services are started.
term
term
Bl_ # service postfix restart
# service mailscanner restart
Testing MailScanner
Now that MailScanner has been deployed, we can test its functionality by 
monitoring the mail log. Let us send a test mail and see what happens.
term
T # tail /var/log/mail.log
Mar 3 02:46:39 ubuntu postfix/smtpd[31616]: connect from 
localhost[127.0.0.1]
Mar 3 02:46:39 ubuntu postfix/smtpd[31616]: E5F3C44FB1: 
client=localhost[127.0.0.1], sasl_method=LOGIN, sasl_username=sarmed
Mar 3 02:46:39 ubuntu postfix/cleanup[31620]: E5F3C44FB1: hold: header
Received: from [server_ip] (localhost [127.0.0.1])??by ubuntu.example.tst 
(Postfix) with ESMTPA id E5F3C44FB1??for ; Mon, 3 Mar 2014 02:46:39 
+0600 (BDT) from localhost[127.0.0.1]; from= to= proto=ESMTP helo= 
<[server_ip]>
Mar 3 02:46:39 ubuntu postfix/cleanup[31620]: E5F3C44FB1: message-id=
Mar 3 02:46:40 ubuntu postfix/smtpd[31616]: disconnect from 
localhost[127.0.0.1]
Mar 3 02:46:40 ubuntu MailScanner[31695]: MailScanner E-Mail Virus 
Scanner version 4.74.16 starting...
Mar 3 02:46:40 ubuntu MailScanner[31695]: Read 848 hostnames from the 
phishing whitelist

Mar 3 02:46:40 ubuntu MailScanner[31570]: New Batch: Scanning 1 
messages, 2572 bytes
Mar 3 02:46:40 ubuntu MailScanner[31695]: Read 4278 hostnames from
the phishing blacklist
Mar 3 02:46:40 ubuntu MailScanner[31695]: Using SpamAssassin results 
cache
Mar 3 02:46:40 ubuntu MailScanner[31695]: Connected to
SpamAssassin cache database
Mar 3 02:46:40 ubuntu MailScanner[31695]: Enabling SpamAssassin auto­
whitelist functionality...

Mar 3 02:46:41 ubuntu MailScanner[31695]: Using locktype = flock Mar 3
02:46:41 ubuntu MailScanner[31570]: Virus and Content Scanning:
Starting
Mar 3 02:46:48 ubuntu MailScanner[31570]: Requeue: 
E5F3C44FB1.283A6 to 13B8344FB3
Mar 3 02:46:48 ubuntu MailScanner[31570]: Uninfected: Delivered 1 
messages
Mar 3 02:46:48 ubuntu postfix/qmgr[31519]: 13B8344FB3: from=, 
size=1879, nrcpt=1 (queue active)
Mar 3 02:46:48 ubuntu postfix/local[31637]: 13B8344FB3: to=, relay=local, 
delay=8.6, delays=8.6/0/0/0.02, dsn=2.0.0, status=sent (delivered to mailbox)

Mar 3 02:46:48 ubuntu postfix/qmgr[31519]: 13B8344FB3: removed
The summary of the log is provided below.
Postfix held the email after the SMTP connection. The email was placed
in /var/spool/postfix/hold.
MailScanner scanned the email: (1) spam-check from blacklist, (2) 
spamcheck from spamassassin online database, and (3) virus and content 
scanning.

MailScanner changed the queue ID for the email.
After the mail was found clean, it was handed over to Postfix with the new 
queue ID.
Postfix delivered the email to destination account.
To sum up, MailScanner integrated with Clam Antivirus and SpamAssassin 
is a very powerful tool, and is a must for production mail servers. It can fend 
off exploitation of most existing mail server vulnerabilities. This tutorial 
covers the minimum configuration for securing a mail server using 
MailScanner. The parameters of MailScanner, Clam Antivirus and 
SpamAssassin are highly customizable, and can be modified to meet different 
requirements.
Hope this helps.
Linux Tutorials
How to monitor system temperature 
on Linux
In most cases, you are not supposed to be worried about the temperature of 
your computer. Barring manufacturing defects, hardware is designed so that 
its temperature does not exceed maximum operating temperature. But even 
without any hardware fault, overheating can occur due to various software 
issues, e.g., buggy graphics card driver, misconfigured fan control program, 
malfunctioning CPU frequency scaling daemon, etc. As pointed out by Ben 
in the comment, another quite common cause for overheating is dust, dirt and 
debris clogging the cooling system (fan, heat sink and ventilation openings). I 
can imagine this could happen quite often with older hardware.
Overheating may become serious enough to cause permanent damage on 
your hardware. So watch out for any overheating issue in your system. Even 
better, have temperature monitoring system in place, so that you will be 
alerted if system temperature suddenly goes up.
In this tutorial, I describe how to monitor system temperature on Linux.

There are several user space tools on Linux, which allow you to check and 
monitor temperature of various system components.
lm-sensors is a software tool that draws from hardware embedded sensors to 
monitor temperatures, voltage, humidity and fans. hddtemp is a tool that can 
measure the temperature of hard drives from S.M.A.R.T. readings.
psensor is a graphical front-end for temperature monitoring, which visualizes 
temperature readings from CPUs, NVidia/ATI/AMD GPUs, hard disks, etc. 
In the following, I will describe how to set up psensor to monitor the 
temperature of CPUs and hard drives.
Install psensor on Linux Desktop
psensor can visualize system temperature based on the information obtained 
from other tools such as lm-sensors and hddtemp. Thus you need to install psensor 
along with those prerequisites.
To install psensor on Ubuntu or Debian:
xterm
T $ sudo apt-get install Im-sensors hddtemp psensor
_!■ Another way to install psensor on Ubuntu is to use their PPA 
repository which contains a more recent version of psensor.
xterm
$ sudo add-apt-repository ppa:jfi/ppa
$ sudo apt-get update
$ sudo apt-get install lm-sensors hddtemp psensor
To install psensor on Fedora:
term
$ sudo yum install lm_sensors hddtemp
$ sudo yum install gcc gtk3-devel GConf2-devel lm_sensors-devel cppcheck 
libatasmart-devel libcurldevel json-c-devel libmicrohttpd-devel help2man 
libnotify-devel libgtop2-devel make

$ wget http://wpitchoune.net/psensor/files/psensor0.8.0.3.tar.gz
$ tar xvfvz psensor-0.8.0.3.tar.gz 
$ cd psensor-0.8.0.3
$ ./configure
$ make
$ sudo make install
Due to the requirement for GTK3 libraries, psensor is not compatible 
with the GNOME 2 desktop of CentOS or RHEL 6.
Configure psensor on Linux
Before launching psensor, you need to configure lm_sensors and hddtemp first.
lm_sensors configuration
To configure lm_sensors, run the following command. Choose yes to every 
question.
xterm
T__ $ sudo sensors-detect
This command will probe for and detect embedded sensors in your hardware 
(including CPUs, memory controllers, I/O chips), and automatically 
determine which driver modules need to be loaded to check temperature on 
your system.
Once sensor probing is completed, you will be asked to add detected driver 
module(s) to /etc configuration, so they can be loaded automatically upon 
boot.

On Ubuntu or Debian, detected driver modules will be added to /etc/modules.
On Fedora, the driver information will be added to /etc/sysconfig/lm_sensors.
Next, proceed to load necessary modules as follows.
On Ubuntu:
xterm
T__ $ sudo service module-init-tools start

$ sudo service lm_sensors start
hddtemp configuration
You also need to launch hddtemp which monitors the temperature of hard 
drives.
Run the following command to launch hddtemp as a daemon. Replace /dev/sda 
with the disk drive to monitor on your system.
term
T $ sudo hddtemp -d /dev/sda
Monitor System Temperature with psensor
To start monitoring temperature with psensor, simply run:
term
T $ psensor
The psensor window shows a list of available sensors, and visualizes 
temperature readings from these sensors. You can selectively enable or 
disable each sensor.

Optionally, you can set an alarm level for each sensor, so that you can be 
notified when the temperature from a sensor exceeds a threshold.
The default temperature unit used by psensor is Celsius. A recent version

(0.7-0.8) of psensor can convert temperature unit between Celsius and
Fahrenheit. If the version of psensor you are using is outdated (e.g.,
0.6.x), and does not have unit conversion, install psensor from its PPA 
repository (for Ubuntu users) or build it from its source (for Debian users).
Linux Tutorials
How to change camera resolution 
programmatically in Android
If you are building a custom Camera app on Android, the following is an 
Android code example that you may find useful. This code snippet allows 
you to change camera resolution programmatically. The code is applicable to 
Android API level 5 and higher.
When you adjust camera resolution, you should first check available camera 
resolutions supported by a given Android device hardware. Then choose one 
among them to fit your need.
mCamera = Camera.open();
Camera.Parameters params = mCamera.getParameters();
// Check what resolutions are supported by your camera List<Size> sizes = 
params.getSupportedPictureSizes();
// Iterate through all available resolutions and choose one.
// The chosen resolution will be stored in mSize.
Size mSize;
for (Size size : sizes) {
Log.i(TAG, "Available resolution: "+size.width+" "+size.height); if 
(wantToUseThisResolution(size)) { 
mSize = size;
break;
}
}

Log.i(TAG, "Chosen resolution: "+mSize.width+" "+mSize.height);
params.setPictureSize(mSize.width, mSize.height);
mCamera.setParameters(params);
Linux Tutorials
How to secure SSH login with

onetime passwords on Linux
As someone says, security is a not a product, but a process. While SSH 
protocol itself is cryptographically secure by design, someone can wreak 
havoc on your SSH service if it is not administered properly, be it weak 
passwords, compromised keys or outdated SSH client.
As far as SSH authentication is concerned, public key authentication is in 
general considered more secure than password authentication. However, key 
authentication is actually not desirable or even less secure if you are logging 
in from a public or shared computer, where things like stealth keylogger or 
memory scraper can always a possibility. If you cannot trust the local 
computer, it is better to use something else. This is when one-time passwords 
come in handy. As the name implies, each one-time password is for single­
use only. Such disposable passwords can be safely used in untrusted 
environments as they cannot be re-used even when they are stolen.
One way to generate disposable passwords is via Google Authenticator. In 
this tutorial, I am going to demonstrate yet another way to create one-time 
passwords for SSH login: OTPW, One-Time PassWord login package. 
Unlike Google Authenticator, you do not rely on any third party for one-time 
password generation and verification.
What is OTPW?
OTPW consists of one-time password generator and PAM-integrated 
verification routines. In OTPW, one-time passwords are generated apriori 
with the generator, and carried by a user securely (e.g., printed in a paper 
sheet). Cryptographic hash of the generated passwords are then stored in the 
SSH server host. When a user logs in with a one-time password, OTPW's 
PAM module verifies the password, and invalidates it to prevent re-use.
Step One: Install and Configure OTPW on Linux
For Debian, Ubuntu or Linux Mint:

Install OTPW packages with apt-get.
xterm
T__ $ sudo apt-get install libpam-otpw otpw-bin
Open a PAM configuration file for SSH ( /etc/pam.d/sshd) with a text editor, and 
comment out the following line (to disable password authentication).
#@include common-auth
and add the following two lines (to enable one-time password 
authentication):
auth required pam_otpw.so
session optional pam_otpw.so

# PAM configuration for the Secure Shell service
# Standard Un
x
 authentication. 
^include common-auth| «- - - - - - -
*
Comment it out
# Enable one-time password authentication
auth
required
session
optional
Add these lines
# Disallow non-root logins when /etc/nologin exists.
account required
pannologin.so
# Uncomment and edit /etc/security/access.conf if you need to set complex 
# access limits that are hard to express in sshd config.
# account required pam access.so
# Standard Un
x
 authorization, 
^include common-account
*
For Fedora or CentOS/RHEL:
OTPW is not available as a prebuilt package on Red Hat based systems. So 
let's install OTPW by building it from the source.
First, install prerequites:
xterm
$ sudo yum git gcc pam-devel
$ git clone https://www.cl.cam.ac.uk/~mgk25/git/otpw $ cd otpw
__ ^^MOpen Makefile with a text editor, and edit a line that starts with 
pamlib= as follows.
On 64-bit system:

PAML IB=/usr/lib64/security
On 32-bit system:
PAML IB=/usr/lib/security
Compile and install it. Note that installation will automatically restart an SSH 
server. So be ready to be disconnected if you are on an SSH connection.
term
Ilk_ $ make
$ sudo make install
Now you need to update SELinux policy since /usr/sbin/sshd tries to write to 
user's home directory, which is not allowed by default SELinux policy. The 
following commands will do. If you are not using SELinux, skip this step.
$ sudo grep sshd /var/log/audit/audit.log | audit2allow
-M mypol
$ sudo semodule -i mypol.pp
Next, open a PAM configuration file for SSH ( /etc/pam.d/sshd) with a text 
editor, and comment out the following line (to disable password 
authentication).
#auth substack password-auth

and add the following two lines (to enable one-time password 
authentication):
auth required pam_otpw.so
session optional pam_otpw.so
Step Two: Configure SSH Server for One-time 
Passwords
The next step is to configure an SSH server to accept one-time passwords.
Open /etc/ssh/sshd_config with a text editor, and set the following three 
parameters. Make sure that you do not add these lines more than once, 
because that will cause an SSH server to fail.
UsePrivilegeSeparation yes
ChallengeResponseAuthentication yes
UsePAM yes
You also need to disable default password authentication. Optionally, enable 
public key authentication, so that you can fall back to key-based 
authentication in case you do not have one-time passwords.

PubkeyAuthentication yes
PasswordAuthentication no
Now restart SSH server.
term
lb__ $ sudo service ssh restart
_ I^Bor:
term
T $ sudo systemctl restart sshd
Step Three: Generate One-time Passwords with
OTPW
As mentioned earlier, you need to create one-time passwords beforehand, and 
have them stored on the remote SSH server host. For this, run otpw-gen tool as 
the user you will be logging in as.
term
■_$ cd ~
$ otpw-gen > temporary_password.txt

It will ask you to set a prefix password. When you later log in, you need to 
type this prefix password AND one-time password. Essentially the prefix 
password is another layer of protection. Even if the password sheet falls into 
the wrong hands, the prefix password forces them to brute-force.
Once the prefix password is set, the command will generate 280 one-time 
passwords, and store them in the output text file (e.g.,

temporary_password.txt). Each password (length of 8 characters by default) is 
preceded by a three-digit index number. You are supposed to print the file in 
a sheet and carry it with you.
OTPW list generated 2015-03-27 17:06 on ubuntuserver
1 
ft 
1 . 
■ .
112 F+oH 52q+
168 MhvY hDCT
224 EwuG c9X:
©00 y6Qj weHd
056 TuQb 2a=Q
001 wjYv 8%Ma
057 cPVm MYOo
113 pApE bPki
169 qaJH Bh90
225 M4ZN cPV4
002 TYij nkU6
058 GUAK aetD
114 FQwV BID
170 G8u+ a+=G
226 z5Tz cerO
003 bvnM Pj/m
059 NSuP DpEU
115 W3n3 oTHP
171 Joun oijw
227 Z=Bb Qg/x
004 h%v/ R3sF
060 vzha 9H/V
116 LWMg jTKx
172 pJzM 0:Ze
228 5FXf :Kq9
005 DYCs SOfH
061 ifcX QFHx
117 GT96 KfaT
173 6nN5 QK4g
229 gpo+ 6=iV
006 XkZZ 2CrF
062 LqFV 6Mev
118 W=sy
174 hBcQ gqW9
230 Ww LS3Z
007 Ee/X D+bQ
063 vbmR t6eL
119 r+BV ZpGr
175 oGUr /5un
231 PNdX HF+n
008 PTAA LQfN
064 d4hr HdGb
120 Gd4q ByN+
176 15%q EZQJ
232 zE3L 7bqQ
009 fgBL c=FT
065 coh/ az:0
121 Bkc= b4a4
177 U:sq PJUN
233 MqDV sBxn
010 05S= 76Q%
066 eszm CPNf
122 AxGW d5Vd
178 xvg4 GMuk
234 M9GZ M=/v
011 UUyv QVBp
067 4eXP :duE
123 SgB/ Fzpz
179 wPbo 766Z
235 icWo :ZeG
012 IkU Axny
068 xdHH J%c/
124 vNXb zLroP
180 sCed XBSJ
236 /uuC CXbL
You will also see ~/.otpw file created, where cryptographic hashs of these 
passwords are stored. The first three digits in each line indicate the index 
number of the password that will be used for SSH login.
term
$ more ~/.otpw
OTPW1
280 3 12 8
191ai+:ENwmMqwn 
218tYRZc%PIY27a 
241ve8ns%NsHFmf 
055W4/YCauQJkr: 
102ZnJ4VWLFrk5N

2273Xww55hteJ8Y 
1509d4b5=A64jBT 
168FWBXY%ztm9j% 
000rWUSdBYr%8UE 
037NvyryzcI+YRX 
122rEwA3GXvOk=z
Test One-time Passwords for SSH Login
Now let's login to an SSH server in a usual way:
xterm
$ ssh [email protected]_host
__ ^Bf OTPW is successfully set up, you will see a slightly different 
password prompt:
Password 191:

Now open up your password sheet, and look for index number 191 in the 
sheet.
023 kBvp tq/G 079 jKEw /HRM 135 oW/c /UeB 191 fOO+ PeiD 247 vAnZ
EgUt
According to sheet above, the one-time password for number 191 is foo+peid.
You need to prepend your prefix password to it. For example, if 
your prefix password is 000, the actual one-time password you need to type 
is 000foo+peid.
Once you successfully log in, the password used is automatically invalidated. 
If you check ~/.otpw, you will notice that the first line is replaced with ---
-------, meaning that password 191 has been voided.
OTPW1
280 3 12 8 
218tYRZc%PIY27a
241ve8ns%NsHFmf
055W4/YCauQJkr:

102ZnJ4VWLFrk5N 
2273Xww55hteJ8Y 
1509d4b5=A64jBT 
168FWBXY%ztm9j% 
000rWUSdBYr%8UE 
037NvyryzcI+YRX 
122rEwA3GXvOk=z
Conclusion
In this tutorial, I demonstrated how to set up one-time password login for 
SSH using OTPW package. You may realize that a print sheet can be 
considered a less fancy version of security token in two-factor authentication. 
Yet, it is simpler and you do not rely on any third-party for its 
implementation. Whatever mechanism you are using to create disposable 
passwords, they can be helpful when you need to log in to an SSH server 
from an untrusted public computer. Feel free to share your experience or 
opinion on this topic.
Linux Tutorials
How to monitor DHCP traffic from

the command line on Linux
Question: I want to find out what IP address is assigned to a host via DHCP 
by monitoring DHCP request and response on the wire. How can I monitor 
DHCP traffic from the command line?
If you want to monitor DHCP communication between a DHCP server and a 
client, you can run a packet sniffing tool on the same local network, and 
capture DHCP traffic. There are a couple of sniffing tools you can use.
Method One: tcpdump
The first method to capture DHCP traffic is to use venerable tcpdump tool. In 
this case, you want to define a filter so that tcpdump dumps only DHCP related 
traffic. In DHCP, UDP port 67 is used by a DHCP server, and UDP port 
number 68 is used by DHCP clients. Thus, you want to capture traffic with 
port number 67 or 68 as follows.
xterm
T__ $ sudo tcpdump -i <network-interface> port 67 or port 68 -e -n

$ sudo tcpdump -i vmnet8 port 67 or port 68 -e -n 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode 
listening on vmnet8, link-type EN10MB (Ethernet), capture size 65535 bytes 
23:36:35.611316 00:Oc:29:24:de:ee > ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), 
length 342: 0.O.0.0.68 > 255.255.255.255.67: BOOTP/DHCP, Request from 00:0c:29:2 
4:de:ee, length 300
23:36:36.612527 00:50:56:f3:b7:3c > 00:0c:29:24:de:ee, ethertype IPv4 (0x0800), 
length 342: 172.16.253.254.67 > 172.16.253.131.68: BOOTP/DHCP, Reply, length 300 
23:36:36.612929 00:0c:29:24:de:ee > ff:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), 
length 342: 0.0.0.0.68 > 255.255.255.255.67: BOOTP/DHCP, Request from O0:0c:29:2 
4:de:ee, length 300
23:36:36.640754 00:50:56:f3:b7:3c > 00:0c:29:24:de:ee, ethertype IPv4 (0x0800), 
length 342: 172.16.253.254.67 > 172.16.253.131.68: BOOTP/DHCP, Reply, length 300
The above tcpdump output shows that IP address 172.16.253.131 is assigned to a 
client with hardware address 00:0c:29:24:de:ee.
Method Two: dhcpdump
The second method to monitor DHCP requests and responses is to use 
dhcpdump, which is a command-line DHCP packet dumper program.
To install dhcpdump on Debian or Ubuntu:
xterm
lb__ $ sudo apt-get install dhcpdump
__ ^^Bto install dhcpdump on CentOS, first set up Repoforge on your 
system, and then run:
xterm
T $ sudo yum install dhcpdump
_J^®To install dhcpdump on Fedora:
xterm
T $ sudo yum install dhcpdump
__ ^^B'The following command will dump DHCP requests and responses 
in a humanreadable format.
term

$ sudo dhcpdump -i <network-interface>

$.
$ sudo dhcpdump -i vmnet8 
TIME: 2014-05-11 22:52:57.661 
IP: 0.0.0.0 (0:c:29:24:de:ee) > 255.255.255.255 (ff:ff:ff:ff:ff:ff) 
OP: 1 (BOOTPREQUEST) 
HTYPE: 1 (Ethernet) 
HLEN: 6 
HOPS: 0 
XID: 881f3411 
SECS: O 
FLAGS: 0
CIADDR: 0.0.0.0
YIADDR: 0.0.0.0
SIADDR: 0.0.0.0
GIADDR: 0.0.0.0 
CHADDR: 00:0c:29:24:de:ee:00:00:00:00:00:00:00:00:00:00 
SNAME: . 
FNAME: .
OPTION: 53 ( 
1) DHCP message type 
1 (DHCPDISCOVER)
OPTION: 55 ( 17) Parameter Request List 
1 (Subnet mask)
28 (Broadcast address) 
2 (Time offset) 
121 (Classless Static Route) 
15 (Domainname) 
6 (DNS server) 
12 (Host name) 
40 (NIS domain) 
41 (NIS servers) 
42 (NTP servers) 
26 (Interface MTU) 
119 (Domain Search) 
3 (Routers)
121 (Classless Static Route) 
249 (MSFT - Classless route) 
33 (Static route) 
42 (NTP servers)
TIME: 2014-05-11 22:52:58.655
IP: 172.16.253.254 (0:50:56:f3:b7:3c) > 172.16.253.131 (0:c:29:24:de:ee)
OP: 2 (BOOTPREPLY)
HTYPE: 1 (Ethernet)
HLEN: 6
HOPS: G
XID: 881f3411
SECS: 0
FLAGS: 0
CIADDR: 0.0.0.0
YIADDR: 172.16.253.131
SIADDR: 172.16.253.254
GIADDR: 0 J3.0 .J0______________________________________________
CHADDR: 00:0c:29:24:de:ee:00:00:00:00:00:00:00:00:00:00
SNAME: .
FNAME: .
OPTION:
53 (
1)
DHCP message type
2 (DHCPOFFER)
OPTION:
54 (
4)
Server identifier
172.16.253.254
OPTION:
51 (
4)
IP address leasetime
1800 (30m)
OPTION:
1 (
4)
Subnet mask
255.255.255.0
OPTION:
28 (
4)
Broadcast address
172.16.253.255
OPTION:
15 (
11)
Domainname
localdomain
OPTION:
6 (
4)
DNS server
172.16.253.2
OPTION:
3 (
4)
Routers
172.16.253.2

The output shown by dhcpdump is more detailed than that of tcpdump. yiaddr 
field is populated with the IP address offered by a DHCP server to a 
client, and chaddr field is the hardware address of the requesting client. It 
also shows other information such as DHCP lease time, subnet mask, DNS 
server, etc.
dhcpdump can filter DHCP responses such that it captures only DHCP 
responses sent to a particular hardware address.
For example, the following command will capture DHCP response packets 
sent to client whose hardware address starts with 00:ci:b5.
xterm
T__ $ sudo dhcpdump -i ethO -h A00:c1:b5
Linux Tutorials
How to install non-free packages on 
Debian
Question: I need to install some proprietary device driver on Debian, which 
is part of a non-free package. However, I cannot find and install the package 
in my Debian box. How can I install non-free packages on Debian?
The Debian project is distributed as a collection of packages, 90,000 of them, 
as of Debian 10 "buster" release. These packages are categorized into three 
areas: main, contrib and non-free, mainly based on licensing requirements, e.g., 
Debian Free Software Guidelines (DFSG).
The main area contains free software that complies with DFSG. The contrib area 
contains free software that complies with DFSG, but relies on 
non-free software for compilation or execution. Finally, the non-free area 
contains non-free packages that are not compliant with DFSG but 
redistributable. The main repository is considered a part of Debian, but 
neither contrib or non-free repository is. The latter two are maintained 

and provided only as a convenience to users.
Enable Contrib and Non-free Repositories on 
Debian
If you want to install any non-free package maintained by Debian, you need 
to enable contrib and non-free repositories. To do so, open
/etc/apt/sources.list with a text editor, and append "contrib nonfree" to each source.
The following is an example of /etc/apt/sources.list for Debian 10 "buster" 
Release.
deb http://deb.debian.org/debian/ buster main contrib non-free deb-src 
http://deb.debian.org/debian/ buster main contrib non-free
deb http://security.debian.org/debian-security buster/updates main contrib 
non-free
deb-src http://security.debian.org/debian-security buster/updates main contrib 
non-free 
# buster-updates, previously known as 'volatile'
deb http://deb.debian.org/debian buster-updates main contrib non-free deb- 
src http://deb.debian.org/debian buster-updates main contrib nonfree

After modifying sources of packages, run the following command to

download package index files for contrib and non-free repositories.
xterm
T $ sudo apt update
I Ilf you are using aptitude, run the following instead.
xterm
$ sudo aptitude update
__ ^^B^^Bnow you are ready to search and install any non-free package 
on Debian.
xterm
lb__ $ apt search nonfree

dan$debian>$ _ _
dan@debian:~$ apt search nonfree|
Sorting... Done
Full Text Search... Done
bsdganes-nonfree/stable 2.17-8 and64
rogue, the classic dungeon exploration game
context-nonfree/stable 2007.03.22-2 all
Non-free items from the ConTeXt distribution
dahdi-firnware-nonfree/stable 2.11.1.0.20170917-1 all
DAHDI non-free firnware
firnware-linux-free/stable,now 3.4 all [installed,autonatic]
Binary firnware for various drivers in the Linux kernel
firnware-linux-nonfree/stable 20190114-2 all
Binary firnware for various drivers in the Linux kernel (neta-package)
firnware-nisc-nonfree/stable 20190114-2 all
Binary firnware for various drivers in the Linux kernel
fonts-ipafont-nonfree-jisx0208/stable 1:00103-7 all
Japanese TrueType font, IPAfont (JISX0208)
Linux Tutorials
How to install Perl modules from
CPAN

Perl has a mechanism to import Perl modules which are external software 
libraries that can be used inside Perl scripts. CPAN is a public repository of a 
large number of useful Perl modules contributed by Perl developers 
worldwide. Since the number of core Perl modules that come with Perl is 
small, you often need to install external Perl modules from CPAN.
You can search for any Perl module at CPAN. Once you identify the name of 
the Perl module to install (e.g., html::template), you can install the Perl module 
as follows.
First, make sure that you have C compiler (e.g., gcc) installed. While most 
Perl modules are written in Perl, some modules are written in C using XS 
interface. So you need to set up Linux C development environment.
Next, launch an interactive CPAN shell, and build/install your Perl module as 
follows.
xterm
Hl__ $ sudo perl -MCPAN -e shell
cpan> install HTML::Template
__ ^^^Alternatively, you can install a Perl module using a simple Perl 
command line:
xterm
lb__ $ sudo perl -MCPAN -e 'install HTML::Template'
If the above commands give you the error saying "Can't locate CPAN.pm in 
@INC", you first need to install CPAN using a standard Linux package 
manager as follows.
Install CPAN on CentOS, Fedora or RHEL
term
$ sudo yum -y install perl-CPAN
Install CPAN on Ubuntu or Debian

xterm
T $ sudo apt-get install perl-modules
_J^Hyou should now be able to use perl -mpcan command to manage 
other Perl modules.
In case you are behind a proxy, and so would like to use CPAN behind 
proxy, you can use the CPAN console to configure proxy for CPAN as 
follows.
xterm
lb_ $ sudo perl -MCPAN -e shell
cpan> o conf init /proxy/
__B^^B'rhe above command will ask you for proxy server settings, and 
commit the information on your system.
Linux Tutorials
How to fix "fatal error: 
jsoncpp/json/json.h: No such file or 
directory"
Question: I am trying compile a C++ application, but I am getting the 
following error:
fatal error: jsoncpp/json/json.h: No such file or directory 
How can I fix this problem?
The error indicates that you are missing JsonCpp development files (i.e., 
JsonCpp library and header files). JsonCpp is a C++ library for 
JSONformatted data manipulation. Here is how to install JsonCpp 
development files on various Linux distros.

Install JsonCpp on Ubuntu, Debian or Linux Mint
term
Ilk_ $ sudo apt-get install libjsoncpp-dev
Install JsonCpp on Fedora or CentOS/RHEL 7 or 
Higher
xterm
$ sudo yum install jsoncpp-devel
On CentOS/RHEL 6 or lower, JsonCpp is not available as a pre-
built package. So you can build and install JsonCpp from the source.
Install JsonCpp from the Source on Fedora, 
CentOS or RHEL
$ sudo yum install cmake
$ git clone https://github.com/open-source
parsers/jsoncpp.git
$ cd jsoncpp
$ mkdir -p build/debug
$ cd build/debug
$ cmake -DCMAKE_BUILD_TYPE=debug
DJSONCPP_LIB_BUILD_SHARED=OFF -G "Unix Makefiles" ../../ 
$ make
$ sudo make install

Linux Tutorials
How to set up WireGuard VPN 
server on Ubuntu 20.04
Traditionally, VPN implementation has existed in two forms. In-kernel VPN 
implementation such as IPsec performs heavy-duty per-packet crypto 
processing in the kernel in a "bump-in-the-stack" fashion (i.e., between IP 
stack and the network drivers). This gives speed as there is no context switch 
between kernel and userspace during packet processing. But it comes with 
high management complexity in separate userspace control plane (e.g., IKE). 
An alternative form of VPN implementation is userspace TUN/TAP-based 
solutions such as OpenVPN, Tinc, n2n, where crypto processing is performed 
by a userspace VPN daemon. Naturally, these TUN/TAP-based VPN 
solutions have poor performance compared to IPsec mainly because network 
packets traverse the kernel and userspace boundary multiple times, resulting 
in frequent context switches and packet copies. Despite their performance 
disadvantage, userspace VPN solutions enjoy more popularity than the 
inkernel counterpart due to their ease of use and configuration.
What is WireGuard?
WireGuard is a relatively new entrant in the VPN software scene, with a bold 
promise to dethrone OpenVPN from the top in terms of speed, ease of use, 
and auditability. To be competitive with in-kernel IPsec in terms of 
performance, WireGuard implements its data path in the kernel. But to avoid 
the pitfalls of IPsec's complex stateful control plane, WireGuard allows it to 
be configured and administered via a stateless virtual interface using the 
standard ip and ifconfig tools. WireGuard kernel module is implemented with 
less than 4,000 lines of code (which amounts to only 1% of OpenVPN or 
IPsec implementations) and uses state-of-the-art proven cryptography. Such 
minimal codebase facilitates rigorous and timely security audits from the 
community as it goes through ongoing updates.

Whether or not WireGuard will fullfill its promise will remain to be seen, but 
so far, it has demonstrated a strong track record of success in growing its 
popularity. The fact that WireGuard has made it into the Linux kernel 5.6 is a 
solid testament to its credibility. As Linus once put it, it's simply a work of 
art.
Use Cases of WireGuard VPN Server
While the title of this tutorial is setting up WireGuard VPN server, 
technically it is not correct. Unlike OpenVPN, there is no notion of server 
and client in WireGuard. Rather, the network endpoints connected by 
WireGuard are called peers, and they talk to each other directly via pairwise 
WireGuard tunnels. Similar concepts are already available in existing peer-to- 
peer VPNs such as Tinc or n2n. Even in such peer-to-peer VPN design, one 
designated peer can play the role of a VPN server for all the other peers. This 
is a traditional VPN service use case, where the goal is to hide your digital 
footprint, circumvent geofencing, protect yourself from snooping on public 
WiFi, etc. In this use case, you set up a designated WireGuard peer on a 
virtual private server (VPS), and have it route all your traffic as a default 
gateway. Since the WireGuard peer is running on VPS, completely under 
your control, you don't have to worry about your online activities on VPN 
secretly being logged, which is always a possibility for existing free VPN 
hosting. It's also possible for the WireGuard VPN server to be configured to 
relay traffic among connected users (in case they want to, but cannot directly 
exchange traffic behind NATs).
With such VPN use case in mind, I describe in this tutorial how to set up 
Wireguard VPN server on Ubuntu environment. The tested platform is 
Ubuntu server 20.04.1 LTS (Focal Fossa) with kernel 5.8.0-38. However, 
most of this tutorial (except for distro-specific WireGuard installation and 
firewall setup) is applicable to all Linux distributions including Debian, 
Fedora and Arch Linux.
Install WireGuard on Ubuntu
Not surprisingly, WireGuard has already been incorporated into the default

repositories of all modern Linux distributions including Ubuntu 20.04. This 
makes WireGuard installation pretty straightforward:
term
lb_ $ sudo apt install wireguard
Configure WireGuard as a VPN Server
Most manual WireGuard configuration can be done with standard ip or ifconfig 
tools, except for crypto configuration. For crypto setup and 
others, WireGuard comes with command-line tools called wg and wgquick.
WireGuard configuration can best be done with the root as it involves 
privileged operations. So first switch to the root:
term
$ sudo -i
_^^Blhe rest of the precedure will be performed as the root.
In WireGuard, each connected peer (including server and clients) needs to 
generate its own cryptographic (public/private) key pair for authentication 
and encryption. So the first step in WireGuard server configuration is to 
generate a key pair for the server as follows.
xterm
# cd /etc/wireguard
# umask 077
# wg genkey | tee /etc/wireguard/privatekey | wg pubkey | tee 
/etc/wireguard/publickey

In the above, the umask command makes sure that only the root has read, write 
and execute permissions on /etc/wireguard. The key pair generated by wg will be 
found in /etc/wireguard. Every client who wants to connect to the server also 
needs to generate his or her own key pair. Client-side configuration is 
covered in another tutorial.
Next, go ahead and create a WireGuard server configuration file in 
/etc/wireguard/wg0.conf. A WireGuard configuration file is named as the 
WireGuard interface name follwed by .conf. So when activated, wg0.conf 
will create a virtual interface named wg0. In the configuration file, we define 
among other things a server's listening port, private key, and a private IP 
address to be assigned to the server, etc.
xterm
lb_ # vi /etc/wireguard/wg0.conf
[Interface]
PrivateKey =
SMV0DW6G04+EQDgK5NMzlIEicD0qQ0ORb7njXp4atko=
Address = 10.0.0.1/24
SaveConfig = true
ListenPort = 51820

PostUp = iptables -A FORWARD -i %i -j ACCEPT; iptables -t nat -A
POSTROUTING -o ens3 -j MASQUERADE
PostDown = iptables -D FORWARD -i %i -j ACCEPT; iptables -t nat -D
POSTROUTING -o ens3 -j MASQUERADE
privatekey : You need to fill this field with the content of /etc/wireguard/privatekey. 
Address: This is a private IP address to be assigned to this peer's
WireGuard virtual interface. It can be either IPv4 or IPv6 address. saveconfig: 
If true, this configuration file will automatically be updated 
when a new peer is connected.
listenport: This is the UDP listening port for incoming connections
from other peers. A standard WireGuard port is UDP/51820. postup/postdown: 
The commands specified here are supposed to be
triggered automatically after this server peer is up (postup) or down 
(postdown). The iptables command enables and disables IP
masquerade, which is needed to allow peers with private IP addresses to 
communicate with each other via their public IP addresses. Here, make 
sure that the interface specified with -o (ens3 in this example) is the 
WAN interface of your Ubuntu server.
Make sure that wg0.conf is not readable to unprivileged users.

xterm
T # chmod 600 /etc/wireguard/wg0.conf
-L^JlNow go ahead and activate this configuration with:
xterm
T # wg-quick up wg0
__ ^H'lhis will activate the wg0 virtual interface, assign a private IP 
address to it, and enable IP masquerade on the interface. You can verify the 
state of the interface with: Iflxterm
T_ # wg show wg0
# ip addr show wg0
root@ubuntu:/etc/wireguard# _ _ _ _ _ _ _
1 rootgubuntu:/etc/wi reguard#’wg-quick up wgO|
|[#] ip link add wgQ type wireguard
|[#] wg setconf wg6 /dev/fd/63
[#] ip -4 address add 10.0,0.1/24 dev wgO
,[#] ip link set ntu 1420 up dev wgO
[[#] iptables -A FORWARD -i wgO -j ACCEPT; iptables -t nat -A POSTROUTING -o ens3 -j MASQUERADE
। rootgubuntu:/etc/wireguard#_ _ _ _ _
। rootjubuntu:/etc/wi reguard# [wg show wgo| 
interface: wgO
1 public key: WfV4YceI4n7GNoggilibLgSeD+FidCJ36giBH0FsqyHGnw=
private key: (hidden)
listening port: 51826
root(3ubuntu:/etc/wireguard#
rootjubuntu:/etc/wireguard# _ _ _ _ _ _ _ _
rootjubuntu:/etc/wi reguard# |ip addr show wgO|
4: wgO: <POINTOPOINT,NOARP,UP,Li)WER_UP> ntu 1420 qdisc noqueue state UNKNOWN group default qlen 1006 
link/none
inet 10.0.0.1/24 scope global wgO 
valid.lft forever preferred_lft forever
root@ubuntu:/etc/wireguard#

If you see the wg0 virtual interface, that means a WireGuard peer is 
successfully up and running on Ubuntu.
WireGuard Post Configuration Steps
In order for this WireGuard peer to successfully admit other peers and act as 
their VPN server, you need to perform the following steps.
First, you need to allow incoming UDP connections on the WireGuard's 
listening port (51820) specified in /etc/wireguard/wg0.conf.
The default firewall configuration tool in Ubuntu system is ufw. Let's use ufw 
to open the WireGuard port in the firewall. If you are configuring your 
Ubuntu server over SSH remotely, don't forget to enable port TCP/22 as well.
term
# ufw allow 51820/udp
# ufw allow 22/tcp
# ufw enable
# ufw status verbose

rootgubuntu:/etc/wireguard#
rootjuMi/etc/wireguard# ufw allow 51820/udpj
Rule updated
Rule updated (v6)
Tcptgubyntui/etc/wireguard# ufw allow 22/tcp
Skipping adding existing rule
Skipping adding existing rule (v6)
root@ubuntu:/etc/wireguard#
root?ubuntu:/etc/wireguard# ufw enable
Connand nay disrupt existing ssFconnections. Proceed with operation (y|n)? y
Firewall is active and enabled on systen startup
root@ubuntu:/etc/wireguard#
root@ubuntu:/etc/wireguard# ufw status verbose
Status: active
Logging: on (low)
Default: deny {inconing), allow (outgoing), disabled (routed)
New profiles: skip
To 
Action Fron
kl820/udp
ALLOW IN
Anywhere
22/tcp
ALLOW IN
Anywhere
51820/udp (v6)
ALLOW IN
Anywhere (v6)
22/tcp (v6)
ALLOW IN
Anywhere (v6)
root@ubuntu:/etc/wireguard#
As indicated above, the firewall rules are successfully updated via ufw and 
will be enabled automatically upon system startup.
Next, you need to enable IP forwarding, so that the WireGuard VPN server 
can relay traffic between connected peers, as well as route traffic as a default 
gateway. Open /etc/sysctl.conf and uncomment the following line.

net.ipv4.ip_forward= 1
If you are assigning IPv6 addresses to WireGuard peers, uncomment the 
following line instead.
net.ipv6.conf.all.forwarding=1
Reload updated syscti.conf with:
I_____ ^^Ixterm
lb_ # sysctl -p /etc/sysctl.conf
Auto-start WireGuard VPN Service
If you are running WireGuard VPN on a remote VPS instance, you may want 
to enable WireGuard to auto-start upon VPS boot.
Luckily, WireGuard is already integrated with systemd on Ubuntu 20.04. Thus, 
you can manage WireGuard service with systemctl command (instead of 
running wg-quick manually).
If you have already started WireGuard manually with wg-quick, first you need 
to stop it before using systemctl:
xterm
lb_ # wg-quick down wg0
_L~_JThen you can easily start and stop WireGuard with systemctl:
xterm
lb_ # systemctl start [email protected]
# systemctl stop [email protected]
_J^HNote that wg0 specified in the command should be the name of your 
WireGuard virtual interface.
To enable auto-start of WireGuard on boot:

term
# systemctl enable [email protected]
L^JYou can monitor the status WireGuard with:
I______^^Ixterm
# systemctl status [email protected]
rootjubuntu:/etc/wireguard#
robtgubuntu:/etc/wireguard#| systeFictl status wg-quick^ig8]
|wg
*qukk^g0,
 service ■ WireGuard via wg-qutck(8) for wg0
Loaded: loaded (/lib/systend/systen/wg-quick^.service; enabled; vendor preset: enabled)
Active: active (exited) since Sat 2021
*01-16
 13:06:12 EST; 14s ago
Docs: nan:wg-quick(8)
nan:wg(8)
https://www.wireguard.coA/
https://www.wireguard.con/quickstart/
https://git.zx2c4.con/wireguard-tools/about/src/nan/wg.8
Process: 5818 ExecStart=/usr/bin/wg-quick up wgO (code^exited, status=O/SUCCESS)
Main PTD: 6818 (^exited, status=O/SUCCESS)
Jan 16 18:06:12 ubuntu systepid[l]: Starting WireGuard via wg-quick(8) for wgO..,
Jan 16 18:06:12 ubuntu wg-quick[5818]: [fl ip link add wgfi type wireguard
Jan 16 18:06:12 ubuntu wg-quick[5818]: [fl wg setconf wgO /dev/fd/63
Jan 16 18:06:12 ubuntu wg-quick[5818]: [fl ip -4 address add 10,0.0.1/24 dev wgO
Jan 16 18:06:12 ubuntu wg-quick[5818]: [fl ip link set pitu 1426 up dev wgO
Jan 16 18:06:12 ubuntu wg-quick[5818]: [fl iptables -A FORWARD -i wgO -j ACCEPT; iptables -t nat -A POSTROUTING -o ensJ -j MASQUERADE
Jan 16 18:06:12 ubuntu systend[l]: Finished WireGuard via wg-quick(8) for wgO,
rootjubuntu:/etc/wireguard#
Now your WireGuard VPN server is ready for use!
Summary

In this tutorial, I describe how to set up WireGuard VPN in Ubuntu Server 
20.04 environment. As you can see, WireGuard server-side setup is 
extremely simple, mainly due to the fact that WireGuard (both the kernel 
module and userland tools) has already been adopted by mainline Linux 
distributions. There are various UI frontends for WireGuard in the works, and 
the recent verion (v1.16) of NetworkManager has started to provide native 
support for WireGuard as well. This tutorial provides a detailed guide on 
setting up WireGuard VPN client with NetworkManager GUI.
Linux Tutorials
How to look up the geographic 
location of an IP address from the 
command line
If you want to find out where a given IP address is physically located on 
earth, there are quite a few online GeoIP lookup services you can try (e.g. 
geoiptool.com). These online services are mostly powered by freely available 
GeoIP databases such as those from MaxMind. Besides using such web­
based services, there are different ways to query the GeoIP databases, notably 
via the Linux command line.
In this tutorial, I am going to describe how to geolocate an IP address from 
the command line in Linux.
Method One: geoiplookup
The first method is to use geoiplookup tool which is a command-line client for 
MaxMind's GeoIP databases. geoiplookup allows you to look up the geography 
or network information of an IP address (or hostname). You can install the 
tool (along with the free GeoIP database used by the tool) as follows.
To install geoiplookup on Ubuntu, Debian or Linux Mint:

xterm
T $ sudo apt-get install geoip-bin
I Ito install geoipiookup on Fedora: 
xterm
T $ sudo yum install geoip
__ ^^B'lo install geoiplookup on CentOS/RHEL, first enable EPEL 
repository, and then use yum command:
xterm
lb_ $ sudo yum install geoip
The default installation of geoiplookup comes with geoip.dat database file which 
is located in /usr/share/geoip. With this database, you can look up the country 
information only.
term
$ geoiplookup 23.66.166.151
t—JceolP Country Edition: US, United States
You can download additional GeoIP databases from MaxMind, which give 
you more detailed information about lP addresses beyond country info. You 
can also download more up-to-date geolp.dat from the site. This is 
recommended because geolp.dat may have already been outdated by the time 
you install it from Linux repositories. The GeolP databases available on 
MaxMind website are updated every month.
To install additional GeolP databases from MaxMind, do the following. You 
may want to set up a monthly cronjob to automate this process.
I___ xterm
$ wget
http://geolite.maxmind.com/download/geoip/database/GeoL 

iteCountry/GeoIP.dat.gz
$ wget
http://geolite.maxmind.com/download/geoip/database/GeoL iteCity.dat.gz 
$ wget
http://download.maxmind.com/download/geoip/database/asn
um/GeoIPASNum.dat.gz
$ gunzip GeoIP.dat.gz
$ gunzip GeoIPASNum.dat.gz
$ gunzip GeoLiteCity.dat.gz
$ sudo cp GeoIP.dat GeoIPASNum.dat GeoLiteCity.dat /usr/share/GeoIP/
Now if you re-run geoiplookup, you will see the additional AS number 
information of an IP address. This basically tells you which administrative 
domain the IP address belongs to.
term
■l _$ geoiplookup 128.112.119.209
__ ________GeoIP Country Edition: US, United States
GeoIP ASNum Edition: AS88 Princeton University
When run without any parameter, geoiplookup tool automatically uses geoip.dat 
and geoipasnum.dat only, but not use geolitecity.dat. The latter can give you city­
level information.
To obtain city-level geolocation information, explicitly tell geoiplookup to use 
GeoLiteCity.dat database.
lb_ $ geoiplookup -f /usr/share/GeoIP/GeoLiteCity.dat 23.66.166.151
__ I^H^^MGeoIP City Edition, Rev 1: US, MA, Cambridge, 02142, 
42.362598,

-71.084297, 506, 617
The output includes state, city, zipcode, latitude and longitude. The accuracy 
of the inferred location varies across different countries and networks. For 
example, the geolocation result tends to be more accurate for broadband IP 
addresses, but not as accurate for mobile networks.
Method Two: ipinfo.io
If you want to avoid the hassle of installing and updating GeoIP databases, 
you can try ipinfo.io online service. Unlike other services, ipinfo.io provides 
JSON-based geolocation API, so you can easily look up geolocation from the 
command line, using tools like curl.
term
$ curl ipinfo.io/23.66.166.151
$ curl ipinfo.io/23.66.166.151
{■"
"ip": "23.66.166.151",
"hostname": "a23-66-166-151.deploy.static.akairiaitechnologies.com",
"city"1: "Cambridge",
"region": "Massachusetts",
"country": "US’,
"loc": "42.3626,-71.0843",
"org": "AS16625 Akamai Technologies European AS",
"postal": "02142"
Note that the access to their API is rate-limited at 1,000 API requests per day.

Linux Tutorials
How to run a command on multiple 
servers at once
If you maintain multiple Linux servers or VPS instances, there are cases 
where you want to run the same command(s) on all the servers. For example, 
you may want to install/upgrade packages, patch the kernel, and update 
configurations, etc. It will be a tedious job if you have to log in to each server 
and run the same commands manually. This post is about an administrative 
tool that allows you to run the same commands on many different 
machines at once.
ClusterSSH enables you to make the same change on multiple hosts at once. 
It provides a special console interface where anything you type into the 
console is automatically sent to as many hosts as you want.
Install ClusterSSH on Linux
For Ubuntu, Debian or Linux Mint:
To install ClusterSSH on Ubuntu, Debian or Linux Mint:
term
T $ sudo apt-get install clusterssh
For CentOS or RHEL:
To install ClusterSSH on CentOS or RHEL, first you need to set up EPEL 
repository, and then run the following.
xterm
$ sudo yum install clusterssh

For Fedora:
To install ClusterSSH on Fedora, simply run:
xterm
Bl_ $ sudo yum install clusterssh
Configure ClusterSSH
After installation, the first step is to define a cluster of hosts that you want to 
run commands on. To do that, create a system-wide ClusterSSH 
configuration file as follows.
term
sudo vi /etc/clusters
$
clusters = my_cluster my_cluster2 
my_cluster = host1 host2 host3 host4 
my_cluster2 = host5 host6
If you want a user-specific ClusterSSH configuration, simply use ~/.csshrc 
instead of /etc/clusters. In the above example configuration, I define two 
clusters: my_cluster consisting of four hosts, and my_cluster2 with two hosts. A 
cluster is a group of hosts which you want to log in to, and run commands on.
When you launch ClusterSSH with any user-defined cluster, it will use ssh to 
log in to individual hosts in the cluster, and run any user-typed commands on 
the hosts.

Launch ClusterSSH
To launch ClusterSSH, run cssh command as follows.
term
T $ cssh -l dev my_cluster
_J^HIn the above, dev is a login ID for all the hosts in the cluster, and 
my_cluster is the cluster name.
If you want, you can specify individual hostnames instead of the cluster 
name.
term
lb_ $ cssh -l dev host1 host2 host3
Once cssh command is executed, it will pop up XTerm windows for individual 
hosts, as well as a small window labeled "cssh [2]", which is ClusterSSH 
console window. Whatever you type in the console window will 
simultaneously appear in the XTerm windows of individual hosts.
Essentially, you control all XTerm windows via the single console window.
If you want to run some commands to a specific XTerm window, you can 
simply switch focus to the Xterm window, and type the commands as you 
usually would.
The following screenshot shows ClusterSSH in action, where there are five 
hosts in the cluster, and the console window in the upper left corner is where 
you are supposed to type the commands to run on all five hosts.

09® CSSH:jppfn1
devQjppfnl's password;
Welcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-generic-pae 1686)
CSSH [5]
* Documentation: https://help.ubuntu.com/
Last login: Tue Apr 9 03:59:30 2013 from 172.1G.147.139 
dev@jppfnl:~$ uname -a
Linux jppfnl 3.2.0-29-generic-pae #46-Ubuntu SMP Fri Jul 27 17:25:43 UTC 2012 16
86 1686 i386 GNU/Linux
devQjppfnl:^ date
Tue Apr 9 04:15:55 PDT 2013
dev@jppfnl:~$ 0
OO® CSSH:jppfn2
dev@jppfn2's password:
Welcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-generic-pae 1686)
OO® CSSH:jppfn3
dev@jppfn3's password:
Welcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-generic-pae 1686)
* Documentation: https://help.ubuntu.com/
* Documentation: https://help.ubuntu.com/
Last login: Tue Apr 9 02:49:30 2013 from 172.16.147.139
Last login: Tue Apr 9 02:48:48 2013 from 172.16.147.139
dev@jppfn2:~$ uname -a
dev@jppfn3:~$ uname -a
Linux jppfn2 3.2.0-29-generic-pae #46-Ubuntu SMP Fri Jul 27 17:25:43 U1 Linux jPpfn3 3.2.0-29-generic-pae #46-Ubuntu SMP Fri Jul 27 17:25:43 UTC 2012 i6
86 1686 1386 GNU/Linux
86 1686 1386 GNU/Linux
dev@jppfn2:~$ date
dev@jppfn3:~$ date
Tue Apr 9 04:15:56 PDT 2013
Tue Apr 9 04:15:08 PDT 2013
dev@jppfn2:~$ 0
dev@jppfn3:~$ 0
OO® CSSH:jppfn4
OO® CSSHzjppfnS
dev@jppfn4's password: 
dev@jppfn5's password:
Welcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-generic-pae 1686) Welcome to Ubuntu 12.04.1 LTS (GNU/Linux 3.2.0-29-generic-pae i686)
* Documentation: https://help.ubuntu.com/
Last login: Tue Apr 9 02:49:51 2013 from 172.16.147.139 
dev@jppfn5:~$ uname -a
* Documentation: https://help.ubuntu.com/
Last login: Tue Apr 9 02:49:43 2013 from 172.16.147.139 
dev@jppfn4:~$ uname -a 
...
Linux jppfn4 3.2,0-29-generic-pae #46-Ubuntu SMP Fri Jul 27 17:25:43 UlLinux jppfn5 3.2.0-29-generic-pae #46-Ubuntu SMP Fri Jul 27 17:25:43 UTC 2012 16 
86 1686 1386 GNU/Linux 
............. "" ’......
dev@jppfn4:6'$ date 
Tue Apr 9 04:15:55 PDT 2013 
devQjppfM:^ 0
86 1686 1386 GNU/Linux
dev@jppfn5:~$ date
Tue Apr 9 04:15:56 PDT 2013
deu@jppfn5:~$ 0
Linux Tutorials
How to enable VNC remote console 
in VMware Player
If you want to access a guest VM console of VMware Player running on a 
remote host, you can use a VNC-based remote console. VMware Player 5 has 
a built-in VNC support available. Then you can use any VNC client to access 
a guest VM console running on a remote host.
In this tutorial, I will show you how to enable and access VNC remote 
console in VMware Player. There are two different ways to enable VNC on 
guest VMs created by VMware Player.

The first method is to edit virtual machine settings of your VM via VMware 
Player GUI. Click on Options tab, and select "vnc Connections" setting. Then you 
can enable VNC connections, and specify VNC password/port number, as 
shown below.
The second method to enable VNC is to edit *.vmx  file of your VM manually. 
This method is useful when you cannot use VMware Player GUI for any 
reason. Edit *.vmx  file of your VM to add the following information. Choose a 
unique port number for each concurrently running VM.
RemoteDisplay.vnc.enabled = "TRUE"
RemoteDisplay.vnc.port = "5950"
RemoteDisplay.vnc.password = "your_password"

Once you enable VNC for your VM, and power on the VM, VNC server will 
start running on the specified port number.
Install VNC Client on Linux
To access remote console of the VM, launch VNC client from any host, and 
connect to the VNC server. Here I use Vinagre, a VNC client for GNOME 
Desktop.
To install Vinagre on Ubuntu or Debian:
term
$ sudo apt-get install vinagre
To install Vinagre on CentOS, Fedora or RHEL:
term
$ sudo yum install vinagre
Connect to VNC Remote Console of VMware VM
To launch Vinagre after installation, just run it:
$ vinagre
__^^H()nce Vinagre is launched, connect to the remote VNC server by 
providing VNC server host information (e.g., 192.168.1.10:5950), and clicking on 
Connect. Once connected to the VNC server, you will see the remote console of 
your VM as follows.

Linux Tutorials
How to browse and search API 
documentation offline on Linux

For a programmer, good API documentation is worth more than a library of 
programming textbooks. If you are an avid developer, you probably want to 
make your life easier by keeping all necessary API documentation right at 
your fingertips. Most IDEs provide brief tooltips or IDE-specific ways of 
incorporating API documentation. However, these are mostly IDE-specific, 
and availability and verbosity of documentation vary across languages.
S Cordova
O Corona
0 [css]
3 D3.js
4 [Dart]
O| Django]
Q Dojo Toolkit
& Drupal
<$- ElasticSearch
& Elixir
^4 Emacs Lisp
Q Ember.js
® Emmet.io
IH Erlang
4 Expresses
S3 Expression Engine
> ExtJS
V Flask
□ Font Awesome
Q Foundation
< GLib
Go
Q Grails
@ Groovy
§ Grunt
B Haml
>X [Haskell]
0|html
4 Jade
® Jasmine
[jfej [java SE
g Java EE
EJavaFX
E JavaScript]
& Joomla
S jQuery)
C jQuery Mobile
® jQuery Ul
£3 Knockout.js
J Kobold2D
2aLaravel
F LaTeX
* Less
_ Lo-Dash
4*.
 Lua
M Marionette.js
□ Meteor
0 MomentJS
MongoDB
4 Mongoose
□ Mono
/“ MooTools
B| MySQL]
@ Nginx
4 Node.js
fl NumPy
>. OCaml
cb OpenCV
GL OpenGL
Wi Perl]
This is when Zeal can help you. Zeal is an offline API documentation 
browser. The idea of Zeal comes from Dash, MacOS X documentation 
browser, which comes with 130+ curated document sets (docsets) of popular 
programming languages, scripts, and frameworks. Zeal can access the same 
docsets contributed by Dash. Each docset contains detailed API definition, 
code snippets, and user-contributed notes.
In this tutorial, I will describe how set up Zeal to browse and search API 
documentation offline on Linux platforms.
Install Zeal on Ubuntu

On Ubuntu 14.04 or earlier:
Installing Zeal on Ubuntu LTS is a breeze thanks to its PPA repository. 
xterm
$ sudo add-apt-repository ppa:jerzy-kozera/zeal-ppa $ sudo apt-get update 
$ sudo apt-get install zeal
On Ubuntu 14.10 or later:
At this point, the official PPA has not been updated for the latest Ubuntu
14.10. Thus install Zeal by building it from the source: 
xterm
$ sudo apt-get install git qt5-default libgtk2.0-dev libqt5webkit5-dev 
libappindicator-dev qtbase5-privatedev libxcb-keysyms1-dev
$ git clone https://github.com/jkozera/zeal.git $ cd zeal/zeal
$ qmake
$ make
$ sudo make install
Install Zeal on Debian
First, install QT5 on Debian by using the official QT installer.
Next, install other prerequisites before building Zeal from the source.
xterm
$ sudo apt-get install g++ libxcb-keysyms1-dev zlib1gdev libx11-dev 
libxslt1-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev libgl1- 
mesa-dev 
__B^^BFinally, download the source, and build/install Zeal from the source 
as follows.
term

$ git clone https://github.com/jkozera/zeal.git $ cd zeal/zeal
$ qmake
$ make
$ sudo make install
Install Zeal on Fedora
Install prerequisites for building Zeal with Qt5.
xterm
T__ $ sudo yum install qt5-qtwebkit-devel qt5-qtbase-devel xcb-util-
keysyms-devel bsdtar
_J^HClone the source from the official repository, and build Zeal from 
the source.
B__xterm
$ git clone https://github.com/jkozera/zeal.git 
$ cd zeal/zeal
$ qmake-qt5
$ make
$ sudo make install
Install Zeal on Arch Linux
Installation on Arch Linux is straightforward with AUR.
Launch Zeal on Linux

To launch Zeal, simply run:
xterm
T__ $ zeal
During the first-time launch, you may get a warning message of hotkey 
binding conflicts. To get around the warning, you can disable the Zeal's 
default hotkey (i.e., Alt+Space). To do so, go to Edit -+ Options, and select Clear in 
Hotkey option. If you want, you can set up a custom shortcut for Zeal.
One-time set up for Zeal is to download API docsets, which will be used to 
look up API definitions offline. As mentioned earlier, these docsets are 
contributed as part of MacOS X Dash project. Zeal is able to access the same 
API documents.

Go to Edit -+ Options, choose Docsets tab, and click on Download button.
Then you will see a list of available docsets as follows.

Choose docsets which you want to access offline, and click Download button 
again. Once chosen docsets are downloaded successfully, you will see them 
listed under Docsets as follows. By default, the chosen docsets will be stored in 
~/.local/share/zeal/docsets.

Now you are ready to access API documentation. Simply start typing the API 
you want to look up in the left search bar. The API definition will instantly 
show up in the right sidebar.

Linux Tutorials
How to set up a transparent proxy 
on Linux
A transparent proxy sits between clients and the Internet, acting as a gateway 
for the clients. It is called transparent because clients are not required to 
configure anything for the proxy. A transparent proxy is useful when it is not 
possible or desirable to modify client configuration, ant yet it's still necessary 
for client traffic to go through the proxy.
If you need to set up a transparent proxy on Linux, one of the easiest ways 
is to use Squid, open-source proxy server software. While Squid has rich 
feature sets as a general web-caching proxy server, this tutorial will not 
discuss all those features. Instead, it focuses on describing how to configure 
a transparent proxy with Squid.

Install Squid on Linux
For Ubuntu or Debian:
term
$ sudo apt-get install squid
For CentOS, RHEL or Fedora:
term
$ sudo yum install squid
Configure Squid
The above step will install Squid version 3 ( squid3) on your system, and create 
a default Squid configuration file in /etc/squid3. Squid is set to start 
automatically upon boot. To configure Squid as a transparent proxy, modify 
its configuration file, and restart it as follows.
term
$ sudo vi /etc/squid3/squid.conf 
acl localhost src 100.100.100.0/24 ::1
http_port 3128 transparent
cache_peer 100.100.100.10 parent 8000 0 no-query default
cache_effective_user proxy
cache_effective_group proxy

T__ $ sudo /etc/init.d/squid3 restart
The line starting with acl specifies the clients which are allowed to use the 
proxy. In this case, those whose IP address belongs to 100.100.100.0/24 are added 
to the proxy's white list. The line starting with cache_peer is optional, and 
needed only when your network is behind an upstream proxy or firewall. 
Essentially this line declares the upstream proxy (i.e.,
100.100.100.10: 8000) as a parent proxy to connect to in order to reach any 
external networks.
Next, you need to set up iptables rules so that any HTTP traffic (e.g., destined 
to port 80) is routed through Squid. For that, you can run the following script 
called proxy.sh.
#!/bin/sh
# squid proxy's IP address (which is attached to eth0)
SQUID_SERVER='ifconfig ethO | sed -ne 's/.
*inet
 addr:([A ]*).*/1/p''
# interface connected to WAN INTERNET="eth0"
# interface connected to LAN LAN_IN="eth1"
# Squid port
SQUID_PORT="3128"
# clean old firewall iptables -F 
iptables -X

iptables -t nat -F iptables -t nat -X iptables -t mangle -F iptables -t mangle -X
# load iptables modules for NAT masquerade and IP conntrack modprobe 
ip_conntrack
modprobe ip_conntrack_ftp
# define necessary redirection for incoming HTTP traffic (e.g., 80) iptables -t 
nat -A PREROUTING -i $LAN_IN -p tcp --dport 80 -j REDIRECT --to-port 
$SQUID_PORT


# forward locally generated http traffic to Squid
iptables -t nat -A OUTPUT -p tcp --dport 80 -m owner --uid-owner proxy
- j ACCEPT
iptables -t nat -A OUTPUT -p tcp --dport 80 -j REDIRECT --to-ports 
$SQUID_PORT
# forward the rest of non-HTTP traffic
iptables --table nat --append POSTROUTING --out-interface $INTERNET
- j MASQUERADE
iptables --append FORWARD --in-interface $INTERNET -j ACCEPT
# enable IP forwarding for proxy
Once you have set up iptables rules using this script, you can save the current 
iptables rules permanently, so that you don't need to re-run the script.

Finally, those clients who wish to use the transparent proxy should specify 
the IP address of the transparent proxy as their default gateway.
If you would like more advanced setup for your transparent proxy such as 
web filtering, refer to this tutorial.
Linux Tutorials
What is good reference 
management software on Linux
Have you ever written a paper so long that you thought you would never see 
the end of it? If so, you know that the worst part is not dedicating hours on it, 
but rather that once you are done, you still have to order and format your 
references into a structured convention-following bibliography. Hopefully for 
you, Linux has the solution: bibliography/reference management tools. Using 
the power of BibTex, these programs can help you import your citation 
sources, and spit out a structured bibliography. Here is a non-exhaustive list 
of open-source reference management software on Linux.
1. Zotero

Surely the most famous tool for collecting references, Zotero is known for 
being a browser extension. However, there also exists a convenient Linux 
stand alone program. Among its biggest advantages, Zotero is easy to use, 
and can be coupled with LibreOffice or other text editors to manage the 
bibliography of documents. I personally appreciate the interface and the 
plugin manager. However, Zotero is quickly limited if you have a lot of needs 
about your bibliography.
2. JabRef

JabRef is one of the most advanced tools out there for citation management. 
You can import from a plethora of format, lookup entries from external 
databases (like Google Scholar), and export straight to your favorite editor. 
JabRef integrates your environment nicely, and can even support plugins. 
And as a final touch, JabRef can connect to your own SQL database. The 
only downside to all of this is of course the learning curve.
3. KBibTex

For KDE adepts, the desktop environment has its own dedicated bibliography 
manager called KBibTex. And as you might expect from a program of this 
caliber, the promised quality is delivered. The software is highly 
customizable, from the shortcuts to the behavior and appearance. It is easy to 
find duplicates, to preview the results, and to export directly to a LaTeX 
editor. But the best feature in my opinion is the integration of Bibsonomy, 
Google Scholar, and even your Zotero account. The only downside is that the 
interface seems a bit cluttered at first. Hopefully spending enough time in the 
settings should fix that.
4. Bibfilex

Capable of running in both Gtk and Qt environment, Bibfilex is a user 
friendly bibliography management tool based on Biblatex. Less advanced 
than JabRef or KBibTex, it is fast and lightweight. Definitely a smart choice 
for making a bibliography quickly without thinking too much. The interface 
is slick and reflects just the necessary functions. I give it extra credits for the 
complete manual that you can get from the official download page.
5. Pybliographer

As indicated by its name, Pybliographer is a non-graphical tool for 
bibliography management written in Python. I personally like to use 
Pybliographic as the graphical front-end. The interface is extremely clear and 
minimalist. If you just have a few references to export and don't really have 
time to learn how to use an extensive piece of software, Pybliographer is the 
place to go. A bit like Bibfilex, the intent is on user-friendliness and quick 
use.
6. Referencer

Probably my biggest surprise when doing this list, Referencer is really 
appealing to the eye. Capable of integrating itself perfectly with Gnome, it 
can find and import your documents, look up their reference on the web, and 
export to
Linux Tutorials
How to fix "X11 forwarding request 
failed on channel 0"
Question: When I tried to SSH to a remote host with X11 forwarding option, 
I got "X11 forwarding request failed on channel 0" error after logging in.
Why am I getting this error, and how can I fix this problem?

$
$ ssh -X dev@dvs 
dev@dvs's password:
Xll forwarding request failed on channel O
Welcome to Ubuntu 13.10 (GNU/Linux 3.11.0-12-generic x86_64)
* Documentation: https://help.ubuntu.com/
System information as of Thu May 29 16:14:58 EDT 2014
System load: 0.02 
Processes: 
124
Usage of /: 
44.4% of 7.66GB Users logged in: 
1
Memory usage: 29% 
IP address for eth2: 135.104.54.207
Swap usage: 0%
Graph this data and manage this system at:
https://landscape.canonical.com/
New release '14.04' available.
Run 'do -release-upgrade' to upgrade to it.
First of all, we assume that you already enabled X11 forwarding over SSH 
properly.
If you are getting "X11 forwarding request failed on channel 0" message 
upon SSH login, there could be several reasons. Solutions vary as well, as 
desribed below.
Solution One
For security reason, OpenSSH server, by default, binds X11 forwarding 
server to the local loopback address, and sets the hostname in display 
environment variable to localhost. Under this setup, some X11 clients cannot 
handle X11 forwarding properly, which causes the reported error. To fix this 
problem, add the following line in /etc/ssh/sshd_config file, which will let X11 
forwarding server bind on the wild card address.

term
__ $ sudo vi /etc/ssh/sshd_config
XllForwarding yes
X11UseLocalhost no
Restart SSH server to activate the change:
xterm
Bl_ $ sudo /etc/init.d/ssh restart
_ I^Bor:
xterm
T $ sudo systemctl restart ssh.service
_ I^Bor:
xterm
T__ $ sudo service sshd restart
Solution Two
The broken X11 forwarding error may also happen if the remote host where 
SSH server is running has IPv6 disabled. To fix the error in this case, open
/etc/ssh/sshd_config file, and uncomment addressfamily all (if any). Then add the 
following line. This will force SSH server to use IPv4 only, but not IPv6, so 
that SSH server complies with the IPv4-only server settings.
term
__ $ sudo vi /etc/ssh/sshd_config
AddressFamily inet

Again, restart SSH server to finalize the change.
Linux Tutorials
How to disable Flash plugin on 
Google Chrome browser
Question: When I browse websites on Google Chrome web browser, 
sometimes the Chrome tab that I am on freezes or is significantly lagging for 
a while, and then at the top of the tab it says: "A plugin (Shockwave Flash) 
isn't responding." Apparently the Google Chrome's Flash plugin is 
misbehaving. How can I disable Flash plugin on Google Chrome browser?
Unlike other web browsers, Google Chrome has Flash Player built-in, which 
is known as "Pepper Flash Player." Having the integrated Flash Player means 
that Google Chrome does not rely on an external Flash installation. Naturally, 
there is no need for updating to the latest Flash as Google Chrome 
automatically takes care of it for you. A downside is that any bug or 
incompatibility issues with the built-in Pepper Flash player can cause Google 
Chrome browser to freeze or crash.

When you see the Flash player freeze in Google Chrome, make sure that you 
run the latest Google Chrome as outdated Chrome may have known issues. 
Another potential culprit for choppy Flash performance is the incompatibility 
between Flash player and your graphics card driver. Make sure that you have 
the latest driver for your graphics card.
If none of these solves the freezing Flash issue on Google Chrome, you can 
consider disabling the built-in Flash player plugin.
Disable Flash Plugin on Google Chrome
To disable Flash plugin, type " chrome://plugins" at the browser address bar. This 
will show a list of installed plugins.

If you click on Details link at the top right, you can see the details of each 
plugin. Go to "Adobe Flash Player", and click on Disable link.

Once the Flash plugin is disabled, it will be grayed out as follows.

F| Plugins xl
H C ii chrome://plugins
Widevine Content Decryption Module-Version: 1.4.8.824
Enables Widevine licenses for playback of HTML audio/video content, (version: 1.4.8.824)
Name: 
Widevine Content Decryption Module
Description: Enables Widevine licenses for playback of HTML audio/video content, (version: 1.4.8.824)
Version: 
1.4.8.824
Location: 
/opt/google/chrome/libwidevinecdmadapter.so
Type: 
PPAPI (out-of-process) 
hicshl
Disable
MIME types: MIME type_ _ _ _ _ _ _ _ _ _ _ _ _ Description_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ File extensions
application/x-ppapi-widevine-cdm Widevine Content Decryption Module , /
Disable Always allowed to run
Adobe Flash Player - Version: 20.0.0.228 (Disabled)
Shockwave Flash 20.0 rO
Name: Shockwave Flash
Description: Shockwave Flash 20.0 rO
Version: 20.0.0.228
Location: /opt/google/chrome/PepperFlash/libpepflashplayer.so
Type: 
PPAPI (out-of-process)
Enable
MIME types: MIME type Description 
File extensions
application/x-shockwave-flash Shockwave Flash 
,swf 
application/futuresplash FutureSplash Player ,sp|

Selectively Disable/Enable Flash Content on Google 
Chrome
If you sometimes need to see Flash content on Google Chrome, disabling 
Flash plugin altogether may not be ideal for you. In that case, you can simply 
turn off auto execution of plugin content. In that case, Google Chrome will 
by default not run any plugin content, and "you" decide when to run plugin 
content. Thus, you can selectively enable Flash content whenever you want.
For this, go to Settings and click on "Content settings" under Privacy. Under Plugins 
section, activate "Let me choose when to run plugin content" option.
Once the setting is saved, Google Chrome will not load any Flash content. If 
the current webpage has any plugin content, Google Chrome will show X 
mark at the far right side of the address bar, indicating that plugins are 
blocked on this page. You can click on it to temporarily enable blocked 
plugins.

C D ™odulo.miHW^ Plugins were blocked on this page
Home About LlnuxQ&A List WrlteforUs Advertise Contact
Xmodulo
Linux FAQ$( tips and tutorials
The following plugins were blocked on this page: Learn more 
x Adobe Flash Player
Always allow plugins on http://xmodulo.com
* Continue blocking plugins
Run all plugins this time
Howto enable Software Collections (SCL) on C 
Dm
Last updated on December 8,2015 Authored by Dan Nanni 1 Comment - - - - - - - - - - - - - - - -™- - - - - - - ™ibe
Red Hat Enterprise Linux (RHEL) and its community fork, CentOS, offer 10-year life cycle, meaning 
that each version of RHEL/CentOS is updated with security patches for up to 10 years. While such 
long life cycle guarantees much needed system compatibility and reliability for enterprise users, a 
downside is that core applications and run-time environments grow antiquated as the underlying 
RHEL/CentOS version becomes close to end-of-life (EOF). For example, CentOS 6.5, whose EOL is 
dated to November 30th 2020, comes with python 2.6.6 and MySQL 5,1.73, which are already pretty 
old by today's standard.
^stable
Signupnow
APOWER 
TOOL FOR 
APOWER 
LANGUAGE
Crass-platform 
C/C++ IDE
On the other hand, attempting to manually upgrade development toolchains and run-time 
environments on RHEL/CentOS may potentially break your system unless all dependencies are 
resolved correctly. Under normal circumstances, manual upgrade is not recommended unless you 
know what you are doing.
The Software Collections (SCL) repository came into being to help with RHEL/CentOS users in this 
situation. The SCL is created to provide RHEL/CentOS users with a means to easily and safely install 
and use multiple (and potentially more recent) versions of applications and run-time environments 
"without" messing up the existing system. This is in contrast to other third party repositories which 
could cause conflicts among installed packages.
The lotoct CPI nfforr
Advertise
Here
Related Linux FAQs

Optionally you can also set up plugin exception rules, by which you can 
enable/disable plugins for specific websites. Simply click on "Manage exceptions" 
button to create exception rules.

<f Settings -Plugins
Dan - 0 X
(■ CO chrome^’settingskontentfxceptionsftptugins
QHoO =
x
Plugin exceptions

Linux Tutorials
How to assign multiple IP addresses 
to one network interface on CentOS
The practice of configuring multiple IP addresses on a particular network 
interface is called IP aliasing. IP aliasing is useful when you set up multiple 
sites on virtual web hosting on a single interface, or maintain multiple 
connections to a network each of which serves a different purpose. You can 
assign multiple IP addresses to one network interface from a single subnet or 
completely different ones.
All existing Linux distributions including CentOS supports IP aliasing. Here 
is how to bind multiple IP addresses to a single network interface on CentOS.
If you would like to set up IP aliasing on the fly, there are two ways to do it. 
One way is to use ifconfig, and the other method is to use ip command. Using 
these two methods, let me show you how to add two extra IP addresses to 
eth0.
Set up IP Aliasing with ifconfig
To use the first method:
term
$ sudo ifconfig eth0:1 192.168.10.10 netmask
255.255.255.0 up
$ sudo ifconfig eth0:2 192.168.10.15 netmask
255.255.255.0 up
Set up IP Aliasing with ip

To use the second method:
$
term
sudo ip addr add 192.168.10.10/24 dev eth0
$ sudo ip addr add 192.168.10.15/24 dev eth0
View IP Aliases Defined by ifconfig or ip
To view a list of all IP addresses assigned to eth0 by using either method, run 
the following command.
xterm
lb_ $ sudo ip addr list dev eth0
2: eth0: mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000 link/ether 
00:0c:29:5c:86:f4 brd ff:ff:ff:ff:ff:ff
inet 192.168.91.128/24 brd 192.168.91.255 scope global eth0 inet
192.168.91.10/24 scope global secondary eth0
inet 192.168.91.20/24 scope global secondary eth0
inet6 fe80::20c:29ff:fe5c:86f4/64 scope link
valid_lft forever preferred_lft forever

If you used ifconfig to create IP aliases, you can also use the same command to 
view them.
xterm
H__ $ ifconfig -a
eth0 Link encap:Ethernet HWaddr 00:0C:29:5C:86:F4 inet 
addr:192.168.91.128 Bcast:192.168.91.255 Mask:255.255.255.0
inet6 addr: fe80::20c:29ff:fe5c:86f4/64 Scope:Link UP BROADCAST 
RUNNING MULTICAST MTU:1500 Metric:1 RX packets:22 errors:0 
dropped:0 overruns:0 frame:0 TX packets:102 errors:0 dropped:0 overruns:0 
carrier:0 collisions:0 txqueuelen:1000
RX bytes:3869 (3.7 KiB) TX bytes:18172 (17.7 KiB) Interrupt:19 Base 
address:0x2000 
eth0:1 Link encap:Ethernet HWaddr 00:0C:29:5C:86:F4 inet 
addr:192.168.91.10 Bcast:192.168.91.255
Mask:255.255.255.0
UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1
Interrupt:19 Base address:0x2000
eth0:2 Link encap:Ethernet HWaddr 00:0C:29:5C:86:F4 inet 
addr:192.168.91.20 Bcast:192.168.91.255
Mask:255.255.255.0
UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1
Interrupt:19 Base address:0x2000
lo Link encap:Local Loopback
inet addr:127.0.0.1 Mask:255.0.0.0
inet6 addr: ::1/128 Scope:Host
UP LOOPBACK RUNNING MTU:16436 Metric:1
RX packets:8 errors:0 dropped:0 overruns:0 frame:0
TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:0
RX bytes:480 (480.0 b) TX bytes:480 (480.0 b)


If you would like to permanently assign multiple IP addresses to an 
interface, create corresponding configuration files in /etc/sysconfig/networkscripts.
xterm
T_ $ sudo vi /etc/sysconfig/network-scripts/ifcfg-eth0:1
DEVICE=eth0:1
BOOTPROTO=static
IPADDR=192.168.0.5
NETMASK=255.255.255.0
ONBOOT=yes
Once you have created as many configuration files as IP addresses to assign, 
restart network to activate IP aliasing.
xterm
T_ $ sudo /etc/init.d/network restart
Linux Tutorials
How to create .deb Debian package 
for Java web application
Suppose you have built a Java web application as a Java servlet. Now you 
want to create .deb Debian package for the Java web application, such that 

when you install the .deb package file, it will automatically install any 
prerequisite packages (e.g., Java and Tomcat web server), and deploy the 
Java servlet on Apache Tomcat.
I assume that you already built and exported a Java servlet into .war file. Then 
the Debian package will simply consist of a .war blob, and any dependency 
information. Here is guide on how to create .deb package from .war file.
First, install any necessary packages to build a Debian package.
xterm
Bl_ $ sudo apt-get install dh-make debhelper devscripts fakeroot
iw-Jcreate a directory for your application.
xterm
T $ mkdir myapp-0.1
_ I^^Next, copy your .war application file to the directory.
I_^lxterm
$ cp myapp.war myapp-0.1
$ cd myapp-0.1
Create skeleton files needed for packaging.
xterm
T $ dh_make -s --indep --createorig
Maintainer name : My Name 
Email-Address : [email protected] 
Date : Tue, 02 Apr 2020 11:33:28 -0400 
Package Name : myapp
Version : 0.1
License : blank
Type of Package : Independent
Hit to confirm:

Once you hit <enter>, it will create several skeleton files in myapp0.1/debian/ 
Since you are packaging a pre-built .war file, you don't need any build process 
within packaging rules. Remove any reference to makefile from debian/ruies.
term
T__ $ grep -v makefile debian/ruies > debian/rules.temp $ mv
debian/rules.temp debian/rules
The next step is to modify debian/install. Here you specify a list of files to 
install, as well as their respective installation directories. In our example, files 
to install correspond to .war file, and installation directory is the webapps 
directory of Tomcat. Assuming that .war file was built for Tomcat7 web 
server, you modify debian/install as follows.
xterm
lb_ $ echo myapp.war /var/lib/tomcat7/webapps >
debian/install
__ B^BModify debian/source/format as we will not be building a quilt 
based package.
term
T__ $ echo "1.0" > debian/source/format

Remove unnecessary example files.
term
__ $ rm debian/
*.ex
Modify control information of your package. Importantly, you need to 
specify tomcat7 as dependent package (in "Depends:" field). You don't need to 
specify Java as dependent since Java dependency is implicitly implied by 
adding tomcat7 which in turn depends on Java.
term
__ $ vi debian/control
Source: myapp
Section: unknown
Priority: extra
Maintainer: My Name
Build-Depends: debhelper (>= 8.0.0)
Standards-Version: 3.9.3
Homepage:
#Vcs-Git: git://git.debian.org/collab-maint/indoornav.git
#Vcs-Browser: http://git.debian.org/?p=collab
maint/indoornav.git;a=summary

Package: myapp 
Architecture: all 
Depends: tomcat7
Description: This is my web application.
This is a long description of my web application.
Finally, build the package.
xterm
lb_ $ debuild -us -uc
_J^HAfter building is completed, you will have .deb file generated in the 
parent directory.
xterm
T__ $ ls ../*deb

lmyapp_0.1 - 1_all • deb
Linux Tutorials
How to list all files contained in a
Debian package
Suppose you are trying to install a specific deb package, but want to know 
what files are contained in the package before actual installation. Or imagine 
that you are not sure what package owns a specific program binary or library 
you need.
In these cases, it will be useful if you can check the content of a specific 
deb package before installation. This article tells you how to do it.
In Ubuntu or Debian, there are two ways to show all files contained in a 
particular deb package without installing it.
Method One: apt-file
The first method is to use apt-file command-line tool.
To install and use apt-file on Ubuntu or Debian, do the following.
xterm
T_ $ sudo apt-get install apt-file
$ sudo apt-file update
__ ^^■iNote that you need to run "apt-file update" at first, in order to 
synchronize all available package contents from their sources.
Then to show files belonging to a specific deb package (e.g., tofrodos), simply 
run:
term

lb_ $ sudo apt-file show tofrodos
tofrodos: /usr/bin/fromdos
tofrodos: /usr/bin/todos
tofrodos: /usr/share/doc/tofrodos/NEWS.Debian.gz
tofrodos: /usr/share/doc/tofrodos/changelog.Debian.gz
tofrodos: /usr/share/doc/tofrodos/copyright
tofrodos: /usr/share/doc/tofrodos/readme.txt.gz
tofrodos: /usr/share/doc/tofrodos/tofrodos.html
tofrodos: /usr/share/man/man1/fromdos.1.gz
tofrodos: /usr/share/man/man1/todos.1.gz
The output will show the names of all contained files, along with their 
respective target installation directories.
Method Two: apt-get

The second method to show files in a Debian package is to use apt-get 
command, but with --download-only option.
xterm
T $ apt-get --download-only install tofrodos
__ ^^^Once you run the above command, a deb package called tofrodos will 
get downloaded to /var/cache/apt/archives, but will not be installed.
Then you can use dpkg command to list contents of the locally downloaded 
deb package.
I______^^Ixterm
$ dpkg -c 
$ dpkg -c
1build1_amd64.deb 
drwxr-xr-x root/root 0 2011-10-18 10:47 ./
drwxr-xr-x root/root 0 2011-10-18 10:47 ./usr/
drwxr-xr-x root/root 0 2011-10-18 10:47 ./usr/share/
drwxr-xr-x root/root 0 2011-10-18 10:47 ./usr/share/man/ drwxr-xr-x
root/root 0 2011-10-18 10:47 ./usr/share/man/man1/
- rw-r--r-- root/root 1677 2011-10-18 10:47
./usr/share/man/man1/fromdos.1.gz
drwxr-xr-x root/root 0 2011-10-18 10:47 ./usr/share/doc/ drwxr-xr-x root/root
0 2011-10-18 10:47
./usr/share/doc/tofrodos/
- rw-r--r-- root/root 391 2011-10-18 10:47
./usr/share/doc/tofrodos/NEWS.Debian.gz
- rw-r--r-- root/root 1406 2011-10-18 10:47
./usr/share/doc/tofrodos/copyright
- rw-r--r-- root/root 5832 2011-02-27 04:25
./usr/share/doc/tofrodos/tofrodos.html
- rw-r--r-- root/root 3746 2011-10-18 10:47
./usr/share/doc/tofrodos/changelog.Debian.gz
- rw-r--r-- root/root 5410 2011-02-27 04:25

./usr/share/doc/tofrodos/readme.txt.gz
drwxr-xr-x root/root 0 2011-10-18 10:47 ./usr/bin/
-rwxr-xr-x root/root 12792 2011-10-18 10:47 ./usr/bin/fromdos lrwxrwxrwx
root/root 0 2011-10-18 10:47
./usr/share/man/man1/todos.1.gz -> fromdos.1.gz
lrwxrwxrwx root/root 0 2011-10-18 10:47 ./usr/bin/todos -> fromdos

Linux Tutorials

How to browse the web 
anonymously with Google Chrome
Collecting your online activities is a lucrative business for someone else in 
today's Internet economy. As I speak, your VoIP traffic might be monitored 
by a secretive surveillance program in the name of national security. For 
whatever reason, you may want to conduct online business confidentially 
without being monitored by random eavesdroppers. To protect your online 
privacy from all these potential adversaries, what you want is online 
anonymity.
That's what Tor is about, which is a free software that enables you to 
completely hide your online communication via a large-scale anonymity 
network. Tor can be used for web browsers, VoIP, instant messaging, remote 
logins, etc.
In this tutorial, I will describe how to set up Tor so that you can browse the 
web anonymously inside Google Chrome.
First of all, don't get confused between anonymous browsing and 
incognito/private browsing natively supported by Google Chrome. The 
incognito browsing means that your browsing history (including cookies) is 
not recorded by web browser. Your online communication is still visible by 
external observers such as web servers and eavesdroppers.
Install Tor on Linux
Tor is available as packages in the native repositories of major Linux distros. 
However, it is not recommended to use those packages since they might not 
be patched with the latest stability and security fixes. Instead, use Tor's 
official package repository to install it.
For Debian, Ubuntu or Linux Mint:

To install Tor on Debian, Ubuntu or Linux Mint, use the following 
commands.
term
$ sudo add-apt-repository "deb
http://deb.torproject.org/torproject.org $(lsb_release
-sc) main"
$ gpg --keyserver keys.gnupg.net --recv 886DDD89 $ gpg --export
A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89 | sudo apt-key add 
$ sudo apt-get update
$ sudo apt-get install deb.torproject.org-keyring $ sudo apt-get install tor
__ I^Bonce installed, Tor will be set to auto-start upon boot.
To start Tor for the first time:
term
T__ $ sudo service tor start
For Fedora, CentOS or RHEL:
To install Tor on Fedora, CentOS or RHEL 6, first add the official yum 
repository information. In the repository configuration, replace 
distribution with el/6 (for CentOS/RHEL 6), fc/18 (for Fedora 18), or fc/19 (for 
Fedora 19), etc.
term
$ sudo vi /etc/yum.repos.d/tor.repo 
[tor]
name=Tor experimental repo
enabled=1
baseurl=http://deb.torproject.org/torproject.org/rpm/DISTRIBUTION/$ 
basearch/
gpgcheck=1
gpgkey=http://deb.torproject.org/torproject.org/rpm/RPM-GPG-

KEYtorproject.org.asc
[tor-source]
name=Tor experimental source repo
enabled=1
autorefresh=0
baseurl=http://deb.torproject.org/torproject.org/rpm/DISTRIBUTION/S
RPMS
gpgcheck=1
gpgkey=http://deb.torproject.org/torproject.org/rpm/RPM-GPG- 
KEYtorproject.org.asc

After that, run the following commands to install Tor, and launch it.
Optionally, set it to auto-start upon boot.
term
$ sudo yum install tor
$ sudo service tor start
$ sudo chkconfig tor on
Check if Tor is Running on Linux
First, check the daemon status of Tor: 
xterm
T__ $ sudo service tor status
* tor is running
Then check the default port number of Tor, and verify that Tor daemon is 
listening on that port.
xterm
$ cat /etc/tor/tor-tsocks.conf 
__ ^^Hserver = 127.0.0.1

Install and Configure Chrome Extension
To create Tor on/off button in Google Chrome, install Proxy SwitchySharp 
extension in Google Chrome.
Next, open up Options of Proxy SwitchySharp. Under Proxy Profiles tab, create 
a new profile called tor, and add the following manual configuration.
SOCKS Host: 127.0.0.1
Port: 9050 (which is the Tor port number that you found earlier)

SwitchySharp Options
V Switch Rules Network $ General Q ImporVExport
It» Proxy Profiles
Profile Details
Profile Name
Under General tab, click on Quick Switch checkbox, and populate Cycled Profiles area 
with Direct Connection and tor options. That
way, if you click on SwitchySharp icon multiple times, each of those two 
settings will be enabled in a rotated fashion. Don't forget to save the change. 
When you click on SwitchySharp icon on the top right corner of Google 
Chrome, and the icon stays highlighted, this implies that you are using Tor.
To verify that you are indeed connected to the Tor network, go to 
https://check.torproject.org. You will see the following screen if you are on 
the Tor network. To switch off Tor, simply click on SwitchySharp icon again 
to make it grayed out.

Linux Tutorials
How to apply image effects to 
pictures on Raspberry Pi
Like a common pocket camera which has a built-in function to add various 
effects on captured photos, Raspberry Pi camera board ("Pi Cam" module) 
can actually do the same. With the help of raspistill camera control options, we 
can add the image effects function like we have in a pocket camera.
There are three comman-line applications which can be utilized for taking 
videos or pictures with Pi Cam module, and one of them is the raspistill 
application. The raspistill tool offers various camera control options such as 
sharpness, contrast, brightness, saturation, ISO, exposure, automatic white 

balance (AWB), image effects.
In this article I will show how to apply exposure, AWB, and other image 
effects with raspistill while capturing pictures using raspi cam. To automate the 
process, I wrote a simple Python script which takes pictures and 
automatically applies a series of image effects to the pictures. The raspi cam 
documentation describes available types of the exposure, AWB, and image 
effects. In total, the raspi cam offers 16 types of image effects, 12 types of 
exposure, and 10 types of AWB values.
The simple Python script looks like the following.
#!/usb/bin/python
import os
import time
import subprocess
list_ex=['auto','night']
list_awb=['auto','cloud',flash']
list_ifx=
['blur','cartoon','colourswap','emboss','film','gpen','hatch','negative','oilpain
t','posterise','sketch','solarise','watercolour']
x=0
for ex in list_ex:
for awb in list_awb:
for ifx in list_ifx:
x=x+1
filename='img_'+ex+'_'+awb+'_'+ifx+'.jpg'
cmd='raspistill -o '+filename+' -n -t 1000 -ex '+ex+' -awb '+awb+' ifx '+ifx+' - 
w 640 -h 480'

pid=subprocess.call(cmd,shell=True)
print "["+str(x)+"]-"+ex+"_"+awb+"_"+ifx+".jpg" 
time.sleep(0.25)
print "End of image capture"
The Python script operates as follows. First, create three array/list variable for 
the exposure, AWB and image effects. In the example, we use 2 types of 
exposure, 3 types of AWB, and 13 types of image effects values. Then make 
nested loops for applying the value of the three variables that we have. Inside 
the nested loop, execute the raspistill application. We specify (1) the output 
filename; (2) exposure value; (3) AWB value; (4) image effect value; (5) the 
time to take a photo, which is set to 1 second; and (6) the size of the photo, 
which is set to 640x480px. This Python script will create 78 different 
versions of a captured photo with a combination of 2 types of exposure, 3 
types of AWB, and 13 types of image effects.

To execute the Python script, simply type: 
xterm
T $ python name_of_this_script.py 
__B^^Bhere is the first round of the sample result.
Bonus
For those who are more interested, there is another way to access and control 
the raspi cam besides raspistill. picamera a pure Python interface which provides 
APIs for accessing and controlling raspi cam, so that one can build a complex 

program for utilizing raspi cam according to their needs. If you are skilled at 
Python, picamera is a good feature-complete interface for implementing your 
raspi cam project. The picamera interface is included by default in the recent 
image of Raspbian. If your Raspberry Pi operating system is not new or not 
Raspbian, you can install it on your system as follows.
First, install pip on your system by following this guideline.
Then, install picamera as follows.
xterm
$ sudo pip install picamera
__ ^^^Refer to the official documentation on how to use picamera.
Linux Tutorials
How to build a kernel module with
DKMS on Linux
Suppose you wanted to install a device driver for a new graphics card, a WiFi 
dongle, or a network interface card that you purchased, and for whatever 
reason (e.g., missing prebuilt driver, outdated driver version, non-common 
build options), you decided to compile and install the driver module from the 
source. So you went ahead and downloaded the official source code of the 
device driver, compiled it against the kernel, installed and activated it. The 
new hardware is successfully recognized, and everything works fine.
Now think about what will happen to the driver if you upgrade to a newer 
kernel later. The driver was built outside the stock kernel source tree, so once 
your system is upgraded to a newer kernel, the driver you built will cease to 
work properly.
How to solve this problem? Well, you need to re-compile the driver against 
the newer kernel and re-install it. Obviously it becomes quite cumbersome if 
you have to re-build the driver manually every time you upgrade your kernel.
This is when dynamic kernel module support (DKMS) comes in handy.

The DKMS framework enables you to automatically re-build kernel modules 
into the current kernel tree as you upgrade your kernel. Hardware vendors 
often distribute their hardware's device driver as a DKMS package, so that 
users can auto-update the installed driver while they upgrade to a newer 
kernel. As an end-user you can easily configure DKMS to auto-update any 
custom device driver or kernel module that resides outside the stock kernel 
tree.
In this tutorial, I will demonstrate how to auto-build a kernel module with 
DKMS on Linux.
Install DKMS on Linux
You can install DKMS on various Linux distros as follows. As part of DKMS 
installation, necessary build tools (e.g., gcc, make) and kernel headers will also 
be installed by the distro's package manager.
Install DKMS on Ubuntu, Debian or or Linux Mint
term
$ sudo apt-get install dkms
Install DKMS on Fedora
term
__ $ sudo dnf install dkms
Install DKMS on CentOS or RHEL
First enable EPEL repository and proceed with: 
xterm
lb_ $ sudo yum install dkms

Install DKMS on Arch Linux
term
$ sudo pacman -S dkms
Auto-Build a Kernel Module with DKMS
Now let me show how to configure DKMS to re-build a kernel module for a 
new kernel automatically. In a nutshell, you need to create a DKMS 
configuration for the kernel module, and install the module with DKMS. 
Then any subsequent kernel upgrade will trigger DKMS to re-build the 
module.
In this tutorial, I will use ixgbe Intel NIC driver as an example kernel module. 
First, download and install the source code of the driver under
/usr/src/<driver-name>-<version>.
term
$ wget
http://tenet.dl.sourceforge.net/project/e1000/ixgbe%20s table/4.3.15/ixgbe-
4.3.15.tar.gz
$ sudo tar -xf ixgbe-4.3.15.tar.gz -C /usr/local/src $ sudo mv
/usr/local/src/ixgbe-4.3.15/src
/usr/src/ixgbe-4.3.15
Create the following dkms.conf file under this directory as follows.
term
$ sudo vi /usr/src/ixgbe-4.3.15/dkms.conf
PACKAGE_NAME="ixgbe"
PACKAGE_VERSION="4.3.15"
BUILT_MODULE_NAME[0]="ixgbe"
DEST_MODULE_LOCATION[0]="/kernel/drivers/net/ethernet/intel/ixg

be/"
AUTOINSTALL="yes"
Next, add the module to the kernel tree.
4 ■___ xterm
W__ $ sudo dkms add -m ixgbe -v 4.3.15
Or simply:
xterm
T $ sudo dkms add ixgbe/4.3.15
[dev@localhost -]$
[dev@localhost ~]$|sudojdkms_add -m ixgbe -v 4.3.15|
Creating symlink /var/lib/dkms/ixgbe/4.3.15/source -> 
/usr/src/ixgbe-4.3.15
DKMS: add completed.
[dev@localhost ~]$
Build the specified module against the currently running kernel.
xterm
lb_ $ sudo dkms build -m ixgbe -v 4.3.15
__ I^HOr:

xterm
$ sudo dkms build ixgbe/4.3.15
[devfalocalhost -]$
[dev^localhost ~]$ jsudo dkms build -m ixgbe -v 4.3.15
Kernel preparation unnecessary for this kernel. Skipping...
Building module:
cleaning build area...
make KERNELRELEASE=3.10.0-229.el7.x86_64 -C /lib/modules/3.10.0-229.el7.x86_64/b
uild M=/va r/lib/dkms/ixgbe/4.3.15/build. . . . . . . . . .
cleaning build area...
DKMS: build completed.
[dev@localhost ~]$
Finally, install the module under the current kernel tree.
B__xterm
$ sudo dkms install -m ixgbe -v 4.3.15
Or:
xterm
lb_ $ sudo dkms install ixgbe/4.3.15

Verify Auto-Build of a Kernel Module during

Kernel Upgrade
With DKMS configured for ixgbe driver, let's see what happens to the installed 
driver when we upgrade to a new kernel. In this test, I am going to upgrade 
the kernel from 3.10.0-229 to 3.10.0-327. The test environment used is 
CentOS 7, so I am going to use yum to upgrade to the new kernel. Note that 
the new kernel's matching kernel headers also need to be installed for DKMS 
to re-build the device driver.
Before upgrading the kernel, check the version of ixgbe driver and the kernel.
xterm
Hl_ $ modinfo ixgbe
We can see that the device driver 4.3.15 was installed by DKMS for the
[dev@localhost -1$ modinfo ixgbe
filename:_ _ _
/lib/modules/3.10.e-229.el.7.x86 64.'extra/ixgbe.ko
Version:
4.3.15
license: 
description: 
author: 
rhelversion: 
srcversion: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias:
alias:
gpl ' X
Intel(R) 10 Gigabit KL Express Network Driver
Intel Corporation, <linu)X<s@intel.com>
?.i 
X ixgbe driver 4.3.15 was built
7AED484083B2C5B86424A3A 
■ ,. 
. - 
«9,Q
pci:v00008086d060ei5ADsv
*s(l
*bc
*sc
+i* aya,nsl Rernel
pci: v00008O86d000015ACsv
*sd
*be
*sc
*i
* by D K MS
pci: v00008O86d0O0015ABsv
*sd
*be
*sc
*i*
pci:v000O8O86dOOO015AAsv
*
 sd * be *s  c * i*  
pci:vO0008O86d000015D1Sv
*
 sd * be * s c * i*  
pci:v000O8O86d0OO01563sv
*sd
*bc
*sc
*i*
pci:v00008086dO0001560 sv
*
 sd * be * s c * i*  
pci:v000O8086dO0O01558sv
*sd
*bc
*sc
*i*
pci:v000O8O86d0000154Asv
*
 sd * be * s c * i*  
pci:v000O8086d00O01557sv
*sd
*bc
*sc
*i*
pci:v000O8086d0000154F sv
*
 sd * be * s c * i*

kernel 3.10.0-229.
Now go ahead and install a newer kernel and matching header files.
xterm
Hl_ $ sudo yum install kernel kernel-devel
After rebooting into the new kernel, check the module information of ixgbe 
driver again.
-]$ modinfo ixqbe_______________________
/lib/modules/|3.10.0-327.13.1. el7. x86_64^ext ra/ixgbe. ko
4-3-15 | 
_ 
’
GPL
Intel (R) 10 Gigabit--PCI Express Network Driver
Intel Corporation, <lirFtBGnics@intel.com>
7.2 
ixgbe driver 4.3.15 was automatically
7AED484083B2C5B86424A3A 
re-built against kernel 3.10.0-327
pci:v000O8086d000015ADsv*sd*bc*sc*i*
pci:v00008086d000015ACsv*sd*bc*sc*i*
pci:v00008086d000015ABsv*sd*bc*sc*i*
pci:v00008086d000015AAsv*sd*bc*sc*i*
pci:v00008086d000015Dlsv*sd*bc*sc*i*
pci:v00008086d00001563sv*sd*bc*sc*i*
pci:v00008086d00001560sv*sd*bc*sc*i*
pci:v00008086d00001558sv*sd*bc*sc*i*
pci:v00008086d0000154Asv*sd*bc*sc*i*
pci:v00008086d00001557sv*sd*bc*sc*i*
pci:v00008086d0000154Fsv*sd*bc*sc*i*
pci:v00008086d0000154Dsv*sd*bc*sc*i*
pci:v00008086d00001528sv*sd*bc*sc*i*
[dev@localhost 
filename:_____  
Version:
license: 
description: 
author: 
rhelversion: 
srcversion: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias: 
alias:
You should see that the ixgbe driver of the same version has automatically 
been built for a new kernel 3.10.0-327.
Note that if you had upgraded to a new kernel without installing matching 
kernel headers, DKMS wouldn't have been triggered, and the driver wouldn't 
have been re-built from its source. Instead, a stock kernel driver (if available) 
that came with the new kernel would have been used.
For example, the following is the result of upgrading to a new kernel without 
matching kernel headers, where you can see that the new kernel loaded a 
stock ixgbe driver whose version is a little outdated.

You can always re-build the custom driver at any time by triggering DKMS 
manually as follows. Be sure to install matching kernel headers first.
xterm
$ sudo yum install kernel-devel 
$ sudo dkms build ixgbe/4.3.15 
$ sudo dkms install ixgbe/4.3.15
Conclusion
In this post I demonstrated how to use DKMS to auto-build a kernel module.

Granted, DKMS may not always be preferred especially in production Linux 
environments where development packages and build tools are typically not 
allowed. But for development environments or personal desktop, if you have 
any custom-built kernel module, DKMS makes your life easy by keeping the 
module updated against new kernels.
Linux Tutorials
What are the best plugins to 
increase productivity on Emacs
Some time ago, I went looking for the best plugins to turn Vim into a 
fullfledged IDE. Interestingly, a lot of the comments on that post were about 
how Emacs already has most of these plugins built in, and was already a great 
IDE. Although I can only agree about Emacs' incredible versatility, it is still 
not the ultimate editor when it comes out of the box. Thankfully, its vast 
plugin library is here to fix that. But among the plethora of options available 
to you, it is sometimes hard to know where to start. So for now, let me try to 
assemble a short list of the indispensable plugins to increase your 
productivity while using Emacs. Although I am heavily geared towards 
programming related productivity, most of these plugins would be useful to 
anyone for any usage.
1. Ido-mode
<backgroundcolor r="0.0" g="0.0" b="0.0"/>
cparticLatjath i-'l" duration="20..0‘' r="0.0” q-'fl.B" b="0.4"/>
U:— solar-system.xml 
00% L221 (nXML Valid yas)
Find tile; -/animation/clic/ThemelMilestoneL/FOSSSini/OCNakeCacbe.txt 
iros.h | Makefile | MathDefs.h | MatbUtilities.cpp | ...J
I 
I CMakeLists.txt | Clogs.h I ExplicitEuler.cpp | ExplicitEuler.h I Force.cpp I Force.h I Mac?
Maybe one of the most useful plugin for beginners, Ido stands for 
interactively do. It replaces most of the dry prompts with a fancy character 
matching menu. For example, it will replace the normal prompt to open a file 
with a list of all the files in the current directory. Type any string, and Ido 
will try to match it with the most appropriate file. It makes it really easy to 
visualize your actions, and also to quickly get through a folder where all files 

are named with the same prefix.
2. Smex
<particlepath i="l“ duration="20.0" r="0.0" g="0.8" b="0.4’7>
</scene>
Us— solar-system.xml 
Sot L221 (nXML Valid yas)
H-x {j2048-game | compile I revert-buffer | web-mode | rgrep | yas expand | ansi-term | load-theme | imenu | svn-status | ...}
Not the most famous one, but a good place to complete what Ido-mode 
started: Smex can be a fancy replacement to the normal M-x prompt, with a 
heavy inspiration from Ido-mode. It brings the same interactive search for the 
commands you would normally have to type after calling M-x. It is simple and 
efficient, and a great way to save those additional few fractions of a second 
you normally need.
3. Auto Complete
(setq-default indent-tabs-mode nil)
(setq-default tab-width 4)
(setq tab-width 4)
(sgml-guess-indent)))
Before knowing the existence of this plugin, I spent half of my time on 
Emacs pressing M-/ to complete my words. Now, I have a fancy pop-up to do 
it for me. There is not much more to say about it, except that we all need it.
4. YASnippet

This one is really for the coders. There is always some piece of code that we 
feel we use all the time. For me, it's var_dump(...); exit; to debug PHP. After a 
while of typing it over and over, it just occurred to me that I could have it 
pre-recorded and easily accessible as a snippet of code. With YASnippets, it's 
easy to import snippet files or make your own. After that, a simple press on 
the tabulation key will expand a small keyword into a chunk of pre-written 
code easy to navigate through and modify.
5. Org-mode
For disclaimer, I have only recently started using Org-mode. But it has 
already blown me away. From the hundreds of written pieces I have seen 
around, Org-mode can change your life. The idea behind it is simple: it is a 
mode that simplifies note taking while keeping a plain text format, making it 
easy to navigate through lists of tasks and various data, and perform 
operations such as filtering by priority or due date, or setting a recurrence. 
Yet, from this simple idea, you can accomplish a lot, and it is easy to get 
overwhelmed with all the options. Rather than a long explanation, I urge you 
to go through available tutorials, watch a lot of videos, and see by yourself 
how powerful Org-mode is.
6. Helm
Some love it, but others are not such a big fan of it. I am part of the later. But 
with such a huge following, it is impossible to avoid it. Helm aims to 
transform your Emacs experience completely. Simply described, Helm is a 
framework that will help you find a file or a command quickly from within 
Emacs. Based on your input, it will try to use word completion to guide you 

to the action you have in mind. The feeling is a bit weird at first, but for 
some, Helm is a religion of its own. Although I am not its fan, I do appreciate 
helm-occur which a great tool to search for strings in a large document as it 
shows all occurrences in a separate buffer, making it easy to navigate through 
them. If you are looking for a quick demo to understand what Helm can do, I 
recommend this post.
7. ace-jump-mode
Another plugin with a big following that I am trying to get on board with is 
ace-jump-mode. Master this plugin, and you will be promised to transcend 
the usage of a mouse. Simply described, by triggering ace-jump-mode with a 
shortcut of your choice, you will be prompted for a character. Enter one, and 
all words starting with that character will be highlighted with a unique letter. 
Enter one of the letters on screen, and your cursor will jump straight to the 
word it is highlighting. I have to admit that it is pretty hard to get the reflex to 
use it, but once you have it, it will increase your movement speed in a 
document by a lot.
8. find-file-in-project
If you like Sublime text and its very handy Ctrl-p fuzzy search to open any file 
in a project, then you will love find-file-in-project (or ffip). After setting it up 
by declaring the root of your version control folder, you can summon easily a 
cool text bar that quickly scans and searches through your code base for a 
matching file based on the name you input. I like to have it bound to the F6 
key on my keyboard. It is simple and very handy if you do not know the 
complicate directory structure from the top of your head.
9. Flymake

return $orgList
For IDE lovers, I think that syntax checker is one of the most powerful 
features. It is great for beginners and handy for tired programmers. And 
thanks to Flymake, Emacs users can enjoy it too. Since I work in PHP a lot, 
Flymake does not need any extra configuration. As I write my code, it will 
automatically check my code and highlight any line that contains a problem. 
For compiled languages, Flymake will look for a Makefile that it will use to 
check your code. Absolutely magical.
10. electric-pair
Last, but not least, electric-pair is one of the simplest yet most powerful 
plugin in my opinion. It just automatically closes whatever parenthesis or 
bracket you open. It doesn't look like much at first, but trust me. After 
struggling for the hundredth time to find that matching parenthesis, you will 
be glad to have a plugin to ensure that all your expressions are balanced.
To conclude, Emacs is a fantastic tool. Probably not a shocker. Try these 
plugins and watch as your productivity goes through the roof. This list is of 
course not exhaustive at all. If you want to bring your contribution, feel free 
to do so in the comments. I am myself always looking for new plugins to try 
and new ways to experience Emacs.
Linux Tutorials
How to set up QoS bandwidth rate 
limit on Vyatta router

If you are running a network shared by multiple devices, you probably want 
to set up QoS policies (e.g., average bandwidth rate, burst size), so that 
network bandwidth is properly shared by them. Vyatta software router (now 
renamed to VyOS) supports powerful QoS settings. You can easily define 
any QoS policy, and bind the policy to a specific network interface/port, or to 
specific types of traffic, etc.
If you would like to set up QoS bandwidth rate limit on Vyatta, follow the 
instructions below.
I assume that Vyatta router has two interfaces: eth0 connected to internal 
LAN, and eth1 connected to external networks. Here I will set bandwidth cap 
for upload traffic at 1Mbits/sec and download traffic at 2Mbits/sec.
What I will do is the following. First, define outgoing traffic shaper on eth1 to 
rate limit outbound traffic (i.e., upload traffic), and then define another 
outgoing shaper on eth0 to rate limit inbound traffic (i.e., download traffic). I 
am going to use Vyatta's Command Line Interface (CLI) to set this up.
$ configure
[you are entering Vyatta's CLI]
$ set traffic-policy shaper UPLOAD-POLICY
$ set traffic-policy shaper UPLOAD-POLICY bandwidth 1Mbit
$ set traffic-policy shaper UPLOAD-POLICY default 
bandwidth 50%
$ set traffic-policy shaper UPLOAD-POLICY default ceiling 100%
$ set traffic-policy shaper UPLOAD-POLICY default burst 15k
$ set traffic-policy shaper UPLOAD-POLICY default queue-type fair-queue
$ set interfaces ethernet eth1 traffic-policy out UPLOAD-POLICY
$ set traffic-policy shaper DOWNLOAD-POLICY
$ set traffic-policy shaper DOWNLOAD-POLICY bandwidth 2Mbit
$ set traffic-policy shaper DOWNLOAD-POLICY default bandwidth 50%
$ set traffic-policy shaper DOWNLOAD-POLICY default ceiling 100%

$ set traffic-policy shaper DOWNLOAD-POLICY default burst 15k
$ set traffic-policy shaper DOWNLOAD-POLICY default queue-type fair­
queue
$ set interfaces ethernet eth0 traffic-policy out DOWNLOAD-POLICY
$ commit
$ save
$ exit
[you are exiting Vyatta's CLI]
The resulting /config/config.boot looks like the following:
interfaces {
ethernet eth0 {
address 192.168.10.1/24 
traffic-policy {
out DOWNLOAD-POLICY }
}
ethernet eth1 { 
address dhcp 
...
traffic-policy {
out UPLOAD-POLICY }
}
} 
traffic-policy {
shaper UPLOAD-POLICY { 
bandwidth 1Mbit

default {
bandwidth 50%
burst 15k
ceiling 100%
queue-type fair-queue
}
}
shaper DOWNLOAD-POLICY {
bandwidth 2Mbit
default {
bandwidth 50%
burst 15k
ceiling 100%
queue-type fair-queue
}
}
}


Later if you want to change your bandwidth rate limit on Vyatta, you can 
simply do:
xterm
J
$ configure
$ set traffic-policy shaper DOWNLOAD-POLICY bandwidth 8Mbit
$ set traffic-policy shaper UPLOAD-POLICY bandwidth 4Mbit
$ commit
$ save
$ exit
The change will take effect upon commit. So you do not need to
reboot Vyatta.
Linux Tutorials
How to enable Nux Dextop 
repository on CentOS or RHEL
Question: I would like to install a RPM package which is available only in 
Nux Dextop repository. How can I set up Nux Dextop repository on CentOS 
or RHEL?
Nux Dextop is a third-party RPM repository which contains many popular 
desktop and multimedia related packages (e.g., Ardour, Shutter, etc) for 
CentOS, RHEL and ScientificLinux. Currently, Nux Dextop repository is 
available for CentOS/RHEL 6 and 7.
To enable Nux Dextop repository on CentOS or RHEL, follow the 
instructions below.
First of all, be aware that Nux Dextop is designed to coexist with EPEL 
repository. So you need to enable EPEL in order to use Nux Dextop repo. 
After enabling EPEL, go ahead and install Nux Dextop repository as follows. 
As the first step, import the official GPG key of Nux Dextop repository:

xterm
lb_ $ sudo rpm --import http://li.nux.ro/download/nux/RPMGPG-KEY-
nux.ro
Without GPG key import, you will get the following warning while installing 
Nux Dextop.
warning: /var/tmp/rpm-tmp.y4VqPB: Header V4 RSA/SHA1 Signature, key
ID 85c6cd8a: NOKEY
Then go ahead and install Nux Dextop with yum command as follows. Note 
that Nux Dextop repository comes as an architecture in-dependent RPM, so 
you can install the same RPM on both 32-bit and 64-bit architectures.
On CentOS/RHEL 6.*:
term
$ sudo rpm -Uvh
http://li.nux.ro/download/nux/dextop/el6/x86_64/nuxdextop-release-0-
2.el6.nux.noarch.rpm
On CentOS/RHEL 7:
term
$ sudo rpm -Uvh
http://li.nux.ro/download/nux/dextop/el7/x86_64/nuxdextop-release-0-
1.el7.nux.noarch.rpm

Now verify that Nux Dextop repository is successfully installed:
term
$ yum repolist
[dev@centos7 -]$ yum repolist
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile 
* base: centos.aol.com
* epel: mirror.seas.harvard.edu
* extras: centos.chi.host-engine.com
* nux-dextop: mirror.li.nux.ro
* updates: mirrors.advancedhosters.com
repo id
repo name
status
base/7/x86 64
CentOS-7 - Base
8,465
epel/x86 64
Extra Packages for Enterprise Linux 7 - x86 64
5,646
extras/7/x86 64
CentOS-7 - Extras
30
nux-dextop/x86_64
Nux.Ro RPMs for general desktop use
1,231
updates/7/x86 64
CentOS-7 - Updates
687
repolist: 16,059 
[dev@centos7
For Repoforge/RPMforge Users
According to the author, Nux Dextop is known to cause conflicts with other 
third-party RPM repos such as Repoforge and ATrpms. Therefore, if you 
enabled any third-party repos other than EPEL, it is highly recommend you 
set Nux Dextop repository to default off state. That is, open
/etc/yum.repos.d/nux-dextop.repo with a text editor, and change enabled=1 to enabled=0 
under nux-desktop.
xterm
T $ sudo vi /etc/yum.repos.d/nux-dextop.repo

[nux-dextop]
name=Nux.Ro RPMs for general desktop use
baseurl=h11p://Ii.nux.ro/download/nux/dextop/el7/$basearch/ http://mirror.li.nux 
.ro/li.nux.ro/nux/dextop/el7/$basea rch/ 
snabled=0 < 
gpgcheck=l
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-nux.ro
protect=0
[nux-dextop-testing]
name=Nux.Ro RPMs for general desktop use - testing
baseurl=http://li.nux.ro/download/nux/dextop-testing/el6/$basearch/
enabled=G
gpgcheck=l
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-nux.ro
protect=0
Then whenever you want to install a package from Nux Dextop repo, 
explicitly enable the repo as follows.
xterm
T $ sudo yum --enablerepo=nux-dextop install <packagename>
Linux Tutorials
How to set up a Subversion (SVN) 
server on CentOS or Fedora
The open-source community has been using Subversion (or SVN) widely for 
many collaborative open-source development projects. SVN is supported by 
all major open-source project hosting sites such as Google Code, GitHub, 
SourceForge and Launchpad. You can of course set up your own Subversion 
server in house.
SVN supports several protocols for network access: SVN, SVN+SSH, HTTP,

HTTPS. If you are behind a firewall, HTTP-based Subversion is 
advantageous since SVN traffic will go through the firewall without any 
additional firewall rule setting. In this tutorial, I will describe how to set up 
an HTTP-based Subversion server on CentOS or Fedora platform.
First, install Subversion, as well as Subversion module for Apache HTTP 
server as follows. This will also install Apache HTTP server as a dependency 
if it is installed.
xterm
$ sudo yum install subversion mod_dav_svn 
iw-JNext, create a local SVN repository which will store code
xterm
T__ $ sudo mkdir /svnrepos
$ sudo svnadmin create /svnrepos
Make the SVN repository readable and writable by the same user as the one 
which Apache HTTP server runs as. On CentOS or Fedora platforms, this 
user is apache.
xterm
lb_ $ sudo chown -R apache.apache /svnrepos
I [Edit Apache HTTP server configuration as follows.
xterm
lb_ $ sudo vi /etc/httpd/conf.d/subversion.conf
__ I^^LoadModule dav_svn_module modules/mod_dav_svn.so
LoadModule authz_svn_module modules/mod_authz_svn.so 
<Location /svnrepos>
DAV svn
SVNPath /svnrepos
AuthType Basic
AuthName "Subversion repositories"
AuthUserFile /etc/svn-auth-users
Require valid-user

</Location>
Next, add Subversion users (e.g., user1, user2, user3, etc) who are authorized to 
access the SVN server. Here I use basic authentication provided by Apache 
HTTP server. For that, use htpasswd command which creates usernames and 
password for authenticated HTTP users.
The first time you run htpasswd command, use -c option, which will create an 
initial SVN password file (/etc/svn-auth-users). For subsequent runs of htpasswd 
command, do not use -c option.
I___ xterm
■r_ $ sudo htpasswd -cm /etc/svn-auth-users userl
New password:
Re-type new password:
Adding password for user user1

$ sudo htpasswd -m /etc/svn-auth-users user2 
$ sudo htpasswd -m /etc/svn-auth-users user3
—^^■Now restart Apache HTTP server. 
xterm
lb_ $ sudo service httpd restart
_ ^^Bor:
xterm
lb_ $ sudo systemctl restart httpd
At this point, the SVN server should be accessible via HTTP. Go to: http://<ip- 
address-svn-server>/svnrepos to browse the SVN
repository via web interface. As shown below, you will be prompted to enter 
username and password to proceed.

Also, when you try to access the SVN server from SVN client software, you 
will also be required to authenticate yourself.
dev@dev:~$ svn co http://pocoyo/svnrepos/papers
Authentication realm: <http://pocoyo:80> Subversion repositories 
Password for 'dev':
ATTENTION! Your password for authentication realm:
<http://pocoyo:80> Subversion repositories
can only be stored to disk unencrypted! You are advised to configure 
your system so that Subversion can store passwords encrypted, if 
possible. See the documentation for details.
You can avoid future appearances of this warning by setting the value 
of the 'store-plaintext-passwords' option to either 'yes' or 'no' in 
'/home/dev/.subversion/servers'.
Store password unencrypted (yes/no)? yes
A 
papers/icclO
A 
papers/icc!0/algos-view-traffic.eps
Linux Tutorials
How to set up Samba as a Primary

Domain Controller
A domain controller is a server which groups multiple computers to 
centralize their authentication system. When you are using a domain 
controller, you don't login to your computer, but instead login to the domain 
controller. Every authentication request is handled by the Primary Domain 
Controller (PDC).
Usually you hear about PDC using a Windows based server. In this tutorial, 
I'll describe how to set up a PDC using Samba, which is based on Linux. 
There are four main steps for setting up Samba as a PDC:
Install Samba
Configure /etc/samba/smb.conf
Add domain users
Register all Windows computers with Samba PDC.
1. Samba Installation
If you are using a Debian based Linux, run the following command on the 
terminal window to install Samba:
xterm
$ sudo apt-get install samba
$ sudo apt-get install samba-common
$ sudo apt-get install samba-common-bin
__ ^^Hlf you are using a Red Hat based Linux, you may use rpm or yum 
package manager to install Samba.
2. Samba Configuration
The main configuration of Samba server is found in /etc/samba/smb.conf. For a 
PDC server, there are three part of the file which you need to configure: global, 

netlogon, and homes.
Before you start modifying the configuration file, I suggest you back up the 
existing Samba configuration file.
xterm
M__ $ sudo cp /etc/samba/smb.conf /etc/samba/smb.conf.old__ L-J
Configuring [global] parameters
[global]
workgroup = sambadomain 
netbios name = sambapdc 
server string = Samba PDC 
domain master = yes 
preferred master = yes 
domain logons = yes
add machine script = /usr/sbin/useradd -N -g machines -c Machine -d 
/var/lib/samba -s /bin/false %u
security = user
encrypt passwords = yes 
wins support = yes 
name resolve order = wins lmhosts hosts bcast 
logon path = %N%Uprofile

logon drive = H: 
logon home = %N%U
Change the names of the workgroup and the PDC according to your 
environment, so that they correspond to the actual values in your network. If 
you have another Wins server on your network, remove "wins support = yes", 
because having more than one causes a problem. "wins support = yes" means 
Samba acting as a Netbios server.
Creating lmhosts file
Don't forget to register your domain IP address to the lmhosts file. The lmhosts 

file is a mapper between the IP address of the domain controller 
and Netbios name. When you add a Windows computer to the sambadomain, 
Windows tries to find the PDC's IP address. If Windows fails
to find the PDC's IP address, then you won't be able to register a computer 
with the PDC.
The lmhosts file should be created and placed in /etc/samba/lmhosts. The content of 
lmhosts file is similar to /etc/resolv.conf file, except that you need to register the 
Netbios name instead of the host name. For example, if your PDC has an IP 
address 10.10.101.1 with sambadomain as workgroup name, and sambapdc as the 
Netbios name, the content of the
lmhosts file should look like the following:
10.10.101.1 sambadomain
10.10.101.1 sambapdc
After creating /etc/samba/imhosts, re-run the nmbd daemon as follows:
term
T__ $ sudo nmbd -H /etc/samba/lmhosts -D
Configuring [netlogon] parameters
[netlogon]
path = /var/lib/samba/netlogon 
browseable = no
read only = no 
create mask = 0700 
directory mask = 0700 
valid users = %S

/var/lib/samba/netlogon is a startup directory for PDC logon. When users login to 
the Samba PDC, a script called netlogon.bat in the directory will be executed.
xterm
T $ sudo mkdir -m 0755 /var/lib/samba/netlogon
_J^HFor example, if you want to automatically mount a network drive 
from the PDC, Create the following netlogon.bat script in
/var/lib/samba/netlogon.
## Samba Logon Script
net use x: sambapdcshare
Configuring [homes] parameters
This is a configuration file for PDC user's home directory.
[homes]
valid users = %S
guest ok = yes
read only = yes

Testing the configuration file
After saving all configuration files, test your configuration with the following 
command:
xterm
Bl_ $ sudo testparm
—■■if there is any syntax error detected, fix it and restart Samba.
3. Adding Domain Users
Adding admin user and group for the PDC
In Linux, admin user is the root user. So you need to run the following 
command to add the root user as the Samba admin:
xterm
T $ sudo smbpasswd root
__ I^Byou should not use the same password as the Linux root user.
Create a machines group
The next step is to create a group called machines.
xterm
lb_ $ sudo groupadd -g machines
__ ^^^Samba will automatically add users to this group, as long as you 
configure "add machine script" correctly in [global] section in
/etc/samba/smb.conf.

Create a Linux Account for PDC login
You need to create a user on PDC for domain login. In this example, I will 
create an account that disables Linux login. So every access to the PDC must 
be done via Samba.
For example, creating user dan:
xterm
$ sudo smbpasswd -a dan
Enter the same password twice.
You need to activate the user with the following command:
xterm
T $ sudo smbpasswd -e dan 
.V-Jorant user dan to login to the PDC:
xterm
$ sudo net rpc rights grant "SAMBADOMAINdan"
SeMachineAccountPrivilege SePrintOperatorPrivilege SeAddUsersPrivilege
SeDiskOperatorPrivilege
SeRemoteShutdownPrivilege
^^^^^^^^^^^■xtei'iii
~M $ sudo net groupmap add ntgroup-'Administrator" unixgroup=root 
rid=512 type=d
4. Register Windows Computers with the Samba 
PDC
Under Windows computer properties, change the domain name to 
sambadomain. Reboot your Windows PC, and try to login with 
sambadomain/dan. If you successfully login, then your Samba PDC is ready.

Linux Tutorials
How to reload config.boot in Vyatta
Vyatta (now called VyOS) an open-source virtual router operating system 
based on a customized Debian distribution. Vyatta can easily be installed on 
virtual machines (VMs) created by standard hypervisors such as Xen, KVM 
and VMware, to implement virtual networking for VMs. Vyatta offers a 
specialized management console as well as web-based management interface 
to configure system settings such as network interfaces, firewall rules, QoS 
policy, etc. If you are reconfiguring Vyatta via its management console, you 
may want to know how to reload Vyatta configuration after you are done.
Actually, reloading Vyatta configuration is simpler that you think, if you 
update Vyatta configuration via its CLI (Command Line Interface), instead of 
modifying /config/boot.config directly. For example:
term
$ configure
[You are entering the Vyatta's CLI]
# change Vyatta configuration as you wish
$ commit
$ save
$ exit
[You are exiting the Vyatta's CLI]
If you follow the above sequence, all the changes you made via Vyatta's CLI 
take effect upon a commit. That means you don't need to reload Vyatta 
configuration subsequently.
If your changes do not take effect for some reason, you can do the following 
to reload Vyatta configuration explicitly.

term
$ /etc/init.d/vyatta-rtrmgr restart
Linux Tutorials
How to set up a secure Apache 
webserver on Ubuntu
This tutorial assumes that you have a running Ubuntu Server, that networking 
has been set up, and that you have SSH access.
Apache HTTP server (Apache2) is the default web server used by many 
Linux installations. It is not the only one available, or the best for all 
circumstances, but it covers many usage scenarios. During the installation, 
you may be asked which webserver to reconfigure automatically. Answer 
apache2.
Install Apache2
Use the following command to install Apache2 and other libraries. 
xterm
$ sudo apt-get -y install apt-get install apache2 apache2.2-common apache2- 
doc apache2-mpm-prefork apache2-utils libexpat1 ssl-cert libapache2-mod- 
php5 apache2-utils libexpat1 ssl-cert libapache2-mod-php5 mod-fcgid 
apache2-suexec php-pear php-auth php5-mcrypt mod-fcgid apache2-suexec 
php-pear php-auth php5-mcrypt mod-ruby
Update TimeZone and Check Correct Time

To reduce confusion with shared or mirrored data, all servers ought to run as 
close to as in-sync as possible. Some cryptographic key management systems 
require accurate time. Lastly, for corporate servers, Sarbanes-Oxley and 
HIPAA Security Rules require accurate timestamping.
term
$ sudo apt-get -y install openntpd tzdata
$ sudo dpkg-reconfigure tzdata
$ sudo service openntpd restart

Disable AppArmor Conflicts
While AppArmor is a suite that does provide an additional layer of security, 
it is my opinion that custom profiles will need to be created for each system. 
That is something not covered in this tutorial. So for now, we are going to 
disable it to prevent conflicts with any default configurations.
term
$ sudo /etc/init.d/apparmor stop 
$ sudo update-rc.d -f apparmor remove
$ sudo apt-get remove apparmor apparmor-utils
Note: disabling AppArmor is not recommended for a production web server. 
For those wanting to create a custom AppArmor profile, refer to the official 
documentation.
Protect Against DDoS Attacks
A DDoS attack is a distributed denial-of-service attack. An Apache module 
exists to stop such attacks, and it can be enabled as follows.
xterm
$ sudo apt-get -y install libapache2-mod-evasive $ sudo mkdir -p 
/var/log/apache2/evasive
$ sudo chown -R www-data:root /var/log/apache2/evasive
Append the following to the bottom of mod-evasive.load. 
term
__ $ sudo nano /etc/apache2/mods-available/mod 
evasive.load

DOSHashTableSize 2048
DOSPageCount 20 # maximum number of requests for the same page
DOSSiteCount 300 # total number of requests for any object by the same 
client IP on the same listener
DOSPageInterval 1.0 # interval for the page count threshold DOSSiteInterval 
1.0 # interval for the site count threshold DOSBlockingPeriod 10.0 # time 
that a client IP will be blocked for DOSLogDir “/var/log/apache2/evasive”
DOSEmailNotify [email protected]
Stop Slowloris Attacks
An Apache modules also exist for Slowloris attacks, though the module name 
depends on which version of Ubuntu that you are using. For Ubuntu 12.10 or

later:
xterm
lb_ $ sudo apt-get -y install libapache2-mod-qos
L [Then check configuration in qos.conf:
xterm
lb_ $ sudo nano /etc/apache2/mods-available/qos.conf
# # QoS Settings
<IfModule mod_qos.c>
# handles connections from up to 100000 different IPs
QS_ClientEntries 100000
# will allow only 50 connections per IP
QS_SrvMaxConnPerIP 50
# maximum number of active TCP connections is limited to 256 MaxClients 
256
# disables keep-alive when 70% of the TCP connections are occupied: 
QS_SrvMaxConnClose 180
# minimum request/response speed (deny slow clients blocking the server,
# ie. slowloris keeping connections open without requesting anything): 
QS_SrvMinDataRate 150 1200
# and limit request header and body (carefull, that limits uploads and

# post requests too):
# LimitRequestFields 30
# QS_LimitRequestBody 102400
</IfModule>
Note: If you are running a version of Ubuntu prior to 12.04, use the 
following instead.
term

$ sudo apt-get -y install libapache2-mod-antiloris 
Check config in antiloris.conf.
term
$ sudo nano /etc/apache2/mods-available/antiloris.conf 
<IfModule mod_antiloris.c>
# Maximum simultaneous connections in READ state per IP address
IPReadLimit 5
</IfModule>
Protect Against DNS Injection Attacks
Spamhaus is a module that uses DNSBL in order to block spam relay via web 
forms, preventing URL injection, block http DDoS attacks from bots and 
generally protecting the server from known bad IP addresses.
xterm
$ sudo apt-get -y install libapache2-mod-spamhaus $ sudo touch 
/etc/spamhaus.wl
I [Append the following configuration to apache2.conf
xterm
T $ sudo nano /etc/apache2/apache2.conf 
<IfModule mod_spamhaus.c>
MS_METHODS POST,PUT,OPTIONS,CONNECT
MS_WhiteList /etc/spamhaus.wl
MS_CacheSize 256

</IfModule>
Restart Apache to load new modules.
term
Bl_ $ sudo service apache2 restart
__ I^HOr:
term
T $ sudo systemctl restart apache2
Now the webserver has been installed and is up and running. Point your web 
browser at your domain for a default message that confirms you are working. 
As a final check, run the following to see if your server has any error 
message. If there are errors, you will want to Google them and address them 
now.
term
lb_ $ sudo tail -200 /var/log/syslog
Turn off Server Signature
Another source of potential security attacks is associated with web server 
signature, which reveals the version of Apache and PHP being deployed. The 
more detailed knowledge attackers have on your system, the better chance

they have for exploiting potential vulnerabilities of the system. Follow this 
guideline to disable server signature on Apache.
Linux Tutorials
How to add an additional vif to VM 
in XenServer
XenServer allows us to dynamically create and attach new resources such as 
virtual network interfaces or disk storage to an existing VM. For this, you can 
use the xe command line, which is available in XenCenter or on standalone 
Linux as part of xapi. In the following, I describe how you can add an 
additiona virtual network interface (vif) to a XenServer VM.
Here I assume that you know the name-label of the VM.
Create a network for the new interface to be attached to with the following 
command. It will return the UUID of the created network (network-uuid). 
Remember network-uuid as it is used later.
xterm
Hb_ $ xe network-create name-label="alice-network"
I [Find out uuid of the VM (vm-uuid) with the following command. 
xterm
Hb_ $ xe vm-list name-label=<vm's name-label>
__ ^^HCreate eth1 interface for the VM by using network-uuid and vm-uuid 
information with the following command. Upon success, this will return the 
uuid of the new interface (vif-uuid).
I___^Ixterm
$ xe vif-create vm-uuid=<vm-uuid> network-uuid= _ <network-uuid> 
device=1 mac=random
k [Finally, hot-plug the created interface to the VM.
xterm
Hb_ $ xe vif-plug uuid=<vif-uuid>

Now eth1 should be visible from inside the VM.
Note that you still need to configure eth1 from within VM to make it active.
Linux Tutorials
What are useful online tools for
Linux
As you know, GNU Linux is much more than just an OS. There is literally a 
whole sphere on the Internet dedicated to the penguin OS. If you read this 
post, you are probably inclined towards reading about Linux online. Among 
all the pages that you can find on the subject, there are a couple of websites 
that every Linux adventurer should have in his bookmarks. These websites 
are more than just tutorials or reviews. They are real tools that you can access 
from anywhere and share with everyone. So today I shall propose you a 
nonexhaustive list of sixteen websites that should be in your bookmarks. 
Some of them can also be useful for Windows or Mac users: that's the extent 
of their reach.
1. ExplainShell.com

If you are interested in Linux command line, you should use this website. If 
you are not interested in Linux command line, you should use it even more as 
it will explain in detail how a command works. This could prevent you from 
launching a command detrimental to your computer, and is a good way to 
learn with a great interface.
2. BashrcGenerator.com

□ .bashrc PS1 generator
Generate your .bashrc PS1 prompt easily with a drag and drop interface
Examples and presets of PS1 prompts
Clicking on an example will replace your selection. 
iulienamyoomputer:/usr/local/srcf user at computer: path $ 
lulien®mycMnputer; r/usr/local/srcl r user at computer [ path ]. more readable
Preview of your prompt
Your seiection
hostname (short) hostname (full) username shell name terminal directory directory (basename] time-short 
time with seconds (HHA4M:SS) time (HH:MM)
Drag and drop elements to your selection
new line #/$
Paste to your command line or copy into your .bashrc file for permanent use (example: vim-Abashrc). which I recommend doing of course.
Your generated .bashrc PS1 and additional functions
If you want to begin with Linux command line, or if you want to quickly get 
a customized shell prompt but not sure how, this website will generate for 
you PS1 prompt code to place your .bashrc file in your home directory. You 
can drag and drop the elements that you would like to see in your prompt, 
like your username and the current time, and the website will write the code 
for you. It's easy and very readable. Definitely a must for the lazy.
3. Vim-adventures.com

I only recently discovered this website, but it already sucked in many hours 
of my life. In short: a RPG game with Vim commands. Move your character 
in the isometric levels with the 'h,j,k,l' keys, gain new commands/abilities, 
collect keys, and learn how to use Vim proficiently very quickly.
4. Try Github

1.2 • Checking the Status
Good job! As Git just told us, our "octobox" directory now has an empty repository in 
/.git/. The repository is a hidden directory where Git operates.
To save your progress as you go through this tutorial - and earn a badge when you 
successfully complete it - head over to create a free Code School account. We’ll wait 
for you here.
Next up, let's type the git status command to see what the current state of our project 
is:
tryGit
git status
A A A 
<£! TryGit—890x310
Press enter to submit commands
> git init
Initialized empty Git repository in /.git/ 
Success!
$
The pitch is simple: learn git in 15 minutes. This website simulates a console, 
and walks you though the steps of collaborative editing. The interface is very 
stylish and the intention is worthy. The only downside is for the git allergic. 
But it is definitely a good skill to have, and a good place to learn it.
5. Shortcutfoo.com

shortculFco
Learn shortcuts. Work faster.
Sign In Register
Work faster. Start shortcut drills now using:
Vim Sublime Etnacs TextMate Visual Studio Excel Command Line Photoshop Git
Eclipse ReSharper IntelliJ IDEA RubyMine AppCode PyCharm WebStorm/PhpStorm Gmail
STTweet 3.024 
g.1 
1.1k
Build Muscle Memory
Become a shortcut ninja through interactive 
^j^iing and repetition. With thousands of 
at your fingertips and interactive drills 
quickly become ingrained so vour
Customize Your Training
Add your own shortcuts and create drills specific 
to your needs. The customizable features of 
shortcutFoo allow you to hone in on those 
commands most prevalent to you. and can be
Interval Training newl
Intervals use spaced repetition training to help 
you memorize shortcuts more efficiently. Well 
show you the shortcuts you need to focus on. and 
tell you exactly when to practice to maximize
Contact us!
Another shortcut database, shortcutfoo is a bit more standard in its way to 
present its content to the user, but definitely more straight-forward than funny 
mini-games. The shortcuts of several programs are available and grouped by 
categories. As it might not be super complete for software like Vim, which is 
completely reliant on shortcuts, it is perfect for giving a quick tip or a general 
overview.
6. GitHub Free Programming Books

IP vhf/free-programming-books
® Watch ’271 
* * Star 26,996 V Fork 5,206
Index
• Ada
• Agda
• Alef
• Android
• APL
• Arduino
• ASP.NET MVC
• Assembly Language
o Non-X86
• AutoHotkey
• Autotools
As you might guess from the URL, this is a collection of free online books 
about programming, written collaboratively using git. The content is awesome 
and the authors deserve to be praised for such work. It might not be the 
easiest read at first, but it is one of the most instructive for sure. We can only 
hope that the movement will keep growing.
p .ranch master - free-programming-books / free-programming-books.md
tK MHM5000 4 days ago Merge pull request #1054 from estrabd/master
334 ontnbutoi £ 
< IS Q X 5 A E 3 > 77 R X 
S in« o»iers
gfile 1787 lines (1456 sloe) 117.913kb
Edit Raw Blame History Delete
7. Collabedit.com

If you ever plan on giving a phone interview, you should check out collabedit 
beforehand. It allows you to create a document, select the programming 
language that you want to write in, and then share that document via the 
URL. The people opening the link will be able to freely interact in real time 
with the text, allowing you to judge their programming skills or just exchange 
snippets. It even comes with the proper syntax highlighting and a chat 
widget. In other words, it is the instant-Google Document of programmers.
8. Cpp.sh

C++ shell
cpp.sh
online C++ compiler
1 
// Example program
2
3
4
5
#include <iostream>
#include <string>
6’ {
int main()
7
8
9
10
H } 
12
std::string name;
std::cout « "What is your name? ";
getline (std::cin, name);
std::cout « "Hello, " « name « "!\n";
Get URL
compilation execution
Compilation successful
This is one of those websites that extend beyond just Linux, but it is so useful 
that it deserves its place here. In short, an online development environment 
for C++. Just write your code in your navigator and run it. As a bonus, you 
get an auto-indentation feature, ctrl+z, and the possibility to share the URL 
with your buddy. This is just one of those crazy things that you can do from a 
simple browser.
9. Copy.sh

In continuation with crazy things that you can do from your browser, copy.sh 
lets you run a virtual machine online. Just that. It gained fame relatively 
recently, but the idea is just insane. From the navigator you can select among 
the defaults virtual images to run, or upload your own iso file. The code for 
that feat has been shared on GitHub. Just amazing.
10. Commandlinefu.com

commandlinefu.com
type here to grep the archive...
Random | Hot this week | All-time greats • FAQ | API | Widget
All commands from
all time
sorted by 
E
What’s this? 
Hide
commandhnefu.com is the place to record those command­
line gems that you return to again and again.
Delete that bloated snippets file you've been using and share 
your personal repository with the world. That way others can 
gain from your CLI wisdom and you from theirs too. All 
commands can be commented on, discussed and voted up or 
down.
Terminal - Alt commands - 11,581 results
Smknod replypipe p; nc -k -Ip 1234 < replypipe[ nc 
/var/runZmysocket.sock > replypipe 
tiny proxy in shell to receive on port and write 
p Up
on unix socket 
Down
* Make a FIFO file named replypipe 
* listen on 1234 
* pass the request to unix socket 
• unix socket will reply to replypipe 
* replypipe will write reply to the client
Comments (0) [ Add to favourites ] Report as malicious 1 Submit alternative | Report as a duplicate 
$ ink
tiny proxy in shell to receive on port and write 
Q Up
on unix socket 
Down
Comments (01 | Add to favourites | Report as malicious | Submit alternative | 
Report as a duplicate
$man -Peat urxvt | \ # see description for full command 
Print all available rxvt resources and their 
n Up
comments 
Down
man -Peat urxvt | sed -n 7th: b/./'B/p’Ised ^djsed 7" \[7\][a-z]/s/A */ A/g’ | sed - 
e :a -e 'N;s/\n/@@/g;ta;P;D' | sed’st\"M['@]\+\)®
*[\t
 ]’\(["\']V\)rJ \2\n!
URxvt‘\1\n\n,g' | sed \+\|An\1,g’ | sed ■$,©
*$,
 ,<j' I sed 
| tr-dT"
Prints a precompiled list of options to add to your Xresources.
-U
2014-07-19 18:17:35
User:
Functions:
Tags: 
socket proxy 
http_proxy
2014-07-19 18:13:52 
User:
2014-07-16 23:14:14
User:
Functions:
Tags: 
x resources rxvt
If you have a new feature suggestion or find a bug, please 
get in touch via http://commandlinefu.uservoice.com/
Get involved!
You can sign-in using OpenlD credentials, or register a 
traditional username and password.
OpenlD or Sign in Register
First-time OpenlD users will be automatically assigned a 
username which can be changed after signing in.
Stay in the loop...
Follow the Tweets.
Every new command is wrapped in a tweet and 
posted to Twitter. Following the stream is a great 
way of staying abreast of the latest commands. 
For the more discerning, there are Twitter
Hide
accounts for commands that get a minimum of 3 and 10 votes 
- that way only the great commands get tweeted.
>■ http://twitter.com/commandlinefu
- http://twitter.com/commandlinefu3 
- http://twitter.com/commandlinefu 10
Subscribe to the feeds.
We all keep a big snippet of command-line gems on our computer. 
commandlinefu's goal is to release those snippets to the world. As a 
collaborative database, it resembles the Wikipedia of the command line. 
Everyone is free to register and post their favorite command on the website 
for everyone else to see. You will then be able to access that knowledge from 
everywhere and share it with everyone. If you are interested in mastering the 
shell, commandlinefu also proposes great features like random commands 
and a news feed to learn something new every day.
11. Alias.sh

Another collaborative database, alias.sh (I love the URL) is a bit like 
commandlinefu but for shell aliases. You can share and discover useful 
aliases which will make your CLI experience so much better. I personally 
like the alias to get the dimensions of a picture.
function dim(){ sips $1 -g pixelWidth -g pixelHeight }
All the seconds you save with alias.sh probably accumulate with time, and 
turn to years by the end of your life.
12. Distrowatch.com

€< T>istroWatch.com
--------------------Put the fun back into computing. Use Linux. BSD.
[Type Distribirtion Name 
Go | Select Distribution * Go [
: CiKtom swwh 
Go | 
Random Distribution [
Search
Major Distributions
Submit Distribution
Upcoming Releases
About OistroWatch 
Page Hit Ranking 
Advertise
Related Links
. . . . ...
intni
• Home Page
• DW Weekly. Comments
• Package Management
■ Packages
Get the new release of Linux Mint 17 on a DVD ($5.95) orUSB drive 
($14.95) from OSDisc.com
2X Software
3CX VoIP PBX
Latest News and
Distribution Release: UberStudent 4.0
now supports based
News Filtering Options
Distribution: All 
’ Release: | All »| Month: | All 
* Yean
All ’ Refresh
Advertisement
2014-07-20
Latest Distributions
07/20 UberStudent
*
 4.0
07/20
openSUSE • 13.1.2 
"Edu Li-f-e GNOME 
Classic"
07/19 HandvLinux • 1.6
07/18
07/18
Caixa Magica ■ 22-re
07/18
"Openbox"
Mania rb* 0.8.1D 
"Netbook"
Stephen Ewen has announced the release of 
UberStudent 4.0. an Ubuntu-based Linux distribution that 
includes specialist software for learning and teaching: "I’m 
pleased to announce the release of UberStudent 4.0 
(LTS) 'Socrates’ Xfce edition. UberStudent is a Linux 
distribution for learning, doing and teaching academic 
success at the higher education and advanced secondary 
levels. Much more than just an operating system, 
UberStudent aims to be a complete, ready-to-go out-of- 
the-box learning platform for education that facilitates not 
only immediate user-friendly productivity but cross­
UberStudent platform computer fluency among its users. System: 
Ubuntu 14.04 LTS base: Linux 3.13 kernel; Xfce 4.10; 32-
2X RAS - Application & 
Desktop Publishing and 2X 
MDM ■ Mobile Device 
Management - Hosted or on 
Premise
Free Tech Guides
NEW! HITB Magazine - 
January 2014 Issue
Who does not know Distrowatch? Besides giving a precise ranking of Linux 
distributions based on their website popularity, Distrowatch is also a very 
useful database. Whether you are looking for a new distribution to try, or just 
curious, it presents an exhaustive account of every Linux you can find, with 
information like which default desktop environment it uses, or package 
system, or its default applications. And all the versions, and with easily 
accessible download links. In a word, the Linux database.
13. Linuxmanpages.com

linu/manpagescom
Search For: 
in: LI I Lff! 
KS1mSESS3
Browse Categories
1. General Commands
2. System Calls
3. Subroutines
4. Special Files
5. File Formats
6. Games
7. Macros and Conventions
8. Maintenence Commands
Most Popular Man Pages 
xorg.conf 
mdadm
toolchain-funcs.eclass 
xorgconfig
mDNSResponder 
named.conf
dmraid 
famd.conf
growisofs 
mozplugger
Xorg 
nifd
imapd.conf 
udev
fstab-sync 
dhcpd.conf
modprobe.conf 
fluxbox
mdadm.conf 
mdmpd
LinuxManPages.com © Name.com Links: who.is search | dictionary | ccTLD information
Everything is in the URL: access the manual pages for popular commands 
from anywhere. Not really sure if this would actually be useful for Linux 
users as you can access that from your actual terminal, but the intent is 
remarkable.
14. AwesomeCow.com

Search
Enter the name of a Windows program, 
to see alternatives in Linux.
Program list Find out more about AwesomeCow.com
Support the project: Donate
This is maybe a bit less hardcore Linux, but definitely useful to some. 
Awesomecow is a search engine for finding alternatives to Windows 
software on Linux. It can be helpful for anyone migrating to the penguin, or 
nostalgic of a Windows program. I see this as a strength, showing that Linux 
can compete with the professional spheres when it comes sot software 
quality. Or at least try to.
15. PenguSpy.com

Before Steam started to show up on Linux, gaming was probably one of the 
penguin's weakness. But the website penguspy made the effort of fighting 
that weakness by collecting all Linux compatible games in a database with a 
sexy interface. Games can be sorted by categories, release dates, ratings, etc. 
I really hope that websites like this are not going to disappear because of 
Steam as it remains one of my favorites of this list.
16. Linux Cross Reference by Free Electrons

Finally, for all the experts and the curious, lxr is the anagram from Linux 
Cross Reference, and allows us to interactively view the Linux Kernel code 
online. The navigation is made easy via identifiers, and you can compare the 
different versions of the files with a standard diff markup. The interface is 
sober and straight-forward, and this is just a website that perfectly illustrates 
the concept of open source.
To conclude, there are a lot more websites which deserve to be listed, and 
this might be a topic for a part two to this post. But this is a good start. It 
serves as an appetizer to what can be found online as tools for Linux users. If 
you have any other pages that you would like to share, following this 
thematic, do so in the comments. And maybe contribute to a sequel to this 
list.
Linux Tutorials
How to filter, split or merge pcap 
files on Linux

If you are a network admin who is involved in testing an intrusion detection 
system or network access control policy, you may often rely on offline 
analysis using collected packet dumps. When it comes to storing packet 
dumps, libpcap's packet dump format (pcap format) is the most widely used by 
many open-source packet sniffing and capture programs. If pcap files are used 
as part of penetration testing or any kind of offline analysis, there's often need 
for manipulating pcap files before injecting them into the network.
In this tutorial, I am going to introduce useful pcap manipulation tools and 
show their use cases.
Introduce Editcap and Mergecap
Wireshark , the most popular GUI-based packet sniffer, actually comes with a 

suite of very useful command-line tools. Among them are editcap and mergecap. 
The former is a versatile pcap editor which can filter or split a pcap file in 
various fashions. The latter allows you to merge multiple pcap
files into one. This tutorial is based on these Wireshark CLI tools.
If you already have Wireshark installed, these tools are already available for 
you. If not, go ahead and install Wireshark command-line tools on Linux. 
Note that on Debian-based distributions, you can install Wireshark 
command-line tools without installing Wireshark GUI, while on Red Hat 
based distributions, you need to install the whole Wireshark package.
Install Wireshark Command-line Tools on Debian, 
Ubunu or Linux Mint
term
T $ sudo apt-get install wireshark-common
Install Wireshark Command-line Tools on Fedora, 
CentOS or RHEL
term
lb_ $ sudo yum install wireshark
Once you install Wireshark CLI tools, you can start using editcap and
mergecap tools.
Filter a Pcap File
editcap allows you to filter an input pcap file in various fashions, and save the 
result in a new pcap file.
First of all, you can filter an input pcap file based on start time and/or end 
time. -A <start-time> and -B <end-time> options are used to capture only those 
packets whose arrival time falls within a specific time range (e.g., between

2:30pm and 2:35pm). The time format to use is yyyy-mm-dd hh:mm:ss.
term
■f_ $ editcap -A '2014-12-10 10:11:01' -B '2014-12-10 10:21:01' input.pcap 
output.pcap
If you want to extract specific N packets from an input pcap file, you can also 
do that. The command below extracts 100 packets (from 401 to 500) from 
input.pcap and save them as output.pcap:
term
T $ editcap input.pcap output.pcap 401-500
If you want to filter out duplicate packets in a pcap file, use -D <dupwindow> 
option. This will compare each packet against the previous (<dupwindow> - 1) 
packets in terms of packet length and MD5 hash, and discard the packet if 
any match is found.
term
$ editcap -D 10 input.pcap output.pcap
37568 packets seen, 1 packets skipped with duplicate window of 10
packets.
Alternatively, you can define <dup-window> in terms of time interval. If you use 
-w <dup-time-window> option, it will compare each packet against all the packets 
which arrived within <dup-time-window> seconds to determine its duplicity.
term
lb_ $ editcap -w 0.5 input.pcap output.pcap

50000 packets seen, 0 packets skipped with duplicate time window
equal to or less than 0.500000000 seconds.
Split a Pcap File
editcap can be also useful if you want to split a large pcap file into multiple 
smaller pcap files.
To split a pcap file into multiple pcap files of the same packet count:
xterm
T $ editcap -c <packets-per-file> <input-pcap-file> <output-prefix>
__ l^lEach output pcap file will have the same packet count, and be named 
as <output-prefix>-nnnn.
To split a pcap file into multiple pcap files with the same time interval:
xterm
lb_ $ editcap -i <seconds-per-file> <input-pcap-file> <output-prefix>
Merge Pcap Files
If you want to combine multiple pcap files into one, mergecap is handy. 
When combining pcap files, mergecap, by default, relies on per-packet 
timestamp information in pcap files to sort packets in chronological order.
xterm
lb_ $ mergecap -w output.pcap input.pcap input2.pcap [input3.pcap . . .]
—■■If you want to ignore timestamp information, and simply merge 
multiple pcap files in their order in the command line, use -a option.
For example, the following command will write all packets from input.pcap to 
output.pcap, followed by all packets in input2.pcap.
term

$ mergecap -a -w output.pcap input.pcap input2.pcap
Summary
In this tutorial, I presented several use cases of pcap file manipulation using 
editcap and mergecap. Besides these, there are other pcap related tools 
out there, for example, reordercap for reordering packets, text2pcap for 
text to pcap conversion), pcap-diff for performing diff on pcap files, 
etc. Some of these pcap tools can be really handy along with packet 
injection tools for network penetration testing and various network 
troubleshooting purposes, so better to know they exist!
Do you use any pcap tool out there? If so, what is your use case?
Linux Tutorials
How to get the process ID (PID) of a 
shell script
Question: I want to know the process ID (PID) of the subshell under which 
my shell script is running. How can I find a PID in a bash shell script?
When you execute a shell script, it will launch a process known as a subshell. 
As a child process of the main shell, a subshell executes a list of commands 
in a shell script as a batch (so-called "batch processing").
In some cases, you may want to know the process ID (PID) of the subshell 
where your shell script is running. This PID information can be used under 
different circumstances. For example, you can create a unique temporary file 
in /tmp by naming it with the shell script PID. In case a script needs to 
examine all running processes, it can exclude its own subshell from the 
process list.
In bash, the PID of a shell script's subshell process is stored in a special 

variable called $$. This variable is read-only, and you cannot modify it in a 
shell script. For example: 
#!/bin/bash
echo "PID of this script: $$'
The above script will show the following output.
PID of this script: 6583
Besides $$, bash shell exports several other read-only variables. For example, 
PPID stores the process ID of the subshell's parent process (i.e., main shell). 
UID stores the user ID of the current user who is executing the script. For 
example:
#!/bin/bash
echo "PID of this script: $$"
echo "PPID of this script: $PPID"
echo "UID of this script: $UID"

Its output will be:
PID of this script: 6686
PPID of this script: 4656
UID of this script: 1000
In the above, PID will keep changing every time you invoke a script. That is 
because each invocation of a script will create a new subshell. On the other 
hand, PPID will remain the same as long as you run a script inside the same 
shell.

$ cat my.sh
#!/bin/bash
echo "PID of this script: $$" 
echo "PPID of this script: $PPID 
echo "UID of this script: $UID" 
$ 
$
$ ./my.sh
PID of this script: 7125
PPID of this script: 7108
UID of this script: 1000 
$
For a complete list of built-in bash variables, refer to its man page.
term
__ $ man bash
Linux Tutorials
How to perform BGP traffic 
engineering using Quagga on Linux
The previous tutorials demonstrated how we can turn a CentOS box into a 
BGP router and filter BGP prefixes using Quagga. Now that we understand 
basic BGP configuration, we will examine in this tutorial how to perform 
more advanced traffic engineering on Quagga. More specifically, we will 
show how we can influence the routing path of existing traffic by tuning BGP 
attributes (e.g., local preference).
Routing and Path Selection
In a typical Internet environment where multiple routing paths exist from a 
source to a destination, the actual path taken by traffic is the result of diligent 

traffic engineering which involves multiple factors, including the number of 
router/AS hops in the path, bandwidth capacity, path reliability, congestion in 
the path, and so on.
To be more specific, a routing path chosen by traffic is shaped by individual 
routing decisions made by each intermediate router based on its local routing 
table. The routes stored in the routing table may be statically configured, 
learnt by IGP like OSPF or EIGRP, or learnt by BGP. A single route can be 
learnt by more than one protocol. In such a case, the preferred route depends 
generally on some protocol-specific attribute, for example, prefix length and 
administrative distance. In BGP world, we can influence path decision 
process by tuning BGP attributes such as local preference in BGP routers.
Please note that routing decisions influence forward traffic, i.e., outbound 
traffic that is originated by the router or transit traffic that is forwarded by the 
router.
Path Selection in BGP
In BGP, IPv4 and IPv6 prefixes are propagated globally over the Internet 
through prefix advertisements sent to and received from BGP neighbors. 
When multiple routes are received for a particular prefix, your local BGP 
router will make a decision to forward traffic destinted to that prefix via one 
of the routes. Similarly, a remote router will make its own routing decisions 
based on the prefixes that it learns from others, and some of those prefixes 
may be advertised by yourself. The remote router will send traffic to you if it 
chooses the route you advertised, as the best route for a given prefix.
If you notice closely, the relationship between prefix advertisements and 
traffic can be represented using the following diagram. You can see that the 
traffic flows in the opposite direction of the prefix advertisements.

Prefix
Traffic
BGP is one of the most highly customizable routing protocols. In case the 
same prefix is received from more than one neighbor (with distinct 
routes/paths), many BGP attributes are considered in selecting the best path 
for that prefix, as documented here.
In the following sections, we will discuss how we can tune some of these 
attributes to influence BGP path selection process.
BGP Topology
For this tutorial, we will consider the following topology.


RotuerA exchanges prefixes with two eBGP neighbors, RouterB and RouterC.
RouterD also has eBGP peering with RouterB and RouterC, and exchange 
prefixes with them. For traffic between RouterA and RouterD, we will 
consider the following requirements.
We can modify the configuration of RouterA only, and none of the other 
routers is under our control.
For outgoing traffic towards RouterD, RouterA should prefer the path through 
RouterC. In case this path is unavailable, the path through RouterB will be 
used.
For incoming traffic from RouterD, RouterA wants to load balance the traffic 
between both paths. In case one path fails, all traffic will be carried by the 
other link.
To keep this tutorial simple, we will not be going into the details of BGP 
configuration. I am providing the summary of the configuration for reference.

Influencing Outgoing Traffic with Local Preference
The route taken by outbound traffic from RouterA will depend on the prefixes 

it receives from RouterB and RouterC. While we assume that we are allowed 
to configure RouterA only, we need to have RouterD advertise prefixes to 
RouterA for the purpose of demonstration. We will start by configuring 
RouterD to advertise its own prefixes.
xterm
router-d# conf t
router-d(config)# router bgp 400
router-d(config-router)# network 172.16.1.0 mask 255.255.255.0
Please note that AS200 and AS300 are transit networks. Since there is no active 
prefix filter defined, these networks will forward all prefixes that they learn 
to their neighbors.
Now from RouterA's perspective, it receives a prefix 172.16.1.0/24 via both 
neighbors AS 200 and AS 300. The *>  in the output indicates that the preferred 
path, which is through AS 200, RouterB.

Local preference is a BGP attribute that can be used to influence outbound 
traffic path. It is also one of the first attributes that is checked during path 
selection. By design the route with the highest local preference value is the 
most preferred route. The default local preference is 100.
Be aware that local preference is local to each Autonomous System. That is, 
local preference values are shared only among routers in the same AS, and 
never exposed to other neighboring Autonomous Systems.
Now we will increase local preference to 200 for the routes received from 
RouterC. We will create a route-map in RouterA and use it to modify local 
preference.
xterm
router-a# conf t 

router-a(config)# route-map SET-LP permit 10
router-a(config-route-map)# set local-preference 200 router-a(config-route- 
map)# exit
term
router-a(config)# router bgp 100 
router-a(config-router)# neighbor 10.10.13.3 route-map SET-LP in
The above commands create a route-map named set-lp. The sequence 10 of the 
route-map is a permit statement. As there is no specific match clause, the 
statement will match all prefixes. The set clause will set the local prefix of all 
prefixes to 200, which is higher than the default value. We then call this route­
map within BGP configuration and apply it in the inbound direction for 
neighbor 10.10.13.3, RouterC.
Let's verify that the changes have taken effect.

We can interpret the above output as follows.
The path through RouterC (AS 300) is the preferred path due to higher local 
preference value.
The route through RouterB (AS 200) is still being learnt, has the default local 
preference of 100.
If, for some reason, RouterC goes down, the path through RouteB will be used 
as a backup.

This confirms that we have successfully configured outbound traffic policy to 
match the requirement. traceroute probing should confirm the same.
router-a# traceroute 172.16.1.0
traceroute to 172.16.1.0 (172.16.1.0), 30 hops max, 60 byte packets 
1 10.10.13.3 (10.10.13.3) 1.127 ms 1.021 ms 1.002 ms|
Note: If your ping/traceroute is not working, make sure that you have enabled 
packet forwarding in all four routers.
Load Balancing Incoming Traffic with Selective 
Prefix Advertisements
As far as inbound traffic is concerned, of course we cannot directly 
manipulate remote routers outside the local AS to influence inbound traffic 
sent by them. Instead, incoming traffic to an AS can be indirectly influenced 
by the prefixes that the AS advertise to the world. Remember that the routing 
tables of remote routers are populated with the prefix advertisements they 
receive. Thus by selectivey advertising prefixes from our local RouterA, we 
can influence the routing decision of the neighboring RouterD.
One key factor during route selection in any routing protocol (RIP, OSPF, 
BGP, IS-IS) is the prefix length. The route with the longest prefix is always 
the best route, regardless of any protocol-specific administrative distance, 
attribute, or metric. For example, a prefix with /27 mask is always preferred 
over /24 mask as it has a longer prefix length.
We will utilize this characteristic of route selection to load balance RouterA's 
incoming traffic through RouterB and RouterC. Let us look at the 
prefix that AS 100 owns, and see how we can break it up.
Actual Prefix Prefix Broken Down /22 /23 /24 
192.168.0.0/23 192.168.0.0/24 
192.168.0.0/22 
192.168.1.0/24
192.168.2.0/23 192.168.2.0/24
192.168.3.0/24

In a lab environment, you can use any prefix length that you want including 
/32. However, in production environments like publicly routable prefixes, 
the maximum length of prefix that is allowed is up to /24. For this simulation, 
we will advertise the /24 and /22 prefixes using the following policy.
Advertise first two /24 to AS 200
Advertise the other two /24 to AS 300
Advertise entire /22 to both AS 200 and AS 300
1. Load Balancing and Fall Back Selection
The following is the prefix selection process on RouterA, which leads to load 
balancing its incoming traffic.
The /24 prefixes are the most specific routes as they have the maximum 
subnet mask length. So the preferred path to the first two /24 prefixes would 
be through AS 200, and for the latter two /24 prefixes it would be through AS 
300.
The /24 prefixes are part of the entire /22 prefix. If, for some reason, RouterD 
does not receive /24 advertisements from either neighbor, it will remove the 
route from its routing table. In that case, the only available reference to that 
particular /24 would be through /22. For example, if RouterD stops receiving 
the prefix 192.168.3.0/24, the route will be removed from its routing table. If the 
router has some traffic for this network, the closest available match is 
192.168.0.0/22, which it is receiving from both neighbors. So traffic can still be 
routed to the destination network.
2. Creating Prefix Lists
router-a(config)# ip prefix-list AS200_PRFX_OUT deny 192.168.2.0/23 
router-a(config)# ip prefix-list AS200_PRFX_OUT deny 192.168.2.0/24 
router-a(config)# ip prefix-list AS200_PRFX_OUT deny 
192.168.3.0/24 

router-a(config)# ip prefix-list AS200_PRFX_OUT permit 192.168.0.0/22 le 
24
The above commands will create a prefix-list called as200_prfx_out that will 
deny the specific /23 and /24 prefixes, while allowing all other prefixes within 
the 192.168.0.0/22 subnet as long as the prefix length is up to /24. We will create 
a similar prefix-list for the other /24 prefixes.
term
router-a(config)# ip prefix-list AS300_PRFX_OUT deny 192.168.0.0/23
router-a(config)# ip prefix-list AS300_PRFX_OUT deny 192.168.0.0/24
router-a(config)# ip prefix-list AS300_PRFX_OUT deny 192.168.1.0/24 
router-a(config)# ip prefix-list AS300_PRFX_OUT permit 192.168.0.0/22 le 
24
3. Creating Route-Maps
We will call upon the prefix-lists within route-maps and apply them in the BGP 
configuration.
xterm
router-a(config)# route-map AS200_RMAP_OUT permit 10 router-a(config- 
route-map)# match ip address prefixlist AS200_PRFX_OUT
router-a(config-route-map)# exit
xterm
router-a(config)# route-map AS300_RMAP_OUT permit 10 router-a(config- 
route-map)# match ip address prefix

list AS300_PRFX_OUT
router-a(config-route-map)# exit
The above commands create two route-maps that allow prefixes that are
matched by the prefix-lists that we created earlier. 
xterm
router-a(config)# router bgp 100
router-a(config-router)# neighbor 10.10.12.2 route-map AS200_RMAP_OUT 
out 
router-a(config-router)# neighbor 10.10.13.3 route-map AS300_RMAP_OUT 
out
In the above BGP configuration, we specify that the outbound prefixes 
advertised towards the neighbors in AS 200 and AS 300 must be filtered 
through the route-maps that we have just created.
4. Advertising the Prefixes
Now we will advertise the prefixes within BGP configuration.
term
router-a(config-router)# router bgp 100
router-a(config-router)# network 192.168.0.0 mask 255.255.255.0
router-a(config-router)# network 192.168.1.0 mask 255.255.255.0
router-a(config-router)# network 192.168.2.0 mask 255.255.255.0
router-a(config-router)# network 192.168.3.0 mask 255.255.255.0
router-a(config-router)# network 192.168.0.0 mask 255.255.252.0
5. Verifying the Advertisements

We will verify whether the prefixes are advertised as well as received 
properly by using the following commands.
For advertised routes:
xterm
# show ip bgp <neighbor-IP-address> <advertised-routes>
_L^Jpor received routes:
xterm
lb_ # show ip bgp <neighbor-IP-address> routes
# show ip bgp
_J^®The following screenshot confirms that the routes are being 
advertised and received properly.

router-a# show ip bgp neighbors 16,10,12,2 advertised-routes
BGP table version is 0, local router ID is 10,10,13,1
Status codes: s suppressed, cl damped, h history, ‘ valid, > best, i ■ internal, 
r RIB-failure, S Stale, R Removed
Origin codes: i ■ iIGP, e ■ EGP, ? - incomplete
Network
•> 192.168.0.0/22
•> 192.168.0.0
192.168.1.0
Next Hop 
Metric LocPrf Weight Path
13,13,12,1 
3 
32168 1
13,13,12,1 
3 
32163 1
13,13,12,1 
3 
32168 i
Irouter-d# show ip bgp
BGP table version is 0, local router ID is 10.10.34.4
Status codes: s suppressed, d damped, h history, 1 valid, > best, i ■ internal, 
r RIB-failure, S Stale, R Removed
Origin codes: i ■ IGP, e ■ EGP, 1 ■ incomplete
Total number of prefixes 3
router-a#
router-a#
router-a# show ip bgp neighbors 10,10,13,3 advertised-routes
BGP table version is 3, local router ID is 10,10,13,1
Status codes: s suppressed, :l damped, h history, ' valid, > best, i - internal, 
r RIB-failure, S Stale, R Removed
Total number of prefixes 3 
router-a# |
Origin codes: i ■IGP, e ■ EGP, ? ■ incomplete
Network 
b 192.168.0.0/22 
•> 192.168.2.0 
•> 192.168.3.0
Next Hop 
Metric LocPrf Weight Path
13,13,13,1 
3 
32168 1
10.10,13.1 
3 
32168 1
13,13,13,1 
3 
32168 i
Network
'*>  172,16,1,0/24 
l.
Next Nop 
0,3,0,3
Metric LocPrf Weight Path 
0 
32768 1
|>
TUT"
18.18.24.2
0 390 190 1
_ _ _ _ _ _ _ _ _ _ _ _ _ 08 190 i
Both AS
'> 192,168,"
'> 192,168,1,0
18,18,24,2
10,10,24,2
0™ 0 “01 
0 290 190 1
AS200
rainr-
> 192,168,3,0
TITO
10,10,34,3
0 390 190 1
0 399 190 1 ASM
Total number of prefixes 6 
router-d# |
Advertised routes 
from Router-A
Received routes at
Router-D
• Wife?
6. Testing BGP Fallback
To test whether fallback mechanism works, we will shut down the BGP

peering within RouteA and Routerc.
I______^^Ixterm
T__ router-a(config)# router bgp 100
router-a(config-router)# neighbor 10.10.13.3 shutdown
10.10.12.2
. . .
. . .
0.10.24.
0.10.24.
'> 192.168.0.0
*> 92. 68.1.0
0 g joa
router-a# show ip bgp Himary
3GP router identifier 10.10,13.1. local AS number 100
TIB entries 8, using 1000 bytes of memory
]eers 2, using 9120 bytes of memory
1 AS MsgRcvd MsgSent
Total number of neighbors 2
router-d# show ipogp
3GP table version is 0, local router 10 is IB.10.34.4
Status cooes: s suppressed, d damped, n history, * valid, > best, i ■ internal,
r RIB-failure, S Stale, R Removed
TblVer InOOutQUp/Down State/PfxRc
0 0 0 00:02:07 Idle Admin
'> 172.16.1.0 24
•> 192.168.0.0 22
Total number of prefixes 4
As we can see, the route /22 is still being learnt through AS 200. We can use 
traceroute to verify that the traffic is taking a backup path.________________
[rooWouter-d -]# vtysh
Hello, this is Quagga (version 0.99.22.4).
Copyright 1996-2005 Kunihiro Ishiguro, et al.
router-d# traceroute 192.163.3.0
traceroute to 192.168.3.0 (192.168.3.0), 30 hops max, 60 byte packets
1 10.10.24.2(10.10.24.2) 0.674ms 
0.538ms 
0.426ms
2 10.10.12.1 (10.10.12.1) 1.513 
ms 1.516 
ms 1.319 ms
3 ji » *
4 
  
* * *
Note: If your ping/traceroute is not working, make sure you have enabled packet 
forwarding in all four routers.
7. Summary Configuration
For your reference, here is the final configuration of all four routers.

RouterA:
router bgp 100
network 192.168.0.0/22
network 192.168.0.0/24
network 192.168.1.0/24
network 192.168.2.0/24
network 192.168.3.0/24
neighbor 10.10.12.2 remote-as 200
neighbor 10.10.12.2 route-map AS200_RMAP_OUT out neighbor 10.10.13.3 
remote-as 300
neighbor 10.10.13.3 route-map SET-LP in
neighbor 10.10.13.3 route-map AS300_RMAP_OUT out !
ip prefix-list AS200_PRFX_OUT seq 5 deny 192.168.2.0/23 ip prefix-list
AS200_PRFX_OUT seq 10 deny 192.168.2.0/24 ip prefix-list
AS200_PRFX_OUT seq 15 deny 192.168.3.0/24 ip prefix-list
AS200_PRFX_OUT seq 20 permit 192.168.0.0/22 le 24
ip prefix-list AS300_PRFX_OUT seq 5 deny 192.168.0.0/23
ip prefix-list AS300_PRFX_OUT seq 10 deny 192.168.0.0/24 ip prefix-list
AS300_PRFX_OUT seq 15 deny 192.168.1.0/24 ip prefix-list
AS300_PRFX_OUT seq 20 permit 192.168.0.0/22 le 24 !
route-map SET-LP permit 10
set local-preference 200

route-map AS200_RMAP_OUT permit 10
match ip address prefix-list AS200_PRFX_OUT 
route-map AS300_RMAP_OUT permit 10
match ip address prefix-list AS300_PRFX_OUT

RouterB:
router bgp 200
neighbor 10.10.12.1 remote-as 100
neighbor 10.10.24.4 remote-as 400
RouterC:
router bgp 300
bgp router-id 10.10.34.3
neighbor 10.10.13.1 remote-as 100
neighbor 10.10.34.4 remote-as 400
RouterD:
router bgp 400
network 172.16.1.0/24
neighbor 10.10.24.2 remote-as 200
neighbor 10.10.34.3 remote-as 300

Conclusion
To sum up, we have demonstrated some techniques of BGP traffic 
engineering to influence inbound and outbound traffic. If we know how we 
want to route traffic or prepare backup routes for redundancies, there are a lot 
of options to make it work using BGP, e.g., weight, local preference, ASpath 
prepend, communities, MED, etc. Traffic engineering can be done using 
other protocols as well. Remember that the core of proper traffic engineering 
is planning. At the end of the day, it is planning that is the most important 
phase. Executing the plan (as demonstrated in this tutorial) is what comes 
next.
Hope this helps.
Linux Tutorials
How to install XenCenter on Linux
Citrix XenCenter is client software with GUI for managing XenServer/XCP 
hosts remotely. Using XenCenter, you can create virtual machines (VMs), 
access VM consoles, and configure VM storage and networking. As of this 
writing, Citrix only offers a Windows native client for XenCenter, and they 
don't seem to plan on releasing XenCenter Linux client any time soon. So if 
you would like to install XenCenter on Linux, you need to find an 
alternative to XenCenter on Linux, which is what this post is about.
Fortunately, there is a pretty good open-source alternative to XenCenter on 
Linux, which is called OpenXenManager. It allows users to manage

XenServer and Xen Cloud Platform (XCP) hosts remotely via GUI.
Install OpenXenManager on Linux
For Ubuntu or Debian:
term
$ sudo apt-get install git python-gtk2 glade pythongtk-vnc python-glade2 
python-configobj
$ git clone
https://github.com/OpenXenManager/openxenmanager.git $ cd
openxenmanager
$ sudo python setup.py install
For CentOS, Fedora or RHEL:
xterm
$ sudo yum install pygtk2 gtk-vnc-python rrdtool $ git clone 
https://github.com/OpenXenManager/openxenmanager.git $ cd 
openxenmanager
$ sudo python setup.py install
Manage XenServer Host Remotely with 
OpenXenManager
To launch OpenXenManager, just type the following command: 
xterm
M__ $ openxenmanager

Connect to a remote XenServer host by clicking on Add New Server button on 
the top.
Once you are connected to a remote XenServer host, you will be able to see 
the resources (CPU, memory, storage) available on the host, and access its 
virtual console via OpenXenManager GUI.

To create a guest VM:

To create a new storage repository:

As an open-source clone of XenCenter, OpenXenManager implements pretty 
much the same functionality of XenCenter. The latest OpenXenManager even 
supports Citrix-specific features of XenCenter, such as activating a free 
XenServer license and installing XenServer updates.

Linux Tutorials
How to automatically run a script 
when logging into Ubuntu Desktop
There are circumstances where you wish to have a script run automatically 
when you log in to Ubuntu Desktop. Such a script can configure various 
userspecific or system-wide settings on your Ubuntu system, upon user's 
desktop login.
In Linux, there are start-up scripts named ~/.bash_profile, ~/.bashrc, or ~/.profile 
which get executed when you start a shell. However, in Ubuntu Desktop, 
such start-up scripts get executed when you open up a terminal window, but 
not when you log in to Ubuntu Desktop GUI. Also, when you open multiple 
terminal windows, these kinds of start-up scripts are executed as many times, 
in order to initialize user's shell environment in terminal windows.
If what you want is to run a script at the time of user's Ubuntu Desktop login, 
you can follow this guideline.
Create a XDG configuration file for the start-up script you want to run.
term
T $ vi ~/.config/autostart/my_script.desktop 
[Desktop Entry] 
Type=Application 
Name=My Script
Exec=~/bin/my_custom_script.sh

Icon=system-run
X-GNOME-Autostart-enabled=true
The above XDG configuration file will set up user-specific auto-start. If you 
want all users to use the same start-up script for all users system-wide, create 
a similar XDG configuration file in the following location instead.
term
__ $ sudo vi /etc/xdg/autostart/my_script.desktop 
[Desktop Entry]
Type=Application
Name=My Script
Exec=sudo /sbin/my_custom_script.sh
Icon=system-run
X-GNOME-Autostart-enabled=true

If the start-up script requires sudo access like an above example, you will need 
to set up password-less sudo. Refer to this tutorial for setting up autostart 
services in different types of Linux desktop environments (e.g., GNOME, 
KDE, Xfce, LXDE).
Linux Tutorials
How to set JAVA_HOME 
environment variable automatically 
on Linux
Question: I need to compile a Java program on my Linux box. For that I 
already installed JDK (Java Development Kit), and now I'm trying to set 
java_home environment variable to point to the installed JDK. What is the 
recommended way to set java_home environment variable on Linux?
Many Java programs or Java-based IDE environments require java_home 
environment variable being set. This environment variable is supposed to 
point to the top directory where the Java development kit (JDK) or Java 
runtime environment (JRE) is installed. The JDK contains everything the JRE 
offers, but also provides additional binaries and libraries needed to compile 
Java programs (e.g., compilers, debugger, JavaDoc). While the JDK is 
needed to build Java applications, the JRE alone is sufficient to simply run 
already built Java programs.

When you are trying to set java_home environment variable, the 
complication is that java_home variable will change depending on (1) 
whether you installed JDK or JRE, (2) which version of JDK/JRE you 
installed, and (3) whether you installed Oracle JDK or Open JDK.
So whenever your build or run-time environment changes (e.g., upgrade to a 
newer JDK), you need to adjust java_home accordingly, which is 
cumbersome.
The following export commands will allow you to set java_home environment 
variable automatically regardless of these factors.
If you installed JRE:
export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
If you installed JDK:
export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which javac))))
Place either of the above commands in ~/.bashrc (or /etc/profile) depending on you 
installed JDK or JRE, and it will set java_home permanently.
Note that readlink -f command is used to get the canonical path since java or javac 
can be set up with multiple symlinks.
For example, if you installed Oracle JRE 7, the first export command will 
automatically set java_home to:
/usr/lib/jvm/j ava-7-oracle/jre
If you installed Open JDK version 8, the second export command will set 
java_home to:

/usr/lib/jvm/java-8-openjdk-amd64
In short, these export commands will automatically update java_home variable 
as you re-install/upgrade your JDK/JRE packages or change default Java 
version. No need to adjust java_home manually.
Linux Tutorials
How to compress and minify CSS 
and JavaScript from the command 
line
For web designers, publishers and developers, one cannot stress enough the 
importance of keeping web pages or web applications as lightweight as 
possible. There are various design strategies and client/server-side techniques 
that can be exploited to achieve this goal.
One relatively simple and straightforward technique is " code minification". 
This is the process of eliminating unnecessary characters (e.g., white space, 

new lines, comments, optional delimeters) from source code, thereby 
"compressing" the size of source code at the cost of human readability. CSS 
or JavaScript code is commonly minified to reduce the amount of 
usertransferred bytes from a server.
The most well-known tool to minify CSS and JavaScript is YUI Compressor, 
which is part of YUI (Yahoo! User Interface) JavaScript library written by 
Yahoo! developers. Common use cases of YUI Compressor are in 
conjunction with other build tools such as Apache Ant or Maven, where web 
applications are built automatically minified with the help of YUI 
Compressor.
YUI Compressor can also be used standalone to compress CSS/JavaScript 
files manually. In this tutorial, I will describe how to compress CSS or 
JavaScript from the command line on Linux by using YUI Compressor.
Install YUI Compressor on Linux
The YUI Compressor is written in Java, and its pre-built JAR file (as well as 
its source code) is available for download. Here we are going to use the JAR 
executable.
First, install Java run-time, which is required to run YUI Compressor.
Download and install YUI Compressor as follows.
xterm
$ sudo mkdir -p /usr/share/java
$ sudo wget
https://github.com/yui/yuicompressor/releases/download/ 
v2.4.8/yuicompressor-2.4.8.jar -P /usr/share/java
J^BCreate a start-up script named yc, with the following content. 
xterm
T $ sudo vi /usr/bin/yc
_!■#! /bin/sh

java -jar /usr/share/java/yuicompressor-2.4.8.jar "[email protected]"
xterm
$ sudo chmod +x /usr/bin/yc
L Inow you can use yc command to launch YUI Compressor. 
xterm
$ yc
_J^BvUICompressor Version: 2.4.8
Usage: java -jar yuicompressor-2.4.8.jar [options] [input file]
Global Options
- V, --version Print version information
- h, --help Displays this information
- -type Specifies the type of the input file
- -charset Read the input file using
- -line-break Insert a line break after the specified column number
- v, --verbose Display informational messages and warnings
-o Place the output into . Defaults to stdout. Multiple files can be processed 
using the following syntax:
java -jar yuicompressor.jar -o '.css$:-min.css' *.css  java -jar
yuicompressor.jar -o '.js$:-min.js' *.js
JavaScript Options
--nomunge Minify only, do not obfuscate
--preserve-semi Preserve all semicolons

--disable-optimizations Disable all micro optimizations
If no input file is specified, it defaults to stdin. In this case, the 'type' option is 
required. Otherwise, the 'type' option is required only if the input 
file extension is neither 'js' nor 'css'.
Compress CSS or JavaScript from the Command

Line
To compress or minify a CSS file, use the following command:
xterm
T $ yc default.css -o default-min.css
L IC.beck how much size is reduced before and after CSS compression:
xterm
T $ wc default.css default-min.css
__ 756 1097 14007 default.css 
0 163 5745 default-min.css
In this example, the total byte count for the original CSS file is reduced from 
14KB to 5.7KB after compression.
A compressed/minified CSS file looks like the following:
html, body {height: 1 
} body {ma rgintG;padding : backg r
,sans’Serif;fontsize: 
;color:rgba(
padding: ;font-weight:( y; color:;. >}p,ol,ul{marc 
t-style:none}p{line-height:1 -}a{color:^ }a:hc 
ntainer{margin:< autojwidth: 
}form label{displc
in-bottom:. }form . submit {ma rg in-top: _■ jline-heic 
m input.Text,form select,form textarea{position:rel? 
; display: block; border: ■; backg round: I ;backgroun Hr 
;border-radius;. 
:i; margin:: ;padding:
. rgba( , , , 
); jorder: solid rgba( , ,
ease-in-out;-webkit-transition: jI . ease-in- 
ase-in-out;-ms-transitlon:al . ease-in-out;trans 
ont-size:l -ijoutline; [form input,text:focus,form se 
us box-shadow: 
^background: 
fc
form ,formerize-placeholder{color:?i !important}fc 
er{color: 
!important}form :-moz-placeholder{colc
z-placeholder color: 
tarrjform :-ms-mput-

To compress or minify JavaScript, you can proceed similarly.
term
lb_ $ yc sample.js -o sample-min.js
Linux Tutorials
What are useful Bash aliases and 
functions
As a command line adventurer, you probably found yourself repeating the 
same lengthy commands over and over. If you always ssh into the same 
machine, if you always chain the same commands together, or if you 
constantly run a program with the same flags, you might want to save the 
precious seconds of your life that you spend repeating the same actions over 
and over.
The solution to achieve that is to use an alias. As you may know, an alias is a 
way to tell your shell to remember a particular command and give it a new 
name: an alias. However, an alias is quickly limited as it is just a shortcut for 
a shell command, without the ability to pass or control the arguments. So to 
complement, bash also allows you create your own functions, which can be 
more lengthy and complex, and also accepts any number of arguments.
Naturally, like with soup, when you have a good recipe you share it. So here 
is a list with some of the most useful bash aliases and functions. Note that most 
useful is loosely defined, and of course the usefulness of an alias is dependent 
on your everyday usage of the shell.
Before you start experimenting with aliases, here is a handy tip: if you give 
an alias the same name as a regular command, you can choose to launch the 
original command and ignore the alias with the trick:
term
lb_ command

__ B^MFor example, the first alias below replaces the is command. If you 
wish to use the regular ls command and not the alias, call it via:
Productivity
So these aliases are really simple and really short, but they are mostly based 
on the idea that if you save yourself a fraction of a second every time, it 
might end up accumulating years at the end. Or maybe not.
alias ls="ls --color=auto"
Simple but vital. Make the ls command output in color. 
alias ll = "ls --color -al"
Shortcut to display in color all the files from a directory in a list format. 
alias grep='grep --color=auto'
Similarly, put some color in the grep output. 
mcd() { mkdir -p "$1"; cd "$1";}
One of my favorite. Make a directory and cd into it in one command: mcd 
[name].

cls() { cd "$1"; ls;}
Similar to the previous function, cd into a directory and list its content: cls 
[name].
backup() { cp "$1"{,.bak};}
Simple way to make a backup of a file: backup [file] will create [file].bak in the 
same directory.
md5check() { md5sum "$1" | grep "$2";}
Because I hate comparing the md5sum of a file by hand, this function computes 
it and compares it using grep: md5check [file] [key].
, --------------------------------------------------------------------------------------------
> md5check trick.mp4 697013477cbf63e35bf69770e2b93af1 
697013477cbfb3e35bf69770e2b93afI t rick.mp4
alias makescript="fc -rnl | head -1 >"
Easily make a script out of the last command you ran: makescript [script.sh] 
alias genpasswd="strings /dev/urandom | grep -o '[[:alnum:]]' | head -n 30 | tr 
-d 'n'; echo"

Just to generate a strong password instantly.
- > genpasswd
h2ijM2YwKFhGxItVrD9L0d64wAip5H
alias c="clear
Cannot do simpler to clean your terminal screen. 
alias histg="history | grep"
To quickly search through your command history: histg [keyword] 
alias ..='cd ..'
No need to write cd to go up a directory. 
alias ...='cd ../..'
Similarly, go up two directories.
extract() {
if [ -f $1 ] ; then
case $1 in
* .tar.bz2) tar xjf $1 ;;
* .tar.gz) tar xzf $1 ;;
* .bz2) bunzip2 $1 ;;
* .rar) unrar e $1 ;;
* .gz) gunzip $1 ;;
* .tar) tar xf $1 ;;
* .tbz2) tar xjf $1 ;;

* .tgz) tar xzf $1 ;;
* .zip) unzip $1 ;;
* .Z) uncompress $1 ;;
* .7z) 7z x $1 ;;
* ) echo "'$1' cannot be extracted via extract()" ;;
esac
else
echo "'$1' is not a valid file"
fi
}


L_________________________________________________I
Longest but also the most useful. Extract any kind of archive: extract [archive file]
System Info
Want to know everything about your system as quickly as possible? 
alias cmount-'mount | column -t"
Format the output of mount into columns.

- I R*  j------------------------ -  - - - - - - - - -
' ■ > cmount 
proc 
on /proc 
type proc
ime) 
sys 
on /sys 
type sysfs
ime) 
dev 
on /dev 
type devtmpfs
228k, nr_inodes=4 94057,mo de=755) 
run 
on /run 
type tmpfs
9=755) 
/dev/mapper/vgroup-root on / 
type ext4
securityfs 
on /sys/kernel/security 
type securityfs
ime) 
tmpfs 
on /dev/shm 
type tmpfs
devpts 
on /dev/pts 
type devpts
d=5,mode=620,ptmxmode=O0O) 
tmpfs 
on /sys/fs/cgroup 
type tmpfs
755) 
cgroup 
on /sys/fs/cgroup/systemd 
type cgroup
ime,xattr, release_agent=/usrAib/systemd/systemd-cgroups-agent,nanie=systemd) 
pstore 
on /sys/fs/pstore 
type pstore
ime) 
cgroup 
on /sys/fs/cgroup/cpuset 
type cgroup
ime.cpuset) 
cgroup 
on /sys/fs/cgroup/cpu,cpuacct 
type cgroup
ime,cpu,cpuacct) 
cgroup 
on /sys/fs/cgroup/memory 
type cgroup
ime, memory) 
cgroup 
on /sys/fs/cgroup/devices 
type cgroup
i............-0 7/ k
i (iv,nosuid>nodev ,noeXeif/'relat 
(rw,nosuid,nodev,noexec,relat 
(i'w,nosuid, relatime,size=1976 
(rw,riosuid,nodev,relatime,in(jd 
(rw,relatimefdata=ordered) 
(rw,nosuid,nodev,noexec,relat 
(™,nosuid,nodev)
(rw,nosuid,noexec,relatime,gi 
(ro,nosuid,nodev,noexec,mode= 
, (rw,nosuid,nodev,noexec,relat
(rw,nosuid,nodev,noexec,relat 
(rw,nosuid,nodev,noexec,relat 
(rw,nosuid,nodev,noexec,relat 
(i>’,nosuid,nodev,noexec, relat 
(rw,nosuid,nodev,noexec,relat
alias tree="ls -R | grep ":$" | sed -e 's/:$// -e 's/[A-][A/]
*//--/g'
 -e 's/A/ /'
-e 's/-/|/'"
Display the directory structure recursively in a tree format. 

sbs() { du -b --max-depth 1 | sort -nr | perl -pe 's{([0-9]+)}{sprintf "%.1f%s", 
$1>=2
**
30?
 ($i/2
**
30,
 "G"): $1>=2
**
20?
 ($1/2
**
20,
 "M"): $1>=2
**
10?
($1/2
**
10,
 "K"): ($1, "")}e';}
Sort by size to display in list the files in the current directory, sorted by their size 
on disk.
alias intercept="sudo strace -ff -e trace=write -e write=1,2 -p"
Intercept the stdout and stderr of a process: intercept [some pid]. Note that you will 
need strace installed.
alias meminfo='free -m -l -t'
See how much memory you have left.
-----------------------------------------------------— - - - - ......... .........
- > meminfo
total
used
free
shared
buffers
cached
Msm:
3867
3035
831
252
160
1110
Low:
3867
3035
831
High:
0
0
0
-/+ buffers/cache:
1765
2102
Swap:
511
0
511
Total:
4379
3035
1343
alias ps? = "ps aux | grep"

Easily find the PID of any process: ps? [name]
alias volume="amixer get Master | sed '1,4 d' | cut -d [ -f 2 | cut -d ] -f 1
Displays the current sound volume.
■ -(")---------------
- > volume
48%
Networking
For all the commands that involve the Internet or your local network, there 
are fancy aliases for them.
alias websiteget="wget --random-wait -r -p -e robots=off -U mozilla" 
Download entirely a website: websiteget [url]
alias listen="lsof -P -i -n"
Show which applications are connecting to the network.

'•■> Isof -P -i -n
FD
TYPE DEVICE SIZE/OFF NODE NAME
COMMAND
PID
USER
dropbox
858 roxas
15u
IPv4 202696
0t0
TCP 209.2.216.248:48651->108.160.165.253:443 (ESTABLISHED)
dropbox
858 roxas
19u
IPv4 60111
0t0
TCP 209.2.216.248:57432->108.160.163.115:80 (ESTABLISHED)
dropbox
858 roxas
23u
IPv4 17571
0t0
UDP 17500
dropbox
858 roxas
26u
IPv4 17574
eto
TCP *:17506  (LISTEN)
chromium
1434 roxas
59u
IPv4 205490
0t0
UDP \5353 
/
thunderbi 10690 roxas
62u
IPv4 188694
0t0
TCP 299.2.216.248:32850->194.117.213.11:143 (ESTABLISHED)
thunderbi 10690 roxas
95u
IPv4 77482
0t0
TCP 209.2.216.248:42534->74.125.22.16:993 (ESTABLISHED) 7
thunderbi 19690 roxas
97u
IPv4 165291
ots
TCP 209.2.216.248:49393->98.139.229.184:993 (ESTABLISHED)
thunderbi 10690 roxas
100u
IPv4 77484
0t0
TCP 209.2.216.248:57831->64.233.171.199:993 (ESTABLISHED)’
thunderbi 10690 roxas
101U
IPv4 77486
0t0
TCP 209.2.216.248:42538->74,125.22.16:993 (ESTABLISHED)
thunderbi 10690 roxas
113u
IPv4 77496
0t0
TCP 209.2.216.248:44588->199.16.156.81:443 (ESTABLISHED)
alias port='netstat -tulanp'
Show the active ports.
gmail() { curl -u "$1" --silent "https://mail.google.com/mail/feed/atom" | sed
-e 's///'}
Rough function to display the number of unread emails in your gmail: gmail
[user name]
alias ipinfo="curl ifconfig.me && curl ifconfig.me/host" 
Get your public IP address and host.
getlocation() { lynx -dump http://www.ip-adress.com/ip_tracer/? 
QRY=$1|grep address|egrep 'city|state|country'|awk '{print 
$3,$4,$5,$6,$7,$8}'|sed 'sip address flag '|sed 'sMy';}

Returns your current location based on your IP address.
Useless
So what if some aliases are not all that productive? They can still be fun.
kernelgraph() { lsmod | perl -e 'print "digraph "lsmod" {";<>;while(<>) 
{@_=split/s+/; print ""$_[0]" -> "$_"n" for split/,/,$_[3]}print "}"' | dot Tpng 
| display -;}
To draw the kernel module dependency graph. Requires image viewer. 
alias busy="cat /dev/urandom | hexdump -C | grep "ca fe""
Make you look all busy and fancy in the eyes of non-technical people.

To conclude, a good chunk of these aliases and functions come from my 
personal .bashrc, and the awesome website like commandlinefu.com which I 
already presented in my post on the best online tools for Linux. So definitely 
go check them out, make your own recipes, and if you are so inclined, share 
your wisdom in the comments.
As a bonus, here is the plain text version of all the aliases and functions I 
mentioned, ready to be copy pasted in your .bashrc.
#Productivity
alias ls="ls --color=auto"
alias ll="ls --color -al"
alias grep='grep --color=auto'
mcd() { mkdir -p "$1"; cd "$1";}
cls() { cd "$1"; ls;}
backup() { cp "$1"{,.bak};}
md5check() { md5sum "$1" | grep "$2";}

alias makescript="fc -rnl | head -1 >"
alias genpasswd="strings /dev/urandom | grep -o '[[:alnum:]]' | head -n 30 | tr
- d 'n'; echo"
alias c="clear"
alias histg="history | grep"
alias ..='cd ..'
alias ...='cd ../..'
extract() {
if [ -f $1 ] ; then
case $1 in
* .tar.bz2) tar xjf $1 ;;
* .tar.gz) tar xzf $1 ;;
* .bz2) bunzip2 $1 ;;
* .rar) unrar e $1 ;;
* .gz) gunzip $1 ;;
* .tar) tar xf $1 ;;
* .tbz2) tar xjf $1 ;;
* .tgz) tar xzf $1 ;;
* .zip) unzip $1 ;;
* .Z) uncompress $1 ;;
* .7z) 7z x $1 ;;
* ) echo "'$1' cannot be extracted via extract()" ;;
esac
else

echo "'$1' is not a valid file" 
fi
}
#System info


alias cmount="mount | column -t"
alias tree="ls -R | grep ":$" | sed -e 's/:$// -e 's/[A-][A/]
*//--/g'
 -e 's/A/ /'
-e 's/-/|/'"
sbs(){ du -b --max-depth 1 | sort -nr | perl -pe 's{([0-9]+)}{sprintf "%.1f%s", 
$1>=2
**
30?
 ($1/2
**
30,
 "G"): $1>=2
**
20?
 ($1/2
**
20,
 "M"): $1>=2
**
10?
($1/2
**
10,
 "K"): ($1, "")}e';}
alias intercept="sudo strace -ff -e trace=write -e write=1,2 -p" alias 
meminfo='free -m -l -t'
alias ps?="ps aux | grep"
alias volume="amixer get Master | sed '1,4 d' | cut -d [ -f 2 | cut -d ] -f 1"
#Network
alias websiteget="wget --random-wait -r -p -e robots=off -U mozilla" alias 
listen="lsof -P -i -n"
alias port='netstat -tulanp'
gmail() { curl -u "$1" --silent "https://mail.google.com/mail/feed/atom" | sed
-e 's/</fullcount.
*/n/'
 | sed -e 's/.
*fullcount>//'}
alias ipinfo="curl ifconfig.me && curl ifconfig.me/host"
getlocation() { lynx -dump http://www.ip-adress.com/ip_tracer/?
QRY=$1|grep address|egrep 'city|state|country'|awk '{print 
$3,$4,$5,$6,$7,$8}'|sed 'sip address flag '|sed 'sMy';}
#Funny
kernelgraph() { lsmod | perl -e 'print "digraph "lsmod" {";<>;while(<>) 
{@_=split/s+/; print ""$_[0]" -> "$_"n" for split/,/,$_[3]}print "}"' | dot Tpng 
| display -;}

alias busy="cat /dev/urandom | hexdump -C | grep "ca fe""
Linux Tutorials
How to check SSH protocol version

on Linux
Question: I am aware that there exist SSH protocol version 1 and 2 (SSH1 
and SSH2). What is the difference between SSH1 and SSH2, and how can I 
check which SSH protocol version is supported on a Linux server?
Secure Shell (SSH) is a network protocol that enables remote login or remote 
command execution between two hosts over a cryptographically secure 
communication channel. SSH was designed to replace insecure clear-text 
protocols such as telnet, rsh or rlogin. SSH provides a number of desirable 
features such as authentication, encryption, data integrity, authorization, and 
forwarding/tunneling.
SSH1 vs. SSH2
The SSH protocol specification has a number of minor version differences, 
but there are two major versions of the protocol: SSH1 (SSH version 1.XX) 
and SSH2 (SSH version 2.00).
In fact, SSH1 and SSH2 are two entirely different protocols with no 
compatibility in between. SSH2 is a significantly improved version of SSH1 
in many respects. First of all, while SSH1 is a monolithic design where 
several different functions (e.g., authentication, transport, connection) are 
packed into a single protocol, SSH2 is a layered architecture designed with 
extensibility and flexibility in mind. In terms of security, SSH2 comes with a 
number of stronger security features than SSH1, such as MAC-based 
integrity check, flexible session re-keying, fully-negotiable cryptographic 
algorithms, publickey certificates, etc.
SSH2 is standardized by IETF, and as such its implementation is widely 
deployed and accepted in the industry. Due to SSH2's popularity and 
cryptographic superiority over SSH1, many products are dropping support for 
SSH1. As of this writing, OpenSSH still supports both SSH1 and SSH2, 
while on all modern Linux distributions, OpenSSH server comes with SSH1 
disabled by default.

Check Supported SSH Protocol Version
Method One: /etc/ssh/sshd_config
If you want to check what SSH protocol version(s) are supported by a local 
OpenSSH server, you can refer to /etc/ssh/sshd_config file. Open /etc/ssh/sshd_config 
with a text editor, and look for Protocol field.
If it shows the following, it means that OpenSSH server supports SSH2 only. 
Protocol 2
If it displays the following instead, OpenSSH server supports both SSH1 and 
SSH2.
Protocol 1,2
Method Two: ssh
If you cannot access /etc/ssh/sshd_config because OpenSSH server is running on a 
remote server, you can test its SSH protocol support by using SSH client 
program called ssh. More specifically, we force ssh to use a specific SSH 
protocol, and see how the remote SSH server responds. The following 
command will force ssh command to use SSH1:
xterm
T $ ssh -1 [email protected]_server
I iThe following command will force ssh command to use SSH2:
xterm
T $ ssh -2 [email protected]_server
_^^BiI' the remote SSH server supports SSH2 only, the first command
with -1 option will fails with an error message like this:

Protocol major versions differ: 1 vs. 2
If the SSH server supports both SSH1 and SSH2, both commands should 
work successfully.
Method Three: scanssh
Another method to check supported SSH protocol version of a remote SSH 
server is to run an SSH scanning tool called scanssh. This command-line tool is 
useful when you want to check SSH protocol versions for a bulk of IP 
addresses or the entire local network to upgrade SSH1-capable SSH servers.
Here is the basic syntax of scanssh for SSH version scanning.
term
T $ sudo scanssh -s ssh -n [ports] [IP addresses or CIDR prefix]
The -n option can specify the SSH port number(s) to scan. You can specify 
multiple port numbers separated by comma. Without this option, scanssh will 
scan port 22 by default.
Use the following command to discover SSH servers on 192.168.1.0/24 local 
nework, and detect their SSH protocol versions:
term
lb_ $ sudo scan -s ssh 192.168.1.0/24

$ sudo scanssh -s ssh 192.168.1.0/24
192.168.1.13:22 SSH-2.0-OpenSSH 6.0
192.168.1.12:22 SSH-2.0-OpenSSH~5.3
192.168.1.11:22 SSH-2.0-OpenSSH 6.6Dl Ubuntu-2ubuntul
192.168.1.8:22 SSH-1.99-0penSSH_6.4
192.168.1.1:22 <retused>
192.168.1.10:22 SSH-2.0-OpenSSH 6.7
192.168.1.100:22 <refused>
192.168.1.6:22 <refused>
192.168.1.251:22 <timeout>
192.168.1.248:22 <timeout>
192.168.1.152:22 <timeout>
192.168.1.184:22 <timeout>
192.168.1.242:22 <timeoiit>
192.168.1.241:22 <timeout>
192.168.1.179:22 <timeout>
192.168.1.162:22 <timeout>
If scanssh reports ssh-1.xx-xxxx for a particular IP address, it implies that the 
minimum SSH protocol version supported by the corresponding SSH server is 
SSH1. If the remote server supports SSH2 only, scanssh will show
ssh-2.0-xxxx.
Linux Tutorials
What are useful netcat examples on 
Linux
Often referred to as the "swiss army of knife" for TCP/IP networking, netcat is 
an extremely versatile Linux utility that allows you to do anything under the 
sun using TCP/UDP sockets. It is one of the most favorite tools for system 
admins when they need to do networking related troubleshooting and 
experimentation.

In this tutorial, I am sharing a few useful netcat examples, although the sky is 
the limit when it comes to possible netcat use cases. If you are using netcat 
regularly, feel free to share your use case.
Note that when you are binding to well-known ports (0-1023) with nc, you 
need root privilege. Otherwise, you can run nc as a unprivileged normal user.
1. Test if a particular TCP port of a remote host is open.
A common use case of netcat is to ping a particular port number and check if 
the port is open or not.
term
■l _$ nc -vn 192.168.233.208 5000 
nc: connect to 192.168.233.208 5000 (tcp) failed: Connection refused 
■l _$ nc -v 192.168.233.208 22
__ ^^^^^^fconnection to 192.168.233.208 22 port [tcp/ssh] succeeded!
SSH-2.0-OpenSSH_6.0p1 Debian-4
2. Send a test UDP packet to a remote host.
The command below sends a test UDP packet with one second timeout to a 
remote host at port 5000.
term
■l _$ echo -n "foo" | nc -u -w1 192.168.1.8 5000

3. Perform TCP port scanning against a remote host.
The command below scans ports in the ranges of [1-1000] and and 3000] to check 
which port(s) are open.
xterm
■__ $ nc -vnz -w 1 192.168.233.208 1-1000 2000-3000__ L_J
4. Copy a file (e.g., my.jpg) from 
 to 
.
hostA.com
hostB.com
On hostB.com (receiver):
xterm
lb_ $ nc -lp 5000 > my .jpg
_L~_J(')n hostA.com (sender):
xterm
lb_ $ nc hostB.com 5000 < my .jpg
5. Transfer a whole directory (including its content) from 
 to 
.
hostA.com
hostB.com
On hostB.com (receiver): 
xterm
H__ $ nc -l 5000 | tar xvf
kJc)n hostA.com (sender):
xterm
lb_ $ tar cvf - /path/to/dir | nc hostB.com 5000
6. Perform UDP port scanning against a remote host.
term
nc -vnzu 192.168.1.8 1-65535
$
Connection to 192.168.1.8 68 port [udp/
*]
 succeeded!

Connection to 192.168.1.8 5353 port [udp/
*]
 succeeded!
Connection to 192.168.1.8 16389 port [udp/
*]
 succeeded!
Connection to 192.168.1.8 38515 port [udp/
*]
 succeeded!
Connection to 192.168.1.8 45103 port [udp/
*]
 succeeded!
The above command checks which UDP port(s) of a remote host are open 
and able to receive traffic.
7. Listen on a UDP port and dump received data in text format.
The command below listens on UDP port for incoming messages (lines of 
text).
lb_ $ nc -u localhost 5000
__ ^^■iNote that this command dies after receiving one message. If you 
want to receive multiple messages, use while loop as follows.
term
T__ $ while true; do nc -u localhost 5000; done
8. Back up a (compressed) hard drive (e.g., /dev/sdb) to a remote 
server.
On a remote server:
term

$ nc -Ip 5000 | sudo dd of=/backup/sdb.img.gz
On a local host with a hard drive:
term
__ $ dd if=/dev/sdb | gzip -c | nc remote_server.com 5000
9. Restore a hard drive from a compressed disk image stored in 
a remote server.
On a local host:
xterm
H__ $ nc -lp 5000 | gunzip -c | sudo dd of=/dev/sdb
kJc)n a remote server with a backup disk image (e.g., /backup/sdb.img.gz): 
xterm
M__ $ cat /backup/sdb.img.gz | nc my_local_host.com 5000__ L-d
10. Serve a static web page as a web server.
Type the command below to launch a web server that serves test.html on port 
8000.
xterm
T__ $ while true; do nc -lp 8000 < test.html; done
Now go to http://<host_ip_address>:8000/test.html to access it. Note that in order to 
use a well known port 80, you will need to run nc with root privilege as 
follows.
$ while true; do sudo nc -lp 80 < test.html; done
11. (Insecure) online chat between two hosts.
On one host (192.168.233.203):
xterm
Bl_ $ nc -lp 5000

_I^MOn another host:
xterm
■l _$ nc 192.168.233.203 5000
__ ^^^After running the above commands, anything typed on either host 
appears on the other host's terminal.
12. Launch a remote shell which allows you run from local host 
any commands to be executed on a remote host.
On a remote host (192.168.233.208): 
xterm
H__ $ nc -lp 5000 -e /bin/bash
Uc)n local host:
xterm
■l _$ nc 192.168.233.208 5000
After running the above command on local host, you can start running any 
command from local host's terminal. The command will be executed on the 
remote host, and the output of the command will appear on local host. This 
setup can be used to create a backdoor on a remote host.
13. Create a web proxy for a particular website (e.g.,
www.google.com)
term
$ mkfifo proxypipe
$ while true; do nc -l 5000 0<proxypipe | nc 
www.google.com 80 1> proxypipe; done
The above commands create a named pipe proxypipe, and use nc to redirect all 
incoming tcp/5000 connections to http://www.google.com via the bidirectional pipe.

With this setup, you can access Google by going to
http://127.0.0.1:5000.
14. Create an SSL proxy for a particular website (e.g.,
www.google.com
$ mkfifo proxypipe
$ mkfifo proxypipe2
$ nc -l 5000 -k > proxypipe < proxypipe2 &
$ while true do; openssl s_client -connect
www.google.com:443 -quiet < proxypipe > proxypipe2; done
The above commands use nc to proxy SSL connections to
www.google.com.
15. Stream a video file from a server, and client watches the 
streamed video using mplayer.
On a video server (192.168.233.208):
term
lb_ $ cat video.avi | nc -l 5000
__ ^^Hon a client host: 
tit _$ nc 192.168.233.208 5000 | mplayer -vo x11 -cache 3000
16. Listen on a TCP port using IPv6 address.
The following command let nc use IPv6 address when listening on a TCP 
port. This may be useful to test IPv6 setup.

■l _$ nc -6 -l 5000
T__ $ sudo netstat -nap | grep 5000
tcp6 0 0 :::5000 :::* LISTEN 4099/nc
Linux Tutorials
How to remove overlay scrollbars in
Ubuntu
Since Ubuntu 11.04, Unity desktop comes with overlay scrollbars enabled by 
default. The original intention for overlay scrollbars is to save on screen’s 
realestate, especially since Ubuntu started to run on touchable small screen 
devices where the traditional cursor-driven scrollbars are not necessary.
58:1c
255 Mask:255.255.255.0
Scope:Link
Metric:1 
runs:0 frame:0 
ns:0 carrier:^
6 (19.3 KB)
However, many people are so accustomed to the traditional looking scrollbars 

that they may want to disable this new UI feature. After all, not many 
applications are using those overlay scrollbars yet.
Disable Overlay Scrollbars in Unity
If you want to disable overlay scrollbars in Ubuntu desktop, open up any 
terminal and type the command and press enter.
On Ubuntu 12.04 or earlier:
4 ■__xterm
■f_ $ gsettings set org.gnome.desktop.interface ubuntuoverlay-scrollbars 
false
On Ubuntu 12.10 or later:
Enable Overlay Scrollbars in Unity

If you want to re-enable overlay scrollbars later, run the following command 
in any terminal.
On Ubuntu 12.04 or earlier:
term
$ gsettings reset org.gnome.desktop.interface ubuntuoverlay-scrollbars
On Ubuntu 12.10 or later:
■>_$ gsettings reset com.canonical.desktop.interface scrollbar-mode
Disable Overlay Scrollbars in Unity System-wide
Note that the above scrollbar changes are made on a per-user basis. 
Therefore, if you log in as a different user in Unity, you will need to run the 
same commands to enable/disable overlay scrollbars as the user.
If you want to disable overlay scrollbars system-wide for all users, run the 
following command, and reboot your machine.
xterm
■__ $ sudo sh -c 'echo "export LIBOVERLAY_SCROLLBAR=0" > 
/etc/X11/Xsession.d/80overlayscrollbars'
Linux Tutorials
What is a good subtitle editor on
Linux

If you watch foreign movies regularly, chances are you prefer having 
subtitles rather than the dub. Grown up in France, I know that most Disney 
movies during my childhood sounded weird because of the French dub. If 
now I have the chance to be able to watch them in their original version, I 
know that for a lot of people subtitles are still required. And I surprise myself 
sometimes making subtitles for my family. Hopefully for me, Linux is not 
devoid of fancy and open source subtitle editors. In short, this is the non- 
exhaustive list of open source subtitle editors for Linux. Share your opinion 
on what you think of the best subtitle editor.
1. Gnome Subtitles
Gnome Subtitles is a bit my go to when it comes to quickly editing some 
existing subtitles. You can load the video, load the subtitle text files and 
instantly get going. I appreciate its balance between ease of use and advanced 
features. It comes with a synchronization tool as well as a spell check.
Finally, last but not least, the shortcuts are what makes it good in the end: 

when you edit a lot of lines, you prefer to keep your hands on the keyboard, 
and use the built in shortcuts to move around.
2. Aegisub
Aegisub is already one level of complexity higher. Just the interface reflects a 
learning curve. But besides its intimidating aspect, Aegisub is very complete 
software, providing tools beyond anything I could have imagined before. 
Like Gnome Subtitles, Aegisub has a WYSIWYG approach, but to a whole 
new level: it is possible to drag and drop the subtitles on the screen, see the 
audio spectrum on the side, and do everything with shortcuts. In addition to 
that, it comes with a Kanji tool, a karaoke mode, and the possibility to import 
lua script to automate some tasks. I really invite you to go read the manual 
page before starting using it.
3. Gaupol

At the other end of the complexity spectrum is Gaupol. Unlike Aegisub, 
Gaupol is quick to pick up and adopts an interface very close to Gnome 
Subtitles. But behind this relative simplicity, it comes with all the necessary 
tools: shortcuts, third party extension, spell checking, and even speech 
recognition (courtesy of CMU Sphinx). As a downside, however, I did notice 
some slow-downs while testing it, nothing too serious, but just enough to 
make me prefer Gnome Subtitles still.
4. Subtitle Editor

Subtitle Editor is very close to Gaupol. However, the interface is a little bit 
less intuitive, and the features are slightly more advanced. I appreciate the 
possibility to define "key frames" and all the given synchronization options. 
However, maybe more icons and less text would enhance the interface. As a 
goodie, Subtitle Editor can simulate a "type writer" effect, while I am not 
sure if it is extremely useful. And last but not least, the possibility to redefine 
the shortcuts is always handy.
5. Jubler

Written in Java, Jubler is a multi-platform subtitle editor. I was actually very 
impressed by its interface. I definitely see the Java-ish aspect of it, but it 
remains well conceived and clear. Like Aegisub, you can drag and drop the 
subtitles on the image, making the experience far more pleasant than just 
typing. It is also possible to define a style for subtitles, play sound from 
another track, translate the subtitles, or use the spell checker. However, be 
careful as you will need MPlayer installed and correctly configured 
beforehand if you want to use Jubler fully. Oh and I give it a special credit for 
its easy installation process after downloading the script from the official 
page.
6. Subtitle Composer

Defined as a "KDE subtitle composer," Subtitle Composer comes with most 
of the traditional features evoked previously, but with the KDE interface that 
we expect. This comes naturally with the option to redefine the shortcuts, 
which is very dear to me. But beyond all of this, what differentiates Subtitle 
Composer from all the previously mentioned programs is its ability to follow 
scripts written in JavaScript, Python, and even Ruby. A few examples are 
packaged with the software, and will definitely help you pick up the syntax 
and the usefulness of such feature.
To conclude, whether you, like me, just edit a few subtitles for your family, 
resynchronize the entire track, or write everything from scratch, Linux has 
the tools for you. For me in the end, the shortcuts and the ease-of-use make 
all the difference, but for any higher usage, scripting or speech recognition 
can become super handy.
Which subtitle editor do you use and why? Or is there another one that you 
prefer not mentioned here? Let us know in the comments.

Linux Tutorials
How to fix Android SDK Content
Loader stuck at 0% in Eclipse
Have you encountered the case where Eclipse hangs upon startup, in 
particular when you are developing an Android application with Android 
SDK? When that happens, you would see Android sdk Content Loader stuck at 0% 
at the Eclipse status bar in the right bottom area.
Android SDK Content Loader: (0%) s Wi
There can
be a variety of reasons for that, and here are several ways to fix the problem 
where Eclipse hangs at the Android SDK Content Loader.
Solution One
Often times, this problem can be network related. Check if your network is 
behind a proxy. If so, you need to configure proxy on Eclipse. For that, go to
Windows -^ Preferences -^ General -^ Network Connections, and fill in your proxy info.
Restart Eclipse after that. Conversely, it's also possible that you have 

configured proxy on Eclipse before, but that you are no longer behind proxy. 
Make sure to disable proxy then.
Solution Two
Another solution is to clean up project-specific meta data directories which 
are stored under your workspace directory.
xterm
$ cd
workspace/.metadata/.plugins/org.eclipse.core.resources /.projects 
$ rm -rf *
Restart Eclipse.
Solution Three
Check if an adb process is running. If so, kill the adb process, and restart 
Eclipse.
Linux Tutorials
How to install Apache Tomcat on
Ubuntu or Debian
Question: I need to run a Java web application, and for that I'm trying to use 
Apache Tomcat. How can I install Apache Tomcat on Debian or Ubuntu 
Linux?
Apache Tomcat is an open-source implementation of Java servlet engine and 
a J2EE container. It is one of the most popular applications in today's 
enterprise data centers, and there are many enterprise web applications 
powered by Apache Tomcat.

To install Apache Tomcat on Ubuntu or Debian, follow the steps here.
Apache Tomcat requires Java Development Kit (JDK). Thus, first install JDK 
on your Linux system.
Next, download Apache Tomcat (apache-tomcat-<version>.tar.gz) from its 
official website.
After downloading Tomcat, extract it in the /opt destination folder as follows.
term
T__ $ sudo tar xvzf apache-tomcat-<version>.tar.gz -C /opt
Change the Tomcat conf files world-readable.
term
Bf_ $ sudo chmod +r /opt/apache-tomcat-<version>/conf/
*
 
[Define catalina_home environment variable in your .bashrc file.
term
Bl_ $ vi ~/.bashrc
CATALINA_HOME=/opt/apache-tomcat-7.0.35 
Reload .bashrc file as follows.
term
lb_ $ source ~/.bashrc
Now enable Tomcat by running the following startup script.
term
M__ $ sudo $CATALINA_HOME/bin/startup.sh
At this point, you should be able to access Tomcat by going to http://<ipaddress- 
of-linux>:8080 on your web browser. If everything is okay, you should see the 
following web page.

Linux Tutorials
How to install dos2unix on Linux
DOS-based text files use a pair of carriage return (CR) and line feed (LF) as a 
new-line delimiter. On the other hand, UNIX-based text files simply use LFs 
to terminate each line. In order to convert a text file from DOS format to 
UNIX format, you can use a command-line tool called dos2unix.
Install dos2unix on CentOS/Fedora/RHEL
term
$ sudo yum install dos2unix

Install dos2unix on Ubuntu/Debian
term
__ $ sudo apt install tofrodos
$ sudo In -s /usr/bin/fromdos /usr/bin/dos2unix__
Linux Tutorials
How to record screen activities on
Android
Suppose you want to record and replay screen activities on Android device, 
as part of app demonstration or app review. While there are several Android 
apps that can record screen natively, they have a number of drawbacks, such 
as software bugs, device incompatibility, or root requirement etc. If you don't 
mind connecting your Android device to desktop PC, there is a more reliable 
and device-agnostic way to record Android screen.
In this post, I will describe how to record screen activities on Android as a 
video by using an open-source Desktop application called androidscreencast. 
Since androidscreencast is written in Java, you can run it on Linux as well as 
Windows and MacOS. Your Android device does not need to be rooted for 
this.
First, install Android SDK.
Next, install Java run-time. For example, on Ubuntu or Debian: 
xterm
T $ sudo apt-get install openjdk-6-jre
__ ^^HConnect your Android device to your Linux through USB cable.
Verify that the device is successfully detected by Android SDK, by running: 
xterm
lb_ $ adb devices
__List of devices attached

3931F425198800EC device
Finally, launch androidscreencast as a Java Web Start application as follows. 
xterm
$ wget
http://androidscreencast.googlecode.com/svn/trunk/Andro 
idScreencast/dist/androidscreencast.jnlp
$ javaws androidscreencast.jnlp
Once androidscreencast is successfully launched, you will see the screen of the 
connected Android device appear on your Desktop PC as follows.

To record screen of the connected Android device, just click Record button on 
the top. The recorded screen will be stored as .mov QuickTime video on your 
Desktop.

Linux Tutorials
How to create a Linux LVM 
partition
In Linux, Logical Volume Manager (LVM) manages physical disk drives by 
using the notion of "logical" volumes. A "volume" could be a partition or an 
entire disk drive. Among other things, LVM allows you to create one or more 
logical volumes out of one or more physical volumes, resize logical volumes 
on-the-fly, and move logical volumes across different physical volumes. Such 
flexible disk management of LVM is a great advantage as re-partitioning is 
often necessary while you are running your system. If you would like to try 
out LVM, here is a 3-minute guide on how to create a Linux LVM 
partition. If you are interested, you can always read a more detailed tutorial 
on LVM.
Typically LVM comes with most Linux distros, but to make sure to install a
necessary tool for LVM:
xterm
lb_ $ sudo apt-get install lvm2
First, prepare a physical LVM partition using fdisk. After creating a new 
partition, make sure to set its partition type to 8e (LVM). Assume that the 
drive to create a physical LVM partition on is /dev/sdb.
xterm
lb_ $ sudo fdisk /dev/sdb
[create a LVM partition: /dev/sdbl]
__ I^^Next, create a LVM physical volume on the prepared partition 
/dev/sdb1:
l_^lxterm
$ sudo pvcreate /dev/sdb1

Inside the LVM physical volume, create a LVM volume group called my_vol 
with a physical extent (PE) size of 16MB. Note that PE size needs to be a 
power of 2.
term
__ $ sudo vgcreate -s 16M my_vol /dev/sdbl
* Bc.reate a 1GB logical volume called driveo on volume group my_vol:
term
__ $ sudo lvcreate -L 1G -n drive0 my_vol
At this point, you will see /dev/my_vol/drive0 which is the device name of the 
LVM volume you've just created. Finally, proceed to create a filesystem and 
mount it as follows.
xterm
T__ $ sudo mkfs -t ext3 /dev/my_vol/drive0
$ sudo mount -t ext3 /dev/my_vol/drive0 /mnt
__ ^^Blf you want to have the LVM partition mounted permanently across 
reboots, refer to this tutorial.
Linux Tutorials
How to download GOG games from 
the command line on Linux
if you are a gamer and a Linux user, you probably were delighted when GOG 
announced that it would start proposing games for your favorite OS. if you 
have never heard of GOG before, i encourage you to check out their catalog 
of "good old games", reasonably priced, DRM-free, and packed with goodies. 
However, if the Windows client for GOG existed for quite some time now, an 
official Linux version is nowhere to be seen. So if waiting for the official 
version is uncomfortable for you, an unofficial open source program named 
LGOGDownloader gives you access to your library from the command line.

Install LGOGDownloader on Linux
Install LGOGDownloader on Ubuntu or Debian
For Debian-based users, the official page recommends that you download the 
sources and do:
xterm
_ openssl-dev liboauth-dev libjsoncpp-dev libhtmlcxx-dev libboost-system- 
dev libboost-filesystem-dev libboostregex-dev libboost-program-options-dev 
libboost-datetime-dev libtinyxml-dev librhash-dev help2man $ tar -xvzf
lgogdownloader-2.17.tar.gz
$ cd lgogdownloader-2.17
$ make release
$ sudo make install
Install LGOGDownloader on Fedora

The credit for this build instruction goes to AM (in the comment below).
First, install prerequisite packages with yum.
xterm
T__ $ sudo yum install tinyxml-devel jsoncpp-devel liboauth-devel libcurl-
devel boost-devel help2man
_ ^^^Download source code of other required packages and lgogdownloader.
I___ xterm
$ wget
https://sites.google.com/site/gogdownloader/lgogdownloa der-2.17.tar.gz 
$ wget
http://downloads.sourceforge.net/project/rhash/rhash/1.
3.3/rhash-1.3.3-src.tar.gz
$ wget
http://downloads.sourceforge.net/project/htmlcxx/htmlcx x/0.84/htmlcxx-
0.84.tar.gz
Build and install rhash. 
xterm
$ tar xfvz rhash-1.3.3-src.tar.gz; pushd rhash-1.3.3; make; make lib-shared 
lib-static; sudo make installlib-shared install-lib-static; popd
___^^MTo add /usr/local/lib to your LD_PATH, append /usr/local/lib line to 
/etc/ld.so.conf, and run the following command.
xterm
lb_ $ sudo ldconfig
L [Next, build and install htmlcxx.
xterm
$ tar xfvz htmlcxx-0.84.tar.gz; pushd htmlcxx-0.84; ./configure; vi 
html/tree.h; make; sudo make install; popd

tmlcxx source needs a fix with Fedora. After running the above
command, when vi text editor opens up html/tree.h, add #include <cstddef> at line 
53.
Now all dependencies are taken care of. Finally, let's compile and install 
Igogdownloader.
term
$ tar xfvz lgogdownloader-2.17.tar.gz; pushd 
lgogdownloader-2.17; vi Makefile; make; sudo make install; popd
lgogdownloader 's Makefile needs slight editing as well. When vi editor opens up 
Makefile in the above command, change -I/usr/include/rhash to -I/path/to/rhash- 
1.3.3/librhash at line 16. Once you finish editing Makefile, the rest of the step 
will install lgogdownloader.
Install LGOGDownloader on Arch Linux
If you are an Arch Linux user, an AUR package is waiting for you.
Usage of LGOGDownloader
Once the program is installed, you will need to identify yourself with the 
command:
term
T $ Igogdownloader --login 

dev@ubuntul4:~$ Iqoqdownloader --login
Email:
Password:
HTTP: Login successful
API: Login successful
Saving config: /home/dev/.config/lgogdownloader/config.cfq 
dev@ubuntul4:
Notice that the configuration file if you need it is at 
~/.config/lgogdownloader/config.cfg
Once authenticated, you can list all the games in your library with:
xterm
T__ $ lgogdownloader --list
Then download one with:
ultima 4
war s ow
dev@ubuntu!4:Lgogdownloader -list
beneath a steel sky
dragonsphere
treasure adventure game
tynan 2Q0Q
worlds of ultima the savage empire
riev@ubuntuil4:~$
term
$ Igogdownloader --download --game [game name]

You will notice that lgogdownloader allows you to resume previously interrupted 
downloads, which is nice because typical game downloads are not small.
Like every respectable command line utility, you can add various options:
- -platform [number] to select your OS where 1 is for windows and 4 for Linux.
- -directory [destination] to download the installer in a particular directory.
- -language [number] for a particular language pack (check the manual pages for 
the number corresponding to your language).
- -limit-rate [speed] to limit the downloading rate at a particular speed.
As a side bonus, lgogdownloader also comes with the possibility to check for
updates on the GOG website:
xterm
lb_ $ Igogdownloader --update-check

dev@ubuntul4:~$ Igogdownloader -update-check
New forum replies: 0
New private messages: 0
Updated games: 0
dev@ubuntu!4:
he
result will list the number of forum and private messages you have received, 
as well as the number of updated games.
To conclude, lgogdownloader is pretty standard when it comes to command line 
utilities. I would even say that it is an epitome of clarity and coherence. It is 
true that we are far in term of features from the relatively recent Steam Linux 
client, but on the other hand, the official GOG windows client does not do 
much more than this unofficial Linux version. In other words lgogdownloader is 
a perfect replacement. I cannot wait to see more Linux compatible games on 
GOG, especially after their recent announcements to offer DRM free movies, 
with a thematic around video games. Hopefully we will see an update in the 
client for when movie catalog matches the game library.
What do you think of GOG? Would you use the unofficial Linux client? Let 
us know in the comments.
Linux Tutorials
How to compare two version 
numbers in a shell script
Question: I am writing a shell script in which I need to compare two version 
number strings (e.g., "1.2.30" and "1.3.0") to determine which version is 
higher or lower than the other. Is there a way to compare two version number 
strings in a shell script?
When you are writing a shell script, there are cases where you need to 
compare two version numbers, and proceed differently depending on whether 
one version number is higher/lower than the other. For example, you want to 
check for the minimum version requirement (i.e., $version > 1.3.0). Or you 

want to write a conditional statement where the condition is defined by a 
specific range of version numbers (e.g., 1.0.0 < $version < 2.3.1).
If you want to compare two strings in version format (i.e., "X.Y.Z") in a shell 
script, one easy way is to use sort command. With -V option, the sort command 
can sort version numbers within text (in an increasing order by default). With 
-rV option, it can sort version numbers in a decreasing order.
$ cat input.txt 
2.3.0
1.3.10
3.0.0
1.0.15
2.1.5
$_________________
$ sort -V input.txt 
1.0.15
1.3.10
2.1.5
2.3.0
3.0.0
$
$ sort -rV input.txt
3.0.0
2.3.0
2.1.5
1.3.10
1.0.15
$
Natural version number sorting
Natural version number sorting 
in reverse order
Now let's see how we can use the sort command to compare version numbers 
in a shell script.
For version number string comparison, the following function definitions 
come in handy. Note that these functions use the sort command.
function version_gt() { test "$(echo " [email protected]" | tr " " "n" | sort -V | 
head -n 1)" != "$1"; }
function version_le() { test "$(echo "[email protected]" | tr " " "n" | sort -V |
head -n 1)" == "$1"; }
function version_lt() { test "$(echo "[email protected]" | tr " " "n" | sort -rV | 
head -n 1)" != "$1"; }
function version_ge() { test "$(echo "[email protected]" | tr " " "n" | sort -rV |

head -n 1)" == "$1"; }
These functions perform, respectively, "greater-than", "less than or equal to", 
"less than", and "greater than or equal to" operations against two specified 
version numbers. You will need to use bash shell due to function definitions.
Below is an example bash script that compares two version numbers.
#!/bin/bash
VERSION=$1 VERSION2=$2
function version_gt() { test "$(echo " [email protected]" | tr " " "n" | sort -V |
head -n 1)" != "$1"; }
function version_le() { test "$(echo "[email protected]" | tr " " "n" | sort -V |
head -n 1)" == "$1"; }
function version_lt() { test "$(echo "[email protected]" | tr " " "n" | sort -rV |
head -n 1)" != "$1"; }
function version_ge() { test "$(echo "[email protected]" | tr " " "n" | sort -rV | 
head -n 1)" == "$1"; }
if version_gt $VERSION $VERSION2; then echo "$VERSION is greater 
than $VERSION2" fi
if version_le $VERSION $VERSION2; then
echo "$VERSION is less than or equal to $VERSION2" fi
if version_lt $VERSION $VERSION2; then echo "$VERSION is less than

$VERSION2" fi
if version_ge $VERSION $VERSION2; then
echo "$VERSION is greater than or equal to $VERSION2" 
fi


Linux Tutorials
How to set up server monitoring 
system with Monit
Many Linux admins rely on a centralized remote monitoring system (e.g., 
Nagios or Cacti) to check the health of their network infrastructure. While 
centralized monitoring makes an admin's life easy when dealing with many 
hosts and devices, a dedicated monitoring box obviously becomes a single 
point of failure; if the monitoring box goes down or becomes unreachable for 
whatever reason (e.g., bad hardware or network outage), you will lose 
visibility on your entire infrastructure.
One way to add redundancy to your monitoring system is to install 
standalone monitoring software (as a fallback) at least on any critical/core 
servers on your network. In case a centralized monitor is down, you will still 
be able to maintain visibility on your core servers from their backup monitor.
What is Monit?
Monit is a cross-platform open-source tool for monitoring Unix/Linux 
systems (e.g., Linux, BSD, OSX, Solaris). Monit is extremely easy to install 
and reasonably lightweight (with only 500KB in size), and does not require 
any third-party programs, plugins or libraries. Yet, Monit lends itself to full­
blown monitoring, capable of process status monitoring, filesystem change 
monitoring, email notification, customizable actions for core services, and so 
on. The combination of ease of setup, lightweight implementation and 
powerful features makes Monit an ideal candidate for a backup monitoring 
tool.
I have been using Monit for several years on multiple hosts, and I am very 
pleased how reliable it has been. Even as a full-blown monitoring system, 
Monit is very useful and powerful for any Linux admin. In this tutorial, let 
me demonstrate how to set up Monit on a local server (as a backup monitor) 

to monitor common services. With this setup, I will only scrach the surface of 
what Monit can do for us.
Installation of Monit on Linux
Most Linux distributions already include Monit in their repositories.
For Debian, Ubuntu or Linux Mint:
term
$ sudo aptitude install monit
For Fedora or CentOS/RHEL:
On CentOS/RHEL, you must enable either EPEL or Repoforge repository 
first.
term
lb_ # yum install monit
Monit comes with a very well documented configuration file with a lots of 
examples. The main configuration file is located in /etc/monit.conf in 
Fedora/CentOS/RHEL, or /etc/monit/monitrc in Debian/Ubuntu. Monit 
configuration has two parts: Global and Services sections.
Global Configuration: Web Status Page
Monit can use several mail servers for notifications, and/or an HTTP/HTTPS 
status page. Let's start with the web status page with the following 
requirements.
Monit listens on port 1966.
Access to the web status page is encrypted with SSL. Login requires 
monituser/romania as user/password. Login is permitted from localhost, 
myhost.mydomain.ro, and internal LAN (192.168.0.0/16) only.

Monit stores an SSL certificate in a pem format.
For subsequent steps, I will use a Red Hat based system. Similar steps will be 
applicable on a Debian based system.
First, generate and store a self-signed certificate (monit.pem) in /var/cert.
xterm
# mkdir /var/certs
# cd /etc/pki/tls/certs
# ./make-dummy-cert monit.pem
# cp monit.pem /var/certs
# chmod 0400 /var/certs/monit.pem
__ ^^MNow put the following snippet in the Monit's main configuration 
file. You can start with an empty configuration file or make a copy of the 
original file.
set httpd port 1966 and
SSL ENABLE
PEMFILE /var/certs/monit.pem 
allow monituser:romania 
allow localhost
allow 192.168.0.0/16
allow myhost.mydomain.ro

Global Configuration: Email Notification
Next, let's set up email notification in Monit. We need at least one active 
SMTP server which can send mails from the Monit host. Something like the 
following will do (adjust it for your case):
Mail server hostname: smtp.monit.ro
Sender email address used by monit (from): [email protected] Who will receive 
mail from monit daemon: [email protected] SMTP port used by mail server: 587 
(default is 25)
With the above information, email notification would be configured like this:
set mailserver smtp.monit.ro port 587
set mail-format {
from: [email protected]
subject: $SERVICE $EVENT at $DATE on $HOST
message: Monit $ACTION $SERVICE $EVENT at $DATE on $HOST : 
$DESCRIPTION.
Yours sincerely,
Monit
}
set alert [email protected]

As you can see, Monit offers several built-in variables ( $date, $event, 
$host, etc.), and you can customize your email message for your needs. If 
you want to send mails from the Monit host itself, you need a sendmail- 
compatible program (e.g., postfix or ssmtp) already installed.
Global Configuration: Monit Daemon
The next part is setting up monit daemon. We will set it up as follows.
Performs the first check after 120 seconds. Checks services once every 3 
minutes. Use syslog for logging.
Place the following snippet to achieve the above setting.

set daemon 120
with start delay 240
set logfile syslog facility log_daemon
We must also define idfile, a unique ID used by monit demon, and eventqueue, a 
path where mails sent by monit but undelivered due to 
SMTP/network errors. Verifiy that path (/var/monit) already exists. The 
following configuration will do.
set idfile /var/monit/id
set eventqueue
basedir /var/monit
Test Global Configuration
Now the Global section is finished. The Monit configuration file will look like 
this:
# Global Section
# status webpage and acl's 
set httpd port 1966 and 
SSL ENABLE

PEMFILE /var/certs/monit.pem allow monituser:romania
allow localhost
allow 192.168.0.0/16
allow myhost.mydomain.ro
# mail-server
set mailserver smtp.monit.ro port 587
# email-format
set mail-format {
from: [email protected]
subject: $SERVICE $EVENT at $DATE on $HOST
message: Monit $ACTION $SERVICE $EVENT at $DATE on $HOST : 
$DESCRIPTION.
Yours sincerely,
Monit
}
set alert [email protected]
# delay checks
set daemon 120
with start delay 240
set logfile syslog facility log_daemon
# idfile and mail queue path
set idfile /var/monit/id
set eventqueue
basedir /var/monit


Now it is time to check what we have done. You can test an existing 
configuration file (/etc/monit.conf) by running:
I_____ ^^Ixterm
Hl_ # monit -t
__ ^^Hcontrol file syntax OK
If Monit complains about any error, please review the configuration file 
again. Fortunately, error/warnings messages are informative. For example:
monit: Cannot stat the SSL server PEM file '/var/certs/monit.pem' -- No such 
file or directory
/etc/monit/monitrc:10: Warning: hostname did not resolve
'smtp.monit.ro'
Once you verify the syntax of configuration, start monit daemon, and wait 2 to 
3 minutes:
xterm
Hl_ # service monit start
I BiF you are using systemd, run:
xterm
lb_ # systemctl start monit
__ ^^■iNow open a browser window, and go to https://<monit_host>:1966.
Replace <monit_host> with your Monit hostname or iP address.
Note that if you have a self-signed SSL certificate, you will see a warning 
message in your browser.

After you have completed login, you must see the following page.

Home> 
Use M/Monit io manage all your Monit instances 
Monit5.ll
Monit Service Manager
Monit is running on ubuntusen/er with uptime, 12m and monitonng:
System Status Load 
CPU Memory Swap
ubuntuserver 
[0.00] [0.01] [0.05] O.Ota, 0.0%sy, 0.0ta 4.8% [47.7 MB] 0.0% [0.0 B]
Copyright' 2001-201^ Tildeslash Ail rights reserved Monit web site Monit Wiki , M/Monit

In the rest of the tutorial, let me show how we can monitor a local server and 
common services. You will see a lot of useful examples on the official wiki 
page. Most of them are copy-and-pastable!
Service Configuration: CPU/Memory Monitoring
Let start with monitoring a local server's CPU/memory usage. Copy the 
following snippet in the configuration file.
check system localhost
if loadavg (1min) > 10 then alert
if loadavg (5min) > 6 then alert
if memory usage > 75% then alert
if cpu usage (user) > 70% then alert
if cpu usage (system) > 60% then alert
if cpu usage (wait) > 75% then alert
You can easily interpret the above configuration. The above checks are 
performed on local host for every monitoring cycle (which is set to 120 
seconds in the Global section). If any condition is met, monit daemon will send 
an alert with an email.
If certain properties do not need to be monitored for every cycle, you can use 
the following format. For example, this will monitor average load every other 
cycle (i.e., every 240 seconds).

if loadavg (1min) > 10 for 2 cycles then alert
Service Configuration: SSH Service Monitoring
Let's check if we have sshd binary installed in /usr/sbin/sshd: 
check file sshd_bin with path /usr/sbin/sshd
We also want to check if the init script for sshd exist: 
check file sshd_init with path /etc/init.d/sshd
Finally, we want to check if sshd daemon is up an running, and listens on port 
22:
check process sshd with pidfile /var/run/sshd.pid 
start program "/etc/init.d/sshd start" 
stop program "/etc/init.d/sshd stop" 
if failed port 22 protocol ssh then restart 
if 5 restarts within 5 cycles then timeout
More specifically, we can interpret the above configuration as follows. We 

check if a process named sshd and a pidfile (/var/run/sshd.pid) exist. If either one 
does not exist, we restart sshd demon using init script. We check if a process 
listening on port 22 can speak SSH protocol. If not, we restart sshd daemon. If 
there are at least 5 restarts within the last 5 monitoring cycles (i.e., 5x120 
seconds), sshd daemon is declared nonfunctional, and we do not try to check 
again.
Service Configuration: SMTP Service Monitoring
Now let's set up a check on a remote SMTP mail server (e.g., 192.168.111.102). 
Let's assume that the SMTP server is running SMTP, 
IMAP and SSH on its LAN interface.
check host MAIL with address 192.168.111.102
if failed icmp type echo within 10 cycles then alert
if failed port 25 protocol smtp then alert
else if recovered then exec "/scripts/mail-script"
if failed port 22 protocol ssh then alert
if failed port 143 protocol imap then alert

We check if the remote host responds to ICMP. If we haven't received ICMP 
response within 10 cycles, we send out an alert. If testing for SMTP protocol 
on port 25 fails, we send out an alert. If testing succeeds again after a failed 
test, we run a script (/scripts/mail-script). If testing for SSH and IMAP protocols 
fail on port 22 and 143, respectively, we send out an alert.
Conclusion
In this tutorial, I demonstrate how to set up Monit on a local server. What I 
showed here is just the tip of the iceberg, as far as Monit's capabilities are 
concerned. Take your time and read the man page about Monit (a very good 
one). Monit can do a lot for any Linux admin with a very nice and easy to 
understand syntax. If you put together a centralized remote monitor and 
Monit to work for you, you will have a more reliable monitoring system. 
What is your thought on Monit?
Linux Tutorials
Linux vs. Windows device driver 
model: architecture, APIs and build 
environment comparison
Device drivers are parts of the operating system that facilitate usage of 

hardware devices via certain programming interface so that software 
applications can control and operate the devices. As each driver is specific to 
a particular operating system, you need separate Linux, Windows, or Unix 
device drivers to enable the use of your device on different computers. This is 
why when hiring a driver developer or choosing an R&D service provider, it 
is important to look at their experience of developing drivers for various 
operating system platforms.
The first step in driver development is to understand the differences in the 
way each operating system handles its drivers, underlying driver model and 
architecture it uses, as well as available development tools. For example, 
Linux driver model is very different from the Windows one. While Windows 
facilitates separation of the driver development and OS development and 
combines drivers and OS via a set of ABI calls, Linux device driver 
development does not rely on any stable ABI or API, with the driver code 
instead being incorporated into the kernel. Each of these models has its own 
set of advantages and drawbacks, but it is important to know them all if you 
want to provide a comprehensive support for your device.
In this article we will compare Windows and Linux device drivers and 
explore the differences in terms of their architecture, APIs, build 

development, and distribution, in hopes of providing you with an insight on 
how to start writing device drivers for each of these operating systems.
1. Device Driver Architecture
Windows device driver architecture is different from the one used in Linux 
drivers, with either of them having their own pros and cons. Differences are 
mainly influenced by the fact that Windows is a closed-source OS while 
Linux is open-source. Comparison of the Linux and Windows device driver 
architectures will help us understand the core differences behind Windows 
and Linux drivers.
1.1. Windows driver architecture
While Linux kernel is distributed with drivers themselves, Windows kernel 
does not include device drivers. Instead, modern Windows device drivers are 
written using the Windows Driver Model (WDM) which fully supports 
plugand-play and power management so that the drivers can be loaded and 
unloaded as necessary.
Requests from applications are handled by a part of Windows kernel called 
IO manager which transforms them into IO Request Packets (IRPs) which are 
used to identify the request and convey data between driver layers.
WDM provides three kinds of drivers, which form three layers:
Filter drivers provide optional additional processing of IRPs. Function 
drivers are the main drivers that implement interfaces to individual devices. 
Bus drivers service various adapters and bus controllers that host devices.
An IRP passes these layers as it travels from the IO manager down to the 
hardware. Each layer can handle an IRP by itself and send it back to the IO 
manager. At the bottom there is Hardware Abstraction Layer (HAL) which 
provides a common interface to physical devices.
1.2. Linux driver architecture

The core difference in Linux device driver architecture as compared to the 
Windows one is that Linux does not have a standard driver model or a clean 
separation into layers. Each device driver is usually implemented as a module 
that can be loaded and unloaded into the kernel dynamically. Linux provides 
means for plug-and-play support and power management so that drivers can 
use them to manage devices correctly, but this is not a requirement.
Modules export functions they provide and communicate by calling these 
functions and passing around arbitrary data structures. Requests from user 
applications come from the filesystem or networking level, and are converted 
into data structures as necessary. Modules can be stacked into layers, 
processing requests one after another, with some modules providing a 
common interface to a device family such as USB devices.
Linux device drivers support three kinds of devices:
Character devices which implement a byte stream interface. Block devices 
which host filesystems and perform IO with multibyte blocks of data.
Network interfaces which are used for transferring data packets through the 
network.
Linux also has a Hardware Abstraction Layer that acts as an interface to the 
actual hardware for the device drivers.
2. Device Driver APIs
Both Linux and Windows driver APIs are event-driven: the driver code 
executes only when some event happens: either when user applications want 
something from the device, or when the device has something to tell to the 
OS.
2.1. Initialization
On Windows, drivers are represented by a driverobject structure which is 
initialized during the execution of the driverentry function. This entry point 
also registers a number of callbacks to react to device addition and removal, 
driver unloading, and handling the incoming IRPs. Windows creates a device 

object when a device is connected, and this device object handles all 
application requests on behalf of the device driver.
As compared to Windows, Linux device driver lifetime is managed by kernel 
module's module_init and module_exit functions, which are called when the 
module is loaded or unloaded. They are responsible for registering the 
module to handle device requests using the internal kernel interfaces. The 
module has to create a device file (or a network interface), specify a 
numerical identifier of the device it wishes to manage, and register a number 
of callbacks to be called when the user interacts with the device file.
2.2. Naming and claiming devices
Registering devices on Windows
Windows device driver is notified about newly connected devices in its 
adddevice callback. It then proceeds to create a device object used to 
identify this particular driver instance for the device. Depending on the driver 
kind, device object can be a Physical Device Object (PDO), Function Device 
Object (FDO), or a Filter Device Object (FIDO). Device objects can be 
stacked, 
with a PDO in the bottom.
Device objects exist for the whole time the device is connected to the 
computer. deviceextension structure can be used to associate global data with a 
device object.
Device objects can have names of the form DeviceDeviceName, which are 
used by the system to identify and locate them. An application opens a file 
with such name using createfile API function, obtaining a handle, which then 
can be used to interact with the device.
However, usually only PDOs have distinct names. Unnamed devices can be 
accessed via device class interfaces. The device driver registers one or more 
interfaces identified by 128-bit globally unique identifiers (GUIDs). User 
applications can then obtain a handle to such device using known GUIDs.
Registering devices on Linux

On Linux user applications access the devices via file system entries, usually 
located in the /dev directory. The module creates all necessary entries during 
module initialization by calling kernel functions like
register _chrdev. An application issues an open system call to obtain a file 
descriptor, which is then used to interact with the device. This call (and 
further system calls with the returned descriptor like read, write, or
close) are then dispatched to callback functions installed by the module into 
structures like file_operations or block_device_operations.
The device driver module is responsible for allocating and maintaining any 
data structures necessary for its operation. A file structure passed into the file 
system callbacks has a private_data field, which can be used to store a pointer to 
driver-specific data. The block device and network interface APIs also 
provide similar fields.
While applications use file system nodes to locate devices, Linux uses a 
concept of major and minor numbers to identify devices and their drivers 
internally. A major number is used to identify device drivers, while a minor 
number is used by the driver to identify devices managed by it. The driver 
has to register itself in order to manage one or more fixed major numbers, or 
ask the system to allocate some unused number for it.
Currently, Linux uses 32-bit values for major-minor pairs, with 12 bits 
allocated for the major number allowing up to 4096 distinct drivers. The 
major-minor pairs are distinct for character and block devices, so a character 
device and a block device can use the same pair without conflicts. Network 
interfaces are identified by symbolic names like eth0, which are again distinct 
from major-minor numbers of both character and block devices.
2.3. Exchanging data
Both Linux and Windows support three ways of transferring data between 
user-level applications and kernel-level drivers:
Buffered Input-Output which uses buffers managed by the kernel. For write 
operations the kernel copies data from a user-space buffer into a kernel-

allocated buffer, and passes it to the device driver. Reads are the same, with 
kernel copying data from a kernel buffer into the buffer provided by the 
application.
Direct Input-Output which does not involve copying. Instead, the kernel 
pins a user-allocated buffer in physical memory so that it remains there 
without being swapped out while data transfer is in progress. Memory 
mapping can also be arranged by the kernel so that the kernel and user space 
applications can access the same pages of memory using distinct addresses.
Driver IO modes on Windows
Support for Buffered IO is a built-in feature of WDM. The buffer is 
accessible to the device driver via the AssociatedIrp.SystemBuffer field of 
the IRP structure. The driver simply reads from or writes to this buffer when 
it needs to communicate with the userspace.
Direct IO on Windows is mediated by memory descriptor lists (MDLs). 
These are semi-opaque structures accessible via mdladdress field of the IRP. 
They are used to locate the physical address of the buffer allocated by the 
user application and pinned for the duration of the IO request.
The third option for data transfer on Windows is called method_neither. In 
this case the kernel simply passes the virtual addresses of user-space input 
and output buffers to the driver, without validating them or ensuring that they 
are mapped into physical memory accessible by the device driver. The device 
driver is responsible for handling the details of the data transfer.
Driver IO modes on Linux
Linux provides a number of functions like clear_user, copy_to_user, strncpy_from_user, 
and some others to perform buffered data transfers 
between the kernel and user memory. These functions validate pointers to 
data buffers and handle all details of the data transfer by safely copying the 
data buffer between memory regions.
However, drivers for block devices operate on entire data blocks of known 
size, which can be simply moved between the kernel and user address spaces 
without copying them. This case is automatically handled by Linux kernel for 

all block device drivers. The block request queue takes care of transferring 
data blocks without excess copying, and Linux system call interface takes 
care of converting file system requests into block requests.
Finally, the device driver can allocate some memory pages from kernel 
address space (which is non-swappable) and then use the remap_pfn_range 
function to map the pages directly into the address space of the user process. 
The application can then obtain the virtual address of this buffer and use it to 
communicate with the device driver.
3. Device Driver Development Environment
3.1. Device driver frameworks
Windows Driver Kit
Windows is a closed-source operating system. Microsoft provides a Windows 
Driver Kit to facilitate Windows device driver development by non-Microsoft 
vendors. The kit contains all that is necessary to build, debug, verify, and 
package device drivers for Windows.
Windows Driver Model defines a clean interface framework for device 
drivers. Windows maintains source and binary compatibility of these 
interfaces. Compiled WDM drivers are generally forward-compatible: that is, 
an older driver can run on a newer system as is, without being recompiled, 
but of course it will not have access to the new features provided by the OS. 
However, drivers are not guaranteed to be backward-compatible.
Linux source code
In comparison to Windows, Linux is an open-source operating system, thus 
the entire source code of Linux is the SDK for driver development. There is 
no formal framework for device drivers, but Linux kernel includes numerous 
subsystems that provide common services like driver registration. The 
interfaces to these subsystems are described in kernel header files.
While Linux does have defined interfaces, these interfaces are not stable by 

design. Linux does not provide any guarantees about forward or backward 
compatibility. Device drivers are required to be recompiled to work with 
different kernel versions. No stability guarantees allow rapid development of 
Linux kernel as developers do not have to support older interfaces and can 
use the best approach to solve the problems at hand.
Such ever-changing environment does not pose any problems when writing 
in-tree drivers for Linux, as they are a part of the kernel source, because they 
are updated along with the kernel itself. However, closed-source drivers must 
be developed separately, out-of-tree, and they must be maintained to support 
different kernel versions. Thus Linux encourages device driver developers to 
maintain their drivers in-tree.
3.2. Build system for device drivers
Windows Driver Kit adds driver development support for Microsoft Visual 
Studio, and includes a compiler used to build the driver code. Developing 
Windows device drivers is not much different from developing a user-space 
application in an IDE. Microsoft also provides an Enterprise Windows Driver 
Kit, which enables command-line build environment similar to the one of 
Linux.
Linux uses Makefiles as a build system for both in-tree and out-of-tree device 
drivers. Linux build system is quite developed and usually a device driver 
needs no more than a handful of lines to produce a working binary.
Developers can use any IDE as long as it can handle Linux source code base 
and run make, or they can easily compile drivers manually from terminal.
3.3. Documentation support
Windows has excellent documentation support for driver development. 
Windows Driver Kit includes documentation and sample driver code, 
abundant information about kernel interfaces is available via MSDN, and 
there exist numerous reference and guide books on driver development and 
Windows internals.
Linux documentation is not as descriptive, but this is alleviated with the 
whole source code of Linux being available to driver developers. The

Documentation directory in the source tree documents some of the Linux 
subsystems, but there are multiple books concerning Linux device driver 
development and Linux kernel overviews, which are much more elaborate. 
Linux does not provide designated samples of device drivers, but the source 
code of existing production drivers is available and can be used as a reference 
for developing new device drivers.
3.4. Debugging support
Both Linux and Windows have logging facilities that can be used to 
tracedebug driver code. On Windows one would use dbgprint function for this, 
while on Linux the function is called printk. However, not every problem can 
be resolved by using only logging and source code. Sometimes breakpoints 
are more useful as they allow to examine the dynamic behavior of the driver 
code. Interactive debugging is also essential for studying the reasons of 
crashes.
Windows supports interactive debugging via its kernel-level debugger windbg. 
This requires two machines connected via a serial port: a computer 
to run the debugged kernel, and another one to run the debugger and control 
the operating system being debugged. Windows Driver Kit includes 
debugging symbols for Windows kernel so Windows data structures will be 
partially visible in the debugger.
Linux also supports interactive debugging by means of kdb and kgdb. 
Debugging support can be built into the kernel and enabled at boot time. 
After that one can either debug the system directly via a physical keyboard, 
or connect to it from another machine via a serial port. KDB offers a simple 
command-line interface and it is the only way to debug the kernel on the 
same machine. However, KDB lacks source-level debugging support. KGDB 
provides a more complex interface via a serial port. It enables usage of 
standard application debuggers like GDB for debugging Linux kernel just 
like any other userspace application.
4. Distributing Device Drivers

4.1. Installing device drivers
On Windows installed drivers are described by text files called INF files, 
which are typically stored in C:WindowsINF directory. These files are 
provided by the driver vendor and define which devices are serviced by the 
driver, where to find the driver binaries, the version of the driver, etc.
When a new device is plugged into the computer, Windows looks though 
installed drivers and loads an appropriate one. The driver will be 
automatically unloaded as soon as the device is removed.
On Linux some drivers are built into the kernel and stay permanently loaded. 
Non-essential ones are built as kernel modules, which are usually stored in 
the /lib/modules/kernel-version directory. This directory also contains 
various configuration files, like modules.dep describing dependencies 
between kernel modules.
While Linux kernel can load some of the modules at boot time itself, 
generally module loading is supervised by user-space applications. For 
example, init process may load some modules during system initialization, 
and the udev daemon is responsible for tracking the newly plugged devices 
and loading appropriate modules for them.
4.2. Updating device drivers
Windows provides a stable binary interface for device drivers so in some 
cases it is not necessary to update driver binaries together with the system. 
Any necessary updates are handled by the Windows Update service, which is 
responsible for locating, downloading, and installing up-to-date versions of 
drivers appropriate for the system.
However, Linux does not provide a stable binary interface so it is necessary 
to recompile and update all necessary device drivers with each kernel update. 
Obviously, device drivers, which are built into the kernel are updated 
automatically, but out-of-tree modules pose a slight problem. The task of 
maintaining up-to-date module binaries is usually solved with DKMS: a 
service that automatically rebuilds all registered kernel modules when a new 

kernel version is installed.
4.3. Security considerations
All Windows device drivers must be digitally signed before Windows loads 
them. It is okay to use self-signed certificates during development, but driver 
packages distributed to end users must be signed with valid certificates 
trusted by Microsoft. Vendors can obtain a Software Publisher Certificate 
from any trusted certificate authority authorized by Microsoft. This certificate 
is then cross-signed by Microsoft and the resulting cross-certificate is used to 
sign driver packages before the release.
Linux kernel can also be configured to verify signatures of kernel modules 
being loaded and disallow untrusted ones. The set of public keys trusted by 
the kernel is fixed at the build time and is fully configurable. The strictness of 
checks performed by the kernel is also configurable at build time and ranges 
from simply issuing warnings for untrusted modules to refusing to load 
anything with doubtful validity.
5. Conclusion
As shown above, Windows and Linux device driver infrastructure have some 
things in common, such as approaches to API, but many more details are 
rather different. The most prominent differences stem from the fact that 
Windows is a closed-source operating system developed by a commercial 
corporation. This is what makes good, documented, stable driver ABI and 
formal frameworks a requirement for Windows while on Linux it would be 
more of a nice addition to the source code. Documentation support is also 
much more developed in Windows environment as Microsoft has resources 
necessary to maintain it.
On the other hand, Linux does not constrain device driver developers with 
frameworks and the source code of the kernel and production device drivers 
can be just as helpful in the right hands. The lack of interface stability also 
has an implications as it means that up-to-date device drivers are always 
using the latest interfaces and the kernel itself carries lesser burden of 
backwards compatibility, which results in even cleaner code.

Knowing these differences as well as specifics for each system is a crucial 
first step in providing effective driver development and support for your 
devices. We hope that this Windows and Linux device driver development 
comparison was helpful in understanding them, and will serve as a great 
starting point in your study of device driver development process.
Linux Tutorials
How to apply XenServer update 
patch
Citrix distributes patches or hotfixes of XenServer (now renamed to "Citrix 
Hypervisor") on a regular basis, for fixing bugs or updating security features. 
A given XenServer patch may have dependency on other fixes, in which case 
you will fail to install the patch without applying all dependent fixes first. If 
you would like to apply a patch on your XenServer host, you can use either 
XenCenter client software or XenServer command line interface (CLI).
In this tutorial, I will explain how to install XenServer patches by using 
XenServer CLI.
I assume that you have already downloaded a patch from Citrix website (e.g., 
xs602E00i.zip). Upload it on any XenServer host and upzip it.
xterm
■l _$ unzip XS602E001.zip
The extracted zip file will contain xs602e001.xsupdate. Using XenServer CLI, 
upload this patch to the destination XenServer host where the patch is to be 
applied as follows. It's assume that the XenServer host has an IP address
172.16.244.154.
xterm
T__ $ xe patch-upload -s 172.16.244.154 -u root -pw [rootpassword] file-
name=XE602E001.xsupdate

xxxx-xxx-xxx-xxx-xxxxxx
In the above command, you can omit " -s 172.16.244.154 -u root -pw [root-password]" 
parameters if the destination XenServer host is the local host.
Once patch is successfully uploaded, the above command will return the 
UUID (xxxx-xxx-xxx-xxx-xxxxxx) of the uploaded patch. Make a note of 
this UUID since it is needed to apply the patch.
Confirm that the patch has been uploaded successfully as follows. The output 
should display the uploaded patch.
term
lb_ $ xe patch-list -s 172.16.244.154 -u root -pw [rootpassword]
_L~_Jl;incilly, apply the uploaded path using the UUID obtained above.
term
Bl_ $ xe patch-pool-apply -s 172.16.244.154 -u root -pw [root-password]
uuid=xxxx-xxx-xxx-xxx-xxxxxx
Once you have installed the patch, you must restart the XenServer host for 
the patch to take effect. When there are multiple XenServer hosts in a 
resource pool, you must apply the same patch to all the hosts, one by one in 
sequence. It is not recommended that you apply a patch to only a subset of all 
existing XenServer hosts in a pool.
Linux Tutorials
How to find and kill misbehaving
MySQL queries
Sometimes the complexity of a relational database system can be 
overwhelming. Fortunately, that complexity is an advantage, as with 
MySQL's tools for managing queries. In this tutorial, I will show you how to 
find and kill any misbehaving MySQL queries.

To view the currently-running queries, log in to the MySQL console and run 
the show processlist command:
:term
T mysql> show processlist;
+--------+.
+---------+------- +------- +
| Id | User | Host | db | Command | Time | State | Info | Rows_sent | 
Rows_examined | Rows_read |
+--------+
+---------+------- +------- +
| 78233 | root | 127.0.0.1:37527 | mysql | Sleep | 16474 11 NULL | 6 | 6 | 6 |
| 84546 | root | 127.0.0.1:48593 | mysql | Sleep | 13237 | | NULL | 2 | 2 | 2 |
| 107083 | root | 127.0.0.1:56451 | mysql | Sleep | 15488 11 NULL | 1 | 121 | 
121 |
| 131455 | root | 127.0.0.1:48550 | NULL | Query | 0 | NULL | show 
processlist | 0 | 0 | 0 |

+--------+-------- +------------------+--------- +--------- +------- +------- +
---------+----------- +----------------+----------- +
4 rows in set (0.03 sec)
The first column you should look at is Time, which is the number of seconds 
the process has been "doing the thing it's doing." A process whose command 
is Sleep is waiting for a query to come in, so it's not consuming any resources. 
For any other process, however, a Time of more than a few seconds indicates a 
problem.
In this case, the only query running is our show processlist command. Let's see 
what it looks like if we have a poorly-written query running:
xterm
Bl__mysql> show processlist;
+--------+-------- +------------------+----------- +--------- +-------+------------- +-------
---------------------------- +----------- +----------------+----------- + | Id | User | Host | 
db | Command | Time | State | Info | Rows_sent | Rows_examined | 
Rows_read |
+--------+-------- +------------------+----------- +--------- +-------+------------- +-------
---------------------------- +----------- +----------------+----------- + | 78233 | root | 
127.0.0.1:37527 | example | Sleep | 18046 | | NULL | 6 | 6 | 6 |
| 84546 | root | 127.0.0.1:48593 | example | Sleep | 14809 | | NULL | 2 | 2 | 2 |
| 107083 | root | 127.0.0.1:56451 | example | Sleep | 17060 | | NULL | 1 | 121 | 
121 |
| 132033 | root | 127.0.0.1:54642 | example | Query | 27 | Sending data | select 
max(subtotal) from orders | 0 | 0 | 0 | | 133933 | root | 127.0.0.1:48679 | NULL 
| Query | 0 | NULL | show processlist | 0 | 0 | 0 |
| 134122 | root | 127.0.0.1:49264 | example | Sleep | 0 | |

NULL | 0 | 0 | 0 |
+--------+-------- +------------------+----------- +--------- +------- +------------- +
---------------------------- +----------- +----------------+----------- + 6 rows in set 
(0.00 sec)
Ah! Now we see there is a query that's been running for almost 30 seconds. If

we don't want to let it run its course, we can kill it by passing its Id to the kill
command:
term
mysql> kill 132033;
Query OK, 0 rows affected (0.00 sec) 
mysql>
(Note that MySQL will always report 0 rows affected, because
we're not altering any data.)
Judicious use of the kill command can clean up a backlog of queries. 
Remember, however, that it's not a permanent solution - if those queries came 
from your application, you need to rewrite them, or you'll continue to see the 
same issue reappear.
See also MySQL's documentation on the different Command values: 
https://dev.mysql.com/doc/refman/5.7/en/thread-commands.html
Linux Tutorials
How to install httptunnel on Linux
httptunnel is a GNU/GPL-licensed free software that allows one to create a bi­
directional tunnel encapsulated by HTTP, between client and server. HTTP- 
encapsulated tunnels are useful when you want to use games, IM clients, or 
P2P sharing applications across restrictive firewalls or proxies which tend to 
block pretty much everything except well known traffic such as HTTP traffic. 
httptunnel consists of hts (server) and htc (client) components to establish HTTP 
tunnels in between.
Install httptunnel on Linux
In order to install httptunnel on Linux, follow the steps below.

Install httptunnel on Ubuntu, Mint, or Debian
xterm
lb_ $ sudo apt-get install httptunnel
Install httptunnel on CentOS or RHEL
First set up Repoforge on your system, and then run: 
xterm
lb_ $ sudo yum install httptunnel
Install httptunnel on Fedora
_____^^Kterm
$ sudo yum install httptunnel
Create an HTTP Uunnel with httptunnel
In order to set up an HTTP-encapsulated tunnel using httptunnel, refer to the 
example below.
On server side:
xterm
T $ sudo hts -F <server_ip_addr>:<port_of_your_app> 80
_J^®The above command tells hts to listen on port 80, and to redirect all 
traffic received on port 80 to <port_of_your_app>
On client side:
term
__ $ sudo htc -F <port_of_your_app> <server_ip_addr>:80

The above command tells htc to receive traffic on localhost:
<port_of_your_app> , and to redirect it to <server_ip_addr>:80. At this point, the 
application instances running on two end hosts can communicate with each 
other transparently via an HTTP tunnel.
If htc is running behind HTTP proxy, you can specify the HTTP proxy with - P 
option:
term
T__ $ sudo htc -P <my_proxy.com:proxy_port> -F
<port_of_your_app> <server_ip_addr>: 80
If you are conscious about the security of plain-text HTTP tunnels, or reduce 
the risk of firewall blocking, you can consider setting up SSH tunneling, 
which can protect you against eavesdropping, thereby is more robust against 
potential firewall fingerprinting.
Linux Tutorials
How to set up Ubuntu Desktop VM 
on Amazon EC2
Amazon Web Services (AWS) EC2 is the most widely used pay-as-you-go 
type of elastic compute cloud. You can launch a VM instance in a matter of 
minutes by using one of those Amazon Machine Images (AMIs) offered by 
EC2.
If your use case of EC2 involves Ubuntu Desktop, you need to understand 
that it is less straightforward to launch a Ubuntu Desktop VM instance on 
EC2, due to its desktop environment which needs a screen attached to it.
In this guide, I will describe how to run Ubuntu Desktop on Amazon EC2.
In a nutshell, the approach is to launch a Ubuntu Server instance first. Then 

install desktop environment on it, and access the headless server instance via 
VNC remote desktop.
Install Ubuntu Desktop on AWS EC2
I assume that you already set up a VM on EC2 by choosing Ubuntu Server ami. 
First, install Ubuntu Desktop on the server instance by running the following 
command.
xterm
Bi_ $ sudo apt-get install ubuntu-desktop
__ ^^^Reboot the VM instance.
Next, install VNC server on the VM.
xterm
T $ sudo apt-get install tightvncserver
L [After installation, launch VNC server (as a non-root user):
xterm
Hl_ $ vncserver : 1
The first time you run VNC server, it will ask you for VNC password. The 
VNC password should be at least 6 characters and up to 8 characters long. If 
the typed password is longer than that, only the first 8 characters will be used.
Once VNC server is launched successfully, it will create ~/.vnc directory and 
configuration files in it. A log file for VNC server will be located at
~/.vnc/
*.log
.
Modify ~/.vnc/xstartup as follows in order to launch GNOME session 
automatically upon VNC connection.
xterm
T__ $ vi ~/.vnc/xstartup
#!/bin/sh
xrdb $HOME/.Xresources
xsetroot -solid grey
export XKL_XMODMAP_DISABLE=1

gnome-session --session=ubuntu-2d
After modifying xstartup file, restart VNC server:
xterm
Ilk_ $ vncserver -kill : 1
$ vncserver :1
Last but not least, you must modify the security group associated with your EC2 
instance, to allow inbound VNC traffic on the EC2 instance. Since VNC uses 
tcp/5901 port number, add a new inbound TCP rule for port
5901. Make sure to apply the rule change, which will then be activated right 
away. No need to reboot the VM instance.

This is the end of the procedure for setting up Ubuntu Desktop VM on EC2.
Create a new 
rule:
Port range:
Source:
Custom TCP rule 
▼
TCP
« 
-
eg., 80 or 49152-65535)
0 - 65535 
sg-6elc6e07 (default) 
Delete
22 (SSH) 
0.0.0.0/0 
Delete
p.0.0.0/0
eg., 192.168.2.0/24, sg-47ad482e. or
ZU5fi7ffi(Vrtpfaiiltl
80 (HTTP) 
0.0.0.0/0 
Delete
1194 
0.0.0.0/0 
Delete
3389 (RDP) 
O.O.O.O/O 
Delete
Add Rule
6000 - 7000 
0.0.0.0/0 
Delete
6001 
0.0.0.0/0 
Delete
Apply Rule Changes
UDP
Port (Service) 
Source 
Action
Connect to Ubuntu Desktop VM
On the local host side where you will be running VNC client, follow the rest 
of the steps below.
First, set up an SSH tunnel to the remote EC2 instance by running the 
command below. Using an SSH tunnel for VNC session is strongly required 
since your VNC session goes over the public Internet, and anyone can easily 
snoop on your VNC traffic.
xterm
T__ $ ssh [email protected]_ec2_host -L 5901/127.0.0.1/5901
If the remote EC2 instance is configured to require key authentication, you 
need to specify your private key with -i option in the above ssh command, as 
described here.
Once the SSH tunnel is successfully established, install and launch VNC 
client (gtkvncviewer in this example).
term

T__ $ sudo apt-get install gtkvncviewer
$ gtkvncviewer
Type in the following VNC server info as follows, and connect. Note that 
VNC server end point should be 127.0.0.1:5901, not the remote VNC server, 
since your VNC traffic goes through an SSH tunnel.
Server: 127.0.0.1:5901
Password: your_vnc_password
gtkvncviewer
Connect to a remote desktop (VNC)
Once your
VNC session is established, you will see the familiar looking Ubuntu 
Desktop running on EC2.

@127.0.0.1 -gtkvncviewer
@127.0.0.1
9 < - 4:23 PM ft
Ubuntu Desktop
Disconnect /Sendkeys Fullscreen ^Screenshot ^Makekonondesktop

Linux Tutorials
How to mount HDFS using FUSE
Hadoop Distributed File System (HDFS) is a distributed, scalable filesystem 
developed as the back-end storage for data-intensive Hadoop applications. As 
such, HDFS is designed to handle very large files with "write-once-read- 
many" access model. As HDFS is not a full-fledged POSIX compliant 
filesystem, it cannot be directly mounted by the operating system, and file 
access with HDFS is done via HDFS shell commands.
However, one can leverage FUSE to write a userland application that exposes 
HDFS via a traditional filesystem interface. fuse-dfs is one such FUSEbased 
application which allows you to mount HDFS as if it were a traditional Linux 
filesystem.
If you would like to mount HDFS on Linux, you can install fuse-dfs along 
with FUSE as follows.
I assume that you have a HDFS cluster already up and running, and know the 
HDFS NameNode to connect to. I also assume that you would like to mount 
HDFS on a separate Debian/Ubuntu-based host.
On the host where you would like to mount HDFS, do the following.
First, install Java JDK.
Next, install fuse-dfs and all necessary dependencies as follows.
To install fuse-dfs on CentOS or RHEL 6:
I___ xterm
$ wget
__repository-1.0-1.noarch.rpm
$ sudo yum --nogpgcheck localinstall cdh3-repository1.0-1.noarch.rpm
$ sudo rpm --import
http://archive.cloudera.com/redhat/6/x86_64/cdh/RPMGPG-KEY-cloudera 
$ sudo yum install hadoop-0.20-fuse

To install fuse-dfs on Debian or Ubuntu 10.10 and earlier: 
xterm
$ wget http://archive.cloudera.com/one-click
install/$(lsb_release -cs)/cdh3-repository_1.0_all.deb $ sudo dpkg -i cdh3-
repository_1.0_all.deb
$ sudo apt-get update
$ sudo apt-get install hadoop-0.20-fuse
To install fuse-dfs on Ubuntu 12.04 and higher: 
term
$ wget http://archive.cloudera.com/one-click 
install/maverick/cdh3-repository_1.0_all.deb 
$ sudo dpkg -i cdh3-repository_1.0_all.deb
$ sudo apt-get update
$ sudo apt-get install hadoop-0.20-fuse
__ ^^MOnce fuse-dfs is installed, go ahead and mount HDFS using FUSE as 
follows.
xterm
T__ $ sudo hadoop-fuse-dfs dfs://<name_node_hostname>:
<namenode_port> <mount_point>
__ Once HDFS has been mounted at <mount_point>, you can use most of the 
traditional filesystem operations (e.g., cp, rm, cat, mv, mkdir, rmdir, more, scp). 
However, random write operations such as rsync, and permission related 
operations such as chmod, chown are not supported in FUSE-mounted HDFS.
Linux Tutorials
How to backup files to a remote
FTP server using lftp

lftp is a command-line FTP client with several advanced file transfer features. 
For example, lftp can upload or download a whole directory tree recursively 
and selectively, or resume interrupted file transfers. A popular use case of lftp 
is to mirror local files or folders to a remote FTP server. While
rsync is a popular mirroring software tool, it uses its own file synchronization 
protocol, and so does not work over FTP.
If you would like to back up local files or directory trees to a remote FTP 
server, you can proceed as follows.
Install lftp on Linux
For Debian-based Linux:
To install iftp on Ubuntu, Debian or Linux Mint:
xterm
lb_ $ sudo apt-get install iftp
For Red-Hat-based Linux:
To install lftp on CentOS, Fedora or RHEL: 
xterm
lb_ $ sudo yum install lftp
Back up Files to a Remote FTP Server with lftp
Now go ahead and run iftp command as follows. Note that $username, $password 
and $ftp_hostname need to be populated according to your
FTP server setup.
term

$ lftp -c "set ftp:list-options -a;
open ftp://$username:[email protected]$ftp_hostname; lcd
/path/to/local/directory;
cd /path/to/remote/directory/in/ftp/server;
mirror --reverse --delete --parallel=3 --olderthan='now-7days' --exclude-glob 
.git"
" --reverse" option means uploading files to a remote FTP server. "--delete" 
option means removing files not present in the source directory.
"--parallel=3" option means uploading upto 3 files in parallel. "--exclude-glob .git" 
option means excluding matching folders (e.g.,
.git ).
"--older-than='now-7days'" option means uploading files which were modified 
more than seven days ago.
Linux Tutorials
How to force password change at 
the next login on Linux
Question: I manage a Linux server for multiple users to share. I have just 
created a new user account with some default password, and I want the user 
to change the default password immediately after the first login. Is there a 
way to force a user to change his/her password at the next login?
In multi-user Linux environment, it's a standard practice to create user 
accounts with some random default password. Then after a successful login, a 
new user can change the default password to his or her own. For security 
reasons, it is often recommended to "force" users to change the default 
password after the first login to make sure that the initial one-time password 
is no longer used.

Here is how to force a user to change his or her password on the next 
login.
Every user account in Linux is associated with various password-related 
configurations and information. For example, it remembers the date of the 
last password change, the minimum/maximum number of days between 
password changes, and when to expire the current password, etc.
A command-line tool called chage can access and adjust password expiration 
related configurations. You can use this tool to force password change of any 
user at the next login.
To view password expiration information of a particular user (e.g., alice), run 
the following command. Note that you need root privilege only when you are 
checking password age information of any other user than yourself.
■_____ ^^Ixterin
$ sudo chage -l alice
[dev@localhost ~]sichage -I dev
Last password change ‘ 
: Apr 25, 2015
Password expires 
: never
Password inactive 
: never
Account expires 
: never
Minimum number of days between password change 
: 0
Maximum number of days between password change 
: 99999
Number of days of warning before password expires 
: 7
[dev@localhost ~]s
Force Password Change for a User
If you want to force a user to change his or her password, use the following 
command.
xterm
T__ $ sudo chage -d0 <user-name>

Originally the " -d <N>" option is supposed to set the "age" of a password (in 
terms of the number of days since January 1st, 1970 when the password was 
last changed). So -d0 indicates that the password was changed on January 1st, 
1970, which essentially expires the current password, and causes it to be 
changed on the next login.
Another way to expire the current password is via passwd command.
xterm
T $ sudo passwd -e <user-name>
__ ■> above command has the same effect of "chage -d0", making the 
current password of the user expire immediately.
Now check the password information of the user again, and you will see:
[dev@localhost ~]S chage -I dev
Last password change
Password expires
Password inactive
Account expires
Minimum number of days between password change
Maximum number of days between password change 
Number of days of warning before password expires 
[dev@localhost -]$
password must be changed! 
password must be changed 
password must be changed I, 
never
0
99999
7
When you log in again, you will be asked to change the password. You will 
need to verify the current password one more time before the change.
$ ssh dev@192.168.122.3
dev(3192168.122.31 s oassword:_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
|You are required to change your password immediately (root enforced)
Last login: Sun Oct 4 21:35:19 2015 from 192.168.122.1
WARNING: Your password has expired.
You must change your password now and login again!
Changing password for user dev.
Changing password for dev.
(current) UNIX password: []_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
To set more comprehensive password policies (e.g., password complexity, 
reuse prevention), you can use PAM. See the article for more detail.

Linux Tutorials
How to install Wine on Linux
Question: I need to run a Windows application on Linux, and for that, I 
would like to try Wine. How can I install Wine on [insert your Linux distro]?
Wine is a popular user-space software program which allows one to run 
Windows applications on Linux. Wine provides a run-time environment for a 
Windows application by serving Windows APIs called by the application. As 
of today, there are more than 27,000 Windows applications that are supported 
by Wine.
Install Wine on Ubuntu or Debian
Since Wine is included in the default repository of Debian-based 
distributions, you can install it with apt-get. However, if you are using a 64-bit 
system, you need to enable multi-architecture as Wine is a 32-bit application.
On 64-bit System:
term
$ sudo dpkg --add-architecture i386
$ sudo apt-get update
$ sudo apt-get install wine wine32 wine64 libwine libwine:i386 fonts-wine
On 32-bit System:
_____ ^^■xtenii
$ sudo apt-get install wine wine32 libwine fonts-wine

Install Wine on Ubuntu via PPA Repository
On Ubuntu-derivatives (Kubuntu or Lubuntu) or Linux Mint, you can also 
install Wine by using the Wine PPA maintained by WineHQ team, as shown 
below. This version is more recent than the one included in the default 
repository.
term
$ wget -qO - https://dl.winehq.org/wine
builds/winehq.key | sudo apt-key add
$ sudo apt-add-repository 'https://dl.winehq.org/winebuilds/ubuntu/'
$ sudo apt update
$ sudo apt install wine
Install Wine on Fedora
Installation of Wine is straightforward on Fedora. Simply use yum command. 
xterm
T__ $ sudo yum install wine
Install Wine on CentOS
To install Wine on CentOS, you first need to set up EPEL repository. Then 
use yum command to install it.
xterm
__________ xterm
$ sudo yum install wine
Install Wine on openSUSE

Wine is available in the default repository of openSUSE, so installation is 
hassle free with zypper command.
xterm
T__ $ sudo zypper install wine
Wine Post-installation
Below is the screenshot of the Wine configuration editor ( winecfg) which 
allows you to set application-specific Wine settings.

One useful application to install along with Wine is PlayOnLinux. This 
program allows you to quickly configure Wine for a pool of well-known 
Windows applications and games. Thus it is recommended to install 
PlayOnLinux if you are using Wine.
Troubleshooting
1. I cannot add a Ubuntu PPA repository due to GPG error.
Symptom: apt-add-repository results in the following errors even though I have 
imported the GPG key of the PPA repository.
Err:4 https://dl.winehq.org/wine-builds/ubuntu XXXXX InRelease The 
following signatures couldn't be verified because the public key is not 
available: NO_PUBKEY 76F1A20FF987672F
Reading package lists... Done
W: GPG error: https://dl.winehq.org/wine-builds/ubuntu XXXXX InRelease: 
The following signatures couldn't be verified because the public key is not 
available: NO_PUBKEY 76F1A20FF987672F E: The repository 
'https://dl.winehq.org/wine-builds/ubuntu XXXXX InRelease' is not signed. 
N: Updating from such a repository can't be done securely, and is therefore 
disabled by default.

The reason for the errors is probably because you imported a wrong GPG key 
(Reiease.key). You have to use a new key (winehq.key) as shown below.
xterm
lb_ $ wget -qO - https://dl.winehq.org/wine
builds/winehq.key | sudo apt-key add
Linux Tutorials
How to compile and install Nginx 
web server from source on Linux
As of today, Nginx is reportedly the most popular web server that powers the 

top-1000 websites on the Internet, and that is for a good reason. Built under 
the event-driven architecture, Nginx was designed with scalability in mind 
from day one. With its ability to sustain 10K concurrent connections with 
limited hardware, it's no wonder Nginx is being trusted for any mission- 
critical web server deployment.
In this tutorial, I am going to describe how to compile and install Nginx 
web server from source. While Nginx is available as a standard package on 
major Linux distros, you need to build it from source if you want to enable a 
custom third-party module (e.g., PageSpeed). Note that Nginx does not 
support loadable modules like Apache web server. You need to choose and 
include third-party module(s) to use at compile-time.
In this tutorial, I am going to enable the following extra Nginx modules 
during compile time.
SSL module: enable HTTPS/SSL support.
SPDY module: enable experimental support for SPDY.
Real IP module: change the client's IP address based on HTTP request 
header.
Addition module: add text before and after a HTTP response. XSLT 
module: convert an XML response with the help of XSLT templates. Image 
filter module: transform JPG/GIF/PNG images with libgd. GeoIP module: 
geolocate a client' IP address, using MaxMind databases. Substitution 
module: replace a specified string by another in HTTP responses.
DAV module: enable support for the WebDAV protocol.
FLV module: enable support for Flash Video (FLV) streaming videos. MP4 
module: enable support for H.264/AAC streaming videos. Gunzip module: 
decompresses gzip-compressed responses when necessary.
Gzip static module: send responses with gzip-precompressed content. 
Random index module: serve a randomly chosen index file in a directory. 
Secure link module: check authenticity of requested links.
Stub status module: get some status from Nginx.
Perl module: allow Perl in Nginx config files.
Mail SSL module: enable support for SSL/TLS protocol for mail proxy 
server.
Google Perftools module: enable Google performance tools profiling.

Install Prerequisites
First, install necessary packages to build Nginx as well as extra Nginx 
modules.
On Debian, Ubuntu or Linux Mint:
term
$ sudo apt-get install build-essential zlib1g-dev libpcre3-dev libssl-dev 
libxslt1-dev libxml2-dev libgd2-xpm-dev libgeoip-dev libgoogle-perftools- 
dev libperl-dev
On Fedora, CentOS or RHEL:
4 ■___ xterm
$ sudo yum install gcc-c++ pcre-devel zlib-devel make wget openssl-devel 
libxml2-devel libxslt-devel gd-devel _ perl-ExtUtils-Embed GeoIP-devel 
gperftools-devel L-J
Nginx Compilation
Download the latest stable version of Nginx from the official site.
xterm
lb_ $ wget http://nginx.org/download/nginx-1.4.4.tar.gz
__ ^^^Extract the content. Then run configure with appropriate options, and 
install it as follows.
I___ xterm
$ tar xvfvz nginx-1.4.4.tar.gz
$ cd nginx-1.4.4
$ ./configure --prefix=/usr/local/nginx --sbinpath=/usr/local/sbin/nginx --conf

path=/etc/nginx/nginx.conf --error-log
path=/var/log/nginx/error.log --http-log
path=/var/log/nginx/access.log --pid
path=/run/nginx.pid --lock-path=/run/lock/subsys/nginx
--user=nginx --group=nginx --with-file-aio --with-ipv6
--with-http_ssl_module --with-http_spdy_module --withhttp_realip_module -
-with-http_addition_module --withhttp_xslt_module --with-
http_image_filter_module -with-http_geoip_module --with-http_sub_module
- -withhttp_dav_module --with-http_flv_module --withhttp_mp4_module -- 
with-http_gunzip_module --with 
http_gzip_static_module --with-http_random_index_module
- -with-http_secure_link_module --with
http_degradation_module --with-http_stub_status_module
- -with-http_perl_module --with-mail --with 
mail_ssl_module --with-pcre --with 
google_perftools_module --with-debug
$ make
$ sudo make install
Post Installation Steps
Create a system user/group nginx which Nginx will run as.
xterm
Bl_ $ sudo useradd -r nginx
__ ^^Bcreate an init script which will start/stop Nginx. You can download 
init scripts for different Linux environments from the official site.
At this point, you should be able to start Nginx as follows.
xterm
$ sudo systemctl start nginx
_ ^^Bor:
term
__ $ sudo service nginx start

Verify that Nginx is running okay. 
xterm
$ sudo netstat -nap | grep nginx
dev@kdebian:
dev@kdebian:sudo /etc/init.d/nginx start
|[ ok ] nginx: starting.
dev@kdebian:~$
dev@kdebian:~$ sudo netstat -nap | grep nginx
Step 0 
0 0.0.0.0:80 
0.0.0.0:
*
_ _ _ _ _ LISTEN
37550/nginx
Unix 
3 
[ ] 
STREAM 
CONNECTED 
38291 
37550/nginx
Unix 
3 
[ ] 
STREAM 
CONNECTED 
38290 
37550/nginx
dev@kdebian:~$ 
dev@kdebian: 
dev@kdebian:
After verifying this, point your web browser to the IP address of the host 
where Nginx is running. You will see the following Nginx welcome page.

After installation, the configuration directory of Nginx is found at /etc/nginx, 
and the document root directory at 
/usr/local/nginx/html.
Linux Tutorials
How to make a file immutable on

Linux
Suppose you want to write-protect some important files on Linux, so that 
they cannot be deleted or tampered with by accident or otherwise. In other 
cases, you may want to prevent certain configuration files from being 
overwritten automatically by software. While changing their ownership or 
permission bits on the files by using chown or chmod is one way to deal with 
this situation, this is not a perfect solution as it cannot prevent any action 
done with root privilege. That is when chattr comes in handy.
chattr is a Linux command which allows one to set or unset attributes on a file, 
which are separate from the standard (read, write, execute) file permission. A 
related command is lsattr which shows which attributes are set on a file. While 
file attributes managed by chattr and lsattr are originally supported by EXT file 
systems (EXT2/3/4) only, this feature is now available on many other native 
Linux file systems such as XFS, Btrfs, ReiserFS, etc.
In this tutorial, I am going to demonstrate how to use chattr to make files 
immutable on Linux.
chattr and lsattr commands are a part of e2fsprogs package which comes pre­
installed on all modern Linux distributions.
Basic syntax of chattr is as follows.
xterm
Bl_ $ chattr [-RVf] [operator][attribute(s)] files...
The operator can be + (which adds selected attributes to attribute list), - 
(which removes selected attributes from attribute list), or = (which forces 
selected attributes only).
Some of available attributes are the following.
a : can be opened in append mode only.
A: do not update atime (file access time). c: automatically compressed when 
written to disk. C: turn off copy-on-write.
i: set immutable.

s: securely deleted with automatic zeroing.
Immutable Attribute
To make a file immutable, you can add immutable attribute to the file as 
follows. For example, to write-protect /etc/passwd file:
$ sudo chattr +i /etc/passwd
__ ^^HnoLc' that you must use root privilege to set or unset immutable 
attribute on a file. Now verify that immutable attribute is added to the file 
successfully.
term
lb_ $ lsattr /etc/passwd
Once the file is set immutable, this file is impervious to change for any user. 
Even the root cannot modify, remove, overwrite, move or rename the file. 
You will need to unset the immutable attribute before you can tamper with 
the file again.
To unset the immutable attribute, use the following command:
xterm
T $ sudo chattr -i /etc/passwd

sysadmin@ubuntu:~s Isattr /etc/passwd
. . . . . . . . e-- /etc/passwd 
Add immutable 
sysadmin@ubuntu:~$ 
sysadmin(aiibuntu:--$ 
S'
sysadmin@ubuntu:~$ sudo chattr +i /etc/passwd 
sysadmin@ubuntu:~$ 
sysadmin@ubuntu:~s 
sysadmin@ubuntu:Isattr /etc/passwd
|---i........ e- /etc/passwd| ♦ Check immutable attribute
sysadmin@ubuntu:-s 
sysadmin@ubuntii :-s
sysadmin@ubuntu:~s sudo rm /etc/passwd
rm: cannot remove ‘/etc/passwdr: Operation not permitted
sysadmin@ubuntu:
If you want to make a whole directory (e.g., /etc) including all its content 
immutable at once recursively, use -r option:
xterm
lb_ $ sudo chattr -R +i /etc
Append Only Attribute
Another useful attribute is append-only attribute which forces a file to grow 
only. You cannot overwrite or delete a file with append-only attribute set. 
This attribute can be useful when you want to prevent a log file from being 
cleared by accident.
Similar to immutable attribute, you can turn a file into append-only mode by: 
xterm
$ sudo chattr +a /var/log/syslog
__^^BNote that when you copy an immutable or append-only file to 
another file, those attributes will not be preserved on the newly created file.
Conclusion

In this tutorial, I showed how to use chattr and lsattr commands to manage 
additional file attributes to prevent (accidental or otherwise) file tampering. 
Beware that you cannot rely on chattr as a security measure as one can easily 
undo immutability. One possible way to address this limitation is to restrict 
the availability of chattr command itself, or drop kernel capability 
cap_linux_immutable. For more details on chattr and available attributes, 
refer to its man page.
Linux Tutorials
How to enable SSL for MySQL 
server and client
When users want to have a secure connection to their MySQL server, they 
often rely on VPN or SSH tunnels. Yet another option for securing MySQL 
connections is to enable SSL wrapper on an MySQL server. Each of these 
approaches has its own pros and cons. For example, in highly dynamic 
environments where a lot of short-lived MySQL connections occur, VPN or 
SSH tunnels may be a better choice than SSL as the latter involves expensive 
per-connection SSL handshake computation. On the other hand, for those 
applications with relatively few long-running MySQL connections, SSL 
based encryption can be reasonable. Since MySQL server already comes with 
builtin SSL support, you do not need to implement a separate security layer 
like VPN or SSH tunnel, which has their own maintenance overhead.
The implementation of SSL in an MySQL server encrypts all data going back 
and forth between a server and a client, thereby preventing potential 
eavesdropping or data sniffing in wide area networks or within data centers. 
In addition, SSL also provides identify verification by means of SSL 
certificates, which can protect users against possible phishing attacks.
In this article, we will show you how to enable SSL on MySQL server. Note 
that the same procedure is also applicable to MariaDB server.

Creating Server SSL Certificate and Private Key
We have to create an SSL certificate and private key for an MySQL server, 
which will be used when connecting to the server over SSL. First, create a 
temporary working directory where we will keep the key and certificate files.
term
lb_ $ sudo mkdir ~/cert
$ cd ~/cert
Make sure that OpenSSL is installed on your system where an MySQL server 
is running. Normally all Linux distributions have OpenSSL installed by 
default. To check if OpenSSL is installed, use the following command.
term
T $ openssl version
__ I^BopenSSL 1.0.1f 6 Jan 2014
Now go ahead and create the CA private key and certificate. The following 
commands will create ca-key.pem and ca-cert.pem.
$ openssl genrsa 2048 > ca-key.pem
$ openssl req -sha1 -new -x509 -nodes -days 3650 -key ca-key.pem > ca- 
cert.pem
__ ^^Blhe second command will ask you several questions. Go ahead and 
fill out those fields.
The next step is to create a private key for the server.
Bl_ $ openssl req -sha1 -newkey rsa:2048 -days 730 -nodes keyout server-
key.pem > server-req.pem

__ I^HThis command will ask several questions again, and you can put the 
same answers which you have provided in the previous step.
Next, export the server's private key to RSA-type key with this command 
below.
xterm
lb_ $ openssl rsa -in server-key.pem -out server-key.pem
_L~_Jl;incilly, generate a server certificate using the CA certificate.
term
$ openssl x509 -sha1 -req -in server-req.pem -days 730
-CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 > server-cert.pem
Configuring SSL on MySQL Server
After the above procedures, we should have a CA certificate, a server's 
private key and its certificate. The next step is to configure our MySQL 
server to use the key and certificates.
Before configuring the MySQL server, check whether the SSL options are 
enabled or disabled. For that, log in to the MySQL server, and type the query 
below.
term
_mysql> SHOW GLOBAL VARIABLES LIKE 'have_%ssl';
The result of this query will look like the following.

Bl 
krls@Slamet; - 
- + x
File Edit Tabs Help
kri stasl a met: 
mysql -u root -p
Enter password:
Welcome to the MySQL monitor. Commands end with ; or \g.
Your MySQL connection id is 36
Server version: 5.5.37-0ubuntu0.14.04.1 (Ubuntu)
Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its 
affiliates. Other names may be trademarks of their respective 
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> show global variables like 'have %ssl'; 
+---------------+---------- +
| Variable_name ] Value ]
| have_openssl | DISABLED | 
| havessl 
j DISABLED |
2 rows in set (0.00 sec) 
mysql> |
Note that the default value of have_openssl and have_ssl variables is disabled as 
shown above. To enable SSL in the MySQL server, go ahead
and follow the steps below.
1. Copy or move ca-cert.pem, server-cert.pem, and server-key.pem under /etc directory. 
xterm
$ sudo mkdir /etc/mysql-ssl
$ sudo cp ca-cert.pem server-cert.pem server-key.pem /etc/mysql-ssl
2. Open my.cnf of the server using a text editor. Add or un-comment the lines 
that look like below in [mysqld] section. These should point to the key and 
certificates you placed in /etc/mysql-ssl.
[mysqld] 

ssl-ca=/etc/mysql-ssl/ca-cert.pem 
ssl-cert=/etc/mysql-ssl/server-cert.pem 
ssl-key=/etc/mysql-ssl/server-key.pem
3. In my.cnf, also find bind-address = 127.0.0.1, and change it to: 
bind-address = *
That way, you can connect to the MySQL server from another host.
4. Restart MySQL service.
xterm
$ sudo service mysql restart
_ ^^Hor:
xterm
lb_ $ sudo systemctl restart mysql
_ ^^Bor:
xterm
lb_ $ sudo /etc/init.d/mysql restart
You can check whether the SSL configuration is working or not by 
examining the MySQL error log file (e.g., /var/log/mysql/mysql.log). If no warning 
or error is shown in the error log (like the screenshot below), it means that 
SSL configuration works okay.

U 
krls(®Slamet; /etc/mysql 
- + x
File Edit Tabs Help
15Q713 13:21:30 [Note] /usr/sbin/mysqld: ready for connections.
Version: '5.5.37-0ubuntu0.14.04.1' socket: '/var/run/mysqld/mysqld.sock' port: 3306 (Ubuntu)
150713 13:23:36 [Note] /usr/sbin/mysqld: Normal shutdown
150713 13:23:36 [Note] Event Scheduler: Purging the queue. 0 events
150713 13:23:36 InnoDB: Starting shutdown...
150713 13:23:38 InnoDB: Shutdown completed; log sequence number 1600279
150713 13:23:38 [Note] /usr/sbin/mysqld: Shutdown complete
150713 13:23:38 [Warning] Using unique option prefix myisam-recover instead of myisam-recover-options is deprecated and will be removed i 
n a future release. Please use the full name instead.
150713 13:23:38 [Note] Plugin 'FEDERATED' is disabled.
150713 13:23:38 InnoDB: The InnoDB memory heap is disabled
150713 13:23:38 InnoDB: Mutexes and rw_locks use GCC atomic builtins
150713 13:23:38 InnoDB: Compressed tables use zlib 1.2.8
150713 13:23:38 InnoDB: Using Linux native AID
150713 13:23:38 InnoDB: Initializing buffer pool, size = 128.GM
150713 13:23:38 InnoDB: Completed initialization of buffer pool
150713 13:23:38 InnoDB: highest supported file format is Barracuda.
150713 13:23:39 InnoDB: Waiting for the background threads to start
150713 13:23:40 InnoDB: 5.5.37 started; log sequence number 16G0279
150713 13:23:48 [Note] Server hostname (bind-address): 
port: 3306
150713 13:23:40 [Note] 
- 
resolves to '0.0.0.0';
150713 13:23:40 [Note] 
- 
resolves to
150713 13:23:40 [Note] Server socket created on IP: 'G.G.6.G'.
150713 13:23:40 [Note] Event Scheduler: Loaded 0 events
150713 13:23:40 [Note] /usr/sbin/mysqld: ready for connections.
Version: '5.5.37-0ubuntu0.14.04.1' socket: '/var/run/mysqld/mysqld.sock' port: 3306 (Ubuntu)
150713 13:27:16 [Warning] IP address '192.168.2.8' could not be resolved: Name or service not known
L
Another way to verify SSL configuration is by re-running the have_%ssl query 
inside the MySQL server.
term
_mysql> SHOW GLOBAL VARIABLES LIKE 'have_%ssl';

U 
krls@slamet: "/Pictures 
- + x
File Edit Tabs Help
kristastamet:~/Pictures$ mysql -u root -p
Enter password:
Welcome to the MySQL monitor. Commands end with ; or \g.
Your MySQL connection id is 53
Server version: 5.5.37-0ubuntu0.14.04.1 (Ubuntu)
Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its 
affiliates. Other names may be trademarks of their respective 
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> show global variables like 'have_%ssl'; 
+------------------- +------- +
| Variable_name | Value | 
+------------------- +------- +
haveopenssl 
YES
havessl 
YES
+------------------- +------- +
2 rows in set (0.00 sec)
mysql> |
Creating a User with SSL Privilege
After the server-side SSL configuration is finished, the next step is to create a 
user who has a privilege to access the MySQL server over SSL. For that, log 
in to the MySQL server, and type:
term

mysql> GRANT ALL PRIVILEGES ON *.*  TO ‘ssluser’@’%’ 
IDENTIFIED BY ‘dingdong’ REQUIRE SSL;
mysql> FLUSH PRIVILEGES;
Replace ssluser (username) and dingdong (password) with your own.
If you want to give a specific ip address (e.g., 192.168.2.8) from which the user
will access the server, use the following query instead. 
xterm
mysql> GRANT ALL PRIVILEGES ON *.*  TO 
‘ssluser’@’192.168.2.8’ IDENTIFIED BY 'dingdong' REQUIRE SSL;
mysql> FLUSH PRIVILEGES;

U 
krls@slamet: ~ 
- + x
File Edit Tabs Help
kris(aslametmysql -u root -p
Enter password:
Welcome to the MySQL monitor. Commands end with ; or \g.
Your MySQL connection id is 49
Server version: 5.5.37-0ubuntu0.14.04.1 (Ubuntu)
Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its 
affiliates. Other names may be trademarks of their respective 
owners.
iType 'help;' or '\h‘ for help. Type '\c' to clear the current input statement.
mysql> grant all privileges on *.*  to 'ssluser'O' identified by 'dingdong' req 
luire ssl;
Query OK, fl rows affected (fl.01 sec)
mysql> flush privileges;
Query OK, 0 rows affected (0.90 sec)
mysql> |
Configure SSL on MySQL Client
Now that MySQL server-side configuration is done, let's move to the client 
side. For MySQL client, we need to create a new key and certificate based on 
server's CA key and certificate.
Run the following commands on the MySQL server host where the server's 
CA key and certificate reside.
xterm
$ openssl req -shal -newkey rsa:2048 -days 730 -nodes keyout client­

key.pem > client-req.pem
_J^HSimilar to server-side configuration, the above command will ask 
several questions. Just fill out the fields like we did before.
We also need to convert the generated client key into RSA type as follows.
xterm
■______^^Hx term
$ openssl rsa -in client-key.pem -out client-key.pem 
_J^BFinally we need to create a client certificate using the server's CA 
key and certificate.
xterm
$ openssl x509 -sha1 -req -in client-req.pem -days 730
-CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 > client-cert.pem 
_B^^HNow transfer the ca-cert.pem, client-cert.pem, and client-key.pem files to to 
any host where you want to run MySQL client.
On the client host, use the following command to connect to the MySQL 
server with SSL.
xterm
$ mysql --ssl-ca=ca-cert.pem --ssl-cert=client-cert.pem
--ssl-key=client-key.pem -h <mysql-server-ip-address> u ssluser -p
__ After typing the ssluser's password, you will see the MySQL prompt 
as usual.
To check whether you are on SSL, type status command at the prompt.
xterm
T mysql> status;
If you are connected over SSL, it will show you the cipher information in the 
SSL field as shown below.

▼ 
Terminal - kns@raung:/etc/mysql 
- + X
File Edit View Terminal Tabs Help
kris@raung:/etc/mysqlS mysql -h 192.168.2.7 -u ssluser -p
Enter password:
Welcome to the MySQL monitor. Commands end with ; or \g.
Your MySQL connection id is 51
Server version: 5.5.37-0ubuntu0.14.04.1 (Ubuntu)
Copyright (c) 2000, 2015 r Oracle and/or its affiliates. All rights reserved.
Oracle is a registered trademark of Oracle Corporation and/or its 
affiliates. Other names may be trademarks of their respective 
owners.
Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> status;
mysql Ver 14.14 Distrib 5.5.43, for debian-linux-gnu (1685) using readline 6.3
Connection id: 
51
Current database:
Current user: 
ssluser@192.168.2.8
SSL: 
cipher in use is DHE-RSA-AES256-SUA
Current pager:
stdout 
■
Using outfile:
!> 1
Using delimiter:
J
Server version:
5.5.37-0ubuntu0.14.04.1 (Ubuntu)
Protocol version:
10
Connection:
192.168.2.7 via TCP/IP
Server characterset:
latinl
Db 
characterset:
latinl
Client characterset:
utfa
Conn, characterset:
utfa
TCP port:
3306
Uptime:
7 min 23 sec
Threads: 1 Questions:
145 Slow queries: 0 Opens: 223 Flush tables: 1 Open t
ables: 54 Queries per second avg: 0.327
If you do not want to specify client certificate and key information in the 
command line, you can create ~/.my.cnf file, and put the following information 
under [client] section.
[client]
ssl-ca=/path/to/ca-cert.pem
ssl-cert=/path/to/client-cert.pem

ssl-key=/path/to/client-key.pem
With that, you can simply use the following command line to connect to the 
server over SSL.
I_____ ^^Ixtenn 
________
H__ $ mysql -h <mysql-server-ip-address> -u ssluser -p__ I J
Linux Tutorials
How to resume a large SCP file 
transfer on Linux
Question: I was downloading a large file using SCP, but the download 
transfer failed in the middle because my laptop got disconnected from the 
network. Is there a way to resume the interrupted SCP transfer where I left 
off, instead of downloading the file all over again?
Originally based on BSD RCP protocol, SCP (Secure copy) is a mechanism 
that allows you to transfer a file between two end points over a secure SSH 
connection. However, as a simple secure copy protocol, SCP does not 
understand range-request or partial transfer like HTTP does. As such, 
popular SCP implementations like the scp command line tool cannot resume 
aborted downloads from lost network connections.
If you want to resume an interrupted SCP transfer, you need to rely on 
other programs which support range requests. One popular such program is 

rsync. Similar to scp, rsync can also transfer files over SSH.
Suppose you were trying to download a file ( bigdata.tgz) from a remote host 
remotehost.com using scp, but the SCP transfer was stopped in the middle due to a 
stalled SSH connection. You can use the following rsync command to easily 
resume the stopped transfer. Note that the remote server must have rsync 
installed as well.
__ xterm
$ cd /path/to/directory/of/partially_downloaded_file $ rsync -P --rsh=ssh 
[email protected]:bigdata.tgz _ ./bigdata.tgz
The -P option is the same as --partial --progress, allowing rsync to work with 
partially downloaded files. The --rsh=ssh option tells rsync to use ssh as a remote 
shell.
Once the command is invoked, rsync processes on local and remote hosts 
compare a local file (./bigdata.tgz) and a remote file
([email protected]:bigdata.tgz), determine among themselves what portion of the file 
is not the same, and transfer the discrepancy to either end. In this case, 
missing bytes in the partially downloaded local file is downloaded from a 
remote host.
If the above rsync session itself gets interrupted, you can resume it as many 
time as you want by typing the same command. rsync will automatically restart 
the transfer where it left off.

Linux Tutorials
How to install Maven on CentOS
Apache Maven is a project management software, managing building, 
reporting and documentation of a Java-based development project. In order to 
install and configure Apache Maven on CentOS, follow these steps.
First of all, you need to install Java development kit (JDK). Make sure to 
install JDK, not Java Runtime Environment (JRE).
Then go ahead and download the latest Maven binary from its official site.
For example, for version 3.0.4:
$ wget
http://mirror.cc.columbia.edu/pub/software/apache/maven /maven-
3/3.0.4/binaries/apache-maven-3.0.4-bin.tar.gz $ sudo tar xzf apache-maven-
3.0.4-bin.tar.gz -C /usr/local
$ cd /usr/local
$ sudo ln -s apache-maven-3.0.4 maven
Next, set up Maven path system-wide:
term
lb_ $ sudo vi /etc/profile.d/maven.sh 
export M2_HOME=/usr/local/maven
export PATH=${M2_HOME}/bin:${PATH}
Finally, log out and log in again to activate the above environment variables.

To verify successful installation of Maven, check the version of Maven:
term
lb_ $ mvn -version
Use Maven Behind Proxy
Optionally, if you are using Maven behind a proxy, you must define the 
proxy as follows.
term
lb_ $ vi ~/.m2/settings.xml 
<settings>
<proxies>
<proxy>
<active>true</active>
<protocol>http</protocol>
<host>proxy.host.com</host>
<port>port_number</port>
<username>proxy_user</username>
<password>proxy_user_password</password>
<nonProxyHosts>www.google.com|
*.somesite.com</nonProxyHosts>
</proxy>
</proxies>
</settings>

Linux Tutorials
How to disable HTTP redirect in
wget
Question: When I run wget to fetch a URL X which is redirected to another 
URL Y, wget, by default, goes fetch URL Y automatically. However, I would 
like to force wget to only fetch the original URL X without following the 
redirection. How can I stop wget from following a redirected URL?
In the HTTP specification, a redirect response (with 3xx HTTP response) 
indicates to the web browser that the requested URL is moved at another 
location. The redirect response then contains the URL of the redirect target.
Like regular HTTP clients, wget supports URL redirection (also known as

URL forwarding), which means that when you attempt to download a 
redirected URL, wget will automatically follow URL redirect to fetch the 
redirected target. If for some reason you want to disable URL direction, and 
stop with
3xx status code, you can use --max-redirect=number option with wget. This option is 
used to specify the maximum number of (recursive) redirections to follow, 
which is set to 20 by default.
If you want to disable HTTP redirects in wget, use --max-redirect=0 option as 
follows.
term
T $ wget --max-redirect=0 http://www.aaa.eom/a.html 
--2014-10-31 23:08:58-- http://www.aaa.com/a.html
Resolving aaa.com (aaa.com)... 1.2.3.4
Connecting to aaa.com (aaa.com)|1.2.3.4|:80... connected.
HTTP request sent, awaiting response... 301 Moved Permanently Location: 
http://www.bbb.com/b.html [following]
0 redirections exceeded.
As you can see, when wget encounters a HTTP redirect reponse, it does not 
follow the redirection, and simply stop with "0 redirections exceeded" error 

message. It will also show a received HTTP status code (e.g., 301).
Note that curl, which is another similar HTTP client, behave oppositely. By 
default, curl does not follow URL redirection. To force curl to redirect a URL, 
you have to use -L option.
Linux Tutorials
How to suppress warning or error 
messages in find command
If you have used find command on Linux, you probably have encountered the 
case where the command ends up producing an endless list of warning or 
error messages in its output, because you are "permission denied" to access 
certain files or directories.
term
$ find / -name "my.txt 
find: './proc/18/task/18/fd': Permission denied
find: './proc/18/task/18/fdinfo': Permission denied
find: './proc/18/task/18/ns': Permission denied
find: './proc/18/fd': Permission denied
find: './proc/18/fdinfo': Permission denied
find: './proc/18/ns': Permission denied

Such output is probably meaningless to you, so you may want to exclude 
these warning or error messages from the output, and get actual search result 
only. How can you suppress warnings or errors, and only get search 
result in
find command?
The find command sends any error or warning message to standard error 
output (i.e., stderr). So all you have to do to suppress these messages is to 
redirect stderr to /dev/null.
The following is how to suppress all warnings or errors in find command.
I______^^Ixterm
M__ $ find . -name "my.txt" 2>/dev/null
__ ^Blf you want to capture stderr output in a separate file for later 
inspection, you can do the following instead.
xterm
T $ find . -name "my.txt" 2> find_error.txt
if you want to exclude specific warnings only, you can filter out warning 
messages from stderr selectively by using grep, instead of redirecting the whole 
stderr. For example:
term
$ find . -name "my.txt" 2>&1 | grep -v "Permission denied"
The above command will suppress only "Permission denied' 

warning message, and show any other warning or error messages unfiltered.
Linux Tutorials
How to copy and paste between 
VMware Player VM and host 
desktop
If you are using a virtual machine (VM) running on VMware Player, you 
may have wondered how to copy and paste a text from the guest VM to 
VMware host desktop, and vice versa. Otherwise, you will have to explicitly 
transfer the text between the VM and VMware host by other means (e.g., 
SSH), which is quite cumbersome.
Fortunately the copy and paste problem will just go away if you install 
VMware Tools on your VMware Player VM. VMware Tools offer many cool 
features to your VMs. For example, you can move the cursor in and out of 
your VM console without pressing <ctrl>+<alt>. Also, you can now easily copy 
and paste texts in and out of your VMware Player's VM console and VMware 
host desktop machine.
To install VMware Tools on your VMware Player VM, follow these 
instructions.
Besides copying and pasting texts, VMware Tools even allow you to share 
folders between guest VMs and their host desktop. To enable this feature, 
following the steps below.
During installation of VMware Tools, vmware-install.pl will ask you:
The VMware Host-Guest Filesystem allows for shared folders between the 
host OS and the guest OS in a Fusion or Workstation virtual environment. Do 
you wish to enable this feature?

Make sure to answer yes, which is actually the default answer.
Once you reboot your VM after successfully installing VMware Tools, you 
can freely copy and paste, as well as copy files between VMware Player's 
guest VMs and VMware host desktop.
Linux Tutorials
How to set up a lightweight web 
server on Raspberry Pi
There are a variety of web server software available for Linux-based 
platforms including Raspbian. Using one of those available web server 
software, we can turn Raspberry Pi into a 24/7 available portable web server. 
In this case, however, we must understand that Raspberry Pi has hardware 
limitations in terms of CPU clock speed, memory, etc. As such, we want to 
avoid running resource-heavy software (e.g., Apache) on Raspberry Pi.
Among web server software available for Raspbian, Nginx and Lighttpd are 
two of the most widely used lightweight web server engines. This tutorial 
will describe how to install and set up Nginx or Lighttpd web server on 
Raspbianpowered Raspberry Pi. Choose one as you prefer.
Install and configure Nginx web server on 
Raspberry Pi
Update your repository and install Nginx with apt-get command.
xterm

$ sudo apt-get update
$ sudo apt-get install nginx
__ B^Hchange the default document root directory of Nginx from 
/usr/share/nginx/www to /var/www.
I______^^Ixterm
lb_ $ sudo mkdir /var/www
_L—Bkdit the default site's configurations as follows.
xterm
lb_ $ sudo nano /etc/nginx/sites-available/default 
server {
listen 80;
server_name $domain_name;
root /var/www;
index index.html index.htm;
access_log /var/log/nginx/access.log;
error_log /var/log/nginx/error.log;
}

Start Nginx web server.
xterm
T__ $ sudo service nginx restart
To test if Nginx web server is working or not, put a simple index.html in 
/var/www, and open it with your browser.
Any web server with no server side language will not be fully functioning as 
it cannot serve any dynamic content. For a typical web server, PHP is used as 
a primary server side language. A PHP request sent by user's web browser to 
a web server is handed over to PHP for execution. Once PHP has completed 
its processing, the output is sent back to a web server, which is then finally 
forwarded to the browser.
With Nginx, php-fpm is used to execute PHP programs. So the next step is to 
install php-fpm as well as php-apc. The latter is a PHP extension for accelerating 
PHP performance.
Install php5-fpm and php-apc with apt-get.
xterm
T__ $ sudo apt-get install php5-fpm php-apc

Edit the default site's configuration to add the following inside
server block.
xterm
T__ $ sudo nano /etc/nginx/sites-available/default
server {
listen 80;
server_name $domain_name; root /var/www;
index index.html index.htm;
access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log;
location ~.php$ {
fastcgi_pass unix:/var/run/php5-fpm.sock; 
fastcgi_split_path_info A(.+.php)(/.
*)$;
fastcgi_index index.php;
fastcgi_param SCRIPT_FILENAME
$document_root$fastcgi_script_name;
fastcgi_param HTTPS off;
try_files $uri =404;
include fastcgi_params;
}
}


xterm
T $ sudo service nginx restart
I Ias a test, create a simple PHP file in /var/www as follows.
xterm
T $ sudo nano /var/www/test.php
__ ^^B<?php
phpinfo();
?>
Go to http://<raspberrypi-ip-address>/test.php. If you see information about your PHP 
setup, it means php-fpm is set up properly.

Install and configure Lighttpd web server on
Raspberry Pi
Lighttpd is another speed-enhanced lightweight web server software that can 
run on Raspbian. Proceed as follows to install and configure Lighttpd on 
Raspbian.

Create and add user/group named www-data on your Raspberry Pi. 
xterm
T_ $ sudo addgroup --system www-data
$ sudo adduser www-data www-data
Update your repository, and install Lighttpd with apt-get.
I__
xterm
lb_ $ sudo apt-get update
$ sudo apt-get install lighttpd
To test whether Lighttpd is working or not, go to http://<raspberrypiip- 
address>/index.lighttpd.html from your browser to verify the page is loading.

£ + 
Welcome page -NaFirefox 
* o x
Hie Edit View History Bookmarks Tools Help
U Welcome page |+j
1$ 192.1682&mdex.lighttpd.html 
|]v J 0v jjv Yahoo 
~~h) 
5 v 0
Placeholder page
The owner of this web site has not put up any web pages yet. Please come back later.
You should replace this page with your own web pages as soon as possible.
Unless you changed its configuration, your new server is configured as follows:
»Configuration files can be found in /etc/lighttpd. Please read /etc/lighttpd/conf-available/README file.
»The Documents, which is the directory under which all your HTML files should exist, is set to Mm,
»CGI scripts are looked for in /usr/lib/cgi-bin, which is where Debian packages will place their scripts. You can enable cgi module by using command 
"lighty-enable-mod cgi".
»Log files are placed in Mr/log/lighttpd, and will be rotated weekly. The frequency of rotation can be easily changed by editing /etc/logrotate.d 
/lighttpd.
»The default directory index is index.html, meaning that requests fora directory /foo/bar/ will give the contents of the file /var/www/foo/bar 
/index.html if it exists (assuming that Mm is your Documents).
• You can enable user directones by using command "lighty-enable-mod userdir"
About this page
This is a placeholder page installed by the Debian release of the Lighttpd server package,
This computer has installed the Debian GNU/Unux operating system, but it has nothing to do with the Debian Project. Please do not contact the Debian 
Project about it.
If you find a bug in this Lighttpd package, or in Lighttpd itself, please file a bug report on it. Instructions on doing this, and the list of known bugs of this 
package, can be found in the Debian Bug Tracking System,

Next, install and enable php-cgi for Lighttpd web server. 
xterm
T_ $ sudo apt-get install php5-cgi
$ sudo lighty-enable-mod fastcgi-php
Create a simple PHP file in /var/www, and open Go to 
http://<raspberrypi-ip-address>/test.php through your browser. If 
you see the following PHP page, it means PHP has been set up successfully.
term
$ sudo nano /var/www/test.php
<?php 
phpinfo();

Configure MySQL and phpMyAdmin
In many cases, a web server is operated along with a backend database 
server. This part describes how to install MySQL as a database server and 
phpMyAdmin as the management interface for manipulating databases.
Install MySQL, phpMyAdmin, and php5-mysql.

term
mysql phpmyadmin
During MySQL server installation process, you will be asked to configure the 
password for root user of MySQL. You will also be asked to choose the web 
server installed in the system (Apache2 or Lighttpd). In case of Nginx, you 
can leave the web server selection field empty.
During phpMyAdmin installation, you will be asked to configure database for 
phpMyAdmin. Answer yes. When asked to enter the password of the 
administrative user, provide the password.
Make a link of phpMyAdmin from /usr/share/phpmyadmin to /var/www/phpmyadmin.
xterm
$ sudo In -s /usr/share/phpmyadmin /var/www/phpmyadmin
Open phpMyAdmin from your browser by going to http://<raspberrypiip- 
address>/phpmyadmin, and log in as root using the administrative password that 
you have set earlier.

+ 
phpMyAdmin - Mozilla Firefox 
v * x
File Edrt View History Bookmarks Tools Help
phpMyAdmin 
|4
*|
| Ij 192.168.2.6/phpmyadmin/ 
DVC ®v HvYab A $ v Q
phpMyAdnln

j + 
192j682.6/loralhost|phpMyAdnfiin3.4.1t1deb2-Mozillal:irefox 
* o x
Hie Edit View History Bookmarks Tools Help
$ 192168.26/localhost|phpMyAd... |
I $ 192.168.26'phpmyadmini 
Qf 0v jjv Yahoo 
""h) 
$ v Q
A
phpMyAdm/n
Most
Databases jj'SQL ^Status -^Processes |JPrivileges @ Export B Import <> Variables 1 Charsets ^Engines More
inTAm^nliAn aaaama
General Set^ngs 
I MySQL
inioniiaiion^scnema
j mysql
if? Change password 
1 $emr-via
ft 
r r nr ft, i 
j
|j performance_schema
= ..Pftl , 
M . . 
। Server version: 5. Mwheezyl
= MySQL connection collation«: utf8 general ci 
’ 
J
• Protocol version' 10
0 phpmyadmin
• User rootglocalhost
• MySQL chaiset: UTF-8 Unicode (utf8)
Appearance Settings
f Language v: English
0 Theme /Style: pmahomme j 
»lighttpd/1.4.31
.NySQLctalfMSa
• PHP extension: mysqli»
/ More settings

Linux Tutorials
How to use LVM in Linux
Logical Volume Manager (LVM) is a versatile disk management system that 
can easily be used in Linux or similar operating systems. Traditional 
partitions are created in fixed sizes, and resizing them is a tedious process. 
On the other hand, LVM creates and manages logical volumes off of physical 
hard disks, and provides administrators the flexibility to extend and shrink 
logical volumes easily on demand without damaging stored data. Additional 
hard disks can be added to LVM at will, further increasing existing logical 
volumes. LVM does not need reboot as long as the kernel is aware of the 
existence of a partition.
LVM uses a hierarchical structure as it can be seen in the following diagram.


At the top, we have physical volumes. One or more physical volumes are 
used to create a volume group. Logical volumes are then created from these 
volume groups. As long as there is space available in the volume group, we 
can create logical volumes from the volume group. File system is created on 
these logical volumes, which are then mounted and accessible in the 
operating system.
LVM Test Scenario
This tutorial will describe how to use LVM to create and manage LVM 
volumes in Linux. The tutorial will be divided into two parts. In the first 
part, we will create several logical volumes on one hard disk, and mount 
them in /lvm-mount directory. We will then resize the created volumes. In the 
second part, we will add additional volumes created from a second hard disk 
to LVM.
Preparing Disk Partitions
Disk partitions are created using fdisk. We will create three partitions of 1 GB 
each, though identical sized partitions are not mandatory. Also, the partitions 
are created as type 8e to make them compatible with LVM.
term
__# fdisk /dev/sdb\r\n
Command (m for help): n ## new
Command action
e extended
p primary partition (1-4)
p ## primary

Partition number (1-4): 1 ## partition number
First cylinder (1-1044, default 1): ## hit enter
Last cylinder, +cylinders or +size{K,M,G} (1-1044, default 1044): +1G ## 
size
Command (m for help): t ## change type
Selected partition 1
Hex code (type L to list codes): 8e ## code for LVM
Changed system type of partition 1 to 8e (Linux LVM)
We repeat the same steps to create two other partitions. After the partitions 
are created, we should get an output similar to this:
xterm

Ilk_ # fdisk -l
Device Boot Start End Blocks Id System 
/dev/sdb1 1 132 1060258+ 8e Linux LVM 
/dev/sdb2 133 264 1060290 8e Linux LVM 
/dev/sdb3 265 396 1060290 8e Linux LVM
Preparing Physical Volumes
The newly created partitions are used to store physical volumes. LVM can 
work with different sized physical volumes.
# pvcreate /dev/sdb1
# pvcreate /dev/sdb2
# pvcreate /dev/sdb3
Physical volumes can be verified using the following command. The 
following section contains partial output. /dev/sdb2 is a new physical volume of 
1.01 GiB.
term
# pvdisplay 
--- NEW Physical volume --

PV Name /dev/sdb2
VG Name
PV Size 1.01 GiB 
Allocatable NO 
PE Size 0 
Total PE 0 
Free PE 0 
Allocated PE 0
PV UUID jszvzz-ENA2-g5Pd-irhV-T9wi-ZfA3-0xo092
Physical volumes can be deleted using the following command.
xterm
T_ # pvremove /dev/sdb1
Preparing Volume Groups
The following command creates a volume group named volume-group1 by using 
the physical volumes /dev/sdb1, /dev/sdb2 and /dev/sdb3.
xterm

# vgcreate volume-groupl /dev/sdbl /dev/sdb2 /dev/sdb3 
Volume groups can be verified using the following command.
term
# vgdisplay
--- Volume group --
VG Name volume-group1
System ID
Format lvm2
Metadata Areas 3
Metadata Sequence No 1
VG Access read/write
VG Status resizable
MAX LV 0
Cur LV 0
Open LV 0
Max PV 0
Cur PV 3
Act PV 3
VG Size 3.02 GiB
PE Size 4.00 MiB
Total PE 774
Alloc PE / Size 0 / 0
Free PE / Size 774 / 3.02 GiB
VG UUID bwd2pS-fkAz-lGVZ-qc7C-TaKv-fFUC-IzGNBK

We can view used/total size of the volume group from the output. Logical 
volumes take the space of the volume group. As long as there is free space 
available in the volume group, we can create logical volumes.
Volume groups can be deleted using the following command. 
xterm
T_ # vgremove volume-group1

Creating Logical Volumes
The following command creates a logical volume named lv1 of size 100MB. 
We are using small sized partitions to reduce processing time. The logical 
volume will take its space from the volume group defined earlier.
xterm
lb_ # Ivcreate -L 100M -n lv1 volume-groupl
L [Logical volumes can be verified using the command ivdisplay. 
xterm
lb_ # ivdisplay
--- Logical volume -­
LV Name /dev/volume-group1/lv1
V G Name volume-group1
LV UUID YNQ1aa-QVt1-hEj6-ArJX-I1Q4-y1h1-OFEtlW LV Write Access 
read/write
LV Status available
# open 0
LV Size 100.00 MiB
Current LE 25
Segments 1
Allocation inherit
Read ahead sectors auto
- currently set to 256
Block device 253:2

Now that the logical volume is ready, we can format and mount the logical 
volume like any other EXT2/3/4 partition.
I_^lxterm
# mkfs.ext4 /dev/volume-group1/lv1
# mkdir /lvm-mount
_ # mount /dev/volume-groupl/lvl /lvm-mount/
Once the logical volume is mounted, we can access it by reading/writing to 
the mount point /lvm-mount/. To create and mount additional logical volumes, 

we can repeat this process.
Finally, we can delete any logical volume with ivremove. 
xterm
lb_ # umount /lvm-mount/
# lvremove /dev/volume-group1/lv1
Expanding an LVM Volume
The ability to resize a logical volume is the best part about using LVM. This 
section will discuss how we can expand an existing logical volume. We will 
be expanding the previously created logical volume lv1 to 200 MB.
Note that after resizing a logical volume, we also need to resize the file 
system to match. This extra step varies depending on which file system is 
created in the volume. In this tutorial, we created ext4 file system on lv1, so 
the instruction here focused on ext4 file system (it is compatible with ext2/3 
file system as well). The sequence of the commands is important.
First, we unmount the volume. 
xterm
lb_ # umount /lvm-mount/
I [Then, the size of the volume is set to be 200M.
xterm
Hb_ # lvresize -L 200M /dev/volume-group1/lv1
I [Next, the disk is checked for errors.
xterm
T # e2fsck -f /dev/volume-group1/lv1
V-JAfter that, the ext4 information is updated. 
xterm
# resize2fs /dev/volume-group 1/lv1
__^^H'rhe logical volume should be extended to 200 MB by now. We can 
verify it by checking the LV status.
term

# Ivdisplay
--- Logical volume -­
LV Name /dev/volume-group1/lv1
V G Name volume-group1
LV UUID 9RtmMY-0RIZ-Dq40-ySjU-vmrj-f1es-7rXBwa LV Write Access 
read/write
LV Status available
# open 0
LV Size 200.00 MiB
Current LE 50
Segments 1
Allocation inherit
Read ahead sectors auto
- currently set to 256
Block device 253:2

Now the logical volume can be mounted again, and be used just like any 
partition.
Shrinking an LVM Volume
This section will cover the method of reducing the size of an LVM. The 
sequence of the commands is important. Again, this instruction is valid for 
ext2/3/4 file system.
Note that reducing the size of the logical volume to a value less than stored 
data will end in loss of data.

First, the volume is unmounted.
xterm
T # umount /dev/volume-groupl/lvl
I iThen, the volume is checked for errors.
xterm
# e2fsck -f /dev/volume-groupl/lvl
L [Next, the ext4 information is updated.
xterm
lb_ # resize2fs /dev/volume-group1/lv1 100M
1^1 After that, the logical volume is reduced.
xterm
Hb_ # lvresize -L 100M /dev/volume-group1/lv1
WARNING: Reducing active logical volume to l00.00 MiB 
THIS MAY DESTROY YOUR DATA (filesystem etc.) 
Do you really want to reduce lvl? [y/n]: y
Reducing logical volume lvl to l00.00 MiB
Logical volume lvl successfully resized
Finally, the updated size of the logical volume is verified. 
xterm
lb_ # lvdisplay
--- Logical volume --

LV Name /dev/volume-group1/lv1
VG Name volume-group1
LV UUID 9RtmMY-0RIZ-Dq40-ySjU-vmrj-f1es-7rXBwa LV Write Access 
read/write
LV Status available
# open 0
LV Size 100.00 MiB
Current LE 25
Segments 1
Allocation inherit
Read ahead sectors auto
- currently set to 256
Block device 253:2
Expanding a Volume Group

This section will cover the method of expanding a volume group by adding a 
new physical volume to the volume group. Let us assume that our volume 
group volume-group1 is full, and needs to be expanded. Our current hard disk 
(sdb) does not have any spare partitions, and we have added another hard disk 
(sdc). We will see how we can expand the volume group by adding a partition 
from sdc.
To check the current state of VG.
xterm
# vgdisplay volume-groupl
--- Volume group --
VG Name volume-group1
System ID
Format lvm2
Metadata Areas 3
Metadata Sequence No 8
VG Access read/write
VG Status resizable
MAX LV 0
Cur LV 1
Open LV 0
Max PV 0
Cur PV 3
Act PV 3
VG Size 3.02 GiB
PE Size 4.00 MiB
Total PE 774
Alloc PE / Size 25 / 100.00 MiB
Free PE / Size 749 / 2.93 GiB
VG UUID bwd2pS-fkAz-lGVZ-qc7C-TaKv-fFUC-IzGNBK


First, we create a 2 GB partition sdc1 of type LVM (8e) as explained earlier in 
the tutorial.
term
__# fdisk /dev/sdc
Command (m for help): n
Command action
e extended
p primary partition (1-4)
p
Partition number (1-4): 1
First cylinder (1-1044, default 1):
Using default value 1
Last cylinder, +cylinders or +size{K,M,G} (1-1044, default 1044): +2G
Command (m for help): t
Selected partition 1
Hex code (type L to list codes): 8e
Changed system type of partition 1 to 8e (Linux LVM)
Command (m for help): w
The partition table has been altered!

Then, we create a physical volume /dev/sdci.
xterm
lb_ # pvcreate /dev/sdc1
__ ^^Bnow that the physical volume is ready, we can simply add it to the 
existing volume group volume-group1.

term
T # vgextend volume-groupl /dev/sdcl
e can verify it using vgdisplay.
term
T # vgdisplay
--- Volume group --
VG Name volume-group1
System ID
Format lvm2
Metadata Areas 4
Metadata Sequence No 9
VG Access read/write
VG Status resizable
MAX LV 0
Cur LV 1
Open LV 0
Max PV 0
Cur PV 4
Act PV 4
VG Size 5.03 GiB
PE Size 4.00 MiB
Total PE 1287
Alloc PE / Size 25 / 100.00 MiB
Free PE / Size 1262 / 4.93 GiB
VG UUID bwd2pS-fkAz-lGVZ-qc7C-TaKv-fFUC-IzGNBK


Note that although we have used a separate disk for demonstration, any disk 
of type 8e can be used for expanding a volume group.
To sum up, LVM is a very powerful tool for creating and managing resizable 
partitions. In this tutorial, we have seen how dynamic partitions can be 
created and used using LVM. We have also seen the method of 
expanding/reducing the logical volumes and volume groups, and adding new 
hard disks to LVM.
Hope this helps.


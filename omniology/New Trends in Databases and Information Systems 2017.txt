123
Mārīte Kirikova · Kjetil Nørvåg 
George A. Papadopoulos · Johann Gamper 
Robert Wrembel · Jérôme Darmont
Stefano Rizzi (Eds.)
ADBIS 2017 Short Papers and Workshops
AMSD, BigNovelTI, DAS, SW4CH, DC
Nicosia, Cyprus, September 24–27, 2017
Proceedings
New Trends in Databases 
and Information Systems
Communications in Computer and Information Science 
767

Communications
in Computer and Information Science
767
Commenced Publication in 2007
Founding and Former Series Editors:
Alfredo Cuzzocrea, Xiaoyong Du, Orhun Kara, Ting Liu, Dominik Ślęzak,
and Xiaokang Yang
Editorial Board
Simone Diniz Junqueira Barbosa
Pontiﬁcal Catholic University of Rio de Janeiro (PUC-Rio),
Rio de Janeiro, Brazil
Phoebe Chen
La Trobe University, Melbourne, Australia
Joaquim Filipe
Polytechnic Institute of Setúbal, Setúbal, Portugal
Igor Kotenko
St. Petersburg Institute for Informatics and Automation of the Russian
Academy of Sciences, St. Petersburg, Russia
Krishna M. Sivalingam
Indian Institute of Technology Madras, Chennai, India
Takashi Washio
Osaka University, Osaka, Japan
Junsong Yuan
Nanyang Technological University, Singapore
Lizhu Zhou
Tsinghua University, Beijing, China

More information about this series at http://www.springer.com/series/7899

Mārīte Kirikova
• Kjetil Nørvåg
George A. Papadopoulos
• Johann Gamper
Robert Wrembel
• Jérôme Darmont
Stefano Rizzi (Eds.)
New Trends in Databases
and Information Systems
ADBIS 2017 Short Papers and Workshops
AMSD, BigNovelTI, DAS, SW4CH, DC
Nicosia, Cyprus, September 24–27, 2017
Proceedings
123

Editors
Mārīte Kirikova
Riga Technical University
Riga
Latvia
Kjetil Nørvåg
Norwegian University of Science
and Technology
Trondheim
Norway
George A. Papadopoulos
University of Cyprus
Nicosia
Cyprus
Johann Gamper
Free University of Bozen-Bolzano
Bozen-Bolzano
Italy
Robert Wrembel
Poznan University of Technology
Poznan
Poland
Jérôme Darmont
Université de Lyon, Lyon 2, ERIC EA 3083
Lyon
France
Stefano Rizzi
University of Bologna
Bologna
Italy
ISSN 1865-0929
ISSN 1865-0937
(electronic)
Communications in Computer and Information Science
ISBN 978-3-319-67161-1
ISBN 978-3-319-67162-8
(eBook)
DOI 10.1007/978-3-319-67162-8
Library of Congress Control Number: 2017952393
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The European Conference on Advances in Databases and Information Systems
(ADBIS) celebrates this year its 21st anniversary. Previous ADBIS conferences were
held in St. Petersburg (1997), Poznan (1998), Maribor (1999), Prague (2000), Vilnius
(2001), Bratislava (2002), Dresden (2003), Budapest (2004), Tallinn (2005), Thessa-
loniki (2006), Varna (2007), Pori (2008), Riga (2009), Novi Sad (2010), Vienna
(2011), Poznan (2012), Genoa (2013), Ohrid (2014), Poitiers (2015), and Prague
(2016). The conferences were initiated and supervised by an international Steering
Committee consisting of representatives from Armenia, Austria, Bulgaria, the Czech
Republic, Cyprus, Estonia, Finland, France, Germany, Greece, Hungary, Israel, Italy,
Latvia, Lithuania, FYR of Macedonia, Poland, Russia, Serbia, Slovakia, Slovenia, and
the Ukraine.
ADBIS can be considered as one of the most established and recognized confer-
ences in Europe, in the broad ﬁeld of databases and information systems.
The ADBIS conferences aim at: (1) providing an international forum for presenting
research achievements on database theory and practice, development of advanced
DBMS technologies, and their applications as well as (2) promoting interaction and
collaboration between the database and information systems research communities
from European countries and the rest of the world.
This volume contains short research papers, workshop papers, and doctoral con-
sortium papers presented at the 21st European Conference on Advances in Databases
and Information Systems (ADBIS), held on 24–27 September, 2017, in Nicosia,
Cyprus.
The program of ADBIS 2017 included keynotes, research papers, thematic work-
shops, and a doctoral consortium. The main conference, workshops, and doctoral
consortium had their own international Program Committees. The main conference
attracted 107 paper submissions from 33 countries from all continents. After rigorous
reviewing by the Program Committee (88 reviewers from 36 countries in the Program
Committee and additionally by 26 external reviewers), the 26 papers included in the
LNCS proceedings volume were accepted as full contributions, making an acceptance
rate of 24%. In addition, 12 papers were selected as short contributions and are
included in these proceedings.
The selected short papers span a wide spectrum of topics related to the ADBIS
conference. Most of them are related to database and information systems technologies
for advanced applications. Typical applications are text databases, streaming data, and
graph processing. In addition, there are also papers covering the theory of databases.
Initially, 6 workshops were accepted to be collocated with ADBIS 2017. A work-
shop, to be run, had to receive at least 6 submissions. Two workshops did not reach this
submission level and were canceled. Finally, the following 4 workshops were run:
(1) Workshop on Novel Techniques for Integrating Big Data (BigNovelTI),
(2) Workshop on Data Science: Methodologies and Use-Cases (DaS), (3) Semantic

Web for Cultural Heritage (SW4CH), and (4) Data-Driven Approaches for Analyzing
and Managing Scholarly Data: Systems, Methods, and Applications (AMSD).
BigNovelTI received 8 submissions, out of which 4 were accepted for publication in
this volume. DaS received 20 submissions, out of which 11 were accepted. Addi-
tionally, an invited paper for DaS is included in this volume. SW4CH accepted 6
submissions out of 11. Finally, AMSD accepted 3 submissions out of 8. Thus, the
overall acceptance rate for the four workshops is 51%. A summary of the content
of these workshops is included in the chapter “New Trends in Databases and Infor-
mation Systems: Contributions from ADBIS 2017 Workshops”.
The ADBIS Doctoral Consortium (DC) 2017 was a forum where PhD students had a
chance to present their research ideas to the database research community, receive
inspiration from their peers and feedback from senior researchers, and tie cooperation
bounds. DC papers are single-authored and aim at describing the current status of thesis
research. Out of six submissions, the DC Committee selected three papers that were
presented at the DC, giving an acceptance rate of 50%. Various topics were addressed,
i.e., preference-based stream analysis, database reverse engineering, and conceptual
object-role modeling by reasoning. The DC chairs would like to thank the DC Program
Committee members for their dedicated work.
July 2017
George A. Papadopoulos
Marite Kirikova
Kjetil Nørvåg
Johann Gamper
Robert Wrembel
Jérôme Darmont
Stefano Rizzi
VI
Preface

Organization
ADBIS Program Committee
Bader Albdaiwi
Kuwait University, Kuwait
Bernd Amann
LIP6-UPMC, France
Grigoris Antoniou
University of Huddersﬁeld, UK
Ladjel Bellatreche
LIAS/ENSMA, France
Klaus Berberich
Max Planck Institute for Informatics, Germany
Maria Bielikova
Slovak University of Technology in Bratislava, Slovakia
Doulkiﬂi Boukraa
Université de Jijel, Algeria
Drazen Brdjanin
University of Banja Luka, Bosnia and Herzegovina
Stephane Bressan
National University of Singapore, Singapore
Bostjan Brumen
University of Maribor, Slovenia
Albertas Caplinskas
Vilnius University, Lithuania
Barbara Catania
DIBRIS-University of Genoa, Italy
Marek Ciglan
Institute of Informatics, Slovak Academy of Sciences,
Slovakia
Isabelle Comyn-Wattiau
ESSEC Business School, France
Alfredo Cuzzocrea
ICAR-CNR and University of Calabria, Italy
Ajantha Dahanayake
Georgia College and State University, USA
Christos Doulkeridis
University of Piraeus, Greece
Johann Eder
Alpen Adria Universität Klagenfurt, Austria
Erki Eessaar
Tallinn University of Technology, Estonia
Markus Endres
University of Augsburg, Germany
Werner Esswein
Technische Universität Dresden, Germany
Georgios Evangelidis
University of Macedonia, Greece
Flavio Ferrarotti
Software Competence Centre Hagenberg, Austria
Peter Forbrig
University of Rostock, Germany
Flavius Frasincar
Erasmus University Rotterdam, Netherlands
Jan Genci
Technical University of Kosice, Slovakia
Jānis Grabis
Riga Technical University, Latvia
Gunter Graefe
HTW Dresden, Germany
Franceso Guerra
University of Modena and Reggio Emilia, Italy
Hele-Mai Haav
Institute of Cybernetics at Tallinn University
of Technology, Estonia
Theo Härder
TU Kaiserslautern, Germany
Katja Hose
Aalborg University, Denmark
Ekaterini Ioannou
Technical University of Crete, Greece
Mirjana Ivanovic
University of Novi Sad, Serbia
Hannu Jaakkola
Tampere University of Technology, Finland
Lili Jiang
Univeristy of Umeå, Sweden

Ahto Kalja
Tallinn University of Technology, Estonia
Dimitris Karagiannis
University of Vienna, Austria
Randi Karlsen
University of Tromsø, Norway
Panagiotis Karras
Aalborg University, Denmark
Zoubida Kedad
University of Versailles, France
Marite Kirikova
Riga Technical University, Latvia
Margita Kon-Popovska
Ss. Cyril and Methodius University, FYR of Macedonia
Michal Kopecký
Charles University, Czech Republic
Michal Kratky
VSB-Technical University of Ostrava, Czech Republic
John Krogstie
NTNU, Norway
Ulf Leser
Humboldt-Universität zu Berlin, Germany
Sebastian Link
The University of Auckland, New Zealand
Audrone Lupeikiene
Vilnius University, Lithuania
Hui Ma
Victoria University of Wellington, New Zealand
Leszek Maciaszek
Wrocław University of Economics, Poland
Federica Mandreoli
DII - University of Modena, Italy
Yannis Manolopoulos
Aristotle University of Thessaloniki, Greece
Tadeusz Morzy
Poznan University of Technology, Poland
Martin Nečaský
Charles University, Czech Republic
Kjetil Nørvåg
Norwegian University of Science and Technology,
Norway
Boris Novikov
St. Petersburg University, Russia
Eirini Ntoutsi
Gottfried Wilhelm Leibniz Universität Hannover, Germany
Andreas Oberweis
Karlsruhe Institute of Technology (KIT), Germany
Andreas L. Opdahl
University of Bergen, Norway
Odysseas Papapetrou
EPFL, Switzerland
Jaroslav Pokorný
Charles University in Prague, Czech Republic
Giuseppe Polese
University of Salerno, Italy
Boris Rachev
Technical University of Varna, Bulgaria
Milos Radovanovic
University of Novi Sad, Serbia
Heri Ramampiaro
Norwegian University of Science and Technology
(NTNU), Norway
Tore Risch
University of Uppsala, Sweden
Gunter Saake
University of Magdeburg, Germany
Petr Saloun
VSB-TU Ostrava, Czech Republic
Kai-Uwe Sattler
TU Ilmenau, Germany
Ingo Schmitt
Technical University Cottbus, Germany
Tomas Skopal
Charles University in Prague, Czech Republic
Bela Stantic
Grifﬁth University, Australia
Kostas Stefanidis
University of Tampere, Finland
Panagiotis Symeonidis
Free University of Bozen-Bolzano, Italy
James Terwilliger
Microsoft Corporation, USA
Goce Trajcevski
Northwestern University, USA
Christoph Trattner
MODUL University Vienna, Austria
Raquel Trillo-Lado
Universidad de Zaragoza, Spain
Yannis Velegrakis
University of Trento, Italy
VIII
Organization

Goran Velinov
Ss. Cyril and Methodius University, FYR of Macedonia
Akrivi Vlachou
University of Piraeus, Greece
Gottfried Vossen
ERCIS Muenster, Germany
Robert Wrembel
Poznan University of Technology, Poland
Anna Yarygina
Saint-Petersburg University Russia
Weihai Yu
University of Tromsø, Norway
Arkady Zaslavsky
CSIRO, Australia
Additional Reviewers
Hosam Aboelfotoh
Dionysis Athanasopoulos
George Baryannis
Sotiris Batsakis
Panagiotis Bozanis
Loredana Caruccio
Vincenzo Deufemia
Senén González
Sven Hartmann
Zaid Hussain
Pavlos Kefalas
Julius Köpke
Vimal Kunnummel
Jens Lechtenbörger
Jevgeni Marenkov
Denis Martins
Robert Moro
Ludovit Niepel
Wilma Penzo
Horst Pichler
Benedikt Pittl
Tarmo Robal
Eliezer Souza Silva
Nikolaos Tantouris
Eleftherios Tiakas
H. Yahyaoui
Workshops
AMSD Program Committee
Andreas Behrend
University of Bonn, Germany
Christiane Engels
University of Bonn, Germany
Christoph Lange
Fraunhofer IAIS, Germany
Jens Lehmann
Fraunhofer IAIS, Germany
Rainer Manthey
University of Bonn, Germany
Sahar Vahdati
Fraunhofer IAIS, Germany
Hannes Voigt
Technische Universität Dresden, Germany
BigNovelTI Program Committee
Alberto Abelló
Universitat Politècnica de Catalunya, Spain
Andrea Calì
Birkbeck College London, UK
C. Maria Keet
University of Cape Town, South Africa
Diego Calvanese
Free University Bozen-Bolzano, Italy
Theodoros
Chondrogiannis
Free University of Bozen-Bolzano, Italy
Martin Giese
University of Oslo, Norway
Matteo Golfarelli
University of Bologna, Italy
Organization
IX

Gianluigi Greco
University of Calabria, Italy
Katja Hose
Aalborg University, Denmark
Petar Jovanovic
Universitat Politècnica de Catalunya, Spain
Roman Kontchakov
Birkbeck College London, UK
Antonella Poggi
Sapienza University of Rome, Italy
Oscar Romero
Universitat Politècnica de Catalunya, Spain
Mantas Simkus
TU Vienna, Austria
Sergio Tessaris
Free University of Bozen-Bolzano, Italy
Christian Thomsen
Aalborg University, Denmark
Stefano Rizzi
University of Bologna, Italy
Alejandro Vaisman
Instituto Tecnológico de Buenos Aires, Argentina
Stijn Vansummeren
Université Libre de Bruxelles, Belgium
Panos Vassiliadis
University of Ioannina, Greece
Robert Wrembel
Poznan University of Technology, Poland
Esteban Zimanyi
Université Libre de Bruxelles, Belgium
DaS Program Committee
Julien Aligon
University of Toulouse - IRIT, France
Antonio Attanasio
Istituto Superiore Mario Boella, Italy
Tania Cerquitelli
Politecnico di Torino, Italy
Agnese Chiatti
Pennsylvania State University, USA
Silvia Chiusano
Politecnico di Torino, Italy
Evelina Di Corso
Politecnico di Torino, Italy
Javier A.
Espinosa-Oviedo
Barcelona Supercomputing Center, Spain
Göran Falkman
University of Skövde, Sweden
Fabio Fassetti
Università della Calabria, Italy
Juozas Gordevicius
Institute of Biotechnology, Lithuania
Natalija Kozmina
University of Latvia, Latvia
Patrick Marcel
Université François Rabelais, Tours, France
Erik Perjons
Stockholm University, Sweden
Emanuele Rabosio
Politecnico di Milano, Italy
Francesco Ventura
Politecnico di Torino, Italy
Gatis Vītols
Latvia University of Agriculture, Latvia
Xin Xiao
Huawei Technologies Co., Ltd., China
José Luis Zechinelli
Martini
Universidad de las Américas Puebla, Mexico
SW4CH Program Committee
Trond Aalberg
IDI, NTNU, Norway
Carmen Brando
EHESS-CRH, Paris, France
Benjamin Cogrel
KRDB Research Centre for Knowledge and Data,
Free University of Bozen-Bolzano, Italy
Donatello Conte
LI, Université François Rabelais, Tours, France
Peter Haase
Metaphacts, Walldorf, Germany
X
Organization

Mirian Halfeld
Ferrari Alves
LIFO, University of Orléans, France
Katja Hose
Aalborg University, Denmark
Stéphane Jean
LIAS/ENSMA and University of Poitiers, France
Efstratios Kontopoulos
MKLAB, Thessaloniki, Greece
Cvetana Krstev
University of Belgrade, Serbia
Nikolaos Lagos
Xerox, Grenoble, France
Béatrice Markhoff
LI, Université François Rabelais, Tours, France
Denis Maurel
LI, Université François Rabelais, Tours, France
Carlo Meghini
CNR/ISTI, Pisa, Italy
Isabelle Mirbel
WIMMMICS, University of Nice Sophia Antipolis, France
Alessandro Mosca
SIRIS Academic, S.L., Barcelona, Spain
Dmitry Muromtsev
ITMO University, Russia
Cheikh Niang
AUF, Paris, France
Xavier Rodier
Laboratoire Archéologie et Territoires,
Université François Rabelais, Tours, France
Maria Theodoridou
FORTH ICS, Heraklion, Crete, Greece
Genoveva Vargas-Solar
University of Grenoble, France
Dusko Vitas
University of Belgrade, Serbia
Doctoral Consortium Program Committee
Varunya Attasena
Kasetsart University, Kamphaeng Saen Campus, Thailand
Maria Bielikova
Slovak University of Technology in Bratislava, Slovakia
Doulkiﬂi Boukraa
Université de Jijel, Algeria
Enrico Gallinucci
Università di Bologna, Italy
Adrien Guille
Université de Lyon, France
Sebastian Link
The University of Auckland, New Zealand
Raquel Trillo-Lado
Universidad de Zaragoza, Spain
Organization
XI

Contents
ADBIS 2017 – Short Papers
Distributing N-Gram Graphs for Classification . . . . . . . . . . . . . . . . . . . . . .
3
Ioannis Kontopoulos, George Giannakopoulos, and Iraklis Varlamis
Assessing the Quality of Spatio-Textual Datasets in the Absence
of Ground Truth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
Mouzhi Ge and Theodoros Chondrogiannis
T2K2: The Twitter Top-K Keywords Benchmark. . . . . . . . . . . . . . . . . . . . .
21
Ciprian-Octavian Truică and Jérôme Darmont
Outlier Detection in Data Streams Using OLAP Cubes . . . . . . . . . . . . . . . .
29
Felix Heine
Balancing Performance and Energy for Lightweight Data
Compression Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Annett Ungethüm, Patrick Damme, Johannes Pietrzyk,
Alexander Krause, Dirk Habich, and Wolfgang Lehner
Asynchronous Graph Pattern Matching on Multiprocessor Systems . . . . . . . .
45
Alexander Krause, Annett Ungethüm, Thomas Kissinger,
Dirk Habich, and Wolfgang Lehner
Predicting Access to Persistent Objects Through Static Code Analysis . . . . . .
54
Rizkallah Touma, Anna Queralt, Toni Cortes, and María S. Pérez
Query-Driven Knowledge-Sharing for Data Integration and Collaborative
Data Science . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
Andreas M. Wahl, Gregor Endler, Peter K. Schwab, Sebastian Herbst,
and Richard Lenz
A Declarative Approach to Analyzing Schema Objects
and Functional Dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
Christiane Engels, Andreas Behrend, and Stefan Brass
Dear Mobile Agent, Could You Please Find Me a Parking Space? . . . . . . . .
82
Oscar Urra and Sergio Ilarri
P2P Deductive Databases: Well Founded Semantics
and Distributed Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
L. Caroprese and E. Zumpano

Is Distributed Database Evaluation Cloud-Ready? . . . . . . . . . . . . . . . . . . . .
100
Daniel Seybold and Jörg Domaschka
ADBIS 2017 – Workshops
New Trends in Databases and Information Systems: Contributions
from ADBIS 2017 Workshops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
Andreas Behrend, Diego Calvanese, Tania Cerquitelli, Silvia Chiusano,
Christiane Engels, Stéphane Jean, Natalija Kozmina, Béatrice Markhoff,
Oscar Romero, and Sahar Vahdati
The 1st Workshop on Data-Driven Approaches for Analyzing
and Managing Scholarly Data (AMSD 2017)
Publication Data Integration as a Tool for Excellence-Based Research
Analysis at the University of Latvia . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
Laila Niedrite, Darja Solodovnikova, and Aivars Niedritis
Evaluating Reference String Extraction Using Line-Based Conditional
Random Fields: A Case Study with German Language Publications . . . . . . .
137
Martin Körner, Behnam Ghavimi, Philipp Mayr, Heinrich Hartmann,
and Steffen Staab
CEUR Make GUI - A Usable Web Frontend Supporting the Workflow
of Publishing Proceedings of Scientific Workshops . . . . . . . . . . . . . . . . . . .
146
Muhammad Rohan Ali Asmat and Christoph Lange
The 1st Workshop on Novel Techniques for Integrating Big Data
(BigNovelTI 2017)
A Framework for Temporal Ontology-Based Data Access: A Proposal . . . . .
161
Sebastian Brandt, Elem Güzel Kalaycı, Vladislav Ryzhikov,
Guohui Xiao, and Michael Zakharyaschev
Towards Semantic Assessment of Summarizability in Self-service
Business Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
Luis-Daniel Ibáñez, Jose-Norberto Mazón, and Elena Simperl
Theta Architecture: Preserving the Quality of Analytics
in Data-Driven Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
Vasileios Theodorou, Ilias Gerostathopoulos, Sasan Amini,
Riccardo Scandariato, Christian Prehofer, and Miroslaw Staron
Spatio-Temporal Evolution of Scientific Knowledge . . . . . . . . . . . . . . . . . .
199
Goce Trajcevski, Xu Teng, and Shailav Taneja
XIV
Contents

The 1st International Workshop on Data Science: Methodologies
and Use-Cases (DaS 2017)
Parallel Subspace Clustering Using Multi-core
and Many-core Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
Amitava Datta, Amardeep Kaur, Tobias Lauer, and Sami Chabbouh
Discovering High-Utility Itemsets at Multiple Abstraction Levels . . . . . . . . .
224
Luca Cagliero, Silvia Chiusano, Paolo Garza, and Giuseppe Ricupero
Automatic Organization of Semantically Related Tags
Using Topic Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
Iman Saleh and Neamat El-Tazi
Fuzzy Recommendations in Marketing Campaigns . . . . . . . . . . . . . . . . . . .
246
S. Podapati, L. Lundberg, L. Skold, O. Rosander, and J. Sidorova
Efficient Data Management for Putting Forward Data Centric Sciences . . . . .
257
Genoveva Vargas-Solar
Towards a Multi-way Similarity Join Operator . . . . . . . . . . . . . . . . . . . . . .
267
Mikhail Galkin, Maria-Esther Vidal, and Sören Auer
Workload-Independent Data-Driven Vertical Partitioning . . . . . . . . . . . . . . .
275
Nikita Bobrov, George Chernishev, and Boris Novikov
Can SQ and EQ Values and Their Difference Indicate Programming
Aptitude to Reduce Dropout Rate? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
Juris Borzovs, Natalija Kozmina, Laila Niedrite, Darja Solodovnikova,
Uldis Straujums, Janis Zuters, and Atis Klavins
Fusion of Clinical Data: A Case Study to Predict the Type of Treatment
of Bone Fractures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
Anam Haq and Szymon Wilk
Data Science Techniques for Law and Justice: Current State
of Research and Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
Alexandre Quemy
Using Data Analytics for Continuous Improvement of CRM Processes:
Case of Financial Institution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
Pāvels Gončarovs and Jānis Grabis
Towards a Data Science Environment for Modeling Business Ecosystems:
The Connected Mobility Case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
Adrian Hernandez-Mendez, Anne Faber, and Florian Matthes
Contents
XV

The 2nd International Workshop on Semantic Web
for Cultural Heritage (SW4CH 2017)
Introducing Narratives in Europeana: Preliminary Steps . . . . . . . . . . . . . . . .
333
Carlo Meghini, Valentina Bartalesi, Daniele Metilli,
and Filippo Benedetti
Evaluation of Semantic Web Ontologies for Modelling Art Collections . . . . .
343
Danfeng Liu, Antonis Bikakis, and Andreas Vlachidis
The CrossCult Knowledge Base: A Co-inhabitant of Cultural Heritage
Ontology and Vocabulary Classification. . . . . . . . . . . . . . . . . . . . . . . . . . .
353
Andreas Vlachidis, Antonis Bikakis, Daphne Kyriaki-Manessi,
Ioannis Triantafyllou, and Angeliki Antoniou
The Port History Ontology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
363
Bruno Rohou, Sylvain Laube, and Serge Garlatti
A WordNet Ontology in Improving Searches of Digital
Dialect Dictionary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
373
Miljana Mladenović, Ranka Stanković, and Cvetana Krstev
When It Comes to Querying Semantic Cultural Heritage Data . . . . . . . . . . .
384
Béatrice Markhoff, Thanh Binh Nguyen, and Cheikh Niang
ADBIS Doctoral Consortium
Preference-Based Stream Analysis for Efficient Decision-Support Systems . . .
397
Lena Rudenko
Formalization of Database Reverse Engineering . . . . . . . . . . . . . . . . . . . . .
410
Nonyelum Ndefo
Supporting Conceptual Modelling in ORM by Reasoning . . . . . . . . . . . . . .
422
Francesco Sportelli
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433
XVI
Contents

ADBIS 2017 – Short Papers

Distributing N-Gram Graphs for Classiﬁcation
Ioannis Kontopoulos1(B), George Giannakopoulos1, and Iraklis Varlamis2
1 Institute of Informatics and Telecommunications, N.C.S.R. “Demokritos”,
Agia Paraskevi, Greece
{ikon,ggianna}@iit.demokritos.gr
2 Department of Informatics and Telematics, Harokopio University of Athens,
Kallithea, Greece
varlamis@hua.gr
Abstract. N-gram models have been an established choice for lan-
guage modeling in machine translation, summarization and other tasks.
Recently n-gram graphs managed to capture signiﬁcant language charac-
teristics that go beyond mere vocabulary and grammar, for tasks such as
text classiﬁcation. This work proposes an eﬃcient distributed implemen-
tation of the n-gram graph framework on Apache Spark, named ARGOT.
The implementation performance is evaluated on a demanding text clas-
siﬁcation task, where the n-gram graphs are used for extracting features
for a supervised classiﬁer. A provided experimental study shows the scal-
ability of the proposed implementation to large text corpora and its abil-
ity to take advantage of a varying number of processing cores.
Keywords: Distributed processing · N-gram graphs · Text classiﬁcation
1
Introduction
Text classiﬁcation is a supervised machine learning technique for identifying pre-
deﬁned categories that are related to a speciﬁc document. Several distributed
processing paradigms have been applied to handle classiﬁcation in the increas-
ing volume of generated texts in diﬀerent domains (e.g. [5]). However, promising
methods, such as the n-gram graph (nGG) framework [8–10] still have scalabil-
ity limited to a single machine1. This paper proposes the ARGOT framework, a
distributed implementation of nGGs, tested on a text classiﬁcation task. We re-
engineer the time-consuming processes of feature extraction and graph merging
in order to improve scalability in large corpora, while retaining the framework’s
state-of-the-art performance. Our experimental evaluation also studies the scal-
ability of the solution, with respect to various parameters.
In the following, we summarize related work (Sect. 2), explain the nGG frame-
work (Sect. 3) and ARGOT (Sect. 3). We then experimentally evaluate our app-
roach (Sect. 4) and conclude our work (Sect. 5), summarizing the next steps.
1 JInsect toolkit, found at https://github.com/ggianna/JInsect.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 3–11, 2017.
DOI: 10.1007/978-3-319-67162-8 1

4
I. Kontopoulos et al.
2
Related Work
Distributed text classiﬁcation in Big Data applications oftentimes employ frame-
works such as MapReduce [12,15] and Spark [13] to ensure scalability. To ade-
quately manage big data in associative classiﬁers (because of time complexity
and memory constraints) a map-reduce solution has been presented in [2]. In [3]
the eﬃciency of Support Vector Machine and Naive Bayes classiﬁers is tested
using a Word2V ec model and algorithm on Apache Spark.
Apart from the distributed approach, graph-based techniques have been
developed over the years for accurate text classiﬁcation [4]. Authors in [1] have
employed the nGG framework to extract rich features, usable by a text classiﬁer.
A graph-based approach has been introduced in [11] for encoding the relation-
ships between the diﬀerent terms in the text.
The primary aim – and unique positioning – of this work is to provide a par-
allel and distributed implementation that boosts the scalability of the original
nGG framework. Thus, the focus is not on the classiﬁcation task itself, but on the
properties that aﬀect the scalability of the classiﬁcation solution. These prop-
erties drive the redesign of existing operators (graph creation, graph merging,
graph similarity calculation, etc.) in the distributed setup.
The distributed implementation of nGGs is meant to empower distributed
text analysis components, such as the Big Data Europe2 project “event detec-
tion” workﬂow, where nGGs are used to identify real-world events from textual
streams.
3
Distributed N-Gram Graphs for Classiﬁcation
Character (or word) nGGs represent texts as neighborhoods of n-grams. Given a
string (e.g. “Hello!”), a value n for the n-grams and a window size w, an nGG: (a)
breaks the string into overlapping n-grams (e.g. for 3-grams: “Hel”, “ell”, “llo”,
...), creating one vertex per unique n-gram; (b) connects n-grams found within a
(character) distance of at most w of each other in the original string with edges
(e.g. for w = 2: “Hel-ell”, “Hel-llo”, ...); maps to each edge the number of times
that its connected n-grams were found to be neighbors (e.g. “Hel-llo” →1). The
extracted graph for the “Hello!” input text is shown in Fig. 1.
Given the representation, ARGOT implements and uses four diﬀerent simi-
larity operators for nGG comparison: Size Similarity (SS), expressing the ratio
of graph sizes; Containment Similarity (CS) expressing the proportion of com-
mon edges between graphs, regardless of edge weights; Value Similarity (V S)
which also makes use of the edge weights; Normalized Value Similarity (NV S)
which is a size-ratio-independent version of V S. In our work we also apply the
union (or merge) operator over m graphs, which – given m graphs – creates a
2 This paper is supported by the project “Integrating Big Data, Software and Com-
munities for Addressing Europe’s Societal Challenges – BigDataEurope”, which has
received funding from the European Union’s Horizon 2020 research and innovation
programme under grant agreement No 644564. https://www.big-data-europe.eu/.

Distributing N-Gram Graphs for Classiﬁcation
5
Hel
ell
llo
lo!
1.0
1.0
1.0
1.0
Fig. 1. The N-gram graph representation (n = 3, w = 2) of the “Hello!” string.
new graph GU, which (a) contains as edges the union of the edges of the input
graphs; (b) assigns a weight we to every one of its edges, where we is the average
of the weights of all the edges e across all input graphs where e appears. For
more details on the operators, please consult [8].
The ﬁrst step of the classiﬁcation process is to train the classiﬁer with the
training instances. In the case of nGGs (see Fig. 2), separate class graphs are
composed by merging – using the union operator – the training instances (nGG
representations of documents) of each class. In the classiﬁcation step, each unla-
beled instance (i.e. document) is ﬁrst represented as an nGG. It is then compared
with the N class graphs and the CS, V S and NV S similarities form the 3 × N
feature vector representation of the instance.
abcdef
Text Document
abc
bcd
cde
def
Document Graph
Similarity 
CalculaƟon
Topic 1 Graph
Topic 2 Graph
Topic N Graph
(0.2,0.8,1.0,…,0.4,0.2,0.5,…,0.3,0.5,0.6,…)
VS
CS
NVS
Fig. 2. Extracting the feature vector using the n-gram graph framework.
Despite its performance [10], the nGG classiﬁcation requires a signiﬁcant
amount of time for training (i.e. for constructing the class graphs) and for creat-
ing the feature vectors (i.e. computing graph similarities). The time complexity
of these processes depends both on the number of classes and the amount and
size of training documents and sets a scalability challenge. In order to address
this challenge, we propose a distributed implementation of the classiﬁcation algo-
rithm that uses Apache Spark, a mature, fast and general engine for large-scale
data processing. In this section, the distributed algorithms for the construction
of the class graphs and the computation of graph similarities are explained. The
code of these implementations and the framework developed, which is called
ARGOT, is publicly available and can be found on a public repository3.
3 https://github.com/ioannis-kon/ARGOT.

6
I. Kontopoulos et al.
Distributed class graph creation: As illustrated in Fig. 3a, the process begins
with the transformation of the training text documents to the respective nGGs.
The documents, which can be located in a Hadoop (HDFS), Network (NFS) or
any other distributed ﬁle system, are split into k partitions and each partition is
processed independently. Each edge of the nGG consists of the connected vertex
identiﬁers (ids) and the edge weight. Edges are repartitioned based on the hash
of their corresponding vertex ids. As a result, edges from diﬀerent documents
that map to the same vertex pair are located in the same partition, increasing
merging and calculation eﬃciency. Based on the above, a single nGG is essentially
distributed across the processing nodes.
Document 1
.
.
Document n/2
Document n/2+1
.
.
Document n
Graph 1
.
.
Graph n/2
Graph n/2+1
.
.
Graph n
hdfs, nfs or any other shared network ﬁle system
Convert to graphs
Merged edge 1
.
.
Merged edge m/2
Merged edge m/2+1
.
.
Merged edge m
ParƟƟon 1                              ParƟƟon 2
shuﬄe
(a) Merging of multiple
documents into one class-
representative graph.
Edge 1
.
.
Edge n/2
Edge n/2+1
.
.
Edge n
Edge 1
.
.
Edge n/2
Edge n/2+1
.
.
Edge n
Edge 1
.
.
Edge m
Edge 1
.
.
Edge k (k <= m)
small graph
smaller graph
ParƟƟon 1
ParƟƟon 2
(b) Similarity computation between a small and a distributed
graph.
Fig. 3. Distributed implementation of algorithms.
Distributed graph similarity computation: The second step, as illustrated
in Fig. 3b, is the extraction of similarity features, which is based on the com-
putation of graph similarities between the class graphs and each unclassiﬁed
document. The class graph can contain millions of edges, delaying the compari-
son of each unclassiﬁed instance with the class graphs. To solve this problem, a
graph similarity algorithm similar to the semi-join technique (used in distributed
databases) has been implemented. At each comparison, the smaller document
graph is broadcasted to every partition, and the partial overlap with the portion
of the class graph in each partition is computed. This implementation reduces
the communication overhead between nodes. Since the similarity measures are
based on the overlapping edges only, we ﬁlter the partitions and keep the com-
mon edges (which are at most the edges of the smallest graph). These edges are
collected by the master node, which then computes similarities locally and fast.

Distributing N-Gram Graphs for Classiﬁcation
7
4
Experimental Evaluation
The distributed implementation of the document classiﬁcation algorithm was
evaluated for its performance and scalability on the large-scale, real-world
Reuters RCV2 dataset4, using two diﬀerent hardware infrastructures. The exper-
iments aim to study how several parameters (e.g. the total number of documents
or classes) aﬀect the algorithm performance. The processing steps of the clas-
siﬁcation task, as described in [10], were repeated: (i) on a single node with
24 cores5 and (ii) on a cluster comprising 6 commodity machines, each with 4
cores6. For the implementation of ARGOT, Scala 2.11.7 and Apache Spark 2.0.1
were used.
Reuters RCV2 holds more than 480, 000 news articles in 13 diﬀerent lan-
guages, categorized along a class hierarchy of 104 overlapping topics and has
been widely used [6,14]. In the experiments we used the top four, non overlap-
ping, corpus categories, comprising articles in four languages (see Table 1).
Table 1. Dataset statistics.
Characteristic
Value
Characteristic
Value
Classes
4
# of edges of class graph 1
5, 772, 038
Documents
172, 115
# of edges of class graph 2
10, 134, 473
Total characters (×108) 2.07
# of edges of class graph 3
3, 708, 371
Characters/Document
1, 205.55 # of edges of class graph 4
8, 251, 410
Total Tokens (×108)
30.92
Total # of graph comparisons 688, 460
Tokens/Document
179.66
Total size of dataset
201.69 MB
Av. token length
5.68
Average size of each instance
1.2 KB
Classiﬁcation: In order to evaluate the performance of ARGOT implementa-
tions, we performed a 90%−10% training-test split of the dataset (as in [10]).
The (four) class graphs are created by merging a randomly selected subset (90%)
of the training documents. Then, we follow the process described in Sect. 3 to
represent all instances and train a Naive Bayes classiﬁer. The classiﬁcation pre-
cision on the test set was 95.1%, whereas the maximum precision reported in [7]
was 94% in only a subset of the same data set. This side-result highlights the
value and language-independence of the nGG approach.
Scalability: In order to test the scalability of the implemented algorithms, we
conducted experiments on the complete dataset described in Table 1. ARGOT
4 http://trec.nist.gov/data/reuters/reuters.html.
5 Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50 GHz) and 96 GB of RAM, running Debian 64-bit with
Linux Kernel 3.16.0-4-amd64 and Java OpenJDK 1.8.
6 Intel(R) Core(TM) i5-3330S CPU @ 2.70 GHz) and 8 GB of RAM, totaling in 24 cores and 48
GB of RAM, running OS X 10.10 (14A389) with Kernel Darwin 14.0.0 and Java OpenJDK 1.8,
connected with 100-Mbit ethernet links.

8
I. Kontopoulos et al.
scalability was tested using an increasing number of partitions (from 8 to 48) on
a single node machine and on a cluster of commodity machines. We repeat the
experiments 10 times and report the average times. Figure 4a shows the average
merging time per class, where the distributed merging function scales better for
bigger classes (i.e., class 2). Figure 4b shows the average time for extracting fea-
tures from the instances. It shows great scalability capacity since the algorithm
reduced the extraction time by almost 50% from 8 to 48 partitions. The perfor-
mance improvement is more obvious in Fig. 4c, which shows the average time for
extracting similarities per instance, which was reduced from 1.04 s to 0.58 s (i.e.
43% improvement), from 8 to 48 partitions.
Using the same dataset and partitions setup, we repeated the experiment on
a distributed cluster7, in order to test how data broadcasting and repartitioning
aﬀect performance (cf. Fig. 5a, b and c). We see that performance drops when
increasing the number of partitions, since the communication overhead counters
the distributed processing gain. However, in larger class graphs and datasets the
system is expected to perform better with more partitions.
10
20
30
40
Number of partitions
1
2
3
4
5
6
Avg Merging Time (minutes)
class 1
class 2
class 3
class 4
(a) Graph merging time.
10
20
30
40
Number of partitions
10
20
30
40
50
60
70
Avg Extraction Time (hours)
Feature extraction for training
(b) Feature extraction time.
10
20
30
40
Number of partitions
0
0.5
1
1.5
2
Avg Comparison Time (sec)
Comparison time per instance
(c) Graph comparison time.
Fig. 4. ARGOT performance in a large-scale experiment (single node).
10
20
30
40
Number of partitions
1
2
3
4
5
6
Avg Merging Time (minutes)
class 1
class 2
class 3
class 4
(a) Graph merging time.
10
20
30
40
Number of partitions
10
20
30
40
50
60
70
Avg Extraction Time (hours)
Feature extraction for training
(b) Feature extraction time.
10
20
30
40
Number of partitions
0
0.5
1
1.5
2
Avg Comparison Time (sec)
Comparison time per instance
(c) Graph comparison time.
Fig. 5. ARGOT performance in a large-scale experiment (cluster mode).
Dataset traits: We now examine how the total number of instances and classes
aﬀects performance. Two subsets of the large dataset have been created, each
7 Cluster nodes are connected with 100-Mbit Ethernet links.

Distributing N-Gram Graphs for Classiﬁcation
9
5
10
15
20
Number of partitions
0
0.2
0.4
0.6
0.8
1
Avg Merging Time (min)
class 1
class 2
(a) Merging time - 2 topics.
5
10
15
20
Number of partitions
0
0.2
0.4
0.6
0.8
1
Avg Merging Time (min)
class 1
class 2
class 3
class 4
(b) Merging time - 4 topics.
5
10
15
20
Number of partitions
0.5
1
1.5
2
2.5
Avg Extraction Time (hours)
2500 instances per class
5000 instances per class
(c) Feature extraction time.
Fig. 6. Performance results of the two subset experiments.
comprising 10, 000 documents in total, from two and four classes respectively
(9, 000 training instances and 1, 000 testing instances in each subset).
We run the same experiments (10 times per setting and averaging results),
but this time we used 2 to 24 partitions on the single node setup. Figure 6a and
b show the average merging time per class per experiment, illustrating that the
merging time depends on the number of the documents in a topic. Figure 6c
depicts the average feature extraction time per experiment. While having less
documents per topic and the same number of total training instances, the extrac-
tion time took longer. The diﬀerence is the number of graph comparisons, which
are greater in the last case. From this we can infer that the feature extraction
time depends on the number of topics. From the experiments conducted we can
safely say that the more documents we have per class and the larger graphs we
have, the better the algorithms scale.
Distributed vs. multi-threaded: We compared our method to the current,
multi-threaded implementation of nGG algorithms, JInsect. Figures 7a, b show
the average graph merging times in the aforementioned subsets, correspondingly.
Figure 7c compares the ARGOT average feature extraction time to JINSECT.
ARGOT is faster in graph merging but slower in feature extraction. However,
JInsect failed on the full dataset, due to lack of memory scalability, which was
not a problem for ARGOT. Finally, ARGOT yields the same results as JInsect
in terms of classiﬁcation performance, as expected.
5
10
15
20
Number of partitions
15
20
25
30
35
Avg Merging Time (min)
class 1
class 2
(a) Merging time (2 topics).
5
10
15
20
Number of partitions
10
15
20
Avg Merging Time (min)
class 1
class 2
class 3
class 4
(b) Merging time (4 topics).
5
10
15
20
Number of partitions
0
1
2
3
Avg Extraction Time (hours)
5000 instances (JInsect)
2500 instances (JInsect)
5000 instances (ARGOT)
2500 instances (ARGOT)
(c) Feature extraction time.
Fig. 7. JInsect (i.e. multi-threaded, single-machine) performance evaluation.

10
I. Kontopoulos et al.
5
Conclusion
In this work we presented ARGOT, a distributed implementation of the nGG
algorithms in text classiﬁcation. We illustrated the details of the distributed
implementation and demonstrated the scalability of ARGOT on a real-world,
large-scale, multilingual dataset. We showed how the number of topics and the
number of instances can aﬀect the performance of the algorithms. We also pre-
sented the eﬀect of broadcasting the data to the execution time of the exper-
iment, when using a cluster of computers. We showed that if the number of
instances per topic is large, ARGOT can beneﬁt from the larger number of CPUs
in a cluster/machine. We also compared ARGOT to the established implemen-
tation of nGGs and showed that ARGOT scales better, also exceeding inherent
single-machine limitations. As a side-eﬀect, our method demonstrated state-of-
the-art performance on an established classiﬁcation dataset for a single-label,
multi-class, multilingual setting, highlighting the value of the approach.
References
1. Angelova, R., Weikum, G.: Graph-based text classiﬁcation: learn from your neigh-
bors. In: 29th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, pp. 485–492. ACM, New York (2006)
2. Bechini, A., Marcelloni, F., Segatori, A.: A mapreduce solution for associative
classiﬁcation of big data. Inf. Sci. 332(C), 33–55 (2016)
3. Choi, M., Jin, R., Chung, T.-S.: Document classiﬁcation using Word2Vec and Chi-
square on apache spark. In: Park, J., Pan, Y., Yi, G., Loia, V. (eds.) CSA 2016,
CUTE 2016, UCAWSN 2016. LNEE, vol. 421, pp. 867–872. Springer, Singapore
(2017). doi:10.1007/978-981-10-3023-9 134
4. Das, N., Ghosh, S., Goncalves, T., Quaresma, P.: Comparison of diﬀerent graph
distance metrics for semantic text based classiﬁcation. Polibits 51–58 (2014)
5. Fei, X., Li, X., Shen, C.: Parallelized text classiﬁcation algorithm for processing
large scale TCM clinical data with mapreduce. In: 2015 IEEE International Con-
ference on Information and Automation, pp. 1983–1986, August 2015
6. Ferreira, D.C., Martins, A.F.T., Almeida, M.S.C.: Jointly learning to embed and
predict with multiple languages. In: ACL (2016)
7. Fortuna, B., Shawe-Taylor, J.: The use of machine translation tools for cross-lingual
text mining. In: ICML Workshop on Learning with Multiple Views (2005)
8. Giannakopoulos, G.: Automatic summarization from multiple documents. Ph.D.
thesis (2009)
9. Giannakopoulos, G., Karkaletsis, V., Vouros, G., Stamatopoulos, P.: Summa-
rization system evaluation revisited: N-gram graphs. ACM Trans. Speech Lang.
Process. 5(3), 5:1–5:39 (2008)
10. Giannakopoulos, G., Mavridi, P., Paliouras, G., Papadakis, G., Tserpes, K.: Rep-
resentation models for text classiﬁcation: a comparative analysis over three web
document types. In: 2nd International Conference on Web Intelligence, Mining and
Semantics, pp. 13:1–13:12. ACM, New York (2012)
11. Malliaros, F.D., Skianis, K.: Graph-based term weighting for text categorization.
In: 2015 IEEE/ACM International Conference on Advances in Social Networks
Analysis and Mining 2015, pp. 1473–1479. ACM, New York (2015)

Distributing N-Gram Graphs for Classiﬁcation
11
12. Santoso, J., Yuniarno, E.M., Hariadi, M.: Large scale text classiﬁcation using map
reduce and naive bayes algorithm for domain speciﬁed ontology building. In: 7th
International Conference on Intelligent Human-Machine Systems and Cybernetics,
vol. 1, pp. 428–432 (2015)
13. Semberecki, P., Maciejewski, H.: Distributed Classiﬁcation of Text Documents on
Apache Spark Platform, pp. 621–630. Springer International Publishing, Cham
(2016)
14. Song, Y., Upadhyay, S., Peng, H., Roth, D.: Cross-lingual dataless classiﬁcation
for many languages. In: Twenty-Fifth International Joint Conference on Artiﬁcial
Intelligence, pp. 2901–2907. AAAI Press (2016)
15. Zhou, L., Yu, Z.: Acceleration of MapReduce Framework on a Multicore Processor,
pp. 175–190. Springer International Publishing, Cham (2017)

Assessing the Quality of Spatio-Textual Datasets
in the Absence of Ground Truth
Mouzhi Ge1(B) and Theodoros Chondrogiannis2(B)
1 Masaryk University, Brno, Czech Republic
mouzhi.ge@muni.cz
2 Free University of Bozen-Bolzano, South Tyrol, Italy
tchond@inf.unibz.it
Abstract. The increasing availability of enriched geospatial data has
opened up a new domain and enables the development of more sophis-
ticated location-based services and applications. However, this develop-
ment has also given rise to various data quality problems as it is very
hard to verify the data for all real-world entities contained in a dataset. In
this paper, we propose ARCI, a relative quality indicator which exploits
the vast availability of spatio-textual datasets, to indicate how conﬁdent
a user can be in the correctness of a given dataset. ARCI operates in
the absence of ground truth and aims at computing the relative quality
of an input dataset by cross-referencing its entries among various simi-
lar datasets. We also present an algorithm for computing ARCI and we
evaluate its performance in a preliminary experimental evaluation using
real-world datasets.
Keywords: Spatio-textual data · Data quality · Relative quality
1
Introduction
The current trends in technology, such as smartphones and sensor networks,
along with the proliferation of location-based social networks, such as Foursquare
and Flickr, have motivated the development of new applications and services
which employ spatio-textual datasets, i.e., collections of spatial objects which
carry both spatial and textual information. In addition, spatio-textual queries,
which combine location-based retrieval with textual information that describes
the spatial objects, have recently attracted much attention [3,15].
The fact that real world entities are constantly changing though, e.g., restau-
rants are closing down, museums are being moved etc., makes the maintenance
of spatio-textual datasets very hard. Consequently, it is quite common for spatio-
textual datasets to contain inaccurate information and/or incomplete entries [7].
Furthermore, data across diﬀerent datasets is not always consistent, i.e., entries
of diﬀerent datasets referring to the same real-world entity may provide conﬂict-
ing information. Due to such data quality issues, a number of geospatial appli-
cations and initiatives have been delayed or even canceled, citing poor-quality
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 12–20, 2017.
DOI: 10.1007/978-3-319-67162-8 2

Assessing the Quality of Spatio-Textual Datasets
13
of the available data as the main reason. Although identifying such data quality
problems manually is unrealistic, it is important to provide at least an indication
for the quality of spatio-textual datasets. Such indicators should operate in the
absence of ground truth, i.e., when there is no available veriﬁed data.
In this paper, we propose ARCI, a novel approach to indicate the relative
quality of a spatio-textual dataset by cross-referencing its entries with other
spatio-textual datasets. The ARCI indicator intends to highlight the trustful-
ness of the dataset and conﬁrm the conﬁdence to use the data. Furthermore,
quantifying the ARCI indicator allows us to visualize the data quality, for exam-
ple using a spectrum bar in this paper. While most existing quality assessment
methodologies [6,7] rely on (designed) ground truth data, our approach works
in the absence of ground truth. Instead of focusing on a speciﬁc data quality
dimension such as accuracy or completeness, ARCI uses data alignment as a
quality indicator for the input dataset. We also present ARCI-GP, an algorithm
for computing ARCI and we conduct a preliminary experimental evaluation.
The remainder of the paper is organized as follows. Section 2 overviews the
related work. Section 3 presents some preliminaries on spatio-textual datasets,
spatio-textual similarity search and spatio-textual similarity joins. Our approach
for evaluating the relative quality of spatio-textual data is introduced in Sect. 4.
Section 5 reports on the results of a preliminary experimental evaluation. Finally,
Sect. 6 concludes the paper and outlines future work.
2
Related Work
A data value can be considered dirty if it does not conform to the reference
data [5]. However, in reality it is usually unrealistic to deﬁne a set of comprehen-
sive and correct reference data as the ground truth. Thus, relative data quality
has been recently emerging as a research trend. Cao et al. [4] have deﬁned the
relative accuracy of data entries as the closeness of the value of diﬀerent entries,
and further speciﬁed that the challenge in relative accuracy is creating accuracy
rules and inferring the true value. Their work is still built upon the availability
of master data though.
Regarding the absence of ground-truth data, Galarus and Angryk [7] have
further elaborated this challenge for cases where it is not feasible to obtain
the ground-truth data at all times. In order to leverage this issue, they have
developed a representative artiﬁcial dataset, which can be used as an interpolator
to estimate unknown values. The focus of their approach is to mitigate the
erroneous data that can possibly appear during the data processing. In their
context, it is possible to construct the representative dataset from historical
data, which as facts will not be changed.
Another way to detect data quality problems without using reference data is
to deﬁne data quality rules. In order to discover such rules, diﬀerent approaches
have been proposed such as conditional functional dependencies [5], association
rule learning [1] or deﬁning quality processing requirements from users [11].
These rules are usually determined or inferred in certain context. In this work,

14
M. Ge and T. Chondrogiannis
the experiment has been conducted context-independently and the datasets used
contain a limited set of attributes. Therefore, it is not suitable for this work to
use rules to detect data quality problems.
Furthermore, data quality problems are usually related to certain data qual-
ity dimensions. The research of data quality dimensions can be traced back to
the 90’s. Wang and Strong [16] used an exploratory factor analysis to derive ﬁf-
teen data quality dimensions, which are widely accepted in the subsequent data
quality research. Afterwards, diﬀerent data quality dimensions have been fur-
ther studied and reﬁned such as consistency, accuracy [6] or completeness [13].
This paper does not contribute to a speciﬁc data quality dimension, but instead
introduces an overall quality indicator to show the trust level of the data.
3
Spatio-Textual Similarity Search
Building upon the deﬁnition of spatio-textual objects in [3], a spatio-textual
dataset D
=
{x1, . . . , xk} is a collection of spatio-textual objects xi
=
⟨x.id, x.txt, x.loc, x.attr⟩where x.id is the id of the object within the dataset,
x.txt is a textual attribute of x (i.e. the name), x.loc is the location of x in the
two dimensional geographical space and x.attr is the attribute which is to be
assessed.
To determine whether two spatio-textual objects refer to the same real-world
entity, one can aim either for exact or for approximate matching. Given two
spatio-textual objects x1 and x2, for exact matching x1.txt must be equal to
x2.txt, and x1.loc must also be equal to x2.loc. Clearly, such an approach is
not practical as tiny diﬀerences, especially in the spatial attribute (i.e. the loca-
tions of the objects are few centimeters apart) will prevent a matching. For
approximate matching, we employ a similarity function sim(x1.txt, x2.txt) for
the textual attribute and a distance function dist(x1.loc, x2.loc) for the spatial
attribute. Hence we declare that x1 and x2 refer to the same real-world entity if
sim(x1.txt, x2.txt) > θ and dist(x1.loc, x2.loc) ≤ϵ, where θ is a textual similar-
ity threshold, and ϵ is the maximum distance that two locations can be apart.
Since attributes x.txt and x.loc are used to identify one spatio-textual object
from another, throughout this paper, we refer to these two attributes as the
identiﬁers of x.
To perform the spatio-textual similarity search, ﬁrst we need to decide on
a similarity metric for textual attributes, and a distance metric for spatial
attributes. For textual similarity in particular, a variety of metrics has been
proposed [14]. In this paper, we employ the Levenshtein [10] distance to com-
pute the spatio-textual similarity and we also run experiments using the N-Gram
textual similarity [8]. However, choosing the most suitable similarity metric is out
of the scope of this paper. Regarding the spatial distance between two objects,
it is given by their geodetic distance.

Assessing the Quality of Spatio-Textual Datasets
15
3.1
Spatio-Textual Similarity Joins
An eﬃcient approach to achieve approximate matching between objects from
diﬀerent datasets is to perform a Spatio-textual similarity join(STJoin) [2,3,12].
STJoin queries aim at identifying similar spatio-textual objects across diﬀerent
datasets that are close to each other and have similar textual descriptions. More
speciﬁcally, given two datasets Di and Dj, a textual similarity threshold ϵ and
a spatial distance threshold θ, a spatio-textual similarity join query retrieves all
pairs of objects {xi, xj} where xi ∈Di and xj ∈Dj, the spatial distance of xi and
xj is dist(xi.loc, xj.loc) ≤ϵ and the textual similarity is sim(xi.txt, xj.txt) > θ.
Various algorithms for processing STJoin queries have been proposed.
According to the results of the experimental evaluation in [3], the most eﬃcient
algorithm for evaluating STJoin queries is PPJ-C algorithm. Given a spatio-
textual dataset, PPJ-C ﬁrst deﬁnes a dynamic grid partitioning such that the
length of the side of each grid cell is equal to the distance threshold ϵ. Next,
for each cell C, PPJ-C identiﬁes the set of join cells AC, i.e., cell C along with
its adjacent cells. Finally, for each pair of join cells, the algorithm compares the
elements in the two cells and adds to the result set all pairs of objects that satisfy
both the textual similarity and the spatial distance constraint.
4
Spatio-Textual Data Quality Assessment
In this section, we describe our methodology for assessing the relative quality
of a spatio-textual dataset. Let a spatio-textual dataset D be the dataset the
quality of which we will evaluate. Since we operate in the absence of ground
truth, the main idea behind our approach is to cross-reference the entries of D
with other similar spatio-textual datasets. For example, given a dataset De the
quality of which we want to assess, and another dataset Dr, we ﬁrst match each
spatio-textual object x ∈De with some spatio-textual object y ∈Dr such that
x and y refer to the same real-world entity. Then, for each attribute of x we
compute a relative correctness indicator by comparing the attributes of x with
the attributes of y.
4.1
Attribute Relative Correctness Indicator
We propose the Attribute Relative Correctness Indicator (ARCI) to assess the
quality of a spatio-textual dataset. Let De be the spatio-textual dataset the qual-
ity of which we want to assess and Dr be another spatio-textual dataset. Each
dataset containts spatio-textual objects x = ⟨x.id, x.txt, x.loc, x.attr⟩, where
x.txt and x.loc are the identiﬁers of x. Given a textual similarity threshold
θ, and a spatial distance threshold ϵ, for every spatio-textual object xi ∈De
we compute the spatio-textual object yj ∈Dr which is the most similar to xi
and does not violate the θ and ϵ thresholds (it is possible that no matching
spatial-object is found). Having matched all possible spatial objects, the ARCI
of xi.attr is

16
M. Ge and T. Chondrogiannis
ARCI(xi.attr, yj.attr) =

Sim(xi.attr, yj.attr),
if yj is the best match to xi
0.5,
if ̸ ∃yj that matches xi.
Figure 1 illustrates two datasets: Dataset 1 (De), the dataset the quality of
which we evalute, and Dataset 2, (Dr) another dataset we use as reference. The
identiﬁers of both De and Dr are the name and the location, while the attribute
for which we want to compute the ARCI is the type. The textual similarity is
given by the Levenshtein distance, while the spatial distance is given by the
Euclidean distance. Finally, we set the textual similarity threshold ϵ = 0.6 and
the spatial distance threshold θ = 1.
Dataset 1 (Deval)
id
name
type
loc.
x1
Loacker
Caf´e
(1, 1)
x2
Marilyn
Caf´e
(3, 2)
x3 Cavalino Bianco Restaurant (1, 6)
x4
Dai Carrettai
Restaurant (5, 5)
x5
Enovit Bar
Bar
(8, 9)
x6
Hopfen
Brewery
(7, 5)
Dataset 2 (Dref )
id
name
type
loc.
y1
Da Pichio
Bar
(4, 5)
y2
Loacker
Caf´e
(1, 1)
y3
Nadamas
Bar
(5, 1)
y4
Marylin
Caf´e
(3, 3)
y5 Enovit Wine Bar Wine Bar (9, 8)
y6 Stadt Caf´e Citt`a
Caf´e
(8, 2)
y7
Nussbaumer
Restaurant (3, 7)
y8
Hopfen & Co.
Restaurant (7, 5)
y9
Carrettai
Restaurant (7, 7)
Fig. 1. Sample spatio-textual datasets.
First, we attempt to match each entry xi ∈De with an entry yi ∈Dr.
The result of this operation is illustrated in Fig. 2. Having computed the match-
ing spatio-textual objects, we compute the ARCI for the attribute ”type” for
each spatio-textual object in De. For x1 and x2 we have an exact match, hence
ARCI(x1.attr) = 1 and ARCI(x2.attr) = 1. For x1 and x2 the ARCI indi-
cates that both x1.attr and x2.attr are correct with regard to Dr. For, x3
and x4 no matching object in Dr was found; thus ARCI(x3.attr) = 0.5 and
ARCI(x4.attr) = 0.5 meaning that it was not possible to verify the correct-
ness of the attributes. Finally, for elements x5 and x6 the respective ARCIs are
ARCI(x5.attr) = 0.36 and ARCI(x6.attr) = 0.1 which are relatively low and,
therefore, our approach indicates that x5.attr and x6.attr might be wrong.
xi ∈Deval yj ∈Dref Sim(xi.txt, yj, txt) dist(xi.loc, yi.loc) ARCI
x1
y2
1
0
1
x2
y4
0
1
1
x3
-
-
-
0.5
x4
-
-
-
0.5
x5
y5
0.667
1.414
0.36
x6
y8
0.667
0
0.1
0
0.5
1
Fig. 2. Matching entries of De and Dr of Fig. 1.

Assessing the Quality of Spatio-Textual Datasets
17
Note that ARCI cannot be computed for the identiﬁers of a spatio-textual
object. Apparently, the selection of the identiﬁers (especially the textual one)
is crucial and can possibly aﬀect the eﬃciency of our approach. However, as
determining the best possible identiﬁers of a spatio-textual object is out of the
scope of this paper, we work under the assumption that each spatio-textual
object comes with the proper identiﬁers.
4.2
The ARCI-GP Algorithm
To compute the ARCI for each spatio-textual object in a dataset we propose
ARCI-GP, an algorithm which is inspired by the PPJ-C [3] algorithm for spatio-
textual similarity joins. Let De be the spatio-textual dataset the quality of which
we want to assess and Dr be the reference dataset. First, ARCI-GP deﬁnes a
dynamic grid partitioning and organizes the spatio-textual objects of both De
and Dr into cells. The grid is deﬁned such that the length of the side of every
cell equals the spatial distance threshold ϵ. For each cell C, ARCI-GP identiﬁes a
set of join cells AC, i.e., the set of the adjacent cells of C along with C itself. For
every element x ∈C of De, AC contains the objects y ∈Dr which can possibly
match with x. Due to the properties of the grid, the distance of all other elements
y′ ̸∈AC from x is more than ϵ, i.e., they violate the spatial distance constraint.
Next, ARCI-GP computes for each object x ∈C of De the object y ∈AC of Dr
which is the most similar to x. To determine the best match of x, the algorithm
examines only the objects in AC. During this process, it is possible that the
algorithm does not ﬁnd any matching object to x. The same process is executed
over all the cells that contain at least one object x ∈De.
Algorithm 1 illustrates the pseudocode of our ARCI-GP algorithm. First, the
result set R is initialized in Line 1 and a grid partition GR is constructed for
the spatio-textual objects in both input datasets De and Dr. In Lines 3–16
the algorithm iterates over the cells of the partition that contain at least one
element of De and, for each cell C, it computes the set of join cells AC. Then,
for each cell C′ ∈AC, the algorithm attempts to ﬁnd a match between the
element oi ∈C that is an element of De, and the element oj ∈C′ that is an
element of Dr (Lines 6–16). A new entry r is initialized to null in Line 7. Each
element oi is matched with only one element oj for which the textual similarity
Sim(oi.txt, oj.txt) is maximum and the spatial distance dist(oi.loc, oj.loc) is
minimum (Lines 8–12). If a match is found, i.e. r is not null, then r is added to
the result set with the computed ARCI value in Line 14. Otherwise, r is added to
the result set with the default 0.5 ARCI value. Finally, the result set is returned
in Line 17.

18
M. Ge and T. Chondrogiannis
Algorithm 1. ARCI-GP (De,Dr,ϵ,θ)
Input: Collection of spatio-textual objects De; collection of spatio-textual
objects Dr; spatial distance threshold ϵ; textual similarity threshold θ
Output: Result set R
1 R ←∅;
2 GR ←ConstructGridPartitioning (De ∪Dr,ϵ);
3 foreach cell C ∈GR do
4
AC ←IdentifyJoinCells (GR,C);
5
foreach cell C′ ∈AC do
6
foreach object oi ∈C ∩De do
7
intitialize entry r ←null;
8
simmax = θ;
9
foreach object oj ∈C ∩Dr do
10
if Sim(oi.txt, oj.txt) > simmax and dist(oi.loc, oj.loc) ≤ϵ then
11
r ←⟨oi, ARCI(oi, oj)⟩;
12
simmax ←Sim(oi.txt, oj.txt);
13
if r ̸= null then
14
R ←R ∪r ;
▷Best match added to result
15
else
16
R ←R ∪⟨o, 0.5⟩;
▷No match was found
17 return R;
5
Preliminary Experimental Evaluation
In this section, we report the results of a preliminary experimental evaluation
and compare two diﬀerent implementations of our ARCI-GP algorithm. We use
two diﬀerent spatio-textual datasets in our experiments. The ﬁrst dataset (De)
contains 500, 000 spatio-textual objects and was compiled by combining datasets
obtained from Tourpedia1. The second dataset (Dr) contains 1, 000, 000 spatio-
textual objects and was obtained by querying the Foursquare API2 using ids
obtained from [9]. To observe the eﬀect of the textual similarity metric, we
measure the runtime of ARCI-GP using two diﬀerent metrics: the Levenshtein
similarity [10] and the NGram similarity [8]. We implemented our algorithm with
Java 1.8 and the tests run on a machine with 4 Intel Xeon X5550 (2.67 GHz)
processors and 48 GB main memory running Ubuntu Linux.
Figure 3 shows our measurement results on the runtime of our ARCI-GP
algorithm by varying the sizes of De and Dr. More speciﬁcally, Fig. 3a shows
the runtime of ARCI-GP varying the size of De from 100,000 to 500,000 entries
using the entire Dr dataset, and Fig. 3b shows the runtime of ARCI-GP varying
the size of Dr from 200,000 to 1,000,000 entries using the entire Dr dataset. In
both ﬁgures we observe that the runtime of ARCI-GP increases with the size of
1 http://tour-pedia.org/about/datasets.html.
2 https://developer.foursquare.com.

Assessing the Quality of Spatio-Textual Datasets
19
(a)
(b)
Fig. 3. Performance of PPLJ.
De and Dr. However, we can observe that the runtime increases faster with the
size of De. For example, the runtime for De = 2·105 and Dr = 106 (Fig. 3a) and
the runtime for De = 4·105 and Dr = 5·105 (Fig. 3b). Although in the ﬁrst case
the total number of involved entries is higher by 300,000 entries, the runtime
is approximately the same. Apparently, the size of De inﬂuences the runtime
of ARCI-GP much more than the size of Dr. Such a result is to be expected as
ARCI-GP considers all the elements in De, while many of the elements in Dr are
ﬁltered out by the grid partitioning.
Finally, with regard to the textual similarity metric, we observe that our
algorithm is almost two time faster when using the Levenshtein similarity than
when using the NGram similarity. Since the algorithm requires the execution
of many textual similarity operations, the computational cost of the similarity
metric has a great inﬂuence on the total runtime of the ARCI-GP.
6
Conclusion
In this paper, we proposed ARCI, an indicator which operates in the absence
of ground truth and shows the relative quality of a spatio-textual dataset by
cross-referencing it with similar datasets. ARCI is computed for the attributes
of a data entry in a spatio-textual dataset to indicate its relative correctness. We
have also shown that ARCI can be used directly to provide visual information on
the quality of the data, e.g., using a spectrum bar. Furthemore, we proposed an
algorithm for computing ARCI and evaluated its performance using real-world
spatio-textual datasets.
As future work, we will explore diﬀerent strategies to develop eﬃcient algo-
rithm for computing ARCI such as executing textual-matches ﬁrst. We will
also consider utilizing indexing structures to improve performance even further.
Finally, we plan to investigate alternative metrics for computing textual similar-
ity such as semantic similarity metrics.

20
M. Ge and T. Chondrogiannis
References
1. Abedjan, Z., Akcora, C.G., Ouzzani, M., Papotti, P., Stonebraker, M.: Temporal
rules discovery for web data cleaning. Proc. VLDB Endowment 9(4), 336–347
(2015)
2. Ballesteros, J., Cary, A., Rishe, N.: Spsjoin: parallel spatial similarity joins. In:
Proceedings of the 19th ACM SIGSPATIAL GIS Conference, pp. 481–484 (2011)
3. Bouros, P., Ge, S., Mamoulis, N.: Spatio-textual similarity joins. Proc. VLDB
Endowment 6(1), 1–12 (2012)
4. Cao, Y., Fan, W., Yu, W.: Determining the relative accuracy of attributes. In:
Proceedings of the 2013 ACM SIGMOD Conference, pp. 565–576 (2013)
5. Chiang, F., Miller, R.J.: Discovering data quality rules. Proc. VLDB Endowment
1(1), 1166–1177 (2008)
6. Cong, G., Fan, W., Geerts, F., Jia, X., Ma, S.: Improving data quality: consistency
and accuracy. In: Proceedings of the 33rd VLDB Conference, pp. 315–326 (2007)
7. Galarus, D., Angryk, R.: A smart approach to quality assessment of site-based
spatio-temporal data. In: Proceedings of the 24th ACM SIGSPATIAL GIS Con-
ference, pp. 55:1–55:4 (2016)
8. Kondrak, G.: N-gram similarity and distance. In: Consens, M., Navarro, G. (eds.)
SPIRE 2005. LNCS, vol. 3772, pp. 115–126. Springer, Heidelberg (2005). doi:10.
1007/11575832 13
9. Levandoski, J.J., Sarwat, M., Eldawy, A., Mokbel, M.F.: Lars: a location-aware
recommender system. In: Proceedings of the 28th IEEE ICDE, pp. 450–461 (2012)
10. Levenshtein, V.: Binary codes capable of correcting deletions, insertions, and rever-
sals. Soviet Phys. Doklady 10, 707–710 (1965)
11. Missier, P., Embury, S., Greenwood, M., Preece, A., Jin, B.: Quality views: cap-
turing and exploiting the user perspective on data quality. In Proceedings of the
32nd VLDB Conference, pp. 977–988 (2006)
12. Rao, J., Lin, J., Samet, H.: Partitioning strategies for spatio-textual similarity
join. In: Proceedings of the 3rd ACM International Workshop on Analytics for Big
Geospatial Data, pp. 40–49 (2014)
13. Razniewski, S., Nutt, W.: Completeness of queries over incomplete databases. Proc.
VLDB Endowment 4(11), 749–760 (2011)
14. Recchia, G., Louwerse, M.: A comparison of string similarity measures for toponym
matching. In: Proceedings of The 1st ACM International COMP Workshop, pp.
54:54–54:61 (2013)
15. Tsatsanifos, G., Vlachou, A.: On processing top-k spatio-textual preference queries.
In: Proceedings of the 18th EDBT Confernce, pp. 433–444 (2015)
16. Wang, R.Y., Strong, D.M.: Beyond accuracy: what data quality means to data
consumers. J. Manage. Inf. Syst. 12(4), 5–33 (1996)

T2K2: The Twitter Top-K Keywords Benchmark
Ciprian-Octavian Truic˘a1(B) and J´erˆome Darmont2
1 Computer Science and Engineering Department, Faculty of Automatic Control
and Computers, University Politehnica of Bucharest, Bucharest, Romania
ciprian.truica@cs.pub.ro
2 Universit´e de Lyon, Lyon 2, ERIC EA 3083, Lyon, France
jerome.darmont@univ-lyon2.fr
Abstract. Information retrieval from textual data focuses on the con-
struction of vocabularies that contain weighted term tuples. Such vocab-
ularies can then be exploited by various text analysis algorithms to
extract new knowledge, e.g., top-k keywords, top-k documents, etc. Top-
k keywords are casually used for various purposes, are often computed
on-the-ﬂy, and thus must be eﬃciently computed. To compare competing
weighting schemes and database implementations, benchmarking is cus-
tomary. To the best of our knowledge, no benchmark currently addresses
these problems. Hence, in this paper, we present a top-k keywords bench-
mark, T2K2, which features a real tweet dataset and queries with various
complexities and selectivities. T2K2 helps evaluate weighting schemes
and database implementations in terms of computing performance. To
illustrate T2K2’s relevance and genericity, we show how to implement the
TF-IDF and Okapi BM25 weighting schemes, on one hand, and relational
and document-oriented database instantiations, on the other hand.
Keywords: Top-k keywords · Benchmark · Term weighting · Database
systems
1
Introduction
Analyzing textual data is a current challenge, notably due to the vast amount
of text generated daily by social media. One approach for extracting knowledge
is to infer from texts the top-k keywords to determine trends [1,14], or to detect
anomalies or more generally events [7]. Computing top-k keywords requires build-
ing a weighted vocabulary, which can also be used for many other purposes such
as topic modeling and clustering. Term weights can be computed at the appli-
cation level, which is ineﬃcient when working with large data volumes because
all information must be queried and processed at a layer diﬀerent from storage.
A presumably better approach is to process information at the storage layer
using aggregation functions, and then return the result to the application layer.
Yet, the term weighting process remains very costly, because each time a query
is issued, at least one pass through all documents is needed.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 21–28, 2017.
DOI: 10.1007/978-3-319-67162-8 3

22
C.-O. Truic˘a and J. Darmont
To compare combinations of weighting schemes, computing strategies and
physical implementations, benchmarking is customary. However, to the best of
our knowledge, there exists no benchmark for this purpose. Hence, we propose
in this paper the Twitter Top-K Keywords Benchmark (T2K2), which features
a real tweet dataset and queries with various complexities and selectivities. We
designed T2K2 to be somewhat generic, i.e., it can compare various weighting
schemes, database logical and physical implementations and even text analytics
platforms [18] in terms of computing eﬃciency. As a proof of concept of T2K2’s
relevance and genericity, we show how to implement the TF-IDF and Okapi
BM25 weighting schemes, on one hand, and relational and document-oriented
database instantiations, on the other hand.
The remainder of this paper is organized as follows. Section 2 reviews text-
oriented benchmarks. Section 3 provides T2K2’s generic speciﬁcation. Section 4
details T2K2’s proof of concept, i.e., its instantiation for several weighting
schemes and database implementations. Finally, Sect. 5 concludes this paper and
hints at future research.
2
Related Work
Term weighting schemes are extensively benchmarked in sentiment analysis [15],
semantic similarity [11], text classiﬁcation and categorization [8,9,11,13], and
textual corpus generation [19]. Benchmarks for text analysis focus mainly on
algorithm accuracy, while either term weights are known before the algorithm
is applied, or their computation is incorporated with preprocessing. Thus, such
benchmarks do not evaluate weighting scheme construction eﬃciency as we do.
Other benchmarks evaluate parallel text processing in big data applications
in the cloud [4,5]. PRIMEBALL notably speciﬁes several relevant properties
characterizing cloud platforms [4], such as scale-up, elastic speedup, horizon-
tal scalability, latency, durability, consistency and version handling, availabil-
ity, concurrency and other data and information retrieval properties. However,
PRIMEBALL is only a speciﬁcation; it is not implemented.
3
T2K2 Speciﬁcation
Typically, a benchmark is constituted of a data model (conceptual schema and
extension), a workload model (set of operations) to apply on the dataset, an
execution protocol and performance metrics [3]. In this section, we provide a
conceptual description of T2K2, so that it is generic and can cope with various
weighting schemes and database logical and physical implementations.
3.1
Data Model
The base dataset we use is a corpus of 2 500 000 tweets that was collected using
Twitter’s REST API to read and gather data. Moreover, we applied preprocess-
ing steps to the raw corpus to extract the additional information needed to build

T2K2: The Twitter Top-K Keywords Benchmark
23
a weighted vocabulary: (1) extract all tags and remove links; (2) expand contrac-
tions, i.e., shortened versions of the written and spoken forms of a word, syllable,
or word group, created by omission of internal letters and sounds [2], e.g., “it’s”
becomes “it is”; (3) extract sentences and remove punctuation in each sentence,
creating a clean text; (4) for each sentence, extract lemmas and create a lemma
text; (5) for each lemma t in tweet d, compute the number of co-occurrences ft,d
and term frequency TF(t, d), which normalizes ft,d.
T2K2 database’s conceptual model (Fig. 1) represents all the information
extracted after the text preprocessing steps. Information about tweet Author
are a unique identiﬁer, ﬁrst name, last name and age. Information about author
Gender is stored in a diﬀerent entity to minimize the number of duplicates
of gender type. Documents are identiﬁed by the tweet’s unique identiﬁer and
store the raw tweet text, clean text, lemma text, and the tweet’s creation date.
Writes is the relationship that associates a tweet to its author. Tweet location
is stored in the Geo Location entity to avoid duplicates again. Word bears a
unique identiﬁer and the actual lemma. Finally, weights ft,d and TF(t, d) for
each lemma and each document are stored in the Vocabulary relationship.
Fig. 1. T2K2 conceptual data model
The initial 2 500 000 tweet corpus is split into 5 diﬀerent datasets that all keep
an equal balance between the number of tweets for both genders, location and
date. These datasets contain 500 000, 1 000 000, 1 500 000, 2 000 000 and 2 500 000
tweets, respectively. They allow scaling experiments and are associated to a scale
factor (SF) parameter, where SF ∈{0.5, 1, 1.5, 2, 2.5}, for conciseness sake.
3.2
Workload Model
The queries used in T2K2 are designed to achieve two goals: (1) compute dif-
ferent term weighting schemes using aggregation functions and return the top-k

24
C.-O. Truic˘a and J. Darmont
keywords; (2) test the performance of diﬀerent database management systems.
T2K2 queries are suﬃcient for achieving these goals, because they test the query
execution plan, internal caching and the way they deal with aggregation. More
precisely, they take diﬀerent group by attributes into account and aggregate the
information to compute weighting schemes for top-k keywords.
T2K2 features four queries Q1 to Q4 that compute top-k keywords w.r.t. con-
straint(s): c1(Q1), c1 ∧c2(Q2), c1 ∧c3(Q3), c1 ∧c2 ∧c3(Q4). c1 is Gender.Type =
pGender, where parameter pGender ∈{male, female}. c2 is Document.Date ∈
[pStartDate, pEndDate], where pStartDate, pEndDate ∈[2015-09-17 20:41:35,
2015-09-19 04:05:45] and pStartDate < pEndDate. c3 is Geo location.X ∈
[pStartX, pEndX] and Geo location.Y ∈[pStartY, pEndY], where pStartX,
pEndX ∈[15, 50], pStartX < pEndX, pStartY, pEndY ∈[−124, 120] and
pStartY < pEndY. Queries bear diﬀerent levels of complexity and selectivity.
3.3
Performance Metrics and Execution Protocol
We use each query’s response time t(Qi) as metrics in T2K2. Given scale factor
SF, all queries Q1 to Q4 are executed 40 times, which is suﬃcient according to
the central limit theorem. Average response times and standard deviations are
computed for t(Qi). All executions are warm runs, i.e., either caching mecha-
nisms must be deactivated, or a cold run of Q1 to Q4 must be executed once (but
not taken into account in the benchmark’s results) to ﬁll in the cache. Queries
must be written in the native scripting language of the target database system
and executed directly inside said system using the command line interpreter.
4
T2K2 Proof of Concept
In this section, we aim at illustrating how T2K2 works and at demonstrating that
it can adequately benchmark what it is designed for, i.e., weighting schemes and
database implementations. For this sake, we ﬁrst compare the TF-IDF and Okapi
BM25 weighting schemes in terms of computing eﬃciency. Second, we seek to
determine whether a document-oriented database is a better solution than in a
relational databases when computing a given term weighting scheme.
4.1
Weighting Schemes
Let D be the corpus of tweets, N = |D| the total number of documents (tweets)
in D and n the number of documents where some term t appears. The TF-IDF
weight is computed by multiplying the augmented term frequency TF(t, d) =
K + (1 −K) ·
ft,d
maxt′∈d(ft′,d)) by the inverted document frequency IDF(t, D) =
1 + log N
n , i.e., TFIDF(t, d, D) = TF(t, d) · IDF(t, D). The augmented form
of TF prevents a bias towards long tweets when the free parameter K is set
to 0.5 [12]. It uses the number of co-occurrences ft,d of a word in a document,
normalized with the frequency of the most frequent term t′, i.e., maxt′∈d(ft′,d).

T2K2: The Twitter Top-K Keywords Benchmark
25
The Okapi BM25 weight is given in Eq. (1), where ||d|| is d’s length, i.e., the
number of terms appearing in d. Average document length avgd′∈D(||d′||) is used
to remove any bias towards long documents. The values of free parameters k1
and b are usually chosen, in absence of advanced optimization, as k1 ∈[1.2, 2.0]
and b = 0.75 [10,16,17].
Okapi(t, d, D) =
TFIDF(t, d, D) · (k1 + 1)
TF(t, d) + k1 · (1 −b + b ·
||d||
avgd′∈D(||d′||))
(1)
The sum S TFIDF(t, d, D) = N
i=1 TFIDF(t, di, D) of all TF-IDFs and
the sum S Okapi(t, d, D) = N
i=1 Okapi(t, di, D) of all Okapi BM25 weights
constitute the term’s weights that are used to construct the list of top-k key-
words.
4.2
Relational Implementation
Database. The logical relational schema used in both relational databases man-
agement systems (Fig. 2) directly translates the conceptual schema from Fig. 1.
Fig. 2. T2K2 relational logical schema
Queries. Text analysis deals with discovering hidden patterns from texts. In
most cases, it is useful to determine such patterns for given groups, e.g., males
and females, because they have diﬀerent interests and talk about disjunct sub-
jects. Moreover, if new events appear, depending on the location and time of day,
these subject can change for the same group of people. The queries we propose
aim to determine such hidden patterns and improve text analysis and anomaly
detection.
Let us express T2K2’s queries in relational algebra. c1, c2 and c3 are the
constraints deﬁned in Sect. 3.2, adapted to the relational schema.

26
C.-O. Truic˘a and J. Darmont
Q1
=
γL(πdocuments.id,words.word,fw(vocabulary.count,vocabulary.tf)(σc1(docu-
ments
▷◁c4
documents authors
▷◁c5
authors
▷◁c6
genders
▷◁c7
vocabulary
▷◁c8
words))),
where
c4
to
c8
are
join
conditions;
fw
is
the
weight-
ing function that computes TF-IDF or Okapi BM25, which takes two
parameters:
vocabulary.count
=
ft,d
and
vocabulary.tf
=
TF(t, d);
γL
is
the
aggregation
operator,
where
L
=
(F, G),
with
F
=
sum(fw(vocabulary.count, vocabulary.tf)) and G is the words.word attribute
that appears in the group by clause.
Q2 = γL(πdocuments.id,words.word,fw(vocabulary.count,vocabulary.tf)(σc1∧c2(docu-
ments ▷◁c4 documents authors ▷◁c5 authors ▷◁c6 genders ▷◁c7 vocabulary ▷◁c8
words))).
Q3 = γL(πdocuments.id,words.word,fw(vocabulary.count,vocabulary.tf)(σc1∧c3(docu-
ments ▷◁c4 documents authors ▷◁c5 authors ▷◁c6 genders ▷◁c7 vocabulary ▷◁c8
words ▷◁c9 geo location))), where c9 is the join condition between documents
and geo location.
Q4
=
γL(πdocuments.id,words.word,fw(vocabulary.count,vocabulary.tf)(σc1∧c2∧c3
(documents ▷◁c4 documents authors ▷◁c5 authors ▷◁c6 genders ▷◁c7 vocabulary ▷◁c8
words ▷◁c9 geo location))).
4.3
Document-Oriented Implementation
Database.
In
a
Document
Oriented
Database
Management
System
(DODBMS), all information is typically stored in a single collection. The many-
to-many Vocabulary relationship from Fig. 1 is modeled as a nested document
for each record. The information about user and date become single ﬁelds in a
document, while the location becomes an array. Figure 3 presents an example of
the DODBMS document.
{
i d
:
644626677310603264 ,
rawText
:
”Amanda ’ s
car
i s
too much f o r my headache” ,
cleanText
:
”Amanda i s
car
i s
too much f o r my headache” ,
lemmaText
:
”amanda car
headache” ,
author
:
970993142 ,
geoLocation
:
[
32 , 79
] ,
gender
:
”male” ,
age
:
23 ,
lemmaTextLength
:
3 ,
words
:
[
{ ” t f ”
:
1 , ”count”
:
1 , ”word”
:
”amanda” } ,
{ ” t f ”
:
1 , ”count”
:
1 , ”word”
:
” car ” } ,
{ ” t f ”
:
1 , ”count”
:
1 , ”word”
:
”headache”}
] ,
date
:
ISODate ( ”2015−09−17T23 : 3 9 : 1 1Z” ) }
Fig. 3. Sample DODBMS document

T2K2: The Twitter Top-K Keywords Benchmark
27
Queries. In DODBMSs, user-deﬁned (e.g., JavaScript) functions are used to
compute top-k keywords. The TF-IDF weight can take advantage of both native
database aggregation (NA) and MapReduce (MR). However, due to the multi-
tude of parameters involved and the calculations needed for the Okapi BM25
weighting scheme, the NA method is usually diﬃcult to develop. Thus, we rec-
ommend to only use MR in benchmark runs.
5
Conclusion
Jim Gray deﬁned four primary criteria to specify a “good” benchmark [6]. Rele-
vance: The benchmark must deal with aspects of performance that appeal to the
largest number of users. Considering the wide usage of top-k queries in various
text analytics tasks, we think T2K2 fulﬁlls this criterion. We also show in Sect. 4
that our benchmark achieves what it is designed for.
Portability: The benchmark must be reusable to test the performances of dif-
ferent database systems. We successfully instantiated T2K2 within two types of
database systems, namely relational and document-oriented systems.
Simplicity: The benchmark must be feasible and must not require too many
resources. We designed T2K2 with this criterion in mind (Sect. 3), which is par-
ticularly important for reproducibility. We notably made up parameters that are
easy to setup.
Scalability: The benchmark must adapt to small or large computer architectures.
By introducing scale factor SF, we allow users to simply parameterize T2K2 and
achieve some scaling, though it could be pushed further in terms of data volume.
In future work, we plan to expand T2K2’s dataset signiﬁcantly to aim at big
data-scale volume. We also intend to further our proof of concept and valida-
tion eﬀorts by benchmarking other NoSQL database systems and gain insight
regarding their capabilities and shortcomings. We also plan to adapt T2K2 so
that it runs in the Hadoop and Spark environments.
References
1. Bringay, S., B´echet, N., Bouillot, F., Poncelet, P., Roche, M., Teisseire, M.: Towards
an on-line analysis of tweets processing. In: Hameurlain, A., Liddle, S.W., Schewe,
K.-D., Zhou, X. (eds.) DEXA 2011. LNCS, vol. 6861, pp. 154–161. Springer,
Heidelberg (2011). doi:10.1007/978-3-642-23091-2 15
2. Cooper, J.D., Robinson, M.D., Slansky, J.A., Kiger, N.D.: Literacy: Helping Stu-
dents Construct Meaning. Cengage Learning, Boston (2014)
3. Darmont, J.: Data processing benchmarks. In: Khosrow, M. (ed.) Encyclopedia of
Information Science and Technology, 3rd edn., pp. 146–152. IGI Global, Hershey
(2014)

28
C.-O. Truic˘a and J. Darmont
4. Ferrarons, J., Adhana, M., Colmenares, C., Pietrowska, S., Bentayeb, F., Darmont,
J.: PRIMEBALL: a parallel processing framework benchmark for big data appli-
cations in the cloud. In: Nambiar, R., Poess, M. (eds.) TPCTC 2013. LNCS, vol.
8391, pp. 109–124. Springer, Cham (2014). doi:10.1007/978-3-319-04936-6 8
5. Gattiker, A.E., Gebara, F.H., Hofstee, H.P., Hayes, J.D., Hylick, A.: Big data text-
oriented benchmark creation for Hadoop. IBM J. Res. Dev. 57(3/4), 10: 1–10: 6
(2013)
6. Gray, J.: The Benchmark Handbook for Database and Transaction Systems, 2nd
edn. Morgan Kaufmann, Burlington (1993)
7. Guille, A., Favre, C.: Event detection, tracking, and visualization in twitter: a
mention-anomaly-based approach. Soc. Netw. Anal. Min. 5(1), 18 (2015)
8. Kılın¸c, D., ¨Oz¸cift, A., Bozyigit, F., Yildirim, P., Y¨ucalar, F., Borandag, E.: TTC-
3600: A new benchmark dataset for turkish text categorization. J. Inf. Sci. 43(2),
174–185 (2017)
9. Lewis, D.D., Yang, Y., Rose, T.G., Li, F.: RCV1: A new benchmark collection for
text categorization research. J. Mach. Learn. Res. 5, 361–397 (2004)
10. Manning, C.D., Raghavan, P., Sch¨utze, H.: Introduction to Information Retrieval.
Cambridge University Press, New York (2008)
11. O’Shea, J., Bandar, Z., Crockett, K.A., McLean, D.: Benchmarking short text
semantic similarity. Int. J. Intell. Inf. Database Syst. 4(2), 103–120 (2010)
12. Paltoglou, G., Thelwall, M.: A study of information retrieval weighting schemes for
sentiment analysis. In: 48th Annual Meeting of the Association for Computational
Linguistics, pp. 1386–1395 (2010)
13. Partalas, I., Kosmopoulos, A., Baskiotis, N., Arti`eres, T., Paliouras, G., Gaussier,
´E., Androutsopoulos, I., Amini, M., Gallinari, P.: LSHTC: a benchmark for large-
scale text classiﬁcation. CoRR abs/1503.08581 (2015)
14. Ravat, F., Teste, O., Tournier, R., Zurﬂuh, G.: Top Keyword: an aggregation func-
tion for textual document OLAP. In: Song, I.-Y., Eder, J., Nguyen, T.M. (eds.)
DaWaK 2008. LNCS, vol. 5182, pp. 55–64. Springer, Heidelberg (2008). doi:10.
1007/978-3-540-85836-2 6
15. Reagan, A.J., Tivnan, B.F., Williams, J.R., Danforth, C.M., Dodds, P.S.: Bench-
marking sentiment analysis methods for large-scale texts: a case for using
continuum-scored words and word shift graphs. CoRR abs/1512.00531 (2015)
16. Sp¨arck Jones, K., Walker, S., Robertson, S.E.: A probabilistic model of information
retrieval: development and comparative experiments: part 1. Inf. Process. Manage.
36(6), 779–808 (2000)
17. Sp¨arck Jones, K., Walker, S., Robertson, S.E.: A probabilistic model of information
retrieval: development and comparative experiments: part 2. Inf. Process. Manage.
36(6), 809–840 (2000)
18. Truic˘a, C.O., Darmont, J., Velcin, J.: A scalable document-based architecture for
text analysis. In: International Conference on Advanced Data Mining and Appli-
cations (ADMA), pp. 481–494 (2016)
19. Wang, L., Dong, X., Zhang, X., Wang, Y., Ju, T., Feng, G.: TextGen: a realistic text
data content generation method for modern storage system benchmarks. Front. Inf.
Technol. Electron. Eng. 17(10), 982–993 (2016)

Outlier Detection in Data Streams
Using OLAP Cubes
Felix Heine(B)
Department of Computer Science, Faculty IV,
Hannover University of Applied Sciences and Arts,
Ricklinger Stadtweg 120, 30459 Hannover, Germany
felix.heine@hs-hannover.de
Abstract. Outlier detection is an important tool for many application
areas. Often, data has some multidimensional structure so that it can be
viewed as OLAP cubes. Exploiting this structure systematically helps
to ﬁnd outliers otherwise undetectable. In this paper, we propose an
approach that treats streaming data as a series of OLAP cubes. We then
use an oﬄine calculated model of the cube’s expected behavior to ﬁnd
outliers in the data stream. Furthermore, we aggregate multiple outliers
found concurrently at diﬀerent cells of the cube to some user-deﬁned
level in the cube. We apply our method to network data to ﬁnd attacks
in the data stream to show its usefulness.
1
Introduction
Outlier detection is an important basic technique used in many application areas
including data quality, fraud detection, or intrusion detection in networks. In this
paper, we look at outlier detection in multidimensional data. We present a novel
method for outlier detection that exploits the multidimensional nature of data.
It is a one-class anomaly detection approach that includes a training phase using
only-normal data. The training is done oﬄine, the detection phase can be run
either online or oﬄine. Exploiting the multidimensional structure of data helps
to ﬁnd outliers that are undetectable at the highest aggregation level or to collect
more evidence for suspicious data by looking at various multidimensional views.
This also helps to ﬁnd contextual and collective outliers (see [2]). First, aggre-
gating the data in diﬀerent ways allows for contextual models by assigning diﬀer-
ent models to diﬀerent groupings. Second, collective outliers can be spotted more
easily when looking at the grouping where the outlier becomes most evident. In
order to deﬁne and search the groupings in a systematic way, we treat all data
sources as multidimensional data cubes. For most data, this is a natural way
to model the data. For example, network data can be grouped and aggregated
e.g. using target IP ranges, source IP ranges, and protocols at diﬀerent layers.
This work has partly been developed in the project IQM4HD (reference number:
01IS15053B). IQM4HD is partly funded by the German ministry of education and
research (BMBF) within the research program KMU Innovativ.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 29–36, 2017.
DOI: 10.1007/978-3-319-67162-8 4

30
F. Heine
The advantage of this approach is the natural treatment of collective and contex-
tual outliers. We integrate the anomaly detection results coming from diﬀerent
views of the data to collect evidence and to build a ﬁnal score for each entity
of interest. The method itself is general. Domain knowledge is only needed to
properly deﬁne the cube’s dimensions and metrics.
The main contribution of this paper is a one-class method to ﬁnd anomalies
in multidimensional data cubes using models that describe the normal behavior
of the data at a ﬁne-grained level. This method is complemented by a second
method that combines anomaly scores that were calculated for diﬀerent data
views and to relate these scores to relevant units of inspection. These units
are application dependent and deﬁne the granularity of the generated anomaly
events. For network data, a useful unit of inspection is a single network con-
nection. As an application example, the method is applied to network data. We
provide an evaluation using data from the 1998 DARPA IDS evaluation.
The next section describes the proposed method in detail on a generic level
that is independent from a speciﬁc application. Transfer to a network example
scenario is done in the evaluation Sect. 3. Before concluding, related work is
described. Please note that we assume that the reader is familiar with basic
OLAP cube terminology and iceberg cubes. For an introduction, see [4,14].
2
Anomaly Detection in Multidimensional Data
In this section, we describe our approach. The basis is time-related multidi-
mensional data, i.e. a stream of tuples that each contain a time-stamp, multiple
dimensional attributes (categorical) and multiple metric attributes (continuous).
We furthermore assume to have recorded old data that well reﬂects normal
behavior and can be used to train the system.
We split both training and test data into a sequence of cubes by accumulating
data over an interval ΔT. In the training phase, this is done oﬄine. Here, we
build the models that describe the normal behavior of the data. During the
online operation, we collect data until the interval is completed and then build
the cube for the past interval. We then use the models to ﬁnd outliers in the
current time slice. As an outlier will be visible in multiple aggregation views, we
furthermore integrate multiple outlier scores in a single score per unit. We now
describe details for both phases.
2.1
Training Phase
The basic idea is to build a separate model for each cell of the cube, and to check
online data against this model. The training data must not contain anomalies. As
a basic model, we start with a Gaussian model, however, more complex models
are possible.
The model should be able to assess the outlier score for the metric values in a
single cell as well as the correlation between diﬀerent cells. Our basic model cap-
tures the distribution of each metric in individual cells without inter-relationships

Outlier Detection in Data Streams Using OLAP Cubes
31
between the metrics as a univariate Gaussian model. Thus it is easy to calcu-
late model parameters (estimated standard deviation and estimated mean), and
easy to apply (calculation of Z-scores). More complex models could ﬁt a time
series model to a cell, e.g. an autoregressive model. However, already this simple
model leads to a huge number of parameters, when looking at the whole cube,
thus we aim to limit the number cells where a model is created. The number of
underlying data samples (size of a cell) is a good indicator to do the pruning. We
use a system-wide parameter Tm as a threshold for the minimal number of time
steps where the data must be present for a cell in order to build an individual
model for this cell. This avoids models that are not backed by enough data and
limits the size of the overall model.
Beside the behavior of individual cells, we also aim to model the relationship
between cells. We capture the relationship between each cell and all of its parent
cells by providing a model for every direct connection in the cube lattice. In the
initial implementation, this model uses the ratio between the parent cell’s metrics
and the child cell’s metrics. As long as we restrict the aggregation functions to
summation over non-negative values, we know that the parent’s metric is always
larger or equal to the child’s metric. This leads to a ratio between 1 and 0, that
we then model using a univariate Gaussian.
2.2
Online Operation
During online operation, we collect data for each time interval (again of size ΔT)
and build an iceberg cube as soon as the interval is complete. Here, the iceberg
condition is a cell with a minimal count of Ti underlying base records. For each
cell that appears in the iceberg cube, a model is looked up. If one is found, we
calculate an outlier score and store it to an outlier cube. Also for all upper level
cells in the cube lattice, the scores are calculated and stored.
There will be lots of cases where no ﬁtting model is available. This happens
either when the training data did not contain the test cell, or when the training
data contained fewer than Tm time intervals with data for the test cell. In this
case, the cell model was pruned from the model. So we cannot compute any
score for this cell. However, due to the collection of scores from related cells, we
are still able to compute a score for each unit of interest.
The result of the ﬁrst step is an outlier cube, that contains anomaly score
facts corresponding to the facts in the base cube. In the second step, we relate the
anomaly scores to the units of inspection. In the case of network data, the unit
of inspection is a connection, corresponding to a base cell of the cube. However,
this does not need to be the case.
For each cell that corresponds to a unit of inspection we collect all related
scores. These are all scores from either ancestor cells or descendent cells of the
inspected cell in the outlier score cube. We store all scores in a single record that
integrates all evidence that is related to the inspected unit. In this record, we
have scores related to the correlation models, and there is one score per metric,
both for the basic models and the correlation models.

32
F. Heine
This results in a large, however ﬁxed number of scores for each unit. The
number is ﬁxed as it only depends on the cube model and not on the current
data instances. For our sample cube, there are more than 1000 individual scores
per unit. Each additional dimension would multiply this number. However, due
to iceberg conditions, not every score might be present for every unit.
We have multiple options to structure this record. In order to lose as few
information as possible, we could reserve one column for each potential ancestor
or descendent, resulting in high-dimensional records. To make the dimensionality
smaller, we map outlier scores to the distance in the cube lattice. By this, we
mean the length of the shortest path from the inspected cell to the outlier score
cell. Thus the cell itself has distance zero, the parent cells have distance one,
direct children have distance −1, and so on. In the case of our example network
cube, we have 11 diﬀerent distances.
In case a unit lives for more than one time unit, we have multiple such
records. For the ﬁnal result, we build both the average score per distance over
all time units and the maximum score over all time units. So the ﬁnal score
record for each unit looks as follows. By mind and maxd, we mean the minimum
and maximum distance of any cell in the cube to the units of interest. These
values only depend on the cube structure and are thus ﬁxed for a given cube. In
our example, mind is 0 and maxd is 10.
(avgscrmind, . . . , avgscrmaxd, maxscrmind, . . . , maxscrmaxd)
In the ﬁnal step, all scores contained in the records that were build in the
last step must be integrated into a single score that is used to decide whether
the speciﬁc unit is regarded to be an outlier or not. Each individual attribute of
the record describes some evidence. When there is a huge score in the cell itself
(distance 0), this is a clear indicator for an outlier. However, not all outliers
will be visible at this layer. Some will only be visible when looking at more
distant cells. In a denial of service attack, for example, the individual connection
itself does not look suspicious, while the aggregated number of connections from
various sources to a single target might generate an outlier score. In the other
extreme, an outlier score at the apex cell is very unspeciﬁc, so it should not make
every cell at the same time an outlier. So we propose to weight the scores using
their distance to the relevant cell:
score =
maxd

d=mind
1
|d| + 1(avgscrd + maxscrd)
3
Evaluation
In this section, we evaluate the method using data from the network domain. We
use the well-known dataset from the 1998 DARPA IDS evaluation.1 The goal of
the evaluation is to gain insights in the quality of the solution and to explore
the eﬀects of diﬀerent parameters with respect to the detection capability. Here,
the DARPA data is very useful as we have ground truth available.
1 See https://www.ll.mit.edu/ideval/data/1998data.html.

Outlier Detection in Data Streams Using OLAP Cubes
33
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
FPR
TPR
Tm=100
Tm=10
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
FPR
TPR
Ti=16
Ti=0
Fig. 1. Evaluation results.
First, we preprocess the training data by removing all attacks. Thus we start
with training data that only contains normal data, while the test data still con-
tains attacks and normal connections. We model a cube from the data including
diﬀerent metrics like packet count, data volume and speciﬁc metrics e.g. for cap-
turing speciﬁc ﬂags. The system also adds by default another fact that counts the
number of base cells below the current cell in the lattice that have at least one
record. For the dimensions, we ﬁrst use the IP source and destination address.
We build a hierarchy for these dimensions based on the /24, /16, and /8 address
preﬁxes. IP communication will have two additional dimensions that indicate
the layer 4 and layer 7 protocol ﬁelds. In layer 4, this ﬁeld diﬀerentiates between
UDP, TCP, ICMP, or other protocols. In layer 7, the initial target port (well
known port) for a UDP or TCP connection will be used. For ICMP, the command
type is used. These two protocol attributes build a protocol hierarchy.
The DARPA data contains diﬀerent types of attacks. We will focus on DoS
attacks, as they result in typical outlier patterns in the data that we aim to
detect with our method. The main parameters are the threshold used to build the
models (Tm), and the threshold used to build the current time frame’s iceberg
cube (Ti). First, we compare the results for model thresholds Tm = 10 and
Tm = 100; see the left side of Fig. 1. The detection is much better at a larger
model threshold value. This is an indicator that the models build with only a few
sample time frames in the training data are too volatile. They seem to introduce
too speciﬁc normal behavior so that later on diﬀerent behavior is ﬂagged wrongly
as anomalous behavior. For both curves, we set Ti = 0.
Now we look at what happens when we restrict our attention to cells that
fulﬁll a certain iceberg condition. In the right graph of Fig. 1, the detection
capability for DoS attacks is shown with an iceberg threshold of Ti = 0 to Ti = 16
during online operation, meaning we exclude cells with fewer than 16 packets for
a given time frame. Here, the detection capability is nearly unchanged. This is
quite intuitive, as outliers consist of a larger data volume and thus are typically

34
F. Heine
part of the iceberg cube. This means that restricting the computation to iceberg
cubes is a good way to improve performance without losing quality.
Finally, we provide preliminary performance insights for the online phase.
We processed two weeks of network data. The overall throughput is averaged
to 30 MBit/sec. The results where measured on a commodity PC with Intel i7
processor. However, this includes heavy I/O overhead as we are currently logging
many intermediate results. Thus we think it is possible to analyze data streams
with much higher data rates in an online fashion.
4
Related Work
In this section, we compare our work to similar work in the area of outlier
detection (see e.g. [1,2]). In [5], Le and Han describe the SUITS algorithm to
ﬁnd anomalies in OLAP cubes viewed as multi-dimensional time-series data.
However, they focus on unsupervised outlier detection. Their notion of outliers
assumes that time series from child cells normally have the same shape compared
to the parent cell at another scale. Any cell that deviates from this assumption
is an outlier. This is fundamentally diﬀerent to our approach of taking past data
as a normal reference and is only useful for oﬄine processing. For an alternative
approach to unsupervised outlier detection in OLAP cubes, see [6]. A similar
outlier deﬁnition based on unsupervised analysis of an OLAP cube is given
by Sarawagi et al. [11]. For each cell, the outlier score stems from the relative
deviation to an “expected value”, which is computed from related cells in various
ways. They do not have a special processing for the time dimension. The outlier
score is used in an interactive environment to guide the user to interesting cells
of the cube. Also Dunstan et al. [3] describe the idea to present the outliers in
reports using various groupings. This resembles our idea of collecting evidence
from diﬀerent related cells. Palpanas et al. [9] give a method to reconstruct a
cube’s base data based on marginal distributions (i.e. aggregate values) using
maximum entropy estimation. By comparing the reconstructed values and the
actual values, outliers can be identiﬁed.
There is also work that looks at OLAP cubes in data streams. In [4], stream
cubes are deﬁned. However, in order to reduce the number of cells, the authors
use a minimal interest layer. This contradicts our idea to look also at very
detailed levels when there is enough data present. Restricting the cube to a
minimal interest layer might lead to overlooking important hints for outliers. In
Rettig et al. [10], an infrastructure is proposed to ﬁnd outliers in streaming data
from the telco domain in real time. There are some similar ideas, as e.g. the
cell2 usage data has multidimensional structure including a region and an event
type dimension. However, the aggregation is speciﬁc to the application example
in contrast to our generic method. Furthermore, data is compared to older time
steps for outlier detection and not to any model.
In [12], the authors follow an approach using OLAP cubes to detect out-
liers in wireless networks. Apart from a diﬀerent outlier score deﬁnition,
2 A mobile network cell, not a cube cell.

Outlier Detection in Data Streams Using OLAP Cubes
35
the main diﬀerence to our work is that they use domain expertise to prede-
ﬁne those cuboids that are explored in search for outliers. Our method explores
all cuboids dynamically that contain enough data (iceberg).
There is a large body of work about outlier detection in high dimensional
data, see e.g. Chap. 5 of [1]. However, as pointed out in [5], the data model of the
OLAP approach is diﬀerent. Outliers are deﬁned in terms of distance to other
data points in the high-dimensional space, and an important approach to ﬁnding
outliers is to look at the data using diﬀerent projections. This is in contrast to our
method. We look at each metric (i.e. feature) individually, which means that we
use one-dimensional projections in the feature space. However, we use diﬀerent
subsets of the data, with aggregated values of the individual metric values based
on the OLAP dimensions. The only similarity is on a very high level: to ﬁnd
outliers, it is beneﬁcial to look at the data from multiple diﬀerent perspectives.
M¨uller et al. [8], as an example for this line of work, explore subspace clusters
and collect outlier indicators from these subspaces. However, a single outlier is
always a single data row, no collective outliers can be found with this method.
5
Conclusion
This paper presents a new approach to outlier detection in a stream of multidi-
mensional data records. The basic idea is to calculate models speciﬁc to various
groupings of the data, using an oﬄine training phase and normal-only training
data. This makes the approach a one-class anomaly detection method. In the
online detection phase, a cube is built from the data for a series of small time
intervals. The models are used to calculate multiple outlier scores at diﬀerent
cells of the cube.
The outlier scores in these cubes are then related to interesting real-world
entities in order to collect evidence for the outlierness of these entities. If entities
live for multiple time steps, further evidence is collected from subsequent cubes.
The collected scores are condensed into a single outlier score for each entity.
To make the computation eﬃcient and to avoid volatile models, only cube
cells with enough training data will be included in the cube model. During online
operation, a cube cell without a model is ignored. However, due to evidence from
other cells, outliers can still be found. Furthermore, test data cube cells with too
few data can also be ignored, using an iceberg cube approach.
An evaluation based on an network data scenario shows that the method is
able to distinguish between normal data and attacks very well. The evaluation
also shows the eﬀects of diﬀerent parameter settings, with the conclusion that
using appropriate thresholds for the model is indeed important to avoid spurious
models. Restricting the attention to the iceberg cube during processing does not
lower the detection quality.

36
F. Heine
References
1. Aggarwal, C.C.: Outlier Analysis, 1st edn. Springer, New York (2013). doi:10.1007/
978-1-4614-6396-2
2. Chandola, V., Banerjee, A., Kumar, V.: Anomaly detection: a survey. ACM Com-
put. Surv. 41(3), 15:1–15:58 (2009). http://doi.acm.org/10.1145/1541880.1541882
3. Dunstan, N., Despi, I., Watson, C.: Anomalies in multidimensional contexts. WIT
Transa. Inform. Commun. Technol. 42, 173 (2009). http://www.witpress.com/
elibrary/wit-transactions-on-information-and-communication-technologies/42/
19978
4. Han, J., Chen, Y., Dong, G., Pei, J., Wah, B.W., Wang, J., Cai, Y.D.: Stream
cube: an architecture for multi-dimensional analysis of data streams. Distrib. Par-
allel Databases 18(2), 173–197 (2005). http://link.springer.com/article/10.1007/
s10619-005-3296-1
5. Li, X., Han, J.: Mining approximate top-k subspace anomalies in multi-dimensional
time-series data. In: Proceedings of the 33rd International Conference on Very
Large Data Bases, pp. 447–458. VLDB Endowment (2007)
6. Lin, S., Brown, D.E.: Outlier-based Data Association: Combining OLAP and
Data Mining. Department of Systems and Information Engineering University of
Virginia, Charlottesville, VA 22904 (2002). http://web.sys.virginia.edu/ﬁles/tech
papers/2002/sie-020011.pdf
7. Lippmann, R., Fried, D., Graf, I., Haines, J., Kendall, K., McClung, D., Weber,
D., Webster, S., Wyschogrod, D., Cunningham, R., Zissman, M.: Evaluating intru-
sion detection systems: the 1998 DARPA oﬀ-line intrusion detection evaluation.
In: DARPA Information Survivability Conference and Exposition, DISCEX 2000,
Proceedings, vol. 2, pp. 12–26 (2000)
8. M¨uller, E., Assent, I., Iglesias, P., M¨ulle, Y., B¨ohm, K.: Outlier ranking via sub-
space analysis in multiple views of the data. In: 2012 IEEE 12th International
Conference on Data Mining, pp. 529–538. IEEE (2012). http://ieeexplore.ieee.org/
xpls/abs all.jsp?arnumber=6413873
9. Palpanas, T., Koudas, N., Mendelzon, A.: Using datacube aggregates for approx-
imate querying and deviation detection. IEEE Trans. Knowl. Data Eng. 17(11),
1465–1477 (2005)
10. Rettig, L., Khayati, M., Cudr´e-Mauroux, P., Pi´orkowski, M.: Online anomaly detec-
tion over Big Data streams. In: 2015 IEEE International Conference on Big Data
(Big Data), pp. 1113–1122 (2015)
11. Sarawagi, S., Agrawal, R., Megiddo, N.: Discovery-driven exploration of OLAP
data cubes. In: Schek, H.-J., Alonso, G., Saltor, F., Ramos, I. (eds.) EDBT
1998. LNCS, vol. 1377, pp. 168–182. Springer, Heidelberg (1998). doi:10.1007/
BFb0100984
12. Sithirasenan, E., Muthukkumarasamy, V.: Substantiating anomalies in wireless
networks using group outlier scores. J. Softw. 6(4), 678–689 (2011)
13. Thatte, G., Mitra, U., Heidemann, J.: Parametric methods for anomaly detection
in aggregate traﬃc. IEEE/ACM Trans. Networking 19(2), 512–525 (2011)
14. Xin, D., Han, J., Li, X., Wah, B.W.: Star-cubing: Computing iceberg cubes by
top-down and bottom-up integration. In: Proceedings of the 29th International
Conference on Very Large Data Bases, vol. 29, pp. 476–487. VLDB Endowment
(2003)

Balancing Performance and Energy
for Lightweight Data Compression Algorithms
Annett Ungeth¨um(B), Patrick Damme, Johannes Pietrzyk, Alexander Krause,
Dirk Habich, and Wolfgang Lehner
Database Systems Group, Technische Universit¨at Dresden, Dresden, Germany
{annett.ungethuem,patrick.damme,johannes.pietrzyk,alexander.krause,
dirk.habich,wolfgang.lehner}@tu-dresden.de
Abstract. Energy consumption becomes more and more a critical
design factor, whereby performance is still an important requirement.
Thus, a balance between performance and energy has to be established.
To tackle that issue for database systems, we proposed the concept of
work-energy proﬁles. However, generating such proﬁles requires exten-
sive benchmarking. To overcome that, we propose to approximate work-
energy-proﬁles for complex operations based on the proﬁles of low-level
operations in this paper. To show the feasibility of our approach, we use
lightweight data compression algorithms as complex operations, since
compression as well as decompression are heavily used in in-memory
database systems, where data is always managed in a compressed repre-
sentation. Furthermore, we evaluate our approach on a concrete hardware
system.
Keywords: Energy eﬃciency · In-memory databases · Compression
1
Introduction
Database systems constantly adapt to new hardware features to satisfy perfor-
mance demands [7,8,14]. However, these features do not only inﬂuence the per-
formance, but also the energy consumption [3]. As energy consumption becomes
more and more a critical factor [5], a balance between performance and energy
has to be established [12]. To tackle this balancing challenge in a ﬁne-grained way
for in-memory database systems, we proposed the concept of work-energy proﬁles
in a previous work [13]. A work-energy proﬁle exposes a relation between reach-
able performances and the resulting energy-eﬃciency, thereby a proﬁle covers a
wide range of CPU features, such as diﬀerent vector extensions, multithreading,
CPU pinning, and frequency scaling [13]. Thus, these work-energy proﬁles can
be used to determine the most energy-eﬃcient hardware conﬁguration for any
required performance demand. However, generating work-energy proﬁles requires
extensive benchmarking and must be done once for every database operator and
every possible hardware conﬁguration [12,13]. Therefore, the number of neces-
sary benchmark tests can quickly add up to thousands or millions. Even if every
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 37–44, 2017.
DOI: 10.1007/978-3-319-67162-8 5

38
A. Ungeth¨um et al.
test needs only a one second micro benchmark, a full benchmark for a database
system on a speciﬁc hardware system would need hours or days.
Our Contributions and Outline. To overcome this benchmarking overhead,
we present a novel approach to approximate work-energy proﬁles of complex
operations from work-energy proﬁles of low-level operations in this paper. In
detail, our contributions are: (i) We brieﬂy summarize the core concept of our
work-energy proﬁles in Sect. 2. In particular, we state which low-level proﬁles are
necessary for our approximation approach. (ii) To illustrate our approximation
approach, we use a lightweight data compression algorithm and we describe the
concrete algorithm in Sect. 3. (iii) Then, our approximation approach for complex
operations is introduced in Sect. 4. Thereby the approximation is based on linear
combination of work-energy proﬁles of low-level operations. (iv) We evaluate our
approach on a concrete hardware system to show the feasibility in Sect. 5. (v)
We conclude the paper with related work and a summary in Sects. 6 and 7.
2
Work-Energy-Proﬁles
Modern hardware, especially CPUs, oﬀers a lot of features like vectorization [3],
multithreading, or frequency scaling [9]. These features usually have an inﬂuence
on performance as well as energy consumption. However, the mapping between
hardware conﬁgurations – meaning which features to which extend should be
used in which way –, performance, and energy-eﬃciency is not trivial [13]. To
capture these eﬀects for all possible hardware conﬁgurations, we proposed the
concept of work-energy proﬁles in [13].
A work-energy proﬁle is a set of the useful work done during a ﬁxed time
span and the required energy for this work for all possible hardware conﬁgu-
rations [13]. Thus, the work-energy proﬁle for a speciﬁc application has to be
benchmarked on a concrete hardware system [13]. Figure 1 shows three diﬀerent
example work-energy proﬁles, which we measured on a concrete hardware. While
the performance is plotted on the x-axis, the y-axis shows the energy-eﬃciency.
Each dot in this graph represents a speciﬁc hardware conﬁguration. We measured
the performance as work done per second and the energy-eﬃciency as work done
per Joule (work energy quotient – WEQ). Hence, a work-energy-proﬁle is a set
of (performance,WEQ)-tuples, each of them representing one speciﬁc hardware
conﬁguration. As we can see in Fig. 1, diﬀerent hardware conﬁgurations oﬀer the
same performance range with a high variance in the energy-eﬃciency. To balance
performance and energy, the hardware conﬁguration with the highest energy-
eﬃciency within a desired performance range should be used for application
execution. This hardware conﬁguration can be extracted from our work-energy
proﬁle.
Generally, our main focus is on energy-eﬃcient in-memory database sys-
tems [12]. Here, the performance and energy-eﬃciency of a hardware conﬁg-
uration depends on a multitude of factors (e.g., data characteristics and size,
operator types, etc.). Moreover, main memory bandwidth and latency are lim-
iting factors that could cause a non trivially predictable hardware behavior.

Balancing Performance and Energy
39
(a) Read
(b) Write
(c) Lookup
Fig. 1. ODROID-XU3-based work-energy proﬁles for diﬀerent memory access patterns.
To get a deeper understanding and a speciﬁc foundation, we propose to bench-
mark work-energy proﬁles only for ﬁne-grained memory access patterns – data-
base primitives – which are highly utilized in in-memory database systems:
(i) read-primitive, (ii) write-primitive, (iii) lookup-primitive, (iv) compute-
primitive, and (v) processing-primitive, whereas the processing is a combination
of read and compute, since for data processing a set of data has to be read ﬁrst
and then some kind of computation is triggered. The ratio between read and
compute can vary, which we consider in our benchmark.
Figure 1 depicts the resulting work-energy proﬁles for these primitives on
an ARM big.LITTLE hardware system. Concretely, we used an ODROID-XU3,
which consists of a big and a little cluster, each of them featuring 4 cores. Addi-
tionally, the ODROID-XU3 is equipped with on-board power sensors allowing
us to measure the power level of individual core clusters and the main memory
separately. The diﬀerent combinations of cores and their frequencies add up to
roughly 6000 diﬀerent conﬁgurations [13] and each dot in Fig. 1 represents a
speciﬁc conﬁguration. As we can see, the shapes of the work-energy proﬁles of
our primitives are diﬀerent.
3
Example Operation RLE
To describe our approximation approach, the physical operation must be pre-
cisely known. In this paper, we focus on run-length encoding (RLE), since RLE
is a heavily applied compression technique in in-memory database systems [1,6].
RLE tackles uninterrupted sequences of occurrences of the same value, so called
runs. In its compressed format, each run is represented by its value and length.
Therefore, the compressed data is a sequence of such pairs as illustrated in
Fig. 2(a). In addition to the simplicity of RLE, there are two other advantages:
(i) RLE can be easily parallelized by data partitioning, so that the paralleliza-
tion itself does not produce any mentionable communication overhead between
the processing cores and (ii) RLE can be vectorized [2].
Our vectorized implementation consists of four steps as shown in Fig. 2(b)
using the ARM NEON implementation of SIMD. In step one, four copies of

40
A. Ungeth¨um et al.
Fig. 2. The basic idea of RLE compression is to not store every value individually, but
only once followed by the number of sequential elements with the same value.
the current input element are loaded into one 128-bit vector register using the
vdupq n u32() NEON operation. In Step two, the next four input elements are
loaded into a second 128-bit vector register using the vld1q u32() intrinsic. In
step three, these four values are compared in parallel with the current run value
using vceqq u32(). The result is stored in a third vector register. In each 32-
bit element of this vector register, either all bits are set or all bits are not set,
depending on whether the corresponding elements were equal or not. In step four,
from this register we extract a bit mask by ANDing it with the constant vector
(1, 2, 4, 8) using vandq u32(), storing the result to memory using vst1q u32(),
and sequentially ORing the lowest bytes of all four elements. The number of
trailing ones in this bit mask tells us for how many elements the current run
continues. We look up this number in a table created oﬄine and indexed with
the 16 possible masks. If the obtained number is four, then we have not seen the
run’s end yet, and continue at step 2. Otherwise, we have reached the run’s end
and append the run value and run length to the output and continue with step
1 at the next element after the run’s end.
4
Work-Energy-Proﬁle Approximation
Unfortunately, the benchmarking of a work-energy proﬁle for a speciﬁc primi-
tive on the ODROID-XU3 takes about eight hours. To do this for all physical
database operators or even queries would be way too much overhead. However,
the proﬁles are necessary to determine the most energy-eﬃcient hardware con-
ﬁguration for a demanded performance. To tackle this challenge, our idea is to
approximate the work-energy proﬁles of complex operations from the proﬁles of
these low-level primitives. In this section, we want to demonstrate that our idea
is feasible using RLE as an example.
4.1
RLE and Low-Level Operations
As just described, our vectorized RLE algorithm is ﬁxed, but the input data
determines the execution behavior, thereby two extremes arise. One extreme is

Balancing Performance and Energy
41
obtained if there is no run in the input data at all (average run length equals
1). In this case, RLE-compressed data is twice as large as the original data,
because for each array element, a run length of 1 is additionally stored. This
means for the processing, that our vectorized compression algorithm performs
essentially random reads and random writes with a ratio of 1:1. Random reads,
since we always read 4 elements in each iteration, with 3 of them already being
read in the previous iteration, producing overlapping reads. The second extreme
occurs when each element in the uncompressed data array equals a single value
(average run length equals number of elements). In this case, RLE-compressed
data consists of two values, the single value and the number of elements as run
length and these two values are written once by the compression algorithm. Thus,
the read/write ratio approaches 1:0, while the read accesses are still random.
Furthermore, between the reads and writes, there is also the actual compression
which is a computation bound work. If this computation is slower than the I/O
accesses, the memory access pattern is not the bottleneck for the performance
anymore, but the computation itself.
Therefore, three low-level operations are used within our vectorized RLE
compression as well as decompression algorithms: (i) read, (ii) write, and (iii)
compute. Depending on the input data, these operations are composed diﬀer-
ently. While the number of compute operations per read operation is constant,
the ratio between read and write operations changes depending on the average
run length. Hence, the proﬁle for a speciﬁc average run length is within the
spectrum between read-bound and write-bound operations.
4.2
Approximation Using Linear Combination
To approximate work-energy proﬁles, we propose to combine low-level primitives
in a linear way. The new proﬁle, containing (performance, WEQ)-tuples for i
diﬀerent conﬁgurations, is obtained from j low-level proﬁles and the tuples of
the new proﬁle are determined by
(Performance, WEQ)i,new = f −1(
j−1

0
wj ∗f(Performance, WEQ)i,j)
(1)
where wj is a weighting factor which describes the inﬂuence of the proﬁle j.
The adjustment function f modiﬁes the performance- and WEQ-values for the
combination. This is necessary if the limiting factors, i.e. the low-level-proﬁles,
do not scale linearly when the parameters of the operation are changed.
For RLE compression, the only available parameter is the average run length.
This parameter deﬁnes the number of read and write accesses, which deﬁne the
scaling of the work-energy proﬁle, i.e. the adjustment function f. Whereas the
ratio between the read and write accesses deﬁnes the weighting factors wj. The
number of read or write accesses as a function of the run length rl, can be
extracted from the vectorized algorithm presented in Sect. 3. As shown in Eqs. 2
and 3, we denote to the number of reads and writes as countreads and countwrites
respectively. A sequence of run lengths is described as RL = [rl0, rl1, rl|RL|−1]

42
A. Ungeth¨um et al.
with countruns = |RL| runs, and k is the vector width. For a constant run length,
countruns can be computed by countelements/rl.
countreads =
|RL|−1

i=0
(2 + ⌊rli −1
k
⌋)
(2)
countwrites = 2 ∗countruns
(3)
Both functions are rational. Hence, our function f must be rational, too. Since
there are no polynoms or exponential parts in either countreads or countwrites,
it is safe to use the most simple rational function f(Performance, WEQ) =
(1/P erformance, 1/W EQ) for Eq. 1.
The ratio between the read and write operations, and therefore the weighting
factors wj, follow from the Eqs. 2 and 3 as well:
w0 : w1 = 2 ∗countruns :
|RL|−1

i=0
(2 + ⌊rli −1
k
⌋).
5
Evaluation
To validate the results of our proﬁle approximation approach, we also bench-
marked the proﬁles on the ODROID-XU3. For comparing the quality of an
approximated proﬁle to a benchmarked proﬁle, we ﬁltered the conﬁgurations
which are part of the Pareto front of the measured and the approximated pro-
ﬁle. Then, we divided the proﬁle into ten performance ranges and compared the
approximated and measured conﬁgurations which are the closest to the middle
of these performance ranges. As a result, there are two measures to quantify
the quality of the approximated proﬁle: (1) the chance that the approximated
conﬁguration actually is in the performance range, in which we expect it to be,
and (2) the mean deviation of the WEQ from the measured conﬁguration in the
middle of this performance range.
Figure 3 shows the results for our RLE compression algorithm applied on
synthetic data containing values with an average run length of 45. The ﬁgure
shows that the match of the approximation and the benchmarked proﬁle is not
exact but close to the optimal solution. To quantify the diﬀerence, we calculated
the two measures as described above: (1) The chance that a conﬁguration, which
we expect to be close to the middle of a speciﬁed performance range, is actually
in this performance range, is 100% in our example. This means, that all ten
conﬁgurations, we ﬁltered for the ten performance ranges were actually within
a 10% radius of this performance range. (2) The mean deviation of the WEQ
from the measured conﬁguration was only 3%.
We conducted the evaluation on diﬀerent data sets as well as on diﬀerent
hardware systems. In almost all cases, we observed a similar behavior, so that
we are able to conclude that our approximation approach is well-suited for the
considered operations of RLE compression and decompression.

Balancing Performance and Energy
43
Fig. 3. A benchmarked work-energy proﬁle for RLE compression on the ODROID-XU3
with average run length of 45. The approximated optimal conﬁgurations are highlighted
in orange, the actual optimum is highlighted in green. (Color Figure Online)
6
Related Work
Lossless compression techniques play an important role in in-memory database
systems [1,6]. They reduce not only the amount of needed space, but also the
time spent on i/o instructions. Thus, they have already been investigated for
query performance [1]. Further works on vectorized compression techniques also
focus on performance [10], but not on energy eﬃciency. An extensive experimen-
tal evaluation on lightweight compression techniques has been done by Damme
et al. [2]. However, to the best of our knowledge, none of these works explic-
itly regards energy eﬃciency. Vice versa, the works regarding energy-eﬃciency
(1) focus on query execution, usually evaluated by running a TPC benchmark,
rather than on compression itself [11], and (2) treat the performance-energy-
tradeoﬀas a binary decision between energy-eﬃciency and performance [15]. In
general, there are analytical models to estimate and reduce the energy consump-
tion [15] and benchmark-based approaches [4]. The latter has only been applied
for homogeneous systems and analytical models become more complex the more
complex the hardware becomes. Regarding the ever-growing world of heteroge-
neous hardware, we developed an approach which is mostly benchmark based
and applicable to various systems.
7
Conclusion and Outlook
As energy consumption becomes more and more a critical design factor, a balance
between performance and energy has to be established. To tackle this challenge
for in-memory database systems, we proposed the concept of work-energy pro-
ﬁles in [13]. A work-energy proﬁle is a set of the useful work done during a ﬁxed
time span and the required energy for this work for all possible hardware con-
ﬁgurations [13]. In this paper, we proposed to approximate work-energy-proﬁles
for complex operations based on the work-energy proﬁles of low-level operations

44
A. Ungeth¨um et al.
and demonstrated the feasibility on a concrete example. In future work, we are
going to generalize this approximation approach to other physical database oper-
ators. This is essential to achieve our main goal of integrating these work-energy
proﬁles into query optimization.
Acknowledgments. This work is partly funded within the DFG-CRC 912 (HAEC)
and by the DFG-project LE-1416/26.
References
1. Abadi, D.J., et al.: Integrating compression and execution in column-oriented data-
base systems. In: SIGMOD (2006)
2. Damme, P., et al.: Lightweight data compression algorithms: an experimental sur-
vey (experiments and analyses). In: EDBT (2017)
3. Firasta, N., et al.: Intel AVX: new frontiers in performance improvements and
energy eﬃciency. Intel White Paper (2008)
4. G¨otz, S., et al.: Energy-eﬃcient databases using sweet spot frequencies. In: UCC
2014 (2014)
5. Harizopoulos, S., et al.: Energy eﬃciency: the new holy grail of data management
systems research. In: CIDR (2009)
6. Hildebrandt, J., Habich, D., Damme, P., Lehner, W.: Compression-aware in-
memory query processing: vision, system design and beyond. In: Blanas, S.,
Bordawekar, R., Lahiri, T., Levandoski, J., Pavlo, A. (eds.) IMDM/ADMS -
2016. LNCS, vol. 10195, pp. 40–56. Springer, Cham (2017). doi:10.1007/
978-3-319-56111-0 3
7. Karnagel, T., et al.: Adaptive work placement for query processing on heteroge-
neous computing resources. PVLDB 10(7), 733–744 (2017)
8. Kissinger, T., et al.: ERIS: a numa-aware in-memory storage engine for analytical
workload. In: ADMS@VLDB, pp. 74–85 (2014)
9. Le Sueur, E., et al.: Dynamic voltage and frequency scaling: the laws of diminishing
returns. In: Proceedings of the 2010 International Conference on Power Aware
Computing and Systems, pp. 1–8 (2010)
10. Lemire, D., Boytsov, L.: Decoding billions of integers per second through vector-
ization. Softw. Pract. Exper. 45(1) (2015)
11. M¨uhlbauer,
T.,
R¨odiger,
W.,
Seilbeck,
R.,
Kemper,
A.,
Neumann,
T.:
Heterogeneity-conscious parallel query execution: getting a better mileage while
driving faster! In: DaMoN@SIGMOD (2014)
12. Ungeth¨um, A., et al.: Energy elasticity on heterogeneous hardware using adaptive
resource reconﬁguration LIVE. In: SIGMOD, pp. 2173–2176 (2016)
13. Ungeth¨um, A., Kissinger, T., Habich, D., Lehner, W.: Work-energy proﬁles: general
approach and in-memory database application. In: Nambiar, R., Poess, M. (eds.)
TPCTC 2016. LNCS, vol. 10080, pp. 142–158. Springer, Cham (2017). doi:10.1007/
978-3-319-54334-5 10
14. Willhalm, T., et al.: Simd-scan: ultra fast in-memory table scan using on-chip
vector processing units. PVLDB 2(1), 385–394 (2009)
15. Xu, Z., et al.: Dynamic energy estimation of query plans in database systems.
In: 2013 IEEE 33rd International Conference on Distributed Computing Systems
(ICDCS), pp. 83–92. IEEE (2013)

Asynchronous Graph Pattern Matching
on Multiprocessor Systems
Alexander Krause(B), Annett Ungeth¨um, Thomas Kissinger, Dirk Habich,
and Wolfgang Lehner
Database Systems Group, Technische Universit¨at Dresden, Dresden, Germany
{Alexander.Krause,Annett.Ungethuem,Thomas.Kissinger,Dirk.Habich,
Wolfgang.Lehner}@tu-dresden.de
Abstract. Pattern matching on large graphs is the foundation for a vari-
ety of application domains. Strict latency requirements and continuously
increasing graph sizes demand the usage of highly parallel in-memory
graph processing engines that need to consider non-uniform memory
access (NUMA) and concurrency issues to scale up on modern multi-
processor systems. To tackle these aspects, graph partitioning becomes
increasingly important. Hence, we present a technique to process graph
pattern matching on NUMA systems in this paper. As a scalable pattern
matching processing infrastructure, we leverage a data-oriented archi-
tecture that preserves data locality and minimizes concurrency-related
bottlenecks on NUMA systems. We show in detail, how graph pattern
matching can be asynchronously processed on a multiprocessor system.
1
Introduction
Recognizing comprehensive patterns on large graph-structured data is a pre-
requisite for a variety of application domains such as biomolecular engineer-
ing [11], scientiﬁc computing [17], or social network analytics [12]. Due to the
ever-growing size and complexity of the patterns and underlying graphs, pattern
matching algorithms need to leverage an increasing amount of available compute
resources in parallel to deliver results with an acceptable latency. Since modern
hardware systems feature main memory capacities of several terabytes, state-of-
the-art graph processing systems (e.g., Ligra [16] or Galois [10]) store and process
graphs entirely in main memory, which signiﬁcantly improves scalability, because
hardware threads are not limited by disk accesses anymore. To reach such high
memory capacities and to provide enough bandwidth for the compute cores,
modern servers contain an increasing number of memory domains resulting in a
non-uniform memory access (NUMA). To further scale up on those NUMA sys-
tems, pattern matching on graphs needs to carefully consider issues such as the
increased latency and the decreased bandwidth when accessing remote memory
domains, as well as the limited scalability of synchronization primitives such as
atomic instructions [21].
The widely employed bulk synchronous parallel (BSP) processing model [19],
which is often used for graph processing, does not naturally align with pattern
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 45–53, 2017.
DOI: 10.1007/978-3-319-67162-8 6

46
A. Krause et al.
matching algorithms [3]. That is because a high number of intermediate results
is generated and need to materialized and transferred within the communication
phase. Therefore we argue for an asynchronous processing model that neither
requires a full materialization nor limits the communication to a distinct global
phase. For eﬃcient pattern matching on a single NUMA system, we employ a
ﬁne-grained data-oriented architecture (DORA) in this paper, which turned out
to exhibit a superior scalability behavior on large-scale NUMA systems as shown
by Pandis et al. [13] and Kissinger et al. [6]. This architecture is characterized
by implicitly partitioning data into small partitions that are explicitly pinned to
a NUMA node to preserve a local memory access.
Contributions. Following to a discussion of the foundations of graph pattern
matching in Sect. 2, the contributions of the paper are as follows:
(1) We adapt the data-oriented architecture for scale-up graph pattern matching
and identify the partitioning strategy as well as the design of the routing table
as the most crucial components within such an infrastructure (Sect. 3).
(2) We describe an asynchronous query processing model for graph pattern
matching and present the individual operators a query is composed of. Based
on the operator characteristics, we identify redundancy in terms of partition-
ing as an additional critical issue for our approach (Sect. 4).
(3) We thoroughly evaluate our graph pattern matching approach on multiple
graph datasets and queries with regard to scalability on NUMA systems.
Within our evaluation, we focus on diﬀerent options for the partitioning
strategy, routing table, and redundancy as our key challenges (Sect. 5).
Finally, we discuss the related work in Sect. 6 and conclude the paper in Sect. 7
including promising directions for future work.
2
Foundations of Graph Pattern Matching
Within this paper, we focus on pattern matching for edge-labeled multigraphs
as a general and widely employed graph data model [12,14]. An edge-labeled
multigraph G(V, E, ρ, Σ, λ) consists of a set of vertices V , a set of edges E,
an incidence function ρ : E →V × V , and a labeling function λ : E →Σ that
assigns a label to each edge, according to which edge-labeled multigraphs allow
any number of labeled edges between a pair of vertices. A prominent example
for edge-labeled multigraphs is RDF [2].
Pattern matching is a declarative topology-based querying mechanism where
the query is given as a graph-shaped pattern and the result is a set of matching
subgraphs [18]. For instance, the query pattern depicted in Fig. 1 searches for a
vertex V1, that has two outgoing edges targeting V2 and V3. Additionally, the
query pattern seeks a fourth vertex V4 which also has two outgoing edges to the
same target vertices. The query pattern forms a rectangle with four vertices and
four edges of which we search for all matching subgraphs in a graph. A well-
studied mechanism for expressing such query patterns are conjunctive queries

Asynchronous Graph Pattern Matching on Multiprocessor Systems
47
Fig. 1. Scalable graph pattern matching based on a data-oriented architecture [6,13].
(CQ) [20], which decompose the pattern into a set of edge predicates each con-
sisting of a pair of vertices and an edge label. Assuming a wildcard label, the
exemplary query pattern in Fig. 1 can be decomposed into the conjunctive query
{(V1
V1
V1, ∗, V2), (V1
V1
V1, ∗, V3), (V4
V4
V4, ∗, V3), (V4
V4
V4, ∗, V2)}, where the bold vertices represent
the source vertex of an edge. These four edge predicate requests form a sequence,
that is processed by starting at each vertex in the data graph, because the query
pattern does not specify a speciﬁc starting vertex.
3
Scalable Graph Pattern Matching Architecture
In this section, we brieﬂy describe our target architecture and refer to the
extended version of our paper [7] for more details. Figure 1 illustrates a NUMA
system with N sockets which can run multiple worker threads concurrently, based
on the underlying hardware. A graph of the form described in Sect. 2 can be dis-
tributed among the main memory regions, which are attached to one of the
sockets. The distribution of the graph among these memory regions inherently
demands graph partitioning and an appropriate partitioning strategy.
Partitioning Strategy. However, partitioning a graph will most likely lead to
edges, which span over multiple partitions, like the edges A →B and D →B
on the left hand side of Fig. 1. For instance, if vertex A is considered as a
potential match for a query pattern, the system needs to lookup vertex B
in another partition. Moving to another partition requires that the complete
matching state needs to be transferred to another worker, which requires com-
municational eﬀorts between the two responsible workers. Hence, the selection of
the partitioning strategy is crucial when adapting the data-oriented architecture
for graph pattern matching, because locality in terms of the graph topology is
important [8].
Routing Table. Because one partition can not always contain all the necessary
information for one query, it is inevitable to communicate intermediate results
between workers. The communication is handled by a high-throughput message
passing layer, which hides the latency of the communication network, as depicted

48
A. Krause et al.
in Fig. 1. The system stores the target socket and partition information in a
crucial data structure, the routing table. The routing table determines the target
partition as well as the target NUMA node per vertex. Thus, the routing table
needs to be carefully designed, because real world graphs often feature millions
of vertices and billions of edges.
Since routing table and partitioning strategy depend on each other, we con-
sider the following three design options for our discussion and evaluation:
Compute Design. This design uses a hash function to calculate the target
partition of a vertex based on its identiﬁer on-the-ﬂy and stores no data at
all. Nevertheless, due to the simplicity of the routing table, the partitioning
strategy can not take any topology-based locality information into account.
Lookup Design. The lookup design consists of a hash map, which stores a
precomputed graph partitioning, i.e. one partition entry per vertex in the
graph, thus this design doubles the memory footprint, since the graph is
stored once as graph data and once in the routing table as topology data. As
partitioning strategy, we use the well known multilevel k-Way partitioning to
create a disjoint set of partitions. This heuristical approach creates partitions
with high locality and tries to minimize the edge cut of the partitioning [5].
Hybrid Design. We created this design to combine the advantages of the two
previous approaches, i.e. a small and locality preserving routing table. To
enable this combination, we employ a dictionary as auxiliary data structure
that maps virtual vertex ids to the original vertex ids of the locality aware
graph partitioning. The dictionary is only used for converting the vertex ids
of ﬁnal query results. This range-based routing table maps dense ranges of
virtual ids to the respective partition and has very low memory footprint such
that the routing table easily ﬁts into the cache of the multiprocessors.
4
Graph Pattern Matching Processing Model
The architecture introduced in Sect. 3 needs speciﬁc operators for pattern match-
ing on NUMA systems. We identiﬁed three logical operators, which are necessary
to model conjunctive queries as described in Sect. 2:
Unbound Operator. The unbound operator performs a parallel vertex scan
over all partitions and returns edges matching the speciﬁed label. The
unbound operator is always the ﬁrst operator in the pattern matching process.
Vertex-Bound Operator. The vertex-bound operator takes an intermediate
matching result as input and tries to match a new vertex in the query pattern.
Edge-Bound Operator. The edge-bound operator ensures the existence of
additional edge predicates between vertices which are matching candidates
for certain vertices of the query pattern. It performs a data lookup with a
given source and target vertex as well as a given edge label. If the lookup
fails, both vertices are eliminated from the matching candidates. Otherwise
the matching state is passed to the next operator or is returned as ﬁnal result.

Asynchronous Graph Pattern Matching on Multiprocessor Systems
49
To actually compose a query execution plan (QEP), the query compiler sequen-
tially iterates over the edge predicates of the conjunctive query. For each edge
predicate, the query compiler determines whether source and/or target vertex
are bound and selects the appropriate operator for the respective edge predicate.
For the example query pattern in Fig. 1, the resulting operator assignments of
the QEP are shown in Fig. 2(c).
Each operator is asynchronously processed in parallel and generates new
messages that invoke the next operator in the QEP. Hence, diﬀerent worker
threads can process diﬀerent operators of the same query at the same point in
time. Based on the operator and its parametrization, we distinguish two ways of
addressing a message that are related to the routing table:
Unicast. A unicast addresses a single graph partition and requires that the
source vertex is known respectively bound by the operator. This case occurs
for the vertex-bound operator if the source vertex is bound and for the edge-
bound operator.
Broadcast. A broadcast targets all partitions of a graph, which increases
the pressure on the message passing layer and requires the message to be
processed on all graph partitions and thus, negatively aﬀects the scalability.
Additionally, vertex-bound operators that bound the target vertex require a
broadcast.
Broadcasts generated by vertex-bound operators signiﬁcantly hurt the scalability
of our approach. The cause of this problem is inherently given by the data-
oriented architecture, because a graph can either be partitioned by the source
or the target vertex of the edges. Hence, we identify redundancy in terms of
partitioning as an additional challenge for our approach. To reduce the need for
broadcasts to the initial unbound operator, we need to redundantly store the
graph partitioned by source vertex and partitioned by target vertex. However,
the need for redundancy depends on the query pattern as well as on the graph
itself as we will show within our evaluation.
Fig. 2. Query patterns and example operators for the query from Fig. 1.

50
A. Krause et al.
5
Evaluation
In this section, we brieﬂy describe our ﬁndings and refer to our extended version
for an in depth explanation of the individual results [7]. We used a bibliographical
like graph, which we call biblio for the remainder of this paper. The biblio graph
was generated using the graph benchmark framework gMark [1] and has 546 k
vertices, 780 k edges and an average out degree of 2.85 per vertex. Our NUMA
system consists of four sockets equipped with an Intel Xeon E7-4830 CPU and a
total of 128 GB of main memory. We deﬁned two queries which are shaped like
shown in Figs. 2(a) and (b) and ran them on the biblio graph.
Routing Table and Partitioning Strategy. Based on Fig. 3 we examine the
inﬂeunce of the routing table on the query performance. The ﬁgure shows the
query runtime for the V query on the biblio graph, which we scaled up from
factor 1 to factor 32. On the left hand side, we show the sole inﬂuence of
the routing table and on the right hand side of the ﬁgure we show the query
runtime per routing table design, if redundancy is used. In Fig. 3(a) we can see
that our hybrid design marginally outperforms the memory intensive lookup
design with a k-Way partitioning. The compute design and the lookup design
which uses a hash function perform equally in terms of query performance.
The advantage of our hybrid design and the k-Way based lookup design stems
from the better graph partitioning algorithm, because neighborhood locality
of adjacent vertices is considered. Our experiments showed, that the compute
design results in the lowest time spent in the routing table per worker, which
is not surprising. However, our hybrid design almost reaches the same routing
table time due its the small size.
Avoiding Broadcasts with Redundancy. In Sect. 4 we mentioned that
broadcasts hurt the scalability of a system. This issue is depicted in Fig. 4.
The ﬁgure shows the scalability of our systems for both query types from
Fig. 2. On the right hand side, we see that the Quad query suﬀers more from
broadcasts. The reason is, that many tuples are matched for predicate 2 (c.f.
Fig. 2(b)), which leads to a high number of broadcasts during the evaluation
of predicate 3. For the V query on the left hand side of the Figure, we can
see that the employment of redundancy still decreases the query runtime, but
not as much as for the Quad query, because the broadcasting predicates are
not dominant for this speciﬁc query instance.
Combining Redundancy and Routing Table Optimizations. Aside from
testing both optimization techniques individually, we combined them to exam-
ine their synergy. In Fig. 3(b), we demonstrate the query performance of the V
query on the biblio graph, again scaled up to factor 32. By adding redundancy to
the query execution, all routing table designs greatly beneﬁt in terms of query
performance. However, we can now see a bigger advantage of our hybrid design,
compared to the lookup k-Way design.

Asynchronous Graph Pattern Matching on Multiprocessor Systems
51
(a) Query runtime without redundancy
(b) Query runtime with redundancy
Fig. 3. V query on the biblio graph using diﬀerent scale factors.
Fig. 4. Impact of redundancy, both queries on the biblio graph.
6
Related Work
Graph analytics is a widely studied ﬁeld, as the survey from McCune et al. [9]
shows. Many systems leverage the increased compute performance of scale-up or
scale-out systems to compute graph metrics like PageRank and the counting of
triangles [15], Single Source Shortest Path [15] or Connected Components [15].
Many of the available systems are inspired by the Bulk Synchrones Processing
Model [19], which features local processing phases which are synchronized by a
global superstep. A general implementation is the Gather-Apply-Scatter para-
digm, as described in [4]. Despite working on NUMA systems, these processing
engines are globally synchronized and lack the scalability of a lock-free archi-
tecture. We improve this issue by leveraging a high throughput message passing
layer for asynchronous communication between the worker threads. However,
in contrast to the systems mentioned above, we are calculating graph pattern
matching and not graph metrics, like for instance GraphLab which is the only
asynchronous graph processing engine according to [9].
7
Conclusions
In this paper, we showed that the performance of graph pattern matching
on a multiprocessor system is determined by the communication behavior,

52
A. Krause et al.
the employed routing table design and the partitioning strategy. Our Hybrid
routing table design implementation allows the system to leverage both the
advantages from a Compute design and a Lookup design. Because of an inter-
mediate step, the underlying graph partitioning algorithm is interchangable and
can thus be adapted to speciﬁc partitioning requirements. Furthermore we could
show that avoiding broadcasts is equally important. This issue was mitigated
by introducing redundancy in the system. The added memory footprint can be
mitigated with the positive inﬂuence of our Hybrid design, since it scales directly
with the number of data partitions in the system.
Acknowledgments. This work is partly funded within the DFG-CRC 912 (HAEC).
References
1. Bagan, G., et al.: Generating ﬂexible workloads for graph databases. PVLDB 9,
1457–1460 (2016)
2. Decker, S., et al.: The semantic web: the roles of xml and rdf. IEEE 4, 63–73 (2000)
3. Fard, A., et al.: A distributed vertex-centric approach for pattern matching in
massive graphs. In: 2013 IEEE International Conference on Big Data (Oct 2013)
4. Gonzalez, J.E., et al.: Powergraph: Distributed graph-parallel computation on nat-
ural graphs. In: OSDI (2012)
5. Karypis, G., et al.: A fast and high quality multilevel scheme for partitioning
irregular graphs. SIAM J. Sci. Comput. 20(1), 359–392 (1998)
6. Kissinger, T., et al.: ERIS: A numa-aware in-memory storage engine for analytical
workload. In: ADMS (2014)
7. Krause, A., et al.: Asynchronous graph pattern matching on multiprocessor systems
(2017). https://arxiv.org/abs/1706.03968
8. Krause, A., et al.: Partitioning Strategy Selection for In-Memory Graph Pat-
tern Matching on Multiprocessor Systems (2017). http://wwwdb.inf.tu-dresden.
de/europar2017/. Accepted at Euro-Par 2017
9. McCune, R.R., et al.: Thinking like a vertex: A survey of vertex-centric frameworks
for large-scale distributed graph processing. ACM Comput. Surv. 48(2), 25:1–25:39
(2015)
10. Nguyen, D., et al.: A lightweight infrastructure for graph analytics. In: SIGOPS
(2013)
11. Ogata, H., et al.: A heuristic graph comparison algorithm and its application
to detect functionally related enzyme clusters. Nucleic Acids Res. 28, 4021–4028
(2000)
12. Otte, E., et al.: Social network analysis: a powerful strategy, also for the information
sciences. J. Inf. Sci. 28, 441–453 (2002)
13. Pandis, I., et al.: Data-oriented transaction execution. PVLDB 2, 928–939 (2010)
14. Pandit, S., et al.: Netprobe: A fast and scalable system for fraud detection in online
auction networks. In: WWW (2007)
15. Seo, J., et al.: Distributed socialite: A datalog-based language for large-scale graph
analysis. PVLDB 6, 1906–1917 (2013)
16. Shun, J., et al.: Ligra: a lightweight graph processing framework for shared memory.
IN: SIGPLAN (2013)
17. Tas, M.K., et al.: Greed is good: Optimistic algorithms for bipartite-graph partial
coloring on multicore architectures. CoRR (2017)

Asynchronous Graph Pattern Matching on Multiprocessor Systems
53
18. Tran, T., et al.: Top-k exploration of query candidates for eﬃcient keyword search
on graph-shaped (RDF) data. In: ICDE (2009)
19. Valiant, L.G.: A bridging model for parallel computation. Commun. ACM 33,
103–111 (1990)
20. Wood, P.T.: Query languages for graph databases. SIGMOD 41, 50–60 (2012)
21. Yasui, Y., et al.: Numa-aware scalable graph traversal on SGI UV systems. IN:
HPGP (2016)

Predicting Access to Persistent Objects
Through Static Code Analysis
Rizkallah Touma1(B), Anna Queralt1, Toni Cortes1,2, and Mar´ıa S. P´erez3
1 Barcelona Supercomputing Center (BSC), Barcelona, Spain
{rizk.touma,anna.queralt,toni.cortes}@bsc.es
2 Universitat Polit`ecnica de Catalunya (UPC), Barcelona, Spain
3 Ontology Engineering Group (OEG), Universidad Polit´ecnica de Madrid,
Madrid, Spain
mperez@fi.upm.es
Abstract. In this paper, we present a fully-automatic, high-accuracy
approach to predict access to persistent objects through static code
analysis of object-oriented applications. The most widely-used previ-
ous technique uses a simple heuristic to make the predictions while
approaches that oﬀer higher accuracy are based on monitoring appli-
cation execution. These approaches add a non-negligible overhead to the
application’s execution time and/or consume a considerable amount of
memory. By contrast, we demonstrate in our experimental study that
our proposed approach oﬀers better accuracy than the most common
technique used to predict access to persistent objects, and makes the
predictions farther in advance, without performing any analysis during
application execution.
1
Introduction
Persistent Object Stores (POSs), such as object-oriented databases and object-
relational mapping systems (e.g. Hibernate, DataNucleus, dataClay [11]), are
storage systems that expose persistent data in the form of objects and relations
between these objects. This structure is rich in semantics ideal for predicting
access to persistent data [8] and has invited a signiﬁcant amount of research
due to the importance of these predictions in areas such as prefetching, cache
replacement and dynamic data placement.
In this paper, we present a fully-automatic approach that predicts access to
persistent objects through static code analysis of object-oriented applications.
Our approach takes advantage of the symmetry between application objects and
POS objects to perform the prediction process before the application is executed
This work has been supported by the European Union’s Horizon 2020 research and
innovation program (grant H2020-MSCA-ITN-2014-642963), the Spanish Govern-
ment (grant SEV2015-0493 of the Severo Ochoa Program), the Spanish Ministry
of Science and Innovation (contract TIN2015-65316) and Generalitat de Catalunya
(contract 2014-SGR-1051). The authors would also like to thank Alex Barcel´o for
his feedback on the formalization included in this paper.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 54–62, 2017.
DOI: 10.1007/978-3-319-67162-8 7

Predicting Access to Persistent Objects Through Static Code Analysis
55
and does not cause any overhead. In our experimental study, we demonstrate the
viability of the proposed approach by answering the following research questions:
– RQ1: What is the accuracy of the proposed approach?
– RQ2: How much in advance can the approach make the predictions?
We also compare our approach with the Referenced-Objects Predictor (see
Sect. 2) and the experimental results show that our approach oﬀers better accu-
racy in all of the studied benchmarks, with reductions in false positives of as
much as 30% in some cases. Moreover, our approach predicts accesses farther in
advance giving additional time for the predictions to be utilized.
2
Related Work
The simplest technique to predict access to persistent objects is the Referenced-
Objects Predictor (ROP), which is based on the following heuristic: each time
an object is accessed, all the objects referenced from it are likely to be accessed
as well [8]. In spite of its simplicity, this predictor is widely used in commercial
POSs because it does not involve a complex and costly prediction process.
More complex approaches have been based on analysis done during appli-
cation execution using various techniques such as Markov-Chains [9], traversal
proﬁling [5,6] and the Lempel-Ziv compression algorithm [2]. The approach pre-
sented in [4] introduces type-level prediction based on the argument that patterns
do not necessarily exist between individual objects but rather between object
types. Type-level access prediction can capture patterns even when diﬀerent
objects of the same type are accessed and does not store information for each
individual object, which reduces the amount of used memory. In general, the
main drawbacks of these approaches are the overhead they add to application
execution time and the fact that they are based on a most-common case scenario
which might lead to erroneous predictions in some cases.
The approach brought forward in this paper combines the idea of application
type graphs, presented in [6], with type-level prediction. The work in [6] proceeds
by creating an object graph and generating object-level access hints based on
proﬁling done during application execution. On the other hand, our approach
generates type-level access hints based on static code analysis, thus beneﬁting
from the advantages of type-level prediction while avoiding the issues stemming
from performing the process during application execution.
Previous approaches that use static analysis to predict access to persistent
data have targeted speciﬁc types of data structures such as linked data structures
[1,3,7], recursive data structures [10,13] or matrices [12]. To the best of our
knowledge, our work is the ﬁrst that predicts access to persistent objects of any
type prior to application execution.
3
Running Example
Figure 1 shows the partial implementation of a bank management system,
all classes represent persistent types except the BankManagement class. The

56
R. Touma et al.
Fig. 1. Example object-oriented application code
method setAllTransCustomers() updates the customers of all the transactions
to a new customer restricting such updates to customers of the same company.
In order to do so, it retrieves and iterates through all the Transaction objects
and then navigates to the referenced Account and Customer until reaching the
Company of each transaction and compares it with the new customer’s company.
For this example, ROP would predict that each time a Transaction object is
accessed, the referenced Transaction Type, Account and Employee objects will be
accessed as well. However, the method setAllTransCustomers() does not access
the predicted Transaction Type and Employee objects but needs the Customer
and Company objects which are not predicted.
On the other hand, using static code analysis we can see that when setAll-
TransCustomers() is executed it accesses: (1) the object BankManagement. man-
ager, (2) all the Transaction objects, and (3) the Account, Customer and Com-
pany objects of each transaction by calling getAccount() and setCustomer(). We
can also see that getAccount() might access the Department of the Employee of
a Transaction, depending on which branch of the conditional statement starting
on line 19 is executed. Using this information, we can automatically generate
method-speciﬁc access hints that predict which objects are going to be accessed.
4
Proposed Approach
Assuming we have an object-oriented application that uses a POS, we deﬁne T as
the set of types of the application and PT ⊆T as its subset of persistent types.
Furthermore, ∀t ∈T we deﬁne (1) Ft : the set of persistent member ﬁelds of t
such that ∀f ∈Ft : type(f) ∈PT, (2) Mt : the set of member methods of t.

Predicting Access to Persistent Objects Through Static Code Analysis
57
4.1
Type Graphs
Application Type Graph. The type graph of an application, as deﬁned in
[6], is a directed graph GT = (T, A) where:
– T is the set of types deﬁned by the application.
– A is a function T × F →PT × {single, collection} representing a set of
associations between types. Given types t and t′ and ﬁeld f, if A(t, f) →(t′, c)
then there is an association from t to t′ represented by f ∈Ft where type(f) =
t′ with cardinality c indicating whether the association is single or collection.
Example. Figure 2 shows the type graph of the application from Fig. 1. Some
of the associations of this type graph are: (1) A(Bank Management, trans) →
(Transaction, collection), (2) A(Transaction, acc) →(Account, single).
Fig. 2. Type graph GT of the applica-
tion from Fig. 1. Solid lines represent
single associations and dashed lines
represent collection associations.
Fig. 3. Method type graph Gm of the
method getAccount() from Fig. 1. Navi-
gations highlighted in gray are branch-
dependent.
Method Type Graph.
We construct the type graph Gm of a method
m ∈Mt from the associations that are navigated by the method’s instructions.
A navigation of an association t ⇁f t′ is triggered when an instruction accesses
a ﬁeld f in an object of type t (navigation source) to navigate to an object of
type t′ (navigation target) such that A(t, f) →(t′, c). A navigation of a col-
lection association has multiple target objects corresponding to the collection’s
elements.
Example. Figure 3 shows Gm of method getAccount() from Fig. 1.
Augmented Method Type Graph. We construct the augmented method
type graph AGm of a method m ∈Mt by adding association navigations that
are caused by the invocation of another method m′ ∈Mt′ to Gm as follows:
– The type graph of the invoked method Gm′ is added to Gm through the
navigation t ⇁f t′ that caused the invocation of m′.

58
R. Touma et al.
– The association navigations that are triggered by passing a persistent object
as a parameter to m′ are added directly to Gm.
Example. Figure 4 shows the augmented method type graph AGm of method
setAllTransCustomers().
Note
that
the
navigations
BankManagement
⇁manager
Customer
⇁comp
Company are triggered by passing the persis-
tent object Bank Management.manager as a parameter to the method setCus-
tomer(newCust).
Fig. 4. Augmented method type graph AGm of setAllTransCustomers() from Fig. 1.
4.2
Access Hints
We traverse a method’s augmented graph and generate its set of access hints as:
AHm =

ah | ah = f1.f2. . . . .fn where ti ⇁fi ti+1 ∈AGM : 1 ≤i < n

Each access hint ah ∈AHm corresponds to a sequence of association navigations
in AGm and indicates that the navigations’ target object(s) is/are accessed.
Example. The augmented method type graph AGm of Fig. 4 results in the
following set of access hints for method setAllTransCustomers() (hints starting
with the collection trans predict that all its elements will be accessed):
AHm = {trans.type, trans.emp, trans.acc.cust.comp, manager.comp}
4.3
Nondeterministic Application Behavior
Branch-Dependent Navigations. They are navigations that might not be
triggered depending on a method’s branching behavior and which of its branches
are executed. Including branch-dependent navigations in Gm might result in false
positives if the branch from which the navigation is triggered is not executed
while excluding them might result in a miss if the branch is indeed executed
(both strategies are evaluated in Sect. 5). We divide them in two types:

Predicting Access to Persistent Objects Through Static Code Analysis
59
– Navigations not triggered inside all the branches of a conditional statement.
– Navigations of collection associations not triggered in all the iterations of a
loop statement due to branching instructions (continue, break, return).
Example. In Fig. 3, the navigations Transaction ⇁emp Employee ⇁dept
Department, highlighted in gray, are branch dependent (they are only trig-
gered in one of the conditional statement’s branches) while the navigation
Transaction ⇁emp Employee is not (it is triggered inside both branches).
Overridden Methods. A method m ∈Mt might have overridden methods
OMm in the subtypes of its type STt. When an object is deﬁned of type t but
initialized to a subtype t′ ∈STt, the methods executed on the object are not
known until runtime. Hence, using the access hints of m might lead to erroneous
predictions. We propose to handle this case by adding one of the following sets
of access hints to AHm (both strategies are evaluated in Sect. 5):
– 
m′∈OMm AHm′: intersection of access hints of overridden versions of m.
– 
m′∈OMm AHm′: union of access hints of overridden versions of m.
5
Evaluation
We implemented a prototype of our approach in Java using IBM Wala and eval-
uated it on two benchmarks speciﬁcally designed for POSs and two benchmarks
typically used for computation-intensive workloads:
– OO7: the de facto standard benchmark for POSs and OO databases.
– JPAB: measures the performance of ORMs compliant with Java Persistent
API (JPA) using 4 types of workloads (persist, retrieve, query and update).
– K-Means: a clustering algorithm typically used as a big data benchmark.
– Princeton Graph Algorithms (PGA): a set of various graph algorithms
with diﬀerent types of graphs (undirected, directed, weighted).
We compared our approach with the ROP explained in Sect. 2 using the
minimum possible depth of 1 as well as a depth of 3 to predict access to objects.
In all the experiments, we used Hibernate 4.1.0 with PostgreSQL 9.3 as the
persistent storage. In the following, we present our experimental results.
RQ1: What Is the Accuracy of the Proposed Approach? We answered
this question by testing the diﬀerent strategies proposed in Sect. 4.3 to deal with
branch-dependent navigations and overridden methods. Figure 5 shows the True
Positive Ratio (correctly predicted objects / accessed objects) and False Positive
Ratio (incorrectly predicted objects / total predicted objects) of these strategies
compared with ROP. Regardless of the used strategy, our approach results in
fewer false positives in all of the studied benchmarks.

60
R. Touma et al.
Fig. 5. True Positive Ratio (TPR) and False Positive Ratio (FPR) of our approach (left
of the dashed line) compared with ROP (right of the dashed line). Columns represent:
- ¬BDNs / BDNs : exclude / include branch-dependent navigations
- ∪OMs / ∩OMs : intersection / union of overridden methods’ access hints
The only exception is taking the union of overridden methods’ access hints
with JPAB, represented by the solid-colored set of columns in Fig. 5(b), which
results in a sharp increase in false positives. This is due to the implementation
of JPAB which includes ﬁve diﬀerent tests each with its independent persistent
classes, all of which are subclasses of a common abstract class. Hence, taking the
union of overridden methods’ access hints results in predicting access to many
objects unrelated to the test being executed. We reran the analysis excluding the
methods of the common abstract class and their overridden versions. The results
are shown by the dotted set of columns in Fig. 5(b) and indicate that excluding
this case, the behavior of JPAB is similar to that of the other benchmarks.
Based on the results of this experiment, we recommend excluding branch-
dependent navigations when memory resources are scarce, since this strategy
does not result in any false positives. By contrast, branch-dependent navigations
should be included when we are willing to sacriﬁce some memory, which will be
occupied with false positives, in return of a higher true positive ratio. Finally, we
recommend to always take the intersection of overridden methods’ access hints
in order to avoid problems with special cases similar to JPAB.

Predicting Access to Persistent Objects Through Static Code Analysis
61
Fig. 6. The x-axis represents the number of persistent accesses between the prediction
that a persistent object o will be accessed and the actual access to o. The y-axis
represents the percentage of accesses that are predicted for each x-axis value. Solid
lines represent our approach and dashed lines represent ROP.
RQ2: How Much in Advance Can the Approach Make the Predictions?
We measured how much in advance our approach can make the predictions and
compared it with ROP by calculating the number of persistent accesses between
the time that an object o is predicted to be accessed and the actual access to
o. For example, Fig. 6 shows that with OO7, 95% of predictions made by our
approach are done at least 1 persistent access in advance and 70% of predictions
at least 10 persistent accesses in advance. The results shown in Fig. 6 indicate
that in the case of JPAB, the improvement we obtain over ROP is very small
because the benchmark’s data model does not allow for predictions to be made
far in advance. However, with the other benchmarks, most signiﬁcantly with
K-Means, our approach is able to predict accesses much farther in advance.
6
Conclusions
In this paper, we presented a novel approach to automatically predict access to
persistent objects through static code analysis of object-oriented applications.
The approach performs the analysis before application execution and hence does
not add any overhead. The experimental results show that our approach achieves
better accuracy than the most common prediction technique, the Referenced-
Objects Predictor. Moreover, the true advantage of our approach comes from
the fact that it can predict access to persistent objects farther in advance which
indicates that the predictions can be exploited to apply smarter prefetching,
cache replacement policies and/or dynamic data placement mechanisms.

62
R. Touma et al.
References
1. Cahoon, B., McKinley, K.S.: Data ﬂow analysis for software prefetching linked data
structures in Java. Proc. PACT 2001, 280–291 (2001)
2. Curewitz, K.M., Krishnan, P., Vitter, J.S.: Practical prefetching via data compres-
sion. SIGMOD Rec. 22(2), 257–266 (1993)
3. Gornish, E.H., Granston, E.D., Veidenbaum, A.V.: Compiler-directed data
prefetching in multiprocessors with memory hierarchies. In: Proceedings of ICS
1990, pp. 354–368. ACM (1990)
4. Han, W., Whang, K., Moon, Y.: A formal framework for prefetching based on the
type-level access pattern in object-relational DBMSs. IEEE Trans. Knowl. Data
Eng. 17(10), 1436–1448 (2005)
5. He, Z., Marquez, A.: Path and cache conscious prefetching (PCCP). VLDB J.
16(2), 235–249 (2007)
6. Ibrahim, A., Cook, W.R.: Automatic prefetching by traversal proﬁling in object
persistence architectures. In: Thomas, D. (ed.) ECOOP 2006. LNCS, vol. 4067,
pp. 50–73. Springer, Heidelberg (2006). doi:10.1007/11785477 4
7. Karlsson, M., Dahlgren, F., Stenstr¨om, P.: A prefetching technique for irregular
accesses to linked data structures. In: Proceedings of HPCA, pp. 206–217 (2000)
8. Knaﬂa, N.: A prefetching technique for object-oriented databases. In: Small, C.,
Douglas, P., Johnson, R., King, P., Martin, N. (eds.) BNCOD 1997. LNCS, vol.
1271, pp. 154–168. Springer, Heidelberg (1997). doi:10.1007/3-540-63263-8 19
9. Knaﬂa, N.: Analysing object relationships to predict page access for prefetching.
In: Proceedings of POS-8 and PJW-3, pp. 160–170. Morgan Kaufmann (1999)
10. Luk, C.-K., Mowry, T.C.: Compiler-based prefetching for recursive data structures.
In: Proceedings of ASPLOS VII, pp. 222–233. ACM (1996)
11. Mart, J., Queralt, A., Gasull, D., et al.: Dataclay: a distributed data store for
eﬀective inter-player data sharing. J. Syst. Softw. 131, 129–145 (2017)
12. Mowry, T.C., Lam, M.S., Gupta, A.: Design and evaluation of a compiler algorithm
for prefetching. In: Proceedings of ASPLOS V, pp. 62–73. ACM (1992)
13. Stoutchinin, A., Amaral, J.N., Gao, G.R., Dehnert, J.C., Jain, S., Douillet,
A.: Speculative prefetching of induction pointers. In: Wilhelm, R. (ed.) CC
2001. LNCS, vol. 2027, pp. 289–303. Springer, Heidelberg (2001). doi:10.1007/
3-540-45306-7 20

Query-Driven Knowledge-Sharing for Data
Integration and Collaborative Data Science
Andreas M. Wahl(B), Gregor Endler, Peter K. Schwab, Sebastian Herbst,
and Richard Lenz
Computer Science 6 (Data Management),
FAU Erlangen-N¨urnberg, Erlangen, Germany
andreas.wahl@fau.de
Abstract. Writing eﬀective analytical queries requires data scientists to
have in-depth knowledge of the existence, semantics, and usage context of
data sources. Once gathered, such knowledge is informally shared within
a speciﬁc team of data scientists, but usually is neither formalized nor
shared with other teams. Potential synergies remain unused. We intro-
duce our novel approach of Query-driven Knowledge-Sharing Systems
(QKSS). A QKSS extends a data management system with knowledge-
sharing capabilities to facilitate user collaboration without altering data
analysis workﬂows. Collective knowledge from the query log is extracted
to support data source discovery and data integration. Knowledge is for-
malized to enable its sharing across data scientist teams.
1
Introduction
Data scientists work according to their expert knowledge gained by solving pre-
vious data analysis challenges and maintain individual mental models of the
available data sources. These models encompass knowledge about when certain
data sources are useful, how they can be linked, how their content can be inter-
preted or what domain vocabulary is used. Within a team of data scientists,
this knowledge is shared through personal interaction. In most cases however,
it is not formally documented or shared between teams. Due to the complex-
ity of analytical questions, it is common that multiple teams of data scientists
work separately on data analysis challenges, especially in larger organizations.
When diﬀerent teams do not directly interact with each other, they miss out on
opportunities to share their knowledge and to proﬁt from experiences of others.
Contribution. To overcome the above deﬁciencies, we propose Query-driven
Knowledge-Sharing Systems (QKSS). A QKSS extends a data management sys-
tem by adding services that formalize knowledge implicitly contained in a cen-
tralized query log and make it available to other users. Parts of the underlying
mental model of each query are extracted. This model is mapped to actually
available data sources by using previously generated mappings of related queries.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 63–72, 2017.
DOI: 10.1007/978-3-319-67162-8 8

64
A.M. Wahl et al.
Our contribution comprises multiple aspects: (1) We introduce shards of
knowledge as an abstraction for the concepts behind query-driven knowledge-
sharing. (2) We provide a formal model for building, evolving, and querying
shards. (3) We explain the integration of a QKSS with existing data analysis
tools and processes by suggesting a reference architecture.
2
Query-Driven Knowledge-Sharing Systems
We consider the term knowledge to denote domain knowledge about data sources
required to query them. Such knowledge contains, among others, the following
aspects:
(1) What data sources are available? (2) What parts of data sources can
be used for speciﬁc analytical purposes? (3) Which vocabulary and semantics
are used to describe data sources? (4) How can data sources be related to each
other? (5) Who is using which data sources in which temporal context?
2.1
Services of a QKSS
To explain the services a QKSS oﬀers to data scientists and the beneﬁts it
provides, we describe a user story from a clinical research scenario, including
simpliﬁed example queries.
Consider three teams of data scientists accessing a QKSS (Fig. 1). The QKSS
manages diﬀerent data sources containing data from electronic health records.
Alice and Bob from team 1 use medication plans from data source D1 (JSON
format) and the relational database D2 for their main focus of drug dosage analy-
sis. Team 2 (Carol and Dan) also conduct drug dosage analysis, but rely on data
from the relational databases D2, D3 and data source D4 (CSV format). Erin
constitutes team 3 and specializes in time series analysis of patient monitoring
data. She uses two data sources D5 (Avro format) and D6 (Parquet format)
from a distributed ﬁle system.
MedicaƟon
QKSS
PaƟent Monitoring
Team 1 (Alice, Bob)
MedicaƟon
Team 2 (Carol, Dan)
PaƟent Monitoring
Team 3 (Erin)
D1
D2
D3
D4
D5
D6
Fig. 1. Collaboration through a QKSS
Shared-Knowledge Support for Querying. Initially, none of the teams is
aware of the others. The QKSS provides a SQL interface for data access. While
Alice from team 1 is waiting for the completion of a query (Fig. 2), the QKSS
detects that both teams rely on D2 by analyzing previous queries (Fig. 3).
The QKSS subsequently presents Alice with information and hints for future
queries based on the collective knowledge of team 2. By analyzing queries of

Query-Driven Knowledge-Sharing for Data Integration
65
SELECT D2.id, D2.Department
FROM D1 JOIN D2 ON D2.id = D1.PatNr
WHERE D1.Agent LIKE ’Dexametha%’;
Fig. 2. Exemplary query by Alice
SELECT MIN(D2.Age)
FROM D4 JOIN D2 USING id
WHERE D4.ActiveAgent = ’Salbutamol’;
SELECT D2.id, D3.Substance, D3.Dose
FROM D3 JOIN D2 ON D2.id = D3.Patient
Fig. 3. Exemplary queries by Team 2
team 2, the QKSS ﬁnds that data sources D3 and D4 have already been linked to
D2. It therefore shows Alice a uniﬁed view of D2 and these sources. To help Alice
with exploring the newly discovered sources, the QKSS ranks them according to
frequency of use and temporal occurrence within query sessions of team 2.
Bob has recently received the order to investigate whether the ingestion of
certain active agent combinations correlates with the occurrence of critical vital
parameters. He is not aware of any data sources containing vital parameters
yet, but has a notion of the kind of data he is looking for. Bob imagines vital
parameter entries to have attributes such as HeartRate and BloodPressure. He
uses this mental model to formulate a query referencing the hypothetical data
source VitalParameters (Fig. 4).
SELECT HeartRate, BloodPressure
FROM VitalParemeters
WHERE HeartRate >= 130;
Fig. 4. Exemplary query by Bob
SELECT BloodPr AS BloodPressure
FROM D5
WHERE HeartRate < 90 AND Time > ’16-01-10’;
SELECT HRt AS HeartRate, BP AS
BloodPressure
FROM D6
WHERE PId = ’P41’ AND TStmp = ’1475693932’;
Fig. 5. Exemplary queries by Erin
The QKSS utilizes his assumptions about the structure and semantics of a
ﬁctional data source named VitalParameters to suggest actual data sources.
Using the knowledge extracted from Erin’s queries (Fig. 5), the QKSS detects
similarities between VitalParameters and the data sources D5 and D6. These
have been queried before using similar structural assumptions and vocabulary.
Thus, the QKSS can recommend D5 and D6 as replacement for the ﬁctional data
source in Bob’s query. Erin’s alias names for schema elements automatically
become part of an ontology. Bob can use this ontology for easier comprehen-
sion of the vocabulary used by other teams. He can provide feedback about the
suggestions through an interactive dialog. The QKSS remembers the mapping
between Bob’s expectations and the actually available data sources and oﬀers to
automatically generate a view that corresponds to his mental model. Bob can
directly use this view in future queries.

66
A.M. Wahl et al.
Management of Shared Knowledge. Whenever synergies between teams
are discovered, data scientists may decide to incorporate the shared knowledge
of others in the representation of their own mental models. The QKSS provides
mechanisms to subscribe to the knowledge contained in queries of others and
therefore enables collaborative pay-as-you-go data integration. As long as Bob
is involved in patient monitoring data analysis, he subscribes to Erin’s queries
and augments the formalized representation of his own mental model with hers.
He can automatically see the same uniﬁed view of the patient monitoring data
sources that she does. When he is no longer interested in this topic, he may
unsubscribe from her queries and return to his prior medication-centric view.
2.2
Shards of Knowledge
To implement this subscription process and other QKSS services, we introduce
shards of knowledge as an abstraction for knowledge from the query log. A shard
of knowledge captures the mental model of data sources a group of data scientists
forms over a period of time. We use the expression shard because single mental
models may be incomplete, while the combination of all mental models yields
the overall organizational knowledge inferable from the query log. As depicted in
Fig. 6, shards encapsulate knowledge models constructed from speciﬁc portions
of the query log. Subsequently, we formally describe the lifecycle of shards to
illustrate their usage by data scientists.
Fig. 6. From log entries to shards of knowledge
Instantiation. Shards are instantiated according to the preferences of data sci-
entists. They determine which knowledge, in form of queries, is to be considered
for each shard.
→Example: An initial set of shards for a QKSS could encompass one shard
for each team, incorporating all previous queries of the team members. In our
example scenario, the QKSS manages an initial set of three shards.
Query Log. The query log L is a set of log entries of type L. Each entry contains
a user identiﬁer u of type U, a timestamp t of type T and a query q of type Q
(Fig. 7(1)).
Only certain portions of the log L are relevant for the data scientists using
the QKSS. These portions are extracted using functions extr that apply a set of
ﬁlter predicates of type F on the log (Fig. 7(2)). We provide exemplary deﬁnitions
suitable for a QKSS (Fig. 7(3)/(4)): Each ﬁlter predicate denotes the user u

Query-Driven Knowledge-Sharing for Data Integration
67
whose queries are to be considered, as well as the time period of relevant log
entries using the timestamps tstart and tend. Using a set of ﬁlter predicates F of
type F, we can extract log entries from a log L using the function extr.
→Example: For our scenario of three initial shards we would create the ﬁlter
predicates F1, F2 and F3 (Fig. 8). tα and tω indicate that log entries from the
whole lifespan of the QKSS are included and the QKSS considers future updates
to the log. Fixed time spans could also be speciﬁed.
L : Set L
with L = (u : U, t : T, q : Q)
(1)
extr : Set F × Set L →Set L
(2)
F = (u : U, tstart : T, tend : T)
(3)
extr(F, L) = {l ∈L|f ∈F.((l.u = f.u)
(4)
∧(f.tstart ≤l.t ≤f.tend))}
Fig. 7. Extracting relevant queries
F1 = {(Alice, tα, tω), (Bob, tα, tω)}
F2 = {(Carol, tα, tω), (Dan, tα, tω)}
F3 = {(Erin, tα, tω)}
Fig. 8. Exemplary ﬁlter predicates
Knowledge Models. Query log extracts are used by log mining algorithms to cre-
ate knowledge models (Fig. 6). The algorithms ai create individual data struc-
tures Ki from a subset of the query log to represent the extracted knowledge
(Fig. 9(5)). The indices i are elements of an index set I that can be used to label
all algorithms provided by the QKSS.
Each knowledge model of type K consists of the product of the diﬀerent
knowledge aspects extracted by the log mining algorithms (Fig. 9(6)). The func-
tion createModelL,extr,I instantiates a model by applying the algorithms ai on
an extracted portion of the query log (Fig. 9(7)).
→Example: For our example scenario, the QKSS provides a variety of log
mining algorithms. An algorithm alinks extracts join edges between data sources
from the query log to form a graph of data sources to reason over. Another
algorithm asession partitions the query log into explorative sessions. An algo-
rithm astruct tracks how referenced portions of data sources are structured. To
detect synonyms between the vocabularies of diﬀerent teams, aonto maintains an
ontology. By using the resulting index set I (Fig. 10(a)), the knowledge model
for team 1 is created (Fig. 10(b)), for example.
Shards. A shard of type S formalizes the mental model shared by multiple data
scientists (Fig. 11(8)). A set F of ﬁlter predicates is used to instantiate the shard
with the relevant portions of the query log. These are used to create a knowledge
model K of type K which contains all knowledge of the shard.
The function createShardL,extr,I takes a set of ﬁlter predicates F of type F
to create a shard for a given Log L, a given extraction function extr and a given
index set I of log mining algorithms (Fig. 11(9)). L, extr and I are identical for
all shards of a speciﬁc QKSS instance.

68
A.M. Wahl et al.
ai : Set L →Ki
r
u¨
f
i ∈I
(5)
K =

i∈I
Ki = (Ki1, ..., Kin)
(6)
createModelL,extr,I : Set F →K
(7)
createModelL,extr,I(F ) =

i∈I
ai (extr(F, L))
Fig. 9. Creating knowledge models
I = {links, session, struct, onto}
(a)
K1 = createModelL,extr,I(F1) =
(b)
= (alinks (extr(F1, L)), asession (extr(F1, L)),
astruct (extr(F1, L)), aonto (extr(F1, L))) =
= (Klinks, Ksession, Kstruct, Konto)
Fig. 10. Exemplary knowledge model
S = (F : Set F, K : K)
(8)
createShardL,extr,I : Set F →S
(9)
createShardL,extr,I(F ) = (F, createModelL,extr,I(F ))
Fig. 11. Creating shards
s1 = createShardL,extr,I(F1)
s2 = createShardL,extr,I(F2)
s3 = createShardL,extr,I(F3)
Fig. 12. Exemplary shards
→Example: In our scenario, each team might create a shard using the ﬁlter
predicates F1–F3 (Fig. 12).
Evolution. Shards are dynamic and evolve over time. New log entries matching
the ﬁlter predicates of a shard can become part of the underlying knowledge
models. To provide data scientists with additional ﬂexibility, the QKSS supports
several operations to manage shards. While we consider shards to be immutable
in our functional model, implementations may destroy or recycle shards.
Lifecycle Operations. Two shards can be merged into a single shard, to reﬂect
in-depth collaboration between data scientist teams (Fig. 13(10)). Shards can
also be expanded or narrowed (Fig. 13(11)/(12)). Thereby, speciﬁc parts of the
query log can be added to or excluded from a shard.
mergeL,extr,I : S × S →S
mergeL,extr,I(s1, s2) = createShardL,extr,I (s1.F ∪s2.F )
(10)
expandL,extr,I : S × F →S
expandL,extr,I(s, F ) = createShardL,extr,I (s.F ∪F )
(11)
narrowL,extr,I : S × F →S
narrowL,extr,I(s, F ) = createShardL,extr,I (s.F −F )
(12)
Fig. 13. Basic operations on shards
Shard Comparison. Two shards are considered similar if their knowledge mod-
els are similar. The knowledge extracted by log mining algorithms ai can be
compared by using algorithm-speciﬁc functions simKi (Fig. 14(13)). Two knowl-
edge models of type K can be compared to each other using a function simK
(Fig. 14(14)). A weight function w allows to determine the inﬂuence of speciﬁc

Query-Driven Knowledge-Sharing for Data Integration
69
simKi : Ki × Ki →
)
3
1
(
]
1
;
0
[
simK : (I →[0; 1]) × K × K →
)
4
1
(
]
1
;
0
[
simK(w, K1, K2)=

i∈I
(w(i) · simKi(K1.Ki, K2.Ki)) with

i∈I
w(i)=1
simS : (I →[0; 1]) × S × S →
)
5
1
(
]
1
;
0
[
simS(w, s1, s2) = simK(w, s1.K, s2.K)
Fig. 14. Similarity functions
algorithms from the index set I on model similarity. The function simS derives a
numerical value for shard similarity by comparing the knowledge models of two
shards (Fig. 14(15)). Data scientists can manually compare shards or rely on
automatic comparisons by the QKSS. The system suggests evolution operations
based on these comparisons, which can be reviewed by the data scientists.
→Example: Assume that all algorithms except astruct are assigned a weight
factor of 0. Because of the structural similarity of s1 and s2 determined by
simKstruct (s2 incorporates exactly half of the sources of s1), the QKSS auto-
matically suggests team 1 to merge s2 into s1.
Query Processing. The knowledge model of a shard represents the mental
model of the data scientists. Queries can be written against this mental model
and do not have to reference actually available data sources. Before they can
be evaluated against these data sources, the QKSS reasons over the knowledge
model (Fig. 15). After logging a query, the QKSS determines if the query is fully
speciﬁed, which means it only contains schema elements that belong to actually
available data sources. If this is the case, the query is processed normally. The
knowledge model belonging to the shard of the querying user is evaluated in
parallel to generate recommendations. These may include hints about similar
data sources, similar users, or synonyms for schema elements.
We also allow queries to be underspeciﬁed, as data scientists should be able
to express queries using their mental models of the data sources. Whenever the
QKSS encounters an underspeciﬁed query, it evaluates the knowledge model in
order to modify the query to be processable using the actually available data
sources. If it cannot decide about relevant data sources, it collects feedback from
the user through an interactive dialog. Otherwise, the knowledge model is used
to rewrite the query to be consistent with the mental model of the user while
using actual data sources.
[yes]
[no]
[yes]
Query
fully spec.?
InteracƟon
req.?
[no]
Query
query
Log
Evaluate
know. model
Collect
feedback
Rewrite
query
Evaluate
know. model
Process
query
Show
recomm.
Return
results
Fig. 15. QKSS query processing workﬂow

70
A.M. Wahl et al.
3
Reference Architecture
To demonstrate the feasibility of our approach, we are developing a reference
QKSS [7]. This implementation adheres to our QKSS architecture (Fig. 16).
Users pose queries via the SQL API which is a wrapper for the native query
interface of an existing data management system (DMS). Established analysis
workﬂows remain intact, as analysis tools simply connect to the QKSS instead
of a DMS. The GUI acts as a companion to present relevant knowledge and
suggests modiﬁcations to the queries and shard lifecycle operations.
Incoming queries are stored in the centralized query log by the query inter-
ception component. Intercepted queries can be rewritten or extended and sub-
sequently forwarded to the DMS. Result retrieval is handled by the DMS. The
shard management oversees the lifecycle of all shards in the QKSS and generates
knowledge models from the query log to be stored in the model repository. It
also monitors the query log to update models if necessary. The knowledge sharing
component provides relevant knowledge to rewrite or extend intercepted queries
by evaluating the models from the model repository using the inference engine.
It adjusts models according to the queries of the data scientists. Additionally, it
monitors all shards of the QKSS to provide suggestions for lifecycle operations,
such as merging similar shards.
Data 
Sources
Query 
IntercepƟon
Query 
Log
Inference
Engine
SQL API
GUI
Knowledge 
Sharing
Management Sys.
Data
Analysis Tool
Model 
Repository
Shard 
Management
A        B:   A uses B
Fig. 16. QKSS reference architecture
4
Evaluation Methods
We assess the overall usefulness of our approach by analyzing how a QKSS
supports data scientists with their data analysis tasks. Additionally, we examine
if the performance of our reference implementation is suﬃcient for analytical
ad-hoc queries and interactive usage.
Usefulness: The usefulness of our approach is evaluated during a user study.
Knowledge models are created from queries of the participating users. The mod-
els are presented to the users to judge to what extent their intentions are cor-
rectly captured. By using logs of varying size, the number of queries required to
create meaningful models can be assessed. To assess the result quality of shard
comparisons, participating users validate if knowledge models that are marked
similar by the QKSS are actually built from similar log portions. Subsequently,
two groups of users are formed. Both groups get the task to answer speciﬁc

Query-Driven Knowledge-Sharing for Data Integration
71
analytical questions using a given set of unfamiliar data sources. While both
groups use the QKSS to access the data sources through a single interface, only
one group receives recommendations based on prepared knowledge models from
the QKSS. We analyze how a QKSS can support the users by comparing the
working speed and the result quality of both groups.
Performance: We measure the computational eﬀort required for initial knowl-
edge model creation, model maintenance when new queries are added to the log,
model evaluation during query processing, and model comparison. To simulate
diﬀerent application environments, query logs of varying size and complexity are
processed by the log mining algorithms. We also measure the overall response
time of the system during the processing of ad-hoc queries.
5
Related Work
Dataspace systems [4] rely on user feedback to incrementally adapt the managed
data to the expectations of their users. However, existing implementations of
dataspace systems do not suﬃciently consider scenarios where diﬀerent groups of
users with heterogeneous expectations work with a common set of data sources.
Mental models of data scientists may diﬀer from the actual schema and con-
tent of data sources. Some approaches allow queries with references to unknown
schema elements [2,6]. However, they do not consider advanced temporal and
social connections inferable from the query log.
Our approach diﬀers from query recommendation [3] and completion [5], as
we want to enable the users to specify complete queries using individual mental
models. We aim to adjust the actually available data sources to the mental
models of the users and not to force users to adjust to the data sources. Thus,
we minimize bias caused by anchoring and adjustment, psychological phenomena
that have been found to have adverse eﬀects on query and result quality [1].
6
Summary
We introduce Query-driven Knowledge-Sharing Systems (QKSS) to support data
scientists in integrating these data sources and querying them for data analysis
tasks. Using a QKSS, data scientists can externalize tacit knowledge about data
sources without manual documentation eﬀort, explore how others interact with
data sources, and discover relevant data sources. Shards of knowledge provide an
intuitive abstraction for the user-facing concepts of a QKSS. They encapsulate
knowledge models derived from relevant portions of the query log.

72
A.M. Wahl et al.
References
1. Allen, G., Parsons, J.: Is query reuse potentially harmful? Anchoring and adjustment
in adapting existing database queries. ISR 21(1), 56–77 (2010)
2. Eberius, J., Thiele, M., Braunschweig, K., Lehner, W.: DrillBeyond: processing
multi-result open world SQL queries. In: SSDBM 2015 (2015)
3. Eirinaki, M., Abraham, S., Polyzotis, N., Shaikh, N.: QueRIE: collaborative data-
base exploration. KDE 26(7), 1778–1790 (2014)
4. Franklin, M., Halevy, A., Maier, D.: From databases to dataspaces: a new abstrac-
tion for information management. SIGMOD Rec. 34(4), 27–33 (2005)
5. Khoussainova, N., Kwon, Y., Balazinska, M., Suciu, D.: SnipSuggest: context-aware
autocompletion for SQL. PVLDB 4(1), 22–33 (2010)
6. Li, F., Pan, T., Jagadish, H.V.: Schema-free SQL. In: SIGMOD 2014 (2014)
7. Wahl, A.M.: A minimally-intrusive approach for query-driven data integration sys-
tems. In: ICDEW 2016 (2016)

A Declarative Approach to Analyzing Schema
Objects and Functional Dependencies
Christiane Engels1, Andreas Behrend1(B), and Stefan Brass2
1 Institut f¨ur Informatik III, Rheinische Friedrich-Wilhelms-Universit¨at Bonn,
Bonn, Germany
{engelsc,behrend}@cs.uni-bonn.de
2 Institut f¨ur Informatik, Martin-Luther-Universit¨at Halle-Wittenberg,
Halle, Germany
brass@informatik.uni-halle.de
Abstract. Database schema elements such as tables, views, triggers and
functions are typically deﬁned with many interrelationships. In order to
support database users in understanding a given schema, a rule-based
approach for analyzing the respective dependencies is proposed using
Datalog expressions. We show that many interesting properties of schema
elements can be systematically determined this way. The expressiveness
of the proposed analysis is exemplarily shown with the problem of com-
puting induced functional dependencies for derived relations.
Keywords: Schema analysis · Functional dependencies · Datalog
1
Introduction
The analysis of database schema elements such as tables, views, triggers, user-
deﬁned functions and constraints provide valuable information for database users
for understanding, maintaining and managing a database application and its
evolution. In the literature, schema analysis has been investigated for improv-
ing the quality of SQL/program code or detecting program errors [3] and for
determining the consequences of schema changes [10], versioning [8], or match-
ing [11]. In addition, the analysis of schema objects plays an important role for
tuning resp. refactoring database applications [2]. All these approaches rely on
exploring dependencies between schema objects and an in-depth analysis of their
components and interactions. A comprehensive and ﬂexible analysis of schema
elements, however, is not provided as these approaches are typically restricted
to some subparts of a given schema.
The same is true for analysis features provided by commercial systems where
approaches such as integrity checking, executing referential actions or query
change notiﬁcation (as provided by Oracle) already use schema object depen-
dencies but in an implicit and nontransparent way, only. That is, no access to
the underlying meta-data is provided to the user nor can be freely analyzed by
means of user-deﬁned queries. Even the meta-data about tables and SQL views
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 73–81, 2017.
DOI: 10.1007/978-3-319-67162-8 9

74
C. Engels et al.
which are sometimes provided by system tables cover only certain information
of the respective schema elements. In this paper, we propose a uniform app-
roach for analyzing schema elements in a comprehensive way. To this end, the
schema objects are compiled and their meta-data is stored into a Datalog pro-
gram which employs queries for deriving interesting properties of the schema.
This way, indirect dependencies between tables, views and user-deﬁned func-
tions (UDFs) can be determined which is important for understanding follow-up
changes. In order to show the expressiveness of the proposed analysis, our rule-
based approach is applied to the problem of deducing functional dependencies
(FDs) for derived relations, i.e., views, based on FDs deﬁned for base relations.
This so-called FD propagation or FD-FD implication problem has been studied
since the 80s [7,9,12] and has applications in data exchange [6], data integration
[4], data cleaning [7], data transformations [5], and semantic query optimization.
We show that our rule-based approach to schema analysis is well-suited for real-
izing all known techniques for FD propagation indicating the expressiveness of
the proposed analysis. In particular, our contributions are as follows:
– We propose an approach for analyzing the properties of views, tables, trigger
and functions in a uniform way.
– Our declarative approach can be easily extended for reﬁning the analysis by
user-deﬁned queries.
– The employed Datalog solution can be simply transfered into SQL systems.
– In order to show the expressiveness of our approach, the implication problem
for functional dependencies is investigated using our approach.
2
Rule-Based Schema Analysis
A database schema describes the structure of the data stored in a database
system but also contains views, triggers, integrity constraints and user deﬁned
functions for data analysis. Functions and these diﬀerent rule types, namely
deductive, active and normative rules, are typically deﬁned with various inter-
dependencies. For example, views are deﬁned with respect to base relations
and/or some other views inducing a hierarchy of derived queries. In particu-
lar, the expression CREATE VIEW q AS SELECT ... FROM p1, p2, ..., pn leads to
the set {p1 →q, . . . , pn →q} of direct dependencies between the derived rela-
tion q and derived or a base relations pi. which are typically represented by
means of a predicate dependency graph for analyzing indirect dependencies via
the transitive closure, too. This allows for understanding the consequences of
changes made to the instances of the given database schema (referred to as
update propagation in the literature) or to its structure. This is important when
a database user wants to know all view deﬁnitions potentially aﬀected by these
changes. Various dependencies can occur in a database schema such as table-
to-table dependencies induced by triggers or view-to-table dependencies which
can be induced by functions. The analysis of such dependencies can be further
reﬁned by structural details (e.g., negative vs. positive dependencies as needed
for update propagation) as well as by considering the syntactical components of

Analyzing Schema Objects and Functional Dependencies
75
schema objects such as column names (attributes) or operator types (sum, avg,
insert, delete, etc.). To this end, the deﬁnitions of schema objects need to be
parsed and the obtained tokens stored as queryable facts. This kind of analy-
sis is well-known from meta-programming in Prolog which led to the famous
vanilla interpreter. For readability reasons we use Datalog with facts such as
base(R,A) (base relation R with arity A), derived(V,A) (view V with arity A),
dep(To,From) (dependency between relations), call(V,I,O,F) (input I and
output O of function F in view V), attr(R,P,N) (position P of attribute named
N in relation R) for representing meta-information about a given view or user-
deﬁned function. Based on these facts, schema analysis can be realized by queries
like
attr dups(N) ←attr(R1, ,N),attr(R2, ,N),R1<>R2
idb func pred(V) ←derived(V, ),call(V, , , )
base changes(B) ←path(B,f1),base(B, ),func(f1, )
tbl dep(A,B) ←base(A, ),base(B, ),path(A,F),path(F,B),func(F, )
for determining reused attribute names, views calling a function, base tables
possibly changed by function f1, and cyclic dependencies between two base tables
through a function. This way, many interesting properties of schema elements
can be systematically determined which supports users in understanding the
interrelationships of schema elements. Most database systems already allow for
storing and querying meta-data about schema elements in a simple way but a
comprehensive (and in particular user-driven) analysis like this is still missing.
3
Functional Dependency Propagation
In order to show the expressiveness of our approach, we investigate the possi-
bility to compute induced FDs for derived relations using the deductive rules
introduced above. FDs form special constraints which are assumed to hold for
any possible valid database instance. The FD propagation problem is undecid-
able in the general setting for arbitrary relational expressions [9]. Even restricted
to SC views, i.e., relational expressions allowing selection and cross product only,
the propagation problem turns out to be coNP-complete (for an in-depth dis-
cussion on complexity see [7]). In favor of addressing the general setting, we
drop the ambition of achieving completeness by considering a special case, only.
Instead, we allow for arbitrary expressions over all relational operators, multiple
propagation steps and possibly ﬁnite domains1 in order to cover the majority of
practical cases.
3.1
Preliminaries
A functional dependency α = {A1, . . . , An} →B states that the attribute values
of α determine those of B. The restriction to univariate right sides can be done
1 Finite domains may introduce new FDs because of limited value combinations.

76
C. Engels et al.
without loss of generality as well as the representation of FDs satisfying B /∈α,
only.2 We allow α = ∅which means that the attribute values of B are constant.
For our FD propagation rules, we employ a Datalog variant with special data
types for ﬁnite, one-leveled sets and ﬁnite, possibly nested lists. In our approach
we use the extended transitivity axiom
α →B, γ →D, B ∈γ, D /∈α ⇒α ∪(γ −B) →D
(1)
to derive transitive FDs. Note that if B /∈α and D /∈γ, then the derived FD
also satisﬁes D /∈α ∪(γ −B).
Rule Normalization. For our systematic FD propagation approach, we assume
the Datalog rules deﬁning views to be in a normal form, where each rule cor-
responds to exactly one of the relational operators π, π′, σ, ×, ∪, ∩, −, or ⋊⋉.3
Any set of Datalog rules can be transformed into an equivalent set of normalized
rules while preserving important properties like being stratiﬁable [1].
3.2
Representation of FDs and Normalized Rules
We assume that functional dependencies for EDB predicates are given in a rela-
tion edb fd(p, α, B, ID). Here α and B are (sets of) column numbers of the rela-
tion p. The fact represents the functional dependency α→B for the relation p.
The ID is of type list and used to identify the dependency in later steps, e.g.,
in case of union. The derived functional dependencies will be represented in the
same way in an IDB predicate fd(p, α, B, ID′). Here ID’ is related to the depen-
dency’s ID where the FD is derived from for propagated FDs or to a newly
created ID for FDs that arise during the propagation process.
As in normal form every rule corresponds to exactly one operator, we can
reﬁne the above deﬁned dependency relation dep/2 to rel/3 by adding the
respective operator. A fact rel(p,q,op) indicates that a relation p depends
(positively) on q via an operator op which is one of ’projection’, ’extension’,
’selection’ ’product’, ’join’, ’negation’, ’intersection’, and ’union’.
We further introduce an EDB predicate pos(head,body,pos head,pos body)
for storing information on how the positions of non position preserving operators
(cf. Table 1) transform from rule body to head (as FDs are represented via
column numbers). Remembering that each relation is deﬁned via one operator
only and that we exclude self joins for simplicity (cf. Sect. 3.1), the above deﬁned
relation pos/4 is non-ambiguous. Finally, we have two additional EDB predicates
eq(pred,pos1,pos2) and const(pred,pos,val) for information on equality
conditions (e.g., X = Y or X = const) in extension and selection rules.
2 Multivariate right sides and omitted FDs are retrievable via Armstrong’s axioms.
3 In order to simplify the FD propagation process we limit w.l.o.g. a union rule to two
relations and do not allow self joins or cross products.

Analyzing Schema Objects and Functional Dependencies
77
Table 1. Properties of FD propagation categorized by operator
Properties
π
π′ σ
×
∪
∩
−
⋊⋉
FDs are preserved
×a ×
× ×
–
×
×b ×
Positions are preserved
–
×
× –c × × ×
–
Transitive FDs can appear
–
×
× –
–
–
–
×
Additional FDs from equality conditions (variables
and constants)
–
×
× –
–
–
–
–
Additional FDs caused by instance reduction may
appear
–
–
× –
–
× ×
×
× ˆ= yes, – ˆ= no
aThose where all contained variables are maintained
bThose of the minuend
cPositions of the ﬁrst factor are preserved, positions of the second factor get an oﬀset
3.3
Propagation Rules
In this section, we present three diﬀerent types of propagation rules for (a) propa-
gating FDs to the next step, (b) introducing additional FDs arising from equality
constraints, and (c) calculating transitive FDs.
Example 1. Consider the following rule set given in normal form together with
two FDs fd(s, {1}, 2, ID1) and fd(t, {1, 2}, 3, ID2) for the base relations s and t:
p(W,Z)
←
q(W,X,Y,Z)
q(W,X,Y,Z)
←
r(W,X,Y,Z), Y=2
r(W,X,Y,Z)
←
s(W,X), t(X,Y,Z)
Omitting IDs, we obtain the following propagation process: First, both FDs
are propagated to r resulting in fd(r,{1},2,-) and fd(r,{2,3},4,-) (with
the appropriate column renaming for the latter FD). By transitivity we have
fd(r,{1,3},4,-) as a combination of the two. All three FDs are propagated
to q together with fd(q,∅,3,-) resulting from the equality constraint Y = 2.
Applying transitivity results in three more FDs for q, but only fd(q,{1},4,-)
is propagated further to p as fd(p,{1},2,-). The complete list of propagated
FDs including IDs is given in Example 2.
Table 1 summarizes the properties of how FDs are propagated via the diﬀerent
relational operators which form the basis for the propagation rules. In most cases,
the FDs are propagated as they are (with adjustments on the positions for π,
×, and ⋊⋉). If there is a single rule deﬁning a derived relation, the source FDs
transform to FDs for the new relation (restricted to the attributes in use). Union
forms an exception where even common FDs are only propagated in special cases
(cf. Sect. 3.4). For extensions π′ and selections σ where additional FDs can occur
due to equality conditions as well as for joins ⋊⋉transitive FDs may appear so
that taking the transitive closure becomes necessary. In cases where the number
of tuples is reduced (i.e., σ, ∩, ⋊⋉, and −) it is possible that new FDs appear as

78
C. Engels et al.
pos pres
’selection’
’extension’
’negation’
’intersection’
non pos pres
’projection’
’product’
’join’
trans(R) ←base(R, ).
trans(R) ←rel(R, ,’join’).
trans(R) ←eq(R, , ).
trans(R) ←const(R, , ).
Fig. 1. Position preserving (left) and non position preserving (middle) operators, and
relations where transitive FDs may occur (right).
there are less tuples for which the FD constraint must be satisﬁed. The diﬀerent
propagation rules for all relational operators except union are speciﬁed in the
following.
(a) Induced FDs. For direct propagation of FDs from one level to the next, we
distinguish between position preserving and non position preserving operators.
In the ﬁrst case FDs can be directly propagated (2), whereas in the latter adjust-
ments on the column numbers are necessary (3). The EDB predicates pos pres
and non pos pres comprise the respective operators as listed in Fig. 1.
fd(P, α, B, -) ←fd(Q, α, B, -), rel(P, Q, op), pos pres(op).
(2)
fd(P, {X1, . . . , Xn}, Y, -) ←fd(Q, {A1, . . . , An}, B, -),
(3)
pos(P, Q, X1, A1), . . . , pos(P, Q, Xn, An), pos(P, Q, Y, B),
rel(P, Q, op), non pos pres(op).
(b) Additional FDs. For any equality constraint X = Y we can deduce the
dependencies X →Y and Y
→X. Similar, a constant constraint X = c
induces the dependency ∅→X. That is for any fact eq(R,pos1,pos2) and
const(R,pos,val) respectively we derive the following FDs:
fd(R, pos1, pos2, -) ←eq(R, pos1, pos2).
(4)
fd(R, pos2, pos1, -) ←eq(R, pos1, pos2).
(5)
fd(R, ∅, pos, -) ←const(R, pos, val).
(6)
(c) Transitive FDs. Since transitive FDs can only occur for certain operators
it is suﬃcient to deduce them for those cases, only (cf. Table 1):
fd(P, ε, D, -) ←fd(P, α, B, -), fd(X, γ, D, -),
(7)
B ∈γ, D /∈α, ε = α ∪(γ −{B}), trans(P).
fd(P, X, Y, -) ←fd(P, α, X, ID), fd(P, α, Y, ID), trans(P).
(8)
The ﬁrst rule implements the extended transitivity axiom (1) and the second
equates the right sides of two identical FDs (identiﬁed by matching IDs). The
IDB predicate trans/1 comprises all relations where transitive FDs may occur.

Analyzing Schema Objects and Functional Dependencies
79
3.4
Union
In case of union p = p1 ∪p2 even common FDs are only propagated in special
cases. Consider the following example of post codes. In each country, the post
code uniquely identify the city associated with it. But the same post code can be
used in diﬀerent countries for diﬀerent cities. So although we have the FD post
code →city in the relations german post codes and us post codes, it is not a
valid FD in the union of both. A common FD of p1 and p2 is only propagated to p
if the domains of the FD are disjoint, or if they match on common instances. The
ﬁrst case can only be handled safely on schema level if constants are involved.
The latter is the case if the FDs have the same origin and are propagated in a
similar way. Whether two FDs have the same origin can be easily checked (e.g.
using path from Sect. 2). This criteria is not yet enough as the FDs might have
been manipulated during the propagation process (e.g., changes in the ordering,
equality constraints, etc.). We employ identiﬁers to track changes made to certain
FDs using a list structure that adopts the tree structure of [9] who represents
FDs as trees with source domains as leaves and the target domain as the tree’s
root. As the target is already handled in the FD itself, we keep track of the
source domains and transitively composed FDs, only.
At the beginning, each base FD α →B gets a unique identiﬁer IDi. The
idea is to propagate this ID together with the FD and to keep track of the
modiﬁcations made to the FD. For this purpose we attach an ordered tuple,
a (possibly nested) list, to the ID, i.e., IDi[A1, . . . An] for α = {A1, . . . An}.
For the position preserving operators (that in particular do not change the FD’s
structure) the ID is identically propagated in (2). For the non position preserving
operators the positions are updated (using a UDF) similarly to the position
adjustments of the FD itself in (3). The diﬀerence is that the ID maintains an
ordering and the cardinality stays invariant. For constant constraints, we set
the constant value as ID in (6), equality constraints in (4), (5) and (8) get the
(column number of the) left side as ID. In (7) we replace the occurrences of the
column number B in the ID of fd(X, γ, D, -) by the ID of fd(X, α, B, -).
Example 2. For the FD propagation in Example 1 we have the following IDs:
fd(s,
{1},
2, ID1[1]).
fd(q,
{1},
2, ID1[1]).
fd(t, {1,2}, 3, ID2[1,2]).
fd(q, {2,3}, 4, ID2[2,3]).
fd(q, {1,3}, 4, ID2[ID1[1],3]).
fd(r,
{1},
2, ID1[1]).
fd(q,
∅,
3, ’3’).
fd(r, {2,3}, 4, ID2[2,3]).
fd(q,
{2},
4, ID2[2,’3’]).
fd(r, {1,3}, 4, ID2[ID1[1],3]).
fd(q,
{1},
4, ID2[ID1[1],’3’]).
fd(p,
{1},
2, ID2[ID1[1],’3’]).
A common ID implies that the same modiﬁcations have been made to a common
base FD. This means that the FD is preserved in the case of union:
fd(P, α, B, ID) ←fd(P1, α, B, ID), fd(P2, α, B, ID),
(9)
rel(P, P1, ’union’), rel(P, P2, ’union’), path(P1, X), path(P2, X).

80
C. Engels et al.
4
Conclusion
In Sect. 3.3 we introduced our propagation rules for propagating functional
dependencies. To compute the set of propagated FDs these rules are simulta-
neously applied to the input Datalog program in normal form. The rules are
based on the observations in Table 1 which can be easily veriﬁed. The propa-
gated functional dependencies of our approach are not complete as the problem
is undecidable in general. Also limited to a less expressive subset of the rela-
tional operators (e.g., restricted operator order SPC views) one has to assume
the absence of ﬁnite domains to achieve completeness. Nevertheless, we are able
to deal with many cases appearing in real world applications. Our FD propaga-
tion approach can be ﬂexible extended to allow for user-deﬁned functions in the
extension operator and even recursion can be covered with some modiﬁcations.
In [9] the FD implication problem was addressed ﬁrst. We provided a full
declarative approach covering most cases stated in this work. In addition, we are
able to cover linear recursion in a similar way as proposed by [12]. Other related
approaches like the work of [7] for conditional FDs can be incorporated into
our approach, too. Besides these rule-based approaches, a detailed comparison
with the chase, an established algorithm for FD implication, is object for future
research.
References
1. Behrend, A., Manthey, M.: A transformation-based approach to view updating in
stratiﬁable deductive databases. In: FOIKS 2008, pp. 253–271 (2008)
2. Boehm, A.M., Seipel, D., Sickmann, A., Wetzka, M.: Squash: a tool for analyzing,
tuning and refactoring relational database applications. In: Seipel, D., Hanus, M.,
Wolf, A. (eds.) INAP/WLP -2007. LNCS (LNAI), vol. 5437, pp. 82–98. Springer,
Heidelberg (2009). doi:10.1007/978-3-642-00675-3 6
3. Brass, S., Goldberg, C.: Proving the safety of SQL queries. In: QSIC 2005, pp.
197–204 (2005)
4. Cal`ı, A., Calvanese, D., De Giacomo, G., Lenzerini, M.: Data integration under
integrity constraints. Inf. Syst. 29(2), 147–163 (2004)
5. Davidson, S.B., Fan, W., Hara, C.S., Qin, J.: Propagating XML constraints to
relations. In: ICDE 2003, pp. 543–554 (2003)
6. Fagin, R., Kolaitis, P.G., Popa, L., Tan, W.C.: Reverse data exchange: Coping with
nulls. In: PODS 2009, pp. 23–32 (2009)
7. Fan, W., Ma, S., Hu, Y., Liu, J., Wu, Y.: Propagating functional dependencies
with conditions. PVLDB 1(1), 391–407 (2008)
8. Herrmann, K., Voigt, H., Behrend, A., Rausch, J., Lehner, W.: Living in parallel
realities co-existing schema versions with a bidirectional database evolution lan-
guage. In: SIGMOD 2017 (2017)
9. Klug, A.C.: Calculating constraints on relational expressions. TODS 5(3), 260–290
(1980)
10. Maule, A., Emmerich, W., Rosenblum, D.S.: Impact analysis of database schema
changes. In: ICSE 2008, pp. 451–460 (2008)

Analyzing Schema Objects and Functional Dependencies
81
11. Milo, T., Zohar, S.: Using schema matching to simplify heterogeneous data trans-
lation. In: VLDB 1998, p. 122 (1998)
12. Param´a, J.R., Brisaboa, N.R., Penabad, M.R., Places, ´A.S.: Implication of func-
tional dependencies for recursive queries. In: Ershov Memorial Conference 2003,
pp. 509–519 (2003)

Dear Mobile Agent, Could You Please Find Me
a Parking Space?
Oscar Urra(B) and Sergio Ilarri
Department of Computer Science and Systems Engineering, I3A,
University of Zaragoza, Zaragoza, Spain
ourra@itainnova.es, silarri@unizar.es
Abstract. Vehicular ad hoc networks (VANETs) have attracted a great
interest in the last years due to their potential utility for drivers in appli-
cations that provide information about relevant events (accidents, emer-
gency brakings, etc.), traﬃc conditions or even available parking spaces.
To accomplish this, the vehicles exchange data among them using wire-
less communications that can be obtained from diﬀerent sources, such as
sensors or alerts sent by other drivers. In this paper, we propose searching
of parking spaces by using a mobile agent that jumps from one vehicle to
another to reach the parking area and obtain the required data directly.
We perform an experimental evaluation with promising results that show
the feasibility of our proposal.
Keywords: Vehicular networks · Mobile agents · Distributed query
processing · Data management · Parking spaces
1
Introduction
Vehicular networks (VANETs) [7] are mobile ad hoc networks where the vehicles
can establish connections among them by using short-range wireless communica-
tions (such as IEEE 802.11p [11]) and exchange data that can be interesting for
the drivers, such as information about traﬃc jams, accidents, or scarce resources
such as available parking spaces. However, deploying applications that retrieve
and exploit those data is not an easy task [4], since the vehicles are continuously
moving and the interval of time for exchanging data is very short.
The data exchanged by the vehicles can be stored locally on them and a
driver that wants to retrieve information can submit a query over those data
by following diﬀerent approaches: (1) push-based query processing assumes that
the relevant data are proactively pushed into the vehicles [1], so the query can
be processed locally by exploiting the previously received data; (2) pull-based
query processing implies disseminating the query in the network in such a way
that a number of vehicles are explicitly asked about data that could be relevant
for the query. Push-based approaches are easier to deploy and simpler, but the
potential queries are constrained by the data that are actively exchanged.
On the other hand, we believe that mobile agents could be suitable for dis-
tributed query processing in VANETs. Mobile agents [8] are programs that have
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 82–90, 2017.
DOI: 10.1007/978-3-319-67162-8 10

Dear Mobile Agent, Could You Please Find Me a Parking Space?
83
the capability to pause their execution, move to another computer, and resume
their execution. In this way, they can locate the relevant data sources and move
there to process the data locally and ﬁlter out the irrelevant information, instead
of sending the collected data to a central location, which may not be possible in
a VANET due to the limitations and high costs of mobile networks.
In this paper, we study the potential use of mobile agents to retrieve data
about available parking spaces in a city. The structure of the paper is as follows.
In Sect. 2, we describe an approach to solve the problem by using mobile agents.
In Sect. 3, we analyze a use case scenario to retrieve information about available
parking spaces, and present an experimental evaluation. In Sect. 4, we present
some related work and, ﬁnally, in Sect. 5 we show our conclusions and outline
some prospective lines of future work.
2
Retrieving Parking Spaces Using Mobile Agents
We consider a scenario where vehicles exchange data about nearby available
parking spaces by using a push-based data sharing approach. However, push-
based approaches only disseminate popular data that are expected to be relevant
in a nearby area, based on the evaluation of spatio-temporal criteria. Therefore,
a pure push-based approach cannot handle situations where a driver is inter-
ested in retrieving information about available parking spaces located in further
areas. Instead, we need to explicitly disseminate a query (pull-based approach)
to retrieve data stored by vehicles located near the destination area. For that
purpose, a mobile agent can autonomously jump from car to car to reach the
relevant data sources that store data about those parking spaces (i.e., vehicles
near the destination area) and query them, following the steps described in [10]:
1. A mobile agent is created that will reach the destination area by jumping
from car to car, by using only ad hoc short-range wireless communications.
2. The agent retrieves data about available parking spaces by querying the local
databases available in vehicles inside the destination area.
3. Once the agent has retrieved enough data about available parking spaces, it
returns to the vehicle of the driver interested in those parking spaces (again,
by hopping from vehicle to vehicle) and provides the results collected.
3
Experimental Evaluation
In this section, we ﬁrst describe the use case scenario that we propose and then
we perform a number of experiments to evaluate the performance of our proposal.
We repeated each experiment 50 times (with diﬀerent random starting positions
for the vehicles and diﬀerent trajectories) and we report the average results.

84
O. Urra and S. Ilarri
3.1
Experimental Setup
In the use case considered, a person traveling along a highway approaches a city,
and when he/she is 15 km from the city he/she wants to obtain information about
available parking spaces near the city center. Then, a mobile agent is launched,
that travels to that area to collect information about the parking spaces by
querying the vehicles present there, and will return to the driver information
about a speciﬁc number of parking spaces. For the evaluation, we have used the
MAVSIM simulator [9], that allows the simulation of both mobile agents and
traﬃc in a realistic way (it uses road maps extracted from OpenStreetMaps and
simulates the blocking of signals by buildings using the method described in [6]).
The map scenario chosen (Fig. 1) is a portion of the city of Zaragoza (Spain)
and some fragments of interurban roads where the driver asking for parking
spaces is located when the experiment starts. We consider an urban area, where
the speed of vehicles is 50 km/h and buildings block the wireless signals, and
a interurban area, where vehicles can travel up to 120 km/h, and there are no
buildings blocking radio signals. The rectangle shown in Fig. 1 shows the des-
tination area (the city center), whose size is 0.25 km2. Some parameters of the
simulations are shown in Table 1, and a few of them deserve further explanations:
(a) Selected roads
(b) Whole area in OpenStreetMaps
Fig. 1. Scenario map used in the simulation
– The hop strategy refers to the algorithm used by the mobile agent when it trav-
els by hopping among vehicles and it must choose, among several candidates,
the most promising to reach the agent’s ﬁnal destination. We have chosen a
greedy heuristic (called map distance) that uses the remaining distance (the
shortest path) to the destination and has a good performance [10].
– The mobile agent’s hop delay is the time needed by the mobile agent to hop
from a vehicle to another one within the communication range. After per-
forming experiments with real mobile devices [10], we decided not to assume a
best-case scenario and establish the travel time, pessimistically, as one second.
– The default percentage of vehicles with relevant data is set to 50%, which
means that only half of the vehicles have useful data for the query process-
ing. This percentage allows to simulate cases where some vehicles do not
participate in the data sharing of information about parking spaces.

Dear Mobile Agent, Could You Please Find Me a Parking Space?
85
Table 1. Simulation parameters
Parameter
Default value
Map dimensions
4 × 4 Km (urban area)
20 Km (interurban area)
Map scenario
Zaragoza (Spain) and its
surroundings
Size of the destination area
0.25 Km2
Distance to the destination area
15000 m
Density of vehicles
50 vehicles/Km2 (urban area)
10 vehicles/Km (interurban area)
Speed of the vehicles
50 Km/h ± 10% (urban area)
120 Km/h ± 15% (interurban area)
Hop strategy
MAP (map distance)
Percentage of vehicles with relevant data
50%
Total number of parking spaces in the
destination area
220 parking spaces
Percentage of available parking spaces
per vehicle
10%
Data processing delay
5 s
Number of available parking spaces to
retrieve
10 available parking spaces
Parking occupancy
90%
Data collection and warning timeouts
4 and 3 min respectively
Communication range and mobile agents’
hop delay
250 m and 1 s respectively
– The percentage of available parking spaces per vehicle is the percentage of
available parking places within the area that are stored in a vehicle. We have
chosen a default valor of 10% for this parameter, which is quite pessimistic.
This means that, if there are 50 available parking spaces, each vehicle with
data would have information about 5 of those available parking places. The
smaller this value, the higher the number of vehicles that the mobile agent
will need to visit to collect information about available parking places.
– The data collecting timeout is the maximum time that the agent will invest in
collecting data. If this task takes too long and this timeout is exceeded, the
agent will stop collecting data and try to return the results collected so far
to the driver. Therefore, the amount of data collected could be smaller than
the amount of available parking spaces required by the driver.
– The warning timeout represents an amount of time that, if exceeded, will lead
the agent to enlarge the search area (100 m in each direction) to try to ﬁnd
more available spaces.

86
O. Urra and S. Ilarri
3.2
Inﬂuence of the Interurban Vehicle Density
In this experiment, we evaluate how the retrieval of information about avail-
able parking spaces is inﬂuenced by the density of vehicles in the interurban
area. These areas are characterized by long roads (lengths of tens of kilometers),
a small number of intersections, and the lack of buildings that block wireless
signals. Due to the long length of the road, the most suitable traﬃc density
measurement unit is the number of vehicles per linear kilometer (vehicles/km).
We vary this value from 2 vehicles/km up to 20 vehicles/km.
Figure 2(a) shows how the time required to obtain the desired information
about available parking spaces decreases as the density of vehicles increases. On
the other hand, Fig. 2(b) shows the total number of hops performed by the agent.
This value is the number of times that the mobile agent moves successfully from
one vehicle to another using the wireless connection, and it can be used as a
measure of the bandwidth required by the query processing. When the density
of vehicles increases, the number of hops also increases, since the mobile agent
is constantly looking for more promising vehicles to try to reach sooner the
destination area. The growth in the number of hops, however, is not very steep
and it stabilizes for about 8 to 10 vehicles/km.
(a) Time to solve the query
(b) Number of agent hops
Fig. 2. Inﬂuence of the interurban vehicle density
We have also measured the number of vehicles whose data have been
processed by the mobile agent looking for information about available park-
ing spaces. This number refers to the number of vehicles that have data about
parking places, since there also exist other irrelevant vehicles where the agent
can move but that do not contain data about parking spots. Since the query is
solved in the phase of data collection (in the urban area) the interurban den-
sity has no inﬂuence. The number of vehicles processed remains approximately
constant, speciﬁcally between 6 and 7 vehicles. We omit the ﬁgure due to space
constraints.
3.3
Inﬂuence of the Occupancy of the Parking Spaces
In this experiment, we vary the ratio of available parking places in the scenario,
from 0% (all the parking places are available) to 100% (there is no available
parking space) in which case the agent will always fail in its task.

Dear Mobile Agent, Could You Please Find Me a Parking Space?
87
Figure 3(a) shows the total time needed to solve the query. The rate of parking
occupancy has little impact on the total time, since the mobile agent spends most
of the time traveling to the destination area and returning to the originator car,
whereas the actual data collection process is performed quite fast.
Figure 3(b) shows the number of vehicles whose data have been processed by
the mobile agent. This number remains quite small until the parking occupancy
increases considerably. When there is no available parking space (occupancy
of 100%), the number of processed vehicles rises abruptly, due to the eﬀorts
performed by the agent to try to solve the query looking for non-existing data.
In such a case, the timeout established for data collection is reached and the
agent returns with an empty response, which might as well be an useful answer.
(a) Query solving time
(b) Vehicles queried
(c) Time per phase
Fig. 3. Inﬂuence of the occupancy of the parking spaces
The average time spent in the diﬀerent phases of the query processing is
shown in Fig. 3(c). The ﬁrst phase (mobile agent traveling to the destination
area) takes always the same time, since the ratio of available parking spaces does
not have any inﬂuence on it. In the second phase (mobile agent collecting data),
the time invested increases with the occupancy, since the agent will need to visit
more vehicles to ﬁnd information. Finally, the time invested in the fourth phase
(agent traveling back to the origin vehicle) decreases slightly as the occupancy of
parking spaces increases; this may seem surprising but, while the mobile agent
is collecting data, the origin vehicle keeps traveling towards the target area, so
the distance that the agent will need to traverse to reach the vehicle decreases.
We also measured the percentage of the number of parking spaces requested
by the user that are collected and returned by the agent. For all the occu-
pancy rates, the result obtained was 100% (i.e., information about all the spaces
requested are found by the agent), with the exception of when there are no avail-
able parking spaces, where the result was 0%. As we increase the occupancy rate
by 10% increments, the following worse case scenario evaluated corresponded to
an occupancy of parking spaces of 90%, but with this rate there were still enough
available parking spaces to satisfy the query (more speciﬁcally, 22).
3.4
Inﬂuence of the Number of Requested Available Parking Spaces
In this experiment, we vary the number of available parking spaces that the
driver wants to retrieve. The parking occupancy ratio is set to 90%, as this is

88
O. Urra and S. Ilarri
a quite challenging scenario. The number of available parking spots in the area
is initially 22, so we vary the requested parking spaces from 2 to 22, and if the
agent cannot ﬁnd enough available parking spaces within the searching area, it
will enlarge it according to the parameter warning timeout. Figure 4(a) shows the
time needed to solve the query. As expected, the higher the number of parking
spots to collect, the higher the time needed by the mobile agent to complete the
process. However, the query processing times are not excessive in any case.
(a) Time to solve the query
(b) Number of hops
(c) Vehicles processed
Fig. 4. Inﬂuence of the number of requested available parking spaces
Figure 4(b) shows the number of hops performed by the agent. When the
number of requested parking spaces increases, the number of hops also increases,
since the agent needs to visit more vehicles. Consistently with this result,
Fig. 4(c) shows the number of relevant vehicles that the mobile agent needs
to visit, which grows, since the mobile agent needs to visit an increasing number
of them to ﬁnd the information they have about the existing available parking
spots.
Finally, the percentage of collected data was 100% in all the cases, with the
exception of the case of 22 places, where a 98% of the requested parking spaces
(i.e., around 21.5 places, on average) were found despite the eﬀorts performed
by the mobile agent, due to the randomness of the information available in the
vehicles and the high parking occupancy.
4
Related Work
Most research on query processing in vehicular networks has focused on push-
based approaches (e.g., [1]) and only a few works consider pull-based approaches
instead (e.g., [3]). The main reason is higher simplicity: push-based approaches
avoid some challenges that appear when a query is disseminated in a VANET
and the results need to be collected and communicated to the originating vehicle.
Moreover, the use of mobile agents for query processing in vehicular networks
has not been studied in depth, except for our previous work presented in [10].
Helping drivers to ﬁnd parking spaces is a topic that has received consider-
able research attention. So, smart parking systems can be designed and deployed
to provide information about parking spaces in speciﬁc areas (e.g., see [5]). How-
ever, these infrastructure-based solutions are quite expensive and not available

Dear Mobile Agent, Could You Please Find Me a Parking Space?
89
globally. On the contrary, it would be interesting to have solutions that are
ﬂexible enough to obtain information about any available on-street parking.
This motivated the development of proposals that exploit data dissemination in
VANETs [4]. Information about the availability of parking spaces can be quite
volatile, and some proposals try to take this into account. In [2], an alloca-
tion protocol is proposed for sharing information about available parking spaces
using only ad hoc communications. This proposal guarantees that the informa-
tion about a parking space is provided only to a single driver, which avoids
competition problems if several drivers receive alerts about the same parking
space.
Up to the authors’ knowledge, existing proposals to provide information
about available parking spaces are push-based, so they cannot be used in sce-
narios like the one studied in this paper. Moreover, the use of mobile agents to
search available parking spaces is also new. Finally, the experimental evaluations
focus either on urban scenarios or highways, whereas we have considered a mixed
scenario that includes both an urban area and an inter-urban region.
5
Conclusions and Future Work
In this paper, we have presented an approach that uses the technology of mobile
agents to ﬁnd available parking places in a city area by looking in situ for that
information among the data collected individually by the vehicles that circulate
in that area. As opposed to the existing related work, the novelty of our study
resides in the use of a pull-based approach to query about parking spaces in
areas that are not located near the driver that submits the query (thus allowing
new interesting use cases), the use of mobile agents in a parking space searching
scenario, and the experimental evaluation with real maps combining both city
and interurban areas. The experimental results presented show the performance
of the proposal in diﬀerent conditions and its feasibility.
As a future work, we plan to improve the behavior of the mobile agent to
complete the query in less time and with a better use of the available bandwidth.
Acknowledgments. This work has been supported by the projects TIN2016-78011-
C4-3-R (AEI/FEDER, UE), TIN2013-46238-C4-4-R, and DGA-FSE.
References
1. Cenerario, N., Delot, T., Ilarri, S.: A content-based dissemination protocol for
VANETs: exploiting the encounter probability. IEEE Trans. Intell. Transp. Syst.
12(3), 771–782 (2011)
2. Delot, T., Ilarri, S., Lecomte, S., Cenerario, N.: Sharing with caution: managing
parking spaces in vehicular networks. Mob. Inf. Syst. 9(1), 69–98 (2013)
3. Delot, T., Mitton, N., Ilarri, S., Hien, T.: GeoVanet: a routing protocol for query
processing in vehicular networks. Mob. Inf. Syst. 7(4), 329–359 (2011)
4. Ilarri, S., Delot, T., Trillo-Lado, R.: A data management perspective on vehicular
networks. IEEE Commun. Surv. Tutor. 17(4), 2420–2460 (2015)

90
O. Urra and S. Ilarri
5. Kotb, A.O., Shen, Y.C., Huang, Y.: Smart parking guidance, monitoring and reser-
vations: a review. IEEE Intell. Transp. Syst. Mag. 9(2), 6–16 (2017)
6. Martinez, F.J., Fogue, M., Toh, C.K., Cano, J.C., Calafate, C.T., Manzoni, P.:
Computer simulations of VANETs using realistic city topologies. Wirel. Pers. Com-
mun. 69(2), 639–663 (2013)
7. Olariu, S., Weigle, M.C.: Vehicular Networks: From Theory to Practice, 1st edn.
Chapman & Hall/CRC, Boca Raton (2009)
8. Trillo, R., Ilarri, S., Mena, E.: Comparison and performance evaluation of mobile
agent platforms. In: The Third International Conference on Autonomic and
Autonomous Systems (ICAS 2007), pp. 41–46. IEEE Computer Society (2007)
9. Urra, O., Ilarri, S.: MAVSIM: testing VANET applications based on mobile agents.
In: Vegni, A.M., Agrawal, D.P. (eds.) Cognitive Vehicular Networks, pp. 199–224.
CRC Taylor and Francis Group, Boca Raton (2016)
10. Urra, O., Ilarri, S., Trillo-Lado, R.: An approach driven by mobile agents for data
management in vehicular networks. Inf. Sci. 381, 55–77 (2017)
11. Uzcategui, R.A., Sucre, A.J.D., Acosta-Marum, G.: Wave: a tutorial. IEEE Com-
mun. Mag. 47(5), 126–133 (2009)

P2P Deductive Databases: Well Founded
Semantics and Distributed Computation
L. Caroprese and E. Zumpano(B)
DIMES, University of Calabria, 87036 Rende, Italy
{l.caroprese,e.zumpano}@dimes.unical.it
Abstract. This paper stems from previous works of the same authors in
which a declarative semantics for Peer-to-Peer (P2P) systems, deﬁned in
terms of Preferred Weak Models, is proposed. Under this semantics only
facts not making the local databases inconsistent can be imported. As in
the general case a P2P system may admit many preferred weak models
whose computational complexity is prohibitive, the paper looks for a
more pragmatic solution. It assigns to a P2P system its Well Founded
Model, a partial deterministic model that captures the intuition that if
an atom is true in a preferred weak model, but it is false in another
one, then it is undeﬁned in the well founded model. The paper presents
a distributed algorithm for the computation of the well founded model
and a system prototype.
1
Introduction
Several proposals considering the issue of managing the coordination, the inte-
gration of information [4–6,9,13,18,20] as well as the computation of queries in
a P2P system have been proposed in the literature [2,11]. This paper follows the
proposal in [6–10] in which a diﬀerent interpretation of mapping rules has led
to the proposal of a semantics for a P2P system deﬁned in terms of Preferred
Weak Models. Under this semantics only facts not making the local databases
inconsistent can be imported, and the preferred weak models are the consistent
scenarios in which peers import maximal sets of facts not violating constraints.
Example 1. Consider a P2P in which the peer: (i) P3 contains two atoms: r(a)
and r(b); (ii) P2 imports data from P3 using the (mapping) rule q(X) ← r(X)1.
Moreover imported atoms must satisfy the constraint ←q(X), q(Y ), X ̸= Y
stating that the relation q may contain at most one tuple; (iii) P1 imports data
from P2, using the (mapping) rule p(X) ← q(X). P1 also contains the rules
s ←p(X) stating that s is true if the relation p contains at least one tuple, and
t ←p(X), p(Y ), X ̸= Y , stating that t is true if the relation p contains at least
two distinct tuples.
The intuition is that, with r(a) and r(b) true in P3, either q(a) or q(b) could
be imported in P2 and, consequently, only one tuple is imported in the relation
1 Please, note the special syntax we use for mapping rules.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 91–99, 2017.
DOI: 10.1007/978-3-319-67162-8 11

92
L. Caroprese and E. Zumpano
p of the peer P1. Note that whatever is the derivation in P2, s is derived in P1
while t is not derived. Therefore, s and t are, respectively, true and false in P1. ⊓⊔
A P2P system may admits many preferred weak models and the computa-
tional complexity is prohibitive. Therefore, a more pragmatic solution is needed.
The paper ﬁrst introduces a rewriting technique that allows modeling a P2P
system PS as a unique logic program, Rewt(PS), that can be used as a com-
putational vehicle to calculate the semantics of the P2P system; then presents
the Well Founded Model Semantics, that allows obtaining a deterministic model
whose computation is polynomial time.
Moreover, the paper presents a distributed algorithm for the computation of
the well founded model and provides some details on the implementation of a
system prototype for query answering in P2P network based on the proposed
semantics.
2
P2P Systems: Syntax and Semantics
Familiarity is assumed with deductive database [1], logic programming, stable
models, head cycle free (HCF) program, well founded model and computational
complexity [3,12,16,17,19]. A (peer) predicate symbol is a pair i : p, where i is
a peer identiﬁer and p is a predicate symbol. A (peer) atom is of the form i : A,
where i is a peer identiﬁer and A is a standard atom. A (peer) literal is a peer
atom i : A or its negation not i : A. A conjunction i : A1, . . . , i : Am, not i :
Am+1, . . . , not i : An, φ, where φ is a conjunction of built-in atoms, will be also
denoted as i : B, with B equals to A1, . . . , Am, not Am+1, . . . , not An, φ.
A (peer) rule can be of one of the following three types:
– standard rule. It is of the form i : H ←i : B, where i : H is an atom and
i : B is a conjunction of atoms and built-in atoms.
– integrity constraint. It is of the form ←i : B, where i : B is a conjunc-
tion of literals and built-in atoms.
– mapping rule. It is of the form i : H ← j : B, where i : H is an atom, j : B
is a conjunction of atoms and built-in atoms and i ̸= j.
i : H is called head while i : B (resp. j : B) is called body. Negation is allowed just
in the body of integrity constraints. The deﬁnition of a predicate i:p consists of
the set of rules in whose head the predicate symbol i:p occurs. A predicate can be
of three diﬀerent kinds: base predicate, derived predicate and mapping predicate.
A base predicate is deﬁned by a set of ground facts; a derived predicate is deﬁned
by a set of standard rules and a mapping predicate is deﬁned by a set of mapping
rules. An atom i : p(X) is a base atom (resp. derived atom, mapping atom) if
i : p is a base predicate (resp. standard predicate, mapping predicate). Given
an interpretation M, M[D] (resp. M[LP], M[MP]) denotes the subset of base
atoms (resp. derived atoms, mapping atoms) in M.

P2P Deductive Databases
93
Deﬁnition 1. P2P System. A peer Pi is a tuple ⟨Di, LPi, MPi, ICi⟩, where
(i) Di is a set of facts (local database); (ii) LPi is a set of standard rules; (iii)
MPi is a set of mapping rules and (iv) ICi is a set of constraints over predicates
deﬁned by Di, LPi and MPi. A P2P system PS is a set of peers {P1, . . . , Pn}. □
Given a P2P system PS = {P1, . . . , Pn}, where Pi = ⟨Di, LPi, MPi, ICi⟩,
D, LP, MP and IC denote, respectively, the global sets of ground facts, stan-
dard rules, mapping rules and integrity constraints, i.e. D = 
i∈[1..n] Di,
LP = 
i∈[1..n] LPi, MP = 
i∈[1..n] MPi and IC = 
i∈[1..n] ICi. In the rest of
this paper, with a little abuse of notation, PS will be also denoted both with
the tuple ⟨D, LP, MP, IC⟩and the set D ∪LP ∪MP ∪IC.
We now review the Preferred Weak Model semantics in [6,7]. For each peer
Pi = ⟨Di, LPi, MPi, ICi⟩, the set Di ∪LPi is a positive normal program, thus
it admits just one minimal model that represents the local knowledge of Pi. It is
assumed that each peer is locally consistent, i.e. its local knowledge satisﬁes ICi
(i.e. Di ∪LPi |= ICi). Therefore, inconsistencies may be introduced just when
the peer imports data. The intuitive meaning of a mapping rule i : H ← j : B ∈
MPi is that if the body conjunction j : B is true in the source peer Pj the atom
i : H can be imported in Pi only if it does not imply (directly or indirectly) the
violation of some constraint in ICi.
Given a mapping rule r = H ← B, the corresponding standard logic rule
H ←B will be denoted as St(r). Analogously, given a set of mapping rules MP,
St(MP)= {St(r) | r ∈MP} and given a P2P system PS = D∪LP ∪MP ∪IC,
St(PS) = D ∪LP ∪St(MP) ∪IC.
Given an interpretation M, an atom H and a conjunction of atoms B:
– valM(H ←B) = valM(H) ≥valM(B),
– valM(H ← B) = valM(H) ≤valM(B).
Therefore, if the body is true, the head of a standard rule must be true,
whereas the head of a mapping rule could be true. Intuitively, a weak model M is
an interpretation that satisﬁes all standard rules, mapping rules and constraints
of PS and such that each atom H ∈M[MP] (i.e. each mapping atom) is
supported from a mapping rule H ← B whose body B is satisﬁed by M. A
preferred weak model is a weak model containing a maximal subset of mapping
atoms.
Deﬁnition 2. (Preferred) Weak Model. Given
a
P2P
system PS =
D ∪LP ∪MP ∪IC, an interpretation M is a weak model for PS if {M} =
MM(St(PSM)), where PSM is the program obtained from ground(PS) by
removing all mapping rules whose head is false w.r.t. M. Given two weak models
M and N, M is said to preferable to N, and is denoted as M ⊒N, if M[MP] ⊇
N[MP]. Moreover, if M ⊒N and N ̸⊒M, then M ⊐N. A weak model M is
said to be preferred if there is no weak model N such that N ⊐M.
The set of weak models for a P2P system PS will be denoted by WM(PS),
whereas the set of preferred weak models will be denoted by PWM(PS).
⊓⊔

94
L. Caroprese and E. Zumpano
Theorem 1. For every consistent P2P system PS, PWM(PS) ̸= ∅.
Example 2. Consider a P2P system PS in which the peer: P2 contains the
facts q(a) and q(b), whereas P1 contains the mapping rule p(X) ← q(X)
and the constraint ←p(X), p(Y ), X ̸= Y . WM(PS) are: M0 = {q(a), q(b)},
M1 = {q(a), q(b), p(a)} and M2 = {q(a), q(b), p(b)}, whereas PWM(PS) are
M1 and M2 as they import maximal sets of atoms from P2.
⊓⊔
3
Computing the Preferred Weak Model Semantics
This section presents an alternative characterization of the preferred weak model
semantics, that allows to model a P2P system PS with a single logic program
Rewt(PS). Let’s ﬁrstly introduce some preliminaries. Given an atom A = i :
p(x), At denotes the atom i : pt(x) and Av denotes the atom i : pv(x). At will
be called the testing atom, whereas Av will be called the violating atom.
Deﬁnition 3. Given a conjunction
B = A1, . . . , Ah, not Ah+1, . . . , not An, B1, . . . , Bk, not Bk+1, . . . , not Bm, φ (1)
where Ai (i ∈[1.. n]) is a mapping atom or a derived atom, Bi (i ∈[1.. m]) is a
base atom and φ is a conjunction of built in atoms, we deﬁne
Bt = At
1, . . . , At
h, not At
h+1, . . . , not At
n, B1, . . . , Bk, not Bk+1, . . . , not Bm, φ
(2)
Therefore, given a negation free conjunction B
=
A1, . . . , Ah, B1, . . . ,
Bk, . . . , φ, then Bt = At
1, . . . , At
h, B1, . . . , Bk, φ. In the following, the rewriting of
a P2P system is reported.
Deﬁnition 4. Rewriting of an integrity constraint. Given an integrity
constraint i =
←B (that is of the form (1)) its
rewriting
is
deﬁned
as
Rewt(i) = {Av
1 ∨· · · ∨Av
h ←Bt}.
⊓⊔
If Bt (of the form (2)), is true, at least one of the violating atoms Av
1, . . . , Av
h
is true. Therefore, at least one of the atoms A1, . . . , Ah cannot be inferred.
Deﬁnition 5. Rewriting of a standard rule. Given a standard rule s =
H ←B, its rewriting is deﬁned as Rewt(s) = {H ←B; Ht ←Bt; Av
1∨· · ·∨Av
h ←
Bt, Hv }.
⊓⊔
In order to ﬁnd the mapping atoms that, if imported, generate some inconsis-
tencies (i.e. in order to ﬁnd their corresponding violating atoms), all possible
mapping testing atoms are imported and the derived testing atoms are inferred.
In the previous deﬁnition, if Bt is true and the violating atom Hv is true, then the
body of the disjunctive rule is true and therefore it can be deduced that at least
one of the violating atoms Av
1, . . . , Av
h is true (i.e. to avoid such inconsistencies
at least one of atoms A1, . . . , Ah cannot be inferred).

P2P Deductive Databases
95
Deﬁnition 6. Rewriting of a mapping rule.Given a mapping rule m =
H ← B, its rewriting is deﬁned as Rewt(m) = {Ht ←B; H ←Ht, not Hv }. ⊓⊔
Intuitively, to check whether a mapping atom H generates some inconsisten-
cies, if imported in its target peer, a testing atom Ht is imported in the same
peer. Rather than violating some integrity constraint, it (eventually) generates,
by rules obtained from the rewriting of standard rules and integrity constraints,
the atom Hv. In this case H, cannot be inferred and inconsistencies are pre-
vented.
Deﬁnition 7. Rewriting of a P2P system. Given a P2P system PS =
D ∪LP ∪MP ∪IC, then: Rewt(MP) = 
m∈MP Rewt(m), Rewt(LP) =

s∈LP Rewt(s), Rewt(IC) =

i∈IC Rewt(i) and Rewt(PS) = D∪Rewt(LP)∪
Rewt(MP) ∪Rewt(IC)
⊓⊔
Deﬁnition 8. Total Stable Model. Given a P2P system PS and a stable
model M for Rewt(PS), the interpretation obtained by deleting from M its
violating and testing atoms, denoted as T (M), is a total stable model of PS.
The set of total stable models of PS is denoted as T SM(PS).
⊓⊔
Example 3. Consider the P2P system PS in Example 2. From Deﬁnition (7) we
obtain:
Rewt(PS) ={q(a); q(b); pt(X) ←q(X); p(X) ←pt(X), not pv(X);
pv(X) ∨pv(Y ) ←pt(X), pt(Y ), X ̸= Y }
The stable models of Rewt(PS) are: M1 = {q(a), q(b), pt(a), pt(b), pv(a), p(b)},
M2 = {q(a), q(b), pt(a), pt(b), p(a), pv(b)}. Then, the total stable models of PS
are T SM(PS) = {{q(a), q(b), p(b)}, {q(a), q(b), p(a)}}.
⊓⊔
Theorem 2. For every P2P system PS, T SM(PS) = PWM(PS).
⊓⊔
4
Well Founded Semantics and Distributed Computation
A P2P system may admit many preferred weak models whose computational
complexity has been shown to be prohibitive [6,7]. Therefore, a deterministic
model whose computation is guaranteed to be polynomial time is needed. In
more details, the rewriting presented in Sect. 3 allows modeling a P2P system by
a single disjunctive logic program. By assuming that this program is HCF, it can
be rewritten into an equivalent normal program for which a Well Founded Model
Semantics can be adopted. Such a semantics allows to compute in polynomial
time a deterministic model describing the P2P system, by capturing the intuition
that if an atom is true in a total stable (or preferred weak) model of PS and is
false in another one, then it is undeﬁned in the well founded model.
Theorem 3. Let PS = D ∪LP ∪MP ∪IC be a P2P system, then Rewt(PS)
is HCF iﬀthere are not two distinct atoms occurring in the body of a rule in
ground(LP ∪IC) mutually dependent by positive recursion.
⊓⊔
we assume each P2P system PS is s.t. Rewt(PS) is HCF. PS will be called as
HCF P2P system. From previous hypothesis, it follows that Rewt(PS) can be
normalized as SM(Rewt(PS)) = SM(Normalized(Rewt (PS))).

96
L. Caroprese and E. Zumpano
Deﬁnition 9. Rewriting of an HCF P2P system. Given an HCF P2P
system PS, Reww(PS) = Normalized(Rewt(PS)).
⊓⊔
Therefore, the preferred weak models of an HCF P2P system PS corresponds to
the stable models of the normal program Reww(PS). Next step is to adopt for
Reww(PS) a three-valued semantics that allows computing deterministic models
and in particular the well founded model.
Deﬁnition 10. Well Founded Semantics. Given an HCF P2P system PS
and the well founded model of Reww(PS), say ⟨T, F⟩, the well founded model
semantics of PS is given by ⟨T (T), T (F)⟩.
⊓⊔
Example 4. The rewriting of the HCF P2P system PS in Example 3 is:
Reww(PS) ={q(a); q(b); pt(X) ←q(X); p(X) ←pt(X), not pv(X);
pv(X) ←pt(X), pt(Y ), X ̸= Y, not pv(Y );
pv(Y ) ←pt(X), pt(Y ), X ̸= Y, not pv(X)}
The well founded model of Reww(PS) is ⟨{q(a), q(b), pt(a), pt(b)}, ∅⟩and the
well founded semantics of PS is given by ⟨{q(a), q(b)}, ∅⟩. The atoms q(a) and
q(b) are true, while the atoms p(a) and p(b) are undeﬁned.
⊓⊔
Theorem 4. Let PS be a HCF P2P system, then deciding: (i) whether an inter-
pretation M is a preferred weak model of PS is P-time; (ii) whether an atom
A is true in some preferred weak model of PS is NP-complete; (iii) whether
an atom A is true in every preferred weak model of PS is coNP-complete; (iv)
whether an atom A is true in the well founded model of PS is P-time.
⊓⊔
Reww(PS) allows to compute the well founded semantics of PS in polyno-
mial time. In this section we present a technique allowing to compute the well
founded model in a distributed way. The basic idea is that each peer computes
its own portion of the “unique logic program”, sending to the other peers the
result.
Deﬁnition 11. Let PS= be an HCF P2P system, where Pi = ⟨Di, LPi, MPi,
ICi⟩, for i ∈[1..n]. Then, Reww(Pi) = Reww(Di ∪LPi ∪MPi ∪ICi).
Previous deﬁnition allows to derive a single normal logic program for each
peer. Observe that, Reww(PS) = 
i∈[1..n] Reww(Pi).
Example 5. The rewritings located on peers P1 and P2 of Example 2 are:
Reww(P1)
= {pt(X) ←q(X); p(X) ←pt(X), not pv(X);
pv(X) ←pt(X), pt(Y ), not pv(Y ), X ̸= Y ;
pv(Y ) ←pt(X), pt(Y ), not pv(X), X ̸= Y }.
Reww(P2)
= {q(a); q(b)}.
⊓⊔
The idea is the following: if a peer receives a query, then it will recursively
query the peer to which it is connected through mapping rules, before being able
to calculate its answer. Formally, a local query submitted by a user to a peer does

P2P Deductive Databases
97
not diﬀer from a remote query submitted by another peer. Once retrieved the
necessary data from neighbor peers, the peer computes its well founded model
and evaluates the query (either local or remote) on that model; then if the query
is a remote query, the answer is sent to the requesting peer. For the sake of
presentation, a partial model reports, using the syntax [T, U], the sets of true
and undeﬁned atoms, instead of the sets of true and false atoms.
Example 6. Consider the P2P system in Example 2. If P1 receives the query
p(X), it submits the query p(X) to the peer P2. Once P2 receives the query
q(X), it computes its well founded model W2 = [{q(a), q(b)}, ∅]. P1 receives
the data, populates its local database and computes its well founded model
W1 = [{pt(a), pt(b)}, {pv(a), pv(b), p(a), p(b)}] and ﬁnally, evaluates the query
p(X) over W1. The answer will be [∅, {p(a), p(b)}]. Observe that, the peer replies
providing two undeﬁned atoms: p(a) and p(b).
⊓⊔
Algorithm: ComputeAnsweri
input: 1) i : q(X) - a query
2) s - a sender which is a peer Pk (remote query) or null (local query)
output: [T, U] where T (resp. U) is the set of true (resp. undeﬁned) atoms
begin
while true
wait for an input i : q(X) and s;
P = Reww(Pi);
for each (i : h(X) ← j : b(X)) ∈MPi
[T, U] = ComputeAnswerj(j : b(X), Pi);
P = P ∪T ∪{a ←not a | a ∈U};
end for
[Tw, Uw] = ComputeWellFoundedModel(P);
answer = [{i : q(x) | i : q(x) ∈Tw}, {i : q(x) | i : q(x) ∈Uw}];
if isNull(s) then show(answer);
else send(answer, Pk);
end if
end while
end
We associate to a P2P system, PS, a graph g(PS) whose nodes are the peers
of PS and whose edges are the pairs (Pi, Pj) s.t. (i : H ← j : B) ∈PS. We
say that PS is acyclic if g(PC) is acyclic. In order to guarantee the termination
of the computation, from now on we assume that our P2P systems are acyclic.
Without loss of generality, we assume that each mapping rule is of the form
i : p(X) ← j : q(X). The behavior of the peer Pi = ⟨Di, LPi, MPi, ICi⟩within
the P2P system can be modeled by the algorithm ComputeAnsweri, running on
the peer Pi. It receives in input a query i : q(X) submitted by a sender s which
can be another peer or a user. For each mapping rule i : h(X) ← j : b(X), the
peer Pi queries the peer Pj asking for j : b(X). Pi will receive true and undeﬁned
tuples that will enrich the local knowledge. Observe that, true atoms will be
simply inserted into P while for each undeﬁned atom a, a rule a ←not a allowing
to infer the correct truth value for a (undeﬁned) in the well founded model, is
inserted. Once the local knowledge has been updated, the local well founded

98
L. Caroprese and E. Zumpano
model is computed by means of the function ComputeWellFoundedModel and
the answer for the query is extracted. If the sender is a user then the answer is
simply shown, otherwise (if it is a peer) it is sent back to it.
A system prototype for query answering in a P2P network based on the
proposed semantics has benn implemented. The system has been developed using
Java.
The communication among peers is performed by using JXTA libraries [15].
A peer is a node in the JXTA network. JXTA deﬁnes a set of protocols providing
implementation for basic and complex P2P functionalities allowing each device
in the network to communicate and interact as a peer and guarantees some
advantages in the development of P2P applications (e.g. it can be run on diﬀerent
digital devices (PCs, PDAs)). XSB [21] is a logic programming and deductive
database system used for computing the answer to a given query using the well
founded semantics. InterProlog [14] is an open source front-end that provides
Java with the ability to interact with XSB engine.
References
1. Abiteboul, S., Hull, R., Vianu, V.: Foundations of Databases. Addison-Wesley,
Boston (1994)
2. Bertossi, L., Bravo, L.: Query answering in peer-to-peer data exchange systems.
In: Lindner, W., Mesiti, M., T¨urker, C., Tzitzikas, Y., Vakali, A.I. (eds.) EDBT
2004. LNCS, vol. 3268, pp. 476–485. Springer, Heidelberg (2004). doi:10.1007/
978-3-540-30192-9 47
3. Ben-Eliyahu, R., Dechter, R.: Propositional Semantics for Disjunctive Logic Pro-
grams. In: JICSLP, pp. 813–827 (1992)
4. Cal`ı, A., Calvanese, D., De Giacomo, G., Lenzerini, M.: On the decidability and
complexity of query answering over inconsistent and incomplete databases. In:
PODS, pp. 260–271 (2003)
5. Calvanese, D., De Giacomo, G., Lenzerini, M., Rosati, R.: Logical foundations of
peer-to-peer data integration. In: PODS, pp. 241–251 (2004)
6. Caroprese, L., Greco, S., Zumpano, E.: A logic programming approach to querying
and integrating P2P deductive databases. In: FLAIRS, pp. 31–36 (2006)
7. Caroprese, L., Molinaro, C., Zumpano, E.: Integrating and querying P2P deductive
databases. In: IDEAS, pp. 285–290 (2006)
8. Caroprese, L., Zumpano, E.: Consistent data integration in P2P deductive data-
bases. In: Prade, H., Subrahmanian, V.S. (eds.) SUM 2007. LNCS (LNAI), vol.
4772, pp. 230–243. Springer, Heidelberg (2007). doi:10.1007/978-3-540-75410-7 17
9. Caroprese, L., Zumpano, E.: Modeling cooperation in P2P data management
systems. In: An, A., Matwin, S., Ra´s, Z.W., ´Sl ezak, D. (eds.) ISMIS 2008.
LNCS (LNAI), vol. 4994, pp. 225–235. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-68123-6 25
10. Caroprese, L., Zumpano, E.: Handling preferences in P2P systems. In: Lukasiewicz,
T., Sali, A. (eds.) FoIKS 2012. LNCS, vol. 7153, pp. 91–106. Springer, Heidelberg
(2012). doi:10.1007/978-3-642-28472-4 6
11. Franconi, E., Kuper, G., Lopatenko, A., Seraﬁni, L.: A robust logical and com-
putational characterisation of peer-to-peer database systems. In: Aberer, K.,
Koubarakis, M., Kalogeraki, V. (eds.) DBISP2P 2003. LNCS, vol. 2944, pp. 64–76.
Springer, Heidelberg (2004). doi:10.1007/978-3-540-24629-9 6

P2P Deductive Databases
99
12. Gelfond, M., Lifschitz, V.: The stable model semantics for logic programming. In:
Proceedings of Fifth Conference on Logic Programming, pp. 1070–1080 (1998)
13. Halevy, A., Ives, Z., Suciu, D., Tatarinov, I.: Schema mediation in peer data man-
agement systems. In: Interenational Conference on Database Theory, pp. 505–516
(2003)
14. InterProlog. http://www.declarativa.com/interprolog/
15. Project JXTA. http://www.jxta.org/
16. Lone, Z., Truszczy´nski, M.: On the problem of computing the well-founded seman-
tics. In: Lloyd, J., Dahl, V., Furbach, U., Kerber, M., Lau, K.-K., Palamidessi,
C., Pereira, L.M., Sagiv, Y., Stuckey, P.J. (eds.) CL 2000. LNCS, vol. 1861, pp.
673–687. Springer, Heidelberg (2000). doi:10.1007/3-540-44957-4 45
17. Lloyd, J.W.: Foundations of Logic Programming. Springer-Verlag, Heidelberg
(1987)
18. Madhavan, J., Halevy, A.Y.: Composing mappings among data sources. In: VLDB,
pp. 572–583 (2003)
19. Papadimitriou, C.H.: Computational Complexity. Addison-Wesley, Boston (1994)
20. Tatarinov, I., Halevy., A.: Eﬃcient Query reformulation in peer data management
systems. In: SIGMOD, pp. 539–550 (2004)
21. XSB Project. http://xsb.sourceforge.net

Is Distributed Database Evaluation
Cloud-Ready?
Daniel Seybold(B) and J¨org Domaschka
Institute of Information Resource Management, Ulm University, Ulm, Germany
{daniel.seybold,joerg.domaschka}@uni-ulm.de
Abstract. The database landscape has signiﬁcantly evolved over the
last decade as cloud computing enables to run distributed databases on
virtually unlimited cloud resources. Hence, the already non-trivial task
of selecting and deploying a distributed database system becomes more
challenging. Database evaluation frameworks aim at easing this task by
guiding the database selection and deployment decision. The evaluation
of databases has evolved as well by moving the evaluation focus from per-
formance to distribution aspects such as scalability and elasticity. This
paper presents a cloud-centric analysis of distributed database evalua-
tion frameworks based on evaluation tiers and framework requirements.
It analysis eight well adopted evaluation frameworks. The results point
out that the evaluation tiers performance, scalability, elasticity and con-
sistency are well supported, in contrast to resource selection and avail-
ability. Further, the analysed frameworks do not support cloud-centric
requirements but support classic evaluation requirements.
Keywords: NoSQL · Distributed database · Database evaluation ·
Cloud
1
Introduction
Relational database management systems (RDBMS) have been the common
choice for persisting data for many decades. Yet, the database landscape has
changed over the last decade and a plethora of new database management sys-
tems (DBMS) have evolved, namely NoSQL [20] and NewSQL [15]. These are
promising persistence solutions not only for Web applications, but also for new
domains such as “BigData” and “IoT”. While NewSQL database systems are
inspired by the relational storage model, the storage models of NoSQL database
system can be further classiﬁed into key-value stores, document-oriented stores,
column-oriented stores and graph-oriented stores [20]. NoSQL and NewSQL
DBMS are designed to satisfy requirements such as high performance or scalabil-
ity by running on commodity hardware as a distributed database management
system (DDBMS), providing a single DBMS, which is spread over multiple nodes.
An element of the overall DDBMS is termed database node.
An enabler of the DBMS evolvement is cloud computing by providing fast
access to commodity hardware via elastically, on-demand, self-service resource
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 100–108, 2017.
DOI: 10.1007/978-3-319-67162-8 12

Is Distributed Database Evaluation Cloud-Ready?
101
provisioning [17]. Infrastructure as a Service (IaaS) is the preferable way to
deploy a DDBMS, requiring a high degree of ﬂexibility in compute, storage and
network resources [17].
With the number of available NoSQL and NewSQL systems and cloud
resource oﬀerings, the database selection and deployment on the cloud is a chal-
lenging task. Hence, DBMS evaluation is a common approach to guide these
decisions. With the evolvement of the DBMSs, the landscape of database evalu-
ation frameworks (DB-EFs) has evolved as well: from single node evaluation, e.g.
TPC-E 1 of the Transaction Processing Performance Council (TPC), to DDBMS
evaluation, e.g. the Yahoo Cloud Serving Benchmark (YCSB) [7], adding new
evaluation tiers such as scalability, elasticity or consistency. Yet, these DB-EFs
aim at diﬀerent evaluation tiers and diﬀer towards common DB-EF require-
ments [6,14], especially with respect to cloud computing.
In order to facilitate the selection and deployment of DDBMS in the cloud,
we present an analysis of DB-EF with the focus on exploiting cloud comput-
ing. Our contribution is threefold by (1) deﬁning relevant evaluation tiers for
DDBMS deployed in the cloud; (2) extending existing requirements towards
DDBMS evaluation with cloud speciﬁc requirements; (3) analyse existing evalu-
ation frameworks based on the evaluation tiers and evaluation requirements.
The remainder is structured as follows: Sect. 2 introduces the background on
DBMS evaluation. Section 3 deﬁnes the evaluation tiers while Sect. 4 deﬁnes the
requirements towards evaluation frameworks. Section 5 analysis and discusses
existing frameworks. Section 6 concludes.
2
Background
Evaluating DBMS imposes challenges for the evaluation frameworks itself, which
have been discussed over decades. A ﬁrst, but still valid guideline for evaluating
RDBMS deﬁnes the requirements such as relevance to an application domain,
portability to allow benchmarking of diﬀerent systems, scalability to support
benchmarking large systems, and simplicity to ensure that the results are easy
to understand [14]. A more recent guideline adds the DDBMS and the resulting
challenges for supporting diﬀerent deployment topologies and coordination of
distributed experiments [6]. By adopting these challenges, several DB-EFs have
been established over the years, which are analysed in Sect. 5.
An overview of existing DB-EF focuses on the tiers availability and consis-
tency. Yet, general requirements for DB-EF are not introduced and cloud speciﬁc
characteristics are not considered [11]. An overview of DB-EFs for NoSQL data-
base systems is provided by [19]. Yet, the focus lies on evaluating the data model
capabilities, without taking explicitly into accout DDBMS aspects and the usage
of cloud resources. Evaluating the dimensions of consistency in DDBMS, is also
analysed by
[5], introducing client-centric and data-centric consistency met-
rics. Related DB-EF for consistency are presented and missing features for ﬁne-
grained consistency evaluation are outlined. An overview of DB-EF is included
1 http://www.tpc.org/tpce/.

102
D. Seybold and J. Domaschka
in a recommendation compendiums [16] for distributed database selection based
on functional and non-functional requirement Yet, the compendium considers
only the performance evaluation.
3
Distributed Database Evaluation Tiers
With the evolving heterogeneity in DDBMSs their evaluation becomes even
more challenging. DBMS evaluation is driven by workload domains (WD).
On a high level WDs can classiﬁed into transactional (TW) [9], web-oriented
(WOW) [9], Big Data (BDW) [12] and synthetic workloads (SW) [7]. These
WDs drive the need for considering various evaluation tiers, which are distilled
out of database and cloud research.
Resource selection (RS) determines the best matching resources to run a
DBMS. For traditional RDBMSs the focus lies on single node resources,
(CPU, memory, storage). For DDBMSs network, locality and number of nodes
became important factors. By using cloud resources for a DBMS, the cloud
providers tend to oﬀer more heterogeneous resources such as VMs with dedi-
cated storage architectures2, container based resources3, or dedicated resource
locations from data center to physical host level.
Performance (P) evaluates the behaviour of a DBMS against a speciﬁc kind
of workload. Performance metrics are throughput and latency, which are mea-
sured by the evaluation framework.
Scalability (S) deﬁnes the capability to process arbitrary workload sizes by
adapting the DBMS by scaling vertically (scale-up/down) or horizontally
(scale-in/out) [1]. Scaling vertically changes the computing resources of a
single node. Horizontal scaling adds nodes to a DDBMS cluster (scale-out) or
removes nodes (scale-in) In the following the term scalability implies horizon-
tal scalability Measuring scalability is performed by correlating throughput
and latency for growing cluster sizes and workloads. A high scalability rating
is represented by constant latency and proportionally growing throughput
with respect to the number of nodes and the workload size [7].
Elasticity (E) deﬁnes the ability to cope with sudden workload ﬂuctuations
without service disruption [1]. Elasticity metrics are speedup and scaleup [7].
Speedup refers to the required time for a scaling action, i.e. adapting the
cluster size, redistributing data and stabilising the cluster. Scaleup refers
to the beneﬁt of this action, i.e. the throughput/latency development with
respect to the workload ﬂuctuation.
Availability (A) represents the degree to which a DBMS is operational and
accessible when required for use. The availability of a DBMS can be aﬀected
by overload (issuing more requests in parallel than theDBMS can handle
or failures on the resource layer (a node failure). With respect to failures,
DDBMSs apply replication of data to multiple database nodes. A common
2 https://aws.amazon.com/de/ec2/instance-types/.
3 https://wiki.openstack.org/wiki/Magnum.

Is Distributed Database Evaluation Cloud-Ready?
103
metric to measure availability with respect to node failures are the takeover
time, and the performance impact.
Consistency (C) Distributed databases oﬀer diﬀerent consistency guarantees
as there is trade-oﬀbetween consistency, availability and partitioning, i.e. the
CAP theorem [13]. Consistency can be evaluated client-centric (i.e. from the
application developer perspective) and data-centric (i.e. from the database
administrator perspective) [5] . Here, we only consider client-centric consis-
tency that can be classiﬁed into staleness and ordering [5]. Staleness deﬁnes
how much a replica lags behind its master. It is measured either in time or
versions. Ordering deﬁnes all requests must be executed on all replicas in the
same chronological order.
4
Evaluation Frameworks Requirements
Besides the evaluation tiers, DBMS evaluation imposes requirements towards
the DB-EF itself. We brieﬂy present established requirements [6,14] as well as
novel cloud-centric requirements.
Usability (U) eases the framework conﬁguration, execution and extension by
providing suﬃcient documentation and tools to run the evaluation. Hence,
the evaluation process has to be transparent to provide objective results [14].
Distribution/Scalability (D/S) is provided by distributed workload gener-
ation, i.e. the framwork clients can be distributed across multiple nodes in
order to increasing the workload by utilising an arbitrary amount of clients [6].
Measurements Processing (MP) deﬁnes that measurements are gathered
not only in an aggregated but also in a ﬁne-grained manner for further
processing [6]. As the amount of measurements can grow rapidly for multiple
or long running evaluation runs, ﬁle-based persistence might not be suﬃcient.
Hence, advanced persistence options such as time series databases (TSDBS),
will ease the dedicated processing and visualisation.
Monitoring (MO) data improves the signiﬁcance of evaluation results. Hence,
monitoring of the involved resources, clients ,and DBMSs should be supported
by the evaluation framework to provide the basis of a thorough analysis. Again
an advanced persistence solution is beneﬁcial.
Database Abstraction (DA) enables the support of multiple DBMSs by
abstracting database driver implementations. Yet, the abstraction degree
needs to be carefully chosen as a too high abstraction might limit speciﬁc
DBMS features and distort results. Therefore, the abstraction interface should
be aligned with the speciﬁed workload scenarios [6].
Client Orchestration (CO) enables automated evaluation runs. Therefore,
the framework should provide tools that orchestrate evaluations, i.e. provi-
sion (cloud) resources, create, execute and collect the results and clean-up
the clients. Hence, CO eases the creation of arbitrary load patterns and the
simulation of multi-tenant workload patterns.

104
D. Seybold and J. Domaschka
Database Orchestration (DO) enables the management of the DDBMSs to
facilitate repetitive evaluation for diﬀerent resources, conﬁgurations and the
adaptation of the DDBMS based on predeﬁned conditions. Hence, the evalu-
ation framework should provide tools to automatically orchestrate DDBMSs,
i.e. provision resources, setup, conﬁgure and adapt generic DDBMSs.
Multi-phase Workloads (MpW) deﬁne the support of multiple workloads
that run in parallel This is crucial to execute advanced evaluation scenarios.
Further, the speciﬁcation of the load development over a certain time frame
per workload is required to simulate real world scenarios.
Extensibility (E) deﬁnes the need to provide an architecture, which eases the
extension of the framework capabilities, e.g. by adding support for additional
DBMSs or workload types.
5
Analysis of Evaluation Frameworks
In this section we analyse DB-EFs, which focus on DDBMSs. Hereby, we consider
only DB-EFs, which have been published within the evolvement of DDBMSs, i.e.
from 2007 on. In addition, we only consider the original evaluation frameworks
and no minor extensions or evaluations based on these frameworks.
First, we analyse each framework based on the workload domain and sup-
ported evaluation tiers. The results are shown in Table 1. Second, we analyse
each frameworks capabilities against the presented DB-EF requirements from
Sect. 4. The results are shown in Table 2. The analysis applies ✗= not supported,
(✓)= partially supported, ✓= supported. A detailed analysis can be found in an
accompanying technical report [21].
Table 1. Distributed database evaluation tiers
Evaluation framework Evaluation tier
WD
RS
P S
E A C
TPC-E
TW
(✓) ✓✓✗
✗
✗
YCSB [7]
SW
✗
✓✓✓✗
✗
YCSB++ [18]
TW, BDW, SW ✗
✓✓✓✗
✓
BG [3]
WOW
✗
✓✓✗
✗
✓
BigBench [12]
TW, BDW
✗
✓✗
✗
✗
✗
OLTP-bench [9]
WOW, SW
✗
✓✓✗
✗
✗
YCSB-T [8]
TW, SW
✗
✓✓✓✗
✓
LinkBench [2]
WOW
✗
✓✗
✗
✗
✗
The ﬁrst insight of our analysis is that performance, scalability, elasticity and
consistency tiers are well covered, but the resource selection and availability tier
lack support (cf. Sect. 5.2). The second insight points out that the traditional
DB-EF requirements [14] such as usability, distribution and extensibility are well
supported, while monitoring and cloud orchestration are not (cf. Sect. 5.2).

Is Distributed Database Evaluation Cloud-Ready?
105
Table 2. Distributed database evaluation tiers
Evaluation framework Evaluation framework requirement
U
D/S MP MO DA CO DO MpW E
TPC-E
✓
✓
(✓) ✗
(✓) ✗
✗
✓
✓
YCSB [7]
✓
✓
(✓) ✗
✓
✗
✗
✗
✓
YCSB++ [18]
(✓) ✓
✓
✓
(✓) ✓
✗
✓
✗
BG [3]
✓
✓
✓
✓
✓
✗
✗
✓
✓
BigBench [12]
✓
✓
✗
✗
(✓) ✗
✗
✗
✓
OLTP-bench [9]
(✓) ✓
(✓) ✓
(✓) ✓
✗
✓
✓
YCSB+T [8]
(✓) ✓
(✓) ✗
✓
✗
✗
✗
✓
LinkBench [2]
✓
(✓)
(✓) ✓
(✓) ✗
✗
✓
✓
5.1
Results for Evaluation Tiers
The resulting table (cf. Table 1) shows that the early DB-EFs focus on per-
formance, scalability and elasticity, while newer frameworks focus as well on
consistency. Hereby, only the performance tier has established common rating
indices such as throughput, latency or SLA based rating [3]. While multiple
frameworks target the scalability and elasticity tier, a common methodology
and rating index has not yet been established. Yet, the need for a common
evaluation methodology [22] and rating index [10] is already carved out.
Currently not supported evaluation tiers are resource selection and avail-
ability. While resource selection is partially supported by TPC-E, which con-
siders physical hardware conﬁgurations, cloud-centric resource selection is not
in the scope of any of the frameworks. With the increasing heterogeneity of
cloud resources from diverse virtual machines oﬀering to even container based
resources, the consideration of cloud-centric resource selection needs to move
into the focus of novel DB-EFs. Yet, existing DB-EFs can be applied to evalu-
ate DDBMS running on heterogeneous cloud resources, but the DB-EFs do not
oﬀer an explicit integration with cloud resource oﬀerings. Hence, manual resource
management, monitoring and client/DDBMS orchestration hinders cloud-centric
evaluations.
As availability is a major feature of DDBMS it is surprising that it is not
considered by the analysed DB-EFs. Especially, as cloud resources do fail, avail-
ability concepts for applications running on cloud resources is widely discussed
topic. Again, the support of DDBMSs orchestration can enable database speciﬁc
availability evaluations.
5.2
Results for Evaluation Framework Requirements
The analysis of the evaluation framework requirements (cf. Table 2) shows that
usability, scalability, database abstraction and extensibility are covered by all

106
D. Seybold and J. Domaschka
frameworks. Measurement processing is covered as well but only a few frame-
works support advanced features such as visualisation and none of the frame-
works supports advanced storage solutions such as TSDBs. Multi-phase work-
loads are partially covered by the frameworks, especially by the frameworks from
the TW and WOW domains. The monitoring of client resources is partially cov-
ered, but only OLTP-bench considers resource monitoring. While all frameworks
support the distributed execution of evaluations, only two support the orches-
tration of clients, which complicates the distributed evaluation runs. Further,
none of the frameworks supports DBMS orchestration. This fact leads to high
complexity only for setting up the evaluation environment, especially when it
comes to heterogeneous cloud resources. Further, dynamic DDBMS transitions
for evaluating tiers such as elasticity or availability, always require custom imple-
mentations, which impedes the comparability and validity of the results.
6
Conclusion and Future Work
In the last decade the landscape of distributed database systems has evolved and
NoSQL and NewSQL database systems appeared. In parallel, cloud computing
enabled novel deployment option for database systems. Yet, these evolvements
raise the complexity in selecting and deploying an appropriate database system.
In order to ease such decisions, several evaluation frameworks for distributed
databases have been developed. In this paper, we presented an analysis of distrib-
uted database evaluation frameworks based on evaluation tiers and requirements
towards the frameworks itself. The analysis is applied to eight evaluation frame-
works and provides a thorough analysis of their evaluation tiers and capabilities.
The results of this analysis shows that the performance, scalability, elasticity,
and consistency tiers are well covered, while resource selection and availability
are not considered by existing evaluation frameworks. With respect to the frame-
work requirements, traditional requirements are covered [14], while cloud-centric
requirements such as orchestration are only partially supported.
The analysis shows, that existing frameworks can be applied to evaluate
distributed databases in the cloud, but there are still unresolved issues on the
evaluation tier side, i.e. the support for resource selection and availability evalua-
tion, and on the framework requirement side, i.e. the orchestration of clients and
databases and exploitation of advanced storage solutions. This hinders repeata-
bility [14] of evaluations on heterogeneous cloud resources as well as dynamic
transition in the cluster. Yet, cloud computing research already oﬀers approaches
to enable automated resource provisioning and application orchestration in the
cloud based on Cloud Orchestration Tools (COTs) [4]. Integrating COT into
evaluation frameworks can be an option to ease the distributed execution of
evaluation runs as well as orchestrating database clusters across diﬀerent cloud
resources. As COTs provide monitoring and adaptation capabilities, they can
ease the evaluation of dynamic cluster transitions by deﬁning advanced evalua-
tion scenarios with dynamic database cluster adaptations.

Is Distributed Database Evaluation Cloud-Ready?
107
Future work will comprise the analysis of COTs with respect to their exploita-
tion in database evaluation frameworks. In addition, the design and implemen-
tation of a cloud-centric database evaluation framework is ongoing.
Acknowledgements. The research leading to these results has received funding from
the EC’s Framework Programme HORIZON 2020 under grant agreement number
644690 (CloudSocket) and 731664 (MELODIC). We thank Moritz Keppler and the
Daimler TSS for their valuable and constructive discussions.
References
1. Agrawal, D., Abbadi, A., Das, S., Elmore, A.J.: Database scalability, elastic-
ity, and autonomy in the cloud. In: Yu, J.X., Kim, M.H., Unland, R. (eds.)
DASFAA 2011. LNCS, vol. 6587, pp. 2–15. Springer, Heidelberg (2011). doi:10.
1007/978-3-642-20149-3 2
2. Armstrong, T.G., Ponnekanti, V., Borthakur, D., Callaghan, M.: Linkbench: a
database benchmark based on the facebook social graph. In: SIGMOD (2013)
3. Barahmand, S., Ghandeharizadeh, S.: Bg: A benchmark to evaluate interactive
social networking actions. In: CIDR (2013)
4. Baur, D., Seybold, D., Griesinger, F., Tsitsipas, A., Hauser, C.B., Domaschka, J.:
Cloud orchestration features: Are tools ﬁt for purpose? In: UCC (2015)
5. Bermbach, D., Kuhlenkamp, J.: Consistency in distributed storage systems. In:
Gramoli, V., Guerraoui, R. (eds.) NETYS 2013. LNCS, vol. 7853, pp. 175–189.
Springer, Heidelberg (2013). doi:10.1007/978-3-642-40148-0 13
6. Bermbach, D., Kuhlenkamp, J., Dey, A., Sakr, S., Nambiar, R.: Towards an exten-
sible middleware for database benchmarking. In: Nambiar, R., Poess, M. (eds.)
Performance Characterization and Benchmarking: Traditional to Big Data. LNCS,
pp. 82–96. Springer, Cham (2015). doi:10.1007/978-3-319-15350-6 6
7. Cooper, B.F., Silberstein, A., Tam, E., Ramakrishnan, R., Sears, R.: Benchmarking
cloud serving systems with ycsb. In: SoCC (2010)
8. Dey, A., Fekete, A., Nambiar, R., Rohm, U.: Ycsb+t: Benchmarking web-scale
transactional databases. In: ICDEW (2014)
9. Difallah, D.E., Pavlo, A., Curino, C., Cudre-Mauroux, P.: Oltp-bench: An exten-
sible testbed for benchmarking relational databases. VLDB 7, 277–288 (2013)
10. Dory, T., Mejias, B., Roy, P., Tran, N.L.: Measuring elasticity for cloud databases.
In: Cloud Computing (2011)
11. Friedrich, S., Wingerath, W., Gessert, F., Ritter, N., Pldereder, E., Grunske, L.,
Schneider, E., Ull, D.: Nosql oltp benchmarking: A survey. In: GI-Jahrestagung
(2014)
12. Ghazal, A., Rabl, T., Hu, M., Raab, F., Poess, M., Crolotte, A., Jacobsen, H.A.:
Bigbench: towards an industry standard benchmark for big data analytics. In:
SIGMOD (2013)
13. Gilbert, S., Lynch, N.: Brewer’s conjecture and the feasibility of consistent, avail-
able, partition-tolerant web services. ACM Sigact News 33, 51–59 (2002)
14. Gray, J.: Benchmark Handbook: For Database and Transaction Processing Sys-
tems. Morgan Kaufmann Publishers Inc, San Francisco (1993)
15. Grolinger, K., Higashino, W.A., Tiwari, A., Capretz, M.A.: Data management in
cloud environments: Nosql and newsql data stores. JoCCASA 2, 22 (2013)

108
D. Seybold and J. Domaschka
16. Khazaei, H., Fokaefs, M., Zareian, S., Beigi-Mohammadi, N., Ramprasad, B.,
Shtern, M., Gaikwad, P., Litoiu, M.: How do i choose the right NoSQL solution?
a comprehensive theoretical and experimental survey. BDIA 2, 1 (2016)
17. Mell, P., Grance, T.: The nist deﬁnition of cloud computing. Technical report,
National Institute of Standards & Technology (2011)
18. Patil, S., Polte, M., Ren, K., Tantisiriroj, W., Xiao, L., L´opez, J., Gibson, G., Fuchs,
A., Rinaldi, B.: Ycsb++: benchmarking and performance debugging advanced fea-
tures in scalable table stores. In: SoCC (2011)
19. Reniers, V., Van Landuyt, D., Raﬁque, A., Joosen, W.: On the state of nosql
benchmarks. In: ICPE (2017)
20. Sadalage, P.J., Fowler, M.: NoSQL Distilled: A Brief Guide to the Emerging World
of Polyglot Persistence. Pearson Education, London (2012)
21. Seybold, D., Domaschka, J.: A cloud-centric survey on distributed database eval-
uation. Technical report. Ulm University (2017)
22. Seybold, D., Wagner, N., Erb, B., Domaschka, J.: Is elasticity of scalable databases
a myth? In: IEEE Big Data (2016)

ADBIS 2017 – Workshops

New Trends in Databases and Information
Systems: Contributions
from ADBIS 2017 Workshops
Andreas Behrend1, Diego Calvanese2, Tania Cerquitelli3(B), Silvia Chiusano3,
Christiane Engels1, St´ephane Jean4, Natalija Kozmina5, B´eatrice Markhoﬀ6,
Oscar Romero7, and Sahar Vahdati1
1 University of Bonn, Bonn, Germany
2 Free University of Bozen-Bolzano, Bolzano, Italy
3 Politecnico di Torino, Turin, Italy
tania.cerquitelli@polito.it
4 ENSMA Poitiers, Poitiers, France
5 University of Latvia, Riga, Latvia
6 University of Tours, Tours, France
7 Universitat Polit`ecnica de Catalunya, Barcelona, Spain
Abstract. In the last few years, research on database and information
system technologies has been rapidly evolving thanks to the new para-
digms of software and hardware adopted by modern scientiﬁc and more
invasive applications. A huge and heterogeneous amount of data should
be eﬃciently stored, managed, and analyzed exploiting proper technolo-
gies for such novel and more interesting data-driven applications. New
and cutting-edge research challenges arise that have been attracting great
attention from both academia and industry. The 21st European Confer-
ence on Advances in Databases and Information Systems (ADBIS 2017),
held on September 24–27, 2017 in Nicosia, Cyprus includes four thematic
workshops covering some emerging issues concerning such new trends
in database and information system research. The aim of this paper is
to present such events, their motivations and topics of interest, as well
as brieﬂy outline their programs including interesting keynotes, invited
papers and a wide range of research, application, and industrial contribu-
tions selected for presentations. The selected papers have been included
in this volume.
1
Introduction
The ADBIS conferences aim at providing a forum for the dissemination of
research accomplishments and promoting interaction and collaboration between
the database and information system research communities from European coun-
tries and the rest of the world. The ADBIS conferences provide an international
platform for the presentation of research on database theory, development of
advanced DBMS technologies, and their applications. The 21st edition of ADBIS,
held on September 24–27, 2017 in Nicosia, Cyprus includes four thematic work-
shops covering emerging and cutting-edge research topics concerning new trends
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 111–122, 2017.
DOI: 10.1007/978-3-319-67162-8 13

112
A. Behrend et al.
in database and information system research. Papers accepted at the ADBIS
main conference span a wide range of topics in the ﬁeld of databases and infor-
mation systems ranging from innovative platforms for data handling to emerging
hardware and software technologies for database and information systems, from
data management on novel architectures (cloud and MapReduce environments)
to interesting and novel machine learning algorithms for eﬀectively support-
ing decision-making process. Meanwhile, the general idea behind each workshop
event was to collect contributions from various domains representing new trends
in the broad research areas of databases and information systems. Speciﬁcally,
the following workshop events have been organized:
– The 1st Workshop on Novel Techniques for Integrating Big Data (BigNovelTI
2017)
– The 1st International Workshop on Data Science: Methodologies and Use-
Cases (DaS 2017)
– The 2nd International Workshop on Semantic Web for Cultural Heritage
(SW4CH 2017)
– The 1st Workshop on Data-Driven Approaches for Analyzing and Managing
Scholarly Data (AMSD 2017)
Each workshop had its own international program committee, whose mem-
bers served as the reviewers of papers included in the rest of this volume. In
the following, for each workshop, a brief introduction of its main motivations
and topics of interest is presented, as well as its program including interesting
keynotes, invited papers and the selected papers for presentations. These papers
have been included in this volume. Finally, some acknowledgements from the
workshop organizers are provided.
2
The 1st Workshop on Novel Techniques for Integrating
Big Data (BigNovelTI 2017)
Introduction. The 1st Workshop on Novel Techniques for Integrating Big Data
(BigNovelTI 2017) has been organised by Diego Calvanese (Free University of
Bozen-Bolzano, Italy) and Oscar Romero (Universitat Polit`ecnica de Catalunya,
Spain).
The challenges posed by Big Data require novel data integration techniques
beyond Data Warehousing (DW), which has been the in-company de-facto
standard for data integration. On the one hand, bottom-up data integration
approaches, performing a virtual integration, are a good ﬁt for integrating dis-
parate autonomous and heterogeneous data sources in large-scale distributed
decision support systems, a current trend to contextualise the in-house data.
Virtual data integration however tends to suﬀer from poor performance, which
hinders the right-time analysis approach, and needs to be adapted and combined
with materialisation to meet the new requirements brought by Big Data. On the
other hand, variety and the need to deal with external non-controlled sources

New Trends in Databases and Information Systems
113
in an automatic way require to look at this problem from a broader perspec-
tive than the one of traditional data management, and semantics need to be
included in the data integration processes. In such setting, domain knowledge is
represented in an ontology, and inference over such knowledge is used to support
data integration. However, this approach poses signiﬁcant computational chal-
lenges that still need to be overcome, and hence its potential has not been fully
exploited yet in real world applications. For these reasons, (i) providing diﬀerent
degrees of coupling of the integrated data sources based on their heterogeneity
and autonomy, and (ii) dealing with and integrating semantics as a ﬁrst-class
citizen are open questions for novel scenarios.
Program. The workshop included a panel session and four selected research
papers. The panel session sought to promote the interaction and collabora-
tion between the data management and knowledge representation communities,
which tackle data integration from diﬀerent perspectives. As main outcome,
promising research lines in this research area were presented and discussed.
The four selected papers address diﬀerent interesting research issues related
to data integration for Big Data. Brandt et al. presented a framework for tempo-
ral ontology-based data access (OBDA). In particular, compared to the standard
OBDA, the framework envisions the addition of static and temporal rules, where
the latter are based on datalognrMTL. Ib´a˜nez et al. introduced a collaborative
approach that stores queries in a multidimensional-aware (based on an exten-
sion of the QB4OLAP RDF vocabulary) knowledge base that is then analysed
to help users assessing semantic consistency of aggregation queries in self-service
BI contexts. Theodorou et al. presented an adaptive Big Data reference archi-
tecture, named Theta, thought to introduce ﬂexibility in the data analytics part
of the system enabling optimization at runtime to meet the level of data quality
required by the data analyst. Finally, Trajcevski et al. introduced the concept
of knowledge-evolution trajectory as a model to formalise the spatial, temporal,
and content-based aspects of scientiﬁc publications and enable novel querying
techniques based on these three aspects.
Acknowledgements. The BigNovelTI chairs would like to acknowledge the
help of all PC members. The alphabetically ordered list of PC members
contais: Alberto Abell´o (Universitat Polit`ecnica de Catalunya, Spain), Andrea
Cal`ı (Birkbeck College London, U.K.), Theodoros Chondrogiannis (Free
University of Bozen-Bolzano, Italy), Martin Giese (University of Oslo, Norway),
Gianluigi Greco (University of Calabria, Italy), Katja Hose (Aalborg University,
Denmark), Petar Jovanovic (Universitat Polit`ecnica de Catalunya, Spain), Maria
C. Keet (University of Cape Town, South Africa), Roman Kontchakov (Birk-
beck College London, U.K.), Antonella Poggi (Sapienza University of Rome,
Italy), Mantas Simkus (TU Vienna, Austria), Sergio Tessaris (Free University
of Bozen-Bolzano, Italy), Christian Thomsen (Aalborg University, Denmark),
Stefano Rizzi (University of Bologna, Italy), Alejandro Vaisman (Instituto Tec-
nol´ogico de Buenos Aires, Argentina), Stijn Vansummeren (Universit´e Libre de

114
A. Behrend et al.
Bruxelles, Belgium), Panos Vassiliadis (University of Ioannina, Greece), Robert
Wrembel (Poznan University of Technology, Poland), and Esteban Zimanyi (Uni-
versit Libre de Bruxelles, Belgium).
3
The 1st International Workshop on Data Science:
Methodologies and Use-Cases (DaS 2017)
Introduction. The 1st International Workshop on Data Science: Methodologies
and Use-Cases (DaS 2017) has been organized by Tania Cerquitelli (Politecnico
di Torino, Italy), Silvia Chiusano (Politecnico di Torino, Italy), and Natalija
Kozmina (University of Latvia, Latvia).
Data science is an interdisciplinary ﬁeld about scientiﬁc processes, method-
ologies, and systems to extract useful knowledge or insights from data in various
forms. Data can be analyzed using data mining, machine learning, data analysis
and statistics, optimizing processes and maximizing the knowledge exploitation
in real-life settings. DaS 2017 is a workshop aimed at fostering and sharing
research and innovation on data science. The workshop allows researchers and
practitioners to exchange their ideas and experiences on designing and devel-
oping data science applications, discuss the main open issues, and share novel
solutions for data management and analytics.
The DaS topics of interest include methodologies, models, algorithms, and
architectures for data science, scalable and/or descriptive data mining and
machine learning techniques for knowledge discovery, data warehouses and large-
scale databases, and experiences with data-driven project development and
deployment in various application scenarios.
Keynote Presentation. Rossano Schifanella (computer scientist at the Uni-
versity of Turin) gave a talk entitled Combined Eﬀect of Content Quality and
Social Ties on User Engagement. He is a visiting scientist at Nokia Bell Labs,
Cambridge, UK, and a former visiting scientist at Yahoo Labs and at Indiana
University School of Informatics and Computing where he was applying com-
putational methods to model social behavior in online platforms. His research
mainly focuses on a data-driven analysis of the behavior of (groups of) individ-
uals and their interactions on social media platforms.
In his talk, Rossano presented a large scale study on the complex intertwine-
ment between quality, popularity, and social ties in an online photo sharing plat-
form, proposing a methodology to democratize exposure and foster long term
users engagement.
Invited Paper. Genoveva Vargas-Solar contributed to DaS with an invited
paper entitled Eﬃcient Data Management for Putting forward Data-centric Sci-
ences. She is a senior scientist of the French Council of Scientiﬁc Research
(CNRS) and since 2008, Genoveva is deputy director the Franco-Mexican Lab-
oratory of Informatics and Automatic Control (LAFMIA) an international

New Trends in Databases and Information Systems
115
research unit established at CINVESTAV. Her research interests concern distrib-
uted and heterogeneous databases, reﬂexive systems and service based database
systems. She conducts fundamental and applied research activities for address-
ing these challenges on diﬀerent architectures ARM, raspberry, cluster, cloud,
and HPC.
The novel and multidisciplinary data-centric and scientiﬁc movement
promises new and not yet imagined applications that rely on massive amounts
of evolving data that need to be cleaned, integrated, and analysed for mod-
elling, prediction, and critical decision making purposes. In her paper, Genoveva
explains the key challenges and opportunities for data management in this new
scientiﬁc context, and discusses how data management can best contribute to
data-centric sciences applications through clever data science strategies.
Selected Papers. The DaS workshop is composed of 11 papers discussing a
variety of interesting research issues, application domains, and experience reports
in the area of data science. Speciﬁcally, the workshop is composed of three sec-
tions covering diﬀerent aspects in the area of data analytics for data science (4
papers authored by Datta et al., Cagliero et al., Saleh and El-Tazi, and Podapati
et al.), data management for data analytics (3 papers authored by Galkin et al.,
Bobrov et al., and Borzovs et al.), and data science use cases (4 papers authored
by Haq and Wilk, Quemy, Gonˇcarovs and Grabis, and Hernandez-Mendez et al.).
In the following we present the research papers for each of the three areas above.
In the Data analytics for data science section, the paper entitled Parallel
Subspace Clustering using Multi-core and Many-core Architectures (Datta et al.)
presents a multi-core approach exploiting CPU computation to parallelize the
SUBSCALE algorithm. The authors report results over performances and dis-
cuss the concurrency problems that eﬀect the parallelization of the SUBSCALE
algorithm. A novel pattern, named Generalized High-Utility Itemset (GHUI),
has been proposed in the paper entitled Discovering High-utility Itemsets at
Multiple Abstraction Levels (Cagliero et al.) to represent sets of item groups,
each one characterized by a high total proﬁt. The GHUI pattern exploits the
available data taxonomies to model high-utility itemsets at diﬀerent abstrac-
tion levels. The signiﬁcance of the proposed pattern and the performance of
the GHUI mining algorithm have been evaluated on retail data with the aim
of planning advertising campaigns of retail products. Aimed at grouping tags
associated with the StackOverﬂow posts in semantically coherent clusters, a
technique based on the Latent Dirichlet Allocation (LDA) has been proposed in
the paper entitled Automatic Organization of Semantically Related Tags using
Topic Modelling (Saleh and El-Tazi). First, by analyzing post texts through
LDA, topics are discovered and associated with the posts. Then, groups of tags
associated with the topics are built. A measure to evaluate the quality of the
extracted groups is presented. The approach has been experimentally validated
on a real dataset extracted from StackOverﬂow. The paper entitled Fuzzy Rec-
ommendations in Marketing Campaigns (Podapati et al.) proposes a technique
based on fuzzy logic to put in relationship the proneness of the customers of

116
A. Behrend et al.
telecommunication companies to be infrastructure-stressing (i.e., operating in
zones of high demand that might produce network service failures) with geo-
demographic customer segments. The aim is to shape the marketing campaigns
understanding whether there are segments of users that are too infrastructure-
stressing and should be avoided.
In the Data management for data analytics section, the paper entitled
Towards a Multi-way Similarity Join Operator (Galkin et al.) puts forward an
approach to overcome limitations of most of the existing query engines that
rely on binary join-based query planners and execution methods with complex-
ity depending on the number of data sources involved. The authors propose
a multi-way similarity join operator (MSimJoin) that accepts more than two
inputs and is able to identify duplicates corresponding to similar entities. In the
paper titled Workload-Independent Data-driven Vertical Partitioning, Bobrov et
al. introduce a new class of vertical partitioning algorithms that don’t exploit
workload information and are data-driven. The proposed algorithm relies on
the database logical schema to extract functional dependencies from tables and
perform partitioning accordingly. The authors experimentally compared their
algorithm with two existing workload-dependent algorithms. Borzovs et al. in
the paper entitled Can SQ and EQ Values and their Diﬀerence Indicate Pro-
gramming Aptitude to Reduce Dropout Rate? discuss the inadequacy of metrics
and models available in the reviewed literature when predicting the potential
dropouts among students of Computer Science study programs. Gained results
in their empirical evaluation disprove the hypothesis that students’ programming
aptitude would correlate with the systemizing quotient (SQ), empathy quotient
(EQ), and its diﬀerence.
Four interesting experience reports respectively in health care, law and jus-
tice, ﬁnancial, and business domains are discussed in the Data science use cases
section. A critical issue in health care domain is the ability to predict the proper
treatment for patients aﬀected by a certain pathology. The paper entitled Fusion
of Clinical Data: A Case Study to Predict the Type of Treatment of Bone Frac-
tures (Haq and Wilk) presents the application of data fusion techniques, such
as combination of data (COD) and interpretation (COI) approaches, in building
a decision model for patients with bone fractures to distinguish between those
who have to undergo a surgery and those who should be treated non-surgically.
An extensive and critical review of the current state-of-the-art research in data
science techniques for law and justice is presented in the paper Data Science
Techniques for Law and Justice: Current State of Research and Open Problems
(Quemy). The author describes the diﬃculties of analyzing legal environments,
identiﬁes four fundamental problems, and discusses how they are covered by the
current top approaches available in the literature. In the paper entitled Using
Data Analytics for Continuous Improvement of CRM Processes: Case of Finan-
cial Institution, Gonˇcarovs and Grabis present a case study where data mining
strategies have been exploited to identify the Next Best Oﬀer (NBO) for sell-
ing ﬁnancial products to bank’s customers. Based on the experience gained on
this case study, the authors eventually point out that the interaction between

New Trends in Databases and Information Systems
117
business owners and data analysts is necessary to improve the models. Aimed at
supporting the business ecosystems modeling (BEM) management process, the
Business Ecosystem Explorer (BEEx) tool is introduced in the paper entitled
Towards a Data Science Environment for Modeling Business Ecosystems: The
Connected Mobility Case (Hernandez-Mendez et al.). BEEx empowers end-users
to adapt not only the BEM but also the visualizations. The authors describe
their experience on modeling the Connected Mobility Ecosystem in the context
of the project TUM Living Lab Connected Mobility (TUM LLCM).
Acknowledgements. The DaS co-chairs would like to acknowledge the help
of all PC members who provided comprehensive, critical, and constructive
comments. The alphabetically ordered list of PC members contains: Julien
Aligon (University of Toulouse - IRIT, France), Antonio Attanasio (Istituto
Superiore Mario Boella, Italy), Agnese Chiatti (Pennsylvania State Univer-
sity, USA), Evelina Di Corso (Politecnico di Torino, Italy), Javier A. Espinosa-
Oviedo (Barcelona Supercomputing Center, Spain), G¨oran Falkman (University
of Sk¨ovde, Sweden), Fabio Fassetti (Universit`a della Calabria, Italy), Patrick
Marcel (University Fran¸cois Rabelais of Tours, France), Kjetil Nørv˚ag (Norwe-
gian University of Science and Technology, Norway), Erik Perjons (Stockholm
University, Sweden), Emanuele Rabosio (Politecnico di Milano, Italy), Francesco
Ventura (Politecnico di Torino, Italy), Gatis V¯ıtols (Latvia University of Agri-
culture, Latvia), Xin Xiao (Huawei Technologies Co., Ltd., China), Jos´e Luis
Zechinelli Martini (Universidad de las Americas - Puebla, Mexico).
4
The 2nd International Workshop on Semantic Web
for Cultural Heritage (SW4CH 2017)
Introduction. The 2nd International Workshop on Semantic Web for Cultural
Heritage (SW4CH 2017) has been organized by B´eatrice Markhoﬀ(University
of Tours, France) and St´ephane Jean (ENSMA, Poitiers, France). The aim of
this workshop is to bring together interdisciplinary research teams involved in
Semantic Web solutions for Cultural Heritage. Interdisciplinarity is a key point
in this ﬁeld, as Software Engineers, Data Scientists, and various Humanity and
Social Scientists have to work together for inventing, devising and implementing
novel ways of digital engagement with heritage based on the semantic web prin-
ciples, data, and services. It is crucial to have a place to exchange experiences,
present states of the art, and discuss challenges. The University of Tours, ISAE-
ENSMA Engineering School, and University of Poitiers are involved in such
interdisciplinary projects, within the scope of the Intelligence des Patrimoines
(I-Pat) programme, funded by the R´egion Centre Val de Loire, and led by the
CESR (Centre d’tudes Sup´erieur de la Renaissance), a DARIAH laboratory and
one of the principal European laboratories in the ﬁeld of the Renaissance.

118
A. Behrend et al.
Invited Paper. An invited talk has been given by Carlo Meghini, prime
researcher at CNR-ISTI and the head of the Digital Libraries group in the NeMIS
Lab of ISTI. His presentation was about Narratives in Digital Libraries. One of
the main problems of the current Digital Libraries (DLs) is the limitation of the
discovery services oﬀered to the users, which typically boil down to returning
a ranked list of objects in response to a natural language query. No semantic
relation among the returned objects is usually reported, which could help the
user in obtaining a more complete knowledge on the subject of the search. The
introduction of the Semantic Web, and in particular of the Linked Data, has the
potential of improving the search functionalities of DLs. In order to address this
problem, Carlo Meghini introduced narratives as new ﬁrst-class objects in DLs,
and presented preliminary results on this endeavor.
Selected Papers.
The workshop selected 6 papers addressing interesting
research issues in the context of Semantic Web for Cultural Heritage. In this
ﬁeld, research teams work in knowledge engineering for building, extending and
using ontologies, for representing their data. Ontologies are clearly identiﬁed
as an integration means, a base for interconnecting projects and datasets. In
this context, the CIDOC-CRM plays a central role, as shown in the number of
projects that use it.
The ﬁrst presented paper, Introducing Narratives in Europeana: Preliminary
Steps (C. Meghini et al.), reports on a particular case of the work introduced in
the invited talk, namely using a tool developed to enrich the Europeana content.
A narrative is here deﬁned as a semantic network that consists of linked events,
their spatio-temporal properties, participating entities, and objects included in
the digital collection, which can be represented using the CIDOC-CRM.
The second paper, Evaluation of Semantic Web Ontologies for Modelling Art
Collections (D. Liu et al.), presents an evaluation of three existing ontologies
used in the Cultural Heritage ﬁeld, CIDOC-CRM, the Europeana Data Model,
and VRA (Core), with respect to the representation of Art Collections. The
comparison is based on the concrete modelling of four diﬀerent artworks.
The third paper, The CrossCult Knowledge Base: a co-inhabitant of cultural
heritage ontology and vocabulary classiﬁcation (A. Vlachidis et al.), discusses
design rationales of the CrossCult Knowledge Base, devised to support a cross-
cultural public access to European History, fully compliant with the CIDOC-
CRM ontology and equipped with speciﬁc thesauri. Several choices made during
the design process which involved inter-disciplinary teams are discussed.
The fourth paper, The Port History Ontology (B. Rohou et al.), describes
the ﬁrst results of a multidisciplinary research project on an extension of the
CIDOC-CRM dedicated to the history of ports. The Port History Ontology is
mainly based on a pre-deﬁned model representing the spatio-temporal evolution
of ports.
The ﬁfth paper, A WordNet ontology in advancing search digital dialect dic-
tionary (M. Mladenovi´c et al.), introduces an ontology-based method for con-
necting a standard language (Serbian) and a dialect in Serbian to improve the

New Trends in Databases and Information Systems
119
search over the dialect dictionary using keywords entered in the standard lan-
guage. The proposal deﬁnes SWRL rules to infer new synonyms.
The last presented paper, When it comes to Querying Semantic Cultural Her-
itage Data (B. Markhoﬀet al.), provides an overview of projects and approaches
for querying semantic web data, including ontology-based mediation, federated
query systems and full web querying, with a focus on solutions already used for
semantic Cultural Heritage data.
All in all, on the one hand this second edition of SW4CH clearly reﬂects the
increasing number of CIDOC-CRM uses, for modeling a variety of CH domains,
and, on the other hand, it highlights the development of ontology-based meth-
ods for connecting, integrating and querying several Digital Cultural Heritage
resources.
Acknowledgements. The SW4CH co-chairs would like to acknowledge the
help of all PC members, who carefully evaluated each submission and pro-
vided useful comments to all authors. The alphabetically ordered list of PC
members contains: Trond Aalberg (NTNU, Norway), Carmen Brando (EHESS-
CRH, France), Benjamin Cogrel (Free University of Bozen-Bolzano, Italy),
Donatello Conte (University of Tours, France), Peter Haase (Metaphacts, Ger-
many), Mirian Halfeld Ferrari Alves (University of Orl´eans, France), Katja Hose
(Aalborg University, Denmark), Efstratios Kontopoulos (University of Thessa-
loniki, Greece), Cvetana Krstev (University of Belgrade, Serbia), Nikolaos Lagos
(Xerox, France), Denis Maurel (University of Tours, France), Carlo Meghini
(CNR/ISTI Pisa, Italy), Isabelle Mirbel (University of Nice Sophia Antipo-
lis, France), Alessandro Mosca (SIRIS Academic Barcelona, Spain), Dmitry
Muromtsev (ITMO University, Russia), Cheikh Niang (AUF Paris, France),
Xavier Rodier (University of Tours, France), Maria Theodoridou (FORTH
ICS, Heraklion, Crete, Greece), Genoveva Vargas-Solar (University of Greno-
ble, France), Dusko Vitas (University of Belgrade, Serbia).
5
The 1st Workshop on Data-Driven Approaches
for Analyzing and Managing Scholarly Data (AMSD
2017)
Introduction. The 1st Workshop on Data-Driven Approaches for Analyzing and
Managing Scholarly Data (AMSD 2017) was organized by Andreas Behrend (Uni-
versity of Bonn, Germany) and Christiane Engels (University of Bonn, Germany).
Supporting new forms of scholarly data publication and analysis has become
a crucial issue for scientiﬁc institutions worldwide. Even though there are already
several search engines, bibliography websites and digital libraries (e.g., Google
Scholar, CiteSeerX, Microsoft Academic Search, DBLP) supporting the search
and analysis of publication data, the oﬀered functionality is still rather lim-
ited. For example, the provided keyword-based search does not employ the
full opportunities an advanced semantic search engine could oﬀer. In addition,

120
A. Behrend et al.
a comprehensive content analysis beyond title and abstract as well as the capa-
bility to take the quality and relevance of the publications into account is not
provided so far.
International community forums such as FORCE11 and Research Data
Alliance (RDA) already engage for open and semantic publishing which enable
various opportunities for new analytics on scholarly data. The global vision is a
uniform system combining all forms of scholarly data oﬀering a comprehensive
semantic search and advanced analytics including:
– Knowledge extraction and reasoning about scientiﬁc publications and events,
– Quality assessment of scientiﬁc publications and events, e.g. via rankings or
quality metrics,
– Recommender systems for conferences and workshops, and
– Identiﬁcation of research schools and key papers or key researcher within a
ﬁeld using citation analysis.
In this ﬁrst edition of the AMSD workshop, we want to encourage especially
researchers from the database community (but also from the ﬁelds informa-
tion retrieval, digital libraries, graph databases, machine learning, recommender
systems, visualization, etc.) to combine their resources in order to exploit the
possibilities of scholarly data analysis. The main goals of the AMSD workshop
are to facilitate the access of scholarly data, to enhance the ability to assess
the quality of scholarly data and to foster the development of analysis tools for
scholarly data. The following topics will be addressed:
– Managing scholarly data, i.e. representation, categorization, connection and
integration of scholarly data in order to foster reusability, knowledge sharing,
and analysis,
– Analyzing scholarly data, i.e. designing and implementing novel and scal-
able algorithms for knowledge extraction and reasoning about and assessing
the quality of scientiﬁc publications and events with the aim of forecasting
research trends, establishing recommender systems, and fostering connections
between groups of researchers,
– Applications on scholarly data, i.e. providing novel user interfaces and appli-
cations for navigating and making sense of scholarly data and highlighting
their patterns and peculiarities.
Program. The workshop consists of 3 accepted papers addressing interesting
research challenges to support new forms of scholarly data publication and analy-
sis. In the paper entitled Publication Data Integration As a Tool for Excellence-
Based Research Analysis at the University of Latvia a publication data inte-
gration tool is presented designed and used at the University of Latvia for
excellence-based research analysis. Besides the usual bibliographic data about
authors (name, aﬃliation) and publications, quantity, quality and structural
bibliometric indicators are included for a excellence-based analysis. In the paper
Evaluating Reference String Extraction Using Line-Based Conditional Random

New Trends in Databases and Information Systems
121
Fields: A Case Study with German Language Publications the authors address
the problem of extracting individual reference strings from the reference section
of scientiﬁc publications. They propose an approach named RefExt that apply
a line-based conditional random ﬁelds rather than constructing the graphical
model based on the individual words, dependencies and patterns that are typi-
cal in reference sections provide strong features while the overall complexity of
the model is reduced.
Finally, in the paper CEUR Make GUI A usable web frontend supporting the
workﬂow of publishing proceedings of scientiﬁc workshops a new GUI web front
end for facilitating the generation of proceedings in CEUR-WS is described. A
previously developed command line tool has proven to be not feasible for this
purpose. The performed user evaluation shows a 40% less execution time of the
GUI variant compared to the command line version and rather positive usability
scores.
Keynote Presentations. This year, two keynote presentations were oﬀered
at AMSD. The ﬁrst talk was provided by Johann Gamper from the University
of Bozen-Bolzano who is an expert in temporal data management. In his talk
he discussed key requirements for managing temporal data in current database
systems. In particular, he showed how the requirements can be mapped to simple
and powerful primitives for the relational model, and identiﬁes a range of open
problems when dealing with so-called long data.
The second talk was given by Andreas Behrend from the University of Bonn
who works on a cloud-based tool for managing and analyzing publication data.
In his talk he presented the OpenResearch project in which a semantic wiki is
developed for storing and querying information about scientiﬁc events, research
projects, publishers, journals, and articles.
Acknowledgements. The AMSD co-chairs would like to express their grat-
itude to all PC members, who carefully evaluated each submission and pro-
vided useful comments to the authors. This year, our PC consists of the fol-
lowing members: S¨oren Auer (TIB University of Hannover, Germany), Andreas
Behrend (University of Bonn, Germany), Christiane Engels (University of Bonn,
Germany), Said Fathalla (University of Bonn), Christoph Lange (Fraunhofer
IAIS, Germany), Heba A. Mohamed (University of Bonn, Germany), Sara
Soliman (University of Alexandria), Sahar Vahdati (Fraunhofer IAIS, Germany),
Hannes Voigt (University of Dresden, Germany).
6
Conclusions
ADBIS 2017 workshop organizers would like to express their thanks to everyone
who contributed to the success of their events and to this volume content. We
thank the authors, who submitted papers to the workshops in the context of
ADBIS 2017. Special thanks go to the Program Committee members as well

122
A. Behrend et al.
as to the external reviewers of the ADBIS 2017 workshops, for their support
in evaluating the submitted papers, providing comprehensive, critical, and con-
structive comments and ensuring the quality of the scientiﬁc program and of this
volume.
Finally a special thank to ADBIS 2017 workshop chairs Johann Gamper (Free
University of Bozen-Bolzano, Italy) and Robert Wrembel (Poznan University of
Technology, Poland) to continuously support us during the ADBIS 2017 work-
shop organization time frame with useful suggestions and fruitful discussion.
We all hope you will ﬁnd the volume content an useful contribution to pro-
mote novel ideas for further research and developments in the areas of databases
and information systems. Enjoy the reading!

The 1st Workshop on Data-Driven
Approaches for Analyzing and
Managing Scholarly Data (AMSD 2017)

Publication Data Integration as a Tool
for Excellence-Based Research Analysis
at the University of Latvia
Laila Niedrite(&), Darja Solodovnikova(&), and Aivars Niedritis
Faculty of Computing, University of Latvia, Riga, Latvia
{laila.niedrite,darja.solodovnikova,
aivars.niedritis}@lu.lv
Abstract. The evaluation of research results can be carried out with different
purposes aligned with strategic goals of an institution, for example, to decide
upon distribution of research funding or to recruit or promote employees of an
institution involved in research. Whereas quantitative measures such as number
of scientiﬁc papers or number of scientiﬁc staff are commonly used for such
evaluation, the strategy of the institution can be set to achieve ambitious sci-
entiﬁc goals. Therefore, a question arises as to how more quality oriented
aspects of the research outcomes should be measured. To supply an appropriate
dataset for evaluation of both types of metrics, a suitable framework should be
provided, that ensures that neither incomplete, nor faulty data are used, that
metric computation formulas are valid and the computed metrics are interpreted
correctly. To provide such a framework with the best possible features, data
from various available sources should be integrated to achieve an overall view
on the scientiﬁc activity of an institution along with solving data quality issues.
The paper presents a publication data integration system for excellence-based
research analysis at the University of Latvia. The system integrates data avail-
able at the existing information systems at the university with data obtained
from external sources. The paper discusses data integration ﬂows and data
integration problems including data quality issues. A data model of the inte-
grated dataset is also presented. Based on this data model and integrated data,
examples of quality oriented metrics and analysis results of them are provided.
Keywords: Research evaluation  Research metrics  Data integration  Data
quality  Information system  Data model
1
Introduction
Evaluation in science nowadays is turning into a routine work based on metrics [1]. In
circumstances of a wide diversity of metrics, it is signiﬁcant that they are chosen
carefully, e.g. in compliance with research institution’s strategic goals.
Research indicators can be used for different purposes [2]: science policy making at
the state level, distribution of research funding, organization and management activi-
ties, e.g. in Human Resource Management for recruiting or promoting employees
involved in research, content management and decisions at individual researchers’
© Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 125–136, 2017.
DOI: 10.1007/978-3-319-67162-8_14

level, e.g. where and what to publish, and providing consumer information, e.g. uni-
versity rankings that include science indicators. These usage examples correspond to
the group of research performance indicators and do not include input indicators, such
as number of researchers.
At the institutions level, the research evaluation can be performed to support the
achievement of strategic goals. Whereas quantitative measures such as number of scientiﬁc
papers, amount of funding, number of scientiﬁc staff and many others are commonly used
for such evaluation, the strategy of the institution can be set to achieve ambitious scientiﬁc
goals. Therefore, a question arises as to how more quality oriented aspects of the research
outcomes can be measured. For example, at the state level to allocate funds to excellent
institutions, measurement methods from three categories are used: peer review-based
models, publication count-based models and citation-based models [3].
We will propose a data integration architecture for bibliometric information anal-
ysis, which is one of the research performance indicators. Bibliometric indicators can
be also classiﬁed in more detail as quantity, quality and structural indicators [4].
A quantity indicator is, for example, number of publications. An example of a quality
indicator is h-index. Structural indicators allow to evaluate connections, for example,
co-authors from different ﬁelds, institutions or countries.
To supply an appropriate dataset for evaluation of both types of metrics for mea-
suring quantitative and qualitative aspects, a suitable framework should be provided,
that ensures that neither incomplete, nor faulty data are used, that metric computation
formulas are discussed and are valid and the computed metrics are interpreted correctly.
To provide such a framework with the best possible features, data from various
available sources should be integrated to achieve an overall view on the scientiﬁc
activity of an institution along with solving data quality issues.
The principles characterizing the best practice in metrics-based research assessment
are given in the “Leiden manifesto” [1]. Among these principles, some of them should
be considered when building a data collection and integration system for effective
science evaluation, for example, data collection should be transparent, the institutions
and persons that are evaluated can verify the data provided for evaluation, the indicator
values should be updated regularly.
Research information management has many properties that are typical for data
integration scenarios: many data sources with inconsistent data models, heterogeneity,
and many involved stakeholders with diverse goals [5]. Knowledge from data inte-
gration ﬁeld can be used to simplify the data collection and integration in research
information ﬁeld. For example, a uniform data model or standard can be used [5].
One such standard is CERIF (Common European Research Information Format),
which includes information about projects, persons, publications, organizations, and
other entities. Many research information systems in Europe are built interrelated with
this standard [6]. However, there are other standards and models that are signiﬁcant to
describe research information, among them DOI (Digital Object Identiﬁer) to identify
publications and ORCID (Open Researcher and Contributor ID) to identify authors of
publications.
Today there are many efforts trying to implement information systems to support
research evaluation activities. Institutions develop their own or use commercial or
non-commercial products to maintain data about research results. Many information
126
L. Niedrite et al.

sources have been used at the University of Latvia (LU) for a while to get the insight
into the actual situation with research activities and their outcomes, but this process
needed improvements by providing an integrated information oriented to scientiﬁc
excellence. The requirements for research evaluation in Latvia are declared in the
regulations issued by the government and prescribe how the funding for scientiﬁc
institutions is calculated [7, 8]. As stated in these regulations, the productivity of
scientiﬁc work is evaluated according to the number of publications indexed in Scopus
or Web of Science (WoS). These quantitative data must be extended with data nec-
essary for computation of qualitative indicators.
The paper presents a publication data management system for excellence-based
research analysis at LU. The system integrates data available at the university infor-
mation system with data from the library information system as well as with data
obtained via API from Scopus and WoS databases. The paper discusses data integration
ﬂows and data integration problems including data quality issues. A data model of the
integrated dataset is also presented. Based on this data model and integrated data,
examples of quality oriented metrics and analysis results of them are provided.
2
Related Work
A research information system in Scandinavia [9] is an example of such system that is
implemented and used in Denmark, Finland, Norway, and Sweden and mostly contains
integrated, high quality bibliometric data. The system is used for performance-based
funding. It is remarkable, that this system also has its own publication indicator that by
weighting the results from different ﬁelds allows to compare them.
In the ﬁeld of data integration, the recent approaches are focused on mappings
between models and integration processes. Also in research information systems,
mappings between different systems should be considered as important parts, they
provide speciﬁcations for data integration processes [5].
The Polish Performance-research funding system allows to evaluate 65 parameters.
962 research units provided data about more than a million research outcomes for the
4-year period. The data collection process was performed through submission of the
questionnaire through the Information System on Higher Education in Poland. The
study [10] was performed to ﬁnd out the most important metrics to facilitate the
transition to more targeted system to meet the excellence requirements, where only the
most important metrics are reported. The research showed that many of existing metrics
are not signiﬁcant for the evaluation.
Italian experience [11] shows the implementation of a research information system
in Italy, where 66 Italian institutions introduced IRIS, that is a system based on DSpace
[12] and customized for Italian environment. ORCID also was used at a national level.
Entities, attributes and relations in this system are compliant with the CERIF ontology
[13]. The collected huge amount of data allowed to understand the whole situation in
research and, for example, to develop new publication strategies. The authors of the
study mention also the problems with the data quality, when not all institutions control
the data collection process and do not implement data validation processes of data
provided by researchers.
Publication Data Integration as a Tool for Excellence-Based Research Analysis
127

3
Data Model
To support analysis and evaluation of the scientiﬁc activity of LU members involved in
research, both employees and students, we propose a system architecture that integrates
information about publications from all accessible data sources that include the library
information system ALEPH, LU management information system LUIS, SCOPUS and
WoS databases. To maintain publication information and support reporting and anal-
ysis, the publication data from multiple sources are linked and stored in a repository in
LUIS. The data model of the repository is depicted in Fig. 1.
The central class to store bibliographical data about a publication as well as number
of citations in SCOPUS and WoS databases is Publication. The bibliographical data in
this class are entered by the author of the publication or the faculty staff or they are
populated during the data loading process from ALEPH or SCOPUS databases. For
each publication indexed by SCOPUS or WoS, we also include the corresponding
number of citations as well as publication identiﬁers used in both databases to maintain
a link with SCOPUS and WoS as data sources.
The information about authors of the publication is reﬂected by the classes Author
and SCOPUS Author. The class Author represents ordered authors of the publication,
which are afﬁliated with LU as well as with other institutions.
Authors recognized as afﬁliated with LU are also linked to LU Person class, which
stores personal and other information used for different functionality of LUIS. For
foreign authors, we store only their name as it appears on the paper. If a publication is
indexed by SCOPUS, we also collect author information from it, which includes name,
surname, H-index and author ID assigned by SCOPUS, which is used in the author
matching process.
We also store the information about the afﬁliation of the publication with LU
department or faculty, which is represented as the class LU Department. This infor-
mation is obtained automatically from data about the work place of the author and may
PublicaƟon
Type
Subtype
Title
PublicaƟon_name
Volume
Issue
Series
Publisher
Published_place
Pages
ISSN
ISBN
DOI
URL
File
Allow_public_access
Status
SCOPUS_ID
SCOPUS_cited_by_count
WoS_UID
WoS_cited_by_count
Autor
LU_author_id
Non_LU_author_display_name
SCOPUS Author
Author_ID
Name
Surname
H-index
LU Person
Journal CitaƟon Metrics
Output
CitaƟon_count
CiteScore
Percent_cited
SJR
SNIP
Year
Status
SCOPUS AﬃliaƟon
Name
AlternaƟve_name
City
Country
Type
LU Department
Name
SCOPUS Subject
Name
AbbreviaƟon
FOS Category
Code
Name
Journal Rank
Rank
PercenƟle
*
*
*
*
*
0..1
*
*
*
1..*
1
*
1..*  {ordered}
1
*
*
*
*
1
*
Serial Tiltle
ISSN
Title
Publisher
Type
Open_access_status
Open_access_type
Allow_author_paid
*
0..1
1
*
0..1
*
 
Fig. 1. Publication repository data model
128
L. Niedrite et al.

be corrected by the responsible faculty or library staff. If the publication is indexed by
SCOPUS, we also store the information about the institutions the publication authors
are afﬁliated with in the class SCOPUS Afﬁliation. This information is necessary to
analyze connections with co-authors from different institutions or countries.
For the analysis of the quality of publications, we use not only the citation number,
but also other citation metrics provided by SCOPUS. The absolute values of such
metrics are calculated for journals and serial conference proceedings yearly and their
values are represented by the class Journal Citation Metrics. We also collect infor-
mation about the open access status of the journal or conference proceedings and
include it in the class Serial Title. In SCOPUS database, journals are also ranked
among other journals that belong to the same subject areas according to their values for
CiteScore metric [14], an alternative to WoS Journal Impact Factor. Journal rank
information is represented by the class Journal Rank and it is connected with the
corresponding subject area (class SCOPUS Subject). For reporting on publications of
different OECD categories, we store the correspondence of SCOPUS subject areas and
Field of Science and Technology (FOS) categories.
4
Scenarios of Obtaining Publication Data
To accumulate the most complete list of publications authored by LU staff and students
in the central repository in LUIS, we gather publication data from different sources, link
publications to the correspondent members of LU staff, correct errors and duplicates
and provide the consolidated information using a set of reports used by the manage-
ment of LU. In the following section the 4 scenarios of obtaining publication data as
well as data ﬂows related to each scenario are discussed.
4.1
Publication Data Added by Authors
LU employees, PhD and Master’s degree students are able to add information about
their co-authored publications to the repository themselves. The process when publi-
cation data are entered by authors is depicted in Fig. 2. LUIS system maintains user
proﬁles for all LU members. Among various other information about a user, a proﬁle
includes a section devoted to research, which in turn contains a list of author’s pub-
lications obtained from the repository. An author can supplement this list by adding
newly published articles or articles, which were not loaded automatically. Before
adding them, an author is automatically requested to search for his/her publications in
the LUIS repository and library information system ALEPH, which are not linked with
the author’s proﬁle, to avoid creation of duplicates. If a desired article is found, it is
Author
LUIS
ALEPH
Bibliographical data 
synchronizaƟon
Library staﬀ
Fig. 2. Publication data added by authors
Publication Data Integration as a Tool for Excellence-Based Research Analysis
129

possible to add it to the proﬁle. In this case, the author does not need to supply any
additional information about the article.
If, however, the article is not present in either of the systems, the author has to
specify the type and subtype of the publication (for example, journal article, book
chapter, book, etc.) and supply bibliographical information. Besides, an author must
indicate the status of the publication: published, submitted for publication, developed
or under development, attach publication ﬁle (full text or book cover) and indicate
whether it can be accessed publicly at the e-resource repository of LU. After the author
has ﬁnished entering publication data, they are transferred to the library information
system ALEPH. To ensure the best possible data quality, members of the library staff
validate publication data, correct any errors if present and approve a publication.
Finally, publication data are synchronized back with LUIS and become available for
evaluation and reports.
Besides entering new data, LUIS users can unlink mistakenly attached publications
from their proﬁles, which were automatically loaded to the repository from other
sources or conﬁrm that author matching was performed correctly.
4.2
Publication Data Added by Faculty Staff
Data about publications authored by the faculty members can also be entered into the
repository by specially designated faculty staff (Fig. 3). The procedure for adding data
is similar to the one that is performed by authors. The differences are that faculty staff
can record data about publications authored by other faculty members, correct erro-
neous links between publications and authors and adjust the list of afﬁliated LU
departments.
4.3
Publication Data Obtained from SCOPUS and Web of Science
There are two external sources of publication data that are used in the data loading
process: SCOPUS and WoS citation database systems. SCOPUS offers API, which
allows to search for publications authored by LU members and extract bibliographical
data of such publications as well as various citation metrics. The extraction and loading
process (Fig. 4) is run daily. Articles that were published during the last 2 years are
inserted into the repository or updated daily, but all other publications are updated on a
weekly basis. Bibliographical information is extracted from SCOPUS and loaded into
the repository table which corresponds to the class Publication of the repository data
model. Data about authors (unique author identiﬁer, name, surname, H-index) and
publication and author’s afﬁliation are loaded into tables which correspond to the
classes Author, SCOPUS Author and SCOPUS Afﬁliation of the repository data model.
Faculty staﬀ
LUIS
ALEPH
Bibliographical data 
synchronizaƟon
Library staﬀ
Fig. 3. Publication data added by faculty staff
130
L. Niedrite et al.

Afﬁliations are associated with authors as well as with publications directly. In addition
to bibliographical and author information, citation metrics are also obtained that
include current number of citations of individual publications as well as citation metrics
obtained for the particular journal or conference proceedings: Source Normalized
Impact per Paper (SNIP) [15], the SCImago Journal Rank (SJR) [16], CiteScore [14].
Previously, it was possible to obtain Impact per Publication (IPP) metric [17], which is
not available from SCOPS anymore, so this number is retained for previously loaded
publications and is not loaded for the new ones.
The ﬁrst step of the SCOPUS data loading process that is executed on any new
publication is a recognition phase. The main goal of this phase is to identify publi-
cations that are already registered in the repository, but that are newly indexed by
SCOPUS, to avoid creation of duplicates. The ﬁrst criterion used for the recognition is
Document Object Identiﬁer (DOI) which is unique for every publication. If the
matching publication with the same DOI is not found in the repository, the search
based on the similar title and publication year is performed. To determine the existing
publication with the most similar title in the repository, Jaro-Winkler similarity [18] is
used because there may be different alternatives of title spelling as well as data quality
issues are sometimes present. Different thresholds for Jaro-Winkler similarity were
tested and experimental evaluation of matching results revealed that the most suitable
threshold is 0,93 and currently this coefﬁcient is used to consider titles of publications
similar.
If the recognition process detects an existing publication in the repository or if a
publication has already been previously updated with SCOPUS data, we update the
number of citations for the publication as well as journal citation metrics and establish a
link with a corresponding Scopus record for newly indexed publications by means of
ﬁlling SCOPUS ID attribute of the Publication class (Fig. 1).
If a processed publication is new to the system, a new instance of the Publication
class is created with all the bibliographical information obtained from SCOPUS
database, publication authors are also represented as instances of Author and SCOPUS
Author classes and citation metrics as well as journal rank data are created or updated if
information about a journal has been previously loaded.
In case of a new publication, the author matching process is performed, when for
each author afﬁliated with LU, a corresponding instance of LU Person class is searched
for and, if found, it is associated with a corresponding instance of the Author class. The
primary criterion for author matching process is the SCOPUS author identiﬁer, which
allows to uniquely identify authors, whose publications have been previously loaded
into the repository. If author search by identiﬁer is unsuccessful, the matching process
Web of 
Science
LUIS
ALEPH
Bibliographical data 
synchronizaƟon
SCOPUS
Fig. 4. Publication data obtained from SCOPUS and Web of Science
Publication Data Integration as a Tool for Excellence-Based Research Analysis
131

uses the secondary criterion, which is a combination of author’s name and surname.
Author matching by precise name and surname produces insufﬁcient results, because
publication authors tend to use different spelling of their names and surnames that not
always corresponds to their full names. Furthermore, a full name can contain special
characters of the local language, which may be substituted with English language
characters in the publication in a different way. For example, Latvian letter ‘Š’ may be
substituted with English letter ‘S’ or with two symbols ‘SH’. To solve this data quality
issue, the author matching based on names and surnames is performed using
Jaro-Winkler similarity between the full name as it appears in the publication and
ofﬁcial author’s full name, i.e. LU Person instance with the highest Jaro-Winkler
similarity coefﬁcient that exceeds a threshold is linked to the publication. We use the
same threshold for similarity coefﬁcient 0.93, which was selected based on the
experimental matching results. After a match is found, we also establish an association
between the instance of the SCOPUS Author class and the corresponding instance of
the LU Person class to use SCOPUS author identiﬁer as the primary criterion for
matching future publications.
When a new publication is loaded into the repository, the second process phase –
publication data synchronization with the library information system ALEPH is exe-
cuted. During this phase, publication data are exported to ALEPH, bibliographical
information is supplemented and any errors are manually corrected by the library staff
to maintain the best possible data quality and ﬁnally the updated data are imported back
to the repository.
Another data source used in the integration process is WoS web services. The
version of web services available at the University of Latvia does not include journal
citation metrics and provides only limited bibliographical information, number of
citations, full names and sometimes Researcher identiﬁers of publication authors if they
have registered such identiﬁer in WoS database. Since the afﬁliation of authors is not
available, we have discovered that author matching process for WoS data produces too
many incorrectly identiﬁed authors, therefore, a decision has been made to add new
Web of Science publications to the repository manually. Besides, the integration
process also matches publication data obtained from WoS with publications already
available at the repository. Just as for publications loaded from SCOPUS, the primary
criterion used for matching is DOI and the secondary criteria are title and publication
year. The integration process regularly updates WoS number of citations for recognized
publications.
4.4
Publication Data Added by Library Staff
There is a considerable number of publications that are not indexed by SCOPUS and
are authored by LU members, especially in the humanities. The information about such
publications is necessary to perform accurate evaluation of the scientiﬁc activity of the
institution. Therefore, library staff members manually add bibliographical information
about publications to the library system ALEPH (Fig. 5). This is done when a librarian
comes across a new publication authored by any LU member in some journal or
conference proceedings or when the information about a new publication is obtained
from the list of recently indexed publications in Web of Science database, which is
132
L. Niedrite et al.

monthly distributed by Web of Science. When new publication data appear in ALEPH,
a synchronization process is conducted that integrates bibliographical information into
the repository to ensure that it always represents an overall view on publication data.
The synchronization process includes author matching phase. During this phase, for
each author of a publication, a corresponding LU person is searched for, using the same
algorithm based on the full name similarity as in other matching phases. If the cor-
responding LU person is found, the publication is attached to his/her LUIS proﬁle. We
use the same minimal Jaro-Winkler similarity coefﬁcient 0,93 to consider names
similar.
In addition to entering new publications to ALEPH, library staff members are also
responsible for correcting and supplementing bibliographical data for publications
added by authors and by faculty staff and for publications data imported from Scopus
database.
5
Data Analysis: Case Study
The context of the data collection and integration can be described with the total
number of publications of LU researchers for the last 30 years that equals to 42417.
6967 publications out of them are indexed by Scopus and 7764 publications are
indexed by WoS. The case study is performed using data that corresponds to the LU
Faculty of Computing and the time frame that was chosen for research results evalu-
ation was 2012–2016. We have already performed an initial evaluation of research
performance by means of quantitative metrics [19]. To give a context for the further
data analysis some numbers, e.g. publication count and Scopus publications, should be
mentioned. The total number of publications is decreasing at the faculty from 99 in year
2012 to 79 in year 2015, but the number of Scopus publications is growing and in year
2015 was 55.
The goal of the case study was to deﬁne metrics based on the data attributes
provided by the data model of the integrated publication information system, and with
the goal to evaluate the quality of the publications, to ﬁnd out the positive trends as
well as the problems with the quality.
The quality aspects of a publication can be indirectly described with the source
quality characteristics, e.g. journal quartiles that are computed from the citation count
of all journal publications, because we can presume that the journal with the highest
quartile Q1 will accept the best publications. Another group of quality indicators
Library staﬀ
ALEPH
LUIS
Bibliographical data 
synchronizaƟon
Author matching
 
Fig. 5. Publication data added by library staff
Publication Data Integration as a Tool for Excellence-Based Research Analysis
133

directly describe quality of the publications and are computed from citation counts of
publications. Further in this section different analysis scenarios for research output
evaluation with quality metrics that can be implemented with the new publication
module and data integration infrastructure are described.
For the 1st analysis scenario the following research question was formulated: “How
many faculty publications in Scopus are published in sources with and without com-
puted quartiles?” Later more detailed analysis was performed to ﬁnd out, how the
publication count is divided among quartiles.
Results are shown in Fig. 6(a) and (b). The results show an unsatisfactory trend for
the faculty, that in the year 2015 the number of publications in sources without
quartiles was increasing. The detailed analysis showed that in all years, except the last
year 2016, the biggest number of publications belongs to quartile Q3. Regarding the
excellence, the number of publications with Q1 is increasing in the last 3 years.
For the 2nd analysis scenario, the following research question was formulated:
“How many faculty publications in Scopus are not cited comparing with all publica-
tions and the publications in sources with computed quartiles?” Figure 7 shows the
trend that the proportion of uncited publications remains unchanged in sources with
computed quartiles, but is growing among all Scopus publications.
0
10
20
30
40
50
60
2012
2013
2014
2015
2016
Scopus
publicaƟons in
sources without
quarƟles
Scopus
publicaƟons in
sources with
quarƟles
Scopus
publicaƟons
5
8
3
6
10
6
4
3
9
4
11
15
29
10
1
4
6
8
7
20
0
10
20
30
40
50
2012
2013
2014
2015
2016
PublicaƟon count  with Q1
PublicaƟon count  with Q2
PublicaƟon count  with Q3
PublicaƟon count  with Q4
Fig. 6. Publication count in Scopus sources (a) with and without quartiles (b) detailed count by
quartiles
0
10
20
30
40
50
60
2012
2013
2014
2015
2016
Scopus publicaƟons
Scopus quarƟles are 
computed
PublicaƟons with citaƟon 
count = 0
PublicaƟons with citaƟon 
count = 0 in sources with 
computed quarƟles
Fig. 7. Publications that are not cited
134
L. Niedrite et al.

For the 3rd analysis scenario the research question was formulated: “How many are
there citation counts in Scopus sources with quartiles and without computed quartiles?”
Figure 8(a) shows the trend that the citation count in Scopus sources without computed
quartiles is decreasing, but at the same time the citation count in the years 2012–2015 is
remaining stable. More detailed analysis (see Fig. 8(b)) shows a signiﬁcant citation
count of Q3 publications, however this can be explained with bigger amount of Q3
publications among all others.
These results can help to decide at each individual researcher’s level to try pub-
lishing their works in sources with one quartile higher, but for the faculty, the shift from
sources with Q3 to Q2 may be the most promising and realistic.
6
Conclusions
The main contribution of this paper is the architecture, that implements different ﬂows
of data and integrates them into one consistent system for research output evaluation.
The architecture is based on the idea to ensure data quality, control, and also the
integration process transparency, involving publication authors in different roles – as
information providers or approvers.
The model of integrated dataset was determined mostly by existing information
systems at the university and external interfaces (WoS and Scopus). The model is
integrated with the global data model of the management information system of the
University of Latvia, so the research evaluation can be extended with other types of
metrics not only the bibliometric ones.
The proposed architecture ensures calculation of metrics for the publications
evaluation both quantitative and qualitative ones. However, this paper concentrates
mostly on the qualitative ones to support the scientiﬁc excellence. The quantitative
metrics can help to set further goals, based on ﬁndings of performed analysis.
51
31
4
14
4
116
125
93
92
35
0
20
40
60
80
100
120
140
160
180
2012
2013
2014
2015
2016
CitaƟon count
in Scopus
sources with
quarƟles
CitaƟon count
in Scopus
sources
without
quarƟles
55
64
35
44
27
12
15
3
32
0
48
37
52
8
0
1
9
3
8
8
0
20
40
60
80
100
120
140
2012
2013
2014
2015
2016
CitaƟon count in
Scopus sources Q4
CitaƟon count in
Scopus sources Q3
CitaƟon count in
Scopus sources Q2
CitaƟon count in
Scopus sources Q1
Fig. 8. Citation count in Scopus sources (a) with and without quartiles (b) detailed citation count
by quartiles
Publication Data Integration as a Tool for Excellence-Based Research Analysis
135

References
1. Hicks, D., Wouters, P., Waltman, L., De Rijcke, S., Rafols, I.: The Leiden Manifesto for
research metrics. Nature 520(7548), 429–431 (2015). doi:10.1038/520429a
2. Kosten, J.: A classiﬁcation of the use of research indicators. Scientometrics 108(1), 457–464
(2016). doi:10.1007/s11192-016-1904-7
3. Aagaard, K., Bloch, C., Schneider, J.W.: Impacts of performance-based research funding
systems: the case of the Norwegian Publication Indicator. Res. Eval. 24(2), 106–117 (2015).
doi:10.1093/reseval/rvv003
4. Nikolić, S., Penca, V., Ivanović, D., Surla, D., Konjović, Z.: Storing of bibliometric
indicators in CERIF data model. In: International Conference on Internet Society
Technology (2013). doi:10.13140/2.1.2196.5121
5. Quix, C., Matthias, J.: Information integration in research information systems. Procedia
Comput. Sci. 33, 18–24 (2014). doi:10.1016/j.procs.2014.06.004
6. Jörg, B.: CERIF: the common European research information format model. Data Sci. J. 9,
24–31 (2010). doi:10.2481/dsj.CRIS4
7. Rampāne, I., Rozenberga, G.: Latvijas Universitātes publikāciju citējamība datubāzēs
(2012–2015). Alma Mater (vasara), pp. 26–28 (2016)
8. Cabinet Regulation No. 1316: Regulations regarding calculation and assignment of
grant-based funding for research institutions. https://likumi.lv/doc.php?id=262508
9. Sivertsen, G.: Data integration in Scandinavia. Scientometrics 106(2), 849–855 (2016).
doi:10.1007/s11192-015-1817-x
10. Kulczycki, E., Korzeń, M., Korytkowski, P.: Toward an excellence-based research funding
system: evidence from Poland. J. Informetr. 11(1), 282–298 (2017). doi:10.1016/j.joi.2017.
01.001
11. Galimberti, P., Mornati, S.: The Italian model of distributed research information
management systems: a case study. Procedia Comput. Sci. 106, 183–195 (2017). doi:10.
1016/j.procs.2017.03.015
12. DSpace-CRIS Home. https://wiki.duraspace.org/display/DSPACECRIS/DSpace-CRIS+Home
13. The International Organisation for Research Information. http://eurocris.org/cerif/main-
features-cerif
14. Teixeira da Silva, J.A., Memon, A.R.: CiteScore: a cite for sore eyes, or a valuable,
transparent metric? Scientometrics 111(1), 553–556 (2017). doi:10.1007/s11192-017-2250-0
15. Moed, H.F.: Measuring contextual citation impact of scientiﬁc journals. J. Informetr. 4(3),
265–277 (2010). doi:10.1016/j.joi.2010.01.002
16. Gonzalez-Pereira, B., Guerrero-Bote, V.P., Moya-Anegon, F.: A new approach to the metric
of journals’ scientiﬁc prestige: the SJR indicator. J. Informetr. 4(3), 379–391 (2010). doi:10.
1016/j.joi.2010.03.002
17. Hardcastle, J.: New journal citation metric – impact per publication (2014). http://
editorresources.taylorandfrancisgroup.com/new-journal-citation-metric-impact-per-
publication/
18. Winkler, W.: The state of record linkage and current research problems. Technical report,
Statistics of Income Division, US Census Bureau (1999)
19. Niedrite, L., Solodovnikova, D.: University IS architecture for the research evaluation
support. In: 11th International Scientiﬁc and Practical Conference “Environment.Technol-
ogy. Resources”, pp. 112–117. Rezekne Academy of Technologies, Rezekne (2017). doi:10.
17770/etr2017vol2.2528
136
L. Niedrite et al.

Evaluating Reference String Extraction Using
Line-Based Conditional Random Fields: A Case
Study with German Language Publications
Martin K¨orner1(B)
, Behnam Ghavimi2
, Philipp Mayr2
,
Heinrich Hartmann3
, and Steﬀen Staab1
1 Institute for Web Science and Technologies,
University of Koblenz-Landau, Koblenz, Germany
{mkoerner,staab}@uni-koblenz.de
2 GESIS – Leibniz Institute for the Social Sciences, Cologne, Germany
{behnam.ghavimi,philipp.mayr}@gesis.org
3 Independent, Munich, Germany
heinrich@heinrichhartmann.com
Abstract. The extraction of individual reference strings from the ref-
erence section of scientiﬁc publications is an important step in the cita-
tion extraction pipeline. Current approaches divide this task into two
steps by ﬁrst detecting the reference section areas and then grouping the
text lines in such areas into reference strings. We propose a classiﬁca-
tion model that considers every line in a publication as a potential part
of a reference string. By applying line-based conditional random ﬁelds
rather than constructing the graphical model based on individual words,
dependencies and patterns that are typical in reference sections provide
strong features while the overall complexity of the model is reduced.
We evaluated our novel approach RefExt against various state-of-the-art
tools (CERMINE, GROBID, and ParsCit) and a gold standard which
consists of 100 German language full text publications from the social
sciences. The evaluation demonstrates that we are able to outperform
state-of-the-art tools which rely on the identiﬁcation of reference section
areas.
Keywords: Reference extraction · Citations · Conditional random
ﬁelds · German language papers
1
Introduction
Citation data shows the link between eﬀorts of individual researchers, topics, and
research ﬁelds. Despite the widely acknowledged beneﬁts, the open availability
of citation data is unsatisfactory. Some commercial companies such as Elsevier
and Google do have access to citation data and utilize them to supply their users
with eﬀective information retrieval features, recommendation systems, and other
knowledge discovery processes. Yet, the majority of smaller information retrieval
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 137–145, 2017.
DOI: 10.1007/978-3-319-67162-8 15

138
M. K¨orner et al.
systems, such as Sowiport [1] for the social sciences, lack comprehensive citation
data.
Recent activities like the “OpenCitations Project” or the “Initiative for Open
Citations” aim to open up this ﬁeld and improve the current situation. The
“Extraction of Citations from PDF Documents” (EXCITE) project1 at GESIS
and University of Koblenz-Landau is in line with these initiatives and aims to
make more citation data available to researchers with a particular focus on the
German language social sciences. The shortage of citation data for the interna-
tional and German social sciences is well known to researchers in the ﬁeld and
has itself often been subject to academic studies [2]. In order to open up citation
data in the social sciences, the EXCITE project develops a set of algorithms for
the extraction of citation and reference information from PDF documents and
the matching of reference strings against bibliographic databases.
In this paper, we will consider the earlier steps in the extraction process that
result in individual reference strings. There are several factors that result in the
diﬃculty of the reference extraction task. One such factor is the high number of
possible reference styles. According to Zotero2, there exist more than four hun-
dred diﬀerence citation styles in the social sciences alone. Further, there exists
a large variety of layouts for publications including diﬀerent section headings,
headers, footers, and varying numbers of text columns. Figure 1 shows three
challenging examples where the reference section does not contain a heading,
where the reference strings contain a line break after the author names, and
where reference strings strongly diﬀer in their length, respectively.
Current solutions that perform reference string extraction have in common
that they ﬁrst identify the reference section and then, in a separate step, segment
the reference section into individual reference strings. Thereby, errors that are
made during the classiﬁcation of reference sections directly impact the accuracy
of the reference string extraction. For example, if a paragraph that contains ref-
erence strings was not recognized as part of the reference section, its reference
strings will not be considered in the following step. To prevent this, our approach
does not extract reference strings from an area that is ﬁrst identiﬁed as the refer-
ence section. Instead, this indicator of a possible reference section is considered
as only one of many features in a machine learning model that directly classiﬁes
the text lines as reference strings given the full text of the research paper. Other
features are based on the text layout and the content of a given text line. A key
observation here is that a text line usually does not contain information of more
than one reference string. This allows the model to operate not on a word level
but on a text line level.
The performance of our approach was evaluated using a novel gold standard
for publications in the German language social sciences. To allow for a fair
comparison, existing methods were retrained on the same data set that is used
in our approach. As a result, the evaluation also provides insights into how well
existing methods adapt to publications in the German language social sciences.
1 https://west.uni-koblenz.de/en/research/excite.
2 https://www.zotero.org/styles/.

Evaluating Reference String Extraction
139
Fig. 1. Examples for diﬃcult reference sections. The publications are part of the eval-
uation dataset and have the SSOAR-IDs 35306, 43525, and 48511, respectively. Doc-
uments available from http://www.ssoar.info/ssoar/handle/document/<ID> by replac-
ing <ID> with the corresponding SSOARID.
The remainder of this paper is structured as follows. In Sect. 2, we present
the related work in the area of reference string extraction. Section 3 introduces a
novel approach3 to reference string extraction that does not rely on the detection
of reference zones. This approach is evaluated in Sect. 4 using a new gold standard
for reference string extraction in the area of German language social sciences.
Section 5 contains a summary and possible future work.
2
Related Work
There exists a considerable amount of literature about the extraction of biblio-
graphic information from the reference section of scientiﬁc publications [4–10].
Reviewing this literature shows that there are two categories of approaches.
One group concentrates on the reference string segmentation task by assuming
the reference strings to be given [4–6]. The other group considers the reference
string extraction from an article in the PDF or text format [7–10]. Further, all
reference string extraction approaches follow two common steps.
3 This approach was described in a preprint by K¨orner [3].

140
M. K¨orner et al.
The ﬁrst step identiﬁes the text areas of the publication that contain the
reference strings. Councill, Giles, and Kan [8] as well as Wu et al. [9] use a
set of regular expressions to locate the beginning and end of reference sections.
Tkaczyk et al. [10] apply a layout analysis on publications given as PDF ﬁles
which results in textual areas that are grouped into zones. These zones are
then classiﬁed as “metadata”, “body”, “references”, or “other” using a trained
Support Vector Machines (SVMs) model [10]. Lopez [7] trains a conditional
random ﬁeld (CRF) [11] model that performs a segmentation of textual areas
into zones similar to Tkaczyk et al. [10].
In a second step, the lines in the identiﬁed areas are grouped into individual
reference strings. Councill, Giles, and Kan [8] as well as Wu et al. [9] apply reg-
ular expressions to detect possible markers of reference strings such as numbers
or identiﬁers surrounded by brackets. If such markers are found, the lines are
grouped accordingly. If no markers are found, the lines are grouped based on
the line length, ending punctuation, and strings that appear to be author name
lists [8]. Tkaczyk et al. [10] use the k-means learning algorithm to perform a clus-
tering into two groups: The ﬁrst lines of reference strings and all other lines. The
features for this clustering include layout information such as the distance to the
previous line and textual information such as a line ending with a period [10].
As with the reference area detection, Lopez [7] learn a CRF model for this task.
This model uses an input format that is diﬀerent from the one that is used for
their ﬁrst CRF model. Tokens are split at white spaces and for each token, a list
of features is created. Such features include layout information such as the font
size and font weight as well as textual features such as the capitalization of the
token and whether the token resembles a year, location, or name.
3
Approach
As previously discussed, a typical problem of existing reference string extrac-
tion approaches is a wrong classiﬁcation of textual areas during the ﬁrst step
(see Sect. 2). Another key insight is that reference strings commonly start in a
new line. This is used in the previously described k-means clustering algorithm
by Tkaczyk et al. [10] and provides potential advantages over a word-based
approach. For example, a line-based model drastically reduces the number of
assigned target variables while still allowing the expression of relevant features.
Further, it can capture patterns that repeat every few lines more naturally than
a word-based model which focuses on a more local context.
These two insights are leveraged by applying a line-based classiﬁcation model
on the whole publication. For this, a possible set of labels consists of B-REF,
I-REF, and O where B-REF denotes the ﬁrst line of a reference string, I-REF
a line of a reference string which is not the ﬁrst line, and O any other line.
This is based on the Beginning-Intermediate-Other (BIO) notation [12]. For our
evaluation, we assigned one of the three labels to every text line in a publication.
Having such a labeling, it is then possible to automatically extract the reference
strings by concatenating a line labeled with B-REF together with the following
lines labeled with I-REF until reaching a line that is labeled with B-REF or O.

Evaluating Reference String Extraction
141
Our model RefExt4 uses both textual and layout features. In our evaluation
we used textual features that signalize whether a line only consists of a num-
ber, starts with a capitalized letter, ends with a number, ends with a period,
ends with a comma, or contains a year, a year surrounded by braces, a page
range, an ampersand character, a quotations mark, a colon, a slash, or opening
and closing braces. Another type of textual features counts the occurrences of
numbers, words, periods, commas, and words that only consist of one capital-
ized letter. Further, we used four layout features. One signalizes whether the
current line is indented when compared to the previous line. Another detects a
gap between the current and previous line that is larger than a predeﬁned value.
The third layout feature is assigned to a line that contains less characters than
the previous one. The last layout feature signalizes the position of a given line
in the whole document. For this, the current line number is divided by the total
number of lines. A more detailed description of the used features, together with
all evaluation results, is provided on GitHub5.
One advantage of using CRFs is that features do not have to be independent
from each other due to the modeled conditional probability distribution [13].
Another advantage is the possibility to include contextual information. To do
so, a CRF model with a high Markov order can be applied. This is feasible due
to the line-based approach and the resulting lower number of random variables
in the model when compared to a word-based approach.
4
Evaluation
The gold standard that is used in the following evaluation is based on 100
German publications from the SSOAR repository6 in the PDF format. Since
this evaluation focuses on the reference string extraction, documents that con-
sist of scanned pages or that do not contain a reference section were excluded
beforehand from the otherwise random selection. The resulting papers contain
an average of 54 reference strings with a total number of 5,355 reference strings.
Figure 2 gives an overview of the publication types and publication years of the
gold standard documents. Resulting from the fact that existing reference string
extraction tools use diﬀerent input formats for their training procedures and also
show diﬀerences in text-encoding of the resulting reference strings, a number of
annotation ﬁle formats were created and manually inspected. This resulting gold
standard is available on GitHub7.
Since most existing citation information extraction tools focus on English
language publications, the possibility to adapt the tool to German language
publications is crucial. Two tools that allow such retraining are CERMINE [10]
and GROBID [7]. For ParsCit [8], an older version allows the adaption of the
regular expressions that detect reference section headings and other relevant
4 https://github.com/exciteproject/refext.
5 https://github.com/exciteproject/amsd2017.
6 http://www.ssoar.info/.
7 https://github.com/exciteproject/ssoar-gold-standard.

142
M. K¨orner et al.
Thesis
Collection
Conference Paper
Journal Article
Monograph
Report
Review
Working Paper
1993
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
Year of Publication
0
1
2
3
4
5
6
Number of Publications
Fig. 2. Distribution of papers in the gold standard based on the publication type and
year of publication.
headings such as appendices. Other tools that do not allow a retraining, such
as PDFX [14] and pdfextract8, were excluded due to their low performance on
German language publications. The evaluation considers the performance on a
line level based on the BIO notation in Table 1 and on a reference string level in
Table 2 using the metrics macro precision, macro recall, and macro F1-score.
To compute macro metrics, a metric is ﬁrst calculated on the individual pub-
lications and then averaged over all publications. Thereby, publications with a
large amount of reference strings do not have a bigger impact on the ﬁnal metric
than publications with only a few reference strings. Further, in order to reduce
the inﬂuence of the chosen split into training and testing data, the evaluation
was performed using 10-fold cross-validation where each fold contains ten ran-
domly chosen papers of the gold standard for testing and the remaining ninety
papers for training. In the result tables, CERMINE, GROBID, and ParsCit are
abbreviated with CER, GRO, and Pars, respectively. The suﬃxes D and T sig-
nalize whether the tool was using its default model or a model that was trained
on the ninety publications from the gold standard. Pars-M represents version
101101 of ParsCit with modiﬁed regular expressions that match against German
language section headings such as “Literatur” and “Anhang”. Pars-D uses the
latest ParsCit version as of May 31, 2017 which instead uses a trained model
8 https://www.crossref.org/labs/pdfextract.

Evaluating Reference String Extraction
143
Table 1. Macro-metrics of BIO-annotated reference lines using 10-fold cross-validation
on 100 German social science publications.
Metric
CER-D CER-T Pars-D Pars-M GRO-D GRO-T RefExt-T
B-REF Precision 0.719
0.734
0.683
0.769
0.692
0.871
0.916
B-REF Recall
0.600
0.557
0.620
0.688
0.789
0.865
0.952
B-REF F1-Score
0.616
0.589
0.616
0.689
0.712
0.861
0.922
I-REF Precision 0.729
0.755
0.577
0.678
0.664
0.857
0.882
I-REF Recall
0.340
0.313
0.809
0.843
0.839
0.871
0.944
I-REF F1-Score
0.432
0.415
0.647
0.716
0.703
0.855
0.902
Table 2. Macro-metrics of reference string extraction using 10-fold cross-validation on
100 German social science publications.
Metric
CER-D CER-T Pars-D Pars-M GRO-D GRO-T RefExt-T
Precision 0.296
0.303
0.558
0.617
0.627
0.847
0.879
Recall
0.233
0.220
0.552
0.595
0.718
0.839
0.906
F1-Score
0.245
0.235
0.542
0.590
0.650
0.837
0.885
for detecting reference sections. We were not able to retrain this model with the
given source code and documentation. Further, we used CERMINE version 1.13
and GROBID version 0.4.1. Our approach, RefExt version 0.1.0, is based on
the CRF models of MALLET [15] as well as the PDF text extraction and read-
ing order detection of CERMINE. For the ﬁnite state transducer of MALLET
we applied states in the “three-quarter” order. Thereby, the network contains a
weight between every pair of adjacent labels as well as between the label and its
corresponding features. Further, we applied a set of conjunctions that allow the
usage of features of the previous two and following two lines. We found this net-
work structure to perform similar to more complex structures while providing a
reduced training time and a lower risk of overﬁtting. The learning was performed
using the label log-likelihood with L1-regularization with a weight of 20.
The results show that RefExt is able to outperform the other tools on our
gold standard. Over the 100 documents divided into ten folds, there were two
publications9 for which RefExt had a recall of zero in terms of reference strings.
In both cases, the year number appeared at the end of the reference strings
which is uncommon for the training corpus. In addition, one of the reference
styles includes a line break after the listed authors in a reference string10 which
is also unusual. GROBID had a recall of zero in seven publications in terms
of reference strings. Interestingly, the two publications that were problematic
9 http://www.ssoar.info/ssoar/handle/document/32521 and http://www.ssoar.info/
ssoar/handle/document/43525.
10 Shown as the ﬁrst example in Fig. 1.

144
M. K¨orner et al.
for RefExt had a recall of 1.0 and 0.662 in GROBID, respectively. Thereby, a
combined approach might be worthwhile.
5
Summary and Future Work
We have presented a novel approach to reference string extraction using line-
based CRFs. The evaluation demonstrated that this approach outperforms exist-
ing tools when trained on the same amount of annotated data it the area of
German language social sciences. Yet, there are several aspects that require fur-
ther eﬀorts. Having a precision and recall of around 0.9 is not suﬃcient for the
usage in a productive system and it remains to be evaluated how the perfor-
mance improves when extending the training data. Improvements might also be
possible by adding more domain-speciﬁc features. Examples for such features
are last name dictionaries or words that commonly appear in German language
reference strings such as “Hrsg.” and “Zeitschrift”. Further, a number of jour-
nals in the German social sciences such as “Totalitarismus und Demokratie”11
and “S¨udosteurop¨aische Hefte”12 use a citation style where references are not
grouped in a separate reference section but instead appear in the footnotes. This
could present an interesting use case of our approach.
Acknowledgements. This work has been funded by Deutsche Forschungsgemein-
schaft (DFG) as part of the project “Extraction of Citations from PDF Documents
(EXCITE)” under grant numbers MA 3964/8-1 and STA 572/14-1. We would like
to thank Dominika Tkaczyk for her support regarding the CERMINE tool as well
as Alexandra Bormann, Jan H¨ubner, and Daniel Kosti´c for contributing to the gold
standard that was used in this research.
References
1. Hienert, D., Sawitzki, F., Mayr, P.: Digital library research in action-supporting
information retrieval in Sowiport. D-Lib Mag. 21(3/4) (2015)
2. Moed, H.F.: Citation Analysis in Research Evaluation, vol. 9. Springer, Dordrecht
(2005)
3. K¨orner, M.: Reference String Extraction Using Line-Based Conditional Random
Fields. ArXiv e-prints (2017)
4. Peng, F., McCallum, A.: Information extraction from research papers using con-
ditional random ﬁelds. Inf. Process. Manage. 42(4), 963–979 (2006)
5. Cortez, E., da Silva, A.S., Gon¸calves, M.A., Mesquita, F., de Moura, E.S.: FLUX-
CiM: ﬂexible unsupervised extraction of citation metadata. In: Proceedings of
the 7th ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 215–224. ACM
(2007)
6. Groza, T., Grimnes, G.A., Handschuh, S.: Reference information extraction and
processing using conditional random ﬁelds. Inf. Technol. Libr. (Online) 31(2), 6
(2012)
11 http://www.hait.tu-dresden.de/td/home.asp.
12 http://suedosteuropaeische-hefte.org/.

Evaluating Reference String Extraction
145
7. Lopez, P.: GROBID: combining automatic bibliographic data recognition and term
extraction for scholarship publications. In: Agosti, M., Borbinha, J., Kapidakis, S.,
Papatheodorou, C., Tsakonas, G. (eds.) ECDL 2009. LNCS, vol. 5714, pp. 473–474.
Springer, Heidelberg (2009). doi:10.1007/978-3-642-04346-8 62
8. Councill, I.G., Giles, C.L., Kan, M.Y.: ParsCit: an open-source CRF reference
string parsing package. In: Proceedings of LREC, vol. 2008, pp. 661–667 (2008)
9. Wu, J., Williams, K., Chen, H.H., Khabsa, M., Caragea, C., Ororbia, A., Jordan,
D., Giles, C.L.: CiteSeerX: AI in a digital library search engine. In: AAAI, pp.
2930–2937 (2014)
10. Tkaczyk, D., Szostek, P., Fedoryszak, M., Dendek, P.J., Bolikowski, L.: CERMINE:
automatic extraction of structured metadata from scientiﬁc literature. Int. J. Doc.
Anal. Recogn. (IJDAR) 18(4), 317–335 (2015)
11. Laﬀerty, J., McCallum, A., Pereira, F., et al.: Conditional random ﬁelds: proba-
bilistic models for segmenting and labeling sequence data. In: Proceedings of the
Eighteenth International Conference on Machine Learning, ICML, vol. 1, pp. 282–
289 (2001)
12. Houngbo, H., Mercer, R.E.: Method mention extraction from scientiﬁc research
papers. In: COLING 2012, 24th International Conference on Computational Lin-
guistics, Proceedings of the Conference: Technical Papers, 8–15 December 2012,
Mumbai, India, pp. 1211–1222 (2012)
13. Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Tech-
niques. MIT Press (2009)
14. Constantin, A., Pettifer, S., Voronkov, A.: PDFX: fully-automated PDF-to-XML
conversion of scientiﬁc literature. In: Proceedings of the 2013 ACM symposium on
Document engineering, pp. 177–180. ACM (2013)
15. McCallum, A.K.: MALLET: a machine learning for language toolkit (2002)

CEUR Make GUI - A Usable Web Frontend
Supporting the Workﬂow of Publishing
Proceedings of Scientiﬁc Workshops
Muhammad Rohan Ali Asmat1,3(B) and Christoph Lange2,3
1 RWTH Aachen, Aachen, Germany
m.rohan.a.asmat@gmail.com
2 University of Bonn, Bonn, Germany
math.semantic.web@gmail.com
3 Fraunhofer IAIS, Sankt Augustin, Germany
Abstract. CEUR-WS.org is a widely used open access repository for
computer science workshop proceedings. To publish a proceedings vol-
ume there, workshop organisers have to follow a complex, error-prone
workﬂow, which mainly involves the creation and submission of an
HTML table of contents. With ceur-make we had previously provided
a command-line tool for partially automating this workﬂow. However,
in a recent usability evaluation we conﬁrmed that the tool is diﬃcult to
learn, highly dependent on other software, not portable and hard to use.
We sought to solve these issues with a web-based user interface, which
we present here. A usability evaluation of the latter proves signiﬁcant
improvements.
Keywords: Scholarly publishing · Open access · User experience · User
interfaces
1
Introduction
Scientiﬁc social networks such as ResearchGate and free-of-charge open access
repositories such as the Computing Research Repository (CoRR1) have sig-
niﬁcantly lowered the barrier for sharing research results in the form of indi-
vidual papers. Open access repositories for complete proceedings of scientiﬁc
events include the Proceedings of Machine Learning Research (PMLR) and
the Electronic Proceedings in Theoretical Computer Science (EPTCS), address-
ing speciﬁc ﬁelds of computer science, and the CEUR Workshop Proceedings
(CEUR-WS.org), addressing workshops from all over computer science.2 Each
of these employ an individual workﬂow for publishing, which proceedings editors
and/or authors need to follow strictly to keep the eﬀort low for those who run
the service, usually volunteers. For example, PMLR requires editors to provide
1 http://arxiv.org.
2 http://proceedings.mlr.press/, http://www.eptcs.org/, http://ceur-ws.org.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 146–157, 2017.
DOI: 10.1007/978-3-319-67162-8 16

CEUR Make GUI - A Usable Web Frontend Supporting the Workﬂow
147
a BibTeX metadata database following speciﬁc rules3, EPTCS acts as an over-
lay to CoRR, i.e. requires papers to be pre-published there, and CEUR-WS.org
requires editors to provide an HTML table of contents following a certain struc-
ture4. Here, we focus on facilitating the latter by adding a web-based graphical
user interface to a tool that auto-generates such tables of content, improving over
the usability issues of the previous standalone command-line version of that tool.
Section 2 provides a more precise problem statement. Section 3 discusses
related work. Section 4 presents the design and implementation of our web-based
user interface. Section 5 evaluates the usability of the frontend in comparison to
its command-line backend. Section 6 concludes with an outlook to future work.
2
Problem Statement
2.1
The Publishing Workﬂow
The HTML table of contents of a CEUR-WS.org workshop proceedings volume
includes metadata about the workshop (title, date, venue, proceedings editors,
etc.) and each of its papers (title, authors). This structure is prescribed5; around
once a year, it has so far evolved a bit, e.g., in the form of more explicit seman-
tic annotations to facilitate reuse of the metadata. Besides following the latest
template and producing syntactically valid HTML, requirements for proceed-
ings editors include following a consistent capitalisation scheme for paper titles,
providing full names of authors, and using relative links to the full texts of the
individual papers (typically PDF ﬁles). The HTML table of contents together
with the full texts has to be submitted to CEUR-WS.org as a ZIP archive.
2.2
Automation of the Workﬂow with Ceur-Make
Traditionally, proceedings editors had to prepare the submission ZIP ﬁle man-
ually. With ceur-make6, the second author, technical editor of CEUR-WS.org,
has provided a tool to automate part of this job – aiming at three objectives:
– Helping proceedings editors to learn more quickly how to create a table of
contents, reducing their eﬀort, and helping recurrent editors to cope with
structural changes.
– Reducing the workload of the volunteers who carry out the subsequent pub-
lishing steps at CEUR-WS.org; so far, around one in ten submissions requires
further communication with its editors to resolve problems, mainly rooted in
the table of contents.
– Reducing the implications that subsequent improvements to the structure of
the table of contents have on both proceedings editors and the CEUR-WS.org
team by reducing their exposure to manual editing.
3 http://proceedings.mlr.press/spec.html.
4 http://ceur-ws.org/HOWTOSUBMIT.html#PREPARE.
5 http://ceur-ws.org/Vol-XXX/.
6 https://github.com/ceurws/ceur-make.

148
M.R.A. Asmat and C. Lange
For ceur-make, the metadata about the workshop and its papers have to be pro-
vided in two XML ﬁles. ceur-make can auto-generate the latter XML ﬁle from
the metadata that the widely used EasyChair submission and review manage-
ment system exports in LNCS mode (cf. Sect. 3.1). From these two XML ﬁles,
ceur-make auto-generates an HTML table of contents and ﬁnally a ZIP archive
conforming with the CEUR-WS.org requirements. In addition, ceur-make can
generate a BibTeX database to facilitate the citation of the papers in a pro-
ceedings volume, as well as a copyright form by which the authors agree to the
publication of their papers with CEUR-WS.org.
2.3
Shortcomings of Ceur-Make
Shortcomings of ceur-make include that it depends on a Unix-style command line
environment and a number of software packages that typically only developers
have installed: the Make build automation tool7, the Saxon XSLT processor and
the Perl scripting language. Furthermore, it requires proceedings editors to edit
one or two XML ﬁles manually, without validating their content with regard
to all rules that editors should follow. It also requires them to follow certain
conventions for naming and arranging ﬁles and directories; most importantly,
the sources of ceur-make have to be downloaded to the same directory in which
the proceedings volume is being prepared. These reasons may explain why ceur-
make has so far only been used for less than one in ten proceedings volumes.
2.4
Research Objectives
The objectives of our research were 1. to assess the shortcomings of ceur-make
in a more systematic way, and 2. to overcome them by providing a user-friendly
web frontend to ceur-make.
3
Related Work
3.1
Conference Management Systems
The complex process of managing scientiﬁc events (conferences, workshops, etc.)
is facilitated by a broad range of systems, of which we brieﬂy review three
representatives and their proceedings generation capabilities. Parra et al. have
reviewed further systems without providing details on proceedings generation [6].
In computer science, EasyChair8 enjoys particularly wide usage.9 EasyChair
features a special “proceedings manager” interface, which is initialised by adding
all accepted papers and then supports the collection of the ﬁnal (“camera ready”)
versions, including a printable form (usually PDF), editable sources (LATEX,
7 https://www.gnu.org/software/make/.
8 http://www.easychair.org.
9 EasyChair has so far been used to manage 53,739 events and has had 1,954,080 users
(http://www.easychair.org/users.cgi, accessed 2017-04-18).

CEUR Make GUI - A Usable Web Frontend Supporting the Workﬂow
149
Word, or anything else, e.g., HTML, in a ZIP archive), and a copyright transfer
form. Proceedings chairs can deﬁne an order of papers and add or edit addi-
tional documents such as a preface. Speciﬁc support for exporting all these ﬁles
and their metadata is provided for events that publish in Springer’s Lecture
Notes in Computer Science (LNCS) series. Microsoft’s Conference Manage-
ment Toolkit (CMT10) assists with publishing accepted papers to CoRR. With
a professional license, ConfTool11 supports the export of metadata in multiple
formats (Excel, XML and CSV) to facilitate proceedings generation.
3.2
Usability Evaluation of Command Line Vs. GUI
Comparing the usability of command-line (CLI) vs. graphical user interfaces
(GUI) has been a long-standing research topic. Hazari and Reaves have evaluated
the performance of students in technical writings tasks using a graphical word
processor vs. a command-line tool [3]. Starting from the same level of background
knowledge and given the same time for training, a signiﬁcantly larger share of
users felt comfortable using the GUI rather than the command line; also, their
task-based performance was slightly higher with the GUI. Gracoli is an operating
system shell with a hybrid user interface that combines GUI and CLI [12]. Its
design is motivated by common drawbacks of CLIs, which are stated as follows: –
the user can interact with the application in a limited way; – the output is hard
to understand for the user;– the user does not easily get a clue of how to perform
a task.
4
Design and Implementation of CEUR Make GUI
4.1
Architecture
The CEUR Make GUI is a graphical layer built on top of ceur-make. Figure 1
shows its three-layer architecture (Interface, Middleware, and Storage).
The Interface Layer consists of all the presentation elements. It displays
visual elements, handles dependencies on external libraries for user interface
elements, styles the web pages, validates forms and manages user interaction the
web pages. It also initiates the communication with the Middleware Layer on
user’s request and displays the results from the Middleware. Technologies used
on Interface Layer include standard web technologies used for front end clients
(HTML 5, CSS, JavaScript), and the following libraries: Materialize CSS12 is
a JavaScript and CSS library based on Google’s Material Design principles13,
used here to incorporate standard design patterns into the GUI. jQuery Steps14
is used to create wizards for taking inputs.
10 https://cmt.research.microsoft.com/cmt/.
11 http://www.conftool.net/.
12 http://materializecss.com.
13 https://material.io/guidelines/.
14 http://www.jquery-steps.com.

150
M.R.A. Asmat and C. Lange
Fig. 1. System architecture of CEUR make graphical user interface
The Middleware Layer generates artifacts required for publishing at
CEUR-WS.org. The Middleware Layer creates the ﬁles, as requested through
the Interface Layer, by running ceur-make. It returns links to the artifacts stored
at the Storage Layer to the Interface Layer, thus acting as a service provider.
The Storage Layer stores the ﬁles that are created temporarily on the
server. It separates the ﬁles based on the user’s identity and then also based on
the workﬂow that the user chooses to create the artifacts for publishing (manual
metadata input vs. EasyChair import).
4.2
Interface
We aim at providing an easy to use, task oriented interface. On the main screen,
we give users the option of switching between four tasks: viewing announcements,
viewing published proceedings, publishing a proceeding and reporting an issue
through a navigational menu (cf. Fig. 2). Further, we separate the site navigation
of the two proceedings publishing workﬂows using a Card design pattern
[9],
representing each workﬂow as an independent card (cf. Fig. 3a). We follow the
Wizard design pattern [11] to collect workshop metadata input from users (cf.
Fig. 3b). For the list of all proceedings volumes15, we follow the style of the
Fig. 2. Navigational menu of CEUR make graphical user interface
15 For now, we only implemented the list of proceedings volumes as a hard-coded
mockup for the purpose of evaluating the usability of our user interface design.

CEUR Make GUI - A Usable Web Frontend Supporting the Workﬂow
151
(b) Workshop metadata input wizard
(a) Workﬂows for Publishing Proceedings
Fig. 3. Workﬂow screens
CEUR-WS.org user interface but make it more accessible by following standard
design patterns. We applied the Pagination design pattern [10] to address the
problem of the current CEUR-WS.org site that one has to scroll down a lot
because all content is displayed at once; secondly, we applied the Autocomplete
design pattern [8] to facilitate the task of ﬁnding proceedings volumes already
published at CEUR-WS.org easier.
Source code, documentation and a working installation of the CEUR Make
GUI are available at https://github.com/ceurws/ceur-make-ui.
5
Evaluation
5.1
Methodology
Participants. Twelve persons participated in the evaluation of the usability of
the ceur-make CLI vs. the GUI. We chose nine participants with previous CEUR-
WS.org publishing experience16 and three participants without. The latter were
trained to publish at CEUR-WS.org to avoid learning biases in our evaluation
results.
Procedure. The participants were divided into two groups based on their avail-
ability. Those who were physically available participated in a Thinking Aloud
test [7], and the other ones participated in a Question Asking test:
Thinking Aloud: Participants were provided with task deﬁnitions as explained
below. They were asked to think aloud about their plans and their interac-
tion with the system, particularly including problems they faced or unusual
mental models, while the evaluator took notes. The task completion time was
recorded for the purpose of comparison.
16 Among them we would have preferred to have some with ceur-make experience, but
this proved infeasible given the small number of people who had ever used it.

152
M.R.A. Asmat and C. Lange
Question Asking: In a video conferencing setting with screen sharing (using
Skype), the evaluator performed each task according to its deﬁnition. The
participants were allowed to ask questions during the usability test, where
the evaluator also asked questions to test the user’s understanding. From an
audio recording, the evaluator compiled a transcript of pain points afterwards.
Following a within-subject design setup17, each participant ﬁrst had to test
the CLI and then the GUI. The participants were given four tasks to be per-
formed in each system, designed to cover all major use cases of the system in a
comparable way: 1. Initiate generation of a proceedings volume, 2. Generating
workshop metadata, 3. Generating table of contents metadata, and 4. Search
a proceedings volume. Each tasks were subdivided into smaller steps, e.g., as
follows for Task 4:
Task 4 −Search a Proceedings Volume
1. Go to the proceedings page at http://ceur-ws.org (or in the GUI, respec-
tively).
2. Search the proceedings volume that has the name “Cultures of Partici-
pation in the Digital Age 2015”.
For the full list of task deﬁnitions, please see Appendix A and B of [1].
Usability tests were followed by a post study questionnaire for each user,
which was created and ﬁlled using Google Forms18. The questionnaire was
divided into following sections:
System Usability Scale (SUS [4]), a ten point heuristic questionnaire to
evaluate general usability of the system on a Likert scale from 1 (strongly
agree) to 5 (strongly disagree).
Question for User Interaction Satisfaction (QUIS [5]), a 27 point ques-
tionnaire to evaluate speciﬁc usability aspects of the system, covering overall
reaction to the software, screen layout, terminology and system information,
as well as learning and system capabilities, from a scale from 0 (lowest) to 9
(highest). The mean score was calculated for every user.
Dataset. All users used the same input data for both systems to ensure unbiased
comparability of the content created and of completion times across users and
systems. A full record of the data is provided in Appendix A and B of [1].
5.2
Results
This section summarizes the evaluation results; for full details see Appendix C
and D of [1].
17 https://web.mst.edu/∼psyworld/within subjects.htm.
18 https://www.google.com/forms/about/.

CEUR Make GUI - A Usable Web Frontend Supporting the Workﬂow
153
Table 1. Quantitative usability evaluation results using thinking aloud
Tasks
CEUR Make (Min) CEUR Make GUI (Min)
Task 1 0.13
0.10
Task 2 4.77
2.88
Task 3 2.40
1.46
Task 4 0.76
0.10
Quantitative Results (Completion Times). Table 1 shows the completion
times per system and task - in detail:
1. Task 1 (Initiate generation of a proceedings volume): This required
entering a terminal command for the CLI and pressing a button in the GUI.
On average, this took less time in the CLI, but the diﬀerence is too marginal
to be signiﬁcant.
2. Task 2 (Generating Workshop Metadata): This required entering work-
shop metadata into the GUI input wizard, and using a text editor and the
command line in the CLI. The diﬀerence in completion time is signiﬁcant:
completing the task using the GUI took only 60% of the time taken using the
CLI, which emphasizes the user-friendliness of the GUI.
3. Task 3 (Generating Table of Contents Metadata): This required enter-
ing metadata of two papers similarly as for Task 2, with similar results.
4. Task 4 (Search a proceedings volume): This task took 7.6 times as long
on the CEUR-WS.org homepage compared to the GUI. This result highlights
the importance of using the autocomplete design pattern for searching in the
graphical user interface, compared to just the “ﬁnd in page” search built into
browsers.
Overall, users took signiﬁcantly less time to complete tasks with the GUI, which
proves the usability improvement it provides over the CLI.
Qualitative Results. Notes recorded while performing the usability test were
categorized in ten heuristics, i.e.: Speed in performing a task, Documenta-
tion of the software, Ease in performing a Task, Learnability of the software,
clear Navigation structure of the system, Portability of the system, Error
correction chances, easy to use Interface, Dependency on other systems and
Features to be added. Table 2 shows the number of responses of the twelve par-
ticipants for each qualitative heuristic, where “bad” means they were not com-
fortable using it, “good” means they liked the software, and “excited” means that
the user is interested but would like to see more features to be implemented.
The total number of qualitative responses was 36 for the CLI and 34 for
the GUI. 15 good responses were recorded for the CLI, regarding the heuris-
tics Speed, Documentation and Task, whereas 21 bad responses were recorded

154
M.R.A. Asmat and C. Lange
Table 2. Qualitative results for ceur-make and the CEUR make GUI
Heuristics
ceur-make # of responses
from users
CEUR make GUI # of responses
from users
Speed
Good
3
Neutral
–
Documentation Good
4
Neutral
–
Task
Good
8
Neutral
–
Learnability
Bad
5
Good
5
Navigation
Bad
4
Good/Bad
5/2
Portability
Bad
2
Good
2
Error
Bad
2
Good
3
Interface
Bad
4
Good
8
Dependency
Bad
4
Good
3
Feature
Neutral
–
Excited
6
Total responses
36
34
regarding the heuristics Learnability, Portability, Navigation, Error, Interface
and Dependency. For the GUI no response was recorded against the heuristics
Speed, Documentation and Task, which were reported as good for the CLI. This
was the case because users did not require documentation to operate the GUI
as they never requested for it from the evaluator, speed was not an issue while
using it as they never complained about it, and it enabled them to perform
their respective tasks. 26 good responses were recorded for the GUI against the
heuristics Learnability, Portability, Navigation, Error, Interface and Dependency,
which were all recorded as bad in case of the CLI. This highlights the usability
improvement provided by the GUI over the CLI. Only two bad responses were
recorded for the GUI, against the heuristic Navigation, which means a slight
improvement in navigation is required −as quoted by a user: “ceur-make make
things easier but has a complex setup, whereas the GUI is straightforward and
requires no prior learning. With little improvement in the ﬂow of screens it could
be even better.”
Moreover, for the GUI, 6 responses were recorded as excited, against the
heuristic Feature, which means users would like to use the software and would
like additional features to be integrated.
Post Evaluation Questionnaire. The overall usability of the two systems was
evaluated using System Usability Scale19. The SUS score for ceur-make was
41.25, which is below grade F (x-axis) as shown in Fig. 4. This rating demands
immediate usability improvements. On the other hand, the SUS score of the GUI
was 87.08, which is above grade A (x-axis). This means that the GUI has a good
usability and its users would recommend it to others.
19 http://www.userfocus.co.uk/articles/measuring-usability-with-the-SUS.html.

CEUR Make GUI - A Usable Web Frontend Supporting the Workﬂow
155
Fig. 4. SUS score: ceur-make vs CEUR make graphical user interface
Results of the Questionnaire for User Interaction Satisfaction reﬂect
a high usability improvement of the GUI over the CLI. For the questions related
to the learnability of the system, a visible diﬀerence in mean scores was recorded
for easy to remember the commands, learning to operate the system and trying
new features by trial and error. For these three questions, mean scores of the CLI
were 3.75, 3.25, and 4.25 (all below average) and for the GUI they were 8.5, 8.25,
and 8.0 (all above average). Likewise, mean scores for the GUI for the questions
related to information representation, including information organization, posi-
tioning of messages, highlighting of information to simplify task, prompts and
progress were 7.75, 8.0, 6, 6, and 6.25 (above average), whereas for the CLI they
were 4, 4, 2.25, 4, and 3.25 −i.e.a notable diﬀerence. Another highlight was that
users appreciated that the GUI was considered to be designed for all levels of
users as backed by a mean score of 7.75, whereas the CLI was considered not to
be designed for all levels of users as its mean score was just 3.
6
Conclusion
We aimed at automating a workﬂow for publishing scientiﬁc results with open
access, focused on the CEUR-WS.org workshop proceedings repository. We
developed a graphical user interface on top of the ceur-make command line tool
and systematically evaluated the usability of both. Quantitative results on task
completion time prove that the GUI is more eﬃcient in performing common
tasks. Qualitative evaluation suggests that on all heuristics where ceur-make
performed badly, i.e., learnability, navigation, portability, error, interface and
dependency, the GUI yielded good responses. In the post-evaluation question-
naires, a notable diﬀerence was recorded in the SUS scores of the two systems:

156
M.R.A. Asmat and C. Lange
grade F for ceur-make vs. grade A for the GUI. 11 out of 27 QUIS questions of
ceur-make had responses below average, and others were satisfactory, whereas
for the GUI all responses were above average. Overall, results indicate that the
usability of the GUI has noticeably improved over the command line. As our
evaluation setup covered most typical tasks of proceedings editors, the results
suggest that the GUI makes the overall process of publishing with CEUR-WS.org
more eﬀective and eﬃcient and thus will attract a broad range of users. Thanks to
the input validation of the metadata wizard and to the detailed explicit semantic
RDFa annotations of tables of contents that ceur-make outputs, broad usage of
the GUI will improve the quality of CEUR-WS.org metadata, largely eliminating
the need for reverse-engineering data quality by information extraction (cf. [2]).
Future Work The next immediate step is to oﬃcially invite all CEUR-WS.org
users to use the GUI for preparing their proceedings volumes. Partly inspired
by feedback from the evaluation participants, we are planning to enhance the
GUI with functionality addressing the following use cases (all ﬁled as issues at
https://github.com/ceurws/ceur-make-ui/issues/): User Proﬁles would help
to automatically suggest information related to the editors while working on
that section of the workshop metadata (Issue #1). Even without user proﬁles,
building and accessing a database of previously published workshops’ metadata
would facilitate input, e.g., by auto-completing author names, and by reusing
metadata of a workshop’s previous edition. The RDF linked open data embed-
ded into ceur-make generated tables of contents or extracted from old tables of
contents (cf. [2]) can serve as such a database once collected in a central place
(#5). A Collaborative Space for Editors would support multiple editors to
work in parallel on a proceedings volume (#2). Saving System State would
improve user experience and give users more control (#4). Currently, there is
no way to restore the state of the interface, where one left in case the browser
is accidentally closed or users want to complete the task later. Extraction of
Author Names, Titles and Page Numbers from the full texts of the papers
would further lower task completion time, as the system would automatically
suggest most metadata (#3).
References
1. Asmat, M.R.A.: A Usable Web Frontend for Supporting the Workﬂow of Publishing
Proceedings of Scientiﬁc Workshops. MA thesis. RWTH Aachen (2016). http://
eis-bonn.github.io/Theses/2016/Rohan Asmat/thesis.pdf
2. Dimou, A., et al.: Challenges as enablers for high quality linked data: insights
from the semantic publishing challenge. In: PeerJ Computer Science: Semantics,
Analytics, Visualisation: Enhancing Scholarly Data (2017)
3. Hazari, S.I., Reaves, R.R.: Student preferences toward microcomputer user inter-
faces. Comput. Educ. 22, 225–229 (1994)
4. Measuring Usability with the System Usability Scale (SUS). MeasuringU (2017).
https://measuringu.com/sus/. Accessed 17 Apr 2017
5. Norman, K.L., Shneiderman, B.: Questionnaire for User Interaction Satisfaction
QUIS. http://www.lap.umd.edu/quis/. Accessed 25 Sep 2016

CEUR Make GUI - A Usable Web Frontend Supporting the Workﬂow
157
6. Parra, L., et al.: Comparison of Online Platforms for the Review Process of Con-
ference Papers. In: CONTENT (2013)
7. Thinking
Aloud:
The
Number
1
Usability
Tool.
Nielsen
Norman
Group
(2017).
https://www.nngroup.com/articles/thinking-aloud-the-1-usability-tool/.
Accessed 10 Apr 2017
8. User Interaction Design Pattern Library: Autocomplete. UIPatterns (2017).
http://ui-patterns.com/patterns/Autocomplete. Accessed 17 Apr 2017
9. User Interaction Design Pattern Library: Card. UIPatterns (2017). http://
ui-patterns.com/patterns/cards. Accessed 17 Apr 2017
10. User Interaction Design Pattern Library: Pagination. UIPatterns (2017). http://
ui-patterns.com/patterns/Pagination. Accessed 17 Apr 2017
11. User Interaction Design Pattern Library: Wizard. UIPatterns (2017). http://
ui-patterns.com/patterns/Wizard. Accessed 17 Apr 2017
12. Verma, P.: Gracoli : a graphical command line user interface. In: CSCW (2013)

The 1st Workshop on Novel Techniques
for Integrating Big Data
(BigNovelTI 2017)

A Framework for Temporal Ontology-Based
Data Access: A Proposal
Sebastian Brandt1, Elem G¨uzel Kalaycı2, Vladislav Ryzhikov2,
Guohui Xiao2(B), and Michael Zakharyaschev3
1 Siemens CT, Munich, Germany
sebastian-philipp.brandt@siemens.com
2 KRDB Research Centre for Knowledge and Data,
Free University of Bozen-Bolzano, Bolzano, Italy
{kalayci,ryzhikov,xiao}@inf.unibz.it
3 Department of Computer Science and Information Systems,
Birkbeck, University of London, London, UK
michael@dcs.bbk.ac.uk
Abstract. Predictive analysis gradually gains importance in industry.
For instance, service engineers at Siemens diagnostic centres unveil hid-
den knowledge in huge amounts of historical sensor data and use this
knowledge to improve the predictive systems analysing live data. Cur-
rently, the analysis is usually done using data-dependent rules that are
speciﬁc to individual sensors and equipment. This dependence poses sig-
niﬁcant challenges in rule authoring, reuse, and maintenance by engi-
neers. One solution to this problem is to employ ontology-based data
access (OBDA) that provides a conceptual view of data via an ontology.
However, classical OBDA systems do not support access to temporal data
and reasoning over it. To address this issue, we propose a framework of
temporal OBDA. In this framework, we use extended mapping languages
to extract information about temporal events in RDF format, classical
ontology and rule languages to reﬂect static information, as well as a
temporal rule language to describe events. We also propose a SPARQL-
based query language for retrieving temporal information and, ﬁnally,
an architecture of system implementation extending the state-of-the-art
OBDA platform Ontop.
Keywords: Temporal logic · Ontology-based data access · Ontop
1
Introduction
Analysis of the logs sensor data is an important problem in industry, as it reveals
crucial insights into the performance and conditions of devices. The outcomes
of this analysis, known also as retrospective diagnostics, enables IT experts to
improve the capabilities of real-time systems monitoring abnormal or potentially
dangerous events developing in devices, i.e., the systems that perform predictive
diagnostics. For complex devices (such as those we consider in the use-case below)
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 161–173, 2017.
DOI: 10.1007/978-3-319-67162-8 17

162
S. Brandt et al.
such events do not amount to simply measurable instances (say, the temperature
above 100◦C), but involve a number of measurements from sensors attached to
a device, with each of them having a certain temporal duration and occurring
in a certain temporal sequence.
Our central use-case is monitoring turbine conditions at Siemens, which main-
tains thousands of devices related to power generation, including gas and steam
turbines. It provides operational support for these devices through a global net-
work of more than 50 remote diagnostic centres. These centres are linked to a
common database centre. Siemens wants to employ retrospective and predic-
tive diagnostics in order to anticipate problems with turbines and take relevant
countermeasures. A major challenge in this task is the access to heterogeneous
data sources, since various turbine models have diﬀerent schemas of underlying
databases storing sensor measurements.
We propose a solution based on detaching the conceptual view of an event,
such as ‘HighRotorSpeed of the turbine tb01 in the period from 2017-06-06
12:20:00 to 2017-06-06 12:20:03’ from a concrete database(s) that store the log
of the sensors of that turbine. Thus, we work in the spirit of ontology-based data
access (OBDA) [11,23], where only the high level conceptual layer of classes
and properties is exposed to the end-users, while the complex structure of the
underlying data sources is hidden from them. (In fact, those classes and proper-
ties are mapped to the data source schemas through a declarative speciﬁcation.)
In addition, an ontology is used to model the domain of interest by asserting
conceptual relations (e.g., isA) between the classes and properties. There are
several systems implementing the OBDA approach, some of which (Ontop1 and
Morph2) are freely available, whereas others (Stardog3, Mastro4 and Ultrawrap5)
are distributed under commercial licences.
Unfortunately, none of the available OBDA systems supports access to tem-
poral data well enough to work with the events relevant to our analysis. On the
one hand, the common mapping languages are not tailored towards extracting
validity time intervals of conceptual assertions. On the other hand—and this is a
bigger problem—the supported ontology languages do not allow one to construct
classes and properties whose temporal validity depends on the validity of other
classes and properties, which is essential for deﬁning complex temporal events.
(In fact, the OBDA systems used industrially are based on the lightweight ontol-
ogy languages, such as OWL 2 QL proﬁle of the Web Ontology Language (OWL)
and DL-Lite [11], a lightweight Description Logic, in order to guarantee maxi-
mally eﬃcient query answering.) When limited to a classical ontology language,
one approach to enable the extraction of temporal events is by extending the
end-user query language with various temporal operators [5,7,15,17,18,20,22].
However, this leaves the burden of encoding the complex events in temporal
1 http://ontop.inf.unibz.it/.
2 https://github.com/oeg-upm/morph-rdb.
3 http://stardog.com/.
4 http://www.obdasystems.com/mastro.
5 https://capsenta.com/.

Temporal Ontology-Based Data Access
163
queries to the end-user. In the Siemens scenario, this is a prohibitive limitation,
since the end-users are the Siemens service engineers who are not trained in
temporal logic.
Therefore, we pursue a more expressive setting, where the ontology language
is extended by temporal operators that are capable of deﬁning complex temporal
events. In several works [3,4,6], non-temporal ontology languages were extended
with LTL-based operators such as ‘next time’ or ‘eventually’. Unfortunately,
sensor data tend to come at irregular time intervals, which makes it impossible
to adequately represent ‘10 s’ or ‘1 min’ in LTL. The same problem occurs if
metric temporal logic is used with assumption of discrete time [14]. To overcome
this limitation, a language datalogMTL based on metric temporal logic MTL
over dense time was proposed [8]. This language also appears to be capable of
capturing the events of interest for the diagnostic tasks at Siemens.
In this paper, using a running example from the Siemens use-case, we sketch
our proposal of a framework for temporal OBDA with datalognrMTL, a non-
recursive version of datalogMTL, as a rule language to describe events. At the
same time, this framework supports the standard OWL 2 QL language to model
static knowledge (such as a conﬁguration of modules of a turbine), and extends
it with non-recursive datalog rules to describe static knowledge of more complex
structure. We outline the extension of the standard mapping language R2RML
and the query language SPARQL to extract information on the validity intervals
of conceptual assertions. Finally, we discuss an implementation of the proposed
framework in the OBDA system Ontop.
2
A Framework of Temporal OBDA
In this section, we present our framework for temporal OBDA. We ﬁrst consider
the notion of OBDA speciﬁcation, which is comprised of a set of mappings,
static ontology, and static and temporal rules. Then we deﬁne a temporal OBDA
instance and its speciﬁcation through RDF triples. Finally, we present a query
language for those instances based on τ-SPARQL [24].
2.1
Temporal OBDA Speciﬁcation and Instance
A traditional OBDA speciﬁcation is a triple

O, M, S

, where O is an ontology,
M a set of mapping assertions between ontological entities and data sources,
and S a database schema. As we want to build temporal OBDA on top of the
standard atemporal one, we allow the use of existing speciﬁcations, calling O a
static ontology, M a set of static mapping assertions, and S a database schema.
In what follows we will extend the static vocabulary Σs of classes and properties
occurring in O and M by a disjoint temporal vocabulary Σt. We now describe
static ontologies in more detail using an example from the Siemens use case.

164
S. Brandt et al.
Static Ontology. At Siemens, the devices used for power generation are mon-
itored by many diﬀerent kinds of sensors reporting the temperature, pressure,
vibration and other relevant measurements. Siemens uses an ontology in order
to model the static knowledge regarding the machines and their deployment
proﬁles, sensor conﬁgurations, component hierarchies and functional proﬁles.
Example 1 below gives a snippet of the static conceptual model formalized in
the syntax of the description logic DL-LiteR [2].
Example 1. The signature Σs of the Siemens static ontology O contains the
following ontological entities (concepts and roles):
Train, Turbine, GasTurbine, SteamTurbine, TurbinePart,
PowerTurbine, Burner, Sensor, RotationSpeedSensor,
TemperatureSensor, isMonitoredBy, isPartOf, isDeployedIn.
Some of the axioms (concept inclusions) of O are shown below:
GasTurbine ⊑Turbine,
SteamTurbine ⊑Turbine,
RotationSpeedSensor ⊑Sensor,
TemperatureSensor ⊑Sensor,
PowerTurbine ⊑TurbinePart,
Burner ⊑TurbinePart,
∃isMonitoredBy ⊑TurbinePart,
∃isMonitoredBy−⊑Sensor,
∃isPartOf ≡TurbinePart,
∃isPartOf−⊑Turbine,
∃isDeployedIn ⊑Turbine,
∃isDeployedIn−⊑Train.
It turns out, however, that the language of DL-LiteR is not able to capture
all the static knowledge required in the Siemens use case. We complement this
ontology with nonrecursive datalog static rules.
Static Rules. In the Siemens use case, we have turbine parts that are monitored
by a number of diﬀerent co-located sensors within the same turbine, for example,
a temperature sensor and a rotation speed sensor. This situation can be readily
described by a datalog rule with a ternary predicate and a complex body such
as the one in the example below, but not by DL-LiteR axioms.
Example 2. Static rules R can be of the form
ColocTempRotSensors(tb, ts, rs) ←isMonitoredBy(pt, ts), TemperatureSensor(ts),
isMonitoredBy(pt, rs), RotationSpeedSensor(rs),
isPartOf(pt, tb), Turbine(tb).
Note that we extend Σs to contain also the (static) symbols occurring in R.
We also require those rules to be non-recursive (that is, contain no cycles in the
deﬁnitions of their predicates). Finally, observe that the static rules in R may
contain concepts and roles from O.

Temporal Ontology-Based Data Access
165
Temporal Rules. Siemens is interested in detecting abnormal situations in the
working equipment as well as in monitoring running tasks in order to see whether
they proceed ordinarily. A typical event that is crucial to monitor would be a
normal start of a turbine. However, this complex event is composed of many
subevents distributed in time, each of which is characterized by a temporal
duration. One of these subevents is described in the following example.
Example 3. Purging is Over is a complex event of a certain turbine such that
the following hold:
(i) there is a pair of sensors co-located in some part of a turbine, one of which
is a rotor speed sensor and the other one a temperature sensor;
(ii) the temperature sensor detects that the main ﬂame was burning for at
least 10 s;
(iii) at the same time, the following occurred within preceeding 10 min:
– the rotor speed sensor measures the speed of at most 1000 rpm for at least
30 s,
– within preceding 2 min the rotor speed measured at least 1260 rpm for at
least 30 s.
The described event is illustrated below:
PurgingIsOver
ts
tb
rs
MainFlameOn
10s
10m
2m
HighRotorSpeed
30s
LowRotorSpeed
1m
Temporal diagnostic patterns of this sort can be described by means of a
datalognrMTL program [8], which is a set of nonrecursive datalog rules with
temporal operators of the metric temporal logic MTL [1]. The event ‘purging is
over’ can be formalized by the following datalognrMTL program T .
Example 4. The program T includes the rules:
PurgingIsOver(tb) ←⊟[0s,10s] MainFlameOn(ts),
(0,10m]

⊟(0,30s] HighRotorSpeed(rs),
(0,2m] ⊟(0,1m] LowRotorSpeed(rs)

,
ColocTempRotSensors(tb, ts, rs).
HighRotorSpeed(tb) ←rotorSpeed(tb, v), v > 1260.
LowRotorSpeed(tb) ←rotorSpeed(tb, v), v < 1000.

166
S. Brandt et al.
In such programs, we require a (countably inﬁnite) temporal alphabet Σt that
is disjoint from the static Σs. Intuitively, the predicates in Σt hold on domain
objects within some validity time intervals. This is in contrast to the predicates
in Σs that are assumed to hold independently of the time (or, equivalently, at
all time instants). In datalognrMTL programs like T , only predicates from Σt
can occur in the head of the rules, whereas their bodies can contain predicates
from both Σt and Σs. Moreover, as mentioned above, we require T to be non-
recursive. Thus, intuitively, the temporal rules in T deﬁne temporal predicates
in terms of both temporal and static ones. In the example above, Σt contains
the predicates
PurgingIsOver, MainFlameOn, rotorSpeed, HighRotorSpeed, LowRotorSpeed.
Databases and Mappings. In our approach, we assume that databases have
generic schemas. However, in temporal OBDA, we have to deal with temporal
data. Therefore, we are particularly interested in databases with tables contain-
ing timestamp columns.
Example 5. An example data schema S for the Siemens data, including sensor
measurements and deployment details, can look as follows (the primary keys of
the tables are underlined):
tb measurement

timestamp, sensor id, value

,
tb sensors

sensor id, sensor type, mnted part, mnted tb

,
tb deployment

turbine id, turbine type, deployed in

,
tb components

turbine id, component id, component type

.
Snippets
of
data
from
the
tables
tb measurement,
tb sensor
and
tb components, respectively, are given below:
tb measurement
timestamp
sensor id value
2017-06-06 12:20:00
rs01
570
2017-06-06 12:21:00
rs01
680
2017-06-06 12:21:30
rs01
920
2017-06-06 12:22:50
rs01
1278
2017-06-06 12:23:40
rs01
1310
...
2017-06-06 12:32:30
mf01
2.3
2017-06-06 12:32:50
mf01
1.8
2017-06-06 12:33:40
mf01
0.9
...
tb sensors
sensor id sensor type mnted part mnted tb
rs01
0
pt01
tb01
mf01
1
b01
tb01
...
...
tb components
turbine id component id component type
tb01
pt01
0
tb01
b01
1
...
In classical OBDA, mapping assertions take the form φ(x) ⇝ψ(x), where
φ(x) is a query over the schema S and ψ(x) is an atom mentioning a symbol in
Σs and variables x.

Temporal Ontology-Based Data Access
167
Example 6. Given the static ontology O, the alphabet Σs from Example 1, and
the schema S from Example 5, the following are examples of mapping assertions:
SELECT sensor id AS X FROM tb sensors
WHERE sensor type = 1 ⇝TemperatureSensor(X)
SELECT component id AS X FROM tb components
WHERE component type = 1 ⇝Burner(X)
SELECT mnted part AS X, sensor id AS Y
FROM tb sensors ⇝isMonitoredBy(X, Y)
By applying these mapping assertions to the databases from Example 5, we
retrieve the following facts:
Burner(b01), TemperatureSensor(mf01),
isMonitoredBy(pt01, rs01), isMonitoredBy(b01, mf01).
We call the mapping assertions that deﬁne concepts of Σs static mapping asser-
tions and denote sets of them by Ms.
On the other hand, we also require temporal mapping assertions of the form
φ(x, begin, end) ⇝ψ(x)@⟨tbegin, tend⟩
where φ(x, begin, end) is a query over S such that the variables begin and end
are mapped to values of the date/time format, tbegin is either a variable begin or
a constant temporal value including ∞, −∞(and similarly for tend), ‘⟨’ is either
‘(’, indicating that an interval is left-open, or ‘[’, indicating that an interval is
left-closed (and similarly for ‘⟩’). Temporal mapping assertions are required to
deﬁne predicates from Σt only. We denote sets of such mapping assertions by
Mt.
Example 7. Given Σt and T from Example 4 and the schema S from Example 5,
the following is an example of a temporal mapping assertion:
SELECT sensor id, value,
timestamp AS begin, LEAD(timestamp, 1) OVER W AS end
FROM tb measurement
WINDOW W AS (PARTITION BY sensor id ORDER BY timestamp)
⇝rotorSpeed(sensor id, value)@[begin, end)
The temporal facts we retrieve from the database in Example 5 are as follows:
rotorSpeed(rs01, 570)@[2017-06-06 12:20:00, 2017-06-06 12:21:00),
rotorSpeed(rs01, 680)@[2017-06-06 12:21:00, 2017-06-06 12:21:30),
rotorSpeed(rs01, 920)@[2017-06-06 12:21:30, 2017-06-06 12:22:50),
rotorSpeed(rs01, 1278)@[2017-06-06 12:22:50, 2017-06-06 12:23:40).

168
S. Brandt et al.
For instance, the ﬁrst fact above states that the rotor speed measured by the
sensor rs01 was 570 in the speciﬁed interval. Note that the interval is left-
closed and right-open, which reﬂects the logic of how turbine sensor outputs are
produced: a sensor outputs a value when a measured value diﬀers suﬃciently
from the previously returned value.
Temporal OBDA Speciﬁcation. Thus, in the temporal OBDA framework,
an OBDA speciﬁcation is the octuple
S = ⟨Σs, Σt, Ms, Mt, O, R, T , S⟩,
where Σs (Σt) is a static (respectively, temporal) vocabulary, Ms (Mt) a set of
static (respectively, temporal) mapping assertions, O an ontology, R (T ) a set
of static (respectively, temporal) rules, and S is a database schema. In the table
below we clarify the intuition behind the components of S and the speciﬁcation
languages.
Component Deﬁnes predicates in In terms of predicates in Language
Ms
Σs
S
R2RML
Mt
Σt
S
R2RML
O
Σs
Σs
DL-LiteR/OWL 2 QL
R
Σs
Σs
non-recursive datalog
T
Σt
Σs ∪Σt
datalognrMTL
Temporal OBDA Instance. A temporal OBDA instance I is a pair ⟨S, D⟩
with a temporal OBDA speciﬁcation S and a database instance D compliant
with the database schema S in S.
Recall that the semantics of a classical OBDA instance is given by the exposed
RDF graph consisting of a set of triples of the form (subject, property, object);
see, e.g., [19]. In temporal OBDA, we advocate the use RDF Datasets, that is,
a set of named RDF graphs and a default graph, following the model of RDF
stream proposed by the W3C RDF Stream Processing Community Group [13].
To model temporal facts, for each validity interval, we introduce a graph identi-
ﬁer and collect the triples within the interval into this graph. The details of inter-
vals (namely, the beginning and the end) of these graph identiﬁers are described
in the default graph using the vocabulary from the W3C TIME ontology [26].
The static triples also reside in the default graph. We leave a formal deﬁnition
of our semantics to future work and only explain here the underlying intuition
using the following example.
Example 8. The RDF dataset exposed by our running example includes the
static facts in Example 6 and the temporal facts in Example 7. For instance,

Temporal Ontology-Based Data Access
169
the static facts Burner(b01), isMonitoredBy(b01, mf01) are represented by two
triples
(b01, a, Burner), (b01, isMonitoredBy, mf01).
The temporal fact
rotorSpeed(rs01, 570)@[2017-06-06 12:20:00, 2017-06-06 12:21:00)
is modeled as the named graph
GRAPH g0 {(rs01, rotorSpeed, 570)}
and the following set of triples in the default graph:
(g0, a, time:Interval),
(g0, time:isBeginningInclusive, true), (g0, time:isEndInclusive, false),
(g0, time:hasBeginning, b0), (b0, time:inXSDDateTimeStamp, ‘2017-06-06 12:20:00’),
(g0, time:hasEnd, e0), (e0, time:inXSDDateTimeStamp, ‘2017-06-06 12:21:00’).
2.2
Query Answering
The RDF datasets exposed by temporal OBDA instances are ready to be queried
with a proper query language. We begin by an example of a query that is of
interest in the context of the Siemens use-case.
Example 9. Suppose there is a train with the ID T001 in which a gas turbine
is deployed. An interesting query would be ‘ﬁnd all gas turbines deployed in the
train with the ID T001 and time periods of their accomplished purgings’.
Our query language is based on τ-SPARQL proposed in [24]. In τ-SPARQL,
the atomic triple patterns, such as
?person a : Person
or
?child : childOf ?parent
can be preﬁxed by an expression [?v1, ?v2] that represents a validity time interval
of the pattern. In our case, the intervals have open and closed ends, therefore we
extend the format of the timestamp to ⟨?e1, ?v1, ?v2, ?e2⟩, where ?e1 is either
‘true’ or ‘false’ depending on whether the validity time interval of the statement
is left-closed or left-open, and similarly for ?e2. Moreover, we alter the notation
of τ-SPARQL and write intervals in a preﬁx, as it is more consistent with our
notation for OBDA instances. Thus, we formulate the query from Example 9 as:
PREFIX ss: <http :// siemens.com/ns#>
PREFIX st: <http :// siemens.com/temporal/ns#>
PREFIX
obda: <https :// w3id.org/obda/vocabulary#>
SELECT ?tb ?left_edge ?begin ?end ?right_edge
WHERE {
?tb a ss:GasTurbine ; ss:isDeployedIn ss:train_T001 .
{?tb a st:PurgingIsOver }@<? left_edge ,?begin ,?end ,? right_edge >
}

170
S. Brandt et al.
Note that we allow the use of @⟨?e1, ?v1, ?v2, ?e2⟩only as a suﬃx of atomic
patterns whose predicates are taken from Σt. In fact, the use of a SPARQL-based
query language does not allow us to construct queries with predicates of arity
more than 2 like ColocTempRotSensors from Example 2. However, this does not
seems to be a major limitation, since such predicates as ColocTempRotSensors
are auxiliary (the latter was needed for a convenient deﬁnition of the unary
temporal predicate PurgingIsOver in Example 4).
Similarly to τ-SPARQL, our query language is a shorthand format for tem-
poral queries and each query is translated into standard SPARQL using the
GRAPH constructor. For example, the query above is translated as follows:
PREFIX xsd: <http :// www.w3.org /2001/ XMLSchema#>
PREFIX
time: <http :// www.w3.org /2006/ time#>
PREFIX ss: <http :// siemens.com/ns#>
PREFIX st: <http :// siemens.com/temporal/ns#>
PREFIX
obda: <https :// w3id.org/obda/vocabulary#>
SELECT ?tb ?left_edge ?begin ?end ?right_edge
WHERE {
?tb a ss:GasTurbine ; ss:isDeployedIn ss:train_T001 .
GRAPH ?g { ?tb a st:PurgingIsOver .}
?g a time:Interval ;
time: isBeginningInclusive ?left_edge
;
time:hasBeginning [ time: inXSDDateTimeStamp ?begin ] ;
time:hasEnd [ time: inXSDDateTimeStamp ?end ] ;
time: isEndInclusive ?right_edge .
}
3
System Architecture and Implementation in Ontop
In this section, we describe a system architecture of temporal OBDA, including
concrete languages for the inputs and a query answering algorithm, and also how
to implement this architecture by extending the OBDA system Ontop.
Concrete Languages. Our principle when choosing concrete languages for the
inputs is to be compliant with relevant existing standards whenever possible; we
only extend and create new syntax/languages when it is absolutely necessary.
Recall that, in classical OBDA systems (e.g., Ontop [9] and Mastro [10]), the
adopted concrete ontology language is usually OWL 2 QL [21], the language for
mappings is R2RML, and the query language is SPARQL 1.1 [16] under OWL 2
QL entailment regime [19,25]. Ontop also provides a more compact mapping
language that is equivalent to R2RML.
In temporal OBDA, we continue to use OWL 2 QL for static ontologies. For
the temporal rule language, we propose a new language since there is no existing
language available. The proposed concrete syntax for datalognrMTL is inspired

Temporal Ontology-Based Data Access
171
by Datalog, SWRL, and SPARQL. We continue to use R2RML as a mapping
language. Meanwhile, noticing that it is rather verbose to map all the tempo-
ral information in R2RML, we also extend the Ontop mapping language [9]
to provide an alternative compact mapping language close to the one used in
Example 7. As for the query language, we support both τ-SPARQL based lan-
guage and plain SPARQL as discussed in Sect. 2.2.
Query Answering Algorithm. The core of a temporal OBDA system is the
query answering algorithm. We propose a rewriting-based algorithm extending
the structure proposed in [9]. The algorithm takes as inputs a temporal OBDA
instance (S, D) and a SPARQL query q, and returns the answers of q over
(S, D). The algorithm consists of two main stages: (1) an oﬄine stage compiling
S into a set of mapping assertions MS using the mapping saturation technique,
(2) an online stage translating q into an SQL query Q with respect to MS,
evaluating Q over D, and converting the SQL answers to SPARQL answers. We
give some details of the components in this algorithm:
The oﬄine stage has two sub-stages:
(i) The ﬁrst sub-stage can be thought of as consisting of two phases: In phase
(a) the system classiﬁes the static ontology O as in [9] and in phase (b) it
saturates the input mapping Ms with the classiﬁed ontology and obtains
the saturated mapping MO
s ;
(ii) the second sub-stage also has two phases: In phase (a) the system saturates
MO
s with static rules R as in [27] and obtains the mapping MO,R
s
; and in
phase (b) it saturates Mt with MO,R
s
and T w.r.t. [8], and obtains the ﬁnal
saturated mapping MS.
The online stage is for answering temporal queries. In step (1), the ontology-
mediated query is rewritten according to the classiﬁed ontology and normalized
datalognrMTL program. Then in step (2) the rewritten query is unfolded by
substituting each triple occurring in the rewritten query by its saturated mapping
speciﬁcation MS. Obviously, it is not expected that the resulting unfolded query
is eﬃcient. So, in step (3), the system applies some optimization techniques in
order to eliminate redundant self-joins and replace joins of unions with unions of
joins. Then, in step (4), the system translates the optimized query into the target
query languages, and in step (5) it delegates the evaluation of the translated
query to DB engines. Finally in step (6), the results retrieved from DB engines
are transformed into SPARQL answer form.
Implementation in Ontop. Ontop is a state-of-the-art OBDA system devel-
oped at the Free University of Bozen-Bolzano. It is the core OBDA engine
of the EU FP7 Optique project focused on end-user access to big data [12].
More recently, Ontop has also been integrated in Stardog to provide support
for SPARQL end-user queries through a virtual RDF graph. Ontop supports the
standard W3C recommendations related to OBDA (such as OWL 2 QL, R2RML,
SPARQL, and the OWL 2 QL entailment regime in SPARQL). The system is

172
S. Brandt et al.
available as a Protege plugin, and an extensible open-source Java library sup-
porting OWL API and RDF4J.
We aim to support temporal OBDA in Ontop by taking advantage of the com-
ponents already available in Ontop (e.g., (i), (5), (6)) for the query answering
algorithm, implementing the new components (i.e., (ii)), and extending further
some of the existing components (i.e., (1), (2), (3) and (4)).
Acknowledgements. This research has been partially supported by the project
“Ontology-based analysis of temporal and streaming data” (OBATS), funded through
the 2017 call issued by the Research Committee of the Free University of Bozen-
Bolzano.
References
1. Alur, R., Henzinger, T.A.: Real-time logics: complexity and expressiveness. Inf.
Comput. 104(1), 35–77 (1993)
2. Artale, A., Calvanese, D., Kontchakov, R., Zakharyaschev, M.: The DL-Lite family
and relations. J. Artif. Intell. Res. 36(1), 1–69 (2009)
3. Artale,
A.,
Kontchakov,
R.,
Kovtunova,
A.,
Ryzhikov,
V.,
Wolter,
F.,
Zakharyaschev, M.: First-order rewritability of temporal ontology-mediated
queries. In: Proceedings of IJCAI 2015, pp. 2706–2712. AAAI Press (2015)
4. Artale, A., Kontchakov, R., Wolter, F., Zakharyaschev, M.: Temporal description
logic for ontology-based data access. In: Proceedings of the 23rd International Joint
Conference on Artiﬁcial Intelligence, IJCAI 2013. IJCAI/AAAI (2013)
5. Baader, F., Borgwardt, S., Lippmann, M.: Temporalizing ontology-based data
access. In: Bonacina, M.P. (ed.) CADE 2013. LNCS, vol. 7898, pp. 330–344.
Springer, Heidelberg (2013). doi:10.1007/978-3-642-38574-2 23
6. Basulto, V.G., Jung, J., Kontchakov, R.: Temporalized EL ontologies for accessing
temporal data: complexity of atomic queries. In: Proceedings of the 25th Interna-
tional Joint Conference on Artiﬁcial Intelligence (IJCAI 2016). AAAI Press (2016)
7. Borgwardt, S., Lippmann, M., Thost, V.: Temporal query answering in the descrip-
tion logic DL-Lite. In: Proceedings of FroCoS 2013, pp. 165–180 (2013)
8. Brandt, S., Kalaycı, E.G., Kontchakov, R., Ryzhikov, V., Xiao, G., Zakharyaschev,
M.: Ontology-based data access with a horn fragment of metric temporal logic. In:
AAAI (2017)
9. Calvanese, D., Cogrel, B., Komla-Ebri, S., Kontchakov, R., Lanti, D., Rezk, M.,
Rodriguez-Muro, M., Xiao, G.: Ontop: answering SPARQL queries over relational
databases. Semant. Web 8(3), 471–487 (2017)
10. Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-
Muro, M., Rosati, R., Ruzzi, M., Savo, D.F.: The mastro system for ontology-based
data access. Semant. Web J. 2(1), 43–53 (2011). Listed among the 5 most cited
papers in the ﬁrst ﬁve years of the Semantic Web Journal
11. Calvanese, D., Giacomo, G., Lembo, D., Lenzerini, M., Rosati, R.: Tractable rea-
soning and eﬃcient query answering in description logics: the DL-Lite family. J.
Autom. Reasoning 39(3), 385–429 (2007)
12. Giese, M., Soylu, A., Vega-Gorgojo, G., Waaler, A., Haase, P., Jim´enez-Ruiz, E.,
Lanti, D., Rezk, M., Xiao, G., ¨Oz¸cep, ¨O.L., Rosati, R.: Optique - zooming in on
big data access. IEEE Comput. 48(3), 60–67 (2015)

Temporal Ontology-Based Data Access
173
13. R. S. P. C. Group. RDF stream processing: Requirements and design principles.
W3C draft community group report, W3C (2016)
14. Guti´errez-Basulto, V., Jung, J.C., Ozaki, A.: On metric temporal description logics.
In: ECAI 2016, pp. 837–845 (2016)
15. Guti´errez-Basulto, V., Klarman, S.: Towards a unifying approach to representing
and querying temporal data in description logics. In: Kr¨otzsch, M., Straccia, U.
(eds.) RR 2012. LNCS, vol. 7497, pp. 90–105. Springer, Heidelberg (2012). doi:10.
1007/978-3-642-33203-6 8
16. Harris, S., Seaborne, A., Prud’hommeaux, E.: SPARQL 1.1 query language. W3C
recommendation, W3C (2013)
17. Kharlamov, E., Brandt, S., Jimenez-Ruiz, E., Kotidis, Y., Lamparter, S., Mailis,
T., Neuenstadt, C., ¨Oz¸cep, O., Pinkel, C., Svingos, C., Zheleznyakov, D., Horrocks,
I., Ioannidis, Y., Moeller, R.: Ontology-based integration of streaming and static
relational data with optique. In: Proceedings of the 2016 International Conference
on Management of Data, SIGMOD 2016, pp. 2109–2112. ACM, New York (2016)
18. Klarman, S., Meyer, T.: Querying temporal databases via OWL 2 QL. In: Pro-
ceedings of RR 2014, pp. 92–107 (2014)
19. Kontchakov, R., Rezk, M., Rodr´ıguez-Muro, M., Xiao, G., Zakharyaschev, M.:
Answering SPARQL queries over databases under OWL 2 QL entailment regime.
In: Mika, P., et al. (eds.) ISWC 2014. LNCS, vol. 8796, pp. 552–567. Springer,
Cham (2014). doi:10.1007/978-3-319-11964-9 35
20. M¨oller, R., ¨Oz¸cep, ¨O., Neuenstadt, C., Zheleznyakov, C., Kharlamov, E.: D5.1:
a semantics for temporal and stream-based query answering in an obda context.
Optique project deliverable, FP7-318338, EU (2013)
21. Motik, B., Fokoue, A., Horrocks, I., Wu, Z., Lutz, C., Cuenca Grau, B.: OWL Web
Ontology Language proﬁles. W3C Recommendation, World Wide Web Consortium
(2009)
22. ¨Oz¸cep, ¨O.L., M¨oller, R., Neuenstadt, C.: A stream-temporal query language for
ontology based data access. In: Lutz, C., Thielscher, M. (eds.) KI 2014. LNCS, vol.
8736, pp. 183–194. Springer, Cham (2014). doi:10.1007/978-3-319-11206-0 18
23. Poggi, A., Lembo, D., Calvanese, D., Giacomo, G., Lenzerini, M., Rosati, R.:
Linking data to ontologies. In: Spaccapietra, S. (ed.) Journal on Data Seman-
tics X. LNCS, vol. 4900, pp. 133–173. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-77688-8 5
24. Tappolet, J., Bernstein, A.: Applied temporal RDF: eﬃcient temporal querying of
RDF data with SPARQL. In: Aroyo, L., Traverso, P., Ciravegna, F., Cimiano, P.,
Heath, T., Hyv¨onen, E., Mizoguchi, R., Oren, E., Sabou, M., Simperl, E. (eds.)
ESWC 2009. LNCS, vol. 5554, pp. 308–322. Springer, Heidelberg (2009). doi:10.
1007/978-3-642-02121-3 25
25. W3C. SPARQL 1.1 entailment regimes. Technical report, W3C, March 2013
26. W3C. Time ontology in OWL. W3C working draft, OGC & W3C (2017)
27. Xiao, G., Rezk, M., Rodr´ıguez-Muro, M., Calvanese, D.: Rules and ontology based
data access. In: Kontchakov, R., Mugnier, M.-L. (eds.) RR 2014. LNCS, vol. 8741,
pp. 157–172. Springer, Cham (2014). doi:10.1007/978-3-319-11113-1 11

Towards Semantic Assessment
of Summarizability in Self-service
Business Intelligence
Luis-Daniel Ib´a˜nez1, Jose-Norberto Maz´on2(B), and Elena Simperl1
1 University of Southampton, Southampton, UK
{l.d.ibanez,e.simperl}@soton.ac.uk
2 DLSI, Universidad de Alicante, Alicante, Spain
jnmazon@dlsi.ua.es
Abstract. Traditional Business Intelligence solutions allow decision
makers to query multidimensional data cubes by using OLAP tools,
thus ensuring summarizability, which refers to the possibility of accu-
rately computing aggregation of measures along dimensions. With the
advent of the Web of Open Data, new external sources have been used
in Self-service Business Intelligence for acquiring more insights through
ad-hoc multidimensional open data cubes. However, as these data cubes
rely upon unknown external data, decision makers are likely to make
meaningless queries that lead to summarizability problems. To overcome
this problem, in this paper, we propose a framework that automatically
extracts multidimensional elements from SPARQL query logs and creates
a knowledge base to detect semantic correctness of summarizability.
Keywords: OLAP · Data cube · Summarizability · Open data
1
Introduction
Traditional Business Intelligence (BI) rely on a well-controlled, consistent and
certiﬁed Data Warehouse (DW) from which multidimensional cubes are designed
in order to analyze data fast and accurately. Data cubes consists of measures
or facts (representing events of interests for analysts) and dimensions (as diﬀer-
ent ways that data can be viewed, aggregated, and sorted). These Data Cubes
(see Fig. 1 for a simpliﬁed example) are the basis of OLAP (OnLine Analytical
Processing) tools that traditionally support the decision making process [5].
The advent of the Web of Data and the Open Data initiatives have made
a tremendous amount of data to become available that can be combined with
locally warehoused data, potentially improving the decision making process. The
capability of (automatically) incorporating external data for supporting decision
making processes gives rise to a novel branch of BI called Self-Service BI [1]. The
goal of Self-Service BI is to accomplish the search, extraction, integration and
querying of external data, while minimising the intervention of DW designers
or programmers. Consequently, unlike the well-controlled and closed scenario
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 174–185, 2017.
DOI: 10.1007/978-3-319-67162-8 18

Towards Semantic Assessment of Summarizability
175
Fig. 1. Sample data cube for ﬁnancial aid given by Finland, measured in amount and
commitments, covering the Recipient Country, Year and Sector dimensions. Source: [7]
Table 1. Temperatures in cities versus temperatures in reactors
City
Temperature
London
14
Southampton 19
Manchester
11
Is it meaningful to sum
temperatures across cities?
Reactor Temperature
R1
1400
R2
1900
R3
1100
Is it meaningful to sum
temperatures across reactors?
of traditional BI, users of self-service BI do not necessarily know the nature of
new external data to be analyzed [2], raising the issue of how to know if the
multidimensional queries they ask on them are meaningful (i.e., semantically
correct). This is especially signiﬁcant when computing aggregations of measures
with respect to dimensions, since summarizability problems may arise [11].
The simpliﬁed example on Table 1 illustrates the issue of semantics in summa-
rizability. On the left side, a local dataset including cities in the UK is enriched
with their temperatures, from external sources; on the right hand, a dataset con-
taining chemical reactors in an industrial complex is enriched with temperature
measures for each reactor. A syntactically-valid query for both datasets is to
sum the temperature across cities/reactors. However, depending on the seman-
tics, the query could be meaningless: while a meteorologist would argue that
computing the sum of the temperatures across cities is not meaningful (instead,
average could be more sensible), for the chief of the industrial complex, the sum
of the temperatures across reactors being below or above a certain threshold
might indicate a malfunction. Despite the syntactic correctness of both queries,
a meaningful summarization also depends on the particular semantic of the
involved multidimensional elements.
Previous works [6,9,11,15] propose categories of measures and dimensions
and develop algorithms to check if a given query ensures summarizability. How-
ever, in all of them, the task of categorising measures and dimensions is left to
the designer of the cube which disagrees with the self-service essence. Further-
more, when integrating Web Data on the ﬂy (even curated sources of statistical
open data like those coming from The World Bank1 or Eurostat2), there is no
1 http://data.worldbank.org/.
2 http://ec.europa.eu/eurostat/data/database.

176
L.-D. Ib´a˜nez et al.
metadata on measures or dimensions available. At best, they provide a verbose
explanation as part of their metadata3. Therefore, introducing manual anno-
tations is required for categorizing MD elements, which hinders the ability to
provide fast responses of self-service BI.
These issues lead us to argue that at Web-of-Data-Scale, the large amount
of possible combinations of measure, dimensions and contexts makes very hard
the development of an universal category of measures, thus, instead of trying to
automate the mapping of measure and dimension types to one of the current cate-
gorisations, we propose to leverage the knowledge of query logs, made in possibly
diﬀerent contexts, by extracting semantic consistency information from queries
made by others. The idea is to provide a hint on the semantic consistency of an
aggregation query by looking if others have made the same or a similar query (in
a crowdsourcing spirit). To improve the conﬁdence on the hints, we consider the
addition of provenance information to let users accept them or not depending
on their own parameters (from the simple existence of a similar query, to more
complex trust-based algorithms). Speciﬁcally, our contributions are as follows:
(i) a framework to enable hinting of the summarizability of multidimensional
queries based knowledge from queries issued by others, (ii) an extension to an
existing vocabulary for describing Data Cubes in RDF for expressing the seman-
tics of summarizability; and (iii) an algorithm to extract the multidimensional
elements from SPARQL queries conforming to a multidimensional pattern.
The rest of the paper is organised as follows: preliminary deﬁnitions are pro-
vided in Sect. 2; Sect. 3 reviews the related work; Sect. 4 describe our general
framework; Sect. 5 describes our extension to the QB4OLAP vocabulary; Sect. 6
describes our algorithm to extract semantics of multidimensional elements from
SPARQL queries and reports our preliminary results; and ﬁnally, Sect. 7 con-
cludes the paper and provides future work directions.
2
Preliminaries
The W3C recommends the use of the RDF Data Cube vocabulary [3]4 (QB)
to publish multidimensional data on the web. Multidimensional data in QB is
comprised by a description of the observations it holds and a set of statements
about them. Each observation is characterized by a set of dimensions that deﬁne
what the observation applies to, along with metadata describing what has been
measured, how it was measured and how the observations are expressed.
Example 1.1 shows the SPARQL version (assuming the cube is in RDF format
following QB) of the multidimensional query What is the sum of commitment aid
that the Philippines received from 2005 to 2008? Commitment is the measure,
country and year are dimensions, while sum is the aggregation function.
Example 1.1. Example SPARQL multidimensional query: What is the sum of com-
mitment aid that the Philippines received from 2006 to 2008?
PREFIX qb :
http :// purl . org / linked −data/cube#
3 E.g., http://ec.europa.eu/eurostat/cache/metadata/en/hbs esms.htm.
4 https://www.w3.org/TR/vocab-data-cube/.

Towards Semantic Assessment of Summarizability
177
PREFIX ex :
http :// example−namespace . org/#
SELECT sum(? commitment ) {
? aid a qb : Observation
.
? aid
ex : commitment ?commitment
.
? aid
ex : r e c i p i e n t −country
ex : Philippines
.
? aid
ex : refYear
?y
.
f i l t e r
( year
(? y)=2006 OR year
(? y)=2008) }
However, as pointed out in [18], QB vocabulary has no support for represent-
ing: (i) dimension hierarchies, since QB only allows representing relationships
between dimension instances, (ii) aggregate functions, (iii) descriptive attributes
of dimension hierarchy levels. To overcome these drawbacks, QB was extended
in QB4OLAP [4] to fully support the multidimensional model, also enabling the
implementation of typical OLAP operations, such as rollup, slice, dice, and drill-
across using standard SPARQL queries. Unfortunately, although QB4OLAP
includes the concept of aggregation functions and how they are associated to
measures, they do not provide the means to describe summarizability.
Executing a multidimensional querying on a data cube [17] refers to obtain a
set of measures indexed by one or more dimensions and applying an aggregation
function (e.g., sum, count or average) to it. Lenz and Shoshani [11] established
the importance of summarizability for queries on multidimensional data, since
violations of this property may lead to erroneous conclusions and decisions. They
show that summarizability is dependent on (i) types of measures and (ii) the
speciﬁc dimensions under consideration. Moreover, they state three necessary
conditions for summarizability. The ﬁrst of these conditions, called disjointness,
states that consecutive levels of a dimension hierarchy must have a many-to-one
relationship. The second condition, called completeness, requires that all the
elements of each level of a dimension hierarchy exist as well as that each element
in a level is assigned to one element of the upper level. These two conditions
are at the syntactic level and they should be solved by manipulating schema
to create hierarchies that ensure these summarizability constraints [16]. The
third condition, called type compatibility, ensures that the aggregate function
applied to a measure is summarizable according to the type of the measure
(e.g., stock, ﬂow and value per-unit as deﬁned by Lenz and Shoshani) and the
type of the related dimensions (e.g., temporal, non-temporal). According to [11],
this third condition ensures the semantic correctness of summarizability in a
multidimensional query.
After elaborating a multidimensional query and before executing it, a user
would like to know if the summarizability is semantically ensured according to
the type compatibility condition. In other words, and recalling our Example 1.1:
does it make sense to sum the commitments across recipient countries and years?.
The problem of semantic correctness of summarizability can be formalised as
follows:
Deﬁnition 1 (Semantic correctness of summarizability). Given a multi-
dimensional query Q with aggregation function F, measure M and set of dimen-
sions D = [D1, ..., Dn], decide if the application of F to the set of observations

178
L.-D. Ib´a˜nez et al.
characterized by M and D has types that are semantically compatible. If so,
we say that M is semantically summarizable by F across D, and denote it as
M
F−→D and then semantic correctness of summarizability is ensured.
Back to Example 1.1, if the commitment measure is categorised as summariz-
able across country and year following one of the approaches in [9,11,15], then
one can use their proposed methods to decide if the query is meaningful or not.
Importantly, this can be only done in traditional BI because multidimensional
elements are known before analysis and they can be categorized in advance
according to the expertise of DW designers, thus limiting users to meaning-
ful queries. However, in self-service BI, multidimensional elements coming from
external sources are unknown beforehand, and are uncategorised.
3
Related Work
An in-depth analysis of summarizability problems and a taxonomy of reasons
why summarizability may not hold is presented by Horner and Song [6]. They dis-
tinguish schema problems (e.g., disjointness and completeness) from data prob-
lems (e.g., inconsistencies and imprecision) and computational problems (e.g.,
type compatibility in the sense of [11]), give typical examples for each problem-
atic case, and suggest guidelines for their management. A survey on summa-
rizability issues and solutions is in [13]. Categorisation of measures is the main
approach for ensuring semantic correctness of summarizability. The most recent
categorization is proposed by Niemi et al. [15]. They consider measures and cate-
gorize them, depending on their nature, into tally, semi-tally, reckoning, snapshot
and conversion factor. Also, they propose an algorithm to decide summarizabil-
ity according to this novel categorization. Main pitfall of these approaches is
that they deﬁne a ﬁxed categorization for measures in order to ensure semantic
correctness of summarization queries. This is useful in traditional BI scenarios
based only on querying internal and well-known data, but is has limitations in
self-service BI scenarios that query unforeseen data from heterogeneous domains
(see simpliﬁed example on Table 1).
Several eﬀorts have advanced in the direction of realising self-service BI on
the Web of Data based on Linked Data Cubes and the QB vocabulary. K¨ampgen
et al. [8] propose a mapping from OLAP operations to SPARQL based on RDF
Data Cube. They deﬁne every multidimensional element but they do not auto-
matically deal with summarizability problems and they manually create a spe-
ciﬁc measure for each possible aggregation function. H¨oﬀner [7] deﬁnes Question-
Answering (in the sense of natural language questions) on top of Linked Data
Cubes.
Varga et al. [? ] propose a series of steps to automate the enrichment of
QB data sets with speciﬁc QB4OLAP semantics; being the most important,
the deﬁnition of aggregate functions and the detection of new concepts in the
dimension hierarchy construction. They state that not every aggregate function
can be applied to a measure and give a valid result. Therefore, techniques to

Towards Semantic Assessment of Summarizability
179
automate the association between measures and aggregate functions should be
provided. To do so, they create a mapping of measures to aggregate functions
called MAggMap. However, they assume that the user must explicitly provide
the MAggMap mapping since they state that the large variety of measure and
aggregate function types makes the compatibility check a tedious task that can
hardly be fully automated.
Nebot and Berlanga [14] consider semantic correctness of summarization in
multidimensional queries on LOD in the context of exploring the potential analy-
sis of LOD datasets. They propose a statistical framework to automatically dis-
cover candidate multidimensional patterns hidden in LOD. They consider dif-
ferent kind of aggregation semantics than the previous literature: (i) applicable
to data that can be added together; (ii) applicable to data that can be used for
average calculations; and (iii) applicable to data that is constant, i.e., it can only
be counted. However, they assume the existence of a function that gives for each
dimension type returns the compatible aggregation functions which should be
manually established by a designer or an expert of the domain.
4
Detecting Semantic Correctness of Summarizability
Our proposal focuses on leveraging the information on query logs about aggrega-
tions made by decision makers, to provide them with hints on the summarizabil-
ity of their multidimensional queries based on what others have done on their
data. Figure 2 gives an overview of the framework. The ﬂow of the framework is
as follows:
1. A decision maker wants to execute a multidimensional query on a data cube
without measure categorisation available (e.g. on a self-service BI scenario).
Before issuing the query to the cube, the decision maker asks a Summariz-
ability Hint Module (SHM) to get a hint on summarizability of the query.
2. A Summarizability Knowledge Base (SUM-KB) is previously constructed by
extracting from query logs how others have used multidimensional queries
that ensure summarizability on diﬀerent cubes and knowledge bases.
3. The SHM takes the query Q and the knowledge from SUM-KB and produces
a hint about the summarizability of Q (i.e., summarizability awareness).
4. If the decision maker accepts the hint, the query is executed on his/her data
cube and this decision of considering that Q ensures summarizability is added
to SUM-KB.
To realize this scenario, we need to solve three problems, namely (i) how to
describe semantic correctness of summarizability, (ii) how to extract semantic
correctness knowledge from query logs, and (iii) how to compute a hint from said
information. The ﬁrst problem is solved by extending the QB4OLAP vocabulary
to consider summarizability (we refer reader to Sect. 5 for further details), while
the two latter problems are formalized as follows.

180
L.-D. Ib´a˜nez et al.
Fig. 2. Overview of framework for detecting semantic correctness of summarizability
Deﬁnition 2 (Extraction of multidimensional elements (EME)). Given
a multidimensional query string Q, extract the aggregation function F, the mea-
sure M and the dimensions D it uses. We say that Q summarizes M by F across
D, and denote it as M
F=⇒D.
Section 6 proposes an algorithm to solve EME for SPARQL queries. Besides
the multidimensional elements of queries, the hint process can beneﬁt of having
available metadata about their context or provenance, e.g., who executed it, on
which dataset, etc. We formalise this as a Summarizability statement.
Deﬁnition 3 (Summarizability statement). Let M
F=⇒D be the EME of a
query Q and C a set containing the context or provenance of Q. We call the
tuple (M
F=⇒D, C) a summarizability statement.
In layman terms, a summarizability statement encodes the fact that someone
or something issued a multidimensional query with M, F and D and therefore
considered it summarizable. In Sect. 5 we provide an extension of the QB4OLAP
vocabulary that allows us to encode summarizability statements in RDF. Finally,
we formalize the problem of providing a summarizability hint:
Deﬁnition 4 (Summarizability Hint).
Given a set S of summarizabil-
ity statements and an instance of the semantic correctness of summarizability
denoted as M
F−→D (cf. Sect. 2), return one of the following three outputs:
1. M
F−→D is hinted by the statements in S
2. M
F−→D is not hinted by S
3. The knowledge in S is inconclusive to hint summarizability.
5
QB4OLAP Extension for Constructing SUM-KB
We chose to extend QB4OLAP [4] to encode summarizability statements because
it already considers aggregation functions, measures and dimensions. We want

Towards Semantic Assessment of Summarizability
181
Fig. 3. Extension to QB4OLAP to encode summarizability statements
to express the following knowledge (recall 3): The Measure M has been summa-
rized by function F across dimensions D1, ..., Dn in context C. Figure 3 shows
a diagram describing our extension (we only show the classes from QB4OLAP
linked to the classes and properties deﬁned by us). We encode the n-ary rela-
tionship between a measure and the aggregation function, dimensions and con-
text on which it was summarized by means of the Summarizability State-
ment (SS) class and three properties: has summarizability statement that relates
a qb:MeasureProperty with a SS; across dimension that relates a SS with a
rdfs:Class and by aggregation that relates a SS with a qb4o:AggregationFunction.
We chose to relate SSs to rdfs:Class instead of qb:DimensionProperty because
knowledge bases that are not data cubes or data cubes not in QB format usually
regard dimensions as concepts or classes instead of properties. QB provides the
mean to link a qb:DimensionProperty to the concept and/or class it represents
through the qb:concept and rdfs:Range properties, therefore, a hint to a query
on RDF Data Cube could be provided by examining the summarizability state-
ments in SUM-KB refer to the same or similar classes or concepts. If available,
context information can be linked to summarizability statements and be used as
input for the hinting process. Figure 3 shows the sample statement linked to a
prov:agent through prov:wasAttributedTo property of PROV ontology [10].
6
EME for SPARQL Queries
An algorithm to solve the EME (Extraction of Multidimensional Elements) prob-
lem (cf. Deﬁnition 2) for SPARQL queries has been developed. For queries exe-
cuted on data cubes modelled with the QB vocabulary, e.g., Example 1.1, prop-
erties used are typed as qb:DimensionProperty or qb:MeasureProperty, making
EME straightforward. Algorithm 1 shows EME for the class of queries on QB.
This class is characterized as multidimensional queries centered around a vari-
able of type qb:Observation.

182
L.-D. Ib´a˜nez et al.
Input: SPARQL Query Q on RDF Data Cube
Output: Set of {Mi
Fj
==⇒Dk}
S ←∅;
Aggs ←Q.getAggregateF unctions();
foreach F ∈Aggs do
aggV ariable ←fun.getV ariable();
D ←∅;
M = Property of type qb:MeasureProperty of the BGP in Q having aggV ariable as object ;
foreach Property P in Q of type qb:DimensionProperty do
S ∪{M
F
=⇒{P.getRDF SRange(), P.getQBConcept()}};
end
end
Algorithm 1. EME for SPARQL queries using RDF Data Cube
Example 1.2. Example SPARQL aggregation query from USEWOD: Compute max-
imum, minimum and average runtime of movies starring Clint Eastwood
PREFIX:
dbo:<http :// dbpedia . org / ontology/>
PREFIX:
dbp:<http :// dbpedia . org / resource/>
SELECT ?movie max(? runtime ) min (? runtime ) avg (? runtime )
WHERE
{?movie dbo : runtime ? runtime
.
?movie dbo : s t a r r i n g
dbp : Clint Eastwood
.
}
GROUP BY ?movie
However, for queries made on Knowledge Bases that are not QB data cubes,
there is no explicit link to dimensions and measures. Therefore, we need to deﬁne
an algorithm to perform EME. Our Algorithm 2 is divided in three main steps.
We will illustrate it with the query depicted in Example 1.2.
1. Extract the aggregation functions and their aggregation variables. In the
example, there are three aggregation functions: max, min and avg, all of
them with ?runtime as aggregation variable.
2. For each aggregation function, locate the Basic Graph Pattern (BGP) that
contains the aggregation variable as object. We call this the measure BGP of
the query. In our example, the measure BGP is ?movies dbo:runtime ?run-
time. The predicate of this pattern is the Measure type of the query. In our
example dbo:runtime.
3. To extract the dimensions there are two cases:
(a) If the query has a Group By clause, the GroupBy variables indicate the
dimensions. To get their type, we check for each one if the query has
a BGP explicitly stating their rdf:type. This is not the case in Exam-
ple 1.2. Note that if the query had included the BGP ?movie rdf:type
schema:Movie, the algorithm would add schema:Movie as a dimension
to the summarizability statement. If there is not such a BGP, we add
as dimensions the rdfs:domain of the predicates of the BGPs having
the group variable as subject, and the rdfs:range of the predicates of
the BGPs having the group variable as subject (as in our running
example). The algorithm identiﬁes both BGPs in the query as con-
taining the ?movie variable and dereferences the predicates dbo:runtime
and dbo:starring to get their domains. Both properties have as domain
dbo:Work.

Towards Semantic Assessment of Summarizability
183
Input: SPARQL Query Q using aggregate functions
Output: Set of {Mi
Fj
==⇒Dk}
S ←∅;
Aggs ←Q.getAggregateF unctions();
foreach F ∈Aggs do
aggV ariable ←fun.getV ariable();
measureBGP ←Q.getBGP containing(aggV ariable) ;
M ←measureBGP.getP redicate() ;
if Q.hasGroupBy() then
foreach GroupBy variable v do
if Q.hasBGP(v,rdf:type,a type) then
S ∪{M
F
=⇒{a type}}
else
Dims ←∅foreach BGP in Q having v as subject do
Dims ∪{BGP.getRDF SDomain()}
end
foreach BGP in Q having v as object do
Dims ∪{BGP.getRDF SRange()}
end
S ∪{M
F
=⇒Dims}
end
else
if Q.hasBGP(measureBGP.getSubject(),rdf:type,a type) then
S ∪{(M
F
=⇒{a type})}
else
Dims ←∅foreach BGP in Q having measureBGP.getSubject() as subject do
Dims ∪{BGP.getRDF SDomain()}
end
foreach BGP in Q having measureBGP.getSubject() as object do
Dims ∪{BGP.getRDF SRange()}
end
S ∪{M
F
=⇒Dims}
end
end
Algorithm 2. EME for SPARQL queries
(b) Else, if there is no GroupBy clause, we take the variable that appears as
subject in the measure BGP and apply the same procedure that we do
to GroupBy variables: check if the rdf:type is explicit in the query, and if
not, get the domain and ranges of the predicates involving the variable.
Complexity of Algorithm 2 is O(numF × numBGP) where numF is the
number of aggregation functions and numB the number of Basic Graph Patterns.
From the EME and the context/provenance metadata of the query, it is
straightforward to produce RDF summarizability statements following the exten-
sion to QB4OLAP described in Sect. 5. The following excerpt shows it for
Example 1.2 for the Max aggregation function:
Example 1.3. Example of Summarizability Statement extracted from Example 1.2
PREFIX dbo:<http :// dbpedia . org / ontology/>
PREFIX qb4o:<http :// purl . org / qb4olap / cubes#>
PREFIX ex:<http :// example . org/#>
PREFIX prov:<http ://www. w3 . org /ns/prov#>
dbo : runtime qb4o : Summarizability Statement
:
SS instance
:
SS instance
qb4o : by aggregation
qb4o :Max
:
SS instance
qb4o : across dimension dbo : Work
\−−−−Example Context/Provenance
information −−−−/
:
SS instance
prov : wasAttributedBy ex : Jane Doe
A simple way to provide a solution to the summarizability hinting prob-
lem (cf. Deﬁnition 4) is to ask the SUM-KB if a query using the same multi-
dimensional elements exists. For example, given a query with F = qb4o:Min,

184
L.-D. Ib´a˜nez et al.
M = dbo:runtime and D = dbo:Work, the following ASK query on SUM-KB
returns TRUE if there is a summarizability statement using the same combina-
tion, and FALSE otherwise:
Example 1.4. SPARQL ASK query to solve Summarizability Hinting problem
PREFIX dbo:<http :// dbpedia . org / ontology/>
PREFIX qb4osum:<http :// purl . org /qb4sum/ cubes#>
ASK {
dbo : runtime qb4osum : Summarizability Statement ? s s i n s t a n c e
.
? s s i n s t a n c e
qb4osum : by aggregation
qb4o : Min .
? s s i n s t a n c e
qb4osum : across dimension dbo : Work .
}
7
Conclusion and Future Work
The capability of (automatically) incorporating external data for supporting
decision making processes gives rise to a novel BI scenario labelled as Self-Service
BI [1]. Within this scenario decision makers are likely to make meaningless
queries that lead to summarizability problems. To overcome these problems, we
propose a framework that contain (i) a Summarizability Hint Module (SHM)
that allows decision makers to be aware on the potential summarizability prob-
lems of multidimensional queries; and (ii) a Summarizability Knowledge Base
(SUM-KB) constructed by extracting multidimensional elements (EME), from
multidimensional query logs, that contain information about semantic correct-
ness of summarization queries. To proof-concept our framework, we generated a
ﬁrst version of SUM-KB from the DBpedia logs of the USEWOD 2016 research
dataset [12] by ﬁltering the queries that use aggregation functions (8946 queries
out of 35124962). We applied Algorithm 25 to the remaining queries. Only in
0.5% of the queries the extraction was successful. Results suggest that the log of
a public endpoint where many people come to learn SPARQL and the percent-
age of queries using aggregation is not signiﬁcant. Therefore, although using the
USEWOD 2016 dataset shows the feasibility of our approach works, as future
work, we plan to extend our experimentation to be conducted on query logs from
other endpoints (where more multidimensional queries are executed). Also, the
algorithm that extracts multidimensional elements (EME) will be extended for
considering a larger class of SPARQL queries.
Acknowledgments. Jose-Norberto Maz´on is funded by grant number JC2015-00284
under “Jos´e Castillejo” research program from Spanish Government (Ministerio de
Educaci´on, Cultura y Deporte en el marco del Programa Estatal de Promoci´on del
Talento y su Empleabilidad en I+D+i, Subprograma Estatal de Movilidad, del Plan
Estatal de Investigaci´on Cient´ıﬁca y T´ecnica y de Innovaci´on 2013–2016). This work
is also funded by research project TIN2016-78103-C2-2-R from Spanish Government
(Ministerio de Econom´ıa, Industria y Competitividad).
5 Available at https://github.com/ldibanyez/eme-extractor.

Towards Semantic Assessment of Summarizability
185
References
1. Abell´o, A., Darmont, J., Etcheverry, L., Golfarelli, M., Maz´on, J.N., Naumann,
F., Pedersen, T., Rizzi, S.B., Trujillo, J., Vassiliadis, P., Vossen, G.: Fusion cubes:
towards self-service Business Intelligence. Int. J. Data Warehous. Min. 9(2), 66–88
(2013)
2. Abello, A., Romero, O., Pedersen, T.B., Berlanga, R., Nebot, V., Aramburu, M.J.,
Simitsis, A.: Using semantic web technologies for exploratory OLAP: a survey.
IEEE Trans. Knowl. Data Eng. 27(2), 571–588 (2015)
3. Cyganiak, R., Reynolds, D., Tennison, J.: The RDF data cube vocabulary. Tech-
nical report, W3C (2014). https://www.w3.org/TR/vocab-data-cube/
4. Etcheverry, L., Vaisman, A., Zim´anyi, E.: Modeling and Querying Data Warehouses
on the Semantic Web Using QB4OLAP. In: Bellatreche, L., Mohania, M.K. (eds.)
DaWaK 2014. LNCS, vol. 8646, pp. 45–56. Springer, Cham (2014). doi:10.1007/
978-3-319-10160-6 5
5. Golfarelli, M., Rizzi, S.: Data Warehouse Design: Modern Principles and Method-
ologies. McGraw-Hill Inc., New York (2009)
6. H¨oﬀner, K., Lehmann, J., Usbeck, R.: CubeQA - question answering on RDF data
cubes. In: International Semantic Web Conference (ISWC) (2016)
7. Horner, J., Song, I.-Y.: A Taxonomy of Inaccurate Summaries and Their Manage-
ment in OLAP Systems. In: Delcambre, L., Kop, C., Mayr, H.C., Mylopoulos, J.,
Pastor, O. (eds.) ER 2005. LNCS, vol. 3716, pp. 433–448. Springer, Heidelberg
(2005). doi:10.1007/11568322 28
8. K¨ampgen, B., O’Riain, S., Harth, A.: Interacting with statistical linked data via
OLAP operations. In: Simperl, E., Norton, B., Mladenic, D., Della Valle, E., Fun-
dulaki, I., Passant, A., Troncy, R. (eds.) ESWC 2012. LNCS, vol. 7540, pp. 87–101.
Springer, Heidelberg (2015). doi:10.1007/978-3-662-46641-4 7
9. Kimball, R., Ross, M.: The Kimball Group Reader: Relentlessly Practical Tools
for Data Warehousing and Business Intelligence. Wiley, Indianapolis (2010)
10. Lebo, T., Sahoo, S., McGuinness, D.: PROV-O: the PROV ontology. Technical
report, W3C (2013). https://www.w3.org/TR/prov-o
11. Lenz, H.J., Shoshani, A.: Summarizability in OLAP and statistical data bases.
In: Proceedings of the Ninth International Conference on Scientiﬁc and Statistical
Database Management, pp. 132–143. IEEE Computer Society,., January 1997
12. Luczak-Roesch, M., Aljaloud, S., Berendt, B., Hollink, L.: Usewod 2016 research
dataset. doi:10.5258/SOTON/385344
13. Maz´on, J.N., Lechtenb¨orger, J., Trujillo, J.: A survey on summarizability issues in
multidimensional modeling. Data Knowl. Eng. 68(12), 1452–1469 (2009)
14. Nebot, V., Berlanga, R.: Statistically-driven generation of multidimensional ana-
lytical schemas from linked data. Knowl. Based Syst. 110, 15–29 (2016)
15. Niemi, T., Niinim¨aki, M., Thanisch, P., Nummenmaa, J.: Detecting summarizabil-
ity in OLAP. Data Knowl. Eng. 89, 1–20 (2014)
16. Pedersen, T.B.: Managing Complex Multidimensional Data. In: Aufaure, M.-A.,
Zim´anyi, E. (eds.) eBISS 2012. LNBIP, vol. 138, pp. 1–28. Springer, Heidelberg
(2013). doi:10.1007/978-3-642-36318-4 1
17. Rafanelli, M., Bezenchek, A., Tininini, L.: The aggregate data problem: a system
for their deﬁnition and management. ACM Sigmod Rec. 25(4), 8–13 (1996)
18. Varga, J., Etcheverry, L., Vaisman, A.A., Romero, O., Pedersen, T.B., Thomsen,
C.: Qb2olap: Enabling olap on statistical linked open data. In: 32nd International
Conference on Data Engineering (ICDE), pp. 1346–1349. IEEE (2016)

Theta Architecture: Preserving the Quality
of Analytics in Data-Driven Systems
Vasileios Theodorou1(B), Ilias Gerostathopoulos2, Sasan Amini2,
Riccardo Scandariato3, Christian Prehofer4, and Miroslaw Staron3
1 Intracom SA Telecom Solutions, Athens, Greece
theovas@intracom-telecom.com
2 Technische Universit¨at M¨unchen, Munich, Germany
Gerostat@in.tum.de, sasan.amini@tum.de
3 University of Gothenburg, Gothenburg, Sweden
{riccardo.scandariato,Miroslaw.Staron}@cse.gu.se
4 Fortiss GmbH, Munich, Germany
prehofer@fortiss.org
Abstract. With the recent advances in Big Data storage and process-
ing, there is a real potential of data-driven software systems, i.e., systems
that employ analysis of large amounts of data to inform their runtime
decisions. However, for these decisions to be trustworthy and dependable,
one needs to deal with the well-known challenges on the data analy-
sis domain: data scarcity, low-quality of data available for analysis, low
veracity of data and subsequent analysis results, data privacy constraints
that hinder the analysis. A promising solution is to introduce ﬂexibility
in the data analytics part of the system enabling optimization at runtime
of the algorithms and data streams based on the combination of veracity,
privacy and scarcity in order to preserve the target level of quality of the
data-driven decisions. In this paper, we investigate this solution by pro-
viding an adaptive reference architecture and illustrate its applicability
with an example from the traﬃc management domain.
Keywords: Big Data · Reference architecture · Data veracity
1
Introduction
Modern systems collect raw data from the users and their personal devices (like
health trackers), raw data from the environment and its smart objects (smart
thermostats, home automation devices), as well as higher-level data coming
from information providers like social platforms, open-data sites (e.g., Open-
StreetMap), and other silos of information. Beyond functionality, the success of
such systems is tied to the availability of the information that is processed as
well as its quality. Functionality is often centered around the analysis of data to
extract useful information (e.g. make user-speciﬁc recommendations, adapt to
user habits to make an application more ergonomic, etc.).
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 186–198, 2017.
DOI: 10.1007/978-3-319-67162-8 19

Theta Architecture: Preserving the Quality of Analytics
187
We believe that we are moving from traditional software systems, which are
functionality- and software-centric, towards systems that are data-centric and
where the functionality is driven by the availability of data and the decisions
drawn from them. This is true not only for systems that perform oﬄine analysis
(e.g. business intelligence systems), but also, and even more so, for systems that
employ real-time analysis of data to inform their runtime decisions (personalized
advertising, traﬃc control).
In this paper, we focus on systems that make decisions at runtime based
on Big Data analytics. For these decisions to be trustworthy and dependable,
one needs to deal though with the well-known challenges on the data analysis
domain: data scarcity, low-quality of data available for analysis, low veracity of
data and subsequent analysis results, data privacy constraints that hinder the
analysis. Indeed, we argue that unless a data-centric system deals with the above
issues eﬀectively at runtime, it will not matter whether it can process terabytes
or petabytes at very high rates (which, in itself, is a noteworthy achievement
of current Big Data systems)—as the resulting decision recommendation cannot
be trusted.
Promising solutions already exist in the data analysis domain, where several
data cleaning and data preparation methods have been proposed [7]. While these
are certainly relevant and important, this paper takes a diﬀerent (yet comple-
mentary) approach and proposes to introduce ﬂexibility to change data streams
in runtime. This will allow the system architects to depart from the current,
more rigid model where the data analytics schema is planned ahead of deploy-
ment (which data sources to use, which information to mine, which algorithms
to use) and kept ﬁxed afterwards, at run time. The main drawbacks of such
systems are that (i) the quality of the decisions will decay over time in case the
quality of the analyzed data drops, and (ii) the system cannot opportunistically
adapt or even improve in light of new operating conditions (e.g., when a new,
richer, better data source becomes available).
In this work we introduce a novel reference architecture for adaptive data-
driven systems (called Theta architecture) that can change used data sources
and data analysis algorithms at runtime to preserve the target quality of its
outcome. To achieve our ambitious goal we have set up an agenda consisting of
three items: (i) identify the need for adaptive Big Data analytics in the context
of data-driven systems via concrete examples, (ii) propose a high level architec-
ture for adaptive Big Data analytics in data-driven systems, (iii) evaluate our
architecture via applying it in concrete systems identiﬁed in (i). In this paper,
we present our ﬁrst steps towards fulﬁlling (i) and (ii) from above. In particular,
we identify and describe an example of a data-driven system where adaptive Big
Data analytics are of high value. The example comes from the vehicular traﬃc
management domain and is presented in Sect. 2. We present the Theta archi-
tecture in Sect. 3 and illustrate its use on the running example. Finally, after
presenting an overview of the related work in Sect. 4, we provide the concluding
remarks in Sect. 5.

188
V. Theodorou et al.
2
Real-Time Freeway Traﬃc Control System
In this section, we present an example of a data-driven system to showcase that
adapting the data analytics processes at runtime is beneﬁcial. This is used for
both motivating and exemplifying our approach.
The main objective of traﬃc management on freeways is to increase the
freeway capacity, i.e. to maximize the vehicular ﬂow per time unit:
flow (veh/h) = speed (km/h) ∗density (veh/km)
In order to increase the capacity of a freeway, existing control measures involve a
combination of dynamically changing the speed limit, opening or closing freeway
lanes and recommending alternative routes through in-car navigation devices and
Variable Message Signs (VMS).
Fig. 1. Data sources in real-time freeway control system.
To know when to make the decision of applying the control measures, a
freeway traﬃc control system (FTCS) can rely on data coming from diﬀer-
ent sources (Fig. 1). Data sources include inductive loop detectors installed on
the pavement, surveillance cameras, cars transmitting their position, speed, etc.
(referred to as ﬂoating car data in the Intelligent Transport System domain),
Doppler radars, and ultrasonic and passive infrared sensing devices. In addi-
tion, environmental detectors are commonly used for freeway traﬃc control
such as scatter measurements—measuring visibility range—and precipitation
detectors—measuring thickness of water ﬁlm on the road. Data from these dif-
ferent sources are transmitted to a central traﬃc control center where they are
analyzed (this typically involves also visualizing the data).
We envision a fully automated real-time FTCS. In this case, a human oper-
ator simply conﬁgures the FTCS upon startup, e.g. by setting the thresholds on
calculated ﬂow for opening and closing the hard shoulder. The FTCS performs
the calculation of traﬃc ﬂow in predeﬁned time windows (e.g. every 30 s) and,
based on these calculations, makes autonomous decisions to apply (a combina-
tion of) the available control measures. Applying the opening of a hard shoulder,

Theta Architecture: Preserving the Quality of Analytics
189
for instance, results in changing the signs on the VMS gantries and disseminating
the information about the opening/closing of a lane or a diﬀerent speed limit to
the in-vehicle navigation devices.
A challenge in order to achieve a fully automated FTCS is to be able to
deal with non-veracious (untrue) data that may creep in the analysis and ulti-
mately inﬂuence the control logic, which resides in the control center. Data can
have low veracity (i.e., little correspondence to reality) due to several reasons,
e.g., sensor inaccuracy, sensor faults, sensor tampering, communication errors,
to name a few. For example, dirt could block the view of a camera or water
spray from the passing vehicles may inﬂuence the visibility range at the location
of the scatter measurements. A loop detector may start malfunctioning due to
pavement cracking or moving, inadequate sealant application, electronics unit
failure, breakdown of wire insulation, electrical surges, etc.
Ideally, FTCS should be able to detect that it is using non-veracious data
and adapt its analytics logic by choosing diﬀerent data sources or a diﬀerent
analytics algorithm (e.g., one that makes weaker assumptions on the veracity of
incoming data).
3
Theta Architecture
We have already pointed out the need for introducing ﬂexibility in the data
analytics as part of our running example in Sect. 2. We believe this need extends
to other systems that need to make correct and reliable real-time decisions based
on large numbers of collected data. In this section, we propose a high level
architecture to support this.
Our proposed architecture, termed Theta architecture, is depicted in Fig. 2.
It consists of three main sub-systems: Business subsystem, Analytics subsystem
and Adaptation subsystem. We describe each one in turn and exemplify them
by applying them to our running example (Fig. 3).
Business Subsystem. This part represents any software-intensive cyber-physical
system that needs to be controlled at runtime based on analysis of data collected
from its execution, optionally enriched with external data.
Interfaces. Business communicates with the other two subsystems through two
interfaces, the Data & metadata and the Adaptation interface. The ﬁrst is used by
the Analytics to collect data in a pull (e.g., polling a RESTfull API [26]) or a push
(e.g., publishing Kafka [15] messages) fashion. Together with the actual data
values (e.g., speed, precipitation, etc.), data points may also contain metadata
such as reported accuracy of sensors, ownership, etc.
The second interface is used by the Adaptation for requesting the Business
to adapt. It is the task of the system designer to deﬁne how adaptation requests
from Adaptation are translated into actual changes in Business. We note that
a change can aﬀect the cyber part of the system (e.g., setting a parameter of
a route planner software to a new value) or the physical one (e.g., switching a
traﬃc light).

190
V. Theodorou et al.
Fig. 2. Theta architecture.
Fig. 3. Application of Theta architecture to the running example.

Theta Architecture: Preserving the Quality of Analytics
191
Example. In our running example (Fig. 3), Business is the traﬃc control system
deployed in the freeway, consisting of a number of diﬀerent types of sensors,
e.g., inductive loop detectors, cars, cameras, radars, etc. Each sensor provides
diﬀerent data according to its type. For instance, a loop detector provides data
on the number of vehicles that passed in a time interval, together with its average
occupancy in the interval. A car provides its position and speed, while a camera
provides either live feed or how many cars have been detected at a time interval.
Business includes also two actuators, in-vehicle navigation systems and variable
message signs, which are used to implement the “open/close hard shoulder”
directive from Adaptation.
Analytics Subsystem. It is responsible for generating actionable information by
large-scale data analysis. It should be able to provide diﬀerent types of data
analysis, from simple but data-intense computations to statistical hypothesis
testing and machine learning and data mining.
In terms of the analytics processes that are run in Analytics, we distinguish
between processes that support the data-driven functionality of the business
system (business analytics) and processes that support the reasoning of the
quality of input sources (metadata analytics). Metadata analytics not only use
and summarize the metadata on provided accuracies of sensed data (e.g., from
conﬁdence interval values accompanying GPS measurements or weather data-
based quality estimations on camera feeds), but also generate inferred metadata
about the data sources based on the actual data values. Flexible formalisms can
be employed for maintaining metadata about quality measurements, such as the
Data Quality Vocabulary (DQV1).
Technically, Analytics can be implemented as a Big Data processing system
following one of the existing lambda or kappa architectures or forks of them
and the corresponding Big Data stacks (e.g., HDFS and Hive/Pig, Kafka and
Spark/Flink).
Interfaces. Analytics exposes two interfaces for its interaction with the other
two subsystems, the Results & data source quality and the Adaptation interface.
The ﬁrst interface is used by Adaptation in order to access the results of the
business analytics processes, as well as to retrieve quality information about the
available and utilized data sources. An interesting approach would be for the
Adaptation to receive provided and inferred metadata about the data sources in
an event-driven fashion, e.g., with notiﬁcations being triggered when veracity of
some source is estimated to fall behind a predeﬁned threshold.
The Adaptation interface is used by Adaptation for imposing adaptations on
Analytics. The adaptations we currently consider take the form of switching the
running business analytics processes and/or their input data sources. We also
envision more ﬁne-grained adaptations such as parameter conﬁguration of the
analytics algorithms embodied in the running processes.
Example. In our running example, Analytics comprises (a minimum of) three
analytics processes. Two of them are business analytics; they compute the traﬃc
1 https://www.w3.org/TR/vocab-dqv/.

192
V. Theodorou et al.
ﬂow based on diﬀerent data sources and computations algorithms. The compu-
tation may involve also near-future prediction of the traﬃc ﬂow (e.g., based on
regression using historical data). A third one is metadata analytics; it provides
an estimation of the veracity of data sources such as loop detectors or cars. In
case of loop detectors, the veracity calculation process can involve performing
some well-known plausibility checks about loop detector data from the traﬃc
domain [13].
Adaptation Subsystem. It is the subsystem responsible for decision making. This
is performed in two independent adaptation loops, the business and the analytics
loop. The ﬁrst is responsible for planning and executing adaptations on Business
based on the business analytics results of Analytics. The second is responsible for
planning and executing adaptations on Analytics based on the metadata analyt-
ics results of Analytics. To this end, data proﬁling [2] and metadata extraction
[27] play a crucial role. A notable adaptation in the latter loop is the substitu-
tion of running business analytics processes with others that have either the same
input data or diﬀerent ones (a sensible option when the veracity of previously
used input data gets high).
By making these two loops explicit in Theta architecture, we achieve separa-
tion between the concern of performing reliable and robust business analytics—
by adapting Analytics—and using them to inform business decisions—by adapt-
ing Business).
Interfaces. Adaptation communicates with the other two subsystems with three
interfaces (not depicted in Figs. 2 and 3). Via the ﬁrst one, it receives the results
of both the business and metadata analytics. The second and third ones are used
to propagate the adaptation directives to both Analytics and Business.
Example. In our running example, the two loops in Adaptation take the fol-
lowing form. The business loop continuously checks the computed values of traf-
ﬁc ﬂow (current values or near-future predictions) and issues the adaptation
requests of opening or closing the hard shoulder in order to maximize the ﬂow.
In this case, decision making is based on well-known, empirically-validated rules
on when to open/close a hard shoulder in a freeway based on the ﬂow [11].
The analytics loop continuously checks the data source veracity measures cal-
culated in Analytics. If the measures for the active business analytics processes
indicate low veracity of input data, the analytics loop looks for alternative
processes that can calculate the same result with diﬀerent inputs to substitute
the running processes. For example, the analytics loop switches the process that
calculates the traﬃc ﬂow based on loop detector data to an equivalent one that
uses Floating Car Data, when a number of loop detectors start malfunctioning.
4
Overview of Related Work
Our approach builds upon existing works in Big Data analytics (relevant for
the Analytics subsystem), self-adaptive systems (relevant for the Adaptation

Theta Architecture: Preserving the Quality of Analytics
193
subsystem), and data veracity deﬁnition and management (relevant for the Meta-
data Analytics inside the Analytics subsystem and the Analytics loop inside the
Adaptation subsystem), which we overview below.
4.1
Big Data Analytics
Analytics on Big Data refer to data analysis approaches able to scale to large
amounts of data (e.g. petabytes or exabytes) which are typically unstructured
or semi-structured [30], i.e. they do not follow a particular schema. Big Data are
considered to encompass the “5-V” properties: volume, velocity, variety, veracity,
and value [21]. Here, the value is of particular importance, since Big Data analyt-
ics is ideally concerned with deriving insightful information, or even actionable
results, from raw data by applying techniques from statistics, machine learn-
ing, and data mining. Big Data analytics has been used in a diverse set of
applications, from analyzing user clicks on websites to analyzing the results of
high-energy physics experiments. It has also been used in the analysis of soft-
ware artifacts such as source code, execution traces, and runtime logs (software
analytics [23]).
On the tooling side, Hadoop [1] is an open-source ecosystem that has become
the de facto standard. A distinction is typically made between analyzing histor-
ical data in batch mode (full-data analytics) and analyzing data as they come
in stream mode (“tuple-at-at-time” analytics) [22]. Important tools in the ﬁrst
category are Hadoops MapReduce, Hive, and Pig; Flink and Kafka Streams are
representative of the second category. Spark is a hybrid system that combines
batch and stream processing. Stream processing in Spark relies on analysis of
micro-batches, rather than on analyzing one tuple at a time. Spark comes also
with extensive support in machine learning algorithms (its MLlib library).
When architecting a Big Data analytics system, two main approaches exist,
namely lambda and kappa architecture. Lambda architecture [22] combines both
batch and stream analytics in two layers called batch and speed layer, respec-
tively. The results of the two layers are combined at a third layer, the serving
layer (a NoSQL database). Essentially, stream processing complements batch
processing by analyzing the data that comes in the system since the start of
batch processing (which typically has high latency). An alternative to lambda
is kappa architecture, where stream processing is used for both the batch and
stream processing. This builds on the idea that stream processing, when applied
for large windows that can ﬁt in all the historical data, essentially corresponds
to batch processing. We note here that both lambda and kappa architectures
focus on solving performance issues (e.g. by balancing throughput and latency),
and not on quality issues of data and data analysis results—which is our focus.
4.2
Self-adaptive Systems
Self-adaptation refers to the ability of a system to change its structure and/or
behavior at runtime in response to external stimuli and changes in internal state.
Self-adaptation is typically achieved in three fundamental ways: (i) by relying on

194
V. Theodorou et al.
a detailed application model, e.g., Markov Decision Processes [12], and employing
simulations or other means of state-space traversal to infer the best response of
the system, (ii) by identifying control parameters and employing feedback-based
control techniques from control theory [8], and (iii) by reconﬁguring architecture
models, typically with the help of Event-Condition-Action rules [10].
Self-adaptation techniques typically follow the MAPE-K loop [14], which
divides self-adaptation into four phases: Monitoring activities, Analyzing run-
time metrics, Planning strategies, and Execute planall based on a shared Knowl-
edge base. Self-adaptation strategies are expressed as actions involving particular
architecture reconﬁgurations; they are applicable under certain conditions in the
presence of certain events or situations [4]. Actions can be associated with the
satisfaction of one or more system goals, typically quantiﬁed via ﬁtness or utility
functions [28].
Although adapting a system based on analysis of data collected from its
execution is a well-researched idea, there is a vacuum of approaches that use
Big Data analytics in self-adaptation. In our recent work, we have proposed
an approach to do so [29], since we believe that as the amount of data collected
from a running system and its environment increases, Big Data analytics become
relevant for self-adaptation.
In our approach, we employ two self-adaptation loops: one that controls the
data-driven system itself (by employing Big Data techniques in its Analysis
phase), and one that controls the analytics subsystem (by switching data sources
and analysis algorithms).
4.3
Data Veracity
Historically, the notion of veracity is derived from the area of sociology and its
major popularity lies in the area of criminology—the ability to detect whether a
witness is veracious or not [17,20]. In that particular context, the term veracity
is used both in relation to actors (e.g. witnesses) and their statements [3]. The
latter refers to judging the truthfulness of a statement and is in scope for our
purposes as well.
In our context, we consider the deﬁnition of veracity as quoted by Krotoﬁl
[16] who deﬁnes the veracity as the property that an assertion truthfully reects
the aspect it makes a statement about. We can see a direct relation to the ﬁeld of
criminology and also see the challenges related to automated assessment of the
veracity in the context of software systems.
For instance, the veracity of the data can be violated by: (i) non-adequate
measurement of a physical property by a sensor because of the inappropriate
design of the sensor (ii) non-adequate measurement caused by a faulty sensor
during the operation (iii) non-adequate measurement caused by an obstructed
sensor (iv) faulty data caused by a malicious agents tempering with the sensor
data.
Lukainova and Rubin recognize data veracity as the sum of a number of
quality attributes such as correctness, accuracy, free from biases and free from
errors [19]. Based on this work and our previous work [31], we can see that

Theta Architecture: Preserving the Quality of Analytics
195
veracity can be modelled as a composition of elements. To be able to perform
such modelling, we need to ﬁrst decide upon which elements are relevant (e.g.
correctness, free or errors) and how they relate to each other.
In our approach, we ﬁrst need to model and assess the veracity of the data
that is being used in Big Data analytics In our running example, a rudimentary
way to do is to apply existing plausibility checks on the sensor readings (such
checks are e.g. well-known for inductive loop detector readings). We then pro-
pose to switch between diﬀerent data sources and corresponding data analysis
algorithms (i.e. Big Data jobs) at runtime when the veracity of the used data
drops below a threshold. In other words, we intend to use simple threshold-based
adaptation rules to adapt the analytics subsystem in order to increase the quality
of its results.
4.4
Data Source Evaluation
Data source selection has received interest since the advent of the Internet.
Florescu et al. [9] use probabilistic knowledge on a mediation schema that quan-
titatively determines the probability of information being covered by speciﬁc
data source and provide corresponding data source ranking in cases of overlap-
ping data sources (i.e., sources containing same documents). Naumann et al. [25]
additionally put into play data source quality and propose a methodology that
weights diﬀerent data sources with regards to their information quality, consider-
ing quantiﬁed quality criteria (e.g., relevance to speciﬁc query) and cost and thus
formulating linear programs. Mihaila et al. [24] introduce the use of metadata
to maintain in XML format content and data quality information about data
sources and by relaxing accuracy requirements, they propose a methodology for
eﬃcient source selection and ranking.
More recently, Dong et al. [6] have dealt with the problem of selecting the
subset of data sources that maximize quality gain and minimize cost. In their
work, they showcase the peculiar behavior of information gain as a result of
utilizing multiple data sources of varying information coverage and accuracy.
This work poses a pragmatic view on data source selection and can provide a
stepping stone for conducting analysis on integrating data sources in presence
of errors and false values.
Examining the quality of web sources, Dong et al. [5] use an information
extraction system to employ aside from exogenous signals, inference about prob-
ability of correctness of facts which they deﬁne as accuracy of a web source.
They introduce a novel methodology for assessing source and extracted data
correctness, which can pave the way for veracity inference in case of multiple
available data sources. Finally, data source quality assessment approaches have
been proposed [18,32] that can deal with high variety and variability of available
sources.
In our approach, we combine explicit metadata derived from the Business
subsystem with statistical methods, to determine the veracity of data sources and
to detect anomalies. This analysis provides feedback for decisions on data source
selection and switching, which aim at maximizing veracity while minimizing cost.

196
V. Theodorou et al.
5
Discussion
In this section, we conclude by providing a discussion that reﬂects on our pro-
posed architecture:
1. Essentially, if we disregard Analytics from Theta architecture, the resulting
architecture degenerates into the classic MAPE-K loop, comprised of a con-
trolled subsystem (Business) and a controller (Adaptation).
2. Both the business and the analytics loops in Adaptation can be arbitrarily
complex. In our ﬁrst attempts based on Apache Kafka and Python for traﬃc
management of a simulated freeway, we have successfully considered only
simple adaptation logics (e.g. based on a short number of rules); however,
Theta architecture imposes no restrictions to the complexity of the adaptation
logics.
3. Cluster-based approaches at the Analytics subsystem are only necessary if
data size is large enough to render single-node approaches impractical.
4. We note here that although data veracity is the primary concern in our run-
ning example, our architecture can be used for adapting between data sources
based on other concerns such as data privacy and conﬁdentiality.
References
1. Apache Hadoop (2017). http://hadoop.apache.org/
2. Abedjan, Z., Golab, L., Naumann, F.: Data proﬁling: a tutorial. In: Proceedings of
the 2017 ACM International Conference on Management of Data, SIGMOD 2017,
pp. 1747–1751 (2017)
3. Carey, P.W., Mehler, J., Bever, T.G.: Judging the veracity of ambiguous sentences.
J. Verbal Learn. Verb. Behav. 9(2), 243–254 (1970)
4. Cheng, S.W., Garlan, D., Schmerl, B.: Stitch: a language for architecture-based
self-adaptation. J. Syst. Softw. 85(12), 1–38 (2012)
5. Dong, X.L., Gabrilovich, E., Murphy, K., Dang, V., Horn, W., Lugaresi, C., Sun, S.,
Zhang, W.: Knowledge-based trust: estimating the trustworthiness of web sources.
Proc. VLDB Endow. 8(9), 938–949 (2015)
6. Dong, X.L., Saha, B., Srivastava, D.: Less is more: selecting sources wisely for
integration. In: Proceedings of the 39th International Conference on Very Large
Data Bases, PVLDB 2013, pp. 37–48. VLDB Endowment (2013)
7. Dustdar, S., Pichler, R., Savenkov, V., Truong, H.L.: Quality-aware service-oriented
data integration: requirements, state of the art and open challenges. SIGMOD Rec.
41(1), 11–19 (2012)
8. Filieri, A., et al.: Software engineering meets control theory. In: Proceedings of
SEAMS 2015, pp. 71–82. IEEE, May 2015
9. Florescu, D., Koller, D., Levy, A.Y.: Using probabilistic information in data inte-
gration. In: Proceedings of the 23rd International Conference on Very Large Data
Bases, VLDB 1997, Athens, Greece, pp. 216–225, 25–29 August 1997
10. Garlan, D., Cheng, S.W., Huang, A.C., Schmerl, B., Steenkiste, P.: Rainbow:
architecture-based self-adaptation with reusable infrastructure. Computer 37(10),
46–54 (2004)

Theta Architecture: Preserving the Quality of Analytics
197
11. Geistefeldt, J.: Operational experience with temporary hard shoulder running in
Germany. Transp. Res. Rec. J. Transp. Res. Board 2278(6), 67–73 (2012)
12. Ghezzi, C., Pinto, L.S., Spoletini, P., Tamburrelli, G.: Managing non-functional
uncertainty via model-driven adaptivity. In: Proceedings of ICSE 2013, pp. 33–42.
IEEE (2013)
13. Gladbach, B.: Bundesanstalt fr Straenwesen: Merkblatt fr die Ausstattung von
Verkehrsrechnerzentralen und Unterzentralen (MARZ). Technical report, Ausgabe
1999 (1999)
14. Kephart, J., Chess, D.: The vision of autonomic computing. Computer 36(1), 41–
50 (2003)
15. Kreps, J., Narkhede, N., Rao, J., et al: Kafka: a distributed messaging system for
log processing. In: Proceedings of the 6th International Workshop on Networking
Meets Databases (NetDB 2011), pp. 1–7 (2011)
16. Krotoﬁl, M., Larsen, J., Gollmann, D.: The process matters. In: Proceedings of the
10th ACM Symposium on Information Computer and Communications Security.
Association for Computing Machinery (ACM) (2015)
17. Levine, T.R., Park, H.S., McCornack, S.A.: Accuracy in detecting truths and lies:
documenting the “veracity eﬀect”. Commun. Monogr. 66(2), 125–144 (1999)
18. Li, Q., Li, Y., Gao, J., Zhao, B., Fan, W., Han, J.: Resolving conﬂicts in hetero-
geneous data by truth discovery and source reliability estimation. In: Proceedings
of the 2014 ACM SIGMOD International Conference on Management of Data, pp.
1187–1198. ACM (2014)
19. Lukoianova, T., Rubin, V.L.: Veracity roadmap: is Big Data objective, truthful
and credible? (2014)
20. Mann, S., Vrij, A.: Police oﬃcers’ judgements of veracity tenseness, cognitive load
and attempted behavioural control in real-life police interviews. Psychol. Crime
Law 12(3), 307–319 (2006)
21. Marr, B.: Big Data: the 5 vs. everyone must know. https://www.linkedin.com/
pulse/20140306073407-64875646-big-data-the-5-vs-everyone-must-know
22. Marz, N., Warren, J.: Big Data: Principles and Best Practices of Scalable Realtime
Data Systems, 1st edn. Manning Publications Co., Greenwich (2015)
23. Menzies, T., Zimmermann, T.: Software analytics: so what? IEEE Softw. 30(4),
31–37 (2013)
24. Mihaila, G.A., Raschid, L., Vidal, M.: Using quality of data metadata for source
selection and ranking. In: Proceedings of the Third International Workshop on the
Web and Databases, pp. 93–98 (2000)
25. Naumann, F., Freytag, J.C., Spiliopoulou, M.: Quality driven source selection using
data envelope analysis. In: Third Conference on Information Quality (IQ 1998),
pp. 137–152 (1998)
26. Pautasso, C., Zimmermann, O., Leymann, F.: Restful web services vs. “Big”
web services: making the right architectural decision. In: Proceedings of the 17th
International Conference on World Wide Web, WWW 2008, pp. 805–814. ACM,
New York (2008)
27. Quix, C., Hai, R., Vatov, I.: Metadata extraction and management in data lakes
with GEMMS. CSIMQ 9, 67–83 (2016)
28. Salehie, M., Tahvildari, L.: Self-adaptive software: landscape and research chal-
lenges. ACM Trans. Auton. Adapti. Syst. 4(2), 1–40 (2009)

198
V. Theodorou et al.
29. Schmid, S., Gerostathopoulos, I., Prehofer, C., Bures, T.: Self-adaptation based
on big data analytics: a model problem and tool. In: Proceedings of the 12th
International Symposium on Software Engineering for Adaptive and Self-Managing
Systems (SEAMS 2017), pp. 102–108. IEEE Press, Piscataway (2017). https://doi.
org/10.1109/SEAMS.2017.20
30. Srinivasa, S., Bhatnagar, V. (eds.): BDA 2012. LNCS, vol. 7678. Springer,
Heidelberg (2012)
31. Staron, M., Scandariato, R.: Data veracity in intelligent transportation systems:
the slippery road warning scenario. In: 2016 IEEE Intelligent Vehicles Symposium
(IV), pp. 821–826. IEEE (2016)
32. Zhang, Y., Wang, H., Gao, H., Li, J.: Eﬃcient accuracy evaluation for multi-modal
sensed data. J. Comb. Optim. 32(4), 1068–1088 (2016)

Spatio-Temporal Evolution
of Scientiﬁc Knowledge
Goce Trajcevski(B), Xu Teng, and Shailav Taneja
Department of Electrical Engineering and Computer Science,
Northwestern University, Evanston, USA
{goce,shailavt1,xuteng2}@eecs.northwestern.edu
Abstract. In this work we take a ﬁrst step towards the problem of inte-
grating the content and the spatio-temporal aspects of the evolution of
the (published) scientiﬁc knowledge. A lot of research has been invested
in developing tools and search engines that will enable more eﬃcient
querying of relevant medical (and broader scientiﬁc) data from various
perspectives, spanning from retrieval of similar documents/images to
HCI-based ﬂexible query-answering systems. Variety of methodologies
have been developed, founded on knowledge-bases, statistics, semantic
similarity, etc. and quite a few systems are available (e.g., Medline).
Parallel to this, another body of research works has emerged over the
past couple of decades, targeting the eﬃcient management of mobility
and spatio-temporal data. What motivates this work is the observation
that fusing the data (and corresponding techniques) developed in these
two broad research ﬁelds could enable novel categories of queries that
can be used to investigate various evolving spatio-temporal relationships
between particular scientiﬁc topics.
We present a novel model and a formalization of this conﬂuence, in
what we call Knowledge-Evolution Trajectories (KET). We also provide
a preliminary proof-of-concept implementation that enables answering
novel categories of queries pertaining to KET data with a few ini-
tial observations regarding the impact of diﬀerent data-representation
approaches.
1
Introduction and Motivation
Shortly after the co-emerging of the ﬁelds of spatial [1,13,23] and temporal [8] data
management, the miniaturization of computing devices and advances in Global
Positioning Systems (GPS) technology have spurred a plethora of applications
that demanded some type of a Location-Awareness (LBS) [22]. This, originated the
ﬁelds of Spatio-Temporal Databases (STDb) [9] and Moving Objects Databases
(MOD) [10], addressing various aspects of managing such data – from modelling,
through indexing and query processing [7,9,18,20,28]. The peculiar feature of the
popular query-categories – e.g., range, (k) Nearest-Neighbor ((k-)NN) [26,29,30]
Research supported by NSF – CNS 1646107 and III 1213038, and ONR – N00014-
14-1-0215.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 199–210, 2017.
DOI: 10.1007/978-3-319-67162-8 20

200
G. Trajcevski et al.
in MOD settings is that they are typically: (1) continuous (i.e., their answers may
have to be re-evaluated based on the changes in the motion of the entities); and/or
(2) persistent (i.e., their answers may need to be re-evaluated based both on the
changes of the motion as well as the history of such changes) [7,18]. More recently,
researchers have turned their attention to modelling, representing, and querying
spatio-temporal trajectories which also have some kind of annotated information
associated with the location and/or time – bringing about the concept of semantic
(resp. symbolic, spatio-textual) trajectories [19].
Complementary to these developments, the need to reduce (or even eliminate)
labor-intensive process associated with retrieval of textual documents matching
particular criteria, along with the contemporary advances in information sys-
tems, have spurred the ﬁeld of Information Retrieval (IR) [16,21,24] starting in
the middle of the XX century. To enable eﬀective retrieval relevant documents
by various IR strategies, the documents are typically transformed into a suit-
able representation, often accompanied by preparation of suitable indexes [24].
In the subsequent decades, a plethora of research works1 followed, enabling liter-
ature searches [6,17], detection of various semantic correlations among (topics)
in existing publications [14], etc. In addition, several prototype systems and
publicly available search engines have been generated over the years (e.g., MED-
LARS [6], Medline (https://www.nlm.nih.gov/bsd/pmresources.html)).
At the heart of the motivation for this work is the observation that despite
the rather long co-existence of the two ﬁelds (IR and STDb/MOD) and their
respective rich histories, not much has been done in terms of exploiting the
possible conﬂuence of the two – which, as we will discuss shortly, could enable
novel categories of queries of relevance for various entities, from researchers, to
government agencies. For example, most of the works related to spatio-temporal
aspects of medical phenomena pertain to: – modelling the temporal, spatial,
and evolutionary nature of subject’s conditions [4]; exploring the spreading of
diﬀerent chemicals, or respondses/reactions to particular stimuli (e.g., [2,12].
Our goal in this work is to provide a foundation for addressing the problem
of modelling the spatio-temporal aspects of the evolution of scientiﬁc knowledge
and take a step towards enabling novel queries. Speciﬁcally, we aim at answering
queries such as:
Q1: Retrieve the authors and institutions located in Eastern Europe, who have
published results related to the topic of heterocyclic compounds between 2005 and
2010.
Q2: Retrieve all the topics that were published by an institution in Pennsylvania
between 2008 and 2012.
1 We do not aim at presenting a comprehensive overview of the vast body of works
from the well-established ﬁeld of IR in this paper (cf. [6,21]. The purpose of this
section is to provide a motivation for the research presented here.

Spatio-Temporal Evolution of Scientiﬁc Knowledge
201
Our main contributions are two-fold:
1. We introduce the concept of Knowledge-Evolution Trajectory (KET) as a
model to formalize the fusion between the spatial, temporal, and content-
based aspects of scientiﬁc publications.
2. We provide a preliminary proof-of-concept implementation that demonstrates
the feasibility of the proposed model. Speciﬁcally, we created a SQL Server
database that: (a) contains the data pertaining to medical publications,
fetched from PubMed; (b) We generated the geospatial information about
each publication by using Google Map API to convert the institution name
into (Lat, Lon), and then used ArcGIS to generate the values to be used by
the Geometry type of SQL Server.
We also conducted some preliminary experiments which, in addition to
demonstrating the feasibility of our objectives, also point out some interesting
research challenges.
In the rest of this paper, Sect. 2 introduces the KET model, followed by Sect. 3
in which we describe the current case-study implementation and the experimen-
tal observations. We summarize and outline directions for future work in Sect. 4.
2
Modelling Spatio-Temporal Evolution of Scientiﬁc
Literature
We now present the main aspects of the KET model. We ﬁrstly introduce the
concept of symbolic trajectories.
Semantic (synonymously, Symbolic or Enriched or Spatio-textual) Trajecto-
ries [3,5,11,15,19] embed contextual and/or situational knowledge into location-
in-time data. In a MOD [10] setting, a trajectory is modelled as a structure of the
form Tri = [oID, (xi1, yi1, ti1), . . . , (xik, yik, tik)], where xij and yij (1 ≤j ≤k)
are the coordinates of the location (lij = (xij, yij)) of the object with a unique
oID, obtained at time instant tij. In-between two consecutive updates, the
object’s motion is approximated in accordance with some kind of an interpo-
lation. STs attempt also to describe the kinds of activities associated with a
particular location and time – e.g., “stop”, “move”, “walk”, “eat”, etc. For-
mally (cf. [5,19]), a semantic trajectory STi is a sequence of so-called, semantic
episodes sei,m as follows:
STi = [sei1, sei2, sei3, . . . seim], and each semantic episode is a tuple of the
form:
seij = (daij, spij, xin
ij , yin
ij , tin
ij , xout
ij , yout
ij , tout
ij , tagListij)
where:
– daij = deﬁning annotation; typically expressing an activity (verb) such as
stop or move.
– spij = semantic location/position of the activity, like a POI (e.g., a museum,
restaurant, zoo), home, work, etc.

202
G. Trajcevski et al.
– tin
ij and tout
ij
= entry/exit times of a semantic position.
– xin
ij , yin
ij , xout
ij , yout
ij
= entry/exit coordinates of a semantic position.
– tagListij = any additional semantic information, like transportation mode,
additional activity description (e.g., eat), etc.
constitute the j-th semantic episode of the i-th semantic trajectory.
Fig. 1. Semantic trajectory and spatio-temporal range querying
Left portion of Fig. 1 illustrates the concept of a semantic trajectory (cf. [19]),
and the right portion (cf. [15]) illustrates yet another way of visualizing a seman-
tic trajectory (i.e., color-coded activity) along with the semantics of processing
the query Retrieve all the individuals who were running in the region R1, between
7:00 and 8:00 AM.
There are two fundamental observations when it comes to the existing model
of semantic trajectories, and the KET model that we are proposing:
1. There is no concept of a “motion”, as commonly perceived in MOD trajec-
tories (even when augmented with annotation). The scientiﬁc publications
are associated with spatial data – e.g., the locations of the institution of the
participating authors, and those are not mobile entities. However, there is an
evolution over the temporal dimension that, in part, is associated with spatial
values.
2. The scientiﬁc publications have a lot more contextual attributes, and those
attributes are composite/richer. Namely, considering a typical meta-data2,
they contain:
– Title;
– Category (in accordance with an established nomenclature of the correspond-
ing ﬁeld; possibly a set of such categories, augmented with a set of keywords).
– A set of authors names and the corresponding institution of his aﬃliation.
2 Aside from the main body of the text of the respective publications, or other
attributes associated with, e.g., publishers, forum/venue, etc...

Spatio-Temporal Evolution of Scientiﬁc Knowledge
203
It is precisely the elements of the institution that contain an implicit spatial
value, which we use for enabling the novel categories of queries.
Based on (1) and (2) above, we deﬁne the concept of Scientiﬁc Publication
Point (SPP) as follows:
SPPi =
(PID, Title, category, [(author1i, inst1i, x1i, y1i), . . . , (authorki, instki, xki, yki)], Tpub)
We re-iterate that, in practice, most of the bibliographical data sources contain
the name of the institution (along with its postal-address) for each author –
however, not the actual coordinates for the corresponding address. This, in turn,
eliminates the possibility of asking any queries containing predicates ranging over
spatial domains. However, such queries may provide insight into data/trends
that could inﬂuence both government funding as well as institutional/individual
collaboration plans, as exempliﬁed by the query: Retrieve all the institutions
within 100 Km from the coastal line in China, which have received more than
10M renminbi research funding in the last 4 years for medical research, but have
published less than 20 articles on the topic of cytostatics.
Fig. 2. KET for Geo-constrained Query
Assuming that the temporal value in each SPPi (i.e., SPPi.Tpub) is repre-
sented at a uniform particular level of granularity (e.g., (month, year)), we now
present formally the model for a KET (Knowledge-Evolution Trajectory) deﬁned
as follows:
Deﬁnition 1. A Knowledge-Evolution Trajectory is a sequence
[α1(SPP (1)), α2(SPP (2)), . . . , αn(SPP (n))] where:
– αi denotes an operator from relational algebra or a spatial predicate, applied
to SPPi
– For any pair SPP (i), SPP (j), (i < j) ⇒SPP (i).Tpub < SPP (j).Tpub

204
G. Trajcevski et al.
Given the deﬁnition of SPPi, the role of αi in Deﬁnition 1 is to extract the
proper content of interest, the evolution of which needs to be queried. Thus,
for example, we can focus on a particular author by applying (SPPi).author =
′Jones′. However, the main beneﬁt of the proposed model is that we can also
apply spatial predicates such as: (SPPi). (xmi, ymi)IN ′Pennsylvania′.
Clearly, the KET model generalizes the traditional model of a trajectory used
in MOD literature and, for that matter, also generalizes the semantic trajectories.
Figure 2 shows an example of a KET corresponding to an answer of the query
Retrieve all the collaborative works between Midwest-based and California-based
institutions, between January 2009 and December 2010. As can be seen, instead
of a traditional (x, y) point, we now have collection of Geo-points that constitute
each one of the trajectories of the answer. Moreover, we can also see that a
particular Geo-point can participate in > 1 KET – for as long as the constraints of
the query are satisﬁed. Thus, we have a collaboration between an institution from
California and Illinois on a publication related to T-cell studies, in December of
2010.
Fig. 3. KET for participation constraint query
Figure 3 illustrates another example of a KET – which visually (and type-
wise) has a highest resemblance of a traditional moving point-object trajectory.
However, it shows an example of an actual answer from our implementation,
corresponding to the query Retrieve all the institutions that have published an
article on cytostatics in which all the authors were from a same institution,
between January, 2014 and July 2015.
We close this section with another example-query the answer of which, in
some sense, does resemble spatio-temporal trajectories, but yet it has its own
distinct semantics of the temporal evolution.
Figure 4 shows the answer to the query Retrieve which topics/categories had
most publications, for the researchers from the SouthWest University, as well as
for the ones from Arizona State University, between 2008 and 2012.

Spatio-Temporal Evolution of Scientiﬁc Knowledge
205
Fig. 4. Following most popular topics per institution
3
Case-Study: Spatio-Temporal Evolution in PubMed
We now describe in detail the current implementation of our proof-of-concept
system for which the context is restricted to the PubMed data, pertaining to the
various medical publications available on MedLine. We used SQL Server 2014
and we wrote python scripts to extract the data from PubMed and populate
the tables3. As mentioned earlier, the PubMed data does not contain geo-spatial
information4 – whereas SQL Server provides two diﬀerent geospatial data types:
Geometry type and Geography type. Thus, in our prototype system, we selected
[Publish ID], [Title], [Author] (including their names and institutions), [Publish
Date] from the returned XML records from PubMed – however, in addition,
we populated the entries for the corresponding Geometry type in which the
coordinate system is World Geodetic System (WGS) 1984. To populate the cor-
responding values for the geospatial information, we used a two-step procedure:
(1) Google Map API was used to convert the institution name into (Lat, Lon)
pair of values.
(2) Subsequently, ArcGIS was used to convert from (ﬂoat Lat, ﬂoat Lon) into
the corresponding Geometry type.
3 We note that all the data, code for the queries, as well as the scripts used to generate
the values for the spatial attributes, is publicly available at https://github.com/
ShailavTaneja/PubMedDerivedDataAndImplementation.
4 The
description
of
the
standard
meta-data
used
in
PubMed
is
available
at
https://www.ncbi.nlm.nih.gov/books/NBK3828/#publisherhelp.Example of a
Standard XML.

206
G. Trajcevski et al.
This is what enabled us to specify queries that can capture the spatio-
temporal evolution of the knowledge represented in scientiﬁc – in this case,
medical – publications.
In the ﬁrst iteration, we had a “na¨ıve” representation of the data residing in
a single table, with attributes:
- [Publish ID] varchar(50),
- [Title] varchar(500),
- [Publish Date] date,
- [Author Name] varchar(50),
- [Institution Name] varchar(150),
- [Geo Information] geometry,
- [Subcategory] varchar(150),
- [Category] varchar(150)
The table had 409702 rows in total.
Fig. 5. Normalized database schema
Subsequently, to eliminate the redundancy, we normalized the na¨ıve table,
and the schemata that we used in the implementation is shown in Fig. 5, corre-
sponding to the following tables:
– Main – with three attributes: [Publish ID] varchar(50), [Title] varchar(500),
[Publish Date] date. 32768 row in total
– Author – with three attributes: [Publish ID] varchar(50), [Author Name] var-
char(50), [Institution] varchar(150). 188786 rows in total.
– Category is a table capturing the hierarchy of the categories. It has three
attributes: [Index] hierarchyid, [Information] varchar(150), [Node Level] var-
char(50). 68652 rows in total. Hierarchyid is a special data type and works
as the key of Category table. / represents root, /*/ represents the ﬁrst level
and so on
– Geospatial – with two attributes: [Institution] varchar(150), [Location]
geometry. 1316 rows in total.

Spatio-Temporal Evolution of Scientiﬁc Knowledge
207
In the sequel, we describe an example of the diﬀerence between evaluating
queries in the na¨ıve representation and the normalized one. Consider the follow-
ing:
Qtopic: Retrieve the KET for publications addressing the topic of diagnosis, in
the period of October 1, 2012 – September 1, 2016.
The query returns a set of temporally annotated points (geo locations) for
the topic, during that period.
The corresponding SQL statement for the na¨ıve implementation is:
SELECT DISTINCT [Publish ID], [Publish Date], Location.STX, Location.STY
FROM NativeTable
WHERE ([Publish Date] between ’2012-10-01’ and ’2016-09-01’) and
([Subcategory] = ’diagnosis’)
ORDER BY [Publish Date]
The SQL implementation for the same query in the normalized version is:
DECLARE @catLevel hierarchyid;
SET @catLevel = (SELECT Index
FROM Category
WHERE Information = ’diagnosis’)
SELECT DISTINCT MainTable.[Publish ID], MainTable.[Publish Date],
Location.STX, Location.STY
FROM MainTable, Author, Geospatial
WHERE (MainTable.[Publish Date] between ’2012-10-01’ and ’2016-09-01’) and
(MainTable.[Publish ID] in
(SELECT Information
FROM Category
WHERE Index.IsDescendantOf(@catLevel) = 1 and [Node Level] = ’Publish ID’))
and
(MainTable.[Publish ID] = Author.[Publish ID]) and
(Author.Institution = Geospatial.Institution)
ORDER BY MainTable.[Publish Date]
To illustrate the impact of the diﬀerent database representation, we ﬁrst
illustrate the beneﬁts in terms of eliminating the redundancy via the normalized
representation:
As shown in Fig. 6, the na¨ıve implementation requires approximately ﬁve
times more space than the normalized one.
When it comes to the eﬃciency of the execution with each implementation we
report the corresponding measurements observed when executing Qtopic (labeled
Q1 in Fig. 7) and the query
Q2: Retrieve the works jointly co-authored by Masaki Mori and Yuichiro Doki,
between 2009–2010 and 2012–2013
on a Windows 10 machine, with Intel(R) Core(TM) i5-5257U CPU (2.70 GHz,
2701 Mhz), with 2 Cores (4 Logical Processors) and 8 GB of RAM.
As shown in Fig. 7, the normalized implementation also yields a much more
eﬃcient execution than the na¨ıve one.

208
G. Trajcevski et al.
Fig. 6. Space requirements
Fig. 7. Eﬃciency of execution (in milliseconds)
4
Conclusions and Future Work
We addressed the problem of modelling and querying the spatio-temporal evolu-
tion of the knowledge recorded in scientiﬁc publications, and took the ﬁrst steps
towards providing a formalism to capture such evolution across the (geo)spatial
domain, as well as other contextual attributes. We proposed the concept of KET
(Knowledge-Evolving Trajectories) as a possible uniﬁcation between two ﬁelds
(Information Retrieval and Spatio-Temporal/Moving Objects Databases). This
type of uniﬁcation provides opportunities for novel query categories which, to
our knowledge, have not been formally treated in the literature. In addition, we
provided a proof-of-concept implementation of our approach for the data avail-
able in PubMed, and compared the impacts of a na¨ıve design of the database vs.
a normalized one – albeit for limited set of queries. Our initial evaluations have

Spatio-Temporal Evolution of Scientiﬁc Knowledge
209
demonstrated that in addition to reducing the space requirements, the normal-
ized approach also enables faster execution of the KET-based queries.
Related Works: There are plethora of works in each of the ﬁelds of IR [6,
14,17,21] and STDb/MOD [7,9,10,18,20,23,28] – to mention but a few. The
novelty of our proposed approach is to provide a formalism for fusing these works,
along with the integration of the respective existing datasets, enabling novel
query-categories. The closest formalism to our proposed approach is the one of
semantic/symbolic trajectories [19]. However, as we argued, this approach: (1)
has too simplistic model of the spatio-temporal evolution; and (2) is lacking the -
dimensionality of the contexts typically associated with scientiﬁc publications.
We believe to have scratched the surface of a direction that may be of interest
in many applications of societal relevance and, moreover, can pose interesting
challenges. As part of our future work, we are planning to extend the KET model,
and augment the current implementation so that it can incorporate diﬀerent
publications’ data sources. In addition, although we have provided some prelim-
inary discussions related to the eﬃciency, a challenging problem is to address
the eﬃcient processing diﬀerent types of KET-based queries.
Last, but not the least, it seems rather intuitive that investigations along
the direction of warehousing spatio-temporal evolution of scientiﬁc publications
along with further semantic similarity searches [25,27] data may yield novel
categories of analytical queries.
References
1. Bedard, Y., Merrett, T., Han, J.: Fundamentals of spatial data warehousing for
geographic knowledge discovery. Geogr. Data Min. Knowl. Discov. 2, 53–73 (2001).
Taylor and Francis
2. Bilgen, M., Abbe, R., Liu, S.J., Narayana, P.A.: Spatial and temporal evolution
of hemorrhage in the hyperacute phase of experimental spinal cord injury: in vivo
magnetic resonance imaging. Magn. Ressonance Med. 43(4), 594–600 (2000)
3. Bogorny, V., Renso, C., de Aquino, A.R., de Lucca Siqueira, F.: Constant - A
conceptual data model for semantic trajectories of moving objects. GIS 18(1),
66–88 (2014)
4. Chu, W.W., Cardenas, A.F., Taira, R.T.: Kmed: A knowledge-based multimedia
medical distributed database system. Inf. Sci. 20(2), 75–96 (1995)
5. Damiani, M.L., G¨uting, R.H.: Semantic trajectories and beyond. In: Proceedings
of IEEE - MDM, pp. 1–3. Brisbane, Australia (2014)
6. Dee, C.R.: The development of the medical literature analysis and retrieval system
(medlars). J. Med. Library Assoc. 94(5), 416–425 (2007)
7. Ding, H., Trajcevski, G., Scheuermann, P.: Towards eﬃcient maintenance of con-
tinuous queries for trajcectories. GeoInformatica 12(3), 255–288 (2008)
8. Etzion, O., Jajodia, S., Sripada, S. (eds.): Temporal Databases: Research and Prac-
tice. LNCS, vol. 1399. Springer, Heidelberg (1998)
9. G¨uting, R.H., B¨ohlen, M.H., Erwig, M., Jensen, C.S., Lorentzos, N., Schneider,
M., Vazirgiannis, M.: A foundation for representing and queirying moving objects.
ACM TODS 25, 1–42 (2000)
10. G¨uting, R.H., Schneider, M.: Moving Objects Databases. Morgan Kaufmann,
San Francisco (2005)

210
G. Trajcevski et al.
11. G¨uting, R.H., Vald´es, F., Damiani, M.L.: Symbolic trajectories. ACM Trans. Spat.
Algorithms Syst. 1(2), 7:1–7:51 (2015)
12. Hirano, Y., Stefanovic, B., Silva, A.C.: Spatiotemporal evolution of the fmri
response to ultrashort stimuli. J. Neurosci. 31(4), 1440–1447 (2011)
13. Hjaltason, G.R., Samet, H.: Distance browsing in spatial databases. ACM Trans.
Database Syst. 24(2), 265–318 (1999)
14. Hristovski, D., Kastrin, A., Dinevski, D., Rindﬂesch, T.C.: Constructing a graph
database for semantic literature-based discovery. In: MEDINFO 2015: eHealth-
enabled Health - Proceedings of the 15th World Congress on Health and Biomedical
Informatics, p. 1094 (2015)
15. Issa, H.: Spatio-textual trajectories: models and applications. PhD thesis, Univer-
sita degli studi di Milano (2017)
16. Korfhage, R.: The impact of personal computers on library-based information sys-
tems. SIGIR Forum 12(4), 10–13 (1978)
17. Lowe, H.J., Barnett, G.O.: Understanding and using the medical subject heading
(mesh) vocabulary to perform literature searches. J. Am. Med. Assoc. 271(14),
1103–1108 (1994)
18. Mokbel, M.F., Aref, W.G.: SOLE: scalable on-line execution of continuous queries
on spatio-temporal data streams. VLDB J. 17(5), 971–995 (2008)
19. Parent, C., Spaccapietra, S., Renso, C., Andrienko, G.L., Andrienko, N.V.,
Bogorny, V., Damiani, M.L., Gkoulalas-Divanis, A., de Macˆedo, J., Pelekis, N.,
Theodoridis, Y., Yan, Z.: Semantic trajectories modeling and analysis. ACM Com-
put. Surv. 45(4), 42 (2013)
20. Pelanis, M., Saltenis, S., Jensen, C.S.: Indexing the past, present, and anticipated
future positions of moving objects. ACM Trans. Database Syst. 31(1), 255–298
(2006)
21. Salton, G.: Automatic Text Processing. Addison Wesley, Massachusetts (1989)
22. Schiller, J.H., Voisard, A. (eds.): Location-Based Services. Morgan Kaufmann,
San Francisco (2004)
23. Shekhar, S., Chawla, S.: Spatial Databases: A Tour. Prentice Hall, New Jersy
(2003)
24. Taine, S.I.: New program for indexing at the national library of medicine. Bull.
Med. Libr. Assoc. 47(2), 117 (1959)
25. Trajcevski, G., Donevska, I., Vaisman, A.A., Avci, B., Zhang, T., Tian, D.:
Semantics-aware warehousing of symbolic trajectories. In: Proceedings of the 6th
ACM SIGSPATIAL International Workshop on GeoStreaming, IWGS 2015, pp.
1–8, 3–6 November 2015, Bellevue, WA, USA (2015)
26. Trajcevski,
G.,
Tamassia,
R.,
Cruz,
I.,
Scheuermann,
P.,
Hartglass,
D.,
Zamierowski, C.: Ranking continuous nearest neighbors for uncertain trajectories.
VLDB J. 20(5), 767–791 (2011)
27. Vaisman, A.A., Zim´anyi, E.: Data Warehouse Systems: Design and Implementa-
tion. Data-Centric Systems and Applications. Springer, Heidelberg (2014)
28. Xing, X., Mokbel, M.F., Aref, W.G., Hambrusch, S.E., Prabhakar, S.: Scalable
spatio-temporal continuous query processing for location-aware services. In: Inter-
national Conference on Scientiﬁc and Statistical Database Management (SSDBM)
(2004)
29. Xiong, X., Mokbel, M.F., Aref, W.G.: Sea-cnn: Scalable processing of continuous
k-nearest neighbor queries in spatio-temporal databases. In: ICDE, pp. 643–654
(2005)
30. Yu, X., Pu, K.Q., Koudas, N.: Monitoring k-nearest neighbor queries over moving
objects. In: ICDE, pp. 631–642 (2005)

The 1st International Workshop on Data
Science: Methodologies and Use-Cases
(DaS 2017)

Parallel Subspace Clustering Using Multi-core
and Many-core Architectures
Amitava Datta1, Amardeep Kaur1, Tobias Lauer2(B), and Sami Chabbouh2
1 School of Computer Science and Software Engineering,
University of Western Australia, Perth, Australia
2 Department of Electrical Engineering and Information Technology,
Oﬀenburg University of Applied Sciences, Oﬀenburg, Germany
tobias.lauer@hs-offenburg.de
Abstract. Finding clusters in high dimensional data is a challenging
research problem. Subspace clustering algorithms aim to ﬁnd clusters
in all possible subspaces of the dataset where, a subspace is the sub-
set of dimensions of the data. But exponential increase in the number
of subspaces with the dimensionality of data renders most of the algo-
rithms ineﬃcient as well as ineﬀective. Moreover, these algorithms have
ingrained data dependency in the clustering process, thus, parallelization
becomes diﬃcult and ineﬃcient. SUBSCALE is a recent subspace clus-
tering algorithm which is scalable with the dimensions and contains inde-
pendent processing steps which can be exploited through parallelism. In
this paper, we aim to leverage, ﬁrstly, the computational power of widely
available multi-core processors to improve the runtime performance of
the SUBSCALE algorithm. The experimental evaluation has shown lin-
ear speedup. Secondly, we are developing an approach using graphics
processing units (GPUs) for ﬁne-grained data parallelism to accelerate
the computation further. First tests of the GPU implementation show
very promising results.
Keywords: Data mining · Subspace clustering · Multi-core architec-
tures · Many-core architectures · GPU computing
1
Introduction
The growing size and dimensions of data these days have set new challenges for
data mining research. Clustering is a data mining process of grouping similar
data points into clusters without any prior knowledge of the underlying data
distribution. Due to the curse of dimensionality, data group together diﬀerently
under diﬀerent subsets of dimensions, called subspaces. Subspace clustering algo-
rithms attempt to ﬁnd clusters in all possible subsets of dimensions of a given
data set [1,2].
The area of subspace clustering is of critical importance with diverse appli-
cations [1]. But due to the exponential search space with the increase in dimen-
sions, subspace clustering becomes computationally very expensive. With the
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 213–223, 2017.
DOI: 10.1007/978-3-319-67162-8 21

214
A. Datta et al.
wide availability of multi-core processors and the spread of many-core coproces-
sors such as GPUs, parallelization seems to be an obvious choice to reduce this
computational cost.
SUBSCALE is a recent subspace clustering algorithm to ﬁnd the subspace
clusters without enumerating the data points or computing any redundant clus-
ters [3,4]. This algorithm combines the dense set of points across all single dimen-
sions of the data to ﬁnd non-trivial subspace clusters. Although SUBSCALE
algorithm scales well with the dimensions and performs faster than other sub-
space clustering algorithms, it is still compute intensive due to the generation
of combinatorial 1-dimensional dense set of points. However, the compute time
can be reduced by parallelizing the computation of the dense units.
In this paper, we aim to parallelize the SUBSCALE algorithm in two ways
and investigate its runtime performance. First, we exploit current multi-core
architectures with up to 48 processing cores using OpenMP. The experimental
evaluation demonstrates the speedup of up to a factor of 27. This modiﬁed
algorithm is faster and scalable for high dimensional large data sets. Second, we
use many-core graphics processing units to exploit data parallelism on a ﬁne-
granular level, with a signiﬁcant speedup, especially for larger computations.
The latter work is ongoing and results are expected to further improve with
optimizations in the implementation.
In the next section we discuss the current related literature. Section 3 explains
subspace clustering and the algorithm we parallelize. In Sect. 4, we describe our
multi-core parallelization and analyse the performance of the parallel imple-
mentation. Section 5 describes our current work of massive parallelization using
GPUs with preliminary results. The paper is concluded in Sect. 6.
2
Related Work
Over the past few years, there has been extensive research on clustering algo-
rithms [2]. The underlying premise that data group together diﬀerently under
diﬀerent subsets of dimensions opened the challenging domain of subspace clus-
tering algorithms [1,5].
However, the increase in the dimensions of data impedes the performance
of clustering algorithms which are known to perform very well with low dimen-
sions. Moreover, most of the subspace clustering algorithms have less obvious
parallel structures [6,7]. This is partially due to the data dependency during the
processing sequence [8].
SUBSCALE [3,4] is a recent subspace clustering algorithm and requires only
k database scans to process a k-dimensional dataset. Also, this algorithm is scal-
able with dimensions and its structure contains the computation of independent
tasks which can be parallelized. In the next section, we brieﬂy discuss the SUB-
SCALE algorithm and our modiﬁcations for multi-core parallel implementation.

Parallel Subspace Clustering Using Multi-core and Many-core Architectures
215
3
Subspace Clustering
This section provides the basics and deﬁnitions of subspace slustering and a
brief description of SUBSCALE [4], the algorithm which we aim to make more
eﬃcient through parallelization.
Given an n × k set of data points, a point Pi is a k-dimensional vector
{P 1
i , P 2
i , . . . , P k
i } such that, P d
i is the projection of a point Pi on the dth dimen-
sion. A subspace is a subset of k dimensions. A subspace cluster Ci = (P, S) is
a set of points P, such that the projections of these points in subspace S, are
dense.
According to the Apriori principle [6], a dense set of points in a subspace S
of dimensionality a, is dense in all of 2a projections of S. Thus, it is suﬃcient to
ﬁnd a cluster in its maximal subspace. A cluster Ci = (P, S) is called a maximal
subspace cluster, if there is no other cluster Cj = (P, S′) such that S′ ⊃S.
The SUBSCALE Algorithm ﬁnds such maximal subspace clusters by combining
the dense points from single dimensions and without computing the redundant
non-maximal clusters.
3.1
SUBSCALE Algorithm
The main idea behind the SUBSCALE algorithm is to ﬁnd the dense sets of
points (also called density chunks) in all of the k single dimensions, generate the
relevant signatures from these density chunks, and collide them in a hash table
(hTable) to directly compute the maximal subspace clusters as explained below.
Density Chunks. Based on two user deﬁned parameters ϵ and τ, a data point
is dense if it has atleast τ points within ϵ distance. The neighbourhood N(Pi)
of a point Pi in a particular dimension d is a set of all the points Pj such that
L1(P d
i , P d
j ) < ϵ, Pi ̸= Pj. L1 is the distance metric. Each dense point along with
its neighbours, forms a density chunk such that each member of this chunk is
within ϵ distance from each other.
The smallest possible dense set of points is of size τ + 1, called a dense unit.
In a particular dimension, a density chunk of size t can have

t
τ+1

possible
combinations to form the dense units. Some of these dense units may or may
not contain projections of the higher dimensional maximal subspace clusters.
Without any prior information of the underlying data distribution, it is not
possible to know the promising dense units in advance. Only viable solution is
to check which of these dense units from diﬀerent dimensions contain identical
points.
Signatures. The SUBSCALE algorithm proposed a novel way to match the
dense units by assigning signatures to them. To create signatures, each of the n
data points is mapped to a random, unique and large integer key. The sum of
the mapped keys of the data points in each dense unit is termed as its signature.

216
A. Datta et al.
According to observations 2 and 3 in the SUBSCALE paper [3], two dense
units with equal signatures would have identical points in them with extremely
high probability. Thus, collisions of the signatures across dimensions dr, . . . , ds
implies that, the corresponding dense unit exists in the maximal subspace, S =
{dr, . . . , ds}. We refer our readers to the extended version of the original paper
[4] for the detailed explanation. Each single dimension may have zero or more
dense chunks, which in turn generate diﬀerent number of signatures in each
dimension. Some of these signatures will collide with the signatures from the
other dimensions to give a set of dense points in the maximal subspace.
Hash Table. The SUBSCALE algorithm uses a hash table data structure
hTable to store collision information about each signature. An hTable has a
ﬁxed number of slots and each slot can store one or more signatures, depend-
ing upon the implementation (Fig. 1). When a signature Sig is generated in a
dimension d, it is mapped to a slot in the hTable. In this paper, we used modulo
numSlots for mapping of signatures to a slot. If a slot already contains a signa-
ture Sig′ such that Sig = Sig′, then d is appended to the dimension-list attached
to Sig.
Fig. 1. hTable data structure.
Since the size of each dense unit is τ + 1, the value of a signature generated
from a dense unit will lie in the range R = [(τ +1)·minK, (τ +1)·maxK], where
minK and maxK are the smallest and the largest keys respectively. Also, if
numSigd is the number of total signatures in a dimension d, then the total num-
ber of signatures in a k-dimensional data set will be totalsig = k
d=1 numSigd.
If memory is no onstraint a hash table with |R| slots can easily accommodate
totalsig, as typically, totalsig ≪R. Since memory is a constraint, the range R can
be split into multiple slices and each slice can be processed independently using a
separate and smaller hash table. The computations for each slice is not dependent
of other slices. The split factor called sp determines the number of splits of R and
its value can be set according to the available working memory. Also, the clus-
ter quality is not aﬀected by splitting of hash table computations. The clusters
are formed by combining the dense units in each maximal subspace. The total

Parallel Subspace Clustering Using Multi-core and Many-core Architectures
217
number of dense units are decided by the density chunks created through epsilon
neighbourhoods in single dimensions. As long as all dense units are processed,
same clusters will be generated through sequential or parallel methods.
4
Multi-core Parallelization Using OpenMP
We used the OpenMP platform with C to parallelize the SUBSCALE algorithm.
OpenMP is a set of complier directives and callable runtime library routines to
facilitate shared-memory parallelism [9].
4.1
Processing Dimensions in Parallel
The generation of signatures from the density chunks in each single dimension
is independent of other dimensions. Thus, the dimensions can be divided among
the available processing cores to be run in parallel using threads. The hash table
hTable is shared among threads. However, the problem of thread contention
arises when multiple threads try to get mutually exclusive access of the same
slot of hTable to update or store the signature information. Without mutually
exclusive access, two threads with the same signatures generated from two diﬀer-
ent dimensions, would overwrite the same slot of hTable. The maximal subspace
of a dense unit can only be found by having the information about which of the
dimensions generated this dense unit. We discuss the results from this method
in Sect. 4.3.
4.2
Processing Slices in Parallel
The other approach to avoid thread contention is to utilise the splitting of the
range R of expected signature values as proposed by the SUBSCALE algorithm.
The slices created through the splitting can be processed in parallel as each slice
generates signatures from diﬀerent range compared to other slices. Each slice
requires a separate hash table. Though this approach helps to achieve faster
clustering performance from the SUBSCALE algorithm, the memory required
to store all of the hash tables can still be a constraint. Since R denotes the whole
range of computation sums that are expected during the signature generation
process, we can bring these slices into the main working memory one by one.
Each slice is again split into sub-slices to be processed with multiple threads.
The total number of signatures can be pre-calculated from the dense chunks in
all dimensions. The results and their evaluation are discussed in the next section.
4.3
Results and Analysis
The experiments were carried out on the IBM Softlayer Server Quad Intel Xeon
E7-4850, 2 GHz, with 48 cores, 128 GB RAM and Ubuntu 15.04 kernel. Hyper-
threading was disabled on the server so that each thread could run on a separate
physical core and parallel performance could be measured fairly. The parallel

218
A. Datta et al.
version of the SUBSCALE algorithm was implemented in C using OpenMP
directives. Also, we used 14-digit non-negative integers for the key database.
The two main datasets for this experiment: 4400 × 500 madelon dataset [10]
and 3661×6144 pedestrian dataset [11,12], are publicly available. These datasets
were also used by authors of the SUBSCALE algorithm.
Multiple Cores for Dimensions.
We used the madelon data set with
ϵ = 0.000001, τ = 3 and experimented with three diﬀerent number of slots in the
shared hTable: 0.1 million, 0.5 million and 1 million. Figure 2a shows the runtime
performance of the madelon data set by using multiple threads for dimensions.
We observe that performance improves slightly by processing dimensions in par-
allel but as discussed before, thread contention due to mutually exclusive access
to the same slot in the shared hash table results in performance degradation.
Multiple Cores for Slices. To avoid this memory contention due to shared
hTable, we split the hash table computations into slices according to the SUB-
SCALE algorithm and distribute these slices among multiple cores. Figure 2b
shows the results of the runtime versus the number of threads used for process-
ing the slices of the madelon dataset. The hash computation was sliced with
diﬀerent values of split factor sp ranging between 200 and 2000. We can see the
performance boost by using more threads. The speed up is shown in Fig. 2c,
which becomes linear as the number of slices increases.
Scalability with Dimensions. The 6144 dimensional pedestrian dataset is
used to study the speed up with the increase in dimensions. With ϵ = 0.000001
and τ = 3, 19860542724 total signatures are expected which would require
∼592 GB of working memory to store the hash tables. To overcome this huge
memory requirement, we can split these signature computations twice. We used
a split factor of 60 to bring down the memory requirement for total hash tables.
Each of these 60 slices were further split into 200 subslices to be run on 48 cores.
12 4
8
16
32
48
0
200
400
600
800
1000
Runtime (s)
No. of threads
100000 slots in hTable
500000 slots in hTable
1000000 slots in hTable
Dimensions in parallel
1 4
8
16
32
48
0
200
400
600
800
Runtime (s)
No. of threads
sp=200
sp=500
sp=100
sp=1500
sp=2000
Slices in parallel
1 4
8
16
32
48
0
5
10
15
20
Speedup
No. of threads
sp=200
sp=500
sp=100
sp=1500
sp=2000
Speed up
Fig. 2. madelon dataset: ϵ = 0.000001 and τ = 3. In (a), the total dimensions are
divided among available cores using threads. Due to thread contention, the runtime
fails to improve. In (b), the slices of hash computation (sp denotes the split factor) are
distributed among multiple cores and runtime improves with number of threads.

Parallel Subspace Clustering Using Multi-core and Many-core Architectures
219
The number of slots in each hTable are ﬁxed using total signatures
sp
. The execu-
tion time decreases drastically with increase in the number of threads. It took
around 26 h to ﬁnish processing all of the 60 × 200 slices. The sequential version
of SUBSCALE clustering algorithm takes about ∼720 h.
5
Fine-Grained Parallelization Using GPUs
In this section, we describe an alternative way of parallelization with a much
ﬁner-grained task structure, suitable for parallelization on graphics processing
units (GPUs). This work is ongoing, the results are preliminary.
5.1
Levels of Granularity
Finer-grained parallelism than the one described so far can be achieved on two
levels of granularity. First, we observe that within a dimension all density chunks
can be processed independently, since they are necessarily disjoint. Hence, if a
dimension contains k dense chunks, k independent tasks can be executed in
parallel by k threads. The downside of this approach is that the number and
sizes of density chunks in a dimension is not known a priori, thus resulting in
a vastly varying number of active threads with varying workloads during the
process, which is not eﬃcient and not very suitable for GPU parallelization.
However, we note that the processing within a single density chunk can also
be parallelized. Recall that this computation consists of computing the signa-
tures of all dense units, i.e. all possible combinations of τ + 1 elements, in that
chunk. Signature computation only requires read access to the points in the
respective dense unit, so even for non-disjoint units there are no thread con-
ﬂicts. Furthermore, since all the dense units of the same dimension result in
diﬀerent signatures (with very high probability, cf. Sect. 3.1), hash collisions are
very unlikely during the parallel computation within one dimension. Hence, one
notable advantage of this task structure is that there will be no thread contention
for accessing the hash table, if diﬀerent dimensions are processed sequentially.
(Note, however, that this does not preclude parallel computation of dimensions.)
5.2
Parallel Task Structure
In this parallel approach, a task consists of computing the signature value for
one single dense unit and hashing it. All such tasks can be executed in parallel.
1
for each dense unit du[i] of length tau in parallel
2
signature := 0
3
for j = 0 to tau
4
signature := signature + key(du[i].point[j])
5
htable.insert(signature)
Algorithm 1. Code for parallel computation of signatures

220
A. Datta et al.
Since a density chunk of t elements has

t
τ+1

dense units, this approach
results in a large number of small and almost identical tasks. As τ is constant
within one execution of the algorithm and since there are no branches, all tasks
execute the same sequence of instructions, but on diﬀerent data. This type of
data parallelism is well-suited for implementation using GPUs. We have devel-
oped an implementation using Nvidia's CUDA architecture and programming
model [13]. This model enables data parallelism by allowing scalable grids of
threads, depending on the size of the data to be processed. Each thread is iden-
tiﬁed by its ID and can use this ID, e.g. to determine memory locations for
reading input and writing output data. In our case, the ID is required to iden-
tify the dense unit to process.
5.3
Computing Subsets Eﬃciently
Algorithm 1 presupposes that the thread with ID i knows how to retrieve the
i-th dense unit, i.e. the subset {Pi0, , Piτ } of projected points whose signature
it computes. In a sequential scenario, this is not a problem, as the subsets of
size τ + 1 can be enumerated one after another, using an ordering in which it
is computationally cheap to calculate the lexicographically next subset from a
given one [14]. In our parallel scenario, however, each thread needs to identify
its relevant subset independently, without reference to other results, i.e., the
i-th thread must ﬁnd the i-th subset without access to the (i −1)-th subset.
Calculating directly the i-th subset is signiﬁcantly more complex than advancing
to the next subset from a given one. Using the combinatorial representation
(or, combinadics) of index i allows for a relatively eﬃcient computation of the
corresponding subset [15], but still involves O(τ · log t) calculations of binomial
coeﬃcients. Using a table of pre-calculated binomial coeﬃcients can improve
eﬃciency at the cost of extra memory usage.
An alternative solution – which is used in our current implementation – is
precomputing an array containing the lexicographic enumeration of all

t
τ+1

dense units within a density chunk of size t, i.e. the i-th position of the array
contains a representation of the i-th dense unit. A straightforward and space-
eﬃcient encoding of subsets of size τ + 1 of a set with t elements is a bit string
of length t with exactly τ + 1 bits set to 1. The precomputation of the array
is sequential but uses a very eﬃcient implementation to compute the lexico-
graphically next bit permutation [16]. Calculating the dense unit array of length
∼500000 for a density chunk of size t = 60 and dense units of size τ + 1 = 4
takes about 12 ms on an Intel Core i7-4720HQ @2.6 GHz. Computing an array
of ∼50 million permutations (t = 60, τ + 1 = 6) takes 1252 ms.
We are also working on a possible parallelization of this precomputation,
similar to the idea of parallel preﬁx calculation [17].
5.4
GPU-Based Hashing
Hashing the calculated signatures into htable can also be carried out on the GPU.
GPU-based hashing has been extensively studied by Alcantara, who proposed

Parallel Subspace Clustering Using Multi-core and Many-core Architectures
221
several eﬃcient hashing schemes [18]. Our approach, based on the implemen-
tation used in [19], is currently being implemented and hence, not part of the
evaluation presented here. Note that GPU memory is a limiting factor for the
hash table size. State of the art GPUs come with up to 16 GB of RAM, which
is suﬃcient to accommodate each partial table of the slicing approach described
in Sect. 4.2.
5.5
Performance Evaluation
Our current implementation of the GPU approach is a ﬁrst step. It has not
been optimized regarding the GPU's memory hierarchy and hence does not
beneﬁt from caching eﬀects. Also, it does currently not use more intricate CUDA
functions such as, for instance, the SHFL (shuﬄe) command, which might be
interesting for combinatoric tasks like subset enumeration.
The performance of the GPU algorithm was tested on an Intel Core i7-
4720HQ @2.6 GHz machine equipped with an Nvidia GeForce GTX 950M GPU
hosting 640 processing units (CUDA cores) and 4 GB of GPU RAM, against the
sequential CPU algorithm for computing signatures, run on the same machine.
The results are shown in Table 1. They do not include the time for precompu-
tation of subsets and for hashing the signatures, but include all transfer times
between GPU and host memory required for the GPU computations.
Table 1. Performance of CPU and GPU algorithms for computing signatures.
#Signatures computed Time CPU (ms) Time GPU (ms) Speedup factor
1,770
0.4
1.0
0.4
34,220
11.5
1.9
6.1
487,635
148.8
13.8
10.8
5,461,512
1770.0
135.8
13.0
50,063,860
17692.5
1162.2
15.2
For smaller numbers of signatures, GPU is slower than CPU. This was to
be expected as there is always a small but non-negligible ramp-up cost for GPU
kernels Note that the speedup factor increases with the amount of calculations.
Note that the GPU used for this preliminary evaluation is a relatively small
model; high-performance Tesla GPUs contain thousands of CUDA cores and
achieve signiﬁcantly higher processing power.
6
Conclusion
In this paper, we have presented two independent ways of parallelizing the SUB-
SCALE algorithm. First, we have described the use of a common shared memory

222
A. Datta et al.
multi-core architecture. Achieving the parallelization by assigning CPU cores to
slices of the hash table for store signatures and ﬁnding larger-dimensional dense
units, the results have shown linear speedup with the number of cores.
The second approach uses ﬁner-granular data parallelism and can be imple-
mented eﬃciently on graphics processing units (GPUs). First performance tests
show very promising results, especially for larger data sets. This part of the
work is ongoing; we are currently implementing the full functionality including
GPU-based parallel hashing of the signatures.
The two approaches do not exclude each other. In fact, they can complement
each other, using multi-core parallelism for coarse-grained tasks (processing of
dimensions or has table slices) and many-core data parallelism for ﬁner-grained
subtasks (such as individual signature computation). Future work includes this
combination of both approaches, making the best possible use of the diﬀerent
processing resources.
References
1. Parsons, L., Haque, E., Liu, H.: Subspace clustering for high dimensional data: a
review. ACM SIGKDD Explor. Newsl. 6(1), 90–105 (2004)
2. Aggarwal, C.C., Reddy, C.K.: Data Clustering: Algorithms and Applications, 1st
edn. Chapman & Hall/CRC, Boca Raton (2013)
3. Kaur, A., Datta, A.: Subscale: fast and scalable subspace clustering for high dimen-
sional data. In: 2014 IEEE International Conference on Data Mining Workshop
(ICDMW), pp. 621–628 (2014)
4. Kaur, A., Datta, A.: A novel algorithm for fast and scalable subspace clustering of
high-dimensional data. J. Big Data 2(1), 17 (2015)
5. Sim, K., Gopalkrishnan, V., Zimek, A., Cong, G.: A survey on enhanced subspace
clustering. Data Min. Knowl. Disc. 26(2), 332–397 (2013)
6. Agrawal, R., Gehrke, J., Gunopulos, D.: Automatic subspace clustering of high
dimensional data for data mining applications. In: Proceedings of the ACM
SIGMOD International Conference on Management of Data, pp. 94–105 (1998)
7. Kailing, K., Kriegel, H.P., Kroger, P.: Density-connected subspace clustering for
high-dimensional data. In: SIAM International Conference on Data Mining, pp.
246–256 (2004)
8. Zhu, B., Mara, A., Mozo, A.: CLUS: parallel subspace clustering algorithm on
spark. In: Morzy, T., Valduriez, P., Bellatreche, L. (eds.) ADBIS 2015. CCIS, vol.
539, pp. 175–185. Springer, Cham (2015). doi:10.1007/978-3-319-23201-0 20
9. Dagum, L., Menon, R.: OpenMP: an industry standard API for shared-memory
programming. IEEE Comput. Sci. Eng. 5, 46–55 (1998)
10. Bache, K., Lichman, M.: UCI Machine Learning Repository (2013)
11. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the KITTI
dataset. Int. J. Rob. Res. 32(11), 1231–1237 (2013)
12. Zhu, J., Liao, S., Lei, Z., Yi, D., Li, S.Z.: Pedestrian attribute classiﬁcation in
surveillance: database and evaluation. In: ICCV Workshop on Large-Scale Video
Search and Mining (LSVSM 2013), Sydney (2013)
13. Nvidia: CUDA home page. http://www.nvidia.com/object/cuda home new.html.
Accessed 26 May 2017

Parallel Subspace Clustering Using Multi-core and Many-core Architectures
223
14. Loughry, J., van Hemert, J., Schoofs, L.: Eﬃciently enumerating the subsets of a
set (2000). applied-math.org/subset.pdf
15. McCaﬀrey, J.: Generating the mth lexicographical element of a mathematical com-
bination. MSDN Library (2004)
16. Anderson,
S.E.:
Bit
Twiddling
Hacks
compute
the
lexicographically
next
bit
permutation.
http://graphics.stanford.edu/∼seander/bithacks.html#
NextBitPermutation. Accessed 26 May 2017
17. Harris, M., Sengupta, S., Owens, J.D.: Parallel preﬁx sum (scan) with CUDA.
GPU gems 3(39), 851–876 (2007)
18. Alcantara, D.A.F.: Eﬃcient hash tables on the GPU. Ph.D. thesis, University of
California Davis (2011)
19. Strohm, P.T., Wittmer, S., Haberstroh, A., Lauer, T.: GPU-accelerated quantiﬁ-
cation ﬁlters for analytical queries in multidimensional databases. In: Bassiliades,
N., Ivanovic, M., Kon-Popovska, M., Manolopoulos, Y., Palpanas, T., Trajcevski,
G., Vakali, A. (eds.) New Trends in Database and Information Systems II. AISC,
vol. 312, pp. 229–242. Springer, Cham (2015). doi:10.1007/978-3-319-10518-5 18

Discovering High-Utility Itemsets at Multiple
Abstraction Levels
Luca Cagliero(B), Silvia Chiusano, Paolo Garza, and Giuseppe Ricupero
Dipartimento di Automatica e Informatica, Politecnico di Torino, Turin, Italy
{luca.cagliero,silvia.chiusano,paolo.garza,giuseppe.ricupero}@polito.it
Abstract. High-Utility Itemset Mining (HUIM) is a relevant data min-
ing task. The goal is to discover recurrent combinations of items char-
acterized by high proﬁt from transactional datasets. HUIM has a wide
range of applications among which market basket analysis and service
proﬁling. Based on the observation that items can be clustered into
domain-speciﬁc categories, a parallel research issue is generalized itemset
mining. It entails generating correlations among data items at multiple
abstraction levels. The extraction of multiple-level patterns aﬀords new
insights into the analyzed data from diﬀerent viewpoints. This paper
aims at discovering a novel pattern that combines the expressiveness of
generalized and High-Utility itemsets. According to a user-deﬁned tax-
onomy items are ﬁrst aggregated into semantically related categories.
Then, a new type of pattern, namely the Generalized High-utility Itemset
(GHUI), is extracted. It represents a combinations of items at diﬀerent
granularity levels characterized by high proﬁt (utility). While proﬁtable
combinations of item categories provide interesting high-level informa-
tion, GHUIs at lower abstraction levels represent more speciﬁc correla-
tions among proﬁtable items. A single-phase algorithm is proposed to
eﬃciently discover utility itemsets at multiple abstraction levels. The
experiments, which were performed on both real and synthetic data,
demonstrate the eﬀectiveness and usefulness of the proposed approach.
Keywords: High-Utility Itemset Mining · Generalized itemset mining ·
Data mining · Knowledge discovery
1
Introduction
Frequent itemset mining is an exploratory data mining technique which focuses
on discovering recurrent combinations of items (of arbitrary size) that occur
in potentially large transactional data [1]. Frequent itemsets have been used in
many research contexts, among which market basket analysis [1], service proﬁl-
ing [3], to discover correlations between multiple data items. For example, in the
context of market basket analysis, each row of the dataset (transaction) repre-
sents a diﬀerent market basket. Transactions contain the subsets of purchased
items. Frequent itemsets represent sets of items that customers frequently pur-
chased together. For instance, itemset {Coke, bread} indicates that customers
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 224–234, 2017.
DOI: 10.1007/978-3-319-67162-8 22

Discovering High-Utility Itemsets at Multiple Abstraction Levels
225
who purchased coke frequently purchased bread as well. Since generating all
the possible combinations of items in a transactional dataset is computationally
intractable [1], frequent itemset mining entails discovering only the combina-
tions of items whose frequency of occurrence (support) is above a given thresh-
old. However, the traditional itemset mining problem relies on three (potentially
unreliable) assumptions:
(A) Items appear at most once in each transaction (e.g., we disregard the
amounts of purchased items within each basket).
(B) Items have all the same importance in the analyzed data (e.g., the unit
proﬁt is assumed to be the same for all the items in the market).
(C) The semantic relationships between items are ignored (e.g., the co-
occurrences of items belonging to the same product group within the same
basket are considered as uncorrelated with each other).
To overcome limitations (A) and (B), the concept of High-Utility Itemset (HUI)
has been proposed [10]. HUIs represent sets of frequently co-occurring items that
are characterized by averagely high utility within the analyzed data. To mine
HUIs, items in the transactional dataset are enriched with both per-transaction
weights (hereafter denoted as internal utilities) and global weights (denoted as
external utility). For example, in the context of market basket analysis internal
utilities represent per-item amounts (e.g. the customer purchased 3 bottles of
coke), while external utilities indicate unit proﬁts (bottles of coke cost 5 USD
each). Utility itemsets represent sets of items whose total yield is above a given
(user-speciﬁed) threshold. This knowledge may be exploited to perform cross-
selling, to plan promotions, or to eﬀectively arrange items on the shelves.
To overcome limitation (C), correlations between items at higher abstraction
levels can be analyzed [11]. Based on a taxonomy built on top of the analyzed
data, items are aggregated into semantically related groups (e.g. items Coke and
Water into group Beverage). Then, generalized itemsets, which represent corre-
lations among data items at diﬀerent abstraction levels (e.g., not only {Coke,
bread} but also {Beverage, Food}), can be extracted.
Many algorithms have been proposed to eﬃciently extract HUIs [7–9,12]
and to select compact subsets of HUIs, e.g., the closed HUIs [5] and the top-
k HUIs [13]. Existing solutions are eﬃcient in term of temporal and spatial
scalability, but, to the best of our knowledge, they are unable to cope with
multiple-level data (limitation (C)). On the other hand, parallel works addressed
the generalized itemset mining problem by performing bottom-up [2,3,11] or top-
down [6] taxonomy visits during candidate itemset generation. However, since
they do not consider item utilities, they still suﬀer from limitations (A) and (B).
This paper aims at bridging the gap between HUI mining and generalized
itemset mining. To this purpose, it proposes a new type of pattern, namely the
Generalized High-utility Itemsets (GHUIs). The proposed approach allows both
multiple appearances of the same item within each transaction and diﬀerent
per-item proﬁts. Unlike traditional HUIs, GHUIs are extracted from a trans-
actional dataset enriched with a taxonomy, which describes the semantic is-a

226
L. Cagliero et al.
Table 1. Transactional dataset
Transaction id Items and internal utility
TId1
(Coke, 2), (Bread, 2), (Steak, 1)
TId2
(Water, 3), (Pasta, 2), (Steak, 1)
TId3
(Water, 2), (Bread, 2)
TId4
(Coke, 1), (Bread, 2)
Table 2. External utilities of items
in dataset
Item
External utility
Water
1
Coke
5
Bread
1
Pasta
2
Steak
10
Coke
Water
Beverage
Bread
Pasta
Food
Steak
Fig. 1. Taxonomy on items in the dataset
relationships between data items. These relationships are exploited to drive the
process of knowledge generalization thus generating proﬁtable combinations of
items at multiple abstraction levels. To extract GHUIs we proposed ML-HUI
Miner, which extends a state-of-the-art HUI mining algorithm to cope with data
enriched with taxonomies. The newly proposed algorithm integrates taxonomy
information into the utility itemset mining process to mine GHUIs in a single-
phase mining session (Table 4).
Preliminary experiments performed on both real retail data and benchmark
datasets show the eﬃciency and eﬀectiveness of the proposed approach.
The paper is organized as follows. Sections 2 and 3 introduce preliminary con-
cepts and formalizes the newly proposed pattern, respectively. Section 4 presents
the mining algorithm used to discover the newly proposed pattern. In Sect. 5 we
summarized the experiments performed on real datasets, while Sect. 6 draws
conclusions and discusses future works.
2
Preliminaries
A transactional dataset is a set of transactions [1], where each transactions is a
set of data items (i.e., objects identiﬁed by literals). Herafter, let us denote as I
the set of all possible items and as tj ∈I the j-th transaction of a transactional
dataset D. Items are characterized by (i) Internal Utility, denoted as iu(i, tj)
which indicates the relative importance of item i ∈I in transaction tj, and (ii)
External Utility, denoted as eu(i), which indicates the relative importance of
item i in D with respect to all the other items in the dataset.
Table 1 reports an example of market basket dataset consisting of 4 transac-
tions (identiﬁed by Tids 1–4)).

Discovering High-Utility Itemsets at Multiple Abstraction Levels
227
Table 3. High-Utility Itemsets.
minutil (itemsets) = 17
Itemset
Utility
{Steak}
20
{Steak, Coke}
20
{Coke, Bread}
19
{Steak, Coke, Bread} 22
Table 4. Generalized High-Utility Itemsets.
minutil (generalized itemsets) = 30
Generalized Itemset Utility
{Food}
30
{Food, Beverage}
50
The utility of item i in transaction tj, hereafter denoted as u(i, tj), is com-
puted as eu(i) · iu(i, tj). In the running example, it indicates the total income
related to an item appearing in the market basket (e.g., the price of all the
bottles of coke in a given market basket).
Itemsets are sets of items of arbitrary size. We will denote as k-itemset a set
of k items. The utility of itemset I in transaction tj is the sum of the utilities of
all the corresponding items, i.e., u(I, tj) = 
i∈I u(i, tj). The utility of itemset I
in the transactional dataset D is obtained by summing the utilities of the itemset
in all the dataset transactions, i.e., u(I, D) = 
tj∈D u(I, tj), where we assume
that u(I, tj) = 0 if I /∈tj.
A notable type of itemset is the High-Utility Itemset. Given user-speciﬁed
minimum utility threshold minutil, an itemset mined from dataset D is an High-
Utility Itemset (HUI) if and only if u(I, D) > minutil. Given a transactional
dataset D and a minimum utility threshold minutil, the High-Utility Itemset
Mining (HUIM) problem entails discovering all the HUIs in D.
The HUIs extracted from the dataset in Table 1 by enforcing a minimum
utility threshold minutil = 20 are enumerated in Table 3, where the corresponding
utility value is given too.
Example. {Coke, Bread} is HUI because the utility values of the 2-itemset for
each transaction are: 12 (5×2+1×2) in TId1, 0 in TId2 and TId3 (no matches),
and 7 (5 × 1 + 1 × 2) in TId4.
To eﬃciently extract HUIs, the Transaction-Weighted Utilization (TWU) has
been introduced [12]. It is an over-estimate of the utility of the itemset, which can
be exploited to prune the search space because it satisﬁes the following downward
closure property: given two itemsets I1 and I2 such that I1 ⊂I2, if the TWU of
I1 is below the utility threshold minutil even the TWU of I2 does. The TWU of
an itemset I, denoted as twu(I), is deﬁned as the sum of the transaction utilities
of all the transactions containing I, where the transaction utility of a transaction
is the sum of the utility values of all its items. A formal deﬁnition of the TWU
measure of itemset I follows: twu(I) = 
tj∈D|I⊆tj

i∈tj u(i, tj).

228
L. Cagliero et al.
3
Generalized High-Utility Itemsets
Our goal is to discover HUIs that incorporate knowledge at multiple granularity
levels. To this aim, we generalize items at diﬀerent abstraction levels.
Let T be a taxonomy (i.e., a is-a hierarchy), which aggregates items in I
into higher-level concepts, hereafter denoted as generalized items. Generalized
items represent higher-level categories which group individual items based on
their semantic meaning. Let G be the set of generalized items in T . For the
sake of simplicity, hereafter we will assume that in the given taxonomy T each
item i ∈I is aggregated into exactly one generalized item g ∈G (i.e., each item
belongs to a speciﬁc higher-level category). Generalized items can be further
generalized as other generalized items at higher granularity levels.
For each generalized item g ∈G, Desc(g, T ) ∈I denote the subset of descen-
dant items of g according to the given taxonomy. For our purposes, we formalize
the concept of level of a generalized item g ∈G in the taxonomy, hereafter
denoted as l(g, T ), as the length of the shortest path between g and any leaf
node in the taxonomy. Note that, by construction, the level of non-generalized
itemsets is zero, while the maximum level of an item corresponds to the taxon-
omy height (i.e., the length of the longest path from any node in T to a leaf
node).
Example. Figure 1 depicts an example of taxonomy built on items occurring in
the running example dataset (see Table 1). For instance, items Coke and Water
are aggregated into the generalized item Beverage. The level of Beverage and is
one, whereas the level of Coke, and Water is zero.
A generalized itemset is a set of generalized items in G. Similar to [3,6,11]
we focus our analyses on the combinations of generalized items having the same
level, because they compactly represent information at a given abstraction level.
Herafter we will denote as level of a generalized itemset the level of its items.
For our purposes, we extend the concept of utility to generalized items and
itemsets. Speciﬁcally, the utility of a generalized item g in a transaction tj,
hereafter denoted as u(g, tj), is the sum of the utility values of all the descendant
items, while the utility of g in a transactional dataset is the sum of all the per-
transaction utilities. More formal deﬁnitions follow.
Deﬁnition (Utility of a generalized item). Let g be a generalized item, D a
transactional dataset, and T be a taxonomy. The utility of g in a transaction tj ∈
D is deﬁned as u(g, tj) = 
i∈Desc(g,T )eu(i) · iu(i, tj). The utility of generalized
item g in D is calculated as u(g, D) = 
i∈Desc(g,T ) u(i, D).
□
Similar deﬁnitions hold on itemsets (i.e., sets of items). The utility of a gen-
eralized itemset in a transactional dataset indicates the overall proﬁt of a com-
bination of item categories.
Deﬁnition (Utility of a generalized itemset). Let GI be a generalized
itemset and let D a transactional dataset, and T be a taxonomy. The utility of a

Discovering High-Utility Itemsets at Multiple Abstraction Levels
229
generalized itemset GI in transaction tj is deﬁned as u(GI, tj) = 
g∈GI u(g, tj).
The utility of a generalized itemset GI is in the transactional dataset D is deﬁned
as u(GI, D) = 
tj∈D u(GI, tj), where we assume u(GI, tj) = 0 if GI /∈tj.
□
Example. Let us consider again the market basket dataset reported in Table 1
and the taxonomy in Fig. 1. The per-transaction utility of generalized item Bev-
erage is 10 in transaction with TId1, 3 in transaction with TId2, 2 in transaction
with TId3, and 5 in transaction with TId4. Hence, the utility of Beverage in the
dataset is 20 (10 + 3 + 2 + 5).
In this work we address the extraction of a selection of generalized itemsets,
called Generalized High-Utility Itemsets (GHUIs).
Deﬁnition (Generalized High-Utility Itemset). Let minutil be a (user-
speciﬁed) minimum utility threshold, let D be a transactional dataset, let T be
a taxonomy, and let GI be a generalized itemset. A generalized itemset GI is a
Generalized High-Utility Itemset (GHUI) in D if and only if u(GI, D) > minutil. ⊓⊔
Example. The GHUIs mined from the dataset in Table 1 by enforcing a minimum
utility threshold for generalized itemsets equal to 30 for GHUIs are enumerated
in Table 4.
Given a transactional dataset D, a taxonomy T , and a minimum utility
threshold minutil, the Generalized High-Utility Itemset Mining (GHUIM) prob-
lem addressed by this work entails discovering all the HUIs and GHUIs.
Per-level utility thresholds. The utility of a generalized itemset incorporates
those of all of its descendant itemsets. Hence, itemsets at higher abstraction
levels are more likely to satisfy a ﬁxed minimum utility threshold than lower-
level ones. To overcome this issue, we adapt the utility threshold to the level of
generalization of the considered itemsets. The key idea is to set higher utility
thresholds for itemsets including items at higher abstraction levels. We set the
same utility threshold for all itemsets having the same level. Speciﬁcally, given
a user-speciﬁed least minimum utility threshold minutil associated with non-
generalized itemsets (level = 0), the minimum utility threshold thr(l) associated
with level-l generalized itemsets is minutil if l = 0, thr(l) = α(l) · minutil
otherwise, where α(l): N →[1, +∞) is a monotonically increasing (user-speciﬁed)
function.
4
The ML-HUI Miner Algorithm
To extract Generalized High-Utility Itemsets we propose a new algorithm,
namely Multiple-Level High-Utility Itemset Miner (ML-HUI Miner). The main
features of GHUI-Miner are summarized below.
(a) Taxonomy-driven HUI mining. ML-HUI Miner supports transactional data
enriched with taxonomy information. This allows us to extract patterns at
diﬀerent abstraction levels.

230
L. Cagliero et al.
Algorithm 1. The ML-HUI Miner algorithm
Input:
transactional dataset D, taxonomy T , minimum utility threshold minutil, function α(l)
Output:
O, the set of High-Utility Itemsets and Generalized High-Utility Itemsets satisfying the
per-level utility thresholds
{Initializations}
1: I←set of items in D
2: GI←set of generalized items in T
{Preparation}
3: scan D and T to compute Transaction-Weighted-Utility (TWU) of items in I
4: Compute Transaction-Weighted-Utility of items in GI
5: I∗←set of items in D such that TWU is above minutil(level=0)
6: GI∗←set of generalized items g in T such that TWU is above α·minutil(l(g, T ))
7: Build utility list and Estimated Utility Co-occurrence Structure of (generalized) items in I∗∪GI∗
{Recursive depth-ﬁrst search}
8: O ←Recursive generation of the combinations of (generalized) items in I∗and GI∗whose items
share the same level and selection of all combinations satisfying the utility threshold α(l)·minutil
(b) Single-step extraction of generalized and non-generalized HUIs. The pro-
posed algorithms explores the dataset and the taxonomy to generate
multiple-level patterns in a single phase (i.e., without the need for multiple
runs).
(c) Prevent the generation of uninteresting combinations of items. Similar to [6,
11], we focus on extracting itemsets including only items with the same level.
Thus, GHUI-Miner prevents the generation of itemsets consisting of items
with diﬀerent levels in the taxonomy.
A high-level pseudo-code of the ML-HUI Miner algorithm is given in
Algorithm 1. First, ML-HUI Miner scans the dataset and the taxonomy to
identify the single non-generalized and generalized items whose Transaction-
Weighted-Utility (TWU) is above the per-level utility threshold (Lines 3–6 in
Algorithm 1). To this aim, the taxonomy is explored in a bottom-up fashion. The
dataset and the accessory structures are properly adapted to prevent the gener-
ation of combinations of mixed-level items (according to Point (c). To compute
the per-level utility thresholds, the user-speciﬁed threshold minutil is adjusted
using function α according to the level of each generalized item. Then, we com-
pute the utility list associated with all non-generalized and generalized items
whose TWU satisﬁes the per-level utility threshold (Line 7 in Algorithm 1). The
utility list is a compact data structure that contains for each (generalized) item
i (i) the list of transactions tj such that i ∈tj, (ii) the utility values of the
(generalized) item in each transaction u(i, tj), and (iii) the sum of the utilities of
the remaining items with the same taxonomy level within each transaction, i.e.,

q∈tj∧q̸=i∧l(q,T )=l(i,T) U(q, tj). For our purposes, we extended the utility list
proposed in [8] to integrate information about the generalized items at the same
taxonomy level appearing in the taxonomy. The utility list is provided as input
to the depth-ﬁrst recursive procedure (Line 8 in Algorithm 1), which does not
need to access neither the dataset nor the taxonomy. Speciﬁcally, starting from
single items (generalized and not) the recursive procedure computes their exact
utility value and then explores all their extensions using a depth-ﬁrst strategy
based on the utility list. To avoid generating itemsets consisting of items with

Discovering High-Utility Itemsets at Multiple Abstraction Levels
231
diﬀerent level, extensions are selectively generated. The recursive procedure is
similar to the one adopted by FHM [8].
5
Experiments
To evaluate the performance of the ML-HUI Miner algorithm, we conducted
experiments on four benchmark UCI datasets coming from diﬀerent domains
(Connect, Mushroom, Chess, Retail [4]), which have already been used to
evaluate the performance of recently proposed High-Utility Itemset mining algo-
rithms (e.g., FHM [8]). The number of transactions per dataset varies from 3,196
(Chess) to 67,557 (Retail). The analyzed datasets show diﬀerent characteristics
(e.g., number of distinct items Connect 129, Mushroom 119, Chess 75, Retail
2,741). The external utilities in the datasets range from 1 to 1,000. They were
synthetically generated by using a log-normal distribution. Internal utilities are
uniformly distributed between 1 and 5. To set the per-level utility threshold
values (thr(l)), we deﬁned α(l) = γ · f(l), where f(l) is the average number of
level-0 descendants per level-l item. To generalize items at higher abstraction
levels, we generated taxonomies on top of data items. Speciﬁcally, on the Retail
dataset, which stores real sales of an online store, we generated a 2-level tax-
onomy by aggregating products into the corresponding group provided by the
online store. The 2,741 available products are clustered into 38 produt groups
(e.g., Kitchen, Toy). Products are evenly distributed across product groups. For
instance, Kitchen clusters 1,513 products, while Toy 236. On the other datasets
we generated a synthetic taxonomy where each generalized item aggregates, on
average, 10 randomly selected products. The experiments were performed on a
2.67 GHz Intel Xeon workstation with 32 GB of RAM, running Ubuntu 12.04.
5.1
Performance Analysis
Figure 2 shows the number of mined (non-generalized) HUIs and GHUIs by vary-
ing the values of minutil and γ on a representative dataset (i.e., Chess). For
both types of patterns, the number of mined itemsets is inversely proportional to
the minutil value. By decreasing the values of minutil and γ the total number
of mined patterns super-linearly increases. The number of GHUIs is two orders
of magnitude lower than those of HUIs due to the signiﬁcantly lower number
of possible item combinations at higher taxonomy levels. The increase in the
number of candidate itemsets due to taxonomy integration implies also a time
complexity increase. Figure 3 shows the time spent by the ML-HUI Miner algo-
rithm on the analyzed datasets with decreasing minutil values. Since the value
of γ aﬀects only the extraction of HGUIs, whose number is orders of magnitude
lower than the number of HUIs, varying γ value slightly aﬀects the execution
time. For this reason, we report the execution time only for the representative
value γ = 0.2 on all datasets. Despite a larger number of combinations were
explored, the extraction times remain acceptable (few ms) on all the analyzed
datasets. We compared also the execution times of the newly proposed ML-HUI

232
L. Cagliero et al.
Fig. 2. Chess: number of mined patterns
Fig. 3. Execution time (γ = 0.2)
Miner with those of the FHM algorithm [8], which extracts only non-generalized
HUIs. ML-HUI Miner execution time approximately doubles that of FHM, even
when more than two aggregation levels are integrated in the taxonomy.
5.2
Knowledge Discovery
We present some example GHUIs mined from the Retail dataset (minutil =
10000, γ = 0.2). The GHUIs with length 1 reveal the most proﬁtable product
groups. For example, Kitchen is the category with maximal utility. Toy ranked
second (utility gap from Kitchen to Toy 87%), Home is the third (92%), Oﬃce
product ranked 4th (96%) and Law & Patio 5th (96%). For all the remaining cat-
egories the utility gap with respect to Kitchen was above 97%. The GHUIs with
length greater than 1 point out combinations of groups that yielded high proﬁts
when the respective products were sold together. Let us consider, for instance,
GHUI {Musical-Instruments, Home}. It indicates that the jointly sale of prod-
ucts in these categories provided a high income. Among GHUIs including group
Musical-Instruments, GHUIi = {Musical-Instruments, Kitchen} is the one with
highest utility. The other GHUIs in order of decreasing utility are: {Musical-
Instruments, Toy} (utility −75% w.r.t. GHUIi), {Musical Instruments, Home}
(utility −91% w.r.t. GHUIi), and {Musical-Instruments, Oﬃce-Product} (utility
−94% w.r.t. GHUIi). These patterns can can be exploited to ﬁgure out which
categories of products should be promoted in the same advertising campaign,

Discovering High-Utility Itemsets at Multiple Abstraction Levels
233
e.g., while planning a campaign on products belonging to Musical Instrument,
products of Kitchen, Toy, Home or Oﬃce-Product category should be advertised
as well. The utility value provided us a ranking of the most appealing groups
of products to advertise together with products of Musical Instrument. GHUIs
provide high-level knowledge related to single products that do not satisfy the
utility threshold. Let us consider again group Musical Instrument. Only two
non-generalized HUIs were extracted, i.e., {Red-Harmonica} (utility 26,205) and
{Blue-Harmonica} (utility 10,271). However, the utility of the GHUI {Musical-
Instruments} is 40,741. Hence, the utility value of {Musical-Instruments} is not
only due to the {Red-Harmonica} and {Blue-Harmonica} products, but even
to other products of the same group that do not satisfy the utility threshold.
Considering GHUI {Musical-Instruments} allows us to consider also the con-
tribution of the other products, even if their single proﬁts are averagely low.
Moreover, GHUIs represent correlations among products that can not be easily
inferred while considering only HUIs. For instance, even if correlations between
products of group {Musical-Instruments} and products of other groups have
not been extracted, the high-lever correlations between Musical Instrument and
other groups were extracted and can be analyzed.
6
Conclusions and Future Work
We proposed a new pattern, called GHUI, which represents sets of items groups,
each one characterized by a high total proﬁt. The signiﬁcance of the proposed
pattern and the performance of the proposed GHUI mining algorithm have been
evaluated on retail data, with the goal of planning advertising campaigns of retail
products. Future extensions of the work will address the eﬃcient extraction of
signiﬁcant subsets of GHUIs (e.g., closed or minimal GHUIs).
References
1. Agrawal, R., Imielinski, T., Swami, A.: Mining association rules between sets of
items in large databases. In: ACM SIGMOD 1993, pp. 207–216 (1993)
2. Baralis, E., Cagliero, L., Cerquitelli, T., D’Elia, V., Garza, P.: Expressive general-
ized itemsets. Inf. Sci. 278, 327–343 (2014)
3. Cagliero, L.: Discovering temporal change patterns in the presence of taxonomies.
IEEE Trans. Knowl. Data Eng. 25(3), 541–555 (2013)
4. Fournier-Viger, P., Gomariz, A., Gueniche, T., Soltani, A., Wu, C.-W., Tseng,
V.S.: SPMF: a Java open-source pattern mining library. J. Mach. Learn. Res.
15(1), 3389–3393 (2014)
5. Fournier-Viger, P., Zida, S., Lin, J.C., Wu, C., Tseng, V.S.: Eﬃcient closed high-
utility itemset mining. In: Proceedings of the 31st Annual ACM Symposium on
Applied Computing, Pisa, Italy, pp. 898–900, 4–8 April 2016
6. Han, J., Fu, Y.: Discovery of multiple-level association rules from large databases.
In: VLDB Conference, pp. 420–431 (1995)
7. Krishnamoorthy, S.: Pruning strategies for mining high utility itemsets. Expert
Syst. Appl. 42(5), 2371–2381 (2015)

234
L. Cagliero et al.
8. Lin, J.C., Fournier-Viger, P., Gan, W.: FHN: an eﬃcient algorithm for mining
high-utility itemsets with negative unit proﬁts. Knowl. Based Syst. 111, 283–298
(2016)
9. Liu, J., Wang, K., Fung, B.C.M.: Direct discovery of high utility itemsets without
candidate generation. In: 12th IEEE ICDM Conference, pp. 984–989, December
2012
10. Liu, Y., Liao, W., Choudhary, A.: A two-phase algorithm for fast discovery of
high utility itemsets. In: Ho, T.B., Cheung, D., Liu, H. (eds.) PAKDD 2005.
LNCS (LNAI), vol. 3518, pp. 689–695. Springer, Heidelberg (2005). doi:10.1007/
11430919 79
11. Srikant, R., Agrawal, R.: Mining generalized association rules. In: VLDB 1995, pp.
407–419 (1995)
12. Tseng, V.S., Shie, B.-E., Wu, C.-W., Yu, P.S.: Eﬃcient algorithms for mining
high utility itemsets from transactional databases. IEEE Trans. Knowl. Data Eng.
25(8), 1772–1786 (2013)
13. Tseng, V.S., Wu, C.W., Fournier-Viger, P., Yu, P.S.: Eﬃcient algorithms for mining
top-k high utility itemsets. IEEE TKDE 28(1), 54–67 (2016)

Automatic Organization of Semantically Related
Tags Using Topic Modelling
Iman Saleh(B) and Neamat El-Tazi
Faculty of Computers and Information, Cairo University, Giza, Egypt
{iman.saleh,n.eltazi}@fci-cu.edu.eg
Abstract. The use of social media platforms such as social networks,
weblogs and community question answering websites increased largely in
the last few years. This increase in usage contributed to the vast explosion
in available online content. Some of these platforms use social tagging,
tags manually inserted by content authors, as a way to facilitate content
description and discovery. In this paper, our goal is to automatically
group semantically related tags in order to organize the large amount
of tags contributed by various users. Our approach is based on using
a topic model to discover topics of documents, and then grouping top
tags related to documents assigned to each topic. We perform a set of
experiments using diﬀerent number of topics to provide diﬀerent levels of
details for the generated tag groups. The dataset used in our experiments
is extracted from Stack Overﬂow, a community question answering web-
site used by programming professionals. Tag groups generated by our
technique are presented and evaluated.
Keywords: Stack overﬂow · Tags · Topic modelling · LDA · Organize
tags
1
Introduction
The amount of data available online in social media platforms is increasing
largely. This large amount of data contains hidden information that need to
be extracted. A way of organizing and categorizing this massive amount of data
is to use tags inserted by users. However, the number of tags can increase largely
over time. It is necessary in such a case to organize tags themselves to help users
discover hidden topics in the data. In our work we are interested in Stack Over-
ﬂow community question answering platform and how to organize its tags.
Stack Overﬂow is the largest online community question answering website
designed to help developers share their programming knowledge. It allows them
to post programming related questions and receive answers to them. The website
contains1 around 13.4 million questions tagged with around 48,228 tags. Ques-
tions are manually tagged by users and revised by privileged users. Tags in Stack
Overﬂow are used in research studies to predict tags of posts [10,16–18,22,25],
1 All numbers mentioned in the paper date back to March 2017.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 235–245, 2017.
DOI: 10.1007/978-3-319-67162-8 23

236
I. Saleh and N. El-Tazi
map tags to Wikipedia concepts [11], suggest and group tag synonyms [3,4], ana-
lyze trends of topics [2] and mine challenges encountered by developers [20]. Tags
are important because they help categorize posts and hence do proper analysis
of Stack Overﬂow content. The large number of tags in Stack Overﬂow makes
it important to organize them into meaningful groups to be able to browse and
analyze Stack Overﬂow content easily.
In this paper we introduce an automatic method to organize Stack Overﬂow
tags. Our method is based on using Latent Dirichlet Allocation (LDA) topic
model [5] to group semantically related tags. We ﬁrst categorize a sample of
posts in Stack Overﬂow into a number of topics. Each topic is assigned a set of
key phrases that serve as a title for the topic using Open Calais tool2 to provide
a meaningful title for each group of words produced by LDA. Then we extract
top tags assigned to the posts of each topic and consider them a group of related
tags. We do some ﬁltering on groups of tags to remove tags that are common
among most of the topics. In order to evaluate how accurate tag groups are, we
introduce a method inspired from word intrusion task in [6]: tag intrusion. This
method is used to measure how semantically coherent tags are and whether they
correspond to natural grouping for humans. We perform a set of experiments
using diﬀerent number of topics and provide proper evaluation of our technique.
The following is a summary of the contributions introduced by this paper:
– An automatic method to group Stack Overﬂow tags into semantically mean-
ingful topics using LDA topic model (Sect. 3).
– A formal evaluation technique for our method (Sect. 3.4).
2
Related Work
Social tags in social media platforms were utilized in diﬀerent contexts. [23]
consider Twitter hashtags as topics and use them to automatically generate an
overall sentiment polarity for a given hashtag. [7] utilize Twitter hashtags as an
indicator of events. They use hashtags to discover breaking events. Hashtags are
considered topics in Twitter as explained in [13] and used to predict the popu-
larity of a topic. Semantic relationship between Twitter hashtags was explored
in [8]. They construct a graph of hashtags and entities drawn from tweets such
that edges capture semantic relatedness. A predictive model that incorporates
the eﬀect of time on individual hashtag reuse and social hashtag reuse was intro-
duced in [12]. Their goal is to introduce a hashtag recommendation algorithm
that considers time dimension in its recommendations. Stack Overﬂow and Twit-
ter are similar in using tags to deﬁne topics. However, Twitter tags are much
more diverse and changes frequently overtime unlike Stack Overﬂow tags which
are more stable over time.
Grouping Stack Overﬂow tags was previously studied in [3]. Their approach
was mainly focused on ﬁnding tags that are synonyms to each other. They used
some strategies to achieve their goal based on studying the set of tag synonyms
2 http://www.opencalais.com/.

Automatic Organization of Semantically Related Tags
237
that is provided by Stack Overﬂow. Their work focused only on android related
tags. They further extended their work to group tag synonyms using graph
clustering techniques [4]. In our work we are interested in similar tags in a
broader sense and not only tags that are synonyms to each other. Working on tag
synonyms only will result in a large number of tag groups that cannot be browsed
easily. [9] used agglomerative clustering technique to group Stack Overﬂow tags.
They only considered tags themselves and did not use Stack Overﬂow posts in
their experiments. We introduce an alternative method that makes use of Stack
Overﬂow posts and proper evaluation of our method. [21] also used k-medoids
clustering in order to infer semantically related software tags. They measure
the similarity of two tags based on the number of documents tagged by both
of them and the similarity of textual content of documents tagged by them.
However, the dataset they used is much smaller than our dataset and we believe
their approach might not scale well to a large number of tags. [19] used Stack
Overﬂow posts to build a software speciﬁc resource similar to WordNet. Their
approach is based on the textual content of the posts and not their tags. A
large taxonomy of subsumption relations between pairs of Stack Overﬂow tags
was created using machine learning in [26]. They use several features such as
lexical tags features and features extracted from tags wiki description. Their
work focuses mainly on discovering relationships between tag pairs instead of
creating tag groups. Finding trends of topics in Stack Overﬂow was explored in
[2]. They also used LDA to ﬁnd topics of Stack Overﬂow posts. Clusters of tags
under each topic are manually created and referred to as technologies. Trends
of topics and technologies are then extracted over time. In our work we want
to automate the process of grouping tags under a certain topic. We also think
that tag trends themselves carry important information in addition to topic or
technology trends. Our analysis is based on tags themselves and how to compare
between them. [20] also used LDA to ﬁnd topics in Stack Overﬂow related to
diﬀerent Web API’s. Their goal is to ﬁnd common challenges encountered by
developers. They investigate how topics related to diﬀerent Web API’s evolve
over time. In our work we are interested in all topics in Stack Overﬂow rather
than speciﬁc Web API’s. [1] used LDA to mainly analyze problems faced by
developers in Stack Overﬂow. They also provide an analysis of Stack Overﬂow
usage in week days.
3
Grouping Stack Overﬂow Tags Using LDA
LDA is a generative probabilistic model that is used to model text corpora.
It is based on (1) representing documents as a random mixture over k latent
topics (2) modeling topics as distributions over words. In this model, topics are
groups of semantically related words in a corpus of documents. For example, a
group of words like “entity, model, database, query, mongodb” are semantically
related and refer to concepts related to database and data management. Also,
a document similar to the one shown in Fig. 1 is clearly about git and version
control systems. So it belongs to the topic represented by the following group

238
I. Saleh and N. El-Tazi
of words: “git, branch, repository, svn, repo, github”. We also notice that the
document is labeled with the following tags: “git, alias, git-push”. Therefore
these three tags can be grouped together since they are semantically related and
refer to the same topic.
Title: Why does git push origin @ not work?
Post: We can push the head of a branch like below
$ git push origin HEAD
And, we can use @ for alias of HEAD.
$ git show @
Then why does the below command gives me an error?
$ git push origin @
fatal: remote part of refspec is not a valid name in @
Tags: git, alias, git-push
Fig. 1. An example of a stack overﬂow post
In our work we are interested in creating tag groups that are semantically
related in order to organize Stack Overﬂow massive content. Working towards
this end, we train LDA using a subset of Stack Overﬂow posts, extract tags
assigned to each post, and then associate these tags with the most likely topic
assigned to the post. Details of our method are explained in the following sub-
sections.
3.1
Training LDA Model
We use MALLET [15] implementation of LDA to model a Stack Overﬂow posts
dump3. When we investigated posts we found that some of them are of low
quality and create noise in our model. Therefore we ﬁltered data to include only
a subset of posts that are assigned a score greater than or equal to 5. The score of
a post is assigned by Stack Overﬂow community users as the diﬀerence between
the number of upvotes and downvotes.
We split the dataset into 2 folds to make sure our technique is valid for Stack
Overﬂow data. The ﬁrst fold, DS1, consists of 463,983 posts while the second,
DS2, contains 360,610 posts. Data in each fold was selected randomly ﬁrst then
ﬁltered based on score. Code snippets were removed and tags of each post were
included in training data. Best answer for a post is also included if available.
We used Stanford parser [14] to stem data and use only noun words found in a
post. LDA has three parameters: k, alpha and beta. The ﬁrst parameter is the
number of topics. The second parameter, alpha, controls the mixture of topics
in a document; whether a document is likely to contain a mixture of most topics
rather than only speciﬁc few topics. The third parameter, beta, controls the
mixture of words in a topic; whether a topic is likely to contain most words
3 Dump downloaded in October 2016.

Automatic Organization of Semantically Related Tags
239
rather than only speciﬁc words. We train LDA using 10, 30, 50 and 80 topics
and compare between the four values. Alpha value used in our work is 50/k and
beta value is 200/V, as recommended in [24], where V is the vocabulary size.
Vocabulary size for DS1 is 184,147, and 143,023 for DS2. The model is trained
using 2000 iterations since this is a reasonable number for the model to converge
and ﬁnd good topics.
3.2
Assigning Titles to Each Topic
Topics generated by LDA are mainly groups of words that are semantically
related. To generate topics that are meaningful while browsing tag groups, we
need to have a title for each group of words; or a set of phrases that describe
the topic represented by those words. In order to generate a title for each topic,
we use the top 20 words of each topic as an input to Open Calais API, a tool
used for tagging text. For each topic, a set of tags and their relevance to input
text are produced (as shown in Table 1). These tags can be used as topics titles.
Tags common among all topics such as “computing”, “software” and “software
engineering” are excluded from the set of title tags.
Table 1. Open calais tags of topic15 and topic27 and their relevance to top words
Topic index Topic top 20 words
Open calais tags
15
css, html, div, tag, width, image,
browser, style, height, content,
javascript, jquery, bootstrap, size,
font, svg, background, selector,
chrome, border
JavaScript libraries 66%, HTML
66%, Ajax 66%, Bootstrap 66%,
Open formats 66%, JQuery
66%, Cascading Style Sheets
66%, HTML element 66%, Web
design 66%
27
table, sql, database, column, query,
row, mysql, server, record, index,
statement, oracle, postgresql,
sqlite, procedure, key, insert,
transaction, order, clause
Data management 100%, SQL
keywords 66%, Database
management systems 66%,
Database trigger 66%, MySQL
66%, Databases 66%,
PostgreSQL 66%, Join 66%,
Insert 66%
3.3
Creating Tag Groups Using LDA
LDA model assigns a mixture of topics to each document. For each topic, we
collect all posts that most likely belong to it. Then we extract tags assigned to
these posts. We refer to the tag group of topici produced using an LDA model
trained with k topics as TTGk
i . For each TTGk
i , we count how many times a
tag appears associated with that topici. Each TTGk
i is then assigned the top 5%
tags according to frequency of occurrence in topic posts. An example of top tags
associated with topic15 (Table 1) is shown in Table 2.

240
I. Saleh and N. El-Tazi
Table 2. TTG30
15 top tags and their fre-
quency of occurrence
Tags in TTG30
15
Frequency
css
10,462
html
7,292
javascript
3,645
jquery
3,079
css3
2,087
twitter-bootstrap
1,454
html5
1,271
Table 3. topic15 top tags that appear
in 50% or less of TTG30
i
Tags in UTG30
15
css
css3
twitter-bootstrap
html5
svg
google-chrome
css-selectors
However, grouping tags is not a straightforward task. We notice that some
tags appear in all topics or most of them. For instance, javascript tag can appear
in topics related to database, version control systems, android as well as regular
expressions. Therefore, a tag that appears in most or all of the topics cannot be
assigned to a speciﬁc group of tags since it is shared among topics. Therefore,
we create a group of tags called “shared tags”. We refer to shared tags group
created using an LDA model trained with k topics as STGk. An STGk group
contains tags that appear in 50% or more of all TTGk
i . For instance, when k = 10,
if a tagi is found in 5 or more TTGk
i groups, it is a shared tag. Table 4 shows a
sample of shared tags produced by LDA model with k = 10, 30, 50 and 80 and
appeared in more than 50% of all TTGk
i . We also create another group of tags
for each topic that contains the remaining tags. These are tags that are either
unique to a topic or appear in less than 50% of all TTGk
i . We refer to the group
of unique tags of a topici created using an LDA model trained with k topics as
UTGk
i . Table 3 shows tags in UTG30
15.
Table 4. Example of shared tags extracted using LDA model trained with k topics
100% of topics
90% or more and less than 100%
STG10 android, asp.net, c++, c#, cocoa,
ﬁle, ios, iphone, java, javascript,
jquery, json, .net, objective-c, php,
python, ruby, ruby-on-rails, string,
visual-studio, performance,
visual-studio-2010
arrays, angularjs, api, delphi,
eclipse, c, css, html, json, node.js,
osx, qt, r, unit-testing, vb.net,
windows, winforms, wpf, xml
STG30 c#, java, javascript, python, .net
android, c++, ios, php, python,
ruby
STG50 c#, java
javascript, .net, php, python
STG80 N/A
c#, java, javascript, python

Automatic Organization of Semantically Related Tags
241
Table 5. Tag intrusion results for datasets DS1 and DS2
k = 10 k = 30 k = 50 k = 80
DS1 0.73
0.80
0.81
0.83
DS2 0.77
0.79
0.76
0.80
3.4
Evaluating Tag Groups
An appropriate quantitative measure is needed to evaluate extracted tag groups.
It should be able to measure the coherence of tags in a tag group and whether
they are actually semantically related. In order to achieve this goal, we use tag
intrusion, a measure inspired from word intrusion task explained in [6]. Word
intrusion task measures semantic coherence of top words in an LDA topic. This
measure was shown to capture aspects of LDA models that are not captured by
other measures of model quality based on held out likelihood. So we believe that
a similar measure can be useful in assessing semantic coherence of tag groups.
In word intrusion task, top words of a topici produced by an LDA model are
selected. A word wintruder is then selected randomly from the top words of
another topicj and included together with the set of top words of topici. wintruder
must not be present in the top words of topici. Annotators are then asked to ﬁnd
the word that does not belong to the group. In our case we call the evaluation
task tag intrusion. We aim at ﬁnding how cohesive tag groups are, i.e. degree of
relatedness between tags in a tag group UTGk
i . We create the task as follows:
given an LDA model trained with k topics, for each UTGk
i , the top ﬁve tags
are selected. Then another random UTGk
j is selected. Next, we randomly choose
a tag tagintruder from the top 3 tags in UTGk
j such that tagintruder /∈UTGk
i .
We count how many times the intruder tag was selected correctly by 3 diﬀerent
annotators and use the percent as a measure of tag groups cohesion. Given that
taganswer is the intruder tag selected by annotator s and S is the total number of
annotators, Eq. 1 shows how we calculate tag intrusion. Table 5 shows the results
of tag intrusion using tag groups extracted from LDA trained with k topics equal
to 10, 30, 50 and 80.
TagIntrusionk = (
s(k
i=1(tagintruder = taganswer)/k))
S
(1)
4
Discussion
The results of tag intrusion (Table 5) show that our method achieves at least
73% score for tag intrusion. We used two diﬀerent sets in order to validate our
results and notice that the diﬀerence between scores for both is less than or
equal to 5% which indicates our algorithm generalizes well on Stack Overﬂow
data. Also, shared tags and tags under speciﬁc topic groups are almost the same
for both datasets as shown in Table 6. Tag groups for “Regular Expressions and

242
I. Saleh and N. El-Tazi
Character Encoding” and “Version control software” topics extracted from both
datasets are almost the same. The table also shows that shared tags extracted
from both datasets are very similar.
Table 6. Tag groups sorted based on frequency of appearance with topic, LDA k = 30
DS1
DS2
Regular
expressions
and character
encoding
regex, string, unicode, utf-8,
parsing, perl, encoding, bash,
character-encoding, split
regex, string, unicode, utf-8,
perl, encoding, bash,
character-encoding, split,
replace
Version control
software
git, svn, github, version-control,
mercurial, jenkins, branch,
merge, tortoisesvn, tfs
git, svn, github,
version-control, mercurial,
branch, merge, tortoisesvn,
docker, jenkins
Tags in ST30
appearing in
all topics
javascript, java, c#, python
Java, c#, .net, python
The number of topics does not have a signiﬁcant eﬀect on tag intrusion
score. We think that the choice of the number of topics depends on whether
we are interested in high level or low level topics. The lower the number of
LDA topics, the more diverse tags will be grouped under a single topic. This
is clear in Table 4 were the number of shared tags is inversely proportional to
the number of LDA topics. For example, “regex” tag is considered as a shared
tag for k = 10. When k is increased to 30, the tag appears as the most frequent
tag under a speciﬁc topic: “character encoding and regular expressions”. Also,
as the number of topics increases, some topics are close to each other and its
hard to distinguish between them. For instance, when we set k = 80, two topics
produced very similar top tags as shown in Table 7. We think that organizing
tags into groups is particularly useful to explore tag trends and how they change
over time. A user can ﬁrst choose a topic of interest and then browse tags trends
under a topic. This can also facilitate comparing trends of two tags over time.
For example, frequent tags include “javascript, java, c#”. Then “ios” comes
next with lower frequency. Non-frequent tags, compared to the previous tags,
include “objective-c”, “scala”. We ﬁnd this result some how consistent with the
frequency of questions in which these tags appear as shown in Table 84.
4 Numbers obtained from http://StackOverﬂow.com/tags.

Automatic Organization of Semantically Related Tags
243
Table 7. Similar tag groups produced using LDA trained with k=80
Tag group Topic titles
Top tags
UTG80
64
Smartphones, IOS SDK, Integrated
development environments, Xcode,
Tablet computers, IPad, Swift,
GPS navigation devices, Cocoa
xcode, swift, cocoa, osx, xcode6,
cocoa-touch, core-data, ipad,
ios8, xcode4
UTG80
36
IOS, IOS SDK, TvOS, Xcode,
Swift, IPhone
uitableview, swift, cocoa-touch,
xcode, ipad, ios8, ios7, cocoa,
uiview
Table 8. Frequencies of top shared tags in stack overﬂow
javascript java
c#
ios
objective-c scala
Freq. 1.3 m
1.2 m 1.0 m 481.3 k 273.9 k
63.1 k
5
Conclusion and Future Work
In this paper we introduced an automatic method to organize Stack Overﬂow
tags. We use LDA to group tags that are semantically similar. We evaluated our
method using tag intrusion score; a measure inspired from word intrusion that is
used to evaluate LDA. We plan to extend our work by ﬁnding better methods to
group Stack Overﬂow tags based on semantic similarity. A comparison between
our method and other methods used to organize social tags needs to be performed
as well. Finally, we plan to launch a demo to visualize our results.
Acknowledgements. This work is funded by Information Technology Academia
Collaboration program organized by Information Technology Industry Development
Agency in Cairo, Egypt. We thank Tarek Nabhan, Ahmed Hany and Noura Hassan for
their guidance.
References
1. Allamanis, M., Sutton, C.: Why, when, and what: analyzing stack overﬂow ques-
tions by topic, type, and code. In: Proceedings of the 10th Working Conference on
Mining Software Repositories, pp. 53–56. IEEE Press (2013)
2. Barua, A., Thomas, S.W., Hassan, A.E.: What are developers talking about? an
analysis of topics and trends in stack overﬂow. Empir. Softw. Eng. 19(3), 619–654
(2014)
3. Beyer, S., Pinzger, M.: Synonym suggestion for tags on stack overﬂow. In: Proceed-
ings of the 2015 IEEE 23rd International Conference on Program Comprehension,
ICPC 2015, pp. 94–103. IEEE Press (2015)
4. Beyer, S., Pinzger, M.: Grouping android tag synonyms on stack overﬂow. In:
Proceedings of the 13th International Conference on Mining Software Repositories,
MSR 2016, pp. 430–440. ACM (2016)
5. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn.
Res. 3(Jan), 993–1022 (2003)

244
I. Saleh and N. El-Tazi
6. Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J.L., Blei, D.M.: Reading tea
leaves: how humans interpret topic models. In: Advances in Neural Information
Processing Systems, pp. 288–296. Curran Associates, Inc. (2009)
7. Cui, A., Zhang, M., Liu, Y., Ma, S., Zhang, K.: Discover breaking events with
popular hashtags in twitter. In: CIKM (2012)
8. Ferragina, P., Piccinno, F., Santoro, R.: On analyzing hashtags in twitter. In:
ICWSM (2015)
9. Gajduk, A., Madjarov, G., Gjorgjevikj, D.: Intelligent tag grouping by using an
aglomerative clustering algorithm. In: The 10th Conference for Informatics and
Information Technology CIIT (2013)
10. Gruetze, T., Krestel, R., Naumann, F.: Topic shifts in stackoverﬂow: ask it like
socrates. In: M´etais, E., Meziane, F., Saraee, M., Sugumaran, V., Vadera, S. (eds.)
NLDB 2016. LNCS, vol. 9612, pp. 213–221. Springer, Cham (2016). doi:10.1007/
978-3-319-41754-7 18
11. Joorabchi, A., English, M., Mahdi, A.E.: Automatic mapping of user tags to
wikipedia concepts: the case of a q&a website-stackoverﬂow. J. Inf. Sci. 41, 570–583
(2015)
12. Kowald, D., Pujari, S.C., Lex, E.: Temporal eﬀects on hashtag reuse in twitter: a
cognitive-inspired hashtag recommendation approach. In: WWW (2017)
13. Ma, Z., Sun, A., Cong, G.: On predicting the popularity of newly emerging hashtags
in twitter. JASIST 64, 1399–1410 (2013)
14. Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J.R., Bethard, S., McClosky,
D.: The stanford coreNLP natural language processing toolkit. In: ACL (System
Demonstrations), pp. 55–60 (2014)
15. McCallum, A.K.: Mallet: a machine learning for language toolkit (2002). http://
mallet.cs.umass.edu
16. Romero, J.J.F., Guerrero, M.G., Calder, F.: Multi-class multi-tag classiﬁer sys-
tem for stackoverﬂow questions. In: 2015 IEEE International Autumn Meeting on
Power, Electronics and Computing (ROPEC), pp. 1–6. IEEE (2015)
17. Saha, A.K., Saha, R.K., Schneider, K.A.: A discriminative model approach for
suggesting tags automatically for stack overﬂow questions. In: Proceedings of the
10th Working Conference on Mining Software Repositories, pp. 73–76. IEEE Press
(2013)
18. Stanley, C., Byrne, M.D.: Predicting tags for stackoverﬂow posts. In: 12th Inter-
national Conference on Cognitive Modelling, pp. 414–419 (2013)
19. Tian, Y., Lo, D., Lawall, J.: Automated construction of a software-speciﬁc word
similarity database. In: 2014 Software Evolution Week - IEEE Conference on Soft-
ware Maintenance, Reengineering and Reverse Engineering (CSMR-WCRE), pp.
44–53 (2014)
20. Venkatesh, P.K., Wang, S., Zhang, F., Zou, Y., Hassan, A.E.: What do client
developers concern when using web APIs? an empirical study on developer forums
and stack overﬂow. In: IEEE International Conference on Web Services (ICWS),
pp. 131–138. IEEE (2016)
21. Wang, S., Lo, D., Jiang, L.: Inferring semantically related software terms and
their taxonomy by leveraging collaborative tagging. In: International Conference
on Software Maintenance, pp. 604–607. IEEE Computer Society (2012)
22. Wang, S., Lo, D., Vasilescu, B., Serebrenik, A.: EnTagRec: an enhanced tag rec-
ommendation system for software information sites. In: Proceedings of the 2014
IEEE International Conference on Software Maintenance and Evolution, pp. 291–
300. IEEE Computer Society (2014)

Automatic Organization of Semantically Related Tags
245
23. Wang, X., Wei, F., Liu, X., Zhou, M., Zhang, M.: Topic sentiment analysis in
twitter: a graph-based hashtag sentiment classiﬁcation approach. In: CIKM (2011)
24. Wood, J.: Source-LDA: Enhancing probabilistic topic models using prior knowledge
sources. CoRR abs/1606.00577 (2016)
25. Xia, X., Lo, D., Wang, X., Zhou, B.: Tag recommendation in software informa-
tion sites. In: Proceedings of the 10th Working Conference on Mining Software
Repositories, pp. 287–296. IEEE Press (2013)
26. Zhu, J., Shen, B., Cai, X., Wang, H.: Building a large-scale software program-
ming taxonomy from stackoverﬂow. In: The 27th International Conference on Soft-
ware Engineering and Knowledge Engineering, pp. 391–396. KSI Research Inc. and
Knowledge Systems Institute Graduate School (2015)

Fuzzy Recommendations
in Marketing Campaigns
S. Podapati1, L. Lundberg1, L. Skold2, O. Rosander1,
and J. Sidorova1(&)
1 Department of CS and Engineering,
Blekinge Institute of Technology, Karlskrona, Sweden
julia.a.sidorova@gmail.com
2 Telenor, Stockholm, Sweden
Abstract. The population in Sweden is growing rapidly due to immigration. In
this light, the issue of infrastructure upgrades to provide telecommunication
services is of importance. New antennas can be installed at hot spots of user
demand, which will require an investment, and/or the clientele expansion can be
carried out in a planned manner to promote the exploitation of the infrastructure
in the less loaded geographical zones. In this paper, we explore the second
alternative. Informally speaking, the term Infrastructure-Stressing describes a
user who stays in the zones of high demand, which are prone to produce service
failures, if further loaded. We have studied the Infrastructure-Stressing popu-
lation in the light of their correlation with geo-demographic segments. This is
motivated by the fact that speciﬁc geo-demographic segments can be targeted
via marketing campaigns. Fuzzy logic is applied to create an interface between
big data, numeric methods for its processing, and a manager who wants a
comprehensible summary.
Keywords: Intelligent data mining  Call detail records  Fuzzy membership
function  Geo-demographic segments  Marketing
1
Introduction
In the era of big data a mapping is desired from multitudes of numeric data to a useful
summary and insights expressed in a natural language yet with a mathematical pre-
cision [1]. Fuzzy logic bridges from mathematics to the way humans reason and the
way the human world operates. Clearly, the “class of all real numbers which are much
greater than 1,” or “the class of beautiful women,” or “the class of tall men,” do not
constitute classes or sets in the usual mathematical sense of these terms. Yet, “the fact
remains that such imprecisely deﬁned notions play an important role in human
thinking, particularly in the domains of decision-making, abstraction and communi-
cation of information” [2]. Few works exist in business intelligence that use fuzzy logic
due to certain inherent difﬁculties of creating such applications, and yet; despite them,
such applications are possible and very useful, e.g. the reader can be referred to a
review [3]. The challenges include the following. Firstly, not every problem permits
trial and error calibration of threshold values. Secondly, the operators, membership
© Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 246–256, 2017.
DOI: 10.1007/978-3-319-67162-8_24

functions and inference methods need to have tangible meanings, which can be very
context-dependent. Thirdly, fuzzy theory is a borderline discipline with psycholin-
guistics, which is less objective than formal sciences (such as logic or set theory) and
may require yet unavailable knowledge about human cognition. The notion of fuz-
zieness has distinct understandings and there are important consequences of those
discrepancies, e.g. [4] is a review of theoretical models and their empirical validations.
The main two types of fuzzy technology are fuzzy knowledge based systems [3] and
fuzzy clustering [5]. Our idea is neither of the two, and it aims to implement the above
mentioned insight by Zadeh about completing a useful summary from multitudes of
data. Fuzzy logic enables us to formulate a natural language interface between big data,
numeric analytics, and a manager, hiding the compexity of data and methods and
providing him/her with a comprehensible summary. We summarize data using lin-
guistic hedges (very, rather, highly) and formulate queries such as “Tell me which
neighbourhoods are safe to target, if I want more clients but my infrastructure is highly
loaded”. “Tell me, whether the infrastructure is rather loaded or highly loaded in the
region.” Our speciﬁc application is targeting different user segments to ﬁll in the spare
capacity of the network in a network-friendly manner. In [6], the notion of Infras-
tructure-Stressing (IS) Client was proposed together with the method to reveal such
clients from the customer base. Informally, IS clients use the infrastructure in a
stressing manner, such as always staying in the zones of high demand, where the
antennas are prone to service failures, if further loaded. Being IS is not only a function
of the user’s qualities, but also of the infrastructure, and of the relative mobility of the
rest of the population.
For marketing campaigns geodemographic segmentations (like ACORN or
MOSAIC) are used, since it is known how the segments can be targeted to achieve the
desired goal, as for example, the promotion of a new mobile service in certain neig-
bourhoods. The client’s home address determines the geodemographic category.
People of similar social status and lifestyle tend to live close [7, 8]. Geodemographic
segmentation provides a collective view point, where the client is seen as a represen-
tative of the population who live nearby. However, in recent research, it has been
shown that the problem of resource allocation in the zones with nearly overloaded and
underloaded antennas is better handled relying on individual modelling based on the
client’s historical trajectories [9]. The authors completed a user segmentation based on
clustering of user trajectories and it was demonstrated that network planning is more
effective, if trajectory-based segments are used instead of geo-demographic segments.
Our aim is to explore the ways to connect the individual trajectory-based view on IS
customers and the geodemographic view in order to devise analytics capable to
complete the efﬁcient analysis based on the individual view point and yet be useful in
marketing campaigns in which geodemographic groups are targeted. As a practical
conclusion, we have compiled a ranked list of the segments according to their
propensity to contain IS clients and crafted two queries:
Fuzzy Recommendations in Marketing Campaigns
247

1. Which segments contain a low or moderate number of IS clients? (target them,
while the infrastructure is still rather underloaded)
2. Which segment is highly devoid of IS clients? (target them, when the customer base
becomes mature and the infrastructure becomes increasingly loaded).
The simulation of the resulting fuzzy recommendations guarantees the absence of
false negatives, such as, concluding that certain segments are safe to hire from, but in
fact that would lead to a service failure at some place in the network.
The rest of the paper is organised as follows. Section 2 describes the data set. In
Sect. 3 the proposed methodology is explained. In Sect. 4, the experiments are
reported, and ﬁnally the conclusions are drawn and discussion is held in Sect. 5.
2
Data Set
The study has been conducted on anonymized geospatial and geodemographic data
provided by a Scandinavian telecommunication operator. The data consist of CDRs
(Call Detail Records) containing historical location data and calls made during one
week in a mid-size region in Sweden with more than one thousand radio cells. Several
cells can be located on the same antenna. The cell density varies in different areas and
is higher in city centers, compared to rural areas. The locations of 27010 clients are
registered together with which cell serves the client. The location is registered every
ﬁve minutes. During the periods when the client does not generate any trafﬁc, she does
not make any impact on the infrastructure and such periods of inactivity are not
included in the resource allocation analysis. Every client in the database is labeled with
her MOSAIC segment. The ﬁelds of the database used in this study are:
• the cells IDs with the information about which users it served at different time
points,
• the location coordinates of the cells,
• the time stamps of every event (5 min resolution),
• the MOSAIC geodemographic segment for each client, and
• the Telenor geodemographic segment for each client.
There are 14 MOSAIC segments present in the database; for their detailed
description the reader is referred to [15]. The six in-house Telenor segments were
developed by Telenor in collaboration with InsightOne, and, to our best knowledge,
though not conceptually different from MOSAIC, they are especially crafted for
telecommunication businesses.
248
S. Podapati et al.

3
A Link Between IS and Geodemographic Segments
3.1
Notation and Deﬁnitions of Fuzzy Logic
Deﬁnition (in the style of [2]). A fuzzy set A in X is characterized by a membership
function fA(x), which associates with each point in X a real number in the interval [0,
1], with the value of fA(x) at x representing the “grade of membership” of x in A. For the
opposite quality: fnotA(x) = 1 −fA(x).
Fuzzy membership scores reﬂect the varying degree to which different cases belong
to a set. Under the six value fuzzy set, there are six degrees of membership 1: fully in,
[0.9−1): mostly but not fully in, [0.6−0.9): more or less in, [0.4−0.6): more or less out,
[0.1−0.4): mostly but not fully out, [0−0.1): fully out. For a comprehensive guide of
good practices in fuzzy logic analysis in social sciences the reader is referred to, for
example, [10].
Linguistic Hedges:
• Rather will be added to a quality A, if the square root of its membership function
fA(x)1/2 is close to 1.
• Very will be added to a quality A, if the square of its membership function fA(x)2 is
close to 1.
• Extremely will be added to a quality A, if fA(x)3 is close to 1.
The principles for calculating the values of hedged membership functions, for
example fveryA(x) = fA(x)2, are described in [14]. Then, given the new membership
function, the same principle applies: the closer to 1, the higher is the degree of
membership.
3.2
Query Formulation
To
keep
the
formulations
and
questions
naturally
sounding,
the
word
infrastructure-friendly (IF) is used. The quality IF is deﬁned as the opposite to IS:
fIF(segmenti) = 1 −fIS(segmenti), for some segment i. As mentioned above, within the
same geodemographic segment, the clients differ with respect to the degree of being IS.
When the infrastructure is not overloaded, that is, the recent historical load is still
signiﬁcantly smaller than the capacity, then virtually any client is welcome. As the
infrastructure becomes more loaded, the operator wants to be more discriminative. We
deﬁne being “loaded” for an antenna as a fuzzy variable:
floaded antenna j
ð
Þ ¼ maxall tfload j; t
ð
Þ  capacity antenna j
ð
Þ1g:
This quality is measured in man units. Being loaded for infrastructure is deﬁned as:
floaded infrastructure
ð
Þ ¼ maxall antennas j floaded antenna j
ð
Þ
f
g:
Fuzzy Recommendations in Marketing Campaigns
249

Since being loaded is a dangerous quality, we set the strength of the system to be
equal to the strength of its weakest component, and for this reason the equation above
we use the max operator.
Queries:
1. Which segments to target, provided that rather IF users are acceptable clientele?
2. Which segments to target, provided that only very IF are wanted?
Depending on the load, there are different rankings of segments. If initially some
segments were in the same tier, for example, “very IF segments”, some of them fall out
of the tier, as the hedge operator is applied and the value of the membership function is
squared (for “extremely IF”). The context, when to apply Query 1 or 2, becomes
clariﬁed via calculating floaded (infrastructure) and checking the applicability of dif-
ferent hedges. The method to obtain fuzzy heuristics is summarized to the sequence of
the following steps.
Step 1: The IS clients in the customer base are revealed with the method [6] (the
algorithm is reproduced as function reveal_IS clients in Algorithm 1), and each
client is labeled with the IS/notIS descriptor.
Step 2: The propensity of a segment to contain IS clients is deﬁned as the frequency
of IS clients among its members and it is calculated from the data:
fIS segmenti
ð
Þ ¼ frequencyIS segmenti
ð
Þ
For linguistic convenience the term Infrastructure-Friendly (IF) is introduced and is
set to be opposite to IS:
fIF segmenti
ð
Þ ¼ 1  fIS segmenti
ð
Þ
Step 3: The ranking of segments is carried out with respect to their IF quality and
the hedged values of the mebership function are calculated: for all segments i, frather
IF(segmenti), fvery IF(segmenti), and fextremely IF(segmenti). Given a hedge, which also
codes the severity of the context, the segments fall into the different tiers (corre-
sponding to one of the six fuzzy values): “fully in”, “mostly but not fully in”, “more
or less in”, and so on.
Step 4: Locally for the region under analysis, the infrastructure is assessed as
loaded, very loaded, or extremely loaded, and thus the severity of the context is
assessed. A ranking from Step 3 corresponding to a particular hedge is selected (as a
leap of faith further veriﬁed in the next section).
The above is depicted as a ﬂow chart in Fig. 1, and formalized as Algorithm 1. The
reasoning behind combinatorial optimization is discussed in detail in [11]. The rea-
soning behind the function label_ISclients is discussed in detail in [6].
250
S. Podapati et al.

Algorithm 1:
computation of the fuzzy recommendation heuristic. 
Variables: 
•
clientSet: set of with IDs of clients;
•
I: the set with geodemographic segments {segment1, …, segmentk}; 
•
D:
the mobility data for a region that for each user contain 
client’s ID, client’s geodemographic segment, time stamps when the client 
generated traffic, and which antenna served the client.
•
Si: the number of subscribers that belong to a geodemographic 
segment i;
•
Σall i
Si,t,j : the footprint, i.e. the number of subscribers that 
belong to a geodemographic segment i, at time moment t, who are registered 
with a particular cell j; 
•
Cj: the capacity of cell j
in terms of how many persons it can 
safely handle simultaneously;
•
x: the vector with the scaling coefficients for the geodemographic 
segments or other groups such as IS clients;
•
xIS: the coefficent for the IS segment from the vector x;
•
Nt,j= number of users at cell j at time t;.
Input: data set D: <userID, time stamp t, cell j>. 
label_ISclients;
for i in I{
ratherIF[i] = false
veryIF[i] = false
extremelyIF[i] = false
degreeIS = frequency(userIDIS,I)
degreeIF = 1- degreeIS
if (degreeIF1/2 ≥0.9) then ratherIF[i]=true
if (degreeIF2 ≥0.9) then veryIF[i]=true
if (degreeIF3 ≥0.9) then extremelyIF[i]=true
}
function label_ISclients{
[I. Characterize each user with respect to her relative mobility.]
for each userID {
trajectoryID = cellt1, …, cellt2016;
relativeTrajectoryID = Nt1,j, …, Nt2016,j;
sortedTrajectoryID = sortdecreas_or.(relativeTrajectoryID);
topHotSpotsID = Σk=1..100(5%)sortedTrajectoryID[k];
Fuzzy Recommendations in Marketing Campaigns
251

userTopHotSpots = <userID, topHotSpotsID>
}
rankedUserList = sortdecreasing_or(rankedUserList)
[II. Initialization.]
xstressing = 0;
setStressingUsers = ∅.
[III. Reveal the infrastructure-stressing clients.]
While (xstressing = 0) do {
tentativeStressingUsers = head1%(rankedUserList);
setFriendlyUsers = bottom1%(rankedUserList);
otherUsers = rankedUserList – tentativeSetStressingUsers -
setFriendlyUsers;
[Confirm the tentative labeling via combinatorial optimization.]
I = {stressing, medium, friendly};
{xstressing, xmedium, xfriendly} = combinatorial_optimization(I,D);
IF (xstressing = 0), THEN {
tentativeSetStressingUsers = tentativeStressingUsers
[take the field userID from tentativeStressingUsers]
< userID >;
setStressingUsers = setStressingUsers + 
tentativeSetStressingUsers<UserID>
D = D - Dstressing
} [end of while]
for id in <userIDs> do {
if (id ∈setStressingUsers) then label(id,”IS”)
else label(id,”notIS”)
} [end loop on id in <userIDs>]
} [end reveal_ISclients]
function combinatorial_optimization(I,D){
solve
Maximize
Σi∈{IF,other,IS} Si xi, 
subject to:
for all j,t, Σi∈{IF,other,IS} Si,t,j xi ≤Cj
} returns {xIF,xother,xIS}.
Output: array ratherIF[], veryIF[], extremelyIF[]. 
3.3
Query Simulation
In the above, when deciding which context should be applied, we relied on an intuitive
rule: If the infrastructure is <hedge> loaded, then <hedge> IS segments are suitable to
hire clients from. For example, in the case of a rather loaded infrastructure, rather IS
segments are suitable targets. Given the expected success of the campaign, e.g. the
campaign can attract 300 new clients or 1500 clients, it is possible to simulate the
impact of the expected result on the infrastructure. A warning is thrown, if some
antenna is overloaded, i.e. when the expected footprint by the segment violates a
restriction for some segment i, at some antenna j, some time moment t:
252
S. Podapati et al.

analysis of the
recent load
fuzzy modeling 
of the load 
reveal IS
rank the segments into different 
tiers for different contexts
calculate which hedge should apply 
a recommendation in a 
natural language 
simulation/verification of the expected 
effect on the infrastructure
fuzzy recommendation 
Fig. 1. The ﬂow chart for the calculation of fuzzy recommendation for a marketing campaign.
Fig. 2. The number of IS clients in different MOSAIC categories.
Fuzzy Recommendations in Marketing Campaigns
253

aSi;j;t  Cj;
where a is a scaling coefﬁcient:
a ¼ expected number of new clients' current number of clients
ð
Þ1:
This is a justiﬁable approximation, since there is a high predictability in user
trajectories within different segments, e.g. [12, 13].
4
Experiment
1. Reveal the IS clients. Applying the algorithm to reveal IS clients, we have added a
ﬁeld to the data set with the label IS or not IS as a descriptor for each client.
2. Calculate degree of infrastructure-friendliness for each segment. In the whole
customer base, 7% of subscribers were revealed to be IS [6]. We have obtained the
distribution of the IS clients within the MOSAIC and Telenor segments and
depicted them in Figs. 2 and 3, respectively. The degree of the infrastructure-
friendliness is reported in Tables 1 and 2, for MOSAIC and Telenor segments,
respectively.
3. Reasoning behind the queries. Tables 1 and 2 simulate the reasoning behind the
query results for different contexts (codiﬁed via a hedge) for the MOSAIC and
Telenor segments, respectively. Each of the 14 MOSAIC classes qualiﬁes as rather
IF, which are those with fIF(i)1/2 > 0.9. Once the customer base becomes larger and
the spare capacity diminishes, only very IF will be wanted, which are those with
fIF(i)2 > 0.9. Out of those, only 9 segments qualify as very IF and ﬁve segments
qualify as extremely IF (fIF(i)3 > 0.9). The customer population was subjected to the
same analysis with respect to Telenor segmentation. As follows from Table 2, each
of the six Telenor segments is rather friendly, and there are four and three very and
extremely IF segments, respectively.
Fig. 3. The number of IS clients in different Telenor segments.
254
S. Podapati et al.

5
Results
When it comes to designing strategies of accommodating many more clients, being
IS-prone for a segment is an important quality. We have studied the correlation
between IS users and the geo-demographic segments, motivated by the fact that we can
target the geo-demographic segments (MOSAIC and Telenor) in marketing campaigns.
For different contexts, we have completed candidate rankings of geo-demographic
segments, and, given the absence of other preferences, the top-tier segments are
preferable. Which ranking out of several candidate ones is taken depends on the hedge
calculated for the intensiveness of infrastructure exploitation. The simulation of the
expected effect guarantees no false negatives, such as saying that certain segments are
safe to hire from, but in fact that would lead to a service failure at some place and time
in the network. For the implementation, please check https://sourceforge.net/projects/
telenor-user-mobility/?source=navbar.
Table 1. The reasoning behind the query results for the MOSAIC segments.
Segment fIF(i) fIF(i)1/2 rather IF? fIF(i)2 very IF? fIF(i)3 extremely IF?
A
0.96
0.97
yes
0.92
yes
0.88
no
B
0.98
0.98
yes
0.96
yes
0.94
yes
C
0.93
0.96
yes
0.86
no
0.79
no
D
0.92
0.95
yes
0.84
no
0.77
no
E
0.96
0.97
yes
0.92
yes
0.88
no
F
0.92
0.95
yes
0.86
no
0.79
no
G
0.93
0.96
yes
0.86
no
0.79
no
H
0.96
0.97
yes
0.92
yes
0.88
no
I
0.97
0.98
yes
0.94
yes
0.91
yes
J
0.92
0.95
yes
0.86
no
0.79
no
K
0.97
0.98
yes
0.94
yes
0.91
yes
L
0.98
0.98
yes
0.96
yes
0.94
yes
M
0.96
0.97
yes
0.92
yes
0.88
no
N
0.95
0.97
yes
0.9
yes
0.85
no
Table 2. The reasoning behind the query results for the Telenor segments.
Segment fIF(i) fIF(i)1/2 rather IF? fIF(i)2 very IF? fIF(i)3 extremely IF?
CA
0.94
0.97
yes
0.88
no
0.82
no
MM
0.99
0.89
yes
0.98
yes
0.97
yes
QA
0.96
0.92
yes
0.92
yes
0.88
no
T
0.98
0.87
yes
0.96
yes
0.94
yes
CC
0.92
0.8
yes
0.86
no
0.79
no
VA
0.97
0.91
yes
0.94
yes
0.91
yes
Fuzzy Recommendations in Marketing Campaigns
255

References
1. Zadeh, L.: Fuzzy logic and beyond - a new look. In: Zadeh, L., King-Sun, F., Konichi, T.
(eds.) Fuzzy Sets and Their Applications to Cognitive and Decision Processes: Proceedings
of the US-Japan Seminar on Fuzzy Sets and Their Applications, Held at University of
California, Berkeley, California, 1–4 July 2014. Academic Press (2014)
2. Zadeh, L.A.: Fuzzy sets. Information and control 8(3), 338–353 (1965)
3. Meyer, A., Zimmermann, H.J.: Applications of fuzzy technology in business intelligence.
Int. J. Comput. Commun. Control 6(3), 428–441 (2011)
4. Bilgiç, T., Türksen, I.B.: Measurement of membership functions: theoretical and empirical
work. Fundam. fuzzy sets 1, 195–232 (2000)
5. Tettamanzi, A., Carlesi, M., Pannese, L., Santalmasi, M.: Business intelligence for strategic
marketing: predictive modelling of customer behaviour using fuzzy logic and evolutionary
algorithms. In: Applications of Evolutionary Computing, pp. 233–240 (2007)
6. Sidorova, J., Skold, L., Lundberg, L.: (A) Revealing Infrastructure-Stressing Clients in the
Customer Base of a Scandinavian Operator using Telenor Mobility Data and HPI Future
SoC Lab Hardware Resources. Hasso Plattner Institute. A Technical report. https://www.
researchgate.net/publication/312153463_Optimizing_the_Utilization_in_Cellular_
Networks_using_Telenor_Mobility_Data_and_HPI_Future_SoC_Lab_Hardware_Resources
7. Haenlein, M., Kaplan, A.M.: Unproﬁtable customers and their management. Bus. Horiz. 52
(1), 89–97 (2009)
8. Debenham, J., Clarke, G., Stillwell, J.: Extending geodemographic classiﬁcation: a new
regional prototype. Environ. Planning A 35(6), 1025–1050 (2003)
9. Sagar, S., Skold, L., Lundberg L., Sidorova, J.: Trajectory segmentation for a recommen-
dation module of a customer relationship management system. In: The 2017 International
Symposium on Advances in Smart Big Data Processing (in press). https://www.researchgate.
net/publication/316657841_Trajectory_Segmentation_for_a_Recommendation_Module_of_
a_Customer_Relationship_Management_System
10. Ragin, C.C., Rihoux, B.: Qualitative Comparative Analysis Using Fuzzy Sets (fsQCA)
(2009)
11. Sidorova, J., Skold, L., Rosander, O., Lundberg L.: Discovering insights in telecommuni-
cation business from an interplay of geospatial and geo-demographic factors. In: The 1st
International Workshop on Data Science: Methodologies and Use-Cases (DaS 2017) at 21st
European Conference on Advances in Databases and Information Systems (ADBIS 2017).
LNCS, 28–30 August 2017, Larnaca, Cyprus (2017, in press)
12. Song, C., Qu, Z., Blumm, N., Barabási, A.L.: Limits of predictability in human mobility.
Science 327(5968), 1018–1021 (2010)
13. Lu, X., Wetter, E., Bharti, N., Tatem, A.J., Bengtsson, L.: Approaching the limit of
predictability in human mobility. Sci. Rep. 3 (2013)
14. Zadeh, L.A.: A fuzzy-set-theoretic interpretation of linguistic hedges (1972)
15. InsightOne MOSAIC lifestyle classiﬁcation for Sweden, http://insightone.se/en/mosaic-
lifestyle/. Accessed 15 Apr 2017
256
S. Podapati et al.

Eﬃcient Data Management for Putting Forward
Data Centric Sciences
Genoveva Vargas-Solar(B)
Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG-LAFMIA, 38000 Grenoble, France
genoveva.vargas@imag.fr
http://www.vargas-solar.com
Abstract. The novel and multidisciplinary data centric and scientiﬁc
movement promises new and not yet imagined applications that rely on
massive amounts of evolving data that need to be cleaned, integrated,
and analysed for modelling, prediction, and critical decision making pur-
poses. This paper explores the key challenges and opportunities for data
management in this new scientiﬁc context, and discusses how data man-
agement can best contribute to data centric sciences applications through
clever data science strategies.
1
Introduction
Data centric sciences are leading to diﬀerent trendy applications for smart cities,
homes and energy; urban computing, monitoring and student assessment from
kinder garten to university, automatic health control and monitoring, ﬁnances
self-management. These applications still provide partial solutions to multi-
variable and multi-facet problems related to the understanding of complex sys-
tems. For example, the personalization of drugs for better attacking diseases,
understanding social behaviours of individuals and societies, modelling and
reproducing human skills, and processes like creativity, artistic creation, neuronal
behaviour and prediction of natural changes and phenomena. The backbone of
these problems are data collections that must be harvested and they must be
representative enough and with speciﬁc properties so that they can serve as raw
material to processing and analysis algorithms. These processes and algorithms
can reproduce and understand such phenomena and systems, analyse them, and
deduce valid conclusions.
Thus, it is necessary to deal with data collections characterized by their
volume, production rate (velocity), variety, multiple veracity, and value. Besides
these properties, data collections are also determined by the type of content
they group and the conditions in which they are consumed. Paul McFedries in
his article Beyond Just Big Data [11] classiﬁes data collections into:
– thick data, which combines both quantitative and qualitative analysis;
– long data, which extends back in time hundreds or thousands of years;
– hot data, which is used constantly, meaning it must be easily and quickly
accessible, and
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 257–266, 2017.
DOI: 10.1007/978-3-319-67162-8 25

258
G. Vargas-Solar
– cold data, which is used relatively infrequently, so it can be less readily
available.
These types of data collections can be consumed according to diﬀerent pat-
terns that determine the conditions in which they are accessed:
– shared by several users and accessed in a concurrent manner, with speciﬁc
isolation or atomicity requirements;
– stored only for archival purposes;
– distributed for promoting parallel data processing and analytics;
– continuous processing for data ﬂows with eﬃcient read/writes.
Cloud and High Performance Computing have evolved to respond to emerging
data management and processing challenges introduced by data centric sciences.
There is a long path to go through, but the scientiﬁc community and industry
has started designing the next generation of data management and processing
stacks and evolve towards a data science. Data science will deal with the exa-
scales and its associated variety, not in terms of format but in terms of diﬀerent
properties: hot/cold, long/cold, thick data.
On the other hand, the emergence of Big Data some years ago denoted
the challenge of dealing with huge collections of heterogeneous data contin-
uously produced and to be exploited through data analytics processes. First
approaches have addressed data volume and processing scalability challenges.
Solutions have addressed the problem of balancing and scheduling the delivery
of computing, storage, memory, and communication resources (bandwidth and
reliability) for greedy analytics and mining processes with high in-memory and
computing cycles requirements.
Based on these previous results, emerging data centric sciences (e.g., data
science, network science, computational science, social electronic sciences, digi-
tal humanities) develop methodologies weaving data management, greedy algo-
rithms, and programming models that must be tuned to be deployed in diﬀerent
target computer architectures.
This paper focuses data collections with diﬀerent uses and access patterns
because these properties tend to reach limits of:
– the storage capacity (main memory, cache and disks) required for archiving
data collections permanently or during a certain period, and
– the pace (computing speed) in which data must be consumed (harvested,
prepared and processed).
These aspects must be addressed when dealing with data centric sciences prob-
lems. They are discussed in this paper. Accordingly the remainder of the paper is
organized as follows. Section 2 gives an overview of existing approaches address-
ing data storage infrastructures, data analytics under a data science vision, and
infrastructures providing computing capacity for processing data using greedy
algorithms. Section 3 proﬁles data centric sciences and introduces their particular
requirements with respect to data as an enabling backbone. It gives the general

Eﬃcient Data Management for Putting Forward Data Centric Sciences
259
lines for storing and delivering data and thereby contributing to the eﬃcient
execution of data analytics processes running on parallel environments. Finally,
Sect. 4 concludes the paper and discusses perspectives.
2
Scaling up Data Processing
The emergence of Big Data has dealt with huge collections of heterogeneous data
continuously produced and to be exploited through data analytics processes.
Big Data management and processing are no longer only associated to scien-
tiﬁc applications with prediction, analytics requirements, data centric sciences
also call for data aware management to address the understanding and automatic
control of complex systems, to the decision making in critical and non-critical
situations. Given that data centric sciences rely on data collections that stem
from complex and diverse gathering processes, two data management issues are
key for enabling analysis workﬂows:
– data storage and distribution, and
– runtime environments that provide enough computing resources for executing
greedy data exploration and analytics algorithms.
The next sections discuss these topics.
2.1
Data Storage and Distribution
Data management in many data analytics workﬂows underlines the need to
reduce overhead when data are read, updated, and put into storage supports
(memory, cache) as stated by the RUM conjecture proposed in [3]. Several
platforms address some aspect of the problem like Big Data stacks [1,6]; data
processing environments (e.g., Hadoop, Spark, CaﬀeonSpark); data stores deal-
ing with the CAP (consistency, availability and partition tolerance) theorem
(e.g., NoSQL’s); and distributed ﬁle systems (e.g., HDFS). The principle is to
deﬁne API’s (application programming interface) to be used by programs to
interact with distributed data management, processing, and analytics layers that
can cope with distributed and parallel architectures.
Objects and components persistence has been an important issue addressed
already by consolidated middleware such as JBOSS and PiJAMA. The new
exascale requirements introduced by greedy processes often related to Big Data
processing has introduced objects persistence issues again. Particularly for the
“passivation” (storage of the execution state and environment of a component)
of greedy processes execution in the presence of failures. Instead of loosing costly
processes (in time and computing resources) they can be stored and the restarted
in without loosing already executed tasks.
In order for exascale and/or Big Data systems to deliver the needed I/O
performance, new storage devices such as NVRAM or Storage Class Memories
(SCM) need to be included into the storage/memory hierarchy. Given that the
nature of these new devices are closer to memory than to storage (low latencies,

260
G. Vargas-Solar
high bandwidth, and byte-addressable interface) using them as block devices for
a ﬁle system does not seem to be the best option. DataClay [5] proposes object
storage to enable both the programmer, and DataClay, to take full advantage of
the coming high-performance and byte-addressable storage devices. Today, given
the lack of such devices, DataClay performs a mapping of such abstractions to
key-value stores such as Kinetic drives from Seagate1.
Data structures and associated functions are sometimes more important for
some requirements rather than non functional properties like RUM or CAP. Non-
relational databases have emerged as solutions when dealing with huge data sets
and massive query work load. These systems have been redesigned to achieve
scalability and availability at the cost of providing only a reduced set of low-level
data management functions, thus forcing the client application to take care of
complex logic.
The large spectrum of data persistence and management solutions are
adapted for addressing workloads associated with Big Data volumes; and either
simple read write operations or with more complex data processing tasks. The
challenge today is choosing the right data management combination of tools for
variable application requirements and architecture characteristics. Plasticity of
solutions is from our point of view the most important property of such tools
combination.
Once data has been stored, possibly under a distributed strategy for address-
ing storage space and data availability, analytics processes can run on top of the
whole collection or some sample of it. These processes require very often to be
parallelized in order to process the whole data collections in reasonable time.
Many data centric science solutions require this type of parallel settings. Thus,
there exist diﬀerent types of parallel runtime environments that enable to deploy
analytics processes with diﬀerent degrees of transparency.
2.2
Parallel Runtime Environments
Today maybe because of the emergence of Big Data and greedy algorithms
and applications requiring computing resources, parallel architectures have come
back in the arena. There are diﬀerent kinds of computing, memory, and storage
resources providers that adopt their own method for delivering such resources
for executing programs. According to [9] there are three categories of resources
provision: PaaS frameworks, programming models for computing intensive work-
loads, and programming models for Big Data.
Platform-as-a-Service (PaaS) layers oﬀer APIs to write applications. For
example, in the Microsoft Azure Cloud programming model applications are
structured according to roles, which use APIs to communicate (queues) and
to access persistent storage (blobs and tables). Microsoft Generic Worker pro-
poses a mechanism to develop a Worker Role that eases the porting of legacy
code in the Azure platform [13]. Google App Engine provides libraries to invoke
external services and queue units of work (tasks) for execution; furthermore, it
1 http://www.seagate.com.

Eﬃcient Data Management for Putting Forward Data Centric Sciences
261
allows to run applications programmed in the Map-Reduce model. Data transfer
and synchronization are handled automatically by the runtime. Environments
for computing workload intensive applications use in general the bag of tasks
execution model conceiving an application as composed of independent paral-
lel tasks. For example, the Cloud BigJob, Amazon EC2, Eucalyptus, Nimbus
Clouds, and ProActive that oﬀer a resource manager developed to mix Cloud
and Grid resources [2,12]. Map-Reduce programming is maybe the most promi-
nent programming model for data intensive applications. Map-Reduce based
runtime environments provide good performance on cloud architectures above
all on data analytics tasks working on large data collections. Microsoft Day-
tona2 proposes an iterative Map-Reduce runtime for Windows Azure to support
data analytics and machine learning algorithms. Twister [7] is an enhanced Map-
Reduce runtime with an extended programming model for iterative Map-Reduce
computations. Hadoop [4] is the most popular open source implementation of
Map-Reduce on top of the Hadoop Distributed File System (HDFS). The use
of Hadoop avoids the lock into a speciﬁc platform allowing to execute the same
Map-Reduce application on any Hadoop compliant service, as the Amazon Elas-
tic Map-Reduce3.
For data centric sciences parallel tasks, strategies for dealing with data per-
sistence, eﬃcient read and write in memory operations, data preparation (trans-
formation, cleaning, collocation) must be integrated within the runtime environ-
ments. These environments must provide interfaces, either API or annotations to
enable programmers to transparently tag their data with their associated prop-
erties. It will be up to the runtime environment to provide data management
services or integrate existing ones that can ensure eﬃcient data I/O thanks to
well adapted operations with underlying processes for providing the right data
in the right moment and in the right place.
3
Supporting Data Centric Sciences Experiments
More than ever, researchers in all disciplines ﬁnd themselves wading through
more and more kinds of data. Frequently, there is no standard system for storing,
organizing, or analysing this data. Doing so requires a set of skills researchers
must largely learn independently. In recent years, data science has emerged as the
ﬁeld that exists at the intersection of math and statistics knowledge, expertise
in a science discipline, and so-called hacking skills, or computer programming
ability. Yet, data science is more of an emerging interdisciplinary philosophy,
a wide-ranging modus operandi that entails a cultural shift in the academic
community. The term means something diﬀerent to every data scientist, and
in a time when all researchers create, contribute to, and share information that
describes how we live and interact with our surroundings in unprecedented detail,
all researchers are data scientists. The philosophy is promising and opens new
2 http://research.microsoft.com/en-us/projects/daytona/.
3 Amazon elastic Map-Reduce. http://aws.amazon.com/documentation/elasticmap
reduce/.

262
G. Vargas-Solar
possibilities to develop methodologies that take advantage of information and
communication technologies that ﬁnd themselves new challenges and scientiﬁc
opportunities in other sciences included in the data science umbrella.
The rapid evolution of computer architectures delivering an increasing
amount of resources have opened new opportunities for data centric sciences.
As said before, these sciences address complex problems modelled by several
related variables that interact as complex systems with plenty of constraints
(climate change, traﬃc control, social behavior in crowds, simulation of bio-
logical processes and social phenomena, prediction of diseases). For addressing
these kind problems, data centric sciences perform experiments that weave data
management, with greedy artiﬁcial intelligence and data mining algorithms and
computing architectures services.
We consider that eﬃcient data management is the backbone of these exper-
iments. For us, it is no longer pertinent to reason with respect to a set of com-
puting, storage, and memory resources. Instead, it is necessary to conceive algo-
rithms and processes considering an unlimited set of resources usable via a pay
as U go model, energy consumption or services reputation and provenance mod-
els. Rather than designing processes and algorithms considering as threshold the
resources availability, computing service providers (e.g. cloud providers) change
this vision and rather take into consideration the economic cost of the processes
vs. the resources they consume.
It is necessary to develop novel strategies and tools for storing and delivering
terabytes of on-line data collections consisting of billions of records, optimizing
the consumption of resources while reducing the overhead of exploiting them by
parallel programs.
3.1
Data Analytics for Data Science
Methods for querying and mining Big Data are fundamentally diﬀerent from
traditional statistical analysis on small samples. Big Data are often noisy,
dynamic, heterogeneous, inter-related, and untrustworthy. Nevertheless, even
noisy Big Data could be more valuable than tiny samples. Indeed, general sta-
tistics obtained from frequent patterns and correlation analysis usually over-
power individual ﬂuctuations and often disclose more reliable hidden patterns
and knowledge.
Big Data forces to view data mathematically (e.g., measures, values distrib-
ution) ﬁrst and establish a context for it later. For instance, how can researchers
use statistical tools and computer technologies to identify meaningful patterns of
information? How shall signiﬁcant data correlations be interpreted? What is the
role of traditional forms of scientiﬁc theorizing and analytic models in assessing
data? What you really want to be doing is looking at the whole data set in ways
that tell you things and answers questions that you are not asking. All these
questions call for well-adapted infrastructures that can eﬃciently organize data,
evaluate and optimize queries, and execute algorithms that require important
computing and memory resources.

Eﬃcient Data Management for Putting Forward Data Centric Sciences
263
Big Data has enabled the next generation of interactive data analysis with
real-time answers. In the future, queries towards Big Data will be automatically
generated for content creation on websites, to populate hot-lists or recommen-
dations, and to provide an ad-hoc analysis of data sets to decide whether to keep
or to discard them [8]. Scaling complex query processing techniques to terabytes
while enabling interactive response times is a major open research problem today.
Analytical pipelines can often involve multiple steps, with built-in assump-
tions. By studying how best to capture, store, and query provenance, it is pos-
sible to create an infrastructure to interpret analytical results and to repeat
the analysis with diﬀerent assumptions, parameters, or data sets. Frequently,
it is data exploration and visualization that allow Big Data to unleash its true
impact. Visualization can help to produce and comprehend insights from Big
Data. Visual.ly, Tableau, Vizify, D3.js, R, are simple and powerful tools for
quickly discovering new things in increasingly large datasets.
In parallel to data science addressing (Big) Data under diﬀerent perspectives,
emerges computational science uses advanced computing capabilities to:
– understand and solve complex problems fusing numerical and non-numerical
algorithms and modelling and simulation tools;
– computer and information science that develop advanced hardware, software,
networking, and data management systems needed to solve computationally
demanding problems; and
– computing infrastructure that supports science and engineering problem
solving.
In practice, it is the application of computer simulation and other forms of com-
putation from numerical analysis and theoretical computer science to solve prob-
lems in various scientiﬁc disciplines, for example the one reports in [10] about
the application of Computational Science in Archaeology. A collection of prob-
lems and solutions in computational science can be found in [14]. Computational
science techniques are being applied to perform digital humanities research that
are exported for experiment reproducibility. For example, the action Museum 2.0
that explores ways that web 2.0 philosophies can be applied in museum design.
Museums share their data collections for digital social computational scientist
to perform research and visualize them.
3.2
Data Collections Sharding and Storage for Data Centric
Sciences
Data sharding has its origins in centralized systems that had to partition ﬁles,
either because the ﬁle was too big for one disk, or because the ﬁle access rate
could not be supported by a single disk. Relational distributed databases use
data sharding when they place relation fragments at diﬀerent network sites.
Data sharding allows parallel database systems to exploit the I/O bandwidth of
multiple disks by reading and writing them in parallel. Relational DBMS imple-
ment strategies for sharding data (i.e., tuples): round robin seems appropriate

264
G. Vargas-Solar
for processes accessing the relation by sequentially scanning all of it on each
query, hash-partitioning seems suited for sequential and associative access to
data avoiding the overhead of starting queries on multiple disks.
While sharding is a simple concept that is easy to implement, it raises several
physical database design issues. Each data collection must have a sharding strat-
egy and a set of disk fragments. Increasing the degree of sharding usually reduces
the response time for an individual query and increases the overall throughput of
the system. In parallel DBMS for sequential scans, the response time decreases
because more processors and disks are used to execute the query; for associative
scans, the response time improves because fewer tuples are stored at each node,
and hence the size of the index that must be searched decreases.
The NoSQL momentum introduces new datasets storage possibilities. Graph,
key-value, multidimensional records stores can provide storage solutions with
eﬃcient read/write operations possibilities. Sharding can be also guided by the
structure of data, and it can be coped to ad-hoc stores that can ensure persis-
tency and retrieval. This implies also a tuning eﬀort of diﬀerent NoSQL stores
that should then provide eﬃcient that reads/writes with speciﬁc degrees of avail-
ability, fault tolerance, and consistency. NoSQL stores rely on automatic replica-
tion mechanisms. Reads and data querying implemented at the application level
are then executed by applying Map-Reduce execution models. Sharded data
collections are thus the key to parallel executions.
The NoSQL approach leads to performant ad-hoc per problem solutions.
The DBMS approaches, in contrast, promote one-ﬁts all solutions where shard-
ing strategies can be tuned. What is true, it that the pertinence of sharding
strategies depends on analysis and processing requirements and available storage
and memory resources. In both cases, data collections sharding requires eﬀort,
expertise, and a lot of testing for ﬁnding the best balance to achieve good data
processing and analysis performance. We believe that sharding data collections
must be based on clever data organizations and indexing on ﬁle systems, data
stores, caches, or memories to reduce eﬀort and ensure performant exploitation
(retrieval, aggregation, analysis).
Therefore, automatic and elastic data collections sharding toolsare necessary
to parametrize data access and exploitation by parallel programs willing to be
performant and scale-up in diﬀerent target architectures:
– Delivering of datasets along distributed process units avoiding bottlenecks.
• Diﬀerent data structures used to reduce the overhead associated to read,
updates and persistence requests (RUM conjecture) on diﬀerent target
architectures.
• New strategies and algorithms to enable the transfer of large data sets
between distributed processing units. We will explore techniques such
as data streaming and process transfer instead of bulk data transfer, as
potential solutions.
• Ensure properties like availability, consistency, and partition tolerance
(CAP theorem) that can be reinforced in an adaptable and dynamic
manner.

Eﬃcient Data Management for Putting Forward Data Centric Sciences
265
– Assessing and predicting storage (disk, cache, memory) resources for opti-
mally delivering data and scaling up parallel processing.
With the new computing, storage, and memory requirements stemming from
data centric science problems, the use of cluster oriented architectures providing
such resources has increased and is somehow democratized particularly with the
emergence of the cloud. Data management requirements have to be revisited they
vary from simple storage coupled with read/write requirements with reasonable
data processing performance needs to real critical applications dealing with huge
data collections to be processed by greedy algorithms. In such settings it is
possible to exploit parallelism for processing data by increasing availability and
storage reliability thanks to duplication and collocation.
4
Conclusion and Perspectives
Digging into data today requires data science and data engineering methodolo-
gies that can apply information and communication technologies in the best way
to address the challenges introduced by the characteristics of data collections
(big, continuous, multi-form, and multimedia with diﬀerent veracity degrees)
and that require computing resources in order to be exploited. Yet, data collec-
tions are not only a digital artifact, they represent content determined by the
conditions in which it has been authored, produced, collected, and digitalized,
by its provenance, by the intention behind its exploitation, and by the conditions
and rules in which it can be reused. These characteristics have to be exhibited
and be accessible to (social) scientists wishing to use them as raw material to
perform research and analysis.
It is no longer possible to practice digital social sciences without being sup-
ported by data science and engineering to avoid empirical use of technology to
maintain and curate data collections. The way algorithms and technology are
used has to avoid technological bias pollution of results. If some data processing
and cleaning is necessary because of mathematical or technical reasons, this has
to be known by the scientist performing the analysis, and it has to be reported in
the results obtained. The mathematical and technical conditions in which analy-
sis and visualisation of results are done must be considered in the interpretation
of those results. This will ensure the credibility of the experiment performed on
digital data using ICT and it will provide an objective perception of the results
and interpretations. Data science and engineering do not have sense without
having concrete problems with explicit requirements, rules and expectations.
The challenge is to have tools that can change their preferences towards data
management and analytics and provide elastic strategies for implementing these
operations. Such strategies should evolve as data acquire diﬀerent structures and
semantics as a result of the data processing operations applied on them.
Acknowledgement. This work has been partially funded by the project MULTI-
POINT, the cooperation contract Clouding Things and the COST EU Actions
KEYSTONE.

266
G. Vargas-Solar
References
1. Alexandrov, A., Bergmann, R., Ewen, S., Freytag, J.C., Hueske, F., Heise, A., Kao,
O., Leich, M., Leser, U., Markl, V., et al.: The stratosphere platform for big data
analytics. VLDB J. 23(6), 939–964 (2014)
2. Amedro, B., Baude, F., Caromel, D., Delb´e, C., Filali, I., Huet, F., Mathias, E.,
Smirnov, O.: An eﬃcient framework for running applications on clusters, grids,
and clouds. In: Antonopoulos, N., Gillam, L. (eds.) Cloud Computing, pp. 163–
178. Springer, London (2010)
3. Athanassoulis, M., Kester, M., Maas, L., Stoica, R., Idreos, S., Ailamaki, A.,
Callaghan, M.: Designing access methods: The rum conjecture. In: International
Conference on Extending Database Technology (EDBT) (2016)
4. Borthakur, D.: The hadoop distributed ﬁle system: Architecture and design.
Hadoop Proj. Website 11(2007), 21 (2007)
5. Cortes, T., Queralt, A., Mart´ı, J., Labarta, J.: DataClay: Towards Usable and
Shareable Storage Big Data and Extreme-Scale Computing (BDEC), White
paper, pp. 1–3. http://www.exascale.org/bdec/sites/www.exascale.org.bdec/ﬁles/
whitepapers/dataClay%20at%20BDEC%20Barcelona.pdf
6. Franklin, M.: The berkeley data analytics stack: Present and future. In: 2013 IEEE
International Conference on Big Data, pp. 2–3. IEEE (2013)
7. Gunarathne, T., Zhang, B., Tak-Lon, W., Qiu, J.: Scalable parallel computing
on clouds using twister4azure iterative mapreduce. Futur. Gener. Comput. Syst.
29(4), 1035–1048 (2013)
8. Idreos, S., Alagiannis, I., Johnson, R., Ailamaki, A.: Here are my data ﬁles. here
are my queries. where are my results? In: Proceedings of 5th Biennial Conference
on Innovative Data Systems Research, number EPFL-CONF-161489 (2011)
9. Lordan, F., Tejedor, E., Ejarque, J., Rafanell, R., Alvarez, J., Marozzo, F., Lezzi,
D., Sirvent, R., Talia, D., Badia, R.M.: Servicess: An interoperable programming
framework for the cloud. J. Grid Comput. 12(1), 67–91 (2014)
10. Marwick, B.: Computational reproducibility in archaeological research: basic prin-
ciples and a case study of their implementation. J. Archaeol. Method Theor. 24,
1–27 (2016)
11. McFedries, P.: Beyond just big data, We’re all data geeks now. IEEE Spectr. 53(8),
29 (2015). http://spectrum.ieee.org/computing/software/
12. Peng, J., Zhang, X., Lei, Z., Zhang, B., Zhang, W., Li, Q.: Comparison of several
cloud computing platforms. In: 2009 Second International Symposium on Informa-
tion Science and Engineering (ISISE), pp. 23–27. IEEE (2009)
13. Simmhan, Y., Van Ingen, C., Subramanian, G., Li, J.: Bridging the gap between
desktop and the cloud for escience applications. In: 2010 IEEE 3rd International
Conference on Cloud Computing (CLOUD), pp. 474–481. IEEE (2010)
14. Steeb, W.H., Hardy, Y., Hardy, A., Stoop, R.: Problems and solutions in scientiﬁc
computing with C++ and java simulations world scientiﬁc publishing (2004)

Towards a Multi-way Similarity Join Operator
Mikhail Galkin1,2,3(B), Maria-Esther Vidal2, and S¨oren Auer1,2
1 Enterprise Information Systems (EIS), University of Bonn, Bonn, Germany
{galkin,auer}@cs.uni-bonn.de
2 Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS),
Sankt Augustin, Germany
vidal@cs.uni-bonn.de
3 ITMO University, Saint Petersburg, Russia
Abstract. Increasing volumes of data consumed and managed by enter-
prises demand eﬀective and eﬃcient data integration approaches. Addi-
tionally, the amount and variety of data sources impose further challenges
for query engines. However, the majority of existing query engines rely
on binary join-based query planners and execution methods with com-
plexity that depends on the number of involved data sources. Moreover,
traditional binary join operators are not able to distinguish between sim-
ilar and diﬀerent tuples, treating every incoming tuple as an indepen-
dent object. Thus, if tuples are represented diﬀerently but refer to the
same real-world entity, they are still considered as non-related objects.
We propose MSimJoin, an approach towards a multi-way similarity join
operator. MSimJoin accepts more than two inputs and is able to iden-
tify duplicates that correspond to similar entities from incoming tuples
using Semantic Web technologies. Therefore, MSimJoin allows for the
reduction of both the height of tree query plans and duplicated results.
Keywords: Semantic data management · Semantic Web · Join opera-
tors
1
Introduction
Data integration problems have tackled by the research community for many
years. Growing volumes of data and the need for deeper insights have fostered
new data integration paradigms. In enterprises, data processing pipelines might
involve dozens of data sources which comprise a signiﬁcant burden on the query
processing engines as their query planning mechanisms construct tree-like plans
relying on binary join operators. Additionally, an abundance of existing data
sources often contains references to the same real-world entities expressed in
diﬀerent formats, terms, schemata, or RDF vocabularies. Such a heterogeneity
poses a substantial challenge for query processing as few of common join tech-
niques tackle entities equivalence unless such entities are encoded in exactly the
same way with matching variables names and respective values.
The increasing adoption of Resource Description Framework (RDF), Linked
Open Data, and Semantic Web technologies within enterprises in various
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 267–274, 2017.
DOI: 10.1007/978-3-319-67162-8 26

268
M. Galkin et al.
domains [10] provides evidences of the maturity of the technology stack, and
how this is used to tackle large-scale heterogeneous data integration problems.
Furthermore, Semantic Web technologies oﬀer a set of tools to bridge the gap
between large-scale heterogeneous data and uniﬁed query processing. Speciﬁ-
cally, RDF1 is able to serve as a lingua franca model that uniformly expresses a
plethora of domain-speciﬁc schemata. SPARQL2 is a query language to extract
data from RDF data sources.
In this article, we present an approach that contributes to both outlined
problems, i.e., an idea towards a multi-way similarity join operator MSimJoin
that is capable of processing more than two input sources and tackle semantic
equivalence of the input entities (i.e., that refer to the same real-world object).
A join with an arbitrary arity allows for simplify query plans whereas semantic
similarity mechanisms deal with data redundancy, thus increasing overall answer
completeness. The proposed architecture describes how such components com-
prise a physical operator and specify inputs and outputs of the operator.
The remainder of this article is structured as follows: Sect. 2 motivates the
need in multi-way similarity join operators on an illustrative example. Section 3
gives an overview of the proposed join algorithm. Section 4 analyzes state of the
art in federated query processing and similarity joins. Finally, Sect. 5 draws our
conclusions and outlines the directions of our future work.
2
Motivating Example
Figure 1 illustrates the intuition behind the idea of a multi-way join plan. Given
a complex SPARQL query (cf. Fig. 1a) that consists of ten triple patterns and
only one data source is able to answer a particular triple pattern, i.e., the query
environment is federated. The task of answering the query presents severe diﬃ-
culties for existing federated query engines that employ only binary join plans.
For instance, some variants of bushy plans (cf. Fig. 1b) and (cf. Fig. 1c) have
to perform nine joins that is computationally expensive. On the other hand,
employing a multi-way join operator, i.e., 5-way as presented in Fig. 1d, allows
for a signiﬁcant simpliﬁcation of a query plan. That is, only three joins have to
be performed, two multi-way joins that integrate the results of ﬁve respective
triple patterns each (t1-t5 that share a join variable ?s1 and t6-t10 that share
?a) and one binary join to produce a ﬁnal answer of t1 triple pattern, i.e., a join
between ?s1 and ?a.
Moreover, usually data sources are not aligned among each other and there-
fore contain replicated and redundant data. For instance, Fig. 2 illustrates the
case when three data sources that answer t8, t9, t10 of a query in Fig. 1a return
entities, namely <CCR> and <Creedence Clearwater Revival>, that refer to the
same real-world object. Traditional join operators consider tuples with such enti-
ties as completely diﬀerent and thus, given that ?a is a join variable, do not yield
1 https://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/.
2 https://www.w3.org/TR/sparql11-query/.

Towards a Multi-way Similarity Join Operator
269
SELECT *  WHERE {
       ?s1   <http://dbpedia.org/property/associatedActs>               ?a.
       ?s1   <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>    ?aa.
       ?s1   <http://dbpedia.org/property/dateOfBirth>                     ?o3.
       ?s1   <http://dbpedia.org/property/placeOfDeath>                 ?o4.
       ?s1   <http://dbpedia.org/ontology/birthYear>                        ?o5.
       ?a     <http://dbpedia.org/property/background>                    ?b.
       ?a     <http://dbpedia.org/ontology/occupation>                     ?o6.
       ?a     <http://dbpedia.org/ontology/associatedBand>             ?ab.
       ?a     <http://dbpedia.org/ontology/associatedMusicalArtist> ?o1.
       ?a     <http://dbpedia.org/property/associatedActs>               ?o2 . }
t1
t2
t3
t4
t5
t6
t7
t8
t9
t10
(a) Complex SPARQL query of ten triple patterns
t1
t2
t3
t9
t10
t8
D1
D2
D3
D8
D9
D10
t4
D4
t5
D5
D7
D6
t7
t6
(b) Balanced Bushy Tree Plan
t1
t2
t3
t9
t10
t8
D1
D2
D3
D8
D9
D10
t4
D4
t5
D5
D7
D6
t7
t6
(c) Bushy Plan on Nested Loop joins
t1
t2
t3
t9
t10
t8
D1
D2
D3
D8
D9
D10
t4
D4
t5
D5
D7
D6
t7
t6
(d) Multi-way join plan
Fig. 1. Motivating example. (a) A complex SPARQL query. (b) An example of a
bushy tree plan. (c) An example of a nested loop joins. (d) Multi-way plan.
t9
t10
t8
D8
D9
D10
 ?a: <Creedence_Clearwater_Revival>, ?ab: <The_Blue_Velvets>
 ?a: <CCR>, ?o1: <The_Blue_Velvets>
 ?a: <CCR>, ?o1: <The_Golliwogs>
 ?a: <Creedence_Clearwater_Revival>, ?o2: <The_Blue_Velvets>
 ?a: <Creedence_Clearwater_Revival>, ?o2: <The_Golliwogs>
no result
(a) Non-similarity operator
t9
t10
t8
D8
D9
D10
 ?a: <Creedence_Clearwater_Revival>, ?ab: <The_Blue_Velvets>
       ?a: <CCR>, ?o1: <The_Blue_Velvets>
       ?a: <CCR>, ?o1: <The_Golliwogs>
 ?a: <Creedence_Clearwater_Revival>, ?o2: <The_Blue_Velvets>
 ?a: <Creedence_Clearwater_Revival>, ?o2: <The_Golliwogs>
{?a:<CCR>, ?ab:<The_Blue_Velvets>,
      ?o1:<The_Blue_Velvets>, ?o2: <The_Blue_Velvets>  }
{?a:<CCR>, ?ab:<The_Blue_Velvets>,
      ?o1:<The_Golliwogs>, ?o2: <The_Blue_Velvets>}
{?a:<CCR>, ?ab:<The_Blue_Velvets>, 
      ?o1:<The_Blue_Velvets>, ?o2: <The_Golliwogs>}
{?a:<CCR>, ?ab:<The_Blue_Velvets>, 
      ?o1:<The_Golliwogs>, ?o2: <The_Golliwogs>}
(b) Semantic similarity operator
Fig. 2. Motivating Example (continued). (a) A non-similarity join operator is not
able to identify semantically equivalent entities and therefore produces zero results. (b)
A semantic similarity operator matches <CCR> with <Creedence Clearwater Revival>
and produces four tuples.
any output as <CCR> and <Creedence Clearwater Revival> do no match syn-
tactically. With the growing number of accessible data sources it is hardly ever
possible to ensure that only unique entities arrive to the engine.
We call entities semantically equivalent (or equivalent) if they refer to the
same real-world entity but are represented diﬀerently. For instance, both notions
<CCR> and <Creedence Clearwater Revival> refer to one music band, and thus
they are semantically equivalent. Furthermore, the task of the join operator when
executing against a federation of sources is to tackle equivalent entities and
produce valid results. The example of such an operator is presented in Fig. 2b.
The operator is able to match values of the syntactically diﬀerent join variable
?a and yields four results (obtained as a Cartesian product of non-join variable
values ?ab, ?o1, ?o2) whereas a non-similarity operator is not able to yield any
result (cf. Fig. 2a). This article presents an approach towards such a join operator

270
M. Galkin et al.
that combines both features, that is, support for numerous inputs and semantic
similarity mechanisms.
3
Our Approach: MSimJoin
SPARQL provides standardized means for querying federations of data
sources [3]. In this work the problems of query rewriting and optimal source
selection are out of the scope, we thus resort to existing approaches, e.g., [15].
Instead, we assume the data sources are identiﬁed and properly queried so that
every source returns a stream of tuples to be joined by a join variable with other
streams. Therefore, the input state of a n-way join operator consists of n input
streams.
A multi-way join operator is envisioned to accepts more than two such
streams and produce a join only if an instantiation of a join variable is shared in
tuples among all sources. Figure 3 illustrates the intuition behind the proposed
similarity join operator. In order to apply semantic similarity techniques, input
tuples are converted to the format of RDF molecules [5], a semantic data struc-
ture that allows for additional annotation of variables and their values in a tuple
using common vocabularies. Given that the original query is written in SPARQL,
RDF molecules can be constructed using query predicates, e.g., the returned
tuple from the data source ?a: <CCR>, ?o1: <The Golliwogs> is converted to
an RDF triple <CCR> dbp:associatedMusicalArtist <The Golliwogs>.
The operator employs and extends the classical probe-insert methodology
to support an arbitrary arity and store intermediate results. For instance, as
depicted in Fig. 3a, a tuple arrives from the source A. It is subsequently converted
to an RDF molecule and probed against respective collections of intermediate
results. To produce results as soon as they are ready, a molecule is probed against
intermediate results in a speciﬁc order, i.e., at ﬁrst, against a collection BCDE
that might contain join results arrived before from B, C, D, and E sources,
respectively, i.e., of n−1 length. If the tuple from A and a tuple from BCDE share
the same join variable instantiation, then the join is found and the result is put
into the output collection ABCDE. If the join condition is not met, the molecule
is probed against collections of n −2 length that do not contain tuples from A,
namely BCD, BCE, BDE, CDE. Contrary, if the join condition is satisﬁed, then
a new intermediate result is inserted into a respective collection, i.e., ABCD,
ABCE, ABDE, ACDE. Otherwise, the molecule is probed against collections
of n −3 length that combine intermediate results from two sources according
to Fig. 3. Finally, if all intermediate collections did not satisfy a join condition,
the molecule is compared against main collections that accumulate direct RDF
molecules from sources B, C, D, E. Any yielded intermediate results are inserted
into AB, AC, AD, AE, respectively. The pipeline for a tuple arrived from the
source A is depicted in Fig. 3a. A similar pipeline for a tuple arrived from B is
depicted in Fig. 3b. The main diﬀerence for the source B is in the new probing
collections that do not contain B and inserts into respective collections with B,
e.g., the intermediate join among B and CDE is inserted into BCDE.

Towards a Multi-way Similarity Join Operator
271
Fig. 3. Multi-way similarity join intuition. A resource tuple arrives from one
of the source datasets. Auxiliary collections contain intermediate join result obtained
from respective sources. The incoming the tuple is semantically probed against a set
of auxiliary and main collections. The completed joinable result is put in the output
collection (ABCDE). New intermediate results are inserted into a respective collection,
e.g., a join between A and BC is stored in ABC.
Moreover, a distinctive feature of the approach consists in the probing mech-
anism. While traditional join operators search for a full syntactic matching of a
join variable instantiation, a similarity join operator performs a semantic com-
parison of the probed RDF molecules. The operator resorts to semantic simi-
larity functions, for example, GADES [13] that are able to deduce a numerical
value of relatedness of two molecules given an ontology. An ontology provides
additional knowledge and facts, logical axioms, class hierarchies necessary for
computation of a similarity score. If the resulting similarity value exceeds a
certain threshold then the molecules are considered similar and the join oper-
ator processes the given join variable instantiations as the same. For instance,

272
M. Galkin et al.
a semantic similarity function is able to compute a high score for <CCR> and
<Creedence Clearwater Revival> resources. Therefore, the join operator is
able to identify a join among the molecules that contain such instantiations
and yield a complete result as aimed in the example in Fig. 2b. The threshold
value for a semantic similarity function might be chosen either manually, i.e.,
after analyzing the distribution of similarity scores in order to keep the balance
between number of possible candidates and their overall relatedness, or might be
learned automatically if the function resorts to machine learning algorithms [8].
3.1
Query Plan Tree Height Reduction
Managing arbitrary arity allows the query optimizer to identify tree plans of
lower height. This property of MSimJoin-based plans is illustrated in Fig. 1.
MSimJoin joins triple patterns that share the same join variable and pushes
the result further to the tree plan. At higher levels binary joins still have to
be performed. Therefore, Y MSimJoin operators are considered as Y inputs
for binary joins which leads to the following tree height reduction. Given Y
MSimJoin operators of arity in range [n; k] in a query plan P, the height of P
converges to ⌈log2(Y ) + 1⌉in the best case, i.e., comprising a bushy tree plan
of MSimJoins, and to Y in the worst case of a left-linear plan. The resulting
height h is thus lying in the range ⌈log2(Y ) + 1⌉≤h ≤Y . Note that arity of
MSimJoin operators does not aﬀect the resulting height. For instance, two ﬁve-
way MSimJoins in Fig. 1d comprise a bushy plan of height h = ⌈log22 + 1⌉= 2.
On the other hand, binary joins are not able to complete the join of Y
[n; k] sources within one step. Therefore, for binary join plans there exists an
additional sub-tree that increases the tree plan height, i.e., the sub-tree of [n; k]
sources that share the same join variable. In the best case of a bushy tree plan
(cf. Fig. 1b) the additional height ha converges to ha = ⌈log2(n)⌉. In the worst
case of a left linear plan the additional height converges to ha = k−1 as depicted
in Fig. 1c. Having in mind that the ﬁrst layer when computing MSimJoin tree
height is already included in ha the overall height of a binary plan tree equals
to h = ⌈log2(n)⌉+ ⌈log2(Y )⌉in the best case and h = k + Y −2 in the worst
case when both additional and main components comprise a left linear join plan.
For instance, a bushy tree-like variant of a plan in Fig. 1b has a total height of
⌈log25⌉+ ⌈log22⌉= 4, whereas the height of another bushy tree-like option with
a left linear additional component in Fig. 1c equals to h = 5 −1 + ⌈log22⌉= 5.
Clearly, application of MSimJoin is able to reduce the query plan tree height.
4
Related Work
Existing federated query engines [1,2,11,14] that process SPARQL queries
employ various binary join algorithms and construct query plans complying
with the binary joins. ANAPSID [2] is an adaptive query engine that reacts
on possible data source delays or blocks; it is able to adjust query execution

Towards a Multi-way Similarity Join Operator
273
schedulers accordingly. ANAPSID employs a range of binary join operators and
deﬁnes two own operators, namely agjoin and adjoin. FedX [11] collects meta-
data about the given source sending auxiliary SPARQL queries. FedX resorts
to nested loop binary joins as well. TPF [14] introduces an original web access
interface based on direct triple patterns requests and build query plans based on
nested loop binary joins. nLDE [1] extends a TPF query mechanism involving a
binary gjoin operator in the query processing. All the approaches employ binary
join operators, whereas our approach supports a multi-way setup with numerous
inputs.
On the other hand, applications of semantic similarity functions in SPARQL
query processing received little attention. Instead, there exist techniques that
apply string similarity algorithms [4,6,16], set similarity algorithms [7,9], and
graph similarity techniques [12,17]. However, such algorithms have never been
applied to SPARQL. In contrast, our approach resorts to semantic similarity
mechanisms in order to deduce relatedness between join variable instantiations,
and thus achieving higher query completeness.
5
Conclusion
We presented an approach towards a multi-way similarity join operator,
MSimJoin, that tackles data duplication and redundancy by applying seman-
tic similarity mechanisms. The operator is able to both support numerous input
streams and compute similarity scores among received resources. The future work
aims at implementing a physical multi-way semantic similarity join operator and
evaluating it against state of the art benchmarks and real RDF datasets.
References
1. Acosta, M., Vidal, M.-E.: Networks of linked data eddies: an adaptive web query
processing engine for RDF data. In: Arenas, M., et al. (eds.) ISWC 2015. LNCS,
vol. 9366, pp. 111–127. Springer, Cham (2015). doi:10.1007/978-3-319-25007-6 7
2. Acosta, M., Vidal, M.-E., Lampo, T., Castillo, J., Ruckhaus, E.: ANAPSID: an
adaptive query processing engine for sparql endpoints. In: Aroyo, L., Welty, C.,
Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., Blomqvist, E. (eds.)
ISWC 2011. LNCS, vol. 7031, pp. 18–34. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-25073-6 2
3. Buil-Aranda, C., Arenas, M., Corcho, O., Polleres, A.: Federating queries in
SPARQL1.1: syntax, semantics and evaluation. Web Semant. Sci. Serv. Agents
World Wide Web 18, 1–17 (2013)
4. Feng, J., Wang, J., Li, G.: Trie-join: a trie-based method for eﬃcient string simi-
larity joins. VLDB J. 21(4), 437–461 (2012)
5. Fern´andez, J.D., Llaves, A., Corcho, O.: Eﬃcient RDF interchange (ERI) format
for RDF data streams. In: Mika, P., et al. (eds.) ISWC 2014. LNCS, vol. 8797, pp.
244–259. Springer, Cham (2014). doi:10.1007/978-3-319-11915-1 16
6. Li, G., Deng, D., Wang, J., Feng, J.: Pass-join: a partition-based method for simi-
larity joins. PVLDB 5(3), 253–264 (2011)

274
M. Galkin et al.
7. Mann, W., Augsten, N., Bouros, P.: An empirical evaluation of set similarity join
techniques. PVLDB 9(9), 636–647 (2016)
8. Morales, C., Collarana, D., Vidal, M.-E., Auer, S.: MateTee: a semantic similar-
ity metric based on translation embeddings for knowledge graphs. In: Cabot, J.,
Virgilio, R., Torlone, R. (eds.) ICWE 2017. LNCS, vol. 10360, pp. 246–263.
Springer, Cham (2017). doi:10.1007/978-3-319-60131-1 14
9. Ribeiro, L.A., Cuzzocrea, A., Bezerra, K.A.A., Nascimento, B.H.B.: Incorporating
clustering into set similarity join algorithms: the SjClust framework. In: Hartmann,
S., Ma, H. (eds.) DEXA 2016. LNCS, vol. 9827, pp. 185–204. Springer, Cham
(2016). doi:10.1007/978-3-319-44403-1 12
10. Schmachtenberg, M., Bizer, C., Paulheim, H.: Adoption of the linked data best
practices in diﬀerent topical domains. In: Mika, P., et al. (eds.) ISWC 2014. LNCS,
vol. 8796, pp. 245–260. Springer, Cham (2014). doi:10.1007/978-3-319-11964-9 16
11. Schwarte, A., Haase, P., Hose, K., Schenkel, R., Schmidt, M.: FedX: optimization
techniques for federated query processing on linked data. In: Aroyo, L., Welty,
C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., Blomqvist, E. (eds.)
ISWC 2011. LNCS, vol. 7031, pp. 601–616. Springer, Heidelberg (2011). doi:10.
1007/978-3-642-25073-6 38
12. Shang, Z., Liu, Y., Li, G., Feng, J.: K-join: knowledge-aware similarity join. IEEE
Trans. Knowl. Data Eng. 28(12), 3293–3308 (2016)
13. Traverso, I., Vidal, M.-E., K¨ampgen, B., Sure-Vetter, Y.: Gades: a graph-based
semantic similarity measure. In: SEMANTiCS, pp. 101–104. ACM (2016)
14. Verborgh, R., Sande, M.V., Hartig, O., Herwegen, J.V., Vocht, L.D., Meester, B.D.,
Haesendonck, G., Colpaert, P.: Triple pattern fragments: a low-cost knowledge
graph interface for the web. J. Web Sem. 37–38, 184–206 (2016)
15. Vidal, M.-E., Castillo, S., Acosta, M., Montoya, G., Palma, G.: On the selec-
tion of SPARQL endpoints to eﬃciently execute federated SPARQL queries. In:
Hameurlain, A., K¨ung, J., Wagner, R. (eds.) Transactions on Large-Scale Data-
and Knowledge-Centered Systems XXV. LNCS, vol. 9620, pp. 109–149. Springer,
Heidelberg (2016). doi:10.1007/978-3-662-49534-6 4
16. Wandelt, S., Deng, D., Gerdjikov, S., Mishra, S., Mitankin, P., Patil, M., Siragusa,
E., Tiskin, A., Wang, W., Wang, J., Leser, U.: State-of-the-art in string similarity
search and join. SIGMOD Rec. 43(1), 64–76 (2014)
17. Wang, Y., Wang, H., Li, J., Gao, H.: Eﬃcient graph similarity join for information
integration on graphs. Front. Comput. Sci. 10(2), 317–329 (2016)

Workload-Independent Data-Driven
Vertical Partitioning
Nikita Bobrov1, George Chernishev1,2(B), and Boris Novikov1,2
1 Saint Petersburg University, Saint Petersburg, Russia
nikita.v.bobrov@gmail.com, chernishev@gmail.com, borisnov@acm.org
2 JetBrains Research, Prague, Czech Republic
http://www.math.spbu.ru/user/chernishev/
Abstract. Vertical partitioning is a well-explored area of automatic
physical database design. The classic approach is as follows: derive an
optimal vertical partitioning scheme for a given database and a work-
load. The workload describes queries, their frequencies, and involved
attributes.
In this paper we identify a novel class of vertical partitioning algo-
rithms. The algorithms of this class do not rely on knowledge of the work-
load, but instead use data properties that are contained in the workload
itself. We propose such algorithm that uses a logical scheme represented
by functional dependencies, which are derived from stored data. In order
to discover functional dependencies we use TANE — a popular functional
dependency extraction algorithm. We evaluate our algorithm using an
industrial DBMS (PostgreSQL) on number of workloads. We compare the
performance of an unpartitioned conﬁguration with partitions produced
by our algorithm and several state-of-the-art workload-aware algorithms.
Keywords: Physical design · Vertical partitioning · Functional depen-
dency
1
Introduction
Vertical partitioning is a technique which aims to improve a database perfor-
mance by reducing the amount of data to be read from disk. This problem was
of interest to database community since the earliest days. Despite being old,
the problem of automatic selection of a vertical partitioning scheme is not yet
solved. A database administrator still plays the critical role in the selection of
this type of data layout.
However, a signiﬁcant amount of experience was accumulated, a large number
of approaches was developed. In overall there were dozens of studies published.
This number of studies is justiﬁed by the fact that an optimal solution is an
NP-hard problem for many diﬀerent formulations of the vertical partitioning
This work is partially supported by Russian Foundation for Basic Research grant
16-57-48001.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 275–284, 2017.
DOI: 10.1007/978-3-319-67162-8 27

276
N. Bobrov et al.
problem [2,20,29]. The number of diﬀerent vertical partitioning schemes for a
single table is equal to the Bell number of attributes [15]. Thus, the algorithms
proposed in these studies are approximate.
All studies use one and the same problem formulation: for a given database
and a given workload ﬁnd the vertical partitioning scheme that optimizes some
performance metric. The metric is usually represented by a response time or a
throughput and the database is represented by a single table or a group of tables.
The workload is the information regarding queries, their frequencies, involved
attributes, arrival patterns, and so on.
However, there are scenarios when vertical partitioning is required, but
queries are not known, or known but can not be used for tuning. These scenarios
lead to a novel class of vertical partitioning algorithms, that has the following
properties:
Workload-independence states that the algorithm does not rely on the knowl-
edge of a workload. There are several cases [13] when a workload is not avail-
able or hard to identify, for example proxy database caching [22]. Another
example is the case when there are errors in physical parameter estimations
that make the knowledge of the workload useless.
Data-driven property states that the algorithm pays special attention to the
stored data. The data contains semantic information that can be exploited
for vertical partitioning scheme selection. The semantic information is largely
ignored in the existing works on automatic physical database tuning. The
only semantic information that is commonly employed is the one discovered
and used by a query optimizer during “what-if” invocations. However, it is
employed only during conﬁguration assessment and the algorithms do not
make use of it directly.
The goal of this study is to construct a workload-independent data-driven
vertical partitioning algorithm. The core idea of our approach is to exploit the
logical scheme of a database by extracting the functional dependencies from
database tables and use them to perform partitioning.
In our previous study [4] we have evaluated TANE — an algorithm for func-
tional dependency extraction. Unlike other evaluations we studied precision-
related metrics of this algorithm. We have demonstrated algorithm suitability
for the construction of the workload-independent data-driven advisor — the
results were surprisingly good for the approximate algorithm. In this paper we
continue our work and use TANE for a functional dependency extraction stage
of our partitioning algorithm.
The contributions of this paper are the following:
– We identify a novel class of vertical partitioning algorithms that does not
include a workload as its component. We justify its existence by describing
several use-cases when such an algorithm will be useful.
– We develop a vertical partitioning algorithm which uses functional dependen-
cies embedded in data to produce a partitioning scheme.
– We perform an evaluation of our approach by comparing our algorithm not
only with unpartitioned conﬁgurations, but also with several state-of-the-art
workload-aware algorithms.

Workload-Independent Data-Driven Vertical Partitioning
277
2
Related Work
There are dozens of vertical partitioning studies. All of them followed the same
approach: for a given database and a workload derive an optimal vertical parti-
tioning scheme.
One can classify these works into two types [7,8]:
1. In heuristic approach [6,17,20,24] some procedure is used to construct the
partitioning scheme. This procedure does not use any cost-function, instead
it uses some assumptions regarding the solution. This approach was popular
in the 80s-90s, then it was abandoned in favor of the cost-based one due to
the low quality of solutions. However, recently appeared a number of promis-
ing applications [12,13], like an on-line vertical partitioning [19,28], where a
database adapts its vertical partitioning schema on the ﬂy.
2. In cost-based approach [9,15,21,25,26] a cost function and an enumeration
algorithm are devised and used to construct a partitioning scheme. The cost
function uses various workload and hardware parameters to estimate beneﬁts
of the candidate partitioning scheme. Currently, this is the most popular
approach due to quality and reliability of results. The most active subarea is
the vertical partitioning for in-memory systems [14,16].
Several studies [1,23] also adopt mixed approach, where poor conﬁgurations
are pruned by heuristics followed by a cost-based enumeration. Initially, the
popularity of heuristic and mixed studies was justiﬁed by the tiny computational
resources available in the 80s. Nevertheless, some of the recent studies [1] still
employ it.
We do not discuss these approaches in detail due to space constraints, we
refer an interested reader to surveys [7,8].
Another physical design aspect that needs to be mentioned is usage of logical
scheme information. Currently, this aspect is largely ignored by the physical
design research community. There is only a handful of studies which employ it
[5,10,26,27,30] (see a more detailed survey in [4]). At the same time, exploitation
of the information hidden in data may allow to signiﬁcantly improve quality of
the obtained conﬁgurations.
Finally, a recent study [18] presents a comparison of several classic vertical
partitioning algorithms. Authors conclude with the following — “HillClimb is the
best algorithm for disk-based systems”. In our paper we rely on ﬁndings of this
comparison: we use HillClimb [16] and Navathe’s algorithms [23] for comparison
of our algorithm with the workload-aware approach.
3
Heuristic Aﬃnity-Based Vertical Partitioning
One of the most popular heuristic approaches is the aﬃnity-based one [7], which
is as follows:
1. Construct an Attribute Usage Matrix (AUM) using a workload. The AUM is
a matrix which records usage of each attribute by each query. It stores zero on
(i, j) position if the attribute i is not used in the query j and one otherwise.

278
N. Bobrov et al.
2. Transform the AUM into Attribute Aﬃnity Matrix (AAM), a symmetric
square matrix. The elements of this matrix denote the degree of closeness
between two attributes. The transformation algorithm is speciﬁc to each
study.
3. Generate a vertical partitioning scheme from the AAM using a matrix clus-
tering [6,17,23], graph-based approaches [20,24], and others [7].
This approach was used not only for vertical partitioning in relational data-
bases, but also for various physical design problems [3,11]. In this study we use
mined functional dependencies inside the aﬃnity-based approach.
4
Partitioning Algorithm
The algorithm is presented in the Listing 1. Its input is the AAM built using
the functional dependencies extracted from the source data (a single table). To
extract functional dependencies we use the TANE algorithm, which returns a
set of functional dependencies in the form of LHS →RHS (left and right hand
sides). TANE presents output in such a way that RHS consists only of one
attribute and LHS may contain several attributes. The algorithm is controlled
using the FdDepth parameter which speciﬁes the maximum number of attributes
that are allowed to appear in the LHS.
In order to construct F(N, N) — the AAM, ﬁrstly, functional dependencies
are extracted. Next, for each pair of attributes (i, j) the number of functional
dependencies such that i, . . . →j is calculated.
The lines 4–9 are used to detect attributes which can be considered key can-
didates. These attributes would be duplicated in each Ak and used to reconstruct
the original relation. In order to detect such attributes the algorithm scans the
whole column and checks whether it contains all zeroes. In other words it selects
all attributes which do not appear in RHS in the found dependencies.
Then, we begin to construct partitions in the loop (lines 12–29), until all
attributes are used. On each iteration we start with the highest aﬃnity (the
most “frequent”) among all unused attributes, i.e. max(F(N, N)). We put in
the MaxRelatioIndex RHS of this aﬃnity. Then we look at Top(FdDepth + 1)
attributes in a sorted MaxRelatioIndex column and add them to Relationr.
Consider this set of attributes as a tree, where MaxRelatioIndex is the root,
and MostFrequent are its children.
Next, for each of this child (lines 17–23), the same search and addition of
potential children is conducted, but only for Top(2) attributes. We also record
all NonFrequent attributes, which were not added to the relation, but their
ancestors are in it. After all children of the current MaxRelatioIndex were
considered and the partition is almost constructed, we check whether the Top(1)
of each NonFrequent attribute is in the relation (lines 25–27), and if this is
true — we add NonFrequent to the relation.

Workload-Independent Data-Driven Vertical Partitioning
279
Algorithm 1. FD-based algorithm
Data: Aﬃnity matrix F(N, N), where N – number of attributes in relation R;
F(i, j) – frequency of attribute i in functional dependencies with i in
LHS and j in RHS, e.g. i, . . . →j
Result: Set of partitions A1, . . . , An:

1≤k≤n
Ak = R and ∀i, jAi
 Aj = ø
1 begin
2
UsedAtts ←−{}, CandidateKeyAtts ←−{};
3
Key ←−True;
4
for i = 1 : N do
5
for j = 1 : N do
6
if F(j, i)! = 0 then
7
Key ←−False;
8
break;
end
end
9
if(Key) then CandidateKeyAtts ←−CandidateKeyAtts ∪{i};
end
10
UsedAtts ←−AllRelationAtts \ CandidateKeyAtts;
11
r ←−0;
12
while SizeOf(UsedAtts) < N do
13
MaxRelatioIndex ←−max(F(N, N));
14
MostFrequent ←−Top(FdDepth + 1) of MaxRelatioIndex column in
F(N, N);
15
Relationr ←−MostFrequent ∪MaxRelatioIndex;
16
NonFrequent ←−{};
17
for i ∈MostFrequent do
18
Nearesti ←−Top(2) of i column in F(N, N);
19
for j ∈Nearesti do
20
if j ∈Relationr then
21
Relationr ←−Relationr ∪{j};
22
UsedAtts ←−UsedAtts ∪{j};
else
23
NonFrequent ←−NonFrequent ∪{j};
end
end
end
24
for k ∈NonFrequent do
25
if Top(1)ofF(N, k) ∈Relationr then
26
Relationr ←−Relationr ∪{k};
end
end
27
Print(Relationr);
28
r ←−r + 1;
end
end

280
N. Bobrov et al.
5
Evaluation
In order to evaluate our approach we implemented the proposed algorithm and
used the PostgreSQL DBMS to evaluate the performance of partitioning schemes.
In our experiments an existing1 implementation of the TANE algorithm was used.
5.1
Experimental Setup and Evaluation Procedure
In our experiments the following hardware and software conﬁguration was used:
Intel(R) Core(TM) i5-4670K CPU 3.40 GHz (4 CPUs) RAM 16 GB, Ubuntu
Linux 16.04 (64 bit), PostgreSQL 9.5.6.
To prepare the experiments we had to select benchmarks which provide not
only data, but also queries. The following two benchmarks were used:
1. TPC-H — lineorder (primary) and lineitem, orders, customer, nation; syn-
thetic; 150K rows; 6 queries.
2. IOWA Liquor2: single table; real; 2.7 million rows; 22 attributes; 6 queries.
TPC-H is a well-known benchmark which oﬀers both data and queries. How-
ever, it already possess a well-designed data scheme that does not need vertical
partitioning. The queries in this benchmark also ﬁt this scheme and, thus, it
may be impossible to achieve any improvement at all.
In order to address this problem we have also selected the IOWA Liquor
benchmark. Unfortunately, this benchmark contains only data, but not queries.
Thus, we had to collect and adapt queries employed by users of the “bigquery”
subreddit3. However, we were not able to translate all the queries to PostgreSQL.
The resulting queries and their descriptions are presented on the website4.
The evaluation procedure was performed as follows:
1. Run our partitioning algorithm and populate the obtained vertical partition-
ing scheme with data. Note, that we do not use queries on this stage.
2. Using populated scheme run benchmark queries and record execution times.
3. Compare these times with:
(a) the ones obtained on unpartitioned conﬁguration;
(b) the ones obtained on conﬁgurations generated by the workload-aware
algorithms (HillClimb and Navathe’s);
Both experiments were conducted with the FdDepth parameter set to 4, an
empirically chosen value.
1 TANE implementation. http://www.cs.helsinki.ﬁ/research/fdk/datamining/tane/.
2 Iowa Liquor, https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy.
3 https://www.reddit.com/r/bigquery/comments/37fcm6/iowa liquor sales dataset
879mb 3million rows/?st=j3ppu30u\&sh=35bdeeb2.
4 http://www.math.spbu.ru/user/chernishev/papers/iowa queries.txt.

Workload-Independent Data-Driven Vertical Partitioning
281
5.2
Results
Our ﬁrst series of experiments used the IOWA Liquor dataset. The results are
presented in the Table 1 and the Fig. 1. Note the logarithmic axis in the Fig. 1.
We also used the bold font in our tables to indicate the best algorithm for
every query. The table presents query run times and the overall workload run
time measured in milliseconds. The column “Improvement” shows the relative
improvement of the partitioning scheme for the whole workload compared to the
baseline — the “Original” conﬁguration.
There are six queries in this benchmark, in our evaluation each query was
benchmarked individually. From this experiment you can see that for every query
any partitioned conﬁguration performs better than the unpartitioned one.
Another observation is the following: the cost-based workload-aware algo-
rithms perform better than our algorithm. This is not surprising, since these
algorithms employ a workload and a cost model. Though, it is interesting to
note that the workload-aware algorithms outperform our algorithm 1.5–2 times.
The outcome of this experiment also corroborates Jindal et al. results [18]: Hill-
Climb algorithm is superior to Navathe’s.
The second series of experiments involved the TPC-H benchmark. You can
see the results in the Table 2 and the Fig. 2. Here we also use the logarithmic
axis.
Q1
Q2
Q3
Q4
Q5
Q6
103
104
105
Time (ms)
Relation
Original
Navathe
HillClimb
FD-based
Fig. 1. Query time execution, Iowa Liquor
Table 1. IOWA results
Relation
Q1
Q2
Q3
Q4
Q5
Q6
Overall
Run Time
Improvement
Original
8325
69544
18568
1658
1100
105168
204363
—
—
Navathe’s
6574
53669
12681
1635
635
90285
165479
30671
19.0%
HillClimb
3729 51550
10736 1167
382
87356 154920
47
24.2%
FD-based
4721
61996
18118
1436
930
90297
177498
21
13.1%

282
N. Bobrov et al.
Q1
Q3
Q4
Q10
Q12
Q18
102
104
Time (ms)
Relation
Original
Navathe
HillClimb
FD-based
Fig. 2. Query time execution, TPC-H
Table 2. TPC-H results
Relation
Q1
Q3
Q4
Q10
Q12
Q18
Overall
Run Time
Improvement
Original
414 182
320
7123
231
35885
44155
—
—
Navathe’s
9248
141 101
2993 185
27232
39900
44157
9.3%
HillClimb
9289
142
122
3004
51
29643
42251
128
4.3%
FD-based
10614
323
51
6125
18
42192
59323
190
-34.3%
The TPC-H is a synthetic benchmark with several of its queries spanning
multiple tables. In this experiment we select the primary table (lineorder) which
is going to be partitioned. Secondary tables (lineitem, orders, customer, and
nation) are left intact.
This series yields surprising results. First of all, note that the Q1 performance
dropped more than 20 times for all of the approaches. The queries Q3, Q4,
and Q12 contribute negligible costs in the overall result and should be ignored.
The workload-aware algorithms managed to reduce processing costs for both
Q10 and Q18 — the most resource-consuming queries in the workload. At the
same time our algorithm produced partitioning scheme which was only 14%
better than the original one, while the cost-based approaches produced the 57%
improvement. The query Q18 produced a discouraging result for our approach,
it increased the processing times for the most costly query almost 7%. In overall,
our algorithm produced partitioning scheme which was worse than the original
one, while the workload-aware algorithms succeeded. However, another surprise
was the superiority of Navathe’s algorithm over the HillClimb: it achieved twice
the improvement than the HillClimb.
We have also evaluated the time it takes to produce a solution by each of
the considered algorithms. It is presented in the “Run Time” column in Tables 1
and 2. Here, we can see that the HillClimb and our algorithms are approximately
equal. Moreover, this time is negligible compared to the run times of the work-
load. On the other hand, Navathe’s algorithm exhibits signiﬁcant run times for
both benchmarks.

Workload-Independent Data-Driven Vertical Partitioning
283
6
Conclusions
In this paper we have introduced a notion of the workload-independent data-
driven vertical partitioning. We have identiﬁed the new class of vertical partition-
ing algorithms, that possess these properties. We have outlined scenarios when
such algorithms can be of use. Next, we have described an algorithm belonging
to this class. It employs functional dependencies mined from the data to con-
struct a partitioning scheme. Finally, we have performed an evaluation using the
PostgreSQL DBMS for both real and synthetic benchmarks.
Our algorithm succeeded in generating beneﬁcial partitioning scheme for the
benchmark that had no predeﬁned data schema. As expected the algorithm
produces solutions of slightly worse quality than that of the workload-aware ones.
However, it has much wider application area: it can operate in some scenarios
when workload-aware algorithms can not, i.e. when no workload knowledge is
available.
Acknowledgments. We would like to thank anonymous reviewers for their valuable
comments on this work. This work is partially supported by Russian Foundation for
Basic Research grant 16-57-48001.
References
1. Agrawal, S., Narasayya, V., Yang, B.: Integrating vertical and horizontal parti-
tioning into automated physical database design. In: SIGMOD 2004, pp. 359–370.
ACM, 2004
2. Apers, P.M.G.: Data allocation in distributed database systems. ACM Trans. Data-
base Syst. (TODS) 13(3), 263–304 (1988)
3. Bellatreche, L., Benkrid, S.: A joint design approach of partitioning and allocation
in parallel data warehouses. In: Pedersen, T.B., Mohania, M.K., Tjoa, A.M. (eds.)
DaWaK 2009. LNCS, vol. 5691, pp. 99–110. Springer, Heidelberg (2009). doi:10.
1007/978-3-642-03730-6 9
4. Bobrov, N., Chernishev, G., Grigoriev, D., Novikov, B.: An evaluation of TANE
algorithm for functional dependency detection. In: Ouhammou, Y., et al. (eds.)
MEDI 2017. LNCS, vol. 10563, pp. 208–222. Springer International Publishing,
Cham (2017). doi:10.1007/978-3-319-66854-3 16
5. Boehm, A.M., Seipel, D., Sickmann, A., Wetzka, M.: Squash: a tool for analyzing,
tuning and refactoring relational database applications. In: Seipel, D., Hanus, M.,
Wolf, A. (eds.) INAP/WLP -2007. LNCS (LNAI), vol. 5437, pp. 82–98. Springer,
Heidelberg (2009). doi:10.1007/978-3-642-00675-3 6
6. Cheng, C.-H.: A branch and bound clustering algorithm. IEEE Trans. Syst. Man
Cybern. 25, 895–898 (1995)
7. Chernishev, G.: A survey of dbms physical design approaches. SPIIRAS Proceed-
ings 24, 222–276 (2013)
8. Chernishev, G.: The design of an adaptive column-store system. J. Big Data 4(5),
25 (2017)
9. Cornell, D., Yu, P.: An eﬀective approach to vertical partitioning for physical design
of relational databases. IEEE Trans. SE 16, 248–258 (1990)

284
N. Bobrov et al.
10. De Marchi, F., Lopes, S., Petit, J.-M., Toumani, F.: Analysis of existing databases
at the logical level: the DBA companion project. SIGMOD Rec. 32, 47–52 (2003)
11. Fung, C.-W., Karlapalem, K., Li, Q.: Cost-driven vertical class partitioning for
methods in object oriented databases. VLDB J. 12, 187–210 (2003)
12. Galaktionov, V., Chernishev, G., Novikov, B., Grigoriev, D.: Matrix clustering
algorithms for vertical partitioning problem: an initial performance study. In:
DAMDID/RCDL 2016, Russia, pp. 24–31 (2016)
13. Galaktionov, V., Chernishev, G., Smirnov, K., Novikov, B., Grigoriev, D.A.: A
study of several matrix-clustering vertical partitioning algorithms in a disk-based
environment. In: Kalinichenko, L., Kuznetsov, S.O., Manolopoulos, Y. (eds.) DAM-
DID/RCDL 2016. CCIS, vol. 706, pp. 163–177. Springer, Cham (2017). doi:10.
1007/978-3-319-57135-5 12
14. Grund, M., Kr¨uger, J., Plattner, H., Zeier, A., Cudre-Mauroux, P., Madden, S.:
HYRISE: a main memory hybrid storage engine. Proc. VLDB Endow. 4, 105–116
(2010)
15. Hammer, M., Niamir, B.: A heuristic approach to attribute partitioning. In: SIG-
MOD 1979, pp. 93–101 (1979)
16. Hankins, R.A., Patel, J.M.: Data morphing: an adaptive, cache-conscious storage
technique. In: VLDB 2003, pp. 417–428 (2003)
17. Hoﬀer, J.A., Severance, D.G.: The use of cluster analysis in physical data base
design. In: VLDB 1975, pp. 69–86 (1975)
18. Jindal, A., Palatinus, E., Pavlov, V., Dittrich, J.: A comparison of knives for bread
slicing. Proc. VLDB Endow. 6, 361–372 (2013)
19. Li, L., Gruenwald, L.: SMOPD: a vertical database partitioning system with a fully
automatic online approach. In: IDEAS 2013, pp. 168–173 (2013)
20. Lin, X., Orlowska, M., Zhang, Y.: A graph based cluster approach for vertical
partitioning in database design. Data Knowl. Eng. 11, 151–169 (1993)
21. Ma, H., Schewe, K.-D. Kirchberg, M.: A heuristic approach to fragmentation incor-
porating query information. In: Databases and Information Systems IV - Selected
Papers from the Seventh International Baltic Conference, DB&IS 2006, Vilnius,
Lithuania, 3–6 July 2006. Frontiers in Artiﬁcial Intelligence and Applications, vol.
155. IOS Press (2006). ISBN 978-1-58603-715-4
22. Malik, T., Wang, X., Burns, R., Dash, D., Ailamaki, A.: Automated physical design
in database caches. In: ICDEW 2008, pp. 27–34 (2008)
23. Navathe, S., Ceri, S., Wiederhold, G., Dou, J.: Vertical partitioning algorithms for
database design. ACM Trans. Database Syst. 9, 680–710 (1984)
24. Navathe, S., Karlapalem, K., Ra, M.: A mixed fragmentation methodology for
initial distributed database design. J. Comput. Softw. Eng. 3(4) (1995)
25. Pai-Cheng, C.: A transaction-oriented approach to attribute partitioning. Inf. Syst.
17, 329–342 (1992)
26. Papadomanolakis, S., Ailamaki, A.: AutoPart: automating schema design for large
scientiﬁc databases using data partitioning. In: SSDBM 2004, pp. 383–392 (2004)
27. Qian, L., LeFevre, K., Jagadish, H.V.: CRIUS: user-friendly database design. Proc.
VLDB Endow. 4, 81–92 (2010)
28. Rodr´ıguez, L., Li, X.: A dynamic vertical partitioning approach for distributed
database system. In: SMC 2011, pp. 1853–1858 (2011)
29. Sacca, D., Wiederhold, G.: Database partitioning in a cluster of processors. ACM
Trans. Database Syst. 10, 29–56 (1985)
30. Wiese, D., Rabinovitch, G., Reichert, M., Arenswald, S.: Autonomic tuning expert:
A framework for best-practice oriented autonomic database tuning. In: CASCON
2008, pp. 327–341 (2008)

Can SQ and EQ Values and Their Diﬀerence
Indicate Programming Aptitude to Reduce
Dropout Rate?
Juris Borzovs(B), Natalija Kozmina(B), Laila Niedrite(B),
Darja Solodovnikova(B), Uldis Straujums(B), Janis Zuters(B), and Atis Klavins
Faculty of Computing, University of Latvia, Raina blvd. 19, Riga, Latvia
{juris.borzovs,natalija.kozmina,laila.niedrite,
darja.solodovnikova,uldis.straujums,janis.zuters}@lu.lv
Abstract. A crucial problem that we are currently facing at the Faculty
of Computing of the University of Latvia is that during the ﬁrst study
semester on average 30% of the ﬁrst-year students drop out, whereas
after the ﬁrst year of studies the number of dropouts increases up to
nearly 50%. Thus, our overall goal is to determine in advance appli-
cants that most likely will not ﬁnish the ﬁrst study year successfully.
A hypothesis formulated in another research study was that program-
ming aptitude could be predicted based on the results of two personality
self-report questionnaires −Systemizing Quotient (SQ) and Empathy
Quotient (EQ) −taken by students. The diﬀerence between the SQ and
EQ scores had a strong correlation with grades received for programming
test. We reproduced the circumstances of mentioned empirical study with
our ﬁrst-year students using similar tests to calculate SQ and EQ, and
semester grades in introductory programming course as a quantitative
measure to evaluate programming ability. In this paper, we elaborate on
the empirical setting, measures, and estimation methods of our study,
which produced the results that made us call the stated hypothesis into
question and disprove it.
Keywords: Programming aptitude · Systemizing quotient · Empathy
quotient · Correlation · Dropout rate
1
Introduction and Motivating Example
A notable number of students leave the universities in their ﬁrst years of study,
especially at the engineering and computer science programs. This has been
observed in higher education institutions in many countries for a long period of
time, and remains an acute issue also in recent years. In last years, there is a
persistent trend that up to half of the students drop out during their ﬁrst year
of studies at the Faculty of Computing at the University of Latvia. In its turn, it
served us as an impulse to search for a method on how to determine in advance
applicants that will not ﬁnish the ﬁrst study year successfully. This problem
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 285–293, 2017.
DOI: 10.1007/978-3-319-67162-8 28

286
J. Borzovs et al.
becomes especially important during the programs’ evaluations, because the high
dropout rate is often associated with a low study quality, meanwhile, the experts
evaluating the study programs ask questions on what has been done to reduce
the attrition. However, the attrition causes found by other researchers [1,4,11,14]
are very diﬀerent depending on the country and often are not connected with the
quality of study programs. Also, attrition causes vary for economic reasons, e.g.
high study fee and living expenses for students till psychological reasons such as
adaptation problems to the university environment or lack of information about
the chosen study program when the students face with subjects that they are
not interested in.
The Faculty of Computing at the University of Latvia oﬀers ﬁrst level higher
education Professional study program and Bachelor’s study program. We accept
applications of 260 new students annually, but during the ﬁrst study semester
on average 30% of them drop out. After the ﬁrst year of studies the number
of dropouts increases up to nearly 50%. Therefore, to investigate the dropout
causes, we have performed a number of studies, including the analysis of students’
data from admission of the year 2013, to ﬁnd what characterizes the dropouts
[7]. Several factors were studied: high school grades, intermediate grades for
core courses at the university level, and prior knowledge of programming. The
conclusions of the study were that none of the studied factors is determinant to
identify the students who will drop out. The results also showed that many of
the dropouts did not really begin their studies. Thus, we concluded that planned
activities should be performed before the admission, for instance, providing more
information about the contents of the program or about the profession of the
programmer for prospective students. There should be oﬀered a possibility to
evaluate students’ potential to study computer science.
Our second study [6] was performed to systematize the existing approaches
and ﬁnd out the most promising ones that provide methods of how to deter-
mine the programming aptitude of the prospective students. Searching in Google
Scholar by ‘computer programming aptitude test’ search string, at least 60 publi-
cations for the period from 1960 to 2014 were obtained. Unfortunately, in neither
of them including the latest publications an acceptable solution was found.
All existing approaches to determine computer programming aptitude could
be divided into two categories. The ﬁrst ones are based on the psychological
tests [5,20]. The rest are based on solving speciﬁcally designed non-programming
tasks [9,13,17]. In practice, hybrid tests that contain elements from both groups
of tests are often employed [18]. According to our conclusions, a summary of the
most promising psychological and problem solving tests should be oﬀered to the
prospective students for self-evaluation purposes [5,18,20].
We have decided to take a closer look at the most promising psychological
tests found during our previous study, namely, the tests in Wray’s research [20].
We performed case studies using Wray’s approach to compare acquired results
with the original study before preparing a self-test for programming aptitude
evaluation for all prospective students. Our ﬁrst attempt was reﬂected in a stu-
dent’s Master thesis [12] where schoolchildren were interviewed and the results
of their tests were analysed. However, the results were in contradiction with [20],

Can SQ and EQ Values and Their Diﬀerence
287
so we decided to repeatedly apply the tests for the ﬁrst-year students in the year
2016. The results of the case study are presented in this paper.
The rest of the paper is organized as follows: Sect. 2 gives an overview of the
related work, our experimental study and its results are presented in Sect. 3, and
Sect. 4 ﬁnalizes the paper with conclusions and future work.
2
Related Work
Bishop-Clark and Wheeler [5] have based their study on Myers-Briggs test avail-
able online at [8] to determine which types of college students have better results
in the programming introductory course. Myers-Briggs test evaluates 4 aspects of
personality such as (1) general attitude: Extraverted vs. Introverted; (2) percep-
tion function: Sensing vs. Intuition; (3) judging function: Thinking vs. Feeling;
(4) perception−judging domination: Judging vs. Perceiving. The study [5] con-
cludes that “sensing students performed better on programming assignments
than intuitive students, and that judging students achieved higher programming
averages than perceptive students”. Nevertheless, our goal is to determine the
potential dropouts, so these results cannot be used directly for this purpose.
The paper [20] discusses an experiment carried out among 19 students who
took a programming module of BSc(hons) course in Telecommunications Sys-
tems Engineering. The experiment tested a hypothesis that programming apti-
tude could be predicted based on the results of diﬀerent tests taken by students.
Students who took part in the experiment completed a programming test after
ﬁnishing a programming module and after 5 months were invited to take 4 tests
that could potentially predict programming aptitude.
The best correlation between test results and programming ability deter-
mined by the programming test was obtained for the diﬀerence between the
scores of two personality self-report questionnaires Systemizing Quotient (SQ)
[2] and Empathy Quotient (EQ) [3] produced by the Autism Research Group at
Cambridge. SQ measures the interest of a person in systems of objects and EQ
determines the ability of a person to interact with other people. The diﬀerence
SQ−EQ is suggested to have a link with autistic traits in a person.
Both questionnaires are composed of 60 questions, 40 out of them are used
to determine empathizing or systemizing ability and 20 ﬁller questions are not
scored and added to distract respondent from paying too much attention to
empathizing or systemizing. Each question is expressed in a form of a statement
and has 4 alternatives that a respondent can choose: “strongly agree”, “slightly
agree”, “slightly disagree”, and “strongly disagree”. A score 1 or 2 is assigned
depending on the chosen alternative and the sum of scores calculated out of all
questions is in range from 0 to 80.
The results of the study [20] revealed that each quotient separately had a low
correlation with the programming test, but their diﬀerence had a strong corre-
lation, therefore, the author concluded that these tests could be eﬀectively used
to predict programming aptitude of students before teaching a programming
course.

288
J. Borzovs et al.
3
An Empirical Study
The goal of our empirical study was to reproduce the method applied in [20] to
either conﬁrm or disprove the assumption stated in the paper. More precisely,
we wanted to check if there is a strong correlation between combination of SQ
and EQ values (i.e. SQ−EQ) as well as a moderate correlation between SQ and
EQ individually and students’ grades for the test in programming. In our case,
though the methodology of the experimental study is the same, the context and
population sample diﬀers from that in [20].
3.1
Settings of the Experimentation
Population sample for this study consists of 73 students (29 female and 44 male
students) who were randomly selected of out of 260 ﬁrst-year students of the
Faculty of Computing at the University of Latvia. The students were oﬀered to
ﬁll in two personality questionnaires −SQ [16] and EQ [10] surveys produced
by the Autism Research Group at Cambridge and available online −to assess
systemizing and empathizing respectively. Both tests were translated into Lat-
vian language by the author of [12] and were available online during the time
period from September 23rd to October 9th of 2016 via Google Docs.
Since the population of our study includes many females, we used a revised
version of the Systemizing Quotient [19] with 75 questions in our experimental
study, where items from the original SQ questionnaire with 60 questions that
is considered male-oriented were supplemented by additional questions that are
more relevant to females in general population to avoid bias of study results.
The EQ test contains 60 questions, 40 of which are rated with points, whereas
20 remaining questions are not since they are meant for distraction. Each of
the test questions has 4 diﬀerent outcomes −“strongly agree”, “slightly agree”,
“slightly disagree”, and “strongly disagree” −which correspond to 2, 1, or 0
points depending on the question. Keys for SQ and EQ tests are available online
[10,16]. We created a mapping between students’ answers and key values, and
calculated the total sum of all points for SQ and EQ tests.
We have taken the ﬁnal grade for “Programming I” course that the students
received at the end of fall semester (January, 2017) as a quantitative measure
to evaluate programming ability. Course grade is the sum of multiple grades
that include at-home practical tasks (40% of the grade), 7–10 min long in-class
written tests (20% of the grade), a compulsory written exam (30% of the grade),
and non-obligatory tasks with higher complexity (10% of the grade).
The distribution of ﬁnal grades for all 73 students is given in Fig. 1. Shapiro-
Wilks normality test returns a p-value of 1.038e-06 meaning that grade data is
not normally distributed; the dataset can be classiﬁed as multi-model distribu-
tion with 3 peaks −x = 0, 4, 7. The most frequent grade is 7, which was received
by 14 students. Also, there were 12 students who hadn’t accomplished any task
and, thus, didn’t pass the course. We did not exclude such students’ data from
our dataset to check whether there is a correlation between those who failed and
do not have appropriate programming skills and their corresponding SQ, EQ,

Can SQ and EQ Values and Their Diﬀerence
289
0
1
2
3
4
5
6
7
8
9
10
grade
count
0
4
8
12
Fig. 1. Distribution of the ﬁnal grades of 73 students.
SQ−EQ values. In addition, grade values are ordinal data, which implies that
to compute correlation coeﬃcient we should use Spearman’s rank (ρ).
3.2
Data Analysis
A preliminary step of this research was a study on exploring correlation between
SQ, EQ, SQ−EQ values and semester grades received by schoolchildren [12].
The whole experimentation procedure was the same as described above. The
population samples included 30 pupils of 11th form (14 boys and 16 girls) and 20
pupils of 12th form (11 boys and 9 girls). For pupils of 11th form, SQ correlation
was very weak (≤0.04 for either boys or girls), EQ correlation was classiﬁed as
negative and weak (−0.24 for boys and −0.28 for girls), while SQ−EQ correlation
was positive and weak (0.31 for boys and 0.37 for girls). For pupils of 12th form,
correlation was classiﬁed as very weak (between 0.03 and 0.12) for all values
and categories of participants excluding SQ correlation values for girls −in
this group there was a weak positive correlation (0.24). Thus, the results of
the preliminary study seemed to arise a contradiction with the statement put
forward in [20] −namely, we didn’t spot a tendency of supporting the hypothesis
of positive correlation between SQ−EQ values and semester grades. Moreover,
the preliminary study served as a motivation for us to conduct another empirical
study that would yield valuable results for the University of Latvia by analyzing
data of the ﬁrst-year students of the Faculty of Computing.
The resulting dataset of the current empirical study consisting of student
ID, gender, SQ, EQ, SQ−EQ, and grade values was loaded into RStudio [15] for
further statistical analysis. We formulated null hypothesis that we would like to
verify in term of our empirical study as H0: There is no relationship between
SQ, EQ, or SQ−EQ, and the ﬁnal grade for “Programming I” course.
We built multiple boxplots with ﬁnal grade data as independent variable (x)
and SQ, EQ, SQ−EQ as dependent variable (y). Acquired boxplots for SQ, EQ,
and SQ−EQ are presented in Figs. 2, 3, and 4 respectively. Each ﬁgure includes
3 plots that contain as an input ﬁnal grade data: (a) for all students, (b) for
male students only, and (c) for female students only.

290
J. Borzovs et al.
25
50
75
100
125
0
1
2
3
4
5
6
7
8
9
10
grade
SQ
(a) SQ values for all students
25
50
75
100
125
0
1
2
3
4
5
6
7
8
9
10
grade
SQ
(b) SQ values for male students
40
60
80
100
120
0
1
2
3
4
5
6
7
8
9
grade
SQ
(c) SQ values for female students
Fig. 2. Boxplots representing students’ SQ values plotted against the ﬁnal grade.
20
40
60
0
1
2
3
4
5
6
7
8
9
10
grade
EQ
(a) EQ values for all students
20
30
40
50
60
0
1
2
3
4
5
6
7
8
9
10
grade
EQ
(b) EQ values for male students
20
40
60
0
1
2
3
4
5
6
7
8
9
grade
EQ
(c) EQ values for female students
Fig. 3. Boxplots representing students’ EQ values plotted against the ﬁnal grade.

Can SQ and EQ Values and Their Diﬀerence
291
−30
0
30
60
0
1
2
3
4
5
6
7
8
9
10
grade
SQ − EQ
(a) SQ−EQ values for all students
−30
0
30
60
0
1
2
3
4
5
6
7
8
9
10
grade
SQ − EQ
(b) SQ−EQ values for male students
−30
0
30
60
0
1
2
3
4
5
6
7
8
9
grade
SQ − EQ
(c) SQ−EQ values for female students
Fig. 4. Boxplots representing students’ SQ−EQ values plotted against the ﬁnal grade.
There is a summary of values of Spearman’s rank correlation coeﬃcient (ρ)
calculated for each case in Table 1. Similarly as in boxplots Figs. 2, 3, and 4, we
split the data into 3 groups: all students, only male, and only female, with n
indicating the number of students in the corresponding group. We performed
an evaluation for all students regardless of the grade as well as for a subset of
students who received a ﬁnal grade in course (“Grades > 0”) to check if there
would be a signiﬁcant diﬀerence in ρ values.
We can notice that almost all ρ values in Table 1 −except for the ones in bold
font −indicate a very weak negative or positive correlation between SQ, EQ, or
SQ−EQ, and the ﬁnal grade. There is a weak positive SQ correlation of 0.231
Table 1. Values of Spearman’s ρ between SQ, EQ, and SQ−EQ, and all grades or
grades higher than 0 grouped by all students, female, and male.
All Grades
Grades > 0
n
ρ(SQ)
ρ(EQ)
ρ(SQ−EQ)
n
ρ(SQ)
ρ(EQ)
ρ(SQ−EQ)
All students
73 0.072
0.152
−0.044
61 0.142
0.069
−0.021
(p = 0.546)
(p = 0.2)
(p = 0.711)
(p = 0.274) (p = 0.599)
(p = 0.87)
Only male
44 0.231
0.163
0.092
38 0.133
0.052
0.098
(p = 0.132) (p = 0.291) (p = 0.552)
(p = 0.426) (p = 0.756)
(p = 0.56)
Only female
29 −0.173
0.161
−0.233
23 −0.024
0.32
−0.225
(p = 0.371)
(p = 0.405)
(p = 0.225)
(p = 0.914) (p = 0.136) (p = 0.303)

292
J. Borzovs et al.
within the male group of students (“All grades” subset), and a weak positive
EQ correlation of 0.32 for female group of students (“Grades > 0” subset).
In both grade subsets (“All grades” and “Grades > 0”) there is a weak neg-
ative SQ−EQ correlation within female group of students (−0.233 and −0.225
respectively). However, the acquired values have no similarity with the ﬁnd-
ings from our preliminary study, neither with the study performed by the other
author in [20]. Given that none of the corresponding p-values is less than 0.05,
the null hypothesis (H0) deﬁned in Subsect. 3.2 cannot be rejected, meaning that
there is no evidence of correlation between SQ, EQ, or SQ−EQ, and the ﬁnal
grade for “Programming I” course.
4
Conclusion
The goal of our empirical study was to apply the same method as described
in Wray’s research [20] to either conﬁrm or disprove that there is a strong cor-
relation between SQ−EQ as well as moderate correlation between SQ and EQ
individually and student’s grade for the programming test. We tried to repro-
duce the circumstances of the experiment using similar tests to calculate SQ and
EQ with the actual set of questions, and we invited our ﬁrst-year students to
take part in the experimentation. The exception was that we used the results
of the introductory “Programming I” course instead of the specially developed
test with programming tasks.
The acquired values have no similarity with the study performed by the
author in [20]. The null hypothesis (H0) deﬁned in Subsect. 3.2 cannot be
rejected, which implies that there is no proof of correlation between SQ, EQ,
or SQ−EQ, and the ﬁnal grade for “Programming I” course.
The dropout problem still remains a very pressing one. Therefore, we need
to bring in new ideas for future research. For instance, the same set of data can
be used for a more targeted evaluation −we can ﬁlter out the data for students
with the ﬁnal grade 0 and integrate it with the data from other sources to have a
set of records on “dropouts”, because students with academic debts also received
grade 0. Then, we can investigate more precisely the dropouts’ data and look
for speciﬁc causes.
Other research direction could include application and analysis of another
group of tests that are mostly oriented on problem solving.
Finally, we hope that for our computing study programs it would be feasi-
ble to reach acceptable improvements regarding the dropout rate and ﬁnd an
appropriate tool to predict potential dropouts in the future.
References
1. Araque, F., Rold´an, C., Salguero, A.: Factors inﬂuencing university drop out rates.
Comput. Edu. 53(3), 563–574 (2009)
2. Baron-Cohen, S., Richler, J., Bisarya, D., Gurunathan, N., Wheelwright, S.: The
Systemizing Quotient: an investigation of adults with asperger syndrome or high-
functioning autism, and normal sex diﬀerences. Philos. Trans. Royal Soc. B: Biol.
Sci. 358(1430), 361–374 (2003)

Can SQ and EQ Values and Their Diﬀerence
293
3. Baron-Cohen, S., Wheelwright, S.: The empathy quotient: an investigation of
adults with asperger syndrome or high-functioning autism, and normal sex dif-
ferences. J. Autism Develop. Disord. 34(2), 163–175 (2004)
4. Belloc, F., Maruotti, A., Petrella, L.: University drop-out: an Italian experience.
High. Edu. 60(2), 127–138 (2010)
5. Bishop-Clark, C., Wheeler, D.D.: The Myers-Briggs personality type and its rela-
tionship to computer programming. J. Res. Comput. Edu. 26(3), 358–370 (1994)
6. Borzovs, J., Niedrite, L., Solodovnikova, D.: Computer programming aptitude test
as a tool for reducing student attrition. Environment. Technology. Resources. In:
Proceedings of the International Scientiﬁc and Practical Conference, vol. 3, pp.
29–35 (2015)
7. Borzovs, J., Niedrite, L., Solodovnikova, D.: Factors aﬀecting attrition among ﬁrst
year computer science students: the case of University of Latvia. Environment.
Technology. Resources. In: Proceedings of the International Scientiﬁc and Practical
Conference vol. 3, pp. 36–42 (2015)
8. Carl Jung’s and Isabel Briggs Myers’ typology test. http://www.humanmetrics.
com/hr/you/personalitytype.aspx Accessed 09 June 2017
9. Dehnadi, S., Bornat, R.: The camel has two humps (2006). http://www.eis.mdx.
ac.uk/research/PhDArea/saeed/paper1.pdf Accessed 09 June 2017
10. Empathy Quotient (EQ) for Adults: Empathy Quotient (EQ-60) - paper ver-
sion, EQ scoring key (EQ-60). https://www.autismresearchcentre.com/arc tests
Accessed 09 June 2017
11. Grebennikov, L., Shah, M.: Investigating attrition trends in order to improve stu-
dent retention. Qual. Assur. Edu. 20(3), 223–236 (2012)
12. Klavins, A.: Evaluation of potential students of computer programming by using
relevance between systemizing quotient and empathy quotient (Master thesis in
Latvian). https://dspace.lu.lv/dspace/handle/7/33213 Accessed 09 June 2017
13. Lorenzen, T., Chang, H.-L.: Mastermind c⃝: A predictor of computer programming
aptitude. SIGCSE Bull. 38(2), 69–71 (2006)
14. Paura, L., Arhipova, I.: Cause analysis of students’ dropout rate in higher education
study program. Procedia-Soc. Behav. Sci. 109, 1282–1286 (2014)
15. RStudio homepage. https://www.rstudio.com Accessed 09 June 2017
16. Systemizing Quotient (SQ) (Adult): Systemizing Quotient (SQ), SQ scoring key.
https://www.autismresearchcentre.com/arc tests Accessed 09 June 2017
17. Tukiainen, M., M¨onkk¨onen, E.: Programming aptitude testing as a prediction of
learning to program. In: Proceedings of the 14th Workshop of the Psychology of
Programming Interest Group, pp. 45–57 (2002)
18. University of Kent. Computer programming aptitude test. http://www.kent.ac.
uk/careers/tests/computer-test.htm Accessed 09 June 2017
19. Wheelwright, S., Baron-Cohen, S., Goldenfeld, N., et al.: Predicting autism Spec-
trum Quotient (AQ) from the Systemizing Quotient-revised (SQ-R) and Empathy
Quotient (EQ). Brain Res. 1079(1), 47–56 (2006)
20. Wray, S.: SQ minus EQ can predict programming aptitude. In: Proceedings of the
19th Annual Workshop of the Psychology of Programming Interest Group, pp.
243–254 (2007)

Fusion of Clinical Data: A Case Study to Predict
the Type of Treatment of Bone Fractures
Anam Haq(B) and Szymon Wilk
Poznan University of Technology, Poznan, Poland
anam.haq@put.poznan.pl, szymon.wilk@cs.put.poznan.pl
Abstract. Clinical data is characterized not only by its constantly
increasing volume but also by its diversity. Information collected in clini-
cal information systems such as electronic health records is highly hetero-
geneous and it includes structured laboratory and examination reports,
unstructured clinical notes, images, and more often genetic data. This
heterogeneity poses a signiﬁcant challenge when constructing diagnostic
and therapeutic decision models that should use data from all available
sources to provide a comprehensive support. A possible response to this
challenge is oﬀered by the concept of data fusion and associated tech-
niques. In this paper, we brieﬂy describe the foundations of data fusion
and present its application in a case study aimed at building a decision
model to predict the type of treatment for patients with bone fractures.
Speciﬁcally, the model should distinguish between patients who should
undergo a surgery and those who should be treated non-surgically.
Keywords: Clinical data · Data fusion · Prediction models · Decision
support
1
Introduction
Data fusion is generally deﬁned as the combination of data or information that
is obtained from multiple sources of diversiﬁed format and structure [1]. The
concept of data fusion can be clearly explained by considering the example of
human brain perception system. In order to perceive its surrounding conditions,
the human brain collects information from all its senses i.e., sight, hearing, smell,
taste and touch integrate them together along with the results extracted from
the previous memory of similar experiences and generate order accordingly. The
use of data fusion concepts in human biological system shows the importance of
this notion.
Concept of data fusion and its associated techniques found their use not only
in health care where they oﬀer a foundation for developing advanced decision
support and smart patient monitoring systems [2], but also in other application
areas, such as geospatial system, defense systems, and intelligence services (see
[3] for a review).
There are two prevalent approaches to data fusion: combination of data
(COD) and combination of interpretation (COI) [4]. Both techniques are
explained below:
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 294–301, 2017.
DOI: 10.1007/978-3-319-67162-8 29

Fusion of Clinical Data: A Case Study
295
1. Combination of Data (COD): Features from all data sources are ﬁrst aggre-
gated and the employed to build a single decision model (i.e., to learn a single
classiﬁer).
2. Combination of interpretation (COI): Each data source is used to build a sep-
arate decision model and then outcomes of individual models are aggregated
by a combiner (which can be considered as a meta-decision model or a meta-
classiﬁer) to produce a ﬁnal outcome. Conceptually, COI is similar to con-
structing ensembles of classiﬁers (in particular to the stacking scheme) [5].
Unfortunately, both COI and COD have some inherent drawbacks – COD
suﬀers from the curse of dimensionality, and COI is proved to be sub-optimal
as it is not able to preserve dependencies between data coming from various
sources. To address these shortcomings Lee et al. [6] developed a general fusion
framework (GFF) where COI and COD are considered as two extremes of a
continuous spectrum.
The general schema of GFF is shown in Fig. 1. Selection of transformations
applied to speciﬁc data sources to bring them to a common knowledge rep-
resentation is driven by the characteristics of data, e.g., for sparsely packed
data the dimensionality reduction schemes like principal component analysis
(PCA) should be used. These transformations may also introduce and use source-
speciﬁc classiﬁers. If the latter transformation is applied to all sources, then GFF
becomes COI. On the other hand, if all applied transformation simply copies
data, GFF boils down to COD.
Fig. 1. Schema of GFF (S1 to Sn are disparate sources, T1 to Tn are transformations
of data sources into a common representation, F is the fused feature space, and C is a
classiﬁer based on F) [6].
In this paper we present an on-going clinical case study where a customized
data fusion process based on GFF is employed to build a decision model that
would predict the type of treatment (surgical vs. non-surgical) for patients with
bone fractures. The decision should be based taking into account the general

296
A. Haq and S. Wilk
patient state as well as the characteristic of a fracture [7]. Therefore, a thera-
peutic decision model should be derived from X-ray images and non-image data
using data fusion techniques.
The rest of the text is organized as follows. In the next section we present
a brief overview on related work on data fusion. Then, in Sect. 3 we introduce
our case study and describe its goals, available data sources and the customized
data fusion process. Next, in Sect. 4 we give preliminary results of our analysis.
Finally, in Sect. 5 we provide conclusions and discuss future work.
2
Related Work
Application of COD and COI approaches are discussed in [5,8–11]. Lanckriet
et al. in [8] demonstrated an example of the COD methodology to construct
a support vector machine (SVM) to predict the yeast proteins function. Their
proposal employs a set of kernels to combine complex protein data, amino acid
sequences and gene expressions. K. Kourou et al. in [9] also used the COD
approach to develop diﬀerent classiﬁcation models using machine learning algo-
rithms that are frequently used in the detection and prognosis of various type
of cancers.
Jesneck et al. in [10] described the COI approach that fuses objective ﬁnd-
ings computed from mammograms, radiologist-interpreted ﬁndings and patient
history to diagnose breast cancer. A detection theory approach was employed
to construct classiﬁers. Speciﬁcally, a separate binary classiﬁer employing likeli-
hood ratio was constructed for each feature (thus there were multiple classiﬁers
for a given data source), and then their outcomes were combined as the joint
likelihood ratio of the set of decision variables (capturing outcomes of separate
classiﬁers). Finally, thresholds applied at both levels of classiﬁcation were opti-
mized using a generic algorithm.
Conceptually, COI is similar to constructing ensembles of classiﬁers (in par-
ticular to the stacking scheme), the concept of ensemble classiﬁers is explained
in [5]. The author describe the approaches of constructing ensemble classiﬁers,
and discussed in details the method for integrating the decisions obtained from
each classiﬁer. G. Zorluoglu et al. in [11] created a model for breast cancer diag-
nosis using decision tree, support vector machine and neural networks along
with the ensemble of these three techniques. After comparing the performance
of classiﬁers individually with the ensemble classiﬁer it was concluded that latter
performs better in terms of accuracy.
In [4] the author provides a comparison between the COI and COD by using
them to develop models for four diﬀerent kind of bio-medical image processing
functions. These functions are segmentation of atlas based images, average image
tissue based segmentation, multi-spectral classiﬁcation and deformation-based
group morphometry. The author compared the performance of COI and COD
used in construction of models for the above mentioned functions on the bases
of their versatility and capability of producing reliable and consistent results.
In the remaining part of the text, we describe an on-going clinical case study
where a customized data fusion process based on GFF is employed to build

Fusion of Clinical Data: A Case Study
297
a decision model that would predict the type of treatment (surgical vs. non-
surgical) for patients with bone fractures. The decision should be based taking
into account a general patient state as well as the characteristic of a fracture [7].
Therefore, a therapeutic decision model should be derived from X-ray images
and non-image data using data fusion techniques.
3
Clinical Case Study
3.1
Problem Statement
It is important to distinguish between these patients with bone fractures who
require surgery and these who can be managed non-surgically. Several studies
have shown that surgery is not needed in every case [12]. Surgical treatments are
not only more expensive than non-invasive ones, but they are also more painful.
Moreover, there is a group of patients who may be not suﬃciently clinically ﬁt
for the surgery. Thus, the decision about the type of treatment should be based
not only on the characteristic of a fracture, but also on the patient’s “ﬁtness”,
and in order to develop an appropriate therapeutic decision model image and
non-image data should be fused together.
3.2
Available Data Sources
In this case study we used a data set provided by the Wielkopolska Center of
Telemedicine (https://www.telemedycyna.wlkp.pl/web/guest/home) – a telecon-
sultation platform for patients with multiple injuries. This data set includes 2030
patientswithbonefractures–1593(78.5%)underwentasurgery,andtheremaining
437 (21.5%) were treated non-surgically. Patients are described using 301 features
capturing demographics (e.g., age and gender) results of physical examination and
basic laboratory tests (e.g., blood work), and detailed descriptions of injuries. For
the sake of simplicity, in the subsequent text we will refer to this data and features
as to clinical data and clinical features. Moreover, for almost each patient there is
a collection of X-ray images (usually between 2 and 4) of fractured bones.
From this data set we randomly selected 103 patients – 40 non-surgical and
63 surgical cases. For each patient we manually selected a single X-ray image
representing a fractured bone at the time when management started.
3.3
Data Fusion Process
The main goal of the presented case study was the application of GFF to build
a therapeutic decision model considering both image and non-image features.
Speciﬁcally, we followed the COD approach to transform heterogeneous data
into a single space.
The GFF schema we applied included two data sources S1 and S2 correspond-
ing to X-ray images and clinical data respectively. Moreover, transformations T1
and T2 applied to S1 and S2, respectively, employed diversiﬁed techniques of fea-
ture extraction/construction and selection. The T1 transformation was aimed at

298
A. Haq and S. Wilk
extracting from an X-ray image a single numerical feature indicating the “severity”
of a fracture (such information was not recorded explicitly in the clinical data). It
included the following steps (the ﬂow diagram is presented in Fig. 2:
1. Noise removal with a median ﬁlter and contrast adjustment,
2. Bone edge detection with the Canny operator and removal of disconnected
components,
3. Application of the Hough transform to detect the bone breakage, the process
is explained in detail in [13]. The parameter values are set in such a way
that the transform produces a single peak in result of minute fractures and
multiple peak values for signiﬁcant fracture bones (mean value of these peak
points are considered as feature values),
4. Identifying the location of bone fracture by drawing ellipse around the area.
Fig. 2. Results of extracting features from X-ray images: 1(a–b) – Noise removal and
contrast enhancement, 2(a–b) – Edge detection and disconnected components removal,
3(a–b) – Hough transformation and peak value extraction, (4) – Identifying bone
fracture
The transformation T2 applied to S2 was less complex and it involved the
following steps:
1. Discretization of numerical features using norms deﬁned by clinical experts,
2. Introduction of additional features capturing information about injuries at a
lower granularity level, and
3. Removal of “useless” features (e.g., features with the majority of missing
values or extremely low or high variability).

Fusion of Clinical Data: A Case Study
299
Step (2) may require additional explanation. The originally recorded infor-
mation about injuries was very detailed (e.g., it indicated a precise location of a
fracture) and thus it was impossible to identify any strong patterns in the clini-
cal data. To address this issue we introduced additional variables that captured
more general information (e.g., they indicated that a speciﬁc bone was broken).
Application of T2 resulted in obtaining 113 features.
Transformation T1 was implemented in MATLAB, and T2 was implemented
in a custom Java program (discretization and introduction of new features) and
in WEKA (feature selection).
Finally, the feature extracted from the X-ray images and 113 features from the
clinical data were combined into a fused space that we used to construct and eval-
uate several possible classiﬁers (this analysis was also conducted in WEKA). The
choice of learning algorithm mostly depends upon the type of dataset. The dataset
used in this case study had class information so we have considered supervised
learningalgorithmsforperformanceevaluation.Theclinicaldatacontainsthelarge
number of features and values of those features are categorical so we have selected
algorithms which performs better when applied over such type of dataset. Specif-
ically, we used a naive Bayes (NB) classiﬁer, a C4.5 decision tree (DT), a random
forest (RF) and a support vector machine (SVM) with a linear kernel.
Such selection of classiﬁers was based on our past experience with analysis of
clinical data [14,15] and on results of other studies related to data fusion [16]. In
order to assess the impact of data fusion on the classiﬁcation performance we con-
sidered three versions of each classiﬁer based on the image feature, clinical features
and all (fused) features. Finally, classiﬁcation performance was evaluated using
stratiﬁed 10-fold cross validation.
4
Results
Set of four classiﬁers were used, each representing a diﬀerent learning approach.
The use of these classiﬁers were to compared the performance of the fusion app-
roach on diﬀerent classiﬁcation algorithms. Classiﬁcation performance of speciﬁc
classiﬁers is given in Table 1 where we report overall accuracy, accuracy for both
decision classes and ﬁnally a geometric mean (G-mean) of class-speciﬁc accura-
cies (this measure is less aﬀected by the bias toward any of the classes). Best
results for each classiﬁer are marked with bold.
With the results obtained in Table 1 we can derived the following conclusion
that:
1. Fusion always improved overall accuracy and most cases G-mean (DT was
the only exception).
2. In case of RF fusion improved the accuracy for the non-surgical class while
for other classiﬁers it was more beneﬁcial for the surgical class
3. The overall accuracy of classiﬁers based on the single image-based feature
in most cases was equal or better than the accuracy of classiﬁers based on
clinical features (RF was the only exception). Moreover, the former classiﬁers
turned to be surprisingly accurate in comparison to the classiﬁers based on
fused features.

300
A. Haq and S. Wilk
Table 1. Performance of classiﬁers for various features (overal = overall accuracy,
non-surg, surg = accuracy for the non-surgical and surgical class, respectively)
Classiﬁer Features Overal [%] Non-surg [%] Surg [%] G-mean [%]
NB
Image
78.64
77.50
79.40
78.44
Clinical
78.64
62.50
88.90
74.54
Fused
83.50
67.50
93.70
79.53
SVM
Image
79.61
80.00
79.40
79.70
Clinical
67.96
62.50
71.40
66.81
Fused
80.58
77.50
82.50
80.00
DT
Image
78.64
82.50
76.20
79.29
Clinical
66.20
60.00
69.80
64.71
Fused
79.61
75.00
82.50
78.66
RF
Image
70.87
65.00
74.60
69.60
Clinical
75.72
57.50
87.30
70.85
Fused
79.61
82.76
78.40
80.55
5
Conclusions
In this paper we presented preliminary results from our on-going clinical case
study where we applied to combination of data approach to fuse image and
clinical data. We checked the performance of for classiﬁers – a naive Bayes,
support vector machine, decision tree and random forest – and in each case we
observed the improvement in the overall classiﬁcation accuracy.
The largest increase was observed for the random forest classiﬁer which is
consistent with already published results [16].
This case study is a initial step towards the implementation of a comprehen-
sive framework for clinical data. Such framework that would be able to handle all
the challenges associated with it like clinical data, heterogeneity, class imbalance,
overlapping of decision boundaries, rare cases, outliers and noise.
In the next steps of our case study we will use more data (especially, we will
consider more than one image per patient), introduce additional image-based
features extracted with texture analysis, and ﬁnally apply the combination of
interpretation technique employing specialized ensembles of classiﬁers.
Acknowledgement. The second author would like to acknowledge support by the
Polish National Science Center under Grant No. DEC-2013/11/B/ST6/00963.
References
1. Mitchell, H.B.: Data Fusion: Concepts and Ideas. Springer, Heidelberg (2014).
doi:10.1007/978-3-642-27222-6
2. Lahat, D., Adali, T., Jutten, C.: Multimodal data fusion: an overview of meth-
ods, challenges, and prospects. Proc. IEEE 103(9), 1449–1477 (2015). doi:10.1109/
JPROC.2015.2460697

Fusion of Clinical Data: A Case Study
301
3. Castebedo, F.: A review of data fusion techniques. Sci. World J. (2013). doi:10.
1155/2013/704504
4. Rohlﬁng, T., Pfeﬀerbaum, A., Sullivan, E.V., Maurer, C.R.: Information fusion in
biomedical image analysis: combination of data vs. combination of interpretations.
In: Christensen, G.E., Sonka, M. (eds.) IPMI 2005. LNCS, vol. 3565, pp. 150–161.
Springer, Heidelberg (2005). doi:10.1007/11505730 13
5. Ponti Jr., M.P: Combining classiﬁers: from the creation of ensembles to the decision
fusion. In: 24th SIBGRAPI Conference on Graphics, Patterns and Images Tutorials
(2011)
6. Lee, G., Madabhushi, A.: A knowledge representation framework for integration,
classiﬁcation of multi-scale imaging and non-imaging data: preliminary results in
predicting prostate cancer recurrence by fusing mass spectrometry and histology.
In: International Symposium on Biomedical Imaging: From Nano to Macro. IEEE
(2009)
7. Twiss, T.: Nonoperative treatment of proximal humerus fractures. In: Crosby, L.A.,
Neviaser, R.J. (eds.) Proximal Humerus Fractures. LNCS, pp. 23–41. Springer,
Cham (2015). doi:10.1007/978-3-319-08951-5 2
8. Lanckriet, G., Deng, M., Cristianini, N., Jordan, M., Noble, W.: Kernel-based data
fusion and its application to protein function prediction in yeast. In: Proceedings
of Paciﬁc Symposium on Biocomputing (2004)
9. Kourou, K., Exarchos, T.P., Exarchos, K.P., Karamouzis, M.V., Fotiadis, D.I.:
Machine learning applications in cancer prognosis and prediction. Comput. Struct.
Biotechnol. J. 13, 8–17 (2015). doi:10.1109/SIBGRAPI-T.2011.9
10. Jesneck, J., Nolte, L., Baker, J., Floyd, C., Lo, J.: Optimized approach to decision
fusion of heterogeneous data for breast cancer diagnosis. Med. Phys. 33, 2945–2954
(2006). doi:10.1118/1.2208934
11. Zorluoglu, G.M.: Diagnosis of breast cancer using ensemble of data mining clas-
siﬁcation methods. Int. J. Bioinform Biomed. Eng. 1(3), 318–322 (2015). doi:10.
5829/idosi.wasj.2014.29.dmsct.4
12. Hossain, M., Neelapala, V., Andrew, J.G.: Results of non-operative treatment fol-
lowing hip fracture compared to surgical intervention. Int. J. Care Inj. 40, 418–421
(2008)
13. Myint, S., Khaing, A.S., Tun, H.M.: Detecting leg bone fracture in x-ray images.
Int. J. Sci. Technol. Res. 5, 140–144 (2016)
14. Wilk, S., Stefanowski, J., Wojciechowski, S., Farion, K.J., Michalowski, W.: Appli-
cation of preprocessing methods to imbalanced clinical data: an experimental study.
In: Pi etka, E., Badura, P., Kawa, J., Wieclawek, W. (eds.) Information Technolo-
gies in Medicine. AISC, vol. 471, pp. 503–515. Springer, Cham (2016). doi:10.1007/
978-3-319-39796-2 41
15. Kubat, M., Matwin, S.: Addresing the curse of imbalanced training sets: one-side
selection. In: Proceedings of the 14th International Conference, ICML 1997, pp.
179–186 (1997)
16. Tiwari, P., Viswanath, S., Lee, G., Madabhushi, A.: Multi-model data fusion
schemes for integrated classiﬁcation of imaging and non-imaging biomedical data.
In: International Symposium on Biomedical Imaging: From Nano to Macro. IEEE
(2011). doi:10.1109/ISBI.2011.5872379

Data Science Techniques for Law and Justice:
Current State of Research and Open Problems
Alexandre Quemy1,2(B)
1 IBM Poland Software Lab, Cracow, Poland
aquemy@pl.ibm.com
2 Faculty of Computing, Pozna´n University of Technology, Pozna´n, Poland
Abstract. By comparing the state of research in Legal Analysis to the
needs of legal agents, we extract four fundamental problems and discuss
how they are covered by the current best approaches. In particular, we
review the recent statistical models, relying on Machine Learning coupled
to Natural Language Processing techniques, and the Abstract Argumen-
tation applied to the legal domain before giving some new perspectives
of research.
Keywords: Legal analysis · Abstract Argumentation · Case-Based Rea-
soning
1
Introduction
The legal environment is a messy concept [41] that intrinsically poses a certain
amount of diﬃculties to analyze: grey areas of interpretation, many exceptions,
non-stationarity, deductive and inductive reasoning, non classical logic, etc.
In this paper, we review the large spectrum of studies on the legal domain to
extract a list of major problems to cover: statistical studies, helped by Natural
Language Processing techniques, Case-based Reasoning and Abstract Argumen-
tation. A major classiﬁcation factor appears to be the nature and scope of the
information. Some methods will rely on legal knowledge and reasoning techniques
while others will exploit the available data: past decisions records, non-legal case
features, etc. However, to be broadly adopted by practitioners, a model needs
to be explicit and to provide an explanation, meaning it must integrate legal
knowledge at some point. This is a challenge, in particular for some Machine
Learning techniques such as Random Forests that are well-known to propose
non-analytical and hard-to-interpret results.
The organization of the paper is as follow: in Sect. 2, we state the problems to
discuss their legitimacy. The next Sections are dedicated to the state of research
respectively in statistical models, Case-based Reasoning (CBR) and Abstract
Argumentation (AA). Last, we will discuss the current limitations and argue
that the fundamental problems did not receive a homogeneous interest from the
research community.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 302–312, 2017.
DOI: 10.1007/978-3-319-67162-8 30

Data Science Techniques for Law and Justice
303
2
Law and Justice Fundamental Problems
To deﬁne the Law and Justice problems and orient the research we need to
understand the practitioners. A major debate among the legal community is the
interpretation problem, namely legalism vs realism, i.e. are the judges objectively
applying a method contained in the text (legalism) or do they create their own
interpretation (realism). For Frydman [17], the interpretation is an intellectual
operation to delimit the extent of the rule in the given context of its application.
For Troper [49], the interpretation is iterative because the law is expressed in
generic terms and in the interpretation lie some discretionary elements. Despite
this genericity, new situations will emerge, creating grey zones of interpretation
to be answered by an interpretation [47]. The non-omniscience of judges coupled
to the abstraction of the law often oﬀer a room for manoeuvre on the applicability
of some legal arguments and the details of sentencing. This can lead to several
valid legal justiﬁcations up to a certain degree. Finding the best justiﬁcations
among the possible justiﬁcations w.r.t. some criteria is a concrete problem.
For some jurists, the law must be strictly considered as an incitation mech-
anism to tend to the economic eﬃciency. For a given legal environment, one
might want to control it to reach this goal (e.g. by changing the law). Broadly
speaking, studying how it complies with society goals and ideals is a necessity.
Due to its decentralized and non-stationary nature some drifts might appear
or the current implementation of institutions or policies might not match the
theoretical expectations. Some actors cannot directly change the law but they
have to take decisions according to the current environment (a company needs
to value the risk of its actions w.r.t. the uncertainty and take some decisions).
Those observations lead to distinguish four fundamental problems:
– Predicting the outcome of a case given the legal environment. (Prediction)
– Building a legal justiﬁcation, given some facts, a set of law texts with the
jurisprudence and an outcome. (Justiﬁcation)
– Taking the best decisions w.r.t. the legal environment dynamics and some
criteria. (Decision)
– Modifying the legal environment dynamics to match some criteria. (Control)
The Prediction problem is challenging, even for the best legal experts: 67.4% and
58% accuracy, respectively for the judges and whole case decision, is observed
[42] for the Supreme Court of the United States (SCOTUS). Using crowds, the
Fantasy Scotus1 project reached respectively 85,20% and 84,85% correct pre-
dictions. The Justiﬁcation problem is in a way, an extension of the Prediction
one: the reasons of a decision have to be given, including the sentence details.
The Justiﬁcation does not consist in explaining the prediction but ﬁnding an
explanation based solely on legal factors.
One can illustrate the four problems with the European Union directive to
set ﬁnes for abuses of dominant position2:
1 https://fantasyscotus.lexpredict.com/.
2 http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=URISERV%3Al26118.

304
A. Quemy
“The level of a ﬁne must be suﬃciently high both to punish the ﬁrms involved
and to deter others from practices that infringe the competition rules. [...] The
basic amount is calculated as a percentage of the value of the sales connected
with the infringement [...]. The percentage of the value of sales is determined
according to the gravity of the infringement (nature, combined market share of
all the parties concerned, geographic scope, etc.) and may be as much as 30%.”
The Prediction problem consists in determining if the verdict will sanction or
not the infringement, possibly using non-legal factors. The Justiﬁcation problem
resides in estimating the ﬁnes compatible with the text. It involves estimating the
dynamic quantities mentioned such as the combined market share. A possible
Control problem would be to determine if in practice the calculation reaches
the goals, namely punishing and preventing the other agents to break the law.
Conversely, for a company, a Decision problem would be to determine if given
the current state of the legal environment, an aggressive takeover would not be
perceived as breaking the law and thus leading to undesirable sanctions.
3
Predictive Models
An ideal court, is a court where the judges are perfectly rational, free of all bias
and preferences, and omniscient. In this conﬁguration, all judges must reach the
same decisions, and two exactly similar cases would result in the same decision.
The decisions are not correlated: it is impossible to predict a judgement from
an ideal court using the information from the past cases. Of course, in real life
there is no such ideal court and statistical methods try to detect hidden patterns
between the sequence of decisions to predict future decisions3.
In [34,42] a classiﬁcation tree per Justice is used to predict the votes after
being trained on cases described by 6 visible4 features such as the lower court
circuit or the type of petitioner. The model performs signiﬁcantly better than the
experts, while the diﬀerence of prediction at the judge level is not signiﬁcant. An
important result is also that the experts were better at drawing good conclusions
on the vote of the most extremely ideologically oriented judges5.
The Stochastic Block Model [19] predicted 77% of individual Justice votes
given the votes of the other Justices of the SCOTUS and the history of decisions.
Despite the model is not fully predictive and cannot be used to build legal
explanation, as it does not use any legal arguments or case-based information,
but hidden associations between social actors, it clearly exhibits empirical proofs
to support the attitudinalism paradigm6.
3 One may notice that the less a court is predictable, the closer it is from an ideal
court.
4 That is to say also available to legal experts.
5 Where the measure is calculated as given by Martin and Quinn [38,39].
6 On top the predictions, the authors shown the existence of diﬀerent predictability
between judges, implying a diﬀerence of attitude toward the law, as well as a decrease
in the SCOTUS predictability during some periods or depending on the political
party at the presidence.

Data Science Techniques for Law and Justice
305
A general, robust and fully-predictive model has been proposed in [27] using
Random Forest [18] and successfully identiﬁed 69.7% of the Court decisions
over 60 years. The model is built up on more than 300 features , divided into
3 categories: Court and Justice level information, Case information, Historical
Justice and Court information. The case features account for 23% of the predic-
tive power while the Court background only for less than 5%. In other words,
most of the predictive power holds in the behavioral trend including ideological
shifts. More than the exact percentage for this particular model and Court, this
comforts the realism paradigm once again. The classiﬁer returns the weights aka
the importance of every feature in the prediction. Those weights evolves in time
giving additional comprehensive hints on the Court dynamic despite a complex
interpretation due to the correlation between the features [48].
Using NLP techniques, the authors of [2] achieve 79% accuracy to predict
the decisions of the European Court of Human Rights (ECtHR). They make the
hypothesis that the textual content of the European Convention of the Human
Rights and the case elements holds hints that will inﬂuence the decision of the
Judge. They extracted from the case the top 2000 N-grams, calculated their
similarity matrix based on cosine measure and partition this matrix using Spec-
tral Clustering to obtain a set of interpretable topics. The binary prediction was
made using an SVM with linear kernel. Contrary to the previous studies, they
found out the formal facts are the most important predictive factor which tend
to favor realism. However, as they used legal documents to extract their features,
the non-legal hints are most likely to be less present.
As enlighted by [19,27], one prominent factor in the predictive power of statis-
tical methods is the ideology of the judge. To capture this reality, many estimators
of the ideal point, a latent variable to position the ideology in a continuous space
, have been developed. A taxonomy of estimators can be established accord-
ing to the type of information they rely on: the party aﬃliation [46], the expert
judgements [43,44] or the votes [38,39], possibly including decision-related mate-
rial [23,30,45]. All of them focus on SCOTUS. The Segal-Cover Score [43,44]
is constructed out of editor’s assessments published before the nomination of a
given judge and whose information are extracted and interpreted manually. The
Martin-Quinn Score [33,38,39] uses an Item Response Model [22] with the ideal
point modelled as a random walk, thus evolving in time. NLP techniques are
used in [30,45] to model the inﬂuence of opinion texts (amicus briefs, opinions)
on the ideal point of judges while in [23], the ideal point is expressed in the topic
space of the legal texts, enabling a more complete preference proﬁle per Justice.
4
Case-Based Reasoning
To solve a given problem, a case-based system performs the following cycle [1]:
1. Search for the most related past cases, either by ﬁltering the irrelevant cases
or selecting the closest ones depending on a metric and a KNN algorithm.
2. Adapt the best case solution to the new case.

306
A. Quemy
3. Evaluate and revise the proposed solution, including at least why the solution
is not satisfying.
4. Integrate the solution to the database.
Among the legal CBRs, CATO [3] is one of the most famous. Used to teach
argumentation to students, it consists in 8 basics reasoning moves over a database
of cases described by manually extracted legal factors. The factors are connected
together in a pre-established hierarchy. The links are annotated with a plus (+)
or minus (−) to indicate if a factor attacks or defend another one. In a sense,
CATO can be seen as an ancestor of Abstract Argumentation.
If CBRs are more eﬃcient and reliable than classical rule-based systems when
it comes to law oriented problems [28], many drawbacks subsist: similarity and
relevance of precedent cases are dynamic, non-stationary as social and govern-
mental laws evolve [11]. A novel approach to reason by analogy is to learn some
rules from a set of similar cases and then to use those rules to reason and predict
a new case [25]. The rules represent the prevailing norm in a legal environment
at a given moment and thus can evolve while the law corpus remains the same.
5
Abstract Argumentation
Abstract Argumentation (AA) [13] is a hot topic in non-monotonic reasoning,
that is to say where there is a need to reason with pieces of information that
can turn to be in contradiction with classical logic. The versatility of AA is thus
it can be used as a descriptive tool, for instance by including agent’s beliefs in
order to study and understand a given situation, as well as a normative tool
where, given a knowledge base, we want to infer the best action or decision to
take [4]. In [10] the author deﬁnes the usage of AA with the following three steps:
1. Deﬁning the arguments and the relation(s) between them.
2. Valuating the arguments using their relations, a strength, etc.
3. Selecting some arguments using some criteria (a semantic).
From this very succinct summary, one may catch a glimpse on the interest of
AA applied to the legal domain, either as a modeling or decision aid tool: the
judgment of a case in both Common Law or Civil Law countries follow this exact
procedure of collecting the evidence and legal facts, evaluating their suitability
in the context of the case, take a decision based on those facts.
In a similar fashion of CATO, an Argumentation Framework (AF) where the
arguments are taken manually from a set of cases and the attack deduced from
the opinions has been proposed [8,9]. Given a particular case, the arguments
that do not apply are removed to obtain a new AF, a subset of the initial one.
The key arguments are calculated to determine if the outcome is admissible, i.e.
in favor of the plaintiﬀ.
Oren et al. [35,36] instantiate arguments as inference rules and use Subjec-
tive Logic [24] to value the strength of the arguments. After giving an algorithm
to propagate the strength of the arguments, they deﬁne a dialogue game protocol

Data Science Techniques for Law and Justice
307
for several agents to argue about the state of the environment. Using Assumption-
Based Argumentation [14] , Dung and Thang [16] model a pool of agents argu-
ing about some arguments in front of judges with a ﬁnal jury taking a decision.
If the jury can also introduce new arguments, they are limited to considering
the probabilities of causal arguments, while the judges are the only one to deter-
mine the admissibility of an argument. Other Assumption-Based Argumentation
applications to the the legal domain can be found in [15,29]. Meanwhile, several
quantitative methods to calculate and propagate the strength of arguments have
been developed [6,40] with e.g. Social AA [31] that intends to model debates and
decision-making in social networks using a voting system. Future work should con-
sider using them in legal analysis due to good results obtained in systems with
similar characteristics.
Recently, CBR based on AA formalism have been proposed. In the previous
approaches the arguments are seen as elements of a case while in AA-CBR the
arguments are the cases, and the attacks relations between cases. The cases are
deﬁned as a set of features and an outcome. In [50], the outcome of a new case
is given depending on the grounded extension and a justiﬁcation7 to support the
decision is built using a Dispute Tree [14] while in [5] the outcome is deduced
from rules learnt from past cases. In [37], the outcome is given after a deliberation
between several agents and a fully adaptive and dynamic approach such that the
agents learn from each other and are able to resolve conﬂict through speciﬁc a
game protocol.
6
Summary, Limitations, and Future Work
As analyzed in Sects. 2, 3, 4 and 5, the approaches can be broken down into
three groups, namely: the statistical models, the case-based reasoning and the
abstract argumentation.
Among the statistical models, we distinguish two categories. First, some
methods use Machine Learning [19,27,34,42] or NLP [2] to predict the outcome
of a case given the past cases or votes. Papers in the second category focus
on modeling and estimating the ideal point of judges and predict votes solely
using this estimation [23,30,33,38,39,43–45]. Those methods tackle the predic-
tion problem but cannot handle the justiﬁcation problem. All models adopt a
binary outcome and further work must integrate sentencing elements for more
complex and detailed outcomes (Table 1).
Case-base reasoning systems evolved in their formalism from ad-hoc struc-
tures to the Abstract Argumentation structure. The most important factor to
distinguish between CBR systems is the way they build the justiﬁcation. In
[3,8,9] the justiﬁcation is based on pre-determined legal factor hierarchy and
past cases description, while in [5,37,50] the factor hierarchy is deduced from
past cases relations. Most CBRs neglect the importance of non-legal factors,
7 To be precise, some previous cases and some of their speciﬁc features. Thus, this is
not a legal justiﬁcation for Roman Law.

308
A. Quemy
Table 1. Comparison of Law and Justice approaches. The features to compare include:
(1) whether the method relies on pre-deﬁned rules or exploit the data (Information), (2)
if it uses Legal Knowledge (LK), (3) the capacity to be applied to any case over years
(General), (4) to adapt to the environment shifts (Robust), (5) to make a prediction
solely based on past information (Fully Pred.), (6) the extra-data used on top of the
information about the current case (Extra Data) and (7) the capacity to justify a
decision (Just.).
Information
LK
General
Robust
Fully Pred.
Extra Data
Just.
Pred. Models
[34,42]
Data-driven
No
No
No
Yes
Past cases
No
[19]
Data-driven
No
Yes
Yes
No
Past votes
No
[27]
Data-driven
No
Yes
Yes
Yes
Past cases
No
[2]
Data-driven
No
Yes
No
Yes
Past cases
No
Ideal Point
[43,44]
Data-driven
No
Yes
No
Yes
Non-legal
No
[33,38,39]
Data-driven
No
Yes
Yes
Yes
Past votes
No
[30,45]
Data-driven
No
Yes
No
Yes
Amicus
No
[23]
Data-driven
No
Yes
No
Yes
Opinions
No
CBR
[3]
Rule-based
Yes
No
No
Yes
Past cases & Legal factors
Yes
AA
[8,9]
Both
Yes
No
No
Yes
Past cases & Legal factors
Yes
[35,36]
Both
Yes
Yes
Yes
Yes
Norm
Yes
[14]
Rule-based
No
Yes
Yes
Yes
-
Yes
[6,40]
Both
No
Yes
No
Yes
-
-
[31]
Rule-based
No
Yes
No
Yes
-
-
AA-CBR
[50]
Both
No
No
No
Yes
Past cases
Partly
[5]
Both
No
Yes
Yes
Yes
Past cases
Partly
[37]
Both
No
Yes
Yes
Yes
Past cases
Partly
and thus, they implicitly work in an ideal court setting, ruining their capacity
to handle the Prediction problem.
Finally, CBRs do not account for the temporal dimension of the legal environ-
ment. Future work must focus on integrating dynamic features into (AA)-CBR
systems. The case features are static and if a feature is shared between cases, e.g.
the judge, they may be considered close because they are judged by the same
person. However, the preferences of the judges change in time and are inﬂuenced
by local documents such as case amicus. Thus this cannot directly be used as a
feature. Conversely, using the value of the ideal point estimation as a feature is
not a good idea: the ideal point strongly depends on many underlying features
(the judge, the area of law, other case feature).
In
Abstract
Argumentation,
apart
from
AA-CBR,
two
kinds
of
approaches emerge. A positive one intends to model real-life decision processes
or environment [14,35,36]. A normative one tries to elaborate methods to select

Data Science Techniques for Law and Justice
309
among the best alternatives and discuss arguments [6,31,40]. If AA is a promis-
ing tool to handle the Prediction and Justiﬁcation problem, the level it operates,
mainly at case level, is not directly suitable for the Decision and Control. Despite
a rich literature on dynamic in AA [7,12,32] including recent attempts to solve
decision process in non-stationnary environments [20,21], as far as we know,
there is no attempt to tackle the Control and Decision problem.
A main pitfall of all the approaches is the lack of automation: the Segal-Cover
score relies on manually extracted information, the features hierarchy in CATO
are manually constructed, etc. Future work should focus on the automation
using the recent progresses in NLP and the availability of data. For instance, it
is possible to update the Segal-Cover score on a daily basis by extracting the
information published over the internet8.
To conclude, many elements of law look like ﬁnance 25 up to 50 years ago
[26]. We do believe transferring risk assessment techniques from ﬁnance to law
is the way to handle all the four problems but requires to corretly quantitatively
model the underlying dynamics of the legal environment: a new challenge for data
science.
References
1. Aamodt, A., Plaza, E.: Case-based reasoning: foundational issues, methodological
variations, and system approaches. AI Commun. 7(1), 39–59 (1994)
2. Aletras, N., Tsarapatsanis, D., Preot¸iuc-Pietro, D., Lampos, V.: Predicting judicial
decisions of the European court of human rights: a natural language processing
perspective. PeerJ Comput. Sci. 2, 93–112 (2016)
3. Aleven, V., Ashley, K.D.: Evaluating a learning environment for case-based argu-
mentation skills. In: Proceedings of International Conference on Artiﬁcial Intelli-
gence and Law (ICAIL), pp. 170–179 (1997)
4. Amgoud, L.: A uniﬁed setting for inference and decision: an argumentation-based
approach. In: Proceedings of International Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 26–33 (2005)
5. Athakravi, D., Satoh, K., Law, M., Broda, K., Russo, A.: Automated inference
of rules with exception from past legal cases using ASP. In: Calimeri, F., Ianni,
G., Truszczynski, M. (eds.) LPNMR 2015. LNCS (LNAI), vol. 9345, pp. 83–96.
Springer, Cham (2015). doi:10.1007/978-3-319-23264-5 8
6. Baroni, P., Romano, M., Toni, F., Aurisicchio, M., Bertanza, G.: Automatic eval-
uation of design alternatives with quantitative argumentation. Argum. Comput.
6(1), 24–49 (2015)
7. Barringer, H., Gabbay, D., Woods, J.: Temporal dynamics of support and attack
networks: from argumentation to zoology. In: Hutter, D., Stephan, W. (eds.) Mech-
anizing Mathematical Reasoning. LNCS, vol. 2605, pp. 59–98. Springer, Heidelberg
(2005). doi:10.1007/978-3-540-32254-2 5
8. Bench-Capon, T.J.: Representation of case law as an argumentation framework.
In: Proceedings of International Conference on Legal Knowledge and Information
Systems (JURIX), pp. 103–112 (2002)
8 e.g.: IBM Watson Services oﬀer query services over hundred of thousands of articles
indexed every day.

310
A. Quemy
9. Bench-Capon, T.J.: Try to see it my way: modelling persuasion in legal discourse.
Artif. Intell. Law 11(4), 271–287 (2003)
10. Cayrol, C., Lagasquie-Schiex, M.C.: On the acceptability of arguments in bipolar
argumentation frameworks. In: Godo, L. (ed.) ECSQARU 2005. LNCS (LNAI),
vol. 3571, pp. 378–389. Springer, Heidelberg (2005). doi:10.1007/11518655 33
11. Delgado, P.: Survey of casebased reasoning as applied to the legal domain (2007,
unpublished)
12. Delobelle, J., Haret, A., Konieczny, S., Mailly, J., Rossit, J., Woltran, S.: Merging
of abstract argumentation frameworks. In: Proceedings of International Conference
on Principles of Knowledge Representation and Reasoning (KR), pp. 33–42 (2016)
13. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77,
321–357 (1995)
14. Dung, P.M., Kowalski, R.A., Toni, F.: Dialectic proof procedures for assumption-
based, admissible argumentation. Artif. Intell. 170(2), 114–159 (2006)
15. Dung, P.M., Thang, P.M.: Towards an argument-based model of legal doctrines in
common law of contracts. In: Proceedings of International Conference on Compu-
tational Logic in Multi-Agent Systems (CLIMA), pp. 111–126 (2008)
16. Dung, P.M., Thang, P.M.: Towards (probabilistic) argumentation for jury-based
dispute resolution. In: Proceedings of International Conference on Computational
Models of Argument (COMMA), pp. 171–182 (2010)
17. Frydman, B.: Le sens des lois: histoire de l’interpr´etation et de la raison juridique.
Penser le droit, Bruylant, Bruxelles (2005)
18. Geurts, P., Ernst, D., Wehenkel, L.: Extremely randomized trees. Mach. Learn.
63(1), 3–42 (2006)
19. Guimer`a, R., Sales-Pardo, M.: Justice blocks and predictability of U.S. supreme
court votes. PLOS ONE 6(11), 1–8 (2011)
20. Hadoux, E., Beynier, A., Maudet, N., Weng, P., Hunter, A.: Optimization of prob-
abilistic argumentation with markov decision models. In: Proceedings of Interna-
tional Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 2004–2010 (2015)
21. Hadoux, E., Beynier, A., Weng, P.: Sequential decision-making under non-
stationary environments via sequential change-point detection. In: Workshop on
Learning over Multiple Contexts (LMCE) at ECML-PKDD (2014)
22. Hambleton, R.: Fundamentals of Item Response Theory. Measurement Methods
for the Social Science. SAGE Publications, Newbury Park (1991)
23. Islam, M.R., Hossain, K., Krishnan, S., Ramakrishnan, N.: Inferring multi-
dimensional ideal points for US supreme court justices. In: Proceedings of Inter-
national Conference on Artiﬁcial Intelligence (AAAI), pp. 4–12 (2016)
24. Jøsang, A.: Subjective Logic: A Formalism for Reasoning Under Uncertainty. Arti-
ﬁcial Intelligence: Foundations, Theory, and Algorithms. Springer, Switzerland
(2016)
25. Kannai, R., Schild, U.J., Zeleznikow, J.: There is more to legal reasoning with
analogies than case based reasoning, but what? In: Dershowitz, N., Nissan, E.
(eds.) Language, Culture, Computation. Computing of the Humanities, Law, and
Narratives. LNCS, vol. 8002, pp. 440–451. Springer, Heidelberg (2014). doi:10.
1007/978-3-642-45324-3 15
26. Katz, D.M., Bommarito, M.J.: Fin(legal)tech - law’s future from ﬁnance’s past
(talk)
(2017).
https://speakerdeck.com/danielkatz/ﬁn-legal-tech-laws-future-
from-ﬁnances-past-professors-daniel-martin-katz-plus-michael-j-bommarito.
Accessed 21 Mar 2017

Data Science Techniques for Law and Justice
311
27. Katz, D.M., Bommarito, M.J., Blackman, J.: Predicting the behavior of the
supreme court of the united states: a general approach. SSRN Electron. J. (2014)
28. Kowalski, A.: Case-based reasoning and the deep structure approach to knowledge
representation. In: Proceedings of International Conference on Artiﬁcial Intelli-
gence and Law (ICAIL), pp. 21–30 (1991)
29. Kowalski, R.A., Toni, F.: Abstract argumentation. Artif. Intell. Law 4(3), 275–296
(1996)
30. Lauderdale, B.E., Clark, T.S.: Scaling politically meaningful dimensions using texts
and votes. Am. J. Polit. Sci. 58(3), 754–771 (2014)
31. Leite, J., Martins, J.: Social abstract argumentation. In: Proceedings of Interna-
tional Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 2287–2292 (2011)
32. Mailly, J.: Dynamic of argumentation frameworks. In: Proceedings of International
Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 3233–3234 (2013)
33. Martin, A.D., Quinn, K.M., Epstein, L.: The median justice on the united states
supreme court. N.C. Law Rev. 83, 1275–1322 (2004)
34. Martin, A.D., Quinn, K.M., Kim, P.T., Ruger, T.W.: Competing approaches to
predicting supreme court decision making. Perspect. Polit. 2, 761–767 (2004)
35. Modgil, S., Faci, N., Meneguzzi, F., Oren, N., Miles, S., Luck, M.: A framework
for monitoring agent-based normative systems. In: Proceedings of International
Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 153–
160 (2009)
36. Nirn, O.: An Argumentation Framework Supporting Evidential Reasoning with
Applications to Contract Monitoring. Ph.D. thesis, University of Aberdeen (2007)
37. Onta˜n´on, S., Plaza, E.: An argumentation-based framework for deliberation in
multi-agent systems. In: Rahwan, I., Parsons, S., Reed, C. (eds.) ArgMAS 2007.
LNCS (LNAI), vol. 4946, pp. 178–196. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-78915-4 12
38. Quinn, K.M., Martin, A.D.: Dynamic ideal point estimation via markov chain
Monte Carlo for the U.S. supreme court, 1953–1999. Polit. Anal. 10(2), 134–153
(2002)
39. Quinn, K.M., Park, J.H., Martin, A.D.: Improving judicial ideal point estimates
with a more realistic model of opinion content (2006, unpublished)
40. Rago, A., Toni, F., Aurisicchio, M., Baroni, P.: Discontinuity-free decision support
with quantitative argumentation debates. In: Proceedings of International Confer-
ence on Principles of Knowledge Representation and Reasoning (KR), pp. 63–73
(2016)
41. Rissland, E.L.: AI and similarity. IEEE Intell. Syst. 21(3), 39–49 (2006)
42. Ruger, T.W., Kim, P.T., Martin, A.D., Quinn, K.M.: The supreme court fore-
casting project: legal and political science approaches to predicting supreme court
decisionmaking. Columbia Law Rev. 104(4), 1150–1210 (2004)
43. Segal, J.A., Cover, A.D.: Ideological values and the votes of U.S. supreme court
justices. Am. Polit. Sci. Rev. 83(2), 557–565 (1989)
44. Segal, J.A., Epstein, L., Cameron, C.M., Spaeth, H.J.: Ideological values and the
votes of U.S. supreme court justices revisited. J. Polit. 57(3), 812–823 (1995)
45. Sim, Y., Routledge, B.R., Smith, N.A.: The utility of text: the case of amicus briefs
and the supreme court. Comput. Res. Repos. (2014)
46. Spitzer, M.L., Cohen, L.: Solving the chevron puzzle. J. Law Contemp. Probl. 57,
65–110 (1994)
47. Sunstein, C.R.: How law constructs preferences. Georget. Law J. 86, 2637–2652
(1998)

312
A. Quemy
48. Tolosi, L., Lengauer, T.: Classiﬁcation with correlated features: unreliability of
feature ranking and solutions. Bioinformatics 27(14), 1986–1994 (2011)
49. Troper, M.: La th´eorie du droit, le droit, l’´etat. In: L´eviathan. Presses universitaires
de France (2001)
50. ˇCyras, K., Satoh, K., Toni, F.: Abstract argumentation for case-based reasoning.
In: Proceedings of International Conference on Principles of Knowledge Represen-
tation and Reasoning (KR), pp. 549–552 (2016)

Using Data Analytics for Continuous
Improvement of CRM Processes: Case
of Financial Institution
Pāvels Gončarovs(&) and Jānis Grabis(&)
Information Technology Institute, Riga Technical University,
Riga LV-1658, Latvia
pavels.goncarovs@gmail.com, grabis@rtu.lv
Abstract. Data analytics capabilities integrated with Customer Relationship
Management Systems play an important role to enable customer-centric sales
activities at ﬁnancial institutions. This paper reports a case study on developing
a data mining model to identify the Next Best Offer (NBO) for selling ﬁnancial
products to bank’s customers. The case study emphasizes importance of col-
laboration among data scientists and business representatives in iterative
reﬁnement of the prediction models. It has been shown that the iterative
reﬁnement and combination of various modeling techniques lead to accuracy
improvement by 30% and facilitates acceptance of the modeling results.
Keywords: Data analytics  Next Best Offer  Analytical CRM  Data mining
process  Combination of association  Classiﬁcation and clustering
1
Introduction
The ability to access, analyze, and manage vast volumes of data while rapidly evolving
the Information Architecture has long been a goal at many Financial institutions [12].
Many have long standing data warehouses and have used analytics tools. Predictive
analytics and forecasting models in a Big Data environment enable institutions to make
right investment decisions for higher institutional impact.
Financial institutions have found that variety of inbound channels (web, call center,
ATM, branch) is increasing in recent years, while traditional outbound channels (cold
calling, direct mail and messages) are increasingly challenged [8]. Banks with a list of
20 products to sell to every customer are overloaded with information and questions
about the products to sell. Their customers expect personal advice from their advisers
more than ever before because they know the banks have data about them. In the past,
analysts examined data acquisition extensively, but now research has moved to data
analysis [7]. It is this analysis and how the data is subsequently used that makes it so
meaningful. Data analysis needs to be used to form responses to real time shifts in
customer actions and behavior. It must then help facilitate a comprehensive analysis of
the relationship between customers, products, pricing, promotions, and sales.
Data analytics provide an opportunity to transform from a product-centric focus to a
more customer-centric view. Making relevant product offers is a key to building
© Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 313–323, 2017.
DOI: 10.1007/978-3-319-67162-8_31

successful customer relationships. Data analytics, supported by CRM, can be used
throughout the organization, from forecasting customer behavior and purchasing patterns
to identifying trends in sales activities. Banks improve proﬁtability and loyalty by
determining the NBO for every customer interaction. The challenge is to market to the
right customer at the right time with the right offering at the right place. That can be
achieved by using increasingly granular data, from detailed demographics and psycho-
graphics to customers’ transactional data to create highly customized offers These are
called “next best offers” or NBO [8, 13]. The NBO model is a different approach, rep-
resenting a single, continuous campaign or marketing program, that selects the optimal
product’s offer, or decision, at a time. NBO data analysis has been often used as a
background application for customer cross-selling.
In this case study, we investigate a new customer up-sell approach based on using
data analytics in integration with CRM processes. The case study focuses on iterative
development of the analytical model. The model adaptively proﬁles user’s behavior
from their transactional records. Customer segmentation and new direct offers can be
based on user proﬁles. Predictive models at the foundation of NBO technologies create
recommendations that anticipate what a customer wants even before the customer fully
realizes the need. We show how data mining techniques can help in Bank marketing
projects. Moreover, we also show some interesting observations, and thus may moti-
vate new research and development.
The rest of the paper is structured as follows. Section 2 introduces key concepts of
the propose research. Section 3 reports the case study. Section 4 concludes.
2
Foundations
In applications, a data mining process can be broken into six phases: business under-
standing, data understanding, data preparation, modeling, evaluation and deployment
(Fig. 1), as deﬁned by the CRISP-DM [11]. This process is followed to identify NBO.
2.1
Next Best Offer as a Part of CRM Processes
An integrated approach to NBO management requires a broad business perspective –
not just implementing another software package. Typically, the NBO initiative involves
integration with the following infrastructure and tools: (1) analytical CRM (Customer
information storage and business rules and decision automation engine. Predictive
models can be integrated with a business rules engine which drives the workﬂow.);
(2) predictive analysis, data mining, and statistical modeling tools; and (3) visualization
tool. (BI). Enterprise CRM supports all aspects of customer’s life cycle (Fig. 2).
Analytical CRM deals with the analysis of customer data for strategic or tactical
purposes to enhance both customer and ﬁrm value. The realized outcomes from ana-
lytical CRM depend on the quality of the underlying data, the sophistication and skill
of analytical methods and their application. Customer data are the lifeblood of CRM, so
ﬁrms need to build a knowledge management strategy that will support the collection,
analysis [9].
314
P. Gončarovs and J. Grabis

Analytical CRM framework distinguishes between strategic, analytical, and oper-
ational aspects of CRM. Strategic CRM refers to the managerial decision-making
processes involved with deﬁning and building a customer-oriented business strategy,
business processes and culture, and requisite supporting technology models. This
strategic model encompasses a variety of intelligent and creative executive decisions,
as captured at the center of the CRM framework.
2.2
Research Questions
The two research concerns addressed in this paper are:
Fig. 1. CRISP-DM (Cross Industry Standard Process for Data Mining) [11]
Fig. 2. CRM supports customer life cycle
Using Data Analytics for Continuous Improvement of CRM Processes
315

1. IT (data scientists) and Business representative’s collaboration;
2. To identify customer behavior proﬁles’ inﬂuence to propensity prediction that
customer to buy the offered product.
The intelligent sales support system needs to be made part of all the sales activity.
The job of IT (data scientists) is not to make fancy models but to facilitate the
decision-making process, and help business users. The biggest gratiﬁcation for job in
data science is through seeing value that your model has generated for the business. The
models will only generate value for businesses when it is deployed properly with the
complete understanding of the end users. Development of models requires predomi-
nantly the understanding of science and business. However, successful NBO deploy-
ment requires understanding of decision engines and the complete business story. Most
importantly it requires creativity to provide right and succinct information to the deci-
sion maker to answer all their questions with simplicity. In this Case Study, Bank relies
on data mining process and that step-by-step six sequential steps to determine the
optimal IT (data scientists) and Business representative’s collaboration strategy.
Creating the NBO strategy is not an exact science, so we must experiment and test.
We survey many techniques related to data mining and data classiﬁcation techniques.
We select clustering algorithm k-means to improve the training phase of Classiﬁcation.
The behavior of a customer is checked regularly by the clustering-based model and an
alert is raised when the behavior deviates from its behavior proﬁle. The source dataset
for Classiﬁcation data mining model is added with behavior categories attributes in the
result of the clustering-based model’s evaluation. In this Case Study, we rely on
modeling step of data mining to determine customer’s behavior clusters inﬂuence to
classiﬁcation’s algorithm’s (decision tree induction) result.
2.3
Literature Review
Financial marketers are sitting on a wealth of opportunity; the data needed to drive
customer experiences that are personalized, relevant, and seamless across channels.
According to the 2016 Digital Trends in Financial Services study, 62 percent of
respondents indicate a single customer view is a top priority in the advancement of
digital maturity. [7] Ideally, CRM is “a cross-functional process for achieving a con-
tinuing dialogue with customers, across all of their contact and access points, with
personalized treatment of the most valuable customers, to increase customer retention
and the effectiveness of marketing initiatives” [9] Role of Analytical CRM continu-
ously increase in enterprise. Analytical CRM is the use of data to develop relationship
strategies.
Creating NBOs is an inexact but constantly improving science. Like any science, it
requires experimentation. Some offers will work better than others; companies must
measure the performance of each and apply the resulting lessons. It would be hard for
any company to incorporate every possible customer, product, and context variable into
an NBO model [8].
Historically, business intelligence and data warehouses have been associated with
back ofﬁce employees. Over time, knowledge workers evolved to demand richer, more
diverse insights. Pervasive BI is the ability to deliver integrated right-time DW
316
P. Gončarovs and J. Grabis

information to all users – including front-line employees, suppliers, customers, and
business partners. As usage matured, requirements to include predictive analytics,
event-driven alerts, and operational decision support have become the norm [2].
Methods for querying and mining Big Data are fundamentally different from tra-
ditional statistical analysis on small samples. Data Mining requires integrated, cleaned,
trustworthy, and efﬁciently accessible data, declarative query and mining interfaces,
scalable mining algorithms, and big-data computing environments. The following types
of data modeling exist: Association, Classiﬁcation, Clustering, Forecasting, Regres-
sion, Sequence Discovery and Visualization [10].
Clustering algorithms have been studied extensively in the last ten years, with many
traditional clustering techniques successfully applied. [4, 5] Clustering techniques has
usually been applied as a ﬁrst step in the data mining process to analyze hidden
structures and reveal interesting patterns in the data [6]. The K-Means algorithm is well
known for its efﬁciency in clustering large data sets [1].
Data classiﬁcation means categorization of data into different category according to
rules. Existing research performed over the classiﬁcation algorithm learns from the
training set and builds a model and that is used to classify new objects. Decision tree is
useful because construction of classiﬁers does not require any domain knowledge. It
can handle high dimensional data. The learning and classiﬁcation steps of decision tree
induction are simple and fast. Their representation of acquired knowledge in tree form
is easy to assimilate by users. Decision tree classiﬁers have good accuracy [1].
3
Case Study
Data mining is integrated as a part of the CRM systems at a Latvian bank. The
integration aim is to improve efﬁciency of CRM activities and that of cross-sell and
up-sell activities in particular. This case study investigates an iterative elaboration of
data mining capabilities as a result of collaboration among business actors and IT
specialist.
3.1
Business Understanding
As indicated above, the NBO analysis is an important part of the cross-sell and up-sell
activities. From the business perspective, in order to identify the NBO for a customer,
the bank focuses on four primary questions:
1. How risky is the customer if he takes up the proposed product?
2. What is the propensity of the customer to take up this product?
3. How proﬁtable is the customer expected to be if he takes up the new proposed
product?
4. What is the utility of the value propositions of the new product to the customer?
The ﬁrst three points focus on the proﬁtability for the bank and the last point
focuses on the use of this product for the customer. Both proﬁtability and customer
centricity need to be balanced. Therefore, all four questions should be considered
before making an offer to the customer. An effective NBO action can increase both
Using Data Analytics for Continuous Improvement of CRM Processes
317

customer value and customer satisfaction by presenting relevant treatments during each
customer interaction. Banks convert a customer from potential attrition to a new
cross-sell lead by using propensity of a customer to buy new products to make
cross-sell offers. Propensity prediction is based on past trends of product take up by
various segments of customers. The advantage of using propensity of a customer to
take up a product for targeting is that the success rate is expected to be higher.
The direct product offer for every customer depends on propensity of that customer
to buy the new product. NBO is identiﬁed following the process shown in Fig. 3.
1. To Make a target list of Bank products (example list)
2. To Sort/ﬁlter target products with highest propensity (from every customer view)
3. To Calculate propensity prediction that customer to buy the offered product (for
every target Bank product)
The products with the highest propensity of the customer to take up this product are
offered to them.
3.2
Data Understanding
To create an effective NBO, detailed data about customers, bank’s offerings and pur-
chasing context must be collected and integrated. The main goal of the Data Under-
standing step is to answer such questions:
1. What data is available for the task?
2. Is this data relevant?
3. Is additional relevant data available?
4. How much historical data is available?
Demographics, socioeconomic, or geographic characteristics of the customers are
the traditionally and widely used variables for customers analysis. Customer intelli-
gence data mining models may be the most powerful and simplest technique for
generating knowledge from CRM data, however, this approach does not consider the
customer behavior data [14]. By ﬁltering and extracting the necessary data from the
•
•
•
•
•
•
•
•
Fig. 3. Identiﬁcation of the NBO process from the business perspective
318
P. Gončarovs and J. Grabis

data warehouses it is possible to develop the behavioural-based customer intelligence
data mining models. In behavioural-based analysis, customer data can be classiﬁed by
Recency, Frequency and Monetary variables. Recency shows the length of time since
the latest purchase. Frequency is the number of purchases in a period and Monetary
indicates the total amount of spending in a period.
In this case, data proﬁling is used to identify data quality problems and to select
relevant data for the data mining model. It is a speciﬁc kind of data analysis used to
discover and characterize important features of data sets. Proﬁling and other forms of
assessment will identify unexpected conditions in the data. A data quality issue is a
condition of data that is an obstacle to a data consumer’s use of that data. In our
approach to resolve a problem means to ﬁnd a solution and to implement that solution
in data source systems.
Talend Data Preparation platform was used to discover data and select relevant data
for the data mining proposal. Smart guides and visual tools help anyone quickly
understand data attributes and quality status. As an example of data proﬁling, Fig. 4.
shows customers age data proﬁle.
3.3
Data Preparation
The good data preparation is a key to producing valid and reliable models. Data
preparation includes table, record and attribute selection, data transformation and
cleaning. The following data preparation tasks we performed in this case: missing
values replacement, uniﬁed date format, data normalization, converting nominal to
numeric, discretization of numeric data, data validation and statistics and balancing
data.
Fig. 4. Customers age data proﬁle
Using Data Analytics for Continuous Improvement of CRM Processes
319

Balancing data is one of the most signiﬁcant data preparation activities in our case.
Taking the simple route, a modeling system might collapse to always predicting the
majority class (is not interested in express credit), and thereby claim a performance of
90% (as a measure of simple, unweighted accuracy). But in our case, correctly pre-
dicting an example of the minority class (is interested in express credit), is more
important than accidentally misclassifying an example of the majority class. An
approach that has worked for us is balanced stratiﬁed sampling.
3.4
Modeling
Predictive models discover patterns by processing complex datasets. There are different
types of models depending on their purpose and type. In the NBO identiﬁcation case,
association, classiﬁcation and clustering methods are used. These methods are used in
three sequential steps: (1) to group customers by behavior data (Clustering) (2) to
calculate propensity of the customer to take up this product (Decision Tree Induction)
and (3) to make bank’s products preference recommendations (Association Rule
Mining).
Customer behavior data, like accounts balance, payments counts and sums, were
selected and grouped by months for last year. The training dataset consists of customers
who have already taken a target bank product and their behavior data for one year long
period before the deal was made are selected. For each customers’ behavior group of
metrics, 6–12 clusters were created (Fig. 5). This model outputs the cluster centers for a
predeﬁned number of clusters using K-means algorithm.
Clusters with a signiﬁcant proportion of customers who currently have taken the
target product, deﬁne the target customer’s behavior model and indicate a right moment
to make the target product offer. The source dataset with behavior categories attributes
is added to the data mining model at the end of the ﬁrst step.
In the second step, we deploy a decision tree because of its nice tree visualization
and highlighting property. For algorithm’s training reason, we use the 70% partition of
the data for training and the small remaining amount (30%) for evaluation. To train a
decision tree, we specify the column with the class values to learn (Customer sign the
target product agreement), an information (quality) measure, a pruning strategy (if any),
the depth of the tree through the number of records per node (20 records), and the split
strategies for nominal and numerical values. At the end of the training phase, the
Fig. 5. Customers behavior clusters
320
P. Gončarovs and J. Grabis

decision tree’s model shows the decision path through the tree to reach leaves with
signing the target product agreement and not signing the target product agreement
customers.
In the third step, Data mining workﬂow takes Customers product data from the
CRM system and uses an association predictive analytics technique to make bank’s
products preference recommendations for each customer. The model uses the Apriori
algorithm (Agrawal et al. 1993). Frequent item set mining and association rule
induction [Agrawal et al. 1993, 1994] are powerful methods for so-called market basket
analysis. With the induction of frequent item sets and association rules one tries to ﬁnd
sets of products that are frequently bought together, so that from the presence of certain
products in a customer portfolio one can infer (with a high probability) that certain
other products are present. Such information, expressed in the form of rules, can be
used to increase the number of bank products sold. In addition, the workﬂow creates
recommendations report that shows the top products and the other bank products
associated with each in the form “Others who have X also have….”.
Decision tree’s model result and association rule’s recommendations both are
playing a very important role in the NBO. In our case, the recommended product with
the highest propensity of the customer to take up this product is offered to them.
3.5
Evaluation
When we trained a model, it is important to check what if the model has not learned
anything useful? We need to evaluate it before running it for real on real data. For the
evaluation, we use that 30% of data we have kept aside and not used in the training
phase, to check model quality. Evaluation process applies the model to all data rows
one by one and produces the likelihood that that customer has of signing given his/her
contract and operational data (P (Interested in target product = 0/1)). Depending on the
value of such probability, a predicted class will be assigned to the data row (Prediction
(Interested in target product) = 0/1). The number of times that the predicted class
coincides with the original (Interested in target product) class is the basis for any
measure for the model quality as it is calculated by the Scoring process.
IT (data scientists) and Business representative’s collaboration
The Data mining models will only generate value for businesses when they are
deployed properly with the complete understanding of the end users. This is by far the
most difﬁcult part of a data scientist’s job – developing models in contrast is much
Table 1. NBO pilot project’s allocated resources and model accuracy by project iteration
Iteration nr. Spending time Final model accuracy
1
120 h
58%
2
+20 h
59%
3
+20 h
66%
4
+25 h
72%
5
+10 h
72%
6
+20 h
73%
7
+20 h
75%
Using Data Analytics for Continuous Improvement of CRM Processes
321

simpler. It is necessary strong communication between IT (data scientist) and business
representatives in model evaluation step and in all data mining projects steps.
As shown in Table 1. Seven iterations ware made for NBO pilot project. (Express
credit product analysis). It is necessary to have strong cyclical, interactive communi-
cation between IT (data scientist) and business representatives.
3.6
Deployment
The last step in the data mining process is to deploy the models to a production
environment. Deployment is important because it makes the models or their results
available to users so that you can perform any of the following tasks:
1. Use the models to create predictions and make business decisions.
2. Embed data mining functionality directly into an application.
3. Create reports that let users request predictions and view trends.
In that time, we can just provide or business users with predictions reports what
they can use in their cross-sell activities. But in future all NBO results will be
embedded into Business intelligence application as new separated data mart.
4
Summary and Conclusion
The paper reported on application of data mining technologies at the bank to identify
the NBO to customers. The main emphasis was on collaboration among data analysis
and business owners. It has been shown that the iterative approach is necessary to
improve prediction accuracy and to attain useful results for the business. The prediction
accuracy was improved by 30% as the result of seven reﬁnement iterations.
The main observations are that the combination of Association, Classiﬁcation and
Clustering types of data modeling are useful approach to develop the NBO solution,
data proﬁling ﬁlters out unessential data, data balancing is required during the data
preparation and strong cycling, interactive communication between IT (data scientist)
and business representatives is necessary in data mining projects.
References
1. Patel, B.N., Prajapati, S.G., Lakhtaria, K.I.: Bonfring Int. J. Data Min. Efﬁcient classiﬁcation
of data using decision tree 2, 6 (2012)
2. Markarian, J., Brobst, S., Bedell, J.: Critical success factors deploying pervasive BI.
Informatica Teradata MicroStrategy 1007, 3 (2007). EB-5408
3. Pegasystems Inc.: Next-Best-Action Marketing: A Customer Centric Approach (2012)
4. Wang, K., Zhao, Q., Lu, J., Yu, T.: K-proﬁles: a nonlinear clustering method for pattern
detection in high dimensional data. Biomed. Res. Int. 2015, 918954 (2015)
5. Jiang, D., Tang, C., Zhang, A.: Cluster analysis for gene expression data: a survey. IEEE
Trans. Knowl. Data Eng. 16(11), 1370–1386 (2004)
6. Hastie, T., Tibshirani, R., Friedman, J.H.: The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer, New York (2009)
322
P. Gončarovs and J. Grabis

7. Digital Trends in the Financial Services and Insurance Sector (2016)
8. Sanjiv, K.R.: Next Best Action: The One-To-One Future. www.wipro.com
9. Tanner Jr., J.F., Ahearne, M., Leigh, T.W., Mason, C.H., Moncrief, W.C.: CRM in
sales-intensive organizations a review and future directions. J. Pers. Sell. Sales Manag. 25,
169–180 (2005)
10. Najafabadi, M.M., Villanustre, F., Khoshgoftaar, T.M., Seliya, N., Wald, R., Muharemagic,
E.: Deep learning applications and challenges in big data analytics. J. Big Data 2(1), 1–21
(2015)
11. Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., Wirth, R.:
CRISPDM 1.0 step-by-step data mining guide. Technical report, CRISP-DM (2000)
12. Sivarajah, U., Kamal, M.M., Irani, Z., Weerakkody, V.: Critical analysis of Big Data
challenges and analytical methods. J. Bus. Res. 70, 263–286 (2016)
13. Croft, J.: Advanced next best offer marketing using predictive analytics. Appl. Mark. Anal.
1(4), 363–376 (2014). AUTUMN/FALL 2015
14. Sasikala, D., Kalaiselvi, S.: Data Mining for Business Intelligence in CRM System. Sri
Vasavi College, (sfw), Bharathiar University, Erode-638 316, India (2016)
Using Data Analytics for Continuous Improvement of CRM Processes
323

Towards a Data Science Environment
for Modeling Business Ecosystems:
The Connected Mobility Case
Adrian Hernandez-Mendez(B), Anne Faber, and Florian Matthes
Department of Informatics, Chair of Software Engineering
for Business Information Systems, Technical University of Munich,
Boltzmannstr. 3, 85748 Garching Bei M¨unchen, Germany
{adrian.hernandez,anne.faber,matthes}@tum.de
Abstract. Modeling the enterprise within its environment is constantly
gaining more relevance in research and practice. This holistic view can
be formalized as business ecosystem models (BEM). However, the BEM
management process is a challenging task, which involves several stake-
holders in a data-intensive process. In this paper, we adapted an existing
data-science approach and introduced a tool called Business Ecosystem
Explorer (BEEx) for supporting the BEM management process in the
context of connected mobility. The BEEx provides role-based user inter-
faces, which empower end-users to manage the BEM. The evaluation of
the BEEx was conducted with twelve companies, indicating the useful-
ness of the BEEx when fulﬁlling BE related tasks.
Keywords: Data science tool · View-driven approach · Business ecosys-
tems · Visualizations · Connected mobility
1
Introduction
As companies must develop agile mechanisms to adapt to ever-changing condi-
tions within their business ecosystem, business ecosystem models (BEM) have
gained relevance [1]. Applying BEM can support enterprises to model themselves
within their business ecosystems and thus applying an outside-in approach of
enterprise modeling.
In addition to suppliers, customers, business partners, and competitors, a
business ecosystem comprises innovative start-ups and companies for poten-
tial future collaboration as well. To model a company’s ecosystem, it is thus
important to continuously analyze the market by scanning news feeds, social
media, start-up databases, companies web pages, etc. Besides the variety of data
sources, the business ecosystem aﬀects various stakeholders of diﬀerent business
units (e.g. the legal, market analysis, customer department, etc.). Thus, model-
ing business ecosystem requires the collaboration between diﬀerent stakeholders
through a data-intensive process. Overall, as knowledge gained from the business
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 324–330, 2017.
DOI: 10.1007/978-3-319-67162-8 32

Towards a Data Science Environment for Modeling Business Ecosystems
325
ecosystem modeling aﬀects the strategic development of an enterprise, the mod-
eling process must conclude by providing relevant information to decision-makers
creating an added value.
To address these challenges, visual decision support and visual analytic sys-
tems and approaches have been proposed and evaluated (cf., [2–5]), ensuring
that visualizing BEM supports stakeholders and decision-makers applying the
“wide lens” [6] and making informed decisions. However, these approaches don’t
address new challenges that have emerged in the domain of information systems
to reduce the time to execute the business ecosystem modeling process. First,
empowering end-users (i.e., users without programming skills) to adapt not only
the BEM but also the visualizations [7]. Second, reducing the complexity of the
User Interface (UI) for such systems, which requires UIs with a minimal feature-
set and an optimal layout based on the roles of the system [8]. In this paper, we
address these challenges by introducing a tool consisting of multiple UIs tailoring
diﬀerent stakeholders’ needs to understand the business ecosystems. We refer to
such a visual analytic system as a “Business Ecosystem Explorer” (BEEx). For
the development of the BEEx, we followed Hevner’s Design Science approach [9].
The here presented prototype is applied to the context of connected mobility
(CM). CM describes the interconnectedness between means of transportation,
especially cars, traﬃc systems and infrastructure [10] due to advancing digitiza-
tion of mobility, and exhibits a currently emerging business ecosystem as actors
with technology-related business models, such as Google and Apple, challenge
the established players [11,12].
The main contribution of this paper is adapting a data science approach to
address variety and velocity of BEM’s data, through the implementation of a
prototype tool, in the context of the project TUM Living Lab Connected Mobil-
ity (TUM LLCM)1, which empowers end-users to manage BEM by providing
role-based UIs.
2
The Hybrid Wiki Approach
The development of an information system that provides a collaborative envi-
ronment supporting the evolution of data models (e.g., BEM and visualizations)
at runtime by diﬀerent end-users (i.e., users without programming knowledge or
skills) was addressed in [21] by introducing the Hybrid Wiki approach. This app-
roach has been used in diﬀerent use cases and domains such as Enterprise Archi-
tecture Management [13] and Collaborative Product Development [14]. To instan-
tiate the BEM we used the updated Hybrid Wiki metamodel as presented in [15].
The Hybrid Wiki metamodel contains the following model building blocks:
Workspace, Entity, EntityType, Attribute, and AttributeDeﬁnition. These con-
cepts structure the model inside a Workspace and capture its current snapshot
in a data-driven process (i.e., bottom-up process). An entity contains a collection
of attributes, and the attributes are stored as a key-value pair. The attributes
1 http://tum-llcm.de/en/.

326
A. Hernandez-Mendez et al.
have a name and can store multiple values of diﬀerent types, for example, strings
or references to other Entities. The user can create an attribute at run-time to
capture structured information about an entity. An EntityType allows users to
refer to a collection of similar entities, e.g., organizations, persons, etc. The Enti-
tyType consists of multiple AttributeDeﬁnitions, which in turn contain multiple
data validators.
3
Modeling the Connected Mobility Bussines Ecosystems
For modeling the connected mobility business ecosystem, we adapted the four
step data science approach proposed in [16] and encoded the data in each step
using the Hybrid Wiki metamodel. The connected mobility BEM is comprised
of three EntityTypes: organizations, relations, and visualizations, which are
enhanced with data in the process:
First, the industry structure is determined using trade publications. Cate-
gories, of both companies and their interrelations, are identiﬁed and each stored as
one AttributeType. For companies, these are automotive OEMS, parts supplier,
public transportation, technology companies, platform and connectivity provider,
new competitors of aﬀected industries, and public institution. As relation types,
we identiﬁed cooperation, (partially) ownership, funding, negotiation, and sup-
ply. In the next step, relevant companies, describing attributes and their rela-
tions to other entities of the ecosystem are collected. AttributeDeﬁnitions such as
company name, abbreviation, logo, URL, short description, headquarter, CEO,
category, and legal form are gathered for each identiﬁed company using compa-
nies’ web pages, newsfeed, social media, etc. Third, the BEM is represented using
an explicit visual model language. Each visualization has two elements: the ﬁrst
element is the link between the data model and the visualizations, stored in an
AttributedDeﬁntion for each visualization. The second element is the speciﬁcation
of the visualizations, which is described using the visual encodings of the visual
grammar Vega2, as presented and described in [17].
Finally, the collected information are visualized, analyzed, and interpreted.
Figure 1 shows the visualizations implemented in the connected mobility BEEx.
The visualizations can support stakeholders analyzing the ecosystem. For exam-
ple, the modiﬁed ego-network can be used to understand which types of com-
panies are necessary to provide a mobility service, by visualizing the mobility
services in the center and the mobility service providers – oﬀering the mobil-
ity service solutions with a variety of diﬀerent relations – on the outside cir-
cle. Additionally, this visualization allows the interpretation which companies
already address the growing demand for mobility as a service providing mobility
services, thereby acting innovatively.
4
The Connected Mobility Business Ecosystem Explorer
The development and maintaining of an information system that provides a
collaborative environment supporting the evolution of both the model and its
2 https://vega.github.io/vega/.

Towards a Data Science Environment for Modeling Business Ecosystems
327
Fig. 1. BEEx Visualizations used in the connected mobility BEM
instances at runtime by stakeholders and ecosystem experts (i.e., the Hybrid
Wiki approach) can be considered a diﬃcult task. This process involves nontrivial
design decisions and a reference architecture of the connected mobility BEEx.
4.1
Design Considerations
Four main requirements drive the design considerations. First, the BEEx must
support semi-structured and non-structured data (i.e., data variety). Second, the
end-users should be able to modify the BEM and visualizations at run-time (i.e.,
without the need to recompile the system). Third, the BEEx must provide role-
based UIs. Finally, the BEEx must be supported by WEB-based technologies. In
this paper, we describe the decisions we made regarding the following concerns:
development process, architecture style, and visualizations technology
We followed a model-driven development (MDD) process, which is used on
the Hybrid Wiki approach. This decision ensures that the system can handle
diﬀerent types of data, thereby the CM BEEx can address the data variety chal-
lenge in the associated scenario. However, this approach increases the complexity
of the system [15]. Therefore, we select the resource-oriented architecture (ROA)
as architecture style. The ROA enables the communication between the diﬀerent
components using deﬁned resources encoded in JSON format, which increases
the performance in the communication of WEB-based systems (i.e. using the
HTTP protocol) [18]. Thereby, the complexity of the system that supports the
Hybrid Wiki approach can be encapsulated. Hence, each component in the sys-
tem utilizes the optimal resources/features that guarantee the role-based UI.
The visualizations technology selected was based on the visual grammar
Vega. Instead of using an imperative approach (e.g. D3JS3), we select a
3 https://d3js.org.

328
A. Hernandez-Mendez et al.
declarative approach, which guarantees the description of the visualization using
a common visual grammar. This decision ensures that end-users can create visu-
alizations, and bind them to the current BEM at run-time. Additionally, this
approach addresses the data velocity challenge since it allows the end-users to
continuously adapt the BEM and the visualizations.
4.2
Architecture
The BEEx architecture is shown in Fig. 2. The BEEx architecture ensures a clear
separation between execution and development environments, and it is composed
of four main components. The ﬁrst component is the BEEx Modeler, which is
the core component of the modeling environment, allowing the BE Modeler to
create/update/delete the BEM and the visualizations. The next component is
the BEEx Client, which is the core component of the execution environment,
and it allows the BE User to interact with all the visualizations, providing only
read access to the Models Repository through the Query Service. The Model
Repository component stores the BEM and the visualizations in JSON format,
which are exposes using a RESTful service. Finally, the Query Service component
is responsible for extracting the information from the Models Repository, which
is required in each environment using the stated security constraints.
Fig. 2. BEEx architecture with main components.
4.3
First Evaluation of the BEEx Prototype
For the evaluation process, the BEEx is hosted in a TUM University server under
the following addresses, the execution environment https://ecosystem-explorer.
in.tum.de, and the modeling environment https://vmmatthes17.in.tum.de.
Initially, we conducted nine interviews in semi-structured form with nine
diﬀerent companies within two months, following [19]. We aimed at a balance
between receiving some quantiﬁable evaluation but also enabling interviewees
to vary the depth of answer depending on own capability and willingness [20].
The focus of these interviews was to receive feedback regarding the CM BEM.
A sample of attributes was presented, as well as some types of relations and a
visualization of the combination of entities through relations in the form of a

Towards a Data Science Environment for Modeling Business Ecosystems
329
force-directed layout. All interviewees stated that the BEM supported them in
mastering ecosystem related tasks and that their knowledge of the connected
mobility ecosystem was increased. Some suggested immediately additional sce-
narios, for example, the patent management in the pharmaceutical industry.
We used the interviews’ results to update the connected mobility BEEx. As a
next step, we conducted three in-depth interviews with additional three compa-
nies. To obtain a wider range of opinions, we selected three companies of diﬀerent
ﬁelds of activity. Namely, an automotive OEM, a publicly funded non-research
institution and a software company with main business area addressing con-
nected mobility, all actively modeling their business ecosystem. All stated their
perceived limitations with the current in use tools. Additionally, two companies
conﬁrmed that diﬀerent stakeholders within in the enterprise have diﬀerent view-
points towards the enterprise’s business ecosystem. As a limiting factor, it should
be noticed that the presented BEEx prototype required further developed within
the conduction of these in-depth interviews. Nevertheless, all companies agreed
that the prototype foster the understanding of the connected mobility ecosystem
and two continued that it would be interesting to use such a tool within in their
enterprise to collaboratively manage the business ecosystem evolution.
5
Conclusions
In this paper, we adapted an existing data-science approach and introduced a
tool for supporting the BEM process. The design decisions and the architec-
ture ensure that end-users can address variety and velocity of BEM’s data in
the context of the connected mobility business ecosystem. The BEEx provides
role-based UIs, which empowering end-users to manage the BEMs. The ﬁrst
BEEx evaluation indicates that the proposed approach and tool conforms to the
enterprise tasks for BEM. However, further evaluation is required to evaluate
the potential of the BEEx on supporting diﬀerent stakeholders in completing
diﬀerent BEM related tasks.
References
1. Bosch, J.: Speed, data, and ecosystems: The future of software engineering. IEEE
Softw. 33(1), 82–88 (2016)
2. Park, H., Bellamy, M.A., Basole, R.C.: Visual analytics for supply network man-
agement: System design and evaluation. Decis. Support Syst. 91, 89–102 (2016)
3. Huhtamaki, J., Rubens, N.: Exploring innovation ecosystems as networks: Four
european cases. In: Proceedings of the Annual Hawaii International Conference on
System Sciences March 2016, pp. 4505–4514 (2016)
4. Basole, R.C., Huhtam¨aki, J., Still, K., Russell, M.G.: Visual decision support for
business ecosystem analysis. Expert Syst. Appl. 65, 271–282 (2016)
5. Evans, P.C., Basole, R.C.: Revealing the api ecosystem and enterprise strategy via
visual analytics. Commun. ACM 59(2), 26–28 (2016)
6. Adner, R.: The Wide Lens: What Successful Innovators See That Others Miss.
Portfolio Penguin, New York (2013)

330
A. Hernandez-Mendez et al.
7. Satyanarayan, A., Heer, J.: Lyra: An interactive visualization design environment.
Comput. Graphics Forum 33(3), 351–360 (2014)
8. Akiki, P.A., Bandara, A.K., Yu, Y.: Rbuis: Simplifying enterprise application user
interfaces through engineering role-based adaptive behavior. In: Proceedings of the
5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems,
EICS 2013, pp. 3–12. ACM, NY, USA (2013)
9. Hevner, A.R.: A three cycle view of design science research. Scand. J. Inf. Syst.
19(2), 4 (2007)
10. Mitchell, W.: Reinventing the Automobile : Personal Urban Mobility for the 21st
Century. Massachusetts Institute of Technology, Cambridge (2010)
11. Etherington, D., Kolodny, L.: Google’s self-driving car unit becomes waymo (2016).
https://techcrunch.com/2016/12/13/googles-self-driving-car-unit-spins-out-as-
waymo/ Accessed 23 Jan 2017
12. Taylor, M.: Apple conﬁrms it is working on self-driving cars (2016). https://www.
theguardian.com/technology/2016/dec/04/apple-conﬁrms-it-is-working-on-self-
driving-cars Accessed 23 Jan 2017
13. Matthes, F., Neubert, C.: Wiki4eam - using hybrid wikis for enterprise architecture
management. In: Proceedings of the International Symposium on Wikis and Open
Collaboration (2011)
14. Rehm, S., Reschenhofer, T., Shumaiev, K.: Is design principles for empowering
domain experts in innovation: Findings from three case studies. In: Proceedings of
the International Conference on Information Systems (2014)
15. Reschenhofer, T., Bhat, M., Hernandez-Mendez, A., Matthes, F.: Lessons learned
in aligning data and model evolution in collaborative information systems. In: Pro-
ceedings of the 38th International Conference on Software Engineering Companion,
ICSE 2016, pp. 132–141. ACM, NY, USA (2016)
16. Iyer, B.R., Basole, R.C.: Visualization to understand ecosystems. Commun. ACM
59(11), 27–30 (2016)
17. Satyanarayan, A., Russell, R., Hoﬀswell, J., Heer, J.: Reactive Vega: A streaming
dataﬂow architecture for declarative interactive visualization. IEEE Trans. Vis.
Comput. Graphics 22(1), 659–668 (2016)
18. Roberto Lucchi, M.M., Elfers, C.: Resource oriented architecture and rest.
Eur - scientiﬁc and technical research series, EUR 23397 EN - Joint Research
Centre - Institute for Environment and Sustainability
19. Weiss, R.: Learning from strangers : the art and method of qualitative interview
studies. Free Press, New York (1995)
20. Gl¨aser,
J.,
Laudel,
G.:
Experteninterviews
und
qualitative
Inhaltsanalyse:
als Instrumente rekonstruierender Untersuchungen. VS Verlag f¨ur Sozialwiss,
Wiesbaden (2010)

The 2nd International Workshop on
Semantic Web for Cultural Heritage
(SW4CH 2017)

Introducing Narratives in Europeana:
Preliminary Steps
Carlo Meghini, Valentina Bartalesi(B), Daniele Metilli, and Filippo Benedetti
Istituto di Scienza e Tecnologie dell’Informazione “Alessandro Faedo” – CNR,
Via Giuseppe Moruzzi 1, 56124 Pisa, Italy
{carlo.meghini,valentina.bartalesi,daniele.metilli,
filippo.benedetti}@isti.cnr.it
Abstract. We present some preliminary steps towards the introduction
of narratives as ﬁrst-class citizens in digital libraries. The general idea
is to enrich the digital libraries with events providing a rich contextu-
alisation of the digital libraries’ objects. More speciﬁc motivations are
presented in the paper through a set of use cases by diﬀerent actors
who would beneﬁt from using narratives for diﬀerent purposes. We then
consider a speciﬁc digital library, Europeana, the largest European digi-
tal library in the cultural domain. We discuss how the Europeana Data
Model should be extended for representing narratives. We present a tool
supporting the creation and the visualisation of narratives and we show
how the tool has been employed to create a narrative of the life of the
painter Gustav Klimt.
Keywords: Digital libraries · Narrative · Europeana · Ontologies
1
Introduction
Europeana1 is the largest European digital library, containing descriptions of
about 54 millions of cultural heritage objects — books, photographs, maps,
movies, audio ﬁles — provided by more than 3500 of the most important cul-
tural institutions across Europe. Europeana is a digital library for scholars,
researchers, professionals and general users, providing a single access point to
European cultural heritage. Indeed, the digital library collects digital objects
representing cultural heritage items that themselves remain outside the Euro-
peana data space [3]. The current access functionalities of digital libraries are
largely inﬂuenced by Web search engines, whereby users express their informa-
tion need in the form of a natural language query consisting of a few words, and
in response the digital library returns a ranked list of relevant objects. No seman-
tic relations between objects are reported, that is relations connecting objects in
a way that reﬂects their meaning and their context. Europeana is no exception.
Now, it is well known that events play a major role in the contextualization of
objects, by providing an account of the happenings concerning the objects, in
1 https://www.europeana.eu.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 333–342, 2017.
DOI: 10.1007/978-3-319-67162-8 33

334
C. Meghini et al.
terms of the people, the places, the times and the objects that are involved in
such events. In other words, events are the most natural candidates to act as
the “semantic glue” ﬁlling the gaps between objects. Moreover, events are linked
to other events to form “super-events” or stories. However, although events are
included in the Europeana ontology, events do not show up in the Europeana
database due to the fact that they do not show up in the catalogues of libraries,
archives, museums and galleries where the descriptions collected by Europeana
come from. Indeed, events are typically found in historical documents. But the
automatic capturing of events and of their properties from such documents is
an unsolved problem, and will probably remain so for a long time, given the
diﬃculty that machines have to grasp the meaning of the languages used for
inter-human communication (text, images, graphics, audio, audio-visual, and
so on).
In our study, we aim at overcoming the limitations of the search function-
ality of current digital libraries by introducing narrative as a ﬁrst-class concept
in the data model of such digital libraries. The vision is that a user wishing
to know what Europeana has about the Austrian painter Gustav Klimt would
obtain in response not only the ranked list of objects (more or less) concerning
Klimt that the digital library knows about, but also a narrative about Klimt,
that is a semantic network linking such objects in a story that would work as a
contextualisation of the objects themselves, and as such would provide the user
with a larger and more signiﬁcant amount of information. Narrative is a concept
studied in several ﬁelds, from literary studies to cognitive science. Giving an
account of the concept is beyond the scope of this paper. As a matter of fact,
“narrative can be viewed under several proﬁles — as a cognitive structure or
way of making sense of experience, as a type of text, and as a resource for com-
municative interaction” [7]. In our research, we intend a narrative as a semantic
network, meaningful to the user, consisting of events related to one another, to
the entities that compose the events (e.g. agents, places, time, physical objects)
and to the digital objects through semantic relations.
As a ﬁrst step towards the introduction of narratives in the search func-
tionality of digital libraries, this paper presents an experiment we performed on
Europeana. Using a semi-automatic tool to build narratives that we developed,
we constructed a narrative about the life of Gustav Klimt2. This narrative, visu-
alised in form of timeline, could be imported in Europeana and shown as a search
result, thereby placing the objects related to Klimt in the more general context
of the biography (or a part thereof) of their creator.
The paper is structured as follows: Sect. 2 reports some uses cases providing
further evidence of the need of narratives as well as input to the future technical
development. As a ﬁrst step in this direction, Sect. 3 describes how the EDM
needs to be extended in order to represent narratives. As a second step in the
same direction, Sect. 4 presents a tool for building and visualising narratives that
we developed. In Sect. 5 we describe how we created a narrative on the life of
Gustav Klimt and how it could be imported in Europeana. Finally, in Sect. 6 we
report our conclusions.
2 https://dlnarratives.eu/timeline/klimt.html.

Introducing Narratives in Europeana: Preliminary Steps
335
2
Use Cases
In this Section we consolidate our motivations by presenting some use cases of
how narratives could improve the use of Europeana, but also of a digital library
in general, by diﬀerent kinds of users.
– User: Scholar
Scholars, such as historians or biographers, can create and access narratives
about the life and the works of the authors they study. They may provide
their own texts from which the plot was extracted, and they may also be
interested in expressing the primary sources supporting the plot. To this end,
the construction of a narrative by a historian or biographer can be viewed as
an inferential process, using evidence collected from sources to infer proposi-
tions that have been narrated in a text. Diﬀerent scholars can create diﬀerent
narratives on the same topic. The data inserted by the scholar should be
exported in CSV format in order to allow other scholars to make further
analyses on the data.
– User: High School Professor
The tool could be used by a professor as a learning tool. The professor may
create a narrative on a topic of study and show it to the students through
a timeline visualisation. For each created event, the professor should add
her/his description of it using a speciﬁc ﬁeld of the tool. She/he could also
insert fragments from a text book, which are taken into account as secondary
sources. The professor could also add some primary sources for the main
events, in order to examine them in depth. Using the tool the professor can
share her/his timeline with the students. The professor could also enrich
the narratives with Wikimedia Commons3 images or links to related digital
objects included in digital libraries, in order to make the narrative more
attractive for the students.
– User: High School Student
The tool could be used to verify students’ comprehension of history or liter-
ature taught by a professor, who could ask them to create a narrative on a
particular topic using the tool. The students could enrich the narration by
adding appropriate entities and providing textual descriptions of each event,
using the interface of the tool. Alternatively, a quiz could be proposed to the
students on the narrative created by the professor.
– Exhibition or Museum Curator
A narration timeline could be used during a monographic exhibition in order
to associate the works of an artist to her/his biography. The timeline could be
used to help the visitors to better understand the life and works of a painter.
A timeline could also be used in a museum context in order to show to the
visitors the history of the museum and the acquisition of the main artworks. In
the interface it could be useful to have a functionality that allows visualising
only those artworks that are part of the museum’s collections.
Both for exhibitions or museums, the curator who builds a narrative should
3 https://commons.wikimedia.org.

336
C. Meghini et al.
have the possibility to add her/his own new images of the objects kept in the
museum. Furthermore, information about the location of the objects in the
museum rooms could be added through a speciﬁc ﬁeld of the tool.
The ﬁnal user of a museum or exhibition is the visitor, thus, a Web interface
should allow: (i) visualising a narrative on a timeline; (ii) visualising in the
timeline the images of the museum objects; (ii) providing information about
the location of the objects in the rooms of the museum.
– User: Digital Curator
A digital curator would be able to create narratives for the DL objects she/he
would like to promote. For example, a narrative about the Versailles Confer-
ence may be deﬁned by a digital librarian linking the information objects of
the Versailles treaty (e.g. photographs of the people, the ﬁnal declaration,
etc.) to the event representing the Conference. Such event may further be
divided into sub-events which will have to be properly placed on a temporal
axis. This approach aims to build narratives by linking objects to one another
using events. Curators may also be interested in representing the provenance
of the digital objects. The ﬁnal user of a digital curator’s narrative may be a
general user or an expert in the topic of the narration.
3
From the EDM to an Ontology for Narratives
The Europeana Data Model (EDM)4 [5] is an ontology that allows data to be
presented in diﬀerent ways according to the practices of the various organisations
that contribute cultural object descriptions to Europeana. The EDM provides
two diﬀerent approaches for descriptive metadata: “object-centric” and “event-
centric”. The object-centric approach focuses on the object described: informa-
tion comes in the form of statements that directly express the features of the
described object. Such features can be valued as literals or as resources denoting
entities from the real world. A prominent example of an object-centric ontology
is Dublin Core, which is largely used in the cultural heritage sector and beyond.
Event-centric approaches consider that descriptions of objects should focus on
characterizing the various events in which objects have been involved. The idea
is that this approach would lead to establishing richer networks of entities —
by representing the events that constitute an object’s history — than with the
object-centric approach, and in some cases, such as the description of archae-
ological ﬁndings, the only meaningful description. The event-centric approach
underlies models such as the CIDOC CRM [4] and may suit the data of some
(but of course not all) Europeana providers.
An event is represented in the EDM as an instance of the class edm: Event.
The relations connecting events with the resources that characterize events are
represented in the EDM using the three properties: (i) edm:happenedAt, which
links an event to the place of its occurrence; (ii) edm:occurredAt, which links
an event to the time span of its occurrence; and (iii) edm:wasPresentAt, holding
between any resource and an event it is involved in. This is a very general
4 http://pro.europeana.eu/page/edm-documentation#EDMmappingGuidelines.

Introducing Narratives in Europeana: Preliminary Steps
337
property aimed at linking an event with the artifact(s) that were produced by
the event, or with the people that participated (with possibly diﬀerent roles) in
the event, or with the concepts related to the event, and so on.
The concept of event is a core element of the narratives and more generally of
the narratology theory [2]. Conventionally, an event is deﬁned as an occurrence
taking place at a certain time at a speciﬁc location. While the EDM provides the
concept of event, it does not deﬁne any property between events. Thus, it is not
possible to temporally, causally or mereologically order the events that compose
a narrative. Furthermore, the EDM does not provide a classiﬁcation of events
by type, nor deﬁnes roles for the participants in an event. These are limitations
that must be overcome to represent narratives in Europeana.
For the users who want to search for a speciﬁc topic, the Europeana portal
provides a search engine on the textual values of the metadata of the digital
objects included in the digital library. It is possible to reﬁne the search through
some facets provided by the portal: (i) collection (e.g. art, fashion, music); (ii)
media type; (iii) copyright (free re-use, limited re-use or not re-usable); (iv)
providing country; (v) language; (vi) aggregator (organisations performing a data
aggregation role) and (vii) institution that provides the data. The portal does
not currently allow a search based on events, nor does it supply a visualisation
of the digital objects based on the events to which the objects are related.
We performed a SPARQL query through the Europeana SPARQL endpoint5
in order to extract all the instances of the edm: Event class in the current
Europeana database. The query returned zero results. This suggests, as already
pointed out in the Introduction, that the data collected by Europeana is not
organised according to the event-centric approach, nor is this information intro-
duced by the data aggregators that work in the Europeana network.
In order to introduce narratives in Europeana, we developed an ontology for
narratives. A complete description of the classes and relations of the ontology is
available on-line6. To maximise its interoperability, our ontology was developed
as an extension of the CIDOC CRM standard ontology. Since the CIDOC CRM
underlies the EDM, our ontology automatically extends the EDM, re-using its
deﬁnition of event and introducing new relations between events. In particular,
the ontology for narratives allows:
– Representing the events that compose a narrative, linked to each other using
three types of properties:
• Temporal occurrence relation, associating each event with a time interval
during which the event occurs; an event occurs before (or during, or after)
another event just in case the period of occurrence of the former event is
before (or during, or after) the period of occurrence of the latter event.
We reused the intervals deﬁned in Allen’s temporal logic [1], which are
adopted by the CRM.
5 http://sparql.europeana.eu.
6 A description of the classes and relations of our ontology is available at the following
address: https://dlnarratives.eu/ontology.

338
C. Meghini et al.
• Causality relation, linking events that in normal discourse are predicated
to have a cause-eﬀect relation, e.g. the eruption of the Vesuvius caused
the destruction of Pompeii. The causality concept is represented by intro-
ducing a new relation of causal dependency, named causallyDependsOn.
The only causal property of the CRM, P17 was motivated by, cannot be
used for narratives since it relates activities but not events. The causality
relation is deﬁned by the narrator according to his/her own interpretation
of the events.
• Mereological relation, connecting events to other events that include them
as parts, e.g., the birth of Klimt is part of the life of Klimt, represented
using the CRM property P9 consists of.
– Linking an event with the related digital objects included in a digital library.
– Representing the inferential process of a narrator who reconstructs a narrative
starting from the primary sources. In this way our ontology allows to represent
the data provenance.
In order to populate the ontology, we developed a semi-automatic tool that
produces as output an OWL graph [8]. The tool is described in Sect. 4.
4
The Narrative Building and Visualising Tool
In order to aid the user in constructing the narrative, we developed a narrative
building and visualising tool (NBVT for short)7. NBVT is web-based and written
in HTML5, CSS, and JavaScript with jQuery8. It interfaces with a CouchDB9
database to store data and retrieve it on subsequent loadings. The communica-
tion with the database is handled by the PouchDB10 library, which allows NBVT
to store the inserted data locally and optionally synchronise it with a remote
server. Figure 1 shows the main view of NBVT.
To simplify the insertion of data by the user and the subsequent population
of the ontology, we decided to build NBVT on top of an existing knowledge base,
in order to provide the user with a vast and detailed amount of resources with
which to build the narrative. The knowledge base we opted for is Wikidata11,
a free collaborative knowledge base operated by the Wikimedia Foundation [9].
Wikidata was built by extracting structured knowledge from Wikipedia and
other collaborative projects, e.g. Wikisource, Wikibooks, Wikiquote. It currently
contains more than 25 million entities and allows exporting of the data through
the Wikidata API and a SPARQL endpoint called Wikidata Query Service12.
Wikidata encourages collaborative addition and editing of the data by its users
through manual and automated means. NBVT takes as input data inserted
7 https://dlnarratives.eu/tool.html.
8 https://jquery.com.
9 https://couchdb.apache.org.
10 https://pouchdb.com.
11 https://wikidata.org.
12 https://query.wikidata.org.

Introducing Narratives in Europeana: Preliminary Steps
339
Fig. 1. The main view of NBVT: on the left side the entities automatically extracted
from Wikipedia, on the right side the form for constructing an event, and at the bottom
of the ﬁgure, the created events in chronological order.
manually by the user and imported automatically from Wikidata. It initially
imports a few default events from Wikidata, such as births, deaths, company
foundations, etc. The user then adds the remaining events of the narrative one
by one, by inserting the following information:
– The title of the event
– The start and end dates of the event
– The event type
– A set of entities imported from Wikidata or deﬁned by the user
– For each entity, one or more primary or secondary sources and, in the case of
people, the role they played in the event
– A textual description of the event
– Optional textual notes
– One or more digital objects.
At any moment during the narrative construction, the user can switch to the
relations view, which allows her/him to link the events through causal or mere-
ological (part-of) relations through a drag-and-drop interaction.
The output of the web interface is an intermediate JSON representation
of the narrative that is later converted to an OWL graph by a tripliﬁer. The
resulting representation is uploaded into a Virtuoso triple store [6], which can
then be queried to produce visualisations such as tables, graphs, or timelines.
In particular, we use the TimelineJS13 library to build a rich-media timeline
visualisation of the narrative. The collected data will also be accessible through
a SPARQL endpoint.
13 https://timeline.knightlab.com.

340
C. Meghini et al.
5
The Narrative of Klimt’s Life as Case Study
In order to introduce narratives in Europeana, we performed an experiment
creating the narrative of the life of the painter Gustav Klimt. This artist is well-
represented in the digital library, where a search for the string “Gustav Klimt”
currently returns 370 objects.
Fig. 2. An event in the timeline of Klimt’s biography, showing the textual description
of the event, the related digital object, the primary and secondary sources, the related
entities and an image from Wikimedia Commons.
Since we are not art historians nor experts about Klimt, we decided to build
the narrative based on the English Wikipedia page about the painter14. In the
Wikipedia text we identiﬁed the main events of Klimt’s life, and we reproduced
them using NBVT. For each event, we deﬁned the related entities (e.g. persons,
physical objects, location, time). We also reported the Wikipedia fragments of
text that describe the event and an image from Wikimedia Commons and related
to the entities that compose the event. Furthermore, we reported the primary
sources cited by Wikipedia, on the basis of which an event is placed in the
narration. Figure 2 shows the ﬁrst event of Klimt’s life in our narration. When
a user creates an event, NBVT suggests a list of digital objects extracted from
Europeana. The list is built on the basis of the correspondence between the date
of the created event and the value of the “Creation Date” ﬁeld of the Europeana
objects. Through this functionality, we were able to connect digital objects to
the events of our narration, thereby creating a semantic network of events and
14 https://en.wikipedia.org/wiki/Gustav Klimt.

Introducing Narratives in Europeana: Preliminary Steps
341
of the corresponding digital objects. This network is presented as a timeline in
which the events are ordered chronologically. However, it is also possible to (i)
visualise the events in a diﬀerent order, based on their causal or mereological
relations, (ii) highlight the events that contain one or more digital objects, as
shown in Fig. 3, and (iii) ﬁlter the timeline to display only these events.
Fig. 3. Some highlighted events containing digital objects from Europeana.
The Klimt narrative is composed of a total of 54 events. 31 of them are
connected with Europeana digital objects, and 18 are linked to more than one
digital object. The total number of digital objects in the narrative is 127, that is
34% of all Klimt-related objects in Europeana. It should be noted that several
Europeana objects are not related to Klimt’s biography, e.g. posters, modern
objects inspired by Klimt. We estimated that, using NBVT, the manual work
for creating the narrative was about 3 person-days (7 h per day).
Fig. 4. The current results page of the search for “Gustav Klimt” in Europeana.
In order to integrate our narratives into Europeana, NBVT enriches them
with metadata that describe the narrated topic. These metadata could be
matched with those contained in Europeana to enhance its search functionality.
When a user queries Europeana, she/he could obtain as response one or more
narratives related to the topic of the search, along with the classical ranked list of

342
C. Meghini et al.
digital objects. In the web interface, the timeline of each narrative could be visu-
alised by adding a speciﬁc menu section, titled “Narratives”, in the “Reﬁne your
search” menu on the left side of the page. Figure 4 shows the current results
page of the search for “Gustav Klimt” in Europeana. Europeana also has an
“Explore” section in its upper menu, providing a list of particular views on the
Europeana data, e.g. views for people, for time periods, for sources. A new entry
could be added to this section, showing all narratives collected in Europeana.
If there is more than one narrative about the same topic, they could be aggre-
gated by subject. Exploring this page, users could have an immediate idea of all
narratives that Europeana stores and also of the digital objects related to each
narrative.
6
Conclusions and Future Work
We have presented some use cases that would call for the introduction of narra-
tives in digital libraries. We have then discussed how to extend the Europeana
Data Model in order to realize such introduction in Europeana. A ﬁrst experi-
ment has been presented consisting in the creation of a narrative of the life of
Gustav Klimt, the Austrian painter. Such creation has been performed by using
a tool for building and visualising narratives. We plan to extend the experiment
to other artists, thereby further enriching the Europeana database. We are also
working on extending our narrative ontology in various ways: (1) representing
and reasoning about the temporal relations between events; (2) representing in
a richer way the text of narratives, called narrations, and the relation between
narration fragments and their semantic counterparts, events and objects; (3)
introducing narrative templates capturing recurring plots.
References
1. Allen, J.F.: Towards a general theory of action and time. Artif. Intell. 23(2), 123–154
(1984)
2. Bal, M.: Narratology: Introduction to the Theory of Narrative. University of Toronto
Press, Toronto (2009)
3. Concordia, C., Gradmann, S., Siebinga, S.: Not just another portal, not just another
digital library: A portrait of Europeana as an application program interface. IFLA
Journal 36(1), 61–69 (2010)
4. Doerr, M.: The CIDOC conceptual reference module: an ontological approach to
semantic interoperability of metadata. AI Magazine 24(3), 75 (2003)
5. Doerr, M., Gradmann, S., Hennicke, S., Isaac, A., Meghini, C., van de Sompel, H.:
The Europeana Data Model (EDM). In: World Library and Information Congress:
76th IFLA General Conference and Assembly, pp. 10–15 (2010)
6. Erling, O., Mikhailov, I.: RDF support in the Virtuoso DBMS. In: Networked
Knowledge-Networked Media, pp. 7–24. Springer (2009)
7. Herman, D.: Basic elements of narrative. John Wiley & Sons (2011)
8. McGuinness, D.L., Van Harmelen, F., et al.: OWL web ontology language overview.
W3C Recommendation 10(10), 2004 (2004)
9. Vrandeˇci´c, D., Kr¨otzsch, M.: Wikidata: a free collaborative knowledgebase. Com-
mun. ACM 57(10), 78–85 (2014)

Evaluation of Semantic Web Ontologies
for Modelling Art Collections
Danfeng Liu, Antonis Bikakis(&), and Andreas Vlachidis
Department of Information Studies, University College London, London, UK
{danfeng.liu.15,a.bikakis,a.vlachidis}@ucl.ac.uk
Abstract. The need for organising, sharing and digitally processing Cultural
Heritage (CH) information has led to the development of formal knowledge
representation models (ontologies) for the CH domain. Based on RDF and OWL,
the standard data model and ontology language of the Semantic Web, ontologies
such as CIDOC-CRM, the Europeana Data Model and VRA, offer enhanced
representation capabilities, but also support for inference, querying and inter-
linking through the Web. This paper presents the results of a small-scale eval-
uation of the three most commonly used CH ontologies, with respect to their
capacity to fulﬁl the data modelling requirements of art collections.
1
Introduction
The voluminous, diverse and heterogeneous Cultural Heritage (CH) information that is
available in museums, art galleries and other CH institutions, has recently led many of
them to adapt advanced metadata models to better organise their data and enable the
development of enhanced data services to the visitors of their physical collections and
their digital counterparts. Amongst the several available metadata models developed
for such purposes, Semantic Web ontologies are the most widely used, mainly because
of their enhanced expressiveness, allowing to represent complex semantic relationships
among CH entities, but also due to several other properties they enjoy such as
extensibility, generality and inference support.
In the case of art collections, the data modelling challenges are even greater than in
other ﬁelds of CH. Artworks are available in multiple formats (images, texts, etc.), they
are multi-topical (art, science, etc.), multi-cultural and multi-targeted (different recip-
ients [1]. It is therefore even more difﬁcult to develop a data model that can efﬁciently
capture all these different diversities and heterogeneities but in the same time remain
simple in its use.
Motivated by the above observation, this paper attempts to address the following
research questions:
Do the current CH ontologies meet the data modelling requirements of art col-
lections and especially with respect to the following needs of art galleries:
(a) cataloguing (collection & conservation management);
(b) display and publication of metadata (presentation of data);
(c) portals and system management.
© Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 343–352, 2017.
DOI: 10.1007/978-3-319-67162-8_34

Our study is focused on three CH ontologies, which are commonly used for
modelling art collections metadata: the CIDOC Conceptual Reference Model
(CIDOC-CRM), the Europeana Data Model (EDM) and VRA Core. Our methodology
consists of selecting a characteristic sample of artworks and modelling its associated
metadata using the three different ontologies, and then, based on the outcome of the
ﬁrst task, evaluating the three ontologies using a set of criteria associated to the data
modelling needs and requirements outlined by the research questions.
The rest of the paper is structured as follows: Sect. 2 provides the necessary
background information on the three ontologies. Section 3 describes our methodology
in detail. Section 4 presents the results of the evaluation and Sect. 5 concludes.
2
Background
The CH domain was one of the ﬁrst ones to adopt Semantic Web data models, methods
and tools for modelling CH collections and publishing them online [2]. In this domain,
SW technologies are mainly used for two purposes: the development of inner curation
systems and the establishment of open collection databases. Ontologies and data
models such as CIDOC-CRM, EDM and VRA Core are used to standardise the
vocabularies for describing the relevant CH entities and their relationships, and in this
way to enable interoperability among different CH institutions. Below, we provide
some background information on these three data models, and speciﬁcally their most
recent versions (CIDOC-CRM 6.2.11, EDM 5.2.72 and VRA Core 4.03).
CIDOC Conceptual Reference Model (CIDOC-CRM). CIDOC-CRM is a formal
structure developed by the International Committee for Documentation (CIDOC) of the
International Council for Museums (ICOM) for describing the implicit and explicit
concepts and relationships used in CH documentation [3]. Its event centric mechanism,
which employs a vocabulary consisting of 82 classes and 262 properties and following
the RDF semantics, enables the interrelation between people, things, places and
time-spans through common events. In 2016, CIDOC-CRM became an ISO standard
(ISO 21127:2006) for the interchange of cultural heritage information.
Europeana Data Model (EDM). EDM was developed in the context of the Europeana
project as a Semantic Web-based framework for representing cross-domain collection
metadata in museums, libraries and archives [4]. It is aligned to CIDOC-CRM in its
deﬁnition of an event-centric model. To enhance interoperability, it reuses elements
from other Semantic Web vocabularies, such as RDF, the Open Archives Initiative
Object Reuse and Exchange (OAI-ORE) framework, the Simple Knowledge Organi-
zation System (SKOS) namespace, Dublin Core and the W3C Data Catalog Vocabu-
lary (DCAT) [5]. It also introduces 11 new classes and 30 properties.
1 http://www.cidoc-crm.org/Version/version-6.2.1.
2 http://pro.europeana.eu/ﬁles/Europeana_Professional/Share_your_data/Technical_requirements/
EDM_Documentation/EDM_Deﬁnition_v5.2.7_042016.pdf.
3 http://www.loc.gov/standards/vracore/schemas.html.
344
D. Liu et al.

The Visual Resource Association Core Categories (VRA Core). VRA Core was
developed for describing works of visual culture, collections, as well as images that
document them [6]. It, therefore, represents three broad groups of entities, which are
works, images, and collections. Having a much narrower scope than CIDOC-CRM and
EDM, it consists of 19 elements including Title, Record Type, Material, Creator,
Measurements, Technique, Subject, Relation, and Rights. It was originally developed as
an XML Schema, but has recently become available as an RDF Schema.
3
Methodology
The sample we used for the evaluation consists of the four artworks presented in
Table 1. The descriptions of the four artworks contain multiple kinds of information,
meeting our aims for assessing the representation capabilities of the three ontologies. In
the case of the paintings, the descriptions include their technical description, prove-
nance,
exhibition
history,
relevant
bibliography,
but
also
information
about
X-radiographs of the paintings. What is more interesting in the case of the two
sculptures is their relationship: SA is the original work, and SB is the plaster cast of
SA. SB is, however, considered, an artwork itself, created with certain art techniques.
The criteria and measures we used to evaluate the ontologies, presented in Table 2,
were selected after studying the relevant literature on ontology evaluation [7–12]. The
criteria were adopted from the application-based evaluation methodology proposed by
Brank et al. [7]. According to this study, each of the criteria has a speciﬁc purpose:
C1–C2 are associated to the cataloguing needs of a CH institution, C3 is related to
portal & system management, while C4–C6 are related to the presentation and pub-
lication of metadata. The measures were adopted from the Ontology Quality Evaluation
Framework proposed in [8] and the evaluation methodology of [12].
Table 1. The sample of artworks used in our evaluation
Id
Artwork
Artist
Institution
PA Self-Portrait (1659)a
Rembrandt
National Gallery of Art, Washington
PB Queen Elizabeth I (1879)b Unknown
National Portrait Gallery, London
SA David (1501–1504)c
Michelangelo Galleria dell’ Accademia, Florence
SB David (casted 1857)d
Unknown
V&A, London
ahttp://www.nga.gov/content/ngaweb/Collection/art-object-page.79.html
bhttp://www.npg.org.uk/collections/search/portrait/mw02082
chttp://www.accademia.org/explore-museum/artworks/michelangelos-david/facts-about-
david/
chttp://collections.vam.ac.uk/item/O39861/david-plaster-cast-michelangelo/
Evaluation of Semantic Web Ontologies
345

4
Evaluation
4.1
Modelling the Four Artworks
The task of creating ontology-based descriptions of the four artworks using the three
different ontologies consisted of two steps: The ﬁrst step was to create the data model
for each artwork using elements of each ontology. Examples of such data models are
depicted in Figs. 1, 2 and 3. Figure 1 depicts the description of the creation of SA
through three different production events, using terms from CIDOC-CRM. Figure 2
illustrates the use of EDM to describe the relationship between SA and its plaster copy,
SB. Figure 3 demonstrates the use of aggregation in EDM for representing various web
Table 2. Ontology evaluation criteria and measures
Criterion
Measure
Deﬁnition
Lexical, vocabulary &
concept (C1)
Accuracy
Whether the ontology captures
and represents correctly aspects
of the real world
Clarity
The effectiveness of the
ontology in communicating the
deﬁned terms and their intended
meaning
Completeness/competency
Whether the ontology covers all
essential and relevant concepts
in the domain of interest
Hierarchy & taxonomy
(C2)
Conciseness
Whether the ontology includes
any irrelevant or redundant
axioms
Computational-efﬁciency
(C3)
Interoperability
Whether the ontology interacts
or reuses axioms from other
data models
User experience (C4)
Ease of use
Whether it is easy to operate the
ontology and there is
appropriate guidance
Learnability
How easy it is to ﬁnd the
information needed to use, and
whether there is any relevant
documentation
Semantic relations (C5)
Indexing and linking
Whether the deﬁned classes can
act as indices to retrieve the
requested information
Inferencing
Whether the ontology can make
implicit knowledge explicit
through reasoning
Functional adequacy (C6)
Consistent research and
query
Whether the ontology can
achieve better querying and
searching methods
346
D. Liu et al.

resources related to SA. In the second step, we implemented the data models in Pro-
tégé4 by adding appropriate individuals and property assertions with respect to the three
ontologies. The second step aimed at verifying the data models, and also at examining
the ontological inferences that could be made based on the semantics of each ontology.
Figure 4 depicts the OWL/XML statements that describe different image resources
related to PB, using terms from VRA Core.
4.2
Criteria-Based Evaluation
Based on the data modelling tasks described in Sect. 4.1, we assessed the three dif-
ferent ontologies using the measures we discuss in Sect. 3 and present in Table 1. The
results of the evaluation are summarised in Table 3. Sections 4.2.1, 4.2.2 and 4.2.3
discuss in more detail the performance of each ontology.
Fig. 1. The data model of PB using CIDOC-CRM
Fig. 2. Using EDM to model the relationship between SA and SB
4 http://protege.stanford.edu/.
Evaluation of Semantic Web Ontologies
347

Fig. 3. Using EDM to aggregate different online resources
Fig. 4. Using VRA Core to model images of PB
348
D. Liu et al.

4.2.1
CIDOC-CRM
CIDOC-CRM demonstrated its expressive power as a comprehensive ontology model
for modelling cultural heritage information. It contains terms for capturing all aspects
of the descriptions of the four artworks, structuring them in an event-centric way. For
the speciﬁc scope of this experiment, there are, however, quite a lot of non-relevant to
the task classes, which we did not use. Of course, the range of concepts it captures
makes it applicable to a broad range of domains and applications and enhances its
interoperability.
One problem we experienced with CIDOC-CRM was that it was not always easy to
distinguish which class was more appropriate for modelling a speciﬁc entity, as some
of its classes share very similar meanings. For example, both E22 Man-made_Object
and E19 Physical_Man_Made_Thing seem to be appropriate for modelling artworks.
On the other hand, concepts in CIDOC-CRM are very systemically linked and
designed. Once the user gets familiar with the logic and structure of the model, it is
very smooth in its use, either for research or indexing. CIDOC-CRM is speciﬁed in a
language-independent way, and has many expressions, amongst which one is in the
RDF Schema vocabulary.
It is hard to judge the indexing ability of the ontology. On the one hand, it enabled
us to represent nearly all data related to the four artworks. It allows, however, users to
follow different data modelling approaches depending on the needs of the application,
making the implementation of an indexing scheme for artwork difﬁcult. For example,
‘oil on canvas’ (for PA) can be modelled in two different ways: as one individual,
instance of E55 Type, linked to PA through the P2 has type property; or using two
separate instances of E55 Type, ‘oil painting’ and ‘canvas’, linked to PA through the P2
has type property.
CIDOC-CRM is efﬁcient in representing information related to custody and
acquisition. Its expressivity sometimes, does not easily lend to a straightforward
implementation of metadata for describing visual resources, e.g. information describing
an image of the artwork, such as its dimension, time of creation, format, resolution and
technique.
Table 3. Summary of the evaluation of the three ontologies (‘✗’denotes that the ontology
doesn’t exhibit good performance with respect to the corresponding criterion, ‘✓’ denotes that
the ontology performs well, and ‘ ’ that the ontology has excellent performance)
Measure
CRM EDM VRA
Accuracy
✓
Clarity
✗
Completeness/competency
✓
✗
✓
Conciseness
Interoperability
✓
✓
Ease of use
✓
✗
Learnability
✗
Indexing and linking
✓
✓
Inferencing
✗
✓
Consistent research and query
✓
Evaluation of Semantic Web Ontologies
349

4.2.2
EDM
EDM is a metadata schema especially designed for Europeana.org so that its classes
and properties are all appropriate for supporting the speciﬁc tasks of the project.
However, it exhibits a poorly designed concept deﬁnition for some entities, which
makes its use rather confusing. One example is the deﬁnitions of edm:PhysicalThing
and edm:ProvidedCHO; edm:PhysicalThing refers to the persistent physical item that
edm:ProvidedCHO represents. Therefore, the role of edm:PhysicalThing in data
modelling is not clear and may confuse new users of EDM.
EDM is generic, including classes capturing high-level concepts related to artwork,
but does not contain enough specialised classes for representing more speciﬁc metadata
of CH artefacts. For example, we were only able to describe the location of the
artworks at the granularity of country. It also lacks terms for modelling other relevant
CH information, such as production process, acquisition, or technical reports. It is
designed to focus on web resource aggregation and hence only collects the most basic
information related to CH artefacts. Another important point is that EDM is only able
to capture web resources. For example, for the case of the two sculptures (SA and SB),
there was no way to describe their ‘physical’ characteristics in EDM.
The EDM vocabulary is rather difﬁcult to understand and use, as its deﬁnition and
guidelines to its users are unclear in some cases, and lack examples about the use of
terms it imports from external vocabularies.
On the other hand, EDM is a simple and general model for the description of
cultural heritage information, making it appropriate for supporting indexing and
research queries. It can be used to extract exhibition records easily in a simple and clear
representation. And its ability to model aggregated information (through ore:Aggre-
gation, as shown in Fig. 3) is very useful, especially for the information management
needs of a CH portal.
4.2.3
VRA Core
VRA Core was designed to enable object-centred descriptions of artwork with clear
relationships between original work and its images. It smoothly models, for example,
the numerous and complicated visual resources related to PB. It contains all essential
terms to represent useful information about both the artwork and its images in an
accurate, simple and concise way. For example, it uses vra:stylePeriod.style to describe
the art style of the work, which is particularly useful for artwork. Moreover, it provides
speciﬁc terms to describe provenance or acquisition data, such as vra:location.for-
merRepository and vra: location.formerSite.
With respect to the representation of dimensions, it is the most efﬁcient among the
three ontologies; it uses vra:measurements.dimension, vra:measurements.format and
vra:measurements.resolution, which capture the different aspects of dimension for both
physical objects and images.
Note that VRA Core does not provide an element to link a Work to its corre-
sponding Image(s). This is described in [13] as a ‘local implementation issue’. and may
lead to problems when aggregating data from different resources. The solution is rather
partly solved by introducing a generic property called vra:relation, which is used to
describe relations between works and images with domain and range vra:VisualRe-
source (a superclass of vra:Work and vra:Image). A subproperty of vra:relation is
350
D. Liu et al.

vra:relation.depicts, which links instances of vra:Image (domain) to instances of vra:
Work (range). However, vra:Work and vra:Image are not disjoint classes, as some
image might also be a work of art, such as photography. In our modelling task, it was
very difﬁcult to decide whether to deﬁne SB as an image, object or work in this context,
while we also faced the same problem when attempting to represent any copied artwork
of SB.
4.2.4
Evaluation Summary
In Table 4, we summarise the results of the evaluation with respect to the different
research purposes that the ontologies can be used for.
It is obvious that VRA Core and CIDOC-CRM better meet the cataloguing needs of
CH institutions. We evaluate VRA Core as the best among the three ontologies for
metadata presentation due to its efﬁcient data modelling capabilities. EDM, on the other
hand, appeared to be less strong for specialised cataloguing and data presentation.
However, its support for modelling aggregation makes it very useful for the devel-
opment and management of CH portals and information systems.
5
Conclusion
This paper evaluates the three popular Cultural Heritage ontologies with respect to their
abilities to represent works of art. Our evaluation methodology consisted of selecting
four characteristic examples of artwork, for which rich descriptions are available,
creating descriptions of the four artworks using the three ontologies, and (based on the
data modelling tasks) assessing the three ontologies using a set of evaluation criteria
related to different uses of CH ontologies.
The main challenge of this evaluation was that, especially in the case of
CIDOC-CRM and EDM, there were no detailed guidelines on how to use them to
create the descriptions. Although, admittedly, the sample we used is rather small to
generalize our conclusions, our experiment clearly identiﬁes the main strengths and
limitations of each ontology, and its results can be helpful for anyone who wants to
semantically model similar CH information to support different kinds of applications.
Our ﬁndings can be summarized as follows: CIDOC-CRM is a very general ontology,
able to capture a great range of concepts related to the CH domain and in multiple
Table 4. Summary of the evaluation of the three ontologies with respect to their research
purposes
Research purpose
Criteria/metric
CRM EDM VRA
Institutional usage for cataloguing Lexical, vocabulary, concept ✓
✗
Hierarchy, taxonomy
Portals & system management
Computational efﬁciency
✓
✓
Presentation of metadata
User experience
✗
Semantic relations
Evaluation of Semantic Web Ontologies
351

different ways, according to the needs of the underlying application. EDM is more
appropriate for creating and aggregating simpler semantic descriptions. VRA Core is
the most appropriate among the three models for cataloguing purposes and for
describing the relationships between different visual resources.
We plan to extend this work by considering more samples from a greater range of
artwork types. Our endmost goal is to provide guidelines on how to best combine
elements from the three (or other) ontologies in order to satisfy the set of criteria related
to the design purposes of the CH ontologies in the best possible way.
Acknowledgements. This work was partially supported by CrossCult: “Empowering reuse of
digital cultural heritage in context-aware crosscuts of European history”, funded by the European
Union’s Horizon 2020 research and innovation program, Grant #693150.
References
1. Mantegari, G.: Cultural heritage on the semantic web: from representation to fruition. Ph.D.
dissertation, Universita degli Studi di Milano Bicocca (2009). https://boa.unimib.it/handle/
10281/9184
2. Hyvönen, E.: Publishing and Using Cultural Heritage Linked Data on the Semantic Web.
Morgan & Claypool, Palo Alto (2012)
3. Doerr, M.: The CIDOC conceptual reference module: an ontological approach to semantic
interoperability of metadata. AI Mag. 24(3), 75–92 (2003)
4. Doerr, M., Meghini, C., Isaac, A., Hennicke, S., Gradmann, S.: The Europeana data model
(EDM). In: World Library and Information Congress: 76th IFLA General Conference and
Assembly, Gothenburg, Sweden, 10–15 August 2010
5. Europeana: EDM mapping guidelines V2.3 (2016). http://pro.europeana.eu/page/edm-
documentation
6. The Library of Congress: VRA Core 4.0 schemas and documentation (2007). https://www.
loc.gov/standards/vracore/schemas.html
7. Brank, J., Grobelnik, M., Mladenić, D.: A survey of ontology evaluation techniques. In:
Proceedings of the Conference on Data Mining and Data Warehouses (SiKDD 2005) (2005)
8. Duque-Ramos, A., Fernández-Breis, J.T., Stevens, R., Aussenac-Gilles, N.: OQuaRE: a
SQuaRE-based approach for evaluating the quality of ontologies. J. Res. Pract. Inf. Technol.
43(2), 41–58 (2011)
9. Hlomani, H., Stacey, D.: Approaches, methods, metrics, measures, and subjectivity in
ontology evaluation: a survey. Semant. Web J. 1(5), 1–11 (2014)
10. Burton-Jones, A., Storey, V.C., Sugumaran, V., et al.: A semiotic metrics suite for assessing
the quality of ontologies. Data Knowl. Eng. 55(1), 84–102 (2005)
11. Vrandečić, D.: Ontology evaluation. Ph.D. thesis, Karlscruhe Institute of Technology,
Karlscruhe, Germany (2010). http://www.aifb.kit.edu/images/b/b5/OntologyEvaluation.pdf
12. Vrandečić, D.: Ontology evaluation. In: Staab, S., Studer, R. (eds.) Handbook on ontologies,
pp. 293–314. Springer, Heidelberg (2009). doi:10.1007/978-3-540-92673-3_13
13. Van Assem, M.: RDF/OWL representation of VRA (2005). https://www.w3.org/2001/sw/
BestPractices/MM/vra-conversion.html
352
D. Liu et al.

The CrossCult Knowledge Base:
A Co-inhabitant of Cultural Heritage Ontology
and Vocabulary Classiﬁcation
Andreas Vlachidis1(&), Antonis Bikakis1, Daphne Kyriaki-Manessi2,
Ioannis Triantafyllou2, and Angeliki Antoniou3
1 Department of Information Studies,
University College London, London, England
{a.vlachidis,a.bikakis}@ucl.ac.uk
2 Department of Library Science and Information Systems,
Technological Educational Institute of Athens, Athens, Greece
{dkmanessi,triantaﬁ}@teiath.gr
3 Department of Informatics and Telecommunications,
University of Peloponnese, Tripoli, Greece
angelant@uop.gr
Abstract. CrossCult is an EU-funded research project aiming to spur a change
in the way European citizens appraise History, fostering the re-interpretation of
what they may have learnt in the light of cross-border interconnections among
pieces of cultural heritage, other citizens’ viewpoints and physical venues.
Exploiting the expressive power, reasoning and interoperability capabilities of
semantic technologies, the CrossCult Knowledge Base models and semantically
links desperate pieces of Cultural Heritage information, contributing signiﬁ-
cantly to the aims of the project. This paper presents the structure, design
rationale and development of the CrossCult Knowledge Base, aiming to inform
researchers in Digital Heritage about the challenges and opportunities of
semantically modelling Cultural Heritage data.
Keywords: Cultural heritage  Ontology  Digital humanities  Semantic web 
Vocabulary classiﬁcation  CIDOC-CRM
1
Introduction
Without any doubt the era of digital distribution has introduced new exciting avenues
for producing, accessing and consuming information. Within this realm, access to
cultural heritage information has been signiﬁcantly beneﬁted by digital technologies,
facilitating new ways of engaging with heritage and broadening public participation.
Such advances not only enable an interactive engagement with heritage, but also
reinstitute what we mean by heritage and how it can be accessed [1].
The CrossCult Project1 realising the advances of digital technologies, particularly
focused on the aspects of interactivity, recollection, and reﬂection, aims to spur a
1 http://www.crosscult.eu.
© Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 353–362, 2017.
DOI: 10.1007/978-3-319-67162-8_35

change in the way European citizens appraise History. By facilitating interconnections
among pieces of cultural heritage information, public view points and physical venues,
the project aims to foster the re-interpretation of history as we know it, which goes
beyond the conventional siloed presentation of historical data, and focuses on aspects
that are cross-cultural, cross-border, cross-religion, and cross-gender qualities.
A key contribution to this endeavour is the creation of a semantic knowledge base
capable of interrelating a wide set of (existing and future) disparate digital cultural
heritage resources. This paper discusses the scope of the CrossCult Knowledge Base,
the design choices leading to the deﬁnition of its underlying Upper-level ontology, and
the data-modelling outcome of a data sample. The Upper-level ontology delivers for-
malisms that describe the “world” of CrossCult, accommodating common conceptual
arrangements, enabling augmentation, semantic-based reasoning and retrieval across
disparate data resources.
Section 2 outlines relevant projects and the role of standard conceptual models for
mediating semantic interoperability. Section 3 discusses the aims and design choices
leading to the deﬁnition of the CrossCult Upper-level ontology. Section 4 presents the
results of a data modelling exercise aimed at applying the conceptual arrangements and
deﬁnitions of the CrossCult Upper-level ontology to a range of cultural heritage data
resources. The discussion of a particular data modelling follows in Sect. 5, providing
an insight the opportunities and limitations of the adopted modelling method. The last
two sections highlight the most important lessons learned while deﬁning and using the
CrossCult Upper-level ontology and present the future steps towards ﬁnalising the
semantic modelling endeavour.
2
Background
A fundamental problem area in dealing with Cultural Heritage data is to make the
content mutually interoperable, so that it can be searched, linked, and presented in a
harmonised way across the boundaries of the datasets and data silos [2]. In the sphere
of contemporary information science, there is abundance of instruments for managing
and modelling any kind of information including cultural heritage data. The Dublin
Core (DC) Metadata Elements and DC Terms2, the Simple Knowledge Organization
System (SKOS3), the Functional Requirements for Bibliographic Record (FRBR4), the
Europeana Data Model (EDM5), the CIDOC-CRM6, the MIDAS Heritage standard7,
the Lightweight Information Describing Objects (LIDO8) and the VRA Core9 to name
2 http://dublincore.org/documents/dcmi-terms/.
3 https://www.w3.org/2004/02/skos/.
4 https://www.iﬂa.org/publications/functional-requirements-for-bibliographic-records.
5 http://pro.europeana.eu/page/edm-documentation.
6 http://www.cidoc-crm.org/.
7 https://historicengland.org.uk/images-books/publications/midas-heritage/.
8 www.lido-schema.org.
9 https://www.loc.gov/standards/vracore/.
354
A. Vlachidis et al.

but a few, have been employed by numerous projects to harmonise access to content
across disparate datasets [3]. Each model contains merits and limitations determined by
its scope and origin. Some models are deﬁned as nationally accepted standards,
whereas others enjoy an international consent. Some models are domain independent
and lightweight, others are more closely related to particular domain, some are
described as harvesting metadata models and others present integrated manifestations.
In spite of the abundance of models and standards, the nature of cultural heritage
data is such that does not simply lend to a straightforward cataloguing of information in
the same way as warehouse data, administrational information or even library cata-
logues [4]. Inﬂuenced by different scholarly disciplines and perspectives, the cultural
heritage data contain an inherited variability that is reﬂected by a range of different
types of historical objects with their different characteristics. Hence, it is crucially
important semantic interpretation of cultural heritage data to be driven by real world
concepts and events modelling data based on the relationships between empirically
surfaced arrangements rather than artiﬁcial generalisations and ﬁxed ﬁeld schemas [5].
During the past decade, the CIDOC-CRM, a core ontology for cultural heritage
data, has matured and gained a growing popularity among projects aimed at providing
data aggregation and semantic harmonisation of cultural heritage information. Standing
for Conceptual Reference Model (CRM) of the International Council of Museums
(ICOM) – International Committee for Documentation (CIDOC), CIDOC-CRM is a
well-established ISO standard (ISO 21127:2006) in the modelling of cultural heritage
information [6]. It provides an extensible semantic framework that any cultural heritage
information can be mapped to.
The applicability of the CIDOC-CRM in information systems of the broader cul-
tural heritage domain is evident in the literature by numerous large-scale projects such
as, the Oxford University CLAROS10 project, the British Museum ResearchSpace11
and the EU FP7 Ariadne Infrastructure12. The above projects integrate vast datasets of
classical antiquity, museum exhibits and archaeological research respectively, pro-
viding semantic interoperability and access to data based on the ontological and con-
ceptual
deﬁnitions
of
CIDOC-CRM.
Specialisation
of
CRM
instances
to
a
terminological level is achieved by linking to external vocabulary sources, thesauri and
classiﬁcation schemes.
The CRM ontology provides a general mechanism for linking to terminological
specialisations via the implementation of the E55 Type class, which enables connection
to categorical knowledge commonly found in cultural documentation. A common
implementation approach is to link CRM instances to thesauri concepts expressed as
SKOS concepts. Simple Knowledge Organization System (SKOS) is a W3C recom-
mendation designed for representation of thesauri, classiﬁcation schemes, taxonomies,
or any other type of structured controlled vocabulary [7]. It builds upon RDF and
RDFS, and its main objective is to enable easy publication and use of such vocabularies
10 http://www.clarosnet.org/.
11 http://www.researchspace.org/.
12 http://www.ariadne-infrastructure.eu.
The CrossCult Knowledge Base
355

as linked. SKOS structures can be linked to CIDOC-CRM instances to provide a
specialised vocabulary.
3
Upper-Level Ontology – Deﬁnition and Requirements
The CrossCult Upper-level ontology is deﬁned as a generic upper-level conceptual
structure that captures common concepts and relationships across a diverse range of
cultural heritage data. As such, the ontology delivers formalisms that describe the
“world” of CrossCult; it accommodates common conceptual arrangements and enables
augmentation, linking, semantic-based reasoning and retrieval across disparate data
resources. In order to achieve its semantic interoperability aims the Upper-level
ontology adopts a single and generic upper-level design, based on a robust ontological
deﬁnition, enabling efﬁcient semantic-based reasoning and retrieval, while being
scalable to be extended formally to specialised conceptual needs when required.
Speciﬁed as a knowledge representation resource beneﬁting from maximum reuse
of established semantic web resources and standards, the Upper-level ontology adopts
the standard ontology for modelling cultural heritage data, CIDOC-CRM. The use of
CIDOC-CRM guarantees integration under well-deﬁned and interoperable semantics
that support the generic aims of the upper-level structure whilst providing specialisa-
tions that can beneﬁt the individual needs of pilots. On the other hand, CIDOC-CRM as
a formal and generic structure of concepts and relationships is not tied to any particular
vocabulary of types, terms and individuals. This level of abstraction, albeit useful for
the semantics of the broader cultural heritage domain, does not cover the need for a
ﬁner deﬁnition of types, terms and appellations. The need for an additional level of
vocabulary semantics is addressed by the use of thesauri and glossary supplementing
the CIDOC-CRM with specialised terms.
3.1
Rationale and Design Choices
In the process of deﬁning the ontological arrangements, the project reviewed the pilots’
datasets and engaged in a series of meetings before concluding to a set of requirements and
shared semantics across the four pilot’s scenarios and data. The results led to the deﬁnition
of the CC Upper-level ontology, which reuses terminology and maintains full compatibility
with the widely-used standard in cultural heritage documentation CIDOC-CRM (ISO
21127:2006). The version of the upper-level ontology is a subset of CIDOC-CRM
enhanced with additional semantics from the SKOS and FOAF [8] ontology.
The Upper-level ontology accommodates the range of shared semantics of the
following commonly identiﬁed concepts across the four pilots; (a) Physical items, as is
any museum artefact, painting, venue item or landmark, (b) Digital (audio-visual)
content relating to one or more Physical Items, (c) Places of spatial focus, which could
refer to the location of an object, a place of an event or a depicted place on a painting,
(d) Time related deﬁnitions such as dates and periods, (e) Actor as a person or
organisation related to a physical item by properties of ownership creation and illus-
tration and (f) Reﬂective Topics carrying the semantics of subjects and topics of interest
that drive the reﬂection and reinterpretation qualities of the application.
356
A. Vlachidis et al.

The Crosscult speciﬁc class Reﬂective Topic, acts as collection of primarily
physical items (i.e. E22_Man-made Objects) which are aggregated under a common
theme that enables interaction with the content, based on predeﬁned reﬂection and
reinterpretation threads. Instances of the class (threads) can be topics such as Immi-
gration, Women in Society, Healing, Painting Style, etc. Each instance contains links to
relevant subjects from the CCCS vocabulary enabling retrieval and cross-reference,
narratives describing the topic, associations to reﬂection modules (e.g. quiz games and
ratings), while it realises standard CIDOC-CRM relationships across individual
physical items in terms of their location, material, date of production etc. For example,
the individual CC2279 (Fig. 1), is a tombstone of the Middle Antonine period located
at the Museum of Tripolis (Greece), and participating (cc.reﬂects) in the Reﬂective
Topic Woman Appearance. Associations between individual physical items can be
made through the use of a common reﬂection topic whereas other types relationships
can be explored via the standard CIDOC-CRM properties.
3.2
Vocabulary Requirements and Semantics
The upper-level ontology incorporates the SKOS semantics, speciﬁcally the SKOS
Concept and Concept Scheme classes and their associated properties, to provide access
to specialised vocabularies. In CrossCult this need is met by a custom built Classiﬁ-
cation scheme (CCCS) aiming at enhancing the concept representation of the reﬂective
topics developed by the four pilots. This is supplemented by domain dependent
vocabularies of geographical and chronological terms.
The CCCS supplements the CC ontology by providing an additional layer of
semantics through a controlled vocabulary of concepts providing a concise represen-
tation of reﬂection themes and their interrelationships and guiding the reﬂective process
Fig. 1. Data model of museum exhibit 2279 (Archaeological Museum of Tripolis, Greece)
The CrossCult Knowledge Base
357

through these interrelations. The vocabulary incorporated into CCCS accommodates
the reﬂective topics and the relevant social and cultural terminology. In this sense the
Classiﬁcation Scheme can be used as a means for modelling vocabularies contributing
to the cultural heritage domain.
The scheme aggregates terminology from standard thesauri resources such as, the
Arts and Architecture Thesaurus of Getty (AAT), the EUROVOC, the UNESCO
Thesaurus and the Library of Congress Subject Authorities (LC) vocabulary, whereas it
incorporates a limited number of CrossCult speciﬁc terminology designed to accom-
modate specialized needs of the reﬂective process deriving from the pilots’ scenarios
and narratives. The vocabulary is organised and deﬁned in a hierarchical order of
broader-narrower term relationships, whilst CCCS terms can be employed both as
“types” (instances of the E55 Type class) and as “propositional objects” (instances of
E89) to describe the subjects related to individuals of the CC Upper-level ontology.
The reuse of standardised resources ensures the validity of the CCCS structure and
the consistency in the use of its terms. To a lesser extent, project speciﬁc terminology
has been incorporated into the CCCS and has been inter-weaved within its structure. To
ensure the comprehensiveness of CCCS and to maintain the project speciﬁc focus of
the terminology, the contributing terms are derived from the scenario descriptions of
the four pilots and the descriptions of relevant cultural heritage objects, including their
meaning, symbolism, materials, cultural context and creative techniques. The deﬁnition
of the CCCS involved the following steps: (a) Identiﬁcation of relevant vocabulary
based on reviewing pilot scenarios and items involved for the building of the scenarios.
This section relied heavily on cooperation with the historians, museum and venue
curators and social scientists participating to the project as ﬁeld experts; (b) Veriﬁcation
of vocabulary against authority thesauri and incorporation of authority terms as pre-
ferred terms when applicable; (c) Integration of mapped terms into the CCCS structure
considering both original and CCCS hierarchies; (d) Further enhancement of CCCS
vocabulary with related terms, suggested by the mappings with authority thesauri;
(e) reviewing of CCCS structure and supplementing hierarchies as needed.
4
Data Modelling
Data modelling in the context of this paper refers to the speciﬁc process of applying the
conceptual arrangements and deﬁnitions of the CrossCult Upper-level ontology to a
range of disparate cultural heritage data resources. The origin of the data as well their
coverage and granularity vary signiﬁcantly.
Four distinct pilots contribute data to the CrossCult project covering a unique range
of cultural heritage venues across Europe. From the large venue of National Gallery in
London to the considerably smaller venue of the Archaeological Museum in Tripolis
(Greece) and from the archaeological site of thermal springs in Montegrotto (Italy) to
the historical points of interest in the cities of Luxembourg and Malta. Each pilot
contributed data from about 25–30 unique items. The data sample describes museum
exhibits, gallery items, archaeological sites and points of interest in terms of their
unique identiﬁer, associated descriptions, multimedia elements, and relevant keywords
describing their content, use and/or symbolism.
358
A. Vlachidis et al.

The project ingests a wide range of diverse data associated to cultural heritage
objects, events and subjects that span from antiquity to modern times and have a
geographic span that runs across Europe. Hence, data is inherited to a wide array of
formats, technologies, management and classiﬁcation approaches relevant to each data
provider or resource. The data modelling exercise relied on a rigorous set of
Upper-level ontology deﬁnitions in order to express a diverse range of cultural heritage
data on the same level of semantics and with the same degree of granularity.
Overall, the data modelling exercise delivered 80 uniquely identiﬁed items that are
composed of 102 Physical Man Made Objects and 17 Physical Man Made Things. This
translates to 3440 ontology (OWL) statements of named individual declaration and
property assertion.
4.1
Method
The data modelling method addresses issues relating to the diversity of content types,
data formats, and level of data detail. The process is abstracted into three main stages:
(i) selecting and curating the source data for each pilot; (ii) data cleansing and nor-
malisation, followed by data mapping to the Upper-level ontology; and (iii) automatic
data assignment to CC ontology ensuring compliance with the model.
The Manual Data Extraction stage was dedicated to impose a data structure
across a range of unstructured sample data available in text format. The volume of the
data was not such to justify the development of a Natural Language Processing
application for the automatic extraction of information from textual snippets. The task
identiﬁed textual instances of relevant types (i.e. type of exhibit and related material),
temporal and spatial information, dimensions, and other features of interest such as
inscriptions or visual representations.
The Semi-Automatic Database Construction stage aimed at populating a set of
relational database tables with structured data, from spreadsheets originating directly
from the pilots or from the previous Manual Data Extraction stage. The relational
database acted as a mediating layer between the semi-structured data ﬁles and the ﬁnal
OWL output feeding the routines of the Automatic Statements Generation stage with
structured data. The database introduced a series of tables that stored the different types
of CSV data, such as temporal, spatial, dimension, features, and other information
associated to the cultural heritage data and conforming to CIDOC CRM structure of the
ontology.
The Final Automatic OWL Generation stage, ingested the structured data of the
relational database into the CC Upper-level ontology. The process employed a series of
PHP routines driven by SQL queries for retrieving selected database records and
declaring them as ontology individuals using OWL class and property assertions. The
routines cater for the automatic generation of statements with respect to individual(s)
declaration, class assertion, object property assertion, and data property assertion.
String cleansing techniques were also applied for the generation of URI friendly values
whereas in many cases complex SQL Join statements were used for retrieving record
relationships across the database tables.
The CrossCult Knowledge Base
359

4.2
Data Modelling Example
The data modelling exercise delivered a representative example of pilot data with
respect to the semantics of the Upper-level ontology. It managed to harmonise diverse
data under a common semantic layer enriching their structure and enabling inference
and retrieval. A leading modelling choice is the adoption of the specialised
CIDOC CRM classes; E22.Physical Man Made Object and E24.Physical Man Made
Thing, which provide a uniﬁed semantic view to a range of items of interest across the
four pilots. This is augmented by the SKOS Concept and Concept Scheme classes
drawing in the concepts incorporated in the CCCS. Hence, the range of artefacts,
paintings, museum exhibits, monuments, and points of interest is modelled as instances
of the aforementioned specialised classes.
The following example presents the modelling arrangements of museum exhibit
2279 from the Archaeological Museum of Tripolis (Greece). The museum contributes
approximately 25 museums exhibits containing rich descriptions and information about
their temporal, geometrical, spatial and contextual characteristics as seen below.
The example presents some speciﬁc requirements with respect to the modelling of
the provenance of exhibits. The provenance information of the exhibit is accommo-
dated by an E5.Event of type ‘excavation’ that took place in Kynouria (Greece).
Figure 1 captures the semantics of the tombstone with respect to dimension, date of
production, material, and location. The model accommodates relationships to con-
ceptual characteristics that describe the artefact in terms of its reﬂective topic and
subject keywords, these being the notion of death, funerary rites and funerary art
through the ages, etc. It should be noted here that concepts through the structure of the
CCCS can be enhanced at the direct terminology level, i.e. “tombstones” are part of
“cemeteries” and are linked to “funerary sculpture”.
2279: Marble pediment tombstone with a representation of a family 
(enface). The female figure bears a chiton and a cloak. The male figure 
and the boy bear a short chiton. On the architrave there is the 
inscription
C 
. Found in Herod Atticus villa in Loukou, Kynouria. 
Roman era work (middle Antonine era, 161 A.D - 180 A.D.). 
Dimensions: Height 1.60m, Width 0.82m. Location: Room 15, 1st floor
In addition, the inscription of the tombstone is modelled with precise semantics
available from the upper-level ontology where the specialized property P128.carries,
enables the relationship between the actual artefact and the carried inscription to be
fully expressed. It is a different semantic relationship than the P62.depicts that is used
for connecting an artefact with a depicted visual item. It is a ﬁne distinction between
depiction and carried inscription, demonstrating the ﬂexibility and breadth of the
ontology to deal with precise semantics when required.
The notion of women’s dresses is given both as a reﬂective topic and a concept, as
these coincide. The CCCS can lead the user of the app further to the “dress” as a
“culture” element and what this expresses for “women’s appearance”.
360
A. Vlachidis et al.

5
Discussion
The design and development of the CrossCult Knowledge Base is not a straightforward
data modelling exercise, but comes with some interesting research and practical
challenges. The ﬁrst challenge was the selection of the underlying ontology. Despite its
growing popularity in the Cultural Heritage domain and its rich expressive capabilities,
CIDOC-CRM was not an easy selection. Researchers with an Information Science
background preferred solutions based on taxonomies or classiﬁcation systems (e.g.
Dublin Core), while software developers found CIDOC-CRM unnecessary complicated
and verbose for the needs of the platform and mobile apps they develop. Considering
the importance of modelling the relationships between the different cultural heritage
resources used in the project, as well as the need for semantically linking such
resources with external vocabularies and ontologies, we ﬁnally decided to adopt
CIDOC-CRM.
Another critical challenge is related to the population of the ontology with
appropriate individuals and statements describing the available cultural heritage
resources. We presented the process of converting the available unstructured or
semi-structured data into instances of the Upper-level ontology classes and statements
using properties of the ontology. However, the mapping between the terms used by
historians in the original descriptions of the resources and the elements of the ontology
was not in many cases straightforward. Reaching a common understanding of the
precise meaning of the original descriptions, and determining their mappings to the
ontology required extensive communication between the ontology experts and the
historians. By focusing on a representative sample from the four project pilots, we
developed semi-automatic processes, which could then be re-used for all the pilot data.
The different backgrounds of the people who were involved in the development of
the CrossCult Classiﬁcation Scheme (information scientists, historians and museum
experts) brought two more challenges to the project: how to determine the scope of the
vocabulary, and how to come up with a commonly agreed structure. Two decisions that
helped us address such challenges were: (i) to rely as much as possible to standard
external vocabularies such as AAT; (ii) to setup and use an online environment for
collaborative development and management of vocabularies, thesauri and taxonomies.
Among others, the environment enables discussions on the terms and structure of the
ontology, linking the vocabulary to external terms and creating RDF descriptions of the
vocabulary.
6
Conclusion and Future Steps
The paper presented the main design decisions, tasks and challenges associated to the
development of the CrossCult Knowledge Base. Apart from serving the speciﬁc aims
of the project, the research we present in this paper, has three more general contri-
butions to the Digital Heritage domain: (i) it demonstrates the use and deployment of
standard cultural heritage ontologies, which have so far been used mainly for research
purposes, in the context of user-oriented applications; (ii) it develops a vocabulary for
historical reﬂection and integrates it into standard cultural heritage ontologies; (iii) it
The CrossCult Knowledge Base
361

harmonizes datasets describing disparate cultural heritage resources, from museum
exhibits and archaeological sites, to Points of Interest in urban settings.
We also presented a data modelling example, which demonstrated the semantic
description of project pilots’ data with respect to the semantics of the Upper-level
ontology, which underlies the CrossCult Knowledge Base. The next stages will focus:
(i) augmenting the data with media content and narratives that enhance their reﬂection
and re-reinterpretation qualities; (ii) semantically enriching the resource descriptions
with links to external standardised semantic web resources; (iii) further reﬁning the
scope and structure of Reﬂective Topics and their relation to keywords, narratives and
other reﬂection proposals; (iv) extending the ontology to accommodate other
project-related concepts, such as the pilots’ venues and the users of the pilot apps.
Acknowledgements. This work has been funded by CrossCult: “Empowering reuse of digital
cultural heritage in context-aware crosscuts of European history”, funded by the European
Union’s Horizon 2020 research and innovation program, We would like to thank Louis
Deladiennee (LIST), Kalliopi Kontiza (National Gallery), Yannick Naudet (LIST), Joseph
Padﬁeld (National Gallery) and Evgenia Vasilakaki (TEI-A) for their valuable comments and
contributions during the course of this research.
References
1. Adair, B., Filene, B., Koloski, L. (eds.): Letting Go?: Sharing Historical Authority in a
User-Generated World. Left Coast Press, Philadelphia (2011)
2. Hyvönen, E.: Publishing and using cultural heritage linked data on the semantic web. Synth.
Lect. Semant. Web: Theor. Technol. 2(1), 1–159 (2012)
3. Ronzino, P., Amico, N., Niccolucci, F.: Assessment and comparison of metadata schemas for
architectural heritage. In: Proceeding of CIPA, 12 September 2012
4. Oldman, D., Doerr, M., de Jong, G., Norton, B.: Realizing lessons of the last 20 years: A
manifesto for data provisioning & aggregation services for the digital humanities (a position
paper). D-lib magazine, 20(7/8) (2014)
5. King, L., Stark, J.F., Cooke, P.: Experiencing the digital world: the cultural value of digital
engagement with heritage. Herit. Soc. 9(1), 76–101 (2016)
6. Doerr, M.: The CIDOC conceptual reference module: an ontological approach to semantic
interoperability of metadata. AI Mag. 24(3), 75–92 (2003)
7. Miles, A., Bechhofer, S.: SKOS simple knowledge organization system reference (2009).
http://www.w3.org/TR/skos-reference/
8. Brickley, D., Miller, L.: FOAF vocabulary speciﬁcation 0.99 (2014). http://xmlns.com/foaf/
spec/
362
A. Vlachidis et al.

The Port History Ontology
Bruno Rohou1,2(B), Sylvain Laube1, and Serge Garlatti2
1 Centre F. Viete (EA 1161), Universit´e Bretagne Occidentale, Brest, France
bruno.rohou@univ-brest.fr
2 IMT Atlantique, Lab-STICC, Univ. Bretagne Loire, 29238 Brest, France
Abstract. This paper presents a reference ontology, called PHO (Port
History Ontology) as an outcome of a multidisciplinary research project
in the ﬁeld of Digital Humanities (History of Science and Technology or
HST) and knowledge engineering. The PHO ontology is mainly based
on the HST-PORT model representing the spatio-temporal evolution of
ports (from digital Humanities) and the specialization of the CIDOC-
CRM ontology. To ensure genericity and reusability, our ontology relies
on case studies in comparative history of the ports of Brest (France),
Mar del Plata and Rosario (Argentina).
Keywords: History of science and technology · Ontologies · CIDOC-
CRM · DOLCE Lite
1
Introduction
Our research work is part of the research programs “History of port landscapes”
and “Digital Humanities” developed within the framework of the research group
PAM 3D Lab, where Center Fran¸cois Vi´ete (EA 1161) collaborates with Lab-
STICC and CERV. One of those axes concerns the understanding of the scientiﬁc
and technological evolution of ports (Brest in France, Mar del Plata and Rosario
in Argentina) in the contemporary era with a methodological approach consider-
ing the port as a Large Technological System [6]. The aim is to build and validate
new methods in digital humanities both for history and for development of the
scientiﬁc, technological and industrial heritage applied to ports. From a com-
puter perspective, our methods are based on knowledge engineering, ontologies
and semantic web.
The hypothesis is to consider a port as a Large Technological System (or
LTS) [4] whose spatio-temporal evolution as an artifact1 [9] is part of studies in
HST. This evolution can be considered as multi-scale (on both space and time).
The harbor itself consists of a set of artifacts at various levels of granularity such
as reﬁt forms, jetties, wharves, cranes, moorings, or industrial production units
(forges, rope factory, etc.). Periodizing the port in HST requires to highlight
moments of breaks linked to the evolutions of some speciﬁc artifacts and thus
to identify periods where the system is stable between two breaks. In addition,
1 The word “artifact” will be used as equivalent to “human production”.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 363–372, 2017.
DOI: 10.1007/978-3-319-67162-8 36

364
B. Rohou et al.
studying the life cycle of an artifact leads to analyze the nature of human activ-
ities. In the case of LTS, it is a matter of selecting relevant artifacts to periodize
this system and then to characterize the entities in relation to them. Period-
icizing a LTS and deﬁning cycles of evolution of a port over a long period of
time is thus tantamount to selecting relevant artifacts, i.e. artifacts that account
for the studied periodization. Each of the artifacts forms tangible traces of port
evolution at diﬀerent scales of time and space.
The main contributions of our research work are: (i) A port periodization
model, called HST-PORT, developed from the periodization model of ANY-
PORT ports derived from the work of geographers [1]. It is based on ﬁve evolution
phases and a model of human activities with relations between actors, knowl-
edge and artifacts [7]; (ii) A reference ontology called Port History Ontology
(PHO) whose classes, properties and structures are “derived” from the HST-
PORT model. The latter is based on an approach proposed by G. Kassel [5] to
design a formal ontology of artifacts, the CIDOC-CRM ontology, the OWL-Time
ontology and the WGS84-pos ontology for geolocation.
The article begins by a brief state of the art on the CIDOC-CRM ontol-
ogy and its extensions. We then present our HST-PORT periodization model
exempliﬁed with the Rosario harbor (Argentina). The design method for the
PHO reference ontology is then described. We explain the alignment of PHO
with DOLCE, OWL-TIME and WGS84-pos, but more particularly with CIDOC-
CRM. The ports of Rosario and Brest are used to show how the evolution of the
ports is modeled with the ontology PHO and how to query the knowledge base
system to retrieve port evolutions. The article will end with a conclusion and
perspectives.
2
CIDOC-CRM, Cultural Heritage and Ontologies
The modeling of knowledge in the ﬁeld of cultural heritage has already been
the subject of numerous studies leading to the creation of CIDOC-CRM for
the management and enhancement of the museum heritage2. It is an ontology
developed by the International Committee for Documentation (CIDOC) of the
International Council of Museum (ICOM). The aim of the CIDOC-CRM ontol-
ogy is to propose a model describing the concepts and the relations allowing to
describe the cultural heritage of the museums. Its use concerns the tangible and
intangible cultural heritage and more precisely “all types of material collected
and exhibited by museums and related institutions”, as well as “collections, sites
and monuments related to ﬁelds such as history Social, ethnography, archeology,
ﬁne arts and applied arts, natural history, history of science and technology”
[12]. Many communities have adapted CIDOC CRM to meet speciﬁc needs. In
particular, the CRMgeo3 Which combines the CIDOC CRM with GeoSPARQL
2 “The CIDOC Conceptual Reference Model (CRM) provides deﬁnitions and a formal
structure for describing the implicit and explicit concepts and relationships used in
cultural heritage documentation”, http://www.cidoc-crm.org/.
3 CRMgeo: a Spatio-temporal model - http://www.ics.forth.gr/isl/index main.php?
l=e&c=661.

Port History Ontology
365
to provide an ontology providing a standard for the geolocation of cultural her-
itage or the CRMarchaeo4 which is an ontology based also on CIDOC CRM
interested in archaeological excavations. Another extension, called CRMba ded-
icated to archeology to encode metadata about the documentation of archaeo-
logical buildings [3,10,11]. The CRMdig [2] is an extension of the CIDOC-CRM
ontology for capturing the modeling and the query requirements regarding the
provenance of digital objects for e-science. Finally, a domain ontology dedicated
to the conservation-restoration process of cultural objects was designed and focus
on the development of the elements related to events aﬀecting a cultural object
[8]. In the medium term, our main goal is to “submit” an extension of CIDOC-
CRM for history. Our current proposal is an CIDOC-CRM extension for port
history based on our HST-PORT model.
3
The HST-PORT Model
The HST-PORT model uses the fundamental principles of the ANYPORT
model. One of the contributions of this model is to show the evolution of the
relationship between the port and the city. The generic model ANYPORT devel-
oped by Bird [1] allowed to periodize series of ports by studying the evolution
in time of their port facilities. In ANYPORT, the study and description of the
spatio-temporal evolution of a port is based on a set of major evolution phases
and the breaks between these phases from the sources linked to the ports con-
sidered associated with the relevant port entities or facilities - as break traces.
In the same way, our HST-PORT model is composed of:
– A set of major evolution phases: There are ﬁve main phases obtained from
the artifacts’ life cycle. The ﬁve phases are as follows: (i) Phase 0: emergence
of a need/demand; (ii) Phase 1 or phase of project instruction: translation
of this need into a technological problem, emergence of diﬀerent solutions,
choice of a solution; (iii) Phase 2 or phase of the production of the artifact
chosen to solve the problem; (iv) Phase 3 or phase of use of the artifact
accompanied by maintenance and repair phases; (v) Phase 4 or phase of
obsolescence/disappearance of the artifact.
– A set of relevant entities: The considered entities are those that allow the
modeling of human activities, that is to say Actors, Artifacts and Knowledge.
As the ANYPORT model, we assume that it is possible to describe the LTS
evolution from artifacts and human activities. From the port sources, histori-
ans have to ﬁrst ﬁnd and analyze historical sources to identify the ﬁve phases
described above. Secondly, they also have to ﬁnd Artifacts and their relevant
properties which form tangible traces of Port at diﬀerent scales of time and
space. For each of these artifacts, it involved Knowledge, actors and related
4 CRMarchaeo: the Excavation Model - http://www.ics.forth.gr/isl/index main.php?
l=e&c=711.

366
B. Rohou et al.
activities. Now, we show the results of our analyzes of the primary and sec-
ondary sources (10,000 digital photos representing about 500 documents) from
the ports of Brest, Mar del Palta and Rosario.
Example of Relevant Artifacts and Their Properties for Periodization.
At this stage of research, jetties, wharves, cranes and grain elevators are relevant
artifacts because their evolution over time reﬂects the evolution of ports and the
diﬀerent phases. However, these artifacts do not lead to the same periodization
scale (jetties: a century, wharves: several decades, cranes from one year to several
decades). Now, we take the example of the wharf among these artifacts to show
how certain properties are revealing breaks. for this artifact, the revealing break
properties are its characteristic dimensions (length and depth), the geolocation
features, the used technologies and the other artifacts (like crane for example)
present on the wharf. For the “National Wharves” in 1880 in Rosario, these
properties are as follows:
(i) Characteristic sizes: The cumulative length of these platforms is 627 m
and the maximum depth of 7 m. In general, wharves are made up of
small docks (depth less than 3 m), platforms of medium mooring (3 m-5 m),
and docks of great anchorage (beyond 6 m). The data on the length of the
deep docks are as follows: 1880: 625 m; 1906: 1075 m; 1912: 3655 m; 1942:
3655 m. The evolution of length and depth produces a periodization of the
history of this artifact, made of continuity and break.
(ii) The geolocation references: those of Rosario (Coordinates: −32.941397;
−60.632886) and those of the geolocation points determining the contour
of the wharves.
(iii) Technologies: This involves identifying the technology used to build it for
each dock. It will depend directly on the know-how and knowledge available
at the time of the construction activity of the artifact. From a practical
point of view, we will distinguish the technology of the foundation (the
submerged part of the wharf), the technology related to the elevation (the
emerged part of the wharf). It can be a technology related to wood, stone,
concrete.
(iv) Other artifacts present: two types of artifacts have been identiﬁed.
The artifacts related to mobility (crane, voice transport, means of trans-
port...) and those related to storage (warehouse, hangar, grain silos, etc.).
For mobility-related artifacts, the relevant indicators for periodization are
energy, maximum transport capacity and type of artifact (mobile crane,
ﬂoating crane, ﬁxed crane, etc.). For storage artifacts, the total capacity
will be the used indicator.
These analysis, based on our HST-PORT model of port periodization, allowed
us to design our PHO ontology using a design methodology that we will now
explain. By analyzing and studying the diﬀerent historical sources, the HST-
PORT model enable us to acquire the diﬀerent classes of artifacts and related
activities, actors and knowledge to design the PHO ontology.

Port History Ontology
367
4
PHO Ontology Design
From a knowledge engineering perspective, our SHS model is consistent with
the theoretical analysis of G. Kassel [5]. It associates with artefacts (of the same
nature as ours): actions, skills and agents that correspond to our activities,
knowledge and actors. Within this framework, we can consider human activ-
ities as Perdurants involving three classes of Endurants (artifacts, actors and
knowledge). In addition, Kassel shows that it was more pertinent to specialize
a formal ontology like DOLCE rather than others ontologies (Opencyc, SUMO,
etc.) to deﬁne its formal ontology of artifacts. Thus, we specialized the DOLCE
ontology in the same way. We will now specify the alignment of the main classes
of the ontology PHO (Actors, Knowledge, Artifacts and Activities) with CIDOC
CRM, DOLCE, OWL-TIME and WGS84-pos.
The PHO ontology design is based on a specialization of the DOLCE
Lite ontology, the CIDOC-CRM ontology, the “OWL-Time” and “WGS84-pos”
ontologies and the HST-PORT model. We will show in the ﬁrst place how our
PHO reference ontology is organized at the highest level of abstraction. Then,
we will detail the contribution of CIDOC-CRM and its alignment with PHO.
We will end this paragraph by describing the contribution of the HST-PORT
model to the design of the PHO ontology.
4.1
Alignment with DOLCE
The left pane in Fig. 1 shows the alignment Of the classes “Actors”, “Knowl-
edge”, “Artefacts”, and “Activities” in relation to the two subcategories of “phys-
ical objects” and “Non physical objects” and to the Perdurants. Endurants are
entities that persist in time. In the endurants, one distinguishes the “physical
objects” and the “Non physical object”. The former are spatially recognizable.
“Artifacts” is a subclass of the “physical objects”. The class of “Non physical
object” are specialized into two subclasses: “Actors” and “Knowledge”. First,
the class of “Non physical objects” covers the domain of social entities and we
can relate it to the “Actors” class of our ontology (see Fig. 1). Workers, engi-
neers, decision-makers, etc. are considered as actors. Secondly, the class of “Non
physical object” also covers knowledge, that is to say the procedures, the scien-
tiﬁc knowledge implemented in a port. The term “Perdurant” is understood to
mean entities that take place in time and “Endurant” lives by participating in
a “Perdurant” during a Time Interval [5].
It can also be seen that the class “Artifacts” is a subclass of “WGS84-pos:
SpatialThing” in order to spatially locate the actifacts. The main classes of PHO
(Actors, Knowledge, Artefacts and Activities) are not only specializations of the
DOLCE classes, but also CIDOC CRM classes specializations as we will now
specify (multiple inheritance).
4.2
Alignment with CIDOC-CRM
The center and right pane in Fig. 1 shows the alignment of our classes with
those of CIDOC CRM. The classes “Artifacts”, “Actors” and “Knowledge” are

368
B. Rohou et al.
Fig. 1. PHO alignment with CIDOC-CRM, DOLCE and WGS84-pos
respectively subclasses of “E24 Physical Man Made Thing”, “E39 Actor”, “E28
Conceptual object”. The Class “Activities” is a subclass of “E7 Activity” - cen-
tral tree. More precisely:
Artifacts. Artifacts are human productions [9], constructed by the hand of
human beings with a intention. A ﬁrst list of relevant artifacts has been selected
to periodize the history of the evolution of the ports: the wharf, the jetty, the
lifting machine, the storage building (warehouse, silo...), communication chan-
nels.
This concept of artifact is found in a certain way in CIDOC-CRM in a class
called “E70 thing”. If the class “E70 Thing” regroups all kinds of objects, both
objects made by man and objects created by nature; It makes no distinction
between the origin of the objects. This class specializes in several subclasses
including the “E71 Man-Made Thing” class. The class “E71” has many points
in common with the “Artifact” class, the main one being to group man-made
items. However, class “E71” does not diﬀerentiate between a truly constructed
artifact and an artifact left in a project state, between an object that actually
existed and an intangible object. Moreover, class “E71” makes no reference to
the human activity responsible for the existence of the object. The CIDOC CRM
therefore speciﬁed the “E71” class in a subclass called “E24 Physical Man-Made
Thing”. The class “E24” comes close to the deﬁnition of artifact given by Pomian.
This is why we have placed the Artifact class as a subclass of the “E24” class of
the CIDOC CRM and of the “Physical Objects” class of DOLCE.
Actors. In our model, the actors are human beings or a group of human beings.
An actor will produce an activity through the use of an artifact. For the ports,
the actors are very numerous: engineers, construction managers, workers, crane
operators, machine operators, cargo handlers... They will all be responsible for

Port History Ontology
369
an activity and will realize it thanks to their knowledge and their know-how. The
notion of actor is also found in the CIDOC-CRM with the class “E39 Actor”. To
link the activities and actors, CIDOC-CRM foresees a speciﬁc relationship “P14
carried out by” between an activity and the actor who realizes it. The “Actor”
class is therefore a subclass of the “E39-Actor” class of the CIDOC-CRM and
of the “Non Physical Objects” class of DOLCE.
Knowledge. Knowledge can be theory, concept, but also procedure, regulation,
technological know-how, etc. Knowledge is involved in the activity of the actor.
The use of a particular technology in the artifact construction can be considered
a good periodization indicator. If a wooden artifact is replaced by a reinforced
concrete or a lifting device in the construction of a wharf, the replacement of
one motive power by another means that there is a change in technology. The
CIDOC-CRM also groups knowledge in the broad sense in the subclass “E28
Conceptual object”. Thus, the class “Knowledge” is a subclass of the class “E28
Conceptual object” of the CIDOC-CRM and of the “Non Physical Objects” class
of DOLCE.
Activities. Activities represent actions carried out by an actor, involving the
artifact, following procedures or using knowledge or know-how. The life cycle of
an artifact involves many human activities such as artifact design, construction,
repair, use, and destruction. An activity takes place over a period of time; We
can deﬁne its beginning and its end. It is the relationships between the artifact,
the actor and the knowledge that it possesses that will enable the activity to be
carried out. CIDOC CRM also uses the activity concept “E7 Activity” which
specializes in the “Event” class of CIDOC CRM. The “Activities” class is there-
fore a specialization of the “E7-Activity” classes of the CIDOC-CRM and the
“Perdurant” classes of DOLCE.
4.3
Events, Activities and Port Evolution
The CIDOC CRM ontology is an event model that accounts for changes of states,
and therefore for port evolution. We will now show to examples: the Rosario port
and the Brest port. The former enables us to show how our reference ontology
PHO extension of CIDOC CRM and OWL-TIME, allows us to report on Rosario
port evolution. The latter shows how to use SPARQL queries to observe Brest
port evolutions.
The example is as follows: Argentina built in 1880 quays called “National
Wharves” which were demolished and replaced in 1906 by modern quays called
“Importation Wharves”. Then, in 1912, the coastal trade platforms were opened
for trade. In Fig. 2, the location of the port of Rosario can be seen, thanks to
the “cidoc:P53 has formation or current location” property. The evolution of an
artifact is modeled by activities that are limited in time - a beginning, an end.
The property “cidoc:P110i was augmented by” allows us to deﬁne the addition
of three new artifacts (three additions of quays: addition1, addition2, addition3)
in 1880, 1906 and 1912.

370
B. Rohou et al.
Fig. 2. Port evolution of Rosario, “National Quays”
The other two additions modeled by the instances “pho:addition2 quai
rosario” and “pho:addition3 quai rosario” are done in the same way as pre-
viously. On the other hand, the instance “pho:addition3 quai rosario” gives an
account of the addition to the port of Rosario of a new quay line in 1912 repre-
sented by the instance “pho:quai rosario 1912”. This platform line is composed of
two platforms not having the same function. One of them will be to receive cabo-
tage vessels and the other from ocean-going vessels. The “cidoc:P46 is composed
of” property will therefore be used to account for the fact that an artifact may
consist of “Artifacts (docks)”. To represent the function of a wharf, the relation
“cidoc:P19i was made for” is used in the “pho:quai rosario 1912” instance.
The Fig. 3 shows the results of two questions: (i) “On what dates were cranes
added in the port of Brest? what are the number of cranes and the technology
used?”: that is to say the number of cranes, the technology (manual, steam and
Fig. 3. Querying Brest port about cranes and several ports about quays

Port History Ontology
371
Fig. 4. SPARQL queries
electricity) in use and the dates (ii) “On what dates were docks built in the
ports? What are the builders and the technologies used?”: that is to say the
diﬀerent ports, the addition of cranes, the date and their technologies (concrete
block, metallic crate, wood, unknown). The SPARQL queries corresponding to
the previous questions are presented in the Fig. 4: the left one about cranes et
the right one about quays.
5
Conclusion and Perspectives
The PHO ontology is the result of a multidisciplinary research on the modeling
of knowledge at the SHS/STICC joint. In terms of scientiﬁc procedures, have
been developed: (i) in SHS: the HST-PORT space-time evolution model; (ii)
in knowledge engineering: PHO ontology based on the CIDOC-CRM reference
ontology as a translation of the HST-PORT, PHO model which can be considered
as a ﬁrst element of a CIDOC-CRM extension in the domain Of the history of
ports.
Beyond the theme of port history, this research project also aims to develop
and validate reference methodologies in digital humanities. First of all, we see a
very good correspondence between our concepts and those developed by Kassel
in connection with DOLCE. On the other hand, if this work also shows an overlap
with CIDOC-CRM (artifact/actor), several crucial points are locks to work in
a thorough way: the question of the modeling of knowledge and activity on the
one hand and, On the other hand, the fact that CIDOC-CRM certainly allows
to describe events but presents an important gap for historians since it does not
allow to describe states of an endurant. These points are now being worked on in
a broader framework of a collaborative project to create an extension of CIDOC-
CRM initiated by LARHRA (UMR 5190) in Lyon (by and for historians) and
by creating a project Publication of digital corpus on the history of ports on the
symogih.org platform5.
5 http://symogih.org/.

372
B. Rohou et al.
References
1. Claude, M.: C. J. Bird. The major seaports of the United Kingdom. Norois 46(1),
243–245 (1965)
2. Doerr, M., Theodoridou, M.: CRMdig: a generic digital provenance model for sci-
entiﬁc observation. In: TaPP (2011)
3. Guillem, A., Bruseker, G., Ronzino, P.: Process, concept or thing? Some initial
considerations in the ontological modelling of architecture. Int. J. Digit. Libr. 1–11
(2016)
4. Hughes, T.P., et al.: The evolution of large technological systems. In: The Social
Construction of Technological Systems: New Directions in the Sociology and His-
tory of Technology, pp. 51–82 (1987)
5. Kassel, G.: Vers une ontologie formelle des artfacts. In: 20me Journes Francophones
en ingnierie des connaissances, 12 p. (2009)
6. Laube, S., Pourchasse, P., Querrec, S., Garlatti, R., Abiven, M.M.: Histoire com-
pare des arsenaux de brest et venise du point de vue des sciences et des techniques:
approche systmique et humanits numriques. In: Colloque International Les Arse-
naux de Marine du XVIe sicle nos jours, Maison de la Recherche. Maison des Suds,
Universit de Bordeaux-Montaigne (2016, to appear)
7. Laube, S., Rohou, B., Garlatti, S., de Marco, M.A.: Priodiser et comparer lvo-
lution des ports: intrts croiss des humanits numriques et dune approche en his-
toire des sciences et des techniques applique aux ports de brest (france), mar del
plata et rosario (argentine). In: Ports nouveaux, Ports pionniers, XIVe-XXIe sicles,
Ive Colloque international du Rseau de recherche LA GOBERNANZA DE LOS
PUERTOS ATLNTICOS, Lorient, Octobre 2016, to appear
8. Niang, C., Marinica, C., Leboucher, E., Bouiller, L., Capderou, C.: An ontological
model for conservation-restoration of cultural objects. In: Digital Heritage, pp.
157–160. IEEE, Granada, Spain, September 2015
9. Pomian, K.: De l’exception humaine. Le Dbat (2014)
10. Ronzino, P., Amico, N., Niccolucci, F.: Assessment and comparison of metadata
schemas for architectural heritage. In: Proceedings of CIPA (2011)
11. Ronzino, P., Niccolucci, F., Felicetti, A., Doerr, M.: Crmba a crm extension for the
documentation of standing buildings. Int. J. Digit. Libr. 17(1), 71–78 (2016)
12. Szabados, A.-V., Letricot, R.: L’ontologie cidoc crm applique aux objets du patri-
moine antique. In 3e Journes d’Informatique et Archologie de Paris - JIAP 2012,
Paris, France, June 2012. Version auteur de la communication prsente en juin 2012
lors des JIAP-2012, A paratre dans les Actes des 3e Journes d’Informatique et
Archologie de Paris - JIAP 2012

A WordNet Ontology in Improving Searches
of Digital Dialect Dictionary
Miljana Mladenovi´c1(B), Ranka Stankovi´c2, and Cvetana Krstev3
1 EVox Solutions, Belgrade, Serbia
ml.miljana@gmail.com
2 Faculty of Mining and Geology, University of Belgrade, Djusina 7, Belgrade, Serbia
ranka@rgf.bg.ac.rs
3 Faculty of Philology, University of Belgrade, Studentski trg 3, Belgrade, Serbia
cvetana@matf.bg.ac.rs
Abstract. In this paper, we present a method for automatic generation
of a digital resource, which connects all indirect synonyms of a dialect
term to all indirect synonyms of a corresponding term in the standard
language, aiming to improve the search of a digital dialect dictionary.
The method uses SWRL rules deﬁned in the Serbian WordNet ontology
to identify sets of synonymous words. It also uses e-dictionaries to pro-
duce correct lemmas in standard language that users usually employ in
searches. The method was applied and evaluated on verbs and a group
of nouns derived from verbs (verbal nouns). We compared the results
obtained by the system to those produced by humans and achieved the
accuracy of 89.7%.
Keywords: Dialect dictionary · Ontology · WordNet · E-dictionary ·
South Serbian dialect
1
Introduction
The study of dialects has received a new impulse with the development of many
software tools and digital resources for maintaining, enhancing, sharing, visual-
izing and analyzing digital dialect dictionaries. This includes: the development
of digital dictionaries of dialects [1,4,13], the development of tools for the man-
agement of dialectal dictionaries [10], the software for data visualization and
presentation of linguistic dialectal maps [11,12], the analysis of the geographical
distribution of a language and geographical information relevant for linguistic
research [9], the use of Semantic Web-based techniques for representing digital
resources as knowledge based resources and as Linked Open Data (LOD) on the
Web [6,15].
Digital dictionary of the South Serbian dialect1, containing over twenty thou-
sand terms, is the ﬁrst comprehensive implementation [7] of a digital version of
1 On-line at http://www.vranje.co.rs.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 373–383, 2017.
DOI: 10.1007/978-3-319-67162-8 37

374
M. Mladenovi´c et al.
Serbian dialect vocabulary, produced on the basis of traditional dialect dictio-
naries [16,17]. This is the ﬁrst digital resource for Serbian which, in addition to
linguistic information, provides also: sound information (pronunciation) about
terms and examples of the use of words or phrases as they are spoken in the
dialect; graphic information about the geographical location using concepts of
Google Maps; the etymological origin of the words, morphological information
like part of speech, and additional semantic data. The content of the dictionary
can be shared through social networks. Another important aspect of the dic-
tionary is that it allows Web users to expand and complement it. Tools were
developed to enable the search of terms in three ways: (a) search by a term,
(b) search by logical queries created over metadata, (c) terms browsing by ﬁrst
letter. Search by a term implies that the user types a word or its part in the
dialect. He/she can specify whether the search will be carried out for the terms
in the dictionary which: start with the typed word, contain it or are equal to it.
Search results oﬀer information on the number of terms found in the dictionary
that satisfy the given query. This kind of search is standard for on-line dictionary
look-up, but it is based on the presumption that a user knows what she/he is
looking for, which need not be the case for a user not familiar with a dialect.
This problem often encountered by students of a foreign language can be solved
by explaining terms not known in a foreign language by expressing the same
concepts in a language they are familiar with. Two other ways of search (search
by creating a logical query over metadata and browsing of terms by ﬁrst letter)
are more convenient for a novice, but they produce much more information than
is expected or needed, which slows down the learning process.
In this paper we propose a method for connecting the standard language and
the dialect, that would enable search over a digital dialect dictionary by using
terms in the standard language. In Sect. 2, we discuss some previous approaches
to searching digital dialect dictionaries. In Sect. 3, we represent resources used to
improve searching performances of the digital dialect dictionary: Serbian mor-
phological e-dictionaries used to produce all inﬂected forms of standard terms,
and Serbian WordNet (SWN) ontology represented in OWL2 format, for which
we deﬁne the rules expressed in Semantic Web Rule Language (SWRL) to be
used to generate synonymous groups in the SWN ontology on the basis of the
indirect synonymy relation. In Sect. 4, we propose a method for automatic gen-
eration of a digital resource which connects all indirect synonyms of a dialect
term to all indirect synonyms of a corresponding term in the standard language.
In Sect. 5, the method is evaluated on verbs and a group of nouns derived from
verbs – verbal nouns. Finally, we oﬀer some conclusions and suggest directions
for future work in Sect. 6.
2
The Management of Digital Dialect Dictionaries
The challenging task of digitizing a dialect dictionary can be solved in diﬀerent
ways considering software platform, database storage, search framework and
additional management tools. It can be a Web application that uses relational

A WordNet Ontology in Improving Searches of Digital Dialect Dictionary
375
databases, such as Oracle used for storing southern Dutch dialects [13] and
Joseph Wright’s “English Dialect Dictionary” (EED) [8], MS SQLServer for
storing South Serbian dialect dictionary data [7], or it can be one of the machine-
readable and interoperable Semantic Web standards, such as RDF, SKOS and
SKOS-XL which are used, for example, in the case of the dialect dictionary of the
German language [15] and two Austrian dialect dictionaries [2]. When it comes to
search techniques in a dialect dictionary, the metadata are usually used to search
for speciﬁc dialect information. For example, the retrieval of information in EDD
can be limited to the structural units in the entries (heads, deﬁnitions, citations,
comments, variants, etc.), while logical ﬁlters, combining basic Boolean operators
and metadata, can be used for advanced search options. Similar techniques are
used for retrieving information from the South Serbian dialect dictionary data
[7]. Some digital dialect dictionaries like Dutch [15] have records in their database
that contain, besides original headword, additional ﬁelds – dutchﬁeld headword
and search term in standard Dutch – for advanced search options. In this paper
we propose a method which enables a search, not only with one term in the
standard language, but with a set of synonymous terms in order to improve the
search.
3
Resources
3.1
Use of Morphological E-dictionaries
The ﬁrst problem when searching for verbs in a dialect dictionary is the gram-
matical form of the headword of the lexical entry. Namely, the grammatical form
of the headword of the verb lexical entry in the dialect dictionary is ﬁrst person
singular in the present tense, while the user’s intent is to search for verbs using
their inﬁnitive forms, these being headwords for standard language dictionaries.
To support that kind of search, it was necessary to add an inﬁnitive form, that
is, to lemmatize both a dialect verb and verbs in standard Serbian that were
retrieved from its deﬁnition. For the lemmatization task we used Serbian mor-
phological electronic dictionaries and grammars developed within the University
of Belgrade Human Language Technology Group [14].
Morphological electronic dictionaries of Serbian for NLP have been under
development for many years now. In the dictionary of lemmas (DELAS), each
lemma is described in full detail so that the dictionary of forms containing all
necessary grammatical information (DELAF), can be generated from it, and
subsequently used in various NLP tasks. Serbian e-dictionaries of simple forms
have reached a considerable size: they have more than 140,000 lemmas generat-
ing more than 5 million forms and 18,000 multi-word lemmas [5]. Dictionaries
contain mostly standard language, but also some dialect lemmas, as well.
An e-dictionary of forms consists of a list of entries supplied with their lem-
mas, morphosyntactic, semantic and other information, so it is possible to attach
lemma for all inﬂected forms in the dialect dictionary that match a form in the
morphological e-dictionary. After separating all synonyms aligned with a dialect

376
M. Mladenovi´c et al.
lexeme (from the standard language or dialect), inﬁnitive forms were attached
to the original form.
Among 4,152 ﬁltered entries having a dialect form followed by a list of words
or phrases in standard Serbian, 3,452 entries were verbs and the rest were verbal
nouns (gerund). For 3,452 verb entries 7,353 synonyms were detected – related
words or phrases in standard Serbian that describe dialect forms and that have
the same meaning as corresponding dialect words. A few verb entry examples
are:
– batalim | batalen, ostavim, napustim to quit
– batisujem | kvarim, upropaˇs´cujem to ruin
– bednim se | lepo se odevam, doterujem se to dress up
– begam | begaj, ja bega, ti bega, begaje´ci, beˇzim to ﬂee
After lemmatization, we obtained the following result:2
– batalim bataliti | batalen, ostavim ostaviti, napustim napustiti
– batisujem | kvarim kvariti, upropaˇs´cujem upropaˇs´civati
– bednim se | lepo se odevam odevati, doterujem doterivati se
– begam begati | begaj, ja bega begati, ti bega begati, begaje´ci, beˇzim beˇzati
A lemma was assigned for 505 dialect forms out of 3,452 dialect forms given
in the ﬁrst person singular, present tense. Inﬁnitive forms were assigned to
4,384 word forms in standard Serbian that were connected to dialect forms (out
of 7,353). Word forms that were not lemmatized consisted of either another
dialect form e.g. “ja znaja, ti znaja, znam znati to know” not presented in
e-dictioanries, or adjectives used to describe verbs e.g. “zgugurija se, zguguren,
pogurim poguriti se to stoop, to be stooped”.
In the dialect dictionary the relation between verbal nouns and verbs was
established in some entries: “ˇsljakanje | od ˇsljakati to slap”, but it was not done
systematically. Again, we used morphological e-dictionaries in which all verbal
nouns are marked with a special marker which enabled us to establish missing
connections between verbs and verbal nouns. In that way, 700 relations were
established.
3.2
Calculating the Set of Near Synonyms by Using the WordNet
Ontology
The development of the lexico-semantic resource Serbian WordNet (SWN), is
based on the semantic network Princeton WordNet (PWN) [3]. Today, SWN is
a set of more than 22,000 concepts called synsets where a concept is represented
by the set of synonymous word forms that have the same or similar meaning in
a given context. Synsets respect the syntactic categories noun, verb, adjective,
and adverb and can be interconnected by semantic relations, while word forms
can be connected by lexical relations. In SWN ontology there are currently 2,243
verb synsets deﬁned as ontology individuals belonging to the VerbSynset class:
2 Lemmatization was done using Unitex, the corpus processing system (http://
unitexgramlab.org/).

A WordNet Ontology in Improving Searches of Digital Dialect Dictionary
377
<rdf:type
rdf:resource="&swn30;VerbSynset"/>
We wanted to deﬁne rules that can be used to generate synonymous pairs of
verbs found in the SWN ontology that were not based only on the relation of
direct synonymy. By doing that, we created a broader set of synonyms for each
verb deﬁned in SWN ontology. Relations that were used in ﬁnding the broader set
of indirect synonyms are: synonym, similar to, also see, verb group, hyponym.
The total number of synset relations in SWN: synonym 22,162, similar to 371,
also see 242, verb group 191, hyponym 21,554. Figure 1 shows an example of
synsets and relations between them in the SWN ontology which are used to deﬁne
a set of indirect synonymous concepts of the verb “dopustiti – permit, allow, let,
countenance” by using hyponymy-related verbs, synonyms of hyponymy-related
verbs, verbs belonging to the same semantic group of verbs as the observed verb,
and by verbs deﬁned as semantically similar to the observed one.
Fig. 1. Verb synsets in the SWN ontology which are mutually connected with relations
that participate in ﬁnding a set of indirect synonyms of the verb “dopustiti - permit,
allow, let, countenance”.
Although the SWN ontology has been developing by using a free, open-source
ontology editor, Prot´eg´e (http://protege.stanford.edu) and implementing SWRL
rules, in this case we transformed SWN from OWL into Turtle format to be
used in Java+Jena framework. Consequently, the automated reasoning rules for
determining the existence of indirect synonymous pairs in the SWN ontology,
in which the ﬁve previously mentioned relations are involved and which can be
used for generating “indirectSynonymy” relation, are here presented in the form
of the equivalent Jena rules:3
3 We used the following software tools in this paper: Developing tool Eclipse Java EE
IDE Luna and Apache Jena open source software development environment which
allows for reasoning at the level of OWL 2 language by converting SWRL rules into
the Jena rules format.

378
M. Mladenovi´c et al.
"[rule1:(?a eg.:label ?b)(?a eg.:synonym ?c)(?c eg.:label ?e) ->
(?b eg.:indirectSynonymy ?e)]"
"[rule2:(?a eg.:label ?b)(?a eg.:similar_to ?c)(?c eg.:label ?e) ->
(?b eg.:indirectSynonymy ?e)]"
"[rule3:(?a eg.:label ?b)(?a eg.:also_see ?c)(?c eg.:label ?e) ->
(?b eg.:indirectSynonymy ?e)]"
"[rule4:(?a eg.:label ?b)(?a eg.:verb_group ?c)(?c eg.:label ?e) ->
(?b eg.:indirectSynonymy ?e)]"
"[rule5:(?a eg.:label ?b)(?a eg.:hyponym ?c)(?c eg.:label ?e) ->
(?b eg.:indirectSynonymy ?e)]"
By looking at the rules from the set {rule2, . . . , rule5}, it can be seen that each
of them can be expanded with the synonymy relation, yielding the following
expanded set of rules:
"[rule6:(?a eg.:similar_to ?c)(?a eg.:label ?b)(?c eg.:synonym ?d)
(?d eg.:label ?e) -> (?b eg.:indirectSynonymy ?e)]"
"[rule7:(?a eg.:also_see ?c)(?a eg.:label ?b)(?c eg.:synonym ?d)
(?d eg.:label ?e) -> (?b eg.:indirectSynonymy ?e)]"
"[rule8:(?a eg.:verb_group ?c)(?a eg.:label ?b)(?c eg.:synonym ?d)
(?d eg.:label ?e) -> (?b eg.:indirectSynonymy ?e)]"
"[rule9:(?a eg.:hyponym ?c)(?a eg.:label ?b)(?c eg.:synonym ?d)
(?d eg.:label ?e) -> (?b eg.:indirectSynonymy ?e)]";
We have carried out several experiments using diﬀerent lengths of relation
chains taken from the set of ﬁve given relations synonym, similar to, also see,
verb group, hyponym which enabled us to conclude that the suﬃcient length of a
chain is 3. In that way we manually deﬁned 24 rules (combinations of 4 relations
taken 3 at a time) having the form illustrated by the following examples in which
a chain is formed of relations similar to, also see and synonym.
"[rule10:(?a eg.:similar_to ?c)(?a eg.:label ?b)(?c eg.:also_see ?d)
(?d eg.:synonym ?e)(?e eg.:label ?f)->(?b eg.:indirectSynonymy ?f)]"
"[rule11:(?a eg.:verb_group ?c)(?a eg.:label ?b)(?c eg.:also_see ?d)
(?d eg.:synonym ?e)(?e eg.:label ?f)->(?b eg.:indirectSynonymy ?f)]"
"[rule12:(?a eg.:hyponym ?c)(?a eg.:label ?b)(?c eg.:also_see ?d)
(?d eg.:synonym ?e)(?e eg.:label ?f)->(?b eg.:indirectSynonymy ?f)]";
Restrictions that we introduced were the following: (1) a relation from the set
of ﬁve given relations synonym, similar to, also see, verb group, hyponym cannot
be repeated more than once in a given rule; (2) the relation synonym has to be
found only as the last one in a sequence of the given ﬁve relations. In this way
we obtained 33 rules for reasoning about the existence of the indirectSynonymy
relation and, after inferencing, 6,430 indirectSynonymy related pairs of verbs.
4
The Implementation of New Search Features
The proposed method for connecting standard language with the dialect dic-
tionary relies on a table of sets of synonymous standard language words which

A WordNet Ontology in Improving Searches of Digital Dialect Dictionary
379
are related to an equivalent set of dialect entries. This table is used as a part
of the Web tool for an advanced search of the digital dictionary of the South
Serbian dialect. Figure 2 shows resources and procedures included in the process
of generating such a table, named Expanded inverted index table. The table was
created in ﬁve steps.
Fig. 2. Architecture of the system for building a resource that improves the dialect
dictionary search tool.
(1) Automatic extraction of deﬁnitions of verbs and verbal nouns in the digital
dictionary was performed by Transact-SQL stored procedures since dialect
dictionary storing database is MS SqlServer. A total of 4,153 entries rep-
resenting verbs, verbal nouns and their deﬁnitions were extracted, which
represents about 20% of the size of the whole dictionary. Two examples of
table rows (containing an entry and a deﬁnition) obtained in the ﬁrst step
are:
isabim “(imp. isabi; aor. ja isabi, ti isabi; r.pr. isabija, -ila, -ilo) svr. iskvarim,
upropastim.”
to ruin, to destroy
aˇckam “(imp. aˇckaj; impf. aˇckaˇsem) nesvr. kotrljam.”
to roll
(2) E-dictionaries of the standard language were used for detecting verbs occur-
ring in each deﬁnition and for transforming them from a form used in this
dialect dictionary (the nominative case, singular, present tense, for example
a verb upropastim) into the form used in contemporary dictionaries (the
inﬁnitive, for example upropastiti). Two corresponding examples obtained
after the second step are (the inﬁnitives generated by the transformation
process are given in bold):
isabim | isabi, ja isabi, ti isabi, isabija, iskvarim iskvariti, upropastim upropastiti
aˇckam | aˇckaj, aˇckaˇsem, kotrljam kotrljati

380
M. Mladenovi´c et al.
(3) An index table was created by inverting the table obtained in step (2) i.e.
all dialect dictionary entries related to each inﬁnitive representing a verb
in standard language were found. In the case of verbal nouns, they were
joined to verbs from which they were derived, and we subsequently treated
them in the same manner as verbs. In the Subsect. 3.1, we noted that total
of 4,152 headwords of the dialect were related to 4,384 inﬁnitives of verbs
in standard language, of which 4,252 inﬁnitives are unique. For that reason,
this is the length of the inverted table (total number of records). One of
the records is shown below. It is related to the ﬁrst example in step (2) and
represents an inﬁnitive of a verb upropastiti linked to the 8 entries found in
the dialect dictionary whose deﬁnitions contained this verb in the form of
the ﬁrst person singular - upropastim.
upropastiti | isabim, batiˇsem, dokrajiˇsem, istrovim, izabim, izakam, oznobim,
profu´ckam
(4) In this step, the SWN ontology was used in order to implement inference
rules deﬁned in Subsect. 3.2. As a result, a table that in each row contains a
verb lemma and its indirect synonyms (in the standard language), as shown
in the next example, was obtained:
upropastiti | unerediti, uniˇstiti, uprskati, zabrljati, zakrmaˇciti, zasvinjiti
(5) In the ﬁnal step, the tables obtained in steps (3) and (4) were joined aligning
an inﬁnitive of a verb in standard language, representing a set of synonyms in
the dialect, with a verb in standard language representing a set of synonyms
obtained from SWN ontology. The next example represents two joined sets
of synonyms aligned with the verb upropastiti.
upropastiti, unerediti, uniˇstiti, uprskati, zabrljati, zakrmaˇsiti, zasvinjiti |
isabim, batiˇsem, dokrajiˇsem, istrovim, izabim, izakam, oznobim, profu´ckam
On the left side of the vertical line are synonymous words in the standard
dictionary while on the right side are synonym words in the dialect. The use
of this table by the dialect dictionary advanced search tool enables a user to
type any of eight standard language words in order to obtain all equivalent
synonymous words in the dialect.
5
Evaluation
The evaluation of the proposed method can be observed as the evaluation of
a classiﬁcation task, so we have performed an estimation of the accuracy of
pairing digital dictionary entries with standard language entries, comparing the
results obtained by the system with the results given by humans. Two language
experts annotated the inverted table described in step 3 of Sect. 3. The table was
divided into two equal parts and each annotator marked one of them. They used
3 marks for each record in the table: 1 – if an inﬁnitive of the standard language
has the same or similar meaning as verbs of the dialect in the same record;

A WordNet Ontology in Improving Searches of Digital Dialect Dictionary
381
2 – if it is not clear whether a standard language inﬁnitive has the same or similar
meaning as verbs of the dialect in the same record; 3 – if a standard language
inﬁnitive does not have the same or similar meaning as verbs of the dialect in
the same record. At the same time, we created an automatic procedure to check
if there were headwords of a dialect dictionary which were not related to any
inﬁnitive after inverting the table. This procedure classiﬁes inﬁnitives on those
which take a part in relations (related) and those which do not (unrelated). When
we compared human marks 1 with related, we obtained true positives. Human
marks 2 and 3 compared to related gave false positives. Similarly, a comparison
with the unrelated set produced false and true negatives. The confusion matrix
is given in Table 1. Based on the confusion matrix, performance measures were
calculated: precision P = tp/(tp + fp) = 1.000, recall R = tp/(tp + fn)) =
0.874, F1 = 2PR/(P + R) = 0.933, accuracy = 0.897. We can observe that the
proposed method is completely precise, but the problem with the high value of
false negatives lies in shortcomings that were found in the dialect dictionary.
The most frequent are: writing errors, the lack of a verb in the deﬁnition, a verb
is not written in a standard format (ﬁrst case, singular, present tense), a verbal
noun can not be linked to any verb. Also, some standard verbs from deﬁnitions
were missing from e-dictionaries and some dialect verbs were misinterpreted by
e-dictionaries.
Table 1. The confusion matrix of the process deciding whether dictionary entries are
correctly aligned with standard language entries.
System yes System no
Expert yes tp = 3022
fn = 436
Expert no
fp = 0
tn = 784
6
Conclusion
In this paper, we propose a method for improving searches of digital dialect dic-
tionary by oﬀering the possibility to search with terms in the standard language.
The method uses SWRL rules deﬁned in the ontology based on the semantic net-
work Serbian WordNet to identify sets of synonymous words for each verb and
verbal noun deﬁned in the ontology. The method also uses e-dictionaries of the
standard language to extract word forms deﬁning verbs in the dialect dictionary
and to transform them into lemmas (used for search). The method generates a
table joining two sets of synonymous words – one originating from the dialect
dictionary, another from e-dictionaries of standard language – for each verb
extracted from a dialect dictionary. The evaluation of the method, treated as a
classiﬁcation, compared results obtained from the system to data provided by
humans. The accuracy measure was acc = 89.7%. In future work, we will experi-
ment with other parts of speech and we will try to expand the set of ontological
rules used in this system. Also we will use BabelNet and other resources in the
Linguistic Linked Open Data cloud (LLOD), apart from WordNet.

382
M. Mladenovi´c et al.
References
1. ˇCavar, D., Geyken, A., Neumann, G.: Digital dictionary of the 20th century Ger-
man language. In: Jezikoslovne Tehnologije za Slovenski Jezik: Proceedings of JS,
pp. 110–114 (2000)
2. Declerck, T., Wandl-Vogt, E.: How to semantically relate dialectal Dictionaries in
the Linked Data Framework. In: Proceedings of the 8th Workshop on Language
Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH 2014).
pp. 9–12. ACL, Gothenburg, April 2014
3. Fellbaum, C.: WordNet: An Eletronic Lexical Database. The MIT Press,
Cambridge (1998)
4. Karanikolas, N.N., Galiotou, E., Xydopoulos, G.J., Ralli, A., Athanasakos, K.,
Koronakis, G.: Structuring a multimedia tri-dialectal dictionary. In: Habernal, I.,
Matouˇsek, V. (eds.) TSD 2013. LNCS, vol. 8082, pp. 509–518. Springer, Heidelberg
(2013). doi:10.1007/978-3-642-40585-3 64
5. Krstev, C., Vitas, D., Stankovi´c, R.: A lexical approach to acronyms and their
deﬁnitions. In: Mariani, Z.V.J. (ed.) Proceedings of 7th Language and Technol-
ogy Conference, pp. 219–223. Fundacja Uniwersytetu im. A. Mickiewicza, Pozna´n,
November 2015
6. McCrae, J., Aguado-de Cea, G., Buitelaar, P., Cimiano, P., Declerck, T., G´omez-
P´erez, A., Gracia, J., Hollink, L., Montiel-Ponsoda, E., Spohr, D., Wunner, T.:
Interchanging lexical resources on the Semantic Web. Lang. Resour. Eval. 46(4),
701–719 (2012)
7. Mladenovi´c, M.: Digital dictionary of the South Serbian Dialect. Infotheca 15(1),
42–55 (2014)
8. Onysko, A., Markus, M., Heuberger, R.: Joseph Wright’s ‘English Dialect Dictio-
nary’ in electronic form: a critical discussion of selected lexicographic parameters
and query options. Lang. Comput. 69(1), 201–219 (2009)
9. O’Sullivan, D., Unwin, D.: Geographic Information Analysis. Wiley, Upper Saddle
River (2010)
10. Pereira, S., Gillier, R.: TEDIPOR: thesaurus of dialectal Portuguese. In: Proceed-
ings of the 15th EURALEX International Congress, Norway, pp. 267–281 (2012)
11. Petsas, S.: Visualising perceptual linguistic data. University of Edinburgh, Edin-
burgh (2009)
12. Sibler, P., Weibel, R., Glaser, E., Bart, G.: Cartographic visualization in support
of dialectology in support of dialectology. In: Proceedings of the AutoCarto 2012:
The International Symposium on Automated Cartography, Columbus, Ohio, USA
(2012)
13. Van Keymeulen, J., De Tier, V.: The woordenbank van de Nederlandse dialecten
(Wordbase of Dutch Dialects). In: Proceedings of the eLex 2013 Conference, Elec-
tronic lexicography in the 21st Century: Thinking Outside the Paper, Tallinn,
Estonia, pp. 261–279 (2013)
14. Vitas, D., Popovi´c, L., Krstev, C., Obradovi´c, I., Pavlovi´c-Laˇzeti´c, G., Stanojevi´c,
M.: The Serbian Language in the Digital Age. META-NET White Paper Series.
Springer, Heidelberg (2012). doi:10.1007/978-3-642-30755-3. Rehm, G., Uszkoreit,
H. (Series Editors)
15. Wandl-Vogt, E., Declerck, T.: Mapping a traditional dialectal dictionary with
linked open data. In: Proceedings of the 3rd eLex Conference, Electronic Lexi-
cography in the 21st Century: Thinking Outside the Paper, Tallinn, Estonia, pp.
460–471 (2013)

A WordNet Ontology in Improving Searches of Digital Dialect Dictionary
383
16. Zlatanovi´c, M.: Reˇcnik govora juga Srbije: (provincijalizmi, dijalektizmi, varvarizmi
i dr.). Uˇciteljski fakultet, Vranje (1998)
17. Zlatanovi´c, M.: Reˇcnik govora juga Srbije: (provincijalizmi, dijalektizmi, varvarizmi
i dr.). Aurora, Vranje (2011)

When It Comes to Querying Semantic Cultural
Heritage Data
B´eatrice Markhoﬀ1(B), Thanh Binh Nguyen2, and Cheikh Niang3
1 Universit´e Francois Rabelais de Tours, Laboratoire d’Informatique, Tours, France
beatrice.markhoff@univ-tours.fr
2 Universit´e d’Orl´eans, INSA CVL, LIFO, Orl´eans, France
thanh-binh.nguyen@univ-orleans.fr
3 Agence Universitaire de la Francophonie, Paris, France
cheikh.niang@auf.fr
Abstract. As more and more cultural institutions publish their data
using the web-of-data semantic level, there is a need for novel applications
for exploring, analyzing, mining and visualizing such data. A ﬁrst step
for these applications is to be able to query the linked open data. In this
paper we survey the diﬀerent existing systems for this purpose, providing
examples from our experiences in the Cultural Heritage domain, and
discussing some possible mutual enrichment between some of them.
Keywords: Semantic web · Querying · SPARQL endpoint · Federated-
query system · Ontology-Based Data Integration · Cultural Heritage
1
Introduction
When considering research papers in Digital Humanities and Cultural Heritage
(CH) conferences that deal with semantic web topics, most of them report either
on solutions for building ontologies and metadata for CH, or for data curation,
mapping for integration, and enrichment using semantic web standards and tech-
nologies. Indeed, since the end of 2000’s, many projects rely on semantic web
technologies to expose CH data in the Linked Open Data (LOD [2]): a survey
is given in [15], where it is shown that the commonly used process is to convert
museum catalogs into open RDF triplestores, through an extract-transform-load
process. Moreover, proposals described for instance in [7] rely on schema-level
alignments, using existing LOD resources (DBPedia, Geonames) and reference
ontologies, including the CIDOC-CRM. Originally designed for the semantic
integration of information from museums, libraries, and archives, this later has
become an extensible semantic framework that a wide range of CH resources
can be mapped to [8,19]. Based on such resources, nowadays many cultural
institutions provide big Knowledge Bases (KBs) on the web, that can be used
T.B. Nguyen—Supported by a PhD grant Orl´eans-Tours.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 384–394, 2017.
DOI: 10.1007/978-3-319-67162-8 38

When It Comes to Querying
385
by semantic web applications: the British Museum1, EUROPEANA2, the Smith-
sonian American Art Museum3, the BnF4 and so on.
There is a need now for semantic web applications, to help humans exploring
this huge knowledge network, for performing data analysis tasks, and even for
data mining, to extract new knowledge which may complete or correct the exist-
ing KBs. To this end, semantic web application designers ﬁrst have to consider
solutions for accessing semantic web data source(s). When analyzing research
papers on querying the semantic web, lots of them were ﬁrst only dealing with
querying one RDF/RDFS/OWL resource, until the W3C SPARQL speciﬁcations
were published (formal studies are still published recently, that deal with more
expressive solutions). From the end of the 2000’s, two main approaches for query-
ing semantic web data sources can be distinguished: the ﬁrst one addresses the
linked-data’s speciﬁcity, it is represented by the LOD Query systems, while the
second one, in general motivated by users’ needs, corresponds to the Ontology-
Based Data Integration (OBDI) systems. LOD Query systems range from a single
SPARQL Endpoint to Full-Web Query systems based on links traversal [12], and
include the Federated Query systems, surveyed in [20,23]. These latter propose
a single interface to perform a query on a ﬁxed set of SPARQL Endpoints. This
is also the case for OBDI systems, which fulﬁll the needs of a community to
access a set of sources [6,17].
Linked Open Data sources are published on the web according to the prin-
ciples introduced in [2], and now oﬃcially formalized by the W3C5. The LOD-
initiators’ vision of how applications shall use web data, called follow-your-nose,
may be summarized as follows: data providers provide data, while data con-
sumers discover, select and tailor data to their needs. We give a short state of
the art of LOD querying approaches in Sect. 2. It allows us to notice that there is
a need for an upper application level upon such systems, for taking into account
speciﬁc user needs, and in particular the need to safely rely on a tailored view of
the sources. Oﬀering a deﬁned view and a single access point to several sources is
the purpose of data integration systems [14], and this is why we recall the prin-
ciples of existing solutions based on semantic web data integration in Sect. 3.
Building such solutions is now a well deﬁned process when the sources’ owners
contribute to the integration task. But in the LOD open space, sources are not
supposed to contribute to any speciﬁc integrated system, a priori. Nevertheless,
we believe it is feasible to enhance semi-automatic building of mediator systems
for accessing several CH data providers on the LOD, by adapting techniques
used for querying the LOD, and by integrating them at the mediator level.
1 http://collection.britishmuseum.org/.
2 http://labs.europeana.eu/api/linked-open-data-sparql-endpoint.
3 http://americanart.si.edu/collections/search/lod/about/.
4 http://data.bnf.fr/sparql/.
5 https://www.w3.org/TR/dwbp/.

386
B. Markhoﬀet al.
2
LOD Querying Systems
LOD querying approaches can be classiﬁed according to the scope of the queried
data: one single source, a ﬁnite set of query-federated sources and the full web.
We will present their main features following this classiﬁcation.
2.1
Single SPARQL Endpoint
The cultural Knowledge Bases evoked in Introduction are central repository
infrastructures, in the same way as the huge, general purpose, and multilingual
KBs such as Yago or BabelNet, that are automatically constructed by harvesting
public knowledge-sharing platforms [26]. It means that they collect data from
diﬀerent sources and integrate it into a single repository before query processing.
Such central repository infrastructures are supported by Triple Store Manage-
ment Systems6, and provide in general an ontology as the schema to design
queries. Notice that, in the Cultural Heritage ﬁeld, in general an ontology also
plays the role of global schema for integrating the collected data (see Sect. 3,
and this is the very purpose of the CIDOC-CRM’s design. The data managed
by triple stores is queried in SPARQL via a SPARQL endpoint, a web service
that implements the SPARQL protocol deﬁning the communication processes as
well as the accepted and output formats (e.g. RDF/XML, JSON, CSV, etc.).
The major advantage of central repository infrastructures is the direct availabil-
ity of locally stored data, which enables optimized query evaluation techniques.
However, when harvested from independent sources, the queried data is not
always up to date, which may be a serious drawback in the dynamic context
of the web of data. Very few initiatives exist that tackle the challenge of allow-
ing humans (besides applications) to explore cultural heritage Knowledge Bases,
even for a single provider. To our knowledge, the best example of such solution is
the ResearchSpace project7, that uses Metaphactory, the Metaphacts end-to-end
plateform8. The ResearchSpace project is led by the British Museum [19].
2.2
Full-Web Querying
Full-Web query systems refer to approaches where the scope of queries is the
complete set of Linked Data on the Web [20]. Instead of extracting, transforming,
and loading all data from a ﬁxed set of sources before querying it, here all
relevant data for a query is discovered during runtime execution. The query
evaluation is initialized from a single triple pattern as starting point and, in
an iterative process, relevant data is downloaded by dereferencing URIs which
are used to identify Linked Data documents on the web. Parts of the query
are iteratively evaluated based on downloaded data, and additional URIs are
added, which are dereferenced in the next iteration step. Indeed, as an RDF
6 https://www.w3.org/wiki/LargeTripleStores.
7 http://www.researchspace.org/.
8 http://www.metaphacts.com/application-areas/cultural-heritage.

When It Comes to Querying
387
resource may be referred by multiple URIs from multiple independent sources
which may use diﬀerent ontologies to model their RDF knowledge bases, for
instance URIs can be co-referenced via the owl:sameAs property, data from
diﬀerent sources are connected together, and as a consequence applications can
potentially traverse the whole Web of Data by starting from one point. The
evaluating process terminates when there are no more URI with potential results
to follow. Fully relying on the Linked Data principles, the only requirement is
that the needed data should correctly comply with those principles. This method
potentially reaches all data on the web, and the freshest data. But, as the Web
of Data is an unbounded and dynamic space, the querying evaluation may also
not terminate, so practical experiments [13] actually restrict the range of queries
to a ﬁnite part of the Web of Data.
2.3
Federated-Query Systems
A federated-query system refers to a unique interface for querying data from
multiple independent given data sources9, based on a federation query engine
that decomposes the incoming user query into sub-queries, distributes them to
data sources, and constructs the ﬁnal result by combining answers from each
source. The query processing in a federation framework comprises four phases
performed in the following order: query parsing, data source selection, query
optimization and query execution. Query parsing transforms the initial query
into a set of triple patterns. Data source selection is the most studied phase,
as it highly determines the overall performances of Federated Query systems.
Even if no existing solution is directly based on the Full-Web query principles,
owl:sameAs links are taken into account when determining the relevant sources
containing relevant results for each triple pattern of the query, in order to avoid
sending all of them to all participating sources, which is essential to avoid net-
work overloading. Many techniques are proposed to deal with this challenge,
they are categorized as index-free, index-assisted and hybrid [1,22,24,25].
FedX [25] is an index-free federated engine. It sends SPARQL ASK queries
to data sources at runtime to discover potential sources. Thank to the simplicity
of ASK queries which return boolean values, relevant data sources who can
answer parts of the query triple patterns are quickly identiﬁed, but this can
become expensive when the number of triple patterns and the number of data
sources grow, so a cache mechanism is used to save the relevance of each triple
regarding each data sources. In contrast to the index-free fashion, index-assisted
approaches as DARQ [22] rely on statistics to create indexes for all predicates and
types used in the queries, concerning their presence in data sources. As using only
indexed summaries and possibly out-of-date indexes does not guarantee a result
set completeness, those systems must deal with indexes maintenance. Hybrid
9 Notice that this is diﬀerent from the W3C recommendation of a Federated Query
extension for SPARQL 1.1, for executing queries distributed over diﬀerent SPARQL
endpoints by specifying the distant endpoint using the SERVICE keyword, which
supposes that the query author has to manage all this low-level knowledge.

388
B. Markhoﬀet al.
systems as ANAPSID [1] combine indexed data and ASK queries. Proposed
enhancements generally consist in integrating, or dynamically querying, more
knowledge about data sources. For instance, some sources are automatically
discarded by making use of the URI authorities in HiBISCuS [24].
The query optimization phase aims to eliminate unnecessary data transfers
between the federated-query system and the sources by (i) using caching, (ii)
choosing the appropriate join method, (iii) ordering and grouping the triple
patterns. The resulting execution plan is processed in the last phase, the query
execution. In the Federated Query approach, queries are answered based on
the up-to-date data at original sources. In the web context, this is a major
advantage compared to centralized materialized approaches. This is also the
case for Ontology-Based Data Integration (OBDI) systems [6,21]. In the next
section, we recall their principles and report some experiences of building and
using OBDI systems in Cultural Heritage projects.
We do not know any existing web information system for Cultural Heritage
based on Full-Web or Federated-Query solutions. But it is very interesting to
notice that, the more Federated Query systems store information in cache or
index, about their sources on the one hand, and the user queries on the other
hand, the more they resemble to traditional integration systems. In particular,
indexes storing the relationships between query predicates and source predicates
play the same role as GAV or LAV mappings. Compared to the expressive power
of OBDI systems, Federated Query systems lack the ability to compile more
knowledge into the user query in order to get more complete and more correct
results with respect to the user’s needs. Nevertheless, the capabilities they devel-
oped for automatically harvesting knowledge, about sources and about queries,
may be reused to facilitate and enhance OBDI systems building.
3
Ontology-Based Data Integration Systems
The knowledge that OBDI systems add to the user query is stored in their
ontology, that is their global schema. Remember that a data integration system
J is a triple J = ⟨G, M, S⟩, where [14]:
– G is the expected global schema.
– S is the source schemas, i.e. schemas of the sources where data are stored.
– M is the mappings between G and S, i.e. a set of assertions establishing the
connection between the elements of the global schema and those of the source
schema.
We ﬁrst brieﬂy recall the fundamentals for deﬁning and using a global schema,
which takes here the form of a web ontology. Next, we analyze requirements for
sources to become parts of a web based semantic mediation system, through the
mappings, which represent a crucial part of a data integration system. Finally,
we recall the principles of query architectures based on the OBDI paradigm.

When It Comes to Querying
389
3.1
Web Ontology as Global Schema
Remember that two data integration architectures exist, the data warehouse
and the mediation systems. We already mentioned examples of the ﬁrst type of
architectures in the semantic web context: Yago and BabelNet. They rely on an
ontology as the global schema. The main advantage of this architecture is the
eﬃciency of query evaluation. Its main drawbacks are its costs in term of storage,
and in terms of refreshment process, as updates performed on the original data
sources must be propagated to the warehouse. On the contrary, in the mediation
approach, data are kept in sources and information is retrieved dynamically
from original databases, at query time. The integration is virtual in the sense
that data stay in sources, but the user who interacts with the mediator, via
the global schema, feels like interacting with a single database. The challenges
of this solution are related to its query-answering process: when a query qg is
posed in terms of the global schema G, the system must reformulate it in terms of
a suitable set of queries qs posed to the sources, send each computed sub-query
qsi to the involved source Si, and compose the received results into a ﬁnal global
answer for the user. Mediation solutions ﬁt the open, volatile and distributed
web context.
Both of the data warehouse and mediation approaches require the design
of a shared global schema: web ontologies play this role in the semantic web
context. OBDI systems combine semantic web and data integration principles
for overcoming semantic heterogeneity in order to share and eﬃciently reuse
data among autonomous interconnected stakeholders. The following three main
OBDI architectures are traditionally used [6]:
– The single-ontology approach, where source data is directly mapped to a global
ontology. For sources having diﬀerent views of the shared domain, ﬁnding a
consensus in a minimal ontology commitment is known to be a diﬃcult task.
– The multiple-ontologies approach, where source data is described with its own
local ontology, and local ontologies are organized as a peer-to-peer system.
This approach requires the construction of mappings between local ontologies
and the lack of a common vocabulary between them can make this task
diﬃcult.
– The hybrid approach, which combines the two previous ones, using local
ontologies that are mapped to a common top-level vocabulary, alleviating
thereby the deﬁnition of inter-ontology mappings.
The global ontology building is a diﬃcult task that usually requires human
experts, but there already exist many resources and techniques that can facilitate
it. The ResearchSpace [19] and the ARIADNE project for COINS presented
in [10] are good examples of single-ontology OBDI systems that chose the data
warehouse approach: data is mapped and transformed from the source schemata
to RDF triples, compliant with the CIDOC-CRM schema. In [18], a single-
ontology OBDI mediator is presented, where the global ontology was devised
by experts, based on the CIDOC-CRM with extensions and thesaurus of the
conservation-restoration domain. A solution for automating the global ontology

390
B. Markhoﬀet al.
building in an hybrid OBDI system was used in the query architecture presented
in [3], that was designed for a project of prosopography in the Renaissance. The
European project EP-Net also well demonstrated the usefulness of Ontology-
Based Data Access and OBDI for easing the access of scholars to historical and
cultural data, distributed across diﬀerent data sources, including public semantic
resources on the web [5].
3.2
Semantic Web Source
In single-ontology and in hybrid OBDI systems, in order to provide the uniﬁed
global query-interface, the mediator relies on mappings M, between the global
schema G and the local schemas S. Mappings are used for rewriting the global
query into a union of queries that match the local schemas. These mappings
can be directed either from entities in the global schema to entities in the local
sources - Global As View (GAV) mappings, or from entities in the local sources
to the ones in the global schema - Local As View (LAV) mappings [14]. LAV
mappings require more sophisticated inferences to resolve a query on the global
schema than GAV mappings, but they make it easier to add or retrieve data
sources to the mediation system. Some hybrid solutions exist, for instance in [3]
the overall architecture uses both LAV mappings between the sources and the
domain reference ontology and GAV mappings between the global ontology and
the sources.
To participate in a web-ontology-based mediation system, a source must
anchor the data that it wants to share into the global system. To this end,
when the source is not yet a semantic web resource, a required step is to build
an ontological representation of its data, at least for the part that should con-
tribute to the integration process. There are several ways to expose data at the
semantic level, either by using suitable tools such as Ontop [4], X3ML [16], or
Karma [11], or by implementing ad hoc wrappers. For instance in [18], a solution
based on Ontop is implemented for the ﬁrst source (a relational DB), while an
ad hoc wrapper is used to export in RDF the second source (a set of MS-Word
ﬁles). Once the source can be queried in SPARQL, mappings M can be deﬁned
and used to implement the distributed query system.
3.3
Mediator Querying System
We focus here on a concrete implemented example, but keeping the presentation
suﬃciently formal to be reusable. In [3] each submitted query is reformulated
based on the knowledge contained in the global ontology, and on the integration
knowledge (i.e. knowledge about sources, including the mappings). In a nutshell,
given the global ontology Og and a set of source repositories S, this rewriting
processes a global query qg, expressed over Og, by reformulating it into a union
of sources queries Qs, after having compiled the suitable knowledge in Og into
qg. More precisely, the query qg, intended to extract a set of elements from the
distributed semantic databases, is resolved following these steps:

When It Comes to Querying
391
1. Apply the Consistent algorithm [21] with qg as canonical instance, for veri-
fying its consistency w.r.t constraints expressed in Og.
2. Apply the PerfectRef algorithm [21] with q and Og as input, for integrating
the ontological constraints into the initial query.
3. Let Qs be the union of queries resulting from Step 2, apply an adapted
MiniCon processing in order to distribute Qs to the involved sources. Each
source evaluates the received queries over its semantic repository.
4. Let ans(Qs) be the set of answers received from sources, build the global
answer ans(Qg).
The ﬁrst step avoids evaluating queries that could only lead to an empty
result. The second step is a reasoning task performed over the global schema
Og. In [3], as positive assertions are used for expressing the GAV mappings, each
obtained sub-query qs in Qs is expressed using terms of a speciﬁc source, allowing
the MiniCon to distribute them and compute the global answer afterwards. It
is interesting to notice that, compared to Federated Query systems presented in
Sect. 2.3, the diﬃcult challenge they face for data source selection does not exist
with OBDI, because the necessary knowledge is stored in the mediator.
4
Conclusion
When it comes to querying semantic web Cultural Heritage data, this is useful
to have an overview of semantic web querying systems in order to build tailored
services for users. We surveyed the existing solutions for querying the semantic
web, those for LOD querying on the one hand, and those based on OBDI on
the other hand. It is important to notice that, in the Cultural Heritage ﬁeld,
existing semantic data management systems are OBDI systems, in general based
on the CIDOC-CRM or some extensions, and that most of them are centralized
materialized RDF data triple stores, that are queried via a SPARQL endpoint.
Nevertheless, in order to go closer to The Dream of a Global Knowledge Network
for Cultural Heritage [9], it is necessary to be able to also rely on the decentralized
Linked Data that is distributed over the WWW.
To this end, we recalled the alternative to centralized materialized data ware-
houses, that traditionally exists among data integration systems: the mediators,
which allow the user for querying several legacy data sources without extracting
and loading data. Before that, we sketched the principles of existing Full-Web
and Federated-Query systems, noticing that they have the general tendency
to propose some automatic generation of knowledge, about sources and about
queries, this knowledge being stored in cache, or index, or summary, or ranking
table as in [13]. According to the so-called follow-your-nose query principle [2],
these systems are not devised to oﬀer an integrated view of several resources,
on contrary they explicitly leave the data integrating eﬀort to their users, but
some of their methods for harvesting knowledge, and for optimizing query exe-
cution plans, may be re-used to improve the automation of web OBDI mediators
building, and also to improve their query performances. Big KBs that exist on
the semantic web, such as Yago or BabelNet, are harvested from public web

392
B. Markhoﬀet al.
knowledge-sharing platforms, which involves information extraction techniques
to produce RDF data, sometimes from natural language texts. Their creators
also devised methods and techniques [26] that could be re-used for projects ded-
icated to Cultural Heritage.
Now that many Cultural Heritage institutions have opened their data to the
semantic web level, we are convinced that building OBDI systems is the best
way to develop applications for connecting Cultural Heritage data, for many
diﬀerent user needs. This is already done in many projects [5,10,18,19]. More-
over, even though we know that centralized materialized data warehouses are
the most eﬃcient solution for operating complex computations on big data, we
also believe that semantic mediators are the best solutions in order to deal with
the decentralized and highly evolutive features of the web. Our survey highlights
some elements for making their construction simpler and their operation more
eﬃcient, even for querying existing public semantic web resources.
References
1. Acosta, M., Vidal, M.-E., Lampo, T., Castillo, J., Ruckhaus, E.: ANAPSID: an
adaptive query processing engine for SPARQL endpoints. In: Aroyo, L., Welty,
C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., Blomqvist, E. (eds.)
ISWC 2011. LNCS, vol. 7031, pp. 18–34. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-25073-6 2
2. Bizer, C., Heath, T., Berners-Lee, T.: Linked data - the story so far. Int. J. Semant.
Web Inf. Syst. 5(3), 1–22 (2009)
3. Bouchou, B., Niang, C.: Semantic mediator querying. In: International Database
Engineering and Applications Symposium (IDEAS), pp. 29–38. ACM (2014)
4. Calvanese, D., Cogrel, B., Komla-Ebri, S., Kontchakov, R., Lanti, D., Rezk, M.,
Rodriguez-Muro, M., Xiao, G.: Ontop: answering SPARQL queries over relational
databases. Semant. Web 8, 471–487 (2017)
5. Calvanese, D., Liuzzo, P., Mosca, A., Remesal, J., Rezk, M., Rull, G.: Ontology-
based data integration in EPNet: production and distribution of food during the
Roman Empire. Eng. Appl. Artif. Intell. 51, 212–229 (2016). Mining the Humani-
ties: Technologies and Applications
6. Cruz, I.F., Xiao, H.: Ontology driven data integration in heterogeneous networks.
In: Tolk, A., Jain, L.C. (eds.) Complex Systems in Knowledge-Based Environments:
Theory, Models and Applications. SCI, vol. 168, pp. 75–98. Springer, Heidelberg
(2009). doi:10.1007/978-3-540-88075-2 4
7. Damova, M., Dannells, D.: Reason-able view of linked data for cultural heritage.
In: Dicheva, D., Markov, Z., Stefanova, E. (eds.) Third International Conference
on Software, Services and Semantic Technologies S3T 2011, vol. 101, pp. 17–24.
Springer, Heidelberg (2011). doi:10.1007/978-3-642-23163-6 3
8. Doerr, M.: Ontologies for cultural heritage. In: Staab, S., Studer, R. (eds.) Hand-
book on Ontologies. International Handbooks on Information Systems, pp. 463–
486. Springer, Heidelberg (2009). doi:10.1007/978-3-540-92673-3 21
9. Doerr, M., Iorizzo, D.: The dream of a global knowledge network – a new approach.
J. Comput. Cult. Herit. 1(1), 5:1–5:23 (2008)

When It Comes to Querying
393
10. Felicetti, A., Gerth, P., Meghini, C., Theodoridou, M.: Integrating heterogeneous
coin datasets in the context of archaeological research. In: Proceedings of the Work-
shop on Extending, Mapping and Focusing the CRM Co-Located with 19th Inter-
national Conference on Theory and Practice of Digital Libraries, Pozna´n, Poland,
pp. 13–27, 17 September 2015
11. Gupta, S., Szekely, P., Knoblock, C.A., Goel, A., Taheriyan, M., Muslea, M.:
Karma: a system for mapping structured sources into the semantic web. In:
Simperl, E., Norton, B., Mladenic, D., Della Valle, E., Fundulaki, I., Passant, A.,
Troncy, R. (eds.) ESWC 2012. LNCS, vol. 7540, pp. 430–434. Springer, Heidelberg
(2015). doi:10.1007/978-3-662-46641-4 40
12. Hartig, O.: Querying a web of linked data: foundations and query execution. In:
SIGWEB Newsletter, vol. 2014, no. Autumn, pp. 3:1–3:2 (2014)
13. Hartig, O., ¨Ozsu, M.T.: Walking without a map: ranking-based traversal for query-
ing linked data. In: Groth, P., Simperl, E., Gray, A., Sabou, M., Kr¨otzsch, M.,
Lecue, F., Fl¨ock, F., Gil, Y. (eds.) ISWC 2016. LNCS, vol. 9981, pp. 305–324.
Springer, Cham (2016). doi:10.1007/978-3-319-46523-4 19
14. Lenzerini, M.: Data integration: a theoretical perspective. In: PODS, pp. 233–246
(2002)
15. Marden, J., Li-Madeo, C., Whysel, N., Edelstein, J.: Linked open data for cultural
heritage: evolution of an information technology. In: Proceedings of the 31st ACM
International Conference on Design of Communication, SIGDOC 2013, pp. 107–
112. ACM (2013)
16. Minadakis, N., Marketakis, Y., Kondylakis, H., Flouris, G., Theodoridou, M., de
Jong, G., Doerr, M.: X3ML framework: an eﬀective suite for supporting data map-
pings. In: Proceedings of the Workshop on Extending, Mapping and Focusing the
CRM Co-Located with 19th International Conference on Theory and Practice of
Digital Libraries, Pozna´n, Poland, pp. 1–12, 17 September 2015
17. Niang, C., Marinica, C., Leboucher, E., Bouiller, L., Capderou, C., Bouchou, B.:
Ontology-based data integration system for conservation-restoration data (OBDIS-
CR). In: 20th International Database Engineering & Applications Symposium,
IDEAS 2016, Montreal, Canada, pp. 218–223, 11–13 July 2016
18. Niang, X., Marinica, C., Markhoﬀ, B.B., Leboucher, E., Laissus, F., Malavergne,
O., Bouiller, L., Darrieumerlou, C., Capderou, C.: Supporting semantic interoper-
ability in conservation-restoration domain the parcours project. ACM Journal on
Computing and Cultural Heritage (JOCCH) - Special Issue on Digital Infrastruc-
ture for Cultural Heritage 10, 16 (2017)
19. Oldman, D., Doerr, M., de Jong, G., Norton, B., Wikman, T.: Realizing lessons of
the last 20 years: a manifesto for data provisioning & aggregation services for the
digital humanities (a position paper). D-Lib Mag. 20(7/8) (2014)
20. ¨Ozsu, M.T.: A survey of RDF data management systems. Front. Comput. Sci.
10(3), 418–432 (2016)
21. Poggi, A., Lembo, D., Calvanese, D., Giacomo, G., Lenzerini, M., Rosati, R.:
Linking data to ontologies. In: Spaccapietra, S. (ed.) Journal on Data Seman-
tics X. LNCS, vol. 4900, pp. 133–173. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-77688-8 5
22. Quilitz, B., Leser, U.: Querying distributed RDF data sources with SPARQL.
In: Bechhofer, S., Hauswirth, M., Hoﬀmann, J., Koubarakis, M. (eds.) ESWC
2008. LNCS, vol. 5021, pp. 524–538. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-68234-9 39

394
B. Markhoﬀet al.
23. Saleem, M., Khan, Y., Hasnain, A., Ermilov, I., Ngonga Ngomo, A.-C.: A ﬁne-
grained evaluation of SPARQL endpoint federation systems. Semant. Web 7, 493–
518 (2016)
24. Saleem, M., Ngonga Ngomo, A.-C.: HiBISCuS: hypergraph-based source selec-
tion for SPARQL endpoint federation. In: Presutti, V., d’Amato, C., Gandon,
F., d’Aquin, M., Staab, S., Tordai, A. (eds.) ESWC 2014. LNCS, vol. 8465, pp.
176–191. Springer, Cham (2014). doi:10.1007/978-3-319-07443-6 13
25. Schwarte, A., Haase, P., Hose, K., Schenkel, R., Schmidt, M.: FedX: optimization
techniques for federated query processing on linked data. In: Aroyo, L., Welty,
C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N., Blomqvist, E. (eds.)
ISWC 2011. LNCS, vol. 7031, pp. 601–616. Springer, Heidelberg (2011). doi:10.
1007/978-3-642-25073-6 38
26. Weikum, G., Hoﬀart, J., Suchanek, F.M.: Ten years of knowledge harvesting:
lessons and challenges. IEEE Data Eng. Bull. 39(3), 41–50 (2016)

ADBIS Doctoral Consortium

Preference-Based Stream Analysis for Eﬃcient
Decision-Support Systems
Lena Rudenko(B)
University of Augsburg, 86135 Augsburg, Germany
lena.rudenko@informatik.uni-augsburg.de
Abstract. Stream query processing is an important development trend
as more time-oriented data is produced nowadays. It is not easy to ﬁnd
relevant and interesting content in large amount of data. Furthermore
users want to have personalized results of stream data processing which
correspond to their preferences. In this paper I present ﬁrst research
results achieved during my work on my doctoral thesis. I also discuss
open issues and challenges on the way to my goal - the development of
a preference-based stream analyzer for eﬃcient decision-support.
1
Introduction
Today, data processed by humans as well as computers is very large, rapidly
increasing and often in form of data streams. Users want to analyze this data to
extract personalized and customized information in order to learn from this ever-
growing amount of data, e.g., [1–3]. This data comes as a ﬂow or stream. Many
modern applications such as network monitoring, ﬁnancial analysis, infrastruc-
ture manufacturing, sensor networks, meteorological observations, or social net-
works require query processing over data streams, e.g., [4–6]. Therefore stream
data processing is a highly relevant topic today.
A stream is a continuous unbounded ﬂow of data objects made available over
time. These objects are very diﬀerent and can be both simple numbers and com-
plex data, depending on the kind of stream provider. Due to the continuous and
potentially unlimited character of stream data, it needs to be processed sequen-
tially and incrementally. However, queries on streams run continuously over a
period of time and return diﬀerent results as new data arrive. That means, look-
ing on stream data twice is not possible. Hence, analyzing streams can be con-
sidered as a diﬃcult and complex task which is in the focus of current research.
For my doctoral thesis I want to combine the stream and database analysis to
create eﬃcient decision support systems. Let us imagine the following situation:
Example 1. A user wants to visit a football match in an unfamiliar city. He wants
to park at the arena, but on the way he reads in Twitter that the parking is full.
The user is not familiar to this city and needs help to ﬁnd alternative parking.
Supervisor: Markus Endres, University of Augsburg, 86135 Augsburg, Germany.
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 397–409, 2017.
DOI: 10.1007/978-3-319-67162-8 39

398
L. Rudenko
In this situation the user has to manage diﬀerent kinds of information which
should help him to make his decision (park his car) - the streamed data from
Twitter (message about full parking) and data stored in the database (informa-
tion about parking near the arena). The user gets the list from decision sup-
port system containing free parkings near the stadium and associated travelling
routes. He chooses one oﬀered variant and parks his car there.
On the way to create such systems many issues and tasks have to be solved -
evaluation of stream data with user deﬁned preferences, developing of eﬃcient
algorithms to compute preference-based queries on stream data, combination of
database and stream data analysis, etc. In this paper I present the results I’ve
obtained so far:
(P1) An approach to analyze data streams with the support of user preferences.
Queries in the context of preferences are soft constraints that should be
fulﬁlled as closely as possible. If exact matches are not available, optimal
alternatives are better than nothing. This feature of soft constraints is
very important for our stream analysis approach. Despite the really huge
amount of stream records, it often happens that the exact matches cannot
be found. The hard condition approach provides in this case an empty
result and therefore can not satisfy the user.
(P2) The Preference Continuous Query Language (PCQL) for connecting and
querying streams in Preference SQL [3], a SQL extension which supports
preference queries. I outline also my open-source stream processing appli-
cation which allows users to construct queries in a quick and easy manner
and evaluate them on various selectable streams of data.
I also present open challenges which I will address in the future:
(F1) An algorithm for eﬃcient preference stream processing, which exploits the
lattice structure constructed by a preference query. Existing stream eval-
uation approaches have a quadratic worst-case runtime because of tuple-
to-tuple comparisons to ﬁnd the best objects. The proposed algorithm is
based on a lattice structure representing the Better-Than relationships
that must be built only once for eﬀective Pareto preference computation
on data streams.
(F2) An idea to combine database and stream analysis to get the best and most
relevant results. The data stored in database can be changed while the user
waits for the query answer. Thus, possibly he has not the best results. But
the database changes can be handled as event stream and the user can get
notiﬁcation every time, if the stored data has been changed.
The remainder of this paper is organized as follows: Sect. 2 highlights related
work. Section 3 recapitulates essential concepts of the used preference model
and Preference SQL query language. In Sect. 4 I describe the developed results
for preference based stream analysis and in Sect. 5 future challenges and work.

Preference-Based Stream Analysis for Eﬃcient Decision-Support Systems
399
2
Related Work
Many scientists all over the world try to process and to analyze streams to extract
important information from such continuous data ﬂows. Babu and Widom [7],
e.g., focus primarily on the problem, how to deﬁne and evaluate continuous
queries over data streams. Faria et al. [8] describe various applications of nov-
elty detection in data streams, and Krempl et al. [9] discuss challenges for data
stream mining such as: protecting data privacy, handling incomplete and delayed
information, or analysis of complex data. In [10] the authors examine the char-
acteristics of important preference queries (Skyline, top-k and top-k dominat-
ing) and review algorithms proposed for the evaluation of continuous preference
queries under the sliding window streaming model. However, they do not present
any framework for preference-based stream evaluation. An important research
direction associated with stream processing is data stream clustering. This issue
is discussed, e.g., in [11–14]. In [15] the authors investigate queries over con-
tinuous unbounded streams for applications like network monitoring, ﬁnancial
analysis, and sensor networks. For this they present the Stanford Data Stream
Management System (STREAM).
All previous work rely on analyzing the content of a stream or describe stream
processing systems. In this paper we ﬁlter data by a preference-based approach,
which never leads to an empty result and only extracts the most important
information w.r.t. the user’s preference.
3
Preference Background
Preference modeling have been in focus for some time, leading to diverse
approaches, e.g., [16]. A preference P = (A, <P ) is a strict partial order (SPO)
on the domain of A. Thus <P is irreﬂexive and transitive. Some values are con-
sidered to be better than some others. Two values not ordered by the strict
partial order <P are regarded as indiﬀerent, i.e. ¬(x <P y) ∧¬(y <P x). As
strict partial orders are transitive, better-than relations in this preference model
are, too.
The maximal objects of a preference P = (A, <P ) on an input database
relation R are all tuples that are not dominated by any other tuple w.r.t. the
preference. These objects are computed by the preference selection operator
σ[P](R). It ﬁnds all best matching tuples t w.r.t. the preference P, where t.A is
the projection to the attribute set A.
σ[P](R) := {t ∈R | ¬∃t′ ∈R : t.A <P t′.A}
(1)
The evaluation follows a Best-Matches-Only (BMO) query model that retrieves
exact matches if such objects exist and best alternatives else.
3.1
Preference Constructors
To express simple preferences targeting only one attribute diverse base preference
constructors are deﬁned. There are base preference constructors for numerical,

400
L. Rudenko
categorical, temporal, and spatial domains. Subsequently, I present some selected
constructors which are often applied by users.
For example, the categorical (positive) preference POS(A, POS-set) expresses
that a user has a set of preferred values, the POS-set. The negative preference
NEG(A, NEG-set) is the counterpart to the POS preference. It is possible to
combine these preferences to POS/POS or POS/NEG. The AROUND(A, value)
preference expresses a preferred numerical value. From two values, the one with
less deviation from the speciﬁed value is preferred. Note that the dominance
criterion of the base preferences is based on a SCORE function f : dom(A) →
R+
0 , where f(v) describes how far the domain value v is away from the optimal
value. Dominated tuples have higher function values, i.e., x <P y ⇐⇒f(x) >
f(y). For more details we refer to [3].
Complex preferences determine the relative importance of preferences and
combine base or again complex preference constructors. In the Pareto preference
P := P1 ⊗P2 = (A1 × A2, <P ) all preferences are of equal importance, it is
deﬁned as:
(x1, x2) <p (y1, y2) ⇔(x1 <P1 y1) ∧(x2 <P2 y2 ∨x2 = y2))
∨(x2 <P2 y2) ∧(x1 <P1 y1 ∨x1 = y1))
While in a Prioritization preference P := P1 & P2 the preference P1 =
(A1, <P1) is more important than P2 = (A2, <P2). For a more formal deﬁnition
and more detailed information we refer to [3,16].
Example 2. Let us consider the presidential election in France in spring 2017.
A lot of authors posted messages on Twitter to this theme. The preference for
the tweets in German or in English can be expressed by a POS constructor:
P1 := POS(tweet language,{de, en}). Equally important for me is the number
of followers of the author - it must be around 20000. This wish can be expressed
by P2 := AROUND(followers, 20000). Both base preferences can be combined to
Pareto preference P := P1 ⊗P2 to ﬁnd the most relevant tweets.
3.2
Preference SQL
The Preference SQL query language is a declarative extension of SQL by strict
partial order preferences, behaving like soft constraints under the BMO query
model. Syntactically, Preference SQL extends the SELECT statement of SQL by
an optional PREFERRING clause, cp. Fig. 4. The keywords SELECT, FROM and WHERE
are treated as the standard SQL keywords. The PREFERRING clause speciﬁes a
preference which is evaluated after the WHERE condition (Fig. 1).
Example 3. The Pareto preference in Example 2 can be expressed in Preference
SQL as follows, where Election is the database relation which contains informa-
tion about the presidential election in France.
SELECT * FROM
Election
PREFERRING
tweet_language IN (’de’, ’en’)
PARETO
followers
AROUND
20000;

Preference-Based Stream Analysis for Eﬃcient Decision-Support Systems
401
SELECT
. . . <projection, aggregation>
FROM
. . . <table reference>
WHERE
. . . <hard conditions>
PREFERRING . . . <soft conditions>
Fig. 1. Simpliﬁed preference SQL query block.
In the above example IN expresses a POS preference. The keyword PARETO states
a Pareto preference, and PRIOR TO would lead to a Prioritization (not shown).
4
Preference-Based Stream Analysis
In this section I describe ﬁrst results of preference-based stream processing.
I want to analyze the data streams employing the user’s deﬁned preferences.
My approach is based on University prototype of the Preference SQL system
[3]. This system was developed to run queries against bounded data sets that
are stored persistently in a relational database, but I would like to analyze the
unbounded stream data. Hence, I need a framework that transforms the data
from a stream into a Preference SQL processable format. In this section I ﬁrst
describe my processing framework, then I give an overview of the Preference
Continuous Query Language (PCQL) I have developed, and ﬁnally I present my
application to query the streams using preferences.
4.1
Stream Processing Framework
To transform the data into a format which is compatible with Preference SQL,
I use Apache Flink 1. For the implemented framework and application prototype
I decided to use Twitter as an example data source, because it has an open API
and a lot of free accessible data (tweets). In addition, Apache Flink provides a
Twitter Streaming API connector for direct connection to this social network.
When processing data streams with Preference SQL, two fundamental tasks
must be carried out:
1. The data must have a Preference SQL processable format, because Preference
SQL works on attribute based data, but streams are often encoded in various
formats, e.g., as JSON2-objects as in Twitter.
2. The result computation must be adapted to stream properties, since the
dataﬂow is continuous. There is no “ﬁnal” result after some data of the stream
is processed. Hence, the result must be calculated and adjusted as soon as
new data arrive.
The lower part of Fig. 2 provides an overview of the system architecture. The
incoming data stream is processed by Apache Flink, where the StreamProcessor
1 https://ﬂink.apache.org/.
2 http://www.json.org/.

402
L. Rudenko
transforms the data into a Preference SQL readable format. The DataAccumulator
builds chunks of objects, which can be processed by Preference SQL. It is com-
posed of the client application to submit user’s queries, parser and optimizer to
construct query execution plan. Preference SQL evaluates the user preference on
these data chunks and presents the result to the user.
Fig. 2. Application for stream analysis with Preference SQL and ETL process in
Apache Flink.
4.2
Object Representation in a Preference SQL Processible Format
The objects delivered by a data stream are encoded and not compatible with
Preference SQL. That means the data must be structured and needs an attribute
based format like attributeName = attributeValue. The transformation of the
stream objects to a list of single attribute values occurs in the StreamProcessor,
cp. Fig. 2. For this one has to implement the mapping of the object in the stream
to a table structured format. The data types of the attributes can be extracted
from the stream objects.
After the object encoding and preparation the endless stream must be split
into ﬁnite parts to be processed in Preference SQL. This is done by grouping
objects into chunks. This concept is similar to moving window, described in [17].
The grouping occurs in the DataAccumulator, see Fig. 2. It takes a stream of
processed objects as input and provides a stream of chunks as output. Each
chunk itself is ﬁnite and can be processed with the Preference SQL system.
In the last step the data chunks are analyzed within Preference SQL. The
input is a stream of chunks, the output contains the most preferred objects w.r.t.
the preference speciﬁed by a user, i.e., the BMO-set.

Preference-Based Stream Analysis for Eﬃcient Decision-Support Systems
403
4.3
Finding the BMO-set of a Stream
To ﬁnd the best results in each chunk the Block-Nested-Loop Algorithm (BNL)
[18] is used. This algorithm is based on tuple-to-tuple comparison approach and
has a worst-case complexity of O(n2), where n is the number of input objects.
Example 4. A user wants to ﬁnd the tweets in German or alternatively in English
with hashtags #Macron or #LePen. About the other candidates he needs no
information. Language and hashtags are equally important for him.
The evaluation of this preference with BNL on one chunk consisting of four
tweets is schematically represented in Fig. 3.
Fig. 3. Example of data evaluation with BNL.
In the ﬁrst step tweet1 is added to the current BMO-set. In the second
step tweet2 is compared to tweet1. They are indiﬀerent and both are in the
new BMO-set. The next candidate tweet3 must be compared with all objects
from the current BMO-set. It is better than tweet1 and better than tweet2.
The worse candidates are removed from BMO-set, the better one is added to it.
Finally, tweet4 is compared to tweet3. It is worse than tweet3 and will not
be added to the BMO-set. The ﬁnal result consists of tweet3.
But the BMO-set of one chunk is not the ﬁnal result, because when new data
arrives it may occur that the next chunk contains better objects w.r.t. the user
preference. Hence, the current temporary BMO-set must be compared to the
objects from the next chunks in the ETL process, and so on.
More detailed description of this concept: Let c1, c2, ... be the chunks provided
by the ETL processor. The Preference SQL system evaluates the user preference
P on the ﬁrst chunk, i.e., σ[P](c1). Since c2 could contain better objects we
also have to compare the new objects from c2 with the current BMO-set, i.e.,
it will be computed σ[P](σ[P](c1) ∪c2), and so on. However, this leads to a
computational overhead if c2 is large. Therefore I apply a pre-ﬁlter to c2, i.e.,
I ﬁrst compute σ[P](c2) and afterwards apply the preference selection to the
union of σ[P](c1) and σ[P](c2), since the following holds [19]:
σ[P](c1 ∪c2) = σ[P](σ[P](c1) ∪σ[P](c2))
(2)

404
L. Rudenko
This leads to a correct result and is more eﬃcient than the ﬁrst approach.
However, it is still very ineﬃcient. A large number of comparison leads to a
quadratic worst-case to run time. To make the response time better, I want to
develop and to implement a lattice based stream algorithm to evaluate Pareto
queries in linear run time. I refer to the idea described in [20–22] and want to
adapt this algorithm to stream data. For more details I refer to Sect. 5.1.
4.4
Preference Continuous Query Language
Now I want to present a preference stream query language for stream data.
The Preference SQL syntax was described in Sect. 3. Preference based stream
queries have some diﬀerences. Syntax of Stream Preference SQL is illustrated
in Fig. 4a, in Fig. 4b one can ﬁnd the following example presented in this query
language: the user wants to read tweets about the presidential election in France.
The text of these tweets must not be French.
SELECT
STREAM <projection >
FROM
<stream_reference >
WHERE
<hard_conditions >
PREFERRING <soft_conditions >
(a) Stream Preference SQL syntax.
SELECT
STREAM *
FROM
TwitterStream
WHERE
tweet_language
<> ’fr’
PREFERRING
hashtag IN
(’# franceelection ’);
(b) Stream Preference SQL example.
Fig. 4. Stream preference SQL query block.
The WHERE- and PREFERRING-blocks have no changes compared with Preference
SQL (see Sect. 3). In the FROM-clause we refer to the stream using the keyword
STREAM, instead to a database relation. The stream name is deﬁned in the stream
connector implementation. For more details see [23].
During my work a demo application was developed in the context of prefer-
ence based stream processing [24]. Figure 2 shows its main view. My application
illustrates the query building process over streams and allows users to construct
queries and to evaluate them easily.
5
Challanges and Future Work
I want to concentrate on two general directions in the near future:
(1) Development, implementation and test of a cost eﬃcient lattice-based algo-
rithm for stream evaluation w.r.t users preferences.
(2) Combined analysis of stream data and data stored in databases to get the
most relevant result.

Preference-Based Stream Analysis for Eﬃcient Decision-Support Systems
405
5.1
Eﬃcient Preference Stream Processing
Processing of preference based queries requires high eﬃcient evaluation algo-
rithms, especially on time-oriented data streams. In my approach I use the BNL
Algorithm (see Sect. 4.3) which was brought in a database context with the Sky-
line operator in [18]. This algorithm is not very eﬃcient, because it is based
on tuple-to-tuple comparison. The new stream objects received later can match
the user preferences better than the objects already recognized in previously
computed BMO-sets. New objects have to be compared with all from the cur-
rent BMO-set. The continuous compare process is the most expensive operation
of preference-based stream evaluation. Therefore I want to design and imple-
ment an algorithm which avoids tuple-to-tuple comparison and the associated
quadratic worst-case runtime of BNL. It will be also interesting to study memory
requirements of this algorithm.
Standard Lattice Skyline. I refer to the algorithms Hexagon [21] and Lattice
Skyline [20] as well as [22] which exploit the lattice induced by a Pareto query
over discrete domains to compute the best objects. Visualization of such lattices
is often done using Better-Than-Graphs (BTG) (similar to Hasse diagrams),
graphs in which edges state dominance. Each node in the BTG contains objects
which are mapped to the same feature vector by a scoring function f. All values
in the same node are indiﬀerent. The algorithm in general consists of three
phases:
(1) Construction phase for initialization of the BTG.
(2) Adding phase: Tuples are read and mapped to the nodes.
(3) Removal phase: Remove dominated and empty nodes.
The elements of the dataset D that compose the BMO-set is build up by those
nodes in the BTG that have no path leading to them from another non-empty
node. All other nodes have direct or transitive edges from the BMO-set nodes,
and therefore are dominated. The worst case complexity of such lattice algo-
rithms is linear w.r.t. the number of input tuples and the size of the BTG [20].
For the implementation of such algorithms the lattice is usually represented
by an array, where each position stands for one node in the lattice. The array
stores the empty, non-empty, and dominated node states. The nodes are visited
in a breadth-ﬁrst traversal order (BFT). Non-empty nodes cause a depth-ﬁrst
traversal (DFT), where the dominance ﬂags are set. Finally those nodes repre-
sent the BMO-set which are both non-empty and non-dominated.
In Fig. 5 one can see the BTG constructed for the query from Example 4 and
assignment of some tweets to the graph’s nodes. The best objects are on the top
of BTG, the worst ones on the bottom.
Stream-based Lattice Skyline (SLS). Stream Lattice Skyline has some
changes in comparison to the standard version:

406
L. Rudenko
Fig. 5. BTG for the query from Example 4.
(1) Construction phase does not change.
(2) Adding phase: Since the stream data is inﬁnite, the adding phase, in which
the objects are mapped to their nodes, runs after the start continuously,
thereby in parallel to the BFT and DFT.
(3) Removal phase: The dominated and empty nodes are not removed from
the lattice, only the objects associated with these nodes are deleted. It is
unknown which object will be received from stream later, therefore the whole
array is required. Moreover the BMO-set found by SLS is the result set only
for the current data chunks. The next chunk can contain better objects
and the BMO-set will change. The objects corresponding to the non-empty,
non-dominated nodes will not be removed from the BTG. Therefore the
evaluation of objects, received in later chunks, requires “only” the BTG-
throughput, not the expensive candidates comparison with all the objects
in BMO-set.
Unfortunately, the SLS algorithm presented above is restricted to Pareto
preferences. To evaluate basic preferences using this approach, our algorithm
must be adjusted as described in [25].
Challenges of SLS. My approach has some open challenges. For BTG building
(construction phase) the domain maximum of each base preference composing
the Pareto preference is needed. Streams have no meta-data about their objects,
so the domain knowledge is missing completely or for some dimensions. To handle
the unrestricted domains the SLS algorithm has to be adapted. The BTG will
be constructed for the attributes with known domains and for the unknown one
its locally optimum value (lov) will be stored in the non-empty nodes during
the adding phase. During the removal phase the objects belonging to dominated
nodes will not be discarded immediately, but they have to be compared with
regard to the lovs of the attribute with unrestricted domain. For the details I
refer to [20,22]. I want to expand the idea with locally optimum value for more
than one attribute for SLS. Last but not least future work includes numerous
experiments on the performance of SLS.

Preference-Based Stream Analysis for Eﬃcient Decision-Support Systems
407
5.2
Combination of Stream and Database Analysis
Despite increasing importance of stream data analysis, queries over data saved
in databases still play an important role, such as looking for some product on
Amazon, searching for suitable ﬂight to holiday place or choice of nearest park-
ing. Changes in the data occur often, also during the query process. The user is
not notiﬁed about these changes and the obtained query result is not consistent
with the data from database.
Example 5. Let us consider the following situation: The user wants to buy a car
and he is looking for a model on the web-page of some car dealer. Actually, he
wants a red model, but the black one is cheaper. He takes the black car and
thinks, he has a good deal. What the user doesn’t know, is that the dealer has a
special oﬀer for the desired red car now. The oﬀer is added to the database during
the user’s oﬀering process but after the query processing and result searching.
The provided result set doesn’t include the new oﬀer. The user has also no
notiﬁcation about the changes and he buys a black car instead of the desired red
one.
Notiﬁcations provided in form of stream data might help us to avoid the
described situation. Each data change will be stored in the database but also a
related event will be added to the notiﬁcation stream. In the simplest case the
stream event has time stamp and reference to changed relation. The user will be
informed, if some changes occur in the queried relation, regardless of whether the
data was added, removed or changed. It is better to save in the stream event not
only the aﬀected relation, but also kind of changes (adding, removing, updating
etc.) and the related attribute. In this case the user will be informed only about
changes relevant for his query.
Let us consider again Example 1. The analysis of stream data helps the user
to get the information about full arena parking. City’s existing parkings with
position and number of places are stored in the database. Query answer is addi-
tionally inﬂuenced by the free-places-information received as data stream. The
notiﬁcations about changes at the parkings can help the user to make a decision
and to choose one parking from list oﬀered by a decision support system.
Such combination of stream and database data analysis can be very produc-
tive for eﬀective decision making. This is, what I want to accomplish at the end
of my doctoral thesis.
6
Conclusion
In this paper I present the contributions I’ve made so far: An approach, frame-
work and application to analyze and query data streams with the support of user
preferences as well as a Preference Continuous Query Language (PCQL) for con-
necting and querying streams in Preference SQL. I describe open challenges I
am going to work on in the next time to accomplish my goal - development of a
preference-based stream analyzer for eﬃcient decision-support systems.

408
L. Rudenko
References
1. Stefanidis, K., Koutrika, G., Pitoura, E.: A survey on representation, composition
and application of preferences in database systems. ACM Trans. Database Syst.
36(3), 19:1–19:45 (2011)
2. Golfarelli, M., Rizzi, S.: Expressing OLAP preferences. In: Winslett, M. (ed.)
SSDBM 2009. LNCS, vol. 5566, pp. 83–91. Springer, Heidelberg (2009). doi:10.
1007/978-3-642-02279-1 7
3. Kießling, W., Endres, M., Wenzel, F.: The preference SQL system - an overview.
Bull. Tech. Comm. Data Eng. IEEE CS 34(2), 11–18 (2011)
4. Chen, J., DeWitt, D.J., Tian, F., Wang, Y.: NiagaraCQ: a scalable continuous
query system for internet databases. In: Proceedings of SIGMOD 2000, pp. 379–
390. ACM, New York (2000)
5. Bonnet, P., Gehrke, J., Seshadri, P.: Towards sensor database systems. In: Tan,
K.-L., Franklin, M.J., Lui, J.C.-S. (eds.) MDM 2001. LNCS, vol. 1987, pp. 3–14.
Springer, Heidelberg (2001). doi:10.1007/3-540-44498-X 1
6. Sankaranarayanan, J., Samet, H., Teitler, B.E., Lieberman, M.D., Sperling, J.:
Twitterstand: news in Tweets. In: Proceedings of ACM 2009, pp. 42–51 (2009)
7. Babu, S., Widom, J.: Continuous queries over data streams. SIGMOD Rec. 30(3),
109–120 (2001)
8. Faria, E.R., Gon¸calves, I.J.C.R., de Carvalho, A.C.P.L.F., Gama, J.: Novelty detec-
tion in data streams. Artif. Intell. Rev. 45(2), 235–269 (2016)
9. Krempl, G., ˇZliobaite, I., Brzezi´nski, D., H¨ullermeier, E., Last, M., Lemaire, V.,
Noack, T., Shaker, A., Sievi, S., Spiliopoulou, M., Stefanowski, J.: Open challenges
for data stream mining research. In: SIGKDD 2014 Explorations Newsletter, vol.
16, no. 1 (2014)
10. Kontaki, M., Papadopoulos, A.N., Manolopoulos, Y.: Continuous processing of
preference queries in data streams. In: Leeuwen, J., Muscholl, A., Peleg, D.,
Pokorn´y, J., Rumpe, B. (eds.) SOFSEM 2010. LNCS, vol. 5901, pp. 47–60.
Springer, Heidelberg (2010). doi:10.1007/978-3-642-11266-9 4
11. Silva, J.A., Faria, E.R., Barros, R.C., Hruschka, E.R., de Carvalho, A., Gama, J.,
Clustering, D.S.: Data stream clustering: a survey. ACM Comput. Surv. 46(1), 13
(2013)
12. Baruah, R.D., Angelov, P., Baruah, D.: Dynamically evolving clustering for data
streams. In: IEEE Conference on EAIS 2014, pp. 1–6 (2014)
13. Gu, X., Angelov, P.P.: Autonomous data-driven clustering for live data stream. In:
SMC 2016, pp. 1128–1135. IEEE (2016)
14. Kastner, J., Endres, M., Kießling, W.: A pareto-dominant clustering approach for
pareto-frontiers. In: Proceedings of the Workshops of the EDBT/ICDT 2017 Joint
Conference, Venice, Italy (2017)
15. Arasu, A., Babcock, B., Babu, S., Datar, M., Ito, K., Nishizawa, I., Rosenstein,
J., Widom, J.: STREAM: the stanford stream data manager. In: Proceedings of
SIGMOD 2003, pp. 665–665. ACM, New York (2003)
16. Kießling, W.: Foundations of preferences in database systems. In: Proceedings of
VLDB 2002, Hong Kong SAR, China, pp. 311–322. VLDB Endowment (2002)
17. Babcock, B., Datar, M., Motwani, R.: Sampling from a moving window over
streaming data. In: Proceedings of SODA 2002, Philadelphia, USA, pp. 633–634
(2002)
18. B¨orzs¨onyi, S., Kossmann, D., Stocker, K.: The skyline operator. In: Proceedings of
ICDE 2001, pp. 421–430. IEEE Computer Society, Washington (2001)

Preference-Based Stream Analysis for Eﬃcient Decision-Support Systems
409
19. Hafenrichter, B., Kießling, W.: Optimization of relational preference queries. In:
Proceedings of ADC 2005, Darlinghurst, Australia, pp. 175–184 (2005)
20. Morse, M., Patel, J.M., Jagadish, H.V.: Eﬃcient skyline computation over low-
cardinality domains. In: Proceedings of VLDB 2007, pp. 267–278 (2007)
21. Preisinger, T., Kießling, W.: The hexagon algorithm for pareto preference queries.
In: Proceedings of the 3rd Multidisciplinary Workshop on Advances in Preference
Handling in Conjunction with VLDB 2007, Vienna, Austria (2007)
22. Endres, M., Kießling, W.: High parallel skyline computation over low-cardinality
domains. In: Manolopoulos, Y., Trajcevski, G., Kon-Popovska, M. (eds.) ADBIS
2014. LNCS, vol. 8716, pp. 97–111. Springer, Cham (2014). doi:10.1007/
978-3-319-10933-6 8
23. Rudenko, L., Endres, M., Roocks, P., Kießling, W.: A preference-based stream
analyzer. In: Workshop STREAMEVOLV 2016, Riva del Garda, Italy (2016)
24. Rudenko, L., Endres, M.: Personalized stream analysis with PreferenceSQL. In:
BTW 2017, Stuttgart, Germanym, pp. 181–184 (2017)
25. Endres, M., Preisinger, T.: Beyond skylines: explicit preferences. In: Candan, S.,
Chen, L., Pedersen, T.B., Chang, L., Hua, W. (eds.) DASFAA 2017. LNCS, vol.
10177, pp. 327–342. Springer, Cham (2017). doi:10.1007/978-3-319-55753-3 21

Formalization of Database Reverse Engineering
Nonyelum Ndefo(B)
Free University of Bozen-Bolzano, Piazza Domenicani 3, 39100 Bolzano, Italy
ndefo@inf.unibz.it
Abstract. During the life cycle of an Information System, the origi-
nal design of the database may be diﬃcult to acquire. The database
schema may have been continuously modiﬁed and drifted away seman-
tically from the intended design, or perhaps no conceptual modelling
method was employed at all. A conceptual schema oﬀers a much richer
description of a domain than the database schema, this makes it impor-
tant for, among other reasons, the maintenance of semantic consistency
of the database at runtime. Database reverse engineering involves the
retrieval of domain semantics from an existing set of database schemas
into a conceptual schema. Though several research works exist on creat-
ing database reverse engineering methodologies, the process in itself has
never been properly and fully formalised with an emphasis on its correct-
ness. This paper introduces the ongoing research surrounding database
reverse engineering and the goal of rendering a formal reverse engineer-
ing framework. The expected formalism will be valuable to database and
domain experts as a sound foundation for implementing better reverse
engineering tools.
Keywords: Reverse engineering · Relational database · Conceptual
modelling · Mapping · Schema transformation
1
Introduction
The concept behind reverse engineering has been used for diﬀerent causes across
computer science disciplines. In the development and maintenance of software
systems, reverse engineering tries to reconstruct design information from an
implemented system [17]. In data management practices, reverse engineering has
been studied in close connection with databases, more speciﬁcally with relational
databases.
Database Reverse Engineering (DBRE) is a means to recover domain seman-
tics encoded in a speciﬁed database and represent them as a high-level concep-
tual schema that corresponds to the possible design speciﬁcations of the database
schema [5].
1.1
DBRE Motivation
In general, the reasons to reverse engineer a database may include data migra-
tion from legacy systems, data integration, data exchange at conceptual level,
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 410–421, 2017.
DOI: 10.1007/978-3-319-67162-8 40

Formalization of Database Reverse Engineering
411
semantic acquisition for system maintenance, for domain (re)documentation, and
for query answering using the conceptual schema [7].
To fully comprehend the DBRE cause, an understanding of database design
procedure is also necessary. Database design, or forward engineering, is a process
which maps elements in a conceptual schema to elements in a database via a
series of tasks such as requirement analysis, entity and relationship identiﬁcation
through dependencies (constraints), key selection, normalisation, etc.
During the database design process, while the speciﬁcation transitions from
the conceptual schema to the logical and physical schemas, it is often found that
domain knowledge has been left implicit or lost within the database. This can be
attributed to the fact that the relational schema is semantically poorer than most
conceptual schemas, and it cannot explicitly express certain domain information,
e.g. generalisation and specialisation hierarchies, unions and disjointness of entity
sets [12].
Fig. 1. Semantic loss during Database Design [5]
Furthermore during the life cycle of the database system, the structure of
the database is prone to changes over the years of its usage and is subject to
semantic degradation as described in Fig. 1. Whether this degradation is caused
by the poor semantic nature of the relational database, tricky original design
or the use of certain optimisation practices e.g. de-normalization of some rela-
tions to increase performance, the database users remain at risk of losing the
understanding of its contents.
With no concrete knowledge of the initial design strategy used to forward
engineer the database system, no detailed documentation through which contin-
uous modiﬁcations to the database could have been tracked, and the unavailabil-
ity of the original database developers, the problem becomes not only non-trivial
but also tedious [8]. As one of the main beneﬁts of a conceptual schema is to pro-
vide users which an abstract and eminently clear representation of the domain,
the DBRE process is required to carefully implement the backward transitioning

412
N. Ndefo
of the forward engineering process, ﬁlling up the perceived semantic gaps in the
database schema to construct a conceptual schema.
The rest of this paper is organised as follows: Sect. 2 gives an overview of
the DBRE approaches, also highlighting the need for a completely formal frame-
work. In Sect. 3, several related works are reviewed including the state of the
art research on DBRE and some studies on schema transformation in database
systems. The plan for formalising the entire DBRE process is detailed in Sect. 4,
including the choice of conceptual schema and formal language to illustrate the
formalism, and important concepts of interest concerning schema transforma-
tion. Finally, Sect. 5 provides a summary and additional information about the
research.
2
Overview of DBRE Approaches
The DBRE approaches usually perform what is referred to as semantic acquisi-
tion as part of the ﬁrst step of the process. This is done by analysing the database
schema along with other sources such as the schema’s DDL, data instances, and
the SQL queries derived from application programs that use the database, in the
hope of obtaining a richer description of the domain. These approaches use the
mentioned sources (singly or combinatorially) and are able to attain additional
semantic information from them, this includes all information about elements,
which are basically relations1 and constraints.
After these sources have been explored and the database schema has been
enriched with new-found information, then succeeding steps can occur: the clas-
siﬁcation and conceptualisation of elements. In the conceptualisation step, each
element within the relational schema is transformed into a corresponding element
in the conceptual schema by a set mappings.
Here, the term conceptual schema is used loosely and considers high-level
data models which describe semantics of data in similar ways, for instance, in
the Entity Relationship (ER) model with its use of entities, which are viewed as
classes in the Uniﬁed Modelling Language (UML) class diagrams, concepts in
ontology languages, and objects in Object Role Modelling (ORM). Even though
conceptual schemas in general can have various degrees of expressiveness, it can
be proven that at least most of the common elements can be accurately translated
among these similar data models. A survey of these approaches shows that a
preferred target schema for re-engineering database schemas is the ER model or
its upgraded version Enhanced Entity Relationship (EER) [1,6,11,14,15,18,21].
Other choices have been Object Modelling Technique (OMT) [19], ERC+(an
extension of the ER model with multivalued objects and multi-instantiation)
[3], and First Order Logic (FOL) ontologies [4,13,14]. Table 1 shows a nicely
structured overview of certain similarities and contrasts among of the existing
approaches.
1 Relations consist of typed attributes, with each attribute belonging to its own ﬁnite
attribute domain.

Formalization of Database Reverse Engineering
413
Table 1. Comparison of DBRE approaches
Approach
Source schema
Other sources
Target schema
Formal Verification
Extracted constraints
Automation
Astrova [4]
Relational schema in
3NF
Data instances
Ontology for the Semantic
Web
None
Key attributes,
Inclusion, Disjointness,
Intersection constraints
Semi-
automated
Alhajj [1]
Schema in 3NF
Data instances
EER
None
Key attributes,
Cardinality, Inclusion
constraints
Automated
Andersson [3]
Schema (no
assumptions on normal
forms)
SQL queries, DDL, Data
instances
ERC
None
Key attributes,
Inclusion constraints
Non-automated
Chiang et al. [6]
Schema in 3NF
Data instances, DDL
EER
None
Keys, Inclusion
constraints
Semi-
automated
Petit et al. [18]
Schema in 3NF
Data instances, SQL
equi-join queries
EER
None
Key attributes,
Inclusion constraints,
Disjointness constraints
Semi-
automated
Lammari et al. [12]
Schema
DDL, SQL queries, Data
instances (if necessary)
EER
None
Key attributes,
Inclusion and
Intersection constraints
Automated (or
Semi-
automated, if
necessary)
Lin et al. [13]
Schema in 3NF
Data Instances
OWL DL ontology
None
Key attributes, Classes,
Data-type Property,
Object Property,
Cardinality, Inclusion
constraints,
Disjointness, Equivalent
Classes, Covering
Automated
Signore et al. [21]
Schema assumed to be
in 3NF
Application program code
with embedded SQL
ER Model
None
Key attributes, IS-A
hierarchies
Non-automated
Markowitz & Makowsky [15] Schema in BCNF
None
EER
Proof sketches for
mappings
Key attributes,
Inclusion constraints
Non-automated
Lubyte & Tessaris [14]
Schema in 3NF
None
DLR-Lite (Description
Logic) Ontology
Proof for equivalence
preserving mapping
Key attributes,
Inclusion, Disjointness,
Covering, Mandatory
and Optional
Participation,
Functionality, Role
Typing
Automated
Blaha & Premerlani [19]
Schema (no
assumptions on normal
forms)
Data instances
OMT
None
Key attributes,
inclusion constraints,
Cardinality
Non-automated
Johannesson [11]
Schema in 3NF
Conceptual schema defined
as a pair of language L and
a set IC of typing,
mapping and generalization
constraints
Proof sketch for schema
dominance
Key attributes,
Inclusion constraints
Non-automated

414
N. Ndefo
2.1
Problems with Existing Approaches
More compactly, a DBRE setting should consist of a source schema, a target
schema and a well-deﬁned transformation process between the schemas. Closely
inspecting these approaches, it is discovered that they are mostly non-automated
and involve human interaction not just for a ﬁnal user validation but often
times mid-process. Therefore the mappings induced by the transformations most
likely rely on heuristic techniques and informal rules dependent on the expertise
and view point of the users. Consequently, each approach, if used on the same
database schema may channel towards a relatively diverse target schema, with
probable semantic inconsistencies.
The above claim is attributed moreover to the little eﬀort demonstrated
in presenting a guaranteed measure for correctness post-transformation, thus
rendering the validity of the reverse engineering process questionable.
Take for instance the case of classifying an association between two relations,
say SSN and PERSON. Relation SSN holds records of taxation details for each
person while relation PERSON holds basic information for the each person.
PERSON[ssn, name, address...]
SSN[ssn, tax category, issuing state, ...]
Both relations have the same key ssn, used to uniquely identify each tuple in
both PERSON and SSN. If no information about referential integrity is known,
based on the set of heuristic rules in [6], this association is justiﬁed as a subtype
inclusion constraint because the relations share the same key attribute, and is
therefore mapped accordingly. Whereas Lin et al. [13] argue that it may not be
so that “a PERSON is a SSN ” or vice versa just for this reason and consider
a “more natural interpretation” which portrays this association as a one-to-one
or one-to-many binary relationship.
Another usually overlooked issue concerns the handling of nullable attributes
(null values) within DBRE. Since the relational and conceptual schemas are of
diﬀerent signatures, the interpretation of nullable attributes in the conceptual
schema needs to be properly addressed with respect to preserving information.
This aspect is yet to be thoroughly treated whether formally or informally.
We believe that providing documented formal descriptions for the process
would help in resolving these issues and other related ones. Then we can estab-
lish the most logically plausible mappings which rightly preserve source schema
semantics in the target schema, and verify that a DBRE transformation can be
indeed correct.
However, before delving into our plans to address these issues, we consider it
necessary to review relevant literature in the reverse engineering context through
which substantial contributions have been produced. In the next section some
notes on these notable contributions are presented.

Formalization of Database Reverse Engineering
415
3
Related Work
Since the 1990s, studies concerning DBRE have been carried out, of which the
most referenced have been listed in Sect. 2.
In [11], the database schema in 3NF is translated into a conceptual schema
represented by a pair consisting of a language and a set of typing, mapping,
and generalisation constraints. The methodology investigates object structures
and how they can be identiﬁed in the database schema based on the correla-
tion between keys and inclusion constraints. The work concludes by deﬁning the
notion of schema dominance as a measure of correctness for the reverse engi-
neering method. As future work, the paper suggests a potential line of research:
investigating the inﬂuence of views on conceptual modelling. We plan to improve
on this work by investigating this schema dominance on diﬀerent reverse engi-
neering scenarios, after deﬁning mappings (views) of elements in one schema
based on the signature of the other schema.
The work of [15] identiﬁes object structures in a database, taking as input
the relational schema in Boyce-Codd normal form (BCNF) consisting of key
constraints and key-based inclusion constraints. The output of the approach is
an EER model. This method argues against the informal mappings presented in
other works and introduces some formal mapping descriptions for a procedure
which determines convertibility of a BCNF normalised relational schema into the
EER model, although BCNF is considered as one of the stricter normal forms
in the context of realistic database schemas.
The most recent research works steer towards reverse engineering a rela-
tional database to logic-based ontologies. In [14], an ER model is extracted from
an existing relational database, with mappings deﬁned between the extracted
schema and the relational database by associating views over the latter as a
means to access the underlying data. The approach subtly exploits the Global-
as-View (GAV) mappings from data integration. It highlights important points
such as obtaining an equivalence preserving schema transformation and uses
description logic to express semantics of the schema. Our approach contributes
to this by considering in addition reverse mappings similar to the Local-as-View
(LAV) to verify correctness DBRE in terms of losslessness.
Astrova [4] focuses on reverse engineering a relational database into an OWL
ontology for use on the Semantic Web. The approach conﬁrms semantics by
analysing not only the correlation between primary and foreign keys, but also the
correlation among data values and attributes. The reverse engineering process
is simple and shows some mapping for relations, attributes, relationships, and
constraints. However it is not formally presented and there is no veriﬁcation
for information consistency. The work claims to show an extraction of more
semantics from the relational database compared to other approaches.
The work in [13] describes a DBRE-based automatic algorithm to extract
an OWL-DL ontology with richer semantics from a given relational database.
Compared to the others mentioned above, this approach goes a step further by
illustrating the correspondences among semantics of the ER model, database
schema, and OWL ontology.

416
N. Ndefo
In general, the reverse engineering approaches could rely heavily on human
intervention, be semi-automated or fully automated. However it is to be noted
that in any system, before complete automation can occur for any process there
should be a set of formal deﬁnitions properly set in place. Though these works
have contributed signiﬁcantly to this ﬁeld, they will be used as building blocks
for rendering a more complete framework.
Since schema transformation is a fundamental part of DBRE, it is useful to
study transformation practices involving relational databases.
Hainaut et al. in [9] oﬀer a generic technique for schema transformation for
a database, describing schema transformation as consisting of two types of map-
pings: structural mapping and instance mapping. The ﬁrst ensures that elements
in the source schema can be replaced by elements in the target schema, and
the latter ensures correspondences between instances in both schemas. It also
introduces transformation concepts such as schema reversibility and semantics
preservation.
McBrien and Poulovassilis in [16] present a schema transformation approach
based on combining both GAV and LAV mapping approaches. The coined name,
BAV, meaning Both-as-View leverages the beneﬁts of both GAV and LAV views.
The concept of reversibility is used to maintain here as a desired aspect of the
transformation process.
The work presented in [2] aims to discover semantic matches between two
databases along with their already existing conceptual schemas. The approach
described here ﬁnds the mappings based on the correspondences (or links)
between attributes and relations.
Qian, in [20], emphasizes that the correctness of a schema transformation is
achieved when the instances and constraints in the source schema are preserved.
Three transformation properties are highlighted: instance, constraint, and infor-
mation preservation. The last is a consequence of the ﬁrst two. The work deﬁnes
an information preserving transformation as in terms of containment between
the source and target schemas denoted by s and t respectively i.e. to determine
if s ⊆t and t ⊆s. And if these containments hold then s is said to be equivalent
to t, denoted by s ≡t.
This section has presented concise descriptions of other research works related
to the one proposed. As stated earlier, schema transformation is a pivotal aspect
in DBRE, and any plan to produce a useful method should focus primarily on
how semantic equivalences are identiﬁed between the source and target schemas
and how eventual maps are created. This therefore justiﬁes the following section
which discusses the details of the proposed formalism.
4
Formalisation Plan of DBRE
By deﬁnition, one understands that DBRE consists of three main aspects i.e. the
source schema, schema transformation, and the target schema. The approach to
formalise DBRE rests on grasping the semantics of each of the individual aspects
and how they relate. This section gives brief details on the source and target
schemas, and the formalisation plan.

Formalization of Database Reverse Engineering
417
4.1
Source Schema
We will provide various realistic scenarios of database schema as examples. We
assume that semantic acquisition has already been carried out on the source
schema by analysing the input sources mentioned in Sect. 2, and the schema
is now complete with all intended semantics from the domain i.e. all possible
information about relations and constraints are known. At this point, the schema
is also presumed to have been decomposed into a suﬃcient normal form, ideally
3NF, so that the conceptual schema elements can be seamlessly identiﬁed and
classiﬁed prior to conceptualisation.
4.2
Target Schema
For the target schema, ORM [10] will be used to describe the semantics extracted
from the source schema. Modelling in ORM is fact-based i.e. data is described as
elementary facts, which are predicates asserting that an object type participates
in a role (relationship). The model is attribute-free, representing the relationship
between an object and its attributes also as roles. The example in Fig. 2 illus-
trates two fact types: one binary and the other ternary. The binary role seeks
connects the object types Student and Degree. In the ternary fact, the role ...was
awarded...on... connects the same object types but now with another object type
Date.
Fig. 2. Example ORM diagram (http://www.orm.net)
Integrity constraints and other constraints can be expressed clearly. Given the
same example in Fig. 2, the bars over the roles indicate uniqueness constraints
over each entity participating in those roles. The other constraint seen in the
diagram is the exclusion constraint (circled ‘x’) between the roles seeks and ...was
awarded...on... indicating disjointness between the instances in these roles.
An advantage of using ORM over the other similar conceptual schemas,
besides the fact that it has a standard notation and its ability to model relatively
more features, is the possibility to automatically derive natural language verbal-
isation for its conceptual schemas using the Natural ORM Architect (NORMA)
tool within Microsoft Visual Studio development environment. The verbalisation
texts are expressed with regards to FORML, the controlled language for ORM

418
N. Ndefo
and as far we know, an automatic verbalisation feature with such convenient
readability is yet to be embedded in any of the other common modelling tools.
The following verbalisation texts explain Fig. 2:
– Student was awarded Degree on Date.
– For each Student and Degree, that Student was awarded that Degree on at
most one Date.
– Student seeks Degree.
– Each Student seeks at most one Degree.
– It is possible that more than one Student seeks the same Degree.
– For each Student and Degree, at most one of the following holds: that Student
seeks that Degree; that Student was awarded that Degree on some Date.
This feature is very useful for veriﬁcation with non-technical users who may
have trouble understanding the non-trivial technical terminology in the diagram.
4.3
Schema Transformation
Before discussing the schema transformation aspect, we brieﬂy describe the lan-
guage intended for expressing the semantics. For the purpose of this research
and the nature of the schemas in question, a language which is able to clearly
express relational elements is most suitable.
Relational Algebra. Relational algebra is a procedural query language that
manipulates instances of relations in a database schema by applying operators
in order to also produce as output relational instances. Relational algebra is the
backbone of the database query language SQL. Below are the main operators
used by the algebra, some of which are inherited from set theory:
∩
intersection
π projection
∪
union
σ
selection
−
minus
▷◁
join
× cartesian product ←
rename
We use the formula below to illustrate an example for the syntactic structure
of a relational algebra query expression:
stellar students ←πnr, age
σDegree.code=Student.degree code F K
and Degree.final score>98
(Student × Degree)
(1)
Apart from query expressions, the procedural system of the algebra can also
be employed to describe constraints over relations. Key constraints, inclusion
constraints, and functional dependencies are examples of important constraints
which can be captured by relational algebra. Constraints can be seen as boolean
expressions that are valid i.e. always true. To describe constraints, we depend
on the full power of relational algebra, plus assertions between an expression

Formalization of Database Reverse Engineering
419
and the empty set ∅. Take for example the model in Fig. 2. We interpret seeks
and ...was awarded...on... simply as binary and ternary relations respectively.
To show the exclusion constraint between the two roles, we use the following
expressions:
Temp ←πnr F K, degree code F K(wasAwardedOn)
Temp ∩Seeks = ∅
Expressions in relational algebra can also be translated into relational calcu-
lus which is a variant of ﬁrst-order logic.
Transformation. The transformation aspect of DBRE represents the core of
the entire process, and therefore requires the profoundest consideration. Here,
we summarise the plan to formalise this aspect.
A semantically enriched source schema s is a pair (Σs, λs), where Σs is
the signature of the schema consisting of a set of relations, and λs is a set of
constraints in s. The target schema t is a pair (Σt, λt) where Σt is the signature
of the conceptual schema consisting of objects connected by roles, and λt is a
set of constraints in t. Conventionally, a DBRE schema transformation yields a
set of mappings μ for each relation rs such that rs ∈s and is transformed into
a corresponding object ot such that o ∈t, without violating λs.
After conceptualisation of each relational term into a corresponding concep-
tual term, a mechanism is required to check the correctness of the transfor-
mation. Our research work particularly contributes to this by expanding the
transformation process. We propose including inverse mappings as a means to
check correctness, therefore deﬁning mappings not only in the direction s →t
but also from t →s, for every element in both schemas.
More formally, we describe the semantics of the mapping below:
The mapping μ is a function symbol such that μ : rs →vt, where vt is an
associated view in t i.e. a relational algebraic expression for the corresponding
ot which associates with rs. Then we say that μ−1 is the inverse of μ such that
μ−1 : ot →vs, where vs is an associated view in s i.e. a relational algebraic
expression for the corresponding rs which associates with ot. Therefore we may
also state that μ is a bijection. In μ, each rs is mapped to a vt expression in a
LAV-like manner. In μ−1, the reverse holds, where each object ot is mapped to
a vs expression, as would GAV mappings.
Though s and t describe the same domain, they are projected diﬀerently.
Deﬁning views over them will require decomposing and joining of these projec-
tions. For this reason we need to ensure that both μ and μ−1 can be carried out
in such a way that the semantics of the data and their constraints are not lost
between them. This leads to investigating the schema transformation property
known as losslessness.
We say that μ and μ−1 together are lossless if and only if λs |= λt[oi
t/vs(oi
t)]
and λt |= λs[ri
s/vs(ri
s)]. More verbosely, this means that λs entails λt following
the substitution of each ot according to vs, the deﬁned view for ot over the

420
N. Ndefo
language of s, and λt entails λs following the substitution of each rs according
to vt, the deﬁned view for rs over the language of t.
If this losslessness property is always true, then we can assert that
μ−1(μ(rs)) = rs, for each rs. This means that μ−1 ◦μ which maps from
rs →ot →rs constitutes the identity function idrs for all rs, and μ ◦μ−1
which maps from ot →rs →ot constitutes the identity function idot for all ot.
Intuitively, with the evidence of a lossless transformation we can then aﬃrm
that the DBRE process is correct and complete, then a possibility opens up for
building queries over the conceptual schema itself.
5
Summary
The main contribution of this research will be the formalisation of the entire
DBRE process, with focus on ensuring correctness on the conventional methods.
The submission of this paper marks the conclusion of the ﬁrst 6 months of the
ongoing PhD research work. The past months have been dedicated to literature
study of which we have been able to produce substantially. Having covered the
signiﬁcant grounds for this research work, the immediate next step is to demon-
strate DBRE in action, covering various natural scenarios, with the aﬃliated
proofs to support its correctness.
References
1. Alhajj, R.: Extracting the extended entity-relationship model from a legacy rela-
tional database. Inform. Syst. 28(6), 597–618 (2003)
2. An, Y., Borgida, A., Miller, R.J., Mylopoulos, J.: A semantic approach to discov-
ering schema mapping expressions. In: 2007 IEEE 23rd International Conference
on Data Engineering, pp. 206–215, April 2007
3. Andersson, M.: Extracting an entity relationship schema from a relational database
through reverse engineering. In: Loucopoulos, P. (ed.) ER 1994. LNCS, vol. 881,
pp. 403–419. Springer, Heidelberg (1994). doi:10.1007/3-540-58786-1 93
4. Astrova, I.: Reverse engineering of relational databases to ontologies. In: Bussler,
C.J., Davies, J., Fensel, D., Studer, R. (eds.) ESWS 2004. LNCS, vol. 3053, pp.
327–341. Springer, Heidelberg (2004). doi:10.1007/978-3-540-25956-5 23
5. Chiang, R.H.L., Barron, T.M.: Quality issues in database reverse engineering: an
overview. In: Proceedings for Operating Research and the Management Sciences,
pp. 185–189, June 1995
6. Chiang, R.H.L., Barron, T.M., Storey, V.C.: Reverse engineering of relational data-
bases: extraction of an eer model from a relational database. Data Knowl. Eng.
12(2), 107–142 (1994)
7. Hainaut, J.-L.: Introduction to database reverse engineering. LIBD Lecture Notes
(2002)
8. Hainaut, J.-L.: Research in database engineering at the university of namur. ACM
SIGMOD Rec. 32(4), 124–128 (2003)
9. Hainaut, J.-L., Tonneau, C., Joris, M., Chandelon, M.: Schema transformation
techniques for database reverse engineering. In: Proceedings of the 12th Interna-
tional Conference on ER Approach, pp. 353–372. Springer, Arlington-Dallas (1993)

Formalization of Database Reverse Engineering
421
10. Halpin, T.: Object-role modeling: principles and beneﬁts. Int. J. Inform. Syst.
Model. Des. 1(1), 33–57 (2010)
11. Johannesson, P.: A method for transforming relational schemas into conceptual
schemas. In: Proceedings of the 10th International Conference Data Engineering,
1994, pp. 190–201. IEEE (1994)
12. Lammari, N., Comyn-Wattiau, I., Akoka, J.: Extracting generalization hierarchies
from relational databases: a reverse engineering approach. Data Knowl. Eng. 63(2),
568–589 (2007)
13. Lin, L., Zhuoming, X., Ding, Y.: Owl ontology extraction from relational databases
via database reverse engineering. JSW 8(11), 2749–2760 (2013)
14. Lubyte, L., Tessaris, S.: Automatic extraction of ontologies wrapping rela-
tional data sources. In: Bhowmick, S.S., K¨ung, J., Wagner, R. (eds.) DEXA
2009. LNCS, vol. 5690, pp. 128–142. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-03573-9 10
15. Markowitz, V.M., Makowsky, J.A.: Identifying extended entity-relationship object
structures in relational schemas. IEEE Trans. Softw. Eng. 16(8), 777–790 (1990)
16. McBrien, P., Poulovassilis, A.: Data integration by bi-directional schema trans-
formation rules. In: Proceedings of the 19th International Conference on Data
Engineering, 2003, pp. 227–238. IEEE (2003)
17. A M¨uller, H., Jahnke, J.H., Smith, D.B., Storey, M.-A., Tilley, S.R., Wong, K.:
Reverse engineering: a roadmap. In: Proceedings of the Conference on the Future
of Software Engineering, pp. 47–60. ACM (2000)
18. Petit, J.-M., Toumani, F., Kouloumdjian, J.: Relational database reverse engi-
neering: a method based on query analysis. Int. J. Coop. Inform. Syst. 4(02n03),
287–316 (1995)
19. Premerlani, W.J., Blaha, M.R.: An approach for reverse engineering of relational
databases. In: Proceedings Working Conference on Reverse Engineering, pp. 151–
160, May 1993
20. Qian, X.: Correct schema transformations. In: Apers, P., Bouzeghoub, M.,
Gardarin, G. (eds.) EDBT 1996. LNCS, vol. 1057, pp. 114–128. Springer,
Heidelberg (1996). doi:10.1007/BFb0014146
21. Signore, O., Loﬀredo, M., Gregori, M., Cima, M.: Reconstruction of ER schema
from database applications: a cognitive approach. In: Loucopoulos, P. (ed.) ER
1994. LNCS, vol. 881, pp. 387–402. Springer, Heidelberg (1994). doi:10.1007/
3-540-58786-1 92

Supporting Conceptual Modelling in ORM
by Reasoning
Francesco Sportelli(B)
Free University of Bozen-Bolzano, Bolzano, Italy
fsportelli@unibz.it
Abstract. Object-Role Modelling (ORM) is a framework for modelling
and querying information at the conceptual level. It comes to support the
design of large-scale industrial applications allowing the users to easily
model the domain. The expressiveness of the ORM constraints may lead
to implicit consequences that can go undetected by the designer in com-
plex diagrams during the software development life cycle. To avoid these
issues we perform the reasoning on ORM diagrams in order to detect
relevant formal properties, such as inconsistencies or redundancies, that
cause a software quality degradation leading to an increment of develop-
ment times and costs.
In this paper we present an extension of ORM formalisation by Deriva-
tion Rules, which are additional ORM constructs that capture some rel-
evant information of the domain that cannot be expressed in standard
ORM.
Moreover, we provide a tool (UCM Framework) which enables reason-
ing on conceptual modelling software along with an implemented case of
study (ORMiE).
Keywords: ORM · Conceptual modelling · Reasoning · Rules
1
Overview
Conceptual modelling is a critical step during the development of a database sys-
tem. It is the detailed description of the universe of discourse in a language that is
understandable by users of the business domain. Object-Role modelling (ORM)
is a conceptual language for modelling, which includes a graphical and textual
language for specifying models, a textual language for formulating queries, as
well as procedures for constructing ORM models, and mapping to other kinds of
models like UML and ER. ORM is fact-oriented, i.e., it models the information
in a way that it can be verbalized using sentences that are easily understandable
by domain experts and even for those who are not familiar with IT in general.
Unlike ER and UML, fact-oriented models are attribute-free, treating all facts
(sentences) as relationships (unary, binary, ternary etc.) and this makes it more
stable and adaptable to changing business requirements. For example, instead of
using the attributes Person.isSmoker and Person.hiredate, fact-oriented models
use the fact types Person smokes and Person was hired on Date [1].
c
⃝Springer International Publishing AG 2017
M. Kirikova et al. (Eds.): ADBIS 2017, CCIS 767, pp. 422–431, 2017.
DOI: 10.1007/978-3-319-67162-8 41

Supporting Conceptual Modelling in ORM by Reasoning
423
The ORM constraints expressiveness may lead to implicit consequences that
can go undetected by the designer in complex diagrams; this may also lead to
various forms of inconsistencies or redundancies in the diagram itself that give
rise to the degradation of the quality of the design and/or increased development
times and costs. The approach used to solve this issue involves the automated
reasoning in order to detect inconsistencies and redundancies. Moreover, ORM
diagrams can be equipped with Derivation Rules which are additional ORM
constructs which are able to express knowledge that is not expressible with
standard ORM. The usage of those rules brings to a further complexity so the
goal of this paper is to detect a decidable fragment in order to perform the
reasoning task even on those ORM diagrams equipped with those rules.
We also introduce the state of the art starting from the ﬁrst ORM formal-
isation until the most recent one. Then we introduce an overview of the ORM
language, focusing on its main features like fact-oriented, the verbalisation and
the graphical notation, providing also the running example that will be shown in
the rest of the paper. The section concerning the research has three subsections:
1. UCM Framework, a tool designed to activate automated reasoning on con-
ceptual modelling software;
2. ORMiE, a tool which performs reasoning on ORM diagrams using UCM
Framework;
3. Derivation Rules, where we show the results achieved so far concerning the
formalisation.
We conclude the paper presenting the list of what is still to be done in the frame
of the PhD.
2
Motivation
The ORM formalisation has been treated in years of research and still today
it is a topic of interest. Formalising such language allows to activate reasoning
procedures, carried out by Description Logics reasoners. Since the reasoning is
able to detect relevant formal properties, such as inconsistencies or redundancies,
the purpose of the reasoning is to support the modeller during the modelling
which is a delicate process during the development of a software or a database.
Without this support such issues could lead to a degradation of the quality of
the design and an increase of development times and costs. Especially in large
diagrams those issues are hard to spot by naked eye, so the need of this approach
is crucial to prevent mistakes during the software or database development.
Although several papers presented their own ORM formalisation, no one has
taken into account the formalization of derivation rules so far. Derivation rules
are new ORM constraints which are able to express knowledge that is beyond
normal ORM capabilities, but this feature leads to an increase of expressiveness
of the diagrams. For this reason, the challenge is to identify a decidable fragment
in order to extend the reasoning even on those ORM diagrams equipped with
those rules.

424
F. Sportelli
Another challenge has more a methodological ﬂavour, which involves the
development of a system that is able to extend any conceptual modelling appli-
cations with reasoning services. The direct impact of this research involves the
modellers, the developers and those who need a support tool which easily checks
the consistency of conceptual schema in order to save time during the develop-
ment life cycle.
3
Related Work
The ORM formalisation started with Terry Halpin’s PhD Thesis [2]. In the con-
text of design conceptual and relational schemas, Halpin formalized the NIAM
language that is the ancestor of ORM. In his thesis there is the ﬁrst attempt
to formalize a modelling language in order to perform the reasoning task, so
the main objective is to provide formal basis for reasoning about conceptual
schemas and for making decision choices. After the spreading of ORM and its
implementation in NORMA [3,4], ORM became more popular so the logicians’
community took into account the possibility to formalize this very expressive
language.
In 2007, Jarrar formalizes ORM using DLRifd [5], an extension of Descrip-
tion Logics introduced in [6]. The paper shows that a formalisation in OWL
SHOIN would be less eﬃcient than DLRifd because some ORM constraints
cannot be translated (predicate uniqueness, external uniqueness, set-comparison
constraints between single roles and between not contiguous roles, objectiﬁcation
n-ary relationships). In [7], Jarrar encodes ORM into OWL SHOIN. Another
formalisation of ORM in DLRifd was done by Keet in [8].
In 2009 OWL2 was recommended by W3C Consortium as a standard of ontol-
ogy representation on the Web bringing some beneﬁts: it is the recommended
ontology web language; it is used to publish and share ontologies on the Web
semantically; it is used to construct a structure to share information standards
for both human and machine consumption; automatic reasoning can be done
against ontologies represented in OWL2 to check consistency and coherency of
these ontologies.
An ORM formalisation based on OWL2 is proposed by Franconi in [9], where
he introduces a new linear syntax and FOL semantics for a generalization of
ORM2, called ORM2plus, allowing the speciﬁcation of join paths over an arbi-
trary number of relations. The paper also identiﬁes a “core” fragment of ORM2,
called ORM2zero, that can be translated in a sound and complete way into the
ExpTime-complete Description Logic ALCQI. In [10] is provided a provably cor-
rect encoding of a fragment of ORM2zero into a decidable fragment of OWL2
and it is discussed how to extend ORM2zero in a maximal way by retaining at
the same time the nice computational properties of ORM2zero.
The most recent paper related to ORM formalisation is [11] where Artale
introduces a new extension of DLR, namely DLR+. This paper is strictly con-
nected with this work because the logic DLR+ it is meant to represent n-ary

Supporting Conceptual Modelling in ORM by Reasoning
425
relationships which are suitable for languages like ORM. The ORM implemen-
tation we use is an ongoing work based on DLR+. In particular, the decidable
fragment we use is DLR±, obtained by imposing a simple syntactic condition
on the appearance of projections and functional dependencies in DLR+. In the
paper is also provided an OWL encoding and it is proved that DLR± captures
a signiﬁcant fragment of ORM2.
Since this work is also focused on the formalisation of derivation rules, we
need to mention OCL. OCL stands for Object Constraint Language, it is the
declarative language for describing rules that apply to UML diagrams for deﬁning
constraints in order to support the conceptual modelling, like Derivation Rules
for ORM. In [12] has been provided a formalisation of a fragment of this language
and has been also proved the equivalence between relational algebra and the
fragment with only FOL features, namely OCLF O.
We conclude this section stating that, to best of our knowledge, we use DLR±
in order to build a decidable ORM mapping into OWL. In particular, we focus
on Derivation Rules formalisation.
4
ORM
ORM stands for Object-Role Modelling. It is a language that allows users to
model and query information at the conceptual level where the world is described
in terms of objects (things) playing roles (parts in relationships) [13]. The idea
behind ORM and its approach is that an object-role model avoids the need to
write long documents in ambiguous natural language prose. It’s easy for non-
technical sponsors to validate an object-role model because ORM tools can gen-
erate easy-to-understand sentences. After an object-role model has been vali-
dated by non-technical domain experts, the model can be used to generate a
class model or a fully normalised database schema. ORM main features are:
– fact-oriented, all facts and rules are modelled in terms of controlled natural
language (FORML) sentences easy to understand even for non-technical users;
– attribute-free, unlike ER and UML, makes it more stable and adaptable to
changing business requirements;
– graphical, it has a graphical notation implemented by the software NORMA;
– formalised, it has a clear syntax and semantics, so reasoning on an ORM
diagram is enabled.
Unlike ER or UML, ORM makes no use of attributes in its base models;
although this often leads to larger diagrams, an attribute-free approach has
advantages for conceptual analysis, including simplicity, stability, and ease of
validation. Attribute-free models with a controlled natural language facilitate
model validation by verbalisation and population. Model validation should be
a collaborative process between the modeller and the business domain expert
who best understands the business domain. All facts, fact types, constraints and
derivation rules may be verbalised naturally in unambiguous language that is

426
F. Sportelli
Fig. 1. ORM diagram example
easily understood by domain experts who might not be experts in the software
systems ultimately used for the physical implementation.
The meaning of the diagram in Fig. 1 is the following: a person can be a
citizen or a visitor; each person is identiﬁed by one and only one document which
can only be either a visa or an id card. The entities are depicted by smooth
rectangles and the relationships by a sequence of tiny boxes according to the
cardinality relationship. The purple dot represents the mandatory constraint,
the dash on the tiny rectangle box is the uniqueness constraint, the equivalent
of the relational keys. The arrows among entities represents the ISA relationship.
Finally, the circle with the cross inside means disjointness; the one with another
circle inside means covering; the combination of this two is the circle we see
between Visa and IDCard. The notation (.Name) and (.Id) inside Person and
Document it is a graphical shortcut provided by NORMA for top level entities.
Intuitively, it means that each person has a name and each document has an id.
The corresponding FORML verbalization is the following:
Person is an entity type.
Citizen is an entity type.
Visitor is an entity type.
Document is an entity type.
VISA is an entity type.
IDCard is an entity type.
Person is identified by Document.
Each Citizen is an instance of Person.
Each Visitor is an instance of Person.
Each VISA is an instance of Document.
Each IDCard is an instance of Document.
Each VISA is an instance of Document.
Each IDCard is an instance of Document.
Each Person has exactly one Document.
For each Document, at most one Person has that Document.
For each Document, exactly one of the following holds: that Document is
some VISA; that Document is some IDCard.
This feature turns out to be helpful during the modelling phase especially when the
non-IT stakeholders interact with the software engineers in order to reach a mutual com-
prehension about the meaning of the diagram. For example, if the non-IT stakeholder

Supporting Conceptual Modelling in ORM by Reasoning
427
detects unexpected sentences which do not reﬂect the software speciﬁcations, it is easy
for the modeller to modify the interested part.
5
Contributions
The main goal of the research concerns to enrich the conceptual modelling by reasoning
in order to detect constraints which can lead to unexpected software behaviours, or to
infer new knowledge. This research is characterized by the synergy of the methodolog-
ical and the theoretical aspects. UCM Framework (Universal Conceptual Modelling
Framework) and ORMiE (ORM Inference Engine) are tools developed to implement
the ORM language in order to perform the reasoning over ORM schemas. Moreover,
the theoretical part is focused on the formalisation of ORM Derivation Rules which
has been also implemented in the aforementioned tools.
5.1
UCM Framework
Usually conceptual modelling tools do not take into account the problem of check-
ing whether the semantics of the conceptual schema is consistent or not. To tackle
this situation we developed UCM Framework which activates reasoning on concep-
tual modelling applications. UCM Framework has several features: it provides API for
developers, reasoning services, the import/export of ontologies and diagrams in dif-
ferent languages like ORM, UML and ER. In Fig. 2 is shown the architecture of the
framework. Each conceptual modelling application communicates with the core sys-
tem using a speciﬁc driver, both for input and for the output where the inferences
are encoded. The input schema is ﬁrst encoded in a data structure (UCM Model) by
API services, then the reasoning is performed by Fact++ reasoner [14]. After that, the
inferences are stored into another data structure (UCM Inferred Model) by API and
inferences are delivered to the destination application by drivers. Using this approach
one can easily integrate this framework in order to enrich its conceptual modelling
application by reasoning. Currently, two applications use this framework: Menthor [15]
and ORMiE.
5.2
ORMiE
ORMiE (ORM Inference Engine) is an extension of NORMA, which is the oﬃcial
ORM-based Microsoft Visual Studio conceptual modelling tool [3]. ORMiE uses UCM
Framework, so it is just one example how UCM Framework works on a target ORM-
based software. ORMiE activates automated reasoning over ORM diagrams providing
an interface where mistakes, redundancies or more in general new inferred knowledge
are shown. It takes advantage of all nice features from Visual Studio Framework being
such a powerful tool for those who need to model a domain following the ORM method-
ology. Moreover, ORMiE is able to perform the reasoning on those ORM diagrams
equipped with Derivation Rules.
5.3
Derivation Rules
Derivation Rules are special ORM constructs which express knowledge that would oth-
erwise not be expressible by standard ORM, so their expressibility is far reaching than

428
F. Sportelli
Fig. 2. UCM framework architecture
standard ORM. Their purpose is to derive new information from other information,
like triggers, stored procedures and views in SQL. The goal is to enable the reasoning
on those rules in order to extend the reasoning even on those ORM diagrams equipped
with Derivation Rules. There are two kind of Derivation Rules: the Subtype Deriva-
tion Rules and the Fact Type Derivation Rules. A Subtype Derivation Rule deﬁnes
all the instances which belongs to a subentity by a set of constraints deﬁned in the
rule deﬁnition. The reason because those rules are applied on subentities is because
in some diagrams the is-a relationship between entities is too weak to capture the
entire desired semantics of the diagram; a FactType Derivation Rule is placed on the
predicates, namely the ORM roles.
The Derivation Rules we are focusing on are Subtype Derivation Rules. We want
to formalise those rules in order to activate the reasoning on those diagrams with
Subtype Derivation Rules. To achieve this goal it is important to take into account
that the reasoning lays on logic, so we need to ﬁnd a way to encode derivation rules
into a logical language. First of all we need to understand how a derivation rule is
made from a structural point of view, in other words we need to detect a clear syntax.
Then, at the syntax is assigned a corresponding semantics and in the end an encoding
into a logical language is performed in order to made the reasoning possible. In [16]
is provided the full methodology used to formalise those rules and their mapping into
OWL.
We provide an example with the graphical notation implemented by NORMA. For
example, the diagram in Fig. 1 does not tell us which are exactly the people in the
entity Citizen and exactly the people in the entity Visitor. We only know that a person
can be a citizen or a visitor, but in the ORM standard notation there are no constraints
able to capture these sets. If we want to use this knowledge we have to use the ORM
Derivation Rules like in Fig. 3.
As we can see, a derivation rule is deﬁned by an asterisk on entities and a text
which deﬁnes the meaning of the rule. We state that all the people which are identiﬁed
by an id card are citizens; all the people which are identiﬁed by a visa are visitors. It

Supporting Conceptual Modelling in ORM by Reasoning
429
Fig. 3. ORM diagram example with derivation rules
Fig. 4. Inferred disjoint and covering constraints
is important to observe that the text is not just a collections of words, instead it is in
controlled-natural language format that is to say it is well deﬁned by a precise syntax.
What can be the outcome of the diagram in Fig. 3? The answers is in Fig. 4. We
obtained a disjunction and covering between the entities Citizen and Visitor. The
disjunction is inferred because there is no chance to ﬁnd a common element between
the entity Visa and IDCard. Since the derivation rules capture separately the two sets,
visitors and citizens, even the corresponding entities have no element in common. What
about the covering? Since Visa and IDCard cover Document and since Person has the
mandatory constraint on the relationship is identiﬁed by, each person must participate
to this relation; in addition to this, the two derivation rules ensure that the sum of the
instances in Citizen and Visitor are exactly those who are in Person. So it is not possible
to ﬁnd an instance which is not in Citizen of in Visitor. To prove this, now we add the
entity Illegal: people without documents, neither a visa nor id card. The outcome of the
reasoning is shown in Fig. 5. Illegal is red because it is an empty set. This means that
the only consistent world is given by the entity Illegal with no instances, because there
are no instances in Illegal which satisfy the rules of the diagram. Again, this is because
Person has the mandatory constraint on the is identiﬁed by relationship and because
the set of Person is already taken by the entities Citizen and Visitor. Therefore, there

430
F. Sportelli
Fig. 5. Illegal is inconsistent
is no way an instance in Illegal could be in Person. The counter-example is trivial: if
we remove the mandatory constraint on Person then Illegal would not be inconsistent
anymore.
6
Conclusion and Future Works
We have seen in this paper an extension of the current ORM formalisation introducing
ORM Derivation Rules. These rules express knowledge that is beyond ORM capabil-
ities, but they are far expressive than standard ORM. Therefore, we have formalised
a non-trivial decidable fragment in order to enable the reasoning over those ORM
diagrams equipped with these rules. The reasoning procedure detects relevant formal
properties as inconsistencies, redundancies or implicit constructs, helping the modeller
to prevent unexpected software behaviours.
We presented UCM Framework, a tool which is speciﬁcally designed to enrich with
reasoning any conceptual modelling software. It supports popular conceptual modelling
languages like ER, UML and even ORM. A tool which makes use of this framework is
ORMiE, a Microsoft Visual Studio plugin used to manage ORM diagrams.
The research and development of UCM Framework continues on two tracks: from
one side we plan to add the explanation feature in order to enhance the understanding of
the diagram by the user perspective, since this service explains why and how something
went wrong during the modelling; while on the other hand we plan to extend the
reasoning even on the instances of the conceptual schema.
Finally, an ongoing theoretical work concerns the formalisation of Fact Type Deriva-
tion Rules in order to capture a relevant decidable fragment that will be implemented
in the UCM Framework.

Supporting Conceptual Modelling in ORM by Reasoning
431
References
1. Halpin, T.A.: Object-role modeling: Principles and beneﬁts. IJISMD 1(1), 33–57
(2010)
2. Halpin, T.: A Logical Analysis of Information Systems: static aspects of the data-
oriented perspective. PhD thesis (July 1989)
3. Curland, M., Halpin, T.: The NORMA software tool for ORM 2. In: Soﬀer, P.,
Proper, E. (eds.) CAiSE Forum 2010. LNBIP, vol. 72, pp. 190–204. Springer,
Heidelberg (2011). doi:10.1007/978-3-642-17722-4 14
4. Sportelli, F.: NORMA: A software for intelligent conceptual modeling. In: Pro-
ceedings of the Joint Ontology Workshops 2016 Episode 2: The French Summer of
Ontology co-located with the 9th International Conference on Formal Ontology in
Information Systems (FOIS 2016), Annecy, France, 6–9 July 2016 (2016)
5. Jarrar, M.: Towards automated reasoning on ORM schemes. In: 26th International
Conference on Conceptual Modeling, ER 2007, pp. 181–197 (2007)
6. Calvanese, D., De Giacomo, G., Lenzerini, M.: Identiﬁcation constraints and
functional dependencies in description logics. In: Proceedings of the Seventeenth
International Joint Conference on Artiﬁcial Intelligence, IJCAI 2001, Seattle,
Washington, USA, 4–10 August 2001, pp. 155–160 (2001)
7. Jarrar, M.: Mapping ORM into the SHOIN/OWL description logic. In: On the
Move to Meaningful Internet Systems 2007: OTM 2007 Workshops, OTM Con-
federated International Workshops and Posters, AWeSOMe, CAMS, OTM Acad-
emy Doctoral Consortium, MONET, OnToContent, ORM, PerSys, PPN, RDDS,
SSWS, and SWWS 2007, Proceedings, Vilamoura, Portugal, 25–30 November 2007,
Part I, pp. 729–741 (2007)
8. Keet, C.M.: Mapping the object-role modeling language ORM2 into description
logic language dlrifd. CoRR, abs/cs/0702089 (2007)
9. Franconi, E., Mosca, A., Solomakhin, D.: The formalization of ORM2 and its
encoding in OWL2. In: International Workshop on Fact-Oriented Modeling (ORM
2012) (2012)
10. Franconi, E., Mosca, A.: Towards a Core ORM2 language (Research Note). In:
Demey, Y.T., Panetto, H. (eds.) OTM 2013. LNCS, vol. 8186, pp. 448–456.
Springer, Heidelberg (2013). doi:10.1007/978-3-642-41033-8 58
11. Artale, A., Franconi, E.: Extending DLR with labelled tuples, projections, func-
tional dependencies and objectiﬁcation. In: Proceedings of the 29th International
Workshop on Description Logics (2016)
12. Franconi, E., Mosca, A., Oriol, X., Rull, G., Teniente, E.: Logic foundations of the
OCL modelling language. In: Ferm´e, E., Leite, J. (eds.) JELIA 2014. LNCS, vol.
8761, pp. 657–664. Springer, Cham (2014). doi:10.1007/978-3-319-11558-0 49
13. Halpin, T.A., Morgan, T.: Information Modeling and Relational Databases, 2nd
edn. Morgan Kaufmann, San Francisco (2008)
14. Fact++ reasoner. http://owl.man.ac.uk/factplusplus/
15. Moreira, J.L.R., Sales, T.P., Guerson, J., Braga, B.F.B., Brasileiro, F., Sobral, V.,
Menthor editor: An ontology-driven conceptual modeling platform. In: Proceed-
ings of the Joint Ontology Workshops 2016 Episode 2: The French Summer of
Ontology co-located with the 9th International Conference on Formal Ontology in
Information Systems (FOIS 2016), Annecy, France, 6–9 July 2016 (2016)
16. Sportelli, F., Franconi, E.: Formalisation of ORM derivation rules and their map-
ping into OWL,” in On the Move to Meaningful Internet Systems: OTM 2016 Con-
ferences - Confederated International Conferences: CoopIS, C&TC, and ODBASE
2016, Proceedings, Rhodes, Greece, 24–28 October 2016, pp. 827–843 (2016)

Author Index
Amini, Sasan
186
Antoniou, Angeliki
353
Asmat, Muhammad Rohan Ali
146
Auer, Sören
267
Bartalesi, Valentina
333
Behrend, Andreas
73, 111
Benedetti, Filippo
333
Bikakis, Antonis
343, 353
Bobrov, Nikita
275
Borzovs, Juris
285
Brandt, Sebastian
161
Brass, Stefan
73
Cagliero, Luca
224
Calvanese, Diego
111
Caroprese, L.
91
Cerquitelli, Tania
111
Chabbouh, Sami
213
Chernishev, George
275
Chiusano, Silvia
111, 224
Chondrogiannis, Theodoros
12
Cortes, Toni
54
Damme, Patrick
37
Darmont, Jérôme
21
Datta, Amitava
213
Domaschka, Jörg
100
El-Tazi, Neamat
235
Endler, Gregor
63
Engels, Christiane
73, 111
Faber, Anne
324
Galkin, Mikhail
267
Garlatti, Serge
363
Garza, Paolo
224
Ge, Mouzhi
12
Gerostathopoulos, Ilias
186
Ghavimi, Behnam
137
Giannakopoulos, George
3
Gončarovs, Pāvels
313
Grabis, Jānis
313
Habich, Dirk
37, 45
Haq, Anam
294
Hartmann, Heinrich
137
Heine, Felix
29
Herbst, Sebastian
63
Hernandez-Mendez, Adrian
324
Ibáñez, Luis-Daniel
174
Ilarri, Sergio
82
Jean, Stéphane
111
Kalaycı, Elem Güzel
161
Kaur, Amardeep
213
Kissinger, Thomas
45
Klavins, Atis
285
Kontopoulos, Ioannis
3
Körner, Martin
137
Kozmina, Natalija
111, 285
Krause, Alexander
37, 45
Krstev, Cvetana
373
Kyriaki-Manessi, Daphne
353
Lange, Christoph
146
Laube, Sylvain
363
Lauer, Tobias
213
Lehner, Wolfgang
37, 45
Lenz, Richard
63
Liu, Danfeng
343
Lundberg, L.
246
Markhoff, Béatrice
111, 384
Matthes, Florian
324
Mayr, Philipp
137
Mazón, Jose-Norberto
174
Meghini, Carlo
333
Metilli, Daniele
333
Mladenović, Miljana
373
Ndefo, Nonyelum
410
Nguyen, Thanh Binh
384

Niang, Cheikh
384
Niedrite, Laila
125, 285
Niedritis, Aivars
125
Novikov, Boris
275
Pérez, María S.
54
Pietrzyk, Johannes
37
Podapati, S.
246
Prehofer, Christian
186
Quemy, Alexandre
302
Queralt, Anna
54
Ricupero, Giuseppe
224
Rohou, Bruno
363
Romero, Oscar
111
Rosander, O.
246
Rudenko, Lena
397
Ryzhikov, Vladislav
161
Saleh, Iman
235
Scandariato, Riccardo
186
Schwab, Peter K.
63
Seybold, Daniel
100
Sidorova, J.
246
Simperl, Elena
174
Skold, L.
246
Solodovnikova, Darja
125, 285
Sportelli, Francesco
422
Staab, Steffen
137
Stanković, Ranka
373
Staron, Miroslaw
186
Straujums, Uldis
285
Taneja, Shailav
199
Teng, Xu
199
Theodorou, Vasileios
186
Touma, Rizkallah
54
Trajcevski, Goce
199
Triantafyllou, Ioannis
353
Truică, Ciprian-Octavian
21
Ungethüm, Annett
37, 45
Urra, Oscar
82
Vahdati, Sahar
111
Vargas-Solar, Genoveva
257
Varlamis, Iraklis
3
Vidal, Maria-Esther
267
Vlachidis, Andreas
343, 353
Wahl, Andreas M.
63
Wilk, Szymon
294
Xiao, Guohui
161
Zakharyaschev, Michael
161
Zumpano, E.
91
Zuters, Janis
285
434
Author Index


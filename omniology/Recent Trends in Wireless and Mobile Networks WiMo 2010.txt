Communications
in Computer and Information Science
84

Abdulkadir Özcan Nabendu Chaki
Dhinaharan Nagamalai (Eds.)
Recent Trends in
Wireless and Mobile
Networks
Second International Conference, WiMo 2010
Ankara, Turkey, June 26-28, 2010
Proceedings
1 3

Volume Editors
Abdulkadir Özcan
Girne American University, Girne, TRNC
E-mail: akadirozcan@mynet.com
Nabendu Chaki
University of Calcutta, India
E-mail: nchaki@gmail.com
Dhinaharan Nagamalai
Wireilla Net Solutions PTY Ltd, Australia
E-mail: dhinthia@yahoo.com
Library of Congress Control Number: 2010929490
CR Subject Classiﬁcation (1998): C.2, H.4, D.2, H.3, D.4, C.2.4
ISSN
1865-0929
ISBN-10
3-642-14170-6 Springer Berlin Heidelberg New York
ISBN-13
978-3-642-14170-6 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
springer.com
© Springer-Verlag Berlin Heidelberg 2010
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
06/3180
5 4 3 2 1 0

 
Preface 
The International Conference on Wireless and Mobile networks (WiMo) aims to bring 
together innovative ideas and new research trends in wireless and mobile networks. 
Wireless networks are the best inventions in history. Wireless networking gives you a 
cheap and easy way to share one Internet connection between multiple computers, 
eliminating the need for more than one modem. You can even add new computers to 
your network simply by plugging in a wireless card and switching them on––they have 
an Internet connection straight away! There aren't many wired networks that can say 
that. This conference is dedicated to addressing the challenges in the areas of wireless 
and mobile networks.  It looks for significant contributions to wireless and mobile 
computing in theoretical and practical aspects. The wireless and mobile computing 
domain emerges from integrating personal computing, networks, communication tech-
nologies, cellular technology and Internet technology. Modern applications are emerg-
ing in the area of mobile ad hoc networks and sensor networks. WiMo 2010 intended 
to cover contributions in both design and analysis in the context of mobile, wireless, 
ad hoc, and sensor networks. The goal of the conference was to bring together re-
searchers and practitioners from academia and industry to focus on advanced wireless 
and mobile computing concepts and establish new collaborations in these areas. 
The main topics include:  
• Architectures, protocols, and algorithms to cope with mobile and wireless  
         networks  
• Distributed algorithms of mobile computing  
• OS and middleware support for mobile computing and networking  
• Routing and communication primitives in ad hoc and sensor networks  
• Synchronization and scheduling issues in mobile and ad hoc networks  
• Resource management in mobile, wireless and ad hoc networks  
• Data management on mobile and wireless computing  
• Integration of wired and wireless networks  
• Broadband access networks  
• Energy saving protocols for ad hoc and sensor networks  
• Complexity analysis of algorithms for mobile environments  
• Information access in wireless networks  
• Algorithms and modeling for tracking and locating mobile users  
• Satellite communications  
• Cryptography, security and privacy of mobile and wireless networks  
• Performance of mobile and wireless networks and systems  
• Mobile ad hoc and sensor networks  
• Wireless multimedia systems  
• Service creation and management environments for mobile/wireless systems  
• Recent trends in mobile and wireless applications 

 
Preface 
VI 
There were 172 submissions to the conference and the Program Committee selected 
38 papers for publication. Following this approach, the book is organized as a collec-
tion of papers from  The Second International Conference on Wireless and Mobile 
Networks (WiMo-2010), The Second International Workshop on Computer Networks 
and Communications (CoNeCo - 2010), The Second International Workshop on Ubiq-
uitous Computing (UbiC-2010), The First International Workshop on Internet Engi-
neering and Web Services (InWeS-2010),  The Second International Workshop on 
Grid Computing (GridCoM - 2010), The First International Workshop on Communi-
cations Security and Information Assurance (CSIA- 2010).  
Finally, we would like to thank the General Chairs, local organizing team and Pro-
gram Committee members and reviewers for arranging and organizing this conference. 
 
 
 
Abdulkadir Özcan 
Nabendu Chaki 
Dhinaharan Nagamalai 
 
 

 
Organization 
General Chairs 
Meghanathan 
Jackson State University, USA  
Publicity Chairs 
Balasubramanian K 
Lefke European University, Cyprus  
Hwangjun Song 
Pohang Univ of Science and Technology,  
South Korea  
Michal Wozniak 
Wroclaw University of Technology, Poland  
Steering Committee  
Selma Boumerdassi 
Cnam/cedric, France 
Chih-Lin Hu 
National Central University, Taiwan 
Dhinaharan Nagamalai 
Wireilla Net Solutions PTY LTD, Australia 
Krzysztof Walkowiak 
Wroclaw University of Technology, Poland  
Atilla Elci 
Eastern Mediterranean University (TRNC), North 
Cyprus  
Abdul Kadhir Ozcan 
The American University, North Cyprus  
Robert C. Hsu 
Chung Hua University, Taiwan 
Sajid Hussain 
Fisk University, Nashville, USA  
Jacques DEMERJIAN 
CS (Communication & Systems) France 
Nabendu Chaki 
University of Calcutta, India 
Program Committee Members  
Jeong-Hyun Park 
Electronics Telecommunication Research  
Institute, South Korea 
Vishal Sharma 
Metanoia Inc, USA  
H.V. Ramakrishnan 
Dr.MGR University, India 
Balasubramanian K 
Lefke European University, Cyprus  
Yeong Deok Kim 
Woosong University, South Korea 
Yun Ji Na 
AICIT, Dongkuk University, South Korea 
A. Arokiasamy 
Eastern Mediterranean University, Cyprus  
Andy Seddon 
Asia Pacific Institute of Information Technology , 
Malaysia  
Balasubramanian K 
Lefke European University, Cyprus  
Balasubramanian Karuppiah 
Dr.MGR University, India  
Bong-Han, Kim 
Chongju University, South Korea  
Cho Han Jin 
Far East University, South Korea  

 
Organization 
VIII 
David W Deeds 
Shingu College, South Korea  
Girija Chetty 
University of Canberra, Australia  
Henrique Joao Lopes Domingos 
University of Lisbon, Portugal  
Jacques Demerjian 
CS, Homeland Security, France  
Jeyanthy N 
VIT University, India  
Jose Enrique Armendariz-Inigo 
Universidad Publica de Navarra, Spain  
Krzysztof Walkowiak 
Wroclaw University of Technology, Poland  
Marco Roccetti 
Universty of Bologna, Italy  
Michal Wozniak 
Wroclaw University of Technology, Poland  
Murugan D 
Manonmaniam Sundaranar University, India  
N. Krishnan 
Manonmaniam Sundaranar University, India  
Phan Cong Vinh 
London South Bank University, United Kingdom  
Yannick Le Moullec 
Aalborg University, Denmark  
John Karamitsos 
University of the Aegean, Samos, Greece 
Khoa N. Le 
Griffith School of Engineering, Gold Coast 
Campus, Australia  
Al-Sakib Khan Pathan 
Kyung Hee University, South Korea  
Lu Yan 
University of Hertfordshire, UK  
Lei SHU 
National University of Ireland, Galway  
Sattar B. Sadkhan 
University of Babylon, Iraq  
Nidaa Abdual Muhsin Abbas 
University of Babylon, Iraq  
Kamalrulnizam Abu Bakar 
Universiti Teknologi Malaysia, Malaysia 
Doina Bein 
The Pennsylvania State University, USA 
Organized By  
ACADEMY & INDUSTRY RESEARCH COLLABORATION CENTER (AIRCC) 
 

Table of Contents
WIMO-2010
Multipath Routing Based on Path Bandwidth in Multi-channel
Wireless Mesh Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Hong-Jong Jeong, Hongseok Yoo, Dongkyun Kim, and Jungsoo Park
The Eﬃciency of RSA Encrypted Video Calls on Mobile Internet
Devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Mert Civriz, Derya Birant, and Alp Kut
E2IRP: A New Energy Eﬃcient Integrated Routing Protocol for
MANET . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Rakesh Kumar Mishra, Sankhayan Choudhury, and Nabendu Chaki
Multi-connection TFRC Video Streaming in a Concatenated Network:
Latency and Video Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
Salah S. Al-Majeed and Martin Fleury
Performance Evaluation of the Number of Database Accesses in
Cellular Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
Mustafa Vahabzadeh Dolama and Akbar Ghaﬀarpour Rahbar
BotSpot: Anonymous and Distributed Malware Detection . . . . . . . . . . . . .
59
P´eter Kenyeres, Attila Szentgy¨orgyi, Tam´as M´esz´aros, and
G´abor Feh´er
Throughput Maximisation of Diﬀerent Signal Shapes Working on
802.16e Mobile Multihop Network Using Novel Cognitive Methods . . . . .
71
Barbaros Preveze and Aysel S¸afak
A Variant of Merkle Signature Scheme to Protect AODV Routing
Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
Satria Mandala, M.A. Ngadi, Abdul Hanan Abdullah, and
Abdul Samad Ismail
Secure Spectrum Sensing and Decision in Cognitive Radio Networks . . . .
99
Seda Demira˘g Ers¨oz, Suzan Bayhan, and Fatih Alag¨oz
DRLC: A New Robust and Dynamically Load Balanced Clustering
Scheme for Wireless Sensor Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
Ismail Tellioglu and Hacı A. Mantar
Clustered De Bruijn Based Multi Layered Architectures for Sensor
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
Anas Abu Taleb, Jimson Mathew, and Dhiraj K. Pradhan

X
Table of Contents
An Adaptive Codec and Frame Size Modiﬁcation Based QoS Algorithm
over Multi-rate WLANs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
M. Fatih T¨uys¨uz and Hacı A. Mantar
Eﬃcient Scheduling of Low Cost Popular Services over a DVB-SH/3G
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
Azza Jedidi and Fr´ed´eric Weis
Semantic Routing for Improved Network Management in the Future
Internet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
John Strassner, Sung-Su Kim, and James Won-Ki Hong
Mirror Routing for Satellite Networks with Cross-Layer Optimization . . .
177
Zhijiang Chang and Georgi Gaydadjiev
Channels Intersection Weight Based Routing in Cognitive Radio
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
Lu Wang and Wei Wu
A Comparison on MANETs’ Service Replication Schemes: Interest
versus Topology Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Mohamed Hamdy, Abdelouahid Derhab, and Birgitta K¨onig-Ries
Applying Vehicular Ad Hoc Networks for Reduced Vehicle Fuel
Consumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
Maazen Alsabaan, Kshirasagar Naik, and Amiya Nayak
An Analytical Model for Dynamic Inter-Operator Resource Sharing in
4G Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
Ahmet Cihat Toker, Fikret Sivrikaya, Nadim El Sayed, and
Sahin Albayrak
Exact BER Performance of Antenna Array-Based Receiver Using
Multi-user Detection in a Multipath Channel . . . . . . . . . . . . . . . . . . . . . . . .
241
Rim Haddad and Ridha Bouallegue
On the Time between Successive Multi-path Discoveries and Hop
Count Per Multi-path for Zone-Disjoint Routing in Mobile Ad Hoc
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
Natarajan Meghanathan
Knowledge, Opportunities and Information Ethics. . . . . . . . . . . . . . . . . . . .
266
Syed Vickar Ahamed and Sevki S. Erdogan
An Optimized MANET Gateway Discovery Based on Fuzzy Logic . . . . . .
273
Antonio J. Yuste, Alicia Trivi˜no, Eduardo Casilari, and
Francisco D. Trujillo

Table of Contents
XI
A Survey on Application of Neural Networks in Energy Conservation
of Wireless Sensor Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
Neda Enami, Reza Askari Moghadam, and Abolfazl Haghighat
CONECO-2010
A Dynamic Distributed Tree Based Tracking Algorithm for Wireless
Sensor Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
Aysegul Alaybeyoglu, Aylin Kantarci, and Kayhan Erciyes
Distributed Weighted Node Shortest Path Routing for Wireless Sensor
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304
Onur Yilmaz and Kayhan Erciyes
Host Based Dynamic Throughput Maximization Model for IEEE
802.11 WLAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
Murat Koyuncu, Mehmet Kazim Gercek, and Tuncay Ercan
Joint Reliable and Power-Eﬃcient CDS-Based Topology Control for
Wireless Multi-hop Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
Elahe S. Hosseini, Mahshid Yassaei, Alireza Ejlali,
Hamid R. Rabiee, and Vahid Esmaeelzadeh
Vertical HandoﬀDecision Schemes for Heteregeneous Wireless
Networks: An Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
Fatma Tansu and Muhammed Salamah
Browser Games: The New Frontier of Social Gaming . . . . . . . . . . . . . . . . .
349
Juha-Matti Vanhatupa
IT Security Assessment for Interdisciplinary Research . . . . . . . . . . . . . . . .
356
Syed M. Rahman, Syed Vickar Ahamed, and
Sevki S. Erdogan
Structure and Communication of Knowledge . . . . . . . . . . . . . . . . . . . . . . . .
367
Michael R. Peterson, Syed Vickar Ahamed, and Sevki S. Erdogan
Existing Recognition Base Usability Features of the Graphical
Password . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
Ali Mohamed Eljetlawi and Noraﬁda Ithnin
CSIA-2010
A Bilinear Pairing Based Hidden-Signature Scheme . . . . . . . . . . . . . . . . . .
389
Mohamed Rasslan and Amr Youssef
An X.509 Based Licensed Digital Signature Framework for Hierarchical
Organizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
Alper Ugur and Ibrahim Sogukpinar

XII
Table of Contents
UBIC-2010
Intelligent Network Applications for Medical Systems . . . . . . . . . . . . . . . . .
409
Syed M. Rahman and Syed Vickar Ahamed
Proposal of an On-demand Software Deployment System Based
on Application Streaming, Virtualization Techniques and P2P
Transport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
Rafael Augusto Teixeira, Marcos Antˆonio Cavenaghi,
Renata Spolon Lobato, and Roberta Spolon
INWES-2010
Enhancing Web Caching Using Web Usage Mining Techniques . . . . . . . . .
425
Samia Saidi and Yahya Slimani
An Eﬃcient Lightweight Authentication Protocol for Mobile Ad Hoc
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
Heshem A. EL Zouka
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445

Multipath Routing Based on Path Bandwidth in
Multi-channel Wireless Mesh Networks
Hong-Jong Jeong1, Hongseok Yoo1, Dongkyun Kim1,⋆, and Jungsoo Park2
1 Kyungpook National University
{hjjeong,hsyoo}@monet.knu.ac.kr
dongkyun@knu.ac.kr
2 Electronics and Telecommunications Research Institute (ETRI)
jspark@etri.re.kr
Abstract. Mesh routers in wireless mesh networks (WMNs) utilize mul-
tiple radio interfaces in order to improve the performance of wireless
network, resulting in transmitting multiple packets simultaneously on
diﬀerent orthogonal channels without interference. For the purpose of
achieving the goal, routing protocols should be designed to improve the
utilization of the network resources when selecting routes between source
and destination nodes. However, single path routing protocols cannot ex-
plore the path providing suﬃcient bandwidth if the source traﬃc requires
more bandwidth than the available bandwidth of the route. In this pa-
per, we therefore propose a novel multipath routing algorithm which can
explore multiple paths with suﬃcient bandwidth and transmit packets
along disjointed paths simultaneously. we also introduce the path met-
rics which can estimate the available bandwidth of the path, considering
the interface utilization information of each node, the channel diversity
of route, and the intra-ﬂow of the traﬃc. Using NS-2 simulations, we
observe that our proposed multi-path routing protocol contributes to
the performance improvements in terms of the end-to-end delay and the
throughput in WMNs.
1
Introduction
Recently, wireless mesh networks (WMNs) have been paid much attention to
provide end users with high-speed and high-bandwidth backhaul access service.
Mesh routers constructing WMNs perform two main functionalities: (a) pro-
viding end-users with the network connectivity as an access network and (b)
building self-organized backbone network in a wide area through the multihop
networking techniques [1]. However, as packet transmissions over multihop wire-
less links suﬀer from channel contention and packet collision problems, overall
network throughput is degraded. In particular, the increasing number of hops
harms the network performance severely. In order to overcome these problems
and increase the network performance, mesh routers are equipped with multi-
ple radio interfaces, so that multiple packets are allowed to be simultaneously
⋆Corresponding author.
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 1–12, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

2
H.-J. Jeong et al.
transmitted on diﬀerent orthogonal channels. In such multi-channel and multi-
interface WMNs, routing protocols have been designed to improve the utilization
of multi-channel and multi-interface resources with the interference level and
channel diversity as their routing metrics
[2]. However, conventional routing
protocols in WMNs generally maintain only a single path between source and
destination nodes. In particular, these routing protocols can neither fully uti-
lize multi-channel/multi-interface resources nor achieve signiﬁcant performance
improvements due to the limited capacity of the single path. As a result, sin-
gle path routing protocols cannot explore the path with suﬃcient bandwidth if
the source traﬃc requires more bandwidth than the available bandwidth of the
route.
Multipath routing which maintains multiple paths for packet transmission
between the source and destination nodes can address the limitation of single
path routing and improve the network performance. Conventional multipath
routing algorithms exploit multiple link-disjoint/node-disjoint paths to provide
fault tolerance, load balancing, and improvement of end-to-end delay [3]. In
order to achieve higher performance through multipath techniques in WMNs,
a multipath routing algorithm should consider not only the redundancy and
disjointness of paths, but also the inherent characteristics of a multi-channel
and multi-interface environment, such as interference and channel diversity, as
in single-path routing protocols.
In this paper, we focus on providing the multiple paths with suﬃcient band-
width to transmit the source-destination traﬃc. We therefore propose an on-
demand multipath routing protocol which estimates available bandwidth of paths
and selects two decoupled paths with suﬃcient bandwidth for the source traﬃc. In
order to estimate the available bandwidth of each path, we introduce novel path
metrics based on the interface utilization information of each node, the channel
diversity of route, and the intra-ﬂow interference of the traﬃc. In order to improve
resource utilization of the network and achieve more performance, each node sep-
arates incoming and outgoing interfaces when forwarding packets.
The rest of this paper is organized as follows. Section 2 discusses existing mul-
tipath routing techniques in WMNs. Section 3 illustrates our proposed routing
scheme in detail. Section 4 presents the performance evaluation of our proposed
scheme. Finally, some concluding remarks are given in Section 5.
2
Related Works
A lot of multipath routing techniques were introduced to achieve fault tolerance,
load balancing, and bandwidth aggregation. They also attempted to reduce delay
and increase end-to-end throughput in mobile ad hoc networks (MANETs) and
WMNs [3]. In MANETs, each node maintains multiple paths in order to cope
with route disconnections and path breaks which frequently occur due to node
mobility and link failures. Generally, node-disjoint paths or link-disjoint paths
were found to improve network performance, but high protocol overhead was
required [4]. In WMNs, it is assumed that network topologies are static and each

Multipath Routing Based on Path Bandwidth in Multi-channel WMNs
3
mesh router utilizes multiple orthogonal channels, unlike MANETs. Multipath
routing schemes in WMNs usually focus on increasing the network performance
with eﬃcient utilization of multiple channel resources [5] [6] [7]. These works are
using their routing metrics which were proposed for the single-path routings,
such as ETT and WCETT [2]. These routing metrics cannot utilize the multi-
channel and multi-interface resources eﬃciently since they do not estimate the
available bandwidth of multiple routes and also do not deﬁne the number of
routes to transmit the source-destination traﬃc. In
[8], a multipath routing
scheme, called Joint Multi-channel and Multi-path control (JMM), was proposed
to increase the end-to-end throughput in multi-channel WMNs. Performance
could be improved by transmitting packets along diﬀerent routes simultaneously
with packet scheduling of multi-channel links. However, this scheme supports
only the single radio interface of each node.
3
Our Proposed Scheme
In this paper, we introduce our proposed path metrics and multipath routing
algorithm for multi-channel and multi-interface WMNs. Our main contributions
are as follows: (a) Two path metrics are proposed for estimating the path cost and
the available capacity of the path based on the interface utilization of each node,
channel diversity, and inter-ﬂow interference. (b) Multiple paths have interface-
disjointness (deﬁned as below) with each other, which can improve overall net-
work performance. (c) In order to improve the utilization of network resource,
each node separates incoming interface and outgoing interface, which allows con-
current packet reception and transmission.
3.1
Assumptions
The schemes to allocate available multiple channels to the interfaces of each node
are classiﬁed into three classes: static, dynamic and hybrid [9]. We assume that
each node is equipped with multiple radios which are tuned to diﬀerent channels
permanently or for a long period of time. For the MAC and PHY protocol,
we use IEEE 802.11a which can provide 12 orthogonal channels. In order to
assign channels to radio interfaces, existing channel assignment schemes can be
used [10].
In this paper, an interface-disjoint path is deﬁned as a path which does not
share any interface of the node with other paths. Traditional multipath protocols
usually deﬁne two types of disjoint paths, namely node-disjoint and link-disjoint.
The node-disjoint and link-disjoint paths require that the diﬀerent paths do not
have any common nodes and links on their paths, respectively. The concept of
node-disjoint path, however, does not work in multi-interface WMNs, since mul-
tiple traﬃc ﬂows can be transmitted simultaneously using its multiple interfaces
even if they share the same node.
In case that the single path can provide suﬃcient bandwidth for the data
traﬃc between the source and destination nodes, our proposed routing protocol

4
H.-J. Jeong et al.
do not ﬁnd additional paths, because of the lack of beneﬁt in the throughput
and end-to-end delay (refer to our experimental result). In order to reduce the
overhead that the multipath routing will have in its route discovery process, we
assume that maximum two paths are found between the source and destination
nodes.
3.2
Link Metrics
One of the key challenges in improving network performance is to avoid the
acquisition of paths having bottleneck links when ﬁnding paths between the
source and destination nodes, since the transmissions over the bottleneck links
can reduce the overall performance. The bottleneck link is deﬁned as the link
having the lowest network resources. In a wireless multi-hop network, both of
a heavily transmitting node and its neighbor nodes suﬀer from lack of wireless
channel resources due to the characteristics of a broadcast medium. As a result,
the link which is in their transmission ranges and directly connected to them is
considered a bottleneck link. Therefore, the link metric should take the inﬂuence
of neighbors’ packet transmissions into account. In order to ﬁnd the bottleneck
links and to estimate the available bandwidth of a link, we introduce two kinds of
link metrics: Link Busy T ime (LBT) and Available Link Bandwidth (ALB).
The LBT of a link l deﬁnes the amount of time which is spent over the link l
in transmitting and receiving packets, including the time during which the link
l is blocked due to other neighbors’ packet transmissions. The ALB of a link
l represents the amount of data rate which can be spent in transmitting and
receiving packets over the link l.
Hence, the LBT of a link l which consists of interfaces u and v can be deﬁned
as
LBTl(u,v) = max(tt(u) + tr(u) + tb(u)
T
, tt(v) + tr(v) + tb(v)
T
)
(1)
where tt, tr, and tb are the times required to transmit a packet, to receive a
packet, and to be blocked during T period of time, respectively. Based on the
usage of a link and the current data rate at PHY layer, we can estimate the avail-
able bandwidth of a link. Speciﬁcally, we can approximate the above-mentioned
ALB of the link l as follows:
ALBl = Rl × (1 −el) × (1 −δ)
(2)
where Rl, el, and δ are the available data rate, the bit error rate over the link
l, and MAC protocol overhead required in message exchanges such as RTS and
CTS, respectively. The available data rate of the link l(u,v), Rl, can be calculated
by minimum available data rate of u and v, min(Ru, Rv), using Equation 3.
Ru =

1 −tt + tr + tb
T

× ru
(3)
where ru is the actual data rate at the PHY layer of u. In order to calculate
the LBT and ALB during the route discovery phase, the available data rate of

Multipath Routing Based on Path Bandwidth in Multi-channel WMNs
5
IF_1 (Ru1 = 8.8)
IF_2 (Ru2 = 37.8)
A
B
IF_3 (Ru3 = 16.2)
IF_1 (Rv1 = 9.9)
IF_2 (Rv2 = 32.4)
IF_3 (Rv3 = 10.8)
r1 = 11 Mbps
r2 = 54 Mbps
r3 = 54 Mbps
ALBl1 = 5.28
ALBl2 = 19.44
ALBl3 = 6.48
Fig. 1. An example of calculated available link bandwidth
each interface is delivered to the neighboring nodes by piggybacking it into the
route request (RREQ) message. With these available data rates of the connected
interfaces, each node can estimate the ALB of each link using the Equation 2.
Figure. 1 shows an example of calculating the ALB where we assume the error
rate of each link is zero and the MAC protocol overhead is 0.4.
3.3
Path Metrics
In this paper, we introduce two path metrics: Accumulated Link Busy T ime
(ALBT) and Available Path Bandwidth (APB). ALBT is determined by a
path cost which consists of the sum of LBTs of each link over a path and the
largest value among sums of LBTs for each channel. ALBT implies the amount
of time that the channel resource is used and it represents the channel diversity
of the path as well. As the traﬃc load of the path and the concentration level of
channel usage of a speciﬁc channel increase, the ALBT of a path also increases.
Hence, the ALBT of a path p is deﬁned as follows:
ALBTp = α ·
n

i=1
LBTi + (1 −α) · max
1≤j≤k
⎧
⎨
⎩

Hop i is on Ch j
LBTi
⎫
⎬
⎭
(4)
where 0 ≤α ≤1, and n and k are the number of nodes over the path and the
number of channels used in the network, respectively.
The APB of a path indicates the available bandwidth which can be spent
for additional packet transmissions. Estimating the available bandwidth of a
path is diﬃcult, because the wireless channel condition keeps changing and it is
sensitive to the co-channel interference. Moreover, the available bandwidth of a
path cannot be higher than that of the bottleneck link over the path. In order to
estimate APB, we therefore consider both of the link capacity of each link over
the path and the interference of an intra-ﬂow traﬃc. Hence, we approximate the
APB of the path p as follows:
APBp = min
1≤l≤n
ALBl
N(l)

(5)
where n and N(l) are the number of nodes over the path and the number of links
which are assigned the same channel as the link l in the interference range of the
link l, respectively. Each link shares the wireless channel resource among links
assigned the same channel in the interference range when forwarding packets

6
H.-J. Jeong et al.
along the route. Hence, as a link has more number of neighbor links with the
same channel of itself, the available bandwidth of the link decreases. The APB
of a path can be interpreted as the available link bandwidth of the path for the
intra-ﬂow interference caused by neighbor links using the same channel.
3.4
Multi-path Routing Algorithm
The main concept of our proposed routing protocol is to ﬁnd interface-disjoin
paths during the route discovery phase in an on-demand manner. In order to
improve utilization of multi-channel and multi-interface resources, our proposed
multipath routing algorithm performs route discovery based on two rules, as
follow:
– Interface disjointness: As described in Section 3.1, interface-disjointness al-
lows multiple paths between source and destination to share the same in-
termediate node without using the same interface. Although the interface-
disjoint paths share the same node, packet transmissions over each path can
be performed independently through the multiple interfaces, because they
use diﬀerent multi-channel interfaces. Moreover, ﬁnding interface-disjoint
paths can increase the number of disjoint routes as compared to ﬁnding
node-disjoint paths.
– Incoming-outgoing interface separation: In order to allow the concurrency
of packet reception and transmission while forwarding the source traﬃc, an
intermediate node has to separate outgoing interface from incoming interface
when ﬁnding a path to the destination. This can reduce the queuing delay
and improve the interface utilization of intermediate nodes on the path.
The route discovery procedure in our proposed scheme conforms to the general
reactive routing protocols, such as AODV and DYMO [11] [12]. In our proposed
routing protocol, when a node has data to transmit to the destination node, it
ﬁnds two interface-disjoint paths to the destination node through the multipath
route discovery phase so that the two paths can have more APB than required
bandwidth of source traﬃc and minimize ALBT. Figure 2 shows an example
of two interface-disjoint paths where each node is equipped with 3 interfaces.
As shown in the ﬁgure, two paths share node A but use diﬀerent interfaces with
each other for the interface disjointness. The multipath route discovery performs
twice of RREQ and RREP exchanges to establish the primary and secondary
paths between source and destination nodes. The RREQ message contains the
path-identiﬁer, LBT and ALB of each link of transmitting nodes in addition to
the basic information required to perform an ordinary routing protocol.
Our protocol introduces a new notion of using path-identiﬁer, called path-ID,
in order to identify multiple paths to the same destination node. General reactive
unicast routing protocols utilize the sequence number and hop-count to manage
the freshness of routes and eliminate routing-loops. Therefore, multiple RREQ
messages with the same sequence number and hop-count are not forwarded at
the intermediate nodes, since they drop such duplicated packets. In order to

Multipath Routing Based on Path Bandwidth in Multi-channel WMNs
7
Fig. 2. An example of multipath routing with 3 interfaces
ﬁnd multiple paths having the same or less hop-count, we add the path-ID
ﬁeld into the RREQ and RREP messages, which can identify the primary and
secondary paths. The routing table also creates the path-ID in its route entry. In
addition, the route entry contains not only outgoing interface, but also incoming
interface for the route decision among multiple route entries to the destination
node.
In the primary path discovery phase, our protocol ﬁrst ﬁnds a path by using
the strategy to satisfy the bandwidth requirement of source traﬃc with mini-
mum ALBT of the path. The primary route discovery is performed by ﬂooding
the RREQ message which includes the path-ID value of PRIMARY-PATH-ID.
Before ﬂooding the RREQ message, the source node creates the RREQ message
and appends the available data rate and the link busy time of each interface for
the link metric calculation at the next-hop node. After receiving the RREQ mes-
sage, a node evaluates the freshness of routing information through the sequence
number, hop count, and path-ID and calculates the LBT and ALB of each link
using the Equations
1 and 2 described in Section 3.2. Before forwarding the
RREQ message to its neighbor nodes, the node creates the RREQ message and
appends the available data rate and the link busy time of each interface except
the one selected as the primary path for the link metric calculation at the next-
hop node. LBT and ALB of the link with its previous-hop are also included
in the RREQ message for the path metric calculation at the destination node.
The ﬂow chart in Figure3 shows a procedure where intermediate nodes handle
an incoming RREQ message.
When the destination node receives the RREQ message, the node collects
several RREQ messages for a period time, which have traveled through the
diﬀerent paths. Based on the path metric described in 3.3, the destination node
selects the primary path with larger APB than the bandwidth required by the
source node as well as with minimum ALBT. If the primary path can provide
the source node with suﬃcient bandwidth, the secondary path discovery will be
omitted. Then, the destination node sends an RREP message to the source node
along the path.

8
H.-J. Jeong et al.
Evaluate the freshness of routing 
information through the sequence number, hop 
count, and path-ID
Calculate ALB and LBT of each link
Piggyback the available data rate and 
link busy time of each interface and the ALB and LBT 
of each link  into the RREQ. 
fresh
stale
Is this node the destination of RREQ?
Yes
Yes
No
Calculate APB and ALBT of the path 
to the source node
Does the sum of the APB of paths 
to the source has larger value than the 
bandwidth requirement of source traffic?
Yes
Yes
No
Send RREP to notify the source node 
that the  paths cannot provide required 
bandwidth
Receive RREQ
Discard RREQ
Forward RREQ
Send RREP to notify the 
source node the APB of path(s)
Select the path with larger APB than 
the bandwidth required by the source node 
as well as with minimum ALBT
Select the path with the largest APB 
Fig. 3. Flow chart for incoming RREQ message at the intermediate node
The secondary path discovery follows the primary path discovery. In the sec-
ondary path discovery phase, the protocol tries to ﬁnd the interface-disjoint
path with the primary path that can maximize the APB of the path. The path-
ID value of the secondary path is assigned SECONDARY-PATH-ID in order
to identify multiple paths to the same destination node. In order to ﬁnd the
interface-disjoint path, an intermediate node will not choose the same interface
which has been selected as the primary path. Except this policy, the secondary
path discovery carries out the same procedure of primary path discovery.
According to the number of radio interfaces of each node, our protocol has
various numbers of multiple paths. For the simplicity, however, we assume that
our proposed routing protocol ﬁnds two paths between the source and destination
nodes.
3.5
Packet Transmission
After establishing multiple paths between source and destination nodes, the
source node need to schedule the packet transmission through the multiple paths
which have diﬀerent APB. In our proposed scheme, the source node selects a path
m to transmit packets with a probability pm according to the APB of the paths.
pm =
APBm
n
i APBi
(6)
where, n is the set of paths to the destination.

Multipath Routing Based on Path Bandwidth in Multi-channel WMNs
9
4
Performance Evaluation
4.1
Simulation Settings
To evaluate performance, we implemented our on-demand multipath routing
protocol based on DYMO protocol in the ns-2 simulator. We tested our mul-
tipath routing protocol over a 3 x 3 grid network topology in 1500mx1500m
terrain. One source-destination pair was selected in the network. Data packets
were transmitted from the source to destination node through multiple paths
constructed with our proposed routing protocol. CBR (constant bit rate) traﬃc
was transmitted with the diﬀerent data rate from 1 Mbps to 5 Mbps. Each node
was equipped with two, three or four IEEE 802.11a radios which were assigned
diﬀerent channels.
In the simulation, we investigated the performance with 5 diﬀerent scenarios:
our proposed multipath routing with 3 and 4 channels, and single-path rout-
ing with 1, 3, and 4 channel(s), denoted by MP3, MP4, SP1, SP3, and SP4,
respectively. The performance metrics were the throughput between source and
destination nodes, the drop rate of interface queue and the average end-to-end
delay. The throughput is deﬁned as the average throughput of the CBR traﬃc
between the source and destination nodes. The drop rate of interface queue is de-
ﬁned as the number of dropped packets among enqueued packets at the sending
queue of each radio.
4.2
Simulation Results
Figure 4 shows the throughput of various scenarios under diﬀerent data sending
rates from the source node (from 1Mbps to 5Mbps). As shown in Figure 4, when
the sending rates goes up, the throughput of each scenario increases at almost
the same rate with diﬀerent highest throughput except SP1. The throughput
of MP4 increases continuously up to almost 4500 kbps. In the MP3, SP3 and
SP4 cases, their throughput increases continuously till about 3000 Kbps, 2000
Kbps, and 2000 Kbps at the sending rates of 3Mbps, 2Mbps, and 2Mbps, re-
spectively. After ending the increment of the throughput, each scenario keeps
their throughput level at the higher sending rates. The end point of increas-
ing represents their available bandwidth of the path(s). Before the data sending
rate arrives at their available bandwidth of the path, each scenario can achieve
almost the same throughput, regardless of the number of paths and channels.
Based on this result, we infer that our multipath routing does not trigger the
additional route discovery if the primary path can provide suﬃcient bandwidth
for the traﬃc.
In order to observe the inﬂuence of diﬀerent numbers of routes, we compare
MP4 with SP4. The throughput of both scenarios increases by almost the same
rate till the 2Mbps sending rate. After the point, only the throughput of MP4
keeps growing up to almost 4500kbps. Finally, at the 5Mbps sending rate, MP4
achieves almost twice times as much as the throughput of SP4. This is because
the traﬃc is separated into diﬀerent paths, which allows to use more network

10
H.-J. Jeong et al.
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 4500
 5000
 1
 2
 3
 4
 5
Throughput [Kbps]
Sending Rate [Mbps]
Our Multipath with 4 channels
Our Multipath with 3 channels
Single-path with 4 channels
Single-path with 3 channels
Single-path with 1 channel
Fig. 4. Comparison of throughput
resources. Between MP3 and MP4, we can ﬁnd the inﬂuence of the diﬀerent
number of channels. MP4 with a large number of channels achieve higher avail-
able bandwidth. However, in the SP3 and SP4 cases, both achieve almost the
same throughput even if SP4 uses more channels since the interface utilization of
each node over the single path arrives at the maximum. Based on these results,
we can conjecture that a larger number of channels and paths can improve the
network capacity. Moreover, we can conclude that if the single-path has suﬃcient
available bandwidth, multiple paths cannot get the beneﬁt of the performance.
Figure 5 shows the average packet drop rates at sending queues of each inter-
face over the path in various scenarios. Each scenario starts dropping packets at
the diﬀerent sending rates. In case of single-path routings, the drop rates of SP1,
SP3 and SP4 start increasing rapidly at the sending rates of 1Mbps, 2Mbps, and
2Mbps, respectively. In case of multipath routings, only the drop rate of MP3
starts increasing at the 3Mbps sending rate. Especially, at the 5Mbps sending
rate, single-path scenarios we have the drop rate of about 50% and 70% in single-
path scenarios, while multipath scenarios have maximum 20%. In particular, the
drop rate in the MP4 case approaches zero. When using a more number of chan-
nels and paths, their packet drop rates become smaller. This is because using a
lager number of paths and channels can improve the network capacity and reduce
the channel contentions. We can ﬁnd that the beginning point of the increment
of the queueing drop is the same point where the growing of the throughput is
stopped. From Figure 5, we observe that the performance of drop rate is tightly
coupled with that of throughput as shown in Figure 4.
Finally, we measured the average end-to-end delay with various scenarios. As
shown in Figure 6, all of scenarios have almost the same end-to-end delay (about
3.5ms) at the 1Mbps sending rate. However, as the sending rate increases, each
of scenarios increases the delay by diﬀerent rates. SP1 has the largest end-to-end
delay (147ms on average), while MP4 achieves the end-to-end delay of 16ms on
average. This is because the single path suﬀers from higher channel contention,
which results in a long queueing delay. In the SP3, SP4, and MP3 cases, 53ms,
56ms, and 40ms are achieved for their average end-to-end delay, respectively.
Multipath routings achieve a shorter delay than single-path routings, because

Multipath Routing Based on Path Bandwidth in Multi-channel WMNs
11
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 1
 2
 3
 4
 5
Dropped Packets at IFQ [packet]
Sending Rate [Mbps]
Our Multipath with 4 channels
Our Multipath with 3 channels
Single-path with 4 channels
Single-path with 3 channels
Single-path with 1 channel
Fig. 5. Comparison of average packet drop rate at IFQ
 0
 50
 100
 150
 200
 1
 2
 3
 4
 5
End-to-End Delay [ms]
Sending Rate [Mbps]
Our Multipath with 4 channels
Our Multipath with 3 channels
Single-path with 4 channels
Single-path with 3 channels
Single-path with 1 channel
Fig. 6. Comparison of end-to-end delay
packet transmissions between source and destination nodes can be dispersed
into two paths. At the 5Mbps sending rate, MP3 has a longer delay than single-
path routings with multiple channels. This is ascribed to the higher packet drop
rates of single-path routings at 5Mbps sending rate. That is, almost 50% of
packets are discarded at the sending queue due to the channel contention on the
single-path.
5
Conclusions
In this paper, we proposed an on-demand multipath routing algorithm which can
explore multiple interface-disjoint paths with suﬃcient bandwidth to deliver the
source-destination traﬃc over multi-channel and multi-interface WMNs. In order
to select the multiple paths, we introduced two path metrics: Accumulated Link
Busy T ime (ALBT) and Available Path Bandwidth(APB) based on the link
utilization of each node, channel diversity, intra-ﬂow interference and available
link bandwidth of a bottleneck link. In route discovery phase, multiple interface-
disjoint paths can be explored by exchanging RREQ and RREP messages with
path-ID. From the ns-2 simulation with the implementation of our multipath

12
H.-J. Jeong et al.
routing protocol, we proved that our multipath routing can explore multiple
paths and achieve higher performance improvements in terms of the network
throughput and end-to-end delay. Especially, the scenario with two paths using
4 channels achieved almost twice as much as the available path bandwidth of
the single-path one in the same condition.
References
1. Akyildiz, I.F., Wang, X., Wang, W.: Wireless mesh networks: a survey. Computer
Networks Journal 47(4), 445–487 (2005)
2. Draves, R., Padhye, J., Zill, B.: Routing in Multi-Radio, Multi-Hop Wireless Mesh
Networks. In: Proc. of MobiCom 2004 (September 2004)
3. Tariquea, M., Tepeb, K.E., Adibic, S., Erfani, S.: Survey of multipath routing
protocols for mobile ad hoc networks 32(6), 1125–1143 (November 2009)
4. Ye, Z., Krishnamurthy, S.V., Tripathi, S.K.: A framework for reliable routing in
mobile ad hoc networks. In: Proc. of IEEE INFOCOM 2003 (April 2003)
5. Tsai, J., Moors, T.: Interference-aware Multipath Selection for Reliable Routing in
Wireless Mesh Networks. In: Proc. of IEEE MASS 2007 (October 2007)
6. Nandiraju, N.S., Nandiraju, D.S., Agrawal, D.P.: Multipath Routing in Wireless
Mesh Networks. In: Proc. of IEEE MASS 2006 (October 2006)
7. Shi, Z., Lin, J., Jiang, X., Huang, L., Yu, B.: Multipath Routing Based Adaptive
Multi-Channel MAC Protocol for Wireless Mesh Networks. In: Proc. of ASID 2008
(August 2008)
8. Tam, W.H., Tseng, Y.C.: Joint Multi-Channel Link Layer and Multi-Path Routing
Design for Wireless Mesh Networks. In: Proc. of IEEE INFOCOM 2007 (May 2007)
9. Skalli, H., Ghosh, S., Das, S.K., Conti, M.: Channel Assignment Strategies for
Multiradio Wireless Mesh Networks: Issues and Solutions. IEEE Communication
Magazine 45(11), 86–95 (2007)
10. Raniwala, A., Gopalan, K., Chiueh, T.: Centralized Channel Assignment and Rout-
ing Algorithms for Multichannel Wireless Mesh Networks. ACM SIGMOBILE Mo-
bile Computing and Communications Review 8(2), 50–65 (2004)
11. Perkins, C., Royer, E., Das, S.: Ad hoc On-Demand Distance Vector (AODV)
routing. RFC 3561, IETF (July 2003)
12. Chakeres, I., Perkins, C.: Dynamic MANET On-demand (DYMO) Routing. IETF
Internet-Draft, draft-ietf-manet-dymo-17.txt (March 2009)

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 13–22, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
The Efficiency of RSA Encrypted Video Calls  
on Mobile Internet Devices  
Mert Civriz, Derya Birant, and Alp Kut 
Department of Computer Engineering, Dokuz Eylul University,  
Tinaztepe Campus, Izmir, Turkey 
mertcivriz@gmail.com, {derya,alp}@cs.deu.edu.tr 
Abstract. Third Generation (3G) cellular network technology allows the trans-
mission of information and voice at higher data rates. 3G security model 
achieves confidentiality and integrity by using KASUMI block cipher algorithm 
as a standard. However, a number of serious weaknesses in the KASUMI cipher 
have been identified. For this reason, users may need other encryption applica-
tions. If phone user wants to be sure that the data transmitted can only be de-
crypted by the intended receiver, the user should use an encryption application. 
This work looks for the answers of the question if it is possible to make effi-
cient and stable encrypted video calls and this paper presents the performance 
results of RSA encryption of a video call on mobile internet devices. To meas-
ure the process, an application is developed which captures live images from 
webcam then encrypts the images with RSA and then sends it over TCP-IP  
protocol to receiver. 
Keywords: RSA, Mobile Internet Devices, Performance Evaluation. 
1   Introduction 
Public-key cryptosystems are very important as they provide confidentiality, authenti-
cation, data integrity and non-repudiation [1,2]. In public key encryption technique, a 
key is split into two keys and they are called as public and private keys. Public key is 
advertised to the unsecure channel and private key is kept secret.  It is not possible to 
generate private key using the public key. So, someone who knows the public  
key cannot decrypt a message after it has been encrypted using the public key. RSA 
algorithm is mainly a public key encryption technique used widely in network  
communications. [3] 
This paper focuses on the performance evaluation of RSA encryption of a video 
call on mobile internet devices. The purpose of the study is to answer the question if it 
is possible to make efficient and stable encrypted video calls. Encrypted video calls 
are necessary to ensure that the data transmitted can only be decrypted by the in-
tended receiver.  
In this study, a client/server application is developed to measure the performance 
of RSA encryption on video calls. The client part of the application has 3 modules: (i) 
webcam image capture module, (ii) RSA image encryption module and (iii) TCP-IP 

14 
M. Civriz, D. Birant, and A. Kut 
 
file transfer module. It captures live images from webcam then encrypts the images 
with RSA and then sends it over TCP-IP protocol to receiver. The server part of the 
application consists of 2 modules: (i) RSA image decryption module and (ii) TCP-IP 
file transfer module. 
The rest of the paper is organized as follows. In Section 2, we review the previous 
RSA algorithm implementations on mobile devices. In Section 3, the description of 
RSA algorithm is given in brief and then basic features and benefits of RSA’s security 
are explained. In Section 4, we present our application and its modules. In Section 5, 
the experimental performance results are presented and discussed. Finally, the conclu-
sions and future works are given in Section 6.   
2   Related Works 
There are several works over this low-power client communication with powerful 
server scenario. RSA algorithm implementations on mobile devices are also frequent. 
[4, 5] 
In 2002; Zhu et al. proposed an efficient authenticated key exchange protocol 
based on RSA [6]. In 2003; Wong, Chan and Zhu shorten the size of message sent 
from the server to the client and from the client to the server, then released a modified 
version of their more efficient key exchange protocol [7]. 
For security purposes, Wu, Garfinkel and Miller created a system based on web au-
thentication with a mobile phone which used as an authentication token [8]. 
In addition, there are some faster implementations of RSA algorithms on mobile 
phones. For performance purposes of RSA algorithm implementation on mobile 
phones, Hwang, Su, and Huang proposed a new method which accomplishes 1024 bit 
encryption at 7.9 milliseconds on 200 MHz clock frequency [9]. 
3G allows the transmission of information and voice at higher data rates and sup-
ports multimedia data applications such as video. KASUMI is a block cipher used in 
mobile communications systems. However, a number of serious weaknesses in the 
KASUMI cipher have been identified [10]. So, users may need to use some encryp-
tion techniques such as RSA to ensure that the data transmitted can only be decrypted 
by the intended receiver. 
3   Description of the Algorithm 
A diagrammatic representation of public key encryption is shown in Figure 1. In this 
case, A needs to send a message to B using a public key encryption algorithm. Key 
generator generates the keys for B and distributes the public key to each person who 
needs to send a message to B. The private key is kept secret and only B has to it. In 
this case, key generator gives the public key to A so that A can send message to B. 
In RSA algorithm decryption is possible only through private key. And there is no 
way which private key is generated using public key. So the message transmitted from 
A to B using RSA encryption is secure even though others know B’s public key. For 
two-way communication between A and B there will be another set of keys for A. [11]. 

 
The Efficiency of RSA Encrypted Video Calls on Mobile Internet Devices 
15 
 
 
Fig. 1. Public key encryption 
RSA’s security relies on finding solutions of three difficult problems: (i) trap door 
function, (ii) factorization and (iii) discrete logarithm. 
3.1   Trap Door Function 
RSA encryption made using the basis of trapdoor function concept. Trapdoor func-
tions can be computed easily in one direction but it believed to be difficult to compute 
in opposite direction (inverse of the function) [12]. For example for a function f(x) = 
y; if it is difficult to find x by using y it is said that f(x) is a trap door function. 
3.2   Factorization 
RSA algorithm takes its force from the difficulty of factoring multiplication of big 
prime numbers. Since the factorization of the multiplication of two large prime num-
bers would be computationally very hard to do, the system that would require the 
factorization will be safe. 
3.3   Discrete Logarithm 
When computing discrete logarithms of a number, it is easy to get a result of particu-
lar exponent, but for a given number, it is very difficult to find the exponent. 
RSA uses block cipher technique which plain text and cipher text are integers be-
tween 0 and n-1 from some n. Encryption and decryption are of form in Equation 1, 
for some plain text M and cipher text C: 
C =  Me mod n 
M = Cd mod n 
(1)
Here n is public modulus. Both sender and receiver must know the value of n. The 
sender knows the value of e and only receiver knows the value of d. Thus, this is a 
public-key encryption algorithm with a public key of KU={e, n} and private key of 
KR={d, n}. For the algorithm to be satisfactory for public-key encryption, the follow-
ing requirements must be met; 

16 
M. Civriz, D. Birant, and A. Kut 
 
─ It is possible to find values of e, d, n such that Med mod n = M mod n for all 
M<n.  
─ It is relatively easy to calculate Me and Cd for all values of M<n.  
─ It is infeasible to determine d given e and n. 
The RSA key generation, encryption and decryption processes are given in Table 1 
and Table 2.  
Table 1. Key generation process 
Select two strong primes[4]  p and q 
random prime p of length n/2 bits ,such 
that gcd(p-1, e) = 1 
Calculate                 n  
n = p x q        
Calculate                 Φ(n)  
Φ(n) = (p-1) x (q-1) 
Select integer  
   e 
gcd(Φ(n),e)=1;  1<e<Φ(n);  e and Φ(n) 
are relative prime 
Calculate 
   d    
d = e-1 mod Φ(n)  
Create public key   KU 
KU = {e,n}  
 
Create private key  KR  
KR = {d,n} 
Table 2. Encryption and decryption process 
Ciphertext     C  
C = Me (mod n) , M < n 
Plaintext        M 
M = Cd (mod n) 
 
The e exponent must be odd, as p – 1 and q – 1 will both have factors of two in 
them. The value of d is such that for any m that does not divide n, will have the prop-
erty that (me)d  is congruent to m mod n; similarly, (md)e  is also congruent to the same 
value. [11] 
A cipher text which is encrypted with a public key of sender can only be decrypted 
with corresponding private key (digital signature). Similarly, a cipher text which is 
encrypted with a private key of sender can only be decrypted with corresponding 
public key (data Integrity, authentication and  non-repudiation). [13] 
4   Materials and Methods 
4.1   Application 
This study consists of two applications based on classic server/client model develop-
ment. Client/server model stands for the relationship between two computer programs 
in which one program, the client, makes a service request from another program or 
makes its duty assigned by the server where the server fulfills the appropriate re-
sponse to request or acknowledges client duties as be done. The client/server model 
sets a standard to interconnect programs that are distributed across different locations 
over a network. So the client/server model has become one of the central ideas of 

 
The Efficiency of RSA Encrypted Video Calls on Mobile Internet Devices 
17 
 
network computing. Most business applications being written today use the cli-
ent/server model.  
In the application, the client starts communication, while the server waits passively 
for and then responds to clients that contact it. The client and server, together, consti-
tute an application. The terms client and server roles are changeable and relies on the 
typical situation in which the server makes a particular capability — for example, a 
database service or a camera server, like developed here — can be available to any 
client that is able to communicate with it. [14] 
The client application which working on Mobile Internet Devices (MID) is set in 3 
modules:  
(i) Webcam image capture module,  
(ii) RSA image encryption module and  
(ii) TCP-IP file transfer module;  
The server application consists of 2 modules:  
(i) RSA image decryption module and  
(ii) TCP-IP file transfer module. 
4.1.1   Webcam Image Capture Module 
This module invokes the API's inside "avicap32.dll" windows library to interact with 
the webcam device and wraps the basic functionality exposed by the library to create 
signatures for the methods inside dll. This module used to capture frames at intervals 
which are adjustable by user. In this study, 20 frames per second (50 milliseconds 
interval) used for testing. 
4.1.2   RSA Image Encryption Module 
This module encrypts the images captured by capture module and sends it to third 
module. This module also has a subroutine which generates a public/private key pair 
and sends to server. The key pair is sent unencrypted as a XML string. Because key 
security is not relevant with the purpose of project and it is not necessary to give fur-
thers effort on this issue but solution is as follows: 
─ Two systems is to have an RSA keypair whose public key is known to both the 
sender and the receiver then when data needs to be sent the receiver generates a 
new RSA keypair, encrypts the public key of that keypair with the common 
public key and sends the encrypted public key to the sender. The sender de-
crypts the receivers public key using its private key (which the receiver does not 
need to know, just as the sender does not need to know the receivers generated 
private key), generates a symmetric encryption key, encrypts the data with the 
symmetric key and then encrypts the symmetric key using the public key re-
ceived from the receiver. Both the encrypted symmetric key and the encrypted 
data are then sent to the receiver which uses the generated private key to decrypt 
the symmetric key and then decrypts the data. 
The encryption process is being done as follows: 
─ Firstly, image is split to blocks as size of keysize / 8 and minus 11 (padding 
bytes). Because of the fact that RSA algorithm doesn’t allow and doesn’t meant 

18 
M. Civriz, D. Birant, and A. Kut 
 
to be to encrypt data more than keysize / 8. Then each block is encrypted and af-
ter that each encrypted block cipher is merged in one byte array again. 
4.1.3   TCP –IP File Transfer Module  
The application uses TCP socket communications for file transfer. UDP is faster but 
not reliable. We can’t allow a single byte to be lost. Because, if we lose a single byte 
on transmission, we lose the entire cipher block. 
On client side this module sends encrypted images over network. On the other 
hand on server side this module receives the encrypted images and sends them to 
decryption module. 
This module uses TCP/IP socket communication to provide interaction between 
client and server. A socket is an abstraction through which an application may send 
and receive data, in much the same way as an open-ﬁle allows an application to read 
and write data to stable storage. A socket allows an application to “plugin” to the 
network and communicates with other applications that are also plugged into the same 
network. Information written to the socket by an application on one machine can be 
read by an application on a different machine, and vice versa. 
4.1.4   RSA Image Decryption Module 
This module decrypts the images received by transfer module. The decryption pro-
gress is processed as an encryption progress. Firstly data is being split to keysize / 8, 
then is being decrypted and merged into one byte array. 
The steps built for measuring performance:  (Figure 2) 
1) 
Server requests client’s key over network; 
2) 
Client sends its key; 
3) 
Server opens a channel for client; 
4) 
Client start webcam capture module; 
5) 
Once an image captured, client sends image to RSA encryption module; 
6) 
RSA encryption module encrypts image and sends resulting byte array to file 
transfer module; 
7) 
This module opens a TCP socket and transmits byte array to server; 
8) 
When server receives the encrypted data it sends data to decryption module; 
9) 
Decryption module divides array into blocks required for decryption then de-
codes the array; 
10) 
After that server converts array to image and shows it in a picture box. 
To measure performance a "time" variable (start time) is stored at 5th step on the 
client side. And after the image shown on the server’s picture box; this time variable 
is compared with current time on server. Finally, results are stored on a text file. 
4.2   Hardware 
Mobile Internet Devices (MID) are  multimedia-capable handheld computer providing 
wireless Internet access. MIDs are larger than smart phones but smaller than the Ultra 
Mobile PC (UMPC). They have been described as filling a niche between smart 
phones and Tablet PCs. MIDs consist of ultra low voltage processors which make the 
RSA encryption on this architecture is a very time-consuming process. 

 
The Efficiency of RSA Encrypted Video Calls on Mobile Internet Devices 
19 
 
 
Fig. 2. System schema 
The configuration of MID used on tests in our study is: 
─ Intel Atom Z500, Single Core, 800MHZ, 512KB L2 Cache, 400 MT/S FSB,   
0.712 - 1.1 V Voltage, 0.65 W Thermal Design Power. 
─ 512MB DDR2 RAM,400Mhz Memory 
─ Windows XP SP3 
The configuration of server machine is: 
─ Intel Core 2 Quad Q9300,2.5GHZ,6MB L2 Cache,1333 MT/S FSB 
─ 4GB DDR2 RAM,800 Mhz Memory 
─ Windows XP SP3 
5   Performance Results 
Two performance evaluation studies were done: firstly, the RSA image encryption 
efficiency on MIDs was measured, and then the same operation was done on a PC. As 
shown in Figure 3 and Figure 4, there is big difference on encryption times when we 
done a test on MID and on an old single core PC. RSA algorithm has exponential 
growth. Because RSA encryption is designed for encrypting data smaller than its key 
size. We can deduct from Figure 3 that with keys bigger than 1280 bit on a MID for 
larger files there will be so much delay to talk. But because of the fact that the testing  
 

20 
M. Civriz, D. Birant, and A. Kut 
 
 
Fig. 3. Encryption times of a 100kb JPEG file encrypted with different key size on test MID 
 
Fig. 4. Encryption times of a 100kb JPEG file encrypted with different key size on a configura-
tion of 2 GHz, 512KB L2 Cache, Single Core CPU and 1GB DDR2 RAM  
MID has only 300 KP camera; the sizes of the images captured don’t go beyond 30 
KB. The delay of transmitting files won’t be that much. In fact, if the file size de-
creases the encryption times of different key sizes converges. Under this circumstance 
transfer time becomes more important. So only encryption time will not be enough 
other measurement types must be added. 
If transfer time added to the measurement criteria so as to measure not only the en-
cryption also the transfer and decryption time results; the results will be more reliable 
as shown in Figure 5. It’s clear that if about 1 second delay is acceptable it is possible 
to make encrypted phone calls with key size up to 1024 bit over WIFI or 3G. It is 
known that key-size lower than 1024 bit is not reliable (even 1024 bit is not). But 
1024 bit RSA encryption is approved as a standard encryption algorithm for most of 
authorities. For the figure with 300KP camera, 1024 bit key size requires 1.3 seconds 
delay. On the other hand, today, there are phones which have up to 5 MP cameras. 

 
The Efficiency of RSA Encrypted Video Calls on Mobile Internet Devices 
21 
 
With that kind of camera even if we think that MID’s will have better configuration in 
future at some point it will be impossible to make encrypted calls with RSA because 
of the fact that the algorithm is very slow. On that case, the video must be encrypted 
with symmetrical keys for faster data rate. Because, RSA is only meant to encrypt 
files smaller than key size it is not efficient to encrypt larger files with this algorithm. 
Using symmetrical keys for encrypting video and asymmetrical keys for sharing the 
symmetrical keys is may be a better solution for applications require faster data rates.  
 
Fig. 5. RSA Encrypted Image Encryption + Transfer Time with corresponding key sizes 
6   Conclusion 
This paper presents the performance evaluation of RSA encryption of a video call on 
mobile internet devices. It answers the question if it is possible to make efficient and 
stable encrypted video calls. A client/server application with several modules was 
developed to measure the performance and the performance results were interpreted.  
In the future, this work may be used for push-to-talk like video communication 
types where performance of the application can be tolerant as it only requires re-
cording video for some time and then encrypting with RSA. 
References 
1. Ding, J., Gower, J.E., Schmidt, D.: Multivariate Public Key Cryptosystems (Advances in 
Information Security). Springer, New York (2006) 
2. Miller, F.P., Vandome, A.F., McBrewster, J.: Public-key Cryptography. Alphascript Pub-
lishing (2009) 
3. Mollin, R.A.: RSA and Public-Key Cryptography. Chapman & Hall, Boca Raton (2002) 
4. Diffie, W., Hellman, M.: New Directions in Cryptography. IEEE Trans. Info. The-
ory 22(6), 644–654 (1976) 

22 
M. Civriz, D. Birant, and A. Kut 
 
5. Lehane, B., Doyle, L., O’Mahony, D.: Shared RSA Key Generation in a Mobile Ad Hoc 
Network. In: Proceedings of IEEE Military Communications Conference, vol. 2, pp. 814–
819 (2003) 
6. Zhu, F., Wong, D.S., Chan, A.H., Ye, R.: Password Authenticated Key Exchange based on 
RSA for Imbalanced Wireless Network. In: Chan, A.H., Gligor, V.D. (eds.) ISC 2002. 
LNCS, vol. 2433, pp. 150–161. Springer, Heidelberg (2002) 
7. Wong, D.S., Chan, A.H., Zhu, F.: More Efficient Password Authenticated Key Exchange 
Based on RSA. In: Johansson, T., Maitra, S. (eds.) INDOCRYPT 2003. LNCS, vol. 2904, 
pp. 241–264. Springer, Heidelberg (2003) 
8. Wu, M., Garfinkel, S., Miller, R.: Secure Web Authentication with Mobile Phones. In: 
DIMACS Workshop on Usable Privacy and Security Systems (2004) 
9. Hwang, R.J., Su, F.-F., Huang, L.-S.: Fast Firmware Implementation of RSA-Like Secu-
rity Protocol for Mobile Devices. Wireless Personal Communications 42(2), 213–223 
(2007) 
10. Balderas, T., Cumplido, R.: An Efficient Hardware Implementation of the KASUMI Block 
Cipher for Third Generation Cellular Networks. In: Proceedings of the global signal proc-
essing conference, GSPx 2004, Santa Clara, CA (2004) 
11. Rivest, R.L., Shamir, A., Adleman, L.: A Method for Obtaining Digital Signatures and 
Public-Key Cryptosystems. Communications of the ACM 21(2), 120–126 (1978) 
12. Crandall, R., Pomerance, C.: Prime Numbers: A computational Perspective, 2nd edn., 
ch. 5. Springer, Heidelberg (2005) 
13. Paar, C., Pelzl, J.: Understanding Cryptography. In: The RSA Cryptosystem, ch. 7. 
Springer, Heidelberg (2010) 
14. Makofske, D.B., Donahoo, M.J., Calvert, K.L.: TCP /IP Sockets in C#. Morgan Kauf-
mann, San Francisco (2004) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 23–33, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
E2IRP: A New Energy Efficient Integrated Routing 
Protocol for MANET 
Rakesh Kumar Mishra1, Sankhayan Choudhury2, and Nabendu Chaki3  
1 Feroze Gandhi Institute of Engineering and Technology, Rae Bareli, Uttar Pradesh, India 
2,3 Department of Computer Science & Engineering, University of Calcutta, India 
rakesh.mishra.rbl@gmail.com, sankhayan@gmail.com,  
nabendu@ieee.org 
Abstract. Educational and Business MANETs are a different class of networks 
that are instantiated for the cause and get abandoned as the task is over. Such 
for-the-purpose networks have a limited span and may occasionally be interfac-
ing with any surrounding networks. The proposed protocol, Energy Efficient In-
tegrated Routing Protocol (E2IRP), is a layer 2 enhancement that suggests an 
upgradation to IEEE 802.11 to support service oriented routing for minimizing 
the energy consumption over the network. Enhancing layer 2 for these tasks re-
lieves higher layers to implement other features like security and application 
specific control structures. The protocol can perform Unicasting, Multicasting 
and Broadcasting in a seamlessly integrated fashion. E2IRP does not require any 
delivery infrastructure and has provision for non-participating nodes to sleep. 
Thus the protocol is expected to perform well in terms of bandwidth utilization 
and energy efficiency for a low set-up time. 
Keywords: Energy Efficiency, 802.11 MAC, Service Discovery, Routing,  
protocol, Service oriented routing. 
1   Introduction 
MANET paradigm has gained momentum with applications ranging on different facets 
of modern society. A for-the-purpose MANET is a wireless network where no infra-
structure is assumed to implement the network functions and no authority is in-charge 
of managing and controlling the network. Such network may prove to be useful in 
automation at exhibitions, offices and conference venues without any investment to 
infrastructure. These networks are not designed with any specific application, but need 
to support any legacy TCP/IP application [1]. Physical standard supporting MANET 
provides an effective solution by dynamically interconnecting devices in a short range 
and forming single-hop network (Bluetooth, Wifi). In multi-hop ad hoc network par-
ticipating nodes communicate directly to neighbor through single-hop wireless com-
munication standards [1] or by relaying through intermediate nodes to one or more 
hosts and recipients are identified by a single destination address or any other address-
ing scheme [5]. Such for-the-purpose MANETs are characterized by relatively short 
duration, low inter-network communication, small network span and mobility but 
stress for least network setup time and power efficiency.  

24 
R.K. Mishra, S. Choudhury, and N. Chaki 
 
The other concerning issues on MANET that is relevant to this paper include rout-
ing [3, 4], addressing besides service and resource discovery [6]. Presently, these are 
implemented at layer 3 and above. This results in more computation and communica-
tion overhead, leading to wastage of bandwidth and power. The optimization can be 
achieved through minimizing the computational load as well as network interface 
activity.  
Provisioning of routing, service management and topology assessment at the lower 
layer in such for-the-purpose networks will relieve the upper layers to concentrate 
towards the QoS and Security issues. The proposed protocol is an extension of the 
patent document [2] submitted earlier this year. Designing a integrated routing proto-
col that can support all forms of transmissions like unicasting, multicasting and 
broadcasting is our prime objective. The paper has been segmented as Section 2 Re-
lated Work, Section 3 discusses the motivation of our work and Section 4 discussing 
functioning of 802.11 DCF, Section 5 will be elaborating Energy Efficient Integrated 
Routing Protocol and Section 6 discusses the benefit of E2IRP. 
2   Related Works 
Si and Li in [12] have proposed a MAC layer solution which seamlessly includes all 
the three mode of communication: Unicasting, Multicasting and Broadcasting. The 
have proposed to use variable size MAC frame that contains one or more destination 
MAC addresses. The frames are broadcast and received by all the neighbors for 
communication. Neighbours enlisted within the frame will buffer the succeeding data 
frame while others will drop. This has been implemented with dual tone system. A 
tone is a short preamble less signal. Receiver Busy Tone (RBT) is raised by the des-
tined neighbours after the receiving MRTS and exists till data frame is received. Ac-
knowledgement Busy Tone (ABT) is transmitted after the receipt of data frame to 
acknowledge the receipt. This protocol is capable of providing both reliable and unre-
liable communication and depends for higher layer to inform about service. However, 
MRTS uses variable size frame and thus have varying transmission time. Besides, all 
the nodes have to be participative during the entire session of communication and 
hence lot of energy is wasted. It deploys multi-channel communication system.  
Gupta et. al. in [13] has devised a multi-tone based protocol for improving reliabil-
ity at MAC layer communication. It uses three kinds of tones: Busy, NCTS and 
NACK. When a RTS is not received by any node it raises NCTS tone at signaling 
channel. In case NCTS is absent both sender and receiver raise their busy tone over 
signaling channel, informing adjoining nodes about the transmission. NACK tone is 
generated when a node receives corrupted data frame. This protocol improves the 
reliability but demands multi-channel communication. Multicasting is become possi-
ble by the support from higher layers. 
Tang and Gerla [14][15] have proposed a MAC protocol to support both multicast-
ing and broadcasting communication. Initially, RTS and CTS handshake is per-
formed. After sending the RTS, the sender waits for WAIT_FOR_CTS time interval 
to receive all CTS. A node, either listening another transmission or not interested in 
transmission pushes itself to YIELD phase. In [15], the receiver has Direct Sequence 
capture capability. It avoids the CTS frame collision. In [17] provision of NACK is 

 
E2IRP: A New Energy Efficient Integrated Routing Protocol for MANET 
25 
 
made so that a receiver if couldn’t listen to data frame transmission, inform the prob-
lem. To accommodate NACK, special time slot is defined after data frame is transmit-
ted but could not assure the CTS frame collision. In both of the works [15, 17], the 
sender relies on control frame for assuming the successful delivery and could not 
assess the delivery status.  
In BMW [16], the broadcasting is achieved by recursively unicasting the data 
frame to enlisted neighbours. Thus with n neighbours, a sender has to go into conten-
tion phase n times. To implement the process, each node has to maintain a neighbour 
list which is provided by higher layers. CTS RTS and ACK are suitably modified, to 
reflect the concerned frame number. The transmission is completed when all the 
nodes in neighbour list are communicated. This procedure is reliable but not efficient 
in terms of bandwidth consumption.  
Min et. al., made a novel proposal where RTS/CTS control frame is individually 
generated for each recipient and single data frame is transmitted [17]. The receiving 
node does not reply the ACK for the data frame until RACK (Request-for-ACK) is 
generated by the sender for individual recipient. RACK is used to serialize the ACK 
frames. This process has resolved the collision of CTS / ACK frames. However, a 
long trail of RTS/CTS exchange is required before data is actually delivered. Each 
node keep on sensing the media for its possible turn and hence may be remaining 
active. Jain and Samir in their proposal [18] have mentioned a protocol wherein the 
RTS signal is modified to accommodate the addresses of 4 consecutive next hop re-
cipients of the packet.  The relative positioning of addresses define the priority of 
nodes. The CTS frame is modified to contain the address of receiver of RTS frame. 
CTS frame are responded in sequence defined in RTS frame and their log is main-
tained at sender station. Similarly, after DATA frame transmission designated nodes 
reply with modified ACK, these are also logged at sender. In this approach, one has to 
rely on layer 3 routing protocol for obtaining the neighbourhood list. In case of multi-
ple transmissions from the same source to same receiver it is difficult to distinguish 
among the CTS.  
3   Scope of the Work 
There are a couple of major motivations behind this work. The first of these is to 
design a routing algorithm that incorporates service identification provisioned at layer 
2. Let's explain the purpose. 
In for-the-purpose network nodes are primarily involved in intra-network commu-
nication and usually do not communicate to external network. Moreover, these will be 
limited span networks with restricted mobility. Hence, a routing protocol functioning 
over MAC address will eventually be more productive than conventional routing 
protocol due to reduction in layer 3 packet formation. 
Functionally, everything over a network is a service which is either extended to a 
user or counterpart nodes in a network. Service and Resource discovery is primary 
concern for efficient operation within a network. Several service discovery models 
have been proposed over the years namely GSD, Allia, Lanes, DSDP, GCLP [10]. 
These are mostly deployed above layer 3. All of these have a common approach that, 
with each call of service by a client a minimum cost route is discovered from client to 

26 
R.K. Mishra, S. Choudhury, and N. Chaki 
 
the available server. Challenges that a MANET needs to address for service manage-
ment includes dynamic service discovery and association, minimization of communi-
cation overhead, ensuring service quality and predictability and compatibility across 
all platforms [8]. Again, service discovery may be complemented by route discovery. 
Thus combining together the two processes will be more productive and same is rec-
ommended in [7], [9] and [10]. We conclude that a routing algorithm, incorporating 
service identification provisioned at layer 2, may be more productive as compared to 
its counterparts at layer 3. 
 
NI 
S1 
N2 
N5
N6
S2
N7
N3
N4
 
Fig. 1. An Ad-hoc Network 
Table 1. 
Service Code 
Class 
Source
Destination 
1 
Multicast
S1 
N2, N3, N7, S2 
2 
Unicast 
S2 
N1 
 
The other motivation is to attain improved bandwidth utilization. The physical 
layer in MANETs is broadcasting in nature and every transmission is equally  
accessible to all neighbouring nodes. 802.11 do not support acknowledgement of 
communication beyond unicasting. Multicast Advantage technique improves energy 
and bandwidth usage by increasing the transmission power that leads to increase in 
accessibility of transmission to a larger set of recipients [5]. In this technique single 
transmission is used for ‘n’ number of receivers with no acknowledgements. If a rout-
ing protocol be designed with Multicast Advantage with acknowledgement will im-
proves the energy and bandwidth utilization. 
We propose a routing strategy for the for-the-purpose networks that includes Uni-
casting, Multicasting and Broadcasting in an integrated way. The proposed E2IRP 
(Energy Efficient Integrated Routing Protocol) is a routing protocol that incorporates 
Service Oriented Routing by amalgamating the service and routing together. The 
protocol recommends an up gradation in existing IEEE 802.11 MAC protocol to sup-
port reliability and efficient bandwidth utilization. 

 
E2IRP: A New Energy Efficient Integrated Routing Protocol for MANET 
27 
 
4   The IEEE 802.11 DCF – An overview  
The IEEE 802.11 [10] is the only standard which is designed to support both infra-
structure and ad hoc wireless networks. 802.11 define Distributed coordination Func-
tion (DCF) based on a distributed, contention based carrier sensing with collision 
avoidance (CSMA/CA) MAC protocol. Control frames of 802.11 has duration field 
which is being used to set the Network Access Vector (NAV) within the node to iden-
tify the time period after which these have to enter into contention phase. If a channel 
is found to be idle for DIFS (DCF Inter-Frame Space) the sender node raises RTS 
(Request-To-Send). Receiver waits for the SIFS (Small Inter-Frame Space) duration 
and after which receiver sends CTS (Clear-To-Send). Subsequently DATA frame is 
sent and it is replied by Acknowledgement frame both are distanced at SIFS duration. 
This protocol enables peer-to-peer unicast communication between one-hop 
neighbours.  
Let us consider the topology depicted in fig 1. Within the network nodes desig-
nated as Si, where i ∈[1,2] are the sources for respective services and Ni where i ∈[1-
7] are intermediate routers or destined nodes. A source node may also be a recipient 
node or intermediate router. We have assumed the following service distribution 
model (Table 1). 
In above paradigm N4 and N6 are not interested in any of the communications and a 
network has to support both unicasting and multicasting in a seamlessly integrated 
fashion.  
According to IEEE 802.11 DCF, unicasting will be performed by sending data 
frame across any of the path, decided by higher layer. Let selected path to N1 be <S2, 
N4, N2, N1> then transmission begin with S2 raising RTS (Request-to-send) for N4 
after Short Inter Frame Space (SIFS) N4 send CTS  (Clear-to-Send) addressed to S2. 
After SIFS time Data frame is transmitted from S2 and subsequently, being acknowl-
edged by N4. This phenomenon of RTS-CTS-DATA-ACK propagates from N4 to N2 
and from N2 to N1. 
Multicast service will be accomplished by recursively unicasting the frame across 
several paths. Service 1 starts with S1 raising RTS (Request-to-send) signal for N2, 
correspondingly N2 will send CTS (Clear-to-Send) addressed to S1. On receiving CTS 
S1 sends Data frame and N2 acknowledges the same with ACK frame.  Then RTS is 
generated for N3 through N5 and S2 and so on. 
During the entire process all neighbours keep on updating their NAV by recording 
the duration field value of individual frame. This action cost in unnecessary power 
wastage in non-participating nodes. IEEE 802.11 do not have any provision of multi-
casting and broadcasting data transmissions.  
MANETs in business and educational origin always need communication setup for 
instantaneous collaboration and usually for short durations. As IEEE 802.11 does not 
define the delivery infrastructure, a routing protocol over IEEE 802.11 has to define 
infrastructure before the start of transmission. Hence, it has a larger set up time. IEEE 
802.11 does not include the concept of service at its core hence found to be incapable 
to provide full functional support its higher layer.  

28 
R.K. Mishra, S. Choudhury, and N. Chaki 
 
5   The Proposed E2IRP Routing Protocol  
Energy Efficient Integrated Routing Protocol (E2IRP) is designed for “for-the-
purpose” MANETs with the objective of providing energy efficiency and service 
oriented routing strategy. The basic idea is to drop node address from a frame rather 
frame should identify the service to which it belongs. The decision to participate in 
communication is governed by the interest of node in the service. If service within a 
transmission is relevant to node becomes responsive else it will creep into sleep mode 
to conserve energy. This way E2IRP at Layer 2 itself filters out the prospect recipient 
and improves routing decisions and network longevity.  
A service is a basic routing decision entity. A service is known over the network 
through its distinguished codes. The same service may be provided by more than one 
node but with different service code. A node can initialize a service but can become 
persistent only when there exists at least one request for it. If a transmission is not 
cleared by neighbours imply that either service infrastructure is partitioned or service 
is no more required. All the services with no subscriber will be terminated subse-
quently. All the nodes must be aware about the service codes and the code table will 
be refreshed periodically. 
Initially E2IRP comes up with minimum set of services referred as Network Main-
tenance and Operation Services. During the startup phase of network nodes will be 
performing the neighbourhood assessment and Service table initialization.  
E2IRP includes the routing decisions based upon service solicitation and availabil-
ity. In this protocol functional parameters like Energy Efficiency and Service Man-
agement are included at layer 2 routing decisions for optimal performance. Protocol 
minimizes network setup time by enabling the routing as soon as service and corre-
sponding request is initiated. 
E2IRP modifies IEEE 802.11 RTS control frame to IRTS (Integrated RTS) frame 
(Table 2) which besides the usual content also carries the service code and frame 
sequence. 
Table 2. 
IRTS <RTS Frame> + {service code, Frame Seq} 
ICTS <CTS Frame> + {own_mac_id, Frame Seq} 
IACK <ACK Frame> + {own_mac_id, Frame Seq} 
 
Service code enables service oriented routing and frame sequence will used to 
avoid routing loops. For each IRTS one or more ICTS (Integrated CTS) will be re-
plied by the interested neighbours. Each ICTS conforms to neighbourhood and ser-
vice distribution list. The IRTS signal tracked by non-participating neighbours to set 
their respective NAV as reflected in IRTS minus TACK (time to transmit ACK frame). 
For duration computed as above the non-participating nodes goes to sleep mode thus 
conserving their energy. All the nodes which have sent the ICTS will remain active 
during the transmission session. Sender after collecting the ICTS from neighbours 

 
E2IRP: A New Energy Efficient Integrated Routing Protocol for MANET 
29 
 
responds with queued frame that will be heard by all participating nodes and cached 
in their respective buffer. Each recipient sent IACK (Integrated ACK) to the sender 
confirming the received fames of service. 
Both ICTS and IACK plays very essential role. The presence of node MAC_Id 
within the frame helps in establishing the neighbourhood otherwise a periodical spe-
cial beaconing has to be performed. Presence of Frame Sequence number in ICTS 
will indicate to sender the expected sequence number by neighbour. On the other 
hand the presence of sequence number in IACK refers to acknowledgement for frame 
of a specified service code indicated by IRTS. This approach will help the participat-
ing node to differentiate between current and retransmitted frames and adopt suitable 
strategy for power saving. 
E2IRP is a six phase communication phenomenon which proceeds as 
1. Contention Phase – Node sense media for acquisition for specific time in-
terval if found idle modified RTS (IRTS) signal is raised. 
2. All the nodes listening IRTS do following as per their role 
a. 
Participating Node:  Send out  modified CTS (ICTS) 
b. Non Participating Node: Update their NAV and go to sleep mode 
3. Sender Station node wait for appropriate time or check that channel idle 
state for a longer duration and act accordingly 
a. 
Channel Idle or duration is expired: Sends Data Frame 
b. Otherwise: Track for ICTS from neighbouring nodes. 
4. After predefined time period modified ACK (IACK) are generated by re-
ceiving station. 
5. Sender receives the IACK and conform it to ICTS received list to define 
path reliability. 
6. Sleep node awake and step 1 resumes for next transmission. 
We assume the same topology as designed in fig 1. S1 commences the service 1 by 
raising the IRTS frame. Since, N1 is a non participating node thus it will prefer to go 
into sleep with its NAV set to time t1= (TIRTS + SIFS + TICTS(N5) + TICTS(N2) +  α * SIFS 
+ TDATA+ TIACK(N5) + TIACK(N2)) - β * TIACK where α, β are the suitable constants. α is 
multiplied to SIFS so that collided ICTS frame can reappear for transmission, simi-
larly  β is a fractional value of TIACK at which we recommend a node to revive and 
sense the media. 
N5 and N2 will sent their respective ICTS signals to sender S1 giving clear channel 
intimation. After waiting for  α * SIFS time interval sender transmits Data frame. As 
frame is not transmitted for specific node hence will be listened and cached by every 
live node in vicinity. After receiving the Data each participating node reply with IACK. 
At N5 the process is again repeated wherein the data frame is delivered to S2 and 
N7. As N3 is visible from both S2 and N4 hence both can transmit Data frame to N3. 
Service 2 following the same procedure will deliver to N5 and N4. In the respective 
turn, N4 and N5 will be sending same service id and frame sequence number. As both 
have the frame in buffer hence are not interested in transmission from each other. N4 
is responded by N2 only and N5 is responded by S1. Let us assume S1 got hold of the 

30 
R.K. Mishra, S. Choudhury, and N. Chaki 
 
channel prior to N2 and data is delivered to destination node. On its turn N2 will not be 
responded by either neighbours and thus transmission is abandoned.  
Although our protocol do not require the topology assessment but to augment the 
needs of future layer 3 solutions topology assessment is included as service under 
Network Maintenance and Operation services. IRTS frame with service code belong-
ing to this subset will cause neighbours to reply either with ICTS frame or a DATA 
frame. ICTS frame help a node in assessing its 1-hop neighbours while DATA frame 
either contain list of I-hop neighbours or Services existed at 1-hop neighbourhood.  
Say, if IRTS is generated by S1 this will be responded by ICTS frame from N1, N2 
and N5. Tracking these ICTS frames S1 formulates its 1-hop neighbourhood directory. 
Similarly, IRTS frame with another service code is responded by adjacent nodes with 
their respective neighbourhood directory. Thus S1, can identify other node nodes 
connected to N5 viz, N4, N7 and S2 thus identifies its 2-hop neighbours. Similarly, 
service distribution over two hop neighbourhood can be estimated. 
6   Comparison between IEEE 802.11 and E2IRP 
Again refer to table – 1, there are two services namely 1 and 2 having S1 and S2 re-
spectively as their sources. Service 1 is multicast service from S1 to N2, N3, N7, S1, 
while service 2 is a unicast service from S2 to N1. Multicast from the perspective of 
802.11 link layer is actually a recursive unicast service. 
a) Traffic generated in IEEE 802.11 standard is computed on the bsis of 
number of edges traversed by the frame from source to destination 
through a cost effective path. For each edge traversed by a message frame 
there will be frames transmitted namely RTS, CTS, DATA and ACK. 
 
Service 
Source 
Destination 
Path 
Edges 
Frame 
count 
2 
S2 
N1 
<S2, 
N4 
N2, N1> 
3 
12 
<S1 N2> 
1 
4 
<S1, N5, 
S2, N3> 
3 
12 
<S1, 
N5,N7> 
2 
8 
1 
S1 
<N2, N3, 
N7, S2> 
<S1, N5, 
S2> 
2 
8 
Total Frames originated 
44 
 
Thus, to accommodate both the services under the assumption that higher 
layer traffic is ignorable, total 44 frames transmission is required. In mul-
ticast session S1 has to carry out same transmission four times through 
each path. The assumption although is not fair to be considered but  
keep the computations simple this is being done. Further, it may be worth 

 
E2IRP: A New Energy Efficient Integrated Routing Protocol for MANET 
31 
 
mentioning that IEEE 802.11 depends upon the higher layers for discov-
ery of paths from source to destination. 
b) E2IRP is a protocol which exploits the broadcasting characteristic of the 
transmission media. Receiver is not selected by the sender rather a node 
interested into the services extended by the sender, may be it is intermedi-
ate node to another node at next hop or it may itself is a consumer, keeps 
itself active and receives the transmission while other will creep into sleep 
mode. 
Let us assume that N1 has intimated S1, N2 of its requirement while S2 
intimated N5, N2 that service 2 is available. Same is applicable with ser-
vice 2 also. The routing process is elaborated as: 
 
Service Source Destination Transmission
Frames 
(RTS+n(CTS)+DATA+n(IACK) 
S2-> N5, N4 
1+2+1+2 = 6 
N5 -> S1 
1+1+1+1 = 4 
N4-> N2 
1+1+1+1 = 4 
2 
S2 
N1 
S1 -> N1 
1+1+1+1 = 4 
S1 -> N1, N2, 
N5 
1+3+1+3 = 8 
N5 -> N4, S2, 
N7 
1+3+3+1 = 8 
1 
S1 
N2, N3, N7, 
S2 
N4 ->N3 
1+1+1+1 = 4 
 
Intermediate nodes, in some instances, in the neighbourhood will not be 
listening to the transmission because either those already have the frame 
or have learnt that their interested neighbour has the data in previous 
transmission. 
The important characteristic for the process is that multicasting is not a 
recursive unicasting in E2IRP it is a monotonically increasing process. 
The entire transmission process of both services 1 and 2 in E2IRP requires 
38 frames in comparison to 44 frames in the IEEE 802.11 standard which 
is under non-realist assumption. E2IRP at MAC layer do not require any 
support from the higher layers to route the packet where in 802.11 it is 
mandatory.  
7   Conclusions 
E2IRP is an Energy Efficient Integrated Routing Protocol which has the characteris-
tics like low setup time, Power efficiency, and Bandwidth optimization. It merges the 
concept of Service with Routing to provide a seamless integration of all formats of 
data communication viz. unicasting, multicasting and broadcasting without any addi-
tional overhead. 

32 
R.K. Mishra, S. Choudhury, and N. Chaki 
 
Least Setup Time is one of major characteristics of the protocol. This characteristic 
allows quick deployment of the services over the network. No join and leave is re-
quired for the participating nodes as in layer 3 multicasting hence limited control 
information is generated. Topology assessment is included within basic signaling with 
no extra flow over the network. No addition logical infrastructure is required to be 
setup to enable unicasting, multicasting and broadcasting communication. This also 
contributes to small setup time. Seamless Integration of different mode of communi-
cation within the same protocol structure is also supported in E2IRP. Same procedure 
is implemented for unicasting, multicasting and broadcasting services. Mesh is always 
available as basic distribution infrastructure which can accommodate all form of 
communication with high reliability. 
Power control is prime consideration in MANET. This has direct influence over 
the longevity of network. In E2IRP the approach adopted ensures energy efficiency by 
reducing the number of frames required to enable communication. Route discovery is 
not required frequently; same is achieved through the communication control frames 
(ICTS, IACK). A node by tracking these frames can assess the 1-hop topology. NAV 
will be updated only once using IRTS frame. Afterward nodes goes to sleep and will 
be activating while last IACK is transmitted. The protocol does not require establish-
ing any specific infrastructure like tree, mesh, core etc. 
In E2IRP the control information flow will be reduced by unifying service discov-
ery with routing and ensures better bandwidth utilization. Earlier these were managed 
by the layer 3 and above. E2IRP through its basic design ensures the assessment of 
topology up to second hop thus provides effective foundation for deployment of Net-
work Coding [11] for MANET. In conclusion, in E2IRP, a new protocol has been 
proposed with better bandwidth utilization, higher energy efficiency and lower set-up 
time.     
References 
1. Conti, M., Giordano, S.: Multihop Ad Hoc Networking: The Theory. IEEE Communica-
tions Magazine, 78–86 (April 2007) 
2. Choudhury, S., Mishra, R.K., Chaki, N.: Energy Efficient Integrated Routing Protocol 
(E2IRP). Proposal submitted with US Patent Site, ISA IN-800283 
3. Deb, D., Roy, S.B., Chaki, N.: LACBER: A New Location Aided Routing Protocol for 
GPS Scarce MANET. International Journal of Wireless & Mobile Networks 
(IJWMN) 1(2), 22–35 (2009) 
4. Saha, S., Chaki, R., Chaki, N.: A New Reactive Secure Routing Protocol for Mobile Ad-
Hoc Networks. In: 7th IEEE International Conference on Computer Information Systems 
and Industrial Management Applications (CISIM), Ostrava, The Czech Republic, June 26 - 
June 28 (2008) ISBN: 978-0-7695-3184-7 
5. Wieselthier, J.E., Nguyen, G.D., Ephremides, A.: Algorithms for energy-efficient multi-
casting in static ad hoc wireless networks. Mobile Networks and Applications archive 6(3), 
251–263 (2001) 
6. Hoebeke, J., Moerman, I., Deoedt, B., Demeester, P.: Overview of Mobile Ad Hoc Net-
work: Application & Challenges,  
 http://wwww-di.inf.puc-rio.br/~endler/courses/Mobile/papers/ 
 Manet-Challenges.pdf 

 
E2IRP: A New Energy Efficient Integrated Routing Protocol for MANET 
33 
 
7. Sailhan, F., Issarny, V.: Scalable Service Discovery for MANET. In: Proc. of Pervasive 
Computing and Communication, pp. 245–244 (2005) 
8. Kozet, U.C., Tassiulas, L.: Network Layer Support for Service Discovery in Mobile Ad 
Hoc Networks. In: Proc. of 22nd Annual Joint Conf. of IEEE Computer and Comm. Socie-
ties, INFOCOM 2003, vol. 3, pp. 1965–1975 (2003) 
9. Outay, F., Veque, V., Bouallegue, R.: Survey of Service Discovery Protocols and Benefits 
of Combining Service and Route Discovery. Int’l. Jour. Comp. Sc. and Network Secu-
rity 7(11) (November 2007) 
10. IEEE Std 802-11, IEEE Standard for Wireless LAN Medium Access Control (MAC) and 
Physical Layer (PHY) Specifications [ISO/IEC 8802-11: 1999] (1999) 
11. Fragouli, C., et al.: Network Coding: An Instant Primer. ACM SIGCOMM Computer 
Communication Review 36(1), 63–68 (2006) 
12. Si, W., Li, C.: RMAC: A Reliable Multicast MAC Protocol for Wireless Ad Hoc Net-
works. In: Proc. of the Int’l. Conf. on Parallel Processing, ICPP 2004 (August 2004) 
13. Gupta, S., Shankar, V., Lalwani, S.: Reliable -Multicast MAC Protocol for Wireless 
LANs. In: Proc. IEEE ICC (2003) 
14. Tang, K., Gerla, M.: MAC Layer Broadcast Support in 802.11 Wireless Networks. In: 
Proc. IEEE MILCOM 2001, pp. 544–548 (October 2001) 
15. Tang, K., Gerla, M.: Random Access MAC for Efficient Broadcast Support in Ad Hoc 
Networks. In: IEEE WCNC 2000, pp. 454–459 (September 2000) 
16. Tang, K., Gerla, M.: MAC Reliable Broadcast in Ad Hoc Network. In: Proc. IEEE MIL-
COM, pp. 1008–1013 (October 2001) 
17. Sun, M.-T., Huang, L., Arora, A., Lai, T.-H.: Reliable MAC Layer Multicast in 802.11 
Wireless Networks. In: Proc. IEEE Int’l. Conf. on Parallel Processing, ICCP 2002 (2002) 
18. Jain, S., Das, S.R.: MAC layer Multicast in Wireless Multihop Networks. In: Proc. of 
IEEE Int’l. Conf. on Communication System Software and Middleware, pp. 1–10 (2006) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 34–45, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Multi-connection TFRC Video Streaming in a 
Concatenated Network: Latency and Video Quality 
Salah S. Al-Majeed1and Martin Fleury2 
1 London School of Commerce, United Kingdom 
Salah.saleh@lsclondon.co.uk 
2 University of Essex, Colchester, United Kingdom 
fleum@essex.ac.uk 
Abstract. Sending a single video stream over multiple TCP-Friendly Rate Con-
trol (TFRC) connections is a promising lightweight way of coping with wireless 
channel losses and traffic congestion in a concatenated network (one consisting 
of a broadband wireless link and a wired all-IP network). Multi-connections in-
troduce the need for data re-ordering at the receiver. This paper considers the 
potential delay at an uplink destination on the Internet. It also considers the im-
pact on video quality of packet drops due both to channel loss and router buffer 
overflow, when the TFRC congestion controller is applied. Results for an IEEE 
802.16e (mobile WiMAX) link show a worthwhile gain in video quality from 
using three or more connections over a single connection but with start-up delay 
n the multi-connection case due to the need to avoid possible buffer underflow.  
1   Introduction 
In this paper, we employ a form of MULTTFRC [1] with multiple TCP-friendly Rate 
Control (TFRC) [2] connections to stream video across a concatenated network. Such 
all-IP or Next Generation Networks are being widely developed (for instance in the 
UK in BT’s 21CN) as a cost-effective replacement for traditional telephony networks. 
A concatenated network combines an access network with a core network that may 
consist of heterogeneous sub-networks. We assume a broadband wireless access link 
with a core wired-network. Specifically, IEEE 802.16e (mobile WiMAX) [3] is mod-
eled as the broadband wireless link.  In such a network, a video stream is subject to 
packet loss due to wireless channel conditions and to traffic congestion on the wired 
network, as well as congestion on the access network. 
In video streaming across an all-IP network (one in which the IP packet format is 
universal though the Multi-Protocol Labeling System may provide circuit-switched 
routing in the core), unreliable UDP transport serves to reduce delay at the expense of 
some packet loss, while application-layer TCP emulation [4], such as TFRC, acts as a 
form of cooperative congestion control (assuming most other traffic is carried through 
TCP transport). However, TCP emulation by the application is not the same as TCP. 
TCP itself is unsuitable for delay-variation intolerant video streaming, because it 
introduces unbounded delay in support of a reliable service. Instead, TCP emulation 
mimics the average behavior of TCP, but is not ‘reliable’ and does not result in the 

 
Multi-connection TFRC Video Streaming in a Concatenated Network 
35 
 
‘saw-tooth’-like rate fluctuations that arise from TCP’s aggressive congestion control 
algorithms. The latter can cause disconcerting quality fluctuations at an end-user’s 
display if the streaming quality is varied according to the congestion level.   
In multi-connection TFRC video streaming, a single video source is multiplexed 
onto several connections across the wireless link in order to improve the wireless 
channel utilization, resulting in an increase in throughput. TFRC’s main role when 
congestion occurs is to reduce the video streaming data rate across the wired portion 
of the concatenated network. It does this in response to packet drops at intermediate 
routers, which signal the presence of contending traffic. Unfortunately, TFRC can 
misinterpret as congestion packet losses due to wireless interference and noise. 
Though cross-layer approaches to avoid misinterpretation are possible, these are com-
plex to implement and inflexible. By multiplexing a video stream across multiple 
connections it is hoped that the impact of packet loss on one or more of these connec-
tions will be mitigated by the rate across the remaining connections.  
There is widespread interest in interactive IPTV (it is a goal of BT’s 21CN). In 
Brazil already, mobile WiMAX is the basis of a networked digital TV service and 
uplink interactive services are in active development [5]. We ask what would occur if 
multiple TFRC connections were opened in the uplink (UL) from a WiMAX sub-
scriber station (SS) to base station (BS) in the presence of cross traffic from other 
mobile SSs. Thus congestion also occurs on the uplink as well as fluctuating wireless 
channel conditions. In this situation, congestion will occur at the WiMAX real-time 
polling service (rtPS) queue and packet loss will occur over the wireless channel. We 
also consider the effect of packet loss as the multiple connections pass over the core 
IP-network when other traffic sources contend for access to buffers at intermediate 
routers.  Propagation over the wired network is realistically assumed to be error-free, 
as it may well consist of optical fiber links.  
This paper’s main contribution is the finding that as the number of connections in-
creases, reduced packet loss leads to improved video quality, because of the reduced 
sending time in sending the same video data. In contrast, in [1] improved video qual-
ity comes by increasing the quantity of video data that can be sent over the multiple 
connections. Of course, increased video data implies a lower compression ratio and, 
hence, higher quality. Unfortunately, if the number of connections varies, as it does in 
[1, 6, 7] then sending rate oscillations occur. If the compression ratio was varied at the 
source (either by changing the quantization parameter at the codec if live video or 
through a bit-rate transcoder) then oscillations in rate again run the risk of disconcert-
ing changes in displayed video quality. However, we show that the quality increases 
anyway without the need to change the compression ratio and by keeping the number 
of connections constant. This is because with multiple TFRC connections, TFRC is 
better able to control its sending rate. In fact, TFRC [2] was designed for a high num-
ber of streams and has special measures if the number of streams is not high. Possibly, 
the difference in findings occurs because in work on MULTTFRC [1, 6, 7], appar-
ently no account of the impact of cross-traffic occurs except to test the fairness of the 
scheme to coexistent traffic.  
In our approach, video data is statically multiplexed onto the TFRC connections. 
The unit of multiplexing was taken to be a Group-of-Pictures (GOP) [8], with an 

36 
S.S. Al-Majeed and M. Fleury 
 
Intra-refresh rate of 15. Just as in Peer-to-Peer video streaming, when video is deliv-
ered as chunks from a number of sources, there is a need to employ a reordering 
buffer. As a result, the start-up delay in the scenario tested was about 6 s, but the gain 
in video quality (PSNR) compared to using a single connection was over one dB, a 
worthwhile gain. Start-up delay may be attributable to features of TFRC itself, which 
implies that a modified TFRC or an alternative congestion controller may reduce the 
delay. Again earlier work did not give much consideration to the effect of congestion 
in the TFRC feedback path, which we also now consider. 
2   Scenario Investigated 
The scenario tested in this paper is shown in Fig. 1. The following describes the Wi-
MAX part and this description is followed by a description of the inset, showing traf-
fic sources and sinks within the core IP network. 
2.1   WiMAX System 
In Fig. 1, once a BS has allocated bandwidth to each SS, each SS must manage its 
queue according to the data arrival rate from user applications. In WiMAX Point-to-
Multipoint (PMP) mode, there is no SS-to-SS communication unless it is via the BS. 
WiMAX networks support multiple service classes to accommodate heterogeneous 
traffic with varying requirements. WiMAX’s rtPS is most suitable for real-time video 
services, particularly for Variable Bitrate Video (VBR), which is employed to main-
tain delivered video quality but may lead to ‘bursty’ arrival rates. Other congesting  
 
UL
UL
UL
BS
SS
SS
SS
IP Core Network
C
R
A
100 Mbps
2 ms
R
100 Mbps
2 ms
100 Mbps
2 ms
5 Mbps
2 ms
B
C
100 Mbps
2 ms
 
Fig. 1. Concatenated network with inset showing routing across the core network, A, B and C 
being sources and sinks, and R = router  

 
Multi-connection TFRC Video Streaming in a Concatenated Network 
37 
 
traffic is assumed to enter the non-real-time Polling Service (nrtPS) queue at the SS. 
In our experiments for both queues, a drop-tail queuing discipline was simulated. 
Queue sizes were all set to fifty packets. This value was selected as it seems appropri-
ate to mobile, real-time applications for which larger buffer sizes might lead both to 
increased delay and also greater active and passive energy consumption at the buffer’s 
memory. 
The WiMAX system operating in PMP mode was simulated by well-known ns-2 
simulator (v. 2.29) augmented by a WiMAX module [10]. The simulator is allowed to 
reach steady-state over 20 s with other traffic passing over the network. The PHY 
settings selected for WiMAX simulation are given in Table 1, with additionally the 
MAC settings defaulted from [10]. The DL/UL ratio is not intended to be realistic but 
to aid in testing multiple-connection TFRC, as in practice the DL would be allocated 
the majority of the bandwidth. The antenna is modeled for comparison purposes as a 
half-wavelength dipole. The Gilbert-Elliott ‘bursty’ channel model is further ex-
plained in Section 2.5. The frame length is significant, as a longer frame reduces de-
lay at the MS by permitting more data to be removed from any queues at each polling 
time. The value of 20 ms is at the high end of the available durations in the Standard 
[3] in order to reduce this source of queuing delay for real-time video streaming.  
Table 1. Simulated WiMAX settings, OFDMA = Orthogonal Frequency Division Multiple 
Access, QAM = Quadrature Amplitude Modulation, TDD = Time Division Duplex 
Parameter 
Value 
PHY 
Frequency band 
Duplexing mode 
Frame length 
Max. packet length 
Raw data rate 
IFFT size  
Modulation 
Guard band ratio 
DL/UL ratio 
Path loss model 
Channel model 
MS transmit power 
BS transmit power 
Approx. range to MS 
Antenna type 
Antenna gains 
MS antenna height 
BS antenna height 
OFDMA 
5 GHz 
TDD 
20 ms 
1024 B 
10.67 Mbps 
1024 
16-QAM 1/2 
1/8 
1:3 
Two-ray ground 
Gilbert-Elliott 
250  mW 
20 W 
0.7 km 
Omni-directional 
0 dBD 
1.5 m 
32 m 

38 
S.S. Al-Majeed and M. Fleury 
 
2.2   WiMAX Traffic Characteristics 
There were three SSs communicating to the BS, with one of the SS sending a VBR 
video sequence encoded with the H.264/Advanced Video Codec (AVC) [11] and split 
between the multiple TFRC connections. The other SSs are simply introduced as 
sources of competing traffic across the wireless link and do not indicate the likely size 
of a WiMAX network, which obviously could be larger. A trace file was input to ns-2 
and packet losses recorded in the output. The output serves to calculate the PSNR. 
Video quality comparisons were made under the EvalVid environment [12]. As a test, 
we used the ‘Paris’ clip H.264 VBR-encoded at 30 Hz (frame/s) at Common Interme-
diate Format (CIF) (352×288 pixel/frame) with initial quantization parameter set to 
26 (from a range 0 to 51).  The slice size was fixed at the encoder to be a maximum of  
900 B. Paris consists of two figures seated around a table in a TV studio setting, with 
high spatial coding complexity. H.264’s Baseline profile was selected, as this is more 
easily supported by mobile devices because of its reduced computational overhead. 
The Intra-refresh rate was every 15 frames with IPBB…I structure, i.e. the GOP size 
was 15. 1063 frames were transmitted. Simple Previous Frame Replacement (PFR) 
was set for error concealment at the decoder. 
Table 2 records the simulated traffic characteristics for the three SSs communica-
tion with the BS. Network Adaptation Layer Units (NALUs) from the H.264 codec 
were encapsulated with Real Time Protocol (RTP) headers. After the addition of IP 
headers, these in turn formed a single WiMAX MAC Packet Data Unit (MPDU), 
which are variable-sized WiMAX packets. For simplicity, a WiMAX MPDU is now 
referred to as a packet.  
For TFRC, the inter-packet sending time gap was varied according to the TFRC 
equation [2], not the simplified version reported in [7]. As described in [2], TFRC is a 
receiver-based system in which the packet loss rate is found at the receiver and fed-
back to the sender in acknowledgment messages. The sender calculates the round-trip 
time from the acknowledgment messages and updates the packet sending rate. A 
throughput equation models TCP New Reno to find the sending rate: 
)
32
1(
8
3
3,1
min
3
2
)
,
,
,
(
2
p
p
bp
t
bp
t
s
p
s
t
t
TFRC
rto
rtt
rto
rtt
+
⎟⎟
⎠
⎞
⎜⎜
⎝
⎛
+
=
  
(1) 
where trtt is the round-trip time, trto is TCP’s retransmission timeout, s is the segment 
size (TCP’s unit of output) (herein set to the packet size), p is the normalized packet 
loss rate, wm is the maximum window size, and b is the number of packets acknowl-
edged by each ACK. b is normally set to one and trto = 4trtt. It is important to notice 
that trto comes to dominate TFRC’s behavior in high packet loss regimes [2], which is 
why it is unwise to use a simplified form of (1). General inspection of (1) indicates 
that if the round-trip time and/or the packet loss rate (the two independent variables in 
the denominator of (1)) increase then the throughput reduces.  

 
Multi-connection TFRC Video Streaming in a Concatenated Network 
39 
 
Table 2. Simulated WiMAX traffic characteristics 
SS-UL 
Service  
type 
Traffic  
type 
Protocol 
Packet  
Size (B) 
1 
rtPS 
 
nrtPS 
VBR (video) 
CBR 
FTP 
Multiple TFRC 
UDP 
TCP 
Variable 
1000 
2 
rtPS 
nrtPS 
CBR 
FTP 
UDP 
TCP 
1000 
3 
rtPS 
nrtPS 
CBR 
FTP 
UDP 
TCP 
1000 
SS-DL 
 
 
 
 
1,2 
3 
rtPS 
nrtPS 
CBR 
FTP 
UDP 
TCP 
1000 
 
In our variant to standard TFRC, the packet size, s, in the TFRC equation was dy-
namically altered according to the EvalVid-created trace file sizes. This variant makes 
for more responsive control rather than the mean packet length employed in the origi-
nal TFRC formulation [2]. TFRC was originally intended for video-on-demand appli-
cations, when it is feasible to calculate the mean packet length. Setting a mean packet 
length is inappropriate for interactive multimedia applications. The underlying TFRC 
transport protocol was set to UDP, as is normal. 
Coexisting rtPS queue CBR sources were all sent at 1500 kbps, i.e. at a similar rate 
to the video source.  The inter-packet gap was 0.03 s for the CBR traffic. The FTP 
applications, which continuously supplied data according to available bandwidth, 
were set up out of convenience as a way of occupying the nrtPS queues; otherwise a 
Best-Effort (BE) queue might be more appropriate. Likewise, the DL traffic is simply 
selected to fully occupy the DL link capacity. 
2.3   Core Network Traffic Characteristics 
In Fig. 1, all links except a bottleneck link within the core network are set to 100 
Mbps to easily accommodate the traffic flows entering and leaving the network. The 
link delays are minimal (2 ms) in order to avoid confusing propagation delay with re-
ordering delay. A bottleneck link with capacity set to 5 Mbps is set up between the 
two routers. The buffer size in each router was set to 50 packets. This arrangement is 
not meant to physically correspond to a network layout but to represent the type of 
bottleneck that commonly lies at the core network edge before entry into a corporate 
or campus network.  
Node A sources to node B a CBR stream at 1.5 Mbps with packet size 1 kB and 
sinks a continuous TCP FTP flow sourced at node B. Node B also sources an FTP 
flow to the BS and a CBR stream at 1.5 Mbps with packet size 1 kB (see Table 2 
downlink). Other SS sources apart from the video connections do not pass over the 
core network shown but are assumed to be routed elsewhere after passing the Wi-
MAX BS. Node C in Fig. 1 is the sink for the TFRC multiple connections. 

40 
S.S. Al-Majeed and M. Fleury 
 
2.4   Management of Connections 
To systematically test the effect of multiple TFRC connections the number of TFRC 
connections was incrementally stepped up in successive experiments.  In MULTTFRC 
itself, the number of connections is changed over time according to the average round-
trip time of all the connections, but this hides the interpretability of results. As remarked 
earlier, it is also unclear from [1, 6, 7] how a single video stream would be apportioned 
between a varying number of connections. In our experiments, a single queue was seg-
mented into GOPs (15 frames). Each connection was statically allocated its GOPs, 
which are taken in interleaved manner from the video sequence. As previously men-
tioned, this assumes that a re-ordering buffer is available at the receiver. 
2.5   Channel Model 
A Gilbert-Elliott two-state, discrete-time, ergodic Markov chain [13] modeled the 
wireless channel error characteristics at the ns-2 physical layer. The probability of 
remaining in the good state was set to 0.95 and of remaining in the bad state was 0.94, 
with both states modeled by a Uniform distribution. The packet loss probability in the 
good state was fixed at 0.01 and the bad state default was 0.05. However, the bad 
state packet loss probability, PB, was also varied as [0.01 , 0.02 , …, 0.1]. In this way, 
we were able to judge the effect of worsening burst error channel conditions.  
3   Evaluation 
Initial investigations considered the WiMAX link alone in Fig. 1. Table 3 shows the 
average data-rate over time when transmitting the Paris clip over multiple connec-
tions, for two different WiMAX frame sizes: the default from Table 1 and 5 ms 
(frame duration code 2 in the Standard [3]). Clearly, TFRC is able to multiplex more 
data onto a link as the number of connections increases, though observation of a time-
wise plot of throughput shows that during transmission TFRC sharply reduces its 
overall sending rate in response to packet loss.  Because the sending period for one 
connection is more than the display period of Paris with the shorter frame duration, 
the longer frame duration is clearly preferable.  
Fig. 2 plots the video stream packet drop rate relative to channel packet error rate. 
Included in the percentages in Fig. 2 are any additional packet losses arising from 
buffer overflow at the SS caused by the SS packet scheduler being otherwise occupied 
servicing the rtPS queues in the three SSs. In this Figure, the shorter frame size is  
 
Table 3. Sending periods and throughputs from the video streaming MS to the WiMAX BS 
No. of connections 
SS to BS (s) 
frame size 5 ms 
Throughput 
(kbps) 
SS to BS (s) 
frame size 20 ms 
Throughput 
(kbps) 
1-conn 
2-conn 
3-conn. 
4-conn. 
71.4 
35.8 
23.3 
17.4 
217 
437 
663 
889 
33.5 
20.5 
17.7 
14.6 
467 
754 
874 
1059 

 
Multi-connection TFRC Video Streaming in a Concatenated Network 
41 
 
 
Fig. 2. Average packet drop rate for an increasing number of connections, according to channel 
error rate 
employed, which will give more favorable results than the longer frame size. As will 
be observed, no strong effects result from increasing the number of connections. 
Moreover, for all but the highest error rates the packet loss rate is below 10%. 
An interesting comparison is with the throughput when the core network is in-
cluded, Table 4. There is a similar pattern to the throughputs in Table 3 but the rates 
are reduced to when streaming only over the WiMAX link. We interpret this effect as 
not due to TFRC’s response to packet loss but due to its response to the increased 
round trip time caused by queuing delay in the buffer prior to the bottleneck link in 
Fig. 1. This is confirmed by the increase in per slice/packet end-to-end delay as more 
connections are added, Table 3. In effect, the packets from other connections inter-
vene in the router buffers causing an increase in latency.  
More significantly for reconstruction of the video stream is the GOP ordering, 
which for four connections is shown in Fig. 3. Notice that the first GOP contains 
parameters that are fixed throughout the sequence, a feature of the H.264/AVC codec. 
Therefore, this GOP is transported more quickly. To avoid a sudden injection of traf-
fic into the network, connection starting times were offset by 0.5 s.  
A noticeable feature of this Figure is the lengthier start-up periods in sending initial 
GOPs on each of the connections. This does mean that about 6 s of frames (amount-
ing to 90 frames) should be stored in the reordering buffer, to avoid the possibility of 
subsequent underflow in the decoder’s playout buffer. As the destination is on the 
fixed network the reorder buffer is not expected to be a drain on energy resources, as 
it might be on an SS. 6 s is longer than an ideal start-up time of around 2 s but not too 
large to be objectionable to the user. Interestingly, when comparing with the through-
put reported in [1] and repeated in [7], for MULTTFRC there are periods of at least 
three seconds when the throughput is approximately over half the peak rate. The ag-
gregate throughput also may oscillate. In fact, on finding this problem, we compared 
with [1] and discovered that allowance was made for 10 s start-up buffering before 

42 
S.S. Al-Majeed and M. Fleury 
 
beginning decode, also to avoid buffer underflow. However, that work [1] used data 
from MPEG-4 at a lower 10 frame/s to test buffer occupancy. 
The cause of the initial lengthier start-up periods may be a combination of factors. 
However, the early response of TFRC appears to be implicated. The initial rate of 
TFRC is set to one packet/s and no default settings for round-trip time or packet loss 
rate are used in the throughput equation (1). Normally, if no acknowledgement arrives 
within two round trip times then TFRC reduces its sending rate by half and goes into 
a slow start, similarly to TCP. However, the initial default value of the no-feedback 
timer is set to 2 s, which implies that TFRC’s initial rate may be prolonged if ac-
knowledgments are lost or delayed. If acknowledgement drops or delays still occur 
then it is possible that the rate will be halved again before slow-start. However, the 
timeout interval will be shorter as it is now given by: 
 
                                        timeout  =     max(4r, (2s)/TFRC)  
 
 
(2) 
 
where r is  the estimated round-trip time, as before s is the packet size, and TFRC is 
the sending rate given by (1). Though in [7] it was acknowledged that drastic reduc-
tions in sending rate could occur due to the onset of slow-start, this was attributed to 
heavy packet loss and not to the loss or delay of acknowledgments, without data 
packet loss necessarily occurring.  
Table 4. Sending periods and throughputs from the video streaming SS to the core network 
detination (node C in Fig. 1) 
No. of connections 
MS to node c 
frame size 20 ms 
Throughput 
(kbps) 
1-conn 
2-conn 
3-conn. 
4-conn. 
35.2 
22.4 
21.6 
15.6 
444 
690 
716 
991 
Table 5. Mean per slice/packet end-to-end delay 
No. of connections 
Mean end-to-end 
delay (s) 
1-conn. 
2-conn. 
3-conn. 
4-conn. 
0.035 
0.036 
0.039 
0.062 
Corresponding to Fig. 3, Fig. 4 plots individual throughputs and the aggregate 
throughput. As might be expected from Fig. 3, throughput gradually climbs until a 
plateau is reached. There is evidently some unfairness between the TFRC flows as 
connection 3 needs to prolong its delivery because of lower throughput at an earlier 
stage. However, there are less oscillations in rate than reported for MULTTFRC  
[1, 7], which is explained by the static scheduling scheme employed by us.  

 
Multi-connection TFRC Video Streaming in a Concatenated Network 
43 
 
 
Fig. 3. Example arrival sequence at the receiver (node C in Fig. 1) showing the start and end 
times. GOP 1 contains the parameter-set for the sequence. Connection start times are staggered 
by 0.5 s. 
 
Fig. 4. Example run showing throughput over time for individual connections and the aggregate 
throughput 
Packet loss over time displays an oscillatory pattern for the example in Fig. 5, 
which is why it is unwise to rely on mean loss statistics alone. Based on the packet 
loss patterns the average PSNR was found when increasing the number of connec-
tions, as recorded in Table 6. The frame sizes are adjusted in Table 6 to account for 
the buffer underflow that would occur were the shorter frame size to be used through-
out the network path. However, counter-intuitively, employing a shorter frame size 
for a few connections over the WiMAX link alone results in lower video quality than 

44 
S.S. Al-Majeed and M. Fleury 
 
when sending over the complete path. This is best explained by buffer overflow at the 
SS, caused by the short WiMAX frame size, rather than packet losses on the WiMAX 
wireless channel. When the number of connections increases, TFRC is better able to 
regulate its rate and the video quality increases over the single wireless link. Notice, 
however, that using smaller frame size even when the video quality is high can lead to 
excessive delay at the SS buffers if traffic is heavy. 
Table 6. Video quality (PSNR) according to number of connections 
No. of connections 
PSNR (dB) recorded  
at WiMAX base station 
(frame size 5 ms) 
PSNR (dB) recorded at  
node C in Fig. 1  
(frame size 20 ms) 
1-conn. 
2-conn. 
3-conn. 
4-conn. 
26.72 
31.32 
35.92 
35.32 
31.84 
32.34 
33.15 
33.34 
 
Fig. 5. Aggregate packet loss numbers for connections for a sample run over time 
4   Conclusion 
This paper has conducted a relatively realistic investigation of multiple TFRC connec-
tions for uplink video streaming over a concatenated network, consisting of a WiMAX 
access network and a fixed network with a bottleneck at the network edge. The study 
has shown that with static scheduling of the video stream over the connections, in-
creased throughput results. Reducing the video send time reduces the risk from wireless 
channel error. However, it also implies that reordering at the receiver is required. The 
resulting start-up delay was about 6s for the reasonably complex test video. With a 
moderate number of connections (four were used) video quality improved by over one 
dB for streaming across the modeled network. Using a smaller WiMAX frame size can 
lead to further improvements across the wireless link itself but there is a risk of exces-
sive queuing at the subscriber station devices causing unacceptable delays. The role of 

 
Multi-connection TFRC Video Streaming in a Concatenated Network 
45 
 
the feedback channel is important, as loss or delay of acknowledgment packets seems to 
be implicated in the TFRC congestion controllers’ slow start-up, one of the potential 
causes of buffer underflow. Further investigation will consider the role of acknowledg-
ments and whether a reduction in the acknowledgment rate may improve performance 
further. It may also be possible to ‘warm-up’ the TFRC connection handlers by sending 
non-video data to start with, which could be discarded thereafter.  This can reduce the 
size of the reordering buffer, if such a reduction were required.  
References 
1. Chen, M., Zakhor, A.: Rate Control for Streaming Video over Wireless. IEEE Wireless 
Comms. 12(4), 32–14 (2005) 
2. Handley, M., Pahdye, J., Floyd, S., Widmer, J.: TCP-Friendly Rate Control (TFRC): Pro-
tocol Specification. RFC 3448 (2003) 
3. IEEE, 802.16e-2005. IEEE Standard for Local and Metropolitan Area Networks. Part 16: 
Air Interface for Fixed and Mobile Broadband Wireless Access Systems (2005) 
4. Widmer, J., Denda, R., Mauve, M.: A Survey on TCP-friendly Congestion Control. IEEE 
Network 15(3), 28–37 (2001) 
5. Meloni, L.G.P.: A New WiMAX Profile for DTV Return Channel and Wireless Access. 
In: Chen, K.-C., de Marca, J.R.B. (eds.) Mobile WiMAX, pp. 291–392. Wiley & Sons, 
Chichester (2008) 
6. Chen, M., Zakhor, A.: Rate Control for Streaming Video over Wireless. In: IEEE INFO-
COM, pp. 1181–1190 (2004) 
7. Chen, M., Zakhor, A.: Multiple TFRC Connection Based Rate Control for Wireless Net-
works. IEEE Trans. Multimedia 8(5), 1045–1062 (2006) 
8. Sadka, A.: Compressed Video Communications. Wiley & Sons, Chichester (2006) 
9. Balkrishnan, H., Padmanabhan, V., Seshan, S., Katz, R.: A Comparison of Mechanisms for 
Improving TCP Performance over Wireless Links. IEEE/ACM Trans. on Networking 5(6), 
756–769 (2007) 
10. Tsai, F.C.-D., et al.: The Design and Implementation of WiMAX Module for NS-2 Simu-
lator. In: Workshop on NS2: The IP Network Simulator, article no. 5 (2006) 
11. Wiegand, T., Sullivan, G.J., Bjontegaard, G., Luthra, A.: Overview of the H.264/AVC 
Video Coding Standard. IEEE Trans. Circuits Syst. Video Technol. 13(7), 560–576 (2003) 
12. Klaue, J., Rathke, B., Wolisz, A.: EvalVid - A Framework for Video Transmission and 
Quality Evaluation. In: Int. Conf. on Modeling Techniques and Tools for Computer Per-
formance, pp. 255–272 (2003) 
13. Haßlinger, G., Hohlfeld, O.: The Gilbert-Elliott model for packet loss in real time services 
on the Internet. In: 14th GI/ITG Conf. on Measurement, Modelling, and Evaluation of 
Computer and Commun. Systs., pp. 269–283 (2008) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 46–58, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Performance Evaluation of the Number of Database 
Accesses in Cellular Networks 
Mustafa Vahabzadeh Dolama and Akbar Ghaffarpour Rahbar  
Computer Networks Research Lab, Department of Electrical Engineering, 
Sahand University of Technology, Tabriz, Iran 
{m_vahabzadeh,ghaffarpour}@sut.ac.ir 
Abstract. In a wireless network, all information about users must be stored in 
one or more databases. Since a user location in cellular networks is not fixed 
and can change along the time, the information of a user that moves to a new 
location must be updated. Besides, when user x wants to communicate with 
user y, the location of user y must be extracted from the relevant database. 
Therefore, the database must be accessed for updating, recording, deleting, and 
searching information of a user. Thus, the most important criterion of an algo-
rithm is to have a small database access time. In this paper, we compare the 
number of database accesses required for updating, deleting, and searching un-
der different approaches that have been proposed in wireless networks. 
Keywords: mobile users, updating cost, update rate, searching cost, deleting cost. 
1   Introduction 
In cellular networks such as Personal Communication System (PCS), the location of 
users is not fixed and may change in time. Therefore, to make a communication be-
tween user x and user y, the system must first find the location of user y. Therefore, 
the location of users must be tracked from time to time [1]. In PCS, a small geo-
graphical area (called cell) is served by a Base Station (BS). Several cells are grouped 
into a Location Area (LA), and several LAs make a PCS. The Mobile Terminals 
(MTs) in a cell directly communicate with the BS of the cell. Several BSs are con-
nected to a Base Station Controller (BSC), and several BSCs are connected to a Mo-
bile Switching Center (MSC)[2],[3]. 
In a typical telephone system, we have one database that stores all users informa-
tion permanently. Therefore, the location of each user can be found easily by search-
ing the database. However, in wireless networks the location of users not fixed. When 
a user enters to a new location, the information of this user must be updated. With the 
increase of the number of mobile users in wireless networks, the database access 
becomes a bottleneck because more database accesses (for updating, deleting, search-
ing, and recording new information) are necessitated in time[4]. Thus, choosing a 
good algorithm for tracking users in wireless networks depends on the number of 
database accesses that it needs.  
The objective of this paper is to compare the number of database accesses under 
the methods proposed in wireless network for tracking mobile users. 

 
Performance Evaluation of the Number of Database Accesses in Cellular Networks 
47 
 
The remainder of this paper is organized as follows. Location management meth-
ods are explained in Section 2. In Section 3, we compare location management meth-
ods by an example. Finally, a brief conclusion is presented in Section 4. 
2   Methods Proposed for Tracking Mobile Users 
Many strategies have been proposed to reduce the overhead of database accesses in 
PCS. In this section, we will briefly describe and compare some location management 
approaches such as two-tier architecture [6], Forwarding Pointer [5], Virtual Layer 
[7], Virtual Layer with Forwarding Pointer (VL-FP) [8], and Overlap Region [9].  
2.1   Two-Tier Architecture 
Two-tier architecture [6] uses a two-level database system: (1) HLR that maintains all 
permanent information of each user and a pointer to another database; and (2) Visitor 
Location Register (VLR) that stores temporary location information of users. The 
VLR database is maintained at each LA. Therefore, 
i. 
When mobile user x enters the PCS (i.e., user turns the mobile on), a new re-
cord is created in both HLR and VLR in order to store the information of user 
x. Thus, one HLR and one VLR accesses are required. 
ii. 
When mobile user x moves from LAi to LAj, the information of the user x in 
VLRi is deleted and a new record is created in VLRj. In addition, a message is 
sent to HLR by VLRj in order to update the user x pointer from VLRi to VLRj. 
Therefore, one HLR access and two VLR accesses are necessary. 
iii. 
When mobile user x decides to call mobile user y: 
 
a. If both user x and user y are in the same LAi, the location of user y is found 
from VLRi. Thus, one VLR access is needed. 
b. If both user x and user y are not in the same LAi, first the location of user y is 
searched in VLRi. Since the information cannot be found in VLRi, the rele-
vant VLRj can be found from HLR. Finally, the location of user y is found 
from VLRj. Therefore, one HLR access and two VLR accesses are required 
to find the location of user y.  
Since, the access of the HLR database takes more time than the access of a 
VLR database due to the large size of the HLR database, the two-tier architec-
ture can reduce search cost when both user x and user y are in the same LA. 
However, when user x and user y are not in the same LA, the HLR, the new 
VLR and old VLR all must be accessed for appropriate functions. This in turn 
increases the number of database accesses. 
Finally, when user x turns his mobile off or exits from the PCS, the information of 
user x in HLR and VLR should be deleted. To delete the information of user x, one 
HLR and one VLR accesses are necessitated. 

48 
M.V. Dolama and A.G. Rahbar 
 
2.2   Forwarding Pointer 
When a user frequently moves in a boundary between LAs, more HLR accesses are 
required for updating in the two-tier architecture and HLR may likely become a bot-
tleneck. The Forwarding Pointers technique [5] has been proposed to efficiently re-
duce the volume of HLR accesses required for updating. In this approach, the main 
idea is to set up a forwarding pointer from an old database to a new database when a 
user leaves the old LA toward a new LA. Therefore, 
i. 
Like the two-tier architecture when mobile user x enters the PCS (i.e., user x 
turns his mobile on), one HLR and one VLR accesses are needed. 
ii. 
When mobile user x moves from LAi to LAj, a new record is created in VLRj 
and a pointer is set to VLRj from VLRi. Therefore, two VLR accesses are only 
needed. 
iii. 
When mobile user x calls mobile user y: 
 
a. If both user x and user y are in the same LAi, the location of user y is either 
directly found from VLRi or following the pointers chain. Thus, l VLR ac-
cesses are necessitated, where l is the length of the pointers chain. We have l 
= 1 if the information is retrieved directly from VLRi.  
b. If both user x and user y are not in the same LAi, the location of user y is first 
searched in VLRi and the relevant pointers chain. Since the information can-
not be found, the relevant VLRj can be found from HLR. Finally, the loca-
tion of user y is either directly found from VLRj or by following the pointers 
chain. Therefore, one HLR access and 2 × l VLR accesses are needed to find 
the location of user y. 
Finally, when user x turns its mobile off or exits from the PCS, the information of 
user x in HLR and VLR must be deleted. One HLR and l VLR accesses are needed to 
delete the information of user x. 
Since no update is required in the HLR database, the update cost goes down. When 
the length of the pointer chain is less than 5, according to analytical estimation in [5], 
this method can reduce the total cost by 20% to 60%. Although this technique can 
reduce the total cost, the frequent updates problem still exists when a user moves and 
backs in the boundary of an LA. 
2.3   Virtual Layer Scheme 
The virtual layer scheme [7] has been proposed to construct a new location database 
architecture (see Fig.1). The bold lines in Fig.1 represent the original layer and the 
dotted lines represent the virtual layer. For every virtual layer, one VLR is needed 
(i.e., subVLR). 
In this scheme, one SubMSC is necessitated for each virtual layer. The SubMSCs 
are connected to the covered MSC. For example in Fig.1, consider MTx moves from 
position A to B, B to C and then comes back to position A. Initially in position A, 
HLR and VLR1 have created an entry for MTx. When MTx moves to position B, the 
SubMSC4 creates a new entry for MTx and VLR1 must be updated. Then, when MTx 
moves from position B to C and C to A, no update is needed because the virtual layer 
has not changed.  

 
Performance Evaluation of the Number of Database Accesses in Cellular Networks 
49 
 
 
Fig. 1. The demonstration of the virtual layer 
The goal of this scheme is to reduce both location updating rate and location updat-
ing cost, especially when the MTs reside near the boundaries of LA and frequently 
cross through the boundary to another LA. 
i. 
Like the two-tier architecture when mobile user x enters the PCS (i.e., user x 
turns his mobile on), one HLR and one VLR accesses are necessitated. 
ii. 
When VLR is active: mobile user x moves from one LAi to LAj; 
 
a. If the information of user x already exists in subVLRk: VLRi must be deac-
tivated and subVLRk must be activated. Therefore, one VLR and one 
subVLR accesses are necessary. 
b. If the information of user x does not exist in subVLRk: A new record is cre-
ated in subVLRk and the information in previous subVLR must be deleted. 
Besides, VLRi must be deactivated and subVLRk must be activated. There-
fore, one VLR and two subVLR accesses should be done. 
 
iii. 
When subVLR is active: mobile user x moves from virtual layer i to virtual 
layer j; 
 
a. If the information exists in VLRk; the information in VLRk is updated. 
VLRk must be activated and subVLRi must be deactivated.  Therefore, one 
VLR and one subVLR accesses are required. 
b. If the information does not exist in VLRk; a new record is created in VLRk 
and the previous record must be deleted. Hence, a message is sent to HLR 
by VLRk in order to update the user x VLR pointer (from previous VLR to 
VLRk). Besides, VLRk must be activated and subVLRi must be deactivated. 
Thus, one HLR access, one subVLR, and two VLR accesses are required. 
 

50 
M.V. Dolama and A.G. Rahbar 
 
iv. 
When mobile user x calls mobile user y: 
 
a. If both user x and user y are in the same LAi, the location of user y is found 
from VLRi. Thus, one VLR access is needed.  
b. If both user x and user y are not in the same LAi, the location of user y is 
first searched in VLRi. Since the information cannot be found from VLRi, 
the relevant VLRj can be found from HLR. Finally, the location of user y is 
found from VLRj. Therefore, one HLR access and 2 VLR accesses are ne-
cessitated to find the location of user y. 
Finally, when user x turns his mobile off or exits from PCS, the information of user x 
in HLR and VLR should be deleted. For this purpose, one HLR and one VLR ac-
cesses are necessary. 
2.4   Virtual Layer with Forwarding Pointers 
Chang and Lin have proposed an improved the scheme [8] that uses forwarding point-
ers in virtual layer to reduce the update cost. The possible state of a user in this 
scheme is:  
i. 
Similar to the two-tier architecture when mobile user x enters the PCS (i.e., 
user x turns his mobile on), one HLR and one VLR accesses are necessitated. 
ii. 
When VLR is active: mobile user x moves from one LAi to LAj; 
 
a. If the information of user x already exists in subVLRk: VLRi must be deacti-
vated and subVLRk must be activated. Therefore, one VLR and one subVLR 
accesses should be performed. 
b. If the information of user x cannot be found in subVLRk: A new record is 
created in subVLRk and the information in previous subVLR must be de-
leted. Besides, VLRi must be deactivated and subVLRk must be activated. 
Therefore, one VLR and two subVLR accesses are required. 
 
iii. 
When subVLR is active: mobile user x moves from virtual layer i to virtual 
layer j; 
 
a. If the information exists in VLRk; the information in VLRk is updated. 
VLRk must be activated and subVLRi must be deactivated.  Therefore, one 
VLR and one subVLR accesses are needed. 
b. If the information does not exist in VLRk; a new record is created in VLRk 
and a message is sent by VLRk to previous VLR to set a pointer to VLRk. 
Furthermore, VLRk must be activated and subVLRi must be deactivated. 
Thus, one subVLR and two VLR accesses are required. 
 
iv. 
When mobile user x calls mobile user y: 
 
a. Like the Forwarding Pointer scheme, l VLR accesses are necessitated. 
b. Like the Forwarding Pointer scheme, one HLR access and 2 × l VLR ac-
cesses are required to find the location of user y. 

 
Performance Evaluation of the Number of Database Accesses in Cellular Networks 
51 
 
Finally, when user x turns his mobile off or exits from PCS, the information of user x 
in HLR and VLR should be deleted. To do this, one HLR and l VLR accesses are 
required. 
2.5   Overlap Region 
The Virtual Layer scheme [7] requires the reconstruction of the PCS architecture. The 
architecture has to require extra equipments. To overcome the reconstruction of the 
PCS, the Virtual Overlap scheme [9] with time stamp has been proposed. Fig.2 de-
picts the structure of the Virtual Overlap [9]. Each Overlap Region (OR) has seven 
LAs. The bold line in Fig. 2 represents the Overlapping Region for LA5, and there-
fore, we have OR5 = {LA1, LA2, LA4, LA5, LA6, LA9, LA10}. In Fig.2, the OR for 
LA6 is OR6= {LA2, LA3, LA5, LA6, LA7, LA10, LA11}. Each LA has an associated 
MSC and VLR. 
 
Fig. 2. The structure of virtual overlap in PCS 
In the Virtual Overlap scheme [9], each VLR has two fields: (1) TS which indi-
cates the time that a mobile user enters the associated LA; and (2) OR which indicates 
the Overlap Region in which the mobile user has registered last time. Therefore, 
i. 
Similar to the two-tier architecture when mobile user x enters the PCS (i.e., 
user x turns his mobile on), one HLR and one VLR accesses are necessary. 
ii. 
When mobile user x moves from LAi to LAj; 
 
a. If LAi and LAj are in the same virtual overlap region, a new record is cre-
ated in VLRj and the TS field of VLRj records the current time. Therefore, 
one VLR access is necessitated. 
b. If LAi and LAj are not in the same virtual overlap region, a new record is 
created in VLRj and the TS field of VLRj records the current time. Thus, a 
message is sent to HLR in order to update the user x data. Furthermore, the 
information of user x in previous OR (with 7 VLRs) must be deleted. 
Therefore, one HLR and eight VLR accesses are required. 
 

52 
M.V. Dolama and A.G. Rahbar 
 
iii. 
When mobile user x calls mobile user y: 
 
a. If both user x and user y are in the same LAj, the location of user y is found 
from seven VLRs in the relevant OR. Thus, seven VLR accesses are 
needed.  
b. If both user x and user y are not in the same LAj, first the location of user y 
is searched in VLRj. Since the information cannot be found from VLRj, a 
message is sent to HLR by VLRj and then the relevant VLRi can be found 
in HLR. Finally, the associated overlap region is found from the OR field 
of VLRi, and then the location of user x is searched in seven VLRs in the 
relevant OR. Therefore, one HLR access and 8 VLR accesses are required 
to find the location of user y. 
Finally, when user x turns its mobile off or exits from the PCS, the information of 
user x in HLR and seven VLRs on OR that user has resided before should be deleted. 
To delete the information of user x, one HLR and seven VLR accesses are necessary. 
3   Performance Evaluation 
In this section, we shall compare the schemes stated in Section 2. First, the number of 
database accesses under different schemes will be illustrated for each possible action 
of a user. Then, we shall discuss the number of databases by an example. Table 1 
shows the comparison of different schemes in terms of the number of database ac-
cesses for possible status of a user. 
Table 1. Comparison of database accesses 
scheme 
 
Two-tier 
architecture 
[6] 
Forwarding 
Pointer [5] 
Virtual 
Layer[7] 
(VL-FP) 
[8] 
Overlap 
Region [9] 
operation 
 
HLR 
VL R 
HLR 
VLR 
HRL 
VLR 
HLR 
VLR 
HLR 
VLR 
user is turned on
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
user is turned off
1 
1 
1 
l
1 
1 
1 
l
1 
7 
minimum 
access
0 
1 
0 
1 
0 
1 
0 
1 
0 
7 
searching 
a user 
maximum 
access
1 
2 
1 
2 × l
1 
2 
1 
2 × l
1 
8 
minimum 
access
1 
2 
0 
2 
0 
0 
0 
0 
0 
1 
user 
moves 
from one 
LA to 
another 
LA 
maximum 
access
1 
2 
0 
2 
1 
3 
0 
3 
1 
8 
 

 
Performance Evaluation of the Number of Database Accesses in Cellular Networks 
53 
 
In PCS, tracking the mobile users may be more important issue than other tasks. 
Therefore, a good method must provide a small database access when a user moves 
from one LA to another LA. In Table 1, Virtual Layer and (VL-FP) have small data-
base accesses, but need reconstruction of the PCS. Furthermore, when the length of 
the chain in Forwarding Pointer and (VL-FP) schemes goes up, the number of data-
base accesses increases. Since the access of HLR database takes more time, Overlap 
Region reduces update cost when user backs and forths in boundary of LAs (just need 
one VLR access) which is comparable with the two-tier architecture (that needs one 
HLR and two VLRs accesses). In searching the user location, the two-tier architecture 
and Virtual Layer always provide small number of database accesses, and Overlap 
Region has more database accesses than other schemes. 
Fig.3 shows an example that user x moves from position A to F through positions 
B, C, D, E, and F. 
i. 
Initially, user x enters LA5 or is turned on in LA5. The following procedures 
are performed: 
 
a. VLR5 creates a new entry for user x. 
b. VLR5 sends a registration message to HLR to create an entry and to set a 
pointer to VLR5. 
 
ii. 
When user x moves from A to B: 
 
a. Two-tier architecture: VLR9 creates a new record for user x and sends a 
message to HLR to update information. Then, the information in VLR5 is 
deleted. 
b. Forwarding Pointer: VLR9 creates a new record for user x and sends a mes-
sage to VLR5 to set a pointer to VLR9. 
c. Virtual Layer: When user x enters LA9, a new record is created in subVLR2 
and VLR5 is deactivated. Then, user x enters the virtual layer 3 from virtual 
layer 2. Therefore, a new record is created in VLR9 and the information in 
VLR5 is deleted. Hence, a message is sent to HLR by VLR9 to update rele-
vant information. 
d. (VL-FP): When user x enters LA9, a new record is created in subVLR2 and 
VLR5 is deactivated. Then, user x enters the virtual layer 3 from virtual 
layer 2. Therefore, a new record is created in VLR9 and the information in 
VLR5 is deleted. Hence, a message is sent to VLR5 to set a pointer to 
VLR9. 
e. Overlap Region with Time Stamp: when a user enters LA9, because LA9 is 
in OR5 a new record is created in VLR9 and the TS field of VLR9 stores the 
time that user has entered LA9. Moreover, the OR field of VLR9 stores the 
user x overlap region number (OR5). 
 
iii. 
Movement from position B to position C: 
 
a. Two-tier architecture: VLR10 creates a new record for user x and sends a 
message to HLR to update information. Then, the information in VLR9 is 
deleted. 
 

54 
M.V. Dolama and A.G. Rahbar 
 
b. Forwarding Pointer: VLR10 creates a new record for user x and sends a 
message to VLR9 to set a pointer to VLR10. 
c. Virtual Layer and (VL-FP) : When user x enters LA10, a new record is cre-
ated in subVLR5 and VLR9 is deactivated. Furthermore, the information of 
user x is deleted from subVLR2. 
d. Overlap Region with Time Stamp: when a user enters LA10, because LA10 
is in OR5 a new record is created in VLR10 and the TS field of VRL10 re-
cords the time that user has entered LA10. In addition, the OR field of 
VLR10 stores the user x overlap region number (OR5). 
iv. 
When user x moves from position C to position D: 
 
a. Two-tier architecture: VLR5 creates a new record for user x and sends a 
message to HLR to update information. Then, the information in VLR10 is 
deleted. 
b. Forwarding Pointer: VLR5 updates user x information, because the infor-
mation already exists in VLR5. Then, a message is sent to VLR10 to set a 
pointer to VLR5. 
c. Virtual Layer: When user x crosses the boundary of virtual layers in the di-
rection of C to D, VLR10 creates a new record and sends a message to HLR 
to update information. Then, the information in VLR9 is deleted. When a 
user reenters LA5 again, a new record is created in subVLR4 and VLR10 is 
deactivated. Furthermore, the information of user x is deleted from 
subVLR5. 
d. (VL-FP) : When user x crosses the boundary of virtual layers in the direc-
tion of C to D, VLR10 creates a new record and sends a message to VLR9 to 
set a pointer to VLR10. Then, when a user enters LA5, a new record is cre-
ated in subVLR4 and VLR10 is deactivated. Furthermore, the information of 
user x is deleted from subVLR5. 
e. Overlap Region with Time Stamp: when a user enters LA5, because LA5 is 
in OR5 and the information already exists in VLR5, the TS field of VLR5 is 
only updated. 
 
v. 
When user x moves from position D to position E: 
 
a. Two-tier architecture: VLR5 creates a new record for user x and sends a 
message to HLR to update information. Then, the information in VLR10 is 
deleted. 
b. Forwarding Pointer: VLR6 creates a new record for user x and sends a mes-
sage to VLR5 to set a pointer to VLR6. 
c. Virtual Layer and (VL-FP) : Since the movement is in the same virtual 
layer, no update is required.  
d. Overlap Region with Time Stamp: when a user enters LA6, because LA6 is 
in OR5 a new record is created in VLR6 and the TS field of VLR6 stores the 
time that user has entered LA6. Furthermore, the OR field of VLR6 stores 
the user x overlap region number (OR5). 
 

 
Performance Evaluation of the Number of Database Accesses in Cellular Networks 
55 
 
 
Fig. 3. An example of user movement in PCS 
vi. 
Finally, user x moves from position E to position F: 
 
a. Two-tier architecture: VLR11 creates a new record for user x and sends a 
message to HLR to update information. Then, the information in VLR6 is de-
leted. 
b. Forwarding Pointer: VLR11 creates a new record for user x and sends a 
message to VLR6 to set a pointer to VLR11. 
c. Virtual Layer: When user x crosses the boundary of virtual layers in the di-
rection of E to F, VLR6 creates a new record and sends a message to HLR 
to update information. Then, the information in VLR10 is deleted. Then, 
when user enters LA11, a new record is created in subVLR7 and the infor-
mation of user x is deleted from subVLR4. 
d. (VL-FP): When user x crosses the boundary of virtual layers in the direc-
tion of E to F, VLR6 creates a new record and sends a message to VLR10 to 
set a pointer to VLR6.Then, when a user enters LA11, a new record is cre-
ated in subVLR7 and VLR6 is deactivated and subVLR7 is activated. 
e. Overlap Region with Time Stamp: when a user enters LA11, because LA11 
is not in OR5, a new record is created in VLR11 and the TS field is set to the 
current time. In addition, the OR field of VLR11 is set to OR11. Then, a 
message is sent to HLR by VLR11 to update the information. After all, the 
information of user x is deleted from all VLRs in OR5.  
Suppose that user y in LA14 wants to call user x. First, VLR14 is queried, but the rele-
vant information cannot be found. Hence, a message is sent to HLR by VLR14.  
a. Two-tier architecture: From the HLR database, the associated VLR (i.e., 
VLR11) is found and the information is retrieved from VLR11. 
b. Forwarding Pointer: From the HLR database, the associated VLR (i.e., 
VLR5) is found and the information is retrieved from VLR5 by following 
the chains (i.e., VLR6, VLR11). 
 

56 
M.V. Dolama and A.G. Rahbar 
 
c. Virtual Layer: From the HLR database, the associated VLR (i.e., VLR11) is 
found and the information is retrieved from VLR11. 
d. (VL-FP): From the HLR database, the associated VLR (i.e., VLR5) is 
found and the information is retrieved from VLR5 by following the chains 
(i.e., VLR6, VLR11). 
e. Overlap Region with Time Stamp: From the HLR database, the associated 
VLR (i.e., VLR11) is found. Then, the information is searched in OR11 that 
consists of VLR6, VLR7, VLR10, VLR11, VLR12, VLR15, and VLR16. 
Table 2 shows the number of database accesses among different schemes for this 
example. We assume all database accesses have the same cost.  
Table 2. Comparison of database accesses under the example of Fig.3 
scheme 
Two-tier 
architecture 
[6] 
Forwarding 
Pointer [5] 
Virtual 
Layer[7] 
(VL-FP) [8] 
overlap 
region [9] 
Path 
HLR 
VLR 
HLR 
VLR 
HLR 
VLR 
HLR 
VLR 
HLR 
VLR 
A (Initial) 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
A →B 
1 
2 
0 
2 
1 
3 
0 
3 
0 
1 
B →C 
1 
2 
0 
2 
0 
3 
0 
3 
0 
1 
C →D 
1 
2 
0 
2 
1 
5 
0 
5 
0 
1 
D →E 
1 
2 
0 
2 
0 
0 
0 
0 
0 
1 
E →F 
1 
2 
0 
2 
1 
5 
0 
5 
1 
8 
user y call 
user x 
1 
2 
1 
5 
1 
2 
1 
5 
1 
8 
Total  
database 
access 
7 
13 
2 
16 
5 
19 
2 
22 
3 
21 
Normalized 
cost CU,T/CU,V 
7α  + 13 
  2α  +  16 
     5α  +  19 
       2α  +  22 
       3α  +  21 
 
Let the database access cost for HLR (CU,H) be equal to 
CU,H = α × CU,V,                                                      (1) 
where CU,V is the VLR access cost and α ≥ 1. Then, the total database access cost 
(CU,T)  according to VLR access cost can be obtained from Eq. (2).  
CU,T  = CU,H +  CU,V.                                                 (2) 
From Eq.(2), the normalized access cost value of CU,T/CU,V can be obtained (last row 
in Table 2). 
As a result for the example in Fig.3, the Forwarding Pointer has the smallest data-
base accesses in total (18 accesses: 16 VLR and 2 HLR accesses). Therefore, this 
method is better than others. Recall, when the length of Forwarding Pointers goes up 
and when user alternatively moves in boundaries between LAs, this scheme is not a 

 
Performance Evaluation of the Number of Database Accesses in Cellular Networks 
57 
 
good candidate for PCS. Virtual Layer and (VL-FP)  need reconstruction of the PCS. 
Overlap Region scheme reduces database access for updating, however, it needs more 
database accesses for searching the location of users. 
According to Fig.4, with increases the value of α, the two-tier architecture scheme 
has largest cost than the other schemes. Note that the HLR database must be accessed 
for every action in PCS including searching, updating, deleting, and creating new 
record. Since in Forwarding Pointer and (VL-FP) , access to HLR is avoided by using 
the forwarding pointer chain from one VLR to another VLR, the cost of these meth-
ods are smaller than others. 
 
Fig. 4. Normalized cost for example (Fig.3) for different values of α 
4   Conclusion 
In this paper, we have studied five location management schemes and the number of 
database accesses for inserting, updating, deleting, and searching. In addition, we 
have compared these methods with an example. According to our comparison, Over-
lap Region, Virtual Layer, and (VL-FP) have a small number of database accesses 
when a user frequently moves in the boundary of LAs. For searching the user loca-
tion, the Overlap Region has more database accesses than others. Therefore, if the 
user movement in boundaries is more than the user calls, the Overlap Region is the 
best candidate. However, if a user frequently makes calla with other users, the Over-
lap Region could not be a good candidate. With the increases of the mobile users in 
the PCS, the size of the HLR database goes up and the two-tier architecture cannot be 
a good method. This is why the HLR database must be accessed for every possible 
action including inserting, updating, deleting, and searching. 
References 
1. Chen, K.T., Su, S.L., Chang, R.F.: Design and analysis of dynamic mobility tracking in 
wireless personal communication networks. IEEE Transactions on Vehicular Technol-
ogy 51(3), 486–497 (2002) 

58 
M.V. Dolama and A.G. Rahbar 
 
2. Jain, R., Lin, Y.B., Lo, C., Mohan, S.: A caching strategy to reduce network impacts of 
PCS. IEEE Journal on Selected Areas in Communications 12(8), 1434–1444 (1994) 
3. Li, J., Pan, Y.: Dynamic database management for PCS networks. In: Proceedings of the 
21st International Conference on Distributed Computing Systems, Phoenix, Arizona, USA, 
pp. 683–686 (2001) 
4. Pitoura, E., Samaras, G.: Locating objects in mobile computing. IEEE Transactions on 
Knowledge and Data Engeering 13(4), 571–592 (2001) 
5. Jain, R., Lin, Y.B.: An auxiliary user location strategy employing forwarding pointers to 
reduce network impacts of PCS. Wireless Networks 1, 197–210 (1995) 
6. Pitoura, E., Samaras, G.: Locating objects in mobile computing. IEEE Transactions on 
Knowledge and Data Engineering 13(4), 571–592 (2001) 
7. Chung, D., Choo, H., Youn, H.Y., Shin, D.R.: Reduction of location update traffic using 
virtual layer in PCS. In: Kim, W., Ling, T.-W., Lee, Y.-J., Park, S.-S. (eds.) Hu-
man.Society.Internet 2001. LNCS, vol. 2105, pp. 398–410. Springer, Heidelberg (2001) 
8. Chang, C.C., Lin, I.C.: The strategy of reducing the location update traffic using forwarding 
pointers in virtual layer architecture. Computer Standards and Interfaces 25(5), 501–513 
(2003) 
9. Chang, C.C., Lin, I.C., Lin, C.C.: A Novel Location Tracking Scheme for Reducing Loca-
tion Updating Traffic in a Personal Communication System. Wireless Personal Communi-
cations 44, 139–152 (2008) 

BotSpot: Anonymous and Distributed Malware
Detection
P´eter Kenyeres1, Attila Szentgy¨orgyi1, Tam´as M´esz´aros2, and G´abor Feh´er1
1 Budapest University of Technology and Economics Department of
Telecommunications and Media Informatics
2 Budapest University of Technology and Economics Mathematical Institute
Abstract. Widespread usage of broadband Internet connections has al-
lowed the birth of a new threat against service providers and subscribers
as well. Botnets are vast networks of compromised hosts under the con-
trol of single masters who possess the ability to launch crippling denial of
service attacks, send vast quantities of unsolicited e-mail messages and in-
fect thousands of vulnerable systems with privacy-violating spyware and
other forms of malicious software. Our goal is to propose a distributed
architecture and introduce novel algorithms for malicious (potential bot-
net) activity recognition based on network traﬃc statistics generated
by NetFlow. Scalability and robustness were the main principles dur-
ing the design of the architecture. In this paper, we demonstrate that
we are able to reduce the number of NetFlow records signiﬁcantly with
an own aggregation scheme. Furthermore, we are able to detect botnet
participant computers (zombies) with the help of aggregated samples
originating from various local networks, while the algorithms provide ut-
most anonymity to network operators.
Keywords: anonymous, distributed botnet detection, netﬂow.
1
Introduction
In the last decade the global Internet threats transformed considerably from the
previous plain attacks executed individually to those distributed attacks that are
capable of disabling whole infrastructures. This new kind of threat - indirectly or
directly - seeps into the everyday life of millions of people and it does not spare
the business world either. In most cases botnets are responsible for these attacks.
Actually, malicious botnets are multitude of infected computers that are re-
motely controlled by master host via one or more controller hosts. The master
host itself is a computer that is used by the owner of the botnet to send com-
mands to controllers. In most cases, these controllers are infected hosts as well
and take a part in the network’s coordination: relaying the instructions to exec-
utive hosts (bots).
Botnets are used for various malicious purposes such as: distributed denial-of-
service (DDoS) attacks, sending spam, phising or trojan e-mails, serving phising
sites, distributing pirated media, stealing personal information, performing click
fraud, etc. Besides, they also have aggressive exploit activity as they rope in new
vulnerable systems to increase size of the network.
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 59–70, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

60
P. Kenyeres et al.
These kind of attacks are detectable relatively easily. For example, V. Sekar
et al. [17] introduced a triggered multi-stage architecture for Internet Service
Providers (ISP) to detect and mitigate attacks based on using their network
routers’ SNMP [22] and Netﬂow data. A. Garg et al. rather focused on the
end users and they built a Linux-based prototype to guarantee graceful server
degradation in the face of DoS attacks in [23]. Their prototype keeps track of
server and network resources at the network layer and allows aggregate resource
regulation. Despite these solutions the elimination or paralysis of sources of the
attacks raise more serious challenges.
In this paper we introduce a novel security architecture which is capable to
work globally, scalable, eﬃcient and can be anonymous. The architecture relies
on a peer-to-peer (P2P) distributed hash table (DHT) to satisfy the scalability
and the global availability requirements. Because of the high volume of network
traﬃc NetFlow [20] is used to reduce the storage space required for traﬃc logs.
Data anonymization is a key issue in the system, because joined peers do not have
the intention of revealing its traﬃc properties. With our proposed architecture
network administrators will be able to detect new threads and they can react to
the infections eﬃciently.
The remainder of the paper is organized as follows. In Section 2 the related
work is presented. In Section 3 the system model is introduced including the
system architecture and the diﬀerent type of nodes participating in it. In Sec-
tion 4 the components of our system and realization of the design priorities are
presented. In Section 5 eﬃciency of algorithms are evaluated. Finally, some open
questions are discussed and the results are summarized in Section 6.
2
Related Work
Researchers have proposed many diﬀerent approaches to detect botnet behaviour
in a monitored network. At ﬁrst, the aim merely was the recognition of the
existence of a malicious (potentially botnet) behaviour, particularly in case of
the botnets those use IRC [21] and HTTP protocols based Command and Control
(C&C) channels [9] [10] [11] [12]. An anomaly-based passive analysis algorithm
is presented in [9] which uses trigger based reports of suspicious host activities
to detect IRC controllers. Rishi [10] creates signatures by the known IRC bot
nicknames to identify IRC botnets. [11] [12] integrate the recent results of the
machine learning and data mining techniques to detect botnet activities. In
BotSniﬀer [18], it is suggested that use of spatial-temporal correlation in network
traﬃc and application of statistical algorithms to recognize botnets in the local
network. The Bothunter [16] also shows correlation based approach of the botnet
detection process.
In [19], simultaneously sent group activity patterns are searched in DNS traﬃc
to ﬁnd distributed bots. According to [24], the DNS black lists give another
opportunity of bot identiﬁcation. By the fact that spammers use these lists
frequently to ﬁnd which of their hosts are banned, analysis of such queries can
lead to additional bot identiﬁcation.

BotSpot: Anonymous and Distributed Malware Detection
61
However, due to the fact that full payload of the packets are not always known
furthermore, their processing and storage may cause problems in the case of high
speed networks. New solutions were born for overcoming these issues. [13] [14]
use compression methods before the classiﬁcation of the network traﬃc.
Beside all these, several methods are founded on detecting and blocking the
C&C channel of the botnet [10] [11] [16], and by these processes initially fast
and eﬃcient results can be reached. At the same time, botnets have a consid-
erably ﬂexible structure which can change its behaviour in wide spectrum very
quickly. Further problem of the currently existing solutions, almost all of them
are designed to use data from one single network only.
We may say that botnets use basically distributed architecture (except that
case when owner of the botnet - also as known as botmaster - controls the bots
individually, but this situation is out of the scope of our paper). And the consid-
erable part of the attacks embittering our everyday life (spam and particularly
DDoS) are successful, only if they are executed with many computers in near
identical time from many distinct places. According to that extended and dis-
tributed protection is desirable, which can be reached by collecting data from
diﬀerent local networks. Thus, the whole malware network’s recon, disablement
and elimination become quicker and easier. However, it brings up the following
problems: the sample recognition can be quite diﬃcult in the networks because
of the diﬀerent structure and their unique traﬃc patterns. The users’ resistance
may mean additional diﬃculty, if their interest is connected with that their net-
works’ construction or their communication proceeding not to be exposed.
Currently, this area of the botnet issue is quite open. Exactly, this is where
BotSpot can ﬁll the vacuum and can prove the necessity of a distributed ar-
chitecture which provides eﬃciency, robustness and utmost anonimity. We put
steps to organizing the defence based on the separately collected network traﬃc
data. Furthermore, the anonymity guaranteed by our algorithms helps to win
the users’ conﬁdence.
3
System Architecture
The proposed architecture is depicted in Figure system. and consists of four
diﬀerent types of nodes: agents, honeypots, data processors and distributors.
The ﬁrst three are connected via a P2P network designed for distributed data
search and transfer considering scalability issues. The applied overlay network is
structured and implements a Distributed Hash Table (DHT). This property is
required to attach as many nodes to the system as possible to reach a globally
available and distributed malware detection system. The roles of the components
in the system are the following:
– Agents: Computers in enterprises and home users are connected to the In-
ternet via a local gateway. This gateway separates their own LAN from the
Internet. In enterprise networks or just in a SOHO environment gateways are
capable to dump network traﬃc. Agents can be placed next to gateways, be-
cause it is a requirement for agents to collect and process traﬃc information

62
P. Kenyeres et al.
Fig. 1. System architecture with agents, honeypots data processors and distributors
of their LAN. Agents are connected to the P2P network and can communi-
cate with other agents. Nevertheless, agents have another important role in
the network: they monitor the traﬃc and try to detect malicious activities
coming from its own subnet, such as DoS attack or spam. When an agent has
detected a new threat, it should search the current designated data processor
using the overlay network and has to send the anonymized and compressed
traﬃc data of the suspicious node to the ﬂow processor.
– Honeypots: These entities mark the suspicious traﬃc. When a new threat
was detected, honeypots should create traﬃc traces of the malware, mark
their command and control (C&C) channel and have to send the marked
and anonymized trace to the current data processor.
– Data processor: It is a designated node of the P2P network. However, only
one data processor is presented in the network at same time, but it is chang-
ing at times. It collects the reports of malicious activities and the correspond-
ing anonymized and compressed ﬂows from agents and the anonymized and
marked ﬂows from honeypots. Its task is to create clusters from the data,
evaluate the results and if malicious activities are detected it will have to
send the network traces to a distributor.
– Distributors: Distributors are responsible for collecting anomalous traces and
sharing them with the agents. Distributors are independent from the P2P
network. They accept requests from the data processors and serve the avail-
able sample updates to the agents.
3.1
Overlay Network
Robustness is a requirement for the architecture to eliminate DoS attacks. Re-
sources (for instance: zombie networks) controlled by the attacker can be divided
into two sets. One set of the zombies disables the detection system by distributed
denial of service attack (DDoS) while the other set of infected computers commits

BotSpot: Anonymous and Distributed Malware Detection
63
the originally planned attack. Several papers - for example [6] [7] - describe methods
to defend a computer against ﬂood attacks. Our transfer solution applies a protec-
tion against DoS as well, by using the Secret Overlay Service (SOS) [8], because this
method guarantees the further high of design priorities, such as ﬂexibility, scala-
bility and fairness in task distribution.
Basically, SOS is a large distributed ﬁrewall which has two essential parts.
First of all, targets are protected by sophisticated ﬁlters against ’unauthorized’
traﬃc. On the another hand, a multilevel hierarchy endeavors to hide the ac-
cess of target nodes from the attacker. This hierarchy with ﬁltering mechanisms
makes this scheme robust against the DDoS attacks, because each component
is easily replicated within the architecture. If the attacker blocks a node A its
duties will be reassigned by the DHT and node B will take charge. Hence, the
attacker has to block node B also, while his attack against node A can not
to be stopped. Otherwise, that one who is released rejoins to the system. The
resistance of a SOS network against DoS attacks considerably depends on the
number of nodes that participate in the overlay.
4
Methodology
In this chapter, we describe the details of the aformentioned task, such as task
distribution, ﬂow aggregation, sample generation.
4.1
Task Distribution Method
For the analysis of the collected attack samples a responsible node is appointed
in the DHT. The current data processor is determined by locally stored and
maintained seed value. This seed value is denoted by S and it serves as input to
the hash function of the DHT. The value received in this manner always selects
one node from the DHT. The chosen node transmits the packets towards to the
data processor by SOS routing mechanism.
The next value of S has to be locally computable to reduce operational mes-
sage overhead. The designation of the data processor by changing value of S may
happen according to the following methods fundamentally:
– Time interval: in this case, S is actually a numerator increased after a certain
interval. Typically, this interval means a couple of hours.
– Victim network address: at this time the IP address (S) of the attacked
network is used as input of the hash function.
– Attack types: certain attack types are predeﬁned (e.g.: scanning, DDoS,
spam, etc.). All these types are associated to unique values. Finally, these
values are going to serve as S.
4.2
Flow Aggregation
Our method is based on NetFlow [20] logs. The most important ﬁelds in a Net-
Flow record are the source and destination IP addresses, the source and destina-
tion port numbers and the transport protocol, since these deﬁne a session. If four

64
P. Kenyeres et al.
of them (including the IP addresses and the protocol) are the same in two Net-
Flow records, namely the same IP addresses are communicating with the same
transport protocol, and at least one of them on the same port, we can assume
that the two records belong to the same session and it is unnecessary to treat
them separately. In the ﬁrst step ﬂow regrouping can be done by putting ﬂow
records with the similar connection parameters to the same group to represent
each group by a single ﬂow.
Note that if it is true for two ﬂows A and B that srcIPA = destIPB,
destIPA = srcIPB, srcPortA = destPortB OR destPortA = srcPortB, then
these ﬂows can be aggregated, because these ﬂows represents the diﬀerent direc-
tion of the same connection. The up and down direction can be chosen arbitrary.
First, outlier ﬁltering is applied for both directions to exclude anomalies and
irrelevant data. This ﬁltering due to all dimensions respectively is done by com-
puting the m mean and the ς variance of the values. If the variance is relatively
high (if
ς
m is grater than a ﬁxed ε0), then the ﬂow with the most outlying value
will be discarded. This step is iterated until there will be no more outliers. The
last step is the aggregation of the remaining ﬂows in each group to obtain a
representant. The values for packets, octets and active time will be added up,
the earliest start time and the latest end time will be selected, and 5 more values
will be computed: number of ﬂows aggregated, mean packet size, mean active
time, duration (the time elapsed between the earliest start time and the latest
end time), up/down+down/up (up stands for the sum of octets in the ﬂows
with direction up and down). This 5-tuple will represent a group. The IP ad-
dresses, port numbers and the transport protocol are omitted to get a kind of
anonymity.
4.3
Flow Processing and Sample Generation
The incoming aggregated NetFlow logs have to be classiﬁed to obtain ﬂow sam-
ples belonging to the botnet traﬃc we want to detect. Logs are sent by agents
which are detected an attack. If this agent is a honeypot, the traﬃc logs will
contain botnet traﬃc related ﬂows (C&C channel communication and attack).
These ﬂows are trusted in the sense that these are originated from a trusted
entity and can be used as a sample of the botnet traﬃc. For this reason these
ﬂows are referred as labelled ﬂows. The ﬂows captured by honeypots that do not
belong to the C&C channel can be labelled diﬀerently or simply omitted. The
classiﬁer should handle the huge dataset size and the consequences of multiple
botnet activities. To address these challenges a semi-supervised learning tech-
nique is applied, which is a modiﬁed version of the general method discussed in
[1]. Clustering [2] is applied to partition the data set that consists of labelled and
unlabelled ﬂows. Several previous works [3] [4] [5] demonstrated that clustering
of Internet traﬃc using ﬂow statistics has the ability to group together ﬂows
according to the same traﬃc. In this paper we applied the X-Means algorithm
[2], since it is relatively simple, easy to implement and oﬀers fast computation,
demonstrated good results in previous works e.g. [3] and converges in a few
number of iterations. After clustering supervised learning is applied to label

BotSpot: Anonymous and Distributed Malware Detection
65
the clusters using the labelled ﬂows. Unlabelled ﬂows are used to improve the
precision of the classiﬁer.
It is not our purpose to identify all of the clusters, our aim is just to select
those, which belong to botnet communication. Now we will discuss the details
of the classiﬁcation method.
Cluster Identiﬁcation and Sample Generation. The output of X-Means is
a set of vectors, which are the centers of the clusters. If a vector x is given, it is
assigned to the cluster with the nearest center. Next step is the identiﬁcation of
the botnet traﬃc related cluster(s). A probabilistic method is used, similar to the
one described in [1]. Let pi be the probability of the event that the ith cluster,
Ci, i = 1, 2, . . . , K is the cluster belonging to the botnet communication. These
pi probabilities are estimated with the maximum likelihood estimate ni
n , where
ni is the number of labelled vectors in the ith cluster and n is the total number
of labelled vectors. According to these estimated probabilities the cluster with
the highest probability is considered to belong to the botnet communication.
For the sample we consider the cluster center and calculate an ε threshold value.
This value has the largest radius such that the sphere around the center of this
cluster with such radius is disjoint from all of the spheres around the other
cluster centers with the same radius. So the sample will be the (C, ε) pair.
Multiple Sources. If the NetFlow log contains only one botnet trace, it can be
assumed that the same cluster will contain the most of the labelled ﬂows with
high probability. If it is not the case, then several clusters according to each
source with relatively high estimated probability will be available. In that case
all clusters over a p0 (a priori chosen) probability can be selected, and a sample
can be constructed for all of them. Each of them will belong to a diﬀerent botnet.
4.4
Sample Redistribution
Sample redistribution can be performed via distributors in the way that agents
connect periodically to make regular updates or distributors can push new sam-
ples to the agents. Distributors could share the samples via HTTP protocol using
e.g. a web service or any standardized way. However, sample distribution is an
important part of the system, there are existing ways to perform this operation
such as [26] [27]. This is the reason why it is not the key issue of the paper. In
addition, we note that further investigation is needed to ﬁnd the best solution
for the problem.
4.5
Command and Control Channel Recognition
After agents have downloaded the samples they can apply the C&C channel
recognition procedure. First of all, all agents have to aggregate their ﬂows to
present a similar data structure to the aggregated sample. It not just decreases
the size of the data set, but oﬀers relatively fast search and preserves anonymity
as well.

66
P. Kenyeres et al.
To select all botnet related ﬂows from the agent’s ﬂow set the clustering
method discussed in Section 4.3. can be applied. Let x be a vector from the
agents aggregated ﬂow set. Then the following steps are required:
1. Calculate the distances of the feature vector x from the centers in the sam-
ples: d1 = d(x, C1), . . . , dr = d(x, Cr)
2. Select an index i, if there exists such that di < εi (Note that if such an index
exists, then it will be unique.)
We can assume that this vector belongs to the corresponding cluster and so to
the botnet communication. It’s because if this vector is added to the training
data set, then after the next iteration of X-Means the vector will be an element
of this cluster.
5
Experimental Evaluation
We have implemented BotSpot algorithms in native C (a code of approximately
5000 lines in total) and we have created a testbed network in the laboratory
of the university to collect Netﬂow logs which contain certain malware traﬃc.
The input data of our test scenarios were made up of following three sources
fundamentally:
– Netﬂow logs collected from the campus-wide wired and wireless network
of the Department of Telecommunications and Media Informatics at the
Budapest University of Technology and Economics (BME). The each log is
10-minute long and they were collected in the time period 17-19 April 2008.
The total size of the data was more than 5 GB, and it contains more than
100 million ﬂows.
– Malwares were captured by our HoneyPot [25] during half year ran, such as
Worm.PadoBot, Worm.Korgo, Sasser, etc.
– Furthermore, open source botnets which are available in the Internet, like
SdBot, AgoBot, Beatrix.
At ﬁrst, the performance of the aggregation scheme was tested separately by
each campus log. For all the 432 of 10-minute logs the compression ratio of the
algorithm was between 0.3 and 0.35. Which means it reduced the size of the data
set by it’s 2/3. The average single-threaded preprocessing running time for one
10-minute log was less than 10 seconds. Figure 2 shows the size of the original
and the aggregated data sets in the time period of one day.
Next, we did the aggregation for longer time intervals. Clearly, it improved
the compression ratio, because in a longer time interval more ﬂows were grouped
together. Figure 3a shows, how this ratio is improved by increasing the length
of the time interval observed.
In contrary to the compression ratio, when the length of the observed time
interval was increased, the running time increased as well. Figure 3b shows this
phenomenon.

BotSpot: Anonymous and Distributed Malware Detection
67
Fig. 2. Size of the original and the aggregated data sets over one day
Fig. 3. The compression ratio and running time over diﬀerent time intervals
According to the results we can state that the compression ratio will be grow-
ing if longer time interval is selected. When the compression ratio increases,
the processing time increases as well. Beyond a certain point this kind of delay
can not be tolerated any longer and at this point a threshold can be stated.
This threshold takes place, where the growing of the processing time turns from
linear to exponential. In addition, it depends on the resources of the running
environment. Our results was generated by a desktop computer with a Pentium
Core2 2.4 GHz processor and 2 GB of RAM and the threshold was at approxi-
mately 7 hours. However, recent study of the Storm botnet [15] evinced that bots
are short-lived: it takes them just over 4 minutes after boot-up until receiving
a control message, most of them remain in operation only for a little under 4
hours. Thus, approximately seven-hour aggregation interval is suﬃcient for the
detection.
Further, we tested the C&C channel recognition algorithm. The data set came
from a laboratory testbed. We simulated three virtual LAN networks, installed

68
P. Kenyeres et al.
Fig. 4. False positive rate depending on number of the samples
with Windows XP operating systems, and we infected them with diﬀerent botnet
clients. The three subnets were connected to a gateway, that was in connection,
a botnet controller and a victim as well and for the IRC botnet clients an IRC
server was created also. Besides the legal traﬃc generated by the computers of
the subnets, such as FTP, HTTP and e-mail, we simulated an attack against
the victim directed by the botnet controller. The sample for the channel was
generated from the NetFlow log by BotSpot sample generation process discussed
in Section 4.3.
Figure 4. shows the connection between the number of the received malware
samples and the eﬃciency of the recognition. The recognition becomes more
eﬃcient (number of the false positive elements signiﬁcantly decreasing) by in-
creasing number of the malware samples.
Fig. 5. Processing time with aggregation scheme and without it

BotSpot: Anonymous and Distributed Malware Detection
69
Finally, to prove necessity of the aggregation scheme we repeated the previous
test without it. Figure 5. shows that the process time is growing in excessive rate
without the aggregation step, the system would not be able to handle the nec-
essary data quantity in tolerable time. The accomplishment of the aggregation
step is necessary to the treatment of networks existing in the practice.
6
Conclusion
In this paper, we have shown an architecture for anonymous and distributed
malware detection. After the basics of system we presented our solution propos-
als on all emerging piece of the problem, such as provide scalability, robustness
and anonymization together with generate and distribute malware sample in
multi-domain environment. In addition, we proposed two algorithms: one for
the reduction of the huge amount of network statistical data and another for the
detection with the help of samples which was generated from the attacks. We
demonstrated the strength of the algorithms: i) the aggregation method reduced
the NetFlow entries to one third in practice ii) the detection algorithm was able
to ﬁnd botnet clients using the aggregated samples. We note that these samples
provide anonymity in that sense they do not contain any kind of valid IP infor-
mation. Consequently, each and every user can be sure that their network traﬃc
is not revealed totally. Hereby, spying usage of the system is not possible. As
a result, there is no need to establish mutual and unconditional trust among all
participants. This property of the architecture can facilitate to make extensive
use of the system.
Our future work includes adding a more sophisticated algorithm for the botnet
C&C channel recognition and botnet detection algorithms. Although, gathering
NetFlow data from real networks, where the traﬃc were reliably identiﬁed, is a
quite complicated task, it is necessary to fully validate the results of the detection
and prevention algorithms.
References
1. Erman, J., Mahanti, A., Arlitt, M., Cohen, I., Williamson, C.: Oﬄine/Realtime
Traﬃc
Classiﬁcation
Using Semi-Supervised Learning. Performance
Evalua-
tion 64(9-12), 1194–1213 (2007)
2. Pelleg, D., Moore, A.: X-means: Extending K-means with eﬃcient estimation of the
number of clusters. In: 17th Int. Conf. on Machine Learning, pp. 727–734 (2000)
3. Erman, J., Arlitt, M., Mahanti, A.: Traﬃc Classiﬁcation using Clustering Algo-
rithms. In: SIGCOMM 2006 MineNet Workshop, Pisa, Italy (2006)
4. McGregor, A., Hall, M., Lorier, P., Brunskill, J.: Flow Clustering Using Machine
Learning Techniques. In: Barakat, C., Pratt, I. (eds.) PAM 2004. LNCS, vol. 3015,
pp. 205–214. Springer, Heidelberg (2004)
5. Zander, S., Nguyen, T., Armitage, G.: Automated Traﬃc Classiﬁcation and Ap-
plication Identiﬁcation using Machine Learning. In: LCN 2005, Sydney, Australia
(2005)

70
P. Kenyeres et al.
6. Peng, T., Leckie, C., Ramamohanarao, K.: Protection from distributed denial of
service attacks using history-based IP ﬁltering. In: ICC, vol. 1, pp. 482–486 (2003)
7. Kargl, F., Maier, J., Weber, M.: Protecting web servers from distributed denial of
service attacks. In: Int. World Wide Web Conf., pp. 514–524. ACM, Hong Kong
(2001)
8. Keromytis, A.D., Misra, V., Rubenstein, D.: SOS: Secure Overlay Services. In:
ACM SIGCOMM, Pittsburgh, USA, pp. 61–72 (2002)
9. Karasaridis, A., Rexroad, B., Hoeﬂin, D.: Wide-scale botnet detection and charac-
terization. In: HotBots 2007, p. 7. USENIX Association, Cambridge (2007)
10. Goebel, J., Holz, T.: Rishi: Identify bot contaminated hosts by irc nickname eval-
uation. In: HotBots 2007. USENIX Association, Cambridge (2007)
11. Livadas, C., Walsh, R., Lapsley, D., Strayer, W.T.: Using machine learning tech-
niques to identify botnet traﬃc. In: 2nd IEEE LCN WoNS 2006, Tampa, USA
(2006)
12. Masud, M.M., Gao, J., Khan, L., Han, J., Thuraisingham, B.: Peer to peer botnet
detection for cyber-security: a data mining approach. In: 4th workshop on Cyber
security and information intelligence research. ACM, Oak Ridge (2008)
13. Reiter, M.K., Yen, T.-F.: Traﬃc aggregation for malware detection. In: Zamboni,
D. (ed.) DIMVA 2008. LNCS, vol. 5137, pp. 207–227. Springer, Heidelberg (2008)
14. Wehner, S.: Analyzing worms and network traﬃc using compression. Journal of
Computer Security 15, 303–320 (2007)
15. Kreibich, C., Kanich, C., Levchenko, K., Enright, B., Voelker, G.M., Paxson, V.,
Savage, S.: On the Spam Campaign Trail. In: 1st USENIX Workshop on Large-
Scale Exploits and Emergent Threats (2008)
16. Gu, G., Porras, P., Yegneswaran, V., Fong, M., Lee, W.: BotHunter: Detecting
malware infection through ids-driven dialog correlation. In: Security 2007 (2007)
17. Sekar, V., Duﬃeld, N., Spatscheck, O., Van Der Merwe, J., Zhang, H.: LADS:
Large-scale Automated DDoS detection System. In: USENIX ATC, pp. 171–184
(2006)
18. Gu, G., Zhang, J., Lee., W.: BotSniﬀer: Detecting Botnet Command and Control
Channels in Network Traﬃc. NDSS (2008)
19. Choi, H., Lee, H., Lee, H., Kim, H.: Botnet Detection by Monitoring Group Activ-
ities in DNS Traﬃc. In: IEEE CIT, Aizu-Wakamatsu, Japan, pp. 715–720 (2007)
20. Cisco Systems NetFlow Services Export Version 9, RFC 3954 (2004)
21. Internet Relay Chat Protocol, RFC 1459 (1993)
22. A Simple Network Management Protocol (SNMP), RFC 1157 (1990)
23. Garg, A., Reddy, N.: Mitigation of DoS attacks through QoS regulation. Micropro-
cessors and Microsystems 28(10), 521–530 (2004)
24. Ramachandran, A., Feamster, M., Dagon, D.: Revealing botnet membership using
dnsbl counter-intelligence. In: 2nd Workshop on SRUTI (2006)
25. Spitzner, L.: Honeypots - Tracking hackers. Pearson Education, Inc., London (2003)
26. Androutsellis-Theotokis, S., Spinellis, D.: A survey of peer-to-peer content distri-
bution technologies. ACM Computing Surveys (CSUR) 36, 335–371 (2004)
27. Turrini, E., Panzieri, F.: Using P2P Techniques for Content Distribution Inter-
networking: A Research Proposal. In: 2th Int. Conf. on P2P Computing, p. 171
(2002)

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 71–86, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Throughput Maximisation of Different Signal Shapes 
Working on 802.16e Mobile Multihop Network Using 
Novel Cognitive Methods 
Barbaros Preveze1 and Aysel Şafak2 
1 Cankaya University Electronics and Communication Engineering,  
Ogretmenler cad. No:14, 06530, Balgat Ankara, Turkey 
2 Başkent University Electrical and Electronics Engineering, 
Eskişehir yolu 20. Km Bağlıca kampüsü, Ankara, Turkey 
b.preveze@cankaya.edu.tr, asafak@baskent.edu.tr 
Abstract. This study proposes cognitive methods for managing the multimedia 
packet traffic of 802.16e system providing the dynamic  spectrum sharing   
by TDM & OFDMA under heavy traffic conditions.  The cognitive tech-
niques are used in order to maximize the  throughput of a system having 
different signal shapes such as  video,  voice or data packets. All real time  
voice and video packets transmissions have been provided with full success 
where the loss rate of data packets has been minimized using the cognitive 
methods. By these methods buffers are managed successfully for the given pa-
rameters, optimum buffer sizes have been formulized and it’s shown that we 
brought the throughput up to  its calculated theoretical limits for multi-hop 
mobile wimax (802.16e). 
Keywords: cognitive, cooperation, multimedia, throughput, packet loss, 
802.16. 
1   Introduction 
In a wireless mobile network , all  the nodes try to send packets in different signal 
shapes such as video, voice or data packets to random destination points through a 
calculated multi-hop path, so they have to share the spectrum among each other. 
In this study OFDMA and TDMA spectrum sharing techniques are used as in 
802.16.e [1,2,3,4]. 
In our scenario there are 6 relay nodes in a mobile multi-hop network moving in 
10 km2 area, and they’re all having a real time video and voice conversation  with 
other random nodes in the network. Every node generates heavy traffic load on 
the network by generating and trying to transmit as much data packets as it can. On 
the network side some new proposed cognitive methods are used in addition to OF-
DMA subchannelisation and TDM to achieve the traffic without any real time 
voice and data packet loss while minimizing the number of lost data packets. The 
parameters in cognitive networks are updated based on the active monitoring of radio 
frequency spectrum, user behavior and network state [5] and the nodes can have 

72 
B. Preveze and A. Şafak 
 
any kind of updated information about other nodes in the network and use the in-
formation they have, for end to end performance purposes of whole network. 
2   Theoretical  Analysis  of  Multi-Hop  Mobile  Wimax  (MWM) 
Network 
According to the equations (1-16) given in Table 1 and using the same input parame-
ters  in  the  simulation  program  ,the  maximum  possible  throughput  that  a sys-
tem may provide can be determined theoretically as; 
Using B (Bandwidth)= 10 MHz (between 2.3 GHz & 2.4 GHz) [4] ,choosing 
modulation = 64 QAM , DTPS (Data Packet size) =150 bytes, VCPS (Voice Packet 
size)=10 bytes, VDPS (video Packet size)=512 bytes, , FL(Frame Length) =5 ms 
(1/0.005 =200 frames per second), FEC (Forward Error Correction) Rate =3/4, and 
using rest of the parameters as the standards defined [1] or calculating those parame-
ters using the formulations in Table 1. 
With the selected parameters shown above the standards give us [1] ; 
NOSC = FFT =1024 (number of sub carriers) 
 
(18) 
NODS = 720  (number of sub carriers) 
 
 
(19) 
BPS = log2 (64) = 6  (bits per sub carrier) 
 
 
(20) 
NOS = 30  (number of subchannels) 
 
 
(21) 
OFDM symbols / frame = 44   
 
 
 
(22) 
OFDM saymbols / frame = 48  
 
 
 
(23)  
Since 64 QAM is used for modulation each sub carrier will have 6 bits of data,    
BPS = 6x720 = 4320 bits from eq. 4 
 
 
(24) 
Maximum allocatable Bandwidth per subchannel = 4320
144
30
=
 bits             (25) 
using ¾ FEC, MAU = 144x ¾ = 108 bits for a subchannel per frame               (26) 
Since FL value is chosen to be 5 ms which has been selected by the Wimax forum 
for initial certification profiles [1].  if a device is allotted to a single subchannel in 
every frame the transmission rate through the subchannel will be; 
Subchannel capacity(SCC) = 44x108x200 = 950400 bits/second      (27) 
Multiplying this value by number of subchannels, i.e. for the complete bandwidth 
Max. spectrum usage  = 950400 bits / second x 30  = 28, 512 x106 = 3564000 bytes / s     (28) 
which also matches with the calculations made in [1] and the standards in [6]. 
Graphical representations of maximum data rates of different input parameters are 
presented on figures 1(a),1(b) and 1(c). 

 
Throughput Maximisation of Different Signal Shapes 
73 
 
 
Table 1. The system parameters for mobile WIMAX  defined by the standards and used in the 
simulation and formulations [1] 
NOS : Number of subchannels 
tan
defined bythes
dards          [1] 
GR  :Guard Rate    
1 8
GR =
                              (1) 
TS :Sampling Period 
1
91.4 mirosecond
TS
FS
s
=
=
    (2)      
 FT  : Total used Frequency  
1
9,718 kHz
FT
TST
=
=
            
 
(3) 
TST :Total symbol Time 
9
8 102.8 microsec.
TST
xTS
=
=
      
 (4) 
TSG :Sampling guard interval 
8
11.4 miroseconds
TSG
TS
=
=
     
 
(5)     
SPF:Symbols per frame 
SPF
FL TST
=
                         (6)    
FL : Frame Length 
tan
 
2.5,4,5,8,10,12.5  20  
defined bythes
dardsas
or
ms  
[1] 
N : Number of nodes 
Defined as 6 in our simulation  
SRPS :Saymbol rate per second 
SRPS
FPSxDSPF
=
            (7) 
FPS :Frames per second 
1
FPS
FL
=
                         (8)    
SCDRSWF : subchannel data rate in a 
second with FEC          
(
)
SCDRSWF
SCDRFWFx FPS
=
(9)    
SCDRFWF : subchannel data rate in a 
frame with FEC 
SCDRFWF
DSPFxMAU
=
            (10) 
DSPF :Data symbols per Frame 
tan
defined bythes
dards   [1] 
MAU : Minimum Allocatable Unit 
BPSWF
MAU
NOS
=
                           (11) 
SR : Sampling Rate  
B x OSF
SR =
                               (12) 
B : Bandwidth  
input parameter  
OSF : Over Sampling factor 
8/ 7
OSF =
                                 (13) 
DRPS : Data rate per second 
DRPSWF
FECxDRPS
=
           (14) 
 FEC: Forward Error Correction 
1 2,3 4,5 6
canbechosenoneof
 [1] 
DRPS : Data rate per second  
DRPS SRPSxBPS
=
                              (15)    
BPSWF :Bits per symbol with FEC  
BPSWF
FECxBPS
=
                     (16) 
BPS : Bits per symbol 
2
log (
)
BPS
QAM xNODS
=
  
(17) 
NODS  :No of data subcarriers 
tan
defined bythes
dards     [1] 
VDPS :Video packet size   
input parameter  
VCPS :Voice packet size   
input parameter  
DTPS : Data packet size 
input parameter  
DS     : Delay Spread 
input parameter  
FS      : Frequency spacing 
10.94
FS
MHz
=
 
FFT    : Number of subcarrers 
tan
defined bythe s
dards         [1] 
AHC  : Average Hop count  
 
 
 
calculated bythe simulation  
MHC : Maximum hop count 
tan
defined bythe s
dards  
BPSC : Bits per subcarrier 
  

74 
B. Preveze and A. Şafak 
 
 
                        (a)                                     (b)                                         (c) 
Fig. 1. Maximum Data rates (in bytes) versus (a) Bandwidth (10 MHz chosen)  ,(b) FEC 
rate (3/4 chosen) (c) QAM mode (64 chosen ,log2
64=6) 
3   Packet Carriage of Multi-hop Mobile Wimax 
3.1   Real time Multimedia Packet Carriage of MWM 
For video conversation the nodes are considered to generate 50 picture frames per 
second, which will mean 50 of 200 OFDM frames within a subchannel in a sec-
ond will be used to send these frames. By dividing the SCC we had in eq. 27 to usage 
rate; (950400 Mbits / sec) 
(
)
950400 Mbits / sec    
237600 
 
29700 
(200 / 50)
videorate
bits
bytes
=
=
=
              
(29)
 
which can be handled by a device using H.264 video codec. Since we have 30 
subchannels in each frame and since we have 200 frames in a second (from eq 6) 
VDPS = Subchannel Capacity in a frame (SCCIF) 
 =3564000 bytes / s  (from eq 28) / 30 / 200 = 594 bytes  (30) 
for which 1 of 4 (50 /200) subchannel allocation in every frame will be enough for 
voice conversation. On the other hand if we consider that ; each node in the net-
work is having voice conversation in parallel to video conversation, and 16 kbps 
voice packets can be considered due to low latency requirements. This means; 
VCPS = 16 kbits / 200 = 80 bits = 10 bytes [1]                  (31) 
For which one subchannel allocation in every frame will be enough. 
 

 
Throughput Maximisation of Different Signal Shapes 
75 
 
3.2   Non-real Time Data Carriage of MWM 
In order to overcome the heavy traffic conditions, the nodes will use different 
number  of  subchannels  (SC)  and  the  number  of  subchannels  allocated  for  
data transmission for that frame can be calculated by ;  
 
                    (SC –  ( NOHx 1 SC for voice conversations + 1 or 2 SC used by videopackets ) 
   i.e. (30  –  (6x1 + 1 or 2) = 22 or 23 subchannels / node                                     (32) 
where number of SC used by different data types is known by the system. Since 
only 1 of 4 SC is used by each node for video conversations 1 or 2 SC will be used 
in a frame by the nodes for video packet transmission, so the nodes will transmit their 
data packets through 22 or 23 subchannels. The optimum data packet size, for mini-
mum bandwidth wasting, can be calculated by; 
(SCDRFWF)
594
198 
594
SCDRFWF
150
ref
DTPS
bytes
floor
floor
DTPS
=
=
=
⎛
⎞
⎛
⎞
⎜
⎟
⎜
⎟
⎜
⎟
⎝
⎠
⎝
⎠
                                                         
(33)
 
Where DTPSref       means the DTPS that we want to use in our system and  DTPS is the 
optimum packet size value nearest to DTPSref that exactly fits to the subchannel.  
3.3   Theoretical Determination of Maximum Bandwidth Usage Upperbound 
If the capacity of one subchannel is not multiple of size of a data packet, the last 
packet can not be placed in the subchannel ,this bandwidth can not be used and it will 
be wasted; since 6 of 30 subchannels are used for voice packet transmissions and 1 
or 2 subchannels are used for video packet transmission in our system, left 22 or 23 
of 30 subchannels will be used by data packet transmissions. So if 1 SC is used for 
video packets; 23 SC will be used for data packets of all system. Then if the packet 
size is chosen to be 150 bytes; 
wasted bandwidth by data transmission = ((594 bytes ) − (150 bytesx3) ) x23 = 3312 bytes   (34) 
will be wasted  in 23 subchannels in a  frame. If the  DTPS = 198 bytes, then 
a subchannel will be able to carry 3 complete packets without any bandwidth  
wastage. 
Note that in real time multimedia communication if some other waiting transit 
voice packets came from other nodes exist, they can also be sent using the 
same subchannel with other voice packets, but the rest of the subchannel bandwidth 
will again be wasted .This condition is also valid for video packets if and only if the 
video packet is chosen smaller than half of subchannel capacity. Considering this 
state the maximum possible data transmission via the whole channels can be calcu-
lated as in eq 36, since maximum theoretical hop count can at most be 5 hops for a 
network with 6 hops each packet can stay in the network for at max 5 frames dura-
tion. As a result; Totally 6 newly generated voice packets in the current frame + 6 
voice packets generated in last 5 frames = 6 + 5x6 = 36 voice packets will be sent 
through the subchannels in each frame by all nodes.  
 

76 
B. Preveze and A. Şafak 
 
The total capacity of the full channel is calculated as 3564000 bytes/s in eq 28 since 
the system allocates one separate subchannel for voice packets of each node in each 
frame and since 36 voice packets will be sent at max in each frame, totally 36x10 bytes 
=360 bytes will be transmitted by the nodes, via 6 subchannels allocated for voice 
packets of 6 nodes. That is; (6 SC x 594 bytes)-(10 bytes x36 Voice packets) =3204 
bytes will be wasted by the nodes in a frame, where wastage in a second (200 frames) 
is equal to ; 
 (6x594) − (10x36)) x 200 = 640800 bytes                          (35) 
On the other hand when the nodes will use only 1 subchannel at half  of every 4 
frames, 23 subchannels will be allocated for data packets and ½ packet size will 
be wasted  in 2  frame  which  equals to  1  subchannel  per  4  frames  wastage or  
0.25 packets/frame x 200 frames x198 bytes =9900 bytes.  Since we selected the size 
of video packets equal to SCC with maximum quality that a subchannel can handle 
as calculated in eq 30, it will waste no bandwidth. The size of data packet is 
also selected such that multiple of it exactly fits in a subchannel and it will use the 
whole subchannel with no bandwidth wastage in case of full load. As a result; the 
maximum possible successful bandwidth usage that can be provided with given pa-
rameters and under given conditions is; 
3564000 bytes – 640800 bytes (eqs 28, 35) -9900  =  2913300 bytes = 2.91x106     bytes    (36) 
3.4   Calculation of Optimum Buffer Size 
Choosing the buffer size of the system is a critical point, where choosing larger buffer 
sizes will require more memory, and stores more packets into the buffer which causes 
the packets wait for longer time in the buffers queues on the other hand choosing  
small  buffer  sizes  will  cause  more  packet  losses.  As  a  mathematical approach  
for  calculating  the  optimal  buffer  size  that  uses  minimum  memory  to provide 
maximum successful packet transmission; 
Since the number of hop counts is averaged as 2.17 hops as a result of  simulation 
runs, it can be rounded up to AHC= 3 in the calculations and 3 hops means packets 
of last 3 frames will stay in the buffers of the nodes for about 3 frames on average. 
Therefore totally (NOH)x(AHC+1)x(number of packets generated by a node in a 
frame) packets will be transmitted by all nodes during each frame. Since NOH=6 
nodes and average number of hops in the simulation is rounded up to AHC=3, buffer 
size of (3+1)x6x 11 (from eq 44 )= 264 data packet slots will be enough provided 
that number hop counts for that instant is not greater than the calculated average of 
the system. 
Optimum data packet buffer size for our system is calculated theoretically as in 
eq 37 and graphically presented in figure 2, for different number of average 
hop counts and number of nodes using the same spectrum. Optimum buffer sizes for 
voice and video buffers are also formulized in eq. 40. 

 
Throughput Maximisation of Different Signal Shapes 
77 
 

;ĂͿ




;ďͿ 
 
Fig. 2. Optimum Buffer Sizes for data packets   a) in bytes and b) in number of  slots 
for different number of average hop counts and different number of nodes sharing the spectrum 
 
Fig. 3. Optimum Buffer Sizes for data, voice, video packets and sum of all for different num-
ber of average hop counts and different number of nodes sharing the spectrum 
 
2
log (
)
  
 
 
 
 
 (
1) (
1) 
8
QAM xNODS x FECx DSPF
optimumdata Buffer Size
x NOS
NOH
x AHC
bytes
xNOS
⎛
⎞
=
−
−
+
⎜
⎟
⎝
⎠
   
2
3
log (64) 720  
 44
4
 
 
 (30
6
2) (3 1) 52272  
51  
8 30
x
x
x
optimumBuffer Size
x
x
bytes
kbytes
x
⎛
⎞
⎜
⎟
=
−−
+
=
=
⎜
⎟
⎜
⎟
⎝
⎠
     
( 37)
 
it results with  78408 = 76 kbytes and 396 slots , by use of max hop count = 5instead of AHC = 3       ( 38) 
The number of slots for data packets each 198 bytes is calculated as; 
optimum Buffer slotnumbers = 52272 bytes / 198 bytes = 264 Slots           (39) 

78 
B. Preveze and A. Şafak 
 
optimum video /voice Buffer Size = ( packet size) x(VPSR) x( NOH ) x( AHC + 1)bytes 
with  (VPSR)x( NOH ) x( AHC + 1) slots   
 
 
 
(40) 
 
optimum voice Buffer Size = (10) x(1) x(6)x(3 +1) = 240 bytes with VSPR =  1  
(41) 
optimum video Buffer Size =(594) x 1
4
 x(6) x(3 + 1) = 3564 bytes = 3.48 kb 
with   (VPSR) x( NOH ) x( AHC + 1) slots with VSPR = 1 / 4                (42) 
Where VPSR is video/voice packet sending rate (taken as 1/4 for our system) So that it 
has been provided that the total traffic management of all data types can be done using 
only the buffer size of 54648 bytes +240 bytes+3564  bytes=58452 bytes = 57.08 kb 
and the total buffer sizes needed to manage the traffic is graphically presented in  
figure 3  for different number of AHC and NOH using the spectrum. 
4   Cognitive Approaches for Maximized Network Throughput 
By the cognitive methods used in the systems the nodes observe the   environ-
ment, orient the information  of  locations, buffer statuses , packet loss ratios, 
average hop counts and unsuccessful transmission trial rates, then they decide 
what to do and act (OODA loop).   . In this system a 802.16.e mobile  Wimax 
network is simulated by dynamic spectrum sharing of TDM & OFDMA, The 
nodes manage their buffers by making decisions using the information they  have 
about the state of the buffers of their  packets  destination  nodes.  The  nodes  in  
the  network  are  in  movement  with random way point model [7] with random 
speeds (up to 60 km/h in our system) ,so their positions continuously change. 
Each of the nodes generates its necessary voice and video packets and generates 
as much data packets as it can in order to provide a heavy traffic load on the net-
work Nodes continuously use the fastest path algorithm in [8] and sent their pack-
ets through the next node on the path going towards the final destination. Note 
that throughput is calculated as successfully transmitted data rate in bytes from a 
node to it’s final destination in a second and successful bandwidth usage is calcu-
lated as   successfully transmitted data rate in bytes from a node to its next node 
on the path going towards the final destination ,where loss ratio is calculated by; 
loss ratio = lost packets/(lost packets + successfully sent packets)           (43) 
4.1   Cognitivity in Buffer Management 
4.1.1   Fastest Path Route Recalculation Algorithm 
As it is obvious the main factor to increase the throughput is emptying the buffers 
as soon as possible, to transport as more packets as possible in a unit of time 
which requires the best dynamic spectrum sharing technique. For this  purpose  
the packets should be arrived to final destination by minimum number of hops 
which is handled by the fastest path Algorithm in [8]. Since the nodes in the net-
work are in movement and since their positions continuously change, when a  

 
Throughput Maximisation of Different Signal Shapes 
79 
 
transit packet of some other node taken from the buffer in to the forward process, 
the node recalculates the fastest path and determines the next node towards the  
destination, making calculation for the packets of other nodes is part of the  co-
operation, this algorithm is activated in all cases of our study for providing a fair 
comparison among all. 
4.1.2   Adaptive Data Rate 
Using adaptive data transmission algorithm , the transmission rate is  decreased 
when a congession occurs in the network, as it’s also shown in [9] that adaptive 
rate has a positive effect on throughput of the system. At this point since heavy 
network traffic is provided by the nodes , the maximum data genera-
tion/transmission  rate for a node in a frame can be calculated by the nodes as ; 
Since whole Spectrum of 10 MHz (2.4 GHz – 2.5 GHz [4]) is used the system can 
transport 3564000 bytes/s as calculated in eq 8 and 22 or 23 subchannels will be 
used in a frame for data packets as calculated in eq 32,  all the nodes using the 
spectrum can totally use 22/30 or 23/30 of total channel capacity in a frame and  
one of them will load 1/6 of this allocated spectrum such that,  
Assuming that only 1 node is transmiting video packet and 6 nodes are transmitting voice packets
23 1
1
Max.Data Rate of a node 
3564000 bytes/s x
(
)
 2277 bytes/node  11.5 11 
/
30 6
200
 Assumin
x x
packets node
=
=
=
≈
g that only 2 nodes are transmiting video packets and 6 nodes are transmitting voice packets 
22 1
1
Max.Data Rate of a node 
3564000 bytes/s x
(
)
 2178 bytes/node  11 
/
30 6
200
x x
packets node
=
=
=
 
 
(44)
 
4.1.3   Continuous Buffer Checking 
The nodes make end to end check of their buffers at their idle times in order to 
receive their own packets as soon as possible, to increase the number of empty slots 
in the buffers. By this way less number of packets get lost. And the nodes pro-
duce and transmit more data by the decrease of loss ratio, this is in the nature of 
adaptive rate ransmission.  Continuous buffer checking is also activated as fastest 
path algorithm in all cases, in order to provide a fair comparison. 
4.1.4   Buffer Management by Queue Optimization 
If a node starts to send its packets to the buffer of the next node and if the  first 
packet can not find an empty slot there, it gets lost , the sender stops  transmit-
ting packets to that node and tries alternative packet transmissions from its buffer 
during it’s frame turn. Each node determines the next nodes of its packets going 
towards the final destinations using the fastest path algorithm and resorts its 
buffer such that the packets with same destinations are grouped to be sent together 
and the packets whose next node is emptier will be sent first according to dynamic 
spectral aids algorithm. 
4.2   Dynamic Spectral Aids 
In our system 6 of 30 subchannels are allocated to voice packets of 6 nodes (one 
for each) , and 1 or 2 subchannels are allocated to video packets of the nodes 

80 
B. Preveze and A. Şafak 
 
which ever wants to send video packets through the subchannel. The rest of the 
subchannels are allocated to the node which gives alert of full buffer most. If a  
node starts to transmit packets it makes buffer optimization and finds the packets 
whose calculated next nodes buffer is emptiest. If the buffer of  emptiest one is 
also full, the node losts its first trial packet of chosen destination and gives it’s turn 
to the node who gave alert of the ‘spectrum need’ more during its frame, at the end 
of each frame the spectrum is allocated to the node who needs the spectrum most. 
5   Results and Discussioon 
Table  2  indicates  the  activation  conditions  of  the  simulation  results  shown  in 
figures 4-9. When none of the methods is activated, the system behaves under 
normal conditions, the resultant graph shown in fig. 4 (a) indicates the packet loss 
occurrence percentages in periods of last 3 frames in the simulation .On figure 4 
(b) the interval andwidth including wastage(UBBIW)”  indicates the bandwidth 
wastage amount in the ckets  using    a  subchannel.  On  the  other  hand,  when  
11.5  data  packet  is rounded down to 11 data frames as  in  eq 44, 0.5 data 
packet can not be fit in the channel and ignored, by this ignorance 0.5 packet 
x23 SC x198 bytes data packet x 200 frames/sec = 455400 bytes/s will not be 
used which gives us the interval between the upper dashed line in fig. 4(b) which 
indicates the total transmission  rate for the nodes and UBBIW in fig 4(b) . In fig-
ure 4 (a) the reason of having the throughput line very below the UBB is: several 
spectral usages of same multi-hop packets in order to arrive to the final destina-
tion. This state is valid for all the conditions analysed. The linearly increasing 
number of sent packets and lost  packets are also illustrated on figure 4(c) , the 
numeric values taken from the simulation are listed in tables 3 and 4. 
In case of deactivating the adaptive rate, when the buffer size is optimized and 
the buffers are  managed  such that no  one  will be  forced to  receive data  when  
it has congestion in its buffers. Here the packet loss is expected to occur later 
with respect to figure 4(a), where figure 5 (a) verifies this assumption. On the other 
hand since the dynamic spectral aids are also activated in this case, spectrum 
usage  efficiency will increase and  packet  loss rate  will  decrease  which  causes  
and  inceasement  on  the throughput. But since the adaptive rate is not active the 
buffers of the nodes will be forced to  receive more  new generated  packets, in  
case of congestion it only  uses spectral  aids  ,  but  when  the  nodes  become  
congested  again  after  a  while,  some packets will start to get lost in the system  
until the congestions on the buffers gets down. The effects of all methods are  
also calculated and presented  graphically in figures 10-13. 
If larger buffer size is used, packet loss ratio decreases with respect to  
smaller buffer sizes and this will have an increase effect on the throughput. It’s seen 
on figure 12   that packet loss ratio is decreased due to larger buffer size, and   
on figure  11 indicates  that the throughput of the system is not improved by using 
large buffer size instead of optimum size ,because larger buffers store and hold 
more packets inside. 
 

 
Throughput Maximisation of Different Signal Shapes 
81 
 
 
(a) 
 
(b) 
  
(c) 
 
Fig. 4.  (a) Packet loss average of the network vs. frame count in last 3 frames (b) bandwidth up-
per bound, total transmitted data rate and throughput and of the system vs. frame count,  (c) over-
all sent/ lost packet counts for video/voice/ data packets, without any cognitive methods. 
Table 2. The applied cognitivity methods in figures 4 to 9 
■ : The method is applied ,  □ : The method is not applied 
Figure 
Adaptive rate 
usage 
With Optimum 
buffer size 
(264 slots) 
With 
Larger buffer 
sizes 
(500 slots) 
Dynamic 
spectrum aids 
in OFDMA 
Buffer 
management 
by queue 
optimization 
Fig. 4 
□ 
□ 
□ 
□ 
□ 
Fig. 5 
□ 
■ 
□ 
■ 
■ 
Fig. 6 
■ 
□ 
■ 
■ 
■ 
Fig. 7 
■ 
■ 
□ 
■ 
□ 
Fig. 8 
■ 
■ 
□ 
□ 
■ 
Fig. 9 
■ 
■ 
□ 
■ 
■ 
 

82 
B. Preveze and A. Şafak 
 
 
Fig. 5. Packet loss average of the whole network vs frame count in last period of 3 frames 
without usage of adaptive rate 
 
Fig. 6. Bandwidth upper bound, total transmitted data rate and throughput and of the system vs 
frame count, using larger buffer size 
When  the adaptive rate method  is activated the total data  transmission rate of  
all nodes varies according to congestions and packet losses occurring in the network 
in the last period, this variation can be read from the upper dashed line in fig. 6. 
 
  
Fig. 7. Bandwidth upper bound, total transmitted data rate and throughput and  of the 
system vs. frame count, without usage of buffer management by queue optimization 
 

 
Throughput Maximisation of Different Signal Shapes 
83 
 
As it is presented in figure 7 , deactivating the usage of buffer management  by queue 
optimization, allows more congestions on the buffers of the nodes and  more packet losses 
occur , so the adaptive rate method takes account, it decreases the rate of data generation/ 
transmission to the network to avoid congestion and packet losses on the network. But at 
the same time by less packet transmission and more packet losses with respect to all acti-
vated condition, in absence case of   buffer  management the packet loss ratio increased 
most .Efficient channel bandwidth  usage and throughput will be decreased, as seen in 
figure 12 and packet loss ratio increases at most as seen on figure 13. 
The dynamic spectral aids algorithm stands for cooperation and in this algo-
rithm the nodes give their subchannel usage turns in a frame to other nodes  
which gives alert of buffer congestion most. 
 
Fig. 8. Bandwidth  upper  bound,  total transmitted data rate and throughput and  of  the 
system vs. frame count, without usage of cognitive Dynamic spectrum aids in OFDMA. 
In such a system the total throughput of the system will increase, since the spectral 
efficiency of the whole system increases as seen on figure 12 and figure 8 shows the 
performance results of the system when spectrum aids algorithm is not activated. 
When all the methods are activated as in figure 9  we expect to have the best results 
in  terms of throughput, and  because  of  using  the  optimum  buffer  size  we expect 
the performance results at least same or better than the condition using large buffer size, 
where our expectations are verified by figures 11, 12 and 13. 
  
Fig. 9. Bandwidth upper bound, total transmitted data rate and throughput and of the system 
vs frame count, with usage of all the cognitive methods and optimum buffer size 

84 
B. Preveze and A. Şafak 
 
As it is observed from figure 10  that number of sent voice and video packets 
have approximately same results for each case, the reason of this is;  a separate 
subchannel is allocated for only one or two real time voice and video packets at 
each frame and their buffers are not overloaded by the system as in buffers of  
data packets, so there exists no voice or video packet losses during the transmis-
sion, and number  of sent- lost packets of  the high traffic loaded data packets 
shown in figure 10 determines the system throughput performance. 
Table 3. The result of simulation runs for applying conditions of cognitivity methods 
  
 
Fig. 10.  The improvement amounts of the cognitivities applied to the network in terms of the 
number of video voice and data packets successfully transmitted to the final destination in 1000 
frames 
 
It is declared in [4] that the throughput in Mobile Wimax can vary between 
10 Mbps  and 17 Mbps, It is also verified by figure 11 that the throughputs are 
always between 10 -17  Mbps in all cases and the throughput is maximized to 
1608800 when all cognitive methods are activated. As presented in figure 11 that, 
the throughput of the network is maximized for 802.16e mobile Wimax System 
using the  mentioned cognitive methods . It is also obvious in figure 11 that 
improvement  is  supplied at most by the spectral aids in OFDMA algorithm, 
that’s why the  system throughput average performance is minimized at most dur-
ing the absence case of this algorithm. 
 

 
Throughput Maximisation of Different Signal Shapes 
85 
 
Table 4. The result of simulation runs for application conditions of cognitivity methods 
 
  
  
Fig. 11. Throughput improvement amount of the system by use of different cognitivities 
One of the most important criteria that effect the throughput of the system is efficient 
usage  of  the  spectrum  and  decreasing  the  number  of  packet  losses,    the  average 
spectrum usage amounts of the successfully sent packets are seen on figure 12 and the 
greatest effective spectrum usage percentage is provided when  all the methods are acti-
vated with larger buffer size or optimum buffer size respectively.  
Figure 13 indicates that minimization of packet losses is also provided when 
all methods are activated using the cognitive techniques that have cooperative be-
haviors among network members and optimizations made on the system. 
  
Fig. 12. Average Effective Spectral Usage of the system by use of different cognitivities 

86 
B. Preveze and A. Şafak 
 
 
Fig. 13. Average Data  Packet loss Ratio of  the system by use of different cognitivities 
6   Conclusion 
In this study, the efficient spectrum sharing is provided by using a new proposed spec-
tral  aid  algorithm  in  addition  to  OFDMA  and  TDMA  techniques.  Optimum 
buffer size for different numbers of average hop counts and node counts is formulized 
and calculated, its validated by the simulation that  buffer size larger than optimum 
buffer size can not improve the bandwidth with respect to optimum buffer size .The 
throughput performance of the system has been separately improved by each methods 
used in the simulation , and it is shown that spectral aids has %19 improvement on the 
throughput  while  buffer  management  has  %  10,adaptive  rate  has  %  4  and  the 
throughput is improved by % 27 when all methods are activated. Optimum buffer size 
also  improves  the  throughput  by  %  2  with  respect  to  larger  buffer,  even  by  less 
memory usage. 
References 
1. Kumar, A.: Mobile Broadcasting with Wimax. Elsevier Inc., Amsterdam (2008) 
2. Han, B., Jia, W., Lin, L.: Performance evaluation of scheduling in IEEE 802.16 based wire-
less mesh networks. Science Direct, computer communicatins (2006) 
3. Mobile WiMAX – Part II: A Comparative Analysis (May 2006) 
4. Mobile WiMAX – Part I: A Technical Overview and Performance Evaluation Prepared on 
Behalf of the WiMAX Forum, February 21 (2006) 
5. http://en.wikipedia.org/wiki/Cognitive_radio 
6. Mobile WiMAX: A Performance and Comparative Summary, Prepared by Doug Gray 
(September 2006); Copyright 2006 WiMAX Forum 
7. Deborah, E., Daniel, Z., Li, T., Yakov, R., Kannan, V.: Source: Demand Routing: Packet 
format and forwarding specification (version 1) (January 1995) 
8. Yi, X., Wanye, W.: Finding the Fastest Path in Wireless Networks. IEEE ICC, North Caro-
lina State Univ. (2008) 
9. Iannone, L., Fdida, S.: Can Multi_rate Radios reduce end_to_end_delay in mesh network? 
A simulation case study 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 87–98, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
A Variant of Merkle Signature Scheme to Protect AODV 
Routing Protocol 
Satria Mandala, M.A. Ngadi, Abdul Hanan Abdullah, and Abdul Samad Ismail 
Faculty of Computer Science and Information System 
Universiti Teknologi Malaysia (UTM Skudai) 
Johor - Malaysia 
satriamandala@hotmail.com, dr.asri@utm.my, hanan@utm.my, 
abdsamad@utm.my 
Abstract. Wireless mobile Ad-Hoc networks (MANETs) are networks of mo-
bile nodes with no fixed infrastructures. Their peer-to-peer communication ar-
chitectures over shared wireless medium with no clear line of defense pose a 
number of challenges to security design. Some previous works have used a 
form of digital signature for protection. However, there are still limitations in 
the implementation. This paper1 proposes a variant of the Merkle Signature 
Scheme (MSS) to protect an Ad-Hoc On-demand Distance Vector (AODV) 
protocol, a common routing protocol for MANETs. The new protocol is named 
Extra Secure AODV or ESAODV in short. ESAODV provides hop-count pro-
tection, digital signature and protocol enforcement. Through these features sev-
eral security benefits on the routing packets have been established, namely fast 
authentication, integrity, and non-repudiation. Several experiments have been 
conducted using different digital signatures of the MSS variant signatures. The 
result shows that ESAODV is effective for combating routing attacks, and 
works properly although the malicious are 20% of the nodes total number in 
MANETs. In terms of features, ESAODV are also far superior than secure 
AODV (SAODV) and Authenticated Routing for Ad hoc Networks (ARAN). 
Keywords: Security of MANET, Merkle Signature implementation, Message 
authentication protocol, Digital Signature. 
1   Introduction 
GROUPS of mobile computers (or nodes) that cooperate and forward packets to one 
another beyond direct wireless transmission range are well known as mobile ad-hoc 
networks (MANETs). MANETs are more vulnerable to attacks than wired network 
due to the nature of MANETs themselves, such as unavailability of a central admini-
stration, open medium, mobility, capability to perform self organization among the 
mobile nodes participant, and limitation in power and resources. 
Security implementation in MANETs instigates new kind of attacks particularly 
when the protocol adopted is not adaptable to the behavior of the networks. A simple 
                                                           
1  This work was supported by the MINISTRY OF SCIENCE, TECHNOLOGY AND INNO-
VATION (MOSTI) of Malaysia under project number 79280. 

88 
S. Mandala et al. 
 
case is inappropriate selection of digital signature scheme for securing the packets. If 
Digital Signature Algorithm (DSA) is adopted, its slowness in signing and verifying 
message signatures causes a serious security problem in the network when attackers 
flood the network with a large number of forged routing packets. Routers in the net-
works will be busy just for signing, verifying, and rejecting the forged routing pack-
ets. Memory resources of the routers will overflow almost immediately. On the hand, 
adopting existing digital signatures such as ‘Rivest, Shamir and Adleman’ (RSA) 
public-key cryptography or Elliptic Curve Cryptography Digital Signature Algorithm 
(ECCDSA) may also be susceptible to crypto attacks. Shor [16] has proven that the 
foundations of these digital signatures, i.e. factoring integers and finding discrete 
logarithm problems, are not intractable in quantum computers. In addition, Hallgren 
and Vollmer [1] have categorized these digital signatures in a group ‘broken by quan-
tum computer’.  
The quantum computers are likely to be common in a very near future. Vander-
sypen et al. [17] in 2001 successfully implemented Shor’s algorithm on a 7-qubit 
quantum computer. In 2004, Buchmann et al. [14] stated that the next 15 to 20 years 
from that year, the quantum computers would be sufficiently large to implement 
Shor’s ideas for breaking the above digital signatures. In addition, he also pointed out 
that some digital signatures such as ‘Number Theory is Really Useful’ (NTRU), 
SFLASH2 and and Merkle scheme are still un-breakable by the quantum computers.   
Developments of integer factorization and discrete logarithms have gotten signifi-
cant results. Some security cryptosystems to replace the vulnerable digital signatures 
have also been proposed. Buchmann et al. [4] [18] have developed variant of digital 
signatures based on Merkle scheme. These digital signatures are free from factoriza-
tion attacks, superior in computation speed as compared to the existing RSA and 
ECDSA, and can be efficiently implemented in Java Cryptographic Service Provider. 
Recently, Stainwandt et al. [15] also improved the efficiency of this Merkle digital 
signature variant. Bennet [19] declared that some cryptosystems such as symmetric 
encryption algorithms, one-way functions, and cryptographic hash algorithms would 
be resistant to attacks in quantum computers. Unfortunately, many existing so-called 
secure protocols in MANETs, e.g. SAODV [10] and ARAN [11], do not implement 
these digital signatures. 
These existing security schemes of MANETs, which employ digital signatures, are 
not robust enough and impractical. The SAODV packets are significantly large be-
cause of the digital signature design of this protocol. If the double digital signature 
mechanism is used, the protocol will get worst in terms of time and memory con-
sumption. For the ARAN, it uses Public Key Infrastucture (PKI) system for securing 
MANETs and has been proven to consume large computation resources [24]. Cerri 
and Ghioni [20] found similar problems when they were exploring the SAODV. As a 
result, they extended SAODV and proposed Adaptive SAODV (A-SAODV). How-
ever, their proposal could not solve the inefficiency of SAODV due to the usage of 
double signatures.  
It is therefore necessary to come up with a new secure protocol for MANETs, 
which does not rely on the complexity of integer factoring and discrete logarithms, 
                                                           
2  There has not been a formal justification of what the SFLASH stands for. SFLASH is a C* - -
algorithm with a special choice of parameters (See Courtois [23] for details). 

 
A Variant of Merkle Signature Scheme to Protect AODV Routing Protocol 
89 
 
and at the same time immune to the quantum computer attacks (or factorization at-
tacks). The protocol should also expedite the signing and authentication of packets 
and could resist attacks like black hole attacks, routing loop attacks, replay attacks, 
and spoofing attacks.  
This paper proposes a new security scheme called ESAODV (for Extra Secure 
AODV). The proposed security scheme is based on variant of MSS digital signatures 
[4] [18], a hash chain and some protocol enforcement. It discusses the development of 
the prototype of ESAODV protocol for securing the MANETs using cryptosystems, 
which have been identified secure from factorization attacks. This paper also com-
pares ESAODV with the most popular protocols, i.e., SAODV and ARAN. 
The rest of this paper is organized as follows. The following section describes the 
theoretical framework. In section 3, the proposed security scheme is discussed.  
Section 4 presents descriptions on how the simulations were set-up and the output of 
the results. A discussion  in section 5 presents the comparison of ESAODV with the 
SAODV in terms of features and section 6 concludes the results of the work.  
2   Theoretical Framework 
Lamport [2] proposed the use of one-way hash chain as a password protection scheme 
in an insecure environment. The one-way hash chain is useful for developing digital 
signatures such as in Coronado MSS (CMSS) [4] and generalized Merkle Signature 
Scheme (GMSS) [18]. The following presents a theoretical perspective of hash chain, 
CMSS, and AODV protocol.  
2.1   Hash Chain 
Referring to Lamport [2], a hash chain is a successive application of hash function 
h(x), for example h(h(h(h(x)))), which is denoted as h4(x). Figure 1 shows one –way 
hash chain that consists of 4 hash functions.  
2.2   The Variants of a Merkle Signature 
CMSS and GMSS are variants of MSS [3] and are more efficient than the MSS be-
cause they reduces the size of MSS private key, accelerate key pair generation, and 
speed up signature generation. CMSS is capable of signing 240 documents, meanwhile 
GMSS is 280 documents. The time taken is far better than both RSA and Elliptic Curve 
DSA (ECDSA). The detailed discussion on CMSS and GMSS can be found in [4] [18]. 
This paper considers these digital signatures for securing the routing protocol in 
MANETs.  
V0
V1
V2
V3
 
Fig. 1. One-way chain 

90 
S. Mandala et al. 
 
2.3   The AODV Protocol 
AODV routing protocol [5] is one of several published routing protocols for MANETs, 
and currently the protocol gets many attentions from research communities of wireless 
and networking. This protocol uses four different types of packets to maintain and to 
discover routes over the network. Route Request (RREQ) and Route Reply (RREP) 
packets are used for discovering routes. Route Error (RERR) packets and HELLO 
packets are used for maintaining routes.  
2.4   The AODV Shortcomings 
AODV protocol came with no security scheme. It means that the AODV packets are 
without any encryption, integrity check or authentication and are assumed to be sent by 
trusted nodes. As such the networks are vulnerable to numerous of attacks. The follow-
ing summarizes some possible attacks in the network [6] [7][8][ 9]: 
• 
Black hole attack – any exploitation of the AODV packets, such as alteration of 
RREP destination sequence number (DSN) to higher value, or hop count decre-
ment will result a black-hole attack. 
• 
Routing Loop attack – the attacker spoofs destination address and combine with 
increasing sequence number or decreasing the hop count.  
• 
Replay attack – the attacker injects ‘routing information’ that has been captured 
previously, to disturb the function of routing in the network, or to advertise the at-
tacker as a legitimate node and perform black hole attack.  
• 
Spoofing attack – this is a special case of integrity attacks whereby the attacker 
impersonates a legitimate node to forge RREQ, RREP and RERR, due to the lack 
of authentication in the ad hoc routing protocols.  
• 
Selfish attack – attacker selectively drops or not forwards the RREQ and RREP it 
receives without acceptable reason.  
• 
Wormhole attack – two attackers create a tunnel that is linked through a private 
connection.  
• 
Denial of service (DoS) attack – the attacker injects a large amount of junk packets 
into the network.  
• 
Crypto attack – attacker breaks the primitive cryptography, such as breaking fac-
torization problems in RSA or ECC to gain access of network resources, such as 
stolen password, masquerading identity and spoofing routing packets. 
3   Extra Secure AODV (ESAODV)  
In this section, core of the ESAODV will be detailed out. The first part briefly dis-
cusses the security requirements and some assumptions made. This is followed by the 
description of the proposed algorithm and the architecture. Subsequently, the imple-
mentation considerations will be discussed.  
3.1   Security Requirements and Assumption 
The proposed protocol will have requirements and assumption as follows:  
• 
The destination node can authenticate packets from the originator (route creator) 
and each of receiving nodes can authenticate packets from the previous hops. 

 
A Variant of Merkle Signature Scheme to Protect AODV Routing Protocol 
91 
 
Receiving RREQSec packets  
1. Start 
2. Packet classifier Å packets 
3. if (RREQSec) 
4. 
Packet extractor ÅRREQSec 
5. 
Packets: Original RREQ + hC protection  + created_inNode + 
signature + pK 
6. 
Hop Count tester Å hop_count +1, has_from + max_hop + 
top_hash 
7. 
Signature Verifier Å pK + Signature + Non mutable field bytes 
8. 
if (Hop Count tester && Signature  
 
 
Verifier) 
9.  
Update Route if It is necessity 
10.  
end if 
11.  
if (thisNode = = destination || has more fresh rt to dest. than
 
    packet) 
12.  
   Sig GeneratorÅNon mutable RREP 
13.  
    hC Protect. Gen Å 0, maxHop = ttl = HopToOrig 
14.  
   Packet Builder Æ Orig RREP + hC  protection +  
 
   created_inNode  + signature+pK 
15.  
   Sent out RREPSec  to lower layer 
16.  
else 
17.  
Packet Forwarder Å RREQSec 
18.  
end if 
19.   end if 
20. Stop 
 
Broadcast RREQSec packets  
1. Start 
2. Packet Dest = Extractor Å RREQSec 
3. NextHop = Find thisNodeRtTable(Packet Dest) 
4. if (NextHop!=null)  
5.  
Packet Forwarder ÅRREQSec 
6. else 
7.     Sig Generator ÅNon mutable RREQ 
8.     hC Protect. Gen Å0, maxHop = ttl= 1,3, … 
9.     Packet Builder ÅOrig RREQ + hC protection +created_inNode 
 
 +  signature+ pK thisNode   
Broadcast RREQSec packets (continued) 
10. Broadcast 
11. end if 
12. Stop 
 
Receiving RREPSec packets  
1. Start 
2.  
PacketDest = extractor ÅRREPSec 
3.  
PacketOrig = extractor ÅRREPSec 
4.  
routeEntry = Find ThisNodeRtTable(Dest) 
5.  
if (routeEntry !=null) 
6.  
 
add route as Routing Disc. Success 
7.  
 
add precursorSet 
8.  
end if 
9.  
if (thisNode address is Packet Dest) 
10. 
 
Generate RREPSec (similar process  
 
with generating  RREPSec in  
 
 
Receiving RREQSec algorithm) 
11. 
else 
12. 
haveRtToNtHop = FindThisNodeRtTable  
 
 
 
                  (Orig) 
13. 
 
if (ttl >0 && haveRtToNtHop) 
14. 
       
Forwarding RREPSec 
15. 
 
end if 
16. 
end if 
17.Stop 
 
Receiving RERRSec packets  
1. Start 
2. ListUnreachable = Find ThisNodeRtTable 
 
(RERRSecs) 
3. if (ListUnreachable!=null && ttl> 0) 
4.  
    Sig Generator = ListUnracable 
5.  
    Packet Builder ß Orig RERR +      
 
    created_inNode + signature + pK  
6. Packet Forwarder (Precursor) Å RERRSec 
7. end if 
8. Stop 
• 
The hop count value is protected using hash chain. It cannot be reduced  
by a malicious node, but could be increased by one or retained  
unchanged.  
• 
Nodes in the network have capabilities for keys (private and public keys) crea-
tion, signatures generation, and signatures verification.  
• 
Each node has only one pair of keys(private and public keys). The digital sig-
nature algorithm is well known by all nodes in the network. 
3.2   Proposed Algorithm and Architecture 
The ESAODV aims to secure routing packets of the AODV protocol. AODV routing 
packet has been modified to be ESAODV-compliant as shown in Figure 2(a). The 
developed architecture is presented in Figure  2(b). The heart of ESAODV consists of 
three components: hash chain for securing the hop count, digital signature for authen-
ticating non-mutable fields of the packets and a protocol enforcement mechanism. 
Using this enforcement, this protocol will put address of any nodes, which packets 
have been modified, in created_inNode field. The algorithm how ESAODV handles 
these routing packets are below:  
 
Algorithms: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

92 
S. Mandala et al. 
 
 max_hop
: a maximum number hops (nodes) for routing discovery
 has_from
: a current value of successive hash function from this node
 top_hash
: a highest value of successive hash function
 created_inNode
: an address of packet creator or packet modifier
 digital_sig_algo
: name of algorithm to create signature
signature
: for RREQ: signature of original RREQ minus hop count and DSN of the
                                  originator
 
  for RREP: signature of original RREP minus hop count
 
  for RERR: signature of original RERR
 public_key
: public key of the signature from originator, middle node or destination node
(a)
RREQSec
Original RREQ
hash_from
top_hash
created_inNode
digital_sig_algo
signature
max_hop
public_key
RREPSec
Original RREP
hash_from
top_hash
created_inNode
digital_sig_algo
signature
max_hop
public_key
RERRSec
Original RERR
digital_sig_algo
signature
public_key
created_inNode
Hello
(b)
CryptoSystem (Bouncy Castle &
FlexiProvider)
verifier of packet
signature
hop count tester
signature
generator
hop count protection
generator
packet classifier
packet builder
packet forwarder
extractor
RREQSec, RREPSec, RERRSec
RREQSec, RREPSec, RERRSec
Network Layer
DataLink Layer
Packet Rejecter
The Core of ESAODV
 
Fig. 2. ESAODV’s packets and architecture 
As depicted on Figure 2(b), the ESAODV has several modules - packet classifier, 
extractor, hop count tester, verifier signature, signature generator, hop count protec-
tion generator, packet builder, packet rejecter, and packet forwarder. The following 
paragraphs are short descriptions of how packets are processed with respect to the 
modules presented in Figure 2(b). 
Packets arrive to the system will be identified by the packet classifier to determine 
the type of packet. This protocol has four packet types, i.e. Route Request Secure 
(RREQSec), Route Replay Secure (RREPSec), Route Error Secure (RERRSec) and 
Hello packet. All packets except the Hello packet will be extracted by the extractor to 
identify components within the packets. This will be followed by the integrity evalua-
tion and hop count verification of the extracted packets. Two modules, i.e. the packet 
signature verifier and the hop count tester will handle these tasks. Also at this point, 
the RERRSec is excluded from the hop count verification as this packet has no hop 
count, but integrity evaluation is still considered on this packet. Any alteration to the 
hop counts of RREQSec and RREPSec either by incrementing or decrementing the 
value will trigger the hop count tester to generate error notifications and will reject the 
packet through packet rejecter. Violations on the packet integrity will also trigger 
error notifications and will reject the packet too. If the evaluation and verification are 
succeed, this protocol may update routing information to routing table. Before passing 
the packet to the upper layer, the protocol will call signature generator to sign the 
packet, call hop count protection generator to generate hash_from, and then the 
packet builder wraps the signature, hop count protection (which consists of max_hop, 
hash_from and top_hash), created_inNode and public key into secure packets and 
pass them to the packet forwarder. 
3.3   Implementation Considerations 
In this section, hop count, digital signature and protocol enforcement will be  
described. 

 
A Variant of Merkle Signature Scheme to Protect AODV Routing Protocol 
93 
 
h(h(h(h(h(h(x))))))
h(h(h(h(x))))
hop_count =4
Dest
this
Node
Origi
nator
hop_count=6
distance = 2
distance = 4
X
start from 0,
start from 3,
pre-hash
hash chain
 
Fig. 3. Hop count computation 
Hop count protection implements a hash chain to authenticate the hop count of 
RREQSec and RREPSec messages. This mechanism will avoid unauthorized change 
of hop count by any nodes during the travel throughout the network. In ESAODV, 
middle node is allowed to response a route request packet (RREQSec) with RREPSec 
whenever the node has a fresh enough route to the destination node. This mechanism 
creates verification problem in next hop. To solve this problem, this protocol will shift 
the starting point of hashing successive to a new value. Figure 3 shows this idea. Sup-
posing that the distance of the originator to this middle node is 4, and the distance of 
this middle node to the destination is 2. When this middle node responses to the origi-
nator with RREPSec, this node must perform pre-hashing twice before computing the 
value of hop count itself. The total of hashing successive is still 6, but the starting point 
of hashing to be shifted is to 3. 
Digital signatures will be used to protect the integrity of the immutable data for 
RREQSec, RREPSec and RERRSec packets. This work carefully determines the 
immutable field of these packets, especially for the RREPSec packet. Perkins et al.  
[5] indicated that the middle node could generate route replay packets. The middle 
node must copy its known sequence number for the destination into the destination 
sequence number field in the route replay packet. As such, the value of DSN in the 
route replay packet is dynamic. ESAODV excludes this field from the signing proc-
ess. This is in contrast to SAODV [10] which signs everything except the hop count. 
The process of signing and verifying in ESAODV is as follows: The RREQSec packet 
is signed by the originator node, and will be verified by the middle and the destination 
nodes. The RREPsec can be signed by the destination or the middle nodes, and will be 
verified by other middle and the originator nodes. The RERRSec is signed by any 
node that encounters un-reachability.  
Protocol enforcement will put any node address that modifies RREQSec, RREPSec 
and RERRSec into the created_inNode field of these packets respectively. This is 
useful for monitoring behavior of nodes in the networks. In addition, the ESAODV 
does not introduce new kind of packet just for monitoring purposes because it can 
increase network traffic. Using this mechanism, if any node falsify by being a mali-
cious node to the packets, the close neighbor to the malicious node will be easyly to 
identify the malicious node address, and reject the garbage packets from the node. 
The reason of rejection is to avoid continuous verification error in following hops.  

94 
S. Mandala et al. 
 
4   Simulation and Results 
The simulation scenarios consider two situations: ESAODV without attacks and 
ESAODV under attacks. All of these simulations were run on Intel(R) Core(TM)2 
Duo CPU T8300 2.4.0 GHz computer with Java j2sdk1.6.0_18 installed. The 
BouncyCastle [21] and FlexiProvider [22] were used as security libraries of protec-
tions. JisT/Swans simulator [11], [12] was utilized as the place to run the protocol.  
Table 1. Table of experiment parameters  
Field dimension 
3000x3000 
Placement model/mobility 
grid  10x10/static 
Spatial propagation 
hierarchy(hier):5  
Traffic 
non cbr 
Rate of sending messages on each node 
1 message/minutes 
Transmissions (repetition sending packets) per client 
1 
Number of Nodes 
10 - 100 
Timings (start,duration,resolution) 
10,1000,60 
CMSS/GMSS/RSA  
10/10/1024 
KeyPair Generator Size 
Normal AODV 
- 
CMSSwithSHA1andWinternitzOTS ω =1 (CMSS1) 
CMSSwithSHA1andWinternitzOTS ω =2 (CMSS2) 
GMSSwithSHA1 
String Algorithms 
SHA1withRSA (1024 bits) 
Hash function for Hash Chain 
SHA1 
Number malicious nodes per configurations (for under 
attack) 
20% * total number of nodes 
Simulation runs 
10 times per configurations 
 
One of the goals of this paper is to prove that ESAODV is effective in routing protec-
tion scheme. For the reason, this paper will use some metrics to measure the effective-
ness of the protocol outputs through simulations, they are: speed, memory consumption, 
and ratio of route success for numbers of route deliveries and security cost. All of these 
metrics are tried in two simulation scenarios - no attacks and under attacks.  
The simulations were set-up using parameters in Table 1. The parameters are field 
dimension, time of simulation, traffic, spatial, and some security parameters such as 
algorithms for digital signatures, and hash functions. In addition, the number of mali-
cious nodes per configuration is 20% of the nodes total number, which is selected 
randomly. Each simulation was repeated ten times to get representative results. 
Results of ESAODV using merkle digital signatures, i.e. CMSS1, CMSS2, GMSS 
and existing RSA, in simulations without attacks, are presented in Figures 4 and 5. 
These results capture speed, memory, security cost, and ratio of route success. The 
results show the effectiveness and the flexibility of the protocol. The results also show 
the performance of the protocol in relation to the digital signature adopted. The fastest 
of computation speed is given by GMSS scheme. Meanwhile, the memory usage of 
this protocol is also the lowest when using the same scheme (GMSS). As expected, 
the speed of computation and memory usage increase when the number of nodes 
increases. This is because large number of nodes will induce a lot of packets in the 
network. For 100 nodes this protocol must generate about 3000 signatures for packets, 
verify about 267000 packets signatures, generate about 97000 hop count protections, 
and verify about 267000 hop counts (See Figure 5(a) for details). 

 
A Variant of Merkle Signature Scheme to Protect AODV Routing Protocol 
95 
 
Number Of Nodes
(a)
0
10
20
30
40
50
60
70
80
90
100
Elapsed time (milli seconds)
0
40x103
80x103
120x103
160x103
200x103
240x103
280x103
Number Of Nodes vs Computation Speed (CMSS2) 
Number Of Nodes vs Computation Speed (CMSS1) 
Number Of Nodes vs Computation Speed (RSA) 
Number Of Nodes vs Computation Speed (GMSS) 
Number Of Nodes
(b)
0
10
20
30
40
50
60
70
80
90
100
Memory Consumption (bytes)
0
2x106
4x106
6x106
8x106
10x106
12x106
14x106
Number Of Nodes vs Memory Consumption (CMSS2)  
Number Of Nodes vs Memory Consumption (CMSS1) 
Number Of Nodes vs Memory Consumption (RSA) 
Number Of Nodes vs Memory Consumption (GMSS) 
 
Fig. 4. Computation speeds (a) and memory usages (b) of ESAODV without attacks 
Number Of Nodes
(a)
0
10
20
30
40
50
60
70
80
90
100
Number of Packets
0
50x103
100x103
150x103
200x103
250x103
300x103
Number of Nodes vs Signed ESAODV packets
Number of Nodes vs Verified ESADV packets 
Number of Nodes vs HopCount generation 
Number of Nodes vs HopCount verification 
  
Number Of Nodes
(b)
0
10
20
30
40
50
60
70
80
90
100
RouteSuccess/RouteDelivery
0
3
6
9
12
15
18
21
24
Nodes vs RouteSucc/RouteDelivery  (AODV)
Nodes vs RouteSucc/RouteDelivery  (ESAODV)
 
Fig. 5. Security cost (a) and ratio route success (b) of ESAODV without attacks  
Number Of Nodes
(a)
0
10
20
30
40
50
60
70
80
90
100
Elapsed time (milli seconds)
0
40x103
80x103
120x103
160x103
200x103
240x103
280x103
 Nodes vs Computation Speed (ESAODV udr attck)
Nodes vs Computation Speed (ESAODV)
Number Of Nodes
(b)
0
10
20
30
40
50
60
70
80
90
100
Memory Consumption (bytes)
0
2x106
4x106
6x106
8x106
10x106
12x106
14x106
Number Of Nodes vs Memory Consumption
Number Of Nodes vs Memory ESAODV (Under Attck)
 
Fig. 6. Computation speeds (a) and memory usages (b) of ESAODV under attack  

96 
S. Mandala et al. 
 
Number Of Nodes
(a)
0
10
20
30
40
50
60
70
80
90
100
Number of Packets
0
20x103
40x103
60x103
80x103
100x103
120x103
Nodes vs Signed ESAODV packets (Under attck)
Nodes vs Verified ESADV packets (Under attck)
Number of Nodes vs HopCount generation (Under attck)
Number of Nodes vs Hopcount verification (Under attck)
Number Of Nodes
(b)
0
10
20
30
40
50
60
70
80
90
100
RouteSuccess/RouteDelivery
0
3
6
9
12
15
18
21
24
Nodes vs RtSucc/RtDelivery (ESAODV - udr attck)
Nodes vs RtSucc/RtDelivery (ESAODV)
 
Fig. 7. Security cost (a) and ratio route success (b) of ESAODV under attack 
The ratio of route success is dependent on some factors such as traffic density, 
channel capacity, routing protocol, delay and many others. Security implementation 
can be categorized as nodal processing delay. This delay in ESAODV comes from 
four security features, i.e., signature generation, signature verification, hop count 
protection generation and hop count protection verification. To evaluate the ef-
feciency of the ESAODV protocol in the ratio of route success, this work performed a 
comparison with the AODV protocol. The AODV protocol has been set-up using the 
same parameters in Table 1. The results show that the ratio of route success per route 
delivery in ESAODV and AODV is almost similar. For 20 nodes the ratio of AODV 
is 21.92, meanwhile for ESAODV is 18.9189.  
Figures 6 and 7 are the results of ESAODV simulation when 20% of the total 
nodes in the network are malicious. The malicious nodes have been set-up to attack 
actively to the protocol throughout the simulation time. The computation speed, 
memory usage, security cost, and ratio of the route success of  ESAODV under at-
tacks are measured, and these metrics are compared to ESAODV without attack. For 
the purpose of simulation, the simulation chose CMSS2 as the representative of digi-
tal signature to be implemented for securing the MANET in the two situations, i.e., no 
attack and under attack. Figure 6(a) shows that the computation speed of ESAODV 
under attack is less than ESAODV without attack. This is because the protocol rejects 
automatically whenever its find malicious packets. As a result, less processing time is 
required to handle malicious packets and less memory will be used. (Figure 6(b)). The 
security cost of ESAODV under attack is much lighter than without attack (see Figure 
7(a)). In 100 nodes under the attack situation, the cost is about 30% less than the 
without attacks. Unfortunately, the beneficial results of this protection are followed by 
the decrease of route success (Figure 7b). However, if this is compared to ESAOV 
without attacks, the decreasing ratio of route success is not significant. In under attack 
condition, the ratio of route success for 40 nodes is around 15, on the contrary in 
without attack condition, the ratio is around 18. 
5   Discussion 
This section discusses some general comparison of ESAODV with the existing rout-
ing protection for MANET, such as ARAN and SAODV, in terms of their protocol 
efficiency and security characteristics.  

 
A Variant of Merkle Signature Scheme to Protect AODV Routing Protocol 
97 
 
In terms of computation efficiency, ESAODV, ARAN and SAODV require the 
originator to sign every packet it sends, and the intermediate nodes are required to 
verify the signature for each routing packet it processes. However, ESAODV signed 
smaller portion of the AODV packet’s field than SAODV and ARAN, though in a 
case of RREQSec, the ESAODV signed original RREQ minus ‘hop count and DSN’ 
of the originator, in contrary to SAODV that signed everything except the hop count.  
In addition, ESAODV does not have double message signature such the SAODV. 
Moreover, using PKI like ARAN in this type of network will require huge computa-
tion due to the very nature of PKI system. 
In terms of message forgery, both of SAODV and ARAN relied on current signa-
tures technology, which known to be prone to factorization attacks. ESAODV, on the 
other hand, uses signatures based on Merkle Signature hash function that could resist 
factorization attacks. In addition, ESAODV also introduces early detection of security 
violation, which is missing in SAODV and ARAN. 
For hop count protection, SAODV employed HMAC for hash chain, in contrast to 
ESAODV that used normal hash such SHA1. The use of HMAC is known to increase 
the problem of key distribution. 
Since ESAODV uses single signature, the memory consumption is much  
smaller than SAODV that used double signatures and ARAN that used Public Key 
Cryptosystem. 
6   Conclusion 
We have shown that our proposed protocol named Extra Secure Ad-Hoc On-demand 
Distance Vector (ESAODV) is effective in combating security attacks in MANET. 
Simulations based on two scenarios, i.e. without and under attacks, have been perform. 
Even if 20% of total nodes are malicious nodes, ESAODV is still effective to serve 
routing process in the network. When the protocol is compared to the existing protocol, 
ESAODV is far superior in handling message forgery, and at the same time computa-
tion efficiency, and use less memory. 
References 
1. Hallgren, S., Vollmer, U.: Quantum Computing. In: Bernstein, D.J., Buch-mann, J., Dah-
men, E. (eds.) Post-Quantum Cryptography Book Chapter. Springer, Heidelberg (2009) 
2. Lamport, L.: Password authentication with insecure communication. Communications of 
the ACM 24(11), 770–772 (1981) 
3. Merkle, R.: A certified digital signature. In: Brassard, G. (ed.) CRYPTO 1989. LNCS, 
vol. 435, pp. 218–238. Springer, Heidelberg (1990) 
4. Buchmann, J., Garcia, L.C.C., Dahmen, E., Döring, M., Klintsevich, E.: CMSS - an im-
proved Merkle signature scheme. In: Barua, R., Lange, T. (eds.) INDOCRYPT 2006. 
LNCS, vol. 4329, pp. 349–363. Springer, Heidelberg (2006) 
5. Perkins, C.E., Royer, E.M., Das, S.R.: Ad Hoc On-Demand Distance Vector (AODV) 
Routing. IETF INTERNET DRAFT, MANET working group, Draft-ietfmanet-aodv-13.txt 
(2003) 

98 
S. Mandala et al. 
 
6. Kurosawa, S., et al.: Detecting Blackhole Attack on AODV-based Mobile Ad Hoc  
Networks by Dynamic Learning Method. International Journal of Network Security 5(3), 
338–346 (2007) 
7. Zhen, J., Srinivas, S.: Preventing Replay Attacks for Secure Routing in Ad Hoc Networks. 
In: Pierre, S., Barbeau, M., Kranakis, E. (eds.) ADHOC-NOW 2003. LNCS, vol. 2865, pp. 
140–150. Springer, Heidelberg (2003) 
8. Mandala, S., Ngadi, M.A., Abdullah, A.H.: A Survey on MANET Intrusion Detection. The 
International Journal of Computer Science and Security 2(1) (2008) ISSN:1985-1533 
9. Hu, Y., Perrig, A., Johnson, D.: Packet leashes: A defense against wormhole attacks in 
wireless ad hoc networks. In: Proceedings of IEEE INFOCOM 2003 (2003) 
10. Zapata, M.G.: Key Management and Delayed Verification for Ad. Hoc Networks. Journal 
of High Speed Networks 15(1), 93–109 (2006) 
11. Sanzgiri, K., Dahill, B., Levine, B.N., Shields, C., Belding-Royer, E.: A Secure Routing 
Protocol for Ad hoc Networks. In: Proc. of IEEE International Conference on Network 
Protocols (ICNP), pp. 78–89 (2002) 
12. Barr, R., Haas, Z.J., Renesse, R.V.: Jist: An efficient approach to simulation using virtual 
machines. Software. Practice & Experience 35(6), 539–576 (2005) 
13. Barr, R., Haas, Z.J., Renesse, R.V.: Scalable Wireless Ad Hoc Network Simulation. In: 
Handbook on Theoretical and Algorithmic Aspects of Sensor, Ad hoc Wireless, and Peer-
to-Peer Networks, Auerbach, ch. 19, pp. 297–311 (2005) 
14. Buchmann, J., Coronado, C., Doring, M., Engelbert, D., Ludwig, C., Overberck, R., 
Schimidt, A., Vollmer, U.: Post-Quantum signatures (2004) (Preprint) 
15. Stainwandt, R., Villany, V.I.: A one-time signature using run-length encoding. J. Informa-
tion Processing Letters 108, 179–185 (2008) 
16. Shor, P.W.: Polynomial-time algorithms for prime factorization and discrete logarithms on 
a quantum computer. SIAM J. Computing 26, 1484–1509 (1997) 
17. Vandersypen, L.M.K., Steffen, M., Breyta, G., Yannoni, C.S., Sherwood, M.H., Chuang, 
I.L.: Experimental realization of shor’s quantum factoring algorithm using nuclear mag-
netic resonance. Nature 414, 883–887 (2001) 
18. Buchmann, J., et al.: Merkle signatures with virtually unlimited signature capacity. In: 
Katz, J., Yung, M. (eds.) ACNS 2007. LNCS, vol. 4521, pp. 31–45. Springer, Heidelberg 
(2007) 
19. Bennet, C.H., Bernstein, E.: Strenghsts and waknesses fo quantum computing. SIAM J. 
Comput. 26(5), 1510–1523 (1997) 
20. Cerri, D., Ghioni, A.: Securing AODV: The A-SAODV Secure Routing Prototype. IEEE 
Communications Magazine (2008) 
21. Bouncy Castle Provider, http://www.bouncycastle.org 
22. FlexiProvider,  
  http://www.cdc.informatik.tu-darmstadt.de/flexiprovider/ 
23. Courtois, N., Goubin, L., Patarin., J.: SFLASH, a Fast Asymmetric Signature Scheme for 
low-cost Smartcards – Primitive Specification and Supporting Documentation,  
 http://www.minrank.org/sflash-b-v2.pdf 
24. Tan, J., Chen, J., Liu, Y.: An Efficient Authentication Strategy for Reprogramming of Sen-
sor Networks. In: Int. conference on Computational Intelligence and Security (2007)  

Secure Spectrum Sensing and Decision in
Cognitive Radio Networks
Seda Demira˘g Ers¨oz1,2, Suzan Bayhan1, and Fatih Alag¨oz1
1 Department of Computer Engineering
Bo˘gazi¸ci University, ˙Istanbul, Turkey
2 T¨UB˙ITAK-UEKAE Kocaeli, Turkey
seda@uekae.tubitak.gov.tr,
{bayhan,alagoz}@boun.edu.tr}
Abstract. Cognitive radios (CRs) are proposed as a solution to ineﬃ-
cient utilization of the overcrowding spectrum and opening space for the
new wireless communication technologies. CRs have the ability to moni-
tor the spectrum activities and decide on the best transmission channel
depending on the measurements; thereby provide agility in spectrum ac-
cess. However, new abilities of CRs are exposed to new security threats
in addition to the existing security threats experienced in wireless net-
works. In this paper, we brieﬂy evaluate these new security threats and
provide cryptographic and non-cryptographic countermeasures to them.
We also present the proposed security solution in [1] and provide the
security evaluation of this proposed architecture. Moreover, we propose
a security architecture to mitigate the threats to spectrum sensing and
decision making accordingly.
1
Introduction
The basic purpose of the cognitive radio networks (CRN) is to facilitate the
spectrum access in an agile way so that spectrum holes can be utilized by unli-
censed user (secondary user, SU) whenever a licensed user (primary user, PU)
does not use it. Determining the spectrum holes and identiﬁcation of PUs are
achieved by spectrum sensing. Spectrum sensing and decision protocols must
ensure harmless inference with PU signals. Additionally, these protocols must
not require any modiﬁcation in the primary network. Hence, all SUs must adhere
to the rules speciﬁed by the cognitive protocols. Securing spectrum sensing and
decision protocol could help to provide these two main constraints of CRNs. In
the literature, there is a plethora of research on spectrum sensing and decision
protocols most of which do not consider the security issues and assume that
there is no corrupted insider or malicious adversary. However, there is also a
plethora of research showing how easily an attacker can cause a denial of service
(DoS) in CRNs which do not provide security services [2,3,4]. This shows that to
assure the persistence of PUs’ service, it is important to introduce cryptographic
protection for spectrum sensing and decision protocols, which therefore should
be one of the main design issues for CRNs.
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 99–111, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

100
S.D. Ers¨oz, S. Bayhan, and F. Alag¨oz
While a traditional radio allows minimal user interaction and has unalter-
able receiver transmitter operations, a CR has advanced functionalities such as
remote reconﬁgurability, spectrum sensing, spectrum decision, spectrum policy
based operation, spectrum mobility and geo-location. These new functionali-
ties come with new DoS vulnerabilities. Main attacks on spectrum sensing and
decision protocols can be listed as: primary user emulation attack, traditional
jamming, malicious injection, modiﬁcation or replay of spectrum sensing and
decision data. In PU emulation attacks, attacker creates a waveform suﬃciently
similar to that of the PU to trigger a false positive in the spectrum sensing algo-
rithm. In [5], a public key cryptography based PU identiﬁcation mechanism that
prevents malicious SUs from masquerading as PUs is proposed. This identiﬁca-
tion mechanism is provided by appending a signature to the PU signal. Since
the two main constraints in the design of CRs are to make sure that opportunis-
tic access of SUs is done without any disruption to the PUs and modiﬁcations
to primaries themselves, securing the procedure of PU detection is out of the
scope of this paper. There are also some methods to achieve robust PU detection
without using any cryptographic services [6,7,8]. We assume that PU detection
procedure uses one of these robust methods.
The rest of the paper is organized as follows. In Section 2, we brieﬂy deﬁne the
network model considered in this paper. In Section 3, we give an evaluation of se-
curity threats experienced in spectrum sensing and decision protocols. Addition-
ally, we summarize countermeasures in this section. In Section 4,we explain the
cryptographic solution given in [1] and potential attacks on this cryptographic
architecture. Section 5 presents our security solution for spectrum sensing and
decision period. Section 6 concludes this paper.
2
CR Network Model
We consider an infrastructure-based CRN which is divided into cells, each of
which has a BS and a number of CR nodes. As the CR engine of the cell, BS has
policy database, CR toolbox (contains neural networks, genetic algorithms, sim-
ulated annealing, bayesian reasoning, etc.), operating system, radio interface,
spectrum sensing and geo-location information components. As a CR mobile
agent, CR nodes have radio interface, sensor, policy information, operating sys-
tem and geo-locator components.
Each CR node senses the spectrum periodically and sends the outcomes of
sensing with its geolocation to the BS. BS collects all sensory information from
the nodes in its range, and then applies one or more methods from the tool-
box to this information in order to perform the spectrum decision. Finally, BS
broadcasts spectrum decision to the nodes in its range.
3
Security Threats Analysis and Countermeasures
The threat analysis is inspired from the methodology described in [9] and sum-
marized in Table 1 and Table 2. Threats are evaluated by the occurrence likeli-
hood which is assessed by the motivation of the attacker or technical diﬃculties,

Secure Spectrum Sensing and Decision in Cognitive Radio Networks
101
Table 1. Risk Analysis
Impact
Low
Medium
High
Likelihood
Unlikely Minor
Minor
Major
Possible Minor
Major
Critical
Likely
Major Critical
Critical
Table 2. Risk Countermeasures
Risk
Countermeasure
Minor
No primary need for countermeasures
Major
Threat cannot be ignored
Critical Should be minimized with highest priority
and their impacts are calculated by the level of the dramatic consequences of an
attack given rise by these. The threats related to the spectrum analysis and de-
cision protocol and their countermeasures are explained in this section. Further-
more, threat analysis is presented in Table 3. Primary User Emulation (PUE),
eavesdropping and traditional jamming, faulty data injection, intentional/non-
intentional modiﬁcations and replay attacks are employed as input to our threat
analysis.
3.1
PUE Attack
In PUE attack, attacker creates a waveform similar to that of the PU to trigger
a false positive in the spectrum sensing algorithm. SU which senses the PU
signal can correctly identify the PU signal by only applying a cryptographic
authentication protocol between PUs and SUs. Since one of the main constraints
in the design of CRs is to assure that this opportunistic access is done without
any modiﬁcations to the PUs, mitigating PU emulation attack should be achieved
by using non-cryptographic methods proposed in [6,7,8].
3.2
Eavesdropping and Traditional Jamming Attack
Traditional jamming is a simple DoS attack which is performed by transmitting
a continuous high-power signal to obstruct the reception of the original signal.
There are robust transmission security techniques like spread spectrum and fre-
quency hopping to mitigate this attack. Since the available signal bandwidth for
CRNs is mostly limited and not predictable, it is hard to implement a robust
transmission security technique on these networks.
CRNs are designed to operate in a range of frequency bands to be able to
communicate in the presence of PUs. Therefore, a traditional jammer needs to
jam a set of bands simultaneously or have the ability of detecting the CR as
it switches between a set of bands. This advantage of CRNs against traditional
jamming attack, holds only when the spectrum analysis and decision procedure
are suﬃciently secured to provide the conﬁdentiality of available band list for SU
communications. Otherwise, an attacker can easily eavesdrop the available band
list and only jam these bands to prevent the communication of SUs. Therefore, in
security architecture of CRNs, as a cryptographic solution conﬁdentiality should
be addressed to mitigate jamming attacks.
3.3
Faulty Data Injection
A malicious adversary or insider node may send false spectrum sensing data to
the BS, causing faulty spectrum sensing decision. Since CRs have the ability to

102
S.D. Ers¨oz, S. Bayhan, and F. Alag¨oz
learn from the past events, this type of attack has also long-term eﬀects on CRNs
and can be called as belief manipulation attacks [10]. Furthermore, a malicious
adversary may act as a BS and broadcast a faulty spectrum decision message,
which results in the blockage of SU communications. In case of malicious adver-
saries, cryptographic solutions like entity and message authentication can easily
prevent this type of attacks. For an insider attacker (byzantine attacker), cryp-
tographic protocols may not provide a solution without robust data fusion which
is used to combine spectrum analysis data from multiple CR nodes and gather
that information in order to achieve eﬃcient inferences. Therefore, mandatory
cryptographic services to prevent faulty data injection by a malicious adversary
should be provided and a robust data fusion technique against byzantine attack
[11] should be addressed.
3.4
Intentional/Non-intentional Modiﬁcations
In this paper, intentional modiﬁcation means that an attacker modiﬁes trans-
mitted data intentionally in a way that it is also a meaningful data which is
intended by the attacker. On the other hand, a non-intentional modiﬁcation is a
random change in data which can be caused either by jamming or interference
of the medium. Data integrity is another security service in wireless networks.
that assures the data is received exactly as it is sent without any modiﬁcations
on the transmission path. This is due to the reason that, unlike their wired
counterparts, the wireless medium is easily accessible to adversaries. Since the
modiﬁed spectrum sensing and decision messages cause faulty available band list,
spectrum sensing and decision data need to be protected during the transmis-
sion from intentional or non-intentional modiﬁcations by malicious adversaries
or noisy channels.
Table 3. Spectrum Sensing and Decision Security Threats Evaluation
Countermeasure
Threats
Likelihood Impact
Risk
Cryptographic Service
Non-Cryptographic Service
PUE
Possible
High
Critical Can
be
provided
by
some
modification
in
primary
users
like
in
[5],
not
sug-
gested
One of the robust methods
[6,7,8].
Traditional Jamming
Likely
High
Critical Confidentiality
Apply
a
robust
transmis-
sion
security
techniques
(like
spread
spectrum
or
frequency hopping)
Faulty Data Injection
Likely
High
Critical Entity and data origin au-
thentication
Robust
data
fusion
tech-
niques
Intentional
Data
Modifica-
tion
Unlikely
High
Major
Integrity
Error
detecting/correcting
codes
Non-intentional Data Modi-
fication
Likely
High
Critical Integrity
Error
detecting/correcting
codes
Message Replay
Likely
High
Critical Cryptographic nonce
-
Eavesdropping
Likely
High
Critical Confidentiality
Apply a robust transmission
security techniques
3.5
Replay Attack
Replay attack is a form of network attack in which a valid data transmission is
maliciously or fraudulently repeated. If an attacker captures a spectrum sensing

Secure Spectrum Sensing and Decision in Cognitive Radio Networks
103
data and replays it in another spectrum sensing period, it may result in faulty
spectrum decision by the BS. If an adversary sends the previously captured
spectrum decision data and replays it to the nodes in its range, then it may also
result in faulty spectrum decision.
Replay attacks can be prevented by inserting a cryptographic nonce (number
used once) in messages. Nonce is often a random or pseudo-random number issued
in a protocol to ensure that old communications cannot be reused to perform re-
play attacks. To ensure that a nonce is used only once, it should be time-variant
(timestamp) or generated with enough random bits to ensure probabilistically in-
signiﬁcant chance of repeating a previously generated value [12].
4
The Analyzed Secure Spectrum Decision Architecture
Jakimoski and Subbalakshmi in [1] propose a cryptographic protocol to secure
spectrum sensing. It is claimed that the security architecture has suﬃciently
enough protection against malicious adversary. A clustered infrastructure based
dynamic spectrum access network is considered where each cluster consists of
a cluster head and a number of mobile nodes. The cluster head is the central
authority which decides the channel availability data based on sensing data pro-
vided by the nodes in the cluster. A cluster head and corresponding nodes in its
coverage have time synchronization. Time is divided into equally sized time slots.
Three main events are listed as (1) joining the spectrum decision process, (2)
sending the spectrum sensing information, and (3) sending channel assignment.
Steps of security architecture are explained in Figure 1.
4.1
Initialization and Join Protocol
In initialization period, cluster heads and nodes are all given a public and private
key pairs generated by a certiﬁcate authority. Moreover, they all generate their
key chain as shown in Figure 1. Key Chain is used as key derivation and com-
mitment parameter for keys of message authentication code (MAC) algorithms.
Each node entering to the network performs the related tasks referred to as
join operations deﬁned by the join protocol. The protocol provides the data
origin authentication service by using digital signature. To join the spectrum
sensing period, CR node A sends a digitally signed join message to the cluster
head. This message contains the current interval (ts), identity of the cluster
head (IDH), identity of the node A (IDA), the ﬁnal value of key chain (K0)
and an additional data (D). Cluster head sends back a digitally signed message
containing the current interval (ts), its identity (IDH), the key derivation used
in the previous interval (ts−1) and some information M.
4.2
Sending Spectrum Sensing Data
In this step, data origin authentication is ensured via MAC algorithms. Node A
constructs a message that contains current interval ti, IDH, IDA and sensing

104
S.D. Ers¨oz, S. Bayhan, and F. Alag¨oz
F
F
F
KA,n
KA,n-2
KA,1
KA,n-1
KA,0
Key Chain Generation
Cluster Head H
Node A
(ts, IDH, IDA, KA,0,DA,SignA(m))
m
(ts, IDH, KH,t_s-1,M,SignH(m))
Join Protocol
m
Sending Spectrum Sensing Data
Node A
(ts, IDH, SA,Mac_KA,i(m))
µ
td
KA,i
Cluster Head H
m
Sending Spectrum Decision Data
Cluster Head H
Node A
(ts, IDH, IH, Mac_KH,i(m), SignH(m))
m
µ
td
KH,i
Notations
----------------------------------------------------------------------------------------------------------------------------------------------------------
x
F is a one-way function
x ts ± current time interval
x
td ± time delay
x
IDH - ID of cluster head
x
IDA - ID of node A
x KH,t_s-1 ± key commitment using in current interval
x KA,i ± key derivation parameter using in current interval
x KA,0 ± key commitment using in current interval
x KH,i ± key derivation parameter using in current interval
----------------------------------------------------------------------------------------------------------------------------------------------------------
Fig. 1. Steps of Security Architecture in [1]
information (S). Then node A calculates MAC of the message by using key K′
i
which is derived from Ki. The derivation function is performed by ﬂipping the
value of the last bit of the corresponding key Ki. After some time delay, node A
also sends Ki to the cluster head.
By the time the cluster head receives the Ki, by using the previous cycle
key derivation parameter Ki−1, it validates if F (Ki−1) is equal to Ki or not
(commitment). Then it derives K′
i by ﬂipping the value of the last bit of the
corresponding key Ki. After the key derivation, the cluster head calculates the
MAC of the message.
4.3
Broadcasting the Spectrum Decision
Once the cluster head decides which channels are available, it constructs a mes-
sage which contains current interval, IDH and spectrum decision I. Then the
cluster head broadcasts the message by appending the MAC of the message and
its digital signature. After some time delay it reveals the key Ks which will be
used in the key derivation of the MAC algorithm. The new nodes entering the
network which have not performed the join operations yet, verify the authenticity
of the message using the digital signature.
4.4
Security Analysis
While the integrity and source authentication of the messages are achieved by the
proposed protocol in [1], conﬁdentiality is not provided. However, eavesdropping
of the messages in the spectrum analysis and decision protocols is an eﬃcient
way of performing some DoS attacks like traditional jamming.

Secure Spectrum Sensing and Decision in Cognitive Radio Networks
105
For an eﬀective jamming scenario in CRNs, a traditional jammer needs to
have certain advanced jamming techniques. Since CRs have the ability to op-
erate in many diﬀerent bands for communicating in the presence of PUs; the
jammer has to jam many diﬀerent communication bands simultaneously or have
a reliable technique to detect CR as it switches among various bands. This is
an advantage of the CRNs over traditional jamming attacks only if there is a
strong conﬁdentiality mechanism in the spectrum sensing and decision protocol
[4]. Otherwise, as it is an issue in the proposed protocol, an attacker can easily
capture the spectrum decision message which is broadcast by the cluster head.
Next, attacker can obtain all the available communication frequencies in order
to jam the communication of SUs or to perform a PUE attack. Therefore, to
achieve a secure and robust spectrum sensing and decision procedure, conﬁden-
tiality, integrity, entity and data origin authentication should all be considered
as the building blocks of the security architecture [12].
After sending spectrum sensing or spectrum decision messages, senders reveal
the key commitment and derivation parameters without any security services
whatsoever. Therefore an attacker can easily change these parameters. Then
the messages which are prepared by using these modiﬁed data are rejected by
the receiver. If this attack is applied in spectrum sensing messages, then the
cluster head performs a spectrum decision without a strong contribution of all
nodes in its coverage area. Authors in [1] also give a solution for byzantine faults
(caused by a faulty insider node) by performing a majority voting. The proposed
solution is resistant up to ⌈Nm/2⌉-1 byzantine faulty nodes where Nm stands
for the number of nodes in the cluster. However the modiﬁed key commitments
could also be used to attack the proposed solution for the byzantine faults. The
byzantine nodes can change commitments of the other healthy nodes, and then
cluster head can perform the spectrum decisions by using only the spectrum
analysis messages of byzantine nodes which cause a spectrum decision failure.
In [1] freshness of the messages are not properly handled. It is stated that
since a node sends only one message per interval, the pair (IDBS, t) can be used
as a nonce for message of the node. Since the cluster head can perform more
than one message in a given interval, triple (Message Type Field, IDBS, t) can
be used as a nonce. In join operation, the message sent by the cluster head is not
an identical response for the join attempt. The cluster head provides the same
message for every join attempt in a given interval. Since the message types are
the same, freshness could not be performed by the triple (Message Type Field,
IDBS, t) for these messages. A cryptographic nonce should be seriously provided
to propose a security architecture which has resistance for replay attacks.
In [1], authors claim that the main advantage of the proposed protocol is that
it does not require providing secrecy of the commitment due to the fact that
key derived from the commitment is used to construct only one MAC tag which
is sent before revealing the commitment. However, one-time usage limitation of
the commitments aﬀects the spectrum decision by prohibiting retransmission of
any erroneous spectrum analysis messages.

106
S.D. Ers¨oz, S. Bayhan, and F. Alag¨oz
In addition to these critiques, performance of the proposed scheme should also
be considered. The length of the generated key chain is an important trade-oﬀ
parameter for the eﬃciency of the proposed protocol. While small key chains
open a way to rapid re-generation which causes important real-time delays; long
ones lead to larger initialization delays and memory requirements. Also, nodes
and cluster heads have to wait for some time to reveal the key commitments,
and this causes some real-time delays. Another eﬃciency problem occurs when
nodes and cluster heads check the validity of the authentication tag. They need
to verify the correctness of the key commitment and then generate the message
authentication key by applying a function on this commitment before checking
the authentication tag. These operations also cause real-time delays in spectrum
sensing protocol.
5
Proposed Security Architecture
Spectrum sensing and decision threat analysis points out that conﬁdentiality,
entity and data origin authentication, cryptographic nonce and integrity services
are essential to obtain a robust security architecture. Conﬁdentiality assures that
the data is transformed in such a way that it is unintelligible to an unauthorized
entity. In this architecture, conﬁdentiality is provided by employing a symmetric
encryption algorithm. To ensure entity authentication, key establishment and
key agrement, a modiﬁed version of Station-to-Station protocol, a variation of
Diﬃe-Hellman protocol [12], is proposed. Integrity is provided by using either
a digital signature or a MAC algorithm each of which also provides data origin
authentication. The freshness of the messages are ensured by using cryptographic
random number in entity authentication protocols, and timestamps for other
protocols. Initialization, join network, sending spectrum sensing data, multicast
spectrum decision, handover between base stations and re-keying are six main
events that are handled by the spectrum sensing and decision protocol. We will
deﬁne our security architecture by considering these six events.
5.1
Initialization
Initialization scheme proceeds as follows:
– All nodes and BSs are given their public and private key pairs issued by a
certiﬁcation authority.
– Diﬃe-Hellman key exchange parameter a and p are distributed all CR en-
tities. They are both public and used by all CR entities. Parameter p is a
prime number and parameter a (usually called a generator) is an integer less
than p, with the following property: for every number n between 1 and p−1
inclusive, there is a power k of a such that n = ak mod p
– All nodes and BSs have unique IDs and time synchronization with other CR
entities.
– Each node constructs an individual MAC algorithm key while the BSs con-
struct their individual spectrum decision group encryption key.

Secure Spectrum Sensing and Decision in Cognitive Radio Networks
107
5.2
Join Network
Since the conﬁdentiality of the messages are needed to mitigate some type of
attacks explained in Table 3, CR node and BS need to establish a key with a
mutual entity authentication protocol. This key is used as a secret key between
CR node and the BS. Furthermore, a derivation of this secret key is used for
distributing the MAC algorithm symmetric key and group encryption keys.
Since the BS multicasts the spectrum decision, a group key management
should be also provided. We decide to use Logical Key Hierarchy (LKH) as
a solution to the group key management to take advantage of the eﬃciency over
dynamic networks [13]. In LKH a central server (key server) holds a key tree
which is used for key distribution and key updates. The leaf nodes represent the
nodes whereas the root node represents the key server (BS). In the key setup
phase, each node is authenticated to the key server and ends up sharing a key
with the server. These keys are cryptographically symmetric keys shared be-
tween the two parties and are denoted as leaf keys. The secret key between node
and the BS (leaf key) as depicted in Figure 2. The other keys in LKH, related
to the node are denoted as KLKH in Figure 2.
The basic steps of the protocol proceeds as follows:
– CR node generates a secret random x which satisﬁes 1 ≤x ≤p −2 and sends
ax mod p to BS.
– BS generates a secret random y which satisﬁes 1 ≤y ≤p −2 and a ran-
dom r1, then computes the shared key K = (ax)y. Furthermore, BS signs
(r1, ay, ax) and encrypts concatenation of r1 and signature. Then BS sends
ay mod p and encrypted message to CR node.
– CR node also computes the shared key K = (ay)x and decrypts the received
message and veriﬁes the signature of BS. If successful, CR node accepts that
K is actually shared with BS.
– CR node sends an analogous message which contains r2 instead of r1. Then
it constructs the key encryption key, KEK = K XOR r1 XOR r2.
– BS similarly decrypts the received message and veriﬁes CR node’s signa-
ture. If successful, BS accepts that K is actually shared with CR node and
constructs key encryption key KEK = K XOR r1 XOR r2.
– BS sends the group keys KLKH appending a hash value of the group keys
encrypted by KEK. CR node sends MAC algorithm symmetric key KMAC
appending a hash value encrypted by KEK.
The details of join network protocol are shown in Figure 2.
5.3
Sending Sensing Data and Multicast Decision
As explained in the previous sections, in sensing period, nodes sense the spec-
trum to detect spectrum holes. Then they prepare the channel availability data
and send it to the BS. BS takes all channel availability data and applies some
cognitive methods to construct the spectrum decision. After completion of the
spectrum decision, BS multicasts it to the CR nodes in its range. The spectrum
sensing and decision scheme proceeds as follows:

108
S.D. Ers¨oz, S. Bayhan, and F. Alag¨oz
Base Station
Node
ax mod p
ay mod p, Enc_K(IDB, r1,SignB(IDB, r1, ay, ax))
x
Pre-shared Diffie-Hellman
parameter a, p
x
IDB ± unique ID of base
station
x
Diffie-Hellman key K= axy
x
Key encryption key(KEK)
         KEK = K XOR r1 XOR r2
x
Group keys = KLKH
x
Enc_K(m) -  m encrypted
by secret K.
x
SignB ± signature of base
station
x
Pre-shared Diffie-Hellman
parameter a, p
x
IDN ± unique ID of node
x
Diffie-Hellman key K= axy
x
Key encryption key(KEK)
x
KEK = K XOR r1 XOR r2
x
MAC key of Node, KMAC
x
Enc_K(m) - m encrypted
by secret K.
x
SignN ± signature of node
Enc_K(IDN, r2, SignN(IDN, r2, ax, ay))
Enc_KEK(KLKH, Hash(KLKH))
Enc_KEK(KMAC,Hash(KMAC))
Fig. 2. Proposed Security Architecture - Join Network
Base Station
Node
x
KG ± Group Key
x
IDB ± unique ID of base
station
x
S ± Spectrum decision data
x
Enc_KG(m) -  m encrypted
by KG
x
K ± symmetric Diffie-
Hellman key
x
IDN ± unique ID of node
x
ts ± timestamp
x
D ± Spectrum Sensing
x
KMAC ± MAC key of node
x
Mac_KMAC(m) ± MAC by
using KMAC as a key
IDB, IDN, Enc_K(ts, D)),Mac_KMAC(m)
m
Enc_KG(ts, S,SignB(ts, S))
Spectrum Sensing
Spectrum Decision
Fig. 3. Proposed Security Architecture - Spectrum Sensing and Decision
New Base Station
Node
x
KEK ± Key Encryption
Key
x
IDB - ID of base station
x
KLKH ± Group keys
x
K ± Encrption key of
node
x
IDN - ID of node
x
ts - timestamp
x
Req - group key
request
IDB, IDN, Enc_K(ts, Req)),Mac_KMAC(m)
m
Enc_KEK(ts, KLKH,SignB(ts, KLKH))
Fig. 4. Proposed Security Architecture - Handover between Cells
– CR node encrypts timestamp ts and spectrum sensing data D by using secret
key K, calculates the MAC value of ID of BS, ID of CR node and encrypted
data by using KMAC as a key. Then, CR node sends this message to BS.
– BS calculates the MAC value of the received message. If it is exactly same
as the received one, BS decrypts the encrypted data. If timestamp is valid,
then BS takes the spectrum sensing data as a valid one.
– After collecting spectrum sensing data, BS constructs a spectrum decision
data. Then, BS signs timestamp ts and spectrum decision S. Furthermore,
BS encrypts ts, S and signature by using the group key KG and multicasts
this encrypted message.

Secure Spectrum Sensing and Decision in Cognitive Radio Networks
109
– CR Node decrypts the received multicast message, check the validity of ts
and signature, if successful, CR node take the spectrum decision data as a
valid policy.
In Figure 3 security architecture for sending sensing data and multicast spec-
trum decision is detailed.
5.4
Handover between Base Stations
As stated in the previous sections, our network model is divided into cells each
of which contains a BS and mobile CR nodes. Due to mobility, CRs need to
handover from one cell to another that necessities a join operation with the
switched cell’s BS. In order to eliminate this necessity each time a CR node
joins the network, we assume that the serving BS sends information of new
nodes including their encryption, key encryption and MAC keys to its neighbor
BSs. In this way, nodes do not have to establish join protocol each time they
cross into a new cell. CR node, crosses into a new cell, sends a request to new BS
in order to obtain group keys. Handover schemes proceeds as follows (Figure 4):
– CR node encrypts timestamp ts and request data Req by using secret key
K, calculate the MAC value of ID of new BS, ID of CR node and encrypted
data by using KMAC as a key. Then, CR node sends this message to the
new BS.
– BS checks the validity of the MAC value. If successful, BS decrypts the
received message and checks the validity of ts. If timestamp is valid, BS
checks the request. BS signs ts and group keys, encrypts concatenation of
ts, group keys and signature by using KEK and sends to CR node.
– CR node decrypts the received message, checks the validity of timestamp
and signature. If successful, CR node accepts the group keys.
5.5
Re-keying
In order to protect forward and backward communication secrecy of groups,
whenever a mobile node enters or leaves the group, all keys shared with that
mobile node needs to be re-keyed. However, for spectrum sensing and decision
protocol, there is no need to re-key the group keys after a handover occurs,
since the encrypted data does not have any conﬁdentiality over the nodes in the
network. The conﬁdentiality is provided to make sure that any adversary can
not take an advantage for some DoS attack which is shown in Table 3. Re-keying
is needed to be handled, when a node leaves the entire network.
6
Conclusion
In most of works in the literature, it is shown that if security services are not
provided, spectrum sensing and decision protocols are more vulnerable to secu-
rity threats. In this paper, we have discussed some security threats to spectrum

110
S.D. Ers¨oz, S. Bayhan, and F. Alag¨oz
sensing and decision protocol, given their analysis and cryptographic or non-
cryptographic countermeasures. Security architecture in [1] has been examined
and the security vulnerabilities of this architecture have been discussed. Finally,
we have proposed security architecture for spectrum sensing and decision proto-
col which has provided all security services that is shown as a countermeasure
in Table 3.
Acknowledgment
This work has been supported by the State Planning Organization (DPT) of Re-
public of Turkey under the project TAM, with the project number 2007K120610.
S. Demira˘g Ers¨oz also acknowledges the support of TUBITAK-UEKAE (Na-
tional Research Institute of Electronics and Cryptology).
References
1. Jakimoski, G., Subbalakshmi, K.: Towards secure spectrum decision. In: IEEE Intl.
Conf. on Commun., ICC 2009 (2009)
2. Brown, T., Sethi, A.: Potential cognitive radio denial of service attacks and reme-
dies. In: Proceedings of the international symposium on advanced radio technolo-
gies, pp. 26–28 (2007)
3. Jakimoski, G., Subbalakshmi, K.: Denial-of-service attacks on dynamic spectrum
access networks. In: CogNets Workshop, IEEE International Conference on Com-
munications Workshops, pp. 524–528 (2008)
4. Brown, T., Sethi, A.: Potential cognitive radio denial-of-service vulnerabilities and
protection countermeasures: a multi-dimensional analysis and assessment. Mobile
Networks and Applications 13, 516–532 (2008)
5. Mathur, C., Subbalakshmi, K.: Digital Signatures for Centralized DSA Networks.
In: 4th IEEE Consumer Communications and Networking Conference, CCNC 2007,
pp. 1037–1041 (2007)
6. Jin, Z., Anand, S., Subbalakshmi, K.: Detecting primary user emulation attacks in
dynamic spectrum access networks. In: IEEE Intl. Conf. on Commun., ICC 2009
(2009)
7. Chen, R., Park, J., Reed, J.: Defense against primary user emulation attacks in
cognitive radio networks. IEEE Journal on Selected Areas in Communications 26,
25–37 (2008)
8. Anand, S., Jin, Z., Subbalakshmi, K.: An analytical model for primary user em-
ulation attacks in cognitive radio networks. In: Proc. IEEE Symposium of New
Frontiers in Dynamic Spectrum Access Networks (DySPAN 2008), pp. 1–6 (2008)
9. ETSI: Telecommunications and internet protocol harmonization over networks
(TIPHON) release 4; protocol framework deﬁnition; methods and protocols for
security; part 1: Threat analysis. Technical Speciﬁcation. Technical Report ETSI
TS 102 165-1 V4.1.1, ETSI (2003)
10. Clancy, T., Goergen, N.: Security in cognitive radio networks: Threats and mitiga-
tion. In: International Conference on Cognitive Radio Oriented Wireless Networks
and Communications, Crowncom 2008 (2008)
11. Rui-liang, C., Park, J., Hou, Y., et al.: Toward secure distributed spectrum sensing
in cognitive radio networks. IEEE Communications Magazine 46, 50–55 (2008)

Secure Spectrum Sensing and Decision in Cognitive Radio Networks
111
12. Menezes, A., Van Oorschot, P., Vanstone, S.: Handbook of Applied Cryptography.
CRC Press, Florida (2001)
13. Arslan, M., Alagoz, F.: Security issues and performance study of key management
techniques over satellite links. In: 11th Intenational Workshop on Computer-Aided
Modeling, Analysis and Design of Communication Links and Networks, pp. 122–
128 (2006)
14. Cordeiro, C., Challapali, K., Birru, D., Shankar, S.: IEEE 802.22: an introduction to
the ﬁrst wireless standard based on cognitive radios. Journal of communications 1,
38–47 (2006)
15. Burbank, J.: Security in cognitive radio networks: The required evolution in ap-
proaches to wireless network security. In: 3rd International Conference on Cognitive
Radio Oriented Wireless Networks and Communications, CrownCom 2008, pp. 1–7
(2008)
16. Schneier, B.: Applied cryptography: protocols, algorithms, and source code in C.
Wiley, India (2007)

A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 112–122, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
DRLC: A New Robust and Dynamically Load Balanced  
Clustering  Scheme for  Wireless Sensor Networks 
Ismail Tellioglu and Hacı A. Mantar 
Gebze Institute of Technology, Computer Science Department, 
Istanbul Street No: 101 41400, Gebze/Kocaeli Turkey 
{tellioglu@,hamantar}@bilmuh.gyte.edu.tr 
Abstract. In this  paper  we present a robust and  dynamically load bal- anced 
clustering scheme (DRLC) for wireless sensor networks. Firstly, we introduce  
a simple  but effective dynamic and robust cluster head elec- tion protocol that 
makes decision  based  on the  nodes’ energy  level and the number of 
neighbors. Since re-clustering is heavy and  costly process, providing surviv-
ability of clusters in case of the cluster head (CH) failure is an important issue. 
To overcome this problem, we introduce  a dynamic backup CH selection 
method  that doesn’t need global  topology knowl- edge or extra nodes in the 
network. In case of CH failures this backup  CH undertakes the CH’s role and 
recovers cluster immediately. Secondly, we introduce a dynamic  load  balanc-
ing method to prevent  overloading of CHs in network. When  a CH’s load in-
creases, the CH sends a message to inform its members about the load so that 
some of these members may change their CHs. This decreases the loads of 
overloaded CHs and increases the lifetime  of that cluster. The experimental  re-
sults verify the achievement of our model. 
Keywords: Wireless  sensor  networks, clustering, load  balancing, ro- bustness. 
1   Introduction 
Recent advances  in micro electronic  systems  and  wireless communication tech- 
nology provide opportunity to develop small and low powered sensors. Wireless 
sensor networks (WSNs) consist of hundreds or thousands of small and low pow- 
ered sensors that sense the areas for various  applications such as health,  border 
monitoring, battlefield  surveillance,  habitat monitoring [1] [2][3]. 
In WSNs, sensors collect various  data  from environment and  send this  data 
to a base station (BS) directly  or via multi-hop  communication. Sensors gener- 
ally have low powered batteries and operate  in harsh  environments. These harsh 
environments limit  transmission capabilities and  increase  energy consumption. 
In addition, changing  or recharging  batteries is inefficient or unfeasible  in most 
situations. Thus,  developing energy efficient routing  and  topology control  algo- 
rithms  is an important issue in WSNs. 
Clustering  is an effective method  for reducing  energy consumption. In clus- 
tering methods,  a WSN is partitioned into smaller groups called clusters. Cluster 

 
DRLC: A New Robust and Dynamically Load Balanced  Clustering  Scheme 
113 
heads are the leader of the groups, which are responsible for gathering data  from 
their  members,  applying  data  aggregation  or fusion and  sending  this  data  to a 
BS. In  recent years  a lot  of protocols  have  been  developed  for clustering  i.e., 
[4][5][6]. Generally,  these protocols focused on optimal  clustering.  However, clus- 
tering  also brings some problems to WSN. One of them is the single point of fail-
ure. Since sensors operate in harsh environments and prone to failures, a CH may 
fail in unexpected time. When a CH fails or runs out of energy, the whole cluster 
is isolated until next clustering  process. This means a lot of energy consumption 
unduly. 
Another problem  is the  load  balancing  in  clusters. Some  clusters  may  be 
exposed to heavy data  traffic while other clusters have light traffic. This situation 
may happen  because  the traffic generated by sensors is generally  not  uniform. 
Thus,  even if two  clusters  have the  same  member  size, their  CHs’  traffic  load 
may vary significantly. 
In this  work, we firstly  present a dynamic  clustering  algorithm. Differently 
from previous studies which use static backups, our algorithm uses dynamic backups. 
The dynamic backup  scheme increases the effectiveness backup CH when the traf-
fic generated by sensors is not uniform. 
Secondly, we introduce a dynamic load balancing method. When a CH has 
heavy traffic load, it informs some of members by broadcasting a message. Upon 
receiving this message, sensors that can communicate with other CHs change their 
clusters if their  join request  is accepted  by the adjacent CH. 
The rest of the paper is organized as follows. In Section 2 we give a more de- 
tailed overview of some related works. Section 3 explains the proposed algorithm 
in details.  Section  4 gives the simulation  results  and  we conclude  the  paper  in 
Section 5. 
2   Related Works 
A lot  of clustering  based  protocols  have been proposed  for WSNs.  In LEACH 
[5] every sensor has equal chance to become CH independently from its energy 
level. In clustering  phase  every sensor, which has not  been in cluster  head  role 
yet,  selects  a random  number  between  0 and  1. If this  number  is less than  a 
pre-defined  threshold  value calculated  by using desired cluster  head  percentage 
and  round  number,  sensor is a CH candidate for that round.  LEACH does not 
consider  CH  fault  problem  and  periodically  changes  cluster  heads  to  balance 
energy consumption of nodes. HEED [6] is another  clustering algorithm that does 
not pay attention for fault tolerance.  HEED uses residual energy information and 
intra  cluster  communication cost to build clusters  and to elect CHs. 
DED  [7] is one of the  clustering  algorithms  that consider  robustness. DED 
uses energy consumption rate  and residual  energy information to determine CH 
probability for each sensor. Sensors with high probabilities are tentative  cluster 
heads.  Each  sensor  joins least  costly  cluster  head.  For  robustness, each sensor 
determines a back up for its own. The preferable  back up for each sensor is an- other 
CH within cluster range. If sensor can not find a backup CH within cluster range,  it  
determines a neighbor  node whose CH is different than  its  own or a neighbor node 

114 
I. Tellioglu and H.A. Mantar 
whose CH is the same but  has a backup.  If CH fails, each sensor sends its data  via 
backup.  DED does not offer any method  for CH failure detec- tion.  CMATO  [8] 
presents  a model that does not  use any backup  mechanism. Detecting CH failure is 
a common duty of all cluster  members.  If a sensor can’t overhear  of CH messages 
for a while, it constructs an unable list and broadcasts this  list.  Every  sensor  that 
can not  hear  CH messages  for a certain  time  adds its own ID to that list.  If the  
list’s size reaches  a predetermined  percentage  of cluster size, CH is considered as 
failed. Cluster  members reconstruct a cluster by using a function  that considers 
sensors’ energy and neighbour  size. CMATO  also introduces a method for handling  
medium errors. Since CMATO  determines CH fail by cooperation  of all cluster  
members,  determining CH faults  can be costly. 
FTEP [9] proposes a two-level dynamic  clustering  method  and  uses backup 
CH for the  fault  tolerance.  Each CH determines a backup  that has highest  en- 
ergy within its members.  If CH fails, predetermined backup overtakes CH’s role. 
FTEP determines a static  back up at the end of clustering  process and does not 
consider energy consumption of back up. If back up is exposed to heavy traffic until 
overtaking CH role, it can drain  its energy rapidly  after  taking  CH role. 
An alternative approach  is to dedicate  two cluster heads for each cluster [10]- 
[12]. In [11], a node placement problem is handled  as k minimum dominating set 
problem.  This method  tries to find minimum  subset of CHs in which every node 
can communicate at least k cluster  head. To achieve fault  tolerance  k should be 
selected more than  one. Both  of these methods’ achievements  are dependent on 
node density,  and they  enforce each sensor to reach more than  one CH directly. 
Similarly,  REED  [12] constructs a k independent CH-overlays  on the  network 
where each node needs to reach  k of these  CHs. The  achievement  of REED  is also 
dependent on node density.  CH number can be much more than  the number in con-
ventional  clustering  methods. 
For load balancing problem, in [13], CHs broadcast a message indicating  start of 
clustering  and sensors reply this message. Nodes that can communicate with only  
one  gateway  are  assigned  to  that gateway.  Every  gateway  calculates  its own 
load.  This  method  employs  an  objective  function  to  keep the  variance  of the  
cardinality of each cluster  minimal.  The  objective function  is calculated  by as-
signing  the  nodes to CHs they  can reach.  The  node becomes the  part  of the clus-
ter  for which it minimizes the objective function.  Thus, the load of gateways is kept 
uniform. However, this method does not address the load balancing  prob- lem within  
clusters  and  fault  tolerance  problem.  RCCT  [14] also considers load balancing 
and brings a new approach  for LEACH. It improves clustering by using energy level 
and  distance  knowledge among  cluster  heads.  The  most  energetic sensor in its 
members  receives data  and  sends it to BS. Thus,  the  cluster  head role is parti-
tioned between cluster  head and the most energetic  member  of CH. 
RCCT also consider fault tolerance mechanism. If the  members does not hear 
broadcast message during two cooperation slots, they run a local re-clustering 
mechanism. 

 
DRLC: A New Robust and Dynamically Load Balanced  Clustering  Scheme 
115 
3   Proposed Method 
We assume that sensors have the following common features: 
− Each sensor is static  and sensors may have different or equal initial energy. 
− Each sensor has bidirectional links with its neighbors. 
− Each sensor has minimum three different transmission ranges. Mini-
mum level (l1 ) transmission range  is used  by cluster  members  for 
multi  hop  commu- nication.  Second level (l2 ) transmission range  is 
used for clustering  process and the highest  transmission range (l3 ) 
is used for only CH-CH and CH-BS communication. These trans-
mission ranges should be arranged according  to features  of WSN. 
3.1  Cluster Formation Algorithm 
Cluster  formation  algorithm consists of three  phases as shown in Fig. 1. Table1 
shows the  various  symbols used in the  algorithm. Firstly,  each sensor finds its 
neighbors  in l2    transmission range.  Then  each  sensor  calculates  a normalized 
waiting  time  for sending  cluster  head  advertisement that consists  its  current 
energy level and neighbor count (1). 
T = (A/(Ecur + 1)) + (B/|Nl2 |)                                 (1)  
The  sensors that have more energy and  more neighbors  have higher  probability to 
be cluster  head. This has two advantages. Since cluster  heads consume much  more  
energy  than  cluster  members,  choosing  more  energetic  sensors  as CH prolongs  
cluster’s  lifetime.  By considering  neighbor  count,  we avoid choos- ing sensors 
placed at extreme  points  of network  as cluster  head. 
In second phase,  each sensor  waits  for C Hadv   messages  from other  sensors 
and  adds  these  messages  to  its  C Hcand   queue  until  the  waiting  time  ex-
pires. These C Hadv  messages include  IDs, energy levels and  positions  of send-
ers.  If a sensor accepts one or more C Hadv  messages, it cancels its waiting time 
and gives up contenting  for being cluster head in this period. When a sensor’s wait-
ing time expires and its’ C Hcand  is empty, this sensor sends C Hadv  messages 
to all sensors in its Nl2  list and  declares  that it is a cluster  head  for this  round.  
At the  end of this  phase,  sensors calculate  each CH’s cost in their  C Hcand  
queue by using (2), which takes cost function  energy levels and distances  of CH 
candidates into account.  To calculate  this  cost,  sensors obtain  energy and  the  
position  of CHs from C Hadv messages. After  calculating these  costs,  sensors 
send join requests to the least costly CH candidate. These join requests  also include 
current energy level of nodes to be used in back up CH selection. 
In last  phase, every sensor determines intra-cluster neighbours  located  in l1 
transmission range  and  adds  these  neighbors  to Nin list.  Each  CH determines 
its  neighbor CHs  according  to received  messages. CH  also  chooses  the  most 
energetic sensor in minimum transmission range as backup and informs this node by 
sending cluster members  list. This  member  list is used by backup in case of any  CH  
failure. CH chooses backup CH within minimum  transmission range and lets  
 

116 
I. Tellioglu and H.A. Mantar 
 
Fig. 1. Cluster formation algorithm 
minimum change in coverage area between CH and backup. Thus, when backup  un-
dertakes CH role, the cluster  is not exposed to significant changes. 
C Hcost = α ∗ d(i, C Hcand ) + (1 − α) ∗ (C Hcand .Ecur )                   (2) 
3.2   Rescuing From CH Faults 
CH and  backup  CH send alive messages to each other  to inform that they  are alive 
at the end of each transmission period. These aliveness messages are small packets  
including  energy  levels of nodes. Besides,  each sensor  sends  its  energy level for 
each period  by employing  piggybacking  in one of data  packets.  Thus, CH can 
dynamically  change the  backup  according  to the  energy levels of other members  
and backup. 
If backup does not receive any aliveness packet from CH during two transmis- 
sion period or CH’s energy level falls below to critical energy level, the backup CH 
assumes that CH has lost its functionality. It then  informs all cluster  members 
about this situation and sends its position and energy knowledge via C Hadv mes- 
sage immediately. The sensors that are not member of failed cluster and overhear 
backup’s message update  their C Hcand  queue. After receiving this message,  

 
DRLC: A New Robust and Dynamically Load Balanced  Clustering  Scheme 
117 
clus- ter members employ the lines 13-17 and 24-27 in clustering  formation  algo-
rithm. Sensors calculate  costs according to the last C Hadv  messages they received 
from other  CHs and the messages they received from backup.  Then  they send join 
re- quest to the sensor that has the minimum  cost. Thus, all the sensors in member 
list choose new cluster  head and  determine their  intra-cluster neighbors.  Failed 
cluster  head is healed locally without needing a global clustering  process. More- 
over, by updating the  backup dynamically,  the  most  energetic  sensor overtakes 
CH role and new CH can carry  out this role longer. 
Table 1. Symbols and Their Meanings 
A and  B 
Positive  constants. 
T 
Waiting time before sending C Hadv
message. 
C Hadv 
Ch  advertisement message. 
C Hcand 
List  of reachable CHs and  their 
Ecur 
Current energy  level of node. 
Nin 
Within cluster  neighbor  list in level
1 transmission range. 
Nl2 
Neighbor list in level 2 transmission
range. 
Tmax 
Maximum   time  for  cluster  forma-
tion  phase. 
C Hid 
CH id for node. 
C Hbacku p 
Backup  CH for node. 
l1 
Minimum  
transmission 
range
(taken  10m). 
l2 
Medium  transmission range  (taken
35m). 
l3 
Maximum  
transmission 
range
(taken  50m). 
C Hcost 
Cost  of choosing that CH candidate
as cluster  head. 
3.3   Dynamic Load Balancing 
We propose a dynamic load balancing scheme to avoid overloaded CHs from con- 
suming their  energy rapidly.  When a CH’s traffic rate  reaches to predetermined 
threshold value,  CH  assumes  that some of its  members  generate  high  traffic. 
Then,  CH sends a broadcast message to all of its members to inform them about 
this situation. Members that are not in CH level 1 transmission range and have 
other  cluster head options in their C Hcand list  try  to  be member of another clus-
ter. They send join request to the least costly CH  in  their  C Hcand lists. When the  
neighbor CH receives this  message and  its traffic rate  is proper, CH sends  accep-
tance message and  adds  that sensor to its member  list.  Otherwise, CH sends a 
reject message. If a sensor receives acceptance message, it discovers new intra-
cluster neighbors and updates its Nin  list. For saving energy, if a sen- sor receives 

118 
I. Tellioglu and H.A. Mantar 
reject reply, it doesn’t employ this procedure again until least costly neighbor CH 
changes. 
By employing this dynamic load balancing algorithm, some members of over- 
loaded cluster are transferred to the  neighbor clusters. Thus, the  traffic load of clus-
ter  decreases  and  the  overloaded  cluster  head  perform its  duty  for longer time. 
4   Simulation Results 
Simulations  are performed  in 100 x 100 square  meter  area  with  randomly  dis- 
tributed 100 sensors.  Sensors’ transmission level values are  given  in  Table  1. 
Each  sensor’s initial  energy level is between 0.9-1 joules. Packet  lengths  are 10 
Kbit  for data  packets,  2 Kbit  for C Hadv  packets and 0.1 Kbit  for aliveness 
pack- ets. We assume  an idealized  MAC layer with no collision for simplification  
and nodes consume negligible energy when they are not sending or receiving packets 
by putting themselves  into sleep mode. 
We used the same energy model described in [15] and we consider consumed en-
ergy for data,  C Hadv  and aliveness packets. 
Etx = (α1   + α2 d2 ) ∗ k                                                  (3) 
Erx = α1   ∗ k                                                          (4)  
Where  Etx   is the  energy  to send k bits  and  Erx  is the  energy  to receive  k 
bits. a1 and a2 are numerical  constants 50 nj/bit and  100 pj/m2 , respectively and 
r is the number  of bits in a packet. 
In figures and  tables  DRLC  represents  explained  method  in Section  3 with all 
details and properties. Fig. 2 shows average energy consumption performance of 
LEACH  and  DRLC  and  Fig.  3 shows the  packet  sending  performances  of 
DRLC and LEACH until the first CH runs out of energy. Both figures prove the ef-
fectiveness of DRLC in clustering  compared  to LEACH. Since DRLC considers  
 
 
Fig. 2. Average  energy  consumption performance 

 
DRLC: A New Robust and Dynamically Load Balanced  Clustering  Scheme 
119 
 
Fig. 3. Packet sent performance until  firs CH dies 
energy levels of nodes during  CH election phase, DRLC is more likely to choose 
more energetic  nodes than  LEACH.  Thus,  as shown in Fig. 3, DRLC  can send 
more packets  until first CH runs out of energy. Besides by considering  neighbor 
counts  of nodes during  clustering  phase  DRLC  constructs more stable  clusters 
and avoids choosing CHs at extreme  points. Thus, DRLC shows a better average 
energy consumption performance. 
Fig. 4 compares  DRLC,  LEACH and  the  version of DRLC  without backup al-
gorithms’  performances  in case of CH failures. 10 CH faults were generated for 
random  clusters  in the  same time  intervals  for all three  methods.  As Fig.4 de- 
picts, DRLC gives the best performance  among these three algorithms.  LEACH’s 
performance  decreases  dramatically in case of CH failures.  Since LEACH does not 
consider CH failure problem,  its networks  is re-clustered  in case of CH fail- ure, 
and therefore  average energy consumption of network increases. DRLC also gives 
better performance  than  DRLC-without backup.  As shown in the  figure DRLC  
consumes  %5 less energy per packet.  These two observations prove that consider-
ing  CH  faults  is an  important need  for clustering  based  methods  re- gardless  of 
how good clustering  method  is. Besides, DRLC  solves this  problem effectively and 
decreases the effects of CH faults. 
 
Fig. 4. Performance evaluation for CH faults 

120 
I. Tellioglu and H.A. Mantar 
Table 2 shows the results  for the comparison  between DRLC and the version of 
DRLC with static  backup.  Since DRLC dynamically  updates backup CH and 
chooses the most energetic backup, in each situation, the dynamic backup always 
can send more packet than  the static  backup. 
Table 2. Dynamic and  Static Backup  CH Comparison 
Number 
of  
CH 
fault 
Number   of   pack-
ets can be sent  by
new CH  (DRLC) 
Number   of   pack-
ets can be sent  by
new  CH   (DRLC-
static  backup) 
1 
1187 
1167 
2 
1206 
1135 
3 
1162 
1141 
4 
1188 
1131 
5 
1176 
1152 
6 
1088 
1067 
7 
1159 
1096 
8 
1143 
1114 
9 
1154 
1134 
10 
1151 
1127 
 
Fig. 5. Load balancing performance for 1.5 packets/1second 
Fig 5, 6 and 7 show the load balancing  performance  of DRLC,  LEACH and 
DRLC-without load balancing  algorithms. The results  are obtained under  vari- 
ous traffic rate  until  first CH runs  out  of energy. For  this  experiment we chose 
two clusters  for each  method  and  generated high  traffic  within  these  clusters. 
DRLC  gives the  best performance  for all three  states.  As shown  in the  Fig.7 the  
effect of load  balancing  becomes  more  critical  in heavy  traffic  conditions. DRLC  
prolongs  the  overloaded  cluster’s  lifetime  by  spreading  the  traffic  load among 
clusters  and sends much more packets  to BS than  the other  methods.  If we con-
sider  that re-clustering is a heavy process and  consumes  high energy, it is obvious 
that prolonging  the  clusters’ lifetime and avoiding from re-clustering process 
promises a longer lifetime for network. 

 
DRLC: A New Robust and Dynamically Load Balanced  Clustering  Scheme 
121 
 
Fig. 6. Load balancing performance for 2 packets/1second 
 
Fig. 7. Load balancing performance for 3 packets/1second 
5   Conclusion 
In  this  paper,  we developed  a  dynamic,  robust  and  load  balanced  clustering 
protocol  for wireless sensor  networks.  The  proposed  model dynamically  builds 
clusters by considering the energy and number of neighbour sensors. In addition, the 
proposed method  considers CH faults  and employs dynamic  backup  CH for healing 
cluster  locally and quickly. By balancing  load among clusters  heads, our method 
prolongs the clusters  lifetime and avoids heavy re-clustering process. 
References 
1. Akyildiz, I.F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: A Survey of sensor networks. 
IEEE Communications Magazine 40(8), 102–114 (2002) 
2. Schwiebert, L., Gupta, S.K.S., Weinmann, J.: Research challenges in wireless networks of 
biomedical sensors. Mobile Computing and Networking, 151–165 (2001) 

122 
I. Tellioglu and H.A. Mantar 
3. Mainwaring, A., Polastre, J., Szewczyk, R., Culler, D., Anderson, J.: Wireless sensor net-
works for habitat monitoring. In: ACM International Workshop on Wireless Sensor Net-
works and Applications (WSNA 2002), Atlanta, GA (September 2002) 
4. Manjeshwar, A., Agrawal, D.P.: TEEN: a protocol for enhanced efficiency in wireless sen-
sor networks. In: Proceedings of the 1st International Workshop on Parallel and Distrib-
uted Computing Issues in Wireless Networks and Mobile Computing, San Francisco, CA 
(April 2001) 
5. Heinzelman, W., Chandrakasan, A., Balakrishnan, H.: Energy-efficient communication 
protocol for wireless sensor networks. In: Proceeding of the Hawaii International Confer-
ence System Sciences, Hawaii (January 2000) 
6. Younis, O., Fahmy, S.: HEED: A hybrid, energy-efficient, distributed clustering approach 
for ad-hoc sensor networks. IEEE Transactions on mobile computing 3(4) (October- De-
cember 2004) 
7. Hasan, M.M., Jue, J.P.: Resource efficient survivable clustering for wireless sensor net-
works. In: IEEE Global Telecommunications Conference, November 26-30, pp. 1154–
1158 (2007) 
8. Lai, Y., Chen, H.: CMATO:Energy-efficient fault tolerant mechanism for clustered wire-
less sensor networks. In: Computer Communications and Networks, ICCCN 2007 (2007) 
9. Bansal, N., Sharma, T.P., Misra, M., Joshi, R.C.: FTEP: Fault tolerant election protocol for 
multi-level clustering in homogeneous wireless sensor networks. In: 16th IEEE Interna-
tional Conference on ICON 2008, December 12-14 (2008) 
10. Yang, M., Wang, J., Gao, Z., Jiang, Y., Kim, Y.: Coordinated robust routing by dual clus-
ter heads in layered wireless sensor networks. In: IEEE Proc. of the 8th Int’l. Symposium 
on Parallel Architectures, Algorithms, and Networks, pp. 454–461 (2005) 
11. Kuhn, F., Moscibroda, T., Wattenhofer, R.: Fault-Tolerant clustering in ad-hoc and sensor 
networks. In: Proceedings of the 26th IEEE International Conference on Distributed Com-
puting Systems (2006) 
12. Younis, O., Fahmy, S., Santi, P.: An architecture for robust sensor network communica-
tions. Int’l. Journal of Distributed Sensor Networks (IJDSN) 1(3/4), 305–327 (2005) 
13. Guptai, G., Younis, M.: Performance evaluation of load balanced clustering of wireless 
sensor networks. In: 10th International Conference on ICT 2003, February 23 - March 1, 
vol. 2, pp. 1577–1583 (2003) 
14. Ghelichi, M., Jahanbakhshand, S.K., Sanaei, E.: RCCT: Robust clustering with coopera-
tive transmission for energy efficient wireless sensor networks. In: Fifth International Con-
ference on Information Technology: New Generations, ITNG 2008, April 7-9 (2008) 
15. Bhardjaw, M., et al.: Upper bound on the lifetime of sensor networks. In: Proceedings of 
ICC 2001 (June 2001) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 123–136, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Clustered De Bruijn Based Multi Layered Architectures 
for Sensor Networks 
Anas Abu Taleb, Jimson Mathew, and Dhiraj K. Pradhan 
University of Bristol, Department of Computer Science, 
BS8 1UB Bristol, UK 
{abutaleb,csxjm,pradhan}@cs.bris.ac.uk 
Abstract. Wireless sensor networks are expected to operate in an unattended 
manner for long periods of time. As a result, they should be able to tolerate 
faults and maintain a reasonable performance level. Therefore, we propose two 
fault tolerant clustered De Bruijn based multi layered architectures, a fault tol-
erant routing scheme and a distributed fault diagnosis algorithm. The perform-
ance of the proposed work was analyzed according to the end-to-end delay and 
the data success rate. Also, the performance was compared to that of mesh  
networks. Our simulation results show that De Bruijn based networks perform 
better in both fault free and faulty situations.   
Keywords: Wireless Sensor Network, Fault Tolerance, De Bruijn Graph, Fault 
Diagnosis. 
1   Introduction and Related Work 
Wireless Sensor Networks can be defined as a highly distributed networks consisting 
of small, light weight sensor nodes, which are densely deployed to monitor a system 
or phenomena by measuring physical parameters such as temperature, or pressure [1]. 
These networks are expected to operate in an unattended manner for periods ranging 
from days to years without any human intervention. As a result, wireless sensor net-
works (WSN) should have the ability to tolerate faults and maintain a reasonable 
performance level.  
When addressing fault tolerance in WSNs, two types of node failures must be 
taken into account; the first is when the sensor has died or not providing data at all. 
While the second type occurs when we have an active node providing incorrect data. 
Identifying faulty sensor nodes is not an easy task as it is difficult and time consuming 
for the base station to keep the information about all the sensor nodes in the network.  
Therefore, there is a need for distributed algorithms that are capable of providing a 
certain level of fault tolerance. 
In this paper we propose two clustered De Bruijn based architectures. In every cluster 
the sensor nodes are arranged in a De Bruijn graph; this graph has unique features that 
make it suitable to implement a high-performance, low energy consumption, and more 
reliable sensor network structure. Also, a distributed fault diagnosis algorithm, inspired 
by our previous work in [2], is proposed and applied in each cluster. 

124 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
Energy efficiency and fault tolerance are two important characteristics that must 
exist in every sensor network. As a result, researchers addressed those issues in dif-
ferent ways. In order to achieve energy efficiency researchers proposed clustering 
algorithms to reduce energy consumption. The algorithm proposed in [3] is a distrib-
uted and a randomized one that arranges the sensor nodes in clusters, also the pro-
posed algorithm is extended to achieve a hierarchy consisting of clusterheads in order 
to reduce energy consumption. In [4, 5] cluster based techniques were proposed to 
improve the network’s efficiency and information retrieval abilities. In [6] a cluster 
based protocol is proposed aiming to distribute energy load in an even way by ran-
domly rotating the clusterheads. The protocol proposed in [6] was enhanced in [7] by 
restricting communication to the neighboring nodes only. Furthermore, the research 
proposed in [8] is also based on clustering with a single sink in the network. As sensor 
nodes are considered to be self-organized, they can find the route to the sink without 
the need for node’s location information.  
To achieve fault tolerance, some of the proposed techniques are based on clustered 
WSNs while others are not. The authors in [9] and [10] proposed techniques to 
achieve fault tolerance and to diagnose sensor nodes while having a clustered net-
work. In the research proposed in [11] disjoint paths and comparisons at the sink are 
used to identify faulty paths. The authors in [12] tackled the problem of achieving 
fault tolerance in WSNs by combining hierarchical routing with having the sensor 
nodes in each level interconnected according to a De Bruijn graph. 
The rest of this paper is organized as follows. In Section 2, we present the basic 
definitions and notations and the fault model. The proposed architectures and algo-
rithms are presented in Section 3. Section 4 presents the simulation results and con-
clusions are drawn in Section 5. 
2   Preliminaries 
The network topology has a great impact on the network performance and reliability. 
There are well-defined relationships between the topology and the packet delay, the 
routing algorithm, fault tolerance and fault diagnosis. For instance, the packet delay is 
directly proportional to the distance between the source and destination nodes, i.e. it 
depends on the number of hops required for the packet to reach its destination. The 
motivation behind this work is to investigate a new routing algorithm and hierarchies 
and to study their effect on the network performance. In addition, we propose a fault 
diagnosis algorithm which was inspired by our previous work in [2]. 
The work proposed in this paper is based on De Bruijn graph [13]. This graph has 
interesting properties that make it important to investigate its use in WSNs. The de-
gree of this graph is bounded, which means the degree of the network remains fixed 
even when the network size increases. In addition, this graph has interesting proper-
ties such as small diameter, high connectivity and easy routing. Furthermore, De 
Bruijn graph contains some important networks such as ring. Regarding fault toler-
ance and extensibility, these graphs maintain a good level of fault tolerance and self-
diagnosability.  For instance, in the presence of a single fault in the network, it takes 
four additional hop to detour around the faulty node and the control information 

 
Clustered De Bruijn Based Multi Layered Architectures for Sensor Networks 
125 
 
needed to do so can be integrated locally between the faulty node’s neighbors. Also, 
De Bruijn graph is extensible in two methods that are described in [13]. 
The De Bruijn graph denoted as DB(r, k) has 
kr
N =
 nodes with diameter k and 
degree 2r. This corresponds to the state graph of a shift register of length k using r-ary 
digits. A shift register changes a state by shifting in a digit in the state number in one 
side, and then shifting out one digit from the other side. If we represent a node by 
(
)
0
1
2
1
...,
,
i
i
i
i
I
k
k
−
−
=
. 
(1)
where
)1
(
,...,
1,0
,
−
∈
r
j
i
,
(
)1
0
−
≤
≤
k
j
, then its neighbors are represented by 
p
i
i
i
k
k
0
3
2
...,
,
−
−
and 
1
2
1
,...,i
i
pi
k
k
−
−
, where 
(
)1
,...,
1,0
−
=
r
p
. 
(2)
The DB(2, k), which is called binary de Bruijn graph, can be obtained as follows. If 
we represent a node I by a k-bit binary number, say,
0
1
2
1
...,
,
I
I
I
I
I
k
k
−
−
=
, then its 
neighbors can be presented as 
0
...,
,
0
1
2
I
I
I k−
, 
1
...,
,
0
1
2
I
I
I k−
, 
1
2
1
...,
,
0
I
I
I
k
k
−
−
, 
and 
0
2
1
...,
,
1
I
I
I
k
k
−
−
. Figure 1 shows the DB (2, 3) binary De Bruijn graph. 
 
Fig. 1. DB(2, 3) Binary de Bruijn graph 
In this paper we consider WSNs as a multilayer De Bruijn structures where each 
link represents a bidirectional communication link of the network. The network to-
pology should allow each node to send packets to every other node. The routing algo-
rithm determines the path through which a packet is delivered to the destination node. 
The distance between two nodes is the shortest length between nodes. The diameter of 
the network is the largest distance between any two nodes in the network. The degree 
of a node is the number of communication links (edges) associated with that node. 
The degree of the network is the largest degree of any node in the given network.  
2.1   Fault Model  
The following model is used in the diagnosis. We consider computation and commu-
nication faults at the sensor nodes level. This means, the sensor node can be dead 
because of energy depletion or it can have an obstacle near it affecting its ability to 

126 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
communicate. For the computational fault, the sensor node might be suffering form 
problems affecting its ability to manipulate the sensed data correctly.  
Only one fault can occur at every cluster. In other words, we can have multiple 
faults in different clusters and levels, but only one fault will be injected in the cluster. 
Finally, the MTBF(mean-time-between-failures) is expected to be much longer that 
the time required to diagnose a faulty node. 
3   Multi-layered Architecture 
In this section, we present two multi-layered architectures and a new routing scheme. 
Then we propose a new distributed fault diagnosis algorithm that can be used in each 
layer.  We assume that the sensor nodes are static. For indoor applications such as 
smart homes and smart buildings, this assumption is reasonable. The first architecture 
is proposed as a modification for the one proposed in [10] that will be briefly de-
scribed in this section. 
In [10] a concurrent diagnosis algorithm for cluster based sensor network is pro-
posed, where redundant nodes at the clusterhead level are used as checkers and par-
ticipate in the testing process. The sensor nodes in every cluster are required to send 
multiple copies of the same packet to their associated clusterhead and the checker 
nodes. Hence, the clusterhead and the checker nodes aggregate the data received and 
exchange it among themselves before the diagnosis algorithm is triggered. 
To be able to execute the diagnosis algorithm each clusterhead and checker node 
must maintain a neighbor table that will be used in the diagnosis process and will be 
searched according to the depth first algorithm. The diagnosis algorithm is divided 
into three phases. The first phase is used to diagnose the sensor nodes. Therefore, the 
status of every sensor node, belonging to the clusterhead and the checker nodes is 
determined by searching the neighbors table to find that node and its neighbor. After 
that, the reading of that node will be checked according to the readings provided by 
its neighbor. In this way the status of each sensor node is decided. Then, the algorithm 
proceeds to the second phase in which the clusterheads are diagnosed. Here, the clus-
terhead and the checker nodes exchange their aggregated data and the information 
acquired about the sensor nodes’ status in the neighbor table. Further, each node com-
pares its data with that received from the other nodes to determine the status of the 
clusterhead. If the checker nodes discovered that the clusterhead is faulty they will 
send their diagnosis information and the aggregated data to the base station. In addi-
tion, the faulty clusterhead can send its data to the base station. On the other hand, if 
one of the checker nodes is faulty, the fault free checker nodes refrain from sending 
data to the base station, while the clusterheads and the faulty checker nodes send data 
to the base station. In the third phase, the base station takes the final decision to de-
termine the status of the sensor nodes. 
From the above description and according to Fig. 3 in [10], it can be observed  
that the above mentioned architecture and algorithm suffers from the following  
drawbacks: 
• 
Processing the neighbors table requires long time especially with large 
numbers of nodes in the cluster.  

 
Clustered De Bruijn Based Multi Layered Architectures for Sensor Networks 
127 
 
• 
Each sensor node must send three copies of the same packet to the associ-
ated clusterhead and checker nodes, which consumes the sensor node’s en-
ergy and makes the network congested. 
• 
Having w checker nodes results in w exchange messages and w comparisons. 
• 
If we have m clusterheads, 2m checker nodes are needed. Thus, the network 
should be highly dense. 
 
Our work was motivated by the above discussion. As a result, we propose architecture 
A in which we reduce the number of copies the sensor nodes are required to send. 
Also we make use of De Bruijn graph to provide a certain level of fault tolerance 
without the need for spare checker nodes. Furthermore, we propose a fault tolerant 
routing algorithm and a distributed fault diagnosis algorithm that can be applied at 
different levels of the hierarchy. Finally we propose architecture B to enhance the 
performance of the network under architecture A. 
3.1   Architecture A 
The basic structure of architecture A is shown in Fig 2. The nodes in layer 1 are clus-
terheads. They are connected in a De Bruijn graph and are connected to the base  
station (BS). In addition, all the nodes in layer 2 are connected directly to their corre-
sponding clusterheads as shown in Fig 2. Starting from layer 3 until layer N, the adja-
cent layers are connected by two links between two different nodes. In other words, in 
each layer there are some nodes that act as access points between the current layer and 
the layer in the upper and the lower levels. These access point nodes are responsible 
 
 
Fig. 2. Architecture A: Clustered De Bruijn Based multi layered Network 

128 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
for passing messages between clusters in different levels. Also, having two access 
points in each cluster provides a certain level of fault tolerance as it keeps the network 
connected when one of the access points in the cluster fails. Note that, the number of 
nodes in a cluster increases as we go down in the hierarchy. 
For example, if node 14 in layer 3 needs to send data to its clusterheads it follows 
the following procedure. Node 14 has to send two copies of the same packet; one 
copy to node 1 and the other to node 7. The packets are routed from node 14 to nodes 
1 and 7 according to the routing algorithm described in Fig 4. After that, nodes 1 and 
7 in layer 3 will pass the packet to the nodes they are connected to in layer 2, i.e. 
nodes 4 and 6 respectively. Nodes 4 and 6 will check the source address in the packet 
header. As a result they know the packets are sent from a lower layer. Thus they will 
forward the packets to nodes 1 and 3. This process continues until the packets arrive 
to corresponding clusterheads, i.e. nodes 2 and 3 in layer 1. 
The clusterheads, nodes 2 and 3, aggregates the values received from the sensor 
nodes. Then, both clusterheads exchange the aggregated value and compare both 
values. If a mismatch occurs the distributed self diagnosis algorithm, described in 
section D, is triggered to detect faulty nodes. 
3.2   Architecture B 
Figure 3 illustrates the architecture B. Here, each cluster has its own clusterhead and 
all the nodes in a cluster are organized in a De Bruijn graph and are directly con-
nected to their associated clusterhead. In addition, all clusterheads are directly  
connected to the base station and are connected in a De Bruijn graph too. Also, each 
cluster contains access point nodes that will keep the network connected if the clus-
terhead fails.  
 
Fig. 3. Architecture B: Clustered De Bruijn Based multi layered Network  

 
Clustered De Bruijn Based Multi Layered Architectures for Sensor Networks 
129 
 
The sensor nodes in each cluster report data to their associated clusterhead. Then, 
the clusterhead is responsible for forwarding the sensed data to the base station. Note 
that, it takes two hops for a packet to arrive to the base station as the sensor nodes are 
connected directly to the clusterhead. 
When suffering from a failure at the clusterhead, the packet will be routed to one of 
the access points in the same cluster according to the routing algorithm described in 
Fig 4. Furthermore, the access point forwards it to the higher level cluster. The access 
point in each cluster keep forwarding the packet upwards until it reaches layer 1, 
where the packet will be forwarded to the clusterhead then to the base station. In the 
presence of failure, either at the clusterhead or at the sensor nodes levels the distrib-
uted self diagnosis algorithm, described in section 3.4, is triggered to detect faulty 
nodes. 
3.3   Routing Algorithm 
Routing in both architectures can be divided into inter layer routing and intra layer 
routing. Inter layer routing is illustrated in Fig. 4 and is derived from the properties of 
De Bruijn graph. For example let the source address be 
0
1
2
1
...,
,
x
x
x
x
x
k
k
−
−
=
 and 
the destination address be 
0
1
2
1
...,
,
d
d
d
d
d
k
k
−
−
=
. There are two paths between the 
source and the destination. The first path, path 1, is obtained be shifting the source 
address to the left, then append the destination address bits to it. The second path, 
path 2, is generated by shifting the source address to the right and appending the des-
tination address bits to it. 
Path 1 is used as the default for each node. When receiving a packet, each node 
calculates the next hop according to path 1, then it checks whether the next node is 
still a neighbor for it or not, as the neighbor node could have been disconnected from 
the network because it was diagnosed as faulty or has died. If the node is still a 
neighbor the packet is forwarded to it. If not, the current node switches to use path 2. 
Thus, all nodes receiving the packet after this node will use path 2. 
)
(
...,
,
0
1
2
1
source
x
x
x
x
x
k
k
−
−
=
1
0
1
3
2
...,
,
−
−
−
k
k
k
d
x
x
x
x
2
1
0
1
4
3
...,
,
−
−
−
−
k
k
k
k
d
d
x
x
x
x
)
(
...,
,
0
1
2
1
source
x
x
x
x
x
k
k
−
−
=
1
2
1
0
...,
,
x
x
x
d
k
k
−
−
2
2
1
0
1
...,
,
x
x
x
d
d
k
k
−
−
 
Intra layer routing is used to pass packets between clusters at different levels. In archi-
tecture A, these paths are used all the time to route packets from sensor nodes to the 
base station. On the other hand, in architecture B, these paths are used when the clus-
terhead for one of the clusters fail. As a result, intra layer routing will be used to pass 
the packet up until it reaches layer 1 where it will be forwarded to the base station. In 
addition, in case of failure occurring at one of the access points in one of the paths; 
because the two access points in each cluster are connected, the packet is routed to the 
other access point which forwards it up in the hierarchy. In other words, in the pres-
ence of failure in one of the vertical paths the packet can be routed to the base station 
using the second path. 

130 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
 
addr(curr) 
== 
addr(dest)
Receive packet
Yes
accept packets
End
No
 
addr(dest) == 
addr(neig)?
Yes
next = addr(neigh)
No
 
flag == 1
temp = path1
 
isNeighbor
(curr, 
temp)
Yes
next = temp
No
flag == 2
next = path2
Yes
No
next = path2
hop = hop + 1
Route packet to the 
next node
1
0
1
3
2
...
−
−
−
k
k
k
d
x
x
x
x
1
2
1
0
...x
x
x
d
k
k
−
−
1
2
1
0
...x
x
x
d
k
k
−
−
 
Fig. 4. Inter layer routing algorithm 
3.4   The Distributed Fault Diagnosis Algorithm 
Here, the distributed fault diagnosis algorithm is described. The important property of 
our algorithm is that there is no central node that will carry out the diagnosis process. 
The number of nodes in each cluster will be assumed to be equal to rm, where r is a 
parameter that bounds the number of faults that can be diagnosed in each cluster and 
will be referred to as base parameter in this paper. The variable m is the radix-r repre-
sentation of the node address e.g. 
0
2
1
...,
,
y
y
y
m
m
−
−
is the radix-r representation of 
node y. Also, the number of faults that can be diagnosed is equal to r -1 [2]. In addi-
tion, we assume that nodes can test their neighbors only. 
The algorithm is based on building directed tree structure for each De Bruijn based 
cluster. According to our previous work in [2], r different tree structures can be built 
where each one of them has a different root. In this paper, the base variable r is equal 
to 2 which mean we can diagnose only one fault in every cluster. Furthermore, we can 
build two tree structures for every cluster. Figures 5 and 6 illustrate two trees that can 
be built for a 16-node De Bruijn based cluster. 
Consider Fig. 5, the following conditions are satisfied: 
• 
The test tree must contain all the nodes in the cluster. 
• 
The number of non leaf nodes is equal to rm-1. 
• 
The number of leaf nodes is (r – 1)rm-1. 
• 
Any combination of r – 1 nodes must appear in at least one tree 
 

 
Clustered De Bruijn Based Multi Layered Architectures for Sensor Networks 
131 
 
The algorithm is triggered at the clusterheads level. As a result the first tree is built 
to test the clusterheads. The test tree is traversed in an inorder fashion. According to 
Fig. 5 the root node, 0, initiates the process by sending a test packet to node 8. Then, 
node 8 checks if it is a leaf node. In this case, node 8 in a non leaf node, thus a test 
packet will be sent to its left child, node 4. This process continues until we reach a 
leaf node. When a leaf node, for example node 1, receives a test packet, it will exe-
cute the required computation for the test and send the result back to its parent, node 
2. Node 2 compares the result received from node 1 with the expected or the prede-
fined on. If a miss match occurs node 1 will be considered faulty and its status will be 
reported back to the root node that is responsible for sending it to the base station. 
Note that, the algorithm will stop after finding the faulty node. Also, the faulty 
node can be detected only if it is a leaf node in the test tree shown in Fig. 5. However, 
if the faulty node is a non leaf node in the first tree, the algorithm cannot diagnose 
whether the non leaf node is faulty or there is a communication problem between that 
node and one of its children. As a result, when a non leaf node is suspected to be 
faulty, the algorithm will stop searching the tree shown in Fig. 5 and will construct the 
second test tree shown in Fig. 6. After constructing the second tree, the test packets 
will be passed in the same manner as mentioned before. The faulty node can be de-
tected because; it is a leaf node in the second tree. After diagnosing the nodes at one 
level, the algorithm proceeds to test the nodes in the subsequent level. 
The test packet sent to diagnose the nodes triggers the tested node to perform a 
specific computation whose result is known in advance. Therefore, if the tested node 
provides a value that deviates from the expected one it will be diagnosed as faulty. 
 
Fig. 5. Diagnosis algorithm Tree A 
 
Fig. 6. Diagnosis algorithm tree B 

132 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
4   Simulation Results 
The techniques presented in this paper have been implemented in C++ based on 
SENSE simulation tool [14] to evaluate the performance. The simulation use MAC 
IEEE 802.11 DCF that SENSE implements. In the simulation, we randomly choose 
different numbers of sources to send packets to two fixed destinations. Also, the num-
ber of nodes was increased gradually. The performance of mesh networks was com-
pared to that of De Bruijn under the same number of nodes and under faulty and fault 
free conditions. Note that he results obtained are the average of 10 runs for each case. 
Architecture A, presented in this paper, is compared to the one presented in [10] in 
terms of the number of required messages to accomplish the diagnosis process. To 
start with, sensor nodes in [10] are required to send three copies of the same packet, 
whereas our first architecture requires two copies only. i.e. if there are N nodes in the 
cluster their technique requires 3N messages while ours requires 2N messages. After 
that, the diagnosis algorithm starts, table 1 illustrates a comparison between the two 
techniques.  
Cases 1 and 2 in table 1 represent the cases where the faulty nodes were the leaf 
nodes in the first test tree, while the remaining two cases are gained when we have to 
build the second test tree. For the algorithm in [10], cases 1 and 2 stands for the case 
when the clusterhead is faulty and the other two cases are for the case when the 
checker node is faulty. 
Table 1. Number of Packets Requires 
Case 
Number of messages for  
architecture A 
Number of messages 
for ref [10]  
architecture 
1 
7 
8 
2 
9 
9 
3 
13 
7 
4 
15 
8 
 
Now we proceed to compare the performance of De Bruijn based network using 
our fault tolerant routing algorithm with the performance of mesh networks using XY 
routing. To compare the performance we choose 10 random sources sending two 
copies of the same packet to two fixed nodes (access point nodes). Also, the perform-
ance of architectures A and B is compared. According to [15], XY routing works by 
comparing the x and y coordinates of the current node to those of the destination. A 
packet will be routed horizontally until the x coordinate of the current node is equal to 
that of the destination. Then, the packet will be routed vertically until it reaches its 
destination. 
Figures 7 shows the performance, in terms of end-to-end delay and success rate, of 
the two paths used in a De Bruijn graph. It can be observed that, the delay increases 
when the cluster size increases. However, the success rate decreases when increasing 
the number of nodes in the cluster. 
 

 
Clustered De Bruijn Based Multi Layered Architectures for Sensor Networks 
133 
 
 
Fig. 7. Path 1 and Path 2 End-To-End delay and Success Rate 
 
Fig. 8. Mesh End-to-End delay 
The performance of mesh networks was studied according the same metrics.  
Figures 8 and 9 show that the performance of mesh is better when the network size is 
small. While the end-to-end delay gets higher and the success rate decreases when the 
number of nodes in the network is large.In Fig. 10 and 11, the performance of De 
Bruijn based networks is compared to that of mesh. From the figures, it can be ob-
served that under large network sizes De Bruijn based network performed better than 
mesh. Both types of networks had similar end-to-end delay values when the network 
size was small, until 32 nodes. For network sizes greater than 32 nodes, the mesh 
performance highly increased while the De Bruijn based networks were able to main-
tain a reasonable increase in the end-to-end-delay values. For the success rate, it can 
be concluded that De Bruijn based networks maintained almost a stable performance 
for different network sizes. On the other hand, the mesh based network could not keep 
its performance at the same level as the network size was increased. 

134 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
 
Fig. 9. Mesh success rate 
 
Fig. 10. DB and mesh delay comparison without faults 
 
Fig. 11. DB and mesh success rate comparison without faults 

 
Clustered De Bruijn Based Multi Layered Architectures for Sensor Networks 
135 
 
 
Fig. 12. DB and mesh comparison with faults 
 
Fig. 13. Comparison of the performance of the two hierarchies 
In Fig 12, a single fault was injected at each network and the simulation was run 10 
times to get the average values. Thus, it can be concluded that, De Bruijn based net-
works has better end-to-end delay performance than mesh networks in the presence of 
faults. Figure 13 illustrates the performance of the two hierarchies. It can be observed 
that the second hierarchy obtained much better performance, because in the fault free 
case sensor nodes send their data directly to their associated clusterhead and then to 
the base station. However, in the first hierarchy the nodes have to use the vertical 
paths to send the packets to the clusterheads and the base station. 
5   Conclusion 
In this paper, we presented two fault tolerant multi layered hierarchies based on De 
Bruijn graph. The performance of De Bruin graph and the presented hierarchies was 
simulated and studied. When the performance of De Bruijn graph was compared to 
that of mesh networks, a De Bruijn based network has shown its ability to maintain a 
high performance level with and without having faults injected in the network. 

136 
A.A. Taleb, J. Mathew, and D.K. Pradhan 
 
References 
1. Siva Ram Murthy, C., Manoj, B.S.: Ad Hoc Wireless Networks Architectures and Proto-
cols. Prentice Hall, New Jersey (2004) 
2. Pradhan, D.K., Reddy, S.M.: A fault-tolerant communication architecture for distributed 
systems. IEEE Trans. on comput., 863–870 (1982) 
3. Bandyopadhyay, S., Coyle, E.J.: An energy efficient hierarchical clustering algorithm for 
wireless sensor networks. In: INFOCOM 2003. Twenty-Second Annual Joint Conference 
of the IEEE Computer and Communications. IEEE Societies, March 30 - April 3, vol. 3, 
pp. 1713–1723 (2003) 
4. Manjeshwar, A., Agrawal, D.P.: TEEN: a routing protocol for enhanced efficiency in wire-
less sensor networks. In: Proceedings of 15th International Parallel and Distributed Proc-
essing Symposium, pp. 2009–2015 (April 2001) 
5. Manjeshwar, A., Agrawal, D.P.: APTEEN: a hybrid protocol for efficient routing and 
comprehensive information retrieval in wireless sensor networks. In: Proceedings of Inter-
national Parallel and Distributed Processing Symposium, IPDPS 2002, Abstracts and  
CD-ROM, p. 195 (2002) 
6. Heinzelman, W.R., Chandrakasan, A., Balakrishnan, H.: Energy-efficient communication 
protocol for wireless microsensor networks. In: Proceedings of the 33rd Annual Hawaii In-
ternational Conference on System Sciences, January 4-7, vol. 2, p. 10 (2000) 
7. Lindsey, S., Raghavendra, C.S.: Pegasis: Power-efficient gathering in sensor information 
systems. In: Proc. of the IEEE, pp. 924–935 (2002) 
8. Lin, C., Chou, P., Chou, C.: HCCD: Hierarchical Cluster-based Data Dissemination in 
Wireless Sensor Networks with Mobile Sink. In: Proc. of the 2006 Int. Conf. on Wireless 
communications and mobile computing, Vancouver, British Columbia, Canada (July 2006) 
9. Gupta, G., Younis, M.: Fault-tolerant Clustering of Wireless Sensor Networks. In: Proc. of 
IEEE WCNC, pp. 1579–1584 (2003) 
10. Cho, C., Choi, Y.: Concurrent Diagnosis of Clustered Sensor Networks. In: Boavida, F., 
Plagemann, T., Stiller, B., Westphal, C., Monteiro, E. (eds.) NETWORKING 2006. LNCS, 
vol. 3976, pp. 1267–1272. Springer, Heidelberg (2006) 
11. Ssu, K., Chou, C., Jiau, H.C., Hu, W.: Detection and diagnosis of data inconsistency fail-
ures in wireless sensor networks. The Int. Journal of Computer and Telecom. Network-
ing 50(9), 1247–1260 (2006) 
12. Huynh, T.T., Hong, C.S.: A Novel Heirarchical Routing Protocol for Wireless Sensor 
Networks. LNCS, pp. 339–347. Springer, Heidelberg (2005) 
13. Samatham, M.R., Pradhan, D.k.: THE De Bruijn Multiprocessor Networ: A versatile Paral-
lel Processing and Sorting Network for VLSI. IEEE Trans. on Computers 38(4) (April 
1989) 
14. Chen, G., et al.: SENSE – Sensor Network Simulator and Emulator,  
 http://www.cs.rpi.edu/cheng3/sense/ 
15. Zhang, W., Hou, L., Wang, J., Geng, S., Wu, W.: Comparison Research between XY and 
Odd-Even Routing Algorithm of a 2-Dimension 3X3 Mesh Topology Network-on-Chip. In: 
WRI Global Congress on Intelligent Systems, GCIS 2009, May 19-21, vol. 3, pp. 329–333 
(2009) 

A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 137–147, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
An Adaptive Codec and Frame Size Modification Based 
QoS Algorithm over Multi-rate WLANs 
M. Fatih Tüysüz and Hacı A. Mantar 
Gebze Institute of Technology, Computer Science Department, 
Istanbul Street No: 101 41400, Gebze/Kocaeli Turkey 
{Ftuysuz,hamantar}@bilmuh.gyte.edu.tr  
Abstract. The demand for Voice over IP (VoIP) services over wireless local 
area networks (WLANs) and Quality of Service (QoS) support in WLANs have 
grown substantially in recent years. IEEE 802.11e QoS enhancement standard 
has an important role on this expansion as well. It provides a suitable solution 
for delay sensitive real-time multimedia applications. Under the circumstance 
of any deterioration of wireless link conditions, IEEE 802.11 PHY/MAC speci-
fications allow mobile nodes to select a proper transmission rate to optimize bit 
transmission. However, if a node selects lower transmission rate, packet  
losses and delays increase due to multi-rate effect. Therefore, all active calls 
have a general degradation. In this paper, we propose a fast, efficient, adaptive, 
proactive codec and frame size modification algorithm for the optimization of 
maximum throughput and voice quality on multi-rate WLANs.  
Keywords: Voice over IP, multi-rate WLANs, QoS, IEEE 802.11e. 
1   Introduction 
IEEE 802.11 WLANs has reached an important stage and has become a common tech-
nology for wireless access with the deployment of IEEE 802.11e QoS solutions. How-
ever, there are still unsolved performance issues on the optimization of throughput, 
limiting the delay times and packet loss ratio. Besides, in maximum network utiliza-
tion, changing the transmission rate of a mobile node due to SNR degradation 
(presence of walls, interference, distance changes, humudity, rain etc) affects the QoS 
parameters of all active VoIP sessions. Since it is not possible to handle these sudden 
rate change with standart implementations efficiently, a new algorithm is needed for a 
fast and efficient solution. First, wireless channel occupancy time which is occupied by 
an Access point (AP) and wireless stations can be reduced by changing the codecs or 
frame sizes of mobile nodes. Thus, the negative effect of the transmission rate falls can 
be minimized with a little degradation of voice quality. 
Using small frame sizes in VoIP, limits the number of simultaneous VoIP 
sessions due to large MAC, IP, RTP overheads of IEEE 802.11. On the other hand, 
using bigger frame sizes allows more VoIP sessions. There is a trade-off between 
using bigger frame size and small frame size. Overhead size ratio can be reduced by 
using bigger frame size but packet error rate may increase as well. Therefore, if a 
mobile node has a decrease in transmission rate, it is best to lower the codec of the 
node instead of expanding the frame size and increase the frame sizes of other 
nodes. 

138 
M.F. Tüysüz and H.A. Mantar 
 
Fig. 1. IEEE 802.11 based network communication scenario 
In this paper, we focus on the effect of rate changes on multi-rate networks using 
RTCP feedbacks and MAC layer information. If any rate change occurs, our 
algorithm analytically calculates the possible degradation value and fix it by changing 
related codecs or frame sizes.  
The rest of the paper is organized as follow: In Section II, we briefly describe the 
related works to our proposed algorithm. In Section III, the proposed algorithm and 
analytic calculations are explained in more detail. In Section IV, we present the 
simulation process and an analysis of the results. Finally Section V reports the final 
evaluation and concludes the paper with a brief explanation about future works. 
2   Related Works 
There have been a few previous works focusing on RTCP feedback to set the quality 
of the transmitted media. In [7], authors propose a codec-based cross layer algorithm 
that has three phases to set the QoS. However, it does not check the media if it is error 
prone or congestion prone and there is no intention to change the frame size of nodes 
for a fast reaction. Since it has three phases to set the codec changes, it results in a 
slow algorithm. This work is extended by adding a connection admission control 
mechanism in [8]. It defines a new grade of service-related parameter Q which cap-
tures the trade-off between dropping and blocking probabilities and perceived speech 
quality.  
A channel estimation algorithm for selecting the optimal output rate of the speech 
coder is proposed in [9]. It uses AMR codec. If the AMR codec is used under a multi-
rate scenario, all mobile nodes flow through the erroneous channel change to a lower 
codec at the same time. Thus, it reduces the overall MOS more than necessary. 
Another work about changing codecs is in [10]. It is based on jitter buffer value. 
When buffer is out of threshold, it changes the codec rate of a node to prevent possi-
ble losses. 
In [11] and [12], authors implement the Media Gateway (MGW) algorithm on the 
Access point. A decision is given to change codecs from wired to wireless networks 

 
An Adaptive Codec and Frame Size Modification Based QoS Algorithm 
139 
or vice versa. Codec change process is made in AP. This process adds additional 
delay to system.  
Effect of packet size on loss rate and delay is also an important study [14]. The op-
timal frame size, minimum delay and the information of packet loss ratio improve the 
performance of algorithms. Authors classify losses as congestion and wireless losses 
in [18]. Since they are totally different types of losses, we cannot apply the same 
treatment to them. For example if the medium is error prone (wireless losses prone), 
an attempt to use a bigger frame size causes more packet losses because of the in-
crease of unsuccessful transmission probability. As a result, as already been men-
tioned earlier, there is a trade-off between reducing the header overhead by adopting a 
larger frame size and the need to reduce packet error rate in the error prone 
environment by using small frame size. 
3   Proposed Algorithm 
In this paper, we present a fast and efficient algorithm that uses RTCP feedback and 
MAC information for the transmission rate adaption in mobile nodes. Whenever a 
mobile node changes its transmission rate, our algorithm evaluates the active voice 
sessions and prevents the possible QoS degradation by changing some codecs and/or 
frame sizes based on analytical calculations. The decision mechanism has only one 
phase to provide faster reaction. When a rate change occurs, first, the algorithm calcu-
lates the number of active voice sessions and compares the result with the maximum 
number of users that the network can handle with certain QoS constraints. If the ca-
pacity is not full, only one codec change will be enough to keep system performance 
as before. However, if the capacity is full, the algorithm determines the new threshold 
value and changes one or more codecs or frame sizes of mobile users according to this 
threshold value to protect network from congestion in milliseconds without waiting 
the next RTCP feedback value.  
The algorithm works in centralized mode and installed in the Access Point. It uses 
the Session Initiation Protocol (SIP) to decide codec or frame size changes and re-
negotiate the new codec without interrupting the call. Before we explain the algorithm 
in more detail, it would be proper to mention about some calculations. As we already 
know, we have two trumps (codec and frame size) to keep the network away from 
congestions after a transmission rate decreases on a heavily loaded network. Reducing 
the codec rate degrades our mean opinion score. On the other hand, using bigger 
frame size affects our network badly if the channel is an error prone channel. Using 
these trumps efficiently is our first priority. If the transmission rate of a mobile node 
decreases, changing its codec to a lower one (but not the frame size) and changing 
one or more frame sizes of other nodes would be the best solution to recover system. 
Because the node that decreases its transmission rate most likely has the biggest error 
prone probability and using bigger frame sizes for that node does not help recovering 
the network.  
The maximum number of simultaneous VoIP calls Smax can be calculated with the 
following formula [6],  

140 
M.F. Tüysüz and H.A. Mantar 
                           S୫ୟ୶ൌ
୘୮
ଶሺ୘ୢ୧୤ୱ  ା ୘ୱ୧୤ୱ ା ୘୴୭୧ୡୣ ା ୘ୟୡ୩ሻା ሺ୘ୱ୪୭୲  ሺి౓ౣ౟౤
మ
ሻሻ
                      (1) 
Tp is the packetization interval, Tdifs and Tsifs are the lengths of distributed inter frame 
space (DIFS) and short inter frame space (SIFS), Tvoice and Tack are the times for send-
ing a voice packet and ACK, Tslot is the slot time, and CWmin is the minimum conten-
tion window size of binary exponential backoff. Tvoice and Tack are defined as,  
T୴୭୧ୡୣൌL ൅
୐୫ୟୡା୐ୢୟ୲ୟ
ୖౚ
                   Tୟୡ୩ൌL ൅
୐ୟୡ୩
ୖౘ                          (2) 
where L is the transmission time of all PHY headers, Lmac is the headers of MAC 
layer, Ldata is data length, Lack is acknowledgement length, Rd (11Mbps for 802.11b) 
and Rb (2Mbps) are data rate and basic rate,  respectively. 
In above, we assume that all active calls use the same data rate.  Under the multi-
rate network conditions, the data rate of all the calls are not the same.  Thus, we need 
to make a new definition to calculate the capacity of a multi-rate network. The pre-
vious equation can be derived to a new formula [6], 
Sthreshold = ∑∑sሺx, rሻ
ଶሺ୘ୢ୧୤ୱା୘ୱ୧୤ୱା୘୴୭୧ୡୣା୘ୟୡ୩ሻାሺ୘ୱ୪୭୲ሺి౓ౣ౟౤
మ
ሻሻ
୘୮ 
൏1               (3) 
where s(x, r) is the number of calls using codec c and rate r. Algorithm calculates the 
new threshold value according to this equation and then decide about how many co-
decs or frame sizes need to change to obtain the maximum throughput. Transmission 
time parameters can be seen in Figure 2.  
 
Fig. 2. Voice packet transmission time 
First, algorithm collects MAC layer information and RTCP periodic packets to 
check if there is a rate change. RTCP feedback data can be easily obtained from 
RTCP sender and receiver reports (our algorithm uses two “one second fast feed-
backs”). The algorithm keeps a multidimensional control array containing caller_ID, 
Tdelay, Loss, R1, R2, Rmean values to enable sudden calculations and reactions. It does not 
wait for the next RTCP feedback.  
     A[ 6 ][ N ]  =  { { Caller_1, Tdelay, Loss, R1, R2, Rmean  },  
                               { Caller_2, Tdelay, Loss, R1, R2, Rmean  }, 
                               { Caller_3, Tdelay, Loss, R1, R2, Rmean  }, 
                                       .            .          .      .     .       .  
                              { Caller_N, Tdelay, Loss, R1, R2, Rmean  }} 
 
 

 
An Adaptive Codec and Frame Size Modification Based QoS Algorithm 
141 
After a rate change, it makes system active not to process new RTCP feedback value 
until codec or frame size changes are adapted. Algorithm gives a decision about how 
many codecs and/or frame size changes are enough to recover network after it calcu-
lates the new threshold value. It applies the changes and makes the algorithm passive. 
As long as the next RTCP feedback is not above threshold value, we can say that 
codec or frame size changes recovered system well. Otherwise, algorithm calculates a 
new codec rate or frame size for the mobile node and applies new changes. Besides, 
our algorithm keeps previous RTCP feedback information dynamically in a multidi-
mensional array to calculate threshold value without waiting for the next RTCP as we 
mentioned earlier. Therefore, algorithm processing time will be milliseconds because 
it does not wait for the next RTCP.  The flowchart of the algorithm can be seen in 
Figure 3. 
For further clarification, we can briefly write the pseudo code of codec and frame 
size modification part as follow,  
calculate new threshold from new MAC information 
while threshold > 1 
     codec down 
     calculate new threshold 
     if  threshold > 1 
         choose best R from table 
         if  Rbest  > Rthreshold 
         frame size up 
     else 
         codec down  
     if Lvalue  is on limit 
         leave the loop 
 
As seen, if the threshold value is bigger than 1, the algorithm first reduces the co-
dec rate of the node that had a transmission rate decrease. Then, it calculates the new 
threshold value analytically by using the new values of the node whose codec rate is 
reduced. If the threshold is still bigger than 1, algorithm picks another mobile node 
that has the best Rmean value by looking at the multidimensional array. If this Rbest 
value is bigger than Rthreshold which is equal to R factor value 70. Then algorithm ex-
pands the frame size of the node. Otherwise, it reduces the codec rate. Lastly, it 
checks on the new acceptable number of codec or frame size modification. If it is 
above the limit value Lvalue which means a new attempt to change a codec or a frame 
size will not be a QoS solution anymore, the algorithm exit the loop and become  
passive.  
Lvalue depends on the worst codec rate and maximum frame size we want to use as a 
last solution for our VoIP session. In our simulations, we used G711 codec with 20 
milliseconds frame size for the start and G729 codec with 40 milliseconds frame size 
for the worst to keep voice quality as high as we can. Codec parameters and MOS 
values can be seen in Table 1.  

142 
M.F. Tüysüz and H.A. Mantar 
 
Fig. 3. Algorithm Flowchart 
Table 1. Codec parameters and MOS values 
 
Codec 
 
Bit Rate (Kbps) 
 
MOS Score 
G711 
64 
4.1 
G726 
32 
3.85 
G729 
8 
3.7 
G723.1 
5.3 
3.6 
4   Simulations 
The performance of the proposed algorithm was analysed by simulations using the 
OMNET simulator and C++ implementations. First of all, there are many parameters 
used to calculate formulas, implement MAC layer and IEEE 802.11e standards at the 
simulation. Some of these important parameters are shown in Table 2 and Table 3.   
Extensive simulations have been tested to see the results of our algorithm. As it 
shall be seen, our analytic calculations and simulation results have small differences 
because of the random contention window parameters and different probability of 
error ratio for all VoIP sessions defined in the simulation.  
We mainly focused on two separate VoIP sessions. One consists of 12 VoIP calls 
(maximum number of users for G711 codec, 20 milliseconds frame size for 802.11b) 
and the other one is the combination of 5 VoIP sessions (G711 codec, 20 milliseconds 
frame size) and 5 data sessions contains 1000 byte messages for each session. 
Throughputs and mean opinion scores (MOS) can be seen in Figure 4, 5 and 6.   

 
An Adaptive Codec and Frame Size Modification Based QoS Algorithm 
143 
Table 2. Contention parameters for access categories 
 
Access Categories 
 
AC_VO 
AC_VI 
 
AC_BE 
 
AC_BK 
AIFS Number 
2 
2 
3 
7 
CWmin 
7 
15 
31 
31 
CWmax 
15 
31 
1023 
1023 
Table 3. Timing for 1000 byte data messages at 11 Mbps 
NAME 
VALUE 
UNIT 
SLOTTIME 
20 
us 
MSG_SIZE 
1000 
byte 
BITRATE 
11 
Mbps 
NAME 
LENGTH 
(bits) 
BITRATE 
(Mbps) 
  TIME (us)
DIFS 
50 
1 
50 
BACKOFF(avg) 
310 
1 
310 
PREAMBLE 
192 
1 
192 
HEADER 
240 
11 
21,81 
DATA 
8000 
11 
727,27 
CRC 
32 
11 
2,909 
SIFS 
10 
1 
10 
PREAMBLE 
192 
1 
192 
ACK 
112 
2 
56 
 
Fig. 4. Total throughput of VoIP flows 
 

144 
M.F. Tüysüz and H.A. Mantar 
We analyzed the performance issues according to throughput and MOS values by 
implementing MAC layer and IEEE 802.11e standards with parameters in Table 2 and 
Table 3. At the first case, we reduced the transmission rate of one of these 12 mobile 
nodes from 11 Mbps to 1 Mbps at 45th second and another node transmission rate is 
reduced from 11 Mbps to 1 Mbps again at 60th second. Analytic results are also add-
ed to figures to make it easy for comparison.  
As shown in Figure 4, after a rate decrease, the total throughput reduces corres-
pondingly. Besides, it affects not only its session but also all VoIP sessions and caus-
es a general degradation of all VoIP flows. Simulation results show that if the algo-
rithm does not run,  the total number of voice frames reaches to AP in a second is 
about 590 frames at normal conditions (before any rate decrease occurs). After a rate 
change at 45th second, the total number reduces to 505, and after last reduce at 60th 
second, the number of total voice frames in a second reduces to 435 frames. If our 
algorithm runs, it changes only one codec from G711 to G729 and expands one frame 
size from 20 milliseconds to 40 milliseconds. Thus, the total frame number reaches to 
the access point remains almost the same as the old value (590 frames).  
 
Fig. 5. Average mean opinion score (MOS)  
Results are similar for mean opinion scores (MOS) as shown in Figure 5. Since on-
ly one codec rate is being reduced, the negative effect of the reduction will be so 
slight (only for the node whose transmission rate reduced) compared to the effect of 
the run without any algorithm.  
At the second case, we analyzed a heterogeneous traffic (combination of 5 VoIP 
sessions and 5 data sessions) to examine the total throughput and MOS scores by 
implementing IEEE 802.11e EDCA standards, which has different access categories 
for VoIP and data flows. We used 8000 bits data messages for data frames as shown 
in Table 3. Since these data frames are much bigger than what we used for VoIP 
flows (i.e. 1280 bits for G711), the effect of our algorithm cannot be the same with  
 

 
An Adaptive Codec and Frame Size Modification Based QoS Algorithm 
145 
 
Fig. 6. Total throughput of voice and data sessions 
the first case. As depicted in Figure 6, we analyzed a transmission rate reduction ef-
fect at 50th second. There are two scenarios here. The first scenario has a reduction of 
a VoIP flow and the other is the reduction of the data flow. If a transmission rate of a 
VoIP flow reduces, it is easy to recover the negative effect of reduced rate. Because it 
has only 160 bytes (1280 bits) length alike the other VoIP flows. A codec change 
from G711 to G729 and two frame size expansions from 20 milliseconds to 40 milli-
seconds are sufficient to recover the network as we could calculate it analytically or 
with a simulation run of the algorithm. However, if a transmission rate of a data flow 
reduces, it would not be easy to recover the network since it has bigger data frame 
size (8000 bits).   
As a result, simulation and analytic calculations for heterogeneous traffic is shown 
in Figure 6. The result indicates that the algorithm recovers the negative effect of a 
reduction of a VoIP flow without encountering any problem. However, it only recov-
ers the network up to a point if data frame transmission rate declines. Therefore, all 
active calls have a general degradation on the perceived voice quality. But again, even 
if the algorithm cannot recover the network completely, it still improves the overall 
throughput and MOS.  
5   Conclusions 
We propose a codec and frame size based decision algorithm for a multi-rate IEEE 
802.11 wireless network. The proposed algorithm uses RTCP feedback and MAC layer 
rate change information to recover network from the quality degradation of active 
calls. The proposed algorithm is simple, scalable, fast and efficient. It uses the earlier 
RTCP feedbacks and put them in a multidimensional array for proper use. Under the 
circumstance of a rate change, the algorithm calculates codec and frame size modifica-
tions and prevents the possible channel degradation in milliseconds before it occurs. 

146 
M.F. Tüysüz and H.A. Mantar 
Besides, in the worst case scenario, in the event of any wrong analytic calculations, 
algorithm takes the new RTCP feedback and prevents the channel degradation again 
with codec and frame size modifications by looking at Tdelay, Loss and Rmean values at 
the new RTCP feedback. Therefore, even if the analytic calculations unexpectedly go 
wrong, processing time of our algorithm be the same with other algorithms which uses 
RTCP feedbacks without analytic calculations.   
There are several points left to be considered and to be evaluated in the future. First 
of all, a separation study of error prone and congestion prone network to find out an 
optimal throughput and frame size values can be combined with multi-rate network 
codec and frame size modification. A new algorithm can be proposed for extended 
service set (ESS). Also, since the algorithm can work perfectly with a connection 
admission control or a jitter buffer on receiver side, a new study can be adapted easily 
to integrate the algorithm with a connection admission control (CAC) or a jitter buffer 
as well.  
References 
1. Beuran, R.: VoIP over Wireless LAN Survey, Internet Research Center, Japan Advanced 
Institute of Science and Technology (JAIST) (April 2006) 
2. Akın, G., Özay, B., Ketenci, S.: Kablosuz Ağlarda Servis Kalitesi, Çanakkale Onsekiz 
Mart Üniversitesi, Akademik Bilişim (2008) 
3. Cai, L., Xiao, Y., Shen, X., Cai, L., Mark, J.W.: Voip over Wlan: Voice capacity, admis-
sion control, QoS, and MAC. International Journal of Communication Systems 19(4), 
491–508 (2006) 
4. Zhang, J., Yang, D., Quan, Z.: Voice Quality of VoIP in Mobile Communication Systems, 
Beijing University of Posts and Telecommunications (2006) 
5. Bohge, M., Renwanz, M.: A Realistic VoIP traffic generation and evaluation tool for om-
net++. Omnet++ 2008, Marseille, France (2008) 
6. Sfairopoulou, A.: A cross-layer mechanism for QoS improvements in VoIP over multi-rate 
WLAN networks, PhD thesis, Pompeu Fabra University (April 2008) 
7. Sfairopoulou, A., Macián, C., Bellalta, B.: QoS adaptation in SIP-based VoIP calls in mul-
ti-rate IEEE 802.11 environments. In: ISWCS 2006, Valencia (2006) 
8. Sfairopoulou, A., Bellalta, B., Macian, C.: How to Tune VoIP Codec Selection in 
WLANs? IEEE Communications Letters 12(8) (August 2008) 
9. Servetti, A., De Martin, J.C.: Interactive Speech Transmission Over 802.11 Wireless Lans, 
Cercom, Torino, Italy (2003) 
10. Hirannaiah, R.M., Jasti, A., Pendse, R.: Influence of Codecs on Adaptive Jitter Buffer Al-
gorithm. In: GRASP Symposium, Wichita State University (2007) 
11. Tebbani, B., Haddadou, K.: Codec-based Adaptive QoS Control for VoWLAN with Diffe-
rentiated Services. IEEE (2008), 978-1-4244-2829-8/08 
12. Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY) Specification, 
IEEE Std. 802.11 (1997)  
13. Koo, K.-J., Kim, D.-y.: A scalable speech/audio coder control algorithm to improve the 
QoS of VoIP calls over WLANs. In: Wireless Conference on Electronics and Telecommu-
nications Research Institute (ETRI), pp. 1–6 (2008) 
14. Korhonen, J., Wang, Y.: Effect of Packet Size on Loss Rate and Delay in Wireless Links. 
In: IEEE Communications Society / WCNC 2005 (2005) 

 
An Adaptive Codec and Frame Size Modification Based QoS Algorithm 
147 
15. Modiano, E.: An adaptive algorithm for optimizing the packet size used in wireless ARQ 
protocols. MIT Lincoln Laboratory, Lexington 5(4), 279–286 (1999) 
16. Kawata, T., Yamada, H.: Adaptive Multi-Rate VoIP for IEEE 802.11 Wireless Networks 
with Link Adaptation Function. In: Global Telecommunications Conference (2006) 
17. Yin, J., Wang, X., Agrawal, D.P.: Optimal Packet Size in Error-prone Channel for IEEE 
802.11 Distributed Coordination Function. In: WCNC 2004. IEEE Communications  
Society (2004) 
18. Huang, C.-W., Chindapol, A., Ritcey, J.A., Hwang, J.-N.: Link Layer Packet Loss Classi-
fication for Link Adaptation in WLAN. In: 4th Annual Conference on Information 
Sciences and Systems, Princeton (March 2006) 
19. Trad1, A., Ni, Q., Afifi, H.: Adaptive VoIP Transmission over Heterogeneous 
Wired/Wireless Networks, INRIA, Planete Project 2004, Route des Lucioles (2004) 
20. Lee, C.-W., Yang1, C.-S., Su, Y.-C.: Low-Complexity Adaptive Packet Size Assignment 
Schemes for Real-Time Scalable Video Transmission over WLANs. In: International Con-
ference on Communications and Mobile Computing, Leipzig, Germany (2009) 

Eﬃcient Scheduling of Low Cost Popular
Services over a DVB-SH/3G Network
Azza Jedidi and Fr´ed´eric Weis
INRIA,
IRISA, Campus de Beaulieu, France
{azza.jedidi,frederic.weis}@irisa.fr
Abstract. Fourth generation networks are the result of the convergence
of the diﬀerent existing technologies, taking beneﬁt from their comple-
mentary properties. In this context, recent research has addressed Digi-
tal Video Broadcast (DVB) networks, and has tried to couple them with
complementary networks. In our work, we couple DVB-SH with a 3G net-
work, thus providing a bidirectional interactivity path. DVB-SH beneﬁts
from a very high bandwidth capacity that allows unidirectional IP-TV
channels broadcast. A residual bandwidth in the DVB-SH path may still
be available because of the variable bit rates of served ﬂows. In our work,
we focus on this residual bandwidth. We realize an eﬃcient switching of
some 3G popular services, to the residual bandwidth of DVB Networks.
The goal is to provide interactive low cost services over DVB networks.
In this paper, we propose a new algorithm to eﬃciently schedule our
3G switched services over DVB residual bandwidth.1
1
Introduction
Recently, a growing interest has been shown in multimedia networking mainly
due to the emergence of eﬃcient audio/video encoding techniques and the pro-
liferation of enhanced audio-visual services. The demand for these kinds of ap-
plications has quickly increased. Major advances in communication and network
technologies have made multimedia services technically and economically feasible
in any type of environment. Digital TV and multicast IP represent the best pro-
cesses to deliver multimedia content respectively through broadcast and Internet-
based networks. Regarding broadcasting standards, Digital Video Broadcasting
(DVB) is expected to be the prominent European television broadcast standard
for the next decades, as well through a satellite-based technology (DVB-S), as in
terrestrial television (DVB-T), cable (DVB-C) or for hand-held devices (DVB-
H). The DVB technology provides relatively high bandwidth data channels but
based on uni-directionality, thus neglecting interactivity.
1 This work has been done within the TVMSL (T´el´eVision Mobile Sans Limite)
project led by Alcatel-Lucent. This project plans to develop a DVB-SH standard
suitable for hybrid satellite and terrestrial transmission. It is supported by the French
innovation Agency OSEO.
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 148–162, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Eﬃcient Scheduling of Low Cost Popular Services
149
DVB-SH, satellite services for hand-held devices, is a hybrid (satellite/
terrestrial) standard. It is deﬁned as a system for IP based media content and
data delivery for hand-held terminals, via satellite. Satellite transmission guar-
antees wide area coverage. Moreover, it is coupled with terrestrial gap ﬁllers
assuring service continuity in areas where the satellite signal cannot be received
(built-up areas for example). DVB-SH provides users with a variety of services,
which could be classiﬁed in several categories. It oﬀers real-time applications.
Examples are TV-like broadcasting, live broadcasting and notiﬁcation, which
consists in broadcast notiﬁcations sent according to the preferences of the user
(notifying a football fan of the retransmission of his preferred team matches for
instance) and games, like real-time quizzes or multiplayer online role-playing
games, etc. It also provides applications to download. For large general audi-
ences, data ﬁle purchase services are oﬀered, either on a subscription basis, such
as downloading every morning the electronic version of the user’s newspaper, or
on an impulsive purchase basis, like for ﬁlms, books and audio CD purchase.
DVB network 
3G  cellular  network 
Bimode mobile terminals
Unidirectional path 
Broadcast channel 
Bidirectional path 
Upload link 
Download link 
TV  
broadcaster 
Service 
provider 
Fig. 1. Coupling DVB with 3G network
Besides, one of the main characteristics of the Internet world is its bidirec-
tionality, permitting full interactivity to users. In the context of our study, a
DVB-SH broadcast network is combined with a third generation cellular network
(3G network) to ensure this bidirectionality, as shown in Figure 1. Actually, this
convergence takes beneﬁt from 3G and DVB networks. 3G network characteris-
tics, especially upload link, enable added-value services and applications that are
interactive and more personalized. DVB-SH beneﬁts from an expensive but very
high bandwidth capacity that allows unidirectional IP-TV channels broadcast.
A residual bandwidth in the DVB-SH path may still be available because of the
variable bit rates of served ﬂows as shown in Figure 2. DVB bandwidth being
very expansive, we tried to deﬁne scenarios of services that eﬃciently use DVB
residual bandwidth. We proved through simulations that thanks to this small
bandwidth several low cost services can be oﬀered to a large group of users,

150
A. Jedidi and F. Weis
Fig. 2. Residual bandwidth
which was not possible using only 3G networks. The idea was to realize an eﬃ-
cient switching of IP data, coming from 3G networks, to the residual bandwidth
of DVB networks. This scenario occurs especially when 3G contents or services
become very popular. Thus their transmission may take beneﬁt from the large
broadcasting capacities oﬀered by DVB-SH.
Switching our services over DVB residual bandwidth allows an optimization of
DVB brodcast capacities exploitation. However, DVB residual bandwidth being
very small (a few kilobits), using the residual bandwidth introduces a trans-
mission delay. As a compensation, those services are oﬀered at low costs, thus
making them attractive for users. In our previous studies [7], the 3G switched
ﬂows were inserted over DVB-SH residual bandwidth in a First In First Out
mode. In this paper, we reﬁne our ﬂow switching model through the deﬁnition
of a more adapted scheduling algorithm. A detailed description of the algorithm
is given, then its performances are evaluated and compared to some classical
scheduling algorithms.
The next section presents related work to DVB/3G network inter-working.
Section 3 describes the system architecture that will be used in this work. Sec-
tion 4 presents the service scenario. Section 5 presents our proposed scheduling
algorithm. The algorithm is validated and compared to other scheduling policies.
Simulation results are shown in section 6 and section 7 . Finally, section 8 draws
our conclusions.
2
Related Work
Several works have addressed the coupling of DVB with another network, in
order to achieve bidirectional channels. The main goal of those works was to en-
able users to interactively communicate with the network using the ﬁxed line and
mobile phone. For instance, the study [1] discussed the coupling of DVB-C/DVB-
T with a satellite return channel and terrestrial systems as ADSL, ATM. The
goal was to achieve interactive services with users participation. The ATHENA

Eﬃcient Scheduling of Low Cost Popular Services
151
European Research project [3] has proposed a speciﬁc structure of hybrid com-
munications access network, exploiting the particularities of the DVB-T system
in oﬀering broadband connectivity to INTERNET for a large category of users.
In the context of ATHENA project, the authors in [4] have proposed to manage
the bandwidth between IP data and DVB-T ﬂows. The solution is based on two
bandwidth management systems, respectively for DVB-T and IP, inter-working
together in order to perform speciﬁc improving tasks on the corresponding ﬂows.
In [2], the authors have studied the coupling of DVB-H/DVB-T with a cellular
network. The goal was to use bidirectional channels in order to improve the han-
dover between a DVB network and another network. Thus, the lost data during
the handover have been recovered through the up-link channels.
In our work, we are going further in inter-working DVB-SH and mobile IP
networks. We address several aspects of switching data to DVB networks, mainly:
services that may be proposed to utilize the DVB residual bandwidth, switching
decision to look if a network switch is useful or not, IP data insertion to insert
ﬂows coming from the 3G network in the DVB network and ﬂow scheduling to
optimize the use of the residual bandwidth.
3
Proposed Architecture
The architecture studied in the scope of this work is based on an unidirectional
DVB-SH broadcast network, coupled with a third generation cellular network.
We concentrate our eﬀorts on innovative low cost services that may be brought
by non real time ﬂows while eﬃciently using the DVB residual bandwidth. Fig-
ure 1 has shown a simple DVB-3G architecture in which 3G and DVB paths are
completely separated. A more realistic coupling architecture takes into account
that 3G and DVB paths may be linked through some entities to provide a contin-
uous service. Hence, a new entity is needed to switch IP data from 3G to DVB,
and then to insert these data within DVB residual bandwidth. We introduce a
device called Unicast-Broadcast Router (UBR) that manages the interface with
the service provider. It is mainly responsible for inserting IP data switched from
3G network in the DVB-SH network.
Fig. 3. IP data switch

152
A. Jedidi and F. Weis
Figure 3 details the architecture elements. The content creation is on the
broadcaster/service platforms in the DVB-SH network side, and the ASP (Ap-
plication Service Provider) is on the ISP (Internet Service Provider) on the 3G
network side. Both, they are responsible for content creation and service appli-
cation. They feed terminals with content encoded in the appropriate format, via
streaming, download, or ﬁle carousel delivery. Moreover, they generate service
description metadata. The DVB-SH broadcast network is not only responsible
for video ﬂow broadcasting, but it also takes into account IP ﬂows that need to
be encapsulated before being broadcasted. The 3G network permits bidirectional
transmission of IP data, thus providing interactive and personalized services. The
terminal is the user device, which acquires and consumes the received content.
It is bimode, i.e. it has two network interfaces: 3G and DVB-SH. The considered
terminals have a random mobility in the network. The IP encapsulator manages
IP ﬂow encapsulation on MPEG 2-TS packets; it handles time slicing and error
correction mechanisms.
4
Service Scenario Description
Users subscribe to services via the 3G network. The latter reserves a unicast
channel for each user. The more users’ requests, the more unicast channels. Thus,
based on the number of requests, it might be interesting to switch from multiple
unicast channels to the residual bandwidth of the DVB network. The initial idea
behind this use case is to design low cost services, which occupy the initially not
used DVB bandwidth. Hence, a component in the 3G infrastructure, at the ASP
level of the architecture, stores and manages the number of subscription. Then,
the ﬂows are queued and asynchronously inserted in the DVB-SH network as
shown in Figure 3.
This paper is focused on 3G scheduled services. A scheduled service is a service
whose availability is announced in advance, so that users can subscribe before
the service start. Thus, the number of subscriptions to a 3G scheduled service is
known before it starts.
Very popular services lead to deploying a huge number of unicast 3G channels.
Hence, it becomes interesting to switch the service over the residual DVB-SH
broadcast channel instead of deploying as many unicast channels. Obviously,
the delivery of the switched services will be delayed as they are transmitted over
a very small bandwidth. Nevertheless, we achieve a valuable gain as we avoid
to charge our 3G network with many unicast connections delivering the same
content. The challenge is to guarantee, for these services, an acceptable user
experience, especially in terms of transmission delays.
This approach is interesting for videos of several minutes of sustainability;
they may start several minutes later. Moreover, those services are oﬀered at
low costs to compensate the introduced delay of delivery. Targeted services are
periodic video delivery of ﬂash news like weather and traﬃc, asynchronous video
delivery of commercial information as in supermarkets or downloading of popular
softwares.

Eﬃcient Scheduling of Low Cost Popular Services
153
4.1
Switching Decision
If a 3G scheduled service is very popular, the ISP server sends a notiﬁcation
message to the UBR router. This message contains several ﬁelds including the
content size, a binary ﬁeld which indicates if the service is able to be delayed
or not and the maximum acceptable transmission delay. Upon receipt of this
message, the UBR router analyzes those diﬀerent ﬁelds and decides either to
accept or to reject the request. If the request is rejected, the 3G server sends data
immediately through 3G networks. If it is accepted, the 3G service is scheduled
to be broadcasted over the DVB residual bandwidth.
4.2
First Results
In [7], we have proved through simulations that, we can switch two or three
popular IP scheduled services of about 5 megabits of content over DVB residual
bandwidth, while maintaining acceptable transmission delays. The simulation
parameters we used are detailed in section 7.1. We also have shown that delays
become more important as the number of 3G switched services and the amount
of delivered data increase.
Even if those services are delivered at very low costs, we should guarantee
satisfying delivery delays to oﬀer a good quality of experience. Starting from
this observation, we explore possible quality of service enhancements.
In [6], we propose to extend DVB residual bandwidth. In fact, a 3G operator
may pay for reserving some DVB channels, dedicated to switched 3G ﬂows. For
example, in France, the regulation authority speciﬁes that a 120 kbits/s extra
DVB bandwidth will be reserved for a future use. This use could be the design
of complementary innovative services for Personal Mobile TV as speciﬁed in [8].
In our study [6], we consider that this additionnal bandwidth may be allo-
cated for switching our 3G popular services. We prooved through simulations
that this additive residual bandwidth allows a considerable decrease of service
transmission delay, thus enhancing users’quality of experience. Moreover, thanks
to residual bandwidth extension, we are no more limited to the delivery of asyn-
chronous services, with no real time constraints. We can also consider 3G real-
time services with low data rates, like audio services for example. Such services
may become popular, thus involving many unicast connections. The switching
over DVB-SH network becomes an interesting alternative.
Another solution to reduce our service transmission delays is to propose a
more adapted scheduling algorithm for the services switched over the residual
bandwidth. In [6], we have shown through several examples, that the choice of
the scheduling algorithm has an important impact on the quality of our services,
especially in terms of service transmission delay.
In this paper, we propose a new scheduling algorithm that optimizes the use
of DVB bandwidth while oﬀering a good quality of experience to users.

154
A. Jedidi and F. Weis
5
Eﬃcient Scheduling of the Switched Services
Popular 3G services, which are switched over the DVB path, have obviously
lower priority than original DVB services. They are inserted over the DVB path
whenever a residual bandwidth is available. More details about IP data insertion
over the DVB path are given in [7]. However, the scheduling of 3G popular
services, asking simultaneously for being broadcasted over the residual DVB
bandwidth, is the main issue of our proposal. In this chapter, we describe how
a scheduled service can be switched over the DVB residual bandwidth. Then we
propose a new algorithm, which eﬃciently schedules the switched services over
DVB residual bandwidth.
5.1
Switching Request Arrival
If an ISP scheduled service is very popular, the corresponding server asks for
broadcasting this service over DVB-SH residual bandwidth. The UBR receives
the new request and tries to classify it in the list of waiting requests, based on
the choosen scheduling algorithm. In the remainder of this paper, we present our
proposed scheduled algorithm. Our algorithm aims to gurantee a good quality
of service to all the switched services subscribers. In fact, if a switching request
is accepted, the algorithm gurantees that the service is delivered before trans-
mission deadline expiration. Our algorithm is evaluated and compared to other
algorithms with diﬀerent scheduling criteria. The UBR maintains an ordered list
of ISPs requesting to broadcast their contents. The list is ordered based on the
selected scheduling algorithm.
Figure 4 summarizes the processing of ISP ﬂows scheduling and insertion
over DVB-SH residual bandwidth. Each time an ISP scheduled server asks for
broadcasting a content(1), the UBR classiﬁes the new request in the list of
waiting requests, based on the scheduling algorithm(2). Each time the residual
bandwidth is freed, the UBR checks the waiting queue(3), looking for the next
request to be served (head of the queue). Then, the UBR informs the corre-
sponding ISP that he can start delivering its content over the residual band-
width(4). There is only one 3G popular service using the residual bandwidth at
once.
Fig. 4. Processing of the ISP switched services

Eﬃcient Scheduling of Low Cost Popular Services
155
5.2
Switching Request Main Fields
An important point is that the impact of delays on the user experience depends
on the service type. A software download could be delayed by two or three hours
if it is oﬀered at a very low price, but such a delay is not acceptable for a car
driver asking for the traﬃc ﬂash news, before starting his trip. Some services
have more severe constraints, and should be broadcasted over the residual band-
width before the others. This observation led us to deﬁne for each 3G popular
service a maximum acceptable delay for the transmission of its whole content.
Thus, each ISP server requesting to broadcast its popular content over the resid-
ual bandwidth indicates if it accepts to wait before being transmitted or not.
Moreover, it precises the maximum acceptable delay for the end of its service
delivery and the size of its content. Based on those criterias, we designed our
scheduling algorithms.
5.3
Request Processing
When an ISP scheduled server request arrives. The UBR analyses the message
ﬁelds. Then two cases can be considered:
– If the server refuses to wait, the UBR checks if there are requests already
scheduled in the waiting queue. Two cases are possible. If no other requests
are already scheduled in the waiting queue, i.e. this ISP is the ﬁrst one to
request for being broadcasted over the residual bandwidth, then the request
is accepted and the delivery starts immediately. Else, the request is rejected.
– If the server accepts to wait, the UBR processes the scheduling algorithm and
tries to schedule the new service. Depending on the scheduling algorithm,
the request can be either accepted or rejected.
All the scheduling algorithms check the request coherence, i.e. they check if the
content size could be delivered over the residual bandwidth, with respect to the
provided acceptable delay. In fact, as the residual bandwidth is small, we can not
deliver on time services with very important content sizes and very small delays.
If the request is not coherent it is rejected. In our simulations, we assume that
all the considered services accept to wait. The considered scheduling policies are
presented in the next section.
6
Scheduling Algorithms
6.1
Examples of Scheduling Policies
In this section, we present the scheduling algorithms to which we compare our
proposed scheduling algorithm.
FIFO algorithm. All coherent requests are accepted and scheduled in the wait-
ing queue in a First In First Out mode, regardless of their maximum acceptable
delay.

156
A. Jedidi and F. Weis
FIFO Enhanced algorithm. All coherent requests are accepted. Services are
still scheduled in the queue in a FIFO fashion, regardless of their maximum
acceptable delay. However, each time the residual bandwidth is freed, the UBR
checks the ISP request in the head of the queue. If the maximum acceptable
delay is already out of date, then the request is rejected so that it is transmitted
over 3G.
The service delay being already expired, it seems better to transmit it im-
mediately via 3G rather than waiting for a slow transmission over the residual
bandwidth. Our goal is to limit the exceeding delay. However, the bad side of
such an approach is that the UBR has accepted the request at ﬁrst, thus making
the ISP server wait, even beyond its acceptable delay. And, at last, the UBR
has rejected the request. This case leads to a ”disappointment” and a bad user
experience.
Smaller Size First algorithm. Privileging the transmission of smaller con-
tents maximizes the total number of broadcasted ﬂows in the residual bandwidth.
That is why, in this algorithm, we simply accepted all the coherent upcoming re-
quests regardless of their acceptable delays and we classiﬁed them in the waiting
queue based on their sizes, regardless of their maximum acceptable delay.
Smaller Size First Enhanced algorithm. In this algorithm, all coherent
requests are accepted. Services are still scheduled in the queue based on the
smaller size ﬁrst approach, and regardless of their maximum acceptable delay.
However, each time the residual bandwidth is freed, the UBR checks the ISP
request in the head of the queue. If the maximum acceptable delay is already
out of date, then the request is rejected so that it is transmitted over 3G. This
enhanced version of the algorithm limits the exceeding delay. However, it may
lead to ”disappointment” and bad user experience, for services that are accepted,
delayed and ﬁnally rejected.
For all those presented policies, we maximize the number accepted request as we
accept all the coherent requests regardless of their maximum acceptable delay.
FIFO based algorithms represent a ”fair” approach as they respect the chrono-
logical order of request arrival. Size based algorithms maximize the total number
of broadcasted ﬂows over the residual bandwidth, thus reducing the load of the
3G path.
6.2
Our Proposed Scheduling Algorithm: Nearest Deadline First
Each time a 3G server wants to switch a popular service over the DVB path, it
speciﬁes the maximum acceptable delay for the delivery of this service. In our
algorithm, we consider that if the switching request is accepted, the service is
guaranteed to be delivered before the expiration of the transmission deadline.
Our goal is to guarantee for the accepted requests a good quality of service.
Guarantee of service delivery before the transmission deadline. Our
algorithm aims to maximize the number of broadcasted popular services, i.e.

Eﬃcient Scheduling of Low Cost Popular Services
157
to maximize the number of accepted broadcast requests. If a service broadcast
request is accepted, the service is guaranteed to be delivered with respect to the
maximum transmission delay.
When the ISP service receives the UBR acceptation response, it ”trusts” the
UBR and waits for being broadcasted. If, at last, the UBR is unable to broadcast
the content on time, this leads to a bad quality of service. That is why, each
time the UBR tries to insert a new ISP popular service, our algorithm checks
that the the delays of previously accepted services are still respected. Else, our
algorithm refuses to schedule the new service to preserve the delays of the already
waiting ones. In fact, the insertion of a new service in the waiting queue could
delay the transmission start of some already waiting ﬂows. In some cases, the
algorithm should refuse to schedule a new service to preserve the already waiting
ones.
Description and validation of the algorithm. The UBR maintains a list of
the accepted ISP requests that are waiting for the residual bandwidth. This list
is ordered by increasing maximum acceptable delay. To make the comparison
easier, we put this delay in a date format. Each time a 3G service ﬁnishes its
delivery via the residual bandwidth, the UBR pops up the ISP request which is at
the head of the queue, and orders the corresponding ISP to start its delivery. At
the same time, the UBR estimates the date of next residual bandwidth freeing.
We call this date D. A new ISP request arrives. If an ISP scheduled server refuses
to wait, its request is accepted only if it is the ﬁrst one to request using the
residual bandwidth, i.e. there is no other ISP using or waiting for DVB residual
bandwidth. If the ISP scheduled server accepts to wait, the UBR processes the
following scheduling algorithm. Let us consider the service Sx that wants to
broadcast its service over the residual bandwidth. The service is deﬁned by the
couple(tx ,dx ), where tx is the deadline date of service entire transmission over
DVB residual bandwidth, and dx is the worst case duration of transmission of
the service. In the waiting queue, the services are classiﬁed by order of increasing
transmission deadline. The UBR compares the service transmission deadline tx
with the transmission deadlines of the already waiting ISP services. It tries to
insert the service Sx in the waiting queue based on the classiﬁcation of services
transmission deadlines.
Let us consider that there are two services Si−1 and Si already in the waiting
queue, such as:ti−1 ≤tx ≤ti
(1)
We want to insert Sx in the waiting queue after service Si−1, i.e. in the ith
position. This is possible only if: D+
i−1

j=0
dj+dx ≤tx
(2)
This is shown through ﬁgure 5. Actually, if this inequality is not respected, Sx can
not be inserted and delivered on time. The request is simply rejected. However,
if (2) is respected, this means that Sx can be delivered without disturbing the
services delivered before tx.

158
A. Jedidi and F. Weis
Fig. 5. Scheduling a new request
We still have to check that the insertion of Sx does not disturb the delivery
of the other services already waiting in the queue, i.e those whose transmission
deadline is after tx. We have to check that their deadlines are still respected,
after the insertion of Sx, i.e. that: ∀(j ≥i), D +
i−1

k=0
dk + dx +
j
k=i
dk ≤tj
(3)
The veriﬁcation of (3) means that the insertion of Sx has not impacted the
already waiting services that are after the ith position in the queue.
If (2) and (3) are veriﬁed, then the service Sx is accepted and inserted at the
ith position in the waiting queue. However, if (3) is not veriﬁed, we know that:
∃j ≥i, D +
i−1

k=0
dk + dx +
j
k=i
dk ≻tj
(4a)
Moreover as (j ≥i) and as the services are classiﬁed by increasing deadline, we
have: tj ≻tx
(4b)
(4a) and (4b) lead us to say: ∃j ≥i, D +
i−1

k=0
dk +dx +
j
k=i
dk ≻tj ≻tx
(4).
(4a) means that Sx could not be inserted in the ith position in the queue, as it
disturbs the delivery of already accepted services.
Finally, is it possible to insert Sx in another position in the queue?
If such a position l exists, it would be necessarily such as l ≻j ≻i
(5a)
In order to verify: D +
l−1

k=0
dk + dx ≤tx
(5b)
This is equivalent to say: D+
i−1

k=0
dk+
j
k=i
dk+
l−1

k=j+1
dj+dx ≤tx
(5)
(5) is absurd if we assume (4)
Finally we showed that that if we can not insert the request in the position i
(position corresponding to the acceptable delay order), we can not insert it any
where else in the queue without disturbing the already scheduled services.

Eﬃcient Scheduling of Low Cost Popular Services
159
Fig. 6. Residual bandwidth
7
Evaluation of the Scheduling Algorithms
A simulation environment was necessary for evaluating our proposal. In this
section, we present our tool and our simulation parameters (section 7.1). Then
the validation and results of our simulations are presented (section 7.2).
7.1
Simulation Parameters
Our simulations focus on 3G ﬂow delivery over the DVB-SH path. We evaluated
several tools, and we ﬁnally chose to use OPNET Academic edition simulator
[5] since it is more adapted for realistic large-scale scenarios. Using OPNET,
we have simulated the functional entities of our architecture. Indeed, the imple-
mentation of many components was needed, for example DVB content creation,
ASP application server, UBR router, DVB/3G networks and mobile terminals.
The implemented DVB-SH network oﬀers a total bandwidth of 2615 kbits/s.
There are 10 DVB channels provided on this path, each channel is broadcasted at
a variable bit rate of about 260 kbits/s. Actually, we have obtained those values
based on a real H.264 ﬂow, corresponding to DVB-SH standard requirements.
We used pcap (packet capture) that consists of an application programming
interface (API) for capturing network traﬃc. The pcap ﬁle has been generated
by capturing the output of a video encoder, and it has been used to produce DVB
traﬃc and thus the corresponding residual bandwidth. The resulting residual
bandwidth varies between 9 and 15 kbits/s and has an average value of 13.5
kbits/s, as shown in Figure 6.
On 3G network side, the data rate for services is 256 kbits/s. We consider
20 ISP scheduled servers. Each one possesses a popular content and wants to
deliver it over DVB residual bandwith. In our simulations, the ISP servers have
acceptable delays that vary between 30 minutes and 12 hours. Actually, a 12
hours delay is not excessive. Some software downloads or non critical updates

160
A. Jedidi and F. Weis
Table 1. Number of rejected requests
Content size
1 MB 3 MB 5 MB
Algorithms
FIFO
0
0
0
FIFO Enhanced
0
0
1
Smaller Size First
0
0
1
Smaller Size First Enhanced
0
0
1
Nearest Deadline First
0
3
10
are delayed, to be delivered by night on low traﬃc periods. For the acceptable
delays of our 3G servers, we used a uniform distribution of values between 30
minutes and 12 hours.
For each of the presented scheduling algorithm, we run several series of sim-
ulations. Our goal was to vary the services content size and to study the evo-
lution of the number of rejected requests and to check the respect of delays
for each algorithm. We runned simulations with content sizes varying between
1 to 7 Megabytes. For instance, in the simulation with content sizes around 1
Megabyte, the 20 servers have content size values which represent a uniform dis-
tribution between 0.5 Megabytes and 1.5 Megabytes. And, in the simulation with
content sizes around 2 Megabytes, the 20 servers have content size values which
represent a uniform distribution between 1.5 Megabytes and 2.5 Megabytes, and
so on.
7.2
Simulation Results
In this section, the performances of the scheduling algorithms are compared.
Our objective is to ﬁnd a compromise between the number of switched services
(i.e. the number of accepted ISP requests) and the respect of service delivery
deadline. To do so, we determined three comparison metrics.
1. The number of rejected requests : Maximizing the number of accepted re-
quests is an important issue, as we want to decongest 3G networks from
multiple unicast connexions delivering the same content.
2. The number of out of date delivered services.
3. Deadline exceedance rate, deﬁned as follows:
(Real delay - maximum acceptable delay)/maximum acceptable delay.
We obtained a percentage of the deadline exceedance for each service. We
calculate the average value of deadline exceedance rate for each simulation.
Number of rejected services. Table 1 summarizes the number of rejected
requests for each simulation. As expected, FIFO, FIFO Enhanced, Smaller Size
First and Smaller Size First Enhanced algorithm minimize the number of rejected
requests. The table shows that our Nearest Deadline First proposed algorithm
rejects more requests than the other algorithms. However, our scenario targets
small content size services (1 to 3 Megabytes). For a content size of about 3

Eﬃcient Scheduling of Low Cost Popular Services
161
Table 2. Number of out of date delivered services
Content size
1 MB 3 MB 5 MB
Algorithms
FIFO
2
7
12
FIFO Enhanced
2
7
12
Smaller Size First
1
7
13
Smaller Size First Enhanced
1
7
13
Nearest Deadline First
0
0
0
Table 3. Mean deadline exceedance rate
Content size
1 MB
3 MB
5 MB
Algorithms
FIFO
15.1 % 142.03 % 189.45 %
FIFO Enhanced
15.1 % 142.03 % 168.02 %
Smaller Size First
0 %
46.81 % 105.69 %
Smaller Size First Enhanced
0 %
46.81 % 105.69 %
Nearest Deadline First
0 %
0 %
0 %
MegaBytes our algorithm rejects only 3 requests. This means that 17 switching
requests out of 20 have been accepted, which is a very satisfying result.
Respect of service delivery deadline. We runned several simulations while
varying the size of our delivered contents. As expected, our algorithm delivers
all the accepted ﬂows on time. For the other algorithms, we face an important
number of services delivered after delay expiration, as shown in table 2. For
instance, for a content size of about 3 megabytes, seven services are delivered
out of date. The more the ﬁle size increases, the more this number increases.
Table 3 shows that our algorithm delivers all the accepted ﬂows on time. So,
the deadline exceedance rate is 0%. For content sizes of around 3 Megabytes,
FIFO and FIFO enhanced algorithms face an average deadline exceedance rate
value of 142 %. Smaller size ﬁrst and smaller size ﬁrst enhanced algorithms face
a deadline exceedance rate of about 47%. The rates increase considerably when
the content size increases. We presented several possible scheduling algorithms,
where we tried to ﬁnd a compromise between the number of switched services
(i.e. the number of accepted ISP requests) and the respect of service delivery
deadline. Simulations have shown that for very small content of 1,5 Megabytes
or less, all the algorithms provide quite satisfying values in terms of delays.
Moreover, all the algorithms did not rejected any request. So, we can consider
that they are all equivalent for such a content size.
Services with about 3 Megabytes of content size are also considered as small
content services and belong to our scenario targeted services. For such contents,
FIFO, FIFO enhanced, Smaller size ﬁrst and Smaller size ﬁrst enhanced algo-

162
A. Jedidi and F. Weis
rithms continue accepting all the requests, thus leading to considerable exceeding
delays. Those results are conﬁrmed for higher content sizes. At the opposite, our
algorithm satisﬁes the delays of all the requets it accepted. We can conclude that
for our low cost switched services scenario, our Nearest Deadline First proposed
algorithm is the most appropriate, as it makes the best compromise between a
satisfying number of accepted switching requests and the respect of the services
delivery deadline.
8
Conclusion and Perspectives
In our work, we study innovative services that take beneﬁt of the coupling be-
tween DVB-SH and 3G network. Our approach focuses on DVB residual band-
width and its potential exploitation. First, we inserted some small popular 3G
contents in this residual bandwidth. The delivery was a little bit delayed as the
residual bandwidth is very small, however the service is eﬃcient as it replaces
numerous unicast connexions by a broadcast delivery over a bandwidth initially
unused. Moreover, user are satisﬁed as such a service is oﬀered at very low cost.
In a second step, we tried to enhance our results through the deﬁnition of a new
algorithm for scheduling our 3G popular services over DVB residual bandwidth.
We proved that our algorithm makes a good compromise between a satisfying
number of accepted switching requests and the respect of the service delivery
deadline.
References
1. Sesena, J.: Commonalities and peculiarities of DVB-S, DVB-C and DVB-SMATV
systems (comm’s and pec’s of DVB systems). In: International Broadcasting Con-
vention (1995)
2. Schmidt, K., Gunter, C., Rothermel, A.: Improving the mobility of DVB handheld
devices with inter-carrier interference compensation. In: IEEE International Sym-
posium on Consumer Electronics (2004)
3. IST-507312 fp6 european research project ATHENA (digital switchover: Developing
infrastructures for broadband access), http://www.ist-athena.org
4. Negru, D.: Convergence of IP and Digital Video Broadcasting networks: From re-
source management to service provisioning. Ph.D. dissertation, Universite De Ver-
sailles (September 2006)
5. http://www.opnet.com
6. Jedidi, A., Tlais, M., Weis, F., Kerboeuf, S.: Eﬃcient Switched Services over a DVB-
SH/3G Network. In: 5th International ICST Mobile Multimedia Communications
Conference (2009)
7. Jedidi, A., Tlais, M., Weis, F.: Coupling 3G with DVB networks for low cost ser-
vices. In: International Conference on Engineering Management and Service Sciences
(2009)
8. Conseil Superieur de L’audiovisuel, Synthese de la consultation sur le d´eveloppement
des services interactifs en t´el´evision mobile personnelle
9. DVB, IP Datacast over DVB-H: Electronic Service Guide (ESG), DVB Document
A099 Rev.1 (September 2008)

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 163–176, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Semantic Routing for Improved Network Management  
in the Future Internet 
John Strassner, Sung-Su Kim, and James Won-Ki Hong 
Pohang University of Science and Technology (POSTECH), 790-784 Pohang, Korea 
{johns,kiss,jwkhong}@postech.ac.kr 
Abstract. One of the most fundamental management aspects of the Future 
Internet is the representation of management and operational data. The vast ma-
jority of languages and data structures used by network device manufacturers, 
such as SNMP-based designs and Command Line Interfaces, are data-oriented 
and are not conducive to representing semantic knowledge. Furthermore, such 
languages have no ability to represent business concepts, such as a Service 
Level Agreement, or higher-level concepts, such as the ability to maximize 
revenue for all users. To solve this problem, we draw inspiration from semantic 
routing, which traditionally has been used to connect users and applications 
with desired content and services based on intent, meaning, and other semantic 
qualities. We define a type of semantic routing that can be used for both seman-
tic querying and network management tasks, and use it to create a semantic 
overlay network that enables routing to be done on the meaning of the packets. 
This can be used to provide valuable insight into how best to implement  
personalized and context-aware services, as well as choose between multiple 
reconfiguration alternatives for a given scenario. 
Keywords: network management, overlay network, semantic relatedness,  
semantic routing. 
1   Introduction 
One of the most fundamental management aspects of the Future Internet is the repre-
sentation of management and operational data. Currently, such data has been built by 
network device manufacturers to ease the management of their (vendor-specific) 
device or device families. The vast majority of languages and data structures used by 
network device manufacturers, such as SNMP-based designs [1] and Command Line 
Interfaces [2], are data-oriented and are not conducive to representing semantic 
knowledge, such as the effects of a command, its hardware and/or software require-
ments, and the true meaning of what that command does. Furthermore, such lan-
guages have no ability to represent business concepts, such as a Service Level 
Agreement, or higher-level concepts, such as “maximize revenue for all users”. 
This paper is part of our research in developing a new management approach for 
the future Internet that is backwards compatible with legacy implementations, yet able 
to take advantage of new clean-slate approaches. Specifically, this paper focuses on 

164 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
creating a semantic routing overlay network that can be used to route messages based 
on their meaning or intention. For example, suppose we were interested in finding out 
the best way to drive from one location to another. Instead of having to know the set 
of IP addresses of sensors along several routes, we could instead query for any traffic 
sensors that are located in a particular location. We could also query for other proper-
ties, such as weather or events that were local to where we wanted to go, to determine 
if they might influence traffic. This example is difficult for existing search engines to 
handle, as (1) it is not based on keywords, and (2) we are searching for very different 
content in multiple nodes. 
Our research is exploring two parallel uses of semantic routing. The first is as a 
semantic query and retrieval service, which enables the dynamic construction of smart 
peer to peer overlays to connect users and applications with content and services 
based on intent, meaning, and other semantic qualities. The second is to better man-
age advanced services in next generation and future Internet scenarios, where the use 
of semantics can provide valuable insight into choosing between multiple alternative 
reconfiguration options, as well as how best to implement personalized and context-
aware services. 
The organization of this paper is as follows. Section 2 describes the difference be-
tween traditional and semantic routing. Section 3 compares different related work. 
Section 4 defines our approach in detail, and Section 5 presents experiments to test 
our approach. Conclusions and proposed future work are contained in Section 6. 
2   Traditional Routing versus Semantic Routing 
 
Routing algorithms for packet-switched networks are responsible for selecting a path 
in a network to send traffic [3]. The sending of traffic from a source to a set of inter-
mediate nodes to a destination is called forwarding; hence, routing can be viewed as 
the process that controls forwarding. Most routing algorithms only use a single path; 
however, multipath routing can use multiple alternative paths. The advantage of mul-
tipath routing is that alternative paths can be leveraged to improve fault tolerance or 
security, or to increase bandwidth; its disadvantage is increased cost. Our approach 
can be viewed as a simplified form of multipath routing: a control decision is made in 
the routing process whether to use traditional or semantic routing. Note that this does 
not preclude the use of multipath routing in the IP network. 
Most networks have little, if any, concept of semantics used in their routing. For 
example, it is common to compute the “best” route, where “best” is determined by 
some metric or set of metrics (e.g., time to forward) and not save any other routes that 
were found in the analysis. Examples of such algorithms include Dijkstra’s algorithm 
[4], which is used in many protocols, such as Open Shortest Path First [5]. Various 
routing metrics [6] have been proposed to choose one path over another. In contrast, 
our approach considers semantics as a primary metric in routing for those cases where 
semantic routing (as opposed to traditional routing) is used. 
Different protocols can use different routing metrics and algorithms that are not 
compatible with each other, preventing them from being directly compared. This 
prevents selecting the best path from among multiple protocols. Therefore, external 
heuristics are used to select among the different protocols. Typically, the routing table 

 
Semantic Routing for Improved Network Management in the Future Internet 
165 
 
only stores the best route to a given node; additional information may be stored in 
other repositories specific to that protocol, but separate from the routing table, in 
order to control the size of the routing table. Finally, since routing metrics are specific 
to a particular routing protocol, routers that use multiple protocols use one or more 
heuristics to select between routes learned from different routing protocols. Our ap-
proach enables semantic routing to also use routing metrics, and hence be viewed as 
another protocol to be compared in a traditional routing process. 
Conceptually, we want to develop a new set of routing metrics that are semantic in 
nature. This means that they can be used to represent non-functional factors, such as 
cost and availability, as well as user desires and needs. For example, with traditional 
routing, there is no way to say “give me the best connection no matter what the cost 
for this type of traffic, but give me the least expensive cost connection for this other 
type of traffic.” 
More importantly, while semantic routing will use different algorithms compared 
to standard network routing, it is important to make semantic routing appear to be 
another routing process; otherwise, it will be very difficult to seamlessly integrate 
semantic routing with other routing processes. This would defeat the purpose of using 
semantic routing as an alternative to standard network routing.  
3   Related Work 
There are a large number of P2P systems that operate in different ways. This section 
will describe representative P2P systems that can be used to provide an overlay over 
existing network infrastructure that we can use for semantic routing. 
Pure P2P systems define their peers as having identical functionality. That is, every 
peer can retrieve files as well as supply them, meaning that searching is fully distrib-
uted. Examples of this type of system include Gnutella, Freenet, and Limewire [7]. 
The main benefit of this type of system is resilience; any node can join or leave the 
network without adversely affecting the ability to find content. However, the main 
drawback is lack of scalability. 
Hybrid P2P systems define two types of nodes: regular nodes and a set of stand-
alone nodes that are used specifically to support search functionality. The regular 
nodes use metadata to describe their contents, which is then indexed by the special 
nodes. A regular node contacts the special nodes that index the contents of the system 
to find appropriate content; the search nodes are used to find content in the regular 
nodes and direct the request to an appropriate node. Examples of this approach in-
clude Napster and Pointera [8]. The advantage of this approach is twofold: (1) search-
ing can be much more efficiently performed in a centralized manner, and (2) metadata 
can be used to provide a very complete searching facility. However, this is also a 
drawback, because it is impossible to predict which set of keywords will be used, and 
hence, different keyword-based searches can provide different results. In addition, the 
centralized functions used in these systems cannot scale. 
Super-peer based systems, such as Edutella [9], contain two types of nodes: super-
peers that index content and regular nodes that query for content. A super-peer acts as 
an indexing server to a set of regular peers (like the hybrid system), called a cluster. 

166 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
Each super-peer indexes the content of the peers connected to it. However, super-peers 
are also connected to each other (as in a pure system) and collaborate by submitting and 
answering queries on behalf of regular nodes and themselves. This enables a super-peer 
to forward the query to other super-peers if it cannot satisfy the query. In order to re-
solve single point of failures for super-peers, redundancy is introduced. In this approach, 
every super-peer that is connected to each other for providing redundancy is also con-
nected to every client, and hence maintains a complete index of all of the client data, as 
well as the indexes of other partners. The drawback is the additional overhead required 
for maintaining the super-peer connections, along with the extra traffic generated and 
processes necessary to keep the indices aligned. 
We will define a semantic overlay network that is implemented on top of an exist-
ing IP network. Our semantic overlay network will function in a manner similar to a 
P2P network. However, we will use a different algorithm to organize nodes and re-
trieve information, as explained in Section IV. 
4   Design of Our Semantic Routing System 
We want to create an alternative routing model, one in which semantics can be used 
to manage both the creation of the overlay used to manage the network nodes as well 
as to efficiently search for and find knowledge and resources that are of interest to a 
particular query. This version of our approach seeks to add semantic routing capabili-
ties to existing IP systems. This rules out a clean-slate design, since one of our pri-
mary goals is to maintain backward compatibility. 
The following subsections will explore (1) how to integrate semantic routing into 
the existing IP system, and (2) how to implement semantic routing. This latter subject 
raises three important issues involving the differences in addressing, routing, and 
forwarding between traditional IP and semantic networks. 
4.1   Integrating Semantic Routing with Traditional IP Routing 
Our approach to managing the future Internet is predicated on maintaining backwards 
compatibility with existing and legacy approaches while enabling new approaches to 
be integrated. Therefore, we make our semantic routing approach appear as another 
ordinary routing protocol, as shown in Fig. 1. This enables semantic routing to be 
seamlessly combined with IP routing. In traditional routing, different interior routing 
protocols can each advertise the same route. There can be m different ways of routing 
a source to a destination, consisting of i semantic and j traditional routes. If two or 
more routing protocols (semantic and/or traditional) provide route information for the 
same destination, then we use a heuristic called administrative preference to select 
between them. However, since semantic routing is potentially more computationally 
complex than traditional IP routing, we only want to use semantic routing when it is 
needed. Therefore, we define an additional heuristic, called semantic relevance, to 
weight the output of the semantic routing algorithms. 

 
Semantic Routing for Improved Network Management in the Future Internet 
167 
 
 
 
Fig. 1. Conceptual Overview of the Semantic Routing Process 
 
We can accommodate this by first, making the range of the semantic route less 
than traditional routing and second, by multiplying the semantic routing weight by a 
semantic relevance factor, which is 1 if the routing is semantic in nature, and 10 if 
not. This has the effect of making the semantic route preferred if the request is seman-
tic in nature, and most likely not preferred otherwise. The use of a separate semantic 
relevance parameter enables us to selectively adjust the use of semantic routing as we 
integrate it with traditional routing. We use the following weight ranges for each type 
of routing protocol: 
• Most preferred interior routing protocol: 
50 
• Least preferred interior routing protocol: 
200 
• Most preferred semantic route: 
1 
• Least preferred semantic route: 
50 
(1) 
4.2   Differences in Addressing between Semantic and Traditional Routing 
In a traditional IP network, the transport protocol operates on specialized addressing 
information contained in or associated with a message; the actual content associated 
with the message is typically not used. Hence, a destination address is either directly 
known (for unicast) or, for multicast, a group address is known. In contrast, a seman-
tic network has no specialized addressing information; rather, it routes directly on the 
content of the message. In our system, this corresponds to a set of attributes. If we 
define D to be the system schema, and as such the set of all attributes that an object 
can take on, then queries as well as nodes can both be represented by the same set of 
attributes. Note that the set of attributes D does not have to be large; this can be 
…
Semantic
Routing
Algorithm 1
…
Semantic Routing
Algorithm i
Routing
Table
Longest
Prefix Match
Forwarding
Process
Packet Flow
Administrative
Preference
Semantic
Relevance
Semantic
Routing
Algorithm i
Semantic
Routing
Algorithm 2
Traditional
Routing
Algorithm 2
Traditional
Routing
Algorithm j
Traditional
Routing
Algorithm 1
Install
Best Route

168 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
viewed as the set of keywords that are used in a system. However, this means that a 
message may or may not have a destination at a particular time. 
We identify the semantics of an object using a set of attributes D={d1, d2, …, dj}. 
The simplest case is for a node to be associated with exactly one object. We can then 
define a node profile as the set of attributes P={ p1, p2, …, pj} and a client request as a 
similar set of attributes Q={ q1, q2, …, qj}. Queries are then matched by computing 
the dot product of P and Q: 
1
( , )
j
i
i
i
sim Q P
q
p
=
=
•
∑
                                                (2) 
This is shown in Fig. 2. Any node can store data as well as query for data. A query is 
characterized by an object profile D. The nodes register their interest in objects by 
sending their node profiles to the semantic network, which then forwards the query to 
those nodes whose node profile matches the object profile of the client. 
 
Fig. 2. Matching Information in the Semantic Network 
For example, assume that a node is a traffic sensor, where each traffic sensor is lo-
cated on an intersection in a city. A traffic sensor can be described by the attributes 
{traffic_sensor, street_name_1, street_name_2, city_name, zip_code}, where 
street_name_1 and street_name_2 are the names of the streets that intersect. In this 
example schema, the use of the traffic_sensor attribute enables a search to easily find 
all sensors that are used for traffic monitoring; the search can then be customized by 
using one or more additional attributes to focus the search. 
The node profile contains multi-dimensional information (i.e., the set of attributes 
{p1, p2, …, pi} that identifies objects contained in that node). Hence, mapping a multi-
dimensional structure to a single scalar unicast or multicast address results in the loss 
of information, and thus cannot be used. Hence, we use a directed graph to represent 
our information. 
One way to represent this information is to use the W3C’s Resource Description 
Framework (RDF) [10], which uses XML [11] as its syntax. Fig. 3 shows an example 
object profile for a traffic sensor. In this example, line 3 defines a “tsnsr” namespace, 
enabling lines 5-X to define semantic attributes of traffic sensors. 

 
Semantic Routing for Improved Network Management in the Future Internet 
169 
 
2:  
xmlns:rdf = “http://www.w3.org/TR/WD-rdf-syntax#” 
3:  
xmlns:tsnsr = “http://schema.sensors.traffic/1.1”>
4:  
<rdf:Description> 
5:  
 
<tsnsr:Name>Traffic Sensor x453</tsnsr:Name> 
6:  
 
<tsnsr:GPS> 
7:  
 
 
<tsnsr:Position latitude=”36° 4’ 60 N” 
8:  
 
 
 
 
 
 
   longitude=”129° 22’ 0 E”/> 
9:  
 
 
<tsnsr:City>Pohang</tsnsr:City> 
10:  
 
 
<tsnsr:Province>Gyungbuk</tsnsr:Province> 
11:  
 
</tsnsr:GPS> 
11:  
 
<tsnsr:Streets> 
12:  
 
 
<tsnsr:Street1>Hyoja-Dong</Street1> 
13:  
 
 
<tsnsr:Street2><Daejam-Dong</Street2> 
14:  
 
</tsnsr:Streets> 
11:  
 
... 
... 
X:  
 
</rdf:Description> 
X+1: 
</rdf:RDF> 
1: <rdf:RDF 
 
Fig. 3. Example of an Object Profile for a Traffic Sensor 
Fig. 4 shows an example of a node profile that would be interested in the content of 
the traffic sensor object profile shown in Fig. 3. In this example, the node profile 
defines a logical AND constraint (on line y), which means that all elements between it 
and its corresponding end tag (line z) must be true in order for the semantic network 
to match the object profile with the node profile. Two restrictions are defined that are 
part of this AND constraint; the first limits the location of the sensors to within two 
kilometers of a particular location (defined in lines y+4 and y+5), while the second 
looks for an exact match of a street named “Hyoja-Dong”. 
Conceptually, node profiles express interest in information. If we use Fig. 3 and 
Fig. 4 as examples of the query and node profiles, respectively, and define the opera-
tor * as representing any value, then we have: 
1: ... 
y:  
<node_profile:And> 
y+1: 
 
<tsnsr:Loc> 
y+2: 
 
 
<tsnsr:Position> 
y+3: 
 
 
 
<node_profile:distance within=”2” units=”km”> 
y+4: 
 
 
 
 
 
 
 
 
latitude=”36” 
y+5: 
 
 
 
 
 
 
 
 
longitude=”129”/> 
y+6: 
 
 
</tsnsr:Position> 
y+7: 
 
<tsnsr:Loc> 
y+8: 
 
<tsnsr:Streets> 
y+10:  
 
<node_profile:string-match>Hyoja-Dong 
y+11:  
 
</node_profile:string-match> 
10:  
 
</tsnsr:Streets> 
11:  
 
... 
...
z:  
</node_profile:And> 
z+1: 
... 
 
Fig. 4. Example of a Node Profile that is Interested in a Traffic Sensor 

170 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
 
Attribute 
Name 
Object Profile 
Node Profile 
1 
Name 
Traffic Sensor x453 
* 
2 
latitude 
36° 4’ 60 N 
    34° *’ *N - 
36° *’ *N 
3 
longitude 
129° 22’ 0 E 
129° *’ *E 
4 
City 
Pohang 
* 
5 
Province 
Gyungbuk 
* 
6 
Street1 
Hyoja-Dong 
Hyoja-Dong 
7 
Street2 
Daejam-Dong 
* 
Fig. 5. Simple Semantic Routing by using the Inner Product of Profiles 
Fig. 5 shows that the object profile of Fig. 3 will match the node profile of Fig. 4 
because the latitude and longitude values of the object profile are contained within the 
latitude and longitude value ranges of the node profile, and all other attributes are 
either the same (i.e., Street1) or do not care (e.g., attribute 1). The matching can be 
easily done using, for example, a trie data structure [12] or one of its variants, such as 
a Patricia trie. 
4.3   Forwarding in a Semantic Network 
In traditional IP routing, the destination address contained in the packet is looked up 
in the routing table of the router, and a longest prefix match is done; if a match is 
found, the packet is forwarded on the corresponding output port. If no match is found, 
then the packet is forwarded on the default link, which will drop the packet if the 
default link does not contain a default entry. 
In a semantic network, the forwarding process consists of matching object and 
node profiles. If the two profiles match, then the message is forwarded on the ports 
that are associated with the matching node profiles. The matching operator can be 
configured to act as a direct match operator or as a filter (as shown in Fig. 5). A de-
fault filter can be defined to catch messages that do not match any node, just as in the 
traditional case; this is useful for logging error messages for unmatched messages. 
This becomes more complex if multiple objects are associated with a single node.  
For example, a node could be a home gateway serving multiple devices, or a node 
could be a database containing multiple documents. In this case, the node profile is an 
array of individual object profiles. The node profile can be represented in two differ-
ent ways. One way is as a single bit vector, where a “1” means that one or more ob-
jects have the corresponding attribute, and a “0” means that none of the objects have 
the corresponding attribute. The advantage of this approach is simplicity, which trans-
lates to less storage. However, all this does is identify that a node has one or more 
objects that satisfy the request; additional work is still required to actually find the set 
of objects that match the query. The second way is as a j x k multi-dimensional array, 
consisting of k documents that each have j attributes. This enables the set of docu-
ments to be found in one step, but requires additional storage and processing power to 
find them. Section 5 discusses experimental tradeoffs between these two strategies. 

 
Semantic Routing for Improved Network Management in the Future Internet 
171 
 
4.4   Routing in a Semantic Network 
In traditional IP networks, routing is the process of creating and disseminating reach-
ability information. This information is used to create routing tables. As the network 
topology changes, the routing tables are updated accordingly. In a Semantic Network, 
routing is done very differently. This is because (1) nodes are not described by artifi-
cial addresses, but rather by the content that they contain, and (2) while IP addresses 
are scalar, content data are multi-dimensional data. 
There are many ways to design semantic routing; two are building a routing tree 
that is made up of a set of filters and turning the semantic network into a small-world 
network. Since our attributes are strings, a trie data structure is a natural choice, since 
searching for a key takes at worst case O(l) time for a key of length l, while binary 
search trees can take up to O(l log n) time, where n is the number of elements in the 
tree. More importantly, longest-prefix matching is very straightforward using tries. 
If nodes are associated with a single object, the routing tree filtering approach 
works very well. However, if there are a large number of nodes, then it becomes in-
creasingly difficult to guarantee a fast response, because there is no mechanism to 
keep the number of hops small. This is exacerbated when nodes are associated with 
multiple objects. In addition, when a node has multiple objects, there is no easy way 
to control the assignment of objects to nodes in an optimal manner. 
This last problem is a clue to a novel solution. If we want to obtain an optimal se-
mantic distribution of those objects (i.e., enable each object to be associated with a set 
of other objects that are semantically related to it), then we need to enable the topol-
ogy to self-organize, based on the current content of the objects. This parallels the 
small-world [13] phenomenon of social science, which is also exhibited by networks, 
and especially P2P networks. Conceptually, if the system starts with documents being 
randomly associated with different nodes, a small-world network would enable the 
system to reorganize to group documents of similar interests in the same sets of 
nodes. For our system, this means that a node will advertise a node profile consisting 
of a set of attributes; this node profile is then used to match objects having similar 
object profiles. The objects are then redistributed according to the best match between 
the profile of an object and the profile of a node. Hence, once the network stabilizes, 
as objects change, the network facilitates their association with the node that best 
describes their content. 
4.5   Enhancing the Semantics of Our Semantic Routing 
In traditional IP networks, messages are routed to a known destination. In our seman-
tic network, we need to map a much larger address space than that used by IP. 
IPv4/v6 addresses have a limited range (0 to 232 or 2128). However, semantic addresses 
can have a much larger range, since the address space is the product of the number of 
all possible resources that have to be individually addressed and the number of possi-
ble ways to describe each resource. This latter factor is potentially very large, since 
we want to enable users to use multiple descriptions to address a resource (e.g., using 
synonyms). Therefore, we need a scalable network topology that can cover a large 
semantic routing space without having long routing times (i.e., long numbers of hops 
to reach a destination). 

172 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
We satisfy the large address space problem by translating the user input into one or 
more common terms, and then searching on the terms. This is explained using Fig. 6. 
 
Fig. 6. Matching Information in the Semantic Network 
We define a common lexicon that contains the vocabulary used in the semantic net-
work along with definitions, synonyms, and other lexical functions. This is part of a 
larger linguistics-based effort that is beyond the scope of this paper. The lexicon con-
tains software that translates the user’s query into terms that are stored in the semantic 
network. This enables the user to use much of their native terminology, and not be lim-
ited by our system, while dramatically reducing the complexity and storage require-
ments of each node. 
4.6   Characteristics of a Small-World Network 
In a small-world network, each node has many short-range connections (i.e., its near-
est neighbors), but only a few random long-range connections. A small-world  
network has two attractive properties: (1) a low number of hops between any two 
randomly chosen nodes, which implies fast routing, and (2) a high clustering of 
nodes, which implies that a small-world network can quickly self-organize and pro-
vide good query capabilities, even under heavy demand. These properties are de-
scribed by the clustering coefficient and the characteristic path length metrics in [14]. 
We change the formulation of these two metrics because we use a directed strongly 
connected graph, since in a network, routing between nodes can be asymmetric. 
Hence, we can define a weighted version of the clustering coefficient by multiplying 
the contribution of the clustering of a node by how related its content is. This means 
that each edge from a neighbor w counts only as much as the semantic relatedness 
between w and v. This makes sense, because in a small-world network, nodes are 
surrounded by dense neighborhoods of similar nodes. 
Watts et al. [13] proposed a β-model that models the topology of a small world 
network, as shown in Fig. 7. In this model, the topology is an interpolation of a regu-
lar network and a random network. That is, peers can be densely clustered, but have 
few characteristic paths. The β-model starts with an initial regular graph, and ran-
domly replaces a lattice edge by a random edge with a probability β. The network is 
completely regular if β = 0, while it is completely random if β = 1. When β is set to an 
intermediate value between 0 and 1, the graph behaves like a small-world network. 

 
Semantic Routing for Improved Network Management in the Future Internet 
173 
 
 
Fig. 7. Conceptual overview of the semantic routing process 
We can use a similar “rewiring” procedure to create our small-world network. We 
use the same rewiring procedure as in [13]. In [15], Walsh points out that due to the 
high clustering of nodes in such a topology, search can be very difficult since local 
decisions quickly propagate globally. Walsh solves the problem using a strategy 
called randomization and geometric restarts. In this approach, the search is restarted 
after a fixed number of nodes have been visited. The cutoff value for restarting the 
search increases geometrically to ensure that the search converges rapidly. 
4.7   Creating Our Small-World Network 
Speed in answering queries is directly proportional to the path length from source to 
destination. Instead of trying to build a single very large network, we build a large set 
of small networks. This enables us to rigidly control the shortest path in each of the 
small networks, and then control the shortest overall path throughout the set of net-
works by making the set of networks a small-world network. 
A semantic network will, over time, self-organize into groups of nodes that have 
similar interests. We use the concept of semantic relatedness [16] to group nodes into 
a community of interest (CoI). Semantic relatedness measures how close the meaning 
of one entity is to the meaning of another entity using one or more lexical relation-
ships, such as synonymy (e.g., “bank” and “lending institution”), antonymy (e.g., 
“accept” and “reject”), meronymy (e.g., “court” is a part of “government”), and other 
domain-specific relationships. Semantic relatedness can be measured in a number of 
ways. We use the measure defined by Jiang and Conrath [17], as it outperformed 
other measures for our specialized vocabulary. 
We create our semantic network from a set of CoIs arranged as a small-world net-
work – in other words, as a group of groups. The small-world network is a regular 
graph that has been rewired, as described in [13], to provide small-world characteris-
tics. We use a β factor of 0.45, which is large enough to prevent network partitioning 
yet small enough to not distort the topology too much. Each node in the small-world 
network is a CoI, which is a group of nodes having high mutual semantic relatedness. 
This group of nodes forms a complete graph (i.e., its diameter is 1), since each object 
in the CoI is semantically related to each other. 
So far, we have considered the information retrieval aspect of our network. In par-
ticular, creating CoIs enables us to make use of group locality, which is a critical 

174 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
social networking feature, and reflects the fact that nodes in a social network tend to 
work in groups. However, it doesn’t solve the problem of temporal locality – the 
desire for the same set of resources to be used over a small time period. 
This latter is commonly implemented in management systems using hierarchy 
and/or aggregation. For example, a node that gathers data from other nodes in order to 
compute statistics about those nodes has a fundamentally different relationship than 
nodes that offer semantically related information to retrieve or functions to execute. 
Hence, we create a second set of links, called supervisory links, between nodes that 
exhibit such management functionality. Note that this also reinforces group locality, 
as it can be used to manage the content of particular groups. 
5   Experimental Results 
In our experiments, we defined four different types of sensor data in each node: ‘traf-
fic’, ‘temperature’, ‘wind’, and ‘humidity’. We can use two different approaches to 
find sensor data, as mentioned in section 4.3. The first one is using a vector to repre-
sent the type of data to be found, and then performing a search on the matched data; 
the second is to use a j x k multi-dimensional array to find the requested document in 
a single operation. In this experiment, we implement both two different approaches 
and compare the result with different parameters. 
Figure 6 shows the comparison between using j x k multidimensional array and us-
ing vector array. We assume that there are seven attributes to find data and if the 
number of matched attributes is greater than 4, the data is retrieved. The time to find 
matched data is measured with increased number of nodes. As result show, when the  
 
 
Fig. 8. Comparison of a Vector Array vs. a j x k Multi-dimensional Array 

 
Semantic Routing for Improved Network Management in the Future Internet 
175 
 
 
Fig. 9. Time to Find Data with different Number of Matched Data 
number of nodes are smaller than 10000, there is no differences between them but as 
the number of nodes exceed 10000, the time to find data using j x k multidimensional 
array is bigger than using vector array. Figure 7 shows the time to find sensor 
datawhich has various numbers of matched attributes. We measured time with differ-
ent threshold t using using j x k multidimensional and vector array. Figure shows that 
when the number of matched attributes is smaller than 4, vector array approach shows 
shorter searching time compared with j x k multidimensional approach. This result 
presents that if we want to find data only partially matched, vector array approach is 
more powerful. For example, if we want to find data with attributes ‘traffic’, it is 
more powerful to use vector array than find data with attributes ‘traffic’, ‘Pohang’, 
‘Hyoja-dong’.   
6   Conclusions 
Progress in the management of the future Internet and networked applications will 
increasingly depend on being able to understand the semantics of monitored data. 
This work notes the similarity between this task and previous work in social net-
works, where the desire to find data based on meaning, as opposed to addresses, plays 
a significant role. We seek to design a system that can support semantic routing for 
both of these important use cases. 
However, there is a significant difference between these scenarios. Social networks 
have, up to now, assumed the existence of a separate application without any con-
straints. In stark contrast, network management will, in the foreseeable future, be 
rigidly constrained by backwards compatibility. While clean-slate approaches are 
thought provoking, the harsh reality of business implies limited adoption of such 
approaches, due to their lack of backward compatibility. In this economic environ-
ment, service providers and enterprises will not replace existing equipment! 

176 
J. Strassner, S.-S. Kim, and J.W.-K. Hong 
 
Therefore, we have designed a semantic routing system that can seamlessly inte-
grate with existing approaches. It is implemented as an overlay, avoiding the physical 
disruption of the network. It appears as another interior routing process, facilitating its 
incorporation into existing network topology designs. This is in spite of its significant 
differences in routing and forwarding. 
This first paper outlines our approach and shows the viability of attribute-based 
search and retrieval. Future work will expand on this foundation, and compare this 
approach to pure DHT systems and conduct more extensive tests. 
 
Acknowledgments. This work is sponsored in part by the WCU (World Class Uni-
versity) program through the Korea Science and Engineering Foundation funded by 
the Ministry of Education, Science and Technology (Project No. R31-2008-000-
10100-0). 
References 
1. Harrington, D., Preshun, R., Wijnen, B.: An Architecture for Describing Simple Network 
Management Protocol Management Frameworks, RFC3411, STD0062 (December 2002) 
2. Cisco, http://www.cisco.com/warp/cpropub/45/tutorial.htm 
3. Medhi, D., Ramasamy, K.: Network Routing: Algorithms, Protocols, and Architectures. 
Morgan Kaufman, San Francisco (2007) 
4. Dijkstra, E.: A Note on Two Problems in Connexion with Graphs. Numerische Mathe-
matik 1, 269–271 
5. Moy, J.: OSPF Version 2, RFC 2328 
6. Baumann, R., Heimlicher, S., Strasser, M., Weibel, A.: A Survey on Routing Metrics, TIK 
Report 262, ETH-Zentrum, Switzerland (2007) 
7. http://www.gnutellaforums.com/ 
8. Yang, B., Garcia-Molina, H.: Comparing Hybrid Peer-to-Peer Systems. In: Proc. of the 
27th Int. Conference on Very Large Data Bases (VLDB 2001), Italy, pp. 561–570 (2001) 
9. Nejdl, W., Siberski, W., Wolpers, M., Schmitz, C.: Routing and clustering in schema-
based super peer networks. In: 2nd International Workshop on Peer-to-Peer Systems 
(2003) 
10. http://www.w3.org/TR/REC-rdf-syntax/ 
11. http://www.w3.org/TR/xmlschema-0/ 
12. Fredkin, E.: Trie memory. Communications of the ACM 3(9), 490–499 (1960) 
13. Watts, D.J.: Networks, Dynamics, and the Small-World Phenomenon. American Journal of 
Sociology 105(2), 493–527 (1999) 
14. Watts, D.J., Strogatz, S.: Collective dynamics of ‘small-world’ networks. Nature 393, 440–
442 (1998) 
15. Walsh, T.: Search in a small world. In: Proc. of the 16th Intl. Joint Conference on Artificial 
Intelligence, pp. 1172–1177 (1999) 
16. Gabrilovich, E., Markovich, S.: Computing Semantic Relatedness using Wikipedia-based 
Explicit Semantic Analysis. In: Proc. of the 20th Intl. Joint Conference on Artificial Intelli-
gence, pp. 1606–1611 (2007) 
17. Jiang, J., Conrath, D.: Semantic similarity based on corpus statistics and lexical taxonomy. 
In: Proc. of the Intl. Conference on Research in Computational Linguistics, pp. 19–33 
(1997) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 177–189, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Mirror Routing for Satellite Networks  
with Cross-Layer Optimization 
Zhijiang Chang and Georgi Gaydadjiev  
Computer Engineering Laboratory, Delft University of Technology 
Mekelweg 4, 2628 CD Delft, The Netherlands 
Tel.: +31 15 278 6177 
{zhijiangchang,georgi}@ce.et.tudelft.nl 
Abstract. Several strategies have been proposed for routing in the Low Earth 
Orbit (LEO) satellite  networks. The multi-layered routing approaches are 
envisioned as promising because they use Middle Earth Orbit  (MEO) satel-
lite  to  extend  the  LEO  satellite  network’s communication capabilities. The 
previously proposed multi-layered routing approaches, however, still assume that 
the satellites in the same layer share similar characteristics. This assumption is 
not true in the future satellite networks. This is because the satellites in  the 
future will be heterogeneous with various computation, communication and 
power capacities that lead to more complicated route construction challenges. 
In order to solve this problem, we propose the usage of cross-layer designs that 
can collect information from the neighboring satellites and evaluate their ca-
pacity during route construction and  maintenance phases. This paper ﬁrst 
analyzes the advantages and disadvantages of different satellite routing ap-
proaches. Then a multi-layered routing scheme called Mirrored Routing with  
Cross-layer optimization (MRCL) is introduced. In order to reduce overhead 
caused by the routing scheme, a hop count limitation, instead of a strict group-
ing policy, is used to direct packets to the MEO layer. According to our simu-
lations, the end-to-end delay can be reduced 15% when a proper hop count 
limitation is selected. The novel  routing scheme also signiﬁcantly reduces the 
packet loss and the routing overhead (in terms of bytes of routing information) 
compared to the routing with out cross-layer optimization and  hop limitation. 
We also simulate and careful investigate the performance of MRCL using 
various hop count limitation conﬁguration. 
1   Introduction 
The future satellite applications require self-organized, dynamic network topology 
without predeﬁned constellation. Such applications include: 1) deep-space exploration 
that need to relay data using other satellites; 2) LEO satellites control for satellites 
that do not have a direct link to ground stations. In order to support the above applica-
tions, researchers have proposed multiple-layer satellite routing structures that utilize the 
MEO and GEO satellites to extend the coverage of satellite telecommunication. These 
proposals ( [1] etc.) use the geographic information of the satellites to calculate the 
routing information or establish formation. These approaches, however, assume that 

178 
Z. Chang and G. Gaydadjiev 
 
the quality of Inter Satellite Link (ISL)  is simply a function of the distance between 
two satellites. They do not consider the communication and computation capacities 
of each satellite, and the interferences from the space such as the solar wind and those 
caused by the inter-satellite communication itself. 
In order to develop a realistic multiple layer satellite routing architecture that 
fulﬁlls the requirements of future applications, we take the advantages of cross-layer 
design. The cross-layer design optimizes the overall network performance by sacriﬁcing 
the layer’s independence [2]. A strict modularity and layer independence may lead to 
non-optimal performance in IP based next generation satellite networks. With the cross-
layer approaches, the link quality information can be used during the routing discovery 
and maintenance phases to avoid establishing unstable ISLs among the satellites in the 
same layer as well as satellites in different layers. Furthermore, instead of organizing 
the satellites in a strict grouping fashion, we use hop count limitation to determine 
whether LEO layer or MEO layer routing is preferable. This signiﬁcantly reduces the 
computation and communication overheads. By regarding the MEO satellites and ground 
stations as the backbone of the network architecture, the LEO satellite can select links 
to the ground station and the ISL to the MEO satellites. Consequently, the proposed 
routing scheme is named as Mirrored Routing with Cross Layer optimization for satel-
lites (MRCL). 
The main contributions of this paper are: 
 
− Analysis of satellite routing approaches and their limitations; 
− Novel Mirror Routing with Cross-layer optimization for satellite networks 
(MRCL); 
− Careful simulation to validate the advantages of our routing scheme using  
ns-2. 
 
This paper is organized as follows. The satellite routing proposals and their advan-
tages and disadvantages are analyzed in section 2. The section 3 presents the Mirrored 
Routing with Cross Layer optimization. The simulation results are demonstrated in 
section 4. We ﬁnally conclude the discussion in section 5. 
2   Traditional Satellite Routing Approaches and Their Limitations 
Centralized route construction is conducted by the “master nodes” in the network. 
The centralized routing may leads to static connection among the satellites if the routes 
are not recalculated frequently. Therefore, the centralized routing approaches such as 
the ones proposed in  [3, 4] are not suitable for the ad hoc satellite networks. 
The distributed satellite routing construction no longer depends on the “master 
nodes”. Consequently, the distributed routing provides more feasibility, stability and 
adaptability to the network compared to the centralized one. The distributed satel-
lite routing, however, does not take into account the geographic nature of the satellite 
networks. The inter-satellite links (ISL)  between LEO satellites and the ISL between 
LEO and MEO satellites have very different propagation delays. They cannot be 
treated with the same routing policy. 

 
Mirror Routing for  Satellite Networks with Cross-Layer Optimization 
179 
 
The Multiple Layer Satellite Routing (MLSR)  [1] solve the above problem by 
grouping the satellites to the LEO, MEO and GEO layers. In MLSR, each satellite 
collects its topology information and sends it to its manager in upper layer. Satellites 
in the top layer calculate the individual routing tables for all satellites separately and 
send the tables to the corresponding satellites, which causes high computation over-
heads. Furthermore, one of the assumptions of MLSR is that  the MEO satellite 
constellation is arbitrary as long as it has a global coverage. This assumption cannot 
be taken for granted if we consider the fact that the MEO satellites may belong to 
different organizations just like the LEO satellites. 
A dynamic routing algorithm named Double-Layered Satellite Network Routing Algo-
rithm (DLRA)  is proposed in [5] . In DLRA, double-layered satellite networks con-
sisting of LEO and MEO satellites can make the convenience  of those advantages of 
LEO and MEO satellites in short-distance and long-distance communications. The ba-
sic principle of DLRA is that traffic of short-distance communications is routed only 
through the LEO layer, and long-distance ones are accessed by the LEO layer and 
routed through the MEO layer. The shortage of DLRA is that the ISLs are considered 
to be stable and their quality is only related to the distance between two satellites. 
The above multi-layered routing proposals, in general, use the geographic infor-
mation to form subnetworks. This approach, however, strongly relies on the geo-
graphic information, and neglects the impact of other factors that affect the quality of 
communication links such as the satellite’s communication and power capacity. Fur-
thermore, these architectures assume the satellite network is homogeneous, and the 
satellites in the same layer share similar characteristics. In reality, the heterogeneous  
satellites have various power,  computation and communication capacities. This  
leads to  much more  complex route selection problem then the homogeneous  satel-
lite networks.  The  multi-layered routing mechanism is also based on the assump-
tion that  the satellites are capable of recognizing the satellites in their own layer. 
This is only reliable if the satellite broadcast their own proﬁles when establishing rout-
ing table. Consequently, more overhead is introduced. Our proposal  considers the 
satellite network scalable (multi-layer), and limits the information exchange within the 
same layer as much as possible. 
3   Mirrored Routing with Cross-Layer Optimizations 
We propose a Mirrored Routing with Cross-layer optimizations (MRCL) that uses 
link quality information, instead of geographic location information to predict and se-
lect routes. In a network where MEO satellites form global coverage, multiple routes 
exist from one source to one destination. These routes are ranked according to their 
predicted stability. Furthermore, in order to reduce the over-head in the resource-limited 
LEO satellites environment and reduce queue length in LEO satellites, hop count limi-
tation in LEO layer is used to direct the packet from LEO layer to MEO layer. No 
strict grouping and voting for LEO/MEO  up and down links are required. 
 
 

180 
Z. Chang and G. Gaydadjiev 
 
3.1   Multi Layer Satellite Network 
The satellite network is divided into two layers: 
1) 
MEO layer: The MEO layer refers to the collection of all MEO satellites in the 
network. This layer is positioned at an altitude between the GEO and the LEO 
layers. The constellation of the MEO satellites can be arbitrary as long as global 
coverage is achieved at all times.  
2) 
LEO layer: The LEO layer consists of all LEO satellites in the network. This 
layer has lower altitude than MEO layer. We assume that the LEO satellites form 
a Walker Star  type constellation. They do not necessarily form a single con-
nected network. The LEO satellites do not guarantee the global coverage. The 
LEO satellites, due to their shorter life time, are designed to be smaller than 
the MEO satellites. Consequently, they have less power and communication capac-
ity than MEO satellites. 
The coverage of the MEO satellite network is better than that of the LEO satellite 
networks because of MEO satellites’ higher orbits. The GSLs of MEO satellites is 
more stable (last longer) than the GSLs of LEO satellites because MEO satellites’ 
higher orbits. Therefore,  we can assume the ground stations and the MEO satellites 
form the backbone of the satellite network. In this backbone the links are stable but 
with various quality. For instance, the GSL  of MEO satellites may suffer bit error 
rate (BER) ranging from 0.1% to 10%. This is similar to Internet that  have con-
gestion, which also has package loss due to buffer overﬂow in routers.  
3.2   Assumptions for  the Satellite Network 
There are many assumptions for the satellite network design and its various activi-
ties including constellation, access policy and network architecture, etc. In this pa-
per, discussions and results are conducted and obtained based on the following basic 
assumptions: 
1) 
The MEO satellites and the ground stations (GS) can provide continuously and 
seamlessly coverage for its immediate lower satellite layer. This means the MEO 
satellites and ground stations are in constant stable state to provide access and 
routing functionalities to LEO satellites. The communication between the GS and 
MEO satellites are continuous. Consequently, the GS and MEO satellites form a 
“mirrored” backbone of the network, while the LEO satellites are between this 
mirrored backbone. This is illustrated in ﬁgure 1. 
2) 
Satellites in LEO layer are organized into the polar constellation. 
3) 
We only consider the space segment of the LEO/MEO  satellite constellation 
and their connectivity to the ground stations, while the ground users terminals 
are beyond of discussion in the paper. Consequently, we do not discuss terminal 
handover issue in this paper.  
4) 
All satellites in the network are capable of on-board processing and routing. 
 

 
Mirror Routing for  Satellite Networks with Cross-Layer Optimization 
181 
 
MEO  satellites and their inter-connection
Broadband GSLs 
of MEO satellites
Satellites between the
”mirrored” backbone
dynamic-connection
of LEO satellites to the 
network backbone
 
Fig. 1. LEO satellites in the mirrored network backbone 
5) 
The ISLs can be always maintained between the LEO and MEO satellite lay-
ers. The ISLs in LEO layer (if any) should turn off when any of its connected 
LEO satellites enters the polar area. ISLs in MEO layer are functioning all the 
time. 
In such a network, the MEO  satellites and ground stations do not need to keep 
and maintain the network topology of the LEO  satellites. Instead of forming a 
strict  hierarchy, the mirrored routing structure only guarantee the network backbone 
that consists of the ground stations and MEO satellites. This is because of the follow-
ing reasons: 
 
 
− The  number of LEO  satellites is unpredictable. In the future, more 
and more small and micro LEO satellites are projected to be launched by 
many organizations such as industry and research groups, as well as uni-
versities. It is unrealistic to maintain their information. 
− The LEO satellites have shorter life time due to their design and mis-
sion purposes. Therefore, the LEO satellites are much more dynamic 
than the MEO stationary satellites. The network topology is also fast 
changing. 
− The LEO satellites have extreme various power and communication ca-
pacities. Consequently, even when the LEO satellite is at a predeﬁned 
position that can be detected by the network backbone, it is still unclear 
if the connection can be established. 
− The  LEO  satellites are not designed to be operational during the 
whole mission. For instance, they may shutdown to reserve power. This 
type of self controlled behavior has great impact on network topology. 
Such behaviors, however, cannot be predicted by ground stations or MEO 
satellites that, we assume, are always operational. 
3.3   Cross-Layer Information 
We propose an architecture that  uses cross-layer information to optimize the  
performance of the routing protocol and reduces the overhead caused by the routing. 

182 
Z. Chang and G. Gaydadjiev 
 
An integrated MAC/PHY  layer that provides more accurate and adequate informa-
tion to other cross-layer optimizations [6] is used to provide such low level informa-
tion that reﬂect the real-time wireless link situation. The proposed MRCL then use 
this information at  both the LEO  and MEO satellite layers. 
The cross-layer designs have potential risk when interact  with each other due to 
reasons such as shared information and adaptation loops [7]. In order to prevent such 
problems, we use the infrastructure for cross-layer design interaction proposed in [6] to 
ensure that the optimizations are loop-free, and behave correctly according to their 
designs. 
In the integrated Mac/PHY layer of the above proposal, share communication chan-
nels are established among the data link control layer (DLC),  MAC layer and physi-
cal layer. Therefore information such as BER and SN R  is available to upper layers. 
In order to simplify the information to upper layers, we deﬁne a normalized variable 
called Ranking of Link (Rl ) to rank wireless link quality of all ISLs in terms of BER 
and SN R, and collision possibility Pcollision  of outgoing packages from this node. 
Higher Rl  value indicates more network congestion or package loss. 
Rl  is calculated using two probability functions: 
1) 
1) error probability Pe   as a function of BER and SN R,  and 
2) 
2) collision possibility Pc   of outgoing packets from this node. 
A higher Rs  value indicates more network congestion or package loss (1). 
 
Rl = f (Pe , Pc ), Rl  ∈ (0, 1]                                              (1)  
The error related to noise and collision are equally important indications of link qual-
ity. Therefore the Rl   is calculated as the weighted sum of probability of error Pe   
and collision Pc   (2). For simpliﬁcation, we use (3) in our simulation. 
Rl = We   × Pe  + Wc   × Pc                                                                 (2) 
We   = Wc   = 0.5                                                  (3)  
A more careful selection of the two parameters may improve the accuracy of Rl . But 
as stated earlier, this accuracy does not inﬂuence the performance of the proposed 
mechanisms. In order to rank multiple available routes according to the link quality, we 
add a variable S ∈ (0, 1] in the routing cache/table to indicate the stability of the 
route as shown in the algorithm below. The value of S equals to 1 when the route is 
most stable. The value of S  decreases when the route becomes less stable. S  is up-
dated 50 times during the time when the satellite travels between the two polar re-
gions. The value of S  is calculated according to Rl   and the availability of the 
route. If the satellite passes the polar region and starts moving in another direction, S 
is reset to 1 and the calculation starts over. The calculation of S is shown in the routing 
construction at LEO and MEO layers. 

 
Mirror Routing for  Satellite Networks with Cross-Layer Optimization 
183 
 
3.4   High Level  Routing Policy 
The packets in the mirrored satellite network are processed and forwarded individu-
ally in every satellite on their paths. The routing decisions are stored in routing 
caches/tables onboard the satellites. These tables must be updated to reﬂect the 
changes in the network topology and in the traffic load carried by the network. The fol-
lowing issues were considered when designing the MRCL. 
 
− Computational Complexity: The satellite network of both LEO and MEO sat-
ellites consists of a large number of nodes. The periodic routing table calcu-
lations are performed  in the satellite network and require high processing 
power in a power limited environment especially for LEO satellites. To cope 
with this problem, we develop the LEO layer routing based on Dynamic Source 
Routing (DSR) for the following reasons: (i)DSR is on-demand routing that  
does not use periodic messages to update the routing information. Conse-
quently, it consumes less bandwidth and energy than table driven (proactive) 
routing protocols. According to [8], DSR has the smaller overhead than other 
protocols when pause time is 0s. (ii) DSR records the complete route from 
source to destination. Therefore, all the intermediate links are known to the 
source. The source can optimize the route using the links’ information. (iii) The 
intermediate nodes also utilize the route cache information efficiently to reduce 
the control overhead. (iv) DSR  does not maintain a routing table and conse-
quently needs less memory space. A simple ID instead of full IP  address can be 
used in such networks. Both the limited hop-count and the simple ID reduce the 
overhead in packets. Thus, the main disadvantage of the DSR is avoided. 
− Communication Overhead: In order to reﬂect the current condition of the satel-
lite network to the routing decisions, the up-to-date link delays must be used 
while calculating the routing tables. The  collection of the delay measure-
ments puts additional communication load on the satellite network. In our pro-
posal, LEO satellites reactively construct the routing table to save the computa-
tion and communication resources onboard. The MEO satellites with much higher 
capacity perform proactive routing table construction. 
− Delay and hop count assumption: The measured link delays used in MRCL in-
clude the propagation and processing delays. Although the propagation delay is a 
major part of the link delays, the processing and queuing delays can become lar-
ger than the propagation delay on the congested links. Further more, the LEO 
satellites have much less computation and communication capacities than the 
MEO satellites due to their smaller size and shorter life time. Therefore, we as-
sume that after the packet is relayed among the LEO satellites for N  hops, the 
total end-to-end delay is longer than relaying with MEO links because of the 
processing delay and retransmission on the LEO satellites. The N  is deﬁned as 
hop count limitation in LEO  layer. When multiple LEO  and MEO layer 
routes are available, the route with MEO satellites should be selected if hop 
count of the LEO route exceeds N , even if the propagation delay of the LEO 
routing is smaller. 

184 
Z. Chang and G. Gaydadjiev 
 
3.5   Routing Table Calculation in  LEO Satellites 
There are the following cases concerning the relative quality of the ISL between two 
LEO satellites and their ISL to MEO satellites and ground stations. 
 
− The ISL between the LEO satellites is good enough in terms of BER  
and stability. 
− The ISL between the LEO satellites is good in terms of BER  but is go-
ing to vanished due to fast changing relative position of the two satellites. 
− The ISL between the LEO satellites is not good enough to establish di-
rect communication. Route can be found using either GSL or ISL to 
MEO satellites. 
− The  connection between two LEO  satellites cannot be established. 
This means at least one of the LEO satellites is not connected to the 
backbone. 
 
At the LEO layer, the satellites try to use the MEO satellites to route if the the desti-
nation is not accessible in N  hops. If a bigger N  is selected, the LEO satellites pre-
fer to use ISL at the LEO layer. We use the hop count limitation instead of constructing 
complete LEO satellite groups like MLSR. This is because the topology at the LEO 
layer changes very quickly so that the grouping maintenance consumes a lot of re-
sources. If there is no packets being delivered, the computation is useless. We prefer 
more reactive fashion to determine whether to use MEO layer route or not, instead of 
proactive calculation like grouping in MLSR. In reality, if we consider the coverage 
difference between the LEO and MEO satellites and the delay caused by packet relay, 
we can ﬁnd out that the N is a small number. This is because the delay caused by the 
queues of the LEO satellites is much longer than the propagation delay between MEO 
satellite and LEO satellite or GS if hop count N  is too big. The following algorithm 
is used at the LEO layer to calculate the routing table: 
 
Algorithm to calculate  stability  variable S and route  construction in LEO 
 
FOR  (each found   route i ) 
       S_{i}  = S_{i} / (1  + sum of  all the R_{l} 
of   LEO  ISLs  on  the  route) 
ENDFOR 
 
IF  (route is only found   in LEO  layer) 
      use the route in LEO  layer  with the  highest  S  
ELSEIF (route is found   in LEO  layer  and  MEO  layer) 
            IF (hop   count > $N$) 
                 use   the MEO  layer  route with the highest S 
              ELSE 
                 use   the LEO  layer  route with the highest S 
            ENDIF 
ELSEIF (route is found   only in MEO  layer) 
    use   MEO  layer route with the  highest S  
ENDIF 

 
Mirror Routing for  Satellite Networks with Cross-Layer Optimization 
185 
 
3.6   Routing Table Calculation in  MEO Satellites 
In the route discovery phase, the ISLs between LEO  satellites and ISLs between 
LEO and MEO satellites are both ranked according to the Rl  algorithm. The differ-
ence is that the LEO ISLs calculation is performed every time when route discovery 
is required. The ranking of ISLs between the MEO satellites are only calculated peri-
odically because the relative positions among MEO satellites change much slower than 
those among LEO satellites. We assume that in the routing discovery phase, the 
route involves least ISL between two LEO satellites should be selected if the link 
quality ranking is the same. This is because the ISLs between LEO satellites are 
much less stable than ISLs between MEO satellites due to the rapid movements of 
LEO satellites. The following algorithm is used at the MEO layer to construct rout-
ing table according to the stability variable. 
Algorithm to calculate  stability  variable S and route  construction in MEO 
 
FOR  (each observation  time) 
IF  (if route is available) 
                  S  = S / (1  + sum of  all the  
              R_{l}   of  MEO  ISLs  on  the  route)  
ELSE (route is broken) 
                     S  = S/2 
         ENDIF 
IF  (ISL   is between LEO  satellites) 
             select  the link with highest S 
ELSE (ISL   is between LEO  and  MEO/GEO  satellites) 
                  select  the route with least  LEO  ISLs 
            IF  (more  than one  route are  selected)  
                  select  the route with  highest S 
               ENDIF  
ENDIF 
ENDIF ENDFOR 
 
The route construction is reactive on the LEO layer, and proactive on the MEO 
layer. This approach reduces the unnecessary overhead caused by fast change topol-
ogy of LEO satellites, and still beneﬁts from the much reliable MEO satellite routes. 
4   Validation and Results 
In order to validate our cross-layer optimizations, we implement the Mirrored routing 
with cross-layer optimizations in the Network Simulator 2 (ns-2) version 2.28 [9]. Our 
simulation is based on the ns-2 satellite package provided by [10]. In order to validate our 
cross-layer optimizations, the following improvements are made to the satellite package: 
 
– The energy model is introduced to simulate the satellite’s behavior without 
the energy source (in the shadow of the earth);  
– 802.11 MAC like collision is introduced to evaluate the probability of  
collision; 

186 
Z. Chang and G. Gaydadjiev 
 
– The success receipt of package is calculated using probability of error, which 
is a function of distance; 
– The satellite package and the DSR package in ns-2 are modiﬁed in order to 
replace the centralized routing with MRCL.  
We use the following conﬁguration for our study: 
1) 
LEO satellites: 10 to 50 LEO satellites on random polar orbits (altitude 500-
800km) within 5 degrees deviation with random start elevation degree (based on lon-
gitude 4.0 E);  GSL from 500kbps to 2Mbps for each LEO satellite; ISL from 
1Mbps to 2Mbps for LEO satellites; both symmetric links. 
2) 
MEO satellites: 10 MEO satellite nodes as Intermediate Circular Orbit (ICO) [11], 
two orthogonal planes at an altitude of 10,390 kilometers, 5 satellites per orbit; 2  
intra-orbit ISLs, 2 inter-orbit ISLs, global coverage; GSL and ISL are both set to 
100Mbps to simulate unlimited capacity.  
3) 
Three Ground stations: A (in Delft 51.9792 N, 4.375 E);  B  (New York 
40.30N, 73.24W); C (Beijing 39.92N, 116.46E). Data Sources: 10 CBR on 
UDP simulates the realtime data from GS A to B and from GS B to C. 20 
FTP  on TCP simulates non-realtime data from satellites to GS; duration: 1 day 
(86400s). 
We use the centralized routing calculation to simulate the routing based on location 
information without CL optimization. The LEO satellites choose the closest GS or 
MEO satellite to establish communication link if necessary. Then we compare the 
result with the proposed MRCL. 
Figure 2 demonstrates the end-to-end packet delay with different hop limitation. 
The MRCL always has less delay than shortest path ﬁrst based routing without CL  
optimization (WO CL).  When there are few LEO  satellites (15 satellites), which 
means few ISL  in the LEO  layer, the impact of hop count limitation is neglectful. 
When the number of LEO  satellites increases (20-35 satellites), more hop in the 
LEO layer (N  = 3) provides better performance. This is because the LEO satel-
lites form several isolated groups. Consequently, delay is decreased by allowing more 
hops inside the group. When the number of LEO satellites continues to increase (more 
than 40), the LEO satellites start to form bigger groups and eventually one big net-
work that includes all LEO satellites. In this case, the payload signiﬁcantly increases 
because more routes are available among the LEO satellites. The queue delay domi-
nates the delay in the LEO layer. Therefore, by encouraging the LEO satellites to use 
MEO satellites (N  = 1), the delay is reduced. Also, for the same reason we predict 
that  theperformance of even bigger N  would be closer to shortest path ﬁrst scenario. 
Figure 3 shows the throughput comparison. The performance of N  = 3 is almost 
equivalent to that  of the shortest path ﬁrst, which means most of the  packets are 
already dropped within the ﬁrst three hops. By using more MEO satellites that have 
much higher bandwidth and better link stability (N  = 1), the packet loss is 
signiﬁcantly reduced. The reason why the performances  are more or less the same 
when few LEO satellites are in the network (10 to 20)is that the LEO satellites are 
isolated and there are few routes available within the LEO layer. Consequently the 
packets are forwarded by the MEO satellites no matter the value of N . 

 
Mirror Routing for  Satellite Networks with Cross-Layer Optimization 
187 
 
WO CL
    MRCL (N=1)
MRCL (N=2)
End-to-end delay (ms)
400
380
360
340
320
300
280
260
240
220
200
MRCL (N=3)
10 15 20 25 30 35 40 45 50
Number of LEO satellites
 
Fig. 2. End-to-end delay with different hop limitation N 
Packet loss probability (%) 
6
WO CL
5
MRCL (N=1) 
MRCL (N=2)
4
MRCL (N=3)
3
2
1
0
10 15 20 25 30 35 40 45 50
Number of LEO satellites
 
Fig. 3. Packet loss probability with different hop limitation N 
Figure 4 shows the packet overhead in term of bytes in packets introduced by the 
MRCL.  There are three kinds of packet overheads introduced by the MRCL:  over-
head caused by cross-layer optimization, overhead caused by the routing table con-
struction and overhead in packets to carry route information (because of DSR like 
routing in LEO layer). The results show that our proposal always has less packet 
overhead than centralized shortest path ﬁrst. When the hop count limitation N  in-
creases, the packet is allowed to stay in the LEO layer for longer time. This means the 
packet header must contains the complete route information longer just like DSR. 
Therefore, the packet overhead is also higher. Although we cannot compare the packet 
overhead between our proposal and the MLSR due to lack of information, it is clear 
that the MLSR is proactive while MRCL is reactive on the LEO layer. The packet 
overhead of proactive routing is generally much higher than reactive routing because of 
periodical broadcasting of routing information and route maintenance even there is 
no traffic in the network. 
 
 

188 
Z. Chang and G. Gaydadjiev 
 
MRCL (N=1)
MRCL (N=2)
MRCL (N=3)
Packet Overhead %
10
9
WO CL
8
7
6
5
4
3
2
1
0
10 15 20 25 30 35 40 45 50
Number of LEO satellites
 
Fig. 4. Packet overhead with different hop limitation N 
5   Conclusion and Future Work 
The MRCL provided a simple solution for dynamic interconnection of LEO satellites by 
using both ground station and MEO satellites as the network backbone. Compared with 
MLSR, the proposed MRCL introduced less overhead because no calculation of 
grouping or summary links is required. The MRCL also considered the real link quality 
by using the cross-layer information from MAC/PHY layers instead of the geographic 
information. The MRCL signiﬁcantly reduced packet loss when a proper hop limita-
tion was selected. Furthermore, the MRCL employed reactive routing at the LEO layer 
and proactive routing at the MEO layer, which reduced the overhead caused by fast 
changing topology at  LEO layer and beneﬁted from the stable routes on the MEO 
layer. Consequently, in a more realistic scenario as shown by the simulation results, 
the overhead was reduced because no useless routing request is sent out during the dis-
cover phase. In the future, we will continue designing satellite routing protocols us-
ing CL optimizations. We will focus on satellite architecture without any constellation 
at all to establish a real ad hoc networking for future satellite applications. 
Acknowledgment 
This  work is supported by the Dutch Government as part of the decree on subsi-
dies for investments in the knowledge infrastructure (Bsik) program. This work is 
done within the Micro Satellite (MISAT)  project. 
References 
1. Akyildiz, I., Ekici, E., Bender, M.: Mlsr: a novel routing algorithm for multi-layered satel-
lite ip networks. IEEE/ACM Transactions on Networking 10(3) (June 2002) 
2. Kawadia, V., Kumar, P.: A cautionary perspective on cross layer design. IEEE Wireless 
Commun. 12(1), 3–11 (2005) 
3. Chang, H.S., et al.: Performance comparison of optimal routing and dynamic routing in 
low-earth orbit satellite networks. In: VTC 1996, Atlanta, GA (1996) 

 
Mirror Routing for  Satellite Networks with Cross-Layer Optimization 
189 
 
4. Gragopoulos, I., Papapetrou, E., Pavlidou, F.: Performance study of adaptive routing algo-
rithms for leo satellite constellations under self-similar and poisson traffic. Space Commu-
nication, 16 
5. Yuan, Z., Zhang, J., Liu, Z.: Routing in leo/meo double-layered satellite networks. In: In-
ternational Conference on Wireless Communications, Networking and Mobile Computing, 
WiCOM 2006 (2006) 
6. Chang, Z., Gaydadjiev, G.N., Vassiliadis, S.: Infrastructure for cross-layer designs interac-
tion. In: The 16th IEEE International Conference on Computer Communications and Net-
works (IC3N), August 2007, pp. 19–25 (2007) 
7. Srivastava, V., Motani, M.: The road ahead for cross-layer design. In: Proceedings of 2005 
2nd International Conference on Broadband Networks, pp. 551–556. IEEE, Los Alamitos 
(2005) 
8. Broch, J., Maltz, D.A., Johnson, D.B., Hu, Y.-C., Jetcheva, J.: A performance comparison 
of multi-hop wireless ad hoc network routing protocols. In: Mobile Computing and Net-
working (MobiCom), pp. 85–97 (1998) 
9. ns 2, The network simulator version 2, http://www.isi.edu/nsnam/ns/ 
10. Henderson, T.R., Katz, R.H.: Network simulation for leo satellite networks. In: American 
Institute of Aeronautics and Astronautics (2000) 
11. Intermediate circular orbit. ICO Global Communications, http://www.ico.com 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 190–201, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Channels Intersection Weight Based Routing in 
Cognitive Radio Networks* 
Lu Wang and Wei Wu 
School of Computer Science and Engineering 
Beihang University Beijing, China  
wanglu@vrlab.buaa.edu.cn 
Abstract. In  multi-hop cognitive  radio  networks,  the communication  links 
among the cognitive nodes fail easily with the  dynamic appearances of the 
licensed users. We propose an approach to compute the weight of the available 
channels intersections. The  cognitive nodes compare the weights of different 
routing paths and update the route-table to reduce the re-routing times caused 
by the network changing, in order to reduce packet loss rate in cognitive radio 
networks. Simulation results show that in a multi-hop cognitive radio network 
with  frequent  change  of  the  licensed  users  and  uneven  distribution  of  the 
available  spectrum, our  protocol provides less packet loss rate in the commu-
nication channels and proper overhead in the control channel. 
Keywords: Cognitive Radio Network, Routing Protocol, Spectrum Assignment, 
Packet Loss Rate, Overhead. 
1   Introduction 
With the rapid development of wireless mobile applications, the limited spectrum 
resources are unable to meet the growing demand for spectrum. Even though the 
number of available frequency bands is decreasing, considerable portions of the fre-
quency spectrum are not fully utilized at a given place or at some given time [1]. 
The cognitive  radio technology [2] allows  secondary users to  sense, identify and 
intelligently access the unoccupied spectrum, thereby enhancing the utilization of the 
wireless spectrum. 
In cognitive radio networks, each cognitive node can be dynamic, real-time 
sensing of unused spectrum and form a collection of the Spectrum Opportunity 
(SOP) [3]. The cognitive nodes will communicate in an appropriate frequency 
channel selected from SOP. In such a multi-hop cognitive radio network, spectrum 
distributes evenly, which means that each SOP may not be the same and will 
change at any time. Routing in such a network is much more difficult. It’s a very 
                                                           
* This work is supported by the Doctoral Program Foundation of Education Minis-
try of China (No. 200800060018), and the Aviation Science Fund (No. 
2009ZD51038). 

 
Channels Intersection Weight Based Routing in Cognitive Radio Networks 
191 
 
important subject in the field of cognitive radio research to route in such networks 
with reducing the packet loss rate  nd overhead as low as possible so as to enhance 
the network performance. 
Q. Wang and H. Zheng first proposed a method which combines MAC layer and 
network layer. They study how the interaction of routing and spectrum management is 
related. Such a jointing improves the network performance [4]. Their research opens 
up a new way of thinking in the cognitive radio network routing study. 
S. Krishnamurthy et al. introduced a routing algorithm [5] which collects the node 
location information using GPS or other devices to obtain the network topology. The 
information helps the cognitive nodes in the routing process to find the appropriate 
routing path. Furthermore, they considered a distributed algorithm to find neighbor 
nodes based on TDMA in the cognitive radio network [6], [7], and described how the 
cognitive nodes configure themselves on the basis of above all [8], [9]. S. Krishna-
murthy and others combine the effective communication frequency band selection  in 
MAC layer  and exact routing in network layer.  Nevertheless in  their method the 
ancillary equipments, such as GPS are necessary. The cognitive nodes have to obtain 
the radio network topology, and the routing process is closely related to the node lo-
cation information. If the cognitive node hasn’t the positioning auxiliary equipment in 
the network, it will not complete the routing process well. 
G. Cheng, W. Liu and etc. studied the time delay based routing algorithm in cogni-
tive radio networks. The cognitive nodes compute the delay caused by band switching,  
backoff,  and  existing data  flows.  The routing algorithm  they proposed carries out 
the spectrum allocation and routing selection according to the delay information [10], 
[11], [12]. This routing algorithm takes the data transmission delay as the only metric. 
However in cognitive radio networks, the nodes will re-route due to the frequent 
routing failures caused by the dynamic changes of spectrum. It will affect the perform-
ance of the data transmission links and the whole network seriously. The study of 
them reduces the transmission delay for the data packets, in spite of the performance 
influence. 
In this paper we present an on-demand routing method in multi-hop cognitive radio 
networks. The cognitive nodes compute the weight of the public available channels 
between  the neighbor nodes to complete the routing approach, combing with the 
“Shortest Path First”. We take the packet loss rate and the overhead as the measure to 
derive our method. The simulations show that the data communication link using this 
routing method has a better transmission performance when available spectrum is 
changing frequently. 
The rest of the paper is organized as follows. Section 2 describes the basis of our 
study, including the notations, the assumptions and the routing problem description. In 
Section 3 we formulate the effective time of the transmission link with the available 
channels, and introduce the process of our routing method. The simulations  and 
analysis of the results are shown in Section 4 . In Section 5  we summarize the 
accomplished work and look towards the next stage of study. 

192 
L. Wang and W. Wu 
 
2   System Model 
2.1   Notations 
The various notations used in the paper are as follows. 
N 
The number of nodes in the cognitive radio network 
 
M 
The total number of available channels in the cognitive radio network 
 
L 
The number of licensed users in the cognitive radio network  
 
n 
The number of nodes in the transmission link 
 
Ai  
The node of transmission link; 0<=i<n 
 
E 
{ A0, A1, ……, Ai, ……, An-1}; the transmission link, A0  is the source  
 
node and An is the destination. 
 
Ui  
The set of available sending channels of node Ai 
Vi  
The set of available receiving channels of node Ai 
 
Ki  
The number  of public available transmission  channels between  adjacent  
 
nodes Ai and Ai+1; the element number of Ui∩Ｖ i+1 
U 
 
The sending channels set of the cognitive radio network; 
 
 
V 
The receiving channels set of the cognitive radio network,
 
 
pi,j 
The probability that the transmission link between Ai and Aj is available 
 
T 
The total transmission time 
 
t  
The expectation of valid transmission time; the expectation of time that the  
 
transmission link not fail 
 
Ii 
{A0, An-1, Ki-1, Ui-1, Wi  }; an item in the route-table of Ai in the link from  
 
A0 to An-1 
 
Wi 
The weight of the available channels intersection from A0   to Aj   along  
 
the link; Wi = Wi-1+α+Ki*β; W0=0 
 
α 
The coefficient of hops count in intersection weight computation 
 
β 
The coefficient of available channels count in intersection weight  
 
computation 
 
The considered multi-hop cognitive radio network consists of the nodes with the 
cognitive function. The multi-hop data communication link is made up by the 
source node, the destination node, as well as the intermediate nodes forwarding data 
in the path from the source node to the destination. 

 
Channels Intersection Weight Based Routing in Cognitive Radio Networks 
193 
 
 
Fig. 1. Four cognitive nodes in a transmission link 
2.2   Assumptions 
We assume that each cognitive node is equipped with a traditional wireless interface in 
addition to the cognitive radio transceiver. The cognitive radio transceiver is used to 
transmit communication data. The nodes form a common control channel with the 
traditional  wireless  interface.  Protocol  messages  such  as routing  consultations  are 
transmitted in the control channel. SOP information is collected timely and shared 
between MAC and network layer [10]. The conflicting nodes which want to work in 
the same channel at the same time will use IEEE 802.11 MAC protocol to access the 
channel [13]. 
We assume that Ui  and Vi  of Ai  are independent. The cognitive node Ai  decides 
to use which of Ki  channels to transmit data by consulting with Ai+1. 
The cognitive node will select some channels in one of which to communicate with 
other nodes after it completes routing consultation on the control channel. In order to 
simplify the problem, we assume that the cognitive node take a channel to be readily 
available if there are no licensed users working on it. In fact, multiple data flows 
will be used  since there may be a number  of communication  links in  cognitive 
radio networks. The cognitive node have a chance to become an intermediate forward-
ing one to switch on more than one communication channels, thus every data flow is 
likely to be delayed. Every data flow will wait the certain channel available while it 
forwarded by the intermediate node in a TDMA-based cognitive radio network. We 
only study the communication case in a single data flow. It will be our main re-
search content in the next stage to work over the complex situation generated by a 
number data flows working in parallel. 
2.3   Problem Description 
The most important feature of cognitive radio networks is the existence of li-
censed users whose dynamic appearance will influence the cognitive nodes’ behaviors. 
In a cognitive radio network, the cognitive nodes must stop communicating on the 
channel if they apperceive the licensed user working on it. In usual routing algo-
rithm of cognitive radio networks, the routing procedure will not only determine the 

194 
L. Wang and W. Wu 
 
nodes along the transmission link, but also identify the channels used by every adjacent 
nodes. Once a channel can not be used, the link using the channel will fail. The re-
routing procedure is required. Such condition will cause a large number of packets 
loss and management overhead will increase. All of these will impact the network 
performance significantly. 
Indeed the communicating nodes need not route again while there are other avail-
able channels which is not occupied by the licensed users. The cognitive nodes may 
switch from the busy channel to the free one and continue transmitting data with the 
neighbor node. If and only if the adjacent nodes have no public available channels, the 
communication link will be rebuilt. 
In  the  cognitive  network,  the licensed  users  vary dynamically and  the public 
available channels change accordingly. The most important issue which the routing 
algorithm would solve is how to reduce packet loss rate remarkably with the cost of 
proper overhead. 
3   Analysis and Implementation 
3.1   Valid Transmission Time 
We consider a multi-hop communication link in the cognitive radio network. The 
source node is A0, and the destination node is An-1. 
The  cognitive  nodes detect  the licensed  user  in  a  fixed frequency.  When  the 
cognitive node finds one or more licensed nodes are working on a certain channel, 
it will drop the channel until the next detection. The cognitive node then communi-
cates with its neighbor on the vacant channels which are not occupied by any licensed 
users. We suppose the licensed users work on some certain channels at irregular 
intervals, and each licensed user has equal probability of using each certain channel. 
The probability pi,j+1  shows that the communication link between Ai   and Ai+1  is 
available, namely there is at least one communication channel. 
, 
i 
pi,i+1  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
1      
 
                    
 
Ki 
(
)!
i
K
L
L
L
i
L
i
i
i
L
L
P
L
K
K
L
K
K
        
(1)
 
As shown in (1), Pi,i+1 is a monotone decreasing function with respect to Ki. As we 
know, Ki<=M. Thus when Ki  equals to M, Pi,i+1 gets the maximum 
 
 
 
 
 
 
 
 
Max(pi,i+1
  
 
 
 
 
 
 
1 
 
 
 
 
(
)!
L
L
L
L
M
M
         
(2)
 
The multi-hop communication link from A0 to An-1 consists of n nodes. If the whole 
link is available, every section link from one node to next must be valid.  Thus 
the probability that the communication is usable is 

 
Channels Intersection Weight Based Routing in Cognitive Radio Networks 
195 
 
p
1
2
0
n
i
p
                                                                 
(3)
 
If there are no available channels between the cognitive node Ai    and Ai+1,  the 
communication link between them will fail. Therefore the source node A0   will 
route again or fix the local link to the destination node. We compute the expectation of 
valid transmission time is 
t
1                                                     (4) 
We substitute (3) into (4). The result is 
t
2
0
n
i
p
                                 
(5)
 
This shows that t is a monotone increasing function about pi,i+1. When every pi,i+1 
gets the maximum, t gets the maximum. At that time, Ki  equals to M. 
 
 
 
 
 
 
Max(t
 
 
 
 
 
 
 
 
T 
 
 
L M 
!
L n
L
L
T
L
M
M
                      
(6)
 
It can be seen that t is the most if the available communication channels are the most. 
At that time the available communication channels set includes all the available chan-
nels in the cognitive radio network. In fact, the channels which adjacent nodes can use 
are part of the whole channels set, due to the uneven distribution of available spectrum 
and the licensed users’ existence. 
As said before, t is the expectation of valid transmission time. If the value of t is 
less, the failures of transmission link are more. So the packet loss rate will increase. To 
reduce  the  packet  loss  rate,  we  need  improve  the  valid  transmission  time.  The 
cognitive nodes will find out a communication link from A0    to An-1, in which  
the intermediate forwarding nodes are less and the available channels in intersection of 
the every adjacent two nodes are more. 
In the sequel, we will introduce a protocol named as CWRP (Channels Intersection 
Weight On-demand Routing Protocol). 
3.2   Routing Protocol 
Routing in a multi-hop cognitive radio network is made up of two parts, namely de-
termining the data transmission path and fixing on the communication channels. In 
the first stage, the routing path makes sure of a series of nodes along the link from A0 
to An-1. Every next node is in the range of communication for the last one. There are 
some usable channels between every adjacent two nodes. In the second stage, the two 
adjacent nodes consult with each other to determine the channel which will be used. In 
future we will describe the consultation method in a separate article. And in this paper 
we focus on the routing path in the first stage. 

196 
L. Wang and W. Wu 
 
The routing protocol presented is an on-demand routing method. The basic strategy 
of it follows AODV [14]. 
In addition, CWRP has a number of features else. As mentioned in section 3 part 
A), we want to find a path by CWRP, in which the intermediate forwarding nodes are 
less and the available channels in intersection of every adjacent two nodes are more. So 
we use the weight to measure each possible path. Every intermediate node receiving 
RREQ will compute the weight of the forward path from the source node to itself ac-
cording the available channels sets along the path. It will compare the value with the 
weight of stored path in the reverse route-table and decide whether update the reverse 
route-table item. When RREP is transmitted, the route-table items are concerned. 
During the concrete implementation we do not distinguish the available sending 
channels set and the available receiving channels one. Every cognitive node Ai has a 
set of available transmission channels, said with Ui. 
The routing algorithm is as follow: 
1) 
The source node encapsulates Q in the routing request packet where Q = { 
N0，Nn，K，Ui，W}，and K＝0，Ui＝U0，W＝null. At that time i=0, and 
Ni is N0; 
2) 
The node Ni broadcasts RREQ on the control channel;  
 
3) 
All the nodes who receive RREQ packet decapsulate RREQ and com-
pute the reverse routing transmission channels intersection weight. 
Taking Ni+1  as an example, the computation procedure is: 
IF Ni+1   = Ni     OR Ui     ∩ Ui+1   = null THEN 
drop RREQ 
exit 
END IF 
K←K+1 
W←W∪{the num of set elements in Ui     ∩  Ui+1}  
Weight←the num of set elements which in W 
lookup route-table, find the reverse route item I={Nn, 
N0, Kitem, Uitem, Witem, Weightitem} 
IF I=null OR K<Kitem    OR (K<=Kitem+ α  AND Weight<Weightitem)  
THEN 
update route-table item with I={ Nn, N0, K, Ui∩Ui+1, 
W, Weight}  
ELSE 
drop RREQ 
exit 
END IF 
IF Ni+1=Nn    THEN 
goto step d) 
END IF  

 
Channels Intersection Weight Based Routing in Cognitive Radio Networks 
197 
 
Ui    ←Ui+1  
i←i+1 
goto step b) 
 
4) 
The destination node encapsulates P in the routing reply packet where P = 
{Nn，N0，K，Ui，W}，and K＝0，Ui＝Un，W＝null. At that time i=0, 
and Ni is Nn; 
5) 
The node Ni broadcasts RREP on the control channel; 
6) 
All the nodes who receive RREP packet decapsulate RREP and compute the 
reverse routing transmission channels intersection weight. It is similar to c). 
7) 
At the end of routing process, the source node N0  receives the RREP packet.  
The communication link is built up. 
During the computation of the above routing process, we take the routing path as bet-
ter on which the channel intersection is smaller. 
In the computation, Wi   = Wi-1+α+Ki*β,  we use constants α and β. α is  the 
coefficient of hops count, and  β is the coefficient of available  channels count  in 
intersection weight computation. CWRP uses comprehensively two kinds of  policy, 
“shortest path first” and “available channels intersection most”. In usual routing proc-
ess, the policy “shortest path first" has priority always. It shows better performance in 
the network in which the nodes have only one communication channel and  have  
nothing to do more than determining the forwarding nodes along  the transmission  
link. On the other  hand the strategy “available channels intersection most” pays 
more attention on the usability of the channels. We can  choose different values of  
α according to different network  environments  and make the strategy “shortest 
path first” working suitably. In this way, we can get more alternative routing paths. 
The strategy “available channels intersection most” with different β values is used 
to choose a better transmission link with excellent comprehensive performance. 
Moreover, we will talk about  overhead. If a cognitive  node gives up using a 
channel, it will announce such information to its neighbors. If it is communicating with 
some other nodes in the channel, they will consult the vicarious channel in the control 
channel. It will produce some overhead in the control channel. 
4   Simulation and Analysis 
4.1   Simulation Environment 
We conduct experimental simulations in NS2 to quantify the performance of CWRP. The 
topology of our experimental cognitive radio network is a 6 * 6 grid, as shown in Figure 
2. The distance between two adjacent nodes is 250 m. Each node has 2~4 neighbor  
nodes in its wireless signal coverage. Such a network topology is fully connected, and 
there are many possible paths from the source node to the destination. The cognitive node 
chooses an appropriate path according to CWRP and forwards the data. 

198 
L. Wang and W. Wu 
 
 
Fig. 2. Cognitive radio network topology in simulations 
During the simulations, the system selects available channels from 6 channels to 
constitute SOP for every cognitive node randomly. In such scenarios, we evaluate the 
impact of the heterogeneity of spectrum’s distribution in the cognitive network. The 
licensed users appear in certain frequencies with the position and working frequency 
generated by the system randomly. 
The arguments used in the simulations are listed in Table 1. 
4.2   Simulation Results 
For comparison, we evaluate the performance of the typical routing schema, AODV, 
which can be used in the multi-channel network after being advanced. 
We start a CBR traffic from the source to the destination with the packets sized 100 
bytes at the rate of 64kb/s for 300 seconds. The performance of the two schemes upon 
the licensed users’ different appearance is evaluated then. We adjust the frequency of 
the licensed user’s change from 1 to 30 times every 300 seconds. In each change, the 
system selects the situation and channel randomly for the licensed user. The arguments 
will last until next change. In each situation, we repeat the simulation 20 times. The 
average result is shown in figure 3 and figure 4. 
In figure 3, the results show that with the increase of the licensed user change fre-
quency, the transmission link packet loss rate gradually increases and the destination 
node receives the data packets gradually decreased. But in comparison with AODV, 
the figure shows that the packet loss rate change of the communication link using 
CWRP is flatter. When the licensed user changes more than 15 times, the packet loss  
 

 
Channels Intersection Weight Based Routing in Cognitive Radio Networks 
199 
 
Table 1. Simulation arguments 
Parameter 
Value 
situation count 
41 
repeat times every 
situation 
 
20 
each simulation 
time 
 
300 seconds 
licensed user 
extent 
 
350 m 
packet size 
100 bytes 
Simulation 
Configuration 
transfer rate 
64 kb/s 
network topology 
6*6 grid 
Network 
Environment 
node count N 
36 
signal attenuation 
model 
two-ray ground 
path loss model 
available channels
[0, 6] 
Node 
Configuration 
receive energy 
threshold 
 
9.50808e-11 
α 
1 
Routing 
Algorithm 
β 
1 
 
rate of AODV link is twice which the CWRP link receives. When the licensed user 
changes 30 times nearly in 300 seconds, the packet loss rate of AODV is about 90% 
and the packet loss rate of CWRP is about 65%. 
In figure 4 with the licensed user’s changes increasing, the overhead packets of 
CWRP increase faster than AODV. But integration of the received data packets, the 
overhead packets are in the reasonable scope. 
The packet loss is lower and the overhead packets increase reasonably. CWRP 
shows better network transmission performance. 
 
Fig. 3. Cumulative packet loss rate with varying licensed user change frequency 

200 
L. Wang and W. Wu 
 
 
Fig. 4. Overhead packets and data packets with varying licensed user change frequency 
5   Conclusions 
We propose an on-demand protocol for routing with calculating the weight of available 
channels intersection in cognitive radio networks. The protocol takes account of the 
fact that the communication link fails constantly and the packet loss rate increases 
significantly.  Such  messy situation  is just  because  of  the frequency’s  distribution 
unbalance and the licensed user’s dynamic changes. Simulations show that the com-
munication link selected with CWRP has better performance in the multi-hop cogni-
tive radio network, through the licensed user change regularly. 
Basing on the research results that a single data flow is transmitted now, we will 
make a work on the intersecting flows. 
References 
1. FCC, Spectrum Policy Task Force Report, ET Docket No. 02-135 (November 2002) 
2. Joseph, M.: Cognitive radio: An integrated agent architecture for software-defined radio 
(2000) 
3. Chunsheng, X., Bo, X., Chien-chung, S.: A novel layered graph model for topology forma-
tion and routing in dynamic spectrum access networks. In: Proc. IEEE DySPAN 2005, 
Baltimore, USA, November 2005, pp. 308–317 (2005) 
4. Wang, Q., Zheng, H.: Route and Spectrum Selection in Dynamic Spectrum Networks. In: 
Proceedings of IEEE Consumer Communications and Network Conference, CNCC (2006) 
5. Krishnamurthy, S., Thoppian, M., Venkatesan, S., Prakash, R.: Control channel based 
MAC-layer configuration, Routing and situation awareness for cognitive radio networks. 
In: Proceedings of the IEEE Military Communications Conference (MILCOM) (October 
2005) 
6. Krishnamurthy, S., Chandrasekaran, R., Mittal, N., Venkatesan, S.: Brief Announcement: 
Synchronous Distributed Algorithms for Node Discovery and Configuration in Multi - 
channel Cognitive Radio Networks. In: Dolev, S. (ed.) DISC 2006. LNCS, vol. 4167, pp. 
572–574. Springer, Heidelberg (2006) 

 
Channels Intersection Weight Based Routing in Cognitive Radio Networks 
201 
 
7. Krishnamurthy, S., Mittal, N., Chandrasekaran, R., Venkatesan, S.: Neighbor Discovery in 
Multi-Receiver Cognitive Radio Networks. International Journal of Computers and Appli-
cations 31(1) (2009) 
8. Krishnamurthy, S., Thoppian, M., Kuppa, S., Venkatesan, S., Chandrasekaran, R., Mittal, 
N., Prakash, R.: Time-efficient layer-2 auto-configuration for cognitive radios. In: Pro-
ceedings of the 17th IASTED International Conference on Parallel and Distributed Com-
puting and Systems, Phoenix, Arizona, USA, November 2005, pp. 459–464 (2005) 
9. Krishnamurthy, S., Thoppian, M., Kuppa, S., Chandrasekaran, R., Mittal, N., Venkatesan, 
S., Prakash, R.: Time-efficient Distributed Layer-2 Auto-configuration for Cognitive Ra-
dio Networks. Computer Networks 52(4), 831–849 (2008) 
10. Cheng, G., Liu, W., Li, Y., Cheng, W.: Spectrum Aware On-demand Ruting in Cognitive 
Radio Networks. In: proceedings of IEEE DySPAN 2007 (2007) 
11. Cheng, G., Liu, W., Li, Y., Cheng, W.: Joint On-Demand Routing and Spectrum Assign-
ment in Cognitive Radio Networks. In: Proceedings of IEEE ICC 2007 (2007) 
12. Yang, Z., Cheng, G., Liu, W., Yuan, W., Cheng, W.: Local Coordination based Routing 
and Spectrum Assignment in Multi-hop Cognitive Radio Networks. Mobile Networks and 
Applications 13(1-2), 67–81 (2008) 
13. IEEE Standard for wireless LAN-medium access control and physical layer specification, 
P802.11 (1999) 
14. Perkins, C.E., Royer, E.M.: Ad hoc on-demand distance vector routing. In: Proceedings of 
IEEE Workshop on Mobile Computing Systems and Applications (WMCSA 1999),  
New Orleans, USA, pp. 90–100 (1999) 

A Comparison on MANETs’ Service Replication
Schemes: Interest versus Topology Prediction
Mohamed Hamdy1, Abdelouahid Derhab2, and Birgitta K¨onig-Ries1
1 Institute of Computer Science, Friedrich-Schiller-University Jena,
07743 Jena, Germany
{mohamed.hamdy,birgitta.koenig-ries}@uni-jena.de
2 Department of Computer Engineering, CERIST Center of Research,
16030 Algiers, Algeria
aderhab@mail.cerist.dz
Abstract. Mobile ad hoc networks (MANETs) are characterized by
high dynamics in particular with respect to the formation of network
partitions. The presence of unconnected partitions makes the deployed
services inaccessible to some network participants. Service replication is
employed as an approach to overcome this problem and to ensure higher
service availability. Several protocols and algorithms for service replica-
tion in MANETs have been proposed. Most of these approaches apply
topological analysis (like partitioning prediction) schemes to produce the
required replication decisions. One approach, SDP, the Service Distribu-
tion Protocol, bases its decisions on analyzing interest in the service, an
application layer concept. In this paper, we compare the performance of
approaches based on these two criteria. First, we analyze protocols based
on topology prediction and choose two typical representatives of this cat-
egory, namely PSRP and SSRP. We then compare SDP as the only can-
didate using service interest to these two approaches. This comparison
is based on an extensive set of simulation runs which are discussed in
detail in the paper.
1
Introduction
The challenging features of MANETs, especially the partitioning behavior, de-
crease the availability of network resources for their participants. Since sharing
resources and capabilities in a collaborative manner is vital in such networks,
ensuring higher availability of resources is very important as well. Service orien-
tation can provide a solution for supporting MANETs’ collaborative activities.
Resources of the network can be represented as services. These services can be
oﬀered to the other network participants. However, to ensure high service avail-
ability and accessibility, measures need to be taken to counteract the eﬀects
of network partitioning. Service replication across multiple nodes can help to
increase service availability. In general, replication can ensure higher resource
availability and is being applied in many systems and applications such as dis-
tributed databases and storage systems. Unfortunately, none of these replication
schemes can be applied directly to MANETs. Since ad hoc networks consist of
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 202–216, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

A Comparison on MANETs’ Service Replication Schemes
203
an ever changing set of distinct partitions, replication protocols need to take
this into account. In contrast to more classical settings, partitioning is more the
rule than the exception in these networks and should thus not hinder protocol
performance signiﬁcantly.
Over the last few years, replication protocols and algorithms have been de-
veloped to overcome the challenges of MANETs. Since the partitioning behavior
is the main actor that aﬀects service availability, most of these protocols focus
on obtaining a complete image of the current network status and try to pre-
dict the network partitioning behavior. In general, all of these protocols and
algorithms share a main feature: they query the lower network layer, e.g., the
routing components, to obtain information. Besides being expensive and time
consuming, these querying processes make the proposed protocols and algorithms
architecture-dependent.
The service distribution protocol (SDP) [1,2,3] has been proposed to overcome
this disadvantage of the other protocols. Its main contribution is to base the repli-
cation decision only on information from the application layer which is a lot cheaper
to obtain than information from the lower network layers. It uses information that
indicates the service popularity and does not need to predict the partitioning be-
havior. Based on the importance of a speciﬁc service, which is time varying, SDP’s
replication decision is computed shared among the service provider and its clients.
Moreover, when a service becomes unimportant for clients, it will be hibernated
(shut down). In this paper, we aim to compare the performance of protocols based
on topology prediction to that of SDP, to the best of our knowledge up to now
the only protocol that is based on application layer information, only. This pa-
per discusses the various concepts behind service replication in MANETs and how
the existing protocols and algorithms achieve these concepts. Moreover, a detailed
comparison among these protocols is shown. Finally, a case study that contains a
detailed simulation which shows the performance of SDP compared to two other
replication protocols, namely: (a) The pull-based service replication protocol for
mobile networks (PSRP) [4] and (b) A Self-stabilizing PSRP (SSRP) [5] as based
on predicting network partitioning, is presented.
The structure of the paper is as follows: In Section 2, a set of related proto-
cols and algorithms are presented. Basic concepts and criteria of these presented
protocols and algorithms are introduced and discussed in Section 3. A detailed
comparison among the presented protocols and algorithms is given in Section 4
in which the reasons for selecting the protocols PSRP and SSRP to be compared
with SDP in the next section of the case study are presented and discussed. A
case study that compares results of simulation on SDP to the protocols perfor-
mance of PSRP and SSRP is shown in Section 5. Finally, conclusions are stated
in Section 6.
2
Service Replication Protocols and Algorithms for
MANETs
In this section we introduce a set of notable service replication protocols for
MANETs. A brief description for each is presented.

204
M. Hamdy, A. Derhab, and B. K¨onig-Ries
Hauspie et al. [6] present a replication decision algorithm based on link evalu-
ation. It assumes presence of a suitable routing protocol that can be queried for
long routes in dense MANETs. The authors assume that not only the network
partitioning behavior is the main actor in the process of service replication but
also the long wireless routes, routes with several intermediate nodes, which af-
fect the quality of the delivered functionalities. So, partitioning behavior here is
widened to include the low quality links between nodes which may appear bro-
ken in terms of reliability, bandwidth, delay, besides lack of transmission power.
The algorithm periodically checks the existence of these long and critical routes
(between a speciﬁc server and a client node) and tries to deploy a replica at the
closest valid node to the client by employing a set of concepts like robustness
of paths and service path quality which are based on number of essential nodes
(essential for a path between two nodes to be valid) in the path. Also a thresh-
old length of path (in terms of number of hops) is used in order to compute the
robustness of path between two nodes. The robustness considers the number of
disjoint paths and their lengths.
Karen et al. [7] introduce a partition prediction service coverage algorithm
for MANETs. Once a partitioning behavior has been detected, it tries to push
a replica in the farthest node in the departing partition. In case of presence of
two or more servers in the same network partition the server with the eldest
ID is allowed to continue. A client should search for reliable providers in terms
of similar relative speeds. Besides, assuming the presence of a suitable routing
protocol and GPS, the authors assume the existence of group mobility patterns
in the network and use these patterns to predict the network partitioning. A set
of algorithms to perform the velocity analysis is proposed. A reference velocity
group mobility model (RVGM) is introduced in [7], which is an extension of the
reference point group mobility model (RPGM) model [8]. RVGM identiﬁes a
certain number of mobility groups with the references that deﬁne their mobility
behavior. The servers are supposed to know the location information of their
clients and apply a clustering prediction scheme to compute proactively the
future partitions. The replication process should be delayed as much as possible
to reduce the eﬀects of prediction errors. Clients should rank their servers and
pose their queries to the most stable server which is in the same mobility group
and with nearer velocity characteristics. Clients are asked to update their favorite
servers periodically.
Zheng et al. [9] provide a dynamic, adaptive replica allocation algorithm which
aims at minimizing the communication cost of the replication process. It com-
bines the two concepts of topological change analysis and frequency of requests
(read/write requests) to perform its replication processes. It allocates the pro-
duced replicas where it can minimize its access cost. It suggests a usage of GPS
or suitable routing protocol for the analysis of the topology. It assumes that
the replica nodes (providers) can exchange a write request (transactions) im-
mediately once they receive any transaction that changes the replica’s state.
The adaptive replica allocation algorithm for MANETs (ARAM) is introduced.
ARAM uses the collected requests of a replica neighbors (the neighbor set) to

A Comparison on MANETs’ Service Replication Schemes
205
dynamically evaluate the allocation scheme. ARAM makes periodically tests on
the collected request in a varying time interval (t). The shorter t intervals, the
more ability to deal with higher and frequent network status changes. Stable
neighbor is deﬁned as the node that has a stable path (all of the intermediate
links are available at speciﬁc time) to each node in its neighborhood set. ARAM
is extended to be ARAM-SN which extend the network status analysis to ﬁnd a
stable neighbors to host the replicas.
Dustdar et al. [10] introduce a web service replication protocol for mobile ser-
vices. It depends also on an analysis for topological changes and selects the best
node for hosting a new replica. The nodes are informed about the whole net-
work status through an installed replicator component. The proposed replication
mechanism requires: (a)The ”monitor” component which checks periodically for
each change of network status and spreads the required information about that.
The most powerful nodes (in terms of available resources) are selected to be the
monitors. ”Monitors are responsible for partitioning the nodes by their IDs into
groups and detecting changes into these groups”[10]. Moreover, they are able to
merge these groups into a new group. (b) The ”replica placement mechanism”
which keep track of the replica status and if it needs to be corrected. Each client
should ﬁnd a primary copy of the service for its requests based on the avail-
able resource. A related leader election based on the available information about
the mobile nodes available resources take place for any new replica allocation.
(c) For service synchronization, it assumes that background lazy updates for
the concurrent replicas will be suﬃcient. An internal database to keep track of
the global status of the network is required. The protocol suggests service hi-
bernation based on monitoring and analysis of the network status. (d) Service
invocation is done through a standard web service description protocol (WSDL).
A related WSDL-ﬁnder component is supposed to keep track about the changes
of the oﬀered services in the network and provide always valid WSDL ﬁles for
the clients.
Derhab et al.[4] propose a service replication protocol, PSRP, that can predict
network partitioning before its occurrence. The protocol constructs a Directed
Acyclic Graph (DAG), rooted at the server node. The protocol uses TORA [11]
partition detection mechanism that helps nodes to detect if they still have a
path to the server. To predict network partitioning, the protocol classiﬁes links
into strong and weak based on their residual lifetimes. A node that loses its last
strong outgoing link toward the server, executes a diﬀusion computation in order
to ﬁnd an alternative route composed only of strong links toward the server. If
no such a route is found, the node concludes that the server node is about to be
unreachable. In this case, it downloads the service and becomes a passive server.
If there are no servers in the new predicted partition, the passive server becomes
an active server. If there is more than one active server in a network partition,
a merging algorithm will be triggered. An enhanced version of PSRP, SSRP [5],
is proposed to work in a highly dynamic topology. When topology changes at
a high rate, multiple diﬀusion computations can be triggered concurrently. In
this case, PSRP chooses to stop the oldest computations in favor of the most

206
M. Hamdy, A. Derhab, and B. K¨onig-Ries
recent one, which means that PSRP restarts execution from scratch whenever
a new topological change occurs. If topology changes do not stop, PSRP will
not terminate. Thus, network partitioning might never be predicted or it is
predicted after the partitioning has already occurred. To ﬁx this shortcoming,
SSRP changes the criterion used by PSRP to order computations and propose
to stop newer computations in favor of the oldest one.
SDP, the service distribution protocol, is based on the interest information
only which can be provided simply from the application layer. This SDP feature
enables it to be architecture independent. Unlike the other approaches, SDP
needs no network status analysis or partition predication techniques. A replica
is hosted by clients once they show enough interest (in terms of requesting fre-
quency). Presence of a service-requesting behavior regarding a speciﬁc service
is assumed. SDP provides two server election modes [3], short and long election
mode, to be used during the leader election processes, i.e., to determine whether
and where to host or hibernate a replica. Mobile clients should ﬁnd the best
replica, in case of multiple concurrent running replicas, to direct their requests
for. During service discovery processes and by using a suitable service ranking
criteria, a common index can attribute a set of equivalent replicas. The best
replica or service is selected based on this common index. Replicas which get
more interest will be replicated more, others will be hibernated. SDP assumes
by using a suitable caching scheme and the mobility of the mobile nodes, an
interesting service will prevail through the network.
3
Basic Concepts and Criteria
In this section, a list of criteria that can describe the service replication process
is proposed. We will use these criteria to compare the proposed approaches. Of
course, the properties and characteristics of service replication protocols should
not be limited to this criteria set [12,13]. But, according to the relevance to
service replication, we have merged and concluded these criteria as mentioned
in the following subsections.
3.1
Replication Decision
It refers to what triggers the replication process and has two dimensions: time
and location. In this research, the term ”replicator component” refers to the
component that is responsible for the replication process.
Time. Producing replicas is controlled by a set of events that reﬂects the cur-
rent status of the environment. At the occurrence of some events (individually,
mutually or concurrently) the replicator component decides to produce a (set
of) replica(s). In MANETs, the partitioning behavior is a popular candidate to
be used as an indicator for necessity of the replication process. Based on the
prediction of newly forming partition, the replicator component tries to push a
replica inside it before the partitioning process ﬁnishes. Other approaches, as in
[6], especially for dense MANETs, try to keep the number of hops to reach the

A Comparison on MANETs’ Service Replication Schemes
207
service as small as possible. Also, service popularity has been introduced as an
indicator to be taken into consideration during the replication process by SDP.
Location. The replica placement processes complement the replication process.
It allocates the new produced replica to a new place in the network. This process
is based mainly on the motives of the replication process. Partitioning behavior
of MANETs, avoiding long paths to a service, and the service interest aﬀect
the replica placement process. Regarding the natures of MANETs, the eﬃcient
service replication process of services in such networks requires continuous repli-
cation decisions to ensure higher service availability. The process of placing a new
replica on a mobile node (replica allocation process) should be complemented
by another process, which deallocates no longer needed replicas.
3.2
Selection
Selection of one mobile node (leader) from a set of alternatives to be responsi-
ble to deliver a speciﬁc functionality inside its partition is a common issue in
MANETs. The selection of a leader is known as the ”leader election” problem
[5,3]. Leader election is very important for managing a set of concurrently run-
ning replicas in a given situation. Leader election can be achieved based on some
criteria like mobile node stability, available resources, locations and the network
topology features. From a client perspective, a service invocation can be under-
stood as a selection process as well, in which mobile clients are selecting one out
of a set of reachable providers. The following questions can describe generally
the role of the service invocation in the selection: How can a client ﬁnd a service
provider? What should it do when it looses its provider? Can a client continue
from a speciﬁc point of execution when switching between two providers?
3.3
Management of Concurrent Replicas
Client requests to a service can be either read, write, commit, or abort requests
(transaction). Transactions term comes from the ﬁeld of database and concur-
rency control. It indicates the whole required operations to be done to meet
(respond) a service request on the provider side. Some of theses transactions
can change the status of a service or one of its replicas. If the service consis-
tency needs to be ensured, not only how to maintain a speciﬁc service consistent
against a set diﬀerent transactions should be taken into consideration, but also
how to manage the required updates between the whole set of concurrently run-
ning replicas. Regarding the partitioning behavior, two network partitions may
remain separated forever each with its own replica which operates under diﬀerent
sequences of transactions. In this case, while replicas are in separate partitions,
no recovery mechanism can be applied. How are update messages exchanged
between the diﬀerent replicas? How can the state of a service be synchronized
to stand consistently with diﬀerent client requests? The previously mentioned
questions are very important to be answered by any replication approach for
services in MANETs.

208
M. Hamdy, A. Derhab, and B. K¨onig-Ries
3.4
Taxonomy
Three surveys [12,13], which introduce classiﬁcations for data, services, and
databases replication in MANETs and P2P environments, are considered in
this paper. Since, service replication in ad hoc environments can be regarded
as a special case of replication in P2P environments. [14] introduces a survey on
Peer-to-Peer (P2P) replication systems and introduces a set of important crite-
ria and features that can systematize the presented protocols and algorithms in
this study. These provided criteria of the three surveys can be summarized as
follows:
– Single master vs. multi-master: This criterion determines the relation be-
tween replicas inside the network. A primary copy accepts read and write
operations, while the secondary copy accepts only read operations. A single
master replication approach allows only one primary copy for each replicated
service. A multi-master replication approach allows many primary copies for
the replicated service.
– Full replication vs. partial replication: This criterion reﬂects the targeted
locations to be hosting the produced replicas. While full replication denotes
that the service is to be replicated over all available nodes in the network,
partial replication refers that the replicas will be partially hosted on some
of the mobile nodes.
– Synchronous vs. asynchronous: Regarding service states, synchronous service
replication approaches assume service/replicas to be synchronized (against
the diﬀerent applied transactions). On the other hand, asynchronous service
replication approaches does not require (some of/all) service/replicas to be
synchronized at least for sometime.
– Optimistic vs. pessimistic: Optimistic replication approaches expect that
conﬂict between concurrently running replicas will be very rare, while the
pessimistic approaches expect the opposite and implement more conﬂict-
aggressive mechanisms to keep the concurrent replicas synchronized.
– Pull-based vs. push-based: In pull-based replication approaches, clients are
supposed to ask their own replicas from a server. In contrast, push-based
replication approaches let servers dominate the whole replication process.
– Network partitioning awareness: Regarding to the partitioning behavior of a
MANET, most of the replication algorithms are supposed to be aware about
the ongoing to be formed partitions in order to deploy a replica inside these
future partitions.
– Real-Time awareness: In especial applications that use MANTES, like mil-
itary applications, soft and real-time transaction handling should be taken
into consideration by the replication approach.
– Energy consumption awareness: In a MANET, mobile participant are run-
ning on batteries, so, power consumption should be considered in the repli-
cation process.

A Comparison on MANETs’ Service Replication Schemes
209
4
Comparisons
The protocols which have been discussed in Section 2 have been compared to
each other against the criteria which have been discussed in Section 3.
The important notes on this comparison are: (1) All of the protocols answer
the question of the selection except Hauspie’s [6]. Conceptually, the replication
process in [6] does not consider this problem although, in a very dense MANET
and after a motion transition, it is very probably that we have two very close (in
number of hops) active servers, (2) About management of the concurrent replica
although, SDP is an optimistic replication approach, it assumes presences of
a lazy background update process for service consistency. SDP also allows the
set of clients in a certain network partition to dynamically elect their provider
nodes based on the service interest, the uninteresting services will not receive
requests from the client and then will be shut down. Neither Hausepie’s nor
Karen’s show their consideration regarding this important issue, (3) All of the
protocols require a certain set of requirements or infrastructure to be function-
ing except SDP, (4) Regarding the presented taxonomy, half of the protocols
are push based and half of them are optimistic replication protocols. All of the
protocols are partitioning-aware except SDP. Regarding the other replication
taxonomy criteria the protocols are equivalents, (5) About the suggested per-
formance matrices measured by these protocols, the concepts that they try to
quantify are overlapped. The replication cost is used in many terms like the
service cost and communication cost. The service coverage and availability are
presenting the same concept. The eﬀects of the replication process on the other
network functionality are presented by the no. of emitted packets in Hausepie’s
and network traﬃc and scalability in Dustdar’s, (6) On the one hand, prediction
error and access/update cost are parameters to show the signiﬁcant performance
of the proposed protocols based on speciﬁc ideas of the solution design. On the
other hand, SDP provides a set of measurements for the correctness of replica
allocation relatively to the optimum service distribution at any time. These cor-
rectness measurements are independent indicators and can evaluate any service
replication protocol independent of its design, ﬁnally (7) For the comparison’s
candidates, SDP, PSRP and SSRP, all of them are attributed by high service
availability, low service cost and replication degree. The main point to be high-
lighted here is that SDP is an architecture independent unlike the other two
protocols which require presence of a speciﬁc routing protocol (TORA).
Based on the comparisons, the protocols PSRP and SSRP have been cho-
sen to be compared in terms of performance to SDP. PSRP and SSRP show a
complete set of solutions for service replication in ad hoc network. They pro-
vide typical well described partitioning behavior prediction mechanisms based
on TORA routing component with a pull based replication scheme.PSRP and
SSRP execute the partition prediction only when some topological changes and
not periodically and this behavior enables them to produce less complexity mech-
anisms for replication. As proved in [13], using analytical studies, both of PSRP
and SSRP showed the best complexity for prediction replication and query costs
compared to the other alternatives. The main shortage of PSRP and SSRP is

210
M. Hamdy, A. Derhab, and B. K¨onig-Ries
absence of any service caching scheme, on the other hand, since SDP requires
time to accumulate the service interest of the set of concurrently running active
replicas, this leads to have multiple running active replicas in the same network
partition longer than the others. Of course, it is not a dominative behavior of
SDP but it happens in speciﬁc situations where the hibernation threshold is
longer than the required time of publishing the service and receive at least one
service request in the short leader election mode [3].
5
Case Study
In this study, a detailed comparison among three service replication approaches
namely SDP, PSRP and SSRP is presented. Regardless of the issue of QoS
requirements, PSRP and SSRP assume as SDP that one active service or replica
inside a network partition can satisfy all of these partition participants’ requests.
The performance matrices used in this case study are merged and uniﬁed from
the introduced matrices in [2,4,5], they can be summarized as follows:
– Service Availability (for PSRP and SSRP): The ratio between the number
of nodes that can access the service to the network size (total number of
nodes).
– Success Ratio (for SDP): Average ratio of the number of succeeded service
requests to the total number of the generated service requests.
SDP distinguishes between services that are important to the clients, at
a given point in time, and services that are not. SDP therefore tries to
maximize availability of the important services and does not care about the
others. Therefore, using the standard service availability metrics would be
unfair towards SDP. On the other hand, using SDP speciﬁc criteria would
be meaningless for PSRP and SSRP that have no notion of interest in a
service. However, from a user’s perspective the semantics behind those two
measurements is the same: Both measures how likely he is to get access to
a service he wants to use.
– Weighted Rational Allocation Correctness Ratio (WRCR) [2]: is a ratio be-
tween 0 and 1 that indicates the correctness of the overall replica allocation
in all of the network partitions (n) at a certain time relatively to the assumed
optimum replica distribution. It is computed as follows:
WRCR =
n

i=0
(
Pzi
networksize.RCR(Pi))
(1)
where (Pzi) is the partition size of a given ith partition and (RCR) is the
Rational Allocation Correctness Ratio[2] in a given network partition (Pi):
RCR(Pi) =
⎧
⎨
⎩
0
Ri = 0
1
Riin{1, 2}
3
Ri . P zi−Ri
P zi−2 Ri > 2, Pi > 2
(2)
where (Ri) is the number of active replicas in the ith partition.

A Comparison on MANETs’ Service Replication Schemes
211
RCR represents an inverse proportional relation between the size of a network
partition to the number of active replicas inside it. The higher number of ac-
tive replicas, the lower RCR. It assumes that only one active replica inside a
partition is the optimum service distribution so it gives 1 in this case. Because
SDP is a replication approach and assumes that at least a replication process
should proactively take place once, SDP does not penalize the case of presence of
two active replicas inside a partition and considers this also as an optimum dis-
tribution. Moreover, SDP assumes only one service (the most interesting) will
continue and the other one will be hibernated after a certain time interval of
hibernation check. PSRP and SSRP have also their merge mechanism for merg-
ing the multiple running services, and they do not penalize these cases. For all
other cases, RCR decreases as number of active replicas increases on rational
basis. WRCR accumulates all RCRs overall network partitions relatively to the
weight of its partition to the whole network size.
WRCR has many advantageous features. Besides, measuring how a certain
service distribution is near to the optimum assumed distribution, it represents
also an indication for another set of the replication process quantitative measure-
ments like service cost and the precise of the partitioning prediction. [2] shows
more details about WRCR and discusses other diﬀerent proposed computation
methods to highlight the correctness of the replica allocation process. [13] intro-
duces an extended set of quantitative and qualitative criteria and measurements
for replication in MANETs.
In order to understand a comparison between SDP and both of PSRP and
SSRP, the following facts need to be considered.
1. For SDP, it is important to have a requesting model. This is the main in-
ﬂuence in the replication/hibernation processes. It determines the generated
requests and how the clients produce them regarding the service content.
[2] introduces and discusses a proposed requesting (calling) model and how
the produced gross interest can be quantiﬁed. In this work, two gross inter-
est modes are used [2]: (a)”Rich” gross interest which generates a high and
frequent number of requests with short time intervals of pauses regarding
a speciﬁc service and (b) ”Poor” gross interest which generates a very low
number of requests with relative long time intervals of pauses. Two diﬀerent
client groups (2 and 4 client groups categories) are used. These groups diﬀer
in their maximum requesting rate. In cases of 2 Client groups, the maximum
calling rate will be 1 call/minute (client groups: 0 and 1 calls/minute). In
cases of 4 Client groups, the maximum calling rate will be 1 call/minute
(client groups: 0,1, ... 3 calls/minute). The replication threshold, i.e., the
minimum calling rate a client must reach in order to be allowed to obtain a
replica (set to equal the maximum calling rate). Regarding the replication
threshold, the higher number of client groups the lower number of clients
ready to receive their own replicas we get. Regarding the gross interest, the
reacher gross interest the higher number of clients ready to receive their own
replicas we get.

212
M. Hamdy, A. Derhab, and B. K¨onig-Ries
2. PSRP and SSRP assume that all of the deployed services are important
to be replicated once they detected motive changes in the network status
or topology. So, we can note that the services to be replicated using these
protocols are gaining, in terms of SDP gross interest, a very rich gross in-
terest. In spite of this note, in this comparison, results are also investigated
using a poor gross interest settings for SDP. These results, as we are going
to show, represent also the performance of SDP with uninteresting services
and should considered in the advantages for SDP.
5.1
Settings
SDP settings. A short election mode [3] for the concurrent replicas is used by
SDP, in which the provider hibernates (if the replica is already cached) its replica,
if they do not receive at least one call in the minimum time interval which is
allowed for the provider to get at least one call. [3] suggests that time interval to
be one second. Otherwise the hibernation minimum time interval is one minute.
The replication threshold is set to be |MaximumRequestingRate| : 1minutes,
which means that a node which achieve number of request equals the maximum
allowed requesting rate has the right to request its own replica. Finally, a general
requirements’ index [1] is attributing the diﬀerent services and replicas which
diﬀers a 20% normally about a general index of 0.5.
Network settings. The random way point mobility model [15] is applied, in
which each node selects uniformly and randomly a destination in a 600m×600m
square area with no obstacles and a speed between 0 and a certain maximum
speed Vmax (6 m/s), then it stays during a pause time between 0 and a certain
maximum pause time of 15 seconds before selecting a new random destination
and a speed and so on. Each mobile node can cover a 75 m radio transmission
range about itself. One node is selected to host the original service in the be-
ginning of the simulation. The simulation time is set to be 2 hours. The showed
results are coming out of 20 times runs. The performance matrices are evaluated
against diﬀerent network sizes varying between 10 and 140 nodes.
Fig. 1. Service Availability and WRCR of PSRP, SSRP and SDP Success Ratio (2
client groups) vs. the network size

A Comparison on MANETs’ Service Replication Schemes
213
Fig. 2. Service Availability and WRCR of PSRP, SSRP and SDP Success Ratio (4
client groups) vs. the network size
5.2
Results and Analysis
PSRP and SSRP performance analysis. For PSRP, in Figure 1, the service
availability is high when the network size (N) is low (i.e., when N <= 20), then
it starts to decrease with an increasing in N (i.e. when 30 < N < 60), and ﬁnally
it increases with any further increasing in N (i.e. N >= 60). This is can be ex-
plained as follows: To predict network partitioning, PSRP is designed to be trig-
gered at a determined time interval, σ, ahead of a possible partitioning. PSRP
works eﬃciently when the partition is aﬀected by one topological change. In
case of multiple topological changes (i.e., a new topological change occurs before
PSRP has terminated), PSRP restarts execution from scratch whenever a new
topological change. If topology changes do not stop, PSRP will not terminate. In
this case, network partitioning might never be predicted or it is predicted after
the partitioning has already occurred. When N is low, the service availability is
approximately 1. This is due to the fact the network partitions are composed
only of a few nodes, and in almost cases PSRP can predict the partitioning
within σ time units. When N increases (30 < N < 60), multiple concurrent
changes occur frequently and hence PSRP becomes less accurate in predicting
future network partitioning, which decreases the service availability. After a cer-
tain level N: (N >= 70), it becomes very high, and most of the nodes belong
to large connected partitions. Although nodes move around, high node density
means that partitions remain connected for longer durations, which means less
multiple topological changes and high service availability. As SSRP is designed
to handle concurrent topological changes and be accurate in predicting network
partitioning, it can ensure the highest service availability. SSRP produces higher
numbers of replicas then PSRP (higher service cost) to deal with the frequently
concurrent topological changes quickly. When PSRP becomes less accurate in
predicting future partitions (especially when 30 < N < 60), it fails to produce
the necessary servers whereas SSRP succeeds in doing so.
About the correctness of replicas’ allocation process which is expressed in
terms of WRCR, it ﬁrst decrease with N for both of PSRP and SSRP, and
then after some level at (N >= 60) they start to increase with any further
increasing of N. SSRP’s feature of higher service cost comes at the expense of the
correctness of replicas’ allocation process. Ensuring higher availability against
the multiple concurrent formed partitions leads to have worse replica allocation

214
M. Hamdy, A. Derhab, and B. K¨onig-Ries
process. WRCR(SSRP) continues collapsing as higher number of small partitions
appear (see the partition analysis for the same network settings in [2]) and more
number of replicas are required. Starting from (N >= 60), the WRCR of SSRP
starts increasing because the formed partitions are dense enough to be continued
unpartitioned. On the other hand, less number of generated replicas by PSRP
ensures that it has higher corrected replica allocation process with more than
(0.81) average WRCR. As a conclusion here, regarding our performance analysis,
PSRP can be attributed by a high but not absolute service availability with very
high correctness replica allocation correctness ratio (WRCR). Ensuring relatively
absolute service availability by SSRP leads to low correctness replica allocation
correctness ratio (WRCR).
SDP performance analysis. First, as shown in Figure 1, using a maximum
requesting rate of 1 call per minute, two client groups will be found which means
just half of the network participants will participate in the SDP replication
processes at a time. The replication threshold here (1 call per minute) considers
that any requesting node is interested in getting a replica from the service. As
mentioned before, SDP’s performance is expressed in terms of success ratio. For
the rich gross interest, starting from moderate number of N: (N >= 50), it
achieves a comparable service availability to PSRP. For the poor gross interest
in which the service to be replicated does not receive enough interest from the
network participants because it is not interesting for most of the mobile nodes,
SDP can achieve a better service availability starting from N: (N >= 70). It is
very important here to keep into mind that the other service replication protocols
except SDP usually consider the oﬀered services are very vital to be replicated
(always rich) apart from their importance for the network participants . SDP can
estimate a varying importance degree for the oﬀered services and then evaluate
its interest-based replication decisions accordingly [1].
About the correctness of replicas’ allocation (WRCR), the rich gross inter-
ests achieves the same SSRP’s WRCR starting from N: (N >= 40)and PSRP’s
WRCR starting from N: (N >= 50). For the poor gross interest, although the
expected number of the generated replicas [2] is low, the service allocation cor-
rectness (WRCR) is generally high and comparable to SSRP’s starting from N:
(N >= 60).
Second, as shown in Figure 2, with a maximum requesting rate of 3 calls
per minute, four client groups will be found which means that just one quarter
of the network participants will participate in the SDP replication processes at
a time. The service availability increases as the N increases and decreases as
the number of the client groups increases. The availability of SDP in rich gross
interest is negatively aﬀected by the four client groups. Again, it is important
that we need to keep into mind that by increasing the number of client groups the
importance degree of the oﬀered service and the related interest will decrease. In
spite of having a less interesting service it can achieve a high service availability
compared to PSRP starting from N: (N >= 60). On the other hand applying a
poor gross interest with the four client groups simulates a ”don’t care” attitude
of the mobile clients to the oﬀered service. The generated number of requests in

A Comparison on MANETs’ Service Replication Schemes
215
this case will be very limited and the number of interested clients will be very low.
So, the low oﬀered service availability in this case will be obvious. In Figure 2,
WRCR is negatively aﬀected by the presented low number of the interested
clients regarding the high number of client groups. In case of the rich gross
interest, WRCR is generally shifted down of the PRSP’s starting from N: size
(N >= 60) and better than SSRP’s starting from a moderate N: (N >= 50). For
the poor gross interest, WRCR is generally low but starting from N: (N >= 70),
it can achieve around and higher WRCR than SSRP’s. This means although the
very low service interest that has been presented in these cases of the higher
number of client groups, SDP can maintain a comparable performance to PSRP
and SSRP regarding the replica allocation correctness ratio.
6
Conclusions
The contribution of this work is in comparing the interest-based SDP against
other typical traditional topology prediction replication protocols. The SDP pro-
tocol has been introduced as a protocol that uses new concepts based only on the
service interest which enabled it to avoid making any network topological anal-
ysis or partitioning prediction besides being network architecture independent.
We have identiﬁed a number of criteria to compare diﬀerent service replication
protocols in ad-hoc networks. Criteria that dominate the service replication pro-
cess regarding all of a set of notable protocols have been identiﬁed and discussed.
Two protocols (PSRP and SSRP) has been identiﬁed as candidates to represent
typical mobile service replication protocols to be compared to SDP. Reasons of
this selection have been discussed. A detailed simulation has been elaborated
for investigations of the three protocols’ performance. The results highlight the
diﬀerence between PSRP and SSRP performance and how increasing the service
availability can aﬀect the replica allocation correctness. On the other hand, an
ascending service availability and replica allocation correctness have been shown
by SDP. Regarding the performance analysis, service availability and replica al-
location correctness of SDP, PSRP and SSRP can be compared in the intervals
of the moderate network size. SDP has an advantage of enabling clients to deter-
mine their popular services to be replicated. Even for the low popularity services,
SDP can deal positively with them.
References
1. Hamdy, M., K¨onig-Ries, B.: The Service Distribution Protocol for MANETs- Cri-
teria and Performance Analysis. In: Book of Communications in Computer and
Information Science, Book of the selected papers of the ICETE 2008. CCIS, vol. 48,
pp. 467–479. Springer, Heidelberg (2009)
2. Hamdy, M., K¨onig-Ries, B.: Eﬀects of diﬀerent hibernation behaviors on the ser-
vice distribution protocol for mobile networks and its replica placement process.
In: International Workshop on the Role of Services, Ontologies, and Context in
Mobile Environments (RoSOC-M 2009) in conjunction with the 10th International
Conference on Mobile Data Management (MDM 2009), Taipei, Taiwan (May 2009)

216
M. Hamdy, A. Derhab, and B. K¨onig-Ries
3. Hamdy, M., K¨onig-Ries, B.: Leader election modes of the service distribution proto-
col for mobile ad hoc networks. In: Proceedings of the fourth German Community
Conference on Mobility and Mobile Information Systems (4. Konferenz Mobilitt
und mobile Informationssysteme) (MMS 2009), Mnster, Germany (March 2009)
4. Derhab, A., Badache, N.: A pull-based service replication protocol in mobile ad
hoc networks. European Transactions on Telecommunications 18, 1–11 (2007)
5. Derhab, A., Badache, N.: Self-stabilizing algorithm for high service availability in
spite of concurrent topology changes in ad hoc mobile networks. Journal of Parallel
Distributed Computing 68(6), 752–768 (2008)
6. Hauspie, M., Simplot, D., Carle, J.: Replication decision algorithm based on link
evaluation for services in manet. Technical report, University of Lille, France (2001)
7. Wang, K., Li, B.: Eﬀcient and guaranteed service coverage in partitionable mobile
ad-hoc networks. In: Proceedings of the IEEE Computer and Communications
Societies Twenty-First Annual Joint Conference (INFOCOM 2002), New York,
USA (2002)
8. Hong, X., Gerla, M., Pei, G., Chiang, C.-C.: A group mobility model for ad hoc
wireless networks. In: MSWiM 1999: Proceedings of the 2nd ACM international
workshop on Modeling, analysis and simulation of wireless and mobile systems, pp.
53–60. ACM, New York (1999)
9. Jing, Z., Jinshu, S., Kan, Y., Yijie, W.: Stable Neighbor Based Adaptive Replica
Allocation in Mobile Ad Hoc Networks. In: Bubak, M., van Albada, G.D., Sloot,
P.M.A., Dongarra, J. (eds.) ICCS 2004. LNCS, vol. 3036, pp. 373–380. Springer,
Heidelberg (2004)
10. Dustdar, S., Juszczyk, L.: Dynamic replication and synchronization of web services
for high availability in mobile ad-hoc networks. Service Oriented Computing and
Applications 1, 19–33 (2007)
11. Park, C.: Internet-Draft: Temporally-Ordered Routing Algorithm (TORA) - Func-
tional Speciﬁcation (1999),
http://tools.ietf.org/html/draft-ietf-manet-tora-spec-02
12. Padmanabhan, P., Gruenwald, L., Vallur, A., Atiquzzaman, M.: A survey of data
replication techniques for mobile ad hoc network databases. The VLDB Jour-
nal 17(5), 1143–1164 (2008)
13. Derhab, A., Badache, N.: Data replication protocols for mobile ad-hoc networks: A
survey and taxonomy. IEEE Communications Surveys and Tutorials 11(2), 33–51
(2009) (Second Quarter)
14. Martins, V., Pacitti, E., Valduriez, P.: Survey on data replication in p2p systems.
Technical report, INRIA, Rennes, France (2006)
15. Bettstetter, C., Hartenstein, H., P´erez-Costa, X.: Stochastic properties of the ran-
dom waypoint mobility model. Wirel. Netw. 10(5), 555–567 (2004)

Applying Vehicular Ad Hoc Networks for
Reduced Vehicle Fuel Consumption
Maazen Alsabaan1, Kshirasagar Naik1, and Amiya Nayak2
1 Department of Electrical and Computer Engineering, University of Waterloo,
Waterloo, ON, Canada N2L 3G1
2 School of Information Technology and Engineering, University of Ottawa, Ottawa,
ON, Canada K1N 6N5
{malsabaa,knaik}@uwaterloo.ca, anayak@site.uottawa.ca
Abstract. With recent advances in the development of wireless com-
munication networks, Vehicular Ad hoc Networks (VANETs) have been
receiving considerable research interest. One of the major applications of
VANETs is Intelligent Transportation Systems (ITS). To exchange and
distribute messages, geocast protocols have been proposed for ITS. Al-
most all of these protocols evaluate network performance level, instead
of evaluating the protocol impact on the vehicular system. Nowadays,
many drivers are becoming increasingly concerned with rising fuel cost.
Therefore, it is desirable to create new “economical” geocast (EG) pro-
tocols. The main goals of this paper are to motivate communications
researchers to design EG protocols, demonstrate the ability to integrate
fuel consumption models with VANETs, and illustrate the necessity of
transmitting information to vehicles in order for drivers to choose the eco-
nomical path. Simulation results demonstrate that signiﬁcant amounts
of fuel will be saved if such an EG protocol is used.
Keywords: fuelconsumption,economicalgeocast,intelligenttransporta-
tion systems, vehicular ad hoc networks.
1
Introduction
During the past several years, fuel prices in many countries have been rising
considerably. For instance, the gasoline price in western Canada almost doubled
from about 53 cents/liter in 1998 to 102 cents/liter in 2009 [1]. Fuel expenditure
has become signiﬁcant enough, so that people in many countries need to take it
into consideration when allocating their budget. Therefore, new ways to reduce
fuel expenditure are needed to be introduced.
A signiﬁcant amount of fuel is wasted due to drivers getting lost or not taking
a very direct route to their destination, high acceleration, stop-and-go condi-
tions, congestion, long distance routes, high speeds, and vehicle’s model and
year. In case of vehicle’s model, some organizations, such as Oﬃce of Energy Ef-
ﬁciency (OEE) in Canada are mandated to lead citizens to energy eﬃciency at
home, at work, and on the road [2]. OEE published a fuel consumption guide in
cooperation with vehicle manufacturers, Natural Resources Canada (NRCan),
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 217–228, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

218
M. Alsabaan, K. Naik, and A. Nayak
and Transport Canada (TC) [3]. This guide assists people, particularly those
who want to purchase a new vehicle, by comparing relative fuel consumption
ratings among vehicles of diﬀerent models. The other cases can be alleviated by
implementing Intelligent Transportation Systems (ITS).
ITS is a mixture of tools such as software, hardware, traﬃc engineering con-
cepts, and communication technology that can be integrated in order to be ap-
plied to the transportation system to improve its eﬃciency and safety [4]. In ITS
technology, navigation (e.g., Google Map) is a fundamental tool that helps driver
select a suitable path such as the shortest path. In [5], a navigation tool has been
designed especially for minimizing fuel consumption and vehicle emissions. To
alleviate congestion, a number of scheduling methods have been proposed [6,7];
however, it is not always that vehicles passing the uncongested route consume
less fuel than ones on the congested route.
Various forms of wireless communications technologies have been proposed
for ITS. Vehicular ad hoc networks (VANETs) are a promising research area in
ITS applications [8]. With VANETs, drivers can be informed about all kinds of
events and conditions, which could impact their travel. To exchange and dis-
tribute messages, broadcast or geocast routing protocols have been proposed for
ITS applications [9,10,11]. Almost all these protocols evaluate network-centric
performance level (e.g., message delays, packet delivery ratio, etc.), instead of
evaluating the impact of the protocol on the vehicular system (e.g., fuel con-
sumption, emissions, travel time, etc.).
In this paper, the impact of using a geocast protocol on the vehicle fuel con-
sumption is studied. To the best of our knowledge, our attempt is a ﬁrst in the
ﬁeld. Designing or proposing the communication protocols that are suitable in
applications such as reducing fuel consumption is out of the scope. The main
contributions of this work are to:
• Motivate researchers working in the ﬁeld of communication to design eco-
nomical geocast protocols, which means geocast protocols that focus on min-
imizing fuel consumption, thus saving money;
• Demonstrate the ability to integrate fuel consumption models with vehicular
networks;
• Illustrate the necessity of sending information to vehicles in order for drivers
to choose an appropriate path to a target to minimize fuel consumption.
The remainder of this paper is organized as follows. The research background
is reviewed in Section 2. Our system model is described in Section 3, including
traﬃc model, accident model, fuel consumption model, and communication sys-
tem. Simulation results are presented in Section 4. In Section 5, main issues to
design an economical geocast protocol have been discussed. Finally, conclusions
are drawn in Section 6.
2
Background
This research brings together two key areas: vehicle fuel consumption models
and geocast protocols.

Applying Vehicular Ad Hoc Networks for Reduced Vehicle Fuel Consumption
219
2.1
Fuel Consumption Models
A number of research eﬀorts have attempted to develop vehicle fuel consump-
tion models. Due to their simplicity, macroscopic fuel consumption and emission
models have been proposed [12,13]. These models compute fuel consumption and
emissions based on average link speeds. Therefore, they do not consider transient
changes in a vehicle’s speed and acceleration levels. To overcome this limitation,
microscopic fuel consumption and emission models have been proposed [14,15]. In
these models, a vehicle’s fuel consumption and emission can be predicted second-
by-second. An evaluation study has been applied on a macroscopic model called
MOBILE6 and two microscopic models: the Comprehensive Modal Emissions
Model (CMEM) and the Virginia Tech Microscopic model (VT-Micro) [15]. It
has been demonstrated that the VT-Micro and CMEM models produce more re-
liable fuel consumption and emission estimates than the MOBILE6 [12]. Figure
1 shows the link between transportation models and fuel consumption estimates.
Microscopic models are well suited for ITS applications since these models are
concerned with computing fuel consumption and emission by tracking individ-
ual vehicles instantaneously. The following subsections brieﬂy describe the two
widely used microscopic models.
Fig. 1. Summary of the link between transportation and fuel consumption models
CMEM Model. The development of the CMEM began in 1996 by researchers
at the University of California, Riverside. The term “comprehensive” is utilized
to reﬂect the ability of the model to predict fuel consumption and emissions
for a wide variety of vehicles under various conditions. The CMEM model was
developed as a power-demand model. It estimates about 30 vehicle/technology
categories from the smallest Light-Duty Vehicles (LDVs) to class 8 Heavy-Duty
Trucks (HDTs) [14]. The required inputs for CMEM include vehicle operational
variables (e.g., second-by-second speed and acceleration) and model-calibrated
parameters (e.g., cold-start coeﬃcients and engine-out emission indices). The
cold-start coeﬃcients are to measure the emissions that are produced when
vehicles start operation, while engine-out emission indices are the amount of
engine-out emissions in grams per a gram of fuel consumed [14,16]. The CMEM
model was developed using vehicle fuel consumption and emission testing data

220
M. Alsabaan, K. Naik, and A. Nayak
collected from over 300 vehicles on three driving cycles, following the Federal Test
Procedure (FTP), US06, and the Model Emission Cycle (MEC). Both second-
by-second engine-out and tailpipe emissions were measured.
VT-Micro Model. The VT-Micro model was developed using vehicle fuel con-
sumption and emission testing data collected at the Oak Ridge National Labo-
ratory (ORNL) and the Environmental Protection Agency (EPA). These data
include fuel consumption and emission rate measurements as a function of the
vehicle’s instantaneous speed and acceleration levels. Therefore, the input vari-
ables of this model are the vehicle’s instantaneous speed and acceleration. The
model was developed as a regression model from experimentation with numer-
ous polynomial combinations of speed and acceleration levels as shown in the
following equation.
ln(MOEe) =
3
i=0
3
j=0(Le
i,j × si × ai),
for a ⩾0
3
i=0
3
j=0(M e
i,j × si × ai),
for a < 0
(1)
where s is the instantaneous speed (km/h), a is the instantaneous acceleration
(km/h/s), Measure of Eﬀectiveness (MOE) is the instantaneous fuel consump-
tion or emission rate (L/s or mg/s), e is an index denoting fuel consumption
or emission type, and Le
i,j and M e
i,j represent model regression coeﬃcients for
MOEe at speed power i and acceleration power j for positive and negative
accelerations, respectively.
As noticed from the above equation, the model is separated for positive and
negative accelerations because vehicles exert power in positive accelerations,
while vehicles do not exert power in the negative accelerations. The VT-Micro
model is inserted into a microscopic traﬃc simulator called ”INTEGRATION”
to compute vehicles’ fuel consumption and emissions [17,18]. This model has
been used in this paper due to its simplicity and high accuracy since it produces
vehicle emissions that are consistent with the ORNL data. The correlation co-
eﬃcient between the ORNL data and the model predicted values ranges from
92% to 99% [19].
Example of using the VT-Micro Model. Sample model coeﬃcients for
estimating fuel consumption rates for a composite vehicle are introduced in Ta-
ble 1. The composite vehicle was derived as an average across eight light-duty
vehicles.The required input parameters of the model are s, a, Le
i,j, and M e
i,j.
Consider a vehicle started traveling. A microscopic traﬃc model has to be
utilized in order to measure the vehicle instantaneous speed and acceleration.
Simulation of Urban Mobility (SUMO) has been used in this regard. SUMO is
a microscopic traﬃc simulation package developed by employees of the Institute
of Transportation Systems at the German Aerospace Center [20].
VT-Micro model has a speed-acceleration boundary. For instance, at speed
50 km/h, the maximum acceleration that can be used in the model is around
2.2 m/s2 [19]. In this example, the maximum vehicle speed, acceleration and
deceleration are set to 50 km/h, 2 m/s2 and -1.5 m/s2, respectively. The second-
by-second speed and acceleration are computed for the ﬁrst 5 seconds of the

Applying Vehicular Ad Hoc Networks for Reduced Vehicle Fuel Consumption
221
Table 1. Sample VT-Micro model coeﬃcients for estimating fuel consumption
Coefficients
s0
s1
s2
s3
Positive a
a0
-7.73452
0.02799
-0.0002228
1.09E-06
a1
0.22946
0.0068
-0.00004402
4.80E-08
a2
-0.00561
-0.00077221
7.90E-07
3.27E-08
a3
9.77E-05
0.00000838
8.17E-07
-7.79E-09
Negative a
a0
-7.73452
0.02804
-0.00021988
1.08E-06
a1
-0.01799
0.00772
-0.00005219
2.47E-07
a2
-0.00427
0.00083744
-7.44E-06
4.87E-08
a3
0.00018829 -0.00003387
2.77E-07
3.79E-10
vehicle’s trip as shown in Table 2. It is noticed that all accelerations are pos-
itive. By applying the input parameters to Equation 1, the fuel consumption
estimates should be as demonstrated in Table 2. Clearly from the table, the in-
crease or decrease of the fuel consumption is based on the speed and acceleration.
Although fuel consumption normally increases with increasing acceleration, it is
not the largest amount at the time of the highest acceleration, which is 6.156
km/h/s at the 2nd second because of the speed eﬀect on the fuel consumption.
Likewise, at the highest speed, which is 27.072 km/h at the 5th second, the
fuel consumption is not the largest amount as the acceleration is low at the 5th
second.
Table 2. Instantaneous speed, acceleration and fuel consumption
Time (s)
1
2
3
4
5
Speed (km/h)
7.2
13.356
18.648
23.184
27.072
Acceleration (km/h/s)
7.2
6.156
5.292
4.536
3.888
Fuel Consumption (liter) 0.002338176 0.002502677 0.00260202 0.002611232 0.002555872
2.2
Geocast Protocols
Geocast is a network protocol that aims to deliver data packets from a source
node to all nodes that currently reside in selected geographical regions. Some
important beneﬁts from geocast protocols can be introduced to ITS applications.
Using safety application as an example, suppose a vehicle can identify itself as
crashed by vehicular sensors that detect events like airbag inﬂation. Then, it
sends a warning message to nearby vehicles. After receiving the message, drivers
are aware of the crash; therefore, they can take an action such as choosing a
diﬀerent route or slowing down their cars gradually.

222
M. Alsabaan, K. Naik, and A. Nayak
Fig. 2. Possible network architectures for geocast in vehicular networks
As shown in Figure 2, the network architectures for geocast in vehicular net-
works can be Inter-Vehicle Communication (IVC), infrastructure-based vehicle
communication, and Hybrid Vehicle Communication (HVC). IVC is a direct
radio communication between vehicles without control centers. Thus, vehicles
need to be equipped with network devices that are based on a radio technol-
ogy, which is able to organize the access to channels in a decentralized manner
(e.g., IEEE 802.11 and IEEE 802.11p). In addition, multi-hop routing protocols
are required, in order to forward the message to the destination that is out of
the sender’s transmission range. In infrastructure-based vehicle communication,
ﬁxed gateways are used for communication such as access points in a Wireless
Local Area Network (WLAN). This network architecture could provide diﬀerent
application types and large coverage. However, the infrastructure cost has to
be taken into account. HVC is an integration of IVC with infrastructure-based
communications.
3
System Model
3.1
Traﬃc Model
As shown in Figure 3, vehicles’ trips initiate from the Original (O) to the Des-
tination (D). Two highways (Hwys) with N-lanes have been considered. Hwy 1
with length L1 is the main route for vehicles since it has the minimum travel
time. Hwy 2 with length L2, where L2 = 1.5 × L1, is the alternative route.
3.2
Accident Model
An accident is modeled as temporal reductions in capacity, where such capacity
reductions are speciﬁed as an eﬀective number of lanes blocked by the accident
for a given length and time. The model requires start time of the accident, time
at which the traﬃc impact of the accident ends, number of lanes blocked by the
accident, distance of the blocked lanes.

Applying Vehicular Ad Hoc Networks for Reduced Vehicle Fuel Consumption
223
3.3
Fuel Consumption Model
Comparison of MOBILE5a, MOBILE6, VT-Micro, and CMEM models for hot-
stabilized light-duty gasoline vehicle fuel consumption and emissions has been
studied in reference [21]. According to that study, the VT-Micro model is su-
perior to the others for its accuracy. Therefore, the VT-Micro model has been
used in this paper. A more detailed description of the model is provided in
Subsection 2.1.
3.4
Communication System
Assume the existence of IVC. Each vehicle is equipped with an Application
Unit (AU) and On-Board Unit (OBU) [22]. An AU is an in-vehicle entity that
runs applications. It is assumed in this study that the AU can detect the crash
occurrence of its vehicle. Also, it is assumed that the AU is equipped with
position data and map (e.g., GPS). In addition to AU, OBU is another in-vehicle
entity, which is responsible for vehicle-to-vehicle communication. The OBU is
connected to the AU. In this study, it is assumed that the OBU is equipped with
a (short range) wireless communication device. A multi-hop routing protocol is
assumed in order to allow forwarding of data to the destination that has no
direct connectivity with the source.
In this study, the use of geographical positions for addressing and routing
of data packet (geocast) is assumed. The destination is addressed as all nodes
in a geographical region. Designing or proposing the communication protocols
that are suitable in applications such as reducing fuel consumption is out of the
scope of this paper. The main objective of this study is to encourage communi-
cations researchers to propose protocols with a goal of minimizing vehicle fuel
consumption.
4
Simulation Study
The lengths of the highways and accident are shown in Figure 3. Hwy 1 and
Hwy 2 are 4-lane one direction. For both highways, the free-ﬂow speed is 90
km/h. One of the important road segment characteristics is its basic saturation
ﬂow rate which is the maximum number of vehicles that would have passed the
segment after one hour per lane. Another important characteristic is the speed
at the basic saturation ﬂow or speed-at-capacity [4]. In this study, the basic
saturation ﬂow rate per lane is 2000 vehicles per hour with speed 70 km/h.
Vehicles enter the system uniformly in terms of the vehicle headway with a
rate of 2500 vph/lane. For example, 2500 vehicles per hour uniformly depart
from the origin between 9:00 and 9:10 am. In this case, a total of 864 vehicles
will be generated with headway averaging 1.44 seconds.
The simulator used in this study is a trip-based microscopic traﬃc simula-
tor, named INTEGRATION. The INTEGRATION model is designed to trace
individual vehicle movements from a vehicle’s origin to its destination at a deci-
second level of resolution by modeling car-following, lane changing, and gap

224
M. Alsabaan, K. Naik, and A. Nayak
Fig. 3. System model
acceptance behavior [17,18]. In this paper, the total fuel consumption has been
computed in four diﬀerent scenarios:
Scenario 1: All vehicles traveled on Hwy 1 with no accident;
Scenario 2: All vehicles traveled on Hwy 1 where an accident is happened;
Scenario 3: All vehicles traveled on Hwy 2;
Scenario 4: Some vehicles changed their route from Hwy 1 to Hwy 2.
It is obvious that Hwy 1 is the best choice in terms of distance, travel time,
fuel consumption, and emissions. However, if an accident happened on Hwy 1, it
might not be the best choice for the drivers. Focusing on the fuel consumption,
it is assumed that each vehicle has a navigation system that advises drivers
on route selection based on minimizing trip fuel consumption. Figure 4 shows
the impact of increasing the number of traveling vehicles on the total vehicles’
fuel consumption. It is clear and expected that Scenario 1 is most economical.
Consequently, the navigation system will advise the driver to travel on Hwy 1.
However, if an accident happened on Hwy 1, a signiﬁcant amount of fuel can
be wasted due to stop-and-go conditions and congestion. It can be noticed from
Figure 4 that Scenario 2 is more economical than Scenario 3 in light traﬃc
density. Conversely, Scenario 3 becomes more economical than Scenario 2 with
increasing traﬃc density.
Since navigation systems are not aware of the sudden events (e.g., accidents),
vehicle-to-vehicle communications will be needed. With a focus on geocast, two
main points have to be considered in order to design an economical protocol:
Geocast region. The warning message has to be delivered to the region so that
drivers can ﬁnd a new path to avoid congestion.
Delivered Message. A warning message will be issued once an accident occurs
in order to alert nearby vehicles. Based on the results shown in Figure 4,
not all alert routes (i.e. routes with accident “Scenario 2”) consume more
fuel than no alert routes “Scenario 3”. Therefore, we need to deﬁne when
the status of the most economical route will change from Hwy 1 to Hwy 2
(this depends on the traﬃc density). Then, ﬁnd a way to inform the drivers.

Applying Vehicular Ad Hoc Networks for Reduced Vehicle Fuel Consumption
225
Fig. 4. Total fuel consumption versus number of traveling vehicles
Fig. 5. The impact of the EG protocol on the amount of fuel consumption
The presented system model requires a geocast protocol that can inform all vehi-
cles, which have traveled beyond the nearest exit to the accident site. Moreover,
it needs a geocast protocol that is able to advise the ﬁrst 973 traveling vehicles
to continue on Hwy 1, while others change their route to Hwy 2. Figure 5 shows
the amount of fuel consumption if the above requirements can be met.
5
Discussions
The important issues that have to be taken into account in designing an eco-
nomical geocast protocol are as follows:

226
M. Alsabaan, K. Naik, and A. Nayak
Calculating the Desirable Number of Traveling Vehicles on Hwy 1:
The desirable number of traveling vehicles is 973 in this study. This number
was obtained from the simulator. However, research is needed to be done
to estimate this number. This information can be used in designing com-
munication applications. Consequently, the geocast packet will contain this
information when a geocast is performed. In conclusion, ITS applications
and tools should be able to calculate this kind of information and inject it
to the geocast packet.
Traﬃc Density versus Fuel Consumption: In many cases the shortest
path in terms of time or distance will also be the minimum fuel consumption.
However, this is not true in several cases which increase traﬃc. For instance,
congestion will start if an accident happened. In this case, stop-and-go con-
ditions will occur; thus, more fuel will be consumed. Therefore, changing to
another path even if it is longer is preferred. In addition, it is important to
point that in some cases, an accident might happen on a highway, but the
vehicles do not need to change the path since it is still the best in terms of
fuel consumption. This issue depends on the traﬃc density.
Deﬁning the region of interest: In this work, the target region is 2 km be-
yond the nearest exit. However, the idea of region of interest needs to be
investigated. In references [23,24,25], the region of interest has been deter-
mined base on the type of warning messages and traﬃc density. Moreover,
two metrics have been deﬁned to study the eﬀect of data dissemination: com-
munication cost and additional travel cost. Communication cost is the least
number of vehicles involved in retransmitting, while the additional travel
cost is the cost diﬀerences associated with the paths calculated before and
after propagating information.
6
Conclusions
This work was inspired by the fact that, to date, many drivers are becoming
increasingly concerned with the rising fuel cost. Geocast protocols in vehicular
ad hoc networks can play a primary role in reducing vehicle fuel expenditure.
The previous geocast protocols seek to improve mainly the performance of the
communications systems; however, they do not show how the improvement of
the communication system performance will aﬀect the performance metrics that
are meaningful to both the scientiﬁc community and the general public, such as
fuel consumption.
Vehicles’ fuel consumption was measured using VT-Microscopic model in four
scenarios. When Hwy 1 does not have any accidents, it has the minimum fuel
consumption compared to Hwy 2. On the other hand, when an accident occurs,
vehicles on Hwy 1 waste more fuel. It was found that up to the point 973 traveling
vehicles, the economical route is Hwy 1 even though an accident occurred on it.
In this case, a geocast protocol that can inform drivers when the alternative
route becomes more economic is needed.
Simulation results demonstrated the impact of increasing the number of trav-
eling vehicles on the total vehicles’ fuel consumption. At 2000 traveling vehicles,

Applying Vehicular Ad Hoc Networks for Reduced Vehicle Fuel Consumption
227
total vehicles’ fuel consumption can be reduced up to 20% if the EG protocol
is used allowing vehicles to change to the economical route. It is expected that
the percentage of the vehicles’ fuel consumption reduction will increase with an
increasing number of traveling vehicles.
In the future, the traﬃc density of Hwy 1 with an accident will be reduced
when vehicles begin to change their route to Hwy 2. Hwy 1 will become the
economical route again after a certain time. Therefore, the impact of having
an intelligent EG, which is able to give updated information periodically, will
be studied. Another future work will be done for the purpose of evaluating the
eﬀect of expanding the geocast range on vehicles’ fuel consumption on a highway
segment that has no exit.
Acknowledgments. This research was supported by a research grant from the
Natural Science and Engineering Research Council (NSERC) of Canada and by
a scholarship from King Saud University, Saudi Arabia.
The authors wish to thank Professor Hesham Rakha of Civil and Environ-
mental Engineering at Virginia Tech for providing them a spreadsheet with the
VT-Micro model coeﬃcients.
References
1. Wiebe, E.C.: Gasoline Prices in Parts of Canada between 1998 and 2009,
http://climate.uvic.ca/people/ewiebe/car/fuel_price.html
2. Oﬃce of Energy Eﬃciency, http://oee.nrcan.gc.ca
3. Fuel Consumption Guide, Government of Canada, Ottawa (2009)
4. Chowdhury, M.A., Sadek, A.W.: Fundamentals of intelligent transportation sys-
tems planning. Artech House Publishers (2003)
5. Barth, M., Boriboonsomsin, K., Vu, A.: Environmentally-friendly navigation. In:
Proceedings of the IEEE Intelligent Transportation Systems Conference, pp. 684–
689 (2007)
6. Kuriyama, H., Murata, Y., Shibata, N., Yasumoto, K., Ito, M.: Congestion Allevi-
ation Scheduling Technique for Car Drivers Based on Prediction of Future Conges-
tion on Roads and Spots. In: Proceedings of the IEEE Intelligent Transportation
Systems Conference, pp. 910–915 (2007)
7. Yamashita, T., Izumi, K., Kurumatani, K., Nakashima, H.: Smooth Traﬃc Flow
with a Cooperative Car Navigation System. In: Proceedings of the fourth interna-
tional joint conference on Autonomous agents and multiagent systems, pp. 478–485
(2005)
8. Moustafa, H., Zhang, Y.: Vehicular Networks: Techniques, Standards, and Appli-
cations. Auerbach Publications, Taylor & Francis Group, USA (2009)
9. Sichitiu, M.L., Kihl, M.: Inter-vehicle communication systems: a survey. IEEE
Communications Surveys & Tutorials 10(2), 88–105 (2008)
10. Broustis, I., Faloutsos, M.: Routing in Vehicular Networks: Feasibility, Modeling,
and Security. International Journal of Vehiculer Technology 2008, Article ID 267513
(2008)
11. Li, F., Wang, Y.: Routing in vehicular ad hoc networks: a survey. IEEE VT Mag-
azine 2(2), 12–22 (2007)

228
M. Alsabaan, K. Naik, and A. Nayak
12. Environmental Protection Agency: User’s Guide to Mobile 6, Mobile Source Emis-
sion Factor Model, Ann Arbor, Michigan (2002)
13. California Air Resources Board: User’s Guide to EMFAC, Calculating emission
inventories for vehicles in California (2007)
14. Barth, M., An, F., Younglove, T., Scora, G., Levine, C., Ross, M., Wenzel, T.:
Comprehensive modal emission model (CMEM), version 2.0 user’s guide, Riverside,
California (2000)
15. Ahn, K., Rakha, H.: Field Evaluation of Energy and Environmental Impacts of
Driver Route Choice Decisions. In: Proceedings of the IEEE Intelligent Trans-
portation Systems Conference, pp. 730–735 (2007)
16. UK Department for Environment, Food & Rural Aﬀairs: Cold Start Advanced-user
guide. iss. 1 (2008)
17. Van Aerde, M., Associates, Ltd.: INTEGRATION release 2.30 for Windows: Users
guide. Fundamental features, vol. I (2005)
18. Van Aerde, M., Associates, Ltd.: INTEGRATION release 2.30 for Windows: Users
guide. Fundamental features, vol. II (2005)
19. Ahn, K., Rakha, H., Trani, A., Van Aerde, M.: Estimating vehicle fuel consumption
and emissions based on instantaneous speed and acceleration levels. Journal of
Transportation Engineering 128(2), 182–190 (2002)
20. Centre for Applied Informatics (ZAIK), Institute of Transport Research, German
Aerospace Centre: Sumo - simulation of urban mobility,
http://sumo.sourceforge.net
21. Rakha, H., Ahn, K., Trani, A.: Comparison of MOBILESa, MOBILE6, VT-
MICRO, and CMEM Models for Estimating Hot-Stabilized Light-Duty Gasoline
Vehicle Emissions. Canadian Journal of Civil Engineering 30, 1010–1021 (2003)
22. Car-to-car communication consortium, http://www.car-to-car.org
23. Rezaei, F., Naik, K., Nayak, A.: Investigation of Eﬀective Region for Data Dissemi-
nation in Road Networks Using Vehicular Ad hoc Network. In: IEEE International
Conference on Fuzzy Systems, Korea (2009)
24. Rezaei, F., Naik, K., Nayak, A.: Propagation of Traﬃc Related Information in
Road Networks. In: Canadian Society for Civil Engineering (2009)
25. Rezaei, F.: Investigation of Eﬀective Region for Warning Data Dissemination in
Vehicular Networks. A thesis Presented to the University of Waterloo, Department
of Electrical and Computer Engineering, Waterloo, Ontario, Canada (2009)

An Analytical Model for Dynamic
Inter-Operator Resource Sharing
in 4G Networks
Ahmet Cihat Toker, Fikret Sivrikaya, Nadim El Sayed, and Sahin Albayrak
Technische Universit¨at Berlin,
Ernst-Reuter Platz 7 Berlin, Germany
{ahmet-cihat.toker,sahin.albayrak,fikret.sivrikaya,
nadim.elsayed}@dai-labor.de
http://www.dai-labor.de
Abstract. Realtime resource sharing between operators is an eﬃcient
approach to deal with the ever increasing wireless traﬃc. Simple and
closed form formulas relating cooperation terms to the network perfor-
mance measures are needed, so that the operators can take cooperate/not
cooperate decisions, and track the beneﬁts/losses of sharing. Analytical
solutions to resource sharing the among access networks of a single opera-
tor exist. However the fact that operators will not share network internal
data calls for solutions that are separable from each other. In this paper
we provide such a closed form formulation, validate it with simulations
and propose a simple negotiation mechanism between two operators uti-
lizing this model. This simple model can be extended to model more
complex interactions between operators, interaction between more than
two operators, or can be used to evaluate long term cooperation policies.
Keywords: resource sharing, 4G, performance modeling, cooperation,
traﬃc engineering.
1
Introduction
Telecommunication network management practices are strongly rooted in the
monopolistic telecom operators. The liberalization of the operators has only
changed the landscape in a way that there were multiple closed operators rather
than one closed operator. This trend has had related implications on the network
operator and the user side.
The operator networks are usually centrally managed, poorly integrated with
outside components, and strictly isolated from external access. On the other hand
the Internet was born out of the need for integrating networks. The exposure
of users to the proliﬁc Internet services means that similar service models will
have to be provided by the next generation telecom networks. The clash between
these two opposite approaches poses important challenges for network operators.
This is due to the fundamental risk associated with their networks turning into
mere bit-pipes. In order for future telecom networks to be economically viable,
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 229–240, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

230
A.C. Toker et al.
they should provide a similar user experience with Internet services, albeit in
a more managed and reliable manner. Here lies the grand challenge of the so-
called Telco 2.0 operators. The operators have to oﬀer even more data intensive
applications on their networks to make their operations proﬁtable. This comes
in a time, when the increasing data traﬃc is starting to hurt user experience,
and pose itself as the biggest risk facing the operators.
There are three strategies that the network operators and broadband service
providers can follow under these circumstances. The competitive strategy would
be aggressive capacity expansion. The cooperative option would be: (i) employing
untapped networking resources, such as community wireless networks or sharing
new spectrum or (ii) establishing strategic partnerships with other operators to
share the existing spectrum.
Clearly, capacity extension is a brute-force solution to the problem and can
only be extended to the point when the investment costs drive access prices
beyond market prices. The cooperation with community networks is also viable,
under the condition that community network fees are below a threshold value
[1]. This necessities a wide community participation, which would down the
connection fees. Finally, even though possible, the current agreements between
operators are oﬀ-line in nature and can only be reached after long legal and
ﬁnancial negotiations by the involved parties. Despite the diﬃculties associated
with cooperative solutions, they represent a far more long term and sustainable
solution compared to capacity extension. We believe that the dynamic resource
sharing between two licensed or virtual operators and cooperation between a
licensed operator and a wireless community network are of similar nature, and
provide the best solution to the operators capacity scarcity problem.
An important issue for the dynamic resource sharing is the ownership of dig-
ital identities of individual users. Current practices in the telecommunications
area tie the users to a single operator even though the number of players in the
market has long been growing. The users tend to manually combine their sub-
scriptions to multiple operators in order to take simultaneous advantage of their
diﬀerent oﬀers that are suited for a variety of services. User-centric networking
is a new approach to the relation to the ownership of identity in next generation
networking. In its most generic sense, the user-centric view in telecommunica-
tions considers that the users are free from subscription to any one network
operator and are the real owners of their identities. This allows them to dy-
namically choose the most suitable transport infrastructure from the available
network providers for their terminal and application requirements. An important
result of this is the churn rates approaching session initiation rates. Therefore
dynamic resource sharing is an important aspect of user-centric networking.
The PERIMETER project [2], funded by the European Union under the
Framework Program 7 (FP7), has been investigating such user-centric network-
ing paradigm for future telecommunication networks. The key innovation in
PERIMETER project is the introduction of a distributed database implemented
on user terminals, which stores the Quality of Experience (QoE) reports about
networks generated by the users. The users utilize the reports of current and past

An Analytical Model for Dynamic Inter-Operator Resource Sharing
231
users to decide which network to choose. This database can be used to overcome
important challenges for the introduction of dynamic resource sharing, which
are the lack of analytical solutions to model load balancing and the information
asymmetry and lack of transparency between diﬀerent operators. The availabil-
ity of the open quality of experience database to the operators alleviates the
information asymmetry problem. The operators can exploit this database in
order to gather information about the other operators, with whom they may
cooperate. With this information at hand, the operators need analytical models
that can be evaluated in real-time in order to take the cooperation decisions.
These analytical models should be separable, meaning that the operators should
be able to verify them by using information available to both of them.
In this paper we present such an analytical model its veriﬁcation based on
discrete event simulations. In the next section we introduce the requirements
and motivations of the analytical model. We formulate the problem formally in
Section 3 and compare it to the state of the art in Section 4. We propose the
analytical model in Section 5 and apply it to a single class resource sharing sce-
nario in Section 6. We provide a simple negotiation mechanism for the operators
in Section 7 and conclude with the evaluation of our model with simulation in
Section 8.
2
Motivation and Requirements
The problem we are addressing is the minimization or avoidance of possible
degradation in user perceived quality of experience in an access network as the
number of users increases in an open user-centric network environment. The
delay a session request experiences is a common performance measure that can
be used to handle a variety of service types. Therefore, we choose the delay as
the performance metric of dynamic resource sharing mechanisms.
The method with which the avoidance or minimization is achieved is by bor-
rowing network layer resources from an access network that belong to another
operator (community, virtual or real operator). In a user-centric environment,
the operators have to ﬁnd additional resources, not to degrade the QoE, other-
wise the users will be moving away to alternative operators. What would be the
incentives for the donor operator to lend some of its resources to the borrower?
A quick answer would be that if the donor operator is under-utilized at that
particular point of time, then it could increase its utilization to a point where
it still can serve its current users, thereby increasing its revenues. However, the
challenge of user centricity comes from the fact that users can instantaneously
decide on the operators they choose. The donor operator may choose to ignore
the borrowing operator, in an attempt to drive the QoE in the borrowing net-
work down, and gain more users. Therefore the dynamics of the resource sharing
between two operators become strategy dependent, and not trivial.
The aforementioned problem is not speciﬁc to the dynamic resource shar-
ing in user-centric networking. As Dohler discusses in [3], the problem is not
only technological, but also strategic. We adhere to Dohler’s approach, in which

232
A.C. Toker et al.
he proposes that the success of any cooperative solution to any communica-
tions problem is more possible if the cooperation decisions are taken by software
agents, and the beneﬁts of these decisions are clear to the owners of these agents.
Therefore we derive a simple and intuitive formula for the relation between the
average delay and cooperation parameters.
Another important requirement to the model is the separability. Generally, the
state space describing two independent systems interacting with each other is two
dimensional. The solution of the probability distribution of such a distribution
requires the knowledge of the states of the independent systems. This is not
possible for two cooperating operators, since the operators will be reluctant to
share their operation information with each other. If the performance metrics can
be calculated by openly available information, without requiring the knowledge
about the operative status of the other operators we call such a model separable.
Final requirement on the modeling approach is the capability to eﬃciently
model heterogenous wireless networks. It is foreseen that the 4G networks will
be composed of heterogenous wireless access technologies. Therefore the model
should be able to accommodate a variety of them.
3
Problem Formulation
We consider a location where the users have two wireless operators to choose
from, operator A and operator B. The users generate requests with exponential
inter arrival times of mean 1/λ, which have sizes that are also exponentially
distributed with mean 1/μ. Since these users are not in contractual agreements
with the operators, they can choose either one of the operators with probabilities
PA and PB. The operators utilize a call admission control (CAC) mechanism that
block incoming requests with probability Pb, or transfer to the other operator
with a probability PT . The operators employ these techniques in order to provide
a maximum delay guarantee to the users given by Dmax.
Considering the requirements we listed in Section 2, we have decided to use
Queueing Networks [4] as the modeling framework. Queueing networks are a
generalization of the classical queueing systems. They are concerned with the
performance modeling of systems that are composed of interconnected queueing
stations. The main goal of the queueing networks is the derivation of the joint
probability distribution of the number of jobs in each service station. One of the
most important results of Queueing Networks is the Baskett, Chandy, Muntz and
Palacios (BCMP) theorem, named after the researchers who jointly developed
the theorem in [5]. The theorem states that the joint probability distribution
of a queueing network can be written as the product of marginal probability
distributions of the individual service stations. Furthermore, each service stations
behaves like a traditional M/M/1 queue, with a modiﬁed input traﬃc rate,
reﬂecting the network topology. This formulation satisﬁes the separability and
simplicity requirements.
Processor Sharing (PS) is a queueing service discipline ﬁrst analyzed by Klein-
rock in [6]. It is an idealized version of the Round Robin (RR) service discipline,

An Analytical Model for Dynamic Inter-Operator Resource Sharing
233
in which the service quanta that each job in the queue receives is inﬁnitesimal.
In the limit the server operates in a manner that each job receives a service
rate equivalent to the overall server capacity divided by the number of jobs in
the queue. We have chosen to model individual radio access networks by PS
discipline. The intuition that the wireless access network can be modeled as a
service station that simultaneously serve the active users can be traced back to
the work of Telatar [7]. Furthermore, it has been shown that the Weighted Fair
Queueing service discipline, used widely in radio network base stations, approx-
imates processor sharing when the packet size is small compared to the session
size [8].
4
State of the Art
Historically, queueing networks have provided very attractive models for a wide
variety of communication networks and applications running on these networks.
Early applications include [9] Conway’s queueing model for the performance
evaluation of Signalling System 7 (SS7).
One of the most active application of queueing networks to the 4G network-
ing has been the work of Kouvastos et al. Kouvastos ﬁrst employed a queue-
ing network model to analyze the performance of ATM Asynchronous Transfer
Mode switches developed for the ISDN Integrated Services Digital Networkss
[10]. The author applied the same queueing model ﬁrst to performance analysis
of GSM/GPRS base stations [11], and subsequently to the performance analy-
sis of hypothetical 4G base stations accommodating diﬀerent services [12] and
[13]. A hypothetical 4G cell is modeled as a combination of three service cen-
ters. Voice calls are handled by a classical Erlang loss system, the data calls are
handled by a PS (Processor Sharing) system and ﬁnally streaming calls are han-
dled by a FCFS (First Come First Serve) system. The authors make use of the
maximum entropy principle to ﬁnd a product form approximation that yields
a closed form solution [14]. Similar to this work, we use PS service stations to
model heterogeneous access networks. Our model not only takes into account
multiple access networks, but also diﬀerent operators. Furthermore, we use the
BCMP method to provide exact solution to the queueing model. Our solution is
also computationally more eﬃcient than this, as it does not include any recursive
solutions.
In [15]the authors provide an analysis of a hierarchically integrated WLAN
and cellular network by employing queueing networks. In a hierarchical integra-
tion architecture [16], the WLAN cells are used as high-speed hot spots, and
are carefully positioned as an overlay on the cellular infrastructure. The natural
question that arises in such an architecture is when the overlay will be used. As-
suming that the network operator has the ﬁnal say on this decision, the authors
employ the ”WLAN ﬁrst” resource allocation policy. In this policy the calls are
admitted to the WLAN cell as long as the capacity is not reached. Both of these
works consider a single operator owning the heterogeneous access networks.
To the best of our knowledge BCMP networks have not been applied in mod-
eling 4G networks.

234
A.C. Toker et al.
5
Analytical Model
In order to develop a tractable, closed form, and separable solution for the delay
performance metric we use the BCMP theorem on the queueing network depicted
in Figure 1. Each wireless operator is modeled by a tandem of queues. The ﬁrst
stage IS queue, represents the CAC decision making. The traﬃc exiting the ﬁrst
stage queue enters the PS queue which jointly models the shared air interface
and access router that connects the base station to the backbone.
The ﬁrst step of applying the BCMP model is the solution of the traﬃc
equations. For open networks they can be expressed as:
λi = λ · p0,i +
4

j=1
λj · pj,i,
iϵ{1, 2, 3, 4}.
(1)
λ is the external arrival rate, and pi,j represent the routing probability between
service stations i and j where the subscript 0 denotes the external environment.
One can calculate the utilizations of individual service stations by using the
service rates of the individual server stations, ρi =
λi
μi . Once these modiﬁed
utilizations are computed, the joint probability distribution can be written as:
π(n1, n2, n3, n4) =

i=1,3
e−ρi ρni
i
ni! ·

i=2,4
(1 −ρi)ρni
i .
(2)
Let us substitute the routing probabilities of the BCMP model with the system
parameters we deﬁned in Section 3. The indices i = 1, 2 describe the operator
A and i = 3, 4 the operator B. p0,1 and p0,3 correspond to the users operator
preferences, PA and PB respectively. p3,2 and p1,4 are the transfer ratios of the
operators, PT,A and PT,B. The CAC mechanisms reject calls with probabilities
Pb,A and Pb,B, hence we have P1,0 = Pb,A and P3,0 = Pb,B. By using total
probability principle we obtain P1,2 = 1−PT,A−Pb,A and P3,4 = 1−PT,B −Pb,B.
Let us analyze the utilization of the PS part of the individual operators. Given
Fig. 1. The queueing network model

An Analytical Model for Dynamic Inter-Operator Resource Sharing
235
that the users generate requests whose sizes are distributed according to an
exponential distribution of average μ bits and the operators access networks have
a capacity of CA and CB bits per second, we have μ1 = μ · CA and μ2 = μ · CB.
Thus we have:
ρP S,A = (PA · (1 −PT,A −Pb,A) + PB · PT,B) ·
λ
μCA .
ρP S,B = (PB · (1 −PT,B −Pb,B) + PA · PT,A) ·
λ
μCB .
(3)
These equations show an intuitive and linear relationship between operator uti-
lizations and transfer and blocking probabilities. Speciﬁcally, an operator may
increase its utilization by accepting additional traﬃc from the other operator,
or may decrease its utilization by transferring traﬃc to the other operator or
by increasing the CAC level. When increasing the utilization, the condition
ρP S,A, ρP S,B < 1 should be considered to guarantee stability. These utilizations
can be used to ﬁnd the expected delay conditioned on the request size x given
in bits at the PS side of the operators. These are given by:
DP S,i(x) =
x/Ci
1 −ρP S,i
.
DP S,B(x) =
x/CB
1 −ρP S,B
.
(4)
6
Application of the Model to Single Class Resource
Sharing
In a single service class scenario, the sharing of resources to avoid overload sit-
uations becomes mutually exclusive with the under-utilization situations. This
means that the borrowing operator will not donate resources, and the donor oper-
ator will not borrow resources from each other. Let us arbitrarily assign operator
A to be the donor operator, and operator B to be the borrowing operator. In
other words Operator B sends some of its traﬃc to, and hence borrow resources
from, operator A. Furthermore, let us assume the average service demand x is
ﬁxed.
In this case the delays become a function of the transfer probability PT and
the operator preferences of the users PA, PB. This means that the exchange of
resources is one way, i.e. PT,A = 0. The problem is characterized the relative
operator preferences of the users PA, PB; the delay thresholds Dmax,A, Dmax,B;
and the CAC probabilities Pb,A and Pb,B.
In a resource sharing scenario, the donor operator has to ﬁnd the amount of
traﬃc it can accept, equivalently the amount of resources it can donate, without
increasing the expected delay of the already accepted users above a threshold

236
A.C. Toker et al.
Dmax,A, described by (5). The donor operator ﬁnds the maximum PT value that
satisﬁes this condition described by which we term PT,D:
DA(PT,D) = DP SA(PT,D) = Dmax,A.
(5)
On the other side the borrowing operator has to calculate how much traﬃc it
should transfer, or equivalently how much resources it should borrow, in order
to reduce the expected delay in its access network to a threshold. However, it
also has to consider the increase in delay it induces on the donor operators. We
are considering a seamless scenario, in which the transferred users are not aware
of the fact that their session request is served by an alternative operator. The
borrowing operator should not load the donor operator excessively, since this
excessive loading would increase the delays experienced by the transferred users,
who would associate this with the borrowing operator.
There are two approaches for considering this aspect. One can consider the
maximum of the delays in the two networks as in (6).
max {DP SA(PT,R), DP SB(PT,R)} ≤Dmax,B(x).
(6)
An alternative is to consider the average delay as experienced by the users of
the borrowing operator, as in (7).
E {DB(PT,R)} = (1−PT,R)·DP SB(PT,R)+PT,R·DP SA(PT,R) ≤Dmax,B(x). (7)
In both cases, the borrowing operator chooses the transfer probability value PT,R
that minimizes the delay. We choose the second option, as it is more fair to the
users of the borrowing operator. In order to calculate E {DB(PT,R)} operator
B has to be able to calculate the utilization of operator A, which requires the
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
PT,R
Delay (s)
 
 
CB = 7000
CB =5000
CB = 3000
Fig. 2.
The variation of average delay versus transfer probability for CA
=
5000bps, DA,init = 0.6

An Analytical Model for Dynamic Inter-Operator Resource Sharing
237
knowledge of Pb,A. This is an private information that is not available to operator
B. However this can be solved by utilizing the open user experience database,
proposed by PERIMETER. We assume operator B is able to gather the initial
average delay in operator A, DA,init, by exploiting the database. From this value
it can calculate initial value of the utilization via (4). Employing (3) allows us
to formulate the utilization of operator A after resource sharing, by consulting
only to the publicly available average delay. Using this value one is able to write
down the expression for E {DB(PT,R)}:
E {DB(PT,R)} =
x
CA · PT,R
x
CADA,init −PBλ
μCA · PT,R
+
x
CB · (1 −PT,R)
1 −PB(1 −Pb,B −PT,R)
λ
μCB
. (8)
Equation 8 is plotted for varying borrowing operator capacities in Figure 2. It
can be observed that the amount of reduction in the average delay is directly
proportional with the capacity of the donor operator. For all cases the delay is
a rational function of transfer probability with global minima.
7
A Simple Negotiation Mechanism for Single Class
Resource Sharing
A simple mechanism for the interaction between the operators based on these
values works as follows. The borrowing operator calculates the PT,R value that
minimizes (8) and communicates this to the the donor operator. Donor oper-
ator checks if this value increases its average delay above the limit. If PT,R is
acceptable for the donor operator, it replies with the same probability, which
corresponds to an agreement. If not, the donor operator replies with the maxi-
mum PT,D that it can handle calculated from (5). An agreement is reached on
this value. If the ﬁnal agreed PT is not enough for the borrower operator to reach
its delay goals, it has to increase its blocking probability in order to meet its
delay goals.
8
Simulation Results
We simulated the system presented in Fig. 2 using OPNET Modeler 15.0 [17].
Users initiate session requests with an inter-arrival time of 0.5 seconds. These
requests go according to the user preferences to the respective operator. At
the operator the IS server is modeled as a decision entity without any queuing
or delay, which forwards the session requests either to the operator’s own PS-
server or to the other operator’s server, according to the transfer probability.
The PS-server is modeled as a queue, which handles the session requests with
the PS-queuing discipline. We implemented the PS queuing discipline in three
steps.
According to [18] the PS-queuing discipline has one distinguishing property,
that is, the delays are linear to the service demands for a given utilization as in

238
A.C. Toker et al.
500
1000
1500
2000
2500
3000
0
0.2
0.4
0.6
0.8
1
1.2
Packet length in Bits
Delay (s)
Delay linearity for PB = 0.8 and PT = 0.6
 
 
Borrower Theory
Borrower Simulation
Donor Theory
Donor Simulation
Fig. 3. Linearity of delay with packet size
Equations (4). In order to verify the correctness of our PS implementation, we
checked if the developed model possess this property.We simulated the system
with λ = 0.5, the average demand 1
μ set to 2500, the server capacities for operator
A and B set to 5000, the user preference of B set to PB = 0.8 and the transfer
probability PT B = 0.6. The results depicted in Fig. 3, which veriﬁes that our
OPNET models conformance with PS discipline.
We ran the simulations for the varying values of transfer probability and gath-
ered the average delay in individual networks. In Fig. 4 we verify our theoretical
results by comparing compare the simulation values to the theoretical results de-
rived in (4). It can be observed that increasing transfer ratio reduces the delay
in a diminishing manner, that is the eﬀectiveness of dynamic resource sharing
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
PT,B
Delay (s) 
 average delay at the donor operators (A)
 
 
Theory, PB = 0.8
Theory, PB = 0.6
Simulation, PB = 0.8
Simulation, PB = 0.6
(a) Donor
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
PT,B
Delay (s)
 average delays at borrower operators (B)
 
 
Theory, PB = 0.8
Theory, PB = 0.6
Simulation, PB = 0.8
Simulation, PB = 0.6
(b) Borrower
Fig. 4. Comparison of analytical results with simulation

An Analytical Model for Dynamic Inter-Operator Resource Sharing
239
reduces with higher transfer probabilities. Furthermore, increasing the transfer
ratio above the diminishing return boundary, the vicinity of PT,B = 0.4 in this
case, increases the donor operator delay substantially. This justiﬁes the use of
average delay over the operators deﬁned in (8) as the decision criterium. For
the value of PT,B = 0.4, we are able to reduce borrower delay from 2.5 to 1.25
seconds, while increasing the donor delay from 0.6 to 0.8 seconds. We believe
that this demonstrates the viability of dynamic resource sharing.
9
Conclusion and Future Work
In this paper we have presented a simple and separable analytical model for
the dynamic resource sharing between operators. The simplicity of the model
allows operators to take real-time decisions to cooperate or not to cooperate
with other operators in the same geographic area, where coverage is shared. The
separability of our solution allows each operator to base their decisions on openly
available data. We have not exploited the separable solution to its fullest extent
and have presented a solution based on the mean value of delays experienced by
end users. We have shown analytically and using simulations, that the average
expected delay can be reduced with dynamic resource sharing.
We will extend this work in order to exploit the separable solution to delay
metric. Speciﬁcally, we will propose a solution in which operators take their
decisions not based on average delay values, but on probabilities that delay will
exceed the thresholds. Furthermore, we intend to extend this model to include
multiple service classes. In this scenario a simultaneous borrowing and lending
of resources will be of interest.
References
1. Manshaei, M.H., Marbach, P., Hubaux, J.P.: Evolution and market share of wireless
community networks, pp. 508–514 (June 2009)
2. Toker, A.C., Cleary, F., Fiedler, M., Ridel, L., Yavuz, B.: Perimeter: Privacy-
preserving contract-less, user centric, seamless roaming for always best connected
future internet. In: Proceedings of 22th World Wireless Research Forum. (2009)
3. Dohler, M., Meddour, D.E., Senouci, S.M., Saadani, A.: Cooperation in 4g - hype
or ripe? IEEE Technology and Society Magazine 27(1), 13–17 (2008)
4. Bolch, G., Greiner, S., de Meer, H., Trivedi, S.: Queueing Networks and Markov
Chains: Modeling and Performance Evaluation, 2nd edn. Wiley, Chichester (2006)
5. Baskett, F., Chandy, K.M., Muntz, R.R., Palacios, F.G.: Open, closed, and mixed
networks of queues with diﬀerent classes of customers. J. ACM 22(2), 248–260
(1975)
6. Kleinrock, L.: Time-shared systems: a theoretical treatment. J. ACM 14(2), 242–
261 (1967)
7. Telatar, I.E., Gallager, R.G.: Combining queueing theory with information theory
for multiaccess. IEEE Journal on Selected Areas in Communications 13(6), 963–969
(1995)

240
A.C. Toker et al.
8. Parekh, A.K., Gallager, R.G.: A generalized processor sharing approach to ﬂow
control in integrated services networks: the single-node case. IEEE/ACM Transac-
tions on Networking 1(3), 344–357 (2002)
9. Queueing network modeling of signaling system No.7. In: Global Telecommunica-
tions Conference, 1990, and Exhibition. ’Communications: Connecting the Future’,
GLOBECOM 1990. IEEE, Los Alamitos (December 1990)
10. Kouvatsos, D.: Performance modelling and cost-eﬀective analysis of multiservice
integrated networks 9(3), 127–135 (August 2002)
11. Kouvatsos, D.D., Awan, I., Al-Begain, K.: Performance modelling of gprs with
bursty multiclass traﬃc 150(2), 75–85 (May 2003)
12. Performance Modelling of a Wireless 4G Cell under GPS Scheme. In: The 2005
Symposium on Applications and the Internet Workshops, Saint Workshops 2005
(January 2005)
13. Performance modeling of a wireless 4G cell under a GPS scheme with hand oﬀ. In:
Next Generation Internet Networks (April 2005)
14. Jaynes, E.T.: Prior probabilities. IEEE Transactions on Systems Science and Cy-
bernetics 4(3), 227–241 (2007)
15. Modelling Cellular/Wireless LAN Integrated Systems with Multi-Rate Traﬃc Us-
ing Queueing Network. In: 4th International Conference on Wireless Communica-
tions, Networking and Mobile Computing, WiCOM 2008 (October 2008)
16. Salkintzis, A.K.: Interworking techniques and architectures for wlan/3g integration
toward 4g mobile data networks. IEEE, Wireless Communications 11(3), 50–61
(2004); IEEE Personal Communications
17. Opnet: Opnet oﬃcial website, http://www.opnet.com
18. Kleinrock, L.: Queueing Systems. Computer Applications, vol. II. Wiley Inter-
science, New York (1976)

Laboratory research in telecom systems 6’Tel@ SUP’COM 
rim.haddad@yahoo.ca
ridha.bouallegue@supcom.rnu.tn
The use of multiple adaptive antennas has been recognized as a key technology to 
significantly improve the spectral efficiency of next generation, multiuser wireless 
communication networks. Several smart antenna systems have been proposed, and 
demonstrated at the base station (BS) of the wireless communication system, and 
these have shown that significant increases in capacity are possible [1],[2]. 
In principle, a MUD receiver allows constructive combination of multi-path signals 
received by an array of antennas while minimizing the MAI’s contribution. Besides, 
providing the average error probability for K users with DOA’s (Direction of Arrival) 
uniformly distributed within a symmetric support around the array broadside has been 
derived for chip and phase asynchronous DS-CDMA system 
. 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 241–253, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 

The Bit Error Rate (BER) is considered to be one of the most important performance 
measures for communication systems and hence it has been extensively studied. The 
exact analytical evaluation of the probability of error in DS-CDMA system, is still an 
open subject.  
Gaussian approximations of the Multiuser Interference (MUI) are used to reduce
 the
 
problem, and to be tractable namely when the average performance is of interest.  
However, the accuracy of the Gaussian approximation technique depends on
 the
 
specific configuration of the system.  
When analyzing the BER performance of DS-CDMA systems, the interference 
sources, namely the Multiple Access Interference (MAI) are commonly assumed to be 
Gaussian distributed, for coherent receivers over Nakagami-m fading channels [5], 
and for linear Minimum Mean Square Error (MMSE)-MUD. 
Hence in this paper, we will derive an accurate BER formula for Nakagami – faded 
DS-CDMA in the context of asynchronous transmission and we proposed to adapt the 
Gaussian approximations to antenna array systems by properly accounting for the 
noise and the MUI after beamforming. This is carried out by considering an 
approximation of the angular gain in adaptive antenna array systems. To the best of 
the author’s knowledge, the term adaptive antenna array implies the beamforming of 
the receiving system. 
 
 
In our paper, we propose a novel approach to evaluate the average probability of error 
by considering an approximation of the spatial filter. The angular gain function is 
approximated by a fixed beamwidth     , for the passband and by an attenuation   . 
For the user of interest, all the remaining     interferers are partitioned into in-
beam / out-beam MUI. The analytical formulas ofthe average error probability is 
counted differently for the in-beam/out-beam interferers and we validate our research 
for single antenna systems. 
We organize the rest of the paper as follows. In section 2 we introduce our system 
model, followed by the array gain approximation in section 3. The average probability 
of error with adaptive antenna and computation results are provided in section 4 and 
section 5 respectively. We conclude in section 6. yx
242 
R. Haddad and R. Bouallegue 


In Fig. 2 the spatial response of the array due to an incident plane wave from    
direction is modeled by the array steering vector  (  ) [6], [7].Wherein  (  ) is the 
    vector that describes the array response to the DoA   , and the     element for 
a linear of half-wavelength spaced antennas is: 
, (  )-     (  (   )      )   
(3) 
At the receivers, the     vector of that received signal for the     user. 
  ( )   (  )  ( )    ( )   
(4) 
  ( )   (  ) ∑  
 
 
∑      (         )  
 
   
 
(5) 
The received signal of the   users’ signal can be written as: 
 ( )  ∑
  ( )
 
   
   ( ) .   
(6) 
The noise  ( ) is assumed to be a zero-mean temporally and spatially uncorrelated 
Gaussian process, with  ,    -     ( ),    is the power of the AWGN. We 
assume that the spatial correlation of noise arising from intercell interference is not 
considered. 
After the beamformingwith the      spatial filter    for the  
  
user, the output
 of the  
  
 filter matched to   ( ) is: 
   
⁄ , -  ∫  
 (         ) ( )      
(7) 
    , -  ∑∑  
  (    ) ∑        .(   )           /
 
   
       , -  
 
 
   
 
(8) 
 (    )    
   (  ), where   
  is the weight vector and H denotes Hermitian 
transpose: is the spatial gain of the beamformer designed for the angle  .And 
    ( )  ∫  (   )  
 ( )    is the cross correlation function between signatures. 
The matched filter output contains the self interferenceISI and the MUI. The self 
interference ISI is written by this equation: 
    
   ( )   (    ) ∑  
 ∑
        .(   )           /
 
   
 
 .               (9) 
The MUI is: 
    
   ( )  ∑
∑  
  (    ) ∑
        .(   )           /
 
   
 
 
       
.    
(10) 
244 
R. Haddad and R. Bouallegue 

The noise power after the Beamforming is    
⁄
 . To make the analytic evaluation of 
error probability computation feasible, we have to assume that the waveforms are 
randomly generated on each BPSK symbol with outcome uniform on *     +. 
Beamforming is the most common spatial processing technique that an antenna array 
can use [8]. The signals from different antenna elements are weighted and “summed” 
to “optimize” the quality of the signal (Fig. 3). 
With the proper selection of Beamforming criterion (in our case we choose the 
conventional beamforming), it’s possible to point beam towards the desired user 
and/or place nulls in the direction of the interferers. 
In a multipath environment, the MUD receiver employs the maximal ratio combining 
by filtering each path of each user’s signal with the corresponding space filter and 
then combining the match filter outputs
. 
 
Fig. 3 Block diagram of a smart antenna system
 
All the paths experience the same level of noise and interference. The decision 
variable after the beamforming is assumed to be obtained by the maximal ratio 
combining. 
  , -  ∑
        , -
 
   
 .               
(11) 
The use of adaptive antenna array in MUD receivers is expected to be effective 
mainly in reducing intercell interference. However, to evaluate the advantage of the 
array processing in reduction of intercell interference a simplified model of CDMA 
system can be viewed as a synchronous model with an increased number of fictitious 
users. Therefore, the synchronous model CDMA for one path,                    
for   , is considered here as a useful example to gain insights on the array processing 
gain in MUD [01]. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
.
 
Exact BER Performance of Antenna Array-Based Receiver 
245 

  , -  ∑
∑  
 
 
     (   
⁄ )    
 
   
     , -; for         . 
(12) 
where           (      ). 
The delays      and phases      are i.i.d random variables uniformly distributed.The 
used model for frequency selective multipath channel for mobile communication is 
the Nakagami model, the amplitudes |    | are independent random variables with 
Nakagami probability density function. 
 (|    |       
 ) where: 
 (|    |       
 )  
   |    |    
    
   ( )
   ( 
 |    | 
    
 
).               
(13) 
 ( ) is the gamma function. The fading parameter  (     ) spans different 
distributions: The Rayleigh distribution for     while in the limit     the fading 
channel converges to a no-fading channel. 
 
To simplify, we assume that the power delay profile of the path strengths is the same 
for all the users: 
    
 
     
    (  (   )).  
(14) 
The parameter       is the decay rate, the total average fading power. 
  
 ( )  ∑
    
 
 
   
     
  (   ). 
(15) 
It 
depends 
on 
the 
decay 
rate 
and 
the 
number 
of 
paths: 
 (   )   ,     (   )- ,     (  )-
⁄
. For     the power delay profile is 
uniform and (   )   . 
For the user of interest (say   ) conventional beamforming weights are considered 
    (  )  
⁄
 , the angular gain function for conventional beamforming is 
 (   
⁄
)   (  )  ( )  
⁄
. 
To simplify the computations in the following, the gain | (   
⁄
)|  can be 
approximated by a piece-line function|   (   
⁄
)|
 that models the pass-band or in-
beam with support     (  )  ,      (  )       (  )- with a linear gain and 
the out-beam (with support  ̅   (  ) with an equivalent attenuation   ) [11]. 
The received CDMA signal after spatial filtering and dispreading is: 
246 
R. Haddad and R. Bouallegue 














1
0
1
1
1
2
1
2
1
1











DOA
DOA
BW
eq
f
for
f
for
G
 
(16) 
The beamwidth    (  ) depends on the number of antennas M and   . The support 
      (  )   ̅   (  )  ,        
⁄
⁄
- covers all the admissible DoAs in a 
mobile system with three-cell sectorization the angles range in       
For small deviations from the broadside (for          ), the beamwidth     
optimized for      can be transformed into the beamwidth for any value    by [12]: 
   (  )  
   
     . 
(17) 
By using this model, the in-beam interferers can be easily evaluated for any DoA of 
interest   as in those in the support     (  ).The approximation parameters are 
expected to overestimate the average BER when employing minimum variance 
beamforming with a small number of users K. 
We consider a system with    and  ,          -, the error probability 
  ,       
⁄
  -   ,   | (  )|
⁄
- depends only on the number of in-beam 
interferers   and not on their DoAs. 
The average error probability reduces: 
( )  ∑
  0   
  
    1
   
    
∫
 (  )  (  )   
   
⁄
    
⁄
 . 
(18) 
 (  )is the probability of having   in-beam interferers. The probability of an In-beam 
interferer: 
 (   (  ))  ∫
  ( )
      (  )
      (  )
  .               
(19) 
Depends on the beamwidth    (  )          
⁄
.For DOAs uniformly distributed 
within the support,         
⁄
⁄
-, the probability (19) depends on the beamwidth 
   (  ) compared to the overall support  . 
 (    )      (  )
  
 
 
     
 
(20) 
Where   
    
   depends on the beamforming criterion exploited. 
The gain can be approximated by: 
 
Exact BER Performance of Antenna Array-Based Receiver 
247 

The average BER (18) becomes: 
 ( )  ∑
   
   
    
(   
  
)  (      )  0  
  
    1.               
(21) 
Where  (      )  (   )       for matched filter receiver (receiver with   
 branch) the BER depends on the interference: [11] 
  0  
  
    1   *.
  
    
  
 (  )
  /
   
⁄
+.               
(22) 
 , -is the Gaussian Q-function.The average level of interference (for chip and phase 
asynchronous) , -: 
  
 (  )   
     
             
  
 
(23) 
Counted for the   in-beam and the (      )out-beam users. For small values of 
η, the number of in-beam interferers is small. The more accurate approximation in 
[13] could be used for receivers with adaptive antennas. 
For MMSE-MUD receivers with adaptive antenna arrays, the BER depends only on 
the in-beam users as the attenuated out-beam users are approximately decoupled. The 
evaluation of BER for the user of interest [4]: 
  .   
  
    /   [(
   
   
   
     
   
    
     . 
     /)
  
⁄
].               
(24) 
The Gaussian approximation is based on the asymptotic analysis for      is used 
to derive (24), it is accurate enough to yield meaningful conclusions in the evaluation 
of the benefits of the MMSE-MUD in a receiving system employing an adaptive array 
system, and it decouples the effects of the spreading signatures in the analysis. 
The array gain   
⁄  for the AWGN is included either in the BER for matched-filter 
receiver (22) and in MMSE-MUD (24). In order to compare the benefits of the spatial 
diversity for a varying number of antennas, the SNR is measured after the 
Beamforming as       
⁄
. 
The average BER for any arbitrary DoAs distribution is conceptually dependent on 
 (      (  )) to get  (  )for each   and then averaging with respect to   (18). 
The DoA’s distribution might become a design parameter. 
This occurs when MAI reduction is obtained by dynamically assigning the radio 
resource according to their DoAs. 
In this section, we extend the concepts discussed above to the Nakagami m-fading 
case. The average performance needs to be evaluated from (18) by averaging with the 
248 
R. Haddad and R. Bouallegue 

Pdf of fading. The error probability  (  
⁄
   ) depends on the instantaneous fading 
SNR  (     ) and on the number of in-beam interferers   . 
Under the Gaussian approximation, the SNR at the decision variable has been 
evaluated by Eng and Milsten [5] for propagation over Nakagami fading channels. 
 (     )  ∑
|    |
  (
  
     
  (   )
 
      (      )
  
   
  (   )  
  
)
  
 
   
.   (25) 
Let the number of resolvable multipaths of each user be  , for     (no multipath), 
the last term in (25) vanishes as  (   )   ; if     it is  (   )    and the overall 
level of MUI and ISI (self interference) increases according to the degree of time 
diversity:  . 
The error probability depends on the desired user    and the number of in-beam 
interferers    is  (   
⁄
   ) can be evaluated in term of effective SNR: [5] 
 ̅(  )  
 (    )
   (   )  (
  
   
    (   )
 
      (      )
  
 
 (   )  
  
)
  
.             (26) 
For the case of Rayleigh fading (ξ=1) and flat delay profile    . 
 
The equation  (   
⁄
   )    (  
     
⁄
   ): 
  (  
     
⁄
   )   
 [   (  ) ∑.  
 / (    
 
)
 
   
   
] 
(27) 
where: 
 (  )  √
 ̅(  )
   ̅(  ).               
(28) 
To evaluate the average BER, we sum over the cardinality of the in-beam set and 
average with respect to    like the equation (24). 
For uniformly distributed DoAs, it
 
is:
 
 ( )  ∑
   (   
  
)  (      ) (  
     
⁄
   )
   
    
.               
(29) 
which is dual equation of (21) for fading channels. 
In this section, we carry out the simulated results that have been obtained by applying 
a model of spatial filter that allows describing the angular gain function: the in-beam 
is approximated with a fixed beam-width and the out-beam with an equivalent 
attenuation. 
 
Exact BER Performance of Antenna Array-Based Receiver 
249 

The approximations used to evaluate the average performance for DS-CDMA 
receivers are validated here with numerical results. In the proceeding simulations, the 
following assumptions are made for all the users: 
- 
The multipath channel parameters: number of paths  , DoAs    are independent 
and uniformly distributed. 
- 
All the users are received with the same average power as in a system with a 
perfect power control. 
- 
The   omni-directional antennas are arranged in a uniform linear array half 
wavelength spaced based on the conventional beamforming. 
- 
The SNR is measured after the beamforming so that the   
⁄
 array gain for 
AWGN is implicitly compensated to focus the attention on the gain arising from 
spatial diversity. 
Fig. 4 illustrates the average probability of 2D-RAKE receivers versus SNR (solid 
line) for     branch,     antennas,     and 16 users (N=31) for no-fading 
channel. 
Numerical experiments show that MUD receiver performance degrades down to 
single antenna receiver when the spatial filtering is in no way effective in reducing the 
MUI (for low SNR). Single user/single antenna (M=1) lower bound for AWGN (Fig. 
4) and Rayleigh fading channel (Fig. 5). Simulation results are close to the analytical 
results proposed in this paper by accounting for the effects of the in-beam/out-beam 
interferers.Fig.5 shows the average BER using the same parameters as in Fig.4 in 
Rayleigh fading channel. 
 
Fig. 4. Average BER versus SNR for no fading channel for L=1 path, M=8 antennas, K=8 and 
16 users
 
Simulation results are close to the analytical results proposed in this paper
 by
 
accounting for the effects of the in-beam / out-beam interferers. 
Besides, the receivers based on the adaptive arrays demonstrate the efficacy in 
reducing the overall interference level. 
250 
R. Haddad and R. Bouallegue 

 
Fig. 5. Average BER versus SNR in Rayleigh fading channel for L=1 path, M=8 antennas, K=8 
and 16 users
 
 
Fig. 6 Average BER versus the number of users K for no fading and Rayleigh fading channels 
for L=1 path, M=8 and 16 antennas
 
Exact BER Performance of Antenna Array-Based Receiver 
251 
. 

 
Fig. 7.Average BER versus SNR for 2D-RAKE; M=8 antennas and K=16, single antenna 
(M=1) RAKE and single user/single antenna in multi-path Rayleigh fading channels (L=1, 2, 4)
 
The influence of the interference can be reduced by decreasing the probability of 
having an in-beam interferer  . This is illustrated in Fig.6, where the BER 
performance is shown by varying the number of users for no-fading and Rayleigh 
fading channels. 
From Fig.6, it can be noticed that the same average BER can be obtained by doubling 
the number of antennas   and the number of users   either for no-fading or fading 
channels. 
Therefore, as a rule, the average performance (or the level of the in-beam 
interference) remains the same as far as the ratio     remains constant. This 
conclusion can be shown even when we neglect the influence of the out-beam 
interference. 
Fig. 7 investigates the average BER for MUD receiver with adaptive antennas for 
propagation over L paths frequency selective Rayleigh fading channel (for L=1, 2, 4) 
versus SNR (M=8, K=16, N=31). 
Fig. 7 shows either for varying SNR (       
   
⁄
) or increasing number of 
users, that multipath channels (large L) and angular diversity can improve satisfactory 
performance when exploited jointly. 
252 
R. Haddad and R. Bouallegue 

The effects of the spatial filter for adaptive uniform linear arrays can be described by 
an equivalent model that is useful to evaluate the average interference and to find an 
accurate approximation of the average probability of error. We showed in simulations 
that we can influence in the number of antennas and users to evaluate BER. The BER 
is expected to fall well below the optimum when more number of antennas is used, 
but with a trade-off of increased cost and complexity. Besides, we noticed that the 
average performance (or the level of the in-beam interference) remains the same as far 
as the ratio   
⁄  remains constant.In a continuation of the study, which we have 
already started, is to evaluate the average BER in forward link (base to mobiles) 
where each user experience the same temporal channel for all the received signals.  
References 
1. Piazza, D., Kirsh, N.J., Forenza, A.: Design and Evaluation of a reconfigurable antenna 
array for MIMO systems. IEEE Transactions on Antennas and Propagation 56(3), 869–881 
(2008) 
2. Haddad, R., Bouallègue, R.: BER Performance in DS-CDMA Systems using a 
Beamformer. In: IEEE International Symposium on Industrial Electronics, vol. 1, pp. 585–
588 (July 2006) 
3. Haddad, R., Bouallègue, R.: BER Performance in Space-Time Processing receiver using 
Adaptive Antennas over Rayleigh Fading Channels. In: Proc. IEEE International 
Conference on signal Processing and Communication, pp. 1483–1486 (November 2007) 
4. Haddad, R., Bouallègue, R.: BER Performance of Smart Antenna Systems Operating over 
Rayleigh fading Channels. In: Proc. IEEE Wireless Days 2008, pp. 1–5 (November 2008) 
5. Eng, T., Milstein, L.B.: Coherent DS-CDMA performance in Nakagami multi-path fading. 
IEEE Trans. Commun. 43, 1134–1143 (1995) 
6. Zhang, L., Xie, N.: Non linear optimization for adaptive antenna array receivers with a 
small data-record size. Wireless Communications & Mobile Computing 9, 239–249 (2009) 
7. Li, H.J., Liu, T.Y.: Comparison of beamforming techniques for W-CDMA communication 
systems. IEEE Transactions on Vehicular Technology 52(4), 752–760 (2003) 
8. Choi, S., Choi, J., Im, H.J., Choi, B.: A novel adaptive beamforming algorithm for antenna 
array CDMA systems with strong interferers. IEEE Transactions on Vehicular 
Technology 51(5), 808–816 (2002) 
9. Wang, X., Poor, H.V.: Space-Time Multiuser Detection in Multi-path CDMA channel. 
IEEE Trans. on Comm. 47, 185–190 (1999) 
10. Khalaj, B.H., Paulraj, A., Kailath, T.: 2D RAKE receivers for CDMA cellular system. In: 
Proc. IEEE GLOBECOM conf., pp. 400–404 (December 1994) 
11. Spagnolini, U.: A simplified model for probability of error in DS-CDMA systems with 
adaptive antenna arrays. In: Proc. IEEE International Conference on Communications 
(ICC), pp. 2271–2275 (June 2001) 
12. Johnson, D.H., Dudgeon, D.E.: Array Signal Processing: Concepts and Technique. 
Prentice-Hall, Englewood Cliffs (1993) 
13. Holtzman, J.: A simple accurate method to calculate spread-spectrum multiple access error 
probabilities. IEEE Trans. on Comm. 40, 461–464 (1992) 
 
Exact BER Performance of Antenna Array-Based Receiver 
253 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 254–265, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
On the Time between Successive Multi-path Discoveries 
and Hop Count Per Multi-path for Zone-Disjoint Routing 
in Mobile Ad Hoc Networks 
Natarajan Meghanathan 
Jackson State University 
Jackson, MS 39217, USA 
natarajan.meghanathan@jsums.edu 
Abstract. We propose an algorithm to compute zone-disjoint multi-paths for 
mobile ad hoc networks and compare its performance with that of the single-
path minimum-hop routing algorithm as well as with the node-disjoint multi-
path routing algorithm proposed in our earlier work. Simulation results indicate 
that the number of zone-disjoint paths per multi-path set can be at most 2, 
which is far lower than the number of node-disjoint paths per multi-path set. 
Also, the time between zone-disjoint multi-path discoveries is far lower than the 
time between node-disjoint multi-path route discoveries and can be at most 10% 
more than the time between single minimum-hop path route discoveries. How-
ever, there is no appreciable difference in the average hop counts per zone-
disjoint multi-path set and node-disjoint multi-path set and they can be only at 
most 12% more than the average minimum hop count determined using single-
path routing. 
Keywords: Mobile Ad hoc Networks, Multi-Path Routing, Zone-Disjoint, 
Node-Disjoint, Route Discoveries, Hop Count, Simulation. 
1   Introduction 
A Mobile Ad hoc Network (MANET) is a dynamic distributed system of wireless 
nodes that move independent of each other and have limited battery charge. The wire-
less medium is shared among the nodes and the channel is of limited bandwidth. 
MANET nodes often operate in limited transmission range to conserve battery charge 
as well as to reduce interference that may result due to long-range transmissions. As a 
result, routes in MANETs are often multi-hop in nature and a node assists its peers in 
route discovery and data propagation. MANET routing protocols are predominantly of 
two different types: reactive and proactive. Reactive or on-demand routing protocols 
use a network-wide flooding of Route Request (RREQ) messages to build and main-
tain routes, but only when needed. Proactive routing protocols tend to maintain routes 
between all pairs of nodes all the time and hence in the presence of a dynamically 
changing topology, incur considerable route maintenance overhead compared to on-
demand protocols [2]. Hence, most of the recent research in MANETs is on reactive 
on-demand routing and we restrict ourselves to this routing technique in this paper. 

On the Time between Successive Multi-path Discoveries and Hop Count per Multi-path 
255 
 
On-demand routing protocols incur higher route discovery latency and also incur 
frequent route discoveries in the presence of a dynamically changing topology, typical 
of MANETs. Recent research has started to focus on multi-path routing for reduced 
route discovery overhead, fault tolerance and load balancing. Three different multi-
path routing strategies have been proposed in the MANET literature: link-disjoint 
routing, node-disjoint routing and zone-disjoint routing. For a given source s and 
destination d, the set of link-disjoint s-d routes comprises of paths that have no link 
present in more than one constituent s-d path. A set of node-disjoint s-d routes com-
prises of paths that have no node (other than the source and destination) present in 
more than one constituent s-d path. A set of zone-disjoint s-d routes comprises of 
paths such that an intermediate node in one path is not a neighbor node of an interme-
diate node in another path. Multi-path on-demand routing protocols tend to compute 
multiple paths between a source-destination (s-d) pair, in a single route discovery 
attempt. A new network-wide route discovery operation is initiated only when all the 
s-d paths fail. Throughout the paper, we use the terms ‘path’ and ‘route’ inter-
changeably. They mean the same. 
Multi-path routing can provide fault tolerance, load balancing and higher aggregate 
bandwidth. However, to achieve maximum level of fault tolerance and effectively 
balance the load as well as maximize the aggregate bandwidth across the multiple s-d 
paths, it is important to discover paths that are as independent as possible. Metrics such 
as correlation factor and coupling factor are used to calculate the relative degree of 
independence among the multiple paths [6]. The coupling factor, measured only for 
node-disjoint paths, indicates the number of links connecting two node-disjoint paths. 
The correlation factor, measured for all the three types of multi-paths, is the average 
number of nodes that are blocked from receiving data on one of the paths when a node 
in the other path is transmitting. Network topology and channel characteristics (meas-
ured using correlation and coupling factors) can severely limit the gain obtained from 
multi-path routing [7].  
The three multi-path routing strategies can be ranked as follows, in the increasing 
order of independence: link-disjoint routing, node-disjoint routing and zone-disjoint 
routing. It may not be always possible to simultaneously send data across two link-
disjoint paths or two node-disjoint paths as the transmission of data in a link that is 
part of one path may require a node that is part of another path to remain idle (con-
trolled by the channel access mechanism). It has been observed earlier [11] that larger 
the correlation factor between two node-disjoint paths, the larger will be the average 
end-to-end delay for both the paths and also the larger will be the difference in the 
end-to-end delay along the two paths. If two link-disjoint or node-disjoint routes are 
physically close enough to interfere with each other during data communication, the 
nodes in these multi-path routes may constantly contend for accessing the shared 
channel and the multi-path routing protocol may end up performing worse than any 
single-path routing protocol [9]. In [10], the authors argue that benefits (improvement 
in throughput and reduction in end-to-end delay) obtained with multi-path routing 
become insignificant with respect to single-path routing if we take into consideration 
the interference between the multiple paths and the cost of discovering these paths. 
Thus, multi-path routing may not be a sound strategy if the constituent multiple paths 
suffer interference among themselves. This motivates the need to consider zone-
disjoint path routing as a potentially effective multi-path routing strategy because the 

256 
N. Meghanathan 
 
intermediate nodes of the zone-disjoint paths are not located in the neighborhood of 
each other and zone-disjoint paths have a coupling factor of zero. Zone-disjoint rout-
ing has a correlation factor of 2 while using omni-directional antennas [9] and 0 with 
directional antennas [8]. 
The tradeoff between the three multi-path routing strategies is that for a given net-
work topology, network density and node mobility, the number of link-disjoint paths 
may be more than the number of node-disjoint paths, which may be more than the 
number of zone-disjoint paths. In an earlier work [4], we have observed that the  
number of link-disjoint paths can be as large as 40% more than the number of node-
disjoint paths and the time between two successive link-disjoint multi-path route dis-
coveries can be at most 30% more than the time between two successive node-disjoint 
multi-path route discoveries. However, no such results are available comparing the 
number of node-disjoint paths vis-à-vis the number of zone-disjoint paths as well as 
the time between two successive node-disjoint multi-path route discoveries vis-à-vis 
the time between two successive zone-disjoint multi-path route discoveries. This 
forms the motivation for our research in this paper. 
The rest of the paper is organized as follows: Section 2 proposes the algorithm 
used in this paper to determine the sequence of zone-disjoint routes over the duration 
of a source-destination (s-d) session. Section 3 describes the simulation environment, 
illustrates the simulation results comparing zone-disjoint routing with node-disjoint 
routing and interprets the results. Section 4 concludes the paper. 
2   Algorithm to Determine the Set of Zone-Disjoint Paths 
We now propose the algorithm used to determine the sequence of zone-disjoint paths 
in our simulation studies. Let G (V, E) be the graph representing a snapshot of the 
network topology collected at the time instant in which we require a set of zone-
disjoint routes from a source node s to a destination node d. Note that V is the set of 
vertices (nodes) and E is the set of edges (links) in the network. We say there is a link 
between two nodes if the distance between the two nodes is less than or equal to the 
transmission range of the nodes. We assume all nodes are homogeneous, use omni-
directional antennas and have identical transmission range. 
Figure 1 illustrates the algorithm to determine the set of zone-disjoint s-d routes, 
PZ, on a graph G collected at a particular time instant. We use the Dijkstra O(n2) algo-
rithm [3] to determine the minimum hop s-d path in a graph of n nodes. If there exist 
at least one s-d path in G, we include the minimum hop s-d path p in PZ. We then 
remove all the intermediate nodes (nodes other than the source s and destination d) 
that were part of the minimum hop s-d path p and also all their neighbor nodes from 
the original graph G to obtain the modified graph G’ (V’, E’). We determine the 
minimum hop s-d path in the modified graph G’, add it to the set PZ and remove the 
intermediate nodes that were part of this s-d path and all their neighbor nodes to ob-
tain a new updated graph G’(V’, E’). We then repeat this procedure until there exists 
no more s-d paths in the network. The set PZ  is now said to contain the set of zone-
disjoint s-d paths in the original network graph G. Note that when we remove a node v 
from a network graph, we remove all the links associated with the node (i.e., links 

On the Time between Successive Multi-path Discoveries and Hop Count per Multi-path 
257 
 
belonging to the adjacency list Adj-list(v)). The algorithm to determine the set of 
node-disjoint s-d routes, PN, on a network graph is very similar to the zone-disjoint 
multi-path routing algorithm described here. The only difference is that we only  
remove the intermediate nodes that were part of the minimum hop s-d path from a 
network graph. The procedure is repeated until there exist no more s-d paths in the 
network. More information about the algorithm to determine the set of node-disjoint 
paths can be found in [4]. 
The zone-disjoint and node-disjoint multi-path routing algorithms used in this re-
search could be implemented in a distributed fashion in ad hoc networks by flooding 
the RREQ message, letting the destination node to select and inform about the zone-
disjoint and node-disjoint routes to the source through the RREP messages. The 
source could send the data packets to the destination using these routes in the increas-
ing order of hop count (i.e., use the least hop count route until it exists and then use 
the next highest hop count path as long as it exists and so on) or distribute the data 
packets simultaneously through several paths with paths that have minimum hop 
count being used more. 
 
 
Input: Graph G (V, E), Source s and Destination d 
Output: Set of Zone-Disjoint Paths PZ 
Auxiliary Variables: Graph G’ (V’, E’) 
Initialization: G’ (V’, E’) Å G (V, E), PZ Å φ 
 
Begin  
1 
While ( ∃at least one s-d path in G’) 
2 
      p  Å Minimum hop s-d path in G’ 
3 
     PZ Å PZ U {p} 
4 
      
∀
∈
≠
∈
−
vertex u p u s d
edge e Adj list u
,
,
,
,
( )
G’(V’, E’) Å G’(V’-{u}, E’-{e}) 
5 
      
∀
∈
≠
∈
≠
∈
−
vertex u p u s d
v Neighbor u v s d
edge e Adj list v
,
,
,
( ),
,
, '
( )
G’(V’, E’) Å G’(V’-{v}, E’-{e’}) 
6 
end While  
7 
return PZ 
End  
 
Fig. 1. Algorithm to Find the Set of Zone-Disjoint s-d Paths in a Network Graph 
 

258 
N. Meghanathan 
 
3   Simulation Environment and Results 
We ran our simulations in both square and rectangular network topologies of dimen-
sions 1000m x 1000m and 2000m x 500m respectively. Both these network topolo-
gies have the same area. The average neighborhood size is determined as follows: N* 
πR2/A, where N is the number of nodes in the network, R is the transmission range of 
a node and A is the network area. The transmission range per node used in all of our 
simulations is 250 m. The simulations on both the square and rectangular network 
topologies were conducted for different values of the average node densities repre-
senting the neighborhood size: 10 neighbors per node (50 nodes, low density), 20 
neighbors per node (100 nodes, moderate density) and 30 neighbors per node (150 
nodes, high density). By running the simulations in both square and rectangular net-
work topologies, we intend to study the impact of the variation in node distribution 
for a fixed value of average node density. Square topologies will have more uniform 
node distribution compared to rectangular topologies.  
We use the Random Waypoint mobility model [1], one of the most widely used 
models for simulating mobility in MANETs. According to this model, each node 
starts moving from an arbitrary location to a randomly selected destination with a 
randomly chosen speed in the range [vmin .. vmax]. Once the destination is reached, the 
node stays there for a pause time and then continues to move to another randomly 
selected destination with a different speed. We use vmin = 0 and pause time of a node 
is also set to 0. The values of vmax used are 10, 30 and 50 m/s representing low mobil-
ity, moderate mobility and high mobility levels respectively.  
We obtain a centralized view of the network topology by generating mobility trace 
files for 1000 seconds under each of the above simulation conditions. We sample the 
network topology for every 0.25 seconds. Note that, two nodes a and b are assumed to 
have a bi-directional link at time t, if the Euclidean distance between them at time t 
(derived using the locations of the nodes from the mobility trace file) is less than or 
equal to the wireless transmission range of the nodes. Each data point in Figures 2 – 4 
is an average computed over 10 mobility trace files and 15 s-d pairs from each of the 
mobility trace files. The starting time for each s-d session is uniformly distributed 
between 1 to 20 seconds. 
3.1   Determining the Sequence of Multi-path and Single-path Routes 
We determine the sequence of zone-disjoint routes over the entire simulation time 
period as follows: When an s-d path is required at a given sampling time instant and 
there is none known, run the zone-disjoint multi-path algorithm to determine the set 
of zone-disjoint routes for a given s-d pair. We assume s-d routes in a zone-disjoint 
path set are used during the succeeding sampling time instants in the increasing order 
of the hop count. In other words, the s-d route with the next highest hop count is used 
as long as it exists and so on. We thus persist with the determined zone-disjoint multi-
path set of s-d routes as long as at least one path in the set exists. We repeat the above 
procedure till the end of the simulation time period. 

On the Time between Successive Multi-path Discoveries and Hop Count per Multi-path 
259 
 
To determine the sequence of node-disjoint routes over the entire simulation time 
period, we follow a similar procedure except that we use the node-disjoint multi-path 
algorithm [4] to determine the set of node-disjoint s-d routes at a given sampling time 
instant, use these routes in the increasing order of the hop count as long as at least one 
path in the set exists, and repeat this procedure till the end of the simulation time 
period. We also determine the sequence of minimum-hop single-path s-d routes by 
running the minimum hop Dijkstra algorithm [3] on the network graph generated at 
the simulation time instant when an s-d route is used until it exists and the procedure 
is repeated over the duration of the network simulation session. The sequence of 
minimum-hop single-path s-d routes is used as a benchmark to evaluate the relative 
increase in the time between multi-path route discoveries vis-à-vis single-path discov-
eries and the corresponding increase in the average hop count for multi-path zone-
disjoint and node-disjoint routes. 
3.2   Performance Metrics 
We measure the following performance metrics: 
• 
Average Number of Paths per Multi-Path Set: This is the number of disjoint paths 
(zone-disjoint or node-disjoint, depending on the algorithm) determined during a 
multi-path route discovery, averaged over all the s-d sessions. In the case of sin-
gle-path routing, the number of paths determined per route discovery is 1. 
• 
Average Time between Successive Multi-Path/Single-Path Route Discoveries: 
This is the time between two successive broadcast multi-path (or single-path) dis-
coveries, averaged over all the s-d sessions over the simulation time. As we opt 
for a route discovery only when all the paths in a multi-path set fails, this metric 
is a measure of the lifetime of the set of multi-paths and a larger value is preferred 
for a routing algorithm or protocol. 
• 
Average Hop Count per Multi-Path/Single-Path: The average hop count for a 
given routing strategy is the time-averaged hop count of the individual paths that 
are used in a sequence over the entire simulation time period. For example, if the 
sequence of minimum hop paths used comprise of a 2-hop path for 2 seconds, 
then a 3-hop path for 3 seconds and then again a 2-hop path for 5 seconds, the 
time-averaged hop count for the routing strategy comprising the sequence of 
minimum 
hop 
paths 
over 
a 
10-second 
simulation 
time 
period 
is 
(2*2+3*3+2*5)/10 = 2.3 seconds.  
3.3   Average Number of Paths per Multi-path Set 
The number of paths per multi-path set for zone-disjoint routing has been observed to 
be significantly smaller than that observed for node-disjoint routing. With zone-disjoint 
routing, when the intermediate nodes of the minimum hop path and also their neighbor 
nodes are removed from the network graph, the probability of an alternate path be-
tween the source and destination decreases significantly. For a given network topology, 
node density and multi-path routing algorithm, the number of paths per multi-path set 
remains almost the same at all the three levels of node mobility. We also observe that 
for a given node density, level of node mobility and multi-path routing algorithm, the 

260 
N. Meghanathan 
 
number of paths per multi-path set in square networks is more than that obtained for 
rectangular networks. This can be attributed to the uneven distribution (distribution of 
more nodes in one direction compared to the other direction) in rectangular networks 
compared to square networks. In square networks, for a given node velocity, as we 
increase the average node density from 10 neighbors per node (50 node network) to 30 
neighbors per node (150 node network), the average number of paths per multi-path set 
for zone-disjoint routing increases from 1.38 to 1.97 (43% increase); while the average 
number of paths per multi-path set for node-disjoint routing increases from 4.44 to 
18.41 (315% increase). On the other hand, in rectangular networks, for a given node 
velocity, as we increase the node density from 10 to 30 neighbors per node, the average 
number of paths per multi-path set for zone-disjoint routing increases from 1.07 to 1.18 
(10% increase); while the average number of paths per multi-path set for node-disjoint 
routing increases from 2.57 to 11.96 (365% increase). It is important to note that on the 
average, there can be at most two zone-disjoint paths in a square network and only one 
zone-disjoint path in a rectangular network (i.e., nothing more than a single-path) when 
we operate these networks with node density as large as 30 neighbors per node. 
    
      
 
       Fig. 2.1. 1000m x 1000m, vmax = 10 m/s            Fig. 2.2.  2000m x 500m, vmax = 10 m/s 
 
    
      
 
       Fig. 2.3. 1000m x 1000m, vmax = 30 m/s             Fig. 2.4.  2000m x 500m, vmax = 30 m/s 
 
    
      
 
        Fig. 2.5. 1000m x 1000m, vmax = 50 m/s             Fig. 2.6.  2000m x 500m, vmax = 50 m/s 
Fig. 2. Average Number of Paths per Multi-Path Set 
 

On the Time between Successive Multi-path Discoveries and Hop Count per Multi-path 
261 
 
3.4   Average Time between Successive Multi-path/ Single-path Discoveries 
In square network topologies, for a given level of node mobility, the time between 
successive node-disjoint multi-path route discoveries is 1.82 (node density of 10 
neighbors per node) to 3.26 (node density of 30 neighbors per node) times more than 
the time between successive zone-disjoint multi-path route discoveries. In rectangular 
network topologies, for a given level of node mobility, the time between successive 
node-disjoint multi-path route discoveries is 1.59 (at 10 neighbors per node) to 3.67 
(at 30 neighbors per node) times more than the time between successive zone-disjoint 
multi-path route discoveries. On the other hand, for a given level of node mobility, in 
square network topologies, the time between successive zone-disjoint multi-path route 
discoveries is 1.14 (at 10 neighbors per node) to 1.42 (at 30 neighbors per node) times 
more than the time between successive single-path route discoveries. In rectangular 
network topologies, the time between successive route discoveries for single-path 
routing and zone-disjoint routing is almost the same with the latter being at most 10% 
more than the former. Thus, we cannot significantly reduce the route discovery con-
trol overhead with zone-disjoint multi-path routing.  
    
      
 
      Fig. 3.1. 1000m x 1000m, vmax = 10 m/s             Fig. 3.2.  2000m x 500m, vmax = 10 m/s 
 
    
      
 
     Fig. 3.3. 1000m x 1000m, vmax = 30 m/s             Fig. 3.4.  2000m x 500m, vmax = 30 m/s 
 
    
      
 
     Fig. 3.5. 1000m x 1000m, vmax = 50 m/s              Fig. 3.6.  2000m x 500m, vmax = 50 m/s 
Fig. 3. Average Time between Successive Multi-Path/ Single-Path Discoveries 
 

262 
N. Meghanathan 
 
Even though we observe a direct correlation between the number of paths per 
multi-path set and the time between successive multi-path route discoveries, for both 
zone-disjoint and node-disjoint routing, the increase in the number of paths per multi-
path set with increase in node density does not yield a corresponding proportional 
increase in the time between successive multi-path route discoveries. For example, in 
square network topologies, even though the number of zone-disjoint paths per multi-
path set increases from 1.38 to 1.97 with increase in node density from 10 to 30 
neighbors per node, the time between successive zone-disjoint multi-path route dis-
coveries can be at most 20% larger. Similarly, for node-disjoint path routing, as we 
increase node density from 10 to 30 neighbors per node, even though the absolute 
value for the number of paths per multi-path set increases from 4.4 to 18.4, the time 
between successive multi-path route discoveries increases only by at most 120%.  
For rectangular network topologies, the increase in the time between successive 
multi-path route discoveries with increase in node density from 10 to 30 neighbors per 
node is relatively low compared to that incurred with square network topologies. This 
can be also attributed to the relatively unstable nature of the minimum-hop routes in 
rectangular network topologies compared to square network topologies. The mini-
mum-hop routes in rectangular network topologies have a larger hop count (explained 
more in Section 3.5) compared to those incurred with square network topologies. 
Each of the links in a minimum hop path has almost the same probability of failure in 
both square and rectangular network topologies [5]. As a result, since there are more 
hops, the probability of failure of a minimum hop path is more in rectangular network 
topologies compared to square network topologies.  
The impact of the topology shape on the stability of the routes is also vindicated by 
the relatively rapid decrease in the lifetime per multi-path set in rectangular network 
topologies with increase in the level of node mobility compared to that incurred in 
square network topologies. For a given node density, as we increase the maximum 
node velocity from 10 m/s to 50 m/s, the time between successive multi-path route 
discoveries reduces at most by a factor of 3.3 in square network topologies and at 
most by a factor 0f 4.0 in rectangular network topologies. 
3.5   Average Hop Count per Multi-path/ Single-path 
For any level of node mobility and node density, the average hop count per zone-
disjoint multi-path set and node-disjoint multi-path set can be respectively at most 
10% and 12% (for square network topologies) and 3% and 8% (for rectangular net-
work topologies) more than that of the minimum hop count obtained via single-path 
routing. Thus, there is relatively insignificant difference in the hop count incurred by 
the zone-disjoint or node-disjoint and the single-path routing strategies.  
In terms of the absolute numbers, for a given level of node mobility, routing strat-
egy and network topology, the average hop count incurred with the multi-path routes 
as well as the single-path routes decreases with increase in node density. The decrease 
is more predominant (by a factor of at most 10%) with the single-path routing strategy 
compared to the two multi-path routing strategies (only by a factor of at most 5%). 
This can be attributed to the fact that the constituent routes of the zone-disjoint multi-
paths and node-disjoint multi-paths may not be minimum hop routes. Also, both the 
number of paths per multi-path set and the time between successive multi-path route 

On the Time between Successive Multi-path Discoveries and Hop Count per Multi-path 
263 
 
discoveries increase with increase in node density. The above observation is espe-
cially more relevant for node-disjoint multi-path routing.  
We also observe that for a given level of node mobility and node density, the aver-
age hop count per routing strategy in a rectangular network topology can be 40% - 
50% more than that incurred in a square network topology. This can be attributed to 
the fact that in the rectangular network topologies the nodes are more predominantly 
distributed in one-dimension (actually in the longer of the two dimensions) and this 
contributes to the relatively larger hop count compared to the square network topolo-
gies where nodes are more uniformly distributed [5]. The relatively larger hop count 
contributes to the unstable nature of the minimum hop routes in rectangular network 
topologies compared to those discovered in square network topologies.  
    
      
 
       Fig. 4.1. 1000m x 1000m, vmax = 10 m/s             Fig. 4.2. 2000m x 500m, vmax = 10 m/s 
 
    
      
 
      Fig. 4.3. 1000m x 1000m, vmax = 30 m/s             Fig. 4.4. 2000m x 500m, vmax = 30 m/s 
 
    
      
 
      Fig. 4.5. 1000m x 1000m, vmax = 50 m/s             Fig. 4.6. 2000m x 500m, vmax = 50 m/s 
Fig. 4. Average Hop Count per Multi-Path Set/ Single-Path 
4   Conclusions and Future Work 
The high-level contribution of this paper is a simulation-based analysis to evaluate the 
tradeoffs between zone-disjoint routing, node-disjoint routing and single-path routing 
for mobile ad hoc networks. The zone-disjoint path routing algorithm proposed in this 

264 
N. Meghanathan 
 
paper can be used to arrive at benchmarks for the time between successive zone-
disjoint multi-path discoveries, the number of zone-disjoint paths per multi-path set 
and the hop count per zone-disjoint multi-path set. Based on the simulation results 
obtained in this paper, one could conclude that, on the average, the number of zone-
disjoint paths can be as large as 2 and the time between successive zone-disjoint 
multi-path discoveries can be at most 42% (for square topologies) and 10% (for rec-
tangular topologies) more than that incurred with single-path routing. On the other 
hand, the time between successive node-disjoint multi-path route discoveries can be 
as large as 225%-267% more than that incurred with zone-disjoint routing. The corre-
sponding increase in the average hop count per node-disjoint multi-path set is only 
12% more than that of the minimum hop single-paths. Also, the worst-case difference 
in the average hop count per zone-disjoint multi-path set and node-disjoint multi-path 
set is within 5% and this is relatively insignificant compared to the significant reduc-
tion in the route discovery overhead that can be potentially brought about through 
node-disjoint routing. As future work, we would develop distributed routing protocols 
based on our zone-disjoint and node-disjoint routing algorithms and compare the two 
routing protocols with respect to metrics such as throughput and end-to-end delay. We 
will study the benefits and drawbacks associated in simultaneously routing through at 
most two zone-disjoint paths vis-à-vis routing through multiple node-disjoint paths. 
References 
1. Bettstetter, C., Hartenstein, H., Perez-Costa, X.: Stochastic Properties of the random Way 
Point Mobility Model. Wireless Networks 10(5), 555–567 (2004) 
2. Broch, J., Maltz, D.A., Johnson, D.B., Hu, Y.C., Jetcheva, J.: A Performance of Compari-
son of Multi-hop Wireless Ad hoc Network Routing Protocols. In: 4th International Con-
ference on Mobile Computing and Networking, pp. 85–97. ACM, Dallas (1998) 
3. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms, 2nd 
edn. MIT Press/ McGraw Hill, New York (2001) 
4. Meghanathan, N.: Stability and Hop Count of Node-Disjoint and Link-Disjoint Multipath 
Routes in Ad hoc Networks. In: 3rd International Conference on Wireless and Mobile 
Computing, Networking and Communications (WiMob). IEEE, New York (2007) 
5. Meghanathan, N.: Impact of Range of Simulation Time and Network Shape on the Hop 
Count and Stability of Routes in Mobile Ad hoc Networks. IAENG International Journal 
of Computer Science, 36(1) (2009) 
6. Mueller, S., Tsang, R.P., Ghosal, D.: Multipath Routing in Mobile Ad hoc Networks: Is-
sues and Challenges. In: Calzarossa, M.C., Gelenbe, E. (eds.) MASCOTS 2003. LNCS, 
vol. 2965, pp. 209–234. Springer, Heidelberg (2004) 
7. Pearlman, M.R., Haas, Z.J., Sholander, P., Tabrizi, S.S.: On the Impact of Alternate Path 
Routing for Load Balancing in Mobile Ad hoc Networks. In: 1st Annual Workshop on 
Mobile Ad hoc Networking and Computing (MobiHoc), pp. 3–10. ACM, Boston (2000) 
8. Roy, S., Saha, D., Bandyopadhyay, S., Ueda, T., Tanaka, S.: Improving End-to-End Delay 
through Load Balancing with Multipath Routing in Ad hoc Networks using Directional 
Antenna. In: Das, S.R., Das, S.K. (eds.) IWDC 2003. LNCS, vol. 2918, pp. 225–234. 
Springer, Heidelberg (2003) 
 

On the Time between Successive Multi-path Discoveries and Hop Count per Multi-path 
265 
 
9. Saha, D., Roy, S., Bandyopadhyay, S., Ueda, T., Tanaka, S.: An Adaptive Framework for 
Multipath Routing via Maximally Zone-Disjoint Shortest Paths in Ad hoc Wireless  
Networks with Directional Antenna. In: Global Telecommunications Conference 
(GLOBECOM), pp. 226–230. IEEE, San Francisco (2003) 
10. Waharte, S., Boutaba, R.: Totally Disjoint Multipath Routing in Multihop Wireless Net-
works. In: International Conference on Communications (ICC), vol. 12, pp. 5576–5581. 
IEEE, Istanbul (2006) 
11. Wu, K., Harms, J.: On-demand Multipath Routing for Mobile Ad hoc Networks. In: 4th 
European Personal Mobile Communications Conference (EPMCC), Vienna, Austria 
(2001) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 266–272, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Knowledge, Opportunities and Information Ethics 
Syed Vickar Ahamed and Sevki S. Erdogan 
University of Hawaii at Hilo 
200 Kawili Street, Hilo 
Hawaii 96720 
{sahamed,sevki}@hawaii.edu 
Abstract. High-speed Internet provides unprecedented opportunities in the in-
formation domain. The wealth of information in the web-based knowledge banks 
brings about innumerable business prospects. This relentless rush for knowledge 
lowers the information ethics. The rewards had brought a stream of successful 
dot-com corporations in the 1990s. The dangers have also brought bankruptcy 
for most of these corporations. The plunderers are the knowledgeable and skilled 
few and the victims are the ignorant who are not able to see through the games 
that Internet exploiters can play. The boom and bust of the dot-com opportunists 
is grim reminder of how knowledge cycle has taken the ignorant few to the depth 
of despair. In this paper, we suggest that intelligent knowledge processing soft-
ware should curtail the abuse of the IP networks, much as the network security 
programs that block the spread of spam and viruses. 
Keywords: High-speed networks, Business ethics, Knowledge protection, 
Knowledge m, Knowledge and information processing. 
1   Introduction 
Most of the Internet users explore and benefit from the convenience and opportunities 
offered by the network revolution within the legal and ethical framework of society.  
However, such boundaries are vague and volatile. Human beings sometimes lose the 
perspective of the entire picture [1] of knowledge, wisdom, social movement, and 
social justice that is embedded in the historic, legal, and ethical dimensions1 in the 
society. An active role of a new breed of knowledge machines (KM) is proposed in 
this paper to facilitate the human control of distributed computer systems. Knowledge 
machines (KMs) process synthesized information from any IT platforms (such as 
DSS, EIS, SAP, PeopleSoft, etc.) to the derive knowledge content (of significant 
value, if there is any) from the numerical and graphical content of such information 
systems. Typically, these machines dispense knowledge from WWW sites and navi-
gate the flow of information depending on the user needs, preferences, and choices.  
A schematic of the methodology deployed in intelligent Internets and knowledge net-
works is shown in Figure 1. The Internet is shown as the solid line ellipse and a new 
secure Internet signaling network (equivalent to the CCS7) is shown by the ellipse 
                                                           
1 This work is supported by EPSCoR award EPS-0903833 from National Science Foundation to 
the University of Hawaii. 

 
Knowledge, Opportunities and Information Ethics 
267 
 
with the dash-dot line. Within these two outer ellipses lies a national archive of wis-
dom, knowledge, ethics, and cultural value of any nation. A full-fledged Internet-
based intelligent network configuration is shown with a knowledge transfer point 
(KTP) knowledge, wisdom, and other bases that serve as the reference points for the 
nation. An independent knowledge control point (KCP) holds the content-based ad-
dress for knowledge and wisdom that is held in the national archive of knowledge and 
wisdom. Both the knowledge management system (KMS) and knowledge-
provisioning environment (KPE) are shown. The function of the KMS is to update, 
validate and consolidate then knowledge held in the archives. The function of the 
KPE is to provision and dispense selected modules and blocks of information from 
the archives. The function of safeguarding and preventing abuse of knowledge and 
information can be shared with the KMS.  
Three grades knowledge providers at national, regional, and local levels are shown 
in Figure 1. The national level knowledge functions are shown within the two outer 
ellipses. Other international interconnect blocks are shown as hexagons, the regional 
knowledge service providers (RKSPs) are shown as pentagons, and the local ISP are 
shown as triangles. Management of the regional knowledge bases is carried out by a 
KBMS (equivalent to the DBMS used extensively in computational environments) 
rather than by KCP, KMS or KPS at national level. This difference is more a matter 
of detail than a conceptual difference. In the normal mode of Internet service provi-
sioning, the routine services are provided by the ISPs. When the knowledge seekers 
tap into the Internet, the knowledge intelligent peripheral or KIP (equivalent to the 
intelligent peripheral in the IN environment) senses the need to invoke the coopera-
tion of the RKSPs, the national knowledge services providers (NKSPs), or the inter-
national KSPs. The cooperation between KSPs facilitates the Internet users. 
If there is a consortium of global knowledge providers, then the national knowl-
edge, and wisdom, etc., bases of all nations can be consolidated under the patronage 
of the United Nations, much like the health and food programs. This “world bank” of 
knowledge, wisdom, and culture is the pinnacle at the fourth level (see Figure 2) and 
provides the framework for nations to share values, ethics, and morality much like 
sharing other world resources. The role of information and knowledge in the growth 
and stability of some nations has been demonstrated by the recent establishment of 
cabinet-level offices (e.g., energy, national security) in any national environment. 
To retain objectivity of minding and monitoring concrete social initiatives, (knowl-
edge) machine based human system (Figure 2) that is likely to be more effective than 
the current human systems where the eminent self interest of decision makers can 
influence their decisions. The long-term social gains from such social initiatives can 
be overshadowed by the lack of rigorous and scientific methodologies.     
In the modern times, we face a situation where educated, business minded, but op-
portunistic folks are actively at work. The ultimate goals of such folks are occasionally 
slanted towards maximizing the self-interest [2] rather than serving the society. The 
slow and insidious social movement in some unwary nations and societies is towards 
exploitation of those ignorant in differentiation skills between justice and injustice, 
truth and deception, virtue and arrogance, and benevolence and exploitation. Well-
documented cases of Internet based child molesters and identity thieves, and even fi-
nancial agencies ready to offer loans and then foreclose properties who default in their 
payments. These social ills are still rampant in stable Western societies. In the past, 
occupation of foreign lands and the slave trade have left dark imprints in history. 

268 
S.V. Ahamed and S.S. Erdogan 
 
       
 
Fig. 1. Framework of an intelligent Internet-based network with traditional intelligent network (IN) components. ISP = internet services provid-
ers; KIP - knowledge intelligent peripheral attached to ISP local network; RKSP - regional knowledge services providers; KB - knowledge 
base(s); KBMS - KB management systems; KCP - knowledge control point; KTP - knowledge transfer point; KMS - knowledge management 
system; KPE - knowledge provisioning environment; DDS - Dewey decimal system/Library of Congress system knowledge bases. 

 
Knowledge, Opportunities and Information Ethics 
269 
 
2   Knowledge Machines to Damp Internet Opportunism 
Knowledge machines (KMs) [3] offer new methodologies to achieve well-directed 
goals based in authenticated “knowledge software”.  Machines can generally be out-
witted by human being and for this reason, an ironclad framework of software secu-
rity is necessary to prevent the abuse of the machine that will process information and 
knowledge to provide wisdom to the human beings.   
The positive advantages of the network-based knowledge machine are speed of 
processing information and the global reach to all relevant knowledge bases around  
 
 
Fig. 2. Schematic representation of a knowledge machine (KM) based system to monitor, and to 
stabilize social movements dynamically, within a complex society. The feedback control is ac-
complished as information and accumulated knowledge flows up the seven (3 through 10) layers 
of the pyramid. The society resides outside of the pyramid and the patterns of change and move-
ment are tracked in the information and knowledge bases within the machine. The knowledge-
ware is supported by the mathematical, statistical, AI and social models and incremental changes 
are matched against those tracked by the sensor array in (1). Feedback correction is computed by 
scientific SW in layers (6) and (7). Most of the learning and model development occurs in these 
two layers. The social control is exerted via the human/robotic control centers in (4).  

270 
S.V. Ahamed and S.S. Erdogan 
 
the world.  Such machines deploy the opportunities provided by the Internet, but un-
der the collective wisdom of socially documented rules and conduct encoded in the 
“knowledge software” that drives these machines. Being harder to tamper (due to nu-
merous layers of security), these machines will be less impulsive than human beings 
under similar situations. To this extent, the recommendations and the suggested pro-
cedures offered by the machines will be close to being just, fair, and respectful of the 
rights of all the segments of human beings within the society. 
In reality, the human organizations have a set of standard operating procedures 
(SOPs) for corporate executives. The medical code of ethics offers guidance to the 
medical professionals when the discretionary medical practices may interfere with the 
rights of patients, doctors, and/or staff members. When the procedures and organiza-
tions become complex, simple and intelligent, decision support systems offer some 
rule based decision-making processes that offer guidelines for human beings.  In the 
same vein, when social and ethical procedures that influence global communities and 
the rights of small nations, the knowledge machine and their appropriate software can 
help global organizations (such as United Nations, international monetary institutions, 
save the children agencies, etc.) weigh and consider all the social entities in a fine 
balance of etiquette and justice. Such machine will overrule the self-interest of agen-
cies and political lobby groups. Over time, these lobbies become a part of social set-
ting in power centers, such as those in Washington, New York, Geneva, Paris, and 
elsewhere and around the world. 
Financiers, lobby groups, presidents, power centers, and dynamic power shifts 
threaten the stability of human organizations that make crucial decisions. In most 
cases, the conflict between organizational values and self-preservation of the corpo-
rate leaders become apparent. Generally, the self-preservation and self-propagation 
instincts win out.  The recent scandals at Enron, Arthur Anderson, and Global Cross-
ing bear witness [2] that decision-makers favor personal gain over legal and ethical 
considerations. The acceleration grows especially faster due to the high-speed back-
bone networks [3] with executives whose positions are precarious. The power and 
position held by such executives comes under scrutiny. Both are jeopardized quickly 
and precipitously. Any deceptive practices in the past cause heightened anxiety since 
public exposure comes over the high-speed media links. Once more, high-capacity 
backbone networks play an insidious role, and contribute to the dissemination of any 
unethical practices of corporate executives.  
This dilemma is not to be seen as a flaw in democracy. The deduction of Aristote-
lian [4] wisdom that governed the social stability needs the computational support of 
the modern computer systems to stabilize the more complex modern society.  When 
everything is weighed and considered, the role of knowledge and wisdom gets even 
more emphatic in providing social justice. It is our contention that the ancient Roman 
foundations of democracy are still valid and viable. It becomes necessary to provide 
the modern tools and knowledge-ware to support the social scientists and philoso-
phers in drawing axioms of modern wisdom for the 21st century, rather than gazing in 
a crystal ball of the age-old and undocumented wisdom. The role of fair-minded hu-
man beings (like the Supreme Court judges) at the top of hierarchy of machines is 
necessary to guide the machine through a mirage of possibilities.  The wisdom to 
guide the machines becomes a prerequisite to the wisdom to guide the society since 
these machines and their human counterparts provide the social driving force.  

 
Knowledge, Opportunities and Information Ethics 
271 
 
A select group of social, computer, and network scientists guide the machine at 
lower levels (3-7, Figure 2) to offer a set of viable, just, ethical, and socially benevo-
lent goals derived from age-old wisdom and modern technology. The byways and 
highways to progress are computationally accurate and as flawless as the current so-
cial conditions would permit. 
3   A Stable Social Balance 
The purpose of the knowledge machine (KM) is to offer viable solutions for 
implemeting social juctice while mediating and moderating social change. In 
monitoring the flow of information up the seven layers (3 through 10, in Figure 2), 
conditions for normal ranges are examined, much as the machines autmatically 
monitor the vital statistics of patients in a medical setting. Conditions leading to 
instability are filtered out or modified to suit the particular socioeconomic setting.  
Such a monitoring strategy exists to prevent national economics from swinging 
between catastrophic busts and short-lived booms. In the present context, the 
knowledge machine facilitates a social parallelism while enforcing social justice and 
negotiating social change without causing social unrest. The bulls and bears in the 
social economy are both mderated to be in the restrained social cycles. Such machines 
also ensure the social/civil rights of all the constituting groups. Most of the advanced 
societies protect their citizen by providing medical care, social security, and other 
social services. While such practices are administered by limited staff and incomplete 
scientific methodologies, the use of modern computer, network, and communication 
technologies will offer more dependable means of administering social justice. Such 
machines also curtail the abuse of power and block corruption within the society.   
The deployment of knowledge machines in the social setting is likely to be resisted 
by politicians, power brokers and government officials, much like the early diagnostic 
programs such as Mycin [5], or NeoMycin [6] were resisted by the medical 
professionals. The use of the more recent object oriented machines and 
knowledgeware is likely to serve the social entities much more than the Management 
Information Systems (MIS) type of information processing platforms [7] based on 
conventional computer systems.   
4   Conlusions  
Greater deployment of scientific methodologies in the social setting is likely to offer 
greater social justice and ethics at a pace that a majority of the constitutent will 
welcome. The social changes towards a better human society are likey to accerate 
rather than spin around. Human judgment alone may yield satisfactory solution in a 
most of the cases but a computer-aided-social-support (CASS) system can offer 
greater benfits in the longer run. In the corporate environment, the MIS platforms 
offer greater corporate stability and accountability in the business community.   
Voilent fluctuation in the social patterns are damped out by extrpolative effects of 
corrections in the past and the effectiveness of such correction. While the knowledge 
machine offers the corrective measures, it also track both the microscopic and 

272 
S.V. Ahamed and S.S. Erdogan 
 
macroscopic effet of the proposed changes in both qualitative and quantative terms.  
The mistakes of the past are systematically studied and eleminated and the benefit to 
cost ratio is enhanced.  If the criterion for optimality is imposed, then the machine 
uses the oprations research (OR) techniques to achieve the optimal goals.  When the 
machine is suplemented by the laws of ethics and justice, it will enforce such laws 
and an attempt to willfully violate these laws will cause an alert signal to the law 
enforcement authorities or the judicial system within the country. 
References 
[1] Bryce, R., Ivins, M.: Pipe Dreams: Greed, Ego, and the Death of Enron, 1st edn. Harper-
Collins Canada / Public Affairs (2002); Cruver, B.: Anatomy of Greed: The Unshredded 
Truth from an Enron Insider. Carroll Graf Publishers (2002); Bernstein, P.A.: What’s 
wrong with Telecom. IEEE Spectrum, 26–29 (January 2003) 
[2] Preisel, B., Bouwman, H., Steinfield, C.: E-Life after the Dot Com Bust. Physica-Verlag, 
Heidelberg (2004) 
[3] Ahamed, S.V.: Intelligent Internet Knowledge Networks: Processing of Wisdom and Con-
cepts. Wiley, John & Sons, Incorporated, Hoboken (2007) 
[4] Barnes, J. (ed.): The Complete Works of Aristotle, vol. 1, 2. Princeton University Press, 
Princeton (1995) 
[5] Shortcliffe, E.: MYCIN: Computer-Based Medical Consultations. American Elsevier, New 
York (1976); See also Buchanan, B.G., Shortcliffe, E.H.: Rule-Based Expert System: The 
Mycin Experiment at Stanford Heuristic Programming Project. Addison Wesley, Boston 
(1984) 
[6] Kintsch, W., et al.: About NeoMycin, Methods and Tactics in Cognitive Science. Law-
rence Erlbaum, Mahwah (1984); see also: Luger, G.F.: Artificial Intelligence, Structures 
and Strategies for Complex Problem Solving. Addison Wesley, Boston (2005) 
[7] SAP (UK) Limited, SAP Solutions, and eLearning, Feltham, Middlesex, England (2002) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 273–282, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
An Optimized MANET Gateway Discovery Based on 
Fuzzy Logic 
Antonio J. Yuste1, Alicia Triviño2, Eduardo Casilari2, and Francisco D. Trujillo2 
1 Dpto. Ingeniería de Telecomunicación, Universidad de Jaén 
23700 Linares, Spain 
ajyuste@ujaen.es 
2 Dpto. Tecnología Electrónica, E.T.S.I. Telecomunicación, Universidad de Málaga 
29010 Málaga, Spain 
{atc,ecasilari,fdtrujillo}@uma.es 
Abstract. The interconnection of Mobile ad hoc networks (MANETs) and the 
Internet is supported by a Gateway. The gateway is responsible for informing 
about some configuration parameters as well as for facilitating the creation of 
the routes to the Internet in the MANET nodes. For these tasks, several control 
messages are generated. The way in which these messages are originated differ-
entiates the integration supports for MANETs. In particular, in the hybrid 
Global Connectivity support the Gateway generates periodic Modified Router 
Advertisements (MRA) which are broadcast in an area close to the Gateway. 
The optimum values to define the periodicity of these messages and the area in 
which they are  propagated  depend on the network conditions. Therefore, 
an automatic and dynamic algorithm is recommended to be  implemented in 
the Gateway to adjust these two parameters. In this sense, this paper pre-
sents a technique by which the interval of emission of the MRA messages is 
controlled by a fuzzy system. The fuzzy system captures several network con-
ditions such as the link stability or the number of sources. The simulation re-
sults show that the proposed scheme outperforms other adaptive approaches 
for the gateway discovery in MANETs. 
Keywords: MANET, Internet, Gateway Discovery, Fuzzy Logic. 
1   Introduction 
A Mobile Ad hoc NETwork (MANET) is composed of independent mobile devices 
that communicate through wireless links. Due to their unrestricted mobility, mobile 
nodes should constantly self-configure to maintain the routes between nodes. Fur-
thermore, the paths between any two nodes could be composed of intermediate nodes 
that cooperatively retransmit and route the packets to the final destination. Therefore, 
every node in a MANET is a router which can move in any direction and with any 
speed independently of other nodes’ movements. This behavior turns into a continu-
ous change of the wireless links that must be taken into account by the protocols 
executed by the nodes in the MANET. 

274 
A.J. Yuste et al. 
 
Due to their ability to work independently, MANETs were originally developed 
for military communications and also for rescue operations where traditional infra-
structures were not operative. However, the success of wireless communications has 
prompted the extension of this technology in other scenarios such as conferences, 
visiting theme parks or recreation areas. In  these  situations,  different  types  of ele-
ments (PDA, laptop, cellular phones) can coexist. Additionally, these civil applica-
tions may be demanded to provide access to external networks, especially to the 
Internet. The connection between the two networks (the MANET and the Internet) is 
achieved via a special router that acts as the Internet Gateway. When this element is 
present, the MANET is referred as hybrid mobile ad hoc network. The Gateway has 
two  main tasks. Firstly, it is capable to route the packets from the Internet to 
a MANET node. Conventional Access Routers do not implement any ad hoc routing 
protocol so they need to be completed by an Internet Gateway. On the other hand, the 
Gateway informs about the configuration parameters by generating Modified Router 
Advertisement (MRA) messages. Upon reception, the nodes also create, update or 
optimize the route to the Internet Gateways which are employed to send the packets 
to the Internet hosts. The process by which these messages are generated differenti-
ates the integration supports [1] [2] [3]. Among them, the Global Connectivity  
support [2] is the most popular one. In this scheme, discovery processes may be clas-
sified into three main categories: proactive, reactive and hybrid. Under the reactive 
discoveries, the nodes only search for a route to the Internet Gateway (IGW) when 
they have some data to send to external hosts and they do not keep a valid route to 
the IGW. To proceed, the node generates a Modified Router Solicitation (MRS) mes-
sage which is replied by the Internet Gateway with a unicast MRA message. Con-
versely, the proactive algorithms are characterized by the periodic emission of MRA 
messages by the IGW. These MRA messages are periodically broadcast to all the 
nodes in the MANET  every T seconds.  After  receiving a  MRA message, the  
changes on the wireless   links   could   make   the   route   to   the   Gateway   inva-
lid.   Under   these circumstances, the nodes will operate as in the reactive scheme. 
Therefore, when needed, it will emit a MRS message and it will wait for a unicast 
MRA message. Finally, the  hybrid  schemes combines the two previous ap-
proaches as the MRA messages are periodically generated in a area close to the 
Gateway but the nodes outside this zone  update the  route to the  IGW in a reac-
tive  way. In the  hybrid approach, the proactive zone is defined by the TTL (Time 
To Live) value of the IP (Internet  Protocol)  header in the  MRA message [4].  
This parameter  restricts the number of forwarding that the message can have. By 
adjusting the TTL value, the hybrid  gateway discovery  can  behave  as  proactive  
(the  TTL  set  to  the  network diameter) or as reactive (the TTL set to 0). 
The three kinds of gateway discovery process generate control messages (MRA or 
MRS). In order to improve the network performance, it is important to reduce the 
number of these control messages while keeping valid routes to the Gateways in the 
nodes that need them. By this way, the scarce wireless resources are released to the 
data  packets  which  experiment  lower  delay  and  losses.  Since  hybrid  gateway 
discovery is able to emulate all gateway discovery schemes, in this paper we 
will focus on optimizing it. However, tuning the values of the TTL and the T parame-
ters is not a trivial task in conventional MANET applications. Specifically, the opti-
mum value for the T parameter depends on several factors such as the number of 

 
An Optimized MANET Gateway Discovery Based on Fuzzy Logic 
275 
 
sources, their  positions, their  route  duration or  the  density of  nodes [5].  Fur-
thermore, as network conditions are expected to vary unpredictably in real MANET 
scenarios, an autonomous and dynamic algorithm to adjust the T and TTL parame-
ters is strongly recommended.  
The main drawback to overcome is the fact that there is not any mathematical as-
sociation that relates these factors with the optimum value of T setting. To proceed, 
our  proposed  scheme  dynamically  adapts  the  value  of  T by  a  fuzzy  algorithm. 
MANETs  can  benefit  from  fuzzy  systems  as  they  are  able  to  capture  generic 
behaviors of the MANET according to the parameters settings. For instance, we can 
state  that  a  stable  MANET  needs  less  MRA  messages  than  a  MANET  with 
continuous link changes. Constructing several similar rules, the fuzzy system has 
shown to provide optimized network performance in terms of packet losses, delay 
and control overhead. 
The rest of the paper is structured as follows. In Section 2, we present a detailed 
description about other adaptive techniques. Section 3 explains our algorithm which 
is evaluated by simulations presented in Section 4. Finally, Section 5 draws the main 
conclusions of our work. 
2   Related Work 
The hybrid gateway discovery possesses two parameters to be configured: the inter-
val of emission of the MRA messages (T) and the area in which these messages are 
periodically broadcast (TTL). 
Concerning the adjustment of the TTL parameter, one of the first proposals was 
the Maximal Source Coverage (MSC) [6]. In this proposal, the Gateway sends the 
advertisement message with the TTL set to the minimum number of hops required to 
reach  all  the  sources  that  are  employing  this  gateway  to  communicate  with  
the Internet. An optimized scheme of the MSC algorithm is presented in [7]. In this 
approach, the nodes that are delimiting the proactive zone act as proxy for the Inter-
net Gateway so they can reply to the MRS messages with unicast MRA messages. 
The scheme is known as Low Overhead and Scalable Proxied (LOSP) algorithm. 
Alternatively, the work in [8] presents a novel technique in which the decision about 
forwarding the MRA messages is transferred to the MANET nodes. In particular, just 
the mobile nodes that are relaying data packets are allowed to forward the MRA mes-
sages. In this sense, the area of the proactive zone is decided in a distributed way. 
On the other hand, the work in [9] analyzes the adjustment of the T parameter. It 
presents an algorithm in which the appropriateness of broadcasting an MRA message 
depends on the number of active sources that communicate with the Internet as 
well as the number of intermediate nodes that are used for these communications. 
With these two parameters, the authors defined the so-called Regulated Mobility 
Degree (RMD). If this factor exceeds a threshold, the algorithm considers that a 
MRA is needed and, in turn, the message is sent. The main drawback of this 
solution is determining  the  threshold  as  its  optimum  value  also  depends  on  
the  network conditions. This issue is not studied in the paper. 
Focusing on the simultaneous tuning of the T and TTL parameters, the authors in 
[10] propose the use of an auto-regressive filter. Then, the traffic load in the Internet 

276 
A.J. Yuste et al. 
 
Gateways is monitored to identify the traffic rate and the number of received 
MRS. Taking into account these two parameters and the number of link changes, the 
system is expected to improve the network performance. The main drawback of this 
proposal is that there is not any specific formulation and no evaluation is shown. 
3   Proposed Gateway Discovery 
Our  proposed  scheme  is  supported  by  a  fuzzy  system  which  can  use  several 
parameters about the MANET conditions. In particular, the system takes into account 
the following parameters: 
 
• 
 The  number  of  MRS  (NMRS).  The  ratio  between  the  number  of  MRS 
messages that the sources originate and the number of active sources. To make 
the Gateway able to compute this parameter, the MRS messages are only re-
plied by the Internet Gateway. 
NMRS= number  of  MRS messages
                number of  traffic sources                                             (1) 
• 
 The link changes (LC). The mobility near the Internet Gateway is measured as 
the number of link changes that the Gateway detects divided by the number of 
traffic sources that are using this gateway. With this parameter, the mobility 
near the Internet Gateway is considered. 
LC= number  of  link changes  
          number of  traffic sources                                         (2) 
• 
 The TTL changes (TTLC). It represents the number of changes in the distance 
of the sources to the Gateways. The distance is measured as the number of 
hops that the messages have to be forwarded to reach the final source from the 
Internet Gateway. This distance can be inferred from the TTL field in the data 
packets that the sources generate to the Internet. As they are routed by the 
Internet Gateway, this element can easily become aware of the distance and of 
their changes. In particular, the average value of the TTL changes is computed 
as expressed in the following equation. This metric provides a hint about the 
mobility of the MANET nodes which are not directly connected to the Gateway. 
TTLC = number  of  TTL Changes
              number of  traffic sources                                      (3) 
These three parameters are estimated every second. In order to introduce them into 
the fuzzy system, they need to be fuzzified. Particularly, the input variables have 
three possible variables: low, moderate and high. The selected membership func-
tions are represented in Figure 1. We have used a triangle-based function since they 
have been  extensively used in the real time applications due to their simple formu-
las and their computational efficiency. 

 
An Optimized MANET Gateway Discovery Based on Fuzzy Logic 
277 
 
 
Fig. 1. Membership function of the inputs for the fuzzy system 
As we can observe, the inputs are assumed to have a value from 0 to 1. It could 
happen that one of the used parameter is greater than one. In this case, the affected 
parameter is set to 1 so that the system can cope with this eventuality. 
From the inputs, the application applies some fuzzy rules into an inference sys-
tem. The fuzzy rules for our system are presented in Table 1. 
Table 1. Fuzzy rules for the proposed system to adapt the T parameter 
Number of MRS
(NMRS)
Link Changes
(LC)
TTL Changes
(TTLC)
Convenience
Low
Low
Low
Very Low
Low
Low
Moderate
Very Low
Low
Low
High
Low
Low
Moderate
Low
Very Low
Low
Moderate
Moderate
Low
Low
Moderate
High
Moderate
Low
High
Low
Low
Low
High
Moderate
Moderate
Low
High
High
High
Moderate
Low
Low
Very Low
Moderate
Low
Moderate
Low
Moderate
Low
High
Moderate
Moderate
Moderate
Low
Low
Moderate
Moderate
Moderate
Moderate
Moderate
Moderate
High
High
Moderate
High
Low
Moderate
Moderate
High
Moderate
High
Moderate
High
High
Very High
High
Low
Low
Low
High
Low
Moderate
Moderate
High
Low
High
High
High
Moderate
Low
Moderate
High
Moderate
Moderate
High
High
Moderate
High
Very High
High
High
Low
High
High
High
Moderate
Very High
High
High
High
Very High
 
 

278 
A.J. Yuste et al. 
 
Then, the fuzzy system obtains an output called convenience as it represents 
the appropriateness  of  emitting  a  MRA  message.  The  membership  of  the con-
venience is represented in the Figure 2. 
 
Fig. 2. Membership functions of the output (called convenience) for the fuzzy system 
Once  the  output  is  computed,  the  Gateway  compares  it  to  a  predetermined 
threshold (set to 0.5 in our experiments). When it exceeds the threshold, the Gateway 
considers that a MRA message is necessary in the network and, consequently, it 
broadcasts a new one. Under these circumstances, the process to decide about the 
emission of new MRA messages is paused along 3 seconds. By this interruption, the 
MRA messages, which are an extended version of Router Advertisement (RA) mes-
sages, also follows the recommendations presented in [11]. On contrary, the deci-
sion phase is triggered every  second  with  updated  measurements. summarizes the 
scheme of the fuzzy system. 
 
 
 
Fig. 3. Scheme of the Fuzzy system 
The  presented  fuzzy  system  controls  the  interval  of  emission  of Figure  3 
the  MRA messages. As we intend to apply it in a hybrid gateway discovery scheme, 
the TTL value is adjusted according to the MSC message due to its simplicity and its 
good results. Although the LOSP is an improved version of the MSC, it is not  

 
An Optimized MANET Gateway Discovery Based on Fuzzy Logic 
279 
 
appropriate for the proposed scheme as it does not allow the Gateway to compute the 
parameter “Number of MRS”. 
4   Evaluation Results 
The evaluation of our proposal has been conducted by a comparative approach. In order 
to analyze different scenarios but keeping the same conditions for all the evaluated algo-
rithms, the use of a simulation tool becomes necessary. In this sense, our study is  
supported by the Network Simulator tool ns-2.31 [12]. As a previous step, the tool was 
extended with the Global connectivity support [13] and the evaluated schemes. In par-
ticular, we have compared our scheme, called the Fuzzy-based Gateway Discovery 
(FGD), to LOSP, ADD and RMD (all these algorithms are described  in  the  Related  
Work).  By  comparing  to  LOSP,  we  can  evaluate  the goodness of the proposed 
fuzzy system to tune the interval of emission of the MRA messages (the T parameter). It 
is important to note that FGD also adapts the TTL value by means of the MSC scheme, 
which is a simplified version of the LOSP scheme. On the other hand, ADD represents a 
distributed algorithm which controls the forwarding of the MRA messages by the 
MANET nodes. Finally, RMD focuses on controlling the T parameter taking into ac-
count some network conditions. Comparing the results of FGD to RMD and ADD, we 
can decide about the appropriateness of adjusting the T and TTL parameters simultane-
ously. The RMD is configured according to [9]. 
The performance of the MANET is evaluated with the following metrics: 
Packet Loss Rate. It is defined as the ratio between the number of lost data 
packets and the total number of data packets transmitted by the sources.  
Normalized Routing Overhead. It represents the number of control packets di-
vided by the total number of received packets. For this computation, each time 
a control packet is retransmitted, it is considered as a new control packet. 
The simulation area is a rectangle of 1500x300 m2  where nodes move according to 
the Time-variant Community Mobility Model (TVCM) mobility pattern [14]. This 
is a realistic model obtained from traces of wireless LAN (Local Area Networks). It 
includes two mobility characteristics: skewed location visiting preferences and peri-
odical re-appearance. It also incorporates communities that the mobile nodes often 
visit. In our scenarios, we define two random communities. The maximum speed 
of the nodes is set to 20 m/s. 
Concerning the traffic, we have varied the number of Constant Bit Rate (CBR) 
sources in our simulations. However, they all emit at a traffic rate of 10 packet/s 
with a packet length of 320 bytes. 
In all the scenarios, the simulation time is set to 1000 seconds. Since we are inter-
ested in studying the behavior of MANETs in a steady state, the first 100 seconds of 
the simulations are considered a warm-up period and, consequently, they are not 
taken into account for the analysis. For each analyzed condition and algorithm, the 
simulations are repeated 30 times. The figures show the averaged results. 
For the evaluation, we have varied two basic parameters: the number of nodes and 
the number of sources. By this way, we pretend to evaluate the goodness of our pro-
posal in a wide spectrum of scenarios. Table 2 summarizes the parameters of the 
simulations. 

280 
A.J. Yuste et al. 
 
Table 2. Simulation Parameters 
Simulation Area 
1500x300 m2 
Transmission Range 
250 m 
Simulation Time 
1000 s 
Runs per point 
30 
Integration Support 
Global Connectivity 
Local Repair disabled 
Link Layer feedback activated 
Link Layer 
802.11b 
RTS/CTS enables 
Mobility Model 
TVCM 
Maximum speed: 20 m/s 
Number of nodes 
[40, 45, 50, 55] 
Number of sources 
[15, 20, 25, 30, 35, 40] 
Number of Gateways 
[2, 4] 
 
Firstly, we vary the number of nodes in the MANET from 40 to 55. For this case, 
we assume that there are 40 traffic sources and 4 Gateways. The Gateways share the 
same prefix so a MANET node could use any Gateway. The Gateways are placed in 
the  corners of the  simulation area.  Figure 4  shows the  packet loss rate  and  
the normalized overhead for this kind of scenarios. 
Packet Loss Rate (%)
Normalized Routing Overhead
28
LOSP 
RMD
27
ADD 
FGD 
2.25
2.2
LOSP 
RMD 
ADD
FGD
26
2.15
25
2.1
24
2.05
23
22
21
40 
45 
50
55
Number of Nodes
1.95
40  
45 
50
55
Number of Nodes
 
Fig. 4. Packet Loss Rate and Normalized Overhead as a function of the number of nodes 

 
An Optimized MANET Gateway Discovery Based on Fuzzy Logic 
281 
 
As we can see, incrementing the number of nodes reduces the packet losses. This 
is due to the improvement of the gateway connectivity which leads to more available 
routes to the Internet Gateway. However, the routes are composed of a higher num-
ber of nodes which provokes that their lifetimes are lower. In turn, the MANET 
nodes need to generate more control messages to discover alternative routes to the 
Gateway. This explains why the normalized routing overhead experiments an incre-
ment. In the analyzed conditions, FGD outperforms the other proposals. 
Another set of experiments consists in analyzing the impact of the number of 
sources. To proceed, we placed 2 Gateways at opposite corners. The MANET is 
composed of 50 nodes and the number of sources varies from 15 to 40. Figure 5 
represents the obtained results for the packet loss rate and the normalized overhead. 
Packet Loss Rate (%)
Normalized Routing Overhead
35
LOSP 
RMD 
ADD
30
FGD
2.6
2.5
LOSP 
RMD 
ADD 
FGD
2.4
25
2.3
20
2.2
15
2.1
10
2
5
15 
20  
25  
30 
35
40
Number of data sources
1.9
15 
20  
25  
30 
35
40
Number of data sources
 
Fig. 5. Packet Loss Rate and Normalized Overhead as a function of the number of data sources 
Figure 5 shows that there is little difference in the packet loss rate obtained by the 
evaluated techniques. Losses always increase when the number of data sources is 
incremented. When augmenting the traffic sources, more nodes need updated 
routes to the Internet. If they do not possess a valid route, they will emit a MRS mes-
sage to obtain an alternative route. This behavior will induce extra overhead as repre-
sented in the Figure. Concerning the overhead, the FGD is able to reduce the number 
of control messages while keeping an acceptable packet loss rate. 
Compared to the LOSP and the ADD, the performance of the MANET with the 
FGD is better. This behavior reinforces the need for adaptive techniques that controls 
the T parameter. However, this adjustment must be done with the TTL value as the  
results improve. This effect can be observed if we compared the results of the FGD 
and the RMD algorithms.  

282 
A.J. Yuste et al. 
 
5   Conclusions 
The integration of a MANET into the Internet is achieved by an Internet Gateway 
which emits MRA messages. In the hybrid Global Connectivity support, the mes-
sages are generated periodically in an area close to the Gateway. By means of a 
fuzzy system, this paper presents a technique by which the dimension of this area (in 
terms of the number of hops from the Gateway) and the periodicity of the Gateway 
advertisements are controlled. The fuzzy system takes into account the number of 
link changes, the movement of the traffic sources and the number of solicitation that 
the sources generate. The simulation results show the goodness of our proposal in 
terms of packet loss rate and normalized overhead. 
References 
1. Jelger, C., Noel, T., Frey, A.: Gateway and address autoconfiguration for IPv6 ad hoc net-
works (work in progress). IETF Internet draft (2006) 
2. Wakikawa, R., Malinen, J., Perkins, C., Nilsson, A., Tuominen, A.: Global Connectivity 
for IPv6 Mobile Ad Hoc Networks (work in progress). IETF Internet draft (2006) 
3. Ruffino, S., Stupar, P.: Automatic configuration of IPv6 addresses for MANET with mul-
tiple gateways (work in progress). IETF Internet draft (2006) 
4. Defense Advanced Research Project Agency: Internet Protocol. IET RFC 791 (1981) 
5. Yuste, A., Triviño, A., Trujillo, F.D., Casilari, E., Díaz-Estrella, A.: Connectivity Gateway 
discovery in MANETS. In: Cerdà-Alabern, L. (ed.) EuroNGI/EuroFGI 2008. LNCS, 
vol. 5122, pp. 128–141. Springer, Heidelberg (2008) 
6. Ruiz, P., Gomez-Skarmeta, A.: Maximal Source coverage adaptive gateway discovery for 
hybrid ad hoc networks. In: Nikolaidis, I., Barbeau, M., Kranakis, E. (eds.) ADHOC-NOW 
2004. LNCS, vol. 3158, pp. 28–41. Springer, Heidelberg (2004) 
7. Ros, F., Ruiz, P.: Low Overhead and Scalable Proxied Adaptive Gateway Discovery for 
Mobile Ad Hoc Networks. In: IEEE International Conference on Mobile Adhoc and Sen-
sor Systems (MASS), pp. 226–235 (2006) 
8. Javiad, U., Rasheed, F., Meddour, E.E., Ahmed, T.: Adaptive distributed gateway discov-
ery in hybrid wireless networks. In: IEEE Wireless Communications and Networking Con-
ference (WCNC), pp. 2735–2740 (2008) 
9. Rakeshkumar, V., Misra, M.: An Efficient Mechanism for Connecting MANET and the 
Internet through Complete Adaptive Gateway Discovery. In: First International Confer-
ence on Communication System Software and Middleware, pp. 1–5 (2006) 
10. Ghassemian, M., Friderikos, V., Aghvami, A.: A Generic Algorithm to Improve the per-
formance of proactive ad hoc mechanisms. In: IEEE International Symposium on a World 
of Wireless, Mobile and multimedia Networks (2005) 
11. Narten, T., Nordmark, E., Simpson, W.: Neighbor Discovery for IP Version 6. IETF RFC 
2461 (1998) 
12. Fall, K., Varadhan, K.: ns Notes and Documentation. The VINT Project (2009) 
13. Hamidian, A., Körner, U., Nilsson, A.: Performance of Internet Access solutions in mobile 
ad hoc networks. In: EuroNGI Workshop, pp. 189–209 (2004) 
14. Hsu, W., Spyropoulos, T., Psounis, K., Helmy, A.: Modeling time-variant user mobility in 
wireless mobile networks. In: IEEE Infocom., pp. 758–766 (2007) 

A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 283–294, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
A Survey on  
Application of Neural Networks 
in Energy Conservation of Wireless Sensor Networks 
Neda Enami, Reza Askari Moghadam, and Abolfazl Haghighat 
Department of Information Technology and Communications, Faculty of Engineering,  
Payam Noor University, Tehran, Iran 
Nedaenami@yahoo.com, Askari@pnu.ac.ir, at_haghighat@yahoo.com 
Abstract. There are many restrictions for Wireless Sensor Networks that en-
force us to use low power batteries as their source of energy. Moreover since 
we often use these networks in rough and inaccessible environments, normally 
there is low possibility to change or recharge dead nodes. Today, Dynamic 
Power Management approaches with purpose of reduction of energy consump-
tion in sensor nodes, after deployment and designing of the network draw atten-
tions of many research studies. Therefore, there was a strong interest to use  
intelligent and capable tools such as Neural Networks in recent years, due to 
their simple parallel distributed computation, distributed storage, data robust-
ness, auto-classification of sensor readings, dimensionality reduction and sensor 
data prediction obtained simply from the outputs of the neural-networks algo-
rithms which lead to lower communication costs and energy conservation. This 
paper aims to present the most important applications of neural networks in re-
duction of energy consumption of WSNs, up to now.  
Keywords: Wireless Sensor Networks, Energy Conservation, Neural Networks. 
1   Introduction 
With the appearance of microelectronics, as sensor nodes became cheaper, smaller 
and lower weight, their batteries became smaller too. Also since we often use these 
networks in rough and inaccessible environments such as battlefields, volcanoes, 
forests and so on, normally there is low possibility to change or recharge the defective 
or dead nodes. Hence, the main difference between WSNs and other classic wireless 
networks is that WSNs are hypersensitive and vulnerable to energy. Moreover the 
performance of WSNs severely depends on lifetime of the network. So, energy con-
servation is a serious and critical issue in designing of WSNs with longevity. Energy 
conservation should be gained by wisely management of energy sources. While some 
of the energy uses may be useful and necessary for WSN, such as sending or receiv-
ing of data, query processing when requested and sending of queries and data to adja-
cent nodes, there are some wasteful uses of energy for these networks that should be 
avoided or minimized. The major wasteful energy consumptions can be due to: 1-idle 
listening, 2-retransmitting due to collisions, 3-Overhearing, 4-Control packets over-
head and 5-Sending the message while the destination node is not ready to receive 

284 
N. Enami, R. Askari Moghadam, and A. Haghighat 
[1, 2]. Moreover, the following three points should be considered and highlighted 
about the energy consumption of different parts of a sensor node and are important in 
choosing the appropriate method for reducing of energy consumption in WSNs: Fist, 
energy consumption of communication subsystem is much more than that of compu-
tation subsystem. It is shown that transmitting of a bit of data needs to same amount 
of energy as running of a few thousands of instructions [1]. So, there should be some 
tradeoff between communication and processing tasks. Secondly, energy consumption 
of radio in all modes of reception, transmit and idle is the same extent. While energy 
consumption of radio part is reduced at least an order of magnitude in sleep mode. 
Thus it is reasonable to turn off the radio as long as we can. Third, according to spe-
cific application, sensory subsystem may be a considerable source for energy con-
sumption. In this case, it should be considered in energy efficient approaches. Overall, 
the most important problem is to reduce energy consumption of sensor nodes while 
satisfying the application's requirements. So we should select the appropriate tool to 
solve the above problems. In this paper, we attempt to show Neural Networks can be 
effective tools as they were used before in different papers. This paper aims to present 
the most important applications of neural networks in energy conservation of WSNs, 
up to now. Hence, first we present a general survey on different approaches of reduc-
ing energy consumption in WSNs and then we show how neural networks can help to 
different aspects of energy efficiency in WSNs. 
2   Classification of Approaches 
Up to now, different approaches for reducing of energy consumption were applied in 
WSNs. Here we choosed the classification presented in [1] which divided all these 
approaches into three general groups: Duty cycling, Data driven and Mobility based 
approaches (Fig.1. show the taxonomy of approaches to energy conservation of 
WSNs).  
 
Fig. 1. The taxonomy of approaches to energy conservation of WSNs [1] 

 
A Survey on Application of Neural Networks in Energy Conservation of WSNs 
285 
2.1   Duty Cycling Approaches 
Duty Cycling approaches mainly focus on network subsystem. Ideally, as soon as 
there is no more data to send or receive, the radio should be turned off. As soon as 
there is a new packet of data to send or receive, the radio should be turned on again. 
In this way, nodes are periodically altering between sleep and wake up modes. This is 
called duty cycling. In other words, duty cycling is a fraction of node's life time, in 
which they are active. Thus it is necessary to synchronize the time of sleep and wake 
up of sensor nodes that are doing a common task. So duty cycling always demands for 
a sleep/wakeup scheduling algorithm that can determine which nodes and when 
should go to sleep or wake up without loosing important events. These algorithms 
typically have to synchronize the duty cycling of adjacent nodes to exchange the data 
packets with each other. Duty cycling itself can be divided into two parts that are both 
complementary of each other: topology control and power management which are 
normally complementary of each other. 
Topology control protocols reduce the number of active nodes dynamically to re-
duce energy consumption while satisfying the network requirements. These methods 
let redundant unneeded nodes go to sleep until they are needed. But these methods use 
different criteria for determining which nodes to sleep and when to wake- up. 
Power management methods are further subdivided into two broad categories ac-
cording to the network architecture layer that the method is implementing on.  The 
power management protocols can be implemented as independent sleep-wakeup pro-
tocols on the top of MAC protocols or they can be implemented exactly with MAC 
protocol itself.  The latter method allows optimizing of media access control functions 
based on sleep–wakeup patterns that used. On the other hand, power management 
controls that are independent of MAC layer can provide more flexibility since they 
can be designed appropriate to the requirements of specified application and can be 
used with every MAC protocol [24]. 
2.2   Data Driven Approaches 
One important challenge in Duty Cycling approaches is inattention to sampled data of 
sensor nodes. But Data driven methods can increase energy efficiency in two ways: 
Decreasing of unneeded samples (data reduction) and Data acquisition patterns with 
energy efficiency. 
The reason for data reduction is that as sensor's measurements often have strong 
spatial – temporal correlation with each other, it is not necessary to send duplicate 
information to the sink. Even if the cost of sampling is not important to us, unneeded 
data samples waste the energy. The energy efficient data acquisition patterns can 
decrease energy consumption of sensory subsystem and somewhat communication 
subsystem. Reducing communications may be insufficient, so energy conservation 
schemes have to reduce the number of acquisitions (i.e. data samples). By reducing 
data samples, number of communications decrease as well. Data reduction methods 
further divided into three sub methods: In-Network Processing, Data compression 
(Network Coding) and Data Prediction Methods. 

286 
N. Enami, R. Askari Moghadam, and A. Haghighat 
In-network processing methods try to accomplish data aggregation (computing the av-
erage) or data fusion at intermediate nodes among source to sink. With the assumption 
that the cost of transmitting just one bit is equal to running of few thousands of instruc-
tions, reducing of communications lead to energy conservation. To do so, intermediate 
nodes have to fuse several events into one event and reduce the number of times of 
transmitting information and also the total amount of transmitted data to conserve energy. 
Since sensor nodes may create many redundant data, equivalent packets from several 
sources can be integrated so that the numbers of transmission decrease. 
The amount of transmitting data of source nodes decrease by encoding of informa-
tion at source nodes and decoding the information at destination (often sink) node. 
This is called as Network Coding.  As in-network processing methods can not be used 
when all of the original data packets are needed at receiving nodes, such as applica-
tions that need to global information, recently network coding methods have pre-
sented to decrease the total traffic of the networks. Multicast capacity that can be 
defined as maximum rate, in which a transmitter can send common information to a 
set of receivers, can be achievable by network coding, while generally it is not 
achievable through routing. The main idea of network coding is possibility of combin-
ing data (such as XOR or linear combination) in intermediate (relay) nodes. Sending 
of encoded packets instead of individual packets decrease the energy consumption 
without increasing of delay. 
Data prediction methods have to build an abstraction i.e. a model describing data 
evolution of a sensed phenomenon which can predict values that measured by sensor 
nodes within certain error bounds. This model resides at both sensor nodes and sink. 
If the model provides enough accuracy, queries issued by users may be evaluated at 
the sink by this model instead of exact data acquisition from nodes. Generally, data 
prediction decreases transmission of information from source nodes thus reduce en-
ergy consumption needed for communication. But if the model is not accurate 
enough, making communications among sensor nodes and sink is inevitable and  
consume energy. Data prediction approaches further subdivided into three methods 
according to how they make the model: Stochastic, Time series forecasting and Algo-
rithmic approaches.  
2.3   Mobility Based Approaches 
In the case of mobile sensor nodes, mobility can be used as a tool for reducing energy 
consumption besides duty cycling and data driven approaches. In static wireless sen-
sor network, which sensor nodes are immobile, the data packets from sensor nodes 
have to follow a multi hop route to get to the sink. Therefore, some of possible routes 
may be loaded more than others and it is likely that nodes that are nearest to the sink 
have to relay more packets than others and come to energy depletion faster. If some of 
the nodes (possibly sink) are mobile, traffic flow will change. If mobile devices are 
responsible for gathering data from static nodes, immobile nodes should wait for 
reaching of mobile devices to forward their messages through them. As a result, nor-
mal nodes can conserve energy since distance, involvement and transmission over-
head will decrease. 

 
A Survey on Application of Neural Networks in Energy Conservation of WSNs 
287 
3   Neural Networks Capabilities 
A Neural Network is a large system containing parallel or distributed processing 
components called neurons connected in graph topology. These neurons are con-
nected through weighted connections called synapses. In other words, Artificial Neu-
ral Networks are arithmetic algorithms which are able to learn complicated mapping 
between input and output according to supervised training or they can classify input 
data in an unsupervised manner. Moreover some of the algorithms developed within 
the classical artificial neural networks, can be easily adopted to wireless sensor  
network platforms and in the same time they can meet the requirements for sensor 
networks like: simple parallel distributed computation, distributed storage, data ro-
bustness and auto-classification of sensor readings. Dimensionality reduction, ob-
tained simply from the outputs of the neural-networks clustering algorithms, leads to 
lower communication costs and energy savings [3].  
One of the difficulties in NNs is choosing of appropriate topology for the problem. 
This selection depends on properties of the problem, the most possible methods for 
solving the problem and also to the properties of NN. Moreover there are different 
types of training rules which are inspired from biology science which determine the 
way NNs learn. In most of these networks, training is based on learning by example. 
Thus, a set of correct input- output  data are often(not always)given to the network  
and using these examples, the network should change the weights values so that by 
inputting new data the network can return correct answers as output what we call 
"learning". One of the most important properties of NNs is ability to recognize the 
data affected by noise or intentional change and to remove those variations after 
learning.  Indeed, the knowledge of NN is stored on weights of its connections and it 
doesn't need to store information anywhere. There are different types of NN's topolo-
gies, each have different capabilities according to the application needed. The net-
work's capabilities depend on its structure, dynamics and training rules. The most 
important applications of NN include prediction, classification and identification. 
3.1   Neural Networks in Duty Cycling  
Neural networks have been used in [4, 5] for Dynamic Power Management of WSNs. 
They used NNs to schedule duty cycling of sensor nodes. They proposed a neural 
method to decide which nodes and when have to wake up. They considered that the 
time of next event is a non-stationary series that can be predicted using Wavelet Neu-
ral Networks as accurate as possible. The neural network they used is actually a three 
layered Back Propagation which uses Morlet Wavelet transforms at hidden layer 
(fig.2 shows the network topology structure).  The nodes which are at deeper sleep, 
consumes more energy to wake up. So state of the nodes can be determined with 
prediction of time series of next event and by defining a threshold relative to re-
mained energy of nodes and comparing of those with each other. Simulation results 
showed that using the proposed method, energy consumption will be considerably 
reduced and total lifetime of wireless sensor network can extremely increase. The 
authors compared their proposed policy with last method of Dynamic Power Man-
agement and gained better results. The results showed that with different rates of 
event lost, energy consumption will increase through their method. But still some 

288 
N. Enami, R. Askari Moghadam, and A. Haghighat 
open problems remain. First problem is that these methods do not analyze delay of the 
system due to waiting for sleeping nodes to wake up while delay is an important pa-
rameter in WSN.  Secondly, there must be a solution for preventing nodes from loos-
ing events while they are asleep and it should be efficiently managed. 
 
Fig. 2. The Wavelet Back Propagation Neural network Topology Structure [3] 
3.2   Neural Networks in Routing 
As said, routing in WSNs is one of the most important problems which can support 
efficient operation of the network [21]. Due to energy constraints of each node in 
WSN, routing should be done in a way that maximizes the network lifetime. In [6] 
authors proposed an intelligent method based on self organizing neural networks that 
optimize routing according to energy conservation and computation power of each 
node. This algorithm have been designed for a wireless sensor node called MODA-
BER created by Artificial intelligence center of university of Isfahan and will be 
tested on that node. This paper actually proposed a new method for routing in WSNs 
in which each wireless node use a self-organizing (competitive) neural network to 
decide about containing the data packet and participate in routing or dropping the 
packet. The Self Organizing Map (SOM) learning algorithm is used for training of 
neural network. As soon as a packet arrived, its feature vector is extracted and this 
vector is sent to self organizing NN of that node. After winning of node in competi-
tion against other nodes, it is allowed to send the packet and participate in routing. 
Otherwise it should drop the packet. Since the time complexity of the algorithm is 
linear, it seems to be applicable to wireless nodes. 
Today, clustering gives the best results in WSNs routing; this is the reason why re-
searchers in [7] adopted an unsupervised connectionist learning method by introduc-
ing the evolutionary and dynamic clustering aspect. They used clustering methods 
based on unsupervised connectionist learning techniques and different properties of 
the most famous clustering protocol, LEACH-C [8, 9] and proposed a new routing 
algorithm called LEA2C. Their routing also algorithm outperforms EECS [10] proto-
cols. They presented the connectionist learning by the minimization of the distance 
between the input samples (sensor nodes coordinates) and the map prototypes (refer-
ents), weighted by a neighborhood function. The neuron has the nearest weight vector 
by using Euclidean distance would be the winner. Then the winner and its neighbors 
should update their weight vector and the process repeats until the stabilization of 
SOM.However they optimize the cluster numbers by K-Means algorithm (fig. 3 
shows the SOM neural network structure).  

 
A Survey on Application of Neural Networks in Energy Conservation of WSNs 
289 
 
Fig. 3. Self Organizing MAP Neural Network Structure [16] 
3.3   Neural Networks in Data Reduction  
New sensing methods with energy efficiency through prediction of sensor measure-
ments have shown great ability in reducing communications in sensor networks. In 
these methods, sink node extract model of time series to predict local readings instead 
of communicate with sensor nodes and receive actual measurements which consume 
too much energy. But most of methods were only limited to autoregressive linear 
models and considered for modeling of linear Phenomenon. Naturally, these linear 
models are inappropriate to estimate noisy multidimensional non-linear processes. In 
[11] authors proposed a frame integrated with non-linear time series models to  
approximate measurements and can reduce energy consumption by learning of a map-
ping that adapted with long lasting properties of needed process. As a result, it elimi-
nates the demand for continual re-estimating of parameters of the model. Eventually, 
with reducing of communication among sink node and sensor nodes, energy con-
sumption is minimized. Researchers used recursive Elman Neural networks to do so. 
Also they used methods based on second order Newton for training of the neural 
network. 
3.4   Neural Networks in In-Network Processing  
Knowing that computation less than communication needs to energy, considerable 
energy conservation can be achievable through In-network processing especially data 
fusion. One of important issues of data fusion of WSNs is necessity of using an intel-
ligent system which can fuse heterogeneous data obtained from different sources, 
with high accuracy, automatically and efficiently. Even if the data had been affected 
by noise or intentional manipulating, data fusion method has to classify and identify 
data. One of these intelligent tools is Neural Networks. Sensor fusion with predefined 
numbers can be done with known methods such as Kaman filter or Bayesian theorem. 
But in cases that there is no specified statistical model for uncertainty (error estima-
tion), instead we should use other methods such as rule based sensor fusion, Fuzzy 
Logic or Neural Networks [12]. Neural Networks can be easily adopted to wireless 
sensor network platforms due to their simple parallel distributed computation, distrib-
uted storage, data robustness and auto-classification of sensor readings. Dimensional-
ity reduction, obtained simply from the outputs of the neural-networks clustering 
algorithms, leads to lower communication costs and energy savings [3]. Detection 
Statistics based on training methods of Artificial Neural Networks with actual or 

290 
N. Enami, R. Askari Moghadam, and A. Haghighat 
simulated values are learned to Neural Network. Neural Network can eliminate envi-
ronmental or intentional jamming on sensory data according to training that they 
receive. Specific structured are considered for processing of detection statistics and 
sensors properties. Meanwhile heterogeneous sensor fusion is supported. 
In distributed systems, track to track association take tracks which were created on 
different sensors and try to associate and classify tracks that are related to the same 
target. This algorithm is NP-hard for more than two targets, and it needs to an ap-
proximative method to find the solution. A neural network method based on Hopfield 
structure proposed in [26] for this problem which always finds the optimal solution in 
17.4 percent of the times and finds a way that approximate the proper solution in 
remained time. Hopfield NNs have a feedback from output to input. So they can  
provide a dynamic response. These networks may be unstable but stability can be 
supported with forcing the weight matrix to be symmetric with zeros along its main 
diagonal. A recurrent network contains an associative memory. Therefore, just like a 
human memory, if a part of this memory is supplied, the network can return the full 
memory. Associative nature of neural networks has been used to identify targets that 
given very little information about them. In simple examples, these networks have no 
mistake in identifying of objects so they can be applied to identification and classifi-
cation of targets [12]. 
Back Propagation neural networks have been used for providing navigation capa-
bilities comparable with state of the art. Multi-layer networks explicitly need long 
training time. Radial basis function neural networks (RBFs) (such as those use net-
works with Localized Receptive Fields [27]) learn faster than Back Propagation ones 
because only one layer of weights needs to be modified. One of the most important 
problems of Multi layer neural networks is that determining of the number of appro-
priate hidden units is experimental. To solve this problem, Dynamic Node Crea-
tion(DNC) system [28]   have been proposed which starts from a small network and 
increases the size one node at a time until the network size become large enough to do 
requested job. It is shown in [29] that with a specified number of nodes, Back Propa-
gation neural networks simply tend to overtraining. While some neural networks 
consist of LRF and DNC don’t have such problems. However neural networks which 
encode output have been known as the most efficient networks. Object position esti-
mation has been also done by using neural networks. For example Neurally Inspired 
Contact Estimator (NICE) [30], have accuracy equivalent with Maximum Likelihood 
estimator (MLE), even one order faster. Recently, genetic algorithms have been used 
for designing of artificial neural networks for data fusion. Such methods have been 
applied to data fusion system of an electronic nose [31]. However neural networks are 
general purpose approaches to solve problems, but processing is too cumbersome in 
these networks. One of the main problems of these networks is how to identify the 
volume of training data. Applying too much or too little training data may cause the 
system to decide wrongly. So for identification in each platform and average weight 
taken from identification decisions, using of expert systems was proposed [32]. 
In [13] energy efficient organization for WSNs with the purpose of target tracking 
is proposed. Target positioning through cooperative sensing can be achieved with 
multisensory fusion. Last positioning results are used for adaptive prediction of tar-
get's trajectory. With composing of autoregressive moving average model (ARMA) 
and radial basis function neural networks (RBF), target position prediction can be 

 
A Survey on Application of Neural Networks in Energy Conservation of WSNs 
291 
efficiently done. Besides, by providing an efficient organization method, energy effi-
ciency of WSN will increase. In [14] researchers focused on using classification 
methods based on ART1 neural networks with the goal of reducing data traffic of 
node resulting in energy conservation. Sensor data which have too much redundancy 
first, have been classified by neural network in each node. Then, classified data were 
sent. In this way communication bandwidth increased efficiently. They used corpora-
tive routing and also Directed Diffusion for data routing. Both data routing results 
with and without classification have been compared according to WSN lifetime in-
crement. Authors in [15] proposed a new approach for optimal designation of duties 
by elastic neural network in tracking objects. First, a model of multi-coalition tracking 
multi-target is designed. Then disjoint fully connected sub graphs of neurons are con-
structed to solve the problem of optimized task allocation in tracking multi-target and 
increment of energy consumption when dynamic coalitions compete and conflict for 
the resources of sensor nodes. Compared with conventional methods, simulation re-
sults showed that the energy consumption of the tracking system was reduced signifi-
cantly and the tracking accuracy improved greatly demonstrating the effectiveness of 
elastic neural network in handling optimized task allocation problem of multi-sensor 
multi-target tracking. Multiple elastic neural network modules (MEMs) are an im-
provement to Self Organizing Map (SOM). MEMs generalize principles of self orga-
nizing model to enable management of wide range of complex optimization problems 
such as computer vision.Automatic context classification/recognition, usually by the 
analysis of measurement data from many sensor nodes, is a fundamental problem in 
human-computer interaction. Generally, mapping the sensor data to a context is quite 
difficult because of the requirement of real-time classification and possibility of train-
ing patterns which contain sensor noise [16]. In [16, 17] paper, context classifier 
based on Kohonen Self-Organizing Map and online classification of sensor data have 
studied. 
3.5   Neural Networks in Mobile Wireless Sensor Networks 
The mobile sensor data association in target tracking is one of most important tech-
niques for WSN. The main issue in data association tracking algorithms is to partition 
the sensor data into sets of observations produced by the same target, and the other 
one is to avoid the couple effect exists between the mobile sensors for the same target. 
Data Association Algorithms (DAAs) consist of three parts: acquiring, processing and 
combining. Mobile sensor tracking with DAA is a prerequisite step for mobile sensor 
surveillance systems over WSN deployment [18]. 
There are several algorithms for DAA have been proposed for Multiple Target 
Tracking problem such as JPDA (Joint probabilistic data association) technique [33] 
which is appropriate for environments with high false targets. But these techniques 
may cause some unreliability or latency due to all neighbor base methods usually con-
sider the relations between sensor measurements and existing target tracks independ-
ently. So neural network approaches based on Hopfield Neural networks (HNN) have 
been proposed in [34] to solve this problem. HNN which takes weighted objective cost 
and constraints into an overall energy function is employed to combine with the neural 
network approaches to work out good tracking results. Competitive Hopfield Neural 
Networks (CHNN) algorithm has been applied in image processing applications  

292 
N. Enami, R. Askari Moghadam, and A. Haghighat 
before. The difficulty for applying this method in DAA was that the determination of 
weight values was too difficult and it usually fell into irrational results. Recently re-
searchers combined HNN with genetic algorithm, called HNN-GA [35]. They used this 
technique in a mobile based strategy in which using low network load and cooperation 
of mobile agents, it could optimize task allocations among nodes. In [18] the authors 
tried to take advantages of HNN so they improved the CHNN method which can solve 
the above said problems by artfully managing of the updating function and the cost 
measurements. CHNN is an improved HNN in which a decision is made fully coopera-
tively. Each neuron receives information from other neurons and also gives informa-
tion to others. With this collective information, each neuron goes to a stable state with 
the lowest value of predefined energy function. The global association of mobile sen-
sor measurements and existing tracks will result to increase accuracy of mobile sensor 
tracking systems because in environments with dense targets the measurements pro-
duced by close targets can confuse the DAA algorithm and result in inaccurate rela-
tions. Moreover the competitive updating scheme of weights can solve the problems 
mentioned above and guarantee the convergence into a stable solution.  
4   Conclusion 
Since sensor nodes usually work with low power batteries, energy is a very scarce and 
valuable resource in WSNs. Therefore, energy consumption has to be managed rea-
sonably to increase the lifetime of sensor nodes. 
Today, Neural Networks are applied as effective tools in all aspects of reducing 
energy consumption such as duty cycling, data driven and mobility based approaches 
in WSNs. Neural Networks with their incomparable capabilities such as prediction, 
classification, identification and data fusion of sensor data can affect considerably on 
reducing energy consumption of WSNs by reducing communication or computation 
cost and result to prolong their operational lifetime. 
Consequently, more studies on different topologies of neural networks and their 
training algorithms should be done in future for different purposes to reduce energy 
consumption. Moreover, in multilayer networks, it is necessary to examine different 
types of transform functions at hidden layer. Likewise, self organizing (competitive) 
neural networks structures seem to be compatible with Wireless Sensor Networks 
applications due to self-organized and unsupervised nature of WSNs.  At the end, 
there are still many open problems to study on neural approaches of energy conserva-
tion in WSNs. 
References 
1. Anastasi, G., Conti, M., Francesco, D.M., Passarella, A.: Energy Conservation in Wireless 
Sensor Networks: a Survey. Ad Hoc Networks 7(3), 537–568 (2009) 
2. Shwe, H.Y., Xiao-hong, J., Horiguchi, S.: Energy Saving in Wireless Sensor Network. 
Journal of Communication and Computer 6(5), 20–28 (2009) 
3. Kulakov, A., Davcev, D., Trajkovski, G.: Application of wavelet neural-networks in wireless 
sensor networks. In: Sixth International Conference on Software Engineering, Artificial Intel-
ligence, Networking and Parallel/Distributed Computing and First ACIS International Work-
shop on Self-Assembling Wireless Networks (SNPD/SAWN 2005), pp. 262–267 (2005) 

 
A Survey on Application of Neural Networks in Energy Conservation of WSNs 
293 
4. Shen, Y., Guo, B.: Dynamic Power Management based on Wavelet Neural Network in 
Wireless Sensor Network. In: IEEE IFIP International Conference on Network and Parallel 
Computing – Workshops, pp. 431–436 (2007) 
5. Shen, Y., Guo, B.: Wavelet Neural Network Approach for Dynamic Power Management in 
Wireless Sensor Networks. In: 2008 International Conference on Embedded Software and 
Systems (ICESS 2008), pp. 376–381 (2008) 
6. Shahbazi, H., Araghizadeh, M.A., Dalvi, M.: Minimum Power Intelligent Routing In Wire-
less Sensors Networks Using Self Organizing Neural Networks. In: IEEE International 
Symposium on Telecommunications, pp. 354–358 (2008) 
7. Dehni, L., Krief, F., Bennani, Y.: Power Control and Clustering in Wireless Sensor Net-
works. In: Challenges in Ad Hoc Networking, pp. 31–40 (2006) 
8. Heinzelman, W.H., Chandrakasan, A., Balakrishnan, H.: Energy-Efficient Communication 
Protocol for wireless Sensor Networks. In: IEEE Proceeding of 33rd Hawaii International 
Conference on System Sciences, pp. 1–10 (2000) 
9. Heinzelman, W.H., Chandrakasan, A., Balakrishnan, H.: An Application-Specific Protocol 
Architecture for Wireless Microsensor Networks. IEEE Transactions on Wireless Commu-
nications 1(4), 660–670 (2002) 
10. Ye, M., Li, C., Chen, G., Wu, J.: EECS: An Energy Efficient Clustering Scheme in Wire-
less Sensor Networks. In: Proceedings of IEEE Int’l. Performance Computing and Com-
munications Conference (IPCCC), pp. 535–540 (2005) 
11. Park, I., Takeshi, M.: Energy Reduction in Wireless Sensor Networks through Measure-
ment Estimation with Second Order Recurrent Neural Networks. In: IEEE Third Interna-
tional Conference on Networking and Services (ICNS) (2007) 
12. Smith, D., Singh, S.: Approaches to Multisensor Data Fusion in Target Tracking: A Sur-
vey. IEEE Transactions on Knowledge and Data Engineering 18(12) (2006) 
13. Xue, W., Sheng, W., Ma, J.J., Bi, D.W.: Energy-efficient Organization of Wireless Sensor 
Networks with Adaptive Forecasting. In: Sensors 2008, vol. 8, pp. 2604–2616 (2008) 
14. Mohamed, W.K., Mirza, O., Kawtharani, J.: BARC:A Battery Aware Reliable Clustering 
algorithm for sensor networks. Journal of Network and Compute Applications 32(6), 
1183–1193 (2009) 
15. Mei, L., Haihao, L., Shen, Y., Fan, J., Huang, S.H.: Elastic neural network method for 
multi-target tracking task allocation in wireless sensor network. Computers and Mathemat-
ics with Applications 57(11-12), 1822–1828 (2009) 
16. Yun, S.U., Youk, Y.S., Kim, S.H.: Study on Applicability of Self-Organizing Maps to 
Sensor Network. In: International Symposium on Advanced Intelligent Systems, Sokcho, 
Korea (2007) 
17. Catterall, E., Laerhoven, K.V., Strohbach, M.: Self-Organization in Ad Hoc Sensor Net-
works: An Empirical Study on Arterial Life. In: Proceedings of eighth international con-
ference on Artificial life, pp. 260–263 (2002) 
18. Feng, X., Xu, Z.H.: A Neural Data Fusion Algorithm for Wireless Sensor Networks. In: 
Pacific-Asia Conference on Circuits, Communications and Systems, pp. 54–57 (2009) 
19. Shen, Y.J., Wang, M.-S.: Broadcast scheduling in wireless sensor networks using fuzzy 
Hopfield neural network. Expert Systems with Applications: An international Jour-
nal 34(2), 900–907 (2008) 
20. Pacific, F., Emery, W.J., Frate, F.D.: Neural Networks for Data Fusion. In: IEEE Data Fu-
sion Technical Committee, Remote Sensing Data Fusion Contest (2007) 
21. Al-Karaki, J.N.: Kamal. A. E.: Routing techniques in wireless sensor networks: a survey. 
IEEE Wireless Communications 11(6), 6–28 (2004) 

294 
N. Enami, R. Askari Moghadam, and A. Haghighat 
22. Yick, J., Mukherjee, B., Ghosal, D.: Wireless sensor network survey. Computer Networks: 
The International Journal of Computer and Telecommunications Networking 52(12), 
2292–2330 (2008) 
23. Kulkarni, R.V., Forster, A., Venayagamoorthy, G.K.: Computational Intelligence in Wire-
less Sensor Networks: A Survey. IEEE Communications Surveys and Tutorials 13(1) 
(2009) 
24. Demirkol, I., Ersoy, C., Alagoz, F.: MAC Protocols for wireless Sensor Networks: a Sur-
vey. IEEE Communications Magazine 44(4), 115–121 (2006) 
25. Lee, S.H., Yoo, J.J., Chung, T.C.: Distance-based Energy Efficient Clustering for Wireless 
Sensor Networks. In: Proceeding of the 29th Annual IEEE International Conference on Lo-
cal Computer Networks (LCN 2004), pp. 567–568 (2004) 
26. Winter, M., Favier, G.: A Neural Network for Data Association. In: Proc. IEEE Int’l. Conf. 
Acoustics, Speech, and Signal Processing, vol. 2, pp. 1041–1044 (1999) 
27. Moody, J., Darken, C.: Fast Learning in Networks of Locally Tuned Processing Units. 
Neural Computation 1, 281–294 (1989) 
28. Ash, T.: Dynamic Node Creation in Backpropagation Networks. Technical report, Inst. for 
Cognitive Science, Univ. of California, San Diego (1989) 
29. Ghosh, J., Holmberg, R.L.: Multisensor Fusion Using Neural Networks. In: Proc. Second 
IEEE Symp. Parallel and Distributed Processing (1990) 
30. DeAngelis, C.M., Whitney, J.E.: The Neurally Inspired Contact Estimator (NICE) In: 
Proc. IEEE Oceanic Eng. Soc. Conf., vol. 3, pp. 1619–1623 (1998) 
31. Abdel-Aty-Zohdy, H.S., Ewing, R.L.: Intelligent Information Processing Using Neural 
Networks and Genetic Algorithms. In: Proc. 43rd Midwest Symp. Circuits and Systems, 
August 2000, pp. 840–845 (2000) 
32. Kittler, J.: Multi-Sensor Integration and Decision Level Fusion. In: Proc. DERA/IEE 
Workshop Intelligent Sensor Processing (Ref. No.2001/050), pp. 6/1–6/6 (2001) 
33. Chung, Y.N., Chong, C.Y., Bar-Shalom, Y.: Joint Probabilistic data and association Dis-
tributed sensor Networks. IEEE Trans. Automa. Contr. AC-31, 889–897 (1986) 
34. Sengupta, D., Iltis, R.A.: Neural solution to Multitarget Tracking Data Association Prob-
lem. IEEE Trans. Aerosp. Electron. Syst. 25, 86–108 (1989) 
35. Salcedo-Sanz, S., Yao, X.: A hybrid Hopfield network-Genetic algorithm approach for the 
terminal assignment problem. IEEE Transactions on Systems, Man, and Cybernetics Part 
B: Cybernetics 34(6), 2343–2353 (2004) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 295–303, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
A Dynamic Distributed Tree Based Tracking Algorithm 
for Wireless Sensor Networks 
Aysegul Alaybeyoglu1, Aylin Kantarci2, and Kayhan Erciyes3 
1 Celal Bayar University, Computer Eng. Dept., Muradiye, Manisa, Turkey  
aysegul.alaybeyoglu@bayar.edu.tr 
2 Ege University, Computer Eng.  Dept., Bornova,  Izmir, Turkey 
aylin.kantarci@ege.edu.tr 
3 Izmir University, Computer Eng. Dept., Uckuyular, Izmir, Turkey  
kayhan.erciyes@izmir.edu.tr 
Abstract. We propose a dynamic, distributed tree based tracking algorithm for 
very fast moving targets in wireless sensor networks, with speeds much higher 
than reported in literature. The aim of our algorithm is to decrease the miss ratio 
and the energy consumption while tracking objects that move in high speeds. In 
order to do this, the root node which is determined dynamically in accordance 
with the node’s distance to the target, forms lookahead spanning trees along the 
predicted direction of the target. As the miss ratio decreases, the usage of re-
covery mechanisms which are employed to detect a target again that is moving 
away from the predicted trajectory also decreases. This decrease reduces the 
energy consumption and increases the network lifetime. We describe all the 
phases of the algorithm in detail and show by simulations that the  proposed al-
gorithm performs well to track very fast moving targets. We also compare the 
algorithm with the generic cluster, generic tree and dynamic multi cluster based 
tracking algorithms in terms of miss ratio and energy consumption. 
Keywords: target tracking, wireless sensor networks, localization. 
1   Introduction 
Recent technological advancements made sensor nodes cheap and readily avail- 
able for academic and industrial usage. Wireless Sensor Networks may consist of 
thousands of nodes deployed in a large area. Sensor nodes are suitable for various 
application types due to their sensing and wireless communication capabilities. 
Military surveillance, habitat monitoring and target tracking are some of the im-
portant types of applications for sensor networks [1, 2]. 
In target tracking applications, when a mobile target is sensed by some of the 
nodes, its position is calculated by cooperation of these nodes using localization 
techniques and aggregated data is sent to the sink node. There are many kinds of 
tracking algorithms in literature[3-6]. Tree based tracking is one of the type of these 
algorithms that underlies the proposed tracking algorithm. 

296 
A. Alaybeyoglu, A. Kantarci, and K. Erciyes 
 
In the generic spanning tree based tracking approach [7], spanning trees are 
formed dynamically as the events occur in the network area. A root node is selected 
from the nodes that detect the target. The root collects the sensed data and cal-
culates the location, speed and the direction of the target. With these information, 
it predicts the next location and sends a warning message to the node closest to the 
target’s predicted location. The node receiving this warning message, becomes the 
new root node and forms its spanning tree to be ready to detect the target. 
In the existing studies, the current root node at time t predicts the location 
only for time t+1. In case the target moves at high speed, it can pass by a group 
of nodes very fast without being detected. Therefore in these studies, as the tar-
get increases its speed, the probability of missing that target also increases. Due 
to the high probability of target miss, existing tracking studies uses recovery 
mechanisms frequently for target missing conditions. Using recovery mechanisms 
frequently consumes significant energy to detect the target again. In our study, in 
order to reduce the miss ratio of target, we propose to form lookahead span- 
ning trees in the predicted trajectory. Additionally, as a recovery mechanism we 
propose to increase the number of waked up nodes gradually. As the probability 
of missing the target decreases, the need for recovery mechanisms also decreases 
leading to reduction in energy consumption. 
To summarize, an intended contribution of our work is that we propose multi 
cluster version of the dynamic spanning tree based approach for very fast mov- ing 
targets. The other intended contribution is that we compare the proposed tracking 
algorithm with generic cluster and generic spanning tree based tracking ap-
proaches. In addition, we also compare the proposed algorithm with the multi 
cluster based tracking algorithm [8] for target miss ratios and energy consump- 
tion. Lastly, by decreasing the usage of recovery mechanisms we also decrease the 
energy consumption leading to increment in network lifetime. The rest of this pa-
per is organized as follows: In Section 2, the proposed tracking algorithm is ex-
plained in detail. The performance evaluations obtained from simulation results 
are presented in Section 3 where comparisons with few other algorithms are also 
done. Finally, conclusions and future works are given in Section 4. 
2   The Algorithm 
The Dynamic Multi Spanning Tree Based Tracking Algorithm(DMSTA) aims to 
track very fast moving targets by decreasing the ratio of missing the target. In 
order to decrease the ratio of target misses, the algorithm forms lookahead trees 
along the predicted target direction. The algorithm is designed in a distributed 
manner. The pseudocode of the proposed algorithm is given in Algorithm1. The 
algorithm contains root election, initial tree formation, dynamic root management, 
lookahead tree formation and recovery mechanism phases. In this section, these 
phases are explained in detail. 
 
 

 
A Dynamic Distributed Tree Based Tracking Algorithm for WSNs 
297 
 
Algorithm 1. The pseudocode of the DM ST A for nodei
1:  if I am an ordinary  node
2:  
if the target is detected
3:  
select a rootj node
4:  
if i am root  msg is received
5:  
become member  node
6:  
if hop count  value is greater  than  zero
7:  
send i am root  msg to neighbor nodes
8:  if I am a member  node
9:  
if the target is detected
10:  
send information msg to leaderj node
11:  
if you are root  msg is received
12:  
if hop count  is greater  than  zero
13:  
send you are root  msg to the neighbor node towards
14:  
the direction  of the target
15:  
else  if hop count  is equal to zero
16:  
become root  node
17:  
if tree hop count  is greater  than  zero
18:  
send you are root msg to the neighbor node towards
19:  
the direction  of the target
20:  
if raymond  root  msg is received
21:  
become root  node
22: if I am a root  node
23:  
if information msg is received
24:  
calculate location,  speed and direction  of the target
25:  
calculate the number  of spanning  tree to be formed
26:  
(tree  hop count)
27:  
send you are root  msg to the node towards  the direction  of the
28:  
target
 
2.1   Root Election and the Initial Tree Formation 
When the target enters the tracking  area, nodes in active state  and closer to the target 
detect  it and form the initial  tree by first selecting a node to be the root of the  tree.  
We  use a two  phase  timer  based  election  algorithm which  selects the  node closest 
to the  target as the  root  node. In this  algorithm, each node i that detects  the target, 
sets a timer  which varies in accordance  with the node’s received  signal  strength. The  
higher  the  received  signal  strength, the  smaller the  timer  value  is set.  The  node 
whose timer  expires  first  is the  closest  node to  the  target. Each  node waits  for 
expiration of its  timer  and  does not  send any messages to its neighbors until the 
timer  expires. If a node does not receive any candidate  messages until the timer  ex-
pires, it becomes a root candidate and sends candidate(rss, id)  message to its 
neighbors  including  its rss  and  its own id. Otherwise,  it gives up and selects the node 
that sends the candidate  message as the root node. Since the nodes may not be in one 
hop communication range of each other,  two or more root candidates may exist after 
the first phase. For that reason,  after  the  first phase,  each root candidate node i, sets 

298 
A. Alaybeyoglu, A. Kantarci, and K. Erciyes 
 
a second timer.  If until  the  second timer  expires,  the  root  candidate node i, receives 
a candidate message  with  higher  rss  value,  it  gives up  the  candidacy.  As a result,  
all root candidate nodes give up,  but  one with  the  highest  rss  value  becomes the  
root node. 
2.2   Dynamic Root Management 
After  the  initial  tree  is formed, the  root node collects all sensed data  from the 
nodes  in  its  tree.  As the  target moves,  the  closest  node to  the  target in  the 
tree  also  changes.  From  the  received  signal  strength values, the  current  root 
node  notices  that there  is a  node  closer  to  the  target than   itself  and  sends 
raymond  root  message to the closest node. 
 
Fig. 1. Root Changes  Dynamically 
As a result,  in the  proposed  algorithm, the  root  node changes  dynamically 
so that the node closest to the target always becomes the root node and target 
direction  is predicted  more accurately. In sequence, the tree  self organizes itself 
by  rearranging the  affected  links.  We  inspired  from  Raymond’s  algorithm, a  
token based algorithm for mutual exclusion on distributed system to reconfigure 
the tree [9]. It can be seen from the Fig. 1 that when the root node changes, the 
path  from member  nodes to the root node also dynamically  changes. 
2.3   Lookahead Tree Formation 
Upon detection  of the  targets, the  root node calculates  the  location,  speed and 
the  direction  of the  target. In accordance  with  the  calculated  speed,  the  root 
node defines the  number  of spanning  trees  to  be formed  and  starts lookahead 
spanning  tree  formation  process. Fig. 2 illustrates an example  operation  of the 
lookahead  tree formation  process. 
As it can be seen from Fig. 2, current root node n1, collects sensed data  via its 
spanning  tree and calculates  the location,  speed and the direction  of the target. 
n1 sends you are root message including hop count  and trajectory  parameters to 
the neighbor node n5 that is closest node to the target’s  direction.  The hop count 
parameter defines the number of spanning  trees to be formed along the direction 
of the  target. n5 forms its spanning  tree  and  decreases  the  hop count  value.  
 

 
A Dynamic Distributed Tree Based Tracking Algorithm for WSNs 
299 
 
n 1 6
n 4
n 2
n 1
n 1 7
n 1 8
n 7
n 5
n 1 2
n 1 0
n 1 1
n 1 3
n 9
n 1 4
h o p _ c o u n t - -
h o p _ c o u n t = 0
h o p _ c o u n t - -
n 3
h o p _ c o u n t ! = 0
h o p _ c o u n t - -
h o p _ c o u n t ! = 0
n 1 5
n 6
n 8
y o u _ a r e _ r o o t  ( h o p _ c o u n t  ,  t r a j e c t o r y )
m e s s a g e
i _ a m_ r o o t  ( i d i )    m e s s a g e
 
Fig. 2. Lookahead  Spanning  Tree Formation 
If  hop count  value is still greater  than  zero, it continues  to send the  you are 
root message received from node n1, to the  neighbor  node n11 that is closest to 
the target’s  direction.  This process continues until the hop count value becomes 
zero. 
2.4   Recovery Mechanism 
In case the  target moves at  high  speed,  it  can  pass  by a group  of nodes very 
fast  without being  detected. Therefore  as  the  target increases  its  speed,  the 
probability of missing that target also increases. A recovery mechanism is needed 
when the target missing occurs. A basic solution to recapture the missing target 
is to wake up all sensor nodes in tracking  area which results  in very high energy 
consumption. 
In  the  proposed  algorithm, when  the  current root  node notices  the  target 
missing,  it enlarges  its tree  by flooding alarm  message to its n hop neighbors. 
When  a node receives this message, it sends a reply message including  received 
signal strength value if it has detected  the target. Current root node continues to 
enlarge its tree until it determines the target’s  location. The recovery mechanism 
used in the proposed algorithm is given in Fig. 3. 
Tracking  very  fast  moving  target with  the  existing  approaches   results  in 
high miss ratios.  Due to the  high miss ratios,  existing  tracking  approaches  use 
recovery mechanisms  much more frequently  than  the proposed algorithm which 
also results  in more energy consumption. 

300 
A. Alaybeyoglu, A. Kantarci, and K. Erciyes 
 
n
n + 1
 
Fig. 3. Recovery Mechanism 
3   Simulation Results 
We implemented our tracking  algorithm in ns2  simulator  [10] version 2.31. Our 
mobility  pattern of the target is random  waypoint model which is supported by 
ns2. The speed of the target is varied to measure  the detection  capability of our 
algorithm under  different  mobility  conditions.  The  speeds  are  chosen  from  30 
m/s  to 100 m/s. 
In order  to  evaluate  the  performance  of DMSTA,  four different  algorithms 
are implemented: The Generic Dynamic Cluster  Based Tracking Algorithm (GD- 
CTA)  which constructs a cluster  dynamically  upon detection  of the  target, the 
Generic Dynamic Spanning Tree Based Tracking Algorithm (GDSTA) which con-
structs the  trees  dynamically   upon  detection   of the  target, the  Dynamic 
Multi  Cluster  Based  Tracking  Algorithm  (DMCTA)which pre-constructs  clus- 
ters  along the  trajectory of the  target and  finally our Dynamic  Spanning  Tree 
Based Tracking  Algorithm  (DMSTA). 
We plotted the  target’s  movements  against  speed to illustrate the  tracking 
accuracy  of our algorithm. As shown in Fig. 4.a and  Fig. 4.b, calculated  coor- 
dinates  are very approximate to the  real coordinates  when the  speed is varied 
between 30 m/s  to 40 m/s  and 50 m/s  to 60 m/s. 
When  the  speed  is varied  between  70 m/s  to  80 m/s  and  90 m/s  to  100 
m/s,  the  difference  between  real  positions  and  calculated  positions  is greater 
but  it is very important that the trajectory is preserved  under  very high mobile 
conditions  as shown in Fig. 4.c and Fig. 4.d. 
We  measured  the  miss ratios  of GDCTA,  GDSTA,  DMCTA  and  DMSTA 
against  very high speeds in Fig. 5. We obtained from the  simulations  that the 
proposed algorithm gives better results  than  the generic approaches  against  tar- 
get misses. 
When  we compared  the  proposed algorithm with DMCTA,  the  best perfor- 
mance is obtained with DMSTA in terms of target misses for fast moving targets. 
The  reason  for this  is the  higher  probability of catching  the  target with  one of 
the formed lookahead  spanning  trees. 
 

 
A Dynamic Distributed Tree Based Tracking Algorithm for WSNs 
301 
 
(a)  Mobile Scenario.  
(b)  High Mobile Scenario.
(c)  Very High Mobile Scenario.  
(d)  Extremely High Mobile Scenario.
 
Fig. 4.  Actual  and  Calculated Traces  for Dynamic  Multi  Spanning  Tree  Based Target 
Tracking  Algorithm 
 
Fig. 5. Comparison  of Miss Ratios 

302 
A. Alaybeyoglu, A. Kantarci, and K. Erciyes 
 
 
Fig. 6. Comparison  of Energy  Consumptions 
In Fig. 6, we compared the algorithms  in terms of energy consumption. As the 
target misses increase,  in order  to  recapture the  missing  target, the  need  and 
usage  of recovery  mechanisms  also increase  leading  high  energy  consumption. 
As it can be seen from the  figure, the  highest  energy consumption is obtained 
in GDCTA  algorithm because  of the  frequently  usage of recovery  mechanisms. 
GDSTA and DMCTA  follows GDCTA  in terms  of miss ratios.  Inspide  of looka- 
head tree formation,  the proposed algorithm DMSTA results  in the least energy 
consumption due to the least usage of recovery mechanism. 
4    Conclusion 
In this paper, a new distributed tracking  algorithm namely DMSTA is proposed 
for very fast moving targets at speeds up to 100 m/s  which is about  three  times 
higher than  the highest  speed of objects that can be tracked by sensor networks 
reported  in literature. The  aim of our  algorithm is to decrease  the  miss ratios 
as well as energy consumption. The proposed algorithm is compared  with three 
different tracking algorithms  in terms of miss ratios and energy consumption and 
it has been shown by the simulation  results that, for a range of speed values from 
30 m/s  to 100 m/s,  DMSTA  gives the  best performance  in terms  of both  miss 
ratio  and  energy consumption. DMSTA performs  well in miss ratios  because  of 
the lookahead  tree formation.  As the miss ratio  decreases, the usage of recovery 
mechanisms  also decreases resulting  in less energy consumption. We also showed 
by simulations  that the proposed algorithm performs well by constructing paths 
very close to the original motion  of the target at high speeds. 

 
A Dynamic Distributed Tree Based Tracking Algorithm for WSNs 
303 
 
References 
1. Thorstensen, B., Syversen, T., Walseth, T., Bjornvold, T.: Electronic Shepherd: A Low-
Cost, Low-Bandwidth, Wireless Network System. In: Proc. Second Intl. Conference on 
Mobile Conference on Mobile Systems, Applications and Services, pp. 245–255 (2004) 
2. Zhang, P., Sadler, C.M., Lyon, S.A., Martonosi, M.: Hardware Design Experiences in Ze-
braNet. In: SenSys 2004, ACM, pp. 227–238 (2004) 
3. Yang, W., Fu, Z., Kim, J., Park, M.-S.: An Adaptive Dynamic Cluster-Based Protocol for 
Target Tracking in Wireless Sensor Networks. In: Proc. of. WAIM 2007, pp. 157–167 
(2007) 
4. Chen, W., Hou, J.: Dynamic Clustering for Acoustic Target Tracking in Wireless Sensor 
Networks. IEEE Transactions on Mobile Computing 20, 258–271 (2004) 
5. Brooks, R.R., Ramanathan, P., Sayeed, A.M.: Distributed Target Classification and Track-
ing in Sensor Networks. IEEE Signal Processing Magazine 91, 1163–1171 (2002) 
6. Suganya, S.: A Cluster-Based Approach for Collaborative Target Tracking in Wireless 
Sensor Networks. In: Proc. of ICETET 2008, pp. 276–281 (2008) 
7. Zhang, W., Cao, G.: DCTC: Dynamic Convoy Tree-Based Collaboration for Target Track-
ing in Sensor Networks. IEEE Transactions on Wireless Communications, 1689–1701 
(2004) 
8. Alaybeyoglu, A., Erciyes, K., Kantarci, A., Dagdeviren, O.: Tracking Fast Moving Targets 
in Wireless Sensor Networks. IETE Technical Review (2010) 
9. Chow., R., Johnson, T.: Distributed Operating Systems and Algorithms. Addison Wesley, 
Reading (1997) 
10. VINT Project. Network Simulator version 2 (NS-2), in Technical report,  
 http://www.isi.edu/nsnam/ns 

Distributed Weighted Node Shortest Path
Routing for Wireless Sensor Networks
Onur Yilmaz and Kayhan Erciyes
Izmir University of Economics, Computer Eng. Dept.,
Balcova, Izmir 35350, Turkey
onur.yilmaz@ieu.edu.tr
Izmir University, Computer Eng. Dept.,
Uckuyular, Izmir 35340, Turkey
kayhan.erciyes@izmir.edu.tr
Abstract. Routing in Wireless Sensor Networks contains challenges,
including limited energy constraints, network density, wireless channel
errors. Diﬀerent approaches exist in literature to overcome these chal-
lenges, such as data centric, location based and hierarchical routing. Most
routing protocols in Wireless Sensor Networks are dealing with energy
eﬃciency and network lifetime. In this paper, we present a shortest path
routing algorithm based on Chandy-Misra’s distributed shortest path al-
gorithm regarding both node weight and edge weight. X percent of edge’s
weight and (100 - X) percent of node’s weight form a total cost between
neighbor and source node which is used in order to generate the shortest
paths and construct a spanning tree. Variation of X percent, node weight
and edge weight provide resilience for shaping needed paths and change
the spanning tree’s structure. When at least one node is close to critical
energy level or a fault occurs, the routing algorithm is re-executed and
new paths are generated. In order to obtain energy eﬃcient paths, high
network lifetime and ﬁnding out the overheads, we analyze the simula-
tion results by assigning the battery level to node weight, communication
cost to edge weight and %10, %30, %60 and %80 to X separately.
1
Introduction
Advances in hardware and wireless network technologies have created low-cost,
low-power, multifunctional miniature sensor devices [1]. These devices make up
huge distributed networks to perform a task, called Wireless Sensor Networks
(WSN). WSN are used as a new solution to challenges of many ﬁelds for in-
stance, disaster relief operations, biodiversity mapping, intelligent buildings, fa-
cility management and health care.
Researches face various challenges while working on every layer of network
stack. The most important challenges are limited in power, computational ca-
pacities and memory. In addition, sensors are prone to failures. Due to network
density and wireless communication, collisions and congestions occur more than
other networks[2]. Many new algorithms have been proposed so far to overcome
these challenges.
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 304–314, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

Distributed Weighted Node Shortest Path Routing
305
Routing algorithms in WSN have diﬀerent approaches depending on the ap-
plications requirements. [3] Data-centric, hierarchical and location based ap-
proaches are leading ones. Data-centric protocols are query-based and depend
on the naming of desired data, for instance SPIN[6] and Directed Diﬀution[7].
The goal of hierarchical protocols is clustering the nodes in order to save en-
ergy, for instance LEACH[11], PEGASIS[12] . Location-based use the position
instead of whole network, for instance MECN[9], SMECN[10], GAF[8]. The last
category is network ﬂow . In network ﬂow approach, primary goal is maximize
the network lifetime[13] or generating shortest paths[14].[3]
In this paper, we present a shortest path routing algorithm based on Chandy-
Misra’s distributed shortest path algorithm regarding both node weight and edge
weight. X percent of edge’s weight and (100 - X) percent of node’s weight form a
total cost between neighbor and source node which is used in order to generate
the shortest paths and construct a spanning tree. Variation of X percent, node
weight and edge weight provide resilience for shaping needed paths and change
the spanning tree’s structure. When at least one node is closing to critical energy
level or a fault occurs, the routing algorithm is re-executed and new paths are
generated. In order to obtain energy eﬃcient paths, high network lifetime and
ﬁnding out the overheads, we analyze the simulation results by assigning the
battery level to node weight, communication cost to edge weight and %10, %30,
%60 and %80 to X separately.
2
Related Works
Routing protocols are classiﬁed including data-centric, hierarchical, location-
based, QoS and network ﬂow [3].
SPIN[6], Chatzigiannakis et al.[4], and Directed Diﬀution[7] are data-centric
protocols. SPIN name the data with meta-data and this information is exchanged
among the neighbors. Directed diﬀusion is query based protocol which use the
interest messages. Sink sends interest messages to the network and if a node’s
data match with the interest message, it sends gradients towards the sink. Dur-
ing the source is sending gradients, the path is formed. Chatzigiannakis et al.
proposed a fault tolerant data dissemination protocol. In order to provide fault
tolerance to protocol, the nodes increase their transmission range while the nodes
are forming paths for selecting safe nodes.
LEACH[11], PEGASIS[12], CPEQ[5] are hierarchical protocols which use the
clustering. In LEACH, the clusters and cluster heads of sensor networks are
formed based on the receiving signal strength. Data is routed to sink through
the cluster heads. PEGASIS’s diﬀerence from LEACH is; it forms chains from
sensors instead of forming clusters. A node is selected to communicate with sink.
CPEQ is a for event-driven, query-based and periodic WSNs and provides fault
tolerance by forming multipaths between cluster heads.
MECN[9], SMECN[10], GAF[8] are location based protocols. In this approach,
location information is used for energy eﬃciency. if the region to be sensed
is known, the query is only diﬀused that region. Thus, energy dissipation is
prevented.

306
O.Yilmaz and K. Erciyes
[13] and [14] pursue network ﬂow approach. Shortest paths are generated from
sink to all nodes in this approach. A cost function is introduced in [11] to achieve
the maximum lifetime. Communication cost and residual energy form a total cost
as our algorithm but the algorithm wasn’t implemented in distributed manner
for simulation results and the energy dissipation of algorithm execution was not
considered.
3
Distributed Weighted Node Shortest Path Routing
(DWNSPR)
3.1
Overview of Algorithm
In this paper, our goal is to obtain energy eﬃcient paths from sink to all nodes
which are formed including higher energy level nodes considering lower commu-
nication costs and to change the paths dynamically when a node’s energy in a
path is close to critical level or a fault occurs. We think that tree structure ﬁts
our goal. The nodes at top of the tree, perform more forwarding then the nodes
at down of the tree in tree structure . If we construct the tree which higher en-
ergy nodes considering lower communication costs take place at top of the tree
with regard to location and neighboring, the WSN’s lifetime can increase . In
addition to this, a critical energy node take place as a leaf node if it isn’t at the
articulation point in graph. Thus, more fault-tolerant paths can be achieved.
Chandy-Misra’s distributed shortest path algorithm[15] nearly ﬁts our re-
quirements. Chandy-Misra’s algorithm forms shortest-paths from initiator node
to non-initiator nodes. A spanning tree is constructed after termination of ini-
tiator node. This algorithm starts when the initiator node sends to all neighbors
edge’s weight(probe message) between initiator and neighbor node severalty.
When the neighboring nodes get probe message, they do the same as initiator
node and sends probe to all neighbors. When a node receives probe message,
it checks whether new cost has lower value than existing one. If new cost value
is lower, it sets the sender as a parent and resends the probe messages. This
process continues until all nodes terminate. Due to this algorithm is distributed,
a termination detection mechanism is needed. In order to provide termination
detection, a node waits ack from all neighbors for each probe message. If a node
receives all acks, it sends ack to its pred. In this way, initiator node receives all
acks after all nodes terminate.
Our new algorithm is based on Chandy-Misra’s algorithm. A node sends only
edge’s weight in probe message in Chandy-Misra’s algorithm. In order to ﬁt our
requirements, we improve the algorithm with forming cost value with x percent
of edge’s weight and (100 - x) percent of node’s weight. Our contribution is to
variation of x shape the structure of spanning tree. For instance, if we assign
energy level to node weight, communication cost to edge weight and %90 to x
then, higher energy nodes locate at top of the tree. Thus, forwarding is mostly
done by higher energy level nodes. After termination of sink, nodes start to send
data to sink. If a node’s energy is close the critical level or a fault occurs while

Distributed Weighted Node Shortest Path Routing
307
Fig. 1. Comparison of two algorithm’s spanning tree
sending or receiving messages, it sends alert message to sink. Sink has a timer.
When the timer is triggered, sink checks whether any incoming alert message
exists. If it received at least one message in that constant time, our new algorithm
is restarted. Hereby, a critical energy level node or faulty node can be located at
leaf of the spanning tree or can be reduced the child count. This update of the
spanning tree’s structure gains us fault-prevention and high network lifetime.
The Fig. 1 displays an example of diﬀerence between Chandy-Misra’s algorithm
and DWNSPR. A WSN is simulated at ﬁrst as a undirected graph. A spanning-
tree is displayed at second graph after the Chandy-Misra’s algorithm execution
and the arrows indicate the spanning-tree. Another spanning-tree is also displayed
at third graph after the DWNSPR with %10 edge weight execution. The spanning-
tree at second graph is constructed regarding only communication cost but the
spanning-tree at third graph is constructed regarding both residual energy and
communication cost. Due to the cost is formed %10 of communication cost and
%90 of residual energy at third graph, higher energy level nodes are mostly se-
lected in generated paths. For instance, v2 has 2 children at second graph but v2
is a leaf node at third graph because of v2’s residual energy is low.
Fig.2 displays how the DWNSPR provides fault-prevention and network life-
time. At second graph, v5 is close to critical energy level and sends an alert
message to sink. After sink receives alert message, sink restarts the DWNSPR.
After the re-execution of algorithm, v5 becomes a leaf node. Thus, a possible
fault from depletion of node is prevented and network lifetime is increased by
forwarding load is passed from v5 to v2.
Fig. 2. Re-execution of DWNSPR

308
O.Yilmaz and K. Erciyes
Obtaining paths including higher energy level nodes at top levels can cause
increasing the running time and energy dissipation of algorithm and hop counts,
path length, and delays of paths. Our another goal in this paper is to analyze
which x value gives us the best tree considering these metrics.
3.2
Description of Algorithm
The algorithm we propose is described informally as follows. The sink starts the
algorithm by sending a PROBE message including cost value which is formed
with %x of edge weight and %(100-x) of node weight to its neighbors. Any
node i that receives a PROBE message, compares the cost value with existing
value. If the new cost is lower than existing parent’s cost, node i sets the sender
as its parent and sends PROBE message including cost value which is formed
with adding parents cost to %x of edge weight and %(100-x) of node weight to
neighbors. Any node i that sends PROBE message waits ACK message from
neighbors for each sent PROBE message. If node i receives all ACK messages,
it sends ACK message to its parent. Sink terminates after all node terminates.
Any node i that is at critical energy level sends ALERT message to sink. Sink
periodically checks whether received an ALERT message. If sink receives an
ALERT message, it restarts the algorithm by sending PROBE message.
Every node in the WSN performs the same local algorithm except Sink. Fig. 3
displays the FSM of DWNSPR for Sink and Fig. 4 displays the FSM of DWNSPR
for every node in WSN. Sink can be in IDLE, PARENT and TERMINATION
states and each node can be either in IDLE, CHILD, PARENT and TERMINA-
TION states.
The descriptions of FSM states:
– IDLE: Initially all nodes are in IDLE state. If Sink doesn’t have any neigh-
bor, all nodes in WSN stay in IDLE state
– PARENT: All nodes and Sink pass PARENT state after they send PROBE
message
Fig. 3. FSM of DWNSPR for Sink

Distributed Weighted Node Shortest Path Routing
309
Fig. 4. FSM of DWNSPR for Node
– CHILD: When all nodes except Sink receive PROBE, they pass CHILD state
– TERMINATION : When a node receives all waiting acks, it passes TERMI-
NATION state and sends ack to parent except Sink
The following is a list of messages used in DWNSPR:
– PROBE: Sent by a parent to the neighbors. Cost and SenderID are ﬁelds of
message
– ACK: Sent by the child to parent acknowledging for termination. SenderID
is ﬁeld of message
– ALERT: Sent by any node which is at critical energy level. SenderID is ﬁeld
of message
The following is a list of variables used in DWNSPR:
– X Value:Indicates that what percent of edge’s and node’s weight form the
cost value.
4
Simulations
4.1
Simulation Details
In order to evaluate our algorithm, measure the performance and ﬁnd out the
overheads, we have used ns-2 network simulator. The metrics which we decided
were running time and energy dissipation of algorithm and average hop counts,
average path lengths and average delays of generated paths for each edge weight
percentage. We executed the algorithm for each metric in 100, 150, 200, 250 and
300 nodes.

310
O.Yilmaz and K. Erciyes
Table 1. Simulation Parameters
Area
1000x1000m
Nodes Count
100, 150, 200, 250, 300
Mac
IEEE 802.11
Transmit Power
0.360w
Receiving Power
0.395w
Idle Power
0.335w
Initial Energy
Randomly generated between 2j to 10j
Transmission Range
75m
Network area is 1000x1000m. In our experiments, due to network connectivity,
we located nodes randomly by at least one node must be contained in per 50x50m
area. The sink is located at (0, 0) origins. The initial energy of nodes are assigned
randomly between 2j to 10j. IEEE 802.11 was used in mac layer, but we assume
that collisions are handled in mac layer and no corruption occurs . The following
table shows all parameters used in simulation.
4.2
Simulation Results
Fig. 5 displays the running-time results of the DWNSPR ranging from %10 to
%80 edge weight percent for 100 to 300 nodes. Run-time values increase almost
linearly each edge weight ratio, except for the some cases which may be due
to their random distribution. Running-times decrease %10 to %80 edge weight
percent in every density. This indicates that generating energy eﬃcient paths
require more time. In 300 node, 0.8s is needed in worst case to generate energy
eﬃcient paths.
Fig. 6 displays the energy dissipation results of the DWNSPR ranging from
%10 to %80 edge weight percent for 100 to 300 nodes. Energy consumption
Fig. 5. Running time of DWNSPR

Distributed Weighted Node Shortest Path Routing
311
Fig. 6. Average energy dissipation of DWNSPR
is an important challenge in WSN and we show the energy dissipation of our
algorithm. Due to a node consumes energy while sending, receiving message and
the idle waiting time, energy dissipation is dependent to these three metrics.
Fig. 5 indicates that generating energy eﬃcient paths require more time and
more transmissions. We can guess from the Fig. 5 results that generating energy
eﬃcient paths consume more energy.
The results in Fig. 6 conforms our inference. For instance, at %10 percent
0.18j energy and at %80 percent 0.13j energy are consumed in 300 node. 0.05j
energy is dissipated to generate more energy eﬃcient paths. When we look at
the relation between node count and energy dissipation, nearly 0.02j energy is
required for per 50 nodes.
The results above are related to algorithms execution. Another metrics which
have to be analyzed, are how the variation of edge weight percent aﬀect to delay,
Fig. 7. Average delay of paths

312
O.Yilmaz and K. Erciyes
Fig. 8. Average hop count of paths
Fig. 9. Average length of paths
hop count and path length of paths, because in order to obtain energy eﬃcient
paths, these metrics can increase.
Fig. 7 displays the average delays of generated paths after execution of the
DWNSPR ranging from %10 to %80 edge weight percent for 100 to 300 nodes.
There are few diﬀerences between diﬀerent edge weight percents in delays of
paths except for the case of %10 at 200, 250 and 300 nodes. Above ﬁgure shows
us that energy eﬃcient paths still have more overheads and have more delay at
high density, but the overhead is reasonable.
Fig. 8 displays the average hop counts of generated paths after execution of
the DWNSPR ranging from %10 to %80 edge weight percent for 100 to 300
nodes. Hop count is an important metric for network lifetime because if the hop
count increase for generating energy eﬃcient paths, this means that more nodes
include for forwarding data from source to sink. Thus, more energy is dissipated

Distributed Weighted Node Shortest Path Routing
313
at total in order to send a data to sink. As a result, generating energy eﬃcient
paths become out of purpose and worse aﬀect to network lifetime. The result at
the above chart shows that hop counts nearly same at each edge weight ratio in
constant node count. Thus, average energy consumption of paths for forwarding
data from source to sink is nearly same at each ratio and network life time
doesn’t aﬀect worse from energy eﬃcient paths.
Fig. 9 displays the average path lengths of generated paths after execution
of the DWNSPR ranging from %10 to %80 edge weight percent for 100 to 300
nodes. Path length is important as much as hop count metric. Like hop count,
path lengths are nearly same at each edge weight percent and network life time
doesn’t aﬀect worse from energy eﬃcient paths.
5
Conclusions
We proposed a distributed algorithm for constructing energy eﬃcient routing
paths from sink to entire network. Our algorithm is based on Chandy-Misra’s
distributed shortest path algorithm. The contribution in our improvement is to
X percent of edge’s weight and (100-X) percent of node’s weight form a total
cost for ﬁnding shortest paths. X value determines the structure of routing paths.
We assigned energy level of node to node weight, communication cost to edge
weight and executed the algorithm with assigning %10, %30, %60 and %80
values to X separately to ﬁnd out the overhead of generated paths. Simulation
results show that X value’s increase in other words selecting highest energy level
nodes in paths cause (0.05 - 0.1)second overhead in running time and (0.01 -
0.03)joule overhead in energy dissipation metrics in all measured densities. The
other metrics about the generated paths also aﬀect the X value’s increase but it is
reasonable diﬀerences. In conclusion, although generating energy eﬃcient paths
have overheads to WSN, simulation results show that X=%10 is reasonable.
The algorithm is re-executed for network lifetime and fault-prevention when
the sink receives an alert message from a node which is close to critical energy
level or a fault occurs. After the end of re-execution, corresponding node is
excluded from all paths or is reduced the count of included paths. Thus, network
lifetime and fault-prevention is achieved.
References
1. Bharathidasan, A., Ponduru, V.A.S.: Sensor Networks: An Overview. IEEE Poten-
tials 22(2), 20–23 (2003)
2. Akyildiz, I.F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: Wireless sensor
networks: a survey. Elsevier Computer Networks (2001)
3. Akkaya, K., Younis, M.: A Survey on Routing Protocols for Wireless Sensor Net-
works. Elsevier Ad Hoc Network Journal (2005)
4. Chatzigiannakisa, I., Kinalisa, A., Nikoletseas, S.: Fault-tolerant and eﬃcient data
propagation in wireless sensor networks using local, additional network informa-
tion. Journal of Parallel and Distributed Computing (2006)

314
O.Yilmaz and K. Erciyes
5. Boukerche, A., Martirosyan, A., Pazzi, R.: An Inter-cluster Communication based
Energy Aware and Fault Tolerant Protocol for Wireless Sensor Networks. Mobile
Network and Applications (2008)
6. Heinzelman, W., Kulik, J., Balakrishnan, H.: Adaptive protocols for informa-
tion dissemination in wireless sensor networks. In: Proceedings of the 5th An-
nual ACM/IEEE International Conference on Mobile Computing and Networking
(MobiCom 1999), Seattle, WA (August 1999)
7. Intanagonwiwat, C., Govindan, R., Estrin, D.: Directed diﬀusion: a scalable and
robust communication paradigm for sensor networks. In: Proceedings of the 6th
Annual ACM/IEEE International Conference on Mobile Computing and Network-
ing (MobiCom 2000), Boston, MA (August 2000)
8. Xu, Y., Heidemann, J., Estrin, D.: Geography-informed energy conservation for ad
hoc routing. In: Proceedings of the 7th Annual ACM/IEEE International Confer-
ence on Mobile Computing and Networking (MobiCom 2001), Rome, Italy (July
2001)
9. Rodoplu, V., Ming, T.H.: Minimum energy mobile wireless networks. IEEE Journal
of Selected Areas in Communications 17(8), 1333–1344 (1999)
10. Li, L., Halpern, J.Y.: Minimum energy mobile wireless networks revisited. In:
Proceedings of IEEE International Conference on Communications (ICC 2001),
Helsinki, Finland (June 2001)
11. Heinzelman, W., Chandrakasan, A., Balakrishnan, H.: Energy-eﬃcient communi-
cation protocol for wireless sensor networks. In: Proceeding of the Hawaii Interna-
tional Conference System Sciences, Hawaii (January 2000)
12. Lindsey, S., Raghavendra, C.S.: PEGASIS: power eﬃcient gathering in sensor in-
formation systems. In: Proceedings of the IEEE Aerospace Conference, Big Sky,
Montana (March 2002)
13. Chang, J.-H., Tassiulas, L.: Maximum lifetime routing in wireless sensor networks.
In: Proceedings of the Advanced Telecommunications and Information Distribution
Research Program (ATIRP 2000), College Park, MD (March 2000)
14. Ye, F., et al.: A scalable solution to minimum cost forwarding in large scale sensor
networks. In: Proceedings of International Conference on Computer Communica-
tions and Networks (ICCCN), Dallas, TX (October 2001)
15. Chandy, K.M., Misra, J.: Distributed Computation on Graphs: Shortest Path
Algorithms. Communications of ACM (November 1982)

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 315–326, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Host Based Dynamic Throughput Maximization Model 
for IEEE 802.11 WLAN 
Murat Koyuncu1, Mehmet Kazim Gercek2, and Tuncay Ercan3 
1 Dept. of Information Systems Engineering, Atilim University, Ankara, Turkey 
2 Undersecretariat of Customs, Prime Ministry, Ankara, Turkey 
3 Dept. of Computer Engineering, Yasar University, Izmir, Turkey 
mkoyuncu@atilim.edu.tr, mkg@gumruk.gov.tr,  
tuncay.ercan@yasar.edu.tr 
Abstract. As the demand for uninterrupted Internet access grows, the popular-
ity of wireless communication increases. However, wireless communication has 
some problems compared to conventional wired communication. Especially, if 
widely used Wireless Local Area Network (WLAN) applications are taken into 
consideration, it becomes an important issue to balance the load among avail-
able access points. It is impossible to balance the load when wireless hosts as-
sociate with an access point by using the classical approach of Received Signal 
Strength Index (RSSI). Some solutions containing a central server, requiring a 
specific brand of access point or protocol revisions have been proposed previ-
ously, but none of them has been favored as a generally accepted solution. In 
this study, a proposal which is central server free and requires no modifications 
to the existing infrastructure is presented. The proposed model is based on a 
dynamic determination of the least loaded access point to associate with, in or-
der to balance load and maximize throughput.  
Keywords: Wireless Communication, Wireless Local Area Network (WLAN), 
IEEE 802.11, load balancing, throughput maximization. 
1   Introduction 
Rapid developments in computer networks have changed the way of getting and stor-
ing information. Today, all kind of business and governmental institutes have their 
own data communication systems within their branches. They typically store their 
business specific data into central database servers and have their distant branches 
access this information to manage their work via network infrastructure. Usage of 
networking is not left only to business specific areas. Today, the network of networks, 
i.e. the Internet, can be accepted as the most popular invention of mankind. All kinds 
of information can be accessed instantly if you are connected to the Internet. People 
access the Internet for entertainment, business and social interactions. Consequently, 
people want the Internet to be accessible even if they are moving, and that leads to an 
increase in popularity of wireless networking.  
Wireless networking has some advantages as well as several problems. Load balanc-
ing can be regarded as one of the popular problems of Wireless Local Area Network 

316 
M. Koyuncu, M.K. Gercek, and T. Ercan 
 
(WLAN).  Handoff from one access point (AP) to another due to mobility may result in 
some AP to be overloaded. Strength of received signal does not give any information 
about the load of an AP. AP with the highest RSSI may be over-loaded while an AP 
with relatively lower RSSI could be idle. This situation causes ineffective usage of total 
available capacity. Therefore, it is required to implement an idea to balance load among 
APs serving in the same coverage area and increase the total throughput. However, the 
application of workstation distribution among APs concerning their load is a difficult 
task because of the movement ability of workstations [1]. Researchers try to define and 
offer some solutions to this problem. The proposed solutions may be grouped into four 
approaches: AP-based approach, central server-based approach, host-based approach 
and combined approach.  
AP-based solutions are generally proprietary solutions of AP producers [2]. Beside 
their solutions, some of the researchers thought that if handoff is done by AP and not 
by host itself, the load balancing could be reached. Manodham et al. [3] support this 
idea by defining a novel AP with two transceivers in which an additional one is used 
for communicating with neighboring APs for analyzing load and deciding handoff 
time. Velayos et al. [4] suggest that an agent running on AP should monitor network 
status. Agents send and receive information regarding load of AP to neighboring APs 
and select the best candidate host to transfer.  
Some researchers argue that load balancing can be achieved by using a central 
server. Bejerano et al. [5] claim that, in order to maximize the overall system 
throughput, the users may shift to idle or lightly loaded APs. They suggest that, by 
using an algorithm and collected information, the network operation center (NOC) 
can design a load balanced situation and inform connected hosts about new ideal 
associations. Jabri et al. [6] propose a central load balancing server, which downloads 
parameters from APs and then using a special algorithm tries to find the best host 
distribution among APs.  
The host based approach has been favored by Chen et al. [7]. They suggest that 
probe delay (PD) data can be an indicator of load on an AP. Especially if more than 
one probe is analyzed to calculate a Mean Probe Delay (MPD), it can be a more accu-
rate indicator. Due to exponential backoff time usage of 802.11, if traffic is heavier, 
transmission delay will be longer. Probe request and response frames which follow 
Distributed Coordination Function (DCF) procedure, the most popular media access 
model in wireless communication, are used by active scanning function. Probe re-
sponses are examined and AP over some SNR threshold is selected as a candidate. 
Then the one with minimum MPD is chosen as the least loaded AP to associate with.  
Some solutions to load balancing need to be run on both AP and wireless hosts 
(combined approach). Sheu and Wu tried to design an algorithm, i.e. Dynamic Load 
Balancing Algorithm (DLBA), running on both AP and wireless host [8].  One unique 
approach in load balancing has been proposed by Papanikos and Logothetis [1]. They 
suggest that the wireless host should be associated with an AP, according to the num-
ber of connected workstations and mean RSSI value. Since standard protocols do not 
have information about either number of workstations connected or mean of RSSI, 
protocol modifications of beacon and additional features in probe response are  
required.  
Although these efforts have defined the problems and suggested some solutions, 
none of them has been accepted as a standard by IEEE 802.11 working group. IEEE 

 
Host Based Dynamic Throughput Maximization Model for IEEE 802.11 WLAN 
317 
 
802.11 group has experienced some improvement in QoS based load balancing stud-
ies, but it has not been finalized yet [9]. In this paper, a novel load balancing and 
throughput maximization approach, which is named as “Host Based Dynamic 
Throughput Maximization (HBDTM) Model, is presented. The load should be dis-
tributed among available APs according to their bridging capacity taking into account 
both interfaces i.e. Radio and Ethernet. Whenever the proposed model runs, some 
parts of the load in the over-loaded AP will be transferred to relatively less loaded 
APs, and finally the load of APs will converge to a degree proportional to the capacity 
of each AP. In this definition, the load transfer is done by disassociation of hosts with 
low bandwidth usage from over-loaded AP and their association to less loaded one. 
The proposed solution uses a host-based approach, and it is dynamic since the load of 
APs are checked periodically during a session. 
Since all analysis and decision making procedures are carried out at host level by 
an agent running on each host, the proposed solution has advantages of being fully 
decentralized and autonomous. There is no need for centralized server or AP modifi-
cation. It can be used in any WLAN environment. The module also considers load 
balancing from a different point of view relative to previous studies in such a way 
that, it takes both interfaces into account during load balancing and throughput maxi-
mizing efforts. Therefore, the model is also different from previously proposed host 
based studies. 
This paper is arranged as follows: The proposed model’s technical layout is expli-
cated in Section 2. Implementation and test results are given in Section 3. Conclusions 
and future work are presented in Section 4. 
2   HBDTM Model  
2.1   General Design Issues 
Researchers generally suggest that load should be distributed among APs either using 
a central server or an AP-centric solution. These works consider only radio interface 
of AP and disregard Ethernet interface. However, the connection to wired infrastruc-
ture via Ethernet interface also determines performance of AP. For example, consider 
two APs both serving 54 Mbps access rate to associated hosts at its radio interface. If 
one of them has limited rate at Ethernet connection due to interface speed or colli-
sions, while the other provides higher speed without problems, the second AP may 
give better service to its associated hosts. Fig. 1 shows such an infrastructure.  
Similarly, imagine two neighboring cafés serving customers for accessing the 
Internet via their APs. Assume that some customers are in the coverage area of both 
APs, promising 54 Mbps of access rate, without considering their location of seats. If 
Café A has an Internet access rate of 8 Mbps but Café B has an access rate of only 2 
Mbps, customers whose wireless computers associated with the AP at Café A (AP-A) 
may be better served with the speed of the Internet access. However, it is not possible 
to guarantee customer satisfaction only considering connection bandwidth. If there are 
many customers connected to AP-A, downloading long files, customers connected to 
AP at Café B (AP-B) may be better served. In such a case, associating with AP serv-
ing relatively less uplink speed may offer better performance.  

318 
M. Koyuncu, M.K. Gercek, and T. Ercan 
 
 
Fig. 1. Typical network infrastructure with APs connected to backbone via different uplink 
speeds 
Unfortunately, there is no mechanism for such associations to maximize through-
put as seen in Café A and Café B example. Classical association deals only with RSSI 
and SNR but not load and/or uplink rate of AP. Researchers try to find a solution to 
the load balancing issue by only considering radio interfaces of AP as mentioned 
before. If there is no central server, probe delay time data can be used to guess load of 
AP as the most feasible method [7]. However, probe delay of radio interface cannot 
give exact information about the path to be used. Ethernet interface, i.e. uplink rate to 
wired infrastructure, should also be taken into account.  
The proposed approach (HBDTM Model) does not require; 
z 
A central server which makes network more complex, 
z 
AP modifications which force brand name specific solutions, 
z 
Protocol changes. 
The proposed model is transparent to wireless agent of operating system and runs on 
currently installed infrastructure without requiring any change. It offers a long lasting 
solution to the load balancing and throughput maximizing problem in such a way that 
load balancing should not be provided only at initial association but also during nor-
mal operation by checking load status of APs, periodically. 
One important issue in load balancing observed in previous works is the method of 
guessing load. If a central server or an AP centric solution is not used, the host should 

 
Host Based Dynamic Throughput Maximization Model for IEEE 802.11 WLAN 
319 
 
find a method to estimate load of the AP to associate with. This study has been also 
intended to find a mechanism to see the load of AP. For this reason, HBDTM model 
relies on visiting (associating with) each AP to learn load state of each periodically. 
2.2   Details of the Design 
The “Host Based Dynamic Throughput Maximizing (HBDTM)” model, as its name 
implies, is based on dynamically monitoring of bandwidth usage status of all APs 
under consideration and association with “Better Access Rate Offering to Gateway” 
(BAROG) AP, considering also Ethernet interface as well as radio interface to maxi-
mize throughput. Each host dynamically measures its bandwidth usage and decides 
whether it is a low bandwidth usage host (LBU host) or not. If it is a LBU host, then it 
associates to the next available AP in order to check load status. Through this process, 
if a BAROG AP is found, the wireless host makes handoff to this AP as a last action. 
As can be inferred from this explanation, the model offers a fully autonomous and 
decentralized solution. Decisions are made by hosts without depending on a central 
server or AP.  
The BAROG AP term needs to be explained before going into details of the model. 
As seen in the café example given above, both interfaces play an active role in per-
formance of AP. In order to have a balanced load environment to maximize through-
put, both interfaces should be taken into account. For this reason, generated traffic 
passing through both interfaces should be examined. This definition requires another 
player in the model. To be able to analyze traffic, a far end node should be determined 
in such a way that the route to it should pass over both interfaces. In other words, a 
special node located at Ethernet side should be found. This node can be the gateway 
of the local area network, or any web site having ability to answer ICMP requests if 
the gateway is just the AP itself. Considering Fig.1, AP-1 and AP-2 are connected to 
the same gateway. In this scenario, access rate between the host and the gateway is 
measured through AP-1 and AP-2 separately and the one giving the best rate is de-
termined as BAROG AP. 
According to the model, if the host’s bandwidth usage is determined as “low”, this 
host is selected to make a series of temporary handoffs to find BAROG AP. Selecting 
LBU hosts for such an action is an important decision. If a high bandwidth usage host 
is selected for this purpose, its association with less loaded AP may change its status 
to an over-loaded one, and this process may continue with subsequent meaningless 
series of handoffs. As it can be understood from this idea, LBU host handoff to a 
BAROG AP affects BAROG AP load status to a limited extent. During handoff proc-
ess, TCP connection is protected by Freeze TCP approach [10], and therefore upper 
layer connections are not affected by the switching process. At the same time cur-
rently associated hosts of new AP are not heavily disturbed since coming new host is 
just spending low amount of served bandwidth. After switching of a host, over-loaded 
or low access rate AP, which was the one associated before, starts to offer better per-
formance to its clients after this handoff. Finally, overall throughput of the network is 
maximized at the end of some series of handoff processes done by LBU hosts. Fig. 2 
summarizes the algorithm of the model running on each host. 
 

320 
M. Koyuncu, M.K. Gercek, and T. Ercan 
 
Wireless Host
Already 
Associated 
with an 
AP?
no
Associate with an AP
using 
classical approach
Run Environment Identifier module to examine environment
Run LBU Host Chooser module to identify host's candidacy
Run Access Rate Calculator module to find access rate to GW
Is there any 
more AP to 
associate with?
Run Result Analyzer module 
to determine BAROG AP and 
associate with it
yes
no
yes
Are there at 
least two APs in 
the area?
yes
no
Wait for random timeout 
period for the next run
yes
no
Is host an 
LBU host?
Run Temporary Handoff Maker
module to associate with 
the next AP
 
Fig. 2. Flowchart of the HBDTM model 
In this model all procedures are carried out by the host itself. An add-on service 
running on host’s wireless agent checks hosts bandwidth usage and decides whether 
this host should temporarily handoff to other APs. Since all procedures are carried out 
by add-on service running on the host, there is no need to use a central server in the 
network topology. 
According to the approach, a service linked to the wireless adapter should run 
automatically after random timeout periods. We used a value between 120 and 300 
seconds as timeout periods in the tests. The aim of using random timeout periods for 
the next run of the model is to prevent whole LBU hosts handoff to other APs at the 
same time. Because if all LBU hosts run the model at the same time and temporarily 
handoff to next AP together, they all may be associated with a certain AP, resulting in 
false decisions about load status of it.  
The model should also consider movement of the host. If host moves, its RSSI may 
start to decrease, giving rise to associate with another AP serving better RSSI. This 
handoff, caused by nature of wireless communication, may affect efforts aimed by the 
model. For this reason, the model should re-run without waiting for timeout period 
after such handoffs. In order to understand running method of the model, some com-
ponents mentioned in the algorithm needs to be examined. The main modules of the 
model are introduced briefly below: 
 

 
Host Based Dynamic Throughput Maximization Model for IEEE 802.11 WLAN 
321 
 
Bandwidth Usage Counter: HBDTM is a central server free solution. All hosts have 
their own decision-maker mechanism running independent of others. Since the model 
is based on temporary handoff of LBU hosts, every host should check its own traffic 
usage to decide its own candidacy. Therefore, in every host, Bandwidth Usage 
Counter runs as a service continuously monitoring its bandwidth usage. This module 
runs independently from the algorithm given in Fig.2. 
Environment Identifier: The Environment Identifier module runs to get information 
about the number of APs with RSSI of each in the coverage area. Since the aim is to 
balance load in order to maximize throughput, there should be at least two APs with 
acceptable RSSI values, serving in the same coverage area.  
LBU Host Chooser: Which host to play the active role is a challenging issue. Since 
HBDTM requires handoff of LBU host to next available APs, it must be decided 
which can be accepted as “LBU host”. For example, it is accepted that classical Inter-
net usage is based on web page browsing and it costs about 100 - 125 kbps of band-
width usage. Therefore, hosts just surfing on the web may be determined as LBU 
hosts.  
Access Rate Calculator: The Access Rate Calculator module plays important role in 
the model. As stated by Bejerano et al. [5], the load of AP can be given in terms of 
time it takes to complete transmission of certain amount of traffic. In other words, a 
certain amount of traffic should be created and time required for this purpose should 
be recorded to have an index value. This module measures access rate by sending pre-
loaded ICMP packages to the gateway.  
Temporary Handoff Maker: The Temporary Handoff Maker module helps the host 
to check load status of the next AP in the coverage area. This procedure is carried out 
by making handoff to the next AP, and then running the “Access Rate Calculator” 
module. The process is repeated up to the last AP in the area. In other words, this 
component helps host to associate with each AP in order to see the access rate status 
of each. It differs from the classical handoff procedure. Normally, a handoff is ex-
pected to occur if RSSI drops below acceptable strength. However, in this model, the 
next AP may offer lower RSSI when compared to the previous one. If RSSI is over an 
acceptable threshold, the host can still work while connected to this lower RSSI AP 
and increase the total throughput.  
An important issue regarding handoff execution is keeping TCP’s congestion win-
dow size at its current state. Since, the model requires subsequent handoff procedures, 
the host may be affected by restart of slow start phases of TCP’s congestion window 
management system by loosing packets whenever handoff occurs. Even low traffic 
usage hosts may face this problem. According to the idea behind the model, this loss 
may decrease total throughput. For this reason necessary action should be taken in 
order to keep the congestion window at its current status. As proposed by Goff et al. 
[10], sending zero window size propagation just before handoff will freeze the re-
transmit timer of the sender and enters it into a persist mode. After a successful hand-
off, a non-zero window propagation sent by the receiver will restart transmission of 
the sender at previous window size, resulting in no decrease in the congestion win-
dow. The problem with predicting correct handoff time which has been criticized by 

322 
M. Koyuncu, M.K. Gercek, and T. Ercan 
 
Lee et al. [11] and Qu et al. [12] is not an issue in HBDTM, since handoff is requested 
by the host itself but not a mandatory action activated by a decrease in RSSI. The host 
having the intension for handoff can activate precautions before the handoff process 
and can keep TCP transmission at its original state.  
Result Analyzer: Last component deals with analyzing results. After finishing subse-
quent temporary handoff procedures with rate calculation at each, the host should 
decide which AP is BAROG AP to associate with. This procedure compares load 
indexes of all APs, which are calculated by the “Access Rate Calculator” module and 
chooses BAROG AP.  
3   Test and Discussion 
In order to prove the model, it is preferred to make a real world testing. By using a 
real world test environment, it is intended to see not only contribution of the model to 
load balancing and throughput maximizing effort, but also practicability of the model.  
In the test bed, two APs connected to a switch which has another connection to a 
PC acting as a gateway have been used. Seven hosts equipped with wireless adapters 
have been associated with two APs manually. A gateway with two Ethernet interfaces 
have been used as a router to access a server located out of the LAN. To be able to see 
usage of model, one AP has been set to have different uplink speed at its connected 
switch port during the test. In order to minimize outer effects, those may deteriorate 
test results, necessary care has been taken. In the area covering test hosts, only test 
APs have been allowed to operate. Ethernet interfaces have been disabled in order to 
ensure all data communication at each host has been carried out via WLAN interface. 
At test phase, desktop computers equipped with external wireless adapters (ASUS 
WL-167g USB WLAN Adapter) have been used. In order to ignore time required to 
ask IP addresses from DHCP server after handoff, all PC’s have been set with static 
IP configuration. During the tests, all wireless hosts, gateway and server have been 
chosen to be operated by Linux. Two APs (ASUS Wireless Router WL-520g) having 
the same coverage area have been set up to bridge wireless hosts to wired infrastruc-
ture, with the SSIDs AP-Test1 and AP-Test2. In the test bed, each host had ability to 
associate with both APs. Channels of APs have been manually set to 1 and 6 rela-
tively to prevent possible signal interference. When active scanning has been initiated 
manually at each host, it has been noticed that all APs have more than 30/100 link 
quality which is an aggregate value containing level of interference, bit or frame error 
rate, received signal strength, some of the timing synchronization value etc. depend-
ing totally on the driver of interface [13].  
Using the ideas and algorithms mentioned in Section 2.2, necessary programs have 
been coded using Python programming language. At test phases, each host has been 
charged to use some amount of network resources. 125 kBps (1000 kbps) is accepted 
as the limit for LBU host candidacy. For this reason, during the test, each host has 
been assigned to one of the following groups: 
z LBU host group having average of <1000 kbps  
z Non-LBU host group having average of >=1000 kbps  
 

 
Host Based Dynamic Throughput Maximization Model for IEEE 802.11 WLAN 
323 
 
Two scenarios have been planned in order to test functionality of the model. In the first 
scenario, wireless hosts have been divided into two APs as three hosts have been asso-
ciated with one AP and remaining four with other AP. Then, the hosts have started to 
operate with charged load. The bandwidth usage of each host has been recorded at their 
own log file by using “Bandwidth Usage Counter” module on the basis of kB/min. In 
each log file, data on bandwidth usage has been indicated as the sum of data transmit-
ted and received. During the first test, the model has not yet been applied. 
For the second test, the proposed model with coded programs has been installed 
into each host and the necessary configuration has been completed. Same association 
as in the first test has been manually arranged. Then the model has been run as a ser-
vice to automatically check load status of each AP and balance load so as to maximize 
throughput. In this test, 1024 bytes of ICMP packets have been used to determine the 
BAROG AP, by the “Access Rate Calculator” module. The bandwidth usage of each 
host has also been recorded during the test. As expected, LBU hosts have been faced 
with a subsequent series of handoffs to find BAROG AP.  
Table 1. Throughput and load ratios of APs 
AP-test1 
AP-test2 
Phase  
Throughput 
(kB) 
% Load 
Throughput 
(kB) 
% Load 
Total  
Throughput  
(kB) 
First test (with classical 
model) 
774367 
51.72 
722823 
48.28 
1497190 
Second test (with 
HBDTM Model) 
530662 
32.60 
1097280 
67.40 
1627942 
Table 1 shows the result of a 30 minute test. Cumulative bandwidth usage value per AP 
can be seen in this table. Some of the loads previously observed on AP-test1 have been 
transferred to other AP and finally total throughput increased. When transferred load ratio 
has been calculated using data given in the table, the following value is obtained. 
Transferred Load = (774367 – 530662) / 774367 x 100 = 31.47 % 
The graph which is given in Fig. 3 shows minute based throughput values. It is seen 
that, starting from the first minute, the model performs better than the previous test 
and lasts to the end of the test. The total and minute based throughputs increase when 
the model is applied to the environment having at least two APs. Calculated benefit 
(contribution of the model) at the end of the test can be obtained as follows: 
Contribution of the Model = (1627942 - 1497190) / 1497190 x 100 = 8.73 % 
Link quality of each host has also been recorded during test. By using this data, aver-
age link quality can be calculated per AP as seen in Table 2. It is noticed that average 
link quality of AP-test1 increases and link quality of APs converges with each other 
after implementation of the model.  

324 
M. Koyuncu, M.K. Gercek, and T. Ercan 
 
Time Based Throughput
0
10000
20000
30000
40000
50000
60000
70000
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30
Time (min)
Throughput (kB)
Initial
With HBDTM Model
 
Fig. 3. Graph showing minute based bandwidth usage comparison 
Table 2. Average Link Quality comparison  
Phase 
AP-test1 
AP-test2 
First test (with classical 
model) 
32.35 
36.73 
Second test (with HBDTM 
Model) 
34.71 
36.65 
Initially 32.35/100 value of link quality of AP-test1 has increased to 34.71/100. 
This means an increase in link quality as follows: 
Link Quality Increase = (34.71 – 32.35) / 32.25 x 100 = 7.29 % 
At each model run during the second test, if host is an LBU host, preloaded ICMP 
packets were sent to the gateway in order to get access rate index value while associ-
ated with each AP. This traffic has also been analyzed in order to see the effect of the 
traffic created by the model. Cost of the model is calculated on average as 0.00307 % 
extra traffic created by the model. When compared with the benefit obtained by the 
model, its cost is negligible. 
4   Conclusions and Future Work 
In this study, a novel approach has been proposed to increase total throughput in a 
WLAN having multiple APs. According to this approach, a fully autonomous host 
dynamically monitors its bandwidth usage and decides its candidacy for temporary 

 
Host Based Dynamic Throughput Maximization Model for IEEE 802.11 WLAN 
325 
 
handoff, and if it is an LBU host, it makes a series of temporary handoffs to all avail-
able APs in order to find the best one to associate with. Associating with this AP does 
not ensure living in a balanced load environment. In other words, disassociation of 
only one host from over-loaded AP for association with a less loaded one cannot 
finish the load balancing process. However, at the end of some series of the handoff 
processes, they can find and associate with the best AP i.e. BAROG AP. At the end of 
these procedures, throughput maximization with balanced load is reached to a certain 
extend.  
Contribution of the model to load balancing and throughput maximizing effort has 
been proven by the tests done on the real test environment. Some amount of load has 
been transferred to less loaded BAROG AP by the help of the model. In other words, 
thanks to the model, the host decides which AP to associate with in order to have 
better bandwidth usage and to lead load balancing in the environment. As a conse-
quence, better bandwidth usage characteristics have been reached with certain amount 
of increase in total throughput. An increase has been observed not only in total 
throughput, but also in link quality values. Finally, all hosts living either in over-
loaded AP or BAROG AP have better bandwidth usage with better link quality lead-
ing to an increase in total throughput.  
At the end of the test, it has also been proven that, running only an HBDTM Model 
agent in a wireless host is enough to have better bandwidth usage in a multiple AP 
environment where all APs will be loaded with respect to their capacity considering 
both interfaces. It is evident that significant benefit can be reached by using autono-
mous and dynamic HBDTM Model in such a way that, there is no need to consider 
brand of APs, security vulnerabilities arising from central server or protocol revisions. 
But in an environment with a lesser number of wireless hosts or low traffic usage state 
where APs are not over-loaded, the model may not be needed in order to balance load. 
It may also be true for the model to have no effect on balancing load where all avail-
able APs are over-loaded. In such cases, the model runs as it is expected, but contri-
bution of the model may not be worth mentioning. 
As future work, the following improvements/tests could be applied in the model to 
improve the proposed model:  
• 
LBU host should be determined dynamically, i.e. the level defining LBU host 
should be changed accordingly during operation of the model. 
• 
Functionality of the model should be proven in an environment full of moving 
wireless hosts. 
• 
If it is running in an environment full of LBU hosts, candidacy of LBU hosts 
should be controlled by the historical handoff actions. In other words, to prevent 
frequent handoff of hosts, either the number of handoffs per host may be limited 
or timeout period may be extended.  
• 
Functionality of the model should be monitored in an environment in which 
some hosts are equipped with 802.11b interface and others with 802.11g. Ac-
cording to the idea behind the model, it can also be a solution to wireless anom-
aly problems. Hosts with 802.11b equipped interfaces may be associated with a 
specific AP to keep others running at a higher rate.   

326 
M. Koyuncu, M.K. Gercek, and T. Ercan 
 
References 
1. Papanikos, I., Logothetis, M.: A Study on Dynamic Load Balance for IEEE 802.11b Wire-
less LAN. In: Int. Conf. on Advances in Communication and Control, vol. 8, pp. 83–89 
(2002) 
2. Cisco Wireless LAN Controller,  
 http://www.cisco.com/en/US/docs/wireless/controller/3.2/ 
 command/reference/cli_3_2.pdf (Last Accessed: May 2009)  
3. Manodham, T., Loyola, L., Miki, T.: A Seamless Handoff Scheme with AP Load Balance 
for Real-Time Services Support in 802.11 Wireless LANs. IEICE Transactions on Com-
munications E91-B(5), 1463–1471 (2008) 
4. Velayos, H., Aleo, V., Karlsson, G.: Load Balancing in Overlapping Wireless LAN Cells. 
IEEE Int. Conf. on Communications 7, 3833–3836 (2004) 
5. Bejerano, Y., Han, S.J., Li, L.: Fairness and Load Balancing in Wireless LANs Using As-
sociation Control. ACM Transactions on Networking 15(3), 560–573 (2007) 
6. Jabri, I., Krommenacker, N., Divoux, T., Soudani, A.: IEEE 802.11 Load Balancing: An 
Approach for QoS Enhancement. Int. J. of Wireless Information Networks 15(1), 16–30 
(2008) 
7. Chen, J.-C., Chen, T.-C., Zhang, T., van den Berg, E.: Effective AP Selection and Load 
Balancing In IEEE 802.11 Wireless LANs. In: Proc. IEEE Global Telecommunications 
Conference (GLOBECOM), pp. 1–6 (2006) 
8. Sheu, S.-T., Wu, C.-C.: Dynamic Load Balance Algorithm (DLBA) for IEEE 802.11 Wire-
less LAN. J. of Science and Engineering 2(1), 45–52 (1999) 
9. Status of Project IEEE 802.11v,  
 http://www.ieee802.org/11/Reports/tgv_update.htm  
   (Last Accessed: June 2009) 
10. Goff, T., Moronski, J., Phatak, D.S., Gupta, V.: Freeze-TCP: A True End-To-End TCP 
Enhancement Mechanism for Mobile Environments. In: Proc. IEEE INFOCOM, pp. 1537–
1545 (2000) 
11. Lee, M., Kang, M., Kim, M., Mo, J.: A Cross-Layer Approach for TCP Optimization over 
Wireless and Mobile Networks. Computer Communications 31, 2669–2675 (2008) 
12. Qu, Z.-W., Zhang, J.: Optimization of the Transmission Mechanism for Wireless TCP. The 
Journal of China Universities of Posts and Telecommunications 15(1), 38–42 (2008) 
13. Ubuntu Man Pages For “iwconfig”,  
 http://manpages.ubuntu.com/manpages/karmic/man8/ 
 iwconfig.8.html (Last Accessed: August 2009) 

Joint Reliable and Power-Eﬃcient CDS-Based
Topology Control for Wireless Multi-hop
Networks
Elahe S. Hosseini1, Mahshid Yassaei1, Alireza Ejlali1,
Hamid R. Rabiee1, and Vahid Esmaeelzadeh2
1 Sharif University of Technology, Advanced Information and Communication
Technology Research Center, Iran, Tehran
{e hosseini,yassaei}@ce.sharif.edu, {rabiee,ejlali}@sharif.edu
2 Iran University of Science and Technology, Wireless Networks Research Laboratory,
Iran, Tehran
v esmaeelzadeh@iust.ac.ir
Abstract. High reliability and low power consumption are the crit-
ical objectives in wireless networks and the network topology is an
eﬀective issue in these objectives. This paper investigates these two
objectives in the wireless multi-hop networks simultaneously. For this
purpose, a connected dominating set CDS-based topology control
approach is proposed. In this approach a distributed topology control
algorithm with diﬀerent power adjustment measures is suggested.
Our goal is to self-organize this network with minimum interference
and power consumption subject to connectivity preservation. Unlike
many reliability enhancement algorithms, the proposed mechanism
does not compromise power consumption. Experimental results show
that the proposed topology control mechanism improves lifetime, relia-
bility as compared to the traditional CDS-based mechanism signiﬁcantly.
Keywords: Wireless Multi-Hop Networks, Topology Control, Hierarchi-
cal Topology Organization, Connected Dominating Set.
1
Introduction
In wireless multi-hop networks, nodes co-operate with each other without any pre-
conﬁgured infrastructure to handle the network traﬃc. The nodes form the net-
work topology based on their locations and transmission ranges autonomously [1].
Due to the presence of battery operated devices and error prone channels,
power conservation and data communication reliability are prominent issues in
wireless networks. Usually, error recovery process and reliability enhancement
methods impose negative eﬀects on the network performance and power con-
sumption [2]. Therefore to achieve the desirable levels of power consumption
and data communication reliability, a trade-oﬀbetween these objectives must
be considered (The connectivity and interference measures have been considered
as reliability measures in this paper).
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 327–337, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

328
E.S. Hosseini et al.
In wireless networks, topology is a main issue that is highly related to net-
work power consumption and performance. Omission of redundant and unnec-
essary topology information is called topology management [3]. [3] categorized
the topology management methods in Ad-Hoc networks into: Topology control
[4] and Hierarchical topology organization (clustering) [3], [5], [6].
The main goal of Topology control mechanisms is to minimize the power con-
sumption in order to extend the network lifetime [7]. This operation is done by
adjusting the transmission power of nodes while preserving the desired network
properties (e.g. connectivity) [4]. Also this minimization conﬁnes data commu-
nication interference. Several basic topology control algorithms have been pro-
posed. In the CBTC [8], each node increases its transmission power locally until
it can reach a neighbor node in each direction. Li et al.s proposed the Local
Minimum Spanning Tree (LMST) algorithm [9], in which each node builds its
MST locally. Then each node keeps on-tree nodes that are its neighbors in the
ﬁnal topology. This derived topology preserves the network connectivity, bi-
directionality and a node degree is bounded by 6. LINT and LILT [10] use
heuristics to adaptively adjust the node transmission power to reach the certain
number of node neighbors. In CLTC [11] nodes are divided into non-overlapping
clusters and two topology control mechanism are applied inside the clusters and
outside of the clusters. In this algorithm, it is proved that the resulting topol-
ogy is connected. [7] Proposed a centralized algorithm (LIFE) that computes an
interference-minimal connectivity-preserving topology.
In topology management based on hierarchicaltopology organization,nodes are
divided into groups with diﬀerent roles [11] and diﬀerent energy levels and oper-
tional modes [12]. The main results of these methods are simpliﬁcation of critical
functions such as bandwidth allocation, power control, etc [3]. Often a subset of
nodes are selected as network backbone [3], [13]. CDS is one of the best descriptions
of the hierarchicaltopology organization[3]. In graph theory a vertex of a graph is a
member of Dominating Set (DS) or at least is adjacent to a member of Dominating
Set. The connected DS sub-graph is named as Connected Dominating Set (CDS).
The CDS is appropriate for dense networks with high topology information redun-
dancy [3]. In the networks with a CDS communication backbone, the CDS nodes
change due to power, connectivity and etc constraints during the network lifetime.
The CDS nodes have the critical roles in this networks. Hence they are more power
consuming than others. Finding CDS with minimum nodes is an NP-hard problem
[13]. The CDS-based methods [3], [5], [13],[14] usually attempt to reduce the CDS
size based on various heuristic algorithms in order to conserve power. [5] presented
a CDS making algorithm which eliminates some CDS nodes to minimize the CDS
size using two pruning rules. [13], [14] extended the pruning rules of [5] to make the
more minimized CDS. According to rule k [14], if a CDS node has some non-CDS
(which are not a member of CDS) neighbors who are the neighbors of any k other
CDS nodes with higher priorities, the considered CDS node converts to non-CDS
node. The Span algorithm presented in [6] selects some nodes with high priority as
coordinators to form CDS. [15] shown that node degree and its residual power are
the main keys in the node priority and activity status decisions.

Joint Reliable and Power-Eﬃcient CDS-Based Topology Control
329
These methods usually do not consider the setting of nodes transmission
range. The non-CDS nodes do not participate in forwarding and routing tasks.
Using overestimated transmission range in non-CDS nodes may cause communi-
cation interference and wasting of nodes power as well. Also in these networks,
fast depletion of the power of the CDS nodes severely impacts the functionality
of the network.
In this paper, we have focused on large and dense wireless multi-hop networks
with CDS backbone. In dense topologies, due to high simultaneous transmission
of data packets between neighbor nodes, high interference may be induced. This
causes unnecessarily energy consumption. Since in many wireless applications,
the nodes are power constrained and data reliability is critical; Our main ob-
jective is to reduce power consumption and communication interference as well.
In this paper, a distributed CDS-based Topology Control algorithm is proposed.
This algorithm addresses the questions such as: ”which nodes must set their
transmission range?” and ”according to which criteria must nodes set their trans-
mission range?”. In this method each node set its transmission range according
to its role and some diﬀerent proposed measures. These diﬀerent measures de-
pend on the node interference value and distance of each node to its neighbors.
The role of nodes depends on node degree, residual power of nodes and nodes
neighbor conditions.
In this mechanism, the power consumption is directly proportional to CDS
size and nodes transmission ranges. Topology control aims to conserve the trans-
mission power and causes to change CDS size and data interference. As opposed
to previous works [3], [5], [13],[14], in our proposed approach, the CDS size is
not be strict minimal. This causes to distribute the network load and then the
network load is balanced among more CDS nodes. In fact, we proposed a dis-
tibuted topology control algorithm in order achieve the proper level of reliability
and power usage.
This paper is organized as follows. Section 2 proposes a topology control
method in CDS backbone. Experimental results are presented in Section 3. Fi-
nally, in Section 4 we conclude the paper and present the future works.
2
The Proposed CDS-Based Topology Control Approach
In this paper the main goal is to consider a trade-oﬀbetween power consump-
tion and interference of data communication while preserving of connectivity
in wireless multi-hop networks. To achieve this objective, we propose a topol-
ogy control mechanism on the network with CDS backbone. The CDS usually
is used as backbone of the network especially in high density networks. We
named the nodes which are members of CDS as ”Forwarder” and the other
nodes as ”Non-forwarder”. Since Forwarders route packets and forward the net-
work traﬃc, they consume more power than Non-forwarders in each time unit.
In this method, it has been assumed that the network nodes are power con-
strained. We used the power consumption model of nodes that is employed in [13]
and node interference measure of data communication proposed in [16]. Nodes

330
E.S. Hosseini et al.
transmission range usually related to the node interference amount. [16] deﬁned
the interference based on coverage model that says how many nodes are af-
fected by communication over a certain link. The topology control mechanism
causes to change the nodes transmission range, neighbors set of the nodes. In this
network, total power consumption is considered as sum of two amounts: Sum
of Forwarders power consumption (PtotalF ) and Sum of Non-forwarders power
consumption (PtotalNF ).
Ptotal = PtotalF + PtotalNF = nF PF + (N −nF )PNF
(1)
nF , PF and PNF are CDS size, Forwarders power consumption and Non-
forwarders power consumption respectively. It is obvious that PF >> PNF be-
cause being a Forwarder consumes more power. Plus the fact that, number of
Non-forwarder nodes is usually much greater than number of Forwarder nodes
(N −nF >> nF ). When CDS size is reduced, PtotalF decreases. While the topol-
ogy control mechanism is applied to the network, the nodes transmission ranges
and their neighbors set are changed. If nodes transmission ranges diminish, their
PtotalNF decreases. Also the node interference amount is reduced because the
interference measure [16] is proportional to nodes transmission range directly.
Hence in order to guarantee the nodes connectivity, some of Non-forwarders
are converted to Forwarders. Thus according to Equ.2, nF and PtotalF increase
correspondingly (α is the number of Non-forwarder nodes which convert to For-
warder). When nF is raised, the network load is distributed and balanced among
more Forwarder nodes. Thus Forwarders lifetime increases and disconnection
probability of the network decreases.
Ptotal = (nF + α)PF + (N −nF −α)PNF
(2)
On the contrary, if nodes transmission ranges grow, disconnection probability
of the network decreases. The fewer number of Forwarders is adequate; Hence
the nF and PtotalF go down. In this scenario, the variations of PtotalF and
PtotalNF are in contradiction. The interference measure is directly proportional
to PtotalNF and also connectivity probability is directly proportional to PtotalF .
Therefore, lower power consumption and higher reliability are achieved with
proper setting of nodes transmission ranges.
2.1
Distributed Topology Control Algorithm
The operation of our proposed algorithm divided into four stages that is ap-
plied on each node autonomously. The structure of this algorithm is depicted in
Figure 1. These stages are as follows:
In ﬁrst step, the role of nodes is determined. Nodes decide about their role,
based on nodes neighbor conditions and network measures. It is used from a
CDS construction method such as marking process [5] which guarantees the
network connectivity. Thus the set of Forwarder nodes (CDS) that forms the
main backbone of the network are determined.

Joint Reliable and Power-Eﬃcient CDS-Based Topology Control
331
Fig. 1. The basic steps of the proposed algorithm
In second step, in order to reach the MCDS, the size of the CDS (generated
from the previous step) is reduced. We used from the pruning rules of [13],
[14]. So as to attain the objective of this paper (reduce power consumption and
increase reliability), the Energy-Level-Based pruning rule of [13], [14] is selected.
According to this rule, Forwarder nodes with low degree or power level can be
converted to Non-forwarder nodes in appropriate conditions.
In third step, the transmission range of the nodes is adjusted. Since the CDS
forms the main backbone of the network, the connectivity of Forwarders is vital.
The reducton of transmission ranges in Forwarder nodes especially in mobile
nodes may be a risk from the network connectivity aspect. Furthermore, CDS
formation step, maintenance and updating operations using many control mes-
sages is inevitable. Topology control mechanisms impose many control messages
on the network. Therefore it is preferred that do not apply topology control
mechanisms to the Forwarder nodes. Non-forwarder nodes only need to connect
to at least one Forwarder in data transmission. Reduction of Non-Forwarders
transmission range are appropriate from power conservation and interference as-
pects. Thus Non-forwarders set their transmission range to reach at least one
Forwarder.
In this method, each Non-forwarder may connect to several Forwarders. Now,
a question arises that, ”according to which criterion, Non-forwarders must be
set their transmission range?”. In CDS backbone and its operations, Degree
and residual power of nodes are the main factors in the network lifetime and
reliability of data transmission. The degree measure tends to reduce the CDS
size and the power usage in each round. The node Residual power measure tends
to frequently change the nodes role thus cause to fairly balance network load
and prolong network life. In this network, we want to decrease the interference
of connections as far as possible. This decision must be related to power and
interference factors that aﬀect CDS formation and network lifetime. Due to
objective of this paper, the Non-forwarder transmission range can be adjusted
according to one of the following proposed factors:

332
E.S. Hosseini et al.
1. Distance to nearest Forwarder neighbor of the considered Non-forwarder
node (depicted in Figure 2(a)).
2. Distance to Forwarder neighbor node, with less interference communication
link between the Forwarder neighbor node and the considered Non-forwarder
node (depicted in Figure 2(b)).
3. Distance to Forwarder neighbor node with less value of product [Distance
to nearest Forwarder neighbor × Interference of their communication link]
(Depicted in Figure 2(c)).
4. Distance to most far Forwarder neighbor node of the considered Non-
forwarder node (depicted in Figure 2(d)).
First factor tends to reduce the necessary power usage of transmissions and
decreases the number of interfering neighbors. The second factor tends to connect
any non-forwarder to a Forwarder that the connection between them have the less
interference hence try to increase the reliability of transmissions. The third factor
is a compound of the ﬁrst two factors and points to both of power conservation
and reliability increment. Finally in fourth factor the largest transmission range
(a) First measure
(b) Second measure
(c) Third measure
(d) Fourth measure
Fig. 2. The network topology with apply the proposed topology control measures

Joint Reliable and Power-Eﬃcient CDS-Based Topology Control
333
of nodes is considered hence causes the less power conservation and interference
reducing. These factors have been evaluated in simulation experiments in section
3. The simulation results clearly investigate the eﬀectiveness of these proposed
factor. Therefore one of the stated factors is selected as topology control measure
and applied to the network in third step of this algorithm.
Since transmission range of Non-forwarders adjusted to connect a considered
Forwarder, environmental changes such as mobility of nodes, alternation of nodes
residual power, etc may endanger the network connectivity. In the last step of
this algorithm, the connectivity status of each Non-forwarder is evaluated. If
topology control process hurts the connectivity of a Non-forwarder, this node
increases its transmission range as far as possible.
3
Experimental Results
In this section, we carried out a series of experiments to assess the proposed
algorithm and illustrate the eﬃciency of it. These experiments were evaluated
using OMNet++ framework in the environment of 500m×400m and random
deployments that can easily generate irregular topologies. Nodes positions are
adjusted by auniform random distribution. It is assumed that Nodes in this map
have initial transmission range of 100 meter. The transmission range of each node
can vary from 20 to 100 meter. To consider diﬀerent node densities, the number
of network nodes varies from 40 nodes to over 200 nodes. All of the network
nodes are at full battery capacities ﬁrstly. The power of each node varies from
200 to 2000J. the topology connections are considered as bidirectional links. In
our network topology, a node directly connects to all the nodes that reside in its
communication range. For each node density, the experiments are repeated 10
times and the average values are taken.
In this simulation the Forwarder ratio considered as ratio of CDS size to
network size and the power consumption model of nodes that is employed in [13]
is used. In ﬁgures 3(a), the curves of 1, 2, 3 and 4 represent the result of the
ﬁrst, second, third and fourth proposed topology control measure respectively.
The curve of NON represents CDS-based wireless multi-hop network without
this topology control mechanism.
In this paper, the CDS structure is constructed through a marking process
which is proposed in [5] and by using of [13], [14] the number of CDS nodes is
minimized to some extent.
3.1
Forwarder Ratio
In our experiments, reduction of transmission range of nodes leds to decrease the
power consumption. By use of the ﬁrst proposed topology control factor, trans-
mission range of each Non-forwarder (each Non-forwarder connects to a nearest
Forwarder neighbor) become smallest relative to other proposed factors. For the
mentioned reason in section 2, the size of CDS is increased more than other fac-
tors (As depicted in Figure 3(a)). In the second factor, each Non-forwarder in its

334
E.S. Hosseini et al.
(a) Forwarder Ratio vs. Nodes Number
(b) Interference vs. Nodes Number
(c) Lifetime vs. Nodes Number
(d) Message Overhead vs. Nodes Number
Fig. 3. The comparison of the reliability and power consumption parameters in the
networks with the apply of diﬀerent proposed topology control measures
transmission range had some Forwarder neighbors. The probability of CDS size
increment lowered relative to the ﬁrst factor. The CDS size increment in using
of the third factor is an intermediate state between two other previous factors
(very near to second factor) and needed to fewer number of Forwarders than the
ﬁrst factor. The fourth proposed topology control factor provides biggest node
transmission range than other factors. Thus as depicted in Figure 3(a), apply of
the topology control mechanism causes to increase the Forwarder ratio.
3.2
Interference
Reduction of nodes transmission range reduces the data communication inter-
ference. In the ﬁrst proposed topology control factor, Non-forwarders had the
smallest transmission range and thus the lowest interference value. By use of
the second factor, Non-forwarders have the larger transmission range than ﬁrst
factor and higher interference value. In use of the third factor, Non-forwarders

Joint Reliable and Power-Eﬃcient CDS-Based Topology Control
335
had larger (near to ﬁrst factor) transmission range than ﬁrst factor and its in-
terference reducing is very near to the second factor. Whereas the fourth factor
provides largest nodes transmission range than other factors, interference value
of this factor is more than others. As depicted in Figure 3(b), third factor have
almost the best result from the interference aspect.
3.3
Lifetime
We deﬁned the lifetime of the network as the number of rounds before the ﬁrst
node has no power left to perform the assigned task. The topology control mech-
anisms adjust the transmission power. Then they cause to conserve the nodes
power consumption. Hence the network can survive more. In our experiments, by
use of the ﬁrst proposed topology control factor, CDS size and power consump-
tion increased. Thus the lifetime of network is lower than the other two factors.
In the second factor, the network have the smaller CDS size with more inter-
ference compared of the ﬁrst factor. Whenever interference and number of data
retransmissions increase, the power consumption and lifetime of the network
in this factor is more than the ﬁrst factor but less than third factor. Whereas
fourth factor provided biggest nodes transmission range and interference value,
the number of data retransmissions in this factor is higher. Consequently the
lifetime of fourth factor is lower than others. As depicted in Figure 3(c), by use
of the third factor whereas the Forwarder ratio is lower than ﬁrst factor and
number of retransmissions was less than second factor, the optimal lifetime is
attained related to others. Execution of The network lifetime is enhanced by
means of topology control mechanism on CDS structure.
3.4
Message Overhead
Generally in a wireless networks with CDS backbone, more control messages
are required to be exchanged due to CDS formation, maintenance and updating
operations. By use of the ﬁrst factor, Forwarder ratio increases. This , high
role rotation, which then caused message overhead to increase. In the second
factor, despite more interference (more than ﬁrst factor and third factor), data
retransmissions increased. As depicted in Figure 3(d), message the overhead in
second factor was lower than ﬁrst factor and higher than using the third method.
4
Conclusions
In this paper, we investigated the impact of nodes tansmission range changes
on the network power consumption and reliability. We proposed a distributed
topology control mechanism on wireless multi-hop networks with CDS backbone.
This mechanism enhances the joint network power consumption and reliability.
This mechanism leads to increase the CDS size. However this causes to decrease
the power usage. The larger CDS size causes to the network load would distribute
and balance among more Forwarder nodes. Thus the network nodes can be

336
E.S. Hosseini et al.
survived more. Simulation results depicted that the proposed topology control
improves the network communication interference by 7%-50% (by use of diﬀerent
topology control measures) as compared to the traditional CDS-based agorithms
[3]. The CDS size increased about 3%-31%. The lifetime of the network increased
10%-30% rounds as compared to [3]. Also, simulation results depicted that the
third proposed topology control factor is the most proper factor to achieve our
objective.
MCDS construction and topology control mechanism imposes extra message
overhead to the network. Therefore, in our future work we intend to reduce the
message overhead by predicting the role of the nodes in the given network.
Acknowledgment
The authors would like to thank members of Sharif Digital Media Lab (DML) for
their invaluable cooperation. This work was supported by Iran elecommunication
Research Center (ITRC).
References
1. Furht, B.: Encyclopedia of Multimedia. Springer Publishing Company, Incorpo-
rated, Heidelberg (2008)
2. Lettieri, P., Fragouli, C., Srivastava, M.B.: Low power error control for wireless
links. In: MobiCom 1997: Proceedings of the 3rd annual ACM/IEEE international
conference on Mobile computing and networking, pp. 139–150. ACM, New York
(1997)
3. Bao, L., Garcia-Luna-Aceves, J.J.: Topology management in ad hoc networks. In:
Proceedings of the 4th ACM international symposium on Mobile ad hoc networking
\&amp; computing, pp. 129–140. ACM, Annapolis (2003)
4. Santi, P.: Topology control in wireless ad hoc and sensor networks. ACM Comput.
Surv. 37(2), 164–194 (2005)
5. Wu, J., Li, H.: On calculating connected dominating set for eﬃcient routing
in ad hoc wireless networks. In: Proceedings of the 3rd international workshop
on Discrete algorithms and methods for mobile computing and communications,
pp. 7–14. ACM, New York (1999)
6. Chen, B., Jamieson, K., Balakrishnan, H., Morris, R.: Span: an energy-eﬃcient co-
ordination algorithm for topology maintenance in ad hoc wireless networks. Wirel.
Netw. 8(5), 481–494 (2002)
7. Burkhart, M., von Rickenbach, P., Wattenhofer, R., Zollinger, A.: Does topology
control reduce interference? In: Proceedings of the 5th ACM international sympo-
sium on Mobile ad hoc networking and computing, pp. 9–19. ACM, Roppongi Hills
(2004)
8. Li, L., Halpern, J.Y., Bahl, P., Wang, Y.M., Wattenhofer, R.: A cone-based dis-
tributed topology-control algorithm for wireless multi-hop networks. IEEE/ACM
Transactions on Networking 13(1), 147–159 (2005)
9. Li, N., Hou, J.C., Sha, L.: Design and analysis of an MST-based topology control
algorithm. IEEE Transactions on Wireless Communications 4(3), 1195–1206 (2005)

Joint Reliable and Power-Eﬃcient CDS-Based Topology Control
337
10. Ramanathan, R., Rosales-Hain, R.: Topology control of multihop wireless networks
using transmit poweradjustment. In: Proceedings of Nineteenth Annual Joint Con-
ference of the IEEE Computer and Communications Societies, IEEE INFOCOM
2000, vol. 2 (2000)
11. Liu, R.: CLTC: a Cluster-Based topology control framework for ad hoc networks.
IEEE Transactions on Mobile Computing 3(1), 18–32 (2004)
12. Zheng, R., Kravets, R.: On-demand power management for ad hoc networks (2003)
13. Wu, J., Gao, M., Stojmenovic, I.: On calculating power-aware connected domi-
nating sets for eﬃcient routing in ad hoc wireless networks. In: ICPP 2002: Pro-
ceedings of the 2001 International Conference on Parallel Processing, pp. 346–356.
IEEE Computer Society, Washington (2001)
14. Wu, J., Dai, F.: A generic distributed broadcast scheme in ad hoc wireless networks.
IEEE Transactions on Computers 53(10), 1343–1354 (2004)
15. Universitaria, M.D.F.: New metrics for dominating set based energy eﬃcient ac-
tivity scheduling in ad hoc networks. In: Proceedings of the 28th Annual IEEE
International Conference on Local Computer Networks (LCN 2003), vol. 742,
p. 17 (2003)
16. Moaveni-Nejad, K., Li, X.Y.: Low-interference topology control for wireless ad hoc
networks. Ad Hoc & Sensor Wireless Networks 9, 53 (2005)

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 338–348, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Vertical Handoff Decision Schemes for Heteregeneous 
Wireless Networks: An Overview 
Fatma Tansu and Muhammed Salamah 
Computer Engineering Department, Eastern Mediterranean University,  
TRNC, Mersin 10 Turkey 
{fatma.tansu,muhammed.salamah}@emu.edu.tr 
Abstract. The next generation wireless networks are expected to support differ-
ent applications within a heterogeneous network environment. In such net-
works, mobile users can use the advantage of diverse networks by switching be-
tween them. The most challenging issue in heterogeneous wireless networks is 
the Always Best Connected (ABC) concept in order to support best connectivity 
to mobile users at anywhere anytime from any network. Recently, many vertical 
handoff decision schemes have been proposed in the literature in order to pro-
vide solution to ABC. As traditional handoff decision is mainly based on re-
ceived signal strength (RSS), vertical handoff decision must take into account 
other factors like user mobility, network conditions, pricing issues of diverse 
networks, and user preferences in addition to RSS. In this paper, we first outline 
the main characteristics needed for vertical handoff decision, and then we pre-
sent an overview of the most recent and interesting vertical handoff decision 
schemes. We classify these strategies and present their main characteristics. 
Keywords: Heterogeneous wireless networks, vertical handoff, handoff decision. 
1   Introduction 
The next generation wireless network environments are increasingly become inte-
grated to support anywhere, anytime connectivity. Recent research has focused on the 
integration  of different technologies in a heterogeneous wireless network environ-
ment in order to meet ABC requirements for various applications like high data 
rate, and multimedia with appropriate quality of service (QoS). One promising way 
to construct a heterogeneous wireless network is to integrate  microcellular networks  
(such  as  Wireless  Local  Area Networks  (WLANs)),  and  macrocellular networks 
(such as General Packet  Radio  Services  (GPRS),  Universal  Mobile Telecommu-
nication System (UMTS), etc.). Each technology was separately designed to  meet  
different  coverage  areas,  bandwidth,  cost  models  and traffic  needs. Microcel-
lular  networks have a competing  and complementary  technology  to macrocellular 
networks [1]. They provide high  data rate services at low cost over small cover-
age area with low mobility. On the other hand, macrocellular  networks provide 
lower data rate services over larger coverage area with higher mobility and cost.  
By  revealing  the  ABC  concept,  integration  of  diverse networks in such hetero-
geneous wireless networks becomes a challenging task. Generally, heterogeneous 

 
Vertical Handoff Decision Schemes for Heteregeneous Wireless Networks 
339 
 
wireless networks have  overlapping coverage areas of two or more diverse net-
works like picocell,  microcell, macrocell and global cell in hierarchical manner, 
where higher  levels in the hierarchy provide lower bandwidths and higher delays  
over  larger  coverage  areas  [2].  Fig. 1 shows  a  typical  architecture  of  a hetero-
geneous wireless network. 
 
Fig. 1. Architecture of a heterogeneous wireless network 
One major aim of heterogeneous wireless networks is to take the advantages of 
high data rate of lower levels, and wide coverage area of higher levels. The exis-
tence of such networks will enable users to move freely from one network to another, 
which is referred as vertical handoff [2, 3]. An important goal of vertical handoff is to 
maintain a  user active connectivity while the user is changing its point of  attach-
ment. Therefore, an efficient vertical  handoff decision strategy for such networks is a 
challenging issue in the recent research topics. 
This paper presents the most interesting and recent vertical handoff  decision 
strategies in the literature. These strategies provide solutions to ABC concept by 
answering  the  question of when and where a mobile user should handoff  in a het-
erogeneous network. The  paper is organized  as follows: Section 2 gives informa-
tion  about handoffs in heterogeneous wireless networks. An overview of vertical 
handoff decision process is given in section 3. A comprehensive classification of  ver-
tical  handoff decision schemes  and  review  of  most  recent  and  interesting 
schemes are given in section 4. Section 5 highlight the open issues and  indicates 
future directions. Finally, section 6 concludes the paper. 
2   Handoff in Heterogeneous Wireless Networks 
Handoff is a process of transferring a mobile station’s (MS) point of  attachment be-
tween cells of the same or different networks while maintaining its active connection 
[4]. In macrocellular networks, such point  of attachment is referred as base station 

340 
F. Tansu and M. Salamah 
 
(BS), and in microcellular networks it is usually called access point (AP) [5].  
In homogeneous networks, a handoff decision is mainly based on the RSS at the MS, 
and it is required when RSS degradation occurs at the MS. This type of handoff is 
referred as horizontal  handoff. However, in heterogeneous networks, a vertical hand-
off may be initiated for the convenience and/or preferences of a user rather than  
degradation in RSS [6]. In order to make an efficient vertical handoff decision, other 
factors like user mobility, network conditions, pricing issues of diverse networks, and 
user preferences should also be taken into account [7]. Moreover, vertical handoffs 
are  classified  into  two  categories: a downward  vertical  handoff,  and  an  upward 
vertical handoff [3]. The first type is a handoff to a wireless network with a smaller 
coverage area with higher bandwidth per unit area;  whereas the second type is a 
handoff to a wireless network with a larger coverage area with lower bandwidth per 
unit area. 
Despite the types of handoff, the handoff decision process can be located  in a 
network, or the MS [8]. The handoff decision is based on some measurements and 
information in order to perform an efficient one. These measurements  and informa-
tion can be collected by the network, MS or both as follows: (i) mobile- assisted  
handoff  (MAHO):  the mobile participates in the handoff  decision  by collecting 
information and measurements which are used by the network for deciding handoff,  
(ii)  network-controlled handoff (NCHO): the network decides to make handoff by 
collecting information of the MS, (iii) mobile-controlled handoff (MCHO): the MS 
collects the information and decides handoff on its own. 
3   Vertical Handoff Decision Process 
In order to solve the problem of making a correct decision to handoff, several handoff 
decision strategies have been proposed in the literature [8-20]. Handoff decision 
strategies are employed to determine when and where the handoff should occur. In 
traditional handoff decision strategies, RSS  comparisons are adequate to perform a 
horizontal handoff [6, 21]. These strategies can be classified as follows: 
•  RSS: the new BS is selected, if RSSnew  >RSScurrent. 
• 
 RSS and Threshold: the new BS is selected, if RSSnew      >   RSScurrent and      
RSScurrent < Threshold. 
•  RSS plus Hysteresis: the new BS is selected, if RSSnew > RSScurrent + Hysteresis. 
•  RSS, Hysteresis and Threshold: the new BS is selected, if RSSnew   >  RSScurrent + 
Hysteresis and RSScurrent  < Threshold. 
•  Strategy plus Dwell timer: Dwell timer can be used with the strategies above. A timer 
is started when the condition is true and  handoff is performed, if the condition  
continues to be true until the timer expires. 
However, in heterogeneous networks, RSS comparisons are not sufficient to make a 
vertical handoff decision, as they do not take into account the various attachment 
options for the mobile users [2]. As the heterogeneous networks give options to 
mobile users to benefit  the advantages of diverse network characteristics, the follow-
ing  crucial factors should be considered for a sufficient vertical handoff decision: 

 
Vertical Handoff Decision Schemes for Heteregeneous Wireless Networks 
341 
 
•  User mobility: User mobility includes dynamic factors like speed and location 
information. 
•  Network conditions: Network conditions include factors like the number of active 
users and available bandwidth in the networks. 
•  Service type: Different service types may affect the handoff decision as the re-
quirements of each service differ in terms of data rate, latency, etc. 
•  User preferences: A user may prefer one type of network over another in terms of 
cost and QoS. 
•  Pricing  issues:  Pricing  issues  include financial  costs  which  may  affect   users’ 
choice in the handoff decision. 
4   Classification of Vertical Handoff Decision Schemes 
This section classifies the well-known vertical handoff schemes proposed in the lit-
erature. The classification is done depending on the strategy used in the scheme for 
deciding on a vertical handoff. 
4.1   Schemes with Cost Function Based Strategies 
Cost function is a measurement of determining how beneficial a particular network is 
while giving handoff decision depending on crucial factors (e.g. bandwidth, data rate, 
monetary cost, etc.) [3]. The cost function is calculated for each network that covers 
the service area of a user. The function can be outlined in two dimensions: (1) types of 
services requested by the user, and (2) the cost to the network according to specific 
parameters like bandwidth,  monetary cost, and power consumption. This strategy aims 
to optimize the  resource usage of two integrated wireless network from their service 
providers’ point of view. The general form of the cost  function f n for a wireless net-
work n is [3]: 
f n    = ∑ ∑ ws ,i .pn
s ,I                                                                            (1) 
                                                                                      
s
       
i
 
where pns ,I  is the cost in the ith parameter to carry out service s on network n, 
and ws ,I  is the weight (level of importance) assigned by the user for using the 
ithparameter to perform services (note that ∑ wi   = 1 ). 
                                                                                                     i 
In the recent research literature, several papers apply the cost  function  based 
strategy in their vertical handoff decision schemes. In [8], the proposed cost function 
measures the benefit obtained by handing of to a particular network by focusing on 
the combination of two different types of user  services (i.e. bit rate (CBR) and 
available bit rate (ABR)) spread among integrated WLAN and GPRS networks. The 
cost function is calculated by considering  the normalized  QoS  provided by a par-
ticular wireless network for the service requested by the user and the weight which 
indicates the impact  of the QoS parameter on the user/network. The QoS factor is 
only based on the bandwidth calculation for both CBR and ABR with equal weights. 
The vertical handoff decision is given after the calculation of cost function for each 

342 
F. Tansu and M. Salamah 
 
network and service. The lowest calculated value of the cost function shows  the 
network that provides the most benefit to the user by handing of to that network. 
The authors in [9] proposed a vertical handoff decision scheme based on a cost 
function for Wireless Wide Area Network (WWAN) and WLAN integrated networks. 
The factors that are taken into account of the cost function are data throughput, power 
consumption  and monetary cost with the weights assigned  to each  factors. The per-
formance of  the proposed scheme is analyzed with different combination of weights 
for two types of traffics (i.e. UDP and TCP) and the vertical handoff decision is given 
to handoff to the network with lowest cost value. 
Moreover, in [10], a QoS negotiation-based vertical handoff decision  scheme is 
developed in order to balance the user satisfaction and network efficiency. A merit 
function is adopted to find the best possible network for the users by combining QoS 
factors like RSS, bandwidth, cost, delay and bit error rate (BER) with their corre-
sponding weights. The performance of the  scheme is analyzed with different 
combination of weights, different service  classes of different bandwidth levels and 
speed of the user. The network with  the largest merit function is preferred and the 
handoff is processed to that network if it has enough capacity. 
Another cost function based vertical handoff decision between WLAN and UMTS 
networks is proposed in [11]. The handoff decision depends on the RSS, bandwidth, 
load of the network, security, signal-to-noise ratio, power requirement, user’s speed 
and preference of the user. All these factors are combined in the cost function with 
their corresponding weights for two types of service (i.e. real time and non-real time 
services). The highest calculated value of the cost function shows the network that 
provides the most benefit to the user by handing of to that network. 
The cost function based strategy used in the above schemes does not evaluate the 
users’ satisfaction in terms of cost and QoS. From the user point of  view, more 
advanced function is needed in order to perform the most convenient handoff depend-
ing on the user’s specific needs. 
4.2   Schemes with User- Centric Strategies 
The aim of user-centric strategies is to evaluate handoff decision by taking into 
account the cost and the QoS, from the user’s point of view, in which the most 
convenient wireless network can be selected depending on the user’s specific needs. 
The proposed scheme in [12] evaluates the impact of the vertical handoffs on the 
performance of the applications running on the MS by taking into  account user 
preferences in terms of cost and QoS. A metric is defined in  order to balance the 
overall cost of vertical handoffs with actual benefits they bring to user’s networking 
needs. In order to evaluate the cost of handoffs between WLAN and GPRS networks 
in terms of monetary cost, three  scenarios  and cost functions for different sessions 
(FTP, HTTP, and Telnet)  are considered: (i) the user is charged based on the time 
spent in the GPRS  network, while the use of the WLAN is free of charge. The 
monetary cost function is as follows 
)
(
1
s
c
T
C
GPRS
GPRS u
 
                                                 (2) 
where TGPRS is the time spent by the user in GPRS network and cGPRS is the fee 
that the GPRS service provider charges to the user. (ii) the user is charged on a 

 
Vertical Handoff Decision Schemes for Heteregeneous Wireless Networks 
343 
 
volume base when accessing the GPRS network, while the use of the WLAN is 
free of charge. The monetary cost is represented as follows 
)
(
2
Kb
c
V
C
GPRS
GPRS u
 
                                                   (3) 
where VGPRS is the data that has been transmitted over the GPRS network (ex-
pressed in Kb) and cGPRS  is the cost associated to the transport of each Kb. (iii) 
the user is charged in both GPRS and WLAN networks based on a volume based and 
time based charging models, respectively. The cost function is 
)
(
)
(
3
s
c
T
Kb
c
V
C
WLAN
WLAN
GPRS
GPRS
u

u
 
                                (4) 
The handoff decision in [12] is given depending on the charging model and the user’s 
willingness to pay for its application. 
The authors in [13] introduce a pricing model for WLAN/GPRS integrated networks, 
which is mainly based on the portion of the network usage in the particular network. In 
the handoff decision, the user perspectives (e.g. willingness to pay) on charges are taken 
into account. The user is charged depending on the session volume of data that has been 
transmitted over the GPRS network, while the accessing of WLAN is charged with the 
connection  fee. The model uses a demand function in order to show the effect of prices 
charged in the integrated network in terms of overall revenue. 
Furthermore, [14] proposes an efficient vertical handoff decision scheme between 
WLAN and GPRS networks which is intended mainly on the user satisfaction. The 
handoff decision criteria of the proposed scheme include  crucial factors like user 
mobility, network conditions, pricing issues, and user preferences in addition to the 
RSS. The scheme ensures the selection of the most appropriate network in terms of 
cost and acceptable QoS according to  users’ preferences. The function used in the 
calculation of the monetary cost is same as in (4). Moreover, the QoS tested in [14] is 
the probability of rejecting the connection of a user to a particular network. The users 
prefer to handoff to the network with lower cost or to the network with higher QoS. 
The described user-centric based schemes investigate the handoff decision  issue 
from the user’s point of view in which the handoff decision is given depending on the 
user satisfaction in terms of cost and QoS. 
4.3   Schemes with Multiple Attribute Decision Strategies 
The Multiple Attribute Decision Making (MADM) refers to making preferences 
among multiple alternatives (i.e. diverse networks) and multiple attributes (e.g. net-
work characteristics, user preferences, etc.). MADM can be classified in four 
groups [15]: (i) Simple Additive Weighting (SAW): the score of the particular net-
work is determined by the weighted sum of all the attribute values, (ii) Technique for 
Order Preferences by Similarity to Ideal Solution (TOPSIS): the preferred network is 
the one which is the closest to the ideal solution and the farthest from the worst case 
solution, (iii) Analytic Hierarchy Process (AHP): decomposes the network selection 
problem into several sub-problems and assigns a weight value for each sub-problem, 
(iv)  Grey Relational Analysis (GRA): used to rank the candidate networks and to 
select the one with the highest ranking. 

344 
F. Tansu and M. Salamah 
 
SAW is the classical method of MADM. In order to perform handoff decision to 
the best network at the best moment between WLAN and GPRS integrated networks, 
[16] used SAW method. The score function is based on the factors of usage expense, 
link capacity, and power consumption with the  weights assigned to each of them. 
These weights are assigned  depending on the remaining battery and the user prefer-
ence. The score of every wireless network interface is calculated, and the handoff 
decision is given to handoff to the network interface with the highest score. Another 
SAW method is used in [17] to perform the vertical handoff decision. The score 
function is  composed of bandwidth, dropping probability and the cost of the ser-
vice of the particular network with the weights assigned to each factor. 
Moreover, AHP method is the most popular one as it decomposes the problem into 
simpler sub-problem. The proposed handoff decision algorithm in [18] used AHP to 
calculate the weights of various traffic parameters (e.g. bandwidth, delay, jitter, etc.), 
and used the SAW method to calculate the score function. The users prefer to handoff 
to the network with highest value of score function. 
It is worth to mention that, one of the hot research topics is the use of fuzzy logic to 
solve MADM problems when imprecise information is represented in  fuzzy terms. 
Fuzzy logic is used to represent the imprecise information of some attributes of the 
networks and the preferences of the user. The fuzzy MADM method consists of two 
steps. First, information gathered is fed into fuzzifier in order to be converted into 
fuzzy sets. Then, one of the MADM  methods is used in order to determine the 
network to handoff. The authors in [19] propose a fuzzy-based vertical handoff deci-
sion scheme between  WLAN and GSM networks which is mainly based on 
different service types (i.e. voice and data) and users’ speed. By taking into account 
WLAN’s  data-oriented and GSM’s voice-oriented natures, mobile users prefer to 
connect to WLAN to send data files and GSM to carry on a voice call. Additionally, 
vertical handoff decision in this scheme is given also depending  on users’ speed 
where fast users are trying to stay connected to GSM and prevented from connecting 
to WLAN, since WLAN is designed for low speed users. 
Moreover, in [15], RSS, bandwidth, network coverage, and speed of the user are 
fed into the fuzzifier  and AHP method is used in the handoff decision after fuzzifica-
tion. Similarly, [20] proposes a fuzzy MADM handoff decision scheme where inputs 
like RSS, bandwidth, monetary cost,  battery status and latency are considered as 
fuzzy inputs. The scheme also uses AHP method with the fuzzy inputs in order to 
determine the network with highest score in order to handoff to. 
Table 1 and Table 2 illustrate a qualitative comparisons  of the given vertical handoff 
decision schemes according to their properties and characteristics, respectively. Num-
bers used in the tables indicate the references. Note that, all the given schemes consider 
a multi-criteria solution for a vertical handoff decision. For instance, different combina-
tions of crucial  factors such as users’ speed, bandwidth, different service types, and the 
preference of the user in terms of cost and QoS are taken into account. 
In addition, important characteristics of vertical handoff decision schemes in terms 
of service types supported, efficiency and flexibility are also mentioned in Table 1. 
Service types that are concerned in the Table 1 show that all of the strategies support 
non-real time (e.g. file transferring) and real-time (e.g.  voice/video conferencing) 
applications, except the user-centric strategy where it supports only the non-real time 

 
Vertical Handoff Decision Schemes for Heteregeneous Wireless Networks 
345 
 
applications. Besides, an efficient handoff  decision can be obtained with MADM 
strategies where efficiency means to obtain accurate decision with good performance. 
Flexibility of handoff decision schemes, that is to adapt the scheme with additional 
parameters, is one of the important characteristics of MADM strategy [6]. 
Moreover, the factors which the schemes consider in their vertical handoff decision 
criteria are shown in Table 2. User consideration includes the user satisfaction which is 
one of the main purposes of the user-centric strategies.  As the pricing issue is becoming 
a very important factor on the decision of  vertical handoff, most of the schemes con-
sider also the monetary cost in their handoff decision criteria. In addition, the table illus-
trates that QoS metric are considered mostly in the schemes which use MADM strategy 
compared to other strategies.  However, the consideration of users’ speed (i.e. mobility) 
and  power  consumption are scheme-dependent, since they are considered as crucial for 
some schemes and less important for others as shown in Table 2. 
Table 1. Comparisons of Vertical Handoff Decision Schemes 
 
Properties of the Schemes 
Ref. No 
Strategy 
Service Type  
Supported 
Efficiency 
Flexibility 
[8] 
Cost Function 
Based 
Non-real time  
& Real-time 
Medium 
High 
[9] 
Cost Function 
Based 
Non-real time  
& Real-time 
Medium 
High 
[10] 
Cost Function 
Based 
Non-real time  
& Real-time 
Medium 
High 
[11] 
Cost Function 
Based 
Non-real time  
& Real-time 
Medium 
High 
[12] 
User Centric 
Non-real time 
Medium 
High 
[13] 
User Centric 
Non-real time 
Medium 
High 
[14] 
User Centric 
Non-real time 
Medium 
High 
[16] 
MADM 
Non-real time & 
Real-time 
High 
High 
[17] 
MADM 
Non-real time & 
Real-time 
High 
High 
[18] 
MADM 
Non-real time& 
Real-time 
High 
High 
[19] 
Fuzzy 
Non-real time & 
Real-time 
High 
Medium 
[15] 
Fuzzy-  
MADM 
Non-real time & 
Real-time 
High 
Medium 
[20] 
Fuzzy-  
MADM 
Non-real time & 
Real-time 
High 
Medium 
 

346 
F. Tansu and M. Salamah 
 
Table 2. Comparisons of Vertical Handoff Decision Schemes 
 
Criteria of the Schemes in Vertical Handoff Decision 
Ref. 
No 
User 
Consideration 
Monetary
Cost 
QoS 
Power 
Usage 
Mobility 
[8] 
Low 
No 
Yes 
No 
No 
[9] 
Low 
Yes 
No 
Yes 
No 
[10] 
Low 
Yes 
No 
No 
Yes 
[11] 
Low 
No 
Yes 
Yes 
Yes 
[12] 
High 
Yes 
No 
No 
No 
[13] 
High 
Yes 
No 
No 
No 
[14] 
High 
Yes 
Yes 
No 
Yes 
[16] 
Medium 
Yes 
No 
Yes 
No 
[17] 
Medium 
Yes 
No 
No 
No 
[18] 
Medium 
No 
Yes 
No 
No 
[19] 
Medium 
No 
Yes 
No 
Yes 
[15] 
Medium 
Yes 
Yes 
Yes 
Yes 
[20] 
Medium 
Yes 
Yes 
Yes 
No 
5   Future Directions 
Pricing issue has a crucial impact on the decision of vertical handoff in a heterogene-
ous wireless network environment. The pricing models in such networks can be 
divided into two types [22, 23]: (i) non-competitive pricing model: there is only one 
service provider who adjusts its offered price for any  service, (ii) competitive 
pricing  model (i.e. oligopoly market model): there are multiple service providers 
whose objectives are different from each other. Recent research has proposed many 
handoff decision schemes based on the pricing in heterogeneous wireless networks in 
which a single wireless service provider is available to the user. The future vision of 
vertical handoff decision  schemes is to deal with the pricing models which will be 
adopted by multiple service providers. 
In oligopoly market model, multiple service providers compete with each other to 
offer wireless service to the mobile users in order to maximize their revenue. For such 
market, different competitive scenarios can be formulated: 
•  Competition in Price: A finite number of service providers decide on the  prices 
simultaneously depending on the needs of the mobile users (e.g. amount of bandwidth). 

 
Vertical Handoff Decision Schemes for Heteregeneous Wireless Networks 
347 
 
•  Leader-Follower  Competition:  In this scenario,  at least one service provider defined 
as a leader who can make the decision on the price before other service providers 
(i.e. followers). 
The above pricing models adopted by different service providers are crucial and will im-
pact the decisions of users in selecting a network. 
6   Conclusion 
Vertical handoff decision schemes are very important for supporting best connectivity 
to mobile users anywhere at any time from any network. In this paper, we give an 
overview of the most recent and interesting decision schemes by focusing on the 
strategies and crucial factors. We classified these decision schemes based on the three 
strategies: cost function based, user-centric and MADM.  The crucial factors that are 
taken into consideration are mainly based on different combinations of RSS, network 
conditions, user mobility, service type and user preferences. The schemes with user- 
centric strategy support only non-real time applications where other strategies support 
real-time  applications as well. Moreover, no matter what the strategy is, monetary 
cost and QoS metrics have an important impact on the vertical handoff decision. 
Although many of the vertical handoff decisions schemes in the literature look 
promising, there are still many open issues that need to be considered. We highlight 
these open issues and indicate the future directions in this area. 
References 
1. Salkinitzs, A.K., Fors, C., Pazhyannur, R.: WLAN-GPRS Integration for Next Generation 
Mobile Data Networks. IEEE Wireless Communications 9, 112–124 (2002) 
2. Stemm, M., Katz, R.: Vertical Handoffs in Wireless Overlay Networks, Mobile Networks 
and Applications. Mobile Networks and Applications 3, 335–350 (1998) 
3. McNair, J., Zhu, F.: Vertical Handoff in Fourth-Generation Multinetwork Environments. 
IEEE Wireless Communications 11, 8–15 (2004) 
4. Akyildiz, I.F., et al.: Mobility Management in Next Genartaion Wireless Systems. Pro-
ceedings of the IEEE 87, 1347–1784 (1999) 
5. Majlesi, A., Khalaj, B.H.: An adaptive fuzzy logic based handoff algorithm for interwork-
ing between WLANs and mobile networks. In: The 13th IEEE International Symposium 
on Personal, Indoor and Mobile Radio Communication, vol. 5, pp. 2446–2451 (2002) 
6. Kassar, M., Kervella, B., Pujelle, G.: An Overview of vertical handoff decision strategies 
in heterogeneous wireless networks. Computer Communications 31, 2607–2620 (2008) 
7. Zhu, F., McNair, J.: Optimizations for Vertical Handoff Decision Algorithms. In: IEEE 
Wireless Communication and Networking Conf., vol. 2, pp. 867–872 (2004) 
8. Zhu, F., McNair, J.: Multiservice Vertical Handoff Decision Algorithms. EURASIP Jour-
nal Wireless Communication Network 2006, 1–13 (2006) 
9. Hong, K., Lee, S., Kim, L., PyungJung, S.: Cost-Based Vertical Handover Decision Algo-
rithm for WWAN/WLAN Integrated Networks. EURASIP Journal on Wireless Communi-
cations and Networking 2009 (2009) 

348 
F. Tansu and M. Salamah 
 
10. Song, Q., Jamalipour, A.: A quality of service negotiation-based vertical handoff decision 
scheme in heterogeneous wireless systems. European Journal of Operational Research 191, 
1059–1074 (2008) 
11. Wang, Y.-H., Hsu, C.-P., Huang, K.-F., Huang, W.-C.: Handoff decision scheme with 
guaranteed QoS in heterogeneous network. In: IEEE International Conference on  
Ubi-Media Computing, pp. 138–143 (2008) 
12. Calavagna, A., Modica, G.D.: A cost-based approach to vertical handover policies between 
WiFi and GPRS. Wireless Communicaton and Mobile Computing 5, 603–617 (2005) 
13. Yaiparoj, S., Harmantzis, F., Gunasekaran, V.: On the economics of GPRS networks with 
Wi-Fi integration. European Journal of Operational Research 187, 1459–1475 (2008) 
14. Tansu, F., Salamah, M.: An Efficient Vertical Handoff Decision for Microcellular and 
Macrocellular Interworking. International Journal of Communication Systems 12(12), 
1495–1513 (2009) 
15. Kassar, M., Kervella, B., Pujolle, G.: An Intelligent Handover Management System for 
Future Generation Wireless Networks. EURASIP Journal on Wireless Communications 
and Networking 2008 (2008) 
16. Chen, L.-J., Sun, T., Chen, B., Rajendran, V., Gerla, M.: A Smart Decision Model for Ver-
tical Handoff. In: Proceedings of 4th ANWIRE International Workshop on Wireless Inter-
net and Reconfigurability, pp. 1–5 (2004) 
17. Tawil, R., Pujolle, G., Salazar, O.: A Vertical Handoff Decision Scheme in Heterogeneous 
Wireless Systems. In: IEEE Vehicular Technology Conference, pp. 2626–2630 (2008) 
18. Wu, J.-S., Yang, S.-F., Hwang, B.-J.: A terminal-controlled vertical handover decision 
scheme in IEEE 802.21-enabled heterogeneous wireless networks. International Journal of 
Communication Systems 22, 819–834 (2009) 
19. Tansu, F., Salamah, M.: A Fuzzy-Based Vertical Handoff Scheme for Interworking Be-
tween WLAN and GSM. In: New Trends in Computer Networksbook, pp. 309–318. Impe-
rial College Press, London (2005) 
20. Chan, P.M.L., Hu, Y.F., Sheriff, R.E.: Implementation of fuzzy multiple objective decision 
making algorithm in a heterogeneous mobile environment. In: IEEE Wireless Communica-
tions and Networking Conf., vol. 1, pp. 332–336 (2002) 
21. Pahlavan, K., et al.: Handoff in Hybrid Mobile Data Networks. IEEE Personal Communi-
cations 7, 34–47 (2000) 
22. Niyato, D., Hossain, E.: Competitive Pricing in Heterogeneous Wireless Access Networks: 
Issues and Approaches. IEEE Network 22, 4–11 (2008) 
23. Sengupta, S., Anand, S., Chatterjee, M., Chandramouli, R.: Dynamic pricing for service 
provisioning and network selection in heterogeneous networks. Physical Communica-
tion 2, 138–150 (2009) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 349–355, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Browser Games: The New Frontier of Social Gaming 
Juha-Matti Vanhatupa 
Department of Software Systems, Tampere University of Technoloy, 
P.O. BOX 553 FI-33101 Tampere, Finland 
juha.vanhatupa@tut.fi 
Abstract. Browser games are nowadays a very popular type of computer 
games. Those games are played directly in web browser and do not need soft-
ware installations; therefore those are available for more players. Even those 
have existed rather long time, there were no comprehensive study about 
browser games. This paper targets at analyzing browser games and categorizing 
of those. We also discuss financial opportunities involved in browser game 
genre.  
Keywords: Browser Games, Game Analysis, Game Categorization.  
1   Introduction 
Multiplayer games played inside a web browser have benefits over traditional com-
puter games. Browser games offer a good platform for social gaming, because players 
can interact each other – attack other players, form alliances and work together to 
accomplish greater goals.  
The games are played inside a web browser, which lowers start-up threshold of 
gaming as software installation is not needed. The browser games are available also 
for players, who have never bought a single computer game and have no intensions to 
ever do so. Browser games can be played anywhere with normal web browser and in 
short time periods in the middle of player’s normal life, few moves in a browser game 
takes only few minutes instead of hours.  
As start-up threshold is small, browser games are easy to advertise to friends. 
Friends playing some browser game are a strong lure to get people play a certain 
game. This is especially typical in Facebook [7] application distribution, because 
people send invitations to applications to their friends. Facebook applications also 
encourage this behavior by offering bonuses for player with lot of friends playing or 
at least added to their account same application.  
Instead of computer controlled foes, a browser game can offer a battlefield, where 
each opponent is a human player. This can be very strong motivator for competitive 
players to play browser games. Artificial intelligence controlled opponents can also 
be used as opponents, or end antagonist like Natars in Travian [20].  
The player needs no installation packages to play browser games, therefore no 
money can be gain from selling those to the players. The profit of the game develop-
ers must come from somewhere else. They can make money by advertising, selling 
new units, other game resource or extra features to the players.  

350 
J.-M. Vanhatupa 
 
Although time for single move in a browser game can be very short, the duration of 
a browser game is usually very long or eternal. Therefore it is possible to create rich-
ness impossible to normal multiplayer games. For example, some Hattrick [12] play-
ers have played the game with same team more than ten real-life years.  
Even though browser games have existed a rather long time, there were no com-
prehensive studies about the subject. This paper focuses on browser games properties 
and categorizing those. We have discovered five common features of the browser 
games and made a categorization, which divides the browser games into five catego-
ries. We also discuss financial opportunities involved in browser game genre. These 
results were found based on a survey, which targeted important browser games.  
We proceed as follows. In the next Section we briefly discuss history of browser 
games. In Section 3 we present our definition of browser game. In Section 4 we dis-
cuss our categorization of browser games. In Section 5 we briefly look at technologies 
used in browser games. In Section 6 we discuss financial opportunities included in 
browser game genre. Finally, related work is discussed in Section 7, before conclud-
ing remarks in Section 8. 
2   History of Browser Games 
Browser games have similarities to multi-user dungeons (MUDs), which appeared in 
late 1970s [1]. Those allowed multiple players to explore virtual dungeons, despite 
the fact that those focused on social interaction between players. Combat systems 
were usually rather simple. Like browser games, MUDs were multiplayer games, 
game time was very long or eternal and each player had one account and therefore 
controlled single character.  
Later came Earth 2025, released in October 1996, which claims to be the first of 
the generation of unique, interactive games designed to be played directly on the web 
[6]. Since those days web access has become more and more general and this growth 
has increased number of browser games. People spend more time on the web than 
ever before. Web browser games have come popular and spread into many different 
game genres. 
One main motivator in recent growth of browser games is popularity of Facebook 
social utility [7]. Anyone with some programming experience can write his or her 
own browser game and offer it to the other users of the Facebook. Facebook had more 
than 300 million active users in January 2010 and popular Facebook browser games 
can have twenty million or more active users monthly [9]. The most popular browser 
game at the moment Mafia Wars [15] had more than 23 millions monthly active users 
in January 2010. There are more popular applications in Facebook, which are listed as 
game application, but those do not fill our definition of a browser game.  
3   Browser Game Definition 
To fill our definition of browser game, a game has to fulfill five features. The features 
have been found by surveying number of browser games. In the survey we played the 
game at least the time needed to research all the important features of the game. The 
testing time for a single game was from an hour to more than a year.  

 
Browser Games: The New Frontier of Social Gaming 
351 
 
Browser games are i) multiplayer. The game has a lot, probably thousands of users, 
who can interact with each other. There is more interaction than shared score list 
between the players. Usually players can, for example, attack each other and form 
alliances. 
The games are played in the ii) web browser. Usually no installation of any appli-
cation is needed. Though some games offer graphic packages, but installation of those 
do not modify the rules of the game. Packages affect only to the graphic of the game. 
In case of Facebook application, the player has to allow the application to access his 
or her account, however this operation is not regular installation. 
The games are iii) always on. It is possible to get attacked or receive message even 
when not online. Some games allow sitters, a player can name couple of other players 
who can log in his or her account when he or she is not online. Sitters can play using 
the account like the original player. This can be especially helpful during the players’ 
holiday trips or other pauses in normal gaming. It is also possible to gain some advan-
tage by playing in different timing than other players, as it is hard to defend against 
sneak attacks that happen during a night time.  
The iv) duration of games is very long or eternal. Game time of browser games can 
go on in real time, the game can be turn-based, or it can queue actions and for making 
them happen on a time base. The games are usually restarted after the end.  
Each player has (usually) a single v) account, and he or she controls one character, 
group or nation. Some games offer second or even third account as additional feature 
if the player pays an extra fee. 
4   Browser Game Types 
Browser games as regular computer games are divided into several different types. 
Browser games in general are good at creating large worlds with a lot of players, 
which explains the large number of strategy games and role-playing games. Several 
games also combine these two types, for example, Dogs of Seas [5] and Dark Orbit 
[4]. We have divided browser games into five types. On the following we go through 
those five types.  
Strategy games emphasize decision-making skills of the player. There is usually a 
large world with a lot of players and player formed alliances. The player has to battle 
his or her way to the top. Travian [20] is a good example of strategy game. In the start 
of the game the player gets one village and he or she has to build powerful nation. 
Players can form alliances and wage wars. Length of one Travian game is about a 
year.  
Role-playing games concentrate on character development and role-playing. The 
player usually controls only one character instead of a nation or group. A good exam-
ple of role-playing games is Forumwarz [10], a parody role-playing game that takes 
place on the Internet. In Forumwarz the player tries to wipe out Internet forums.  
Manager games put player into decision-making position of a team or other group. 
An example of those games is Hat Trick [12], a popular football manager game. In 
Hat Trick each player gains control of a football team and competes against other 
player’s teams. The players registering in the game are divided into divisions based 
on their real-world residence location and matches are played some once a week. 

352 
J.-M. Vanhatupa 
 
Outcome of the matches are calculated by the simulation engine of the game, based 
on the players’ attributes and other variables.  
Shooter games are perhaps the most simple browser games. The goal is to shoot 
target and get points. Games vary, but the goal is always the same. Example of this 
group is Stick Arena [19]. Were players run around the maps, picking up weapons 
along the way and use those to kill opponents. The players are ranked using numbers 
of deaths they have inflicted.  
Social networking games are the last group. There is no combat or character devel-
opment; instead the game focuses on social interaction. The Habbo Hotel [11] is an 
example of those applications. The Habbo Hotel also offers virtual goods for sale and 
the player can decorate his or her avatars’ apartment with those. 
5   Browser Game Technologies 
There are several technologies to implement browser games. The game can run based 
on a web browser plugin, for example, Flash Player, Shockwave or Java. The games 
usually assist the player in downloading the correct plugin if it does not exist.  
The second possibility is to use server-side scripting, for example, PHP, ASP or 
Java (using jsp-pages). The server executes a script and responses to the client with a 
dynamic web page.  
In [13] Häsel introduces two architectural choices for browser games, client-server 
architecture and Peer-to-Peer architecture. In client-server architecture, the serves 
keeps clients up to date, clients requests are delivered to the server, which responses 
and delivers data to the other clients. This can be problematic in real-time browser 
games as it generates lot of network traffic and the server must process requests of all 
clients.  
In Peer-to-Peer architecture each peer has same responsibilities as every other peer. 
Instead sending actions to the server, each player communicates with each other 
player. It uses less bandwidth and can be more efficient than client-server architec-
ture. However, cheating can be hard to avoid in Peer-to-Peer architectures, because 
every client has its own copy of the game state.  
Nowadays many browser games contain good graphics and need more resources 
from the client and server than early text-based browser games. However, most of 
browser games are still played because those games have an interesting game idea and 
good playability. It has not been proven that browser games with good graphics 
would have great advantage in luring players. 
6   Financial Opportunities 
Advertising is very common in browser games. Even the game itself is almost never 
disturbed with pop-ups; the banners are common sight in browser games. The devel-
oper sells advertising space for companies and gains money when people see or click 
the banners. For example, Facebook advertises itself as possibility to reach exact 
audience and connect to real customers. Facebook also allows advertiser to track her 

 
Browser Games: The New Frontier of Social Gaming 
353 
 
or his advertisement progress in real-time and choose target audience by user’s loca-
tion, age, gender, education or other user information [8]. 
Extra features are also a common way of making money in browser games. Usu-
ally it is possible to buy some virtual currency unit with real money and this virtual 
currency can be used to buy extra features, units, or user interface shortcuts. In addi-
tion to extra features, Hattrick [12] also sells T-shirts and possibility to get important 
match events, for example, goals and player injuries by SMS messages directly to cell 
phone.   
In The Continuum™ [3], the online collectible wargame from Seven Lights, the 
game is started with buying units with real money. Starter pack has 40 units and Booster 
pack has 15. Anyone can play for free, because they receive one booster pack at the 
registration and can play with them as long as he or she wants. However, new units can 
only be gain by buying new starters or booster. This idea is originally used by collective 
card games and online versions of those, for example Magic: The Gathering [16].  
Extra accounts can be sold to the players. For example, in Forumwarz [10] each 
player can create one account for free of charge. The second and third accounts can 
only be bought. Other two classes cannot be played without buying new accounts.  
The successful browser games or even the whole developer company can some-
times be sold to other company. For example, success of Club Penguin [2], led to that 
the developer company, New Horizon Interactive was sold to the Walt Disney Com-
pany in August 2007 for the sum of $350 million [21].    
The players gain rarely any real money from playing the browser games. Even cash 
prizes are common feature of for example, Internet Poker, real world cash prizes are 
seldom used in browser games. However, Dark Orbit [4] offers real money prizes. 
Each month’s winner gains a sum of several thousand Euros.  
7   Related Work 
There are some studies about browser games, which analyses player motivations, the 
players, and cheating in browser games.   
Schultheiss [17] has studied long-term motivations to play MMOGs and found 
several reasons for people to play MMOGs. Most important factors, which lead to 
play researched browser game Space Merchant Realms [18] were competition, learn-
ing, integration of reality and escapism. The research also points out that there are 
certain group of core players and on the other hand a fluctuating group of players, 
who come to the game and do not remain a longer time. This can be explained by 
many reasons; one of those is the difficulty or dominance of the core players. To fix 
this problem, several games give beginners some advantage, for example, they cannot 
be attacked at the beginning, to ease this learning phase. Despite the beginning advan-
tage, a complex browser games can be very difficult for beginners. Game start is 
usually very important moment of the game and critical mistakes at that point can ruin 
player’s rest of the game. Core players are also looking for easy points or resources at 
beginners’ expense. In addition, in browser games were players are divided into areas 
and players can only attack other players near him or her, core players might want to 
eliminate all players, which start the game in same area, just to eliminate the possible 
threat in the future.  

354 
J.-M. Vanhatupa 
 
Cheating is one of the main problems in all MMOGs, not only in browser games. 
In [14] Kabus et al. represent that retaining control over the global game state is nec-
essary to avoid cheating and to lock out players without valid submission. This con-
trol retaining might make use of Peer-to-Peer architecture difficult.  
8   Conclusions 
Online and multiplayer are current trends in computer games, and number of browser 
games are constantly growing. Browser games are usually in size lot smaller than 
regular computer games and like mobile games, browser games do not need as high 
quality graphics, therefore developing those is faster. The observe is that by develop-
ing browser games, it is usually hard to find financial opportunities, as there are no 
game packages to sell. The profit must come from other sources, for example from 
additional features sold to the players or from advertising inside the games. Although, 
disturbing advertising might get some players leave the game.   
One main feature of many analyzed browser game was a large virtual world with a 
lot of players. It is obvious that large consistent worlds are fascinating for the players 
as it ensures a lot of opponents and possible friends.  
Number of different browser games has grown in recent years and this develop-
ment continues. Especially computer graphics used in those has developed. Browser 
games also vary more than before and possible new game types will appear in the 
future. There is still a lot of potential in browser games as new technologies and ideas 
come into practice. 
References 
1. Batron, M.: Dungeons & Desktops. In: The History of Computer Role Playing Game,  
pp. 37–43. A.K. Peters, Wellesley (2008) 
2. Club Penguin Browser Game, http://www.clubpenguin.com/ 
3. The Continuum Browser Game, http://www.thecontinuum.com/ 
4. DarkOrbit Browser Game, http://www.darkorbit.com/ 
5. Dogs of the Seas Browser Game, http://www.dogsoftheseas.com/ 
6. Earth 2025 Browser Game, http://games.swirve.com/earth 
7. Facebook Social Utility, http://www.facebook.com/ 
8. Facebook Advertising, http://www.facebook.com/advertising/ 
9. Facebook Statistics,  
 http://www.facebook.com/press/info.php?statistics 
10. ForumWarz Browser Game, http://www.forumwarz.com/ 
11. Habbo Hotel Virtual World, http://www.habbo.com/ 
12. Hattrick Browser Game, http://www.hattrick.org/ 
13. Häsel, M.: Rich Internet Architectures for Browser-Based Multiplayer Real-Time Games – 
Design and Implementation Issues of virtual-kicker.com. In: Enokido, T., Barolli, L., Ta-
kizawa, M. (eds.) NBiS 2007. LNCS, vol. 4658, pp. 157–166. Springer, Heidelberg (2007) 
14. Kabus, P., Terpstra, W., Cilia, M., Buchmann, A.: Addressing Cheating in Distributed 
MMOGs. In: Proceedings of the 4th ACM SIGCOMM workshop on Network and system 
support for games (2005) 

 
Browser Games: The New Frontier of Social Gaming 
355 
 
15. Mafia Wars Facebook application,  
 http://www.facebook.com/apps/application.php?id=10979261223 
16. Magic: The Gathering collectible card game,  
 http://www.wizards.com/magic/multiverse/default.aspx 
17. Schultheiss, D.: Long-term Motivations to Play MMOGs: A Longitudinal Study on Moti-
vations, Experience and Behavior. In: Proceedings of the DiGRA 2007 - Situated Play, 
Digital Games Research Association International Conference 2007, pp. 344 – 348 (2007) 
18. Space Merchant Realms Browser Game, http://www.smrealms.de/ 
19. Stick Arena Browser Game,  
 http://www.xgenstudios.com/play/stickarena 
20. Travian Browser Game, http://www.travian.com/ 
21. Walmsley, A.: Kids’ Virtual Worlds are Maturing Nicely,  
 http://www.marketingmagazine.co.uk/news/756021/ 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 356–366, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
IT Security Assessment for Interdisciplinary Research 
Syed M. Rahman∗, Syed Vickar Ahamed, and Sevki S. Erdogan 
Department of Computer Science and Engineering 
University of Hawaii-Hilo, 200 W. Kawili Street, Hilo, HI  96720 
{srahman,sahamed,sevki}@hawaii.edu 
Abstract. The Internet has never been more insecure than ever before and sci-
entific research projects are not immune from these rising risks, threats, and 
vulnerabilities. In this paper, we have aimed to  identify possible threats, threat 
sources, and vulnerabilities of a distributed, interdisciplinary, scientific research 
project. We have selected our current NSF-EPSCoR1 funded research project as 
a case study. We have also briefly discussed  security vulnerability trends for 
the year of 2010 and beyond. Finally, some countermeasures have been recom-
mended to protect or recover assets from these attacks and vulnerabilities.  
Keywords: IT security, assessment, vulnerability, threats, EPSCoR. 
1   Introduction 
Information security problem is any act or action that unfavorably affects an IT re-
source’s confidentiality, integrity,  availability, authenticity, and accountability.  We 
know the major security issues often originate from a software bug, a configuration 
defect, or a design flaw. If we do not deploy effective countermeasures such as up-
dates and safeguards regularly, a vulnerability is likely to materialize, followed by an 
exploit that could lead to a security breach. Social engineering is yet other path that an 
attacker can use to apply various deception techniques and gather  information about 
or gain unauthorized access to a system. 
The color pattern in Figure 1 follows the US Department of Homeland Security’s 
scheme (red is the highest danger) to demonstrate the risk levels of various vulner-
abilities, exploits, and breaches. Boxes without color are the risk sources [1]. Mal-
ware, phishing sites and phony antivirus software programs are not only proliferating 
at a record pace but becoming more sophisticated and targeted each day- to the point 
that going online is now more risky than ever before[4].  No one can ignore cyber 
security issues anymore. Indeed,  a few weeks ago the cyber-security coordinator 
from the President Obama administration warned in a private conference that the 
country needs "to prepare for a digital disaster" such as a cyber-attack on infrastruc-
ture that she mentioned could cost $700 Billion [8]. 
                                                           
∗ Contact Author. 
1 This work is supported by EPSCoR award EPS-0903833 from the National Science Founda-
tion to the University of Hawaii. 

 
IT Security Assessment for Interdisciplinary Research 
357 
 
 
Fig. 1. Causes of cyber-security issues[1] 
In this paper, we have used a National Science Foundation (NSF) funded research pro-
ject to identify security threats and vulnerabilities for a typical project of scientific research 
as a case study.  This multi-discipline research project(total $20 Million)has a large IT 
infrastructure used by numerous stakeholders in the project. Its network includes a variety 
of servers, executing a range of application software typical of a research project. It also 
uses applications' software and kind of data that are far less common, some of which di-
rectly relates to environmental, ecosystem, genomics of Hawaii Islands. Many of these 
systems (e.g. some sensors) used to be isolated, with no network connections between 
them. In recent years they (e.g. sensors) have been connected together, and connected with 
wireless/mobile networks.  However, this means they are now potentially accessible from 
the Internet, which has greatly increased the risks to these systems. 
1.1   Why Security Is Crucial? 
Information security is very crucial in this project and we need to take proper protec-
tion to prevent possible attacks (both inside and outside) to protect our resources.  
 
 
 
   Fig. 2. Incidents and Events by Category[3]        Fig. 3. Top Five Incidents vs. All Others[3] 
 

358 
S.M. Rahman, S.V. Ahamed, and S.S. Erdogan 
 
United States Computer Emergency Readiness Team (US-CERT)[3]  interacts with 
federal agencies, industry, the research community, state and local governments, and 
others to collect reasoned and actionable cyber security information and to identify 
emerging cyber security threats. Based on the information reported, US-CERT was 
able to identify the following cyber security trends (figure 2 and 3) for fiscal year 
2009 first quarter [3]. 
Figure 2 displays the overall distribution of cyber security incidents and events 
across the six major categories. The percentage of Category 5 (Scans, Probes, or At-
tempted  Access) reports decreased for the second consecutive quarter. This was a 
2.9% decrease in CAT 5 incidents compared to the previous quarter. The percentage 
of Malicious Code incidents increased by 3.3%. 
Figure 3 is a breakdown of 
the top five incidents and events 
versus all others. Phishing re-
mained the most prevalent 
incident type, accounting for 
70% of all incidents reported. 
This was a slight percentage 
decrease of 1.8% from the pre-
vious quarter. on the other 
hand, The sophistication of 
attack tools has gone up, while 
the level of skill required to use 
those tools has gone down(refer 
to figure 4). At this stage, the 
attacker takes advantage of his 
or her ability to steal confiden-
tial and proprietary data and 
sells it for profit or uses it for 
military intelligence[1]. 
1.2   Security Vulnerability Trends for 2010 and Beyond 
In this section, we have discussed the security vulnerability trends in the of 2010 and 
beyond. We believe our research project will encounter with the similar vulnerability. 
Hackers and malware spreaders are becoming more sophisticated, meaning computer 
users need to become wiser and more proactive, too. Whether we are using our mo-
bile phone to check e-mail and surf the Web or an enterprise IT administrator charged 
with safeguarding our  data, Symantec identifies the following 13 security issues will 
be most relevant in 2010[4]: 
¾ Antivirus is not enough: We have reached an inflection point, where new 
malicious programs are actually being created at a higher rate than good pro-
grams. Approaches to security that looks for ways to include all software 
files, such as reputation-based security, will become key in 2010 [4]. 
¾ Social engineering as the primary attack vector: More and more, attackers 
are going directly after the end user and attempting to hoax them into 
 
Fig. 4. Attack tool trends[1] 

 
IT Security Assessment for Interdisciplinary Research 
359 
 
downloading malware or divulging sensitive information under the auspice 
that they are doing something perfectly innocent [4]. 
¾ Rogue security software vendors escalate their efforts: In 2010, Symantec 
expect to see the propagators of rogue security software scams take their ef-
forts to the next level, even by hijacking users’ computers, rendering them 
useless and holding them for their financial gain.  
¾ Social networking third-party applications will fraud targets: With the 
popularity of social networking sites poised for another year of unprece-
dented expansion, expect to see fraud being targeted toward social site users 
to raise[4]. 
¾ Windows 7 will come in the crosshairs of attackers: Microsoft’s new op-
erating system is no exception, and as Windows 7 hits the pavement and 
gains traction in 2010, it is almost certain that attackers will find ways to ex-
ploit its users[4]. 
¾ Fast Flux botnets will increase: As industry countermeasures continue to 
reduce the effectiveness of traditional botnets, Symantec expect to see more 
using this method to carry out attacks. 
¾ URL-shortening services become the phisher's best friend: In an attempt 
to evade anti-spam filters through obfuscation, expect spammers to use 
shortened URLs to carry out their evil deeds[4]. 
¾ Mac and Mobile Malware Will Increase: because Mac and Smartphone's 
continue to increase in popularity in 2010, more attackers will devote time to 
creating malware to exploit these devices[4]. 
¾ Spammers breaking more rules: since the economy continues to suffer and 
more people seek to take advantage of the loose restrictions of the Federal 
Trade Commission's Can-Spam Act, there will be more organizations selling 
unauthorized e-mail address lists and more less-than-legitimate marketers 
spamming those lists[4]. 
¾ As spammers adapt, volume will continue to: Since 2007, spam has in-
creased on average by 15 percent a year. Spam volumes will continue to 
fluctuate in 2010 as spammers continue to adapt to the sophistication of se-
curity software and the intervention of responsible ISPs and government 
agencies across the globe[4]. 
¾ Specialized malware on the rise: Highly specialized malware was uncov-
ered in 2009 that was aimed at exploiting certain ATMs, indicating a degree 
of insider knowledge about their operation and how they could be exploited. 
Expect this trend to continue in 2010, including the possibility of malware 
targeting electronic voting systems, both those used in political elections and 
public telephone voting, such as that connected with reality television pro-
grams and competitions [4]. 
¾ CAPTCHA[6] technology will improve: This will prompt more businesses 
in emerging economies to offer real people employment to manually gener-
ate accounts on legitimate Web sites. Symantec estimates that the individuals 
will be paid less than 10 percent of the cost to the spammers, with the ac-
count farmers charging $30-$40 per 1,000 accounts[4]. 
¾ Instant messaging spam will surge: By the end of 2010, Symantec predicts 
that one in 300 IM messages will contain a URL. Also, in 2010, Symantec 

360 
S.M. Rahman, S.V. Ahamed, and S.S. Erdogan 
 
predicts that one in 12 hyperlinks overall will be linked to a domain known 
to be used for hosting malware. IM threats will largely be comprised of unso-
licited spam messages containing malicious links, especially attacks aimed at 
compromising legitimate IM accounts[4]. 
2   Case Study: NSF-EPSCoR Funded Research Project 
We have selected our current NSF-EPSCoR funded research project as a case study 
for this study. The primary focus of  our current project is to enhance the sustainabil-
ity of Hawaii's science and technology capabilities for understanding and predicting 
how invasive species, anthropogenic activities and climate change impact the biodi-
versity, ecosystem function and current or potential human use of Hawaiian focal 
species. A significant focus will build on the foundation created through prior EP-
SCoR awards, by expanding competitiveness in new areas of science and technology 
research and education, and improving relationships of the academic research com-
munity with the local community. Integral components of this Initiative will include 
increasing capacity for CyberInfrastructure, data visualization and modeling, as well 
as broadening the diversity of the  State’s science, technology, engineering and math 
(STEM) workforce through enhancement of hands-on research experiences for under-
graduate and graduate students in concert with broader community outreach. Re-
searchers or stakeholders of this project are located in different states in the USA and 
their research background varies from Computer Science, Biology, Forestry, Marine 
Science, Astronomy, Physics, Mathematics, Geology, Geophysics, and so on. Cur-
rently, more than 30 active researchers are involved in this five-years long project and 
they are located in different states in the USA.   
Research will be designed and executed by two teams, the Environmental Dynam-
ics and Ecosystem Responses group and the Ecological Genomics and Metabolomics 
group. The two teams will work in a coordinated and complementary fashion to ac-
complish the successful testing  of their hypotheses. Synergetic study design will 
utilize a transect on the Big Island and a virtual geological time series transect. Along 
these transects, a variety of physical, chemical, biological,  and ecological measure-
ments will be made by members of the EPSCoR project. The parameters  to be chosen 
will be based on hypotheses generated by EPSCoR researchers having to do with  the 
ways in which various forcing functions affect key components of the ecosystems 
found along  the transects, with particular attention being paid to those of historical 
importance to the original human inhabitants of Hawaii. 
Three major teams are involved in this distributed project. Many scientists are in-
volved and physically located in different places in the state of Hawaii. Majors teams 
are as follows:  
¾ Environmental Dynamics and Ecosystem Responses (ENDER): 
¾ Ecological Genomics and Metabolomics (ECOGEM) 
¾ Cyberinfrastructure Plan for Next Generation Research and Education in 
Hawaii (CYBER) 
 

 
IT Security Assessment for Interdisciplinary Research 
361 
 
Local 
Computers 
Local 
Computers
Local 
Computers
Local 
Computers 
Local 
Computers
Local 
Computers
Local 
Computers 
Local 
Computers
Local 
Computers 
and Servers 
Data 
storage
Data
storage
ECOGEM 
HIMB 
Biology 
Biology 
ENDERS 
Geography
Biology 
Geology & Geophysics 
Marine Science 
sk
SK
Data & Information
 Knowledge and Security Center 
CYBER
 
Fig. 5. Major components of the discussed scientific project 
2.1   Knowledge and Security Center  
Cyber team is considered as the central team of the current EPSCoR project and provides 
supports to other two teams. Cyber team is focused on data manipulations (to include  
geo-spatially referenced data management, visualization and predictive modeling), cyber-
infrastructure/supercomputer operations, and analytical laboratories for genomics, metabo-
lite and environmental chemistry will provide essential services to all research teams. 
Recently, we have established a security and knowledge center to provide logistic support 
to our research team in regards to information security and knowledge processing [5].  
In addition, Cyber team will provide IT security in different layers of this scientific 
project including  
¾ user authentication 
¾ user/data confidentiality 
¾ data/services availability 
¾ data/services integrity, and user accountability. 
3   Identify Threats, Threat Sources, and Vulnerabilities 
3.1   Threat Identification 
Identifying the threats or risks the assets are exposed to directly addresses the second 
of our three fundamental questions: “How are those assets threatened?”. The goal of 

362 
S.M. Rahman, S.V. Ahamed, and S.S. Erdogan 
 
this stage is to identify potentially significant risks to the assets listed. This requires 
answering the following questions for each asset[2]: 
¾ Who or what could cause it harm? and  
¾ How could this occur?  
The chance of natural threats occurring in any particular area is usually well known 
from insurance statistics. Lists of other potential threats may be found in the stan-
dards, in the results of IT security surveys, and in information from government secu-
rity agencies. The annual computer crime surveys, such as those conducted by the 
CSI/FBI in the US and AusCERT in Australia, provide useful general guidance on the 
broad IT threat environment and the most common problem areas. However, this 
general guidance needs to be tailored to the organization and the risk environment it 
operates in. This involves consideration of vulnerabilities in the organization’s IT 
systems, which may indicate that some risks are either more or less likely than the 
general case[2].  
3.2   Threat Sources 
A threat may be either natural or man-made, and may be accidental or deliberate. This 
is known as the threat-source. The classic “natural” threat sources are those often 
referred to as “acts of god”, and include damage caused by fire, flood, storm, earth-
quake and other such natural events. It also includes “environmental threats” such as 
long-term loss of power or natural gas. Or it may be the result of chemical contamina-
tion or leakage. Alternatively a threat source may be a human agent acting either 
directly or indirectly. When evaluating possible human threat sources, it is worth 
considering their reason and capabilities for attacking this organization, including 
their [2]: 
¾ motivation: why would they target this organization, how motivated are 
they? 
¾ capability: what is their level of skill in exploiting the threat? 
¾ resources: how much time, money, and other resources could they  
deploy? 
¾ probability of attack: how likely and how often would your assets be 
targeted? 
¾ deterrence: what are the consequences to the attacker of being  
identified? 
3.3   Vulnerability Identification 
Answering the second of these questions “How could this occur?” involves identify-
ing flaws or weaknesses in the organization’s IT systems or processes which could be 
exploited by a threat. This will help determine the applicability of the threat to the 
organization, and its significance. The outcome of this step should be a list of threats 
and vulnerabilities, with brief descriptions of how and why they might occur [2]. 
¾ identify exploitable flaws or weaknesses in organization’s IT systems or 
processes 
¾ hence determine applicability and significance of threat to organization 

 
IT Security Assessment for Interdisciplinary Research 
363 
 
¾ note need combination of threat and vulnerability to create a risk to an asset 
¾ again can use lists of potential vulnerabilities in standards etc. 
4   Risk Analysis and Possible Countermeasures 
4.1   Analyze Risks 
The ideal would be to specify the likelihood as a probability value, and the conse-
quence as a monetary cost to the organization should it occur. The resulting risk is 
then simply given as [2]: 
Risk = Probability threat occurs  x  Cost to organization 
This can be directly equated to the value the threatened asset has for the organization, 
and hence specify what level of expenditure is reasonable to reduce the probability of 
its occurrence to an acceptable level. Hence most risk analyses use qualitative, rather 
than quantitative, ratings for both these items. The goal is then to order the resulting 
risks to help determine which need to be most urgently treated, rather than give them 
an absolute value [2]. 
4.2   Identify Assets 
Next, the key assets had to be identified. We can conduct interviews with key IT and 
other stakeholders in our research team. A number of the researchers emphasized how 
important the reliability of the EPSCoR  network and nodes were to the research 
team. Key asset of our EPSCoR research team, in terms of information security, are as 
follows [2]: 
¾ reliability and integrity of EPSCoR nodes and network 
¾ integrity of stored file and database information 
¾ availability, integrity of sensors system 
¾ availability, integrity, confidentiality of Integrated portal 
¾ availability, integrity, confidentiality of Visualization and Data Model-
ing tool 
¾ availability, integrity, confidentiality of the high-performance comput-
ing systems 
¾ availability, integrity of maintenance/production system 
¾ availability, integrity and confidentiality of mail services 
4.3   Threats and Vulnerabilities 
Having determined the list of key assets, the analyst needed to identify significant 
threats to these assets, and to specify the likelihood and consequence values such as [2]  
¾ unauthorized modification of the network 
¾ corruption, theft, loss of info 
¾ attacks/errors affecting database 
¾ attacks/errors affecting EPSCoR nodes (individual computers/laptops) 
¾ attacks/errors affecting integrated portal 

364 
S.M. Rahman, S.V. Ahamed, and S.S. Erdogan 
 
¾ attacks/errors affecting Visualization and Data Modeling tool 
¾ attacks/errors affecting mail server 
¾ attacks/errors affecting high-performance computing systems 
4.4   Risk Treatment 
EPSCoR man-
agement 
may 
decide that for 
business 
rea-
sons, given an 
overall view of 
the 
organiza-
tion, some risks 
with 
lower 
levels should be 
treated ahead of 
other risks. This 
is both a reflec-
tion of limita-
tions in the risk 
analysis process 
in the range of 
ratings accessi-
ble and their 
elucidation, and 
management’s 
viewpoint of the organization as a whole. Figure 6 shows a range of possibilities for 
costs verses levels of risk. If the cost of treatment is high, but the risk is low, then it is 
usually not feasible to proceed with such treatment. Alternatively, where the risk is 
high, and the cost comparatively low, then obviously treatment should take place. The 
most difficult area occurs between these extremes. This is where management must 
make a business decision about the most effective use of their existing resources. This 
decision generally requires a more detailed investigation of the treatment options [2].  
4.5   Risk Register 
Consider that externally sourced attacks are increasing, and known cases of attacks on 
EPSCoR networks exist,  we have concluded that whilst an attack was very unlikely, 
it could still possibly occur. Thus a likelihood rating of "Rare" was chosen. The con-
sequence of successful attack could have serious consequences as it could affect the 
safety of personnel in the mine, and the financial impact could be. A consequence 
rating of "Major" was selected. This results in a risk level of High. For integrity of 
stored information, a likelihood rating of "Possible" was chosen. We have summa-
rized and presented our proposal with the final result of this security risk assessment 
process is shown in table 1. 
 
 
Fig. 6. Cost vs. risk level [2] 

 
IT Security Assessment for Interdisciplinary Research 
365 
 
Table 1. Security risk assessment summary [2] 
Asset 
Threat/  
Vulnerability 
Existing 
Controls 
Likelihood 
Consequence 
Level of  Risk 
Risk Priority 
Reliability 
and 
integrity of the 
EPSCoR 
nodes 
and network 
Unauthorized 
modification  
of 
network 
system 
layered 
firewalls & 
servers 
Rare 
Major 
High 
1 
Integrity 
of 
stored 
file 
and 
database  
information 
Corruption, 
theft, loss of
info 
firewall, 
policies 
Possible 
Major 
Extreme 
2 
Availability,  
integrity,  
Confidentiality  
of 
Integrated 
portal 
Attacks/errors 
affecting  
system 
firewall, 
policies 
Almost 
Certain 
Major 
High 
3 
Availability 
and 
integrity 
of 
Data  
Visualization  
and 
Modeling 
Tool 
Attacks/errors 
affecting  
system 
firewall, 
policies 
Possible 
Moderate High 
4 
Availability 
and 
integrity 
of 
Maintenance/ 
Production  
System 
Attacks/errors 
affecting 
sys-
tem 
firewall, 
policies 
Possible 
Minor 
Medium 
5 
Availability,  
integrity 
and 
confidentiality  
of mail services 
Attacks/errors 
affecting  
system 
firewall, 
ext 
mail 
gateway 
Almost 
Certain 
Moderate High 
6 
 
 

366 
S.M. Rahman, S.V. Ahamed, and S.S. Erdogan 
 
5   Conclusion 
Internet and digital infrastructure are essential parts of our lives. Internet users are in 
increasing danger of security threats. Simply stated, there are thousands of risks, 
threats, and vulnerabilities already out there and every moment more and more so-
phisticated attacks are taking place around the world. In this paper, we have discussed 
issues involving interdisciplinary research. We have concluded that researchers, 
stakeholders, and systems of any research project are not immune from  these vulner-
abilities. Researchers must utilize emerging tools and technology to protect their 
valuable resources. In table 1, we have  summarized our study mentioning existing 
tools, risk-likelihood, risk-consequence, risk-level, and risk-priority of a typical re-
search project. These assessment need to be monitored and updated periodically. We 
have concluded that web portals, databases, and mail servers are the most vulnerable 
entities. We recommend that EPSCoR research team should be proactive and protect 
their assets from malware and malicious users. 
References 
1. Liu, S., Cheng, B.: Cyberattacks: Why, What, Who, and How. In: IT Pro., IEEE Computer 
Society, Los Alamitos (May- June 2009) 
2. Stallings, W., Brown, L.: Computer Security: Principles and Practice, 1st edn. Pearson Edu-
cation, Inc., Upper Saddle River (2008) 
3. US-CERT, Cyber Security Trends, Metrics, and Security Indicators 4(1) (June 16, 2009), 
http://www.us-cert.gov/press_room/trendsanalysisQ109.pdf 
4. Barrett, 
L.: 
Symantec’s 
‘Unlucky 
13’ 
Security 
Trends 
for 
2010 
(2010), 
http://www.internetnews.com/security/article.php/3849371/Syma
ntecs+Unlucky+13+Security+Trends+for+2010.htm 
(web 
retrieve 
on  
December 8, 2009) 
5. Ahamed, S.: Intelligent Internet Knowledge Networks. John Wiley and Sons, Hoboken 
(2006) 
6. Captcha, http://www.captcha.net/ (web retrieve on December 8, 2009) 
7. Pak, C., Cannady, J.: Asset priority risk assessment using hidden markov models. In: Pro-
ceedings of the 10th ACM conference on SIG-information technology education, Fairfax, 
Virginia, USA, pp. 65–73 (2009) 
8. Menn, J.: Cyber-criminals breaching trusted websites. Financial Times Ltd, FT.com,  
London (September 15, 2009) 
9. Shih, D.-H., Lin, B., Chiang, H.-S., Shih, M.-H.: Security aspects of mobile phone virus: a 
critical survey. Industrial Management + Data Systems 108(4), 478–494 (2008) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 367–378, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Structure and Communication of Knowledge 
Michael R. Peterson, Syed V. Ahamed, and Sevki S. Erdogan 
University of Hawaii at Hilo 
200 Kawili Street, Hilo 
Hawaii 96720 
{mrp2,sahamed,sevki}@hawaii.edu 
Abstract. The knowledge domain is more encompassing than that of wealth 
and materials. For dealing with knowledge utility, all factors (its scarcity, its to-
tal utility, its marginal utility, its utilitarian value, its exchange value, etc.) that 
influence its evaluation must be considered. From a communication perspec-
tive, knowledge can be traced backward and extrapolated forward, much like 
scientific parameters. From a structural perspective, we propose that processing 
of knowledge be based on the most basic and fewest truisms. These truisms are 
based on reality and permit characterization of information and knowledge. To 
this extent, computational processing does not depend on the philosophic writ-
ings of earlier economists. However, the truisms are validated from a longer-
term philosophic interpretation of how these truisms have survived so that they 
can be expanded and reused in computational environments. This approach 
permits machines to process knowledge based on content of a piece of informa-
tion and to enhance content. 
Keywords: Economics of knowledge, Knowledge representation, Knowledge 
functions, Knowledge and information processing. 
1   Introduction 
The evolution of society is based on the systematic collection, validation, and de-
ployment of gainful knowledge. Knowledge can range from gossip to well-guarded 
national secrets. Gossip and rumor which have little value, are filtered out of the 
computational processes. On the other hand, knowledge that is rare or unique enters 
the computational domain to be examined, refined, and enhanced. Knowledge is col-
lected systematically (from the Internet traffic), validated extensively (from the 
WWW knowledge banks), and deployed widely (from the dictionary of axioms avail-
able from the WWW wisdom bases). The true wealth of knowledge (if there is any) is 
thus evaluated rather than the raw format of knowledge in which it was presented. 
Knowledge processing becomes a precursor to the enrichment of knowledge or the 
distilling of wisdom. 
Material and monetary wealth has been discussed Adam Smith and has evolved as a 
basis for national and international trade and commerce. John Maynard Keynes 
(1883─1946) and his fiscal policy issues are still held in esteem in monitoring the 
growth of nations [1]. Unlike monetary wealth, combined information and knowledge  

368 
M.R. Peterson, S.V. Ahamed, and S.S. Erdogan 
 
( I«»K )  has many facets and implications. Whereas the measurement of wealth is sca-
lar and has a numeric measure as the currency value, the wealth of knowledge has more 
numerous measures. After all, the evolution of society is based on systematic collection, 
validation, and deployment of gainful knowledge. Knowledge can range from hearsay 
to well-guarded national secrets. Unfounded information and gossip have only marginal 
value and such information has no significance. On the other hand, if the knowledge 
discloses a rare discovery, an invention, or trade secret, then its value is at a premium. If 
the information has social significance, is rare and is still not disclosed, then the value of 
that information is high. However, information kept in total secrecy has no value unless 
it is derogatory or damaging. Even long and extended periods of torture are justified for 
prisoners of war who supposedly have “information” about the enemy! Rare and dam-
aging information has only blackmail value. For these reasons, the economics and strat-
egy for dealing with knowledge and information need different considerations from 
those established in typical economics or game theory [2].  
A certain commonality exists in the economics of knowledge and traditional macro-
economics. Money that gets stagnant and does not get invested leads to the liquidity 
trap [1]. The business community refuses to invest and grow because the economic 
opportunities are too low even though the interest rates may be low. Valuable techno-
logical information that does not find its way into production lines remains as paper in 
patent offices. In a sense, the possibility of an information-rich but stagnant society 
starts to become real, somewhat like the Japanese society in the 1980s. Valuable 
knowledge and information (like money) need deployment. Like savings that are in-
vested (Savings = Investment in classical macroeconomic theory), knowledge (Knowl-
edge = Production in knowledge economy) distilled from information needs to be 
channeled into corporations. Channeling such knowledge into institutions of learning 
creates a multiplier effect (like that in the national economy) in the ( I«»K )  domain. 
2   Velocity of Flow of Knowledge  
A certain velocity of flow of knowledge and information ( I«»K ) is necessary for either 
information or knowledge to be productive. Information that gets too stagnant (like 
money during liquidity trap conditions) or too fluid (like money during rampant infla-
tionary conditions) loses its potential to be socially valuable. A certain viscosity in the 
flow of ( I«»K ), like money flow) makes the activity rewarding and economically justi-
fied. Information that finds no channel(s) for communication has exhausted its life cycle. 
A limited commonality also exists in the economics of information and traditional 
microeconomics. The value of information and knowledge ( I«»K ) that is transacted 
becomes comparable to the value of goods or assets that are transacted. However,  
( I«»K ) does not get depleted like goods or assets that are physically exchanged. The 
depletion of the value of ( I«»K ) follows an exponential decay rather than a sudden 
change. The rate of decay can be quite sudden (high exponent) for some types of  
( I«»K ), (e.g., weapons and warfare technologies) compared to others (e.g., educational 
or medical technologies). The sharing of ( I«»K ) may bring down the value as an expo-
nential decay but it still retains some utility for both parties. Both parties benefit from 
the economic rewards yet retain the wealth of information. Monetary and material 
wealth that is shared loses value and utility simultaneously. The value of ( I«»K )  varies 

 
Structure and Communication of Knowledge 
369 
 
with server ─ client relationships. Conflictive and cooperative roles are both feasible, 
thus altering the laws of economics of knowledge and information.   
Mainly, ( I«»K ) that has social, financial, social, ethical, or moral implication is a 
resource that is not as immediately exhaustible as monetary or materialistic wealth. 
Like any other resource, ( I«»K ) can be accumulated, enhanced, stored, or even 
squandered; however, this resource has special properties. The enhancement of  
( I«»K ) is a mental/machine activity differing from the enhancement of material 
wealth, which is a production/robotic activity. For the differences cited above,  
( I«»K ), "objects" are treated as hyperdimensional objects that follow the laws of 
processing but are not quite aligned with the processing of numbers, scalars (such as 
currency values), or text. Modern computers are capable of processing vectors and 
graphical objects. Current software packages that handle complex number (x + iy), 
two dimensional space for electrical engineers and mathematicians perform as 
smoothly as the software packages that handle three-dimensional (X,Y,Z) space for 
graphics designers and movie makers. In dealing with the ( I«»K ) , "objects", special 
compilers are necessary. Such compilers should perform lexical, syntactic, and se-
mantic analyses of information objects that can identify other information objects and 
relate themselves to the newly found objects by variable and adaptive role-based link-
ages. A recursive compiler can handle such a scenario.  
The processing of graphics entities [3] starts to assume the initial flavor of the 
processing of the information objects. Some of the steps suggested in this chapter are 
initial and rudimentary, but they can be modified and enhanced1 to suit different types 
of information object(s) and their interactions. Processing of information objects de-
pends on the application. On the one hand, mechanical and routine transactions of 
information objects are akin to data processing in banking and commerce. On the 
other hand, when information has human and social implications, then a new software 
layer that emulates human processes (such as love, hate, needs, feelings, education, 
counseling) becomes necessary. Generally, human interactions follow an underlying 
economic framework of the exchange of resources. On a very short-term basis, the 
marginal utility theory [4] starts to unfold in most transactions. Perceived fairness and 
valuation are of essence in most cases. In dealing with information, most humans fol-
low a fairness and value judgment analysis unless it is willfully transgressed.  
The rational component of human processes follows simple programming ap-
proaches. The emotional component is tackled by suggesting (and adapting) a series 
of statistical paths ranging from common to rare reactions. Such reactions are  
documented in the knowledge bases around the world, and steps are adapted in neural 
networks. In such instances, the machine-generated resolution of information can be 
superior to all-human solution since machines can evaluate every type of emotional 
response in every culture and can suggest a customized response closer to the tastes of 
the humans involved.   
While machines are communicating or exchanging information, they strictly abide 
by the I/O commands of humans or of the basic core operating system. While human 
                                                           
1 For algebraic operations (multiply, divide, matrix, etc.) for complex numbers, the develop-
ment of software routines followed much later after the assembly level programs for process-
ing real numbers. 

370 
M.R. Peterson, S.V. Ahamed, and S.S. Erdogan 
 
beings process information, the value and worth of the information are initially as-
sessed, and modified by learning, clarification and negotiation. While machines are 
processing information, the information processing units2 (IPUs) alter the structural 
relationships between objects and objects, objects and their attributes, and relation-
ships between object X attributes with object Y attributes. The scenario is depicted in 
Figure 1. The alteration and redistribution of relationships is not altogether random 
(unless it is the last resort).  
Instead, they are based on laws of probabilities as to which of the relationships are 
most common and likely to form secure bonds (e.g., information about automobiles and 
information about octane values; or information about hang gliders and information  
 
Fuzzy boundaries of any 
complex information 
object, like a nation and 
society, society and 
individual,  parents and  
childern, individual and 
family members, etc. 
Object 
Object 
Object 
L
l 3
Object 
Object 
Attributes  
          Relationship Level 1   
          Relationship Level 3  
          Relationship Level 2  
Bilateral Relationship 1   
Relationship Level 4  
    Relationship Level 5  
 
 
Fig. 1. Representation of a complex information/knowledge object with five lower-level infor-
mation objects. Strong, weak, casual, unilateral, and bilateral relationships are shown. 
 
                                                           
2 Information processing and knowledge processing are used interchangeably in this paper 
since the forward processing (distilling) of information leads to knowledge and the backward 
processing (parsing) of knowledge leads to information. The same machine may be able to 
process in either direction, perhaps by changing its control memory chip sets. At this stage, it 
is premature to speak specifically to the many possibilities that still lie ahead.  

 
Structure and Communication of Knowledge 
371 
 
about wing span of birds). In the process, the machines also investigate unusual and 
uncommon relationships (e.g., information about the design of hang gliders for Austra-
lian coasts and information about the design of hang gliders for Scandinavian coasts), 
giving rise to novel and unique information, knowledge, or scientific principles (if any). 
Machines have an advantage in processing vast amounts of information quickly 
and accurately. The incremental changes in information are tallied to the incremental 
changes in external conditions to optimize and predict the information for a given set 
of new conditions. Incremental changes over any of the parameters (such as time, 
attributes, or environmental conditions) are accurately tracked and labeled. Processing 
the key information object(s) that forms the nucleus (nuclei) of the raw information 
and then reconstituting the information object(s) identifies opportunities for possibly 
new and valuable information, knowledge, or scientific principles. This is the funda-
mental clue to crossing from the information mode to the knowledge mode.  
Unlike monetary wealth and the wealth of nations [5] that are depleted, the wealth 
of information is shared. Unlike monetary wealth, information ( I«»K ) has signifi-
cantly different attributes. Whereas universal and numerical values can be assigned to 
monetary wealth, information has overlapping qualities and fuzzy parameters to 
transact information.   
Complexity theory [6] starts to resemble knowledge theory because of the highly 
variable nature of ( I«»K ), “objects” and their interrelationships. Most of the precepts 
of complexity theory become applicable when dealing with information and knowl-
edge. However, in dealing with ( I«»K ) , we limit the processing to a confined num-
ber of objects that do not make the information processing chaotic. The self-contained 
structure is statistically prioritized with statistically weighted relationships between 
the objects that are considered valid for the processing of ( I«»K ) "objects." In  
addition, the limitations of the computer system (accuracy, memory size, speed, and 
possible switching capability) define the size and the “body of knowledge” (or the 
“complex initial object”) that the computers will handle.  
The knowledge processing system filters out any “objects” that are likely to cause 
chaotic and unstable oscillations in the processing. It refuses to process inconsistent 
information, much like computers that refuse garbled data. During the execution phase, 
irrational requests to process information are terminated and the error condition is in-
timated, just like computers that refuse to execute impossible numeric operations. 
Unlike complexity theory, the knowledge theory will perform legitimate functions on 
objects for which some earlier statistical information is available on world-wide 
knowledge banks. If the extent of information is too restrictive, the learn mode [7] is 
invoked to build a knowledge base for the unknown object. The machine guards itself 
from being drawn into an execution mode that ends in catastrophe by establishing non 
circular forward and backward pointers. Even though recursion is permitted, the depth 
of recursion is made consistent with the machine capacity. Rationality is given higher 
priority than the task of execution of a knowledge program. These bounds of rational-
ity contain the fuzzy bounds of knowledge that is under process. To this extent, it re-
gains its own stable operating condition, just as a human being would attempt to do. 
Thus, overall knowledge processing systems have fair a chance of solving complex 
knowledge problems that human beings by themselves cannot attempt.  
The knowledge processing system limits the size of the body of knowledge proc-
essed by a quantitative measure of the capacity of the machine in relation to the  

372 
M.R. Peterson, S.V. Ahamed, and S.S. Erdogan 
 
requirement of “complex initial object.” No such limitation is imposed in complexity 
theory. For this reason, knowledge theory is based on the computer systems that will 
attempt to solve a knowledge problem. Knowledge theory is a valid tool in initially 
formulating the problem and becoming strategic in its solution. The system resource 
expended to change the status of information and knowledge (see P3 in Section 4) 
during the course of the solution of the problem will be (in most instances) the bottle-
neck. In essence, complexity theory is an open-ended theory, but knowledge theory 
works in the context of machines having discrete (binary or hyperspace) representa-
tions, limited in their memory, I/O, switching capacities, and speed of operation.  
To this extent, knowledge theory is like information theory that works in most non-
chaotic but extremely noisy environments. Knowledge theory does not violate any of 
the principles (such as auto-organization, edge of chaos, power of connections, circular 
causality, try&learn, and ologrammatic principle) set forth by complexity theory. To 
some extent, auto-organization and try&learn are based on the survey of the world-wide 
knowledge bases on the Internet to find out how other complex knowledge objects have 
accomplished auto-organization and adaptation. To this extent, quantification within 
knowledge theory (like that within information theory) becomes totally feasible.  
Shared information loses value at a relatively low rate. Whereas there is a sugges-
tion of strict zero-sum game [2] in transacting the wealth of nations and individuals, 
there is an impression of elastic zero-sum game as two parties share knowledge and 
information. Wealth (i.e., all the utilities combined together) and value rather than 
price of information are only perceived at the time of sharing information. The sale 
price of a commodity or an asset can only arise in a free-market environment. The 
price for sharing information is perceived between buyer and seller and not determined 
by market forces. Sometimes the value of information in a document, a book, or a 
scripture far exceeds the price of the book and sometimes the converse can be the case.  
In the knowledge domain, an approximation for the scarcity, value and life of the 
information is feasible.  Along the scarcity, value, and lifetime (three dimensional 
curve) five coordinates points can be readily identified: (1) totally unshared and secret 
information has no value and indeterminate life, (2) guarded information has high 
value and relatively long life, (3) information shared with a select clientele has high-
est value until it starts to leak and slowly erodes in value, (4) media information has a 
media price and short life, and finally (5) gossip and trivia has junk value and dissi-
pates without a trail.  The value of information in a socioeconomic setting has at least 
three additional dimensions, the truth contained, the elegance or appeal conveyed, and 
the social benefit that can be derived from the information.    
To deal with the complex nature of information from a computational and processing 
perspective, we propose four (truism, philosophic, scientific, and economic) dimensions 
or senses, shown in Figure 2, in which information can be characterized. In dealing with 
information as an object, truism of all information objects (not their content) states the 
truth (as well as it is known) about the entire object class. Similarly, philosophic charac-
terization of all information objects (not their content) states the philosophic nature (as 
well as it is known) about the entire object class, and so on.  
Processing an information object can alter its four characteristics (T, P, S, E). In 
fact, constructive processing will make marginal information (objects) into significant 
information (objects), if there is any significance. Worthless information is filtered 

 
Structure and Communication of Knowledge 
373 
 
Link of an Object to its T, 
P, S & E nature 
Processing of the  
information that alters  
balance of relations  
Class of 
Information 
Objects 
S
P
T 
E 
out from any scientific knowledge processing3. The process can be deduce, interpret, 
derive, systematize, analogize, categorize, conceptualize, rationalize, and generalize 
or any other process that has a scientific basis.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 2. Representation of the class of knowledge and information objects with four characteris-
tics: truism, philosophic, scientific, and economic nature of knowledge 
In order to initiate the information processing to search out new information or 
new knowledge from vast amounts of information, three steps are proposed: observa-
tion of reality, philosophic validation, and scientific principles that can be generalized 
and deployed elsewhere. The observation of reality is fundamental to all sciences. 
Since information has illusive boundaries and flexible formats, the concept resides in 
the content of information and goes deeper than a statement or representation of in-
formation. In a sense, information is the water that can be poured into any vessel. The 
water is a real information object with its own properties and the vessel is the secon-
dary information object. Together, they form a (partially) stable object group. 
The philosophic validation is necessary to provide a long-term continuity and sta-
bility of information, such that any inference/scientific conclusion can be drawn. To 
continue with the earlier example, if the water is poured into a vessel carved out of 
                                                           
3 Most compilers permit programs (program-objects) from proceeding to the execution phase 
only if they are free of all syntactic, all semantic and all linkage errors. In a similar vein, in-
formation that is inherently false, malicious, or laden with pornography will not gain access 
to information-object processing systems. 

374 
M.R. Peterson, S.V. Ahamed, and S.S. Erdogan 
 
ice, there will be neither water nor ice over a long enough period4 of contemplation to 
form a stable object group (i.e., water in a vessel). It becomes necessary to probe the 
wealth of knowledge and information (water) in society (vessel) to validate the reality 
as a stable and dependable basis. 
3   Truisms in Knowledge Domain  
Observation of reality over long periods leads to generality or truism. In dealing with 
Knowledge, three notions are suggested:  
T1. Knowledge has a life cycle.  
T2. Knowledge can be altered, but any alteration of knowledge needs expenditure of 
energy.   
T3. Knowledge has impact. The list is short and other dependent truisms can be  
derived from the three listed above. The truism layer is shown at the top of Figure 3. 
The T1 to T3 list is kept deliberately short with the hope that the list of derived sci-
entific principles will also be elementary and short. This would reduce the basic op-
erations that a computer system will have to perform while processing knowledge.  
4   Philosophic Validation of Knowledge 
Only four philosophic validations (P1─P4) are suggested and the list is deliberately 
kept short to reduce the instruction set for the machine to process knowledge. It is 
depicted as the middle layer in Figure 3. Based on T1, the justified philosophic vali-
dation (at this time) is as follows: 
P1. Knowledge is timely or obsolete and it can change its characteristics over time. Typi-
cally, human or machine processing changes the derived knowledge. It ranges from 
mere gossip to a scientific principle or an equation in physics. When the linkages to the 
raw knowledge are not retained, the processed knowledge may assume the identity of a 
new knowledge object. Hence the concept of Aristotle's beginning, middle, and end be-
comes fuzzy in the knowledge domain. We refer to this particular validation as P1.  
Based on T2, two philosophic validations (P2 and P3) are feasible and presented  
    separately because the implications are different.  
P2. Boundaries of knowledge and information are vague and fuzzy. Returning to the 
example of water in a vessel, information is blended in the human perception. 
Much like the features of beauty that lie in the eye of the beholder, the boundaries 
of knowledge lie in the mind of the receptor. Human perception becomes a fading 
memory (or a leaky bucket) to hold knowledge (water). When a machine receives 
knowledge, the knowledge objects, their structure, and their relationships are ana-
lyzed and stored with the timeline for that specific "body" of knowledge.  
In a sense, the "knowledge compiler" performs a lexical, syntactic, semantic, and 
timeline analysis on "knowledge inputs" and identifies the knowledge objects, their 
structures, and their relationships.  
                                                           
4 Unless the situation is adiabatic at 32˚ F, which becomes too specific to draw any general 
conclusions. 

 
Structure and Communication of Knowledge 
375 
 
T2- can alter 
percentages
T3 - has
Impact
T1 - has  
Lifecycle 
Truism (T)
Level
S1
S2
S4
S5
S3
Scientific
(S) Level
 Three Levels (T, P and S)  of Information
P1,
Time-
Variant
Philosophic 
(P) Level 
P2,
Fuzzy and
Vague
Boundaries
P3, Truth, 
Value
Elegance, in
different %'s
P4
Rewards  
and
Retributions, % 
Information is 
dynamic. at any  
instant, information 
can be tagged by a 
timeline and 
differentiated 
(discretely) with 
respect to time, 
interpolated, 
extrapolated, 
corrected and 
restored. Information 
can be grouped (i.e., 
integrated) over time, 
independently or with 
current or past 
information  object(s).  
Information is 
dynamic. At any 
instant, information 
can be tagged by a 
timeline and 
differentiated 
(discretely) with 
respect to time, 
interpolated. 
extrapolated, 
corrected and 
restored. Information 
can be grouped (i.e., 
integrated) over time, 
independently or with 
current or past 
information-object(s).
Processed 
Information 
can be 
shared to 
offer 
enhanced or 
modified 
rewards or 
retributions.  
Processed 
information 
can be 
enhanced, 
polished, or 
made more 
beneficial.  
However, it 
retains its 
linkages to  
raw  
Information. 
Information has 
confidence levels 
regarding three 
features (truth, 
social value and 
elegance). Truth 
and accuracy are 
generally the 
easiest to 
evaluate with 
level of 
confidence tests.  
The other two 
features (social 
value and 
elegance) are 
harder to 
evaluate.
Information has  
confidence levels 
regarding three 
features (truth, 
social value and 
elegance). Truth 
and accuracy are 
generally the 
easiest to 
evaluate with 
level of 
confidence tests.  
The other two 
features (social 
value and 
elegance) are 
harder to 
evaluate.
Boundaries of 
information 
are vague and 
fuzzy. Such 
boundaries 
are also 
flexible and 
can be made 
inclusive or 
exclusive. A 
single 
information 
object exists 
with discrete 
topological 
structure, 
graph, and/or 
graphical 
pattern. 
Groups of 
such 
information 
objects also 
have
structure, 
graphs, and/or
patterns of 
strong/weak 
relationships 
within 
information 
objects and/or 
their 
attributes. 
Information can be 
processed recombined, 
merged, fragmented, and 
restructured with or 
without other information 
objects. It can be 
segmented (differentiated 
or partially  differentiated 
with respect any other 
information-objects 
and/or attributes), 
grouped (integrated), 
interpolated, extrapolated, 
rationalized, and restored 
in object or attribute 
space.
Information can be 
processed recombined, 
merged, fragmented, and 
restructured with or 
without other information 
objects. It can be 
segmented (differentiated 
or partially  differentiated 
with respect any other 
information-objects 
and/or attributes), 
grouped (integrated), 
interpolated, extrapolated, 
rationalized, and restored 
in object or attribute 
space.
Scientific   
Implications 
Economic  
Implications 
 
Fig. 3. Three (T, P and S) levels of knowledge processing  

376 
M.R. Peterson, S.V. Ahamed, and S.S. Erdogan 
 
 
Implication P3 based on T2 is  
P3. Knowledge has three qualitative features; truth contained, social value conveyed, 
and the inherent elegance in content in variable proportions. Knowledge can also 
have the opposite features (falsehood, social malice, and ugliness) in variable pro-
portions. An equally important principle is that the change of status of knowledge 
implies an effort (equivalent to force) to bring about the change sustained over the 
displacement of the status (thus invoking a concept of psychological or social en-
ergy or the deployment of resources).  
To fall back on the example of water and the vessel, if the water carries three par-
tially dissolved solutes (sugar, sweetener, and honey), then the viscosity changes thus 
altering the fluid mechanics and the concentration levels in different sections of the 
vessel. Furthermore, any alteration of the concentration level, after an equilibrium 
condition is reached, needs energy for the change (such as stirring, shaking, vibrating, 
or adding more water). The scientific basis for predicting the concentration contours 
becomes quite complex and even unpredictable (like the weather). However, when a 
machine has a basis of estimating the truth (sugar), the social value (sweetener), and 
the elegance (honey) independently (based on statistical sampling of other knowledge 
objects and their relationships), then the raw/processed knowledge can be scientifi-
cally evaluated with appropriate confidence levels.  
Based on T3, the validation for P4 is stated as follows, 
P4. Sharing of knowledge can bring rewards or retributions in any variable propor-
tion. This particular implication carries little impact in the scientific domain but be-
comes significant in the social and economic domains. In the socioeconomic realm, 
it is generally accepted practice to exchange items of similar value (including 
knowledge, patents, techniques, and ideas). It is also frequent to find the extent of 
damage inflicted as retribution. In the knowledge domain, litigation and penalties 
are imposed when negative knowledge and bad publicity are purposely circulated.  
5   Scientific Principles in Knowledge Domain 
Five scientific principles (S1─S5) are derived from the four philosophic validations 
(P1─P4). The first principle, S1, results from P1 and is stated as follows. 
S1. It is implied that knowledge is dynamic. At any instant, knowledge can be seg-
mented (differentiated with respect to time), encoded, communicated, corrected, 
interpolated, extrapolated, restored and even reconstituted. Knowledge can be 
grouped (i.e., integrated over time), independently or with current or past knowl-
edge object(s). If knowledge objects are treated as dynamic and continuous in the 
time domain, then differentiation and integration become possible. The analog 
and closed-form operations are irrelevant, but finite and event-driven changes are 
sensed from information and knowledge bases. For instance, every scientific 
meeting or conference adds or subtracts from the collective knowledge base of a 
community. Human beings and/or machines can process new knowledge-objects 
continuously. When finite changes are necessary then the commitment of re-
sources becomes essential. Hence the concept of (expected) incremental or mar-
ginal costs are evaluated and equated to the (expected) incremental or marginal 
benefit that is gained.  

 
Structure and Communication of Knowledge 
377 
 
The second principle, S2, is derived as an extension of P2 and is stated as follows: 
S2. Boundaries of knowledge and information are vague and fuzzy. Such boundaries 
are also flexible and can be made inclusive or exclusive of other knowledge ob-
jects. Single knowledge object exist as topological structures, “graphs,” and/or 
graphical patterns. Groups of information objects also have structure, graphs, 
and/or patterns of relationships within the information objects and/or their attrib-
utes. Structures, graphs, and patterns (see Figure 1) can have scientific implications 
for stability. In the domain of knowledge, the knowledge objects need reasonable 
bonds to remain existent for a period. Insecure bonds between objects only results 
in short-lived rumors and gossip. 
The third principle S3 also results as an extension of P2 and stated as follows: 
S3. A knowledge object can be processed, corrected, recombined, merged, frag-
mented, and restructured by itself or in conjunction with other knowledge objects. 
It (they) can also be segmented (differentiated or partially differentiated with re-
spect to other knowledge objects or attributes), grouped (integrated), interpolated, 
extrapolated, rationalized, and restored in object or attribute space. The basic tools 
of (discrete) mathematics become applicable in dealing with the continuity of 
knowledge over time and the continuity of (structural relationships or discrete) 
contours with respect to other objects or their attributes.  
The fourth principle S4 results from P3 and stated as follows. 
S4. Knowledge has confidence levels regarding three features (truth, social value and 
elegance). Truth and accuracy are generally the easiest to evaluate in context to 
other similar single knowledge-objects or other multiple knowledge-objects with 
level of confidence tests. Generally, (local and global) knowledge bases that con-
tain knowledge about similar objects can provide a basis for confidence tests. The 
other two features (social value and elegance) become harder to evaluate.  
The fifth principle S5 results from P4 and is stated as follows: 
S5. Processed knowledge can be shared to offer enhanced or modified rewards or  
retributions. Processed knowledge retains its linkages to raw knowledge. Human 
processing of knowledge has taken a firm hold in society. Transitory knowledge 
processed by the human mind is dispersed as conversation. Knowledge that is more 
important is documented and retained for further reference. In the realm of process-
ing by intelligent machines or systems, knowledge can provide more value (truth, 
social significance, or elegance) in the processed mode, especially if the processing 
is done on a scientific basis by following principles (S1─S4). For example, segmen-
tation and recombination offer a slightly different form of truth (that is equally 
valid) as the original truth. Similarly, mere rearranging of the words can sometimes 
make a hidden context or idea more apparent and so on. From a computational per-
spective, simple differentiation tests (i.e., event analysis and correlation studies) can 
reveal the more sensitive knowledge objects with a complex knowledge structure. 
6   Conclusions 
This paper presents two major aspects of knowledge. First, in dealing with knowledge, the 
truism tempered by long-term philosophic validation leads to scientific principles. These 
principles are formulated as qualitative and statistical relationships to start a basis of 
knowledge theory by which the differentiation, integration and sensitivity of knowledge 
can be estimated. Primary and secondary knowledge objects are introduced to offer 

378 
M.R. Peterson, S.V. Ahamed, and S.S. Erdogan 
 
knowledge structure and dependence. The quantitative basis and content of a body of 
knowledge are established by the number of secondary objects, their structural relation-
ships, the number of attributes of each secondary object, and their own relation matrices.  
Granularity of the knowledge space is defined as the smallest prism formed by the 
numerical precision of the computer systems, the lowest Hamming distance between 
the code words that the networks can carry at their maximum speed, and the percep-
tion of human beings who will sense the microprism of knowledge. At least one di-
mension of this prism is personality dependent, even though the numerical precision 
of the computers and lowest Hamming distance through the network can be accu-
rately quantified for that particular human-machine system.  
Second, in dealing with the theory of knowledge, the comparison with complexity 
theory shows that knowledge theory is closely intertwined to the quantity of knowl-
edge (see the paragraph above) in any primary knowledge object. However, knowledge 
theory is always retractable and (almost) never gets chaotic for three reasons.  
(1) The linkage (forward and backward pointers, depth of recursion, size of mem-
ory) built in the operating systems of computers will prevent tail-chasing loops 
through the many knowledge objects.  
(2) The seven OSI layers will automatically prevent networks from getting 
trapped in endless send-resend cycles of any packets, sessions, blocks, knowledge 
objects and so on. 
(3) The human beings who monitor the machines are capable of preventing ma-
chines from senseless and silly pursuits in the knowledge domain. 
Knowledge processing systems are based firmly on the triad of machines, networks 
and humans working in conjunction and cooperation. Three mechanisms {the ma-
chines, (their architectures and operating systems), the networks (their layering and 
protocol) and the human beings (their natural intelligentsia)} work synergistically in 
making knowledge environment complex but manageable.  
Acknowledgements 
This work is supported by EPSCoR award EPS-0903833 from National Science 
Foundation to the University of Hawaii. 
References 
1. Keynes, J.M.: The General Theory of Employment, Interest, and Money. Prometheus 
Books, Buffalo (1997) 
2. Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior. Princeton  
University Press, Princeton (2004) 
3. Parker, J.R.: Algorithms for Image Processing and Computer Vision. John Wiley and Sons, 
Hoboken (1996) 
4. Arena, R., Quere, M. (eds.): The Economics of Alfred Marshall: Revisiting Marshall’s  
Legacy. Palgrave Macmillan, New York (2003) 
5. Smith, A.: The Wealth of Nations. Prometheus Books, Buffalo (1991) 
6. Byrne, D.S.: Complexity Theory and Social Sciences. Routledge, New York (1998) 
7. Ahamed, S.V.: Intelligent Internet Knowledge Networks. John Wiley and Sons, Hoboken 
(2006) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 379–388, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Existing Recognition Base Usability Features of the 
Graphical Password 
Ali Mohamed Eljetlawi1 and Norafida Ithnin2 
1 Faculty of Computer Science and Information Systems, 
Centre for Advanced Software Engineering (CASE),  
Universiti Teknologi Malaysia, City Campus, Jalan Semarak, 
54100 Kuala Lumpur, Malaysia 
2 Faculty of Computer Science and Information Systems, Universiti Teknologi Malaysia,  
81300 Skudai, Johor 
jetlawei@yahoo.com, afida@utm.my 
Abstract. Graphical passwords are an alternative authentication method to al-
phanumeric passwords in which users click on images to authenticate them-
selves rather than type alphanumeric strings. This research aims to study the us-
ability features of the recognition base graphical password methods  available 
and  extract the usability features of the existing methods. In this paper we 
study the recognition base graphical password type with the available methods 
from the usability point of view according to previous studies and surveys. 
Then we match the usability features (General usability features, existing us-
ability features for existing graphical password methods, and ISO usability fea-
tures) to the existing graphical password methods and make a comparison study 
between these methods and the usability features. We have found that there is 
no method has the most important usability features. Thus, by completing this 
study a set of usability features is suggested to be in one graphical password 
system. This set includes the easy of use, memorize, creation, learning  and sat-
isfaction. Moreover, this work proposes to build a new system of  graphical 
password system that provides promising usability features. 
Keywords: Graphical password, Graphical password techniques, Recognition 
base graphical password methods, Usability. 
1   Introduction 
The password is a very common and widely authentication method still used up to now 
but because of the huge advance in the uses of computer in many applications as data 
transfer, sharing data, login to emails or internet, some drawbacks of normal password 
appear like stolen the password, forgetting the password, week password, etc so a big 
necessity to have a strong authentication way is needed to secure all our applications as 
possible, so researches come out with advanced  password  called graphical password 
where they tried to improve the password techniques and avoid the weakness of normal 
password. Alphanumeric passwords were first introduced in the late 1960s [1]. Today, 
many networks, computer systems and Internet-based environments used this technique 

380 
A.M. Eljetlawi and N. Ithnin 
 
to authenticate their users. The vulnerabilities of this technique  have been well known 
generally. Dictionary attack is the commonly method used by hackers to break or crack 
the alphanumeric password, such attack is very efficient mechanism because its only  
need a little time to discover the users passwords. Another major drawback of this 
method is the difficulty of remembering the passwords. As studied by Gilhooly (2005) 
[2], the good and hard to guess or break passwords basically difficult to memorize. Re-
cent studies from Dhamija et al (2000) [3] showed that humans are only capable to 
memorize a limited number of passwords, because of this syndrome, they often to write 
down, share and use the same passwords for different current account. Graphical pass-
word techniques have been proposed as an alternative to alphanumeric based techniques. 
It has been designed to overcome the known weakness of traditional alphanumeric pass-
word. It also designed to make the passwords more memorable, easier for people to use 
and therefore more secure. Based on the two assumptions; first, humans can remember 
pictures better than alphanumeric characters and second, a picture worth a thousand 
passwords; some psychological studies and company software seem to agree with these 
assumptions [4,5]. As known  generally, the main drawbacks  for the current  graphical  
password schemes are the shoulder-surfing problem and usability problem. Even though 
graphical passwords are difficult to guess and break, if someone direct observe during the 
password enter sessions, he/she probably figure out the password by guessing it ran-
domly. Nevertheless, the issue of how to design the authentication systems which have 
both the security and usability elements is yet another example of what making the  chal-
lenge of  Human  Computer  Interaction (HCI)  and security communities. 
Another important question is; do all the existing graphical passwords schemes 
fulfill the ISO usability requirements? In this research the ISO 9241 and ISO 13407 
will be considered from the usability point. ISO 9241-11 defined usability as; the 
extent to which  a product can be used  by specified  users to achieve specified  
goals with effectiveness, efficiency and satisfaction in a specified context of use. 
Currently there are 17 ISO usability requirements which are divided into two 
parts. Parts 1-9 is the hardware environments and the other parts 10-17 is in the 
software development [6]. 
2   Literature Review 
2.1   Graphical Password Techniques 
Knowledge based techniques are the most widely used authentication techniques 
and include both text-based and picture-based passwords. The picture-based tech-
niques can be further divided into two categories: recognition-based and recall- 
based graphical techniques. Using recognition-based techniques, a user is presented 
with a set of images and the user passes the authentication by recognizing  and iden-
tifying the images he or she selected during the registration stage. Using recall- 
based techniques, a user is asked to reproduce something that he or she created  or 
selected earlier during the registration stage. [7] 
The existing Graphical password schemes are categorized as based either on: 
1- Recognition Base technique. 
2- Recall Base technique. 

 
Existing Recognition Base Usability features of the Graphical Password 
381 
 
3   Summary of the Recognition Base Graphical Password Methods 
3.1   Recognition Base Technique 
In recognition based techniques, users are given a set of pictures and they pick and 
memorize some of them. During authentication, the users need to recognize and iden-
tify the pictures they have picked earlier. 
There are six methods for this category which are: 
1- Method one is Dhamija and Perrig algorithm. 
2- Method two is Sobrado and Birget algorithm. 
3- Method three is Man, et al. algorithm. 
4- Method four is Jansen et al. algorithm. 
5- Method five is Takada and Koike. 
6- Method six is Passface algorithm. 
3.1.1   Dhamija and Perrig Algorithm 
This  method  is proposed a graphical authentication  sceme based on Hash Visuali-
zation technique. In their system, user will be asked to select certain number of im-
ages from a set of random pictures generated by a program (Figure 1). Later, user 
will be required to identify the pre-selected  images to be authenticated. The results 
showed  that 90% of all participants succeeded in the authentication using their tech-
nique, while only 70% succeeded using text-based passwords and  PINS. The 
average log-in time, however, is longer than the traditional approach, but has a 
much smaller failure rate. A drawback is that the server needs to store a large 
amount of pictures which may have to be transferred over the network, delaying the 
authentication process. Another weakness of this system is that the server needs to 
store the seeds of the portfolio images of each user in plain text. Interface-wise, 
the process of selecting a picture from picture database can be tedious and time  
consuming for the user. [8] 
 
Fig. 1. Random arts used by Dhamija and Perrig 
 

382 
A.M. Eljetlawi and N. Ithnin 
 
3.1.2   Sobrado and Birget Algorithm 
Sobrado and Birget (2002) [1] proposed several numbers of graphical passwords 
technique that overcome the shoulder-surfing attacks. In their first scheme which 
they called  “triangle  scheme”,  a  user  needs  to  selects  their  pass-object  among  
many displayed object. To be authenticated, a user needs to recognize all the pre-
selected pass-object which was selected during the registration phase.  The user 
requires to click inside the convex-hull which formed by the pass-object (Figure 2). 
 
Fig. 2. A shoulder-surfing scheme-1 
The probability of successful login by random clicking maybe occur because of 
the size of the convex-hull can be large. To make the password space large 
enough and difficult to guess, Sobrado and Birget suggested using 1000 objects on 
the login process. However, by increasing the number of object will cause the dis-
play become more crowded and difficult to find the pass-object, but if the  number 
of objects is reduced, the password space will become smaller because of the size 
of the convex- hull can be large. If this problem happen the password will be easy to 
crack and guess. to  recognize all the previously seen pass-objects. However, only 
three pass-objects were involved in this technique. One of the pass-objects will 
place into the moveable frame. To be authenticated, user only needs to move by ro-
tating the frame until all the pass-object is located in form of straight line (Figure 3). 
To minimize the likelihood of logging, Sobrado and Birget suggest repeating the 
process  several  times by clicking or rotating it randomly. However, this step wills more 
unpleasant,  confusing and time consuming since there are too many non-pass objects. 
3.1.3   Man, et al. Algorithm 
They proposed  shoulder-surfing resistant algorithm. In this algorithm, a user selects 
a number of pictures as pass-objects. Each pass-object has several variants and each 
variant is assigned a unique code. During authentication, the user is challenged 
with several scenes. Each scene contains several pass-objects (each in the form of 
a randomly chosen variant) and many decoy-objects. The user has to type in a  
 

 
Existing Recognition Base Usability features of the Graphical Password 
383 
 
 
Fig. 3. Moveable frame scheme 
 
Fig. 4. Shoulder surfing scheme-2 
string with the unique codes corresponding to the pass-object variants present in the 
scene as well as a code indicating the relative location of the pass-objects in refer-
ence to a pair of eyes (Figure 4) [9]. 
3.1.4   Jansen et al. Algorithm 
Jensen et  al (2003) [10] proposed a graphical password scheme based on “picture 
password”. This scheme was designed especially for mobile devices such as PDAs. 

384 
A.M. Eljetlawi and N. Ithnin 
 
Throughout the password creation, a user has to select the theme first (e.g. sea and 
shore, cat and dog and etc) which consists of thumbnail photos. Afterward, a user has to 
selects and registers a sequence of the  selected  thumbnail photo to form a password 
(Figure 5). The user needs to recognize and identify  the previously seen photos and 
touch it by using stylus with the correct sequence in order to be authenticated however, 
as the number of thumbnail photos is limited only to 30, the size of the password space 
is considered small. A numerical value is assigned for each thumbnail photo and the  
sequence of selection will produce a numerical password. This numerical password is 
shorter than  the length of alphanumeric password. To overcome this problem a user can 
select one or two thumbnail photos as one single action in order to create and enlarge 
the password space. However, the increase of this password selection will make the 
memorability of the created password become more complex and difficult. 
 
Fig. 5. A graphical password scheme proposed by Jansen 
3.1.5   Takada and Koike 
This technique allows users to use their favorite image for authentication [11]. The 
users first  register their favorite images (pass-images) with the server. During au-
thentication, a user has to go through several rounds of verification. At each round, 
the user either selects a pass-image among several decoy-images or chooses nothing 
if no pass-image is present. The program would authorize a user only if all verifica-
tions are successful. Allowing users to register their own images makes it easier for 
user to remember their pass-images. 
3.1.6   Passface Algorithm 
Based on the assumption that human can recall human faces easier than other pic-
tures, Real User Corporation has developed their own commercial product named 
Passfaces TM [5]. Basically, Passfaces TM works as follows. User are requires to select 
the previously seen human face picture from a grid of nine faces which one of the 
face is the known face and the rest is the decoy faces (Figure 6). This step con-
tinuously repeated until all the four  face is identified. User needs to recognize 
all the face selected during the enrollment stage. User  is  authenticated if all the 
correct face is successfully identified. 

 
Existing Recognition Base Usability features of the Graphical Password 
385 
 
 
Fig. 6. Passfaces 
There are several drawbacks with Passfaces TM is that some faces displayed in 
the grid may not be impressive by certain users. For example, if the user has to look 
at some faces that  he or she does not like, the login process will become unpleas-
ant. Another drawback of Passfaces TM is that it cannot be used by people who are 
face- blind. According to the comparative studies conducted by Brostoff and Sasse 
(2000) [12] which 34 subjects involved in the test showed that, the Passfaces TM 
password is easier to remember compare with textual passwords. Result also showed 
that Passfaces TM took longer time to login than textual passwords. Empirical and 
comparative studies by Davis et al (2004) [12] showed that, in Passfaces TM the 
user’s choice is highly affected by race, the gender of the user and the attractive-
ness of the faces. This will make the Passfaces TM password somewhat predictable by 
the attackers. 
4   Usability 
Usability is an important issue in order to develop a good scheme that can sat-
isfy the user needs and requirements. As stated by the ISO 9241-11 standard, they 
defined usability as the extent to which a product can be used by specified users 
to achieve specified goals with effectiveness, efficiency and satisfaction in a speci-
fied context of use [6]. In graphical password scheme one of  the main argu-
ments for graphical passwords is that  pictures are easier to remember than text 
strings (memorability). Preliminary user studies conducted by some researchers 
seem to agree with this opinion. However, recently user studies are still limited to 
support this view. There are  many factors associated with the memorablity issues. 
In order to make the graphical password are more memorable, earlier researchers 
have suggested that the created password must be meaningful, frequently used and 

386 
A.M. Eljetlawi and N. Ithnin 
 
fun [13, 8, and 5]. Nevertheless, there are some securities tradeoffs need to be 
aware if we agree with these suggestions. For example, it is easy to guess, break and 
predict the password if it is too meaningful. Additional usability issue is that the 
password registration and login process are time consuming, especially in recogni-
tion-based schemes. For example, user has to pick images from a large set of selec-
tions during the registration phase and user has to scan many images to identify a 
few pass-images in order to be authenticated [1]. Because of this process, it will 
become unpleasant and difficult especially for the newbie’s in graphical passwords 
environment. 
5   Usability Features of the Recognition Base Graphical Password 
Methods 
In human computer interaction and computer science, usability usually refers to the 
elegance and clarity with which the interaction with a computer program or a web 
site is designed. 
A study of the usability features and sub features of recognition base graphical 
password methods has been done and bymapping the usability features to the recog-
nition base systems we can notice that the feature easy of use that the sub feature use 
mouse easily is corresponding to three first methods which are Damija, Sobrado, 
Man and Passfaces method also where the other methods are specially designed 
for mobile and can not use mouse for that but for sub feature login time the recog-
nition base graphical password methods need long login time  and the sub feature 
is not provided by any of them where for the feature easy to memorize we  can  
see that remembering faces is provided by the methods Takada and Passfaces only 
and the other methods not support that sub feature but for the sub feature remember-
ing pictures the methods Damija, Jansen and Takada are provide this sub feature 
where the other three methods not. But for feature easy to create there are two sub 
features, creation of the password and dealing easily with the system, we can see 
that for the sub feature create the password the methods Man and Passfaces only 
provide that sub feature where the other methods not support that feature but the 
dealing easily with the system sub feature supported only by Passfaces method and 
the other methods not support that sub feature. The fourth feature of the existing  
graphical password methods is the ease to execute which has two sub features also, 
short time execution and good views, for the sub feature short time for execution 
only the method Jansen has short time but other methods has long time to execute 
the task and therefore these methods not support this sub feature where the sub fea-
ture good views only provided by Passfaces method. The last feature of the existing 
graphical password methods is the ease to recognize feature and it has two sub 
features, easy to understand and not complicated where the sub feature easy to 
understand is provided by the methods Damija and Passfaces only and the other 
methods not support that sub feature but thesub feature not complicated is not pro-
vided by any graphical password methods. 
 
 

 
Existing Recognition Base Usability features of the Graphical Password 
387 
 
6   Result and Discussion 
In this paper, we have conducted a comprehensive and  comparative study of existing 
recognition base graphical password techniques from the point view of usability features. 
Even though the main argument for graphical passwords is that humans are better at 
memorizing graphical passwords than alphanumeric character passwords, the existing user 
studies are very limited and there is not yet convincing the fact to support this argument. 
We have found that the  existing recognition base graphical passwords schemes does not 
have attractive usability features for the users which mean that the usability features 
needed to be studied more and develop more usable  systems for the Graphical Password. 
In this paper a collection of usability features has been derived to be implemented in the 
graphical password systems to be more usable for the users. These features are the ease of 
use and creation which means that the use of the system to choose the password can be 
done easily and the creation of the password is easy also that indicate that the password 
chosen can be done without any complication such as using the mouse or  keyboard to 
choose the password and navigation throw the system easily done and the user should be 
satisfied by using the new system to create the password, where the second feature is the 
ease to memory or to memorize the password that the user have chosen which mean that 
the user can easily memorize the faces or pictures used as password , furthermore the other 
feature is the learnability which mean that the system can easily learn how to use and how 
to explore it to get more help or use it in full mode and the last feature is the design ac-
ceptable which indicate that the system has an accepted views to the user. 
7   Conclusion 
A collection of usability features has been derived as a future usability features should 
be implemented in any new usable graphical password system. By mapping between 
the recognition base graphical password methods and the usability features and sub 
features for the existing recognition base graphical password methods, general  
 
Table 1. Derived usability features 
Features 
Features elements 
Easy to use 
Login time Use mouse 
 
Use keyboard 
Simple built 
Easy 
navigation 
Easy to create 
Login time Choose 
pictures 
Choose photos 
Simple process 
Easy to memorize 
Colored 
pictures 
Limited 
pictures 
Category 
(sport-natural- 
human) 
 
Easy to learn 
Simple 
view 
Some hints 
Easy to go 
back 
 

388 
A.M. Eljetlawi and N. Ithnin 
 
features and ISO features and by study, extract the proper features from the compari-
son studies and previous studies and surveys the new features advised to be built in 
the new graphical password system will be illustrated in the table 1. 
References 
[1] Sobrado, L., Birget, J.: Graphical Passwords, Rutgers University, Camden New Jersey. 
The Rutgers Scholar, An Electronic Bulletin of Undergraduate Research, vol. 4 (2002) 
(Accessed on June 2007) 
[2] Gilhooly, K.: Biometrics: Getting Back to Business. Computerworld (May 2005) 
[3] Dhamija, R., Perrig, A.: Déjà vu: A User Study Using Images for Authentication. In: Pro-
ceedings of the 9th USENIX Security Symposium (August 2000) 
[4] Shepard, R.N.: Recognition memory for words, sentences, and pictures. Journal of Verbal 
Learning and Verbal Behavior 6, 156–163 (1967) 
[5] Real User Corporation, PassfacesTM, http://www.realuser.com (Accessed on 
June 2007) 
[6] ISO-International 
Organization 
for 
Standardization, 
http://www.iso.org  
(Accessed on May 2007) 
[7] Mohamed, A.H.N.: Graphical password: Security and Usability Issues. In: UTM seminar 
(July 3-4, 2007) 
[8] Dhamija, R., Perrig, A.: Deja Vu: A User Study Using Images for Authentication. In: 
Proceedings of 9th USENIX Security Symposium (2000) 
[9] Man, S., Hong, D., Mathews, M.: A shoulder-surfing resistant graphical Password 
scheme. In: Proceedings of International conference on security and management, Las 
Vegas, NV (2003) 
[10] Jansen, W., Gavrila, S., Korolev, V., Ayers, R., Swanstrom, R.: Picture Password: A Vis-
ual Login Technique for Mobile Devices. NIST Report NISTIR 7030 (2003) 
[11] Takada, T., Koike, H.: Awase-E: Image-based Authentication for Mobile Phones using 
User’s Favorite Images. In: Chittaro, L. (ed.) Mobile HCI 2003. LNCS, vol. 2795,  
pp. 347–351. Springer, Heidelberg (2003) 
[12] Brostoff, S., Sasse, M.A.: Are Passfaces more usable than passwords: A Field Trial In-
vestigation. In: People and Computers XIV – Usability or Else: Proceedings of HCI. 
Springer, Sunderland (2000) 
[13] Davis, D., Monrose, F., Reiter, M.K.: On User Choice in Graphical Password Schemes. 
In: Proceedings of the 13th USENIX Security Symposium, San Diego, California (2004) 
[14] Wiedenbeck, S., Waters, J., Birget, J.C., Brodskiy, A., Memon, N.: Authentication Using 
Graphical Passwords: Basic Results. In: Human-Computer Interaction International 
(HCII 2005), Las Vegas, NV (2005) 

A Bilinear Pairing Based Hidden-Signature
Scheme
Mohamed Rasslan1 and Amr Youssef2
1 Electrical and Computer Engineering Department
2 Concordia Institute for Information Systems Engineering
Concordia University
Montreal, Quebec, H3G 1M8, Canada
m rassla@encs.concordia.ca, youssef@ciise.concordia.ca
Abstract. In this paper we propose a bilinear pairing based hidden
blind signature scheme. The proposed scheme allows the signer to ap-
pend information to the hidden signed message. The requester cannot
modify neither this information nor the signed message. This added in-
formation can be used to stamp the signature with a certain date and/or
other relevant information which is an essential requirement in applica-
tions such as notary services and patent time proof. After issuing the
signature by the signer, the requester can verify that the signature has
the designated date and place. The security of the proposed scheme is
analyzed against diﬀerent cryptanalytic attacks.
1
Introduction
Blind signature schemes, ﬁrst introduced by Chaum [1,2], are the core of many
applications such as e-coins and e-voting. The original Chaum’s scheme has three
active parties: the message’s author, also referred to as the requester since she
requests the signature on the message, the signer, and the veriﬁer who ensures
that the signature was issued by the signer. The blind signature is composed of
four ordered algorithms: blinding, signing, unblinding, and verifying. The scheme
is initiated by the requester (author) who blinds the message and then sends this
blinded version to a singer who signs it and sends the signed blinded message
back to the requester. Then, the requester unblinds the message. The result is
a digital signature of the original message. In Chaum’s scheme the resultant
message-signature pair is unlinkable. In particular, in the unblinding phase, the
requester modiﬁes the signature and produces a version that is unlinkable to the
one generated by the signer. Hence, the signer has no way to resume this link
again even after revealing the message. Some applicators ﬁnds this unlinkability
undesirable but at the same time, they are interested in the blinding of the
message.
Horster et al. [3,4,5] classiﬁed blind signature schemes into four classes: the
hidden, the weak, the interactive and the strong blind signatures. In hidden blind
signatures, the signer does not know the message to be signed but she knows the
signature parameters. The signer has the chance to store these parameters and
A. ¨Ozcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 389–397, 2010.
c
⃝Springer-Verlag Berlin Heidelberg 2010

390
M. Rasslan and A. Youssef
can recognize the signature later by comparing them with a given signature. In
weak blind signatures, the signer does not know either the message to be signed
nor the signature parameters. But, after revealing the message, she can easily
link her signature to that message. The reason behind the signer’s ability to rec-
ognize the message-signature pair is the existence of a relationship between the
blinded signature parameters and the unblinded parameters. Interactive blind
signatures are similar to weak blind signatures in their generation, except that
interactive blind signatures use interactive proof to demonstrate the knowledge
of a signature and hence the signer does not have the ability to link the blinded
and the unblinded signature parameters. Consequently, the signer cannot link
a given message to his stored parameters. Finally, the strong blind signature
grants full anonymity and the signer cannot link the stored parameters and the
public signature parameters.
Camenisch et al. [6] proposed the ﬁrst blind signatures based on the discrete
logarithm problem. Subsequently, in 1995, Harn [7] proved that unlinkability
(untraceability) cannot be achieved in Camenisch et. al.’s scheme. But, according
to Patrick et al. [3,5], Camenisch et al.’s scheme can be classiﬁed as a hidden
signature. Also, Patrick et al. in [4,5] introduced a hidden signature protocol
that is based on the ideas of the testimonial scheme [8] and the Meta-ElGamal
signature scheme [9].
In this paper we propose a bilinear pairing based hidden blind signature
scheme that enables the signer to insert a piece of temporal or spatial informa-
tion in the signature. The requester, while being able to verify the correctness
of the inserted information, cannot tamper with it. The proposed scheme has
several applications such as notary services, testament applications and patent
time proof.
2
Preliminary
An abstract understanding of bilinear mapping requires knowledge of Gap Diﬃe-
Hellman groups and bilinear groups. Gap Diﬃe-Hellman groups are created
from disjointing computational and decisional Diﬃe-Hellman problems. Bilinear
groups are based on the existence of a bilinear map. Let G be an additive cyclic
group of prime order p, and P is its generator. In this group, the well-known
Diﬃe-Hellman problems carry on as follows [10,11,12].
2.1
Diﬃe-Hellman Problems
Computational Diﬃe-Hellman (CDH): Given P, aP, Q ∈G, compute aQ ∈G.
An algorithm that solves the computational Diﬃe-Hellman problem is a prob-
abilistic polynomial time Turing machine,that on input P, aP, Q, outputs aQ
with non-negligible probability. The Computational Diﬃe-Hellman assumption
means that there is no such a probabilistic polynomial time Turing machine.
This assumption is believed to be true for many cyclic groups, such as the prime
subgroup of the multiplicative group of ﬁnite ﬁelds [13].

A Bilinear Pairing Based Hidden-Signature Scheme
391
Decisional Diﬃe-Hellman (DDH): Given P, aP, Q, bQ ∈G, decide whether
a equals b. Quadruples of this form (P, aP, Q, bQ) are named Diﬃe-Hellman
quadruples.
Inverse Computational Diﬃe-Hellman problem (InvCDH): Given P, xP, output
1
xP where 1
x denotes the multiplicative inverse of x ∈Z∗
p.
In [14], Bao et al. study various computational and decisional Diﬃe-Hellman
problems by providing reductions between them in a high granularity setting.
They considered the variations of Diﬃe-Hellman problems deﬁned over some
cyclic group with an explicit group structures. One variant of these compu-
tational Diﬃe-Hellman problem is the Inverse Computational Diﬃe-Hellman
assumption. They showed that this variation of computational Diﬃe-Hellman
problem is equivalent to the classic computational Diﬃe-Hellman problem if the
order of an underlying cyclic group is a large prime.
Gap Diﬃe-Hellman Groups (GDH): GDH are examples of gap problems pre-
sented in [15]. There are many subgroups of group Z∗
q that have prime orders,
and both the CDH and DDH assumptions are believed to be held over these
groups. The subgroup G with the prime order p is one of these. However, on
certain elliptic-curve groups, the DDH problem is easy to solve, whereas CDH
is believed to be hard [11]. Such groups are named Gap Diﬃe-Hellman (GDH)
groups. Hence, if G belongs to these speciﬁc elliptic-curve groups, we call it a
Gap Diﬃe-Hellman group.
2.2
Bilinear Maps
Bilinear groups. Until now, there is no known implementable example of GDH
groups except bilinear maps. A bilinear group is any group that possesses such
a map e, and on which CDH is hard.
Bilinear maps. Assume that G is an additive group and GT is a multiplicative
group such that |G| = |GT | = |p|, where p is a prime number. P is the generator
of G. Then, the map e : G × G →GT is a computable bilinear map if it satisﬁes:
1) Computability : There is an eﬃcient algorithm to
compute e (P , Q) for all P , Q ∈G .
2) Bilinearity : for all P , Q ∈G and a, b ∈Z,
we have e (aP , bQ) = e (P , Q)ab .
3) Non −Degeneracy : e (P , P) ̸= 1. In other words,
if P is a generator of G, then e (P, P) generates GT .
Bilinear Diﬃe-Hellman Problem. The group G is a subgroup of the additive
group of points of an elliptic curve E (Fq). The group GT is a subgroup of the
multiplicative group of ﬁnite ﬁeld F ∗
q and |G| = |GT | = |p|, where p is a prime
number. Let e : G × G →GT be a bilinear pairing on (G, GT ). The bilinear
Diﬃe-Hellman problem (BDHP) is the following: Given P, aP, bP, cP, compute
e(P, P)abc.

392
M. Rasslan and A. Youssef
Typically, the mapping e : G × G →GT will be derived from either the Weil
or the Tate pairing on an elliptic curve over a ﬁnite ﬁeld. More comprehensive
details on GDH groups, bilinear pairings, and other parameters are deﬁned in
[16,17,18,19,20].
3
Proposed Scheme
The proposed hidden digital signature scheme involves three parties, the re-
quester (R), the signer (S), and the veriﬁer (V). It is based on two protocols,
the signing protocol and the veriﬁcation protocol. The signing protocol runs two
algorithms, the ﬁrst one is the blinding algorithm which is executed by R (the
author of the message) and the other is the signing algorithm, executed in three
steps by S. We assume that there is a trusted authority who establishes and
manages the setup of the public key cryptosystem. To implement our scheme,
we ﬁrst need a security parameter that deﬁnes the level of bit strength that the
signature will provide. We then need to deﬁne groups G and GT and a pairing
e : G × G →GT . To do this we choose an elliptic curve E (Fq) with embed-
ding degree k, where q is a prime power and it is the order of the ﬁnite ﬁeld
Fq. Also, p is a prime such that p|#E (Fq) where #E (Fq) is the order of the
group E (Fq), which is the number of points on an elliptic curve E over a ﬁeld
F, including the point at inﬁnity. We then randomly pick a point P ∈E (Fq) , P
is a point of order p in E (Fq) and is called p-torsion point of the curve E. Let
P be the generator of the group G and e(P, P) the generator of the group GT ,
which are cyclic groups of order p. G is a cyclic subgroup of E (Fq) and GT is
a cyclic subgroup of F ∗
qk. Our scheme utilizes two cryptographic hash functions
H : {0, 1}∗→G and h : {0, 1}∗→Z∗
p, the security analysis will treat H and h
as random oracles. Let M be the message and m = h (M) . The signer’s secret
key is x ∈Z∗
p and its public key is Q = xP ∈G.
3.1
Signing Protocol
The signing protocol runs two algorithms, the blinding algorithm by the re-
quester R and the signing algorithm by S.
Blinding Algorithm. Throughout this step, the requester aims to get the
signer’s signature without disclosing the message content. At the same time,
the requester wants to make sure that the signer is the designated recipient of
the blinded message. We can achieve this through double blinding the message
by putting two locks on it. The ﬁrst lock serves to blind the message from the
signer. The second one is designated to the signer; she is the only one who can
unlock it. These two locks can be done in one single mathematical operation;
the requester will calculate r = mQ and sends it to the signer. Consequently,
the signer is the only one who can calculate m · P by using the multiplicative
inverse of her secret key (the ﬁrst lock), r = m · Q = m · x · P. The result of this
operation is still blind with respect to the signer’s view and she needs to solve
the discrete logarithm problem, to get m out of m · P.

A Bilinear Pairing Based Hidden-Signature Scheme
393
Signing Algorithm. Step 1: The signer receives r = m · Q, and then she
calculates r
′as follows:
r
′ = 1
x · r
= 1
x · m · Q
= 1
x · m · x · P
= m · P
Step 2: The signer generates the signature parameter z =< nounce||date||place >
and then calculates H (z) .
Step 3: The signer generates the signature (s, z) such that s =

H (z) + r
′
·
1
x = H(z) + r
′
x
.
3.2
Veriﬁcation Protocol
After revealing the message M, the signature can be publicly veriﬁed by any
veriﬁer V, using the bilinear pairing, by verifying that
e (s, Q)
?= e (H (z) + h (M) · P, P )
The correctness of the proposed scheme is proved by noting that
e (s, Q) = e

H(z) + r
′
x
, x · P

= e

H(z) + m·P
x
, x · P

= e (H (z) + m · P, P )
1
x · x
= e (H (z) + h (M) · P, P )
4
Security Analysis
A blind digital signature scheme is secure if it satisﬁes both the blindness and
the non-forgeability properties [21]. In what follows, we show how the blindness
property is achieved in our scheme and then we show how our scheme resits
diﬀerent forgery attacks.
4.1
Blindness
In the proposed scheme, the blindness algorithm depends on the hardness of the
discrete logarithm problem in a group deﬁned over an elliptic curve and there
is no known algorithm that enables the eﬃcient computation of discrete logs in
this setup. Hence, from the signer’s point of view, calculating m from mP is a
hard problem and it is equivalent to solving a discrete logarithm problem in a
group deﬁned over an elliptic curve. On the other hand, the adversary has to
calculate m from mQ which is a hard problem and is equivalent to solving a
discrete logarithm problem in a group deﬁned over an elliptic curve.

394
M. Rasslan and A. Youssef
q
: prime power, the order of the finite field Fq
E (Fq)
: elliptic curve has embedding degree k
p
: prime, p |#E (Fq)
G
: additive cyclic subgroup of E (Fq)
P
: point on the elliptic curve and the group generator of G
GT
: multiplicative cyclic subgroup of F ∗
q
e
: pairing, e : G × G →GT
e(P, P) : the group generator of GT
Q
: the signer′s public key, point on the elliptic curve Q = x · P ∈G
H
: cryptographic hash function H : {0, 1}∗→G
h
: cryptographic hash function h : {0, 1}∗→Z∗
p
Requester R
Signer S
M : message
x : signer′s secret key x ∈Z∗
p
Computes r = m · Q
r = m·Q
−→
r
′= 1
x · r
= 1
x · m · Q
= 1
x · m · x · P
= m · P
z =< nounce||date||place >
s =

H (z) + r
′
· 1
x = H(z) + r
′
x
(s, z)
←−

s = H(z) + r
′
x
,
z

Fig. 1. Signing
Protocol
Message Holder H
V erifier V
(s, z) , M
(s, z), M
−→
e (s, Q)
?=
e (H (z) + m · P, P)
Fig. 2. Veriﬁcation
Protocol
4.2
Non-forgeability
In the signing algorithm, the signature is constructed by calculating a point on
the elliptic curve E (Fq). In this operation, the “s” part of the signature is the
result of multiplying the term

H(z) + r
′ 
by 1
x. Note that

H(z) + r
′ 
is a
point on the elliptic curve E (Fq) and the term

H(z) + r
′ 
x
is also a point on the
elliptic curve E (Fq). The only one who can perform this operation is the signer,
who is the owner of the secret x. The Inverse Computational Diﬃe-Hellman
assumption (InvCDH) guarantees that the adversary cannot ﬁnd 1
xP. This pre-
vents the adversary from generating a signature on a chosen message/stamp.

A Bilinear Pairing Based Hidden-Signature Scheme
395
Theorem 1: If The veriﬁer will accept a random s
′ as a valid signature for
H (z) + r
′ with negligible probability 1/p.
Proof: Since any group of prime order is cyclic, it follows that subgroup G is
isomorphic to Zp.
Theorem 2: Given a message m1, its corresponded signature (s1, z1) and a cho-
sen stamp z2 ̸= z1, then ﬁnding any m2 ̸= m1, that satisﬁes s1 = (H(z2) + m2P )
x
is equivalent to solving the DLP in a group deﬁned over the elliptic curve E (Fq).
Proof: Assume that s1 = (H(z1) + m1P )
x
= (H(z2) + m2P )
x
. Thus (H (z1) + m1P)
= (H (z2) + m2P) and hence
m2P = (H (z1) −H (z2) + m1P).
Thus recovering m2 is equivalent to solving the above DL problem.
Theorem 3: Given a message m1, its corresponded signature (s1, z1) and a
chosen message m2 ̸= m1, then ﬁnding any random stamp z2 ̸= z1, that satisﬁes
s1 = (H(z2) + m2P )
x
is equivalence to breaking the one-wayness property of the
hash function H.
Proof: Assume that s1 = (H(z1) + m1P )
x
= (H(z2) + m2P )
x
, hence (H (z1) + m1P) =
(H (z2) + m2P) and
H (z2) = m1P −m2P + H (z1) .
Consequently, ﬁnding z2 is equivalent to breaking the one-wayness property of
the hash function H.
5
Conclusion
We proposed a bilinear pairing based hidden blind signature scheme that allows
the signer to append information to the hidden signed message. The proposed
scheme has applications in notary services and patent time proof. It should be
noted that, in our scheme, it is still possible to check that the notary has signed
the testament even if the signature scheme has been broken in the meantime.
This can be achieved by asking the notary to look in the list of signature pa-
rameters and compare the given signature parameters with the stored ones.
References
1. Chaum, D.: Blind signatures for untraceable payments. In: Advances in Cryptology
- Crypto 1982, pp. 199–203. Springer, Heidelberg (1983)
2. Chaum, D.: Security without identiﬁcation: transaction systems to make big
brother obsolete. Communications of the ACM 28(10), 1030–1044 (1985)

396
M. Rasslan and A. Youssef
3. Horster, P., Petersen, H.: Classiﬁcation of blind signature schemes and examples
of hidden and weak blind signatures. Presented at the Rump Session of Eurocrypt
1994, Technical Report TR-94-1, May 1994, Theoretical Computer Science and
Information Security, Department of Computer Science, University of Technology
Chemnitz-Zwickau, Germany, Perugia, Italy, May 1994, p. 6 (1994)
4. Horster, P., Michels, M., Petersen, H.: Hidden signature schemes based on the
discrete logarithm problem and related concepts, Technical Report TR-94-40-R,
Theoretical Computer Science and Information Security, Department of Computer
Science, University of Technology Chemnitz-Zwickau, Germany (April 1995)
5. Horster, P., Michels, M., Petersen, H., Petersen, H.: Meta-Message recovery and
Meta-Blind signature schemes based on the discrete logarithm problem and their
applications. In: Safavi-Naini, R., Pieprzyk, J.P. (eds.) ASIACRYPT 1994. LNCS,
vol. 917, pp. 224–237. Springer, Heidelberg (1995)
6. Camenisch, J., Piveteau, J., Stadler, M.: Blind signatures based on discrete loga-
rithm problem. In: De Santis, A. (ed.) EUROCRYPT 1994. LNCS, vol. 950, pp.
428–432. Springer, Heidelberg (1995)
7. Han, L.: Cryptanalysis of the blind signatures based on the discrete logarithm
problem. IEE Electronic Letters, 1136–1137 (1995)
8. Horster, P., Knobloch, H.-J.: Discrete Logarithm based protocols. In: Davies, D.W.
(ed.) EUROCRYPT 1991. LNCS, vol. 547, pp. 399–408. Springer, Heidelberg
(1991)
9. Horster, P., Michels, M., Petersen, H.: Meta-ElGamal signature schemes. In:
Proceedings 2nd ACM conference on Computer and Communications security,
Fairfax, Virginia, November 2-4, pp. 96–107 (1994)
10. Boneh, D., Franklin, M.: Identity-based encryption from the Weil pairing. In:
Kilian, J. (ed.) CRYPTO 2001. LNCS, vol. 2139, pp. 213–229. Springer, Heidelberg
(2001)
11. Boneh, D., Lynn, B., Shacham, H.: Short Signatures from the Weil Pairing.
In: Boyd, C. (ed.) ASIACRYPT 2001. LNCS, vol. 2248, pp. 514–532. Springer,
Heidelberg (2001)
12. Pﬁtzmann, B., Sadeghi, A.: Anonymous ﬁngerprint with direct non-repudiation.
In: Okamoto, T. (ed.) ASIACRYPT 2000. LNCS, vol. 1976, pp. 401–414. Springer,
Heidelberg (2000); International Association for Cryptologic Research
13. Diﬃe, W., Hellman, M.: New directions in cryptography. IEEE Transactions on
Information Theory, IT No. 2(6), 644–654 (1976)
14. Bao, F., Deng, R., Zhu, H.: Variations of Diﬃe-Hellman Problem. In: Qing, S.,
Gollmann, D., Zhou, J. (eds.) ICICS 2003. LNCS, vol. 2836, pp. 301–312. Springer,
Heidelberg (2003)
15. Okamoto, T., Pointcheval, D.: The gap problems: A new class of problems for
the security of cryptographic primitives. In: Kim, K.-c. (ed.) PKC 2001. LNCS,
vol. 1992, pp. 104–118. Springer, Heidelberg (2001)
16. Martin, L.: Introduction to Identity-Based Encryption. Information Security and
Privacy Series, ch. 3. Artech House, INC. (2008)
17. Joux, A., Nguyen, K.: Separating decision Diﬃe-Hellman from Diﬃe-Hellman in
cryptographic groups. Journal of Cryptology 16, 239–247 (2003)
18. Frey, G., Muller, M., Ruck, H.: The Tate pairing and the discrete logarithm applied
to elliptic curve cryptosystems. IEEE Transactions on Information Theory 45(5),
1717–1719 (1999)

A Bilinear Pairing Based Hidden-Signature Scheme
397
19. Galbraith, S., Harrison, K., Soldera, D.: Implementing the tate pairing. In: Fieker,
C., Kohel, D.R. (eds.) ANTS 2002. LNCS, vol. 2369, pp. 324–337. Springer,
Heidelberg (2002)
20. Gaudry, P., Hess, F., Smart, N.: Constructive and destructive facets of Weil descent
on elliptic curves. Journal of Cryptology 15(1), 19–46 (2002)
21. Juels, A., Luby, M., Ostrovsky, R.: Security of blind digital signatures. In: Kaliski
Jr., B.S. (ed.) CRYPTO 1997. LNCS, vol. 1294, pp. 150–164. Springer, Heidelberg
(1997)

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 398–408, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
An X.509 Based Licensed Digital Signature Framework 
for Hierarchical Organizations∗ 
Alper Ugur and Ibrahim Sogukpinar 
Network and Information Security Lab., Gebze Institute of Technology, Kocaeli, Turkey 
{augur,ispinar}@bilmuh.gyte.edu.tr 
Abstract. Digital signatures are used for integrity of the signed media and au-
thentication of the signers in digital environment. Beyond these the signature 
might provide an authorization service for a verifier to check whether the signer 
has license to sign the document in workflow. This verification is important in 
case of exchanging valuable documents between different organizational levels 
and also crucial in digital document archive. In this work, the necessity of sig-
nature authorization was described and also implementation of the authorization 
with X.509 based templates in hierarchical organizations was expressed. Also, a 
license structure for digital signature and a framework where signatures realize 
authorization of the signers on a document in hierarchical organization structure 
were proposed.  
Keywords: digital signature, hierarchical authorization. 
1   Introduction 
The security mechanisms as authentication, nonrepudation and integrity control of the 
documents in organizations are commonly handled with digital signatures. Authoriza-
tion is another crucial mechanism in a hierarchical document workflow. The digital 
signatures have deficiency to provide authorization of the signer with his/her signa-
ture on the document. Identifying digital signature with an authorization information 
would made it complete by means of security mechanisms which are required in a 
hierarchical document workflow. 
It is a question that if we really need an extra effort for this “completeness” of digi-
tal signature. The truth is behind the curtain that answers of the following questions 
would reveal it. 
What makes a signature valid in a hierarchical organization? Is the traditional veri-
fication still legitimate in case of hierarchical document workflow? As in traditional 
verification, would it be appropriate to prove the validity of the document if we just 
verify the signature? Is there any difference between verifiable signature and author-
ized verifiable signature on a document? 
It is common that any signer could sign and approve a digital document with his 
legal and valid secret key. Digital signature applications work with best effort; take a 
                                                           
∗ Supported by The Scientific and Technological Research Council of Turkey (TÜBİTAK), The 
Support Program for Scientific and Technological Research Projects (No. 108E132). 

An X.509 Based Licensed Digital Signature Framework for Hierarchical Organizations 
399 
 
digital document and a private signature key and output a signed digital document 
which is verifiable with the corresponding public key. But in hierarchical organiza-
tions this effort may become meaningless as part of authorization. 
Considering hierarchical structure of a bank; one department evaluates the condi-
tion of the customers and grants credit. As all documents are in digital environment 
someone could take an evaluation document and sign it using his/her own private key, 
then effortlessly create a signed evaluation document. The signature is valid because 
it is easily verifiable with his/her public key.  
But the fact is only the signature is valid; it is invalid as an authorization manner 
because merely the assigned employee has license to sign the approval document in 
the process. 
Also an authorized person could be promoted to another/upper level/class in organ-
izational hierarchy and this promotion expands his/her previous approval rights on 
documents. A credit evaluator may become chief of the department who grants cred-
its. The promoted employee’s prior valid and verifiable signature on the evaluation 
paper now would mean grant to a credit. So, signature of the successor -the new au-
thorized person- must be declared to the hierarchical structure, as it is valid and veri-
fiable on these level documents and he/she is authorized. And also it must be ensured 
that if there was a predecessor, he/she could not sign the documents anymore with the 
corresponding level of authorization. 
At the end of the authorization time interval -if the assignment is not extended- 
there must be a prevention mechanism to detect the old licensed signature as invalid if 
it was established after the interval. As in our example case, the chief might be as-
signed to official duty for limited period. Hence his/her grants or rejections would be 
valid for the duration only. His/her signature should be invalid if his/her term of office 
would not be extended.  
In literature there are many works dealing with authentication of the signers, proxy 
signatures or delegation of the signatures[1,2,3,4,5,6,7,8 ]. The important gap seems 
to be most of them were related with personal authorization problem and verification 
of enrolling personal privileges. 
In 2009 Ugur, Sogukpinar [9] have addressed the authorization of organizational 
signature and proposed a framework to authorize signatures by embedding authoriza-
tion information called “license” to the signature or adjusting the signature key with 
the license as a self-authorized signature based on pairing based cryptography.  
This paper has focused on implementation of licensed digital signature framework 
based on X.509. The structure of X.509 will provide flexibility of implementation as 
authorization information is also a certification in the framework.  
Unlike mentioned work the license -the authorization information- improved to en-
capsulate hierarchical level information which assists sequential multi-signatures with 
parsing different level signatures on a document. Also committed authorization tags 
on documents to improve verification of licensed signatures in the framework. 
This paper is organized as follows; related works are introduced in the second sec-
tion. In the third section, the authorization framework has been presented for licensed 
signature based on X.509. In the fourth section, the framework has been analyzed. 
And the last section is conclusion. 

400 
A. Ugur and I. Sogukpinar 
 
2   Related Works 
In previous section, the authorization problems of signature in a hierarchical organiza-
tion were discussed. An implementation of licensed digital signature framework based 
on X.509 for authorization of signatures on a digital document in hierarchical organi-
zations was proposed.  
In literature, many works were presented on proxy signatures [1, 2, 3] or delegated 
signatures [4, 5, 6, 7, 8] deal with signatures with authorization. The schemes are 
based on the sign-on behalf problem. However this paper focused on the possessed 
organizational signature authorizations not assignment of personal signature authori-
zations in those works. Possessing of multiple signature authorizations which is inevi-
table in hierarchical organizations is also covered. 
Many works also deal with information about the signer. Self-certified, identity 
based and hierarchical identity based schemes were presented by many researchers 
[10, 11, 12, 13, 14, 15]. But this information is limited with signers’ personal specific 
identifications; IDs such as name, mail address etc. used to verify the person. And all 
did not project the information in organizational sense. 
In 2007, Lui et al [16] extended the mentioned information concept with SPKI as 
5-tuple delegation information. The 5-tuple consists of delegator and issuers public 
keys, validity time, re-delegation information and authorization subject.  
In the same period, Ugur and Sogukpinar [17] have proposed a hierarchical signa-
ture scheme with organizational authorization information as clearance, level and 
authorization subject. They presented usage of organizational information that differs 
from personal information. Wang et al. [15] presented same subject as a role-based 
access control and delegation with hierarchical identity based signatures. 
In 2009 Ugur and Sogukpinar [9] reconstructed their prior work on authorized sig-
natures as a new term called licensed digital signature and they presented a frame-
work for licensed digital signatures. The framework was based on combining signed 
document with organizational authorization information called license and the pro-
posed signature schemes to achieve the mentioned operation. The framework was 
intended to solve authorization weakness of digital signatures in general.  
The proposed license structure and authorization model has its own structure. 
However, if the license could be adapted to X.509 PKI standards, the framework 
would find a wide usage area as employed X.509 applications. 
X.509 [18] is part of X.500 series of recommendation that define a directory ser-
vice, and defines a framework for provision of authentication services by the X.509 
directory to its users. The directory maintains a database of attributes and information 
about its users.  
X.509 specification defined and standardized a general, flexible certificate format 
[19, 20]. X.509 based on certificates associated to the users in directory. Each certifi-
cate placed in directory is created by Certificate Authorities. This framework also 
suggests a profile for authorization based on X.509 Attribute Certificates [22]. 
In the following section, the framework for licensed signatures based on X.509 has 
been presented. 

An X.509 Based Licensed Digital Signature Framework for Hierarchical Organizations 
401 
 
3   Framework for Authorized Signature 
The framework in [9] consists of three foundations: License Structure where the au-
thorization information encapsulated, Signature Schemes where the licenses associ-
ated with signatures, and Authorization Authority Model where the generation and 
validation of licenses are provided. The mentioned framework was re-formed as to be 
easily adapted to X.500 directories and X.509 certificate structures in hierarchical 
organizations in this paper. 
3.1   License Structure 
License structure in [9] was used as organizational authorization information about 
signers. The license structure is also given in Fig.1 as a reference of the proposed 
adaption to X.509 based implementations. The structure consists of 4 main parts and 
each part has 2 fields, and the fields are given briefly as: 
HEADER consists of Subject and Validity Interval fields. Subject defines authori-
zation subject, and Validity Interval defines validity of defined authorization subject 
with the boundaries not before and not after. 
AUTHORIZATION ENTITIES consist of directly involved parties creation and us-
age of license. Possessor is a pointer to the owner of the authorization subject with 
this license. Designator defines who creates this license. Designator determines the 
possessor and validity interval.  
APPROVAL consists of validity control fields of the license. Approval permission 
is the signature validates the authorization subject and all three fields given before 
as precaution for forgery or undeniability. Approval Entity is the entity on superior 
level of Designator or organizational authorization authority where verification of 
the licensed signature created. 
AUTHORIZED SIGNATURE DATA consist of a signature timestamp and a re-
served field. Signature Timestamp carries the generation time of authorized signa-
ture by the possessor. Reserved field is reserved for future extensions. 
Two different approaches have been proposed to revise the structure. As mentioned in 
authorization profile RFC [22], extension field of a X.509 certificate could be used to 
encapsulate authorization information. It could be proposed that if the extensions field 
of traditional X.509 format is used for authorization subject and authorization validity 
intervals, it would complete the license structure as a X.509 certificate. The revised 
design of the license structure as a X.509 certificate is given in Fig. 2.  
 
Ai : Authorization subject 
T-Ai : Validity Interval of Ai  
PUi  : public key , IDi 
PUj  : public key, IDj 
PUAA  : public key, IDAA 
S = EPRAA(Ai, T-Ai, PUi, PUj, PUAA) 
AA:  License approval/verification authority 
tA : timestamp of licensed signature  
-Reserved for future development- 
Fig. 1. License of an authorized digital signature in a hierarchical structure [9] 
 

402 
A. Ugur and I. Sogukpinar 
 
X.509 
Licence Structure 
on X.509 
Version 
As in X.509 
Certificate serial number 
As in  X.509 
Signature Algorithm Identifier 
Algorithm 
Parameters 
As in  X.509 
Issuer Name 
AA or j 
Period of Validity  
Not before 
Not after 
T-Ai 
Subject Name 
i 
Subject Public-key Info  
Algorithms 
Parameters 
As in  X.509 
Key 
PUi 
Issuer Unique Identifier 
IDAA or IDj 
Subjects Unique Identifier 
IDi 
Extensions 
Ai 
 
tA 
Signature 
S 
Algorithms 
Parameters 
Encrypted 
As in X.509 
Fig. 2. License of an authorized digital signature in a hierarchical structure as a X.509 certificate 
Term 
in 
X.509 
AC 
(RFC3281) 
  Field in license structure 
Attribute Certificate 
License 
Attribute Authority 
AA : Hierarchical license approval/ 
verification authority of organization 
Issuer  
AA or designator IDj 
holder  
Possessor IDi 
Attribute 
Authorization subject Ai 
attributeCertValidityPeriod
T-Ai :Interval of Authorization(Ai) 
Validity  
signatureValue 
S 
Extension 
tA :timestamp of license signature 
Fig. 3. License Structure Fields in Attribute Certificate Profile 
 

An X.509 Based Licensed Digital Signature Framework for Hierarchical Organizations 
403 
 
This structure must be used as only for license for authorization information even 
though encapsulating approved public key and identifier of the signer. The public key 
must be certified apart from this license. License must be used only to bind authoriza-
tion information to signer to create authorized digital signature for security reasons. 
A second approach could be proposed for revision of license structure according to 
X.509 based framework implementations as attribute certificate utilization. The Attrib-
ute certificates were designed to bind authorization to certified identity information [22].  
The corresponding terms of attribute certificate (AC) to the license structure in the 
proposed framework is given in Fig. 3.  
Similar to the previous approach AC must be created individual form public key 
certificate but also has a bound to the public key of the signer. All the fields given in 
Fig. 3 (extensions included) must be defined as “critical” to satisfy verifiability and 
undeniability. 
3.2   Signature Schemes 
Licensed digital signatures were based on associating authorization information and 
signature. The block structure of authorization information called license is defined in 
preceding section. In this section integration of license and digital signatures were 
presented. 
Two optional schemes can be implemented in integration process. One of them 
combines the license with the message in signing phase and the other scheme is based 
on embedding the license to the signature key. License would be the revised authori-
zation information that defined before as X.509 certificate according to license struc-
ture or an attribute certificate to compromise the implementations based on X.509 
frameworks. 
3.2.1   License Integration Scheme the license is encapsulated with the message. The 
signature generation phase is similar to traditional digital signature schemes. While 
Skli( ) denotes generated signature with the kli key of signer li and L denotes the li-
cense; resulting digital signature will be [ Skli ( m||L )|| m||L] . 
Verification of the licensed signature differs from traditional one. Skli ( m||L ) is 
verified with the pair of kli as usual in public-key or other cryptographic signatures 
schemes. Also, the license must be verified by Authorization Authority to complete 
the verification process. tsAli field in L must be taken into consideration to satisfy 
validation. 
3.2.2   Licensed Signature Key Scheme the license is embedded with the key of signer 
to obtain a licensed signature key. Licensed signature key generation phase is presented 
as follows: 
While SkAli( ) denotes generated signature with the kAli licensed signature key of 
signer li and L denotes the license; resulting digital signature will be; 
[ SkAli ( m)|| m||L] where kAli =(kli,L). 
Verification of L with the help of Authorization Authority is similar to previous 
scheme. This scheme differs from the preceding one in key generation and signature 
verification phases.  

404 
A. Ugur and I. Sogukpinar 
 
Pairing based cryptography scheme in signature algorithm could be used to gener-
ate licensed signature key, the notation and parameters would be as in [10,12]. 
The parameters of the scheme are: 
 
Skli             
: signature of li in organization  
L 
           
: license, information of hierarchical authorization 
SkAli (Skli,L)  :Licensed signature 
(Sli-pub, Sli-prv) : the private and public keys of li 
P 
   
: generator of gap Diffie-Hellman group G 
n 
   
: n Є Z*q  nonce 
m 
   
: message 
H1:{0,1}*->G,H2:{0,1}* X G->Z*q: cryptographic hash functions 
 
Signature key generation is determined with operations in (2) with the variables 
(generator and private key) defined in (1): 
P Є G, e(P,P)=1;Sli-prv Є Z*
q;                                           (1) 
Sli-pub = Sli-prv.P and Skli =Sli-prv.H1(L).                                  (2) 
Licensed digital signature is built up with the licensed signature key, the message and 
nonce generated in this phase. The operations are given in (3) and the signature will 
be (SkAli,R): 
R=n.H1(L), r =H2(m,R) and  SkAli=(n+r).Skli.                            (3) 
e(P,SkAli ) = e(Sli-pub, R+r.H1(L))                                    (4) 
Verification of the signature is carried out with help of bilinear pairing properties. If 
the equation (4) holds the licensed signature will be accepted after L is verified by the 
Authorization Authority.  
The licensed signature is generated with the Sli-prv, the private key of li and the ad-
vantage of the scheme is signature verification is done with already known Sli-pub, the 
public key of li without any demand of updating the signature key tree for a new au-
thorized key. 
Signature schemes in licensed digital signature framework could be implemented 
on directory without extra effort. In multi-signature required documents the construc-
tion of signatures need some arrangements. 
3.2.3   Hierarchically Sequential Multisignature Scheme. As X.509 suggests that 
Certification Authorities be arranged in a hierarchy so that navigation is straightfor-
ward; any signer in hierarchical organization automatically arranged in hierarchy with 
their authorizations as hierarchical levels. 
Integration hierarchical IDs to authorizations could also be proposed so that in case 
of sequential multi-signature on documents in work-flow could be implemented. 
Li et al [21] proposed a key management scheme for role hierarchy in distributed 
systems. They suggested a binding key scheme over roles with hash algorithms.  
Assuming hierarchy as tree siblings of a node has key of their parents’ keys com-
bined in a one way hash function. Example for sibling S1 whose parents are P2 and 
P1, Key of S1 is like H2(H2(P1),H1(P2)). 

An X.509 Based Licensed Digital Signature Framework for Hierarchical Organizations 
405 
 
To implement mentioned structure in hierarchical authorization tree, the licenses 
would be associated with given certification number carrying derived authoritative 
key (license authorization ID) in hierarchy. 
Then sequential multi-signature on the document could be easily implemented with 
envelope technique according to the license authorization ID. 
With this approach, if document D in workflow must be signed by signers i, j and 
k. Their license authorization IDs for signing D would be Ai, Aj and Ak. 
The derived authorization will be as H3(H2(Ak),H2(H1(Aj),H0(Ai)) with assump-
tion of  n number of hash functions exists. The document would be sequentially multi-
signed in with existence of both three signatures. 
If document has authorization IDs in it where signature is required for related au-
thorization then licenses could be associated with the IDs and would increase per-
formance generation of valid signatures.  
The documents will also be stored in X.500 directory according to their derived au-
thorization IDs.  
3.3   Authorization Authority Model 
The other basis of the framework is the Authorization Authority Model. It consists of 
hierarchical license management units where the licenses are stored, exchanged and 
verified on-line. 
Authorization Authorities (AA) creates a license for an authorization and signer as 
the definitions and fields given before. The corresponding entity of Authorization 
Authority in X.509 implementation will be Attribute Authority. By all means the 
functionality of Attribute Authority would be expanded according to the verification 
steps in signatures schemes mentioned before. 
Hierarchical AA model is designed as follows; each level in an organization has an 
AA. These levels are may be units, branches, departments etc. Even related organiza-
tions AAs would be associated hierarchically.  
AA creates license and sends the license to the signer. Signer puts the timestamp 
on it when he/she uses it for the licensed signature. 
The signature generation and verification protocol in terms of Authorization Au-
thority is given in Fig. 4. 
Principal role of AA starts when a licensed signature needs to be verified. The veri-
fier extracts the license from the signature and asks for AA if the signature and corre-
sponding license holds. This is way of asking if the signer is licensed to sign the au-
thorized document. AA checks if the license is valid or not and controls the licenses 
integrity with the SAA field. If it holds AA compares Ali; the authorization subject on 
the license and the subject that verifier presents. 
There are two optional validation interval controls in the model: pre-control and post-
control. As it is clear by naming the validation controls can be done before the licensed 
signature creation or after the signing operation as in signature verification phase. 
In pre-control, signing operation of licensed signature is not permitted before/after 
the valid time interval defined in TAli where the authorization subject Ali does not 
exist. In post-control, time-stamp TSAli created by the signer party and TAli will be 
crosschecked by the AA. If interval and the authorization subject hold for the signer 
ksi the licensed signature will be accepted as valid. 

406 
A. Ugur and I. Sogukpinar 
 
 
Fig. 4. Authorization Authority Process Scheme 
There are also two optional license verification methods supported by the model: 
offline and online verification. 
Offline verification of licenses is implemented in verifier part. Frequently used li-
censes stored in a secure cache for offline verification. If license is generated by prior 
level it can also be verified by the approval entity of the license. 
Online verification is implemented as follows: verifier asks validity of the license 
to the AA of the level. If AA has the license information crosschecks the license and 
returns a Boolean value as valid or not. If AA has not the license that the verifier 
asking AA sends the request to the superior AA in the hierarchy, and so on until the 
request of the verifier finds an answer. 
The information exchange between organizational levels and AA hierarchy is as-
sumed as secure whether provided with trust mechanisms or encryption. 
4   Analysis of the Framework 
The proposed framework is the revision of proposed Framework of Licensed Digital 
Signature which is presented in ref. [9] for the X.509 based implementations. 
Thus the X.500 standardized license structure would cover wide range of applica-
tions and environments. 
This work also provides templates of X.509 structured certificates and Attribute 
Certificates for license structure in licensed digital signatures. 
Framework consists of license structure, association of license with digital signa-
ture and verification of the license.  
The framework differs from prior works with usage of license as organizational au-
thorization information instead of personal identifications as given Section 2. 
There are two different approaches presented in the framework to satisfy the au-
thorization. First one is the most flexible one can be adapted to many signature 
schemes used in literature just like DSA, RSA, or ECDSA. The license can be encap-
sulated with the document as a single document. In the validation phase, validation of 
the license must be handled by AAs as third parties. 

An X.509 Based Licensed Digital Signature Framework for Hierarchical Organizations 
407 
 
Second approach associates the license with the signature key. With this approach 
an authorized signature key is generated without any change of currently employed 
signature keys. The licensed signature can still be verified with the related public key. 
Pairing based cryptography is used to provide the key flexibility. Any id-based 
signature could be employed as licensed signature with organizational authorization 
licenses as Ids. 
Licenses create a link between the signer’s public key and the authorization infor-
mation. But the link between signer’s identity and the public key must be also pro-
vided with traditional certificates in X.509 frameworks. 
The framework is designed based on verification of signer’s authorizations on the 
signed media. The identification of the signer and validation of used public key could 
be done in AAs or CAs in system. But it is recommended that the processes of crea-
tion of authorization licenses and private-public key certifications done in separate 
division respectively in AAs and in CAs. This would create certification chains that 
support security of the key management system. 
License creation and verification stages of the framework gives extra overhead 
comparing with the traditional signature systems between individual parties. But also 
signature authorization is essential for hierarchical organization document workflow. 
The mentioned overhead is a trade off for robustness of the organizational authoriza-
tion of work-flow system. 
However the license creation would require similar process time as in X.509   cer-
tificate creation. Verification time would also be similar to X.509 based frameworks 
if it’s done offline. 
The mapping of signature key and corresponding licensed signature key is based 
on the cryptographic bilinear map with bilinearity, non-degeneracy and computability 
properties of pairings. Proof of the scheme can also be done with the random oracle 
model.  
Security of the licensed signature scheme is based on Diffie-Hellman problem. 
5   Conclusion and Future works 
In this paper, the necessity of signature authorization of digitally signed documents in 
a workflow has been explained and a framework has been proposed to obtain this 
authorization for hierarchical organizations based on X.509.  
Templates of the licensed digital signature structures for the X.509 and Attribute 
certificates were presented. Thus the X.500 standardized license structure would 
cover wide range of applications and environments.  
The fields those must be processed as critical in X.509 profile for the license were 
defined.  
The authorized sequential multi-signature version of the scheme was also intro-
duced and an implementation example of it also presented by using authorization Ids 
embedded to documents signed. These authorization Ids created link not only between 
license and signature but also license and document. 
The analysis of the authorized sequential multi-signature and certification chains to 
link private-public key pairs, their certificate and authorization licenses by the AAs 
and CAs were left as future work.  

408 
A. Ugur and I. Sogukpinar 
 
References 
1. Mambo, M., Usuda, K., Okamoto, E.: Proxy signatures: Delegation of the power to sign 
messages. IEICE Trans. Fundamentals E79-A(9) (1996) 
2. Wang, G., Bao, F., Zhou, J., Deng, R.H.: Security Analysis of Some Proxy Signatures. In: 
Lim, J.-I., Lee, D.-H. (eds.) ICISC 2003. LNCS, vol. 2971, pp. 305–319. Springer,  
Heidelberg (2004) 
3. Wang, G.: Designated-Verifier Proxy Signature Schemes. In: IFIP/ SEC 2005, pp. 409–
423. Springer, Heidelberg (2005) 
4. Chaum, D., Antwerpen, H.V.: Undeniable Signatures. In: Brassard, G. (ed.) CRYPTO 
1989. LNCS, vol. 435, pp. 212–217. Springer, Heidelberg (1990) 
5. Pedersen, T.P.: Distributed provers with applications to undeniable signatures. In: Davies, 
D.W. (ed.) EUROCRYPT 1991. LNCS, vol. 547, pp. 221–242. Springer, Heidelberg (1991) 
6. Saednia, S., Kremer, S., Markowitch, O.: An efficient strong designated verifier signature 
scheme. In: Lim, J.-I., Lee, D.-H. (eds.) ICISC 2003. LNCS, vol. 2971, pp. 40–54. 
Springer, Heidelberg (2004) 
7. Steinfeld, R., Bull, L., Wang, H., Piperzyk, J.: Universal Designated-Verifier Signatures. In: 
Laih, C.-S. (ed.) ASIACRYPT 2003. LNCS, vol. 2894, pp. 523–542. Springer, Heidelberg 
(2003) 
8. Ogata, W., Kurosawa, K., Heng, S.-H.: The Security of the FDH Variant of Chaum’s Un-
deniable Signature Scheme. In: Vaudenay, S. (ed.) PKC 2005. LNCS, vol. 3386, pp. 328–
345. Springer, Heidelberg (2005) 
9. Ugur, A., Sogukpinar, I.: A Framework for Licensed Digital Signatures. In: The First In-
ternational Workshop on Network & Communications Security Co-l.w Netcom 2009, 
Chennai, India, December 27-29 (2009) 
10. Shamir, A.: Identity-Based Cryptosystems and Signature Schemes. In: Blakely, G.R., 
Chaum, D. (eds.) CRYPTO 1984. LNCS, vol. 196, pp. 47–53. Springer, Heidelberg (1985) 
11. Boneh, D., Franklin, M.K.: Identity-Based Encryption from the Weil Pairing. In: Kilian, J. 
(ed.) CRYPTO 2001. LNCS, vol. 2139, p. 213. Springer, Heidelberg (2001) 
12. Paterson, K.G.: ID-based signatures from pairings on elliptic curves. IEEE Communica-
tions Letters 38(18), 1025–1026 (2002) 
13. Cha, J.C., Cheon, J.H.: An identity-based signature from gap Diffie-Hellman groups. In: 
Desmedt, Y.G. (ed.) PKC 2003. LNCS, vol. 2567, pp. 18–30. Springer, Heidelberg (2002) 
14. Lin, C.-Y., Wu, T.-C., Zhang, F., Hwang, J.-J.: New identity-based society oriented signature 
schemes from pairings on elliptic curves. Applied Mathematics and Computing 160 (2005) 
15. Jin, W., Li, D., Li, Q., Xi, B.: Constructing Role-Based Access Control and Delegation 
Based on Hierarchical IBS. In: IFIP NPC Workshop (2007) 
16. Lui Richard, W.C., Hui Lucas, C.K., Yiu, S.M.: Delegation with supervision. Information 
Sciences 177(19), 4014–4030 (2007) 
17. Ugur, A., Sogukpinar, I.: A New Hierarchical Signature Scheme with Authorization, ISC-
Turkey (2007) 
18. Internet X.509 Public Key Infrastructure Certificate and Certificate Revocation List (CRL) 
Profile, RFC 5280. IETF (May 2008) 
19. Stallings, W.: Cryptography and Network Security Principles and Practices, 4th edn. Pear-
son Ed., London (2006) 
20. Adams, C., Lloyd, S.: Understanding PKI Concepts, Standards, and Deployment Consid-
erations, 2nd edn. Addison Wesley, Reading (2003) 
21. Li, C., Yang, C., Cheung, R.: Key Management for Role Hierarchy in Distributed Systems. 
Journal of Network and Comp. Applications 30, 920–936 (2007) 
22. An Internet Attribute Certificate Profile for Authorization, RFC 3281, IETF (2002) 
23. Internet X.509 Public Key Infrastructure Subject Identification Method (SIM) (October 
2006) 

A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 409–416, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Intelligent Network Applications for Medical Systems 
Syed M. Rahman1 and Syed Vickar Ahamed2 
1 Assistant Professor 
Dept. of Computer Science & Engineering 
University of Hawaii-Hilo,  
200 W. Kawili Street 
 Hilo, HI  96720, USA 
srahman@hawaii.edu 
2 Visiting Professor 
Dept. of Computer Science & Engineering 
University of Hawaii-Hilo 
 200 W. Kawili Street 
Hilo, HI  96720, USA 
sahamed@hawaii.edu 
Abstract. The role of intelligent communication systems in medical field is 
immediate and globally pervasive.   Most of the modern hospitals and medical 
networks depend on the deployment of Intelligent Networks (IN) techniques for 
routing the calls and recipients to correct locations. Seeking the correct infor-
mation from the appropriate medical knowledgebase is generally content-based.  
A decade earlier, the choice of knowledge base was entirely within discretion of 
the user.  However, with the increasing dependability of network intelligence, 
the intelligent elements and agents embedded in the network can route the calls, 
services and agents to quickly generate the supporting data and procedures to 
attend to any emergency, medical or security issues.  We extend these concepts 
to the use in hospitals, medical centers, and local communities.  
Keywords: medical networks, intelligent internet, knowledge-base, hospital 
networking. 
1   Introduction 
Medical networks1 can assume various configurations. In a microscopic version, the 
network of medical records kept in doctors’ offices and accessed by a group of local 
users qualifies as a miniature medical network. In more extended versions, hospital 
and insurance companies keep the records of their patients and clients.  
Global medical databases can be organized and strictly monitored for accuracy and 
completeness. The size of databases and the ability to access them for use access and 
its eventual use become serious concerns. An all-encompassing global medical data-
base would be unfeasible and ineffective; however, a network of medical knowledge 
                                                           
1 This work is supported by EPSCoR award EPS-0903833 from the National Science Founda-
tion to the University of Hawaii. 

410 
S.M. Rahman  and S.V. Ahamed 
bases for a community, nation, or the globe holds scientific and social potential. In 
this paper, the architectural configurations of hospital-based medical networks are 
presented along with the possibility of expansion to community, regional, or nation-
based networks. 
2   Intelligent Medical Networks 
Intelligent medical network (IMN) configurations and designs were initiated at the 
City University of New York as far back as 1990 by Krol [1].  Other researchers, 
Mollah [2] and Waraporn [3], have extended the methodology.  In an attempt to blend 
content-based intelligence into IMNs, we present two configurations.  Both utilize the 
intelligent Internet architectures and the national medical expert teams that constantly 
scan the activities in hospitals and medical centers in order to filter out the routine 
activity and select the unusual circumstances to add to regionally and internationally 
located medical knowledge bases. 
The proposed customized medical networks may be suited to small hospitals or for 
multinational global access. Specialty based networks aiding in the diagnosis and treat-
ment of special conditions and rare diseases are also feasible.  In the current Internet 
environment, the search engines provide access to information but lack the human con-
sultative platform for interactive sessions between experts.  In addition, the financial and 
economic aspects of transactions are not handled by the plain old search engines.  More 
explicitly, financial cost is not associated with search results covering possible diagnos-
tic and treatment options, despite that fact that cost is an important factor in decision 
making.  The inclusion of the medical service providers who maintain extensive infor-
mation ranging from the patient ailments to the medical insurance coverage and medi-
cines will complete the treatment program for each of their customers much like the 
credit card companies can track the financial activities of their clients. 
3   Hospital-Based Medical Networks 
A hospital-based integrated medical computer system for processing medical and 
patient information and for evolving medical knowledge, diagnoses, and prognoses is 
depicted in Figure 1. The system consists of a medical processor, including a mem-
ory, and a plurality of medical data banks to which it is connected.  The medical proc-
essor and the medical data banks are designed to work in tandem for executing a  
plurality of instructions and/or obtaining information. Numerous processor hardware 
modules are connected to the medical processor.  
The modules include a communication module (CM), a switching module (SM), an 
administrative module (AM) and a knowledge module (KM). This configuration is 
suitable for handling the traffic of a large community of hospitals or a local medical 
center. The capacity of the centralized Main Hub of the Hospital (See Figure 1) is 
made consistent with the expected service from the AM, KM and SM. For small local 
clinics providing routine medical services, a standard server environment will suffice.  
When the medical staff carries handheld and portable devices for communication, 
synergy of functions can be achieved within the hospital or medical facility complex.   

 
Intelligent Network Applications for Medical Systems 
411 
 
 
AM
SM
2
SM
3
PROCEDURE 
ANALYSIS/ 
RESULTS 
Tissue 
Work 
Blood 
Work 
Radiology 
Research 
& Labs. 
Imaging 
Patient 
Access 
Point 
Patient 
Access 
Point 
Patient 
Access 
Point 
KM
CM 
MKB k
MKB j
MKB i
General 
Patient 
Database 
Subject Hierarchical Knowledge-Bases 
Knowledge Bus 
Procedure and Lab Results Bus
Command Bus 
Patient Bus 
Images, Video, Observation and  
High Capacity Secure Link 
Internet 
Global 
Access 
Points  
Main Hub of Hospital
Physician 
Remote Access
Point 
Physician 
Remote  
Access Point 
Physician 
Remote  
Access Point 
Physician 
Office 
Physician 
Office
Pat. 
ID
Pat. 
ID
Pat. 
ID
Staff 
Staff
Staff
Physician 
Office 
SM
1
Physician's Command/ Input Bus 
Internet 
Global
Access
Points
Medical
Processor 
Units 
(MPU )
 
 
Fig. 1. Computation and communication perspective of medical services provided in a hospital environment 

412 
S.M. Rahman  and S.V. Ahamed 
Data and information security issues become paramount under these conditions. 
Main Hub of the Hospital becomes a customized medical computer to serve the pa-
tients, doctors and the medical research community. 
There are three hardware, firmware, and software modules in the processor.  These 
modules function as a communication module—to control data communication  
between the other modules, the memory, and the medical processor. The effective 
communication from module to module is thus established. The hardware for the 
switching module(s) selects and switches between the various medical data banks for 
solving a particular problem. It also facilitates the administrative module to perform 
housekeeping functions, including multitasking control with resource allocation, real-
time multitasking, and scheduling of tasks.  Within a knowledge module , the hard-
ware performs knowledge processing functions and stores pertinent information in the 
medical data banks. 
General patient databases, physician access point units, patient access point units, 
and service facilities are connected to the medical data banks and medical processor 
via several buses. In an alternative integrated medical computer system, numerous 
processors are included with their own memories and modules and are linked together 
to establish a processor net unit. This system can be used in a campus environment, 
where several buildings encompass the hospital or where several hospitals are inter-
linked over local area networks. One such configuration is shown in Figure 1. The 
nature of components and their capacities are matched to the variety of services pro-
vided, and sizes of the links and routers are matched to the expected client base that is 
served by the medical facilities 
4   Internet-Based Medical Networks 
Large groups of independent hospital networks may be integrated (and programmed) 
to coexist within one larger medical network as independent intelligent networks 
sharing the network infrastructure. The functionality can also be forced to perform as 
a group of localized medical networks [4] for the particular hospital rather than one 
massive medical network for a region or a country. The role of the numerous medical 
service providers (MSPs) will thus be confined to local area services and information 
access rather than to a few global medical service providers. The main advantage of 
having a large number of smaller and localized MSPs is that the cost and complexity 
of a localized MSP is much lower than a large MSP. The new competition at the lo-
calized MSP services will reduce the medical expenses and overall customer costs.  
More local MSPs and businesses can offer the medical information and services 
such as discounted prescription drugs, hospital and nursing services, or physical ther-
apy. Such an increase in the supply side of medical information will facilitate the 
medical field to become more competitive and thus contain medical costs in the long 
run. 
By enhancement of the basic IN architecture of the 1980s to suit the TINA (Tele-
communications Information Networking Architecture) architectures[5], network 
designers have brought a sense of interoperability within the framework all INs. 
While it is desirable to have this ideology already built in every network, there at least 
three drawbacks:  

 
Intelligent Network Applications for Medical Systems 
413 
 
 
 
Fig. 2. Framework of an intelligent Internet based on traditional intelligent network components 

414 
S.M. Rahman  and S.V. Ahamed 
 the standards need a long time to evolve 
 the standard interfaces and software modules that perform the functional 
components (service specific feature of the TINA Network) or FC’s may 
not be readily available, and  
 the backbone network may not be ready to host and execute the network 
programming language for each of functional components (FCs) for 
each of the medical subscribers.  
The proposed MSP-based architectures is shown in Figure 2 (The nomenclature for 
the figure 2 has added an endnote2).  This configuration permits the doctors and hos-
pitals to have an open information policy (such as services, charges, type of care, and 
so on) brought about by competition on the supply side. 
It also facilitates a trend for numerous specialized MSPs to participate in the re-
gional, national, or global medical economy. This step is akin to the distribution of 
localized medical information (from the supply perspective) from local medical data 
banks rather than sharing very large databanks via global or Internet access. In the 
ensuing knowledge society, these MSPs will be akin to local banks functioning in 
cooperation with the regional banks, national banks and the federal reserve system. 
Many such MSPs can serve a large number of patients who do not need the very high-
speed or intricacies that the high-technology global Internet is capable of providing to 
all its users. Smaller databases, localized customers, and larger local area traffics will 
permit smaller MSPs to customize and target their services to a specific clientele and 
user community.  
Under program control, the localized medical knowledge networks (MKNs) that 
interconnect the MSPs may also be forced to work interdependently, exchanging 
current information as it becomes available to perform a variety of functions hitherto 
possible only by human beings. The individual patient and user databases are based 
on the patient (SS) numbers and assigned secure and logical address within the host 
network. Each logical medical knowledge network functions as a personal intelligent 
network (PIN) developed in [6]. It is dedicated to that particular patient only for the 
illness or for life. These MKNs provide all medical intelligent network (MIN), [4] and 
personal communication network (PCN) [6] services and operate under a patient-
driven programmable code. 
It also facilitates a trend for numerous specialized MSPs to participate in the re-
gional, national, or global medical economy. This step is akin to the distribution of 
localized medical information (from the supply perspective) from local medical data 
banks rather than sharing very large databanks via global or Internet access. 
In the ensuing knowledge society, these MSPs will be akin to local banks functioning 
in cooperation with the regional banks, national banks and the Federal Reserve System. 
Many such MSPs can serve a large number of patients who do not need the very  
                                                           
2 The nomenclature in the figure is as follows: MSP = medical services providers; MIP = medi-
cal intelligent peripheral attached to ISP local network; RMSP = regional medical services 
providers; MB = medical knowledge base(s); MBMS = MB management systems; MPE = 
Medical services provisioning environments, MKCP = medical knowledge control point; 
MKTP =  medical knowledge transfer point; MMS = Medical information management sys-
tem; MPE = medical services provisioning environment; DDS = Dewey decimal sys-
tem/Library of Congress system bases. 

 
Intelligent Network Applications for Medical Systems 
415 
high-speed or intricacies that the high-technology global Internet is capable of providing 
to all its users. Smaller. databases, localized customers, and larger local area traffic will 
permit smaller MSPs to customize and target their services to a specific clientele and user 
community. The patient has the privilege and freedom to “instruct” the host network to 
perform all the legitimate and ethically acceptable backbone medical and communication 
network functions. These functions may be as simple as the banking services (such as 
account balance, on-line payments, credit card activity, etc.) currently offered by local 
banks or as complex as the blend of sophisticated hierarchical and cascaded intelligent 
telecommunication functions that the TINA network is capable of performing. 
Figure 2 depicts a possible regional medical network architecture that permits the 
regulation, authentication, and monitoring the medical services by a government or a 
nonprofit entity. The specialized network management system (SNMS) monitors and 
tracks the activity to prevent abuse of the medical services in a region, a state, or a 
nation. Many other possible uses of such a network infrastructure (such as child-care, 
nursing home care, or emergency care) are also possible. 
5   Conclusion 
Medical computers, hospital network and global medical service provisioning system 
have started to converge as efficient centralized medical systems serving the commu-
nal, regional, national or global needs of patients, doctors, and medical research pro-
fession.  In the configuration presented in this paper, we have not included the billing, 
prescription, and insurance aspects.  However, another module (known as the billing 
module (BM)) in any medical facility will channel the related information the appro-
priate communal, regional, national, or global billing center.  When such medical 
systems are designed with the same care and coherence as the intelligent communica-
tion systems, the chances of errors, abuse, and fraud can be virtually eliminated from 
the medical profession.  
Acknowledgements 
We take this opportunity to thank our doctoral students, Drs. Marina Krol, Nazli 
Hardy-Mollah, and Norgrit Waraporn who developed their dissertations under our 
supervision at the Graduate Center of City University of New York during the years 
1994 to 2006. 
References 
1. Krol, M.: Intelligent Medical Network, Ph.D. Dissertation, City University of New York 
(1996); For first reading on Intelligent Networks, S. V. Ahamed. In: Encyclopedia of Tele-
communications, ch. 9, pp. 159–174. Academic Press, New York (1989) 
2. Mollah, N.: Design and Simulation of International Intelligent Medical Networks, Ph.D., 
Dissertation, City University of New York (2005) 

416 
S.M. Rahman  and S.V. Ahamed 
3. Waraporn, N., Ahamed, S.V.: Intelligent medical search engine by knowledge machine. In: 
Proc. 3rd, Int. Conf. on Inf. Tech.: New Generations. IEEE Computer Society, Los Alamitos 
(2006); Waraporon, N.: Intelligent Medical Databases for Global Access, Ph.D. Dissertation 
at the Graduate Center of the City University of New York (2006) 
4. Ahamed, S.V., Lawrence, V.B.: Evolving network architectures for medicine, education and 
government usage. In: Pacific Telecommunications Council, Hilton Hawaiian Village, Janu-
ary 14–18 (2002) 
5. CCITT Recommendation M.3010, Principles for Telecommunication Management Network 
(TMN) TINA Architecture; Rec. M.3200. TMN Management Service; M.3400. TMN Man-
agement Functions; Rec. M.3020. TMN Interface Specification Methodology; Rec. M.3180 
Catalogue Network Information Model; Rec. M.3300. TMN Management Capabilities. 
IBMN  
6. Ahamed, S.V., Lawrence, V.B.: Intelligent Broadband Multimedia Networks. Kluwer Aca-
demic Publishers, Boston (1997) 
7. Li, C.C., Shiao, Y.S., et al.: A conceptual design and demonstration for an e-care system. In: 
Proceedings of the 2009 Euro American Conference on Telematics and Information Sys-
tems: New Opportunities to increase Digital Citizenship, Prague, Czech Republic (2009) 
8. Stroulia, E., et al.: Software engineering for health education and care delivery systems: The 
Smart Condo project. In: Proceedings of the 2009 ICSE Workshop on Software Engineering 
in Health Care, pp. 20–28. IEEE Computer Society, Washington (2009) 

 
A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 417–424, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Proposal of an On-demand Software Deployment System 
Based on Application Streaming, Virtualization 
Techniques and P2P Transport 
Rafael Augusto Teixeira1, Marcos Antônio Cavenaghi1, Renata Spolon Lobato2,  
and Roberta Spolon1 
1 Computing department, Unesp – Univ Estadual Paulista, Eng. Luiz Edmundo Carrijo Coube 
Avenue 1401, 17033360, Bauru, São Paulo, Brasil  
2 Institute of Biosciences, Letters and Science, Unesp – Univ Estadual Paulista,  
Cristóvão Colombo Street 2265, 15054000, São José do Rio Preto, São Paulo, Brasil 
orafaelteixeira@gmail.com, marcos@fc.unesp.br,  
renata@ibilce.unesp.br, roberta@fc.unesp.br 
Abstract. This paper presents the work in progress of an on-demand software 
deployment system based on application virtualization concepts which elimi-
nates the need of software installation and configuration on each computer. 
Some mechanisms were created, such as mapping of utilization of resources by 
the application to improve the software distribution and startup; a virtualization 
middleware which give all resources needed for the software execution; an 
asynchronous P2P transport used to optimizing distribution on the network; and 
off-line support where the user can execute the application even when the 
server is not available or when is out of the network.  
Keywords: Application virtualization, software deployment, application 
streaming, sequencing, middleware. 
1   Introduction 
As computer technology is growing constantly, new software is launched and the user 
needs to spend more and more time with software setup and management. Indeed, this 
is a problem taking this to the big companies with thousands of users and computers. 
In this case, virtualization techniques became an important allied to provide the nec-
essary hardware and software abstraction, avoiding hardware conflicts and improving 
the user experience. The virtualization of entire operation system has been very im-
portant for data centers and enterprise infrastructure, facilitating the management and 
reducing hardware costs. Nowadays, the user experience also is in focus through 
application virtualization techniques where a small subset of operating system can be 
virtualized to execute specifics user application in a controlled virtual environment.  
According VMWare [15] Application virtualization enables the deployment of 
software without modifying the local operating system or file system. It allows soft-
ware to be delivered and updated in an isolated environment ensuring the integrity of 

418 
R.A. Teixeira et al. 
 
the operating system and all applications. Application conflicts are significantly re-
duced. A single application can be bundled and deployed to multiple operating system 
versions. Application are easier to provision, deploy, upgrade, and rollback. 
There are similar approach like Citrix® XenApp [2] and VMWare® ThinApp  [3] 
designed to perform application streaming, but according a recent research [4] they 
presents some problems like 64 bits application, some cannot be adapted to web con-
text, need of previous components installation and runs in the kernel mode which 
need special permissions to run rightly, further, the major of that related system re-
quire high investments with hardware infrastructure and licensing. Also, there are 
related works like [1][8][9], which this work make progress of highlights characteris-
tics, thus building a software streaming framework applicable in various sceneries. 
In our approach we explore the host operating system without modified it. All API1 
call are made directly to operating system when the compatibility is detected, thus the 
software execution is not delayed. When a different version of operating system is 
detect the middleware target the correct library to the application. The key point is the 
middleware that runs in the user mode, without the need of previous installation. This 
model is perfectly adapted to the web context and to improve the download of pack-
ages which compose the software, a smart P2P transport was designed in order to 
address this issue, decreasing the server network overhead. 
The main steps of an application virtualization framework [5] are (i) capture all 
software components installed when a setup program is performed; (ii) a good way to 
distribute the software image; and (iii) a fast middleware to provide the necessary 
resource when the application need. In the next sections we discuss all mechanisms 
adopted in our system.  
2   Software Image Capturing 
A complete image of software will be created to be distributed to the nodes in the 
network. There are two main steps to the image creation, these steps are discussed 
bellow. 
2.1   Capturing the Software Installation 
To make portable an application we need to know all resources that the application 
uses and at exactly time when the system asks we need to provide them. The same 
middleware responsible for virtualizes the software is also responsible to capture all 
changes made by the software program setup; after the setup program is ready the 
middleware will execute and monitor it.  
Normally the main resources changed by a common setup program are: files, sys-
tem registries and environment variables. At the same time, new programs or others 
program setup can launched during a software installation and needed be captured 
too.  
                                                           
1 API (Application Programming Interface) is a set of functions which a software program 
implements in order to interact with other software. In this paper is also used referring the 
windows API. 

 
Proposal of an On-demand Software Deployment System 
419 
 
All resources changed in this moment are separated of the rest of host machine and 
saved on a specific cache folder; new files are saved on this place keeping the original 
folder hierarchy, when the setup program want to modify an existent file, a copy is 
made into the cache folder, and the original file is preserved.  The registry operations 
follow the same mechanism. All new registries are stored into a separated file main-
taining the root and subparts address of registry for posterior search. All registry dele-
tion and registry changes is made in this file, thus this file will be the basis of virtual 
registry space used by the middleware discussed in the next section. 
2.2   Sequencing and Packing Resources 
Distribute all the entire folder containing the resource is not the better way. Indeed 
many megabytes of installation program are not necessary to execute the software. 
For example, the Adobe Reader 9.1 program setup generates 218 Mbytes of files and 
registry operations, and the needed to execute the main application is only 44.1 
Mbytes.  
The resource sequencing improve the startup time of application, unlike Micro-
soft® App-V [6] that sequence the binaries instructions during the software executing, 
here it is made monitoring the main resources utilized by the application when it 
starts, thus we can separate the resources with high priority to delivery first.  
Moreover, this method make possible divide the large software image into small 
packets to distribute the application. When the firsts and more important packets are 
received, the middleware can start the software immediately while the rest of packets 
are being supplying asynchronously. With the use of small packets is possible adopt a 
P2P distribution strategy that will be discussed in the software image distribution 
section. 
3   Virtualization Middleware 
The main function of an application virtualization framework is intercept system calls 
[5] [7], mainly file system operations, registry API and others object usage, in related 
works [8] [9] [1] used these mechanisms but always with modules operating in kernel 
mode of operating system. The basis of middleware is API Interception, when the 
application calls some API function of host operating system, the middleware inter-
cept the call redirecting to the virtual container created for the application execution. 
In this environment some basics modules are needed because some principles of vir-
tualization [10] still are valid to the application virtualization context, mainly the use 
of unmodified binaries where the application doesn’t need change itself to run in a 
virtual environment; and any changes made by virtualized application in the host 
operating system is denied. 
The main characteristics of middleware are described below; our priority is show 
the differences of our design methodology of previous related works and existent 
commercial solutions. The exact description of each module can be made in an ex-
tended paper posteriorly. 
 

420 
R.A. Teixeira et al. 
 
3.1   Virtual File System  
A virtual file system is performed to give an isolated environment to application, but 
the mechanism adopted differs from previous related works [9]. The approach is not 
to intercept all file operations, but only the operation which intend to change some-
thing in the host operating system. When a application starts, innumerous file access 
to query files information is performed and not all intend to modify the file or direc-
tory, then when a operating system compatibility is founded, the middleware allow 
some instructions be executed natively reducing the delay time of each call. One clear 
example is the Microsoft .Net Framework [11], nowadays many application uses this 
framework to facilitate the software development and the news versions of Windows 
operating system already came with this module installed. 
In general the file operations are made over API calls, manly the CreateFile(). 
When an application need to write a file for example, the CreateFile() function is 
called with the write mode as parameter, if the application intend to create a new file, 
middleware redirect the original path to the virtual space, if the application will mod-
ify some existent archive, in this moment a copy-on-change method is performed to 
protect the host system making a copy of this resource to virtual file system area, and 
there the changes is performed. If the application tries opening the same file again, the 
calling is redirect to the virtual file system space. The others file system API, in gen-
eral, uses the handle returned by CreateFile() function that already point to correct 
file (virtual or not), so is not necessary intercept all file API functions, reducing inter-
ceptions overhead. 
3.2   Virtual Registry Space 
A similar method is used to build a virtual registry space. The registries are com-
monly used to store installation information and configurations. But the application 
not uses only owns registries, but constantly the application query information about 
the operating system and user information. When the application queries some regis-
try, the virtual space is consulted, and case it could not be founded, the host system is 
also queried. 
The virtual registry space follows a similar mechanism describe in [8], but with 
some differences to improve the search performance. All registry captured during the 
software installation is transformed into a XML2 file. In this way the search by the 
registry is resumed into specify the correct path to the key, like a binary tree search. 
When the application query a registry, the path is queried in the XML file, if the 
registry is found a copy of all first level values of the selected key are performed to a 
reserved area in the host registry area, in general the “HKEY_CURRENT_USER\ 
<application_name>\” is used for this purpose. This way, minimal changes was 
necessary to return a valid handle when the RegOpenKeyExW() is called. The applica-
tion pass the complete registry path to this function which verifies the  existence and 
returns a handle case the registry was founded, after this, other functions is used to 
                                                           
2 XML (Extensible Markup Language) is a set of rules for encoding several data types  
electronically. 

 
Proposal of an On-demand Software Deployment System 
421 
 
query or modify values in the registry. All this calls is performed passing the handle 
returned by RegOpenKeyExW() function. By using a reserved but valid registry entry 
in the host operating system, it is not necessary create a special data structure to store 
all RegOpenKeyExW() calls, because the handle returned is real and pointed to an 
existent registry.  
3.3   Sub Process Creation Monitor 
Some application during its execution, can executes another process. These process 
needed be virtualized too. As the application starts another, the CreateProcess() is 
called and intercepted, whether the target application is native of host system the 
execution occurs normally, case the application is part of virtualized software, a new 
instance of middleware is executed first targeting the new process. After this the exe-
cution of new process continues in the virtualized environment as the main software. 
This assure that all resources needed by the new process will be provided and all 
changes made by it will be stored in the reserved areas without modify the host oper-
ating system.  
3.4   Environment Variable Interception 
During the software installation the changes made in the environment variable are 
intercepted. This is reached, making a difference between before and after the setup 
program is executed. Any change is stored and when the application starts in the vir-
tual environment, the environment variables running in the host system are updated 
with the captured information. This treatment of environment variable is not men-
tioned in the related works referenced in this paper.  
3.5   Manifests 
The manifests are XML files which describe the assembly involved by the software 
[12]. This file is used to determine the correct version of library used by the applica-
tion, in the practice, all libraries describe in this files needed be the same installed by 
program setup to avoid conflicts. The main problem is software cannot starts if the 
file manifests is out of the correct location, inside the windows folder. Addressing this 
issue, when a manifest file is founded, it is copied to the host operating system and it 
is removed when the application stop execution. 
4   Software Image Streaming   
When the middleware starts in a client machine in the network, the server is contacted 
to initialize the software image distribution. Using the sequencing process we can 
identify the mains resources used by the application to delivery it first, dividing the 
original software image into small packets.  
The server also sends a list of available nodes in the network and all packets that 
each node have. Then the client requests to the server the first package and at same 
time elect others available nodes to requests the subsequent packages. Thus the server 
is not overloaded with clients starting at same time.  

422 
R.A. Teixeira et al. 
 
The server is also responsible to testing periodically the nodes to assure the high 
disponibility of the resources. The packages are compressed using a common algo-
rithm for this, as they are received they are uncompressed in to the virtual file system 
space. 
After the firsts and more important package is downloaded, the middleware starts 
the application while the rest of application packages is received. This is important to 
assure that when the application is executed in off-line mode, all resources necessary 
to any option of software will be available, even if the server is not present. This is 
possible because the software image is stored like a cache in the user machine. This 
also allows the user restore the original application from the cache. 
5   Preliminar Prototype 
The objective of this paper is show the working in progress of our solution, but some 
preliminary results already can be observed. Actually we are working to integrating 
all system modules: software image capture, distribution process with P2P transport 
and virtualization middleware to get complete results, since the image acquisition to 
the execution in the client machine. Some preliminary results are showed bellow per 
module using the common application case. We are using computer running with a 
clean installation of Windows Seven Ultimate 64 bits version with Intel Core 2 Duo 
2.0 GHZ processor. 
 
Software image capturing. A common Adobe Reader 9.1 installation involve around 
218.13 Mbytes of new files, modifies files and registry entries, using a common com-
press algorithm like zip compress, the size of package downs to about 98.2 Mbytes.  
Associating the sequencing method, the basic image containing the essentials re-
sources to startup the program (essential package) is about 44.1 Mbytes and 22.2 
Mbytes with compression which corresponds to 89,82% of original package size.  
 
Software image streaming. The distribution of the package obtained in the first phase 
which has 98.2 Mbytes (compressed) using common network sharing is about 9 sec-
onds. Using the P2P approach, only the essential package is delivered first by the 
server and takes about 2.4 seconds to be transferred, this is the time which the mid-
dleware will wait to startup the application. The rest of complete package will be 
divided into minor packages of 1 Mbytes. With more one node in the network, the 
time of synchronize the rest of image software (about 76,0 Mbytes) was reduce from 
about 6.9 seconds to about 6 seconds, the reduce is not significant, but the major 
benefit was the reduction of server download overhead. When we used this mecha-
nism the server is always available to new client requests finding to download the 
essential package. 
 
Virtualization middleware. Some results are obtained comparing a clean execution of 
the application with a virtualized execution. The numbers is not considering the 
streaming time of packages. Using the Adobe Reader 9.1 example, a clean execution 

 
Proposal of an On-demand Software Deployment System 
423 
 
occurs in about 0.8 seconds, when the virtualization middleware is employed the 
execution time is about 1.7 seconds. This preliminary result is very motivator compar-
ing to other related systems [4], where the same software is adopted to the testing. 
6   Licensing and Actual Limitations 
The licensing is real problem to all application streaming solutions. The software-as-
services is a tendency, but many commercial software still uses the license per instal-
lation. All files and registry modified during the process of installation and conse-
quently during the activation process, are stored and distributed to all client nodes that 
desire executes this application.  Trying reducing the licensing problem, the only way 
to access the packages is using the middleware, because all packages are encrypted. 
But is known this is not the best way to protect the software piracy. The best way is 
the software industry change the actual licensing way, like has been happened with 
web applications. Maybe a license per user usage can address this problem too. 
A limitation of this model, are the specifics servers software like databank solu-
tions and software installed as services in to the operating system. This kind of appli-
cation need special attention and will be supported in next step.  
The integration inter application and the host operating system is also a limitation, 
for example the Adobe® Reader [13] Plug-in for Internet Explorer, where a PDF file 
is automatically opened inside the Internet Explorer where a link for this kind of file 
is clicked.  
7   Conclusion 
This paper showed the work in progress of a robust application streaming solution 
which helps the software deployment. Also eliminate the need of configuring the 
applications on each computer. The middleware operating in the user mode can exe-
cute many applications at same time and don’t need special permissions. The P2P 
transport is another innovation compared to the related works, improving the applica-
tion streaming and decrease the server overhead. 
The next steps are finish the integration of all solution modules and make new ex-
periments with 20-30 clients machines starting the application streaming at the same 
time. And work at actual limitations discussed into prior section.  
In the future we intend to allow the application streaming also to Linux operating 
system, and execute windows applications in the Linux environment. It will be possi-
ble using some techniques of emulation of the Wine project [14].  
References 
1. Zhang, Y., Wang, X., Hong, L.: Portable Desktop Applications Based on 
P2PTransportation and Virtualization. In: Proceedings of the 22nd Large Installation Sys-
tem Administration Conference (LISA 2008), pp. 133–144. USENIX Association, San 
Diego (November 2008) 

424 
R.A. Teixeira et al. 
 
2. Citrix® XenApp.,  
 http://www.citrix.com/english/ps2/products/ 
 product.asp?contentid=186 
3. VMWare® ThinApp., http://www.vmware.com/products/thinapp/ 
4. Huisman, S., Haverink, M.: Application Virtualization Comparison Chart September 2009. 
VirtualFuture.info. (September 1, 2009) 
5. How to build an Application Virtualization Framework. VDI® works (July 1, 2008), 
http://vdiworks.com/wp/?p=15 
6. Microsoft® Application Virtualization,  
 http://www.microsoft.com/systemcenter/appv/default.mspx 
7. Russinovich, M., Solomon, A.S., Ionescu, A.: Windows® Internals: Including Windows 
Server 2008 and Windows Vista, 5th edn. Microsoft Press (June 17, 2009) 
8. Yu, Y., Guo, F., Nanda, S., Lam, L., Chiueh, T.: A Feather-weight Virtual Machine for 
Windows Applications. In: Proceedings of the Second ACM/USENIX Conference on Vir-
tual Execution Environments (VEE 2006) (June 2006) 
9. Alpern, B., Joshua, A., Bala, V., Frauenhofer, T., Mummert, T., Pigott, M.: PDS: A Virtual 
Execution Environment for Software Deployment. In: Proceedings of the First ACM/ 
USENIX International Conference on Virtual Execution Environments (March 2005) 
10. Barham, P., Dragovic, B., Fraser, K., Hand, S., Harris, T., Ho, A., Neugebauer, R., Pratt, 
I., Warfield, A.: XEN and the Art of Virtualization. In: Proceedings of the 19th ACM 
Symposium on Operating System Principles (October 2003) 
11. Microsoft. Net Framework,  
  http://msdn.microsoft.com/pt-br/netframework/default.aspx 
12. Software Development Manifests,  
  http://msdn.microsoft.com/en-us/library/ 
 aa375365%28VS.85%29.aspx 
13. Adobe® Reader, http://get.adobe.com/br/reader/ 
14. Wine Project page, http://www.winehq.org/ 
15. VMWare® Enterprise of virtualization segment, http://www.vmware.com/ 

A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 425–435, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
Enhancing Web Caching  Using Web Usage 
Mining Techniques 
Samia Saidi and Yahya Slimani  
Department of Computer Science  
Faculty of Sciences of Tunis 
University of Sciences of Tunis 
samia.saidi@esti.rnu.tn, yahya.slimani@fst.rnu.tn 
Abstract. Performance and other service quality attributes are crucial to user 
satisfaction of web services. Web Mining provides the key to un- derstanding 
web traffic behavior, which in turn explain the increasing interest in this domain 
and its high number of its possible applications. In this paper, we apply Web 
Usage Mining techniques to propose an intelligent caching solution with the 
goal of improving the quality of ser- vice of web sites. We found that empower-
ing caching with a prefetching engine that predicates the components of pages 
to be used in the near future by users can enhance web sites performances. This 
is allowed by analyzing the historical of navigation of a web site  reported in 
log files and by determining the set of components to be sollicitated in the fu-
ture using frequent closed itemsets. 
Keywords: Web caching, Web Usage Mining, Web log files, Web page com- 
ponents, Frequent closed itemsets. 
1   Introduction 
Web Usage Mining (WUM) techniques have been applied in many fields such that 
web personalization, E-commerce and so on [15]. In this work we are focusing on 
the application of WUM to enhance  web caching performance. The intelligent 
web caching method that we proposed has mainly as objective the selection or the 
prefetching of web page components to cache based on user profiles. Previous works 
in this field focused mainly on analyzing [3, 13, 16] raw log files to select which 
web page to replace in cache. Our objective in this work is to propose a frame-
work that handles new web exigencies. Mainly the prefetching of components of 
pages and not whole pages. 
Remember that, in order to reduce the overhead for generating dynamic data 
in systematic web sites, it is useful to generate data corresponding to a dynamic 
object once, then store the object in a cache, and subsequently serve requests to 
the object from cache instead of invoking the server again. In fact, the caching of a 
whole page can be of limited utility, especially in the case of personalized pages, 
where each client would need a different version of the same page. More- over, 
different web page components can have different update frequency, so that, cach-
ing the entire page needs, when one update is necessary, the recomputation of 

426 
S. Saidi and Y. Slimani 
the whole page even if only some parts of the page have been updated. So, we 
choose to cache components of pages and not entire pages, since the caching of 
components of pages can enhance the web performance by  specifying pre- cisely 
candidate web page components for caching. Hence, choosing a suitable tech-
nique of fragmentation of pages is crucial. On the other hand, having  the set of 
web page components,  selecting  which components  to cache is extremely im-
portant, we deal  here  with  the  problem  of prefetching  or with  the  problem of 
selection of components  to cache. In fact, a user’s browsing sequence will fol- 
low the  hyperlinks  between  Web  objects.  That is, if object  A has  a hyperlink 
to object B, the  probability that B will be accessed, given A has been accessed 
already,  will increase significantly. Hence, if we prefetch  those objects witch are 
very likely to be referenced the client’s subsequent requests,  part  of the network 
latency  can be hidden  within  the time between client’s consecutive  requests  [5]. 
In this  paper,  we propose a method  of selection of web page components  based 
on the preferences  of users, obtained by applying  techniques  of WUM. 
Given the high traffic in the Internet, caching of documents is an important 
technique  to reduce  latency,  bandwidth consumption, and  server load. Consoli- 
dating  a Web cache with a suitable  prefetching  policy can enhance performance. 
Nevertheless,  good performance  requires  a prefetching  policy targeted to char- 
acteristics of the Web applications and of the client. Our method  of prefetching 
provides  the  ability  to  prefetch  objects  that could  be used  in the  near  future 
using the historical  of navigation  of the users. The rest of this paper is organized 
as follows. Section  2 reviews related  works about  caching and  fragmentation of 
web pages. Section 3 reviews the problem of selecting web page components.  Sec- 
tion 4 describes our proposed approach  based on techniques  of WUM. Section 5 
deals with some experimental  results.  Finally, section 6 concludes the paper and 
highlights  some future  works. 
2    Related Works 
It is more efficient if we concentrate in the caching of components  of pages and 
not  whole pages  in  order  to  enhance  the  QoS  of web servers.  Some  authors, 
like Labrinidis  and  al.  [9–12] deal  with  a  special  caching  (materialization) of 
webviews which  are  database query  results  (i.e.  database views)  with  HTML 
formatting commands  or XML semantic  tags.  These views are organized  in or- 
der to rapidly  answer various types of web queries. The authors in [12] deal with 
materialized, non materialized and  virtual  webviews. They  proposed a solution 
based  on collecting  multiple  statistics and  on estimating two metrics,  namely 
Quality  of service (QoS)  and  Quality  of Data  (QoD).  The  proposed  system  in- 
vests in the materialization when the global QoD of the solution  exceeds a fixed 
threshold. In the other  hand,  the solution  invests in dematerialization when the 
global QoD of the solution is lower than  the fixed threshold. This solution is very 
hard  to implement because  of the  large number  of statistics required  besides of 
the  number  of estimations based  on the  Recursive Prediction Error  Method  [8] 
that must  be considered  by it. Moreover, the authors doesn’t consider any con- 
straint of size for the suggested  cache. In fact, the proposed asynchronous  cache 

 
Enhancing Web Caching  Using Web Usage Mining Techniques 
427 
stores  all the  given webviews under  one policy of materialization. Besides, this 
solution  does’nt treat any aspect  of replacement which incurs  significant space 
overhead.  For  these  reasons,  we found first that selection  based  on a more rig- 
orous method  of prediction  can improve the results  of materialization. Then,  we 
focus on a method  based  on the  preferences  of users  and  thus  on technics  of 
WUM.  Furthermore we believe that Labrinidis  and  al.  works store  only  web- 
views and doesn’t consider other  interesting components.  Hence, we found that 
the  application of an appropriate technique  of fragmentation can be beneficial 
since it considers all types of components  of pages and not only webviews. 
Datta and  al. in [6] proposed  a caching  of fragment  of pages where scripts 
of pages  are  composed  on  multiple  code  blocks.  A code  block  can  be  reused 
if it  is tagged  within  the  script.  When  the  script  is executed,  the  tags  inform 
the  server to check the  cache before executing  the  code block. If the  requested 
fragment is founded in the cache they bypass  the logical code of the block. Else 
they  execute  the  block and  subsequently copied  it  in the  cache.  The  authors 
here use one technique  of replacement called the Least Likely to be Used (LLU) 
which is based  on a predictive  technique:  a component of a page to be cached 
replaces  the  least  likely to be used one. For that, we found that the  fragmenta- 
tion  of web pages is necessary  to get candidates set of page components  to be 
cached.  The  problem  of fragmentation is mainly  treated by Challenger  and  al. 
[2, ?]  that propose a fragment-based method  for the  design of web sites.  Rela- 
tions between pages and page fragments  are predefined in a fixed graph  and are 
generally  specified by the  user.  The  authors have deployed  systems  using  two 
approaches  for creating  and  modifying an  Object  Dependency Graph  (ODG). 
Fragmentation methods  are used in different purposes. For example Bouras  and 
al.  [1] invest  in  fragmentation to  eliminate  redundant data  transfer   over  the 
web. The algorithm of fragmentation is viewed here as an html  filter. This filter 
fragmented the web pages using tags.  It stripped  all images and considers frag- 
ment delimiters  between  the  tags  of <table>, because  they  represent the  most 
popular  structuring tags. An other  method  of fragmentation is proposed by Ra- 
maswamy and al. [14], which is mainly  used for detecting  interesting fragments 
for caching.  This  method  has as goal the  detection  of interesting fragments  in 
dynamic web pages,  which  exhibit  potential benefits  and  thus  are  interesting 
cache units.  They  define candidate fragments  as fragments  that are  shared  by 
over a threshold  of fragments  and  that have different  personalization and  life- 
time characteristics. For that, they first propose a hierarchy  of the dynamic  web 
pages and  a particular data  structure that helps in the  detection  of fragments. 
In  fact,  they  convert  web pages  to  their  corresponding  Augmented Fragment 
(AF)  deduced  from Document Object  Model (DOM)  tree  and  prune  the  frag- 
ment tree  by eliminating  the  text  formatting nodes. The  result  of the  first step 
is a specialized DOM tree that contains  only the content of structured tags ( like 
<TABLE>, <TR>, <P>). The  second step  annotates fragments  of the  frag- 
ment tree obtained in the first step.  Second, they propose an efficient algorithm 
to detect  maximal fragments  that are shared  among multiple  documents. Third, 
they  develop a practical algorithm that effectively detects  fragments  based  on 
their  lifetime  and  personalization characteristics. Having  that, the  selection  of 
the  suitable  set  of page  components  is critical  to  enhance  caching,  so it  must 

428 
S. Saidi and Y. Slimani 
be based  on  a  good technique  of prefetching.  For  that, we decide  to  develop 
one approach  based  on the  analysis  of the  historical  of navigation  of web users 
allowed by techniques  of WUM to predicate  the set of page components  to select 
for caching. 
3   Problem Definition 
Selecting  web page components  to cache is an important problem  for web sites 
managers  because  caching  a good set  of page  components  allows reutilization 
and thus improves the performance  of web sites especially for dynamic  ones. As 
a solution,  we propose a new approach  that suggests to use the  access patterns 
of users in order  to select a suitable  set of web page component  to be cached. 
In fact, the access patterns of users are an important and useful knowledge that 
can lead to find the best page components  to be materialized. 
We use the  terminology  of Web  page component   to  deal with  a part  of a 
web page which  can  be dynamic,  i.e. generated from  database queries  (de-
fined  as webviews by  Labrinidis  and  al.  [12]). The  selection  of web pages  
components acts  complementary to  caching.  Thus,  it  is helpful  to  follow one 
strategy  for selection allowing the selection of the web page components  the 
most referenced together. We define the problem  of selecting page components  
as follows: Using knowledge about users access patterns  mined  from Web Log 
files select web page components  to be cached so that these latter have high prob-
ability to be referenced in the near future. 
4   Description of the Proposed Approach 
4.1   Architecture 
Starting from the typical three-tier architecture of modern  web servers, we sug- 
gest to add a new component to this architecture (see figure 1). This component, 
that we call Pre-fetching  engine, communicates to the web cache module  a good 
set of web page components  to be cached. The inputs  of this component are web 
log files and  the  source code of a web site.  Yet,  this  component has as role to 
analyze web log files to structure them on paths  of navigation. In parallel,  it has 
to split  web pages to generate  a structured set of pages.  These  two results  are 
combined to generate  a structured table  containing  paths  of navigation  of users 
with web components  of web pages. This result  will then  performed  to generate 
the  set of components  of web pages to cache based  on mining  Frequent Closed 
Itemsets. The  generated set  will depicts  a suitable  set  of components  of web 
pages which are the  most  used by the  users.  We mainly  focused on the  role of 
the prefetching  engine that select and store a set of candidate page components 
for caching after  applying  one policy of replacement. 
 

 
Enhancing Web Caching  Using Web Usage Mining Techniques 
429 
 
Fig. 1. Number  of Frequent Closed  Itemsets and  Items  Generated 
4.2   Preparation Step 
The  WUM techniques  provide  knowledge about  user behavior’s  on a Web site. 
This knowledge is expressed through the relation  patterns hidden in the log files. 
Before extracting this knowledge, it is necessary  to prepare  the rows of a log file 
into  structured valuable  data.  In our  approach,  we had  three  phases  namely  : 
(i) the  pretreatment of web log files, (ii) the  fragmentizing  web pages, (iii) the 
integration of web page components  into  structured transactions composed by 
web page components  used by one web user. 
Pretreatment of Web Log Files. Remember that a log file is the  primary 
source of data  in web usage mining. It is a plain text  file, where user queries are 
ordered  in chronological  time  order  to  represent the  fine-grained  navigational 
behavior  of visitors.  Each  hit  against  the  server,  corresponding  to  an  http re- 
quest,  generates  a single entry  in the  server  access logs. The  log format  may 
vary, but  it contains  fields identifying  the time and the date  of the request,  the 
IP  address  of the  user,  the  resource  requested,  the  status of the  request,  the 
http method  used, the user agent (browser  and operating  system  type and ver- 
sion), the referring web resource, and, if available,  client-side cookies identifying 
uniquely a repeat  visitor.  An example  of a server  access log entry  is depicted 
in  Table  1. Some  fields in  the  log entries  have  been  changed  to  respect  pri- 
vacy. Web  log entry  1 shows the  date  and  the  time  of the  user  access, having 
the  IP  address  1.2.60.136 spending  546 s on the  page.  The  uri  of the  system 
is /version − f r/phpwebgallery − 1.3.4/picture.php , the  uri  of the  query  is 
cat = mostv isited − imagei d = 7&expand  = 23, 15, 18, 19 and  the  port  is 80. 
Information on user agent are M ozilla/5.0 + (compatible; +Googlebot/2.1; + + 
http  : //www.google.com/bot.html),  the  referrer  is not  mentioned  by this  en- 
try.  Finally,  the  three  status of query  substatus and  win32 status are  200 0 0 
respectively. 
Table 1. A web server  log entry 
1 
2007   −
02   −   0100 
: 
00 
:  
42SI GET /version  −   fr/phpwebgallery   −
1.3.4/picture.phpcat
=
mostvisited   −    imageid
=
7&expand
=
23, 15, 18, 1980 −1.2.60.136Mozilla/5.0 + (compatible; +Googlebot/2.1; + + http :
//www.google.com/bot.html) −20000546
 

430 
S. Saidi and Y. Slimani 
The  aims of the  preprocessing  step  in a WUM process are : (i) convert  the 
raws log file into  a set  of transactions (one transaction being the  list  of pages 
visited  by one user)  ; (ii)  eliminate  the  non-interesting or noisy requests  (e.g. 
implicit  requests  or requests  made by Web robots). 
Some  steps  were already  proposed  by  authors like Cooley [4] and  Tanasa 
[15]. However,  we added  some new steps  and  modified  some existing  ones to 
propose a complete  pretreatment methodology  more suitable  for dynamic  web 
caching.  As a  result,  steps  to  follow are  data  fusion,  data  cleaning  and  data 
structuration. Tanasa in [15] deals with  the  step  of data  summarization where 
he first transfers  the  structured file containing  visits  or episodes (if identified) 
to a relational database. Afterwards,  he applies  the  data  generalization at  the 
request  level (for  URLs)  and  the  aggregated data   computation for  episodes, 
visits  and  user sessions to completely  fill in the  database. Ziane and  al. in [17] 
proposed ,for this step, to load structured data  extracted from log files into a data 
cube structure in order  to perform  data  mining  as well as traditional On Line 
Analytical  Processing (OLAP). For the first step of data  fusion, we join the set 
of log files into one log file by applying  a specific algorithm that inserts  records 
of the  resulting  log file based  on chronological  time  order.  For  privacy  reasons, 
we remove the host names or the IP addresses  and replace them  with identifiers 
keeping  information about  the  domain  extension.  Then,  for the  following step 
of data  cleaning  we remove  mainly  all  unnecessary  requests,  such  as  implicit 
requests  for the objects embedded  in the web pages and the requests  generated 
by non human  clients  of the  web site  like the  web robots.  For  this  removal,  it 
is necessary  to distinguish between the implicit  and the explicit requests  for the 
images since explicit  requests  represent the  real actions  of the  users.  Although 
Tanasa in [15] stresses  on that the  decision for supporting or removing  images 
from web log files depends  mainly  on the  purpose  of WUM.  In fact,  for a web 
cache application for example, it is more important to predict  requests  for these 
files than  requests  for other  files like text  files because of the size of images. We 
decide to remove images because we will fragmentize  web pages and then we will 
retrieve images. For the removal of Web Robots (WR)  we scan periodically a web 
site.  It  follows all the  hyperlinks  from a Web  page.  Thus,  a WR  will generate 
a  huge  number  of requests  on  a  web site,  since the  number  of requests  from 
one WR  may be equal  to the  number  the  Web  sites URIs.  For  identifying  the 
requests  generated by a WR, we use a simple heuristic  based on the list of user 
agents known as robots.  But  databases containing  these lists are not exhaustive 
and  each day new WR’s appear  or are  renamed.  Once all the  WRs  identified, 
the requests  that they generate  can be removed. The third  step of preprocessing 
is the data  structuration which groups the unstructured requests  of a log file by 
user, user session, page view, visit, and episode. Thus at the end of this step, the 
log file will be a set of transactions. A transaction is a user session, a visit or an 
episode. In our case we don’t need to identify episodes since we apply techniques 
of WUM for caching and we exclude the factor of ontology for the identification 
of episodes. For the identification of users, the log file provides only the computer 
address (name or IP) and the user agent. For web sites requiring user registration, 

 
Enhancing Web Caching  Using Web Usage Mining Techniques 
431 
the log file contains  also the user login (as the third  record in a log entry).  In this 
case, we use this  information as user identification. When  the  user login is not 
available,  we consider  (if necessary)  each IP  as a user,  although we know that 
an  IP  address  can  be used by several  users.  For  the  session identification, the 
difficulties were well described in [4, 15]. For that, if the  user login is available, 
we combine  the  user  login field with  the  pair  (Host,  User  Agent) to  separate 
the  user sessions. We choose this  solution  because  a registered  user might use 
different  computers  or browsers when exploring  the  web site and  the  inclusion 
of the user agent allows us to better distinguish between users within  a common 
host.  For  the  page view identification the  requests  are  grouped  by page views 
using the following algorithm: 
 
–  When  the  request  for the  page view pi   is in the  log file, we remove the  log 
entries  corresponding to the  embedded  resources  from one page Pi , and  we 
keep only the request  for Pi . 
–  When  the request  for Pi  is absent (due to the browser or proxy cache), but 
some entries for its corresponding resources are present and these entries have 
Pi  in the referrer field, we replace the entries corresponding to the resources 
with a request  for Pi  and we set the time of this request  to ti = mintime(li ), 
where li  is the  corresponding  log entry  for the  resource  ri . Cooley [4]  here 
deal with an algorithm of path  completion. 
 
Then,  several heuristics  can be used to split  the  user session into visits [4]. We 
follow the  heuristic  dictating that a  new  visit  begins  each  time  when  a  gap 
exceeding  a threshold  of time  between  two  page  views. Thus,  at  this  level we 
get  a  set  of n  page  views  P  = {P1 , P2 , . . . , Pn },  and  a  set  of m  user  vis-
its V  = {v1 , v2 , . . . , vm },  where  each  vi   ∈ V  is a subset  of the  pages  P .  
Conceptually  speaking,  we can  view each  transaction as a sequence  of ordered  
pairs: v =< (p1v , ω(p1v )), (p2v ,ω(p2v )),. . . , (pnv ,ω(pnv )) >, where ω(piv ) is 
the weight associated  to page piv  in the transaction v. We choose to represent 
these weights in a binary  manner,  to note the existence  or non-existence  of a 
page in a trans- action.  This result  can be represented by a binary  relation  de-
picted  as follows : R1  is defined over the couple (V, P ), where V is the set of 
visits and P  is the set of web pages. (V, P ) ∈ R1  if and only if the visit v ∈ V 
contains  the page p ∈ P .  
Fragmentizing Web Pages. In parallel to this pretreatment process, we apply a 
fragmentation algorithm to select interesting candidate page components  for 
caching,  since manual  markup  of web page components  in dynamic  web pages is 
both  labor-intensive  and  error-prone. Furthermore, the  manual  approach  for de-
tection  of web page  components  becomes  unmanageable and  unrealistic  for the  
caching of components  that deal with  multiple  content providers.  It is cru- cial to 
detect  interesting fragments  in dynamic  web pages. However, the method proposed 
in [14] represents  a good method  of fragmentation adapted to caching, since as said 
before, it focuses on candidates for caching and proposes an alter- native  solution  

432 
S. Saidi and Y. Slimani 
for the  selection  of interesting of page components  for caching. In our case a sim-
pler  method  of fragmentation is adopted since our method  of selection  is based  on 
WUM  techniques.  For  that, we use an  algorithm dealing with the parsing  of the 
html  code of pages to identify all the components  of one page (static or dynamic)  to 
be a candidate for the  pre-selection  of components to materialize. The result  is 
then  depicted  in the second association  R2   between web pages and  web page com-
ponents,  defined over the  couple (P, C ), where P is the  set of web pages and  C is 
the  set of web page components.  We said that (p, c) ∈ R2    if and only if the web 
page p ∈ P contains  the component c ∈ C . 
Integration of Web Page Components into Visits. Having the two binary 
relations Ri  and R2 , we define a new relation R that is the composition of Ri  and R2 . 
Ri  : V → P , R2   : P → C (V(x, y) ∈ V xC ), (xRy)  ⇔ ∃z(xRi z) V(zR2 y). The gen-
erated relation  R is composed by visits of the  pre-treated log files and  from page  
components  detected  during  the  parsing  of web pages.  The  algorithm of selection 
of the suitable  set to be cached is based on the mining of this suitable set  using  the  
generation   of frequent closed  itemsets  under  a  parameterizable support. 
4.3   Processing Step 
For  the  formulation, visits  of users  of one web site  are  the  set  of objects,  and 
web page  components  are  the  items.  Our  objective  is to  generate  a set  of in- 
teresting web page components  which are accessed frequently  by a set of users. 
We believe that this  set  will be a significant  subset  of the  initially  defined set 
of web page components.  For  that, generating  this  subset  using frequent item- 
sets, can be critical  for two reasons : first, the huge number  of frequent itemsets 
to  be generated, and  how to  generate,  from these  frequent  itemsets  the  set  of 
the  best components  to  cache.  To  generate  items  from itemsets  we can  apply 
two  set operations  of intersection or of union.  We don’t  apply  the  intersection 
of itemsets,  especially  because  when  we apply  this  operation  on some simple 
visits  by web page components,  we found  the  empty  set.  We then  explore  the 
possibility  of the  union  because  this  operation  is less complex  and  because  it 
generates  a subset  of items  (web page components) which are accessed with  a 
fixed frequency with consideration to all users. To reduce the number of frequent 
itemsets,  we considered  three  possibilities  : the  generation  of frequent itemsets,  
the generation  of closed frequent itemsets  and finally the generation  of maximal 
itemsets.  If we refer to the  definition  proposed  by Gouda  and  al. [7] we found 
that a frequent itemset  is closed if it has no superset  with  the  same frequency. 
A frequent itemset  is called maximal  if it is not  a subset  of any other  frequent 
itemset. 
For the set of frequent itemsets  there will be a generation  of all possible sub- 
sets of items that are upper than  a fixed number of visits. Thus, if we have many 
long frequent patterns, the number of generated itemsets  can be very high mainly 
because a frequent pattern of length l implies the presence of 2l  − 2 additional 
frequent patterns. For  that, exploring  the  set  of generated closed  
 

 
Enhancing Web Caching  Using Web Usage Mining Techniques 
433 
itemsets  can reduce  the  first set generated by the  frequent itemsets.  But  it is 
crucial to verify that there  is no loose of information, especially after  the  op-
eration  of union of the  items  of closed  itemsets.  Remembering  that the  set  
of closed  frequent itemsets  is composed of supersets of itemsets  under  different 
supports. Now, we can  formulate  the  problem  of the  generation  of closed 
itemsets  as follows: we consider  the  set  of frequent itemsets  generated by  
applying  one algorithm of generation  of frequent  itemsets.  The  set  of closed 
frequent  itemsets  will be a subset  of this  latter especially  because  we remove 
all itemsets  which are  sub- sets  of other  itemsets  under  a same  support.  
Then,  we focus only  on the  set of items  generated by the  union  of itemsets  
of this  set.  We found that there  is no loose of information generated by  the  
union,  since the  union  of subsets  of one superset  gives the items of this super-
set.  Furthermore, if we explore the set of maximal  itemsets  and  we formulate  
this  differently  there  will be elimination of all inclusion  between  closed item-
sets.  Thus,  only supersets will be kept  from the  generated closed frequent  
itemsets,  and  the  union  of these  supersets gives the  same results  given by the  
union of items of closed itemsets.  Hence, one can interchangeably choose be-
tween  the  three  proposed  solutions  and  may  choose directly  the  generation  of 
maximal  itemsets.  However, after  carrying  out  some experimental  results,  we 
found that existing implementations are time consum- ing. For  this  reason,  we 
rely on the  generation  of closed frequent itemsets.  We select one implementa-
tion of generation  of closed frequent itemsets  to generate the set of closed fre-
quent itemsets  with a support  equal to 1/3. We apply to this set an algorithm 
for generating  the union of the items of itemsets.  This latter is the result  gen-
erated by the pre-fetching  engine. 
5   Experimental Results 
To illustrate our  proposal,  we consider  an experimental  web site  with  30 Web 
pages. Applying  the  fragmentation algorithm, after  parsing  web pages,  we 
get70 page components. 
After parsing  the  web log file and  applying  the  associated  pretreatment, we 
generate  variable  numbers  of visits,  depending  on navigation  of users related  to 
the web site. We then apply CHARM algorithm [18] to generate  the set of closed 
frequent itemsets.  After that, we obtain  the set of items (web page components) 
generated by the union of frequent closed itemsets.  If we analyze results obtained 
under different supports, we found that although the number of closed sets varies 
under  different supports, while the  number  of items  generated by the  union  of 
frequent itemsets  remain  constant. It  generally  varies  on the  1/3 of the  total 
number  of web components. 
Results  obtained by CHARM  and  by the  implementation of the  union  are 
illustrated in Figure  4. 

434 
S. Saidi and Y. Slimani 
 
Fig. 2. Number  of Frequent Closed  Itemsets and  Items  Generated 
 
Fig. 3. Number of Frequent Closed Itemsets and Items Generated 
6   Conclusion and Future Works 
In this paper,  we proposed a new approach  for selecting a set of components  of 
pages to be cached. This  approach  is based  on WUM techniques.  We first inte- 
grate page component  into  visits  of users  of one web site  in the  preprocessing 
step.  Then,  we use the generation  of frequent closed itemsets  to filter the set of 
the  most  sollicitated components  to be candidate for caching.  Implementation 
efforts are carried  out  in order  to test  the  proposed  prototype for materializa- 
tion  on real  web sites  and  to  compare  its  performance  with  existing  methods. 
 
Fig. 4. Number of Frequent Closed  Itemsets and Items Generated 
Moreover, research efforts are undertaken to take into account the aspects of 
the update  propagation of the  cached  web page components  and  the  algorithm 
of placement to adapt for the proposed solution. 

 
Enhancing Web Caching  Using Web Usage Mining Techniques 
435 
 
Fig. 5. Number  of Frequent Closed  Itemsets and  Items Generated 
References 
1. Challenger, J., Dantzig, P., Iyengar, A., Witting, K.: A fragment-based approach for effi-
ciently creating dynamic web content. ACM Trans. Internet Techn. 5(2), 359–389 (2005) 
2. Chang, C.-Y., Chen, M.-S.: A new cache replacement algorithm for the integration of web 
caching and prefectching. In: Proceedings of CIKM, Virginia, USA, pp. 632–634 (2002) 
3. cooley, R.: Web usage mining: Discovery and application of interesting patterns from web 
data. PhD thesis, University of Minnesota, USA (2000) 
4. Crovella, M., Barford, P.: The network effects for prefetching. In: Proc. IEEE INFOCOM 
1998, pp. 1232–1240 (1998) 
5. Datta, A., Dutta, K., Thomas, H., VanderMeer, D.: A comparative study of alternative 
middle tier caching solutions to support dynamic web content acceleration. In: Proceedings 
of the 27th VLDB Conference, Roma, Italy, pp. 11–14 (2001) 
6. Gouda, K., Zaki, M.-J.: Genmax: An efficient algorithm for mining maximal frequent 
itemsets. Data Mining and Knowledge Discovery (2005) 
7. Jacobson, V.: Congestion avoidance and control. In: Proceedings of ACM SIGCOMM, 
Stanford, CA, USA, pp. 314–329 (1988) 
8. Labrinidis, A., Roussopoulos, N.: On the materialization of web views. In: Proc. of the 
ACM SIGMOD Conference, Philadelphia, Pennsylvania, USA, pp. 367–378 (1999) 
9. Labrinidis, A., Roussopoulos, N.: Web views materialization. In: Proc. of the ACM SIG-
MOD Conference, Dallas, Texas, United States, pp. 79–84 (2000) 
10. Labrinidis, A., Roussopoulos, N.: Online view selection for the web. In: Proc. of the ACM 
SIGMOD Conference, Madison, Wisconsin, pp. 56–68 (2002) 
11. Labrinidis, A., Roussopoulos, N.: Exploring the trade-off between performance and data 
freshness in database-driven web servers. The VLDB Journal 13(3), 240–255 (2004) 
12. Nanopoulos, A., Katsaros, D., Manolopoulos, Y.: Exploiting web log mining for web 
cache enhancement. In: Kohavi, R., Masand, B., Spiliopoulou, M., Srivastava, J. (eds.) 
WebKDD 2001. LNCS (LNAI), vol. 2356, pp. 68–87. Springer, Heidelberg (2002) 
13. Ramaswamy, L., Iyengar, A., Liu, L., Douglis, F.: Automatic detection of fragments in 
dynamically generated web pages. In: Proceedings of the 13th International Conference on 
World Wide Web WWW 2004, New York, USA, pp. 443–454 (2004) 
14. tanasa, D.: Web Usage Mining: Contributions to Intersites Logs Preprocessing and Se-
quential Pattern Extraction with Low Support. PhD thesis, Thesis University of Nice 
Sophia Antipolis, French (2005) 
15. Wu, Y.-H., Chen, A.-L.-P.: Prediction of web page accesses by proxy server log. World 
Wide Web 5(1), 67–88 (2002) 
16. Zaiane, O.-R., Xin, M., Han, J.: Discovering web access patterns and trends by applying 
olap and data mining technologies on web logs. In: Proceedings, IEEE International Forum 
on Research and Technology Advances in Digital Libraries, pp. 19–29 (1998) 
17. Zaki, M.-J., Hisiao, C.-J.: Charm: An efficient algorithm for closed itemset mining. In: 2nd 
SIAM Intl. Conf. on Data Mining, Arlington, VA, USA, pp. 457–473 (2002) 

A. Özcan, N. Chaki, and D. Nagamalai (Eds.): WiMo 2010, CCIS 84, pp. 436–444, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
An Efficient Lightweight Authentication Protocol for 
Mobile Ad Hoc Network 
Heshem A. EL Zouka 
College of Engineering &Technology, Arab Academy for Science &  
Technology and Maritime Transport, Alexandria, Egypt  
helzouka@aast.edu, helzouka@hotmail.com 
Abstract. The aim of this paper is to address the problems associated with the 
current wireless ad-hoc networks, which are characterized by the lack of infra-
structure. This characteristic makes it difficult to apply generic administrative 
approaches to solve problems such as intrusion detection. Due to the dynamic 
changes of topology in such networks, it is difficult to determine whether the 
node participating in the routing domain is Byzantine or indeed an authenticated 
node. The paper discusses also how mobile agent technology can be used to 
perform the authentication mechanism, and hence improve the utilization of the 
network bandwidth and provide an efficient profile management in mobile  
ad-hoc networks. 
Keywords: Security Models, Mobile Ad hoc Networks, Mobile Agents,  
Authentication Mechanisms. 
1   Introduction 
Mobile ad hoc networks (MAHNs) are gaining more and more attention and in the 
near future MAHNs are expected to be used widely in many areas affecting our daily 
lives. Wide applications make use of MAHNs, for example, it can be used in military 
tactical communication networks, emergency relief operations, commercial and edu-
cational use in remote areas and other security sensitive applications [1], [2]. The to-
pology of MAHN consists of a set of mobile nodes cooperating together without the 
aid of base stations, access points or any other central control. Connection in such 
networks is done through wireless communications among the mobile hosts them-
selves in a large mobile dynamic network.  In military applications, security becomes 
a necessity and one of the most concerns. Besides, in military environment further 
recommendations have to be done with the limitation of power and utilization of 
channel, since a mobile node may not be able to establish connection directly with 
other nodes in a single hop fashion. Further, the probability of interception or detec-
tion and jam resistance should be low and address the security issues in such applica-
tions [3]. Thus, any security breaches occurred with these networks may cause the 
security performance to be degraded exponentially.  
Concerning the authentication protocols used in mobile ad hoc networks, for rout-
ing and data packet delivery, previous assumptions were made concerning utilizing 

 
An Efficient Lightweight Authentication Protocol for Mobile Ad Hoc Network 
437 
asymmetric cryptography systems. However, due to the limited energy supply of 
these networks, it is assumed to be infeasible to use Public key algorithm in MAHN, 
considering a critical scale of network sizes.  
On the other hand, many studies append proving the falsehood of such assumptions 
and the ability to form a more secure authentication mechanism under certain ar-
rangements [4], [5]. Stating those facts led us to think about how mobile agent MA 
technology can be used to perform the authentication mechanism, and offers more 
benefits related to bandwidth and latency of the network.  Besides, MA can provide 
an efficient profile management in terms of the applied routing protocols in mobile 
ad-hoc networks. 
2   Mobile Agents Background 
Mobile agents (MAs) appeared as a result of Remote Procedure Code PRC and Re-
mote Evaluation RE evolution. RPC is an Inter-process communication technology 
that allows a computer program to cause a subroutine or procedure to execute in an-
other address space (commonly on another computer on a shared network) without 
the programmer explicitly coding the details for this remote interaction [6], [7].  RE 
on the other hand, sends the executable code to another address space which offers 
more dynamic operations. The executing program of MA can migrate itself from ma-
chine to machine in a heterogeneous network as illustrated in figure 1, which com-
pares the MA the traditional client/server system. 
 
 
 
 
 
 
 
 
 
 
 
 
Fig. 1. Client-Server versus Mobile Agents 
Main important fact about MA is that they decide when and where to move based 
on their preference. Such preferences can be educated by noticing the surrounding 
environments and state which offers a better decision. Further, MA is a special kind 
of software that propagates over the network either periodically or on demand, when 
required by the applications. MAs provide the network with many advantages over 
conventional agents:  

438 
H.A. EL Zouka 
1. Computation bundles: converts computational client/server round trips to re-
locatable data bundles, thus, reducing network load. 
2. Parallel processing: asynchronous execution on multiple heterogeneous net-
work hosts. 
3. Dynamic adaptation: actions are dependent on the state of the host  
environment. 
4. Tolerant to network faults: able to operate without an active connection be-
tween client and server. 
5. Flexible maintenance: to change an agent's actions, only but the source 
(rather than the computation hosts) must be updated. 
Therefore, mobile agent has the unique ability to transport itself from one system to 
another in the same network. This ability allows it to transport itself from one system 
in a network in a similar manner as objects transfer.  
In contrast to Remote evaluation and Code on demand programming paradigms, 
mobile agents are active, as that they can choose to migrate between computers at any 
time during their execution. This makes them a powerful tool for implementing dis-
tributed applications in computer network. Agents typically possess several of the 
following characteristics; they are: 
• 
Autonomous: the agent has to be independent and proactive to changes 
rather than reacting to changes as they occur. 
• 
Adaptive / learning: The agent states learning from the surrounding envi-
ronment and the nearby agents. 
• 
Mobile: The ability for the agent to move towards any direction is very  
important as it makes the agent very flexible. 
• 
Persistent: the ability for the agent to persist in the environment till the agent 
finishes its goal. 
• 
Goal oriented: each agent carries out a certain specific goal in mind to 
achieve. 
• 
Communication / Collaborative: All agents must communicate with each 
other to achieve the best optimal model. 
• 
Flexible: in acquiring and gathering data from different sources. 
3   Routing Protocols 
Different routing protocols in ad hoc networks can be divided into two categories: table 
driven and on-demand routing protocols. These two routing protocols focus on the con-
tent of the data packets they receive and the decision that is made according to the data 
whether to forward or drop the packets based on the authentication mechanism it used.  
For example, in table driven routing protocols, a node sends an interest to all other nodes 
in the pass, and information is maintained at each node. However, in "on demand" rout-
ing protocol, a sensing task computes the routes as it required via  sequence of interac-
tions throughout the network. During the data transmission, a route discovery protocol is 
invoked in attempt to find the proper path to the destination. In recent years, many other 
routing protocols are proposed for mobile ad hoc networks. From security point of view, 
we consider only two widely studied routing protocols; DSR and AODV. [8] 

 
An Efficient Lightweight Authentication Protocol for Mobile Ad Hoc Network 
439 
3.1   Protocol Security 
Dynamic Source Routing (DSR); is a self maintaining protocol, and usually depends on 
the source routing technique, i.e.  it doesn’t use many advertisements that can help it to 
be known to other nodes in the network. It computes the routing paths by using routing 
information of the surrounding nodes and then maintains them. Packets, then, travel 
from node to node depending on the hop cost which is previously stored in sender 
(packet header) information. Security threats could occur intensely or by accidently if 
malicious node successfully launched the rushing attack and placed itself in the estab-
lished routing table. On the other hand, ad hoc On Demand Distance Vector Routing 
Protocol (AODV); is proposed as a combination of DSDV and DSR. It is a route dis-
cover and maintenance protocol. So, if a link fails, additional control packets have to be 
generated and broadcasted in attempt to discover newer paths. The frequent changes in 
topologies in such protocol could cause a security breaches in the network such as: de-
nial of services (DoSs), physical layer jamming, malicious attacks and many others.  
Further, in such protocol, an intruder can degrade communication privacy by  
simply forwarding packets to unauthenticated nodes, and hence network could misin-
terpret the cause of packet drops. The authentication mechanism proposed in this re-
search paper, suggest a model that fixes the problems associated with such a mobile 
ad hoc protocol. Encoding information in less authentication process by means of 
using mobile agents, reduce the consumption of expensive resources such as trans-
mission bandwidth and energy. Once the authentication process is successfully 
passed, the agent facilitate verification by preserving the statistical regularity from 
node to node, and hence improving the utilization of the network and provide an effi-
cient security and trust management in mobile ad hoc network. 
4   Authentication Protocols  
Authentication mechanism is the process which confirms the identity of any user try-
ing to log on to the network or access network resources. Applying cryptography 
models adds a security layer to the implemented authentication protocol, which en-
sures protection for all the communicated nodes in the ad hoc network. Basically, 
there are two encryption models used with authentication protocols, namely symmet-
ric encryption model and asymmetric encryption model. 
In symmetric cryptography model, the encryption key is related to the decryption 
key, they may be identical or there is a simple transformation to go between the two 
keys. The key, in practice, represent a shared secret between two or more parties that 
can be used to maintain a private information link.  
One of the disadvantages in symmetric cryptography is the requirement of a shared 
secret key, with one copy at each side. Such requirement imposes the necessity of a 
secure channel to exchange the shared key. However, symmetric key algorithms are 
generally much less computationally intensive than asymmetric key algorithms. 
Asymmetric Cryptography model employs a pair of different but associated keys. 
The public key is released to the public, while the private key is kept with its owner.  
One of the advantages of employing asymmetric model in the authentication proto-
col is that the sending node and the receiving node have separate secrets and never 
need to send a copy of their private keys to each other.  

440 
H.A. EL Zouka 
This prevents malicious nodes from copying a key while it is in transition. So, in 
asymmetric key algorithm, each communicating node pair, share the public key of the 
other, and they use the private/public algorithm to establish and maintain reliable trust 
relation, and hence, authenticate each other. [9] 
Certificate of CA, certificate authority, is still needed to establish that trust relation 
between any two mobile nodes in the network. The role of the CA is to issue a digital 
certificate that contains a public key and identify of the owner. Thus, the certificate is 
issued for each participating node in the network as a part of a public key infrastruc-
ture (PKI).  In addition, the CA checks with a registration authority (RA) the informa-
tion provided by the requestor of a digital certificate. If the RA verifies the requestor's 
information, the CA can then issue a certificate that will allow the new node to regis-
ter and participate into the network activities. This process is known as key distribu-
tion or key management [10]. 
If CA is unavailable/down, nodes can't get the public key of other nodes to estab-
lish secure communication. Also, If CA key confidentiality is compromised; intruder 
can decipher any communication key in the network.  Further, the intruder can also 
issue public key certificates and sign messages on behalf of the legitimate CA. It is 
more recommended to distribute the trust to a set of nodes in order to solve this prob-
lem. This will allow these nodes to share key management responsibilities [11]. 
5   Security Implications and Solutions 
If the number of users grows in ad hoc network, mobile nodes are very likely to use a 
peer to peer connection, causing a security risk for the network, as illustrated previ-
ously. Further, mutual authentication between adjacent peers is required, since peer 
sending request will need to verify that the request has been received. Thus, running 
authentication/identification protocols between any two communicating nodes in mo-
bile ad hoc network will cause traffic overhead and energy consumption.   
More secure and robust authentication mechanism, as proposed in this paper, will 
preserve the bandwidth and reduce the latency which was caused by the mutual au-
thentication process. A mobile agent system has been employed in this network to 
solve the delay associated with mutual authentication between network nodes. We 
aim at preserving nodes energy as a result of decreasing computation cycles used in 
authenticating mobile nodes. Reducing the communication overheads caused by mu-
tual authentication, will put the whole ad hoc network in a structure which can be 
optimized according to available resources.  
The main important advantage of using MA is their ability to carry out most of the 
authentication process, such as encrypting password, issuing certificates, session level 
encryption and providing confidentiality for the participating mobile nodes. For each 
of these processes, one mobile agent will be assigned for the authentication purposes.  
Then, the agent can migrate at any point during its execution. Further, the MA will 
propagate itself from the host node to the destination nodes incrementally, running the 
mutual authentication processes as shown in figure 2. The research work in this paper 
schedules mobile agent migration in a way that improves the bandwidth performance 
and preserves energy consumption in mobile nodes. 
 

 
An Efficient Lightweight Authentication Protocol for Mobile Ad Hoc Network 
441 
 
Fig. 2. The Authentication Model 
Accordingly, the architecture model lies in having the mobile agent codes perform-
ing some sort of hand checking to verify mutual authentication of its execution  
environment. Once the mobile agents migrate with the knowledge of the destination 
public key, a checking mechanism is performed.  A list of checksum operations are 
performed as the mobile agents traverse through the ad hoc network as illustrated in 
the following pseudo-code: 
STATE1: C1 = checksum (memIoc1, memIoc2 … memIocn); 
STATE2: if (C1 == Check1) 
STATE3: send status1 
STATE4: else 
STATE5: send status2 
STATE6: … 
STATE7: C2 = checksum (memI1, memI2,… memIm); 
STATE8: if (C2 == Check2) 
STATE9: send status3 
STATE10: else 
STATE11: send status4 
Thus, each agent reads a set of memory locations from the host node and authenti-
cates each mobile node whenever "if-bind-operation" is provided. This model assures 
security and protects the network from malicious mobile codes, without generating 
excessive traffic. Moreover, the migration code verifies their execution environment 
and unlimited forms of checks can be executed. However, if the mobile agent deter-
mines a malicious node in its path, it rejects the join request with the network and 
relocates another path to the destination.  

442 
H.A. EL Zouka 
6   Simulation Results  
In order to compare the authentication mechanisms we implemented, we used  
GlomoSim simulation package in the design of lightweight authentication protocol for 
mobile ad hoc network [12], [13]. Firstly, we deployed 200 mobile nodes over an area 
of 100x100 m2 to be used in our authentication model. Then, we increased the num-
ber of nodes to 500 to be scattered over the area. Figure 3 shows the random deploy-
ment of 100 and 500 nodes including the trust center, which maintain and stores 
hashes.  
 
Fig. 3. Initial Deployment of The Network 
The model was implemented with the assumption that all nodes are homogeneous, 
having the same initial energy level of 20 joules, with sustainable energy for the trust 
center. Figure 4 shows the energy consumption of the overall implemented ad hoc 
network in both models. We observe that traditional authentication protocols gener-
ally aim at minimizing the network risk at the expense of energy consumption. 
 
Fig. 4. Accumulated Energy Consumption  

 
An Efficient Lightweight Authentication Protocol for Mobile Ad Hoc Network 
443 
After employing the new authentication method which is based on mobile agent 
technology, the results showed that the energy consumption is significantly improved 
to be the least compared to the traditional mutual authentication method.  
After employing the mobile agent technique in those authentication protocols, it is 
noted that more than 75% of the nodes preserved their energy and provided a more 
balanced energy consumption model, since all nodes perform the process of authenti-
cation locally.  
Further, employing mobile agents in this fashion results in better performance and 
improves network bandwidth. 
7   Conclusion and Future Work 
In this paper we discussed how mobile agent technology can be used to perform the 
authentication mechanism, and hence improve the utilization of the network band-
width and provide an efficient profile management in mobile ad-hoc networks.  
The analysis of experimental results showed that, the traditional authentication al-
gorithms insert too much delay, resulting in poor performance compared to the pro-
posed authentication model. After employing the mobile agent technique in those 
authentication protocols, the results showed that the energy consumption is signifi-
cantly improved and it is noted that more than 70% of the nodes preserved their en-
ergy compared to the other authentication mechanisms.  
On comparing, an efficient utilization of network bandwidth is achieved providing 
more advantages over the other. This is because it allows the authentication protocols 
to deploy their services locally.  
In the future we plan to enhance our algorithm further to handle even the routing 
protocols in mobile ad hoc network. Applying other independent and cooperative  
mobile agents will be also investigated. 
References 
1. Camp, T., Boleng, J., Davies, V.: A Survey of Mobility Models for Ad Hoc Network Re-
search. Wireless Communication and Mobile Computing. Special issue on Mobile Ad Hoc 
Networking: Research, Trends and Applications 2(5), 483–502 (2002) 
2. Hong, X., Gerla, G.P.M., Chiang, C.: A group mobility model for ad-hoc wireless net-
works. In: 2nd ACM International Workshop on Modeling and Simulation of Wireless and 
Mobile Systems (MSWiM 1999), p. 8 (August 1999) 
3. Hu, Y.-C., Perrig, A., Johnson, D.B.: Ariadne: A Secure On-Demand Routing Protocol for 
Ad Hoc Networks. Wireless Networks (WINET), ACM and Springer 11(1-2), 21–38 
(2005) 
4. Myles, A., Johnson, D.B., Perkins, C.: A Mobile Host Protocol Supporting Route Optimi-
zation and Authentication. IEEE Journal on Selected Areas in Communications, special is-
sue on Mobile and Wireless Computing Networks 13(5), 839–849 (1995) 
5. Hubaux, J.-P., Buttyán, L., Capkun, S.: The quest for security in mobile ad hoc networks. 
In: Proceedings of the ACM Symposium on Mobile Ad Hoc Networking and Computing 
(MobiHOC), Long Beach, CA, USA (October 2001) 

444 
H.A. EL Zouka 
6. Qi, H., Xu, Y., Wang, X.: Mobile-agent based collaborative signal and information proc-
essing. Proceedings of the IEEE 91, 1172–1183 (2003) 
7. Lange, D.B., Oshima, M.: Programming and Deploying Java Mobile Agents with Aglets. 
Addison Wesley Longman, Reading (1998) 
8. Buttyán, L., Hubaux, J.-P.: Stimulating cooperation in self-organizing mobile ad hoc net-
works. Mobile Networks and Applications 8(5), 579–592 (2003) 
9. Tung, B., Neuman, C., Hur, M., Medvinsky, A., Medvinsky, S., Wray, J.: Public-key cryp-
tography for initial authentication. In: Chan, A.H., Gligor, V.D. (eds.) ISC 2002. LNCS, 
vol. 2433. Springer, Heidelberg (2002) 
10. Kaufman, C. (ed.): The Internet Key Exchange (IKEv2) Protocol, RFC 4306 (December 
2005) 
11. Eastlake, D., Jones, P.: US Secure Hash Algorithm 1 (SHA1), RFC 3174, Motorola and 
Cisco (September 2001) 
12. Takai, M., Ahuja, R., Tang, K., Bagrodia, R., Gerla, M.: GloMoSim: A Scalable Network 
Simulation Environment. UCLA Computer Science Department Technical Report 990027 
(1999) 
13. Mostafa, S., Zouka, H.E., Naser, M.A.E.: Hybrid Encryption Secure Routing Protocols for 
Wireless Sensor Network. In: 1st International Conference on Sensor Networks and Appli-
cations (SNA), San Francisco, CA, pp. 109–114 (November 2009) 

Author Index
Abdullah, Abdul Hanan
87
Ahamed, Syed Vickar
266, 356, 367, 409
Alag¨oz, Fatih
99
Alaybeyoglu, Aysegul
295
Albayrak, Sahin
229
Al-Majeed, Salah S.
34
Alsabaan, Maazen
217
Bayhan, Suzan
99
Birant, Derya
13
Bouallegue, Ridha
241
Casilari, Eduardo
273
Cavenaghi, Marcos Antˆonio
417
Chaki, Nabendu
23
Chang, Zhijiang
177
Choudhury, Sankhayan
23
Civriz, Mert
13
Derhab, Abdelouahid
202
Dolama, Mustafa Vahabzadeh
46
Ejlali, Alireza
327
El Sayed, Nadim
229
EL Zouka, Heshem A.
436
Eljetlawi, Ali Mohamed
379
Enami, Neda
283
Ercan, Tuncay
315
Erciyes, Kayhan
295, 304
Erdogan, Sevki S.
266, 356, 367
Ers¨oz, Seda Demira˘g
99
Esmaeelzadeh, Vahid
327
Feh´er, G´abor
59
Fleury, Martin
34
Gaydadjiev, Georgi
177
Gercek, Mehmet Kazim
315
Haddad, Rim
241
Haghighat, Abolfazl
283
Hamdy, Mohamed
202
Hong, James Won-Ki
163
Hosseini, Elahe S.
327
Ismail, Abdul Samad
87
Ithnin, Noraﬁda
379
Jedidi, Azza
148
Jeong, Hong-Jong
1
Kantarci, Aylin
295
Kenyeres, P´eter
59
Kim, Dongkyun
1
Kim, Sung-Su
163
K¨onig-Ries, Birgitta
202
Koyuncu, Murat
315
Kut, Alp
13
Lobato, Renata Spolon
417
Mandala, Satria
87
Mantar, Hacı A.
112, 137
Mathew, Jimson
123
Meghanathan, Natarajan
254
M´esz´aros, Tam´as
59
Mishra, Rakesh Kumar
23
Moghadam, Reza Askari
283
Naik, Kshirasagar
217
Nayak, Amiya
217
Ngadi, M.A.
87
Park, Jungsoo
1
Peterson, Michael R.
367
Pradhan, Dhiraj K.
123
Preveze, Barbaros
71
Rabiee, Hamid R.
327
Rahbar, Akbar Ghaﬀarpour
46
Rahman, Syed M.
356, 409
Rasslan, Mohamed
389
S¸afak, Aysel
71
Saidi, Samia
425
Salamah, Muhammed
338
Sivrikaya, Fikret
229
Slimani, Yahya
425
Sogukpinar, Ibrahim
398
Spolon, Roberta
417

446
Author Index
Strassner, John
163
Szentgy¨orgyi, Attila
59
Taleb, Anas Abu
123
Tansu, Fatma
338
Teixeira, Rafael Augusto
417
Tellioglu, Ismail
112
Toker, Ahmet Cihat
229
Trivi˜no, Alicia
273
Trujillo, Francisco D.
273
T¨uys¨uz, M. Fatih
137
Ugur, Alper
398
Vanhatupa, Juha-Matti
349
Wang, Lu
190
Weis, Fr´ed´eric
148
Wu, Wei
190
Yassaei, Mahshid
327
Yilmaz, Onur
304
Yoo, Hongseok
1
Youssef, Amr
389
Yuste, Antonio J.
273


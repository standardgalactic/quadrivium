
Digital Signal Processing
Using MATLAB
®
A PROBLEM SOLVING COMPANION
Fourth Edition
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Digital Signal Processing
Using MATLAB
®
A PROBLEM SOLVING COMPANION
Fourth Edition
Vinay K. Ingle
John G. Proakis
Northeastern University
Australia • Brazil • Mexico • Singapore • United Kingdom • United States
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

       This is an electronic version of the print textbook. Due to electronic rights restrictions,
some third party content may be suppressed. Editorial review has deemed that any suppressed 
content does not materially affect the overall learning experience. The publisher reserves the right 
to remove content from this title at any time if subsequent rights restrictions require it. For
valuable information on pricing, previous editions, changes to current editions, and alternate 
formats, please visit www.cengage.com/highered to search by ISBN#, author, title, or keyword for 
materials in your areas of interest.
       Important Notice: Media content referenced within the product description or the product 
text may not be available in the eBook version.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Digital Signal Processing Using MATLAB®: 
A Problem Solving Companion, Fourth Edition
Vinay K. Ingle and John G. Proakis
Product Director, Global Engineering: 
Timothy L. Anderson
Media Assistant: Ashley Kaupert
Product Assistant: Teresa Versaggi
Marketing Manager: Kristin Stine
Director, Content and Media Production: 
Sharon L. Smith
Senior Content Project Manager: Jennifer Risden
Production Service:  MPS Limited
Copyeditor:  Richard Camp
Proofreader: Jennifer Grubba
Indexer: Larry Sweazy
Compositor: MPS Limited
Senior Art Director: Michelle Kunkler
Internal Designer: Carmela Periera
Cover Designer:  Jennifer Wahi
Cover Image: Zeljko Radojko/Shutterstock.com
Intellectual Property 
Analyst: Christine Myaskovsky
Project Manager: Sarah Shainwald
Text and Image Permissions Researcher: 
Kristiina Paul
Manufacturing Planner: Doug Wilke
© 2017, 2012 Cengage Learning®
ALL RIGHTS RESERVED. No part of this work covered by the copyright 
herein may be reproduced, transmitted, stored, or used in any form or by 
any means graphic, electronic, or mechanical, including but not limited to 
photocopying, recording, scanning, digitizing, taping, Web distribution, 
information networks, or information storage and retrieval systems, 
except as permitted under Section 107 or 108 of the 1976 United States 
Copyright Act, without the prior written permission of the publisher.
Library of Congress Control Number: 2015944167
ISBN: 978-1-305-63512-8
Cengage Learning
20 Channel Center Street
Boston, MA 02210
USA
Cengage Learning is a leading provider of customized learning 
solutions with employees residing in nearly 40 diﬀ erent countries 
and sales in more than 125 countries around the world. Find your local 
representative at www.cengage.com.
Cengage Learning products are represented in Canada by Nelson 
Education Ltd.
To learn more about Cengage Learning Solutions, visit 
www.cengage.com/engineering.
Purchase any of our products at your local college store or at our 
preferred online store www.cengagebrain.com.
MATLAB is a registered trademark of The MathWorks, 
3 Apple Hill Drive, Natick, MA.
For product information and technology assistance, contact us at
Cengage Learning Customer & Sales Support, 1-800-354-9706.
For permission to use material from this text or product, 
submit all requests online at www.cengage.com/permissions.
Further permissions questions can be emailed to 
permissionrequest@cengage.com.
Printed in the United States of America
Print Number: 01 Print Year: 2015
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.
WCN: 02-200-203

v
Contents
PREFACE
xi
1
INTRODUCTION
1
1.1
Overview of Digital Signal Processing
2
1.2
A Brief Introduction to MATLAB
5
1.3
Applications of Digital Signal Processing
18
1.4
Brief Overview of the Book
20
2
DISCRETE-TIME SIGNALS AND SYSTEMS
22
2.1
Discrete-Time Signals
22
2.2
Discrete Systems
36
2.3
Convolution
40
2.4
Diﬀerence Equations
47
2.5
Problems
53
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

vi
CONTENTS
3
THE DISCRETE-TIME FOURIER ANALYSIS
59
3.1
The Discrete-Time Fourier Transform (DTFT)
59
3.2
The Properties of the DTFT
67
3.3
The Frequency Domain Representation
of LTI Systems
74
3.4
Sampling and Reconstruction of Analog Signals
80
3.5
Problems
97
4
THE z-TRANSFORM
103
4.1
The Bilateral z-Transform
103
4.2
Important Properties of the z-Transform
107
4.3
Inversion of the z-Transform
112
4.4
System Representation in the z-Domain
118
4.5
Solutions of the Diﬀerence Equations
128
4.6
Problems
134
5
THE DISCRETE FOURIER TRANSFORM
141
5.1
The Discrete Fourier Series
142
5.2
Sampling and Reconstruction in the z-Domain
149
5.3
The Discrete Fourier Transform
154
5.4
Properties of the Discrete Fourier Transform
165
5.5
Linear Convolution Using the DFT
180
5.6
The Fast Fourier Transform
187
5.7
Problems
200
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

CONTENTS
vii
6
IMPLEMENTATION OF DISCRETE-TIME
FILTERS
212
6.1
Basic Elements
213
6.2
IIR Filter Structures
214
6.3
FIR Filter Structures
228
6.4
Overview of Finite-Precision Numerical Eﬀects
239
6.5
Representation of Numbers
240
6.6
The Process of Quantization and Error
Characterizations
255
6.7
Quantization of Filter Coeﬃcients
262
6.8
Problems
277
7
FIR FILTER DESIGN
291
7.1
Preliminaries
292
7.2
Properties of Linear-Phase FIR Filters
295
7.3
Window Design Technique
309
7.4
Frequency-Sampling Design Technique
330
7.5
Optimal Equiripple Design Technique
344
7.6
Problems
360
8
IIR FILTER DESIGN
370
8.1
Some Preliminaries
371
8.2
Some Special Filter Types
374
8.3
Characteristics of Prototype Analog Filters
385
8.4
Analog-to-Digital Filter Transformations
407
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

viii
CONTENTS
8.5
Lowpass Filter Design Using MATLAB
427
8.6
Frequency-Band Transformations
432
8.7
Problems
445
9
SAMPLING RATE CONVERSION
458
9.1
Introduction
459
9.2
Decimation by a Factor D
461
9.3
Interpolation by a Factor I
470
9.4
Sampling Rate Conversion by a Rational Factor I/D
477
9.5
FIR Filter Designs for Sampling Rate Conversion
482
9.6
FIR Filter Structures for Sampling Rate Conversion
500
9.7
Problems
510
10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
518
10.1
Analysis of A/D Quantization Noise
518
10.2
Round-OﬀEﬀects in IIR Digital Filters
530
10.3
Round-OﬀEﬀects in FIR Digital Filters
557
10.4
Problems
569
11
APPLICATIONS IN ADAPTIVE FILTERING
573
11.1
LMS Algorithm for Coeﬃcient Adjustment
575
11.2
System Identiﬁcation or System Modeling
578
11.3
Suppression of Narrowband Interference in
a Wideband Signal
579
11.4
Adaptive Line Enhancement
582
11.5
Adaptive Channel Equalization
582
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

CONTENTS
ix
12
APPLICATIONS IN COMMUNICATIONS
586
12.1
Pulse-Code Modulation
586
12.2
Diﬀerential PCM (DPCM)
590
12.3
Adaptive PCM and DPCM (ADPCM)
593
12.4
Delta Modulation (DM)
597
12.5
Linear Predictive Coding (LPC) of Speech
601
12.6
Dual-Tone Multifrequency (DTMF) Signals
605
12.7
Binary Digital Communications
609
12.8
Spread-Spectrum Communications
611
13
RANDOM PROCESSES*
614
13.1
Random Variable
615
13.2
A Pair of Random Variables
628
13.3
Random Signals
642
13.4
Power Spectral Density
650
13.5
Stationary Random Processes through LTI
Systems
658
13.6
Useful Random Processes
668
13.7
Summary and References
684
14
LINEAR PREDICTION AND OPTIMUM
LINEAR FILTERS*
686
14.1
Innovations Representation of a Stationary
Random Process
687
*Chapters 13–15 are available for download from the Instructor’s Companion Website
at www.cengage.com/login.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

x
CONTENTS
14.2
Forward and Backward Linear Prediction
701
14.3
Solution of the Normal Equations
717
14.4
Properties of the Linear Prediction-Error Filters
730
14.5
AR Lattice and ARMA Lattice-Ladder Filters
734
14.6
Wiener Filters for Filtering and Prediction
743
14.7
Summary and References
766
15
ADAPTIVE FILTERS*
769
15.1
Applications of Adaptive Filters
769
15.2
Adaptive Direct-Form FIR Filters
815
15.3
Summary and References
849
BIBLIOGRAPHY
B-1
INDEX
I-1
*Chapters 13–15 are available for download from the Instructor’s Companion Website
at www.cengage.com/login.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xi
Preface
Since the beginning of the 1980s, we have witnessed a revolution in
computer technology and an explosion in user-friendly applications. This
revolution is still continuing today, with low-cost laptop systems that rival
the performance of expensive workstations. This technological prowess
should be brought to bear on the educational process and, in particular,
on eﬀective teaching that can result in enhanced learning. This problem-
solving companion book on digital signal processing (DSP) makes a con-
tribution toward reaching that goal. The fourth edition continues our in-
novative approach of blending MATLAB®-based learning with traditional
teaching to advanced DSP topics such as optimal and adaptive ﬁlters.
The teaching methods in signal processing have changed over the
years from the simple “lecture-only” format to a more integrated “lecture-
laboratory” environment in which practical hands-on issues are taught
using DSP hardware. However, for eﬀective teaching of DSP, the lecture
component must also make extensive use of computer-based explanations,
examples, and exercises. For the past three decades, the MATLAB soft-
ware developed by The MathWorks, Inc. has established itself as the de
facto standard for numerical computation in the signal-processing com-
munity and as a platform of choice for algorithm development. There are
several reasons for this development, but the most important reason is
that MATLAB is available on practically all computing platforms. In this
book, we have made an attempt at integrating MATLAB with traditional
topics in DSP so that it can be used to explore diﬃcult topics and solve
problems to gain insight. Many problems or design algorithms in DSP re-
quire considerable amount of computation. It is for these that MATLAB
provides a convenient tool so that multiple scenarios can be tried with
ease. Such an approach can enhance the learning process.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xii
PREFACE
SCOPE OF THE BOOK
This book is primarily intended for use as a problem-solving companion
book in senior-level undergraduate or ﬁrst-year graduate courses on DSP.
Although we assume that the student (or user) is familiar with the funda-
mentals of MATLAB, we have provided a brief introduction to MATLAB
in Chapter 1. This book is not written as a textbook in DSP, because
of the ready availability of excellent textbooks. What we have tried to
do is to provide enough depth to the material augmented by MATLAB
functions and examples so that the presentation is consistent, logical, and
enjoyable. Therefore, this book can also be used as a self-study guide by
anyone interested in DSP.
WHAT IS NEW IN THE FOURTH EDITION
• A new Chapter 13 provides a review on random variables and random
processes, including bandpass processes. Extensive use of MATLAB
examples makes these topics easier to understand.
• A new Chapter 14 discusses linear prediction and optimal (or Wiener)
ﬁlters, preparing students for graduate studies.
• A new Chapter 15 deals with theory and applications of adaptive ﬁl-
ters. This chapter contains easy-to-understand LMS and RLS algo-
rithms with an extensive set of practical applications, including system
identiﬁcation, echo and noise cancellation, and adaptive arrays. All al-
gorithms and applications are explained and analyzed using MATLAB.
• The coverage of lattice/ladder ﬁlters has moved from Chapter 6 to
Chapter 14 for a more logical presentation of information.
• All MATLAB functions and scripts have been tested and updated so
that they can execute on MATLAB-2014b version and later. Similarly,
all MATLAB plots have been recreated with superior graphic elements.
• We have trimmed many included MATLAB scripts from their plotting
commands to streamline their appearance and to reduce unnecessary
printing. However, all scripts and functions will be made available in
their entirety on the book website.
ORGANIZATION OF THE BOOK
The ﬁrst ten chapters of this book discuss traditional material typically
covered in an introductory course on DSP. The next two chapters are
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

PREFACE
xiii
presented as applications in DSP, with emphasis on MATLAB-based
projects. The last three chapters deal with advanced material in DSP and
are intended for graduate studies. In order to keep the size and the cost
of the book down, we have provided these last three chapters through our
book’s instructor website. Information on how to obtain these chapters
is provided in the next section. The following is a list of chapters and a
brief description of their contents.
Chapter 1, Introduction: This chapter introduces readers to the discipline
of signal processing and presents several applications of digital signal
processing, including musical sound processing, echo generation, echo
removal, and digital reverberation. A brief introduction to MATLAB
is also provided.
Chapter 2, Discrete-Time Signals and Systems: This chapter provides a
brief review of discrete-time signals and systems in the time domain.
Appropriate use of MATLAB functions is demonstrated.
Chapter 3, The Discrete-Time Fourier Analysis: This chapter discusses
discrete-time signal and system representation in the frequency domain.
Sampling and reconstruction of analog signals are also presented.
Chapter 4, The z-Transform: This chapter provides signal and sys-
tem description in the complex frequency domain. MATLAB tech-
niques are introduced to analyze z-transforms and to compute inverse
z-transforms. Solutions of diﬀerence equations using the z-transform
and MATLAB are provided.
Chapter 5, The Discrete Fourier Transform: This chapter is devoted to the
computation of the Fourier transform and its eﬃcient implementation.
The discrete Fourier series is used to introduce the discrete Fourier
transform, and several of its properties are demonstrated using
MATLAB. Topics such as fast convolution and fast Fourier transform
are thoroughly discussed.
Chapter 6, Implementation of Discrete-Time Filters: This chapter dis-
cusses several structures for the implementation of digital ﬁlters.
Several useful MATLAB functions are developed for the determination
and implementation of these structures. In addition to considering
various ﬁlter structures, we also treat quantization eﬀects when ﬁnite-
precision arithmetic is used in the implementation of IIR and FIR
ﬁlters.
Chapter 7, FIR Filter Design: This chapter and the next introduce the im-
portant topic of digital ﬁlter design. Three important design techniques
for FIR ﬁlters—namely, window design, frequency sampling design, and
the equiripple ﬁlter design—are discussed. Several design examples are
provided using MATLAB.
Chapter 8, IIR Filter Design: Included in this chapter are techniques used
in IIR ﬁlter design. The chapter begins with the treatment of some
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xiv
PREFACE
basic ﬁlter types—namely, digital resonators, notch ﬁlters, comb ﬁl-
ters, allpass ﬁlters, and digital sinusoidal oscillators. This is followed
by a brief description of the characteristics of three widely used analog
ﬁlters. Transformations are described for converting these prototype
analog ﬁlters into diﬀerent frequency-selective digital ﬁlters. The chap-
ter concludes with several IIR ﬁlter designs using MATLAB.
Chapter 9, Sampling Rate Conversion: This chapter treats the important
problem of sampling rate conversion in digital signal processing. Topics
treated include decimation and interpolation by integer factors, sam-
pling rate conversion by a rational factor, and polyphase ﬁlter struc-
tures for sampling rate conversion.
Chapter 10, Round-Off Effects in Digital Filters: The focus of this chap-
ter is on the eﬀects of ﬁnite-precision arithmetic to the ﬁltering aspects
in signal processing. Quantization noise introduced in analog-to-digital
conversion is characterized statistically, and the quantization eﬀects in
ﬁnite-precision multiplication and additions are also modeled statisti-
cally. The eﬀects of these errors in the ﬁlter output are characterized as
correlated errors, called limit cycles, and as uncorrelated errors, called
round-oﬀnoise.
Chapter 11, Applications in Adaptive Filtering: This chapter is the ﬁrst
of two chapters on projects using MATLAB. Included is an introduc-
tion to the theory and implementation of adaptive FIR ﬁlters with
projects in system identiﬁcation, interference suppression, narrowband
frequency enhancement, and adaptive equalization.
Chapter 12, Applications in Communications: This chapter focuses on
several projects dealing with waveform representation and coding
and with digital communications. Included is a description of pulse-
code modulation (PCM), diﬀerential PCM (DPCM) and adaptive
DPCM (ADPCM), delta modulation (DM) and adaptive DM (ADM),
linear predictive coding (LPC), generation and detection of dual-
tone multifrequency (DTMF) signals, and a description of signal de-
tection applications in binary communications and spread-spectrum
communications.
Chapter 13, Random Processes: This is the ﬁrst of the last three chap-
ters that are available online through the website for the book. In this
chapter, we provide a brief review of analytical concepts in random
signals that model waveform variations and provide sound techniques
to calculate the response of linear ﬁlters to random signals. We begin
by deﬁning probability functions and statistical averages, and continue
with pairs of random variables. These concepts are extended to random
signals, in terms of second-order statistics, and then delve into station-
ary and ergodic processes, correlation functions, and power spectra. We
apply this theory to processing of random signals through LTI systems
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

PREFACE
xv
using both the time and frequency domains. Finally, we discuss a few
representative random processes, including Gaussian, Markov, white
noise, and ﬁltered noise processes.
Chapter 14, Linear Prediction and Optimum Linear Filters: In this chap-
ter, we treat the problem of optimum ﬁlter design from a statistical
viewpoint. The ﬁlters are constrained to be linear, and the optimiza-
tion criterion is based on the minimization of the mean square error. We
discuss the design of optimum ﬁlters for linear prediction, which has
applications in speech signal processing, image processing, and noise
suppression in communication systems. This design technique requires
the solution of a set of linear equations with special symmetry. We de-
scribe two algorithms, Levinson–Durbin and Schur, which provide the
solution to the equations through computationally eﬃcient procedures
that exploit the symmetry properties. The last section of this chapter
treats an important class of optimum ﬁlters called Wiener ﬁlters, which
are widely used in many applications involving the estimation of signals
corrupted with additive noise.
Chapter 15, Adaptive Filters: The focus of this chapter is on adaptive fil-
ters, which have adjustable coeﬃcients for use in applications in which
the ﬁlter coeﬃcients cannot be designed a priori due to unknown or
changing statistics. We begin with several practical applications in
which adaptive ﬁlters have been successfully used in the estimation of
signals corrupted by noise and other interference. Adaptive ﬁlters incor-
porate algorithms that allow the ﬁlter coeﬃcients to adapt to changes
in the signal statistics. We describe two basic algorithms: the least-
mean-square (LMS) algorithm, which is based on gradient optimization
for determining the coeﬃcients, and the class of recursive least-squares
(RLS) algorithms.
ABOUT THE ONLINE RESOURCES
This book is an outgrowth of our teaching of a MATLAB-based under-
graduate DSP course over several years. Most of the MATLAB functions
discussed in this book were developed for this course. These functions
are collected in the book toolbox called DSPUM v4 and are available
online on the book’s companion website. Many examples in the book con-
tain MATLAB scripts. Similarly, many ﬁgure plots were created using
MATLAB scripts. All these scripts are made available at the companion
website for the beneﬁt of students and instructors. Students should study
these scripts to gain insight into MATLAB procedures. We will appre-
ciate any comments, corrections, or compact coding of these functions
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

xvi
PREFACE
and scripts. Solutions to problems and the associated script ﬁles will be
made available to instructors adopting the book through the companion
website.
To access the book’s companion website and all additional course
materials, please visit www.cengage.com/login. After signing in, search
for the ISBN of your title (from the back cover of your book) using the
search box at the top of the page. This will take you to the companion
site where these resources can be found.
Further information about MATLAB and related publications may
be obtained from:
The MathWorks, Inc.
Natick, MA 01760
Phone: (508) 647-7000
Fax: (508) 647-7001
E-mail: info@mathworks.com
WWW: http://www.mathworks.com
ACKNOWLEDGMENTS
We are indebted to numerous students in our undergraduate DSP course
at Northeastern University who provided us a forum to test teaching ideas
using MATLAB and who endured our constant emphasis on MATLAB.
Many eﬃcient MATLAB functions used in this book were developed
by some of these students. We are also indebted to reviewers of the
original edition, whose constructive criticism resulted in a better presen-
tation of the material: Abeer A. H. Alwan, University of California, Los
Angeles; Steven Chin, Catholic University; Prof. Huaichen, Xidian Uni-
versity, P. R. China; and Joel Trussel, North Carolina State University.
The following reviewers provided additional encouragement, numerous
reﬁnements, and useful comments for the second edition: Jyotsna Bapat,
Fairleigh Dickinson University; David Clark, California State Polytechnic
University; Artyom Grigoryan, University of Texas, San Antonio; Tao Li,
University of Florida; and Zixiang Xiong, Texas A & M University. Based
on their use of the second edition, the following reviewers provided sev-
eral suggestions, changes, and modiﬁcations that led to the third edition:
Kalyan Mondal, Fairleigh Dickinson University; Artyom M. Grigoryan,
University of Texas at San Antonio; A. David Salvia, Pennsylvania State
University; Matthew Valenti, West Virginia University; and Christopher
J. James, University of Southampton, UK. Finally, the fourth edition
was motivated by the constructive feedback and comments provided by
Wasfy B. Mikhael, University of Central Florida; Hongbin Li, Stevens
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

PREFACE
xvii
Institute of Technology; Robert Paz, New Mexico State University;
and Ramakrishnan Sundaram, Gannon University. We sincerely thank
all of them.
We would also like to take this opportunity to acknowledge several
people at Cengage Learning without whom this project would not have
been possible. We thank the Product Director, Timothy Anderson, who
oversees the Global Engineering publishing program at Cengage, for en-
couraging the fourth edition. Media Assistant Ashley Kaupert worked on
the revisions and helped see the fourth edition through development to
production. This project could not have been completed within time lim-
its without their constant push. Senior Content Project Manager Jennifer
Risden oversaw the book’s production process. We thank them all for
their professional help. Finally, we express our sincere gratitude to Richard
Camp for his diligent copy editing and everyone at Cengage Learning who
aided in the development of this edition.
Vinay K. Ingle
John G. Proakis
Boston, Massachusetts
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

1
C H A P T E R 1
Introduction
During the past several decades, the ﬁeld of digital signal processing
(DSP) has grown to be important, both theoretically and technologically.
A major reason for its success in industry is the development and use
of low-cost software and hardware. New technologies and applications in
various ﬁelds are now taking advantage of DSP algorithms. This will lead
to a greater demand for electrical and computer engineers with a back-
ground in DSP. Therefore, it is necessary to make DSP an integral part
of any electrical engineering curriculum.
Three decades ago an introductory course on DSP was given mainly
at the graduate level. It was supplemented by computer exercises on ﬁlter
design, spectrum estimation, and related topics using mainframe (or mini)
computers. However, considerable advances in personal computers and
software during the past three decades have made it necessary to introduce
a DSP course to undergraduates. Since DSP applications are primarily
algorithms that are implemented either on a DSP processor [36] or in
software, a fair amount of programming is required. Using interactive
software, such as MATLAB, it is now possible to place more emphasis
on learning new and diﬃcult concepts than on programming algorithms.
Interesting practical examples can be discussed, and useful problems can
be explored.
With this philosophy in mind, we have developed this book as a com-
panion book (to traditional textbooks like [71, 79]) in which MATLAB is
an integral part in the discussion of topics and concepts. We have chosen
MATLAB as the programming tool primarily because of its wide avail-
ability on computing platforms in many universities across the world.
Furthermore, a low-cost student version of MATLAB has been available
for several years, placing it among the least expensive software products
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

2
Chapter 1
INTRODUCTION
for educational purposes. We have treated MATLAB as a computational
and programming toolbox containing several tools (sort of a super calcu-
lator with several keys) that can be used to explore and solve problems
and, thereby, enhance the learning process.
This book is written at an introductory level in order to introduce
undergraduate students to the exciting and practical ﬁeld of DSP. We
emphasize that this is not a textbook in the traditional sense but a com-
panion book in which more attention is given to problem solving and
hands-on experience with MATLAB. Similarly, it is not a tutorial book in
MATLAB. We assume that the student is familiar with MATLAB and is
currently taking a course in DSP. The book provides basic analytical tools
needed to process real-world signals (a.k.a. analog signals) using digital
techniques. We deal mostly with discrete-time signals and systems, which
are analyzed in both the time and the frequency domains. The analysis
and design of processing structures called filters and spectrum analyzers
are among some of the most important aspects of DSP and are treated in
great detail in this book. Similarly, the topics of ﬁnite word-length eﬀects
on ﬁlter performance as well as on ﬁlter output and the sampling-rate
conversion between two DSP systems are of practical signiﬁcance. These
are also treated extensively in this book. To further our philosophy of
MATLAB-based learning to advanced topics taught in graduate courses,
we have also included some material from statistical and adaptive signal
processing areas such as random signals, linear prediction, optimal ﬁlters,
and adaptive ﬁlters.
In this chapter, we provide a brief overview of DSP and an introduc-
tion to MATLAB.
1.1 OVERVIEW OF DIGITAL SIGNAL PROCESSING
In this modern world, we are surrounded by all kinds of signals in
various forms. Some of the signals are natural, but most of the signals
are man-made. Some signals are necessary (speech), some are pleasant
(music), while many are unwanted or unnecessary in a given situation.
In an engineering context, signals are carriers of information, both useful
and unwanted. Therefore, extracting or enhancing the useful informa-
tion from a mix of conﬂicting information is the simplest form of signal
processing. More generally, signal processing is an operation designed for
extracting, enhancing, storing, and transmitting useful information. The
distinction between useful and unwanted information is often subjective
as well as objective. Hence signal processing tends to be application
dependent.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Overview of Digital Signal Processing
3
1.1.1 HOW ARE SIGNALS PROCESSED?
The signals that we encounter in practice are mostly analog signals. These
signals, which vary continuously in time and amplitude, are processed
using electrical networks containing active and passive circuit elements.
This approach is known as analog signal processing (ASP)—for example,
radio and television receivers.
Analog signal: xa(t) −→Analog signal processor −→ya(t) :Analog signal
They can also be processed using digital hardware containing adders,
multipliers, and logic elements or using special-purpose microprocessors.
However, one needs to convert analog signals into a form suitable for
digital hardware. This form of the signal is called a digital signal. It takes
one of the ﬁnite number of values at speciﬁc instances in time, and hence
it can be represented by binary numbers, or bits. The processing of digital
signals is called DSP; in block diagram form it is represented by
Analog
Equivalent Analog Signal Processor
PrF
ADC
Digital
DSP
Digital
Discrete System
DAC
PoF
Analog
The various block elements are discussed as follows.
PrF: This is a preﬁlter or an antialiasing ﬁlter, which conditions the analog
signal to prevent aliasing.
ADC: This is an analog-to-digital converter, which produces a stream of
binary numbers from analog signals.
Digital Signal Processor: This is the heart of DSP and can represent a general-
purpose computer or a special-purpose processor, or digital hardware,
and so on.
DAC: This is the inverse operation to the ADC, called a digital-to-analog
converter, which produces a staircase waveform from a sequence of
binary numbers, a ﬁrst step toward producing an analog signal.
PoF: This is a postﬁlter to smooth out staircase waveform into the desired
analog signal.
It appears from the above two approaches to signal processing, analog
and digital, that the DSP approach is the more complicated, containing
more components than the “simpler looking” ASP. Therefore, one might
ask, Why process signals digitally? The answer lies in the many advan-
tages oﬀered by DSP.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

4
Chapter 1
INTRODUCTION
1.1.2 ADVANTAGES OF DSP OVER ASP
A major drawback of ASP is its limited scope for performing complicated
signal-processing applications. This translates into nonﬂexibility in pro-
cessing and complexity in system designs. All of these generally lead to
expensive products. On the other hand, using a DSP approach, it is pos-
sible to convert an inexpensive personal computer into a powerful signal
processor. Some important advantages of DSP are these:
1. Systems using the DSP approach can be developed using software run-
ning on a general-purpose computer. Therefore, DSP is relatively con-
venient to develop and test, and the software is portable.
2. DSP operations are based solely on additions and multiplications, lead-
ing to extremely stable processing capability—for example, stability
independent of temperature.
3. DSP operations can easily be modiﬁed in real time, often by simple
programming changes or by reloading of registers.
4. DSP has lower cost due to VLSI technology, which reduces costs of
memories, gates, microprocessors, and so forth.
The principal disadvantage of DSP is the limited speed of operations
due to the DSP hardware, especially at very high frequencies. Primarily
because of its advantages, DSP is now becoming a ﬁrst choice in many
technologies and applications, such as consumer electronics, communica-
tions, wireless telephones, and medical imaging.
1.1.3 TWO IMPORTANT CATEGORIES OF DSP
Most DSP operations can be categorized as being either signal analysis
tasks or signal filtering tasks:
Digital Signal
Analysis
Digital Filter
Measurements
Digital Signal
Signal analysis
This task deals with the measurement of signal prop-
erties. It is generally a frequency-domain operation. Some of its applica-
tions are
• spectrum (frequency and/or phase) analysis,
• speech recognition,
• speaker veriﬁcation, and
• target detection.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
5
Signal filtering
This task is characterized by the signal-in signal-out
situation. The systems that perform this task are generally called filters.
It is usually (but not always) a time-domain operation. Some of the ap-
plications are
• removal of unwanted background noise,
• removal of interference,
• separation of frequency bands, and
• shaping of the signal spectrum.
In some applications, such as voice synthesis, a signal is ﬁrst analyzed
to study its characteristics, which are then used in digital ﬁltering to
generate a synthetic voice.
1.2 A BRIEF INTRODUCTION TO MATLAB
MATLAB is an interactive, matrix-based system for scientiﬁc and engi-
neering numeric computation and visualization. Its strength lies in the fact
that complex numerical problems can be solved easily and in a fraction
of the time required by a programming language such as Fortran or C. It
is also powerful in the sense that, with its relatively simple programming
capability, MATLAB can be easily extended to create new commands and
functions.
MATLAB is available in a number of computing environments: PCs
running all ﬂavors of Windows, Apple Macs running OS-X, UNIX/Linux
workstations, and parallel computers. The basic MATLAB program is
further enhanced by the availability of numerous toolboxes (collections of
specialized functions in speciﬁc topics) over the years. The information in
this book generally applies to all these environments. In addition to the
basic MATLAB product, the Signal Processing toolbox (SP toolbox) is
required for this book. The original development of the book was done us-
ing the professional version 3.5 running under DOS. The MATLAB scripts
and functions described in the book were later extended and made com-
patible with the present version of MATLAB. Furthermore, through the
services of www.cengagebrain.com, every eﬀort will be made to preserve
this compatibility under future versions of MATLAB.
In this section, we will undertake a brief review of MATLAB. The
scope and power of MATLAB go far beyond the few topics discussed in
this section. For more detailed tutorial-based discussion, students and
readers new to MATLAB should also consult several excellent reference
books available in the literature, including [29], [35], and [76]. The infor-
mation given in all these references, along with the online MATLAB’s
help facility, usually is suﬃcient to enable readers to use this book.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

6
Chapter 1
INTRODUCTION
The best approach to become familiar with MATLAB is to open a
MATLAB session and experiment with various operators, functions, and
commands until their use and capabilities are understood. Then one can
progress to writing simple MATLAB scripts and functions to execute a
sequence of instructions to accomplish an analytical goal.
1.2.1 GETTING STARTED
The interaction with MATLAB is through the command window of its
graphical user interface (GUI). In the command window, the user types
MATLAB instructions, which are executed instantaneously, and the re-
sults are displayed in the window. In the MATLAB command window,
the characters “>>” indicate the prompt that is waiting for the user to
type a command to be executed. For example,
>> command;
means an instruction command has been issued at the MATLAB prompt.
If a semicolon (;) is placed at the end of a command, then all output from
that command is suppressed. Multiple commands can be placed on the
same line, separated by semicolons. Comments are marked by the percent
sign (%), in which case MATLAB ignores anything to the right of the sign.
The comments allow the reader to follow code more easily. The integrated
help manual provides help for every command through the fragment
>> help command;
which will provide information on the inputs, outputs, usage, and func-
tionality of the command. A complete listing of commands sorted by
functionality can be obtained by typing help at the prompt.
There are three basic elements in MATLAB: numbers, variables, and
operators. In addition, punctuation marks (,, ;, :, etc.) have special
meanings.
Numbers
MATLAB is a high-precision numerical engine and can han-
dle all types of numbers—that is, integers, real numbers, complex num-
bers, among others—with relative ease. For example, the real number 1.23
is represented as simply 1.23, while the real number 4.56 × 107 can be
written as 4.56e7. The imaginary number √−1 is denoted either by 1i or
1j, although in this book we will use the symbol 1j. Hence the complex
number whose real part is 5 and whose imaginary part is 3 will be written
as 5+1j*3. Other constants preassigned by MATLAB are pi for π, inf
for ∞, and NaN for not a number (e.g., 0/0). These preassigned constants
are very important and, to avoid confusion, should not be redeﬁned by
users.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
7
Variables
In MATLAB, which stands for MATrix LABoratory, the ba-
sic variable is a matrix, or an array. Hence, when MATLAB operates on
this variable, it operates on all its elements. This is what makes it a pow-
erful and an eﬃcient engine. MATLAB now supports multidimensional
arrays; we will discuss only up to two-dimensional arrays of numbers.
1. Matrix: A matrix is a two-dimensional set of numbers arranged in
rows and columns. Numbers can be real- or complex-valued.
2. Array: This is another name for matrix. However, operations on arrays
are treated diﬀerently from those on matrices. This diﬀerence is very
important in implementation.
The following are four types of matrices (or arrays).
• Scalar: This is a 1 × 1 matrix or a single number that is denoted by
the variable symbol, that is, lowercase italic typeface like
a = a11
• Column vector: This is an (N × 1) matrix or a vertical arrangement
of numbers. It is denoted by the vector symbol, that is, lowercase bold
typeface like
x = [xi1]i:1,...,N =
⎡
⎢⎢⎢⎣
x11
x21
...
xN1
⎤
⎥⎥⎥⎦
A typical vector in linear algebra is denoted by the column vector.
• Row vector: This is a (1 × M) matrix or a horizontal arrangement of
numbers. It is also denoted by the vector symbol, that is,
y = [y1j]j=1,...,M =
y11 y12 · · · y1M

A one-dimensional discrete-time signal is typically represented by an
array as a row vector.
• General matrix: This is the most general case of an (N × M) matrix
and is denoted by the matrix symbol, that is, uppercase bold typeface
like
A = [aij]i=1,...,N;j=1,...,m =
⎡
⎢⎢⎢⎣
a11 a12 · · · a1M
a21 a22 · · · a2M
...
...
...
...
aN1 aN2 · · · aNM
⎤
⎥⎥⎥⎦
This arrangement is typically used for two-dimensional discrete-time
signals or images.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

8
Chapter 1
INTRODUCTION
MATLAB does not distinguish between an array and a matrix except for
operations. The following assignments denote indicated matrix types in
MATLAB:
a = [3] is a scalar,
x = [1,2,3] is a row vector,
y = [1;2;3] is a column vector, and
A = [1,2,3;4,5,6] is a matrix.
MATLAB provides many useful functions to create special matrices.
These include zeros(M,N) for creating a matrix of all zeros, ones(M,N)
for creating matrix of all ones, eye(N) for creating an N × N identity
matrix, and so on. Consult MATLAB’s help manual for a complete list.
Operators
MATLAB provides several arithmetic and logical operators,
some of which follow. For a complete list, MATLAB’s help manual should
be consulted.
= assignment
== equality
+ addition
- subtraction or minus
* multiplication
.* array multiplication
ˆ power
.ˆ array power
/ division
./ array division
<> relational operators
& logical AND
| logical OR
˜ logical NOT
’ transpose
.’ array transpose
We now provide a more detailed explanation on some of these operators.
1.2.2 MATRIX OPERATIONS
Following are the most useful and important operations on matrices.
• Matrix addition and subtraction: These are straightforward oper-
ations that are also used for array addition and subtraction. Care must
be taken that the two matrix operands be exactly the same size.
• Matrix conjugation: This operation is meaningful only for complex-
valued matrices. It produces a matrix in which all imaginary parts are
negated. It is denoted by A∗in analysis and by conj(A) in MATLAB.
• Matrix transposition: This is an operation in which every row (col-
umn) is turned into column (row). Let X be an (N × M) matrix. Then
X
′ = [xji] ;
j = 1, . . . , M, i = 1, . . . , N
is an (M × N) matrix. In MATLAB, this operation has one additional
feature. If the matrix is real-valued, then the operation produces the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
9
usual transposition. However, if the matrix is complex-valued, then the
operation produces a complex-conjugate transposition. To obtain just
the transposition, we use the array operation of conjugation, that is,
A.′ will do just the transposition.
• Multiplication
by
a
scalar: This is a simple straightforward
operation in which each element of a matrix is scaled by a constant,
that is,
ab ⇒a*b (scalar)
ax ⇒a*x (vector or array)
aX ⇒a*X (matrix)
This operation is also valid for an array scaling by a constant.
• Vector-vector multiplication: In this operation, one has to be care-
ful about matrix dimensions to avoid invalid results. The operation
produces either a scalar or a matrix. Let x be an (N × 1) vector and
y be a (1 × M) vector. Then
x ∗y ⇒xy =
⎡
⎢⎣
x1
...
xN
⎤
⎥⎦

y1 · · · yM

=
⎡
⎢⎣
x1y1
· · ·
x1yM
...
...
...
xNy1
· · ·
xNyM
⎤
⎥⎦
produces a matrix. If M = N, then
y ∗x ⇒yx =

y1 · · · yM

⎡
⎢⎣
x1
...
xM
⎤
⎥⎦= x1y1 + · · · + xMyM
• Matrix-vector multiplication: If the matrix and the vector are com-
patible (i.e., the number of matrix-columns is equal to the vector-rows),
then this operation produces a column vector:
y = A*x ⇒y = Ax =
⎡
⎢⎣
a11
· · ·
a1M
...
...
...
aN1
· · ·
aNM
⎤
⎥⎦
⎡
⎢⎣
x1
...
xM
⎤
⎥⎦=
⎡
⎢⎣
y1
...
yN
⎤
⎥⎦
• Matrix-matrix multiplication: Finally, if two matrices are compat-
ible, then their product is well deﬁned. The result is also a matrix with
the number of rows equal to that of the ﬁrst matrix and the number
of columns equal to that of the second matrix. Note that the order in
matrix multiplication is very important.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

10
Chapter 1
INTRODUCTION
Array Operations
These operations treat matrices as arrays. They are
also known as dot operations because the arithmetic operators are preﬁxed
by a dot (.), that is, .*, ./, or .ˆ.
• Array multiplication: This is an element by element multiplication
operation. For it to be a valid operation, both arrays must be the same
size. Thus we have
x.*y →1D array
X.*Y →2D array
• Array exponentiation: In this operation, a scalar (real- or complex-
valued) is raised to the power equal to every element in an array, that is,
a.ˆx ≡
⎡
⎢⎢⎢⎢⎣
ax1
ax2
...
axN
⎤
⎥⎥⎥⎥⎦
is an (N × 1) array, whereas
a.ˆX ≡
⎡
⎢⎢⎢⎢⎣
ax11
ax12
· · ·
ax1M
ax21
ax22
· · ·
ax2M
...
...
...
...
axN1
axN2
· · ·
axNM
⎤
⎥⎥⎥⎥⎦
is an (N × M) array.
• Array transposition: As explained, the operation A.′ produces trans-
position of real- or complex-valued array A.
Indexing Operations
MATLAB provides very useful and powerful
array-indexing operations using operator :. It can be used to generate
sequences of numbers as well as to access certain row/column elements
of a matrix. Using the fragment x = [a:b:c], we can generate numbers
from a to c in b increments. If b is positive (negative), then we get
increasing (decreasing) values in the sequence x.
The fragment x(a:b:c) accesses elements of x beginning with index
a in steps of b and ending at c. Care must be taken to use integer values
for indexing elements. Similarly, the : operator can be used to extract a
submatrix from a matrix. For example, B = A(2:4,3:6) extracts a 3 × 4
submatrix starting at row 2 and column 3.
Another use of the : operator is in forming column vectors from row
vectors or matrices. When used on the right-hand side of the equality (=)
operator, the fragment x=A(:) forms a long column vector x of elements
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
11
of A by concatenating its columns. Similarly, x=A(:,3) forms a vector x
from the third column of A. However, when used on the right-hand side
of the = operator, the fragment A(:)=x reformats elements in x into a
predeﬁned size of A.
Control-Flow
MATLAB provides a variety of commands that allow
us to control the ﬂow of commands in a program. The most common
construct is the if-elseif-else structure. With these commands, we can
allow diﬀerent blocks of code to be executed depending on some condition.
The format of this construct is
if condition1
command1
elseif condition2
command2
else
command3
end
which executes statements in command1 if condition-1 is satisﬁed;
otherwise, it executes statements in command2 if condition-2 is satis-
ﬁed, or, ﬁnally, statements in command3.
Another common control ﬂow construct is the for..end loop. It is
simply an iteration loop that tells the computer to repeat some task a
given number of times. The format of a for..end loop is
for index = values
program statements
:
end
Although for..end loops are useful for processing data inside of arrays by
using the iteration variable as an index into the array, whenever possible
the user should try to use MATLAB’s whole array mathematics. This will
result in shorter programs and more eﬃcient code. In some situations, the
use of the for..end loop is unavoidable. The following example illustrates
these concepts.
□
EXAMPLE 1.1
Consider the following sum of sinusoidal functions:
x(t) = sin(2πt) + 1
3 sin(6πt) + 1
5 sin(10πt) =

k=1,3,5
1
k sin(2πkt),
0 ≤t ≤1
Using MATLAB, we want to generate samples of x(t) at time instances
0:0.01:1. We will discuss three approaches.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

12
Chapter 1
INTRODUCTION
Approach 1
Here we will consider a typical C or Fortran approach, that is, we will use two
for..end loops, one each on t and k. This is the most ineﬃcient approach in
MATLAB, but possible.
>> t = 0:0.01:1; N = length(t); xt = zeros(1,N);
>> for n = 1:N
>>
temp = 0;
>>
for k = 1:2:5
>>
temp = temp + (1/k)*sin(2*pi*k*t(n));
>>
end
>>
xt(n) = temp;
>> end
Approach 2
In this approach, we will compute each sinusoidal component in one step as a
vector, using the time vector t = 0:0.01:1, and then add all components using
one for..end loop.
>> t = 0:0.01:1; xt = zeros(1,length(t));
>> for k = 1:2:5
>>
xt = xt + (1/k)*sin(2*pi*k*t);
>> end
Clearly, this is a better approach with fewer lines of code than the ﬁrst one.
Approach 3
In this approach, we will use matrix-vector multiplication, in which MATLAB
is very eﬃcient. For the purpose of demonstration, consider only four values for
t = [t1, t2, t3, t4]. Then
x(t1) = sin(2πt1) + 1
3 sin(2π3t1) + 1
5 sin(2π5t1)
x(t2) = sin(2πt2) + 1
3 sin(2π3t2) + 1
5 sin(2π5t2)
x(t3) = sin(2πt3) + 1
3 sin(2π3t3) + 1
5 sin(2π5t3)
x(t4) = sin(2πt4) + 1
3 sin(2π3t4) + 1
5 sin(2π5t4)
which can be written in matrix form as
⎡
⎢⎢⎢⎣
x(t1)
x(t2)
x(t3)
x(t4)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎢⎣
sin(2πt1) sin(2π3t1) sin(2π5t1)
sin(2πt2) sin(2π3t2) sin(2π5t2)
sin(2πt3) sin(2π3t3) sin(2π5t3)
sin(2πt4) sin(2π3t4) sin(2π5t4)
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎣
1
1
3
1
5
⎤
⎥⎦
= sin
⎛
⎜
⎜
⎜
⎝2π
⎡
⎢⎢⎢⎣
t1
t2
t3
t4
⎤
⎥⎥⎥⎦

1 3 5

⎞
⎟
⎟
⎟
⎠
⎡
⎢⎣
1
1
3
1
5
⎤
⎥⎦
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
13
or, after taking transposition,

x(t1)
x(t2)
x(t3)
x(t4)

=

1
1
3
1
5

sin
⎛
⎜
⎝2π
⎡
⎢⎣
1
3
5
⎤
⎥⎦

t1
t2
t3
t4

⎞
⎟
⎠
Thus the MATLAB code is
>> t = 0:0.01:1; k = 1:2:5;
>> xt = (1./k)*sin(2*pi*k’*t);
Note the use of the array division (1./k) to generate a row vector and ma-
trix multiplications to implement all other operations. This is the most compact
code and the most eﬃcient execution in MATLAB, especially when the number
of sinusoidal terms is very large.
1.2.3 SCRIPTS AND FUNCTIONS
MATLAB is convenient in the interactive command mode if we want to
execute few lines of code. But it is not eﬃcient if we want to write code of
several lines that we want to run repeatedly or if we want to use the code
in several programs with diﬀerent variable values. MATLAB provides two
constructs for this purpose.
Scripts
The ﬁrst construct can be accomplished by using the so-called
block mode of operation. In MATLAB, this mode is implemented using
a script ﬁle called an m-ﬁle (with an extension .m), which is only a text
ﬁle that contains each line of the ﬁle as though you typed them at the
command prompt. These scripts are created using MATLAB’s built-in
editor, which also provides for context-sensitive colors and indents for
making fewer mistakes and for easy reading. The script is executed by
typing the name of the script at the command prompt. The script ﬁle must
be in the current directory or in the directory of the path environment.
As an example, consider the sinusoidal function in Example 1.1. A general
form of this function is
x(t) =
K
	
k=1
ck sin(2πkt)
(1.1)
If we want to experiment with diﬀerent values of the coeﬃcients ck and/or
the number of terms K, then we should create a script ﬁle. To implement
the third approach in Example 1.1, we can write a script ﬁle
% Script file to implement (1.1)
t = 0:0.01:1; k = 1:2:5; ck = 1./k;
xt = ck * sin(2*pi*k’*t);
Now we can experiment with diﬀerent values.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

14
Chapter 1
INTRODUCTION
Functions
The second construct of creating a block of code is through
subroutines. These are called functions, which also allow us to extend the
capabilities of MATLAB. In fact, a major portion of MATLAB is assem-
bled using function ﬁles in several categories and using special collections
called toolboxes. Functions are also m-ﬁles (with extension .m). A major
diﬀerence between script and function ﬁles is that the ﬁrst executable
line in a function ﬁle begins with the keyword function followed by an
output-input variable declaration. As an example, consider the compu-
tation of the x(t) function in Example 1.1 with an arbitrary number of
sinusoidal terms, which we will implement as a function stored as m-ﬁle
sinsum.m.
function xt = sinsum(t,ck)
% Computes sum of sinusoidal terms of the form in (1.1)
% x = sinsum(t,ck)
%
K = length(ck); k = 1:K;
ck = ck(:)’; t = t(:)’;
xt = ck * sin(2*pi*k’*t);
The vectors t and ck should be assigned prior to using the sinsum
function. Note that ck(:)’ and t(:)’ use indexing and transposition
operations to force them to be row vectors. Also note the comments im-
mediately following the function declaration, which are used by the help
sinsum command. Suﬃcient information should be given there for the user
to understand what the function is supposed to do.
1.2.4 PLOTTING
One of the most powerful features of MATLAB for signal and data anal-
ysis is its graphical data plotting. MATLAB provides several types of
plots, starting with simple two-dimensional (2D) graphs and progressing
to complex, higher-dimensional full-color plots. We will examine only the
2D plotting, which is the plotting of one array versus another in a 2D coor-
dinate system. The basic plotting command is the plot(t,x) command,
which generates a plot of x values versus t values in a separate ﬁgure
window. The arrays t and x should be the same length and orientation.
Optionally, some additional formatting keywords can also be provided in
the plot function. The commands xlabel and ylabel are used to add
text to the axes, and the command title is used to provide a title on
the top of the graph. When plotting data, one should get into the habit
of always labeling the axes and providing a title. Almost all aspects of
a plot (style, size, color, etc.) can be changed by appropriate commands
embedded in the program or directly through the GUI.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
15
The following set of commands creates a list of sample points, eval-
uates the sine function at those points, and then generates a plot of a
simple sinusoidal wave, putting axis labels and title on the plot.
>> t = 0:0.01:2; % sample points from 0 to 2 in steps of 0.01
>> xt = sin(2*pi*t); % Evaluate sin(2 pi t)
>> plot(t,xt,’b’); % Create plot with blue line
>> xlabel(’t in sec’); ylabel(’x(t)’); % Label axis
>> title(’Plot of sin(2\pi t)’); % Title plot
The resulting plot is shown in Figure 1.1.
For plotting a set of discrete numbers (or discrete-time signals), we
will use the stem command, which displays data values as a stem, that
is, a small circle at the end of a line connecting it to the horizontal axis.
The circle can be open (default) or ﬁlled (using the option ’filled’).
Using Handle Graphics (MATLAB’s extensive manipulation of graphics
primitives), we can resize circle markers. The following set of commands
displays a discrete-time sine function using these constructs.
>> n = 0:1:40; % sample index from 0 to 40
>> xn = sin(0.1*pi*n); % Evaluate sin(0.1 pi n)
>> stem(n,xn,’b’,’filled’,’marker size’,4); % Stem-plot
>> xlabel(’n’); ylabel(’x(n)’); % Label axis
>> title(’Stem Plot of sin(0.1\pi n)’); % Title plot
The resulting plot is shown in Figure 1.2.
MATLAB provides an ability to display more than one graph in the
same ﬁgure window. By means of the hold on command, several graphs
0
0.5
1
1.5
2
–1
–0.5
0
0.5
1
t in sec
x(t)
Plot of sin(2p t)
FIGURE 1.1
Plot of the sin(2πt) function
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

16
Chapter 1
INTRODUCTION
0
5
10
15
20
25
30
35
40
n
x(n)
Stem Plot of sin(0.1p n)
–1
–0.5
0
0.5
1
FIGURE 1.2
Plot of the sin(0.1πn) sequence
can be plotted on the same set of axes. The hold off command stops
the simultaneous plotting. The following MATLAB fragment (Figure 1.3)
displays graphs in Figures 1.1 and 1.2 as one plot, depicting a “sampling”
operation that we will study later.
>> plot(t,xt,’b’); hold on; % Create plot with blue line
>> Hs = stem(n*0.05,xn,’b’,’filled’); % Stem-plot with handle Hs
>> set(Hs,’markersize’,4); hold off; % Change circle size
Another approach is to use the subplot command, which displays
several graphs in each individual set of axes arranged in a grid, using the
parameters in the subplot command. The following fragment (Figure 1.4)
displays graphs in Figure 1.1 and 1.2 as two separate plots in two rows.
0
0.5
1
1.5
2
Plot of sin(2p t) and its samples
t in Sec
x(t) and x(n)
–1
–0.5
0
0.5
1
FIGURE 1.3
Simultaneous plots of x(t) and x(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Brief Introduction to MATLAB
17
0
0.5
1
1.5
2
t in sec
x(t)
0
5
10
15
20
25
30
35
40
n
x(n)
Stem Plot of sin(0.1p n)
–1
–0.5
0
0.5
1
–1
–0.5
0
0.5
1
Plot of sin(2p t)
FIGURE 1.4
Plots of x(t) and x(n) in two rows
. . .
>> subplot(2,1,1); % Two rows, one column, first plot
>> plot(t,xt,’b’); % Create plot with blue line
. . .
>> subplot(2,1,2); % Two rows, one column, second plot
>> Hs = stem(n,xn,’b’,’filled’,’marker size’,4); % Stem-plot
. . .
The plotting environment provided by MATLAB is very rich in
its complexity and usefulness. It is made even richer using the Handle
Graphics constructs. Therefore, readers are strongly recommended to
consult MATLAB’s manuals on plotting. Many of these constructs will
be used throughout this book.
In this brief review, we have barely made a dent in the enormous
capabilities and functionalities in MATLAB. Using its basic integrated
help system, detailed help browser, and tutorials, it is possible to acquire
suﬃcient skills in MATLAB in a reasonable amount of time.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

18
Chapter 1
INTRODUCTION
1.3 APPLICATIONS OF DIGITAL SIGNAL PROCESSING
The ﬁeld of DSP has matured considerably over the last several decades
and now is at the core of many diverse applications and products. These
include
• speech/audio (speech recognition/synthesis, digital audio, equalization,
etc.),
• image/video (enhancement, coding for storage and transmission,
robotic vision, animation, etc.),
• military/space (radar processing, secure communication, missile guid-
ance, sonar processing, etc.),
• biomedical/health care (scanners, ECG analysis, X-ray analysis, EEG
brain mappers, etc.), and
• consumer electronics (cellular/mobile phones, digital television, digital
camera, Internet voice/music/video, interactive entertainment systems,
etc.).
These applications and products require many interconnected com-
plex steps, such as collection, processing, transmission, analysis, audio/
display of real-world information in near real time. DSP technology has
made it possible to incorporate these steps into devices that are inno-
vative, aﬀordable, and of high quality (e.g., the iPhone from Apple).
A typical application to music is now considered as a motivation for the
study of DSP.
Musical sound processing
In the music industry, almost all musical
products (songs, albums, etc.) are produced in basically two stages. First,
the sound from an individual instrument or performer is recorded in an
acoustically inert studio on a single track of a multitrack recording device.
Then, stored signals from each track are digitally processed by the sound
engineer by adding special eﬀects and combined into a stereo recording,
which is then made available either on a CD or as an audio ﬁle.
The audio eﬀects are artiﬁcially generated using various signal-
processing techniques. These eﬀects include echo generation, reverbera-
tion (concert hall eﬀect), ﬂanging (in which audio playback is slowed down
by placing the DJ’s thumb on the flange of the feed reel), chorus eﬀect
(when several musicians play the same instrument with small changes in
amplitudes and delays), and phasing (a.k.a. phase shifting, in which an
audio eﬀect takes advantage of how sound waves interact with each other
when they are out of phase). We now discuss a few of these sound eﬀects
in some detail.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Digital Signal Processing
19
Echo Generation
The most basic of all audio eﬀects is that of time
delay, or echoes. It is used as the building block of more complicated eﬀects
such as reverb or ﬂanging. In a listening space such as a room, sound
waves arriving at our ears consist of direct sound from the source as well
as reflected oﬀthe walls, arriving with diﬀerent amounts of attenuation
and delays.
Echoes are delayed signals, and as such they are generated using delay
units. For example, the combination of the direct sound represented by
discrete signal y[n] and a single echo appearing D samples later (which
is related to delay in seconds) can be generated by an equation (called a
diﬀerence equation) of the form
x[n] = y[n] + αy[n −D],
|α| < 1
(1.2)
where x[n] is the resulting signal and α models attenuation of the di-
rect sound. Diﬀerence equations are implemented in MATLAB using the
filter function. Available in MATLAB is a short snippet of Handel’s
“Hallelujah Chorus,” which is a digital sound about 9 seconds long, sam-
pled at 8192 sam/sec. To experience the sound with echo in (1.2), execute
the following fragment at the command window. The echo is delayed by
D = 4196 samples, which amount to 0.5 sec of delay.
load handel; % The signal is in y and sampling freq in Fs
sound(y,Fs); pause(10); % Play the original sound
alpha = 0.9; D = 4196; % Echo parameters
b = [1,zeros(1,D),alpha]; % Filter parameters
x = filter(b,1,y); % Generate sound plus its echo
sound(x,Fs); % Play sound with echo
You should be able to hear the distinct echo of the chorus in about a
half second.
Echo Removal
After executing this simulation, you may experience
that the echo is an objectionable interference while listening. Again DSP
can be used eﬀectively to reduce (almost eliminate) echoes. Such an echo-
removal system is given by the diﬀerence equation
w[n] + αw[n −D] = x[n]
(1.3)
where x[n] is the echo-corrupted sound signal and w[n] is the output
sound signal, which has the echo (hopefully) removed. Note again that
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

20
Chapter 1
INTRODUCTION
this system is very simple to implement in software or hardware. Now try
the following MATLAB script on the signal x[n].
w = filter(1,b,x);
sound(w,Fs)
The echo should no longer be audible.
Digital Reverberation
Multiple close-spaced echoes eventually lead
to reverberation, which can be created digitally using a somewhat more
involved diﬀerence equation
x[n] =
N−1
	
k=0
αky[n −kD]
(1.4)
which generates multiple echoes spaced D samples apart with exponen-
tially decaying amplitudes. Another natural-sounding reverberation is
given by
x[n] = αy[n] + y[n −D] + αx[n −D],
|α| < 1
(1.5)
which simulates a higher echo density.
These simple applications are examples of DSP. Using techniques,
concepts, and MATLAB functions learned in this book, you should be
able to simulate these and other interesting sound eﬀects.
1.4 BRIEF OVERVIEW OF THE BOOK
The book is organized roughly in four parts. The ﬁrst three parts are
included in the printed copy, as was done in the previous editions of the
book, while the last part on advanced topics is available only through the
book’s website (see the Preface for instructions on accessing the book’s
website).
The ﬁrst part of this book, which comprises Chapters 2 through 5,
deals with the signal-analysis aspect of DSP. Chapter 2 begins with basic
descriptions of discrete-time signals and systems. These signals and sys-
tems are analyzed in the frequency domain in Chapter 3. A generalization
of the frequency-domain description, called the z-transform, is introduced
in Chapter 4. The practical algorithms for computing the Fourier trans-
form are discussed in Chapter 5 in the form of the discrete Fourier trans-
form and the fast Fourier transform.
Chapters 6 through 8 constitute the second part of this book, which is
devoted to the signal-ﬁltering aspect of DSP. Chapter 6 describes various
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Brief Overview of the Book
21
implementations and structures of digital ﬁlters. It also introduces ﬁnite-
precision number representation and ﬁlter coeﬃcient quantization and
their eﬀect on ﬁlter performance. Chapter 7 introduces design techniques
and algorithms for designing one type of digital ﬁlter called finite-duration
impulse response (FIR) filters, and Chapter 8 provides a similar treatment
for another type of ﬁlter called infinite-duration impulse response (IIR)
filters. In both chapters, only the simpler but practically useful techniques
of ﬁlter design are discussed. More advanced techniques are not covered.
The third part, which consists of Chapters 9 through 12, provides
important topics and applications in DSP. Chapter 9 deals with the use-
ful topic of sampling-rate conversion and applies FIR ﬁlter designs from
Chapter 7 to implement practical sample-rate converters. Chapter 10 ex-
tends the treatment of ﬁnite-precision numerical representation to signal
quantization and the eﬀect of ﬁnite-precision arithmetic on ﬁlter perfor-
mance. The last two chapters provide some practical applications in the
form of projects that can be done using material presented in the ﬁrst 10
chapters. In Chapter 11, concepts in adaptive ﬁltering are introduced, and
simple projects in system identiﬁcation, interference suppression, adaptive
line enhancement, and so forth, are discussed. In Chapter 12, a brief in-
troduction to digital communications is presented, with projects involving
such topics as PCM, DPCM, and LPC being outlined.
Finally, Chapters 13 through 15 round out the last part of the book
and it is intended as topics in advanced undergraduate or graduate
courses. In Chapter 13, we treat signals as stochastic entities, called
random processes (or signals), and provide their probabilistic as well
as statistical descriptions through the concept of random variables. We
also discuss a few representative random processes. In Chapter 14, we
develop theory for optimum filters for processing random signals. For this
treatment, we consider ﬁlters for linear prediction and Wiener ﬁlters for
estimating signals corrupted by additive noise. In Chapter 15, we present
adaptive filters, which are stochastic systems for applications in which
statistics is not known a priori. We provide descriptions of LMS and RLS,
two basic yet important adaptive algorithms.
In all these chapters, the central theme is the generous use and ad-
equate demonstration of MATLAB, which can be used as an eﬀective
teaching as well as learning tool. Most of the existing MATLAB functions
for DSP are described in detail, and their correct use is demonstrated in
many examples. Furthermore, many new MATLAB functions are devel-
oped to provide insights into the working of many algorithms. The authors
believe that this hand-holding approach enables students to dispel fears
about DSP and provides an enriching learning experience.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

22
C H A P T E R 2
Discrete-Time
Signals and
Systems
We begin with the concepts of signals and systems in discrete time. A
number of important types of signals and their operations are introduced.
Linear and shift-invariant systems are discussed mostly because they are
easier to analyze and implement. The convolution and the diﬀerence equa-
tion representations are given special attention because of their impor-
tance in digital signal processing and in MATLAB. The emphasis in this
chapter is on the representations and implementation of signals and sys-
tems using MATLAB.
2.1 DISCRETE-TIME SIGNALS
Signals are broadly classiﬁed into analog and discrete signals. An analog
signal will be denoted by xa(t), in which the variable t can represent any
physical quantity but we will assume that it represents time in seconds. A
discrete signal will be denoted by x(n), in which the variable n is integer-
valued and represents discrete instances in time. Therefore, it is also called
a discrete-time signal, which is a number sequence and will be denoted by
one of the following notations:
x(n) = {x(n)} = {. . . , x(−1), x(0)
↑
, x(1), . . .}
where the up-arrow indicates the sample at n = 0.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
23
In MATLAB, we can represent a finite-duration sequence by a row
vector of appropriate values. However, such a vector does not have any
information about sample position n. Therefore, a correct representation
of x(n) would require two vectors, one each for x and n. For example, a
sequence x(n) = {2, 1, −1, 0
↑
, 1, 4, 3, 7} can be represented in MATLAB by
>> n=[-3,-2,-1,0,1,2,3,4];
x=[2,1,-1,0,1,4,3,7];
Generally, we will use the x-vector representation alone when the sample
position information is not required or when such information is trivial
(e.g., when the sequence begins at n = 0). An arbitrary infinite-duration
sequence cannot be represented in MATLAB, due to the ﬁnite memory
limitations.
2.1.1 TYPES OF SEQUENCES
We use several elementary sequences in digital signal processing for anal-
ysis purposes. Their deﬁnitions and MATLAB representations follow.
1. Unit sample sequence:
δ(n) =

1,
n = 0
0,
n ̸= 0 =

. . . , 0, 0, 1
↑
, 0, 0, . . .

In MATLAB, the function zeros(1,N) generates a row vector of N
zeros, which can be used to implement δ(n) over a ﬁnite interval. How-
ever, the logical relation n==0 is an elegant way of implementing δ(n).
For example, to implement
δ(n −n0) =

1,
n = n0
0,
n ̸= n0
over the n1 ≤n0 ≤n2 interval, we will use the following MATLAB
function.
function [x,n] = impseq(n0,n1,n2)
% Generates x(n) = delta(n-n0); n1 <= n <= n2
% ----------------------------------------------
% [x,n] = impseq(n0,n1,n2)
%
n = [n1:n2]; x = [(n-n0) == 0];
2. Unit step sequence:
u(n) =

1,
n ≥0
0,
n < 0 = {. . . , 0, 0, 1
↑
, 1, 1, . . .}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

24
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
In MATLAB, the function ones(1,N) generates a row vector of N ones.
It can be used to generate u(n) over a ﬁnite interval. Once again, an
elegant approach is to use the logical relation n>=0. To implement
u(n −n0) =

1,
n ≥n0
0,
n < n0
over the n1 ≤n0 ≤n2 interval, we will use the following MATLAB
function.
function [x,n] = stepseq(n0,n1,n2)
% Generates x(n) = u(n-n0); n1 <= n <= n2
% ------------------------------------------
% [x,n] = stepseq(n0,n1,n2)
%
n = [n1:n2]; x = [(n-n0) >= 0];
3. Real-valued exponential sequence:
x(n) = an, ∀n; a ∈R
In MATLAB, an array operator “.ˆ” is required to implement a real
exponential sequence. For example, to generate x(n) = (0.9)n, 0 ≤
n ≤10, we will need the following MATLAB script.
>> n = [0:10]; x = (0.9).ˆn;
4. Complex-valued exponential sequence:
x(n) = e(σ+jω0)n, ∀n
where σ produces an attenuation (if <0) or ampliﬁcation (if >0)
and ω0 is the frequency in radians. A MATLAB function exp is
used to generate exponential sequences. For example, to generate
x(n) = exp[(2 + j3)n], 0 ≤n ≤10, we will need the following MATLAB
script.
>> n = [0:10]; x = exp((2+3j)*n);
5. Sinusoidal sequence:
x(n) = A cos(ω0n + θ0), ∀n
where A is an amplitude and θ0 is the phase in radians. A MAT-
LAB function cos (or sin) is used to generate sinusoidal sequences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
25
For example, to generate x(n) = 3 cos(0.1πn + π/3) + 2 sin(0.5πn),
0 ≤n ≤10, we will need the following MATLAB script.
>> n = [0:10]; x = 3*cos(0.1*pi*n+pi/3) + 2*sin(0.5*pi*n);
6. Random sequences: Many practical sequences cannot be described
by mathematical expressions like those above. These sequences are
called random (or stochastic) sequences and are characterized by pa-
rameters of the associated probability density functions. In MATLAB,
two types of (pseudo-) random sequences are available. The rand(1,N)
generates a length N random sequence whose elements are uniformly
distributed between [0, 1]. The randn(1,N) generates a length N Gaus-
sian random sequence with mean 0 and variance 1. Other random se-
quences can be generated using transformations of the above functions.
7. Periodic sequence: A sequence x(n) is periodic if x(n) = x(n + N),
∀n. The smallest integer N that satisﬁes this relation is called the
fundamental period. We will use ˜x(n) to denote a periodic sequence.
To generate P periods of ˜x(n) from one period {x(n),
0 ≤n ≤N−1},
we can copy x(n) P times:
>> xtilde = [x,x,...,x];
But an elegant approach is to use MATLAB’s powerful indexing capa-
bilities. First we generate a matrix containing P rows of x(n) values.
Then we can concatenate P rows into a long row vector using the
construct (:). However, this construct works only on columns. Hence
we will have to use the matrix transposition operator ’ to provide the
same eﬀect on rows.
>> xtilde = x’ * ones(1,P);
% P columns of x; x is a row vector
>> xtilde = xtilde(:);
% Long column vector
>> xtilde = xtilde’;
% Long row vector
Note that the last two lines can be combined into one for compact
coding. This is shown in Example 2.1.
2.1.2 OPERATIONS ON SEQUENCES
Here we brieﬂy describe basic sequence operations and their MATLAB
equivalents.
1. Signal addition: This is a sample-by-sample addition given by
{x1(n)} + {x2(n)} = {x1(n) + x2(n)}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

26
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
It is implemented in MATLAB by the arithmetic operator “+”. How-
ever, the lengths of x1(n) and x2(n) must be the same. If sequences are
of unequal lengths, or if the sample positions are diﬀerent for equal-
length sequences, then we cannot directly use the operator +. We have
to ﬁrst augment x1(n) and x2(n) so that they have the same position
vector n (and hence the same length). This requires careful attention
to MATLAB’s indexing operations. In particular, logical operation of
intersection “&”, relational operations like “<=” and “==”, and the
find function are required to make x1(n) and x2(n) of equal length.
The following function, called the sigadd function, demonstrates these
operations.
function [y,n] = sigadd(x1,n1,x2,n2)
% Implements y(n) = x1(n)+x2(n)
% -----------------------------
% [y,n] = sigadd(x1,n1,x2,n2)
%
y = sum sequence over n, which includes n1 and n2
%
x1 = first sequence over n1
%
x2 = second sequence over n2 (n2 can be different from n1)
%
n = min(min(n1),min(n2)):max(max(n1),max(n2));
% Duration of y(n)
y1 = zeros(1,length(n)); y2 = y1;
% Initialization
y1(find((n>=min(n1))&(n<=max(n1))==1))=x1;
% x1 with duration of y
y2(find((n>=min(n2))&(n<=max(n2))==1))=x2;
% x2 with duration of y
y = y1+y2;
% Sequence addition
Its use is illustrated in Example 2.2.
2. Signal multiplication: This is a sample-by-sample (or “dot”) multi-
plication given by
{x1(n)} · {x2(n)} = {x1(n)x2(n)}
It is implemented in MATLAB by the array operator .*. Once again,
the similar restrictions apply for the .* operator as for the + operator.
Therefore, we have developed the sigmult function, which is similar
to the sigadd function.
function [y,n] = sigmult(x1,n1,x2,n2)
% Implements y(n) = x1(n)*x2(n)
% -----------------------------
% [y,n] = sigmult(x1,n1,x2,n2)
%
y = product sequence over n, which includes n1 and n2
%
x1 = first sequence over n1
%
x2 = second sequence over n2 (n2 can be different from n1)
%
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
27
n = min(min(n1),min(n2)):max(max(n1),max(n2));
% Duration of y(n)
y1 = zeros(1,length(n)); y2 = y1;
%
y1(find((n>=min(n1))&(n<=max(n1))==1))=x1;
% x1 with duration of y
y2(find((n>=min(n2))&(n<=max(n2))==1))=x2;
% x2 with duration of y
y = y1 .* y2;
% Sequence multiplication
Its use is also given in Example 2.2.
3. Scaling: In this operation, each sample is multiplied by a scalar α.
α {x(n)} = {αx(n)}
An arithmetic operator (*) is used to implement the scaling operation
in MATLAB.
4. Shifting: In this operation, each sample of x(n) is shifted by an
amount k to obtain a shifted sequence y(n).
y(n) = {x(n −k)}
If we let m = n−k, then n = m+k and the above operation is given by
y(m + k) = {x (m)}
Hence this operation has no eﬀect on the vector x, but the vector n is
changed by adding k to each element. This is shown in the function
sigshift.
function [y,n] = sigshift(x,m,k)
% Implements y(n) = x(n-k)
% -------------------------
% [y,n] = sigshift(x,m,k)
%
n = m+k; y = x;
Its use is given in Example 2.2.
5. Folding: In this operation, each sample of x(n) is ﬂipped around
n = 0 to obtain a folded sequence y(n).
y(n) = {x(−n)}
In MATLAB, this operation is implemented by fliplr(x)function for
sample values and by -fliplr(n) function for sample positions, as
shown in the sigfold function.
function [y,n] = sigfold(x,n)
% Implements y(n) = x(-n)
% -----------------------
% [y,n] = sigfold(x,n)
%
y = fliplr(x); n = -fliplr(n);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

28
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
6. Sample summation: This operation diﬀers from the signal addition
operation. It adds all sample values of x(n) between n1 and n2.
n2
	
n=n1
x(n) = x(n1) + · · · + x(n2)
It is implemented by the sum(x(n1:n2)) function.
7. Sample products: This operation also diﬀers from the signal mul-
tiplication operation. It multiplies all sample values of x(n) between
n1 and n2.
n2

n1
x(n) = x(n1) × · · · × x(n2)
It is implemented by the prod(x(n1:n2)) function.
8. Signal energy: The energy of a sequence x(n) is given by
Ex =
∞
	
−∞
x(n)x∗(n) =
∞
	
−∞
|x(n)|2
where superscript ∗denotes the operation of complex conjugation.1
The energy of a ﬁnite-duration sequence x(n) can be computed in
MATLAB using
>> Ex = sum(x .* conj(x)); % One approach
>> Ex = sum(abs(x) .ˆ 2); % Another approach
9. Signal power: The average power of a periodic sequence ˜x(n) with
fundamental period N is given by
Px = 1
N
N−1
	
0
|˜x(n)|2
□
EXAMPLE 2.1
Generate and plot each of the following sequences over the indicated interval.
a. x(n) = 2δ(n + 2) −δ(n −4),
−5 ≤n ≤5.
b. x(n) = n[u(n)−u(n−10)]+10e−0.3(n−10)[u(n−10)−u(n−20)], 0 ≤n ≤20.
c. x(n) = cos(0.04πn) + 0.2w(n), 0 ≤n ≤50, where w(n) is a Gaussian
random sequence with zero mean and unit variance.
d. ˜x(n) = {..., 5, 4, 3, 2, 1, 5
↑
, 4, 3, 2, 1, 5, 4, 3, 2, 1, ...}; −10 ≤n ≤9.
1The symbol * denotes many operations in digital signal processing. Its font (roman
or computer) and its position (normal or superscript) will distinguish each operation.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
29
Solution
a. x(n) = 2δ(n + 2) −δ(n −4),
−5 ≤n ≤5.
>> n = [-5:5];
>> x = 2*impseq(-2,-5,5) - impseq(4,-5,5);
>> stem(n,x); title(’Sequence in Problem 2.1a’)
>> xlabel(’n’); ylabel(’x(n)’);
The plot of the sequence is shown in Figure 2.1.
b. x(n) = n [u(n) −u(n −10)]+10e−0.3(n−10) [u(n −10) −u(n −20)], 0 ≤n ≤
20.
>> n = [0:20]; x1 = n.*(stepseq(0,0,20)-stepseq(10,0,20));
>> x2 = 10*exp(-0.3*(n-10)).*(stepseq(10,0,20)-stepseq(20,0,20));
>> x = x1+x2;
>> subplot(2,2,3); stem(n,x); title(’Sequence in Problem 2.1b’)
>> xlabel(’n’); ylabel(’x(n)’);
The plot of the sequence is shown in Figure 2.1.
−5
0
5
−2
−1
0
1
2
3
Sequence in Example 2.1a
n
x(n)
0
5
10
15
20
0
5
10
Sequence in Example 2.1b
n
x(n)
0
10
20
30
40
50
−1
−0.5
0
0.5
1
Sequence in Example 2.1c
n
x(n)
−10
−5
0
5
0
2
4
6
Sequence in Example 2.1d
n
xtilde(n)
FIGURE 2.1
Sequences in Example 2.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

30
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
c. x(n) = cos(0.04πn) + 0.2w(n),
0 ≤n ≤50.
>> n = [0:50]; x = cos(0.04*pi*n)+0.2*randn(size(n));
>> subplot(2,2,2); stem(n,x); title(’Sequence in Problem 2.1c’)
>> xlabel(’n’); ylabel(’x(n)’);
The plot of the sequence is shown in Figure 2.1.
d. ˜x(n) = {..., 5, 4, 3, 2, 1, 5
↑
, 4, 3, 2, 1, 5, 4, 3, 2, 1, ...}; −10 ≤n ≤9.
Note that over the given interval, the sequence ˜x (n) has four periods.
>> n = [-10:9]; x = [5,4,3,2,1];
>> xtilde = x’ * ones(1,4);
xtilde = (xtilde(:))’;
>> subplot(2,2,4); stem(n,xtilde); title(’Sequence in Problem 2.1d’)
>> xlabel(’n’); ylabel(’xtilde(n)’);
The plot of the sequence is shown in Figure 2.1.
□
□
EXAMPLE 2.2
Let x(n) = {1, 2, 3
↑
, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1}. Determine and plot the following
sequences.
a. x1(n) = 2x(n −5) −3x(n + 4)
b. x2(n) = x(3 −n) + x(n) x(n −2)
Solution
The sequence x(n) is nonzero over −2 ≤n ≤10. Hence
>> n = -2:10; x = [1:7,6:-1:1];
will generate x(n).
a. x1(n) = 2x(n −5) −3x(n + 4).
The ﬁrst part is obtained by shifting x(n) by 5, and the second part by
shifting x(n) by −4. This shifting and the addition can be easily done using
the sigshift and the sigadd functions.
>> [x11,n11] = sigshift(x,n,5); [x12,n12] = sigshift(x,n,-4);
>> [x1,n1] = sigadd(2*x11,n11,-3*x12,n12);
>> subplot(2,1,1); stem(n1,x1); title(’Sequence in Example 2.2a’)
>> xlabel(’n’); ylabel(’x1(n)’);
The plot of x1(n) is shown in Figure 2.2.
b. x2(n) = x(3 −n) + x(n) x(n −2).
The ﬁrst term can be written as x(−(n −3)). Hence it is obtained by ﬁrst
folding x(n) and then shifting the result by 3. The second part is a multipli-
cation of x(n) and x(n−2), both of which have the same length but diﬀerent
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
31
−6
0
15
−20
−10
0
10
Sequence in Example 2.2a
n
−7
0
12
0
10
20
30
40
Sequence in Example 2.2b
n
x1(n)
x2(n)
FIGURE 2.2
Sequences in Example 2.2
support (or sample positions). These operations can be easily done using the
sigfold and the sigmult functions.
>> [x21,n21] = sigfold(x,n); [x21,n21] = sigshift(x21,n21,3);
>> [x22,n22] = sigshift(x,n,2); [x22,n22] = sigmult(x,n,x22,n22);
>> [x2,n2] = sigadd(x21,n21,x22,n22);
>> subplot(2,1,2); stem(n2,x2); title(’Sequence in Example 2.2b’)
>> xlabel(’n’); ylabel(’x2(n)’);
The plot of x2(n) is shown in Figure 2.2.
□
Example 2.2 shows that the four sig* functions developed in this
section provide a convenient approach for sequence manipulations.
□
EXAMPLE 2.3
Generate the complex-valued signal
x(n) = e(−0.1+j0.3)n,
−10 ≤n ≤10
and plot its magnitude, its phase, its real part, and its imaginary part in four
separate subplots.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

32
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
−10
−5
0
5
10
−4
−2
0
2
Real Part
n
Amplitude
−10
−5
0
5
10
−2
−1
0
1
Imaginary Part
n
Amplitude
−10
−5
0
5
10
0
1
2
3
Magnitude Part
n
Magnitude
−10
−5
0
5
10
−200
−100
0
100
200
Phase Part
n
Degrees
FIGURE 2.3
Complex-valued sequence plots in Example 2.3
Solution
MATLAB script:
>> n = [-10:1:10]; alpha = -0.1+0.3j;
>> x = exp(alpha*n);
>> subplot(2,2,1); stem(n,real(x));title(’Real Part’);xlabel(’n’)
>> subplot(2,2,2); stem(n,imag(x));title(’Imaginary Part’);xlabel(’n’)
>> subplot(2,2,3); stem(n,abs(x));title(’Magnitude Part’);xlabel(’n’)
>> subplot(2,2,4); stem(n,(180/pi)*angle(x));title(’Phase Part’);xlabel(’n’)
The plot of the sequence is shown in Figure 2.3.
□
2.1.3 DISCRETE-TIME SINUSOIDS
In the last section, we introduced the discrete-time sinusoidal sequence
x(n) = A cos(ω0n + θ0), for all n, as one of the basic signals. This signal
is very important in signal theory as a basis for Fourier transform and
in system theory as a basis for steady-state analysis. It can be conve-
niently related to the continuous-time sinusoid xa(t) = A cos(Ω0t + θ0)
using an operation called sampling (Chapter 3), in which continuous-time
sinusoidal values at equally spaced points t = nTs are assigned to x(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
33
The quantity Ts is called the sampling interval, and Ω0 = ω0/Ts is called
the analog frequency, measured in radians per second.
The fact that n is a discrete variable, whereas t is a continuous
variable, leads to some important diﬀerences between discrete-time and
continuous-time sinusoidal signals.
Periodicity in time
From our deﬁnition of periodicity, the sinusoidal
sequence is periodic if
x[n + N] = A cos(ω0n + ω0N + θ) = A cos(ω0n + θ0) = x[n]
(2.1)
This is possible if and only if ω0N = 2πk, where k is an integer. This
leads to the following important result (see Problem P2.5):
The sequence x(n) = A cos(ω0n + θ0) is periodic if and only if f0
△=
ω0/2π = k/N, that is, f0 is a rational number. If k and N are a
pair of prime numbers, then N is the fundamental period of x(n) and
k represents an integer number of periods kTs of the corresponding
continuous-time sinusoid.
Periodicity in frequency
From the deﬁnition of the discrete-time si-
nusoid, we can easily see that
A cos[(ω0 + k2π)n + θ0] = A cos(ω0n + kn2π + θ0)
= A cos(ω0n + θ0)
since (kn)2π is always an integer multiple of 2π. Therefore, we have the
following property:
The sequence x(n) = A cos(ω0n+θ) is periodic in ω0 with fundamen-
tal period 2π and periodic in f0 with fundamental period 1.
This property has a number of very important implications:
1. Sinusoidal sequences with radian frequencies separated by integer mul-
tiples of 2π are identical.
2. All distinct sinusoidal sequences have frequencies within an interval of
2π radians. We shall use the so-called fundamental frequency ranges
−π < ω ≤π
or
0 ≤ω < 2π
(2.2)
Therefore, if 0 ≤ω0 < 2π, the frequencies ω0 and ω0 + m2π are
indistinguishable from the observation of the corresponding sequences.
3. Since A cos[ω0(n + n0) + θ] = A cos[ω0n + (ω0n0 + θ)], a time shift is
equivalent to a phase change.
4. The rate of oscillation of a discrete-time sinusoid increases as ω0 in-
creases from ω0 = 0 to ω0 = π. However, as ω0 increases from ω0 = π
to ω0 = 2π, the oscillations become slower. Therefore, low frequencies
(slow oscillations) are at the vicinity of ω0 = k2π, and high frequencies
(rapid oscillations) are at the vicinity of ω0 = π + k2π.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

34
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
2.1.4 SOME USEFUL RESULTS
There are several important results in discrete-time signal theory. We will
discuss some that are useful in digital signal processing.
Unit sample synthesis
Any arbitrary sequence x(n) can be synthe-
sized as a weighted sum of delayed and scaled unit sample sequences, such
as
x(n) =
∞
	
k=−∞
x(k)δ(n −k)
(2.3)
We will use this result in the next section.
Even and odd synthesis
A real-valued sequence xe(n) is called even
(symmetric) if
xe(−n) = xe(n)
Similarly, a real-valued sequence xo(n) is called odd (antisymmetric) if
xo(−n) = −xo(n)
Then any arbitrary real-valued sequence x(n) can be decomposed into its
even and odd components
x(n) = xe(n) + xo(n)
(2.4)
where the even and odd parts are given by
xe(n) = 1
2 [x(n) + x(−n)]
and
xo(n) = 1
2 [x(n) −x(−n)]
(2.5)
respectively. We will use this decomposition in studying properties of the
Fourier transform. Therefore, it is a good exercise to develop a simple
MATLAB function to decompose a given sequence into its even and odd
components. Using MATLAB operations discussed so far, we can obtain
the following evenodd function.
function [xe, xo, m] = evenodd(x,n)
% Real signal decomposition into even and odd parts
% -------------------------------------------------
% [xe, xo, m] = evenodd(x,n)
%
if any(imag(x) ˜= 0)
error(’x is not a real sequence’)
end
m = -fliplr(n);
m1 = min([m,n]); m2 = max([m,n]); m = m1:m2;
nm = n(1)-m(1); n1 = 1:length(n);
x1 = zeros(1,length(m));
x1(n1+nm) = x; x = x1;
xe = 0.5*(x + fliplr(x));
xo = 0.5*(x - fliplr(x));
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete-Time Signals
35
−10
−5
0
5
10
0
0.5
1
Rectangular Pulse
n
x(n)
−10
−5
0
5
10
0
0.5
1
Even Part
n
xe(n)
−10
−5
0
5
10
−0.5
0
0.5
Odd Part
n
xo(n)
FIGURE 2.4
Even-odd decomposition in Example 2.4
The sequence and its support are supplied in x and n arrays, respectively.
It ﬁrst checks if the given sequence is real and determines the support
of the even and odd components in m array. It then implements (2.5)
with special attention to the MATLAB indexing operation. The resulting
components are stored in xe and xo arrays.
□
EXAMPLE 2.4
Let x(n) = u(n) −u(n −10). Decompose x(n) into even and odd components.
Solution
The sequence x(n), which is nonzero over 0 ≤n ≤9, is called a rectangular
pulse. We will use MATLAB to determine and plot its even and odd parts.
>> n = [0:10]; x = stepseq(0,0,10)-stepseq(10,0,10);
>> [xe,xo,m] = evenodd(x,n);
>> subplot(2,2,1); stem(n,x); title(’Rectangular Pulse’)
>> xlabel(’n’); ylabel(’x(n)’); axis([-10,10,0,1.2])
>> subplot(2,2,2); stem(m,xe); title(’Even Part’)
>> xlabel(’n’); ylabel(’xe(n)’); axis([-10,10,0,1.2])
>> subplot(2,2,4); stem(m,xo); title(’Odd Part’)
>> xlabel(’n’); ylabel(’xo(n)’); axis([-10,10,-0.6,0.6])
The plots shown in Figure 2.4 clearly demonstrate the decomposition.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

36
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
A similar decomposition for complex-valued sequences is explored in
Problem P2.5.
The geometric series
A one-sided exponential sequence of the form
{αn,
n ≥0}, where α is an arbitrary constant, is called a geometric
series. In digital signal processing, the convergence and expression for the
sum of this series are used in many applications. The series converges for
|α| < 1, while the sum of its components converges to
∞
	
n=0
αn −→
1
1 −α,
for |α| < 1
(2.6)
We will also need an expression for the sum of any ﬁnite number of terms
of the series given by
N−1
	
n=0
αn = 1 −αN
1 −α , ∀α
(2.7)
These two results will be used throughout this book.
Correlations of sequences
Correlation is an operation used in many
applications in digital signal processing. It is a measure of the degree to
which two sequences are similar. Given two real-valued sequences x(n) and
y(n) of ﬁnite energy, the cross-correlation of x(n) and y(n) is a sequence
rxy(ℓ) deﬁned as
rxy(ℓ) =
∞
	
n=−∞
x(n)y(n −ℓ)
(2.8)
The index ℓis called the shift or lag parameter. The special case of (2.8)
when y(n) = x(n) is called autocorrelation and is deﬁned by
rxx(ℓ) =
∞
	
n=−∞
x(n)x(n −ℓ)
(2.9)
It provides a measure of self-similarity between diﬀerent alignments of the
sequence. MATLAB functions to compute auto- and cross-correlations are
discussed later in the chapter.
2.2 DISCRETE SYSTEMS
Mathematically, a discrete-time system (or discrete system for short) is
described as an operator T[·] that takes a sequence x(n) (called excitation)
and transforms it into another sequence y(n) (called response). That is,
y(n) = T[x(n)]
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete Systems
37
In DSP, we will say that the system processes an input signal into an out-
put signal. Discrete systems are broadly classiﬁed into linear and nonlinear
systems. We will deal mostly with linear systems.
2.2.1 LINEAR SYSTEMS
A discrete system T[·] is a linear operator L[·] if and only if L[·] satisﬁes
the principle of superposition, namely,
L[a1x1(n) + a2x2(n)] = a1L[x1(n)] + a2L[x2(n)], ∀a1, a2, x1(n), x2(n)
(2.10)
Using (2.3) and (2.10), the output y(n) of a linear system to an arbitrary
input x(n) is given by
y(n) = L[x(n)] = L

∞
	
n=−∞
x(k) δ(n −k)

=
∞
	
n=−∞
x(k)L[δ(n −k)]
The response L[δ(n −k)] can be interpreted as the response of a linear
system at time n due to a unit sample (a well-known sequence) at time k.
It is called an impulse response and is denoted by h(n, k). The output
then is given by the superposition summation
y(n) =
∞
	
n=−∞
x(k)h(n, k)
(2.11)
The computation of (2.11) requires the time-varying impulse response
h(n, k), which in practice is not very convenient. Therefore, time-invariant
systems are widely used in DSP.
□
EXAMPLE 2.5
Determine whether the following systems are linear.
1.
y(n) = T[x(n)] = 3x2(n)
2.
y(n) = 2x(n −2) + 5
3.
y(n) = x(n + 1) −x(n −1)
Solution
Let y1(n) = T

x1(n)

and y2(n) = T

x2(n)

. We will determine the
response of each system to the linear combination a1x1(n) + a2x2(n) and
check whether it is equal to the linear combination a1x1(n) + a2x2(n)
where a1 and a2 are arbitrary constants.
1.
y(n) = T[x(n)] = 3x2(n): Consider
T

a1x1(n) + a2x2(n)

= 3 [a1x1(n) + a2x2(n)]2
= 3a2
1x2
1(n) + 3a2
2x2
2(n) + 6a1a2x1(n)x2(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

38
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
which is not equal to
a1y1(n) + a2y2(n) = 3a2
1x2
1(n) + 3a2
2x2
2(n)
Hence the given system is nonlinear.
2.
y(n) = 2x(n −2) + 5: Consider
T

a1x1(n) + a2x2(n)

= 2 [a1x1(n −2) + a2x2(n −2)] + 5
= a1y1(n) + a2y2(n) −5
Clearly, the given system is nonlinear even though the input-output
relation is a straight-line function.
3.
y(n) = x(n + 1) −x(1 −n): Consider
T[a1x1(n) + a2x2(n)] = a1x1(n + 1) + a2x2(n + 1) + a1x1(1 −n)
+ a2x2(1 −n)
= a1[x1(n + 1) −x1(1 −n)]
+ a2[x2(n + 1) −x2(1 −n)]
= a1y1(n) + a2y2(n)
Hence the given system is linear.
□
Linear time-invariant (LTI) system
A linear system in which an
input-output pair, x(n) and y(n), is invariant to a shift k in time is called
a linear time-invariant system, that is,
y(n) = L[x(n)] ⇒L[x(n −k)] = y(n −k)
(2.12)
For an LTI system, the L[·] and the shifting operators are reversible as
shown here.
x(n) −→L [·] −→y(n) −→Shift by k −→y(n −k)
x(n) −→Shift by k −→x(n −k) −→L [·] −→y(n −k)
□
EXAMPLE 2.6
Determine whether the following linear systems are time-invariant.
1.
y(n) = L[x(n)] = 10 sin(0.1πn)x(n)
2.
y(n) = L[x(n)] = x(n + 1) −x(1 −n)
3.
y(n) = L[x(n)] = 1
4x(n) + 1
2x(n −1) + 1
4x(n −2)
Solution
First we will compute the response yk(n)
△= L[x(n −k)] to the shifted
input sequence. This is obtained by subtracting k from the arguments of
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Discrete Systems
39
every input sequence term on the right-hand side of the linear transforma-
tion. To determine time-invariance, we will then compare it to the shifted
output sequence y(n −k), obtained after replacing every n by (n −k) on
the right-hand side of the linear transformation.
1.
y(n) = L[x(n)] = 10 sin(0.1πn)x(n): The response due to shifted
input is
yk(n) = L[x(n −k)] = 10 sin(0.1πn)x(n −k)
while the shifted output is
y(n −k) = 10 sin[0.1π(n −k)]x(n −k) ̸= yk(n)
Hence the given system is not time-invariant.
2.
y(n) = L[x(n)] = x(n + 1) −x(1 −n): The response due to shifted
input is
yk(n) = L[x(n −k)] = x(n −k + 1) −x(1 −n −k)
while the shifted output is
y(n−k) = x(n−k)−x(1−[n−k]) = x(n−k+1)−x(1−n+k) ̸= yk(n).
Hence the given system is not time-invariant.
3.
y(n) = L[x(n)] = 1
4x(n) + 1
2x(n −1) + 1
4x(n −2): The response due
to shifted input is
yk(n) = L[x(n −k)] = 1
4x(n −k) + 1
2x(n −1 −k) + 1
4x(n −2 −k)
while the shifted output is
y(n −k) = 1
4x(n −k) + 1
2x(n −k −1) + 1
4x(n −k −2) = yk(n)
Hence the given system is time-invariant.
□
We will denote an LTI system by the operator LTI [·]. Let x(n) and
y(n) be the input-output pair of an LTI system. Then the time-varying
function h(n, k) becomes a time-invariant function h(n −k), and the out-
put from (2.11) is given by
y(n) = LTI [x(n)] =
∞
	
k=−∞
x(k)h(n −k)
(2.13)
The impulse response of an LTI system is given by h(n). The mathemati-
cal operation in (2.13) is called a linear convolution sum and is denoted by
y(n)
△= x(n) ∗h(n)
(2.14)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

40
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
Hence an LTI system is completely characterized in the time domain by
the impulse response h(n).
x(n) −→h(n) −→y(n) = x(n) ∗h(n)
We will explore several properties of the convolution in Problem P2.14.
Stability
This is a very important concept in linear system theory. The
primary reason for considering stability is to avoid building harmful sys-
tems or to avoid burnout or saturation in the system operation. A system
is said to be bounded-input bounded-output (BIBO) stable if every bounded
input produces a bounded output.
|x(n)| < ∞⇒|y(n)| < ∞, ∀x, y
An LTI system is BIBO stable if and only if its impulse response is abso-
lutely summable.
BIBO Stability ⇐⇒
∞
	
−∞
|h(n)| < ∞
(2.15)
Causality
This important concept is necessary to make sure that sys-
tems can be built. A system is said to be causal if the output at index n0
depends only on the input up to and including the index n0; that is, the
output does not depend on the future values of the input. An LTI system
is causal if and only if the impulse response
h(n) = 0,
n < 0
(2.16)
Such a sequence is termed a causal sequence. In signal processing, unless
otherwise stated, we will always assume that the system is causal.
2.3 CONVOLUTION
We introduced the convolution operation (2.14) to describe the response
of an LTI system. In DSP, it is an important operation and has many other
uses that we will see throughout this book. Convolution can be evaluated
in many diﬀerent ways. If the sequences are mathematical functions (of
ﬁnite or inﬁnite duration), then we can analytically evaluate (2.14) for all
n to obtain a functional form of y(n).
□
EXAMPLE 2.7
Let the rectangular pulse x(n) = u(n) −u(n −10) of Example 2.4 be an input
to an LTI system with impulse response
h(n) = (0.9)n u(n)
Determine the output y(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Convolution
41
−5
0
5
10
15
20
25
30
35
40
45
50
0
0.5
1
1.5
2
Input Sequence
n
x(n)
−5
0
5
10
15
20
25
30
35
40
45
50
0
0.5
1
1.5
2
Impulse Response
n
h(n)
FIGURE 2.5
The input sequence and the impulse response in Example 2.7
Solution
The input x(n) and the impulse response h(n) are shown in Figure 2.5. From
(2.14),
y(n) =
9
	
k=0
(1) (0.9)(n−k) u(n −k) = (0.9)n
9
	
k=0
(0.9)−k u(n −k)
(2.17)
The sum in (2.17) is almost a geometric series sum except that the term u(n−k)
takes diﬀerent values depending on n and k. There are three possible conditions
under which u(n −k) can be evaluated.
CASE i
n < 0: Then u(n −k) = 0,
0 ≤k ≤9. Hence, from (2.17),
y(n) = 0
(2.18)
In this case, the nonzero values of x(n) and h(n) do not overlap.
CASE ii
0 ≤n < 9: Then u(n −k) = 1, 0 ≤k ≤n. Hence, from (2.17),
y(n) = (0.9)n
n
	
k=0
(0.9)−k = (0.9)n
n
	
k=0
[(0.9)−1]k
= (0.9)n 1 −(0.9)−(n+1)
1 −(0.9)−1
= 10[1 −(0.9)n+1],
0 ≤n < 9
(2.19)
In this case, the impulse response h(n) partially overlaps the input x(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

42
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
−5
0
5
10
15
20
25
30
35
40
45
50
0
2
4
6
8
Output Sequence
n
y(n)
FIGURE 2.6
The output sequence in Example 2.7
CASE iii
n ≥9: Then u(n −k) = 1, 0 ≤k ≤9, and from (2.17),
y(n) = (0.9)n
9
	
k=0
(0.9)−k
= (0.9)n 1 −(0.9)−10
1 −(0.9)−1 = 10(0.9)n−9[1 −(0.9)10],
n ≥9
(2.20)
In this last case, h(n) completely overlaps x(n).
The complete response is given by (2.18), (2.19), and (2.20). It is shown in
Figure 2.6, which depicts the distortion of the input pulse.
□
This example can also be done using a method called graphical convo-
lution, in which (2.14) is given a graphical interpretation. In this method,
h(n −k) is interpreted as a folded-and-shifted version of h(k). The output
y(n) is obtained as a sample sum under the overlap of x(k) and h(n −k).
We use an example to illustrate this.
□
EXAMPLE 2.8
Given the two sequences
x(n) = [3, 11, 7, 0
↑
, −1, 4, 2],
−3 ≤n ≤3;
h(n) = [2, 3
↑
, 0, −5, 2, 1],
−1 ≤n ≤4
determine the convolution y(n) = x(n) ∗h(n).
Solution
In Figure 2.7, we show four plots. The top-left plot shows x(k) and h(k), the
original sequences. The top-right plot shows x(k) and h(−k), the folded version
of h(k). The bottom-left plot shows x(k) and h(−1−k), the “folded and shifted
by −1” version of h(k). Then
	
k
x(k)h(−1 −k) = 3 × (−5) + 11 × 0 + 7 × 3 + 0 × 2 = 6 = y(−1)
The bottom-right plot shows x(k) and h(2 −k), the “folded and shifted by 2”
version of h(k), which gives
	
k
x(k)h(2−k) = 11×1+7×2+0×(−5)+(−1)×0+4×3+2×2 = 41 = y(2)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Convolution
43
−5
0
5
−5
0
5
10
x(k) and h(k)
k
Amplitude
Solid: x   Dashed: h
−5
0
5
−5
0
5
10
x(k) and h(−k)
k
Amplitude
n = 0
Solid: x   Dashed: h
−5
0
5
−5
0
5
10
x(k) and h(−1−k)
k
Amplitude
n = −1
Solid: x   Dashed: h
−5
0
5
−5
0
5
10
x(k) and h(2−k)
k
Amplitude
n = 2
Solid: x   Dashed: h
FIGURE 2.7
Graphical convolution in Example 2.8
Thus we have obtained two values of y(n). Similar graphical calculations can
be done for other remaining values of y(n). Note that the beginning point (ﬁrst
nonzero sample) of y(n) is given by n = −3 + (−1) = −4, while the end point
(the last nonzero sample) is given by n = 3 + 4 = 7. The complete output is
given by
y(n) = {6, 31, 47, 6, −51
↑
, −5, 41, 18, −22, −3, 8, 2}
Students are strongly encouraged to verify the above result. Note that the re-
sulting sequence y(n) has a longer length than both the x(n) and h(n) sequences.
□
2.3.1 MATLAB IMPLEMENTATION
If arbitrary sequences are of inﬁnite duration, then MATLAB cannot
be used directly to compute the convolution. MATLAB does provide a
built-in function called conv that computes the convolution between two
ﬁnite-duration sequences. The conv function assumes that the two se-
quences begin at n = 0 and is invoked by
>> y = conv(x,h);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

44
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
For example, to do the convolution in Example 2.7, we could use
>> x = [3, 11, 7, 0, -1, 4, 2];
h = [2, 3, 0, -5, 2, 1];
>> y = conv(x, h)
y =
6
31
47
6
-51
-5
41
18
-22
-3
8
2
to obtain the correct y(n) values. However, the conv function neither
provides nor accepts any timing information if the sequences have arbi-
trary support. What is needed is a beginning point and an end point of
y(n). Given ﬁnite duration x(n) and h(n), it is easy to determine these
points. Let
{x(n); nxb ≤n ≤nxe}
and
{h(n); nhb ≤n ≤nhe}
be two ﬁnite-duration sequences. Then referring to Example 2.8, we
observe that the beginning and end points of y(n) are
nyb = nxb + nhb
and
nye = nxe + nhe
respectively. A simple modiﬁcation of the conv function, called conv m,
which performs the convolution of arbitrary support sequences, can now
be designed.
function [y,ny] = conv_m(x,nx,h,nh)
% Modified convolution routine for signal processing
% --------------------------------------------------
%
[y,ny] = conv_m(x,nx,h,nh)
%
[y,ny] = convolution result
%
[x,nx] = first signal
%
[h,nh] = second signal
%
nyb = nx(1)+nh(1); nye = nx(length(x)) + nh(length(h));
ny = [nyb:nye];
y = conv(x,h);
□
EXAMPLE 2.9
Perform the convolution in Example 2.8 using the conv m function.
Solution
MATLAB script:
>> x = [3, 11, 7, 0, -1, 4, 2]; nx = [-3:3];
>> h = [2, 3, 0, -5, 2, 1]; nh = [-1:4];
>> [y,ny] = conv_m(x,nx,h,nh)
y =
6
31
47
6
-51
-5
41
18
-22
-3
8
2
ny =
-4
-3
-2
-1
0
1
2
3
4
5
6
7
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Convolution
45
Hence
y(n) = {6, 31, 47, 6, −51
↑
, −5, 41, 18, −22, −3, 8, 2}
as in Example 2.8.
□
An alternate method in MATLAB can be used to perform the convo-
lution. This method uses a matrix-vector multiplication approach, which
we will explore in Problem P2.17.
2.3.2 SEQUENCE CORRELATIONS REVISITED
If we compare the convolution operation (2.14) with that of the cross-
correlation of two sequences deﬁned in (2.8), we observe a close resem-
blance. The cross-correlation ryx(ℓ) can be put in the form
ryx(ℓ) = y(ℓ) ∗x(−ℓ)
with the autocorrelation rxx(ℓ) in the form
rxx(ℓ) = x(ℓ) ∗x(−ℓ)
Therefore, these correlations can be computed using the conv m function
if sequences are of ﬁnite duration.
□
EXAMPLE 2.10
In this example, we will demonstrate one application of the cross-correlation
sequence. Let
x(n) = [3, 11, 7, 0
↑
, −1, 4, 2]
be a prototype sequence, and let y(n) be its noise-corrupted-and-shifted version
y(n) = x(n −2) + w(n)
where w(n) is a Gaussian sequence with mean 0 and variance 1. Compute the
cross-correlation between y(n) and x(n).
Solution
From the construction of y(n), it follows that y(n) is “similar” to x(n −2) and
hence their cross-correlation would show the strongest similarity at ℓ= 2. To
test this out using MATLAB, let us compute the cross-correlation using two
diﬀerent noise sequences.
% Noise sequence 1
>> x = [3, 11, 7, 0, -1, 4, 2]; nx=[-3:3]; % Given signal x(n)
>> [y,ny] = sigshift(x,nx,2);
% Obtain x(n-2)
>> w = randn(1,length(y)); nw = ny;
% Generate w(n)
>> [y,ny] = sigadd(y,ny,w,nw);
% Obtain y(n) = x(n-2) + w(n)
>> [x,nx] = sigfold(x,nx);
% Obtain x(-n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

46
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
−4
−2
0
2
4
6
8
0
100
200
Lag Variable l
rxy
Cross-correlation: Noise Sequence 1 
Maximum
−4
−2
0
2
4
6
8
0
100
200
Maximum
Lag Variable l
rxy
Cross-correlation: Noise Sequence 2 
FIGURE 2.8
Cross-correlation sequence with two diﬀerent noise realizations
>> [rxy,nrxy] = conv_m(y,ny,x,nx);
% Cross-correlation
>> subplot(1,1,1), subplot(2,1,1);stem(nrxy,rxy)
>> axis([-5,10,-50,250]);xlabel(’Lag Variable l’)
>> ylabel(’rxy’);title(’Cross-correlation: Noise Sequence 1’)
%
% Noise sequence 2
>> x = [3, 11, 7, 0, -1, 4, 2]; nx=[-3:3]; % Given signal x(n)
>> [y,ny] = sigshift(x,nx,2);
% Obtain x(n-2)
>> w = randn(1,length(y)); nw = ny;
% Generate w(n)
>> [y,ny] = sigadd(y,ny,w,nw);
% Obtain y(n) = x(n-2) + w(n)
>> [x,nx] = sigfold(x,nx);
% Obtain x(-n)
>> [rxy,nrxy] = conv_m(y,ny,x,nx);
% Cross-correlation
>> subplot(2,1,2);stem(nrxy,rxy)
>> axis([-5,10,-50,250]);xlabel(’Lag Variable l’)
>> ylabel(’rxy’);title(’Cross-correlation: Noise sequence 2’)
From Figure 2.8, we observe that the cross-correlation indeed peaks at ℓ= 2,
which implies that y(n) is similar to x(n) shifted by 2. This approach can be
used in applications like radar signal processing in identifying and localizing
targets.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Diﬀerence Equations
47
Note that the signal-processing toolbox in MATLAB also provides a
function called xcorr for sequence correlation computations. In its sim-
plest form,
>> xcorr(x,y)
computes the cross-correlation between vectors x and y, while
>> xcorr(x)
computes the autocorrelation of vector x. It generates results that are
identical to the one obtained from the proper use of the conv m function.
However, the xcorr function cannot provide the timing (or lag) informa-
tion (as done by the conv m function), which then must be obtained by
some other means.
2.4 DIFFERENCE EQUATIONS
An LTI discrete system can also be described by a linear constant coeﬃ-
cient diﬀerence equation of the form
N
	
k=0
aky(n −k) =
M
	
m=0
bmx(n −m),
∀n
(2.21)
If aN ̸= 0, then the diﬀerence equation is of order N. This equation de-
scribes a recursive approach for computing the current output, given the
input values and previously computed output values. In practice, this
equation is computed forward in time, from n = −∞to n = ∞. There-
fore, another form of this equation is
y(n) =
M
	
m=0
bmx(n −m) −
N
	
k=1
aky(n −k)
(2.22)
A solution to this equation can be obtained in the form
y(n) = yH(n) + yP(n)
The homogeneous part of the solution, yH(n), is given by
yH(n) =
N
	
k=1
ckzn
k
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

48
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
where zk, k = 1, . . . , N are N roots (also called natural frequencies) of the
characteristic equation
N
	
0
akzN−k = 0
This characteristic equation is important in determining the stability of
systems. If the roots zk satisfy the condition
|zk| < 1, k = 1, . . . , N
(2.23)
then a causal system described by (2.22) is stable. The particular part of
the solution, yP(n), is determined from the right-hand side of (2.21). In
Chapter 4, we will discuss the analytical approach of solving diﬀerence
equations using the z-transform.
2.4.1 MATLAB IMPLEMENTATION
A function called filter is available to solve diﬀerence equations nu-
merically, given the input and the diﬀerence equation coeﬃcients. In its
simplest form, this function is invoked by
y = filter(b,a,x)
where
b = [b0, b1, ..., bM]; a = [a0, a1, ..., aN];
are the coeﬃcient arrays from the equation given in (2.21) and x is the
input sequence array. The output y has the same length as input x. One
must ensure that the coeﬃcient a0 not be zero.
To compute and plot impulse response, MATLAB provides the func-
tion impz. When invoked by
h = impz(b,a,n);
it computes samples of the impulse response of the ﬁlter at the sample
indices given in n with numerator coeﬃcients in b and denominator co-
eﬃcients in a. When no output arguments are given, the impz function
plots the response in the current ﬁgure window using the stem function.
We will illustrate the use of these functions in the following example.
□
EXAMPLE 2.11
Consider the following diﬀerence equation:
y(n) −y(n −1) + 0.9y(n −2) = x(n);
∀n
a.
Calculate and plot the impulse response h(n) at n = −20, . . . , 100.
b.
Calculate and plot the unit step response s(n) at n = −20, . . . , 100.
c.
Is the system speciﬁed by h(n) stable?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Diﬀerence Equations
49
Solution
From the given diﬀerence equation, the coeﬃcient arrays are
b = [1]; a=[1, -1, 0.9];
a. MATLAB script:
>> b = [1]; a = [1, -1, 0.9];
n = [-20:120];
>> h = impz(b,a,n);
>> subplot(2,1,1); stem(n,h);
>> title(’Impulse Response’); xlabel(’n’); ylabel(’h(n)’)
The plot of the impulse response is shown in Figure 2.9.
b. MATLAB script:
>> x = stepseq(0,-20,120);
s = filter(b,a,x);
>> subplot(2,1,2); stem(n,s)
>> title(’Step Response’); xlabel(’n’); ylabel(’s(n)’)
The plot of the unit step response is shown in Figure 2.9.
c. To determine the stability of the system, we have to determine h(n) for all n.
Although we have not described a method to solve the diﬀerence equation,
−20
0
20
40
60
80
100
120
−1
−0.5
0
0.5
1
Impulse Response
h(n)
n
−20
0
20
40
60
80
100
120
0
1
2
Step Response
n
s(n)
FIGURE 2.9
Impulse response and step response plots in Example 2.11
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

50
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
we can use the plot of the impulse response to observe that h(n) is practically
zero for n > 120. Hence the sum 
|h(n)| can be determined from MATLAB
using
>> sum(abs(h))
ans = 14.8785
which implies that the system is stable. An alternate approach is to use the
stability condition (2.23) using MATLAB’s roots function.
>>z = roots(a);
magz = abs(z)
magz =
0.9487
0.9487
Since the magnitudes of both roots are less than 1, the system is stable.
□
In the previous section, we noted that if one or both sequences in
the convolution are of inﬁnite length, then the conv function cannot be
used. If one of the sequences is of inﬁnite length, then it is possible to use
MATLAB for numerical evaluation of the convolution. This is done using
the filter function, as we will see in the following example.
□
EXAMPLE 2.12
Let us consider the convolution given in Example 2.7. The input sequence is of
ﬁnite duration
x(n) = u(n) −u(n −10)
while the impulse response is of inﬁnite duration
h(n) = (0.9)n u(n)
Determine y(n) = x(n) ∗h(n).
Solution
If the LTI system, given by the impulse response h(n), can be described by a
diﬀerence equation, then y(n) can be obtained from the filter function. From
the h(n) expression,
(0.9) h(n −1) = (0.9) (0.9)n−1 u(n −1) = (0.9)n u(n −1)
or
h(n) −(0.9) h(n −1) = (0.9)n u(n) −(0.9)n u(n −1)
= (0.9)n [u(n) −u(n −1)] = (0.9)n δ(n)
= δ(n)
The last step follows from the fact that δ(n) is nonzero only at n = 0. By
deﬁnition, h(n) is the output of an LTI system when the input is δ(n). Hence
substituting x(n) for δ(n) and y(n) for h(n), the diﬀerence equation is
y(n) −0.9y(n −1) = x(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Diﬀerence Equations
51
−5
0
5
10
15
20
25
30
35
40
45
50
0
2
4
6
8
Output Sequence
n
y(n)
FIGURE 2.10
Output sequence in Example 2.12
Now MATLAB’s filter function can be used to compute the convolution in-
directly.
>> b = [1]; a = [1,-0.9];
>> n = -5:50; x = stepseq(0,-5,50) - stepseq(10,-5,50);
>> y = filter(b,a,x);
>> subplot(2,1,2); stem(n,y); title(’Output Sequence’)
>> xlabel(’n’); ylabel(’y(n)’); axis([-5,50,-0.5,8])
The plot of the output is shown in Figure 2.10, which is exactly the same as
that in Figure 2.6.
□
In Example 2.12, the impulse response was a one-sided exponential se-
quence for which we could determine a diﬀerence equation representation.
This means that not all inﬁnite-length impulse responses can be converted
into diﬀerence equations. The above analysis, however, can be extended to
a linear combination of one-sided exponential sequences, which results in
higher-order diﬀerence equations. We will discuss this topic of conversion
from one representation to another one in Chapter 4.
2.4.2 ZERO-INPUT AND ZERO-STATE RESPONSES
In DSP, the diﬀerence equation is generally solved forward in time from
n = 0. Therefore, initial conditions on x(n) and y(n) are necessary to
determine the output for n ≥0. The diﬀerence equation is then given by
y(n) =
M
	
m=0
bmx(n −m) −
N
	
k=1
aky(n −k); n ≥0
(2.24)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

52
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
subject to the initial conditions
{y(n); −N ≤n ≤−1}
and
{x(n); −M ≤n ≤−1}
A solution to (2.24) can be obtained in the form
y(n) = yZI(n) + yZS(n)
where yZI(n) is called the zero-input solution, which is a solution due
to the initial conditions alone (assuming they exist), while the zero-state
solution, yZS(n), is a solution due to input x(n) alone (or assuming that
the initial conditions are zero). In MATLAB, another form of the function
filter can be used to solve for the diﬀerence equation, given its initial
conditions. We will illustrate the use of this form in Chapter 4.
2.4.3 DIGITAL FILTERS
Filter is a generic name that means a linear time-invariant system designed
for a speciﬁc job of frequency selection or frequency discrimination. Hence
discrete-time LTI systems are also called digital ﬁlters. There are two
types of digital ﬁlters.
FIR filter
If the unit impulse response of an LTI system is of ﬁnite
duration, then the system is called a finite-duration impulse response (or
FIR) ﬁlter. Hence, for an FIR ﬁlter, h(n) = 0 for n < n1 and for n > n2.
The following part of the diﬀerence equation (2.21) describes a causal FIR
ﬁlter:
y(n) =
M
	
m=0
bmx(n −m)
(2.25)
Furthermore, h(0) = b0, h(1) = b1, . . . , h(M) = bM, while all other h(n)’s
are 0. FIR ﬁlters are also called nonrecursive or moving average (MA)
ﬁlters. In MATLAB, FIR ﬁlters are represented either as impulse response
values {h(n)} or as diﬀerence equation coeﬃcients {bm} and {a0 = 1}.
Therefore, to implement FIR ﬁlters, we can use either the conv(x,h)
function (and its modiﬁcation that we discussed) or the filter(b,1,x)
function. There is a diﬀerence in the outputs of these two implementations
that should be noted. The output sequence from the conv(x,h) function
has a longer length than both the x(n) and h(n) sequences. On the other
hand, the output sequence from the filter(b,1,x) function has exactly
the same length as the input x(n) sequence. In practice (and especially
for processing signals), the use of the filter function is encouraged.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
53
IIR filter
If the impulse response of an LTI system is of inﬁnite dura-
tion, then the system is called an infinite-duration impulse response (or
IIR) ﬁlter. The part of diﬀerence equation (2.21)
N
	
k=0
aky(n −k) = x(n)
(2.26)
describes a recursive ﬁlter in which the output y(n) is recursively com-
puted from its previously computed values and is called an autoregressive
(AR) ﬁlter. The impulse response of such a ﬁlter is of inﬁnite duration,
and hence it represents an IIR ﬁlter. The general equation (2.21) also de-
scribes an IIR ﬁlter. It has two parts: an AR part and an MA part. Such
an IIR ﬁlter is called an autoregressive moving average, or an ARMA,
ﬁlter. In MATLAB, IIR ﬁlters are described by the diﬀerence equation
coeﬃcients {bm} and {ak} and are implemented by the filter(b,a,x)
function.
2.5 PROBLEMS
P2.1
Generate the following sequences using the basic MATLAB signal functions and the basic
MATLAB signal operations discussed in this chapter. Plot signal samples using the stem
function.
1. x1(n) = 3δ(n + 2) + 2δ(n) −δ(n −3) + 5δ(n −7), −5 ≤n ≤15.
2. x2(n) = 5
k=−5 e−|k|δ(n −2k), −10 ≤n ≤10.
3. x3(n) = 10u(n) −5u(n −5) −10u(n −10) + 5u(n −15).
4. x4(n) = e0.1n[u(n + 20) −u(n −10)].
5. x5(n) = 5[cos(0.49πn) + cos(0.51πn)], −200 ≤n ≤200. Comment on the waveform
shape.
6. x6(n) = 2 sin(0.01πn) cos(0.5πn), −200 ≤n ≤200. Comment on the waveform shape.
7. x7(n) = e−0.05n sin(0.1πn + π/3), 0 ≤n ≤100. Comment on the waveform shape.
8. x8(n) = e0.01n sin(0.1πn), 0 ≤n ≤100. Comment on the waveform shape.
P2.2
Generate the following random sequences and obtain their histogram using the hist
function with 100 bins. Use the bar function to plot each histogram.
1. x1(n) is a random sequence whose samples are independent and uniformly distributed
over [0, 2] interval. Generate 100,000 samples.
2. x2(n) is a Gaussian random sequence whose samples are independent with mean 10 and
variance 10. Generate 10,000 samples.
3. x3(n) = x1(n) + x1(n −1) where x1(n) is the random sequence given in part 1 above.
Comment on the shape of this histogram and explain the shape.
4. x4(n) = 4
k=1 yk(n) where each random sequence yk(n) is independent of others with
samples uniformly distributed over [−0.5, 0.5]. Comment on the shape of this histogram.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

54
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
P2.3
Generate the following periodic sequences and plot their samples (using the stem function)
over the indicated number of periods.
1. ˜x1(n) = {. . . , −2, −1, 0
↑
, 1, 2, . . .}periodic. Plot ﬁve periods.
2. ˜x2(n) = e0.1n[u(n) −u(n −20)]periodic. Plot three periods.
3. ˜x3(n) = sin(0.1πn)[u(n) −u(n −10)]. Plot four periods.
4. ˜x4(n) = {. . . , 1
↑
, 2, 3, . . .}periodic + {. . . , 1
↑
, 2, 3, 4, . . .}periodic, 0 ≤n ≤24. What is the
period of ˜x4(n)?
P2.4
Let x(n) = {2, 4, −3, 1
↑
, −5, 4, 7}. Generate and plot the samples (use the stem function) of
the following sequences.
1. x1(n) = 2x(n −3) + 3x(n + 4) −x(n)
2. x2(n) = 4x(4 + n) + 5x(n + 5) + 2x(n)
3. x3(n) = x(n + 3)x(n −2) + x(1 −n)x(n + 1)
4. x4(n) = 2e0.5nx(n) + cos (0.1πn) x (n + 2) , −10 ≤n ≤10
P2.5
The complex exponential sequence ejω0n or the sinusoidal sequence cos (ω0n) are periodic if
the normalized frequency f0
△= ω0
2π is a rational number; that is, f0 = K
N , where K and N
are integers.
1. Prove the above result.
2. Generate exp(j0.1πn), −100 ≤n ≤100. Plot its real and imaginary parts using the stem
function. Is this sequence periodic? If it is, what is its fundamental period? From the
examination of the plot, what interpretation can you give to the integers K and N
above?
3. Generate and plot cos(0.1n), −20 ≤n ≤20. Is this sequence periodic? What do you
conclude from the plot? If necessary, examine the values of the sequence in MATLAB to
arrive at your answer.
P2.6
Using the evenodd function, decompose the following sequences into their even and odd
components. Plot these components using the stem function.
1. x1(n) = {0
↑
, 1, 2, 3, 4, 5, 6, 7, 8, 9}
2. x2(n) = e0.1n[u(n + 5) −u(n −10)]
3. x3(n) = cos(0.2πn + π/4), −20 ≤n ≤20
4. x4(n) = e−0.05n sin(0.1πn + π/3), 0 ≤n ≤100
P2.7
A complex-valued sequence xe(n) is called conjugate-symmetric if xe(n) = x∗
e(−n), and a
complex-valued sequence xo(n) is called conjugate-antisymmetric if xo(n) = −x∗
o(−n). Then
any arbitrary complex-valued sequence x(n) can be decomposed into x(n) = xe(n) + xo(n),
where xe(n) and xo(n) are given by
xe(n) = 1
2 [x(n) + x∗(−n)]
and
xo(n) = 1
2 [x(n) −x∗(−n)]
(2.27)
respectively.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
55
1. Modify the evenodd function discussed in the text so that it accepts an arbitrary
sequence and decomposes it into its conjugate-symmetric and conjugate-antisymmetric
components by implementing (2.27).
2. Decompose the sequence
x(n) = 10 exp([−0.1 + j0.2π]n),
0 ≤n ≤10
into its conjugate-symmetric and conjugate-antisymmetric components. Plot their real
and imaginary parts to verify the decomposition. (Use the subplot function.)
P2.8
The operation of signal dilation (or decimation or down-sampling) is deﬁned by
y(n) = x(nM)
in which the sequence x(n) is down-sampled by an integer factor M. For example, if
x(n) = {. . . , −2, 4, 3
↑
, −6, 5, −1, 8, . . .}
then the down-sampled sequences by a factor 2 are given by
y(n) = {. . . , −2, 3
↑
, 5, 8, . . .}
1. Develop a MATLAB function dnsample that has the form
function [y,m] = dnsample(x,n,M)
% Downsample sequence x(n) by a factor M to obtain y(m)
to implement the above operation. Use the indexing mechanism of MATLAB with
careful attention to the origin of the time axis n = 0.
2. Generate x(n) = sin(0.125πn), −50 ≤n ≤50. Decimate x(n) by a factor of 4 to generate
y(n). Plot both x(n) and y(n) using subplot, and comment on the results.
3. Repeat the above using x(n) = sin(0.5πn), −50 ≤n ≤50. Qualitatively discuss the eﬀect
of down-sampling on signals.
P2.9
Using the conv_m function, determine the autocorrelation sequence rxx(ℓ) and the cross-
correlation sequence rxy(ℓ) for the following sequences:
x(n) = (0.9)n ,
0 ≤n ≤20;
y(n) = (0.8)−n,
−20 ≤n ≤0
Describe your observations of these results.
P2.10 In a certain concert hall, echoes of the original audio signal x(n) are generated due to the
reﬂections at the walls and ceiling. The audio signal experienced by the listener y(n) is a
combination of x(n) and its echoes. Let
y(n) = x(n) + αx(n −k)
where k is the amount of delay in samples and α is its relative strength. We want to
estimate the delay using the correlation analysis.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

56
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
1. Determine analytically the cross-correlation ryx(ℓ) in terms of the autocorrelation rxx(ℓ).
2. Let x(n) = cos(0.2πn) + 0.5 cos(0.6πn), α = 0.1, and k = 50. Generate 200 samples of
y(n) and determine its cross-correlation. Can you obtain α and k by observing ryx(ℓ)?
P2.11 Consider the following discrete-time systems:
T1[x(n)] = x(n)u(n)
T2[x(n)] = x(n) + n x(n + 1)
T3[x(n)] = x(n) + 1
2x(n −2) −1
3x(n −3)x(2n)
T4[x(n)] = n+5
k=−∞2x(k)
T5[x(n)] = x(2n)
T6[x(n)] = round[x(n)]
where round[·] denotes rounding to the nearest integer.
1. Use (2.10) to determine analytically whether these systems are linear.
2. Let x1(n) be a uniformly distributed random sequence between [0, 1] over 0 ≤n ≤100,
and let x2(n) be a Gaussian random sequence with mean 0 and variance 10 over
0 ≤n ≤100. Using these sequences, verify the linearity of these systems. Choose any
values for constants a1 and a2 in (2.10). You should use several realizations of the above
sequences to arrive at your answers.
P2.12 Consider the discrete-time systems given in Problem P2.11.
1. Use (2.12) to determine analytically whether these systems are time-invariant.
2. Let x(n) be a Gaussian random sequence with mean 0 and variance 10 over 0 ≤n ≤100.
Using this sequence, verify the time invariance of the above systems. Choose any values
for sample shift k in (2.12). You should use several realizations of the above sequence to
arrive at your answers.
P2.13 For the systems given in Problem P2.11, determine analytically their stability and causality.
P2.14 The linear convolution deﬁned in (2.14) has several properties:
x1(n) ∗x2(n) = x1(n) ∗x2(n)
: Commutation
[x1(n) ∗x2(n)] ∗x3(n) = x1(n) ∗[x2(n) ∗x3(n)]
: Association
x1(n) ∗[x2(n) + x3(n)] = x1(n) ∗x2(n) + x1(n) ∗x3(n) : Distribution
x(n) ∗δ(n −n0) = x(n −n0)
: Identity
(2.28)
1. Analytically prove these properties.
2. Using the following three sequences, verify the above properties:
x1(n) = cos(πn/4)[u(n + 5) −u(n −25)]
x2(n) = (0.9)−n[u(n) −u(n −20)]
x3(n) = round[5w(n)], −10 ≤n ≤10, where w(n) is uniform over [−1, 1]
Use the conv m function.
P2.15 Determine analytically the convolution y(n) = x(n) ∗h(n) of the following sequences, and
verify your answers using the conv_m function.
1. x(n) = {2, −4, 5, 3
↑
, −1, −2, 6}, h(n) = {1, −1
↑
, 1, −1, 1}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
57
2. x(n) = {1, 1, 0
↑
, 1, 1}, h(n) = {1, −2, −3, 4
↑
}
3. x(n) = (1/4)−n[u(n + 1) −u(n −4)], h(n) = u(n) −u(n −5)
4. x(n) = n/4[u(n) −u(n −6)], h(n) = 2[u(n + 2) −u(n −3)]
P2.16 Let x(n) = (0.8)nu(n), h(n) = (−0.9)nu(n), and y(n) = h(n) ∗x(n). Use three columns and
one row of subplots for the following parts.
1. Determine y(n) analytically. Plot ﬁrst 51 samples of y(n) using the stem function.
2. Truncate x(n) and h(n) to 26 samples. Use conv function to compute y(n). Plot y(n)
using the stem function. Compare your results with those of part 1.
3. Using the filter function, determine the ﬁrst 51 samples of x(n) ∗h(n). Plot y(n) using
the stem function. Compare your results with those of parts 1 and 2.
P2.17 When the sequences x(n) and h(n) are of ﬁnite duration Nx and Nh, respectively, then
their linear convolution (2.13) can also be implemented using matrix-vector multiplication.
If elements of y(n) and x(n) are arranged in column vectors x and y, respectively, then
from (2.13) we obtain
y = Hx
where linear shifts in h(n −k) for n = 0, . . . , Nh −1 are arranged as rows in the matrix H.
This matrix has an interesting structure and is called a Toeplitz matrix. To investigate this
matrix, consider the sequences
x(n) = {1
↑
, 2, 3, 4, 5}
and
h(n) = {6
↑
, 7, 8, 9}
1. Determine the linear convolution y(n) = h(n) ∗x(n).
2. Express x(n) as a 5 × 1 column vector x and y(n) as a 8 × 1 column vector y. Now
determine the 8 × 5 matrix H so that y = Hx.
3. Characterize the matrix H. From this characterization, can you give a deﬁnition of a
Toeplitz matrix? How does this deﬁnition compare with that of time invariance?
4. What can you say about the ﬁrst column and the ﬁrst row of H?
P2.18 MATLAB provides a function called toeplitz to generate a Toeplitz matrix, given the ﬁrst
row and the ﬁrst column.
1. Using this function and your answer to Problem P2.17, part 4, develop another MATLAB
function to implement linear convolution. The format of the function should be
function [y,H]=conv_tp(h,x)
% Linear Convolution using Toeplitz Matrix
% ----------------------------------------
% [y,H] = conv_tp(h,x)
% y = output sequence in column vector form
% H = Toeplitz matrix corresponding to sequence h so that y = Hx
% h = Impulse response sequence in column vector form
% x = input sequence in column vector form
2. Verify your function on the sequences given in Problem P2.17.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

58
Chapter 2
DISCRETE-TIME SIGNALS AND SYSTEMS
P2.19 A linear and time-invariant system is described by the diﬀerence equation
y(n) −0.5y(n −1) + 0.25y(n −2) = x(n) + 2x(n −1) + x(n −3)
1. Using the filter function, compute and plot the impulse response of the system over
0 ≤n ≤100.
2. Determine the stability of the system from this impulse response.
3. If the input to this system is x(n) = [5 + 3 cos(0.2πn) + 4 sin(0.6πn)] u(n), determine the
response y(n) over 0 ≤n ≤200 using the filter function.
P2.20 A “simple” digital diﬀerentiator is given by
y(n) = x(n) −x(n −1)
which computes a backward ﬁrst-order diﬀerence of the input sequence. Implement this
diﬀerentiator on the following sequences, and plot the results. Comment on the
appropriateness of this simple diﬀerentiator.
1. x(n) = 5 [u(n) −u(n −20)]: a rectangular pulse
2. x(n) = n [u(n) −u(n −10)] + (20 −n) [u(n −10) −u(n −20)]: a triangular pulse
3. x(n) = sin
πn
25

[u(n) −u(n −100)]: a sinusoidal pulse
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

59
C H A P T E R 3
The Discrete-Time
Fourier Analysis
We have seen how a linear and time-invariant system can be represented
using its response to the unit sample sequence. This response, called the
unit impulse response h(n), allows us to compute the system response to
any arbitrary input x(n) using the linear convolution
x(n) −→h(n) −→y(n) = h(n) ∗x(n)
This convolution representation is based on the fact that any signal
can be represented by a linear combination of scaled and delayed unit
samples. Similarly, we can also represent any arbitrary discrete signal
as a linear combination of basis signals introduced in Chapter 2. Each
basis signal set provides a new signal representation. Each representation
has some advantages and some disadvantages depending upon the type
of system under consideration. However, when the system is linear and
time-invariant, only one representation stands out as the most useful. It
is based on the complex exponential signal set {ejωn} and is called the
discrete-time Fourier transform.
3.1 THE DISCRETE-TIME FOURIER TRANSFORM (DTFT)
If x(n) is absolutely summable, that is, ∞
−∞|x(n)| < ∞, then its discrete-
time Fourier transform is given by
X(ejω)
△= F[x(n)] =
∞
	
n=−∞
x(n)e−jωn
(3.1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

60
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
The inverse discrete-time Fourier transform (IDTFT) of X(ejω) is given
by
x(n)
△= F−1[X(ejω)] = 1
2π
π

−π
X(ejω)ejωndω
(3.2)
The operator F[·] transforms a discrete signal x(n) into a complex-valued
continuous function X(ejω) of real variable ω, called a digital frequency,
which is measured in radians/sample.
□
EXAMPLE 3.1
Determine the discrete-time Fourier transform of x(n) = (0.5)n u(n).
Solution
The sequence x(n) is absolutely summable; therefore, its discrete-time Fourier
transform exists.
X(ejω) =
∞
	
−∞
x(n)e−jωn =
∞
	
0
(0.5)n e−jωn
=
∞
	
0
(0.5e−jω)n =
1
1 −0.5e−jω =
ejω
ejω −0.5
□
□
EXAMPLE 3.2
Determine the discrete-time Fourier transform of the following ﬁnite-duration
sequence:
x(n) = {1, 2
↑
, 3, 4, 5}
Solution
Using deﬁnition (3.1),
X(ejω) =
∞
	
−∞
x(n)e−jωn = ejω + 2 + 3e−jω + 4e−j2ω + 5e−j3ω
□
Since X(ejω) is a complex-valued function, we will have to plot its
magnitude and its angle (or the real part and the imaginary part) with
respect to ω separately to visually describe X(ejω). Now ω is a real vari-
able between −∞and ∞, which would mean that we can plot only a
part of the X(ejω) function using MATLAB. Using two important prop-
erties of the discrete-time Fourier transform, we can reduce this domain
to the [0, π] interval for real-valued sequences. We will discuss other useful
properties of X(ejω) in the next section.
3.1.1 TWO IMPORTANT PROPERTIES
We will state the following two properties without proof.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete-Time Fourier Transform (DTFT)
61
1. Periodicity: The discrete-time Fourier transform X(ejω) is periodic
in ω with period 2π.
X(ejω) = X(ej[ω+2π])
Implication: We need only one period of X(ejω) (i.e., ω ∈[0, 2π], or
[−π, π], etc.) for analysis and not the whole domain −∞< ω < ∞.
2. Symmetry: For real-valued x(n), X(ejω) is conjugate symmetric.
X(e−jω) = X∗(ejω)
or
Re[X(e−jω)] = Re[X(ejω)]
(even symmetry)
Im[X(e−jω)] = −Im[X(ejω)]
(odd symmetry)
|X(e−jω)| = |X(ejω)|
(even symmetry)
̸ X(e−jω) = −̸ X(ejω)
(odd symmetry)
Implication: To plot X(ejω), we now need to consider only a half-
period of X(ejω). Generally, in practice this period is chosen to be
ω ∈[0, π].
3.1.2 MATLAB IMPLEMENTATION
If x(n) is of inﬁnite duration, then MATLAB cannot be used directly
to compute X(ejω) from x(n). However, we can use it to evaluate the
expression X(ejω) over [0, π] frequencies and then plot its magnitude and
angle (or real and imaginary parts).
□
EXAMPLE 3.3
Evaluate X(ejω) in Example 3.1 at 501 equispaced points between [0, π] and
plot its magnitude, angle, real part, and imaginary part.
Solution
MATLAB script:
>> w = [0:1:500]*pi/500;
% [0, pi]
axis divided into 501 points.
>> X = exp(j*w) ./ (exp(j*w) - 0.5*ones(1,501));
>> magX = abs(X); angX = angle(X);
realX = real(X); imagX = imag(X);
>> subplot(2,2,1); plot(w/pi,magX); grid
>> title(’Magnitude Part’); ylabel(’Magnitude’)
>> subplot(2,2,3); plot(w/pi,angX); grid
>> xlabel(’frequency in pi units’); title(’Angle Part’); ylabel(’Radians’)
>> subplot(2,2,2); plot(w/pi,realX); grid
>> title(’Real Part’); ylabel(’Real’)
>> subplot(2,2,4); plot(w/pi,imagX); grid
>> xlabel(’frequency in pi units’); title(’Imaginary Part’); ylabel(’Imaginary’)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

62
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
0
0.5
1
0.5
1
1.5
2
Magnitude Part
Magnitude
0
0.5
1
−0.8
−0.6
−0.4
−0.2
0
Frequency in π Units
Angle Part
Radians
0
0.5
1
0.5
1
1.5
2
Real Part
Real
0
0.5
1
−0.8
−0.6
−0.4
−0.2
0
Frequency in π Units
Imaginary Part
Imaginary
FIGURE 3.1
Plots in Example 3.3
The resulting plots are shown in Figure 3.1. Note that we divided the w array by
pi before plotting so that the frequency axes are in the units of π and therefore
easier to read. This practice is strongly recommended.
□
If x(n) is of ﬁnite duration, then MATLAB can be used to compute
X(ejω) numerically at any frequency ω. The approach is to implement
(3.1) directly. If, in addition, we evaluate X(ejω) at equispaced frequen-
cies between [0, π], then (3.1) can be implemented as a matrix-vector mul-
tiplication operation. To understand this, let us assume that the sequence
x(n) has N samples between n1 ≤n ≤nN (i.e., not necessarily between
[0, N −1]) and that we want to evaluate X(ejω) at
ωk
△= π
M k,
k = 0, 1, . . . , M
which are (M + 1) equispaced frequencies between [0, π]. Then (3.1) can
be written as
X(ejωk) =
N
	
ℓ=1
e−j(π/M)knℓx(nℓ),
k = 0, 1, . . . , M
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete-Time Fourier Transform (DTFT)
63
When {x (nℓ)} and {X(ejωk)} are arranged as column vectors x and X,
respectively, we have
X = Wx
(3.3)
where W is an (M + 1) × N matrix given by
W
△=

e−j(π/M)knℓ; n1 ≤n ≤nN,
k = 0, 1, . . . , M

In addition, if we arrange {k} and {nℓ} as row vectors k and n, respec-
tively, then
W =

exp

−j π
M kT n

In MATLAB, we represent sequences and indices as row vectors; therefore,
taking the transpose of (3.3), we obtain
XT = xT 
exp

−j π
M nT k

(3.4)
Note that nT k is an N × (M + 1) matrix. Now (3.4) can be implemented
in MATLAB as follows.
>> k = [0:M]; n = [n1:n2];
>> X = x * (exp(-j*pi/M)) .ˆ (n’*k);
□
EXAMPLE 3.4
Numerically compute the discrete-time Fourier transform of the sequence x(n)
given in Example 3.2 at 501 equispaced frequencies between [0, π].
Solution
MATLAB script:
>> n = -1:3; x = 1:5;
k = 0:500; w = (pi/500)*k;
>> X = x * (exp(-j*pi/500)) .ˆ (n’*k);
>> magX = abs(X); angX = angle(X);
>> realX = real(X); imagX = imag(X);
>> subplot(2,2,1); plot(k/500,magX);grid
>> title(’Magnitude Part’)
>> subplot(2,2,3); plot(k/500,angX/pi);grid
>> xlabel(’Frequency in \pi Units’); title(’Angle Part’)
>> subplot(2,2,2); plot(k/500,realX);grid
>> title(’Real Part’)
>> subplot(2,2,4); plot(k/500,imagX);grid
>> xlabel(’Frequency in \pi Units’); title(’Imaginary Part’)
The frequency-domain plots are shown in Figure 3.2. Note that the angle plot
is depicted as a discontinuous function between −π and π. This is because the
angle function in MATLAB computes the principal angle.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

64
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
0
0.5
1
0
5
10
15
Magnitude Part
Magnitude
0
0.5
1
−4
−2
0
2
4
Frequency in π Units
Angle Part
Radians
0
0.5
1
−5
0
5
10
15
Real Part
Real
0
0.5
1
−10
−5
0
5
Frequency in π Units
Imaginary Part
Imaginary
FIGURE 3.2
Plots in Example 3.4
The procedure of Example 3.4 can be compiled into a MATLAB
function—say, a dtft function—for ease of implementation. This is ex-
plored in Problem P3.1. This numerical computation is based on deﬁnition
(3.1). It is not the most elegant way of numerically computing the discrete-
time Fourier transform of a ﬁnite-duration sequence. In Chapter 5, we will
discuss in detail the topic of a computable transform called the discrete
Fourier transform (DFT) and its eﬃcient computation, called the fast
Fourier transform (FFT). Also, there is an alternate approach based on
the z-transform using the MATLAB function freqz, which we will dis-
cuss in Chapter 4. In this chapter, we will continue to use the approaches
discussed so far for calculation as well as for investigation purposes.
In the next two examples, we investigate the periodicity and symme-
try properties using complex-valued and real-valued sequences.
□
EXAMPLE 3.5
Let x(n) = (0.9 exp (jπ/3))n , 0 ≤n ≤10. Determine X(ejω) and investigate
its periodicity.
Solution
Since x(n) is complex-valued, X(ejω) satisﬁes only the periodicity property.
Therefore, it is uniquely deﬁned over one period of 2π. However, we will evaluate
and plot it at 401 frequencies over two periods between [−2π, 2π] to observe its
periodicity.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete-Time Fourier Transform (DTFT)
65
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
2
4
6
8
|X|
Magnitude Part
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
Frequency in π Units
Radians/π
Angle Part
FIGURE 3.3
Plots in Example 3.5
MATLAB script:
>> n = 0:10; x = (0.9*exp(j*pi/3)).ˆn;
>> k = -200:200; w = (pi/100)*k;
>> X = x * (exp(-j*pi/100)) .ˆ (n’*k);
>> magX = abs(X); angX =angle(X);
>> subplot(2,1,1); plot(w/pi,magX);grid
>> ylabel(’|X|’)
>> title(’Magnitude Part’)
>> subplot(2,1,2); plot(w/pi,angX/pi);grid
>> xlabel(’Frequency in \pi Units’); ylabel(’Radians/\pi’)
>> title(’Angle Part’)
From the plots in Figure 3.3, we observe that X(ejω) is periodic in ω but is not
conjugate-symmetric.
□
□
EXAMPLE 3.6
Let x(n) = (−0.9)n, −5 ≤n ≤5. Investigate the conjugate-symmetry property
of its discrete-time Fourier transform.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

66
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
5
10
15
Magnitude Part
|X|
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
Frequency in π Units
Radians/π
Angle Part
FIGURE 3.4
Plots in Example 3.6
Solution
Once again, we will compute and plot X(ejω) over two periods to study its
symmetry property.
MATLAB script:
>> n = -5:5; x = (-0.9).ˆn;
>> k = -200:200; w = (pi/100)*k;
X = x * (exp(-j*pi/100)) .ˆ (n’*k);
>> magX = abs(X); angX =angle(X);
>> subplot(2,1,1); plot(w/pi,magX);grid;
axis([-2,2,0,15])
>> ylabel(’|X|’)
>> title(’Magnitude Part’)
>> subplot(2,1,2); plot(w/pi,angX/pi);grid;
axis([-2,2,-1,1])
>> xlabel(’Frequency in \pi Units’); ylabel(’Radians/\pi’)
>> title(’Angle Part’)
From the plots in Figure 3.4, we observe that X(ejω) is not only periodic in ω
but is also conjugate-symmetric. Therefore, for real sequences we will plot their
Fourier transform magnitude and angle graphs from 0 to π.
□
3.1.3 SOME COMMON DTFT PAIRS
The discrete-time Fourier transforms of the basic sequences discussed in
Chapter 2 are very useful. The discrete-time Fourier transforms of some
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Properties of the DTFT
67
TABLE 3.1
Some common DTFT pairs
Signal Type
Sequence x(n) DTFT X

ejω
, −π ≤ω ≤π
Unit impulse
δ(n)
1
Constant
1
2πδ(ω)
Unit step
u(n)
1
1 −e−jω + πδ(ω)
Causal exponential
αnu(n)
1
1 −αe−jω
Complex exponential
ejω0n
2πδ(ω −ω0)
Cosine
cos(ω0n)
π[δ(ω −ω0) + δ(ω + ω0)]
Sine
sin(ω0n)
jπ[δ(ω + ω0) −δ(ω −ω0)]
Double exponential
α|n|
1 −α2
1 −2α cos(ω) + α2
Note: Since X

ejω
is periodic with period 2π, expressions over only
the primary period of −π ≤ω ≤π are given.
of these sequences can be easily obtained using the basic deﬁnitions (3.1)
and (3.2). These transform pairs and those of few other pairs are given
in Table 3.1. Note that, even if sequences like unit step u(n) are not
absolutely summable, their discrete-time Fourier transforms exist in the
limiting sense if we allow impulses in the Fourier transform. Such se-
quences are said to have ﬁnite power, that is, 
n |x(n)|2 < ∞. Using
this table and the properties of the Fourier transform (discussed in Sec-
tion 3.2), it is possible to obtain discrete-time Fourier transform of many
more sequences.
3.2 THE PROPERTIES OF THE DTFT
In the previous section, we discussed two important properties that
we needed for plotting purposes. We now discuss the remaining useful
properties, which are given below without proof. Let X(ejω) be the
discrete-time Fourier transform of x(n).
1. Linearity: The discrete-time Fourier transform is a linear transforma-
tion; that is,
F [αx1(n) + βx2(n)] = αF [x1(n)] + βF [x2(n)]
(3.5)
for every α, β, x1(n), and x2(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

68
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
2. Time shifting: A shift in the time domain corresponds to the phase
shifting.
F [x(n −k)] = X(ejω)e−jωk
(3.6)
3. Frequency shifting: Multiplication by a complex exponential corre-
sponds to a shift in the frequency domain.
F

x(n)ejω0n
= X(ej(ω−ω0))
(3.7)
4. Conjugation: Conjugation in the time domain corresponds to the
folding and conjugation in the frequency domain.
F [x∗(n)] = X∗(e−jω)
(3.8)
5. Folding: Folding in the time domain corresponds to the folding in the
frequency domain.
F [x(−n)] = X(e−jω)
(3.9)
6. Symmetries in real sequences: We have already studied the conju-
gate symmetry of real sequences. These real sequences can be decom-
posed into their even and odd parts, as discussed in Chapter 2.
x(n) = xe(n) + xo(n)
Then
F [xe(n)] = Re

X(ejω)

F [xo(n)] = j Im

X(ejω)

(3.10)
Implication: If the sequence x(n) is real and even, then X(ejω) is
also real and even. Hence only one plot over [0, π] is necessary for its
complete representation.
A similar property for complex-valued sequences is explored in
Problem P3.7.
7. Convolution: This is one of the most useful properties that makes
system analysis convenient in the frequency domain.
F [x1(n) ∗x2(n)] = F [x1(n)] F [x2(n)] = X1(ejω)X2(ejω)
(3.11)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Properties of the DTFT
69
8. Multiplication: This is a dual of the convolution property.
F [x1(n)·x2(n)] = F [x1(n)] ∗⃝F[x2(n)]
△= 1
2π
π

−π
X1(ejθ)X2(ej(ω−θ))dθ
(3.12)
This convolution-like operation is called a periodic convolution and
hence denoted by
∗⃝. It is discussed (in its discrete form) in
Chapter 5.
9. Energy: The energy of the sequence x(n) can be written as
Ex =
∞
	
−∞
|x(n)|2 = 1
2π
π

−π
|X(ejω)|2dω
(3.13)
=
π

0
|X(ejω)|2
π
dω
(for real sequences using even symmetry)
This is also known as Parseval’s theorem. From (3.13), the energy den-
sity spectrum of x(n) is deﬁned as
Φx(ω)
△= |X(ejω)|2
π
(3.14)
Then the energy of x(n) in the [ω1, ω2] band is given by
ω2

ω1
Φx(ω)dω,
0 ≤ω1 < ω2 ≤π
In the next several examples, we will verify some of these properties
using ﬁnite-duration sequences. We will follow our numerical procedure
to compute discrete-time Fourier transforms in each case. Although this
does not analytically prove the validity of each property, it provides us
with an experimental tool in practice.
□
EXAMPLE 3.7
In this example, we will verify the linearity property (3.5) using real-valued
ﬁnite-duration sequences. Let x1(n) and x2(n) be two random sequences uni-
formly distributed between [0, 1] over 0 ≤n ≤10. Then we can use our numer-
ical discrete-time Fourier transform procedure as follows.
MATLAB script:
>> x1 = rand(1,11); x2 = rand(1,11); n = 0:10;
>> alpha = 2; beta = 3;
k = 0:500; w = (pi/500)*k;
>> X1 = x1 * (exp(-j*pi/500)).ˆ(n’*k);
% DTFT of x1
>> X2 = x2 * (exp(-j*pi/500)).ˆ(n’*k);
% DTFT of x2
>> x = alpha*x1 + beta*x2;
% Linear combination of x1 & x2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

70
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
>> X = x * (exp(-j*pi/500)).ˆ(n’*k);
% DTFT of x
>> % Verification
>> X_check = alpha*X1 + beta*X2;
% Linear combination of X1 & X2
>> error = max(abs(X-X_check))
% Difference
error =
7.1054e-015
Since the maximum absolute error between the two Fourier transform arrays
is less than 10−14, the two arrays are identical within the limited numerical
precision of MATLAB.
□
□
EXAMPLE 3.8
Let x(n) be a random sequence uniformly distributed between [0, 1] over 0 ≤
n ≤10, and let y(n) = x(n −2). Then we can verify the sample shift property
(3.6) as follows.
>> x = rand(1,11); n = 0:10;
>> k = 0:500; w = (pi/500)*k;
>> X = x * (exp(-j*pi/500)).ˆ(n’*k);
% DTFT of x
>> % Signal shifted by two samples
>> y = x; m = n+2;
>> Y = y * (exp(-j*pi/500)).ˆ(m’*k);
% DTFT of y
>> % verification
>> Y_check = (exp(-j*2).ˆw).*X;
% Multiplication by exp(-j2w)
>> error = max(angle(Y-Y_check))
% Difference
error =
1.2204e-015
□
□
EXAMPLE 3.9
To verify the frequency shift property (3.7), we will use the graphical approach.
Let
x(n) = cos(πn/2),
0 ≤n ≤100
and
y(n) = ejπn/4x(n)
Then using MATLAB,
>> n = 0:100; x = cos(pi*n/2);
>> k = -100:100; w = (pi/100)*k;
% Frequency between -pi and +pi
>> X = x * (exp(-j*pi/100)).ˆ(n’*k);
% DTFT of x
>> y = exp(j*pi*n/4).*x;
% Signal multiplied by exp(j*pi*n/4)
>> Y = y * (exp(-j*pi/100)).ˆ(n’*k);
% DTFT of y
% Graphical verification
>> subplot(2,2,1); plot(w/pi,abs(X)); grid; axis([-1,1,0,60])
>> title(’Magnitude of X’); ylabel(’|X|’)
>> subplot(2,2,2); plot(w/pi,angle(X)/pi); grid; axis([-1,1,-1,1])
>> title(’Angle of X’); ylabel(’radians/\pi’)
>> subplot(2,2,3); plot(w/pi,abs(Y)); grid; axis([-1,1,0,60])
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Properties of the DTFT
71
−1
−0.5
0
0.5
1
0
20
40
60
Magnitude of X
|X|
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
Angle of X
Radians/π
−1
−0.5
0
0.5
1
0
20
40
60
Frequency in π Units
Magnitude of Y
|Y|
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
Frequency in π Units
Angle of Y
Radians/π
FIGURE 3.5
Plots in Example 3.9
>> xlabel(’Frequency in \pi Units’); ylabel(’|Y|’)
>> title(’Magnitude of Y’)
>> subplot(2,2,4); plot(w/pi,angle(Y)/pi); grid; axis([-1,1,-1,1])
>> xlabel(’Frequency in \pi Units’); ylabel(’radians/\pi’)
>> title(’Angle of Y’)
From the plots in Figure 3.5, we observe that X(ejω) is indeed shifted by π/4
in both magnitude and angle.
□
□
EXAMPLE 3.10
To verify the conjugation property (3.8), let x(n) be a complex-valued random
sequence over −5 ≤n ≤10 with real and imaginary parts uniformly distributed
between [0, 1]. The MATLAB veriﬁcation is as follows.
>> n = -5:10; x = rand(1,length(n)) + j*rand(1,length(n));
>> k = -100:100; w = (pi/100)*k;
% Frequency between -pi and +pi
>> X = x * (exp(-j*pi/100)).ˆ(n’*k);
% DTFT of x
% Conjugation property
>> y = conj(x);
% Signal conjugation
>> Y = y * (exp(-j*pi/100)).ˆ(n’*k);
% DTFT of y
% Verification
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

72
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
>> Y_check = conj(fliplr(X));
% conj(X(-w))
>> error = max(abs(Y-Y_check))
% Difference
error =
0
□
□
EXAMPLE 3.11
To verify the folding property (3.9), let x(n) be a random sequence over −5 ≤
n ≤10 uniformly distributed between [0, 1]. The MATLAB veriﬁcation is as
follows.
>> n = -5:10; x = rand(1,length(n));
>> k = -100:100; w = (pi/100)*k;
% Frequency between -pi and +pi
>> X = x * (exp(-j*pi/100)).ˆ(n’*k);
% DTFT of x
% Folding property
>> y = fliplr(x); m = -fliplr(n);
% Signal folding
>> Y = y * (exp(-j*pi/100)).ˆ(m’*k);
% DTFT of y
% Verification
>> Y_check = fliplr(X);
% X(-w)
>> error = max(abs(Y-Y_check))
% Difference
error =
0
□
□
EXAMPLE 3.12
In this problem, we verify the symmetry property (3.10) of real signals. Let
x(n) = sin(πn/2),
−5 ≤n ≤10
Then, using the evenodd function developed in Chapter 2, we can compute
the even and odd parts of x(n) and then evaluate their discrete-time Fourier
transforms. We will provide the numerical as well as graphical veriﬁcation.
MATLAB script:
>> n = -5:10; x = sin(pi*n/2);
>> k = -100:100; w = (pi/100)*k;
% Frequency between -pi and +pi
>> X = x * (exp(-j*pi/100)).ˆ(n’*k);
% DTFT of x
% Signal decomposition
>> [xe,xo,m] = evenodd(x,n);
% Even and odd parts
>> XE = xe * (exp(-j*pi/100)).ˆ(m’*k);
% DTFT of xe
>> XO = xo * (exp(-j*pi/100)).ˆ(m’*k);
% DTFT of xo
% Verification
>> XR = real(X);
% Real part of X
>> error1 = max(abs(XE-XR))
% Difference
error1 =
1.8974e-019
>> XI = imag(X);
% Imag part of X
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Properties of the DTFT
73
−1
−0.5
0
0.5
1
−2
−1
0
1
2
Real Part of X
Re(X)
−1
−0.5
0
0.5
1
−10
−5
0
5
10
Imaginary Part of X
Im(X)
−1
−0.5
0
0.5
1
−2
−1
0
1
2
Frequency in π Units 
Frequency in π Units
Transform of Even Part
XE
−1
−0.5
0
0.5
1
−10
−5
0
5
10
Transform of Odd Part
XO
FIGURE 3.6
Plots in Example 3.12
>> error2 = max(abs(XO-j*XI))
% Difference
error2 =
1.8033e-019
% Graphical verification
>> subplot(2,2,1); plot(w/pi,XR); grid; axis([-1,1,-2,2])
>> title(’Real Part of X’); ylabel(’Re(X)’);
>> subplot(2,2,2); plot(w/pi,XI); grid; axis([-1,1,-10,10])
>> title(’Imaginary Part of X’); ylabel(’Im(X)’);
>> subplot(2,2,3); plot(w/pi,real(XE)); grid; axis([-1,1,-2,2])
>> xlabel(’Frequency in \pi Units’); ylabel(’XE’);
>> title(’Transform of Even Part’)
>> subplot(2,2,4); plot(w/pi,imag(XO)); grid; axis([-1,1,-10,10])
>> xlabel(’Frequency in \pi Units’); ylabel(’XO’);
>> title(’Transform of Odd Part’)
From the plots in Figure 3.6, we observe that the real part of X(ejω) [or the
imaginary part of X(ejω)] is equal to the discrete-time Fourier transform of
xe(n) [or xo(n)].
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

74
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
3.3 THE FREQUENCY DOMAIN REPRESENTATION
OF LTI SYSTEMS
We earlier stated that the Fourier transform representation is the most
useful signal representation for LTI systems. This is due to the following
result.
3.3.1 RESPONSE TO A COMPLEX EXPONENTIAL e jω0n
Let x(n) = ejω0n be the input to an LTI system represented by the impulse
response h(n).
ejω0n −→h(n) −→h(n) ∗ejω0n
Then
y(n) = h(n) ∗ejω0n =
∞
	
−∞
h(k)ejω0(n−k)
=
 ∞
	
−∞
h(k)e−jω0k

ejω0n
(3.15)
= [F[h(n)]|ω=ω0] ejω0n
DEFINITION 1
Frequency Response
The discrete-time Fourier transform of an impulse response is called
the frequency response (or transfer function) of an LTI system and
is denoted by
H(ejω)
△=
∞
	
n=−∞
h(n)e−jωn
(3.16)
Then from (3.15) we can represent the system by
x(n) = ejω0n −→H(ejω) −→y(n) = H(ejω0) × ejω0n
(3.17)
Hence the output sequence is the input exponential sequence modified by
the response of the system at frequency ω0. This justiﬁes the deﬁnition
of H(ejω) as a frequency response because it is what the complex expo-
nential is multiplied by to obtain the output y(n). This powerful result
can be extended to a linear combination of complex exponentials using
the linearity of LTI systems:
	
k
Akejωkn −→h(n) −→
	
k
AkH(ejωk) ejωkn
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Frequency Domain Representation of LTI Systems
75
In general, the frequency response H(ejω) is a complex function of
ω. The magnitude |H(ejω)| of H(ejω) is called the magnitude (or gain)
response function, and the angle ̸ H(ejω) is called the phase response
function, as we shall see below.
3.3.2 RESPONSE TO SINUSOIDAL SEQUENCES
Let x(n) = A cos(ω0n + θ0) be an input to an LTI system h(n). Then
from (3.17) we can show that the response y(n) is another sinusoid of the
same frequency ω0, with amplitude gained by |H(ejω0)| and phase shifted
by ̸ H(ejω0), that is,
y(n) = A|H(ejω0)| cos(ω0n + θ0 + ̸ H(ejω0))
(3.18)
This response is called the steady-state response, denoted by yss(n). It can
be extended to a linear combination of sinusoidal sequences.
	
k
Ak cos(ωkn + θk) −→H(ejω) −→
	
k
Ak|H(ejωk)| cos(ωkn + θk + ̸ H(ejωk))
3.3.3 RESPONSE TO ARBITRARY SEQUENCES
Finally, (3.17) can be generalized to arbitrary absolutely summable se-
quences. Let X(ejω) = F[x(n)], and let Y (ejω) = F[y(n)]; then, using
the convolution property (3.11), we have
Y (ejω) = H(ejω) X(ejω)
(3.19)
Therefore, an LTI system can be represented in the frequency domain by
X(ejω) −→H(ejω) −→Y (ejω) = H(ejω) X(ejω)
The output y(n) is then computed from Y (ejω) using the inverse
discrete-time Fourier transform (3.2). This requires an integral operation,
which is not a convenient operation in MATLAB. As we shall see in
Chapter 4, there is an alternate approach to the computation of output to
arbitrary inputs using the z-transform and partial fraction expansion. In
this chapter, we will concentrate on computing the steady-state response.
□
EXAMPLE 3.13
Determine the frequency response H(ejω) of a system characterized by h(n) =
(0.9)nu(n). Plot the magnitude and the phase responses.
Solution
Using (3.16),
H(ejω) =
∞
	
−∞
h(n)e−jωn =
∞
	
0
(0.9)ne−jωn
=
∞
	
0
(0.9e−jω)n =
1
1 −0.9e−jω
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

76
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
Hence
|H(ejω)| =

1
(1 −0.9 cos ω)2 + (0.9 sin ω)2 =
1
√1.81 −1.8 cos ω
and
̸ H(ejω) = −arctan

0.9 sin ω
1 −0.9 cos ω

To plot these responses, we can either implement the |H(ejω)| and ̸ H(ejω)
functions or the frequency response H(ejω) and then compute its magnitude
and phase. The latter approach is more useful from a practical viewpoint
[as shown in (3.18)].
>> w = [0:1:500]*pi/500;
% [0, pi] axis divided into 501 points.
>> H = exp(j*w) ./ (exp(j*w) - 0.9*ones(1,501));
>> magH = abs(H); angH = angle(H);
>> subplot(2,1,1); plot(w/pi,magH); grid;
>> title(’Magnitude Response’); ylabel(’|H|’);
>> subplot(2,1,2); plot(w/pi,angH/pi); grid
>> xlabel(’Frequency in \pi Units’); ylabel(’Phase in \pi Radians’);
>> title(’Phase Response’);
The plots are shown in Figure 3.7.
□
□
EXAMPLE 3.14
Let an input to the system in Example 3.13 be 0.1u(n). Determine the steady-
state response yss(n).
Solution
Since the input is not absolutely summable, the discrete-time Fourier transform
is not particularly useful in computing the complete response. However, it can
be used to compute the steady-state response. In the steady state (i.e., n →∞),
the input is a constant sequence (or a sinusoid with ω0 = θ0 = 0). Then the
output is
yss(n) = 0.1 × H(ej0) = 0.1 × 10 = 1
where the gain of the system at ω = 0 (also called the DC gain) is H(ej0) = 10,
which is obtained from Figure 3.7.
□
3.3.4 FREQUENCY RESPONSE FUNCTION FROM DIFFERENCE
EQUATIONS
When an LTI system is represented by the diﬀerence equation
y(n) +
N
	
ℓ=1
aℓy(n −ℓ) =
M
	
m=0
bmx(n −m)
(3.20)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Frequency Domain Representation of LTI Systems
77
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
Magnitude Response
|H|
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Frequency in π Units
−0.4
−0.3
−0.2
−0.1
0
Phase in π Radians
Phase Response
FIGURE 3.7
Frequency response plots in Example 3.13
then to evaluate its frequency response from (3.16), we would need the im-
pulse response h(n). However, using (3.17), we can easily obtain H(ejω).
We know that when x(n) = ejωn, then y(n) must be H(ejω)ejωn. Substi-
tuting in (3.20), we have
H(ejω)ejωn +
N
	
ℓ=1
aℓH(ejω)ejω(n−ℓ) =
M
	
m=0
bm ejω(n−m)
or
H(ejω) =
M
m=0 bm e−jωm
1 + N
ℓ=1 aℓe−jωℓ
(3.21)
after canceling the common factor ejωn term and rearranging. This equa-
tion can easily be implemented in MATLAB, given the diﬀerence equation
parameters.
□
EXAMPLE 3.15
An LTI system is speciﬁed by the diﬀerence equation
y(n) = 0.8y(n −1) + x(n)
a. Determine H(ejω).
b. Calculate and plot the steady-state response yss(n) to
x(n) = cos(0.05πn)u(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

78
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
Solution
Rewrite the diﬀerence equation as y(n) −0.8y(n −1) = x(n).
a. Using (3.21), we obtain
H(ejω) =
1
1 −0.8e−jω
(3.22)
b. In the steady state, the input is x(n) = cos(0.05πn) with frequency ω0 =
0.05π and θ0 = 0◦. The response of the system is
H(ej0.05π) =
1
1 −0.8e−j0.05π = 4.0928e−j0.5377
Therefore,
yss(n) = 4.0928 cos(0.05πn −0.5377) = 4.0928 cos [0.05π(n −3.42)]
This means that at the output the sinusoid is scaled by 4.0928 and shifted
by 3.42 sampling intervals. This can be veriﬁed using MATLAB.
>> subplot(1,1,1)
>> b = 1; a = [1,-0.8];
>> n=[0:100];x = cos(0.05*pi*n);
>> y = filter(b,a,x);
>> subplot(2,1,1); stem(n,x);
>> ylabel(’x(n)’); title(’Input Sequence’)
>> subplot(2,1,2); stem(n,y);
>> xlabel(’n’); ylabel(’y(n)’); title(’Output Sequence’)
From the plots in Figure 3.8, we note that the amplitude of yss(n) is approx-
imately 4. To determine the shift in the output sinusoid, we can compare
zero crossings of the input and the output. This is shown in Figure 3.8, from
which the shift is approximately 3.4 samples.
□
In Example 3.15, the system was characterized by a ﬁrst-order dif-
ference equation. It is fairly straightforward to implement (3.22) in
MATLAB, as we did in Example 3.13. In practice, the diﬀerence equa-
tions are of large order, and hence we need a compact procedure to
implement the general expression (3.21). This can be done using a simple
matrix-vector multiplication. If we evaluate H(ejω) at k = 0, 1, . . . , K
equispaced frequencies over [0, π], then
H(ejωk) =
M
m=0 bm e−jωkm
1 + N
ℓ=1 aℓe−jωkℓ,
k = 0, 1, . . . , K
(3.23)
If we let {bm}, {aℓ} (with a0 = 1), {m = 0, . . . , M}, {ℓ= 0, . . . , N}, and
{ωk} be arrays (or row vectors), then the numerator and the denominator
of (3.23) become
b exp(−jmT ω);
a exp(−jℓT ω)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Frequency Domain Representation of LTI Systems
79
0
10
20
30
40
50
60
70
80
90
4.092
100
−5
−1
0
1
5
x(n)
Input Sequence
0
10
20
30
40
50
60
70
80
90
100
−5
0
5
n
y(n)
Output Sequence
3.42
FIGURE 3.8
Plots in Example 3.15
respectively. Now the array H(ejωk) in (3.23) can be computed using a ./
operation. This procedure can be implemented in a MATLAB function to
determine the frequency response function, given {bm} and {aℓ} arrays.
We will explore this in Example 3.16 and in Problem P3.16.
□
EXAMPLE 3.16
A 3rd-order lowpass ﬁlter is described by the diﬀerence equation
y(n) = 0.0181x(n) + 0.0543x(n −1) + 0.0543x(n −2) + 0.0181x(n −3)
+ 1.76y(n −1) −1.1829y(n −2) + 0.2781y(n −3)
Plot the magnitude and the phase response of this ﬁlter, and verify that it is a
lowpass ﬁlter.
Solution
We will implement this procedure in MATLAB and then plot the ﬁlter
responses.
>> b = [0.0181,
0.0543, 0.0543,
0.0181]; % Filter coefficient array b
>> a = [1.0000, -1.7600, 1.1829, -0.2781]; % Filter coefficient array a
>> m = 0:length(b)-1; l = 0:length(a)-1;
% Index arrays m and l
>> K = 500; k = 0:1:K;
% Index array k for frequencies
>> w = pi*k/K;
% [0, pi] axis divided into 501 points.
>> num = b * exp(-j*m’*w);
% Numerator calculations
>> den = a * exp(-j*l’*w);
% Denominator calculations
>> H = num ./ den;
% Frequency response
>> magH = abs(H); angH = angle(H);
% Mag and phase responses
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

80
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
Magnitude Response
Magnitude |H|
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−1
−0.5
0
0.5
1
Frequency in π Units
Phase in π Radians
Phase Response
FIGURE 3.9
Plots for Example 3.16
>> subplot(2,1,1); plot(w/pi,magH); grid; axis([0,1,0,1])
>> ylabel(’Magnitude |H|’);
>> title(’Magnitude Response’);
>> subplot(2,1,2); plot(w/pi,angH/pi); grid
>> xlabel(’Frequency in \pi Units’); ylabel(’Phase in \pi Radians’);
>> title(’Phase Response’);
From the plots in Figure 3.9, we see that the ﬁlter is indeed a lowpass ﬁlter. □
3.4 SAMPLING AND RECONSTRUCTION OF ANALOG SIGNALS
In many applications—for example, in digital communications—real-
world analog signals are converted into discrete signals using sampling
and quantization operations (collectively called analog-to-digital con-
version, or ADC). These discrete signals are processed by digital signal
processors, and the processed signals are converted into analog signals
using a reconstruction operation (called digital-to-analog conversion, or
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
81
DAC). Using Fourier analysis, we can describe the sampling operation
from the frequency-domain viewpoint, analyze its eﬀects, and then ad-
dress the reconstruction operation. We will also assume that the number
of quantization levels is suﬃciently large that the eﬀect of quantization
on discrete signals is negligible. We will study the eﬀects of quantization
in Chapter 10.
3.4.1 SAMPLING
Let xa(t) be an analog (absolutely integrable) signal. Its continuous-time
Fourier transform (CTFT) is given by
Xa(jΩ)
△=
∞

−∞
xa(t)e−jΩtdt
(3.24)
where Ωis an analog frequency in radians/sec. The inverse continuous-
time Fourier transform is given by
xa(t) = 1
2π
∞

−∞
Xa(jΩ)ejΩtdΩ
(3.25)
We now sample xa(t) at sampling interval Ts seconds apart to obtain the
discrete-time signal x(n):
x(n)
△= xa( nTs)
Let X(ejω) be the discrete-time Fourier transform of x(n). Then it can be
shown [79] that X(ejω) is a countable sum of amplitude-scaled, frequency-
scaled, and translated versions of the Fourier transform Xa(jΩ).
X(ejω) = 1
Ts
∞
	
ℓ=−∞
Xa

j
 ω
Ts
−2π
Ts
ℓ

(3.26)
This relation is known as the aliasing formula. The analog and digital
frequencies are related through Ts,
ω = ΩTs
(3.27)
while the sampling frequency Fs is given by
Fs
△= 1
Ts
,
sam/sec
(3.28)
The graphical illustration of (3.26) is shown in Figure 3.10, from which
we observe that, in general, the discrete signal is an aliased version of the
corresponding analog signal because higher frequencies are aliased into
lower frequencies if there is an overlap. However, it is possible to recover
the Fourier transform Xa(jΩ) from X(ejω) [or equivalently, the analog
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

82
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
A /Ts
X (e
A /Ts
–
–Ω0/Ts
0/Ts
0
x (n)
n
0
–5
–2
–1
2
1
5
DTFT
eq. (3.27)
–2π
π
2π
Xa(jΩ) 
xa(t)
1
1
A
0
x (n)
n
0
t
0
CTFT
Sample
– Ω0
Ω0
–π
0
π
2π
–π
Ts  <  π/Ω0
Ts   >  π/Ω0
–2π
)
X (e
)
Ω
0/Ts
Ω
0/Ts
Ω
j
jw
w
w
w
Ω
FIGURE 3.10
Sampling operation in the time and frequency domains
signal xa(t) from its samples x(n)] if the inﬁnite “replicas” of Xa(jΩ) do
not overlap with each other to form X(ejω). This is true for band-limited
analog signals.
DEFINITION 2
Band-Limited Signal
A signal is band-limited if there exists a finite radian frequency Ω0
such that Xa(jΩ) is zero for |Ω| > Ω0. The frequency F0 = Ω0/2π is
called the signal bandwidth in Hz.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
83
Referring to Figure 3.10, if π > Ω0Ts—or equivalently, Fs/2 > F0—
then
X(ejω) = 1
Ts
X

j ω
Ts

;
−π
Ts
< ω
Ts
≤π
Ts
(3.29)
which leads to the sampling theorem for band-limited signals.
THEOREM 3
Sampling Principle
A band-limited signal xa(t) with bandwidth F0 can be reconstructed
from its sample values x(n) = xa(nTs) if the sampling frequency Fs =
1/Ts is greater than twice the bandwidth F0 of xa(t).
Fs > 2F0
Otherwise, aliasing would result in x(n). The sampling rate of 2F0 for
an analog band-limited signal is called the Nyquist rate.
Note: After xa(t) is sampled, the highest analog frequency that x(n) rep-
resents is Fs/2 Hz (or ω = π). This agrees with the implication stated in
property 2 of the discrete-time Fourier transform in Section 3.1. Before
we delve into MATLAB implementation of sampling, we ﬁrst consider
sampling of sinusoidal signals and the resulting Fourier transform in the
following example.
□
EXAMPLE 3.17
The analog signal xa(t) = 4 + 2 cos(150πt + π/3) + 4 sin(350πt) is sampled at
Fs = 200 sam/sec to obtain the discrete-time signal x(n). Determine x(n) and
its corresponding DTFT X(ejω).
Solution
The highest frequency in the given xa(t) is F0 = 175 Hz. Since Fs = 200, which
is less than 2F0, there will be aliasing in x(n) after sampling. The sampling
interval is Ts = 1/Fs = 0.005 sec. Hence we have
x(n) = xa(nTs) = xa(0.005n)
= 4 + 2 cos

0.75πn + π
3

+ 4 sin(1.75πn)
(3.30)
Note that the digital frequency, 1.75π, of the third term in (3.30) is outside the
primary interval of −π ≤ω ≤π, signifying that aliasing has occurred. From
the periodicity property of digital sinusoidal sequences in Chapter 2, we know
that the period of the digital sinusoid is 2π. Hence we can determine the alias
of the frequency 1.75π. From (3.30), we have
x(n) = 4 + 2 cos(0.75πn + π
3 ) + 4 sin(1.75πn −2πn)
= 4 + 2 cos(0.75πn + π
3 ) −4 sin(0.25πn)
(3.31)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

84
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
Using Euler’s identity, we can expess x(n) as
x(n) = 4 + ejπ/3ej0.75πn + e−jπ/3e−j0.75πn + 2jej0.25πn −2jej0.25πn
(3.32)
From Table 3.1 and the DTFT properties, the DTFT of x(n) is given by
X(ejω) = 8πδ(ω) + 2πejπ/3δ(ω −0.75π) + 2πe−jπ/3δ(ω + 0.75π)
+ j4πδ(ω −0.25π) −j4πδ(ω + 0.25π), −π ≤ω ≤π
(3.33)
The plot of X(ejω) is shown in Figure 3.15.
□
3.4.2 MATLAB IMPLEMENTATION
In a strict sense, it is not possible to analyze analog signals using MAT-
LAB unless we use the Symbolic toolbox. However, if we sample xa(t) on
a ﬁne grid that has a suﬃciently small time increment to yield a smooth
plot and a large enough maximum time to show all the modes, then we can
approximate its analysis. Let ∆t be the grid interval such that ∆t ≪Ts.
Then
xG(m)
△= xa(m∆t)
(3.34)
can be used as an array to simulate an analog signal. The sampling in-
terval Ts should not be confused with the grid interval ∆t, which is used
strictly to represent an analog signal in MATLAB. Similarly, the Fourier
transform relation (3.24) should also be approximated in light of (3.34)
as follows:
Xa(jΩ) ≈
	
m
xG(m)e−jΩm∆t∆t = ∆t
	
m
xG(m)e−jΩm∆t
(3.35)
Now if xa(t) [and hence xG(m)] is of ﬁnite duration, then (3.35) is similar
to the discrete-time Fourier transform relation (3.3) and hence can be
implemented in MATLAB in a similar fashion to analyze the sampling
phenomenon.
□
EXAMPLE 3.18
Let xa(t) = e−1000|t|. Determine and plot its Fourier transform.
Solution
From (3.24),
Xa(jΩ) =
∞

−∞
xa(t)e−jΩtdt =
0
−∞
e1000te−jΩtdt +
∞

0
e−1000te−jΩtdt
=
0.002
1 + (
Ω
1000)2
(3.36)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
85
which is a real-valued function since xa(t) is a real and even signal. To evaluate
Xa(jΩ) numerically, we have to ﬁrst approximate xa(t) by a ﬁnite-duration
grid sequence xG(m). Using the approximation e−5 ≈0, we note that xa(t)
can be approximated by a ﬁnite-duration signal over −0.005 ≤t ≤0.005 (or
equivalently, over [−5, 5] msec). Similarly from (3.36), Xa(jΩ) ≈0 for Ω≥
2π (2000). Hence choosing
∆t = 5 × 10−5 ≪
1
2 (2000) = 25 × 10−5
we can obtain xG(m) and then implement (3.35) in MATLAB.
% Analog signal
>> Dt = 0.00005; t = -0.005:Dt:0.005; xa = exp(-1000*abs(t));
% Continuous-time Fourier transform
>>Wmax = 2*pi*2000; K = 500; k = 0:1:K; W = k*Wmax/K;
>>Xa = xa * exp(-j*t’*W) * Dt; Xa = real(Xa);
>>W = [-fliplr(W), W(2:501)]; % Omega from -Wmax to Wmax
>>Xa = [fliplr(Xa), Xa(2:501)]; % Xa over -Wmax to Wmax interval
>>subplot(2,1,1);plot(t*1000,xa);
>>xlabel(’t in msec’); ylabel(’Amplitude’)
>>title(’Analog Signal’)
>>subplot(2,1,2);plot(W/(2*pi*1000),Xa*1000);
>>xlabel(’Frequency in KHz’); ylabel(’Amplitude/1000’)
>>title(’Continuous-Time Fourier Transform’)
Figure 3.11 shows the plots of xa(t) and Xa(jΩ). Note that to reduce the number
of computations, we computed Xa(jΩ) over [0, 4000π] rad/sec (or equivalently,
over [0, 2] KHz) and then duplicated it over [−4000π, 0] for plotting purposes.
The displayed plot of Xa(jΩ) agrees with (3.36).
□
□
EXAMPLE 3.19
To study the eﬀect of sampling on the frequency-domain quantities, we will
sample xa(t) in Example 3.18 at two diﬀerent sampling frequencies.
a. Sample xa(t) at Fs = 5000 sam/sec to obtain x1(n). Determine and plot
X1(ejω).
b. Sample xa(t) at Fs = 1000 sam/sec to obtain x2(n). Determine and plot
X2(ejω).
Solution
a. Since the bandwidth of xa(t) is 2KHz, the Nyquist rate is 4000 sam/sec,
which is less than the given Fs. Therefore, aliasing will be (almost) nonexis-
tent.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

86
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
−6
−4
−2
0
2
4
6
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Analog Signal
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0
0.5
1
1.5
2
Frequency in KHz
Amplitude/1000
Continuous−Time Fourier Transform
FIGURE 3.11
Plots in Example 3.18
MATLAB script:
% Analog signal
>> Dt = 0.00005; t = -0.005:Dt:0.005; xa = exp(-1000*abs(t));
% Discrete-time signal
>> Ts = 0.0002; n = -25:1:25; x = exp(-1000*abs(n*Ts));
% Discrete-time Fourier transform
>> K = 500; k = 0:1:K; w = pi*k/K;
>> X = x * exp(-j*n’*w); X = real(X);
>> w = [-fliplr(w), w(2:K+1)];
X = [fliplr(X), X(2:K+1)];
>> subplot(2,1,1);plot(t*1000,xa);
>> xlabel(’t in msec’); ylabel(’Amplitude’)
>> title(’Discrete Signal’); hold on
>> stem(n*Ts*1000,x); gtext(’Ts=0.2 msec’); hold off
>> subplot(2,1,2);plot(w/pi,X);
>> xlabel(’Frequency in \pi Units’); ylabel(’Amplitude’)
>> title(’Discrete-Time Fourier Transform’)
In the top plot in Figure 3.12, we have superimposed the discrete signal x1(n)
over xa(t) to emphasize the sampling. The plot of X2(ejω) shows that it is a
scaled version (scaled by Fs = 5000) of Xa(jΩ). Clearly there is no aliasing.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
87
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Discrete Signal
Ts=0.2 msec
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0
2
4
6
8
10
Frequency in π Units
Amplitude
Discrete−Time Fourier Transform
FIGURE 3.12
Plots in Example 3.19a
b. Here Fs = 1000 < 4000. Hence there will be a considerable amount of alias-
ing. This is evident from Figure 3.13, in which the shape of X(ejω) is diﬀerent
from that of Xa(jΩ) and can be seen to be a result of adding overlapping
replicas of Xa(jΩ).
□
3.4.3 RECONSTRUCTION
From the sampling theorem and the preceding examples, it is clear that if
we sample band-limited xa(t) above its Nyquist rate, then we can recon-
struct xa(t) from its samples x(n). This reconstruction can be thought of
as a two-step process:
• First, the samples are converted into a weighted impulse train:
∞
	
n=−∞
x(n)δ(t−nTs) = · · ·+x(−1)δ(n+Ts)+x(0)δ(t)+x(1)δ(n−Ts)+· · ·
• Then, the impulse train is ﬁltered through an ideal analog lowpass ﬁlter
band-limited to the [−Fs/2, Fs/2] band:
x(n) −→
Impulse train
conversion
−→
Ideal lowpass
ﬁlter
−→xa(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

88
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Discrete Signal
Ts = 1 msec
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
Discrete−Time Fourier Transform
Amplitude
Frequency in π Units
FIGURE 3.13
Plots in Example 3.19b
This two-step procedure can be described mathematically using an inter-
polating formula [79],
xa(t) =
∞
	
n=−∞
x(n) sinc [Fs(t −nTs)]
(3.37)
where sinc(x) =
sin πx
πx
is an interpolating function. The physical inter-
pretation of the above reconstruction (3.37) is given in Figure 3.14, from
which we observe that this ideal interpolation is not practically feasible,
because the entire system is noncausal and hence not realizable.
□
EXAMPLE 3.20
Consider the sampled signal x(n) from Example 3.17. It is applied as an
input to an ideal D/A converter (i.e., an ideal interpolator) to obtain the
analog signal ya(t). The ideal D/A converter is also operating at Fs = 200
sam/sec. Obtain the reconstructed signal ya(t), and determine whether the
sampling/reconstruction operation resulted in any aliasing. Also, plot the
Fourier transforms Xa(jΩ), X(ejω), and Ya(jΩ).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
89
t
0
t
0
+...=
+...
+
x(n)
x(1)
x(0)
x(–1)
x(2) x(3)
xa(t)
n
0
1
–1
3
–Ts
Ts
2Ts
3Ts
Ts
2Ts
3Ts
2
xa (t) = Σx(n) sinc[Fs(t – nTs)]
t
0
–Ts
Ts
2Ts
3Ts
Sample at
t = nTs
Sampling
Reconstruction
t
t
t
+
+
x(3) sinc[Fs(t – 3Ts)]
x(2) sinc[Fs(t – 2Ts)]
x(1) sinc[Fs(t –Ts)]
x(0) sinc[Fst]
FIGURE 3.14
Reconstruction of band-limited signal from its samples
Solution
We can determine ya(t) using (3.31). However, since all frequencies in the sinu-
soidal sequence x(n) are between the primary period of −π ≤ω ≤π, we can
equivalently obtain ya(t) by substituting n by tFs. Thus from (3.31), we have
ya(t) = x(n)

n=tF s = x(n)

n=200t
= 4 + 2 cos

0.75π200t + π
3

−4 sin(0.25π200t)
= 4 + 2 cos

150πt + π
3

−4 sin(50πt)
(3.38)
As expected, the 175 Hz component in xa(t) is aliased into the 25 Hz component
in ya(t).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

90
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
Using Euler’s identity on the given xa(t) and the CTFT properties, the
CTFT Xa(jΩ) is given by
Xa(jΩ) = 8πδ(Ω) + 2πejπ/3δ(Ω−150π) + 2πe−jπ/3δ(Ω+ 150π)
+ 4jπδ(Ω−350π) −4jπδ(Ω+ 350π)
(3.39)
It is informative to plot the CTFT Xa(jΩ) as a function of the cyclic frequency
F in Hz using Ω= 2πF. Thus the quantity Xa(j2πF) from (3.39) is given by
Xa(j2πF) = 4δ(F) + ejπ/3δ(F −75) + e−jπ/3δ(F + 75)
+ 2jδ(F −175) −2jδ(F + 175)
(3.40)
where we have used the identity δ(Ω) = δ(2πF) =
1
2π δ(F). Similarly, the CTFT
Ya(j2πF) is given by
Ya(j2πF) = 4δ(F) + ejπ/3δ(F −75) + e−jπ/3δ(F + 75)
+ 2jδ(F −25) −2jδ(F + 25)
(3.41)
Figure 3.15a shows the CTFT of the original signal xa(t) as a function of
F. The DTFT X
ejω
of the sampled sequence x(n) is shown as a function of
ω in Figure 3.15b, in which the impulses due to shifted replicas are shown in
gray shade for clarity. The ideal D/A converter response is also shown in gray
shade. The CTFT of the reconstructed signal ya(t) is shown in Figure 3.15c,
which clearly shows the aliasing eﬀect.
□
Practical D/A converters
In practice, we need a diﬀerent approach
than (3.37). The two-step procedure is still feasible, but now we replace
the ideal lowpass ﬁlter by a practical analog lowpass ﬁlter. Another in-
terpretation of (3.37) is that it is an inﬁnite-order interpolation. We want
ﬁnite-order (and in fact low-order) interpolations. There are several ap-
proaches to do this.
• Zero-order-hold (ZOH) interpolation: In this interpolation, a
given sample value is held for the sample interval until the next sample
is received,
ˆxa(t) = x(n),
nTs ≤n < (n + 1)Ts
which can be obtained by ﬁltering the impulse train through an inter-
polating ﬁlter of the form
h0(t) =

1,
0 ≤t ≤Ts
0,
otherwise
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
91
F, Hz
F, Hz
Aliased component
Aliased component
Ideal D/A converter response
(a)
(b)
(c)
–75
–200 –175
–100 –75
0
75
Fs
Fs
100
175
2
200
–25
0
25
75
–π
–2π
2π
–0.75π
–0.25π
0.25π
0.75π
0
π
Ya (j 2πF )
Xa ( j 2πF )
X ( e j   )
rad
sam
e–jp /3
8p
2e–jp/2
4πe jp/2
2πe jp/ 3
2πe –jp/3
4πe –jp/2
2πe –jp/3
4πe –jp/2
2πe jp/3
4πe jp/2
2e jp/2
e jp/3
2e–jp/2
e–jp/3
2e jp/2
e jp/3
4
4
8π
8π
w
w,
FIGURE 3.15
Fourier transforms of the sinusoidal signals xa(t), x(n), and ya(t)
which is a rectangular pulse. The resulting signal is a piecewise-constant
(staircase) waveform that requires an appropriately designed analog
postﬁlter for accurate waveform reconstruction.
x(n) −→ZOH −→ˆxa(t) −→Postﬁlter −→xa(t)
• First-order-hold (FOH) interpolation: In this case, the adjacent
samples are joined by straight lines. This can be obtained by ﬁltering
the impulse train through
h1(t) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
1 + t
Ts
,
0 ≤t ≤Ts
1 −t
Ts
,
Ts ≤t ≤2Ts
0,
otherwise
Once again, an appropriately designed analog postﬁlter is required for
accurate reconstruction. These interpolations can be extended to higher
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

92
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
orders. One particularly useful interpolation employed by MATLAB is
the following.
• Cubic spline interpolation: This approach uses spline interpolants
for a smoother, but not necessarily more accurate, estimate of the ana-
log signals between samples. Hence this interpolation does not require
an analog postﬁlter. The smoother reconstruction is obtained by us-
ing a set of piecewise continuous third-order polynomials called cubic
splines, given by
xa (t) = α0(n) + α1(n) (t −nTs) + α2(n) (t −nTs)2
+ α3(n) (t −nTs)3 ,
nTs ≤n < (n + 1)Ts
(3.42)
where {αi(n), 0 ≤i ≤3} are the polynomial coeﬃcients, which are de-
termined by using least-squares analysis on the sample values [10].
(Strictly speaking, this is not a causal operation but is a convenient
one in MATLAB.)
3.4.4 MATLAB IMPLEMENTATION
For interpolation between samples, MATLAB provides several approaches.
The function sinc(x), which generates the (sin πx) /πx function, can
be used to implement (3.37), given a ﬁnite number of samples. If
{x(n), n1 ≤n ≤n2} is given, and if we want to interpolate xa (t) on
a very ﬁne grid with the grid interval ∆t, then, from (3.37),
xa (m∆t) ≈
n2
	
n=n1
x(n) sinc [Fs(m∆t −nTs)] ,
t1 ≤m∆t ≤t2
(3.43)
which can be implemented as a matrix-vector multiplication operation as
shown below.
>> n = n1:n2; t = t1:t2; Fs = 1/Ts; nTs = n*Ts;
% Ts is the sampling interval
>> xa = x * sinc(Fs*(ones(length(n),1)*t-nTs’*ones(1,length(t))));
Note that it is not possible to obtain an exact analog xa(t) in light of the
fact that we have assumed a ﬁnite number of samples. We now demon-
strate the use of the sinc function in the following two examples and also
study the aliasing problem in the time domain.
□
EXAMPLE 3.21
From the samples x1(n) in Example 3.19a, reconstruct xa(t) and comment on
the results.
Solution
Note that x1(n) was obtained by sampling xa(t) at Ts = 1/Fs = 0.0002 sec. We
will use the grid spacing of 0.00005 sec over −0.005 ≤t ≤0.005, which gives
x(n) over −25 ≤n ≤25.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
93
0
0.5
1
−5
−4
−3
−2
−1
0
1
2
3
4
5
t in msec
Amplitude
Reconstructed Signal from x1(n) Using the sinc Function
FIGURE 3.16
Reconstructed signal in Example 3.21
MATLAB script:
% Discrete-time signal x1(n)
>> Ts = 0.0002; n = -25:1:25; nTs = n*Ts;
x = exp(-1000*abs(nTs));
% Analog signal reconstruction
>> Dt = 0.00005; t = -0.005:Dt:0.005;
>> xa = x * sinc(Fs*(ones(length(n),1)*t-nTs’*ones(1,length(t))));
% Check
>> error = max(abs(xa - exp(-1000*abs(t))))
error =
0.0363
The maximum error between the reconstructed and the actual analog signal is
0.0363, which is due to the fact that xa(t) is not strictly band-limited (and also
we have a ﬁnite number of samples). From Figure 3.16, we note that visually
the reconstruction is excellent.
□
□
EXAMPLE 3.22
From the samples x2(n) in Example 3.17b, reconstruct xa(t) and comment on
the results.
Solution
In this case, x2(n) was obtained by sampling xa(t) at Ts = 1/Fs = 0.001 sec.
We will again use the grid spacing of 0.00005 sec over −0.005 ≤t ≤0.005,
which gives x(n) over −5 ≤n ≤5.
% Discrete-time signal x2(n)
>> Ts = 0.001; n = -5:1:5; nTs = n*Ts;
x = exp(-1000*abs(nTs));
% Analog signal reconstruction
>> Dt = 0.00005; t = -0.005:Dt:0.005;
>> xa = x * sinc(Fs*(ones(length(n),1)*t-nTs’*ones(1,length(t))));
% Check
>> error = max(abs(xa - exp(-1000*abs(t))))
error =
0.1852
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

94
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.5
1
t in msec
Amplitude
Reconstructed Signal from x2(n) Using the sinc Function
FIGURE 3.17
Reconstructed signal in Example 3.22
The maximum error between the reconstructed and the actual analog signals is
0.1852, which is signiﬁcant and cannot be attributed to the nonband-limitedness
of xa(t) alone. From Figure 3.17, observe that the reconstructed signal diﬀers
from the actual one in many places over the interpolated regions. This is the
visual demonstration of aliasing in the time domain.
□
The second MATLAB approach for signal reconstruction is a plotting
approach. The stairs function plots a staircase (ZOH) rendition of the
analog signal, given its samples, while the plot function depicts a linear
(FOH) interpolation between samples.
□
EXAMPLE 3.23
Plot the reconstructed signal from the samples x1(n) in Example 3.19 using the
ZOH and the FOH interpolations. Comment on the plots.
Solution
Note that in this reconstruction, we do not compute xa(t) but merely plot it
using its samples.
% Discrete-time signal x1(n) : Ts = 0.0002
>> Ts = 0.0002; n = -25:1:25; nTs = n*Ts;
x = exp(-1000*abs(nTs));
% Plots
>> subplot(2,1,1); stairs(nTs*1000,x);
>> xlabel(’t in msec’); ylabel(’Amplitude’)
>> title(’Reconstructed Signal from x_1(n) Using ZOH’); hold on
>> stem(n*Ts*1000,x); hold off
%
% Discrete-time signal x1(n) : Ts = 0.001
>> Ts = 0.001; n = -5:1:5; nTs = n*Ts;
x = exp(-1000*abs(nTs));
% Plots
>> subplot(2,1,2); plot(nTs*1000,x);
>> xlabel(’t in msec’); ylabel(’Amplitude’)
>> title(’Reconstructed Signal from x_1(n) Using FOH’); hold on
>> stem(n*Ts*1000,x); hold off
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction of Analog Signals
95
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Reconstructed Signal from x1(n) Using ZOH
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Reconstructed Signal from x1(n) Using FOH
FIGURE 3.18
Signal reconstruction in Example 3.23
The plots are shown in Figure 3.18, from which we observe that the ZOH re-
construction is a crude one and that the further processing of analog signal is
necessary. The FOH reconstruction appears to be a good one, but a careful
observation near t = 0 reveals that the peak of the signal is not correctly repro-
duced. In general, if the sampling frequency is much higher than the Nyquist
rate, then the FOH interpolation provides an acceptable reconstruction.
□
The third approach of reconstruction in MATLAB involves the use
of cubic spline functions. The spline function implements interpolation
between sample points. It is invoked by xa = spline(nTs,x,t), in which
x and nTs are arrays containing samples x(n) at nTs instances, respec-
tively, and t array contains a ﬁne grid at which xa(t) values are desired.
Note once again that it is not possible to obtain an exact analog xa(t).
□
EXAMPLE 3.24
From the samples x1(n) and x2(n) in Example 3.19, reconstruct xa(t) using the
spline function. Comment on the results.
Solution
This example is similar to Examples 3.21 and 3.22. Hence sampling parameters
are the same as before.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

96
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Reconstructed Signal from x1(n) Using Cubic Spline Function
−5
−4
−3
−2
−1
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
t in msec
Amplitude
Reconstructed Signal from x2(n) Using Cubic Spline Function
FIGURE 3.19
Reconstructed signal in Example 3.24
MATLAB script:
% a) Discrete-time signal x1(n): Ts = 0.0002
>> Ts = 0.0002; n = -25:1:25; nTs = n*Ts;
x = exp(-1000*abs(nTs));
% Analog signal reconstruction
>> Dt = 0.00005; t = -0.005:Dt:0.005;
xa = spline(nTs,x,t);
% Check
>> error = max(abs(xa - exp(-1000*abs(t))))
error = 0.0317
The maximum error between the reconstructed and the actual analog signal is
0.0317, which is due to the nonideal interpolation and the fact that xa(t) is
nonband-limited. Comparing this error with that from the sinc (or ideal) inter-
polation, we note that this error is lower. The ideal interpolation generally suf-
fers more from time-limitedness (or from a ﬁnite number of samples). From the
top plot in Figure 3.19, we observe that visually the reconstruction is excellent.
MATLAB script:
% Discrete-time signal x2(n): Ts = 0.001
>> Ts = 0.001; n = -5:1:5; nTs = n*Ts;
x = exp(-1000*abs(nTs));
% Analog signal reconstruction
>> Dt = 0.00005; t = -0.005:Dt:0.005;
xa = spline(nTs,x,t);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
97
% Check
>> error = max(abs(xa - exp(-1000*abs(t))))
error = 0.1679
The maximum error in this case is 0.1679, which is signiﬁcant and cannot be
attributed to the nonideal interpolation or nonband-limitedness of xa(t). From
the bottom plot in Figure 3.19, observe that the reconstructed signal again
diﬀers from the actual one in many places over the interpolated regions.
□
From these examples, it is clear that for practical purposes the spline
interpolation provides the best results.
3.5 PROBLEMS
P3.1
Using the matrix-vector multiplication approach discussed in this chapter, write a
MATLAB function to compute the DTFT of a ﬁnite-duration sequence. The format of
the function should be
function [X] = dtft(x,n,w)
% Computes discrete-time Fourier transform
% [X] = dtft(x,n,w)
%
X = DTFT values computed at w frequencies
%
x = finite duration sequence over n
%
n = sample position vector
%
w = frequency location vector
Use this function to compute the DTFT X(ejω) of the following ﬁnite-duration sequences
over −π ≤ω ≤π. Plot DTFT magnitude and angle graphs in one ﬁgure window.
1. x(n) = (0.6)|n| [u(n + 10) −u(n −11)]. Comment on the angle plot.
2. x(n) = n(0.9)n [u(n) −u(n −21)].
3. x(n) = [cos(0.5πn) + j sin(0.5πn)][u(n) −u(n −51)]. Comment on the magnitude plot.
4. x(n) = {4
↑
, 3, 2, 1, 1, 2, 3, 4}. Comment on the angle plot.
5. x(n) = {4
↑
, 3, 2, 1, −1, −2, −3, −4}. Comment on the angle plot.
P3.2
Let x1(n) = {1
↑
, 2, 2, 1}. A new sequence x2(n) is formed using
x2(n) =
 x1(n),
0 ≤n ≤3
x1(n −4),
4 ≤n ≤7
0,
otherwise
(3.44)
1. Express X2(ejω) in terms of X1(ejω) without explicitly computing X1(ejω).
2. Verify your result using MATLAB by computing and plotting magnitudes of the
respective DTFTs.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

98
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
P3.3
Determine analytically the DTFT of each of the following sequences. Plot the magnitude
and angle of X(ejω) over 0 ≤ω ≤π.
1. x(n) = 2 (0.5)n u(n + 2)
2. x(n) = (0.6)|n| [u(n + 10) −u(n −11)]
3. x(n) = n (0.9)n u(n + 3)
4. x(n) = (n + 3) (0.8)n−1 u(n −2)
5. x(n) = 4 (−0.7)n cos(0.25πn)u(n)
P3.4
The following ﬁnite-duration sequences are called windows and are very useful in DSP:
Rectangular: RM(n) =

1, 0 ≤n < M
0,
otherwise
Hanning: CM(n) = 0.5

1 −cos
2πn
M −1

RM(n)
Triangular: TM(n) =

1 −|M −1 −2n|
M −1

RM(n)
Hamming: HM(n) =

0.54 −0.46 cos
2πn
M −1

RM(n)
For each of these windows, determine their DTFTs for M = 10, 25, 50, 101. Scale
transform values so that the maximum value is equal to 1. Plot the magnitude of the
normalized DTFT over −π ≤ω ≤π. Study these plots and comment on their behavior as
a function of M.
P3.5
Using the deﬁnition of the DTFT in (3.1), determine the sequences corresponding to the
following DTFTs.
1. X(ejω) = 3 + 2 cos(ω) + 4 cos(2ω)
2. X(ejω) = [1 −6 cos(3ω) + 8 cos(5ω)] e−j3ω
3. X(ejω) = 2 + j4 sin(2ω) −5 cos(4ω)
4. X(ejω) = [1 + 2 cos(ω) + 3 cos(2ω)] cos(ω/2)e−j5ω/2
5. X(ejω) = j [3 + 2 cos(ω) + 4 cos(2ω)] sin(ω)e−j3ω
P3.6
Using the deﬁnition of the inverse DTFT in (3.2), determine the sequences corresponding
to the following DTFTs.
1. X(ejω) =

1,
0 ≤|ω| ≤π/3
0,
π/3 < |ω| ≤π
2. X(ejω) =

0,
0 ≤|ω| ≤3π/4
1,
3π/4 < |ω| ≤π
3. X(ejω) =
 2,
0 ≤|ω| ≤π/8
1,
π/8 < |ω| ≤3π/4
0,
3π/4 < |ω| ≤π
4. X(ejω) =
 0,
−π ≤|ω| < π/4
1,
π/4 ≤|ω| ≤3π/4
0,
3π/4 < |ω| ≤π
5. X(ejω) = ω ej(π/2−10ω)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
99
Remember that the above transforms are periodic in ω with period equal to 2π. Hence
functions are given only over the primary period of −π ≤ω ≤π.
P3.7
A complex-valued sequence x(n) can be decomposed into a conjugate symmetric part
xe(n) and a conjugate antisymmetric part xo(n) as discussed in Chapter 2. Show that
F [xe(n)] = XR(ejω)
and
F [xo(n)] = jXI(ejω)
where XR(ejω) and XI(ejω) are the real and imaginary parts of the DTFT X(ejω),
respectively. Verify this property on
x(n) = 2(0.9)−n [cos(0.1πn) + j sin(0.9πn)] [u(n) −u(n −10)]
using the MATLAB functions developed in Chapter 2.
P3.8
A complex-valued DTFT X(ejω) can also be decomposed into its conjugate symmetric
part Xe(ejω) and conjugate antisymmetric part Xo(ejω), that is,
X(ejω) = Xe(ejω) + Xo(ejω)
where
Xe(ejω) = 1
2[X(ejω) + X∗(e−jω)]
and
Xo(ejω) = 1
2[X(ejω) −X∗(e−jω)]
Show that
F −1[Xe(ejω)] = xR(n)
and
F−1[Xo(ejω)] = jxI(n)
where xR(n) and xI(n) are the real and imaginary parts of x(n). Verify this property on
x(n) = ej0.1πn [u(n) −u (n −20)]
using the MATLAB functions developed in Chapter 2.
P3.9
Using the frequency-shifting property of the DTFT, show that the real part of X(ejω) of
a sinusoidal pulse
x(n) = (cos ωon)RM(n)
where RM(n) is the rectangular pulse given in Problem P3.4 is given by
XR(ejω) = 1
2 cos

(ω −ω0)(M −1)
2
$
sin {(ω −ω0) M/2}
sin {(ω −ω0) /2}
+ 1
2 cos

(ω + ω0)(M −1)
2
$
sin {[ω −(2π −ω0)] M/2}
sin {[ω −(2π −ω0)] /2}
Compute and plot XR(ejω) for ωo = π/2 and M = 5, 15, 25, 100. Use the plotting
interval [−π, π]. Comment on your results.
P3.10
Let x(n) = T10(n) be a triangular pulse given in Problem P3.4. Using properties of the
DTFT, determine and plot the DTFT of the following sequences.
1. x(n) = T10(−n)
2. x(n) = T10(n) −T10(n −10)
3. x(n) = T10(n) ∗T10(−n)
4. x(n) = T10(n)ejπn
5. x(n) = cos(0.1πn)T10(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

100
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
P3.11
For each of the linear, shift-invariant systems described by the impulse response,
determine the frequency response function H(ejω). Plot the magnitude response |H(ejω)|
and the phase response ̸ H(ejω) over the interval [−π, π].
1. h(n) = (0.9)|n|
2. h(n) = sinc(0.2n)[u(n + 20) −u(n −20)], where sinc(0) = 1
3. h(n) = sinc(0.2n)[u(n) −u(n −40)]
4. h(n) = [(0.5)n + (0.4)n]u(n)
5. h(n) = (0.5)|n| cos(0.1πn)
P3.12
Let x(n) = A cos(ω0n + θ0) be an input sequence to an LTI system described by the
impulse response h(n). Show that the output sequence y(n) is given by
y(n) = A|H(ejω0)| cos[ω0n + θ0 + ̸ H(ejω0)]
P3.13
Let x(n) = 3 cos (0.5πn + 60◦) + 2 sin (0.3πn) be the input to each of the systems
described in Problem P3.11. In each case, determine the output sequence y(n).
P3.14
An ideal lowpass ﬁlter is described in the frequency domain by
Hd(ejω) =

1 · e−jαω,
|ω| ≤ωc
0,
ωc < |ω| ≤π
where ωc is called the cutoﬀfrequency and α is called the phase delay.
1. Determine the ideal impulse response hd(n) using the IDTFT relation (3.2).
2. Determine and plot the truncated impulse response
h(n) =

hd(n),
0 ≤n ≤N −1
0,
otherwise
for N = 41, α = 20, and ωc = 0.5π.
3. Determine and plot the frequency response function H(ejω), and compare it with the
ideal lowpass ﬁlter response Hd(ejω). Comment on your observations.
P3.15
An ideal highpass ﬁlter is described in the frequency-domain by
Hd(ejω) =

1 · e−jαω,
ωc < |ω| ≤π
0,
|ω| ≤ωc
where ωc is called the cutoﬀfrequency and α is called the phase delay.
1. Determine the ideal impulse response hd(n) using the IDTFT relation (3.2).
2. Determine and plot the truncated impulse response
h(n) =

hd(n),
0 ≤n ≤N −1
0,
otherwise
for N = 31, α = 15, and ωc = 0.5π.
3. Determine and plot the frequency response function H(ejω), and compare it with the
ideal highpass ﬁlter response Hd(ejω). Comment on your observations.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
101
P3.16
For a linear, shift-invariant system described by the diﬀerence equation
y(n) =
M
	
m=0
bmx (n −m) −
N
	
ℓ=1
aℓy (n −ℓ)
the frequency-response function is given by
H(ejω) =
M
m=0 bme−jωm
1 + N
ℓ=1 aℓe−jωℓ
Write a MATLAB function freqresp to implement this relation. The format of this
function should be
function [H] = freqresp(b,a,w)
% Frequency response function from difference equation
%
[H] = freqresp(b,a,w)
% H = frequency response array evaluated at w frequencies
% b = numerator coefficient array
% a = denominator coefficient array (a(1)=1)
% w = frequency location array
P3.17
Determine H(ejω), and plot its magnitude and phase for each of the following systems.
1. y(n) = 1
5
4
m=0 x(n −m)
2. y(n) = x(n) −x(n −2) + 0.95y(n −1) −0.9025y(n −2)
3. y(n) = x(n) −x(n −1) + x(n −2) + 0.95y(n −1) −0.9025y(n −2)
4. y(n) = x(n) −1.7678x(n −1) + 1.5625x(n −2) + 1.1314y(n −1) −0.64y(n −2)
5. y(n) = x(n) −5
ℓ=1 (0.5)ℓy (n −ℓ)
P3.18
A linear, shift-invariant system is described by the diﬀerence equation
y(n) =
3
	
m=0
x (n −2m) −
3
	
ℓ=1
(0.81)ℓy (n −2ℓ)
Determine the steady-state response of the system to the following inputs.
1. x(n) = 5 + 10 (−1)n
2. x(n) = 1 + cos (0.5πn + π/2)
3. x(n) = 2 sin (πn/4) + 3 cos (3πn/4)
4. x(n) = 5
k=0 (k + 1) cos (πkn/4)
5. x(n) = cos (πn)
In each case, generate x(n), 0 ≤n ≤200, and process it through the filter function to
obtain y(n). Compare your y(n) with the steady-state responses in each case.
P3.19
An analog signal xa (t) = sin (1000πt) is sampled using the following sampling intervals.
In each case, plot the spectrum of the resulting discrete-time signal.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

102
Chapter 3
THE DISCRETE-TIME FOURIER ANALYSIS
1. Ts = 0.1 msec
2. Ts = 1 msec
3. Ts = 0.01 sec
P3.20
We implement the following analog ﬁlter using a discrete ﬁlter:
xa (t) −→A/D
x(n)
−→h(n)
y(n)
−→D/A −→ya (t)
The sampling rate in the A/D and D/A is 8000 sam/sec, and the impulse response is
h(n) = (−0.9)n u(n).
1. What is the digital frequency in x(n) if xa (t) = 10 cos (10,000πt)?
2. Determine the steady-state output ya (t) if xa (t) = 10 cos (10,000πt).
3. Determine the steady-state output ya (t) if xa (t) = 5 sin(8,000πt).
4. Find two other analog signals xa (t), with diﬀerent analog frequencies, that will give
the same steady-state output ya(t) when xa(t) = 10 cos(10,000πt) is applied.
5. To prevent aliasing, a preﬁlter would be required to process xa (t) before it passes to
the A/D converter. What type of ﬁlter should be used, and what should be the largest
cutoﬀfrequency that would work for the given conﬁguration?
P3.21
Consider an analog signal xa (t) = cos(20πt), 0 ≤t ≤1. It is sampled at Ts = 0.01, 0.05,
and 0.1 sec intervals to obtain x(n).
1. For each Ts, plot x(n).
2. Reconstruct the analog signal ya (t) from the samples x(n) using the sinc interpolation
(use ∆t = 0.001) and determine the frequency in ya (t) from your plot. (Ignore the end
eﬀects.)
3. Reconstruct the analog signal ya (t) from the samples x(n) using the cubic spline
interpolation, and determine the frequency in ya (t) from your plot. (Again, ignore the
end eﬀects.)
4. Comment on your results.
P3.22
Consider the analog signal xa (t) = cos (20πt + θ) , 0 ≤t ≤1. It is sampled at Ts = 0.05
sec intervals to obtain x(n). Let θ = 0, π/6, π/4, π/3, π/2. For each of these θ values,
perform the following.
1. Plot xa (t) and superimpose x(n) on it using the plot(n,x,’o’) function.
2. Reconstruct the analog signal ya (t) from the samples x(n) using the sinc interpolation
(Use ∆t = 0.001) and superimpose x(n) on it.
3. Reconstruct the analog signal ya (t) from the samples x(n) using the cubic spline
interpolation and superimpose x(n) on it.
4. You should observe that the resultant reconstruction in each case has the correct
frequency but a diﬀerent amplitude. Explain this observation. Comment on the role of
phase of xa (t) on the sampling and reconstruction of signals.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

103
C H A P T E R 4
The z-Transform
In Chapter 3, we studied the discrete-time Fourier transform approach for
representing discrete signals using complex exponential sequences. This
representation clearly has advantages for LTI systems because it describes
systems in the frequency domain using the frequency response function
H(ejω). The computation of the sinusoidal steady-state response is greatly
facilitated by the use of H(ejω). Furthermore, response to any arbitrary
absolutely summable sequence x(n) can easily be computed in the fre-
quency domain by multiplying the transform X(ejω) and the frequency
response H(ejω). However, there are two shortcomings to the Fourier
transform approach. First, there are many useful signals in practice—
such as u(n) and nu(n)—for which the discrete-time Fourier transform
does not exist. Second, the transient response of a system due to ini-
tial conditions or due to changing inputs cannot be computed using the
discrete-time Fourier transform approach.
Therefore, we now consider an extension of the discrete-time Fourier
transform to address these two problems. This extension is called the
z-transform. Its bilateral (or two-sided) version provides another domain
in which a larger class of sequences and systems can be analyzed, and its
unilateral (or one-sided) version can be used to obtain system responses
with initial conditions or changing inputs.
4.1 THE BILATERAL z-TRANSFORM
The z-transform of a sequence x(n) is given by
X(z)
△= Z[x(n)] =
∞
	
n=−∞
x(n)z−n
(4.1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

104
Chapter 4
THE z-TRANSFORM
where z is a complex variable. The set of z values for which X(z) exists
is called the region of convergence (ROC) and is given by
Rx−< |z| < Rx+
(4.2)
for some nonnegative numbers Rx−and Rx+.
The inverse z-transform of a complex function X(z) is given by
x(n)
△= Z−1[X(z)] =
1
2πj
%
C
X(z)zn−1dz
(4.3)
where C is a counterclockwise contour encircling the origin and lying
in the ROC.
Comments:
1. The complex variable z is called the complex frequency given by z =
|z|ejω, where |z| is the magnitude and ω is the real frequency.
2. Since the ROC (4.2) is deﬁned in terms of the magnitude |z|, the shape
of the ROC is an open ring, as shown in Figure 4.1. Note that Rx−
may be equal to zero and/or Rx+ could possibly be ∞.
3. If Rx+ < Rx−, then the ROC is a null space and the z-transform does
not exist.
4. The function |z| = 1 (or z = ejω) is a circle of unit radius in the z-plane
and is called the unit circle. If the ROC contains the unit circle, then
we can evaluate X(z) on the unit circle:
X(z)|z=ejω = X(ejω) =
∞
	
n=−∞
x(n)e−jωn = F[x(n)]
Therefore, the discrete-time Fourier transform X(ejω) may be viewed
as a special case of the z-transform X(z).
Re{z}
Rx+
Rx–
Im{z}
FIGURE 4.1
A general region of convergence
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Bilateral z-Transform
105
Im{z}
Re{z}
0
a
FIGURE 4.2
The ROC in Example 4.1
□
EXAMPLE 4.1
Let x1(n) = anu(n),
0 < |a| < ∞. (This sequence is called a positive-time
sequence.) Then
X1(z) =
∞
	
0
anz−n =
∞
	
0
a
z
n
=
1
1 −az−1 ;
if
a
z
 < 1
=
z
z −a,
|z| > |a| ⇒ROC1:
|a|
&'()
Rx−
< |z| < ∞
&'()
Rx+
Note:
X1(z) in this example is a rational function; that is,
X1(z)
△= B(z)
A(z) =
z
z −a
where B(z) = z is the numerator polynomial and A(z) = z−a is the denominator
polynomial. The roots of B(z) are called the zeros of X(z), whereas the roots
of A(z) are called the poles of X(z). In this example, X1(z) has a zero at the
origin z = 0 and a pole at z = a. Hence x1(n) can also be represented by a
pole-zero diagram in the z-plane in which zeros are denoted by ◦and poles by
×, as shown in Figure 4.2.
□
□
EXAMPLE 4.2
Let x2(n) = −bnu(−n−1), 0 < |b| < ∞. (This sequence is called a negative-time
sequence.) Then
X2(z) = −
−1
	
−∞
bnz−n = −
−1
	
−∞
 b
z
n
= −
∞
	
1
z
b
n
= 1 −
∞
	
0
z
b
n
= 1 −
1
1 −z/b =
z
z −b,
ROC2:
0
&'()
Rx−
< |z| < |b|
&'()
Rx+
The ROC2 and the pole-zero plot for this x2(n) are shown in Figure 4.3.
Im{z}
Re{z}
0
b
FIGURE 4.3
The ROC in Example 4.2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

106
Chapter 4
THE z-TRANSFORM
Note:
If b = a in this example, then X2(z) = X1(z) except for their respective
ROCs; that is, ROC1 ̸= ROC2. This implies that the ROC is a distinguishing
feature that guarantees the uniqueness of the z-transform. Hence it plays a very
important role in system analysis.
□
□
EXAMPLE 4.3
Let x3(n) = x1(n) + x2(n) = anu(n) −bnu(−n −1) (This sequence is called a
two-sided sequence.) Then, using the preceding two examples,
X3(z) =
∞
	
n=0
anz−n −
−1
	
−∞
bnz−n
=

z
z −a, ROC1: |z| > |a|

+

z
z −b, ROC2: |z| < |b|

=
z
z −a +
z
z −b;
ROC3: ROC1 ∩ROC2
If |b| < |a|, then ROC3 is a null space, and X3(z) does not exist. If |a| < |b|,
then the ROC3 is |a| < |z| < |b|, and X3(z) exists in this region as shown in
Figure 4.4.
□
4.1.1 PROPERTIES OF THE ROC
From the observation of the ROCs in the preceding three examples, we
state the following properties.
1. The ROC is always bounded by a circle since the convergence
condition is on the magnitude |z|.
2. The sequence x1(n) = anu(n) in Example 4.1 is a special case of a right-
sided sequence, deﬁned as a sequence x(n) that is zero for some n <
n0. From Example 4.1, the ROC for right-sided sequences is always
outside of a circle of radius Rx−. If n0 ≥0, then the right-sided
sequence is also called a causal sequence.
3. The sequence x2(n) = −bnu(−n−1) in Example 4.2 is a special case of a
left-sided sequence, deﬁned as a sequence x(n) that is zero for some n >
n0. If n0 ≤0, the resulting sequence is called an anticausal sequence.
From Example 4.2, the ROC for left-sided sequences is always inside
of a circle of radius Rx+.
Im{z}
Re{z}
0
a
b
a > b
Im{z}
Re{z}
0
a
b
a < b
FIGURE 4.4
The ROC in Example 4.3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Important Properties of the z-Transform
107
4. The sequence x3(n) in Example 4.3 is a two-sided sequence. The ROC
for two-sided sequences is always an open ring Rx−< |z| < Rx+,
if it exists.
5. The sequences that are zero for n < n1 and n > n2 are called
finite-duration sequences. The ROC for such sequences is the entire
z-plane. If n1 < 0, then z = ∞is not in the ROC. If n2 > 0, then
z = 0 is not in the ROC.
6. The ROC cannot include a pole, since X(z) converges uniformly in
there.
7. There is at least one pole on the boundary of a ROC of a rational X(z).
8. The ROC is one contiguous region; that is, the ROC does not come in
pieces.
In digital signal processing, signals are assumed to be causal since
almost every digital data is acquired in real time. Therefore, the only
ROC of interest to us is the one given in statement 2.
4.2 IMPORTANT PROPERTIES OF THE z-TRANSFORM
The properties of the z-transform are generalizations of the properties
of the discrete-time Fourier transform that we studied in Chapter 3. We
state the following important properties of the z-transform without proof.
1. Linearity:
Z [a1x1(n) + a2x2(n)] = a1X1(z) + a2X2(z);
ROC: ROCx1 ∩ROCx2
(4.4)
2. Sample shifting:
Z [x (n −n0)] = z−n0X(z);
ROC: ROCx
(4.5)
3. Frequency shifting:
Z [anx(n)] = X
z
a

;
ROC: ROCx scaled by |a|
(4.6)
4. Folding:
Z [x (−n)] = X (1/z) ;
ROC: Inverted ROCx
(4.7)
5. Complex conjugation:
Z [x∗(n)] = X∗(z∗);
ROC: ROCx
(4.8)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

108
Chapter 4
THE z-TRANSFORM
6. Diﬀerentiation in the z-domain:
Z [nx(n)] = −z dX(z)
dz
;
ROC: ROCx
(4.9)
This property is also called the multiplication-by-a-ramp property.
7. Multiplication:
Z [x1(n)x2 (n)] =
1
2πj
%
C
X1(ν)X2 (z/ν) ν−1dν;
(4.10)
ROC: ROCx1 ∩Inverted ROCx2
where C is a closed contour that encloses the origin and lies in the
common ROC.
8. Convolution:
Z [x1(n) ∗x2(n)] = X1(z)X2(z);
ROC: ROCx1 ∩ROCx2
(4.11)
This last property transforms the time-domain convolution operation
into a multiplication between two functions. It is a signiﬁcant property
in many ways. First, if X1(z) and X2(z) are two polynomials, then their
product can be implemented using the conv function in MATLAB.
□
EXAMPLE 4.4
Let X1(z) = 2+3z−1+4z−2, and let X2(z) = 3+4z−1+5z−2+6z−3. Determine
X3(z) = X1(z)X2(z).
Solution
From the deﬁnition of the z-transform, we observe that
x1(n) = {2
↑
, 3, 4}
and
x2(n) = {3
↑
, 4, 5, 6}
Then the convolution of these two sequences will give the coeﬃcients of the
required polynomial product.
MATLAB script:
>> x1 = [2,3,4]; x2 = [3,4,5,6]; x3 = conv(x1,x2)
x3 =
6
17
34
43
38
24
Hence
X3(z) = 6 + 17z−1 + 34z−2 + 43z−3 + 38z−4 + 24z−5
Using the conv m function developed in Chapter 2, we can also multiply
two z-domain polynomials corresponding to noncausal sequences.
□
□
EXAMPLE 4.5
Let X1(z) = z + 2 + 3z−1, and let X2(z) = 2z2 + 4z + 3 + 5z−1. Determine
X3(z) = X1(z)X2(z).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Important Properties of the z-Transform
109
Solution
Note that
x1(n) = {1, 2
↑
, 3}
and
x2(n) = {2, 4, 3
↑
, 5}
Using the MATLAB script
>> x1 = [1,2,3]; n1 = [-1:1];
x2 = [2,4,3,5]; n2 = [-2:1];
>> [x3,n3] = conv_m(x1,n1,x2,n2)
x3 =
2
8
17
23
19
15
n3 =
-3
-2
-1
0
1
2
we have
X3(z) = 2z3 + 8z2 + 17z + 23 + 19z−1 + 15z−2
□
In passing, we note that to divide one polynomial by another one, we
would require an inverse operation called deconvolution [79, Chapter 6].
In MATLAB, [p,r] = deconv(b,a) computes the result of dividing b
by a in a polynomial part p and a remainder r. For example, if we divide
the polynomial X3(z) in Example 4.4 by X1(z), as follows,
>> x3 = [6,17,34,43,38,24]; x1 = [2,3,4]; [x2,r] = deconv(x3,x1)
x2 =
3
4
5
6
r =
0
0
0
0
0
0
then we obtain the coeﬃcients of the polynomial X2(z) as expected. To
obtain the sample index, we will have to modify the deconv function as
we did in the conv m function. This is explored in Problem P4.10. This
operation is useful in obtaining a proper rational part from an improper
rational function.
The second important use of the convolution property is in system
output computations, as we shall see in a later section. This interpretation
is particularly useful for verifying the z-transform expression X(z) of a
causal sequence using MATLAB. Note that since MATLAB is a numerical
processor (unless the Symbolic toolbox is used), it cannot be used for
symbolic z-transform calculations. We will now elaborate on this. Let
x(n) be a sequence with a rational transform
X(z) = B(z)
A(z)
where B(z) and A(z) are polynomials in z−1. If we use the coeﬃcients of
B(z) and A(z) as the b and a arrays in the filter routine and excite this
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

110
Chapter 4
THE z-TRANSFORM
ﬁlter by the impulse sequence δ(n), then from (4.11) and using Z[δ(n)] =
1, the output of the ﬁlter will be x(n). (This is a numerical approach
of computing the inverse z-transform; we will discuss the analytical ap-
proach in the next section.) We can compare this output with the given
x(n) to verify that X(z) is indeed the transform of x(n). This is illus-
trated in Example 4.6. An equivalent approach is to use the impz function
discussed in Chapter 2.
4.2.1 SOME COMMON z-TRANSFORM PAIRS
Using the deﬁnition of z-transform and its properties, one can determine
z-transforms of common sequences. A list of some of these sequences is
given in Table 4.1.
TABLE 4.1
Some common z-transform pairs
Sequence
Transform
ROC
δ(n)
1
∀z
u(n)
1
1 −z−1
|z| > 1
−u(−n −1)
1
1 −z−1
|z| < 1
anu(n)
1
1 −az−1
|z| > |a|
−bnu(−n −1)
1
1 −bz−1
|z| < |b|
[an sin ω0n] u(n)
(a sin ω0)z−1
1 −(2a cos ω0)z−1 + a2z−2
|z| > |a|
[an cos ω0n] u(n)
1 −(a cos ω0)z−1
1 −(2a cos ω0)z−1 + a2z−2
|z| > |a|
nanu(n)
az−1
(1 −az−1)2
|z| > |a|
−nbnu(−n −1)
bz−1
(1 −bz−1)2
|z| < |b|
□
EXAMPLE 4.6
Using z-transform properties and the z-transform table, determine the z-
transform of
x(n) = (n −2)(0.5)(n−2) cos
π
3 (n −2)

u(n −2)
Solution
Applying the sample-shift property,
X(z) = Z[x(n)] = z−2Z

n(0.5)n cos
πn
3

u(n)

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Important Properties of the z-Transform
111
with no change in the ROC. Applying the multiplication by a ramp property,
X(z) = z−2

−z dZ[(0.5)n cos( π
3 n)u(n)]
dz
$
with no change in the ROC. Now the z-transform of (0.5)n cos( π
3 n)u(n) from
Table 4.1 is
Z

(0.5)n cos
πn
3

u(n)

=
1 −(0.5 cos π
3 )z−1
1 −2(0.5 cos π
3 )z−1 + 0.25z−2 ;
|z| > 0.5
=
1 −0.25z−1
1 −0.5z−1 + 0.25z−2 ;
|z| > 0.5
Hence
X(z) = −z−1 d
dz

1 −0.25z−1
1 −0.5z−1 + 0.25z−2
$
,
|z| > 0.5
= −z−1

−0.25z−2 + 0.5z−3 −0.0625z−4
1 −z−1 + 0.75z−2 −0.25z−3 + 0.0625z−4
$
,
|z| > 0.5
=
0.25z−3 −0.5z−4 + 0.0625z−5
1 −z−1 + 0.75z−2 −0.25z−3 + 0.0625z−4 ,
|z| > 0.5
MATLAB veriﬁcation: To check that this X(z) is indeed the correct expression,
let us compute the ﬁrst eight samples of the sequence x(n) corresponding to
X(z), as discussed before.
>> b = [0,0,0,0.25,-0.5,0.0625]; a = [1,-1,0.75,-0.25,0.0625];
>> [delta,n]=impseq(0,0,7)
delta =
1
0
0
0
0
0
0
0
n =
0
1
2
3
4
5
6
7
>> x = filter(b,a,delta) % check sequence
x =
Columns 1 through 4
0
0
0
0.25000000000000
Columns 5 through 8
-0.25000000000000
-0.37500000000000
-0.12500000000000
0.07812500000000
>> x = [(n-2).*(1/2).ˆ(n-2).*cos(pi*(n-2)/3)].*stepseq(2,0,7) % original sequence
x =
Columns 1 through 4
0
0
0
0.25000000000000
Columns 5 through 8
-0.25000000000000
-0.37500000000000
-0.12500000000000
0.07812500000000
This approach can be used to verify the z-transform computations.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

112
Chapter 4
THE z-TRANSFORM
4.3 INVERSION OF THE z-TRANSFORM
From equation (4.3), the inverse z-transform computation requires an
evaluation of a complex contour integral that, in general, is a complicated
procedure. The most practical approach is to use the partial fraction ex-
pansion method. It makes use of the z-transform Table 4.1 (or similar
tables available in many textbooks). The z-transform, however, must be
a rational function. This requirement is generally satisﬁed in digital signal
processing.
Central Idea
When X(z) is a rational function of z−1, it can be ex-
pressed as a sum of simple factors using the partial fraction expansion.
The individual sequences corresponding to these factors can then be writ-
ten down using the z-transform table.
The inverse z-transform procedure can be summarized using the fol-
lowing steps:
1. Consider the rational function
X(z) = b0 + b1z−1 + · · · + bMz−M
1 + a1z−1 + · · · + aNz−N , Rx−< |z| < Rx+
(4.12)
2. Express (4.12) as
X(z) =
˜b0 + ˜b1z−1 + · · · + ˜bN−1z−(N−1)
1 + a1z−1 + · · · + aNz−N
&
'(
)
Proper rational part
+
M−N
	
k=0
Ckz−k
&
'(
)
Polynomial part if M≥N
where the ﬁrst term on the right-hand side is the proper rational part
and the second term is the polynomial (ﬁnite-length) part. This can
be obtained by performing polynomial division if M ≥N using the
deconv function.
3. Perform a partial fraction expansion on the proper rational part of
X(z) to obtain
X(z) =
N
	
k=1
Rk
1 −pkz−1 +
M−N
	
k=0
Ckz−k
&
'(
)
M≥N
(4.13)
where pk is the kth pole of X(z) and Rk is the residue at pk. It is
assumed that the poles are distinct, for which the residues are given by
Rk =
˜b0 + ˜b1z−1 + · · · + ˜bN−1z−(N−1)
1 + a1z−1 + · · · + aNz−N
(1 −pkz−1)

z=pk
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Inversion of the z-Transform
113
For repeated poles, the expansion (4.13) has a more general form. If a
pole pk has multiplicity r, then its expansion is given by
r
	
ℓ=1
Rk,ℓz−(ℓ−1)
(1 −pkz−1)ℓ=
Rk,1
1 −pkz−1 +
Rk,2z−1
(1 −pkz−1)2 + · · · + Rk,rz−(r−1)
(1 −pkz−1)r
(4.14)
where the residues Rk,ℓare computed using a more general formula,
which is available in reference [79].
4. Assuming distinct poles as in (4.13), write x(n) as
x(n) =
N
	
k=1
RkZ−1

1
1 −pkz−1

+
M−N
	
k=0
Ckδ(n −k)
&
'(
)
M≥N
5. Finally, use the relation from Table 4.1
Z−1

1
1 −pkz−1

=

pn
ku(n)
|zk| ≤Rx−
−pn
ku(−n −1) |zk| ≥Rx+
(4.15)
to complete x(n).
A similar procedure is used for repeated poles.
□
EXAMPLE 4.7
Find the inverse z-transform of x(z) =
z
3z2 −4z + 1.
Solution
Write
X(z) =
z
3(z2 −4
3z + 1
3) =
1
3z−1
1 −4
3z−1 + 1
3z−2
=
1
3z−1
(1 −z−1)(1 −1
3z−1) =
1
2
1 −z−1 −
1
2
1 −1
3z−1
or
X(z) = 1
2

1
1 −z−1

−1
2

1
1 −1
3z−1

Now, X(z) has two poles: z1 = 1 and z2 = 1
3, and since the ROC is not speciﬁed,
there are three possible ROCs, as shown in Figure 4.5.
a. ROC1: 1 < |z| < ∞. Here both poles are on the interior side of the ROC1;
that is, |z1| ≤Rx−= 1 and |z2| ≤1. Hence from (4.15),
x1(n) = 1
2u(n) −1
2
1
3
n
u(n)
which is a right-sided sequence.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

114
Chapter 4
THE z-TRANSFORM
Im{z}
Re{z}
0 1/3
1
ROC1
Im{z}
Re{z}
1/3
1
ROC2
Im{z}
Re{z}
1/3
1
ROC3
FIGURE 4.5
The ROCs in Example 4.7
b. ROC2: 0 < |z| < 1
3. Here both poles are on the exterior side of the ROC2;
that is, |z1| ≥Rx+ = 1
3 and |z2| ≥1
3. Hence from (4.15),
x2(n) = 1
2 {−u(−n −1)} −1
2
*
− 1
3
n u(−n −1)+
= 1
2
1
3
n
u(−n −1) −1
2u(−n −1)
which is a left-sided sequence.
c. ROC3: 1
3 < |z| < 1. Here pole z1 is on the exterior side of the ROC3, that is,
|z1| ≥Rx+ = 1 and pole z2 is on the interior side, that is, |z2| ≤1
3. Hence
from (4.15),
x3(n) = −1
2u(−n −1) −1
2
1
3
n
u(n)
which is a two-sided sequence.
□
4.3.1 MATLAB IMPLEMENTATION
A MATLAB function residuez is available to compute the residue part
and the direct (or polynomial) terms of a rational function in z−1. Let
X(z) = b0 + b1z−1 + · · · + bMz−M
a0 + a1z−1 + · · · + aNz−N = B(z)
A(z)
=
N
	
k=1
Rk
1 −pkz−1 +
M−N
	
k=0
Ckz−k
&
'(
)
M≥N
be a rational function in which the numerator and the denominator poly-
nomials are in ascending powers of z−1. Then [R,p,C]=residuez(b,a)
computes the residues, poles, and direct terms of X(z) in which two poly-
nomials B(z) and A(z) are given in two vectors b and a, respectively.
The returned column vector R contains the residues, column vector p
contains the pole locations, and row vector C contains the direct terms.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Inversion of the z-Transform
115
If p(k)=...=p(k+r-1) is a pole of multiplicity r, then the expansion in-
cludes the term of the form
Rk
1 −pkz−1 +
Rk+1
(1 −pkz−1)2 + · · · +
Rk+r−1
(1 −pkz−1)r
(4.16)
which is diﬀerent from (4.14).
Similarly, [b,a]=residuez(R,p,C), with three input arguments and
two output arguments, converts the partial fraction expansion back to
polynomials with coeﬃcients in row vectors b and a.
□
EXAMPLE 4.8
To verify our residue calculations, let us consider the rational function
X(z) =
z
3z2 −4z + 1
given in Example 4.7.
Solution
First, rearrange X(z) so that it is a function in ascending powers of z−1:
X(z) =
z−1
3 −4z−1 + z−2 =
0 + z−1
3 −4z−1 + z−2
Now, using the MATLAB script
>> b = [0,1]; a = [3,-4,1]; [R,p,C] = residuez(b,a)
R =
0.5000
-0.5000
p =
1.0000
0.3333
c =
[]
we obtain
X(z) =
1
2
1 −z−1 −
1
2
1 −1
3z−1
as before. Similarly, to convert back to the rational function form,
>> [b,a] = residuez(R,p,C)
b =
0.0000
0.3333
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

116
Chapter 4
THE z-TRANSFORM
a =
1.0000
-1.3333
0.3333
so that
X(z) =
0 + 1
3z−1
1 −4
3z−1 + 1
3z−2 =
z−1
3 −4z−1 + z−2 =
z
3z2 −4z + 1
as before.
□
□
EXAMPLE 4.9
Compute the inverse z-transform of
X(z) =
1
(1 −0.9z−1)2 (1 + 0.9z−1)
,
|z| > 0.9
Solution
We will evaluate the denominator polynomial as well as the residues using the
following MATLAB script:
>> b = 1; a = poly([0.9,0.9,-0.9])
a =
1.0000
-0.9000
-0.8100
0.7290
>> [R,p,C]=residuez(b,a)
R =
0.2500
0.5000
0.2500
p =
0.9000
0.9000
-0.9000
c =
[]
Note that the denominator polynomial is computed using MATLAB’s polyno-
mial function poly, which computes the polynomial coeﬃcients, given its roots.
We could have used the conv function, but the use of the poly function is more
convenient for this purpose. From the residue calculations and using the order
of residues given in (4.16), we have
X(z) =
0.25
1 −0.9z−1 +
0.5
(1 −0.9z−1)2 +
0.25
1 + 0.9z−1 ,
|z| > 0.9
=
0.25
1 −0.9z−1 + 0.5
0.9z

0.9z−1
(1 −0.9z−1)2 +
0.25
1 + 0.9z−1 ,
|z| > 0.9
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Inversion of the z-Transform
117
Hence from Table 4.1 and using the z-transform property of time-shift,
x(n) = 0.25(0.9)nu(n) + 5
9(n + 1)(0.9)n+1u(n + 1) + 0.25 (−0.9)n u(n)
which, upon simpliﬁcation, becomes
x(n) = 0.75(0.9)nu(n) + 0.5n(0.9)nu(n) + 0.25 (−0.9)n u(n)
MATLAB veriﬁcation:
>> [delta,n] = impseq(0,0,7);
x = filter(b,a,delta) % Check sequence
x =
Columns 1 through 4
1.00000000000000
0.90000000000000
1.62000000000000
1.45800000000000
Columns 5 through 8
1.96830000000000
1.77147000000000
2.12576400000000
1.91318760000000
>> x = (0.75)*(0.9).ˆn + (0.5)*n.*(0.9).ˆn + (0.25)*(-0.9).ˆn % answer sequence
x =
Columns 1 through 4
1.00000000000000
0.90000000000000
1.62000000000000
1.45800000000000
Columns 5 through 8
1.96830000000000
1.77147000000000
2.12576400000000
1.91318760000000
□
□
EXAMPLE 4.10
Determine the inverse z-transform of
X(z) =
1 + 0.4
√
2z−1
1 −0.8
√
2z−1 + 0.64z−2
so that the resulting sequence is causal and contains no complex numbers.
Solution
We will have to ﬁnd the poles of X(z) in the polar form to determine the ROC
of the causal sequence.
MATLAB script:
>> b = [1,0.4*sqrt(2)]; a=[1,-0.8*sqrt(2),0.64];
>> [R,p,C] = residuez(b,a)
R =
0.5000 - 1.0000i
0.5000 + 1.0000i
p =
0.5657 + 0.5657i
0.5657 - 0.5657i
C =
[]
>> Mp=(abs(p))’
% Pole magnitudes
Mp =
0.8000
0.8000
>> Ap=(angle(p))’/pi
% Pole angles in pi units
Ap =
0.2500
-0.2500
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

118
Chapter 4
THE z-TRANSFORM
From these calculations,
X(z) =
0.5 −j
1 −0.8e+j π
4 z−1 +
0.5 + j
1 −0.8e−j π
4 z−1 ,
|z| > 0.8
and from Table 4.1, we have
x(n) = (0.5 −j) 0.8ne+j π
4 nu(n) + (0.5 + j) 0.8ne−j π
4 nu(n)
= 0.8n[0.5{e+j π
4 n + e−j π
4 n} −j{e+j π
4 n −e−j π
4 n}]u(n)
= 0.8n 
cos
πn
4

+ 2 sin
πn
4

u(n)
MATLAB veriﬁcation:
>> [delta, n] = impseq(0,0,6);
x = filter(b,a,delta) % Check sequence
x =
Columns 1 through 4
1.00000000000000
1.69705627484771
1.28000000000000
0.36203867196751
Columns 5 through 8
-0.40960000000000
-0.69511425017762
-0.52428800000000
-0.14829104003789
>> x = ((0.8).ˆn).*(cos(pi*n/4)+2*sin(pi*n/4))
x =
Columns 1 through 4
1.00000000000000
1.69705627484771
1.28000000000000
0.36203867196751
Columns 5 through 8
-0.40960000000000
-0.69511425017762
-0.52428800000000
-0.14829104003789
□
4.4 SYSTEM REPRESENTATION IN THE z-DOMAIN
Similar to the frequency response function H(ejω), we can deﬁne the
z-domain function, H(z), called the system function. However, unlike
H(ejω), H(z) exists for systems that may not be BIBO stable.
DEFINITION 1
The System Function
The system function H(z) is given by
H(z)
△= Z [h(n)] =
∞
	
−∞
h(n)z−n;
Rh−< |z| < Rh+
(4.17)
Using the convolution property (4.11) of the z-transform, the output
transform Y (z) is given by
Y (z) = H(z) X(z)
: ROCy = ROCh ∩ROCx
(4.18)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

System Representation in the z-Domain
119
provided ROCx overlaps with ROCh. Therefore, a linear and time-
invariant system can be represented in the z-domain by
X(z) −→H(z) −→Y (z) = H(z) X(z)
4.4.1 SYSTEM FUNCTION FROM THE DIFFERENCE
EQUATION REPRESENTATION
When LTI systems are described by a diﬀerence equation
y(n) +
N
	
k=1
aky(n −k) =
M
	
ℓ=0
bℓx(n −ℓ)
(4.19)
the system function H(z) can easily be computed. Taking the z-transform
of both sides, and using properties of the z-transform,
Y (z) +
N
	
k=1
akz−kY (z) =
M
	
ℓ=0
bℓz−ℓX(z)
or
H(z)
△= Y (z)
X(z) =
M
ℓ=0 bℓz−ℓ
1 + N
k=1 akz−k = B(z)
A(z)
=
b0z−M 
zM + · · · + bM
b0

z−N (zN + · · · + aN)
(4.20)
After factorization, we obtain
H(z) = b0 zN−M
,N
ℓ=1(z −zℓ)
,N
k=1(z −pk)
(4.21)
where zℓ’s are the system zeros and pk’s are the system poles. Thus H(z)
(and hence an LTI system) can also be represented in the z-domain using
a pole-zero plot. This fact is useful in designing simple ﬁlters by proper
placement of poles and zeros.
To determine zeros and poles of a rational H(z), we can use the
MATLAB function roots on both the numerator and the denominator
polynomials. (Its inverse function poly determines polynomial coeﬃcients
from its roots, as discussed in the previous section.) It is also possible to
use MATLAB to plot these roots for a visual display of a pole-zero plot.
The function zplane(b,a) plots poles and zeros, given the numerator
row vector b and the denominator row vector a. As before, the symbol o
represents a zero and the symbol x represents a pole. The plot includes
the unit circle for reference. Similarly, zplane(z,p) plots the zeros in
column vector z and the poles in column vector p. Note very carefully the
form of the input arguments for the proper use of this function.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

120
Chapter 4
THE z-TRANSFORM
4.4.2 TRANSFER FUNCTION REPRESENTATION
If the ROC of H(z) includes a unit circle (z = ejω), then we can evaluate
H(z) on the unit circle, resulting in a frequency response function or
transfer function H(ejω). Then from (4.21),
H(ejω) = b0 ej(N−M)ω
,M
1 (ejω −zℓ)
,N
1 (ejω −pk)
(4.22)
The factor (ejω −zℓ) can be interpreted as a vector in the complex z-plane
from a zero zℓto the unit circle at z = ejω, while the factor (ejω −pk)
can be interpreted as a vector from a pole pk to the unit circle at z = ejω.
This is shown in Figure 4.6. Hence the magnitude response function
|H(ejω)| = |b0||ejω −z1| · · · |ejω −zM|
|ejω −p1| · · · |ejω −pN|
(4.23)
can be interpreted as a product of the lengths of vectors from the zeros
to the unit circle divided by the lengths of vectors from the poles to the
unit circle and scaled by |b0|. Similarly, the phase response function
̸ H(ejω) =[0 or π]
& '( )
Constant
+ [(N −M) ω]
&
'(
)
Linear
+
M
	
1
̸ (ejω −zk) −
N
	
1
̸ (ejω −pk)
&
'(
)
Nonlinear
(4.24)
can be interpreted as a sum of a constant factor, a linear-phase factor,
and a nonlinear-phase factor (angles from the “zero vectors” minus the
sum of angles from the “pole vectors”).
4.4.3 MATLAB IMPLEMENTATION
In Chapter 3, we plotted magnitude and phase responses in MATLAB
by directly implementing their functional forms. MATLAB also provides
Im {z}
Unit
circle
Re{z}
0
pk
z
w
l
FIGURE 4.6
Pole and zero vectors
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

System Representation in the z-Domain
121
a function called freqz for this computation, which uses the preceding
interpretation. In its simplest form, this function is invoked by
[H,w] = freqz(b,a,N)
which returns the N-point frequency vector w and the N-point complex
frequency response vector H of the system, given its numerator and de-
nominator coeﬃcients in vectors b and a. The frequency response is eval-
uated at N points equally spaced around the upper half of the unit circle.
Note that the b and a vectors are the same vectors we use in the filter
function or derived from the diﬀerence equation representation (4.19).
The second form
[H,w] = freqz(b,a,N,’whole’)
uses N points around the whole unit circle for computation.
In yet another form,
H = freqz(b,a,w)
it returns the frequency response at frequencies designated in vector w,
normally between 0 and π. It should be noted that the freqz function can
also be used for numerical computation of the DTFT of a finite-duration,
causal sequence x(n). In this approach, b = x and a = 1.
□
EXAMPLE 4.11
Consider a causal system
y(n) = 0.9y(n −1) + x(n)
a. Determine H(z) and sketch its pole-zero plot.
b. Plot |H(ejω)| and ̸ H(ejω).
c. Determine the impulse response h(n).
Solution
The diﬀerence equation can be put in the form
y(n) −0.9y(n −1) = x(n)
a. From (4.21),
H(z) =
1
1 −0.9z−1 ;
|z| > 0.9
since the system is causal. There is one pole at 0.9 and one zero at the origin.
We will use MATLAB to illustrate the use of the zplane function.
>> b = [1, 0]; a = [1, -0.9];
zplane(b,a)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

122
Chapter 4
THE z-TRANSFORM
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Real Part
Imaginary Part
Pole-Zero Plot
0.9
0
FIGURE 4.7
Pole-zero plot of Example 4.11a
Note that we speciﬁed b=[1,0] instead of b=1 because the zplane function
assumes that scalars are zeros or poles. The resulting pole-zero plot is shown
in Figure 4.7.
b. Using (4.23) and (4.24), we can determine the magnitude and phase of
H(ejω). Once again, we will use MATLAB to illustrate the use of the freqz
function. Using its ﬁrst form, we will take 100 points along the upper half of
the unit circle.
MATLAB script:
>> [H,w] = freqz(b,a,100);
magH = abs(H); phaH = angle(H);
>> subplot(2,1,1);plot(w/pi,magH);grid
>> title(’Magnitude Response’); ylabel(’Magnitude’);
>> subplot(2,1,2);plot(w/pi,phaH/pi);grid
>> xlabel(’Frequency in \pi Units’); ylabel(’Phase in \pi Units’);
>> title(’Phase Response’)
The response plots are shown in Figure 4.8. If you study these plots carefully,
you will observe that the plots are computed between 0 ≤ω ≤0.99π and
fall short at ω = π. This is due to the fact that in MATLAB the lower half
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

System Representation in the z-Domain
123
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
15
Magnitude
Magnitude Response
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.4
−0.3
−0.2
−0.1
0
Frequency in π Units
Phase in π Units
 
Phase Response
FIGURE 4.8
Frequency response plots in Example 4.11
of the unit circle begins at ω = π. To overcome this problem, we will use the
second form of the freqz function as follows.
>> [H,w] = freqz(b,a,200,’whole’);
>> magH = abs(H(1:101)); phaH = angle(H(1:101));
Now the 101st element of the array H will correspond to ω = π. A similar
result can be obtained using the third form of the freqz function.
>> w = [0:1:100]*pi/100;
H = freqz(b,a,w);
>> magH = abs(H); phaH = angle(H);
In the future, we will use any one of these forms, depending on our conve-
nience. Also note that in the plots we divided the w and phaH arrays by pi
so that the plot axes are in the units of π and easier to read. This practice
is strongly recommended.
c. From the z-transform in Table 4.1,
h(n) = Z−1 
1
1 −0.9z−1 , |z| > 0.9

= (0.9)nu(n)
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

124
Chapter 4
THE z-TRANSFORM
□
EXAMPLE 4.12
Given that
H(z) =
z + 1
z2 −0.9z + 0.81
is a causal system, ﬁnd
a. its transfer function representation,
b. its diﬀerence equation representation, and
c. its impulse response representation.
Solution
The poles of the system function are at z = 0.9̸
± π/3. Hence the ROC of this
causal system is |z| > 0.9. Therefore, the unit circle is in the ROC, and the
discrete-time Fourier transform H(ejω) exists.
a. Substituting z = ejω in H(z),
H(ejω) =
ejω + 1
ej2ω −0.9ejω + 0.81 =
ejω + 1
(ejω −0.9ejπ/3)(ejω −0.9e−jπ/3)
b. Using H(z) = Y (z)/X(z),
Y (z)
X(z) =
z + 1
z2 −0.9z + 0.81

z−2
z−2

=
z−1 + z−2
1 −0.9z−1 + 0.81z−2
Cross multiplying,
Y (z) −0.9z−1Y (z) + 0.81z−2Y (z) = z−1X(z) + z−2X(z)
Now taking the inverse z-transform,
y(n) −0.9y(n −1) + 0.81y(n −2) = x(n −1) + x(n −2)
or
y(n) = 0.9y(n −1) −0.81y(n −2) + x(n −1) + x(n −2)
c. Using the MATLAB script
>> b = [0,1,1]; a = [1,-0.9,0.81];
[R,p,C] = residuez(b,a)
R =
-0.6173 - 0.9979i
-0.6173 + 0.9979i
p =
0.4500 + 0.7794i
0.4500 - 0.7794i
C =
1.2346
>> Mp = (abs(p))’
Mp =
0.9000
0.9000
>> Ap = (angle(p))’/pi
Ap =
0.3333
-0.3333
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

System Representation in the z-Domain
125
we have
H(z) = 1.2346 + −0.6173 + j0.9979
1 −0.9e−jπ/3z−1 + −0.6173 −j0.9979
1 −0.9ejπ/3z−1
,
|z| > 0.9
Hence from Table 4.1,
h(n) = 1.2346δ(n) + [(−0.6173 + j0.9979)0.9ne−jπn/3
+ (−0.6173 −j0.9979)0.9nejπn/3]u(n)
= 1.2346δ(n) + 0.9n[−1.2346 cos(πn/3) + 1.9958 sin(πn/3)]u(n)
= 0.9n[−1.2346 cos(πn/3) + 1.9958 sin(πn/3)]u(n −1)
The last step results from the fact that h(0) = 0.
□
4.4.4 RELATIONSHIPS BETWEEN SYSTEM REPRESENTATIONS
In this and the previous two chapters, we developed several system rep-
resentations. Figure 4.9 depicts the relationships among these representa-
tions in a graphical form.
Diff Equation
h(n)
H(z)
H(e jw )
Substitute
z = e jw
Express H(z) in z –1,
cross multiply, and
take inverse
Take inverse
z-transform
Take inverse
DTFT
Take Fourier
transform
Take DTFT,
solve for Y/X
Take
z-transform
Take
z-transform,
solve for Y /X
FIGURE 4.9
System representations in pictorial form
4.4.5 STABILITY AND CAUSALITY
For LTI systems, the BIBO stability is equivalent to ∞
−∞|h(k)| < ∞.
From the existence of the discrete-time Fourier transform, this stability
implies that H(ejω) exists, which further implies that the unit circle |z| = 1
must be in the ROC of H(z). This result is called the z-domain stability
theorem; therefore, the dashed paths in Figure 4.9 exist only if the system
is stable.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

126
Chapter 4
THE z-TRANSFORM
THEOREM 2
z-Domain LTI Stability
An LTI system is stable if and only if the unit circle is in the ROC
of H(z).
For LTI causality, we require that h(n) = 0, for n < 0 (i.e., a right-
sided sequence). This implies that the ROC of H(z) must be outside some
circle of radius Rh−. This is not a suﬃcient condition, since any right-
sided sequence has a similar ROC. However, when the system is stable,
then its causality is easy to check.
THEOREM 3
z-Domain Causal LTI Stability
A causal LTI system is stable if and only if the system function
H(z) has all its poles inside the unit circle.
□
EXAMPLE 4.13
A causal LTI system is described by the following diﬀerence equation:
y(n) = 0.81y(n −2) + x(n) −x(n −2)
Determine
a. the system function H(z),
b. the unit impulse response h(n),
c. the unit step response v(n), that is, the response to the unit step u(n), and
d. the frequency response function H(ejω), and plot its magnitude and phase
over 0 ≤ω ≤π.
Solution
Since the system is causal, the ROC will be outside a circle with radius equal
to the largest pole magnitude.
a. Taking the z-transform of both sides of the diﬀerence equation and then
solving for Y (z)/X(z) or using (4.20), we obtain
H(z) =
1 −z−2
1 −0.81z−2 =
1 −z−2
(1 + 0.9z−1) (1 −0.9z−1),
|z| > 0.9
b. Using the MATLAB script for the partial fraction expansion,
>> b = [1,0,-1]; a = [1,0,-0.81]; [R,p,C] = residuez(b,a);
R =
-0.1173
-0.1173
p =
-0.9000
0.9000
C =
1.2346
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

System Representation in the z-Domain
127
we have
H(z) = 1.2346 −0.1173
1
1 + 0.9z−1 −0.1173
1
1 −0.9z−1 , |z| > 0.9
or from Table 4.1,
h(n) = 1.2346δ(n) −0.1173 {1 + (−1)n} (0.9)nu(n)
c. From Table 4.1, Z[u(n)] = U(z) =
1
1 −z−1 , |z| > 1. Hence
V (z) = H(z)U(z)
=

(1 + z−1)(1 −z−1)
(1 + 0.9z−1) (1 −0.9z−1)
 
1
1 −z−1

,
|z| > 0.9 ∩|z| > 1
=
1 + z−1
(1 + 0.9z−1) (1 −0.9z−1),
|z| > 0.9
or
V (z) = 1.0556
1
1 −0.9z−1 −0.0556
1
1 + 0.9z−1 ,
|z| > 0.9
Finally,
v(n) = [1.0556(0.9)n −0.0556 (−0.9)n] u(n)
Note that in the calculation of V (z) there is a pole-zero cancellation at z = 1.
This has two implications. First, the ROC of V (z) is still {|z| > 0.9} and not
{|z| > 0.9 ∩|z| > 1 = |z| > 1}. Second, the step response v(n) contains no
steady-state term u(n).
d. Substituting z = ejω in H(z),
H(ejω) =
1 −e−j2ω
1 −0.81e−j2ω
We will use the MATLAB script to compute and plot responses.
>> w = [0:1:500]*pi/500; H = freqz(b,a,w);
>> magH = abs(H); phaH = angle(H);
>> subplot(2,1,1); plot(w/pi,magH); grid
>> title(’Magnitude Response’); ylabel(’Magnitude’)
>> subplot(2,1,2); plot(w/pi,phaH/pi); grid
>> xlabel(’Frequency in \pi Units’); ylabel(’Phase in \pi Units’)
>> title(’Phase Response’)
The frequency response plots are shown in Figure 4.10.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

128
Chapter 4
THE z-TRANSFORM
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
Magnitude
Magnitude Response
−0.5
0
0.5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Frequency in π Units
 
Phase in π Units
Phase Response
FIGURE 4.10
Frequency response plots for Example 4.13
4.5 SOLUTIONS OF THE DIFFERENCE EQUATIONS
In Chapter 2, we mentioned two forms for the solution of linear constant
coeﬃcient diﬀerence equations. One form involved ﬁnding the particu-
lar and the homogeneous solutions, while the other form involved ﬁnd-
ing the zero-input (initial condition) and the zero-state responses. Using
z-transforms, we now provide a method for obtaining these forms. In ad-
dition, we will also discuss the transient and the steady-state responses.
In digital signal processing, diﬀerence equations generally evolve in the
positive n direction. Therefore, our time frame for these solutions will be
n ≥0. For this purpose, we deﬁne a version of the bilateral z-transform
called the one-sided z-transform.
DEFINITION 4
The One-Sided z-Transform
The one-sided z-transform of a sequence x(n) is given by
Z+[x(n)]
△= Z [x(n)u(n)]
△= X+(z) =
∞
	
n=0
x(n)z−n
(4.25)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solutions of the Diﬀerence Equations
129
Then the sample shifting property is given by
Z+ [x(n −k)] = Z [x(n −k)u(n)]
=
∞
	
n=0
x(n −k)z−n =
∞
	
m=−k
x(m)z−(m+k)
=
−1
	
m=−k
x(m)z−(m+k) +
 ∞
	
m=0
x(m)z−m

z−k
or
Z+ [x(n −k)] = x(−1)z1−k +x(−2)z2−k +· · ·+x(−k)+z−kX+(z)
(4.26)
This result can now be used to solve diﬀerence equations with nonzero
initial conditions or with changing inputs. We want to solve the diﬀerence
equation
y(n) +
N
	
k=1
aky(n −k) =
M
	
m=0
bmx(n −m), n ≥0
subject to these initial conditions:
{y(i), i = −1, . . . , −N}
and
{x(i), i = −1, . . . , −M}
We now demonstrate its solution using an example.
□
EXAMPLE 4.14
Solve
y(n) −3
2y(n −1) + 1
2y(n −2) = x(n),
n ≥0
where
x(n) =
1
4
n
u(n)
subject to y(−1) = 4 and y(−2) = 10.
Solution
Taking the one-sided z-transform of both sides of the diﬀerence equation, we
obtain
Y +(z) −3
2[y(−1) + z−1Y +(z)] + 1
2[y(−2) + z−1y(−1) + z−2Y +(z)] =
1
1 −1
4z−1
Substituting the initial conditions and rearranging,
Y +(z)

1 −3
2z−1 + 1
2z−2
=
1
1 −1
4z−1 + (1 −2z−1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

130
Chapter 4
THE z-TRANSFORM
or
Y +(z) =
1
1 −1
4z−1
1 −3
2z−1 + 1
2z−2 +
1 −2z−1
1 −3
2z−1 + 1
2z−2
(4.27)
Finally,
Y +(z) =
2 −9
4z−1 + 1
2z−2
(1 −1
2z−1)(1 −z−1)(1 −1
4z−1)
Using the partial fraction expansion, we obtain
Y +(z) =
1
1 −1
2z−1 +
2
3
1 −z−1 +
1
3
1 −1
4z−1
(4.28)
After inverse transformation, the solution is
y(n) =
1
2
n
+ 2
3 + 1
3
1
4
n
u(n)
(4.29)
□
Forms of the solutions
The preceding solution is the complete re-
sponse of the diﬀerence equation. It can be expressed in several forms.
• Homogeneous and particular parts:
y(n) =
1
2
n
+ 2
3

u(n)
&
'(
)
Homogeneous part
+ 1
3
1
4
n
u(n)
&
'(
)
Particular part
The homogeneous part is due to the system poles, and the particular
part is due to the input poles.
• Transient and steady-state responses:
y(n) =
1
3
1
4
n
+
1
2
n
u(n)
&
'(
)
Transient response
+
2
3u(n)
& '( )
Steady-state response
The transient response is due to poles that are inside the unit circle,
whereas the steady-state response is due to poles that are on the unit
circle. Note that when the poles are outside the unit circle, the response
is termed an unbounded response.
• Zero-input (or initial condition) and zero-state responses:
In equation (4.27), Y +(z) has two parts. The ﬁrst part can be inter-
preted as
YZS(z) = H(z)X(z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solutions of the Diﬀerence Equations
131
while the second part as
YZI(z) = H(z)XIC(z)
where XIC(z) can be thought of as an equivalent initial-condition in-
put that generates the same output YZI(z) as generated by the initial
conditions. In this example, xIC(n) is
xIC(n) = {1
↑
, −2}
Now taking the inverse z-transform of each part of (4.27), we write the
complete response as
y(n) =
1
3
1
4
n
−2
1
2
n
+ 8
3

u(n)
&
'(
)
Zero-state response
+

3
1
2
n
−2

u(n)
&
'(
)
Zero-input response
From this example, it is clear that each part of the complete solution
is, in general, a diﬀerent function and emphasizes a diﬀerent aspect of
system analysis.
4.5.1 MATLAB IMPLEMENTATION
In Chapter 2, we used the filter function to solve the diﬀerence equation,
given its coeﬃcients and an input. This function can also be used to ﬁnd
the complete response when initial conditions are given. In this form, the
filter function is invoked by
y = filter(b,a,x,xic)
where xic is an equivalent initial-condition input array. To ﬁnd the com-
plete response in Example 4.14, we will use the MATLAB script
>>
n = [0:7]; x = (1/4).ˆn; xic = [1, -2];
>> format long;
y1 = filter(b,a,x,xic)
y1 =
Columns 1 through 4
2.00000000000000
1.25000000000000
0.93750000000000
0.79687500000000
Columns 5 through 8
0.73046875000000
0.69824218750000
0.68237304687500
0.67449951171875
>> y2 = (1/3)*(1/4).ˆn+(1/2).ˆn+(2/3)*ones(1,8) % MATLAB check
y2 =
Columns 1 through 4
2.00000000000000
1.25000000000000
0.93750000000000
0.79687500000000
Columns 5 through 8
0.73046875000000
0.69824218750000
0.68237304687500
0.67449951171875
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

132
Chapter 4
THE z-TRANSFORM
which agrees with the response given in (4.29). In Example 4.14, we com-
puted xIC(n) analytically. However, in practice, and especially for large-
order diﬀerence equations, it is tedious to determine xIC(n) analytically.
MATLAB provides a function called filtic, which is available only in
the Signal Processing toolbox. It is invoked by
xic = filtic(b,a,Y,X)
in which b and a are the ﬁlter coeﬃcient arrays and Y and X are the initial-
condition arrays from the initial conditions on y(n) and x(n), respectively,
in the form
Y = [y(−1), y(−2), . . . , y(−N)]
X = [x(−1), x(−2), . . . , x(−M)]
If x(n) = 0,
n ≤−1, then X need not be speciﬁed in the filtic function.
In Example 4.14, we could have used
>> Y = [4, 10];
xic = filtic(b,a,Y)
xic =
1
-2
to determine xIC(n).
□
EXAMPLE 4.15
Solve the diﬀerence equation
y(n) = 1
3 [x(n) + x(n −1) + x(n −2)] + 0.95y(n −1) −0.9025y(n −2),
n ≥0
where x(n) = cos(πn/3)u(n) and
y(−1) = −2, y(−2) = −3;
x(−1) = 1, x(−2) = 1
First determine the solution analytically and then by using MATLAB.
Solution
Taking a one-sided z-transform of the diﬀerence equation
Y +(z) = 1
3[X+(z) + x(−1) + z−1X+(z) + x(−2) + z−1x(−1) + z−2X+(z)]
+ 0.95[y(−1) + z−1Y +(z)] −0.9025[y(−2) + z−1y(−1) + z−2Y +(z)]
and substituting the initial conditions, we obtain
Y +(z) =
1
3 + 1
3z−1 + 1
3z−2
1 −0.95z−1 + 0.9025z−2 X+(z) +
1.4742 + 2.1383z−1
1 −0.95z−1 + 0.9025z−2
Clearly, xIC(n) = [1.4742, 2.1383]. Now substituting X+(z) =
1 −0.5z−1
1 −z−1 + z−2
and simplifying, we will obtain Y +(z) as a rational function. This simpliﬁcation
and further partial fraction expansion can be done using MATLAB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solutions of the Diﬀerence Equations
133
MATLAB script:
>> b = [1,1,1]/3; a = [1,-0.95,0.9025];
>> Y = [-2,-3]; X = [1,1];
xic=filtic(b,a,Y,X)
xic =
1.4742
2.1383
>> bxplus = [1,-0.5]; axplus = [1,-1,1]; % X(z) transform coeff.
>> ayplus = conv(a,axplus) % Denominator of Yplus(z)
ayplus =
1.0000
-1.9500
2.8525
-1.8525
0.9025
>> byplus = conv(b,bxplus)+conv(xic,axplus)
% Numerator of Yplus(z)
byplus =
1.8075
0.8308
-0.4975
1.9717
>> [R,p,C] = residuez(byplus,ayplus)
R =
0.0584 + 3.9468i
0.0584 - 3.9468i
0.8453 + 2.0311i
0.8453 - 2.0311i
p =
0.5000 - 0.8660i
0.5000 + 0.8660i
0.4750 + 0.8227i
0.4750 - 0.8227i
C =
[]
>> Mp = abs(p), Ap = angle(p)/pi % Polar form
Mp =
1.0000
1.0000
0.9500
0.9500
Ap =
-0.3333
0.3333
0.3333
-0.3333
Hence
Y +(z) =
1.8075 + 0.8308z−1 −0.4975z−2 + 1.9717z−3
1 −1.95z−1 + 2.8525z−2 −1.8525z−3 + 0.9025z−4
= 0.0584 + j3.9468
1 −e−jπ/3z−1
+ 0.0584 −j3.9468
1 −ejπ/3z−1
+ 0.8453 + j2.0311
1 −0.95ejπ/3z−1 + 0.8453 −j2.0311
1 −0.95e−jπ/3z−1
Now from Table 4.1,
y(n) = (0.0584 + j3.9468) e−jπn/3 + (0.0584 −j3.9468) ejπn/3
+ (0.8453 + j2.031) (0.95)n ejπn/3 + (0.8453 −j2.031) (0.95)n e−jπn/3
= 0.1169 cos(πn/3) + 7.8937 sin(πn/3)
+ (0.95)n [1.6906 cos(πn/3) −4.0623 sin(πn/3)] ,
n ≥0
The ﬁrst two terms of y(n) correspond to the steady-state response, as well as
to the particular response, while the last two terms are the transient response
(and homogeneous response) terms.
To solve this example using MATLAB, we will need the filtic function,
which we have already used to determine the xIC(n) sequence. The solution
will be a numerical one. Let us determine the ﬁrst eight samples of y(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

134
Chapter 4
THE z-TRANSFORM
MATLAB script:
>> n = [0:7]; x = cos(pi*n/3);
y = filter(b,a,x,xic)
y =
Columns 1 through 4
1.80750000000000
4.35545833333333
2.83975000000000
-1.56637197916667
Columns 5 through 8
-4.71759442187500
-3.40139732291667
1.35963484230469
5.02808085078841
% MATLAB verification
>> A=real(2*R(1)); B=imag(2*R(1)); C=real(2*R(3)); D=imag(2*R(4));
>> y=A*cos(pi*n/3)+B*sin(pi*n/3)+((0.95).ˆn).*(C*cos(pi*n/3)+D*sin(pi*n/3))
y =
Columns 1 through 4
1.80750000000048
4.35545833333359
2.83974999999978
-1.56637197916714
Columns 5 through 8
-4.71759442187528
-3.40139732291648
1.35963484230515
5.02808085078871
□
4.6 PROBLEMS
P4.1
Determine the z-transform of the following sequences using the deﬁnition (4.1). Indicate the
region of convergence for each sequence and verify the z-transform expression using
MATLAB.
1. x(n) = {3, 2, 1
↑
, −2, −3}.
2. x(n) = (0.8)nu(n −2). Verify the z-transform expression using MATLAB.
3. x(n) = [(0.5)n + (−0.8)n]u(n). Verify the z-transform expression using MATLAB.
4. x(n) = 2n cos(0.4πn)u(−n).
5. x(n) = (n + 1)(3)nu(n). Verify the z-transform expression using MATLAB.
P4.2
Consider the sequence x(n) = (0.9)n cos(πn/4)u(n). Let
y(n) =

x(n/2), n = 0, ±2, ±4, · · ·
0,
otherwise
1. Show that the z-transform Y (z) of y(n) can be expressed in terms of the z-transform
X(z) of x(n) as Y (z) = X(z2).
2. Determine Y (z).
3. Using MATLAB, verify that the sequence y(n) has the z-transform Y (z).
P4.3
Determine the z-transform of the following sequences using the z-transform table and the
z-transform properties. Express X(z) as a rational function in z−1. Verify your results using
MATLAB. Indicate the region of convergence in each case, and provide a pole-zero plot.
1. x(n) = 2δ(n −2) + 3u(n −3)
2. x(n) = 3(0.75)n cos(0.3πn)u(n) + 4(0.75)n sin(0.3πn)u(n)
3. x(n) = n sin( πn
3 )u(n) + (0.9)nu(n −2)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
135
4. x(n) = n2(2/3)n−2u(n −1)
5. x(n) = (n −3)( 1
4)n−2 cos{ π
2 (n −1)}u(n)
P4.4
Let x(n) be a complex-valued sequence with the real part xR(n) and the imaginary part
xI(n).
1. Prove the following z-transform relations:
XR(z)
△= Z [xR(n)] = X(z) + X∗(z∗)
2
and
XI(z)
△= Z [xI(n)] = X(z) −X∗(z∗)
2
2. Verify these relations for x(n) = exp {(−1 + j0.2π)n} u(n).
P4.5
The z-transform of x(n) is X(z) = 1/(1 + 0.5z−1), |z| > 0.5. Determine the z-transforms of
the following sequences and indicate their region of convergence.
1. x1(n) = x(3 −n) + x(n −3)
2. x2(n) = (1 + n + n2)x(n)
3. x3(n) = ( 1
2)nx(n −2)
4. x4(n) = x(n + 2) ∗x(n −2)
5. x5(n) = cos(πn/2)x∗(n)
P4.6
Repeat Problem P4.5 if
X(z) =
1 + z−1
1 + 5
6z−1 + 1
6z−2 ; |z| > 1
2
P4.7
The inverse z-transform of X(z) is x(n) = (1/2)nu(n). Using the z-transform properties,
determine the sequences in each of the following cases.
1. X1(z) = z−1
z X(z)
2. X2(z) = zX(z−1)
3. X3(z) = 2X(3z) + 3X(z/3)
4. X4(z) = X(z)X(z−1)
5. X5(z) = z2 dX(z)
dz
P4.8
If sequences x1(n), x2(n), and x3(n) are related by x3(n) = x1(n) ∗x2(n), then
∞
	
n=−∞
x3(n) =
-
∞
	
n=−∞
x1(n)
. -
∞
	
n=−∞
x2(n)
.
1. Prove this result by substituting the deﬁnition of convolution in the left-hand side.
2. Prove this result using the convolution property.
3. Verify this result using MATLAB and choosing any two random sequences x1(n) and
x2(n).
P4.9
Determine the results of the following polynomial operations using MATLAB.
1. X1(z) = (1 −2z−1 + 3z−2 −4z−3)(4 + 3z−1 −2z−2 + z−3)
2. X2(z) = (z2 −2z + 3 + 2z−1 + z−2)(z3 −z−3)
3. X3(z) = (1 + z−1 + z−2)3
4. X4(z) = X1(z)X2(z) + X3(z)
5. X5(z) = (z−1 −3z−3 + 2z−5 + 5z−7 −z−9)(z + 3z2 + 2z3 + 4z4)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

136
Chapter 4
THE z-TRANSFORM
P4.10 The deconv function is useful in dividing two causal sequences. Write a MATLAB function
deconv m to divide two noncausal sequences (similar to the conv function). The format of
this function should be
function [p,np,r,nr] = deconv_m(b,nb,a,na)
% Modified deconvolution routine for noncausal sequences
% function [p,np,r,nr] = deconv_m(b,nb,a,na)
%
%
p = polynomial part of support np1 <= n <= np2
% np = [np1, np2]
%
r = remainder part of support nr1 <= n <= nr2
% nr = [nr1, nr2]
%
b = numerator polynomial of support nb1 <= n <= nb2
% nb = [nb1, nb2]
%
a = denominator polynomial of support na1 <= n <= na2
% na = [na1, na2]
%
Check your function on the following operation:
z2 + z + 1 + z−1 + z−2 + z−3
z + 2 + z−1
= (z −1 + 2z−1 −2z−2) + 3z−2 + 3z−3
z + 2 + z−1
P4.11 Determine the following inverse z-transforms using the partial fraction expansion method.
1. X1(z) = (1 −z−1 −4z−2 + 4z−3)/(1 −11
4 z−1 + 13
8 z−2 −1
4z−3). The sequence is
right-sided.
2. X2(z) = (1 + z−1 −4z−2 + 4z−3)/(1 −11
4 z−1 + 13
8 z−2 −1
4z−3). The sequence is
absolutely summable.
3. X3(z) = (z3 −3z2 + 4z + 1)/(z3 −4z2 + z −0.16). The sequence is left-sided.
4. X4(z) = z/(z3 + 2z2 + 1.25z + 0.25), |z| > 1.
5. X5(z) = z/(z2 −0.25)2, |z| < 0.5.
P4.12 Consider the sequence
x(n) = Ac(r)n cos(πv0n)u(n) + As(r)n sin(πv0n)u(n)
(4.30)
The z-transform of this sequence is a second-order (proper) rational function that contains
a complex-conjugate pole pair. The objective of this problem is to develop a MATLAB
function that can be used to obtain the inverse z-transform of such a rational function so
that the inverse does not contain any complex numbers.
1. Show that the z-transform of x(n) in (4.30) is given by
X(z) =
b0 + b1z−1
1 + a1z−1 + a2z−2 ;
|z| > |r|
(4.31)
where
b0 = Ac; b1 = r[As sin(πv0) −Ac cos(πv0)]; a1 = −2r cos(πv0); a2 = r2
(4.32)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
137
2. Using (4.32), determine the signal parameters Ac, As, r, and v0 in terms of the rational
function parameters b0, b1, a1, and a2.
3. Using your results in part b above, design a MATLAB function, invCCPP, that computes
signal parameters using the rational function parameters. The format of this function
should be
function [As,Ac,r,v0] = invCCPP(b0,b1,a1,a2)
P4.13 Suppose X(z) is given as follows:
X(z) =
2 + 3z−1
1 −z−1 + 0.81z−2 , |z| > 0.9
1. Using the MATLAB function invCCPP given in Problem P4.12, determine x(n) in a form
that contains no complex numbers.
2. Using MATLAB, compute the ﬁrst 20 samples of x(n), and compare them with your
answer in the above part.
P4.14 The z-transform of a causal sequence is given as
X(z) =
−2 + 5.65z−1 −2.88z−2
1 −0.1z−1 + 0.09z−2 + 0.648z−3
(4.33)
which contains a complex-conjugate pole pair as well as a real-valued pole.
1. Using the residuez function, express (4.33) as
X(z) =
(
) + (
)z−1
1 + (
)z−1 + (
)z−2 +
(
)
1 + (
)z−1
(4.34)
Note that you will have to use the residuez function in both directions.
2. Now using your function invCCPP and the inverse of the real-valued pole factor,
determine the causal sequence x(n) from the X(z) in (4.34) so that it contains no
complex numbers.
P4.15 For the linear and time-invariant systems described by the following impulse responses,
determine (i) the system function representation, (ii) the diﬀerence equation representation,
(iii) the pole-zero plot, and (iv) the output y(n) if the input is x(n) =  1
4
n u(n).
1. h(n) = 5(1/4)nu(n)
2. h(n) = n(1/3)nu(n) + (−1/4)nu(n)
3. h(n) = 3(0.9)n cos(πn/4 + π/3)u(n + 1)
4. h(n) = (0.5)n sin[(n + 1)π/3]
sin(π/3)
u(n)
5. h(n) = [2 −sin(πn)]u(n)
P4.16 Consider the system shown in Figure P4.1.
1. Using the z-transform approach, show that the impulse response, h(n), of the overall
system is given by
h(n) = δ(n) −1
2δ(n −1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

138
Chapter 4
THE z-TRANSFORM
FIGURE P4.1
System for Problem 4.16
2. Determine the diﬀerence equation representation of the overall system that relates the
output y(n) to the input x(n).
3. Is this system causal? BIBO stable? Explain clearly to receive full credit.
4. Determine the frequency response H(ejω) of the overall system.
5. Using MATLAB, provide a plot of this frequency response over 0 ≤ω ≤π.
P4.17 For the linear and time-invariant systems described by the following system functions,
determine (i) the impulse response representation, (ii) the diﬀerence equation
representation, (iii) the pole-zero plot, and (iv) the output y(n) if the input is
x(n) = 3 cos(πn/3)u(n).
1. H(z) = (z + 1)/(z −0.5), causal system
2. H(z) = (1 + z−1 + z−2)/(1 + 0.5z−1 −0.25z−2), stable system
3. H(z) = (z2 −1)/(z −3)2, anticausal system
4. H(z) =
z
z −0.25 + 1 −0.5z−1
1 + 2z−1 , stable system
5. H(z) = (1 + z−1 + z−2)2
P4.18 For the linear, causal, and time-invariant systems described by the following diﬀerence
equations, determine (i) the impulse response representation, (ii) the system function
representation, (iii) the pole-zero plot, and (iv) the output y(n) if the input is
x(n) = 2(0.9)nu(n).
1. y(n) = [x(n) + 2x(n −1) + x(n −3)] /4
2. y(n) = x(n) + 0.5x(n −1) −0.5y(n −1) + 0.25y(n −2)
3. y(n) = 2x(n) + 0.9y(n −1)
4. y(n) = −0.45x(n) −0.4x(n −1) + x(n −2) + 0.4y(n −1) + 0.45y(n −2)
5. y(n) = 4
m=0(0.8)mx(n −m) −4
ℓ=1(0.9)ℓy(n −ℓ)
P4.19 The output sequence y(n) in Problem P4.18 is the total response. For each of the systems
given in Problem P4.18, separate y(n) into (i) the homogeneous part, (ii) the particular
part, (iii) the transient response, and (iv) the steady-state response.
P4.20 A stable system has four zeros and four poles as given below.
zeros: ± 1, ±j1
Poles: ± 0.9, ±j0.9
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
139
It is also known that the frequency response function H(ejω) evaluated at ω = π/4 is equal
to 1, that is,
H(ejπ/4) = 1
1. Determine the system function H(z), and indicate its region of convergence.
2. Determine the diﬀerence equation representation.
3. Determine the steady-state response yss(n) if the input is x(n) = cos(πn/4)u(n).
4. Determine the transient response ytr(n) if the input is x(n) = cos(πn/4)u(n).
P4.21 A digital ﬁlter is described by the frequency response function
H(ejω) = [1 + 2 cos(ω) + 3 cos(2ω)] cos
ω
2

e−j5ω/2
1. Determine the diﬀerence equation representation.
2. Using the freqz function, plot the magnitude and phase of the frequency response of the
ﬁlter. Note the magnitude and phase at ω = π/2 and at ω = π.
3. Generate 200 samples of the signal x(n) = sin(πn/2) + 5 cos(πn), and process through
the ﬁlter to obtain y(n). Compare the steady-state portion of y(n) to x(n). How are the
amplitudes and phases of two sinusoids aﬀected by the ﬁlter?
P4.22 Repeat Problem 4.21 for the following ﬁlter:
H(ejω) =
1 + e−j4ω
1 −0.8145e−j4ω
P4.23 Solve the following diﬀerence equation for y(n) using the one-sided z-transform approach:
y(n) = 0.81y(n −2) + x(n) −x(n −1), n ≥0;
y(−1) = 2, y(−2) = 2
x(n) = (0.7)nu(n + 1)
Generate the ﬁrst 20 samples of y(n) using MATLAB, and compare them with your answer.
P4.24 Solve the diﬀerence equation for y(n), n ≥0,
y(n) −0.4y(n −1) −0.45y(n −2) = 0.45x(n) + 0.4x(n −1) −x(n −2)
driven by the input x(n) = 
2 +  1
2
n
u(n) and subject to
y(−1) = 0, y(−2) = 3; x(−1) = x(−2) = 2
Decompose the solution y(n) into (i) transient response, (ii) steady-state response,
(iii) zero-input response, and (iv) zero-state response.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

140
Chapter 4
THE z-TRANSFORM
P4.25 A stable, linear, and time-invariant system is given by the system function
H(z) = 4z2 −2
√
2z + 1
z2 −2
√
2z + 4
1. Determine the diﬀerence equation representation for this system.
2. Plot the poles and zeros of H(z), and indicate the ROC.
3. Determine the unit sample response h(n) of this system.
4. Is this system causal? If the answer is yes, justify it. If the answer is no, ﬁnd a causal
unit sample response that satisﬁes the system function.
P4.26 Determine the zero-input, zero-state, and steady-state responses of the system
y(n) = 0.9801y(n −2) + x(n) + 2x(n −1) + x(n −2), n ≥0;
y(−2) = 1, y(−1) = 0
to the input x(n) = 5(−1)nu(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

141
C H A P T E R 5
The Discrete
Fourier Transform
In Chapters 3 and 4, we studied transform-domain representations of dis-
crete signals. The discrete-time Fourier transform provided the frequency-
domain (ω) representation for absolutely summable sequences. The
z-transform provided a generalized frequency-domain (z) representation
for arbitrary sequences. These transforms have two features in common.
First, the transforms are deﬁned for inﬁnite-length sequences. Second, and
the most important, they are functions of continuous variables (ω or z).
From the numerical computation viewpoint (or from MATLAB’s view-
point), these two features are troublesome because one has to evaluate
infinite sums at uncountably infinite frequencies. To use MATLAB, we
have to truncate sequences and then evaluate the expressions at ﬁnitely
many points. This is what we did in many examples in the two previous
chapters. The evaluations were obviously approximations to the exact
calculations. In other words, the discrete-time Fourier transform and the
z-transform are not numerically computable transforms.
Therefore, we turn our attention to a numerically computable trans-
form. It is obtained by sampling the discrete-time Fourier transform in the
frequency domain (or the z-transform on the unit circle). We develop this
transform by ﬁrst analyzing periodic sequences. From Fourier analysis, we
know that a periodic function (or sequence) can always be represented by
a linear combination of harmonically related complex exponentials (which
is a form of sampling). This gives us the discrete Fourier series (DFS) rep-
resentation. Since the sampling is in the frequency domain, we study the
eﬀects of sampling in the time domain and the issue of reconstruction in
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

142
Chapter 5
THE DISCRETE FOURIER TRANSFORM
the z-domain. We then extend the DFS to finite-duration sequences, which
leads to a new transform, called the discrete Fourier transform (DFT).
The DFT avoids the two problems mentioned and is a numerically com-
putable transform that is suitable for computer implementation. We study
its properties and its use in system analysis in detail. The numerical com-
putation of the DFT for long sequences is prohibitively time-consuming.
Therefore, several algorithms have been developed to eﬃciently compute
the DFT. These are collectively called fast Fourier transform (or FFT)
algorithms. We will study two such algorithms in detail.
5.1 THE DISCRETE FOURIER SERIES
In Chapter 2, we deﬁned the periodic sequence by ˜x(n), satisfying the
condition
˜x(n) = ˜x(n + kN),
∀n, k
(5.1)
where N is the fundamental period of the sequence. From Fourier analysis,
we know that the periodic functions can be synthesized as a linear com-
bination of complex exponentials whose frequencies are multiples (or har-
monics) of the fundamental frequency (which in our case is 2π/N). From
the frequency-domain periodicity of the discrete-time Fourier transform,
we conclude that there are a ﬁnite number of harmonics; the frequencies
are { 2π
N k,
k = 0, 1, . . . , N −1}. Therefore, a periodic sequence ˜x(n) can
be expressed as
˜x(n) = 1
N
N−1
	
k=0
˜X(k)ej 2π
N kn,
n = 0, ±1, . . .
(5.2)
where { ˜X(k),
k = 0, ±1, . . . , } are called the discrete Fourier series co-
eﬃcients, which are given by
˜X(k) =
N−1
	
n=0
˜x(n)e−j 2π
N nk,
k = 0, ±1, . . .
(5.3)
Note that ˜X(k) is itself a (complex-valued) periodic sequence with fun-
damental period equal to N, that is,
˜X(k + N) = ˜X(k)
(5.4)
The pair of equations (5.3) and (5.2), taken together, is called the discrete
Fourier series representation of periodic sequences. Using WN
△= e−j 2π
N to
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Series
143
denote the complex exponential term, we express (5.3) and (5.2) as
˜X(k)
△= DFS[˜x(n)] =
N−1

n=0
˜x(n)W nk
N
Analysis or a
DFS equation
˜x(n)
△= IDFS[ ˜X(k)] = 1
N
N−1

k=0
˜X(k)W −nk
N
Synthesis or an inverse
DFS equation
(5.5)
□
EXAMPLE 5.1
Find the DFS representation of the periodic sequence
˜x(n) = {. . . , 0, 1, 2, 3, 0
↑
, 1, 2, 3, 0, 1, 2, 3, . . .}
Solution
The fundamental period of this sequence is N = 4. Hence W4 = e−j 2π
4
=
−j. Now
˜
X(k) =
3

n=0
˜x(n)W nk
4 ,
k = 0, ±1, ±2, . . .
Hence
˜
X(0) =
3

0
˜x(n)W 0·n
4
=
3

0
˜x(n) = ˜x(0) + ˜x(1) + ˜x(2) + ˜x(3) = 6
Similarly,
˜
X(1) =
3

0
˜x(n)W n
4 =
3

0
˜x(n)(−j)n = (−2 + 2j)
˜
X(2) =
3

0
˜x(n)W 2n
4
=
3

0
˜x(n)(−j)2n = −2
˜
X(3) =
3

0
˜x(n)W 3n
4
=
3

0
˜x(n)(−j)3n = (−2 −2j)
□
5.1.1 MATLAB IMPLEMENTATION
A careful look at (5.5) reveals that the DFS is a numerically computable
representation. It can be implemented in many ways. To compute each
sample ˜X(k), we can implement the summation as a for...end loop.
To compute all DFS coeﬃcients would require another for...end loop.
This will result in a two nested for...end loop implementation. This is
clearly ineﬃcient in MATLAB. An eﬃcient implementation in MATLAB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

144
Chapter 5
THE DISCRETE FOURIER TRANSFORM
would be to use a matrix-vector multiplication for each of the relations
in (5.5). We have used this approach earlier in implementing a numerical
approximation to the discrete-time Fourier transform. Let ˜x and ˜X denote
column vectors corresponding to the primary periods of sequences ˜x(n)
and ˜X(k), respectively. Then (5.5) is given by
˜X = WN˜x
˜x = 1
N W∗
N ˜X
(5.6)
where the matrix WN is given by
WN
△=

W kn
N
0≤k,n≤N−1

= k
↓
n −→
⎡
⎢⎢⎢⎢⎢⎣
1
1
· · ·
1
1
W 1
N
· · · W (N−1)
N
...
...
...
...
1 W (N−1)
N
· · · W (N−1)2
N
⎤
⎥⎥⎥⎥⎥⎦
(5.7)
The matrix WN is a square matrix and is called a DFS matrix. The
following MATLAB function dfs implements this procedure.
function [Xk] = dfs(xn,N)
% Computes discrete Fourier series coefficients
% ---------------------------------------------
% [Xk] = dfs(xn,N)
% Xk = DFS coeff. array over 0 <= k <= N-1
% xn = One period of periodic signal over 0 <= n <= N-1
%
N = Fundamental period of xn
%
n = [0:1:N-1];
% Row vector for n
k = [0:1:N-1];
% Row vector for k
WN = exp(-j*2*pi/N);
% Wn factor
nk = n’*k;
% Creates an N by N matrix of nk values
WNnk = WN .ˆ nk;
% DFS matrix
Xk = xn * WNnk;
% Row vector for DFS coefficients
The DFS in Example 5.1 can be computed using MATLAB as
>> xn = [0,1,2,3]; N = 4;
Xk = dfs(xn,N)
Xk =
6.0000
-2.0000 + 2.0000i
-2.0000 - 0.0000i
-2.0000 - 2.0000i
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Series
145
The following idfs function implements the synthesis equation.
function [xn] = idfs(Xk,N)
% Computes inverse discrete Fourier series
% ----------------------------------------
% [xn] = idfs(Xk,N)
% xn = one period of periodic signal over 0 <= n <= N-1
% Xk = DFS coeff. array over 0 <= k <= N-1
%
N = fundamental period of Xk
%
n = [0:1:N-1];
% Row vector for n
k = [0:1:N-1];
% Row vector for k
WN = exp(-j*2*pi/N);
% Wn factor
nk = n’*k;
% Creates an N by N matrix of nk values
WNnk = WN .ˆ (-nk);
% IDFS matrix
xn = (Xk * WNnk)/N;
% Row vector for IDFS values
Caution:
These functions are eﬃcient approaches of implementing (5.5)
in MATLAB. They are not computationally eﬃcient, especially for large
N. We will deal with this problem later in this chapter.
□
EXAMPLE 5.2
A periodic “square wave” sequence is given by
˜x(n) =

1,
mN ≤n ≤mN + L −1
0, mN + L ≤n ≤(m + 1) N −1 ;
m = 0, ±1, ±2, . . .
where N is the fundamental period and L/N is the duty cycle.
a. Determine an expression for | ˜
X(k)| in terms of L and N.
b. Plot the magnitude | ˜
X(k)| for L = 5, N = 20; L = 5, N = 40; L = 5,
N = 60; and L = 7, N = 60.
c. Comment on the results.
Solution
A plot of this sequence for L = 5 and N = 20 is shown in Figure 5.1.
a. By applying the analysis equation (5.3),
˜
X(k) =
N−1

n=0
˜x(n)e−j 2π
N nk =
L−1

n=0
e−j 2π
N nk =
L−1

n=0

e−j 2π
N kn
=
⎧
⎪
⎨
⎪
⎩
L,
k = 0, ±N, ±2N, . . .
1 −e−j2πLk/N
1 −e−j2πk/N , otherwise
The last step follows from the sum of the geometric terms formula (2.7) in
Chapter 2. The last expression can be simpliﬁed to
1 −e−j2πLk/N
1 −e−j2πk/N
= e−jπLk/N
e−jπk/N
ejπLk/N −e−jπLk/N
ejπk/N −e−jπk/N
= e−jπ(L−1)k/N sin (πkL/N)
sin (πk/N)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

146
Chapter 5
THE DISCRETE FOURIER TRANSFORM
−20
−10
0
10
20
30
40
−0.5
0
0.5
1
1.5
n
Amplitude
Three Periods of xtilde(n)
FIGURE 5.1
Periodic square wave sequence
or the magnitude of ˜
X(k) is given by
 ˜
X(k)
 =
⎧
⎪
⎨
⎪
⎩
L,
k = 0, ±N, ±2N, . . .

sin (πkL/N)
sin (πk/N)
 , otherwise
b. The MATLAB script for L = 5 and N = 20:
>> L = 5; N = 20; k = [-N/2:N/2];
% Sq wave parameters
>> xn = [ones(1,L), zeros(1,N-L)];
% Sq wave x(n)
>> Xk = dfs(xn,N);
% DFS
>> magXk = abs([Xk(N/2+1:N) Xk(1:N/2+1)]); % DFS magnitude
>> subplot(2,2,1); stem(k,magXk); axis([-N/2,N/2,-0.5,5.5])
>> xlabel(’k’); ylabel(’Amplitude’)
>> title(’DFS of SQ. Wave: L=5, N=20’)
The plots for this and all other cases are shown in Figure 5.2. Note that
since ˜
X(k) is periodic, the plots are shown from −N/2 to N/2.
c. Several interesting observations can be made from plots in Figure 5.2. The
envelopes of the DFS coeﬃcients of square waves look like “sinc” functions.
The amplitude at k = 0 is equal to L, while the zeros of the functions are
at multiples of N/L, which is the reciprocal of the duty cycle. We will study
these functions later in this chapter.
□
5.1.2 RELATION TO THE z-TRANSFORM
Let x(n) be a ﬁnite-duration sequence of duration N such that
x(n) =

Nonzero, 0 ≤n ≤N −1
0,
elsewhere
(5.8)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Series
147
−10
−5
0
5
10
0
2
4
DFS of SQ. Wave: L=5, N=20
Amplitude
−20
−10
0
10
20
0
2
4
DFS of SQ. Wave: L=5, N=40
Amplitude
−20
0
20
0
2
4
k
Amplitude
DFS of SQ. Wave: L=5, N=60
−20
0
20
0
2
4
6
k
Amplitude
DFS of SQ. Wave: L=7, N=60
FIGURE 5.2
The DFS plots of a periodic square wave for various L and N
Then we can compute its z-transform:
X(z) =
N−1
	
n=0
x(n)z−n
(5.9)
Now we construct a periodic sequence ˜x(n) by periodically repeating x(n)
with period N, that is,
x(n) =

˜x(n), 0 ≤n ≤N −1
0,
elsewhere
(5.10)
The DFS of ˜x(n) is given by
˜X(k) =
N−1
	
n=0
˜x(n)e−j 2π
N nk =
N−1
	
n=0
x(n)

ej 2π
N k−n
(5.11)
Comparing it with (5.9), we have
˜X(k) = X(z)|z=ej 2π
N k
(5.12)
which means that the DFS ˜X(k) represents N evenly spaced samples of
the z-transform X(z) around the unit circle.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

148
Chapter 5
THE DISCRETE FOURIER TRANSFORM
5.1.3 RELATION TO THE DTFT
Since x(n) in (5.8) is of ﬁnite duration of length N, it is also absolutely
summable. Hence its DTFT exists and is given by
X(ejω) =
N−1
	
n=0
x(n)e−jωn =
N−1
	
n=0
˜x(n)e−jωn
(5.13)
Comparing (5.13) with (5.11), we have
˜X(k) = X(ejω)

ω= 2π
N k
(5.14)
Let
ω1
△= 2π
N
and
ωk
△= 2π
N k = kω1
Then the DFS X(k) = X(ejωk) = X(ejkω1), which means that the DFS
is obtained by evenly sampling the DTFT at ω1 =
2π
N intervals. From
(5.12) and (5.14), we observe that the DFS representation gives us a
sampling mechanism in the frequency domain that, in principle, is similar
to sampling in the time domain. The interval ω1 = 2π
N is the sampling
interval in the frequency domain. It is also called the frequency resolution
because it tells us how close the frequency samples (or measurements)
are.
□
EXAMPLE 5.3
Let x(n) = {0
↑
, 1, 2, 3}.
a. Compute its discrete-time Fourier transform X(ejω).
b. Sample X(ejω) at kω1 =
2π
4 k,
k = 0, 1, 2, 3 and show that it is equal to
˜
X(k) in Example 5.1.
Solution
The sequence x(n) is not periodic but is of ﬁnite duration.
a. The discrete-time Fourier transform is given by
X(ejω) =
∞

n=−∞
x(n)e−jωn = e−jω + 2e−j2ω + 3e−j3ω
b. Sampling at kω1 = 2π
4 k,
k = 0, 1, 2, 3, we obtain
X(ej0) = 1 + 2 + 3 = 6 = ˜
X(0)
X(ej2π/4) = e−j2π/4 + 2e−j4π/4 + 3e−j6π/4 = −2 + 2j = ˜
X(1)
X(ej4π/4) = e−j4π/4 + 2e−j8π/4 + 3e−j12π/4 = 2 = ˜
X(2)
X(ej6π/4) = e−j6π/4 + 2e−j12π/4 + 3e−j18π/4 = −2 −2j = ˜
X(3)
as expected.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction in the z-Domain
149
5.2 SAMPLING AND RECONSTRUCTION IN THE z-DOMAIN
Let x(n) be an arbitrary absolutely summable sequence, which may be of
inﬁnite duration. Its z-transform is given by
X(z) =
∞
	
m=−∞
x(m)z−m
and we assume that the ROC of X (z) includes the unit circle. We sample
X(z) on the unit circle at equispaced points separated in angle by ω1 =
2π/N and call it a DFS sequence,
˜X(k)
△= X(z)|z=ej 2π
N k,
k = 0, ±1, ±2, . . .
=
∞
	
m=−∞
x(m)e−j 2π
N km =
∞
	
m=−∞
x(m)W km
N
(5.15)
which is periodic with period N. Finally, we compute the IDFS of ˜X(k),
˜x(n) = IDFS
 ˜X(k)

which is also periodic with period N. Clearly, there must be a relationship
between the arbitrary x(n) and the periodic ˜x(n). This is an important
issue. In order to compute the inverse DTFT or the inverse z-transform
numerically, we must deal with a ﬁnite number of samples of X(z) around
the unit circle. Therefore, we must know the eﬀect of such sampling on
the time-domain sequence. This relationship is easy to obtain.
˜x(n) = 1
N
N−1
	
k=0
˜X(k)W −kn
N
[from (5.2)]
= 1
N
N−1
	
k=0

∞
	
m=−∞
x(m)W km
N

W −kn
N
[from (5.15)]
or
˜x(n) =
∞
	
m=−∞
x(m)
1
N
N−1
	
0
W −k(n−m)
N
&
'(
)
=
⎧
⎨
⎩
1,
n −m = rN
0,
elsewhere
=
=
∞
	
r=−∞
∞
	
m=−∞
x(m)δ(n −m −rN)
∞
	
m=−∞
x(m)
∞
	
r=−∞
δ(n−m−rN)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

150
Chapter 5
THE DISCRETE FOURIER TRANSFORM
or
˜x(n) =
∞
	
r=−∞
x(n −rN) = · · · + x(n + N) + x(n) + x(n −N) + · · · (5.16)
which means that when we sample X(z) on the unit circle, we obtain a
periodic sequence in the time domain. This sequence is a linear combina-
tion of the original x(n) and its inﬁnite replicas, each shifted by multiples
of ±N. This is illustrated in Example 5.5. From (5.16), we observe that if
x(n) = 0 for n < 0 and n ≥N, then there will be no overlap or aliasing in
the time domain. Hence we should be able to recognize and recover x(n)
from ˜x(n), that is,
x(n) = ˜x(n) for 0 ≤n ≤(N −1)
or
x(n) = ˜x(n)RN(n) =

˜x(n), 0 ≤n ≤N −1
0,
elsewhere
where RN(n) is called a rectangular window of length N. Therefore, we
have the following theorem.
THEOREM 1
Frequency Sampling
If x(n) is time-limited (i.e., of finite duration) to [0, N −1], then
N samples of X(z) on the unit circle determine X(z) for all z.
□
EXAMPLE 5.4
Let x1(n) = {6
↑
, 5, 4, 3, 2, 1}. Its DTFT X1(ejω) is sampled at
ωk = 2πk
4 ,
k = 0, ±1, ±2, ±3, . . .
to obtain a DFS sequence ˜
X2(k). Determine the sequence ˜x2(n), which is the
inverse DFS of ˜
X2(k).
Solution
Without computing the DTFT, the DFS, or the inverse DFS, we can evaluate
˜x2(n) by using the aliasing formula (5.16).
˜x2(n) =
∞

r=−∞
x1(n −4r)
Thus x(4) is aliased into x(0), and x(5) is aliased into x(1). Hence
˜x2(n) = {. . . , 8, 6, 4, 3, 8
↑
, 6, 4, 3, 8, 6, 4, 3, . . .}
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction in the z-Domain
151
□
EXAMPLE 5.5
Let x(n) = (0.7)n u(n). Sample its z-transform on the unit circle with N = 5,
10, 20, 50 and study its eﬀect in the time domain.
Solution
From Table 4.1, the z-transform of x(n) is
X(z) =
1
1 −0.7z−1 =
z
z −0.7,
|z| > 0.7
We can now use MATLAB to implement the sampling operation
˜
X(k) = X(z)|z=ej2πk/N ,
k = 0, ±1, ±2, . . .
and the inverse DFS computation to determine the corresponding time-domain
sequence. The MATLAB script for N = 5 is as follows.
>> N = 5; k = 0:1:N-1;
% Sample index
>> wk = 2*pi*k/N; zk = exp(j*wk);
% Samples of z
>> Xk = (zk)./(zk-0.7);
% DFS as samples of X(z)
>> xn = real(idfs(Xk,N));
% IDFS
>> xtilde = xn’* ones(1,8); xtilde = (xtilde(:))’; % Periodic sequence
>> subplot(2,2,1); stem(0:39,xtilde);axis([0,40,-0.1,1.5])
>> ylabel(’Amplitude’); title(’N=5’)
The plots in Figure 5.3 clearly demonstrate the aliasing in the time domain,
especially for N = 5 and N = 10. For large values of N, the tail end of x(n)
0
10
20
30
40
0
0.5
1
1.5
Amplitude
N=5
0
10
20
30
40
0
0.5
1
1.5
Amplitude
N=10
0
10
20
30
40
0
0.5
1
1.5
n
Amplitude
N=20
0
10
20
30
40
0
0.5
1
1.5
n
Amplitude
N=40
FIGURE 5.3
Plots in Example 5.5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

152
Chapter 5
THE DISCRETE FOURIER TRANSFORM
is suﬃciently small to result in any appreciable amount of aliasing in practice.
Such information is useful in eﬀectively truncating an inﬁnite-duration sequence
prior to taking its transform.
□
5.2.1 THE z-TRANSFORM RECONSTRUCTION FORMULA
Let x(n) be time-limited to [0, N −1]. Then from Theorem 1 we should
be able to recover the z-transform X(z) using its samples ˜X(k). This is
given by
X(z) = Z [x(n)] = Z [˜x(n)RN(n)]
= Z[ IDFS{
˜X(k)
& '( )
Samples of X(z)
}RN(n)]
This approach results in the z-domain reconstruction formula:
X(z) =
N−1
	
0
x(n)z−n =
N−1
	
0
˜x(n)z−n
=
N−1
	
0

1
N
N−1
	
0
˜X(k)W −kn
N

z−n
= 1
N
N−1
	
k=0
˜X(k)
N−1
	
0
W −kn
N
z−n

= 1
N
N−1
	
k=0
˜X(k)
N−1
	
0

W −k
N z−1n

= 1
N
N−1
	
k=0
˜X(k)

1 −W −kN
N
z−N
1 −W −k
N z−1

Since W −kN
N
= 1, we have
X(z) = 1 −z−N
N
N−1
	
k=0
˜X(k)
1 −W −k
N z−1
(5.17)
5.2.2 THE DTFT INTERPOLATION FORMULA
The reconstruction formula (5.17) can be specialized for the discrete-time
Fourier transform by evaluating it on the unit circle z = ejω. Then
X(ejω) = 1 −e−jωN
N
N−1
	
k=0
˜X(k)
1 −ej2πk/Ne−jω
=
N−1
	
k=0
˜X(k)
1 −e−jωN
N
*
1 −ej2πk/Ne−jω+
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling and Reconstruction in the z-Domain
153
Consider
1 −e−jωN
N
*
1 −ej2πk/Ne−jω+ =
1 −e−j(ω−2πk
N )N
N

1 −e−j(ω−2πk
N )
= e−j N
2 (ω−2πk
N )
e−1
2 j(ω−2πk
N )

sin

(ω −2πk
N ) N
2

N sin

(ω −2πk
N ) 1
2


Let
Φ(ω)
△=
sin( ωN
2 )
N sin( ω
2 )e−jω( N−1
2
) : an interpolating function
(5.18)
Then
X(ejω) =
N−1
	
k=0
˜X(k)Φ

ω −2πk
N

(5.19)
This is the DTFT interpolation formula to reconstruct X(ejω) from its
samples ˜X (k). Since Φ(0) = 1, we have that X(ej2πk/N) = ˜X(k), which
means that the interpolation is exact at sampling points. Recall the
time-domain interpolation formula (3.33) for analog signals:
xa(t) =
∞
	
n=−∞
x(n) sinc [Fs(t −nTs)]
(5.20)
The DTFT interpolating formula (5.19) looks similar.
However, there are some diﬀerences. First, the time-domain formula
(5.20) reconstructs an arbitrary nonperiodic analog signal, while the
frequency-domain formula (5.19) gives us a periodic waveform. Second, in
(5.19) we use a sin(Nx)
N sin x interpolation function instead of our more familiar
sin x
x
(sinc) function. The Φ(ω) function is a periodic function and hence
is known as a periodic-sinc function. It is also known as the Dirichlet
function. This is the function we observed in Example 5.2.
5.2.3 MATLAB IMPLEMENTATION
The interpolation formula (5.19) suﬀers the same fate as that of (5.20)
while trying to implement it in practice. One has to generate several inter-
polating functions (5.18) and perform their linear combinations to obtain
the discrete-time Fourier transform X(ejω) from its computed samples
˜X(k). Furthermore, in MATLAB we have to evaluate (5.19) on a ﬁner
grid over 0 ≤ω ≤2π. This is clearly an ineﬃcient approach. Another
approach is to use the cubic spline interpolation function as an eﬃcient
approximation to (5.19). This is what we did to implement the time-
domain interpolation formula for analog signals in Chapter 3. However,
there is an alternate and eﬃcient approach based on the DFT, which we
will study in the next section.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

154
Chapter 5
THE DISCRETE FOURIER TRANSFORM
5.3 THE DISCRETE FOURIER TRANSFORM
The discrete Fourier series provides a mechanism for numerically com-
puting the discrete-time Fourier transform. It also alerts us to a potential
problem of aliasing in the time domain. Mathematics dictates that the
sampling of the discrete-time Fourier transform result in a periodic se-
quence ˜x(n). But most of the signals in practice are not periodic. They
are likely to be of ﬁnite duration. How can we develop a numerically com-
putable Fourier representation for such signals? Theoretically, we can take
care of this problem by deﬁning a periodic signal whose primary shape is
that of the ﬁnite-duration signal and then using the DFS on this periodic
signal. Practically, we deﬁne a new transform called the discrete Fourier
transform (DFT), which is the primary period of the DFS. This DFT
is the ultimate numerically computable Fourier transform for arbitrary
ﬁnite-duration sequences.
First we deﬁne a ﬁnite-duration sequence x(n) that has N samples
over 0 ≤n ≤N −1 as an N-point sequence. Let ˜x(n) be a periodic signal
of period N, created using the N-point sequence x(n); that is, from (5.19),
˜x(n) =
∞
	
r=−∞
x(n −rN)
This is a somewhat cumbersome representation. Using the modulo-N
operation on the argument, we can simplify it to
˜x(n) = x(n mod N)
(5.21)
A simple way to interpret this operation is the following: if the argument
n is between 0 and N −1, then leave it as it is; otherwise add or sub-
tract multiples of N from n until the result is between 0 and N −1. Note
carefully that (5.21) is valid only if the length of x(n) is N or less. Further-
more, we use the following convenient notation to denote the modulo-N
operation:
x((n))N
△= x(n mod N)
(5.22)
Then the compact relationships between x(n) and ˜x(n) are
˜x(n) = x((n))N
(Periodic extension)
x(n) = ˜x(n)RN(n)
(Window operation)
(5.23)
The rem(n,N) function in MATLAB determines the remainder after di-
viding n by N. This function can be used to implement our modulo-N
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Transform
155
operation when n ≥0. When n < 0, we need to modify the result to
obtain correct values. This is shown below in the m=mod(n,N) function.
function m = mod(n,N)
% Computes m = (n mod N) index
% ----------------------------
% m = mod(n,N)
m = rem(n,N);
m = m+N;
m = rem(m,N);
In this function, n can be any integer array, and the array m contains the
corresponding modulo-N values.
From the frequency sampling theorem, we conclude that N equispaced
samples of the discrete-time Fourier transform X(ejω) of the N-point se-
quence x(n) can uniquely reconstruct X(ejω). These N samples around
the unit circle are called the discrete Fourier transform coeﬃcients. Let
˜X(k) = DFS ˜x(n), which is a periodic sequence (and hence of inﬁnite
duration). Its primary interval then is the discrete Fourier transform,
which is of ﬁnite duration. These notions are made clear in the follow-
ing deﬁnitions. The discrete Fourier transform of an N-point sequence is
given by
X(k)
△= DFT [x(n)] =
 ˜X(k), 0 ≤k ≤N −1
0,
elsewhere
= ˜X(k)RN(k)
or
X(k) =
N−1
	
n=0
x(n)W nk
N ,
0 ≤k ≤N −1
(5.24)
Note that the DFT X(k) is also an N-point sequence; that is, it is not
deﬁned outside of 0 ≤k ≤N −1. From (5.23),
˜X(k) = X((k))N;
that is, outside the 0 ≤k ≤N −1 interval only the DFS ˜X(k) is de-
ﬁned, which of course is the periodic extension of X(k). Finally, X(k) =
˜X(k)RN(k) means that the DFT X(k) is the primary interval of ˜X(k).
The inverse discrete Fourier transform of an N-point DFT X(k) is
given by
x(n)
△= IDFT [X(k)] = ˜x(n)RN(n)
or
x(n) = 1
N
N−1
	
k=0
X(k)W −kn
N
,
0 ≤n ≤N −1
(5.25)
Once again, x(n) is not deﬁned outside 0 ≤n ≤N −1. The extension of
x (n) outside this range is ˜x(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

156
Chapter 5
THE DISCRETE FOURIER TRANSFORM
5.3.1 MATLAB IMPLEMENTATION
It is clear from the discussions at the top of this section that the DFS is
practically equivalent to the DFT when 0 ≤n ≤N −1. Therefore, the
implementation of the DFT can be done in a similar fashion. If x(n) and
X(k) are arranged as column vectors x and X, respectively, then from
(5.24) and (5.25) we have
X = WNx
x = 1
N W∗
NX
(5.26)
where WN is the matrix deﬁned in (5.7) and will now be called a DFT
matrix. Hence the earlier dfs and idfs MATLAB functions can be re-
named as the dft and idft functions to implement the discrete Fourier
transform computations.
function [Xk] = dft(xn,N)
% Computes discrete Fourier transform
% -----------------------------------
% [Xk] = dft(xn,N)
% Xk = DFT coeff. array over 0 <= k <= N-1
% xn = N-point finite-duration sequence
%
N = Length of DFT
%
n = [0:1:N-1];
% Row vector for n
k = [0:1:N-1];
% Row vector for k
WN = exp(-j*2*pi/N);
% Wn factor
nk = n’*k;
% Creates an N by N matrix of nk values
WNnk = WN .ˆ nk;
% DFT matrix
Xk = xn * WNnk;
% Row vector for DFT coefficients
function [xn] = idft(Xk,N)
% Computes inverse discrete transform
% -----------------------------------
% [xn] = idft(Xk,N)
% xn = N-point sequence over 0 <= n <= N-1
% Xk = DFT coeff. array over 0 <= k <= N-1
%
N = length of DFT
%
n = [0:1:N-1];
% Row vector for n
k = [0:1:N-1];
% Row vector for k
WN = exp(-j*2*pi/N);
% Wn factor
nk = n’*k;
% Creates an N by N matrix of nk values
WNnk = WN .ˆ (-nk);
% IDFT matrix
xn = (Xk * WNnk)/N;
% Row vector for IDFT values
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Transform
157
□
EXAMPLE 5.6
Let x(n) be a four-point sequence:
x(n) =

1, 0 ≤n ≤3
0, otherwise
a. Compute the discrete-time Fourier transform X(ejω) and plot its magni-
tude and phase.
b. Compute the four-point DFT of x(n).
Solution
a. The discrete-time Fourier transform is given by
X(ejω) =
3

0
x(n)e−jωn = 1 + e−jω + e−j2ω + e−j3ω
= 1 −e−j4ω
1 −e−jω = sin(2ω)
sin(ω/2)e−j3ω/2
Hence
X(ejω)
 =

sin(2ω)
sin(ω/2)

and
̸ X(ejω) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−3ω
2 ,
when sin(2ω)
sin(ω/2) > 0
−3ω
2 ± π, when sin(2ω)
sin(ω/2) < 0
The plots are shown in Figure 5.4.
b. Let us denote the four-point DFT by X4 (k). Then
X4(k) =
3

n=0
x(n)W nk
4 ;
k = 0, 1, 2, 3; W4 = e−j2π/4 = −j
These calculations are similar to those in Example 5.1. We can also use
MATLAB to compute this DFT.
>> x = [1,1,1,1]; N = 4;
X = dft(x,N);
>> magX = abs(X), phaX = angle(X)*180/pi
magX =
4.0000
0.0000
0.0000
0.0000
phaX =
0 -134.9810
-90.0000
-44.9979
Hence
X4(k) = {4
↑
, 0, 0, 0}
Note that when the magnitude sample is zero, the corresponding angle is not
zero. This is due to a particular algorithm used by MATLAB to compute the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

158
Chapter 5
THE DISCRETE FOURIER TRANSFORM
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
1
2
3
4
Magnitude
Magnitude of the DTFT
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−200
−100
0
100
200
Frequency in π Units
Degrees
Angle of the DTFT
FIGURE 5.4
The DTFT plots in Example 5.6
angle part. Generally, these angles should be ignored. The plot of DFT values
is shown in Figure 5.5. The plot of X(ejω) is also shown as a dashed line for
comparison. From the plot in Figure 5.5, we observe that X4 correctly gives
four samples of X(ejω), but it has only one nonzero sample. Is this surprising?
By looking at the four-point x(n), which contains all 1’s, one must conclude
that its periodic extension is
˜x(n) = 1, ∀n
which is a constant (or a DC) signal. This is what is predicted by the DFT
X4(k), which has a nonzero sample at k = 0 (or ω = 0) and has no values at
other frequencies.
□
□
EXAMPLE 5.7
How can we obtain other samples of the DTFT X(ejω)?
Solution
It is clear that we should sample at dense (or ﬁner) frequencies; that is, we should
increase N. Suppose we take twice the number of points, or N = 8 instead of
N = 4. This we can achieve by treating x(n) as an eight-point sequence by
appending four zeros.
x(n) = {1
↑
, 1, 1, 1, 0, 0, 0, 0}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Transform
159
0
0.5
1
1.5
2
2.5
3
3.5
4
0
1
2
3
4
Magnitude
Magnitude of the DFT: N=4
0
0.5
1
1.5
2
2.5
3
3.5
4
−200
−100
0
100
200
k
Degrees
Angle of the DFT: N=4
FIGURE 5.5
The DFT plots of Example 5.6
This is a very important operation called a zero-padding operation. This oper-
ation is necessary in practice to obtain a dense spectrum of signals, as we shall
see. Let X8 (k) be an eight-point DFT, then
X8 (k) =
7

n=0
x(n)W nk
8 ;
k = 0, 1, . . . , 7; W8 = e−jπ/4
In this case, the frequency resolution is ω1 = 2π/8 = π/4.
MATLAB script:
>> x = [1,1,1,1, zeros(1,4)]; N = 8;
X = dft(x,N);
>> magX = abs(X), phaX = angle(X)*180/pi
magX =
4.0000
2.6131
0.0000
1.0824
0.0000
1.0824
0.0000
2.6131
phaX =
0
-67.5000 -134.9810
-22.5000
-90.0000
22.5000
-44.9979
67.5000
Hence
X8 (k) = {4
↑
, 2.6131e−j67.5◦,
0, 1.0824e−j22.5◦, 0, 1.0824ej22.5◦,
0, 2.6131ej67.5◦}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

160
Chapter 5
THE DISCRETE FOURIER TRANSFORM
0
1
2
3
4
5
6
7
8
0
1
2
3
4
Magnitude
Magnitude of the DFT: N=8
0
1
2
3
4
5
6
7
8
−200
−100
0
100
200
k
Degrees
Angle of the DFT: N=8
FIGURE 5.6
The DFT plots of Example 5.7: N = 8
which is shown in Figure 5.6. Continuing further, if we treat x(n) as a 16-point
sequence by padding 12 zeros, such that
x(n) = {1
↑
, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
then the frequency resolution is ω1 = 2π/16 = π/8 and W16 = e−jπ/8. There-
fore, we get a more dense spectrum with spectral samples separated by π/8.
The sketch of X16 (k) is shown in Figure 5.7.
It should be clear then that if we obtain many more spectral samples by
choosing a large N value, then the resulting DFT samples will be very close to
each other and we will obtain plot values similar to those in Figure 5.4. However,
the displayed stem-plots will be dense. In this situation, a better approach to
display samples is to either show them using dots or join the sample values using
the plot command (i.e., using the FOH studied in Chapter 3). Figure 5.8 shows
the magnitude and phase of the 128-point DFT x128(k) obtained by padding
120 zeros. The DFT magnitude plot overlaps the DTFT magnitude plot shown
as dotted-line, while the phase plot shows discrepancy at discontinuities due to
ﬁnite N value, which should be expected.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Transform
161
0
2
4
6
8
10
12
14
16
0
1
2
3
4
Magnitude
Magnitude of the DFT: N=16
0
2
4
6
8
10
12
14
16
−200
−100
0
100
200
k
Degrees
Angle of the DFT: N=16
FIGURE 5.7
The DFT plots of Example 5.7: N = 16
0
20
40
60
80
100
120
0
1
2
3
4
Magnitude
Magnitude of the DFT: N=128
0
20
40
60
80
100
120
−200
−100
0
100
200
k
Degrees
Angle of the DFT: N=128
FIGURE 5.8
The DFT plots of Example 5.7 for N = 128 are shown as line
plots
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

162
Chapter 5
THE DISCRETE FOURIER TRANSFORM
Comments:
Based on the last two examples, there are several comments
that we can make.
1. Zero-padding is an operation in which more zeros are appended to the
original sequence. The resulting longer DFT provides closely spaced
samples of the discrete-time Fourier transform of the original sequence.
In MATLAB, zero-padding is implemented using the zeros function.
2. In Example 5.6, all we needed to accurately plot the discrete-time
Fourier transform X(ejω) of x(n) was X4 (k), the four-point DFT. This
is because x(n) had only four nonzero samples, so we could have used
the interpolation formula (5.19) on X4 (k) to obtain X(ejω). However,
in practice, it is easier to obtain X8 (k) and X16 (k), and so on, to fill in
the values of X(ejω) rather than using the interpolation formula. This
approach can be made even more eﬃcient using fast Fourier transform
algorithms to compute the DFT.
3. The zero-padding gives us a high-density spectrum and provides a better
displayed version for plotting. But it does not give us a high-resolution
spectrum, because no new information is added to the signal; only ad-
ditional zeros are added in the data.
4. To get a high-resolution spectrum, one has to obtain more data from
the experiment or observations (see Example 5.8, below). There are
also other advanced methods that use additional side information or
nonlinear techniques.
□
EXAMPLE 5.8
To illustrate the diﬀerence between the high-density spectrum and the
high-resolution spectrum, consider the sequence
x(n) = cos (0.48πn) + cos (0.52πn)
We want to determine its spectrum based on the ﬁnite number of samples.
a. Determine and plot the discrete-time Fourier transform of x(n), 0 ≤n ≤10.
b. Determine
and
plot
the
discrete-time
Fourier
transform
of
x(n),
0 ≤n ≤100.
Solution
We could determine analytically the discrete-time Fourier transform in each
case, but MATLAB is a good vehicle to study these problems.
a. We can ﬁrst determine the 10-point DFT of x(n) to obtain an estimate of its
discrete-time Fourier transform.
MATLAB script:
>> n = [0:1:99]; x = cos(0.48*pi*n)+cos(0.52*pi*n);
>> n1 = [0:1:9] ;y1 = x(1:1:10);
>> subplot(2,1,1) ;stem(n1,y1); title(’signal x(n), 0 <= n <= 9’);xlabel(’n’)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Discrete Fourier Transform
163
0
1
2
3
4
5
6
7
8
9
−2
−1
0
1
2
Signal x(n), 0 <= n <= 9
n
Amplitude
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
Samples of DTFT Magnitude
Frequency in π Units
Magnitude
FIGURE 5.9
Signal and its spectrum in Example 5.8a: N = 10
>> Y1 = dft(y1,10); magY1 = abs(Y1(1:1:6));
>> k1 = 0:1:5 ;w1 = 2*pi/10*k1;
>> subplot(2,1,2);stem(w1/pi,magY1);title(’Samples of DTFT Magnitude’);
>> xlabel(’Frequency in \pi Units’)
The plots in Figure 5.9 show there aren’t enough samples to draw any conclu-
sions. Therefore, we will pad 90 zeros to obtain a dense spectrum. As explained
in Example 5.7, this spectrum is plotted using the plot command.
MATLAB script:
>> n2 = [0:1:99]; y2 = [x(1:1:10) zeros(1,90)];
>> subplot(2,1,1) ;stem(n2,y2) ;title(’signal x(n), 0 <= n <= 9 + 90 zeros’);
>> xlabel(’n’)
>> Y2 =dft(y2,100); magY2 = abs(Y2(1:1:51));
>> k2 = 0:1:50; w2 = 2*pi/100*k2;
>> subplot(2,1,2); plot(w2/pi,magY2); title(’DTFT Magnitude’);
>> xlabel(’Frequency in \pi Units’)
Now the plot shows that the sequence has a dominant frequency at ω = 0.5π
(Figure 5.10). This fact is not supported by the original sequence, which has two
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

164
Chapter 5
THE DISCRETE FOURIER TRANSFORM
0
10
20
30
40
50
60
70
80
90
100
−2
−1
0
1
2
Signal x(n), 0 <= n <= 9 + 90 zeros
n
Amplitude
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
5
10
DTFT Magnitude
Frequency in π Units
Magnitude
FIGURE 5.10
Signal and its spectrum in Example 5.8a: N = 100
frequencies. The zero-padding provided a smoother version of the spectrum in
Figure 5.9.
b. To get better spectral information, we will take the ﬁrst 100 samples of x(n)
and determine its discrete-time Fourier transform.
MATLAB script:
>> subplot(2,1,1); stem(n,x);
>> title(’signal x(n), 0 <= n <= 99’); xlabel(’n’)
>> X = dft(x,100); magX = abs(X(1:1:51));
>> k = 0:1:50; w = 2*pi/100*k;
>> subplot(2,1,2); plot(w/pi,magX); title(’DTFT Magnitude’);
>> xlabel(’Frequency in \pi Units’)
Now the discrete-time Fourier transform plot clearly shows two frequencies,
which are very close to each other (Figure 5.11). This is the high-resolution
spectrum of x(n). Note that padding more zeros to the 100-point sequence will
result in a smoother rendition of the spectrum in Figure 5.11 but will not reveal
any new information. Readers are encouraged to verify this.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
165
0
10
20
30
40
50
60
70
80
90
100
−2
−1
0
1
2
Signal x(n), 0 <= n <= 99
n
Amplitude
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
DTFT Magnitude
Frequency in π Units
Magnitude
FIGURE 5.11
Signal and its spectrum in Example 5.8b: N = 100
5.4 PROPERTIES OF THE DISCRETE FOURIER TRANSFORM
The DFT properties are derived from those of the DFS because mathe-
matically DFS is the valid representation. We discuss several useful prop-
erties, which are given without proof. These properties also apply to the
DFS with necessary changes. Let X(k) be an N-point DFT of the se-
quence x(n). Unless otherwise stated, the N-point DFTs will be used in
these properties.
1. Linearity: The DFT is a linear transform
DFT [ax1(n) + bx2(n)] = a DFT [x1(n)] + b DFT [x2(n)]
(5.27)
Note: If x1(n) and x2(n) have diﬀerent durations—that is, they are
N1-point and N2-point sequences, respectively—then choose N3 =
max(N1, N2) and proceed by taking N3-point DFTs.
2. Circular folding: If an N-point sequence is folded, then the result
x(−n) would not be an N-point sequence, and it would not be possible
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

166
Chapter 5
THE DISCRETE FOURIER TRANSFORM
to compute its DFT. Therefore, we use the modulo-N operation on the
argument (−n) and deﬁne folding by
x ((−n))N =

x(0),
n = 0
x(N −n), 1 ≤n ≤N −1
(5.28)
This is called a circular folding. To visualize it, imagine that the se-
quence x(n) is wrapped around a circle in the counterclockwise direc-
tion so that indices n = 0 and n = N overlap. Then x((−n))N can
be viewed as a clockwise wrapping of x(n) around the circle; hence
the name circular folding. In MATLAB, the circular folding can be
achieved by x=x(mod(-n,N)+1). Note that the arguments in MATLAB
begin with 1. The DFT of a circular folding is given by
DFT [x ((−n))N] = X ((−k))N =

X(0),
k = 0
X(N −k), 1 ≤k ≤N −1
(5.29)
□
EXAMPLE 5.9
Let x(n) = 10 (0.8)n ,
0 ≤n ≤10.
a. Determine and plot x ((−n))11.
b. Verify the circular folding property.
Solution
a. MATLAB script:
>> n = 0:10; x = 10*(0.8) .ˆ n;
y = x(mod(-n,11)+1);
>> subplot(2,1,1); stem(n,x); title(’Original Sequence’)
>> xlabel(’n’); ylabel(’Amplitude’);
>> subplot(2,1,2); stem(n,y);
>> title(’Circularly Folded Sequence x((-n))_{11}’);
>> xlabel(’n’); ylabel(’Amplitude’);
The plots in Figure 5.12 show the eﬀect of circular folding.
b. MATLAB script:
>> X = dft(x,11); Y = dft(y,11);
>> subplot(2,2,1); stem(n,real(X));
>> title(’Real(DFT[x(n)])’); xlabel(’k’);
>> subplot(2,2,2); stem(n,imag(X));
>> title(’Imag(DFT[x(n)])’); xlabel(’k’);
>> subplot(2,2,3); stem(n,real(Y));
>> title(’Real(DFT[x((-n))_{11}])’); xlabel(’k’);
>> subplot(2,2,4); stem(n,imag(Y));
>> title(’Imag(DFT[x((-n))_{11}])’); xlabel(’k’);
The plots in Figure 5.13 verify the property.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
167
0
1
2
3
4
5
6
7
8
9
10
0
5
10
Original Sequence
n
Amplitude
0
1
2
3
4
5
6
7
8
9
10
0
5
10
Circularly Folded Sequence x((–n))11
n
Amplitude
FIGURE 5.12
Circular folding in Example 5.9a
0
5
10
0
10
20
30
40
50
Real(DFT[x(n)])
Amplitude
k
0
5
10
−20
−10
0
10
20
Imag(DFT[x(n)])
Amplitude
k
0
5
10
0
10
20
30
40
50
Real(DFT[x((−n))11])
Amplitude
k
0
5
10
−20
−10
0
10
20
Imag(DFT[x((−n))11])
Amplitude
k
FIGURE 5.13
Circular folding property in Example 5.9b
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

168
Chapter 5
THE DISCRETE FOURIER TRANSFORM
3. Conjugation: Similar to the above property, we have to introduce the
circular folding in the frequency domain:
DFT [x∗(n)] = X∗((−k))N
(5.30)
4. Symmetry properties for real sequences: Let x(n) be a real-
valued N-point sequence. Then x(n) = x∗(n). Using (5.30),
X(k) = X∗((−k))N
(5.31)
This symmetry is called a circular conjugate symmetry. It further im-
plies that
Re [X(k)] = Re [X ((−k))N]
=⇒Circular-even sequence
Im [X(k)] = −Im [X ((N −k))N] =⇒Circular-odd sequence
|X(k)| = |X ((−k))N|
=⇒Circular-even sequence
̸ X(k) = −̸ X ((−k))N
=⇒Circular-odd sequence
(5.32)
Comments:
1. Observe the magnitudes and angles of the various DFTs in Examples
5.6 and 5.7. They do satisfy the above circular symmetries. These sym-
metries are diﬀerent than the usual even and odd symmetries. To visu-
alize this, imagine that the DFT samples are arranged around a circle
so that the indices k = 0 and k = N overlap; then the samples will
be symmetric with respect to k = 0, which justiﬁes the name circular
symmetry.
2. The corresponding symmetry for the DFS coeﬃcients is called the pe-
riodic conjugate symmetry.
3. Since these DFTs have symmetry, one needs to compute X(k) only for
k = 0, 1, . . . , N
2 ;
N even
or for
k = 0, 1, . . . , N −1
2
;
N odd
This results in about 50 percent savings in computation as well as in
storage.
4. From (5.30)
X(0) = X∗((−0))N = X∗(0)
which means that the DFT coeﬃcient at k = 0 must be a real number.
But k = 0 means that the frequency ωk = kω1 = 0, which is the DC
frequency. Hence the DC coeﬃcient for a real-valued x(n) must be a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
169
real number. In addition, if N is even, then N/2 is also an integer.
Then from (5.32),
X (N/2) = X∗((−N/2))N = X∗(N/2)
which means that even the k = N/2 component is also real-valued.
This component is called the Nyquist component since k = N/2 means
that the frequency ωN/2 = (N/2)(2π/N) = π, which is the digital
Nyquist frequency.
The real-valued signals can also be decomposed into their even and odd
components, xe(n) and xo (n), respectively, as discussed in Chapter 2.
However, these components are not N-point sequences, and therefore we
cannot take their N-point DFTs. Hence we deﬁne a new set of components
using the circular folding discussed above. These are called circular-even
and circular-odd components deﬁned by
xec (n)
△= 1
2 [x(n) + x ((−n))N] =

x(0),
n = 0
1
2 [x (n) + x (N −n)] , 1 ≤n ≤N −1
xoc (n)
△= 1
2 [x(n) −x ((−n))N] =

0,
n = 0
1
2 [x (n) −x (N −n)] , 1 ≤n ≤N −1
(5.33)
Then
DFT [xec (n)] = Re [X(k)] = Re [X ((−k))N]
DFT [xoc (n)] = Im [X(k)] = Im [X ((−k))N]
(5.34)
Implication:
If x(n) is real and circular-even, then its DFT is also real
and circular-even. Hence only the ﬁrst 0 ≤n ≤N/2 coeﬃcients are
necessary for complete representation.
Using (5.33), it is easy to develop a function to decompose an N-point
sequence into its circular-even and circular-odd components. The follow-
ing circevod function uses the mod function given earlier to implement
the modulo-N operation.
function [xec, xoc] = circevod(x)
% Signal decomposition into circular-even and circular-odd parts
% --------------------------------------------------------------
% [xec, xoc] = circevod(x)
%
if any(imag(x) ˜= 0)
error(’x is not a real sequence’)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

170
Chapter 5
THE DISCRETE FOURIER TRANSFORM
end
N = length(x); n = 0:(N-1);
xec = 0.5*(x + x(mod(-n,N)+1));
xoc = 0.5*(x - x(mod(-n,N)+1));
□
EXAMPLE 5.10
Let x(n) = 10 (0.8)n ,
0 ≤n ≤10 as in Example 5.9.
a. Decompose and plot the xec(n) and xoc(n) components of x(n).
b. Verify the property in (5.34).
Solution
a. MATLAB script:
>> n = 0:10; x = 10*(0.8) .ˆ n;
>> [xec,xoc] = circevod(x);
>> subplot(2,1,1); stem(n,xec); title(’Circular-Even Component’)
>> xlabel(’n’); ylabel(’Amplitude’); axis([-0.5,10.5,-1,11])
>> subplot(2,1,2); stem(n,xoc); title(’Circular-Odd Component’)
>> xlabel(’n’); ylabel(’Amplitude’); axis([-0.5,10.5,-4,4])
The plots in Figure 5.14 show the circularly symmetric components of x(n).
0
1
2
3
4
5
6
7
8
9
10
0
2
4
6
8
10
Circular−Even Component
n
Amplitude
0
1
2
3
4
5
6
7
8
9
10
−4
−2
0
2
4
Circular−Odd Component
n
Amplitude
FIGURE 5.14
Circular-even and circular-odd components of the sequence in
Example 5.10a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
171
0
5
10
0
10
20
30
40
50
Real(DFT[x(n)])
Amplitude
k
0
5
10
−20
−10
0
10
20
Imag(DFT[x(n)])
Amplitude
k
0
5
10
0
10
20
30
40
50
DFT[xec(n)]
Amplitude
k
0
5
10
−20
−10
0
10
20
DFT[xoc(n)]
Amplitude
k
FIGURE 5.15
Plots of DFT symmetry properties in Example 5.10b
b. MATLAB script:
>> X = dft(x,11); Xec = dft(xec,11); Xoc = dft(xoc,11);
>> subplot(2,2,1); stem(n,real(X)); axis([-0.5,10.5,-5,50])
>> title(’Real(DFT[x(n)])’); xlabel(’k’);
>> subplot(2,2,2); stem(n,imag(X)); axis([-0.5,10.5,-20,20])
>> title(’Imag(DFT[x(n)])’); xlabel(’k’);
>> subplot(2,2,3); stem(n,real(Xec)); axis([-0.5,10.5,-5,50])
>> title(’DFT[xec(n)]’); xlabel(’k’);
>> subplot(2,2,4); stem(n,imag(Xoc)); axis([-0.5,10.5,-20,20])
>> title(’DFT[xoc(n)]’); xlabel(’k’);
From the plots in Figure 5.15, we observe that the DFT of xec(n) is the same as
the real part of X(k) and that the DFT of xoc(n) is the same as the imaginary
part of X(k).
□
A similar property for complex-valued sequences is explored in Prob-
lem P5.18.
5. Circular shift of a sequence: If an N-point sequence is shifted in
either direction, then the result is no longer between 0 ≤n ≤N −1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

172
Chapter 5
THE DISCRETE FOURIER TRANSFORM
Therefore, we ﬁrst convert x(n) into its periodic extension ˜x(n), and
then shift it by m samples to obtain
˜x(n −m) = x ((n −m))N
(5.35)
This is called a periodic shift of ˜x(n). The periodic shift is then con-
verted into an N-point sequence. The resulting sequence
˜x(n −m)RN(n) = x ((n −m))N RN(n)
(5.36)
is called the circular shift of x(n). Once again, to visualize this, imagine
that the sequence x(n) is wrapped around a circle. Now rotate the circle
by k samples and unwrap the sequence from 0 ≤n ≤N −1. Its DFT
is given by
DFT [x ((n −m))N RN(n)] = W km
N X(k)
(5.37)
□
EXAMPLE 5.11
Let x(n) = 10 (0.8)n ,
0 ≤n ≤10 be an 11-point sequence.
a. Sketch x((n + 4))11R11(n), that is, a circular shift by four samples toward
the left.
b. Sketch x((n −3))15R15(n), that is, a circular shift by three samples toward
the right, where x(n) is assumed to be a 15-point sequence.
Solution
We will use a step-by-step graphical approach to illustrate the circular shifting
operation. This approach shows the periodic extension ˜x(n) = x((n))N of x(n),
followed by a linear shift in ˜x(n) to obtain ˜x(n−m) = x((n−m))N, and ﬁnally
truncating ˜x(n −m) to obtain the circular shift.
a. Figure 5.16 shows four sequences. The top-left shows x(n), the bottom-left
shows ˜x(n), the top-right shows ˜x(n + 4), and, ﬁnally, the bottom-right shows
x((n+4))11R11(n). Note carefully that as samples move out of the [0, N −1]
window in one direction, they reappear from the opposite direction. This is
the meaning of the circular shift, and it is diﬀerent from the linear shift.
b. In this case, the sequence x(n) is treated as a 15-point sequence by padding
four zeros. Now the circular shift will be diﬀerent than when N = 11. This
is shown in Figure 5.17. In fact, the circular shift x ((n −3))15 looks like a
linear shift x(n −3).
□
To implement a circular shift, we do not have to go through the
periodic shift as shown in Example 5.11. It can be implemented directly
in two ways. In the ﬁrst approach, the modulo-N operation can be used
on the argument (n −m) in the time domain. This is shown below in the
cirshftt function.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
173
5
0
5
10
15
0
5
10
Original x(n)
Amplitude
n
5
0
5
10
15
0
5
10
Periodic Extension
Amplitude
n
5
0
5
10
15
0
5
10
Periodic Shift
Amplitude
n
5
0
5
10
15
0
5
10
Circular Shift
Amplitude
n
FIGURE 5.16
Graphical interpretation of circular shift, N = 11
0
10
20
0
5
10
Original x(n)
Amplitude
n
0
10
20
0
5
10
Periodic Extension
Amplitude
n
0
10
20
0
5
10
Periodic Shift
Amplitude
n
0
10
20
0
5
10
Circular Shift
Amplitude
n
FIGURE 5.17
Graphical interpretation of circular shift, N = 15
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

174
Chapter 5
THE DISCRETE FOURIER TRANSFORM
function y = cirshftt(x,m,N)
% Circular shift of m samples wrt size N in sequence x: (time domain)
% -------------------------------------------------------------------
% [y] = cirshftt(x,m,N)
% y = output sequence containing the circular shift
% x = input sequence of length <= N
% m = sample shift
% N = size of circular buffer
%
Method: y(n) = x((n-m) mod N)
% Check for length of x
if length(x) > N
error(’N must be >= the length of x’)
end
x = [x zeros(1,N-length(x))];
n = [0:1:N-1];
n = mod(n-m,N);
y = x(n+1);
In the second approach, the property (5.37) can be used in the frequency
domain. This is explored in Problem P5.20.
□
EXAMPLE 5.12
Given an 11-point sequence x(n) = 10 (0.8)n ,
0 ≤n ≤10, determine and plot
x ((n −6))15.
Solution
MATLAB script:
>> n = 0:10; x = 10*(0.8) .ˆ n;
y = cirshftt(x,6,15);
>> n = 0:14; x = [x, zeros(1,4)];
>> subplot(2,1,1); stem(n,x); title(’Original Sequence x(n)’)
>> xlabel(’n’); ylabel(’Amplitude’);
>> subplot(2,1,2); stem(n,y);
>> title(’Circularly Shifted Sequence x((n-6))_{15}’)
>> xlabel(’n’); ylabel(’Amplitude’);
The results are shown in Figure 5.18.
□
6. Circular shift in the frequency domain: This property is a dual
of the preceding property given by
DFT

W −ℓn
N
x(n)

= X ((k −ℓ))N RN(k)
(5.38)
7. Circular convolution: A linear convolution between two N-point
sequences will result in a longer sequence. Once again, we have to
restrict our interval to 0 ≤n ≤N −1. Therefore, instead of linear
shift, we should consider the circular shift. A convolution operation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
175
0
5
10
15
0
5
10
Original Sequence x(n)
n
Amplitude
0
5
10
Circularly Shifted Sequence x((n–6))15
0
5
10
15
n
Amplitude
FIGURE 5.18
Circularly shifted sequence in Example 5.12
that contains a circular shift is called the circular convolution and is
given by
x1(n) N⃝x2(n) =
N−1
	
m=0
x1(m)x2 ((n −m))N ,
0 ≤n ≤N −1
(5.39)
Note that the circular convolution is also an N-point sequence. It has
a structure similar to that of a linear convolution. The diﬀerences
are in the summation limits and in the N-point circular shift. Hence
it depends on N and is also called an N-point circular convolution.
Therefore, the use of the notation N⃝is appropriate. The DFT prop-
erty for the circular convolution is
DFT

x1(n) N⃝x2(n)

= X1(k) · X2(k)
(5.40)
An alternate interpretation of this property is that when we multi-
ply two N-point DFTs in the frequency domain, we get the circular
convolution (and not the usual linear convolution) in the time domain.
□
EXAMPLE 5.13
Let x1(n) = {1, 2, 2} and x2(n) = {1, 2, 3, 4}. Compute the four-point circular
convolution x1(n)
4⃝x2(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

176
Chapter 5
THE DISCRETE FOURIER TRANSFORM
Solution
Note that x1(n) is a three-point sequence, and hence we will have to pad one
zero to make it a four-point sequence before we perform the circular convolution.
We will compute this convolution in the time domain as well as in the frequency
domain. In the time domain, we will use the mechanism of circular convolution;
in the frequency domain, we will use the DFTs.
•
Time-domain approach: The four-point circular convolution is given by
x1(n)
4⃝x2(n) =
3

m=0
x1 (m) x2 ((n −m))4
Thus we have to create a circularly folded and shifted sequence x2((n−m))N
for each value of n, multiply it sample by sample with x1(m), add the samples
to obtain the circular convolution value for that n, and then repeat the
procedure for 0 ≤n ≤3. Consider
x1(m) = {1, 2, 2, 0}
and
x2(m) = {1, 2, 3, 4}
for n = 0
3

m=0
x1(m) · x2 ((0 −m))5 =
3

m=0
[{1, 2, 2, 0} · {1, 4, 3, 2}]
=
3

m=0
{1, 8, 6, 0} = 15
for n = 1
3

m=0
x1(m) · x2 ((1 −m))5 =
3

m=0
[{1, 2, 2, 0} · {2, 1, 4, 3}]
=
3

m=0
{2, 2, 8, 0} = 12
for n = 2
3

m=0
x1(m) · x2 ((2 −m))5 =
3

m=0
[{1, 2, 2, 0} · {3, 2, 1, 4}]
=
3

m=0
{3, 4, 2, 0} = 9
for n = 3
3

m=0
x1(m) · x2 ((3 −m))5 =
3

m=0
[{1, 2, 2, 0} · {4, 3, 2, 1}]
=
3

m=0
{4, 6, 4, 0} = 14
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
177
Hence
x1(n)
4⃝x2(n) = {15, 12, 9, 14}
•
Frequency-domain approach: In this approach, we ﬁrst compute four-point
DFTs of x1(n) and x2(n), multiply them sample by sample, and then take
the inverse DFT of the result to obtain the circular convolution.
DFT of x1(n)
x1(n) = {1, 2, 2, 0} =⇒X1(k) = {5, −1 −j2, 1, −1 + j2}
DFT of x2(n)
x2(n) = {1, 2, 3, 4} =⇒X2(k) = {10, −2 + j2, −2, −2 −j2}
Now
X1(k) · X2(k) = {50, 6 + j2, −2, 6 −j2}
Finally, after IDFT,
x1(n)
4⃝x2(n) = {15, 12, 9, 14}
which is the same as before.
□
Similar to the circular shift implementation, we can implement the
circular convolution in a number of diﬀerent ways. The simplest approach
would be to implement (5.39) literally by using the cirshftt function
and requiring two nested for...end loops. Obviously, this is not eﬃcient.
Another approach is to generate a sequence x ((n −m))N for each n in
[0, N −1] as rows of a matrix and then implement (5.39) as a matrix-
vector multiplication similar to our dft function. This would require
one for...end loop. The following circonvt function incorporates these
steps.
function y = circonvt(x1,x2,N)
% N-point circular convolution between x1 and x2: (time-domain)
% -------------------------------------------------------------
% [y] = circonvt(x1,x2,N)
%
y = output sequence containing the circular convolution
% x1 = input sequence of length N1 <= N
% x2 = input sequence of length N2 <= N
%
N = size of circular buffer
%
Method: y(n) = sum (x1(m)*x2((n-m) mod N))
% Check for length of x1
if length(x1) > N
error(’N must be >= the length of x1’)
end
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

178
Chapter 5
THE DISCRETE FOURIER TRANSFORM
% Check for length of x2
if length(x2) > N
error(’N must be >= the length of x2’)
end
x1=[x1 zeros(1,N-length(x1))];
x2=[x2 zeros(1,N-length(x2))];
m = [0:1:N-1];
x2 = x2(mod(-m,N)+1);
H = zeros(N,N);
for n = 1:1:N
H(n,:) = cirshftt(x2,n-1,N);
end
y = x1*conj(H’);
Problems P5.24 and P5.25 explore an approach to eliminate the for...
end loop in the circonvt function. The third approach would be to im-
plement the frequency-domain operation (5.40) using the dft function.
This is explored in Problem P5.26.
□
EXAMPLE 5.14
Let us use MATLAB to perform the circular convolution in Example 5.13.
Solution
The sequences are x1(n) = {1, 2, 2} and x2(n) = {1, 2, 3, 4}.
MATLAB script:
>> x1 = [1,2,2]; x2 = [1,2,3,4];
y = circonvt(x1, x2, 4)
y =
15
12
9
14
Hence
x1(n)
4⃝x2(n) = {15, 12, 9, 14}
as before.
□
□
EXAMPLE 5.15
In this example, we will study the eﬀect of N on the circular convolution.
Obviously, N ≥4; otherwise, there will be a time-domain aliasing for x2(n).
We will use the same two sequences from Example 5.13.
a. Compute x1(n)
5⃝x2(n).
b. Compute x1(n)
6⃝x2(n).
c. Comment on the results.
Solution
The sequences are x1(n) = {1, 2, 2} and x2(n) = {1, 2, 3, 4}. Even though the
sequences are the same as in Example 5.14, we should expect diﬀerent results
for diﬀerent values of N. This is not the case with the linear convolution, which
is unique, given two sequences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Discrete Fourier Transform
179
a. MATLAB script for ﬁve-point circular convolution:
>> x1 = [1,2,2]; x2 = [1,2,3,4];
y = circonvt(x1, x2, 5)
y =
9
4
9
14
14
Hence
x1(n)
5⃝x2(n) = {9, 4, 9, 14, 14}
b. MATLAB script for six-point circular convolution:
>> x1 = [1,2,2]; x2 = [1,2,3,4];
y = circonvt(x1, x2, 6)
y =
1
4
9
14
14
8
Hence
x1(n)
6⃝x2(n) = {1, 4, 9, 14, 14, 8}
c. A careful observation of four-, ﬁve-, and six-point circular convolutions from
this and the previous example indicates some unique features. Clearly, an
N-point circular convolution is an N-point sequence. However, some sam-
ples in these convolutions have the same values, while other values can be
obtained as a sum of samples in other convolutions. For example, the ﬁrst
sample in the ﬁve-point convolution is a sum of the ﬁrst and the last sam-
ples of the six-point convolution. The linear convolution between x1(n) and
x2(n) is given by
x1(n) ∗x2(n) = {1, 4, 9, 14, 14, 8}
which is equivalent to the six-point circular convolution. These and other
issues are explored in the next section.
□
8. Multiplication: This is the dual of the circular convolution property.
It is given by
DFT [x1(n) · x2(n)] = 1
N X1(k) N⃝X2(k)
(5.41)
in which the circular convolution is performed in the frequency domain.
The MATLAB functions developed for circular convolution can also be
used here since X1 (k) and X2 (k) are also N-point sequences.
9. Parseval’s relation: This relation computes the energy in the fre-
quency domain.
Ex =
N−1
	
n=0
|x(n)|2 = 1
N
N−1
	
k=0
|X(k)|2
(5.42)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

180
Chapter 5
THE DISCRETE FOURIER TRANSFORM
The quantity |X(k)|2
N
is called the energy spectrum of ﬁnite-duration se-
quences. Similarly, for periodic sequences, the quantity |
˜
X(k)
N |2 is called
the power spectrum.
5.5 LINEAR CONVOLUTION USING THE DFT
One of the most important operations in linear systems is the linear convo-
lution. In fact, FIR ﬁlters are generally implemented in practice using this
linear convolution. On the other hand, the DFT is a practical approach
for implementing linear system operations in the frequency domain. As we
shall see later, it is also an eﬃcient operation in terms of computations.
However, there is one problem. The DFT operations result in a circular
convolution (something that we do not desire), not in a linear convolution
that we want. Now we shall see how to use the DFT to perform a linear
convolution (or equivalently, how to make a circular convolution identical
to the linear convolution). We alluded to this problem in Example 5.15.
Let x1(n) be an N1-point sequence, and let x2(n) be an N2-point
sequence. Deﬁne the linear convolution of x1(n) and x2(n) by x3(n),
that is,
x3(n) = x1(n) ∗x2(n)
=
∞
	
k=−∞
x1(k)x2(n −k) =
N1−1
	
0
x1(k)x2(n −k)
(5.43)
Then x3(n) is an (N1 + N2 −1)-point sequence. If we choose N =
max(N1, N2) and compute an N-point circular convolution x1(n)
N⃝
x2(n), then we get an N-point sequence, which obviously is diﬀerent
from x3(n). This observation also gives us a clue. Why not choose N =
N1+N2−1 and perform an (N1+N2−1)-point circular convolution? Then
at least both of these convolutions will have an equal number of samples.
Therefore, let N = N1 + N2 −1, and let us treat x1(n) and x2(n) as
N-point sequences. Deﬁne the N-point circular convolution by x4(n).
x4(n) = x1(n) N⃝x2(n)
(5.44)
=
N−1
	
m=0
x1(m)x2((n −m))N

RN(n)
=
N−1
	
m=0
x1(m)
∞
	
r=−∞
x2(n −m −rN)

RN(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Linear Convolution Using the DFT
181
=
⎡
⎢⎢⎢⎢⎣
∞
	
r=−∞
N1−1
	
m=0
x1(m)x2(n −m −rN)
&
'(
)
x3(n−rN)
⎤
⎥⎥⎥⎥⎦
RN(n)
=

∞
	
r=−∞
x3(n −rN)

RN(n)
using (5.43)
This analysis shows that, in general, the circular convolution is an aliased
version of the linear convolution. We observed this fact in Example 5.15.
Now since x3(n) is an N = (N1 + N2 −1)-point sequence, we have
x4(n) = x3(n);
0 ≤n ≤(N −1)
which means that there is no aliasing in the time domain.
Conclusion:
If we make both x1(n) and x2(n) N = (N1 + N2 −1)-point
sequences by padding an appropriate number of zeros, then the circular
convolution is identical to the linear convolution.
□
EXAMPLE 5.16
Let x1(n) and x2(n) be the following two four-point sequences:
x1(n) = {1, 2, 2, 1} ,
x2(n) = {1, −1, −1, 1}
a. Determine their linear convolution x3(n).
b. Compute the circular convolution x4(n) so that it is equal to x3(n).
Solution
We will use MATLAB to do this problem.
a. MATLAB script:
>> x1 = [1,2,2,1]; x2 = [1,-1,-1,1];
x3 = conv(x1,x2)
x3 =
1
1
-1
-2
-1
1
1
Hence the linear convolution x3(n) is a seven-point sequence given by
x3(n) = {1, 1, −1, −2, −1, 1, 1}
b.We will have to use N ≥7. Choosing N = 7, we have
>> x4 = circonvt(x1,x2,7)
x4 =
1
1
-1
-2
-1
1
1
Hence
x4 = {1, 1, −1, −2, −1, 1, 1} = x3(n)
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

182
Chapter 5
THE DISCRETE FOURIER TRANSFORM
5.5.1 ERROR ANALYSIS
To use the DFT for linear convolution, we must choose N properly. How-
ever, in practice it may not be possible to do so, especially when N is very
large and there is a limit on memory. Then an error will be introduced
when N is chosen less than the required value to perform the circular
convolution. We want to compute this error, which is useful in practice.
Obviously, N ≥max(N1, N2). Therefore, let
max(N1, N2) ≤N < (N1 + N2 −1)
Then, from our previous analysis (5.44),
x4(n) =

∞
	
r=−∞
x3(n −rN)

RN(n)
Let an error e(n) be given by
e(n)
△= x4(n) −x3(n)
=
⎡
⎣	
r̸=0
x3(n −rN)
⎤
⎦RN(n)
Since N ≥max(N1, N2), only two terms corresponding to r = ±1 remain
in the above summation. Hence
e(n) = [x3(n −N) + x3(n + N)] RN(n)
Generally, x1(n) and x2(n) are causal sequences. Then x3(n) is also causal,
which means that
x3(n −N) = 0;
0 ≤n ≤N −1
Therefore,
e(n) = x3(n + N),
0 ≤n ≤N −1
(5.45)
This is a simple yet important relation. It implies that when
max(N1, N2) ≤N < (N1 +N2 −1) the error value at n is the same as the
linear convolution value computed N samples away. Now the linear con-
volution will be zero after (N1+N2−1) samples. This means that the ﬁrst
few samples of the circular convolution are in error, while the remaining
ones are the correct linear convolution values.
□
EXAMPLE 5.17
Consider the sequences x1(n) and x2(n) from the previous example. Evaluate
circular convolutions for N = 6, 5, and 4. Verify the error relations in each case.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Linear Convolution Using the DFT
183
Solution
Clearly, the linear convolution x3(n) is still the same.
x3(n) = {1, 1, −1, −2, −1, 1, 1}
When N = 6, we obtain a six-point sequence:
x4(n) = x1(n)
6⃝x2(n) = {2, 1, −1, −2, −1, 1}
Therefore,
e(n) = {2, 1, −1, −2, −1, 1} −{1, 1, −1, −2, −1, 1} ,
0 ≤n ≤5
= {1, 0, 0, 0, 0, 0}
= x3(n + 6)
as expected. When N = 5, we obtain a ﬁve-point sequence,
x4(n) = x1(n)
5⃝x2(n) = {2, 2, −1, −2, −1}
and
e(n) = {2, 2, −1, −2, −1} −{1, 1, −1, −2, −1} ,
0 ≤n ≤4
= {1, 1, 0, 0, 0}
= x3(n + 5)
Finally, when N = 4, we obtain a four-point sequence,
x4(n) = x1(n)
4⃝x2(n) = {0, 2, 0, −2}
and
e(n) = {0, 2, 0, −2} −{1, 1, −1, −2} ,
0 ≤n ≤3
= {−1, 1, 1, 0}
= x3(n + 4)
The last case of N = 4 also provides the following useful observation.
Observation:
When N = max(N1, N2) is chosen for circular convolution, then
the ﬁrst (M −1) samples are in error (i.e., diﬀerent from the linear convolution),
where M = min(N1, N2). This result is useful in implementing long convolutions
in the form of block processing.
□
5.5.2 BLOCK CONVOLUTIONS
When we want to ﬁlter an input sequence that is being received con-
tinuously, such as a speech signal from a microphone, then for practical
purposes we can think of this sequence as an inﬁnite-length sequence. If
we want to implement this ﬁltering operation as an FIR ﬁlter in which
the linear convolution is computed using the DFT, then we experience
some practical problems. We will have to compute a large DFT, which is
generally impractical. Furthermore, output samples are not available until
all input samples are processed. This introduces an unacceptably large
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

184
Chapter 5
THE DISCRETE FOURIER TRANSFORM
amount of delay. Therefore, we have to segment the inﬁnite-length input
sequence into smaller sections (or blocks), process each section using the
DFT, and ﬁnally assemble the output sequence from the outputs of each
section. This procedure is called a block convolution (or block processing)
operation.
Let us assume that the sequence x(n) is sectioned into N-point se-
quences and that the impulse response of the ﬁlter is an M-point se-
quence, where M < N. Then from the observation in Example 5.17, we
note that the N-point circular convolution between the input block and
the impulse response will yield a block output sequence in which the ﬁrst
(M −1) samples are not the correct output values. If we simply partition
x(n) into nonoverlapping sections, then the resulting output sequence will
have intervals of incorrect samples. To correct this problem, we can parti-
tion x(n) into sections, each overlapping with the previous one by exactly
(M −1) samples, save the last (N −M + 1) output samples, and ﬁnally
concatenate these outputs into a sequence. To correct for the ﬁrst (M −1)
samples in the ﬁrst output block, we set the ﬁrst (M −1) samples in the
ﬁrst input block to zero. This procedure is called an overlap-save method
of block convolutions. Clearly, when N ≫M, this method is more eﬃ-
cient. We illustrate it using a simple example.
□
EXAMPLE 5.18
Let x(n) = (n + 1) ,
0 ≤n ≤9 and h(n) = {1
↑
, 0, −1}. Implement the overlap-
save method using N = 6 to compute y(n) = x(n) ∗h(n).
Solution
Since M = 3, we will have to overlap each section with the previous one by two
samples. Now x(n) is a 10-point sequence, and we will need (M −1) = 2 zeros
in the beginning. Since N = 6, we will need three sections. Let the sections be
x1(n) = {0, 0, 1, 2, 3, 4}
x2(n) = {3, 4, 5, 6, 7, 8}
x3(n) = {7, 8, 9, 10, 0, 0}
Note that we have to pad x3(n) by two zeros since x(n) runs out of values at
n = 9. Now we will compute the six-point circular convolution of each section
with h(n).
y1 = x1(n)
6⃝h(n) = {−3, −4, 1, 2, 2, 2}
y2 = x2(n)
6⃝h(n) = {−4, −4, 2, 2, 2, 2}
y3 = x3(n)
6⃝h(n) = {7, 8, 2, 2, −9, −10}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Linear Convolution Using the DFT
185
Noting that the ﬁrst two samples in each section are to be discarded, we assemble
the output y(n) as
y(n) = {1
↑
, 2, 2, 2, 2, 2, 2, 2, 2, 2, −9, −10}
The linear convolution is given by
x(n) ∗h(n) = {1
↑
, 2, 2, 2, 2, 2, 2, 2, 2, 2, −9, −10}
which agrees with the overlap-save method.
□
5.5.3 MATLAB IMPLEMENTATION
Using this example as a guide, we can develop a MATLAB function to
implement the overlap-save method for a very long input sequence x(n).
The key step in this function is to obtain a proper indexing for the
segmentation. Given x(n) for n ≥0, we have to set the ﬁrst (M −1)
samples to zero to begin the block processing. Let this augmented se-
quence be
ˆx(n)
△= {0, 0, . . . , 0
&
'(
)
(M−1) zeros
, x(n)},
n ≥0
and let L = N −M + 1; then the kth block xk(n),
0 ≤n ≤N −1, is
given by
xk(n) = ˆx(m);
kL ≤m ≤kL + N −1, k ≥0, 0 ≤n ≤N −1
The total number of blocks is given by
K =
/Nx + M −2
L
0
+ 1
where Nx is the length of x(n) and ⌊·⌋is the truncation operation. Now
each block can be circularly convolved with h(n) using the circonvt
function developed earlier to obtain
yk(n) = xk(n) N⃝h(n)
Finally, discarding the ﬁrst (M −1) samples from each yk(n) and con-
catenating the remaining samples, we obtain the linear convolution y(n).
This procedure is incorporated in the following ovrlpsav function.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

186
Chapter 5
THE DISCRETE FOURIER TRANSFORM
function [y] = ovrlpsav(x,h,N)
% Overlap-Save method of block convolution
% ----------------------------------------
% [y] = ovrlpsav(x,h,N)
% y = output sequence
% x = input sequence
% h = impulse response
% N = block length
%
Lenx = length(x); M = length(h);
M1 = M-1; L = N-M1;
h = [h zeros(1,N-M)];
%
x = [zeros(1,M1), x, zeros(1,N-1)]; % Preappend (M-1) zeros
K = floor((Lenx+M1-1)/(L));
% # of blocks
Y = zeros(K+1,N);
% Convolution with succesive blocks
for k=0:K
xk = x(k*L+1:k*L+N);
Y(k+1,:) = circonvt(xk,h,N);
end
Y = Y(:,M:N)’;
% Discard the first (M-1) samples
y = (Y(:))’;
% Assemble output
Note: The ovrlpsav function as developed here is not the most eﬃcient
approach. We will come back to this issue when we discuss the fast Fourier
transform.
□
EXAMPLE 5.19
To verify the operation of the ovrlpsav function, let us consider the sequences
given in Example 5.18.
Solution
MATLAB script:
>> n = 0:9; x = n+1; h = [1,0,-1]; N = 6;
y = ovrlpsav(x,h,N)
y =
1
2
2
2
2
2
2
2
2
2
-9
-10
This is the correct linear convolution as expected.
□
There is an alternate method called an overlap-add method of block
convolutions. In this method, the input sequence x(n) is partitioned into
nonoverlapping blocks and convolved with the impulse response. The re-
sulting output blocks are overlapped with the subsequent sections and
added to form the overall output. This is explored in Problem P5.32.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
187
5.6 THE FAST FOURIER TRANSFORM
The DFT (5.24) introduced earlier is the only transform that is discrete in
both the time and the frequency domains and is deﬁned for ﬁnite-duration
sequences. Although it is a computable transform, the straightforward
implementation of (5.24) is very ineﬃcient, especially when the sequence
length N is large. In 1965, Cooley and Tukey [8] showed a procedure to
substantially reduce the amount of computations involved in the DFT.
This led to the explosion of applications of the DFT, including in the
digital signal processing area. Furthermore, it also led to the development
of other eﬃcient algorithms. All these eﬃcient algorithms are collectively
known as fast Fourier transform (FFT) algorithms.
Consider an N-point sequence x(n). Its N-point DFT is given by
(5.24) and reproduced here,
X(k) =
N−1
	
n=0
x(n)W nk
N ,
0 ≤k ≤N −1
(5.46)
where WN = e−j2π/N. To obtain one sample of X(k), we need N complex
multiplications and (N−1) complex additions. Hence to obtain a complete
set of DFT coeﬃcients, we need N 2 complex multiplications and N(N −1)
≃N 2 complex additions. Also, one has to store N 2 complex coeﬃcients
*
W nk
N
+
(or generate internally at an extra cost). Clearly, the number of
DFT computations for an N-point sequence depends quadratically on N,
which will be denoted by the notation
CN = o

N 2
For large N, o

N 2
is unacceptable in practice. Generally, the pro-
cessing time for one addition is much less than that for one multiplication.
Hence from now on we will concentrate on the number of complex mul-
tiplications, which itself requires four real multiplications and two real
additions.
5.6.1 GOAL OF AN EFFICIENT COMPUTATION
In an eﬃciently designed algorithm, the number of computations should
be constant per data sample, and therefore the total number of compu-
tations should be linear with respect to N.
The quadratic dependence on N can be reduced by realizing that most
of the computations (which are done again and again) can be eliminated
using the periodicity property
W kn
N = W k(n+N)
N
= W (k+N)n
N
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

188
Chapter 5
THE DISCRETE FOURIER TRANSFORM
and the symmetry property
W kn+N/2
N
= −W kn
N
of the factor
*
W nk
N
+
.
One algorithm that considers only the periodicity of W nk
N
is the
Goertzel algorithm. This algorithm still requires CN = o(N 2) multi-
plications, but it has certain advantages. This algorithm is described in
Chapter 12. We ﬁrst begin with an example to illustrate the advantages of
the symmetry and periodicity properties in reducing the number of com-
putations. We then describe and analyze two speciﬁc FFT algorithms that
require CN = o(N log N) operations. They are the decimation-in-time
(DIT-FFT) and decimation-in-frequency (DIF-FFT) algorithms.
□
EXAMPLE 5.20
Let us discuss the computations of a four-point DFT and develop an eﬃcient
algorithm for its computation.
X(k) =
3

n=0
x(n)W nk
4 ,
0 ≤k ≤3;
W4 = e−j2π/4 = −j
Solution
These computations can be done in the matrix form
⎡
⎢⎢⎢⎣
X(0)
X(1)
X(2)
X(3)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
W 0
4 W 0
4 W 0
4 W 0
4
W 0
4 W 1
4 W 2
4 W 3
4
W 0
4 W 2
4 W 4
4 W 6
4
W 0
4 W 3
4 W 6
4 W 9
4
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
⎤
⎥⎥⎥⎦
which requires 16 complex multiplications.
Eﬃcient Approach
Using periodicity,
W 0
4 = W 4
4 = 1
; W 1
4 = W 9
4 = −j
W 2
4 = W 6
4 = −1 ; W 3
4 = j
and substituting in the above matrix form, we get
⎡
⎢⎢⎢⎣
X(0)
X(1)
X(2)
X(3)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
1
1
1
1
1 −j −1
j
1 −1
1 −1
1
j −1 −j
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
x(0)
x(1)
x(2)
x(3)
⎤
⎥⎥⎥⎦
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
189
Using symmetry, we obtain
X(0) = x(0) + x(1) + x(2) + x(3) = [x(0) + x(2)


]
g1
+ [x(1) + x(3)



g2
]
X(1) = x(0) −jx(1) −x(2) + jx(3) = [x(0) −x(2)


]
h1
−j[x(1) −x(3)



h2
]
X(2) = x(0) −x(1) + x(2) −x(3) = [x(0) + x(2)


]
g1
−[x(1) + x(3)



g2
]
X(3) = x(0) + jx(1) −x(2) −jx(3) = [x(0) −x(2)


]
h1
+ j[x(1) −x(3)



h2
]
Hence an eﬃcient algorithm is
Step 1
g1 = x(0) + x(2)
g2 = x(1) + x(3)
h1 = x(0) −x(2)
h2 = x(1) −x(3)

Step 2
X(0) = g1 + g2
X(1) = h1 −jh2
X(2) = g1 −g2
X(3) = h1 + jh2
(5.47)
which requires only two complex multiplications, which is a considerably smaller
number, even for this simple example. A signal ﬂowgraph structure for this
algorithm is given in Figure 5.19.
An Interpretation
This eﬃcient algorithm (5.47) can be interpreted diﬀer-
ently. First, a four-point sequence x(n) is divided into two two-point sequences,
which are arranged into column vectors as shown here:

x(0)
x(2)

,

x(1)
x(3)

=

x(0) x(1)
x(2) x(3)

X(0)
x(0)
X(1)
x(2)
−1
g1
h1
X(2)
x(1)
X(3)
x(3)
−1
g 2
h2
−j
−1
j
FIGURE 5.19
Signal ﬂowgraph in Example 5.20
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

190
Chapter 5
THE DISCRETE FOURIER TRANSFORM
Second, a smaller two-point DFT of each column is taken.
W2

x(0) x(1)
x(2) x(3)

=

1
1
1 −1
 
x(0) x(1)
x(2) x(3)

=

x(0) + x(2) x(1) + x(3)
x(0) −x(2) x(1) −x(3)

=

g1 g2
h1 h2

Then each element of the resultant matrix is multiplied by {W pq
4 }, where p is
the row index and q is the column index; that is, the following dot-product is
performed:

1
1
1 −j

· ∗

g1 g2
h1 h2

=

g1
g2
h1 −jh2

Finally, two more smaller two-point DFTs are taken of row vectors.

g1
g2
h1 −jh2

W2 =

g1
g2
h1 −jh2
 ⎡
⎣
1
1
1 −1
⎤
⎦=

g1 + g2
g1 −g2
h1 −jh2 h1 + jh2

=

X(0) X(2)
X(1) X(3)

Although this interpretation seems to have more multiplications than the eﬃ-
cient algorithm, it does suggest a systematic approach of computing a larger
DFT based on smaller DFTs.
□
5.6.2 DIVIDE-AND-COMBINE APPROACH
To reduce the DFT computation’s quadratic dependence on N, one must
choose a composite number N = LM since
L2 + M 2 ≪N 2
for large N
Now divide the sequence into M smaller sequences of length L, compute
M smaller L-point DFTs, and then combine these into a larger DFT
using L smaller M-point DFTs. This is the essence of the divide-and-
combine approach. Let N = LM; then the indices n and k in (5.46) can
be written as
n = ℓ+ Lm, 0 ≤ℓ≤L −1, 0 ≤m ≤M −1
k = q + Mp, 0 ≤p ≤L −1, 0 ≤q ≤M −1
(5.48)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
191
and write sequences x(n) and X(k) as arrays x(ℓ, m) and X(p, q), respec-
tively. Then (5.46) can be written as
X(p, q) =
L−1
	
ℓ=0
M−1
	
m=0
x(ℓ, m)W (ℓ+Lm)(q+Mp)
N
=
L−1
	
ℓ=0

W ℓq
N
M−1
	
m=0
x(ℓ, m)W Lmq
N

W Mℓp
N
=
L−1
	
ℓ=0
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
W ℓq
N
M−1
	
m=0
x(ℓ, m)W mq
M

&
'(
)
M-point DFT
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
W ℓp
L
&
'(
)
L-point DFT
(5.49)
Hence (5.49) can be implemented as a three-step procedure:
1. First, we compute the M-point DFT array
F(ℓ, q)△
M−1
	
m=0
x(ℓ, m)W mq
M ;
0 ≤q ≤M −1
(5.50)
for each of the rows ℓ= 0, . . . , L −1.
2. Second, we modify F(ℓ, q) to obtain another array.
G(ℓ, q) = W ℓq
N F(ℓ, q),
0 ≤ℓ≤L −1
0 ≤q ≤M −1
(5.51)
The factor W ℓq
N is called a twiddle factor.
3. Finally, we compute the L-point DFTs
X(p, q) =
L−1
	
ℓ=0
G(ℓ, q)W ℓp
L
0 ≤p ≤L −1
(5.52)
for each of the columns q = 0, . . . , M −1.
The total number of complex multiplications for this approach can now
be given by
CN = LM 2 + N + ML2 < o

N 2
(5.53)
We illustrate this approach in the following example.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

192
Chapter 5
THE DISCRETE FOURIER TRANSFORM
□
EXAMPLE 5.21
Develop the divide-and-combine FFT algorithm for N = 15.
Solution
Let L = 3 and M = 5. Then from (5.48), we have
n = ℓ+ 3M,
0 ≤ℓ≤2,
0 ≤m ≤4
k = q + 5p,
0 ≤p ≤2,
0 ≤q ≤4
(5.54)
Hence (5.49) becomes
X(p, q) =
2
	
ℓ=0

W ℓq
15

4
	
m=0
x(ℓ, m)W mq
5

W ℓp
3
(5.55)
To implement (5.55), we arrange the given sequence x(n) in the form of
an array {x(ℓ, m)} using a column-wise ordering as
x(0)
x(3)
x(6)
x(9)
x(12)
x(1)
x(4)
x(7)
x(10)
x(13)
x(2)
x(5)
x(8)
x(11)
x(14)
(5.56)
The ﬁrst step is to compute ﬁve-point DFTs F(ℓ, q) for each of the three
rows and arrange them back in the same array formation
F(0, 0) F(0, 1) F(0, 2) F(0, 3) F(0, 4)
F(1, 0) F(1, 1) F(1, 2) F(1, 3) F(1, 4)
F(2, 0) F(2, 1) F(2, 2) F(2, 3) F(2, 4)
(5.57)
which requires a total of 3×52 = 75 complex operations. The second step
is to modify F(ℓ, q) to obtain the array G(ℓ, q) using the twiddle factors
W ℓq
15
G(0, 0) G(0, 1) G(0, 2) G(0, 3) G(0, 4)
G(1, 0) G(1, 1) G(1, 2) G(1, 3) G(1, 4)
G(2, 0) G(2, 1) G(2, 2) G(2, 3) G(2, 4)
(5.58)
which requires 15 complex operations. The last step is to perform three-
point DFTs X(p, q) for each of the ﬁve columns to obtain
X(0, 0) X(0, 1) X(0, 2) X(0, 3) X(0, 4)
X(1, 0) X(1, 1) X(1, 2) X(1, 3) X(1, 4)
X(2, 0) X(2, 1) X(2, 2) X(2, 3) X(2, 4)
(5.59)
using a total of 5 × 32 = 45 complex operations. According to (5.54), the
array in (5.59) is a rearrangement of X(k) as
X(0)
X(1)
X(2)
X(3)
X(4)
X(5)
X(6)
X(7)
X(8)
X(9)
X(10) X(11) X(12) X(13) X(14)
(5.60)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
193
Finally, after “unwinding” this array in the row-wise fashion, we obtain
the required 15-point DFT X(k). The total number of complex operations
required for this divide-and-combine approach is 135, whereas the direct
approach for the 15-point DFT requires 225 complex operations. Thus
the divide-and-combine approach is clearly eﬃcient.
□
The divide-and-combine procedure can be further repeated if M or L
are composite numbers. Clearly, the most eﬃcient algorithm is obtained
when N is a highly composite number, that is, N = Rν. Such algorithms
are called radix-R FFT algorithms. When N = Rν1
1 Rν2
2 . . ., then such de-
compositions are called mixed-radix FFT algorithms. The one most pop-
ular and easily programmable algorithm is the radix-2 FFT algorithm.
5.6.3 RADIX-2 FFT ALGORITHM
Let N = 2ν; then we choose L = 2 and M = N/2 and divide x(n) into
two N/2-point sequences according to (5.48) as
g1(n) = x(2n)
g2(n) = x(2n + 1);
0 ≤n ≤N
2 −1
The sequence g1(n) contains even-ordered samples of x(n), while g2(n)
contains odd-ordered samples of x(n). Let G1(k) and G2(k) be N/2-point
DFTs of g1(n) and g2(n), respectively. Then (5.49) reduces to
X(k) = G1(k) + W k
NG2(k),
0 ≤k ≤N −1
(5.61)
This is called a merging formula, which combines two N/2-point DFTs
into one N-point DFT. The total number of complex multiplications and
additions reduces to
CN = N 2
2 + N = o

N 2/2

This procedure can be repeated again and again. At each stage, the
sequences are decimated and the smaller DFTs combined. This decima-
tion ends after ν stages when we have N one-point sequences, which are
also one-point DFTs. The resulting procedure is called the decimation-in-
time FFT (DIT-FFT) algorithm, for which the total number of complex
multiplications is
CN = Nν = N log2 N
Clearly, if N is large, then CN is approximately linear in N, which was
the goal of our eﬃcient algorithm. Using additional symmetries, CN can
be reduced to N
2 log2 N. The signal ﬂowgraph for this algorithm is shown
in Figure 5.20 for N = 8.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

194
Chapter 5
THE DISCRETE FOURIER TRANSFORM
X(0)
x(0)
WN
0
WN
1
WN
2
WN
3
WN
4
WN
5
WN
6
WN
7
WN
6
WN
4
WN
0
WN
4
WN
0
WN
4
WN
0
WN
4
WN
0
WN
4
WN
2
WN
0
WN
6
WN
4
WN
2
WN
0
X(1)
x(4)
X(2)
x(2)
X(3)
x(6)
X(4)
x(1)
X(5)
x(5)
X(6)
x(3)
X(7)
x(7)
FIGURE 5.20
Decimation-in-time FFT structure for N = 8
In an alternate approach, we choose M = 2, L = N/2 and follow the
steps in (5.49). Note that the initial DFTs are two-point DFTs, which
contain no complex multiplications. From (5.50),
F(0, m) = x(0, m) + x(1, m)W 0
2
= x(n) + x(n + N/2),
0 ≤n ≤N/2
F(1, m) = x(0, m) + x(1, m)W 1
2
= x(n) −x(n + N/2),
0 ≤n ≤N/2
and from (5.51),
G(0, m) = F(0, m)W 0
N
= x(n) + x(n + N/2),
0 ≤n ≤N/2
G(1, m) = F(1, m)W m
N
= [x(n) −x(n + N/2)] W n
N,
0 ≤n ≤N/2
(5.62)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
195
Let G(0, m) = d1(n) and G(1, m) = d2(n) for 0 ≤n ≤N/2 −1 (since
they can be considered as time-domain sequences); then from (5.52) we
have
X(0, q) = X(2q)
= D1(q)
X(1, q) = X(2q + 1) = D2(q)
(5.63)
This implies that the DFT values X(k) are computed in a decimated
fashion. Therefore, this approach is called a decimation-in-frequency FFT
(DIF-FFT) algorithm. Its signal ﬂowgraph is a transposed structure of
the DIT-FFT structure, and its computational complexity is also equal
to N
2 log2 N.
5.6.4 MATLAB IMPLEMENTATION
MATLAB provides a function called fft to compute the DFT of a vec-
tor x. It is invoked by X = fft(x,N), which computes the N-point DFT.
If the length of x is less than N, then x is padded with zeros. If the argu-
ment N is omitted, then the length of the DFT is the length of x. If x is a
matrix, then fft(x,N) computes the N-point DFT of each column of x.
This fft function is written in machine language and not using
MATLAB commands (i.e., it is not available as a .m ﬁle). Therefore, it
executes very fast. It is written as a mixed-radix algorithm. If N is a
power of two, then a high-speed radix-2 FFT algorithm is employed. If
N is not a power of two, then N is decomposed into prime factors and
a slower mixed-radix FFT algorithm is used. Finally, if N is a prime
number, then the fft function is reduced to the raw DFT algorithm.
The inverse DFT is computed using the ifft function, which has the
same characteristics as fft.
□
EXAMPLE 5.22
In this example, we will study the execution time of the fft function for 1 ≤
N ≤2048. This will reveal the divide-and-combine strategy for various values
of N. One note of caution. The results obtained in this example are valid only
for MATLAB Versions 5 and earlier. Beginning in Version 6, MATLAB is using
a new numerical computing core called LAPACK. It is optimized for memory
references and cache usage and not for individual ﬂoating-point operations.
Therefore, results for Version 6 and later are diﬃcult to interpret. Also, the
execution times given here are for a speciﬁc computer and may vary on diﬀerent
computers.
Solution
To determine the execution time, MATLAB provides two functions. The clock
function provides the instantaneous clock reading, while the etime(t1,t2) func-
tion computes the elapsed time between two time marks t1 and t2. To determine
the execution time, we will generate random vectors from length 1 through 2048,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

196
Chapter 5
THE DISCRETE FOURIER TRANSFORM
compute their FFTs, and save the computation time in an array. Finally, we
will plot this execution time versus N.
MATLAB script:
>> Nmax = 2048;
fft_time=zeros(1,Nmax);
>> for n=1:1:Nmax
>>
x=rand(1,n);
>>
t=clock;fft(x);fft_time(n)=etime(clock,t);
>> end
>> n=[1:1:Nmax];
plot(n,fft_time,’.’)
>> xlabel(’N’);ylabel(’Time in Sec’);
title(’FFT Execution Times’)
The plot of the execution times is shown in Figure 5.21. This plot is very
informative. The points in the plot do not show one clear function but appear
to group themselves into various trends. The uppermost group depicts a o(N 2)
dependence on N, which means that these values must be prime numbers be-
tween 1 and 2048 for which the FFT algorithm defaults to the DFT algorithm.
Similarly, there are groups corresponding to the o

N 2/2
 
, o

N 2/3
 
, o

N 2/4
 
,
and so on, dependencies for which the number N has fewer decompositions.
The last group shows the (almost linear) o (N log N) dependence, which is for
0
500
1000
1500
2000
2500
0
5
10
15
20
25
30
35
40
45
50
N
Time in sec
FFT Execution Times
o(N*N)
o(N*N/2)
o(N*N/4)
o(N*logN)
FIGURE 5.21
FFT execution times for 1 <= N <= 2048
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
197
N = 2ν, 0 ≤ν ≤11. For these values of N, the radix-2 FFT algorithm is used.
For all other values, a mixed-radix FFT algorithm is employed. This shows that
the divide-and-combine strategy is very eﬀective when N is highly composite.
For example, the execution time is 0.16 sec for N = 2048, 2.48 sec for N = 2047,
and 46.96 sec for N = 2039.
□
The MATLAB functions developed previously in this chapter should
now be modiﬁed by substituting the fft function in place of the dft
function. From the preceding example, care must be taken to use a highly
composite N. A good practice is to choose N = 2ν unless a speciﬁc
situation demands otherwise.
5.6.5 FAST CONVOLUTIONS
The conv function in MATLAB is implemented using the filter function
(which is written in C) and is very eﬃcient for smaller values of N (<50).
For larger values of N, it is possible to speed up the convolution using the
FFT algorithm. This approach uses the circular convolution to implement
the linear convolution, and the FFT to implement the circular convolu-
tion. The resulting algorithm is called a fast convolution algorithm. In
addition, if we choose N = 2ν and implement the radix-2 FFT, then the
algorithm is called a high-speed convolution. Let x1 (n) be an N1-point
sequence, and let x2 (n) be an N2-point sequence; then for high-speed
convolution N is chosen to be
N = 2⌈log2(N1+N2−1)⌉
(5.64)
where ⌈x⌉is the smallest integer greater than x (also called a ceiling
function). The linear convolution x1 (n) ∗x2 (n) can now be implemented
by two N-point FFTs, one N-point IFFT, and one N-point dot-product.
x1 (n) ∗x2 (n) = IFFT [FFT [x1 (n)] · FFT [x2 (n)]]
(5.65)
For large values of N, (5.65) is faster than the time-domain convolution,
as we see in the following example.
□
EXAMPLE 5.23
To demonstrate the eﬀectiveness of the high-speed convolution, let us compare
the execution times of two approaches. Let x1 (n) be an L-point uniformly
distributed random number between [0, 1], and let x2 (n) be an L-point Gaussian
random sequence with mean 0 and variance 1. We will determine the average
execution times for 1 ≤L ≤150, in which the average is computed over the
100 realizations of random sequences. (Please see the cautionary note given in
Example 5.22.)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

198
Chapter 5
THE DISCRETE FOURIER TRANSFORM
Solution
MATLAB script:
conv_time = zeros(1,150); fft_time = zeros(1,150);
%
for L = 1:150
tc = 0; tf=0;
N = 2*L-1; nu = ceil(log10(N)/log10(2)); N = 2ˆnu;
for I=1:100
h = randn(1,L);
x = rand(1,L);
t0 = clock; y1 = conv(h,x); t1=etime(clock,t0);
tc = tc+t1;
t0 = clock; y2 = ifft(fft(h,N).*fft(x,N)); t2=etime(clock,t0);
tf = tf+t2;
end
%
conv_time(L)=tc/100;
fft_time(L)=tf/100;
end
%
n = 1:150; subplot(1,1,1);
plot(n(25:150),conv_time(25:150),n(25:150),fft_time(25:150))
Figure 5.22 shows the linear convolution and the high-speed convolution times
for 25 ≤L ≤150. It should be noted that these times are aﬀected by the
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
50
100
150
Sequence Length N
Time in sec
Comparison of Convolution Times
Convolution
High–speed convolution
FIGURE 5.22
Comparison of linear and high-speed convolution times
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Fast Fourier Transform
199
computing platform used to execute the MATLAB script. The plot in Figure 5.22
was obtained on a 33 MHz 486 computer. It shows that for low values of L
the linear convolution is faster. The crossover point appears to be L = 50,
beyond which the linear convolution time increases exponentially, while the
high-speed convolution time increases fairly linearly. Note that since N = 2ν,
the high-speed convolution time is constant over a range on L.
□
5.6.6 HIGH-SPEED BLOCK CONVOLUTIONS
Earlier we discussed a block convolution algorithm called the overlap-and-
save method (and its companion the overlap-and-add method), which is
used to convolve a very large sequence with a relatively smaller sequence.
The MATLAB function ovrlpsav developed in that section uses the DFT
to implement the linear convolution. We can now replace the DFT by the
radix-2 FFT algorithm to obtain a high-speed overlap-and-save algorithm.
To further reduce the computations, the FFT of the shorter (ﬁxed) se-
quence can be computed only once. The following hsolpsav function
shows this algorithm.
function [y] =
hsolpsav(x,h,N)
% High-speed Overlap-Save method of block convolutions using FFT
% --------------------------------------------------------------
% [y] = hsolpsav(x,h,N)
% y = output sequence
% x = input sequence
% h = impulse response
% N = block length (must be a power of two)
%
N = 2ˆ(ceil(log10(N)/log10(2));
Lenx = length(x); M = length(h);
M1 = M-1; L = N-M1;
h = fft(h,N);
%
x = [zeros(1,M1), x, zeros(1,N-1)];
K = floor((Lenx+M1-1)/(L)); % # of blocks
Y = zeros(K+1,N);
for k=0:K
xk = fft(x(k*L+1:k*L+N));
Y(k+1,:) = real(ifft(xk.*h));
end
Y = Y(:,M:N)’; y = (Y(:))’;
A similar modiﬁcation can be done to the overlap-and-add algorithm.
MATLAB also provides the function fftfilt to implement the overlap-
and-add algorithm.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

200
Chapter 5
THE DISCRETE FOURIER TRANSFORM
5.7 PROBLEMS
P5.1
Compute the DFS coeﬃcients of the following periodic sequences using the DFS deﬁnition,
and then verify your answers using MATLAB.
1. ˜x1(n) = {4, 1, −1, 1}, N = 4
2. ˜x2(n) = {2, 0, 0, 0, −1, 0, 0, 0}, N = 8
3. ˜x3(n) = {1, 0, −1, −1, 0}, N = 5
4. ˜x4(n) = {0, 0, 2j, 0, 2j, 0}, N = 6
5. ˜x5(n) = {3, 2, 1}, N = 3
P5.2
Determine the periodic sequences given the following periodic DFS coeﬃcients. First use
the IDFS deﬁnition, and then verify your answers using MATLAB.
1.
˜
X1(k) = {4, 3j, −3j}, N = 3
2.
˜
X2(k) = {j, 2j, 3j, 4j}, N = 4
3.
˜
X3(k) = {1, 2 + 3j, 4, 2 −3j}, N = 4
4.
˜
X4(k) = {0, 0, 2, 0, 0}, N = 5
5.
˜
X5(k) = {3, 0, 0, 0, −3, 0, 0, 0}, N = 8
P5.3
Let ˜x1(n) be periodic with fundamental period N = 40, where one period is given by
˜x1(n) =
! 5 sin(0.1πn), 0 ≤n ≤19
0,
20 ≤n ≤39
and let ˜x2(n) be periodic with fundamental period N = 80, where one period is given by
˜x2(n) =
! 5 sin(0.1πn), 0 ≤n ≤19
0,
20 ≤n ≤79
These two periodic sequences diﬀer in their periodicity but otherwise have the same
nonzero samples.
1. Compute the DFS ˜
X1(k) of ˜x1(n), and plot samples (using the stem function) of its
magnitude and angle versus k.
2. Compute the DFS ˜
X2(k) of ˜x2(n), and plot samples of its magnitude and angle versus k.
3. What is the diﬀerence between the two preceding DFS plots?
P5.4
Consider the periodic sequence ˜x1(n) given in Problem P5.3. Let ˜x2(n) be periodic with
fundamental period N = 40, where one period is given by
˜x2(n) =
!
˜x1(n),
0 ≤n ≤19
−˜x1(n −20), 20 ≤n ≤39
1. Determine analytically the DFS ˜
X2(k) in terms of ˜
X1(k).
2. Compute the DFS ˜
X2(k) of ˜x2(n), and plot samples of its magnitude and angle versus k.
3. Verify your answer in part 1 using the plots of ˜
X1(k) and ˜
X2(k)?
P5.5
Consider the periodic sequence ˜x1(n) given in Problem P5.3. Let ˜x3(n) be periodic with
period 80, obtained by concatenating two periods of ˜x1(n), that is,
˜x3(n) = [˜x1(n), ˜x1(n)]PERIODIC
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
201
Clearly, ˜x3(n) is diﬀerent from ˜x2(n) of Problem P5.3 even though both of them are
periodic with period 80.
1. Compute the DFS ˜
X3(k) of ˜x3(n), and plot samples of its magnitude and angle versus k.
2. What eﬀect does the periodicity doubling have on the DFS?
3. Generalize this result to M-fold periodicity. In particular, show that if
˜xM(n) =
⎡
⎢⎣˜x1(n), ˜x1(n), . . . , ˜x1(n)



M times
⎤
⎥⎦
PERIODIC
then
˜
XM (Mk) = M ˜
X1(k), k = 0, 1, . . . , N −1
˜
XM(k)
= 0,
k ̸= 0, M, . . . , MN
P5.6
Let X(ejω) be the DTFT of a ﬁnite-length sequence
x(n) =
⎧
⎨
⎩
n + 1,
0 ≤n ≤49
100 −n, 50 ≤n ≤99
0,
otherwise
1. Let
y1(n) =
10-point
IDFS
"
X(ej0), X(ej2π/10), X(ej4π/10), . . . , X(ej18π/10)
#
Determine y1(n) using the frequency sampling theorem. Verify your answer using
MATLAB.
2. Let
y2(n) =
200-point
IDFS
"
X(ej0), X(ej2π/200), X(ej4π/200), . . . , X(ej398π/200)
#
Determine y2(n) using the frequency sampling theorem. Verify your answer using
MATLAB.
3. Comment on your results in parts (a) and (b).
P5.7
Let ˜x(n) be a periodic sequence with period N, and let
˜y(n)
△= ˜x(−n) = ˜x(N −n)
that is, ˜y(n) is a periodically folded version of ˜x(n). Let ˜
X(k) and ˜Y (k) be the DFS
sequences.
1. Show that
˜Y (k) = ˜
X(−k) = ˜
X(N −k)
that is, ˜Y (k) is also a periodically folded version of ˜
X(k).
2. Let ˜x(n) = {2
↑
, 4, 6, 1, 3, 5}PERIODIC with N = 6.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

202
Chapter 5
THE DISCRETE FOURIER TRANSFORM
(a) Sketch ˜y(n) for 0 ≤n ≤5.
(b) Compute ˜
X(k) for 0 ≤k ≤5.
(c) Compute ˜Y (k) for 0 ≤k ≤5.
(d) Verify the relation in part 1.
P5.8
Consider the following ﬁnite-length sequence:
x(n) =
! sinc2{(n −50)/2}, 0 ≤n ≤100
0,
else
1. Determine the DFT X(k) of x(n). Plot (using the stem function) its magnitude and
phase.
2. Plot the magnitude and phase of the DTFT X(ejω) of x(n) using MATLAB.
3. Verify that the above DFT is the sampled version of X(ejω). It might be helpful to
combine the above two plots in one graph using the hold function.
4. Is it possible to reconstruct the DTFT X(ejω) from the DFT X(k)? If possible, give the
necessary interpolation formula for reconstruction. If not possible, state why this
reconstruction cannot be done.
P5.9
Let a ﬁnite-length sequence be given by
x(n) =
!
2e−0.9|n|, −5 ≤n ≤5
0,
otherwise
Plot the DTFT X(ejω) of the above sequence using DFT as a computation tool. Choose the
length N of the DFT so that this plot appears to be a smooth graph.
P5.10 Plot the DTFT magnitude and angle of each of the following sequences using the DFT as a
computation tool. Make an educated guess about the length N so that your plots are
meaningful.
1. x(n) = (0.6)|n| [u(n + 10) −u(n −11)]
2. x(n) = n(0.9)n [u(n) −u(n −21)]
3. x(n) = [cos(0.5πn) + j sin(0.5πn)][u(n) −u(n −51)]
4. x(n) = {1, 2, 3, 4
↑
, 3, 2, 1}
5. x(n) = {−1, −2, −3, 0
↑
, 3, 2, 1}
P5.11 Let H(ejω) be the frequency response of a real, causal discrete-time LSI system.
1. If
Re
$
H

ejω%
=
5

k=0
(0.9)k cos (kω)
determine the impulse response h(n) analytically. Verify your answer using DFT as a
computation tool. Choose the length N appropriately.
2. If
Im
$
H

ejω%
=
5

ℓ=0
2ℓsin (ℓω) ,
and
& π
−π
H(ejω)dω = 0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
203
determine the impulse response h(n) analytically. Verify your answer using DFT as a
computation tool. Again choose the length N appropriately.
P5.12 Let X(k) denote the N-point DFT of an N-point sequence x(n). The DFT X(k) itself is an
N-point sequence.
1. If the DFT of X(k) is computed to obtain another N-point sequence x1(n), show that
x1(n) = Nx((−n))N,
0 ≤n ≤N −1
2. Using this property, design a MATLAB function to implement an N-point circular
folding operation x2(n) = x1((−n))N. The format should be
x2 = circfold(x1,N)
% Circular folding using DFT
% x2 = circfold(x1,N)
% x2 = circularly folded output sequence
% x1 = input sequence of length <= N
%
N = circular buffer length
3. Determine the circular folding of the following sequence:
x1(n) = {1, 3, 5, 7, 9, −7, −5, −3, −1}
P5.13 Let X(k) be an N-point DFT of an N-point sequence x(n). Let N be an even integer.
1. If x(n) = x(n + N/2) for all n, then show that X(k) = 0 for k odd (i.e., nonzero for k
even). Verify this result for x(n) = {1, 2, −3, 4, 5, 1, 2, −3, 4, 5}.
2. If x(n) = −x(n + N/2) for all n, then show that X(k) = 0 for k even (i.e., nonzero for
k odd). Verify this result for x(n) = {1, 2, −3, 4, 5, −1, −2, 3, −4, −5}.
P5.14 Let X(k) be an N-point DFT of an N-point sequence x(n). Let N = 4ν where ν is an
integer.
1. If x(n) = x(n + ν) for all n, then show that X(k) is nonzero for k = 4ℓfor 0 ≤ℓ≤ν −1.
Verify this result for x(n) = {1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3}.
2. If x(n) = −x(n + ν) for all n, then show that X(k) is nonzero for k = 4ℓ+ 2 for
0 ≤ℓ≤ν −1. Verify this result for x(n) = {1, 2, 3, −1, −2, −3, 1, 2, 3, −1, −2, −3}.
P5.15 Let X(k) be an N-point DFT of an N-point sequence x(n). Let N = 2µν where µ and ν are
integers.
1. If x(n) = x(n + ν) for all n, then show that X(k) is nonzero for k = 2(µℓ) for 0 ≤ℓ≤
ν −1. Verify this result for x(n) = {1, −2, 3, 1, −2, 3, 1, −2, 3, 1, −2, 3, 1, −2, 3, 1, −2, 3}.
2. If x(n) = −x(n + ν) for all n, then show that X(k) is nonzero for k = 2(µℓ+ 1) for
0 ≤ℓ≤ν −1. Verify this result for x(n) = {1, −2, 3, −1, 2, −3, 1, −2, 3, −1, 2, −3, 1, −2,
3, −1, 2, −3}.
P5.16 Let X(k) and Y (k) be 10-point DFTs of two 10-point sequences x(n) and y(n),
respectively. If
X(k) = exp(j0.2πk),
0 ≤k ≤9
determine Y (k) in each of the following cases without computing the DFT.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

204
Chapter 5
THE DISCRETE FOURIER TRANSFORM
1. y(n) = x((n −5))10
2. y(n) = x((n + 4))10
3. y(n) = x((3 −n))10
4. y(n) = x(n)ej3πn/5
5. y(n) = x(n) 10
⃝x((−n))10
Verify your answers using MATLAB.
P5.17 The ﬁrst six values of the 10-point DFT of a real-valued sequence x(n) are given by
{10, −2 + j3, 3 + j4, 2 −j3, 4 + j5, 12}
Determine the DFT of each of the following sequences using DFT properties.
1. x1(n) = x((2 −n))10
2. x2(n) = x((n + 5))10
3. x3(n) = x(n)x((−n))10
4. x4(n) = x(n) 10
⃝x((−n))10
5. x5(n) = x(n)e−j4πn/5
P5.18 Complex-valued N-point sequence x(n) can be decomposed into N-point circular-conjugate-
symmetric and circular-conjugate-antisymmetric sequences using the following relations:
xccs(n)
△= 1
2 [x(n) + x∗((−n))N]
xcca(n)
△= 1
2 [x(n) −x∗((−n))N]
If XR(k) and XI(k) are the real and imaginary parts of the N-point DFT of x(n), then
DFT [xccs(n)] = XR(k)
and
DFT [xcca(n)] = jXI(k)
1. Prove these relations property analytically.
2. Modify the circevod function developed in the chapter so that it can be used for
complex-valued sequences.
3. Let X(k) = [3 cos(0.2πk) + j4 sin(0.1πk)][u(k) −u(k −20)] be a 20-point DFT. Verify
this symmetry property using your circevod function.
P5.19 If X(k) is the N-point DFT of an N-point complex-valued sequence
x(n) = xR(n) + jxI(n)
where xR(n) and xI(n) are the real and imaginary parts of x(n), then
DFT [xR(n)] = Xccs(k)
and
DFT [jxI(n)] = Xcca(k)
where Xccs(k) and Xcca(k) are the circular-even and circular-odd components of X(k) as
deﬁned in Problem P5.18.
1. Prove this property analytically.
2. This property can be used to compute the DFTs of two real-valued N-point sequences
using one N-point DFT operation. Speciﬁcally, let x1(n) and x2(n) be two N-point
sequences. Then we can form a complex-valued sequence
x(n) = x1(n) + jx2(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
205
and use this property. Develop a MATLAB function to implement this approach with
the following format.
function [X1,X2] = real2dft(x1,x2,N)
% DFTs of two real sequences
% [X1,X2] = real2dft(x1,x2,N)
%
X1 = n-point DFT of x1
%
X2 = n-point DFT of x2
%
x1 = sequence of length <= N
%
x2 = sequence of length <= N
%
N = length of DFT
3. Compute and plot the DFTs of the following two sequences using this function:
x1(n) = cos(0.1πn), x2(n) = sin(0.2πn);
0 ≤n ≤39
P5.20 Using the frequency domain approach, devise a MATLAB function to determine a circular
shift x((n −m))N, given an N1-point sequence x(n) where N1 ≤N. Your function should
have the following format.
function y = cirshftf(x,m,N)
%
Circular shift of m samples wrt size N in sequence x: (freq domain)
%
-------------------------------------------------------------------
%
y = cirshftf(x,m,N)
%
y : output sequence containing the circular shift
%
x : input sequence of length <= N
%
m : sample shift
%
N : size of circular buffer
%
Method: y(n) = idft(dft(x(n))*WNˆ(mk))
%
%
If m is a scalar then y is a sequence (row vector)
%
If m is a vector then y is a matrix, each row is a circular shift
%
in x corresponding to entries in vector m
%
M and x should not be matrices
Verify your function on the following sequence:
x(n) = {5
↑
, 4, 3, 2, 1, 0, 0, 1, 2, 3, 4},
0 ≤n ≤10
with (a) m = −5, N = 12 and (b) m = 8, N = 15.
P5.21 Using the analysis and synthesis equations of the DFT, show that the energy of a sequence
satisﬁes
EX
△=
N−1

n=0
|x(n)|2 = 1
N
N−1

k=0
|X(k)|2
This is commonly referred to as a Parseval’s relation for the DFT. Verify this relation using
MATLAB on the sequence in Problem P5.20.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

206
Chapter 5
THE DISCRETE FOURIER TRANSFORM
P5.22 A 512-point DFT X(k) of a real-valued sequence x(n) has the DFT values
X(0) = 20 + jα;
X(5) = 20 + j30;
X(k1) = −10 + j15;
X(152) = 17 + j23;
X(k2) = 20 −j30;
X(k3) = 17 −j23;
X(480) = −10 −j15;
X(256) = 30 + jβ
and all other values are known to be zero.
1. Determine the real-valued coeﬃcients α and β.
2. Determine the values of the integers k1, k2, and k3.
3. Determine the energy of the signal x(n).
4. Express the sequence x(n) in a closed form.
P5.23 Let x(n) be a ﬁnite length sequence given by
x(n) =

. . . , 0, 0, 0, 1
↑
, 2, −3, 4, −5, 0, . . .
'
Determine and sketch the sequence x((−8 −n))7R7 (n) where
R7 (n) =
! 1, 0 ≤n ≤6
0, elsewhere
P5.24 The circonvt function developed in this chapter implements the circular convolution as a
matrix-vector multiplication. The matrix corresponding to the circular shifts {x((n −m))N;
0 ≤n ≤N −1} has an interesting structure. This matrix is called a circulant matrix, which
is a special case of Toeplitz matrix introduced in Chapter 2.
1. Consider the sequences given in Example 5.13. Express x1(n) as a column vector x1 and
x2((n −m))N as a circulant matrix X2 with rows corresponding to n = 0, 1, 2, 3.
Characterize this matrix X2. Can it completely be described by its ﬁrst row (or column)?
2. Determine the circular convolution as X2x1 and verify your calculations.
P5.25 Develop a MATLAB function to construct a circulant matrix C given an N-point sequence
x(n). Use the toeplitz function to implement matrix C. Your subroutine function should
have the following format.
function [C] = circulnt(x,N)
% Circulant Matrix from an N-point sequence
% [C] = circulnt(x,N)
% C = circulant matrix of size NxN
% x = sequence of length <= N
% N = size of circulant matrix
Using this function, modify the circular convolution function circonvt discussed in the
chapter so that the for...end loop is eliminated. Verify your functions on the sequences in
Problem P5.24.
P5.26 Using the frequency domain approach, devise a MATLAB function to implement the
circular convolution operation between two sequences. The format of the sequence should be
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
207
function x3 = circonvf(x1,x2,N)
% Circular convolution in the frequency domain
%
x3 = circonvf(x1,x2,N)
%
x3 = convolution result of length N
%
x1 = sequence of length <= N
%
x2 = sequence of length <= N
%
N = length of circular buffer
Using your function, compute the circular convolution {4, 3, 2, 1}
4⃝{1, 2, 3, 4}.
P5.27 The following four sequences are given:
x1(n) = {1
↑
, 3, 2, −1}; x2(n) = {2
↑
, 1, 0, −1}; x3(n) = x1(n) ∗x2(n); x4(n) = x1(n)
5⃝x2(n)
1. Determine and sketch x3(n).
2. Using x3(n) alone, determine and sketch x4(n). Do not directly compute x4(n).
P5.28 Compute the N-point circular convolution for the following sequences. Plot their samples.
1. x1(n) = sin(πn/3)R6(n),
x2(n) = cos(πn/4)R8(n);
N = 10
2. x1(n) = cos (2πn/N) RN(n),
x2(n) = sin (2πn/N) RN(n);
N = 32
3. x1(n) = (0.8)n RN(n),
x2(n) = (−0.8)n RN(n);
N = 20
4. x1(n) = nRN(n),
x2(n) = (N −n) RN(n);
N = 10
5. x1(n) = (0.8)nR20,
x2(n) = u(n) −u(n −40);
N = 50
P5.29 Let x1(n) and x2(n) be two N-point sequences.
1. If y(n) = x1(n) N⃝x2(n) show that
N−1

n=0
y(n) =
(N−1

n=0
x1(n)
) (N−1

n=0
x2(n)
)
2. Verify this result for the following sequences:
x1(n) = {9, 4, −1, 4, −4, −1, 8, 3};
x2(n) = {−5, 6, 2, −7, −5, 2, 2, −2}
P5.30 Let X(k) be the eight-point DFT of a three-point sequence x(n) = {5
↑
, −4, 3}. Let Y (k) be
the eight-point DFT of a sequence y(n). Determine y(n) when Y (k) = W 5k
8 X(−k)8.
P5.31 For the following sequences, compute (i) the N-point circular convolution x3(n) = x1(n)
N⃝x2(n), (ii) the linear convolution x4(n) = x1(n) ∗x2(n), and (iii) the error sequence
e(n) = x3(n) −x4(n).
1. x1(n) = {1, 1, 1, 1} ,
x2(n) = cos (πn/4) R6(n);
N = 8
2. x1(n) = cos (2πn/N) R16(n),
x2(n) = sin (2πn/N) R16(n);
N = 32
3. x1(n) = (0.8)n R10(n),
x2(n) = (−0.8)n R10(n);
N = 15
4. x1(n) = nR10(n),
x2(n) = (N −n) R10(n);
N = 10
5. x1(n) = {1, −1, 1, −1} ,
x2(n) = {1, 0, −1, 0};
N = 5
In each case, verify that e(n) = x4 (n + N).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

208
Chapter 5
THE DISCRETE FOURIER TRANSFORM
P5.32 The overlap-add method of block convolution is an alternative to the overlap-save method.
Let x(n) be a long sequence of length ML where M, L ≫1. Divide x(n) into M segments
{xm(n), m = 1, . . . , M} each of length L
xm(n) =
! x(n), mL ≤n ≤(m + 1) L −1
0,
elsewhere
so that
x(n) =
M−1

m=0
xm(n)
Let h(n) be an L-point impulse response. Then
y(n) = x(n) ∗h(n) =
M−1

m=0
xm(n) ∗h(n) =
M−1

m=0
ym(n);
ym(n)
△= xm(n) ∗h(n)
Clearly, ym(n) is a (2L −1)-point sequence. In this method, we have to save the
intermediate convolution results and then properly overlap these before adding to form the
ﬁnal result y(n). To use DFT for this operation, we have to choose N ≥(2L −1).
1. Develop a MATLAB function to implement the overlap-add method using the circular
convolution operation. The format should be
function [y] = ovrlpadd(x,h,N)
% Overlap-Add method of block convolution
% [y] = ovrlpadd(x,h,N)
%
% y = output sequence
% x = input sequence
% h = impulse response
% N = block length >= 2*length(h)-1
2. Incorporate the radix-2 FFT implementation in this function to obtain a high-speed
overlap-add block convolution routine. Remember to choose N = 2ν.
3. Verify your functions on the following two sequences:
x(n) = cos (πn/500) R4000(n),
h(n) = {1, −1, 1, −1}
P5.33 Given the following sequences x1(n) and x2(n):
x1(n) = {2, 1, 1, 2} ,
x2(n) = {1, −1, −1, 1}
1. Compute the circular convolution x1(n) N⃝x2(n) for N = 4, 7, and 8.
2. Compute the linear convolution x1(n) ∗x2(n).
3. Using results of calculations, determine the minimum value of N necessary so that linear
and circular convolutions are the same on the N-point interval.
4. Without performing the actual convolutions, explain how you could have obtained the
result of P5.33.3.
P5.34 Let
x(n) =
! A cos (2πℓn/N), 0 ≤n ≤N −1
0,
elsewhere
= A cos (2πℓn/N) RN(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
209
where ℓis an integer. Notice that x(n) contains exactly ℓperiods (or cycles) of the cosine
waveform in N samples. This is a windowed cosine sequence containing no leakage.
1. Show that the DFT X(k) is a real sequence given by
X(k) = AN
2 δ (k −ℓ) + AN
2 δ(k −N + ℓ);
0 ≤k ≤(N −1),
0 < ℓ< N
2. Show that if ℓ= 0, then the DFT X(k) is given by
X(k) = ANδ(k);
0 ≤k ≤(N −1)
3. Explain clearly how these results should be modiﬁed if ℓ< 0 or ℓ> N.
4. Verify the results of parts 1, 2, and 3 using the following sequences. Plot the real parts of
the DFT sequences using the stem function.
(a) x1(n) = 3 cos (0.04πn) R200(n)
(b) x2(n) = 5R50(n)
(c) x3(n) = [1 + 2 cos (0.5πn) + cos (πn)] R100(n)
(d) x4(n) = cos (25πn/16) R64(n)
(e) x5(n) = [4 cos (0.1πn) −3 cos (1.9πn)] R40(n)
P5.35 Let x(n) = A cos (ω0n) RN(n), where ω0 is a real number.
1. Using the properties of the DFT, show that the real and the imaginary parts of X(k) are
given by
X(k) = XR(k) + jXI(k)
XR(k) = (A/2) cos
"
π(N−1)
N
(k −f0N)
#
sin [π (k −f0N)]
sin [π(k −f0N)/N]
+ (A/2) cos
"
π(N−1)
N
(k + f0N)
#
sin [π (k −N + f0N)]
sin [π(k −N + f0N)/N]
XI(k) = −(A/2) sin
"
π(N−1)
N
(k −f0N)
#
sin [π (k −f0N)]
sin [π(k −f0N)/N]
−(A/2) sin
"
π(N−1)
N
(k + f0N)
#
sin [π (k −N + f0N)]
sin [π(k −N + f0N)/N]
2. This result implies that the original frequency ω0 of the cosine waveform has leaked into
other frequencies that form the harmonics of the time-limited sequence, and hence it is
called the leakage property of cosines. It is a natural result due to the fact that
bandlimited periodic cosines are sampled over noninteger periods. Explain this result
using the periodic extension ˜x(n) of x(n) and the result in Problem P5.34.1.
3. Verify the leakage property using x(n) = cos (5πn/99) R200(n). Plot the real and the
imaginary parts of X(k) using the stem function.
P5.36 Let
x(n) =
! A sin (2πℓn/N) , 0 ≤n ≤N −1
0,
elsewhere
= A sin (2πℓn/N) RN(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

210
Chapter 5
THE DISCRETE FOURIER TRANSFORM
where ℓis an integer. Notice that x(n) contains exactly ℓperiods (or cycles) of the sine
waveform in N samples. This is a windowed sine sequence containing no leakage.
1. Show that the DFT X(k) is a purely imaginary sequence given by
X(k) = AN
2j δ (k −ℓ) −AN
2j δ(k −N + ℓ);
0 ≤k ≤(N −1),
0 < ℓ< N
2. Show that if ℓ= 0, then the DFT X(k) is given by
X(k) = 0;
0 ≤k ≤(N −1)
3. Explain clearly how these results should be modiﬁed if ℓ< 0 or ℓ> N.
4. Verify the results of parts 1, 2, and 3 using the following sequences. Plot the imaginary
parts of the DFT sequences using the stem function.
(a) x1(n) = 3 sin (0.04πn) R200(n)
(b) x2(n) = 5 sin 10πnR50(n)
(c) x3(n) = [2 sin (0.5πn) + sin (πn)] R100(n)
(d) x4(n) = sin (25πn/16) R64(n)
(e) x5(n) = [4 sin (0.1πn) −3 sin (1.9πn)] R20(n)
P5.37 Let x(n) = A sin (ω0n) RN(n), where ω0 is a real number.
1. Using the properties of the DFT, show that the real and the imaginary parts of X(k) are
given by
X(k) = XR(k) + jXI(k)
XR(k) = −(A/2) sin
"
π(N−1)
N
(k −f0N)
#
sin [π (k −f0N)]
sin [π(k −f0N)/N]
+ (A/2) sin
"
π(N−1)
N
(k + f0N)
#
sin [π (k −N + f0N)]
sin [π(k −N + f0N)/N]
XI(k) = −(A/2) cos
"
π(N−1)
N
(k −f0N)
#
sin [π (k −f0N)]
sin [π(k −f0N)/N]
+ (A/2) cos
"
π(N−1)
N
(k + f0N)
#
sin [π (k −N + f0N)]
sin [π(k −N + f0N)/N]
2. This result is the leakage property of sines. Explain it using the periodic extension ˜x(n)
of x(n) and the result in Problem P5.36.1.
3. Verify the leakage property using x(n) = sin (5πn/99) R100(n). Plot the real and the
imaginary parts of X(k) using the stem function.
P5.38 An analog signal xa(t) = 2 sin (4πt) + 5 cos (8πt) is sampled at t = 0.01n for
n = 0, 1, . . . , N −1 to obtain an N-point sequence x(n). An N-point DFT is used to obtain
an estimate of the magnitude spectrum of xa(t).
1. From the following values of N, choose the one that will provide the accurate estimate of
the spectrum of xa(t). Plot the real and imaginary parts of the DFT spectrum X(k).
(a) N = 40,
(b) N = 50,
(c) N = 60.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
211
2. From the following values of N, choose the one that will provide the least amount of
leakage in the spectrum of xa(t). Plot the real and imaginary parts of the DFT spectrum
X(k).
(a) N = 90,
(b) N = 95,
(c) N = 99.
P5.39 Using (5.49), determine and draw the signal ﬂow graph for the N = 8 point, radix-2
decimation-in-frequency FFT algorithm. Using this ﬂow graph, determine the DFT of the
sequence
x(n) = cos (πn/2) ,
0 ≤n ≤7
P5.40 Using (5.49), determine and draw the signal ﬂow graph for the N = 16 point, radix-4
decimation-in-time FFT algorithm. Using this ﬂow graph, determine the DFT of the
sequence
x(n) = cos (πn/2) ,
0 ≤n ≤15
P5.41 Let x(n) be a uniformly distributed random number between [−1, 1] for 0 ≤n ≤106. Let
h(n) = sin(0.4πn),
0 ≤n ≤100
1. Using the conv function, determine the output sequence y(n) = x(n) ∗h(n).
2. Consider the overlap-and-save method of block convolution along with the FFT
algorithm to implement high-speed block convolution. Using this approach, determine
y(n) with FFT sizes of 1024, 2048, and 4096.
3. Compare these approaches in terms of the convolution results and their execution times.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

212
C H A P T E R 6
Implementation
of Discrete-Time
Filters
In earlier chapters, we studied the theory of discrete systems in both
the time and frequency domains. We will now use this theory for the
processing of digital signals. To process signals, we have to design and
implement systems called filters (or spectrum analyzers in some contexts).
The ﬁlter design issue is inﬂuenced by such factors as the type of the ﬁlter
(i.e., IIR or FIR) and the form of its implementation (structures). Hence,
before we discuss the design issue, we ﬁrst concern ourselves with how
these ﬁlters can be implemented in practice. This is an important concern
because diﬀerent ﬁlter structures dictate diﬀerent design strategies.
IIR ﬁlters, as designed and used in DSP, can be modeled by rational
system functions or, equivalently, by diﬀerence equations. Such ﬁlters are
termed autoregressive moving average (ARMA) or, more generally, as re-
cursive ﬁlters. Although ARMA ﬁlters include moving average ﬁlters that
are FIR ﬁlters, we will treat FIR ﬁlters separately from IIR ﬁlters for both
design and implementation purposes.
In addition to describing various ﬁlter structures, we also begin to con-
sider problems associated with quantization eﬀects when ﬁnite-precision
arithmetic is used in the implementation. Digital hardware contains pro-
cessing elements that use ﬁnite-precision arithmetic. When ﬁlters are im-
plemented either in hardware or in software, ﬁlter coeﬃcients as well as
ﬁlter operations are subjected to the eﬀects of these ﬁnite-precision op-
erations. In this chapter, we treat the eﬀects on ﬁlter frequency response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Basic Elements
213
characteristics due to coeﬃcient quantization. In Chapter 10, we will con-
sider the round-oﬀnoise eﬀects in the digital ﬁlter implementations.
We begin with a description of basic building blocks that are used
to describe ﬁlter structures. In the subsequent sections, we brieﬂy de-
scribe IIR, FIR, and lattice ﬁlter structures, respectively, and provide
MATLAB functions to implement these structures. This is followed by
a brief treatment of the representation of numbers and the resulting er-
ror characteristics, which is then used to analyze coeﬃcient quantization
eﬀects.
6.1 BASIC ELEMENTS
Since our ﬁlters are LTI systems, we need the following three elements to
describe digital ﬁlter structures. These elements are shown in Figure 6.1.
1. Adder: This element has two inputs and one output and is shown in
Figure 6.1a. Note that the addition of three or more signals is imple-
mented by successive two-input adders.
2. Multiplier (gain): This is a single-input, single-output element and is
shown in Figure 6.1b. Note that the multiplication by 1 is understood
and hence not explicitly shown.
3. Delay element (shifter or memory): This element delays the sig-
nal passing through it by one sample, as shown in Figure 6.1c. It is
implemented by using a shift register.
Using these basic elements, we can now describe various structures of
both IIR and FIR ﬁlters. MATLAB is a convenient tool in the develop-
ment of these structures that require operations on polynomials.
x1(n)
x1(n) + x2(n)
x2(n)
x(n)
ax(n)
(a) Adder
(b) Multiplier
a
z −1
x(n)
x(n − 1)
(c) Delay Element
FIGURE 6.1
Three basic elements
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

214
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
6.2 IIR FILTER STRUCTURES
The system function of an IIR ﬁlter is given by
H(z) = B(z)
A(z) =
M
n=0 bnz−n
N
n=0 anz−n = b0 + b1z−1 + · · · + bMz−M
1 + a1z−1 + · · · + aNz−N ; a0 = 1
(6.1)
where bn and an are the coeﬃcients of the ﬁlter. We have assumed without
loss of generality that a0 = 1. The order of such an IIR ﬁlter is called N if
aN ̸= 0. The diﬀerence equation representation of an IIR ﬁlter is expressed
as
y(n) =
M
	
m=0
bmx(n −m) −
N
	
m=1
amy(n −m)
(6.2)
Three diﬀerent structures can be used to implement an IIR ﬁlter:
1. Direct form: In this form, the diﬀerence equation (6.2) is implemented
directly as given. There are two parts to this ﬁlter, namely the moving
average part and the recursive part (or equivalently, the numerator
and denominator parts). Therefore, this implementation leads to two
versions: direct form I and direct form II structures.
2. Cascade form: In this form, the system function H(z) in equation
(6.1) is factored into smaller second-order sections, called biquads. The
system function is then represented as a product of these biquads. Each
biquad is implemented in a direct form, and the entire system function
is implemented as a cascade of biquad sections.
3. Parallel form: This is similar to the cascade form, but after factoriza-
tion, a partial fraction expansion is used to represent H(z) as a sum of
smaller second-order sections. Each section is again implemented in a
direct form, and the entire system function is implemented as a parallel
network of sections.
We will brieﬂy discuss these forms in this section. IIR ﬁlters are gen-
erally described using the rational form version (or the direct form struc-
ture) of the system function. Hence we will provide MATLAB functions for
converting direct form structures to cascade and parallel form structures.
6.2.1 DIRECT FORM
As the name suggests, the diﬀerence equation (6.2) is implemented as
given using delays, multipliers, and adders. For the purpose of illustration,
let M = N = 4. Then the diﬀerence equation is
y(n) = b0x(n) + b1x(n −1) + b2x(n −2) + b3x(n −3) + b4x(n −4)
−a1y(n −1) −a2y(n −2) −a3y(n −3) −a4y(n −4)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
215
FIGURE 6.2
Direct form I structure
which can be implemented as shown in Figure 6.2. This block diagram is
called direct form I structure.
The direct form I structure implements each part of the rational func-
tion H(z) separately with a cascade connection between them. The nu-
merator part is a tapped delay line followed by the denominator part,
which is a feedback tapped delay line. Thus there are two separate de-
lay lines in this structure, and hence it requires eight delay elements. We
can reduce this delay element count or eliminate one delay line by inter-
changing the order in which the two parts are connected in the cascade.
Now the two delay lines are close to each other, connected by a unity
gain branch. Therefore, one delay line can be removed, and this reduction
leads to a canonical structure called direct form II structure, shown in
Figure 6.3. It should be noted that both direct forms are equivalent from
the input-output point of view. Internally, however, they have diﬀerent
signals.
x(n)
(a) Normal
(b) Transposed
y(n)
y(n)
b1
b0
b2
b3
b4
z −1
−a1
−a2
−a3
−a4
z −1
z −1
z −1
x(n)
b1
b0
b2
b3
b4
z −1
−a1
−a2
−a3
−a4
z −1
z −1
z −1
FIGURE 6.3
Direct form II structure
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

216
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
6.2.2 TRANSPOSED STRUCTURE
An equivalent structure to the direct form can be obtained using a pro-
cedure called transposition. In this operation, three steps are performed:
1. All path arrow directions are reversed.
2. All branch nodes are replaced by adder nodes, and all adder nodes are
replaced by branch nodes.
3. The input and output nodes are interchanged.
The resulting structure is called the transposed direct form structure. The
transposed direct form II structure is shown in Figure 6.3b. Problem P6.3
explains this equivalent structure.
6.2.3 MATLAB IMPLEMENTATION
In MATLAB, the direct form structure is described by two row vectors:
b containing the {bn} coeﬃcients and a containing the {an} coeﬃcients.
The filter function, which is discussed in Chapter 2, implements the
transposed direct form II structure.
6.2.4 CASCADE FORM
In this form, the system function H(z) is written as a product of
second-order sections with real coeﬃcients. This is done by factoring
the numerator and denominator polynomials into their respective roots
and then combining either a complex conjugate root pair or any two real
roots into second-order polynomials. In the remainder of this chapter, we
assume that N is an even integer. Then
H(z) = b0 + b1z−1 + · · · + bNz−N
1 + a1z−1 + · · · + aNz−N
= b0
1 + b1
b0 z−1 + · · · + bN
b0 z−N
1 + a1z−1 + · · · + aNz−N
= b0
K

k=1
1 + Bk,1z−1 + Bk,2z−2
1 + Ak,1z−1 + Ak,2z−2
(6.3)
where K is equal to N
2 and Bk,1, Bk,2, Ak,1, and Ak,2 are real numbers
representing the coeﬃcients of second-order sections. The second-order
section
Hk(z) = Yk+1(z)
Yk(z)
= 1 + Bk,1z−1 + Bk,2z−2
1 + Ak,1z−1 + Ak,2z−2 ;
k = 1, . . . , K
with
Y1(z) = b0X(z);
YK+1(z) = Y (z)
is called the kth biquad section. The input to the kth biquad section is
the output from the (k −1)th biquad section, and the output from the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
217
FIGURE 6.4
Biquad section structure
kth biquad is the input to the (k + 1)th biquad. Now each biquad section
Hk(z) can be implemented in direct form II, as shown in Figure 6.4. The
entire ﬁlter is then implemented as a cascade of biquads.
As an example, consider N = 4. Figure 6.5 shows a cascade form
structure for this fourth-order IIR ﬁlter.
6.2.5 MATLAB IMPLEMENTATION
Given the coeﬃcients {bn} and {an} of the direct form ﬁlter, we have to
obtain the coeﬃcients b0, {Bk,i}, and {Ak,i}. This is done by the following
function dir2cas.
function [b0,B,A] = dir2cas(b,a)
% DIRECT form to CASCADE form conversion (cplxpair version)
% ---------------------------------------------------------
% [b0,B,A] = dir2cas(b,a)
% b0 = gain coefficient
%
B = K by 3 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
b = numerator polynomial coefficients of DIRECT form
%
a = denominator polynomial coefficients of DIRECT form
% Compute gain coefficient b0
b0 = b(1); b = b/b0;
a0 = a(1); a = a/a0;
b0 = b0/a0;
%
M = length(b); N = length(a);
if N > M
b = [b zeros(1,N-M)];
FIGURE 6.5
Cascade form structure for N = 4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

218
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
elseif M > N
a = [a zeros(1,M-N)]; N = M;
else
NM = 0;
end
%
K = floor(N/2); B = zeros(K,3); A = zeros(K,3);
if K*2 == N
b = [b 0];
a = [a 0];
end
%
broots = cplxpair(roots(b));
aroots = cplxpair(roots(a));
for i=1:2:2*K
Brow = broots(i:1:i+1,:);
Brow = real(poly(Brow));
B(fix((i+1)/2),:) = Brow;
Arow = aroots(i:1:i+1,:);
Arow = real(poly(Arow));
A(fix((i+1)/2),:) = Arow;
end
This function converts the b and a vectors into K×3 size B and A matrices.
It begins by computing b0, which is equal to b0/a0 (assuming a0 ̸= 1).
It then makes the vectors b and a of equal length by zero-padding the
shorter vector. This ensures that each biquad has a nonzero numerator
and denominator. Next, it computes the roots of the B(z) and A(z) poly-
nomials. Using the cplxpair function, these roots are ordered in complex
conjugate pairs. Now every pair is converted back into a second-order
numerator or denominator polynomial using the poly function. The SP
toolbox function, tf2sos (transfer function to second-order section), also
performs a similar operation.
The cascade form is implemented using the following casfiltr
function.
function y = casfiltr(b0,B,A,x)
% CASCADE form realization of IIR and FIR filters
% -----------------------------------------------
% y = casfiltr(b0,B,A,x);
%
y = output sequence
% b0 = gain coefficient of CASCADE form
%
B = K by 3 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
x = input sequence
%
[K,L] = size(B);
N = length(x);
w = zeros(K+1,N);
w(1,:) = x;
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
219
for i = 1:1:K
w(i+1,:) = filter(B(i,:),A(i,:),w(i,:));
end
y = b0*w(K+1,:);
This function employs the filter function in a loop using the coeﬃcients
of each biquad stored in B and A matrices. The input is scaled by b0,
and the output of each ﬁlter operation is used as an input to the next
ﬁlter operation. The output of the ﬁnal ﬁlter operation is the overall
output.
The following MATLAB function, cas2dir, converts a cascade form
to a direct form. This is a simple operation that involves multiplication of
several second-order polynomials. For this purpose, the MATLAB func-
tion conv is used in a loop over K factors. The SP toolbox function sos2tf
also performs a similar operation.
function [b,a] = cas2dir(b0,B,A)
% CASCADE-to-DIRECT form conversion
% ---------------------------------
% [b,a] = cas2dir(b0,B,A)
%
b = numerator polynomial coefficients of DIRECT form
%
a = denominator polynomial coefficients of DIRECT form
% b0 = gain coefficient
%
B = K by 3 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
[K,L] = size(B);
b = [1];
a = [1];
for i=1:1:K
b=conv(b,B(i,:));
a=conv(a,A(i,:));
end
b = b*b0;
□
EXAMPLE 6.1
A ﬁlter is described by the following diﬀerence equation:
16y(n) + 12y(n −1) + 2y(n −2) −4y(n −3) −y(n −4)
= x(n) −3x(n −1) + 11x(n −2) −27x(n −3) + 18x(n −4)
Determine its cascaded form structure.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

220
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
Solution
MATLAB script:
>> b=[1 -3 11 -27 18];
a=[16 12 2 -4 -1];
>> [b0,B,A]=dir2cas(b,a)
b0 = 0.0625
B =
1.0000
-0.0000
9.0000
1.0000
-3.0000
2.0000
A =
1.0000 1.0000
0.5000
1.0000
-0.2500
-0.1250
The resulting structure is shown in Figure 6.6. To check that our cascade struc-
ture is correct, let us compute the ﬁrst eight samples of the impulse response
using both forms.
>> delta = impseq(0,0,7)
delta =
1
0
0
0
0
0
0
0
>> format long
>> hcas=casfiltr(b0,B,A,delta)
hcas =
Columns 1 through 4
0.06250000000000
-0.23437500000000
0.85546875000000
-2.28417968750000
Columns 5 through 8
2.67651367187500
-1.52264404296875
0.28984069824219
0.49931716918945
>> hdir=filter(b,a,delta)
hdir =
Columns 1 through 4
0.06250000000000
-0.23437500000000
0.85546875000000
-2.28417968750000
Columns 5 through 8
2.67651367187500
-1.52264404296875
0.28984069824219
0.49931716918945
□
FIGURE 6.6
Cascade structure in Example 6.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
221
6.2.6 PARALLEL FORM
In this form, the system function H(z) is written as a sum of second-order
sections using partial fraction expansion.
H(z) = B(z)
A(z) = b0 + b1z−1 + · · · + bMz−M
1 + a1z−1 + · · · + aNz−N
=
ˆb0 + ˆb1z−1 + · · · + ˆbN−1z1−N
1 + a1z−1 + · · · + aNz−N
+
M−N
	
0
Ckz−k
&
'(
)
Only if M≥N
=
K
	
k=1
Bk,0 + Bk,1z−1
1 + Ak,1z−1 + Ak,2z−2 +
M−N
	
0
Ckz−k
&
'(
)
Only if M≥N
(6.4)
where K is equal to N
2 and Bk,0, Bk,1, Ak,1, and Ak,2 are real numbers
representing the coeﬃcients of second-order sections. The second-order
section
Hk(z) = Yk(z)
X(z) =
Bk,0 + Bk,1z−1
1 + Ak,1z−1 + Ak,2z−2 ;
k = 1, . . . , K
with
Yk(z) = Hk(z)X(z),
Y (z) =
	
Yk(z),
M < N
is the kth proper rational biquad section. The ﬁlter input is available to
all biquad sections as well as to the polynomial section if M ≥N (which
is an FIR part). The output from these sections is summed to form the
ﬁlter output. Now each biquad section Hk(z) can be implemented in direct
form II. Due to the summation of subsections, a parallel structure can be
built to realize H(z). As an example, consider M = N = 4. Figure 6.7
shows a parallel form structure for this fourth-order IIR ﬁlter.
6.2.7 MATLAB IMPLEMENTATION
The following function dir2par converts the direct form coeﬃcients {bn}
and {an} into parallel form coeﬃcients {Bk,i} and {Ak,i}.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

222
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
FIGURE 6.7
Parallel form structure for N = 4
function [C,B,A] = dir2par(b,a)
% DIRECT form to PARALLEL form conversion
% --------------------------------------
% [C,B,A] = dir2par(b,a)
%
C = polynomial part when length(b) >= length(a)
%
B = K by 2 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
b = numerator polynomial coefficients of DIRECT form
%
a = denominator polynomial coefficients of DIRECT form
%
M = length(b); N = length(a);
[r1,p1,C] = residuez(b,a);
p = cplxpair(p1,10000000*eps);
I = cplxcomp(p1,p);
r = r1(I);
K = floor(N/2); B = zeros(K,2); A = zeros(K,3);
if K*2 == N; %N even, order of A(z) odd, one factor is first order
for i=1:2:N-2
Brow = r(i:1:i+1,:);
Arow = p(i:1:i+1,:);
[Brow,Arow] = residuez(Brow,Arow,[]);
B(fix((i+1)/2),:) = real(Brow);
A(fix((i+1)/2),:) = real(Arow);
end
[Brow,Arow] = residuez(r(N-1),p(N-1),[]);
B(K,:) = [real(Brow) 0]; A(K,:) = [real(Arow) 0];
else
for i=1:2:N-1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
223
Brow = r(i:1:i+1,:);
Arow = p(i:1:i+1,:);
[Brow,Arow] = residuez(Brow,Arow,[]);
B(fix((i+1)/2),:) = real(Brow);
A(fix((i+1)/2),:) = real(Arow);
end
end
The dir2cas function ﬁrst computes the z-domain partial fraction expan-
sion using the residuez function. We need to arrange pole-and-residue
pairs into complex conjugate pole-and-residue pairs followed by real pole-
and-residue pairs. To do this, the cplxpair function from MATLAB can
be used; it sorts a complex array into complex conjugate pairs. However,
two consecutive calls to this function, one each for pole and residue ar-
rays, will not guarantee that poles and residues will correspond to each
other. Therefore, a new cplxcomp function is developed, which compares
two shuﬄed complex arrays and returns the index of one array, which can
be used to rearrange another array.
function I = cplxcomp(p1,p2)
%
I = cplxcomp(p1,p2)
% Compares two complex pairs which contain the same scalar elements
%
but (possibly) at differrent indices.
This routine should be
%
used after CPLXPAIR routine for rearranging pole vector and its
%
corresponding residue vector.
%
p2 = cplxpair(p1)
%
I=[];
for j=1:1:length(p2)
for i=1:1:length(p1)
if (abs(p1(i)-p2(j)) < 0.0001)
I=[I,i];
end
end
end
I=I’;
After collecting these pole-and-residue pairs, the dir2cas function com-
putes the numerator and denominator of the biquads by employing the
residuez function in the reverse fashion.
These parallel form coeﬃcients are then used in the function
parfiltr, which implements the parallel form. The parfiltr function
uses the filter function in a loop using the coeﬃcients of each biquad
stored in the B and A matrices. The input is ﬁrst ﬁltered through the FIR
part C and stored in the ﬁrst row of a w matrix. Then the outputs of all
biquad ﬁlters are computed for the same input and stored as subsequent
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

224
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
rows in the w matrix. Finally, all the columns of the w matrix are summed
to yield the output.
function y = parfiltr(C,B,A,x)
% PARALLEL form realization of IIR filters
% ----------------------------------------
%
[y] = parfiltr(C,B,A,x);
%
y = output sequence
%
C = polynomial (FIR) part when M >= N
%
B = K by 2 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
x = input sequence
%
[K,L] = size(B);
N = length(x);
w = zeros(K+1,N);
w(1,:) = filter(C,1,x);
for i = 1:1:K
w(i+1,:) = filter(B(i,:),A(i,:),x);
end
y = sum(w);
To obtain a direct form from a parallel form, the function par2dir can
be used. It computes poles and residues of each proper biquad and com-
bines these into system poles and residues. Another call of the residuez
function in reverse order computes the numerator and denominator
polynomials.
function [b,a] = par2dir(C,B,A)
% PARALLEL-to-DIRECT form conversion
% ----------------------------------
% [b,a] = par2dir(C,B,A)
%
b = numerator polynomial coefficients of DIRECT form
%
a = denominator polynomial coefficients of DIRECT form
%
C = polynomial part of PARALLEL form
%
B = K by 2 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
[K,L] = size(A); R = []; P = [];
for i=1:1:K
[r,p,k]=residuez(B(i,:),A(i,:));
R = [R;r]; P = [P;p];
end
[b,a] = residuez(R,P,C);
b = b(:)’; a = a(:)’;
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
225
□
EXAMPLE 6.2
Consider the ﬁlter given in Example 6.1.
16y(n) + 12y(n −1) + 2y(n −2) −4y(n −3) −y(n −4)
= x(n) −3x(n −1) + 11x(n −2) −27x(n −3) + 18x(n −4)
Now determine its parallel form.
Solution
MATLAB script:
>> b=[1 -3 11 -27 18];
a=[16 12 2 -4 -1];
>> [C,B,A]=dir2par(b,a)
C =
-18
B =
-10.0500
-3.9500
28.1125
-13.3625
A =
1.0000
1.0000
0.5000
1.0000
-0.2500
-0.1250
The resulting structure is shown in Figure 6.8. To check our parallel structure,
let us compute the ﬁrst eight samples of the impulse response using both forms.
>> format long; delta = impseq(0,0,7);
hpar=parfiltr(C,B,A,delta)
hpar =
Columns 1 through 4
0.06250000000000
-0.23437500000000
0.85546875000000
-2.28417968750000
FIGURE 6.8
Parallel form structure in Example 6.2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

226
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
Columns 5 through 8
2.67651367187500
-1.52264404296875
0.28984069824219
0.49931716918945
>> hdir = filter(b,a,delta)
hdir =
Columns 1 through 4
0.06250000000000
-0.23437500000000
0.85546875000000
-2.28417968750000
Columns 5 through 8
2.67651367187500
-1.52264404296875
0.28984069824219
0.49931716918945
□
□
EXAMPLE 6.3
What would be the overall direct, cascade, or parallel form if a structure contains
a combination of these forms? Consider the block diagram shown in Figure 6.9.
Solution
This structure contains a cascade of two parallel sections. The ﬁrst parallel
section contains two biquads, while the second one contains three biquads. We
will have to convert each parallel section into a direct form using the par2dir
function, giving us a cascade of two direct forms. The overall direct form can be
computed by convolving the corresponding numerator and denominator poly-
nomials. The overall cascade and parallel forms can now be derived from the
direct form.
MATLAB script:
>> C0=0; B1=[2 4;3 1]; A1=[1 1 0.9; 1 0.4 -0.4];
>> B2=[0.5 0.7;1.5 2.5;0.8 1]; A2=[1 -1 0.8;1 0.5 0.5;1 0 -0.5];
>> [b1,a1]=par2dir(C0,B1,A1)
b1 =
5.0000
8.8000
4.5000
-0.7000
FIGURE 6.9
Block diagram in Example 6.3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

IIR Filter Structures
227
a1 =
1.0000
1.4000
0.9000
-0.0400
-0.3600
>> [b2,a2]=par2dir(C0,B2,A2)
b2 =
2.8000
2.5500
-1.5600
2.0950
0.5700
-0.7750
a2 =
1.0000
-0.5000
0.3000
0.1500
0.0000
0.0500
-0.2000
>> b=conv(b1,b2) % Overall direct form numerator
b =
Columns 1 through 7
14.0000
37.3900
27.2400
6.2620
12.4810
11.6605
-5.7215
Columns 8 through 9
-3.8865
0.5425
>> a=conv(a1,a2) % Overall direct form denominator
a =
Columns 1 through 7
1.0000
0.9000
0.5000
0.0800
0.1400
0.3530
-0.2440
Columns 8 through 11
-0.2890
-0.1820
-0.0100
0.0720
>> [b0,Bc,Ac]=dir2cas(b,a) % Overall cascade form
b0 =
14.0000
Bc =
1.0000
1.8836
1.1328
1.0000
-0.6915
0.6719
1.0000
2.0776
0.8666
1.0000
0
0
1.0000
-0.5990
0.0588
Ac =
1.0000
1.0000
0.9000
1.0000
0.5000
0.5000
1.0000
-1.0000
0.8000
1.0000
1.5704
0.6105
1.0000
-1.1704
0.3276
>> [C0,Bp,Ap]=dir2par(b,a) % Overall parallel form
C0 = []
Bp =
-20.4201
-1.6000
24.1602
5.1448
2.4570
3.3774
-0.8101
-0.2382
8.6129
-4.0439
Ap =
1.0000
1.0000
0.9000
1.0000
0.5000
0.5000
1.0000
-1.0000
0.8000
1.0000
1.5704
0.6105
1.0000
-1.1704
0.3276
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

228
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
This example shows that by using the MATLAB functions developed in this
section, we can probe and construct a wide variety of structures.
□
6.3 FIR FILTER STRUCTURES
A ﬁnite-duration impulse response ﬁlter has a system function of the form
H(z) = b0 + b1z−1 + · · · + bM−1z1−M =
M−1
	
n=0
bnz−n
(6.5)
Hence the impulse response h(n) is
h(n) =

bn,
0 ≤n ≤M −1
0,
else
(6.6)
and the diﬀerence equation representation is
y(n) = b0x(n) + b1x(n −1) + · · · + bM−1x(n −M + 1)
(6.7)
which is a linear convolution of ﬁnite support.
The order of the ﬁlter is M −1, and the length of the ﬁlter (which is
equal to the number of coeﬃcients) is M. The FIR ﬁlter structures are
always stable, and they are relatively simple compared to IIR structures.
Furthermore, FIR ﬁlters can be designed to have a linear-phase response,
which is desirable in some applications.
We will consider the following four structures.
1. Direct form: In this form, the diﬀerence equation (6.7) is implemented
directly as given.
2. Cascade form: In this form, the system function H(z) in (6.5) is
factored into second-order factors, which are then implemented in a
cascade connection.
3. Linear-phase form: When an FIR ﬁlter has a linear-phase response,
its impulse response exhibits certain symmetry conditions. In this form,
we exploit these symmetry relations to reduce multiplications by about
half.
4. Frequency-sampling form: This structure is based on the DFT of
the impulse response h(n) and leads to a parallel structure. It is also
suitable for a design technique based on the sampling of frequency
response H(ejω).
We will brieﬂy describe these four forms along with some examples.
The MATLAB function dir2cas developed in the previous section is also
applicable for the cascade form.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures
229
FIGURE 6.10
Direct form FIR structure
6.3.1 DIRECT FORM
The diﬀerence equation (6.7) is implemented as a tapped delay line since
there are no feedback paths. Let M = 5 (i.e., a fourth-order FIR ﬁlter);
then
y(n) = b0x(n) + b1x(n −1) + b2x(n −2) + b3x(n −3) + b4x(n −4)
The direct form structure is given in Figure 6.10. Note that since the
denominator is equal to unity, there is only one direct form structure.
6.3.2 MATLAB IMPLEMENTATION
In MATLAB, the direct form FIR structure is described by the row vector
b containing the {bn} coeﬃcients. The structure is implemented by the
filter function, in which the vector a is set to the scalar value 1, as
discussed in Chapter 2.
6.3.3 CASCADE FORM
This form is similar to that of the IIR form. The system function H(z)
is converted into products of second-order sections with real coeﬃcients.
These sections are implemented in direct form and the entire ﬁlter as a
cascade of second-order sections. From (6.5),
H(z) = b0 + b1z−1 + · · · + bM−1z−M+1
(6.8)
= b0

1 + b1
b0
z−1 + · · · + bM−1
b0
z−M+1

= b0
K

k=1

1 + Bk,1z−1 + Bk,2z−2
where K is equal to ⌊M
2 ⌋and Bk,1 and Bk,2 are real numbers representing
the coeﬃcients of second-order sections. For M = 7, the cascade form is
shown in Figure 6.11.
FIGURE 6.11
Cascade form FIR structure
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

230
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
6.3.4 MATLAB IMPLEMENTATION
Although it is possible to develop a new MATLAB function for the FIR
cascade form, we will use our dir2cas function by setting the denominator
vector a equal to 1. Similarly, cas2dir can be used to obtain the direct
form from the cascade form.
6.3.5 LINEAR-PHASE FORM
For frequency-selective ﬁlters (e.g., lowpass ﬁlters), it is generally desirable
to have a phase response that is a linear function of frequency; that is, we
want
̸ H(ejω) = β −αω,
−π < ω ≤π
(6.9)
where β = 0 or ±π/2 and α is a constant. For a causal FIR ﬁlter with
impulse response over [0, M −1] interval, the linear-phase condition (6.9)
imposes the following symmetry conditions on the impulse response h(n)
(see Problem P6.15):
h(n) = h(M −1 −n);
β = 0, α = M −1
2
, 0 ≤n ≤M −1
(6.10)
h(n) = −h(M −1 −n);
β = ±π/2, α = M −1
2
, 0 ≤n ≤M −1 (6.11)
An impulse response that satisﬁes (6.10) is called a symmetric impulse
response and that in (6.11) is called an antisymmetric impulse response.
These symmetry conditions can now be exploited in a structure called the
linear-phase form.
Consider the diﬀerence equation given in (6.7) with a symmetric im-
pulse response in (6.10). We have
y(n) = b0x(n) + b1x(n −1) + · · · + b1x(n −M + 2) + b0x(n −M + 1)
= b0[x(n) + x(n −M + 1)] + b1[x(n −1) + x(n −M + 2)] + · · ·
The block diagram implementation of this diﬀerence equation is shown in
Figure 6.12 for both odd and even M.
Clearly, this structure requires 50% fewer multiplications than the di-
rect form. A similar structure can be derived for an antisymmetric impulse
response.
6.3.6 MATLAB IMPLEMENTATION
The linear-phase structure is essentially a direct form drawn diﬀerently to
save on multiplications. Hence in MATLAB, representation of the linear-
phase structure is equivalent to the direct form.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures
231
FIGURE 6.12
Linear-phase form FIR structures (symmetric impulse response)
□
EXAMPLE 6.4
An FIR ﬁlter is given by the system function
H(z) = 1 + 16 1
16z−4 + z−8
Determine and draw the direct, linear-phase, and cascade form structures.
a. Direct form: The diﬀerence equation is given by
y(n) = x(n) + 16.0625x(n −4) + x(n −8)
and the direct form structure is shown in Figure 6.13(a).
b. Linear-phase form: The diﬀerence equation can be written in the form
y(n) = [x(n) + x(n −8)] + 16.0625x(n −4)
and the resulting structure is shown in Figure 6.13b.
c. Cascade form: We use the following MATLAB script.
>> b=[1,0,0,0,16+1/16,0,0,0,1];
[b0,B,A] = dir2cas(b,1)
FIGURE 6.13
FIR ﬁlter structures in Example 6.4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

232
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
b0 = 1
B =
1.0000
2.8284
4.0000
1.0000
0.7071
0.2500
1.0000
-0.7071
0.2500
1.0000
-2.8284
4.0000
A =
1
0
0
1
0
0
1
0
0
1
0
0
The cascade form structure is shown in Figure 6.13c.
□
□
EXAMPLE 6.5
For the ﬁlter in Example 6.4, what would be the structure if we desire a cascade
form containing linear-phase components with real coeﬃcients?
Solution
We are interested in cascade sections that have symmetry and real coeﬃcients.
From the properties of linear-phase FIR ﬁlters (see Chapter 7), if such a ﬁlter
has an arbitrary zero at z = r̸ θ, then there must be three other zeros at
(1/r)̸ θ, r̸
−θ, and (1/r)̸
−θ to have real ﬁlter coeﬃcients. We can now
make use of this property. First we will determine the zero locations of the
given eighth-order polynomial. Then we will group four zeros that satisfy this
property to obtain one (fourth-order) linear-phase section. There are two such
sections, which we will connect in cascade.
MATLAB script:
>> b=[1,0,0,0,16+1/16,0,0,0,1];
broots=roots(b)
broots =
-1.4142 + 1.4142i
-1.4142 - 1.4142i
1.4142 + 1.4142i
1.4142 - 1.4142i
-0.3536 + 0.3536i
-0.3536 - 0.3536i
0.3536 + 0.3536i
0.3536 - 0.3536i
>> B1=real(poly([broots(1),broots(2),broots(5),broots(6)]))
B1 =
1.0000
3.5355
6.2500
3.5355
1.0000
>> B2=real(poly([broots(3),broots(4),broots(7),broots(8)]))
B2 =
1.0000
-3.5355
6.2500
-3.5355
1.0000
The structure is shown in Figure 6.14.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures
233
FIGURE 6.14
Cascade of FIR linear-phase elements
6.3.7 FREQUENCY-SAMPLING FORM
In this form, we use the fact that the system function H (z) of an FIR
ﬁlter can be reconstructed from its samples on the unit circle. From our
discussions on the DFT in Chapter 5, we recall that these samples are
in fact the M-point DFT values {H (k) , 0 ≤k ≤M −1} of the M-point
impulse response h (n). Therefore, we have
H (z) = Z [h (n)] = Z [IDFT {H (k)}]
Using this procedure, we obtain [see (5.17) in Chapter 5]
H (z) =
1 −z−M
M
 M−1
	
k=0
H (k)
1 −W −k
M z−1
(6.12)
This shows that the DFT H (k), rather than the impulse response
h (n) (or the diﬀerence equation), is used in this structure. Also note that
the FIR ﬁlter described by (6.12) has a recursive form similar to an IIR
ﬁlter because (6.12) contains both poles and zeros. The resulting ﬁlter is
an FIR ﬁlter since the poles at W −k
M
are canceled by the roots of
1 −z−M = 0
The system function in (6.12) leads to a parallel structure, as shown in
Figure 6.15 for M = 4.
One problem with the structure in Figure 6.15 is that it requires a
complex arithmetic implementation. Since an FIR ﬁlter is almost always a
real-valued ﬁlter, it is possible to obtain an alternate realization in which
only real arithmetic is used. This realization is derived using the symmetry
properties of the DFT and the W −k
M factor. Then (6.12) can be expressed
as (see Problem P6.18)
H (z) = 1 −z−M
M
 L
	
k=1
2 |H (k)| Hk (z) +
H (0)
1 −z−1 + H (M/2)
1 + z−1

(6.13)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

234
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
FIGURE 6.15
Frequency-sampling structure for M = 4
where L = M−1
2
for M odd, L = M
2 −1 for M even, and {Hk (z) , k = 1,
. . . , L} are second-order sections given by
Hk (z) = cos [̸ H (k)] −z−1 cos

̸ H (k) −2πk
M

1 −2z−1 cos
 2πk
M

+ z−2
(6.14)
Note that the DFT samples H (0) and H (M/2) are real-valued and that
the third term on the right-hand side of (6.13) is absent if M is odd. Using
(6.13) and (6.14), we show a frequency-sampling structure in Figure 6.16
for M = 4 containing real coeﬃcients.
6.3.8 MATLAB IMPLEMENTATION
Given the impulse response h (n) or the DFT H(k), we have to determine
the coeﬃcients in (6.13) and (6.14). The following MATLAB function,
dir2fs, converts a direct form h(n) to the frequency-sampling form by
directly implementing (6.13) and (6.14).
FIGURE 6.16
Frequency-sampling structure for M = 4 with real coeﬃcients
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures
235
function [C,B,A] = dir2fs(h)
% Direct form to frequency-sampling form conversion
% -------------------------------------------------
% [C,B,A] = dir2fs(h)
% C = row vector containing gains for parallel sections
% B = matrix containing numerator coefficients arranged in rows
% A = matrix containing denominator coefficients arranged in rows
% h = impulse response vector of an FIR filter
%
M = length(h);
H = fft(h,M);
magH = abs(H); phaH = angle(H)’;
% Check even or odd M
if (M == 2*floor(M/2))
L = M/2-1;
% M is even
A1 = [1,-1,0;1,1,0];
C1 = [real(H(1)),real(H(L+2))];
else
L = (M-1)/2; % M is odd
A1 = [1,-1,0];
C1 = [real(H(1))];
end
k = [1:L]’;
% Initialize B and A arrays
B = zeros(L,2); A = ones(L,3);
% Compute denominator coefficients
A(1:L,2) = -2*cos(2*pi*k/M); A = [A;A1];
% Compute numerator coefficients
B(1:L,1) = cos(phaH(2:L+1));
B(1:L,2) = -cos(phaH(2:L+1)-(2*pi*k/M));
% Compute gain coefficients
C = [2*magH(2:L+1),C1]’;
In this function, the impulse response values are supplied through the
h array. After conversion, the C array contains the gain values for each
parallel section. The gain values for the second-order parallel sections
are given ﬁrst, followed by H (0) and H (M/2) (if M is even). The B
matrix contains the numerator coeﬃcients, which are arranged in length-
2 row vectors for each second-order section. The A matrix contains the
denominator coeﬃcients, which are arranged in length-3 row vectors for
the second-order sections corresponding to those in B, followed by the
coeﬃcients for the ﬁrst-order sections.
A practical problem with the structure in Figure 6.16 is that it has
poles on the unit circle, which makes this ﬁlter critically unstable. If the
ﬁlter is not excited by one of the pole frequencies, then the output is
bounded. We can avoid this problem by sampling H (z) on a circle |z| = r,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

236
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
where the radius r is very close to 1 but is less than 1 (e.g., r = 0.99),
which results in
H (z) = 1 −rMz−M
M
M−1
	
k=0
H (k)
1 −rW −k
M z−k ;
H (k) = H

rej2πk/M
(6.15)
Now approximating H

rej2πk/M
≈H

ej2πk/M
for r ≈1, we can obtain
a stable structure similar to the one in Figure 6.16 containing real values.
This is explored in Problem P6.19.
□
EXAMPLE 6.6
Let h (n) = 1
9{1
↑
, 2, 3, 2, 1}. Determine and draw the frequency-sampling form.
Solution
MATLAB script:
>> h = [1,2,3,2,1]/9;
[C,B,A] = dir2fs(h)
C =
0.5818
0.0849
1.0000
B =
-0.8090
0.8090
0.3090
-0.3090
A =
1.0000
-0.6180
1.0000
1.0000
1.6180
1.0000
1.0000
-1.0000
0
Since M = 5 is odd, there is only one ﬁrst-order section. Hence
H (z) = 1 −z−5
5

0.5818 −0.809 + 0.809z−1
1 −0.618z−1 + z−2
+ 0.0849 0.309 −0.309z−1
1 + 1.618z−1 + z−2 +
1
1 −z−1

The frequency-sampling form is shown in Figure 6.17.
□
□
EXAMPLE 6.7
The frequency samples of a 32-point linear-phase FIR ﬁlter are given by
|H (k)| =
1,
k = 0, 1, 2
0.5,
k = 3
0,
k = 4, 5, . . . , 15
Determine its frequency-sampling form, and compare its computational com-
plexity with the linear-phase form.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures
237
FIGURE 6.17
Frequency-sampling structure in Example 6.6
Solution
In this example, since the samples of the DFT H (k) are given, we could use
(6.13) and (6.14) directly to determine the structure. However, we will use the
dir2fs function for which we will have to determine the impulse response h (n).
Using the symmetry property and the linear-phase constraint, we assemble the
DFT H (k) as
H (k) = |H (k)| ej̸
H(k),
k = 0, 1, . . . , 31
|H (k)| = |H (32 −k)| ,
k = 1, 2, . . . , 31; H (0) = 1
̸ H (k) = −31
2
2π
32 k = −̸ H (32 −k) ,
k = 0, 1, . . . , 31
Now the IDFT of H (k) will result in the desired impulse response.
MATLAB script:
>> M = 32; alpha = (M-1)/2;
>> magHk = [1,1,1,0.5,zeros(1,25),0.5,1,1];
>> k1 = 0:15; k2 = 16:M-1;
>> angHk = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = magHk.*exp(j*angHk);
h = real(ifft(H,M));
[C,B,A] = dir2fs(h)
C =
2.0000
2.0000
1.0000
0.0000
0.0000
0.0000
0.0000
0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

238
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
0.0000
1.0000
0
B =
-0.9952
0.9952
0.9808
-0.9808
-0.9569
0.9569
-0.8944
0.3162
0.9794
-0.7121
0.8265
0.2038
-0.6754
0.8551
1.0000
0.0000
0.6866
-0.5792
0.5191
0.9883
-0.4430
0.4993
-0.8944
-0.3162
-0.2766
0.3039
0.9343
0.9996
-0.9077
-0.8084
A =
1.0000
-1.9616
1.0000
1.0000
-1.8478
1.0000
1.0000
-1.6629
1.0000
1.0000
-1.4142
1.0000
1.0000
-1.1111
1.0000
1.0000
-0.7654
1.0000
1.0000
-0.3902
1.0000
1.0000
0.0000
1.0000
1.0000
0.3902
1.0000
1.0000
0.7654
1.0000
1.0000
1.1111
1.0000
1.0000
1.4142
1.0000
1.0000
1.6629
1.0000
1.0000
1.8478
1.0000
1.0000
1.9616
1.0000
1.0000
-1.0000
0
1.0000
1.0000
0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Overview of Finite-Precision Numerical Eﬀects
239
Note that only four gain coeﬃcients are nonzero. Hence the frequency-sampling
form is
H (z) = 1 −z−32
32
⎡
⎢⎣
2−0.9952 + 0.9952z−1
1 −1.9616z−1 + z−2 + 2 0.9808 −0.9808z−1
1 −1.8478z−1 + z−2 +
−0.9569 + 0.9569z−1
1 −1.6629z−1 + z−2 +
1
1 −z−1
⎤
⎥⎦
To determine the computational complexity, note that since H (0) = 1, the ﬁrst-
order section requires no multiplication, whereas the three second-order sections
require three multiplications each for a total of nine multiplications per output
sample. The total number of additions is 13. To implement the linear-phase
structure would require 16 multiplications and 31 additions per output sample.
Therefore, the frequency-sampling structure of this FIR ﬁlter is more eﬃcient
than the linear-phase structure.
□
6.4 OVERVIEW OF FINITE-PRECISION NUMERICAL EFFECTS
Until now, we have considered digital ﬁlter designs and implementations
in which both the ﬁlter coeﬃcients and the ﬁlter operations such as addi-
tions and multiplications were expressed using inﬁnite-precision numbers.
When discrete-time systems are implemented in hardware or in software,
all parameters and arithmetic operations are implemented using ﬁnite-
precision numbers, and hence their eﬀect is unavoidable.
Consider a typical digital ﬁlter implemented as a direct form II struc-
ture, which is shown in Figure 6.18a. When ﬁnite-precision representation
is used in its implementation, there are three possible considerations that
aﬀect the overall quality of its output. We have to
1. quantize the ﬁlter coeﬃcients {ak, bk} to obtain their ﬁnite word-length
representations {ˆak, ˆbk},
2. quantize the input sequence x(n) to obtain ˆx(n), and
3. consider all internal arithmetic that must be converted to their next
best representations.
Thus the output, y(n), is also a quantized value ˆy(n). This gives us a new
ﬁlter realization, ˆH(z), which is shown in Figure 6.18b. We hope that this
new ﬁlter ˆH(z) and its output ˆy(n) are as close as possible to the original
ﬁlter H(z) and the original output y(n).
Since the quantization operation is a nonlinear operation, the overall
analysis that takes into account all three eﬀects described above is very
diﬃcult and tedious. Therefore, we will study each of these eﬀects sepa-
rately as though it were the only one acting at the time. This makes the
analysis easier and the results more interpretable.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

240
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
x(n)
y(n)
H(z)
x(n)
y(n)
H(z)
x(n)
x(n)
y(n)
y(n)
b0
b0
z−1
z−1
z−1
z−1
z−1
z−1
b1
b2
b3
a1
a2
a3
a1
a2
a3
b1
b2
b3
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
ˆ
(a)
(b)
FIGURE 6.18
Direct form II digital ﬁlter implementation: (a) inﬁnite precision,
(b) ﬁnite precision
We begin by discussing the number representation in a computer—
more accurately, a central processing unit (CPU). This leads to the pro-
cess of number quantization and the resulting error characterization. We
then analyze the eﬀects of ﬁlter coeﬃcient quantization on digital ﬁlter
frequency responses. The eﬀects of multiplication and addition quantiza-
tion (collectively known as arithmetic round-oﬀerrors) on ﬁlter output
are discussed in Chapter 10.
6.5 REPRESENTATION OF NUMBERS
In computers, numbers (real-valued or complex-valued, integers or frac-
tions) are represented using binary digits (bits), which take the value of
either a 0 or a 1. The ﬁnite word-length arithmetic needed for processing
these numbers is implemented using two diﬀerent approaches, depending
on the ease of implementation and the accuracy as well as dynamic range
needed in processing. The fixed-point arithmetic is easy to implement but
has only a ﬁxed dynamic range and accuracy (i.e., very large numbers or
very small numbers). The floating-point arithmetic, on the other hand, has
a wide dynamic range and a variable accuracy (relative to the magnitude
of a number) but is more complicated to implement and analyze.
Since a computer can operate only on a binary variable (e.g., a 1 or
a 0), positive numbers can straightforwardly be represented using binary
numbers. The problem arises as to how to represent the negative numbers.
There are three diﬀerent formats used in each of these arithmetics: sign-
magnitude format, one’s-complement format, and two’s-complement for-
mat. In discussing and analyzing these representations, we will mostly
consider a binary number system containing bits. However, this discussion
and analysis is also valid for any radix numbering system—for example,
the hexadecimal, octal, or decimal system.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
241
In the following discussion, we will ﬁrst begin with ﬁxed-point signed
integer arithmetic. A B-bit binary representation of an integer x is given
by1
x ≡bB−1 bB−2 . . . b0 = bB−1 ×2B−1 +bB−2 ×2B−2 +· · ·+b0 ×20
(6.16)
where each bit bi represents either a 0 or a 1. This representation will help
us to understand the advantages and disadvantages of each signed format
and to develop simple MATLAB functions. We will then extend these
concepts to fractional real numbers for both ﬁxed-point and ﬂoating-point
arithmetic.
6.5.1 FIXED-POINT SIGNED INTEGER ARITHMETIC
In this arithmetic, positive numbers are coded using their binary represen-
tation. For example, using 3 bits, we can represent numbers from 0 to 7 as
0
1
2
3
4
5
6
7
-+----+----+----+----+----+----+----+-
000
001
010
011
100
101
110
111
Thus, with 8 bits the numbers represented can be 0 to 255, with 10 bits
we can represent the numbers from 0 to 1023, and with 16 bits the range
covered is 0 to 65535. For negative numbers, the following three formats
are used: sign-magnitude, one’s-complement, and two’s-complement.
Sign-magnitude format
In this format, positive numbers are repre-
sented using bits as before. However, the leftmost bit (also known as the
most-signiﬁcant bit, or MSB) is used as the sign bit (0 is +, and 1 is −),
and the remaining bits hold the absolute magnitude of the number as
shown here:
Sign Bit
-+
Absolute Magnitude
+---+----------------------+
|
|
|
+---+----------------------+
This system has thus two diﬀerent codes for 0, one for the positive 0, the
other one for the negative 0. For example, using 3 bits, we can represent
numbers from −3 to 3 as
-3
-2
-1
-0
0
1
2
3
-+----+----+----+----+----+----+----+-
111
110
101
100
000
001
010
011
Thus, 8 bits cover the interval [−127, +127], while 16 bits cover [−32, 767,
+32, 767]. If we use B bits in the sign-magnitude format, then we can
represent integers from −(2B−1 −1) to +(2B−1 −1) only.
1Here the letter b is used to represent a binary bit. It is also used for ﬁlter coeﬃcients
{bk}. Its use in the text should be clear from the context.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

242
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
This format has two drawbacks. First, there are two representations
for 0. Second, the arithmetic using the sign-magnitude format requires
one rule to compute addition, another rule to compute subtraction, and a
way to compare two magnitudes to determine their relative value before
subtraction.
MATLAB Implementation
MATLAB is a 64-bit ﬂoating-point com-
putation engine that provides results in decimal numbers. Therefore,
ﬁxed-point binary operations must be simulated in MATLAB. It provides
the function dec2bin to convert a positive decimal integer into a B-bit
representation, which is a symbol (or a code) and not a number. Hence
it cannot be used in computation. Similarly, the function bin2dec con-
verts a B-bit binary character code into a decimal integer. For example,
dec2bin(3,3) gives 011 and bin2dec(’111’) results in 7. To obtain a
sign-magnitude format, a sign bit must be preﬁxed. Similarly, to convert a
sign-magnitude format, the leading bit must be used to impart a positive
or negative value. These functions are explored in Problem P9.1.
One’s-complement format
In this format, the negation (or comple-
mentation) of an integer x is obtained by complementing every bit (i.e., a
0 is replaced by 1 and a 1 by 0) in the binary representation of x. Suppose
the B-bit binary representation of x is bB−1 bB−2 · · · b0; then the B-bit
one’s-complement, ¯x, of x is given by
¯x
△= ¯bB−1 ¯bB−2 · · · ¯b0
where each bit ¯bi is a complement of bit bi. Clearly then
x + ¯x ≡1 1 . . . 1 = 2B −1
(6.17)
The MSB of the representation once again represents the sign bit,
because the positive integer has the MSB of 0 so that its negation (or a
negative integer) has the MSB of 1. The remaining bits represent either
the number x (if positive) or its one’s-complement (if negative). Thus,
using (6.17) the one’s-complement format representation2 is given by
x(1)
∆=

x, x ≥0
|x|, x < 0=

x,
x ≥0
2B −1 −|x|, x < 0=

x,
x ≥0
2B −1 + x, x < 0
(6.18)
Clearly, if B bits are available, then we can represent only integers from
(−2B−1+1) to (+2B−1−1), which is similar to the sign-magnitude format.
2The one’s-complement format refers to the representation of positive and negative
numbers, whereas the one’s-complement of a number refers to the negation of that
number.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
243
For example, using 3 bits, we can represent numbers from −3 to 3 as
-3
-2
-1
-0
0
1
2
3
-+----+----+----+----+----+----+----+-
100
101
110
111
000
001
010
011
which is a diﬀerent bit arrangement for negative numbers compared to
the sign-magnitude format.
The advantage of this format is that subtraction can be achieved by
adding the complement, which is very easy to obtain by simply comple-
menting a number’s bits. However, there are many drawbacks. There are
still two diﬀerent codes for 0, the addition is a bit tricky to implement,
and overﬂow management requires addition of the overﬂow bit to the least
signiﬁcant bit (or 20).
MATLAB Implementation
The one’s-complement of a positive in-
teger x using B bits can be obtained by using the built-in function
bitcmp(x,B), which complements the number’s bits. The result is a dec-
imal number between 0 and 2B −1. As before, the dec2bin can be used
to obtain the binary code. Using (6.18), we can develop the MATLAB
function OnesComplement, which obtains the one’s-complement format
representation. It uses the sign of a number to determine when to use
one’s-complement and can use scalar as well as vector values. The result
is a decimal equivalent of the representation.
function y = OnesComplement(x,B)
% y = OnesComplement(x,B)
% ---------------
% Decimal equivalent of
%
sign-magnitude format integer to b-bit ones’-complement format conversion
%
%
x: integer between -2ˆ(b-1) <
x <
2ˆ(b-1) (sign-magnitude)
%
y: integer between
0 <= y <= 2ˆb-1
(1’s-complement)
if any((x <= -2ˆ(B-1) | (x >= 2ˆ(B-1))))
error(’Numbers must satisfy -2ˆ(B-1) < x <
2ˆ(B-1)’)
end
s = sign(x);
% sign of x (-1 if x<0, 0 if x=0, 1 if x>0)
sb = (s < 0); % sign-bit
(0 if x>=0, 1 if x<0));
y = (1-sb).*x + sb.*bitcmp(abs(x),B);
□
EXAMPLE 6.8
Using the function OnesComplement, obtain one’s-complement format represen-
tation of integers from −7 to 7 using 4 bits.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

244
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
Solution
MATLAB script:
>> x = -7:7
x =
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
>> y = OnesComplement(x,4)
y =
8
9
10
11
12
13
14
0
1
2
3
4
5
6
7
Note that the number 15 is missing since we do not have −0 in our original
array.
□
Two’s-complement format
The disadvantage of having two codes for
the number 0 is eliminated in this format. Positive numbers are coded as
usual. The B-bit two’s-complement, ˜x, of a positive integer x is given by
˜x = ¯x + 1 = 2B −x
or
x + ˜x = 2B
(6.19)
where the second equality is obtained from (6.18). Once again, the
MSB of the representation provides the sign bit. Thus using (6.19), the
two’s-complement format representation3 is given by
x(2) =

x,
x ≥0
|˜x|, x < 0=

x,
x ≥0
2B −|x|, x < 0=

x,
x ≥0
2B + x, x < 0
(6.20)
Thus in B-bit two’s-complement format negative numbers are obtained by
adding 2B to them. Clearly, if B bits are available, then we can represent
2B integers from (−2B−1) to (+2B−1 −1). For example, using 3 bits, we
can represent numbers from −4 to 3 as
-4
-3
-2
-1
0
1
2
3
-+----+----+----+----+----+----+----+-
100
101
110
111
000
001
010
011
This format, by shifting to the right (e.g., by incrementing) the code
of the negative numbers, straightforwardly removes the problem of having
two codes for 0 and gives access to an additional negative number at the
left of the line. Thus 4 bits go from −8 to +7, 8 bits cover the interval
[−127, +127], and 16 bits cover [−32768, +32767].
3Again, the two’s-complement format refers to the representation of positive and neg-
ative numbers, whereas the two’s-complement of a number refers to the negation of
that number.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
245
MATLAB Implementation
Using
(6.20),
we
can
develop
the
MATLAB function TwosComplement, which obtains the two’s-complement
format representation. We can use the bitcmp function and then add 1
to the result to obtain the two’s-complement. However, we will use the
last equality in (6.20) to obtain the two’s-complement since this approach
will also be useful for fractional numbers. The function can use scalar
as well as vector values. The result is a decimal equivalent of the two’s-
complement representation. As before, the dec2bin can be used to obtain
the binary code.
function y = TwosComplement(x,b)
% y = TwosComplement(x,b)
% ---------------
% Decimal equivalent of
%
sign-magnitude format integer to b-bit ones’-complement format conversion
%
%
x: integer between -2ˆ(b-1) <= x <
2ˆ(b-1) (sign-magnitude)
%
y: integer between
0 <= y <= 2ˆb-1
(two’s-complement)
if any((x < -2ˆ(b-1) | (x >= 2ˆ(b-1))))
error(’Numbers must satisfy -2ˆ(b-1) <= x <
2ˆ(b-1)’)
end
s = sign(x);
% Sign of x (-1 if x<0, 0 if x=0, 1 if x>0)
sb = (s < 0); % Sign-bit
(0 if x>=0, 1 if x<0));
y = (1-sb).*x + sb.*(2ˆb+x); % or y = (1-sb).*x + sb.*(bitcmp(abs(x),b)+1);
□
EXAMPLE 6.9
Using the function TwosComplement, obtain the two’s-complement format rep-
resentation of integers from −8 to 7 using 4 bits.
Solution
MATLAB script:
>> x = -8:7
x =
-8
-7
-6
-5
-4
-3
-2
-1
0
1
2
3
4
5
6
7
>> y = TwosComplement(x,4)
y =
8
9
10
11
12
13
14
15
0
1
2
3
4
5
6
7
>> y = dec2bin(y,4); disp(sprintf(’%s’,[y’;char(ones(1,16)*32)]))
1000 1001 1010 1011 1100 1101 1110 1111 0000 0001 0010 0011 0100 0101 0110 0111
□
The two’s-complement format has many interesting characteristics
and advantages. These will be given after we discuss the next format—
namely, the ten’s-complement.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

246
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
Ten’s-complement format
This is a representation for decimal inte-
gers. We will describe it so that we can explore characteristics of two’s-
complement through decimal integers, which is much easier to understand.
Following (6.19), the N-digit ten’s-complement of a positive integer x is
given by
˜x = 10N −x
or
x + ˜x = 10N
(6.21)
Using (6.21), the N-digit ten’s-complement format representation is given
by
x(10N)
∆=

x,
x ≥0
|˜x|, x < 0=

x,
x ≥0
10N −|x|, x < 0=

x,
x ≥0
10N + x, x < 0
(6.22)
Thus in N-digit ten’s-complement format (which is sometimes re-
ferred to as 10N-complement format), negative numbers are obtained by
adding 10N to them. Clearly, when N digits are available, we can represent
10N integers from (−10N
2 ) to (+ 10N
2
−1). For example, using one digit,
we can represent numbers from −5 to 4 as
-5
-4
-3
-2
-1
0
1
2
3
4
-+----+----+----+----+----+----+----+----+----+
5
6
7
8
9
0
1
2
3
4
□
EXAMPLE 6.10
Using the two-digit ten’s-complement, i.e., hundred’s-complement format, per-
form the following operations:
1. 16 −32,
2. 32 −16,
3. −30 −40,
4. 40 + 20 −30,
5. −40 −20 + 30.
Solution
1. 16 −32
First, we note that 16 −32 = −16. If we use the usual subtraction rule
to proceed from right to left generating carries in the process, we cannot
complete the operation. To use the hundred’s-complement format, we ﬁrst
note that in the hundred’s-complement format we have
16(100) = 16,
−16(100) = 100 −16 = 84,
and
−32(100) = 100 −32 = 68
Hence 16 −32 ≡16 + 68 = 84 ≡−16 in the sign-magnitude format, as
expected.
2. 32 −16
In this case, the hundred’s-complement format gives
32 + 84 = 116 ≡16
in the sign-magnitude format by ignoring the generated carry digit. This is
because the sign bits were diﬀerent; therefore, the operation cannot generate
an overﬂow. Hence we check for overﬂow only if the sign bits are same.
3. −30 −40
In this case, the hundred’s-complement format gives
(100 −30) + (100 −40) = 70 + 60 = 130
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
247
Since the sign bits were the same, an overﬂow is generated and the result is
invalid.
4. 40 + 20 −30
This is an example of more than one addition or subtraction. Since the ﬁnal
result is well within the range, the overﬂow can be ignored—that is,
40 + 20 + (100 −30) = 40 + 20 + 70 = 130 ≡30
which is a correct result.
5. −40 −20 + 30
In this case, we have
(100 −40) + (100 −20) + 30 = 60 + 80 + 30 = 170 ≡−30
in the sign-magnitude format, which is, again, a correct result.
□
MATLAB Implementation
Using
(6.22),
one
can
develop
the
MATLAB function, TensComplement, which obtains ten’s-complement
format representation. It is similar to the TwosComplement function and
is explored in Problem P6.23.
Advantages of two’s-complement format
Using the results of the
Example 6.10, we now state the beneﬁts of the two’s-complement format.
These also hold (with obvious modiﬁcations) for the ten’s-complement
format.
1. It provides for all 2B+1 distinct representations for a B-bit fractional
representation. There is only one representation for zero.
2. This complement is compatible with our notion of negation: the com-
plement of a complement is the number itself.
3. It uniﬁes the subtraction and addition operations (subtractions are
essentially additions).
4. In a sum of more than two numbers, the internal overﬂows do not aﬀect
the ﬁnal result so long as the result is within the range (i.e., adding
two positive numbers gives a positive result, and adding two negative
numbers gives a negative result).
Hence in most A/D converters and processors, negative numbers are rep-
resented using two’s-complement format. Almost all current processors
implement signed arithmetic using this format and provide special func-
tions (e.g., an overﬂow ﬂag) to support it.
Excess-2B−1 format
This format is used in describing the exponent
of ﬂoating-point arithmetic; hence it is brieﬂy discussed here. In excess-
2B−1 signed format (also known as a biased format), all positive and
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

248
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
negative integers between −2B−1 and 2B−1 −1 are given by
x(e)
∆= 2B−1 + x
(6.23)
For example, using 3 bits, we can represent the numbers from −4 to
3 as
-4
-3
-2
-1
0
1
2
3
-+----+----+----+----+----+----+----+-
000
001
010
011
100
101
110
111
Notice that this format is very similar to the two’s-complement format,
but the sign bit is complemented. The arithmetic for this format is similar
to that of the two’s-complement format. It is used in the exponent of
ﬂoating-point number representation.
6.5.2 GENERAL FIXED-POINT ARITHMETIC
Using the discussion of integer arithmetic from the last section as a guide,
we can extend the ﬁxed-point representation to arbitrary real (integer
and fractional) numbers. We assume that a given inﬁnite-precision real
number x is approximated by a binary number ˆx with the following bit
arrangement:
ˆx =
±
↑
Sign bit
xx · · · x
& '( )
“L”
Integer bits
▲xx · · · x
& '( )
“B”
Fraction bits
(6.24)
where the sign bit ± is 0 for positive numbers and 1 for negative numbers,
x represents either a 0 or a 1, and ▲represents the binary point. This
representation is in fact the sign-magnitude format for real numbers, as we
will see. The total word length of the number ˆx is then equal to L + B + 1
bits.
□
EXAMPLE 6.11
Let L = 4 and B = 5, which means ˆx is a 10-bit number. Represent 11010▲01110
in decimal.
Solution
ˆx = −(1 × 23 + 0 × 22 + 1 × 21 + 0 × 20 + 0 × 2−1 + 1 × 2−2 + 1 × 2−3 + 1 × 2−4 + 0 × 2−5)
= −10.4375
in decimal.
□
In many A/D converters and processors, the real numbers are scaled
so that the ﬁxed-point representation is in the (−1, 1) range. This has
the advantage that the multiplication of two fractions is always a fraction
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
249
and, as such, there is no overﬂow. Hence we will consider the following
representation:
ˆx = A(± ▲xxxxxx · · · x
&
'(
)
B fraction bits
)
(6.25)
where A is a positive scaling factor.
□
EXAMPLE 6.12
Represent the number ˆx = −10.4375 in Example 6.11 using a fraction-only
arrangement.
Solution
Choose A = 24 = 16 and B = 9. Then
ˆx = −10.4375 = 16 (1▲101001110)
Hence by properly choosing A and B, one can obtain any fraction-only repre-
sentation.
Note: The scalar A need not be a power of two. In fact, by choosing any real
number A we can obtain an arbitrary range. The power-of-two choice for A,
however, makes hardware implementation a little easier.
□
As discussed in the previous section, there are three main formats for
ﬁxed-point arithmetic, depending on how negative numbers are obtained.
For all these formats, positive numbers have exactly the same represen-
tation. In the following, we assume the fraction-only arrangement.
Sign-magnitude format
As the name suggests, the magnitude is
given by the B-bit fraction, and the sign is given by the MSB. Thus
ˆx =

0▲x1x2 · · · xB if x ≥0
1▲x1x2 · · · xB if x < 0
(6.26)
For example, when B = 2, ˆx = +1/4 is represented by ˆx = 0▲01, and
ˆx = −1/4 is represented by ˆx = 1▲01.
One’s-complement format
In this format, the positive numbers have
the same representation as the sign-magnitude format. When the number
is negative, then its magnitude is given by its bit-complement arrange-
ment. Thus
ˆx =

 0▲x1x2 · · · xB if x ≥0
1▲¯x1¯x2 · · · ¯xB if x < 0
(6.27)
For example, when B = 2, ˆx = +1/4 is represented by ˆx = 0▲01, and
ˆx = −1/4 is represented by ˆx = 1▲10.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

250
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
Two’s-complement format
Once again, the positive numbers have
the same representation. Negative numbers are obtained by ﬁrst comple-
menting the magnitude and then modulo-2 adding 1 to the last bit or the
least-significant bit (LSB). Stated diﬀerently, two’s-complement is formed
by subtracting the magnitude of the number from 2. Thus
ˆx =
⎧
⎨
⎩
0▲x1x2 · · · xB
if x ≥0
2 −|x| = 1▲¯x1¯x2 · · · ¯xB ⊕0▲00 · · · 1 = 1▲y1y2 · · · yB if x < 0
(6.28)
where ⊕represents modulo-2 addition and bit y is, in general, diﬀerent
from bit ¯x. For example, when B = 2, ˆx = +1/4 is represented by ˆx =
0▲01, and ˆx = −1/4 is represented by ˆx = 1▲10 ⊕0▲01 = 1▲11.
□
EXAMPLE 6.13
Let B = 3; then ˆx is a 4-bit number (sign plus 3 bits). Provide all possible
values that ˆx can take in each of the three formats.
Solution
There are 24 = 16 possible values that ˆx can take for each of the three formats,
as shown in the following table.
Binary
Sign-Magnitude
one’s
two’s
0▲111
7/8
7/8
7/8
0▲110
6/8
6/8
6/8
0▲101
5/8
5/8
5/8
0▲100
4/8
4/8
4/8
0▲011
3/8
3/8
3/8
0▲010
2/8
2/8
2/8
0▲001
1/8
1/8
1/8
0▲000
0
0
0
1▲000
−0
−7/8
−1
1▲001
−1/8
−6/8
−7/8
1▲010
−2/8
−5/8
−6/8
1▲011
−3/8
−4/8
−5/8
1▲100
−4/8
−3/8
−4/8
1▲101
−5/8
−2/8
−3/8
1▲110
−6/8
−1/8
−2/8
1▲111
−7/8
−0
−1/8
□
In Example 6.13, observe that the bit arrangement is exactly the same
as in the integer case for 4 bits. The only diﬀerence is in the position of
the binary point. Thus the MATLAB programs developed in the previ-
ous section can easily be used with proper modiﬁcations. The MATLAB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
251
function sm2oc converts a decimal sign-magnitude fraction into its one’s-
complement format, while the function oc2sm performs the inverse opera-
tion. These functions are explored in Problem P6.24. Similarly, MATLAB
functions sm2tc and tc2sm convert a decimal sign-magnitude fraction
into its two’s-complement format and vice versa, respectively; they are
explored in Problem P6.25.
6.5.3 FLOATING-POINT ARITHMETIC
In many applications, the range of numbers needed is very large. For
example, in physics one might need, at the same time, the mass of the sun
(e.g., 2.1030kg) and the mass of the electron (e.g., 9.10−31kg). These two
numbers cover a range of over 1060. For ﬁxed-point arithmetic, we would
need 62-digit numbers (or 62-digit precision). However, even the mass of
the sun is not accurately known with a precision of 5 digits, and there is
almost no measurement in physics that could be made with a precision of
62 digits. One could then imagine making all calculations with a precision
of 62 digits and throwing away 50 or 60 of them before printing out the
ﬁnal results. This would be wasteful of both CPU time and memory space.
So what is needed is a system for representing numbers in which the range
of expressible numbers is independent of the number of signiﬁcant digits.
Decimal numbers
The ﬂoating-point representation for a decimal
number x is based on expressing the number in the scientiﬁc notation:
x = ±M × 10±E
where M is called the mantissa and E is the exponent. However, there
are diﬀerent possible representations of the same number, depending on
the actual position of the decimal point—for example,
1234 = 0.1234 × 104 = 1.234 × 103 = 12.34 × 102 = · · ·
To ﬁx this problem, a ﬂoating-point number is always stored using
a unique representation, which has only one nonzero digit to the left
of the decimal point. This representation of a ﬂoating-point number is
called a normalized form. The normalized form of the preceding number
is 1.234 × 103, because it is the only representation resulting in a unique
nonzero digit to the left of the decimal point. The digit arrangement for
the normalized form is given by
ˆx =
sign of M
↓
±
x▲xx · · · x
& '( )
N-bit M
sign of E
↓
±
xx · · · x
& '( )
L-bit E
(6.29)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

252
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
For the negative numbers, we have the same formats as the ﬁxed-point
representations, including the ten’s-complement format.
The number of digits used in the exponent determine the range of
representable numbers, whereas the number of digits used in the mantissa
determine the precision of the numbers. For example, if the mantissa is
expressed using two digits plus the sign, and the exponent is expressed
using two digits plus the sign, then the real number line will be covered as
99
-99
-99
99
-9.99x10
-1.0x10
0
1.0x10
9.99x10
----------+-------------+-----------+-----------+-------------+----------->
|
accessible |
0
|
accessible |
negative
|
negative
| negative
| positive
|
positive
| positive
overflow
|
numbers
| underflow | underflow |
numbers
| overflow
The range of accessible ﬂoating-point numbers with a given representa-
tion can be large, but it is still ﬁnite. In the preceding example (e.g., with
two digits for the mantissa and two digits for the exponent), there are
only 9 × 10 × 10 × 199 =179,100 positive numbers, and as many negative
numbers, plus the number zero, for a total of 358,201 numbers that can
be represented.
Binary numbers
Although the fraction-only ﬁxed-point arithmetic
does not have any overﬂow problems when two numbers are multiplied, it
does suﬀer from overﬂow problems when two numbers are added. Also, the
ﬁxed-point numbers have limited dynamic range. Both of these aspects
are unacceptable for an intensive computational job. These limitations
can be removed by making the binary point ▲ﬂoating rather than ﬁxed.
The ﬂoating-point bit arrangement for binary-number representation
is similar to that for the decimal numbers. In practice, however, two ex-
ceptions are made. The exponent is expressed using L-bit excess-2L−1
format, and the B-bit normalized mantissa is a fractional number with
a 1 following the binary point. Note that the sign bit is the MSB of the
bit pattern. Thus the B-bit mantissa and L-bit exponent (for a total of
B + L + 1 word length) bit pattern is given by (note the reversal of the
mantissa and exponent places)
ˆx =
Sign of M
↓
±
xx · · · x
& '( )
L-bit E
▲1x · · · x
& '( )
B-bit M
(6.30)
where exponent E is adjusted so that we have a normalized mantissa—
that is, 1/2 ≤M < 1. Hence the ﬁrst bit after the binary point is always 1.
The decimal equivalent of ˆx is given by
ˆx = ±M × 2E
(6.31)
For the negative numbers, we can have the same formats as the ﬁxed-point
representations for the mantissa including two’s-complement format.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Representation of Numbers
253
However, the most widely used format for the mantissa is the sign-
magnitude one.
□
EXAMPLE 6.14
Consider a 32-bit ﬂoating-point word with the following arrangement:
ˆx = ± xx · · · x
& '( )
8-bit E
▲1x · · · x
& '( )
23-bit M
Determine the decimal equivalent of
01000001111000000000000000000000
Solution
Since the exponent is 8-bit, it is expressed in excess-27 or in excess-128 format.
Then the bit pattern can be partitioned into
ˆx =
Sign
↓
0 10000011
&
'(
)
E=131
▲11000000000000000000000
&
'(
)
M=2−1+2−2
The sign bit is 0, which means that the number is positive. The exponent code
is 131, which means that its decimal value is 131−128 = 3. Thus the bit pattern
represents the decimal number ˆx = + 
2−1 + 2−2
(23) = 22 + 21 = 6.
□
□
EXAMPLE 6.15
Let ˆx = −0.1875. Represent ˆx using the format given in (6.30), in which B = 11,
L = 4 (for a total of 16 bits), and sign-magnitude format is used for the mantissa.
Solution
We can write
ˆx = −0.1875 = −0.75 × 2−2
Hence the exponent is −2, the mantissa is 0.75, and the sign is negative. The
4-bit exponent, in excess-8 format, is expressed as 8 −2 = 6 or with bit pattern
0110. The mantissa is expressed as 11000000000. Since ˆx is negative, the bit
pattern is
ˆx ≡1011011000000000
□
The advantages of the ﬂoating-point representation are that it has
a large dynamic range and that its resolution, deﬁned as the interval
between two consecutive representable levels, is proportional to the mag-
nitude. The disadvantages include no representation for the number 0 and
the fact that the arithmetic operations are more complicated than their
ﬁxed-point representations.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

254
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
IEEE 754 standard
In the early days of the digital computer revolu-
tion, each processor design had its own internal representation for ﬂoating-
point numbers. Since ﬂoating-point arithmetic is more complicated to
implement, some of these designs did incorrect arithmetic. Therefore, in
1985 IEEE issued a standard (IEEE standard 754-1985, or IEEE-754 for
short) to allow ﬂoating-point data exchange among diﬀerent computers
and to provide hardware designers with a model known to be correct. Cur-
rently, almost all manufacturers design main processors or a dedicated
coprocessor for ﬂoating-point operations using the IEEE-754 standard
representation.
The IEEE-754 standard deﬁnes three formats for binary numbers: a
32-bit single-precision format, a 64-bit double-precision format, and an
80-bit temporary format (which is used internally by the processors or
arithmetic coprocessors to minimize rounding errors).
We will brieﬂy describe the 32-bit single-precision standard. This
standard has many similarities with the ﬂoating-point representation dis-
cussed above, but there are also diﬀerences. Remember, this is another
model advocated by IEEE. The form of this model is
ˆx =
sign of M
↓
±
xx · · · x
& '( )
8-bit E
▲xx · · · x
& '( )
23-bit M
(6.32)
The mantissa’s value is called the significand in this standard. Features
of this model are as follows:
• If the sign bit is 0, the number is positive; if the sign bit is 1, the
number is negative.
• The exponent is coded in 8-bit excess-127 (and not 128) format. Hence
the uncoded exponents are between −127 and 128.
• The mantissa is in 23-bit binary. A normalized mantissa always starts
with a bit 1, followed by the binary point, followed by the rest of the
23-bit mantissa. However, the leading bit 1, which is always present in a
normalized mantissa, is hidden (not stored) and needs to be restored for
computation. Again, note that this is diﬀerent from the usual deﬁnition
of the normalized mantissa. If all the 23 bits representing the mantissa
are set to 0, the signiﬁcand is 1 (remember the implicit leading 1). If
all 23 bits are set to 1, the signiﬁcand is almost 2 (in fact 2−2−23). All
IEEE 754 normalized numbers have a signiﬁcand that is in the interval
1 ≤M < 2.
• The smallest normalized number is 2−126, and the largest normalized
number is almost 2128. The resulting positive decimal range is roughly
10−38 to 1038 with a similar negative range.
• If E = 0 and M = 0, then the representation is interpreted as a de-
normalized number (i.e., the hidden bit is 0) and is assigned a value
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Process of Quantization and Error Characterizations
255
of ±0, depending on the sign bit (called the soft zero). Thus 0 has two
representations.
• If E = 255 and M ̸= 0, then the representation is interpreted as a
not-a-number (abbreviated as NaN). MATLAB assigns a variable NaN
when this happens—for example, 0/0.
• If E = 255 and M = 0, then the representation is interpreted as ±∞.
MATLAB assigns a variable inf when this happens—for example, 1/0.
□
EXAMPLE 6.16
Consider the bit pattern given in Example 6.14. Assuming IEEE-754 format,
determine its decimal equivalent.
Solution
The sign bit is 0 and the exponent code is 131, which means that the exponent
is 131 −127 = 4. The signiﬁcand is 1 + 2−1 + 2−2 = 1.75. Hence the bit pattern
represents
ˆx = +(1 + 2−1 + 2−2)(24) = 24 + 23 + 22 = 28
which is diﬀerent from the number in Example 6.14.
□
MATLAB employs the 64-bit double-precision IEEE-754 format for
all its number representations and the 80-bit temporary format for its in-
ternal computations. Hence all calculations that we perform in MATLAB
are in fact ﬂoating-point computations. Simulating a diﬀerent ﬂoating-
point format in MATLAB would be much more complicated and would
not add any more insight to our understanding than the native format.
Hence we will not consider a MATLAB simulation of ﬂoating-point arith-
metic as we did for ﬁxed-point.
6.6 THE PROCESS OF QUANTIZATION AND ERROR
CHARACTERIZATIONS
From the discussion of number representations in the previous section, it
should be clear that a general inﬁnite-precision real number must be as-
signed to one of the ﬁnite representable numbers, given a speciﬁc structure
for the ﬁnite-length register (i.e., the arithmetic as well as the format).
Usually, in practice, there are two diﬀerent operations by which this as-
signment is made to the nearest number or level: the truncation operation
and the rounding operation. These operations aﬀect the accuracy as well
as general characteristics of digital ﬁlters and DSP operations.
We assume, without loss of generality, that there are B + 1 bits in
the ﬁxed-point (fractional) arithmetic or in the mantissa of ﬂoating-point
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

256
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
arithmetic including the sign bit. Then the resolution (∆) is given by
∆= 2−B

absolute in the case of ﬁxed-point arithmetic
relative in the case of ﬂoating-point arithmetic
(6.33)
6.6.1 FIXED-POINT ARITHMETIC
The quantizer block diagram in this case is given by
x
Inﬁnite-precision
−→Quantizer
B, ∆
Q[·] −→
Q[x]
Finite-precision
where B, the number of fractional bits, and ∆, the resolution, are the pa-
rameters of the quantizer. We will denote the ﬁnite word-length number,
after quantization, by Q[x] for an input number x. Let the quantization
error be given by
e
△= Q[x] −x
(6.34)
We will analyze this error for both the truncation and the rounding
operations.
Truncation operation
In this operation, the number x is truncated
beyond B signiﬁcant bits (i.e., the rest of the bits are eliminated) to obtain
QT[x]. In MATLAB, to obtain a B-bit truncation, we have to ﬁrst scale
the number x upward by 2B, then use the fix function on the scaled
number, and ﬁnally scale the result down by 2−B. Thus the MATLAB
statement xhat = fix(x*2ˆB)/2ˆB; implements the desired operation.
We will now consider each of the three formats.
Sign-magnitude format
If the number x is positive, then after trun-
cation QT[x] ≤x since some value in x is lost. Hence quantizer error for
truncation denoted by eT is less than or equal to 0 or eT ≤0. However,
since there are B bits in the quantizer, the maximum error in terms of
magnitude is
|eT| = 0▲00 · · · 0
& '( )
B bits
111 · · · = 2−B (decimal)
(6.35)
or
−2−B ≤eT ≤0,
for x ≥0
(6.36)
Similarly, if the x < 0 then after truncation QT[x] ≥x since QT[x] is less
negative, or eT ≥0. The largest magnitude of this error is again 2−B, or
0 ≤eT ≤2−B,
for x < 0
(6.37)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Process of Quantization and Error Characterizations
257
1
1
0.75
0.75
0.5
0.5
0.25
0.25
0
0
−0.25
−0.25
−0.5
−0.5
−0.75
−0.75
−1
−1
x
xhat
x
xhat
FIGURE 6.19
Truncation error characteristics in the sign-magnitude format
□
EXAMPLE 6.17
Let −1 < x < 1, and let B = 2. Using MATLAB, verify the truncation error
characteristics.
Solution
The resolution is ∆= 2−2 = 0.25. Using the following MATLAB script, we can
verify the truncation error eT relations given in (6.36) and (6.37).
x = [-1+2ˆ(-10):2ˆ(-10):1-2ˆ(-10)];
% Sign-Mag numbers between -1 and 1
B = 2;
% Number of bits for truncation
xhat = fix(x*2ˆB)/2ˆB
% Truncation
plot(x,x,’g’,x,xhat,’r’,’linewidth’,1); % Plot
The resulting plots of x and ˆx are shown in Figure 6.19. Note that the plot of
ˆx has a staircase shape and that it satisﬁes (6.36) and (6.37).
□
One’s-complement format
For x ≥0, we have the same character-
istics for eT as in sign-magnitude format—that is,
−2−B ≤eT ≤0,
for x ≥0
(6.38)
For x < 0, the representation is obtained by complementing all bits,
including the sign bit. To compute the maximum error, let
x = 1▲b1b2 · · · bB000 · · · = −{▲(1 −b1) (1 −b2) · · · (1 −bB) 111 · · ·}
After truncation, we obtain
QT[x] = 1▲b1b2 · · · bB = −{▲(1 −b1) (1 −b2) · · · (1 −bB)}
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

258
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
1
1
0.75
0.75
0.5
0.5
0.25
0.25
0
0
−0.25
−0.25
−0.5
−0.5
−0.75
−0.75
−1
−1
x
xhat
x
xhat
FIGURE 6.20
Truncation error characteristics in the one’s-complement format
Clearly, x is more negative than QT[x] or x ≤QT[x] or eT ≥0. In fact,
the maximum truncation error is
eTmax = 0▲00 · · · 0111 · · · = 2−B (decimal)
Hence
0 ≤eT ≤2−B,
for x < 0
(6.39)
□
EXAMPLE 6.18
Again let −1 < x < 1 and B = 2 with the resolution ∆= 2−2 = 0.25. Using
MATLAB, verify the truncation error eT relations given in (6.38) and (6.39).
Solution
The MATLAB script uses functions sm2oc and oc2sm, which are explored in
Problem P6.24.
x = [-1+2ˆ(-10):2ˆ(-10):1-2ˆ(-10)];
% Sign-Magnitude numbers between -1 and 1
B = 2;
% Select bits for truncation
y = sm2oc(x,B);
% Sign-mag to one’s complement
yhat = fix(y*2ˆB)/2ˆB;
% Truncation
xhat = oc2sm(yhat,B);
% One’s-complement to sign-mag
plot(x,x,’g’,x,xhat,’r’,’linewidth’,1); % Plot
The resulting plots of x and ˆx are shown in Figure 6.20. Note that the
plot of ˆx is identical to the plot in Figure 6.19 and that it satisﬁes (6.38)
and (6.39).
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Process of Quantization and Error Characterizations
259
Two’s-complement format
Once again, for x ≥0, we have
−2−B ≤eT ≤0,
for x ≥0
(6.40)
For x < 0, the representation is given by 2 −|x| where |x| is the magnitude.
Hence the magnitude of x is given by
|x| = 2 −x
(6.41)
with x = 1▲b1b2 · · · bBbB+1 · · ·. After truncation to B bits, we obtain
QT[x] = 1▲b1b2 · · · bB, the magnitude of which is
|QT[x]| = 2 −QT[x]
(6.42)
From (6.41) and (6.42),
|QT[x]| −|x| = x −QT[x] = 1▲b1b2 · · · bBbB+1 · · · −1▲b1b2 · · · bB
= 0▲00 · · · 0bB+1 · · ·
(6.43)
The largest change in magnitude from (6.43) is
0▲00 · · · 0111 · · · = 2−B (decimal)
(6.44)
Since the change in the magnitude is positive, then after truncation QT[x]
becomes more negative, which means that QT[x] ≤x. Hence
−2−B ≤eT ≤0,
for x < 0
(6.45)
□
EXAMPLE 6.19
Again consider −1 < x < 1 and B = 2 with the resolution ∆= 2−2 = 0.25.
Using MATLAB, verify the truncation error eT relations given in (6.40) and
(6.45).
Solution
The MATLAB script uses functions sm2tc and tc2sm, which are explored in
Problem P6.25.
x = [-1+2ˆ(-10):2ˆ(-10):1-2ˆ(-10)]; % Sign-magnitude numbers between -1 and 1
B = 2;
% Select bits for truncation
y = sm2tc(x);
% Sign-mag to two’s complement
yhat = fix(y*2ˆB)/2ˆB;
% Truncation
xq = tc2sm(yq );
% Two’s-complement to sign-mag
plot(x,x,’g’,x,xhat,’r’,’linewidth’,1); % Plot
The resulting plots of x and ˆx are shown in Figure 6.21. Note that the plot of
ˆx is also a staircase graph but is below the x graph and that it satisﬁes (6.40)
and (6.45).
□
Collecting results (6.36)–(6.40) and (6.45) along with those in
Figures 6.19–6.21, we conclude that the truncation characteristics for
ﬁxed-point arithmetic are the same for the sign-magnitude and the one’s-
complement formats but are diﬀerent for the two’s-complement format.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

260
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
1
1
0.75
0.75
0.5
0.5
0.25
0.25
0
0
−0.25
−0.25
−0.5
−0.5
−0.75
−0.75
−1
−1
x
xhat
x
xhat
FIGURE 6.21
Truncation error characteristics in the two’s-complement format
Rounding operation
In this operation, the real number x is rounded
to the nearest representable level, which we will refer to as QR[x]. In
MATLAB, to obtain a B-bit rounding approximation, we have to ﬁrst
scale the number x up by 2B, then use the round function on the scaled
number, and ﬁnally scale the result down by 2−B. Thus the MAT-
LAB statement xhat = round(x*2ˆB)/2ˆB; implements the desired
operation.
Since the quantization step or resolution is ∆= 2−B, the magnitude
of the maximum error is
|eR|max = ∆
2 = 1
22−B
(6.46)
Hence for all three formats, the quantizer error due to rounding, denoted
by eR, satisﬁes
−1
22−B ≤eR ≤1
22−B
(6.47)
□
EXAMPLE 6.20
Demonstrate the rounding operations and the corresponding error characteris-
tics on the signal of Examples 6.17–6.19 using the three formats.
Solution
Since the rounding operation assigns values that can be larger than the
unquantized values, which can create problems for the two’s- and one’s-
complement formats, we will restrict the signal over the interval [−1, 1 −
2−B−1]. The following MATLAB script shows the two’s-complement format
rounding, but other scripts are similar (readers are encouraged to verify).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

The Process of Quantization and Error Characterizations
261
0
0.75
0.5
0.25
−0.25
−0.5
−0.75
−1
xxhat
0.75
0.5
0.25
0
−0.25
−0.5
−0.75
−1
x
(b) One’s-Complement Format
xhat
0
0.75
0.5
0.25
−0.25
−0.5
−0.75
−1
xxhat
0.75
0.5
0.25
0
−0.25
−0.5
−0.75
−1
x
(b) Sign-Magnitude Format
xhat
0
0.75
0.5
0.25
−0.25
−0.5
−0.75
−1
xxhat
xhat
0.75
0.5
0.25
0
−0.25
−0.5
−0.75
−1
x
(b) Two’s-Complement Format
FIGURE 6.22
Rounding error characteristics in the ﬁxed-point representation
B = 2;
% Select bits for rounding
x = [-1:2ˆ(-10):1-2ˆ(-B-1)];
% Sign-magnitude numbers between -1 and 1
y = sm2tc(x);
% Sign-mag to two’s complement
yq = round(y*2ˆB)/2ˆB;
% Rounding
xq = tc2sm(yq);
% Two’s-complement to sign-mag
The resulting plots for the sign-magnitude, one’s-, and two’s-complement
formats are shown in Figure 6.22. These plots do satisfy (6.47).
□
Comparing the error characteristics of the truncation and rounding
operations given in Figures 6.19 through 6.22, it is clear that the rounding
operation is a superior one for the quantization error. This is because the
error is symmetric with respect to zero (or equal positive and negative
distribution) and because the error is the same across all three formats.
Hence we will mostly consider the rounding operation for the ﬂoating-
point arithmetic as well as for further analysis.
6.6.2 FLOATING-POINT ARITHMETIC
In this arithmetic, the quantizer aﬀects only the mantissa M. However,
the number x is represented by M × 2E where E is the exponent. Hence
the quantizer errors are multiplicative and depend on the magnitude of
x. Therefore, the more appropriate measure of error is the relative error
rather than the absolute error, (Q[x] −x). Let us deﬁne the relative error,
ε, as
ε
△= Q[x] −x
x
(6.48)
Then the quantized value Q[x] can be written as
Q[x] = x + εx = x (1 + ε)
(6.49)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

262
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
When Q[x] is due to the rounding operation, then the error in the man-
tissa is between [−1
22−B, 1
22−B]. In this case, we will denote the relative
error by εR. Then from (6.31), the absolute error, QR[x] −x = εRx, is
between

−1
22−B

2E ≤εRx ≤
1
22−B

2E
(6.50)
Now for a given E, and since the mantissa is between 1
2 ≤M < 1 (this is
not the IEEE-754 model), the number x is between
2E−1 ≤x < 2E
(6.51)
Hence from (6.50) and using the smallest value in (6.51), we obtain
−2−B ≤εR ≤2−B
(6.52)
This relative error relation, (6.52), will be used in subsequent analysis.
6.7 QUANTIZATION OF FILTER COEFFICIENTS
We now study the ﬁnite word-length eﬀects on the ﬁlter responses, pole-
zero locations, and stability when the ﬁlter coeﬃcients are quantized. We
will separately discuss the issues relating to IIR and FIR ﬁlters since we
can obtain simpler results for FIR ﬁlters. We begin with the case of IIR
ﬁlters.
6.7.1 IIR FILTERS
Consider a general IIR ﬁlter described by
H(z) =
M
k=0 bkz−k
1 + N
k=1 akz−k
(6.53)
where aks and bks are the ﬁlter coeﬃcients. Now assume that these coeﬃ-
cients are represented by their ﬁnite-precision numbers ˆaks and ˆbks. Then
we get a new ﬁlter system function
H(z) →ˆH(z)
△=
M
k=0 ˆbkz−k
1 + N
k=1 ˆakz−k
(6.54)
Since this is a new ﬁlter, we want to know how “diﬀerent” this ﬁlter is
from the original one H(z). Various aspects can be compared; for example,
we may want to compare their magnitude responses, or phase responses,
or change in their pole-zero locations, and so on. A general analytical
expression to compute this change in all these aspects is diﬃcult to derive.
This is where MATLAB can be used to investigate this change and its
overall eﬀect on the usability of the ﬁlter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
263
6.7.2 EFFECT ON POLE-ZERO LOCATIONS
One aspect can be reasonably analyzed, which is the movement of ﬁlter
poles when ak is changed to ˆak. This can be used to check the stabil-
ity of IIR ﬁlters. A similar movement of zeros to changes in numerator
coeﬃcients can also be analyzed.
To evaluate this movement, consider the denominator polynomial of
H(z) in (6.53),
D(z)
△= 1 +
N
	
k=1
akz−k =
N

ℓ=1

1 −pℓz−1
(6.55)
where {pℓ}s are the poles of H(z). We will regard D(z) as a function
D(p1, . . . , pN) of poles {p1, . . . , pN} where each pole pℓis a function of the
ﬁlter coeﬃcients {a1, . . . , aN}—that is, pℓ= fℓ(a1, . . . , aN), ℓ= 1, . . . N.
Then the change in the denominator D(z) due to a change in the kth
coeﬃcient ak is given by
∂D(z)
∂ak

=
∂D(z)
∂p1
 ∂p1
∂ak

+
∂D(z)
∂p2
 ∂p2
∂ak

+· · ·+
∂D(z)
∂pN
 ∂pN
∂ak

(6.56)
where, from (6.55),
∂D(z)
∂pi

=
∂
∂pi
 N

ℓ=1

1 −pℓz−1

= −z−1 
ℓ̸=i

1 −pℓz−1
(6.57)
From (6.57), note that

∂D(z)
∂pi

z=pℓ
= 0 for ℓ̸= i. Hence from (6.56) we
obtain
∂D(z)
∂ak

z=pℓ
=
∂D(z)
∂pℓ

z=pℓ
 ∂pℓ
∂ak

or
 ∂pℓ
∂ak

=

∂D(z)
∂ak

z=pℓ

∂D(z)
∂pℓ

z=pℓ
(6.58)
Now
∂D(z)
∂ak
 
z=pℓ
=
∂
∂ak
-
1 +
N
	
i=1
aiz−i
.
z=pℓ
= z−k
z=pℓ= p−k
ℓ
(6.59)
From (6.57), (6.58), and (6.59), we obtain
 ∂pℓ
∂ak

=
p−k
ℓ
−z−1 ,
i̸=ℓ(1 −pi z−1)

z=pℓ
= −
pN−k
ℓ
,
i̸=ℓ(pℓ−pi )
(6.60)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

264
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
Re{z}
0
z1
z2
z3
z*3
z*1
z*2
0
0
0
z*3
z3
z1
z*1
z*2
z2
(a) Direct Form Arrangement
(b) Cascade or Parallel Form Arrangement
Im{z}
FIGURE 6.23
z-plane plots of tightly clustered poles of a digital ﬁlter
Finally, the total perturbation error △pℓcan be expressed as
△pℓ=
N
	
k=1
∂pℓ
∂ak
△ak
(6.61)
This formula measures the movement of the ℓth pole, pℓ, to changes in
each of the coeﬃcient {ak}; hence it is known as a sensitivity formula.
It shows that if the coeﬃcients {ak} are such that if the poles pℓand pi are
very close for some ℓ, i, then (pℓ−pi) is very small and as a result the ﬁlter
is very sensitive to the changes in ﬁlter coeﬃcients. A similar result can
be obtained for the sensitivity of zeros to changes in the parameters {bk}.
To investigate this further in the light of various ﬁlter realizations,
consider the z-plane plot shown in Figure 6.23(a), where poles are tightly
clustered. This situation arises in wideband frequency selective ﬁlters such
as lowpass or highpass ﬁlters. Now if we were to realize this ﬁlter using the
direct form (either I or II), then the ﬁlter has all these tightly clustered
poles, which makes the direct form realization very sensitive to coeﬃcient
changes due to ﬁnite word length. Thus the direct form realizations will
suﬀer severely from coeﬃcient quantization eﬀects.
On the other hand, if we were to use either the cascade or the par-
allel forms, then we would realize the ﬁlter using second-order sections
containing widely separated poles, as shown in Figure 6.23(b). Thus each
second-order section will have low sensitivity in that its pole locations
will be perturbed only slightly. Consequently, we expect that the overall
system function H(z) will be perturbed only slightly. Thus the cascade
or the parallel forms, when realized properly, will have low sensitivity to
the changes or errors in ﬁlter coeﬃcients.
□
EXAMPLE 6.21
Consider a digital resonator that is a second-order IIR ﬁlter given by
H(z) =
1
1 −(2r cos θ) z−1 + r2z−2
(6.62)
Analyze its sensitivity to pole locations when a 3-bit sign-magnitude format is
used for the coeﬃcient representation.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
265
Im{z}
x{n}
y{n}
UC
r
r
0
(a)
(b)
Re{z}
z−1
z−1
−r 2
FIGURE 6.24
Digital ﬁlter in Example 6.21: (a) pole-zero plot, (b) ﬁlter real-
ization
Solution
The ﬁlter has two complex-conjugate poles at
p1 = rejθ
and
p2 = re−jθ = p∗
1
For a proper operation as a resonator, the poles must be close to the unit
circle—that is, r ≃1 (but r < 1). Then the resonant frequency ωr ≃θ.
The zero-pole diagram is shown in Figure 6.24 along with the ﬁlter realiza-
tion. Let r = 0.9 and θ = π/3. Then from (6.62),
a1 = −2r cos θ = −0.9
and
a2 = r2 = 0.81
We now represent a1
and a2, each using 3-bit sign-magnitude format
representation—that is,
ak = ± ▲b1 b2 b3 = ± 
b12−1 + b22−2 + b32−3
,
k = 1, 2
where bj represents the jth bit and ▲represents the binary point. Then for the
closest representation, we must have
ˆa1 = 1▲1 1 1 = −0.875
and
ˆa2 = 0▲1 1 0 = +0.75
Hence |△a1| = 0.025 and |△a2| = 0.06. Consider the sensitivity formula (6.61),
in which
∂p1
∂a1 = −
p2−1
1
(p1 −p∗
1) =
−p1
2 Im {p1} =
−rejθ
2r (sin θ) = ejπ/3
√
3
, and
∂p1
∂a2 = −
p2−2
1
(p1 −p∗
1) =
−1
2 Im {p1} =
1
0.9
√
3
Using (6.61), we obtain
|△p1| ≤
∂p1
∂a1
 |△a1| +
∂p1
∂a2
 |△a2|
=
1
√
3
(0.025) +
1
0.9
√
3
(0.06) = 0.0529
(6.63)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

266
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
To determine the exact locations of the changed poles, consider the changed
denominator
ˆD (z) = 1−0.875z−1 + 0.75z−2 = 
1 −0.866ej0.331πz−1 
1 −0.866e−j0.331πz−1
Thus the changed pole locations are ˆp1 = 0.866ej0.331π = ˆp∗
2. Then |△p1| =
0.9eiπ/3 −0.866ei0.331π = 0.0344, which agrees with (6.63).
□
Analysis using MATLAB
To investigate the eﬀect of coeﬃcient
quantization on ﬁlter behavior, MATLAB is an ideal vehicle. Using func-
tions developed in previous sections, we can obtain quantized coeﬃcients
and then study such aspects as pole-zero movements, frequency response,
or impulse response. We will have to represent all ﬁlter coeﬃcients using
the same number of integer and fraction bits. Hence, instead of quantizing
each coeﬃcient separately, we will develop the function QCoeff for coeﬃ-
cient quantization. This function implements quantization using rounding
operation on sign-magnitude format. Although similar functions can be
written for truncation as well as for other formats, we will analyze the
eﬀects using the Qcoeff function as explained previously.
function [y,L,B] = QCoeff(x,N)
%
[y,L,B] = QCoeff(x,N)
%
Coefficient quantization using N=1+L+B bit representation
%
with rounding operation
%
y: quantized array (same dim as x)
%
L: number of integer bits
%
B: number of fractional bits
%
x: a scalar, vector, or matrix
%
N: total number of bits
xm = abs(x);
L = max(max(0,fix(log2(xm(:)+eps)+1))); % Integer bits
if (L > N)
errmsg = [’ *** N must be at least ’,num2str(L),’ ***’]; error(errmsg);
end
B = N-L;
% Fractional bits
y = xm./(2ˆL); y = round(y.*(2ˆN)); % Rounding to N bits
y = sign(x).*y*(2ˆ(-B));
% L+B+1 bit representation
The Qcoeff function represents each coeﬃcient in the x array using
N+1-bit (including the sign bit) representation. First, it determines the
number of bits L needed for integer representation for the magnitude-wise
largest coeﬃcient, and then it assigns N-L bits to the fraction part. The
resulting number is returned in B. Thus all coeﬃcients have the same bit
pattern L+B+1. Clearly, N ≥L.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
267
□
EXAMPLE 6.22
Consider the digital resonator in Example 6.21. Determine the change in the
pole locations using MATLAB.
Solution
The ﬁlter coeﬃcients, a1 = −0.9 and a2 = 0.81, can be quantized using
>> x = [-0.9,0.81]; [y,L,B] = Qcoeff(x,3)
y = -0.8750
0.7500
L =
0
B =
3
as expected. Now using the following MATLAB script, we can determine the
change in the location of the poles.
% Unquantized parameters
r = 0.9; theta = pi/3; a1 = -2*r*cos(theta); a2 = r*r;
p1 = r*exp(j*theta); p2 = p1’;
% Quantized parameters: N = 3;
[ahat,L,B] = Qcoeff([a1,a2],3); rhat = sqrt(ahat(2));
thetahat = acos(-ahat(1)/(2*rhat)); p1hat = rhat*exp(j*thetahat); p2 = p1’;
% Changes in pole locations
Dp1 = abs(p1-p1hat)
Dp1 = 0.0344
This is the same as before.
□
□
EXAMPLE 6.23
Consider the following IIR ﬁlter with 10 poles closely packed at a radius of
r = 0.9 around angles ±45◦with a separation of 5◦. Due to large number of
poles, the denominator coeﬃcients have values that require 6 bits for the integer
part. Using 9 bits for the fractional part for a total of 16-bit representation, we
compute and plot the new locations of poles:
r = 0.9; theta = (pi/180)*[-55:5:-35,35:5:55]’;
p = r*exp(j*theta); a = poly(p); b = 1;
% Direct form: quantized coefficients
N = 15; [ahat,L,B] = Qcoeff(a,N);
TITLE = sprintf(’%i-bit (1+%i+%i) Precision’,N+1,L,B);
% Comparison of pole-zero plots
subplot(1,2,1); [HZ,HP,Hl] = zplane(1,a);
set(HZ,’color’,’g’,’linewidth’,1); set(HP,’color’,’g’,’linewidth’,1);
set(Hl,’color’,’w’); axis([-1.1,1.1,-1.1,1.1]);
title(’Infinite Precision’,’fontsize’,10,’fontweight’,’bold’);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

268
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
−1
−1
0
0.5
1
−1
0
1
Infinite Precision
10
0
0.5
1
0
0.5
1
Imaginary Part
Imaginary Part
16-bit (1+6+9) Precision
10
−0.5
−0.5
−0.5
−1
−0.5
Real Part
Real Part
0.5
FIGURE 6.25
Pole-zero plots for direct form structure in Example 6.23
subplot(1,2,2); [HZhat,HPhat,Hlhat] = zplane(1,ahat);
set(HZhat,’color’,’r’,’linewidth’,1); set(HPhat,’color’,’r’,’linewidth’,1);
set(Hlhat,’color’,’w’); title(TITLE,’fontsize’,10,’fontweight’,’bold’);
axis([-1.1,1.1,-1.1,1.1]);
Figure 6.25 shows the pole-zero plots for ﬁlters with both inﬁnite and 16-
bit precision coeﬃcients. Clearly, with 16-bit word length, the resulting ﬁlter is
completely diﬀerent from the original one and is unstable. To investigate the
eﬀect of ﬁnite word length on the cascade form structure, we ﬁrst converted
the direct form coeﬃcients into the cascade form coeﬃcients using the dir2cas
function, quantized the resulting set of coeﬃcients, and then converted back to
the direct form for pole-zero plotting. We show results for two diﬀerent word
lengths. In the ﬁrst case, we used the same 16-bit word length. Since the cascade
coeﬃcients have smaller integer parts that require only one integer bit, the
number of fractional bits is 14. In the second case, we used 9 fractional bits
(same as those in the direct form) for a total word length of 11 bits.
% Cascade form: quantized coefficients: same N
[b0,B0,A0] = dir2cas(b,a);
[BAhat1,L1,B1] = Qcoeff([B0,A0],N);
TITLE1 = sprintf(’%i-bit (1+%i+%i) Precision’,N+1,L1,B1);
Bhat1 = BAhat1(:,1:3); Ahat1 = BAhat1(:,4:6);
[bhat1,ahat1] = cas2dir(b0,Bhat1,Ahat1);
subplot(1,2,1);
[HZhat1,HPhat1,Hlhat1] = zplane(bhat1,ahat1);
set(HZhat1,’color’,’g’,’linewidth’,1); set(HPhat1,’color’,’g’,’linewidth’,1);
set(Hlhat1,’color’,’w’); axis([-1.1,1.1,-1.1,1.1]);
title(TITLE1,’fontsize’,10,’fontweight’,’bold’);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
269
0
0.5
1
−1
0
1
Real Part
Real Part
16-bit (1+1+14) Precision
10
−1
0
0.5
1
0
0.5
1
Imaginary Part
Imaginary Part
11-bit (1+1+9) Precision
10
−0.5
−1
−0.5
−0.5
−1
−0.5
0.5
FIGURE 6.26
Pole-zero plots for cascade form structure in Example 6.23
% Cascade form: quantized coefficients: same B (N=L1+B)
N1 = L1+B; [BAhat2,L2,B2] = Qcoeff([B0,A0],N1);
TITLE2 = sprintf(’%i-bit (1+%i+%i) Precision’,N1+1,L2,B2);
Bhat2 = BAhat2(:,1:3); Ahat2 = BAhat2(:,4:6);
[bhat2,ahat2] = cas2dir(b0,Bhat2,Ahat2);
subplot(1,2,2);
[HZhat2,HPhat2,Hlhat2] = zplane(bhat2,ahat2);
set(HZhat2,’color’,’r’,’linewidth’,1); set(HPhat2,’color’,’r’,’linewidth’,1);
set(Hlhat2,’color’,’w’);title(TITLE2,’fontsize’,10,’fontweight’,’bold’);
axis([-1.1,1.1,-1.1,1.1]);
The results are shown in Figure 6.26. We observe that not only for 16-bit rep-
resentation but also for 11-bit representation, the resulting ﬁlter is essentially
the same as the original one and is stable. Clearly, the cascade form structure
has better ﬁnite-word-length properties than the direct form structure.
□
6.7.3 EFFECTS ON FREQUENCY RESPONSE
The frequency response of the IIR ﬁlter in (6.38) is given by
H(ejω) =
M
k=0 bk e−jωk
1 + N
k=1 ak e−jωk
(6.64)
When the coeﬃcients {ak} and {bk} are quantized to {ˆak} and {ˆbk},
respectively, the new frequency response is given by
ˆH(ejω) =
M
k=0 ˆbk e−jωk
1 + N
k=1 ˆak e−jωk
(6.65)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

270
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
One can perform analysis similar to that for the movement of poles to
obtain maximum change in the magnitude or phase responses due to
changes in ﬁlter coeﬃcients. However, such an analysis is very complicated
and may not add any new insight. Hence we will study these eﬀects using
MATLAB. We provide the following two examples.
□
EXAMPLE 6.24
Compute and plot magnitude responses of ﬁlter structures given for the ﬁlter
in Example 6.23.
Solution
The ﬁlter is a bandpass ﬁlter with 10 tightly clustered poles implemented using
the direct and the cascade forms. For the direct form structure, we compute the
magnitude response for inﬁnite precision as well as for 16-bit quantization. For
the cascade form structure, we use 16-bit and 11-bit representations.
r = 0.9; theta = (pi/180)*[-55:5:-35,35:5:55]’;
p = r*exp(j*theta); a = poly(p); b = 1;
w = [0:500]*pi/500; H = freqz(b*1e-4,a,w);
magH = abs(H); magHdb = 20*log10(magH);
% Direct form: quantized coefficients
N = 15; [ahat,L,B] = Qcoeff(a,N);
TITLE = sprintf(’%i-bit (1+%i+%i) Precision (DF)’,N+1,L,B);
Hhat = freqz(b*1e-4,ahat,w); magHhat = abs(Hhat);
% Cascade form: quantized coefficients: Same N
[b0,B0,A0] = dir2cas(b,a);
[BAhat1,L1,B1] = Qcoeff([B0,A0],N);
TITLE1 = sprintf(’%i-bit (1+%i+%i) Precision (CF)’,N+1,L1,B1);
Bhat1 = BAhat1(:,1:3); Ahat1 = BAhat1(:,4:6);
[bhat1,ahat1] = cas2dir(b0,Bhat1,Ahat1);
Hhat1 = freqz(b*1e-4,ahat1,w); magHhat1 = abs(Hhat1);
% Cascade form: quantized coefficients: Same B (N=L1+B)
N1 = L1+B; [BAhat2,L2,B2] = Qcoeff([B0,A0],N1);
TITLE2 = sprintf(’%i-bit (1+%i+%i) Precision (CF)’,N1+1,L2,B2);
Bhat2 = BAhat2(:,1:3); Ahat2 = BAhat2(:,4:6);
[bhat2,ahat2] = cas2dir(b0,Bhat2,Ahat2);
Hhat2 = freqz(b*1e-4,ahat2,w); magHhat2 = abs(Hhat2);
% Comparison of Magnitude Plots
Hf_1 = figure(’paperunits’,’inches’,’paperposition’,[0,0,6,4]);
subplot(2,2,1); plot(w/pi,magH,’g’,’linewidth’,2); axis([0,1,0,0.7]);
%xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Magnitude Response’,’fontsize’,10);
title(’Infinite Precision (DF)’,’fontsize’,10,’fontweight’,’bold’);
subplot(2,2,2); plot(w/pi,magHhat,’r’,’linewidth’,2); axis([0,1,0,0.7]);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
271
%xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Magnitude Response’,’fontsize’,10);
title(TITLE,’fontsize’,10,’fontweight’,’bold’);
subplot(2,2,3); plot(w/pi,magHhat1,’r’,’linewidth’,2); axis([0,1,0,0.7]);
xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Magnitude Response’,’fontsize’,10);
title(TITLE1,’fontsize’,10,’fontweight’,’bold’);
subplot(2,2,4); plot(w/pi,magHhat2,’r’,’linewidth’,2); axis([0,1,0,0.7]);
xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Magnitude Response’,’fontsize’,10);
title(TITLE2,’fontsize’,10,’fontweight’,’bold’);
The plots are shown in Figure 6.27. The top row shows plots for the direct
form, and the bottom row shows those for the cascade form. As expected,
the magnitude plot of the direct form is severely distorted for 16-bit repre-
sentation, while those for the cascade form are preserved even for 11-bit word
length.
□
□
EXAMPLE 6.25
An eighth-order bandpass ﬁlter was obtained using the elliptic ﬁlter design
approach. This and other design methods will be discussed in Chapter 8. The
MATLAB functions needed for this design are shown in the script below.
This design produces direct form ﬁlter coeﬃcients bk and ak, using 64-bit
ﬂoating-point arithmetic, which gives the precision of 15 decimals and hence
can be considered as unquantized coeﬃcients. Table 6.1 shows these ﬁlter
coeﬃcients.
Represent the unquantized ﬁlter coeﬃcients using 16-bit and 8-bit word
lengths. Plot the ﬁlter log-magnitude responses and pole-zero locations for both
the inﬁnite and ﬁnite word-length coeﬃcients.
TABLE 6.1
Unquantized IIR ﬁlter coeﬃcients used in Example 6.25
k
bk
ak
0
0.021985541264351
1.000000000000000
1
0.000000000000000
−0.000000000000004
2
−0.032498273955222
2.344233276056572
3
0.000000000000000
−0.000000000000003
4
0.046424673058794
2.689868616770005
5
0.000000000000000
0.000000000000001
6
−0.032498273955221
1.584557559015230
7
0.000000000000000
0.000000000000001
8
0.021985541264351
0.413275250482975
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

272
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
Magnitude Response
Infinite Precision (DF)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
Magnitude Response
16-bit (1+6+9) Precision (DF)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
Magnitude Response
16-bit (1+1+14) Precision (CF)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
Magnitude Response
11-bit (1+1+9) Precision (CF)
Digital Frequency in π Units
Digital Frequency in π Units
FIGURE 6.27
Magnitude plots for direct and cascade form structures in
Example 6.24
Solution
Unlike the previous example, some of the ﬁlter coeﬃcient values (speciﬁcally
those of the autoregressive part) are greater than 1 and hence require bits for
the integer part. This assignment is done for all coeﬃcients since, in practice,
the same bit-pattern is used for the ﬁlter representation. These and other steps
are given in the following MATLAB script.
% The following 3 lines produce filter coefficients shown in Table 6.1.
wp = [0.35,0.65]; ws = [0.25,0.75]; Rp = 1; As = 50;
[N, wn] = ellipord(wp, ws, Rp, As);
[b,a] = ellip(N,Rp,As,wn);
w = [0:500]*pi/500; H = freqz(b,a,w); magH = abs(H);
magHdb = 20*log10(magH);
% 16-bit word-length quantization
N1 = 15; [bahat,L1,B1] = QCoeff([b;a],N1);
TITLE1 = sprintf(’%i-bits (1+%i+%i)’,N1+1,L1,B1);
bhat1 = bahat(1,:); ahat1 = bahat(2,:);
Hhat1 = freqz(bhat1,ahat1,w); magHhat1 = abs(Hhat1);
magHhat1db = 20*log10(magHhat1); zhat1 = roots(bhat1);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
273
% 8-bit word-length quantization
N2 = 7; [bahat,L2,B2] = QCoeff([b;a],N2);
TITLE2 = sprintf(’%i-bits (1+%i+%i)’,N2+1,L2,B2);
bhat2 = bahat(1,:); ahat2 = bahat(2,:);
Hhat2 = freqz(bhat2,ahat2,w); magHhat2 = abs(Hhat2);
magHhat2db = 20*log10(magHhat2); zhat2 = roots(bhat2);
% Plots
Hf_1 = figure(’paperunits’,’inches’,’paperposition’,[0,0,6,5]);
% Comparison of log-magnitude responses: 16 bits
subplot(2,2,1); plot(w/pi,magHdb,’g’,’linewidth’,1.5); axis([0,1,-80,5]);
hold on; plot(w/pi,magHhat1db,’r’,’linewidth’,1); hold off;
xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Decibels’,’fontsize’,10);
title([’Log-Mag plot: ’,TITLE1],’fontsize’,10,’fontweight’,’bold’);
% Comparison of pole-zero plots: 16 bits
subplot(2,2,3); [HZ,HP,Hl] = zplane([b],[a]); axis([-2,2,-2,2]); hold on;
set(HZ,’color’,’g’,’linewidth’,1,’markersize’,4);
set(HP,’color’,’g’,’linewidth’,1,’markersize’,4);
plot(real(zhat1),imag(zhat1),’r+’,’linewidth’,1);
title([’PZ Plot: ’,TITLE1],’fontsize’,10,’fontweight’,’bold’);
hold off;
% Comparison of log-magnitude responses: 8 bits
subplot(2,2,2); plot(w/pi,magHdb,’g’,’linewidth’,1.5); axis([0,1,-80,5]);
hold on; plot(w/pi,magHhat2db,’r’,’linewidth’,1); hold off;
xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Decibels’,’fontsize’,10);
title([’Log-Mag plot: ’,TITLE2],’fontsize’,10,’fontweight’,’bold’);
% Comparison of pole-zero plots: 8 bits
subplot(2,2,4); [HZ,HP,Hl] = zplane([b],[a]); axis([-2,2,-2,2]); hold on;
set(HZ,’color’,’g’,’linewidth’,1,’markersize’,4);
set(HP,’color’,’g’,’linewidth’,1,’markersize’,4);
plot(real(zhat2),imag(zhat2),’r+’,’linewidth’,1);
title([’PZ Plot: ’,TITLE2],’fontsize’,10,’fontweight’,’bold’);
hold off;
The log-magnitude responses and zero-pole locations of the resulting ﬁlters are
plotted in Figure 6.28 along with those of the original ﬁlter. When 16 bits
are used, the resulting ﬁlter is virtually indistinguishable from the original one.
However, when 8 bits are used, the ﬁlter behavior is severely distorted. The ﬁlter
is still stable, but it does not satisfy the design speciﬁcations.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

274
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
0
1
−40
−20
0
Decibels
Decibels
Log-Mag Plot: 16-bits (1+2+13)
Real Part
Imaginary Part
PZ Plot: 16-bits (1+2+13)
−80
−60
0.2
0.4
0.6
0.8
−1
−1
−0.5
−0.5
1
1
0
0
0.5
0.5
Digital Frequency in π Units
0
1
−40
−20
0
Log-Mag Plot: 8-bits (1+2+13)
−80
−60
0.2
0.4
0.6
0.8
Digital Frequency in π Units
True
8-bit
True
16-bit
16-bit zero
16-bit pole
True zero
True pole
Real Part
Imaginary Part
PZ Plot: 8-bits (1+2+5)
−1
−1
−0.5
−0.5
1
1
0
0
0.5
0.5
8-bit zero
8-bit pole
True zero
True pole
FIGURE 6.28
Plots for the IIR ﬁlter in Example 6.25
6.7.4 FIR FILTERS
A similar analysis can be done for FIR ﬁlters. Let the impulse response
of an FIR ﬁlter be h(n) with system response
H(z) =
M−1
	
n=0
h(n)z−n
(6.66)
Then
∆H(z) =
M−1
	
n=0
∆h(n)z−n
(6.67)
where ∆H(z) is the change due to change in the impulse response h(n).
Hence
∆H (e jω) =
M−1
	
n=0
∆h(n) e−jωn
or
|∆H(e jω)| ≤
M−1
	
n=0
|∆h(n)|
(6.68)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Quantization of Filter Coeﬃcients
275
Now, if each coeﬃcient is quantized to B fraction bits (i.e., total register
length is B + 1), then
|∆h(n)| ≤1
22−B
Therefore,
|∆H(ejω)| ≤1
22−BM = M
2 2−B
(6.69)
Thus the change in frequency response depends not only on the number
of bits used but also on the length M of the ﬁlter. For large M and small
b, this diﬀerence can be signiﬁcant and can destroy the desirable behavior
of the ﬁlter, as we see in the following example.
□
EXAMPLE 6.26
An order-30 lowpass FIR ﬁlter is designed using the firpm function. This and
other FIR ﬁlter design functions will be discussed in Chapter 7. The resulting
ﬁlter coeﬃcients are symmetric and are shown in Table 6.2. We will consider
these coeﬃcients as essentially unquantized. The coeﬃcients are quantized to
16 bits (15 fractional plus 1 sign bit) and to 8 bits (7 fractional and 1 sign bit).
The resulting ﬁlter frequency responses and pole-zero plots are determined and
compared. These and other relevant steps are shown in the following MATLAB
script.
TABLE 6.2
Unquantized FIR ﬁlter coeﬃcients used in
Example 6.26
k
bk
k
0
0.000199512328641
30
1
−0.002708453461401
29
2
−0.002400461099957
28
3
0.003546543555809
27
4
0.008266607456720
26
5
0.000012109690648
25
6
−0.015608300819736
24
7
−0.012905580320708
23
8
0.017047710292001
22
9
0.036435951059014
21
10
0.000019292305776
20
11
−0.065652005307521
19
12
−0.057621325403582
18
13
0.090301607282890
17
14
0.300096964940136
16
15
0.400022084144842
15
% Filter coefficients given in Table 6.2 are computed using the firpm function
b = firpm(30,[0,0.3,0.5,1],[1,1,0,0]);
w = [0:500]*pi/500; H = freqz(b,1,w); magH = abs(H);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

276
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
magHdb = 20*log10(magH);
N1 = 15; [bhat1,L1,B1] = Qcoeff(b,N1);
TITLE1 = sprintf(’%i-bits (1+%i+%i)’,N1+1,L1,B1);
Hhat1 = freqz(bhat1,1,w); magHhat1 = abs(Hhat1);
magHhat1db = 20*log10(magHhat1);
zhat1 = roots(bhat1);
N2 = 7; [bhat2,L2,B2] = Qcoeff(b,N2);
TITLE2 = sprintf(’%i-bits (1+%i+%i)’,N2+1,L2,B2);
Hhat2 = freqz(bhat2,1,w); magHhat2 = abs(Hhat2);
magHhat2db = 20*log10(magHhat2);
zhat2 = roots(bhat2);
% Plots
Hf_1 = figure(’paperunits’,’inches’,’paperposition’,[0,0,6,5]);
% Comparison of log-magnitude responses: 16 bits
subplot(2,2,1); plot(w/pi,magHdb,’g’,’linewidth’,1.5); axis([0,1,-80,5]);
hold on; plot(w/pi,magHhat1db,’r’,’linewidth’,1); hold off;
xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Decibels’,’fontsize’,10);
title([’Log-Mag plot: ’,TITLE1],’fontsize’,10,’fontweight’,’bold’);
% Comparison of pole-zero plots: 16 bits
subplot(2,2,3); [HZ,HP,Hl] = zplane([b],[1]); axis([-2,2,-2,2]); hold on;
set(HZ,’color’,’g’,’linewidth’,1,’markersize’,4);
set(HP,’color’,’g’,’linewidth’,1,’markersize’,4);
plot(real(zhat1),imag(zhat1),’r+’,’linewidth’,1);
title([’PZ Plot: ’,TITLE1],’fontsize’,10,’fontweight’,’bold’);
hold off;
% Comparison of log-magnitude responses: 8 bits
subplot(2,2,2); plot(w/pi,magHdb,’g’,’linewidth’,1.5); axis([0,1,-80,5]);
hold on; plot(w/pi,magHhat2db,’r’,’linewidth’,1); hold off;
xlabel(’Digital Frequency in \pi units’,’fontsize’,10);
ylabel(’Decibels’,’fontsize’,10);
title([’Log-Mag plot: ’,TITLE2],’fontsize’,10,’fontweight’,’bold’);
% Comparison of pole-zero plots: 8 bits
subplot(2,2,4); [HZ,HP,Hl] = zplane([b],[1]); axis([-2,2,-2,2]); hold on;
set(HZ,’color’,’g’,’linewidth’,1,’markersize’,4);
set(HP,’color’,’g’,’linewidth’,1,’markersize’,4);
plot(real(zhat2),imag(zhat2),’r+’,’linewidth’,1);
title([’PZ Plot: ’,TITLE2],’fontsize’,10,’fontweight’,’bold’);
hold off;
The log-magnitude responses and zero-pole locations of the resulting ﬁlters are
computed and plotted in Figure 6.29 along with those of the original ﬁlter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
277
0
1
−80
−60
−40
−20
0
Decibels
Log-Mag Plot: 16-bits (1+0+15)
−2
0
2
−2
−1
−1
0
1
1
2
Imaginary Part
Imaginary Part
30
30
0.2
0.4
0.6
0.8
Digital Frequency in π Units
0
1
−80
−60
−40
−20
0
Decibels
Log-Mag Plot: 8-bits (1+0+7)
0.2
0.4
0.6
0.8
Digital Frequency in π Units
True
16-bit
True
8-bit
Real Part
−2
0
2
−2
−1
−1
0
1
1
2
PZ Plot: 8-bits (1+0+7)
Real Part
8-bit zero
True zero
16-bit zero
True zero
PZ Plot: 16-bits (1+0+15)
FIGURE 6.29
Plots for the FIR ﬁlter in Example 6.26
When 16 bits are used, the resulting ﬁlter is virtually indistinguishable from
the original one. However, when 8 bits are used, the ﬁlter behavior is severely
distorted and the ﬁlter does not satisfy the design speciﬁcations.
□
6.8 PROBLEMS
P6.1
Draw direct form I block diagram structures for each of the following LTI systems with
input node x(n) and output node y(n).
1. y(n) = x(n) + 2 x(n −1) + 3x(n −2)
2. H(z) =
1
1 −1.7z−1 + 1.53z−2 −0.648z−3
3. y(n) = 1.7 y(n −1) −1.36 y(n −2) + 0.576 y(n −3) + x(n)
4. y(n) = 1.6 y(n −1) + 0.64 y(n −2) + x(n) + 2 x(n −1) + x(n −2)
5. H(z) =
1 −3z−1 + 3z−2 + z−3
1 + 0.2z−1 −0.14z−2 + 0.44z−3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

278
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
K
z −1
z −1
z −1
z −1
z −1
z −1
2
0.5
(i)
2
0.5
K
−0.9
(ii)
x(n)
x(n)
y(n)
y(n)
1.8
FIGURE P6.1
Block diagrams for Problem 6.2
P6.2
Two block diagrams are shown in Figure P6.1. Answer the following for each structure.
1. Determine the system function H(z) = Y (z)/X(z).
2. Is the structure canonical (i.e., with the least number of delays)? If not, draw a
canonical structure.
3. Determine the value of K so that H(ej 0) = 1.
P6.3
Consider the LTI system described by
y(n) = a y(n −1) + b x(n)
(6.70)
1. Draw a block diagram of this system with input node x(n) and output node y(n).
2. Now perform the following two operations on the structure drawn in part 1: (i) reverse
all arrow directions and (ii) interchange the input node with the output node. Notice
that the branch node becomes the adder node and vice versa. Redraw the block diagram
so that input node is on the left side and the output node is on the right side. This is the
transposed block diagram.
3. Determine the diﬀerence equation representation of your transposed structure in part 2,
and verify that it is the same equation as (6.70).
P6.4
Consider the LTI system given by
H(z) =
1 −2.818z−1 + 3.97z−2 −2.8180z−3 + z−4
1 −2.536z−1 + 3.215z−2 −2.054z−3 + 0.6560z−4
(6.71)
1. Draw the normal direct form I structure block diagram.
2. Draw the transposed direct form I structure block diagram.
3. Draw the normal direct form II structure block diagram. Observe that it looks very
similar to that in part 2.
4. Draw the transposed direct form II structure block diagram. Observe that it looks very
similar to that in part 1.
P6.5
Consider the LTI system given in Problem P6.4.
1. Draw a cascade structure containing second-order normal direct form II sections.
2. Draw a cascade structure containing second-order transposed direct form II sections.
3. Draw a parallel structure containing second-order normal direct form II sections.
4. Draw a parallel structure containing second-order transposed direct form II sections.
P6.6
A causal linear time-invariant system is described by
y(n) =
4
	
k=0
cos(0.1πk)x(n −k) −
5
	
k=1
(0.8)k sin(0.1πk)y(n −k)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
279
Determine and draw the block diagrams of the following structures. Compute the response
of the system to
x(n) = [1 + 2(−1)n] ,
0 ≤n ≤50
in each case, using the following structures.
1. Normal direct form I
2. Transposed direct form II
3. Cascade form containing second-order normal direct form II sections
4. Parallel form containing second-order transposed direct form II sections
P6.7
An IIR ﬁlter is described by system function
H(z) = 2

1 + 0z−1 + z−2
1 −0.8z−1 + 0.64z−2

+

2 −z−1
1 −0.75z−1

+

1 + 2z−1 + z−2
1 + 0.81z−2

Determine and draw the following structures.
1. Transposed direct form I
2. Normal direct form II
3. Cascade form containing transposed second-order direct form II sections
4. Parallel form containing normal second-order direct form II sections
P6.8
An IIR ﬁlter is described by the system function
H(z) =

−14.75 −12.9z−1
1 −7
8z−1 +
3
32z−2

+

24.5 + 26.82z−1
1 −z−1 + 1
2z−2
 
1 + 2z−1 + z−2
1 + 0.81z−2

Determine and draw the following structures.
1. Normal direct form I
2. Normal direct form II
3. Cascade form containing transposed second-order direct form II sections
4. Parallel form containing transposed second-order direct form II sections
P6.9
A linear time-invariant system with system function
H(z) =
0.05 −0.01z−1 −0.13z−2 + 0.13z−4 + 0.01z−5 −0.05z−6
1 −0.77z−1 + 1.59z−2 −0.88z−3 + 1.2z−4 −0.35z−5 + 0.31z−6
is to be implemented using a ﬂow graph of the form shown in Figure P6.2.
1. Fill in all the coeﬃcients in the diagram.
2. Is your solution unique? Explain.
z −1
z −1
z −1
z −1
z −1
z −1
z −1
z −1
y(n)
x(n)
FIGURE P6.2
Structure for Problem 6.9
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

280
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
P6.10 A linear time-invariant system with system function
H(z) = 0.051 + 0.088z−1 + 0.06z−2 −0.029z−3 −0.069z−4 −0.046z−5
1 −1.34z−1 + 1.478z−2 −0.789z−3 + 0.232z−4
is to be implemented using a ﬂow graph of the form shown in Figure P6.3. Fill in all the
coeﬃcients in the diagram.
z−1
z−1
z−1
z−1
z−1
x(n)
y(n)
FIGURE P6.3
Structure for Problem 6.10
P6.11 Consider the linear time-invariant system given in Problem P6.9.
H(z) =
0.05 −0.01z−1 −0.13z−2 + 0.13z−4 + 0.01z−5 −0.05z−6
1 −0.77z−1 + 1.59z−2 −0.88z−3 + 1.2z−4 −0.35z−5 + 0.31z−6
It is to be implemented using a ﬂow graph of the form shown in Figure P6.4.
1. Fill in all the coeﬃcients in the diagram.
2. Is your solution unique? Explain.
y(n)
x(n)
z −1
z −1
z −1
z −1
z −1
z −1
FIGURE P6.4
Structure for Problem 6.11
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
281
P6.12 The ﬁlter structure shown in Figure P6.5 contains a parallel connection of cascade sections.
Determine and draw the overall
1. direct form (normal) structure,
2. direct form (transposed) structure,
3. cascade form structure containing second-order sections,
4. parallel form structure containing second-order sections.
2
0.5
1.5
−1
−0.9
−0.5
2
3
−0.4
−0.4
3
1
1
−0.8
2
1
1
−0.5
−0.5
0.4
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
z−1
x(n)
y(n)
FIGURE P6.5
Structure for Problem 6.12
P6.13 In the ﬁlter structure shown in Figure P6.6, systems H1(z) and H2(z) are subcomponents
of a larger system H(z). The system function H1(z) is given in the parallel form
H1(z) = 2 +
0.2 −0.3z−1
1 + 0.9z−1 + 0.9z−2 +
0.4 + 0.5z−1
1 −0.8z−1 + 0.8z−2
and the system function H2(z) is given in the cascade form
H2(z) =

2 + z−1 −z−2
1 + 1.7z−1 + 0.72z−2
 
3 + 4z−1 + 5z−2
1 −1.5z−1 + 0.56z−2

1. Express H(z) as a rational function.
2. Draw the block diagram of H(z) as a cascade form structure.
3. Draw the block diagram of H(z) as a parallel form structure.
H(z)
x(n)
H2(z)
H1(z)
y(n)
FIGURE P6.6
Structure for Problem 6.13
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

282
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
P6.14 The digital ﬁlter structure shown in Figure P6.7 is a cascade of two parallel sections and
corresponds to a tenth-order IIR digital ﬁlter system function
H(z) =
1 −2.2z−2 + 1.6368z−4 −0.48928z−6 + 5395456 × 10−8z−8 −147456 × 10−8z−10
1 −1.65z−2 + 0.8778z−4 −0.17281z−6 + 1057221 × 10−8z−8 −893025 × 10−10z−10
x(n)
y(n)
4.0635
4.0635
−0.0793
−0.0815
−1.6
−0.63
−2.8255
0.4
−0.03
0.7747
−0.2076
0.1319
−0.0304
−0.9
−0.35
1.2
−0.5502
−0.2245
−2.4609
−0.8
−0.15
−0.1
z−1
z −1
z−1
z −1
z −1
z −1
z −1
z −1
z −1
z −1
FIGURE P6.7
Structure for Problem 6.14
1. Due to an error in labeling, two of the multiplier coeﬃcients (rounded to four decimals)
in this structure have incorrect values. Locate these two multipliers and determine their
correct values.
2. Determine and draw an overall cascade structure containing second-order sections and
which contains the least number of multipliers.
P6.15 As described in this chapter, a linear-phase FIR ﬁlter is obtained by requiring certain
symmetry conditions on its impulse responses.
1. In the case of symmetrical impulse response, we have h(n) = h(M −1 −n),
0 ≤n ≤M −1. Show that the resulting phase response is linear in ω and is given by
̸ H 
ejω
= −
M −1
2

ω,
−π < ω ≤π
2. Draw the linear-phase structures for this form when M = 5 and M = 6.
3. In the case of antisymmetrical impulse response, we have h(n) = −h(M −1 −n),
0 ≤n ≤M −1. Show that the resulting phase response is given by
̸ H 
ejω
= ±π
2 −
M −1
2

ω,
−π < ω ≤π
4. Draw the linear-phase structures for this form when M = 5 and M = 6.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
283
P6.16 An FIR ﬁlter is described by the diﬀerence equation
y(n) =
6
	
k=0
e−0.9|k−3|x(n −k)
Determine and draw the block diagrams of the following structures.
1. Direct form
2. Linear-phase form
3. Cascade form
4. Frequency-sampling form
P6.17 A linear time-invariant system is given by the system function
H(z) = 2 + 3z−1 + 5z−2 −3z−3 + 4z−5 + 8z−7 −7z−8 + 4z−9
Determine and draw the block diagrams of the following structures.
1. Direct form
2. Cascade form
3. Frequency-sampling form
P6.18 Using the conjugate symmetry property of the DFT
H (k) =

H (0) ,
k = 0
H∗(M −k) ,
k = 1, . . . , M −1
and the conjugate symmetry property of the W −k
M
factor, show that (6.12) can be put in
the form (6.13) and (6.14) for real FIR ﬁlters.
P6.19 To avoid poles on the unit circle in the frequency-sampling structure, one samples H(z) at
zk = rej2πk/M, k = 0, . . . , M −1 where r ≈1(but < 1), as discussed in Section 6.3.
1. Using
H 
rej2πk/M
≈H (k)
show that the frequency-sampling structure is given by
H (z) = 1 −(rz)−M
M
 L
	
k=1
2 |H (k)| Hk (z) +
H (0)
1 −rz−1 + H (M/2)
1 + rz−1

where
Hk (z) =
cos [̸ H (k)] −rz−1 cos 
̸ H (k) −2πk
M

1 −2rz−1 cos  2πk
M

+ r2z−2
,
k = 1, . . . , L
and M is even.
2. Modify the MATLAB function dir2fs (which was developed in Section 6.3) to
implement this frequency-sampling form. The format of this function should be
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

284
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
[C,B,A,rM] = dir2fs(h,r)
% Direct form to frequency-sampling form conversion
% -------------------------------------------------
% [C,B,A,rM] = dir2fs(h,r)
%
C = Row vector containing gains for parallel sections
%
B = Matrix containing numerator coefficients arranged in rows
%
A = Matrix containing denominator coefficients arranged in rows
% rM = rˆM factor needed in the feedforward loop
%
h = impulse response vector of an FIR filter
%
r = radius of the circle over which samples are taken (r<1)
%
3. Determine the frequency sampling structure for the impulse response given in Example
6.6 using this function.
P6.20 Consider the following system function of an FIR ﬁlter:
H(z) = 1 −4z−1 + 6.4z−2 −5.12z−3 + 2.048z−4 −0.32768z−5
1. Provide block diagram structures in the following forms.
(a) Normal and transposed direct forms
(b) Cascade of ﬁve ﬁrst-order sections
(c) Cascade of one ﬁrst-order section and two second-order sections
(d) Cascade of one second-order section and one third-order section
(e) Frequency-sampling structure with real coeﬃcients
2. The computational complexity of a digital ﬁlter structure can be given by the total
number of multiplications and the total number of 2-input additions that are required
per output point. Assume that x(n) is real and that multiplication by 1 is not counted
as a multiplication. Compare the computational complexity of each of these structures.
P6.21 A causal digital ﬁlter is described by the following zeros:
z1 = 0.5 ej60◦,
z2 = 0.5 e−j60◦,
z3 = 2 ej60◦,
z4 = 2 e−j60◦,
z5 = 0.25 ej30◦,
z6 = 0.25 e−j30◦,
z7 = 4 ej30◦,
z8 = 4 e−j30◦
and poles: {pi}8
i=1 = 0.
1. Determine the phase response of this ﬁlter, and show that it is a linear-phase FIR ﬁlter.
2. Determine the impulse response of the ﬁlter.
3. Draw a block diagram of the ﬁlter structure in the direct form.
4. Draw a block diagram of the ﬁlter structure in the linear-phase form.
P6.22 MATLAB provides the built-in functions dec2bin and bin2dec to convert nonnegative
decimal integers into binary codes and vice versa, respectively.
1. Develop a function B = sm2bin(D) to convert a sign-magnitude format decimal integer D
into its binary representation B. Verify your function on the following numbers:
(a) D = 1001
(b) D = −63
(c) D = −449
(d) D = 978
(e) D = −205
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
285
2. Develop a function D = bin2sm(B) to convert a binary representation B into its sign-
magnitude format decimal integer D. Verify your function on the following
representations:
(a) B = 1010
(b) B = 011011011
(c) B = 11001
(d) B = 1010101
(e) B = 011011
P6.23 Using the function TwosComplement as a model, develop a function y = TensComplement
(x,N) that converts a sign-magnitude format integer x into the N-digit ten’s-complement
integer y.
1. Verify your function using the following integers:
(a) x = 1234, N = 6
(b) x = −603, N = 4
(c) x = −843, N = 5
(d) x = −1978, N = 6
(e) x = 50, N = 3
2. Using the ten’s-complement format, perform the following arithmetic operations. In each
case, choose an appropriate value on N for the meaningful result.
(a) 123 + 456 −789
(b) 648 + 836 −452
(c) 2001 + 3756
(d) −968 + 4539
(e) 888 −666 + 777
Verify your results using decimal operations.
P6.24 The function OnesComplement developed in this chapter converts signed integers into one’s-
complement-format decimal representations. In this problem, we will develop functions that
will operate on fractional numbers.
1. Develop a MATLAB function y = sm2oc(x, B) that converts the sign-magnitude format
fraction x into the B-bit one’s-complement-format decimal equivalent number y. Verify
your function on the following numbers. In each case, the numbers to be considered are
both positive and negative. Also, in each case select the appropriate number of bits B.
(a) x = ±0.5625
(b) x = ±0.40625
(c) x = ±0.953125
(d) x = ±0.1328125
(e) x = ±0.7314453125
2. Develop a MATLAB function x = oc2sm(y, B) that converts the B-bit one’s-
complement-format decimal equivalent number y into the sign-magnitude-format
fraction x. Verify your function on the following fractional binary representations:
(a) y = 1▲10110
(b) y = 0.▲011001
(c) y = 1▲00110011
(d) y = 1▲11101110
(e) y = 0▲00010001
P6.25 The function TwosComplement developed in this chapter converts signed integers into two’s-
complement-format decimal representations. In this problem, we will develop functions that
will operate on fractional numbers.
1. Develop a MATLAB function y = sm2tc(x, B) that converts the sign-magnitude-format
fraction x into the B-bit two’s-complement-format decimal equivalent number y. Verify
your function on the following numbers. In each case, the numbers to be considered are
both positive and negative. Also, in each case select the appropriate number of bits B.
(a) x = ±0.5625
(b) x = ±0.40625
(c) x = ±0.953125
(d) x = ±0.1328125
(f) x = ±0.7314453125
Compare your representations with those in Problem P6.24, part 1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

286
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
2. Develop a MATLAB function x = tc2sm(y, B) that converts the B-bit
two’s-complement-format decimal equivalent number y into the sign-magnitude-format
fraction x. Verify your function on the following fractional binary representations:
(a) y = 1▲10110
(b) y = 0.▲011001
(c) y = 1▲00110011
(d) y = 1▲11101110
(e) y = 0▲00010001
Compare your representations with those in Problem P6.24, part 2.
P6.26 Determine the 10-bit sign-magnitude, one’s-complement, and two’s-complement
representation of the following decimal numbers:
(a) 0.12345
(b) −0.56789
(c) 0.38452386
(d) −0.762349
(e) −0.90625
P6.27 Consider a 32-bit ﬂoating-point number representation with a 6-bit exponent and a 25-bit
mantissa.
1. Determine the value of the smallest number that can be represented.
2. Determine the value of the largest number that can be represented.
3. Determine the dynamic range of this ﬂoating-point representation and compare it with
the dynamic range of a 32-bit ﬁxed-point signed integer representation.
P6.28 Show that the magnitudes of ﬂoating-point numbers in a 32-bit IEEE standard range from
1.18 × 10−38 to 3.4 × 1038.
P6.29 Compute and plot the truncation error characteristics when B = 4 for the sign-magnitude,
one’s-complement, and two’s-complement formats.
P6.30 Consider the third-order elliptic lowpass ﬁlter.
H(z) =
0.1214 
1 −1.4211z−1 + z−2 
1 + z−1
(1 −1.4928z−1 + 0.8612z−2) (1 −0.6183z−1)
1. If the ﬁlter is realized using a direct form structure, determine its pole sensitivity.
2. If the ﬁlter is realized using a cascade form structure, determine its pole sensitivity.
P6.31 Consider the ﬁlter described by the diﬀerence equation
y(n) =
1
√
2
y(n −1) −x(n) +
√
2x(n −1)
(6.72)
1. Show that this ﬁlter is an allpass ﬁlter, that is, |H(ejω)| is a constant over the entire
frequency range −π ≤ω ≤π. Verify your answer by plotting the magnitude response
|H(ejω)| over the normalized frequency range 0 ≤ω/π ≤1. Use subplot(3,1,1).
2. Round the coeﬃcients of the diﬀerence equation in (6.72) to three decimals. Is the ﬁlter
still allpass? Verify your answer by plotting the resulting magnitude response, | ˆH1(ejω)|,
over the normalized frequency range 0 ≤ω/π ≤1. Use subplot(3,1,2).
3. Round the coeﬃcients of the diﬀerence equation in (6.72) to two decimals. Is the ﬁlter
still allpass? Verify your answer by plotting the resulting magnitude response, | ˆH2(ejω)|,
over the normalized frequency range 0 ≤ω/π ≤1. Use subplot(3,1,3).
4. Explain why the magnitude | ˆH1(ejω)| is “diﬀerent” from the magnitude | ˆH2(ejω)|.
P6.32 An IIR lowpass ﬁlter designed to meet the speciﬁcations of 0.5 dB ripple in the passband,
60 dB ripple in the stopband, a passband edge frequency ωp = 0.25π, and a stopband edge
frequency ωs = 0.3π is obtained using the following MATLAB script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
287
wp = 0.25*pi; ws = 0.3*pi; Rp = 0.5; As = 60;
[N, Wn] = ellipord(wp/pi, ws/pi, Rp, As);
[b,a] = ellip(N,Rp,As,Wn);
The ﬁlter coeﬃcients bk and ak are in the arrays b and a, respectively, and can be
considered to have inﬁnite precision.
1. Using inﬁnite precision, plot the log-magnitude and phase responses of the designed
ﬁlter. Use two rows and one column of subplots.
2. Quantize the direct form coeﬃcients to four decimals (by rounding). Now plot the
log-magnitude and phase responses of the resulting ﬁlter. Use two rows and one column
of subplots.
3. Quantize the direct form coeﬃcients to three decimals (by rounding). Now plot the
log-magnitude and phase responses of the resulting ﬁlter. Use two rows and one column
of subplots.
4. Comment on the plots in parts 1, 2, and 3.
P6.33 Consider the digital lowpass ﬁlter used in Problem P6.32.
1. Using inﬁnite precision and cascade form realization, plot the log-magnitude and phase
responses of the designed ﬁlter. Use two rows and one column of subplots.
2. Quantize the cascade form coeﬃcients to four decimals (by rounding). Now plot the
log-magnitude and phase responses of the resulting ﬁlter. Use two rows and one column
of subplots.
3. Quantize the cascade form coeﬃcients to three decimals (by rounding). Now plot the
log-magnitude and phase responses of the resulting ﬁlter. Use two rows and one column
of subplots.
4. Comment on the plots in the above three parts and compare them with the similar plots
in Problem P6.32.
P6.34 A length-32 linear-phase FIR bandpass ﬁlter that satisﬁes the requirements of 60 dB
stopband attenuation, lower stopband edge frequency ωs1 = 0.2π, and upper stopband edge
frequency ωs2 = 0.8π is obtained using the following MATLAB script.
ws1 = 0.2*pi; ws2 = 0.8*pi; As = 60;
M = 32; Df = 0.2115;
fp1 = ws1/pi+Df; fp2 = ws2/pi-Df;
h = firpm(M-1,[0,ws1/pi,fp1,fp2,ws2/pi,1],[0,0,1,1,0,0]);
The ﬁlter impulse response h(n) is in the array h and can be considered to have inﬁnite
precision.
1. Using inﬁnite precision, plot the log-magnitude and amplitude responses of the designed
ﬁlter. Use two rows and one column of subplots.
2. Quantize the direct form coeﬃcients to four decimals (by rounding). Now plot the
log-magnitude and amplitude responses of the resulting ﬁlter. Use two rows and one
column of subplots.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

288
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
3. Quantize the direct form coeﬃcients to three decimals (by rounding). Now plot the
log-magnitude and amplitude responses of the resulting ﬁlter. Use two rows and one
column of subplots.
4. Comment on the plots in parts 1, 2, and 3.
5. Based on the results of this problem, determine how many signiﬁcant bits (and not
decimals) are needed in practice to represent FIR direct form realizations.
P6.35 The digital ﬁlter structure shown in Figure P6.8 is a cascade of two parallel sections and
corresponds to a tenth-order IIR digital ﬁlter system function
H(z) =
1 −2.2z−2 + 1.6368z−4 −0.48928z−6 + 5395456 × 10−8z−8 −147456 × 10−8z−10
1 −1.65z−2 + 0.8778z−4 −0.17281z−6 + 1057221 × 10−8z−8 −893025 × 10−10z−10
x(n)
y(n)
4.0635
4.0635
−0.0793
−1.6
−0.63
−2.8255
0.4
−0.03
0.7747
−0.2076
0.1319
−0.0304
−0.9
−0.35
1.2
−0.5502
−0.2245
−2.4609
−0.8
−0.15
−0.1
−0.0815
z −1
z −1
z −1
z −1
z −1
z −1
z −1
z −1
z −1
z −1
FIGURE P6.8
Structure for Problem P6.35
1. Due to an error in labeling, two of the multiplier coeﬃcients (rounded to four decimals)
in this structure have incorrect values. Locate these two multipliers and determine their
correct values.
2. By inspecting the pole locations of the system function H(z), you should realize that
this structure is sensitive to the coeﬃcient quantization. Suggest, with justiﬁcation, an
alternative structure that in your opinion is least sensitive to coeﬃcient quantization.
P6.36 An IIR bandstop digital ﬁlter that satisﬁes the requirements:
0.95 ≤|H(ejω)| ≤1.05,
0 ≤|H(ejω)| ≤0.01,
0.95 ≤|H(ejω)| ≤1.05,
0 ≤|ω| ≤0.25π
0.35π ≤|ω| ≤0.65π
0.75π ≤|ω| ≤π
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
289
can be obtained using the following MATLAB script:
wp = [0.25,0.75]; ws = [0.35,0.65]; delta1 = 0.05; delta2 = 0.01;
[Rp,As] = delta2db(delta1,delta2);
[N, wn] = cheb2ord(wp, ws, Rp, As);
[b,a] = cheby2(N,As,wn,’stop’);
The ﬁlter coeﬃcients bk and ak are in the arrays b and a, respectively, and can be
considered to have inﬁnite precision.
1. Using inﬁnite precision, provide the log-magnitude response plot and the pole-zero plot
of the designed ﬁlter.
2. Assuming direct-form structure and a 12-bit representation for ﬁlter coeﬃcients, provide
the log-magnitude response plot and the pole-zero plot of the designed ﬁlter. Use the
Qcoeff function.
3. Assuming cascade-form structure and a 12-bit representation for ﬁlter coeﬃcients,
provide the log-magnitude response plot and the pole-zero plot of the designed ﬁlter. Use
the Qcoeff function.
P6.37 An IIR lowpass digital ﬁlter that satisﬁes the speciﬁcations:
passband edge:
0.4π,
Rp = 0.5 dB
stopband edge:
0.6π,
As = 50 dB
can be obtained using the following MATLAB script:
wp = 0.4; ws = 0.6; Rp = 0.5; As = 50;
[N, wn] = buttord(wp, ws, Rp, As);
[b,a] = butter(N,wn);
The ﬁlter coeﬃcients bk and ak are in the arrays b and a, respectively, and can be
considered to have inﬁnite precision.
1. Using inﬁnite precision, provide the magnitude response plot and the pole-zero plot of
the designed ﬁlter.
2. Assuming direct-form structure and a 10-bit representation for ﬁlter coeﬃcients, provide
the magnitude response plot and the pole-zero plot of the designed ﬁlter. Use the Qcoeff
function.
3. Assuming cascade-form structure and a 10-bit representation for ﬁlter coeﬃcients,
provide the magnitude response plot and the pole-zero plot of the designed ﬁlter. Use
the Qcoeff function.
P6.38 An IIR highpass digital ﬁlter that satisﬁes the speciﬁcations:
stopband edge:
0.4π,
As = 60 dB
passband edge:
0.6π,
Rp = 0.5 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

290
Chapter 6
IMPLEMENTATION OF DISCRETE-TIME FILTERS
can be obtained using the following MATLAB script:
wp = 0.6; ws = 0.4; Rp = 0.5; As = 60;
[N,wn] = ellipord(wp, ws, Rp, As);
[b,a] = ellip(N,Rp,As,wn,’high’);
The ﬁlter coeﬃcients bk and ak are in the arrays b and a, respectively, and can be
considered to have inﬁnite precision.
1. Using inﬁnite precision, provide the magnitude response plot and the pole-zero plot of
the designed ﬁlter.
2. Assuming direct-form structure and a 10-bit representation for ﬁlter coeﬃcients, provide
the magnitude response plot and the pole-zero plot of the designed ﬁlter. Use the Qcoeff
function.
3. Assuming parallel-form structure and a 10-bit representation for ﬁlter coeﬃcients,
provide the magnitude response plot and the pole-zero plot of the designed ﬁlter. Use
the Qcoeff function.
P6.39 A bandstop linear-phase FIR ﬁlter that satisﬁes the speciﬁcations:
lower stopband edge:
0.4π
upper stopband edge:
0.6π
As = 50 dB
lower passband edge:
0.3π
upper passband edge:
0.7π
Rp = 0.2 dB
can be obtained using the following MATLAB script:
wp1 = 0.3; ws1 = 0.4; ws2 = 0.6; wp2 = 0.7; Rp = 0.2; As = 50;
[delta1,delta2] = db2delta(Rp,As);
b = firpm(44,[0,wp1,ws1,ws2,wp2,1],[1,1,0,0,1,1],...
[delta2/delta1,1,delta2/delta1]);
The ﬁlter impulse response h(n) is in the array b and can be considered to have inﬁnite
precision.
P6.40 A bandpass linear-phase FIR ﬁlter that satisﬁes the speciﬁcations:
0
≤
|H(ejω)|
≤
0.01,
0.95
≤
|H(ejω)|
≤
1.05,
0
≤
|H(ejω)|
≤
0.01,
0 ≤ω ≤0.25π
0.35π ≤ω ≤0.65π
0.75π ≤ω ≤π
can be obtained using the following MATLAB script:
ws1 = 0.25; wp1 = 0.35; wp2 = 0.65; ws2 = 0.75;
delta1 = 0.05; delta2 = 0.01;
b = firpm(40,[0,ws1,wp1,wp2,ws2,1],[0,0,1,1,0,0],...
[1,delta2/delta1,1]);
The ﬁlter impulse response h(n) is in the array b and can be considered to have inﬁnite
precision.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

291
C H A P T E R 7
FIR Filter
Design
We now turn our attention to the inverse problem of designing sys-
tems from the given speciﬁcations. It is an important as well as diﬃcult
problem. In digital signal processing, there are two important types of
systems. The ﬁrst type of systems perform signal ﬁltering in the time
domain and hence are called digital filters. The second type of systems
provide signal representation in the frequency domain and are called
spectrum analyzers. In Chapter 5, we described signal representations
using the DFT. In this and the next chapter, we will study several basic
design algorithms for both FIR and IIR ﬁlters. These designs are mostly
of the frequency selective type; that is, we will design primarily multiband
lowpass, highpass, bandpass, and bandstop ﬁlters. In FIR ﬁlter design,
we will also consider systems like diﬀerentiators or Hilbert transformers,
which, although not frequency-selective ﬁlters, nevertheless follow the
design techniques being considered. More sophisticated ﬁlter designs are
based on arbitrary frequency-domain speciﬁcations and require tools that
are beyond the scope of this book.
We ﬁrst begin with some preliminary issues related to design philos-
ophy and design speciﬁcations. These issues are applicable to both FIR
and IIR ﬁlter designs. We will then study FIR ﬁlter design algorithms in
the rest of this chapter. In Chapter 8, we will provide a similar treatment
for IIR ﬁlters.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

292
Chapter 7
FIR FILTER DESIGN
7.1 PRELIMINARIES
The design of a digital ﬁlter is carried out in three steps:
• Speciﬁcations: Before we can design a ﬁlter, we must have some spec-
iﬁcations. These speciﬁcations are determined by the applications.
• Approximations: Once the speciﬁcations are deﬁned, we use various
concepts and mathematics that we studied so far to come up with a
ﬁlter description that approximates the given set of speciﬁcations. This
step is the topic of ﬁlter design.
• Implementation: The product of the above step is a ﬁlter description
in the form of a diﬀerence equation, a system function H(z), or an
impulse response h(n). From this description, we implement the ﬁlter
in hardware or through software on a computer as we discussed in
Chapter 6.
In this and the next chapter, we will discuss in detail only the second
step, which is the conversion of speciﬁcations into a ﬁlter description.
In many applications like speech or audio signal processing, digital
ﬁlters are used to implement frequency-selective operations. Therefore,
speciﬁcations are required in the frequency domain in terms of the desired
magnitude and phase response of the ﬁlter. Generally, a linear phase re-
sponse in the passband is desirable. In the case of FIR ﬁlters, it is possible
to have exact linear phase, as we have seen in Chapter 6. In the case of
IIR ﬁlters, a linear phase in the passband is not achievable. Hence we will
consider magnitude-only speciﬁcations.
The magnitude speciﬁcations are given in one of two ways. The ﬁrst
approach is called absolute specifications, which provide a set of require-
ments on the magnitude response function |H(ejω)|. These speciﬁcations
are generally used for FIR ﬁlters. IIR ﬁlters are speciﬁed in a somewhat
diﬀerent way, which we will discuss in Chapter 8. The second approach is
called relative specifications, which provide requirements in decibels (dB),
given by
dB scale = −20 log10
|H(ejω)|
|H(ejω)|max
≥0
This approach is the most popular one in practice and is used for both
FIR and IIR ﬁlters. To illustrate these speciﬁcations, we will consider a
lowpass ﬁlter design as an example.
7.1.1 ABSOLUTE SPECIFICATIONS
A typical absolute speciﬁcation of a lowpass ﬁlter is shown in Figure 7.1a,
in which
• band [0, ωp] is called the passband and δ1 is the tolerance (or ripple)
that we are willing to accept in the ideal passband response,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Preliminaries
293
0
(a)
(b)
0
Decibels
Transition
Band
1
0
w
w
wp
ws
p
1 + d1
1 − d1
d2
|H(e jw)|
Rp
As
Passband
Ripple
Stopband
Ripple
FIGURE 7.1
FIR ﬁlter speciﬁcations: (a) absolute (b) relative
• band [ωs, π] is called the stopband and δ2 is the corresponding tolerance
(or ripple), and
• band [ωp, ωs] is called the transition band that imposes no restrictions
on the magnitude response.
7.1.2 RELATIVE (DB) SPECIFICATIONS
A typical relative speciﬁcation of a lowpass ﬁlter is shown in Figure 7.1b,
in which
• Rp is the passband ripple in dB, and
• As is the stopband attenuation in dB.
The parameters given in these two speciﬁcations are obviously related.
Since |H(ejω)|max in absolute speciﬁcations is equal to (1 + δ1), we have
Rp = −20 log10
1 −δ1
1 + δ1
> 0 (≈0)
(7.1)
and
As = −20 log10
δ2
1 + δ1
> 0 (≫1)
(7.2)
□
EXAMPLE 7.1
In a certain ﬁlter’s speciﬁcations, the passband ripple is 0.25 dB and the stop-
band attenuation is 50 dB. Determine δ1 and δ2.
Solution
Using (7.1), we obtain
Rp = 0.25 = −20 log10
1 −δ1
1 + δ1 ⇒δ1 = 0.0144
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

294
Chapter 7
FIR FILTER DESIGN
Using (7.2), we obtain
As = 50 = −20 log10
δ2
1 + δ1 = −20 log10
δ2
1 + 0.0144 ⇒δ2 = 0.0032
□
□
EXAMPLE 7.2
Given the passband tolerance δ1 = 0.01 and the stopband tolerance δ2 = 0.001,
determine the passband ripple Rp and the stopband attenuation As.
Solution
From (7.1), the passband ripple is
Rp = −20 log10
1 −δ1
1 + δ1 = 0.1737 dB
and from (7.2), the stopband attenuation is
As = −20 log10
δ2
1 + δ1 = 60 dB
□
Problem P7.1 develops MATLAB functions to convert one set of spec-
iﬁcations into another.
These speciﬁcations were given for a lowpass ﬁlter. Similar speciﬁca-
tions can also be given for other types of frequency-selective ﬁlters, such
as highpass or bandpass. However, the most important design parame-
ters are frequency-band tolerances (or ripples) and band-edge frequencies.
Whether the given band is a passband or a stopband is a relatively minor
issue. Therefore, in describing design techniques, we will concentrate on
a lowpass ﬁlter. In the next chapter, we will discuss how to transform
a lowpass ﬁlter into other types of frequency-selective ﬁlters. Hence it
makes more sense to develop techniques for a lowpass ﬁlter so that we
can compare these techniques. However, we will also provide examples of
other types of ﬁlters. In light of this discussion, our design goal is the
following.
Problem statement
Design a lowpass ﬁlter (i.e., obtain its system
function H(z) or its diﬀerence equation) that has a passband [0, ωp] with
tolerance δ1 (or Rp in dB) and a stopband [ωs, π] with tolerance δ2 (or
As in dB).
In this chapter, we turn our attention to the design and approximation
of FIR digital ﬁlters. These ﬁlters have several design and implementa-
tional advantages:
• The phase response can be exactly linear.
• They are relatively easy to design since there are no stability problems.
• They are eﬃcient to implement.
• The DFT can be used in their implementation.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
295
As we discussed in Chapter 6, we are generally interested in linear-
phase frequency-selective FIR ﬁlters. Linear-phase response ﬁlters have
several advantages:
• The design problem contains only real arithmetic and not complex
arithmetic
• Linear-phase ﬁlters provide no delay distortion and only a ﬁxed amount
of delay
• For the ﬁlter of length M (or order M −1), the number of opera-
tions are of the order of M/2, as we discussed in the linear-phase ﬁlter
implementation
We ﬁrst begin with a discussion of the properties of the linear-phase
FIR ﬁlters, which are required in design algorithms. Then we will dis-
cuss three design techniques—namely, the window design, the frequency-
sampling design, and the optimal equiripple design techniques for linear-
phase FIR ﬁlters.
7.2 PROPERTIES OF LINEAR-PHASE FIR FILTERS
In this section, we discuss shapes of impulse and frequency responses and
locations of system function zeros of linear-phase FIR ﬁlters. Let h(n),
0 ≤n ≤M −1, be the impulse response of length (or duration) M. Then
the system function is
H(z) =
M−1
	
n=0
h(n)z−n = z−(M−1)
M−1
	
n=0
h(n)zM−1−n
which has (M −1) poles at the origin z = 0 (trivial poles) and (M −1)
zeros located anywhere in the z-plane. The frequency response function
is
H(ejω) =
M−1
	
n=0
h(n)e−jωn,
−π < ω ≤π
Now we will discuss speciﬁc requirements on the forms of h(n) and H(ejω)
as well as requirements on the speciﬁc locations of (M −1) zeros that the
linear-phase constraint imposes.
7.2.1 IMPULSE RESPONSE h(n)
We impose a linear-phase constraint
̸ H(ejω) = −αω,
−π < ω ≤π
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

296
Chapter 7
FIR FILTER DESIGN
   0   
(M−1)/2
 (M−1) 
0
Symmetric Impulse Response: M Odd
n
h(n)
where α is a constant phase delay. Then we know from Chapter 6 that
h(n) must be symmetric, that is,
h (n) = h(M −1 −n),
0 ≤n ≤(M −1) with α = M −1
2
(7.3)
Hence h(n) is symmetric about α, which is the index of symmetry. There
are two possible types of symmetry:
• M odd. In this case, α = (M −1)/2 is an integer. The impulse response
is as shown above.
• M even. In this case, α = (M −1)/2 is not an integer. The impulse
response is as shown here.
  0  
M/2+1  M/2 
 M−1 
0
Symmetric Impulse Response: M Even
n
h(n)
We also have a second type of “linear-phase” FIR ﬁlter if we require
that the phase response ̸ H(ejω) satisfy the condition
̸ H(ejω) = β −αω
which is a straight line, but not through the origin. In this case, α is not
a constant phase delay, but
d̸ H(ejω)
dω
= −α
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
297
is constant, which is the group delay. Therefore, α is called a constant
group delay. In this case, as a group, frequencies are delayed at a constant
rate. But some frequencies may get delayed more and others delayed less.
For this type of linear phase, one can show that
h (n) = −h(M −1−n),
0 ≤n ≤(M −1); α = M −1
2
, β = ±π
2
(7.4)
This means that the impulse response h(n) is antisymmetric. The index
of symmetry is still α = (M −1)/2. Once again, we have two possible
types, one for M odd and one for M even.
• M odd. In this case, α = (M −1)/2 is an integer and the impulse
response is as shown.
   0   
(M−1)/2
  M−1  
0
Antisymmetric Impulse Response: M Odd
n
h(n)
Note that the sample h(α) at α = (M −1)/2 must necessarily be equal
to zero, that is, h((M −1)/2) = 0.
• M even. In this case, α = (M −1)/2 is not an integer and the impulse
response is as shown.
  0  
M/2+1  M/2 
 M−1 
0
Antisymmetric Impulse Response: M Even
n
h(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

298
Chapter 7
FIR FILTER DESIGN
7.2.2 FREQUENCY RESPONSE H(ejω)
When the cases of symmetry and antisymmetry are combined with odd
and even M, we obtain four types of linear-phase FIR ﬁlters. Frequency
response functions for each of these types have some peculiar expressions
and shapes. To study these responses, we write H(ejω) as
H(ejω) = Hr(ω)ej(β−αω);
β = ±π
2 , α = M −1
2
(7.5)
where Hr(ω) is an amplitude response function and not a magnitude re-
sponse function. The amplitude response is a real function, but unlike
the magnitude response, which is always positive, the amplitude response
may be both positive and negative. The phase response associated with
the magnitude response is a discontinuous function, while that associated
with the amplitude response is a continuous linear function. To illustrate
the diﬀerence between these two types of responses, consider the following
example.
□
EXAMPLE 7.3
Let the impulse response be h(n) = {1
↑
, 1, 1}. Determine and draw frequency
responses.
Solution
The frequency response function is
H(ejω) =
2
	
0
h(n)ejωn = 1 + 1e−jω + e−j2ω = *
ejω + 1 + e−jω+
e−jω
= {1 + 2 cos ω} e−jω
From this, the magnitude and the phase responses are
|H(ejω)| = |1 + 2 cos ω| ,
0 < ω ≤π
̸ H(ejω) =

−ω,
0 < ω < 2π/3
π −ω, 2π/3 < ω < π
since cos ω can be both positive and negative. In this case, the phase response
is piecewise linear. On the other hand, the amplitude and the corresponding
phase responses are
Hr(ω) = 1 + 2 cos ω,
̸ H(ejω) = −ω,
−π < ω ≤π
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
299
 0 
2/3
 1 
0
Magnitude Response
Frequency in π Units
Magnitude
 0 
2/3
 1 
−2/3
   0
 2/3
Piecewise Linear Phase
Frequency in π Units
Angle in π Units
 0 
2/3
 1 
0
Amplitude Response
Frequency in π Units
Amplitude
 0 
2/3
 1 
−2/3
   0
Linear Phase
Frequency in π Units
Angle in π Units
FIGURE 7.2
Frequency responses in Example 7.3
In this case, the phase response is truly linear. These responses are shown in
Figure 7.2. From this example, the diﬀerence between the magnitude and the
amplitude (or between the piecewise linear and the linear-phase) responses
should be clear.
□
Type-1 linear-phase FIR filter: Symmetrical impulse response,
M odd
In this case, β = 0, α = (M −1)/2 is an integer and h(n) =
h(M −1 −n),
0 ≤n ≤M −1. Then we can show (see Problem P7.2)
that
H(ejω) =
⎡
⎣
(M−1)/2
	
n=0
a(n) cos ωn
⎤
⎦e−jω(M−1)/2
(7.6)
where sequence a(n) is obtained from h(n) as
a(0) = h
M −1
2

(the middle sample)
a(n) = 2h
M −1
2
−n

,
1 ≤n ≤M −3
2
(7.7)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

300
Chapter 7
FIR FILTER DESIGN
Comparing (7.5) with (7.6), we have
Hr(ω) =
(M−1)/2
	
n=0
a(n) cos ωn
(7.8)
Type-2 linear-phase FIR filter: Symmetrical impulse response,
M even
In this case, again β = 0, h(n) = h(M −1 −n),
0 ≤n ≤
M −1, but α = (M −1)/2 is not an integer. Then we can show (see
Problem P7.3) that
H(ejω) =
⎡
⎣
M/2
	
n=1
b(n) cos

ω

n −1
2
$⎤
⎦e−jω(M−1)/2
(7.9)
where
b(n) = 2h
M
2 −n

,
n = 1, 2, . . . , M
2
(7.10)
Hence
Hr(ω) =
M/2
	
n=1
b(n) cos

ω

n −1
2
$
(7.11)
Note:
At ω = π, we get
Hr(π) =
M/2
	
n=1
b(n) cos

π

n −1
2
$
= 0
regardless of b(n) or h(n). Hence we cannot use this type (i.e., symmetric
h(n), M even) for highpass or bandstop ﬁlters.
Type-3 linear-phase FIR filter: Antisymmetric impulse response,
M odd
In this case, β = π/2, α = (M −1)/2 is an integer, h(n) =
−h(M −1 −n),
0 ≤n ≤M −1, and h((M −1)/2) = 0. Then we can
show (see Problem P7.4) that
H(ejω) =
⎡
⎣
(M−1)/2
	
n=1
c(n) sin ωn
⎤
⎦ej[ π
2 −( M−1
2 )ω]
(7.12)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
301
where
c(n) = 2h
M −1
2
−n

,
n = 1, 2, . . . , M −1
2
(7.13)
and
Hr(ω) =
(M−1)/2
	
n=1
c(n) sin ωn
(7.14)
Note:
At ω = 0 and ω = π, we have Hr(ω) = 0, regardless of c(n) or
h(n). Furthermore, ejπ/2 = j, which means that jHr(ω) is purely imagi-
nary. Hence this type of ﬁlter is not suitable for designing a lowpass ﬁlter
or a highpass ﬁlter. However, this behavior is suitable for approximat-
ing ideal digital Hilbert transformers and diﬀerentiators. An ideal Hilbert
transformer [79] is an allpass ﬁlter that imparts a 90◦phase shift on the
input signal. It is frequently used in communication systems for modula-
tion purposes. Diﬀerentiators are used in many analog and digital systems
to take the derivative of a signal.
Type-4 linear-phase FIR filter: Antisymmetric impulse response,
M even
This case is similar to Type-2. We have (see Problem P7.5)
H(ejω) =
⎡
⎣
M/2
	
n=1
d(n) sin

ω

n −1
2
$⎤
⎦ej[ π
2 −ω(M−1)/2]
(7.15)
where
d(n) = 2h
M
2 −n

,
n = 1, 2, . . . , M
2
(7.16)
and
Hr(ω) =
M/2
	
n=1
d(n) sin

ω

n −1
2
$
(7.17)
Note:
At ω = 0, Hr(0) = 0 and ejπ/2 = j. Hence this type is also suitable
for designing digital Hilbert transformers and diﬀerentiators.
7.2.3 MATLAB IMPLEMENTATION
The MATLAB function freqz computes the frequency response from
which we can determine the magnitude response but not the amplitude
response. The SP toolbox now provides the function zerophase that can
compute the amplitude response. However, it is easy to write simple func-
tions to compute amplitude responses for each of the four types. We pro-
vide four functions to do this.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

302
Chapter 7
FIR FILTER DESIGN
1. Hr type1:
function [Hr,w,a,L] = Hr_Type1(h);
% Computes amplitude response Hr(w) of a Type-1 LP FIR filter
% -----------------------------------------------------------
% [Hr,w,a,L] = Hr_Type1(h)
% Hr = amplitude response
%
w = 500 frequencies between [0 pi] over which Hr is computed
%
a = type-1 LP filter coefficients
%
L = order of Hr
%
h = type-1 LP filter impulse response
%
M = length(h);
L = (M-1)/2;
a = [h(L+1) 2*h(L:-1:1)]; % 1x(L+1) row vector
n = [0:1:L];
% (L+1)x1 column vector
w = [0:1:500]’*pi/500;
Hr = cos(w*n)*a’;
2. Hr type2:
function [Hr,w,b,L] = Hr_Type2(h);
% Computes amplitude response of a Type-2 LP FIR filter
% -----------------------------------------------------
% [Hr,w,b,L] = Hr_Type2(h)
% Hr = amplitude response
%
w = frequencies between [0 pi] over which Hr is computed
%
b = type-2 LP filter coefficients
%
L = order of Hr
%
h = type-2 LP impulse response
%
M = length(h);
L = M/2;
b = 2*[h(L:-1:1)];
n = [1:1:L]; n = n-0.5;
w = [0:1:500]’*pi/500;
Hr = cos(w*n)*b’;
3. Hr type3:
function [Hr,w,c,L] = Hr_Type3(h);
% Computes amplitude response Hr(w) of a Type-3 LP FIR filter
% -----------------------------------------------------------
% [Hr,w,c,L] = Hr_Type3(h)
% Hr = amplitude response
%
w = frequencies between [0 pi] over which Hr is computed
%
c = type-3 LP filter coefficients
%
L = order of Hr
%
h = type-3 LP impulse response
%
M = length(h);
L = (M-1)/2;
c = [2*h(L+1:-1:1)];
n = [0:1:L];
w = [0:1:500]’*pi/500;
Hr = sin(w*n)*c’;
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
303
4. Hr type4:
function [Hr,w,d,L] = Hr_Type4(h);
% Computes amplitude response of a Type-4 LP FIR filter
% -----------------------------------------------------
% [Hr,w,d,L] = Hr_Type4(h)
% Hr = amplitude response
%
w = frequencies between [0 pi] over which Hr is computed
%
d = type-4 LP filter coefficients
%
L = order of d
%
h = type-4 LP impulse response
%
M = length(h);
L = M/2;
d = 2*[h(L:-1:1)];
n = [1:1:L]; n = n-0.5;
w = [0:1:500]’*pi/500;
Hr = sin(w*n)*d’;
These four functions can be combined into one function, called
ampl res, that can be written to determine the type of the linear-phase
ﬁlter and to implement the appropriate amplitude response expression.
This is explored in Problem P7.6. The use of these functions is described
in Examples 7.4 through 7.7.
The zerophase function from the SP toolbox is similar in use to the
freqz function. The invocation [Hr,w, phi] = zerophase(b,a) returns
the amplitude response in Hr, evaluated at 512 values around the top half
of the unit circle in the array w and the continuous phase response in
phi. Thus this function can be used for both FIR and IIR ﬁlters. Other
invocations are also available.
7.2.4 ZERO LOCATIONS
Recall that for an FIR ﬁlter there are (M −1) (trivial) poles at the origin
and (M −1) zeros located somewhere in the z-plane. For linear-phase
FIR ﬁlters, these zeros possess certain symmetries that are due to the
symmetry constraints on h(n). It can be shown (see reference [79] and
Problem P7.7) that if H(z) has a zero at
z = z1 = rejθ
then for the linear-phase constraint, there must be a zero at
z = 1
z1
= 1
r e−jθ
For a real-valued ﬁlter, we also know that if z1 is complex, then there must
be a conjugate zero at z∗
1 = re−jθ, which implies that there must be a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

304
Chapter 7
FIR FILTER DESIGN
−1.5
−1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
4
Real Part
Imaginary Part
Pole-Zero Plot
z1
conj(z1)
1/z1
1/conj(z1)
FIGURE 7.3
A general zero constellation
zero at 1/z∗
1 = (1/r) ejθ. Thus a general zero constellation is a quadruplet
rejθ,
1
r ejθ,
re−jθ,
and
1
r e−jθ
as shown in Figure 7.3. Clearly, if r = 1, then 1/r = 1, and hence the
zeros are on the unit circle and occur in pairs
ejθ
and
e−jθ
If θ = 0 or θ = π, then the zeros are on the real line and occur in pairs
r
and
1
r
Finally, if r = 1 and θ = 0 or θ = π, the zeros are either at z = 1 or
z = −1. These symmetries can be used to implement cascade forms with
linear-phase sections.
In the following examples, we illustrate the preceding properties of
linear-phase FIR ﬁlters.
□
EXAMPLE 7.4
Let h(n) = {−4
↑
, 1, −1, −2, 5, 6, 5, −2, −1, 1, −4}. Determine the amplitude re-
sponse Hr (ω) and the locations of the zeros of H (z).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
305
Solution
Since M = 11, which is odd, and since h(n) is symmetric about α = (11−1)/2 =
5, this is a Type-1 linear-phase FIR ﬁlter. From (7.7), we have
a(0) = h (α) = h(5) = 6, a(1) = 2h(5 −1) = 10, a(2) = 2h(5 −2) = −4
a (3) = 2h (5 −3) = −2, a (4) = 2h (5 −4) = 2, a (5) = 2h (5 −5) = −8
From (7.8), we obtain
Hr(ω) = a(0) + a(1) cos ω + a(2) cos 2ω + a(3) cos 3ω + a(4) cos 4ω + a(5) cos 5ω
= 6 + 10 cos ω −4 cos 2ω −2 cos 3ω + 2 cos 4ω −8 cos 5ω
MATLAB script:
>> h = [-4,1,-1,-2,5,6,5,-2,-1,1,-4];
>> M = length(h); n = 0:M-1;
>> [Hr,w,a,L] = Hr_Type1(h);
>> a,L
a = 6
10
-4
-2
2
-8
L = 5
>> % plotting commands follow
The plots and the zero locations are shown in Figure 7.4. From these plots, we
observe that there are no restrictions on Hr (ω) either at ω = 0 or at ω = π.
There is one zero-quadruplet constellation and three zero pairs.
□
□
EXAMPLE 7.5
Let h(n) = {−4
↑
, 1, −1, −2, 5, 6, 6, 5, −2, −1, 1, −4}. Determine the amplitude
response Hr (ω) and the locations of the zeros of H (z).
Solution
This is a Type-2 linear-phase FIR ﬁlter since M = 12 and since h (n) is sym-
metric with respect to α = (12 −1) /2 = 5.5. From (7.10), we have
b(1) = 2h  12
2 −1
= 12, b(2) = 2h  12
2 −2
= 10, b(3) = 2h  12
2 −3
= −4
b(4) = 2h  12
2 −4
= −2, b(5) = 2h  12
2 −5
= 2,
b(6) = 2h  12
2 −6
= −8
Hence from (7.11) we obtain
Hr(ω) = b(1) cos 
ω 
1 −1
2

+ b(2) cos 
ω 
2 −1
2

+ b(3) cos 
ω 
3 −1
2

+ b(4) cos 
ω 
4 −1
2

+ b(5) cos 
ω 
5 −1
2

+ b(6) cos 
ω 
6 −1
2

= 12 cos
ω
2

+ 10 cos
3ω
2

−4 cos
5ω
2

−2 cos
7ω
2

+ 2 cos
9ω
2

−8 cos
11ω
2

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

306
Chapter 7
FIR FILTER DESIGN
0
5
10
−5
0
5
10
n
h(n)
Impulse Response
0
5
10
−5
0
5
10
n
a(n)
a(n) Coefficients
0
0.5
1
−20
−10
0
10
20
Frequency in π Units
Hr(ω)
Type-1 Amplitude Response
−1
0
1
−1
−0.5
0
0.5
1
10
Real Part
Imaginary Part
Pole-Zero Plot
FIGURE 7.4
Plots in Example 7.4
MATLAB script:
>> h = [-4,1,-1,-2,5,6,6,5,-2,-1,1,-4];
>> M = length(h); n = 0:M-1;
[Hr,w,b,L] = Hr_Type2(h);
>> b,L
b = 12
10
-4
-2
2
-8
L = 6
>> % plotting commands follow
The plots and the zero locations are shown in Figure 7.5. From these plots, we
observe that Hr (ω) is zero at ω = π. There is one zero-quadruplet constellation,
three zero pairs, and one zero at ω = π, as expected.
□
□
EXAMPLE 7.6
Let h(n) = {−4
↑
, 1, −1, −2, 5, 0, −5, 2, 1, −1, 4}. Determine the amplitude re-
sponse Hr (ω) and the locations of the zeros of H (z).
Solution
Since M = 11, which is odd, and since h(n) is antisymmetric about α =
(11 −1)/2 = 5, this is a Type-3 linear-phase FIR ﬁlter. From (7.13), we have
c(0) = h (α) = h(5) = 0, c(1) = 2h(5 −1) = 10, c(2) = 2h(2 −2) = −4
c (3) = 2h (5 −3) = −2, c (4) = 2h (5 −4) = 2, c (5) = 2h (5 −5) = −8
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of Linear-Phase FIR Filters
307
0
5
10
−5
0
5
10
n
h(n)
Impulse Response
0
5
10
−5
0
5
10
n
b(n)
b(n) Coefficients
0
0.5
1
−20
−10
0
10
20
30
Frequency in π Units
Type-2 Amplitude Response
−1
0
1
−1
−0.5
0
0.5
1
11
Real Part
Imaginary Part
Pole-Zero Plot
Hr(ω)
FIGURE 7.5
Plots in Example 7.5
From (7.14), we obtain
Hr(ω) = c(0) + c(1) sin ω + c(2) sin 2ω + c(3) sin 3ω + c(4) sin 4ω + c(5) sin 5ω
= 0 + 10 sin ω −4 sin 2ω −2 sin 3ω + 2 sin 4ω −8 sin 5ω
MATLAB script:
>> h = [-4,1,-1,-2,5,0,-5,2,1,-1,4];
>> M = length(h); n = 0:M-1; [Hr,w,c,L] = Hr_Type3(h);
>> c,L
a = 0
10
-4
-2
2
-8
L = 5
>> % plotting commands follow
The plots and the zero locations are shown in Figure 7.6. From these plots, we
observe that Hr (ω) = 0 at ω = 0 and at ω = π. There is one zero-quadruplet
constellation, two zero pairs, and zeros at ω = 0 and ω = π, as expected.
□
□
EXAMPLE 7.7
Let h(n) = {−4
↑
, 1, −1, −2, 5, 6, −6, −5, 2, 1, −1, 4}. Determine the amplitude
response Hr (ω) and the locations of the zeros of H (z).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

308
Chapter 7
FIR FILTER DESIGN
0
5
10
−5
0
5
10
n
h(n)
Impulse Response
0
5
10
−5
0
5
10
n
c(n)
c(n) Coefficients
0
0.5
1
−10
0
10
20
30
Frequency in π Units
Type-3 Amplitude Response
−1
0
1
−1
−0.5
0
0.5
1
10
Real Part
Imaginary Part
Pole-Zero Plot
Hr(ω)
FIGURE 7.6
Plots in Example 7.6
Solution
This is a Type-4 linear-phase FIR ﬁlter since M = 12 and since h (n) is anti-
symmetric with respect to α = (12 −1) /2 = 5.5. From (7.16), we have
d(1) = 2h  12
2 −1
= 12, d(2) = 2h  12
2 −2
= 10, d(3) = 2h  12
2 −3
= −4
d(4) = 2h  12
2 −4
= −2, d(5) = 2h  12
2 −5
= 2,
d(6) = 2h  12
2 −6
= −8
Hence from (7.17), we obtain
Hr(ω) = d(1) sin

ω

1 −1
2

+ d(2) sin

ω

2 −1
2

+ d(3) sin

ω

3 −1
2

+ d(4) sin

ω

4 −1
2

+ d(5) sin

ω

5 −1
2

+ d(6) sin

ω

6 −1
2

= 12 sin
ω
2

+ 10 sin
3ω
2

−4 sin
5ω
2

−2 sin
7ω
2

+ 2 sin
9ω
2

−8 sin
11ω
2

MATLAB script:
>> h = [-4,1,-1,-2,5,6,-6,-5,2,1,-1,4];
>> M = length(h); n = 0:M-1;
[Hr,w,d,L] = Hr_Type4(h);
>> d,L
d = 12
10
-4
-2
2
-8
L = 6
>> % plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
309
0
5
10
−5
0
5
10
n
h(n)
Impulse Response
0
5
10
−5
0
5
10
n
d(n)
d(n) Coefficients
0
0.5
1
−10
0
10
20
30
Frequency in π Units
Type-4 Amplitude Response
−1
0
1
−1
−0.5
0
0.5
1
11
Real Part
Imaginary Part
Pole-Zero Plot
Hr(ω)
FIGURE 7.7
Plots in Example 7.7
The plots and the zero locations are shown in Figure 7.7. From these plots, we
observe that Hr (ω) is zero at ω = 0. There is one zero-quadruplet constellation,
three zero pairs, and one zero at ω = 0, as expected.
□
7.3 WINDOW DESIGN TECHNIQUE
The basic idea behind the window design is to choose a proper ideal
frequency-selective ﬁlter (which always has a noncausal, inﬁnite-duration
impulse response) and then to truncate (or window) its impulse response
to obtain a linear-phase and causal FIR ﬁlter. Therefore, the emphasis
in this method is on selecting an appropriate windowing function and
an appropriate ideal ﬁlter. We will denote an ideal frequency-selective
ﬁlter by Hd(ejω), which has a unity magnitude gain and linear-phase
characteristics over its passband, and zero response over its stopband. An
ideal LPF of bandwidth ωc < π is given by
Hd(ejω) =

1 · e−jαω, |ω| ≤ωc
0,
ωc < |ω| ≤π
(7.18)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

310
Chapter 7
FIR FILTER DESIGN
where ωc is also called the cutoff frequency and α is called the sample
delay. (Note that from the DTFT properties, e−jαω implies shift in the
positive n direction or delay.) The impulse response of this ﬁlter is of
inﬁnite duration and is given by
hd(n) = F−1 
Hd(ejω)

= 1
2π
π
−π
Hd(ejω)ejωndω
(7.19)
=
1
2π
ωc

−ωc
1 · e−jαωejωndω
= sin [ωc(n −α)]
π(n −α)
Note that hd(n) is symmetric with respect to α, a fact useful for linear-
phase FIR ﬁlters.
To obtain an FIR ﬁlter from hd(n), one has to truncate hd(n) on both
sides. To obtain a causal and linear-phase FIR ﬁlter h(n) of length M, we
must have
h(n) =

hd(n), 0 ≤n ≤M −1
0,
elsewhere
and
α = M −1
2
(7.20)
This operation is called “windowing.” In general, h(n) can be thought of
as being formed by the product of hd(n) and a window function w(n) as
follows:
h(n) = hd(n)w(n)
(7.21)
where
w(n) =
⎧
⎪
⎨
⎪
⎩
some symmetric function with respect to
α over 0 ≤n ≤M −1
0, otherwise
Depending on how we deﬁne w(n), we obtain diﬀerent window designs.
For example, in (7.20)
w(n) =

1, 0 ≤n ≤M −1
0,
otherwise
= RM (n)
which is the rectangular window deﬁned earlier.
In the frequency domain, the causal FIR ﬁlter response H(ejω) is
given by the periodic convolution of Hd(ejω) and the window response
W(ejω); that is,
H(ejω) = Hd(ejω) ∗⃝W(ejω) = 1
2π
π

−π
W

ejλ
Hd

ej(ω−λ)
dλ
(7.22)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
311
0
w
p
wc
−wc
−wc
wc
−p
H(e jw)
Ripples
Minimum
Stopband
Attenuation
Periodic
Convolution
Transition
Bandwidth
0
w
p
−p
W(e jw)
0
w
p
−p
Hd (e jw)
Max Side-Lobe
Height
Main-Lobe
Width
FIGURE 7.8
Windowing operation in the frequency domain
This is shown pictorially in Figure 7.8 for a typical window response, from
which we have the following observations.
1. Since the window w(n) has a ﬁnite length equal to M, its response
has a peaky main-lobe whose width is proportional to 1/M and has
side-lobes of smaller heights.
2. The periodic convolution (7.22) produces a smeared version of the ideal
response Hd(ejω).
3. The main lobe produces a transition band in H(ejω) whose width is
responsible for the transition width. This width is then proportional
to 1/M. The wider the main lobe, the wider will be the transition
width.
4. The side lobes produce ripples that have similar shapes in both the
passband and stopband.
Basic window design idea
For the given ﬁlter speciﬁcations, choose
the ﬁlter length M and a window function w(n) for the narrowest main-
lobe width and the smallest side-lobe attenuation possible.
From observation 4, we note that the passband tolerance δ1 and
the stopband tolerance δ2 cannot be speciﬁed independently. We gen-
erally take care of δ2 alone, which results in δ2 = δ1. We now brieﬂy
describe various well-known window functions. We will use the rectangu-
lar window as an example to study their performances in the frequency
domain.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

312
Chapter 7
FIR FILTER DESIGN
7.3.1 RECTANGULAR WINDOW
This is the simplest window function but provides the worst performance
from the viewpoint of stopband attenuation. It was deﬁned earlier by
w(n) =

1, 0 ≤n ≤M −1
0,
otherwise
(7.23)
Its frequency response function is
W(ejω) =

sin
 ωM
2

sin
 ω
2


e−jω M−1
2
⇒Wr(ω) = sin
 ωM
2

sin
 ω
2

which is the amplitude response. From (7.22), the actual amplitude re-
sponse Hr (ω) is given by
Hr (ω) ≃1
2π
ω+ωc

−π
Wr (λ) dλ = 1
2π
ω+ωc

−π
sin
 ωM
2

sin
 ω
2
 dλ,
M ≫1
(7.24)
This implies that the running integral of the window amplitude response
(or integrated amplitude response) is necessary in the accurate analysis of
the transition bandwidth and the stopband attenuation. Figure 7.9 shows
the rectangular window function w (n), its amplitude response W (ω), the
amplitude response in dB, and the integrated amplitude response (7.24)
in dB. From the observation of plots in Figure 7.9, we can make several
observations.
1. The amplitude response Wr (ω) has the ﬁrst zero at ω = ω1, where
ω1M
2
= π
or
ω1 = 2π
M
Hence the width of the main lobe is 2ω1 = 4π/M. Therefore, the
approximate transition bandwidth is 4π/M.
2. The magnitude of the ﬁrst side lobe (which is also the peak side-lobe
magnitude) is approximately at ω = 3π/M and is given by
Wr

ω = 3π
M
 =

sin
 3π
2

sin
 3π
2M

 ≃2M
3π
for M ≫1
Comparing this with the main-lobe amplitude, which is equal to M,
the peak side-lobe magnitude is
2
3π = 21.22% ≡13 dB
below the main-lobe magnitude of 0 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
313
−22
0
22
0
1
Rectangular Window: M=45
n
w(n)
−1
0
1
0
45
Amplitude Response
Frequency in π Units
−1
0
1
40
13
 0
Amplitude Response in dB
Frequency in π Units
Decibels
−1
    
 1
50
21
 0
Integrated Ampl Resp
Frequency in π Units
Decibels
 Width=1.8π/M
Wr(ω)
FIGURE 7.9
Rectangular window: M = 45
3. The integrated amplitude response has the ﬁrst side-lobe magnitude
at 21 dB. This results in the minimum stopband attenuation of 21 dB
irrespective of the window length M.
4. Using the minimum stopband attenuation, the transition bandwidth
can be accurately computed. It is shown in the accumulated amplitude
response plot in Figure 7.9. This computed exact transition bandwidth is
ωs −ωp = 1.8π
M
which is less than half the approximate bandwidth of 4π/M.
Clearly, this is a simple window operation in the time domain and
an easy function to analyze in the frequency domain. However, there are
two main problems. First, the minimum stopband attenuation of 21 dB is
insuﬃcient in practical applications. Second, the rectangular windowing
being a direct truncation of the inﬁnite length hd (n), it suﬀers from the
Gibbs phenomenon. If we increase M, the width of each side lobe will
decrease, but the area under each lobe will remain constant. Therefore, the
relative amplitudes of side lobes will remain constant, and the minimum
stopband attenuation will remain at 21 dB. This implies that all ripples
will bunch up near the band edges. This is shown in Figure 7.10.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

314
Chapter 7
FIR FILTER DESIGN
−1
0
1
0
1
M = 7
Frequency in π Units
Integrated Ampl Resp
−1
0
1
0
1
M = 21
Frequency in π Units
Integrated Ampl Resp
−1
0
1
0
1
M = 51
Frequency in π Units
Integrated Ampl Resp
−1
0
1
0
1
M = 101
Frequency in π Units
Integrated Ampl Resp
FIGURE 7.10
Gibbs phenomenon
Since the rectangular window is impractical in many applications,
we consider other ﬁxed window functions that provide a ﬁxed amount
of attenuation. These window functions bear the names of the people
who ﬁrst proposed them. Although these window functions can also be
analyzed similar to the rectangular window, we present their results using
MATLAB simulations.
7.3.2 BARTLETT WINDOW
Since the Gibbs phenomenon results from the fact that the rectangular
window has a sudden transition from 0 to 1 (or 1 to 0), Bartlett suggested
a more gradual transition in the form of a triangular window, which is
given by
w(n) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
2n
M −1,
0 ≤n ≤M −1
2
2 −
2n
M −1, M −1
2
≤n ≤M −1
0,
otherwise
(7.25)
This window and its frequency-domain responses are shown in Figure 7.11.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
315
−22
0
22
0
1
Bartlett Window: M=45
n
w(n)
−1
0
1
0
22
Amplitude Response
Frequency in π Units
−1
0
1
60
27
 0
Amplitude Response in dB
Frequency in π Units
Decibels
−1
 1
60
26
 0
Integrated Ampl Resp
Frequency in π Units
Decibels
 Width=6.1π/M
Wr(ω)
FIGURE 7.11
Bartlett window: M = 45
7.3.3 HANN WINDOW
This is a raised cosine window function given by
w(n) =
⎧
⎪
⎨
⎪
⎩
0.5

1 −cos

2πn
M−1

, 0 ≤n ≤M −1
0,
otherwise
(7.26)
This window and its frequency-domain responses are shown in Figure 7.12.
7.3.4 HAMMING WINDOW
This window is similar to the Hann window except that it has a small
amount of discontinuity and is given by
w(n) =
⎧
⎨
⎩
0.54 −0.46 cos

2πn
M−1

, 0 ≤n ≤M −1
0,
otherwise
(7.27)
This window and its frequency-domain responses are shown in Figure 7.13.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

316
Chapter 7
FIR FILTER DESIGN
−22
0
22
0
1
Hann Window: M=45
n
w(n)
−1
0
1
0
23
Amplitude Response
Frequency in π Units
−1
0
1
60
32
 0
Amplitude Response in dB
Frequency in π Units
Decibels
−1
 1
60
44
 0
Integrated Ampl Resp
Frequency in π Units
Decibels
 Width=6.2π/M
Wr(ω)
FIGURE 7.12
Hann window: M = 45
−22
0
22
0
1
Hamming Window: M=45
n
w(n)
−1
0
1
0
24
Amplitude Response
Frequency in π Units
−1
0
1
60
43
 0
Amplitude Response in dB
Frequency in π Units
Decibels
−1
    
 1
70
54
 0
Integrated Ampl Resp
Frequency in π Units
Decibels
 Width=6.6π/M
Wr(ω)
FIGURE 7.13
Hamming window: M = 45
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
317
7.3.5 BLACKMAN WINDOW
This window is also similar to the previous two but contains a second
harmonic term and is given by
w(n) =
⎧
⎨
⎩
0.42 −0.5 cos

2πn
M−1

+ 0.08 cos

4πn
M−1

, 0 ≤n ≤M −1
0,
otherwise
(7.28)
This window and its frequency-domain responses are shown in Figure 7.14.
−22
0
22
0
1
Blackman Window: M=45
n
w(n)
−1
0
1
0
18
Amplitude Response
Frequency in π Units
−1
0
1
90
58
 0
Amplitude Response in dB
Frequency in π Units
Decibels
−1
 1
100
 74
  0
Integrated Ampl Resp
Frequency in π Units
Decibels
 Width=11π/M
Wr(ω)
FIGURE 7.14
Blackman window: M = 45
In Table 7.1, we provide a summary of ﬁxed window function charac-
teristics in terms of their transition widths (as a function of M) and their
minimum stopband attenuations in dB. Both the approximate as well as
the exact transition bandwidths are given. Note that the transition widths
and the stopband attenuations increase as we go down the table. The
Hamming window appears to be the best choice for many applications.
7.3.6 KAISER WINDOW
This is an adjustable window function that is widely used in practice. The
window function is due to J.F. Kaiser and is given by
w(n) =
I0

β
4
1 −(1 −
2n
M−1)2

I0 [β]
,
0 ≤n ≤M −1
(7.29)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

318
Chapter 7
FIR FILTER DESIGN
TABLE 7.1
Summary of commonly used window function characteristics
Window
Transition Width ∆ω
Min. Stopband
Name
Approximate
Exact Values
Attenuation
Rectangular
4π
M
1.8π
M
21 dB
Bartlett
8π
M
6.1π
M
25 dB
Hann
8π
M
6.2π
M
44 dB
Hamming
8π
M
6.6π
M
53 dB
Blackman
12π
M
11π
M
74 dB
where I0[ · ] is the modified zero-order Bessel function given by
I0(x) = 1 +
∞
	
k=0
(x/2)k
k!
2
which is positive for all real values of x. The parameter β controls the
minimum stopband attenuation As and can be chosen to yield diﬀerent
transition widths for near-optimum As. This window can provide diﬀerent
transition widths for the same M, which is something other ﬁxed windows
lack. For example:
• If β = 5.658, then the transition width is equal to 7.8π/M, and the
minimum stopband attenuation is equal to 60 dB. This is shown in
Figure 7.15.
• If β = 4.538, then the transition width is equal to 5.8π/M, and the
minimum stopband attenuation is equal to 50 dB.
Hence the performance of this window is comparable to that of the
Hamming window. In addition, the Kaiser window provides ﬂexible tran-
sition bandwidths. Due to the complexity involved in the Bessel functions,
the design equations for this window are not easy to derive. Fortunately,
Kaiser has developed empirical design equations, which we provide here
without proof. Given ωp, ωs, Rp, and As, the parameters M and β are
given by
Transition width = ∆ω = ωs −ωp
(7.30a)
Filter length M ≃As −7.95
2.285∆ω + 1
(7.30b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
319
−22
0
22
0
1
Kaiser Window: M=45
n
w(n)
−1
0
1
0
23
Amplitude Response
Frequency in π Units
−1
0
1
90
42
 0
Amplitude Response in dB
Frequency in π Units
Decibels
−1
 1
80
60
 0
Integrated Ampl Resp
Frequency in π Units
Decibels
 Width=7.8π/M
Wr(ω)
FIGURE 7.15
Kaiser window: M = 45, β = 5.658
Parameter β =
⎧
⎪
⎨
⎪
⎩
0.1102(As −8.7),
As ≥50
0.5842 (As −21)0.4
+ 0.07886(As −21), 21 < As < 50
(7.30c)
7.3.7 MATLAB IMPLEMENTATION
MATLAB provides several functions to implement window functions dis-
cussed in this section. A brief description of these functions follow.
• w=boxcar(M) returns the M-point rectangular window function in
array w.
• w=bartlett(M) returns the M-point Bartlett window function in
array w.
• w=hann(M) returns the M-point Hann window function in array w.
• w=hamming(M) returns the M-point Hamming window function in
array w.
• w=blackman(M) returns the M-point Blackman window function in
array w.
• w=kaiser(M,beta)
returns
the
beta-valued
M-point
rectangular
window function in array w.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

320
Chapter 7
FIR FILTER DESIGN
Using these functions, we can use MATLAB to design FIR ﬁlters based
on the window technique, which also requires an ideal lowpass impulse
response hd(n). Therefore, it is convenient to have a simple routine that
creates hd(n) as shown here.
function hd = ideal_lp(wc,M);
% Ideal lowpass filter computation
% --------------------------------
% [hd] = ideal_lp(wc,M)
%
hd = ideal impulse response between 0 to M-1
%
wc = cutoff frequency in radians
%
M = length of the ideal filter
%
alpha = (M-1)/2; n = [0:1:(M-1)];
m = n - alpha; fc = wc/pi; hd = fc*sinc(fc*m);
To display the frequency-domain plots of digital ﬁlters, MATLAB
provides the freqz function, which we used in earlier chapters. Using this
function, we have developed a modiﬁed version, called freqz m, which
returns the magnitude response in absolute as well as in relative dB scale,
the phase response, and the group delay response. We will need the group
delay response in the next chapter.
function [db,mag,pha,grd,w] = freqz_m(b,a);
% Modified version of freqz subroutine
% ------------------------------------
% [db,mag,pha,grd,w] = freqz_m(b,a);
%
db = relative magnitude in dB computed over 0 to pi radians
% mag = absolute magnitude computed over 0 to pi radians
% pha = phase response in radians over 0 to pi radians
% grd = group delay over 0 to pi radians
%
w = 501 frequency samples between 0 to pi radians
%
b = numerator polynomial of H(z)
(for FIR: b=h)
%
a = denominator polynomial of H(z) (for FIR: a=[1])
%
[H,w] = freqz(b,a,1000,’whole’);
H = (H(1:1:501))’; w = (w(1:1:501))’;
mag = abs(H);
db = 20*log10((mag+eps)/max(mag));
pha = angle(H);
grd = grpdelay(b,a,w);
7.3.8 DESIGN EXAMPLES
We now provide several examples of FIR ﬁlter design using window tech-
niques and MATLAB functions.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
321
□
EXAMPLE 7.8
Design a digital FIR lowpass ﬁlter with the following speciﬁcations:
ωp = 0.2π,
ωs = 0.3π,
Rp = 0.25 dB
As = 50 dB
Choose an appropriate window function from Table 7.1. Determine the impulse
response and provide a plot of the frequency response of the designed ﬁlter.
Solution
Both the Hamming and Blackman windows can provide attenuation of more
than 50 dB. Let us choose the Hamming window, which provides the smaller
transition band and hence has the smaller order. Although we do not use the
passband ripple value of Rp = 0.25 dB in the design, we will have to check
the actual ripple from the design and verify that it is indeed within the given
tolerance. The design steps are given in the following MATLAB script.
>> wp = 0.2*pi; ws = 0.3*pi;
tr_width = ws - wp;
>> M = ceil(6.6*pi/tr_width) + 1
M = 67
>> n=[0:1:M-1];
>> wc = (ws+wp)/2, % Ideal LPF cutoff frequency
>> hd = ideal_lp(wc,M);
w_ham = (hamming(M))’;
h = hd .* w_ham;
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
delta_w = 2*pi/1000;
>> Rp = -(min(db(1:1:wp/delta_w+1)));
% Actual passband ripple
Rp = 0.0394
>> As = -round(max(db(ws/delta_w+1:1:501))) % Min stopband attenuation
As = 52
% plotting commands follow
Note that the ﬁlter length is M = 67, the actual stopband attenuation is
52 dB, and the actual passband ripple is 0.0394 dB. Clearly, the passband
ripple is satisﬁed by this design. This practice of verifying the passband ripple
is strongly recommended. The time- and the frequency-domain plots are shown
in Figure 7.16.
□
□
EXAMPLE 7.9
For the design speciﬁcations given in Example 7.8, choose the Kaiser window
to design the necessary lowpass ﬁlter.
Solution
The design steps are given in the following MATLAB script.
>> wp = 0.2*pi; ws = 0.3*pi; As = 50;
tr_width = ws - wp;
>> M = ceil((As-7.95)/(2.285*tr_width)+1) + 1
M = 61
>> n=[0:1:M-1];
beta = 0.1102*(As-8.7)
beta = 4.5513
>> wc = (ws+wp)/2;
hd = ideal_lp(wc,M);
>> w_kai = (kaiser(M,beta))’;
h = hd .* w_kai;
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
delta_w = 2*pi/1000;
>> As = -round(max(db(ws/delta_w+1:1:501))) % Min stopband attenuation
As = 52
% plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

322
Chapter 7
FIR FILTER DESIGN
0
33
66
−0.1
0
0.1
0.2
0.3
Ideal Impulse Response
n
hd(n)
0
33
66
0
0.2
0.4
0.6
0.8
1
Hamming Window: M=67
n
w(n)
0
33
66
−0.1
0
0.1
0.2
0.3
Actual Impulse Response
n
h(n)
0
0.20.3
1
50
 0
Log-Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.16
Lowpass ﬁlter plots for Example 7.8
Note that the Kaiser window parameters are M = 61 and β = 4.5513 and that
the actual stopband attenuation is 52 dB. The time- and the frequency-domain
plots are shown in Figure 7.17.
□
□
EXAMPLE 7.10
Let us design the following digital bandpass ﬁlter.
lower stopband edge: ω1s = 0.2π,
As = 60 dB
lower passband edge: ω1p = 0.35π, Rp = 1 dB
upper passband edge: ω2p = 0.65π Rp = 1 dB
upper stopband edge: ω2s = 0.8π
As = 60 dB
These quantities are shown in Figure 7.18.
Solution
There are two transition bands, namely, ∆ω1
△= ω1p −ω1s and ∆ω2
△= ω2s −ω2p.
These two bandwidths must be the same in the window design; that is, there is
no independent control over ∆ω1 and ∆ω2. Hence ∆ω1 = ∆ω2 = ∆ω. For this
design, we can use either the Kaiser window or the Blackman window. Let us
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
323
0
30
60
−0.1
0
0.1
0.2
0.3 Ideal Impulse Response
n
hd(n)
0
30
60
0
0.2
0.4
0.6
0.8
1
Kaiser Window: M=61
n
w(n)
0
30
60
−0.1
0
0.1
0.2
0.3
Actual Impulse Response
n
h(n)
0
0.20.3
1
50
 0
Log-Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.17
Lowpass ﬁlter plots for Example 7.9
use the Blackman Window. We will also need the ideal bandpass ﬁlter impulse
response hd (n). Note that this impulse response can be obtained from two ideal
lowpass magnitude responses, provided they have the same phase response. This
is shown in Figure 7.19. Therefore, the MATLAB routine ideal lp(wc,M) is
suﬃcient to determine the impulse response of an ideal bandpass ﬁlter. The
design steps are given in the following MATLAB script.
60
1
0
0
0.35
0.35
0.65
0.8
1
w
p
Decibels
FIGURE 7.18
Bandpass ﬁlter speciﬁcations in Example 7.10
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

324
Chapter 7
FIR FILTER DESIGN
+
–
0
0
wc2
wc2
p
p
0
wc1
wc1
p
FIGURE 7.19
Ideal bandpass ﬁlter from two lowpass ﬁlters
>> ws1 = 0.2*pi; wp1 = 0.35*pi;
wp2 = 0.65*pi; ws2 = 0.8*pi;
As = 60;
>> tr_width = min((wp1-ws1),(ws2-wp2));
M = ceil(11*pi/tr_width) + 1
M = 75
>> n=[0:1:M-1];
wc1 = (ws1+wp1)/2; wc2 = (wp2+ws2)/2;
>> hd = ideal_lp(wc2,M) - ideal_lp(wc1,M);
>> w_bla = (blackman(M))’;
h = hd .* w_bla;
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
delta_w = 2*pi/1000;
>> Rp = -min(db(wp1/delta_w+1:1:wp2/delta_w)) % Actual passband ripple
Rp = 0.0030
>> As = -round(max(db(ws2/delta_w+1:1:501))) % Min stopband attenuation
As = 75
% plotting commands follow
Note that the Blackman window length is M = 75 and that the actual stopband
attenuation is 75 dB. The time- and the frequency-domain plots are shown in
Figure 7.20.
□
□
EXAMPLE 7.11
The frequency response of an ideal bandstop ﬁlter is given by
He(ejω) =
⎧
⎨
⎩
1,
0 ≤|ω| < π/3
0,
π/3 ≤|ω| ≤2π/3
1, 2π/3 < |ω| ≤π
Using a Kaiser window, design a bandstop ﬁlter of length 45 with stopband
attenuation of 60 dB.
Solution
Note that in these design speciﬁcations, the transition bandwidth is not given.
It will be determined by the length M = 45 and the parameter β of the Kaiser
window. From the design equations (7.30b), we can determine β from As; that is,
β = 0.1102 × (As −8.7)
The ideal bandstop impulse response can also be determined from the ideal
lowpass impulse response using a method similar to Figure 7.19. We can now
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
325
0
37
74
−0.4
−0.2
0
0.2
0.4
Ideal Impulse Response
n
hd(n)
0
37
74
0
0.2
0.4
0.6
0.8
1
Blackman Window: M=75
n
w(n)
0
37
74
−0.4
−0.2
0
0.2
0.4
Actual Impulse Response
n
h(n)
0
0.2 0.35
0.65 0.8
1
60
 0
Log-Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.20
Bandpass ﬁlter plots in Example 7.10
implement the Kaiser window design and check for the minimum stopband
attenuation. This is shown in the following MATLAB script.
>> M = 45; As = 60; n=[0:1:M-1];
>> beta = 0.1102*(As-8.7)
beta = 5.6533
>> w_kai = (kaiser(M,beta))’;
wc1 = pi/3; wc2 = 2*pi/3;
>> hd = ideal_lp(wc1,M) + ideal_lp(pi,M) - ideal_lp(wc2,M);
>> h = hd .* w_kai;
[db,mag,pha,grd,w] = freqz_m(h,[1]);
The β parameter is equal to 5.6533, and, from the magnitude plot in Figure 7.21,
we observe that the minimum stopband attenuation is smaller than 60 dB.
Clearly, we have to increase β to increase the attenuation to 60 dB. The required
value was found to be β = 5.9533.
>> M = 45; As = 60; n=[0:1:M-1];
>> beta = 0.1102*(As-8.7)+0.3
beta = 5.9533
>> w_kai = (kaiser(M,beta))’;
wc1 = pi/3; wc2 = 2*pi/3;
>> hd = ideal_lp(wc1,M) + ideal_lp(pi,M) - ideal_lp(wc2,M);
>> h = hd .* w_kai;
[db,mag,pha,grd,w] = freqz_m(h,[1]);
>> plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

326
Chapter 7
FIR FILTER DESIGN
 0 
1/3
2/3
 1 
60
 0
Magnitude Response in dB
Frequency in π Units
Decibels
FIGURE 7.21
Bandstop ﬁlter magnitude response in Example 7.11 for β =
5.6533
The time- and the frequency-domain plots are shown in Figure 7.22, in which
the designed ﬁlter satisﬁes the necessary requirements.
□
0
22
44
−0.2
0
0.2
0.4
0.6
0.8 Ideal Impulse Response
n
hd(n)
0
22
44
0
0.2
0.4
0.6
0.8
1
Kaiser Window: M=45
n
w(n)
0
22
44
−0.2
0
0.2
0.4
0.6
0.8 Actual Impulse Response
n
h(n)
0 
1/3
2/3
 1 
60
 0
Log-Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.22
Bandstop ﬁlter plots in Example 7.11: β = 5.9533
□
EXAMPLE 7.12
The frequency response of an ideal digital diﬀerentiator is given by
Hd(ejω) =

 jω, 0 < ω ≤π
−jω, −π < ω < 0
(7.31)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
327
Using a Hamming window of length 21, design a digital FIR diﬀerentiator. Plot
the time- and the frequency-domain responses.
Solution
The ideal impulse response of a digital diﬀerentiator with linear phase is given by
hd (n) = F 
Hd(ejω)e−jαω
= 1
2π
π
−π
Hd(ejω)e−jαωejωndω
=
1
2π
0
−π
(−jω) e−jαωejωndω + 1
2π
π
0
(jω) e−jαωejωndω
=
⎧
⎨
⎩
cos π (n −α)
(n −α)
, n ̸= α
0,
n = α
This impulse response can be implemented in MATLAB, along with the
Hamming window to design the required diﬀerentiator. Note that if M is an
even number, then α = (M −1)/2 is not an integer and hd (n) will be zero for
all n. Hence M must be an odd number, and this will be a Type-3 linear-
phase FIR ﬁlter. However, the ﬁlter will not be a full-band diﬀerentiator, since
Hr (π) = 0 for Type-3 ﬁlters.
>> M = 21; alpha = (M-1)/2;
n = 0:M-1;
>> hd = (cos(pi*(n-alpha)))./(n-alpha); hd(alpha+1)=0;
>> w_ham = (hamming(M))’;
h = hd .* w_ham;
[Hr,w,P,L] = Hr_Type3(h);
% plotting commands follow
The plots are shown in Figure 7.23.
□
□
EXAMPLE 7.13
Design a length-25 digital Hilbert transformer using a Hann window.
Solution
The ideal frequency response of a linear-phase Hilbert transformer is given by
Hd(ejω) =

−je−jαω, 0 < ω < π
+je−jαω, −π < ω < 0
(7.32)
After inverse transformation, the ideal impulse response is given by
hd (n) =
⎧
⎨
⎩
2
π
sin2 π (n −α) /2
n −α
, n ̸= α
0,
n = α
which can be easily implemented in MATLAB. Note that since M = 25, the
designed ﬁlter is of Type-3.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

328
Chapter 7
FIR FILTER DESIGN
0
10
20
−1
−0.5
0
0.5
1
Ideal Impulse Response
n
hd(n)
0
10
20
0
0.2
0.4
0.6
0.8
1
Hamming Window: M=21
n
w(n)
0
10
20
−1
−0.5
0
0.5
1
Actual Impulse Response
n
h(n)
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Amplitude Response
Frequency in π Units
Slope in π Units
FIGURE 7.23
FIR diﬀerentiator design in Example 7.12
MATLAB script:
>> M = 25; alpha = (M-1)/2;
n = 0:M-1;
>> hd = (2/pi)*((sin((pi/2)*(n-alpha)).ˆ2)./(n-alpha)); hd(alpha+1)=0;
>> w_han = (hann(M))’;
h = hd .* w_han;
[Hr,w,P,L] = Hr_Type3(h);
>> plotting commands follow
The plots are shown in Figure 7.24. Observe that the amplitude response is
plotted over −π ≤ω ≤π.
□
The SP toolbox provides a function called fir1, which designs con-
ventional lowpass, highpass, and other multiband FIR ﬁlters using the
window technique. This function’s syntax has several forms, including:
• h = fir1(N,wc) designs an Nth-order (N = M −1) lowpass FIR
ﬁlter and returns the impulse response in vector h. By default this
is a Hamming-window based, linear-phase design with a normalized
cutoﬀfrequency in wc that is a number between 0 and 1, where 1
corresponds to π rad/sample. If wc is a two-element vector, that is,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Window Design Technique
329
0
12
24
−1
−0.5
0
0.5
1
Ideal Impulse Response
n
hd(n)
0
12
24
0
0.2
0.4
0.6
0.8
1
Hanning Window: M=25
n
w(n)
0
12
24
−1
−0.5
0
0.5
1
Actual Impulse Response
n
h(n)
−1
0
1
−1
0
1
Amplitude Response
Frequency in π Units
H (ω)
r
FIGURE 7.24
FIR Hilbert transformer design in Example 7.13
wc = [wc1 wc2], then fir1 returns a bandpass ﬁlter with passband
cutoﬀs wc1 and wc2. If wc is a multi-element (more than two) vector,
then fir1 returns a multiband ﬁlter with cutoﬀs given in wc.
• h = fir1(N,wc,’ftype’) speciﬁes a ﬁlter type, where ’ftype’ is:
a. ’high’ for a highpass ﬁlter with cutoﬀfrequency Wn.
b. ’stop’ for a bandstop ﬁlter, if Wc = [wc1 wc2]. The stopband fre-
quency range is speciﬁed by this interval.
c. ’DC-1’ to make the ﬁrst band of a multiband ﬁlter a passband.
d. ’DC-0’ to make the ﬁrst band of a multiband ﬁlter a stopband.
• h = fir1(N,wc,’ftype’,window) or h = fir1(N,wc,window) uses
the vector window of length N+1 obtained from one of the speciﬁed
MATLAB window functions. The default window function used is the
Hamming window.
To design FIR ﬁlters using the Kaiser window, the SP toolbox pro-
vides the function kaiserord, which estimates window parameters that
can be used in the fir1 function. The basic syntax is
[N,wc,beta,ftype] = kaiserord(f,m,ripple);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

330
Chapter 7
FIR FILTER DESIGN
The function computes the window order N, the cutoﬀfrequency vector
wc, parameter β in beta, and the ﬁlter type ftype as discussed. The vector
f is a vector of normalized band edges and m is a vector specifying the
desired amplitude on the bands deﬁned by f. The length of f is twice the
length of m, minus 2; that is, f does not contain 0 or 1. The vector ripple
speciﬁes tolerances in each band (not in decibels). Using the estimated
parameters, the Kaiser window array can be computed and used in the
fir1 function.
To design FIR ﬁlters using window technique with arbitrary-shaped
magnitude response, the SP toolbox provides the function fir2, which
also incorporates the frequency-sampling technique. It is explained in the
following section.
7.4 FREQUENCY-SAMPLING DESIGN TECHNIQUE
In this design approach, we use the fact that the system function H (z)
can be obtained from the samples H(k) of the frequency response H(ejω).
Furthermore, this design technique ﬁts nicely with the frequency-sampling
structure that we discussed in Chapter 6. Let h(n) be the impulse response
of an M-point FIR ﬁlter, let H(k) be its M-point DFT, and let H(z) be
its system function. Then from (6.12), we have
H (z) =
M−1
	
n=0
h (n) z−n = 1 −z−M
M
M−1
	
k=0
H(k)
1 −z−1ej2πk/M
(7.33)
and
H(ejω) = 1 −e−jωM
M
M−1
	
k=0
H(k)
1 −e−jωej2πk/M
(7.34)
with
H (k) = H

ej2πk/M
=

H (0) ,
k = 0
H∗(M −k) , k = 1, . . . , M −1
For a linear-phase FIR ﬁlter, we have
h(n) = ±h(M −1 −n),
n = 0, 1, . . . , M −1
where the positive sign is for the Type-1 and Type-2 linear-phase ﬁlters,
while the negative sign is for the Type-3 and Type-4 linear-phase ﬁlters.
Then H (k) is given by
H (k) = Hr
2πk
M

ej̸
H(k)
(7.35)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
331
where
Hr
2πk
M

=
⎧
⎨
⎩
Hr (0) ,
k = 0
Hr

2π(M−k)
M

, k = 1, . . . , M −1
(7.36)
and
̸ H (k) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
−
M −1
2
 2πk
M

,
k = 0, . . . ,
/M −1
2
0
+
M −1
2
 2π
M (M −k) , k =
/M −1
2
0
+ 1, . . . , M −1
(Type-1 & -2) (7.37)
or
̸ H (k) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩

±π
2

−
M −1
2
 2πk
M

, k = 0, . . . ,
/M −1
2
0
−

±π
2

+
M −1
2
 2π
M (M −k) ,
k =
/M −1
2
0
+ 1, . . . , M −1
(Type-3 & -4)
(7.38)
Finally, we have
h(n) = IDFT [H(k)]
(7.39)
Note that several textbooks (e.g., [71, 79, 83]) provide explicit formu-
las to compute h(n), given H(k). We will use MATLAB’s ifft function
to compute h(n) from (7.39).
Basic idea
Given an ideal ﬁlter Hd(ejω), choose the ﬁlter length M
and then sample Hd(ejω) at M equispaced frequencies between 0 and
2π. The actual response H(ejω) is the interpolation of the samples H(k)
given by (7.34). This is shown in Figure 7.25. The impulse response is
given by (7.39). Similar steps apply to other frequency-selective ﬁlters.
Furthermore, this idea can also be extended for approximating arbitrary
frequency-domain speciﬁcations.
From Figure 7.25, we observe the following:
1. The approximation error—that is, the diﬀerence between the ideal and
the actual response—is zero at the sampled frequencies.
2. The approximation error at all other frequencies depends on the shape
of the ideal response; that is, the sharper the ideal response, the larger
the approximation error.
3. The error is larger near the band edges and smaller within the band.
There are two design approaches. In the ﬁrst approach, we use the
basic idea literally and provide no constraints on the approximation error;
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

332
Chapter 7
FIR FILTER DESIGN
0
1
0
1
2
3
4
5
6
Ideal Response and
Frequency Samples
Frequency Samples and
Approximated Response
7
8
9 10
kw
p
Hd (e jw)
0
1
0
1
2
3
4
5
6
7
8
9 10
kw
p
H(e jw)
FIGURE 7.25
Pictorial description of frequency-sampling technique
that is, we accept whatever error we get from the design. This approach is
called a naive design method. In the second approach, we try to minimize
error in the stopband by varying values of the transition band samples.
It results in a much better design called an optimum design method.
7.4.1 NAIVE DESIGN METHOD
In this method, we set H(k) = Hd(ej2πk/M),
k = 0, . . . , M −1, and use
(7.35) through (7.39) to obtain the impulse response h(n).
□
EXAMPLE 7.14
Consider the lowpass ﬁlter speciﬁcations from Example 7.8:
ωp = 0.2π,
ωs = 0.3π,
Rp = 0.25 dB
As = 50 dB
Design an FIR ﬁlter using the frequency-sampling approach.
Solution
Let us choose M = 20 so that we have a frequency sample at ωp, that is, at
k = 2,
ωp = 0.2π = 2π
20 2
and the next sample at ωs, that is, at k = 3,
ωs = 0.3π = 2π
20 3
Thus we have three samples in the passband [0 ≤ω ≤ωp] and seven samples in
the stopband [ωs ≤ω ≤π]. From (7.36), we have
Hr (k) = [1, 1, 1, 0, . . . , 0
& '( )
15 zeros
, 1, 1]
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
333
Since M = 20, α = 20−1
2
= 9.5 and since this is a Type-2 linear-phase ﬁlter,
from (7.37) we have
̸ H (k) =
⎧
⎨
⎩
−9.52π
20 k = −0.95πk,
0 ≤k ≤9
+0.95π (20 −k) ,
10 ≤k ≤19
Now, from (7.35), we assemble H (k) and from (7.39) determine the impulse
response h (n). The MATLAB script follows.
>> M = 20; alpha = (M-1)/2; l = 0:M-1; wl = (2*pi/M)*l;
>> Hrs = [1,1,1,zeros(1,15),1,1]; %Ideal amp res sampled
>> Hdr = [1,1,0,0]; wdl = [0,0.25,0.25,1]; %Ideal amp res for plotting
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> angH = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
>> [db,mag,pha,grd,w] = freqz_m(h,1);
[Hr,ww,a,L] = Hr_Type2(h);
>> plotting commands follow
The time- and the frequency-domain plots are shown in Figure 7.26. Observe
that the minimum stopband attenuation is about 16 dB, which is clearly unac-
ceptable. If we increase M, then there will be samples in the transition band,
for which we do not precisely know the frequency response. Therefore, the naive
design method is seldom used in practice.
□
7.4.2 OPTIMUM DESIGN METHOD
To obtain more attenuation, we will have to increase M and make
the transition band samples free samples—that is, we vary their values
to obtain the largest attenuation for the given M and the transition
width. This problem is known as an optimization problem, and it is
solved using linear programming techniques. We demonstrate the eﬀect
of transition band sample variation on the design using the following
example.
□
EXAMPLE 7.15
Using the optimum design method, design a better lowpass ﬁlter of Exam-
ple 7.14.
Solution
Let us choose M = 40 so that we have one sample in the transition band
0.2π < ω < 0.3π. Since ω1
△= 2π/40, the transition band samples are at k = 5
and at k = 40 −5 = 35. Let us denote the value of these samples by T1,
0 < T1 < 1; then the sampled amplitude response is
Hr (k) = [1, 1, 1, 1, 1, T1, 0, . . . , 0
& '( )
29 zeros
, T1, 1, 1, 1, 1]
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

334
Chapter 7
FIR FILTER DESIGN
0
0.20.3
1
0
1
Frequency Samples: M=20
Frequency in π Units
 
Hr(k)
0
10
20
−0.1
0
0.1
0.2
0.3
Impulse Response
n
h(n)
0
0.20.3
1
0
1
Amplitude Response
Frequency in π Units
 
Hr(ω)
0
0.20.3
1
16
 0
Magnitude Response
Frequency in π Units
 
Decibels
FIGURE 7.26
Naive frequency sampling design method in Example 7.14
Since α = 40−1
2
= 19.5, the samples of the phase response are
̸ H (k) =
⎧
⎨
⎩
−19.52π
40 k = −0.975πk, 0 ≤k ≤19
+0.975π (40 −k) ,
20 ≤k ≤39
Now we can vary T1 to get the best minimum stopband attenuation. This will
result in the widening of the transition width. We ﬁrst see what happens when
T1 = 0.5 using the following MATLAB script.
% T1 = 0.5
>> M = 40; alpha = (M-1)/2;
>> Hrs = [ones(1,5),0.5,zeros(1,29),0.5,ones(1,4)];
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> angH = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = Hrs.*exp(j*angH);
>> h = real(ifft(H,M));
From the plots of this design in Figure 7.27, we observe that the minimum
stopband attenuation is now 30 dB, which is better than the naive design at-
tenuation but is still not at the acceptable level of 50 dB. The best value for T1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
335
0
0.20.3
1
0
0.5
1
Freq Samples: M=40,T1=0.5
Frequency in π Units
 
Hr(k)
0
20
40
−0.1
0
0.1
0.2
0.3
Impulse Response
n
h(n)
0
0.20.3
1
0
0.5
1
Amplitude Response
Frequency in π Units
 
Hr(ω)
0
0.20.3
1
30
 0
Magnitude Response
Frequency in π Units
 
Decibels
FIGURE 7.27
Optimum frequency design method: T1 = 0.5 in Example 7.15
was obtained by varying it manually (although more eﬃcient linear program-
ming techniques are available, these were not used in this case), and the near
optimum solution was found at T1 = 0.39. The resulting ﬁlter is obtained using
the following MATLAB script.
% T1 = 0.39
>> M = 40; alpha = (M-1)/2;
>> Hrs = [ones(1,5),0.39,zeros(1,29),0.39,ones(1,4)];
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> angH = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
From the plots in Figure 7.28, we observe that the optimum stopband atten-
uation is 43 dB. It is obvious that to further increase the attenuation, we will
have to vary more than one sample in the transition band.
□
Clearly, this method is superior in that by varying one sample we
can get a much better design. In practice, the transition bandwidth is
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

336
Chapter 7
FIR FILTER DESIGN
0
0.20.3
1
0
0.39
1
Freq Samples: M=40,T1=0.39
Frequency in π Units
Hr(k)
0
20
40
−0.1
0
0.1
0.2
0.3
Impulse Response
n
h(n)
0
0.20.3
1
0
0.39
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.20.3
1
43
 0
Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.28
Optimum frequency design method: T1 = 0.39 in Example 7.15
generally small, containing either one or two samples. Hence we need to
optimize at most two samples to obtain the largest minimum stopband
attenuation. This is also equivalent to minimizing the maximum side-lobe
magnitudes in the absolute sense. Hence this optimization problem is also
called a minimax problem. This problem is solved by Rabiner et al. [83],
and the solution is available in the form of tables of transition values.
A selected number of tables are also available in [79, Appendix B]. This
problem can also be solved in MATLAB, but it would require the use of
the Optimization toolbox. We will consider a more general version of this
problem in the next section. We now illustrate the use of these tables in
the following examples.
□
EXAMPLE 7.16
Let us revisit our lowpass ﬁlter design in Example 7.14. We will solve it using two
samples in the transition band so that we can get a better stopband attenuation.
Solution
Let us choose M = 60 so that there are two samples in the transition band. Let
the values of these transition band samples be T1 and T2. Then Hr (ω) is given by
H (ω) = [1, . . . , 1
& '( )
7 ones
, T1, T2, 0, . . . , 0
& '( )
43 zeros
, T2, T1, 1, . . . , 1
& '( )
6 ones
]
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
337
From tables in [79, Appendix B], T1 = 0.5925 and T2 = 0.1099. Using these
values, we use MATLAB to compute h (n).
>> M = 60; alpha = (M-1)/2; l = 0:M-1; wl = (2*pi/M)*l;
>> Hrs = [ones(1,7),0.5925,0.1099,zeros(1,43),0.1099,0.5925,ones(1,6)];
>> Hdr = [1,1,0,0]; wdl = [0,0.2,0.3,1];
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> angH = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
>> [db,mag,pha,grd,w] = freqz_m(h,1);
[Hr,ww,a,L] = Hr_Type2(h);
The time- and the frequency-domain plots are shown in Figure 7.29. The
minimum stopband attenuation is now at 63 dB, which is acceptable.
□
□
EXAMPLE 7.17
Design the bandpass ﬁlter of Example 7.10 using the frequency sampling tech-
nique. The design speciﬁcations are as follows.
lower stopband edge: ω1s = 0.2π,
As = 60 dB
lower passband edge: ω1p = 0.35π, Rp = 1 dB
0
0.20.3
1
0
0.109
0.59
1
Lowpass: M=60,T1=0.59,T2=0.109
Frequency in π Units
Hr(k)
0
30
60
−0.1
0
0.1
0.2
0.3
Impulse Response
n
h(n)
0
0.20.3
1
0
0.109
0.59
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.20.3
1
63
 0
Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.29
Lowpass ﬁlter design plots in Example 7.16
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

338
Chapter 7
FIR FILTER DESIGN
upper passband edge: ω2p = 0.65π Rp = 1 dB
upper stopband edge: ω2s = 0.8π
As = 60 dB
Solution
Let us choose M = 40 so that we have two samples in the transition band.
Let the frequency samples in the lower transition band be T1 and T2. Then the
samples of the amplitude response are
Hr (ω) = [0, . . . , 0
& '( )
5
, T1, T2, 1, . . . , 1
& '( )
7
, T2, T1, 0, . . . , 0
& '( )
9
, T1, T2, 1, . . . , 1
& '( )
7
, T2, T1, 0, . . . , 0
& '( )
4
]
The optimum values of T1 and T2 for M = 40 and seven samples in the passband
[79, Appendix B] are
T1 = 0.109021,
T2 = 0.59417456
MATLAB script:
>> M = 40; alpha = (M-1)/2; l = 0:M-1; wl = (2*pi/M)*l;
>> T1 = 0.109021; T2 = 0.59417456;
>> Hrs=[zeros(1,5),T1,T2,ones(1,7),T2,T1,zeros(1,9),T1,T2,ones(1,7),T2,T1,zeros(1,4)];
>> Hdr = [0,0,1,1,0,0]; wdl = [0,0.2,0.35,0.65,0.8,1];
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> angH = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
>> [db,mag,pha,grd,w] = freqz_m(h,1);
[Hr,ww,a,L] = Hr_Type2(h);
The plots in Figure 7.30 show an acceptable bandpass ﬁlter design.
□
□
EXAMPLE 7.18
Design the following highpass ﬁlter.
Stopband edge: ωs = 0.6π As = 50 dB
Passband edge: ωp = 0.8π Rp = 1 dB
Solution
Recall that for a highpass ﬁlter M must be odd (or Type-1 ﬁlter). Hence we
will choose M = 33 to get two samples in the transition band. With this choice
of M it is not possible to have frequency samples at ωs and ωp. The samples of
the amplitude response are
Hr (k) = [0, . . . , 0
& '( )
11
, T1, T2, 1, . . . , 1
& '( )
8
, T2, T1, 0, . . . , 0
& '( )
10
]
while the phase response samples are
̸ H (k) =
⎧
⎪
⎨
⎪
⎩
−33 −1
2
2π
33 k = −32
33πk, 0 ≤k ≤16
+32
33π (33 −k) ,
17 ≤k ≤32
The optimum values of transition samples are T1 = 0.1095 and T2 = 0.598.
Using these values, the MATLAB design is given in the following script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
339
0
0.2 0.35
0.65 0.8
1
0
0.109
0.59
1
Bandpass: M=40,T1=0.59, T2=0.109
Frequency in π Units
Hr(k)
0
20
40
−0.4
−0.2
0
0.2
0.4
Impulse Response
n
h(n)
0
0.2 0.35
0.65 0.8
1
0
0.109
0.59
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.2 0.35
0.65 0.8
1
60
 0
Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.30
Bandpass ﬁlter design plots in Example 7.17
>> M = 33; alpha = (M-1)/2; l = 0:M-1; wl = (2*pi/M)*l;
>> T1 = 0.1095; T2 = 0.598;
>> Hrs = [zeros(1,11),T1,T2,ones(1,8),T2,T1,zeros(1,10)];
>> Hdr = [0,0,1,1]; wdl = [0,0.6,0.8,1];
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> angH = [-alpha*(2*pi)/M*k1, alpha*(2*pi)/M*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
>> [db,mag,pha,grd,w] = freqz_m(h,1);
[Hr,ww,a,L] = Hr_Type1(h);
The time- and the frequency-domain plots of the design are shown in
Figure 7.31.
□
□
EXAMPLE 7.19
Design a 33-point digital diﬀerentiator based on the ideal diﬀerentiator of (7.31)
given in Example 7.12.
Solution
From (7.31), the samples of the (imaginary-valued) amplitude response are
given by
jHr (k) =
⎧
⎪
⎨
⎪
⎩
+j 2π
M k,
k = 0, . . . ,
5M −1
2
6
−j 2π
M (M −k) , k =
5M −1
2
6
+ 1, . . . , M −1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

340
Chapter 7
FIR FILTER DESIGN
0
0.6
0.8
1
0
0.109
0.59
1
Highpass: M=33,T1=0.109,T2=0.59
Frequency in π Units
Hr(k)
0
16
32
−0.4
−0.2
0
0.2
0.4
Impulse Response
n
h(n)
0
0.6
0.8
1
0
0.109
0.59
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.6
0.8
1
50
 0
Magnitude Response
Frequency in π Units
Decibels
FIGURE 7.31
Highpass ﬁlter design plots in Example 7.18
and for linear phase the phase samples are
̸ H (k) =
⎧
⎪
⎨
⎪
⎩
−M −1
2
2π
M k = −M −1
M
πk, k = 0, . . . ,
5M −1
2
6
+M −1
M
π (M −k) ,
k =
5M −1
2
6
+ 1, . . . , M −1
Therefore,
H (k) = jHr (k) ej̸
H(k),
0 ≤k ≤M −1
and
h (n) = IDFT [H (k)]
MATLAB script:
>> M = 33; alpha = (M-1)/2; Dw = 2*pi/M;
l = 0:M-1; wl = Dw*l;
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> Hrs = [j*Dw*k1,-j*Dw*(M-k2)];
>> angH = [-alpha*Dw*k1, alpha*Dw*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
[Hr,ww,a,P]=Hr_Type3(h);
The time- and the frequency-domain plots are shown in Figure 7.32. We observe
that the diﬀerentiator is not a full-band diﬀerentiator.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
341
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.5
0
0.5
1
1.5
Differentiator, Frequency-Sampling Design: M = 33
Frequency in π Units
Hr in π Units
0
16
32
−1
−0.5
0
0.5
1
Impulse Response
n
h(n)
FIGURE 7.32
Diﬀerentiator design plots in Example 7.19
□
EXAMPLE 7.20
Design a 51-point digital Hilbert transformer based on the ideal Hilbert trans-
former of (7.32).
Solution
From (7.32), the samples of the (imaginary-valued) amplitude response are
given by
jHr (k) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
−j, k = 1, . . . ,
5M −1
2
6
0,
k = 0
+j, k =
5M −1
2
6
+ 1, . . . , M −1
Since this is a Type-3 linear-phase ﬁlter, the amplitude response will be zero
at ω = π. Hence to reduce the ripples, we should choose the two samples (in
transition bands) near ω = π, optimally between 0 and j. Using our previous
experience, we could select this value as 0.39j. The samples of the phase response
are selected similar to those in Example 7.19.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

342
Chapter 7
FIR FILTER DESIGN
0
0.2
0.4
0.6
0.8
1
0
0.39
1
Hilbert Transformer, Frequency-Sampling Design: M = 51
Frequency in π Units
Amplitude
0
25
50
−1
−0.5
0
0.5
1
Impulse Response
n
h(n)
FIGURE 7.33
Digital Hilbert transformer design plots in Example 7.20
MATLAB script:
>> M = 51; alpha = (M-1)/2; Dw = 2*pi/M;
l = 0:M-1; wl = Dw*l;
>> k1 = 0:floor((M-1)/2); k2 = floor((M-1)/2)+1:M-1;
>> Hrs = [0,-j*ones(1,(M-3)/2),-0.39j,0.39j,j*ones(1,(M-3)/2)];
>> angH = [-alpha*Dw*k1, alpha*Dw*(M-k2)];
>> H = Hrs.*exp(j*angH);
h = real(ifft(H,M));
[Hr,ww,a,P]=Hr_Type3(h);
The plots in Figure 7.33 show the eﬀect of the transition band samples.
□
The SP toolbox provides a function called fir2, which combines
the frequency-sampling technique with the window technique to design
arbitrary-shaped magnitude response FIR ﬁlters. After computing the ﬁl-
ter impulse response using the naive design method, fir2 then applies a
selected window to minimize ripples near the band-edge frequencies. This
function’s syntax also has several forms, including:
• h = fir2(N,f,m) designs an Nth-order (N = M−1) lowpass FIR ﬁlter
and returns the impulse response in vector h. The desired magnitude
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Sampling Design Technique
343
response of the ﬁlter is supplied in vectors f and m, which must be
of the same length. The vector f contains normalized frequencies in
the range from 0 to 1, where 1 corresponds to π rad/sample. The ﬁrst
value of f must be 0 and the last value 1. The vector m, contains the
samples of the desired magnitude response at the values speciﬁed in f.
The desired frequency response is then interpolated onto a dense, evenly
spaced grid of length 512. Thus this syntax corresponds to the naive
design method.
• h = fir2(N,f,m,window) uses the vector window of length N+1 ob-
tained from one of the speciﬁed MATLAB window functions. The de-
fault window function used is the Hamming window.
• h = fir2(N,f,m,npt) or h = fir2(N,f,m,npt,window) speciﬁes the
number of points npt for the grid onto which fir2 interpolates the
frequency response. The default npt value is 512.
Note that the fir2 does not implement the classic optimum frequency-
sampling method. By incorporating window design, fir2 has found an
alternative (and somewhat clever) approach to do away with the optimum
transition band values and the associated tables. By densely sampling
values in the entire band, interpolation errors are reduced (but not min-
imized), and stopband attenuation is increased to an acceptable level.
However, the basic design is contaminated by the window operation;
hence, the frequency response does not go through the original sampled
values. It is more suitable for designing FIR ﬁlters with arbitrary-shaped
frequency responses.
The type of frequency-sampling ﬁlter that we considered is called a
Type-A ﬁlter, in which the sampled frequencies are
ωk = 2π
M k,
0 ≤k ≤M −1
There is a second set of uniformly spaced samples given by
ωk = 2π

k + 1
2

M
,
0 ≤k ≤M −1
This is called a Type-B ﬁlter, for which a frequency-sampling structure is
also available. The expressions for the magnitude response H(ejω) and the
impulse response h(n) are somewhat more complicated and are available
in Proakis and Manolakis [79]. Their design can also be done in MATLAB
using the approach discussed in this section.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

344
Chapter 7
FIR FILTER DESIGN
7.5 OPTIMAL EQUIRIPPLE DESIGN TECHNIQUE
The last two techniques—namely, the window design and the frequency-
sampling design—were easy to understand and implement. However, they
have some disadvantages. First, we cannot specify the band frequencies
ωp and ωs precisely in the design; that is, we have to accept whatever
values we obtain after the design. Second, we cannot specify both δ1 and
δ2 ripple factors simultaneously. Either we have δ1 = δ2 in the window
design method, or we can optimize only δ2 in the frequency-sampling
method. Finally, the approximation error—that is, the diﬀerence between
the ideal response and the actual response—is not uniformly distributed
over the band intervals. It is higher near the band edges and smaller in
the regions away from band edges. By distributing the error uniformly,
we can obtain a lower-order ﬁlter satisfying the same speciﬁcations. For-
tunately, a technique exists that can eliminate these three problems. This
technique is somewhat diﬃcult to understand and requires a computer
for its implementation.
For linear-phase FIR ﬁlters, it is possible to derive a set of conditions
for which it can be proved that the design solution is optimal in the sense
of minimizing the maximum approximation error (sometimes called the
minimax or the Chebyshev error). Filters that have this property are called
equiripple ﬁlters because the approximation error is uniformly distributed
in both the passband and the stopband. This results in lower-order ﬁlters.
In the following, we ﬁrst formulate a minimax optimal FIR design
problem and discuss the total number of maxima and minima (collec-
tively called extrema) that one can obtain in the amplitude response of
a linear-phase FIR ﬁlter. Using this, we then discuss a general equiripple
FIR ﬁlter design algorithm, which uses polynomial interpolation for its so-
lution. This algorithm is known as the Parks–McClellan algorithm, and it
incorporates the Remez exchange algorithm for polynomial solution. This
algorithm is available as a subroutine on many computing platforms. In
this section, we will use MATLAB to design equiripple FIR ﬁlters.
7.5.1 DEVELOPMENT OF THE MINIMAX PROBLEM
Earlier in this chapter, we showed that the frequency response of the four
cases of linear-phase FIR ﬁlters can be written in the form
H(ejω) = ejβe−j M−1
2
ωHr(w)
where the values for β and the expressions for Hr(ω) are given in
Table 7.2.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
345
TABLE 7.2
Amplitude response and β-values for linear-phase FIR ﬁlters
Linear-phase FIR Filter Type
β
Hr(ejω)
Type-1: M odd, symmetric h(n)
0
(M−1)/2

0
a(n) cos ωn
Type-2: M even, symmetric h(n)
0
M/2

1
b(n) cos [ω(n −1/2)]
Type-3: M odd, antisymmetric h(n)
π
2
(M−1)/2

1
c(n) sin ωn
Type-4: M even, antisymmetric h(n)
π
2
M/2

1
d(n) sin [ω(n −1/2)]
Using simple trigonometric identities, each expression for Hr(ω) can
be written as a product of a ﬁxed function of ω (call this Q(ω)) and a
function that is a sum of cosines (call this P(ω)). For details, see Proakis
and Manolakis [79] and Problems P7.2–P7.5. Thus
Hr(ω) = Q(ω)P(ω)
(7.40)
where P(ω) is of the form
P (ω) =
L
	
n=0
α(n) cos ωn
(7.41)
and Q(ω), L, P(ω) for the four cases are given in Table 7.3.
TABLE 7.3
Q(ω), L, and P(ω) for linear-phase FIR ﬁlters
LP FIR Filter Type
Q(ω)
L
P(ω)
Type-1
1
M −1
2
L
0
a(n) cos ωn
Type-2
cos ω
2
M
2 −1
L
0
˜b(n) cos ωn
Type-3
sin ω
M −3
2
L
0
˜c(n) cos ωn
Type-4
sin ω
2
M
2 −1
L
0
˜d(n) cos ωn
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

346
Chapter 7
FIR FILTER DESIGN
The purpose of the previous analysis was to have a common form
for Hr(ω) across all four cases. It makes the problem formulation much
easier. To formulate our problem as a Chebyshev approximation problem,
we have to deﬁne the desired amplitude response Hdr(ω) and a weighting
function W(ω), both deﬁned over passbands and stopbands. The weight-
ing function is necessary so that we can have an independent control over
δ1 and δ2. The weighted error is deﬁned as
E (ω)
△= W(ω) [Hdr(ω) −Hr(ω)] ,
ω ∈S
△= [0, ωp] ∪[ωs, π]
(7.42)
These concepts are made clear in the following set of ﬁgures. It shows a
typical equiripple ﬁlter response along with its ideal response.
 0 
0.3
0.5
 1 
−0.05
 0.0 
 0.05
 0.90
 1.0 
 1.10
Frequency in π Units
Amplitude Response of an Equiripple Filter
Amplitude
Ideal Response
Ideal Response
The error [Hdr (ω) −Hr (ω)] response is shown here.
 0 
0.3
0.5
 1 
−0.10
−0.05
 0.0 
 0.05
 0.10
Frequency in π Units
Error Function
Amplitude
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
347
Now if we choose
W (ω) =
⎧
⎨
⎩
δ2
δ1
, in the passband
1, in the stopband
(7.43)
then the weighted error E(ω) response is
 0 
0.3
0.5
 1 
–0.05
0.0 
0.05
Frequency in π Units
Weighted Error Function
Amplitude
Weight = 0.5
Weight = 1.0
Thus the maximum error in both the passband and stopband is δ2. There-
fore, if we succeed in minimizing the maximum weighted error to δ2, we
automatically also satisfy the speciﬁcation in the passband to δ1. Substi-
tuting Hr(ω) from (7.40) into (7.42), we obtain
E (ω) = W (ω) [Hdr (ω) −Q (ω) P (ω)]
= W (ω) Q (ω)
Hdr (ω)
Q (ω) −P (ω)

,
ω ∈S
If we deﬁne
ˆW(ω)
△= W(ω)Q(w)
and
ˆHdr (ω)
△= Hdr (ω)
Q (ω)
then we obtain
E(ω) = ˆW(ω)

ˆHdr(ω) −P(ω)

,
ω ∈S
(7.44)
Thus we have a common form of E(ω) for all four cases.
Problem statement
The Chebyshev approximation problem can now
be deﬁned as follows:
Determine the set of coeﬃcients a(n) or ˜b(n) or ˜c(n) or ˜d(n) [or equiva-
lently a(n) or b(n) or c(n) or d(n)] to minimize the maximum absolute
value of E (ω) over the passband and stopband, that is,
min
over coeﬀ.

max
ω∈S |E (ω)|

(7.45)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

348
Chapter 7
FIR FILTER DESIGN
Now we have succeeded in specifying the exact ωp, ωs, δ1, and δ2. In
addition, the error can now be distributed uniformly in both the passband
and stopband.
7.5.2 CONSTRAINT ON THE NUMBER OF EXTREMA
Before we give the solution to this above problem, we will ﬁrst discuss the
following issue: How many local maxima and minima exist in the error
function E(ω) for a given M-point ﬁlter? This information is used by the
Parks–McClellan algorithm to obtain the polynomial interpolation. The
answer is in the expression P(ω). From (7.41), P (ω) is a trigonometric
function in ω. Using trigonometric identities of the form
cos (2ω) = 2 cos2 (ω) −1
cos (3ω) = 4 cos3 (ω) −3 cos (ω)
...
=
...
P (ω), given in (7.41) can be converted to a trigonometric polynomial in
cos (ω), as follows
P (ω) =
L
	
n=0
β(n) cosn ω
(7.46)
□
EXAMPLE 7.21
Let h(n) =
1
15[1, 2, 3, 4, 3, 2, 1]. Then M = 7 and h(n) is symmetric, which
means that we have a Type-1 linear-phase ﬁlter. Hence L = (M −1)/2 = 3.
Now from (7.7),
α(n) = a(n) = 2h(3 −n),
1 ≤n ≤2;
and
α(0) = a(0) = h(3)
or α(n) =
1
15[4, 6, 4, 2]. Hence
P (ω) =
3
	
0
α(n) cos ωn =
1
15 (4 + 6 cos ω + 4 cos 2ω + 2 cos 3ω)
=
1
15
*
4 + 6 cos ω + 4(2 cos2 ω −1) + 2(4 cos3 ω −3 cos ω)+
= 0 + 0 +
8
15 cos2 ω +
8
15 cos3 ω =
3
	
0
β(n) cosn ω
or β(n) =

0, 0, 8
15, 8
15

.
□
From (7.46), we note that P(ω) is an Lth-order polynomial in cos(ω).
Since cos(ω) is a monotone function in the open interval 0 < ω < π, then
it follows that the Lth-order polynomial P(ω) in cos(ω) should behave
like an ordinary Lth-order polynomial P(x) in x. Therefore, P(ω) has
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
349
at most (i.e., no more than) (L −1) local extrema in the open interval
0 < ω < π. For example,
cos2(ω) = 1 + cos 2ω
2
has only one minimum at ω = π/2. However, it has three extrema in the
closed interval 0 ≤ω ≤π (i.e., a maximum at ω = 0, a minimum at ω =
π/2, and a maximum at ω = π). Thus if we include the end points ω = 0
and ω = π, then P (ω) has at most (L + 1) local extrema in the closed
interval 0 ≤ω ≤π. Finally, we would like the ﬁlter speciﬁcations to be
met exactly at band edges ωp and ωs. Then the speciﬁcations can be met
at no more than (L + 3) extremal frequencies in the 0 ≤ω ≤π interval.
Conclusion
The error function E(ω) has at most (L + 3) extrema in S.
□
EXAMPLE 7.22
Let us plot the amplitude response of the ﬁlter given in Example 7.21 and count
the total number of extrema in the corresponding error function.
Solution
The impulse response is
h(n) = 1
15[1, 2, 3, 4, 3, 2, 1],
M = 7
or
L = 3
and α(n)
=
1
15[4, 6, 4, 2] and β(n)
= 
0, 0, 8
15, 8
15

from Example 7.21.
Hence
P(ω) = 8
15 cos2 ω + 8
15 cos3 ω
which is shown in Figure 7.34. Clearly, P (ω) has (L −1) = 2 extrema in the
open interval 0 < ω < π. Also shown in Figure 7.34 is the error function, which
has (L + 3) = 6 extrema.
□
Let us now turn our attention to the problem statement and equa-
tion (7.45). It is a well-known problem in approximation theory, and the
solution is given by the following important theorem.
THEOREM 1
Alternation Theorem
Let S be any closed subset of the closed interval [0, π]. In order that
P(ω) be the unique minimax approximation to Hdr(ω) on S, it is neces-
sary and sufficient that the error function E(ω) exhibit at least (L + 2)
‘‘alternations’’ or extremal frequencies in S; that is, there must exist
(L + 2) frequencies ωi in S such that
E (ωi) = −E (ωi−1) = ± max
S
|E (ω)|
(7.47)
△= ±δ, ∀ω0 < ω1 < · · · < ωL+1 ∈S
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

350
Chapter 7
FIR FILTER DESIGN
0.0
0.04
0.4
1
0.1
−0.04
1.07
1.0
0.93
Amplitude Response
L − 1 = 2
Extrema
0.4
1
0.1
w/p
w/p
0.07
0
−0.07
Error Function
L + 3 = 6
Extrema
FIGURE 7.34
Amplitude response and the error function in Example 7.22
Combining this theorem with our earlier conclusion, we infer that
the optimal equiripple ﬁlter has either (L + 2) or (L + 3) alternations
in its error function over S. Most of the equiripple ﬁlters have (L + 2)
alternations. However, for some combinations of ωp and ωs, we can get
ﬁlters with (L+3) alternations. These ﬁlters have one extra ripple in their
response and hence are called extraripple ﬁlters.
7.5.3 PARKS–McCLELLAN ALGORITHM
The alternation theorem ensures that the solution to our minimax ap-
proximation problem exists and is unique, but it does not tell us how
to obtain this solution. We know neither the order M (or equivalently,
L), nor the extremal frequencies ωi, nor the parameters {α(n)}, nor the
maximum error δ. Parks and McClellan [74] provided an iterative solution
using the Remez exchange algorithm. It assumes that the ﬁlter length M
(or L) and the ratio δ2/δ1 are known. If we choose the weighting function
as in (7.43), and if we choose the order M correctly, then δ = δ2 when
the solution is obtained. Clearly, δ and M are related; the larger the M,
the smaller the δ. In the ﬁlter speciﬁcations, δ1, δ2, ωp, and ωs are given.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
351
Therefore, M has to be assumed. Fortunately, a simple formula, due to
Kaiser, exists for approximating M. It is given by
ˆ
M = −20 log10
√δ1δ2 −13
2.285∆ω
+ 1;
∆ω = ωs −ωp
(7.48)
The Parks–McClellan algorithm begins by guessing (L + 2) extremal
frequencies {ωi} and estimating the maximum error δ at these frequencies.
It then ﬁts an Lth-order polynomial (7.46) through points given in (7.47).
Local maximum errors are determined over a ﬁner grid, and the extremal
frequencies {ωi} are adjusted at these new extremal values. A new Lth-
order polynomial is ﬁt through these new frequencies, and the procedure
is repeated. This iteration continues until the optimum set {ωi} and the
global maximum error δ are found. The iterative procedure is guaranteed
to converge, yielding the polynomial P(ω). From (7.46), coeﬃcients β(n)
are determined. Finally, the coeﬃcients a(n) as well as the impulse re-
sponse h(n) are computed. This algorithm is available in MATLAB as
the firpm function, which is described shortly.
Since we approximated M, the maximum error δ may not be equal to
δ2. If this is the case, then we have to increase M (if δ > δ2) or decrease
M (if δ < δ2) and use the firpm algorithm again to determine a new
δ. We repeat this procedure until δ ≤δ2. The optimal equiripple FIR
ﬁlter, which satisﬁes all the three requirements discussed earlier, is now
determined.
7.5.4 MATLAB IMPLEMENTATION
The Parks–McClellan algorithm is available in MATLAB as a function
called firpm, the most general syntax of which is
[h] = firpm(N,f,m,weights,ftype)
There are several versions of this syntax:
• [h] = firpm(N,f,m) designs an Nth-order (note that the length of the
ﬁlter is M = N + 1) FIR digital ﬁlter whose frequency response is
speciﬁed by the arrays f and m. The ﬁlter coeﬃcients (or the impulse
response) are returned in array h of length M. The array f contains
band-edge frequencies in units of π, that is, 0.0 ≤f ≤1.0. These fre-
quencies must be in increasing order, starting with 0.0 and ending with
1.0. The array m contains the desired magnitude response at frequen-
cies speciﬁed in f. The lengths of f and m arrays must be the same and
must be an even number. The weighting function used in each band
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

352
Chapter 7
FIR FILTER DESIGN
is equal to unity, which means that the tolerances (δi’s) in every band
are the same.
• [h] = firpm(N,f,m,weights) is similar to the preceding case except
that the array weights speciﬁes the weighting function in each band.
• [h] = firpm(N,f,m,ftype) is similar to the ﬁrst case except that
when ftype is the string ‘differentiator’ or ‘hilbert’, it designs
digital diﬀerentiators or digital Hilbert transformers, respectively. For
the digital Hilbert transformer, the lowest frequency in the f array
should not be 0, and the highest frequency should not be 1. For the
digital diﬀerentiator, the m vector does not specify the desired slope in
each band but the desired magnitude.
• [h] = firpm(N,f,m,weights,ftype) is similar to the above case ex-
cept that the array weights speciﬁes the weighting function in each
band.
To estimate the ﬁlter order N, the SP toolbox provides the function
firpmord, which also estimates other parameters that can be used in the
firpm function. The basic syntax is
[N,f0,m0,weights] = firpmord(f,m,delta);
The function computes the window order N, the normalized frequency
band edges in f0, amplitude response in a0, and the band weights in
weights. The vector f is a vector of normalized band edges and m is a
vector specifying the desired amplitude on the bands deﬁned by f. The
length of f is 2 less than twice the length of m; that is, f does not contain 0
or 1. The vector delta speciﬁes tolerances in each band (not in decibels).
The estimated parameters can now be used in the firpm function.
As explained during the description of the Parks–McClellan algo-
rithm, we have to ﬁrst guess the order of the ﬁlter using (7.48) to use
the function firpm. After we obtain the ﬁlter coeﬃcients in array h, we
have to check the minimum stopband attenuation and compare it with the
given As and then increase (or decrease) the ﬁlter order. We have to repeat
this procedure until we obtain the desired As. We illustrate this procedure
in the following several MATLAB examples. These examples also use the
ripple conversion function db2delta, which is developed in Problem P7.1.
□
EXAMPLE 7.23
Let us design the lowpass ﬁlter described in Example 7.8 using the Parks–
McClellan algorithm. The design parameters are
ωp = 0.2π ,
ωs = 0.3π ,
Rp = 0.25 dB
As = 50 dB
We provide a MATLAB script to design this ﬁlter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
353
>> wp = 0.2*pi; ws = 0.3*pi; Rp = 0.25; As = 50;
>> [delta1,delta2] = db2delta(Rp,As);
>> [N,f,m,weights] = firpmord([wp,ws]/pi,[1,0],[delta1,delta2]);
>> h = firpm(N,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> delta_w = 2*pi/1000; wsi=ws/delta_w+1; wpi = wp/delta_w;
>> Asd = -max(db(wsi:1:501))
Asd = 47.8404
>> N = N+1
N = 43
>> h = firpm(N,f,m,weights); [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(wsi:1:501))
Asd = 48.2131
>> N = N+1
N =
44
>> h = firpm(N,f,m,weights); [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(wsi:1:501))
Asd = 48.8689
>> N = N+1
N = 45
>> h = firpm(N,f,m,weights); [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(wsi:1:501))
Asd = 49.8241
>> N = N+1
N = 46
>> h = firpm(N,f,m,weights); [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(wsi:1:501))
Asd = 51.0857
>> M = N+1
M = 47
Note that we stopped this iterative procedure when the computed stopband
attenuation exceeded the given stopband attenuation As and the optimal
value of M was found to be 47. This value is considerably lower than the
window design techniques (M = 61 for a Kaiser window) or the frequency-
sampling technique (M = 60). In Figure 7.35, we show the time- and the
frequency-domain plots of the designed ﬁlter along with the error function in
both the passband and the stopband to illustrate the equiripple behavior.
□
EXAMPLE 7.24
Let us design the bandpass ﬁlter described in Example 7.10 using the Parks–
McClellan algorithm. The design parameters are
ω1s = 0.2π
ω1p = 0.35π
ω2p = 0.65π
ω2s = 0.8π
;
Rp = 1 dB
;
As = 60 db
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

354
Chapter 7
FIR FILTER DESIGN
0
23
46
−0.1
0
0.1
0.2
0.3
Design Impulse Response
n
h(n)
0
0.20.3
1
50
 0
Log-Magnitude Response
Frequency in π Units
Attenuation (dB)
0
0.20.3
1
0
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.20.3
1
−1.44
−0.32
0
0.32
1.44
Error Response
Frequency in π Units
E(ω)× 102
FIGURE 7.35
Plots for equiripple lowpass FIR ﬁlter in Example 7.23
Solution
The following MATLAB script shows how to design this ﬁlter.
>> ws1 = 0.2*pi; wp1 = 0.35*pi; wp2 = 0.65*pi; ws2 = 0.8*pi;
>> Rp = 1.0; As = 60;
>> [delta1,delta2] = db2delta(Rp,As);
>> f = [ws1,wp1,wp2,ws2]/pi; m = [0,1,0]; delta = [delta2,delta1,delta2];
>> [N,f,m,weights] = firpmord(f,m,delta); N
N = 26
>> h = firpm(N,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> delta_w=2*pi/1000;
>> ws1i=floor(ws1/delta_w)+1; wp1i = floor(wp1/delta_w)+1;
>> ws2i=floor(ws2/delta_w)+1; wp2i = floor(wp2/delta_w)+1;
>> Asd = -max(db(1:1:ws1i))
Asd = 54.7756
>> N = N+1;
>> h = firpm(N,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(1:1:ws1i))
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
355
0
15
30
−0.4
−0.2
0
0.2
0.4
Design Impulse Response
n
h(n)
0
0.2 0.35
0.65 0.8
1
60
 0
Log-Magnitude Response
Frequency in π Units
Attenuation (dB)
0
0.2 0.35
0.65 0.8
1
0
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.2 0.35
0.65 0.8
1
−1.06
0
1.06
Weighted Error
Frequency in π Units
E(ω)× 103
FIGURE 7.36
Plots for equiripple bandpass FIR ﬁlter in Example 7.24
Asd = 56.5910
>> N = N+1;
>> h = firpm(N,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
Asd = -max(db(1:1:ws1i))
>> Asd = 61.2843
>> M = N+1
M = 29
The optimal value of M was found to be 29. The time- and the frequency-domain
plots of the designed ﬁlter are shown in Figure 7.36.
□
□
EXAMPLE 7.25
Design a highpass ﬁlter that has the following speciﬁcations:
ωs = 0.6π,
ωp = 0.75π,
As = 50 dB
Rp = 0.5 dB
Solution
Since this is a highpass ﬁlter, we must ensure that the length M is an odd
number. This is shown in the following MATLAB script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

356
Chapter 7
FIR FILTER DESIGN
>> ws = 0.6*pi; wp = 0.75*pi; Rp = 0.5; As = 50;
>> [delta1,delta2] = db2delta(Rp,As);
>> [N,f,m,weights] = firpmord([ws,wp]/pi,[0,1],[delta2,delta1]); N
N = 26
>> h = firpm(N,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> delta_w = 2*pi/1000; wsi=ws/delta_w; wpi = wp/delta_w;
>> Asd = -max(db(1:1:wsi))
Asd = 49.5918
>> N = N+2;
>> h = firpm(N,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(1:1:wsi))
>> Asd = 50.2253
>> M = N+1
M = 29
Note also that we increased the value of N by two to maintain its even value.
The optimum M was found to be 29. The time- and the frequency-domain plots
of the designed ﬁlter are shown in Figure 7.37.
□
0
14
28
−0.3
−0.2
0
0.2
0.4
Design Impulse Response
n
h(n)
0
0.6 0.75
1
50
 0
Log-Magnitude Response
Frequency in π Units
Attenuation (dB)
0
0.6 0.75
1
0
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.6 0.75
1
−2.88
−0.33
0
0.33
2.88
Error Response
Frequency in π Units
E(ω)× 102
FIGURE 7.37
Plots for equiripple highpass FIR ﬁlter in Example 7.25
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
357
□
EXAMPLE 7.26
In this example, we will design a “staircase” ﬁlter, which has 3 bands with
diﬀerent ideal responses and diﬀerent tolerances in each band. The design spec-
iﬁcations are as follows.
Band-1:
0 ≤ω ≤0.3π, Ideal gain = 1,
Tolerance δ1 = 0.01
Band-2: 0.4π ≤ω ≤0.7π, Ideal gain = 0.5, Tolerance δ2 = 0.005
Band-3: 0.8π ≤ω ≤π,
Ideal gain = 0,
Tolerance δ3 = 0.001
Solution
The following MATLAB script describes the design procedure.
>> w1 = 0; w2 = 0.3*pi; delta1 = 0.01;
>> w3 = 0.4*pi; w4 = 0.7*pi; delta2 = 0.005;
>> w5 = 0.8*pi; w6 = pi; delta3 = 0.001;
>> weights = [delta3/delta1 delta3/delta2 1];
>> Dw = min((w3-w2), (w5-w3));
>> M = ceil((-20*log10((delta1*delta2*delta3)ˆ(1/3))-13)/(2.285*Dw)+1)
>> M = 51
>> f = [0 w2/pi w3/pi w4/pi w5/pi 1];
>> m = [1 1 0.5 0.5 0 0];
>> h = firpm(M-1,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> delta_w = 2*pi/1000;
>> w1i=floor(w1/delta_w)+1; w2i = floor(w2/delta_w)+1;
>> w3i=floor(w3/delta_w)+1; w4i = floor(w4/delta_w)+1;
>> w5i=floor(w5/delta_w)+1; w6i = floor(w6/delta_w)+1;
>> Asd = -max(db(w5i:w6i))
Asd = 62.0745
>> M = M-1; h = firpm(M-1,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(w5i:w6i))
Asd = 60.0299
>> M = M-1; h = firpm(M-1,f,m,weights);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
>> Asd = -max(db(w5i:w6i))
Asd = 60.6068
>> M
M = 49
The time- and the frequency-domain plots of the designed ﬁlter are shown in
Figure 7.38.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

358
Chapter 7
FIR FILTER DESIGN
0
24
48
−0.1
0
0.2
0.4
0.6
Actual Impulse Response
n
h(n)
0
0.30.4
0.70.8
1
60
 0
Log-Magnitude Response
Frequency in π Units
Decibels
0
0.30.4
0.70.8
1
0
0.5
1
Amplitude Response
Frequency in π Units
Hr(ω)
0
0.30.4
0.70.8
1
−1
0
1
Weighted Error
Frequency in π Units
E(ω)×103
FIGURE 7.38
Plots for equiripple staircase FIR ﬁlter in Example 7.26
□
EXAMPLE 7.27
In this example, we will design a digital diﬀerentiator with diﬀerent slopes in
each band. The speciﬁcations are as follows.
Band-1:
0 ≤ω ≤0.2π, Slope = 1 sam/cycle
Band-2: 0.4π ≤ω ≤0.6π, Slope = 2 sam/cycle
Band-3: 0.8π ≤ω ≤π,
Slope = 3 sam/cycle
Solution
We need desired magnitude response values in each band. These can be ob-
tained by multiplying band-edge frequencies in cycles/sam by the slope values
in sam/cycle.
Band-1:
0 ≤f ≤0.1, Slope = 1 sam/cycle ⇒0.0 ≤|H| ≤0.1
Band-2: 0.2 ≤f ≤0.3, Slope = 2 sam/cycle ⇒0.4 ≤|H| ≤0.6
Band-3: 0.4 ≤f ≤0.5, Slope = 3 sam/cycle ⇒1.2 ≤|H| ≤1.5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Optimal Equiripple Design Technique
359
0
12
25
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
Impulse Response
n
h(n)
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.4
0.6
1.2
1.5
Magnitude Response
Normalized Frequency f
Magnitude
FIGURE 7.39
Plots of the diﬀerentiator in Example 7.27
Let the weights be equal in all bands. MATLAB script:
>> f = [0 0.2 0.4 0.6 0.8 1];
% In w/pi units
>> m = [0,0.1,0.4,0.6,1.2,1.5];
% Magnitude values
>> h = firpm(25,f,m,’differentiator’);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
% Plot commands follow
The frequency-domain response is shown in Figure 7.39.
□
□
EXAMPLE 7.28
Finally, we design a Hilbert transformer over the band 0.05π ≤ω ≤0.95π.
Solution
Since this is a wideband Hilbert transformer, we will choose an odd length for
our ﬁlter (i.e., a Type-3 ﬁlter). Let us choose M = 51. MATLAB script:
>> f = [0.05,0.95]; m = [1 1];
h = firpm(50,f,m,’hilbert’);
>> [db,mag,pha,grd,w] = freqz_m(h,[1]);
% Plot commands follow
The plots of this Hilbert transformer are shown in Figure 7.40.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

360
Chapter 7
FIR FILTER DESIGN
0
25
50
−0.8
−0.4
0
0.4
0.8
Impulse Response
n
h(n)
0 0.05
0.95 1
0
1
Magnitude Response
Frequency in π Units
Magnitude
FIGURE 7.40
Plots of the Hilbert transformer in Example 7.28
7.6 PROBLEMS
P7.1
The absolute and relative (dB) speciﬁcations for a lowpass ﬁlter are related by (7.1) and
(7.2). In this problem, we will develop a simple MATLAB function to convert one set of
speciﬁcations into another.
1. Write a MATLAB function to convert absolute speciﬁcations δ1 and δ2 into the relative
speciﬁcations Rp and As in dB. The format of the function should be
function [Rp,As] = delta2db(delta1,delta2)
% Converts absolute specs delta1 and delta2 into dB specs Rp and As
% [Rp,As] = delta2db(delta1,delta2)
Verify your function using the speciﬁcations given in Example 7.2.
2. Write a MATLAB function to convert relative (dB) speciﬁcations Rp and As into the
absolute speciﬁcations δ1 and δ2. The format of the function should be
function [delta1,delta2] = db2delta(Rp,As)
% Converts dB specs Rp and As into absolute specs delta1 and delta2
% [delta1,delta2] = db2delta(Rp,As)
Verify your function using the speciﬁcations given in Example 7.1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
361
P7.2
The Type-1 linear-phase FIR ﬁlter is characterized by
h(n) = h(M −1 −n),
0 ≤n ≤M −1,
M odd
Show that its amplitude response Hr(ω) is given by
Hr(ω) =
L
	
n=0
a(n) cos(ωn),
L = M −1
2
where coeﬃcients {a(n)} are obtained as deﬁned in (7.7).
P7.3
The Type-2 linear-phase FIR ﬁlter is characterized by
h(n) = h(M −1 −n),
0 ≤n ≤M −1,
M even
1. Show that its amplitude response Hr(ω) is given by
Hr(ω) =
M/2
	
n=1
b(n) cos *
ω 
n −1
2
+
where coeﬃcients {b(n)} are obtained as deﬁned in (7.10).
2. Show that Hr(ω) can be further expressed as
Hr(ω) = cos
ω
2

L
	
n=0
˜b(n) cos(ωn),
L = M
2 −1
where coeﬃcients ˜b(n) are given by
b(1) = ˜b(0) + 1
2˜b(1),
b(n) = 1
2
˜b(n −1) + ˜b(n)
,
2 ≤n ≤M
2 −1,
b  M
2

= 1
2
˜b  M
2 −1
.
P7.4
The Type-3 linear-phase FIR ﬁlter is characterized by
h(n) = −h(M −1 −n),
0 ≤n ≤M −1,
M odd
1. Show that its amplitude response Hr(ω) is given by
Hr(ω) =
(M−1)/2
	
n=1
c(n) sin(ωn)
where coeﬃcients {c(n)} are obtained as deﬁned in (7.13).
2. Show that Hr(ω) can be further expressed as
Hr(ω) = sin(ω)
L
	
n=0
˜c(n) cos(ωn),
L = M −3
2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

362
Chapter 7
FIR FILTER DESIGN
where coeﬃcients ˜c(n) are given by
c(1) = ˜c(0) −1
2 ˜c(1),
c(n) = 1
2 [˜c(n −1) −˜c(n)] ,
2 ≤n ≤M −3
2
,
c
M −1
2

= 1
2 ˜c
M −3
2

.
P7.5
The Type-4 linear-phase FIR ﬁlter is characterized by
h(n) = −h(M −1 −n),
0 ≤n ≤M −1,
M even
1. Show that its amplitude response Hr(ω) is given by
Hr(ω) =
M/2
	
n=1
d(n) sin *
ω 
n −1
2
+
where coeﬃcients {d(n)} are obtained as deﬁned in (7.16).
2. Show that the above Hr(ω) can be further expressed as
Hr(ω) = sin
ω
2

L
	
n=0
˜d(n) cos(ωn),
L = M
2 −1
where coeﬃcients ˜d(n) are given by
d(1) = ˜d(0) −1
2 ˜d(1),
d(n) = 1
2
 ˜d(n −1) −˜d(n)
,
2 ≤n ≤M
2 −1,
d
M
2

= 1
2
˜d  M
2 −1
.
P7.6
Write a MATLAB function to compute the amplitude response Hr(ω) given a linear phase
impulse response h(n). The format of this function should be
function [Hr,w,P,L] = Ampl_Res(h);
% Computes amplitude response Hr(w) and its polynomial P of order L,
%
given a linear-phase FIR filter impulse response h.
% The type of filter is determined automatically by the subroutine.
%
% [Hr,w,P,L] = Ampl_Res(h)
% Hr = amplitude response
% w = frequencies between [0 pi] over which Hr is computed
% P = polynomial coefficients
% L = Order of P
% h = Linear-phase filter impulse response
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
363
The function should ﬁrst determine the type of the linear-phase FIR ﬁlter and then use the
appropriate Hr Type# function discussed in this chapter. It should also check if the given
h(n) is of a linear-phase type. Verify your function on sequences given here.
hI(n) = (0.9)|n−5| cos[π(n −5)/12] [u(n) −u(n −11)]
hII(n) = (0.9)|n−4.5| cos[π(n −4.5)/11] [u(n) −u(n −10)]
hIII(n) = (0.9)|n−5| sin[π(n −5)/12] [u(n) −u(n −11)]
hIV(n) = (0.9)|n−4.5| sin[π(n −4.5)/11] [u(n) −u(n −10)]
h(n) = (0.9)n cos[π(n −5)/12] [u(n) −u(n −11)]
P7.7
Prove the following properties of linear-phase FIR ﬁlters.
1. If H(z) has four zeros at z1 = rejθ, z2 = 1
r e−jθ, z3 = re−jθ, and z4 = 1
r e−jθ, then H(z)
represents a linear-phase FIR ﬁlter.
2. If H(z) has two zeros at z1 = ejθ and z2 = e−jθ, then H(z) represents a linear-phase
FIR ﬁlter.
3. If H(z) has two zeros at z1 = r and z2 = 1
r , then H(z) represents a linear-phase FIR
ﬁlter.
4. If H(z) has a zero at z1 = 1 or a zero at z1 = −1, then H(z) represents a linear-phase
FIR ﬁlter.
5. For each of the sequences given in Problem P7.6, plot the locations of zeros. Determine
which sequences imply linear-phase FIR ﬁlters.
P7.8
A notch ﬁlter is an LTI system, which is used to eliminate an arbitrary frequency ω = ω0.
The ideal linear-phase notch ﬁlter frequency response is given by
Hd

ejω
=

0,
|ω| = ω0;
1 · e−jαω, otherwise.
(α is a delay in samples)
1. Determine the ideal impulse response, hd(n), of the ideal notch ﬁlter.
2. Using hd(n), design a linear-phase FIR notch ﬁlter using a length 51 rectangular window
to eliminate the frequency ω0 = π/2 rad/sample. Plot the amplitude response of the
resulting ﬁlter.
3. Repeat part 2 using a length 51 Hamming window. Compare your results.
P7.9
Design a linear-phase bandpass ﬁlter using the Hann window design technique. The
speciﬁcations are
lower stopband edge:
0.2π
upper stopband edge: 0.75π As = 40 dB
lower passband edge: 0.35π
upper passband edge: 0.55π Rp = 0.25 dB
Plot the impulse response and the magnitude response (in dB) of the designed ﬁlter. Do not
use the fir1 function.
P7.10 Design a bandstop ﬁlter using the Hamming window design technique. The speciﬁcations are
lower stopband edge: 0.4π
upper stopband edge: 0.6π As = 50 dB
lower passband edge: 0.3π
upper passband edge: 0.7π Rp = 0.2 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

364
Chapter 7
FIR FILTER DESIGN
Plot the impulse response and the magnitude response (in dB) of the designed ﬁlter. Do not
use the fir1 function.
P7.11 Design a bandpass ﬁlter using the Hamming window design technique. The speciﬁcations are
lower stopband edge: 0.3π
upper stopband edge: 0.6π As = 50 dB
lower passband edge: 0.4π
upper passband edge: 0.5π Rp = 0.5 dB
Plot the impulse response and the magnitude response (in dB) of the designed ﬁlter. Do not
use the fir1 function.
P7.12 Design a highpass ﬁlter using one of the ﬁxed window functions. The speciﬁcations are
stopband edge: 0.4π, As = 50 dB
passband edge: 0.6π, Rp = 0.004 dB
Plot the zoomed magnitude response (in dB) of the designed ﬁlter in the passband to verify
the passband ripple Rp. Do not use the fir1 function.
P7.13 Using the Kaiser window method, design a linear-phase FIR digital ﬁlter that meets the
following speciﬁcations:
0.975 ≤|H(ejω)| ≤1.025,
0 ≤|H(ejω)| ≤0.005,
0.975 ≤|H(ejω)| ≤1.025,
0 ≤ω ≤0.25π
0.35π ≤ω ≤0.65π
0.75π ≤ω ≤π
Determine the minimum-length impulse response h(n) of such a ﬁlter. Provide a plot
containing subplots of the amplitude response and the magnitude response in dB. Do not
use the fir1 function.
P7.14 We wish to use the Kaiser window method to design a linear-phase FIR digital ﬁlter that
meets the following speciﬁcations:
0 ≤|H(ejω)| ≤0.01,
0.95 ≤|H(ejω)| ≤1.05,
0 ≤|H(ejω)| ≤0.01,
0 ≤ω ≤0.25π
0.35π ≤ω ≤0.65π
0.75π ≤ω ≤π
Determine the minimum-length impulse response h(n) of such a ﬁlter. Provide a plot
containing subplots of the amplitude response and the magnitude response in dB. Do not
use the fir1 function.
P7.15 Design the staircase ﬁlter of Example 7.26 using the Kaiser window approach. The
speciﬁcations are as follows.
Band-1:
0 ≤ω ≤0.3π, Ideal gain = 1,
δ1 = 0.01
Band-2: 0.4π ≤ω ≤0.7π, Ideal gain = 0.5, δ2 = 0.005
Band-3: 0.8π ≤ω ≤π,
Ideal gain = 0,
δ3 = 0.001
Compare the ﬁlter length of this design with that of Example 7.26. Provide a plot of the
magnitude response in dB. Do not use the fir1 function.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
365
P7.16 Design a bandpass ﬁlter using a ﬁxed window design technique that has the minimum
length and that satisﬁes the following speciﬁcations:
lower stopband edge = 0.3π
upper stopband edge = 0.6π
$
As = 40 dB
lower passband edge = 0.4π
upper passband edge = 0.5π
$
Rp = 0.5 dB
Provide a plot of the log-magnitude response in dB and stem plot of the impulse response.
P7.17 Repeat Problem P7.9 using the fir1 function.
P7.18 Repeat Problem P7.10 using the fir1 function.
P7.19 Repeat Problem P7.11 using the fir1 function.
P7.20 Repeat Problem P7.12 using the fir1 function.
P7.21 Repeat Problem P7.13 using the fir1 function.
P7.22 Repeat Problem P7.14 using the fir1 function.
P7.23 Consider an ideal lowpass ﬁlter with the cutoﬀfrequency ωc = 0.3π. We want to
approximate this ﬁlter using a frequency-sampling design in which we choose 40 samples.
1. Choose the sample at ωc equal to 0.5, and use the naive design method to compute h(n).
Determine the minimum stopband attenuation.
2. Now vary the sample at ωc, and determine the optimum value to obtain the largest
minimum stopband attenuation.
3. Plot the magnitude responses in dB of the preceding two designs in one plot; comment
on the results.
P7.24 Design the bandstop ﬁlter of Problem P7.10 using the frequency-sampling method. Choose
the order of the ﬁlter appropriately so that there are two samples in the transition band.
Use optimum values for these samples. Compare your results with those obtained using the
fir2 function.
P7.25 Design the bandpass ﬁlter of Problem P7.11 using the frequency-sampling method. Choose
the order of the ﬁlter appropriately so that there are two samples in the transition band.
Use optimum values for these samples. Compare your results with those obtained using the
fir2 function.
P7.26 Design the highpass ﬁlter of Problem P7.12 using the frequency-sampling method. Choose
the order of the ﬁlter appropriately so that there are two samples in the transition band.
Use optimum values. Compare your results with those obtained using the fir2 function.
P7.27 Consider the ﬁlter speciﬁcations given in Figure P7.1. Use the fir2 function and a
Hamming window to design a linear-phase FIR ﬁlter via the frequency-sampling method.
Experiment with the ﬁlter length to achieve the required design. Plot the amplitude
response of the resulting ﬁlter.
P7.28 Design a bandpass ﬁlter using the frequency-sampling method. Choose the order of the
ﬁlter appropriately so that there is one sample in the transition band. Use optimum value
for this sample. The speciﬁcations are as follows:
lower stopband edge = 0.3π
upper stopband edge = 0.7π
$
As = 40 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

366
Chapter 7
FIR FILTER DESIGN
0
0
0.25
0.35
0.65
0.75
1
1
2.02
1.98
Amplitude Response
w
p
FIGURE P7.1
Filter speciﬁcations for Problem P7.27
lower passband edge = 0.4π
upper passband edge = 0.6π
$
Rp = 0.5 dB.
Provide a plot of the log-magnitude response in dB and stem plot of the impulse response.
P7.29 The frequency response of an ideal bandpass ﬁlter is given by
Hd(ejω) =
⎧
⎨
⎩
0,
0 ≤|ω| ≤π/3
1,
π/3 ≤|ω| ≤2π/3
0, 2π/3 ≤|ω| ≤π
1. Determine the coeﬃcients of a 25-tap ﬁlter based on the Parks–McClellan algorithm
with stopband attenuation of 50 dB. The designed ﬁlter should have the smallest
possible transition width.
2. Plot the amplitude response of the ﬁlter using the function developed in Problem P7.6.
P7.30 Consider the bandstop ﬁlter given in Problem P7.10.
1. Design a linear-phase bandstop FIR ﬁlter using the Parks–McClellan algorithm. Note
that the length of the ﬁlter must be odd. Provide a plot of the impulse response and the
magnitude response in dB of the designed ﬁlter.
2. Plot the amplitude response of the designed ﬁlter and count the total number of extrema
in stopband and passbands. Verify this number with the theoretical estimate of the total
number of extrema.
3. Compare the order of this ﬁlter with those of the ﬁlters in Problems P7.10 and P7.24.
4. Verify the operation of the designed ﬁlter on the following signal:
x(n) = 5 −5 cos
πn
2

;
0 ≤n ≤300
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
367
P7.31 Using the Parks–McClellan algorithm, design a 25-tap FIR diﬀerentiator with slope equal
to 1 sample/cycle.
1. Choose the frequency band of interest between 0.1π and 0.9π. Plot the impulse response
and the amplitude response.
2. Generate 100 samples of the sinusoid
x(n) = 3 sin(0.25πn),
n = 0, ..., 100
and process through the preceding FIR diﬀerentiator. Compare the result with the
theoretical “derivative” of x(n). Note: Don’t forget to take the 12-sample delay of the
FIR ﬁlter into account.
P7.32 Design a lowest-order equiripple linear-phase FIR ﬁlter to satisfy the speciﬁcations given in
Figure P7.2. Provide a plot of the amplitude response and a plot of the impulse response.
P7.33 A digital signal x(n) contains a sinusoid of frequency π/2 and a Gaussian noise w(n) of zero
mean and unit variance; that is,
x(n) = 2 cos πn
2 + w(n)
We want to ﬁlter out the noise component using a 50th-order causal and linear-phase FIR
ﬁlter.
1. Using the Parks–McClellan algorithm, design a narrow bandpass ﬁlter with passband
width of no more than 0.02π and stopband attenuation of at least 30 dB. Note that no
other parameters are given and that you have to choose the remaining parameters for
the firpm function to satisfy the requirements. Provide a plot of the log-magnitude
response in dB of the designed ﬁlter.
2. Generate 200 samples of the sequence x(n) and processed through the preceding ﬁlter to
obtain the output y(n). Provide subplots of x(n) and y(n) for 100 ≤n ≤200 on one plot
and comment on your results.
0
0.4
0.5
0.7
0.8
1
0.90
Band 1
Band 2
Band 3
0.45
0.35
0.05
0.00
Amplitude Response
Frequency ω in π Units
FIGURE P7.2
Filter speciﬁcations for Problem P7.32
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

368
Chapter 7
FIR FILTER DESIGN
P7.34 Design a minimum-order linear-phase FIR ﬁlter, using the Parks–McClellan algorithm, to
satisfy the requirements given in Figure P7.1.
1. Provide a plot of the amplitude response with grid lines and axis labeling as shown in
Figure P7.1.
2. Generate the following signals:
x1(n) = cos(0.25πn),
x2(n) = cos(0.5πn),
x3(n) = cos(0.75πn);
0 ≤n ≤100
Process these signals through this ﬁlter to obtain the corresponding output signals
y1(n), y2(n), and y3(n). Provide stem plots of all input and output signals in one ﬁgure.
P7.35 Design a minimum-order linear-phase FIR ﬁlter, using the Parks–McClellan algorithm, to
satisfy the requirements given in Figure P7.3. Provide a plot of the amplitude response with
grid lines and axis labeling as shown in Figure P7.3.
P7.36 The speciﬁcations on the amplitude response (not to scale) of an FIR ﬁlter are given in
Figure P7.4.
1. Using a window design approach and a ﬁxed window function, design a minimum-length
linear-phase FIR ﬁlter to satisfy the given requirements. Provide a plot of the amplitude
response with grid lines as shown in Figure P7.4.
2. Using a window design approach and the Kaiser window function, design a
minimum-length linear-phase FIR ﬁlter to satisfy the given requirements. Provide a plot
of the amplitude response with grid lines as shown in Figure P7.4.
3. Using a frequency-sampling design approach and with no more than two samples in the
transition bands, design a minimum-length linear-phase FIR ﬁlter to satisfy the given
requirements. Provide a plot of the amplitude response with grid lines as shown in
Figure P7.4.
4. Using the Parks–McClellan design approach, design a minimum-length linear-phase FIR
ﬁlter to satisfy the given requirements. Provide a plot of the amplitude response with
grid lines as shown in Figure P7.4.
5. Compare the preceding four design methods in terms of
•
the order of the ﬁlter,
0
0.2 0.25
0.45
0.55
0.7 0.75
1
3.3
2.7
2.2
1.8
1.1
0.9
0.05
−0.05
Amplitude Response
w
p
FIGURE P7.3
Filter speciﬁcations for Problem P7.35
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
369
0.2
0.5
0.7
0
4.3
4.0
0.25
0.45
0.75
1.0
2.1
1.9
0.05
−0.05
Hr(w)
w
p
FIGURE P7.4
Filter speciﬁcations for Problem P7.36
•
the exact band-edge frequencies,
•
the exact tolerances in each band.
P7.37 Design a minimum-order linear-phase FIR ﬁlter, using the Parks–McClellan algorithm, to
satisfy the requirements given in Figure P7.5. Provide a plot of the amplitude response with
grid lines as shown in Figure P7.5.
P7.38 Design a minimum-length linear-phase bandpass ﬁlter of Problem P7.9 using the
Parks–McClellan algorithm.
1. Plot the impulse response and the magnitude response in dB of the designed ﬁlter in one
ﬁgure plot.
2. Plot the amplitude response of the designed ﬁlter and count the total number of extrema
in passband and stopbands. Verify this number with the theoretical estimate of the total
number of extrema.
3. Compare the order of this ﬁlter with that of the ﬁlter in Problem P7.9.
0.3
0.55
0.75
0
3.1
4.1
3.0
4.0
0.35
0.5
0.8
1.0
2.1
1.9
1.0
0.9
Hr(w)
w
p
FIGURE P7.5
Filter speciﬁcations for Problem P7.37
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

370
C H A P T E R 8
IIR Filter
Design
IIR ﬁlters have inﬁnite-duration impulse responses, and hence they can
be matched to analog ﬁlters, all of which generally have inﬁnitely long im-
pulse responses. Therefore, the basic technique of IIR ﬁlter design trans-
forms well-known analog ﬁlters into digital ﬁlters using complex-valued
mappings. The advantage of this technique lies in the fact that both
analog ﬁlter design (AFD) tables and the mappings are available exten-
sively in the literature. This basic technique is called the A/D (analog-
to-digital) ﬁlter transformation. However, the AFD tables are available
only for lowpass ﬁlters. We also want to design other frequency-selective
ﬁlters (highpass, bandpass, bandstop, etc.). To do this, we need to apply
frequency-band transformations to lowpass ﬁlters. These transformations
are also complex-valued mappings, and they are also available in the liter-
ature. There are two approaches to this basic technique of IIR ﬁlter design:
Approach 1:
Design Analog
Lowpass Filter
−→
Apply Frequency-Band
Transformation
s →s
−→
Apply Filter
Transformation
s →z
−→Desired IIR
Filter
Approach 2:
Design Analog
Lowpass Filter
−→
Apply Filter
Transformation
s →z
−→
Apply Frequency-Band
Transformation
z →z
−→Desired IIR
Filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Preliminaries
371
The ﬁrst approach is used in MATLAB to design IIR ﬁlters. A
straightforward use of these MATLAB functions does not provide any
insight into the design methodology. Therefore, we will study the second
approach because it involves the frequency-band transformation in the
digital domain. Hence in this IIR ﬁlter design technique, we will follow
the following steps:
• Design analog lowpass ﬁlters.
• Study and apply ﬁlter transformations to obtain digital lowpass ﬁlters.
• Study and apply frequency-band transformations to obtain other digi-
tal ﬁlters from digital lowpass ﬁlters.
The main problem with these approaches is that we have no control
over the phase characteristics of the IIR ﬁlter. Hence IIR ﬁlter designs
will be treated as magnitude-only designs. More sophisticated techniques,
which can simultaneously approximate both the magnitude and the phase
responses, require advanced optimization tools and hence will not be cov-
ered in this book.
We begin with a discussion on the analog ﬁlter speciﬁcations and the
properties of the magnitude-squared response used in specifying analog
ﬁlters. Next, before we delve into basic techniques for general IIR ﬁlters,
we consider the design of special types of digital ﬁlters—for example,
resonators, notch ﬁlters, comb ﬁlters, and so on. This is followed by a
brief description of the characteristics of three widely used analog ﬁl-
ters: Butterworth, Chebyshev, and elliptic ﬁlters. Finally, we will study
transformations to convert these prototype analog ﬁlters into diﬀerent
frequency-selective digital ﬁlters and conclude this chapter with several
IIR ﬁlter designs using MATLAB.
8.1 SOME PRELIMINARIES
We discuss two preliminary issues in this section. First, we consider the
magnitude-squared response speciﬁcations, which are more typical of ana-
log (and hence of IIR) ﬁlters. These speciﬁcations are given on the relative
linear scale. Second, we study the properties of the magnitude-squared
response.
8.1.1 RELATIVE LINEAR SCALE
Let Ha(jΩ) be the frequency response of an analog ﬁlter. Then the lowpass
ﬁlter speciﬁcations on the magnitude-squared response are given by
1
1 + ϵ2 ≤|Ha(jΩ)|2 ≤1,
|Ω| ≤Ωp
0 ≤|Ha(jΩ)|2 ≤1
A2 ,
Ωs ≤|Ω|
(8.1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

372
Chapter 8
IIR FILTER DESIGN
1
0
1
0
A2
Ω
|Ha(jΩ)|2
Ωp
Ωs
1
1 +
FIGURE 8.1
Analog lowpass ﬁlter speciﬁcations
where ϵ is a passband ripple parameter, Ωp is the passband cutoﬀfre-
quency in rad/sec, A is a stopband attenuation parameter, and Ωs is the
stopband cutoﬀin rad/sec. These speciﬁcations are shown in Figure 8.1,
from which we observe that |Ha(jΩ)|2 must satisfy
|Ha(jΩp)|2 =
1
1 + ϵ2
at Ω= Ωp
|Ha(jΩs)|2 =
1
A2
at Ω= Ωs
(8.2)
The parameters ϵ and A are related to parameters Rp and As, respec-
tively, of the dB scale. These relations are given by
Rp = −10 log10
1
1 + ϵ2 =⇒ϵ =
7
10Rp/10 −1
(8.3)
and
As = −10 log10
1
A2 =⇒A = 10As/20
(8.4)
The ripples, δ1 and δ2, of the absolute scale are related to ϵ and A by
1 −δ1
1 + δ1
=

1
1 + ϵ2 =⇒ϵ = 2√δ1
1 −δ1
and
δ2
1 + δ1
= 1
A =⇒A = 1 + δ1
δ2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Preliminaries
373
8.1.2 PROPERTIES OF |Ha(jΩ)|2
Analog ﬁlter speciﬁcations (8.1), which are given in terms of the
magnitude-squared response, contain no phase information. Now to eval-
uate the s-domain system function Ha(s), consider
Ha(jΩ) = Ha(s)|s=jΩ
Then we have
|Ha(jΩ)|2 = Ha(jΩ)H∗
a(jΩ) = Ha(jΩ)Ha(−jΩ) = Ha(s)Ha(−s)|s=jΩ
or
Ha(s)Ha(−s) = |Ha(jΩ)|2
Ω=s/j
(8.5)
Therefore, the poles and zeros of the magnitude-squared function are dis-
tributed in a mirror-image symmetry with respect to the jΩaxis. Also for
real ﬁlters, poles and zeros occur in complex conjugate pairs (or mirror-
image symmetry with respect to the real axis). A typical pole-zero pat-
tern of Ha(s)Ha(−s) is shown in Figure 8.2. From this pattern, we can
construct Ha(s), which is the system function of our analog ﬁlter. We
want Ha(s) to represent a causal and stable ﬁlter. Then all poles of Ha(s)
must lie within the left half-plane. Thus we assign all left-half poles of
Ha(s)Ha(−s) to Ha(s). However, zeros of Ha(s) can lie anywhere in the
s-plane. Therefore, they are not uniquely determined unless they all are
on the jΩaxis. We will choose the zeros of Ha(s)Ha(−s) lying left to or
on the jΩaxis as the zeros of Ha(s). The resulting ﬁlter is then called a
minimum-phase ﬁlter.
jΩ
s-plane
σ
FIGURE 8.2
Typical pole-zero pattern of Ha(s)Ha(−s)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

374
Chapter 8
IIR FILTER DESIGN
8.2 SOME SPECIAL FILTER TYPES
In this section, we consider the design of several special types of digital
ﬁlters and describe their frequency response characteristics. We begin by
describing the design and characteristics of a digital resonator.
8.2.1 DIGITAL RESONATORS
A digital resonator is a special two-pole bandpass ﬁlter with a pair of
complex-conjugate poles located very near the unit circle, as shown in
the left plot of Figure 8.3. The magnitude of the frequency response of
the ﬁlter is shown in the top right plot of Figure 8.3. The name resonator
refers to the fact that the ﬁlter has a large magnitude response in the
vicinity of the pole position. The angle of the pole location determines
the resonant frequency of the ﬁlter. Digital resonators are useful in many
applications, including simple bandpass ﬁltering and speech generation.
Let us consider the design of a digital resonator with a resonant peak
at or near ω = ω0. Hence we select the pole position as
p1,2 = re±jω0
(8.6)
–1
–0.5
0
0.5
1
–1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
2
Real Part
Imaginary Part
Pole-Zero Plot
–1 
–1/3
 0 
 1/3
 1 
0
0.2
0.4
0.6
0.8
1
Magnitude
Magnitude Response
Digital Resonator Responeses
–1 
–1/3
 0 
 1/3
 1 
–0.5
0
0.5
Phase Response
ω in π Units
Radians / π
FIGURE 8.3
Pole positions and frequency response of a digital resonator with
r = 0.9 and ω0 = π/3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Special Filter Types
375
The corresponding system function is
H(z) =
b0
(1 −rejω0z−1)(1 −re−jω0z−1)
=
b0
1 −(2r cos ω0)z−1 + r2z−2
(8.7)
where b0 is a gain parameter. The frequency response of the resonator is
H

ejω
=
b0

1 −re−j(ω−ω0) 
1 −re−j(ω+ω0)
(8.8)
Since
H

ejω has its peak at or near ω = ω0, we select the gain param-
eter b0 so that
H

ejω0 = 1. Hence
H

ejω0 =
b0
|(1 −r)(1 −re−j2ω0)|
=
b0
(1 −r)
√
1 + r2 −2r cos 2ω0
(8.9)
Consequently, the desired gain parameter is
b0 = (1 −r)
7
1 + r2 −2r cos 2ω0
(8.10)
The magnitude of the frequency response H(ω) may be expressed as
H

ejω =
b0
D1(ω)D2(ω)
(8.11)
where D1(ω) and D2(ω) are given as
D1(ω) =
7
1 + r2 −2r cos(ω −ω0)
(8.12a)
D2(ω) =
7
1 + r2 −2r cos(ω + ω0)
(8.12b)
For a given value of r, D1(ω) takes its minimum value (1 −r) at ω = ω0,
and the product D1(ω)D2(ω) attains a minimum at the frequency
ωr = cos−1
1 + r2
2r
cos ω0

(8.13)
which deﬁnes precisely the resonant frequency of the ﬁlter. Note that
when r is very close to unity, ωr ≈ω0, which is the angular position of
the pole. Furthermore, as r approaches unity, the resonant peak becomes
sharper (narrower) because D1(ω) changes rapidly in the vicinity of ω0.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

376
Chapter 8
IIR FILTER DESIGN
A quantitative measure of the width of the peak is the 3 dB bandwidth
of the ﬁlter, denoted as ∆(ω). For values of r close to unity,
∆ω ≈2(1 −r)
(8.14)
Figure 8.3 illustrates the magnitude and phase responses of a digital res-
onator with ω0 = π/3, r = 0.90. Note that the phase response has its
greatest rate of change near the resonant frequency ωr ≈ω0 = π/3.
This resonator has two zeros at z = 0. Instead of placing zeros at the
origin, an alternative choice is to locate the zeros at z = 1 and z = −1.
This choice completely eliminates the response of the ﬁlter at the frequen-
cies ω = 0 and ω = π, which may be desirable in some applications. The
corresponding resonator has the system function
H(z) =
G(1 −z−1)(1 + z−1)
(1 −rejω0z−1)(1 −re−jω0z−1)
= G
1 −z−2
1 −(2r cos ω0)z−1 + r2z−2
(8.15)
and the frequency response characteristic
H

ejω
= G
1 −e−j2ω
[1 −rej(ω0−ω)][1 −re−j(ω0+ω)]
(8.16)
where G is a gain parameter that is selected so that
H

ejω0 = 1.
The introduction of zeros at z = ±1 alters both the magnitude and
phase response of the resonator. The magnitude response may be ex-
pressed as
H

ejω = G
N(ω)
D1(ω)D2(ω)
(8.17)
where N(ω) is deﬁned as
N(ω) =
7
2(1 −cos 2ω)
(8.18)
Due to the presence of the zeros at z = ±1, the resonant frequency of the
resonator is altered from the expression given by (8.13). The bandwidth
of the ﬁlter is also altered. Although exact values for these two parameters
are rather tedious to derive, we can easily compute the frequency response
when the zeros are at z = ±1 and z = 0 and compare the results.
Figure 8.4 illustrates the magnitude and phase responses for the cases
z = ±1 and z = 0, for pole location at ω = π/3 and r = 0.90. We observe
that the resonator with z = ±1 has a slightly smaller bandwidth than
the resonator with zeros at z = 0. In addition, there appears to be a very
small shift in the resonant frequency between the two cases.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Special Filter Types
377
–1 
–1/3
 0 
 1/3
 1 
0
0.2
0.4
0.6
0.8
1
Magnitude
Magnitude Response
–1 
–1/3
 0 
 1/3
 1 
–0.5
0
0.5
Phase Response
ω in π Units
Radians/ π
FIGURE 8.4
Magnitude and phase responses of digital resonator with zeros at
z = ±1 (solid lines) and z = 0 (dotted lines) for r = 0.9 and ω0 = π/3
8.2.2 NOTCH FILTERS
A notch ﬁlter is a ﬁlter that contains one or more deep notches or, ideally,
perfect nulls in its frequency response. Figure 8.5 illustrates the frequency
response of a notch ﬁlter with a null at the frequency ω = ω0. Notch ﬁlters
are useful in many applications where speciﬁc frequency components must
be eliminated. For example, instrumentation systems require that the
power line frequency of 60 Hz and its harmonics be eliminated.
To create a null in the frequency response of a ﬁlter at a frequency
ω0, we simply introduce a pair of complex-conjugate zeros on the unit
circle at the angle ω0. Hence the zeros are selected as
z1,2 = e±jω0
(8.19)
Then the system function for the notch ﬁlter is
H(z) = b0(1 −ejω0z−1)(1 −e−jω0z−1)
= b0(1 −(2 cos ω0)z−1 + z−2)
(8.20)
where b0 is a gain factor. Figure 8.6 illustrates the magnitude response of
a notch ﬁlter having a null at ω = π/4.
The major problem with this notch ﬁlter is that the notch has a rela-
tively large bandwidth, which means that other frequency components
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

378
Chapter 8
IIR FILTER DESIGN
–1
–0.5
0
0.5
1
–1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
2
Real Part
Imaginary Part 
Pole-Zero Plot
–1 
0 
1 
–50
0
10
Decibels
Magnitude Response
–1 
0 
1 
–1
–0.5
0
0.5
1
Phase Response
ω in π  Units
Radians / π
FIGURE 8.5
Frequency response of a typical notch ﬁlter
around the desired null are severely attenuated. To reduce the band-
width of the null, we may resort to the more sophisticated, longer FIR
ﬁlter designed according to the optimum equiripple design method de-
scribed in Chapter 7. Alternatively, we could attempt to improve the fre-
quency response of the ﬁlter by introducing poles in the system function.
–1
–0.5
0
0.5
1
–1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
2
Real Part
Imaginary Part 
Pole-Zero Plot
–1 
–1/4
0 
1/4
1 
–50
0
10
Decibels
Magnitude Response
–1 
–1/4
0 
1/4
1 
–1
–0.5
0
0.5
1
Phase Response
ω in π Units
Radians / π 
FIGURE 8.6
Frequency response of a notch ﬁlter with ω0 = π/4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Special Filter Types
379
–1
–0.5
0
0.5
1
–1
–0.8
–0.6
–0.4
–0.2
0
0.2
0.4
0.6
0.8
1
Real Part
Imaginary Part
Pole-Zero Plot
–1 
–1/4
 0 
 1/4
 1 
–50
0
10
Decibels
Magnitude Response
–1 
–1/4
 0 
 1/4
 1 
–1
–0.5
0
0.5
1
Phase Response
ω in π Units
Radians / π
FIGURE 8.7
Magnitude and phase responses of notch ﬁlter with poles (solid
lines) and without poles (dotted lines) for ω0 = π/4 and r = 0.85
In particular, suppose that we select the poles at
p1,2 = re±jω0
(8.21)
Hence the system function becomes
H(z) = b0
1 −(2 cos ω0)z−1 + z2
1 −(2r cos ω0)z−1 + r2z−2
(8.22)
The magnitude of the frequency response
H

ejω of this ﬁlter is illus-
trated in Figure 8.7 for ω0 = π/4 and r = 0.85. Also plotted in this ﬁgure
is the frequency response without the poles. We observe that the eﬀect of
the pole is to introduce a resonance in the vicinity of the null and, thus, to
reduce the bandwidth of the notch. In addition to reducing the bandwidth
of the notch, the introduction of a pole in the vicinity of the null may re-
sult in a small ripple in the passband of the ﬁlter due to the resonance
created by the pole.
8.2.3 COMB FILTERS
In its simplest form, a comb ﬁlter may be viewed as a notch ﬁlter in which
the nulls occur periodically across the frequency band, hence the analogy
to an ordinary comb that has periodically spaced teeth. Comb ﬁlters are
used in many practical systems, including the rejections of power-line
harmonics and the suppression of clutter from ﬁxed objects in moving-
target indicator (MTI) radars.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

380
Chapter 8
IIR FILTER DESIGN
We can create a comb ﬁlter by taking our FIR ﬁlter with system
function
H(z) =
M
	
k=0
h(k)z−k
(8.23)
and replacing z by zL, where L is a positive integer. Thus the new FIR
ﬁlter has the system function
HL(z) =
M
	
k=0
h(k)z−kL
(8.24)
If the frequency response of the original FIR ﬁlter is H

ejω
, the frequency
response of the ﬁlter given by (8.24) is
HL

ejω
=
M
	
k=0
h(k)e−jkLω = H

ejLω
(8.25)
Consequently, the frequency response characteristic HL

ejω
is an L-order
repetition of H

ejω
in the range 0 ≤ω ≤2π. Figure 8.8 illustrates the
relationship between HL

ejω
and H

ejω
for L = 4. The introduction of
a pole at each notch may be used to narrow the bandwidth of each notch,
as just described.
FIGURE 8.8
Comb ﬁlters with frequency response HL

ejω
obtained from
H
ejω
for L = 4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Special Filter Types
381
8.2.4 ALLPASS FILTERS
An allpass filter is characterized by a system function that has a constant
magnitude response for all frequencies, that is,
H

ejω = 1,
0 ≤ω ≤π
(8.26)
A simple example of an allpass system is a system that introduces a pure
delay to an input signal, that is,
H(z) = z−k
(8.27)
This system passes all frequency components of an input signal without
any frequency-dependent attenuation. It simply delays all frequency com-
ponents by k samples.
A more general characterization of an allpass ﬁlter is one having a
system function of the form
H(z) = aN + aN−1z−1 + · · · + a1z−N+1 + z−N
1 + a1z−1 + · · · + aN−1z−N+1 + aNz−N
(8.28)
which may be expressed in the compact form as
H(z) = z−N A(z−1)
A(z)
(8.29)
where
A(z) =
N
	
k=0
akz−k,
a0 = 1
(8.30)
We observe that
H

ejω2 = H(z)H(z−1)|z=ejω = 1
(8.31)
for all frequencies. Hence the system is allpass.
From the form of H(z) given by (8.28), we observe that if z0 is a
pole of H(z), then 1/z0 is a zero of H(z). That is, the poles and zeros
are reciprocals of one another. Figure 8.9 illustrates the typical pole-zero
pattern for a single-pole, single-zero ﬁlter and a two-pole, two-zero ﬁlter.
Graphs of the magnitude and phase characteristics of these two ﬁlters are
shown in Figure 8.10 for a = 0.6 and r = 0.9, ω0 = π/4, where A(z) for
the two ﬁlters is, respectively, given as
A(z) = 1 + az−1
(8.32a)
A(z) = 1 −(2r cos ω0)z−1 + r2z−2
(8.32b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

382
Chapter 8
IIR FILTER DESIGN
(a)
(b)
Unit Circle
FIGURE 8.9
Pole-zero locations for (a) one-pole and (b) two-pole allpass ﬁlter
–1
0
1
–40
0
10
Decibels
ω in π Units
Magnitude Response
– 1
0
1
–1
–0.5
0
0.5
1
Phase Response
ω in π Units
Radians / π
FIGURE 8.10
Magnitude and phase responses for one-pole (solid line) and two-
pole (dotted line) allpass ﬁlters
The general form for the system function of an allpass ﬁlter with real
coeﬃcients may be expressed in factored form as
H(z) =
NR

k=1
z−1 −αk
1 −αkz−1
NC

k=1
(z−1 −βk)(z−1 −β∗
k)
(1 −βkz−1)(1 −β∗
kz−1)
(8.33)
where NR is the number of real poles and zeros and NC is the number
of complex-conjugate pairs of poles and zeros. For a causal and stable
system, we require that |αk| < 1 and |βk| < 1.
Allpass ﬁlters are usually employed as phase equalizers. When placed
in cascade with a system that has an undesirable phase response, a phase
equalizer is designed to compensate for the poor phase characteristics of
the system and thus result in an overall linear phase system.
8.2.5 DIGITAL SINUSOIDAL OSCILLATORS
A digital sinusoidal oscillator can be viewed as a limiting form of a two-
pole resonator for which the complex-conjugate poles are located on the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Some Special Filter Types
383
unit circle. From our previous discussion of resonators, the system function
for a resonator with poles at re±jω0 is
H(z) =
b0
1 −(2r cos ω0)z−1 + r2z−2
(8.34)
When we set r = 1 and select the gain parameter b0 as
b0 = A sin ω0
(8.35)
the system function becomes
H(z) =
A sin ω0
1 −(2 cos ω0)z−1 + z−2
(8.36)
and the corresponding impulse response of the system becomes
h(n) = A sin(n + 1)ω0 u(n)
(8.37)
Thus this system generates a sinusoidal signal of frequency ω0 when ex-
cited by an impulse δ(n) = 1.
The block diagram representation of the system function given by
(8.36) is illustrated in Figure 8.11. The corresponding diﬀerence equation
for this system is
y(n) = (2 cos ω0) y(n −1) −y(n −2) + b0δ(n)
(8.38)
where b0 = A sin ω0.
Note that the sinusoidal oscillation obtained from the diﬀerence equa-
tion in (8.38) can also be obtained by setting the input to zero and setting
the initial conditions to y(−1) = 0, y(−2) = −A sin ω0. Thus the zero-
input response to the second-order system described by the homogeneous
diﬀerence equation
y(n) = (2 cos ω0) y(n −1) −y(n −2)
(8.39)
z–1
z –1
z cos (w0)
A sin (w0)d (n)
y (n) = A sin [(n +1 )w0]u(n)
–1
FIGURE 8.11
Digital sinusoidal oscillator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

384
Chapter 8
IIR FILTER DESIGN
with initial conditions y(−1) = 0, y(−2) = −A sin ω0 is exactly the same
as the response of (8.38) to an impulse excitation. In fact, the homo-
geneous diﬀerence equation in (8.39) can be obtained directly from the
trigonometric identity
sin α + sin β = 2 sin
α + β
2

cos
α −β
2

(8.40)
where, by deﬁnition, α = (n + 1)ω0, β = (n −1)ω0, and y(n) = sin(n +
1)ω0.
In practical applications involving modulation of two sinusoidal car-
rier signals in phase quadrature, there is a need to generate the sinusoids
A sin ω0n and A cos ω0n. These quadrature carrier signals can be gener-
ated by the so-called coupled-form oscillator, which can be obtained with
the aid of the trigonometric formulas
cos(α + β) = cos α cos β −sin α sin β
(8.41)
sin(α + β) = sin α cos β + cos α sin β
(8.42)
where by deﬁnition, α = nω0, β = ω0, yc(n) = cos(n+1)ω0, and ys(n) =
sin(n + 1)ω0. Thus, with substitution of these quantities into the two
trigonometric identities, we obtain the two coupled diﬀerence equations
yc(n) = (cos ω0) yc(n −1) −(sin ω0) ys(n −1)
(8.43)
ys(n) = (sin ω0) yc(n −1) + (cos ω0) ys(n −1)
(8.44)
The structure for the realization of the coupled-form oscillator is il-
lustrated in Figure 8.12. Note that this is a two-output system that does
not require any input excitation, but it does require setting the initial
conditions yc(−1) = A cos ω0 and ys(−1) = −A sin ω0 in order to begin
its self-sustaining oscillations.
z –1
z –1
ys(n)
yc(n)
sin (w0)
cos (w0)
cos (w0)
–sin (w0)
FIGURE 8.12
Realization of the coupled form oscillator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
385
8.3 CHARACTERISTICS OF PROTOTYPE ANALOG FILTERS
IIR ﬁlter design techniques rely on existing analog ﬁlters to obtain digital
ﬁlters. We designate these analog ﬁlters as prototype ﬁlters. Three proto-
types are widely used in practice. In this section, we brieﬂy summarize
the characteristics of the lowpass versions of these prototypes: Butter-
worth lowpass, Chebyshev lowpass (Type I and II), and Elliptic lowpass.
Although we will use MATLAB functions to design these ﬁlters, it is nec-
essary to learn the characteristics of these ﬁlters so that we can use proper
parameters in MATLAB functions to obtain correct results.
8.3.1 BUTTERWORTH LOWPASS FILTERS
This ﬁlter is characterized by the property that its magnitude response is
ﬂat in both passband and stopband. The magnitude-squared response of
an Nth-order lowpass ﬁlter is given by
|Ha(jΩ)|2 =
1
1 +
 Ω
Ωc
2N
(8.45)
where N is the order of the ﬁlter and Ωc is the cutoﬀfrequency in rad/sec.
The plot of the magnitude-squared response is as follows.
From this plot, we can observe the following properties:
• At Ω= 0, |Ha(j0)|2 = 1 for all N.
• At Ω= Ωc, |Ha(jΩc)|2 = 1
2 for all N, which implies a 3 dB attenuation
at Ωc.
• |Ha(jΩ)|2 is a monotonically decreasing function of Ω.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

386
Chapter 8
IIR FILTER DESIGN
• |Ha(jΩ)|2 approaches an ideal lowpass ﬁlter as N →∞.
• |Ha(jΩ)|2 is maximally flat at Ω= 0 since derivatives of all orders exist
and are equal to zero.
To determine the system function Ha(s), we put (8.45) in the form of
(8.5) to obtain
Ha(s)Ha(−s) = |Ha(jΩ)|2
Ω=s/j =
1
1 +
 s
jΩc
2N =
(jΩ)2N
s2N + (jΩc)2N
(8.46)
The roots of the denominator polynomial (or poles of Ha(s)Ha(−s)) from
(8.46) are given by
pk = (−1)
1
2N (jΩ) = Ωcej
π
2N (2k+N+1),
k = 0, 1, . . . , 2N −1
(8.47)
An interpretation of (8.47) is that
• there are 2N poles of Ha(s)Ha(−s), which are equally distributed on
a circle of radius Ωc with angular spacing of π/N radians;
• for N odd the poles are given by pk = Ωcejkπ/N,
k = 0, 1, . . . , 2N −1;
• for N even the poles are given by pk = Ωcej( π
2N + kπ
N ),
k = 0, 1, . . . ,
2N −1;
• the poles are symmetrically located with respect to the jΩaxis;
• a pole never falls on the imaginary axis, and falls on the real axis only
if N is odd.
As an example, the poles of third- and fourth-order Butterworth ﬁlters
are shown in Figure 8.13.
0
jΩ
Ωc
s
k = 2N − 1
N = 3
k = 0
0
jΩ
Ωc
s
k = 2N − 1
N = 4
k = 0
FIGURE 8.13
Pole plots for Butterworth ﬁlters
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
387
FIGURE 8.14
Pole plot for Example 8.1
A stable and causal ﬁlter Ha(s) can now be speciﬁed by selecting
poles in the left half-plane, and Ha(s) can be written in the form
Ha(s) =
ΩN
c
,
LHP poles
(s −pk)
(8.48)
□
EXAMPLE 8.1
Given that |Ha(jΩ)|2 =
1
1 + 64Ω6 , determine the analog ﬁlter system function
Ha(s).
Solution
From the given magnitude-squared response,
|Ha(jΩ)|2 =
1
1 + 64Ω6 =
1
1 +
 Ω
0.5
2(3)
Comparing this with expression (8.45), we obtain N = 3 and Ωc = 0.5. The
poles of Ha(s)Ha(−s) are as shown in Figure 8.14.
Hence
Ha(jΩ) =
Ω3
c
(s −p2)(s −p3)(s −p4)
=
1/8
(s + 0.25 −j0.433)(s + 0.5)(s + 0.25 + j0.433)
=
0.125
(s + 0.5)(s2 + 0.5s + 0.25)
□
8.3.2 MATLAB IMPLEMENTATION
MATLAB provides a function called [z,p,k]=buttap(N) to design a nor-
malized (i.e., Ωc = 1) Butterworth analog prototype ﬁlter of order N,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

388
Chapter 8
IIR FILTER DESIGN
which returns zeros in z array, poles in p array, and the gain value k.
However, we need an unnormalized Butterworth ﬁlter with arbitrary Ωc.
From Example 8.1, we observe that there are no zeros and that the poles
of the unnormalized ﬁlter are on a circle with radius Ωc instead of on a
unit circle. This means that we have to scale the array p of the normal-
ized ﬁlter by Ωc and the gain k by ΩN
c . In the following function, called
U buttap(N,Omegac), we design the unnormalized Butterworth analog
prototype ﬁlter.
function [b,a] = u_buttap(N,Omegac);
% Unnormalized Butterworth analog lowpass filter prototype
% --------------------------------------------------------
% [b,a] = u_buttap(N,Omegac);
%
b = numerator polynomial coefficients of Ha(s)
%
a = denominator polynomial coefficients of Ha(s)
%
N = Order of the Butterworth Filter
% Omegac = Cutoff frequency in radians/sec
%
[z,p,k] = buttap(N);
p = p*Omegac;
k = k*OmegacˆN;
B = real(poly(z));
b0 = k;
b = k*B;
a = real(poly(p));
This function provides a direct form (or numerator-denominator) struc-
ture. Often, we also need a cascade form structure. In Chapter 6, we have
already studied how to convert a direct form into a cascade form. The
following sdir2cas function describes the procedure that is suitable for
analog ﬁlters.
function [C,B,A] = sdir2cas(b,a);
% DIRECT form to CASCADE form conversion in s-plane
% -------------------------------------------------
% [C,B,A] = sdir2cas(b,a)
%
C = gain coefficient
%
B = K by 3 matrix of real coefficients containing bk’s
%
A = K by 3 matrix of real coefficients containing ak’s
%
b = numerator polynomial coefficients of DIRECT form
%
a = denominator polynomial coefficients of DIRECT form
%
Na = length(a)-1; Nb = length(b)-1;
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
389
% Compute gain coefficient C
b0 = b(1); b = b/b0;
a0 = a(1); a = a/a0;
C = b0/a0;
%
% Denominator second-order sections:
p= cplxpair(roots(a)); K = floor(Na/2);
if K*2 == Na
% Computation when Na is even
A = zeros(K,3);
for n=1:2:Na
Arow = p(n:1:n+1,:);
Arow = poly(Arow);
A(fix((n+1)/2),:) = real(Arow);
end
elseif Na == 1
% Computation when Na = 1
A = [0 real(poly(p))];
else
% Computation when Na is odd and > 1
A = zeros(K+1,3);
for n=1:2:2*K
Arow = p(n:1:n+1,:);
Arow = poly(Arow);
A(fix((n+1)/2),:) = real(Arow);
end
A(K+1,:) = [0 real(poly(p(Na)))];
end
% Numerator second-order sections:
z = cplxpair(roots(b)); K = floor(Nb/2);
if Nb == 0
% Computation when Nb = 0
B = [0 0 poly(z)];
elseif K*2 == Nb
% Computation when Nb is even
B = zeros(K,3);
for n=1:2:Nb
Brow = z(n:1:n+1,:);
Brow = poly(Brow);
B(fix((n+1)/2),:) = real(Brow);
end
elseif Nb == 1
% Computation when Nb = 1
B = [0 real(poly(z))];
else
% Computation when Nb is odd and > 1
B = zeros(K+1,3);
for n=1:2:2*K
Brow = z(n:1:n+1,:);
Brow = poly(Brow);
B(fix((n+1)/2),:) = real(Brow);
end
B(K+1,:) = [0 real(poly(z(Nb)))];
end
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

390
Chapter 8
IIR FILTER DESIGN
□
EXAMPLE 8.2
Design a third-order Butterworth analog prototype ﬁlter with Ωc = 0.5 given
in Example 8.1.
Solution
MATLAB script:
>> N = 3; OmegaC = 0.5;
[b,a] = u_buttap(N,OmegaC);
>> [C,B,A] = sdir2cas(b,a)
C = 0.1250
B = 0
0
1
A = 1.0000
0.5000
0.2500
0
1.0000
0.5000
The cascade form coeﬃcients agree with those in Example 8.1.
□
8.3.3 DESIGN EQUATIONS
The analog lowpass ﬁlter is speciﬁed by the parameters Ωp, Rp, Ωs, and
As. Therefore, the essence of the design in the case of Butterworth ﬁl-
ter is to obtain the order N and the cutoﬀfrequency Ωc, given these
speciﬁcations. We want
• at Ω= Ωp, −10 log10 |Ha(jΩ)|2 = Rp, or
−10 log10
⎛
⎜
⎜
⎜
⎝
1
1 +
Ωp
Ωc
2N
⎞
⎟
⎟
⎟
⎠= Rp
and
• at Ω= Ωs, −10 log10 |Ha(jΩ)|2 = As, or
−10 log10
⎛
⎜
⎜
⎜
⎝
1
1 +
Ωs
Ωc
2N
⎞
⎟
⎟
⎟
⎠= As
Solving these two equations for N and Ωc, we have
N = log10

10Rp/10 −1

/

10As/10 −1

2 log10 (Ωp/Ωs)
In general, N will not be an integer. Since we want N to be an integer,
we must choose
N =
>
log10

10Rp/10 −1

/

10As/10 −1

2 log10 (Ωp/Ωs)
?
(8.49)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
391
where the operation ⌈x⌉means “choose the smallest integer larger than
x”—for example, ⌈4.5⌉= 5. Since the actual N chosen is larger than re-
quired, speciﬁcations can be either met or exceeded either at Ωp or at Ωs.
To satisfy the speciﬁcations exactly at Ωp,
Ωc =
Ωp
2N
4
10Rp/10 −1

(8.50)
or, to satisfy the speciﬁcations exactly at Ωs,
Ωc =
Ωs
2N
4
10As/10 −1

(8.51)
□
EXAMPLE 8.3
Design a lowpass Butterworth ﬁlter to satisfy the following speciﬁcations.
Passband cutoﬀ: Ωp = 0.2π ; Passband ripple: Rp = 7 dB
Stopband cutoﬀ: Ωs = 0.3π ; Stopband ripple: As = 16 dB
Solution
From (8.49),
N =
>
log10

100.7 −1
/ 
101.6 −1
2 log10 (0.2π/0.3π)
?
= ⌈2.79⌉= 3
To satisfy the speciﬁcations exactly at Ωp, from (8.50) we obtain
Ωc =
0.2π
67
(100.7 −1)
= 0.4985
To satisfy speciﬁcations exactly at Ωs, from (8.51) we obtain
Ωc =
0.3π
67
(101.6 −1)
= 0.5122
Now we can choose any Ωc between the above two numbers. Let us choose
Ωc = 0.5. We have to design a Butterworth ﬁlter with N = 3 and Ωc = 0.5,
which we did in Example 8.1. Hence
Ha(jΩ) =
0.125
(s + 0.5) (s2 + 0.5s + 0.25)
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

392
Chapter 8
IIR FILTER DESIGN
8.3.4 MATLAB IMPLEMENTATION
The preceding design procedure can be implemented in MATLAB as a
simple function. Using the U buttap function, we provide the afd butt
function to design an analog Butterworth lowpass ﬁlter, given its speciﬁ-
cations. This function uses (8.50).
function [b,a] = afd_butt(Wp,Ws,Rp,As);
% Analog lowpass filter design: Butterworth
% -----------------------------------------
% [b,a] = afd_butt(Wp,Ws,Rp,As);
%
b = numerator coefficients of Ha(s)
%
a = denominator coefficients of Ha(s)
% Wp = passband-edge frequency in rad/sec; Wp > 0
% Ws = stopband-edge frequency in rad/sec; Ws > Wp > 0
% Rp = passband ripple in +dB; (Rp > 0)
% As = stopband attenuation in +dB; (As > 0)
%
if Wp <= 0
error(’Passband edge must be larger than 0’)
end
if Ws <= Wp
error(’Stopband edge must be larger than Passband edge’)
end
if (Rp <= 0) | (As < 0)
error(’PB ripple and/or SB attenuation must be larger than 0’)
end
N = ceil((log10((10ˆ(Rp/10)-1)/(10ˆ(As/10)-1)))/(2*log10(Wp/Ws)));
fprintf(’\n*** Butterworth Filter Order = %2.0f \n’,N)
OmegaC = Wp/((10ˆ(Rp/10)-1)ˆ(1/(2*N)));
[b,a]=u_buttap(N,OmegaC);
To display the frequency-domain plots of analog ﬁlters, we provide a
function called freqs m, which is a modiﬁed version of a function freqs
provided by MATLAB. This function computes the magnitude response
in absolute as well as in relative dB scale and the phase response. This
function is similar to the freqz m function discussed earlier. One main
diﬀerence between them is that in the freqs m function the responses are
computed up to a maximum frequency Ωmax.
function [db,mag,pha,w] = freqs_m(b,a,wmax);
% Computation of s-domain frequency response: Modified version
% ------------------------------------------------------------
% [db,mag,pha,w] = freqs_m(b,a,wmax);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
393
%
db = relative magnitude in db over [0 to wmax]
%
mag = absolute magnitude over [0 to wmax]
%
pha = phase response in radians over [0 to wmax]
%
w = array of 500 frequency samples between [0 to wmax]
%
b = numerator polynomial coefficents of Ha(s)
%
a = denominator polynomial coefficents of Ha(s)
% wmax = maximum frequency in rad/sec over which response is desired
%
w = [0:1:500]*wmax/500;
H = freqs(b,a,w);
mag = abs(H);
db = 20*log10((mag+eps)/max(mag));
pha = angle(H);
The impulse response ha(t) of the analog ﬁlter is computed using
MATLAB’s impulse function.
□
EXAMPLE 8.4
Design the analog Butterworth lowpass ﬁlter speciﬁed in Example 8.3 using
MATLAB.
Solution
MATLAB script:
>> Wp = 0.2*pi; Ws = 0.3*pi; Rp = 7; As = 16;
>> Ripple = 10 ˆ (-Rp/20); Attn = 10 ˆ (-As/20);
>> % Analog filter design:
>> [b,a] = afd_butt(Wp,Ws,Rp,As);
*** Butterworth Filter Order =
3
>> % Calculation of second-order sections:
>> [C,B,A] = sdir2cas(b,a)
C = 0.1238
B = 0
0
1
A = 1.0000
0.4985
0.2485
0
1.0000
0.4985
>> % Calculation of frequency response:
>> [db,mag,pha,w] = freqs_m(b,a,0.5*pi);
>> % Calculation of impulse response:
>> [ha,x,t] = impulse(b,a);
The system function is given by
Ha(s) =
0.1238
(s2 + 0.4985s + 0.2485) (s + 0.4985)
This Ha(s) is slightly diﬀerent from the one in Example 8.3 because in that
example we used Ωc = 0.5, while in the afd butt function Ωc is chosen to
satisfy the speciﬁcations at Ωp. The ﬁlter plots are shown in Figure 8.15.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

394
Chapter 8
IIR FILTER DESIGN
0
0.2
0.3
0.5
0
0.1585
0.4467
1
Magnitude Response
Analog Frequency in π Units
|H|
0
0.2
0.3
0.5
30
16
 7
 0
Magnitude in dB
Analog Frequency in π Units
Decibels
0
0.2
0.3
0.5
−1
−0.5
0
0.5
1
Phase Response
Analog Frequency in π Units
Radians in π Units
0
10
20
0
0.05
0.1
0.15
0.2
Impulse Response
Time in Seconds
ha(t)
FIGURE 8.15
Butterworth analog ﬁlter in Example 8.4
8.3.5 CHEBYSHEV LOWPASS FILTERS
There are two types of Chebyshev ﬁlters. The Chebyshev-I ﬁlters have
equiripple response in the passband, while the Chebyshev-II ﬁlters have
equiripple response in the stopband. Butterworth ﬁlters have monotonic
response in both bands. Recall our discussions regarding equiripple FIR
ﬁlters. We noted that by choosing a ﬁlter that has an equiripple rather
than a monotonic behavior, we can obtain a lower-order ﬁlter. Therefore,
Chebyshev ﬁlters provide lower order than Butterworth ﬁlters for the
same speciﬁcations.
The magnitude-squared response of a Chebyshev-I ﬁlter is
|Ha(jΩ)|2 =
1
1 + ϵ2T 2
N
 Ω
Ωc

(8.52)
where N is the order of the ﬁlter, ϵ is the passband ripple factor, which is
related to Rp, and TN (x) is the Nth-order Chebyshev polynomial given by
TN(x) =

cos

N cos−1(x)

, 0 ≤x ≤1
cosh

cosh−1(x)

, 1 < x < ∞
where x = Ω
Ωc
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
395
The equiripple response of the Chebyshev ﬁlters is due to this polynomial
TN(x). Its key properties are (a) for 0 < x < 1, TN(x) oscillates be-
tween −1 and 1, and (b) for 1 < x < ∞, TN(x) increases monotonically
to ∞.
There are two possible shapes of |Ha(jΩ)|2, one for N odd and one for
N even as shown here. Note that x = Ω/Ωc is the normalized frequency.
1
0
1
A 2
x =
|Ha(j Ω)|2
N Odd
1
1
N Even
1
0
1
A 2
|Ha(j Ω)|2
Ωr
Ωc
Ωr
Ωc
Ω
Ωc
x = Ω
Ωc
1
1 + 
1
1 + 
From these two response plots we observe the following properties:
• At x = 0 (or Ω= 0),
|Ha(j0)|2 = 1
for N odd,
|Ha(j0)|2 =
1
1 + ϵ2
for N even.
• At x = 1 (or Ω= Ωc),
|Ha(j1)|2 =
1
1 + ϵ2
for all N.
• For 0 ≤x ≤1 (or 0 ≤Ω≤Ωc), |Ha(jx)|2 oscillates between 1 and
1
1 + ϵ2 .
• For x > 1 (or Ω> Ωc), |Ha(jx)|2 decreases monotonically to 0.
• At x = Ωr, |Ha(jx)|2 = 1
A2 .
To determine a causal and stable Ha(s), we must ﬁnd the poles of
Ha(s)Ha(−s) and select the left half-plane poles for Ha(s). The poles of
Ha(s)Ha(−s) are obtained by ﬁnding the roots of
1 + ϵ2T 2
N
 s
jΩc

The solution of this equation is tedious if not diﬃcult to obtain. It can be
shown that if pk = σk + jΩk,
k = 0, . . . , N −1 are the (left half-plane)
roots of these polynomial, then
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

396
Chapter 8
IIR FILTER DESIGN
σk = (aΩc) cos
π
2 + (2k + 1)π
2N

Ωk = (bΩc) sin
π
2 + (2k + 1)π
2N

k = 0, . . . , N −1
(8.53)
where
a = 1
2

N√α −
N7
1/α

,
b = 1
2

N√α +
N7
1/α

,
and
α = 1
ϵ +

1 + 1
ϵ2
(8.54)
These roots fall on an ellipse with major axis bΩc and minor axis aΩc.
Now the system function is given by
Ha(s) =
K
,
k
(s −pk)
(8.55)
where K is a normalizing factor chosen to make
Ha(j0) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1,
N odd,
1
√
1 + ϵ2 , N even
(8.56)
8.3.6 MATLAB IMPLEMENTATION
MATLAB provides a function called [z,p,k]=cheb1ap(N,Rp) to design
a normalized Chebyshev-I analog prototype ﬁlter of order N and pass-
band ripple Rp and that returns zeros in z array, poles in p array, and
the gain value k. We need an unnormalized Chebyshev-I ﬁlter with arbi-
trary Ωc. This is achieved by scaling the array p of the normalized ﬁlter
by Ωc. Similar to the Butterworth prototype, this ﬁlter has no zeros.
The new gain k is determined using (8.56), which is achieved by scaling
the old k by the ratio of the unnormalized to the normalized denom-
inator polynomials evaluated at s = 0. In the following function, called
U chb1ap(N,Rp,Omegac), we design an unnormalized Chebyshev-I analog
prototype ﬁlter that returns Ha(s) in the direct form.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
397
function [b,a] = u_chb1ap(N,Rp,Omegac);
% Unnormalized Chebyshev-1 analog lowpass filter prototype
% --------------------------------------------------------
% [b,a] = u_chb1ap(N,Rp,Omegac);
%
b = numerator polynomial coefficients
%
a = denominator polynomial coefficients
%
N = order of the elliptic filter
%
Rp = passband ripple in dB; Rp > 0
% Omegac = cutoff frequency in radians/sec
%
[z,p,k] = cheb1ap(N,Rp);
a = real(poly(p));
aNn = a(N+1);
p = p*Omegac;
a = real(poly(p));
aNu = a(N+1);
k = k*aNu/aNn;
b0 = k;
B = real(poly(z));
b = k*B;
8.3.7 DESIGN EQUATIONS
Given Ωp, Ωs, Rp, and AS, three parameters are required to determine
a Chebyshev-I ﬁlter: ϵ, Ωc, and N. From equations (8.3) and (8.4), we
obtain
ϵ =
7
100.1Rp −1
and
A = 10As/20
From these properties, we have
Ωc = Ωp
and
Ωr = Ωs
Ωp
(8.57)
The order N is given by
g =
7
(A2 −1) /ϵ2
(8.58)
N =
⎡
⎢⎢⎢
log10

g +
7
g2 −1

log10

Ωr +
7
Ω2r −1

⎤
⎥⎥⎥
(8.59)
Now using (8.54), (8.53), and (8.55), we can determine Ha(s).
□
EXAMPLE 8.5
Design a lowpass Chebyshev-I ﬁlter to satisfy the following speciﬁcations.
Passband cutoﬀ: Ωp = 0.2π ; Passband ripple: Rp = 1 dB
Stopband cutoﬀ: Ωs = 0.3π ; Stopband ripple: As = 16 dB
Solution
First, compute the necessary parameters.
ϵ =
√
100.1(1) −1 = 0.5088
A = 1016/20 = 6.3096
Ωc = Ωp = 0.2π
Ωr = 0.3π
0.2π = 1.5
g =
7
(A2 −1) /ϵ2 = 12.2429
N = 4
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

398
Chapter 8
IIR FILTER DESIGN
Now we can determine Ha(s).
α = 1
ϵ +

1 + 1
ϵ2 = 4.1702
a = 0.5

N√α −
N7
1/α

= 0.3646
b = 0.5

N√α +
N7
1/α

= 1.0644
There are four poles for Ha(s):
p0,3 = (aΩc) cos
π
2 + π
8

± (bΩc) sin
π
2 + π
8

= −0.0877 ± j0.6179
p1,2 = (aΩc) cos
π
2 + 3π
8

± (bΩc) sin
π
2 + 3π
8

= −0.2117 ± j0.2559
Hence
Ha(s) =
K
3,
k=0
(s −pk)
=
0.03829
(
)&
'
0.89125 × .1103 × .3895
(s2 + 0.1754s + 0.3895) (s2 + 0.4234s + 0.1103)
Note that the numerator is such that
Ha(j0) =
1
√
1 + ϵ2 = 0.89125
□
8.3.8 MATLAB IMPLEMENTATION
Using the U chb1ap function, we provide a function called afd chb1 to
design an analog Chebyshev-II lowpass ﬁlter, given its speciﬁcations. This
is shown below and uses the procedure described in Example 8.5.
function [b,a] = afd_chb1(Wp,Ws,Rp,As);
% Analog lowpass filter design: Chebyshev-1
% -----------------------------------------
% [b,a] = afd_chb1(Wp,Ws,Rp,As);
%
b = numerator coefficients of Ha(s)
%
a = denominator coefficients of Ha(s)
% Wp = passband-edge frequency in rad/sec; Wp > 0
% Ws = stopband-edge frequency in rad/sec; Ws > Wp > 0
% Rp = passband ripple in +dB; (Rp > 0)
% As = stopband attenuation in +dB; (As > 0)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
399
%
if Wp <= 0
error(’Passband edge must be larger than 0’)
end
if Ws <= Wp
error(’Stopband edge must be larger than Passband edge’)
end
if (Rp <= 0) | (As < 0)
error(’PB ripple and/or SB attenuation must be larger than 0’)
end
ep = sqrt(10ˆ(Rp/10)-1);
A = 10ˆ(As/20);
OmegaC = Wp;
OmegaR = Ws/Wp;
g = sqrt(A*A-1)/ep;
N = ceil(log10(g+sqrt(g*g-1))/log10(OmegaR+sqrt(OmegaR*OmegaR-1)));
fprintf(’\n*** Chebyshev-1 Filter Order = %2.0f \n’,N)
[b,a]=u_chb1ap(N,Rp,OmegaC);
□
EXAMPLE 8.6
Design the analog Chebyshev-I lowpass ﬁlter given in Example 8.5 using
MATLAB.
Solution
MATLAB script:
>> Wp = 0.2*pi; Ws = 0.3*pi; Rp = 1; As = 16;
>> Ripple = 10 ˆ (-Rp/20); Attn = 10 ˆ (-As/20);
>> % Analog filter design:
>> [b,a] = afd_chb1(Wp,Ws,Rp,As);
*** Chebyshev-1 Filter Order =
4
>> % Calculation of second-order sections:
>> [C,B,A] = sdir2cas(b,a)
C = 0.0383
B = 0
0
1
A = 1.0000
0.4233
0.1103
1.0000
0.1753
0.3895
>> % Calculation of frequency response:
>> [db,mag,pha,w] = freqs_m(b,a,0.5*pi);
>> % Calculation of impulse response:
>> [ha,x,t] = impulse(b,a);
The speciﬁcations are satisﬁed by a fourth-order Chebyshev-I ﬁlter whose sys-
tem function is
Ha(s) =
0.0383
(s2 + 4233s + 0.1103) (s2 + 0.1753s + 0.3895)
The ﬁlter plots are shown in Figure 8.16.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

400
Chapter 8
IIR FILTER DESIGN
0
0.2
0.3
0.5
0
0.1585
0.8913
1
Magnitude Response
Analog Frequency in π Units
|H|
0
0.2
0.3
0.5
30
16
 1
 0
Magnitude in dB
Analog Frequency in π Units
Decibels
0
0.2
0.3
0.5
−1
−0.5
0
0.5
1
Phase Response
Analog Frequency in π Units
Radians in π Units
0
20
40
60
−0.05
0
0.05
0.1
0.15
0.2
Impulse Response
Time in Seconds
ha(t)
FIGURE 8.16
Chebyshev-I analog ﬁlter in Example 8.6
A Chebyshev-II ﬁlter is related to the Chebyshev-I ﬁlter through a
simple transformation. It has a monotone passband and an equiripple
stopband, which implies that this ﬁlter has both poles and zeros in the
s-plane. Therefore, the group delay characteristics are better (and the
phase response more linear) in the passband than those of the Chebyshev-I
prototype. If we replace the term ϵ2T 2
N(Ω/Ωc) in (8.52) by its recipro-
cal and also the argument x = Ω/Ωc by its reciprocal, we obtain the
magnitude-squared response of Chebyshev-II as
|Ha(jΩ)|2 =
1
1 + [ϵ2T 2
N(Ωc/Ω)]−1
(8.60)
One approach to designing a Chebyshev-II ﬁlter is to design the corre-
sponding Chebyshev-I ﬁrst and then apply these transformations. We will
not discuss the details of this ﬁlter but will use a function from MATLAB
to design a Chebyshev-II ﬁlter.
8.3.9 MATLAB IMPLEMENTATION
MATLAB provides a function called [z,p,k]=cheb2ap(N,As) to design a
normalized Chebyshev-II analog prototype ﬁlter of order N and passband
ripple As and that returns zeros in z array, poles in p array, and the gain
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
401
value k. We need an unnormalized Chebyshev-I ﬁlter with arbitrary Ωc.
This is achieved by scaling the array p of the normalized ﬁlter by Ωc. Since
this ﬁlter has zeros, we also have to scale the array z by Ωc. The new gain
k is determined using (8.56), which is achieved by scaling the old k by the
ratio of the unnormalized to the normalized rational functions evaluated
at s = 0. In the following function, called U chb2ap(N,As,Omegac), we
design an unnormalized Chebyshev-II analog prototype ﬁlter that returns
Ha(s) in the direct form.
function [b,a] = u_chb2ap(N,As,Omegac);
% Unnormalized Chebyshev-2 analog lowpass filter prototype
% --------------------------------------------------------
% [b,a] = u_chb2ap(N,As,Omegac);
%
b = numerator polynomial coefficients
%
a = denominator polynomial coefficients
%
N = order of the Elliptic Filter
%
As = stopband Ripple in dB; As > 0
% Omegac = cutoff frequency in radians/sec
%
[z,p,k] = cheb2ap(N,As);
a = real(poly(p));
aNn = a(N+1);
p = p*Omegac;
a = real(poly(p));
aNu = a(N+1);
b = real(poly(z));
M = length(b);
bNn = b(M);
z = z*Omegac;
b = real(poly(z));
bNu = b(M);
k = k*(aNu*bNn)/(aNn*bNu);
b0 = k;
b = k*b;
The design equations for the Chebyshev-II prototype are similar to
those of the Chebyshev-I except that Ωc = Ωs since the ripples are in the
stopband. Therefore, we can develop a MATLAB function similar to the
afd chb1 function for the Chebyshev-II prototype.
function [b,a] = afd_chb2(Wp,Ws,Rp,As);
% Analog lowpass filter design: Chebyshev-2
% -----------------------------------------
% [b,a] = afd_chb2(Wp,Ws,Rp,As);
%
b = numerator coefficients of Ha(s)
%
a = denominator coefficients of Ha(s)
% Wp = passband-edge frequency in rad/sec; Wp > 0
% Ws = stopband-edge frequency in rad/sec; Ws > Wp > 0
% Rp = passband ripple in +dB; (Rp > 0)
% As = stopband attenuation in +dB; (As > 0)
%
if Wp <= 0
error(’Passband edge must be larger than 0’)
end
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

402
Chapter 8
IIR FILTER DESIGN
if Ws <= Wp
error(’Stopband edge must be larger than Passband edge’)
end
if (Rp <= 0) | (As < 0)
error(’PB ripple and/or SB attenuation must be larger than 0’)
end
ep = sqrt(10ˆ(Rp/10)-1);
A = 10ˆ(As/20);
OmegaC = Wp;
OmegaR = Ws/Wp;
g = sqrt(A*A-1)/ep;
N = ceil(log10(g+sqrt(g*g-1))/log10(OmegaR+sqrt(OmegaR*OmegaR-1)));
fprintf(’\n*** Chebyshev-2 Filter Order = %2.0f \n’,N)
[b,a]=u_chb2ap(N,As,Ws);
□
EXAMPLE 8.7
Design a Chebyshev-II analog lowpass ﬁlter to satisfy the speciﬁcations given
in Example 8.5:
Passband cutoﬀ: Ωp = 0.2π ; Passband ripple: Rp = 1 dB
Stopband cutoﬀ: Ωs = 0.3π ; Stopband ripple: As = 16 dB
Solution
MATLAB script:
>> Wp = 0.2*pi; Ws = 0.3*pi; Rp = 1; As = 16;
>> Ripple = 10 ˆ (-Rp/20); Attn = 10 ˆ (-As/20);
>> % Analog filter design:
>> [b,a] = afd_chb2(Wp,Ws,Rp,As);
*** Chebyshev-2 Filter Order =
4
>> % Calculation of second-order sections:
>> [C,B,A] = sdir2cas(b,a)
C = 0.1585
B = 1.0000
0
6.0654
1.0000
0
1.0407
A = 1.0000
1.9521
1.4747
1.0000
0.3719
0.6784
>> % Calculation of frequency response:
>> [db,mag,pha,w] = freqs_m(b,a,0.5*pi);
>> % Calculation of impulse response:
>> [ha,x,t] = impulse(b,a);
The speciﬁcations are satisﬁed by a fourth-order Chebyshev-II ﬁlter whose sys-
tem function is
Ha(s) =
0.1585 
s2 + 6.0654 
s2 + 1.0407
(s2 + 1.9521s + 1.4747) (s2 + 0.3719s + 0.6784)
The ﬁlter plots are shown in Figure 8.17.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
403
0
0.2
0.3
0.5
0
0.1585
0.8913
1
Magnitude Response
Analog Frequency in π Units
|H|
0
0.2
0.3
0.5
30
16
 1
 0
Magnitude in dB
Analog Frequency in π Units
Decibels
0
0.2
0.3
0.5
−1
−0.5
0
0.5
1
Phase Response
Analog Frequency in π Units
Radians in π Units
0
10
20
30
−0.3
−0.2
−0.1
0
0.1
0.2
Impulse Response
Time in Seconds
ha(t)
FIGURE 8.17
Chebyshev-II analog ﬁlter in Example 8.7
8.3.10 ELLIPTIC LOWPASS FILTERS
These ﬁlters exhibit equiripple behavior in the passband as well as in
the stopband. They are similar in magnitude response characteristics to
the FIR equiripple ﬁlters. Therefore, elliptic ﬁlters are optimum ﬁlters
in that they achieve the minimum order N for the given speciﬁcations
(or alternately, achieve the sharpest transition band for the given order
N). These ﬁlters, for obvious reasons, are very diﬃcult to analyze and,
therefore, to design. It is not possible to design them using simple tools,
and often programs or tables are needed to design them.
The magnitude-squared response of elliptic ﬁlters is given by
|Ha(jΩ)|2 =
1
1 + ϵ2U 2
N
 Ω
Ωc

(8.61)
where N is the order, ϵ is the passband ripple (which is related to Rp),
and UN(·) is the Nth-order Jacobian elliptic function. The analysis of
this function, even on a superﬁcial level, is beyond the scope of this book.
Note the similarity between the preceding response (8.61) and that of the
Chebyshev ﬁlters given by (8.52). Typical responses for odd and even N
are as follows:
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

404
Chapter 8
IIR FILTER DESIGN
1
0
1
A2
Ω
|Ha(j Ω)|2
N Odd
Ωc
N Even
1
0
1
A2
Ω
|Ha( j Ω)|2
Ωc
1
1 + 
1
1 + 
8.3.11 COMPUTATION OF FILTER ORDER N
Even though the analysis of (8.61) is diﬃcult, the order calculation for-
mula is very compact and is available in many textbooks [71, 79, 83]. It
is given by
N =
K(k)K
7
1 −k2
1

K (k1) K
√
1 −k2
(8.62)
where
k = Ωp
Ωs
,
k1 =
ϵ
√
A2 −1
and
K(x) =
 π/2
0
dθ
7
1 −x2 sin2 θ
is the complete elliptic integral of the ﬁrst kind. MATLAB provides the
function ellipke to numerically compute the above integral, which we
will use to compute N and to design elliptic ﬁlters.
8.3.12 MATLAB IMPLEMENTATION
MATLAB provides a function called [z,p,k]=ellipap(N,Rp,As) to de-
sign a normalized elliptic analog prototype ﬁlter of order N, passband
ripple Rp, and stopband attenuation As, and that returns zeros in z array,
poles in p array, and the gain value k. We need an unnormalized elliptic
ﬁlter with arbitrary Ωc. This is achieved by scaling the arrays p and z of
the normalized ﬁlter by Ωc and the gain k by the ratio of the unnormalized
to the normalized rational functions evaluated at s = 0. In the following
function, called U elipap(N,Rp,As,Omegac), we design an unnormalized
elliptic analog prototype ﬁlter that returns Ha(s) in the direct form.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Characteristics of Prototype Analog Filters
405
function [b,a] = u_elipap(N,Rp,As,Omegac);
% Unnormalized elliptic analog lowpass filter prototype
% -----------------------------------------------------
% [b,a] = u_elipap(N,Rp,As,Omegac);
%
b = numerator polynomial coefficients
%
a = denominator polynomial coefficients
%
N = order of the elliptic filter
%
Rp = passband ripple in dB; Rp > 0
%
As = stopband attenuation in dB; As > 0
% Omegac = cutoff frequency in radians/sec
%
[z,p,k] = ellipap(N,Rp,As);
a = real(poly(p));
aNn = a(N+1);
p = p*Omegac;
a = real(poly(p));
aNu = a(N+1);
b = real(poly(z));
M = length(b);
bNn = b(M);
z = z*Omegac;
b = real(poly(z));
bNu = b(M);
k = k*(aNu*bNn)/(aNn*bNu);
b0 = k;
b = k*b;
Using the U elipap function, we provide a function called afd elip
to design an analog elliptic lowpass ﬁlter, given its speciﬁcations. This
follows and uses the ﬁlter order computation formula given in (8.62).
function [b,a] = afd_elip(Wp,Ws,Rp,As);
% Analog lowpass filter design: Elliptic
% --------------------------------------
% [b,a] = afd_elip(Wp,Ws,Rp,As);
%
b = numerator coefficients of Ha(s)
%
a = denominator coefficients of Ha(s)
% Wp = passband-edge frequency in rad/sec; Wp > 0
% Ws = stopband-edge frequency in rad/sec; Ws > Wp > 0
% Rp = passband ripple in +dB; (Rp > 0)
% As = stopband attenuation in +dB; (As > 0)
%
if Wp <= 0
error(’Passband edge must be larger than 0’)
end
if Ws <= Wp
error(’Stopband edge must be larger than Passband edge’)
end
if (Rp <= 0) | (As < 0)
error(’PB ripple and/or SB attenuation must be larger than 0’)
end
ep = sqrt(10ˆ(Rp/10)-1);
A = 10ˆ(As/20);
OmegaC = Wp;
k = Wp/Ws;
k1 = ep/sqrt(A*A-1);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

406
Chapter 8
IIR FILTER DESIGN
capk = ellipke([k.ˆ2 1-k.ˆ2]); % Version 4.0 code
capk1 = ellipke([(k1 .ˆ2) 1-(k1 .ˆ2)]); % Version 4.0 code
N = ceil(capk(1)*capk1(2)/(capk(2)*capk1(1)));
fprintf(’\n*** Elliptic Filter Order = %2.0f \n’,N)
[b,a]=u_elipap(N,Rp,As,OmegaC);
□
EXAMPLE 8.8
Design an analog elliptic lowpass ﬁlter to satisfy the following speciﬁcations of
Example 8.5:
Ωp = 0.2π, Rp = 1 dB
Ωs = 0.3π, As = 16 dB
Solution
MATLAB script:
>> Wp = 0.2*pi; Ws = 0.3*pi; Rp = 1; As = 16;
>> Ripple = 10 ˆ (-Rp/20); Attn = 10 ˆ (-As/20);
>> % Analog filter design:
>> [b,a] = afd_elip(Wp,Ws,Rp,As);
*** Elliptic Filter Order =
3
>> % Calculation of second-order sections:
>> [C,B,A] = sdir2cas(b,a)
C = 0.2740
B = 1.0000
0
0.6641
A = 1.0000
0.1696
0.4102
0
1.0000
0.4435
>> % Calculation of frequency response:
>> [db,mag,pha,w] = freqs_m(b,a,0.5*pi);
>> % Calculation of impulse response:
>> [ha,x,t] = impulse(b,a);
The speciﬁcations are satisﬁed by a third-order elliptic ﬁlter whose system func-
tion is
Ha(s) =
0.274 
s2 + 0.6641
(s2 + 0.1696s + 0.4102) (s + 0.4435)
The ﬁlter plots are shown in Figure 8.18.
□
8.3.13 PHASE RESPONSES OF PROTOTYPE FILTERS
Elliptic ﬁlters provide optimal performance in the magnitude-squared re-
sponse but have highly nonlinear phase response in the passband (which is
undesirable in many applications). Even though we decided not to worry
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
407
0
0.2
0.3
0.5
0
0.1585
0.8913
1
Magnitude Response
Analog Frequency in π Units
|H|
0
0.2
0.3
0.5
30
16
 1
 0
Magnitude in dB
Analog Frequency in π Units
Decibels
0
0.2
0.3
0.5
−1
−0.5
0
0.5
1
Phase Response
Analog Frequency in π Units
Radians in π Units
0
20
40
60
0
0.1
0.2
Impulse Response
Time in Seconds
ha(t)
FIGURE 8.18
Elliptic analog lowpass ﬁlter in Example 8.8
about phase response in our designs, phase is still an important issue in the
overall system. At the other end of the performance scale are the Butter-
worth ﬁlters, which have maximally ﬂat magnitude response and require a
higher-order N (more poles) to achieve the same stopband speciﬁcation.
However, they exhibit a fairly linear phase response in their passband.
The Chebyshev ﬁlters have phase characteristics that lie somewhere in
between. Therefore, in practical applications, we do consider Butterworth
as well as Chebyshev ﬁlters, in addition to elliptic ﬁlters. The choice de-
pends on both the ﬁlter order (which inﬂuences processing speed and
implementation complexity) and the phase characteristics (which control
the distortion).
8.4 ANALOG-TO-DIGITAL FILTER TRANSFORMATIONS
After discussing diﬀerent approaches to the design of analog ﬁlters, we
are now ready to transform them into digital ﬁlters. These transfor-
mations are complex-valued mappings that are extensively studied in
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

408
Chapter 8
IIR FILTER DESIGN
the literature. These transformations are derived by preserving diﬀerent
aspects of analog and digital ﬁlters. If we want to preserve the shape
of the impulse response from analog to digital ﬁlter, then we obtain a
technique called impulse invariance transformation. If we want to con-
vert a diﬀerential equation representation into a corresponding diﬀerence
equation representation, then we obtain a finite difference approximation
technique. Numerous other techniques are also possible. One technique,
called step invariance, preserves the shape of the step response; this is
explored in Problem P8.24. Another technique that is similar to the
impulse invariance is the matched-z transformation, which matches the
pole-zero representation. It is described at the end of this section and is
explored in Problem P8.26. The most popular technique used in practice
is called a Bilinear transformation, which preserves the system function
representation from analog to digital domain. In this section, we will
study in detail impulse invariance and bilinear transformations, both of
which can be easily implemented in MATLAB.
8.4.1 IMPULSE INVARIANCE TRANSFORMATION
In this design method, we want the digital ﬁlter impulse response to look
“similar” to that of a frequency-selective analog ﬁlter. Hence we sample
ha(t) at some sampling interval T to obtain h(n); that is,
h(n) = ha(nT)
The parameter T is chosen so that the shape of ha(t) is “captured” by
the samples. Since this is a sampling operation, the analog and digital
frequencies are related by
ω = ΩT or ejω = ejΩT
Since z = ejω on the unit circle and s = jΩon the imaginary axis, we
have the following transformation from the s-plane to the z-plane:
z = esT
(8.63)
The system functions H(z) and Ha(s) are related through the frequency-
domain aliasing formula (3.27):
H(z) = 1
T
∞
	
k=−∞
Ha

s −j 2π
T k

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
409
Im {z}
Re {z}
jΩ
s
−3p/T
s-plane
z-plane
−p/T
Unit Circle
Many-to-One
Transformation
p/T
3p/T
esT = z
FIGURE 8.19
Complex-plane mapping in impulse invariance transformation
The complex plane transformation under the mapping (8.63) is shown in
Figure 8.19, from which we have the following observations.
1. Using σ = Re(s), we note that
σ < 0 maps into |z| < 1 (inside of the UC)
σ = 0 maps onto |z| = 1 (on the UC)
σ > 0 maps into |z| > 1 (outside of the UC)
2. All semi-inﬁnite strips (shown above) of width 2π/T map into |z| < 1.
Thus this mapping is not unique but a many-to-one mapping.
3. Since the entire left half of the s-plane maps into the unit circle, a
causal and stable analog ﬁlter maps into a causal and stable digital
ﬁlter.
4. If Ha(jΩ) = Ha(jω/T) = 0 for |Ω| ≥π/T, then
H(ejω) = 1
T Ha(jω/T),
|ω| ≤π
and there will be no aliasing. However, no analog ﬁlter of ﬁnite order
can be exactly band-limited. Therefore some aliasing error will occur
in this design procedure, and hence the sampling interval T plays a
minor role in this design method.
8.4.2 DESIGN PROCEDURE
Given the digital lowpass ﬁlter speciﬁcations ωp, ωs, Rp, and As, we want
to determine H(z) by ﬁrst designing an equivalent analog ﬁlter and then
mapping it into the desired digital ﬁlter. The steps required for this pro-
cedure are as follows.
1. Choose T and determine the analog frequencies
Ωp = ωp
Tp
and
Ωs = ωs
T
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

410
Chapter 8
IIR FILTER DESIGN
2. Design an analog ﬁlter Ha(s) using the speciﬁcations Ωp, Ωs, Rp, and
As. This can be done using any one of the three (Butterworth, Cheby-
shev, or elliptic) prototypes of the previous section.
3. Using partial fraction expansion, expand Ha(s) into
Ha(s) =
N
	
k=1
Rk
s −pk
4. Now transform analog poles {pk} into digital poles {epkT } to obtain
the digital ﬁlter:
H(z) =
N
	
k=1
Rk
1 −epkT z−1
(8.64)
□
EXAMPLE 8.9
Transform
Ha(s) =
s + 1
s2 + 5s + 6
into a digital ﬁlter H(z) using the impulse invariance technique in which
T = 0.1.
Solution
We ﬁrst expand Ha(s) using partial fraction expansion:
Ha(s) =
s + 1
s2 + 5s + 6 =
2
s + 3 −
1
s + 2
The poles are at p1 = −3 and p2 = −2. Then from (8.64) and using T = 0.1,
we obtain
H(z) =
2
1 −e−3T z−1 −
1
1 −e−2T z−1 =
1 −0.8966z−1
1 −1.5595z−1 + 0.6065z−2
It is easy to develop a MATLAB function to implement the impulse invari-
ance mapping. Given a rational function description of Ha(s), we can use the
residue function to obtain its pole-zero description. Then each analog pole is
mapped into a digital pole using (8.63). Finally, the residuez function can be
used to convert H(z) into rational function form. This procedure is given in the
function imp invr.
function [b,a] = imp_invr(c,d,T)
% Impulse invariance transformation from analog to digital filter
% ---------------------------------------------------------------
% [b,a] = imp_invr(c,d,T)
%
b = numerator polynomial in zˆ(-1) of the digital filter
%
a = denominator polynomial in zˆ(-1) of the digital filter
%
c = numerator polynomial in s of the analog filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
411
%
d = denominator polynomial in s of the analog filter
%
T = sampling (transformation) parameter
%
[R,p,k] = residue(c,d);
p = exp(p*T);
[b,a] = residuez(R,p,k);
b = real(b’); a = real(a’);
A similar function called impinvar is available in the SP toolbox of MATLAB.
□
□
EXAMPLE 8.10
We demonstrate the use of the imp invr function on the system function from
Example 8.9.
Solution
MATLAB script:
>> c = [1,1]; d = [1,5,6]; T = 0.1;
>> [b,a] = imp_invr(c,d,T)
b =
1.0000
-0.8966
a =
1.0000
-1.5595
0.6065
The digital ﬁlter is
H(z) =
1 −0.8966z−1
1 −1.5595z−1 + 0.6065z−2
as expected. In Figure 8.20, we show the impulse responses and the magnitude
responses (plotted up to the sampling frequency 1/T) of the analog and the
resulting digital ﬁlter. Clearly, the aliasing in the frequency domain is evident.
□
In the next several examples, we illustrate the impulse invariance
design procedure on all three prototypes.
□
EXAMPLE 8.11
Design a lowpass digital ﬁlter using a Butterworth prototype to satisfy
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Solution
The design procedure is described in the following MATLAB script.
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in Hz
>> ws = 0.3*pi;
% Digital stopband freq in Hz
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

412
Chapter 8
IIR FILTER DESIGN
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
Time in Seconds
Amplitude
Impulse Responses
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
Frequency in Hz
Magnitude Responses
Magnitude
Analog Filter
Digital Filter
FIGURE 8.20
Impulse and frequency response plots in Example 8.10
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1;
% Set T=1
>> OmegaP = wp / T;
% Prototype passband freq
>> OmegaS = ws / T;
% Prototype stopband freq
>> % Analog Butterworth prototype filter calculation:
>> [cs,ds] = afd_butt(OmegaP,OmegaS,Rp,As);
*** Butterworth Filter Order =
6
>> % Impulse invariance transformation:
>> [b,a] = imp_invr(cs,ds,T);
[C,B,A] = dir2par(b,a)
C =
[]
B = 1.8557
-0.6304
-2.1428
1.1454
0.2871
-0.4466
A = 1.0000
-0.9973
0.2570
1.0000
-1.0691
0.3699
1.0000
-1.2972
0.6949
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
413
0
0.2 0.3
1
0
0.1778
0.8913
1
Magnitude Response
Frequency in π Units
|H|
0
0.2 0.3
1
15
 1
 0
Magnitude in dB
Frequency in π Units
Decibels
0
0.2 0.3
1
−1
0
1
Phase Response
Frequency in π Units
Radians in π Units
0
0.2 0.3
1
0
2
4
6
8
10
Group Delay
Frequency in π Units
Samples
FIGURE 8.21
Digital Butterworth lowpass ﬁlter using impulse invariance design
The desired ﬁlter is a sixth-order Butterworth ﬁlter whose system function H(z)
is given in the parallel form
H(z) =
1.8587 −0.6304z−1
1 −0.9973z−1 + 0.257z−2 +
−2.1428 + 1.1454z−1
1 −1.0691z−1 + 0.3699z−2
+
0.2871 −0.4463z−1
1 −1.2972z−1 + 0.6449z−2
The frequency response plots are given in Figure 8.21.
□
□
EXAMPLE 8.12
Design a lowpass digital ﬁlter using a Chebyshev-I prototype to satisfy
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

414
Chapter 8
IIR FILTER DESIGN
Solution
The design procedure is described in the following MATLAB script.
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1;
% Set T=1
>> OmegaP = wp / T;
% Prototype passband freq
>> OmegaS = ws / T;
% Prototype stopband freq
>> % Analog Chebyshev-1 prototype filter calculation:
>> [cs,ds] = afd_chb1(OmegaP,OmegaS,Rp,As);
*** Chebyshev-1 Filter Order =
4
>> % Impulse invariance transformation:
>> [b,a] = imp_invr(cs,ds,T);
[C,B,A] = dir2par(b,a)
C =
[]
B =-0.0833
-0.0246
0.0833
0.0239
A = 1.0000
-1.4934
0.8392
1.0000
-1.5658
0.6549
The desired ﬁlter is a fourth-order Chebyshev-I ﬁlter whose system function
H(z) is
H(z) =
−0.0833 −0.0246z−1
1 −1.4934z−1 + 0.8392z−2 +
−0.0833 + 0.0239z−1
1 −1.5658z−1 + 0.6549z−2
The frequency response plots are given in Figure 8.22.
□
□
EXAMPLE 8.13
Design a lowpass digital ﬁlter using a Chebyshev-II prototype to satisfy
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Solution
Recall that the Chebyshev-II ﬁlter is equiripple in the stopband. This means
that this analog ﬁlter has a response that does not go to zero at high frequencies
in the stopband. Therefore, after impulse invariance transformation, the aliasing
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
415
0
0.20.3
1
0
0.1778
0.8913
1
Magnitude Response
Frequency in π Units
|H|
0
0.20.3
1
15
 1
 0
Magnitude in dB
Frequency in π Units
Decibels
0
0.20.3
1
−1
0
1
Phase Response
Frequency in π Units
Radians π Units
0
0.20.3
1
0
5
10
15
Group Delay
Frequency in π Units
Samples
FIGURE 8.22
Digital Chebyshev-I lowpass ﬁlter using impulse invariance design
eﬀect will be signiﬁcant; this can degrade the passband response. The MATLAB
script follows:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1;
% Set T=1
>> OmegaP = wp / T;
% Prototype passband freq
>> OmegaS = ws / T;
% Prototype stopband freq
>> % Analog Chebyshev-1 prototype filter calculation:
>> [cs,ds] = afd_chb2(OmegaP,OmegaS,Rp,As);
*** Chebyshev-2 Filter Order =
4
>> % Impulse invariance transformation:
>> [b,a] = imp_invr(cs,ds,T);
[C,B,A] = dir2par(b,a);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

416
Chapter 8
IIR FILTER DESIGN
0
0.20.3
1
0
0.1778
0.8913
1
Magnitude Response
|H|
0
0.20.3
1
15
 1
 0
Magnitude in dB
Decibels
0
0.20.3
1
−1
0
1
Phase Response
Radians in π Units
0
0.20.3
1
0
5
10
15
Group Delay
Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.23
Digital Chebyshev-II lowpass ﬁlter using impulse invariance
design
From the frequency response plots in Figure 8.23, we clearly observe the pass-
band as well as stopband degradation. Hence the impulse invariance design
technique has failed to produce a desired digital ﬁlter.
□
□
EXAMPLE 8.14
Design a lowpass digital ﬁlter using an elliptic prototype to satisfy
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Solution
The elliptic ﬁlter is equiripple in both bands. Hence this situation is similar to
that of the Chebyshev-II ﬁlter, and we should not expect a good digital ﬁlter.
MATLAB script:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
417
0
0.20.3
1
0
0.1778
0.8913
1
Magnitude Response
|H|
0
0.20.3
1
15
 1
 0
Magnitude in dB
Decibels
0
0.20.3
1
−1
0
1
Phase Response
Radians in π Units
0
0.20.3
1
0
5
10
15
Group Delay
Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.24
Digital elliptic lowpass ﬁlter using impulse invariance design
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1;
% Set T=1
>> OmegaP = wp / T;
% Prototype passband freq
>> OmegaS = ws / T;
% Prototype stopband freq
>> % Analog elliptic prototype filter calculation:
>> [cs,ds] = afd_elip(OmegaP,OmegaS,Rp,As);
*** Elliptic Filter Order =
3
>> % Impulse invariance transformation:
>> [b,a] = imp_invr(cs,ds,T);
[C,B,A] = dir2par(b,a);
From the frequency response plots in Figure 8.24, we clearly observe that once
again the impulse invariance design technique has failed.
□
The advantages of the impulse invariance mapping are that it is a
stable design and that the frequencies Ωand ω are linearly related. But
the disadvantage is that we should expect some aliasing of the ana-
log frequency response, and in some cases this aliasing is intolerable.
Consequently, this design method is useful only when the analog ﬁlter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

418
Chapter 8
IIR FILTER DESIGN
is essentially band-limited to a lowpass or bandpass ﬁlter in which there
are no oscillations in the stopband.
8.4.3 BILINEAR TRANSFORMATION
This mapping is the best transformation method; it involves a well-known
function given by
s = 2
T
1 −z−1
1 + z−1 =⇒z = 1 + sT/2
1 −sT/2
(8.65)
where T is a parameter. Another name for this transformation is the linear
fractional transformation because when cleared of fractions, we obtain
T
2 sz + T
2 s −z + 1 = 0
which is linear in each variable if the other is ﬁxed, or bilinear in s and z.
The complex plane mapping under (8.65) is shown in Figure 8.25, from
which we have the following observations.
1. Using s = σ + jΩin (8.65), we obtain
z =

1 + σT
2 + j ΩT
2
 @ 
1 −σT
2 −j ΩT
2

(8.66)
Hence
σ < 0 =⇒|z| =

1 + σT
2 + j ΩT
2
1 −σT
2 −j ΩT
2
 < 1
σ = 0 =⇒|z| =

1 + j ΩT
2
1 −j ΩT
2

= 1
σ > 0 =⇒|z| =

1 + σT
2 + j ΩT
2
1 −σT
2 −j ΩT
2
 > 1
Im {z}
Re {z}
jΩ
σ
s-plane
z-plane
Unit Circle
One-to-One
Transformation
 =  z
1 + (sT/2)
1 − (sT/2)
FIGURE 8.25
Complex-plane mapping in bilinear transformation
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
419
2. The entire left half-plane maps into the inside of the unit circle. Hence
this is a stable transformation.
3. The imaginary axis maps onto the unit circle in a one-to-one fashion.
Hence there is no aliasing in the frequency domain.
Substituting σ = 0 in (8.66), we obtain
z = 1 + j ΩT
2
1 −j ΩT
2
= ejω
since the magnitude is 1. Solving for ω as a function of Ω, we obtain
ω = 2 tan−1
ΩT
2

or
Ω= 2
T tan
ω
2

(8.67)
This shows that Ωis nonlinearly related to (or warped into) ω but that
there is no aliasing. Hence, in (8.67), we will say that ω is prewarped
into Ω.
□
EXAMPLE 8.15
Transform Ha(s) =
s + 1
s2 + 5s + 6 into a digital ﬁlter using the bilinear transfor-
mation. Choose T = 1.
Solution
Using (8.65), we obtain
H(z) = Ha

2
T
1 −z−1
1 + z−1

T =1

= Ha

21 −z−1
1 + z−1

=
21 −z−1
1 + z−1 + 1

21 −z−1
1 + z−1
2
+ 5

21 −z−1
1 + z−1

+ 6
Simplifying,
H(z) = 3 + 2z−1 −z−2
20 + 4z−1
= 0.15 + 0.1z−1 −0.05z−2
1 + 0.2z−1
□
MATLAB provides a function called bilinear to implement this
mapping. Its invocation is similar to the imp invr function, but it also
takes several forms for diﬀerent input-output quantities. The SP tool-
box manual should be consulted for more details. Its use is shown in the
following example.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

420
Chapter 8
IIR FILTER DESIGN
□
EXAMPLE 8.16
Transform the system function Ha(s) in Example 8.15 using the bilinear func-
tion.
Solution
MATLAB script:
>> c = [1,1]; d = [1,5,6]; T = 1; Fs = 1/T;
>> [b,a] = bilinear(c,d,Fs)
b = 0.1500
0.1000
-0.0500
a = 1.0000
0.2000
0.0000
The ﬁlter is
H(z) = 0.15 + 0.1z−1 −0.05z−2
1 + 0.2z−1
as before.
□
8.4.4 DESIGN PROCEDURE
Given digital ﬁlter speciﬁcations ωp, ωs, Rp, and As, we want to determine
H(z). The design steps in this procedure are the following.
1. Choose a value for T. This is arbitrary, and we may set T = 1.
2. Prewarp the cutoﬀfrequencies ωp and ωs; that is, calculate Ωp and Ωs
using (8.67):
Ωp = 2
T tan
ωp
2

,
Ωs = 2
T tan
ωs
2

(8.68)
3. Design an analog ﬁlter Ha(s) to meet the speciﬁcations Ωp, Ωs, Rp,
and As. We have already described how to do this in the previous
section.
4. Finally, set
H(z) = Ha
 2
T
1 −z−1
1 + z−1

and simplify to obtain H(z) as a rational function in z−1.
In the next several examples, we demonstrate this design procedure
on our analog prototype ﬁlters.
□
EXAMPLE 8.17
Design the digital Butterworth ﬁlter of Example 8.11. The speciﬁcations are
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
421
Solution
MATLAB script:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1; Fs = 1/T;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog Butterworth prototype filter calculation:
>> [cs,ds] = afd_butt(OmegaP,OmegaS,Rp,As);
*** Butterworth Filter Order =
6
>> % Bilinear transformation:
>> [b,a] = bilinear(cs,ds,Fs);
[C,B,A] = dir2cas(b,a)
C = 5.7969e-004
B = 1.0000
2.0183
1.0186
1.0000
1.9814
0.9817
1.0000
2.0004
1.0000
A = 1.0000
-0.9459
0.2342
1.0000
-1.0541
0.3753
1.0000
-1.3143
0.7149
The desired ﬁlter is once again a sixth-order ﬁlter and has six zeros. Since the
sixth-order zero of Ha(s) at s = −∞is mapped to z = −1, these zeros should be
at z = −1. Due to the ﬁnite precision of MATLAB, these zeros are not exactly
at z = −1. Hence the system function should be
H(z) =
0.00057969 
1 + z−16
(1 −0.9459z−1 + 0.2342z−2) (1 −1.0541z−1 + 0.3753z−2) (1 −1.3143z−1 + 0.7149z−2)
The frequency response plots are given in Figure 8.26. Comparing these plots
with those in Figure 8.21, we observe that these two designs are very similar.
□
□
EXAMPLE 8.18
Design the digital Chebyshev-I ﬁlter of Example 8.12. The speciﬁcations are
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

422
Chapter 8
IIR FILTER DESIGN
0
0.20.3
1
0
0.1778
0.8913
1
Magnitude Response
|H|
0
0.20.3
1
15
 1
 0
Magnitude in dB
Decibels
0
0.20.3
1
−1
0
1
Phase Response
Radians in π Units
0
0.20.3
1
0
2
4
6
8
10
Group Delay
Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.26
Digital Butterworth lowpass ﬁlter using bilinear transformation
Solution
MATLAB script:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1; Fs = 1/T;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog Chebyshev-1 prototype filter calculation:
>> [cs,ds] = afd_chb1(OmegaP,OmegaS,Rp,As);
*** Chebyshev-1 Filter Order =
4
>> % Bilinear transformation:
>> [b,a] = bilinear(cs,ds,Fs);
[C,B,A] = dir2cas(b,a)
C = 0.0018
B = 1.0000
2.0000
1.0000
1.0000
2.0000
1.0000
A = 1.0000
-1.4996
0.8482
1.0000
-1.5548
0.6493
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
423
0
0.20.3
1
0
0.1778
0.8913
1
Magnitude Response
|H|
0
0.20.3
1
15
 1
 0
Magnitude in dB
Decibels
0
0.20.3
1
−1
0
1
Phase Response
Radians in π Units
0
0.20.3
1
0
5
10
15
Group Delay
Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.27
Digital Chebyshev-I lowpass ﬁlter using bilinear transformation
The desired ﬁlter is a fourth-order ﬁlter and has four zeros at z = −1. The
system function is
H(z) =
0.0018 
1 + z−14
(1 −1.4996z−1 + 0.8482z−2) (1 −1.5548z−1 + 0.6493z−2)
The frequency response plots are given in Figure 8.27, which are similar to those
in Figure 8.22.
□
□
EXAMPLE 8.19
Design the digital Chebyshev-II ﬁlter of Example 8.13. The speciﬁcations are
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

424
Chapter 8
IIR FILTER DESIGN
Solution
MATLAB script:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1; Fs = 1/T;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog Chebyshev-2 Prototype Filter Calculation:
>> [cs,ds] = afd_chb2(OmegaP,OmegaS,Rp,As);
*** Chebyshev-2 Filter Order =
4
>> % Bilinear transformation:
>> [b,a] = bilinear(cs,ds,Fs);
[C,B,A] = dir2cas(b,a)
C = 0.1797
B = 1.0000
0.5574
1.0000
1.0000
-1.0671
1.0000
A = 1.0000
-0.4183
0.1503
1.0000
-1.1325
0.7183
The desired ﬁlter is again a fourth-order ﬁlter with system function
H(z) =
0.1797 
1 + 0.5574z−1 + z−2 
1 −1.0671z−1 + z−2
(1 −0.4183z−1 + 0.1503z−2) (1 −1.1325z−1 + 0.7183z−2)
The frequency response plots are given in Figure 8.28. Note that the bilinear
transformation has properly designed the Chebyshev-II digital ﬁlter.
□
□
EXAMPLE 8.20
Design the digital elliptic ﬁlter of Example 8.14. The speciﬁcations are
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
Solution
MATLAB script:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1; Fs = 1/T;
% Set T=1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analog-to-Digital Filter Transformations
425
0
0.2 0.3
1
0
0.1778
0.8913
1
Magnitude Response
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
|H|
0
0.2 0.3
1
15
 1
 0
Magnitude in dB
Decibels
0
0.2 0.3
1
−1
0
1
Phase Response
Radians in π Units
0
0.2 0.3
1
0
5
10
15
Group Delay
Samples
FIGURE 8.28
Digital Chebyshev-II lowpass ﬁlter using bilinear transformation
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog elliptic prototype filter calculation:
>> [cs,ds] = afd_elip(OmegaP,OmegaS,Rp,As);
*** Elliptic Filter Order =
3
>> % Bilinear transformation:
>> [b,a] = bilinear(cs,ds,Fs);
[C,B,A] = dir2cas(b,a)
C = 0.1214
B = 1.0000
-1.4211
1.0000
1.0000
1.0000
0
A = 1.0000
-1.4928
0.8612
1.0000
-0.6183
0
The desired ﬁlter is a third-order ﬁlter with system function
H(z) =
0.1214 
1 −1.4211z−1 + z−2 
1 + z−1
(1 −1.4928z−1 + 0.8612z−2) (1 −0.6183z−1)
The frequency response plots are given in Figure 8.29. Note that the bilinear
transformation has again properly designed the elliptic digital ﬁlter.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

426
Chapter 8
IIR FILTER DESIGN
0
0.20.3
1
0
0.1778
0.8913
1
Magnitude Response
|H|
0
0.20.3
1
15
 1
 0
Magnitude in dB
Decibels
0
0.20.3
1
−1
0
1
Phase Response
Radians in π Units
0
0.20.3
1
0
5
10
15
Group Delay
Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.29
Digital elliptic lowpass ﬁlter using bilinear transformation
The advantages of this mapping are that (a) it is a stable design,
(b) there is no aliasing, and (c) there is no restriction on the type of ﬁlter
that can be transformed. Therefore, this method is used exclusively in
computer programs including MATLAB, as we shall see next.
8.4.5 MATCHED-z TRANSFORMATION
In this method of ﬁlter transformation, zeros and poles of Ha(s) are di-
rectly mapped into zeros and poles in the z-plane using an exponential
function. Given a root (zero or pole) at the location s = a in the s-plane,
we map it in the z-plane at z = eaT where T is a sampling interval. Thus
the system function Ha(s) with zeros {zk} and poles {pℓ} is mapped into
the digital ﬁlter system function H(z) as
Ha(s) =
,M
k=1 (s −zk)
,N
ℓ=1 (s −pℓ)
→
H(z) =
,M
k=1

1 −ezkT z−1
,N
ℓ=1 (s −epℓT z−1)
(8.69)
Clearly, the z-transform system function is “matched” to the s-domain
system function.
Note that this technique appears to be similar to the impulse in-
variance mapping in that the pole locations are identical and aliasing
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Lowpass Filter Design Using MATLAB
427
is unavoidable. However, these two techniques diﬀer in zero locations.
Also, the matched-z transformation does not preserve either the impulse
response or the frequency response characteristics. Hence it is suitable
when designing using pole-zero placement, but it is generally unsuitable
when the frequency-domain speciﬁcations are given.
8.5 LOWPASS FILTER DESIGN USING MATLAB
In this section, we will demonstrate the use of MATLAB’s ﬁlter design
functions to design digital lowpass ﬁlters. These functions use the bilinear
transformation because of its desirable advantages as discussed in the
previous section. These functions are as follows.
1. [b,a]=butter(N,wn)
This function designs an Nth-order lowpass digital Butterworth ﬁlter
and returns the ﬁlter coeﬃcients in length N + 1 vectors b and a. The
ﬁlter order is given by (8.49), and the cutoﬀfrequency wn is determined
by the prewarping formula (8.68). However, in MATLAB all digital
frequencies are given in units of π. Hence wn is computed by using the
following relation:
ωn = 2
π tan−1
ΩcT
2

The use of this function is given in Example 8.21.
2. [b,a]=cheby1(N,Rp,wn)
This function designs an Nth-order lowpass digital Chebyshev-I ﬁlter
with Rp decibels of ripple in the passband. It returns the ﬁlter coef-
ﬁcients in length N + 1 vectors b and a. The ﬁlter order is given by
(8.59), and the cutoﬀfrequency wn is the digital passband frequency
in units of π; that is,
ωn = ωp/π
The use of this function is given in Example 8.22.
3. [b,a]=cheby2(N,As,wn)
This function designs an Nth-order lowpass digital Chebyshev-II ﬁlter
with the stopband attenuation As decibels. It returns the ﬁlter coef-
ﬁcients in length N + 1 vectors b and a. The ﬁlter order is given by
(8.59), and the cutoﬀfrequency wn is the digital stopband frequency
in units of π; that is,
ωn = ωs/π
The use of this function is given in Example 8.23.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

428
Chapter 8
IIR FILTER DESIGN
4. [b,a]=ellip(N,Rp,As,wn)
This function designs an Nth-order lowpass digital elliptic ﬁlter with
the passband ripple of Rp decibels and a stopband attenuation of As
decibels. It returns the ﬁlter coeﬃcients in length N + 1 vectors b and
a. The ﬁlter order is given by (8.62), and the cutoﬀfrequency wn is the
digital passband frequency in units of π; that is,
ωn = ωp/π
The use of this function is given in Example 8.24.
All these above functions can also be used to design other frequency-
selective ﬁlters, such as highpass and bandpass. We will discuss their
additional capabilities in Section 8.6.
There is also another set of ﬁlter functions—namely, the buttord,
cheb1ord, cheb2ord, and ellipord functions—that can provide ﬁlter
order N and ﬁlter cutoﬀfrequency ωn, given the speciﬁcations. These
functions are available in the Signal Processing toolbox. In the examples
to follow, we will determine these parameters using the formulas given
earlier. We will discuss the ﬁlter-order functions in the next section.
In the following examples, we will redesign the same lowpass ﬁlters
of previous examples and compare their results. The speciﬁcations of the
lowpass digital ﬁlter are
ωp = 0.2π, Rp = 1 dB
ωs = 0.3π, As = 15 dB
□
EXAMPLE 8.21
Digital Butterworth lowpass ﬁlter design:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog Prototype Specifications:
>> T = 1;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog Prototype Order Calculation:
>> N =ceil((log10((10ˆ(Rp/10)-1)/(10ˆ(As/10)-1)))/(2*log10(OmegaP/OmegaS)));
>> fprintf(’\n*** Butterworth Filter Order = %2.0f \n’,N)
** Butterworth Filter Order =
6
>> OmegaC = OmegaP/((10ˆ(Rp/10)-1)ˆ(1/(2*N))); % Analog BF prototype cutoff
>> wn = 2*atan((OmegaC*T)/2);
% Digital BF cutoff freq
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Lowpass Filter Design Using MATLAB
429
>> % Digital butterworth filter design:
>> wn = wn/pi;
% Digital BF cutoff in pi units
>> [b,a]=butter(N,wn);
[b0,B,A] = dir2cas(b,a)
C = 5.7969e-004
B = 1.0000
2.0297
1.0300
1.0000
1.9997
1.0000
1.0000
1.9706
0.9709
A = 1.0000
-0.9459
0.2342
1.0000
-1.0541
0.3753
1.0000
-1.3143
0.7149
The system function is
H(z) =
0.00057969 
1 + z−16
(1 −0.9459z−1 + 0.2342z−2) (1 −1.0541z−1 + 0.3753z−2) (1 −1.3143z−1 + 0.7149z−2)
which is the same as in Example 8.17. The frequency-domain plots were shown
in Figure 8.26.
□
□
EXAMPLE 8.22
Digital Chebyshev-I lowpass ﬁlter design:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications:
>> T = 1;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog prototype order calculation:
>> ep = sqrt(10ˆ(Rp/10)-1);
% Passband ripple factor
>> A = 10ˆ(As/20);
% Stopband attenuation factor
>> OmegaC = OmegaP;
% Analog prototype cutoff freq
>> OmegaR = OmegaS/OmegaP;
% Analog prototype transition ratio
>> g = sqrt(A*A-1)/ep;
% Analog prototype intermediate cal.
>> N = ceil(log10(g+sqrt(g*g-1))/log10(OmegaR+sqrt(OmegaR*OmegaR-1)));
>> fprintf(’\n*** Chebyshev-1 Filter Order = %2.0f \n’,N)
*** Chebyshev-1 Filter Order =
4
>> % Digital Chebyshev-I Filter Design:
>> wn = wp/pi;
% Digital passband freq in pi units
>> [b,a]=cheby1(N,Rp,wn);
[b0,B,A] = dir2cas(b,a)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

430
Chapter 8
IIR FILTER DESIGN
b0 = 0.0018
B = 1.0000
2.0000
1.0000
1.0000
2.0000
1.0000
A = 1.0000
-1.4996
0.8482
1.0000
-1.5548
0.6493
The system function is
H(z) =
0.0018 
1 + z−14
(1 −1.4996z−1 + 0.8482z−2) (1 −1.5548z−1 + 0.6493z−2)
which is the same as in Example 8.18. The frequency-domain plots were shown
in Figure 8.27.
□
□
EXAMPLE 8.23
Digital Chebyshev-II lowpass ﬁlter design:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications:
>> T = 1;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog prototype order calculation:
>> ep = sqrt(10ˆ(Rp/10)-1);
% Passband ripple factor
>> A = 10ˆ(As/20);
% Stopband attenuation factor
>> OmegaC = OmegaP;
% Analog prototype cutoff freq
>> OmegaR = OmegaS/OmegaP;
% Analog prototype transition ratio
>> g = sqrt(A*A-1)/ep;
% Analog prototype intermediate cal.
>> N = ceil(log10(g+sqrt(g*g-1))/log10(OmegaR+sqrt(OmegaR*OmegaR-1)));
>> fprintf(’\n*** Chebyshev-2 Filter Order = %2.0f \n’,N)
*** Chebyshev-2 Filter Order =
4
>> % Digital Chebyshev-II filter design:
>> wn = ws/pi;
% Digital stopband freq in pi units
>> [b,a]=cheby2(N,As,wn);
[b0,B,A] = dir2cas(b,a)
b0 = 0.1797
B = 1.0000
0.5574
1.0000
1.0000
-1.0671
1.0000
A = 1.0000
-0.4183
0.1503
1.0000
-1.1325
0.7183
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Lowpass Filter Design Using MATLAB
431
The system function is
H(z) =
0.1797 
1 + 0.5574z−1 + z−2 
1 −1.0671z−1 + z−2
(1 −0.4183z−1 + 0.1503z−2) (1 −1.1325z−1 + 0.7183z−2)
which is the same as in Example 8.19. The frequency-domain plots were shown
in Figure 8.28.
□
□
EXAMPLE 8.24
Digital elliptic lowpass ﬁlter design:
>> % Digital filter specifications:
>> wp = 0.2*pi;
% Digital passband freq in rad
>> ws = 0.3*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications:
>> T = 1;
% Set T=1
>> OmegaP = (2/T)*tan(wp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(ws/2);
% Prewarp prototype stopband freq
>> % Analog elliptic filter order calculations:
>> ep = sqrt(10ˆ(Rp/10)-1);
% Passband ripple factor
>> A = 10ˆ(As/20);
% Stopband attenuation factor
>> OmegaC = OmegaP;
% Analog prototype cutoff freq
>> k = OmegaP/OmegaS;
% Analog prototype transition ratio
>> k1 = ep/sqrt(A*A-1);
% Analog prototype intermediate cal.
>> capk = ellipke([k.ˆ2 1-k.ˆ2]);
>> capk1 = ellipke([(k1 .ˆ2) 1-(k1 .ˆ2)]);
>> N = ceil(capk(1)*capk1(2)/(capk(2)*capk1(1)));
>> fprintf(’\n*** Elliptic Filter Order = %2.0f \n’,N)
*** Elliptic Filter Order =
3
>> % Digital elliptic filter design:
>> wn = wp/pi;
% Digital passband freq in pi units
>> [b,a]=ellip(N,Rp,As,wn);
[b0,B,A] = dir2cas(b,a)
b0 = 0.1214
B = 1.0000
-1.4211
1.0000
1.0000
1.0000
0
A = 1.0000
-1.4928
0.8612
1.0000
-0.6183
0
The system function is
H(z) =
0.1214 
1 −1.4211z−1 + z−2 
1 + z−1
(1 −1.4928z−1 + 0.8612z−2) (1 −0.6183z−1)
which is the same as in Example 8.20. The frequency-domain plots were shown
in Figure 8.29.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

432
Chapter 8
IIR FILTER DESIGN
TABLE 8.1
Comparison of four prototype ﬁlters
Prototype
Order N
Actual Rp
Actual As
Butterworth
6
1
17.6
Chebyshev-I
4
1
23.6
Chebyshev-II
4
0.15
15
Elliptic
3
1
16
8.5.1 COMPARISON OF FOUR PROTOTYPE FILTERS
In Examples 8.17–8.20, we designed a lowpass digital ﬁlter using the bi-
linear mapping on four diﬀerent prototype analog ﬁlters satisfying the
speciﬁcations ωp = 0.2π, Rp = 1 dB, ωs = 0.3π, and As = 15 dB. Let us now
compare their performance in terms of order N, the actual passband ripple
Rp measured at ωp, and the actual stopband attenuation As measured at
ωs. This comparison is shown in Table 8.1. The Butterworth, Chebyshev-
I, and elliptic ﬁlters satisfy Rp speciﬁcation exactly at ωp but exceed As
speciﬁcation at ωs while the Chebyshev-II ﬁlter performs exactly opposite.
Clearly, the elliptic prototype gives the best design in terms of the
smallest order while satisfying magnitude speciﬁcations almost exactly.
However, if we compare phase responses of all four ﬁlters, then the ellip-
tic design has the most nonlinear phase response in the passband, while
Butterworth has the least nonlinear phase response.
8.6 FREQUENCY-BAND TRANSFORMATIONS
In the preceding two sections, we designed digital lowpass ﬁlters from
their corresponding analog ﬁlters. Certainly, we would like to design other
types of frequency-selective ﬁlters, such as highpass, bandpass, and band-
stop. This is accomplished by transforming the frequency axis (or band)
of a lowpass ﬁlter so that it behaves as another frequency-selective ﬁl-
ter. These transformations on the complex variable z are very similar
to bilinear transformations, and the design equations are algebraic. The
procedure to design a general frequency-selective ﬁlter is to ﬁrst design
a digital prototype (of ﬁxed bandwidth—say, unit bandwidth) lowpass ﬁl-
ter and then to apply these algebraic transformations. In this section, we
will describe the basic philosophy behind these mappings and illustrate
their mechanism through examples. MATLAB provides functions that
incorporate frequency-band transformation in the s-plane. We will ﬁrst
demonstrate the use of the z-plane mapping and then illustrate the use
of MATLAB functions. Typical speciﬁcations for most commonly used
types of frequency-selective digital ﬁlters are shown in Figure 8.30.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Band Transformations
433
FIGURE 8.30
Speciﬁcations of frequency-selective ﬁlters
Let HLP (Z) be the given prototype lowpass digital ﬁlter, and let H(z)
be the desired frequency-selective digital ﬁlter. Note that we are using
two diﬀerent frequency variables, Z and z, with HLP and H, respectively.
Deﬁne a mapping of the form
Z−1 = G(z−1)
such that
H(z) = HLP (Z)|Z−1=G(z−1)
To do this, we simply replace Z−1 everywhere in HLP by the function
G(z−1). Given that HLP (Z) is a stable and causal ﬁlter, we also want
H(z) to be stable and causal. This imposes the following requirements.
1. G(·) must be a rational function in z−1 so that H(z) is implementable.
2. The unit circle of the Z-plane must map onto the unit circle of the
z-plane.
3. For stable ﬁlters, the inside of the unit circle of the Z-plane must also
map onto the inside of the unit circle of the z-plane.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

434
Chapter 8
IIR FILTER DESIGN
Let ω′ and ω be the frequency variables of Z and z, respectively—that
is, Z = ejω′and z = ejω on their respective unit circles. Then requirement
2 above implies that
Z−1 =
G(z−1)
 =
G(e−jω)
 = 1
and
e−jω′ =
G(e−jω)
 ej̸
G(e−jω)
or
−ω′ = ̸ G(e−jω)
The general form of the function G(·) that satisﬁes these requirements is
a rational function of the allpass type given by
Z−1 = G

z−1
= ±
n

k=1
z−1 −αk
1 −αkz−1
where |αk| < 1 for stability and to satisfy requirement 3.
Now by choosing an appropriate order n and the coeﬃcients {αk}, we
can obtain a variety of mappings. The most widely used transformations
are given in Table 8.2. We will now illustrate the use of this table for
designing a highpass digital ﬁlter.
□
EXAMPLE 8.25
In Example 8.22 we designed a Chebyshev-I lowpass ﬁlter with speciﬁcations
ω′
p = 0.2π,
ω′
s = 0.3π,
Rp = 1 dB
As = 15 dB
and determined its system function
HLP (Z) =
0.001836(1 + Z−1)4
(1 −1.4996Z−1 + 0.8482Z−2)(1 −1.5548Z−1 + 0.6493Z−2)
Design a highpass ﬁlter with these tolerances but with passband beginning at
ωp = 0.6π.
Solution
We want to transform the given lowpass ﬁlter into a highpass ﬁlter such that
the cutoﬀfrequency ω′
p = 0.2π is mapped onto the cutoﬀfrequency ωp = 0.6π.
From Table 8.2,
α = −cos[(0.2π + 0.6π)/2]
cos[(0.2π −0.6π)/2] = −0.38197
(8.70)
Hence
HLP (z) = H(Z)|Z=−z−1−0.38197
1−0.38197z−1
=
0.02426(1 −z−1)4
(1 + 0.5661z−1 + 0.7657z−2)(1 + 1.0416z−1 + 0.4019z−2)
which is the desired ﬁlter. The frequency response plots of the lowpass ﬁlter
and the new highpass ﬁlter are shown in Figure 8.31.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Band Transformations
435
TABLE 8.2
Frequency transformation for digital ﬁlters (prototype lowpass ﬁlter has cutoﬀ
frequency ω′
c)
Type of
Transformation
Transformation
Parameters
Lowpass
z−1 −→z−1 −α
1 −αz−1
ωc = cutoﬀfrequency of new ﬁlter
α = sin [(ω′
c −ωc) /2]
sin [(ω′c + ωc) /2]
Highpass
z−1 −→−z−1 + α
1 + αz−1
ωc = cutoﬀfrequency of new ﬁlter
α = −cos [(ω′
c + ωc) /2]
cos [(ω′c −ωc) /2]
Bandpass
z−1 −→−z−2 −α1z−1 + α2
α2z−2 −α1z−1 + 1
ωℓ= lower cutoﬀfrequency
ωu = upper cutoﬀfrequency
α1 = −2βK/(K + 1)
α2 = (K −1)/(K + 1)
β = cos [(ωu + ωℓ) /2]
cos [(ωu −ωℓ) /2]
K = cot ωu −ωℓ
2
tan ω′
c
2
Bandstop
z−1 −→z−2 −α1z−1 + α2
α2z−2 −α1z−1 + 1
ωℓ= lower cutoﬀfrequency
ωu = upper cutoﬀfrequency
α1 = −2β/(K + 1)
α2 = (K −1)/(K + 1)
β = cos [(ωu + ωℓ) /2]
cos [(ωu −ωℓ) /2]
K = tan ωu −ωℓ
2
tan ω′
c
2
From this example, it is obvious that to obtain the rational function
of a new digital ﬁlter from the prototype lowpass digital ﬁlter, we should
be able to implement rational function substitutions from Table 8.2. This
appears to be a diﬃcult task, but since these are algebraic functions, we
can use the conv function repetitively for this purpose. The following
zmapping function illustrates this approach.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

436
Chapter 8
IIR FILTER DESIGN
0
0.2
1
0
0.8913
1
LP Filter Magnitude Response
|H|
0
0.2
1
30
 1
 0
LP Filter Magnitude in dB
Decibels
0
0.6
1
0
0.8913
1
HP Filter Magnitude Response
|H|
0
0.6
1
30
 1
 0
HP Filter Magnitude in dB
Decibels
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.31
Magnitude response plots for Example 8.25
function [bz,az] = zmapping(bZ,aZ,Nz,Dz)
% Frequency-band transformation from Z-domain to z-domain
% -------------------------------------------------------
% [bz,az] = zmapping(bZ,aZ,Nz,Dz)
% performs:
%
b(z)
b(Z)|
%
---- = ----|
N(z)
%
a(z)
a(Z)|@Z = ----
%
D(z)
%
bNzord = (length(bZ)-1)*(length(Nz)-1);
aDzord = (length(aZ)-1)*(length(Dz)-1);
bzord = length(bZ)-1; azord = length(aZ)-1;
bz = zeros(1,bNzord+1);
for k = 0:bzord
pln = [1];
for l = 0:k-1
pln = conv(pln,Nz);
end
pld = [1];
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Band Transformations
437
for l = 0:bzord-k-1
pld = conv(pld,Dz);
end
bz = bz+bZ(k+1)*conv(pln,pld);
end
az = zeros(1,aDzord+1);
for k = 0:azord
pln = [1];
for l = 0:k-1
pln = conv(pln,Nz);
end
pld = [1];
for l = 0:azord-k-1
pld = conv(pld,Dz);
end
az = az+aZ(k+1)*conv(pln,pld);
end
□
EXAMPLE 8.26
Use the zmapping function to perform the lowpass-to-highpass transformation
in Example 8.25.
Solution
First we will design the lowpass digital ﬁlter in MATLAB using the bilinear
transformation procedure and then use the zmapping function.
MATLAB script:
>> % Digital lowpass filter specifications:
>> wplp = 0.2*pi;
% Digital passband freq in rad
>> wslp = 0.3*pi;
% Digital stopband freq in rad
>>
Rp = 1;
% Passband ripple in dB
>>
As = 15;
% Stopband attenuation in dB
>> % Analog prototype specifications: Inverse mapping for frequencies
>> T = 1; Fs = 1/T;
% Set T=1
>> OmegaP = (2/T)*tan(wplp/2);
% Prewarp prototype passband freq
>> OmegaS = (2/T)*tan(wslp/2);
% Prewarp prototype stopband freq
>> % Analog Chebyshev prototype filter calculation:
>> [cs,ds] = afd_chb1(OmegaP,OmegaS,Rp,As);
** Chebyshev-1 Filter Order =
4
>> % Bilinear transformation:
>> [blp,alp] = bilinear(cs,ds,Fs);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

438
Chapter 8
IIR FILTER DESIGN
>> % Digital highpass filter cutoff frequency:
>> wphp = 0.6*pi;
% Passband-edge frequency
>> % LP-to-HP frequency-band transformation:
>> alpha = -(cos((wplp+wphp)/2))/(cos((wplp-wphp)/2))
alpha = -0.3820
>> Nz = -[alpha,1]; Dz = [1,alpha];
>> [bhp,ahp] = zmapping(blp,alp,Nz,Dz);
[C,B,A] = dir2cas(bhp,ahp)
C = 0.0243
B = 1.0000
-2.0000
1.0000
1.0000
-2.0000
1.0000
A = 1.0000
1.0416
0.4019
1.0000
0.5561
0.7647
The system function of the highpass ﬁlter is
H(z) =
0.0243(1 −z−1)4
(1 + 0.5661z−1 + 0.7647z−2)(1 + 1.0416z−1 + 0.4019z−2)
which is essentially identical to that in Example 8.25.
□
8.6.1 DESIGN PROCEDURE
In Example 8.26, a lowpass prototype digital ﬁlter was available to trans-
form into a highpass ﬁlter so that a particular band-edge frequency was
properly mapped. In practice, we have to ﬁrst design a prototype lowpass
digital ﬁlter whose speciﬁcations should be obtained from speciﬁcations
of other frequency-selective ﬁlters as given in Figure 8.30. We will now
show that the lowpass prototype ﬁlter speciﬁcations can be obtained from
the transformation formulas given in Table 8.2.
Let us use the highpass ﬁlter of Example 8.25 as an example. The
passband-edge frequencies were transformed using the parameter α =
−0.38197 in (8.70). What is the stopband-edge frequency of the highpass
ﬁlter, say ωs, corresponding to the stopband edge ω′
s = 0.3π of the pro-
totype lowpass ﬁlter? This can be answered by (8.70). Since α is ﬁxed for
the transformation, we set the equation
α = −cos[(0.3π + ωs)/2]
cos[(0.3π −ωs)/2] = −0.38197
This is a transcendental equation whose solution can be obtained iter-
atively from an initial guess. It can be done using MATLAB, and the
solution is
ωs = 0.4586π
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Band Transformations
439
Now, in practice, we will know the desired highpass frequencies ωs and
ωp, and we are required to ﬁnd the prototype lowpass cutoﬀfrequencies
ω′
s and ω′
p. We can choose the passband frequency ω′
p with a reasonable
value—say, ω′
p = 0.2π—and determine α from ωp using the formula from
Table 8.2. Now ω′
s can be determined (for our highpass ﬁlter example)
from α and
Z = −z−1 + α
1 + αz−1
where Z = ejω′
s and z = ejωs, or
ω′
s = ̸

−e−jωs + α
1 + αe−jωs

(8.71)
Continuing our highpass ﬁlter example, let ωp = 0.6π and ωs = 0.4586π be
the band-edge frequencies. Let us choose ω′
p = 0.2π. Then α = −0.38197
from (8.70), and from (8.71)
ω′
s = ̸

−e−j0.4586π −0.38197
1 −0.38197e−j−0.38197

= 0.3π
as expected. Now we can design a digital lowpass ﬁlter and transform
it into a highpass ﬁlter using the zmapping function to complete our
design procedure. For designing a highpass Chebyshev-I digital ﬁlter, the
above procedure can be incorporated into a MATLAB function called the
cheb1hpf function, shown here.
function [b,a] = cheb1hpf(wp,ws,Rp,As)
% IIR highpass filter design using Chebyshev-1 prototype
% function [b,a] = cheb1hpf(wp,ws,Rp,As)
%
b = numerator polynomial of the highpass filter
%
a = denominator polynomial of the highpass filter
%
wp = passband frequency in radians
%
ws = stopband frequency in radians
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
%
% Determine the digital lowpass cutoff frequencies:
wplp = 0.2*pi;
alpha = -(cos((wplp+wp)/2))/(cos((wplp-wp)/2));
wslp = angle(-(exp(-j*ws)+alpha)/(1+alpha*exp(-j*ws)));
%
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

440
Chapter 8
IIR FILTER DESIGN
% Compute analog lowpass prototype specifications:
T = 1; Fs = 1/T;
OmegaP = (2/T)*tan(wplp/2);
OmegaS = (2/T)*tan(wslp/2);
% Design analog Chebyshev prototype lowpass filter:
[cs,ds] = afd_chb1(OmegaP,OmegaS,Rp,As);
% Perform bilinear transformation to obtain digital lowpass
[blp,alp] = bilinear(cs,ds,Fs);
% Transform digital lowpass into highpass filter
Nz = -[alpha,1]; Dz = [1,alpha];
[b,a] = zmapping(blp,alp,Nz,Dz);
We will demonstrate this procedure in the following example.
□
EXAMPLE 8.27
Design a highpass digital ﬁlter to satisfy
ωp = 0.6π,
ωs = 0.4586π,
Rp = 1 dB
As = 15 dB
Use the Chebyshev-I prototype.
Solution
MATLAB script:
>> % Digital highpass filter specifications:
>> wp = 0.6*pi;
% Digital passband freq in rad
>> ws = 0.4586*pi;
% Digital stopband freq in rad
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> [b,a] = cheb1hpf(wp,ws,Rp,As);
[C,B,A] = dir2cas(b,a)
C = 0.0243
B = 1.0000
-2.0000
1.0000
1.0000
-2.0000
1.0000
A = 1.0000
1.0416
0.4019
1.0000
0.5561
0.7647
The system function is
H(z) =
0.0243(1 −z−1)4
(1 + 0.5661z−1 + 0.7647z−2)(1 + 1.0416z−1 + 0.4019z−2)
which is identical to that in Example 8.26.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Band Transformations
441
This highpass ﬁlter design procedure can be easily extended to other
frequency-selective ﬁlters using the transformation functions in Table 8.2.
These design procedures are explored in Problems P8.34, P8.36, P8.38,
and P8.40. We now describe MATLAB’s ﬁlter design functions for design-
ing arbitrary frequency-selective ﬁlters.
8.6.2 MATLAB IMPLEMENTATION
In the preceding section, we discussed four MATLAB functions to design
digital lowpass ﬁlters. These same functions can also be used to design
highpass, bandpass, and bandstop ﬁlters. The frequency-band transfor-
mations in these functions are done in the s-plane; that is, they use
Approach-1 discussed on page 370. For the purpose of illustration, we
will use the function butter. It can be used with the following variations
in its input arguments.
• [b,a] = BUTTER(N,wn,’high’) designs an Nth-order highpass ﬁlter
with digital 3 dB cutoﬀfrequency wn in units of π.
• [b,a] = BUTTER(N,wn,)designs an order 2N bandpass ﬁlter if wn is a
two-element vector, wn=[w1 w2], with 3 dB passband w1 < w < w2 in
units of π.
• [b,a] = BUTTER(N,wn,’stop’) is an order 2N bandstop ﬁlter if wn=[w1
w2]with 3 dB stopband w1 < w < w2 in units of π.
To design any frequency-selective Butterworth ﬁlter, we need to know
the order N and the 3 dB cutoﬀfrequency vector wn. In this chapter, we
described how to determine these parameters for lowpass ﬁlters. However,
these calculations are more complicated for bandpass and bandstop ﬁlters.
In their SP toolbox, MATLAB provides a function called buttord to
compute these parameters. Given the speciﬁcations, ωp, ωs, Rp, and As,
this function determines the necessary parameters. Its syntax is
[N,wn] = buttord(wp,ws,Rp,As)
The parameters wp and ws have some restrictions, depending on the type
of ﬁlter:
• For lowpass ﬁlters wp < ws.
• For highpass ﬁlters wp > ws.
• For bandpass ﬁlters wp and ws are two-element vectors, wp=[wp1,
wp2] and ws=[ws1,ws2], such that ws1 < wp1 < wp2 < ws2.
• For bandstop ﬁlters wp1 < ws1 < ws2 < wp2.
Now using the buttord function in conjunction with the butter func-
tion, we can design any Butterworth IIR ﬁlter. Similar discussions apply
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

442
Chapter 8
IIR FILTER DESIGN
for cheby1, cheby2, and ellip functions with appropriate modiﬁcations.
We illustrate the use of these functions through the following examples.
□
EXAMPLE 8.28
In this example, we will design a Chebyshev-I highpass ﬁlter whose speciﬁcations
were given in Example 8.27.
Solution
MATLAB script:
>> % Digital filter specifications:
% Type: Chebyshev-I highpass
>> ws = 0.4586*pi;
% Dig. stopband-edge frequency
>> wp = 0.6*pi;
% Dig. passband-edge frequency
>> Rp = 1;
% Passband ripple in dB
>> As = 15;
% Stopband attenuation in dB
>> % Calculations of Chebyshev-I filter parameters:
>> [N,wn] = cheb1ord(wp/pi,ws/pi,Rp,As);
>> % Digital Chebyshev-I highpass filter design:
>> [b,a] = cheby1(N,Rp,wn,’high’);
>> % Cascade form realization:
>> [b0,B,A] = dir2cas(b,a)
b0 = 0.0243
B = 1.0000
-1.9991
0.9991
1.0000
-2.0009
1.0009
A = 1.0000
1.0416
0.4019
1.0000
0.5561
0.7647
The cascade form system function
H(z) =
0.0243(1 −z−1)4
(1 + 0.5661z−1 + 0.7647z−2)(1 + 1.0416z−1 + 0.4019z−2)
is identical to the ﬁlter designed in Example 8.27, which demonstrates that
the two approaches described on page 370 are identical. The frequency-domain
plots are shown in Figure 8.32.
□
□
EXAMPLE 8.29
In this example, we will design an elliptic bandpass ﬁlter whose speciﬁcations
are given in the following MATLAB script.
>> % Digital filter specifications:
% Type: Elliptic bandpass
>> ws = [0.3*pi 0.75*pi];
% Dig. stopband-edge frequency
>> wp = [0.4*pi 0.6*pi];
% Dig. passband-edge frequency
>> Rp = 1;
% Passband ripple in dB
>> As = 40;
% Stopband attenuation in dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Frequency-Band Transformations
443
  0 
0.46 0.6 
  1 
0
0.1778
0.8913
1
Magnitude Response
  0 
0.46 0.6 
  1 
30
15
 1
 0
Magnitude in dB
Decibels
  0 
0.46 0.6 
  1 
−1
−0.5
0
0.5
1
Phase Response
Angle in π Units
  0 
0.46 0.6 
  1 
−5
0
5
10
Group Delay
Delay in Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.32
Digital Chebyshev-I highpass ﬁlter in Example 8.28
>> % Calculations of elliptic filter parameters:
>> [N,wn] = ellipord(wp/pi,ws/pi,Rp,As);
>> % Digital elliptic bandpass filter design:
>> [b,a] = ellip(N,Rp,As,wn);
>> % Cascade form realization:
>> [b0,B,A] = dir2cas(b,a)
b0 = 0.0197
B = 1.0000
1.5066
1.0000
1.0000
0.9268
1.0000
1.0000
-0.9268
1.0000
1.0000
-1.5066
1.0000
A = 1.0000
0.5963
0.9399
1.0000
0.2774
0.7929
1.0000
-0.2774
0.7929
1.0000
-0.5963
0.9399
Note that the designed ﬁlter is a tenth-order ﬁlter. The frequency-domain plots
are shown in Figure 8.33.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

444
Chapter 8
IIR FILTER DESIGN
0
0.30.4
0.6 0.75
1
0
0.8913
1
Magnitude Response
0
0.30.4
0.6 0.75
1
40
 0
Magnitude in dB
Decibel
Magnitude
s
0
0.30.4
0.6 0.75
1
−1
−0.5
0
0.5
1
Phase Response
Angle in π Units
0
0.30.4
0.6 0.75
1
0
10
20
30
40
Group Delay
Samples
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
FIGURE 8.33
Digital elliptic bandpass ﬁlter in Example 8.29
□
EXAMPLE 8.30
Finally, we will design a Chebyshev-II bandstop ﬁlter whose speciﬁcations are
given in the following MATLAB script.
>> % Digital filter specifications:
% Type: Chebyshev-II bandstop
>> ws = [0.4*pi 0.7*pi];
% Dig. stopband-edge frequency
>> wp = [0.25*pi 0.8*pi];
% Dig. passband-edge frequency
>> Rp = 1;
% Passband ripple in dB
>> As = 40;
% Stopband attenuation in dB
>> % Calculations of Chebyshev-II filter parameters:
>> [N,wn] = cheb2ord(wp/pi,ws/pi,Rp,As);
>> % Digital Chebyshev-II bandstop filter design:
>> [b,a] = cheby2(N,As,ws/pi,’stop’);
>> % Cascade form realization:
>> [b0,B,A] = dir2cas(b,a)
b0 = 0.1558
B = 1.0000
1.1456
1.0000
1.0000
0.8879
1.0000
1.0000
0.3511
1.0000
1.0000
-0.2434
1.0000
1.0000
-0.5768
1.0000
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
445
0
0.25 0.4
0.70.8
1
0
0.8913
1
Magnitude Response
Frequency in π Units
Frequency in π Units
Frequency in π Units
Frequency in π Units
Magnitude
0
0.25 0.4
0.70.8
1
−40
0
Magnitude in dB
Decibels
0
0.25 0.4
0.70.8
1
−1
−0.5
0
0.5
1
Phase Response
Angle in π Units
0
0.25 0.4
0.70.8
1
0
5
10
15
Group Delay
Samples
FIGURE 8.34
Digital Chebyshev-II bandstop ﬁlter in Example 8.30
A = 1.0000
1.3041
0.8031
1.0000
0.8901
0.4614
1.0000
0.2132
0.2145
1.0000
-0.4713
0.3916
1.0000
-0.8936
0.7602
This is also a tenth-order ﬁlter. The frequency domain plots are shown in
Figure 8.34.
□
8.7 PROBLEMS
P8.1
A digital resonator is to be designed with ω0 = π/4 that has two zeros at z = 0.
1. Compute and plot the frequency response of this resonator for r = 0.8, 0.9, and 0.99.
2. For each case in part 1, determine the 3 dB bandwidth and the resonant frequency ωr
from your magnitude plots.
3. Check if your results in part 2 are in agreement with the theoretical results.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

446
Chapter 8
IIR FILTER DESIGN
P8.2
A digital resonator is to be designed with ω0 = π/4 that has two zeros at z = 1 and z = −1.
1. Compute and plot the frequency response of this resonator for r = 0.8, 0.9, and 0.99.
2. For each case in part 1 determine the 3 dB bandwidth and the resonant frequency ωr
from your magnitude plots.
3. Compare your results in part 2 with (8.48) and (8.47), respectively.
P8.3
We want to design a digital resonator with the following requirements: a 3 dB bandwidth of
0.05 rad, a resonant frequency of 0.375 cycles/sam, and zeros at z = 1 and z = −1. Using
trial-and-error approach, determine the diﬀerence equation of the resonator.
P8.4
A notch ﬁlter is to be designed with a null at the frequency ω0 = π/2.
1. Compute and plot the frequency response of this notch ﬁlter for r = 0.7, 0.9, and 0.99.
2. For each case in part 1, determine the 3 dB bandwidth from your magnitude plots.
3. By trial-and-error approach, determine the value of r if we want the 3 dB bandwidth to
be 0.04 radians at the null frequency ω0 = π/2.
P8.5
Repeat Problem P8.4 for a null at ω0 = π/6.
P8.6
A speech signal with bandwidth of 4 kHz is sampled at 8 kHz. The signal is corrupted by
sinusoids with frequencies 1 kH, 2 kHz, and 3 kHz.
1. Design an IIR ﬁlter using notch ﬁlter components that eliminates these sinusoidal
signals.
2. Choose the gain of the ﬁlter so that the maximum gain is equal to 1, and plot the
log-magnitude response of your ﬁlter.
3. Load the handel sound ﬁle in MATLAB, and add the preceding three sinusoidal signals
to create a corrupted sound signal. Now ﬁlter the corrupted sound signal using your
ﬁlter and comment on its performance.
P8.7
Consider the system function of an IIR lowpass ﬁlter
H(z) = K
1 + z−1
1 −0.9z−1
(8.72)
where K is a constant that can be adjusted to make the maximum gain response equal to 1.
We obtain the system function of an Lth-order comb ﬁlter HL(z) using HL(z) = H
zL
.
1. Determine the value of K for the system function in (8.72).
2. Using the K value from part 1, determine and plot the log-magnitude response of the
comb ﬁlter for L = 6.
3. Describe the shape of your plot in part 2.
P8.8
Consider the system function of an IIR highpass ﬁlter
H(z) = K
1 −z−1
1 −0.9z−1
(8.73)
where K is a constant that can be adjusted to make the maximum gain response equal to 1.
We obtain the system function of an Lth-order comb ﬁlter HL(z) using HL(z) = H
zL
.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
447
1. Determine the value of K for the system function in (8.73).
2. Using the K value from part 1, determine and plot the log-magnitude response of the
comb ﬁlter for L = 6.
3. Describe the shape of your plot in part 2.
P8.9
(Adapted from [72]) As discussed in Chapter 1, echos and reverberations of a signal x(n)
can be obtained by scaling and delaying, that is,
y(n) =
∞
	
k=0
αkx(n −kD)
(8.74)
where D is a positive integer for minimum delay and αk > αk−1 > 0.
1. Consider the IIR comb ﬁlter given by
H(z) =
1
1 −az−D
(8.75)
Determine its impulse response. Explain why this ﬁlter can be used as a reverberator.
2. Consider the cascade of three allpass comb ﬁlters
H(z) =
zD1 −a1
1 −a1z−D1 ×
zD2 −a2
1 −a2z−D2 ×
zD3 −a3
1 −a3z−D3
(8.76)
which can be used as a practical digital reverberator. Compute and plot the impulse
response of this reverberator for D1 = 50, a1 = 0.7; D2 = 41, a2 = 0.665; and D3 = 32,
a3 = 0.63175.
3. Repeat part 2 for D1 = 53, a1 = 0.7; D2 = 40, a2 = 0.665; and D3 = 31, a3 = 0.63175.
How does the shape of this reverberator diﬀer from the one in part 2? Which is a good
reverberator?
P8.10 Consider the ﬁrst-order allpass system function given by
H(z) = a + z−1
1 + az−1 ,
0 < a < 1
(8.77)
The phase-delay of a system is deﬁned as Φ(ω) △
= −̸ H
ejω
/ω and is measured in samples.
1. Show that the phase-delay of the system in (8.77) at low frequencies is given by
Φ(ω) ≈1 −a
1 + a
for a ≈1
(8.78)
2. Plot the phase-delay over −π/2 ≤ω ≤π/2 for a = 0.9, 0.95, and 0.99 to verify
Problem P8.10. Comment on the accuracy of the results.
3. Design a ﬁrst-order allpass system that has phase delay of 0.01 samples. Plot its
magnitude and phase-delay responses.
P8.11 Consider the second-order allpass system function given by
H(z) = a2 + a1z−1 + z−2
1 + a1z−1 + a2z−2
(8.79)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

448
Chapter 8
IIR FILTER DESIGN
The phase-delay of a system is deﬁned as Φ(ω) △
= −̸ H
ejω
/ω and is measured in samples.
It can be shown that if we choose
a1 = 1
2 −d
1 + d

,
a2 = (2 −d)(1 −d)
(2 + d)(1 + d)
(8.80)
then phase-delay Φ(ω) at low frequencies is approximated by d in samples. Verify this result
by plotting Φ(ω) over −π/2 ≤ω ≤π/2 for d = 0.1, d = 0.05, and d = 0.01.
P8.12 Design an analog Butterworth lowpass ﬁlter that has a 0.25 dB or better ripple at
500 rad/sec and at least 50 dB of attenuation at 2000 rad/sec. Determine the system
function in a rational function form. Plot the magnitude response, the log-magnitude
response in dB, the phase response, and the impulse response of the ﬁlter.
P8.13 Design an analog Butterworth lowpass ﬁlter that has a 0.5 dB or better ripple at 10 kHz
and at least 45 dB of attenuation at 20 kHz. Determine the system function in a cascade
form. Plot the magnitude response, the log-magnitude response in dB, the group-delay, and
the impulse response of the ﬁlter.
P8.14 Design a lowpass analog Chebyshev-I ﬁlter with an acceptable ripple of 1 dB for |Ω| ≤10
and an attenuation of 50 dB or greater beyond |Ω| = 15 rad/sec. Determine the system
function in a rational function form. Plot the magnitude response, the log-magnitude
response in dB, the group-delay, and the impulse response of the ﬁlter.
P8.15 Design a lowpass analog Chebyshev-I ﬁlter with the following characteristics:
•
a passband ripple of 0.5 dB,
•
passband cutoﬀfrequency of 4 kHz, and
•
stopband attenuation of 45 dB or greater beyond 20 kHz.
Determine the system function in a cascade form. Plot the magnitude response, the
log-magnitude response in dB, the phase response, and the impulse response of the ﬁlter.
P8.16 A signal xa(t) contains two frequencies, 10 kHz and 15 kHz. We want to suppress the
15 kHz component to 50 dB attenuation while passing the 10 kHz component with less than
0.25 dB attenuation. Design a minimum-order Chebyshev-II analog ﬁlter to perform this
ﬁltering operation. Plot the log-magnitude response, and verify the design.
P8.17 Design an analog Chebyshev-II lowpass ﬁlter that has a 0.25 dB or better ripple at 250 Hz
and at least 40 dB of attenuation at 400 Hz. Plot the magnitude response, the
log-magnitude response in dB, the group-delay, and the impulse response of the ﬁlter.
P8.18 A signal xa(t) contains two frequencies, 10 kHz and 15 kHz. We want to suppress the
15 kHz component to 50 dB attenuation while passing the 10 kHz component with less than
0.25 dB attenuation. Design a minimum-order elliptic analog ﬁlter to perform this ﬁltering
operation. Plot the log-magnitude response and verify the design. Compare your design
with the Chebyshev-II design in Problem P8.16.
P8.19 Design an analog elliptic lowpass ﬁlter that has a 0.25 dB or better ripple at 500 rad/sec
and at least 50 dB of attenuation at 2000 rad/sec. Determine the system function in a
rational function form. Plot the magnitude response, the log-magnitude response in dB, the
phase response, and the impulse response of the ﬁlter. Compare your design with the
Butterworth design in Problem P8.12.
P8.20 Write a MATLAB function to design analog lowpass ﬁlters. The format of this function
should be
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
449
function [b,a] =afd(type,Fp,Fs,Rp,As)
%
% function [b,a] =afd(type,Fp,Fs,Rp,As)
%
Designs analog lowpass filters
% type = ’butter’ or ’cheby1’ or ’cheby2’ or ’ellip’
%
Fp = passband cutoff in Hz
%
Fs = stopband cutoff in Hz
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
Use the afd butt, afd chb1, afd chb2, and afd elip functions developed in this chapter.
Check your function using speciﬁcations given in Problems P8.12 through P8.17.
P8.21 We want to design a Chebyshev-I prototype lowpass digital ﬁlter operating at a sampling
rate of 8 kHz with a passband edge of 3.2 kHz, a passband ripple of 0.5 dB, and a minimum
stopband attenuation of 45 dB at 3.8 kHz.
1. Using the impulse invariance transformation with T = 1 sec, design the digital ﬁlter. Plot
the magnitude and the log-magnitude responses as functions of analog frequency in kHz.
2. Repeat part 1 using T = 1/8000 sec.
3. Compare the above two designs in parts 1 and 2 in terms of their frequency responses.
Comment on the eﬀect of T on the impulse invariance design.
P8.22 Design a Butterworth digital lowpass ﬁlter to satisfy the following speciﬁcations.
passband edge:
0.4π,
Rp = 0.5 dB
stopband edge:
0.6π,
As = 50 dB
Use the impulse invariance method with T = 2. Determine the system function in the
rational form, and plot the log-magnitude response in dB. Plot the impulse response h(n)
and the impulse response ha(t) of the analog prototype and compare their shapes.
P8.23 Write a MATLAB function to design digital lowpass ﬁlters based on the impulse invariance
transformation. The format of this function should be
function [b,a] =dlpfd_ii(type,wp,ws,Rp,As,T)
%
% function [b,a] =dlpfd_ii(type,wp,ws,Rp,As,T)
%
Designs digital lowpass filters using impulse invariance
% type = ’butter’ or ’cheby1’
%
wp = passband cutoff in Hz
%
ws = stopband cutoff in Hz
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
%
T = sampling interval
Use the afd function developed in Problem P8.20. Check your function on speciﬁcations
given in Problems P8.21 and P8.22.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

450
Chapter 8
IIR FILTER DESIGN
P8.24 In this problem, we will develop a technique called the step invariance transformation. In
this technique, the step response of an analog prototype ﬁlter is preserved in the resulting
digital ﬁlter; that is, if va(t) is the step response of the prototype and if v(n) is the step
response of the digital ﬁlter, then
v(n) = va(t)|t=nT ,
T : sampling interval
Note that the frequency-domain quantities are related by
Va(s)
△= L [va(t)] = Ha(s)/s
and
V (z)
△= Z [v(n)] = H(z)
1
1 −z−1
Hence the step invariance transformation steps are as follows.
Given Ha(s),
•
divide Ha(s) by s to obtain Va(s),
•
ﬁnd residues {Rk} and poles {pk} of Va(s),
•
transform analog poles {pk} into digital poles *
epkT+
where T is arbitrary,
•
determine V (z) from residues {Rk} and poles *
epkT+
, and ﬁnally
•
determine H(z) by multiplying V (z) by 
1 −z−1
.
Use the above procedure to develop a MATLAB function to implement the step invariance
transformation. The format of this function should be
function [b,a] =stp_invr(c,d,T)
% Step invariance transformation from analog to digital filter
% [b,a] =stp_invr(c,d,T)
%
b = numerator polynomial in zˆ(-1) of the digital filter
%
a = denominator polynomial in zˆ(-1) of the digital filter
%
c = numerator polynomial in s of the analog filter
%
d = denominator polynomial in s of the analog filter
%
T = sampling (transformation) parameter
P8.25 Design the lowpass Butterworth digital ﬁlter of Problem P8.22 using the step invariance
method. Plot the log-magnitude response in dB and compare it with that in Problem P8.22.
Plot the step response v(n) and the impulse response va(t) of the analog prototype and
compare their shapes.
P8.26 In this chapter, we discussed a ﬁlter transformation technique called the matched-z
transformation. Using (8.69), write a MATLAB function called mzt that maps the analog
system function Ha(s) into the digital system function H(z). The format of the function
should be
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
451
function [b,a] = mzt(c,d,T)
% Matched-z transformation from analog to digital filter
% [b,a] = MZT(c,d,T)
%
b = numerator polynomial in zˆ(-1) of the digital filter
%
a = denominator polynomial in zˆ(-1) of the digital filter
%
c = numerator polynomial in s of the analog filter
%
d = denominator polynomial in s of the analog filter
%
T = sampling interval (transformation parameter)
Using this function, transform
Ha(s) =
s + 1
s2 + 5s + 6
into a digital ﬁlter H(z) for the sampling intervals (in seconds) T = 0.05, T = 0.1, and
T = 0.2. In each case, obtain a plot similar to that in Figure 8.20 and comment on the
performance of this technique.
P8.27 Consider an analog Butterworth lowpass ﬁlter that has a 1 dB or better ripple at 100 Hz
and at least 30 dB of attenuation at 150 Hz. Transform this ﬁlter into a digital ﬁlter using
the matched-z transformation technique in which Fs = 1000 Hz. Plot the magnitude and
phase response of the resulting digital ﬁlter and determine the exact band-edge frequencies
for the given dB speciﬁcations. Comment on the results.
P8.28 Consider an analog Chebyshev-I lowpass ﬁlter that has a 0.5 dB or better ripple at 500 Hz
and at least 40 dB of attenuation at 700 Hz. Transform this ﬁlter into a digital ﬁlter using
the matched-z transformation technique in which Fs = 2000 Hz. Plot the magnitude and
phase response of the resulting digital ﬁlter and determine the exact band-edge frequencies
for the given dB speciﬁcations. Comment on the results.
P8.29 Consider an analog Chebyshev-II lowpass ﬁlter that has a 0.25 dB or better ripple at
1500 Hz and at least 80 dB of attenuation at 2000 Hz. Transform this ﬁlter into a digital
ﬁlter using the matched-z transformation technique in which Fs = 8000 Hz. Plot the
magnitude and phase response of the resulting digital ﬁlter, and determine the exact
band-edge frequencies for the given dB speciﬁcations. Comment on the results. Is this a
satisfactory design?
P8.30 Consider the design of the lowpass Butterworth ﬁlter of Problem P8.22.
1. Use the bilinear transformation technique outlined in this chapter and the bilinear
function. Plot the log-magnitude response in dB. Compare the impulse responses of the
analog prototype and the digital ﬁlter.
2. Use the butter function and compare this design with the one in part 1.
P8.31 Consider the design of the digital Chebyshev-1 ﬁlter of Problem P8.21.
1. Use the bilinear transformation technique outlined in this chapter and the bilinear
function. Plot the log-magnitude response in dB. Compare the impulse responses of the
analog prototype and the digital ﬁlter.
2. Use the cheby1 function and compare this design with the one above.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

452
Chapter 8
IIR FILTER DESIGN
P8.32 Design a digital lowpass ﬁlter using elliptic prototype to satisfy the requirements
passband edge: 0.3π, Rp = 0.25 dB
stopband edge: 0.4π, As = 50 dB
Use the bilinear as well as the ellip function and compare your designs.
P8.33 Design a digital lowpass ﬁlter to satisfy the speciﬁcations
passband edge: 0.45π, Rp = 0.5 dB
stopband edge: 0.5π,
As = 60 dB
1. Use the butter function and determine the order N and the actual minimum stopband
attenuation in dB.
2. Use the cheby1 function and determine the order N and the actual minimum stopband
attenuation in dB.
3. Use the cheby2 function and determine the order N and the actual minimum stopband
attenuation in dB.
4. Use the ellip function and determine the order N and the actual minimum stopband
attenuation in dB.
5. Compare the orders, the actual minimum stopband attenuations, and the group delays
in each of the above designs.
P8.34 Write a MATLAB function to determine the lowpass prototype digital ﬁlter frequencies
from an highpass digital ﬁlter speciﬁcations using the procedure outlined in this chapter.
The format of this function should be
function [wpLP,wsLP,alpha] = hp2lpfre(wphp,wshp)
% Band-edge frequency conversion from highpass to lowpass digital filter
% [wpLP,wsLP,a] = hp2lpfre(wphp,wshp)
%
wpLP = passband edge for the lowpass prototype
%
wsLP = stopband edge for the lowpass prototype
% alpha = lowpass-to-highpass transformation parameter
%
wphp = passband edge for the highpass
%
wshp = stopband edge for the highpass
Using this function, develop a MATLAB function to design a highpass digital ﬁlter using
the bilinear transformation. The format of this function should be
function [b,a] = dhpfd_bl(type,wp,ws,Rp,As)
% IIR highpass filter design using bilinear transformation
% [b,a] = dhpfd_bl(type,wp,ws,Rp,As)
% type = ’butter’ or ’cheby1’ or ’chevy2’ or ’ellip’
%
b = numerator polynomial of the highpass filter
%
a = denominator polynomial of the highpass filter
%
wp = passband frequency in radians
%
ws = stopband frequency in radians (wp < ws)
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
Verify your function using the speciﬁcations in Example 8.27.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
453
P8.35 Design a highpass ﬁlter to satisfy the speciﬁcations
stopband edge: 0.4π, As = 60 dB
passband edge: 0.6π, Rp = 0.5 dB
1. Use the dhpfd bl function of Problem P8.34 and the Chebyshev-I prototype to design
this ﬁlter. Plot the log-magnitude response in dB of the designed ﬁlter.
2. Use the cheby1 function for design and plot the log-magnitude response in dB. Compare
these two designs.
P8.36 Write a MATLAB function to determine the lowpass prototype digital ﬁlter frequencies
from an arbitrary lowpass digital ﬁlter speciﬁcations using the functions given in Table 8.2
and the procedure outlined for highpass ﬁlters. The format of this function should be
function [wpLP,wsLP,alpha] = lp2lpfre(wplp,wslp)
% Band-edge frequency conversion from lowpass to lowpass digital filter
% [wpLP,wsLP,a] = lp2lpfre(wplp,wslp)
%
wpLP = passband edge for the lowpass prototype
%
wsLP = stopband edge for the lowpass prototype
% alpha = lowpass-to-highpass transformation parameter
%
wplp = passband edge for the given lowpass
%
wslp = passband edge for the given lowpass
Using this function, develop a MATLAB function to design a lowpass ﬁlter from a
prototype lowpass digital ﬁlter using the bilinear transformation. The format of this
function should be
function [b,a] = dlpfd_bl(type,wp,ws,Rp,As)
% IIR lowpass filter design using bilinear transformation
% [b,a] = dlpfd_bl(type,wp,ws,Rp,As)
% type = ’butter’ or ’cheby1’ or ’chevy2’ or ’ellip’
%
b = numerator polynomial of the bandpass filter
%
a = denominator polynomial of the bandpass filter
%
wp = passband frequency in radians
%
ws = stopband frequency in radians
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
Verify your function using the designs in Problem P8.33.
P8.37 Design a bandpass digital ﬁlter using the Cheby2 function. The speciﬁcations are
lower stopband edge: 0.3π
upper stopband edge: 0.6π As = 50 dB
lower passband edge: 0.4π
upper passband edge: 0.5π Rp = 0.5 dB
Plot the impulse response and the log-magnitude response in dB of the designed ﬁlter.
P8.38 Write a MATLAB function to determine the lowpass prototype digital ﬁlter frequencies
from a bandpass digital ﬁlter speciﬁcations using the functions given in Table 8.2 and the
procedure outlined for highpass ﬁlters. The format of this function should be
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

454
Chapter 8
IIR FILTER DESIGN
function [wpLP,wsLP,alpha] = bp2lpfre(wpbp,wsblp)
% Band-edge frequency conversion from bandpass to lowpass digital filter
% [wpLP,wsLP,a] = bp2lpfre(wpbp,wsbp)
%
wpLP = passband edge for the lowpass prototype
%
wsLP = stopband edge for the lowpass prototype
% alpha = lowpass-to-highpass transformation parameter
%
wpbp = passband-edge frequency array [wp_lower, wp_upper] for the bandpass
%
wsbp = passband-edge frequency array [ws_lower, ws_upper] for the bandpass
Using this function, develop a MATLAB function to design a bandpass ﬁlter from a
prototype lowpass digital ﬁlter using the bilinear transformation. The format of this
function should be
function [b,a] = dbpfd_bl(type,wp,ws,Rp,As)
% IIR bandpass filter design using bilinear transformation
% [b,a] = dbpfd_bl(type,wp,ws,Rp,As)
% type = ’butter’ or ’cheby1’ or ’chevy2’ or ’ellip’
%
b = numerator polynomial of the bandpass filter
%
a = denominator polynomial of the bandpass filter
%
wp = passband frequency array [wp_lower, wp_upper] in radians
%
ws = stopband frequency array [wp_lower, wp_upper] in radians
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
Verify your function using the design in Problem P8.37.
P8.39 We wish to use the Chebyshev-I prototype to design a bandstop digital IIR ﬁlter that meets
the following speciﬁcations:
0.95 ≤|H(ejω)| ≤1.05,
0 ≤|H(ejω)| ≤0.01,
0.95 ≤|H(ejω)| ≤1.05,
0 ≤|ω| ≤0.25π
0.35π ≤|ω| ≤0.65π
0.75π ≤|ω| ≤π
Use the cheby1 function and determine the system function H(z) of such a ﬁlter. Provide a
plot containing subplots of the log-magnitude response in dB and the impulse response.
P8.40 Write a MATLAB function to determine the lowpass prototype digital ﬁlter frequencies
from a bandstop digital ﬁlter speciﬁcations using the functions given in Table 8.2 and the
procedure outlined for highpass ﬁlters. The format of this function should be
function [wpLP,wsLP,alpha] = bs2lpfre(wpbp,wsblp)
% Band-edge frequency conversion from bandstop to lowpass digital filter
% [wpLP,wsLP,a] = bs2lpfre(wpbp,wsbp)
%
wpLP = passband edge for the lowpass prototype
%
wsLP = stopband edge for the lowpass prototype
% alpha = lowpass-to-highpass transformation parameter
%
wpbp = passband-edge frequency array [wp_lower, wp_upper] for the bandstop
%
wsbp = passband-edge frequency array [ws_lower, ws_upper] for the bandstop
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
455
Using this function, develop a MATLAB function to design a bandstop ﬁlter from a
prototype lowpass digital ﬁlter using the bilinear transformation. The format of this
function should be
function [b,a] = dbsfd_bl(type,wp,ws,Rp,As)
% IIR bandstop filter design using bilinear transformation
% [b,a] = dbsfd_bl(type,wp,ws,Rp,As)
% type = ’butter’ or ’cheby1’ or ’chevy2’ or ’ellip’
%
b = numerator polynomial of the bandstop filter
%
a = denominator polynomial of the bandstop filter
%
wp = passband frequency array [wp_lower, wp_upper] in radians
%
ws = stopband frequency array [wp_lower, wp_upper] in radians
%
Rp = passband ripple in dB
%
As = stopband attenuation in dB
Verify your function using the design in Problem P8.39.
P8.41 An analog signal
xa(t) = 3 sin(40πt) + 3 cos(50πt)
is to be processed by a
xa(t) −→A/D −→H(z) −→D/A −→ya(t)
system in which the sampling frequency is 100 sam/sec.
1. Design a minimum-order IIR digital ﬁlter that will pass the ﬁrst component of xa(t)
with attenuation of less than 1 dB and suppress the second component to at least 50 dB.
The ﬁlter should have a monotone passband and an equiripple stopband. Determine the
system function in rational function form and plot the log-magnitude response.
2. Generate 500 samples (sampled at 100 sam/sec) of the signal xa(t) and process through
the designed ﬁlter to obtain the output sequence. Interpolate this sequence (using any
one of the interpolating techniques discussed in Chapter 3) to obtain ya(t). Plot the
input and the output signals and comment on your results.
P8.42 Using the bilinear transformation method, design a tenth-order elliptic bandstop ﬁlter to
remove the digital frequency ω = 0.44π with bandwidth of 0.08π. Choose a reasonable value
for the stopband attenuation. Plot the magnitude response. Generate 201 samples of the
sequence
x(n) = sin [0.44πn] ,
n = 0, . . . , 200
and process thorough the bandstop ﬁlter. Comment on your results.
P8.43 Design a digital highpass ﬁlter H(z) to be used in a
xa(t) −→A/D −→H(z) −→D/A −→ya(t)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

456
Chapter 8
IIR FILTER DESIGN
structure to satisfy the following requirements:
•
sampling rate of 10 Khz,
•
stopband edge of 1.5 Khz with attenuation of 40 dB,
•
passband edge of 2 Khz with ripple of 1 dB,
•
equiripple passband and stopband, and
•
bilinear transformation method.
1. Plot the magnitude response of the overall analog ﬁlter over the [0, 5 Khz] interval.
2. Plot the magnitude response of the digital lowpass prototype.
3. What limitations must be placed on the input signals so that the preceding structure
truly acts as a highpass ﬁlter to them?
P8.44 The ﬁlter speciﬁcations shown in Figure P8.1 can be considered a combination of a
bandpass and a highpass speciﬁcations. Design a minimum-order IIR digital ﬁlter to satisfy
these speciﬁcations. Provide a plot of the magnitude response with grid lines as shown in
Figure P8.1. From your design and plot determine the order of the ﬁlter and the exact
band-edge frequencies.
P8.45 The ﬁlter speciﬁcations shown in Figure P8.2 can be considered as a combination of a
lowpass and a bandpass speciﬁcations. Design a minimum-order IIR digital ﬁlter to satisfy
these speciﬁcations. Provide a plot of the magnitude response with grid lines as shown in
Figure P8.2. From your design and plot determine the order of the ﬁlter and the exact
band-edge frequencies.
P8.46 Design a minimum-order IIR digital ﬁlter to satisfy the following speciﬁcations:
•
a passband over the [0.35π, 0.5π] interval,
•
stopbands over the [0, 0.3π] and [0.6π, π] intervals,
•
passband ripple of 1 dB,
EQUIRIPPLE
EQUIRIPPLE
MONOTONE
MONOTONE
1
0.9
0.5
0.45
0.010
0
0.1
0.2
0.5
0.6
0.7
0.8
1
|H (e jw)|
w
p
FIGURE P8.1
Filter speciﬁcations for Problem P8.44
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
457
Equiripple
Equiripple
Monotone
Monotone
1
0.9
0.5
0.45
0.01
0
0
0.9
0.2
0.3
0.5
0.6
0.8
1
ω
π
|H (e jw)|
FIGURE P8.2
Filter speciﬁcations for Problem P8.45
•
stopband attenuation of 40 dB, and
•
equiripple passbands and stopband.
Determine the system function H(z) of the designed ﬁlter in the rational function form.
Provide a plot of the log-magnitude response in dB. From your design and plot, answer the
following questions.
1. What is the order of the ﬁlter?
2. From your plot what are the exact band-edge frequencies for the given passband and
stopband attenuations?
3. Why is there a discrepancy between the speciﬁcation frequencies and the exact
frequencies?
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

458
C H A P T E R 9
Sampling Rate
Conversion
In many practical applications of digital signal processing, one is faced
with the problem of changing the sampling rate of a signal, either increas-
ing it or decreasing it by some amount. The process of converting a signal
from a given rate to a diﬀerent rate is called sampling rate conversion.
In turn, systems that employ multiple sampling rates in the processing
of digital signals are called multirate digital signal processing systems. In
this chapter, we describe sampling rate conversion and multirate signal
processing in the digital domain.
As an example, consider the system shown in Figure 9.1, in which
an analog signal xa(t) is sampled using the sampling rate of Fs =
1
T
samples/second. The resulting digital signal x(n) is subsequently ﬁltered
using a lowpass ﬁlter (LPF) with a cutoﬀfrequency of ωc.
Thus the output signal y(n) has all its energy in the band 0 ≤ω ≤
ωc = 2πfc. According to the sampling theorem, such a signal may be rep-
resented by the rate of 2fc/T samples/second instead of its existing rate
of Fs = 1/T. Note that |fc| ≤0.5. However, if fc ≪0.5, then 2fc/T ≪Fs.
Hence it would seem more advantageous to lower the sampling frequency
to a value closer to 2fc/T and perform signal processing operations at
this lower rate.
Other applications include the need for an optimal interpolation in
computer tomography and eﬃcient multistage designs of narrowband low-
pass ﬁlters.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Introduction
459
H(w)
Y(w)
Lowpass
Filter
wc
xa(t)
Xa(Ω)
x(n)
y(n)
−wc
wc
–wc
wc
w
w
Ω
0
0
0
p/T
−p/T
p
p
–p
−p
ADC
Fs = 1
T
FIGURE 9.1
A typical signal processing system
9.1 INTRODUCTION
The idea of interpolation is a very familiar concept to most of us and has
its origin in numerical analysis. Typically, interpolation is performed on a
table of numbers representing a mathematical function. Such a table may
be printed in a handbook or stored in a computer memory device. The
interpolation, often simply a linear (or straight line) approximation, cre-
ates an error called the interpolation error. The main diﬀerence between
interpolation in digital signal processing and interpolation in numerical
analysis is that we will assume that the given data is bandlimited to some
band of frequencies and develop schemes that are optimal on this ba-
sis, whereas a numerical analyst typically assumes that the data consists
of samples of polynomials (or very nearly so) and develops schemes to
minimize the resulting error.
To motivate this concept of interpolation in signal processing, it is
helpful to think of an underlying (or original) analog signal xa(t) that
was sampled to produce the given discrete signal x(n). If the xa(t) was
sampled at the minimum required rate, then, according to the sampling
theorem, it can be recovered completely from the samples x(n). If we now
sample this recovered analog signal, at, say, twice the old rate, we have
succeeded in doubling the sampling rate or interpolating by a factor of 2
with zero interpolation error. Speciﬁcally, we have the following.
Original discrete signal:
x(n) = xa(nT)
(9.1)
Reconstructed analog signal:
xa(t) = 
k xa(kT)sin[π(t −kT)/T]
π(t −kT)/T
(9.2)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

460
Chapter 9
SAMPLING RATE CONVERSION
Resampled analog signal:
xa

mT
2

=
	
k
xa(kT)sin

π

m T
2 −kT

/T

π

m T
2 −kT

/T
=
	
k
xa(kT)sin

π
 m
2 −k

π
 m
2 −k

(9.3)
resulting in high-rate discrete signal:
y(m)
△=xa

mT
2

(9.4)
In this formulation of ideal interpolation, the discrete signal was converted
to the analog signal and then back to the discrete signal at twice the rate.
In the subsequent sections, we will study how to avoid this roundabout
approach and perform sampling rate conversion completely in the digital
domain.
The process of sampling rate conversion in the digital domain can
be viewed as a linear ﬁltering operation, as illustrated in Figure 9.2a.
The input signal x(n) is characterized by the sampling rate Fx = 1/Tx,
and the output signal y(m) is characterized by the sampling rate Fy =
1/Ty, where Tx and Ty are the corresponding sampling intervals. In our
treatment, the ratio Fy/Fx is constrained to be rational
Fy
Fx
= I
D
(9.5)
where D and I are relatively prime integers. We shall show that the
linear ﬁlter is characterized by a time-variant impulse response, denoted
Rate Fx =
(a)
(b)
y(m)
y(m)
y(m + 1)
y(m + 2)
y(m + 3)
y(m + 4)
y(m + 5) y(m + 6)
x(n + 1)
x(n)
x(n)
x(n + 2)
x(n + 3)
x(n + 4)
x(n + 5)
ti
Linear Filter
h(n, m)
Fy = 1
Ty
1
Ty
FIGURE 9.2
Sampling rate conversion viewed as a linear ﬁltering process
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Decimation by a Factor D
461
as h(n, m). Hence the input x(n) and the output y(m) are related by the
superposition summation for time-variant systems.
The sampling rate conversion process can also be understood from the
point of view of digital resampling of the same analog signal. Let xa(t)
be the analog signal that is sampled at the ﬁrst rate Fx to generate x(n).
The goal of rate conversion is to obtain another sequence y(m) directly
from x(n), which is equal to the sampled values of xa(t) at a second rate
Fy. As is depicted in Figure 9.2b, y(m) is a time-shifted version of x(n).
Such a time shift can be realized by using a linear ﬁlter that has a ﬂat
magnitude response and a linear phase response (i.e., it has a frequency
response of e−jωτi, where τi is the time delay generated by the ﬁlter). If
the two sampling rates are not equal, the required amount of time shifting
will vary from sample to sample, as shown in Figure 9.2b. Thus the rate
converter can be implemented using a set of linear ﬁlters that have the
same ﬂat magnitude response but generate diﬀerent time delays.
Before considering the general case of sampling rate conversion, we
shall consider two special cases. One is the case of sampling rate reduction
by an integer factor D, and the second is the case of a sampling rate
increase by an integer factor I. The process of reducing the sampling rate
by a factor D (downsampling by D) is called decimation. The process of
increasing the sampling rate by an integer factor I (upsampling by I) is
called interpolation.
9.2 DECIMATION BY A FACTOR D
The basic operation required in decimation is the downsampling of the
high-rate signal x(n) into a low-rate signal y(m). We will develop the
time- and frequency-domain relationships between these two signals to
understand the frequency-domain aliasing in y(m). We will then study
the condition needed for error-free decimation and the system structure
required for its implementation.
9.2.1 THE DOWNSAMPLER
Note that the downsampled signal y(m) is obtained by selecting one out
of D samples of x(n) and throwing away the other (D −1) samples out
of every D samples—that is,
y(m) = x(n)|n=mD = x(mD);
n, m, D ∈{integers}
(9.6)
The block diagram representation of (9.6) is shown in Figure 9.3. This
downsampling element changes the rate of processing and thus is funda-
mentally diﬀerent from other block diagram elements that we have used
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

462
Chapter 9
SAMPLING RATE CONVERSION
x(n)
y(m)
Rate Fx
Rate Fy = Fx
D
D
FIGURE 9.3
A downsampling element
previously. In fact, we can show that a system containing a downsam-
pling element is shift varying. However, this fact does not prohibit the
frequency-domain analysis of y(m) in terms of x(n), as we shall see later.
□
EXAMPLE 9.1
Using D = 2 and x(n) = {1
↑
, 2, 3, 4, 3, 2, 1}, verify that the downsampler is time
varying.
Solution
The downsampled signal is y(m) = {1
↑
, 3, 3, 1}. If we now delay x(n) by one
sample, we get x(n−1) = {0
↑
, 1, 2, 3, 4, 3, 2, 1}. The corresponding downsampled
signal is y1(m) = {0
↑
, 2, 4, 2}, which is diﬀerent from y(m −1).
□
MATLAB Implementation
MATLAB provides the function [y] =
downsample(x,D) that downsamples input array x into output array y
by keeping every Dth sample starting with the ﬁrst sample. An optional
third parameter “phase” speciﬁes the sample oﬀset which must be an
integer between 0 and (D-1). For example,
>> x = [1,2,3,4,3,2,1]; y = downsample(x,2)
y =
1
3
3
1
downsamples by a factor of 2 starting with the ﬁrst sample. However,
>> x = [1,2,3,4,3,2,1]; y = downsample(x,2,1)
y =
2
4
2
produces an entirely diﬀerent sequence by downsampling, starting with
the second sample (i.e., oﬀset by 1).
The frequency-domain representation of the downsampled sig-
nal y(m)
We now express Y (ω) in terms of X(ω) using z-transform
relations. Toward this, we introduce a high-rate sequence ¯x(n), which is
given by
¯x(n)
△=

x(n),
n = 0, ±D, ±2D, . . .
0,
elsewhere
(9.7)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Decimation by a Factor D
463
Clearly, ¯x(n) can be viewed as a sequence obtained by multiplying x(n)
with a periodic train of impulses p(n), with period D, as illustrated in
Figure 9.4. The discrete Fourier series representation of p(n) is
p(n)
△=

1,
n = 0, ±D, ±2D, . . .
0,
elsewhere
= 1
D
D−1
	
ℓ=0
ej 2π
D ℓn
(9.8)
Hence we can write
¯x(n) = x(n)p(n)
(9.9)
and
y(m) = ¯x(mD) = x(mD)p(mD) = x(mD)
(9.10)
as shown in (9.6). Figure 9.4 shows an example of sequences x(n), ¯x(n),
and y(m) deﬁned in (9.7)–(9.10).
n
n
n
m
x(n)
y(m)
p(n)
(a)
(b)
(c)
(d)
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
8
9
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
8
9
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
8
9
−3
−2
−1
0
1
2
3
x(n)
FIGURE 9.4
Operation of downsampling: (a) original signal x(n), (b) periodic
impulse train p(n) with period D = 3, (c) multiplication of x(n) with p(n), and
(d) downsampled signal y(m)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

464
Chapter 9
SAMPLING RATE CONVERSION
Now the z-transform of the output sequence y(m) is
Y (z) =
∞
	
m=−∞
y(m)z−m =
∞
	
m=−∞
¯x(mD)z−m
(9.11)
Y (z) =
∞
	
m=−∞
¯x(m)z−m/D
where the last step follows from the fact that ¯x(m) = 0, except at multi-
ples of D. By making use of the relations in (9.7) and (9.8) in (9.11), we
obtain
Y (z) =
∞
	
m=−∞
x(m)

1
D
D−1
	
k=0
ej2πmk/D

z−m/D
= 1
D
D−1
	
k=0
∞
	
m=−∞
x(m)

e−j2πk/Dz1/D−m
= 1
D
D−1
	
k=0
X

e−j2πk/Dz1/D
(9.12)
The key steps in obtaining the z-transform representation (9.12), for the
(D ↓1) downsampler, are as follows:
• the introduction of the high-rate sequence ¯x(n), which has (D−1) zeros
in between the retained values x(nD), and
• the impulse-train representation (9.8) for the periodic sampling series
that relates x(n) to ¯x(n).
By evaluating Y (z) on the unit circle, we obtain the spectrum of the
output signal y(m). Since the rate of y(m) is Fy = 1/Ty, the frequency
variable, which we denote as ωy, is in radians and is relative to the sam-
pling rate Fy,
ωy = 2πF
Fy
= 2πFTy
(9.13)
Since the sampling rates are related by the expression
Fy = Fx
D
(9.14)
it follows that the frequency variables ωy and
ωx = 2πF
Fx
2πFTx
(9.15)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Decimation by a Factor D
465
A/3
wx
wy
0
−3p
−2p
2p
3p
−p
−p/3
p/3
−p
p
p
A
0
X(wx)
Y(wy)
FIGURE 9.5
Spectra of x(n) and y(m) in no-aliasing case
are related by
ωy = Dωx
(9.16)
Thus, as expected, the frequency range 0 ≤|ωx| ≤π/D is stretched into
the corresponding frequency range 0 ≤|ωy| ≤π by the downsampling
process.
We conclude that the spectrum Y (ωy), which is obtained by evaluat-
ing (9.12) on the unit circle, can be expressed as1
Y (ωy) = 1
D
D−1
	
k=0
X
ωy −2πk
D

(9.17)
which is an aliased version of the spectrum X(ωx) of x(n). To avoid alias-
ing error, one needs the spectrum X(ωx) to be less than full band or
bandlimited (note that this bandlimitedness is in the digital frequency
domain). In fact, we must have
X(ωx) = 0
for
π
D ≤|ωx| ≤π
(9.18)
Then
Y (ωy) = 1
DX
ωy
D

, |ωy| ≤π
(9.19)
and no aliasing error is present. An example of this for D = 3 is shown in
Figure 9.5.
1In this chapter, we will make a slight change in our notation for the DTFT. We will use
X(ω) to denote the spectrum of x(n) instead of the previously used notation X(ejω).
Although this change does conﬂict with the z-transform notation, the meaning should
be clear from the context. This change is made for the sake of clarity and visibility of
variables.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

466
Chapter 9
SAMPLING RATE CONVERSION
Comments:
1. The sampling theorem interpretation for (9.19) is that the sequence
x(n) was originally sampled at D times higher rate than required;
therefore, downsampling by D simply reduces the eﬀective sampling
rate to the minimum required to prevent aliasing.
2. Equation (9.18) expresses the requirement for zero decimation error
in the sense that no information is lost—i.e., there is no irreversible
aliasing error in the frequency domain.
3. The argument
ωy
D occurs because in our notation ω is expressed in
rad/sample. Thus the frequency of y(m) expressed in terms of the
higher-rate sequence x(n) must be divided by D to account for the
slower rate of y(m).
4. Note that there is a factor
1
D in (9.19). This factor is required to make
the inverse Fourier transform work out properly and is entirely consis-
tent with the spectra of the sampled analog signals.
9.2.2 THE IDEAL DECIMATOR
In general, (9.18) will not be exactly true, and the (D ↓1) downsampler
would cause irreversible aliasing error. To avoid aliasing, we must ﬁrst
reduce the bandwidth of x(n) to Fx,max = Fx/2D, or equivalently, to
ωx,max = π/D. Then we may downsample by D and thus avoid aliasing.
The decimation process is illustrated in Figure 9.6. The input se-
quence x(n) is passed through a lowpass ﬁlter, characterized by the
impulse response h(n) and a frequency response HD(ωx), which ideally
satisﬁes the condition
HD(ωx) =

1,
|ωx| ≤π/D
0,
otherwise
(9.20)
Thus the ﬁlter eliminates the spectrum of X(ωx) in the range π/D <
ωx < π. Of course, the implication is that only the frequency components
of x(n) in the range |ωx| ≤π/D are of interest in further processing of
the signal.
IDEAL
LPF
D
y(m)
x(n)
v(n)
Rate: Fx
Fx
Ideal Decimator
= Fy
Fx
D
FIGURE 9.6
Ideal decimation by a factor D
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Decimation by a Factor D
467
The output of the ﬁlter is a sequence v(n) given as
v(n)
△=
∞
	
k=0
h(k)x(n −k)
(9.21)
which is then downsampled by the factor D to produce y(m). Thus
y(m) = v(mD) =
∞
	
k=0
h(k)x(mD −k)
(9.22)
Although the ﬁltering operation on x(n) is linear and time invariant, the
downsampling operation in combination with the ﬁltering results also in
a time-variant system.
The frequency-domain characteristics of the output sequence y(m)
obtained through the ﬁltered signal v(n) can be determined by following
the analysis steps given before—that is, by relating the spectrum of y(m)
to the spectrum of the input sequence x(n). Using these steps, we can
show that
Y (z) = 1
D
D−1
	
k=0
H

e−j2πk/Dz1/D
X

e−j2πk/Dz1/D
(9.23)
or that
Y (ωy) = 1
D
D−1
	
k=0
H
ωy −2πk
D

X
ωy −2πk
D

(9.24)
With a properly designed ﬁlter HD(ω), the aliasing is eliminated and,
consequently, all but the ﬁrst term in (9.24) vanish. Hence
Y (ωy) = 1
DHD
ωy
D

X
ωy
D

= 1
DX
ωy
D

(9.25)
for 0 ≤|ωy| ≤π. The spectra for the sequences x(n), h(n), v(n), and
y(m) are illustrated in Figure 9.7.
MATLAB Implementation
MATLAB provides the function y =
decimate(x,D) that resamples the sequence in array x at 1/D times
the original sampling rate. The resulting resampled array y is D times
shorter—that is, length(y) = length(x)/D. The ideal lowpass ﬁlter
given in (9.20) is not possible in the MATLAB implementation; however,
fairly accurate approximations are used. The default lowpass ﬁlter used
in the function is an eighth-order Chebyshev type-I lowpass ﬁlter with
the cutoﬀfrequency of 0.8π/D. Using additional optional arguments, the
ﬁlter order can be changed or an FIR ﬁlter of speciﬁed order and cutoﬀ
frequency can be used.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

468
Chapter 9
SAMPLING RATE CONVERSION
–p
p
wx
wx
0
0
wy
–p
p
0
p
D
–
p
D
wx
0
H(wx)
– p
D
p
D
X(wx)
V(wx)
Y(wy)
FIGURE 9.7
Spectra of signals in the decimation of x(n) by a factor D
□
EXAMPLE 9.2
Let x(n) = cos(0.125πn). Generate a large number of samples of x(n) and
decimate them using D = 2, 4, and 8 to show the results of decimation.
Solution
We will plot the middle segments of the signals to avoid end-eﬀects due to
the default lowpass ﬁlter in the decimate function. The following MATLAB
script shows details of these operations, and Figure 9.7 shows the plots of the
sequences.
n = 0:2048; k1 = 256; k2 = k1+32; m = 0:(k2-k1);
Hf1 = figure(’units’,’inches’,’position’,[1,1,6,4],...
’paperunits’,’inches’,’paperposition’,[0,0,6,4]);
% (a) Original signal
x = cos(0.125*pi*n); subplot(2,2,1);
Ha = stem(m,x(m+k1+1),’g’,’filled’); axis([-1,33,-1.1,1.1]);
set(Ha,’markersize’,2); ylabel(’Amplitude’);
title(’Original Sequence x(n)’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
% (b) Decimation by D = 2
D = 2; y = decimate(x,D); subplot(2,2,2);
Hb = stem(m,y(m+k1/D+1),’c’,’filled’);
axis([-1,33,-1.1,1.1]);
set(Hb,’markersize’,2); ylabel(’Amplitude’);
title(’Decimated by D = 2’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Decimation by a Factor D
469
% (c) Decimation by D = 4
D = 4; y = decimate(x,D); subplot(2,2,3);
Hc = stem(m,y(m+k1/D+1),’r’,’filled’);
axis([-1,33,-1.1,1.1]);
set(Hc,’markersize’,2); ylabel(’Amplitude’);
title(’Decimated by D = 4’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
xlabel(’n’);
% (d) Decimation by D = 8
D = 8; y = decimate(x,D); subplot(2,2,4);
Hd = stem(m,y(m+k1/D+1),’m’,’filled’);
axis([-1,33,-1.1,1.1]);
set(Hd,’markersize’,2); ylabel(’Amplitude’);
title(’Decimated by D = 8’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
xlabel(’n’);
From Figure 9.8, we observe that the decimated sequences for D = 2 and
D = 4 are correct and represent the original sinusoidal sequence x(n) at lower
sampling rates. However, the sequence for D = 8 is almost zero because the
0
16
32
0
16
32
−1
0
1
Amplitude
−1
0
1
Amplitude
−1
0
1
Amplitude
Original Sequence x(n)
0
16
32
−1
0
1
Amplitude
0
16
32
Decimated by D = 2
Decimated by D = 4
Decimated by D = 8
n
n
FIGURE 9.8
Original and decimated signals in Example 9.2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

470
Chapter 9
SAMPLING RATE CONVERSION
lowpass ﬁlter has attenuated x(n) prior to downsampling. Recall that the cutoﬀ
frequency of the lowpass ﬁlter is set to 0.8π/D = 0.1π, which eliminates x(n).
If we had used the downsampling operation on x(n) instead of decimation, the
resulting sequence would be y(m) = 1, which is an aliased signal. Thus the
lowpass ﬁltering is necessary.
□
9.3 INTERPOLATION BY A FACTOR I
An increase in the sampling rate by an integer factor of I—that is,
Fy = IFx—can be accomplished by interpolating I −1 new samples
between successive values of the signal. The interpolation process can
be accomplished in a variety of ways. We shall describe a process that
preserves the spectral shape of the signal sequence x(n). This process can
be accomplished in two steps. The ﬁrst step creates an intermediate signal
at the high rate Fy by interlacing zeros in between nonzero samples in an
operation called upsampling. In the second step, the intermediate signal
is ﬁltered to “ﬁll in” zero-interlaced samples to create the interpolated
high-rate signal. As before, we will ﬁrst study the time- and frequency-
domain characteristics of the upsampled signal and then introduce the
interpolation system.
9.3.1 THE UPSAMPLER
Let v(m) denote the intermediate sequence with a rate Fy = IFx, which
is obtained from x(n) by adding I −1 zeros between successive values of
x(n). Thus
v(m) =

x(m/I),
m = 0, ±I, ±2I, . . .
0,
otherwise
(9.26)
and its sampling rate is identical to the rate of v(m). The block diagram
of the upsampler is shown in Figure 9.9. Again, any system containing
the upsampler is a time-varying system (Problem P9.1).
I
v(m)
x(n)
Rate IFx = Fv
Rate Fx
FIGURE 9.9
An upsampling element
□
EXAMPLE 9.3
Let I = 2 and x(n) = {1
↑
, 2, 3, 4}. Verify that the upsampler is time varying.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Interpolation by a Factor I
471
Solution
The upsampled signal is v(m) = {1
↑
, 0, 2, 0, 3, 0, 4, 0}. If we now delay x(n) by
one sample, we get x(n−1) = {0
↑
, 1, 2, 3, 4}. The corresponding upsampled signal
is v1(m) = {0
↑
, 0, 1, 0, 2, 0, 3, 0, 4, 0} = v(m −2) and not v(m −1).
□
MATLAB Implementation
MATLAB provides the function [v] =
upsample(x,I) that upsamples input array x into output v by insert-
ing (I-1) zeros between input samples. An optional third parameter,
“phase,” speciﬁes the sample oﬀset, which must be an integer between
0 and (I-1). For example,
>> x = [1,2,3,4]; v = upsample(x,3)
v =
1
0
0
2
0
0
3
0
0
4
0
0
upsamples by a factor of 2 starting with the ﬁrst sample. However,
>> v = upsample(x,3,1)
v =
0
1
0
0
2
0
0
3
0
0
4
0
>> v = upsample(x,3,2)
v =
0
0
1
0
0
2
0
0
3
0
0
4
produces two diﬀerent signals by upsampling, starting with the second
and the third sample (i.e., oﬀset by 1), respectively. Note that the lengths
of the upsampled signals are I times the length of the original signal.
The frequency-domain representation of the upsampled signal
y(m)
The sequence v(m) has a z-transform
V (z) =
∞
	
m=−∞
v(m)z−m =
∞
	
m=−∞
v(m)z−mI = X(zI)
(9.27)
The corresponding spectrum of v(m) is obtained by evaluating (9.27) on
the unit circle. Thus
V (ωy) = X(ωyI)
(9.28)
where ωy denotes the frequency variable relative to the new sampling rate
Fy (i.e., ωy = 2πF/Fy). Now the relationship between sampling rates
is Fy = IFx, and hence the frequency variables ωx and ωy are related
according to the formula
ωy = ωx
I
(9.29)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

472
Chapter 9
SAMPLING RATE CONVERSION
−p
0
0
wx
p
−3p
I
−2p
I
2p
I
3p
I
wy = wx
I
−p
I
p
I
X(wx)
V(wx)
FIGURE 9.10
Spectra of x(n) and v(m) where V (ωy) = X(ωyI)
The spectra X(ωx) and V (ωy) are illustrated in Figure 9.10. We observe
that the sampling rate increase, obtained by the addition of I −1 zero
samples between successive values of x(n), results in a signal whose spec-
trum V (ωy) is an I-fold periodic repetition of the input signal spectrum
X(ωx).
9.3.2 THE IDEAL INTERPOLATOR
Since only the frequency components of x(n) in the range 0 ≤ωy ≤
π/I are unique, the images of X(ω) above ωy = π/I should be rejected
by passing the sequence v(m) through a lowpass ﬁlter with a frequency
response HI(ωy) that ideally has the characteristic
HI(ωy) =

C, 0 ≤|ωy| ≤π/I
0, otherwise
(9.30)
where C is a scale factor required to properly normalize the output
sequence y(m). Consequently, the output spectrum is
Y (ωy) =

CX(ωyI), 0 ≤|ωy| ≤π/I
0,
otherwise
(9.31)
The scale factor C is selected so that the output y(m) = x(m/I) for
m = 0, ±I, ±2I, . . . . For mathematical convenience, we select the point
m = 0. Thus
y(0) = 1
2π
 π
−π
Y (ωy)dωy = C
2π
 π/I
−π/I
X(ωyI)dωy
(9.32)
Since ωy = ωx/I, (9.32) can be expressed as
y(0) = C
I
1
2π
 π
−π
X(ωx)dωx = C
I x(0)
(9.33)
therefore C = I is the desired normalization factor.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Interpolation by a Factor I
473
IDEAL
LPF
I
y(m)
x(n)
v(m)
Rate: Fx
IFx
IFx
Ideal Interpolator
FIGURE 9.11
Ideal interpolation by a factor I
Finally, we indicate that the output sequence y(m) can be expressed
as a convolution of the sequence v(n) with the unit sample response h(n)
of the lowpass ﬁlter. Thus
y(m) =
∞
	
k=−∞
h(m −k)v(k)
(9.34)
Since v(k) = 0 except at multiples of I, where v(kI) = x(k), (9.34)
becomes
y(m) =
∞
	
k=−∞
h(m −kI)x(k)
(9.35)
The ideal interpolator is shown in Figure 9.11.
MATLAB Implementation
MATLAB provides the function [y,h] =
interp(x,I) that resamples the signal in array x at I times the original
sampling rate. The resulting resampled array y is I times longer—that
is, length(y) = I*length(x). The ideal lowpass ﬁlter given in (9.30) is
approximated by a symmetric ﬁlter impulse response, h, which is designed
internally. It allows the original samples to pass through unchanged and
interpolates between so that the mean square error between them and
their ideal values is minimized. The third optional parameter, L, speciﬁes
the length of the symmetric ﬁlter as 2*L*I+1, and the fourth optional
parameter, cutoff, speciﬁes the cutoﬀfrequency of the input signal in π
units. The default values are L = 5 and cutoff = 0.5. Thus, if I = 2,
then the length of the symmetric ﬁlter is 21 for the default L = 5.
□
EXAMPLE 9.4
Let x(n) = cos(πn). Generate samples of x(n) and interpolate them using I = 2,
4, and 8 to show the results of interpolation.
Solution
We will plot the middle segments of the signals to avoid end-eﬀects due to
the default lowpass ﬁlter in the interp function. The following MATLAB
script shows details of these operations, and Figure 9.12 shows the plots of the
sequences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

474
Chapter 9
SAMPLING RATE CONVERSION
−1
0
1
Amplitude
−1
0
1
Amplitude
0
16
32
0
16
32
−1
0
1
Amplitude
−1
0
1
Amplitude
0
16
32
0
16
32
Original Sequence x(n)
Interpolated by I = 2
Interpolated by I = 4
n
n
Interpolated by I = 8
FIGURE 9.12
Original and interpolated signals in Example 9.4
n = 0:256; k1 = 64; k2 = k1+32; m = 0:(k2-k1);
Hf1 = figure(’units’,’inches’,’position’,[1,1,6,4],...
’paperunits’,’inches’,’paperposition’,[0,0,6,4]);
% (a) Original signal
x = cos(pi*n); subplot(2,2,1);
Ha = stem(m,x(m+k1+1),’g’,’filled’); axis([-1,33,-1.1,1.1]);
set(Ha,’markersize’,2); ylabel(’Amplitude’);
title(’Original Sequence x(n)’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
% (b) Interpolation by I = 2
I = 2; y = interp(x,I); subplot(2,2,2);
Hb = stem(m,y(m+k1*I+1),’c’,’filled’);
axis([-1,33,-1.1,1.1]);
set(Hb,’markersize’,2); ylabel(’Amplitude’);
title(’Interpolated by I = 2’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Interpolation by a Factor I
475
% (c) Interpolation by I = 4
I = 4; y = interp(x,I); subplot(2,2,3);
Hc = stem(m,y(m+k1*I+1),’r’,’filled’);
axis([-1,33,-1.1,1.1]);
set(Hc,’markersize’,2); ylabel(’Amplitude’);
title(’Interpolated by I = 4’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
xlabel(’n’);
% (d) Interpolation by I = 8
I = 8; y = interp(x,I); subplot(2,2,4);
Hd = stem(m,y(m+k1*I+1),’m’,’filled’);
axis([-1,33,-1.1,1.1]);
set(Hd,’markersize’,2); ylabel(’Amplitude’);
title(’Interpolated by I = 8’,’fontsize’,TF);
set(gca,’xtick’,[0,16,32]); set(gca,’ytick’,[-1,0,1]);
xlabel(’n’);
From Figure 9.11, we observe that the interpolated sequences for all three values
of I are appropriate and represent the original sinusoidal signal x(n) at higher
sampling rates. In the case of I = 8, the resulting sequence does not appear
to be perfectly sinusoidal in shape. This is due to the fact that the frequency
response of the designed lowpass ﬁlter is not close to that of an ideal ﬁlter. □
□
EXAMPLE 9.5
Examine the frequency response of the lowpass ﬁlter used in the interpolation
of the signal in Example 10.4.
Solution
The second optional argument in the interp function provides the impulse
response from which we can compute the frequency response, as shown in the
following MATLAB script.
n = 0:256; x = cos(pi*n); w = [0:100]*pi/100;
Hf1 = figure(’units’,’inches’,’position’,[1,1,6,4],...
’paperunits’,’inches’,’paperposition’,[0,0,6,4]);
% (a) Interpolation by I = 2, L = 5;
I = 2; [y,h] = interp(x,I); H = freqz(h,1,w); H = abs(H);
subplot(2,2,1);
plot(w/pi,H,’g’); axis([0,1,0,I+0.1]); ylabel(’Magnitude’);
title(’I = 2, L = 5’,’fontsize’,TF);
set(gca,’xtick’,[0,0.5,1]); set(gca,’ytick’,[0:1:I]);
% (b) Interpolation by I = 4, L = 5;
I = 4; [y,h] = interp(x,I); H = freqz(h,1,w); H = abs(H);
subplot(2,2,2);
plot(w/pi,H,’g’); axis([0,1,0,I+0.2]); ylabel(’Magnitude’);
title(’I = 4, L = 5’,’fontsize’,TF);
set(gca,’xtick’,[0,0.25,1]); set(gca,’ytick’,[0:1:I]);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

476
Chapter 9
SAMPLING RATE CONVERSION
0
0.5
1
0
1
2
Magnitude
I = 2, L = 5
0
0.25
1
0
1
2
3
4
Magnitude
I = 4, L = 5
0 0.125
1
0
2
4
6
8
Magnitude
0.125
0
1
0
2
4
6
8
Magnitude
I = 8, L = 5
w/p
w/p
I = 8, L = 10
FIGURE 9.13
Filter frequency responses in Example 9.5
% (c) Interpolation by I = 8, L = 5;
I = 8; [y,h] = interp(x,I); H = freqz(h,1,w); H = abs(H);
subplot(2,2,3);
plot(w/pi,H,’g’); axis([0,1,0,I+0.4]); ylabel(’Magnitude’);
title(’I = 8, L = 5’,’fontsize’,TF); xlabel(’\omega/\pi’,’fontsize’,10)
set(gca,’xtick’,[0,0.125,1]); set(gca,’ytick’,[0:2:I]);
% (d) Interpolation by I = 8, L = 10;
I = 8; [y,h] = interp(x,I,10); H = freqz(h,1,w); H = abs(H);
subplot(2,2,4);
plot(w/pi,H,’g’); axis([0,1,0,I+0.4]); ylabel(’Magnitude’);
title(’I = 8, L = 10’,’fontsize’,TF); xlabel(’\omega/\pi’,’fontsize’,10)
set(gca,’xtick’,[0,0.125,1]); set(gca,’ytick’,[0:2:I]);
The frequency response plots are shown in Figure 9.13. The ﬁrst three plots
are for L = 5, and, as expected, the ﬁlters are all lowpass with passband edges
approximately around π/I frequencies and the gain of I. Also note that the
ﬁlters do not have sharp transitions and thus are not good approximations to
the ideal ﬁlter. The last plot shows the response for L = 10, which indicates
a more sharp transition, which is to be expected. Any value beyond L = 10
results in an unstable ﬁlter design and hence should be avoided.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling Rate Conversion by a Rational Factor I/D
477
9.4 SAMPLING RATE CONVERSION BY A RATIONAL FACTOR I/D
Having discussed the special cases of decimation (downsampling by a fac-
tor D) and interpolation (upsampling by a factor I), we now consider
the general case of sampling rate conversion by a rational factor I/D.
Basically, we can achieve this sampling rate conversion by ﬁrst perform-
ing interpolation by the factor I and then decimating the output of the
interpolator by the factor D. In other words, a sampling rate conversion
by the rational factor I/D is accomplished by cascading an interpolator
with a decimator, as illustrated in Figure 9.14.
We emphasize that the importance of performing the interpolation
ﬁrst and the decimation second is to preserve the desired spectral charac-
teristics of x(n). Furthermore, with the cascade conﬁguration illustrated
in Figure 9.14, the two ﬁlters with impulse response {hu(k)} and {hd(k)}
are operated at the same rate—namely, IFx—and hence can be combined
into a single lowpass ﬁlter with impulse response h(k), as illustrated in
Figure 9.15. The frequency response H(ωv) of the combined ﬁlter must
incorporate the ﬁltering operations for both interpolation and decimation,
and hence it should ideally possess the frequency-response characteristic
H(ωv) =

I, 0 ≤|ωv| ≤min(π/D, π/I)
0, otherwise
(9.36)
where ωv = 2πF/Fv = 2πF/IFx = ωx/I.
Explanation of (9.36)
Note that V (ωv) and hence W(ωv) in
Figure 9.15 are periodic with period 2π/I. Thus
• if D < I, then ﬁlter H(ωv) allows a full period through and there is no
net lowpass ﬁltering;
• if D > I, then ﬁlter must ﬁrst truncate the fundamental period of
W(ωv) to avoid aliasing error in the (D↓1) decimation stage to follow.
Putting these two observations together, we can state that when
D/I < 1, we have net interpolation and no smoothing is required by
IDEAL
LPF
hu(k)
IDEAL
LPF
hd(k)
I
D
y(m)
x(n)
v(k)
w(k)
Rate: Fx
IFx
IFx
IFx
Interpolator
Decimator
Fx = Fy
I
D
FIGURE 9.14
Cascade of interpolator and decimator for sampling rate conver-
sion by a factor I/D
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

478
Chapter 9
SAMPLING RATE CONVERSION
IDEAL
LPF
h(k)
I
D
y(m)
x(n)
v(k)
w(k)
Rate: Fx
IFx
IFx
Ideal Resampler
Fx = Fy
I
D
FIGURE 9.15
Method for sampling rate conversion by a factor I/D
H(ωv) other than to extract the fundamental period of W(ωv). In this
respect, H(ωv) acts as a lowpass ﬁlter as in the ideal interpolator. On
the other hand, if D/I > 1, then we have net decimation. Hence it is
necessary to ﬁrst truncate even the fundamental period of W(ωv) to get
the frequency band down to [−π/D, π/D] and to avoid aliasing in the
decimation that follows. In this respect, H(ωv) acts as a smoothing ﬁlter
in the ideal decimator. When D or I is equal to 1, the general deci-
mator/interpolator in Figure 9.15 along with (9.36) reduces to the ideal
interpolator or decimator as a special case, respectively.
In the time domain, the output of the upsampler is the sequence
v(k) =

x(k/I),
k = 0, ±I, ±2I, . . .
0,
otherwise
(9.37)
and the output of the linear time-invariant ﬁlter is
w(k) =
∞
	
ℓ=−∞
h(k −ℓ)v(ℓ) =
∞
	
ℓ=−∞
h(k −ℓI)x(ℓ)
(9.38)
Finally, the output of the sampling rate converter is the sequence {y(m)},
which is obtained by downsampling the sequence {w(k)} by a factor of
D. Thus
y(m) = w(mD) =
∞
	
ℓ=−∞
h(mD −ℓI)x(ℓ)
(9.39)
It is illuminating to express (9.39) in a diﬀerent form by making a
change in variable. Let
ℓ=
/mD
I
0
−n
(9.40)
where the notation⌊r⌋denotes the largest integer contained in r. With
this change in variable, (9.39) becomes
y(m) =
∞
	
n=−∞
h

mD −
/mD
I
0
I + nI

x
/mD
I
0
−n

(9.41)
We note that
mD −
/mD
I
0
I = (mD) modulo I = ((mD))I
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling Rate Conversion by a Rational Factor I/D
479
−3
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
8
9
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1
2
3
4
5
6
7
8
9
−2
−1
0
x(n)
v(k)
w(k)
y(m)
n
k
k
m
I = 3
I = 3
D = 2
1
2
3
−3
−4
4
−2
−1
0
1
2
3
FIGURE 9.16
Examples of signals x(n), v(k), w(k), and y(m) in the sampling
rate converter of Figure 9.15 for I = 3 and D = 2
Consequently, (9.41) can be expressed as
y(m) =
∞
	
n=−∞
h[nI + ((mD))I] x
/mD
I
0
−n

(9.42)
These operations are shown in Figure 9.16 for I = 3 and D = 2.
It is apparent from (9.41) and Figure 9.16 that the output y(m) is
obtained by passing the input sequence x(n) through a time-variant ﬁlter
with impulse response
g(n, m) = h[nI + ((mD))I]
−∞< m, n < ∞
(9.43)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

480
Chapter 9
SAMPLING RATE CONVERSION
where h(k) is the impulse response of the time-invariant lowpass ﬁlter
operating at the sampling rate IFx. We further observe that for any
integer k,
g(n, m + kI) = h[nI + ((mD + kDI))I] = h[nI + ((mD))I]
= g(n, m)
(9.44)
Hence g(n, m) is periodic in the variable m with period I.
Regarding the computational complexity of the lowpass ﬁlter in the
general resampler, we note that it has a nonzero input only every I sam-
ples and the output is required only every D samples. If we use an FIR im-
plementation for this lowpass ﬁlter, we need only compute its output one
out of every D samples. However, if we instead use IIR implementation,
we would generally have to compute intermediate outputs also because
of the recursive nature of the ﬁlter. However, both types of ﬁlter beneﬁt
from the computational savings due to their sparse input.
The frequency-domain representation of the resampled signal
y(m)
The frequency-domain relationships can be obtained by com-
bining the results of the interpolation and decimation process. Thus the
spectrum at the output of the linear ﬁlter with impulse response h(k) is
V (ωv) = H(ωv)X(ωvI)
=

IX(ωvI), 0 ≤|ωv| ≤min(π/D, π/I)
0,
otherwise
(9.45)
The spectrum of the output sequence y(m), obtained by decimating the
sequence v(n) by a factor of D, is
Y (ωy) = 1
D
D−1
	
k=0
V
ωy −2πk
D

(9.46)
where ωy = Dωv. Since the linear ﬁlter prevents aliasing as implied by
(9.45), the spectrum of the output sequence given by (9.46) reduces to
Y (ωy) =
⎧
⎪
⎨
⎪
⎩
I
DX
ωy
D

, 0 ≤|ωy| ≤min

π, πD
I

0,
otherwise
(9.47)
MATLAB Implementation
MATLAB provides the function [y,h]=
resample(x,I,D) that resamples the signal in array x at I/D times
the original sampling rate. The resulting resampled array y is I/D times
longer (or the ceiling of it if the ratio is not an integer)—that is,
length(y) = ceil(I/D)*length(x). The function approximates the
anti-aliasing (lowpass) ﬁlter given in (9.36) by an FIR ﬁlter, h, designed
(internally) using the Kaiser window. It also compensates for the ﬁlter’s
delay.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Sampling Rate Conversion by a Rational Factor I/D
481
The length of the FIR ﬁlter h that resample uses is proportional to
the fourth (optional) parameter L that has the default value of 10. For
L = 0, resample performs a nearest-neighbor interpolation. The ﬁfth op-
tional parameter beta (default value 5) can be used to specify the Kaiser
window stopband attenuation parameter β. The ﬁlter characteristics can
be studied using the impulse response h.
□
EXAMPLE 9.6
Consider the sequence x(n) = cos(0.125πn) discussed in Example 9.2. Change
its sampling rate by 3/2, 3/4, and 5/8.
Solution
The following MATLAB script shows the details.
n = 0:2048; k1 = 256; k2 = k1+32; m = 0:(k2-k1);
% (a) Original signal
x = cos(0.125*pi*n);
% (b) Sample rate conversion by 3/2: I= 3, D = 2
I = 3; D = 2; y = resample(x,I,D);
% (c) Sample rate conversion by 3/4: I= 3, D = 4
I = 3; D = 4; y = resample(x,I,D);
% (d) Sample rate conversion by 5/8: I= 5, D = 8
I = 5; D = 8; y = resample(x,I,D);
% Plotting commands follow
0
16
32
0
1
Amplitude
Original Sequence x(n)
Sample Rate I/D: I = 3, D = 2
−1
0
16
32
0
1
Amplitude
−1
0
16
32
0
1
Amplitude
−1
0
16
32
0
1
Amplitude
−1
Sample Rate I/D: I = 3, D = 4 
n
n
Sample Rate I/D: I = 5, D = 8
FIGURE 9.17
Original and resampled signals in Example 9.6
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

482
Chapter 9
SAMPLING RATE CONVERSION
The resulting plots are shown in Figure 9.17. The original x(n) signal has 16
samples in one period of the cosine waveform. Since the ﬁrst sampling rate
conversion by 3/2 is greater than 1, the overall eﬀect is to interpolate x(n).
The resulting signal has 16 × 3/2 = 24 samples in one period. The other two
sampling rate conversion factors are less than 1; thus the overall eﬀect is to
decimate x(n). The resulting signals have 16 × 3/4 = 12 and 16 × 5/8 = 10
samples per period, respectively.
□
9.5 FIR FILTER DESIGNS FOR SAMPLING RATE CONVERSION
In practical implementations of sampling rate converters, we must replace
the ideal lowpass ﬁlters of equations (9.20), (9.30), and (9.36) by a prac-
tical ﬁnite-order ﬁlter. The lowpass ﬁlter can be designed to have linear
phase, a speciﬁed passband ripple, and stopband attenuation. Any of the
standard, well-known FIR ﬁlter design techniques (e.g., window method,
frequency-sampling method) can be used to carry out this design. We
consider linear-phase FIR ﬁlters for this purpose because of their ease
of design and because they ﬁt very nicely into a decimator stage where
only one of D outputs is required [see the discussion following (9.44) on
page 480]. We will ﬁrst discuss integer interpolators, followed by integer
decimators and then the rational resamplers. The main emphasis will be
on the speciﬁcations of these FIR lowpass ﬁlters, since the design problem
has already been considered in Chapter 7.
9.5.1 FIR INTEGER INTERPOLATION
Replacing the ideal ﬁlter of the system given on page 473 with an FIR
ﬁlter, we obtain the system shown in Figure 9.18. The relevant equation
that relates the Fourier transforms V (ω) and X(ω) is (9.28), repeated
here for convenience:
V (ω) = X(ωI)
(9.48)
Considering the frequency compression by I and the required amplitude
scale factor of I, the ideal lowpass ﬁlter was determined in (9.30) and
(9.33) to be
HI(ω) =

I,
|ω| < π/I
0,
otherwise
(9.49)
FIR LPF
H(w)
  I
y(m)
x(n)
v(m)
Rate: Fx
IFx
IFx
FIR Interpolator
FIGURE 9.18
An FIR integer interpolator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
483
MATLAB Implementation
To design a linear-phase FIR ﬁlter for
use in interpolation (and, as we shall see later, for decimation) operation,
MATLAB provides the function h = intfilt(I,L,alpha). When used
on a sequence interspersed with I-1 consecutive zeros between every
I samples, the function performs ideal bandlimited interpolation using
the nearest 2*L nonzero samples. It assumes that the bandwidth of the
signal x(n) is alpha times π radians/sample—that is, alpha=1 means the
full signal bandwidth. The length of the ﬁlter impulse response array h
is 2*I*L-1. The designed ﬁlter is identical to that used by the interp
function. Therefore, the parameter L should be chosen carefully to avoid
numerical instability. It should be a smaller value for higher I value but
no more than 10.
□
EXAMPLE 9.7
Design a linear-phase FIR interpolation ﬁlter to interpolate a signal by a factor
of 4, using the bandlimited method.
Solution
We will explore the intfilt function for the design using L = 5 and study the
eﬀect of alpha on the ﬁlter design. The following MATLAB script provides the
detail.
I = 4; L = 5;
% (a) Full signal bandwidth: alpha = 1
alpha = 1; h = intfilt(I,L,alpha);
[Hr,w,a,L] = Hr_Type1(h); Hr_min = min(Hr); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
% (b) Partial signal bandwidth: alpha = 0.75
alpha = 0.75; h = intfilt(I,L,alpha);
[Hr,w,a,L] = Hr_Type1(h); Hr_min = max(Hr(end/2:end)); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
% Plotting commands follow
The plots are shown in Figure 9.19. For the full bandwidth case of alpha = 1,
the ﬁlter has more ripple in both the passband and the stopband with the
minimum stopband attenuation of 22 dB. This is because the ﬁlter transition
band is very narrow. For alpha = 0.75, the ﬁlter speciﬁcations are more lenient,
and hence its response is well behaved with minimum stopband attenuation of
40 dB. Note that we do not have complete control over other design parameters.
These issues are discussed in more detail further along in this section.
□
In the following example, we design a linear-phase equiripple FIR
interpolation ﬁlter using the Parks–McClellen algorithm.
□
EXAMPLE 9.8
Design an interpolator that increases the input sampling rate by a factor of
I = 5. Use the firpm algorithm to determine the coeﬃcients of the FIR ﬁlter
that has 0.1 dB ripple in the passband and is down by at least 30 dB in the
stopband. Choose reasonable values for band-edge frequencies.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

484
Chapter 9
SAMPLING RATE CONVERSION
0
0.25
1
0
4
Amplitude
Amplitude Response: Alpha = 1
Frequency in π Units
Frequency in π Units
0
0.25
1
0
4
Amplitude
Frequency in π Units
0
0.25
1
Frequency in π Units
0
0.25
1
−50
−22
0
Decibels
Log-Mag Response: Alpha = 1
Amplitude Response: Alpha = 0.75
−50
−40
0
Decibels
Log-Mag Response: Alpha = 0.75
FIGURE 9.19
FIR interpolation ﬁlter design plots for I = 4 and L = 5
Solution
The passband cutoﬀfrequency should be ωp = π/I = 0.2π. To get a reasonable
value for the ﬁlter length, we choose the transition width of 0.12π, which gives
stopband cutoﬀfrequency of ωs = 0.32π. Note that the nominal gain of the
ﬁlter in the passband should be equal to I = 5, which means that the ripple
values computed using the decibel values are scaled by 5. A ﬁlter of length
M = 31 achieves the design speciﬁcations given above. The details are given in
the following MATLAB script.
I = 5; Rp = 0.1; As = 30; wp = pi/I; ws = wp+pi*0.12;
[delta1,delta2] = db2delta(Rp,As); weights = [delta2/delta1,1];
F = [0,wp,ws,pi]/pi; A = [I,I,0,0];
h = firpm(30,F,A,weights); n = [0:length(h)-1];
[Hr,w,a,L] = Hr_Type1(h); Hr_min = min(Hr); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
The responses of the designed FIR ﬁlter are given in Figure 9.20. Even
though this ﬁlter passes the original signal, it is possible that some of the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
485
0
10
20
30
−0.5
0
0.5
1
1.5
Amplitude
n
Impulse Response
0
0.2 0.32
1
0
5
Amplitude
Amplitude Response
0
0.2 0.32
1
−50
−30
0
Decibels
Frequency in π Units
Frequency in π Units
Frequency in π Units
Log-Magnitude Response
0
0.2 0.32
1
−0.03
0
0.03
Scaled Ripples
Amplitude
−0.16
0
0.16
FIGURE 9.20
Responses of the FIR interpolation ﬁlter in Example 9.8
neighboring spectral energy may also leak through if the signal is of full
bandwidth of π radians. Hence we need better design speciﬁcations, which are
discussed further along in this section.
□
MATLAB Implementation
To use the FIR ﬁlter for interpolation
purposes (such as the one designed in Example 9.8), MATLAB has pro-
vided a general function, upfirdn, that can be used for interpolation
and decimation as well as for resampling purposes. Unlike other functions
discussed in this chapter, upfirdn incorporates the user-deﬁned FIR ﬁl-
ter (which need not be linear phase) in the operation. When invoked as
y = upfirdn(x,h,I), the function upsamples the input data in the array
x by a factor of the integer I and then ﬁlters the upsampled signal data
with the impulse response sequence given in the array h to produce the
output array y, thus implementing the system in Figure 9.18.
□
EXAMPLE 9.9
Let x(n) = cos(0.5πn). Increase the input sampling rate by a factor of I = 5,
using the ﬁlter designed in Example 9.8.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

486
Chapter 9
SAMPLING RATE CONVERSION
Solution
The steps are given in the following MATLAB script.
% Given Parameters:
I = 5; Rp = 0.1; As = 30; wp = pi/I; ws = 0.32*pi;
[delta1,delta2] = db2delta(Rp,As); weights = [delta2/delta1,1];
n = [0:50]; x = cos(0.5*pi*n);
n1 = n(1:17); x1 = x(17:33); % For plotting purposes
% Input signal plotting commands follow
% Interpolation with Filter Design: Length M = 31
M = 31; F = [0,wp,ws,pi]/pi; A = [I,I,0,0];
h = firpm(M-1,F,A,weights); y = upfirdn(x,h,I);
delay = (M-1)/2; % Delay imparted by the filter
m = delay+1:1:50*I+delay+1; y = y(m); m = 1:81; y = y(81:161); % for plotting
% Output signal plotting commands follow
The signal stem plots are shown in Figure 9.21. The upper left-hand plot shows
a segment of the input signal x(n), and the upper right-hand plot shows the
interpolated signal y(n) using the ﬁlter of length 31. The plot is corrected for
0
4
8
12
16
−1
0
1
Amplitude
n
Input Signal x(n)
0
20
40
60
80
Output y(n): I = 5, Filter Length = 31
m
0
20
40
60
80
m
−1
0
1
Amplitude
−1
0
1
Amplitude
−1
0
1
Amplitude
0
20
40
60
80
m
Output y(n): I = 5, Filter Length = 51
Output y(n): I = 5, Filter Length = 81
FIGURE 9.21
Signal plots in Example 9.9
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
487
ﬁlter delay and the eﬀect of its transient response. It is somewhat surprising that
the interpolated signal is not what it should be. The signal peak is more than
1, and the shape is distorted. A careful observation of the ﬁlter response plot in
Figure 9.20 reveals that the broad transition width and a smaller attenuation
has allowed some of the spectral energy to leak, creating a distortion.
To investigate this further, we designed ﬁlters with larger orders of 51 and
81, as detailed in the following MATLAB script.
% Interpolation with Filter Design: Length M = 51
M = 51; F = [0,wp,ws,pi]/pi; A = [I,I,0,0];
h = firpm(M-1,F,A,weights); y = upfirdn(x,h,I);
delay = (M-1)/2; % Delay imparted by the filter
m = delay+1:1:50*I+delay+1; y = y(m); m = 1:81; y = y(81:161);
% Plotting commands follow
% Interpolation with Filter Design: Length M = 81
M = 81; F = [0,wp,ws,pi]/pi; A = [I,I,0,0];
h = firpm(M-1,F,A,weights); y = upfirdn(x,h,I);
delay = (M-1)/2; % Delay imparted by the filter
m = delay+1:1:50*I+delay+1; y = y(m); m = 1:81; y = y(81:161);
% Plotting commands follow
The resulting signals are shown in lower plots in Figure 9.21. Clearly, for large
orders, the ﬁlter has better lowpass characteristics. The signal peak value ap-
proaches 1, and its shape approaches the cosine waveform. Thus a good ﬁlter
design is critical even in a simple signal case.
□
9.5.2 DESIGN SPECIFICATIONS
When we replace HI(ω) by a ﬁnite-order FIR ﬁlter H(ω), we must allow
for a transition band; thus the ﬁlter cannot have a passband edge up to
π/I. Toward this, we deﬁne
• ωx,p as the highest frequency of the signal x(n) that we want to pre-
serve, and
• ωx,s as the full signal bandwidth of x(n),—that is, there is no energy
in x(n) above the frequency ωx,s.
Thus we have 0 < ωx,p < ωx,s < π. Note that the parameters ωx,p and
ωx,s, as deﬁned, are signal parameters, not ﬁlter parameters; they are
shown in Figure 9.22a. The ﬁlter parameters will be deﬁned based on
ωx,p and ωx,s.
From equation (9.48), these signal parameter frequencies for v(m)
become ωx,p/I and ωx,s/I, respectively, because the frequency scale is
compressed by the factor I. This is shown in Figure 9.22b. A linear-phase
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

488
Chapter 9
SAMPLING RATE CONVERSION
X(wx)
wx,s
−wx,p
−wx,s
wx,p
wx
wy
−p
−p
p
p
V(wy)
0
0
(a)
(b)
We will allow filter to
substantially change
this band.
−2p
I
wx,p
I
wx,s
I
2p – wx,s
I
2p
I
−p
I
p
I
FIGURE 9.22
Frequency parameters: (a) signal, (b) ﬁlter
FIR ﬁlter can now be designed to pass frequencies up to ωx,p/I and to
suppress frequencies starting at (2π −ωx,s)/I. Let
ωp =
ωx,p
I

and
ωs =
2π −ωx,s
I

(9.50)
be the passband and stopband edge frequencies, respectively, of the low-
pass linear-phase FIR ﬁlter given by
H(ω) = Hr(ω)ejθ(ω)
(9.51)
where Hr(ω) is the real-valued amplitude response and θ(ω) is the un-
wrapped phase response. Then we have the following ﬁlter design speciﬁ-
cations:
1
I Hr(ω) ≤1 ± δ1 for |ω| ∈[0, ωp]
1
I Hr(ω) ≤±δ2
for |ω| ∈[ωs, π]
(9.52)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
489
where ωp and ωs are as given in (9.50) and δ1 and δ2 are the pass-
band and stopband ripple parameters, respectively, of the lowpass FIR
ﬁlter.
Comment:
Instead of beginning the stopband at π/I, we were able to
shift it to (2π −ωs) /I. If ωx,s ≪π, then this will be an important con-
sideration to lower ﬁlter order. However, in the worst-case scenario of
ωx,s = π, the stopband will begin at π
I , which is the same as in the ideal
lowpass ﬁlter of (9.49). Almost always, ωx,s < π, and we can then choose
ωx,p as close to ωx,s as we want. However, this will reduce the size of the
transition band, which means a higher ﬁlter order.
□
EXAMPLE 9.10
Design a better FIR lowpass ﬁlter for sampling rate increase by a factor of I = 5
for the signal in Example 9.9.
Solution
Since x(n) = cos(0.5πn), the signal bandwidth and bandwidth to be preserved
are the same—that is, ωx,p = ωx,s = 0.5π. Thus, from (9.50), ωp = 0.5π/5 =
0.1π and ωs = (2π −0.5π)/5 = 0.3π. We will design the ﬁlter for Rp = 0.01
and As = 50 dB. The resulting ﬁlter order is 32, which is 2 higher than the
one in Example 9.9 but with much superior attenuation. The details are given
below.
% Given Parameters:
n = [0:50]; wxp = 0.5*pi; x = cos(wxp*n);
n1 = n(1:9); x1 = x(9:17); % for plotting purposes
I = 5; I = 5; Rp = 0.01; As = 50; wp = wxp/I; ws = (2*pi-wxp)/I;
[delta1,delta2] = db2delta(Rp,As); weights = [delta2/delta1,1];
[N,Fo,Ao,weights] = firpmord([wp,ws]/pi,[1,0],[delta1,delta2],2);N = N+2;
% Input signal plotting commands follow
% Interpolation with Filter Design: Length M = 31
h = firpm(N,Fo,I*Ao,weights); y = upfirdn(x,h,I);
delay = (N)/2; % Delay imparted by the filter
m = delay+1:1:50*I+delay+1; y = y(m); m = 0:40; y = y(81:121);
% Output signal plotting commands follow
[Hr,w,a,L] = Hr_Type1(h); Hr_min = min(Hr); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
% Filter design plotting commands follow
The signal stem plots and ﬁlter design plots are shown in Figure 9.23. The
designed ﬁlter has a minimum stopband attenuation of 53 dB, and the resulting
interpolation is accurate even with the ﬁlter order of 32.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

490
Chapter 9
SAMPLING RATE CONVERSION
0
4
8
−1
0
1
Amplitude
−1
0
1
Amplitude
n
Input Signal x(n)
0
20
40
Output Signal y(n): I = 5
m
0 0.1
0.3
1
0
5
Amplitude
Amplitude Response
Frequency in π Units
Frequency in π Units
0 0.1
0.3
1
−60
−53
0
Decibels
Log-Magnitude Response
FIGURE 9.23
Signal plots and ﬁlter design plots in Example 9.10
9.5.3 FIR INTEGER DECIMATION
Consider the system in Figure 9.6 on page 466 in which the ideal lowpass
ﬁlter is replaced by an FIR ﬁlter H(ω), which then results in the system
shown in Figure 9.24. The relationship between Y (ωy) and X(ω) is given
by (9.24), which is repeated here for convenience:
Y (ωy) = 1
D
D−1
	
k=0
H

ω −2πk
D

X

ω −2πk
D

;
ω = ωy
D
(9.53)
FIR LPF
H(w)
y(m)
x(n)
v(n)
Rate: Fx
Fx
FIR Decimator
= Fy
Fx
D
D
FIGURE 9.24
An FIR integer decimator
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
491
which is nothing but the aliased sum of the H(ω)X(ω). Thus the condition
necessary to avoid aliasing is
H(ω)X(ω) = 0
for
π
D ≤|ω| ≤π
(9.54)
Then
Y (ωy) = 1
DX(ω)H(ω)
(9.55)
as in (9.25), where the ideal ﬁltering was accomplished with HD(ω) as
given in (9.20).
□
EXAMPLE 9.11
Design a decimator that downsamples an input signal x(n) by a factor D = 2.
Use the firpm algorithm to determine the coeﬃcients of the FIR ﬁlter that has
a 0.1 dB ripple in the passband and is down by at least 30 dB in the stopband.
Choose reasonable values for band-edge frequencies.
Solution
The passband cutoﬀfrequency should be ωp = π/D = 0.5π. To get a reasonable
value for the ﬁlter length, we choose the transition width of 0.1π, which gives
stopband a cutoﬀfrequency of ωs = 0.3π. A ﬁlter of length M = 37 achieves the
preceding design speciﬁcations. The details are given in the following MATLAB
script.
% Filter Design
D = 2; Rp = 0.1; As = 30; wp = pi/D; ws = wp+0.1*pi;
[delta1,delta2] = db2delta(Rp,As);
[N,F,A,weights] = firpmord([wp,ws]/pi,[1,0],[delta1,delta2],2);
h = firpm(N,F,A,weights); n = [0:length(h)-1];
[Hr,w,a,L] = Hr_Type1(h); Hr_min = min(Hr); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
% Plotting commands follow
The responses of the designed FIR ﬁlter are given in Figure 9.25. This ﬁlter
passes the signal spectrum over the passband [0, π/2] without any distortion.
However, since the transition width is not very narrow, it is possible that some
of the signal over the transition band may alias into the band of interest. Also,
the 30 db attenuation may allow a small fraction of the signal spectrum from the
stopband into the passband after downsampling. Therefore, we need a better
approach for ﬁlter speciﬁcations, as discussed further along in this section.
□
MATLAB Implementation
As discussed, the upfirdn function can
also be used for implementing the user-designed FIR ﬁlter in the decima-
tion operation. When invoked as y = upfirdn(x,h,1,D), the function
ﬁlters the signal data in the array x with the impulse response given in the
array h and then downsamples the ﬁltered data by the integer factor D to
produce the output array y, thus implementing the system in Figure 9.24.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

492
Chapter 9
SAMPLING RATE CONVERSION
0
36
0
0.5
Amplitude
n
Impulse Response
0
0.5 0.6
1
0
1
Amplitude
Amplitude Response
Frequency in π Units
0
0.5 0.6
1
−50
−31
0
Decibels
Frequency in π Units
Log-Magnitude Response
0
0.5 0.6
−0.006
0
0.006
Unweighted Ripples
Amplitude
Frequency in π Units
1
−0.03
0
0.03
FIGURE 9.25
Responses of the FIR decimation ﬁlter in Example 9.11
□
EXAMPLE 9.12
Using the ﬁlter designed in Example 9.11, decimate sinusoidal signals x1(n) =
cos(πn/8) and x2(n) = cos(πn/2) with frequencies within the passband of
the ﬁlter. Verify the performance of the FIR ﬁlter and the results of the
decimation.
Solution
The following MATLAB script provides the details.
% Given Parameters:
D = 2; Rp = 0.1; As = 30; wp = pi/D; ws = wp+0.1*pi;
% Filter Design
[delta1,delta2] = db2delta(Rp,As);
[N,F,A,weights] = firpmord([wp,ws]/pi,[1,0],[delta1,delta2],2);
h = firpm(N,F,A,weights); delay = N/2; % Delay imparted by the filter
% Input signal x1(n) = cos(2*pi*n/16)
n = [0:256]; x = cos(pi*n/8);
n1 = n(1:33); x1 = x(33:65); % for plotting purposes
% Input signal plotting commands follow
% Decimation of x1(n): D = 2
y = upfirdn(x,h,1,D);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
493
m = delay+1:1:128/D+delay+1; y = y(m); m = 0:16; y = y(16:32);
% Output signal plotting commands follow
% Input signal x2(n) = cos(8*pi*n/16)
n = [0:256]; x = cos(8*pi*n/(16));
n1 = n(1:33); x1 = x(33:65); % for plotting purposes
% Input signal plotting commands follow
% Decimation of x2(n): D = 2
y = upfirdn(x,[h],1,D); %y = downsample(conv(x,h),2);
m = delay+1:1:128/D+delay+1; y = y(m); m = 0:16; y = y(16:32);
% Output signal plotting commands follow
The signal stem plots are shown in Figure 9.26. The left-side plots show the
signal x1(n) and the corresponding decimated signal y1(n), and the right-side
plots show the same for x2(n) and y2(n). In both cases, the decimation appears
to be correct. If we had chosen any frequency above π/2, then the ﬁlter would
have attenuated or eliminated the signal.
□
Input Signal: x1(n) = cos(pn/8)
0
4
8
12
16
0
8
16
24
32
0
1
Amplitude
n
−1
0
1
Amplitude
−1
0
8
16
24
32
0
1
Amplitude
n
−1
0
1
Amplitude
−1
Output Signal: y1(n): D = 2
m
0
4
8
12
16
m
Input Signal: x2(n) = cos(pn/2)
Output Signal: y2(n): D = 2
FIGURE 9.26
Signal plots in Example 9.12
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

494
Chapter 9
SAMPLING RATE CONVERSION
9.5.4 DESIGN SPECIFICATIONS
When we replace the ideal lowpass ﬁlter HD(ω) by a ﬁnite-order FIR ﬁlter
H(ω), we must allow for a transition band. Again we deﬁne
• ωx,p as the signal bandwidth to be preserved, and
• ωx,s as the frequency above which aliasing error is tolerated.
Then we have 0 < ωx,p ≤ωx,s ≤π/D. If we choose ωx,s = π/D, then
the decimator will give no aliasing error. If we choose ωx,s = ωx,p, then
the band above the signal band will contain aliasing errors. With these
deﬁnitions and observations, we can now specify the desired ﬁlter speci-
ﬁcations. The ﬁlter must pass frequencies up to ωx,p, and its stopband
must begin at
 2π
D −ωx,s

and continue up to π. Then none of the k ̸= 0
terms in (9.53)—that is, the “aliases”—will cause appreciable distortion
in the band up to ωx,s. Let
ωp = ωx,p
and
ωs =
2π
D −ωx,s

(9.56)
be the passband and stopband edge frequencies, respectively, of the low-
pass linear-phase FIR ﬁlter given in (9.51). Then we have the following
ﬁlter design speciﬁcations:
Hr(ω) ≤1 ± δ1 for |ω| ∈[0, ωp]
Hr(ω) ≤±δ2
for |ω| ∈[ωs, π]
(9.57)
where ωp and ωs are as given in (9.56) and δ1 and δ2 are the passband and
stopband ripple parameters of the lowpass FIR ﬁlter, respectively. Note
that it does not matter what the spectrum X(ω) is. We simply require
that the product X(ω)H(ω) be very small beginning at ω| = 2π/D −ωx,s
so that k ̸= 0 terms in (9.53) do not provide signiﬁcant contribution in
the band [−ωx,s, ωx,s], which is required to be free of aliasing.
Significance of δ1 and δ2
The ﬁlter ripple parameters δ1 and δ2 have
the following signiﬁcance, which must be taken into consideration while
specifying their values:
• The passband ripple δ1 measures the ripple in the passband and hence
controls the distortion in the signal bandwidth ωp.
• The stopband ripple δ2 controls the amount of aliased energy (also
called leakage) that gets into the band up to ωx,s.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
495
There are (D −1) contributions due to k ̸= 0 terms in (9.53). These
are expected to add incoherently (i.e., have peaks at diﬀerent locations),
so the overall peak error should be about δ2. The actual error depends
on how X(ω) varies over the rest of the band |ω| > ωx,p. Clearly,
the ﬁlter stopband ripple δ2 controls the aliasing error in the signal
passband. Therefore, both δ1 and δ2 affect the decimated signal in its
passband.
Comment:
Comparing the FIR decimator ﬁlter speciﬁcations (9.57) to
those for the FIR interpolator in (9.52), we see a high degree of similarity.
In fact, a ﬁlter designed to decimate by factor D can also be used to
interpolate by the factor I = D, as we see from the following example.
This means that the function intfilt can also be used to design FIR
ﬁlters for decimation.
□
EXAMPLE 9.13
To design a decimate by D stage, we need values for ωx,p and ωx,s (remember
that these are signal parameters). Assume ωx,p = π/(2D), which satisﬁes the
constraint ωx,p ≤π/D and is exactly half the decimated bandwidth. Let ωx,s =
ωx,p. Then the FIR lowpass ﬁlter must pass frequencies up to ωp = π/(2D) and
stop frequencies above ωs = 2π/D −π/(2D) = 3π/(2D).
Now consider the corresponding interpolation problem. We want to inter-
polate by I. We again choose ωx,s = ωx,p, but now the range is ωx,p < π. If we
take exactly half this band, we get ωx,p = π/2. Then according to the speciﬁ-
cations (9.52) for the interpolation, we want the ﬁlter to pass frequencies up to
π/2I and to stop above 3π/2I. Thus for I = D, we have the same ﬁlter speci-
ﬁcations, so the same ﬁlter could serve both the decimation and interpolation
problems.
□
□
EXAMPLE 9.14
Design a decimation FIR ﬁlter for the signal x1(n) in Example 9.12 that has a
better stopband attenuation of As = 50 dB and a lower ﬁlter order.
Solution
The signal bandwidth is ωx,p = π/8, and we will choose ωx,s = π/D = π/2.
Then ωp = π/8 and ωs = (2π/D) −ωx,s = π/2. With these parameters the
optimum FIR ﬁlter length is 13, which is much lower than the previous one of
37 with a higher attenuation.
MATLAB script:
% Given Parameters:
D = 2; Rp = 0.1; As = 50; wxp = pi/8; wxs = pi/D; wp = wxp; ws = (2*pi/D)-wxs;
% Filter Design
[delta1,delta2] = db2delta(Rp,As);
[N,F,A,weights] = firpmord([wp,ws]/pi,[1,0],[delta1,delta2],2); N = ceil(N/2)*2;
h = firpm(N,F,A,weights); delay = N/2; % Delay imparted by the filter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

496
Chapter 9
SAMPLING RATE CONVERSION
0
8
16
24
32
−1
0
1
Amplitude
−1
0
1
Amplitude
n
Input Signal: x(n) = cos(pn/8)
0
4
8
12
16
Output Signal y(n): D = 2
m
0 0.125
0.5
1
0
1
Amplitude
Amplitude Response
Frequency in π Units
0 0.125
0.5
1
Frequency in π Units
−60
−51
0
Decibels
Log-Magnitude Response
FIGURE 9.27
Signal plots and ﬁlter design plots in Example 9.14
% Input signal x(n) = cos(2*pi*n/16)
n = [0:256]; x = cos(pi*n/8);
n1 = n(1:33); x1 = x(33:65); % for plotting purposes
% Input signal plotting commands follow
% Decimation of x(n): D = 2
y = upfirdn(x,h,1,D);
m = delay+1:1:128/D+delay+1; y1 = y(m); m = 0:16; y1 = y1(14:30);
% Output signal plotting commands follow
% Filter Design Plots
[Hr,w,a,L] = Hr_Type1(h); Hr_min = min(Hr); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
% Filter design plotting commands follow
The signal stem plots and the ﬁlter responses are shown in Figure 9.27. The
designed ﬁlter achieves an attenuation of 51 dB, and the decimated signal is
correct.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
497
9.5.5 FIR RATIONAL-FACTOR RATE CONVERSION
Replacing the ideal ﬁlter of the system given on page 478 with an FIR
ﬁlter H(ω), we obtain the system shown in Figure 9.28. In this case, the
relevant ideal lowpass ﬁlter is given by (9.36), which is repeated here for
convenience:
H(ω) =

I, 0 ≤|ω| ≤min(π/D, π/I)
0, otherwise
(9.58)
For the signal x(n), we deﬁne
• ωx,p as the signal bandwidth that should be preserved,
• ωx,s1 as the overall signal bandwidth, and
• ωx,s2 as the signal bandwidth that is required to be free of aliasing error
after resampling.
Then we have
0 < ωx,p ≤ωx,s2 ≤Iπ
D
and
ωx,s1 ≤π
(9.59)
Now, for the interpolation part, the lowpass ﬁlter must pass frequencies
up to ωx,p/I and attenuate frequencies starting at (2π/I −ωx,s1/I). The
decimation part of the ﬁlter must again pass frequencies up to ωx,p/I
but attenuate frequencies above (2π/D −ωx,s2/I). Therefore, the stop-
band must start at the lower of these two values. Deﬁning ﬁlter cutoﬀ
frequencies as
ωp =
ωx,p
I

and
ωs = min
2π
I −ωx,s1
I
, 2π
D −ωx,s2
I

(9.60)
and the corresponding ripple parameters as δ1 and δ2, we have the fol-
lowing ﬁlter speciﬁcations:
1
I Hr(ω) ≤1 ± δ1 for |ω| ∈[0, ωp]
1
I Hr(ω) ≤±δ2
for |ω| ∈[ωs, π]
(9.61)
FIR LPF
H(w)
y(m)
x(n)
Rate: Fx
IFx
IFx
FIR Resampler
Fx = Fy
I
D
D
I
FIGURE 9.28
An FIR rational-factor resampler
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

498
Chapter 9
SAMPLING RATE CONVERSION
where Hr(ω) is the amplitude response. Note that if we set ωx,s1 = π and
ωx,s2 = Iπ/D, which are their maximum values, then we get the ideal
cutoﬀfrequency max[π/I, π/D], as given before in (9.36).
MATLAB Implementation Clearly, the upfirdn function implements
all the necessary operations needed in the rational sampling rate conver-
sion system shown in Figure 9.28. When invoked as y = upfirdn(x,h,
I,D), it performs a cascade of three operations: upsampling the input data
array x by a factor of the integer I, FIR ﬁltering the upsampled signal data
with the impulse response sequence given in the array h, and, ﬁnally, down-
sampling the result by a factor of the integer D. Using a well-designed ﬁlter,
we have complete control over the sampling rate conversion operation.
□
EXAMPLE 9.15
Design a sampling rate converter that increases the sampling rate by a factor of
2.5. Use the firpm algorithm to determine the coeﬃcients of the FIR ﬁlter that
has 0.1 dB ripple in the passband and is down by at least 30 dB in the stopband.
Solution
The FIR ﬁlter that meets the speciﬁcations of this problem is exactly the same
as the ﬁlter designed in Example 9.8. Its bandwidth is π/5.
□
□
EXAMPLE 9.16
A signal x(n) has a total bandwidth of 0.9π. It is resampled by a factor of
4/3 to obtain y(m). We want to preserve the frequency band up to 0.8π and
require that the band up to 0.7π be free of aliasing. Using the Parks–McClellan
algorithm, determine the coeﬃcients of the FIR ﬁlter that has 0.1 dB ripple in
the passband and 40 dB attenuation in the stopband.
Solution
The overall signal bandwidth is ωx,s1 = 0.9π, the bandwidth to be preserved is
ωx,p = 0.8π, and the bandwidth above which aliasing is tolerated is ωx,s2 = 0.7π.
From (9.60) and using I = 4 and D = 3, the FIR ﬁlter design parameters are
ωp = 0.2π and ωs = 0.275π. With these parameters, along with the passband
ripple of 0.1 dB and stopband attenuation of 40 dB, the optimum FIR ﬁlter
length is 58. The details and computation of design plots follow.
% Given Parameters:
I = 4; D = 3; Rp = 0.1; As = 40;
wxp = 0.8*pi; wxs1 = 0.9*pi; wxs2 = 0.7*pi;
% Computed Filter Parameters
wp = wxp/I; ws = min((2*pi/I-wxs1/I),(2*pi/D-wxs2/I));
% Filter Design
[delta1,delta2] = db2delta(Rp,As);
[N,F,A,weights] = firpmord([wp,ws]/pi,[1,0],[delta1,delta2],2);
N = ceil(N/2)*2+1; h = firpm(N,F,I*A,weights);
[Hr,w,a,L] = Ampl_res(h); Hr_min = min(Hr); w_min = find(Hr == Hr_min);
H = abs(freqz(h,1,w)); Hdb = 20*log10(H/max(H)); min_attn = Hdb(w_min);
% Plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Designs for Sampling Rate Conversion
499
4
0
0
−40
−60
0
0.2
0.275
1
1
Amplitude
Decibels
Frequency in π Units
Amplitude Response
0
0.2
0.275
Frequency in π Units
Log-Magnitude Response
FIGURE 9.29
The ﬁlter design plots in Example 9.16
The ﬁlter responses are shown in Figure 9.29, which shows that the designed
ﬁlter achieves the attenuation of 40 dB.
□
9.5.6 FIR FILTERS WITH MULTIPLE STOPBANDS
We now discuss the use of multiple stopbands in the design of FIR in-
teger interpolators when the low sampling rate is more than two times
that required. Let us refer back to the Figure 9.22b on page 488, which
illustrates a typical spectrum V (ω) in integer interpolators. We could
use a lowpass ﬁlter with multiple stopbands of bandwidth ωs/I cen-
tered at 2πk/I for k ̸= 0. For I = 4, such a spectrum is shown in
Figure 9.30(a), and the corresponding ﬁlter speciﬁcations are shown in
Figure 9.30b.
Clearly, these ﬁlter speciﬁcations diﬀer from those given in (9.52) on
page 488 in that the stopband is no longer one contiguous interval. Now,
if ωs < π/2, then there is a practical advantage to using this multiband
design because it results in a lower-order ﬁlter [9]. For π ≥ωs > π/2, the
single-band lowpass ﬁlter speciﬁcation (9.52) is easier and works as well.
Similar advantages can be obtained for FIR integer decimators. We
again ﬁnd that we can substitute a multiple-stopband lowpass ﬁlter
for the single-stopband design given in (9.57). With reference to the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

500
Chapter 9
SAMPLING RATE CONVERSION
0
V(w)
w
w
p
p
wx,p
I
d2
1 + d1
1 − d1
1
0
wx,p
I
2wx,s
I
wx,s
I
2p − wx,s
I
2p + wx,s
I
2p − wx,s
I
2p + wx,s
I
4p − wx,s
I
4p − wx,s
I
2p
I
3p
I
p
I
H(w)
1
I
(a)
(b)
FIGURE 9.30
Multiple-stopband design: (a) signal spectrum, (b) ﬁlter speciﬁca-
tions
signal speciﬁcations on page 494, we note that only part of the bands
[π/D, 3π/D], [3π/D, 5π/D], . . . and so on will get aliased into [−ωs, +ωs].
Therefore, the multiple stopbands are given by [(2π/D)−ωs, (2π/D)+ωs],
[(4π/D) −ωs, (4π/D) + ωs], and so on, centered at 2πk/D, k ̸= 0. Once
again, there are practical advantages when ωs < π/2M.
9.6 FIR FILTER STRUCTURES FOR SAMPLING RATE CONVERSION
As indicated in the discussion in Section 9.4, sampling rate conversion
by a factor I/D can be achieved by ﬁrst increasing the sampling rate by
I, accomplished by inserting I −1 zeros between successive values of the
input signal x(n), followed by linear ﬁltering of the resulting sequence to
eliminate the unwanted images of X(ω), and ﬁnally by downsampling the
ﬁltered signal by the factor D. In this section, we consider the design and
implementation of the linear ﬁlter. We begin with the simplest structure,
which is the direct form FIR ﬁlter structure, and develop its computation-
ally eﬃcient implementation. We then consider another computationally
eﬃcient structure called the polyphase structure, which is used in the im-
plementation of the MATLAB functions resample and upfirdn. Finally,
we close this section by discussing the time-variant ﬁlter structures for
the general case of sampling rate conversion.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures for Sampling Rate Conversion
501
9.6.1 DIRECT FORM FIR FILTER STRUCTURES
In principle, the simplest realization of the ﬁlter is the direct form FIR
structure with system function
H(z) =
M−1
	
k=0
h(k)z−k
(9.62)
where h(k) is the unit sample response of the FIR ﬁlter. After design-
ing the ﬁlter as discussed in the previous section, we will have the ﬁlter
parameters h(k), which allow us to implement the FIR ﬁlter directly, as
shown in Figure 9.31.
Although the direct form FIR ﬁlter realization illustrated in
Figure 9.31 is simple, it is also very ineﬃcient. The ineﬃciency results
from the fact that the upsampling process introduces I −1 zeros between
successive points of the input signal. If I is large, most of the signal
components in the FIR ﬁlter are zero. Consequently, most of the multi-
plications and additions result in zeros. Furthermore, the downsampling
process at the output of the ﬁlter implies that only one out of every D
output samples is required at the output of the ﬁlter. Consequently, only
one out of every D possible values at the output of the ﬁlter should be
computed.
To develop a more eﬃcient ﬁlter structure, let us begin with a decima-
tor that reduces the sampling rate by an integer factor D. From our pre-
vious discussion, the decimator is obtained by passing the input sequence
Upsampler
Downsampler
x(n)
h(0)
y(m)
h(1)
z −1
z −1
h(M − 2)
h(M − 1)
I
3
2
1
D
FIGURE 9.31
Direct form realization of FIR ﬁlter in sampling rate conversion
by a factor I/D
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

502
Chapter 9
SAMPLING RATE CONVERSION
h(0)
x(n)
y(m)
h(1)
z−1
z−1
h(M − 2)
h(M − 1)
(a)
(b)
h(0)
x(n)
y(m)
h(1)
z−1
z−1
h(M − 2)
h(M − 1)
D
D
D
D
D
FIGURE 9.32
Decimation by a factor D: (a) standard realization, (b) eﬃcient
realization
x(n) through an FIR ﬁlter and then downsampling the ﬁlter output by
a factor D, as illustrated in Figure 9.32a. In this conﬁguration, the ﬁlter
is operating at the high sampling rate Fx, while only one out of every
D output samples is actually needed. The logical solution to this ineﬃ-
ciency problem is to embed the downsampling operation within the ﬁlter,
as illustrated in the ﬁlter realization given in Figure 9.32b. In this ﬁl-
ter structure, all the multiplications and additions are performed at the
lower sampling rate Fx/D. Thus we have achieved the desired eﬃciency.
Additional reduction in computation can be achieved by exploiting the
symmetry characteristics of {h(k)}. Figure 9.33 illustrates an eﬃcient re-
alization of the decimator in which the FIR ﬁlter has linear phase and
hence {h(k)} is symmetric.
Next, let us consider the eﬃcient implementation of an interpolator,
which is realized by ﬁrst inserting I −1 zeros between samples of x(n)
and then ﬁltering the resulting sequence. The direct form realization is
illustrated in Figure 9.34. The major problem with this structure is that
the ﬁlter computations are performed at the high sampling rate of IFx.
The desired simpliﬁcation is achieved by ﬁrst using the transposed form
of the FIR ﬁlter, as illustrated in Figure 9.35a, and then embedding the
upsampler within the ﬁlter, as shown in Figure 9.35b. Thus all the ﬁlter
multiplications are performed at the low rate Fx, while the upsampling
process introduces I−1 zeros in each of the ﬁlter branches of the structure
shown in Figure 9.35b. The reader can easily verify that the two ﬁlter
structures in Figure 9.35 are equivalent.
It is interesting to note that the structure of the interpolator, shown
in Figure 9.35b, can be obtained by transposing the structure of the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures for Sampling Rate Conversion
503
h(0)
h(1)
h(M − 2)
h(M − 1)
x(n)
y(m)
z −1
z −1
z −1
z −1
z −1
D
D
D
D
D
D
D
D
FIGURE 9.33
Eﬃcient realization of a decimator that exploits the symmetry in
the FIR ﬁlter
h(0)
v(n)
y(m)
x(n)
h(1)
z −1
z −1
h(M − 2)
h(M − 1)
I
FIGURE 9.34
Direct form realization of FIR ﬁlter in interpolation by a factor I
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

504
Chapter 9
SAMPLING RATE CONVERSION
h(0)
v(n)
y(m)
x(n)
h(1)
z −1
z −1
h(M − 2)
h(M − 1)
(a)
(b)
h(0)
y(m)
x(n)
h(1)
z −1
z −1
h(M − 2)
h(M − 1)
I
I
I
I
I
FIGURE 9.35
Eﬃcient realization of an interpolator
y(m)
x(n)
Input
(a)
Output
y(m)
x(n)
Input
(c)
Output
y(m)
x(n)
Output
(b)
Input
y(m)
x(n)
Output
(d)
Input
D
I
D
D
I
FIGURE 9.36
Duality relationships obtained through transpositions
decimator shown in Figure 9.32. We observe that the transpose of a deci-
mator is an interpolator, and vice versa. These relationships are illustrated
in Figure 9.36, where part b is obtained by transposing part a and part d
is obtained by transposing part c. Consequently, a decimator is the dual
of an interpolator, and vice versa. From these relationships, it follows
that there is an interpolator whose structure is the dual of the decimator
shown in Figure 9.33, which exploits the symmetry in h(n).
9.6.2 POLYPHASE FILTER STRUCTURE
The computational eﬃciency of the ﬁlter structure shown in Figure 9.35
can also be achieved by reducing the large FIR ﬁlter of length M into
a set of smaller ﬁlters of length K = M/I, where M is selected to be a
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures for Sampling Rate Conversion
505
multiple of I. To demonstrate this point, let us consider the interpolator
given in Figure 9.34. Since the upsampling process inserts I −1 zeros
between successive values of x(n), only K out of the M input values
stored in the FIR ﬁlter at any one time are nonzero. At one time instant,
these nonzero values coincide and are multiplied by the ﬁlter coeﬃcients
h(0), h(I), h(2I), . . . , h(M −I). In the following time instant, the non-
zero values of the input sequence coincide and are multiplied by the ﬁlter
coeﬃcients h(1), h(I + 1), h(2I + 1), and so on. This observation leads us
to deﬁne a set of smaller ﬁlters, called polyphase ﬁlters, with unit sample
responses
pk(n) = h(k + nI);
k = 0, 1, . . . , I −1,
n = 0, 1, . . . , K −1
(9.63)
where K = M/I is an integer.
From this discussion, it follows that the set of I polyphase ﬁlters can
be arranged as a parallel realization, and the output of each ﬁlter can be
selected by a commutator, as illustrated in Figure 9.37. The rotation of
the commutator is in the counterclockwise direction, beginning with the
point at m = 0. Thus the polyphase ﬁlters perform the computations at
the low sampling rate Fx, and the rate conversion results from the fact
that I output samples are generated, one from each of the ﬁlters, for each
input sample.
The decomposition of {h(k)} into the set of I subﬁlters with impulse
response pk(n), k = 0, 1, . . . , I −1, is consistent with our previous observa-
tion that the input signal was being ﬁltered by a periodically time-variant
linear ﬁlter with impulse response
g(n, m) = h(nI + (mD)I)
(9.64)
p0(n)
x(n)
Rate: Fx
Rate: Fx
Rate: Fy = IFx
Rate:
Fy = IFx
y(m)
p1(n)
pI − 1(n)
FIGURE 9.37
Interpolation by use of polyphase ﬁlters
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

506
Chapter 9
SAMPLING RATE CONVERSION
where D = 1 in the case of the interpolator. We noted previously that
g(n, m) varies periodically with period I. Consequently, a diﬀerent set of
coeﬃcients is used to generate the set of I output samples y(m), m =
0, 1, . . . , I −1.
Additional insight can be gained about the characteristics of the set
of polyphase subﬁlters by noting that pk(n) is obtained from h(n) by
decimation with a factor I. Consequently, if the original ﬁlter frequency
response H(ω) is ﬂat over the range 0 ≤|ω| ≤ω/I, each of the polyphase
subﬁlters possesses a relatively ﬂat response over the range 0 ≤|ω| ≤π
(i.e., the polyphase subﬁlters are basically allpass ﬁlters and diﬀer pri-
marily in their phase characteristics). This explains the reason for using
the term polyphase in describing these ﬁlters.
The polyphase ﬁlter can also be viewed as a set of I subﬁlters con-
nected to a common delay line. Ideally, the kth subﬁlter will generate a
forward time shift of (k/I)Tx, for k = 0, 1, 2, . . . , I −1, relative to the
zeroth subﬁlter. Therefore, if the zeroth ﬁlter generates zero delay, the
frequency response of the kth subﬁlter is
pk(ω) = ejωk/I
A time shift of an integer number of input sampling intervals (e.g., kTx)
can be generated by shifting the input data in the delay line by I samples
and using the same subﬁlters. By combining these two methods, we can
generate an output that is shifted forward by an amount (k + i/I)Tx
relative to the previous output.
By transposing the interpolator structure in Figure 9.37, we obtain
a commutator structure for a decimator based on the parallel bank of
polyphase ﬁlters, as illustrated in Figure 9.38. The unit sample responses
of the polyphase ﬁlters are now deﬁned as
pk(n) = h(k + nD);
k = 0, 1, . . . , D −1,
n = 0, 1, . . . , K −1
(9.65)
where K = M/D is an integer when M is selected to be a multiple of
D. The commutator rotates in a counterclockwise direction, starting with
the ﬁlter p0(n) at m = 0.
Although the two commutator structures for the interpolator and the
decimator just described rotate in a counterclockwise direction, it is also
possible to derive an equivalent pair of commutator structures having a
clockwise rotation. In this alternative formulation, the sets of polyphase
ﬁlters are deﬁned to have impulse responses
pk(n) = h(nI −k),
k = 0, 1, . . . , I −1
(9.66)
and
pk(n) = h(nD −k),
k = 0, 1, . . . , D −1
(9.67)
for the interpolator and decimator, respectively.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures for Sampling Rate Conversion
507
p0(n)
x(n)
Rate: Fx
Rate: Fy
y(m)
p1(n)
pI − 1(n)
Rate: Fy = Fx
D
FIGURE 9.38
Decimation by use of polyphase ﬁlters
□
EXAMPLE 9.17
For the decimation ﬁlter designed in Example 9.11, determine the polyphase
ﬁlter coeﬃcients {pk(n)} in terms of the FIR ﬁlter coeﬃcients {h(n)}.
Solution
The polyphase ﬁlters obtained from h(n) have impulse responses
pk(n) = h(2n + k)
k = 0, 1;
n = 0, 1, . . . , 14
Note that p0(n) = h(2n) and p1(n) = h(2n + 1). Hence one ﬁlter consists of
the even-numbered samples of h(n), and the other ﬁlter consists of the odd-
numbered samples of h(n).
□
□
EXAMPLE 9.18
For the interpolation ﬁlter designed in Example 9.8, determine the polyphase
ﬁlter coeﬃcients {pk(n)} in terms of the ﬁlter coeﬃcients {h(n)}.
Solution
The polyphase ﬁlters obtained from h(n) have impulse responses
pk(n) = h(5n + k)
k = 0, 1, 2, 3, 4
Consequently, each ﬁlter has length 6.
□
9.6.3 TIME-VARIANT FILTER STRUCTURES
Having described the ﬁlter implementation for a decimator and an inter-
polator, let us now consider the general problem of sampling rate conver-
sion by the factor I/D. In the general case of sampling rate conversion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

508
Chapter 9
SAMPLING RATE CONVERSION
by a factor I/D, the ﬁltering can be accomplished by means of the linear
time-variant ﬁlter described by the response function
g(n, m) = h[nI −((mD))I]
(9.68)
where h(n) is the impulse response of the lowpass FIR ﬁlter, which, ideally,
has the frequency response speciﬁed by (9.36). For convenience, we select
the length of the FIR ﬁlter {h(n)} to be a multiple of I (i.e., M = KI). As
a consequence, the set of coeﬃcients {g(n, m)} for each m = 0, 1, 2, . . . , I−
1, contains K elements. Since g(n, m) is also periodic with period I, as
demonstrated in (9.44), it follows that the output y(m) can be expressed
as
y(m) =
K−1
	
n=0
g

n, m −
5m
I
6
I

x
/mD
I
0
−n

(9.69)
Conceptually, we can think of performing the computations speciﬁed
by (9.69) by processing blocks of data of length K by a set of K ﬁlter
coeﬃcients g(n, m −⌊m/I⌋I), n = 0, 1, . . . , K −1. There are I such sets
of coeﬃcients, one set for each block of I output points of y(m). For each
block of I output points, there is a corresponding block of D input points
of x(n) that enter in the computation.
The block processing algorithm for computing (9.69) can be visual-
ized as illustrated in Figure 9.39. A block of D input samples is buﬀered
and shifted into a second buﬀer of length K, one sample at a time. The
shifting from the input buﬀer to the second buﬀer occurs at a rate of one
Input
Buffer of
Length D
x(n)
Coefficient Storage
Buffer
of
Length
K
1
2
3
K
n = 0
K − 1
Σ
g(n, 0); 0 ≤ n ≤ K −1
g(n, 1); 0 ≤ n ≤ K −1
g(n, I − 1); 0 ≤ n ≤ K −1
Output
Buffer of
Length I
y(m)
Rate: 
Fx
I
D
FIGURE 9.39
Eﬃcient implementation of sampling rate conversion by block
processing
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

FIR Filter Structures for Sampling Rate Conversion
509
D/I
D/I
D/I
D/I
x(n)
z −1
z −1
z −1
x
− 2
mD
I
x
− K + 1
mD
I
g(K − 1,  )
g(2,  )
g(1,  )
g(0,  );   = 0,1,..., I − 1
y(m)
x
− 1
mD
I
x
mD
I
Rate: 
Fx
I
D
Rate: Fx 
FIGURE 9.40
Eﬃcient realization of sampling rate conversion by a factor I/D
sample each time the quantity ⌊mD/I⌋increases by 1. For each output
sample y(l), the samples from the second buﬀer are multiplied by the
corresponding set of ﬁlter coeﬃcients g(n, l) for n = 0, 1, . . . , K −1, and
the K products are accumulated to give y(l), for l = 0, 1, . . . , I −1. Thus
this computation produces I outputs. It is then repeated for a new set of
D input samples, and so on.
An alternative method for computing the output of the sampling rate
converter, speciﬁed by (9.69), is by means of an FIR ﬁlter structure with
periodically varying ﬁlter coeﬃcients. Such a structure is illustrated in
Figure 9.40. The input samples x(n) are passed into a shift register that
operates at the sampling rate Fx and is of length K = M/I, where M
is the length of the time-invariant FIR ﬁlter speciﬁed by the frequency
response given by (9.36). Each stage of the register is connected to a hold-
and-sample device that serves to couple the input sampling rate Fx to
the output sampling rate Fy = (I/D)Fx. The sample at the input to each
hold-and-sample device is held until the next input sample arrives and
then is discarded. The output samples on the hold-and-sample device are
taken at times mD/I, m = 0, 1, 2, . . . . When both the input and output
sampling times coincide (i.e., when mD/I is an integer), the input to the
hold-and-sample is changed ﬁrst; then the output samples the new input.
The K outputs from the K hold-and-sample devices are multiplied by
the periodically time-varying coeﬃcients g(n, m −⌊m/I⌋I), for n = 0,
1, . . . , K −1, and the resulting products are summed to yield y(m). The
computations at the output of the hold-and-sample devices are repeated
at the output sampling rate of Fy = (I/D)Fx.
Finally, rate conversion by a rational factor I/D can also be performed
by use of a polyphase ﬁlter having I subﬁlters. If we assume that the mth
sample y(m) is computed by taking the output of the imth subﬁlter with
input data x(n), x(n −1), . . . , x(n −K + 1), in the delay line, the next
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

510
Chapter 9
SAMPLING RATE CONVERSION
sample y(m + 1) is taken from the (im+1)st subﬁlter after shifting lm+1
new samples in the delay lines where im+1 = (im + D)modI and lm+1 is
the integer part of (im + D)/I. The integer im+1 should be saved to be
used in determining the subﬁlter from which the next sample is taken.
□
EXAMPLE 9.19
For the sampling rate converter designed in Example 9.15, specify the set of
time-varying coeﬃcients {g(n, m)} used in the realization of the converter based
on the structure given in Figure 9.19. Also, specify the corresponding implemen-
tation based in polyphase ﬁlters.
Solution
The coeﬃcients of the ﬁlter are given by (9.43):
g(n, m) = h(nI + (mD)I) = h

nI + mD −
5D
I m
6
I

By substituting I = 5 and D = 2, we obtain
g(n, m) = h

5n + 2m −5
52m
5
6
By evaluating g(n, m) for n = 0, 1, . . . , 5 and m = 0, 1, . . . ., 4, we obtain the
following coeﬃcients for the time-variant ﬁlter:
g(0, m) = {h(0)
h(2)
h(4)
h(1)
h(3)}
g(1, m) = {h(5)
h(7)
h(9)
h(6)
h(8)}
g(2, m) = {h(10) h(12) h(14) h(11) h(13)}
g(3, m) = {h(15) h(17) h(19) h(16) h(18)}
g(4, m) = {h(20) h(22) h(24) h(21) h(23)}
g(5, m) = {h(25) h(27) h(29) h(26) h(28)}
A polyphase ﬁlter implementation would employ ﬁve subﬁlters, each of length
6. To decimate the output of the polyphase ﬁlters by a factor of D = 2 simply
means that we take every other output from the polyphase ﬁlters. Thus the ﬁrst
output y(0) is taken from p0(n), the second output y(1) is taken from p2(n),
the third output is taken from p4(n), the fourth output is taken from p1(n), the
ﬁfth output is taken from p3(n), and so on.
□
9.7 PROBLEMS
P9.1
Consider the upsampler with input x(n) and output v(m) given in (9.26). Show that the
upsampler is a linear but time-varying system.
P9.2
Let x(n) = 0.9nu(n). The signal is applied to a downsampler that reduces the rate by a
factor of 2 to obtain the signal y(m).
1. Determine and plot the spectrum X(ω).
2. Determine and plot the spectrum Y (ω).
3. Show that the spectrum in part (2) is simply the DTFT of x(2n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
511
P9.3
Consider a signal with spectrum
X(ω) =

nonzero, |ω| ≤ω0
0,
ω0 < |ω| ≤π
1. Show that the signal x(n) can be recovered from its samples x(mD) if the sampling
frequency ωs
△= 2π/D ≥2ω0.
2. Sketch the spectra of x(n) and x(mD) for D = 4.
3. Show that x(n) can reconstructed from the bandlimited interpolation
x(n) =
∞
	
k=−∞
x(kD) sinc[fc(n −kD)];
ω0 < 2πfc < ωs −ω0, fc = 1
D
P9.4
Using the function downsample, study the operation of factor-of-4 downsampling on the
following sequences. Use the stem function to plot the original and the downsampled
sequences. Experiment using the default oﬀset value of zero and the oﬀset value equal to 2.
Comment on any diﬀerences.
1. x1(n) = cos(0.15πn), 0 ≤n ≤100
2. x2(n) = sin(0.1πn) + sin(0.4πn), 0 ≤n ≤100
3. x3(n) = 1 −cos(0.25πn), 0 ≤n ≤100
4. x4(n) = 0.1 n, 0 ≤n ≤100
5. x5(n) = {0, 1, 2, 3, 4, 5, 4, 3, 2, 1}PERIODIC, 0 ≤n ≤100
P9.5
Repeat Problem P9.4 using the factor-of-5 downsampler.
P9.6
Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 0.5 at ω = 0, 1 at ω = 0.1π, 1 at ω = 0.2, 0 at ω = 0.22π, and 0 at
ω = π.
1. Compute and plot the DTFT magnitude of x(n).
2. Downsample x(n) by a factor of 2, and plot the DTFT of the resulting sequence.
3. Downsample x(n) by a factor of 4, and plot the DTFT of the resulting sequence.
4. Downsample x(n) by a factor of 5, and plot the DTFT of the resulting sequence.
5. Comment on your results.
P9.7
Using the function decimate, study the operation of factor-of-4 decimation on the
following sequences. Use the stem function to plot the original and the decimated
sequences. Experiment, using both the default IIR and FIR decimation ﬁlters. Comment
on any diﬀerences.
1. x1(n) = sin(0.15πn), 0 ≤n ≤100
2. x2(n) = cos(0.1πn) + cos(0.4πn), 0 ≤n ≤100
3. x3(n) = 1 −cos(0.25πn), 0 ≤n ≤100
4. x4(n) = 0.1 n, 0 ≤n ≤100
5. x5(n) = {0, 1, 2, 3, 4, 5, 4, 3, 2, 1}PERIODIC, 0 ≤n ≤100
P9.8
Repeat Problem P9.7 using the 4th-order IIR ﬁlter and the 15th-order FIR decimation
ﬁlters. Comment on any performance diﬀerences.
P9.9
Repeat Problem P9.7 using the factor-of-5 decimation. Comment on any diﬀerences.
P9.10
Repeat Problem P9.9 using the the 4th-order IIR ﬁlter and the 15th-order FIR decimation
ﬁlters. Comment on any diﬀerences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

512
Chapter 9
SAMPLING RATE CONVERSION
P9.11
Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 0.5 at ω = 0, 1 at ω = 0.1π, 1 at ω = 0.2, 0 at ω = 0.22π, and 0 at
ω = π.
1. Compute and plot the DTFT of x(n).
2. Decimate x(n) by a factor of 2, and plot the DTFT of the resulting sequence.
3. Decimate x(n) by a factor of 4, and plot the DTFT of the resulting sequence.
4. Decimate x(n) by a factor of 5, and plot the DTFT of the resulting sequence.
5. Comment on your results.
P9.12
Using the function upsample, study the operation of factor-of-4 upsampling on the
following sequences. Use the stem function to plot the original and the upsampled
sequences. Experiment using the default oﬀset value of zero and the oﬀset value
equal to 2.
1. x1(n) = sin(0.6πn), 0 ≤n ≤100
2. x2(n) = sin(0.8πn) + cos(0.5πn), 0 ≤n ≤100
3. x3(n) = 1 + cos(πn), 0 ≤n ≤100
4. x4(n) = 0.2 n, 0 ≤n ≤100
5. x5(n) = {1, 1, 1, 1, 0, 0, 0, 0, 0, 0}PERIODIC, 0 ≤n ≤100
P9.13
Using the fir2 function, generate a 91-length sequence x(n) whose frequency-domain
sampled-values are 0 at ω = 0, 0.5 at ω = 0.1π, 1 at ω = 0.2, 1 at ω = 0.7π, 0.5 at
ω = 0.75π, 0 at ω = 0.8π, and 0 at ω = π.
1. Compute and plot the DTFT magnitude of x(n).
2. Upsample x(n) by a factor of 2, and plot the DTFT magnitude of the resulting
sequence.
3. Upsample x(n) by a factor of 3, and plot the DTFT magnitude of the resulting
sequence.
4. Upsample x(n) by a factor of 4, and plot the DTFT magnitude of the resulting
sequence.
5. Comment on your results.
P9.14
Using the function interp, study the operation of factor-of-4 interpolation on the
sequences of Problem P9.12. Use the stem function to plot the original and the
interpolated sequences. Experiment, using the ﬁlter length parameter values equal to 3
and 5. Comment on any diﬀerences in performance of the interpolation.
P9.15
Provide the frequency response plots of the lowpass ﬁlters used in the interpolators of
Problem P9.14.
P9.16
Repeat Problem P9.14, using the factor-of-3 interpolation.
P9.17
Provide the frequency response plots of the lowpass ﬁlters used in the interpolators of
Problem P9.16.
P9.18
Repeat Problem P9.14, using the factor-of-5 interpolation.
P9.19
Provide the frequency response plots of the lowpass ﬁlters used in the interpolators of
Problem P9.18.
P9.20
Using the fir2 function generate a 91-length sequence x(n) whose frequency-domain
sampled-values are 0 at ω = 0, 0.5 at ω = 0.1π, 1 at ω = 0.2, 1 at ω = 0.7π, 0.5 at
ω = 0.75π, 0 at ω = 0.8π, and 0 at ω = π.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
513
1. Compute and plot the DTFT of x(n).
2. Interpolate x(n) by a factor of 2, and plot the DTFT of the resulting sequence.
3. Interpolate x(n) by a factor of 3, and plot the DTFT of the resulting sequence.
4. Interpolate x(n) by a factor of 4, and plot the DTFT of the resulting sequence.
5. Comment on your results.
P9.21
Consider two sequences x1(n) and x2(n), which appear to be related:
x1(n) = max (10 −|n|, 0)
and
x2(n) = min (|n|, 10)
Use the resample function with default parameters.
1. Resample the sequence x1(n) at 3/2 times the original rate to obtain y1(m), and
provide the stem plots of both sequences.
2. Resample the sequence x2(n) at 3/2 times the original rate to obtain y2(m), and
provide the stem plots of both sequences.
3. Explain why the resampled plot of y2(n) has inaccuracies near the boundaries that
y1(n) does not have.
4. Plot the frequency response of the ﬁlter used in the resampling operation.
P9.22
Let x(n) = cos(0.1πn) + 0.5 sin(0.2πn) + 0.25 cos(0.4πn). Use the resample function with
default parameters.
1. Resample the sequence x(n) at 4/5 times the original rate to obtain y1(m), and provide
the stem plots of both sequences.
2. Resample the sequence x(n) at 5/4 times the original rate to obtain y2(m), and provide
the stem plots of both sequences.
3. Resample the sequence x(n) at 2/3 times the original rate to obtain y3(m), and provide
the stem plots of both sequences.
4. Explain which of the three output sequences retain the “shape” of the original sequence
x(n).
P9.23
Let x(n) = {0, 0, 0, 1, 1, 1, 1, 0, 0, 0}PERIODIC be a periodic sequence with period 10. Use
the resample function for the following parts to resample the sequence x(n) at 3/5 times
the original rate. Consider the length of the input sequence to be 80.
1. Use the ﬁlter length parameter L equal to zero to obtain y1(m), and provide the stem
plots of x(n) and y1(m) sequences.
2. Use the default value of the ﬁlter length parameter L to obtain y2(m), and provide the
stem plots of x(n) and y2(m) sequences.
3. Use the ﬁlter length parameter L equal to 15 to obtain y3(m), and provide the stem
plots of x(n) and y3(m) sequences.
P9.24
Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 0 at ω = 0, 0.5 at ω = 0.1π, 1 at ω = 0.2π, 1 at ω = 0.5π, 0.5 at
ω = 0.55π, 0 at ω = 0.6π, and 0 at ω = π.
1. Compute and plot the DTFT of x(n).
2. Resample x(n) by a factor of 4/3, and plot the DTFT of the resulting sequence.
3. Resample x(n) by a factor of 3/4, and plot the DTFT of the resulting sequence.
4. Resample x(n) by a factor of 4/5, and plot the DTFT of the resulting sequence.
5. Comment on your results.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

514
Chapter 9
SAMPLING RATE CONVERSION
P9.25
We want to design a linear-phase FIR ﬁlter to increase the input sampling rate by a factor
of 3 using the intfilt function.
1. Assuming full bandwidth of the signal to be interpolated, determine the impulse
response of the required FIR ﬁlter. Plot its amplitude response and the log-magnitude
response in dB. Experiment with the length parameter L to obtain a reasonable
stopband attenuation.
2. Assuming that bandwidth of the signal to be interpolated is π/2, determine the
impulse response of the required FIR ﬁlter. Plot its amplitude response and the
log-magnitude response in decibels. Again, experiment with the length parameter L to
obtain a reasonable stopband attenuation.
P9.26
We want to design a linear-phase FIR ﬁlter to increase the input sampling rate by a factor
of 5 using the intfilt function.
1. Assuming full bandwidth of the signal to be interpolated, determine the impulse
response of the required FIR ﬁlter. Plot its amplitude response and the log-magnitude
response in decibels. Experiment with the length parameter L to obtain a reasonable
stopband attenuation.
2. Assuming that bandwidth of the signal to be interpolated is 4π/5, determine the
impulse response of the required FIR ﬁlter. Plot its amplitude response and the
log-magnitude response in decibels. Again, experiment with the length parameter L to
obtain a reasonable stopband attenuation.
P9.27
Using the Parks–McClellan algorithm, design an interpolator that increases the input
sampling rate by a factor of I = 2.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.5 dB ripple in the passband and
50 dB attenuation in the stopband. Choose reasonable values for the band-edge
frequencies.
2. Provide plots of the impulse and the log-magnitude responses.
3. Determine the corresponding polyphase structure for implementing the ﬁlter.
4. Let x(n) = cos(0.4πn). Generate 100 samples of x(n), and process it using this ﬁlter to
interpolate by I = 2 to obtain y(m). Provide the stem plots of the both sequences.
P9.28
Using the Parks–McClellan algorithm, design an interpolator that increases the input
sampling rate by a factor of I = 3.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.1 dB ripple in the passband and
40 dB attenuation in the stopband. Choose reasonable values for the band-edge
frequencies.
2. Provide plots of the impulse and the log-magnitude responses.
3. Determine the corresponding polyphase structure for implementing the ﬁlter.
4. Let x(n) = cos(0.3πn). Generate 100 samples of x(n) and process it using this ﬁlter to
interpolate by I = 3 to obtain y(m). Provide the stem plots of both sequences.
P9.29
A signal x(n) is to be interpolated by a factor of 3. It has a bandwidth of 0.4π, but we
want to preserve frequency band up to 0.3π in the interpolated signal. Using the
Parks–McClellan algorithm, we want to design such an interpolator.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.1 dB ripple in the passband and
40 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
515
3. Let x(n) = cos(0.3πn) + 0.5 sin(0.4πn). Generate 100 samples of x(n), and process it
using this ﬁlter to interpolate by I = 3 to obtain y(m). Provide the stem plots of both
sequences.
P9.30
A signal x(n) is to be interpolated by a factor of 4. It has a bandwidth of 0.7π, but we
want to preserve frequency band up to 0.6π in the interpolated signal. Using the
Parks–McClellan algorithm, we want to design such an interpolator.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.5 dB ripple in the passband and
50 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
3. Let x(n) = sin(0.5πn) + cos(0.7πn). Generate 100 samples of x(n) and process it using
this ﬁlter to interpolate by I = 4 to obtain y(m). Provide the stem plots of both
sequences.
P9.31
Using the Parks–McClellan algorithm, design a decimator that downsamples an input
signal x(n) by a factor of D = 5.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.1 dB ripple in the passband and
30 dB attenuation in the stopband. Choose reasonable values for the band-edge
frequencies.
2. Provide plots of the impulse and the log-magnitude responses.
3. Determine the corresponding polyphase structure for implementing the ﬁlter.
4. Using the fir2 function, generate a 131-length sequence x(n) whose frequency-domain
sampled-values are 1 at ω = 0, 0.9 at ω = 0.1π, 1 at ω = 0.2π, 1 at ω = 0.5π, 0.5 at
ω = 0.55π, 0 at ω = 0.6π, and 0 at ω = π. Process x(n) using this ﬁlter to decimate it
by a factor of 5 to obtain y(m). Provide the spectral plots of both sequences.
P9.32
Using the Parks–McClellan algorithm, design a decimator that downsamples an input
signal x(n) by a factor of D = 3.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.5 dB ripple in the passband and
30 dB attenuation in the stopband. Choose reasonable values for the band-edge
frequencies.
2. Provide plots of the impulse and the log-magnitude responses.
3. Let x1(n) = sin(0.2πn) + 0.2 cos(0.5πn). Generate 500 samples of x1(n), and process it
using this to decimate by D = 3 to obtain y1(m). Provide the stem plots of both
sequences.
4. Using the fir2 function, generate a 131-length sequence x2(n) whose frequency-domain
sampled-values are 1 at ω = 0, 0.8 at ω = 0.15π, 1 at ω = 0.3π, 1 at ω = 0.4π, 0.5 at
ω = 0.45π, 0 at ω = 0.5π, and 0 at ω = π. Process x2(n), using this ﬁlter to decimate it
by a factor of 3 to obtain y2(m). Provide the spectral plots of both sequences.
P9.33
A signal x(n) is to be decimated by a factor of D = 2. It has a bandwidth of 0.4π, and we
will tolerate aliasing this frequency 0.45π in the decimated signal. Using the
Parks–McClellan algorithm, we want to design such a decimator.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.1 dB ripple in the passband and
45 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
3. Let x1(n) = cos(0.4πn) + 2 sin(0.45πn). Generate 200 samples of x1(n), and process it
using this ﬁlter to decimate by D = 2 to obtain y1(m). Provide the stem plots of both
sequences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

516
Chapter 9
SAMPLING RATE CONVERSION
4. Using the fir2 function, generate a 151-length sequence x2(n) whose frequency-domain
sampled-values are 1 at ω = 0, 0.9 at ω = 0.2π, 1 at ω = 0.4π, 0.5 at ω = 0.45π, 0 at
ω = 0.5π, and 0 at ω = π. Process x2(n), using this ﬁlter to decimate it by a factor of 2
to obtain y2(m). Provide the spectral plots of both sequences.
P9.34
A signal x(n) is to be decimated by a factor of D = 3. It has a bandwidth of 0.25π, and we
will tolerate aliasing this frequency 0.3π in the decimated signal. Using the
Parks–McClellan algorithm, we want to design such a decimator.
1. Determine the coeﬃcients of the FIR ﬁlter that has 0.1 dB ripple in the passband and
40 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
3. Let x1(n) = cos(0.2πn) + 2 sin(0.3πn). Generate 300 samples of x1(n), and process it
using this ﬁlter to decimate by D = 3 to obtain y1(m). Provide the stem plots of both
sequences.
4. Using the fir2 function, generate a 151-length sequence x2(n) whose frequency-domain
sampled-values are 1 at ω = 0, 1 at ω = 0.1π, 1 at ω = 0.25π, 0.5 at ω = 0.3π, 0 at
ω = 0.35π, and 0 at ω = π. Process x2(n), using this ﬁlter to decimate it by a factor of
3 to obtain y2(m). Provide the spectral plots of both sequences.
P9.35
Design a sampling rate converter that reduces the sampling rate by a factor of 2/5.
1. Using the Parks–McClellan algorithm, determine the coeﬃcients of the FIR ﬁlter that
has 0.1 dB ripple in the passband and 30 dB attenuation in the stopband. Choose
reasonable values for the band-edge frequencies.
2. Provide plots of the impulse and the log-magnitude responses.
3. Specify the sets of the time-varying coeﬃcients g(m, n) and the corresponding
coeﬃcients in the polyphase ﬁlter realization.
4. Let x(n) = sin(0.35πn) + 2 cos(0.45πn). Generate 500 samples of x(n) and process it
using this ﬁlter to resample by 2/5 to obtain y(m). Provide the stem plots of both
sequences.
P9.36
Design a sampling rate converter that increases the sampling rate by a factor of 7/4.
1. Using the Parks–McClellan algorithm, determine the coeﬃcients of the FIR ﬁlter that
has 0.1 dB ripple in the passband and 40 dB attenuation in the stopband. Choose
reasonable values for the band-edge frequencies.
2. Provide plots of the impulse and the log-magnitude responses.
3. Specify the sets of the time-varying coeﬃcients g(m, n) and the corresponding
coeﬃcients in the polyphase ﬁlter realization.
4. Let x(n) = 2 sin(0.35πn) + cos(0.95πn). Generate 500 samples of x(n) and process it,
using this ﬁlter to resample by 7/4 to obtain y(m). Provide the stem plots of both
sequences.
P9.37
A signal x(n) is to be resampled by a factor of 3/2. It has a total bandwidth of 0.8π, but
we want to preserve frequencies only up to 0.6π and require that the band up to 0.75π be
free of aliasing in the resampled signal.
1. Using the Parks–McClellan algorithm, determine the coeﬃcients of the FIR ﬁlter that
has 0.5 dB ripple in the passband and 50 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
517
3. Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 0.7 at ω = 0, 1 at ω = 0.3π, 1 at ω = 0.7π, 0.5 at ω = 0.75π, 0 at
ω = 0.8π, and 0 at ω = π. Process x(n) using this ﬁlter to resample it by 3/2 to obtain
y(m). Provide the spectral plots of both sequences.
P9.38
A signal x(n) is to be resampled by a factor of 4/5. It has a total bandwidth of 0.8π, but
we want to preserve frequencies only up to 0.5π and require that the band up to 0.75π be
free of aliasing in the resampled signal.
1. Using the Parks–McClellan algorithm, determine the coeﬃcients of the FIR ﬁlter that
has 0.1 dB ripple in the passband and 40 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
3. Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 0.7 at ω = 0, 1 at ω = 0.3π, 1 at ω = 0.7π, 0.5 at ω = 0.75π, 0 at
ω = 0.8π, and 0 at ω = π. Process x(n), using this ﬁlter to resample it by 4/5 to obtain
y(m). Provide the spectral plots of both sequences.
P9.39
A signal x(n) is to be resampled by a factor of 5/2. It has a total bandwidth of 0.8π, but
we want to preserve frequencies only up to 0.7π and require that the band up to 0.75π be
free of aliasing in the resampled signal.
1. Using the Parks–McClellan algorithm, determine the coeﬃcients of the FIR ﬁlter that
has 0.5 dB ripple in the passband and 50 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
3. Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 0.7 at ω = 0, 1 at ω = 0.3π, 1 at ω = 0.7π, 0.5 at ω = 0.75π, 0 at
ω = 0.8π, and 0 at ω = π. Process x(n) using this ﬁlter to resample it by a 5/2 to
obtain y(m). Provide the spectral plots of both sequences.
P9.40
A signal x(n) is to be resampled by a factor of 3/8. It has a total bandwidth of 0.5π, but
we want to preserve frequencies only up to 0.3π and require that the band up to 0.35π be
free of aliasing in the resampled signal.
1. Using the Parks–McClellan algorithm, determine the coeﬃcients of the FIR ﬁlter that
has 0.1 dB ripple in the passband and 40 dB attenuation in the stopband.
2. Provide plots of the impulse and the log-magnitude responses.
3. Using the fir2 function, generate a 101-length sequence x(n) whose frequency-domain
sampled-values are 1 at ω = 0, 1 at ω = 0.25π, 1 at ω = 0.5π, 0.5 at ω = 0.55π, 0 at
ω = 0.6π, and 0 at ω = π. Process x(n) using this ﬁlter to resample it by 3/8 to obtain
y(m). Provide the spectral plots of both sequences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

518
C H A P T E R 10
Round-Off Effects
in Digital Filters
In the latter part of Chapter 6, we discussed the ﬁnite-precision num-
ber representations for the purpose of implementing ﬁltering operations
on digital hardware. In particular, we focused on the process of number
quantization, the resulting error characterizations, and the eﬀects of ﬁl-
ter coeﬃcient quantization on ﬁlter speciﬁcations and responses. In this
chapter, we further extend the eﬀects of ﬁnite-precision numerical eﬀects
to the ﬁltering aspects in signal processing.
We begin by discussing analog-to-digital (A/D) conversion noise us-
ing the number representations and quantization error characteristics de-
veloped in Chapter 6. We then analyze the multiplication and addition
quantization (collectively known as arithmetic round-oﬀerror) models.
The eﬀects of these errors on ﬁlter output are discussed as two topics:
correlated errors called limit cycles and uncorrelated round-off noise.
10.1 ANALYSIS OF A/D QUANTIZATION NOISE
From the quantizer characteristics obtained in Chapter 6, it is obvious
that the quantized value Q[x] is a nonlinear operation on the value x.
Hence the exact analysis of the ﬁnite word-length eﬀects in digital ﬁlters
is generally diﬃcult and one has to consider less ideal analysis techniques
that work well in practice.
One such technique is the statistical modeling technique. It converts
the nonlinear analysis problem into a linear one and allows us to examine
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analysis of A/D Quantization Noise
519
QUANTIZER
x(n)
x(n)
x(n) + e(n)
Q[x (n)]
e(n)
⇒
FIGURE 10.1
Statistical model of a quantizer
output-error characteristics. In this technique, we assume that the quan-
tized value Q[x] is a sum of the exact value x and the quantization error e,
which is assumed to be a random variable. When x(n) is applied as an
input sequence to the quantizer, the error e(n) is assumed to be a random
sequence. We then develop a statistical model for this random sequence
to analyze its eﬀects through a digital ﬁlter.
For the purpose of analysis, we assume that the quantizer employs
ﬁxed-point two’s-complement number format representation. Using the
results given previously, we can extend this analysis to other formats as
well.
10.1.1 STATISTICAL MODEL
We model the quantizer block on the input as a signal-plus-noise
operation—that is, from (6.46),
Q[x(n)] = x(n) + e(n)
(10.1)
where e(n) is a random sequence that describes the quantization error se-
quence and is termed the quantization noise. This is shown in Figure 10.1.
Model assumptions
For the model in (10.1) to be mathematically
convenient and hence practically useful, we have to assume reasonable
statistical properties for the sequences involved. That these assumptions
are practically reasonable can be ascertained using simple MATLAB ex-
amples, as we shall see. We assume that the error sequence, e(n) has the
following characteristics:1
1. The sequence e(n) is a sample sequence from a stationary random
process {e(n)}.
2. This random process {e(n)} is uncorrelated with sequence x(n).
3. The process {e(n)} is an independent process (i.e., the samples are
independent of each other).
4. The probability density function (pdf), fE(e), of sample e(n) for each
n is uniformly distributed over the interval of width ∆= 2−B, which
is the quantizer resolution.
1The review of random variables, processes, and the associated terminology is given
in Chapter 13.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

520
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
These assumptions are reasonable in practice if the sequence x(n) is suf-
ﬁciently random to traverse many quantization steps in going from time
n to n + 1.
10.1.2 ANALYSIS USING MATLAB
To investigate the statistical properties of the error samples, we will have
to generate a large number of these samples and plot their distribution
using a histogram (or a probability bar graph). Furthermore, we have to
design the sequence x(n) so that its samples do not repeat; otherwise, the
error samples will also repeat, which will result in an inaccurate analy-
sis. This can be guaranteed either by choosing a well-deﬁned aperiodic
sequence or a random sequence.
We will quantize x(n) using B-bit rounding operation. A similar im-
plementation can be developed for the truncation operation. Since all
three error characteristics are exactly the same under the rounding op-
eration, we will choose the sign-magnitude format for ease in implemen-
tation. After quantization, the resulting error samples e(n) are uniformly
distributed over the [−∆
2 , ∆
2 ] interval. Let e1(n) be the normalized error
given by
e1(n)
△= e(n)
∆
= e(n) 2B
⇒
e1(n) ∈[−1/2, 1/2]
(10.2)
Then e1(n) is uniform over the interval [−1
2, + 1
2], as shown in Figure 10.2a.
Thus the histogram interval will be uniform across all B-bit values, which
will make its computation and plotting easier. This interval will be divided
into 128 bins for the purpose of plotting.
To determine the sample independence, we consider the histogram of
the sequence
e2(n)
△= e1(n) + e1(n −1)
2
(10.3)
which is the average of two consecutive normalized error samples. If
e1(n) is uniformly distributed between [−1/2, 1/2], then, for sample
independence, e2(n) must have a triangle-shaped distribution between
[−1/2, 1/2], as shown in Figure 10.2b. We will again generate a 128-
bin histogram for e2(n). These steps are implemented in the following
MATLAB function.
f1(n)
f2(n)
e1(n)
e2(n)
−1/2
(a)
(b)
1/2
−1/2
1/2
1
2
FIGURE 10.2
Probability distributions of the normalized errors e1(n) and e2(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analysis of A/D Quantization Noise
521
function [H1,H2,Q, estat] = StatModelR(xn,B,N);
% Statistical Model (Rounding) for A/D Quantization error and its Distribution
% -------------
-------------------------------------------------------------
% [H1,H2,Q] = StatModelR(xn,B,N);
%
OUT: H1 = normalized histogram of e1
%
H2 = normalized histogram of e2
%
Q = normalized histogram bins
%
estat = row vector: [[e1avg,e1std,e2avg,e2std]
%
IN:
B = bits to quantize
%
N = number of samples of x(n)
%
xn = samples of the sequence
% Plot variables
bM = 7; DbM = 2ˆbM;
% Bin parameter
M = round((DbM)/2);
% Half number of bins
bins = [-M+0.5:1:M-0.5];
% Bin values from -M to M
Q = bins/(DbM);
% Normalized bins
% Quantization error analysis
xq = (round(xn*(2ˆB)))/(2ˆB);
% Quantized to B bits
e1 = xq-xn; clear xn xq;
% Quantization error
e2 = 0.5*(e1(1:N-1)+e1(2:N));
% Average of two adj errors
e1avg = mean(e1); e1std = std(e1);
% Mean & std dev of the error e1
e2avg = mean(e2); e2std = std(e2);
% Mean & std dev of the error e2
estat = [e1avg,e1std,e2avg,e2std];
% Probability distribution of e1
e1 = floor(e1*(2ˆ(B+bM)));
% Normalized e1 (int between -M & M)
e1 = sort([e1,-M-1:1:M]);
%
H1 = diff(find(diff(e1)))-1; clear e1;
% Error histogram
if length(H1) == DbM+1
H1(DbM) = H1(DbM)+H1(DbM+1);
H1 = H1(1:DbM);
end
H1 = H1/N;
% Normalized histogram
% Probability distribution of e2
e2 = floor(e2*(2ˆ(B+bM)));
% Normalized e2 (int between -M & M)
e2 = sort([e2,-M-1:1:M]);
%
H2 = diff(find(diff(e2)))-1; clear e2;
% Error histogram
if length(H2) == DbM+1
H2(DbM) = H2(DbM)+H2(DbM+1);
H2 = H2(1:DbM);
end
H2 = H2/N;
% Normalized histogram
To validate the model assumptions, we consider the following two ex-
amples. In the ﬁrst example, an aperiodic sinusoidal sequence is quantized
to B bits, and in the second example, a random sequence is quantized to B
bits. The resulting quantization errors are analyzed for their distribution
properties and for their sample independence for various values of B.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

522
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
Distribution of e1 
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 2 BITS
         MEAN = 3.4239e–005
MIN PROB BAR HEIGHT = 0.007446
MAX PROB BAR HEIGHT = 0.00828
              SIGMA = 0.072073
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
Normalized Error e2 
Normalized Error e1 
Distribution of e2
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 2 BITS
         MEAN = 3.4396e–005
MIN PROB BAR HEIGHT = 0.000334
MAX PROB BAR HEIGHT = 0.015212
              SIGMA = 0.063851
FIGURE 10.3
A/D quantization error distribution for the sinusoidal signal in
Example 10.1, B = 2 bits
Through these examples, we hope to learn how small error e must be (or
equivalently, how large B must be) for the above assumptions to be valid.
□
EXAMPLE 10.1
Let x(n) =
1
3{sin(n/11) + sin(n/31) + cos(n/67)}. This sequence is not peri-
odic, and hence its samples never repeat using inﬁnite-precision representation.
However, since the sequence is of sinusoidal nature, its continuous envelope is
periodic and the samples are continuously distributed over the fundamental
period of this envelope. Determine the error distributions for B = 2 and 6 bits.
Solution
To minimize statistical variations, the sample size must be large. We choose
500,000 samples. The following MATLAB script computes the distributions for
B = 2 bits.
clear; close all;
% Example parameters
B = 2; N = 500000; n = [1:N];
xn = (1/3)*(sin(n/11)+sin(n/31)+cos(n/67)); clear n;
% Quantization error analysis
[H1,H2,Q, estat]] = StatModelR(xn,B,N);
% Compute histograms
H1max = max(H1); H1min = min(H1);
% Max and Min of H1
H2max = max(H2); H2min = min(H2);
% Max and Min of H2
The plots of the resulting histogram are shown in Figure 10.3. Clearly, even
though the error samples appear to be uniformly distributed, the samples
are not independent. The corresponding plots for B = 6 bits are shown in
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analysis of A/D Quantization Noise
523
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 6 BITS
         MEAN = −4.1044e–006
MIN PROB BAR HEIGHT = 0.007478
MAX PROB BAR HEIGHT = 0.00814
              SIGMA = 0.0045076
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 6 BITS
         MEAN = −4.1044e–006
MIN PROB BAR HEIGHT = 0.000112
MAX PROB BAR HEIGHT = 0.01572
              SIGMA = 0.0031818
Distribution of e1
Normalized Error e2 
Normalized Error e1 
Distribution of e2
FIGURE 10.4
Quantization error distribution for the sinusoidal signal in
Example 10.1, B = 6 bits
Figure 10.4, from which we observe that the quantization error sequence ap-
pears to satisfy the model assumptions for B ≥6 bits.
□
□
EXAMPLE 10.2
Let x(n) be an independent and identically distributed random sequence whose
samples are uniformly distributed over the [−1, 1] interval. Determine the error
distributions for B = 2 and 6 bits.
Solution
We again choose 500,000 samples to minimize any statistical variations. The
following MATLAB fragment computes the distributions for B = 2 bits.
clear; close all;
% Example parameters
B = 2; N = 500000; xn = (2*rand(1,N)-1);
% Quantization error analysis
[H1,H2,Q, estat]] = StatModelR(xn,B,N);
% Compute histograms
H1max = max(H1); H1min = min(H1);
% Max and Min of H1
H2max = max(H2); H2min = min(H2);
% Max and Min of H2
The plots of the resulting histogram are shown in Figure 10.5. The correspond-
ing plots for B = 6 bits are shown in Figure 10.6. From these plots, we observe
that even for B = 2 bits the quantization error samples are independent and
uniformly distributed.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

524
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 2 BITS
         MEAN = –8.7384e–005
MIN PROB BAR HEIGHT = 0.007522
MAX PROB BAR HEIGHT = 0.008134
              SIGMA = 0.072111
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 2 BITS
         MEAN = –8.7435e–005
MIN PROB BAR HEIGHT = 0.000116
MAX PROB BAR HEIGHT = 0.015504
              SIGMA = 0.050983
Distribution of e1
Normalized Error e2 
Normalized Error e1 
Distribution of e2
FIGURE 10.5
A/D quantization error distribution for the random signal in
Example 10.2, B = 2 bits
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 6 BITS
         MEAN = –7.3429e–006
MIN PROB BAR HEIGHT = 0.00745
MAX PROB BAR HEIGHT = 0.008136
              SIGMA = 0.0045103
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 6 BITS
         MEAN = –7.3519e–006
MIN PROB BAR HEIGHT = 0.000122
MAX PROB BAR HEIGHT = 0.015664
              SIGMA = 0.003197
Normalized Error e1 
Distribution of e1 
Normalized Error e2 
Distribution of e2
FIGURE 10.6
Quantization error distribution for the random signal in Exam-
ple 10.2, B = 6 bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analysis of A/D Quantization Noise
525
Since practical signals processed using a DSP chip are typically ran-
dom in nature (or can be modeled as such), we conclude from these two
examples that the statistical model, with its stated assumptions, is a very
good model.
10.1.3 STATISTICS OF A/D QUANTIZATION NOISE
We now develop a second-order statistical description of the error sequence
e(n) for both the truncation and rounding operations.
10.1.4 TRUNCATION
From (6.57), the pdf fET(e) of eT(n) is uniform over [−∆, 0], as shown in
Figure 10.7a. Then the average of eT(n) is given by
meT
△= E[eT(n)] = −∆/2
(10.4)
and the variance is
σ2
eT
△= E

(eT(n) −meT)2
=
 0
−∆
(e −∆/2)2 fET(e) de
=
 ∆/2
−∆/2
e2
 1
∆

de = ∆2
12
(10.5)
Using ∆= 2−B, we obtain
σ2
eT = 2−2B
12
or
σeT = 2−B
2
√
3
(10.6)
Rounding
From (6.59), the pdf fER(e) of eR(n) is uniform over
[−∆/2, ∆/2], as shown in Figure 10.7b. Then the average of eR(n) is
given by
meR
△= [EeR] = 0
(10.7)
fT(e)
fR(e)
e
1/∆
1/∆
−∆/2
−∆
−∆/2
∆/2
e
0
0
(a)
(b)
FIGURE 10.7
Probability density functions: (a) truncation and (b) rounding
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

526
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
and the variance is
σ2
eR
△= E

(eR(n) −meR)2
=
 ∆/2
−∆/2
e2 fER(e) de =
 ∆/2
−∆/2
e2
 1
∆

de
= ∆2
12
(10.8)
Using (6.45), we obtain
σ2
eR = 2−2B
12
or
σeR = 2−B
2
√
3
(10.9)
Since the samples of the sequence eR(n) are assumed to be independent
of each other, the variance of [eR(n) + eR(n −1)]/2 is given by
var
eR(n) + eR(n −1)
2

= 1
4
2−2B
12
+ 2−2B
12

= 2−2B
24
= 1
2σ2
eR
(10.10)
or the standard deviation is σeR/
√
2.
From the model assumptions and (10.6) or (10.9), the covariance of
the error sequence (which is an independent sequence) is given by
E[e(m)e(n)]
△= Ce(m −n)
△= Ce(ℓ) = 2−2B
12 δ (ℓ)
(10.11)
where ℓ
△= m −n is called the lag variable. Such an error sequence is also
known as a white noise sequence.
10.1.5 MATLAB IMPLEMENTATION
In MATLAB, the sample mean and standard deviation are computed
using the functions mean and std, respectively. The last argument of the
function StatModelR is a vector containing sample means and standard
deviations of unnormalized errors e(n) and [e(n)+e(n−1)]/2. Thus these
values can be compared with the theoretical values obtained from the
statistical model.
□
EXAMPLE 10.3
The plots in Example 10.1 also indicate the sample means and standard devi-
ations of the errors e(n) and [e(n) + e(n −1)]/2. For B = 2, these computed
values are shown in Figure 10.3. Since e(n) is uniformly distributed over the
interval [−2−3, 2−3], its mean value is 0, and so is the mean of [e(n)+e(n−1)]/2.
The computed values are 3.4239 × 10−5 and 3.4396 × 10−5, respectively, which
agree fairly well with the model. The standard deviation of e(n), from (10.9), is
0.072169, while that from the top plot in Figure 10.3 is 0.072073, which again
agrees closely with the model. The standard deviation of the average of the two
consecutive samples, from (10.10), is 0.051031, and from the bottom plot in
Figure 10.3 it is 0.063851, which clearly does not agree with the model. Hence
the samples of e(n) for B = 2 are not independent. This was conﬁrmed by the
bottom plot in Figure 10.3.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analysis of A/D Quantization Noise
527
Similarly, for B = 6 computed statistical values are shown in Figure 10.4.
The computed values of the two means are −4.1044 × 10−6, which agree very
well with the model. The standard deviation of e(n), from (10.9), is 0.0045105,
while that from the top plot in Figure 10.4 is 0.0045076, which again agrees
closely with the model. The standard deviation of the average of the two con-
secutive samples, from (10.10), is 0.0031894, while from the bottom plot in
Figure 10.4 it is 0.00318181, which clearly agrees with the model. Hence the
samples of e(n) for B = 6 are independent. This was also conﬁrmed by the
bottom plot in Figure 10.4.
□
Similar calculations can be carried out for the signal in Example 10.2.
The details are left to the reader.
10.1.6 A/D QUANTIZATION NOISE THROUGH DIGITAL FILTERS
Let a digital ﬁlter be described by the impulse response, h(n), or the fre-
quency response, H(ejω). When a quantized input, Q[x(n)] = x(n)+e(n),
is applied to this system, we can determine the eﬀects of the error sequence
e(n) on the ﬁlter output as it propagates through the ﬁlter, assuming
inﬁnite-precision arithmetic implementation in the ﬁlter. We are generally
interested in the mean and variance of this output-noise sequence, which
we can obtain using linear system theory concepts. Details of these results
are given in Section 13.5.
Referring to Figure 10.8, let the output of the ﬁlter be ˆy(n). Using
LTI properties and the statistical independence between x(n) and e(n),
the output ˆy(n) can be expressed as the sum of two components. Let y(n)
be the (true) output due to x(n) and let q(n) be the response due to e(n).
Then we can show that q(n) is also a random sequence with mean
mq
△= E[q(n)] = me
∞
	
−∞
h(n) = me H(ej0)
(10.12)
where the term H(ej0) is termed the DC gain of the ﬁlter. For truncation,
meT = −∆/2, which gives
mqT = −∆
2 H(ej0)
(10.13)
For rounding, meR = 0 or
mqR = 0
(10.14)
x(n) = x(n) + e(n)
y(n) = y(n) + q(n)
h(n), H(e jw)
ˆ
ˆ
FIGURE 10.8
Noise through digital ﬁlter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

528
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
We can also show that the variance of q(n), for both the truncation and
rounding, is given by
σ2
q = σ2
e
∞
	
−∞
|h(n)|2 = σ2
e
2π
 π
−π
|H(ejω)|2 dω
(10.15)
The variance gain from the input to the output (also known as the nor-
malized output variance) is the ratio
σ2
q
σ2e
=
∞
	
−∞
|h(n)|2 = 1
2π
 π
−π
|H(ejω)|2 dω
(10.16)
For a real and stable ﬁlter, using the substitution z = ejω, the integral in
(10.16) can be further expressed as a complex contour integral
1
2π
 π
−π
|H(ejω)|2 dω =
1
2πj
%
UC
H(z)H(z−1)z−1dz
(10.17)
where UC is the unit circle and can be computed using residues (or the
inverse Z-transform) as
1
2π
 π
−π
|H(ejω)|2 dω =
	
[Res of H(z)H(z−1)z−1 inside UC] (10.18a)
= Z−1 
H(z)H(z−1)

n=0
(10.18b)
10.1.7 MATLAB IMPLEMENTATION
Computation of the variance-gain for the A/D quantization noise can be
carried out in MATLAB using (10.16) and (10.18). For FIR ﬁlters, we can
perform exact calculations using the time-domain expression in (10.16).
In the case of IIR ﬁlters, exact calculations can only be done using (10.18)
in special cases, as we shall see (fortunately, this works for most practical
ﬁlters). The approximate computations can always be done using the
time-domain expression.
Let the FIR ﬁlter be given by the coeﬃcients {bk}M−1
0
. Then using
the time-domain expression in (10.16), the variance-gain is given by
σ2
q
σ2e
=
M−1
	
k=0
|bk|2
(10.19)
Let an IIR ﬁlter be given by the system function
H(z) =
N−1
ℓ=0 bℓz−ℓ
1 + N−1
k=1 akz−k
(10.20)
with impulse response h(n). If we assume that the ﬁlter is real, causal, and
stable and has only simple poles, then using the partial fraction expansion,
we can write
H(z) = R0 +
N−1
	
k=1
Rk
z −pk
(10.21)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Analysis of A/D Quantization Noise
529
where R0 is the constant term and Rk’s are the residues at the pole
locations pk. This expansion can be computed using the residue function.
Note that both poles and the corresponding residues are either real-valued
or occur in complex-conjugate pairs. Then using (10.18a), we can show
that (see [68] and also Problem P10.3)
σ2
q
σ2e
= R2
0 +
N−1
	
k=1
N−1
	
ℓ=1
RkR∗
ℓ
1 −pkp∗
ℓ
(10.22)
The variance-gain expression in (10.22) is applicable for most practical
ﬁlters since rarely do they have multiple poles. The approximate value of
the variance-gain for IIR ﬁlters is given by
σ2
q
σ2e
≃
K−1
	
k=0
|h(n)|2 ,
K ≫1
(10.23)
where K is chosen so that the impulse response values (magnitudewise)
are almost zero beyond K samples. The following MATLAB function,
VarGain, computes variance-gain using (10.19) or (10.22).
function Gv = VarGain(b,a)
% Computation of variance-gain for the output noise process
% of digital filter described by b(z)/a(z)
%
Gv = VarGain(b,a)
a0 = a(1); a = a/a0; b = b/a0; M = length(b); N = length(a);
if N == 1
% FIR Filter
Gv = sum(b.*b);
return
else
% IIR Filter
[R,p,P] = residue(b,a);
if length(P) > 1
error(’*** Variance Gain Not computable ***’);
elseif length(P) == 1
Gv = P*P;
else
Gv = 0;
end
Rnum = R*R’; pden = 1-p*p’;
H = Rnum./pden; Gv = Gv + real(sum(H(:)));
end
It should be noted that the actual output noise variance is obtained by
multiplying the A/D quantization noise variance by the variance-gain.
□
EXAMPLE 10.4
Consider an eighth-order IIR ﬁlter with poles at pk = r ej2πk/8, k = 0, . . . , 7.
If r is close to 1, then the ﬁlter has four narrowband peaks. Determine the
variance-gain for this ﬁlter when r = 0.9 and r = 0.99.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

530
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
Solution
The following MATLAB script illustrates calculations for r = 0.9, which imple-
ments exact as well as approximate approaches.
% Filter Parameters
N = 8; r = 0.9; b = 1; pl = r*exp(j*2*pi*[0:N-1]/N); a = real(poly(pl));
% Variance-gain (Exact)
Vg = VarGain(b,a)
Vg =
1.02896272593178
% Variance-gain (approximate)
x = [1,zeros(1,10000)]; % Unit sample sequence
h = filter(b,a,x);
% Impulse response
VgCheck = sum(h.*h)
VgCheck =
1.02896272593178
Clearly, both approaches give the same variance-gain, which for r = 0.9 is about
3% above unity. For r = 0.99 the calculations are
% Filter Parameters
N = 8; r = 0.99; b = 1; pl = r*exp(j*2*pi*[0:N-1]/N); a = real(poly(pl));
% Variance-gain (Exact)
Vg = VarGain(b,a)
Vg =
6.73209233071894
The variance-gain is more than 673%, which means that when poles are close
to the unit circle, the ﬁlter output can be very noisy.
□
10.2 ROUND-OFF EFFECTS IN IIR DIGITAL FILTERS
With our insight into the quantizer operation and its simpler statistical
model, we are now ready to delve into the analysis of ﬁnite word-length
eﬀects in both IIR and FIR digital ﬁlters. We have already studied the
eﬀects of input signal quantization and ﬁlter coeﬃcient quantization on
ﬁlter behavior. We will now turn our attention to the eﬀects of arithmetic
operation quantization on ﬁlter output responses (in terms of signal-to-
noise ratios). For this study, we will consider both ﬁxed-point and ﬂoating-
point arithmetic. We ﬁrst consider the eﬀects on IIR ﬁlters since, due to
feedback paths, the results are more complicated—yet more interesting—
than those in FIR ﬁlters. The eﬀects on FIR ﬁlters are studied in the next
section.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
531
We will restrict ourselves to the rounding operation of the quan-
tizer due to its superior statistical qualities (no bias or average value).
From (6.59), we know that, for the rounding operation, the quantizer
error, eR, has the same characteristics across all three number representa-
tion formats. Hence for MATLAB simulation purposes, we will consider
the sign-magnitude format because it is easy to program and simulate
for arithmetic operation. However, in practice, two’s-complement format
number representation has advantages over the others in terms of hard-
ware implementation.
Digital ﬁlter implementation requires arithmetic operations of mul-
tiplication and addition. If two B-bit fractional numbers are multiplied,
the result is a 2B-bit fractional number that must be quantized to B bits.
Similarly, if two B-bit fractional numbers are added, the sum could be
more than 1, which results in an overflow (which in itself is a nonlinear
characteristic), or the sum must be corrected using a saturation strategy,
which is also nonlinear. Thus a ﬁnite word-length implementation of the
ﬁlter is a highly nonlinear ﬁlter and must be analyzed carefully for any
meaningful results.
In this section, we will consider two approaches to deal with errors due
to ﬁnite word-length representation. The ﬁrst type of error can occur when
error samples become correlated with each other due to the nonlinearity
of the quantizer. This is called limit-cycle behavior, and it can exist only
in IIR ﬁlters. We will analyze this problem using the nonlinear quantizer
model rather than the statistical model of the quantizer. In the second
type of error, we assume that more nonlinear eﬀects in the quantizer have
been suppressed. Then, using the statistical model of the quantizer, we
develop a quantization noise model for IIR ﬁlters that is more useful in
predicting the ﬁnite word-length eﬀects.
10.2.1 LIMIT CYCLES
Digital ﬁlters are linear systems, but when quantizers are incorporated
in their implementation, they become nonlinear systems. For nonlinear
systems, it is possible to have an output sequence even when there is
no input. Limit cycles is one such behavior that creates an oscillatory
periodic output that is highly undesirable.
DEFINITION 1
Limit cycle
A zero-input limit cycle is a nonzero periodic output sequence pro-
duced by nonlinear elements or quantizers in the feedback loop of a dig-
ital filter.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

532
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
There are two types of limit cycles. The granular limit cycles are due
to nonlinearities in multiplication quantization and are of low amplitude.
The overflow limit cycles are a result of overﬂow in addition and can have
large amplitudes.
10.2.2 GRANULAR LIMIT CYCLES
This type of limit cycle can easily be demonstrated with a simple round-
ing quantizer following a multiplication. We illustrate with the following
example.
□
EXAMPLE 10.5
Consider a simple ﬁrst-order IIR ﬁlter given by
y(n) = α y(n −1) + x(n);
y(−1) = 0,
n ≥0
(10.24)
Let α = −1
2; then this is a highpass ﬁlter, since its pole is near z = −1.
Determine the output y(n) when x(n) = 7
8δ(n), assuming a 3-bit quantizer in
the multiplier.
Solution
After multiplication by α, we have to quantize the result. Let the output due
to this quantization be ˆy(n). Then the actual implementable digital ﬁlter is
ˆy(n) = Q

−1
2 ˆy(n −1)

+ x(n);
ˆy(−1) = 0,
n ≥0
(10.25)
We assume that the input in (10.24) is quantized and that there is no overﬂow
due to addition. Let B = 3 (i.e., we have 3 fraction bits and 1 sign bit), and
let x(n) = 7
8δ(n). Now α = −1
2 is represented by 1▲110 in two’s-complement
format. Hence the output sequence is obtained as
ˆy(0) = x(0)
= +7
8 : 0▲111
ˆy(1) = Q [α ˆy(0)] = Q

−1
2

+7
8

= Q

−7
16

= −1
2 : 1▲100
ˆy(2) = Q [α ˆy(1)] = Q

−1
2

−1
2

= Q

+1
4

= +1
4 : 0▲010
ˆy(3) = Q [α ˆy(2)] = Q

−1
2

+1
4

= Q

−1
8

= −1
8 : 1▲111
ˆy(4) = Q [α ˆy(3)] = Q

−1
2

−1
8

= Q

+ 1
16

= +1
8 : 0▲001
ˆy(5) = Q [α ˆy(4)] = Q

−1
2

+1
8

= Q

−1
16

= −1
8 : 1▲111
...
...
...
...
...
(10.26)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
533
Thus ˆy(n) = ± 1
8 for n ≥5. The desired output y(n) is
y(n) =
7
8, −7
16, 7
32, −7
64,
7
128, · · · , →0

(10.27)
Hence the error sequence is
e(n) = ˆy(n) −y(n) =

0, −1
16, 1
32, −1
64,
9
128, · · · , →±1
8

(10.28)
This shows that the error e(n) slowly builds up to ± 1
8. Hence the error is
asymptotically periodic with period 2.
□
From Example 10.5, it is clear that, in the steady state, the system
has poles on the unit circle and hence the nonlinear system has eﬀectively
become a linear system [37]. This implies that, eﬀectively, for the system
in (10.24)
Q [αˆy(n −1)] =

ˆy(n −1),
α > 0
−ˆy(n −1), α < 0
(10.29)
Also due to the rounding operation, the quantization error is bounded by
±∆/2, where ∆= 2−B is the quantization step, or
|Q [αˆy(n −1)] −αˆy(n −1)| ≤∆
2
(10.30)
From (10.29) and (10.30), we conclude that
|ˆy(n −1)| ≤
∆
2(1 −|α|)
(10.31)
which is the amplitude range of limit-cycle oscillations and is called a dead
band. For the system in Example 10.5, B = 3 and α = −1
2. Hence the
dead-band range is ± 1
8, which agrees with (10.31). If the output ˆy(n −1)
gets trapped in this band when the input is zero, the ﬁlter exhibits the
granular limit cycle. From (10.29), the period of the oscillation is either
1 or 2.
Analysis using MATLAB
In our previous MATLAB simulations, we
did not worry about the quantization in multiplication or addition oper-
ations because the emphasis was on either signal quantization or on ﬁlter
coeﬃcient quantization. The important operation that we have to consider
is the arithmetic overﬂow characteristics. We assume that the represented
numbers are in fractional two’s-complement format. Then in practice, two
overﬂow characteristics are used: a two’s-complement overﬂow, which is a
modulo (periodic) function, and a saturation, which is a limiting function.
These characteristics are shown in Figure 10.9.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

534
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
(a) Two’s-Complement Overflow
(b) Saturation
y
y
1
−1
−1
−1
1
1
−1
−2
2
x
x
FIGURE 10.9
Overﬂow characteristics used in Qfix
To simulate these two eﬀects, we provide the function y = Qfix(x,B,
’Qmode’,’Omode’).
This
function
performs
a
ﬁxed-point
two’s-
complement format quantization using (B+1)-bit representation so that
the resulting number y is between −1 ≤y < 1. The quantization mode,
Qmode, is either a rounding or a truncation operation. The overﬂow char-
acteristic is provided in Omode. Using this function, we can study both
types of limit cycles.
function [y] = QFix(x,B,Qmode,Omode)
% Fixed-point Arithmetic using (B+1)-bit Representation
% -----------------------------------------------------
%
[y] = QFix(x,B,Qmode,Omode)
%
y: decimal equivalent of quantized x with values in [-1,1)
%
x: a real number array
%
B: Number of fractional bits
%
Qmode: quantizer mode
%
’round’: Two’s-complement rounding characteristics
%
’trunc’: Two’s complement truncation characteristics
%
Omode: overflow mode
%
’satur’: Saturation limiter
%
’twosc’: Two’s-complement overflow
% Quantization operation
if strcmp(lower(Qmode), ’round’);
y = round(x.*(2ˆB));
elseif strcmp(lower(Qmode), ’trunc’);
y = floor(x.*(2ˆB));
else
error(’Use Qmode = "round" or "trunc"’);
end;
y = y*(2ˆ(-B)); % (B+1)-bit representation
% Overflow operation
if strcmp(lower(Omode), ’satur’);
y = min(y,1-2ˆ(-B)); y = max(-1,y); % Saturation
elseif strcmp(lower(Omode), ’twosc’);
y = 2*(mod(y/2-0.5,1)-0.5);
% Overflow
else error(’Use Omode = "satur" or "twosc"’);
end;
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
535
□
EXAMPLE 10.6
In this example, simulate the results for the system given in Example 10.5
using the Qfix function with B = 3 bits. In addition, also examine limit-cycle
behavior for the truncation operation in the multiplier and for the case when
the system is a lowpass ﬁlter with coeﬃcient α = 0.5.
Solution
MATLAB script:
% Highpass filter, rounding operation in multiplier
a = -0.5; yn1 = 0; m = 0:10; y = [yn1, zeros(1,length(m))];
x = 0.875*impseq(m(1),m(1)-1,m(end));
for n = m+2
yn1 = y(n-1);
y(n) = QFix(a*yn1,3,’round’,’satur’) + x(n);
end
% Plotting commands follow
% Lowpass filter, rounding operation in multiplier
a = 0.5; yn1 = 0; m = 0:10; y = [yn1, zeros(1,length(m))];
x = 0.875*impseq(m(1),m(1)-1,m(end));
for n = m+2
yn1 = y(n-1);
y(n) = QFix(a*yn1,3,’round’,’satur’) + x(n);
end
% Plotting commands follow
% Highpass filter, Truncation operation in multiplier
a = -0.5; yn1 = 0; m = 0:10; y = [yn1, zeros(1,length(m))];
x = 0.875*impseq(m(1),m(1)-1,m(end));
for n = m+2
yn1 = y(n-1);
y(n) = QFix(a*yn1,3,’trunc’,’satur’) + x(n);
end
% Plotting commands follow
The resulting plots are shown in Figure 10.10. The output signal in the left
plot agrees with that in Example 10.5 and has an asymptotic period of two
samples. The middle plot for α = 0.5 (lowpass ﬁlter) shows that the limit cycle
has a period of one sample with amplitude of 1
8. Finally, the right plot shows
that the limit cycles vanish for the truncation operation. This behavior for the
truncation operation is also exhibited for lowpass ﬁlters.
□
In the case of second-order and higher-order digital ﬁlters, granular
limit cycles not only exist but also are of various types. These cycles in
second-order ﬁlters can be analyzed, and dead-band as well as frequency
of oscillations can be estimated. For example, if the recursive all-pole ﬁlter
is implemented with rounding quantizers in the multipliers as
ˆy(n) = Q[a1ˆy(n −1)] + Q[a2ˆy(n −2)] + x(n)
(10.32)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

536
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
0
5
10
−1
−0.75
−0.5
−0.25
0
0.25
0.5
0.75
1
Amplitude
Sample Index n 
0
5
10
−1
−0.75
−0.5
−0.25
0
0.25
0.5
0.75
1
Amplitude
 a = 0.5, Rounding
Sample Index n 
0
5
10
−1
−0.75
−0.5
−0.25
0
0.25
0.5
0.75
1
Amplitude
Sample Index n 
a = –0.5, Truncation
a = –0.5, Rounding 
FIGURE 10.10
Granular limit cycles in Example 10.6
where ˆy(n) is the quantized output, then using the analysis similar to that
of the 1-order case, the dead-band region is given by
ˆy(n −2) ≤
∆
2(1 −|a2|)
(10.33)
with a1 determining the frequency of oscillations. For more details, see
Proakis and Manolakis [79]. We provide the following example to illustrate
granular limit cycles in second-order ﬁlters using 3-bit quantizers.
□
EXAMPLE 10.7
Consider the second-order recursive ﬁlter
y(n) = 0.875y(n −1) −0.75y(n −2) + x(n)
(10.34)
with zero initial conditions. This ﬁlter has two complex-conjugate poles and
hence is a bandpass ﬁlter. Let the input be x(n) = 0.375δ(n). Analyze the limit
cycle behavior using a 3-bit quantizer.
Solution
In the ﬁlter implementation, the coeﬃcient products are quantized, which
results in
ˆy(n) = Q[0.875ˆy(n −1)] −Q[0.75ˆy(n −2)] + x(n)
(10.35)
where ˆy(n) is the quantized output. We simulate (10.35) in MATLAB using
both the rounding and truncation operations.
% Bandpass filter
a1 = 0.875; a2 = -0.75;
% Rounding operation in multipliers
yn1 = 0; yn2 = 0;
m = 0:20; y = [yn2,yn1,zeros(1,length(m))];
x = 0.375*impseq(m(1),m(1)-2,m(end));
for n = m+3
yn1 = y(n-1); yn2 = y(n-2);
y(n) = QFix(a1*yn1,3,’round’,’satur’)+QFix(a2*yn2,3,’round’,’satur’)+x(n);
end
% Plotting commands follow
% Truncation operation in multipliers
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
537
0
5
10
15
20
−0.5
−0.25
0
0.25
0.5
Amplitude
Sample Index n 
Sample Index n 
0
5
10
15
20
−0.5
−0.25
0
0.25
0.5
Amplitude
Rounding Operation
Truncation Operation
FIGURE 10.11
Granular limit cycles in Example 10.7
yn1 = 0; yn2 = 0;
m = 0:20; y = [yn2,yn1,zeros(1,length(m))];
x = 0.375*impseq(m(1),m(1)-2,m(end));
for n = m+3
yn1 = y(n-1); yn2 = y(n-2);
y(n) = QFix(a1*yn1,3,’trunc’,’satur’)+QFix(a2*yn2,3,’trunc’,’satur’)+x(n);
end
% Plotting commands follow
The resulting plots are shown in Figure 10.11. The round-oﬀlimit cycles have a
period of six samples and amplitude of 0.25, which agrees with (10.33). Unlike
in the case of ﬁrst-order ﬁlters, the limit cycles for the second-order exist even
when truncation is used in the quantizer.
□
10.2.3 OVERFLOW LIMIT CYCLES
This type of limit cycle is also a zero-input behavior that gives an os-
cillatory output. It is due to overﬂow in the addition even if we ignore
multiplication or product quantization in the ﬁlter implementation. This
is a more serious limit cycle because the oscillations can cover the entire
dynamic range of the quantizer. It can be avoided in practice by using
the saturation characteristics instead of overﬂow in the quantizer. In the
following example, we simulate both granular and overﬂow limit cycles in
a second-order ﬁlter, in addition to inﬁnite-precision implementation.
□
EXAMPLE 10.8
To obtain overﬂow in addition, we will consider the second-order ﬁlter with
large coeﬃcient values and initial conditions (magnitude-wise) excited by a
zero input:
y(n) = 0.875y(n −1) −0.875y(n −1);
y(−1) = −0.875, y(−2) = 0.875
(10.36)
The overﬂow in the addition is obtained by placing the quantizer after the
additions as
ˆy(n) = Q[0.875ˆy(n −1) −0.875ˆy(n −1)];
ˆy(−1) = −0.875, ˆy(−2) = 0.875
(10.37)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

538
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
where ˆy(n) is the quantized output. We ﬁrst simulate the inﬁnite-precision op-
eration of (10.36) and compare its output with the granular limit-cycle imple-
mentation in (10.35) and with the overﬂow limit-cycle in (10.37). We use the
rounding operation. The details are in the following MATLAB script.
M = 100; B = 3; A = 1-2ˆ(-B);
a1 = A; a2 = -A; yn1 = -A; yn2 = A;
m = 0:M; y = [yn2,yn1,zeros(1,length(m))];
% Infinite precision
for n = m+3
yn1 = y(n-1); yn2 = y(n-2);
y(n) = a1*yn1 + a2*yn2;
end
% Plotting commands follow
% Granular limit cycle
for n = m+3
yn1 = y(n-1); yn2 = y(n-2);
y(n) = QFix(a1*yn1,B,’round’,’satur’)+QFix(a2*yn2,B,’round’,’satur’);
y(n) = QFix(y(n),B,’round’,’satur’);
end
% Plotting commands follow
% Overflow limit cycle
for n = m+3
yn1 = y(n-1); yn2 = y(n-2);
y(n) = a1*yn1 + a2*yn2;
y(n) = QFix(y(n),B,’round’,’twosc’);
end
% Plotting commands follow
The resulting plots are shown in Figure 10.12. As expected, the inﬁnite-precision
implementation has no limit cycles. The granular limit cycles are of smaller
amplitudes. Clearly, the overﬂow limit cycles have large amplitudes spanning
the −1 to 1 range of the quantizers.
□
0
20
40
60
80
−1
−0.75
−0.5
−0.25
0
0.25
0.5
0.75
1
Amplitude
No Limit Cycles
Sample Index n
0
20
40
60
80
−1
−0.75
−0.5
−0.25
0
0.25
0.5
0.75
1
Amplitude
Granular Limit Cycles
Sample Index n
0
20
40
60
80
−1
−0.75
−0.5
−0.25
0
0.25
0.5
0.75
1
Amplitude
Overflow Limit Cycles
Sample Index n
FIGURE 10.12
Comparison of limit cycles in Example 10.8
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
539
As shown in these examples, the limit-cycle behaviors of many diﬀer-
ent ﬁlters can be studied for diﬀerent quantizer characteristics using the
MATLAB function QFix.
10.2.4 MULTIPLICATION QUANTIZATION ERROR
A multiplier element in the ﬁlter implementation can introduce additional
quantization errors since multiplication of two B-bit fractional numbers
results in a 2B-bit fraction and must be quantized to a B-bit fraction.
Consider a multiplier in ﬁxed-point arithmetic with B = 8. The number
1
√
3 is represented as 0.578125 in decimal. The square of 0.578125 rounded
to 8 bits is 0.3359375 (which should not be confused with 1/3 rounded to
8 bits, which is 0.33203125). The additional error in the squaring opera-
tion is
0.3359375 −(0.578125)2 = 0.001708984375
This additional error is termed as the multiplication quantization error. Its
statistically equivalent model is similar to that of the A/D quantization
error model, as shown in Figure 10.13.
Statistical model
Consider the B-bit quantizer block following the
multiplier element shown in Figure 10.13a. The sequence x(n) and the con-
stant c are quantized to B fractional bits prior to multiplication (as would
be the case in a typical implementation). The multiplied sequence {c x(n)}
is quantized to obtain y(n). We want to replace the quantizer by a simpler
linear system model shown in Figure 10.13b, in which y(n) = c x(n)+e(n),
where e(n) is a multiplication quantization error. For analysis purposes
we assume that the conditions on e(n) are similar to those for the A/D
quantization error:
1. The random signal e(n) is uncorrelated with the sequence x(n) for
rounding operation (or two’s-complement truncation operation) in the
quantizer.
2. The signal e(n) is an independent process (i.e., the samples are inde-
pendent of each other).
3. The probability density function (pdf) fE(e) of e(n) for each n is uni-
formly distributed over the interval of width ∆= 2−B, which is the
quantizer resolution.
Q
x(n)
x(n)
cx(n) + e(n)
Q[cx (n)]
e(n)
c
c
⇒
(a) Quantizer 
(b) Linear System Model
FIGURE 10.13
Linear system model for multiplication quantization error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

540
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
We will emphasize the rounding operation for the rest of this section.
Based on the above model assumptions, the results given in (10.7), (10.9),
and (10.10) are also applicable for the multiplication quantization error
e(n).
We oﬀer the following two MATLAB examples to illustrate this
model. A more thorough investigation of this error can be found in
Rabiner and Gold [83].
□
EXAMPLE 10.9
Consider the sequence given in Example 10.1, which is repeated here:
x(n) = 1
3 [sin(n/11) + sin(n/31) + cos(n/67)]
This signal is multiplied by c = 1/
√
2, quantized to B bits, and the resulting
multiplication is quantized to B bits with rounding. Using the StatModelR
function and 500,000 samples, compute and analyze normalized errors e1(n)
and e2(n), deﬁned in (10.2) and (10.3), respectively.
Solution
The following MATLAB script computes error distribution, for B = 6 bits.
clear; close all;
% Example parameters
B = 6; N = 500000; n = [1:N]; bM = 7;
xn = (1/3)*(sin(n/11)+sin(n/31)+cos(n/67)); clear n;
c = 1/sqrt(2);
% Signal and coefficient quantization
xq = (round(xn*(2ˆB)))/(2ˆB); c = (round(c*(2ˆB)))/(2ˆB);
cxq = c*xq;
% Multiplication of constant and signal
% Quantization error analysis
[H1,H2,Q, estat] = StatModelR(cxq,B,N);
H1max = max(H1); H1min = min(H1);
% Max and min of H1
H2max = max(H2); H2min = min(H2);
% Max and min of H2
The plots of the resulting histogram are shown in Figure 10.14. For the sinu-
soidal signal, when B = 6 bits, the error samples are not uniformly distributed
and the samples are not independent. The means of e(n) and [e(n)+e(n−1)]/2
are small. Their standard deviations are 0.0045105 and 0.0031059, which do
not agree with (10.10). The corresponding plots for B = 12 bits are shown in
Figure 10.15, from which we observe that the quantization error sequence ap-
pears to satisfy the model assumptions for B ≥12 bits. The means of e(n) and
[e(n) + e(n −1)]/2 are very small, and their standard deviations agree closely
with (10.10).
□
□
EXAMPLE 10.10
Let x(n) be an independent and identically distributed random sequence whose
samples are uniformly distributed over the [−1, 1] interval. Using 500,000 sam-
ples to minimize any statistical variations, analyze normalized errors.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
541
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED TO B = 6 BITS
         MEAN = 1.7026e–006
MIN PROB BAR HEIGHT = 0
MAX PROB BAR HEIGHT = 0.017902
              SIGMA = 0.0045105
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED TO B = 6 BITS
         MEAN = 1.6953e–006
MIN PROB BAR HEIGHT = 0
MAX PROB BAR HEIGHT = 0.017606
              SIGMA = 0.0031059
Distribution of e1
Normalized Error e2 
Normalized Error e1 
Distribution of e2
FIGURE 10.14
Multiplication quantization error distribution for the sinusoidal
signal in Example 10.9, B = 6 bits
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED TO B = 12 BITS
         MEAN = –9.0218e–008
MIN PROB BAR HEIGHT = 0.005792
MAX PROB BAR HEIGHT = 0.008184
              SIGMA = 7.0526e–005
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
    0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED TO B = 12 BITS
         MEAN = –9.034e–008
MIN PROB BAR HEIGHT = 9.2e 005
MAX PROB BAR HEIGHT = 0.01578
              SIGMA = 4.9902e–005
Distribution of e1
Normalized Error e2
Normalized Error e1
Distribution of e2
FIGURE 10.15
Multiplication quantization error distribution for the sinusoidal
signal in Example 10.9, B = 12 bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

542
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
Solution
The following MATLAB script computes the distributions for B = 6 bits.
clear; close all;
% Example parameters
B = 6; N = 500000; xn = (2*rand(1,N)-1); bM = 7;
c = 1/sqrt(2);
% Signal and coefficient quantization
xq = (round(xn*(2ˆB)))/(2ˆB); c = (round(c*(2ˆB)))/(2ˆB);
cxq = c*xq;
% Multiplication of constant and signal
% Quantization error analysis
[H1,H2,Q, estat] = StatModelR(cxq,B,N);
H1max = max(H1); H1min = min(H1);
% Max and min of H1
H2max = max(H2); H2min = min(H2);
% Max and min of H2
The plots of the resulting histogram are shown in Figure 10.16. Even for B = 6
bits, the error samples appear to be uniformly distributed (albeit in discrete
fashion) and are independent of each other. The corresponding plots for B = 12
bits are shown in Figure 10.17. It is clear for B = 12 bits that the quantization
error samples are independent and uniformly distributed. Readers should verify
the statistics of these errors given in (10.7), (10.9), and (10.10).
□
From these two examples, we conclude that the statistical model for
the multiplication quantization error, with its stated assumptions, is a
very good model for random signals when the number of bits in the quan-
tizer is large enough.
10.2.5 STATISTICAL ROUND-OFF NOISE—FIXED-POINT ARITHMETIC
In this and the next section, we will consider the round-oﬀeﬀects on IIR
ﬁlters using the multiplication quantization error model developed in the
previous section. Since we emphasize the rounding operation, this model
is also known as a round-oﬀnoise model. We will limit ourselves to the
ﬁrst- and second-order ﬁlters since practical realizations involve ﬁrst- or
second-order sections.
First-order filter
Consider the ﬁrst-order ﬁlter shown in Figure 10.18a.
When a quantizer Q[·] is introduced after the multiplier, the resulting
ﬁlter model is shown in Figure 10.18b, which is a nonlinear system. When
Q [·] is a quantizer based on the round-oﬀcharacteristics, then its eﬀect is
to add a zero-mean, stationary white noise sequence e(n) at the multiplier
output as shown in Figure 10.18c.
Let q(n) be the response due to e(n), and let he(n) be the noise
impulse response (i.e., between e(n) and q(n)). For the system in
Figure 10.18c,
he(n) = h(n) = αnu(n)
(10.38)
Using (10.12) and (10.7), the mean of q(n) is
mq = me
∞
	
0
he(n) = 0
(10.39)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
543
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
    0
1/128
2/128
3/128
4/128
Distribution of e1
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 6 BITS
         MEAN = 9.606e–006
MIN PROB BAR HEIGHT = 0
MAX PROB BAR HEIGHT = 0.015984
              SIGMA = 0.0045115
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
    0
1/128
2/128
3/128
4/128
Normalized Error e2
Normalized Error e1
Distribution of e2
SAMPLE SIZE N = 500000
 ROUNDED T0 B = 6 BITS
         MEAN = 9.6153e–006
MIN PROB BAR HEIGHT = 4.4e 005
MAX PROB BAR HEIGHT = 0.015656
              SIGMA = 0.0031915
FIGURE 10.16
Multiplication quantization error distribution for the random
signal in Example 10.10, B = 6 bits
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
    0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED TO B = 12 BITS
         MEAN = 3.3451e–008
MIN PROB BAR HEIGHT = 0.005692
MAX PROB BAR HEIGHT = 0.008254
              SIGMA = 7.0449e–005
−0.5
−0.375
−0.25
−0.125
0
0.125
0.25
0.375
0.5
    0
1/128
2/128
3/128
4/128
SAMPLE SIZE N = 500000
 ROUNDED TO B = 12 BITS
         MEAN = 3.3403e–008
MIN PROB BAR HEIGHT = 8.4e–005
MAX PROB BAR HEIGHT = 0.015436
              SIGMA = 4.9859e–005
Normalized Error e1 
Distribution of e1
Normalized Error e2
Distribution of e2
FIGURE 10.17
Multiplication quantization error distribution for the random
signal in Example 10.10, B = 12 bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

544
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
z −1
z −1
z −1
x(n)
x(n)
x(n)
y(n) + q(n)
e(n)
y1(n)
y(n)
y1(n)
a
a
a
(a)
(b)
(c)
Q
ˆ
y1(n)
ˆ
Y
FIGURE 10.18
First-order IIR ﬁlter: (a) structure, (b) structure with quantizer,
(c) round-oﬀnoise model
Similarly, using (10.15), the variance of q(n) is
σ2
q = σ2
e
- ∞
	
0
|he(n)|2
.
(10.40)
Substituting σ2
e = 2−2B/12 for rounding and he(n) from (10.38), we
obtain
σ2
q = 2−2B
12
- ∞
	
0
|αn|2
.
= 2−2B
12
∞
	
0

|α|2n =
2−2B
12 (1 −|α|2)
(10.41)
which is the output noise power due to rounding following the multipli-
cation.
However, we also have to prevent a possible overﬂow following the
adder. Let y1(n) be the signal at the output of the adder in Figure 10.18a,
which in this case is equal to y(n). Now the upper bound on y1(n) is
|y1(n)| = |y(n)| =

∞
	
0
h(k) x(n −k)
 ≤
∞
	
0
|h(k)| |x(n −k)|
(10.42)
Let the input sequence be bounded by Xmax (i.e., |x(n)| ≤Xmax). Then
|y1(n)| ≤Xmax
∞
	
0
|h(k)|
(10.43)
Since y1(n) is represented by B fraction bits, we have |y1(n)| ≤1. The
condition (10.43) can be satisﬁed by requiring
Xmax =
1
∞
0 |h(k)| =
1
1/ (1 −|α|) = 1 −|α|
(10.44)
Thus to prevent overﬂow, x(n) must satisfy
−(1 −|α|) ≤x(n) ≤(1 −|α|)
(10.45)
Thus the input must be scaled before it is applied to the ﬁlter as shown
in Figure 10.19.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
545
z −1
z −1
x(n)
x(n)
y(n) + q(n)
e(n)
y1(n)
a
a
Q
ˆ
y(n)
ˆ
y1(n)
ˆ
Xmax
Xmax
(b)
(a)
FIGURE 10.19
Scaled ﬁrst-order IIR ﬁlter: (a) structure with quantizer, (b)
round-oﬀnoise model
Signal-to-noise ratio
We will now compute the ﬁnite word-length
eﬀect in terms of the output signal-to-noise ratio (SNR). We assume
that there is no overﬂow at the output by properly scaling x(n). Let
x(n) be a stationary white sequence, uniformly distributed between
[−(1 −|α|) , (1 −|α|)]. Then
mx = 0
and
σ2
x = (1 −|α|)2
3
(10.46)
Therefore, y(n) is also a stationary random sequence with mean my = 0
and
σ2
y = σ2
x
∞
	
0
|h(n)|2 = (1 −|α|)2
3
1
1 −|α|2 = (1 −|α|)2
3 (1 −|α|2)
(10.47)
Using (10.41) and (10.47), the output SNR is
SNR
△= σ2
y
σ2q
= (1 −|α|)2
3 (1 −|α|2)
12

1 −|α|2
2−2B
= 4

22B
(1 −|α|)2 = 22(B+1) (1 −|α|)2
(10.48)
or the SNR in dB is
SNRdB
△= 10 log10(SNR) = 6.02 + 6.02B + 20 log10(1 −|α|)
(10.49)
Let δ = 1 −|α|, which is the distance of the pole from the unit circle.
Then
SNRdB = 6.02 + 6.02B + 20 log10(δ)
(10.50)
which is a very informative result. First, it shows that the SNR is directly
proportional to B and increases by about 6 dB for each additional bit
added to the word length. Second, the SNR is also directly proportional
to the distance δ. The smaller the δ (or nearer the pole to the unit circle),
the smaller is the SNR, which is a consequence of the ﬁlter characteristics.
As an example, if B = 6 and δ = 0.05, then SNR = 16.12 dB, and if
B = 12 and δ = 0.1, then SNR = 58.26 dB.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

546
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
10.2.6 ANALYSIS USING MATLAB
To analyze the properties of the round-oﬀerrors in IIR ﬁlters, we will
simulate them using the MATLAB function QFix with quantization mode
’round’ and overﬂow mode ’satur’. If proper scaling to avoid overﬂow is
performed, then only the multiplier output needs to be quantized at each
n without worrying about the overﬂow. However, we will still saturate
the ﬁnal sum to avoid any unforeseen problems. In previous simulations,
we could perform the quantization operations on vectors (i.e., perform
parallel processing). Since IIR ﬁlters are recursive ﬁlters and since each
error is fed back into the system, vector operation is generally not possi-
ble. Hence the ﬁlter output will be computed sequentially from the ﬁrst
to the last sample. For a large number of samples, this implementation
will slow the execution speed in MATLAB since MATLAB is optimized
for vector calculations. However, for newer fast processors, the execution
time is within a few seconds. These simulation steps are detailed in the
following example.
□
EXAMPLE 10.11
Consider the model given in Figure 10.19b. We will simulate this model in
MATLAB and investigate its output error characteristics. Let a = 0.9, which
will be quantized to B bits. The input signal is uniformly distributed over
the [−1, +1] interval and is also quantized to B bits prior to ﬁltering. The
scaling factor Xmax is computed from (10.44). Using 100,000 signal samples
and B = 6 bits, the following MATLAB script computes the true output y(n),
the quantized output ˆy(n), the output error q(n), and the output SNR.
close all; clc;
% Example Parameters
B = 6;
% # of fractional bits
N = 100000;
% # of samples
xn = (2*rand(1,N)-1); % Input sequence - uniform distribution
a = 0.9;
% Filter parameter
Xm = 1-abs(a);
% Scaling factor
% Local variables
bM = 7; DbM = 2ˆbM;
% Bin parameter
BB = 2ˆB;
% Useful factor in quantization
M = round(DbM/2);
% Half number of bins
bins = [-M+0.5:1:M-0.5]; % Bin values from -M to M
Q = bins/DbM;
% Normalized bins
YTN = 2ˆ(-bM);
% Ytick marks interval
YLM = 4*YTN;
% Yaxis limit
% Quantize the input and the filter coefficients
xn = QFix(Xm*xn,B,’round’,’satur’); % Scaled input quant to B bits
a = QFix(a,B,’round’,’satur’);
% a quantized to B bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
547
% Filter output without multiplication quantization
yn = filter(1,[1,-a],xn);
% Output using filter routine
% Filter output with multiplication quantization
yq = zeros(1,N);
% Initialize quantized output array
yq(1) = xn(1);
% Calculation of the first sample yq(1)
for I = 2:N;
A1Y = QFix(a*yq(I-1),B,’round’,’satur’); % Quantization of a*y(n-1)
yq(I) = QFix(A1Y+xn(I),B,’round’,’satur’); % I-th sample yq(I)
end
% Output Error Analysis
en = yn-yq;
% Output error sequence
varyn = var(yn); varen = var(en);
% Signal and noise power
eemax = max(en); eemin = min(en);
% Maximum and minimum of the error
enmax = max(abs([eemax,eemin]));
% Absolute maximum range of the error
enavg = mean(en); enstd = std(en);
% Mean and std dev of the error
en = round(en*(2ˆbM)/(2*enmax)+0.5); % Normalized en (integer between -M & M)
en = sort([en,-M:1:(M+1)]);
%
H = diff(find(diff(en)))-1;
% Error histogram
H = H/N;
% Normalized histogram
Hmax = max(H); Hmin = min(H);
% Max and min of the normalized histogram
% Output SNRs
SNR_C = 10*log10(varyn/varen); % Computed SNR
SNR_T = 6.02 + 6.02*B + 20*log10(Xm); % Theoretical SNR
The part of the script not shown above also computes and plots the normal-
ized histogram of the output error and prints the statistical values in the plot,
as shown in Figure 10.20. The error appears to have a Gaussian distribution,
which is to be expected. The exact value of the output SNR is 22.14 dB, which
agrees with the computed value of 22.21 dB. Similar results done for B = 12
bits are shown in Figure 10.21. Again, the simulation results agree with the
model results.
□
Second-order filter
Similar analysis can be done for second-order
ﬁlters with poles near the unit circle. Let the two poles be at complex
locations rejθ and re−jθ. Then the system function of the ﬁlter is given by
H(z) =
1
(1 −rejθz−1)(1 −re−jθz−1) =
1
1 −2r cos(θ) z−1 + r2z−2
(10.51)
with impulse response
h(n) = rn sin{(n + 1)θ}
sin(θ)
u(n)
(10.52)
The diﬀerence equation from (10.51) is given by
y(n) = x(n)−a1y(n−1)−a2y(n−2);
a1 = −2r cos(θ), a2 = r2
(10.53)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

548
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
0
0.1
0.2
0.3
0.4
0.5
0
0.0078
0.0156
0.0234
0.0313
SAMPLE SIZE N = 100000
  PARAMETER a = 0.90625
  SNR(THEORY) = 22.14
ROUNDED T0 B  = 6 BITS
   ERROR MEAN = –3.2612e–005
SNR(COMPUTED) = 22.2105
−0.5
−0.4
−0.3
−0.2
−0.1
Distribution of Output Error
Normalized Error
FIGURE 10.20
Multiplication quantization eﬀects in the ﬁrst-order IIR ﬁlter in
Example 10.11, B = 6 bits
which requires two multiplications and two additions, as shown in
Figure 10.22a. Thus there are two noise sources and two possible lo-
cations for overﬂow. The round-oﬀnoise model for quantization following
0
0.1
0.2
0.3
0.4
0.5
0.0078
0.0
0.0156
0.0234
0.0313
SAMPLE SIZE N = 100000
  PARAMETER a = 0.8999
  SNR(THEORY) = 58.26
ROUNDED TO B  = 12 BITS
   ERROR MEAN = –1.282e–007
SNR(COMPUTED) = 58.134
−0.5
−0.4
−0.3
−0.2
−0.1
Distribution of Output Error
Normalized Error
FIGURE 10.21
Multiplication quantization eﬀects in the ﬁrst-order IIR ﬁlter in
Example 10.11, B = 12 bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
549
z −1
z −1
x(n)
x(n)
x(n)
Xmax
x(n)
e1(n)
e(n)
e(n)
e2(n)
y(n)
–a1
–a2
–a1
–a2
–a1
–a2
–a1
–a2
y(n)
= y(n) + q1(n) + q2(n)
y(n − 1)
y(n − 2)
z −1
z −1
y(n − 1)
y(n − 2)
z −1
z −1
y(n − 1)
y(n − 2)
z −1
z −1
y(n − 1)
y(n − 2)
ˆ
ˆ
ˆ
ˆ
ˆ
y(n)
= y(n) + q(n) 
ˆ
y(n)
= y(n) + q(n) 
ˆ
ˆ
ˆ
(a)
(b)
(c)
(d)
FIGURE 10.22
Second-order IIR ﬁlter: (a) structure, (b) round-oﬀnoise model,
(c) simpliﬁed model, (d) scaled simpliﬁed model
the two multipliers is shown in Figure 10.22b, where the responses q1(n)
and q2(n) are due to noise sources e1(n) and e2(n), respectively. We can
combine two noise sources into one. However, to avoid overﬂow, we have
to scale signals at the input of each adder, which can complicate this
consolidation of sources.
In modern DSP chips, the intermediate results of multiply-add op-
erations are stored in a multiply-accumulate (MAC) unit that has a
double-precision register to accumulate sums. The ﬁnal sum (which
for Figure 10.22b is at the output of the top adder) is quantized to
obtain ˆy(n). This implementation not only reduces the total multipli-
cation quantization noise but also makes the resulting analysis easier.
Assuming this modern implementation, the resulting simpliﬁed model is
shown in Figure 10.22c, where e(n) is the single noise source that is uni-
formly distributed between [−2−(B+1), 2−(B+1)] and q(n) is the response
due to e(n). Note that e(n) ̸= e1(n)+e2(n) and that q(n) ̸= q1(n)+q2(n).
The only overﬂow that we have to worry about is at the output of the
top adder, which can be controlled by scaling the input sequence x(n)
as shown in Figure 10.22d. Now the round-oﬀnoise analysis can be car-
ried out in a fashion similar to that of the ﬁrst-order ﬁlter. The details,
however, are more involved due to the impulse response in (10.52).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

550
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
Signal-to-noise ratio
Referring to Figure 10.22d, the noise impulse
response he(n) is equal to h(n). Hence the output round-oﬀnoise power
is given by
σ2
q = σ2
e
∞
	
n=0
|h(n)|2 = 2−2B
12
∞
	
n=0
|h(n)|2
(10.54)
Since x(n) is quantized, we have |x(n)| ≤1. It is then scaled by Xmax to
avoid overﬂow in the adder. Hence the output signal power is given by
σ2
y = X2
maxσ2
x
∞
	
n=0
|h(n)|2 = X2
max
3
∞
	
n=0
|h(n)|2
(10.55)
assuming that x(n) is uniformly distributed over [−1, +1]. Hence the out-
put SNR is given by
SNR = σ2
y
σ2q
= 4

22B
X2
max = 22(B+1)X2
max
(10.56)
or
SNRdB = 6.02 + 6.02B + 20 log10 Xmax
(10.57)
Following (10.43), (10.44), and (10.45), the scaling factor Xmax is given
by
Xmax =
1
∞
n=0 |h(n)|
(10.58)
which is not easy to compute. However, lower and upper bounds on Xmax
are easy to obtain. From (10.52), the upper bound on the denominator of
(10.58) is given by
∞
	
n=0
|h(n)| =
1
sin θ
∞
	
n=0
rn| sin[(n + 1)θ]| ≤
1
sin θ
∞
	
n=0
rn =
1
(1 −r) sin θ
(10.59)
or the lower bound on Xmax is given by
Xmax ≥(1 −r) sin θ
(10.60)
The lower bound on the denominator of (10.58) is obtained by noting that
|H(ejω)| =

∞
	
n=0
h(n)e−jω
 ≤
∞
	
n=0
|h(n)|
Now from (10.51), the magnitude |H(ejω)| is given by
|H(ejω)| =

1
1 −2r cos(θ)e−jω + r2e−j2ω

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
551
which has the maximum value at the resonant frequency ω = θ, which
can be easily obtained. Hence
∞
	
n=0
|h(n)| ≥
H(ejθ)
 =
1
(1 −r)
7
1 + r2 −2r cos(2θ)
(10.61)
or the upper bound on Xmax is given by
Xmax ≤(1 −r)
7
1 + r2 −2r cos(2θ)
(10.62)
Substituting (10.60) and (10.62) in (10.56), the output SNR is upper and
lower bounded by
22(B+1)(1−r)2 sin2 θ ≤SNR ≤22(B+1)(1−r)2(1+r2 −2r cos 2θ)
(10.63)
Substituting 1 −r = δ ≪1 and after some simpliﬁcation, we obtain
22(B+1)δ2 sin2 θ ≤SNR ≤4

22(B+1)
δ2 sin2 θ
(10.64)
or the diﬀerence between the upper and lower SNR bounds is about 6 dB.
Once again, the output SNR is directly proportional to B and δ. Fur-
thermore, it also depends on the angle θ. Some of these observations are
investigated in Example 10.12.
10.2.7 ANALYSIS USING MATLAB
We will again simulate round-oﬀerrors using the MATLAB function QFix
with quantization mode ’round’ and overﬂow mode ’satur’. Since a
MAC architecture is assumed, we do not have to quantize the intermediate
results and worry about overﬂow. Only the ﬁnal sum needs to be quantized
with saturation. These operations are also simulated in sequential fashion,
which has an impact on execution speed. The simulation steps for the
second-order ﬁlter are detailed in the following example.
□
EXAMPLE 10.12
Consider the model given in Figure 10.22d. We will simulate this model in
MATLAB and investigate its output error characteristics. Let r = 0.9 and
θ = π/3, from which ﬁlter parameters are computed and quantized to B bits.
The input signal is uniformly distributed over the [−1, +1] interval and is also
quantized to B bits prior to ﬁltering. The scaling factor Xmax is determined
using (10.58), which can be obtained in MATLAB by computing the impulse
response for a suﬃciently large number of samples. Using 100,000 signal samples
and B = 6 bits, the following MATLAB script computes the true output SNR,
the computed SNR, and the lower and upper bounds of the SNR.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

552
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
close all; clc;
% Example Parameters
B = 12;
% # of fractional bits
N = 100000;
% # of samples
xn = (2*rand(1,N)-1); % Input sequence - Uniform
r = 0.9; theta = pi/3;% Pole locations
% Computed Parameters
p1 = r*exp(j*theta);
% Poles
p2 = conj(p1);
%
a = poly([p1,p2]);
% Filter parameters
hn = filter(1,a,[1,zeros(1,1000)]); % Imp res
Xm = 1/sum(abs(hn));
% Scaling factor
Xm_L = (1-r)*sin(theta); % Lower bound
Xm_U = (1-r)*sqrt(1+r*r-2*r*cos(2*theta)); % Upper bound
% Local variables
bM = 7; DbM = 2ˆbM;
% Bin parameter
BB = 2ˆB;
% Useful factor in quantization
M = round(DbM/2);
% Half number of bins
bins = [-M+0.5:1:M-0.5]; % Bin values from -M to M
Q = bins/DbM;
% Normalized bins
YTN = 2ˆ(-bM);
% Ytick marks interval
YLM = 4*YTN;
% Yaxis limit
% Quantize the input and the filter coefficients
xn = QFix(Xm*xn,B,’round’,’satur’); % Scaled input quant B bits
a = QFix(a,B,’round’,’satur’);
% a quantized to B bits
a1 = a(2); a2 = a(3);
% Filter output without multiplication quantization
yn = filter(1,a,xn);
% output using filter routine
% Filter output with multiplication quantization
yq = zeros(1,N); % Initialize quantized output array
yq(1) = xn(1);
% sample yq(1)
yq(2) = QFix((xn(2)-a1*yq(1)),B,’round’,’satur’); % sample yq(2)
for I = 3:N;
yq(I) = xn(I)-a1*yq(I-1)-a2*yq(I-2); % Unquantized sample
yq(I) = QFix(yq(I),B,’round’,’satur’); % Quantized sample
end
% Output Error Analysis
en = yn-yq;
% Output error sequence
varyn = var(yn); varen = var(en);
% Signal and noise power
eemax = max(en); eemin = min(en);
% Maximum and minimum of the error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
553
enmax = max(abs([eemax,eemin]));
% Absolute maximum range of the error
enavg = mean(en); enstd = std(en);
% Mean and std dev of the error
en = round(en*(2ˆbM)/(2*enmax)+0.5); % Normalized en (integer between -M & M)
en = sort([en,-M:1:(M+1)]);
%
H = diff(find(diff(en)))-1;
% Error histogram
H = H/N;
% Normalized histogram
Hmax = max(H); Hmin = min(H);
% Max and Min of the normalized histogram
% Output SNRs
SNR_C = 10*log10(varyn/varen);
% Computed SNR
SNR_T = 6.02 + 6.02*B + 20*log10(Xm);
% Theoretical SNR
SNR_L = 6.02 + 6.02*B + 20*log10(Xm_L); % Lower SNR bound
SNR_U = 6.02 + 6.02*B + 20*log10(Xm_U); % Upper SNR bound
The part of the script not shown above also computes and plots the normalized
histogram of the output error and prints the statistical values in the plot, as
shown in Figure 10.23. The error again has a Gaussian distribution. The exact
value of the output SNR is 25.22 dB, which agrees with the computed value of
25.11 dB and lies between the lower bound of 20.89 dB and the upper bound
of 26.47 dB. Similar results done for B = 12 bits are shown in Figure 10.24.
Again, the simulation results agree with the model results.
□
10.2.8 HIGHER-ORDER FILTERS
The analysis of the quantization eﬀects in a second-order ﬁlter can
be applied directly to higher-order ﬁlters based on a parallel realization.
0
0.1
0.2
0.3
0.4
0.5
0
0.0078
0.0156
0.0234
0.0313
SAMPLE SIZE N = 100000
     RADIAL r = 0.9
  SNR(THEORY) = 25.2243
   SNR(LOWER) = 20.8906
ROUNDED TO B  = 6 BITS
  ANGLE THETA = 60 DEG
SNR(COMPUTED) = 25.1065
   SNR(UPPER) = 26.4697
Distribution of Output Error
−0.5
−0.4
−0.3
−0.2
−0.1
Normalized Error
FIGURE 10.23
Multiplication quantization eﬀects in the ﬁrst-order IIR ﬁlter in
Example 10.12, B = 6 bits
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

554
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
0
0.1
0.2
0.3
0.4
0.5
0
0.0078
0.0156
0.0234
0.0313
SAMPLE SIZE N = 100000
     RADIAL r = 0.9
  SNR(THEORY) = 61.3443
   SNR(LOWER) = 57.0106
ROUNDED TO B  = 12 BITS
  ANGLE THETA = 60 DEG
SNR(COMPUTED) = 61.3968
   SNR(UPPER) = 62.5897
Distribution of Output Error
−0.5
−0.4
−0.3
−0.2
−0.1
Normalized Error
FIGURE 10.24
Multiplication quantization eﬀects in the ﬁrst-order IIR ﬁlter in
Example 10.12, B = 12 bits
In this case, each second-order ﬁlter section is independent of all the other
sections, and therefore the total quantization noise power at the output
of the parallel structure is simply the linear sum of the quantization noise
powers of each of the individual sections. On the other hand, the cascade
realization is more diﬃcult to analyze because the noise generated in any
second-order ﬁlter section is ﬁltered by the succeeding sections. To min-
imize the total noise power at the output of the high-order ﬁlter, a
reasonable strategy is to place the sections in the order of decreasing max-
imum frequency gain. In this case, the noise power generated in the early
high-gain section is not boosted signiﬁcantly by the latter sections. Using
the MATLAB techniques developed in the previous sections, it is easier
to simulate ﬁnite word-length implementations and determine the output
SNR for a given cascade structure.
10.2.9 STATISTICAL ROUND-OFF NOISE—FLOATING-POINT
ARITHMETIC
As stated in Chapter 6, the ﬂoating-point arithmetic gives an error that
is relative to the magnitude rather than an absolute error. This results in
a multiplicative noise rather than additive noise—that is, from (6.61),
Q[x(n)] = x(n) + ε(n)x(n) = x(n) {1 + ε(n)}
(10.65)
with
−2−B < ε(n) ≤2−B
(10.66)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in IIR Digital Filters
555
for a (B + 1)-bit mantissa. Hence the mean of the relative error is mε = 0
and its variance is
σ2
ε = 2−2B
3
(10.67)
Since MATLAB is implemented in IEEE-754 ﬂoating-point arithmetic, all
simulations that we perform are IEEE-754 ﬂoating-point calculations. It is
diﬃcult (if not impossible) to simulate arbitrary ﬂoating-point arithmetic
in MATLAB. Therefore, we give theoretical results only.
First-order filter
Consider a ﬁrst-order ﬁlter as before and shown
in Figure 10.25a. For the ﬁnite word-length analysis with ﬂoating-point
arithmetic we need quantizers after both multiplication and addition to
account for rounding oﬀin the mantissa, as shown in Figure 10.25b. Hence
there are two noise sources in the the statistical model, as shown in Fig-
ure 10.25c, where e1(n) is the noise source in the multiplier, e2(n) is the
noise source in the adder, ˆg(n) is an adder sequence prior to quantization,
and ˆy(n) is the quantized output. Now
e1(n) = ε1(n) α ˆy(n −1)
(10.68a)
e2(n) = ε2(n) ˆg(n)
(10.68b)
where ε1(n) and ε2(n) are the relative errors in the corresponding quan-
tizers. The exact analysis even for the ﬁrst-order case is tedious; hence we
make a few practically reasonable approximations. If the absolute values
of the errors are small, then we have ˆy(n−1) ≈y(n−1) and ˆg(n) ≈y(n);
hence from (10.68a) we obtain
e1(n) ≈α ε1(n) y(n −1)
(10.69a)
e2(n) ≈ε2(n) y(n)
(10.69b)
Furthermore, we make the following assumption about the noise sources:
1. ε1(n) and ε2(n) are white noise sources.
2. ε1(n) and ε2(n) are uncorrelated with each other.
z −1
z−1
z−1
x(n)
x(n)
x(n)
y(n) + q(n)
e1(n)
e2(n)
g(n)
y(n − 1)
y(n)
g(n)
a
a
a
Q
Q
ˆ
y(n)
ˆ
g(n)
ˆ
y(n − 1)
ˆ
y(n − 1)
ˆ
(b)
(c)
(a)
FIGURE 10.25
First-order IIR ﬁlter: (a) structure, (b) ﬁnite word-length model
for ﬂoating-point arithmetic, (c) statistical model for ﬂoating-point arithmetic
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

556
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
3. ε1(n) and ε2(n) are uncorrelated with the input x(n).
4. ε1(n) and ε2(n) are uniformly distributed between −2−B and 2−B.
Let x(n) be a zero-mean, stationary random sequence. Then y(n) is
also a zero-mean, stationary sequence. Hence from (10.69),
σ2
e1 = |α|2σ2
ε1σ2
y
(10.70a)
σ2
e2 = σ2
ε2σ2
y
(10.70b)
Let the error in the output due to e1(n) be q1(n), and let that due to
e2(n) be q2(n). Let h1(n) and h2(n) be the corresponding noise impulse
responses. Note that h1(n) = h2(n) = h(n) = αnu(n). Then the total
error q(n) is
q(n) = q1(n) + q2(n)
(10.71)
with
σ2
q = σ2
q1 + σ2
q2
(10.72)
where
σ2
q1 = σ2
e1
∞
	
0
|h1(n)|2
and
σ2
q2 = σ2
e2
∞
	
0
|h2(n)|2
(10.73)
Hence using (10.72), (10.73), and (10.70),
σ2
q =

σ2
e1 + σ2
e2
 
1
1 −|α|2

= σ2
y

1
1 −|α|2
 
|α|2σ2
ε1 + σ2
ε2

(10.74)
Using σ2
ε1 = σ2
ε2 = 2−2B/3, we obtain
σ2
q = σ2
y
2−2B
3
 1 + |α|2
1 −|α|2

(10.75)
Therefore,
SNR = σ2
y
σ2q
= 3

22B 1 −|α|2
1 + |α|2

(10.76)
or
SNRdB = 4.77 + 6.02B + 10 log10(1 −|α|2) −10 log10(1 + |α|2) (10.77)
which is also a very informative result. Some comments are in order.
1. The SNR in (10.76) was derived without assuming any input statistics,
Hence the result is valid for a large class of inputs including white-noise,
narrow-band, or wide-band signals. The ﬂoating-point arithmetic does
not have to worry about the scaling or limiting input values, since it
can handle a large dynamic range.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in FIR Digital Filters
557
2. Using 0 < δ = 1 −|α| ≪1, the SNR in (10.77) can be put in the form
SNRdB ≈4.77 + 6.02B + 10 log10(δ) = O(δ)
(10.78)
This is to be compared with the ﬁxed-point result (10.50), where
SNR ≈O(δ2). Thus the ﬂoating-point result is less sensitive to the
distance of the pole to the unit circle.
3. In ﬂoating-point arithmetic, the output noise variance, σ2
q, in (10.75) is
proportional to σ2
y. Thus, if the input signal is scaled up, so is the noise
variance since σ2
y is also scaled up. Hence the SNR remains constant.
This again should be compared with the ﬁxed-point case (10.41), in
which σ2
q is independent of the input signal. Hence if the signal level
increases, then σ2
y increases, which increases the SNR.
Second-order filter
Similar analysis can be done for the second-order
ﬁlter with poles close to the unit circle. If the poles are given by re±jθ,
then we can show that (see [71])
SNR = σ2
y
σ2q
≈3

22B
4δ sin2θ
3 + 4 cos θ ≈O (δ)
(10.79)
where δ = 1 −r. This again is an approximate result that works very well
in practice. In this case again, the SNR depends on δ rather than on δ2
as in the ﬁxed-point case.
10.3 ROUND-OFF EFFECTS IN FIR DIGITAL FILTERS
We will now turn our attention to the ﬁnite word-length eﬀects in FIR
digital ﬁlters. As before, we will consider the ﬁxed-point and ﬂoating-point
cases separately. We will then conclude this section with some represen-
tative examples.
10.3.1 FIXED-POINT ARITHMETIC
We will consider the eﬀects on two realizations: direct form and cascade
form. There is no parallel form realization for FIR ﬁlters, since we do
not have a partial fraction expansion, except for the frequency-sampling
realization, which can be analyzed using IIR ﬁlter techniques. The analysis
of FIR ﬁlters is much simpler than that for IIR because there are no
feedback paths. One consequence of this is the absence of limit cycles.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

558
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
Direct-form realization
Consider an FIR ﬁlter of length M (i.e.,
there are M samples in the impulse response), which is realized using the
direct form as shown in Figure 10.26a. The ﬁlter coeﬃcients are the sam-
ples of the impulse response h(n). We have to introduce quantizers in the
vertical branches. If we use the implementation in which each multiplier
output is quantized, then we obtain the model shown in Figure 10.26b.
On the other hand, if we implement the ﬁlter in a typical DSP chip, then
the ﬁnal sum is quantized, as shown in Figure 10.26c. We will separately
consider the eﬀects of round-oﬀnoise and scaling (to avoid overﬂow).
Round-off noise
Let the output of the ﬁlter in Figure 10.26b due to
round-oﬀerrors be ˆy(n) = y(n) + q(n). Then
q(n) =
M−1
	
k=0
ek(n)
(10.80)
where ek(n) are the noise sources introduced in each vertical branch to
account for the rounding operations. Since these noise sources are all
independent and identical, the noise power in q(n) is given by
σ2
q =
M−1
	
0
σ2
ek = M σ2
e = M
2−2B
12

= M
3 2−2(B+1)
(10.81)
In Figure 10.26c, the output due to the rounding operation is ˆy(n) =
y(n) + e(n). Hence the noise power in this case is given by
σ2
q = σ2
e = 1
32−2(B+1)
(10.82)
which is smaller by a factor of M compared to (10.81), as expected.
Scaling to avoid overflow
We assume that the ﬁxed-point numbers
have the two’s-complement form representation, which is a reasonable
assumption. Then we will have to check only the overﬂow of the total sum.
Thus this analysis is the same for both implementations in Figure 10.26
and is similar to that for the IIR ﬁlter in (10.42)–(10.44). The upper-
bound on y(n) is obtained as
|y(n)| =

	
h(k) x(n −k)
 ≤Xmax
	
|h(n)|
(10.83)
where Xmax is the upper-bound on x(n). To guarantee that |y(n)| ≤1,
we need the scaling factor Xmax on x(n) as
Xmax ≤
1
 |h(n)|
(10.84)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in FIR Digital Filters
559
z –1
z –1
z –1
z –1
z –1
z –1
z–1
z –1
z –1
x(n)
x(n)
e1(n)
e2(n)
eM−1(n)
eM−1(n)
e0(n)
e(n)
x(n)
y(n)
=  y(n)  + q(n)
h(M – 1)
h(M – 2)
h(1)
h(2)
h(0)
h(M – 1)
h(M – 2)
h(1)
h(2)
h(0)
h(M – 1)
h(M – 2)
h(1)
h(2)
h(0)
y(n)
(a)
(b)
(c)
Q
Q
Q
Q
Q
Q
ˆ
FIGURE 10.26
Direct form FIR ﬁlter: (a) structure, (b) round-oﬀnoise model
with quantizers after each multiplier, (c) round-oﬀnoise mode with one quan-
tizer after the ﬁnal sum
which is the most conservative scaling factor. There are other scaling
factors, depending on the applications—for example, the narrowband
signals use
Xmax ≤
1
max |H(ejω)|
and wideband random signals use
Xmax ≤
1
4σx
7 |h(n)|2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

560
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
Using (10.84) and assuming that x(n) is uniformly distributed over
[−Xmax, +Xmax], the input signal power is given by
σ2
x = X2
max
3
=
1
3 ( |h(n)|)2
(10.85)
Furthermore, assuming that x(n) is also a white sequence, the output
signal power is given by
σ2
y = σ2
x
	
|h(n)|2 = 1
3
 |h(n)|2
( |h(n)|)2
(10.86)
Thus the output SNR is
SNR = σ2
y
σ2q
= 22(B+1)
A
  |h(n)|2
( |h(n)|)2

(10.87)
where A = M for the model in Figure 10.26b or A = 1 for the model in
Figure 10.26c. The corresponding SNR in dB is
SNRdB = 6.02 + 6.02B + 10 log10
-  |h(n)|2
( |h(n)|)2
.
−10 log10 A
(10.88)
10.3.2 ANALYSIS USING MATLAB
This simulation in MATLAB can be done in parallel fashion since there
is no feedback path for the multiplication quantization errors. Using the
function Qfix function with ’round’ mode, we will compute the quan-
tized multiplier output. In the case of M quantizers, assuming two’s-
complement format, we will use the ’twosc’ mode for each quantizer.
Only the ﬁnal sum will be quantized and saturated. In the case of one
quantizer, we need the ’satur’ mode. These simulation steps are detailed
in the following example.
□
EXAMPLE 10.13
Let a fourth-order (M = 5) FIR ﬁlter be given by
H(z) = 0.1 + 0.2z−1 + 0.4z−2 + 0.2z−3 + 0.1z−4
(10.89)
which is implemented as a direct form with B = 12 fractional bit quantizers.
Compute SNRs for models in Figure 10.26b and 10.26c and verify them using
MATLAB simulations.
Solution
We will need the quantities 
|h(n)|2 and (
|h(n)|)2. These quantities should
be computed using 12-bit quantization of the ﬁlter coeﬃcients. These values
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in FIR Digital Filters
561
using the quantized numbers are 
|h(n)|2 = 0.2599 and (
|h(n)|)2 = 1. Us-
ing (10.88), the output SNR is 65.42 dB for ﬁve multipliers and is 72.41 dB
for one multiplier. The following MATLAB script evaluates these and other
quantities.
% Example Parameters
B = 12;
% # of fractional bits
N = 100000;
% # of samples
xn = (2*rand(1,N)-1);
% Input sequence - uniform distribution
h = [0.1,0.2,0.4,0.2,0.1]; % Filter parameters
M = length(h);
% Local variables
bM = 7; DbM = 2ˆbM;
% Bin parameter
BB = 2ˆB;
% Useful factor in quantization
K = round(DbM/2);
% Half number of bins
bins = [-K+0.5:1:K-0.5]; % Bin values from -K to K
Q = bins/DbM;
% Normalized bins
YTN = 2ˆ(-bM);
% Ytick marks interval
YLM = 4*YTN;
% Yaxis limit
% Quantize the input and the filter coefficients
h = QFix(h,B,’round’,’satur’); % h quantized to B bits
Xm = 1/sum(abs(h));
% Scaling factor
xn = QFix(Xm*xn,B,’round’,’satur’);% Scaled Input quant to B bits
% Filter output without multiplication quantization
yn = filter(h,1,xn);
% output using filter routine
% Filter output with multi quant (5 multipliers)
x1 = [zeros(1,1),xn(1:N-1)]; x2 = [zeros(1,2),xn(1:N-2)];
x3 = [zeros(1,3),xn(1:N-3)]; x4 = [zeros(1,4),xn(1:N-4)];
h0x0 = QFix(h(1)*xn,B,’round’,’twosc’);
h1x1 = QFix(h(2)*x1,B,’round’,’twosc’);
h2x2 = QFix(h(3)*x2,B,’round’,’twosc’);
h3x3 = QFix(h(4)*x3,B,’round’,’twosc’);
h4x4 = QFix(h(5)*x4,B,’round’,’twosc’);
yq = h0x0+h1x1+h2x2+h3x3+h4x4;
yq = QFix(yq,B,’round’,’satur’);
% Output Error Analysis
qn = yn-yq;
% Outout error sequence
varyn = var(yn); varqn = var(qn);
% Signal and noise power
qqmax = max(qn); qqmin = min(qn);
% Maximun and minimum of the error
qnmax = max(abs([qqmax,qqmin]));
% Absolute maximum range of the error
qnavg = mean(qn); qnstd = std(qn);
% Mean and std dev of the error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

562
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
qn = round(qn*(2ˆbM)/(2*qnmax)+0.5); % Normalized en (integer between -K & K)
qn = sort([qn,-K:1:(K+1)]);
%
H = diff(find(diff(qn)))-1;
% Error histogram
H = H/N;
% Normalized histogram
Hmax = max(H); Hmin = min(H);
% Max and Min of the normalized histogram
% Output SNRs
SNR_C = 10*log10(varyn/varqn); % Computed SNR
SNR_T = 6.02 + 6.02*B + 10*log10(sum(h.*h)/Xmˆ2) - 10*log10(M); % Theoretical SNR
% Filter output with multi quant (1 multiplier)
yq = QFix(yn,B,’round’,’satur’);
% Output Error Analysis
qn = yn-yq;
% Outout error sequence
varyn = var(yn); varqn = var(qn);
% Signal and noise power
qqmax = max(qn); qqmin = min(qn);
% Maximun and minimum of the error
qnmax = max(abs([qqmax,qqmin]));
% Absolute maximum range of the error
qnavg = mean(qn); qnstd = std(qn);
% Mean and std dev of the error
qn = round(qn*(2ˆbM)/(2*qnmax)+0.5); % Normalized en (integer between -K & K)
qn = sort([qn,-K:1:(K+1)]);
%
H = diff(find(diff(qn)))-1;
% Error histogram
H = H/N;
% Normalized histogram
Hmax = max(H); Hmin = min(H);
% Max and min of the normalized histogram
% Output SNRs
SNR_C = 10*log10(varyn/varqn); % Computed SNR
SNR_T = 6.02 + 6.02*B + 10*log10(sum(h.*h)/Xmˆ2); % Theoretical SNR
The computed and theoretical SNRs as well as output error histograms for the
two models are shown in Figure 10.27. The top plot shows the histogram when
ﬁve multipliers are used. The output error has Gaussian-like distribution with
SNR equal to 65.42 dB, which agrees with the theoretical value. The bottom
plot show the histogram when one multiplier is used. As expected, the error is
uniformly distributed with SNR equal to 72.43 dB, which also agrees with the
theoretical one.
□
Cascade-form realization
Let the ﬁlter be realized by a cascade of
K, second-order (M = 3) sections given by
H(z) =
K
	
i=1
Hi(z)
where Hi(z) = β0i + β1i z−1 + β2i z−2
(10.90)
as shown in Figure 10.28. The overall length of the ﬁlter is M = 2K + 1.
Figure 10.28 also shows the ﬁnite word-length model for the cascade form,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in FIR Digital Filters
563
0
0.1
0.2
0.3
0.4
0.5
0.0078
0
0.0156
0.0234
0.0313
Five Multipliers
SAMPLE SIZE N = 100000
FILT LENGTH M = 5
  SNR(THEORY) = 65.4187
ROUNDED TO B  = 12 BITS
   ERROR MEAN = 1.9514e–007
SNR(COMPUTED) = 65.4241
0
0.1
0.2
0.3
0.4
0.5
0
0.0078
0.0156
0.0234
0.0313
One Multiplier
SAMPLE SIZE N = 100000
FILT LENGTH M = 5
  SNR(THEORY) = 72.4084
ROUNDED TO B  = 12 BITS
   ERROR MEAN = 1.9514e–007
SNR(COMPUTED) = 72.4324
−0.5
−0.4
−0.3
−0.2
−0.1
−0.5
−0.4
−0.3
−0.2
−0.1
Distribution of Output Error
Distribution of Output Error
Normalized Error
FIGURE 10.27
Multiplication quantization eﬀects for the direct form FIR ﬁlter
in Example 10.13
in which quantization noise sources, ei(n) 1 ≤i ≤K, at each section’s
output are incorporated. Let y(n) be the output due to input x(n), and
let q(n) be the output due to all noise sources. We make the following
reasonable assumptions:
1. The sections are implemented using the MAC (multiply-accumulate)
architecture so that there is only one independent noise source in each
section that contributes to ei(n). The other possibility of three multi-
pliers in each section is straightforward.
x(n)
H1(z)
H2(z)
HK(z)
e1(n)
e2(n)
eK−1(n)
eK(n)
y(n) = y(n) + q(n)
ˆ
FIGURE 10.28
Cascade form FIR ﬁlter structure with noise sources inserted for
multiplication quantization
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

564
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
2. The noise sources are independent of each other—that is,
ei(n) ⊥ej(n)
for i ̸= j
3. Each noise source is a white-noise source with σ2
ei = 2−2B/12.
We will now consider the issues of round-oﬀnoise and scaling (to prevent
overﬂow) for the cascade form realization.
Round-off noise
Let the noise impulse response at the output from
the ei(n) node be denoted by gi(n). Then the length of gi(n) is equal to
(M −2i). Let qi(n) be the output noise due to ei(n). Then its power is
given by
σ2
qi = σ2
ei
M−2i
	
0
|gi(n)|2 = 2−2B
12
M−2i
	
0
|gi(n)|2
(10.91)
Since q(n) = K
i=1 qi(n), we obtain the total noise power as
σ2
q =
K
	
i=1
σ2
qi = 2−2B
12
- K
	
i=1
M−2i
	
n=1
|gi(n)|2
.
(10.92)
The expression K
i=1
M−2i
n=1
|gi(n)|2 shows that the error power depends
on the order of the cascade connections. It has been shown that for the
majority of the orderings the noise power is approximately the same.
Scaling to prevent overflow
From Figure 10.28, we note that one
must prevent overﬂow at each node. Let hk(n) be the impulse response
at each node k; then we need a scaling constant Xmax as
Xmax =
1
maxk
 |hk(n)|
so that |y(n)| ≤1. Clearly, this is a very conservative value. A better
approach is to scale the impulse responses of every section {hi(n)} so
that  |hi| = 1 for each i. Hence the output of every section is limited
between −1 and +1 if the input x(n) is distributed over the same interval.
Assuming that x(n) is uniformly distributed over [−1, +1] and is white,
the output signal power is
σ2
y = σ2
x
M−1
	
0
|h(n)|2 = 1
3
M−1
	
0
|h(n)|2
(10.93)
where h(n) is the overall impulse response of the ﬁlter. Let ˆgi be the
corresponding scaled impulse responses in (10.92). Now the output SNR
can be computed as either
SNR = σ2
y
σ2q
= 22(B+1)
M−1
0
|h(n)|2
K
i=1
M−2i
n=1
|ˆgi(n)|2

(10.94)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in FIR Digital Filters
565
or
SNRdB = 6.02(B+1)+10 log10
-M−1
	
0
|h(n)|2
.
−10 log10
- K
	
i=1
M−2i
	
n=1
|ˆgi(n)|2
.
(10.95)
10.3.3 ANALYSIS USING MATLAB
Using the casfiltr function, we can compute the output of the inﬁnite-
precision cascade structure. Using the scaling approach outlined above,
each second-order section can be scaled and used in the simulation of
quantized outputs. Again, all calculations can be done in vector fashion,
which improves the execution speed. These and other simulation steps are
detailed in the following example.
□
EXAMPLE 10.14
Consider the fourth-order FIR ﬁlter given in Example 10.13. Its cascade form
realization has two sections along with a gain constant b0, which can be obtained
using the dir2cas function:
H1(z) = 1+1.4859z−1+2.8901z−2,
H2(z) = 1+0.5141z−1+0.3460z−2, and b0 = 0.1
(10.96)
Note that some of these coeﬃcients are greater than 1, which will cause problems
with coeﬃcient quantization when only B fractional bits are used. Hence we
need to scale each section as explained. The scaled values are
ˆH1(z) = 0.1860 + 0.2764z−1 + 0.5376z−2,
ˆH2(z) = 0.5376 + 0.2764z−1 + 0.1860z−2
(10.97)
and ˆb0 = 1. Thus we do not need to scale the input. Now ˆg1(n) = ˆh2(n) and
ˆg2(n) = 1 in (10.94). Thus from (10.95), the output SNR is 70.96 dB, which
compares well with the one-multiplier direct form implementation (72.41 dB).
These calculations and error histogram plotting are illustrated in the following
MATLAB script.
% Example Parameters
B = 12;
% # of fractional bits
N = 100000;
% # of samples
xn = (2*rand(1,N)-1);
% Input sequence - uniform distribution
h = [0.1,0.2,0.4,0.2,0.1]; % Filter parameters
M = length(h);
% Filter length
[b0,Bh,Ah] = dir2cas(h,1); % Cascade sections
h1 = Bh(1,:);
% Section-1
h2 = Bh(2,:);
% Section-2
h1 = h1/sum(h1);
% Scaled so Gain=1
h2 = h2/sum(h2);
% Scaled so Gain=1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

566
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
% Local variables
bM = 7; DbM = 2ˆbM;
% Bin parameter
BB = 2ˆB;
% Useful factor in quantization
K = round(DbM/2);
% Half number of bins
bins = [-K+0.5:1:K-0.5]; % Bin values from -K to K
Q = bins/DbM;
% Normalized bins
YTN = 2ˆ(-bM);
% Ytick marks interval
YLM = 20*YTN;
% Yaxis limit
% Quantize the input and the filter coefficients
h1 = QFix(h1,B,’round’,’satur’); % h1 quantized to B bits
h2 = QFix(h2,B,’round’,’satur’); % h1 quantized to B bits
xn = QFix(xn,B,’round’,’satur’); % Input quantized to B bits
% Filter output without multiplication quantization
yn = casfiltr(b0,Bh,Ah,xn); % output using Casfiltr routine
% Filter output with multi quant (1 multiplier/section)
xq = QFix(xn,B,’round’,’satur’); % Section-1 scaled input
wn = filter(h1,1,xq);
% Sec-1 unquantized output
wq = QFix(wn,B,’round’,’satur’); % Sec-1 quantized output
wq = QFix(wq,B,’round’,’satur’); % Section-2 scaled input
yq = filter(h2,1,wq);
% Sec-2 unquantized output
yq = QFix(yq,B,’round’,’satur’); % Sec-2 quantized output
% Output Error Analysis
qn = yn-yq;
% Outout error sequence
varyn = var(yn); varqn = var(qn);
% Signal and noise power
qqmax = max(qn); qqmin = min(qn);
% Maximun and minimum of the error
qnmax = max(abs([qqmax,qqmin]));
% Absolute maximum range of the error
qnavg = mean(qn); qnstd = std(qn);
% Mean and std dev of the error
qn = round(qn*(2ˆbM)/(2*qnmax)+0.5); % Normalized en (integer between -K & K)
qn = sort([qn,-K:1:(K+1)]);
%
H = diff(find(diff(qn)))-1;
% Error histogram
H = H/N;
% Normalized histogram
Hmax = max(H); Hmin = min(H);
% Max and min of the normalized histogram
% Output SNRs
SNR_C = 10*log10(varyn/varqn); % Computed SNR
SNR_T = 6.02*(B+1) + 10*log10(sum(h.*h)) ...
- 10*log10(1+sum(h2.*h2)); % Theoretical SNR
The plot is shown in Figure 10.29. The error distribution appears to have a
Gaussian envelope, but the error is not continuously distributed. This behavior
indicates that the output error takes only a ﬁxed set of values, which is due
to a particular set of coeﬃcient values. The computed SNR is 70.85 dB, which
agrees with the above theoretical value. Thus our assumptions are reasonable.□
10.3.4 FLOATING-POINT ARITHMETIC
Analysis for the ﬂoating-point arithmetic is more complicated and tedious.
Hence we will consider only the direct form realization with simpliﬁed
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Round-OﬀEﬀects in FIR Digital Filters
567
0
0.1
0.2
0.3
0.4
0.5
0
0.0391
0.0781
0.1172
0.1563
Cascade Structure
SAMPLE SIZE N = 100000
FILT LENGTH M = 5
  SNR(THEORY) = 70.9563
ROUNDED TO B  = 12 BITS
   ERROR MEAN = 1.9514e–007
SNR(COMPUTED) = 70.8517
−0.5
−0.4
−0.3
−0.2
−0.1
Distribution of Output Error
Normalized Error
FIGURE 10.29
Multiplication quantization eﬀects for the cascade form FIR
ﬁlter in Example 10.14
assumptions. Figure 10.30 shows a direct form realization with a ﬂoating-
point arithmetic model. In this realization, {ηi(n)}, 1 ≤i ≤M −1 are
the relative errors in adders and {εi(n)}, 0 ≤i ≤M −1 are the relative
errors in multipliers, with |ηi| ≤2−2B and |εi| ≤2−2B.
Let A(n, k) be the gain from the kth multiplier to the output node,
which is given by
A(n, k) =
⎧
⎨
⎩
(1 + εk(n)) ,M−1
r=k (1 + ηr(n)) , k ̸= 0
(1 + ε0(n)) ,M−1
r=k (1 + ηr(n)) , k = 0
(10.98)
x(n)
y(n) + q(n)
1 + e0(n)
1 + e1(n)
1 + h1(n)
1 + h2(n)
1 + e2(n)
1 + e3(n)
1 + eM−2(n)
1 + eM−1(n)
1 + hM−2(n)
1 + hM−1(n)
z−1
z −1
z −1
z −1
h(0)
h(1)
h(2)
h(3)
h(M − 2)
h(M − 1)
1
FIGURE 10.30
Multiplication quantization model for direct form ﬂoating-point
implementation of an FIR ﬁlter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

568
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
Let ˆy(n)
△= y(n) + q(n) be the overall output where y(n) is the output
due to the input x(n) and q(n) is the output due to noise sources. Then
ˆy(n) =
M−1
	
k=0
A(n, k) h(k) x(n −k)
(10.99)
Subtracting y(n) = M−1
k=0 h(k) x(n −k) from (10.99), we obtain
q(n) =
M−1
	
k=0
{A(n, k) −1} h(k) x(n −k)
(10.100)
Now from (10.98), the average value of A(n, k) is E[A(n, k)] = 1, and the
average power of A(n, k) is
E[A2(n, k)] =

1 + 1
3 2−2B
M+1−k
≈1 + (M + 1 −k)2−2B
3
for small 2−2B
(10.101)
Assuming that the input signal x(n) is also a white sequence with variance
σ2
x, then from (10.101) the noise power is given by
σ2
q = (M + 1)2−2B
3
σ2
x
M−1
	
k=0
|h(k)|2

1 −
k
M + 1

(10.102)
Since (1 −
k
M+1) ≤1 and using σ2
y = σ2
x
M−1
k=0 |h(k)|2, the noise power
σ2
q is upper bounded by
σ2
q ≤(M + 1) 2−2B
3
σ2
y
(10.103)
or the SNR is lower bounded by
SNR ≥
3
M + 122B
(10.104)
Equation (10.104) shows that it is best to compute products in order of
increasing magnitude.
□
EXAMPLE 10.15
Again consider the fourth-order FIR ﬁlter given in Example 10.13 in which
M = 5, B = 12, and h(n) = {0.1, 0.2, 0.4, 0.2, 0.1}. From (10.104), the SNR is
lower bounded by
SNRdB ≥10 log10

3
M + 1224
= 69.24 dB
and the approximate value from (10.102) is 71 dB, which is comparable to the
ﬁxed-point value of 72 dB. Note that the ﬁxed-point results would degrade with
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
569
less than optimum scaling (e.g., if signal amplitude were 10 dB down), whereas
the ﬂoating-point SNR would remain the same. To counter this, one could put a
variable scaling factor A on the ﬁxed-point system, which is then getting close to
the full ﬂoating-point system. In fact, ﬂoating-point is nothing but ﬁxed-point
with variable scaling—that is, a scaling by a power of two (or shifting) at each
multiplication and addition.
□
10.4 PROBLEMS
P10.1
Let x(n) = 0.5[cos(n/17) + sin(n/23)]. For the following parts, use 500,000 samples of
x(n) and the StatModelR function.
1. Quantize x(n) to B = 2 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
2. Quantize x(n) to B = 4 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
3. Quantize x(n) to B = 6 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
P10.2
Let x(n) = 1
3 [cos(0.1πn) + sin(0.2πn) + sin(0.4πn)]. For the following parts, use 500,000
samples of x(n) and the StatModelR function.
1. Quantize x(n) to B = 2 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
2. Quantize x(n) to B = 4 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
3. Quantize x(n) to B = 6 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
P10.3
Let a real, causal, and stable IIR ﬁlter be given by
H(z) = R0 +
N−1
	
k=1
Rk
z −pk
(10.105)
where all poles are distinct. Using (10.16), (10.18a), and (10.105), show that
σ2
q
σ2e = R2
0 +
N−1
	
k=1
N−1
	
ℓ=1
RkR∗
ℓ
1 −pkp∗
ℓ
P10.4
Consider the lowpass digital ﬁlter designed in Problem P6.39. The input to this ﬁlter is
an independent and identically distributed Gaussian sequence with zero-mean and
variance equal to 0.1.
1. Determine the variance of the ﬁlter output process using the VarGain function.
2. Determine numerically the variance of the output process by generating 500,000
samples of the input sequence. Comment on your results.
P10.5
Design an elliptic bandpass digital ﬁlter that has a lower stopband of 0.3π, a lower
passband of 0.4π, an upper passband of 0.5π, and an upper stopband of 0.65π.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

570
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
The passband ripple is 0.1 dB and the stopband attenuation is 50 dB. The input signal is
a random sequence whose components are independent and uniformly distributed between
−1 and 1.
1. Determine the variance of the ﬁlter output process using the VarGain function.
2. Determine numerically the variance of the output process by generating 500,000
samples of the input sequence. Comment on your results.
P10.6
Consider the ﬁrst-order recursive system y(n) = 0.75 y(n −1) + 0.125δ(n) with zero initial
conditions. The ﬁlter is implemented in 4-bit (including sign) ﬁxed-point
two’s-complement fractional arithmetic. Products are rounded to 3 bits.
1. Determine and plot the ﬁrst 20 samples of the output using saturation limiter for the
addition. Does the ﬁlter go into a limit cycle?
2. Determine and plot the ﬁrst 20 samples of the output using two’s-complement overﬂow
for the addition. Does the ﬁlter go into a limit cycle?
P10.7
Repeat Problem P10.6 when products are truncated to 3 bits.
P10.8
Consider the second-order recursive system y(n) = 0.125δ(n) −0.875 y(n −2) with zero
initial conditions. The ﬁlter is implemented in 5-bit (including sign) ﬁxed-point
two’s-complement fractional arithmetic. Products are rounded to 4 bits.
1. Determine and plot the ﬁrst 30 samples of the output using a saturation limiter for the
addition. Does the ﬁlter go into a limit cycle?
2. Determine and plot the ﬁrst 30 samples of the output using two’s-complement overﬂow
for the addition. Does the ﬁlter go into a limit cycle?
P10.9
Repeat Problem P10.8 when products are truncated to 4 bits.
P10.10 Let x(n) = 1
4[sin(n/11) + cos(n/13) + sin(n/17) + cos(n/19)] and c = 0.7777. For the
following parts, use 500,000 samples of x(n) and the StatModelR function.
1. Quantize cx(n) to B = 4 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
2. Quantize cx(n) to B = 8 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
3. Quantize cx(n) to B = 12 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
P10.11 Let x(n) = be a random sequence uniformly distributed between −1 and 1, and let
c = 0.7777. For the following parts, use 500,000 samples of x(n) and the StatModelR
function.
1. Quantize cx(n) to B = 4 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
2. Quantize cx(n) to B = 8 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
3. Quantize cx(n) to B = 12 bits, and plot the resulting distributions for the error signals
e1(n) and e2(n). Comment on these plots.
P10.12 Consider an LTI system with the input x(n) and output y(n)
y(n) = b0x(n) + b1x(n −1) + a1y(n −1)
(10.106)
1. Draw the direct form I structure for the above system.
2. Let eb0(n) denote the multiplication quantization error resulting from the product
b0x(n), eb1(n −1) from the product b1x(n −1), and ea1(n −1) from the product
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Problems
571
a1y(n −1) in the direct form I realization. Draw an equivalent structure that contains
only one noise source.
3. Draw an equivalent system that can be used to study multiplication quantization error
for the system in (10.106). The input to this system should be the noise source in
part 2, and the output should be the overall output error q(n).
4. Using the model in part 3, determine an expression for the variance of the output error
e(n).
P10.13 Let the system be given by y(n) = a y(n −1) + x(n). Let a = 0.7, which is quantized to B
(fractional) bits in the ﬁlter realization. Let the input sequence be x(n) = sin(n/11),
which is properly scaled to avoid overﬂow in the adder and quantized to B bits prior to
ﬁltering. The multiplications in the ﬁltering operations are also quantized to B bits.
1. Let B = 5. Generate 100,000 samples of x(n), and ﬁlter through the system with
multiplication quantization. Compute the true output, the quantized output, the
output error, and the output SNR. Plot the normalized histogram, and comment on
the results.
2. Let B = 10. Generate 100,000 samples of x(n) and ﬁlter through the system with
multiplication quantization. Compute the true output, the quantized output, the
output error, and the output SNR. Plot the normalized histogram, and comment on
the results.
P10.14 Let the system be given by y(n) = a y(n −1) + x(n). Let a = 0.333, which is quantized to
B (fractional) bits in the ﬁlter realization. Let the input sequence be x(n) = sin(n/11),
which is properly scaled to avoid overﬂow in the adder and quantized to B bits prior to
ﬁltering. The multiplications in the ﬁltering operations are also quantized to B bits.
1. Let B = 5. Generate 100,000 samples of x(n), and ﬁlter through the system with
multiplication quantization. Compute the true output, the quantized output, the
output error, and the output SNR. Plot the normalized histogram and comment on
the results.
2. Let B = 10. Generate 100,000 samples of x(n), and ﬁlter through the system with
multiplication quantization. Compute the true output, the quantized output, the
output error, and the output SNR. Plot the normalized histogram and comment on
the results.
P10.15 Consider the second-order IIR ﬁlter given in (10.51) with r = 0.8 and θ = π/4. The input
to this ﬁlter is x(n) = sin(n/23).
1. Investigate the multiplication quantization error behavior of this ﬁlter for B = 5 bits.
Determine the true output SNR, the computed output SNR, and the upper and
lower bounds of the SNR. Plot the normalized histogram of the output error.
2. Investigate the multiplication quantization error behavior of this ﬁlter for B = 10
bits. Determine the true output SNR, the computed output SNR, and the upper and
lower bounds of the SNR. Plot the normalized histogram of the output error.
P10.16 Consider the second-order IIR ﬁlter given in (10.51) with r = 0. −8 and θ = 2π/3. The
input to this ﬁlter is x(n) = sin(n/23).
1. Investigate the multiplication quantization error behavior of this ﬁlter for B = 5 bits.
Determine the true output SNR, the computed output SNR, and the upper and
lower bounds of the SNR. Plot the normalized histogram of the output error.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

572
Chapter 10
ROUND-OFF EFFECTS IN DIGITAL FILTERS
2. Investigate the multiplication quantization error behavior of this ﬁlter for B = 10 bits.
Determine the true output SNR, the computed output SNR, and the upper and
lower bounds of the SNR. Plot the normalized histogram of the output error.
P10.17 Consider a ﬁfth-order FIR system given by
H(z) = 0.1 + 0.2z−1 + 0.3z−2 + 0.3z−3 + 0.2z−4 + 0.1z−5
which is implemented in a direct form using B = 10 bits. Input to the ﬁlter is a random
sequence whose samples are independent and identically distributed over [−1, 1].
1. Investigate the multiplication quantization errors when all six multipliers are used in
the implementation. Plot the normalized histogram of the output error.
2. Investigate the multiplication quantization errors when one multiplier is used in the
implementation. Plot the normalized histogram of the output error.
P10.18 Consider a fourth-order FIR system given by
H(z) = 0.1 + 0.2z−1 + 0.3z−2 + 0.2z−3 + 0.1z−4
which is implemented in a cascade form containing second-order sections. Input to the
ﬁlter is a random sequence whose samples are independent and identically distributed
over [−1, 1].
1. Investigate the multiplication quantization errors when B = 6 bits is used in the
implementation. Plot the normalized histogram of the output error.
2. Investigate the multiplication quantization errors when B = 12 bits is used in the
implementation. Plot the normalized histogram of the output error.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

573
C H A P T E R 11
Applications
in Adaptive
Filtering
In Chapters 7 and 8, we described methods for designing FIR and IIR dig-
ital ﬁlters to satisfy some desired speciﬁcations. Our goal was to determine
the coeﬃcients of the digital ﬁlter that met the desired speciﬁcations.
In contrast to the ﬁlter design techniques considered in those two
chapters, there are many digital signal processing applications in which
the ﬁlter coeﬃcients cannot be speciﬁed a priori. For example, let us con-
sider a high-speed modem that is designed to transmit data over telephone
channels. Such a modem employs a ﬁlter called a channel equalizer to com-
pensate for the channel distortion. The modem must eﬀectively transmit
data through communication channels that have diﬀerent frequency re-
sponse characteristics and hence result in diﬀerent distortion eﬀects. The
only way in which this is possible is if the channel equalizer has adjustable
coefficients that can be optimized to minimize some measure of the dis-
tortion, on the basis of measurements performed on the characteristics of
the channel. Such a ﬁlter with adjustable parameters is called an adaptive
filter—in this case, an adaptive equalizer.
Numerous applications of adaptive ﬁlters have been described in the
literature. Some of the more noteworthy applications include (1) adaptive
antenna systems, in which adaptive ﬁlters are used for beam steering and
for providing nulls in the beam pattern to remove undesired interference
[97], (2) digital communication receivers, in which adaptive ﬁlters are
used to provide equalization of intersymbol interference and for channel
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

574
Chapter 11
APPLICATIONS IN ADAPTIVE FILTERING
identiﬁcation [81], (3) adaptive noise canceling techniques, in which an
adaptive ﬁlter is used to estimate and eliminate a noise component in
some desired signal [96, 34, 15], and (4) system modeling, in which an
adaptive ﬁlter is used as a model to estimate the characteristics of an un-
known system. These are just a few of the best-known examples on the use
of adaptive ﬁlters.
Although both IIR and FIR ﬁlters have been considered for adap-
tive ﬁltering, the FIR ﬁlter is by far the most practical and widely used.
The reason for this preference is quite simple. The FIR ﬁlter has only
adjustable zeros, and hence it is free of stability problems associated with
adaptive IIR ﬁlters that have adjustable poles as well as zeros. We should
not conclude, however, that adaptive FIR ﬁlters are always stable. On the
contrary, the stability of the ﬁlter depends critically on the algorithm for
adjusting its coeﬃcients.
Of the various FIR ﬁlter structures that we may use, the direct form
and the lattice form are the ones often used in adaptive ﬁltering appli-
cations. The direct form FIR ﬁlter structure with adjustable coeﬃcients
h(0), h(1), . . . , h(N −1) is illustrated in Figure 11.1. The FIR lattice struc-
tures are discussed in Chapter 14 and have adjustable parameters Kn,
called reﬂection coeﬃcients, shown in Figure 14.15.
An important consideration in the use of an adaptive ﬁlter is the
criterion for optimizing the adjustable ﬁlter parameters. The criterion
must not only provide a meaningful measure of ﬁlter performance, but it
must also result in a practically realizable algorithm.
One criterion that provides a good measure of performance in adap-
tive ﬁltering applications is the least-squares criterion, and its counterpart
in a statistical formulation of the problem, namely, the mean-square-error
(MSE) criterion. The least-squares (and MSE) criterion results in a qua-
dratic performance index as a function of the ﬁlter coeﬃcients, and hence
it possesses a single minimum. The resulting algorithms for adjusting the
coeﬃcients of the ﬁlter are relatively easy to implement.
FIGURE 11.1
Direct form adaptive FIR ﬁlter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

LMS Algorithm for Coeﬃcient Adjustment
575
In this chapter, we describe a basic algorithm, called the least-mean-
square (LMS) algorithm, to adaptively adjust the coeﬃcients of an FIR
ﬁlter. The adaptive ﬁlter structure that will be implemented is the di-
rect form FIR ﬁlter structure with adjustable coeﬃcients h(0), h(1), . . . ,
h(N −1), as illustrated in Figure 11.1. After we describe the LMS algo-
rithm, we apply it to several practical systems in which adaptive ﬁlters
are employed.
11.1 LMS ALGORITHM FOR COEFFICIENT ADJUSTMENT
Suppose we have an FIR ﬁlter with adjustable coeﬃcients {h(k), 0 ≤k ≤
N −1}. Let {x(n)} denote the input sequence to the ﬁlter, and let the
corresponding output be {y(n)}, where
y(n) =
N−1
	
k=0
h(k)x (n −k) ,
n = 0, . . . , M
(11.1)
Suppose that we also have a desired sequence {d(n)} with which we can
compare the FIR ﬁlter output. Then we can form the error sequence
{e(n)} by taking the diﬀerence between d(n) and y(n), that is,
e(n) = d(n) −y(n),
n = 0, . . . , M
(11.2)
The coeﬃcients of the FIR ﬁlter will be selected to minimize the sum of
squared errors. Thus we have
E =
M
	
n=0
e2(n) =
M
	
n=0

d(n) −
N−1
	
k=0
h(k)x (n −k)
2
(11.3)
=
M
	
n=0
d2(n) −2
N−1
	
k=0
h(k)rdx(k) +
N−1
	
k=0
N−1
	
ℓ=0
h(k)h (ℓ) rxx (k −ℓ)
where, by deﬁnition,
rdx(k) =
M
	
n=0
d(n)x (n −k) ,
0 ≤k ≤N −1
(11.4)
rxx(k) =
M
	
n=0
x(n)x (n + k) ,
0 ≤k ≤N −1
(11.5)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

576
Chapter 11
APPLICATIONS IN ADAPTIVE FILTERING
We call {rdx(k)} the cross-correlation between the desired output
sequence {d(n)} and the input sequence {x(n)}, and {rxx(k)} is the
autocorrelation sequence of {x(n)}.
The sum of squared errors E is a quadratic function of the FIR ﬁlter
coeﬃcients. Consequently, the minimization of E with respect to the ﬁlter
coeﬃcients {h(k)} results in a set of linear equations. By diﬀerentiating
E with respect to each of the ﬁlter coeﬃcients, we obtain
∂E
∂h(m) = 0,
0 ≤m ≤N −1
(11.6)
and hence
N−1
	
k=0
h(k)rxx (k −m) = rdx(m),
0 ≤m ≤N −1
(11.7)
This is the set of linear equations that yield the optimum ﬁlter coeﬃcients.
To solve the set of linear equations directly, we must ﬁrst com-
pute the autocorrelation sequence {rxx(k)} of the input signal and the
cross-correlation sequence {rdx(k)} between the desired sequence {d(n)}
and the input sequence {x(n)}.
The LMS algorithm provides an alternative computational method for
determining the optimum ﬁlter coeﬃcients {h(k)} without explicitly com-
puting the correlation sequences {rxx(k)} and {rdx(k)}. The algorithm is
basically a recursive gradient (steepest-descent) method that ﬁnds the
minimum of E and thus yields the set of optimum ﬁlter coeﬃcients.
We begin with any arbitrary choice for the initial values of {h(k)}—
say, {h0(k)}. For example, we may begin with h0(k) = 0,
0 ≤k ≤N −1.
Then after each new input sample {x(n)} enters the adaptive FIR ﬁlter,
we compute the corresponding output—say, {y(n)}—form the error signal
e(n) = d(n) −y(n), and update the ﬁlter coeﬃcients according to the
equation
hn(k) = hn−1(k) + △· e(n) · x (n −k) ,
0 ≤k ≤N −1,
n = 0, 1, . . .
(11.8)
where △is called the step-size parameter, x(n −k) is the sample of the
input signal located at the kth tap of the ﬁlter at time n, and e(n)x (n −k)
is an approximation (estimate) of the negative of the gradient for the kth
ﬁlter coeﬃcient. This is the LMS recursive algorithm for adjusting the
ﬁlter coeﬃcients adaptively so as to minimize the sum of squared errors E.
The step-size parameter △controls the rate of convergence of the
algorithm to the optimum solution. A large value of △leads to large
step-size adjustments and thus to rapid convergence, while a small value of
△results in slower convergence. However, if △is made too large the
algorithm becomes unstable. To ensure stability, △must be chosen [81]
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

LMS Algorithm for Coeﬃcient Adjustment
577
to be in the range
0 < △<
1
10NPx
(11.9)
where N is the length of the adaptive FIR ﬁlter and Px is the power in
the input signal, which can be approximated by
Px ≈
1
1 + M
M
	
n=0
x2(n) = rxx (0)
M + 1
(11.10)
The mathematical justiﬁcation of equations (11.9) and (11.10) and
the proof that the LMS algorithm leads to the solution for the optimum
ﬁlter coeﬃcients is given in more advanced treatments of adaptive ﬁlters.
The interested reader may refer to the books by Haykin [30] and Proakis
and Manolakis [79].
11.1.1 MATLAB IMPLEMENTATION
The LMS algorithm (11.8) can easily be implemented in MATLAB.
Given the input sequence {x(n)}, the desired sequence {d(n)}, step size
△, and the desired length of the adaptive FIR ﬁlter N, we can use
(11.1), (11.2), and (11.8) to determine the adaptive ﬁlter coeﬃcients
{h(n), 0 ≤n ≤N −1} recursively. This is shown in the following func-
tion, called lms.
function [h,y] = lms(x,d,delta,N)
% LMS Algorithm for Coefficient Adjustment
% ----------------------------------------
% [h,y] = lms(x,d,delta,N)
%
h = estimated FIR filter
%
y = output array y(n)
%
x = input array x(n)
%
d = desired array d(n), length must be same as x
% delta = step size
%
N = length of the FIR filter
%
M = length(x); y = zeros(1,M);
h = zeros(1,N);
for n = N:M
x1 = x(n:-1:n-N+1);
y = h * x1’;
e = d(n) - y;
h = h + delta*e*x1;
end
In addition, the lms function provides the output {y(n)} of the adaptive
ﬁlter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

578
Chapter 11
APPLICATIONS IN ADAPTIVE FILTERING
We will apply the LMS algorithm to several practical applications
involving adaptive ﬁltering.
11.2 SYSTEM IDENTIFICATION OR SYSTEM MODELING
To formulate the problem, let us refer to Figure 11.2. We have an un-
known linear system that we wish to identify. The unknown system may
be an all-zero (FIR) system or a pole-zero (IIR) system. The unknown
system will be approximated (modeled) by an FIR ﬁlter of length N. Both
the unknown system and the FIR model are connected in parallel and are
excited by the same input sequence {x(n)}. If {y(n)} denotes the output
of the model and {d(n)} denotes the output of the unknown system, the
error sequence is {e(n) = d(n) −y(n)}. If we minimize the sum of squared
errors, we obtain the same set of linear equations as in (11.7). Therefore,
the LMS algorithm given by (11.8) may be used to adapt the coeﬃcients of
the FIR model so that its output approximates the output of the unknown
system.
11.2.1 PROJECT 11.1: SYSTEM IDENTIFICATION
There are three basic modules that are needed to perform this project.
1. A noise signal generator that generates a sequence of random numbers
with zero mean value. For example, we may generate a sequence of
uniformly distributed random numbers over the interval [−a, a]. Such
a sequence of uniformly distributed numbers has an average value of
zero and a variance of a2/3. This signal sequence, call it {x(n)}, will
be used as the input to the unknown system and the adaptive FIR
model. In this case, the input signal {x(n)} has power Px = a2/3. In
MATLAB this can be implemented using the rand function.
FIGURE 11.2
Block diagram of system identiﬁcation or system modeling
problem
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Suppression of Narrowband Interference in a Wideband Signal
579
2. An unknown system module that may be selected is an IIR ﬁlter and
implemented by its diﬀerence equation. For example, we may select an
IIR ﬁlter speciﬁed by the second-order diﬀerence equation
d(n) = a1d (n −1) + a2d (n −2) + x(n) + b1x (n −1) + b2x (n −2)
(11.11)
where the parameters {a1, a2} determine the positions of the poles and
{b1, b2} determine the positions of the zeros of the ﬁlter. These param-
eters are input variables to the program. This can be implemented by
the filter function.
3. An adaptive FIR ﬁlter module where the FIR ﬁlter has N tap coeﬃ-
cients that are adjusted by means of the LMS algorithm. The length
N of the ﬁlter is an input variable to the program. This can be imple-
mented using the lms function given in the previous section.
The three modules are conﬁgured as shown in Figure 11.2. From this
project, we can determine how closely the impulse response of the FIR
model approximates the impulse response of the unknown system after
the LMS algorithm has converged.
To monitor the convergence rate of the LMS algorithm, we may com-
pute a short-term average of the squared error e2(n) and plot it. That is,
we may compute
ASE(m) = 1
K
n+K
	
k=n+1
e2(k)
(11.12)
where m = n/K = 1, 2, . . . . The averaging interval K may be selected
to be (approximately) K = 10N. The eﬀect of the choice of the step-
size parameter △on the convergence rate of the LMS algorithm may be
observed by monitoring the ASE(m).
Besides the main part of the program, you should also include, as an
aside, the computation of the impulse response of the unknown system,
which can be obtained by exciting the system with a unit sample sequence
δ(n). This actual impulse response can be compared with that of the FIR
model after convergence of the LMS algorithm. The two impulse responses
can be plotted for the purpose of comparison.
11.3 SUPPRESSION OF NARROWBAND INTERFERENCE
IN A WIDEBAND SIGNAL
Let us assume that we have a signal sequence {x(n)} that consists of
a desired wideband signal sequence—say, {w(n)}—corrupted by an ad-
ditive narrowband interference sequence {s(n)}. The two sequences are
uncorrelated. This problem arises in digital communications and in signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

580
Chapter 11
APPLICATIONS IN ADAPTIVE FILTERING
detection, where the desired signal sequence {w(n)} is a spread-spectrum
signal, while the narrowband interference represents a signal from another
user of the frequency band or some intentional interference from a jammer
who is trying to disrupt the communication or detection system.
From a ﬁltering point of view, our objective is to design a ﬁlter that
suppresses the narrowband interference. In eﬀect, such a ﬁlter should place
a notch in the frequency band occupied by the interference. In practice,
however, the frequency band of the interference might be unknown. More-
over, the frequency band of the interference may vary slowly in time.
The narrowband characteristics of the interference allow us to esti-
mate s(n) from past samples of the sequence x(n) = s(n) + w(n) and to
subtract the estimate from x(n). Since the bandwidth of {s(n)} is nar-
row compared to the bandwidth of {w(n)}, the samples of {s(n)} are
highly correlated. On the other hand, the wideband sequence {w(n)} has
a relatively narrow correlation.
The general conﬁguration of the interference suppression system is
shown in Figure 11.3. The signal x(n) is delayed by D samples, where
the delay D is chosen suﬃciently large so that the wideband signal com-
ponents w(n) and w(n −D), which are contained in x(n) and x(n −D),
respectively, are uncorrelated. The output of the adaptive FIR ﬁlter is the
estimate
ˆs(n) =
N−1
	
k=0
h(k)x(n −k −D)
(11.13)
The error signal that is used in optimizing the FIR ﬁlter coeﬃcients is
e(n) = x(n) −ˆs(n). The minimization of the sum of squared errors again
leads to a set of linear equations for determining the optimum coeﬃcients.
Due to the delay D, the LMS algorithm for adjusting the coeﬃcients
recursively becomes
hn(k) = hn−1(k) + △e(n)x(n −k −D),
k = 0, 1, . . . , N −1
n = 1, 2, . . .
(11.14)
FIGURE 11.3
Adaptive ﬁlter for estimating and suppressing a narrowband
interference
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Suppression of Narrowband Interference in a Wideband Signal
581
11.3.1 PROJECT 11.2: SUPPRESSION OF SINUSOIDAL INTERFERENCE
Three basic modules are required to perform this project.
1. A noise signal generator module that generates a wideband sequence
{w(n)} of random numbers with zero mean value. In particular, we may
generate a sequence of uniformly distributed random numbers using
the rand function as previously described in the project on system
identiﬁcation. The signal power is denoted as Pw.
2. A sinusoidal signal generator module that generates a sine wave se-
quence s(n) = A sin ω0n, where 0 < ω0 < π and A is the signal ampli-
tude. The power of the sinusoidal sequence is denoted as Ps.
3. An adaptive FIR ﬁlter module using the lms function, where the FIR
ﬁlter has N tap coeﬃcients that are adjusted by the LMS algorithm.
The length N of the ﬁlter is an input variable to the program.
The three modules are conﬁgured as shown in Figure 11.4. In this
project, the delay D = 1 is suﬃcient, since the sequence {w(n)} is a
white noise (spectrally ﬂat or uncorrelated) sequence. The objective is to
adapt the FIR ﬁlter coeﬃcients and then to investigate the characteristics
of the adaptive ﬁlter.
It is interesting to select the interference signal to be much stronger
than the desired signal w(n), for example, Ps = 10Pw. Note that the
power Px required in selecting the step-size parameter in the LMS algo-
rithm is Px = Ps + Pw. The frequency response characteristic H(ejω) of
the adaptive FIR ﬁlter with coeﬃcients {h(k)} should exhibit a resonant
peak at the frequency of the interference. The frequency response of the
interference suppression ﬁlter is Hs(ejω) = 1−H(ejω), which should then
exhibit a notch at the frequency of the interference.
It is interesting to plot the sequences {w(n)}, {s(n)}, and {x(n)}. It
is also interesting to plot the frequency responses H(ejω) and Hs(ejω)
after the LMS algorithm has converged. The short-time average squared
error ASE(m), deﬁned by (11.12), may be used to monitor the conver-
gence characteristics of the LMS algorithm. The eﬀect of the length of
the adaptive ﬁlter on the quality of the estimate should be investigated.
The project may be generalized by adding a second sinusoid of a diﬀer-
ent frequency. Then H(ejω) should exhibit two resonant peaks, provided
FIGURE 11.4
Conﬁguration of modules for experiment on interference suppres-
sion
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

582
Chapter 11
APPLICATIONS IN ADAPTIVE FILTERING
the frequencies are suﬃciently separated. Investigate the eﬀect of the ﬁlter
length N on the resolution of two closely spaced sinusoids.
11.4 ADAPTIVE LINE ENHANCEMENT
In the preceding section, we described a method for suppressing a strong
narrowband interference from a wideband signal. An adaptive line en-
hancer (ALE) has the same conﬁguration as the interference suppression
ﬁlter in Figure 11.3, except that the objective is diﬀerent.
In the adaptive line enhancer, {s(n)} is the desired signal and {w(n)}
represents a wideband noise component that masks {s(n)}. The desired
signal {s(n)} may be a spectral line (a pure sinusoid) or a relatively
narrowband signal. Usually, the power in the wideband signal is greater
than that in the narrowband signal—that is, Pw > Ps. It is apparent
that the ALE is a self-tuning ﬁlter that has a peak in its frequency re-
sponse at the frequency of the input sinusoid or in the frequency band
occupied by the narrowband signal. By having a narrow bandwidth FIR
ﬁlter, the noise outside the frequency band of the signal is suppressed,
and thus the spectral line is enhanced in amplitude relative to the noise
power in {w(n)}.
11.4.1 PROJECT 11.3: ADAPTIVE LINE ENHANCEMENT
This project requires the same software modules as those used in the
project on interference suppression. Hence the description given in
the preceding section applies directly. One change is that in the ALE, the
condition is that Pw > Ps. Second, the output signal from the ALE is
{s(n)}. Repeat the project described in the previous section under these
conditions.
11.5 ADAPTIVE CHANNEL EQUALIZATION
The speed of data transmission over telephone channels is usually limited
by channel distortion that causes intersymbol interference (ISI). At data
rates below 2400 bits, the ISI is relatively small and is usually not a
problem in the operation of a modem. However, at data rates above 2400
bits, an adaptive equalizer is employed in the modem to compensate for
the channel distortion and thus to allow for highly reliable high-speed
data transmission. In telephone channels, ﬁlters are used throughout the
system to separate signals in diﬀerent frequency bands. These ﬁlters cause
amplitude and phase distortion. The adaptive equalizer is basically an
adaptive FIR ﬁlter with coeﬃcients that are adjusted by means of the
LMS algorithm to correct for the channel distortion.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Channel Equalization
583
FIGURE 11.5
Application of adaptive ﬁltering to adaptive channel equalization
A block diagram showing the basic elements of a modem transmitting
data over a channel is given in Figure 11.5. Initially, the equalizer coeﬃ-
cients are adjusted by transmitting a short training sequence, usually less
than 1 second in duration. After the short training period, the transmitter
begins to transmit the data sequence {a(n)}. To track the possible slow
time variations in the channel, the equalizer coeﬃcients must continue to
be adjusted in an adaptive manner while receiving data. This is usually
accomplished, as illustrated in Figure 11.5, by treating the decisions at
the output of the decision device as correct and by using the decisions in
place of the reference {d(n)} to generate the error signal. This approach
works quite well when decision errors occur infrequently, such as less than
one error in 100 data symbols. The occasional decision errors cause only
a small misadjustment in the equalizer coeﬃcients.
11.5.1 PROJECT 11.4: ADAPTIVE CHANNEL EQUALIZATION
The objective of this project is to investigate the performance of an
adaptive equalizer for data transmission over a channel that causes inter-
symbol interference. The basic conﬁguration of the system to be simulated
is shown in Figure 11.6. As we observe, ﬁve basic modules are required.
Note that we have avoided carrier modulation and demodulation, which
is required in a telephone channel modem. This is done to simplify the
simulation program. However, all processing involves complex arithmetic
operations.
The ﬁve modules are as follows.
1. The data generator module is used to generate a sequence of complex-
valued information symbols {a(n)}. In particular, employ four equally
probable symbols s + js, s −js, −s + js, and −s −js, where s is a
scale factor that may be set to s = 1, or it can be an input parameter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

584
Chapter 11
APPLICATIONS IN ADAPTIVE FILTERING
FIGURE 11.6
Experiment for investigating the performance of an adaptive
equalizer
2. The channel ﬁlter module is an FIR ﬁlter with coeﬃcients {c(n),
0 ≤n ≤K −1} that simulates the channel distortion. For distortion-
less transmission, set c(0) = 1 and c(n) = 0 for 1 ≤n ≤K −1. The
length K of the ﬁlter is an input parameter.
3. The noise generator module is used to generate additive noise that is
usually present in any digital communication system. If we are model-
ing noise that is generated by electronic devices, the noise distribution
should be Gaussian with zero mean. Use the randu function.
4. The adaptive equalizer module is an FIR ﬁlter with tap coeﬃcients
{h(k), 0 < k < N −1}, which are adjusted by the LMS algorithm.
However, due to the use of complex arithmetic, the recursive equation
in the LMS algorithm is slightly modiﬁed to
hn(k) = hn−1(k) + △e(n)x∗(n −k)
(11.15)
where the asterisk denotes the complex conjugate.
5. The decision device module takes the estimate ˆa(n) and quantizes it
to one of the four possible signal points on the basis of the following
decision rule:
Re [ˆa(n)] > 0 and Im [ˆa(n)] > 0 −→
1 + j
Re [ˆa(n)] > 0 and Im [ˆa(n)] < 0 −→
1 −j
Re [ˆa(n)] < 0 and Im [ˆa(n)] > 0 −→−1 + j
Re [ˆa(n)] < 0 and Im [ˆa(n)] < 0 −→−1 −j
The eﬀectiveness of the equalizer in suppressing the ISI introduced
by the channel ﬁlter may be seen by plotting the following relevant
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Channel Equalization
585
sequences in a two-dimensional (real–imaginary) display. The data gener-
ator output {a(n)} should consist of four points with values ±1 ± j. The
eﬀect of channel distortion and additive noise may be viewed by display-
ing the sequence {x(n)} at the input to the equalizer. The eﬀectiveness
of the adaptive equalizer may be assessed by plotting its output {ˆa(n)}
after convergence of its coeﬃcients. The short-time average squared error
ASE(n) may also be used to monitor the convergence characteristics of
the LMS algorithm. Note that a delay must be introduced into the output
of the data generator to compensate for the delays that the signal encoun-
ters due to the channel ﬁlter and the adaptive equalizer. For example,
this delay may be set to the largest integer closest to (N + K)/2. Finally,
an error counter may be used to count the number of symbol errors in the
received data sequence, and the ratio for the number of errors to the total
number of symbols (error rate) may be displayed. The error rate may be
varied by changing the level of the ISI and the level of the additive noise.
It is suggested that simulations be performed for the following three
channel conditions.
a. No ISI:
c(0) = 1, c(n) = 0, 1 ≤n ≤K −1
b. Mild ISI:
c(0) = 1, c(1) = 0.2, c(2) = −0.2, c(n) = 0, 3 ≤n ≤K −1
c. Strong ISI: c(0) = 1, c(1) = 0.5, c(2) = 0.5, c(n) = 0, 3 ≤n ≤K −1
The measured error rate may be plotted as a function of the signal-
to-noise ratio (SNR) at the input to the equalizer, where SNR is deﬁned
as Ps/Pn, where Ps is the signal power, given as Ps = s2, and Pn is the
noise power of the sequence at the output of the noise generator.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

586
C H A P T E R 12
Applications in
Communications
Today MATLAB ﬁnds widespread use in the simulation of a variety of
communication systems. In this chapter, we shall focus on several applica-
tions dealing with waveform representation and coding, especially speech
coding, and with digital communications. In particular, we shall describe
several methods for digitizing analog waveforms, with speciﬁc application
to speech coding and transmission. These methods are pulse-code modula-
tion (PCM), diﬀerential PCM and adaptive diﬀerential PCM (ADPCM),
delta modulation (DM) and adaptive delta modulation (ADM), and lin-
ear predictive coding (LPC). A project is formulated involving each of
these waveform-encoding methods for simulation using MATLAB.
The last three topics treated in this chapter deal with signal-detection
applications that are usually encountered in the implementation of a re-
ceiver in a digital communication system. For each of these topics, we
describe a project that involves the implementations via simulation of the
detection scheme in MATLAB.
12.1 PULSE-CODE MODULATION
Pulse-code modulation is a method for quantizing an analog signal for
the purpose of transmitting or storing the signal in digital form. PCM is
widely used for speech transmission in telephone communications and for
telemetry systems that employ radio transmission. We shall concentrate
our attention on the application of PCM to speech signal processing.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Pulse-Code Modulation
587
Speech signals transmitted over telephone channels are usually limited
in bandwidth to the frequency range below 4 kHz. Hence the Nyquist rate
for sampling such a signal is less than 8 kHz. In PCM, the analog speech
signal is sampled at the nominal rate of 8 kHz (samples per second), and
each sample is quantized to one of 2b levels and represented digitally by
a sequence of b bits. Thus the bit rate required to transmit the digitized
speech signal is 8000 b bits per second.
The quantization process may be modeled mathematically as
˜s(n) = s(n) + q(n)
(12.1)
where ˜s(n) represents the quantized value of s(n) and q(n) represents the
quantization error, which we treat as an additive noise. Assuming that a
uniform quantizer is used and the number of levels is suﬃciently large,
the quantization noise is well characterized statistically by the uniform
probability density function
p(q) = 1
∆,
−∆
2 ≤q ≤∆
2
(12.2)
where the step size of the quantizer is ∆= 2−b. The mean square value
of the quantization error is
E(q2) = ∆2
12 = 2−2b
12
(12.3)
Measured in decibels, the mean square value of the noise is
10 log
∆2
12

= 10 log
2−2b
12

= −6b −10.8 dB
(12.4)
We observe that the quantization noise decreases by 6 dB/bit used
in the quantizer. High-quality speech requires a minimum of 12 bits per
sample and hence a bit rate of 96,000 bits per second (bps) or 96 kbps.
Speech signals have the characteristic that small signal amplitudes
occur more frequently than large signal amplitudes. However, a uniform
quantizer provides the same spacing between successive levels through-
out the entire dynamic range of the signal. A better approach is to use
a nonuniform quantizer, which provides more closely spaced levels at the
low signal amplitudes and more widely spaced levels at the large signal
amplitudes. For a nonuniform quantizer with b bits, the resulting quan-
tization error has a mean square value that is smaller than that given
by (12.4). A nonuniform quantizer characteristic is usually obtained by
passing the signal through a nonlinear device that compresses the signal
amplitude, followed by a uniform quantizer. For example, a logarithmic
compressor employed in United States and Canadian telecommunications
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

588
Chapter 12
APPLICATIONS IN COMMUNICATIONS
systems, called a µ-law compressor, has an input-output magnitude char-
acteristic of the form
y = ln (1 + µ|s|)
ln(1 + µ)
sgn(s);
|s| ≤1, |y| ≤1
(12.5)
where s is the normalized input, y is the normalized output, sgn (·) is the
sign function, and µ is a parameter that is selected to give the desired
compression characteristic.
In the encoding of speech waveforms, the value of µ = 255 has been
adopted as a standard in the United States and Canada. This value results
in about a 24 dB reduction in the quantization noise power relative to uni-
form quantization. Consequently, an 8-bit quantizer used in conjunction
with a µ = 255 logarithmic compressor produces the same quality speech
as a 12-bit uniform quantizer with no compression. Thus the compressed
PCM speech signal has a bit rate of 64 kbps.
The logarithmic compressor standard used in European telecommu-
nication systems is called A-law and is deﬁned as
y =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1 + ln(A|s|)
1 + ln A
sgn(s),
1
A ≤|s| ≤1
A|s|
1 + ln A sgn(s),
0 ≤|s| ≤1
A
(12.6)
where A is chosen as 87.56. Although (12.5) and (12.6) are diﬀerent
nonlinear functions, the two compression characteristics are very similar.
Figure 12.1 illustrates these two compression functions. Note their strong
similarity.
FIGURE 12.1
Comparison of µ-law and A-law nonlinearities
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Pulse-Code Modulation
589
In the reconstruction of the signal from the quantized values, the
decoder employs an inverse logarithmic relation to expand the signal
amplitude. For example, in µ-law the inverse relation is given by
|s| = (1 + µ)|y| −1
µ
;
|y| ≤1, |s| ≤1
(12.7)
The combined compressor-expander pair is termed a compander.
12.1.1 PROJECT 12.1: PCM
The purpose of this project is to gain an understanding of PCM compres-
sion (linear to logarithmic) and PCM expansion (logarithmic to linear).
Write the following three MATLAB functions for this project:
1. a µ-law compressor function to implement (12.5) that accepts a zero-
mean normalized (|s| ≤1) signal and produces a compressed zero-mean
signal with µ as a free parameter that can be speciﬁed,
2. a quantizer function that accepts a zero-mean input and produces an
integer output after b-bit quantization that can be speciﬁed, and
3. a µ-law expander to implement (12.7) that accepts an integer input
and produces a zero-mean output for a speciﬁed µ parameter.
For simulation purposes, generate a large number of samples (10,000
or more) of the following sequences: (a) a sawtooth sequence, (b) an expo-
nential pulse train sequence, (c) a sinusoidal sequence, and (d) a random
sequence with small variance. Care must be taken to generate nonperiodic
sequences by choosing their normalized frequencies as irrational numbers
(i.e., sample values should not repeat). For example, a sinusoidal sequence
can be generated using
s(n) = 0.5 sin(n/33),
0 ≤n ≤10,000
From our discussions in Chapter 2, this sequence is nonperiodic, yet it
has a periodic envelope. Other sequences can also be generated in a sim-
ilar fashion. Process these signals through the above µ-law compressor,
quantizer, and expander functions as shown in Figure 12.2, and compute
FIGURE 12.2
PCM project
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

590
Chapter 12
APPLICATIONS IN COMMUNICATIONS
the signal-to-quantization noise ratio (SQNR) in dB as
SQNR = 10 log10
-
N
n=1 s2(n)
N
n=1 (s(n) −sq(n))2
.
For diﬀerent b-bit quantizers, systematically determine the value of µ
that maximizes the SQNR. Also plot the input and output waveforms,
and comment on the results.
12.2 DIFFERENTIAL PCM (DPCM)
In PCM, each sample of the waveform is encoded independently of all
the other samples. However, most signals, including speech, sampled at
the Nyquist rate or faster exhibit signiﬁcant correlation between succes-
sive samples. In other words, the average change in amplitude between
successive samples is relatively small. Consequently, an encoding scheme
that exploits the redundancy in the samples will result in a lower bit rate
for the speech signal.
A relatively simple solution is to encode the diﬀerences between
successive samples rather than the samples themselves. Since diﬀerences
between samples are expected to be smaller than the actual sampled am-
plitudes, fewer bits are required to represent the diﬀerences. A reﬁnement
of this general approach is to predict the current sample based on the
previous p samples. To be speciﬁc, let s(n) denote the current sample of
speech, and let ˆs(n) denote the predicted value of s(n), deﬁned as
ˆs(n) =
p
	
i=1
a (i) s (n −i)
(12.8)
Thus ˆs(n) is a weighted linear combination of the past p samples, and
the a (i) are the predictor (ﬁlter) coeﬃcients. The a (i) are selected to
minimize some function of the error between s(n) and ˆs(n).
A mathematically and practically convenient error function is the sum
of squared errors. With this as the performance index for the predictor,
we select the a (i) to minimize
Ep
△=
N
	
n=1
e2(n)=
N
	
n=1

s(n) −
p
	
i=1
a (i) s (n −i)
2
(12.9)
=rss (0) −2
p
	
i=1
a (i) rss (i) +
p
	
i=1
p
	
j=1
a (i) a (j) rss (i −j)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Diﬀerential PCM (DPCM)
591
where rss (m) is the autocorrelation function of the sampled signal se-
quence s(n), deﬁned as
rss (m) =
N
	
i=1
s (i) s (i + m)
(12.10)
Minimization of Ep with respect to the predictor coeﬃcients {ai(n)} re-
sults in the set of linear equations, called the normal equations,
p
	
i=1
a (i) rss (i −j) = rss (j) ,
j = 1, 2, . . . , p
(12.11)
or in the matrix form,
Ra = r =⇒a = R−1r
(12.12)
where R is the autocorrelation matrix, a is the coeﬃcient vector, and r
is the autocorrelation vector. Thus the values of the predictor coeﬃcients
are established.
Having described the method for determining the predictor coeﬃ-
cients, let us now consider the block diagram of a practical DPCM system,
shown in Figure 12.3. In this conﬁguration, the predictor is implemented
with the feedback loop around the quantizer. The input to the predictor
is denoted as ˜s(n), which represents the signal sample s(n) modiﬁed by
the quantization process, and the output of the predictor is
A˜s =
p
	
i=1
a (i) ˜s (n −i)
(12.13)
The diﬀerence
e(n) = s(n) −A˜s(n)
(12.14)
is the input to the quantizer, and ˜e(n) denotes the output. Each value of
the quantized prediction error ˜e(n) is encoded into a sequence of binary
FIGURE 12.3
Block diagram of a DPCM transcoder: (a) encoder, (b) decoder
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

592
Chapter 12
APPLICATIONS IN COMMUNICATIONS
digits and transmitted over the channel to the receiver. The quantized
error ˜e(n) is also added to the predicted value A˜s(n) to yield ˜s(n).
At the receiver, the same predictor that was used at the transmitting
end is synthesized, and its output A˜s(n) is added to ˜e(n) to yield ˜s(n). The
signal ˜s(n) is the desired excitation for the predictor and also the desired
output sequence from which the reconstructed signal ˜s (t) is obtained by
ﬁltering, as shown in Figure 12.3b.
The use of feedback around the quantizer, as described, ensures that
the error in ˜s(n) is simply the quantization error q(n) = ˜e(n) −e(n)
and that there is no accumulation of previous quantization errors in the
implementation of the decoder. That is,
q(n) = ˜e(n) −e(n) = ˜e(n) −s(n) + A˜s(n) = ˜s(n) −s(n)
(12.15)
Hence ˜s(n) = s(n) + q(n). This means that the quantized sample ˜s(n)
diﬀers from the input s(n) by the quantization error q(n) indepen-
dent of the predictor used. Therefore, the quantization errors do not
accumulate.
In the DPCM system illustrated in Figure 12.3, the estimate or pre-
dicted value ˜s(n) of the signal sample s(n) is obtained by taking a linear
combination of past values ˜s (n −k) ,
k = 1, 2, . . . , p, as indicated by
(12.13). An improvement in the quality of the estimate is obtained by
including linearly ﬁltered past values of the quantized error. Speciﬁcally,
the estimate of s(n) may be expressed as
A˜s(n) =
p
	
i=1
a (i) ˜s (n −i) +
m
	
i=1
b (i) ˜e (n −i)
(12.16)
where b (i) are the coeﬃcients of the ﬁlter for the quantized error sequence
˜e(n). The block diagram of the encoder at the transmitter and the decoder
at the receiver are shown in Figure 12.4. The two sets of coeﬃcients
a (i) and b (i) are selected to minimize some function of the error e(n) =
˜s(n) −s(n), such as the sum of squared errors.
By using a logarithmic compressor and a 4-bit quantizer for the error
sequence e(n), DPCM results in high-quality speech at a rate of 32 kbps,
which is a factor of 2 lower than logarithmic PCM.
12.2.1 PROJECT 12.2: DPCM
The objective of this project is to gain understanding of the DPCM encod-
ing and decoding operations. For simulation purposes, generate correlated
random sequences using a pole-zero signal model of the form
s(n) = a (1) s (n −1) + b0x(n) + b1x (n −1)
(12.17)
where x(n) is a zero-mean unit variance Gaussian sequence. This can be
done using the filter function. The sequences developed in Project 12.1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive PCM and DPCM (ADPCM)
593
FIGURE 12.4
DPCM modiﬁed by the linearly ﬁltered error sequence
can also be used for simulation. Develop the following three MATLAB
modules for this project:
1. a model predictor function to implement (12.12), given the input signal
s(n);
2. a DPCM encoder function to implement the block diagram of
Figure 12.3a, which accepts a zero-mean input sequence and produces a
quantized b-bit integer error sequence, where b is a free parameter; and
3. a DPCM decoder function of Figure 12.3b, which reconstructs the
signal from the quantized error sequence.
Experiment with several p-order prediction models for a given signal
and determine the optimum order. Compare this DPCM implementation
with the PCM system of Project 12.1 (at the end of the chapter) and
comment on the results. Extend this implementation to include an mth-
order moving average ﬁlter as indicated in (12.16).
12.3 ADAPTIVE PCM AND DPCM (ADPCM)
In general, the power in a speech signal varies slowly with time. PCM
and DPCM encoders, however, are designed on the basis that the speech
signal power is constant, and hence the quantizer is ﬁxed. The eﬃciency
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

594
Chapter 12
APPLICATIONS IN COMMUNICATIONS
and performance of these encoders can be improved by having them adapt
to the slowly time-variant power level of the speech signal.
In both PCM and DPCM, the quantization error q(n) resulting from a
uniform quantizer operating on a slowly varying power level input signal
will have a time-variant variance (quantization noise power). One im-
provement that reduces the dynamic range of the quantization noise is
the use of an adaptive quantizer.
Adaptive quantizers can be classiﬁed as feedforward or feedback. A
feedforward adaptive quantizer adjusts its step size for each signal sample,
based on a measurement of the input speech signal variance (power). For
example, the estimated variance, based as a sliding window estimator, is
ˆσ2
n+1 = 1
M
n+1
	
k=n+1−M
s2 (k)
(12.18)
Then the step size for the quantizer is
∆(n + 1) = ∆(n)ˆσn+1
(12.19)
In this case, it is necessary to transmit ∆(n + 1) to the decoder in order
for it to reconstruct the signal.
A feedback-adaptive quantizer employs the output of the quantizer in
the adjustment of the step size. In particular, we may set the step size as
∆(n + 1) = α(n)∆(n)
(12.20)
where the scale factor α(n) depends on the previous quantizer output.
For example, if the previous quantizer output is small, we may select
α(n) < 1 in order to provide for ﬁner quantization. On the other hand,
if the quantizer output is large, then the step size should be increased
to reduce the possibility of signal clipping. Such an algorithm has been
successfully used in the encoding of speech signals. Figure 12.5 illustrates
such a (3-bit) quantizer in which the step size is adjusted recursively
according to the relation
∆(n + 1) = ∆(n) · M(n)
where M(n) is a multiplication factor whose value depends on the quan-
tizer level for the sample s(n) and ∆(n) is the step size of the quantizer for
processing s(n). Values of the multiplication factors optimized for speech
encoding have been given by [39]. These values are displayed in Table 12.1
for 2-, 3-, and 4-bit quantization for PCM and DPCM.
In DPCM, the predictor can also be made adaptive. Thus in ADPCM,
the coeﬃcients of the predictor are changed periodically to reﬂect the
changing signal statistics of the speech. The linear equations given by
(12.11) still apply, but the short-term autocorrelation function of s(n),
rss (m) changes with time.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive PCM and DPCM (ADPCM)
595
FIGURE 12.5
Example of a quantizer with an adaptive step size ([39])
TABLE 12.1
Multiplication factors for adaptive step-size adjustment ([39])
PCM
DPCM
2
3
4
2
3
4
M(1)
0.60
0.85
0.80
0.80
0.90
0.90
M(2)
2.20
1.00
0.80
1.60
0.90
0.90
M(3)
1.00
0.80
1.25
0.90
M(4)
1.50
0.80
1.70
0.90
M(5)
0.80
1.20
M(6)
0.80
1.60
M(7)
0.80
2.00
M(8)
0.80
2.40
12.3.1 ADPCM STANDARD
Figure 12.6 illustrates, in block diagram form, a 32 kbps ADPCM en-
coder and decoder that has been adopted as an international (CCITT)
standard for speech transmission over telephone channels. The ADPCM
encoder is designed to accept 8-bit PCM compressed signal samples at
64 kbps, and by means of adaptive prediction and adaptive 4-bit quanti-
zation to reduce the bit rate over the channel to 32 kbps. The ADPCM
decoder accepts the 32 kbps data stream and reconstructs the signal
in the form of an 8-bit compressed PCM at 64 kbps. Thus we have a
conﬁguration shown in Figure 12.7, where the ADPCM encoder/decoder
is embedded into a PCM system. Although the ADPCM encoder/decoder
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

596
Chapter 12
APPLICATIONS IN COMMUNICATIONS
FIGURE 12.6
ADPCM block diagram
could be used directly on the speech signal, the interface to the PCM
system is necessary in practice in order to maintain compatibility with
existing PCM systems that are widely used in the telephone network.
The ADPCM encoder accepts the 8-bit PCM compressed signal and
expands it to a 14-bit-per-sample linear representation for processing. The
predicted value is subtracted from this 14-bit linear value to produce a
diﬀerence signal sample that is fed to the quantizer. Adaptive quantiza-
tion is performed on the diﬀerence signal to produce a 4-bit output for
transmission over the channel.
Both the encoder and decoder update their internal variables, based
only on the ADPCM values that are generated. Consequently, an ADPCM
FIGURE 12.7
ADPCM interface to PCM system
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Delta Modulation (DM)
597
decoder, including an inverse adaptive quantizer, is embedded in the
encoder so that all internal variables are updated, based on the same
data. This ensures that the encoder and decoder operate in synchronism
without the need to transmit any information on the values of internal
variables.
The adaptive predictor computes a weighted average of the last six
dequantized diﬀerence values and the last two predicted values. Hence
this predictor is basically a two-pole (p = 2) and six-zero (m = 6) ﬁlter
governed by the diﬀerence equation given by (12.16). The ﬁlter coeﬃcients
are updated adaptively for every new input sample.
At the receiving decoder and at the decoder that is embedded in
the encoder, the 4-bit transmitted ADPCM value is used to update the
inverse adaptive quantizer, whose output is a dequantized version of the
diﬀerence signal. This dequantized value is added to the value generated
by the adaptive predictor to produce the reconstructed speech sample.
This signal is the output of the decoder, which is converted to compressed
PCM format at the receiver.
12.3.2 PROJECT 12.3: ADPCM
The objective of this project is to gain familiarity with, and understanding
of, ADPCM and its interface with a PCM encoder/decoder (transcoder).
As described, the ADPCM transcoder is inserted between the PCM com-
pressor and the PCM expander as shown in Figure 12.7. Use the already
developed MATLAB PCM and DPCM modules for this project.
The input to the PCM-ADPCM transcoder system can be supplied
from internally generated waveform data ﬁles, just as in the case of the
PCM project. The output of the transcoder can be plotted. Compar-
isons should be made between the output signal from the PCM-ADPCM
transcoder with the signal from the PCM transcoder (PCM Project 12.1),
and with the original input signal.
12.4 DELTA MODULATION (DM)
Delta modulation may be viewed as a simpliﬁed form of DPCM in which
a two-level (1-bit) quantizer is used in conjunction with a ﬁxed ﬁrst-
order predictor. The block diagram of a DM encoder-decoder is shown in
Figure 12.8. We note that
A˜s(n) = ˜s (n −1) = A˜s (n −1) + ˜e (n −1)
(12.21)
Since
q(n) = ˜e(n) −e(n) = ˜e(n) −

s(n) −A˜s(n)

Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

598
Chapter 12
APPLICATIONS IN COMMUNICATIONS
FIGURE 12.8
Block diagram of a delta modulation system
it follows that
A˜s(n) = s (n −1) + q (n −1)
(12.22)
Thus the estimated (predicted) value of s(n) is really the previous sam-
ple s (n −1) modiﬁed by the quantization noise q (n −1). We also note
that the diﬀerence equation in (12.21) represents an integrator with an
input ˜e(n). Hence an equivalent realization of the one-step predictor is an
accumulator with an input equal to the quantized error signal ˜e(n). In gen-
eral, the quantized error signal is scaled by some value—say, ∆1—which is
called the step size. This equivalent realization is illustrated in Figure 12.9.
In eﬀect, the encoder shown in Figure 12.9 approximates a waveform s (t)
FIGURE 12.9
An equivalent realization of a delta modulation system
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Delta Modulation (DM)
599
by a linear staircase function. In order for the approximation to be rela-
tively good, the waveform s (t) must change slowly relative to the sampling
rate. This requirement implies that the sampling rate must be several (a
factor of at least 5) times the Nyquist rate. A lowpass ﬁlter is usually
incorporated into the decoder to smooth out discontinuities in the recon-
structed signal.
12.4.1 ADAPTIVE DELTA MODULATION (ADM)
At any given sampling rate, the performance of the DM encoder is limited
by two types of distortion, as shown in Figure 12.10. One is called slope-
overload distortion. It is due to the use of a step size ∆1 that is too small to
follow portions of the waveform that have a steep slope. The second type
of distortion, called granular noise, results from using a step size that is too
large in parts of the waveform having a small slope. The need to minimize
both of these two types of distortion results in conﬂicting requirements in
the selection of the step size ∆1.
An alternative solution is to employ a variable size that adapts itself
to the short-term characteristics of the source signal. That is, the step size
is increased when the waveform has a steep slope and decreased when the
waveform has a relatively small slope.
A variety of methods can be used to set adaptively the step size in
every iteration. The quantized error sequence ˜e(n) provides a good indica-
tion of the slope characteristics of the waveform being encoded. When the
quantized error ˜e(n) is changing signs between successive iterations, this
is an indication that the slope of the waveform in the locality is relatively
small. On the other hand, when the waveform has a steep slope, successive
values of the error ˜e(n) are expected to have identical signs. From these ob-
servations, it is possible to devise algorithms that decrease or increase the
step size, depending on successive values of ˜e(n). A relatively simple rule
devised by [38] is to vary adaptively the step size according to the relation
∆(n) = ∆(n −1) K˜e(n)˜e(n−1),
n = 1, 2, . . .
(12.23)
FIGURE 12.10
Two types of distortion in the DM encoder
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

600
Chapter 12
APPLICATIONS IN COMMUNICATIONS
FIGURE 12.11
An example of a delta modulation system with adaptive step size
where K ≥1 is a constant that is selected to minimize the total distortion.
A block diagram of a DM encoder-decoder that incorporates this adaptive
algorithm is illustrated in Figure 12.11.
Several other variations of adaptive DM encoding have been inves-
tigated and described in the technical literature. A particularly eﬀective
and popular technique ﬁrst proposed by [27] is called continuously variable
slope delta modulation (CVSD). In CVSD, the adaptive step-size param-
eter may be expressed as
∆(n) = α∆(n −1) + k1
(12.24)
if ˜e(n), ˜e (n −1), and ˜e(n −2) have the same sign; otherwise,
∆(n) = α∆(n −1) + k2
(12.25)
The parameters α, k1, and k2 are selected such that 0 < α < 1 and
k1 > k2 > 0. For more discussion on this and other variations of adaptive
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Linear Predictive Coding (LPC) of Speech
601
DM, the interested reader is referred to the papers by Jayant [39] and
Flanagan et al. [15] and to the extensive references contained in these
papers.
12.4.2 PROJECT 12.4: DM AND ADM
The purpose of this project is to gain an understanding of delta modula-
tion and adaptive delta modulation for coding of waveforms. This project
involves writing MATLAB functions for the DM encoder and decoder as
shown in Figure 12.9, and for the ADM encoder and decoder shown in
Figure 12.11. The lowpass ﬁlter at the decoder can be implemented as a
linear-phase FIR ﬁlter. For example, a Hanning ﬁlter that has the impulse
response
h(n) = 1
2

1 −cos
 2πn
N −1

,
0 ≤n ≤N −1
(12.26)
may be used, where the length N may be selected in the range 5 ≤N ≤15.
The input to the DM and ADM systems can be supplied from the
waveforms generated in Project 12.1 except that the sampling rate should
be higher by a factor of 5 to 10. The output of the decoder can be plotted.
Comparisons should be made between the output signal from the DM and
ADM decoders and the original input signal.
12.5 LINEAR PREDICTIVE CODING (LPC) OF SPEECH
The linear predictive coding (LPC) method for speech analysis and syn-
thesis is based on modeling the vocal tract as a linear all-pole (IIR) ﬁlter
having the system function
H (z) =
G
1 +
p
	
k=1
ap (k) z−k
(12.27)
where p is the number of poles, G is the ﬁlter gain, and {ap (k)} are the
parameters that determine the poles. There are two mutually exclusive
excitation functions to model voiced and unvoiced speech sounds. On a
short-time basis, voiced speech is periodic with a fundamental frequency
F0, or a pitch period 1/F0, which depends on the speaker. Thus voiced
speech is generated by exciting the all-pole ﬁlter model by a periodic
impulse train with a period equal to the desired pitch period. Unvoiced
speech sounds are generated by exciting the all-pole ﬁlter model by the
output of a random-noise generator. This model is shown in Figure 12.12.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

602
Chapter 12
APPLICATIONS IN COMMUNICATIONS
FIGURE 12.12
Block diagram model for the generation of a speech signal
Given a short-time segment of a speech signal, usually about 20 ms
or 160 samples at an 8 kHz sampling rate, the speech encoder at the
transmitter must determine the proper excitation function, the pitch pe-
riod for voiced speech, the gain parameter G, and the coeﬃcients ap (k).
A block diagram that illustrates the speech encoding system is given in
Figure 12.13. The parameters of the model are determined adaptively
from the data and encoded into a binary sequence and transmitted to the
receiver. At the receiver, the speech signal is synthesized from the model
and the excitation signal.
The parameters of the all-pole ﬁlter model are easily determined from
the speech samples by means of linear prediction. To be speciﬁc, the
output of the FIR linear prediction ﬁlter is
ˆs(n) = −
p
	
k=1
ap (k) s (n −k)
(12.28)
FIGURE 12.13
Encoder and decoder for LPC
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Linear Predictive Coding (LPC) of Speech
603
and the corresponding error between the observed sample s(n) and the
predicted value ˆs(n) is
e(n) = s(n) +
p
	
k=1
ap (k) s (n −k)
(12.29)
By minimizing the sum of squared errors, that is,
E =
N
	
n=0
e2(n) =
N
	
n=0

s(n) +
p
	
k=1
ap (k) s (n −k)
2
(12.30)
we can determine the pole parameters {ap (k)} of the model. The result
of diﬀerentiating E with respect to each of the parameters and equating
the result to zero is a set of p linear equations
p
	
k=1
ap (k) rss (m −k) = −rss (m) ,
m = 1, 2, . . . , p
(12.31)
where rss (m) is the autocorrelation of the sequence s(n) deﬁned as
rss (m) =
N
	
n=0
s(n)s (n + m)
(12.32)
The linear equation (12.31) can be expressed in matrix form as
Rssa = −rss
(12.33)
where Rss is a p × p autocorrelation matrix, rss is a p × 1 autocorrelation
vector, and a is a p × 1 vector of model parameters. Hence
a = −R−1
ss rss
(12.34)
These equations can also be solved recursively and most eﬃciently, with-
out resorting to matrix inversion, by using the Levinson–Durbin algorithm
[54]. However, in MATLAB it is convenient to use the matrix inversion.
The all-pole ﬁlter parameters {ap (k)} can be converted to the all-pole
lattice parameters {Ki} (called the reﬂection coeﬃcients) using the
MATLAB function dir2latc developed in Chapter 6.
The gain parameter of the ﬁlter can be obtained by noting that its
input-output equation is
s(n) = −
p
	
k=1
ap (k) s (n −k) + Gx(n)
(12.35)
where x(n) is the input sequence. Clearly,
Gx(n) = s(n) +
p
	
k=1
ap (k) s (n −k) = e(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

604
Chapter 12
APPLICATIONS IN COMMUNICATIONS
Then
G2
N−1
	
n=0
x2(n) =
N−1
	
n=0
e2(n)
(12.36)
If the input excitation is normalized to unit energy by design, then
G2 =
N−1
	
n=0
e2(n) = rss (0) +
p
	
k=1
ap (k) rss (k)
(12.37)
Thus G2 is set equal to the residual energy resulting from the least-squares
optimization.
Once the LPC coeﬃcients are computed, we can determine whether
the input speech frame is voiced, and if so, what the pitch is. This is
accomplished by computing the sequence
re(n) =
p
	
k=1
ra (k) rss (n −k)
(12.38)
where ra (k) is deﬁned as
ra (k) =
p
	
i=1
ap (i) ap (i + k)
(12.39)
which is the autocorrelation sequence of the prediction coeﬃcients.
The pitch is detected by ﬁnding the peak of the normalized sequence
re(n)/re (0) in the time interval that corresponds to 3 to 15 ms in the
20 ms sampling frame. If the value of this peak is at least 0.25, the frame
of speech is considered voiced with a pitch period equal to the value of
n = Np, where re (Np) /re (0) is a maximum. If the peak value is less than
0.25, the frame of speech is considered unvoiced and the pitch is zero.
The values of the LPC coeﬃcients, the pitch period, and the type of
excitation are transmitted to the receiver, where the decoder synthesizes
the speech signal by passing the proper excitation through the all-pole
ﬁlter model of the vocal tract. Typically, the pitch period requires 6 bits,
and the gain parameter may be represented by 5 bits after its dynamic
range is compressed logarithmically. If the prediction coeﬃcients were to
be coded, they would require between 8 to 10 bits per coeﬃcient for accu-
rate representation. The reason for such high accuracy is that relatively
small changes in the prediction coeﬃcients result in a large change in
the pole positions of the ﬁlter model. The accuracy requirements are less-
ened by transmitting the reﬂection coeﬃcients {Ki}, which have a smaller
dynamic range—that is, |Ki| < 1. These are adequately represented by
6 bits per coeﬃcient. Thus for a tenth-order predictor, the total number
of bits assigned to the model parameters per frame is 72. If the model
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Dual-Tone Multifrequency (DTMF) Signals
605
parameters are changed every 20 µsec, the resulting bit rate is 3,600 bps.
Since the reﬂection coeﬃcients are usually transmitted to the receiver, the
synthesis ﬁlter at the receiver is implemented as an all-pole lattice ﬁlter,
described in Chapter 6.
12.5.1 PROJECT 12.5: LPC
The objective of this project is to analyze a speech signal through an
LPC coder and then to synthesize it through the corresponding PLC
decoder. Use several .wav sound ﬁles (sampled at 8000 sam/sec rate),
which are available in MATLAB for this purpose. Divide speech signals
into short-time segments (with lengths between 120 and 150 samples) and
process each segment to determine the proper excitation function (voiced
or unvoiced), the pitch period for voiced speech, the coeﬃcients {ap (k)}
(p ≤10), and the gain G. The decoder that performs the synthesis is an
all-pole lattice ﬁlter whose parameters are the reﬂection coeﬃcients that
can be determined from {ap (k)}. The output of this project is a syn-
thetic speech signal that can be compared with the original speech signal.
The distortion eﬀects due to LPC analysis/synthesis may be assessed
qualitatively.
12.6 DUAL-TONE MULTIFREQUENCY (DTMF) SIGNALS
DTMF is the generic name for push-button telephone signaling that is
equivalent to the Touch Tone system in use within the Bell System. DTMF
also ﬁnds widespread use in electronic mail systems and telephone banking
systems in which the user can select options from a menu by sending
DTMF signals from a telephone.
In a DTMF signaling system, a combination of a high-frequency tone
and a low-frequency tone represent a speciﬁc digit or the characters *
and #. The eight frequencies are arranged as shown in Figure 12.14, to
accommodate a total of 16 characters, 12 of which are assigned as shown,
while the other four are reserved for future use.
DTMF signals are easily generated in software and detected by means
of digital ﬁlters, also implemented in software, that are tuned to the eight
frequency tones. Usually, DTMF signals are interfaced to the analog world
via a codec (coder/decoder) chip or by linear A/D and D/A converters.
Codec chips contain all the necessary A/D and D/A, sampling, and ﬁl-
tering circuitry for a bidirectional analog/digital interface.
The DTMF tones may be generated either mathematically or from
a look-up table. In a hardware implementation (e.g., in a digital signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

606
Chapter 12
APPLICATIONS IN COMMUNICATIONS
FIGURE 12.14
DTMF digits
processor), digital samples of two sine waves are generated mathemati-
cally, scaled, and added together. The sum is logarithmically compressed
and sent to the codec for conversion to an analog signal. At an 8 kHz
sampling rate, the hardware must output a sample every 125 ms. In this
case, a sine look-up table is not used, because the values of the sine wave
can be computed quickly without using the large amount of data mem-
ory that a table look-up would require. For simulation and investigation
purposes, the look-up table might be a good approach in MATLAB.
At the receiving end, the logarithmically compressed, 8-bit digital
data words from the codec are received and logarithmically expanded to
their 16-bit linear format. Then the tones are detected to decide on the
transmitted digit. The detection algorithm can be a DFT implementa-
tion using the FFT algorithm or a ﬁlter bank implementation. For the
relatively small number of tones to be detected, the ﬁlter bank implemen-
tation is more eﬃcient. We now describe the use of the Goertzel algorithm
to implement the eight tuned ﬁlters.
Recall from the discussion in Chapter 5 that the DFT of an N-point
data sequence {x(n)} is
X (k) =
N−1
	
n=0
x(n)W nk
N ,
k = 0, 1, . . . , N −1
(12.40)
If the FFT algorithm is used to perform the computation of the DFT,
the number of computations (complex multiplications and additions) is
N log2 N. In this case, we obtain all N values of the DFT at once.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Dual-Tone Multifrequency (DTMF) Signals
607
However, if we desire to compute only M points of the DFT, where
M < log2 N, then a direct computation of the DFT is more eﬃcient. The
Goertzel algorithm, which is now described, is basically a linear ﬁltering
approach to the computation of the DFT and provides an alternative to
direct computation.
12.6.1 THE GOERTZEL ALGORITHM
The Goertzel algorithm exploits the periodicity of the phase factors {W k
N}
and allows us to express the computation of the DFT as a linear ﬁltering
operation. Since W −kN
N
= 1, we can multiply the DFT by this factor. Thus
X (k) = W −kN
N
X (k) =
N−1
	
m=0
x (m) W −k(N−m)
N
(12.41)
We note that (12.41) is in the form of a convolution. Indeed, if we deﬁne
the sequence yk(n) as
yk(n) =
N−1
	
m=0
x (m) W −k(n−m)
N
(12.42)
then it is clear that yk(n) is the convolution of the ﬁnite-duration input
sequence x(n) of length N with a ﬁlter that has an impulse response
hk(n) = W −kn
N
u(n)
(12.43)
The output of this ﬁlter at n = N yields the value of the DFT at the
frequency ωk = 2πk/N. That is,
X (k) = yk(n)|n=N
(12.44)
as can be veriﬁed by comparing (12.41) with (12.42).
The ﬁlter with impulse response hk(n) has the system function
Hk (z) =
1
1 −W −k
N z−1
(12.45)
This ﬁlter has a pole on the unit circle at the frequency ωk = 2πk/N.
Thus the entire DFT can be computed by passing the block of input data
into a parallel bank of N single-pole ﬁlters (resonators), where each ﬁlter
has a pole at the corresponding frequency of the DFT.
Instead of performing the computation of the DFT as in (12.42), via
convolution, we can use the diﬀerence equation corresponding to the ﬁlter
given by (12.45) to compute yk(n) recursively. Thus we have
yk(n) = W −k
N yk (n −1) + x(n),
yk (−1) = 0
(12.46)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

608
Chapter 12
APPLICATIONS IN COMMUNICATIONS
FIGURE 12.15
Realization of two-pole resonator for computing the DFT
The desired output is X (k) = yk(N). To perform this computation, we
can compute once and store the phase factor W −k
N .
The complex multiplications and additions inherent in (12.46) can
be avoided by combining the pairs of resonators possessing complex con-
jugate poles. This leads to two-pole ﬁlters with system functions of the
form
Hk (z) =
1 −W k
Nz−1
1 −2 cos (2πk/N) z−1 + z−2
(12.47)
The realization of the system illustrated in Figure 12.15 is described by
the diﬀerence equations
vk(n) = 2 cos 2πk
N vk (n −1) −vk(n −2) + x(n)
(12.48)
yk(n) = vk(n) −W k
Nvk (n −1)
(12.49)
with initial conditions vk (−1) = vk (−2) = 0. This is the Goertzel algo-
rithm.
The recursive relation in (12.48) is iterated for n = 0, 1, . . . , N, but the
equation in (12.49) is computed only once, at time n = N. Each iteration
requires one real multiplication and two additions. Consequently, for a real
input sequence x(n), this algorithm requires N + 1 real multiplications to
yield not only X (k) but also, due to symmetry, the value of X (N −k).
We can now implement the DTMF decoder by use of the Goertzel
algorithm. Since there are eight possible tones to be detected, we require
eight ﬁlters of the type given by (12.47), with each ﬁlter tuned to one of
the eight frequencies. In the DTMF detector, there is no need to compute
the complex value X (k); the magnitude |X(k)| or the magnitude-squared
value |X(k)|2 will suﬃce. Consequently, the ﬁnal step in the computation
of the DFT value involving the numerator term (feedforward part of the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Binary Digital Communications
609
ﬁlter computation) can be simpliﬁed. In particular, we have
|X(k)|2 = |yk(N)|2 =
vk(N) −W k
Nvk (N −1)
2
(12.50)
= v2
k(N) + v2
k (N −1) −

2 cos 2πk
N

vk(N)vk (N −1)
Thus complex-valued arithmetic operations are completely eliminated in
the DTMF detector.
12.6.2 PROJECT 12.6: DTMF SIGNALING
The objective of this project is to gain an understanding of the DTMF
tone generation software and the DTMF decoding algorithm (the Goertzel
algorithm). Design the following MATLAB modules:
1. a tone generation function that accepts an array containing dial-
ing digits and produces a signal containing appropriate tones (from
Figure 12.14) of 0.5 sec duration for each digit at 8 kHz sampling
frequency,
2. a dial-tone generator generating samples of (350 + 440) Hz frequency
at 8 kHz sampling interval for a speciﬁed amount of duration, and
3. a decoding function to implement (12.50) that accepts a DTMF signal
and produces an array containing dialing digits.
Generate several dialing list arrays containing a mix of digits and
dial tones. Experiment with the tone generation and detection modules
and comment on your observations. Use MATLAB’s sound generation
capabilities to listen to the tones and to observe the frequency components
of the generated tones.
12.7 BINARY DIGITAL COMMUNICATIONS
Digitized speech signals that have been encoded via PCM, ADPCM,
DM, and LPC are usually transmitted to the decoder by means of digital
modulation. A binary digital communications system employs two signal
waveforms—say, s1(t) = s(t) and s2(t) = −s(t)—to transmit the binary
sequence representing the speech signal. The signal waveform s(t), which
is nonzero over the interval 0 ≤t ≤T, is transmitted to the receiver if the
data bit is a 1, and the signal waveform −s(t), 0 ≤t ≤T is transmitted
if the data bit is a 0. The time interval T is called the signal interval, and
the bit rate over the channel is R = 1/T bits per second. A typical signal
waveform s(t) is a rectangular pulse—that is, s(t) = A, 0 ≤t ≤T—which
has energy A2T.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

610
Chapter 12
APPLICATIONS IN COMMUNICATIONS
In practice, the signal waveforms transmitted over the channel are
corrupted by additive noise and other types of channel distortions that
ultimately limit the performance of the communications system. As a
measure of performance, we normally use the average probability of error,
which is often called the bit error rate.
12.7.1 PROJECT 12.7: BINARY DATA COMMUNICATIONS SYSTEM
The purpose of this project is to investigate the performance of a binary
data communications system on an additive noise channel by means of
simulation. The basic conﬁguration of the system to be simulated is shown
in Figure 12.16. Five MATLAB functions are required.
1. A binary data generator module that generates a sequence of indepen-
dent binary digits with equal probability.
2. A modulator module that maps a binary digit 1 into a sequence of
M consecutive +1’s, and maps a binary digit 0 into a sequence of M
consecutive −1’s. Thus the M consecutive +1’s represent a sampled
version of the rectangular pulse.
3. A noise generator that generates a sequence of uniformly distributed
numbers over the interval (−a, a). Each noise sample is added to a
corresponding signal sample.
4. A demodulator module that sums the M successive outputs of the
noise corrupted sequence +1’s or −1’s received from the channel. We
assume that the demodulator is time synchronized so that it knows the
beginning and end of each waveform.
5. A detector and error-counting module. The detector compares the out-
put of the modulator with zero and decides in favor of 1 if the output
is greater than zero and in favor of zero if the output is less than zero.
If the output of the detector does not agree with the transmitted bit
from the transmitter, an error is counted by the counter. The error rate
depends on the ratio (called signal-to-noise ratio) of the size of M to
the additive noise power, which is Pn = a2/3.
The measured error rate can be plotted for diﬀerent signal-to-noise
ratios, either by changing M and keeping Pn ﬁxed or vice versa.
FIGURE 12.16
Model of binary data communications system
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Spread-Spectrum Communications
611
12.8 SPREAD-SPECTRUM COMMUNICATIONS
Spread-spectrum signals are often used in the transmission of digital data
over communication channels that are corrupted by interference due to
intentional jamming or from other users of the channel (e.g., cellular tele-
phones and other wireless applications). In applications other than com-
munications, spread-spectrum signals are used to obtain accurate range
(time delay) and range rate (velocity) measurements in radar and navi-
gation. For the sake of brevity, we shall limit our discussion to the use of
spread spectrum for digital communications. Such signals have the char-
acteristic that their bandwidth is much greater than the information rate
in bits per second.
In combatting intentional interference (jamming), it is important to
the communicators that the jammer who is trying to disrupt their com-
munication does not have prior knowledge of the signal characteristics. To
accomplish this, the transmitter introduces an element of unpredictability
or randomness (pseudo-randomness) in each of the possible transmitted
signal waveforms, which is known to the intended receiver, but not to the
jammer. As a consequence, the jammer must transmit an interfering sig-
nal without knowledge of the pseudo-random characteristics of the desired
signal.
Interference from other users arises in multiple-access communica-
tions systems in which a number of users share a common communications
channel. At any given time, a subset of these users may transmit informa-
tion simultaneously over a common channel to corresponding receivers.
The transmitted signals in this common channel may be distinguished
from one another by superimposing a diﬀerent pseudo-random pattern,
called a multiple-access code, in each transmitted signal. Thus a particular
receiver can recover the transmitted data intended for it by knowing the
pseudo-random pattern—that is, the key used by the corresponding trans-
mitter. This type of communication technique, which allows multiple users
to simultaneously use a common channel for data transmission, is called
code division multiple access (CDMA).
The block diagram shown in Figure 12.17 illustrates the basic el-
ements of a spread-spectrum digital communications system. It diﬀers
FIGURE 12.17
Basic spread-spectrum digital communications system
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

612
Chapter 12
APPLICATIONS IN COMMUNICATIONS
from a conventional digital communications system by the inclusion of
two identical pseudo-random pattern generators, one that interfaces with
the modulator at the transmitting end and a second that interfaces with
the demodulator at the receiving end. The generators generate a pseudo-
random or pseudo-noise (PN) binary-valued sequence (±1’s), which is
impressed on the transmitted signal at the modulator and removed from
the received signal at the demodulator.
Synchronization of the PN sequence generated at the demodula-
tor with the PN sequence contained in the incoming received signal is
required in order to demodulate the received signal. Initially, prior to the
transmission of data, synchronization is achieved by transmitting a short
ﬁxed PN sequence to the receiver for purposes of establishing synchro-
nization. After time synchronization of the PN generators is established,
the transmission of data commences.
12.8.1 PROJECT 12.8: BINARY SPREAD-SPECTRUM
COMMUNICATIONS
The objective of this project is to demonstrate the eﬀectiveness of a PN
spread-spectrum signal in suppressing sinusoidal interference. Let us con-
sider the binary communication system described in Project 12.7, and let
us multiply the output of the modulator by a binary (±1) PN sequence.
The same binary PN sequence is used to multiply the input to the demod-
ulator and thus to remove the eﬀect of the PN sequence in the desired
signal. The channel corrupts the transmitted signal by the addition of a
FIGURE 12.18
Block diagram of binary PN spread-spectrum system for simula-
tion experiment
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Spread-Spectrum Communications
613
wideband noise sequence {w(n)} and a sinusoidal interference sequence
of the form i(n) = A sin ω0n, where 0 < ω0 < π. We may assume that
A ≥M, where M is the number of samples per bit from the modula-
tor. The basic binary spread spectrum-system is shown in Figure 12.18.
As can be observed, this is just the binary digital communication system
shown in Figure 12.16, to which we have added the sinusoidal interference
and the PN sequence generators. The PN sequence may be generated by
using a random number generator to generate a sequence of equally prob-
able ±1’s.
Execute the simulated system with and without the use of the PN
sequence, and measure the error rate under the condition that A ≥M
for diﬀerent values of M, such as M = 50, 100, 500, 1000. Explain the
eﬀect of the PN sequence on the sinusoidal interference signal. Thus ex-
plain why the PN spread-spectrum system outperforms the conventional
binary communication system in the presence of the sinusoidal jamming
signal.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

614
C H A P T E R 13
Random
Processes
The main focus in the ﬁrst ten chapters was on characterizing, process-
ing, and ﬁltering deterministic signals—that is, signals deﬁned using exact
mathematical expressions or through their Fourier spectra. In the real
world, there are signals that cannot be completely described as deter-
ministic or that have random ﬂuctuations (or variations) around known
waveforms. Speech signals, audio signals, video signals, and so on, fall
under the ﬁrst category, while noisy signals, received radar signals, and
communication signals fall under the second category. Such signals are
known as random processes or random signals and can be thought of as a
collection or ensemble of sample signals with associated probabilistic de-
scription. Therefore, we need a statistical approach to characterize them
and to process them.
In Chapters 14 and 15, we will consider the processing of random
signals. Although linear ﬁltering of a sample waveform from the ensemble
of the process is akin to convolution operation in the time domain or
ﬁltering in the frequency domain, the emphasis will be on estimating
parameters from observations, detecting signals in noisy environments
and designing optimal ﬁlters for satisfying a given optimality criteria.
In this chapter, we provide an overview of various analytical concepts
to deﬁne randomness of measurements or waveform variations and pro-
vide sound techniques to calculate the response of linear ﬁlters to random
signals. We begin with the relevant probability theory of random vari-
ables by deﬁning probability functions and statistical averages, and con-
tinue with pairs of random variables. We extend these concepts to random
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
615
signals, describe them in terms of second-order statistics, and then delve
into stationary and ergodic processes, correlations, and power spectra.
We also show how this theory can be applied to processing of random sig-
nals through an LTI system using both the time and frequency domains.
Finally, we discuss a few representative random processes, including Gaus-
sian, Markov, white noise, and ﬁltered noise processes.
13.1 RANDOM VARIABLE
When we make measurements of certain variables over a collection of
objects—for example, height in meters or weight in kilograms of a
population—we obtain numbers that ﬂuctuate over a range of values.
Such measurements are called random measurements, the act of measure-
ment is called a random experiment, and the variables are termed random
variables. Even if we consider repeating the same random experiment—
say, rolling a six-sided die—again and again, the outcomes vary and
cannot be predicted. In the context of signal processing, the value of a
noisy signal at each instant cannot be determined precisely and hence
must be considered as a random value.
Even though the numerical outcomes of these random experiments
vary widely every time we repeat them, there appears to be some pattern
of their relative likelihood of obtaining these values. Such patterns can be
understood and quantiﬁed using probability measure. For example, if we
toss a fair coin, we will not know if the outcome of the toss is a head or
a tail. But if we repeat this toss a large number of times, we will observe
that the total number of head/tail outcomes are approximately equal.
13.1.1 PROBABILITY FUNCTIONS
Consider a random variable under measurement such as a noisy voltage
source. We will use capital letters such as X to denote a random variable
and lowercase x to denote its measured value. Thus x is a value on a real
line R. We divide this line into small intervals of length ∆x and count the
number of voltage values, say, Nx, that fall into ∆x at each x. If N is the
total number of times we measure the random voltage X, then Nx/N is
the approximate probability of observing voltage value x and

Nx/N

/∆x
is approximately the probability density of the random variable X at x. We
will denote this probability density function (pdf) as fX(x) and formally
deﬁne it by
fX(x) =
lim
∆x →0
N →∞
 Nx
N∆x

(13.1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

616
Chapter 13
RANDOM PROCESSES
The probability that the random voltage X takes values over the range
x1 < X < x2 is given by integrating fX(x) dx over the interval, or
Pr

x1 < X < x2

=
 x2
x1
fX(x) dx
(13.2)
The pdf has to satisfy important properties for it to be a valid function.
It cannot be negative or imaginary, and every measurement must yield
some real value signifying
 ∞
−∞
fX(x) dx = 1
(13.3)
Another useful probability function is the cumulative distribution
function (CDF), which is the probability that the random variable X takes
a value less than or equal to some speciﬁed value x. It is denoted by FX(x)
and is deﬁned as
FX(x) =
 x
−∞
fX(u) du
(13.4)
This function also has to satisfy properties such as non-negativeness with
increasing x and, from (13.4), and (13.3),
FX(−∞) = 0,
FX(∞) = 1
(13.5)
Discrete Random Variable
So far, we described a continuous random
variable that assumes values in a continuous range over R. If the random
variable X takes a set of discrete values

xi

with probabilities pi, i =
1, 2, . . . such that 
i pi = 1, then the random variable X is called a discrete
random variable. In this case, we can obtain its pdf using impulse functions
and CDF using step functions:
fX(x) =
	
i
piδ(x −xi)
(13.6a)
FX(x) =
	
i
piu(x −xi)
(13.6b)
Finally, we note that, in the general case, a random variable can take a
continuous range of values as well as a discrete set of values. Such a ran-
dom variable is called a mixed random variable. The pdf description using
impulse functions is suﬃcient to describe any type of random variable.
Histogram as a pdf Approximation
Given N observations of a ran-
dom variable X, we have denoted Nx as the number of values of X that
fall into the small interval ∆x at x = xi. Then the plot of Nx vs. x is
called the histogram of the observed values and the plot of the ratio
PX(x) =
Nx
N∆x
(13.7)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
617
vs. x is called the normalized histogram and is a good approximation
of the pdf fX(x) of X from (13.1). MATLAB provides a function, Nx =
histc(x,edges), that provides counts in Nx, given observed values x and
bin edges. Using this function, we can design a MATLAB function, which
we will call pdf1, that computes the pdf as a normalized histogram from
the observed values. It is shown below and its use is given in Example 13.1.
function [Px,xc] = pdf1(x,xmin,xmax,M)
% pdf1: Normalized Histogram as 1-D Probability Density
%
Function (pdf)
%
[Px,xc] = pdf1(x,xmin,xmax,M)
%
Px: normalized histogram over the range [xmin, xmax]
%
xc: histogram bin centers
%
x: data (observation) values
%
xmin: minimun range value
%
xmax: maximum range value
%
M: number of bins
N = length(x);
% Observation count
edges = linspace(xmin,xmax,M);
% Histogram boundaries
Dx = (xmax-xmin)/(M-1);
% Delta_x
xc = [xmin,edges(1:M-1)+Dx/2,xmax];
% Bin centers
edges = [-inf,edges,inf];
% Augment boundaries
Nx = histc(x,edges); Nx = Nx(1:end-1); % Histogram
Px = Nx/(N*Dx);
% Normalized Histogram
end
□
EXAMPLE 13.1
In Section 13.1.3, we will discuss the pdf of a random variable that is sinu-
soidally distributed. In this random variable, the observed values are taken
from a sine waveform over one complete cycle. Thus, to demonstrate use of the
pdf1 function, consider the signal
x(t) = sin(2πt), 0 ≤t ≤1
We will take large samples of this waveform and consider them as the observed
values for computing and plotting the normalized histogram. The following
MATLAB script illustrates the approach.
>> N = 1000000; % Number of observations
>> n = 0:1:N; % Time index
>> x = sin(2*pi*n/N); % Observations
>> [Px,xc] = pdf1(x,-1,1,100); % Normalized Histogram
>> plot(xc(2:end-1),Px(2:end-1),’linewidth’,1);
>> axis([-1.1,1.1,0,3.5]);
>> xlabel(’Range of {\itx}’,’fontsize’,9);
>> ylabel(’Relative Histogram’,’fontsize’,9);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

618
Chapter 13
RANDOM PROCESSES
Range of x
–1
0
1
Relative Histogram
0
0.3183
1
2
3
Approximation of pdf by Normalized Histogram
FIGURE 13.1
Normalized histogram of sinusoidally distributed samples
>> set(gca,’xtick’,[-1,0,1],’ytick’,[0,1/pi,1,2,3,4]); grid;
>> title(’Approximation of pdf by Normalized Histogram’,...
’fontsize’,10,’fontweight’,’bold’);
The resulting histogram is shown in Figure 13.1, which agrees with the pdf of
the sinusoidal distribution shown in Figure 13.6, which will be derived later.□
13.1.2 STATISTICAL AVERAGES
As explained in the discussion leading up to (13.1), the pdf completely
describes the random variable X in a probabilistic sense. However, in
practice it is not always possible to obtain such a function when we
have measurements from an arbitrary random quantity. The pdf contains
much more information about the random variable than we actually need
in practice. A random variable can also be described by characteristic
numbers called moments. These moments are computed using statisti-
cal averaging and form a countable (or discrete) set, whereas the pdf is
uncountable (or continuous).
The analytic calculation of averages is facilitated by the pdf. Assume
that we want to calculate the statistical average of the random variable X
from a large number of measurements. We denote this average by E

X

,
which means “the expectation of X.” Again we divide the real line R into
ﬁne intervals of width dx. Then for a large number of measurements,
the quantity fX(x) dx corresponds to the probability of dx containing the
value x. Thus in the limit as the number of observations become very
large, the average of X is given by
E

X

=
 ∞
−∞
xfX(x) dx
(13.8)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
619
In a similar fashion, we can calculate the average of some function of the
random variable X, such as a power Xn or an exponential exp(X), from a
large number of measurements. Let g(x) be such a function of x. Then its
average is given by
E

g(X)

=
 ∞
−∞
g(x)fX(x) dx
(13.9)
The averages of positive powers, g(X) = E

Xn
, are particularly use-
ful. These averages are the moments of X referred to above. We will denote
these moments by ξX(n), given by
ξX(n) =
 ∞
−∞
xnfX(x) dx
(13.10)
Clearly, ξX(0) = 1, while the ﬁrst moment
ξX(1)
△= µX =
 ∞
−∞
xfX(x) dx
(13.11)
is the statistical average or mean of X. The second moment
ξX(2) =
 ∞
−∞
x2fX(x) dx
(13.12)
is the mean-squared value of X. If X represents a random voltage, then
µX represents the average (or dc) value and ξX(2) represents the average
total power consumed across unit impedance.
Another useful set of statistical averages is the set of central moments
of X, deﬁned by
MX(n)
△= E

(X −µX)n
=
 ∞
−∞
(x −µX)nfX(x) dx
(13.13)
Clearly, MX(0) = 1 and MX(1) = 0. The second central moment MX(2) is
the most important one and is called the variance of the random variable
distribution. It is denoted by σ2
X and related to the mean µX and the
mean-squared value ξX(2) by
σ2
X = MX(2) =
 ∞
−∞
(x −µX)2fX(x) dx = ξX(2) −µ2
X
(13.14)
The square root of the variance is called the standard deviation or the
root-mean-squared (rms) value and gives the average spread of random
measurements from their mean value. Again, if X represents a random
voltage, then σ2
X represents the average ac power consumed across unit
impedance.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

620
Chapter 13
RANDOM PROCESSES
13.1.3 RANDOM VARIABLE MODELS
Although a random variable is completely described by its pdf, it is not
always possible to know this function for every random measurement. In
Example 13.1, we showed how to approximate a pdf using normalized
histogram given a set of measurements. Another approach is to assume a
particular form (or shape) for the pdf based on how the random measure-
ments are created. These forms are called models, and there are several
such models that are used in practice. We will consider the following three
models used often in Chapters 14 and 15.
Uniform Distribution
In this model, a random variable X is uniformly distributed (or equally
likely) over a ﬁnite range of a ≤x ≤b and is denoted by U(a, b). Its pdf
is given by
fX(x) =
⎧
⎨
⎩
1
b −a,
a ≤x ≤b,
0,
otherwise,
∼U(a, b)
(13.15)
and its CDF is given by
FX(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0,
x < a
x −a
b −a ,
a ≤x ≤b
0,
x > b
(13.16)
These probability functions are shown in Figure 13.2. The mean of the
uniform distribution is given by
µX =
 b
a
x
b −a dx = a + b
2
(13.17)
while the mean-squared value is given by
ξX(2) =
 b
a
x2
b −a dx = b2 + ab + a2
3
(13.18)
Hence the variance is given by
σ2
X = ξX(2) −µ2
X = (b −a)2
12
(13.19)
The case of a = 0 and b = 1 is called the standard uniform distribution,
denoted by U(0, 1).
MATLAB Implementation
MATLAB provides the function x=rand
(N,1), which generates a column vector containing N standard uniform
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
621
(b) FX(x)
(a) fX(x)
x
x
1
1
a
b
a
b–a
x–a
b–a
b
FIGURE 13.2
Uniform distribution: (a) pdf, (b) CDF
random numbers each of which is uniform over the interval [0, 1]. These
numbers are independent of each other; that is, generation of each one of
them does not aﬀect the occurrence of any other (this concept is discussed
in Section 13.2.1). To obtain the uniformly distributed random numbers
according to (13.15), we make a simple change of variable X = (b−a)Y+a
where Y is U(0, 1):
>> x = (b-a)*rand(N,1)+a;
The uniform distribution is useful in generating random sinusoidal
waveforms with uniformly distributed phase angles.
Gaussian Distribution
This is a very popular distribution model in many applications. In this
model, the distribution has two parameters, µ and σ, and is denoted by
N(µ, σ2). Its pdf is given by
fX(x) =
1
σ
√
2π e−(x−µ)2/2σ2 ∼N(µ, σ2)
(13.20)
and the CDF is given by
FX(x) = 1
2

1 + erf
x −µ
√
2σ2

(13.21)
where
erf(x) =
2
√π
 x
0
e−λ2 dλ
(13.22)
is called the error function, which is extensively tabulated since the above
integral does not have a closed-form expression. It is available in MATLAB
as the erf(x) function. The Gaussian probability functions are shown in
Figure 13.3.
The mean of the Gaussian distribution is given by the parameter µ,
while its variance is given by the parameter σ2—that is,
µX = µ
and
σ2
X = σ2
(13.23)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

622
Chapter 13
RANDOM PROCESSES
fX(x)
FX(x)
(a)
x
x
1
1
2p
0.5
(b)
m
m
s
FIGURE 13.3
Gaussian distribution: (a) pdf, (b) CDF
Thus the Gaussian distribution is completely described by its ﬁrst two
moments. It should be noted that although X takes values over the entire
real line R, these values are mostly concentrated around its mean µ with
spread given by σ. The case of µ = 0 and σ2 = 1 is the normalized
distribution, or Normal distribution, denoted by N(0, 1), although the
terms “Gaussian” and “Normal” are used synonymously in the literature.
MATLAB Implementation
MATLAB provides the function x =
randn(N,1), which generates a column vector containing N independent
and Normally distributed random numbers with mean 0 and variance 1.
To obtain arbitrary Gaussian distributed random numbers with mean µ
and variance σ2, we make a simple change of variable X = σY + µ where
Y is Normal:
>> x = sigma*randn(N,1)+mu;
Transformation of a Random Variable
In signal processing, we perform operations on random signals. This means
that we transform random signal values, which are random variables, into
other ones using a function or transformation. In a simple operation,
one random variable with known pdf is mapped into another one, and
we wish to calculate the pdf of the transformed random variable. Let
X be the given random variable with pdf fX(x), and let Y be the new
random variable obtained through the function Y = g(X). We wish to
determine the pdf fY(y). If the transformation function g(·) is invertible,
then we have x = g−1(y)
△= h(y) and fY(y) dy = fX(x) dx. Then by direct
substitution, we have
fY(y) dy = fX(x) dx = fX

h(y)

h′(y) dy
(13.24)
where h′(y) = dh(y)/ dy is the derivative of the inverse function. Hence
fY(y) = fX

h(y)

h′(y)
(13.25)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
623
If g(x) is not invertible, then h(y) has multiple values and we sum the
right-hand side of (13.25) over all the multiple values. Another approach,
using uniformly distributed random number, is illustrated in the following
example.
□
EXAMPLE 13.2
Uniformly distributed random numbers in the range (0, 1) can be used to
generate other random numbers with the prescribed probability distribution
function. Consider a random variable Y that has a linear pdf given by
fY(y) =

1
2y,
0 ≤y ≤2
0,
otherwise
(13.26)
with CDF
FY(y) =
⎧
⎪
⎨
⎪
⎩
0,
y < 0
1
4y2,
0 ≤y ≤2
1,
y > 2
(13.27)
These functions are shown in Figure 13.4.
Since the range of FY(y) is the interval (0, 1), we begin by generating a
uniformly distributed random variable X in the range (0, 1) and set
FY(y) = 1
4y2 = x
⇒
y = 2√x
(13.28)
Thus we generate a random variable Y with the linear pdf shown in
Figure 13.4(a). This can be veriﬁed using the pdf1 function as shown below.
The plot of the resulting pdf is shown in Figure 13.5.
>> N = 1000000; x = rand(N,1); % Uniform distribution
>> y = 2*sqrt(x);
% Transformed random variable
>> [Py,yc] = pdf1(y,0,2,100);
% Normalized Histogram
>> plot(yc,Py,’linewidth’,1); axis([-0.1,2.1,0,1.1]);
>> % Plotting commands follow
□
Sinusoidal Distribution
One application of random variable transformation is in obtaining the
sinusoidal distribution, which is useful in many communication signals. It
was mentioned in Example 13.1. Let
X = A sin(Θ)
(13.29)
fY(y)
FY(y)
(a)
0
2
0
1
1
2
y
y
(b)
FIGURE 13.4
Linear distribution: (a) pdf, (b) CDF
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

624
Chapter 13
RANDOM PROCESSES
Range of y 
Relative Histogram
0
0
2
1
pdf of Linear Distribution
FIGURE 13.5
Normalized histogram of linearly distributed samples
where the random variable Θ is uniformly distributed between 0 and 2π
radians. Then X is said to be sinusoidally distributed with amplitude A.
The pdf of Θ is given by
fΘ(θ) =

1
2π,
0 ≤θ < 2π
0,
otherwise
(13.30)
Since g(Θ) = A sin(Θ) in (13.29), the inverse function is given by
Θ = h(X) = sin−1
 X
A

(13.31)
with
h′(x) =
1
√
A2 −x2
(13.32)
Also the h(X) function is not unique and has two values for θ for each
value of x in the range −A ≤x ≤A. Hence we must add two contributions
of the right-hand side in (13.25) to obtain
fX(x) =
⎧
⎨
⎩
1
π
√
A2 −x2 ,
−A ≤x ≤A
0,
otherwise
(13.33)
The corresponding CDF is given by
FX(x) =
 x
−A
dλ
π
√
A2 −λ2 = 1
π
π
2 + sin−1  x
A

(13.34)
These functions are shown in Figure 13.6.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
625
fX(x)
FX(x)
1/(pA)
(b)
(a)
0
x
x
0.5
1
–A
–A
A
0
A
FIGURE 13.6
Sinusoidal distribution: (a) pdf, (b) CDF
The mean of X, using the symmetry of its pdf, is given by
µX = ξX(1) =
 A
−A
x dx
π
√
A2 −x2 = 0
(13.35)
and the variance, which is also its mean-squared value, is given by
σ2
X = ξX(2) =
 A
−A
x2 dx
π
√
A2 −x2 = A2
2
(13.36)
MATLAB Implementation
It is easy to generate sinusoidally dis-
tributed random numbers by ﬁrst generating uniformly distributed num-
bers and then transforming them using (13.29). Assuming previously de-
ﬁned A and N, the following MATLAB fragment generates sinusoidally
distributed random numbers.
>> theta = 2*pi*rand(N,1);
>> x = A*sin(theta);
13.1.4 ESTIMATION OF THE MEAN OF RANDOM VARIABLE
Suppose we have N statistically independent observations x1, x2, . . ., xN
of a random variable X. We wish to estimate the mean value of X from
the N observations. The estimate of the mean value is denoted by ˆµX and
is computed as
ˆµX = 1
N
N
	
n=1
xk
(13.37)
Because ˆµX is a sum of random variables, it is also a random variable. We
note that the expected value of the estimate ˆµX is
E

ˆµX

= 1
N
N
	
n=1
E

xk

= 1
N

µX

N = µX
(13.38)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

626
Chapter 13
RANDOM PROCESSES
where µX is the actual mean of X. Thus the estimate ˆµX is said to be
unbiased.
The variance of the the estimate ˆµX is a measure of the spread or
dispersion of ˆµX relative to its mean value. The variance of ˆµX is denoted
by σ2
ˆµ and is given by
σ2
ˆµ = E

(ˆµX −µX)2
= E

ˆµ2
X

−2E

ˆµX

µX + µ2
X
= E

ˆµ2
X

−µ2
X
(13.39)
But the E

ˆµ2
X

is
E

ˆµ2
X

=
1
N 2
N
	
n=1
N
	
k=1
E

xnxk

= σ2
X
N + µ2
X
(13.40)
where σ2
X is the actual variance of X. Therefore, the variance of ˆµX is
σ2
ˆµ = σ2
X
N
(13.41)
Note that as N →∞, the variance of the estimate tends to zero. Thus
the estimate ˆµX is said to be consistent.
MATLAB Implementation
The built-in MATLAB function mean(X)
computes the average or mean value of the elements in the array X. If X is
a matrix, then mean(X) contains the mean value of each column of X. This
function implements the estimate given in (13.37). Similarly, the var(X)
computes the variance of the values in X.
□
EXAMPLE 13.3
Generate 10 samples of a random variable X that is uniformly distributed in the
interval [0, 2]. Compute the estimate ˆµX of the mean value of X, and compare
the result with the true mean value, µX = 1, of X. Repeat this experiment 100
times, and compute and plot the resulting estimates. Also compute the mean
value and the variance of the estimates, and compare the result with the true
mean value of X.
Solution
The following MATLAB script shows the generation of random variables and
estimation of mean as well as its display from 10 samples.
>> a = 0; b = 2; % Uniform random variable parameters
>> mu_X = (a+b)/2; % True mean value
>> N = 10; % Number of values
>> x = (b-a)*rand(N,1)+a; % Random data values
>> mu_hat = mean(x); % Mean estimate
>> disp([’True Mean value of X is: ’,num2str(mu_X,2)]);
True Mean value of X is: 1
>> disp([’
Estimated Mean is: ’,num2str(mu_hat,2)]);
Estimated Mean is: 0.94
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Variable
627
The above script indicates that the estimated mean is very close to the true
mean even for 10 samples. The following script repeats the above experiment
100 times and computes as well as plots the results.
>> M = 100; % Number of experiments
>> x = (b-a)*rand(N,M)+a; % M experiments, each N values
>> mu_hat = mean(x); % Mean estimate of each column
>> mean_muhat = mean(mu_hat); % Mean of the estimates
>> var_muhat = var(mu_hat); % Variance of the estimates
>> disp([’
Mean value of the estimate is: ’,...
num2str(mean_muhat,2)]);
Mean value of the estimate is: 1
>> disp([’Estimated variance of the estimate is: ’,...
num2str(var_muhat,2)]);
Variance of the estimate is: 0.034
>> disp([’
True variance of the estimate is: ’,...
num2str(var_mutrue,2)]);
True variance of the estimate is: 0.033
% Plotting commands follow
This shows that the mean of the estimates is equal to the true mean and that its
variance is very small and close to the true variance. The plot of the estimates
is shown in Figure 13.7.
□
Experiment Number
0
20
40
60
80
100
Value
0.5
1
1.5
Estimated Mean Values
FIGURE 13.7
The estimated means in Example 13.3 are denoted by asterisks.
The horizontal dashed line represents the plot of ˆµX, and the horizontal solid
line is the true mean of the uniform random variable.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

628
Chapter 13
RANDOM PROCESSES
13.2 A PAIR OF RANDOM VARIABLES
So far, we discussed one random variable as a model to describe a single
value of a random signal (say, at some ﬁxed time). This description was
in terms of a density function that gave probabilistic information as to
relative numbers of occurrence at diﬀerent amplitudes and in terms of
statistical averages such as means and variances. The latter description
also allowed us to relate average signal power quantities to moments.
However, we also want to know how fast or slow these random quan-
tities may be changing over time—that is, their frequency of change. This
is important for frequency-selective operations like ﬁltering. Since one
random variable cannot provide this time (or frequency) dependent infor-
mation, we have to consider two random values separated by some ﬁnite
time. This leads us to the case of a pair of random variables and the
statistical knowledge aﬀorded by them. It turns out that this additional
statistical information is suﬃcient for many practical applications.
13.2.1 JOINT PROBABILITY FUNCTIONS
The generalization of one random variable to two random variables leads
to the extension of the one-dimensional real line R to the two-dimensional
real plane R2. Let (X, Y) be a pair of random variables. Then it is obvious
to note that the pair (X, Y) takes values in a small area ∆x∆y around
(x, y) when X takes values in ∆x around x and Y takes values in ∆y
around y. Thus, analogous to one random variable, we deﬁne the joint
probability density function fXY(x, y) as the limiting value of relative num-
ber of occurrences Nxy/N in ∆x∆y when the number of observations N
approach inﬁnity, or
fXY(x, y)
△=
lim
∆x, ∆y →0
N →∞

Nxy
N∆x∆y

(13.42)
The joint cumulative distribution function is then the probability of the
semi-inﬁnite quarter-plane

X ≤x, Y ≤y

and is given by
FXY(x, y) =
 x
λ=−∞
 y
ν=−∞
fXY(λ, ν) dλ dν
(13.43)
or
fXY = ∂2FXY(x, y)
∂x∂y
(13.44)
The probability that X falls in the range

x1 ≤X ≤x2

and Y falls in
the range

y1 ≤Y ≤y2

is given by
Pr

x1 ≤X ≤x2, y1 ≤Y ≤y2

=
 x2
x1
 y2
y1
fXY(x, y) dx dy
(13.45)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
629
Once again, the above joint probability functions must satisfy axiomatic
properties such as non-negativity, nondecreasing monotonicity (for joint
CDF), and
FXY(−∞, −∞) = FXY(−∞, y) = FXY(x, −∞) = 0
(13.46a)
FXY(∞, ∞) =
 ∞
−∞
 ∞
∞
fXY(x, y) dx dy = 1
(13.46b)
Marginal Probability Functions
These functions deﬁne probabilities of one random variable without re-
gard to the other one and are the same as those for one random variable
but deﬁned using the joint functions. Thus we have two such marginal
distributions,
FX(x) = FXY(x, ∞) =
 x
λ=−∞
 ∞
−∞
fXY(λ, y) dy

dλ =
 x
−∞
fX(λ) dλ
(13.47a)
FY(y) = FXY(∞, y) =
 y
ν=−∞
 ∞
−∞
fXY(x, ν) dx

dν =
 y
−∞
fY(ν) dν
(13.47b)
and from (13.47) two marginal densities,
fX(x) =
 ∞
−∞
fXY(x, y) dy
(13.48a)
fY(y) =
 ∞
−∞
fXY(x, y) dx
(13.48b)
Conditional Probability Functions
As opposed to the marginal functions, these functions for one random
variable are deﬁned based on the observation or knowledge of the value
of the other random variable and are also very useful in practice. The
conditional pdf of X conditioned on the observation of Y = y is denoted
by fX|Y(x|y) and is deﬁned as
fX|Y(x|y)
△= fXY(x, y)
fY(y)
(13.49)
Similarly, we have conditional pdf of Y given X as
fY|X(y|x)
△= fXY(x, y)
fX(x)
(13.50)
From (13.49) and (13.50), we obtain
fXY(x, y) = fX|Y(x|y)fY(y) = fY|X(y|x)fX(x)
(13.51)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

630
Chapter 13
RANDOM PROCESSES
and from (13.51), we obtain
fX|Y(x|y) = fY|X(y|x)fX(x)
fY(y)
(13.52a)
fY|X(y|x) = fX|Y(x|y)fY(y)
fX(x)
(13.52b)
The results in (13.52) are known as Bayes rules and are useful in detection
and estimation theory.
Statistical Independence
Although the marginal probability functions for random variable X were
obtained without regard to Y, the occurrence of Y can still aﬀect the
occurrence of X. On the other hand, if the conditional pdf of X given
Y does not depend on Y and reduces to the marginal pdf of X or vice
versa, we will say that random variables X and Y are statistically (or
mutually) independent, or independent for short. Thus when X and Y are
independent, we have
fX|Y(x|y) = fX(x)
and
fY|X(y|x) = fY(y)
(13.53)
Hence from (13.49) or (13.50),
fXY(x, y) = fX(x)fY(y)
(13.54)
Similarly, we can also show a similar relationship for cumulative distribu-
tion functions when X and Y are independent, that is,
FXY(x, y) = FX(x)FY(y)
(13.55)
Histogram as a Joint pdf Approximation
Given observations of N
random variable pairs (XY), let Nxy denote the number of pairs that fall
into the small area ∆xy at (x, y). Then the three-dimensional plot of
Nxy vs. (x, y) is a two-variable histogram of the observed values, and the
three-dimensional plot of the ratio
PXY(x, y)
△=
Nxy
N∆xy
(13.56)
vs. (x, y) is the normalized histogram and is a good approximation to the
joint pdf fXY(x, y). MATLAB provides a function, Nxy = hist3([x,y],
nbins), that provides counts in Nxy given observed pairs [x,y] and an
nbins(1)×nbins(2) grid of bins. Using this function, one can design a
MATLAB function, which we will call pdf2, that computes the pdf as a
normalized histogram from the observed pairs. The design of this function
is left as an exercise.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
631
Scatter-plot as a pdf Plot
Another visual depiction of the joint pdf
is obtained by using a scatter-plot which is a two-dimensional picture.
It is obtained by plotting (x, y) pairs as dots in the plane. Dot density
indicates value of the pdf; higher density means higher value and vice
versa. The scatter-plot shape also gives the approximate support of the
pdf. Its use is illustrated in Example 13.5.
Transformations of Pairs of Random Variables
We have discussed the transformation of one random variable into another
and calculations of its pdf. In a similar fashion a somewhat more involved
signal processing operation would transform two random variables with
known joint pdf into either one or two new random variables. An applica-
tion of the ﬁrst operation would be the case of a signal in additive noise
and that of the second one would be converting cartesian coordinates into
polar coordinates. In each case we are interested in obtaining the density
functions of the new random variables.
One Function
Let fXY(x, y) be the joint pdf of a pair of random vari-
ables (X, Y) and let W = g(X, Y) be the new random variable. There are
several approaches to determine the pdf of W. One easy to understand
approach is to ﬁrst determine the CDF of W using the function g(·, ·) and
then diﬀerentiate it to obtain its pdf. Toward this, the CDF of W can be
expressed in terms of X and Y as
FW(w) = Pr

W ≤w

= Pr

g(X, Y) ≤w

=

g(x,y)≤w
fXY(x, y) dx dy
(13.57)
where the inequality g(X, Y) ≤w deﬁnes a region in the (x, y) plane over
which integration is performed. The pdf of W is now given by
fW(w) =
d
dw
⎛
⎜
⎝

g(x,y)≤w
fXY(x, y) dx dy
⎞
⎟
⎠
(13.58)
Although elegant in its simplicity, the integral, depending on the joint pdf
and the function g(·, ·), may or may not be analytically tractable.
One particularly important application of this approach is when the
transformation is just the sum of two random variables, for example the
signal plus noise case. Let W = X + Y. Then from (13.57)
FW(w) =

x+y≤w
fXY(x, y) dx dy =
 ∞
y=−∞
 w−y
x=−∞
fXY(x, y) dx dy
(13.59)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

632
Chapter 13
RANDOM PROCESSES
Diﬀerentiating (13.59) and using the Leibnitz’s rule1 from calculus, we
obtain
fW(w) =
 ∞
y=−∞
d
dw
 w−y
x=−∞
fXY(x, y) dx

dy
=
 ∞
y=−∞
fXY(w −y, y) dy
(13.60)
A further special case is when X and Y are independent in which case,
using (13.54), we obtain
fW(w) =
 ∞
y=−∞
fX(w −y)fY(y) dy = fX(w) ∗fY(w)
(13.61)
which is a convolution integral. This is a very signiﬁcant result.
□
EXAMPLE 13.4
Let X ∼U(0, 1) and Y ∼U(0, 1) be two uniformly distributed, independent
random variables and let W = X+Y. Then from (13.61) the pdf of W is given by
fW(w) =

u(w) −u(w −1)

∗

u(w) −u(w −1)

=

1 −|w −1|

u(w) −u(w −2)

(13.62)
which is a triangle between 0 and 2. To verify (13.62), we will generate a large
number of random numbers for X and Y, add them to obtain the corresponding
numbers for W, and then use the pdf1 function to obtain the pdf fW(w). The
following MATLAB script illustrates these steps.
>> N = 1000000;
>> x = rand(N,1); y = rand(N,1); >> w = x+y;
>> [Pw,wc] = pdf1(w,0,2,100);
>> % Plotting commands follow
Plot of the resulting pdf is shown in Figure 13.8.
□
Two Functions
This second transformation case is an extension of the
one random variable result (13.25) to two dimensions. Let two functions
of the two random variables X and Y be
W = g1(X, Y)
and
V = g2(X, Y)
(13.63)
1Leibnitz’s rule states that
∂
∂x
 b(x)
a(x)
f(x, y) dy = ∂b
∂x f

x, b(x)

−∂a
∂x f

a(x), x

+
 b(x)
a(x)
∂
∂xf(x, y) dy.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
633
Range of w
Relative Histogram
0
0
2
1
pdf of Sum of Two Independent Random Variables
FIGURE 13.8
Normalized histogram of sum of two uniformly distributed random
variables
and assuming that this 2 × 2 mapping is invertible, then we also have the
inverse functions
X = h1(W, V)
and
Y = h2(W, V)
(13.64)
Under the mappings (13.64) let the inﬁnitesimal area dw dv in the (w, v)
plane be mapped into area Ax,y in the (x, y) plane so that the following
probabilities are equal
fWV(w, v) dw dv = fXY(h1, h2)Ax,y
(13.65)
in which
Ax,y =
J

x y
w v
 dw dv =

det
⎡
⎢⎢⎢⎣
∂x
∂w
∂x
∂v
∂y
∂w
∂y
∂v
⎤
⎥⎥⎥⎦

dw dv
(13.66)
where J(·) is called the Jacobian of the transformation and represents the
slope plane in two dimensions. Substituting (13.66) in (13.65) we obtain
the desired result
fWV(w, v) = fXY

h1(W, V), h2(W, V)

×

det
⎡
⎢⎢⎢⎣
∂x
∂w
∂x
∂v
∂y
∂w
∂y
∂v
⎤
⎥⎥⎥⎦

(13.67)
As in the one variable case, if the given 2×2 transformation is not invert-
ible then the inverse mapping has multiple roots and contribution from
each root should be added to complete the joint pdf fWV(w, v).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

634
Chapter 13
RANDOM PROCESSES
□
EXAMPLE 13.5
Let random variables X and Y be independent and identically distributed with
uniform distribution U(0, 1). Deﬁne W = X + Y and V = X −Y. Then
X = W + V
2
△= h1(W, V)
and
Y = W −V
2
△= h2(W, V)
and
J
	x y
w v

 =

det
⎡
⎣
1
2
1
2
1
2
−1
2
⎤
⎦

= 1
2
Hence
fWV(w, v) = 1
2fXY
w + v
2
, w −v
2

= 1
2fX
w + v
2

fY
w −v
2

(13.68)
due to independence. Since X and Y are U(0, 1), the joint pdf fWV(w, v) from
(13.68) is also uniform with value of 1
2 over a diamond-shaped region bounded
by four straight lines: w + v = 0, w −v = −2, w + v = 2, and w −v = 0. This
can be veriﬁed using a scatter-plot as shown in the MATLAB script below.
>> x = rand(10000,1); y = rand(10000,1);
>> w = x+y; v = x-y;
>> plot([-0.2,2.2],[0,0],’k’,’linewidth’,0.75); hold on;
>> plot([0,0],[-1.1,1.1],’k’,’linewidth’,0.75);
>> plot(w,v,’.’,’markersize’,2); axis equal;
>> axis([-0.2,2.2,-1.1,1.2]);
>> set(gca,’xtick’,[0,1,2],’ytick’,[-1,0,1]); grid;
>> xlabel(’Range of {\itw}’,’fontsize’,9);
>> ylabel(’Range of {\itv}’,’fontsize’,9);
>> title(’Scatter-Plot: W = X+Y, V = X-Y’,...
>>
’fontsize’,10,’fontweight’,’bold’);
The resulting scatter-plot is shown in Figure 13.9.
□
13.2.2 JOINT STATISTICAL AVERAGES
These averages, or joint moments, are now doubly countable and require
two arguments. They are given by averaging the product of integer powers
of X and Y, or
ξXY(m, n)
△= E

XmYn
=
∞

−∞
xmynfXY(x, y) dx dy
(13.69)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
635
Range of w
Range of v
–1
0
0
1
2
1
Scatter-Plot: W = X+Y, V = X-Y
FIGURE 13.9
Scatter-plot of W = X + Y vs. V = X −Y for X ∼U(0, 1) and
Y ∼U(0, 1)
Clearly, ξXY(0, 0) = 1, while the means of X and Y are given by
ξXY(1, 0) = ξX(1) = µX =
∞

−∞
xfXY(x, y) dx dy =
 ∞
−∞
xfX(x) dx (13.70)
and
ξXY(0, 1) = ξY(1) = µY =
∞

−∞
yfXY(x, y) dx dy =
 ∞
−∞
yfY(y) dy (13.71)
as we have shown before. However, ξXY(1, 1) deﬁnes a new joint moment
denoted by
RXY
△= ξXY(1, 1) = E

XY

=
∞

−∞
xyfXY(x, y) dx dy
(13.72)
which is called the correlation between X and Y. It measures the average
linear relationship between the two random variables.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

636
Chapter 13
RANDOM PROCESSES
Similar to the above moments, we can deﬁne the joint central
moments by subtracting the mean values before averaging as
MXY(m, n)
△= E


X −µX
m
Y −µY
n
=
∞

−∞
(x −µX)m(y −µY)nfXY(x, y) dx dy
(13.73)
Again, MXY(0, 0) = 1 and MXY(1, 0) = MXY(0, 1) = 0. The variances of X
and Y are given by
MXY(2, 0) = MX(2) = σ2
X =
∞

−∞
(x −µX)2fXY(x, y) dx dy
=
 ∞
−∞
(x −µX)2fX(x) dx
(13.74)
and
MXY(0, 2) = MY(2) = σ2
Y =
∞

−∞
(y −µY)2fXY(x, y) dx dy
=
 ∞
−∞
(y −µY)2fY(y) dy
(13.75)
as we have shown before. However, MXY(1, 1) deﬁnes a new joint central
moment denoted by
CXY
△= MXY(1, 1) =
∞

−∞
(x −µX)(y −µY)fXY(x, y) dx dy
(13.76)
which is called the covariance between X and Y. It measures the average
linear relationship between the two random variables after their means
have been subtracted out. Note that from (13.76), it is straightforward to
show that
CXY = RXY −µXµY
(13.77)
Important Concepts
Through the joint moments of correlation and covariance, we now discuss
new important relationships between two random variables.
Uncorrelated Random Variables
If the covariance CXY between X
and Y is zero, then from (13.77) we have
RXY = µ2
Xµ2
Y
or
E

XY

= E

X

E

Y

(13.78)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
637
and we will say that random variables X and Y are uncorrelated with
each other. This is a weaker condition between X and Y compared to
their independence, (13.54), which is a stronger condition. This is because
independence always implies zero correlation between X and Y, while the
inverse is not always true. In Section 13.2.3, we will show that for Gaussian
random variables zero correlation also implies independence.
For uncorrelated random variables X and Y, it is easy to show that the
variance of the sum of two random variables is the sum of their variances,
that is,
σ2
X+Y = σ2
X + σ2
Y
(13.79)
Orthogonal
Random
Variables
Consider
the
expectation
E

(X + Y)2
. Since expectation is a linear operator, we have
E

(X + Y)2
= E

X2 + Y2 + 2XY

= E

X2
+ E

Y2
+ 2E

XY

= E

X2
+ E

Y2
+ 2RXY
(13.80)
If the correlation RXY = 0, then from (13.80) we have
E

(X + Y)2
= E

X2
+ E

Y2
(13.81)
which suggests that X and Y are perpendicular to each other if X and
Y are considered vectors in a Cartesian space. Thus we will say that if
correlation is zero, then X and Y are orthogonal to each other. We note
that this condition holds when X and Y are uncorrelated and either X or
Y or both have zero mean.
Correlation Coefficient
Finally, we deﬁne a normalized covariance
between random variables so that we can compare average linear depen-
dencies between pairs of random variables. This normalized covariance is
called the correlation coefficient and is deﬁned by
ρXY
△=
CXY
σXσY
(13.82)
Clearly, if ρXY = 0, then the covariance CXY = 0 and X and Y are uncor-
related. If ρXY = 1, we will say that X and Y are perfectly correlated and
agree with each other, on the average. However, if ρXY = −1, we will still
say that X and Y are perfectly correlated but disagree with each other, on
the average.
13.2.3 BIVARIATE GAUSSIAN DISTRIBUTION
One of the well-known and often-used models for two random variables
is the two-dimensional jointly Gaussian distribution. It is completely de-
scribed by the ﬁrst two joint moments of random variables X and Y and
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

638
Chapter 13
RANDOM PROCESSES
in its most general form is characterized by ﬁve parameters and its pdf
given by
N

µX, µY; σX, σY; ρXY

△= fXY(x, y) =
1
2πσXσY

1 −ρ2
XY
×
exp

−1
2(1 −ρ2
XY)
	x −µX
σX

2
−2ρXY
	x −µX
σX

 	y −µY
σY

+
	y −µY
σY

2
(13.83)
This is a unimodal bell-shaped surface and is shown in Figure 13.10
for various values of σX, σY, and ρXY and for µX = µY = 0. The contours
of constant-density values are also superimposed on the density surface
for clarity. We observe that in Figure 13.10a contours are circular; in
Figure 13.10b contours are elliptical with the major axis rotated 45◦with
the x-axis; in Figure 13.10c contours are also elliptic but the major axis
rotated more than 45◦; and in Figure 13.10d contours are elliptic but
the major axis rotated more than 45◦in the clockwise direction. The
density surfaces are symmetric with respect to the rotated axis. The non-
zero values of the means simply move the center of the surface to a new
location.
Using direct integration of fXY(x, y) in (13.83), we can show that the
marginal densities are given by
fX(x) =
 ∞
−∞
fXY(x, y) dy =
1
σX
√
2π exp
$
−1
2
x −µX
σX
2%
= N(µX, σ2
X)
(13.84a)
fY(x) =
 ∞
−∞
fXY(x, y) dx =
1
σY
√
2π exp
$
−1
2
y −µY
σY
2%
= N(µY, σ2
Y)
(13.84b)
which are marginal Gaussian random variables. Hence fXY(x, y) in (13.83)
is known as the jointly Gaussian or bivariate Gaussian distribution.
It should further be noted that if ρXY = 0, that is, if the covariance
is zero so that X and Y are uncorrelated, then from (13.83) we have
fXY(x, y) =
1
2πσXσY
exp
$
−1
2
x −µX
σX
2
+
y −µY
σY
2&%
=
1
σX
√
2π exp
$
−1
2
x −µX
σX
2%
×
1
σY
√
2π exp
$
−1
2
y −µY
σY
2%
= fX(x)fY(y)
(13.85)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
639
(a) s X = 1, sY = 1, rXY = 0
(b) s X = 1, s Y = 1, rXY = 0.5
(c) s X = 2, s Y = 3, rXY = 0.5
(d) s X = 2, s Y = 3, rXY = –0.5
pdf
pdf
pdf
pdf
FIGURE 13.10
Bivariate Gaussian distributions for various values of σX, σY,
and ρXY and for µX = µY = 0
which implies independence between X and Y. Hence, only in the case
of the Gaussian distribution, zero correlation between X and Y implies
independence.
Useful Properties
Based on our discussion so far on Gaussian ran-
dom variables, it should be obvious that they possess some unique and
important properties. We enumerate these properties, along with those
that were not discussed:
1. A Gaussian random variable is completely described by its ﬁrst two
moments, which means that higher-order moments can be obtained in
terms of its ﬁrst two moments.
2. The sum of two Gaussian random variable is also Gaussian.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

640
Chapter 13
RANDOM PROCESSES
3. In fact, any linear combination of a number of Gaussian random vari-
ables is also Gaussian.
4. The product of two Gaussian density functions also results in a
Gaussian function.
5. The convolution of two Gaussian functions also results in a Gaussian
function.
6. A countable sum of independent random variables, irrespective of their
distribution functions, results in a Gaussian distribution under some
very general conditions. This result is known as the Central Limit
theorem. Hence a Gaussian distribution is also a stable distribution
for ﬁnite means and variances.
7. If Gaussian random variables are mutually uncorrelated, then they are
also mutually independent.
Other random variable models do not possess all these properties.
□
EXAMPLE 13.6
Generate samples of bivariate Gaussian random variables, X1 and X2, having
speciﬁed mean values µ1 and µ2, variances σ2
1 and σ2
2, respectively, and a cor-
relation coeﬃcient ρ between the two.
Solution
First, we generate samples of two statistically independent, zero-mean, and
unit-variance Gaussian random variables by using the method described in Sec-
tion 13.1.3. Let us denote these random variable values by the vector notation
Y =
y1
y2

(13.86)
Next, we assemble the desired (2×2) covariance matrix
CX =
 σ2
1
ρσ1σ2
ρσ1σ2
σ2
2

(13.87)
and factor it as
CX = C
1
2
X

C
1
2
X
T
(13.88)
We now deﬁne the linearly transformed (2×1) vector X as
X
△=
X1
X2

= C
1
2
X Y + µX,
µX
△=
µ1
µ2

(13.89)
Then the covariance of X is
E

X −µX

X −µX
T
= E

C
1
2
X Y Y T
C
1
2
X
T
= C
1
2
X E

Y Y T
C
1
2
X
T = C
1
2
X I

C
1
2
X
T
= CX
(13.90)
as expected. The most diﬃcult step in this process is the factorization of the
covariance matrix CX, which can be implemented in MATLAB using the matrix
square-root function sqrtm. The following MATLAB script generates 1000 sam-
ples of the bivariate Gaussian distribution for µ1 = 2, µ2 = 1, σ1 = 1, σ2 = 2,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

A Pair of Random Variables
641
x1
–4
0
1
10
x2
–2
0
2
4
6
Scatter-Plot of Correlated Gaussian Random Numbers
FIGURE 13.11
Scatter-plot of Gaussian random numbers in Example 13.6
and ρ = 0.5. The scatter-plot of the resulting samples is shown in Figure 13.11,
in which the distribution center is shown as the “+” symbol.
mu1 = 1; mu2 = 2; % Mean parameters
sd1 = 2; sd2 = 1; % Standard deviations
var1 = sd1ˆ2; var2 = sd2ˆ2; % Variances
rho = 0.5; % Correlation coefficient
Cx = [var1, rho*sd1*sd2; rho*sd1*sd2, var2]; % Cov matrix
% Generate 1000 unit Gaussian random-2 vectors
N = 10000; M = 2; Y = randn(N,M);
% Generate correlated non-zero mean Gaussian random-2 vectors
X = sqrtm(Cx)*Y’+ [mu1;mu2]*ones(1,N); X = X’;
% Plotting commands follow
□
To generate multivariate Gaussian random numbers, we can extend
the approach of Example 13.6 to arbitrary-size (> 2) vectors. The fol-
lowing MATLAB function, X=randnMV(N,mu,C), generates N multivariate
Gaussian random vectors in X given the mean vector µ in mu and the
covariance matrix C in C.
function X = randnMV(N,mu,C)
% randnMV: multivariate Gaussian random vector generator
%
Generates N vectors in X given mean mu and covariance matrix C
%
mu should be a Mx1 column vector; C should be MxM
%
Generated X is NxM
% X = randnMV(N,mu,C)
%
mu = mu(:); M = length(mu);
Y = randn(N,M);
X = sqrtm(C)*Y’+ mu*ones(1,N); X = X’;
end
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

642
Chapter 13
RANDOM PROCESSES
13.3 RANDOM SIGNALS
A random signal or process can be thought of as a collection of waveforms
(or time-varying values) with some assigned probability. Similarly, we can
think of the value of a random signal at each time instant as a random
variable with an assigned pdf. Thus a random signal is a collection of
random variables in the temporal space as well as a collection of sam-
ple waveforms in the ensemble space. This understanding is crucial in
dealing with its probabilistic (or statistical) description as well as for its
processing through linear systems.
A random process can be continuous in time (random signals) or dis-
crete in time (random sequences). We will mostly discuss random signals,
but the derived results will also apply to random sequences with obvious
modiﬁcations that we will also state. In keeping with our random variable
terminology, we will denote a random signal by X(t) and its realization or
sample waveform by x(t). Thus we have a new temporal variable in our
random object.
If a time instance t is ﬁxed (i.e., when we assign a known value), then
X(t) is a random variable and hence has a pdf fX(x; t) or moment functions
such as a mean µX(t), mean-squared value, and variance σ2
X(t) given by
µX(t) = E

X(t)

=
 ∞
x=−∞
xfX(x; t) dx,
(13.91a)
E

X2(t)

=
 ∞
x=−∞
x2fX(x; t) dx,
(13.91b)
σ2
X(t) = E

(X(t) −µX(t))2
= E

X2(t)

−µ2
X(t)
(13.91c)
Then all the concepts that we discussed in Section 13.1 apply at each
time instance of X(t).
If we have two ﬁxed time instances—say, t1 and t2—then X(t1) and
X(t2) are a pair of random variables with joint pdf fX(x1, x2; t1, t2) or
joint moment functions such as correlation RXX(t1, t2) and covariance
CXX(t1, t2) given by2
RXX(t1, t2) = E

X(t1)X(t2)

=
∞

−∞
x1x2fX(x1, x2; t1, t2) dx1 dx2
(13.92a)
CXX(t1, t2) = RXX(t1, t2) −µX(t1)µX(t2)
(13.92b)
in addition to their marginal densities or moments. Since the pair of ran-
dom variables X(t1) and X(t2) are from the same process X(t), we will
2We will assume real-valued random signals. For complex-valued signals, the second
term in the expectation requires complex-conjugation.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Signals
643
henceforth call RXX(t1, t2) the autocorrelation function and CXX(t1, t2)
the autocovariance function. Now the concepts that we discussed in Sec-
tion 13.2 can be used for each such pair from X(t).
□
EXAMPLE 13.7
A random process is given by
X(t) = Ae−tu(t)
(13.93)
which is a causal exponential signal whose amplitude A ∼U(0, 1) is a uniformly
distributed random variable over (0, 1). The marginal density of X(t) for t ≥0,
using the transformation of random variable formula (13.25), is given by
fX(x; t) = fA(xet)et
(since h(x) = xet and h′(x) = et)
= et(u(x) −u(x −e−t)) ∼U(0, e−t)
(13.94)
which is a uniform distribution between 0 and e−t at each t ≥0. Note that as
t →∞, this pdf becomes a narrower pulse with increasing amplitude and in
the limit becomes fX(x; ∞) = δ(x), which means that X(∞) is a deterministic
number zero. The ﬁrst two moments of X(t), using (13.91) and the moments of
uniform distribution, are given by
µX(t) = E

Ae−tu(t)

= E

A

e−tu(t) = 1
2e−tu(t),
(13.95a)
E

X2(t)

= E

A2
e−2tu(t) = 1
3e−2tu(t),
(13.95b)
σX(t) = E

X2(t)

−µ2
X(t) =
1
12e−2tu(t)
(13.95c)
To determine the joint pdf fX(x1, x2; t1, t2), note that since the random variable
A is time independent, the random variables X(t1) and X(t2), for t1, t2 ≥0, are
linearly dependent, that is,
X(t2) = X(t1)e−(t2−t1)
(13.96)
This means that the joint pdf fX(x1, x2; t1, t2) is singular; that is, it contains an
impulse plane in the (x1-x2) domain. Now consider
fX(x1, x2; t1, t2) = fX(x2
x1; t1, t2)fX(x1; t1)
(13.97)
where from (13.94) fX(x1; t1) ∼U(0, e−t1) and from (13.96),
fX(x2
x1; t1, t2) = δ(x2 −x1e−(t2−t1))
(13.98)
which is the singular function that represents an impulse plane at an angle given
by the slope e−(t2−t1). Substituting (13.98) in (13.97), the joint pdf is given by
fX(x1, x2; t1, t2) = et1(u(x1) −u(x1 −e−t1))δ(x2 −x1e−(t2−t1)), t1, t2 ≥0
(13.99)
From (13.92) and (13.95), the autocorrelation and autocovariance functions are
given by
RXX(t1, t2) = E

Ae−t1Ae−t2
= E

A2
e−(t1+t2) = 1
3e−(t1+t2),
(13.100a)
CXX(t1, t2) = RXX(t1, t2) −µX(t1)µX(t2) =
1
12e−(t1+t2)
(13.100b)
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

644
Chapter 13
RANDOM PROCESSES
□
EXAMPLE 13.8
Consider a sinusoidal random process given by
X(t) = A cos(Ω0t + Θ)
(13.101)
where Ω0 is a ﬁxed frequency, the amplitude random variable A is linearly
distributed with pdf
fA(a) =

2a,
0 ≤a ≤1
0,
otherwise
(13.102)
and the phase random variable Θ ∼U(0, 2π). These two random variables are
statistically independent. The mean of X(t) is given by
µX(t) = E

X(t)

= E

A cos(Ω0t + Θ)

= E

A

E

cos(Ω0t + Θ)

= 0
(13.103)
since the average of cosine waveform over one full cycle is zero. The autocorre-
lation of X(t) is given by
RXX(t1, t2) = E

X(t1)X(t2)

= E

A cos(Ω0t1 + Θ)A cos(Ω0t2 + Θ)

= E

A2
E

cos(Ω0t1 + Θ) cos(Ω0t2 + Θ)

(13.104)
The ﬁrst expectation on the right of (13.104) is computed as
E

A2
=
 1
0
a2(2a) da = 1
2
(13.105)
while the second expectation is given by
E

cos(Ω0t1 + Θ) cos(Ω0t2 + Θ)

= 1
2

E

cos

Ω0(t1 −t2)

+ cos(Ω0(t1 + t2) + 2Θ)

= 1
2 cos(Ω0(t1 −t2))
(13.106)
where we have used the trigonometric identity on product of two cosine func-
tions. Substituting (13.105) and (13.106) in (13.104), we obtain
RXX(t1, t2) = 1
4 cos(Ω0(t1 −t2))
(13.107)
□
Cross-correlation Functions
If Y(t) is another random process de-
ﬁned in the same random experiment, then its linear interaction with
X(t) can be described using the joint pdf fXY(x, y; t1, t2) or the cross mo-
ments in addition to its own marginal pdf and auto moments. We deﬁne
the cross-correlation function RXY(t1, t2) and cross-covariance function
CXY(t1, t2) between X(t) and Y(t) as
RXY(t1, t2) = E

X(t1)Y(t2)

=
∞

−∞
xyfXY(x, y; t1, t2) dx dy
(13.108a)
CXY(t1, t2) = RXY(t1, t2) −µX(t1)µY(t2)
(13.108b)
These functions will be useful later in the calculations of linear system
input/output correlations and covariances.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Signals
645
13.3.1 STATIONARITY
For a general random signal, the statistical quantities such as densities or
moments vary or change with time, as in Example 13.7. However, for some
random signals it is possible to have these quantities be time invariant,
as in Example 13.8. In many applications, we model a random signal that
exhibits this time-invariant statistical behavior. It simpliﬁes storage and
processing complexities. We will discuss two types of this time-invariance
property of random signals, collectively known as stationarity.
Strict-Sense Stationarity
In this type of stationarity, all orders of joint density functions or,
alternatively, all orders of joint moments are time invariant, that is, the
ﬁrst-order densities are independent of time
fX(x; t) = fX(x)
(13.109)
and the joint densities are independent of time t but depend only on the
relative interval τ between the two time instances
fX(x1, x2; t1 = t + τ, t2 = t) = fX(x1, x2; τ = t1 −t2)
(13.110)
and so on, for all orders. This also means that
µX(t) = µX,
E

X2(t)

= ξX(2)
(13.111a)
RXX(t1, t2) = RXX(t1 −t2)
(13.111b)
CXX(t1, t2) = CXX(t1 −t2)
(13.111c)
and so on, for all higher-order moments. This is the strongest form of
stationarity since it requires time independence for all orders of pdf and
moments. Hence the name strict-sense stationarity, which is not always
easy to achieve.
Wide-Sense Stationarity
In many practical applications, we need or use only up to second-order
statistical quantities of a random signal. Therefore, why not require time-
invariance only for the ﬁrst two orders of moments? This leads to a weaker
but workable form of stationarity called wide-sense stationarity, or WSS.
We will say that a random signal X(t) is WSS if it satisﬁes the following
three conditions:
E

X2(t)

= ξX(2) < ∞
(ﬁnite average power),
(13.112a)
µX(t) = µX
(a constant),
(13.112b)
RXX(t + τ, t) = RXX(τ)
(function of τ only)
(13.112c)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

646
Chapter 13
RANDOM PROCESSES
The variable τ is called the lag time variable since the random variable
X(t) lags X(t + τ) in time by τ seconds. Clearly, the autocovariance func-
tion CXX(t, t + τ) also exhibits a similar time-invariance as in (13.112c),
CXX(t + τ, t) = CXX(τ)
(13.113)
Note that the random process in Example 13.7 is nonstationary since
both its ﬁrst- and second-order moments are functions of time, whereas
the random process in Example 13.8 is WSS since its mean is constant
and its autocorrelation function depends only on (t1 −t2), which, from
(13.107), can be written as
RXX(t + τ, t) = RXX(τ) = 1
4 cos(Ω0τ)
(13.114)
In the rest of the chapter, we will mostly deal with WSS random
signals. Therefore, if we state that a random signal has a constant mean
and an autocorrelation function that depends on a single time variable,
then the reader can infer that the signal is WSS.
Finally, if X(t) and Y(t) are jointly WSS, then their cross-correlation
and cross-covariance functions depend only on the lag variable τ; that is,
RXY(t + τ, t) = RXY(τ)
and
CXY(t + τ, t) = CXY(τ)
(13.115)
13.3.2 ERGODICITY
Up to this point we treated a random signal as random variables at each t
and computed averages using the ensemble space. Thus we assumed that
we have had access to a very large number of waveforms so that we could
obtain averages, at each t, as weighted (by pdf) sums over ensemble as
explained in (13.8). However, in reality we have only one temporal sample
waveform, x(t), available to us for measurement and analysis. Using our
time integrating meters such as voltmeters, ammeters, or wattmeters we
could determine averages over a suﬃciently long period of time. We then
wonder whether these time averages would give us the same ensemble
averages so that we can replace ensemble expectations by the temporal
integrals.
After a considerable thought, we conclude that, in general, we can
not state that the two methods that we have described above to compute
statistical averages will yield the same result even when the processes
are stationary. Therefore, we introduce a new concept called ergodicity
that would allow us to equate statistics computed over a long time us-
ing one sample waveform to those computed over ensemble of values at
each t. We will call such processes as ergodic processes and, by necessity,
these processes must be stationary. However, not all stationary processes
are ergodic. By ergodicity we basically imply that using any one sample
waveform we have observed all possible variabilities and undulations in
the random process that are inherent in the ensemble of waveforms.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Signals
647
To begin with, consider the computation of the mean value. Let X(t)
be WSS with mean µX and let x(t) be its sample waveform. We will denote
the temporal average of x(t) using angular brackets:
'
x(t)
(
△=
lim
T →∞
1
2T
 T
−T
x(t) dt
(13.116)
where the measurement interval is 2T. If this average is equal to µX, we
will say that X(t) is ergodic in the mean.3
Now consider the temporal averaging to compute the autocorrelation
function RXX(τ) given an arbitrary sample waveform x(t):
Rxx(τ)
△=
'
x(t + τ)x(t)
(
△=
lim
T →∞
1
2T
 T
−T
x(t + τ)x(t) dt
(13.117)
If Rxx(τ) = RXX(τ) for each τ, then we will say that X(t) is ergodic in
the autocorrelation. If τ = 0, then we have Rxx(0) as the average power
in watts from (13.116). Thus if Rxx(0) = RXX(0), then X(t) is ergodic in
the average power.
□
EXAMPLE 13.9
Consider the random process X(t) = A cos(Ω0t + Θ) in which amplitude A and
frequency Ω0 are constants while Θ ∼U(0, 2π) is the random phase. Then its
ﬁrst two moments are given by
µX(t) = E

A cos(Ω0t + Θ)

= AE

cos(Ω0t + Θ)

= 0 = µX
(13.118a)
RXX(t1, t2) = E

A cos(Ω0t1 + Θ)A cos(Ω0t2 + Θ)

= A2E

cos(Ω0t1 + Θ) cos(Ω0t2 + Θ)

= 1
2A2 cos(Ω0(t1 −t2))
(13.118b)
following the steps used in obtaining (13.106). Clearly, X(t) is WSS. Consider
the temporal averaging in (13.116),

x(t)

△=
lim
T →∞
1
2T
 T
−T
A cos(Ω0t + θ) dt = 0 = µX
(13.119)
by choosing T = 2π/Ω0. Thus X(t) is ergodic in the mean. Now consider the
temporal averaging in (13.117),
Rxx(τ) = lim
T →∞
1
2T
 T
−T
A cos(Ω0(t + τ) + θ)A cos(Ω0t + θ) dt
= A2 lim
T →∞
1
2T
 T
−T
1
2(cos(Ω0τ) + cos(Ω0(2t + τ) + θ)) dt
= 1
2A2 cos(Ω0τ) = RXX(τ)
(13.120)
3This equivalence requires certain conditions on the autocorrelation function RXX(τ)
depending on the convergence concept used. These issues are beyond the scope of
this book.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

648
Chapter 13
RANDOM PROCESSES
Thus X(t) is also ergodic in the autocorrelation at each τ. In particular, it is
ergodic in the average power.
□
□
EXAMPLE 13.10
Consider the random process X(t) = A cos(Ω0t + Θ) given in Example 13.8,
which is a WSS process. Its mean and autocorrelation were computed as µX =
0 and RXX(τ) =
1
4 cos(Ω0τ). This process is ergodic in the mean but not in
the autocorrelation since its temporal average, for any observed value a of the
random variable A, is given by
Rxx(τ) = lim
T →∞
1
2T
 T
−T
a cos(Ω0(t + τ) + θ)a cos(Ω0t + θ) dt
= a2 lim
T →∞
1
2T
 T
−T
1
2(cos(Ω0τ) + cos(Ω0(2t + τ) + θ)) dt
= 1
2a2 cos(Ω0τ) ̸= RXX(τ)
(13.121)
□
13.3.3 RANDOM SEQUENCES
The results and the concepts that we have discussed so far for random
signals also apply to random sequences that are discrete-time random
processes. We will denote random sequences by X(n) where n is a sample
index, which we assume is uniformly spaced in time. Then at each n, the
value X(n) is a random variable with pdf fX(x; n), while the ﬁrst two
moments are given by
µX(n) = E

X(n)

=
 ∞
−∞
xfX(x; n) dx,
(13.122a)
E

X2(n)

=
 ∞
−∞
x2fX(x; n) dx,
(13.122b)
σ2
X(n) = E

X2(n)

−µ2
X(n)
(13.122c)
If two time indices, m and n, are ﬁxed, then values X(m) and X(n) rep-
resent a pair of random variables with joint pdf fX(x1, x2; m, n). Then the
autocorrelation and autocovariance sequences, respectively, are given by
RXX(m, n) = E

X(m)X(n)

=
∞

−∞
x1x2 fX(x1, x2; m, n) dx1 dx2,
(13.123a)
CXX(m, n) = RXX(m, n) −µX(m)µX(n)
(13.123b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Random Signals
649
If Y(n) is another random sequence deﬁned along with X(m) with
joint pdf fXY(x, y; m, n), then their cross-correlation and cross-covariance
sequences, respectively, are given by
RXY(m, n) = E

X(m)Y(n)

=
∞

−∞
xy fXY(x, y; m, n) dx dy,
(13.124a)
CXY(m, n) = RXY(m, n) −µX(m)µY(n)
(13.124b)
A random sequence X(n) is WSS if
E

X2(n)

= ξX(2) < ∞,
(13.125a)
µX(n) = µX,
(13.125b)
RXX(n + ℓ, n) = RXX(ℓ)
or
CXX(n + ℓ, n) = CXX(ℓ)
(13.125c)
where ℓis called the lag index variable. Similarly, two mutually WSS
random sequences have cross-correlation and cross-covariance sequences
that are functions of ℓ.
Finally, for an ergodic random sequence X(n), we can compute its
statistical averages using the temporal arithmetic averages from a sample
waveform x(n) as
µX =
'
x(n)
(
△=
lim
N→∞
1
2N + 1
N
	
n=−N
x(n),
(13.126a)
RXX(ℓ) =
'
x(n + ℓ)x(n)
(
△=
lim
N→∞
1
2N + 1
N
	
n=−N
x(n + ℓ)x(n)
(13.126b)
MATLAB Implementation
The SP Toolbox provides the function
[Rxy,lags]=xcorr(x,y,maxlags,’option’)
that estimates cross-correlation values Rxy at lag indices lags between
two data vectors x and y. The correlation values are computed up to
the maxlag index using four option cases: ’none’ (default), ’biased’,
’unbiased’, and ’coeff’, which are described below. It also computes
the autocorrelation values using the invocation
[Rxy,lags]=xcorr(x,maxlags,’option’)
When both x and y are zero-mean arrays, obtained by subtracting
their respective means, then xcorr computes the covariance values.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

650
Chapter 13
RANDOM PROCESSES
By default, xcorr computes estimated correlations, with no normaliza-
tion, given by
ˆRXY(ℓ) =
⎧
⎪
⎨
⎪
⎩
N−ℓ
	
n=0
x(n + ℓ)y(n),
ℓ≥0
ˆRXY(−ℓ),
ℓ< 0
(13.127)
where N is the maxlag index. The elements of the output vector Rxy are
then given by
Rxy(m) = ˆRXY(m −N −1),
m = 1, 2, . . . , 2N + 1
(13.128)
The parameter ‘option’ has the following choices:
• ’biased’: Biased estimate of the cross-correlation function given by
ˆRXY,biased(ℓ) =
 1
N

ˆRXY(ℓ)
(13.129)
• ’unbiased’: Unbiased estimate of the cross-correlation function
given by
ˆRXY,unbiased(ℓ) =

1
N −|ℓ|

ˆRXY(ℓ)
(13.130)
• ’coeff’: Normalizes the sequence so the autocorrelations at zero lag
are identically equal to 1.
• ’none’: Uses the raw, unscaled cross-correlations (default).
□
EXAMPLE 13.11
In Section 13.6.3 and in Example 13.22, we will discuss how to generate a
random process with the autocorrelation sequence of ρ|ℓ|. Samples of such a
process are available in the MATLAB data ﬁle x.mat. Determine and plot the
autocorrelation sequence for this process over the maximum lag of 10.
Solution
The following MATLAB script loads the data ﬁle, uses the xcorr function to
compute autocorrelations, and then plots the random sequence and its autocor-
relation. The resulting plots are shown in Figure 13.12. From the autocorrelation
plot, we can conclude that the correlation coeﬃcient ρ is approximately equal
to 0.7.
>> load x; maxlag = 10;
>> [Rx,lags] = xcorr(x,maxlag,’coeff’);
>> % Plotting commands follow
□
13.4 POWER SPECTRAL DENSITY
In our previous discussions in Section 13.1.2, we related average dc and
ac powers in a stationary random signal to the ﬁrst and second moments
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Power Spectral Density
651
n
0
20
40
60
80
100
x(n)
–4
–2
0
2
4
6
Samples of a Random Sequence
Lags
–10
–5
0
5
10
Correlation Values
0
0.7
1
Autocorrelation Sequence
FIGURE
13.12
Plots
of
the
random
process
and
its
autocorrelation
in
Example 13.11
of a random variable at each t. We further would like to know how the
average ac power is distributed along the frequency scale so that we can
design appropriate frequency selective or optimal ﬁlters. As we explain in
this section, this power distribution can be gleaned from the analysis of
the pairs of random variables and, in particular, from the autocorrelation
function. The result is a new term called power spectral density, or PSD.
To develop this result, we will need the use of Fourier analysis. We
studied and used Fourier transforms of continuous- and discrete-time
signals in previous chapters where signals were mostly deterministic and
satisﬁed certain convergence conditions such as ﬁnite power or absolute
integrability. It is diﬃcult to establish that random signals can satisfy
these conditions. To alleviate this seemingly diﬃcult mathematical prob-
lem, we will consider a truncated version of the random signal for which
the Fourier transform certainly exists for each sample waveform. Then we
will take the ensemble average to remove randomness and a limiting op-
eration in time (toward inﬁnity) to remove truncation in order to obtain
the desired result.
Let XT (t) be a ﬁnite-duration random signal derived from the WSS
random process X(t)
XT (t)
△=

X(t),
−T ≤t ≤T
0,
otherwise
(13.131)
and let XT (jΩ) be its continuous-time Fourier transform (CTFT), which
exists.4 The relevant CTFT pairs are given by
XT (jΩ) =
 ∞
−∞
XT (t)e−jΩt dt =
 T
−T
X(t)e−jΩt dt,
(13.132a)
XT (t) = 1
2π
 ∞
−∞
XT (jΩ)ejΩt dΩ
(13.132b)
4Note the diﬀerence in font representation: sans-serif X for random variable and roman-
italic X for CTFT.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

652
Chapter 13
RANDOM PROCESSES
where Ωis the analog radian frequency in rad/sec. Note that XT (jΩ) is
a random variable. The energy in the truncated random signal XT (t) is
given by
EXT
△=
 T
−T
X2(t) dt =
 ∞
−∞
X2
T (t) dt
(13.133)
The time-averaged power across 1 ohm resistance is then given by
PXT = 1
2T
 ∞
−∞
X2
T (t) dt =
1
2T(2π)
 ∞
−∞
XT (jΩ)
2 dΩ
(13.134)
where for the second equality we have used Parseval’s theorem for CTFT.
Clearly, the term PXT is a random variable since it contains random quan-
tities. Hence we will take an ensemble average to obtain the average power
in the truncated random process as
PXT
△= E

PXT

=
1
2T(2π)
 ∞
−∞
E
XT (jΩ)
2
dΩ
(13.135)
Finally, we take the limit as T →∞to obtain the average power in the
original untruncated random process
PX
△=
lim
T →∞
)
1
2T(2π)
 ∞
−∞
E
XT (jΩ)
2
dΩ
*
= 1
2π
 ∞
−∞
lim
T →∞
E
XT (jΩ)
2
2T
dΩ
(13.136)
A close examination of the quantity inside the integral in (13.136) above
reveals that it is a power density term since the left-hand side is an average
power. We denote this term as
SXX(Ω)
△=
lim
T →∞
E
XT (jΩ)
2
2T
(13.137)
and refer to it as power spectral density (PSD) since it is a power density
as a function of frequency.
It remains to be shown that the PSD is related to the autocorrela-
tion function through the CTFT. Toward this, consider the numerator in
(13.137), which from (13.132a) can be written as
E
XT (jΩ)
2
= E
$ T
−T
 T
−T
X(t1)X(t2)e−jΩ(t1−t2) dt1 dt2
%
=
 T
−T
 T
−T
E[X(t1)X(t2)]e−jΩ(t1−t2) dt1 dt2
=
 T
−T
 T
−T
RXX(t1 −t2)e−jΩ(t1−t2) dt1 dt2
(13.138)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Power Spectral Density
653
where in the last equality we have used the fact that X(t) is a WSS process.
Since the integrand in (13.138) depends only on (t1 −t2), we make the
change of variables τ = t1 −t2 and λ = t1 + t2 and integrate along λ to
transform the double integral into a single integral
E
XT (jΩ)
2
=
 2T
−2T
RXX(τ)(2T −|τ|)e−jΩτ dτ
(13.139)
Finally, substituting (13.139) into (13.137) and taking the limit, we obtain
SXX(Ω) = lim
T →∞
 2T
−2T
RXX(τ)

1 −|τ|
2T

e−jΩτ dτ
=
 ∞
−∞
RXX(τ)e−jΩτ dτ
(13.140)
which shows that the PSD SXX(Ω) is a CTFT of the autocorrelation func-
tion RXX(τ). This important result is known as the Wiener–Khinchin
theorem. The autocorrelation function can be obtained from the PSD
using the inverse CTFT
RXX(τ) = 1
2π
 ∞
−∞
SXX(Ω)ejΩτ dΩ
(13.141)
It should be noted that the total average power in the WSS random
process X(t) is given by
PX = 1
2π
 ∞
−∞
SXX(Ω) dΩ
(13.142)
which from (13.141) is equal to RXX(0) = E

X2(t)

= µ2
X + σ2
X. This is
the sum of the dc and ac powers. Although it is tempting to conclude
that the dc power is equal to µ2
X and ac power is equal to σ2
X, this is not
always true unless the process is also ergodic. However, if the process has
a dc power, then the PSD function will have an impulse at the origin
Ω= 0 with area equal to the dc power. Likewise, if the PSD function has
impulses at nonzero frequencies, then the process is known as a harmonic
process.
Properties of the PSD and Autocorrelation Functions
The PSD SXX(Ω) function possesses several important properties:
1. SXX(Ω) is a real-valued function even for complex-valued random
processes.
2. SXX(Ω) is an even function for real-valued processes or conjugate even
for complex-valued processes.
3. SXX(Ω) is a non-negative function.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

654
Chapter 13
RANDOM PROCESSES
Through the CTFT relations, the autocorrelation function RXX(τ) also
possesses equivalent properties:
1. RXX(τ) is a conjugate symmetric function.
2. For real-valued random processes, RXX(τ) is real and even.
3. RXX(τ) is a non-negative deﬁnite function.
□
EXAMPLE 13.12
Let RXX(τ) = e−α|τ|, α > 0. Determine the PSD SXX(Ω).
Solution
Using (13.140), we have
SXX(Ω) =
 ∞
−∞
e−α|τ|e−jΩτ dτ
=
 0
−∞
eατe−jΩτ dτ +
 ∞
0
e−ατe−jΩτ dτ
=
1
α −jΩ+
1
α + jΩ=
2α
α2 + Ω2
(13.143)
The autocorrelation function RXX(τ) and the PSD SXX(Ω) are plotted in
Figure 13.13.
□
–10
0
10
–5
0
Rxx(t)
Sxx(Ω)
1
1
(a)
(b)
5
Ω
t
FIGURE 13.13
Plots of (a) autocorrelation and (b) PSD in Example 13.12
Cross-Spectral Density Function
Analogous to the PSD (which we will also call an auto-PSD) function, we
can also deﬁne the cross-spectral density (CSD) function as the Fourier
transform of the cross-correlation function. Let X(t) and Y(t) be jointly
WSS random signals with cross-correlation RXY(τ). Then the CSD func-
tion is given by
SXY(Ω)
△=
 ∞
−∞
RXY(τ)e−jΩτ dτ
(13.144)
This function does not have a connotation as a power density function
and, in general, is a complex-valued function. However, it is useful in
analysis.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Power Spectral Density
655
13.4.1 PSD OF RANDOM SEQUENCES
A similar discussion and analysis applies to random sequences. The PSD
of a WSS random sequence X[n] and its autocorrelation are related by the
discrete-time Fourier transform (DTFT) relations
SXX(ω) =
∞
	
ℓ=−∞
RXX(ℓ)e−jωℓ,
(13.145a)
RXX(ℓ) = 1
2π
 π
−π
SXX(ω)ejωℓdω
(13.145b)
where ω is the digital radian frequency in radians/sample. Note that in
this case, the PSD SXX(ω) is periodic in ω with a period of 2π. The total
average power in X[n] is then given by
PX = RXX(0) = 1
2π
 π
−π
SXX(ω) dω
(13.146)
Also note that the PSD and the autocorrelation functions of a random se-
quence follows similar properties given for random signals above. Finally,
for jointly WSS random sequences X(n) and Y(n) with cross-correlation
RXY(ℓ), we deﬁne the CSD function by
SXY(ω)
△=
∞
	
ℓ=−∞
RXY(ℓ)e−jωℓ
(13.147)
□
EXAMPLE 13.13
Let the autocorrelation of a random sequence X(n) be given by RXX(ℓ) =
 1
2
|ℓ|
for all ℓ. Determine its PSD SXX(ω).
Solution
Using (13.145a), we obtain
SXX(ω) =
∞

ℓ=−∞
	1
2

|ℓ|
e−jωℓ=
−1

ℓ=−∞
	1
2

−ℓ
e−jωℓ+
∞

ℓ=0
	1
2

ℓ
e−jωℓ
=
∞

ℓ=1
	1
2ejω

ℓ
+
∞

ℓ=0
	1
2e−jω

ℓ
=
1
2ejω
1 −1
2ejω +
1
1 −1
2e−jω
=
3
5 −4 cos(ω)
(13.148)
The plots of the autocorrelation RXX(ℓ) and PSD SXX(ω) are shown in
Figure 13.14.
□
□
EXAMPLE 13.14
Let the PSD of a random sequence X(n) be given by
SXX(ω) =
16
17 + 8 cos(ω),
−π < ω ≤π
(13.149)
Determine its autocorrelation sequence RXX(ℓ).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

656
Chapter 13
RANDOM PROCESSES
0
Rxx(t)
Sxx(w)
–10
0
1
2
10
1
3
w
p
-p
(a)
(b)
1
3
FIGURE 13.14
Plots of (a) autocorrelation and (b) PSD in Example 13.13
Solution
Using (13.145b), we obtain
RXX(ℓ) = 1
2π
 π
−π
16
17 + 8 cos(ω)ejωℓdω
(13.150)
which is not easy to evaluate. Therefore, we will use the inverse z-transform
approach by converting SXX(ω) in (13.149) into an equivalent z-domain function
˜SXX(z), called the complex PSD. This is done by using
cos(ω) = ejω + e−jω
2
= z + z−1
2

z=ejω
(13.151)
Substituting (13.151) in (13.149), we have
˜SXX(z) =
16
17 + 8(z + z−1)/2 =
16z
4z2 + 17z + 4
=
4z

z + 1
4

(z + 4),
ROC: 1
4 < |z| < 4
(13.152)
since the unit circle must be in the ROC. Now using partial fraction expansion
followed by the use of Table 4.1 (z-transform pairs), we can obtain RXX(ℓ). From
(13.152),
˜SXX(z) =
4z

z + 1
4

(z + 4) = 16
15
	
z
z + 1
4
−
z
z + 4

,
1
4 < |z| < 4
(13.153)
Hence
RXX(ℓ) = 16
15
	
−1
4

ℓ
u(ℓ) + 16
15(−4)ℓu[−ℓ−1]
= 16
15
	
−1
4

|ℓ|
, for all ℓ
(13.154)
The plots of the PSD sX(ω) and autocorrelation RXX(ℓ) are shown in
Figure 13.15.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Power Spectral Density
657
RXX(  )
SXX(w)
16
15
–5
0
5
(b)
(a)
0
0.64
1.76
w
p
–p
FIGURE 13.15
Plots of (a) PSD and (b) autocorrelation in Example 13.14
MATLAB Implementation
The SP Toolbox provides several func-
tions, such as pwelch and cpsd, to estimate PSD and CSD functions,
respectively, from data vectors. These functions use techniques from spec-
tral estimation theory, a topic that is beyond the scope of this book. Since
we have used the xcorr function to estimate autocorrelation lag values,
we will compute the FFT of a large number of suitably zero-padded au-
tocorrelations as a preferred implementation of the PSD estimate.
Since the autocorrelation values being transformed are still ﬁnite
in number (and not inﬁnite as the theory requires), this computation
amounts to windowing the original autocorrelation sequence by a rect-
angular window. This may result in some of the computed PSD values
becoming negative, which violates one of the PSD properties. Therefore,
we will use a window function whose DTFT is always non-negative. Such
windows are known as lag windows, and one such window is the Bartlett
(or triangular) window. It should be noted that there are other choices
available in literature for lag-windows. One side-eﬀect of using nonrectan-
gular window is some loss of resolution in the spectral estimates. However,
this loss can be mitigated by using a large number of autocorrelation lags.
The following PSD function incorporates the above discussed approach.
function [Sx,omega] = PSD(Rx,maxlag,Nfft)
%PSD Computation of PSD using Autocorrelation Lags
%
[Sx,omega] = PSD(Rx,lags,Nfft)
% Sx: Computed PSD values
% omega: digital frequency array in pi units from -1 to 1
% Rx: autocorrelations from -maxlag to +maxlag
% maxlag: maximum lag index (must be >= 10)
% Nfft: FFT size (must be >= 512)
Nfft2 = Nfft/2;
M = 2*maxlag+1; % Bartlett window length
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

658
Chapter 13
RANDOM PROCESSES
Rx = bartlett(M).*Rx(:); % Windowed autocorrelations
Rxzp = [zeros(Nfft2-maxlag,1);Rx;zeros(Nfft2-maxlag-1,1)];
Rxzp = ifftshift(Rxzp); %Zero-padding and circular shifting
Sx = fftshift(real(fft(Rxzp))); % PSD
Sx = [Sx;Sx(1)]; % Circular shifting
omega = linspace(-1,1,Nfft+1); % Frequencies in units of pi
end
□
EXAMPLE 13.15
In Example 13.11 we computed autocorrelation sequence RXX(ℓ), −10 ≤ℓ≤10,
of a random sequence from its observed values. Compute its PSD numerically.
Solution
We will demonstrate the use of the PSD function in the following MATLAB
script.
>> load x; maxlag = 10; %Load random sequence data
>> [Rx,lags] = xcorr(x,maxlag,’coeff’); % Compute ACRS
>> [Sx,omega] = PSD(Rx,maxlag,512); % Compute PSD
>> % Plotting commands follow
The resulting plots are shown in Figure 13.16.
□
n
0
20
40
60
80
100
0
1
x(n)
–4
–2
0
2
4
6
Sample Sequence
w/p
–1
SX(w)
0
4
Power Spectral Density
FIGURE 13.16
Plots of a sample sequence and its PSD in Example 13.15
13.5 STATIONARY RANDOM PROCESSES THROUGH LTI SYSTEMS
One of the important aspects of signal processing is the ﬁltering of signals
through linear systems. In previous chapters, we studied how to ﬁlter
deterministic signals through linear and time-invariant (LTI) systems to
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Stationary Random Processes through LTI Systems
659
LTI System
X(t)
Y(t)
h(t)
H (jΩ)
FIGURE 13.17
Filtering of random process
obtain the output response in both the time domain and the frequency
domain. Now we want to consider ﬁltering of stationary random processes.
What do we mean exactly by this ﬁltering?
To obtain this meaning, we appeal to the ensemble description of the
random process, which is that it is a collection of sample waveforms. When
each input sample waveform is processed through the system, it produces
a ﬁltered sample function. Thus at the output, we also have an ensemble
of sample waveforms or an output random process. The individual sample
waveform ﬁltering aspect is well understood and can be implemented with
ﬁlter structures from Chapter 6. What we really are interested in is how
do we characterize the output random process in terms of its second-
order statistical averages. This is the meaning that we seek. A complete
description of the output process in terms of its joint densities is diﬃcult
to obtain even for simple systems.
Let the input random signal be a WSS process X(t) with a known
mean µX and an autocorrelation function RXX(τ). It is applied as an
input to a LTI system with impulse response h(t) or frequency response
function H(jΩ). Let the output process be Y(t). This system set up is
shown in Figure 13.17. The output process, symbolically, is given by the
convolution integral
Y(t) = X(t) ∗h(t) =
 ∞
λ=−∞
X(λ)h(t −λ) dλ
(13.155)
while the frequency response function H(jΩ) is given by
H(jΩ) =
 ∞
−∞
h(t)e−jΩt dt
(13.156)
We want to determine the mean and autocorrelation function of the out-
put process Y(t) as well as the cross-correlation between the output and
input processes and also want to examine if Y(t) is a stationary process.
Computation of Output Mean Function
The mean of Y(t) from
(13.155) is given by
µY(t) = E

Y(t)

=
 ∞
λ=−∞
E

X(λ)

h(t −λ) dλ
=
 ∞
λ=−∞
µXh(t −λ) dλ =
 ∞
λ=−∞
h(λ) dλ

µX
(13.157)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

660
Chapter 13
RANDOM PROCESSES
Note that the integral in the parenthesis is a constant, which from (13.156)
is equal to H(j0), or the dc-gain value. Hence the output mean is given by
µY(t) =
 ∞
λ=−∞
h(λ) dλ

µX = H(j0)µX = µY
(13.158)
which also is a constant. Thus the output mean is the input mean scaled
by the dc-gain of the LTI system, which makes sense.
Computation of Cross-Correlation and CSD Functions
Con-
sider the cross-correlation between the output Y(t + τ) and X(t). Using
(13.155), we obtain
RYX(t + τ, t) = E

Y(t + τ)X(t)

= E
 ∞
λ=−∞
h(λ)X(t + τ −λ)X(t) dλ

=
 ∞
t1=−∞
h(λ)E

X(t + τ −λ)X(t)

dλ
=
 ∞
t1=−∞
h(λ)RXX(τ −λ) dλ
(13.159)
where in the last equality we have used the stationarity of X(t). The
integral in (13.159) can be identiﬁed as a convolution between h(τ) and
RXX(τ), that is,
RYX(t + τ, t) = h(τ) ∗RXX(τ) = RYX(τ)
(13.160)
Thus the cross-correlation function is time invariant. After taking Fourier
transform of (13.160), the CSD function between the output and the input
is given by
SYX(Ω) = H(jΩ)SXX(Ω)
(13.161)
In a similar fashion, we can compute the cross-correlation and CSD
between the input and the output. The results are
RXY(τ) = h(−τ) ∗RXX(τ)
(13.162a)
SXY(Ω) = H∗(jΩ)SXX(Ω)
(13.162b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Stationary Random Processes through LTI Systems
661
Computation of the Output Autocorrelation and PSD Functions
Now consider the autocorrelation function of the output Y(t). Again using
(13.155), we obtain
RYY(t + τ, t) = E

Y(t + τ)Y(t)

= E
 ∞
λ=−∞
Y(t + τ)h(λ)X(t −λ) dλ

=
 ∞
λ=−∞
h(λ)E[Y(t + τ)X(t −λ)] dλ
=
 ∞
λ=−∞
h(λ)RYX(τ + λ) dλ =
 ∞
λ=−∞
h(−λ)RYX(τ −λ) dλ
(13.163)
where we have used the fact that the cross-correlation RYX(τ) is time in-
variant. The integral in (13.163) can be identiﬁed as a convolution between
h(−τ) and RYX(τ), that is,
RYY(t + τ, t) = h(−τ) ∗RYX(τ) = RYY(τ)
(13.164)
Clearly, the autocorrelation RYY(τ) is also time invariant. Substituting
RYX(τ) from (13.160) in (13.164), we obtain
RYY(τ) = h(τ) ∗h(−τ) ∗RXX(τ)
(13.165)
After taking Fourier transform of (13.165), the output PSD is given by
SYY(Ω) =
H(jΩ)
2SXX(Ω)
(13.166)
which is also a real and non-negative function, thus emphasizing that it
is a valid PSD function.
The ﬁrst convolution term in (13.165) is unique to the LTI system
and can be pre-computed. It is referred to as the system autocorrelation
function and is denoted by
Rh(τ)
△= h(τ) ∗h(−τ)
(13.167)
which implies
RYY(τ) = Rh(τ) ∗RXX(τ)
(13.168)
The Fourier transform of Rh(τ) is referred to as the system power-spectral
function and is denoted by
SH(Ω) =
H(jΩ)
2
(13.169)
which implies
SYY(Ω) = SH(Ω)SXX(Ω)
(13.170)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

662
Chapter 13
RANDOM PROCESSES
Starting with the cross-correlation function RXY(τ), we can obtain
the same result given in (13.165). The corresponding expressions are
RYY(τ) = h(τ) ∗RXY(τ) = h(τ) ∗h(−τ) ∗RXX(τ)
(13.171a)
SYY(Ω) = H(jΩ)SXY(Ω) =
H(jΩ)
2SXX(Ω)
(13.171b)
In conclusion, since the mean µY is a constant and the autocorrelation
RYY(τ) is time invariant, the output process Y(t) is also a WSS process.
Thus the LTI filtering of a WSS process will always result in a WSS
process.
□
EXAMPLE 13.16
A WSS random process X(t) with mean µX = 2 and autocovariance CXX(τ) =
4δ(τ) is applied as an input to a stable LTI system with impulse response
h(t) = e−tu(t)
(13.172)
The output of this system is the random process Y(t). Determine the following
quantities.
1. The mean µY.
Solution
Using (13.158), the mean of Y(t) is given by
µY = µX
 ∞
−∞
h(t) dt = 2
 ∞
0
e−t dt = 2
(13.173)
2. The cross-correlation RXY(τ) and the cross-covariance CXY(τ).
Solution
The autocorrelation of X(t) is given by
RXX(τ) = µ2
X + CXX(τ) = 4 + 4δ(τ)
(13.174)
Now using (13.162a), we have
RXY(τ) = h(−τ) ∗RXX(τ) = eτu(−τ) ∗[4 + 4δ(τ)]
= 4
 0
−∞
eτ dτ + 4eτu(−τ) = 4 + 4eτu(−τ)
(13.175)
Since RXY(τ) = µXµY + CXY(τ), the cross-covariance, CXY(τ), from (13.175) is
given by
CXY(τ) = 4eτu(−τ)
(13.176)
3. The autocorrelation RXX(τ) and the autocovariance CXX(τ).
Solution
From (13.167), the system autocorrelation function is given by
Rh(τ) = h(τ) ∗h(−τ) = [e−τu(τ)] ∗[eτu(−τ)]
=
 τ
−∞eλe−(τ−λ) dλ,
τ < 0,
 0
−∞eλe−(τ−λ) dλ,
τ ≥0, =

1
2eτ,
τ < 0,
1
2e−τ,
τ ≥0, = 1
2e−|τ| (13.177)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Stationary Random Processes through LTI Systems
663
Now from (13.168), (13.174), and (13.177), the output autocorrelation is
given by
RYY(τ) = Rh(τ) ∗RXX(τ) = 1
2e−|τ| ∗[4 + 4δ(τ)]
= 2
 ∞
−∞
e|τ| dτ + 2e−|τ| = 4 + 2e−|τ|
(13.178)
Since µY = 2, the autocovariance CYY(τ) is given by
CYY(τ) = RYY(τ) −µ2
Y = 2e−|τ|
(13.179)
From these calculations, note that cross quantities undergo operations identical
to those for the auto quantities.
□
□
EXAMPLE 13.17
Repeat Example 13.16 using the frequency domain approach by computing the
PSD and CSD quantities.
Solution
The frequency response function is given by
H(jΩ) = F[e−tu(t)] =
1
1 + jΩ
(13.180)
From (13.158), the mean µY is
µY = µXH(j0) = 2(1) = 2
(13.181)
From (13.174), the input PSD is
SXX(Ω) = F[4 + 4δ(τ)] = 8πδ(Ω) + 4
(13.182)
Now from (13.162b), the CSD between X(t) and Y(t) is given by
SXY(Ω) = H∗(jΩ)SXX(Ω) = 8πδ(Ω) + 4
1 −jΩ
= 8πδ(Ω) +
4
1 −jΩ
(13.183)
Hence, after inverse Fourier transformation, we obtain
RXY(τ) = 4 + 4eτu(−τ)
(13.184)
as before in (13.175). From (13.169) and (13.170), the PSD SYY(Ω) is given by
SYY(Ω) =
H(jΩ)
2SXX(Ω) = 8πδ(Ω) + 4
1 + Ω2
= 8πδ(Ω) +
4
1 + Ω2
(13.185)
In Example 13.12 and in (13.143), we obtained the Fourier transform pair
e−a|τ|, a > 0
F
←→
2a
a2 + Ω2
(13.186)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

664
Chapter 13
RANDOM PROCESSES
Using a = 1 in (13.186), we obtain the inverse Fourier transform of the second
term in (13.186). Hence
RYY(τ) = 4 + 2e−|τ|
(13.187)
as before in (13.178).
□
13.5.1 DISCRETE-TIME LTI SYSTEMS
Similar results apply to the discrete-time LTI systems driven by WSS
random sequences. Let h(n) be the impulse response of the system, and
let H(ejω) be the frequency response function, that is,
H(ejω) =
∞
	
n=−∞
h(n)e−jωn
(13.188)
Let X(n) be the input WSS process with mean µX, autocorrelation se-
quence RXX(ℓ), and auto PSD SXX(ω). Let Y(n) be the resulting output
process. Then Y(n) is also WSS with the following statistical quantities.
Output Mean
µY =
+
∞
	
n=−∞
h(n)
,
µX = H(ej0)
- ./ 0
dc-gain
µX
(13.189)
Cross-correlations between input and output
RYX(ℓ) = h(ℓ) ∗RXX(ℓ),
(13.190a)
RXY(ℓ) = h(−ℓ) ∗RXX(ℓ)
(13.190b)
CSD functions between input and output
SYX(ω) = H(ejω)SXX(ω)
(13.191a)
SXY(ω) = H(e−jω)SXX(ω)
(13.191b)
Autocorrelations between input and output
RYY(ℓ) = h(−ℓ) ∗RY,X(ℓ) = h(ℓ) ∗RXY(ℓ)
(13.192a)
= h(ℓ) ∗h(−ℓ)
-
./
0
△= Rh(ℓ)
∗RXX(ℓ) = Rh(ℓ) ∗RXX(ℓ)
(13.192b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Stationary Random Processes through LTI Systems
665
Auto PSD functions between input and output
SYY(ω) = H(e−jω)SYX(ω) = H(ejω)SXY(ω)
(13.193a)
=
H(ejω)
2
-
./
0
△= SH(ω)
SXX(ω) = SH(ω)SXX(ω)
(13.193b)
Average power in the output
E{X2(n)} = RXX[0] = 1
2π
 π
−π
SXX(ω) dω
(13.194)
□
EXAMPLE 13.18
Let X(n) be a WSS random sequence with mean and autocovariance sequence
given by
µX = 1
and
CXX(ℓ) = {1, 2, 3
↑, 2, 1}
(13.195)
It is applied as an input to a stable LTI system given by the impulse response
h(n) = {1
↑, 1, 1, 1}
(13.196)
Determine the following quantities.
1. The mean µY.
Solution
From (13.189), we obtain
µY =
 
∞

ℓ=−∞
h(ℓ)
!
µX = (1 + 1 + 1 + 1)(1) = 4
(13.197)
2. The cross-covariance CYX(ℓ) and cross-correlation RY,X(ℓ).
Solution
The cross-covariance sequences follow operations similar to those in (13.190).
Thus CY,X(ℓ) is given by
CYX(ℓ) = h(ℓ) ∗CXX(ℓ) = {1
↑, 1, 1, 1} ∗{1, 2, 3
↑, 2, 1}
= {1, 3, 6
↑, 8, 8, 6, 3, 1}
(13.198)
which is computed using MATLAB as shown in the following script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

666
Chapter 13
RANDOM PROCESSES
>> h = [1,1,1,1]; nh = [0:3];
>> Cx = [1,2,3,2,1]; lCx = [-2:2];
>> [Cyx,lCyx] = conv_m(Cx,lCx,h,nh)
Cyx =
1
3
6
8
8
6
3
1
lCyx =
-2
-1
0
1
2
3
4
5
The cross-correlation RYX(ℓ) is then given by
RYX(ℓ) = CYX(ℓ) + µYµX = {1, 3, 6
↑, 8, 8, 6, 3, 1} + 4
= {. . . , 4, 4, 5, 7, 10
↑, 12, 12, 10, 7, 5, 4, 4 . . .}
(13.199)
3. The autocovariance CYY(ℓ) and autocorrelation RYY(ℓ).
Solution
The autocovariance sequences also follow operations similar to those in (13.192).
Thus CYY(ℓ) is given by
CYY(ℓ) = h(−ℓ) ∗CYX(ℓ) = {1, 1, 1, 1
↑} ∗{1, 3, 6
↑, 8, 8, 6, 3, 1}
= {1, 4, 10, 18, 25, 28
↑, 25, 18, 10, 4, 1}
(13.200)
which is also computed using MATLAB as shown below.
>> [Cy,lCy] = conv_m(Cyx,lCyx,h,-fliplr(nh))
Cy =
1
4
10
18
25
28
25
18
10
4
1
lCy =
-5
-4
-3
-2
-1
0
1
2
3
4
5
Finally, the autocorrelation RYY(ℓ) is given by
RYY(ℓ) = CYY(ℓ) + µ2
Y = {1, 4, 10, 18, 25, 28
↑, 25, 18, 10, 4, 1} + 16
= {. . . , 16, 16, 17, 20, 26, 34, 41, 44
↑, 41, 34, 26, 20, 17, 16, 16 . . .} (13.201)
□
□
EXAMPLE 13.19
A zero-mean stationary random process with PSD SXX(ω) = 1 is passed through
a linear ﬁlter with impulse response
h(n) =

(0.95)n,
n ≥0
0,
n < 0
(13.202)
Determine the PSD SYY(ω) and the autocorrelation RYY(ℓ) of the output
process Y(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Stationary Random Processes through LTI Systems
667
Solution
It is easily seen that
H(ejω) =
∞

n=0
h(n)e−jωn =
∞

n=0
(0.95e−jω)n
=
1
1 −0.95e−jω
(13.203)
from which we obtain
H(ejω)
2 =
1
1 −0.95e−jω2 =
1
1.9025 −1.9 cos(ω)
(13.204)
Therefore, the PSD of the output is
SYY(ω) =
H(ejω)
2SXX(ω) =
1
1.9025 −1.9 cos(ω)
(13.205)
Note that SYY(ω) is periodic with period 2π. We can determine the autocorre-
lation RYY(ℓ) using the inverse z-transform approach of Example 13.14. How-
ever, in this example, we will use MATLAB to numerically compute samples
of RYY(ℓ). This computation is similar to the one used in the PSD function to
compute SXX(ω) from samples of RXX(ℓ) except for the windowing. Details are
given in the following MATLAB script.
>> N = 1024; omega = linspace(-1,1,N+1)*pi; % Frequency samples
>> Sy = 1./(1.9025-1.9*cos(omega));
>> % Autocorrelation sequence
>> Sy = fftshift(Sy(1:end-1)); % Circular shift; origin at the beginning
>> Ry = real(ifft(Sy));
% Autocorrelation samples
>> Ry = ifftshift(Ry);
% Circular shift; origin at the center
>> Ry = [Ry,Ry(1)];
% Sample symmetry for plotting
>> Rymax = max(Ry);
% Ry[0] value
>> fprintf(’ Ry[0] = %7.4f\n’,Rymax);
Ry[0] = 10.2564
>> Ry = Ry/Rymax;
% Normalized autocorrelation
>> fprintf(’ rho = %4.2f\n’,Ry(N/2));
rho = 0.95
From the resulting printout, note that the maximum autocorrelation is RYY(0) =
10.2564 while the normalized autocorrelation at lag ℓ= ±1 is equal to 0.95.
Since 10.2564 = 1/(1 −0.952), we infer that the autocorrelation sequence is
RYY(ℓ) =
ρ|ℓ|
1 −ρ2 ,
ρ = 0.95
(13.206)
Figure 13.18 shows plots of the PSD SYY(ω) and the ﬁrst ±10 lag values of the
autocorrelation RYY(ℓ), which conﬁrms our inference in (13.206).
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

668
Chapter 13
RANDOM PROCESSES
w/p
–1
0
1
SY(w)
0
10
400
Power Spectral Density
Lag
–10
RY(lag)
0.5
0
0.95
1
Normalized Autocorrelation
FIGURE 13.18
Plots of the PSD and ﬁrst 10 lag samples of the normalized
RYY(ℓ) in Example 13.19
13.6 USEFUL RANDOM PROCESSES
After studying random processes through their statistical and spectral
descriptions as well as analyzing their input-output behavior through LTI
systems, we now consider a few representative and useful random process
models that will be needed in Chapters 14 and 15. One important model
is that of the Gaussian random process which needs only second-order
statistical averages to describe it completely. Another important model
is that of the idealized noise signal called the white noise process. The
term noise is generally used to describe unwanted signals that tend to
disturb and distort the transmission and processing of signals—for exam-
ple, in communication systems—and over which we have limited control.
Using white noise process and appropriately designed LTI systems we
can generate other types of random processes such as Markov processes,
lowpass and bandpass processes, and so on. We will also consider a few
discrete-time processes.
13.6.1 GAUSSIAN PROCESS
Gaussian processes play an important role in many applications, in par-
ticular, in communication systems. We have discussed useful properties of
the Gaussian distribution in Section 13.2.3. These properties make Gaus-
sian processes mathematically tractable and easy to deal with. Another
crucial reason why Gaussian distribution is necessary can be found in the
ever-present thermal noise that is produced by the random movement of
agitated electrons in electrical devices.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
669
To understand this Gaussian behavior, consider a resistor. The free
electrons in a resistor move as a result of thermal agitation. This move-
ment is random and can be in any direction. However, their velocity is
proportional to the ambient temperature; the higher the temperature,
the higher the velocity. This movement generates a current with a ran-
dom value. We can model each electron as a tiny current source whose
current is a random variable with positive or negative values. The total
current generated (which is the thermal noise) is the sum of all these cur-
rent sources. Quantum mechanics suggests that the electron movements
(i.e., current sources) are statistically independent. Thus the thermal noise
is a sum of a large number of IID random sources. Using the central limit
theorem, we conclude that this total current has a Gaussian distribution.
A random process X(t) is a Gaussian process if for all n and all time
instances (t1, t2, . . . , tn), the n random variables {X(ti)}n
i=1 are jointly
Gaussian with pdf given by
fX(x) =
1
(2π)n/2 [det(CX]1/2 exp

−1
2(x −µX)TC−1
XX (x −µX)

(13.207)
where the vector x = [x1, . . . , xn]T denotes the values taken by the n
random variables X(ti)
△= Xi, µX denotes the mean vector containing the
means, E

Xi

= µi
△= µX(ti), of the random variables Xi, and CXX de-
notes the autocovariance matrix of the n random variables with elements
CXX(tk, tℓ) = E

(Xk −µk)(Xℓ−µℓ)

(13.208)
The superscript T denotes the transpose of a vector or a matrix, and the
term C−1 denotes the inverse of matrix C.
From the above deﬁnition of the Gaussian random process, it
should be obvious that at any t, X(t) is Gaussian with mean µX(t)
and variance CXX(t, t). Similarly, if t1 ̸= t2 are two time instances,
then X(t1) and X(t2) are jointly Gaussian with means µX(t1) and
µX(t2), variances CXX(t1, t1) and CXX(t2, t2), and correlation coeﬃcient
ρX(t1, t2) = CXX(t1, t2)/
1
CXX(t1, t1)CXX(t2, t2), and so on. Thus the
Gaussian random process is completely described in the probabilistic
sense. Furthermore, if it is a stationary random process, then µX(t) = µX
and CXX(t + τ, t) = CXX(τ).
Finally, since Gaussianity is preserved under linear transformation,
if a Gaussian process X(t) is passed through a LTI system, the output
is also a Gaussian process. The eﬀect of the system operation is simply
to change the mean function µX(t) →µY(t) and the covariance function
CXX(t1, t2) →CY(t1, t2).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

670
Chapter 13
RANDOM PROCESSES
□
EXAMPLE 13.20
Let X(t) be a Gaussian random process with mean µX(t) = 3 and autocovariance
CXX(t1, t2) = 4e−0.2|t1−t2|.
1. Determine the probability that X(5) ≤2.
Solution
Note that X(t) is a stationary random process with mean µX = 3 and variance
σ2
X = CXX(t, t) = 4. Thus from (13.21) and (13.22), we obtain
Pr[X(5) ≤2] = FX(5)(2) = 1
2

1 + erf
 
2 −3

2(4)
!
= 1
2

1 + erf
	 −1
2
√
2


= 0.3085
(13.209)
which is computed using the MATLAB fragment
>> Pr = 0.5*(1+erf(-1/sqrt(2*4)));
>> fprintf(’Pr[X(5)<=2] = %6.4f\n’,Pr);
Pr[X(5)<=2] = 0.3085
2. Determine the probability that |X(8) −X(5)| ≤1.
Solution
Let Y = X(8) −X(5) be a random variable. Then Y is Gaussian with mean 0
and variance
σ2
Y = CXX(8, 8) + CXX(5, 5) −2CXX(5, 5) = 4 + 4 −8e−0.6
= 3.608
(13.210)
Hence
Pr[|X(8) −X(5)| ≤1] = Pr[|Y | ≤1] = FY(1) −FY(−1)
= 1
2

1 + erf
	
1
√
2 · 3.608


−1
2

1 + erf
	
−1
√
2 · 3.608


= 0.4014
(13.211)
which is also computed using MATLAB as shown below.
>> Pr = 0.5*(1+erf(1/sqrt(2*3.608))) ...
- 0.5*(1+erf(-1/sqrt(2*3.608)));
>> fprintf(’Pr[|X(8)-X(5)|<=1] = %6.4f\n\n’,Pr);
Pr[|X(8)-X(5)|<=1] = 0.4014
□
13.6.2 WHITE NOISE PROCESS
This is an idealization of the thermal noise generated in electronic de-
vices that helps in analysis immensely. Recall the discussion leading up to
the Gaussian process in the previous section. Using quantum mechanical
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
671
Frequency F (GHz)
1000
2000
Power Density (W/Hz)
× 10−21
0
1
2
3
Power Spectrum of Thermal Noise
FIGURE
13.19
Power
spectrum
of
thermal
noise
at
room
temperature,
T = 300 K
analysis, it can be shown that the thermal noise is a zero-mean stationary
process with a PSD given by
STN(2πF) =
ℏF
2

eℏF/kT −1
,
F in Hz
(13.212)
where ℏis Planck’s constant (equal to 6.6 × 10−34 joules-sec.), k is Boltz-
mann’s constant (equal to 1.38 × 10−23 joules/kelvin), and T is the ab-
solute temperature in degrees kelvin (K). A plot of this PSD is shown in
Figure 13.19. It achieves its maximum at F = 0 with the value of kT/2.
The spectrum goes to zero as F →∞, but the rate of decrease is very
slow. For example, at room temperature (T = 300 K), the PSD STN(2πF)
drops to only 90% of the peak value at about 2000 GHz, well beyond the
frequencies used in the conventional systems, including communication
and radar systems.
Since the thermal noise process has a PSD that is approximately
constant over a large range of frequencies, one simple approach to ap-
proximate (13.212) is to consider an ideal process that has PSD which is
exactly constant over the entire frequency range. Such a process will have
an inﬁnite power, and hence it may not be a meaningful physical pro-
cess. However, this idealization is equivalent in usefulness to the impulse
function, which also has inﬁnite power.
The noise analysis in systems is usually based on this idealized form
of noise called white noise (WN), which is a zero-mean stationary pro-
cess. The adjective white is used in the same sense as that of the white
light, which contains equal amounts of all frequencies within the visible
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

672
Chapter 13
RANDOM PROCESSES
t
0
(a)
(b)
0
F
Sw(2p F )
Rw(t )
kT 2
kT 2
FIGURE 13.20
Characteristics of white noise: (a) power spectral density, (b) au-
tocorrelation function
electromagnetic spectrum. The white noise will be denoted by W(t) and
is deﬁned in terms of its PSD as
SWW(2πF) = N0
2 , watts
Hz
or
SWW(Ω) = N0
4π ,
watts
rad/sec
(13.213)
This PSD is shown in Figure 13.20 and is sometimes referred to as the two-
sided power spectral density, emphasizing that this spectrum extends to
both positive and negative frequencies. The parameter N0 is expressed as
N0 = kT,
watts per Hz
(13.214)
so that SWW(2πF) matches the thermal PSD at F = 0. The autocorrela-
tion function of W(t) is given by
RWW(τ) = N0
2 δ(τ)
(13.215)
which reenforces the fact that white noise is an unrealistic process. This
autocorrelation function is also shown in Figure 13.20. Clearly, the sam-
ples W(t1) and W(t2) of white noise for t1 ̸= t2 are uncorrelated. If W(t)
is also Gaussian (since thermal noise is Gaussian), then it is called a
white Gaussian noise (WGN) process. In this case, the components of
the process (i.e., random variables at each t) are also independent. Such
a process is called an independent process.
Discrete-Time White Noise Process
In a similar fashion, a discrete-time white noise process W(n) is deﬁned
as a zero-mean stationary random sequence with PSD
SWW(2πf) = σ2
W
or
SWW(ω) = σ2
W
2π ,
f = ω
2π
(13.216)
where σ2
W is the variance of W(n). The autocorrelation of W(n) is then
given by
RWW(ℓ) = σ2
Wδ(ℓ)
(13.217)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
673
In addition, if W(n) is also Gaussian at each n, then it is termed a WGN
process, which is also an independent process. Sample sequences for inde-
pendent WN processes are easy to generate in MATLAB using the randn
or rand functions. For example,
>> Wn = randn(N,1);
generates N samples of unit variance WGN process, while
>> Wn = rand(N,1);
generates N samples of unit variance independent WN process.
□
EXAMPLE 13.21
Generate 10,000 samples of a WGN random process with variance σ2
W = 4.
Numerically estimate autocorrelation lag values RWW(ℓ), −20 ≤ℓ≤20 and use
these to compute the PSD SWW(2πf). Repeat this procedure over 100 sample
sequences to reduce variability of estimates and average RWW(ℓ) and SWW(2πf)
over these sample functions and plot the resulting quantities.
Solution
We will use the randn function to generate WGN samples, the xcorr function to
estimate autocorrelation lag values, and the PSD function to compute the PSD.
These steps and averaging operations are illustrated in the following MATLAB
script.
>> M = 100; % Number of sample sequences to average over
>> N = 10000; % Number of samples in each sequence
>> varW = 4; % Variance of the process
>> sigW = sqrt(varW); % Standard deviation
>> maxlag = 20; % Maximum number of lag values
>> Nfft = 1024; Nfft2 = Nfft/2; % FFT size
>> Rwsum = zeros(2*maxlag+1,1)’; % Rw initialization
>> Swsum = zeros(Nfft+1,1); % Sw initialization
>> ell = -maxlag:maxlag; % Lag indices for plotting
>> f = linspace(-0.5,0.5,Nfft+1); % Frequency grid for plotting
>> % Loop over M Sample Sequences
>> for k = 1:M
>>
% Generate 10000 samples of WGN Process
>>
wn = sigW*randn(N,1)’;
>>
% Compute ACRS
>>
[Rw,lags] = xcorr(wn, maxlag,’unbiased’);
>>
Rwsum = Rw+Rwsum; % Sum autocorrelations
>>
Sw = PSD(Rw’,maxlag,Nfft); % Compute PSD
>>
Swsum = Sw+Swsum; % Sum PSD values
>> end
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

674
Chapter 13
RANDOM PROCESSES
Lags
–20
0
RW[lags]
0
4
Autocorrelation Samples
Frequency  f
20
–0.5
0
0.5
SW(2p f)
0
4
Power Spectral Density
FIGURE 13.21
Plots of the autocorrelation RWW(ℓ) and PSD SWW(2πf) in
Example 13.21
>> Rw = Rwsum/M;
% Average autocorrelations
>> Sw = Swsum
/M; % Average PSD
>> % Plotting commands follow
The resulting plots of RWW(ℓ) and SWW(2πf) are shown In Figure 13.21. The
white noise property of the generated random numbers is quite obvious from
these plots.
□
13.6.3 MARKOV PROCESS
In some applications, it is advantageous to model a random process whose
future values depend only on the present value. This reduces storage as
well as processing complexities. Such a process is called a Markov process,
and it is a process whose past has no inﬂuence on the future given that its
present is speciﬁed; that is, if tn > tn−1, then the conditional distribution
of a Markov process X(t) based on the inﬁnite past
Pr

X(tn) ≤xn | X(t), t ≤tn−1

= Pr

X(tn) ≤xn | X(tn−1)

(13.218)
depends on its immediate past. It further follows that if t1 < t2 < · · · < tn,
then
Pr

X(tn) ≤xn | X(tn−1), X(tn−2), . . . , X(t1)

= Pr

X(tn) ≤xn | X(tn−1)

(13.219)
If X(t) is a Gaussian process, then it is called a Gauss–Markov process.
It can be shown that continuous-time LTI systems described by linear,
constant-coeﬃcient differential equations and driven by a WGN process
generate Gauss–Markov processes.
Discrete-Time Markov Process
Similarly, discrete-time LTI systems described by linear, constant-
coeﬃcient difference equations and driven by WGN sequence generate
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
675
Gauss–Markov sequences. Hence the simplest method for generating
a Gauss–Markov sequence is to use the ﬁrst-order auto-regressive AR
system
X(n) = ρX(n −1) + W(n)
(13.220)
where W(n) is an independent, zero-mean WGN sequence and the
ﬁlter coeﬃcient ρ represents the correlation coeﬃcient between X(n)
and X(n −1), that is,
E

X(n)X(n −1)

= E

(ρX(n −1) + W(n))X(n −1)

= ρE

X2(n −1)

= ρσ2
X(n −1)
(13.221)
Thus in this process, we have to store only one past value to generate the
next correlated value.
□
EXAMPLE 13.22
Generate 10,000 samples of a Gauss–Markov process X(n) that has correlation
coeﬃcient ρ = 0.7 and average power of 100 watts.
Solution
We will use (13.220) with ρ = 0.7 to generate samples of X(n) from W(n). To
generate the WGN W(n), we will need its variance σ2
W. Following the steps used
in Example 13.14, we can show that the autocorrelation of X(n) in (13.220) is
given by
RXX(ℓ) =
	
σ2
W
1 −ρ2

ρ|ℓ| =
	 σ2
W
0.51

0.7|ℓ|
(13.222)
The average power in X(n) is then given by
RXX[0] = σ2
W
0.51
(13.223)
Hence for RXX[0] = 100, we obtain σ2
W = 51. Now we will generate samples of
W(n) using the randn function and ﬁlter those samples using (13.220) to obtain
samples of X(n). The following MATLAB script provides these steps.
>> N = 10000; varW = 51;
>> wn = sqrt(varW)*randn(N,1);
>> rho = 0.7; b = 1; a = [1,-rho];
>> xn = filter(b,a,wn);
The resulting samples were used in Example 13.11. Plots of these samples and
the normalized autocorrelation are shown in Figure 13.12.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

676
Chapter 13
RANDOM PROCESSES
13.6.4 FILTERED NOISE PROCESS
Even if we model the thermal noise as a white noise process at the input
of a communication receiver system, it gets processed, or, more correctly,
ﬁltered, by subsequent stages of operations in the receiver. At each stage,
a correlated noise process is generated, known as a color noise process, or
colored process for short. If the frequency response function of the receiver
is H(jΩ) and the input is WGN W(t) with PSD SWW(Ω) = N0/(4π), then
the colored process X(t) is also zero mean with PSD
SXX(Ω) = N0
4π
H(jΩ)
2
(13.224)
Generally, the stages in the receiver are bandpass systems, and hence
the ﬁltered noise is a bandpass process. However, the information sig-
nals generally have lowpass power distribution. We now consider both
processes below.
Lowpass Process
A random process X(t) is called lowpass if its power spectrum is large in
the vicinity of F = 0 Hz and small (approaching 0) at high frequencies.
In other words, a lowpass random process has most of its power concen-
trated at low frequencies. The discrete-time random process generated in
Example 13.22 is a lowpass process whose PSD is shown in Figure 13.16.
A lowpass random process X(t) is bandlimited if its power spectrum
is given by SXX(jΩ) = 0 for |Ω| > 2πB. The parameter B is called the
bandwidth in Hz of the random process. It can be generated by ﬁltering
the WN process through a lowpass ﬁlter. Similarly, an ideal bandlimited
lowpass process can be generated by ﬁltering a WN process through an
ideal ﬁlter with frequency response function
H(jΩ) =

1,
|Ω| ≤2πB
0,
|Ω| > 2πB
(13.225)
It has the PSD (in watts per radian) given by
SXX(Ω) =

N0/(4π),
|Ω| ≤2πB
0,
|Ω| > 2πB
(13.226)
which is shown in Figure 13.22(a), from which the average power in this
process is PX = N0
4π (4πB) = N0B watts. The autocorrelation function is
given by
RXX(τ) = N0B sinc(2Bτ)
(13.227)
It is also shown in Figure 13.22. Thus from the zero-crossings, we conclude
that every random variables pair in the ideal bandlimited lowpass process,
separated by τ = 1/2B lag interval, is uncorrelated (due to zero mean)
or is independent if X(t) is also Gaussian.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
677
Sw(Ω)
Rw(τ)
N0B
0
τ
(a)
–2pB
2pB
(b)
Ω
N0
4p
1
2B
3
2B
5
2B
1
B
2
B
FIGURE 13.22
Ideal lowpass process: (a) PSD, (b) autocorrelation function
□
EXAMPLE 13.23
Consider the problem of generating samples of a bandlimited lowpass process by
ﬁltering a discrete-time WGN process through a properly designed IIR elliptic
ﬁlter that approximates the ideal ﬁlter in (13.225). Obtain samples X(n) of a
lowpass process X(t) that is bandlimited to 3 KHz using a ﬁfth-order elliptic
ﬁlter. Choose a sampling rate of 20 KHz.
Solution
First, we will design a ﬁfth-order elliptic ﬁlter by setting passband cutoﬀfre-
quency to ωp = (3/20)2π = 0.3π. To approximate the ideal lowpass ﬁlter, we
will choose passband ripple of 0.1 dB and stopband attenuation of 50 dB, which
are quite reasonable. Now using the ellip function, we obtain the required low-
pass ﬁlter that can be used in ﬁltering WGN samples to obtain samples of the
bandlimited lowpass process. The following MATLAB script gives all the nec-
essary details.
M = 100; % Number of sample sequences to average over
N = 10000; % Number of samples in each sequence
varW = 4; % Variance of the input WGN process
sigW = sqrt(varW); % Standard deviation
maxlag = 50; % Maximum number of lag values
Nfft = 2048; Nfft2 = Nfft/2; % FFT size
Rxsum = zeros(2*maxlag+1,1)’; % Contains Autoc sum for averaging
Sxsum = zeros(Nfft+1,1); % Contains PSD sum for averaging
ell = -maxlag:maxlag; % Lag indices
f = linspace(-0.5,0.5,Nfft+1); % Frequency grid
% Approximation to Ideal LPF using Elliptic Filter
omp = 0.3; % Passband cutoff in pi units
Rp = 0.1; % Passband ripple in dB
As = 50; % Stopband attenuation in dB
Nellip = 5; % Order of the elliptic filter
[b,a] = ellip(Nellip,Rp,As,omp); % Elliptic filter coefficients
% Loop over M Sample Sequences
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

678
Chapter 13
RANDOM PROCESSES
t in ms
x(t)
–4
0
5
10
15
4
Sample Sequence
Lag t  in ms
–2.5
0
2.5
RX[t]
0
1.3
Autocorrelation Function
Frequency F in KHz
–3
0
3
SX(2p F)
0
4
Power Spectral Density
FIGURE 13.23
A few illustrative samples, the autocorrelation values, and the
PSD function of a generated bandlimited lowpass process
for k = 1:M
% Generate 10000 samples of WGN Process
wn = sigW*randn(N,1)’;
xn = filter(b,a,wn); % Filtered WGN using lowpass filter
% Compute ACRS
[Rx,lags] = xcorr(xn, maxlag,’unbiased’);
Rxsum = Rx+Rxsum;
Sx = PSD(Rx,maxlag,Nfft); % Compute PSD
Sxsum = Sx+Sxsum;
end
Rx = Rxsum/M;
Sx = Sxsum/M;
% Plotting commands follow
The top plot in Figure 13.23 shows few samples of the generated process,
while the bottom plots show its autocorrelation and PSD functions, respectively.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
679
Sw(Ω)
H( j2πF)
H ( jΩ)
1
0
F
W(t)
X(t)
–F0 – B
F0 – B
–F0 + B
–F0
F0
Bandpass
N0
4p
F0 + B
FIGURE 13.24
White noise ﬁltered through an ideal bandpass ﬁlter
The autocorrelation function plot clearly shows the sinc function shape, while
the PSD plot is bandlimited to 3 KHz.
□
Bandpass Process
A random process is called a bandpass process if its power spectrum is
large in a band of frequencies centered in the neighborhood of a central
frequency ±F0 and relatively small outside of this band of frequencies. A
random process is called narrowband bandpass if its bandwidth B ≪F0.
Let X(t) be the output of an ideal bandpass ﬁlter H(jΩ) whose input
is a WGN process W(t) with PSD SWW(Ω) = N0/(4π). The ideal band-
pass ﬁlter has a bandwidth of B Hz that is located at frequencies around
F0 Hz as shown in Figure 13.24 and is given by
H(jΩ) =
)
1,
|Ω−Ω0| ≤2πB
0,
otherwise
(13.228)
Since the thermal noise is white and Gaussian, the ﬁltered noise X(t) is
Gaussian but not white. The PSD of the ﬁltered noise is given by
SXX(Ω) =
H(jΩ)
2SWW(Ω)
= N0
4π H(jΩ) =
) N0
4π ,
|Ω−Ω0| ≤2πB
0,
otherwise
(13.229)
and the average power in the ﬁltered process X(t) is given by
PX = 2 × N0
4π × 4πB = 2N0B
watts
(13.230)
□
EXAMPLE 13.24
In this example, we will use the procedure of Example 13.23 to obtain samples
of an approximated bandlimited bandpass process. Generate samples X(n) of
a bandpass process X(t) that is bandlimited between 3 and 6 KHz using a
tenth-order bandpass elliptic ﬁlter. Choose sampling rate of 20 KHz.
Solution
Again, we will ﬁrst design a tenth-order elliptic ﬁlter by setting passband cutoﬀ
frequencies to ωp1 = (3/20)2π = 0.3π and ωp2 = (6/20)2π = 0.6π. To ap-
proximate the ideal bandpass ﬁlter, we will choose passband ripple of 0.1 dB
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

680
Chapter 13
RANDOM PROCESSES
and stopband attenuation of 50 dB, which are quite reasonable. The stopband
cutoﬀfrequencies will be set by the order of the ﬁlter. Now using the ellip
function, we obtain the required bandpass ﬁlter that can be used in ﬁltering
WGN samples to obtain samples of the bandlimited bandpass process. The fol-
lowing MATLAB script gives all the necessary details. A representative segment
of the bandpass process, its autocorrelation, and the resulting PSD functions
are shown in Figure 13.25.
M = 100; % Number of sample sequences to average over
N = 10000; % Number of samples in each sequence
varW = 1; % Variance of the input WGN process
sigW = sqrt(varW); % Standard deviation
maxlag = 50; % Maximum number of lag values
Nfft = 2048; Nfft2 = Nfft/2; % FFT size
Rxsum = zeros(2*maxlag+1,1)’; % Contains Autoc sum for averaging
Sxsum = zeros(Nfft+1,1); % Contains PSD sum for averaging
ell = -maxlag:maxlag; % Lag indices
f = linspace(-0.5,0.5,Nfft+1); % Frequencies in cycles/sam
Fs = 20; % Sampling rate in Khz
F = f*Fs; % Frequencies in KHz
% Approximation to ideal BPF using elliptic filter
omp1 = 0.3; % Lower Passband cutoff in pi units
omp2 = 0.6; % Upper Passband cutoff in pi units
Rp = 0.1; % Passband ripple in dB
As = 50; % Stopband attenuation in dB
Nellip = 5; % Order of the resulting elliptic filter is 2*N
[b,a] = ellip(Nellip,Rp,As,[omp1,omp2]); % Elliptic filter coefficients
% Loop over M sample sequences
for k = 1:M
% Generate 10000 samples of WGN Process
wn = sigW*randn(N,1)’;
xn = filter(b,a,wn); % Filtered WGN using lowpass filter
% Compute ACRS
[Rx,lags] = xcorr(xn, maxlag,’unbiased’);
Rxsum = Rx+Rxsum;
Sx = PSD(Rx,maxlag,Nfft); % Compute PSD
Sxsum = Sx+Sxsum;
end
Rx = Rxsum/M;
Sx = Sxsum/M;
% Plotting commands follow
□
Bandpass processes are suitable for representing modulated signals.
In a communication system, the information-bearing signal is usually a
lowpass random process that modulates a carrier for transmission over
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
681
t in ms
10
5
15
x(t)
–2
0
2
Segment of a Bandlimited Bandpass Process
Lag t in ms 
–2.5
0
2.5
RX(t)
–0.26
0
0.33
Autocorrelation Function
Frequency F in KHz
–6
–3
0
3
6
SX(2p F)
0
1
Power Spectral Density
FIGURE 13.25
A segment of the process, its autocorrelation, and the resulting
PSD functions of a generated bandlimited bandpass process in Example 13.24
a bandpass (narrowband) communication channel. Thus the modulated
signal is a bandpass random process.
A bandpass random process X(t) can be represented in terms of low-
pass processes as
X(t) = Xc(t) cos(2πF0t) −Xs(t) sin(2πF0t)
(13.231)
where Xc(t) and Xs(t) are called the in-phase and quadrature components
of X(t). These random processes Xc(t) and Xs(t) are lowpass processes.
Furthermore, there is an important relationship among these X(t), Xc(t),
and Xs(t) processes, given below without proof:
THEOREM 1
If X(t) is a zero-mean, stationary random process, then processes Xc(t)
and Xs(t) are also zero-mean, jointly stationary processes.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

682
Chapter 13
RANDOM PROCESSES
Lowpass
Filter
WGN
cos(2pF0t)
sin(2pF0t)
WGN
Lowpass
Filter
Xs(t)
–Xs(t) sin(2pF0t)
X(t) = Xc(t) cos(2pF0t)
Xc(t)
FIGURE 13.26
Generation of a bandpass random process
In fact, it can be easily proved that (see [82]) the autocorrelation
functions of Xc(t) and Xs(t) are identical and may be expressed as
RXc(τ) = RXs(τ) = RXX(τ) cos(2πF0τ) + ˆRXX(τ) sin(2πF0τ)
(13.232)
where RXX(τ) is the autocorrelation function of the bandpass process X(t)
and ˆRXX(τ) is the Hilbert transform of RX(τ), which is deﬁned as
ˆRXX(τ) = 1
π
 ∞
−∞
RXX(τ)
τ −t dt
(13.233)
The cross-correlation function of Xc(t) and Xs(t) can be expressed as
RXcs(τ) = RXX(τ) sin(2πF0τ) −ˆRXX(τ) cos(2πF0τ)
(13.234)
□
EXAMPLE 13.25
Generate samples of a Gaussian bandpass random process by ﬁrst generating
samples of two statistically independent Gaussian random processes Xc(t) and
Xs(t) and then using these to modulate the quadrature carriers cos(2πF0t) and
sin(2πF0t), as shown in Figure 13.26.
Solution
On a digital computer or in MATLAB, samples of the lowpass processes Xc(t)
and Xs(t) are generated by ﬁltering two independent Gaussian white noise se-
quences by two identical lowpass ﬁlters. Thus we obtain the samples Xc(n) and
Xs(n), corresponding to the sampled values of Xc(t) and Xs(t). Then Xc(n) mod-
ulates the sampled carrier cos(2πF0nT), and Xs(n) modulates the quadrature
carrier sin(2πF0nT), where T is the appropriate sampling interval.
The MATLAB script for these computations is given below. For illustrative
purposes, we have selected the lowpass ﬁlter to have a system function
H(z) =
1
1 −0.9z−1
(13.235)
Also, we selected T = 0.001 sec or sampling rate of 1000 sam/sec and F0 =
200 Hz. A representative segment of the bandpass process, its autocorrelation,
and the resulting PSD are shown in Figure 13.27.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Useful Random Processes
683
t in ms
100
200
300
x(t)
–6
0
6
Sample Waveform
Lag t in ms
–50
0
50
RX(t)
–3.4
0
5.2
Autocorrelation Function
Frequency F in Hz
–200
0
200
SX(2pF)
0
40
Power Spectral Density
FIGURE 13.27
A segment of the process, its autocorrelation, and the resulting
PSD functions of a generated bandpass process in Example 13.25
M = 100; % Number of sample sequences to average over
N = 10000; % Number of samples in each sample sequence
varW = 1; % Variance of the input WGN process
sigW = sqrt(varW); % Standard deviation
maxlag = 50; % Maximum number of lag values
Nfft = 2048; Nfft2 = Nfft/2; % FFT size
Rxsum = zeros(2*maxlag+1,1)’; % Contains Autoc sum for averaging
Sxsum = zeros(Nfft+1,1); % Contains PSD sum for averaging
ell = -maxlag:maxlag; % Lag indices
f = linspace(-0.5,0.5,Nfft+1); % Frequency grid
Fs = 1000; % Sampling rate in Khz
F = f*Fs; % Frequencies in KHz
F0 = 200; % Carrier frequency in Hz
T = 1/Fs; % Sampling interval
t = (0:N-1)*T; % Sampled time instances
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

684
Chapter 13
RANDOM PROCESSES
% Lowpass filter for illustration
b = 1; a = [1,-0.9];
% Loop over M Sample Sequences
for k = 1:M
% Generate 10000 samples of WGN process
wcn = sigW*randn(N,1)’; % Input WGN for Xc(n)
wsn = sigW*randn(N,1)’; % Input WGN for Xs(n)
xcn = filter(b,a,wcn); % WGN -> lowpass filter -> Xc(n)
xsn = filter(b,a,wsn); % WGN -> lowpass filter -> Xs(n)
xn = xcn.*cos(2*pi*F0*t) - xsn.*sin(2*pi*F0*t); % BP process
% Compute ACRS
[Rx,lags] = xcorr(xn, maxlag,’unbiased’);
Rxsum = Rx+Rxsum;
Sx = PSD(Rx,maxlag,Nfft); % Compute PSD
Sxsum = Sx+Sxsum;
end
Rx = Rxsum/M; Sx = Sxsum/M;
% Plotting commands follow
□
13.7 SUMMARY AND REFERENCES
In this chapter, we have presented probabilistic and statistical description
of signals with randomly varying waveforms and provided tools and tech-
niques to process them through LTI systems. We began in Section 13.1
with the concept of a random variable as a model for single random
measurement. We described it using probability functions, the most im-
portant one being the marginal probability density function (pdf). We
developed MATLAB function pdf1 to simulate the pdf using normalized
histogram. For practical simplicity, we emphasized statistical averages
(or moments) of mean and variance to describe the random variable and
provided tools to estimate its mean value using several observed mea-
surements. We discussed few useful random variable models including
uniform and Gaussian distributions.
In Section 13.2, we extended this treatment to a pair of random vari-
ables using the joint pdf and the joint moments of correlation and co-
variance. We showed the use of scatter-plots to simulate the joint density
function in MATLAB. The important model for the random variable pair
(namely, the bivariate Gaussian distribution) was discussed in detail, in-
cluding enumeration of its unique and important properties.
The major focus of this chapter was on random signals, or stochastic
processes. To this end, in Section 13.3, we modeled the random signal
value X(t) at each ﬁxed time instance t, as a random variable and at
two ﬁxed-time instances, t1 and t2, the two random measurements
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Summary and References
685
(X(t1), X(t2)) as a pair of random variables. This allowed us to extend
and apply concepts such as the marginal and joint densities, the mean
and variance, and the correlations and covariances to random processes
in a straightforward manner. The means and variances became temporal
functions while autocorrelations and autocovariances became joint tem-
poral functions. In practice, we are interested in statistical quantities that
do not vary with time, so we focused on stationary processes in which
means were constant and auto moments were functions of lag variable
τ = t1 −t2. Also, for practical considerations, we discussed ergodic pro-
cesses that allowed us to estimate mean and autocorrelation from the
observation of a single sample waveform. We extended these ideas to
discrete-time random processes or random sequences. The use of xcorr
function in MATLAB was illustrated through several examples.
One of the most useful parameters in signal description is its average
power. In Section 13.4, we developed the Wiener–Khinchin theorem that
related the autocorrelation function of a stationary process to the power
spectral density through Fourier transform and allowed us to obtain aver-
age signal power in the frequency domain. This concept was extended to
random sequences as well. In Section 13.5, we discussed ﬁltering of station-
ary random processes and sequences through LTI systems. We developed
formulas to obtain output means and autocorrelations using convolutions
with the impulse response in the time domain. Likewise, we obtained
formulas to compute output psd functions using the frequency response
function in the frequency domain.
In Section 13.6, we discussed several useful practical random pro-
cesses and how to obtain their samples through simple linear ﬁltering.
The Gaussian process was an extension of the bivariate Gaussian distri-
bution to multivariate distribution at several arbitrary time instances.
The white noise process was developed as an idealized model to describe
thermal noise in electrical components. This noise process was further ﬁl-
tered through recursive equations to obtain Markov processes and through
lowpass and bandpass ﬁlters to obtain lowpass and bandpass processes,
respectively. Finally, we showed how to generate bandlimited lowpass and
bandpass processes ﬁltering white noise processes through carefully de-
signed elliptic ﬁlters.
The material presented in this chapter is available in many excel-
lent textbooks written over the period of last 50 years. One of the best
sources for insightful exposition of probability and stochastic processes is
the book by Papoulis and Pillai [73], now in its fourth edition. For grad-
uate students, books by Stark and Woods [90] and Leon-Garcia [53] are
good choices. For reading at the undergraduate level, books by Miller and
Childers [67], and Yates and Goodman [99] are suﬃcient. This material
is also available either as a single chapter or a part of a chapter in many
books, including Proakis and Salehi [82], Oppenheim and Schafer [71],
Manolakis and Ingle [60], and Manolakis, Ingle, and Kogon [61].
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

686
C H A P T E R 14
Linear Prediction
and Optimum
Linear Filters
The design of ﬁlters to perform signal estimation is a problem that
frequently arises in the design of communication systems and control
systems, in geophysics, and in many other disciplines. In this chapter, we
treat the problem of optimum ﬁlter design from a statistical viewpoint.
The ﬁlters are constrained to be linear and the optimization criterion is
based on the minimization of the mean square error. As a consequence,
only the second-order statistics (autocorrelation and cross-correlation
functions) of a stationary process are required in the determination of
optimum ﬁlters.
Included in this treatment is the design of optimum ﬁlters for linear
prediction. Linear prediction is a particularly important topic in digital
signal processing, with applications in a variety of areas, such as speech
signal processing, image processing, and noise suppression in communica-
tion systems. As we shall observe, the determination of the optimum ﬁlter
for linear prediction requires the solution of a set of linear equations that
have special symmetry. To solve these linear equations, we describe two
algorithms, Levinson–Durbin and Schur, which provide the solution to
the equations through computationally eﬃcient procedures that exploit
the symmetry properties.
The last section of this chapter treats an important class of optimum
ﬁlters called Wiener ﬁlters, which are widely used in many applications
involving the estimation of signals corrupted with additive noise.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
687
14.1 INNOVATIONS REPRESENTATION OF A STATIONARY
RANDOM PROCESS
In this section, we demonstrate that a wide-sense stationary random pro-
cess may be represented as the output of a causal and causally invertible
linear system excited by a white noise process. The condition that the
system is causally invertible also allows us to represent the wide-sense
stationary random process by the output of the inverse system, which is
a white noise process.
Let us consider a wide-sense stationary process X(n) with autocorrela-
tion sequence

RXX(m)

and power spectral density1 SXX(f), |f| ≤1
2. We
assume that SXX(f) is real and continuous for all |f| ≤1
2. The z-transform
of the autocorrelation sequence {RXX(m)} was termed the complex PSD
and is given by2
SXX(z) =
∞

m=−∞
RXX(m)z−m
(14.1)
from which we obtain the power spectral density SXX(f) by evaluating
SXX(z) on the unit circle—that is, by substituting z = exp(j2πf).
Now, let us assume that log S(z) is analytic (possesses derivatives of
all orders) in an annular region in the z-plane that includes the unit circle,
that is, r1 < |z| < r2 where r1 < 1 and r2 > 1. Then log SXX(z) may be
expanded in a Laurent series of the form
log SXX(z) =
∞

m=−∞
v(m)z−m
(14.2)
where the v(m) are the coeﬃcients in the series expansion. We may view
{v(m)} as the sequence with z-transform V (z) = log SXX(z). Equivalently,
we may evaluate log SXX(z) on the unit circle,
log SXX(z)

z=ej2πf = log SXX(f) =
∞

m=−∞
v(m)e−j2πfm
(14.3)
1In this chapter, we slightly abuse our Chapter 13 notation of SXX(2πf) for PSD and
denote it by SXX(f) for simplicity.
2Again, for simplicity we use SXX(z) instead of ˜SXX(z) to denote a complex PSD, as
was done in Chapter 13. It should be noted that SXX(ω), SXX(f), and SXX(z) all refer
to the same basic PSD function but with diﬀerent frequency argument dependencies.
The exact dependency should be clear from the context.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

688
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
so that the v(m) are the Fourier coeﬃcients in the Fourier series expansion
of the periodic function log SXX(f). Hence
v(m) =

1
2
−1
2
[log SXX(f)]ej2πfm df,
m = 0, ±1, . . .
(14.4)
We observe that v(m) = v(−m), since SXX(f) is a real and even function
of f.
From (14.2), it follows that
SXX(z) = exp

∞

m=−∞
v(m)z−m

= exp

−1

m=−∞
v(m)z−m + v(0) +
∞

m=1
v(m)z−m

△= σ2
WH(z)H(z−1)
(14.5)
where, by deﬁnition, σ2
W = exp[v(0)] and
H(z) = exp
 ∞

m=1
v(m)z−m

,
|z| > r1
(14.6)
If (14.5) is evaluated on the unit circle, we have the equivalent represen-
tation of the power spectral density as
SXX(f) = σ2
W
H(f)
2
(14.7)
We note that
log SXX(f) = log σ2
W + log H(f) + log H∗(f)
=
∞

m=−∞
v(m)e−j2πfm
From the deﬁnition of H(z) given by (14.6), it is clear that the causal
part of the Fourier series in (14.3) is associated with H(z) and that the
anticausal part is associated with H

z−1	
.
The ﬁlter with system function H(z) given by (14.6) is analytic in the
region |z| > r1 < 1. Hence, in this region, it has a Taylor series expansion
as a causal system of the form
H(z) =
∞

n=0
h(n)z−n
(14.8)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
689
Linear causal
ﬁlter
H(z)
W(n)
W(n)
White noise
X(n)
X(n)
=
k=0
h(k)W(n−k)
Linear causal
ﬁlter
1
H(z)
White noise
Σ
∞
(a)
(b)
FIGURE 14.1
Filters for generating (a) the random process X(n) from white
noise and (b) the inverse ﬁlter
The output of this ﬁlter to a white noise input sequence W(n) with power
spectral density σ2
W is a stationary random process X(n) with power spec-
tral density SXX(f) = σ2
W
H(f)
2. Conversely, the stationary random pro-
cess X(n) with power spectral density SXX(f) may be transformed into a
white noise process by passing X(n) through a linear ﬁlter with system
function 1/H(z). We call this ﬁlter a noise-whitening filter. Its output,
denoted by W(n), is called the innovations process associated with the sta-
tionary random process X(n). These two relationships are illustrated in
Figure 14.1, where x(n) and w(n) are sample sequences of the stationary
random processes X(n) and W(n), respectively.
The representation of stationary stochastic process X(n) as the output
of an IIR ﬁlter with system function H(z) as given by (14.8) and excited
by a white noise sequence W(n) is called the Wold representation.
□
EXAMPLE 14.1
The autocorrelation function of a wide-sense stationary random process is
given by
RXX(m) = 5
 1
2
|m|
a. Determine SXX(z) and its factors σ2
W, H(z), and H

z−1
as given in (14.5).
b. Plot RXX(m) and SXX(f) for |f| ≤1
2. Note that
SXX(f) =
∞

m=−∞
RXX(m)e−j2πfm and
RXX(m) =

1
2
−1
2
SXX(f)ej2πfm df
c. Implement the ﬁlter H(z) to generate an output sequence x(n) for 0 ≤n ≤
10,000, when the input sequence w(n) is a sample sequence of a white,
zero-mean Gaussian noise sequence with variance σ2
W. Compute and plot
the autocorrelation ˆRXX(m) for |m| ≤50 and the power density spectrum
ˆSXX(f) of the output sequence x(n). Compare these plots with the analytic
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

690
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
results of part (b) and comment on similarities and diﬀerences. The biased
estimates ˆRXX(m) and ˆSXX(f) are deﬁned as follows:
ˆRXX(m) = 1
N
N−m−1

n=0
x∗(n)x(n + m),
0 ≤m ≤N −1
ˆSXX(f) =
N−1

m=−(N−1)
RXX(m)e−j2πfm,
|f| ≤1
2
Solution
Note that this example is similar to Example 13.13.
a. The z-transform of RXX(m) is
SXX(z) =
∞

m=−∞
RXX(m)z−m = 5

−1

m=−∞
1
2

z−m +
∞

m=0
1
2
m
z−m

= 15
4
	
1
(1 −1
2z−1)(1 −1
2z)

,
1
2 < |z| < 2
Therefore, σ2
W = 15
4 and H(z) =
1
1 −1
2z−1
b. The PSD SXX(f) is given by
SXX(f) =
∞

m=−∞
RXX(m)e−j2πfm = 5
∞

m=−∞
 1
2
|m|e−j2πfm
=
15
5 −4 cos(2πf)
using (13.148). Plots of RXX(m) and SXX(f) are shown in Figure 14.2.
Lag m
–10
–5
0
5
10
RXX(m)
0
5
Autocorrelation Sequence
Frequency f, cyc/sam
–0.5
0
0.5
SXX(f)
5/3
15
Power Spectral Density
FIGURE
14.2
Plots
of
autocorrelation
RXX(m)
and
PSD
SXX(f)
in
Example 14.1(b)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
691
Lag m
–10
–5
0
5
10
RXX(m)
0
5
Autocorrelation Estimate
Frequency f, cyc/sam
–0.5
0
0.5
SXX(f)
5/3
15
PSD Estimate
FIGURE 14.3
Plots of the estimated autocorrelation sequence ˆRXX(m) and PSD
ˆSXX(f) in Example 14.1(c)
c. The diﬀerence equation for the ﬁlter H(z) is
x(n) = 1
2x(n −1) + w(n)
where w(n) is a sample sequence from a white Gaussian noise process with
variance σ2
W =
15
4 . Plots of the estimated autocorrelation ˆRXX(m) and its
Fourier transform ˆSXX(f) are shown in Figure 14.3 which were obtained
using the procedures discussed in Chapter 13. We observe that these plots
are similar to those of part (b).
□
14.1.1 RATIONAL POWER SPECTRA
Let us now restrict our attention to the case where the power spectral
density of the stationary random process X(n) is a rational function,
expressed as
SXX(z) = σ2
W
B(z)B(z−1)
A(z)A(z−1) ,
r1 < |z| < r2
(14.9)
where the polynomials B(z) and A(z) have roots that fall inside the unit
circle in the z-plane. Then the linear ﬁlter H(z) for generating the random
process X(n) from the white noise sequence W(n) is also rational, and is
expressed as
H(z) = B(z)
A(z) =

q
k=0 bkz−k
1 + 
p
k=1 akz−1 ,
|z| > r1
(14.10)
where bk and ak are the ﬁlter coeﬃcients that determine the location of
the zeros and poles of H(z), respectively. Thus H(z) is a causal, stable,
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

692
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
and minimum phase linear system. Its reciprocal 1/H(z) is also a causal,
stable, and minimum phase linear system. Therefore, the random pro-
cess X(n) uniquely represents the statistical properties of the innovations
process W(n), and vice versa.
For the linear system with the rational system function H(z) given
by (14.10), the output process X(n) is related to the input process W(n)
by the diﬀerence equation
X(n) +
p

k=1
akX(n −k) =
q

k=0
bkW(n −k)
(14.11)
We distinguish among three speciﬁc cases:
Autoregressive (AR) Process: b0 = 1, bk = 0, k > 0.
In this case, the linear ﬁlter H(z) = 1/A(z) is an all-pole ﬁlter, and
the diﬀerence equation for the input-output relationship is
X(n) +
p

k=1
akX(n −k) = W(n)
(14.12)
In turn, the noise-whitening ﬁlter for generating the innovations process
is an all-zero ﬁlter.
Moving Average (MA) Process: ak = 0, k ≥1.
In this case, the linear ﬁlter H(z) = B(z) is an all-zero ﬁlter, and the
diﬀerence equation for the input-output relationship is
X(n) =
q

k=0
bkW(n −k)
(14.13)
The noise-whitening ﬁlter for the MA process is an all-pole ﬁlter.
Autoregressive, Moving Average (ARMA) Process. In this case,
the linear ﬁlter H(z) = B(z)/A(z) has both ﬁnite poles and zeros in the
z-plane, and the corresponding diﬀerence equation is given by (14.11).
The inverse system for generating the innovations process from X(n) is
also a pole-zero system of the form 1/H(z) = A(z)/B(z).
□
EXAMPLE 14.2
Consider an ARMA process X(n) generated by the diﬀerence equation
X(n) = 1.6X(n −1) −0.63X(n −2) + W(n) + 0.9W(n −1)
where W(n) is a white noise sequence with unit variance.
a. Determine the system function H(z) and the system function of the noise
whitening ﬁlter and its poles and zeros. Is the noise whitening ﬁlter stable?
b. Determine and plot the power spectrum density SXX(f) for |f| ≤1
2.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
693
Frequency f, cyc/sam
–0.5
0
0.5
SXX(f)
0
4011
Power Spectral Density
FIGURE 14.4
Plot of power spectral density ˆSXX(f) in Example 14.2(b)
Solution
a. The system function is given by
H(z) =
1 + 0.9z−1
1 −1.6z−1 + 0.63z−2
Whitening ﬁlter:
B(z) = H(z)−1 = 1 −1.6z−1 + 0.63z−2
1 + 0.9z−1
zeros:
z1,2 = 0.7, 0.9
poles:
p1 = −0.9
Yes, the noise whitening ﬁlter is stable, since
p1
 < 1.
b. The power spectral density is
SXX(f) = σ2
W
H(f)
2 =
1 + 0.9e−j2πf2
|1 −1.6ej2πf + 0.63ej4πf|2
since σ2
W = 1. A plot of SXX(f) is shown in Figure 14.4.
□
□
EXAMPLE 14.3
An ARMA process has an autocorrelation RXX(m), where its z-transform is
given as
SXX(z) = 9

z −1
3

(z −3)

z −1
2

(z −2),
1
2 < |z| < 2
a. Determine the ﬁlter H(z) for generating an output sequence x(n) from a
white noise input sample sequence w(n). Is H(z) unique? Explain.
b. Implement the ﬁlter H(z) to generate an output sequence x(n) for 0 ≤n ≤
10,000, when the input sequence w(n) is a sample sequence from a white,
zero-mean Gaussian noise sequence with unit variance. Compute and plot
the autocorrelation estimates ˆRXX(m) for |m| ≤50 and the power spectral
density estimates ˆSXX(f) for |f| ≤1
2 from the output sequence x(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

694
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
c. Determine a stable linear whitening ﬁlter for the sequence generated in
part (b). Pass the sequence x(n) generated in part (b) through the noise-
whitening ﬁlter for 0 ≤n ≤10,000. Compute the autocorrelation ˆRYY(m)
for |m| ≤50 and the power spectral density ˆSYY(f) for |f| ≤1
2, where y(n)
is the output of the noise-whitening ﬁlter. Plot ˆRYY(m) and ˆSYY(f), and
comment on the results.
Solution
a. The complex PSD can be put in the form
SXX(z) = 27
2

1 −1
3z−1 
1 −1
3z


1 −1
2z−1 
1 −1
2z

For a stable system, denominator

1 −1
2z−1

must be chosen. However,
either numerator factor may be used:
H(z) =

1 −1
3z−1

1 −1
2z−1



Min. phase
or
H(z) =

1 −1
3z


1 −1
2z−1
b. We use the minimum phase system function H(z) to generate the sequence
x(n) = 1
2x(n −1) + w(n) −1
3w(n −1)
We compute the unbiased autocorrelation sequence estimate
ˆRXX(m) =
1
N −m
N−1−m

n=0
x(n)x(n −m),
|m| ≤50
and from it the estimate of the power spectral density
ˆSXX(f) =
N

m=−N
ˆRXX(m)e−j2πfm
Graphs of ˆRXX(m) and ˆSXX(f) are shown in Figure 14.5.
c. The stable whitening ﬁlter is the minimum phase system with system
function
A(z) = 1 −1
2z−1
1 −1
3z−1
Thus we generate the sequence
y(n) = 1
3y(n −1) + x(n) −1
2x(n −1)
and compute the estimates ˆRYY(m) and ˆSYY(f), which are illustrated in
Figure 14.6.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
695
Lag m
–10
–5
0
5
10
RXX(m)
0
1
ACRS Estimate (Coloring)
Frequency f, cyc/sam
–0.5
0
0.5
SXX(f)
0.79
1.75
PSD Estimate (Coloring)
FIGURE 14.5
Plots of the estimated autocorrelation sequence ˆRXX(m) and PSD
ˆSXX(f) in Example 14.3(b)
Lag m
–10
–5
0
5
10
RYY(m)
0
1
ACRS Estimate (Whitening)
Frequency f, cyc/sam 
–0.5
0
0.5
SYY(f)
0
1
PSD Estimate (Whitening)
FIGURE 14.6
Plots of the estimated autocorrelation sequence ˆRYY(m) and PSD
ˆSYY(f) in Example 14.3(c)
14.1.2 RELATIONSHIPS BETWEEN THE FILTER PARAMETERS AND
THE AUTOCORRELATION SEQUENCE
When the power spectral density of the stationary random process is
a rational function, there is a basic relationship that exists between
the autocorrelation sequence RXX(m) and the parameters ak and bk of
the linear ﬁlter H(z) that generates the process by ﬁltering the white
noise sequence W(n). This relationship may be obtained by multiplying
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

696
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
the diﬀerence equation in (14.11) by X∗(n −m) and taking the expected
value of both sides of the resulting equation. Thus we have
E

X(n)X∗(n −m)

= −
p

k=1
akE

X(n −k)X∗(n −m)

+
q

k=0
bkE

W(n −k)X∗(n −m)

(14.14)
Hence
RXX(m) = −
p

k=1
akRXX(m −k) +
q

k=0
bkRWX(m −k)
(14.15)
where RWX(m) is the cross-correlation sequence between W(n) and X(n).
The cross-correlation RWX(m) is related to the ﬁlter impulse response.
That is,
RWX(m) = E

X∗(n)W(n + m)

= E
 ∞

k=0
h(k)W∗(n −k)W(n + m)

= σ2
Wh(−m)
(14.16)
where, in the last step, we have used the fact that the sequence W(n) is
white. Hence
RWX(m) =

0,
m > 0
σ2
Wh(−m),
m ≤0
(14.17)
By combining (14.17) with (14.15), we obtain the desired relationship,
RXX(m) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
−
p

k=1
akRXX(m −k),
m > q
−
p

k=1
akRXX(m −k) + σ2
W
1−m

k=0
h(k)bk+m,
0 ≤m ≤q
R∗
XX(−m),
m < 0
(14.18)
This represents a nonlinear relationship between RXX(m) and the param-
eters ak, bk. The relationship in (14.18) applies, in general, to the ARMA
process. For an AR process, (14.18) simpliﬁes to
RXX(m) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
−
p

k=1
akRXX(m −k),
m > 0
−
p

k=1
akRXX(m −k) + σ2
W,
m = 0
R∗
XX(−m),
m < 0
(14.19)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
697
Thus we have a linear relationship between RXX(m) and the ak parame-
ters. These equations are called the Yule–Walker equations and may be
expressed in the matrix form
⎡
⎢⎢⎢⎣
RXX(0)
RXX(−1)
RXX(−2)
. . .
RXX(−p)
RXX(1)
RXX(0)
RXX(−1)
. . . RXX(−p + 1)
...
...
...
...
...
RXX(p) RXX(p −1) RXX(p −2) . . .
RXX(0)
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
1
a1
a2
...
ap
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
σ2
W
0
0
...
0
⎤
⎥⎥⎥⎥⎥⎦
(14.20)
This correlation matrix is Toeplitz and, hence, can be eﬃciently inverted
by use of the algorithms described in Section 14.3.
Finally, by setting ak = 0, 1 ≤k ≤p, and h(k) = bk, 0 ≤k ≤q,
in (14.18), we obtain the relationship for the autocorrelation sequence in
the case of a MA process, namely,
RXX(m) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
q−m

k=0
bkbk+m,
0 ≤m ≤q
0,
m > q
R∗
XX(−m),
m < 0
(14.21)
□
EXAMPLE 14.4
A moving average (MA) process is described by the diﬀerence equation
X(n) = W(n) −2W(n −1) + W(n −2)
(14.22)
where W(n) is a white noise sequence with variance σ2
W = 1.
a. Determine and plot the autocorrelation RXX(m) and power spectral density
SXX(f).
b. Generate the output sequence x(n), 0 ≤n ≤10,000, when w(n) is a sample
sequence of a white, zero-mean Gaussian noise sequence with unit variance.
Compute and plot the autocorrelation estimates ˆRXX(m) of the sequence
X(n) for |m| ≤50 and the corresponding power spectral density ˆSXX(f) for
f
 ≤1
2. Compare these plots with those in part (a) and comment on any
similarities and diﬀerences.
Solution
a. From (14.22) and using E

W(n)

= 0, we have E

X(n)

= 0. Hence we
obtain
RXX(m) = σ2
W
q

k=0
bkbk+m,
0 ≤m ≤q
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

698
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
With q = 2, b0 = 1, b1 = −2, b2 = 1 and from (14.21), the autocorrelation
values are given by
RXX(0) = σ2
W
2

k=0
b2
k = 6σ2
W = 6,
RXX(±1) = σ2
W
1

k=0
bkbk+1 = −4σ2
W = −4,
RXX(±2) = σ2
W
0

k=0
bkbk+2 = σ2
W = 1, and
RXX(m) = 0,
|m| ≥3
The PSD is given by
SXX(f) =
2

m=−2
RXX(m)e−j2πfm
= (1)e−j2πf(−2) + (−4)e−j2πf(−1) + (6)e−j2πf(0)
+ (−4)e−j2πf(1) + (1)e−j2πf(2)
= 6 −8 cos(2πf) + 2 cos(4πf)
The plots of RXX(m) and SXX(f) are shown in Figure 14.7.
b. Using the diﬀerence equation
x(n) = w(n) −2w(n −1) + w(n −2)
we compute x(n) for 0 ≤n ≤10,000 and estimate the autocorrelation and
power spectral density. These functions are plotted in Figure 14.8. They
closely resemble the function RXX(m) and SXX(f) computed in part (a). □
Lag m
–5
0
5
RXX(m)
–4
0
6
Autocorrelation Sequence
Frequency f, cyc/sam
–0.5
0
0.5
SXX(f)
0
16
Power Spectral Density
FIGURE
14.7
Plots
of
autocorrelation
RXX(m)
and
PSD
SXX(f)
in
Example 14.4(a)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Innovations Representation of a Stationary Random Process
699
Lag m
–5
0
5
RXX(m)
–4
0
1
6
Autocorrelation Estimate
Frequency f, cyc/sam
–0.5
0
0.5
SXX(f)
0
16
PSD Estimate
FIGURE 14.8
Plots of the estimated autocorrelation ˆRXX(m) and PSD ˆSXX(f)
in Example 14.4(b)
□
EXAMPLE 14.5
An AR process is described by the diﬀerence equation
X(n) = X(n −1) −0.6X(n −2) + W(n)
(14.23)
where W(n) is a zero-mean, white noise process with variance σ2
W.
a. Use the Yule–Walker equations to solve for the values of the autocorrelations
RXX(m).
b. Determine the z-transform of the autocorrelation RXX(m) and plot the power
spectral density SXX(f) for
f
 ≤1
2.
c. Generate the output sequence x(n), 0 ≤n ≤10,000, when w(n) is a sample
sequence from a white, zero-mean Gaussian noise process with unit variance.
Compute and plot the autocorrelation estimates ˆRXX(m) of the sequence
x(n) for |m| ≤50 and the corresponding power spectral density estimate
ˆSXX(f) for
f
 ≤1
2. Compare ˆSXX(f) with SXX(f) plotted in part (b).
Solution
a. The Yule–Walker equations are given by
⎡
⎣
RXX(0) RXX(1) RXX(2)
RXX(1) RXX(0) RXX(1)
RXX(2) RXX(1) RXX(0)
⎤
⎦
⎡
⎣
1
−1
0.6
⎤
⎦=
⎡
⎣
1
0
0
⎤
⎦σ2
W
or
⎡
⎣
1
−1 0.6
−1 1.6
0
0.6 −1
1
⎤
⎦
⎡
⎣
RXX(0)
RXX(1)
RXX(2)
⎤
⎦=
⎡
⎣
1
0
0
⎤
⎦σ2
W
Solving,
RXX(0) = 2.5641σ2
W,
RXX(1) = 1.6026σ2
W,
RXX(2) = 0.06416σ2
W
For m ≥3, we have
RXX(m) = RXX(m −1) −0.6RXX(m −2)
and ﬁnally, for m < 0, we have
RXX(m) = RXX(−m)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

700
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Frequency f, cyc/sam
–0.5
–0.138
0
0.138
0.5
SXX(f)
0
11
Power Spectral Density
FIGURE 14.9
Plot of power spectral density SXX(f) in Example 14.5(b)
b. The z-transform of the autocorrelation RXX(m), from (14.5), is
SXX(z) =
∞

m=−∞
RXX(m)z−m = σ2
WH(z)H(z−1)
= σ2
W
1
(1 −z−1 + 0.6z−2)(1 −z + 0.6z2)
and SXX(f) is
SXX(f) =
σ2
W
1 −e−j2πf + 0.6e−j4πf2
The PSD SXX(f) is plotted in Figure 14.9.
c. The estimated autocorrelation
ˆRXX(m) and the power spectral density
ˆSXX(f) are plotted in Figure 14.10. The estimate ˆSXX(f) is a close approxi-
mation to SXX(f) shown in Figure 14.9.
□
Lag m
–20
–10
0
10
20
RXX(m)
–0.94
0
2.57
Autocorrelation Estimate
Frequency f, cyc/sam
–0.5
–0.138 0 0.138
0.5
SXX(f)
0
10
PSD Estimate
FIGURE 14.10
Plots of the estimated autocorrelation ˆRXX(m) and PSD ˆSXX(f)
in Example 14.5(c)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
701
14.2 FORWARD AND BACKWARD LINEAR PREDICTION
Linear prediction is an important topic in digital signal processing, with
many practical applications. In this section, we consider the problem of
linearly predicting the value of a stationary random process either forward
or backward in time. This formulation leads to lattice ﬁlter structures, and
to some interesting connections to parametric signal models.
14.2.1 FORWARD LINEAR PREDICTION
Let us begin with the problem of predicting a future value of a stationary
random process X(n) from observation of past values of the process. We
will denote its observed values by x(n). In particular, we consider the one-
step forward linear predictor, which forms the prediction of the value x(n)
by a weighted linear combination of the past values x(n −1), x(n −2),
. . . , x(n −p). Hence the linearly predicted values of X(n) is the process
ˆX(n) with observed values given by
ˆx(n) = −
p

k=1
ap(k)x(n −k)
(14.24)
where the −ap(k) represent the weights in the linear combination. These
weights are called the prediction coefficients of the one-step forward lin-
ear predictor of order p. (The negative sign in the deﬁnition of ˆx(n) is
for mathematical convenience and conforms with current practice in the
technical literature.)
The diﬀerence between the value of X(n) and the predicted value
ˆX(n) is called the forward prediction error process, denoted by Fp(n),
with observed values given by
fp(n) = x(n) −ˆx(n)
= x(n) +
p

k=1
ap(k)x(n −k)
(14.25)
We view linear prediction as being equivalent to linear ﬁltering when
the predictor is embedded in the linear ﬁlter, as shown in Figure 14.11.
This is called a prediction-error filter with input sequence x(n) and output
sequence fp(n). An equivalent realization for the prediction-error ﬁlter is
shown in Figure 14.12. This realization is a direct form FIR ﬁlter with
system function
Ap(z) =
p

k=0
ap(k)z−k
(14.26)
where, by deﬁnition, ap(0) = 1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

702
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
x(n)
z−1
Forward
Linear
Predictor
+
+
−
fp (n)
x(n −1)
ˆx(n)
FIGURE 14.11
Forward linear prediction
There is another realization of an FIR ﬁlter that takes the form of a
lattice structure. To describe this structure and to relate it to the direct
form FIR ﬁlter structure, let us begin with a predictor of order p = 1.
The output of such a ﬁlter is
f1(n) = x(n) + a1(1)x(n −1)
(14.27)
This output can be obtained from the single-stage lattice ﬁlter, illustrated
in Figure 14.13, by exciting both inputs by x(n) and selecting the output
from the top branch. Thus the output is exactly that given by (14.27)
if we select K1 = a1(1). The parameter K in the lattice ﬁlter is called a
reflection coefficient.
Next, let us consider a predictor of order p = 2. In this case, the
output of the direct form FIR ﬁlter is
f2(n) = x(n) + a2(1)x(n −1) + a2(2)x(n −2)
(14.28)
By cascading two lattice stages as shown in Figure 14.14, it is possible
to obtain the same output as (14.28). Indeed, the two outputs from the
ﬁrst stage are
f1(n) = x(n) + K1x(n −1)
g1(n) = K∗
1x(n) + x(n −1)
(14.29)
x(n)
z–1
z–1
z–1
z–1
+
fp(n)
1
ap(1)
ap(2)
ap(3)
ap(p –1)
ap(p)
FIGURE 14.12
Prediction-error ﬁlter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
703
x(n)
f0(n)
g0(n)
g0(n – 1)
f1(n)
g1(n)
z −1
+
+
K1
K*1
FIGURE 14.13
Single-stage lattice ﬁlter
The two outputs from the second stage are
f2(n) = f1(n) + K2g1(n −1)
g2(n) = K∗
2f1(n) + g1(n −1)
(14.30)
If we focus our attention on f2(n) and substitute for f1(n) and g1(n −1)
from (14.29) into (14.30), we obtain
f2(n) = x(n) + K1x(n −1) + K2[K∗
1x(n −1) + x(n −2)]
= x(n) + (K1 + K∗
1K2)x(n −1) + K2x(n −2)
(14.31)
Now (14.31) is identical to the output of the direct form FIR ﬁlter given
by (14.28), if we equate the coeﬃcients. Thus
a2(2) = K2,
a2(1) = K1 + K∗
1K2
(14.32)
or equivalently,
K2 = a2(2),
K1 = a1(1)
(14.33)
By continuing this process, one can easily demonstrate by induction
the equivalence between an mth-order direct form FIR ﬁlter and an mth-
order or m-stage lattice ﬁlter. The lattice ﬁlter is generally described by
the following set of order-recursive equations:
f0(n) = g0(n) = x(n)
fm(n) = fm−1(n) + Kmgm−1(n −1),
m = 1, 2, . . . , p
gm(n) = K∗
mfm−1(n) + gm−1(n −1),
m = 1, 2, . . . , p
(14.34)
+
+
z−1
z−1
+
+
x(n)
f0(n)
f1(n)
g1(n)
g0(n)
g2(n)
f2(n)
K*1
K*2
K2
K1
FIGURE 14.14
Two-stage lattice ﬁlter
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

704
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
First
Stage
Second
Stage
pth
Stage
z−1
+
+
K*m
Km
x(n)
fp(n)
fm(n)
fm–1(n)
gm–1(n)
gm(n)
f2(n)
g2(n)
f1(n)
g1(n)
f0(n)
g0(n)
gp(n)
FIGURE 14.15
p-stage lattice ﬁlter
Then the output of the p-stage lattice ﬁlter is identical to the output of
a p-order direct form FIR ﬁlter. Figure 14.15 illustrates a p-stage lattice
ﬁlter in block-diagram form, along with a typical stage that shows the
computations given by (14.34).
As a consequence of the equivalence between the direct form prediction-
error FIR ﬁlter and the FIR lattice ﬁlter, the output of the p-stage lattice
ﬁlter is expressed as
fp(n) =
p

k=0
ap(k)x(n −k),
ap(0) = 1
(14.35)
Since (14.35) is a convolution sum, the z-transform relationship is
Fp(z) = Ap(z)X(z)
(14.36)
or equivalently,
Ap(z) = Fp(z)
X(z) = Fp(z)
F0(z)
(14.37)
The mean-square value of the forward linear prediction error process
Fp(n) is
Ef
p = E
Fp(n)
2
= RXX(0) + 2Re
 p

k=1
a∗
p(k)RXX(k)

+
p

k=1
p

ℓ=1
a∗
p(ℓ)ap(k)RXX(ℓ−k)
(14.38)
Ef
p is a quadratic function of the predictor coeﬃcients, and its minimiza-
tion leads to the set of linear equations
RXX(ℓ) = −
p

k=1
ap(k)RXX(ℓ−k),
ℓ= 1, 2, . . . , p
(14.39)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
705
These are called the normal equations for the coeﬃcients of the linear
predictor. The minimum mean-square prediction error is simply
min

Ef
p

≡Ef
p = RXX(0) +
p

k=1
ap(k)RXX(−k)
(14.40)
In the following section, we extend the above development to the
problem of predicting the value of a time series in the opposite direction,
namely, backward in time.
□
EXAMPLE 14.6
Consider the AR(3) process generated by the equation
x(n) = 14
24x(n −1) +
9
24x(n −2) −
1
24x(n −3) + w(n)
where w(n) is a sample sequence of a white noise sequence with variance σ2
W.
a. Determine the coeﬃcient of the optimum p = 3 linear predictor.
b. Determine the autocorrelation sequence RXX(m), 0 ≤m ≤5.
Solution
a. The coeﬃcients of the optimum p = 3 are obtained using
A(z) =
1
1 −14
24z−1 −
9
24z−2 +
1
24z−3
Thus the optimum whitening ﬁlter of order 3 will have z-transform
B(z) = 1 −14
24z−1 −
9
24z−2 +
1
24z−3
b. For an AR process, we have
RXX(m) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
−
p

k=1
akRXX(m −k),
m > 0
−
p

k=1
akRXX(m −k) + σ2
W,
m = 0
R∗
XX(−m),
m < 0
Since we know the {ak}, we can solve for RXX, m = 0, 1, 2, 3. Then we can
obtain RXX(m) for m > 3, by the previous recursions. Start with
⎡
⎢⎢⎢⎢⎣
1 −14
24 −9
24
1
24
−14
24
15
24
1
24
0
−9
24 −13
24
1
0
1
24 −9
24 −14
24
1
⎤
⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎣
RXX(0)
RXX(1)
RXX(2)
RXX(3)
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
σ2
W
0
0
0
⎤
⎥⎥⎥⎥⎦
which gives
RXX(0) = 4.9377σ2
W,
RXX(1) = 4.3287σ2
W,
RXX(2) = 4.1964σ2
W,
RXX(3) = 3.8654σ2
W
Using RXX(m) = −p
k=1 akRXX(m−k), m > 0, we obtain RXX(4) = 3.65σ2
W
and RXX(5) = 3.46σ2
W.
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

706
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
14.2.2 BACKWARD LINEAR PREDICTION
Let us assume that we have the data sequence x(n), x(n −1), . . . ,
x(n −p + 1) from a stationary random process X(n) and we wish to
predict the value x(n −p) of the process. In this case, we employ a
one-step backward linear predictor of order p. Hence
ˆx(n −p) = −
p−1

k=0
bp(k)x(n −k)
(14.41)
The diﬀerence between the value x(n −p) and the estimate ˆx(n −p) is
called the backward prediction error, denoted by gp(n),
gp(n) = x(n −p) +
p−1

k=0
bp(k)x(n −k)
=
p

k=0
bp(k)x(n −k),
bp(p) = 1
(14.42)
The backward linear predictor may be realized either by a direct form
FIR ﬁlter structure similar to the structure shown in Figure 14.11, or as
a lattice structure. The lattice structure shown in Figure 14.15 provides
the backward as well as the forward linear predictor. To prove this point,
let us consider the output of this lattice ﬁlter from the lower branch. This
output is given as
g1(n) = K∗
1x(n) + x(n −1)
(14.43)
Hence the weighting coeﬃcient of the backward predictor is b1(0) = K∗
1.
In the two-stage lattice shown in Figure 14.14, the output of the
second stage from the bottom branch is
g2(n) = K∗
2f1(n) + g1(n −1)
(14.44)
If we substitute from (14.29) for f1(n) and g1(n −1), we obtain
g2(n) = K∗
2x(n) + (K∗
1 + K1K∗
2)x(n −1) + x(n −2)
(14.45)
Hence the weighting coeﬃcients in the backward linear predictor are iden-
tical to the (complex conjugate) coeﬃcients for the forward linear predic-
tor, but they occur in reverse order. Thus we have
bp(k) = a∗
p(p −k),
k = 0, 1, . . . , p
(14.46)
In the z-domain, the convolution sum in (14.42) becomes
Gp(z) = Bp(z)X(z)
(14.47)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
707
or equivalently,
Bp(z) = Gp(z)
X(z) = Gp(z)
G0(z)
(14.48)
where Bp(z) represents the system function of the FIR ﬁlter with coeﬃ-
cients bp(k).
Since bp(k) = a∗
p(p −k), Bp(z) is related to Ap(z) as follows:
Bp(z) =
p

k=0
bp(k)z−k =
p

k=0
a∗
p(p −k)z−k
= z−p
p

k=0
a∗
p(k)zk = z−pA∗
p(z−1)
(14.49)
The relationship in (14.49) implies that the zeros of the FIR ﬁlter with
system function Bp(z) are simply the (conjugate) reciprocals of the zeros
of Ap(z). Hence Bp(z) is called the reciprocal or reverse polynomial of
Ap(z).
Finally, let us consider the minimization of the mean-square error
in a backward linear predictor. The backward prediction-error process is
denoted by Gp(n) with sample sequence gp(n) given by
gp(n) = x(n −p) +
p−1

k=0
bp(k)x(n −k)
= x(n −p) +
p

k=1
a∗
p(k)x(n −p + k)
(14.50)
and its mean-square value given by
Eb
p = E
Gp(n)
2
(14.51)
The minimization of Eb
p with respect to the prediction coeﬃcients yields
the same set of linear equations as in (14.39). Hence, the minimum mean-
square error is
min

Eb
p

≡Eb
p = Ef
p
(14.52)
which is given by (14.40).
14.2.3 CONVERSIONS BETWEEN FIR FILTER COEFFICIENTS AND
LATTICE REFLECTION COEFFICIENTS
Now that we have established these interesting relationships between the
direct form FIR ﬁlter and the FIR lattice ﬁlter, let us return to the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

708
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
recursive lattice equations in (14.34) and transform them to the z-domain.
Thus we have
F0(z) = G0(z) = X(z)
(14.53a)
Fm(z) = Fm−1(z) + Kmz−1Gm−1(z),
m = 1, 2, . . . , p
(14.53b)
Gm(z) = K∗
mFm−1(z) + z−1Gm−1(z),
m = 1, 2, . . . , p
(14.53c)
If we divide each equation by X(z), we obtain the desired results in the
form
A0(z) = B0(z) = 1
(14.54a)
Am(z) = Am−1(z) + Kmz−1Bm−1(z),
m = 1, 2, . . . , p
(14.54b)
Bm(z) = K∗
mAm−1(z) + z−1Bm−1(z),
m = 1, 2, . . . , p
(14.54c)
Thus a lattice ﬁlter is described in the z-domain by the matrix equation

Am(z)
Bm(z)

=

1
Kmz−1
K∗
m
z−1
 
Am−1(z)
Bm−1(z)

(14.55)
The relations in (14.54) for Am(z) and Bm(z) allow us to obtain the direct
form FIR ﬁlter coeﬃcients am(k) from the reﬂection coeﬃcients Km, and
vice versa. We illustrate the procedure in the following example.
□
EXAMPLE 14.7
Given a three-stage lattice ﬁlter with coeﬃcients K1 =
1
4, K2 =
1
2, K3 =
1
3,
determine the FIR ﬁlter coeﬃcients for the direct form structure.
Solution
We solve the problem recursively, beginning with (14.54) for m = 1. Thus we
have
A1(z) = A0(z) + K1z−1B0(z) = 1 + K1z−1
= 1 + 1
4z−1
Hence the coeﬃcients of an FIR ﬁlter corresponding to the single-stage lattice
are a1(0) = 1, a1(1) = K1. Since Bm(z) is the reverse polynomial of Am(z),
we have
B1(z) = 1
4 + z−1
Next, we add the second stage to the lattice. For m = 2, (14.54) yields
A2(z) = A1(z) + K2z−1B1(z)
= 1 + 3
8z−1 + 1
2z−2
Hence the FIR ﬁller parameters corresponding to the two-stage lattice are
a2(0) = 1, a2(1) = 3
8, and a2(2) = 1
2. Also,
B2(z) = 1
2 + 3
8z−1 + z−2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
709
Finally, the addition of the third stage to the lattice results in the polynomial
A3(z) = A2(z) + K3z−1B2(z)
= 1 + 13
24z−1 + 5
8z−2 + 1
3z−3
Consequently, the desired direct form FIR ﬁlter is characterized by the coeﬃ-
cients
a3(0) = 1,
a3(1) = 13
24,
a3(2) = 5
8,
a3(3) = 1
3
□
As this example illustrates, the lattice structure with parameters K1,
K2, . . ., Kp corresponds to a class of p direct form FIR ﬁlters with system
functions A1(z), A2(z), . . . , Ap(z). It is interesting to note that a charac-
terization of this class of p FIR ﬁlters in direct form requires p(p + 1)/2
ﬁlter coeﬃcients. In contrast, the lattice form characterization requires
only the p reﬂection coeﬃcients {Ki}. The reason that the lattice pro-
vides a more compact representation for the class of p FIR ﬁlters is that
appending stages to the lattice does not alter the parameters of the previ-
ous stages. On the other hand, appending the pth stage to a lattice with
(p −1) stages results in an FIR ﬁlter with system function Ap(z) that has
coeﬃcients totally diﬀerent from the coeﬃcients of the lower-order FIR
ﬁlter with system function Ap−1(z).
A formula for recursively determining the ﬁlter coeﬃcients ap(k),
1 ≤k ≤p, can be derived easily from polynomial relationships (14.54).
We have
Am(z) = Am−1(z) + Kmz−1Bm−1(z)
(14.56a)
m

k=0
am(k)z−k =
m−1

k=0
am−1(k)z−k + Km
m−1

k=0
a∗
m−1(m −1 −k)z−(k+1)
(14.56b)
By equating the coeﬃcients of equal powers of z−1, and recalling that
am(0) = 1 for m = 1, 2, . . . , p, we obtain the desired recursive equation
for the FIR ﬁlter coeﬃcients in the form
am(0) = 1
am(m) = Km
...
...
(14.57)
am(k) = am−1(k) + Kma∗
m−1(m −k)
= am−1(k) + am(m)a∗
m−1(m −k),
1 ≤k ≤m −1, m = 1, 2, . . . , p
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

710
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
MATLAB Implementation
The
following
MATLAB
function,
latc2fir, implements (14.56a). Note that the product Kmz−1Bm−1(z)
is obtained by convolving the two corresponding arrays, whereas the
polynomial Bm(z) is obtained by using a fliplr operation on the Am(z)
polynomial.
function [a] = latc2fir(K,a0)
% Lattice form to FIR direct form conversion
% ------------------------------------------
% [a] = latc2fir(K,b0)
%
a = FIR direct form coefficients (prediction coefficients)
%
K = lattice filter coefficients (reflection coefficients)
% a0 = overall gain if \= 1 (optional)
%
if nargin == 1
a0 = 1;
end
p = length(K);
B = 1; A = 1;
for m=1:1:p
A = [A,0]+conv([0,K(m)],B);
B = fliplr(A);
end
a = a0*A;
end
To verify the latc2fir function, consider the lattice reﬂection coef-
ﬁcients used in Example 14.7 above:
>> K = [1/4,1/2,1/3];
>> a = latc2fir(K)
a =
1
13/24
5/8
1/3
Clearly, we obtain the same FIR ﬁlter coeﬃcients.
The formula for conversion from the direct form FIR ﬁlter coeﬃcients
ap(k) to the lattice reﬂection coeﬃcients Ki is also very simple. For the p-
stage lattice, we immediately obtain the reﬂection coeﬃcient Kp = ap(p).
To obtain Kp−1, . . . , K1, we need the polynomials Am(z) for m = p −
1, . . . , 1. From (14.55), we obtain
Am−1(z) = Am(z) −KmBm(z)
1 −|Km|2
,
m = p, . . . , 1
(14.58)
which is just a step-down recursion. Thus we compute all lower-degree
polynomials Am(z) beginning with Ap−1(z) and obtain the desired lattice
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
711
reﬂection coeﬃcients from the relation Km = am(m). We observe that
the procedure works as long as Km ̸= 1 for m = 1, 2 . . . , p −1. From this
step-down recursion for the polynomials, it is relatively easy to obtain a
formula for recursively and directly computing Km, m = p −1, . . . , 1. For
m = p −1, . . . , 1, we have
Km = am(m)
am−1(k) = am(k) −Kmbm(k)
1 −|Km|2
= am(k) −am(m)a∗
m(m −k)
1 −|am(m)|2
(14.59)
which is just the recursion in the Schur–Cohn stability test for the poly-
nomial Am(z).
MATLAB Implementation
Given the coeﬃcients

ap(k)

of the di-
rect form, we can obtain the lattice ﬁlter coeﬃcients

Ki

using (14.58)
and (14.59). This is done by the following MATLAB function, fir2latc.
Note that the equation to compute Bm(z) in (14.58) implies that the
polynomial Bm(z) is a fliplr operation on the Am(z) polynomial.
function [K,a0] = fir2latc(a)
% FIR Direct form to All-Zero Lattice form Conversion
% ---------------------------------------------------
% [K,a0] = fir2latc(b)
%
K = lattice filter coefficients (reflection coefficients)
%
a0 = first coefficient (or gain) of A(z), useful if \= 1
%
a = FIR direct form coefficients (prediction coefficients)
p = length(a)-1;
K = zeros(1,p);
a0 = a(1);
if a0 == 0
error(’a(1) is equal to zero’)
end
A = a/a0;
for m=p+1:-1:2
K(m-1) = A(m);
B = fliplr(A);
A = (A-K(m-1)*B)/(1-K(m-1)*K(m-1));
A = A(1:m-1);
end
end
Finally, the lattice ﬁlter can be implemented using (14.34), which is done
by the latcfilt function, as shown here.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

712
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
function [y] = latcfilter(K,x,a0)
% LATTICE form realization of FIR filters
% ---------------------------------------
% y = latcfilter(K,x,a0)
%
y = output sequence
%
K = LATTICE filter (reflection) coefficient array
%
x = input sequence
%
a0 = Overall Gain (optional)
%
if nargin == 1
a0 = 1;
end
Nx = length(x)-1;
x = a0*x;
p = length(K); %K = K(2:M+1);
fg = [x; [0 x(1:Nx)]];
for m = 1:p
fg = [1,K(m);K(m),1]*fg;
fg(2,:) = [0 fg(2,1:Nx)];
end
y = fg(1,:);
end
The SP Toolbox provides several functions that are similar in func-
tionality to the functions developed in this section. The function K =
tf2latc(a) computes the FIR lattice reﬂection coeﬃcients in array K
given the FIR ﬁlter (or prediction) coeﬃcients in array a, normalized by
a(1). Similarly, the function a = latc2tf(K) produces a general FIR
ﬁlter coeﬃcient array a from the lattice reﬂection coeﬃcients in K with
a(1)=1. Finally, the function [f,g] = latcfilt(K,x) implements the
lattice ﬁlter shown in Figure 14.15. It ﬁlters the input array x with the
FIR lattice coeﬃcients in the array K. The forward lattice ﬁlter output (or
forward prediction error) is in array f, and the backward lattice ﬁlter out-
put (or backward prediction error) is in array g. If |K| ≤1, f corresponds
to the minimum-phase output and g corresponds to the maximum-phase
output (see Section 14.4).
□
EXAMPLE 14.8
An FIR ﬁlter is characterized by its system function
H(z) ≡A3(z) = 1 + 13
24z−1 + 5
8z−2 + 1
3z−3
Determine the lattice coeﬃcients and sketch the lattice ﬁlter structure corre-
sponding to the FIR ﬁlter.
Solution
We follow the step-down procedure given in (14.58).
•
m = 3:
A3(z) = 1 + 13
24z−1 + 5
8z−2 + 1
3z−3,
K3 = 1
3
B3(z) = 1
3 + 5
8z−1 + 13
24z−2 + z−3.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
713
x(n)
z −1
+
+
z −1
+
+
z −1
+
+
1
4
1
4
1
2
1
2
1
3
1
3
g0(n)
g1(n)
f0(n)
f1(n)
f2(n)
f3(n)
g2(n)
g3(n)
FIGURE 14.16
Lattice ﬁlter diagram in Example 14.8
•
m = 2:
A2(z) = A3(z) −K3B3(z)
1 −K2
3
= 1 + 3
8z−1 + 1
2z−2,
K2 = 1
2
B2(z) = 1
2 + 3
8z−1 + z−3.
•
m = 1:
A1(z) = A2(z) −K2B2(z)
1 −K2
2
= 1 + 1
4z−1,
K1 = 1
4
We can also use the fir2latc function:
>> a = [1,13/24,5/8,1/3];
>> [K] = fir2latc(a)
K =
1/4
1/2
1/3
The resulting lattice ﬁlter structure is shown in Figure 14.16.
□
□
EXAMPLE 14.9
The reﬂection coeﬃcient of an FIR lattice ﬁlter are
K1 = 1
2,
K2 = −1
3,
K3 = 1
a. Determine the system function H(z) of the FIR ﬁlter.
b. Determine the zeros of the FIR ﬁlter and sketch the pattern in the z-plane.
c. Repeat when K3 = −1. Comment your results.
Solution
a. The system function is obtained using the following steps:
A1(z) = 1 + K1z−1 = 1 + 1
2z−1,
B1(z) = 1
2 + z−1,
A2(z) = A1(z) + K2B1(z)z−1 = 1 + 1
3z−1 −1
3z−2,
B2(z) = −1
3 + 1
3z−1 + z−2,
H(z) = A3(z) = A2(z) + K3z−1B2(z) = 1 + z−3
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

714
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Real Part
–1
–0.5
0
0.5
1
Imaginary Part
–1
–0.5
0
0.5
1
3
FIGURE 14.17
Pole-zero plot in Example 14.9(b)
Use of the latc2fir function gives
>> K = [1/2,-1/3,1];
>> a = latc2fir(K)
a =
1.0000
0.0000
0.0000
1.0000
b. The zeros are located at z1 = −1 and at z2,3 = e±jπ/3. The pole-zero plot
is shown in Figure 14.17.
c. If K3 = −1, then we have
H(z) = A2(z) −z−1B2(z) = 1 + 2
3z−1 −2
3z−2 −z−3
Use of the latc2fir function gives
>> K = [1/2,-1/3,-1];
>> a = latc2fir(K)
a =
1.0000
0.6667
-0.6667
-1.0000
>> zeros = roots(a)’; mag_zeros = abs(zeros)
mag_zeros =
1.0000
1.0000
1.0000
>> pha_zeros = angle(zeros)/pi
pha_zeros =
0
-0.8136
0.8136
The zeros are then located at z1 = 1, z2,3 = e±j0.8136π. The pole-zero plot
is shown in Figure 14.18.
□
As indicated previously, the recursive equation in (14.59) breaks down
if any of the lattice parameters |Km| = 1. If this occurs, it is indicative that
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Forward and Backward Linear Prediction
715
Real Part
–1
–0.5
0
0.5
1
Imaginary Part
–1
–0.5
0
0.5
1
3
FIGURE 14.18
Pole-zero plot in Example 14.9(c)
the polynomial Am−1(z) has a root located on the unit circle. This was
clearly evident in Figures 14.17 and 14.18. Such a root may be factored
out from Am−1(z), and the iterative process in (14.59) can be carried out
for the reduced-order system.
□
EXAMPLE 14.10
Use the recursive equation (14.59) to determine the FIR lattice ﬁlter parameters
for the AR(3) process given in Example 14.6.
Solution
In this case, p = 3 and a3(3) =
1
24, a3(2) = −9
24, a3(1) = −14
24. The recursive
equation for the reﬂection coeﬃcient is
am−1(k) = am(k) −am(m)a∗(m −k)
1 −|am(m)|2
,
m = p −1, . . . , 1
with Km = am(m).
From these, we obtain the reﬂection coeﬃcient K3 = 0.0417, K2 = −0.3513,
and K1 = −0.8767. For veriﬁcation, we use
>> a = [1,-14/24,-9/24,1/24];
>> K = dir2latc(a)
K =
1.0000
-0.8767
-0.3513
0.0417
□
14.2.4 OPTIMUM REFLECTION COEFFICIENTS FOR THE LATTICE
FORWARD AND BACKWARD PREDICTORS
In Sections 14.2.1 and 14.2.2, we derived the set of linear equations that
provide the predictor coeﬃcients that minimize the mean-square value of
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

716
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
the prediction error. In this section, we consider the problem of optimizing
the reﬂection coeﬃcients in the lattice predictor.
The
forward
prediction
error
process
in
the
lattice
ﬁlter
is
expressed as
Fm(n) = Fm−1(n) + KmGm−1(n −1)
(14.60)
The minimization of E
Fm(n)
2
with respect to the reﬂection coeﬃcient
Km yields the result
Km = −E

Fm−1(n)G∗
m−1(n −1)

E
Gm−1(n −1)
2
(14.61)
or equivalently,
Km = −E

Fm−1(n)G∗
m−1(n −1)


Ef
m−1Eb
m−1
(14.62)
where
Ef
m−1 = Eb
m−1 = E
Gm−1(n −1)
2
= E
Fm−1(n)
2
We observe that the optimum choice of the reﬂection coeﬃcients in
the lattice predictor is the negative of the (normalized) cross-correlation
coeﬃcients between the forward and backward errors in the lattice.3 Since
it is apparent from (14.61) that |Km| ≤1, it follows that the minimum
mean-square value of the prediction error, which may be expressed recur-
sively as
Ef
m =

1 −|Km|2
Ef
m−1
(14.63)
is a monotonically decreasing sequence.
14.2.5 RELATIONSHIP OF AN AR PROCESS TO LINEAR PREDICTION
The parameters of an AR(p) process are intimately related to a predictor
of order p for the same process. To see the relationship, we recall that in an
AR(p) process, the autocorrelation sequence RXX(m) is related to the pa-
rameters ak by the Yule–Walker equations given in (14.19) or (14.20). The
corresponding equations for the predictor of order p are given by (14.39)
and (14.40).
A direct comparison of these two sets of relations reveals that there
is a one-to-one correspondence between the parameters ak of the AR(p)
3The normalized cross-correlation coeﬃcients between the forward and backward
error in the lattice (i.e., −Km) are often called the partial correlation (PARCOR)
coeﬃcients.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
717
process and the predictor coeﬃcients ap(k) of the pth-order predictor. In
fact, if the underlying process x(n) is AR(p), the prediction coeﬃcients of
the pth-order predictor will be identical to ak. Furthermore, the minimum
MSE in the pth-order predictor Ef
p will be identical to σ2
W, the variance
of the white noise process. In this case, the prediction-error ﬁlter is a
noise-whitening ﬁlter that produces the innovations sequence W(n).
□
EXAMPLE 14.11
In Section 14.2.5, we indicated that the noise-whitening ﬁlter Ap(z) for a causal
AR(p) process is a forward linear prediction-error ﬁlter of order p. Show that
the backward linear prediction-error ﬁlter of order p is the noise-whitening ﬁlter
of the corresponding anticausal AR(p) process.
Solution
For the backward linear prediction-error ﬁlter,
ˆx(n−p) = −
p−1

k=0
bp(k)x(n−k)
where
bp(k) = a∗
p(p−k),
k = 0, . . . , p
Thus
ˆx(n −p) = −
p−1

k=0
a∗
p(p −k)x(n −k)
set
p −k = l
= −
p

l=1
a∗
p(l)x(n −p + l) = {x(n −p + 1), . . . , x(n)}
∗{a∗
p(−p), . . . , a∗
p(−1)}
that is, the convolution of {x(n)} with the anticausal sequence {a∗
p(−p), . . . ,
a∗
p(−1)}.
□
14.3 SOLUTION OF THE NORMAL EQUATIONS
In the previous section, we observed that the minimization of the mean-
square value of the forward prediction error resulted in a set of linear
equations for the coeﬃcients of the predictor given by (14.39). These
equations, called the normal equations, may be expressed in the compact
form
p

k=0
ap(k)RXX(ℓ−k) = 0,
ℓ= 1, 2, . . . , p,
ap(0) = 1
(14.64)
The resulting minimum MSE (MMSE) is given by (14.40). If we
augment (14.40) to the normal equations given by (14.64), we obtain
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

718
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
the set of augmented normal equations, which may be expressed as
p

k=0
ap(k)RXX(ℓ−k) =

Ef
p,
ℓ= 0
0,
ℓ= 1, 2, . . . , p
(14.65)
We also noted that if the random process is an AR(p) process, the MMSE
Ef
p = σ2
W. In this section, we describe two computationally eﬃcient algo-
rithms for solving the normal equations. The Levinson–Durbin algorithm,
originally due to Levinson [54] and modiﬁed by Durbin [14], is suitable
for serial processing and has a computation complexity of o(p2). The sec-
ond algorithm, due to Schur [88], also computes the reﬂection coeﬃcients
in o(p2) operations, but with parallel processors the computations can
be performed in o(p) time units. Both algorithms exploit the Toeplitz
symmetry property inherent in the autocorrelation matrix. We begin by
describing the Levinson–Durbin algorithm.
14.3.1 LEVINSON–DURBIN ALGORITHM
The Levinson–Durbin algorithm is a computationally eﬃcient algorithm
for solving the normal equations in (14.64) for the prediction coeﬃcients.
This algorithm exploits the special symmetry in the autocorrelation
matrix
Tp =
⎡
⎢⎢⎢⎣
RXX(0)
R∗
XX(1)
· · · R∗
XX(p −1)
RXX(1)
RXX(0)
· · · R∗
XX(p −2)
...
...
...
RXX(p −1) RXX(p −2) · · ·
RXX(0)
⎤
⎥⎥⎥⎦
(14.66)
Note that Tp(i, j) = Tp(i −j), so that the autocorrelation matrix is a
Toeplitz matrix. Since Tp(i, j) = T ∗
p (j, i), the matrix is also Hermitian.
The key to the Levinson–Durbin solution method, which exploits the
Toeplitz property of the matrix, is to proceed recursively, beginning with
a predictor of order m = 1 (one coeﬃcient), we increase the order re-
cursively, using the lower-order solutions to obtain the solution to the
next-higher order. Thus the solution to the ﬁrst-order predictor, obtained
by solving (14.64), is
a1(1) = −RXX(1)
RXX(0)
(14.67)
and the resulting MMSE is
Ef
1 = RXX(0) + a1(1)RXX(−1)
= RXX(0)

1 −|a1(1)|2
(14.68)
Recall that a1(1) = K1, the ﬁrst reﬂection coeﬃcient in the lattice ﬁlter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
719
The next step is to solve for the coeﬃcients a2(1), a2(2) of the second-
order predictor and express the solution in terms of a1(1). The two equa-
tions obtained from (14.64) are
a2(1)RXX(0) + a2(2)R∗
XX(1) = −RXX(1)
a2(1)RXX(1) + a2(2)R∗
XX(0) = −RXX(2)
(14.69)
By using the solution in (14.67) to eliminate RXX(1), we obtain the
solution
a2(2) = −RXX(2) + a1(1)RXX(1)
RXX(0)

1 −|a1(1)|2
= −RXX(2) + a1(1)RXX(1)
Ef
1
a2(1) = a1(1) + a2(2)a∗
1(1)
(14.70)
Thus we have obtained the coeﬃcients of the second-order predictor.
Again, we note that a2(2) = K2, the second reﬂection coeﬃcient in the
lattice ﬁlter.
Proceeding in this manner, we may express the coeﬃcients of the
mth-order predictor in terms of the coeﬃcients of the (m −1)th-order
predictor. Thus we may write the coeﬃcient vector am as the sum of two
vectors, namely,
am =
⎡
⎢⎢⎢⎣
am(1)
am(2)
...
am(m)
⎤
⎥⎥⎥⎦=
⎡
⎣
am−1
. . .
0
⎤
⎦+
⎡
⎣
dm−1
. . .
Km
⎤
⎦
(14.71)
where am−1 is the predictor coeﬃcient vector of the (m −1)th-order
predictor and the vector dm−1 and the scalar Km are to be determined.
Let us also partition the m × m autocorrelation matrix Tm as
Tm =
Tm−1 Rb∗
m−1
Rbt
m−1 RXX(0)

(14.72)
where Rbt
m−1 = [RXX(m −1)RXX(m −2) . . . RXX(1)] = (Rb
m−1)
t, the
asterisk (*) denotes the complex conjugate, Rt
m denotes the transpose
of Rm, and the superscript b on Rm−1 denotes the vector Rt
m−1 =
[RXX(1)RXX(2) . . . RXX(m −1)], with elements taken in reverse order.
The solution to the equation Tmam = −Rm may be expressed as
Tm−1 Rb∗
m−1
Rbt
m−1 RXX(0)
 am−1
0

+

dm−1
Km
 
= −

Rm−1
RXX(m)

(14.73)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

720
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
This is the key step in the Levinson–Durbin algorithm. From (14.73), we
obtain two equations,
Tm−1am−1 + Tm−1dm−1 + KmRb∗
m−1 = −Rm−1
(14.74)
Rbt
m−1am−1 + Rbt
m−1dm−1 + KmRXX(0) = −RXX(m)
(14.75)
Since Tm−1am−1 = −Rm−1, (14.74) yields the solution
dm−1 = −KmT −1
m−1Rb∗
m−1
(14.76)
But Rb∗
m−1 is just Rm−1 with elements taken in reverse order and conju-
gated; therefore, the solution in (14.76) is simply
dm−1 = Kmab∗
m−1 = Km
⎡
⎢⎢⎢⎣
a∗
m−1(m −1)
a∗
m−1(m −2)
...
a∗
m−1(1)
⎤
⎥⎥⎥⎦
(14.77)
The scalar equation (14.75) can now be used to solve for Km. If we elim-
inate dm−1 in (14.75), we obtain
Km[RXX(0) + Rbt
m−1ab∗
m−1] + Rbt
m−1am−1 = −RXX(m)
Hence
Km = −RXX(m) + Rbt
m−1am−1
RXX(0) + Rbt
m−1ab∗
m−1
(14.78)
Therefore, by substituting the solutions in (14.77) and (14.78)
into (14.71), we obtain the desired recursion for the predictor coeﬃcients
in the Levinson–Durbin algorithm as
am(m) = Km = −RXX(m) + Rbt
m−1am−1
RXX(0) + Rbt
m−1ab∗
m−1
= −RXX(m) + Rbt
m−1am−1
Ef
m−1
(14.79)
am(k) = am−1(k) + Kma∗
m−1(m −k)
= am−1(k) + am(m)a∗
m−1(m −k),
k = 1, 2, . . . , m −1
m = 1, 2, . . . , p
(14.80)
Note that the recursive relation in (14.80) is identical to the recursive rela-
tion in (14.57) for the predictor coeﬃcients, which was obtained from the
polynomials Am(z) and Bm(z). Furthermore, Km is the reﬂection coeﬃ-
cient in the mth-stage of the lattice predictor. This development clearly
illustrates that the Levinson–Durbin algorithm produces the reﬂection co-
eﬃcients for the optimum lattice predictor as well as the coeﬃcients of
the optimum direct form FIR predictor.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
721
Finally, let us determine the expression for the MMSE. For the mth-
order predictor, we have
Ef
m = RXX(0) +
m

k=1
am(k)RXX(−k)
= RXX(0) +
m

k=1
[am−1(k) + am(m)a∗
m−1(m −k)]RXX(−k)
= Ef
m−1[1 −|am(m)|2] = Ef
m−1

1 −|Km|2
m = 1, 2, . . . , p
(14.81)
where Ef
0 = RXX(0). Since the reﬂection coeﬃcients satisfy the property
that |Km| ≤1, the MMSE for the sequence of predictors satisﬁes the
condition
Ef
0 ≥Ef
1 ≥Ef
2 ≥· · · ≥Ef
p
(14.82)
This concludes the derivation of the Levinson–Durbin algorithm for
the solution of the linear equations Tmam = −Rm, for m = 0, 1, . . . , p. We
observe that the linear equations have the special property that the vector
on the right-hand side also appears as a vector in Tm. In the more general
case, where the vector on the right-hand side is some other vector—say,
cm—the set of linear equations can be solved recursively by introducing
a second recursive equation to solve the more general linear equations
Tmbm = cm. The result is a generalized Levinson–Durbin algorithm.
The Levinson–Durbin recursion given by (14.80) require o(m)
multiplications and additions (operations) to go from stage m to stage
m + 1. Therefore, for p stages, it will require of the order of 1 + 2 +
3 + · · · + p(p + 1)/2, or o(p2) operations to solve for the prediction or
reﬂection coeﬃcients, compared with o(p3) operations if we do not exploit
the Toeplitz property of the correlation matrix.
If the Levinson–Durbin algorithm is implemented on a serial com-
puter or signal processor, the required computation time is of the order
of o(p2) time units. On the other hand, if the processing is performed
on a parallel-processing machine utilizing as many processors as neces-
sary to exploit the full parallelism in the algorithm, the multiplications
as well as the additions required to compute (14.80) can be carried out
simultaneously. In that case, the computation can be performed in o(p)
time units. However, the computation in (14.79) for the reﬂection coeﬃ-
cients takes additional time. Certainly the inner products involving the
vectors am−1 and Rb
m−1 can be computed simultaneously by employing
parallel processors. However, the addition of these products cannot be
done simultaneously but, instead, requires o(log p) time units. Hence the
computations in the Levinson–Durbin algorithm, when performed by p
parallel processors, can be accomplished in o(p log p) time units.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

722
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
MATLAB Implementation
The SP Toolbox provides the function
levinson that implements the Levinson–Durbin algorithm. Its invoca-
tion is
[a,E,K] = levinson(Rx,p)
in which the input array Rx contains autocorrelation sequence for positive
lags starting with lag 0. The linear predictor coeﬃcients of order p are
computed in array a along with pth-order prediction error in E and the
corresponding reﬂection coeﬃcients in array K.
□
EXAMPLE 14.12
The autocorrelation sequence of a random process is
RXX(m) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1,
m = 0
−0.5,
m = ±1
0.625,
m = ±2
−0.6875,
m = ±3
0,
otherwise
Use the Levinson–Durbin algorithm to determine the system function Am(z)
for the prediction-error ﬁlter for m = 1, 2, 3, the reﬂection coeﬃcients, and the
corresponding mean sequence prediction error.
Solution
Use the Levinson–Durbin algorithm: For m = 1 we have
a1(1) = −RXX(1)
RXX(0) = K1 = 1
2
Therefore, A1(z) = 1 + 1
2z−1 and E1 = RXX(0)(1 −a2
1(1)) = 3
4. For m = 2,
a2(2) = −RXX(2) + a1(1)RXX(1)
E1
= K2 = −1
2
and
a2(1) = a1(1) + a2(2)a1(1) = 1
4
Therefore, A2(z) = 1 + 1
4z−1 −1
2z−2 and E2 = (1 −a2
2(2))E1 =
9
16. Finally, for
m = 3,
a3(3) = −RXX(3) + a2(1)RXX(2) + a2(2)RXX(1)
E2
= K3 = 1
2
a3(2) = a2(2) + a3(3)a2(1) = −3
8
a3(1) = a2(1) + a3(3)a2(2) = 0
Hence
A3(x) = 1 −3
8z−2 + 1
2z−3
E3 = (1 −a2
3(3))E2 = 27
64 = 0.4219
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
723
We will verify the above results using the levinson function:
>> Rx = [1,-0.5,0.625,-0.6875];
>> [a,E,K] = levinson(Rx)
a =
1.0000
0
-0.3750
0.5000
E =
0.4219
K =
0.5000
-0.5000
0.5000
□
□
EXAMPLE 14.13
Repeat Example 14.12 for an AR process with autocorrelation
RXX(m) = a|m| cos πm
2 ,
0 < a < 1
Solution
From the given autocorrelation RXX(m), we will need ﬁrst four values:
RXX(0) = a|0| cos(0) = 1,
RXX(1) = a|1| cos(π/2) = 0,
RXX(2) = a|2| cos(π) = −a2,
RXX(3) = a|3| cos(3π/2) = 0
Now consider the recursion.
•
m = 1:
a1(1) = −RXX(1)
RXX(0) = 0
⇒
K1 = 0;
A1(z) = 1;
E1 = RXX(0)(1 −a2
1(1)) = RXX(0) = 1
•
m = 2:
a2(2) = −RXX(2) + a1(1)RXX(1)
E1
= a2
a2(1) = a1(1) + a2(2)a1(1) = 0
⇒
K2 = a2;
A2(z) = 1 + a2z−2;
E2 = E1(1 −a2
2(2)) = E1(1 −a4)
= 1 −a4
•
m = 3:
a3(3) = −RXX(3) + a2(1)RXX(2) + a2(2)RXX(1)
E2
= 0
a3(2) = a2(2) + a3(3)a2(1) = a2
a3(1) = a2(1) + a3(3)a2(2) = 0
⇒
K3 = 0;
A3(z) = 1 + a2z−2;
E3 = E2 = 1 −a4
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

724
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
In the following section, we describe another algorithm, due to
Schur [88], that avoids the computation of inner products and, hence, is
more suitable for parallel computation of the reﬂection coeﬃcients.
14.3.2 THE SCHUR ALGORITHM
The Schur algorithm is intimately related to a recursive test for determin-
ing the positive deﬁniteness of a correlation matrix. To be speciﬁc, let us
consider the autocorrelation matrix Tp+1 associated with the augmented
normal equations given by (14.65). From the elements of this matrix, we
form the function
D0(z) = RXX(1)z−1 + RXX(2)z−2 + · · · + RXX(p)z−p
RXX(0) + RXX(1)z−1 + · · · + RXX(p)z−p
(14.83)
and the sequence of functions Dm(z) deﬁned recursively as
Dm(z) =
Dm−1(z) −Dm−1(∞)
z−1[1 −D∗
m−1(∞)Dm−1(z)],
m = 1, 2, . . .
(14.84)
Schur’s theorem states that a necessary and suﬃcient condition for the
correlation matrix to be positive deﬁnite is that |Dm(∞)| < 1 for m =
1, 2, . . . , p.
Let us demonstrate that the condition for positive deﬁniteness of the
autocorrelation matrix Tp+1 is equivalent to the condition that the re-
ﬂection coeﬃcients in the equivalent lattice ﬁlter satisfy the condition
|Km| < 1, m = 1, 2, . . . , p.
First, we note that D0(∞) = 0. Then from (14.84), we have
D1(z) = RXX(1) + RXX(2)z−1 + · · · + RXX(p)z−p+1
RXX(0) + RXX(1)z−1 + · · · + RXX(p)z−p
(14.85)
Hence D1(∞) = RXX(1)/RXX(0). We observe D1(∞) = −K1.
Second, we compute D2(z) according to (14.84) and evaluate the
result at z = ∞. Thus, we obtain
D2(∞) = RXX(2) + K1RXX(1)
RXX(0)(1 −|K1|2)
Again, we observe that D2(∞) = −K2. By continuing this procedure,
we ﬁnd that Dm(∞) = −Km, for m = 1, 2, . . . , p. Hence the condition
|Dm(∞)| for m = 1, 2, . . . , p is identical to the condition |Km| < 1 for m =
1, 2, . . . , p, which ensures the positive deﬁniteness of the autocorrelation
matrix Tp+1.
Since the reﬂection coeﬃcients can be obtained from the sequence of
functions Dm(z), m = 1, 2, . . . , p, we have another method for solving the
normal equations. We call this method the Schur algorithm.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
725
Schur Algorithm Derivation
Let us ﬁrst rewrite Dm(z) as
Dm(z) = Pm(z)
Qm(z),
m = 0, 1, . . . , p
(14.86)
where
P0(z) = RXX(1)z−1 + RXX(2)z−2 + · · · + RXX(p)z−p
(14.87a)
Q0(z) = RXX(0) + RXX(1)z−1 + · · · + RXX(p)z−p
(14.87b)
Since K0 = 0 and Km = −Dm(∞) for m = 1, 2, . . . , p, the recursive Equa-
tion (14.84) implies the following recursive equations for the polynomials
Pm(z) and Qm(z):

Pm(z)
Qm(z)

=

1
Km−1
K∗
m−1z−1 z−1
 
Pm−1(z)
Qm−1(z)

,
m = 1, 2, . . . , p
(14.88)
Thus we have
P1(z) = P0(z) = RXX(1)z−1 + RXX(2)z−2 + · · · + RXX(p)z−p
Q1(z) = z−1Q0(z) = RXX(0)z−1 + RXX(1)z−2 + · · · + RXX(p)z−p−1
(14.89)
and
K1 = −P1(z)
Q1(z)

z=∞
= −RXX(1)
RXX(0)
(14.90)
Next, the reﬂection coeﬃcient K2 is obtained by determining P2(z) and
Q2(z) from (14.88), dividing P2(z) by Q2(z), and evaluating the result at
z = ∞. Thus we ﬁnd that
P2(z) = P1(z) + K1Q1(z)
= [RXX(2) + K1RXX(1)]z−2 + · · · + [RXX(p) + K1RXX(p −1)]z−p
+ K1RXX(p)z−p−1
Q2(z) = z−1[Q1(z) + K∗
1P1(z)]
= [RXX(0) + K∗
1RXX(1)]z−2 + · · · + [RXX(p −1) + K∗
1RXX(p)]z−p−1
+ RXX(p)z−p−2
(14.91)
Thus we observe that the recursive equation in (14.88) is equivalent
to (14.84).
Based on these relationships, the Schur algorithm is described by the
following recursive procedure.
Initialization: Form the 2 × (p + 1) generator matrix
G0 =
 0
RXX(1)
RXX(2)
· · ·
RXX(p)
RXX(0)
RXX(1)
RXX(2)
· · ·
RXX(p)

(14.92)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

726
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
where the elements of the ﬁrst row are the coeﬃcients of P0(Z) and
the elements of the second row are the coeﬃcients of Q0(z).
Step 1. Shift the second row of the generator matrix to the right by one
place and discard the last element of this row, and place a zero in the
vacant position. You will obtain the new generator matrix
G1 =

0
RXX(1)
RXX(2)
· · ·
RXX(p)
0
RXX(0)
RXX(1)
· · ·
RXX(p −1)

(14.93)
The (negative) ratio of the elements in the second column yields the
reﬂection coeﬃcient K1 = −RXX(1)/RXX(0).
Step 2. Multiply the generator matrix by the 2 × 2 matrix
V1 =

1
K1
K∗
1
1

(14.94)
to obtain V1G1:
V1G1 =

0
0
RXX(2) + K1RXX(1) · · · RXX(p) + K1RXX(p −1)
0 RXX(0) + K∗
1RXX(1)
· · ·
· · · RXX(p −1) + K∗
1RXX(p)

(14.95)
Step 3. Shift the second row of V1G1 by one place to the right and, thus,
form the new generator matrix
G2 =

0
0
RXX(2) + K1RXX(1)
· · ·
RXX(p) + K1RXX(p −1)
0
0
RXX(0) + K∗
1RXX(1)
· · ·
RXX(p −2) + K∗
1RXX(p −1)

(14.96)
The negative ratio of the elements in the third column of G2 yields K2.
Steps 2 and 3 are repeated until we have solved for all p reﬂection
coeﬃcients. In general, the 2 × 2 matrix in step m is
Vm =

1
Km
K∗
m
1

(14.97)
and multiplication of Vm by Gm yields VmGm. In Step 3, we shift the
second row of VmGm one place to the right and, thus, we obtain the new
generator matrix Gm+1.
Observe that the shifting operation of the second row in each iteration
is equivalent to multiplication by the delay operator z−1 in the second
recursive equation in (14.88).
Also, note that the division of the polynomial Pm(z) by the polyno-
mial Qm(z) and the evaluation of the quotient at z = ∞are equivalent
to dividing the elements in the (m + 1)th column of Gm. The computa-
tion of the p reﬂection coeﬃcients can be accomplished by use of parallel
processors in o(p) time units. We describe a pipelined architecture for
performing these computations below.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
727
□
EXAMPLE 14.14
An AR(3) process x(n) is characterized by the autocorrelation sequence
RXX(0) = 1, RXX(1) = 1
2, RXX(2) = 1
8, RXX(3) = 1
64
Use the Schur algorithm to determine the three reﬂection coeﬃcients K1, K2,
and K3.
Solution
Following the above steps in the Schur algorithm, we obtain the following.
•
Initialization:
G0 =

0
1
2
1
8
1
64
1
1
2
1
8
1
64

•
Step 1:
G1 =

0
1
2
1
8
1
64
0 1
1
2
1
8

⇒
K1 = −1
2,
V1 =

1
−1
2
−1
2
1

,
V1G1 =

0 0 −1
8 −3
64
0
3
4
7
16
15
128

•
Step 2:
G2 =

0 0 −1
8 −3
64
0 0
3
4
7
16

⇒
K2 = 2
3,
V2 =

1
2
3
2
3 1

,
V2G2 =

0 0 0
47
192
0 0
2
3
13
32

•
Step 3:
G3 =

0 0 0
47
192
0 0 0
2
3

⇒
K3 = −47
128
Hence the reﬂection coeﬃcients are
K1 = −1
2,
K2 = 2
3,
and
K3 = −47
128
□
□
EXAMPLE 14.15
Repeat Example 14.14 for an AR process characterized by the autocorrelation
RXX(m) = a|m| cos πm
2
Solution
The ﬁrst four autocorrelation values are
RXX(0) = 1,
RXX(1) = 0,
RXX(2) = −a2,
and
RXX(3) = 0
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

728
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Again following the steps in the Schur algorithm, we obtain the following.
•
Initialization:
G0 =

0 0 −a2 0
1 0 −a2 0

•
Step 1:
G1 =

0 0 −a2
0
0 1
0
−a2

⇒
K1 = −0
1 = 0,
V1 =

1 0
0 1

,
V1G1 = G1 =

0 0 −a2
0
0 1
0
−a2

•
Step 2:
G2 =

0 0 −a2 0
0 0
1
0

⇒
K2 = a2,
V2 =

1 a2
a2 1

,
V2G2 =

0 0
0
0
0 0 1 −a4 0

•
Step 3:
G3 =

0 0 0
0
0 0 0 1 −a4

⇒
K3 = 0
Hence the reﬂection coeﬃcients are
K1 = 0,
K2 = a2,
and
K3 = 0
as obtained in Example 14.13.
□
Another way of demonstrating the relationship of the Schur algo-
rithm to the Levinson–Durbin algorithm and the corresponding lattice
predictor is to determine the output of the lattice ﬁlter that is obtained
when the input sequence is the correlation sequence

RXX(m), m =
0, 1, . . .

. Thus the ﬁrst input to the lattice ﬁlter is RXX(0), the second
input is RXX(1), and so on; in other words, f0(n) = RXX(n). After the de-
lay in the ﬁrst stage, we have g0(n−1) = RXX(n−1). Hence, for n = 1, the
ratio f0(1)/g0(0) = RXX(1)/RXX(0), which is the negative of the reﬂection
coeﬃcient K1. Alternatively, we may express this relationship as
f0(1) + K1g0(0) = RXX(1) + K1RXX(0) = 0
Furthermore, g0(0) = RXX(0) = Ef
0. At time n = 2, the input to the
second stage is, according to (14.34),
f1(2) = f0(2) + K1g0(1) = RXX(2) + K1RXX(1)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Solution of the Normal Equations
729
and after the unit of delay in the second stage, we have
g1(1) = K∗
1f0(1) + g0(0) = K∗
1RXX(1) + RXX(0)
Now the ratio f1(2)/g1(1) is
f1(2)
g1(1) = RXX(2) + K1RXX(1)
RXX(0) + K∗
1RXX(1) = RXX(2) + K1RXX(1)
Ef
1
= −K2
Hence
f1(2) + K2g1(1) = 0
g1(1) = Ef
1
By continuing in this way, we can show that at the input to the mth
lattice stage the ratio fm−1(m)/gm−1(m−1) = −Km and gm−1(m−1) =
Ef
m−1. Consequently, the lattice ﬁlter coeﬃcients obtained from the Levin-
son algorithm are identical to the coeﬃcients obtained from the Schur
algorithm. Furthermore, the lattice ﬁlter structure provides a method for
computing the reﬂection coeﬃcients in the lattice predictor.
A Pipelined Architecture for Implementing the Schur Algorithm
Kung and Hu [52] developed a pipelined lattice-type processor for imple-
menting the Schur algorithm. The processor consists of a cascade of p
lattice-type stages, where each stage consists of two processing elements
(PEs), which we designate as upper PEs, denoted by A1, A2, . . . , Ap, and
lower PEs, denoted by B1, B2, . . . , Bp, as shown in Figure 14.19.
The PE designated as A1 is assigned the task of performing divi-
sions. The remaining PEs perform one multiplication and one addition
per iteration (one clock cycle). Initially, the upper PEs are loaded with
the elements of the ﬁrst row of the generator matrix G0, as illustrated
in Figure 14.19. The lower PEs are loaded with the elements of the sec-
ond row of the generator matrix G0. The computational process begins
with the division PE A1, which computes the ﬁrst reﬂection coeﬃcient as
K1 = −RXX(1)/RXX(0). The value of K1 is sent simultaneously to all the
PEs in the upper and lower branches.
The second step in the computation is to simultaneously update the
contents of all processing elements. The contents of the upper and lower
PEs are updated as follows.
PE Am:
Am ←Am + K1Bm,
m = 2, 3, . . . , p
PE Bm:
Bm ←Bm + K∗
1Am,
m = 1, 2, . . . , p
The third step involves the shifting of the contents of the upper PEs
one place to the left. Thus, we have
PE Am:
Am−1 ←Am,
m = 2, 3, . . . , p
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

730
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
RXX(1)
RXX(2)
RXX(3)
RXX(p −1)
RXX(p)
RXX(0)
RXX(1)
RXX(2)
RXX(p −2)
RXX(p −1)
A1
A2
A3
Ap−1
Ap
B1
B2
B3
Bp−1
Bp
Ef
m
Km
Km
Km
Km
Km
K *m
Km
K *m
K *m
K *m
K *m
FIGURE
14.19
Pipelined
parallel
processor
for
computing
the
reﬂection
coeﬃcients
At this point, PE A1 contains RXX(2) + K1RXX(1) while PE B1 con-
tains RXX(0) + K∗
1RXX(1). Hence the processor A1 is ready to begin the
second cycle by computing the second reﬂection coeﬃcient K2 = −A1/B1.
The three computational steps beginning with the division A1/B1 are re-
peated until all p reﬂection coeﬃcients are computed. Note that PE B1
provides the minimum mean-square error Ef
m for each iteration.
If τd denotes the time for PE A1 to perform a (complex) division
and τma is the time required to perform a (complex) multiplication and
an addition, the time required to compute all p reﬂection coeﬃcients is
p(τd + τma) for the Schur algorithm.
14.4 PROPERTIES OF THE LINEAR PREDICTION-ERROR FILTERS
Linear prediction-error ﬁlters possess several important properties, which
are described in this section. We begin by demonstrating that the forward
prediction-error ﬁlter is minimum phase.
Minimum Phase Property of the Forward Prediction-Error
Filter
We have already demonstrated that the reﬂection coeﬃcients
Ki are correlation coeﬃcients and, consequently, |Ki| ≤1 for all i. This
condition and the relation Ef
m = (1 −|Km|2)Ef
m−1 may be used to
show that the zeros of the prediction-error ﬁlter are either all inside the
unit circle or all on the unit circle.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Linear Prediction-Error Filters
731
First, we show that if Ef
p > 0, then the zeros |zi| < 1 for every i.
The proof is by induction. Clearly, for p = 1, the system function for the
prediction-error ﬁlter is
A1(z) = 1 + K1z−1
(14.98)
Hence z1 = −K1 and Ef
1 = (1 −|K1|2)Ef
0 > 0. Now, suppose that the
hypothesis is true for p −1. Then, if zi is a root of Ap(z), we have,
from (14.49) and (14.54),
Ap(zi) = Ap−1(zi) + Kpz−1
i
Bp−1(zi)
= Ap−1(zi) + Kpz−p
i
A∗
p−1
 1
zi
 
= 0
(14.99)
Hence
1
Kp
= −
z−p
i
A∗
p−1

1
zi

Ap−1(zi)
≡Q(zi)
(14.100)
We note that the function Q(z) is allpass. In general, an allpass function
of the form
P(z) =
N
!
k=1
zz∗
k + 1
z + zk
,
|zk| < 1
(14.101)
satisﬁes the property that |P(z)| > 1 for |z| < 1, |P(z)| = 1 for |z| = 1,
and |P(z)| < 1 for |z| > 1. Since Q(z) = −P(z)/z, it follows that |zi| < 1
if |Q(z)| > 1. Clearly, this is the case, since Q(zi) = 1/Kp and Ef
p > 0.
On the other hand, suppose that Ef
p−1 > 0 and Ef
p = 0. In this case,
|Kp| = 1 and |Q(zi)| = 1. Since the MMSE is zero, the random process
X(n) is called predictable or deterministic. Speciﬁcally, a purely sinusoidal
random process with sample functions of the form
x(n) =
M

k=1
αkej(nωk+θk)
(14.102)
where the phases θk are statistically independent and uniformly dis-
tributed over (0, 2π), has the autocorrelation
RXX(m) =
M

k=1
α2
kejmωk
(14.103)
and the power density spectrum
SXX(f) =
M

k=1
α2
kδ(f −fk),
fk = ωk
2π
(14.104)
This process is predictable with a predictor of order p ≥M.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

732
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
To demonstrate the validity of this statement, consider passing this
process through a prediction-error ﬁlter of order p > M. The MSE at the
output of this ﬁlter is
Ef
p =
 1/2
−1/2
SXX(f)|Ap(f)|2df
=
 1/2
−1/2
 M

k=1
α2
kδ(f −fk)

|Ap(f)|2df
=
M

k=1
α2
k|Ap(fk)|2
(14.105)
By choosing M of the p zeros of the prediction-error ﬁlter to coincide with
the frequencies fk, the MSE Ef
p can be forced to zero. The remaining p−M
zeros can be selected arbitrarily to be anywhere inside the unit circle.
Finally, we leave it to the reader to prove that if a random process
consists of a mixture of a continuous power spectral density and a discrete
spectrum, the prediction-error ﬁlter must have all its roots inside the
unit circle.
Maximum Phase Property of the Backward Prediction-Error
Filter
The system function for the backward prediction-error ﬁlter of
order p is
Bp(z) = z−pA∗
p(z−1)
(14.106)
Consequently, the roots of Bp(z) are the reciprocals of the roots of the
forward prediction-error ﬁlter with system function Ap(z). Hence, if Ap(z)
is minimum phase, then Bp(z) is maximum phase. However, if the process
X(n) is predictable, all the roots of Bp(z) lie on the unit circle.
Whitening Property
Suppose that the random process X(n) is an
AR(p) stationary random process that is generated by passing white noise
with variance σ2
W through an all-pole ﬁlter with system function
H(z) =
1
1 + 
p
k=1 akz−1
(14.107)
Then the prediction-error ﬁlter of order p has the system function
Ap(z) = 1 +
p

k=1
ap(k)z−k
(14.108)
where the predictor coeﬃcients ap(k) = ak. The response of the prediction-
error ﬁlter is a white noise process W(n). In this case, the prediction-error
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Properties of the Linear Prediction-Error Filters
733
ﬁlter whitens the input process X(n) and is called a whitening ﬁlter, as
previously indicated in Section 14.2.
More generally, even if the input sequence X(n) is not an AR pro-
cess, the prediction-error ﬁlter attempts to remove the correlation among
the signal samples of the input process. As the order of the predictor is
increased, the predictor output ˆX(n) becomes a closer approximation to
X(n) and, hence, the diﬀerence F(n) = ˆX(n) −X(n) approaches white
noise sequence.
□
EXAMPLE 14.16
The z-transform of the autocorrelation RXX(m)—that is, the complex PSD
SXX(z)—of an ARMA process is given as
SXX(z) = σ2
WH(z)H

z−1
= 4σ2
W
9
5 −2z −2z−1
10 −3z−1 −3z
(a) Determine the minimum-phase system function H(z).
(b) Determine the system function H(z) for a mixed-phase stable system.
Solution
(a) The complex PSD can be factored as
SXX(z) = 4σ2
W(5 −2z −2z−1)
9(10 −3z −3z−1)
= 4σ2
W(2 −z−1)(2 −z)
9(3 −z−1)(3 −z)
= σ2
WH(z)H(z−1)
The minimum-phase system H(z) is obtained by choosing the pole and zero
inside the unit circle, or
H(z) =
2
3
 2 −z−1
3 −z−1 =
4
9
 1 −1
2z−1
1 −1
3z−1
(b) The mixed-phase stable system is obtained by choosing the pole inside and
zero outside the unit circle, or
H(z) =
4
3
 1
2 −z−1
3 −z−1 =
2
9
 1 −2z−1
1 −1
3z−1
□
Orthogonality of the Backward Prediction Errors
The backward
prediction error processes Gm(k) from diﬀerent stages in the FIR lattice
ﬁlter are orthogonal. That is,
E

Gm(n)G∗
ℓ(n)

=

0,
0 ≤ℓ≤m −1
Eb
m,
ℓ= m
(14.109)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

734
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
This property is easily proved by substituting for Gm(n) and G∗
ℓ(n)
in (14.109) and carrying out the expectation. Thus
E

Gm(n)G∗
ℓ(n)

=
m

k=0
bm(k)
ℓ

j=0
b∗
ℓ(j)E

X(n −k)X∗(n −j)

=
ℓ

j=0
b∗
ℓ(j)
m

k=0
bm(k)RXX(j −k)
(14.110)
But the normal equations for the backward linear predictor require that
m

k=0
bm(k)RXX(j −k) =

0,
j = 1, 2, . . . , m −1
Eb
m,
j = m
(14.111)
Therefore,
E

Gm(n)G∗
ℓ(n)

=

Eb
m = Ef
m,
m = ℓ
0,
0 ≤ℓ≤m −1
(14.112)
14.5 AR LATTICE AND ARMA LATTICE-LADDER FILTERS
In Section 14.2, we developed the all-zero FIR lattice structure and showed
its relationship to linear prediction. The linear predictor with transfer
function
Ap(z) = 1 +
p

k=1
ap(k)z−k
(14.113)
when excited by an input random process X(n) produces an output that
approaches a white noise sequence as p →∞. On the other hand, if
the input process is an AR(p), then the output of Ap(z) is white. Since
Ap(z) generates an MA(p) when excited by a white noise sequence, the
all-zero lattice is sometimes called an MA lattice. We develop the lattice
structure for the inverse ﬁlter 1/Ap(z), which we call the AR lattice, and
the lattice-ladder structure for an ARMA process.
14.5.1 AR LATTICE STRUCTURE
Let us consider an all-pole system with system function
H(z) =
1
1 + 
p
k=1 ap(k)z−k
(14.114)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

AR Lattice and ARMA Lattice-Ladder Filters
735
The diﬀerence equation for this IIR system is
y(n) = −
p

k=1
ap(k)y(n −k) + x(n)
(14.115)
Now, suppose we interchange the roles of the input and output, that is,
interchange x(n) with y(n), in (14.115). Thus we obtain the diﬀerence
equation
x(n) = −
p

k=1
ap(k)x(n −k) + y(n)
or equivalently,
y(n) = x(n) +
p

k=1
ap(k)x(n −k)
(14.116)
We observe that (14.116) is a diﬀerence equation for an FIR system with
system function Ap(z). Thus an all-pole IIR system can be converted to an
all-zero system by interchanging the roles of the input and output. Based
on this observation, we can obtain the structure of an AR(p) lattice from
an MA(p) lattice by interchanging the input with the output. Since the
MA(p) lattice has y(n) = fp(n) as output and x(n) = f0(n) as input,
we let
x(n) = fp(n)
y(n) = f0(n)
(14.117)
These deﬁnitions dictate that the quantities fm(n) be computed in de-
scending order. This computation can be accomplished by rearranging the
recursive equation for fm(n) in (14.34) and solving for fm−1(n) in terms
of fm(n). Thus we obtain
fm−1(n) = fm(n) −Kmgm−1(n −1),
m = p, p −1, . . . , 1
The equation for gm(n) remains unchanged. The result of these changes
is the set of equations
x(n) = fp(n)
fm−1(n) = fm(n) −Kmgm−1(n −1)
gm(n) = K∗
mfm−1(n) + gm−1(n −1)
y(n) = f0(n) = g0(n)
(14.118)
The corresponding structure for the AR(p) lattice is shown in Figure 14.20.
Note that the all-pole lattice structure has an all-zero path with input
g0(n) and output gp(n), which is identical to the all-zero path in the
MA(p) lattice structure. This is not surprising, since the equation for
gm(n) is identical in the two lattice structures.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

736
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
z 1
z 1
z 1
Input
Output
f0(n) 
 y(n)
x(n) 
 fp(n)
gp(n)
f1(n)
f2(n)
fp
1(n)
Kp
K2
K2
K1
K1
g2(n)
g1(n)
g0(n)
*
*
Kp
*
FIGURE 14.20
Lattice structure for an all-pole AR(p) system
MATLAB Implementation
We observe that the AR(p) and MA(p)
lattice structures are characterized by the same parameters, namely, the
reﬂection coeﬃcients Ki. Consequently, the equations given in (14.57)
and (14.59)—for converting between the system parameters ap(k) in the
direct form realizations of the all-zero system Ap(z) and the lattice pa-
rameters Ki of the MA(p) lattice structure—apply as well to the all-pole
structures. Hence the MATLAB functions fir2latc and latc2fir or
tf2latc and latc2tf discussed in Section 14.2.3 for MA(p) lattice struc-
tures are also applicable to AR(p) lattice structures. In the case of the
tf2latc function, the needed invocation is K = tf2latc(1,a). The im-
plementation of the AR(p) lattice given in (14.118) will be discussed in
the next section.
□
EXAMPLE 14.17
Sketch the lattice ﬁlter obtained in Example 14.14 for generating x(n) from a
white noise excitation.
Solution
The structure is shown in Figure 14.21.
□
w (n)
+
+
+
+
+
+
z−1
z−1
z−1
x(n)
47
128
−47
128
2
3
−2
3
−1
2
1
2
FIGURE 14.21
The AR(3) lattice ﬁlter structure in Example 14.17
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

AR Lattice and ARMA Lattice-Ladder Filters
737
14.5.2 ARMA PROCESSES AND LATTICE-LADDER FILTERS
The all-pole lattice provides the basic building block for lattice-type struc-
tures that implement IIR systems that contain both poles and zeros.
To construct the appropriate structure, let us consider an IIR system
with system function
H(z) =

q
k=0 cq(k)z−k
1 + 
p
k=1 ap(k)z−z = Cq(z)
Ap(z)
(14.119)
Without loss of generality, we assume that p ≥q. This system is described
by the diﬀerence equations
v(n) = −
p

k=0
ap(k)v(n −k) + x(n)
y(n) =
q

k=0
cq(k)v(n −k)
(14.120)
which are obtained by viewing the system as a cascade of an all-pole
system followed by an all-zero system. From (14.120), we observe that
the output y(n) is simply a linear combination of delayed outputs from
the all-pole system.
Since zeros will result from forming a linear combination of previous
outputs, we may carry over this observation to construct a pole-zero sys-
tem by using the all-pole lattice structure as the basic building block. We
have observed that gm(n) in the all-pole lattice may be expressed as a
linear combination of present and past outputs. In fact, the system
Hb(z) ≡Gm(z)
Y (z) = Bm(z)
(14.121)
is an all-zero system. Therefore, any linear combination of gm(n) is also
an all-zero ﬁlter.
Let us begin with an all-pole lattice ﬁlter with coeﬃcients Km, 1 ≤
m ≤p, and add a ladder component by taking as the output a weighted
linear combination of gm(n). The result is a pole-zero ﬁlter that has the
lattice-ladder structure shown in Figure 14.22. Its output is
y(n) =
q

k=0
βkgk(n)
(14.122)
where βk are the parameters that determine the zeros of the system. The
system function corresponding to (14.122) is
H(z) = Y (z)
X(z) =
q

k=0
βk
Gk(z)
X(z)
(14.123)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

738
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
FIGURE 14.22
Lattice ladder structure for pole-zero system
Since X(z) = Fp(z) and F0(z) = G0(z), (14.123) may be expressed as
H(z) =
q

k=0
βk
Gk(z)
G0(z)
F0(z)
Fp(z)
=
1
Ap(z)
q

k=0
βkBk(z)
(14.124)
Therefore,
Cq(z) =
q

k=0
βkBk(z)
(14.125)
This is the desired relationship, which can be used to determine the
weighting coeﬃcients βk.
Given the polynomials Cq(z) and Ap(z), where p ≥q, the reﬂection
coeﬃcients Ki are determined ﬁrst from the coeﬃcients ap(k). By means
of the step-down recursive relation given by (14.58), we also obtain the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

AR Lattice and ARMA Lattice-Ladder Filters
739
polynomials Bk(z), k = 1, 2, . . . , p. Then, the ladder parameters can be
obtained from (14.125), which can be expressed as
Cm(z) =
m−1

k=0
βkBk(z) + βmBm(z)
= Cm−1(z) + βmBm(z)
(14.126)
or equivalently,
Cm−1(z) = Cm(z) −βmBm(z),
m = p, p −1, . . . , 1
(14.127)
By running this recursive relation backward, we can generate all the
lower-degree polynomials, Cm(z), m = p −1, . . . , 1. Since bm(m) = 1,
the parameters βm are determined from (14.127) by setting
βm = cm(m),
m = p, p −1, . . . , 1, 0
(14.128)
This lattice-ladder ﬁlter structure, when excited by a white noise
sequence, generates an ARMA(p, q) process that has a power density
spectrum
SXX(f) = σ2
W
|Cq(f)|2
|Ap(f)|2
(14.129)
and an autocorrelation function that satisﬁes (14.18), where σ2
W is the
variance of the input white noise sequence.
MATLAB Implementation
To obtain a lattice-ladder structure for
a general rational IIR system, we can ﬁrst obtain the lattice coeﬃcients

Km

from Ap(z) using the recursion (14.58) and (14.59). Then we can
solve (14.127) and (14.128) recursively for the ladder coeﬃcients {βm}
to realize the numerator Cq(z). This is done in the MATLAB function
iir2ladr. It can also be used to determine the AR(p) lattice parameters
when the array b is set to b=1.
function [K,beta] = iir2ladr(b,a)
% IIR Direct form to pole-zero Lattice/Ladder form Conversion
% -----------------------------------------------------------
% [K,beta] = iir2ladr(b,a)
%
K = lattice coefficients (reflection coefficients), [K1,...,KN]
%
beta = ladder coefficients, [C0,...,CN]
%
b = numerator polynomial coefficients (deg <= Num deg)
%
a = denominator polynomial coefficients
%
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

740
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
a1 = a(1); a = a/a1; b = b/a1;
q = length(b); p = length(a);
if q > p
error(’
*** length of b must be <= length of a ***’)
end
b = [b, zeros(1,p-q)]; K = zeros(1,p-1);
A = zeros(p-1,p-1); beta = b;
for m = p-1:-1:1
A(m,1:m) = -a(2:m+1)*beta(m+1);
K(m) = a(m+1);
J = fliplr(a);
a = (a-K(m)*J)/(1-K(m)*K(m));
a = a(1:m);
beta(m) = b(m) + sum(diag(A(m:p-1,1:p-m)));
end
It should be noted that to use this function, p ≥q. If q > p, the numerator
Ap(z) should be divided into the denominator Cq(z) using the deconv
function to obtain a proper rational part and a polynomial part. The
proper rational part can be implemented using a lattice-ladder structure,
while the polynomial part is implemented using a direct structure.
To convert a lattice-ladder form into a direct form, we ﬁrst use the
recursive procedure in (14.57) on

Km

coeﬃcients to determine

aq(k)

and then solve (14.126) recursively to obtain

bq(k)

. This is done in the
following MATLAB function, ladr2iir.
function [b,a] = ladr2iir(K,beta)
% Lattice/ladder form to IIR direct form conversion
% -------------------------------------------------
% [b,a] = ladr2iir(K,beta)
%
b = numerator polynomial coefficients
%
a = denominator polymonial coefficients
%
K = lattice coefficients (reflection coefficients)
%
beta = ladder coefficients
%
p = length(K); q = length(beta);
beta = [beta, zeros(1,p-q+1)];
J = 1; a = 1; A = zeros(p,p);
for m=1:1:p
a = [a,0]+conv([0,K(m)],J);
A(m,1:m) = -a(2:m+1);
J = fliplr(a);
end
b(p+1) = beta(p+1);
for m = p:-1:1
A(m,1:m) = A(m,1:m)*beta(m+1);
b(m) = beta(m) - sum(diag(A(m:p,1:p-m+1)));
end
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

AR Lattice and ARMA Lattice-Ladder Filters
741
The lattice-ladder ﬁlter is implemented using (14.34) and (14.122).
This is done in the MATLAB function ladrfilter. It should be noted
that, due to the recursive nature of this implementation along with the
feedback loops, this MATLAB function is neither an elegant nor an eﬃ-
cient method of implementation. It is not possible to exploit MATLAB’s
inherent parallel processing capabilities in implementing this lattice-
ladder structure.
function [y] = ladrfilter(K,beta,x)
% LATTICE/LADDER form realization of IIR filters
% ----------------------------------------------
% [y] = ladrfilter(K,beta,x)
%
y = output sequence
%
K = LATTICE (reflection) coefficient array
% beta = LADDER coefficient array
%
x = input sequence
%
Nx = length(x); y = zeros(1,Nx);
p = length(beta); f = zeros(p,Nx); g = zeros(p,Nx+1);
f(p,:) = x;
for n = 2:1:Nx+1
for m = p:-1:2
f(m-1,n-1) = f(m,n-1) - K(m-1)*g(m-1,n-1);
g(m,n) = K(m-1)*f(m-1,n-1) + g(m-1,n-1);
end
g(1,n) = f(1,n-1);
end
y = beta*g(:,2:Nx+1);
□
EXAMPLE 14.18
Determine the parameters of the lattice-ladder ﬁlter structure for the system
with system function
H(z) = 1 + 1
4z−1 −1
8z−2
1 + 3
8z−1 + 1
2z−2
Solution
From the denominator of H(z), we determine that
B2(z) = 1
2 + 3
8z−1 + z−2
B1(z) = 1
4 + z−1
and that the reﬂection coeﬃcients are K1 =
1
4 and K2 =
1
2. Then from the
numerator of H(z),
C(z) = β0B0(z) + β1B1(z) + β2B2(z)
= β0 + β1
1
4 + z−1

+ β2
1
2 + 3
8z−1 + z−2

1 + 3
8z−1 + 1
2z−2 =

β0 + 1
4β1 + 1
2β2

+

β1 + 3
8β2

z−1 + β2z−2
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

742
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Solving, in a step-down recursion, we obtain β2 = −1
8, β1 = 19
64, and β0 = 253
256.
The MATLAB veriﬁcation is obtained using
>> b = [1,1/4,-1/8]; a = [1,3/8,1/2];
>> [K,beta] = iir2ladr(b,a)
K =
0.2500
0.5000
beta =
0.9883
0.2969
-0.1250
□
□
EXAMPLE 14.19
Convert the following pole-zero IIR system into a lattice-ladder structure:
H(z) =
1 + 2z−1 + 2z−2 + z−3
1 + 13
24z−1 + 5
8z−2 + 1
3z−3
Solution
The MATLAB script is
>> b = [1,2,2,1] a = [1, 13/24, 5/8, 1/3];
>> [K,beta] = iir2ladr(b,a)
K =
0.2500
0.5000
0.3333
beta =
-0.2695
0.8281
1.4583
1.0000
Hence
K1 = 1
4, K2 = 1
5, K3 = 1
3;
and
β0 = −0.2695, β1 = 0.8281, β2 = 1.4583, β3 = 1
The resulting direct form and the lattice-ladder form structures are shown in
Figure 14.23. To check that our lattice-ladder structure is correct, let us compute
the ﬁrst eight samples of its impulse response using both forms.
FIGURE 14.23
IIR system structures in Example 14.19: (a) direct form
(b) lattice-ladder form
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
743
>> [x,n]=impseq(0,0,7)
format long;
hdirect = filter(b,a,x)
hdirect =
Columns 1 through 4
1.00000000000000
1.45833333333333
0.58506944444444
-0.56170428240741
Columns 5 through 8
-0.54752302758488
0.45261700163162
0.28426911049255
-0.25435705167494
>> hladder = ladrfilter(K,C,x)
hladder =
Columns 1 through 4
1.00000000000000
1.45833333333333
0.58506944444444
-0.56170428240741
Columns 5 through 8
-0.54752302758488
0.45261700163162
0.28426911049255
-0.25435705167494
□
We note that the SP toolbox functions discussed earlier can also be
used to obtain IIR (ARMA) lattice-ladder structures or to implement
them. The invocation [K,beta] = tf2latc(b,a) computes the lattice
parameters in array K and the ladder parameters in beta, normalized
by a(1). Note that an error is generated if one or more of the lattice
parameters are exactly equal to 1. Similarly, the invocation [b,a] =
latc2tf(K,beta) computes the numerator and denominator polynomial
coeﬃcients of the system function from K and beta.
Finally, the invocation [f,g] = latcfilt(k,v,x) ﬁlters input in ar-
ray x using the IIR lattice reﬂection coeﬃcients K and ladder coeﬃcients
beta. The forward lattice ﬁlter output (or forward prediction error) is in
array f, and the backward lattice ﬁlter output (or backward prediction
error) is in array g. If |K| ≤1, f corresponds to the minimum-phase output
and g corresponds to the maximum-phase output.
14.6 WIENER FILTERS FOR FILTERING AND PREDICTION
In many practical applications, we are given an input signal x(n), which
consists of the sum of a desired signal s(n) and an undesired noise or inter-
ference w(n), and we are asked to design a ﬁlter that will suppress the un-
desired interference component. In such a case, the objective is to design a
system that ﬁlters out the additive interference while preserving the char-
acteristics of the desired signal s(n). We assume that these signals are sam-
ple sequences of random processes X(n), S(n), and W(n), respectively.4
4According to our notation, lowercase variables, such as x(n), are sample sequences,
while uppercase variables, such as X(n), are random processes. In general discussion
and in ﬁltering equations, we will use lowercase variables; for expectation purposes, we
will use uppercase variables. The distinction between a process and its sample sequence
should be clear from the context.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

744
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Optimum
Linear
Filter
Signal
d(n)
x(n)
s(n)
e(n)
y(n)
w(n)
Noise
FIGURE 14.24
Model for linear estimation problem
In this section, we treat the problem of signal estimation in the
presence of an additive noise disturbance. The estimator is constrained
to be a linear ﬁlter with impulse response h(n), which is designed so
that its output approximates some speciﬁed desired signal process D(n),
with sample sequence d(n). Figure 14.24 illustrates the linear estimation
problem.
The ﬁlter’s input sequence is x(n) = s(n) + w(n) and its output
sequence is y(n)—that is, a sample sequence of the process Y(n). The
diﬀerence between the desired signal and the ﬁlter output is the error
sequence e(n)
△= d(n) −y(n), with the underlying process denoted by
E(n). We distinguish three special cases:
(a) If d(n) = s(n), the linear estimation problem is referred to as filtering.
(b) If d(n) = s(n + D), where D > 0, the linear estimation problem is
referred to as signal prediction. Note that this problem is diﬀerent
than the prediction considered in the previous section, where d(n) =
x(n + D), D ≥0.
(c) If d(n) = s(n −D), where D > 0, the linear estimation problem is
referred to as signal smoothing.
Our treatment will concentrate on ﬁltering and prediction.
The criterion selected for optimizing the ﬁlter impulse response h(n)
is the minimization of the mean-square error. This criterion has the advan-
tages of simplicity and mathematical tractability. The basic assumptions
are that the processes S(n), W(n), and D(n) are zero mean and wide-sense
stationary. The linear ﬁlter will be assumed to be either FIR or IIR. If it is
IIR, we assume that the input data x(n) is available over the inﬁnite past.
We begin with the design of the optimum FIR ﬁlter. The optimum linear
ﬁlter, in the sense of minimum mean-square error (MMSE), is called a
Wiener filter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
745
14.6.1 FIR WIENER FILTER
Suppose that the ﬁlter is constrained to be of length M with coeﬃcients
h(k), 0 ≤k ≤M −1. Hence its output y(n) depends on the ﬁnite data
record x(n), x(n −1), . . . , x(n −M + 1),
y(n) =
M−1

k=0
h(k)x(n −k)
(14.130)
The mean-square value of the error between the desired output d(n) and
the actual output y(n) is
EM = E

|E(n)|2
= E
⎡
⎣
D(n) −
M−1

k=0
h(k)X(n −k)

2⎤
⎦
(14.131)
Since this is a quadratic function of the ﬁlter coeﬃcients, the minimization
of EM yields the set of linear equations
M−1

k=0
h(k)RXX(ℓ−k) = RDX(ℓ),
ℓ= 0, 1, . . . , M −1
(14.132)
where RDX(k) = E

D(n)X∗(n −k)

is the cross-correlation between the
desired process D(n) and the input process X(n), 0 ≤n ≤M −1, and
RXX(k) is the autocorrelation of the input process X(n). This set of linear
equations that speciﬁes the optimum ﬁlter is called the Wiener–Hopf
equation. (These equations are also called the normal equations, which
we encountered earlier in this chapter in the context of linear one-step
prediction.)
In general, the equations in (14.132) can be expressed in matrix
form as
TMhM = RD
(14.133)
where TM is an M × M (Hermitian) Toeplitz matrix with elements
Tℓk = RXX(ℓ−k), and RD is the M × 1 cross-correlation vector with
elements RDX(ℓ), ℓ= 0, 1, . . . , M −1. The solution for the optimum ﬁlter
coeﬃcients is
hopt = T −1
M RD
(14.134)
and the resulting minimum MSE achieved by the Wiener ﬁlter is
MMSEM = min
hm EM = σ2
D −
M−1

k=0
hopt(k)R∗
DX(k)
(14.135)
or equivalently,
MMSEM = σ2
D −R∗t
D T −1
M RD
(14.136)
where σ2
D = E

|D(n)|2
.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

746
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Let us consider some special cases of (14.132). If we are dealing with
ﬁltering, then D(n) = S(n). Furthermore, if S(n) and D(n) are uncorre-
lated random sequences, as is usually the case in practice, then
RXX(k) = RSS(k) + RWW(k)
RDX(k) = RSS(k)
(14.137)
and the normal equations in (14.132) become
M−1

k=0
h(k)

RSS(ℓ−k) + RWW(l −k)

= RSS(ℓ),
ℓ= 0, 1, . . . , M −1
(14.138)
If we are dealing with prediction, then D(n) = D(n+D) where D > 0.
Assuming that S(n) and W(n) are uncorrelated random sequences, we
have
RDX(k) = RSS(k + D)
(14.139)
Hence the equations for the Wiener prediction ﬁlter become
M−1

k=0
h(k)

RSS(l −k) + RWW(l −k)

= RSS(ℓ+ D)
ℓ= 0, 1, . . . , M −1
(14.140)
In all these cases, the correlation matrix to be inverted is Toeplitz.
Hence the (generalized) Levinson–Durbin algorithm may be used to solve
for the optimum ﬁlter coeﬃcients.
□
EXAMPLE 14.20
Let us consider a signal x(n) = s(n) + w(n), where s(n) is a sample sequence
of an AR(1) process S(n) that satisﬁes the diﬀerence equation
s(n) = 0.6s(n −1) + v(n)
where v(n) is a sample sequence of a white noise process V(n) with variance
σ2
V = 0.64 and w(n) is a sample sequence of a white noise process W(n) with
variance σ2
W = 1.
a. Design a Wiener FIR ﬁlter of length M = 2 to estimate the desired signal
S(n).
b. Use MATLAB to design the optimum FIR Wiener ﬁlter for lengths M = 3, 4,
and 5, and the corresponding MMSE for these cases. Comment on how the
MMSE changes as M is increased from M = 2 to M = 5.
Solution
Since S(n) is obtained by exciting a single-pole ﬁlter by white noise, the power
spectral density of S(n) is
SSS(f) = σ2
V
H

ej2πf2 =
0.64
1 −0.6e−j2πf2
=
0.64
1.36 −1.2 cos(2πf)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
747
Using the procedure given in Example 13.14, the corresponding autocorrelation
sequence RSS(m) is
RSS(m) = (0.6)|m|
a. The equations for the ﬁlter coeﬃcients are
2h(0) + 0.6h(1) = 1,
0.6h(0) + 2h(1) = 0.6
Solution of these equations yields the result
h(0) = 0.451
h(1) = 0.165
The corresponding minimum MSE is
MMSE2 = 1 −h(0)RSS(0) −h(1)RSS(1)
= 1 −0.451 −(0.165)(0.6) = 0.45
MATLAB script:
>> varW = 1; M = 2; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M);
>> RD = Rss’;
>> hopt2 = TM\RD
hopt2 =
0.4505
0.1648
>> MMSE2 = Rss(1) - RD’*hopt2
MMSE2 =
0.4505
b. The ﬁlter coeﬃcients and the corresponding MMSE values are obtained
using the following MATLAB script.
>> M = 3; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt3 = TM\RD; MMSE3 = Rss(1) - RD’*hopt3
MMSE3 =
0.4451
>> M = 4; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt4 = TM\RD; MMSE4 = Rss(1) - RD’*hopt4
MMSE4 =
0.4445
>> M = 5; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt5 = TM\RD; MMSE5 = Rss(1) - RD’*hopt5
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

748
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Filter Order m
MMSE
0.44
2
3
4
5
0.445
0.45
0.455
Performance of FIR Wiener Filter
FIGURE 14.25
Plot of MMSE vs. ﬁlter order in Example 14.20
MMSE5 =
0.4445
>> MMSE = [MMSE2,MMSE3,MMSE4,MMSE5];
>> % Plotting commands follow
The resulting plot of the MMSE vs. ﬁlter order is given in Figure 14.25. □
□
EXAMPLE 14.21
a. Repeat Example 14.20 when the variance of V(n) is σ2
V = 0.64 and the
variance of the additive noise W(n) is σ2
W = 0.1.
b. Generate the AR(1) signal sequence s(n) and the corresponding received
sequence
x(n) = s(n) + w(n),
0 ≤n ≤1000
Filter the sequence x(n), 0 ≤n ≤1000, by the Wiener ﬁlter with M =
2, 3, 4, 5 and plot the output y2(n), y3(n), y4(n), and y5(n), along with the
derived signal s(n). Comment on the eﬀectiveness of the Wiener ﬁlter in
estimating the derived signal s(n).
Solution
From Example 14.20, the PSD and autocorrelation of S(n), respectively, are
given by
SSS(f) =
0.64
1.36 −1.2 cos 2πf ,
RSS(m) = (0.6)|m|
a. Using σ2
W = 0.1, we develop and solve new sets of Wiener–Hopf equations
to obtain new Wiener ﬁlters and their corresponding MMSEs. The details
are given in the following MATLAB script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
749
>> varW = 0.1; M = 2; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M);
>> RD = Rss’;
>> hopt2 = TM\RD;
>> MMSE2 = Rss(1) - RD’*hopt2
MMSE2 =
0.0871
>> M = 3; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt3 = TM\RD; MMSE3 = Rss(1) - RD’*hopt3
MMSE3 =
0.0870
>> M = 4; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt4 = TM\RD; MMSE4 = Rss(1) - RD’*hopt4
MMSE4 =
0.0870
>> M = 5; m = 0:M-1;
>> Rss = 0.6.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt5 = TM\RD; MMSE5 = Rss(1) - RD’*hopt5
MMSE5 =
0.0870
>> % Plotting commands follow
Clearly, the MMSE values are very small and quickly settle into a
steady-state value of 0.0807. The plot of MMSE vs. ﬁlter order is shown in
Figure 14.26.
Filter Order m
2
5
4
3
MMSE
0.0870
0.0870
0.0870
0.0870
0.0871
0.0871
Performance of FIR Wiener Filter
FIGURE 14.26
Plot of MMSE vs. ﬁlter order in Example 14.21(a)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

750
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Index n
100
105
110
115
120
125
130
135
140
145
150
Amplitude
–6
–3
0
3
6
Wiener Filter Output
s(n)
y2(n)
y3(n)
y4(n)
y5(n)
FIGURE 14.27
Plots of signal s(n) and Wiener ﬁlter estimates

yk(n)
5
k=2 in
Example 14.21(b)
b. Signal generation and Wiener ﬁltering operations are given in the following
MATLAB script.
>> n = 0:1000; varW = 0.1;
>> varV = 0.64; vn = sqrt(varV)*randn(1,length(n));
>> sn = filter(1,[1-0.6],vn);
>> wn = sqrt(varW)*randn(1,length(n));
>> xn = sn + wn;
>> yn2 = filter(hopt2,1,xn);
>> yn3 = filter(hopt3,1,xn);
>> yn4 = filter(hopt4,1,xn);
>> yn5 = filter(hopt5,1,xn);
The plot of signal estimates, containing samples from n = 100 to n = 150,
is shown in Figure 14.27. From it, we conclude that all estimates are very
close to each other and that they also follow the signal s(n) closely.
□
□
EXAMPLE 14.22
Consider the signal x(n) = s(n) + w(n), where s(n) is a sample function of an
AR process S(n) that satisﬁes the diﬀerence equation
s(n) = 0.8s(n −1) + v(n)
where v(n) is a sample function of a white Gaussian noise process V(n) with
variance σ2
V = 0.49 and w(n) is a sample function of a white Gaussian noise
process W(n) with variance σ2
W = 0.1. The processes V(n) and W(n) are
uncorrelated.
a. Determine the autocorrelation sequence RSS(m) and RXX(m).
b. Use MATLAB to design a Wiener ﬁlter of length M = 2, 3, 4, 5 and the
corresponding MMSE.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
751
c. Generate the signal sequence s(n) and the received sequence x(n) = s(n) +
w(n) for 0 ≤n ≤1000. Filter the sequence x(n), 0 ≤n ≤1000, by the
Wiener ﬁlter for M = 2, 3, 4, 5 and plot the output y2(n), y3(n), y4(n), and
y5(n), along with the derived signal s(n). Comment on the eﬀectiveness of
the Wiener ﬁlter in estimating the derived signal s(n).
Solution
Since S(n) is obtained by exciting a single-pole ﬁlter by white noise, the power
spectral density of S(n) is
SSS(f) = σ2
V
H

ej2πf2 =
0.49
1 −0.8e−j2πf2
=
0.49
1.64 −1.6 cos(2πf)
a. Using the procedure given in Example 13.14, the corresponding autocorre-
lation sequence RSS(m) is
RSS(m) =
 7
6
2 (0.8)|m| = 1.3611(0.8)|m|
The autocorrelation of X(n) is then given by
RXX(m) = RSS(m) + RVV(m) = 1.3611(0.8)|m| + 0.1δ(m)
b. MATLAB script for Wiener ﬁlter design:
>> varW = 0.1; M = 2; m = 0:M-1;
>> Rss = 1.3611*0.8.ˆm; TM = toeplitz(Rss) + varW*eye(M);
>> RD = Rss’;
>> hopt2 = TM\RD;
>> MMSE2 = Rss(1) - RD’*hopt2
MMSE2 =
0.084606379865379
>> M = 3; m = 0:M-1;
>> Rss = 1.3611*0.8.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt3 = TM\RD; MMSE3 = Rss(1) - RD’*hopt3
MMSE3 =
0.084475619407793
>> M = 4; m = 0:M-1;
>> Rss = 1.3611*0.8.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt4 = TM\RD; MMSE4 = Rss(1) - RD’*hopt4
MMSE4 =
0.084473602242929
>> M = 5; m = 0:M-1;
>> Rss = 1.3611*0.8.ˆm; TM = toeplitz(Rss) + varW*eye(M); RD = Rss’;
>> hopt5 = TM\RD; MMSE5 = Rss(1) - RD’*hopt5
MMSE5 =
0.084473571121205
>> % Plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

752
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Filter Order m
2
3
4
5
MMSE× 102
8.4473
8.4606
Performance of FIR Wiener Filter
FIGURE 14.28
Plot of MMSE vs. ﬁlter order in Example 14.22(a)
Again, the MMSE values are very small and quickly settle into a steady-
state value of 0.0845. The plot of MMSE vs. ﬁlter order is shown in
Figure 14.28.
c. Signal generation and FIR Wiener ﬁltering operations are given in the fol-
lowing MATLAB script.
>> n = 0:1000; varW = 0.1;
>> varV = 0.49; vn = sqrt(varV)*randn(1,length(n));
>> sn = filter(1,[1-0.8],vn);
>> wn = sqrt(varW)*randn(1,length(n));
>> xn = sn + wn;
>> yn2 = filter(hopt2,1,xn);
>> yn3 = filter(hopt3,1,xn);
>> yn4 = filter(hopt4,1,xn);
>> yn5 = filter(hopt5,1,xn);
The plot of signal estimates, containing samples from n = 100 to n = 150, is
shown in Figure 14.29. From it we conclude that all estimates are very close to
each other and that they also follow the signal s(n) closely.
□
14.6.2 ORTHOGONALITY PRINCIPLE IN LINEAR MEAN-SQUARE
ESTIMATION
The Wiener–Hopf equation for the optimum ﬁlter coeﬃcients given
by (14.132) can be obtained directly by applying the orthogonality prin-
ciple in linear mean-square estimation. Simply stated, the mean-square
error EM in (14.131) is a minimum if the ﬁlter coeﬃcients h(k) are
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
753
Index n
100
105
110
115
120
125
130
135
140
145
150
Amplitude
–6
–3
0
3
6
Wiener Filter Output
s(n)
y2(n)
y3(n)
y4(n)
y5(n)
FIGURE 14.29
Plots of signal s(n) and Wiener ﬁlter estimates

yk(n)
5
k=2 in
Example 14.22(b)
selected such that the error is orthogonal to each of the data points in
the estimate,
E

E(n)X∗(n −ℓ)

= 0,
ℓ= 0, 1, . . . , M −1
(14.141)
where
e(n) = d(n) −
M−1

k=0
h(k)x(n −k)
(14.142)
Conversely, if the ﬁlter coeﬃcients satisfy (14.141), then the resulting
MSE is a minimum.
When viewed geometrically, the output of the ﬁlter, which is the
estimate
ˆd(n) =
M−1

k=0
h(k)x(n −k)
(14.143)
is a vector in the subspace spanned by the data x(k), 0 ≤k ≤M −1. The
error e(n) is a vector from d(n) to ˆd(n)—that is, d(n) = e(n) + ˆd(n)—as
shown in Figure 14.30. The orthogonality principle states that the length
EM = E
E(n)
2
is a minimum when e(n) is perpendicular to the data
subspace—that is, e(n) is orthogonal to each data point x(k), 0 ≤k ≤
M −1.
We note that the solution obtained from the normal equations
in (14.132) is unique if the data x(n) in the estimate ˆd(n) are linearly
independent. In that case, the correlation matrix TM is nonsingular. On
the other hand, if the data are linearly dependent, then the rank of TM is
less than M and, hence, the solution is not unique. In this case, the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

754
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
d(n)
x(2)
x (1)
e(n)
h(0)x(1)
h(1)x(2)
d(n)
ˆ
FIGURE 14.30
Geometric interpretation of linear MSE problem
estimate ˆd(n) can be expressed as a linear combination of a reduced set
of linearly independent data points equal to the rank of TM.
Since the MSE is minimized by selecting the ﬁlter coeﬃcients to sat-
isfy the orthogonality principle, then the residual minimum MSE is simply
MMSEM = E

E(n)D∗(n)

(14.144)
which yields the result given in (14.135).
□
EXAMPLE 14.23
Derive the Wiener–Hopf equation in (14.142) and the residual MMSE in equa-
tion (14.136) by application of the orthogonality principle.
Solution
The derived signal is d(n) and the estimate of d(n) is
ˆd(n) =
∞

k=0
h(k)x(n −k)
where x(n) is the observed data sequence. The error sequence is deﬁned as
e(n) = d(n) −ˆd(n)
= d(n) −
∞

k=0
h(k)x(n −k)
The impulse response {h(k)} of the optimum ﬁlter that minimizes the MSE is
obtained when the error e(n) is orthogonal to the data in the estimate. Thus
E

E(n)X∗(n −ℓ)

= 0
for
ℓ= 0, 1, . . .
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
755
Hence
E

D(n) −
∞

k=0
h(k)X(n −k)
 
X∗(n −ℓ)

= 0,
ℓ= 0, 1, . . .
E

D(n)x∗(n −ℓ)

=
∞

k=0
h(k)E

X(n −k)X∗(n −ℓ)

RDX(ℓ) =
∞

k=0
h(k)RXX(ℓ−k),
ℓ≥0
The minimum MSE (MMSE) is
E

E(n)D∗(n)

= E
D(n)
2
−
∞

k=0
h(k)E

D∗(n)X(n −k)

= σ2
D −
∞

k=0
h(k)R∗
DX(k)
□
14.6.3 IIR WIENER FILTER
In the previous section, we constrained the ﬁlter to be FIR and obtained
a set of M linear equations for the optimum ﬁlter coeﬃcients. In this
section, we shall allow the ﬁlter to be inﬁnite in duration (IIR), and the
data sequence will also be inﬁnite. Hence the ﬁlter output is
y(n) =
∞

k=0
h(k)x(n −k)
(14.145)
The ﬁlter coeﬃcients are selected to minimize the mean-square error
between the desired output d(n) and y(n), that is,
E∞= E
E(n)
2
= E
D(n) −
∞

k=0
h(k)X(n −k)

2
(14.146)
Application of the orthogonality principle leads to the Wiener–Hopf
equation
∞

k=0
h(k)RXX(ℓ−k) = RDX(ℓ),
ℓ≥0
(14.147)
The residual MMSE is simply obtained by application of the condition
given by (14.144). Thus we obtain
MMSE∞= min
h E∞= σ2
D −
∞

k=0
hopt(k)R∗
DX(k)
(14.148)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

756
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
The Wiener–Hopf equation given by (14.147) cannot be solved
directly with z-transform techniques, because the equation holds only
for ℓ≥0. We shall solve for the optimum IIR Wiener ﬁlter based on the
innovations representation of the stationary random process X(n).
Recall that a stationary random process X(n) with autocorrelation
RXX(k) and power spectral density SXX(f) may be represented by an
equivalent innovations process by passing X(n) through a noise-whitening
ﬁlter with system function 1/G(z), where G(z) is the minimum-phase
component obtained from the spectral factorization of SXX(z), that is,
SXX(z) = σ2
I G(z)G(z−1)
(14.149)
Hence G(z) is analytic in the region |z| > r1, where r1 < 1.
Now, the optimum Wiener ﬁlter may be viewed as the cascade of the
whitening ﬁlter 1/G(z) with a second ﬁlter—say, Q(z)—whose output
y(n) is identical to the output of the optimum Wiener ﬁlter. Since
y(n) =
∞

k=0
q(k)i(n −k)
(14.150)
where i(n) is a sample sequence of the innovations process I(n), and
e(n) = d(n) −y(n), application of the orthogonality principle yields the
new Wiener–Hopf equation as
∞

k=0
q(k)RII(ℓ−k) = RDI(ℓ),
ℓ≥0
(14.151)
But, since I(n) is white, it follows that RII(ℓ−k) = 0, unless ℓ= k. Thus
we obtain the solution as
q(ℓ) = RDI(ℓ)
RII(0) = RDI(ℓ)
σ2
I
,
ℓ≥0
(14.152)
The z-transform of the sequence q(ℓ) is
Q(z) =
∞

k=0
q(k)z−k = 1
σ2
i
∞

k=0
RDI(k)z−k
(14.153)
If we denote the z-transform of the two-sided cross-correlation
sequence RDI(k) by SDI(z),
SDI(z) =
∞

k=−∞
RDI(k)z−k
(14.154)
and deﬁne [SDI(z)]+,

SDI(z)

+ =
∞

k=0
RDI(k)z−k
(14.155)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
757
then
Q(z) = 1
σ2
i

SDI(z)

+
(14.156)
To determine [SDI(z)]+, we begin with the output of the noise-
whitening ﬁlter, which may be expressed as
i(n) =
∞

k=0
v(k)x(n −k)
(14.157)
where v(k), k ≥0, is the impulse response of the noise-whitening ﬁlter,
1
G(z) ≡V (z) =
∞

k=0
v(k)z−k
(14.158)
Then
RDI(k) = E

D(n)I∗(n −k)

=
∞

m=0
v(m)E

d(n)x∗(n −m −k)

=
∞

m=0
v(m)RDX(k + m)
(14.159)
The z-transform of the cross-correlation RDI(k) is
SDI(z) =
∞

k=−∞
 ∞

m=0
v(m)RDX(k + m)

z−k
=
∞

m=0
v(m)
∞

k=−∞
RDX(k + m)z−k
=
∞

m=0
v(m)zm
∞

k=−∞
RDX(k)z−k
= V (z−1)SDX(z) = SDX(z)
G(z−1)
(14.160)
Therefore,
Q(z) = 1
σ2
I
 SDX(z)
G(z−1)

+
(14.161)
Finally, the optimum IIR Wiener ﬁller has the system function
Hopt(z) = Q(z)
G(z) =
1
σ2
I G(z)
 SDX(z)
G(z−1)

+
(14.162)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

758
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
In summary, the solution for the optimum IIR Wiener ﬁlter requires
that we perform a spectral factorization of RXX(z) to obtain G(z), which
is the minimum-phase component, and then we solve for the causal part
of SDX(z)/G(z−1). The following example illustrates the procedure.
□
EXAMPLE 14.24
Let us determine the optimum IIR Wiener ﬁlter for the signal given in Exam-
ple 14.20. For this signal, we have
SXX(z) = SSS(z) + 1 = 1.8

1 −1
3z−1
(1 −1
3z)

1 −0.6z−1
(1 −0.6z)
where σ2
I = 1.8 and
G(z) = 1 −1
3z−1
1 −0.6z−1
The z-transform of the cross-correlation RDX(m) is
SDX(z) = SSS(z) =
0.64

1 −0.6z−1
(1 −0.6z)
Hence

SDX(z)
G

z−1

+
=
	
0.64
(1 −1
3z)(1 −0.6z−1)

+
=
	
0.8
1 −0.6z−1 + 0.266z
1 −1
3z

+
=
0.8
1 −0.6z−1
The optimum IIR ﬁlter has the system function
Hopt(z) =
1
1.8
1 −0.6z−1
1 −1
3z−1
 
0.8
1 −0.6z−1

=
4
9
1 −1
3z−1
and impulse response
hopt(n) = 4
9
1
3
n
,
n ≥0
□
We conclude this section by expressing the minimum MSE given
by (14.148) in terms of the frequency domain characteristics of the
ﬁlter. First, we note that σ2
D ≡

E

|D(n)|2
is simply the value of the
autocorrelation sequence RDD(k) evaluated at k = 0. Since
RDD(k) =
1
2πj
"
C
SDD(z)zk−1 dz
(14.163)
it follows that
σ2
D = RDD(0) =
1
2πj
"
C
SDD(z)
z
dz
(14.164)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
759
where the contour integral is evaluated along a closed path encircling the
origin in the region of convergence of SDD(z).
The second term in (14.148) is also easily transformed to the fre-
quency domain by application of Parseval’s theorem. Since hopt(k) = 0,
for k < 0, we have
∞

k=−∞
hopt(k)R∗
DX(k) =
1
2πj
"
C
Hopt(z)SDX

z−1	
z−1 dz
(14.165)
where C is a closed contour encircling the origin that lies within the com-
mon region of convergence of Hopt(z)SDX

z−1	
. By combining (14.164)
with (14.165), we obtain the desired expression for the MMSE, in the
form
MMSE∞=
1
2πj
"
C

SDD(z) −Hopt(z)SDX

z−1	
z−1 dz
(14.166)
We note that SDD(z) = SSS(z) and SDX

z−1	
= SSS

z−1	
.
□
EXAMPLE 14.25
For the optimum Wiener ﬁlter derived in Example 14.24, the minimum MSE is
MMSE∞=
1
2πj
!
C

0.3555

z −1
3

(1 −0.6z)

dz
There is a single pole inside the unit circle at z = 1
3. By evaluating the residue
at the pole, we obtain
MMSE∞= 0.444
We observe that this MMSE is only slightly smaller than that for the optimum
two-tap Wiener ﬁlter in Example 14.20.
□
□
EXAMPLE 14.26
Determine the optimum causal IIR Wiener ﬁlter for the estimation of the signal
s(n) = 0.8s(n −1) + v(n)
from the observation
x(n) = s(n) + w(n)
where v(n) is a sample sequence of a white noise process V(n) with σ2
v = 0.49
and w(n) is a sample sequence of a white noise process W(n) with variance
σ2
W = 1. The process V(n) and W(n) are uncorrelated.
Solution
The complex cross- and auto-PSDs are given by
SDX(z) = SSS(z) =
0.49

1 −0.8z−1
(1 −0.8z)
(14.167)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

760
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
from which the auto-PSD of X(n) is
SXX(z) = SSS(z) + 1 = 1.78

1 −0.45z−1
(1 −0.45z)

1 −0.8z−1
(1 −0.8z)
This gives the whitening ﬁlter
G(z) =

1 −0.45z−1

1 −0.8z−1
The causal part is then given by

SDX(z)
G

z−1

+
=

0.49

1 −0.8z−1
(1 −0.45z)

+
=

0.766

1 −0.8z−1 +
0.345z
(1 −0.45z)

+
=
0.766

1 −0.8z−1
Now the system function of the optimum Wiener ﬁlter is
Hopt(z) =
1
1.78
 1 −0.8z−1
1 −0.45z−1
 
0.766
1 −0.8z−1

=
0.43
1 −0.45z−1
and the corresponding impulse response is
hopt(n) = 0.43(0.45)nu(n)
Finally, the MMSE for the causal IIR Wiener ﬁlter is given by
MMSE∞=
1
2πj
!
C
"
SSS(z) −Hopt(z)SSS

z−1#
z−1 dz
=
1
2πj
!
C
0.28
(z −0.45)(1 −0.8z) dz =
0.28
1 −0.8z

z=0.45
= 0.438
□
□
EXAMPLE 14.27
Consider the design of the optimum causal IIR Wiener ﬁlter for the signal s(n)
given in Example 14.26, where now the variance of the additive noise w(n) is
σ2
W = 0.1.
a. Determine the system function for the optimum causal IIR ﬁlter and the
minimum MSE.
b. Generate the signal sequence s(n) and the received sequence x(n) for
0 ≤n ≤1000, where
s(n) = 0.8s(n −1) + v(n)
x(n) = s(n) + w(n)
Filter the sequence x(n), 0 ≤n ≤1000, by the optimum causal Wiener
ﬁlter obtained in part (a), and plot the output sequence y(n) along with the
desired sequence s(n). Comment on the eﬀectiveness of the causal Wiener
ﬁlter in estimating the desired signal.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
761
Solution
From (14.167), the complex PSD SSS(z) is
SSS(z) =
0.49

1 −0.8z−1
(1 −0.8z)
a. For σ2
W = 0.1, the auto-PSD of X(n) is
SXX(z) = SSS(z) + 0.1 = 0.6441

1 −0.1242z−1
(1 −0.1242z)

1 −0.8z−1
(1 −0.8z)
(14.168)
This gives the whitening ﬁlter
G(z) =

1 −0.1242z−1

1 −0.8z−1
The causal part is then given by

SDX(z)
G

z−1

+
=

0.49

1 −0.8z−1
(1 −0.1242z)

+
=

0.5441

1 −0.8z−1 +
0.0676z
(1 −0.1242z)

+
=
0.5441

1 −0.8z−1
Now the system function of the optimum Wiener ﬁlter is
Hopt(z) =
1
0.6441

1 −0.8z−1
1 −0.1242z−1
 
0.5441
1 −0.8z−1

=
0.8447
1 −0.1242z−1
and the corresponding impulse response is
hopt(n) = 0.8447(0.1242)nu(n)
Finally, the MMSE for the causal IIR Wiener ﬁlter is given by
MMSE∞=
1
2πj
!
C
"
SSS(z) −Hopt(z)SSS

z−1#
z−1 dz
=
1
2πj
!
C
0.0761
(z −0.1242)(1 −0.8z) dz =
0.0761
1 −0.8z

z=0.1242
= 0.08447.
Clearly, this error is lower than the minimum mean-squared errors given in
Example 14.22. The above calculations were performed using the following
MATLAB script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

762
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
>> varW = 0.1; varV = 0.49; a = 0.8;
>> Ss_num = varV*[0,-1/a]; Ss_den = conv([1,-a],[1,-1/a]);
>> Sx_num = varW*Ss_den + [Ss_num,0]; Sx_den = Ss_den;
>> Sx_num0 = Sx_num(1);
>> Sx_num_roots = roots(Sx_num/Sx_num0);
>> Sx_den0 = Sx_den(1);
>> Sx_den_roots = roots(Sx_den/Sx_den0);
>> Sx_num_factor = [1,-1/Sx_num_roots(1); 1,- Sx_num_roots(2)];
>> Sx_den_factor = [1,-1/Sx_den_roots(1); 1,- Sx_den_roots(2)];
>> Sx_constant = (Sx_num0*Sx_num_roots(1))/(Sx_den0*Sx_den_roots(1));
>> G_num = Sx_num_factor(2,:); G_den = Sx_den_factor(2,:);
>> [R,p,C] = residuez(varV*[0,-Sx_num_roots(1)],...
conv([1,-Sx_den_roots(2)],[1,->> Sx_num_roots(1)]));
>> causal_part_num = R(2);
>> anticausal_part_num = R(1)*(-Sx_num_roots(2));
>> Hopt_num = causal_part_num/Sx_constant;
>> Hopt_den = [1,-Sx_num_roots(2)];
>> DHopt_num = Hopt_den-[Hopt_num,0];
>> Int_num = conv(Ss_num,[0,DHopt_num]);
>> MMSEc = Int_num(end)/(1-Sx_den_roots(2)*Sx_num_roots(2));
MMSEc =
0.084473570633519
b. Signal generation and causal IIR Wiener ﬁltering operations are given in the
following MATLAB script.
>> n = 0:1000; varW = 0.1;
>> varV = 0.49; vn = sqrt(varV)*randn(1,length(n));
>> sn = filter(1,[1-0.8],vn);
>> wn = sqrt(varW)*randn(1,length(n));
>> xn = sn + wn;
>> yn = filter(Hopt_num,Hopt_den,xn);
Plot of signal and its estimate, containing samples from n = 100 to n = 150,
is shown in Figure 14.31. From it, we conclude that the estimated signal
y(n) is very close to the signal s(n).
□
14.6.4 NONCAUSAL WIENER FILTER
In the previous section, we constrained the optimum Wiener ﬁlter to be
causal—that is, hopt(n) = 0 for n < 0. In this section, we shall drop this
condition and allow the ﬁlter to include both the inﬁnite past and the
inﬁnite future of the sequence x(n) in forming the output y(n),
y(n) =
∞

k=−∞
h(k)x(n −k)
(14.169)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
763
Index n
100
105
110
115
120
125
130
135
140
145
150
Amplitude
–8
–4
0
4
8
Causal IIR Wiener Filter Output
s(n)
y(n)
FIGURE 14.31
Plots of signal s(n) and causal IIR Wiener ﬁlter estimate y(n)
in Example 14.27(b)
The resulting ﬁlter is physically unrealizable. It may also be viewed as
a smoothing ﬁlter in which the inﬁnite future signal values are used to
smooth the estimate ˆd(n) = y(n) of the desired signal d(n).
Application of the orthogonality principle yields the Wiener–Hopf
equation for the noncausal ﬁlter in the form
∞

k=−∞
h(k)RXX(ℓ−k) = RDX(ℓ),
−∞< ℓ< ∞
(14.170)
and the resulting MMSEnc as
MMSEnc = σ2
D −
∞

k=−∞
h(k)R∗
DX(k)
(14.171)
Since (14.170) holds for −∞< ℓ< ∞, this equation can be transformed
directly to yield the optimum noncausal Wiener ﬁlter as
Hnc(z) = SDX(z)
SXX(z)
(14.172)
The MMSEnc can also be simply expressed in the z-domain as
MMSEnc =
1
2πj
"
C

SDD(z) −Hnc(z)SDX

z−1	
z−1dz
(14.173)
The following example serves to compare the form of the optimal
noncausal ﬁlter with the optimal causal ﬁlter obtained in the previous
section.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

764
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
□
EXAMPLE 14.28
The optimum noncausal Wiener ﬁlter for the signal characteristics given in
Example 14.20 is given by (14.172), where
SDX(z) = SSS(z) =
0.64

1 −0.6z−1
(1 −0.6z)
and
SXX(z) = SSS(z) + 1 =
2

1 −0.3z−1 −0.3z


1 −0.6z−1
(1 −0.6z)
Then
Hnc(z) =
0.3556

1 −1
3z−1
1 −1
3z

This ﬁlter is clearly noncausal.
The minimum MSE achieved by this ﬁlter is determined from evaluat-
ing (14.173). The integrand is
1
z SSS(z)

1 −Hnc(z)

=
0.3555

z −1
3

1 −1
3z

The only pole inside the unit circle is z = 1/3. Hence the residue is
0.3555
1 −1
3z

z= 1
3
= 0.3555
8
9
 = 0.40
Hence the minimum achievable MSE obtained with the optimum noncausal
Wiener ﬁlter is
MMSEnc = 0.40
Note that this is lower than the MMSE for the causal ﬁlter, as expected.
□
□
EXAMPLE 14.29
a. Determine the system function for the noncausal IIR Wiener ﬁlter for the
signal given in Example 14.27, where σ2
V = 0.49 and σ2
W = 0.1.
b. Determine the MMSE for this ﬁlter and compare this to the MMSE for the
optimum IIR causal Wiener ﬁlter in Example 14.27.
c. Generate the signal sequence s(n) and the received sequence x(n) for
0 ≤n ≤1000. Filter x(n) by the optimum noncausal Wiener ﬁlter obtained
in part (a), and plot the output sequence y(n) along with the desired se-
quence s(n). Comment on the eﬀectiveness of the noncausal Wiener ﬁlter
in estimating the desired signal.
Solution
From (14.167), the complex PSD SSS(z) is
SSS(z) =
0.49

1 −0.8z−1
(1 −0.8z)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Wiener Filters for Filtering and Prediction
765
a. From (14.168), the complex PSD SXX(z) is
SXX(z) = SSS(z) + 0.1 = 0.6441

1 −0.1242z−1
(1 −0.1242z)

1 −0.8z−1
(1 −0.8z)
Hence the optimum noncausal IIR wiener ﬁlter is
Hnc(z) = SSS(z)
SXX(z) =
0.7608

1 −0.1242z−1
(1 −0.1242z)
(14.174)
with impulse response
hnc(n) = 0.7727(0.1242)|n|
which is a noncausal ﬁlter.
b. The minimum MSE achieved by this ﬁlter is determined from evaluat-
ing (14.173). The integrand is
1
z SSS(z)

1 −Hnc(z)

=
0.07608
(z −0.1242)(1 −0.1242z)
The only pole inside the unit circle is z = 0.1242. Hence the residue is
0.07608
1 −0.1242

z=0.1242
= 0.07608
0.9846
 = 0.0773
Hence the minimum achievable MSE obtained with the optimum noncausal
Wiener ﬁlter is
MMSEnc = 0.0773
which is lower than the MMSE of 0.08447 for the causal ﬁlter, as expected.
c. Note that the noncausal ﬁlter in (14.174) can be expressed as a product of
a causal and an anticausal ﬁlter—that is,
Hnc(z) = Hc(z)Hc

z−1
=

√
0.7608
1 −0.1242z−1
 
√
0.7608
1 −0.1242z

(14.175)
Thus the causal ﬁlter part is
Hc(z) =
0.8722
1 −0.1242z−1
(14.176)
This ﬁlter will be used in the filtfilt function to implement the noncausal
ﬁlter Hnc(z). This function ﬁlters the input data in the forward direction
through Hc(z), then reverses the resulting output and ﬁlters it through
Hc(z) again as a backward pass, and then reverses the resulting output to
obtain the output of the noncausal ﬁlter. This also amounts to zero-phase
ﬁltering, as expected. Signal generation and noncausal IIR Wiener ﬁltering
operations are given in the following MATLAB script.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

766
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
Index n 
100
105
110
115
120
125
130
135
140
145
150
Amplitude
–8
–4
0
4
8
Noncausal IIR Wiener Filter Output
s(n)
y(n)
FIGURE 14.32
Plots of signal s(n) and noncausal IIR Wiener ﬁlter estimate
y(n) in Example 14.29(c)
n = 0:1000; varW = 0.1;
varV = 0.49; vn = sqrt(varV)*randn(1,length(n));
sn = filter(1,[1-0.8],vn);
wn = sqrt(varW)*randn(1,length(n));
xn = sn + wn;
Hc_num = sqrt(0.7608); Hc_den = [1,-0.1242];
yn = filtfilt(Hc_num,Hc_den,xn);
Plot of signal and its estimate, containing samples from n = 100 to n = 150,
is shown in Figure 14.32. From it we conclude that the estimated signal y(n)
is very close to the signal s(n).
□
14.7 SUMMARY AND REFERENCES
The major focal point in this chapter was the design of optimum linear
systems for linear prediction and ﬁltering. The criterion for optimality
was the minimization of the mean-square error between a speciﬁed desired
ﬁlter output and the actual ﬁlter output.
In the development of linear prediction, we demonstrated that the
equations for the forward and backward prediction errors speciﬁed a
lattice ﬁlter whose parameters, the reﬂection coeﬃcients Km, were sim-
ply related to the ﬁlter coeﬃcients am(k) of the direct form FIR linear
predictor and the associated prediction-error ﬁlter. The optimum ﬁlter
coeﬃcients Km and am(k) are easily obtained from the solution of the
normal equations.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Summary and References
767
We described two computationally eﬃcient algorithms for solving the
normal equations, the Levinson–Durbin and Schur algorithms. Both algo-
rithms are suitable for solving a Toeplitz system of linear equations and
have a computational complexity of o(p2) when executed on a single pro-
cessor. However, with full parallel processing, the Schur algorithm solves
the normal equations in o(p) time units, whereas the Levinson–Durbin
algorithm requires o(p log p).
In addition to the all-zero lattice ﬁlter that resulted from linear pre-
diction, we also derived the AR lattice (all-pole) ﬁlter structure and the
ARMA lattice-ladder (pole-zero) ﬁlter structure. Finally, we described the
design of the class of optimum linear ﬁlters, called Wiener ﬁlters.
Linear estimation theory has had a rich history of development over
the past four decades. Kailath [41] presents a historical account of the
ﬁrst three decades. The pioneering work of Wiener [98] on optimum lin-
ear ﬁltering for statistically stationary signals is especially signiﬁcant.
The generalization of the Wiener ﬁlter theory to dynamical systems with
random inputs was developed by Kalman [46] and Kalman and Bucy
[47]. Kalman ﬁlters are treated in Meditch [66], Brown [3], and Chui and
Chen [6]. The monograph by Kailath [42] treats both Wiener and Kalman
ﬁlters.
There are numerous references on linear prediction and lattice ﬁlters.
Tutorial treatments on these subjects have been published in the journal
papers by Makhoul [57, 58, 59] and Friedlander [17, 18]. Haykin [30],
Markel and Gray [63], and Tretter [92] provide comprehensive treatments
of these subjects. Applications of linear prediction to spectral analysis are
found in Kay [48], Kay and Marple [49], and Marple [64], to geophysics
in Robinson and Treitel [86, 87], and to adaptive ﬁltering in Haykin [30].
The Levinson–Durbin algorithm for recursively solving the normal
equations was given by Levinson [54] and later modiﬁed by Durbin [14].
Variations of this classical algorithm, called split Levinson algorithms,
have been developed by Delsarte and Genin [12] and Krishna [51]. These
algorithms exploit additional symmetries in the Toeplitz correlation ma-
trix and save about a factor of two in the number of multiplications.
The Schur algorithm was originally described by Schur [88] in a pa-
per published in German. An English translation of this paper appears
in Gohberg [26]. The Schur algorithm is intimately related to the poly-
nomials Am(z), which may be interpreted as orthogonal polynomials. A
treatment of orthogonal polynomials is given in Szeg¨o [91], Grenander and
Szeg¨o [28], and Geronimus [22]. The thesis of Vieira [94] and the papers by
Kailath, Vieira, and Morf [45], Delsarte, Genin, and Kamp [13], and Youla
and Kazanjian [100] provide additional results on orthogonal polynomi-
als. Kailath [43, 44] provides tutorial treatments of the Schur algorithm
and its relationship to orthogonal polynomials and the Levinson–Durbin
algorithm. The pipelined parallel processing structure for computing the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

768
Chapter 14
LINEAR PREDICTION AND OPTIMUM LINEAR FILTERS
reﬂection coeﬃcients based on the Schur algorithm, and the related prob-
lem of solving Toeplitz systems of linear equations, is described in the
paper by Kung and Hu [52]. Finally, we should mention that some addi-
tional computational eﬃciency can be achieved in the Schur algorithm, by
further exploiting symmetry properties of Toeplitz matrices, as described
by Krishna [51]. This leads to the so-called split Schur algorithm, which
is analogous to the split Levinson algorithm.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

769
C H A P T E R 15
Adaptive Filters
In contrast to ﬁlter design techniques described in Chapter 14, which were
based on knowledge of the second-order statistics of the signals, there
are many digital signal processing applications in which these statistics
cannot be speciﬁed a priori. Examples of such applications include channel
equalization and echo cancellation in data communication systems, and
system identiﬁcation and system modeling in control systems. In such
applications, the coeﬃcients of the ﬁlter to be designed cannot be speciﬁed
a priori, because they depend on the characteristics of the communication
channel or the control system. Such ﬁlters, with adjustable parameters, are
usually called adaptive filters, especially when they incorporate algorithms
that allow the ﬁlter coeﬃcients to adapt to changes in the signal statistics.
Adaptive ﬁlters have received considerable attention from researchers
over the past 35 years. As a result, many computationally eﬃcient al-
gorithms for adaptive ﬁltering have been developed. In this chapter, we
describe two basic algorithms: the least-mean-square (LMS) algorithm,
which is based on a gradient optimization for determining the coeﬃcients,
and the class of recursive least-squares algorithms. Before we describe the
algorithms, we present several practical applications in which adaptive
ﬁlters have been successfully used in the estimation of signals corrupted
by noise and other interference.
15.1 APPLICATIONS OF ADAPTIVE FILTERS
Adaptive ﬁlters have been used widely in communication systems,
control systems, and various other systems in which the statistical
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

770
Chapter 15
ADAPTIVE FILTERS
characteristics of the signals to be ﬁltered are either unknown a priori or,
in some cases, slowly time-variant (nonstationary signals). Numerous ap-
plications of adaptive ﬁlters have been described in the literature. Some of
the more noteworthy applications include (1) adaptive antenna systems,
in which adaptive ﬁlters are used for beam steering and for providing
nulls in the beam pattern to remove undesired interference (Widrow,
Mantey, and Griﬃths [97]); (2) digital communication receivers, in which
adaptive ﬁlters are used to provide equalization of intersymbol interfer-
ence and for channel identiﬁcation (Lucky [55], Proakis and Miller [80],
Gersho [23], George, Bowen, and Storey [21], Proakis [77, 78], Magee
and Proakis [56], Picinbono [75], and Nichols, Giordano, and Proakis
[70]); (3) adaptive noise canceling techniques, in which adaptive ﬁlters
are used to estimate and eliminate a noise component in a desired signal
(Widrow et al. [96], Hsu and Giordano [34], and Ketchum and Proakis
[50]); (4) system modeling, in which adaptive ﬁlters are used as models
to estimate the characteristics of an unknown system. These are just a
few of the best-known examples of the use of adaptive ﬁlters.
Although both IIR and FIR ﬁlters have been considered for adaptive
ﬁltering, the FIR ﬁlter is by far the most practical and widely used. The
reason for this preference is quite simple: the FIR ﬁlter has only adjustable
zeros; hence it is free of stability problems associated with adaptive IIR ﬁl-
ters, which have adjustable poles as well as zeros. We should not conclude,
however, that adaptive FIR ﬁlters are always stable. On the contrary, the
stability of the ﬁlter depends critically on the algorithm for adjusting its
coeﬃcients, as will be demonstrated in Sections 15.2 and 15.3.
Of the various FIR ﬁlter structures that are possible, the direct form
and the lattice form are the ones used in adaptive ﬁltering applications.
The direct form FIR ﬁlter structure with adjustable coeﬃcients h(n) is
illustrated in Figure 15.1. On the other hand, the adjustable parameters in
an FIR lattice structure are the reﬂection coeﬃcients Kn. In this chapter,
we consider only direct form FIR ﬁlter structures.
An important consideration in the use of an adaptive ﬁlter is the
criterion for optimizing the adjustable ﬁlter parameters. The criterion
must not only provide a meaningful measure of ﬁlter performance, but it
must also result in a practically realizable algorithm.
For example, a desirable performance index in a digital communi-
cation system is the average probability of error. Consequently, in im-
plementing an adaptive equalizer, we might consider the selection of the
equalizer coeﬃcients to minimize the average probability of error as the
basis for our optimization criterion. Unfortunately, however, the perfor-
mance index (average probability of error) for this criterion is a highly
nonlinear function of the ﬁlter coeﬃcients and the signal statistics. As a
consequence, the implementation of an adaptive ﬁlter that optimizes such
a performance index is complex and impractical.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
771
z–1
z–1
z–1
z–1
Coefficient
Adjustment
h(0)
h(1)
h(2)
h(3)
h(4)
Input
Output
Σ
FIGURE 15.1
Direct form adaptive FIR ﬁlter
In some cases, a performance index that is a nonlinear function of the
ﬁlter parameters possesses many relative minima (or maxima), so that one
is not certain whether the adaptive ﬁlter has converged to the optimum
solution or to one of the relative minima (or maxima). For such reasons,
some desirable performance indices, such as the average probability of
error in a digital communication system, must be rejected on the grounds
that they are impractical to implement.
Two criteria that provide good measures of performance in adaptive
ﬁltering applications are the least-squares criterion and its counterpart in
a statistical formulation of the problem, namely, the mean-square-error
(MSE) criterion. The least-squares (and MSE) criterion results in a qua-
dratic performance index as a function of the ﬁlter coeﬃcients and, hence,
it possesses a single minimum. The resulting algorithms for adjusting the
coeﬃcients of the ﬁlter are relatively easy to implement, as we demon-
strate in Section 15.2.
In this section, we describe several applications of adaptive ﬁlters that
serve as a motivation for the mathematical development of algorithms
derived in Section 15.2. We ﬁnd it convenient to use the direct form FIR
structure in these examples. Although we will not develop the recursive
algorithms for automatically adjusting the ﬁlter coeﬃcients in this section,
it is instructive to formulate the optimization of the ﬁlter coeﬃcients
as a least-squares optimization problem. This development will serve to
establish a common framework for the algorithms derived in the next two
sections.
15.1.1 SYSTEM IDENTIFICATION OR SYSTEM MODELING
In system identiﬁcation or system modeling, we have an unknown system,
called a plant, that we wish to identify. The system is modeled by an FIR
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

772
Chapter 15
ADAPTIVE FILTERS
ﬁlter with adjustable coeﬃcients. Both the plant and model are excited
by an input sequence x(n). If y(n) denotes the output of the plant and
ˆy(n) denotes the output of the model,
ˆy(n) =
M−1

k=0
h(k)x(n −k)
(15.1)
we may form the error sequence
e(n) = y(n) −ˆy(n),
n = 0, 1, . . .
(15.2)
and select the coeﬃcients h(k) to minimize the sum of squared errors,
that is,
EM =
N

n=0

y(n) −
M−1

k=0
h(k)x(n −k)
2
(15.3)
where N + 1 is the number of observations. This is the least-squares
criterion for determining the ﬁlter coeﬃcients, which results in the set of
linear equations
M−1

k=0
h(k)rxx(ℓ−k) = ryx(ℓ),
ℓ= 0, 1, . . . , M −1
(15.4)
where rxx(ℓ) is the autocorrelation (time average) of the sequence x(n)
and ryx(ℓ) is the cross-correlation (time average) of the system output
with the input sequence, deﬁned as1
rxx(ℓ) =
N

n=0
x(n)x(n −ℓ)
(15.5a)
ryx(ℓ) =
N

n=0
y(n)x(n −ℓ)
(15.5b)
By solving (15.4), we obtain the ﬁlter coeﬃcients for the model. Since
the ﬁlter parameters are obtained directly from measurement data at the
input and output of the system, without prior knowledge of the plant, we
call the FIR ﬁlter model an adaptive ﬁlter.
If our only objective were to identify the system by use of the FIR
model, the solution of (15.4) would suﬃce. In control systems applications,
however, the system being modeled may be time-variant, changing slowly
with time, and our purpose for having a model is to ultimately use it for
1In this chapter, we use lowercase rxx(ℓ) and ryx(ℓ) to denote the time-average auto-
correlations and cross-correlations functions obtained directly from the data sequences.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
773
Unknown
Time-Variant
System
FIR Filter
Model
Adaptive
Algorithm
Error Signal
Noise
x(n)
d(n)
d(n)
y(n)
w(n)
ˆ
FIGURE 15.2
Application of adaptive ﬁltering to system identiﬁcation
designing a controller that controls the plant. Furthermore, measurement
noise is usually present at the output of the plant. This noise introduces
uncertainty in the measurements and corrupts the estimates of the ﬁlter
coeﬃcients in the model. Such a scenario is illustrated in Figure 15.2.
In this case, the adaptive ﬁlter must identify and track the time-variant
characteristics of the plant in the presence of measurement noise at the
output of the plant. The algorithms to be described in Section 15.2 are
applicable to this system identiﬁcation problem.
□
EXAMPLE 15.1
System Identiﬁcation
Consider the system identiﬁcation problem as illustrated in Figure 15.2. Suppose
the unknown system has two complex-conjugate poles with a system function
H(z) =
1
1 −2Re(p)z−1 + |p|2z−2
where p is one of the poles. The additive noise sequence w(n) is a sample
sequence of a white, zero-mean Gaussian process W(n) with variance σ2
W = 0.02.
The excitation sequence x(n) is also a sample sequence of a white, zero-mean
Gaussian process X(n) with variance σ2
X = 1. The processes W(n) and X(n) are
uncorrelated. The FIR ﬁlter model has impulse response h(n) for 0 ≤n ≤M−1.
a. Generate the output sequence y(n) for 0 ≤n ≤1000 and the corresponding
desired sequence d(n) for 0 ≤n ≤1000, when p = 0.8ejπ/4 and M = 15.
b. Compute the autocorrelation sequence rxx(m) and the cross-correlation se-
quence ryx(m) for 0 ≤m ≤M −1. Then compute the FIR ﬁlter coeﬃcients
from the least-sequence equation in (15.4).
c. Plot and compare the impulse response of the unknown system with that of
the FIR ﬁlter model. Also plot and compare the frequency response of the
unknown system with that of the model.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

774
Chapter 15
ADAPTIVE FILTERS
d. Repeat parts (a), (b), and (c) when the sequence x(n) is the output of a
ﬁrst-order AR process described by the diﬀerence equation
x(n) = 1
2x(n −1) + v(n)
where v(n) is a sample function of a white Gaussian noise process with
variance σ2
V = 1.
Solution
This solution is obtained using MATLAB.
a. MATLAB script:
>> varW = 0.02;
% Additive noise variance
>> varX = 1;
% Excitation seq Variance
>> N = 1000; n = 0:N;
% # of samples and indices
>> p = 0.8*exp(1j*pi/4);
% Pole location
>> a = [1,-2*real(p),abs(p)ˆ2];
% Plant denominator coeff
>> M = 15;
% FIR filter model order
>> xn = sqrt(varX)*randn(N+1,1); % Input sequence
>> wn = sqrt(varW)*randn(N+1,1); % Noise sequence
>> dn = filter(1,a,xn);
% Output of the plant
>> yn = dn+wn;
% Noisy plant output
b. MATLAB script:
>> [rxx,lags] = xcorr(xn,M-1); % ACRS of x(n)
>> Rxx = toeplitz(rxx(M:end)); % ACRM of x(n)
>> ryx = xcorr(yn,xn,M-1);
% CCRS between y(n) and x(n)
>> ryx = ryx(M:end);
% CCRV between y(n) and x(n)
>> hm = Rxx\ryx;
% Model coeff (or Imp resp)
>> hp = impz(1,a,M+5);
% Plant impulse response
c. MATLAB script:
>> om = linspace(0,1,1001)*pi;
>> Hm = freqz(hm,1,om);
>> Hm_mag = abs(Hm);
>> Hm_pha = angle(Hm)/pi;
>> Hp = freqz(1,a,om);
>> Hp_mag = abs(Hp);
>> Hp_pha = angle(Hp)/pi;
>> % Plotting commands follow
The resulting plots are shown in Figure 15.3. Comparing the impulse re-
sponse plots, we note that the model FIR impulse response is very close to the
plant IIR impulse response for the ﬁrst M = 15 samples. Similarly, the fre-
quency responses of the two systems are very close to each other. In these
plots, a dotted line represents the phase response. A slight ripple in the
model frequency response is the result of windowing eﬀect due to FIR model
assumption.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
775
0
5
10
15
20
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Plant Impulse Response
0
0.2
0.4
0.6
0.8
1
Magnitude
0
1
2
3
4
Plant Frequency Response
Phase in p Units 
–1
–0.5
0
0.5
1
Time Index n
Time Index n
0
5
10
15
20
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Model Impulse Response
0
0.2
0.4
0.6
0.8
1
Magnitude
0
1
2
3
4
Model Frequency Response
Phase in p Units
–1
–0.5
0
0.5
1
w/p
w/p
FIGURE 15.3
Plots of the impulse and frequency responses of the plant and FIR
model systems in Example 15.1(c). The dotted lines represent phase responses
d. The only change in this set up is how the excitation sequence is generated
using a ﬁrst-order AR process.
>> varW = 0.02;
% Additive noise variance
>> varV = 1;
% Excitation seq Variance
>> N = 1000; n = 0:N;
% Number of samples and indices
>> p = 0.8*exp(1j*pi/4);
% Pole location
>> a = [1,-2*real(p),abs(p)ˆ2];
% Plant denominator coeff
>> M = 15;
% FIR filter model order
>> vn = sqrt(varV)*randn(N+1,1); % Gaussian seq for Input x(n)
>> xn = filter(1,[1,-1/2],vn);
% Input sequence x(n)
>> wn = sqrt(varW)*randn(N+1,1); % Noise sequence
>> dn = filter(1,a,xn);
% Output of the plant
>> yn = dn+wn;
% Noisy plant output
>> [rxx,lags] = xcorr(xn,M-1);
% ACRS of x(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

776
Chapter 15
ADAPTIVE FILTERS
>> Rxx = toeplitz(rxx(M:end)); % ACRM of x(n)
>> ryx = xcorr(yn,xn,M-1);
% CCRS between y(n) and x(n)
>> ryx = ryx(M:end);
% CCRV
>> hm = Rxx\ryx;
% Model coeff (or Imp resp)
>> hp = impz(1,a,M+5);
% Plant impulse response
>> om = linspace(0,1,1001)*pi;
>> Hm = freqz(hm,1,om);
>> Hm_mag = abs(Hm); Hm_pha = angle(Hm)/pi;
>> Hp = freqz(1,a,om);
>> Hp_mag = abs(Hp); Hp_pha = angle(Hp)/pi;
The resulting plots are shown in Figure 15.4. Comparisons of the impulse
and frequency response plots are similar to those in part (c). Furthermore,
the results in part (c) and (d) are almost identical.
□
Time Index n
0
5
10
15
20
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Plant Impulse Response
0
0.2
0.4
0.6
0.8
1
Magnitude
0
1
2
3
4
Plant Frequency Response
–1
–0.5
0
0.5
1
Time Index n
0
5
10
15
20
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Model Impulse Response
0
0.2
0.4
0.6
0.8
1
Magnitude
0
1
2
3
4
Model Frequency Response
–1
–0.5
0
0.5
1
Phase in p Units 
Phase in p Units 
w/p
w/p
FIGURE 15.4
Plots of the impulse and frequency responses of the plant and FIR
model systems in Example 15.1(d). The dotted lines represent phase responses
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
777
15.1.2 ADAPTIVE CHANNEL EQUALIZATION
Figure 15.5 shows a block diagram of a digital communication system
in which an adaptive equalizer is used to compensate for the distortion
caused by the transmission medium (channel). The digital sequence of
information symbols a(n) is fed to the transmitting ﬁlter, whose output is
s(t) =
∞

k=0
a(k)p(t −kTs)
(15.6)
where p(t) is the impulse response of the ﬁlter at the transmitter and
Ts is the time interval between information symbols—that is, 1/Ts is the
symbol rate. For purposes of this discussion, we may assume that a(n) is
a multilevel sequence that takes on values from the set ±1, ±3, ±5, . . . ,
±(K −1), where K is the number of possible symbol values.
Typically, the pulse p(t) is designed to have the characteristics illus-
trated in Figure 15.6. Note that p(t) has amplitude p(0) = 1 at t = 0
and p(nTs) = 0 at t = nTs, n = ±1, ±2, . . . . As a consequence, successive
pulses transmitted sequentially every Ts seconds do not interfere with one
another when sampled at the time instants t = nTs. Thus a(n) = s(nTs).
The channel, which is usually well modeled as a linear ﬁlter, dis-
torts the pulse and, thus, causes intersymbol interference. For example,
in telephone channels, ﬁlters are used throughout the system to separate
signals in diﬀerent frequency ranges. These ﬁlters cause phase and ampli-
tude distortion. Figure 15.7 illustrates the eﬀect of channel distortion on
the pulse p(t) as it might appear at the output of a telephone channel.
Now, we observe that the samples taken every Ts seconds are corrupted
by interference from several adjacent symbols. The distorted signal is also
corrupted by additive noise, which is usually wideband.
Transmitter
(Filter)
Receiver
(Filter)
Channel
(Time-Variant
Filter)
Sampler
Data
Sequence
Error Signal
a(n)
â(n)
â(n)
d(n)
Noise
Adaptive
Equalizer
Adaptive
Algorithm
Decision
Device
Reference
Signal
FIGURE 15.5
Application of adaptive ﬁltering to adaptive channel equalization
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

778
Chapter 15
ADAPTIVE FILTERS
–5Ts
5Ts
–4Ts
4Ts
–3Ts
3Ts
–2Ts
2Ts
1
0
t
p(t)
Ts
–Ts
FIGURE 15.6
Pulse shape for digital transmission of symbols at a rate of 1/Ts
symbols per second
At the receiving end of the communication system, the signal is ﬁrst
passed through a ﬁlter that is designed primarily to eliminate the noise
outside of the frequency band occupied by the signal. We may assume
that this ﬁlter is a linear-phase FIR ﬁlter that limits the bandwidth of the
noise but causes negligible additional distortion on the channel-corrupted
signal.
Samples of the received signal at the output of this ﬁlter reﬂect the
presence of intersymbol interference and additive noise. If we ignore for
0
t
q(t)
–5Ts
5Ts
–4Ts
4Ts
–3Ts
3Ts
–2Ts
2Ts
Ts
–Ts
FIGURE 15.7
Eﬀect of channel distortion on the signal pulse in Figure 15.6
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
779
the moment the possible time variations in the channel, we may express
the sampled output at the receiver as
x(nTs) =
∞

k=0
a(k)q(nTs −kTs) + w(nTs)
= a(n)q(0) +
∞

k=0
k̸=n
a(k)q(nTs −kTs) + w(nTs)
(15.7)
where w(t) represents the additive noise and q(t) represents the distorted
pulse at the output of the receiver ﬁlter.
To simplify our discussion, we assume that the sample q(0) is normal-
ized to unity by means of an automatic gain control (AGC) contained in
the receiver. Then the sampled signal given in (15.7) may be expressed as
x(n) = a(n) +
∞

k=0
k̸=n
a(k)q(n −k) + w(n)
(15.8)
where x(n) ≡x(nTs), q(n) ≡q(nTs), and w(n) ≡w(nTs). The term a(n)
in (15.8) is the desired symbol at the nth sampling instant. The second
term,
∞

k=0
k̸=n
a(k)q(n −k)
constitutes the intersymbol interference due to the channel distortion, and
w(n) represents the additive noise in the system.
In general, the channel distortion eﬀects embodied through the sam-
pled values q(n) are unknown at the receiver. Furthermore, the channel
may vary slowly with time such that the intersymbol interference eﬀects
are time-variant. The purpose of the adaptive equalizer is to compensate
the signal for the channel distortion, so that the resulting signal can be
detected reliably. Let us assume that the equalizer is an FIR ﬁlter with
M adjustable coeﬃcients h(n). Its output may be expressed as
ˆa(n) =
M−1

k=0
h(k)x(n + D −k)
(15.9)
where D is some nominal delay in processing the signal through the ﬁlter
and ˆa(n) represents an estimate of the nth information symbol. Initially,
the equalizer is trained by transmitting a known data sequence d(n).
Then, the equalizer output, ˆa(n), is compared with d(n) and an error is
generated that is used to optimize the ﬁlter coeﬃcients.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

780
Chapter 15
ADAPTIVE FILTERS
If we again adopt the least-squares error criterion, we select the coef-
ﬁcients h(k) to minimize the quantity
EM =
N

n=0

d(n) −ˆa(n)
2 =
N

n=0

d(n) −
M−1

k=0
h(k)x(n + D −k)
2
(15.10)
The result of the optimization is a set of linear equations of the form
M−1

n=0
h(k)rxx(ℓ−k) = rdx(ℓ−D),
ℓ= 0, 1, 2, . . . , M −1
(15.11)
where rxx(ℓ) is the autocorrelation of the sequence x(n) and rdx(ℓ) is
the cross-correlation between the desired sequence d(n) and the received
sequence x(n).
Although the solution of (15.11) is obtained recursively in practice (as
demonstrated in the following two sections), in principle we observe that
these equations result in values of the coeﬃcients for the initial adjustment
of the equalizer. After the short training period, which usually lasts less
than 1 second for most channels, the transmitter begins to transmit the
information sequence a(n). In order to track the possible time variations
in the channel, the equalizer coeﬃcients must continue to be adjusted in
an adaptive manner while receiving data. As illustrated in Figure 15.5,
this is usually accomplished by treating the decisions at the output of
the decision device as correct, and using the decisions in place of the
reference d(n) to generate the error signal. This approach works quite well
when decision errors occur infrequently (e.g., less than one decision error
per hundred symbols). The occasional decision errors cause only small
misadjustments in the equalizer coeﬃcients. In Sections 15.2 and 15.3, we
describe the adaptive algorithms for recursively adjusting the equalizer
coeﬃcients.
□
EXAMPLE 15.2
Channel Equalization
Consider the system conﬁguration illustrated in Figure 15.8, below. The channel
ﬁlter system function is
C(z) = 1 −2Re(z0)z−1 + |z0|2z−1
and the equalizer system function is
H(z) =
M−1

k=0
h(k)z−k
The zeros of C(z) are located at z0 = 0.8ejπ/4 and z∗
0 = 0.8e−jπ/4. The in-
put sequence a(n) is a pseudo-random sequence of ±1. The additive noise
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
781
−
Channel
Filter
C(z)
Delay
D
Channel
Equalizer
H(z)
w(n)
x(n)
d(n)
e(n)
y(n)
a(n)
â(n)
FIGURE 15.8
Channel equalization for Example 15.2
w(n) is a sample sequence of a white Gaussian noise process with variance
σ2
W = 0.1.
a. Generate the sequence a(n) and x(n) for 0 ≤n ≤1000.
b. For M = 7 and D = 10, compute the equalizer coeﬃcients h(k), 0 ≤k ≤6
based on the least-squares solution
6

k=0
h(k)rxx(ℓ−k) = rdx(ℓ),
ℓ= 0, 1, . . . , 6
where d(n) = a(n), 0 ≤n ≤1000.
c. Plot the frequency response of the channel ﬁlter C(z), the equalizer ﬁlter
H(z), and the frequency response of the cascade ﬁlter C(z)H(z). Comment
on the results.
Solution
This solution is obtained using MATLAB.
a. MATLAB script:
>> z0 = 0.8*exp(1j*pi/4);
% Zero of C(z)
>> Cb = [1,-2*real(z0),abs(z0)ˆ2]; % Numerator of C(z) or imp resp
>> N = 1000; n = 0:N;
% Length and timing indices
>> varW = 0.1;
% Additive noise variance
>> M = 7;
% FIR Equalizer length
>> an = 2*randi([0,1],N+1,1)-1;
% Pseudorandom symbol sequence
>> yn = filter(Cb,1,an);
% Distorted symbol sequence
>> wn = sqrt(varW)*randn(N+1,1);
% Additive noise sequence
>> xn = yn+wn;
% Noisy distorted symbols
b. MATLAB script:
>> [rxx,lags] = xcorr(xn,M-1); % ACRS of x(n)
>> Rxx = toeplitz(rxx(M:end)); % ACRM of x(n)
>> rdx = xcorr(an,xn,M-1);
% CCRS between d(n) and x(n)
>> rdx = rdx(M:end);
% CCRV
>> heq = Rxx\rdx;
% Equalizer coeff (or Imp resp)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

782
Chapter 15
ADAPTIVE FILTERS
c. The frequency response plots of the channel, equalizer, and the cascade ﬁlter
are computed using the following MATLAB script and shown in Figure 15.9.
om = linspace(0,1,1001)*pi;
Heq = freqz(heq,1,om);
Heq_mag = abs(Heq); Heq_pha = angle(Heq)/pi;
Cz = freqz(Cb,1,om);
Cz_mag = abs(Cz); Cz_pha = angle(Cz)/pi;
CzHeq = Cz.*Heq;
CzHeq_mag = abs(CzHeq); CzHeq_pha = angle(CzHeq)/pi;
The frequency response of the cascade ﬁlter is very nearly that of an allpass ﬁlter
indicating that the equalizer is very close to an inverse ﬁlter for the channel.□
0
0.25
1
Magnitude
0
1
2
3
Frequency Response of the Channel
–1
–0.5
0
0.5
1
0
0.25
1
Magnitude
0
1
2
3
Frequency Response of the Equalizer
–1
–0.5
0
0.5
1
0
0.25
1
Magnitude
0
1
2
3
Frequency Response of the Cascaded Filter
–1
–0.5
0
0.5
1
Phase in p Units 
Phase in p Units 
Phase in p Units 
w/p
w/p
w/p
FIGURE 15.9
Frequency response plots of the channel, equalizer, and the cascade
ﬁlter in Example 15.2(c). The dotted lines represent phase responses.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
783
15.1.3 ECHO CANCELLATION IN DATA TRANSMISSION
OVER TELEPHONE CHANNELS
In the transmission of data over telephone channels, modems (modula-
tors/demodulators) are used to provide an interface between the digital
data sequence and the analog channel. Shown in Figure 15.10 is a block
diagram of a communication system in which two terminals, labeled A and
B, transmit data by using modems A and B to interface to a telephone
channel. As shown, a digital sequence a(n) is transmitted from terminal
A to terminal B while another digital sequence b(n) is transmitted from
terminal B to A. This simultaneous transmission in both directions is
called full-duplex transmission.
As described, the two transmitted signals may be represented as
sA(t) =
∞

k=0
a(k)p(t −kTs)
(15.12)
sB(t) =
∞

k=0
b(k)p(t −kTs)
(15.13)
where p(t) is a pulse as shown in Figure 15.6.
When a subscriber wishes to transmit data over the dial-up switched
telephone network, the local communication link between the subscriber
and the local central telephone oﬃce is a two-wire line, called the local
loop. At the central oﬃce, the subscriber two-wire line is connected to
the main four-wire telephone channels that interconnect diﬀerent central
oﬃces, called trunk lines, by a device called a hybrid. By using transformer
coupling, the hybrid is tuned to provide isolation between the transmis-
sion and reception channels in full-duplex operation. However, due to
impedance mismatch between the hybrid and the telephone channel, the
level of isolation is often insuﬃcient, and, consequently, some of the signal
on the transmitter side leaks back and corrupts the signal on the receiver
Data
Terminal
A
Data
Terminal
B
Transmitter
A
Transmitter
B
Receiver
A
Receiver
B
Modem B
Modem A
Four-Wire
Telephone
Channel
a(n)
b(n)
ˆ
b(n)
a(n)
ˆ
FIGURE 15.10
Full-duplex data transmission over telephone channels
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

784
Chapter 15
ADAPTIVE FILTERS
side, causing an “echo” that is often heard in voice communications over
telephone channels.
To mitigate the echoes in voice transmissions, the telephone compa-
nies employ a device called an echo suppressor. In data transmission, the
solution is to use an echo canceller within each modem. The echo can-
cellers are implemented as adaptive ﬁlters with automatically adjustable
coeﬃcients, just as in the case of transversal (FIR) equalizers.
With the use of hybrids to couple a two-wire to a four-wire channel,
and echo cancellers at each modem to estimate and subtract the echoes,
the data communication system for the dial-up switched network takes the
form shown in Figure 15.11. A hybrid is needed at each modem to isolate
the transmitter from the receiver and to couple to the two-wire local loop.
Hybrid A is physically located at the central oﬃce of subscriber A, while
hybrid B is located at the central oﬃce to which subscriber B is connected.
The two central oﬃces are connected by a four-wire line, one pair used for
transmission from A to B and the other pair used for transmission in the
reverse direction, from B to A. An echo at terminal A due to the hybrid
A is called a near-end echo, while an echo at terminal A due to the hybrid
B is termed a far-end echo. Both types of echoes are usually present in
data transmission and must be removed by the echo canceller.
Suppose we neglect the channel distortion for purposes of this discus-
sion and deal with the echoes only. The signal received at modem A may
be expressed as
sRA(t) = A1sB(t) + A2sA(t −d1) + A3sA(t −d2)
(15.14)
where sB(t) is the desired signal to be demodulated at modem A; sA(t−d1)
is the near-end echo due to hybrid A, sA(t −d2) is the far-end echo due
Transmitter
A
Receiver
A
Echo
Canceller
Adaptive
Algorithm
Transmitter
B
Receiver
B
Echo
Canceller
Adaptive
Algorithm
Hybrid
Hybrid
Hybrid
A
Hybrid
B
Modem A
Telephone Channel
Modem B
Input
Data
Output
Data
Input
Data
Output
Data
Local
Loop
Local
Loop
+
+
+ +
−
−
FIGURE 15.11
Block diagram model of a digital communication system that uses
echo cancellers in modems
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
785
to hybrid B; Ai, i = 1, 2, 3 are the corresponding amplitudes of the three
signal components; and d1 and d2 are the delays associated with the echo
components. A further disturbance that corrupts the received signal is
additive noise, so that the received signal at modem A is
rA(t) = sRA(t) + w(t)
(15.15)
where w(t) represents the sample function of an additive process.
The adaptive echo canceller attempts to estimate adaptively the two
echo components. If its coeﬃcients are h(n), n = 0, 1, . . . , M −1, its
output is
ˆsA(n) =
M−1

k=0
h(k)a(n −k)
(15.16)
which is an estimate of the echo signal components. This estimate is sub-
tracted from the sampled received signal, and the resulting error signal
can be minimized in the least-squares sense to optimally adjust the co-
eﬃcients of the echo canceller. There are several possible conﬁgurations
for placement of the echo canceller in the modem, and for forming the
corresponding error signal. Figure 15.12 illustrates one conﬁguration, in
which the canceller output is subtracted from the sampled output of the
receiver ﬁlter with input rA(t). Figure 15.13 illustrates a second conﬁgu-
ration, in which the echo canceller is generating samples at the Nyquist
rate instead of the symbol rate; in this case, the error signal used to ad-
just the coeﬃcients is simply the diﬀerence between rA(n), the sampled
received signal, and the canceller output. Finally, Figure 15.14 illustrates
the canceller operating in combination with an adaptive equalizer.
Application of the least-squares criterion in any of the conﬁgurations
shown in Figures 15.12–15.14 leads to a set of linear equations for the
Transmitter
Filter
Receiver
Filter
Decision
Device
Symbol Rate
Sampler
Echo
Canceller
Adaptive
Algorithm
Hybrid
Input Data
a(n)
b(n)
rA(t)
ˆ
FIGURE 15.12
Symbol-rate echo canceller
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

786
Chapter 15
ADAPTIVE FILTERS
Transmitter
Filter
Receiver
Filter
Decision
Device
Nyquist Rate
Sampler
Adaptive
Algorithm
Hybrid
a(n)
b(n)
Echo
Canceller
Error Signal
ˆ
FIGURE 15.13
Nyquist rate echo canceller
Transmitter
Filter
Receiver
Filter
Decision
Device
Hybrid
Adaptive
Algorithm
Adaptive
Algorithm
a(n)
b(n)
Echo
Canceller
Error Signal
Sampler
Equalizer
Input Data
ˆ
FIGURE 15.14
Modem with adaptive equalizer and echo canceller
coeﬃcients of the echo canceller. The reader is encouraged to derive the
equations corresponding to the three conﬁgurations.
□
EXAMPLE 15.3
Echo Cancellation
Consider the system conﬁguration shown in Figure 15.15, below, in which an
echo canceller is used to suppress the interference in the detection of the desired
information sequence {b(n)}. The received sequence y(n) is expressed as
y(n) = b(n) + sA(n) + w(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
787
−
Transmit
Filter
Input
Data
Echo
Canceller
Hybrid
Receive
Filter
a(n)
y(n)
b(n)
sA
ˆ (n)
FIGURE 15.15
Echo cancellation for Example 15.3
where b(n) represents the desired received information sequence, w(n) repre-
sents the additive noise, and sA(n) represents the undesired echo to be sup-
pressed. The desired sequence b(n) is a pseudo-random sequence of ±1. The
additive noise sample sequence w(n) is white Gaussian with variance σ2
W = 0.1.
The echo signal sequence is expressed as
sA(n)
4

l=0
c(ℓ)a(n −ℓ)
where the echo coeﬃcients are given by the vector
c = [−0.25, −0.5, 0.75, 0.36, 0.25]
and the sequence a(n) is a pseudo-random sequence that takes the values ±1.
The echo canceller is an FIR ﬁlter with coeﬃcients h(k), 0 ≤k ≤M −1,
whose output is an estimate of the undesired echo, that is,
ˆsA(n) =
M−1

k=0
h(k)a(n −k)
a. Generate the sequences y(n) and sA(n), 0 ≤n ≤1000, and use the least-
squares criterion to minimize the sum of squares
N

n=0

y(n) −sA(n)
2 =
N

n=0

y(n) −
M−1

k=0
h(k)a(n −k)
2
(15.17)
and, thus, solve for the coeﬃcients h(k), 0 ≤k ≤M −1, of the echo
canceller. Select M = 10.
b. Compare the coeﬃcients of the echo canceller with the echo coeﬃcients
c(k), 0 ≤k ≤4, and comment on the results.
Solution
The minimization of the sum of squares in (15.17) lead to the following set of
equations:
M−1

k=0
h(k)raa(ℓ−k) = rya(ℓ),
ℓ= 0, 1, . . . , M −1
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

788
Chapter 15
ADAPTIVE FILTERS
where raa(ℓ) is the estimated autocorrelation sequence of a(n) and rya(ℓ) is the
estimated cross-correlation between y(n) and a(n).
a. The echo canceller coeﬃcients are obtained using the following MATLAB script.
>> c = [-0.25,-0.5,0.75,0.36,0.25]; % Echo coefficients
>> N = 1000; n = 0:N;
% Length and timing indices
>> varW = 0.1;
% Additive noise variance
>> an = 2*randi([0,1],N+1,1)-1; % Pseudorandom symbol seq at Modem-A
>> bn = 2*randi([0,1],N+1,1)-1; % Pseudorandom symbol seq at Modem-B
>> sAn = filter(c,1,an);
% Echo signal sequence at modem-A
>> wn = sqrt(varW)*randn(N+1,1); % Additive noise sequence
>> yn = bn + sAn + wn;
% Received signal at Modem-A
>> M = 10;
% FIR echo canceller order
>> [raa,lags] = xcorr(an,M-1);
% ACRS of a(n)
>> Raa = toeplitz(raa(M:end));
% ACRM of a(n)
>> rya = xcorr(yn,an,M-1);
% CCRS between y(n) and a(n)
>> rya = rya(M:end);
% CCRV
>> hec = Raa\rya;
% Echo canceller coeff (or Imp resp)
>> hec’
ans =
-0.2540
-0.4982
0.7943
0.3285
0.2291
0.0272
0.0139
0.0017
-0.0446
0.0319
b. The echo coeﬃcients are
c = [−0.25, −0.5, 0.75, 0.36, 0.25]
From the echo canceller coeﬃcients

h(k)

from (a), we observe that the ﬁrst
ﬁve coeﬃcients are close to the echo coeﬃcients while the remaining coeﬃcients
are very small.
□
15.1.4 SUPPRESSION OF NARROWBAND INTERFERENCE
IN A WIDEBAND SIGNAL
We now discuss a problem that arises in practice, especially in signal
detection and in digital communications. Let us assume that we have a
signal sequence v(n) that consists of a desired wideband signal sequence
w(n) corrupted by an additive narrowband interference sequence x(n).
The two corresponding processes W(n) and X(n), respectively, are un-
correlated. These sequences result from sampling an analog signal v(t)
at the Nyquist rate (or faster) of the wideband signal w(t). Figure 15.16
illustrates the spectral characteristics of w(n) and x(n). Usually, the in-
terference |X(ej2πf)| is much larger than |W(ej2πf)| within the narrow
frequency band that it occupies.
In digital communications and signal detection problems that ﬁt the
above model, the desired signal sequence w(n) is often a spread-spectrum
signal, while the narrowband interference represents a signal from another
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
789
|V(f )| 
 |X(f )| 
 |W(f )|
|W(f )|
|X(f )|
0
fw
f
FIGURE 15.16
Strong narrowband interference X

ej2πf	
in a wideband signal
W

ej2πf	
user of the frequency band or intentional interference from a jammer who
is trying to disrupt the communications or detection system.
Our objective from a ﬁltering viewpoint is to employ a ﬁlter that
suppresses the narrowband interference. In eﬀect, such a ﬁlter will have a
notch in the frequency band occupied by |X(ej2πf)|, and in practice, the
band occupied by |X(ej2πf)| is unknown. Moreover, if the interference is
nonstationary, its frequency band occupancy may vary with time. Hence
an adaptive ﬁlter is desired.
From another viewpoint, the narrowband characteristics of the inter-
ference allow us to estimate x(n) from past samples of the sequence v(n)
and to subtract the estimate from v(n). Since the bandwidth of x(n) is
narrow compared to the bandwidth of the sequence w(n), the samples
x(n) are highly correlated due to the high sampling rate. On the other
hand, the samples w(n) are not highly correlated, since the samples are
taken at the Nyquist rate of w(n). By exploiting the high correlation be-
tween x(n) and past samples of the sequence v(n), it is possible to obtain
an estimate of x(n), which can be subtracted from v(n).
The general conﬁguration is illustrated in Figure 15.17. The signal
v(n) is delayed by D samples, where D is selected suﬃciently large so
that the wideband signal components w(n) and w(n −D) contained in
v(n) and v(n −D), respectively, are uncorrelated. Usually, a choice of
D = 1 or 2 is adequate. The delayed signal sequence v(n −D) is passed
through an FIR ﬁlter, which is best characterized as a linear predictor of
the value x(n) based on M samples v(n −D −k), k = 0, 1, . . . , M −1.
The output of the linear predictor is
ˆx(n) =
M−1

k=0
h(k)v(n −D −k)
(15.18)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

790
Chapter 15
ADAPTIVE FILTERS
z
D
FIR
Linear
Predictor
Adaptive
Algorithm
(n) 
 w(n) 
 x(n)
(n
D)
e(n) 
 wˆ (n)
xˆ(n)
Error Signal
Decorrelation
Delay
(a)
(b)
(n
D)
xˆ(n)
z 1
z 1
z 1
z 1
h(0)
h(1)
h(2)
h(M 
1)
FIGURE 15.17
Adaptive ﬁller for estimating and suppressing a narrowband
interference in a sideband signal
This predicted value of x(n) is subtracted from v(n) to yield an estimate
of w(n), as illustrated in Figure 15.17. Clearly, the quality of the estimate
x(n) determines how well the narrowband interference is suppressed. It is
also apparent that the delay D must be kept as small as possible in order
to obtain a good estimate of x(n), but must be suﬃciently large so that
w(n) and w(n −D) are uncorrelated.
Let us deﬁne the error sequence
e(n) = v(n) −ˆx(n)
= v(n) −
M−1

k=0
h(k)v(n −D −k)
(15.19)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
791
If we apply the least-squares criterion to optimally select the prediction
coeﬃcients, we obtain the set of linear equations
M−1

k=0
h(k)rvv(ℓ−k) = rvv(ℓ+ D),
ℓ= 0, 1, . . . , M −1
(15.20)
where rvv(ℓ) is the autocorrelation sequence of v(n). Note, however, that
the right-hand side of (15.20) may be expressed as
rvv(ℓ+ D) =
N

n=0
v(n)v(n −ℓ−D)
=
N

n=0
[w(n) + x(n)][w(n −ℓ−D) + x(n −ℓ−D)]
= rww(ℓ+ D) + rxx(ℓ+ D) + rwx(ℓ+ D) + rxw(ℓ+ D)
(15.21)
The correlations in (15.21) are time-average correlation sequences.
The expected value of rww(ℓ+ D) is
E[rww(ℓ+ D)] = 0,
l = 0, 1, . . . , M −1
(15.22)
because w(n) is wideband and D is large enough that w(n) and w(n−D)
are uncorrelated. Also,
E[rxw(ℓ+ D)] = E[rwx(ℓ+ D)] = 0
(15.23)
by assumption. Finally,
E[rxx(ℓ+ D)] = RX(ℓ+ D)
(15.24)
Therefore, the expected value of rvv(ℓ+ D) is simply the statistical auto-
correlation of the narrowband process X(n). Furthermore, if the wideband
signal is weak relative to the interference, the autocorrelation rvv(ℓ) in the
left-hand side of (15.20) is approximately rxx(ℓ). The major inﬂuence of
w(n) is to the diagonal elements of rvv(ℓ). Consequently, the values of
the ﬁlter coeﬃcients determined from the linear equations in (15.20) are
a function of the statistical characteristics of the interference X(n).
The overall ﬁlter structure in Figure 15.17 is an adaptive FIR
prediction-error ﬁlter with coeﬃcients
h′(k) =
⎧
⎪
⎨
⎪
⎩
1,
k = 0
−h(k −D),
k = D, D + 1, . . . , D + M −1
0,
otherwise
(15.25)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

792
Chapter 15
ADAPTIVE FILTERS
0.00
5.00
10.00
15.00
20.00
25.00
30.00
35.00
0.00
0.06
0.12
0.19
0.25
0.31
0.37
0.44
0.50
f
Frequency (cycles/sample)
Filter Response (dB)
FIGURE 15.18
Log-magnitude response characteristics of an adaptive notch
ﬁlter
and a frequency response
H(ejω) =
D+M−1

k=0
h′(k)e−jωk
(15.26)
This overall ﬁlter acts as a notch ﬁlter for the interference. For exam-
ple, Figure 15.18 illustrates the magnitude of the frequency response of
an adaptive ﬁlter with M = 15 coeﬃcients, which attempts to suppress
a narrowband interference that occupies 20% of the frequency band of a
desired spread-spectrum signal sequence. The data was generated pseudo-
randomly by adding a narrowband interference consisting of 100 randomly
phased, equal-amplitude sinusoids to a pseudo-noise spread-spectrum sig-
nal. The coeﬃcients of the ﬁlter were obtained by solving the equations
in (15.20), with D = 1, where the correlation rvv(l) was obtained from
the data. We observe that the overall interference suppression ﬁlter has
the characteristics of a notch ﬁlter. The depth of the notch depends
on the power of the interference relative to the wideband signal. The
stronger the interference, the deeper the notch.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
793
−
FIR
Linear
Predictor
Delay
D = 1
x(n)
wˆ
ˆ
(n)
(n) = w(n) + x(n)
FIGURE 15.19
System for narrowband interference suppression in Example 15.4
The algorithms presented in Section 15.2 are appropriate for estimat-
ing the predictor coeﬃcients continuously, in order to track a nonstation-
ary narrowband interference signal.
□
EXAMPLE 15.4
Suppression of Narrowband Interference
Consider the system conﬁguration shown in Figure 15.19. The input sequence
v(n) is the sum of a wideband signal sequence w(n) and a narrowband inter-
ference sequence x(n). The sequence w(n) is a white Gaussian noise sequence
with variance σ2
w = 1. The narrowband interference sequence consists of a sum
of sinusoids and is expressed as
x(n) = A
100

i=0
cos(2πfin + θi)
where fi = 0.1i/100, i = 0, 1, . . . , 99, θi is uniformly distributed on the interval
(0, 2π) for each i, and the scale factor A = 1. Note that the bandwidth of x(n)
is 0.1 cycles/sample. The estimate of the narrowband interference at the output
of the linear predictor is
ˆx(n) =
M−1

k=0
h(k)v(n −1 −k)
and the error sequence e(n) = v(n)−ˆx(n) produces an estimate of the wideband
signal sequence w(n). Application of the least-squares criterion results in the
set linear equations given in (15.20) for the predictor coeﬃcients.
a. Let M = 15, and generate 2000 samples of the sequence v(n) and solve
(15.20) for the predictor coeﬃcients h(k), k = 0, 1, . . . , 14.
b. The FIR prediction-error ﬁlter having coeﬃcients h′(k), k = 0, 1, . . . , 15, is
deﬁned by (15.25) and its frequency response is
H

ej2πf	
=
M

k=0
h′(k)e−j2πfk,
|f| ≤1
2
Compute H

ej2πf	
and thus illustrate via a plot of 20 log


H

ej2πf	


that
the prediction-error ﬁlter is a notch ﬁlter that suppresses the narrow band
interference contained in the sequence v(n).
c. Repeat parts (a) and (b) when the amplitude of the interference term
A = 1/10.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

794
Chapter 15
ADAPTIVE FILTERS
Solution
The narrowband interference signal is obtained using the following MATLAB
script.
>> i = (0:100)’; fi = 0.1*i/100; thetai = 2*pi*rand(length(i),1);
>> A = 1;
>> N = 2000; n = 0:N;
>> xn = sum(A*cos(2*pi*fi*n+thetai*ones(1,N+1))); xn = xn’;
a. The predictor coeﬃcients are obtained using this MATLAB script:
>> M = 15; D = 1;
>> varW = 1;
>> wn = sqrt(varW)*randn(N+1,1);
>> vn = xn+wn;
>> [rvv,lags] = xcorr(vn,M-1+D);
% ACRS of v(n)
>> Rvv = toeplitz(rvv(M+D:2*M+D-1)); % ACRM of v(n)
>> rv = rvv(M+2*D:end);
>> h = Rvv\rv;
b. Using (15.25), we obtain the predictor error coeﬃcients and its log-magnitude
response:
>> h1 = zeros(M+D,1);
>> h1(1) = 1;
>> h1(D+1:D+M) = -h;
>> f = linspace(0,1,1001)*0.5;
>> H1 = freqz(h1,1,2*pi*f);
>> H1db = 20*log10(abs(H1)/max(abs(H1)));
>> % Plotting commands follow
The plot of the log-magnitude response is shown in Figure 15.20, from which
we observe that the prediction error ﬁlter creates a notch of about 20 dB down
at the bandwidth of the narrowband interfering signal.
c. The scale factor is now reduced to A = 1/10 and new results are obtained.
>> i = (0:100)’; fi = 0.1*i/100; thetai = 2*pi*rand(length(i),1);
>> A = 1/10;
>> N = 2000; n = 0:N;
>> xn = sum(A*cos(2*pi*fi*n+thetai*ones(1,N+1))); xn = xn’;
>> M = 15;
>> D = 1;
>> varW = 1;
>> wn = sqrt(varW)*randn(N+1,1);
>> vn = xn+wn;
>> [rvv,lags] = xcorr(vn,M-1+D);
% ACRS of v(n)
>> Rvv = toeplitz(rvv(M+D:2*M+D-1)); % ACRM of v(n)
>> rv = rvv(M+2*D:end);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
795
Normalized Frequency f
0
0.1
0.5
Decibels
–30
–20
–10
0
Log-Magnitude Response of Prediction-Error Filter: A=1
FIGURE 15.20
Log-magnitude response of the prediction-error ﬁlter in Example
15.4(b)
>> h = Rvv\rv;
>> h1 = zeros(M+D,1);
>> h1(1) = 1;
>> h1(D+1:D+M) = -h;
>> f = linspace(0,1,1001)*0.5;
>> H1 = freqz(h1,1,2*pi*f);
>> H1db = 20*log10(abs(H1)/max(abs(H1)));
>> % Plotting commands follow
The resulting log-magnitude response is shown in Figure 15.21. Now the notch
in the frequency response is much shallower at around 5 dB down.
□
15.1.5 ADAPTIVE LINE ENHANCER
In the preceding example, the adaptive linear predictor was used to es-
timate the narrowband interference for the purpose of suppressing the
interference from the input sequence v(n). An adaptive line enhancer
(ALE) has the same conﬁguration as the interference suppression ﬁlter
in Figure 15.17, except that the objective is diﬀerent.
In the adaptive line enhancer, x(n) is the desired signal and w(n) rep-
resents a wideband noise component that masks x(n). The desired signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

796
Chapter 15
ADAPTIVE FILTERS
Normalized Frequency f
0
0.1
0.5
Decibels
–30
–5
0
Log-Magnitude Response of Prediction-Error Filter: A=0.1
FIGURE
15.21
Log-magnitude
response
of
the
prediction-error
ﬁlter
in
Example 15.4(c)
x(n) is either a spectral line or a relatively narrowband signal. The linear
predictor shown in Figure 15.17(b) operates in exactly the same fashion
as that in Figure 15.17(a) and provides an estimate of the narrowband
signal x(n). It is apparent that the ALE (i.e., the FIR prediction ﬁlter)
is a self-tuning ﬁlter that has a peak in its frequency response at the
frequency of the sinusoid or, equivalently, in the frequency band of the
narrowband signal x(n). By having a narrow bandwidth, the noise w(n)
outside of the band is suppressed and, thus, the spectral line is enhanced
in amplitude relative to the noise power in w(n). This explains why the
FIR predictor is called an ALE. Its coeﬃcients are determined by the
solution of (15.20).
□
EXAMPLE 15.5
Estimation of Narrowband Signal in Wideband Noise
Consider the system conﬁguration given in Example 15.4, where now the se-
quence x(n) is deﬁned as
x(n) = A cos(2πf0n + θ0)
The objective is now to estimate x(n) in the presence of the wideband noise
sequence w(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
797
a. Let M = 15, A = 1, f0 = 0.2, and θ0 = 0, and generate 2000 samples
of the sequence v(n). Solve (15.20) for the predictor coeﬃcients h(k), k =
0, 1, . . . , 14. Then plot given signal x(n), the noisy signal v(n), and the
estimated signal ˆx(n) for 50 < n ≤150. Comment on the quality of the
estimate.
b. Compute the frequency response H

ej2πf	
of the FIR prediction ﬁlter and
plot 20 log


H

ej2πf	


for |f| ≤1
2. Comment on the characteristic of the
ﬁlter.
c. Repeat parts (a) and (b) when A =
1
10 and when A = 10.
d. Repeat parts (a) and (b) when the sequence x(n) contains two frequency
components of the form
x(n) = A cos(2πf1n + θ1) + B cos(2πf2n + θ2)
where f1 = 0.1, f2 = 0.3, θ1 = 0, θ2 = π, and A = B = 1. Comment on the
results of ˆx(n) and H

ej2πf	
.
Solution
The sequence x(n) is obtained using
>> N = 2000; n = (0:N-1)’;
>> xn = A*cos(2*pi*f0*n+th0);
a. The predictor coeﬃcients and the plots are obtained using the following
MATLAB script.
>> M = 15; A = 1; f0 = 0.2; th0 = 0; D = 1;
>> varW = 1;
>> wn = sqrt(varW)*randn(N,1);
>> vn = xn + wn;
>> [rvv] = xcorr(vn,M-1+D);
% ACRS of v(n)
>> Rvv = toeplitz(rvv(M+D:2*M+D-1)); % ACRM of v(n)
>> rv = rvv(M+2*D:end);
>> h = Rvv\rv;
>> xhatn = filter(h,1,[zeros(D,1);vn]);
>> % Plotting commands follow
The plots are shown in Figure 15.22 in which we show samples from n = 50
to n = 150 for clarity. The optimal ﬁlter has a remarkable performance given
how noisy the signal v(n) is. The single tone signal x(n) is clearly enhanced.
b. MATLAB script:
>> f = linspace(-0.5,0.5,1001);
>> Hf = freqz(h,1,2*pi*f);
>> Hfdb = 20*log10(abs(Hf)/max(abs(Hf)));
>> % Plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

798
Chapter 15
ADAPTIVE FILTERS
n
50
70
90
110
130
150
Amplitude
–3
0
3
Original Signal x(n)
n
50
70
90
110
130
150
Amplitude
–3
0
3
Noisy Signal v(n)
n
50
70
90
110
130
150
Amplitude
–3
0
3
Estimated Signal x(n)
FIGURE 15.22
Plots of the original signal, noisy signal, and estimated signal in
Example 15.5(a)
The log-magnitude plot in decibels is shown in Figure 15.23. Observe that
the response has a peak at f = ±0.2 and all other peaks are approximately
10 dB below these peaks.
c. The scripts are almost similar with obvious changes to the scale factor A.
>> % A = 1/10
>> M = 15; A = 1/10; f0 = 0.2; th = 0; D = 1;
>> N = 2000; n = (0:N-1)’;
>> xn = A*cos(2*pi*f0*n+th);
>> varW = 1;
>> wn = sqrt(varW)*randn(N,1);
>> vn = xn + wn;
>> [rvv,lags] = xcorr(vn,M-1+D);
% ACRS of v(n)
>> Rvv = toeplitz(rvv(M+D:2*M+D-1)); % ACRM of v(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
799
Normalized Frequency f
–0.5
–0.2
0
0.2
0.5
Decibels
–40
–30
–20
–10
0
Log-Magnitude Response of Prediction Filter: A=1
FIGURE 15.23
Log-magnitude plot in Example 15.5(b)
>> rv = rvv(M+2*D:end);
>> h = Rvv\rv;
>> xhatn = filter(h,1,[zeros(D,1);vn]);
>> f = linspace(-0.5,0.5,1001);
>> Hf = freqz(h,1,2*pi*f);
>> Hfdb = 20*log10(abs(Hf)/max(abs(Hf)));
>>
>> % A = 10
>> M = 15; A = 10; f0 = 0.2; th = 0; D = 1;
>> N = 2000; n = (0:N-1)’;
>> xn = A*cos(2*pi*f0*n+th);
>> varW = 1;
>> wn = sqrt(varW)*randn(N,1);
>> vn = xn + wn;
>> [rvv,lags] = xcorr(vn,M-1+D);
% ACRS of v(n)
>> Rvv = toeplitz(rvv(M+D:2*M+D-1)); % ACRM of v(n)
>> rv = rvv(M+2*D:end);
>> h = Rvv\rv;
>> xhatn = filter(h,1,[zeros(D,1);vn]);
>> f = linspace(-0.5,0.5,1001);
>> Hf = freqz(h,1,2*pi*f);
>> Hfdb = 20*log10(abs(Hf)/max(abs(Hf)));
The plots are shown in Figure 15.24. The plots in the left-hand column
correspond to A = 1/10. Clearly, the noise is stronger in this case, and
consequently the estimation of x(n) suﬀers. Although most of the noise is
eliminated, the signal is also distorted. This is also evident from the ﬁlter’s
log-magnitude plot. Although there is a peak at f = 0.2, there are other
signiﬁcant peaks also present in the response. The plots in the right-hand
column correspond to A = 10. In this case, the signal x(n) is much stronger
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

800
Chapter 15
ADAPTIVE FILTERS
n
50
70
90
110
Amplitude
–1
0
1
A = 1/10
Original Signal x(n)
n
50
70
90
110
Amplitude
–1
0
1
Noisy Signal v(n)
n
50
70
90
110
Amplitude
–1
0
1
Estimated Signal x(n)
Normalized Frequency f
–0.5
–0.2
0
0.2
0.5
Decibels
–20
–10
0
Log-Magnitude Response
n
50
70
90
110
Amplitude
–10
–5
0
5
10
A = 10
Original Signal x(n)
n
50
70
90
110
Amplitude
–10
–5
0
5
10
Noisy Signal v(n)
n
50
70
90
110
Amplitude
–10
–5
0
5
10
Estimated Signal x(n)
Normalized Frequency f
–0.5
–0.2
0
0.2
0.5
Decibels
–20
–10
0
Log-Magnitude Response
FIGURE 15.24
Signal and log-magnitude plots in Example 15.5(c)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
801
than noise, and consequently the estimation of x(n) is almost distortion-
free. The ﬁlter’s log-magnitude plot shows a stronger and narrower peak at
f = 0.2, thus eliminating most of the noise.
d. In this case, we change x(n) by adding another sinusoid and then execute
the previous script.
>> M = 15; D = 1;
>> A = 1; f1 = 0.1; th1 = 0;
>> B = 1; f2 = 0.3; th2 = pi;
>> N = 2000; n = (0:N-1)’;
>> xn = A*cos(2*pi*f1*n+th1) + B*cos(2*pi*f2*n+th2);
>> varW = 1;
>> wn = sqrt(varW)*randn(N,1);
>> vn = xn + wn;
>> [rvv,lags] = xcorr(vn,M-1+D);
% ACRS of v(n)
>> Rvv = toeplitz(rvv(M+D:2*M+D-1)); % ACRM of v(n)
>> rv = rvv(M+2*D:end);
>> h = Rvv\rv;
>> xhatn = filter(h,1,[zeros(D,1);vn]);
>> f = linspace(-0.5,0.5,1001);
>> Hf = freqz(h,1,2*pi*f);
>> Hfdb = 20*log10(abs(Hf)/max(abs(Hf)));
The signal and log-magnitude plots are shown in Figure 15.25. The ﬁlter’s
log-magnitude plot shows that although the response peaks are at the correct
locations, f1 = 0.1 and f2 = 0.3 cycles/sample, these peaks are broad and let
some noise power go through. Consequently, the estimation of two sinusoid
is not perfect and the waveform is somewhat distorted.
□
15.1.6 ADAPTIVE NOISE CANCELLING
Echo cancellation, the suppression of narrowband interference in a wide-
band signal, and the ALE are related to another form of adaptive ﬁltering
called adaptive noise cancelling. A model for the adaptive noise canceller
is illustrated in Figure 15.26.
The primary input signal consists of a desired signal sequence x(n)
corrupted by an additive noise sequence w1(n) and an additive inter-
ference (noise) w2(n). The additive interference (noise) is also observ-
able after it has been ﬁltered by some unknown linear system that yields
v2(n) and is further corrupted by an additive noise sequence w3(n). Thus
we have available a secondary signal sequence, which may be expressed
as v(n) = v2(n) + w3(n). The sequences w1(n), w2(n), and w3(n) are
assumed to be mutually uncorrelated and zero mean.
As shown in Figure 15.26, an adaptive FIR ﬁlter is used to esti-
mate the interference sequence w2(n) from the secondary signal v(n) and
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

802
Chapter 15
ADAPTIVE FILTERS
n
50
70
90
110
130
150
Amplitude
–2
0
2
Original Signal x(n)
n
50
70
90
110
130
150
Amplitude
–2
0
2
Noisy Signal v(n)
n
50
70
90
110
130
150
Amplitude
–2
0
2
Estimated Signal x(n)
Normalized Frequency f 
–0.5
–0.3
–0.1
0
0.1
0.3
0.5
Decibels
–20
–10
0
Log-Magnitude Response
FIGURE 15.25
Signal and log-magnitude plots in Example 15.5(d)
subtract the estimate ˆw2(n) from the primary signal. The output se-
quence, which represents an estimate of the desired signal x(n), is the
error signal
e(n) = y(n) −ˆw2(n)
= y(n) −
M−1

k=0
h(k)v(n −k)
(15.27)
This error sequence is used to adaptively adjust the coeﬃcients of the
FIR ﬁlter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
803
Adaptive
Algorithm
Unknown
Linear
System H(z)
Adaptive
FIR
Filter
x(n) 
 w1(n)
w2(n)
w3(n)
wˆ 2(n)
Observed Signal y(n)
Observed Signal (n)
Output
Error
Signal
2(n)
FIGURE 15.26
Example of an adaptive noise-cancelling system
If the least-squares criterion is used to determine the ﬁlter coeﬃcients,
the result of the optimization is the set of linear equations
M−1

k=0
h(k)rvv(ℓ−k) = ryv(ℓ),
ℓ= 0, 1, . . . , M −1
(15.28)
where rvv(ℓ) is the sample (time-average) autocorrelation of the sequence
v(n) and ryv(ℓ) is the sample cross-correlation of the sequences y(n) and
v(n). Clearly, the noise cancelling problem is similar to the last three
adaptive ﬁltering applications described above.
□
EXAMPLE 15.6
Noise Cancellation
Consider the system conﬁguration shown in Figure 15.27. The sequence x(n) is
a sample sequence of a broadband signal that is modeled as a white Gaussian
noise process with variance σ2
X. The sequences w2(n) and w3(n) are sample
sequences of white Gaussian noise processes with equal variance σ2
W. Sequences
x(n), w2(n), and w3(n) are mutually uncorrelated. The linear system has a
system function
H(z) =
1
1 −1
2z−1
The objective is to design a linear prediction ﬁlter that estimates and cancels
out the noise component w2(n).
a. Consider the following parameters: predictor order M = 10,σ2
W = 1,σ2
X = 2.
Generate 2000 samples of the sequences x(n), w2(n), and w3(n), and the
sequences y(n), v2(n), and v(n). Then determine the predictor coeﬃcients
that are optimal in the least-squares sense.
b. Plot y(n), x(n), and ˆx(n) on the same graph and comment on the quality
of the noise cancellation scheme.
c. Repeat parts (a) and (b) when σ2
X = 5 and when σ2
X = 0.5. Comment on
the results.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

804
Chapter 15
ADAPTIVE FILTERS
−
FIR
Predictor
H(z)
x (n)
x (n)
y (n)
2(n)
w2(n)
w2(n)
w3(n)
(n)
FIGURE 15.27
Noise cancellation in Example 15.6
Solution
The unknown linear system is speciﬁed using
>> Hzb = 1; Hza = [1-0.5]; % Unknown LTI system parameters
a. The speciﬁed signal and the predictor coeﬃcients are obtained using the
following MATLAB script.
>> M = 10; varW = 1; varX = 2;
>> N = 2000; n = 0:N-1;
>> xn = sqrt(varX)*randn(N,1);
>> w2n = sqrt(varW)*randn(N,1);
>> w3n = sqrt(varW)*randn(N,1);
>> v2n = filter(Hzb,Hza,w2n);
>> yn = xn+w2n;
>> vn = v2n+w3n;
>> rvv = xcorr(vn,M-1);
>> Rvv = toeplitz(rvv(M:end)); % ACRM of v(n)
>> ryv = xcorr(yn,vn,M-1);
% CCRS between y(n) and v(n)
>> ryv = ryv(M:end);
% CCRV
>> hnc = Rvv\ryv;
% Noise canceller coeff (or Imp resp)
b. The ˆx(n) is obtained using
>> w2hatn = filter(hnc,1,vn);
>> xhatn = yn - w2hatn;
The signal plots from n = 50 to n = 150 are shown in Figure 15.28. The
wideband signal x(n) is restored close to its original waveform from its noisy
observation, and thus its noise is almost cancelled.
c. We now execute the previous MATLAB script with diﬀerent values for σX.
>> % (c1) VarX = 5
>> M = 10; varW = 1; varX = 5;
>> N = 2000; n = 0:N-1;
>> xn = sqrt(varX)*randn(N,1);
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
805
n
50
70
90
110
130
150
Amplitude
–4
–2
0
2
4
Original Signal x(n): VarX = 2
n
50
70
90
110
130
150
Amplitude
–4
–2
0
2
4
Noisy Signal y(n)
n
50
70
90
110
130
150
Amplitude
–4
–2
0
2
4
Noise-Cancelled Signal xhat(n)
FIGURE 15.28
Signal plots in Example 15.6(b)
>> w2n = sqrt(varW)*randn(N,1);
>> w3n = sqrt(varW)*randn(N,1);
>> v2n = filter(Hzb,Hza,w2n);
>> yn = xn+w2n;
>> vn = v2n+w3n;
>> rvv = xcorr(vn,M-1);
>> Rvv = toeplitz(rvv(M:end)); % ACRM of v(n)
>> ryv = xcorr(yn,vn,M-1);
% CCRS between y(n) and v(n)
>> ryv = ryv(M:end);
% CCRV
>> hnc = Rvv\ryv;
% Noise canceller coeff (or Imp resp)
>> w2hatn = filter(hnc,1,vn);
>> xhatn = yn - w2hatn;
>> % Plotting commands follow
The resulting signal plots are shown in Figure 15.29. In this case, the sig-
nal is much stronger in power than noise samples. Hence the signal and its
estimated waveforms are almost identical.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

806
Chapter 15
ADAPTIVE FILTERS
n
50
70
90
110
130
150
Amplitude
–6
–3
0
3
6
Original Signal x(n): VarX = 5
n
50
70
90
110
130
150
Amplitude
–6
–3
0
3
6
Noisy Signal y(n)
n
50
70
90
110
130
150
Amplitude
–6
–3
0
3
6
Noise-Cancelled Signal xhat(n)
FIGURE 15.29
Signal plots in Example 15.6(c)1
>> % (c2) VarX = 0.5
>> M = 10; varW = 1; varX = 0.5;
>> N = 2000; n = 0:N-1;
>> xn = sqrt(varX)*randn(N,1);
>> w2n = sqrt(varW)*randn(N,1);
>> w3n = sqrt(varW)*randn(N,1);
>> v2n = filter(Hzb,Hza,w2n);
>> yn = xn+w2n;
>> vn = v2n+w3n;
>> rvv = xcorr(vn,M-1);
>> Rvv = toeplitz(rvv(M:end)); % ACRM of v(n)
>> ryv = xcorr(yn,vn,M-1);
% CCRS between y(n) and v(n)
>> ryv = ryv(M:end);
% CCRV
>> hnc = Rvv\ryv;
% Noise canceller coeff (or Imp resp)
>> w2hatn = filter(hnc,1,vn);
>> xhatn = yn - w2hatn;
>> % Plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
807
n
Amplitude
–3
0
3
Original Signal x(n): VarX = 0.5
n
Amplitude
–3
0
3
Noisy Signal y(n)
n
50
70
90
110
130
150
50
70
90
110
130
150
50
70
90
110
130
150
Amplitude
–3
0
3
Noise-Cancelled Signal xhat(n)
FIGURE 15.30
Signal plots in Example 15.6(c)2
The resulting signal plots are shown in Figure 15.30. In this case, the signal
is much weaker in power than noise samples. Hence the estimated signal
waveform is distorted compared to the original signal.
□
15.1.7 LINEAR PREDICTIVE CODING OF SPEECH SIGNALS
Various methods have been developed over the past four decades for dig-
ital encoding of speech signals. In the telephone system, for example, two
commonly used methods for speech encoding are pulse code modulation
(PCM) and diﬀerential PCM (DPCM). These are examples of waveform-
coding methods. Other waveform-coding methods have also been devel-
oped, such as delta modulation (DM) and adaptive DPCM.
Since the digital speech signal is ultimately transmitted from the
source to a destination, a primary objective in devising speech encoders
is to minimize the number of bits required to represent the speech sig-
nal while maintaining speech intelligibility. This objective has led to the
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

808
Chapter 15
ADAPTIVE FILTERS
development of a class of low bit-rate (10,000 bits per second and be-
low) speech-encoding methods that are based on constructing a model of
the speech source and transmitting the model parameters. Adaptive ﬁl-
tering ﬁnds application in these model-based speech-coding systems. We
describe one very eﬀective method called linear predictive coding (LPC).
In LPC, the vocal tract is modeled as a linear all-pole ﬁlter having
the system function
H(z) =
G
1 −
p
k=1 akz−k
(15.29)
where p is the number of poles, G is the ﬁlter gain, and ak are the parame-
ters that determine the poles. There are two mutually exclusive excitation
functions, used to model voiced and unvoiced speech sounds. On a short-
time basis, voiced speech is periodic with a fundamental frequency F0, or
a pitch period 1/F0, which depends on the speaker. Thus voiced speech
is generated by exciting the all-pole ﬁlter model by a periodic impulse
train with a period equal to the desired pitch period. Unvoiced speech
sounds are generated by exciting the all-pole ﬁlter model by the output
of a random-noise generator. This model is shown in Figure 15.31.
Given a short-time segment of a speech signal, the speech encoder at
the transmitter must determine the proper excitation function, the pitch
period for voiced speech, the gain parameter G, and the coeﬃcients {ak}.
A block diagram that illustrates the source encoding system is given in
Figure 15.32. The parameters of the model are determined adaptively from
the data. Then the speech samples are synthesized by using the model,
and an error signal sequence is generated (as shown in Figure 15.32) by
taking the diﬀerence between the actual and the synthesized sequence.
The error signal and the model parameters are encoded into a binary
sequence and transmitted to the destination. At the receiver, the speech
signal is synthesized from the model and the error signal.
The parameters of the all-pole ﬁlter model are easily determined from
the speech samples by means of linear prediction. To be speciﬁc, consider
White
Noise
Generator
Periodic
Impulse
Generator
All-Pole
Filter
Voiced and
Unvoiced Switch
Speech
Signal
FIGURE 15.31
Block diagram model for the generation of a speech signal
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
809
Determine
Parameters
of Model
and
Excitation
E
n
c
o
d
e
r
All-Pole
Model
Channel
Sampled
Output from
Speech
Source
Excitation
e(n)
x(n)
xˆ(n)
xˆ(n)
Parameter {ak,G} 
FIGURE 15.32
Source encoder for a speech signal
the system shown in Figure 15.33 and assume that we have N signal
samples. The output of the FIR ﬁlter is
ˆx(n) =
p

k=1
akx(n −k)
(15.30)
and the corresponding error between the observed sample x(n) and the
estimate ˆx(n) is
e(n) = x(n) −
p

k=1
akx(n −k)
(15.31)
By applying the least-squares criterion, we can determine the model
parameters ak. The result of this optimization is a set of linear equations
p

k=1
akrxx(ℓ−k) = rxx(ℓ),
ℓ= 1, 2, . . . , p
(15.32)
Adaptive
Algorithm
Adaptive
FIR
Predictor
z 1
Speech Samples
Error Signal
x(n)
xˆ(n)
FIGURE 15.33
Estimation of pole parameters in LPC
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

810
Chapter 15
ADAPTIVE FILTERS
where rxx(ℓ) is the time-average autocorrelation of the sequence x(n).
The gain parameter for the ﬁlter can be obtained by noting that its input-
output equation is
x(n) =
p

k=1
akx(n −k) + Gv(n)
(15.33)
where v(n) is the input sequence. Clearly,
Gv(n) = x(n) −
p

k=1
akx(n −k)
= e(n)
Then
G2
N−1

n=0
v2(n) =
N−1

n=0
e2(n)
(15.34)
If the input excitation is normalized to unit energy by design, then
G2 =
N−1

n=0
e2(n)
= rxx(0) −
p

k=1
akrxx(k)
(15.35)
Thus G2 is set equal to the residual energy resulting from the least-squares
optimization.
In this development, we have described the use of linear prediction
to adaptively determine the pole parameters and the gain of an all-pole
ﬁlter model for speech generation. In practice, due to the nonstationary
character of speech signals, this model is applied to short-time segments
(10 to 20 milliseconds) of a speech signal. Usually, a new set of parameters
is determined for each short-time segment. However, it is often advanta-
geous to use the model parameters measured from previous segments to
smooth out sharp discontinuities that usually exist in estimates of model
parameters obtained from segment to segment. Although our discussion
was totally in terms of the FIR ﬁlter structure, we should mention that
speech synthesis is usually performed by using the FIR lattice structure
and the reﬂection coeﬃcients Ki. Since the dynamic range of the Ki is
signiﬁcantly smaller than that of the ak, the reﬂection coeﬃcients require
fewer bits to represent them. Hence the Ki are transmitted over the chan-
nel. Consequently, it is natural to synthesize the speech at the destination
using the all-pole lattice structure described previously in Section 14.5.1.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
811
−
FIR
Linear
Predictor
Delay
D = 1
x(n)
ˆ
x(n)
e(n)
FIGURE 15.34
Estimation of pole positions in Example 15.7
In our treatment of LPC for speech coding, we have not considered
algorithms for the estimation of the excitation and the pitch period. A
discussion of appropriate algorithms for these parameters of the model
would take us too far aﬁeld and, hence, are omitted. The interested reader
is referred to Rabiner and Schafer [84] and Deller, Hansen, and Proakis
[11] for a detailed treatment of speech analysis and synthesis methods.
□
EXAMPLE 15.7
Estimation of Poles in an All-Pole System
Consider an autoregressive process described by the diﬀerence equation
x(n) = 1.26x(n −1) −0.81x(n −2) + w(n)
(15.36)
where w(n) is a sample sequence of a white Gaussian noise process with variance
σ2
W = 0.1. The system conﬁguration for estimation of the poles is shown in
Figure 15.34.
a. Generate a sequence x(n) for 0 ≤n ≤1000, and use the least-squares
criterion to determine the parameters of a second-order predictor.
b. Determine the zeros of the prediction-error ﬁlter and compare them to the
poles of the system that generated x(n).
c. Repeat parts (a) and (b) when σ2
W = 0.5 and when σ2
W = 1. Comment on
the results.
Solution
The parameters of the second-order AR model are entered using
>> b = 1; a = [1,-1.26,0.81]; % Parameters of the AR model
a. Sequence generation and the parameters of the second-order predictor are
obtained using the following MATLAB script.
>> varW = 0.1; M = 2;
>> N = 1000; n = 0:N-1;
>> wn = sqrt(varW)*randn(N,1);
>> xn = filter(b,a,wn);
>> [rxx] = xcorr(xn,M);
% ACRS of x(n)
>> Rxx = toeplitz(rxx(M+1:2*M)); % ACRM of x(n)
>> rdx = xcorr(xn,M-1);
% CCRS between d(n) and x(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

812
Chapter 15
ADAPTIVE FILTERS
>> rx = rxx(M+2:end);
% CCRV
>> hpr = Rxx\rx;
% Predictor coeff (or Imp resp)
>> hpr’
ans =
1.2416
-0.8206
The predictor coeﬃcients almost agree with the coeﬃcients on the right-hand
side of (15.36).
b. MATLAB script:
>> hpe = [1;-hpr]; % Prediction error imp response
>> hpe_zeros = roots(hpe);
>> fprintf(’Zeros and Poles when varW = 0.1\n’);
>> fprintf([’
Zeros of the Prediction Error Filter are: ’,...
’\n
%6.4f+j%6.4f, %6.4f-j%6.4f\n\n’],...
real(hpe_zeros(1)),imag(hpe_zeros(1)),...
real(hpe_zeros(2)),-imag(hpe_zeros(2)));
>> AR_poles = roots(a);
>> fprintf([’
Poles of the AR Model are: ’,...
’\n
%6.4f+j%6.4f, %6.4f-j%6.4f\n\n’],...
real(AR_poles(1)),imag(AR_poles(1)),...
real(AR_poles(2)),-imag(AR_poles(2)));
The screen printout is
Zeros and Poles when varW = 0.1
Zeros of the Prediction Error Filter are:
0.6208+j0.6597, 0.6208-j0.6597
Poles of the AR Model are:
0.6300+j0.6427, 0.6300-j0.6427
Clearly, the poles and zeros are close to each other.
c. A similar MATLAB script with diﬀerent values for σ2
W gives similar results.
Zeros and Poles when varW = 0.5
Zeros of the Prediction Error Filter are:
0.6296+j0.6345, 0.6296-j0.6345
Poles of the AR Model are:
0.6300+j0.6427, 0.6300-j0.6427
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Applications of Adaptive Filters
813
Zeros and Poles when varW = 1.0
Zeros of the Prediction Error Filter are:
0.6309+j0.6365, 0.6309-j0.6365
Poles of the AR Model are:
0.6300+j0.6427, 0.6300-j0.6427
Even though we change the AR model input variance, the estimation of pre-
dictor model coeﬃcients are obtained by the ratio of autocorrelation values,
which remains unaﬀected within the statistical variation.
□
15.1.8 ADAPTIVE ARRAYS
In the previous examples, we considered adaptive ﬁltering performed on
a single data sequence. However, adaptive ﬁltering has also been widely
applied to multiple data sequences that result from antenna, hydrophone,
and seismometer arrays, where the sensors (antennas, hydrophones, or
seismometers) are arranged in some spatial conﬁguration. Each element
of the array of sensors provides a signal sequence. By properly combining
the signals from the various sensors, it is possible to change the direc-
tivity pattern of the array. For example, consider a linear antenna array
consisting of ﬁve elements, as shown in Figure 15.35(a). If the signals are
simply linearly summed, we obtain the sequence
x(n) =
5

k=1
xk(n)
(15.37)
which results in the antenna directivity pattern shown in Figure 15.35(a).
Now, suppose that an interference signal is received from a direction cor-
responding to one of the sidelobes in the array. By properly weighting
the sequences xk(n) prior to combining, it is possible to alter the side-
lobe pattern such that the array contains a null in the direction of the
interference, as shown in Figure 15.35(b). Thus we obtain
x(n) =
5

k=1
hkxk(n)
(15.38)
where the hk are the weights.
We may also change or steer the direction of the main antenna lobe
by simply introducing delays in the output of the sensor signals prior to
combining. Hence from K sensors, we have a combined signal of the form
x(n) =
K

k=1
hkxk(n −nk)
(15.39)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

814
Chapter 15
ADAPTIVE FILTERS
Look Direction
x1(n)
x2(n)
x3(n)
x4(n)
x5(n)
To Combiner
Look Direction
x1(n)
x2(n)
x3(n)
x4(n)
x5(n)
To Combiner
Interference
Main Lobe
Side Lobes
(a)
(b)
Interference
FIGURE 15.35
Linear antenna array: (a) linear antenna array with antenna
pattern; (b) linear antenna array with a null placed in the direction of the
interference
where the hk are the weights and nk corresponds to an nk-sample delay
in the signal x(n). The choice of weights may be used to place nulls in
speciﬁc directions.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
815
More generally, we may simply ﬁlter each sequence prior to combining.
In such a case, the output sequence has the general form
y(n) =
K

k=1
yk(n)
=
K

k=1
M−1

ℓ=0
hk(ℓ)xk(n −nk −ℓ)
(15.40)
where hk is the impulse response of the ﬁlter for processing the kth sensor
output and the nk are the delays that steer the beam pattern.
The LMS algorithm described in Section 15.2.2 is frequently used in
adaptively selecting the weights hk or the impulse responses hk(ℓ). The
more powerful recursive least-squares algorithms described can also be
applied to the multisensor (multichannel) data problem.
In the treatment given in this chapter, we deal with single-channel
(sensor) signals.
15.2 ADAPTIVE DIRECT-FORM FIR FILTERS
From the examples of the previous section, we observed that there is a
common framework in all the adaptive ﬁltering applications. The least-
squares criterion that we have adopted leads to a set of linear equations
for the ﬁlter coeﬃcients, which may be expressed as
M−1

k=0
h(k)rxx(ℓ−k) = rdx(ℓ+ D),
ℓ= 0, 1, 2, . . . , M −1
(15.41)
where rxx(ℓ) is the autocorrelation of the sequence x(n) and rdx(ℓ) is the
cross-correlation of the sequences d(n) and x(n). The delay parameter D
is zero in some cases and nonzero in others.
We observe that the autocorrelation rxx(ℓ) and the cross-correlation
rdx(ℓ) are obtained from the data and, hence, represent estimates of the
true (statistical) autocorrelation and cross-correlation sequences. As a
result, the coeﬃcients h(k) obtained from (15.41) are estimates of the
true coeﬃcients. The quality of the estimates depend on the length of the
data record that is available for estimating rxx(ℓ) and rdx(ℓ). This is one
problem that must be considered in the implementation of an adaptive
ﬁlter.
A second problem that must be considered is that the underlying ran-
dom process x(n) is usually nonstationary. For example, in channel equal-
ization, the frequency response characteristics of the channel may vary
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

816
Chapter 15
ADAPTIVE FILTERS
with time. As a consequence, the statistical autocorrelation and cross-
correlation sequences—and, hence, their estimates—vary with time. This
implies that the coeﬃcients of the adaptive ﬁlter must change with time
to incorporate the time-variant statistical characteristics of the signal into
the ﬁlter. This also implies that the quality of the estimates cannot be
made arbitrarily high by simply increasing the number of signal sam-
ples used in the estimation of the autocorrelation and cross-correlation
sequences.
There are several ways by which the coeﬃcients of the adaptive ﬁlter
can be varied with time to track the time-variant statistical characteristics
of the signal. The most popular method is to adapt the ﬁlter recursively on
a sample-by-sample basis, as each new signal sample is received. A second
approach is to estimate rxx(ℓ) and rdx(ℓ) on a block-by-block basis, with
no attempt to maintain continuity in the values of the ﬁlter coeﬃcients
from one block of data to another. In such a scheme, the block size must
be relatively small, encompassing a time interval that is short compared
to the time interval over which the statistical characteristics of the data
change signiﬁcantly. In addition to this block-processing method, other
block-processing schemes can be devised that incorporate some block-to-
block continuity in the ﬁlter coeﬃcients.
In our treatment of adaptive ﬁltering algorithms, we consider only
time-recursive algorithms that update the ﬁlter coeﬃcients on a sample-
by-sample basis. In particular, we consider two types of algorithms, the
LMS algorithm, which is based on a gradient-type search for tracking the
time-variant signal characteristics, and the class of recursive least-squares
algorithms, which are signiﬁcantly more complex than the LMS algorithm,
but which provide faster convergence to changes in signal statistics.
15.2.1 MINIMUM MEAN-SQUARE-ERROR CRITERION
The LMS algorithm that is described in the following subsection is most
easily obtained by formulating the optimization of the FIR ﬁlter coeﬃ-
cients as an estimation problem based on the minimization of the mean-
square error. Let us assume that we have available the (possibly complex-
valued) data sequence x(n), which consists of samples from a stationary
random process X(n) with autocorrelation sequence
RXX(m) = E

X(n)X∗(n −m)

(15.42)
From these samples, we form an estimate of the desired sequence D(n)
by passing the observed data x(n) through an FIR ﬁlter with coeﬃcients
h(n), 0 ≤n ≤M −1. The ﬁlter output may be expressed as
ˆd(n) =
M−1

k=0
h(k)x(n −k)
(15.43)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
817
where ˆd(n) represents samples of an estimate D(n) of D(n). The estimation
error process is deﬁned as
E(n) = D(n) −D(n)
= D(n) −
M−1

k=0
h(k)X(n −k)
(15.44)
The mean-square error as a function of the ﬁlter coeﬃcients is
EM = E

|E(n)|2
= E
⎡
⎣
D(n) −
M−1

k=0
h(k)X(n −k)

2⎤
⎦
= E

|D(n)|2 −2Re
M−1

k=0
h∗(ℓ)D(n)X∗(n −ℓ)

+
M−1

k=0
M−1

ℓ=0
h∗(ℓ)h(k)X∗(n −ℓ)X(n −k)

= σ2
D −2Re
M−1

ℓ=0
h∗(ℓ)RDX(ℓ)

+
M−1

k=0
M

ℓ=0
h∗(ℓ)h(k)RXX(ℓ−k)
(15.45)
where, by deﬁnition, σ2
D = E

|D(n)|2
.
We observe that the MSE is a quadratic function of the ﬁlter coeﬃ-
cients. Consequently, the minimization EM with respect to the coeﬃcients
leads to the set of M linear equations,
M−1

k=0
h(k)RXX(ℓ−k) = RDX(ℓ),
ℓ= 0, 1, . . . , M −1
(15.46)
The ﬁlter with coeﬃcients obtained from (15.46), which is the Wiener–
Hopf equation previously derived in Section 14.6.1, is called the Wiener
filter.
If we compare (15.46) with (15.41), it is apparent that these equations
are similar in form. In (15.41), we use estimates of the autocorrelation and
cross-correlation to determine the ﬁlter coeﬃcients, whereas in (15.46)
the statistical autocorrelation and cross-correlation are employed. Hence
(15.46) yields the optimum (Wiener) ﬁlter coeﬃcients in the MSE sense,
whereas (15.41) yields estimates of the optimum coeﬃcients.
The equations in (15.46) may be expressed in matrix form as
TMhM = RD
(15.47)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

818
Chapter 15
ADAPTIVE FILTERS
where hM denotes the vector of coeﬃcients, TM is an M ×M (Hermitian)
Toeplitz matrix with elements Tℓk = RXX(ℓ−k), and RD is an M × 1
cross-correlation vector with elements RDX(ℓ), ℓ= 0, 1, . . . , M −1. The
complex-conjugate of hM is denoted as h∗
M and the transpose as ht
M. The
solution for the optimum ﬁlter coeﬃcients is
hopt = T −1
M RD
(15.48)
and the resulting minimum MSE achieved with the optimum coeﬃcients
given by (15.48)
EMmin = σ2
D −
M−1

k=0
hopt(k)R∗
DX(k)
= σ2
D −RH
DT −1
M RD
(15.49)
where the exponent H denotes the conjugate transpose.
Recall that the set of linear equations in (15.46) can also be obtained
by invoking the orthogonality principle in mean-square estimation (see
Section 14.6.2). According to the orthogonality principle, the mean-square
estimation error is minimized when the error E(n) is orthogonal, in the
statistical sense, to the estimate ˆD(n), that is,
E

E(n)D
∗(n)

= 0
(15.50)
But the condition in (15.50) implies that
E
M−1

k=0
h(k)E(n)X∗(n −k)

=
M−1

k=0
h(k)E

E(n)X∗(n −k)

= 0
or equivalently,
E

E(n)X∗(n −ℓ)

= 0,
ℓ= 0, 1, . . . , M −1
(15.51)
If we substitute for E(n) in (15.51) using the expression for E(n) given
in (15.44), and perform the expectation operation, we obtain the equations
given in (15.46).
Since D(n) is orthogonal to E(n), the residual (minimum) mean-
square error is
EMmin = E

E(n)D∗(n)

= E

|D(n)|2
−
M−1

k=0
hopt(k)R∗
DX(k)
(15.52)
which is the result given in (15.49).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
819
The optimum ﬁlter coeﬃcients given by (15.48) can be solved eﬃ-
ciently by using the Levinson–Durbin algorithm. However, we shall con-
sider the use of a gradient method for solving for hopt iteratively. This
development leads to the LMS algorithm for adaptive ﬁltering.
□
EXAMPLE 15.8
Parameter Estimation Based on MSE Criterion
Consider the random process
X(n) = Gv(n) + W(n),
n = 0, 1, . . . , M −1
where v(n) is a known sequence, G is a random variable with E[G] = 0, and
E[G2] = σ2
G. The sequence W(n) is a white noise sequence with variance σ2
W.
a. Determine the coeﬃcients of the linear estimator for G, given as
G =
M−1

n=0
h(n)X(n)
that minimize the mean-square error
E = E

G −G
	2
.
b. Determine the values of

h(n)

when

v(n)

=

1, 1, 1, −1, −1, −1,
1, −1, −1, 1, −1

, which is the Barker sequence of length M = 11, σ2
G = 1,
and σ2
W = 0.1.
Solution
a. The MSE is given by
E = E
⎡
⎣

G −
M−1

n=0
h(n)X(n)
2⎤
⎦
The minimum MSE is obtained by performing
∂E
∂h(k) = 0
⇒
E

2

G −
M−1

n=0
h(n)X(n)

X(k)

= 0,
k = 0, 1, . . . , M −1
Thus
E

G X(k)

= E
M−1

n=0
h(n)X(n)X(k)

,
k = 0, 1, . . . , M −1
(15.53)
Now,
E

G X(k)

= E

G

G v(k) + W(k)

= σ2
Gv(k),
k = 0, 1, . . . , M −1
(15.54)
assuming that G and W(k) are uncorrelated. Also,
E
M−1

n=0
h(n)X(n)X(k)

=
M−1

n=0
h(n)E

X(n)X(k)

(15.55)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

820
Chapter 15
ADAPTIVE FILTERS
It is easily shown that
E

X(n)X(k)

= σ2
Gv(k)v(n) + σ2
Wδnk
(15.56)
Therefore, substituting (15.54)–(15.56) into (15.53), we have
σ2
Gv(k) = σ2
G
M−1

n=0
h(n)v(k)v(n) + σ2
Wh(k)
(15.57)
Hence the ﬁlter coeﬃcients are the solution of the linear equations
(σ2
Gvvt + σ2
WI)h = σ2
Gv
(15.58)
where
v =

v(0), v(1), . . . , v(M −1)
t,
h =

h(0), h(1), . . . , h(M −1)
t
b. When

v(n)

=

1, 1, 1, −1, −1, −1, 1, −1, −1, 1, −1

, σ2
G = 1, and σ2
W = 0.1,
the solution to the linear equations yields

h(n)

=

0.0901, 0.0901, 0.0901, −0.0901, −0.0901, −0.0901, 0.0901,
−0.0901, −0.0901, 0.0901, −0.0901

□
15.2.2 THE LMS ALGORITHM
There are various numerical methods that can be used to solve the set of
linear equations given by (15.46) or (15.47) for the optimum FIR ﬁlter
coeﬃcients. In the following, we consider recursive methods that have
been devised for ﬁnding the minimum of a function of several variables.
In our problem, the performance index is the MSE given by (15.45),
which is a quadratic function of the ﬁlter coeﬃcients. Hence this func-
tion has a unique minimum, which we shall determine by an iterative
search.
For the moment, let us assume that the autocorrelation matrix TM
and the cross-correlation vector RD are known. Hence EM is a known
function of the coeﬃcients h(n), 0 ≤n ≤M−1. Algorithms for recursively
computing the ﬁlter coeﬃcients and, thus, searching for the minimum EM
have the form
hM(n + 1) = hM(n) + 1
2∆(n)V (n),
n = 0, 1, . . .
(15.59)
where hM(n) is the vector of ﬁlter coeﬃcients at the nth iteration, ∆(n)
is a step size at the nth iteration, and V (n) is a direction vector for
the nth iteration. The initial vector hM(0) is chosen arbitrarily. In this
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
821
treatment, we exclude methods that require the computations of T −1
M ,
such as Newton’s method, and consider only search methods based on
the use of gradient vectors.
The simplest method for ﬁnding the minimum of EM recursively is
based on a steepest-descent search (see Murray [69]). In the method of
steepest descent, the direction vector V (n) = −g(n), where g(n) is the
gradient vector at the nth iteration, deﬁned as
g(n) = dEM(n)
dhM(n)
= 2[TMhM(n) −RD],
n = 0, 1, 2, . . .
(15.60)
Hence we compute the gradient vector at each iteration and change the
values of hM(n) in a direction opposite the gradient. Thus the recursive
algorithm based on the method of steepest descent is
hM(n + 1) = hM(n) −1
2∆(n)g(n)
(15.61)
or equivalently,
hM(n + 1) = [I −∆(n)TM]hM(n) + ∆(n)RD.
(15.62)
We state without proof that the algorithm leads to the convergence of
hM(n) to hopt in the limit as n →∞, provided that the sequence of step
sizes ∆(n) is absolutely summable, with ∆(n) →0 as n →∞. It follows
that as n →∞, g(n) →0.
Other candidate algorithms that provide faster convergence are the
conjugate-gradient algorithm and the Fletcher–Powell algorithm. In the
conjugate-gradient algorithm, the direction vectors are given as
V (n) = β(n −1)V (n −1) −g(n)
(15.63)
where β(n) is a scalar function of the gradient vectors (see Beckman [1]).
In the Fletcher–Powell algorithm, the direction vectors are given as
V (n) = −H(n)g(n)
(15.64)
where H(n) is an M × M positive deﬁnite matrix, computed iteratively,
that converges to the inverse of TM (see Fletcher and Powell [16]). Clearly,
the three algorithms diﬀer in the manner in which the direction vectors are
computed. These three algorithms are appropriate when TM and RD are
known. However, this is not the case in adaptive ﬁltering applications, as
we have previously indicated. In the absence of knowledge of TM and RD,
we may substitute estimates, V (n), of the direction vectors in place of the
actual vectors, V (n). We consider this approach for the steepest-descent
algorithm.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

822
Chapter 15
ADAPTIVE FILTERS
First, we note that the gradient vector given by (15.60) may also be
expressed in terms of the orthogonality conditions given by (15.50). In
fact, the conditions in (15.50) are equivalent to the expression
E

E(n)X∗
M(n)

= RD −TMhM(n)
(15.65)
where XM(n) is the vector with elements X(n −ℓ), ℓ= 0, 1, . . . , M −1.
Therefore, the gradient vector is simply
g(n) = −2E

E(n)X∗
M(n)

(15.66)
Clearly, the gradient vector g(n) = 0 when the error is orthogonal to the
data in the estimate D(n).
An unbiased estimate of the gradient vector at the nth iteration is
simply obtained from (15.66) as
ˆg(n) = −2E(n)X∗
M(n)
(15.67)
where E(n) = D(n) −ˆD(n) and XM(n) is the set of M signal samples in
the ﬁlter at the nth iteration. Thus, with ˆg substituted for g(n), we have
the algorithm
hM(n + 1) = hM(n) + ∆(n)E(n)X∗
M(n)
(15.68)
where h(n) is now a vector random process with vector sample sequence
given by h(n). This is called a stochastic-gradient-descent algorithm. As
given by (15.68), it has a variable step size.
It has become common practice in adaptive ﬁltering to use a ﬁxed
step-size algorithm for two reasons. The ﬁrst is that a ﬁxed step-size al-
gorithm is easily implemented in either hardware or software. The second
is that a ﬁxed step size is appropriate for tracking time-variant signal
statistics, whereas if ∆(n) →0 as n →∞, adaptation to signal variations
cannot occur. For these reasons, (15.68) is modiﬁed to the algorithm
hM(n + 1) = hM(n) + ∆E(n)X∗
M(n)
(15.69)
where ∆is now the ﬁxed step size. This algorithm was ﬁrst proposed by
Widrow and Hoﬀ[95] and is now widely known as the LMS (least-mean-
squares) algorithm. Clearly, it is a stochastic-gradient algorithm.
The LMS algorithm is relatively simple to implement. For this rea-
son, it has been widely used in many adaptive ﬁltering applications. Its
properties and limitations have also been thoroughly investigated. In the
following section, we provide a brief treatment of its important properties
concerning convergence, stability, and the noise resulting from the use of
estimates of the gradient vectors. Subsequently, we compare its properties
with the more complex recursive least-squares algorithm.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
823
MATLAB Implementation
The SP Toolbox does not provide any
function for implementing the LMS algorithm, although it is available
in the Communication Systems Toolbox and DSP Systems Toolbox as
an object in an object-oriented programming environment. This discus-
sion is beyond the scope of this book. In Section 11.1, we provided the
function [h,y] = lms(x,d,delta,M) that computes adaptive ﬁlter co-
eﬃcients

h(n), 0 ≤n ≤M −1

in array h given the input sequence

x(n)

, the desired sequence

d(n)

, step size ∆, and the desired adaptive
FIR ﬁlter length M in respective arrays x, d, delta, and M. In addition,
the lms function provides the output

y(n)

of the adaptive ﬁlter in the
array y. It is given below again for easy reference.
function [h,y] = lms(x,d,delta,M)
% LMS Algorithm for Coefficient Adjustment
% ----------------------------------------
% [h,y] = lms(x,d,delta,N)
%
h = estimated FIR filter
%
y = output array y(n)
%
x = input array x(n)
%
d = desired array d(n), length must be same as x
% delta = step size
%
M = length of the FIR filter
%
N = length(x); y = zeros(1,N);
h = zeros(1,M);
for n = M:N
x1 = x(n:-1:n-M+1);
y = h * x1’;
e = d(n) - y;
h = h + delta*e*x1;
end
Several variations of the basic LMS algorithm have been proposed
in the literature and implemented in adaptive ﬁltering applications. One
variation is obtained if we average the gradient vectors over several itera-
tions prior to making adjustments of the ﬁlter coeﬃcients. For example,
the average over K gradient vectors is
ˆg(nK) = −2
K
K−1

k=0
E(nK + k)X∗
M(nK + k)
(15.70)
and the corresponding recursive equation for updating the ﬁlter coeﬃ-
cients once every K iterations is
hM

(n + 1)K

= hM(nK) −1
2∆ˆg(nK)
(15.71)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

824
Chapter 15
ADAPTIVE FILTERS
In eﬀect, the averaging operation performed in (15.70) reduces the noise
in the estimate of the gradient vector, as shown by Gardner [20].
An alternative approach is to ﬁlter the gradient vectors by a lowpass
ﬁlter and use the output of the ﬁlter as an estimate of the gradient vector.
For example, a simple lowpass ﬁlter for the gradients yields as an output
V(n) = βV(n −1) −ˆg(n),
V(0) = −ˆg(0)
(15.72)
where the choice of 0 ≤β < 1 determines the bandwidth of the lowpass
ﬁlter. When β is close to unity, the ﬁlter bandwidth is small and the
eﬀective averaging is performed over many gradient vectors. On the other
hand, when β is small, the lowpass ﬁlter has a large bandwidth and,
hence, it provides little averaging of the gradient vectors. With the ﬁltered
gradient vectors given by (15.72) in place of ˆg(n), we obtain the ﬁltered
version of the LMS algorithm, given by
hM(n + 1) = hM(n) + 1
2∆V(n)
(15.73)
An analysis of the ﬁltered-gradient LMS algorithm is given in Proakis [78].
15.2.3 PROPERTIES OF THE LMS ALGORITHM
In this section, we consider the basic properties of the LMS algorithm
given by (15.69). In particular, we focus on its convergence properties, its
stability, and the excess noise generated as a result of using noisy gradient
vectors in place of the actual gradient vectors. The use of noisy estimates
of the gradient vectors implies that the ﬁlter coeﬃcients will ﬂuctuate
randomly, and, hence, an analysis of the characteristics of the algorithm
should be performed in statistical terms.
Convergence and Stability
The convergence and stability of the LMS algorithm may be investigated
by determining how the mean value of hM(n) converges to the optimum
coeﬃcients hopt. If we take the expected value of (15.69), we obtain
hM(n + 1) = hM(n) + ∆E

E(n)X∗
M(n)

= hM(n) + ∆[RD −TMhM(n)]
= (I −∆TM)hM(n) + ∆RD
(15.74)
where hM(n) = E

hM(n)

, and I is the identity matrix.
The recursive relation in (15.74) may be represented as a closed-loop
control system, as shown in Figure 15.36. The convergence rate and the
stability of this closed-loop system are governed by our choice of the step-
size parameter ∆. To determine the convergence behavior, it is convenient
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
825
–g(n)
RD
TM
TM hM (n)
Filter
z – 1
z –1
H(z) =
∆
hM (n)
hM (n + 1)
–
–
–
FIGURE 15.36
Closed-loop control system representation of recursive Equation
(15.74)
to decouple the M simultaneous diﬀerence equations given in (15.74), by
performing a linear transformation of the mean coeﬃcient vector hM(n).
The appropriate transformation is obtained by noting that the autocor-
relation matrix TM is Hermitian and, hence, can be represented (see
Gantmacher [19]) as
TM = UΛU H
(15.75)
where U is the normalized modal matrix of TM and Λ is a diagonal matrix
with diagonal elements λk, 0 ≤k ≤M −1, equal to the eigenvalues of TM.
When (15.75) is substituted into (15.74), the latter may be ex-
pressed as
h
o
M(n + 1) = (I −∆Λ)h
o
M(n) + ∆Ro
D
(15.76)
where the transformed (orthogonalized) vectors are h
o
M(n) = U HhM(n)
and Ro
D = U HRD. The set of M ﬁrst-order diﬀerence equations in (15.76)
are now decoupled. Their convergence and their stability is determined
from the homogeneous equation
h
o
M(n + 1) = (I −∆Λ)h
o
M(n)
(15.77)
If we focus our attention on the solution of the kth equation in (15.77),
we observe that
¯h
o(k, n) = C(1 −∆λk)nu(n),
k = 0, 1, 2, . . . , M −1
(15.78)
where C is an arbitrary constant and u(n) is the unit step sequence.
Clearly, ¯h
o(k, n) converges to zero exponentially, provided that
|1 −∆λk| < 1
or equivalently,
0 < ∆< 2
λk
,
k = 0, 1, . . . , M −1
(15.79)
The fastest convergence rate is obtained when ∆= 1/λk.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

826
Chapter 15
ADAPTIVE FILTERS
The condition given by (15.79) for convergence of the homogeneous
diﬀerence equation for the kth normalized ﬁlter coeﬃcient (kth mode
of the closed-loop system) must be satisﬁed for all k = 0, 1, . . . , M −1.
Therefore, the range of values of ∆that ensures the convergence of the
mean of the coeﬃcient vector in the LMS algorithm is
0 < ∆<
2
λmax
(15.80)
where λmax is the largest eigenvalue of TM.
Since TM is an autocorrelation matrix, its eigenvalues are nonnega-
tive. Hence an upper bound on λmax is
λmax <
M−1

k=0
λk = trace TM = M RXX(0)
(15.81)
where RXX(0) is the input signal power, which is easily estimated from
the received signal. Therefore, an upper-bound on the step size ∆is
2/MRXX(0).
From (15.78), we observe that rapid convergence of the LMS algo-
rithm occurs when |1 −∆λk| is small, that is, when the poles of the
closed-loop system in Figure 15.36 are far from the unit circle. However, we
cannot achieve this desirable condition and still satisfy the upper bound
in (15.79) when there is a large diﬀerence between the largest and smallest
eigenvalues of TM. In other words, even if we select ∆to be 1/λmax, the
convergence rate of the LMS algorithm will be determined by the decay
of the mode corresponding to the smallest eigenvalue λmin. For this mode,
with ∆= 1/λmax substituted in (15.78), we have
¯h
o
M(k, n) = C

1 −λmin
λmax
n
u(n)
Consequently, the ratio λmin/λmax ultimately determines the convergence
rate. If λmin/λmax is small (much smaller than unity), the convergence will
be slow. On the other hand, if λmin/λmax is close to unity, the convergence
rate of the algorithm is fast.
□
EXAMPLE 15.9
Choice of Step Size in LMS Algorithm
Let us simulate an adaptive equalizer based on the LMS algorithm. The channel
characteristic is given by the following sample values taken at the symbol rate:
x =

0.05, −0.063, 0.088, −0.126, −0.25, 0.9047, 0.25, 0, 0.126, 0.038, 0.088

The autocorrelation matrix for this channel has an eigenvalue spread of
λmax/λmin = 11. The number of taps selected for the FIR equalizer is M = 11.
The received signal-plus-noise power is normalized to unity. We wish to illus-
trate the convergence characteristics of the LMS algorithm for three diﬀerent
values of the step-size, ∆= 0.045, 0.09, and 0.115. The upper bound for
stability of the algorithm is ∆= 0.18.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
827
Solution
Figure 15.37 illustrates the convergence characteristics of the LMS algorithm
for the three values of ∆. These graphs, called learning curves, were obtained
by averaging the estimated MSE in 200 simulation runs. By selecting ∆= 0.09
(one-half of the upper bound), we obtain a rapidly decaying convergence rate,
as shown in Figure 15.37. If we divide ∆by 2 to 0.045, the convergence rate is
reduced, but the excess MSE is also reduced, so the algorithm performs better in
a time-invariant signal environment; that is, the estimated equalizer coeﬃcients
are closer to their optimum values. The MATLAB script for this example is
given below.
N=500;
% length of the information sequence
K=5;
actual_isi=[0.05 -0.063 0.088 -0.126 -0.25 0.9047 0.25 0 0.126 0.038 0.088];
sigma=0.01;
delta=0.115;
Num_of_realizations=1000;
mse_av=zeros(1,N-2*K);
info = zeros(1,N);
noise = zeros(1,N);
mse = zeros(1,N);
for j=1:Num_of_realizations,
% Compute the average over a number of
% realizations.
% The information sequence
for i=1:N,
if (rand<0.5),
info(i)=-1;
else
info(i)=1;
end
echo off;
end;
% the channel output
y=filter(actual_isi,1,info);
for i=1:2:N, [noise(i), noise(i+1)]=gngauss(sigma); end;
y=y+noise;
% Now the equalization part follows.
estimated_c=[0 0 0 0 0 1 0 0 0 0 0];
% Initial estimate of ISI
for k=1:N-2*K,
y_k=y(k:k+2*K);
z_k=estimated_c*y_k.’;
e_k=info(k)-z_k;
estimated_c=estimated_c+delta*e_k*y_k;
mse(k)=e_kˆ2;
echo off;
end;
mse_av=mse_av+mse;
end;
mse_av=mse_av/Num_of_realizations;
% mean-squared error versus iterations
% Plotting commands follow
□
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

828
Chapter 15
ADAPTIVE FILTERS
0 
50 
100 
150 
200 
250 
300 
350 
400 
450 
500 
10 –3 
10 –2 
10 –1 
10 0 
Number of Iterations
∆ = 0.045
∆ = 0.09
∆= 0.115
FIGURE 15.37
Initial convergence characteristics of the LMS algorithm with
diﬀerent step sizes
□
EXAMPLE 15.10
Choice of the Step Size ∆in Steepest-Descent
Consider the quadratic performance index
J = h2 −40h + 28
Suppose that we search for the minimum of J by using the steepest-descent
algorithm
h(n + 1) = h(n) −1
2∆g(n),
n = 0, 1, . . .
where g(n) in the gradient.
a. Determine the range of values for ∆that provide an overdamped system
for the adjustment process.
b. Plot the expression for J as a function of n for a value of ∆in this range.
Solution
a. The gradient in the performance index is
g(n) =
∂J
∂h(n) = 2h(n) −40
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
829
Therefore, the steepest-descent algorithm is
h(n + 1) = h(n) −1
2∆g(n) = (1 −∆)h(n) + 20∆
For an overdamped system,
|1 −∆| < 1
⇒
0 < ∆< 2
b. Figure 15.38(a) contains a plot of J(n) vs. n. The step size ∆was set to 0.5
and the initial value of h was set to 0. In Figure 15.38(b), we have plotted
J(h(n)) vs. h(n). As it is observed from the ﬁgures the minimum value of J
which is −372, is reached within ﬁve iterations of the algorithm.
□
Analysis of Excess Noise
The other important characteristic of the LMS algorithm is the noise
resulting from the use of estimates of the gradient vectors. The noise in the
gradient-vector estimates causes random ﬂuctuations in the coeﬃcients
about their optimal values and, thus, leads to an increase in the MMSE
at the output of the adaptive ﬁlter. Hence the total MSE is EM,min + E∆,
where E∆is called the excess mean-square error.
For any given set of ﬁlter coeﬃcients hM(n), the total MSE at the
output of the adaptive ﬁlter may be expressed as
Et(n) = EM,min + (hM(n) −hopt)tTM(hM(n) −hopt)∗
(15.82)
where hopt represents the optimum ﬁlter coeﬃcients deﬁned by (15.48).
A plot Et(n) as a function of the iteration n is called a learning curve. If
we substitute (15.75) for TM and perform the linear orthogonal transfor-
mation used previously, we obtain
Et(n) = EM,min +
M−1

k=0
λk
ho(k, n) −ho
opt(k)
2
(15.83)
where the term

ho(k, n) −ho
opt(k)

represents the error in the kth ﬁl-
ter coeﬃcient (in the orthogonal coordinate system). The excess MSE is
deﬁned as the expected value of the second term in (15.83),
E∆=
M−1

k=0
λkE
ho(k, n) −ho
opt(k)
2
(15.84)
To derive an expression for the excess MSE E∆, we assume that the
mean values of the ﬁlter coeﬃcients hM(n) have converged to their op-
timum values hopt. Then the term ∆E(n)X∗
M(n) in the LMS algorithm
given by (15.69) is a zero-mean noise vector. Its covariance is
cov[∆E(n)X∗
M(n)] = ∆2E
E(n)
2
XM(n)XH
M(n)]
(15.85)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

830
Chapter 15
ADAPTIVE FILTERS
0
5
10
15
20
25
30
35
40
45
50
−400
−350
−300
−250
−200
−150
−100
−50
0
50
J(n)
(a)
0
2
4
6
8
10
12
14
16
18
20
−400
−350
−300
−250
−200
−150
−100
−50
0
50
h(n)
n
J(h(n))
(b)
FIGURE 15.38
Convergence characteristics of J for Example 15.10
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
831
To a ﬁrst approximation, we assume that
E(n)
2 is uncorrelated with the
signal vector. Although this assumption is not strictly true, it simpliﬁes
the derivation and yields useful results. (The reader may refer to Mazo
[65], Jones, Cabin, and Reed [40], and Gardner [20] for further discussion
on this assumption). Then
cov[∆E(n)X∗
M(n)] = ∆2E
E(n)
2
E

XM(n)XH
M(n)

= ∆2EM,minTM
(15.86)
For the orthogonalized coeﬃcient vector ho
M(n) with additive noise, we
have the equation
ho
M(n + 1) = (I −∆Λ)ho
M(n) + ∆Ro
D + Wo(n)
(15.87)
where Wo(n) is the additive noise vector, which is related to the noise
vector ∆E(n)X∗
M(n) through the transformation
Wo(n) = U H[∆E(n)X∗
M(n)]
= ∆E(n)U HX∗
M(n)
(15.88)
It is easily seen that the covariance matrix of the noise vector is
cov[Wo(n)] = ∆2EM,minU HTMU
= ∆2EM,minΛ
(15.89)
Therefore, the M components of Wo(n) are uncorrelated and each com-
ponent has the variance σ2
k = ∆2EM,minλk, k = 0, 1, . . . , M −1.
Since the noise components of Wo(n) are uncorrelated, we may con-
sider the M uncoupled diﬀerence equations in (15.87) separately. Each
ﬁrst-order diﬀerence equation represents a ﬁlter with impulse response
(1 −∆λk)n. When such a ﬁlter is excited with a noise sequence Wo
k(n),
the variance of the noise at the output of the ﬁlter is
E


ho(k, n) −ho
opt(k)


2
=
∞

n=0
∞

m=0
(1 −∆λk)n(1 −∆λk)mE

Wo
k(n)Wo∗
k (m)

(15.90)
We make the simplifying assumption that the noise sequence Wo
k(n) is
white. Then (15.90) reduces to
E
ho(k, n) −ho
opt(k)
2
=
σ2
k
1 −(1 −∆λk)2 =
∆2EM,minλk
1 −(1 −∆λk)2
(15.91)
If we substitute the result of (15.91) into (15.84), we obtain the
expression for the excess MSE as
E∆= ∆2EM,min
M−1

k=0
λ2
k
1 −(1 −∆λk)2
(15.92)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

832
Chapter 15
ADAPTIVE FILTERS
This expression can be simpliﬁed if we assume that ∆is selected such
that ∆λk ≪1 for all k. Then
E∆≈∆2EM,min
M−1

k=0
λ2
k
2∆λk
≈1
2∆EM,min
M−1

k=0
λk ≈∆MEM,minRXX(0)
2
(15.93)
where RXX(0) is the power of the input signal.
The expression for E∆indicates that the excess MSE is proportional
to the step-size parameter ∆. Hence our choice of ∆must be based on a
compromise between fast convergence and a small excess MSE. In prac-
tice, it is desirable to have E∆< EM,min. Hence
E∆
EM,min
≈∆MRXX(0)
2
< 1
or equivalently,
∆<
2
MRXX(0)
(15.94)
But this is just the upper bound that we had obtained previously for λmax.
In steady-state operation, ∆should satisfy the upper bound in (15.94),
otherwise the excess MSE causes signiﬁcant degradation in the perfor-
mance of the adaptive ﬁlter.
□
EXAMPLE 15.11
Use of LMS Algorithm in System Identiﬁcation
Use the LMS algorithm to estimate the impulse response of the two-pole system
given in Example 15.1. Initially, set h(k) = 0 for 0 ≤k ≤M −1. The variance
of W(n) is σ2
W = 0.02 and the variance of X(n) is σ2
X = 1. Select the step
size ∆in the LMS algorithm to satisfy (15.94). Carry out the simulation for
0 ≤n ≤1000.
a. After 1000 iterations, plot and compare the impulse response of the two
pole ﬁlter with that of the FIR ﬁlter model. Also plot and compare the
frequency response of the unknown system with that of the model.
b. Compare the results in part (a) with the results obtained in part (c) of
Example 15.1.
Solution
The adaptive ﬁlter order from Example 15.1 is M = 15. Since σ2
X = 1, the signal
power is also RXX(0) = 1. Hence from (15.94), the upper bound on the step size
is ∆= 0.1333. We chose the value of ∆= 0.05, which gives good convergence
and stability.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
833
a. MATLAB script:
>> varW = 0.02; varX = 1;
% Noise and signal variances
>> N = 1000; n = 0:N;
% Number of samples and indices
>> p = 0.8*exp(1j*pi/4);
% Pole location
>> a = [1,-2*real(p),abs(p)ˆ2];
% Plant denominator coeff
>> M = 15;
% FIR filter model order
>> xn = sqrt(varX)*randn(N+1,1); % Input sequence
>> wn = sqrt(varW)*randn(N+1,1); % Noise sequence
>> dn = filter(1,a,xn);
% Output of the plant
>> yn = dn+wn;
% Noisy plant output
>> % FIR filter model coefficients using LMS algorithm
>> delta = 0.05; %2/(M*varX)=0.1333;
>> [hm,yhatn] = lms(xn.’,yn,delta,M);
>> hp = impz(1,a,M+5);
% Plant impulse response
>> % Plots of the impulse and frequency responses
>> om = linspace(0,1,1001)*pi;
>> Hm = freqz(hm,1,om); Hm_mag = abs(Hm); Hm_pha = angle(Hm)/pi;
>> Hp = freqz(1,a,om); Hp_mag = abs(Hp); Hp_pha = angle(Hp)/pi;
>> % Plotting commands follow
The resulting plots are shown in Figure 15.39. After 1000 iterations, the model
coeﬃcients seem to converge to the plant coeﬃcients with very small residual
noise. This is also evident in the frequency response plots. The peak of the
model magnitude response is at the correct location but the overall response
exhibit the eﬀects of FIR truncation and residual noise.
b. Here, we compare the model coeﬃcients and its frequency response obtained
using the LS method and the LMS algorithm.
>> varW = 0.02; varX = 1;
% Noise and signal variances
>> N = 1000; n = 0:N;
% Number of samples and indices
>> p = 0.8*exp(1j*pi/4);
% Pole location
>> a = [1,-2*real(p),abs(p)ˆ2];
% Plant denominator coeff
>> M = 15;
% FIR filter model order
>> xn = sqrt(varX)*randn(N+1,1); % Input sequence
>> wn = sqrt(varW)*randn(N+1,1); % Noise sequence
>> dn = filter(1,a,xn);
% Output of the plant
>> yn = dn+wn;
% Noisy plant output
>> % FIR filter model coefficients using LS method
>> [rxx] = xcorr(xn,M-1); % ACRS of x(n)
>> Rxx = toeplitz(rxx(M:end)); % ACRM of x(n)
>> ryx = xcorr(yn,xn,M-1);
% CCRS between y(n) and x(n)
>> ryx = ryx(M:end);
% CCRV
>> hls = Rxx\ryx;
% Model coeff (or Imp resp)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

834
Chapter 15
ADAPTIVE FILTERS
Time Index n
0
10
20
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Plant Impulse Response
0
0.5
1
Magnitude
0
1
2
3
4
Plant Frequency Response
–1
–0.5
0
0.5
1
Time Index n
0
10
20
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Model Impulse Response
0
0.5
1
Magnitude
0
1
2
3
4
Model Frequency Response
–1
–0.5
0
0.5
1
Phase in p Units 
Phase in p Units 
w/p
w/p
FIGURE 15.39
Plots of the impulse and frequency responses of the plant and
FIR model obtained using the LMS algorithm in Example 15.11(a). The dotted
lines represent phase responses
>> % Plots of the impulse and frequency responses
>> om = linspace(0,1,1001)*pi;
>> Hls = freqz(hls,1,om); Hls_mag = abs(Hls); Hls_pha = angle(Hls)/pi;
>> % Plotting commands follow
The resulting plots are shown in Figure 15.40. Observe that the impulse response
coeﬃcients and the frequency responses obtained using LMS algorithm show the
eﬀects of excess noise especially at high frequencies.
□
□
EXAMPLE 15.12
Use of LMS Algorithm in Channel Equalization
Consider the system conﬁguration shown in Example 15.2. For M = 7 and
D = 10, generate the output sequence ˆa(n) and discard the ﬁrst 10 output
samples to compensate for the transient in the system. Thus ˆa(11) corresponds
to a(1), ˆa(12) corresponds to a(2), and so on. Use the LMS algorithm to adjust
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
835
Time Index n
0
5
10
15
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Impulse Response (LS)
w/p
0
0.5
1
Magnitude
0
1
2
3
4
Frequency Response (LS)
Phase in p Units 
–1
–0.5
0
0.5
1
Time Index n
0
5
10
15
Amplitude
–0.6
–0.3
0
0.3
0.6
0.9
1.2
Impulse Response (LMS)
w/p
0
0.5
1
Magnitude
0
1
2
3
4
Frequency Response (LMS)
Phase in p Units 
–1
–0.5
0
0.5
1
FIGURE 15.40
Comparison of FIR models obtained using the LS method and the
LMS algorithm in Example 15.11(b). The dotted lines represent phase responses
the equalizer ﬁlter coeﬃcients h(k), 0 ≤k ≤6. Carry out the simulation for
n = 1000 samples and stop.
a. Plot the frequency response of the channel ﬁlter C(z), the equalizer ﬁlter
H(z), and the frequency response of the cascade ﬁlter C(z)H(z). Comment
on the results.
b. Compare the results in part (a) with the results in Example 15.2, part (c),
which were obtained from the least-squares criterion.
Solution
Various signals and the model coeﬃcients using the LMS algorithm are com-
puted using the following MATLAB script.
>> z0 = 0.8*exp(1j*pi/4); % Zero of C(z)
>> Cb = [1,-2*real(z0),abs(z0)ˆ2]; % Numerator of C(z) or imp resp
>> N = 1000; n = 0:N-1; % Length and timing indices for sequences
>> varW = 0.1; % Variance of the additive noise
>> % Generation of Sequences a(n), x(n), and d(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

836
Chapter 15
ADAPTIVE FILTERS
>> M = 7;
% FIR equalizer length
>> D = 10;
% Overall delay in processing
>> an = 2*randi([0,1],N,1)-1;
% Pseudorandom symbol sequence
>> yn = filter(Cb,1,an);
% Distorted symbol sequence
>> wn = sqrt(varW)*randn(N,1);
% Additive noise sequence
>> xn = yn+wn;
% Noisy distorted symbols
>> dn = [zeros(D,1);an(1:N)];
% Desired symbols (delayed)
>> % FIR equalizer coefficients using LMS algorithm
>> [rxx,lags] = xcorr(xn,M-1,’unbiased’); % ACRS of x(n)
>> delta = 0.05; %2/(M*rxx(M))=0.1012;
>> [heq,ahatn] = lms(xn.’,dn(D+1:end),delta,M);
a. MATLAB script:
>> om = linspace(0,1,1001)*pi;
>> Heq = freqz(heq,1,om);
>> Heq_mag = abs(Heq); Heq_pha = angle(Heq)/pi;
>> Cz = freqz(Cb,1,om);
>> Cz_mag = abs(Cz); Cz_pha = angle(Cz)/pi;
>> CzHeq = Cz.*Heq;
>> CzHeq_mag = abs(CzHeq); CzHeq_pha = angle(CzHeq)/pi;
The resulting plots are shown in Figure 15.41. The magnitude response of the
product ﬁlter is close to unity, with a small variation over the entire band due to
excess noise characteristics of the LMS algorithm. Overall, the LMS equalizer
provides an adequate performance.
b. Comparison of Figure 15.9 for the LS method and Figure 15.41 shows that the
two equalizer performances are similar over the entire band with LS equalizer
showing a smoother frequency response.
□
□
EXAMPLE 15.13
Use of LMS Algorithm in Echo Cancellation
Consider the echo cancellation system given in Example 15.3. Using the same
system parameters as in Example 15.3, use the LMS algorithm to adapt the
coeﬃcients of the echo canceller by running the simulation for 1000 samples.
a. Compare the coeﬃcients of the echo canceller with the echo coeﬃcients
c(k), 0 ≤k ≤4, and comment on the results.
b. Compare the results obtained with the LMS algorithm with the results
obtained by use of the least-squares method used in Example 15.3.
Solution
From Example 15.3 the system parameters are.
>> c = [-0.25,-0.5,0.75,0.36,0.25]; % Echo coefficients>>
>> N = 1000; n = 0:N; % Length and timing indices
>> varW = 0.1; % Additive noise variance
>> an = 2*randi([0,1],N+1,1)-1; % Pseudorandom symbol sequence at Modem-A
>> bn = 2*randi([0,1],N+1,1)-1; % Pseudorandom symbol sequence at Modem-B
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
837
0
0.25
1
Magnitude
0
1
2
3
Frequency Response of the Channel
Phase in p Units
Phase in p Units
Phase in p Units
–1
–0.5
0
0.5
1
0
0.25
1
Magnitude
0
1
2
3
Frequency Response of the Equalizer
–1
–0.5
0
0.5
1
0
0.25
1
Magnitude
0
1
2
3
Frequency Response of the Cascaded Filter
–1
–0.5
0
0.5
1
w/p
w/p
w/p
FIGURE 15.41
MATLAB plot in Example 15.12(a)
a. The upper limit for the step size from (15.94) for this case is 0.2. However,
good convergence and stability results for 1000 iterations were obtained using
∆= 0.01.
>> sAn = filter(c,1,an); % Echo signal sequence at modem-A
>> wn = sqrt(varW)*randn(N+1,1); % Additive noise sequence
>> yn = bn + sAn + wn; % Received signal at Modem-A
>> M = 10;
% FIR echo canceller order
>> [raa,lags] = xcorr(an,M-1,’unbiased’); % ACRS of a(n)
>> delta = 0.01;
%2/(M*raa(M))=0.2;
>> [hec,Sahatn] = lms(an.’,yn,delta,M);
>> hec(1:5)
hec =
-0.2988
-0.5861
0.7469
0.3010
0.2272
The resulting echo canceller coeﬃcients are reasonable but noisy after 1000
iterations.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

838
Chapter 15
ADAPTIVE FILTERS
b. The corresponding echo canceller coeﬃcients using the LS method from Exam-
ple 15.3 were
>> hec’
ans =
-0.2540
-0.4982
0.7943
0.3285
0.2291
which were closer to the original echo canceller coeﬃcients. This shows again
that the LMS algorithm does produce noisy yet usable results.
□
□
EXAMPLE 15.14
Use of LMS Algorithm in Suppression of Narrowband Interference
Consider the system conﬁguration for the suppression of narrowband interfer-
ence given in Example 15.4. Using the same system parameters as in Exam-
ple 15.4, use the LMS algorithm to adapt the coeﬃcients of the linear predictor
by running the simulation for 1000 iterations.
a. As in part (b) of Example 15.4, compute and plot the frequency response
of the prediction-error ﬁlter and, thus, verify that this ﬁlter is a notch ﬁlter
that suppresses the narrow band interference.
b. Compare the results obtained with the LMS algorithm with the results
obtained by use of the least-squares method used in Example 15.4.
Solution
The signals x(n) and v(n) are generated using
>> i = (0:100)’; fi = 0.1*i/100; thetai = 2*pi*rand(length(i),1);
>> A = 1; N = 1000; n = 0:N;
>> xn = sum(A*cos(2*pi*fi*n+thetai*ones(1,N+1))); xn = xn’;
>> varW = 1; wn = sqrt(varW)*randn(N+1,1);
>> vn = xn+wn;
>> [rvv] = xcorr(vn,M-1+D,’unbiased’);
The upper limit for the step size from (15.94) for this case using rvv(0) is
0.0025. However, for the LMS algorithm the value of ∆= 0.001 was selected
and coeﬃcients of the predictor were computed after 1000 iterations.
>> M = 15; D = 1;
>> delta = 0.001; %2/(M*rvv(M))=0.0025;
>> [h,xhatn] = lms(vn(1:N-D).’,vn(D:N),delta,M);
a. The coeﬃcients of the prediction-error ﬁlter and its frequency response were
computed using the following MATLAB script.
>> h1 = zeros(M+D,1); h1(1) = 1;
>> h1(D+1:D+M) = -h;
>> f = linspace(0,1,1001)*0.5;
>> H1 = freqz(h1,1,2*pi*f);
>> H1db = 20*log10(abs(H1)/max(abs(H1)));
>> % Plotting commands follow
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
839
Normalized Frequency f 
0
0.1
0.5
Decibels
–60
–50
–40
–30
–20
–10
0
Log-Magnitude Response of Prediction-Error Filter: A=1
FIGURE 15.42
Log-magnitude response of the prediction-error ﬁlter using the
LMS algorithm in Example 15.14(a)
The resulting plot is shown in Figure 15.42. Although the ﬁlter creates a notch of
more than 15 dB in the band of interfering signal, the passband has undesirable
ripple. In this case, LMS algorithm seems to perform barely well.
b. Comparing Figure 15.20 for the LS method with Figure 15.42 for the LMS
algorithm it is clear that the LMS algorithm is eﬀectve in suppressing the nar-
rowband interference, but its overall performance is not as good as that of the
LS method.
□
Discussion
The preceding analysis of the excess MSE is based on the assumption that
the mean values of the ﬁlter coeﬃcients have converged to the optimum
solution hopt. Under this condition, the step size ∆should satisfy the
bound in (15.94). On the other hand, we have determined that conver-
gence of the mean coeﬃcient vector requires that ∆< 2/λmax. While a
choice of ∆near the upper bound 2/λmax may lead to initial convergence
of the deterministic (known) gradient algorithm, such a large value of ∆
will usually result in instability of the stochastic-gradient LMS algorithm.
The initial convergence or transient behavior of the LMS algorithm
has been investigated by several researchers. Their results clearly indicate
that the step size must be reduced in direct proportion to the length
of the adaptive ﬁlter, as in (15.94). The upper bound given in (15.94)
is necessary to ensure the initial convergence of the stochastic-gradient
LMS algorithm. In practice, a choice of ∆< 1/MRXX(0) is usually made.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

840
Chapter 15
ADAPTIVE FILTERS
Gitlin and Weinstein [24] and Ungerboeck [93] contain an analysis of the
transient behavior and the convergence properties of the LMS algorithm.
In a digital implementation of the LMS algorithm, the choice of
the step-size parameter becomes even more critical. In an attempt to
reduce the excess MSE, it is possible to reduce the step-size parame-
ter to the point at which the total output MSE actually increases. This
condition occurs when the estimated gradient components e(n)x∗(n −ℓ),
ℓ= 0, 1, M −1, after multiplication by the small step-size parameter ∆,
are smaller than one-half of the least signiﬁcant bit in the ﬁxed-point
representation of the ﬁlter coeﬃcients. In such a case, adaptation ceases.
Consequently, it is important for the step size to be large enough to bring
the ﬁlter coeﬃcients in the vicinity of hopt. If it is desired to decrease
the step size signiﬁcantly, it is necessary to increase the precision in the
ﬁlter coeﬃcients. Typically, 16 bits of precision may be used for the ﬁlter
coeﬃcients, with the 12 most signiﬁcant bits used for arithmetic opera-
tions in the ﬁltering of the data. The 4 least signiﬁcant bits are required
to provide the necessary precision for the adaptation process. Thus the
scaled, estimated gradient components ∆e(n)x∗(n−ℓ) usually aﬀect only
the least signiﬁcant bits. In eﬀect, the added precision also allows for the
noise to be averaged out, since several incremental changes in the least
signiﬁcant bits are required before any change occurs in the upper, more
signiﬁcant bits used in arithmetic operations for ﬁltering of the data. For
an analysis of round-oﬀerrors in a digital implementation of the LMS
algorithm, the reader is referred to Gitlin and Weinstein [24], Gitlin et al.
[25], and Caraiscos and Liu [4].
As a ﬁnal point, we should indicate that the LMS algorithm is appro-
priate for tracking slowly time-variant signal statistics. In such a case, the
minimum MSE and the optimum coeﬃcient vector will be time-variant.
In other words, EM,min is a function of time, and the M-dimensional error
surface is moving with the time index n. The LMS algorithm attempts to
follow the moving minimum EM,min in the M-dimensional space, but it is
always lagging behind due to its use of (estimated) gradient vectors. As a
consequence, the LMS algorithm incurs another form of error, called the
lag error, whose mean-square value decreases with an increase in the step
size ∆. The total MSE can now be expressed as
Etotal = EMmin + E∆+ El
(15.95)
where El denotes the MSE due to the lag.
In any given nonstationary adaptive ﬁltering problem, if we plot the
E∆and El as a function of ∆, we expect these errors to behave as illus-
trated in Figure 15.43. We observe that E∆increases with an increase in
∆, whereas El decreases with an increase in ∆. The total error will ex-
hibit a minimum, which will determine the optimum choice of the step-size
parameter.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
841
Mean-Square Error
l Error Due to Lag
Error Due to
l
opt
∆
∆
∆
∆
Noisy Gradients
FIGURE 15.43
Excess mean-square error E∆and lag error Et as a function of
the step size ∆
When the statistical time variations of the signals occur rapidly, the
lag error will dominate the performance of the adaptive ﬁlter. In such a
case, El ≫EMmin +E∆, even when the largest possible value of ∆is used.
When this condition occurs, the LMS algorithm is inappropriate for the
application and one must rely on the more complex recursive least-squares
algorithms described in Section 15.2.4 to obtain faster convergence and
tracking.
15.2.4 RECURSIVE LEAST-SQUARES ALGORITHMS
FOR DIRECT-FORM FIR FILTERS
The major advantage of the LMS algorithm lies in its computational sim-
plicity. However, the price paid for this simplicity is slow convergence,
especially when the eigenvalues of the autocorrelation matrix TM have a
large spread—that is, when λmax/λmin ≫1. From another point of view,
the LMS algorithm has only a single adjustable parameter for controlling
the convergence rate—namely, the step-size parameter ∆. Since ∆is lim-
ited for purposes of stability to be less than the upper-bound in (15.94),
the modes corresponding to the smaller eigenvalues converge very slowly.
To obtain faster convergence, it is necessary to devise more complex
algorithms, which involve additional parameters. In particular, if the cor-
relation matrix TM has unequal eigenvalues λ0, λ1, . . . , λM−1, we should
use an algorithm that contains M parameters, one for each of the eigenval-
ues. In deriving more rapidly converging adaptive ﬁltering algorithms, we
adopt the least-squares criterion instead of the statistical approach based
on the MSE criterion. Thus we deal directly with the data sequence x(n)
and obtain estimates of correlations from the data.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

842
Chapter 15
ADAPTIVE FILTERS
It is convenient to express the least-squares algorithms in matrix form,
in order to simplify the notation. Since the algorithms will be recursive in
time, it is also necessary to introduce a time index in the ﬁlter-coeﬃcient
vector and in the error sequence. Hence we deﬁne the ﬁlter-coeﬃcient
vector at time n as
hM(n) =
⎡
⎢⎢⎢⎢⎢⎣
h(0, n)
h(1, n)
h(2, n)
...
h(M −1, n)
⎤
⎥⎥⎥⎥⎥⎦
(15.96)
where the subscript M denotes the length of the ﬁlter. Similarly, the input
signal vector to the ﬁlter at time n is denoted as
XM(n) =
⎡
⎢⎢⎢⎢⎢⎣
x(n)
x(n −1)
x(n −2)
...
x(n −M + 1)
⎤
⎥⎥⎥⎥⎥⎦
(15.97)
We assume that x(n) = 0 for n < 0. This is usually called prewindowing
of the input data.
The recursive least-squares problem may now be formulated as fol-
lows. Suppose that we have observed the vectors XM(ℓ), ℓ= 0, 1, . . . , n,
and we wish to determine the ﬁlter-coeﬃcient vector hM(n) that
minimizes the weighted sum of magnitude-squared errors
EM =
n

ℓ=0
wn−ℓeM(ℓ, n)
2
(15.98)
where the error is deﬁned as the diﬀerence between the desired sequence
d(ℓ) and the estimate ˆd(ℓ, n),
eM(ℓ, n) = d(ℓ) −ˆd(ℓ, n)
= d(ℓ) −ht
M(n)XM(ℓ)
(15.99)
and w is a weighting factor in the range 0 < w ≤1.
The purpose of the factor w is to weight the most recent data points
more heavily and, thus, allow the ﬁlter coeﬃcients to adapt to time-
varying statistical characteristics of the data. This is accomplished by
using the exponential weighting factor with the past data. Alternatively,
we may use a ﬁnite-duration sliding window with uniform weighting over
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
843
the window length. We ﬁnd the exponential weighting factor more conve-
nient, both mathematically and practically. For comparison, an exponen-
tially weighted window sequence has an eﬀective memory of
N =

∞
n=0 nwn

∞
n=0 wn =
w
1 −w
(15.100)
and, hence, should be approximately equivalent to a sliding window of
length N.
The minimization of EM with respect to the ﬁlter-coeﬃcient vector
hM(n) yields the set of linear equations
RM(n)hM(n) = DM(n)
(15.101)
where RM(n) is the signal (estimated) correlation matrix deﬁned as
RM(n) =
n

ℓ=0
wn−ℓX∗
M(ℓ)Xt
M(ℓ)
(15.102)
and DM(n) is the (estimated) cross-correlation vector
DM(n) =
n

ℓ=0
wn−ℓX∗
M(ℓ)d(ℓ)
(15.103)
The solution of (15.101) is
hM(n) = R−1
M (n)DM(n)
(15.104)
Clearly, the matrix RM(n) is akin to the statistical autocorrelation
matrix TM, and the vector DM(n) is akin to the cross-correlation vector
Rd, deﬁned previously. We emphasize, however, that RM(n) is not a
Toeplitz matrix, whereas TM is. We should also mention that for small
values of n, RM(n) may be ill conditioned, so that its inverse is not
computable. In such a case, it is customary to initially add the matrix
δIM to RM(n), where IM is an identity matrix and δ is a small positive
constant. With exponential weighting into the past, the eﬀect of adding
δIM dissipates with time.
Now, suppose that we have the solution of (15.104) at time (n −1)—
that is, we have hM(n−1) and we wish to compute hM(n). It is ineﬃcient
and, hence, impractical to solve the set of M linear equations for each
new signal component. Instead, we may compute the matrix and vectors
recursively. First, RM(n) may be computed recursively as
RM(n) = wRM(n −1) + X∗
M(n)Xt
M(n)
(15.105)
We call (15.105) the time-update equation for RM(n).
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

844
Chapter 15
ADAPTIVE FILTERS
Since the inverse of RM(n) is needed, we use the matrix inversion
lemma (see Householder [32]),
R−1
M (n) = 1
w

R−1
M (n −1) −R−1
M (n −1)X∗
M(n)Xt
M(n)R−1
M (n −1)
w + Xt
M(n)R−1
M (n −1)X∗
M(n)

(15.106)
Thus R−1
M (n) may be computed recursively.
For convenience, we deﬁne PM(n) = R−1
M (n). It is also convenient
to deﬁne an M-dimensional vector KM(n), sometimes called the Kalman
gain vector, as
KM(n) =
1
w + µM(n)PM(n −1)X∗
M(n)
(15.107)
where µM(n) is a scalar deﬁned as
µM(n) = Xt
M(n)PM(n −1)X∗
M(n)
(15.108)
With these deﬁnitions, (15.106) becomes
PM(n) = 1
w

PM(n −1) −KM(n)Xt
M(n)PM(n −1)

(15.109)
Let us postmultiply (15.109) by X∗
M(n). Then
PM(n)X∗
M(n) = 1
w[PM(n−1)X∗
M(n)−KM(n)Xt
M(n)PM(n−1)X∗
M(n)]
= 1
w

w + µM(n)

KM(n) −KM(n)µM(n)
 
= KM(n)
(15.110)
Therefore, the Kalman gain vector may also be deﬁned as PM(n)X∗
M(n).
Now we may use the matrix inversion lemma to derive an equation
for computing the ﬁlter coeﬃcients recursively. Since
hM(n) = PM(n)DM(n)
(15.111)
and
DM(n) = wDM(n −1) + d(n)X∗
M(n)
(15.112)
we have, upon substitution of (15.109) and (15.112) into (15.104),
hM(n) = 1
w[PM(n −1) −KM(n)Xt
M(n)PM(n −1)]
× [wDM(n −1) + d(n)X∗
M(n)]
= PM(n −1)DM(n −1) + 1
wd(n)PM(n −1)X∗
M(n)
−KM(n)Xt
M(n)PM(n −1)DM(n −1)
−1
wd(n)KM(n)Xt
M(n)PM(n −1)X∗
M(n)
= hM(n −1) + KM(n)[d(n) −Xt
M(n)hM(n −1)] (15.113)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
845
We observe that Xt
M(n)hM(n−1) is the output of the adaptive ﬁlter
at time n based on use of the ﬁlter coeﬃcients at time n −1. Since
Xt
M(n)hM(n −1) = ˆd(n, n −1) ≡ˆd(n)
(15.114)
and
eM(n, n −1) = d(n) −ˆd(n, n −1) ≡eM(n)
(15.115)
it follows that the time-update equation for hM(n) may be expressed as
hM(n) = hM(n −1) + KM(n)eM(n)
(15.116)
or equivalently,
hM(n) = hM(n −1) + PM(n)X∗
M(n)eM(n)
(15.117)
To summarize, suppose we have the optimum ﬁlter coeﬃcients
hM(n −1), the matrix PM(n −1), and the vector XM(n −1). When
the new signal component x(n) is obtained, we form the vector XM(n)
by dropping the term x(n −M) from XM(n −1) and adding the term
x(n) as the ﬁrst element. Then the recursive computation for the ﬁlter
coeﬃcients proceeds as follows.
1. Compute the ﬁlter output:
ˆd(n) = Xt
M(n)hM(n −1)
(15.118)
2. Compute the error:
eM(n) = d(n) −ˆd(n)
(15.119)
3. Compute the Kalman gain vector:
KM(n) =
PM(n −1)X∗
M(n)
w + Xt
M(n)PM(n −1)X∗
M(n)
(15.120)
4. Update the inverse of the correlation matrix
PM(n) = 1
w[PM(n −1) −KM(n)Xt
M(n)PM(n −1)]
(15.121)
5. Update the coeﬃcient vector of the ﬁlter
hM(n) = hM(n −1) + KM(n)eM(n)
(15.122)
The recursive algorithm speciﬁed by (15.118) through (15.122) is
called the direct form recursive least-squares (RLS) algorithm. It is ini-
tialized by setting hM(−1) = 0 and PM(−1) = 1/δIM, where δ is a small
positive number.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

846
Chapter 15
ADAPTIVE FILTERS
The residual MSE resulting from the preceding optimization is
EM,min(n) =
n

ℓ=0
wn−ℓd(ℓ)
2 −ht
M(n)D∗
M(n)
(15.123)
From (15.122), we observe that the ﬁlter coeﬃcients vary with time
by an amount equal to the error eM(n) multiplied by the Kalman gain
vector KM(n). Since KM(n) is an M-dimensional vector, each ﬁlter co-
eﬃcient is controlled by one of the elements of KM(n). Consequently,
rapid convergence is obtained. In contrast, the time-update equation for
the coeﬃcients of the ﬁlter adjusted by use of the LMS algorithm is
hM(n) = hM(n −1) + ∆X∗(n)eM(n)
(15.124)
which has only the single parameter ∆for controlling the adjustment rate
of the coeﬃcients.
□
EXAMPLE 15.15
Comparison of Convergence Characteristics of LMS Algorithm with
RLS Algorithm
A major advantage of the direct form RLS algorithm over the LMS algorithm
is their faster convergence rate. This characteristic behavior is illustrated in
Figure 15.44, which shows the convergence rate of the LMS and direct form
RLS algorithm for an adaptive FIR channel equalizer of length M = 11. The
statistical autocorrelation matrix TM for the received signal has an eigenvalue
ratio λmax/λmin = 11. All the equalizer coeﬃcients were initially set to zero.
The step size for the LMS algorithm was selected as ∆= 0.02, which represents
a good compromise between convergence rate and excess MSE.
The superiority of the RLS algorithm in achieving faster convergence is
clearly evident. The algorithm converges in less than 70 iterations (70 signal
samples), while the LMS algorithm has not converged in over 600 iterations.
10–1
100
10–2
00
100
200
300
400
500
600
700
Number of Iterations
Mean-Square Error
RLS Algorithm
w 
 0.999
Gradient Algorithm
FIGURE 15.44
Learning curves for RLS and LMS algorithms for adaptive equal-
izer of length M = 11. The eigenvalue spread of the channel is λmax/λmin = 11.
The step size for the LMS algorithm is ∆= 0.02
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Adaptive Direct-Form FIR Filters
847
This rapid rate of convergence of the RLS algorithm is extremely important
in applications in which the signal statistics vary rapidly with time. For exam-
ple, the time variations of the characteristics of an ionospheric high frequency
(HF) radio channel are too rapid to be adaptively followed by the LMS algo-
rithm. However, the RLS algorithm adapts suﬃciently fast to track such rapid
variations (Hsu [33]).
□
The LDU Factorization and Square-Root Algorithms.
The RLS
algorithm is very susceptible to round-oﬀnoise in an implementation of
the algorithm with ﬁnite-precision arithmetic. The major problem with
round-oﬀerrors occurs in the updating of PM(n). To remedy this problem,
we may perform a decomposition of either the correlation matrix RM(n)
or its inverse PM(n). There are several possible decompositions that can
be used to reduce the sensitivity to round-oﬀerrors. To be speciﬁc, let us
consider an LDU decomposition of PM(n). We may write
PM(n) = LM(n) ¯DM(n)LH
M(n)
(15.125)
where LM(n) is a lower-triangular matrix with elements lik, ¯DM(n) is
a diagonal matrix with elements δk, and LH
M(n) is an upper-triangular
matrix. The diagonal elements of LM(n) are set to unity (i.e., lii = 1).
Now, instead of computing PM(n) recursively, we can determine a formula
for updating the factors LM(n) and ¯DM(n) directly, thus avoiding the
computation of PM(n).
The desired update formula is obtained by substituting the factored
form of PM(n) into (15.121) and using (15.107). Thus we have
LM(n) ¯DM(n)LH
M(n)
= 1
wLM(n−1)

¯DM(n−1)−
1
w+µM(n)VM(n −1)V H
M(n −1)

LH
M(n−1)
(15.126)
where, by deﬁnition,
VM(n −1) = ¯DM(n −1)LH
M(n −1)X∗
M(n)
(15.127)
The term inside the brackets in (15.126) is a Hermitian matrix and may
be expressed in an LDU factored form as
ˆLM(n −1) ˆDM(n −1) ˆLH
M(n −1)
= ¯DM(n −1) −
1
w + µM(n)VM(n −1)V H
M(n −1)
(15.128)
Then, if we substitute (15.128) into (15.126), we obtain
LM(n) ¯DM(n) ˆLH
M(n)
= 1
w[LM(n −1) ˆLM(n −1) ˆDM(n −1) ˆLH
M(n −1)LH
M(n −1)]
(15.129)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

848
Chapter 15
ADAPTIVE FILTERS
Consequently, the desired update relations are
LM(n) = LM(n −1) ˆLM(n −1)
¯DM(n) = 1
w
ˆDM(n −1)
(15.130)
The resulting algorithm, obtained from the time-update equations
in (15.130), depends directly on the data vector XM(n) and not on the
“square” of the data vector. Thus the squaring operation of the data
vector is avoided, and, consequently, the eﬀect of round-oﬀerrors is sig-
niﬁcantly reduced.
The RLS algorithms obtained from an LDU decomposition of either
RM(n) or PM(n) are called square-root RLS algorithms. Bierman [2],
Carlson and Culmone [5], and Hsu [33] treat these types of algorithms. A
square-root RLS algorithm based on the LDU decomposition of PM(n),
as just described, is given in Table 15.1. Its computational complexity is
proportional to M 2.
TABLE 15.1
LDU form of square-root RLS algorithm
for j = 1, . . . , 2, . . . , M do
fj = x∗
j(n)
end loop j
for j = 1, 2, . . . , M −1 do
for i = j + 1, j + 2, . . . , M do
fj = fj + lij(n −1)fi
end loop j
for j = 1, 2, . . . , M do
¯dj(n) = dj(n −1)/w
vj = ¯dj(n)fi
end loop j
αM = 1 + vMf ∗
M
dM(n) = ¯dM(n)/αM
¯kM = vM
for j = M −1, M −2, . . . , 1 do
¯kj = vj
αj = αj+1 + vjf ∗
j
λj = fj/αj+1
dj(n) = ¯dj(n)αj+1/α1
for i = M, M −1, . . . , j + 1 do
lij(n) = lij(n −1) + ¯k∗
i λj
¯ki = ¯ki + vjl∗
ij(n −1)
down to j = 2)
end loop i
end loop j
¯KM(n) = [¯k1, ¯k2, . . . , ¯kM]
t
eM(n) = d(n) −¯d(n)
hM(n) = hM(n −1) + [eM(n)/α1] ¯KM(n)
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

Summary and References
849
15.3 SUMMARY AND REFERENCES
We have presented adaptive algorithms for direct form FIR ﬁlter struc-
tures. The algorithms for the direct form FIR ﬁlter consisted of the simple
LMS algorithm due to Widrow and Hoﬀ[95] and the direct form, time-
recursive least-squares algorithms, including the conventional RLS form
given by (15.118–15.122), and the square-root RLS forms described by
Bierman [2], Carlson and Culmone [5], and Hsu [33].
Of these algorithms, the LMS algorithm is the simplest. It is used in
many applications where its slow convergence is adequate. Of the direct
form RLS algorithms, the square-root algorithms have been used in ap-
plications where fast convergence is required. The algorithms have good
numerical properties.
In the direct form RLS algorithms, we used exponential weighting into
the past in order to reduce the eﬀective memory in the adaptation process.
As an alternative to exponential weighting, we may employ ﬁnite-length
uniform weighing into the past. This approach leads to the class of ﬁnite-
memory RLS direct form algorithms described in Cioﬃand Kalaith [7]
and Manolakis, Ling, and Proakis [62].
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

BIBLIOGRAPHY
[1] F. S. Beckman. “The Solution of Lin-
ear Equations by the Conjugate Gradi-
ent Method,” in Mathematical Methods for
Digital Computers, A. Ralston and H. S.
Wilf, eds., Wiley, New York, 1960.
[2] G. J. Bierman. Factorization Methods for
Discrete Sequential Estimation, Academic
Press, New York, 1977.
[3] R. C. Brown. Introduction to Random Sig-
nal Analysis and Kalman Filtering, Wiley,
New York, 1983.
[4] C. Caraiscos and B. Liu. “A RoundoﬀEr-
ror Analysis of the LMS Adaptive Algo-
rithm,” IEEE Trans. Acoustics, Speech,
and Signal Processing, vol. ASSP-32, pp.
34–41, January 1984.
[5] N.
A.
Carlson
and
A.
F.
Culmone.
“Eﬃcient Algorithms for On-Board Ar-
ray Processing,” Record 1979 Interna-
tional Conference on Communications,
pp. 58.1.1–58.1.5, Boston, June 10–14,
1979.
[6] C. K. Chui and G. Chen. Kalman Filter-
ing, Springer-Verlag, New York, 1987.
[7] J. M. Cioﬃand T. Kailath. “Windowed
Fast Transversal Filters Adaptive Algo-
rithms with Normalization,” IEEE Trans.
Acoustics, Speech, and Signal Processing,
vol. ASSP-33, pp. 607–625, June 1985.
[8] J. W. Cooley and J. W. Tukey. “An
Algorithm for the Machine Computation
of Complex Fourier Series,” Mathematical
Computations, vol. 19, pp. 297–301, April
1965.
[9] R. E. Crochiere and L. R. Rabiner. Mul-
tirate Digital Signal Processing, Prentice
Hall, Englewood Cliﬀs, NJ, 1983.
[10] C. de Boor. A Practical Guide to Splines,
Springer-Verlag, New York, 1978.
[11] J.
R.
Deller,
J.
H.
L.
Hansen,
and
J. G. Proakis. Discrete-Time Processing of
Speech Signals, Wiley-IEEE Press, 2000.
[12] P. Delsarte and Y. Genin. “The Split
Levinson
Algorithm,”
IEEE
Trans.
Acoustics, Speech, and Signal Processing,
vol. ASSP-34, pp. 470–478, June 1986.
[13] P. Delsarte, Y. Genin, and Y. Kamp.
“Orthogonal
Polynomial
Matrices
on
the Unit Circle,” IEEE Trans. Circuits
and Systems, vol. CAS-25, pp. 149–160,
January 1978.
[14] J.
Durbin.
“Eﬃcient
Estimation
of
Parameters
in
Moving-Average
Mod-
els,” Biometrika, vol. 46, pp. 306–316,
1959.
[15] J. L. Flanagan et al. “Speech Coding,”
IEEE Transactions on Communications,
vol. COM-27, pp. 710–736, April 1979.
[16] R. Fletcher and M. J. D. Powell. “A
Rapidly
Convergent
Descent
Method
for Minimization,” Comput. J., vol. 6,
pp. 163–168, 1963.
[17] B. Friedlander. “Lattice Filters for Adap-
tive Processing,” Proc. IEEE, vol. 70,
pp. 829–867, August 1982.
[18] B.
Friedlander.
“Lattice
Methods
for
Spectral
Estimation,”
Proc.
IEEE,
vol. 70, pp. 990–1017, September 1982.
[19] F. R. Gantmacher. The Theory of Matri-
ces, vol. I, Chelsea Publishing Company,
Chelsea, NY, 1960.
[20] W. A. Gardner. “Learning Character-
istics
of
Stochastic-Gradient-Descent
Algorithms: A General Study, Analysis
and Critique,” Signal Processing, vol. 6,
pp. 113–133, April 1984.
[21] D. A. George, R. R. Bowen, and J. R.
Storey. “An Adaptive Decision-Feedback
Equalizer,” IEEE Trans. Communication
Technology, vol. COM-19, pp. 281–293,
June 1971.
[22] L. Y. Geronimus. Orthogonal Polynomials
(in Russian) (English translation by Con-
sultant’s Bureau, New York, 1961), 1958.
[23] A. Gersho. “Adaptive Equalization of
Highly
Dispersive
Channels
for
Data
Transmission,”
Bell
Syst.
Tech.
J.,
vol. 48, pp. 55–70, January 1969.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

BIBLIOGRAPHY
B-1
[24] R. D. Gitlin and S. B. Weinstein. “On
the Required Tap-Weight Precision for
Digitally
Implemented
Mean-Squared
Equalizers,” Bell Syst. Tech. J., vol. 58,
pp. 301–321, February 1979.
[25] R. D. Gitlin, H. C. Meadors, and S. B.
Weinstein. “The Tap-Leakage Algorithm:
An Algorithm for the Stable Operation
of a Digitally Implemented Fractionally
Spaced, Adaptive Equalizer,” Bell Syst.
Tech. J., vol. 61, pp. 1817–1839, October
1982.
[26] I.
Gohberg,
ed.
I. Sch¨ur Methods in
Operator Theory and Signal Processing,
Birkhauser Verlag, Stuttgart, Germany,
1986.
[27] J. A. Greefties. “A Digitally Companded
Delta
Modulation
Modem
for
Speech
Transmission,” In Proceedings of IEEE
International Conference on Communica-
tions, pp. 7.33–7.48, June 1970.
[28] O. Grenander and G. Szeg¨o. Toeplitz
Forms and Their Applications, University
of California Press, Berkeley, CA, 1958.
[29] D. Hanselman and B. Littleﬁeld. Master-
ing MATLAB 7, Pearson/Prentice Hall,
Englewood Cliﬀs, NJ, 2005.
[30] S. Haykin. Adaptive Filter Theory, 3rd ed.,
Prentice-Hall, Englewood Cliﬀs, NJ, 1996.
[31] A. S. Householder. “Unitary Triangu-
larization of a Non-Symmetric Matrix,”
J. Assoc. Comput. Math., vol. 5, pp.
204–243, 1958.
[32] A. S. Householder. The Theory of Ma-
trices in Numerical Analysis, Blaisdell,
Waltham, MA, 1964.
[33] F. M. Hsu. “Square-Root Kalman Filter-
ing for High-Speed Data Received over
Fading Dispersive HF Channels.” IEEE
Trans. Information Theory, vol. IT-28,
pp. 753–763, September 1982.
[34] F. M. Hsu and A. A. Giordano. “Digital
Whitening
Techniques
for
Improving
Spread Spectrum Communications Per-
formance in the Presence of Narrowband
Jamming and Interference,” IEEE Trans.
Communications,
vol.
COM-26,
pp.
209–216, February 1978.
[35] B.
R.
Hunt,
R.
L.
Lipsman,
J.
M.
Rosenberg, and Kevin R. Coombes. A
Guide to MATLAB: For Beginners and
Experienced Users, Cambridge University
Press, New York, NY, 2001.
[36] V. K. Ingle and J. G. Proakis. Digital
Signal Processing using the ADSP-2101,
Prentice Hall, Englewood Cliﬀs, NJ, 1991.
[37] L. B. Jackson. “An Analysis of Limit
Cycles Due to Multiplicative Rounding in
Recursive Digital Filters,” Proceedings of
the 7th Allerton Conference on Circuits
and System Theory, pp. 69–78, 1969.
[38] N. S. Jayant. “Adaptive Delta Modula-
tion with One-Bit Memory,” Bell System
Technical Journal, pp. 321–342, March
1970.
[39] N. S. Jayant. “Digital Coding of Speech
Waveforms:
PCM,
DPCM,
and
DM
Quantizers.” Proceedings of the IEEE,
vol. 62, pp. 611–632, May 1974.
[40] S. K. Jones, R. K. Cavin, and W. M.
Reed. “Analysis of Error-Gradient Adap-
tive Linear Equalizers for a Class of
Stationary-Dependent Processes,” IEEE
Trans. Information Theory, vol. IT-28,
pp. 318–329, March 1982.
[41] T. Kailath. “A View of Three Decades
of Linear Filter Theory,” IEEE Trans.
Information
Theory,
vol.
IT-20,
pp.
146–181, March 1974.
[42] T.
Kailath.
Lectures
on
Wiener
and
Kalman Filtering, Springer-Verlag, New
York, 1981.
[43] T.
Kailath.
“Linear
Estimation
for
Stationary
and
Near-Stationary
Pro-
cesses,”
in
Modern
Signal
Processing,
T. Kailath, ed., Hemisphere Publishing
Corp., Washington, D.C., 1985.
[44] T. Kailath. “A Theorem of I. Sch¨ur and
Its Impact on Modern Signal Processing,”
in [26].
[45] T. Kailath, A. C. G. Vieira, and M. Morf.
“Inverses of Toeplitz Operators, Inno-
vations, and Orthogonal Polynomials,”
SIAM Rev., vol. 20, pp. 1006–1019, 1978.
[46] R. E. Kalman. “A New Approach to Lin-
ear Filtering and Prediction Problems,”
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

B-2
BIBLIOGRAPHY
Trans. ASME, J. Basic Eng., vol. 82D,
pp. 35–45, March 1960.
[47] R. E. Kalman and R. S. Bucy. “New
Results
in
Linear
Filtering
Theory,”
Trans. ASME, J. Basic Eng., vol. 83,
pp. 95–108, 1961.
[48] S. M. Kay. Modern Spectral Estimation,
Prentice-Hall, Englewood Cliﬀs, NJ, 1988.
[49] S. M. Kay and S. L. Marple, Jr. “Spec-
trum Analysis: A Modern Perspective,”
Proc.
IEEE,
vol.
69,
pp.
1380–1419,
November 1981.
[50] J.
W.
Ketchum
and
J.
G.
Proakis.
“Adaptive Algorithms for Estimating and
Suppressing
Narrow-Band
Interference
in PN Spread-Spectrum Systems,” IEEE
Trans. Communications, vol. COM-30,
pp. 913–923, May 1982.
[51] H. Krishna. “New Split Levinson, Sch¨ur,
and Lattice Algorithms or Digital Signal
Processing,”
Proc.
1988
International
Conference on Acoustics, Speech, and Sig-
nal Processing, pp. 1640–1642, New York,
April 1988.
[52] S. Y. Kung and Y. H. Hu. “A Highly
Concurrent
Algorithm
and
Pipelined
Architecture for Solving Toeplitz Sys-
tems,” IEEE Trans. Acoustics, Speech,
and
Signal
Processing,
vol.
ASSP-31,
pp. 66–76, January 1983.
[53] A.
Leon-Garcia.
Probability, Statistics,
and Random Processes for Electrical En-
gineering, Pearson Prentice Hall, Upper
Saddle River, NJ, third edition, 2008.
[54] N. Levinson. “The Wiener RMS Error Cri-
terion in Filter Design and Prediction,”
J. Math. Phys., vol. 25, pp. 261–278,
1947.
[55] R. W. Lucky. “Automatic Equalization
for Digital Communications,” Bell Syst.
Tech. J., vol. 44, pp. 547–588, April
1965.
[56] F. R. Magee and J. G. Proakis. “Adap-
tive
Maximum-Likelihood
Sequence
Estimation for Digital Signaling in the
Presence
of
Intersymbol
Interference,”
IEEE Trans. Information Theory, vol.
IT-19, pp. 120–124, January 1973.
[57] J. Makhoul. “Linear-Prediction: A Tu-
torial
Review,”
Proc.
IEEE,
vol.
63,
pp. 561–580, April 1975.
[58] J. Makhoul. “Stable and Eﬃcient Lattice
Methods for Linear Prediction,” IEEE
Trans. Acoustics, Speech, and Signal Pro-
cessing, vol. ASSP-25, pp. 423–428, 1977.
[59] J. Makhoul. “A Class of All-Zero Lattice
Digital Filters: Properties and Applica-
tions.” IEEE Trans. Acoustics, Speech,
and
Signal
Processing,
vol.
ASSP-26,
pp. 304–314, August 1978.
[60] D. G. Manolakis and V. K. Ingle. Applied
Digital
Signal
Processing,
Cambridge
University Press, Cambridge, UK, 2011.
[61] D. G. Manolakis, V. K. Ingle, and S. M.
Kogon. Statistical and Adaptive Signal
Processing, Artech House, Norwood, MA,
2005.
[62] D. G. Manolakis, F. Ling, and J. G.
Proakis. “Eﬃcient Time-Recursive Least-
Squares Algorithms for Finite-Memory
Adaptive Filtering,” IEEE Trans. Circuits
and Systems, vol. CAS-34, pp. 400–408,
April 1987.
[63] J. D. Markel and A. H., Gray, Jr. Linear
Prediction
of
Speech,
Springer-Verlag,
New York, 1976.
[64] S. L. Marple, Jr. Digital Spectral Anal-
ysis
with
Applications,
Prentice-Hall,
Englewood Cliﬀs, NJ, 1987.
[65] J. E. Mazo. “On the Independence The-
ory of Equalizer Convergence,” Bell Syst.
Tech. J., vol. 58, pp. 963–993, May 1979.
[66] J. E. Meditch. Stochastic Optimal Linear
Estimation
and
Control,
McGraw-Hill,
New York, 1969.
[67] S. L. Miller and D. G. Childers. Probability
and Random Processes, Academic Press,
Burlington, MA, second edition, 2012.
[68] S. K. Mitra. Digital Signal Processing: A
Computer-Based Approach, McGraw-Hill,
New York, NY, fourth edition, 2010.
[69] W. Murray, ed. Numerical Methods of
Unconstrained
Minimization,
Academic
Press, New York, 1972.
[70] H.
E.
Nichols,
A.
A.
Giordano,
and
J. G. Proakis. “MLD and MSE Algorithms
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

BIBLIOGRAPHY
B-3
for Adaptive Detection of Digital Sig-
nals
in
the
Presence
of
Interchannel
Interference,”
IEEE
Trans.
Informa-
tion Theory, vol. IT-23, pp. 563–575,
September 1977.
[71] A. V. Oppenheim and R. W. Schafer.
Discrete-Time Signal Processing, Prentice
Hall, Englewood Cliﬀs, NJ, third edition,
2010.
[72] S. J. Orfanidis. Introduction to Signal
Processing,
Prentice
Hall,
Englewood
Cliﬀs, NJ, 1996, pp. 367–383.
[73] A. Papoulis and S. U. Pillai. Probabil-
ity, Random Variables, and Stochastic
Processes, McGraw-Hill, New York, NY,
fourth edition, 2002.
[74] T. W. Parks and J. H. McClellan. “A
Program for the Design of Linear-Phase
Finite Impulse Response Digital Filters,”
IEEE Transactions on Audio and Elec-
troacoustics, vol. AU-20, pp. 195–199,
August 1972.
[75] B. Picinbono. “Adaptive Signal Process-
ing for Detection and Communication,”
in Communication Systems and Random
Process Theory, J. K. Skwirzynski, ed.,
Sijthoﬀen Noordhoﬀ, Alphen aan den
Rijn, The Netherlands, 1978.
[76] R. Pratap. Getting Started with MAT-
LAB7: A Quick Introduction for Scientists
and Engineers, Oxford University Press,
USA, 2005.
[77] J. G. Proakis. “Adaptive Digital Filters
for Equalization of Telephone Channels,”
IEEE Trans. Audio and Electroacoustics,
vol. AU-18, pp. 195–200, June 1970.
[78] J. G. Proakis. “Advances in Equalization
for Intersymbol Interference,” in Advances
in Communication Systems, vol. 4, A. J.
Viterbi, ed., Academic Press, New York,
1975.
[79] J. G. Proakis and D. G. Manolakis. Digital
Signal Processing: Principles, Algorithms
and
Applications,
Prentice
Hall,
Up-
per
Saddle
River,
NJ,
third
edition,
1996.
[80] J. G. Proakis and J. H. Miller. “Adaptive
Receiver for Digital Signaling through
Channels with Intersymbol Interference,”
IEEE Trans. Information Theory, vol.
IT-15, pp. 484–497, July 1969.
[81] J. G. Proakis and M. Salehi. Digital
Communications,
McGraw-Hill,
New
York, NY, ﬁfth edition, 2008.
[82] J. G. Proakis and M. Salehi. Communica-
tion Systems Engineering, Prentice Hall,
Upper Saddle River, NJ, second edition,
2012.
[83] L. R. Rabiner and B. Gold. Theory and
Applications in Digital Signal Processing,
Prentice
Hall,
Englewood
Cliﬀs,
NJ,
1975.
[84] L. R. Rabiner and R. W. Schafer. Digital
Processing of Speech
Signals,
Prentice
Hall, Englewood Cliﬀs, NJ, 1978.
[85] L.
R.
Rabiner,
R.
W.
Schafer,
and
C. A. McGonegal. “An Approach to the
Approximation Problem for Nonrecursive
Digital Filters,” IEEE Transactions on
Audio and Electroacoustics, vol. AU-18,
pp. 83–106, June 1970.
[86] E. A. Robinson and S. Treitel. “Digital
Signal
Processing
in
Geophysics,”
in
Applications of Digital Signal Processing,
A.
V.
Oppenheim,
ed.,
Prentice-Hall,
Englewood Cliﬀs, NJ, 1978.
[87] E. A. Robinson and S. Treitel. Geo-
physical Signal Analysis, Prentice-Hall,
Englewood Cliﬀs, NJ, 1980.
[88] Schur.
“On
Power
Series
Which
Are
Bounded
in the Interior of the Unit
Circle,” (in German) J. Reine Angew.
Math., vol. 147, pp. 205–232, Berlin, 1917.
(For an English translation of this paper,
see [26]).
[89] M. Schetzen and V. K. Ingle. Discrete
Systems
Laboratory
Using
MATLAB,
Brooks/Cole, Paciﬁc Grove, CA, 2000.
[90] H. Stark and J. W. Woods. Probability,
Random Processes, and Estimation The-
ory for Engineers, Prentice Hall, Upper
Saddle River, NJ, fourth edition, 2012.
[91] G. Szeg¨o. Orthogonal Polynomials, 3rd
ed., Colloquium Publishers, no. 23, Amer-
ican Mathematical Society, Providence,
RI, 1967.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

B-4
BIBLIOGRAPHY
[92] S. A. Tretter. Introduction to Discrete-
Time Signal Processing, Wiley, New York,
1976.
[93] G. Ungerboeck. “Theory on the Speed
of Convergence in Adaptive Equalizers
for Digital Communication,” IBM J. Res.
Devel., vol. 16, pp. 546–555, November
1972.
[94] A.
C.
G.
Vieira.
“Matrix
Orthogo-
nal
Polynomials
with
Applications
to
Autoregressive
Modeling
and
Ladder
Forms,” Ph.D. dissertation, Department
of Electrical Engineering, Stanford Uni-
versity, Stanford, Calif., December 1977.
[95] B. Widrow and M. E. Hoﬀ, Jr. “Adaptive
Switching
Circuits,”
IRE
WESCON
Conv. Rec., pt. 4, pp. 96–104, 1960.
[96] B.
Widrow
et
al.
“Adaptive
Noise
Cancelling Principles and Applications,”
Proc.
IEEE,
vol.
63,
pp.
1692–1716,
December 1975.
[97] B. Widrow, P. Mantey, and L. J. Grif-
ﬁths. “Adaptive Antenna Systems,” Proc.
IEEE, vol. 55, pp. 2143–2159, December
1967.
[98] N. Wiener. Extrapolation, Interpolation
and Smoothing of Stationary Time Series
with
Engineering
Applications,
Wiley,
New York, 1949.
[99] R. D. Yates and D. J. Goodman. Prob-
ability
and
Stochastic
Processes,
John
Wiley & Sons, Hoboken, NJ, 2005.
[100] D. Youla and N. Kazanjian. “Bauer-Type
Factorization of Positive Matrices and the
Theory of Matrices Orthogonal on the
Unit Circle,” IEEE Trans. Circuits and
Systems, vol. CAS-25, pp. 57–69, January
1978.
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-1
INDEX
NOTE: Italicized page references are located in the online text.
’ operator, 25
& operator, 26
(:) construct, 24
* operator, 27
.* operator, 10, 26
.ˆ operator, 10, 24
+ operator, 26
<= operator, 26
== operator, 26
A
Absolute speciﬁcations, FIR ﬁlter design, 292–293
Adaptive delta modulation (ADM), 599–600, 601
Adaptive diﬀerential PCM (ADPCM), 586
Adaptive direct-form FIR ﬁlters, 815–848
least-mean-square (LMS) algorithm, 820–824
MATLAB implementation of, 823–824
MATLAB scripts, 838
minimum mean-square-error criterion, 816–820
properties of the LMS algorithm, 824–841
recursive least-squares algorithms, 841–848
Adaptive equalizers, 573
Adaptive ﬁlters, 573–585, 769–849
applications of, 769–815
arrays, 813–815
channel equalization, 582–585, 776–782
coeﬃcient adjustment, 575–578
echo cancellation, 783–788
least-mean-square (LMS) algorithm, 575, 815
line enhancement, 582, 795–801
MATLAB implementation, 577–578
MATLAB scripts, 774, 781–782, 788, 797, 798,
799, 801, 804, 805, 806, 811–813
narrowband interference, suppression of,
579–582, 788–795
noise canceling, 801–807
sinusoidal interference, suppression of, 581–582
speech signals, linear predictive coding of,
807–813
system identiﬁcation (modeling), 578–579,
771–776
wideband signals, 579–582, 788–795
Adaptive line enhancer (ALE), 795–801
Adaptive PCM and DPCM (ADPCM), 593–597
Adders, 213
Addition, signal operation, 25
Adjustable coeﬃcients, 573
A/D quantization noise, 518–530. See also
Quantization
statistics of, 525
through digital ﬁlters, 527–530
afd butt function, 392
afd chb1 function, 398
afd elip function, 405
Aliasing formula, 81
Allpass ﬁlters, 381–382
Allpass function, 506
All-pole systems, estimation of poles in, 811–813
alpha function, 483
Amplitude response, 298
ampl-res function, 303
Analog ﬁlter design (AFD) tables, 370
Analog ﬁlters
Butterworth lowpass, 371, 385–390
Chebyshev lowpass, 371, 394–403
design equations, 390–391
elliptic lowpass, 403–406
frequency response, 298–301
IIR ﬁlter design, 370–457
MATLAB implementation, 387–390, 392–394,
396–403, 427–432
phase responses, 75–77
prototype responses, 385–407
Analog signals, 3–5, 80–97
aliasing formula, 81
band-limited, 82
cubic spine interpolation, 92
digital signal processing (DSP) compared to, 1–2
digital-to-analog (D/A) converters, 80–81
discrete-time Fourier analysis of, 59–102
ﬁrst-order-hold (FOH) interpolation, 91–92, 94
impulse train conversion, 87
interpolation, 90–94
MATLAB implementation, 84–87, 92–97
reconstruction of, 87–92
sampling, 81–84
signal processing (ASP), 3–5
sinusoidal, 75
zero-order-hold (ZOH) interpolation, 90–91, 94
Analog-to-digital conversion (ADC), 80
analysis of A/D quantization noise, 518–530
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-2
INDEX
Analog-to-digital transformations, 407–427
bilinear transformations, 418–426
Butterworth lowpass ﬁlters, 371, 385–390
Chebyshev lowpass ﬁlters, 371, 394–403
design procedure, 390–391
elliptic lowpass ﬁlters, 403–406
impulse invariance transformations, 408–418
matched-z transformation, 426–427
Analysis
error, 182–183
of excess noise, 829–832
Analyzers, spectrum, 291
angle function, 63
Anticausal sequence, 106
Antisymmetric impulse response, 230
Applications of adaptive ﬁlters, 769–815
arrays, 813–815
channel equalization, 776–782
echo cancellation, 783–788
line enhancement, 795–801
MATLAB scripts, 774, 781–782, 788, 797, 798,
799, 801, 804, 805, 806, 811–813
narrowband interference, suppression of,
788–795
noise canceling, 801–807
speech signals, linear predictive coding of,
807–813
system identiﬁcation (modeling), 771–776
wideband signals, 788–795
Approximate transition bandwidth, 312
Approximation error, 331
Approximation theory, 349
Arrays, adaptive, 813–815
Attenuation parameter A, 372
Auto-correlation, 647
power spectral density (PSD), 653–654
sequences, relationships, 695–700
Automatic gain control (AGC), 779
Autoregressive (AR) lattice structure, 734–736
Autoregressive moving average (ARMA) ﬁlter, 212
lattice-ladder ﬁlters, 737–743
Autoregressive moving average (ARMA) process,
692–695
Autoregressive (AR) process, 692
Average linear relationships, 635, 636
Average power, 647
Averages
joint statistical, 634–637
statistical, 618–619
B
Backward prediction-error ﬁlter, 732
Band-edge frequencies, 294
Band-limited signals, 82
reconstruction of, 89
sampling, 83
Bandpass ﬁlters, 339, 441
Bandpass process, 679–684
Bandstop ﬁlters, 441
Bartlett window, 314–315
Bayes rules, 630
beta function, 330
Biased format, 247–248
Bilateral z-transform, 103–107
bilinear function, 419, 420
Bilinear transformations, 408, 418–426
bin2dec function, 242
Binary digital communications, 609–610
Binary digits (bits), 240
Binary numbers, 252–253
Binary point, 248
Binary spread-spectrum communications, 612–613
Biquads, 214
bitcmp function, 243, 245
Bivariate Gaussian distribution, 637–641
Blackman window, 317
Block convolutions, 183–185
high-speed, 199
overlap-add method, 186
Bounded-input bounded-output (BIBO) stability,
40
Butterworth lowpass ﬁlters, 371, 385–390
bilinear transformation, 408, 418–426
design equations, 390–391
frequency-based transformation, 432–435
impulse invariance transformation, 408–418
MATLAB implementation, 387–390, 392–394
buttord function, 441
C
Cancellation
echo, 783–788, 836–838
noise, 801–807
cas2dir function, 219, 230
Cascade form, 214, 216–217, 562–564
ﬁnite-duration impulse response (FIR) ﬁlters,
228, 229–230
inﬁnite-duration impulse response (IIR) ﬁlters,
214
casﬁltr function, 218, 565
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-3
Causal ﬁlters, 373
Causality, 40
linear time-invariant (LTI) system, 40
z-transform, 125–128
CCITT standard, 595
Ceiling function, 197
Central moments, 619
Central processing unit (CPU), 240
Channel equalization, 582–585, 776–782
least-mean-square (LMS) algorithm, 834–836
MATLAB scripts, 781–782, 835, 836
cheby1 function, 442
cheby2 function, 442
Chebyshev error, 344, 347
Chebyshev lowpass ﬁlters, 371, 394–403
MATLAB implementation, 396–397, 398–403
circevod function, 169
circonvt function, 177
Circular conjugate symmetry, 168
Circular convolution, 174–179
Circular-even components, 169
Circular folding, 166
Circular-odd components, 169
Circular shift of sequences, 171
cirshftt function, 172, 177
Closed-loop control systems, 825
Code division multiple access (CDMA), 611
Coeﬃcients
adjustable, 573, 575–578
correlation, 637
ﬁnite-duration impulse response (FIR) ﬁlters,
707–715
lattice reﬂection, 707–715
optimum reﬂection, 715–716
prediction, 701
reﬂection, 702
Column vectors, 63, 119
Comb ﬁlters, 379–380
Commands, MATLAB controls and ﬂow of, 11–13
Common form, 346
Communications, 586–613
adaptive delta modulation (ADM), 599–601
adaptive PCM and DPCM (ADPCM), 593–597
binary digital, 609–610
delta modulation, 597–601
diﬀerential PCM (DPCM), 590–593
linear predictive coding (LPC) of speech,
601–605
pulse-code modulation (PCM), 586–590
spread-spectrum, 611–613
Compander, 589
Complex conjugation property, 107
Complex exponential sequences, 24, 36,
74–75
Complex power spectral density (PSD), 656
Complex-valued exponential sequence, 24
Complex-valued mappings, 270
Computation
of cross-correlation, 660
of cross-spectral density (CSD), 660
of output mean function, 659–660
of power spectral density (PSD), 661–664
Conditional probability functions, 629–630
Conjugate-summary property, 65
Conjugation property, 68
Constant group delay, 297
Constant phase delay, 296
Continuous linear function, 298
Continuously variable slope delta modulation
(CVSD), 600
Continuous random variables, 616
Continuous-time Fourier transform (CTFT),
81, 86
Convergence of least-mean-square (LMS)
algorithm, 824–828
conv function, 43–44, 108, 435
conv m function, 108
Convolution, 40–47
block, 183–185
discrete Fourier transform (DFT), 174–179,
180–186
discrete-time Fourier transform (DTFT), 68
error analysis, 182–183
fast, 197–199
graphical, 42–43
high-speed, 197
high-speed block, 199
linear sum, 39–40
linear time-invariant (LTI) system, 38–40
MATLAB implementation of, 43–45, 47
overlap-add method, 186
periodic, 69
sequence correlation, 45–47
Correlation
coeﬃcients, 637
concept of, 636
cross-correlation functions, 644
of sequences, 36, 45–47
cos function, 24–25
Covariance, 636
cross-covariance functions, 644
normalized, 637
cplxcomp function, 223
cplxpair function, 218, 223
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-4
INDEX
Cross-correlation, 36, 45–47. See also Correlation
computation of, 660
functions, 644
Cross-covariance functions, 644. See also
Covariance
Cross-spectral density (CSD), 654, 660
Cubic spine interpolation, 92
Cumulative distribution function (CDF), 616
Curves, learning, 827
Cutoﬀfrequency, 310
cutoﬀfunction, 473
D
Dead band, 533
dec2bin function, 242, 243
dec3bin function, 245
Decibels (dB), 292
Decimal numbers, 251–252
decimate function, 467
Decimation
design speciﬁcations, 494–496
downsampling, 461–466
by a factor D, 461–470
ideal decimators, 466–470
integers, 490–496
MATLAB implementation, 462–466, 467–470,
491–493
Decimation-in-frequency (DIF-FFT), 188, 195
Decimation-in-time (DIT-FFT), 188, 193
Deconvolution, 109
Delay element, 213
Delta modulation (DM), 597–601, 807
Denominator polynomials, 105
Denormalized number, 254
Dense spectrum of signals, 159
Density, probability, 615
dfs function, 144, 156
dft function, 177
Diﬀerence equations, 47–53
digital ﬁlters, 52–53
frequency response, 76–80
homogeneous solution, 47
linear time-invariant (LTI) system, 47–53
MATLAB implementation of, 48–51
natural frequencies, 48
representation, 119
zero-state input and responses, 51–52
z-transform, 128–134
Diﬀerential PCM (DPCM), 590–593, 807
Diﬀerentiators, 341
Digital ﬁlters, 52–53, 291, 432
A/D quantization noise, 518–530, 525, 527–530
MATLAB analysis of, 520–525
MATLAB implementation of, 526–527, 528–530
round-oﬀeﬀects, 518–572, 530–557
truncation operation, 525–527
Digital resonators, 374–377
Digital signal processing (DSP), 1–21
analog signal processing (ASP) compared to, 3–4
analysis, 2–4
applications of, 18–20
echo generation, 18
echo removal, 19
MATLAB and, 1–2, 18–20
method of, 2–5
musical sound processing, 18
reverberation, 18, 20
signal ﬁltering, 5
Digital sinusoidal oscillators, 382–384
Digital-to-analog conversion (DAC), 80–81
dir2cas function, 217, 223, 228, 230, 268
dir2par function, 221, 223
Direct form, 214–215
ﬁnite-duration impulse response (FIR) ﬁlters,
228, 229, 501–504
inﬁnite-duration impulse response (IIR) ﬁlters,
214–215
MATLAB, 229
Direct-form realization, 558
Discontinuous function, 298
Discrete Fourier series (DFS), 141, 142–147
MATLAB implementation of, 143–146
matrix, 144
relation to the DTFT, 148
relation to z-transform, 146–147
Discrete Fourier transform (DFT), 64, 154–165
block convolutions, 183–185
circular conjugate symmetry, 168
circular convolution, 174–179
circular-even components, 169
circular folding, 166
circular-odd components, 169
circular shift of sequences, 171
decimation-in-frequency (DIF-FFT), 195
decimation-in-time (DIT-FFT), 193
dense spectrum of signals, 159
divide-and-combine approach, 190–193
DTFT interpolation formula, 152–153
energy spectrum, 180
error analysis, 182–183
fast convolutions, 197–199
fast Fourier transform (FFT), 187–199
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-5
frequency-domain approach, 177
goal of eﬃcient computation, 187–190
high-density spectrum, 162
high-resolution spectrum, 162
high-speed block convolutions, 199
linear convolution using, 180–186
MATLAB implementation of, 143–146, 153,
156–165, 185–186, 195–197
matrix, 156
merging formula, 193
N-point sequence, 154
Nyquist component, 169
overlap-add method, 186
periodic conjugate symmetry, 168
periodic shift of sequences, 172
power spectrum, 180
properties of, 165–180
radix-2 FFT algorithm, 193–196
rectangular window, 150
relation to z-transform, 146–147
sampling, 149–152
time-domain approach, 176
twiddle factor, 191
zero-padding operation, 159
z-transform reconstruction formula, 152
Discrete random variables, 616
Discrete-time ﬁlters, 212–290
adder, 213
basic elements of, 213
delay element, 213
error characteristics, 255–262
ﬁlter coeﬃcients, 262–277
ﬁnite-precision numerical eﬀects, 239–240
FIR systems, 228–239
IIR systems, 214–228
MATLAB implementation of, 216, 217, 221–228,
229, 230–233, 234–239, 242, 245, 247
multipliers (gain), 213
pole-zero locations, 262–269
quantization and, 255–277
representation of numbers, 240–255
Discrete-time Fourier transform (DTFT), 59–67
analog signals, 80–97
arbitrary sequences, 75–76
column vectors, 63
complex exponential sequences, 74–75
conjugate-summary property, 65
conjugation property, 68
convolution, 68
discrete Fourier series (DFS) relation to, 148
energy density spectrum, 69
energy property, 69
ﬁnite-duration causal sequence, 121
folding property, 68, 72
frequency response, 74
frequency shifting property, 68, 70
interpolation formula, 152–153
inverse discrete-time Fourier transform
(IDTFT), 60
linearity property, 67
linear time-invariant (LTI) systems, 74–80
MATLAB implementation of, 61–66
matrix-vector multiplication operation, 62
multiplication property, 69
periodicity property, 61
properties, 60–61, 67–73
sequences, 66–67
sinusoidal sequences, 75
symmetries in real sequences, 68
symmetry property, 61, 66
time shifting property, 68
Discrete-time LTI systems, 664–668
Discrete-time Markov process, 674–675
Discrete-time signals, 22–36
complex-valued exponential sequence, 24
correlation of sequences, 36
even and odd synthesis, 34–36
exponential sequences, 24, 51
ﬁnite-duration sequence, 43
geometric series, 36, 41
inﬁnite-duration sequence, 23
MATLAB representations, 23
number sequence, 22
operations on sequences, 25–32
periodic sequence, 25
random sequences, 25
real-valued exponential sequence, 24
sequences, 22–36
sinusoidal sequence, 24–25
unit sample sequence, 23
unit sample synthesis, 34–36
unit step sequence, 23–24
Discrete-time systems, 36–40
convolution, 40–47
diﬀerence equations, 47–53
digital ﬁlters, 52–53
excitation, 36
linear, 37–40
linear time-invariant (LTI) system, 38, 40, 52
MATLAB implementation for, 43–45
response, 51–52
sequence correlation, 45–47
zero-state input and responses, 51–52
Discrete-time white noise process, 672–674
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-6
INDEX
Distribution
Gaussian. See Gaussian distribution
sinusoidal, 623–625
uniform, 620–621
Divide-and-combine approach, 190–193
Dot-product, 19
downsample function, 462
dtft function, 64
Dual-tone multifrequency (DTMF) signals,
605–609
E
Echo cancellation, 783–788
least-mean-square (LMS) algorithm, 836–838
MATLAB scripts, 788, 836, 837
Echo cancellers, 784, 785, 787
modems, 786
Nyquist rates, 786
Echo generation, 18
Echo suppressors, 784
edges function, 617
ellip function, 442, 677, 680
Elliptic ﬁlters, 371
Elliptic lowpass ﬁlters, 403–406
MATLAB implementation, 404–406
Empirical design functions, 318
Energy
density spectrum, 69
property, 69
signals, 28
spectrum, 180
Equalization, channel, 582–585, 776–782
Equalizers, adaptive, 573
Equations
diﬀerence, 47–53
normal, 705, 717–730
order-recursive, 703
Weiner-Hopf, 745
Equiripple ﬁlters, 344–360
constraint of extrema, 348–350
MATLAB implementation, 351–360
minimax problem, 344–348
Parks-McClellan algorithm, 350–351
responses, 394
Ergodicity, 646–648
Errors
analysis, 182–183
discrete-time ﬁlters, 255–262
functions, 621
interpolation, 459
Estimation of the mean, 625–627
etime function, 195
Even and odd synthesis, 34–36
evenodd function, 72
Exact transition bandwidth, 313
Excess noise, analysis of, 829–832
Excitation, 36
Exponential sequences, 24, 51
complex, 24, 36
geometric series, 36
real-valued, 24
Extraripple ﬁlters, 350
Extrema, 344
constraint of, 348–350
F
Far-end echo, 784
Fast convolutions, 197–199
Fast Fourier transform (FFT), 64, 142, 187–199
ceiling function, 197
decimation-in-frequency (DIF-FFT), 188, 195
decimation-in-time (DIT-FFT), 188, 193
divide-and-combine approach, 190–193
dot-product, 19
fast convolutions, 197–199
goal of eﬃcient computation, 187–190
high-speed block convolutions, 199
high-speed convolution, 197
MATLAB implementation of, 195–197
merging formula, 193
radix-2 FFT algorithm, 193–196
twiddle factor, 191
ﬀtﬁlt function, 199
Filter coeﬃcients
discrete-time ﬁlters, 262–277
ﬁnite-duration impulse response (FIR) ﬁlters,
274–277
inﬁnite-duration impulse response (IIR) ﬁlters,
262–274
MATLAB implementation, 266–269
Filtered noise process, 676–684
bandpass process, 679–684
lowpass process, 676–679
MATLAB implementation of, 682
ﬁlter function, 121, 131, 219, 579, 592
Filters
adaptive, 769–849
adaptive direct-form FIR, 815–848
allpass ﬁlters, 381–382
analog-to-digital ﬁlter transformations, 407–427
autoregressive moving average (ARMA), 212
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-7
bandpass, 339, 441
bandstop, 441
Butterworth lowpass, 371, 385–390
causal, 373
Chebyshev, 371
Chebyshev lowpass, 394–403
comb ﬁlters, 379–380
design equations, 390–391
digital, 52–53, 291, 432
digital resonators, 374–377
discrete-time, 212–290
elliptic, 371
elliptic lowpass, 403–406
equiripple, 344–360
extraripple, 350
ﬁnite-duration impulse response (FIR), 52, 212,
228–239
ﬁrst-order, 542–545, 555–557
higher-order, 553–554
highpass, 339, 441
ideal, 309
inﬁnite-duration impulse response (IIR), 53,
212, 214–228
lattice-ladder, 737–743
linear prediction, 686
lowpass, 336
lowpass ﬁlter design using MATLAB, 427–432
moving average (MA), 52
noise-whitening, 689
nonrecursive, 52
notch ﬁlters, 377–379
optimum linear, 686
phase responses of, 406–407
prediction-error, 701
prototype, 432
prototype analog, 385–407
p-stage lattice, 704
second-order, 547–549, 557
stable, 373
types of, 374–384
Wiener, 743–766
ﬁltﬁlt function, 765
ﬁltic function, 132, 133
Finite diﬀerence approximation, 408
Finite-duration causal sequence, 121
Finite-duration impulse response (FIR) ﬁlters,
52, 212
absolute speciﬁcations, 292–293
approximations, 292
cascade form, 228, 229–230
coeﬃcients, 575, 707–715
direct form, 228, 229, 501–504
discrete-time ﬁlters, 228–239
ﬁlter coeﬃcients, 274–277
ﬁxed-point arithmetic, 557–560
ﬂoating-point arithmetic, 566–569
frequency-sampling design technique, 330–343
frequency-sampling form, 228, 233–239
implementation, 292
integer decimation, 490–496
integer interpolation, 482–487
linear-phase form, 228, 230–233
MATLAB analysis of, 560–566
MATLAB implementation, 301–303, 319–320,
351–360
multiple stopbands, 499–500
optimal equiripple design technique, 344–360
polyphase ﬁlter structures, 504–507
properties of linear-phase, 295–309
rational-factor rate conversion, 497–499
relative speciﬁcations, 293–295
round-oﬀeﬀects in, 557–569
sampling rate conversion, 482–510
speciﬁcations, 292
time-variant ﬁlter structures, 507–510
Wiener ﬁlters, 745–752
windowing, 309–330
Finite-duration sequence, 23, 43–44, 107, 142
Finite-precision numerical eﬀects, 239–240
ﬁr2 function, 343
ﬁr2latc function, 711, 713, 736
ﬁrpm algorithm, 483, 491
ﬁrpm function, 275, 351, 353
ﬁrpmord function, 353
First-order ﬁlters, 542–545, 555–557
First-order-hold (FOH) interpolation, 91–92, 94
Fixed-point arithmetic, 240, 557–560
one’s-complement format, 257–258
quantization, 256–261
rounding operation, 260–261
sign-magnitude format, 256–257
truncation operation, 256
two’s-complement format, 259–260
Fixed-point signed integer arithmetic, 241–248
one’s-complement format, 242–244
sign-magnitude format, 241–242
ten’s-complement format, 246–248
two’s-complement format, 244–245
Fletcher-Poweel algorithm, 821
ﬂiplr function, 710, 711
Floating-point arithmetic, 240, 251–254,
566–569
binary numbers, 252–253
decimal numbers, 251–252
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-8
INDEX
Floating-point arithmetic (continued)
IEEE 754 standard, 254–255
quantization, 261–262
Folding property, 68, 72, 107
for...end loop, 143, 177
Forward linear prediction, 701–705
Forward prediction-error ﬁlter, 730–732
freqs m function, 392
Frequencies
circular shift in, 174
natural, 48
periodicity in, 33
sampling, 141, 150. See also Sampling
uncountably inﬁnite, 141
Frequency-band
MATLAB implementation of, 441–445
tolerances, 294
transformations, 432–445
Frequency-domain approach, 177
downsampling, 462–466
resampled signals, 480
Frequency response, 74, 76–80
functions, 298–301
inﬁnite-duration impulse response (IIR) ﬁlters,
269–274
Frequency-sampling form
design technique, 330–343
ﬁnite-duration impulse response (FIR) ﬁlters,
228, 233–239
MATLAB scripts, 335
naive design method, 332–333
optimum design method, 333–343
Frequency selective type, 291
Frequency shifting property, 68, 70, 107
freqz function, 64, 121, 122, 123, 303, 320
freqz m function, 320, 392
ftype function, 330
Full-duplex transmission, 783
G
Gain, 75
variance, 528
Gaussian distribution
bivariate, 637–641
random processes, 668–670
General ﬁxed-point arithmetic, 248–251
one’s-complement format, 249
sign-magnitude format, 249
two’s-complement format, 250–251
Geometric series, 36, 41
Gibbs phenomenon, 313
Goertzel algorithm, 607–609
Granular limit cycles, 532–537
Graphical convolution, 42–43
H
Hamming window, 315–316
Hann window, 315
High-density spectrum, 162
Higher-order ﬁlters, 553–554
Highpass ﬁlters, 339, 441
High-resolution spectrum, 162
High-speed convolution, 197, 199
Histograms
as joint pdf approximations, 630
as pdf approximations, 616–618
Homogeneous solutions, 47
hsolpsav function, 199
h(X) function, 624
Hybrid devices, 783
I
Ideal ﬁlters, 309
idfs function, 145, 156
IEEE 754 standard, 254–255, 262
impinvar function, 411
imp invr function, 410, 411, 419
Improper rational function, 109
impulse function, 393
Impulse invariance transformation, 408–418
Impulse response, 37, 295–297
impz function, 48
Indexing operations, 10–11
Inﬁnite-duration impulse response (IIR) ﬁlters,
53, 212
allpass ﬁlters, 381–382
analog-to-digital ﬁlter transformations, 407–427
bilinear transformation, 408, 418–426
Butterworth lowpass ﬁlters, 385–390
cascade form, 214, 216–217
Chebyshev lowpass ﬁlters, 394–403
comb ﬁlters, 379–380
design, 370–457
design equations, 390–391
digital resonators, 374–377
digital sinusoidal oscillators, 382–384
direct form, 214–215
discrete-time ﬁlters, 214–228
eﬀect on pole-zero locations, 262–269
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-9
eﬀects on frequency response, 269–274
elliptic lowpass ﬁlters, 403–406
ﬁlter coeﬃcients, 262–274
ﬁnite diﬀerence approximation, 408
ﬁrst-order ﬁlters, 555–557
frequency-band transformations, 432–445
granular limit cycles, 532–537
higher-order ﬁlters, 553–554
impulse invariance transformation, 408–418
limit cycles, 531–532
linear fractional transformation, 418
lowpass ﬁlter design using MATLAB, 427–432
matched-z transformation, 426–427
MATLAB analysis of, 533–537, 546–547
MATLAB implementation, 216, 217, 387–390,
392–394, 396–397, 398–403, 404–406, 441–445
mirror-image symmetry, 373
multiplication quantization error, 539–542
notch ﬁlters, 374–379
overﬂow limit cycles, 537–539
parallel form, 214, 221–228
phase responses of, 406–407
prototype ﬁlters, 385–407, 432
relative-linear scale, 371–372
round-oﬀeﬀects, 530–557
second-order ﬁlters, 547–549, 557
signal-to-noise ratio (SNR), 545, 550–551
statistical noise, 542–545, 554–557
transposed structure, 216
types of ﬁlters, 374–384
Wiener ﬁlters, 755–762
Inﬁnite-duration sequence, 23
Inﬁnite sums, 141
Innovations process, 687–700
In-phase components, 681
Input
poles, 130
sequences, 79
zero-state responses and, 51–52
Integers
decimation, 490–496
design speciﬁcations, 487, 494–496
interpolation, 482–487
MATLAB implementation, 483–487, 491–493
Interference, 611
narrowband, suppression of, 788–795
Interpolation, 459–461
cubic spine, 92
design speciﬁcations, 487
DTFT formula, 152–153
by factor I, 470–475
ﬁrst-order-hold (FOH), 91–92, 94
integers, 482–487
MATLAB implementation, 483–487
zero-order-hold (ZOH), 90–91, 94
Intervals, sampling, 81
intﬁlt function, 483
Inverse discrete-time Fourier transform
(IDTFT), 60
Inversion of z-transform, 112–118
J
Jamming, 611
Joint central moments, 636
Joint cumulative distribution function, 628
Joint probability functions, 628–634
conditional probability functions, 629–630
functions, 631–634
histograms as joint pdf approximations, 630
marginal probability functions, 629
scatter-plot as pdf plots, 631
statistical independence, 630
transformations, 631
Joint statistical averages, 634–637
concepts, 636
correlation coeﬃcients, 637
orthogonal random variables, 637
uncorrelated random variables, 636–637
K
kaiserord function, 329
Kaiser window, 317–319
Kalman gain vector, 844
L
ladr2iir function, 740
ladrﬁlter function, 741
latc2ﬁr function, 710, 714, 736
Lattice-ladder ﬁlters, 737–743
MATLAB implementation of, 739–743
Lattice reﬂection coeﬃcients, 707–715
LDU factorization, 847–848
Learning curves, 827
Least-mean-square (LMS) algorithm, 575–578,
815, 816
adaptive direct-form FIR ﬁlters, 820–824
analysis of excess noise, 829–832
channel equalization, 834–836
convergence of, 824–828
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-10
INDEX
Least-mean-square (LMS) algorithm (continued)
echo cancellation, 836–838
MATLAB implementation of, 823–824
MATLAB scripts, 833, 835, 836, 837, 838
narrowband interference, suppression of,
838–839
properties of, 824–841
recursive least-squares (RLS) algorithms,
846–847
stability of, 824–826
system identiﬁcation (modeling), 832–834
Least-signiﬁcant bit (LSB), 250
Least-squares criterion results, 574, 771
Left-sided sequence, 106
length function, 473
Levinson-Durbin algorithm, 718–724, 746
Limit cycles, 531–532
granular, 532–537
overﬂow, 537–539
Linear convolution
block convolutions, 183–185
discrete Fourier transform (DFT), 180–186
error analysis, 182–183
sum, 39–40
Linear discrete-time systems, 37–40
Linearity property, 67, 107, 165
Linear mean-square estimation, 752–755
Linear operators, 637
Linear-phase FIR ﬁlters, 228, 230–233
frequency response functions, 298–301
impulse response, 295–297
MATLAB implementation, 301–303
properties of, 295–309
zero locations, 303–309
Linear prediction
ﬁlters, 686
relationships, 716–717
Linear prediction-error ﬁlters, properties of,
730–734
Linear predictive coding (LPC) of speech,
601–605, 807–813
MATLAB scripts, 811–813
Linear systems
bounded-input bounded-output (BIBO)
stability, 40
causality, 40
discrete-time, 37–40
impulse response, 37
linear time-invariant (LTI) system, 38–40
superposition summation, 37
time-varying impulse responses, 37
Linear time-invariant (LTI) system, 38, 52, 74–80,
658–668
bounded-input bounded-output (BIBO)
stability, 40
causality, 40
convolution, 40–47
cross-correlation, computation of, 660
cross-spectral density (CSD), computation of,
660
determination of, 39–40
diﬀerence equations, 47–53
digital ﬁlters, 52–53
discrete-time, 664–668
frequency response, 74, 76–80
linear systems, 38–40
output auto-correlation, computation of,
661–664
output mean function, computation of,
659–660
power spectral density (PSD), computation of,
661–664
sequence correlation, 45–47
zero-state input and responses, 51–52
Line enhancement, 795–801
MATLAB scripts, 797, 798, 799, 801
lms function, 577, 823
Local loops, 783
Loops
for...end, 143, 177
local, 783
Lowpass ﬁlters, 336
design using MATLAB, 427–432
process, 676–679
Low-rate signals, 461
M
Magnitude, 75, 77
Magnitude-only
designs, 371
speciﬁcations, 292
Mantissa, 251, 262
Marginal probability functions, 629
Markov process, 674
Matched-z transformation, 426–427
MATLAB, 1–2, 5–17
adaptive ﬁltering, 577–578
analog signals, 84–87, 92–97
autoregressive (AR) lattice structure, 736
bivariate Gaussian distribution, 641
Butterworth lowpass ﬁlters, 387–390, 392–394
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-11
cascade form, 230
channel equalization, 835, 836
Chebyshev lowpass ﬁlters, 396–397, 398–403
control and ﬂow of commands, 11–13
convolution, 43–45, 47
decimation, 467–470
diﬀerence equations, 48–51
digital ﬁlters, 520–525
digital signal processing (DSP) and, 1–2, 18–20
direct form structure, 216, 229
discrete Fourier series (DFS), 143–146
discrete Fourier transform (DFT), 156–165,
185–186, 195–197
discrete-time Fourier transform (DTFT), 61–66
discrete-time LTI systems, 667
discrete-time signals, 23
discrete-time systems, 43–45
distribution, 621–622
downsampling, 462–466
echo cancellation, 836, 837
elliptic lowpass ﬁlters, 404–406
equiripple ﬁlters, 351–360
estimation of the mean, 626–627
fast Fourier transform (FFT), 195–197
ﬁlter coeﬃcients, 266–269
ﬁltered noise process, 682
ﬁnite-duration impulse response (FIR) ﬁlters,
319–320
frequency-band transformations, 441–445
frequency-sampling form, 234–239
functions, 14
histograms as joint pdf approximations, 630
indexing operations, 10–11
inﬁnite-duration impulse response (IIR) ﬁlters,
217
integer decimation, 491–493
integer interpolation, 483–487
lattice-ladder ﬁlters, 739–743
lattice reﬂection coeﬃcients, 710–711
least-mean-square (LMS) algorithm, 823–824,
833
Levinson-Durbin algorithm, 722–724
linear-phase FIR ﬁlters, 301–303
linear-phase form, 230–233
lowpass ﬁlter design using, 427–432
matrix operations, 8–13
narrowband interference, suppression of, 838
number representation, 21
one’s-complement format, 243–244
operators, 6, 8
parallel form, 221
plotting, 14–17
power spectral density (PSD), 657–658
random processes, 620–621
random sequences, 649–650
rational factor I/D, 480–482
rational-factor rate conversion, 498–499
round-oﬀeﬀects, 526–527, 528–530, 533–537,
546–547, 560–566
sampling, 153
scripts, 13
sign-magnitude format, 242
sinusoidal distribution, 625
sinusoidal sequence, 34
system identiﬁcation, 774
ten’s-complement format, 247–248
two’s-complement format, 245
upsampling, 471–475
use of, 5–17
variables, 7–8
Wiener ﬁlters, 747, 748, 750, 751, 752, 761,
762, 766
z-transform, 114–118, 120–125, 131–134
Matrix operations
MATLAB, 8–13
matrix-vector multiplication, 92
Matrix-vector multiplication operation, 62
maxlag index, 649
Mean, estimation of the, 625–627
mean function, 526
Mean-squared values, 619
Mean-square-error (MSE), 574, 771
parameter estimation based on, 819–820
mean(X) function, 626
Merging formula, 193
Minimax problem, 336, 344–348
Minimum mean-square-error criterion, 816–820
Minimum MSE (MMSE), 717, 744
Minimum stopband attenuation, 313
Mirror-image symmetry, 373
Mixed-radix FFT algorithm, 193
Mixed random variables, 616
Models
Gaussian distribution, 621–622
random variables, 620–625
system identiﬁcation, 578–579, 771–776
uniform distribution, 620–621
Modems, echo cancellers, 786
Modiﬁed zero-order Bessel functions, 318
Moments, 618, 634–637
Monotone function, 348
Moving average (MA) ﬁlters, 52
Moving average (MA) process, 692
Moving target indicator (MTI) radars, 379
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-12
INDEX
Multiple-access code, 611
Multiple stopbands, 499–500
Multiplication-by-a-ramp property, 108, 111
Multiplication property, 69, 179
Multiplication quantization error, 539–542
Multipliers (gain), 213
Multirate digital signal processing systems, 458
Musical sound processing, 18
N
Narrowband interference, suppression of, 579–582,
788–795
least-mean-square (LMS) algorithm, 838–839
MATLAB scripts, 838
Narrowband process, 679
Narrowband signals in wideband noise,
estimation of, 796–801
Natural frequencies, 48
Near-end echo, 784
Noise
A/D quantization, 525
analysis of excess, 829–832
cancellation, 801–807
MATLAB scripts, 804, 805, 806
pseudo-noise (PN), 612
quantization, 519
round-oﬀ, 558, 564
signal-to-noise ratio (SNR), 545, 550–551
statistical, 519–520, 539, 542–545, 554–557
Noise-whitening ﬁlter, 689
Noncausal Wiener ﬁlters, 762–766
Nonrecursive ﬁlters, 52
Normal equations, 705, 717–730
Levinson-Durbin algorithm, 718–724
Schur algorithm, 724–730
Normalized covariance, 637
Normalized form, 251
Normalized histograms, 617
Not-a-number (NaN), 255
Notch ﬁlters, 377–379
N-point sequence, 154
Null space, 104
Numbers
representation, 21, 240–255
sequence, 10
Numerator polynomials, 105
Numerically computable transforms, 141
nx function, 617
nxy function, 630
Nyquist component, 169
Nyquist rates, 83, 87
echo cancellers, 786
O
oc2sm function, 251
One’s-complement format, 240, 242–244
ﬁxed-point arithmetic, 257–258
ﬁxed-point signed integer arithmetic, 242–244
general ﬁxed-point arithmetic, 249
MATLAB implementation, 243–244
onesComplement function, 243
One-side z-transform, 128
Operations
rounding, 255, 260–261
on sequences, 25–32
truncation, 255, 256
Operators
linear, 637
MATLAB, 6, 8
Optimum linear ﬁlters, 686
Optimum reﬂection coeﬃcients, 715–716
Order-recursive equations, 703
Orthogonality principle, Wiener ﬁlters, 752–755
Orthogonal random variables, 637
Output auto-correlation, computation of, 661–664
Output mean function, computation of, 659–660
Output sequences, 79
Overﬂow, 531
limit cycles, 537–539
scaling to avoid, 558
scaling to prevent, 564
Overlap-add method, block convolutions, 186
ovrlpsav function, 185, 186, 199
P
Pair of random variables, 628–641
Pairs, z-transform, 110–111
par2dir function, 224
Parallel form, 214, 221–228
inﬁnite-duration impulse response (IIR) ﬁlters,
214
Parameters
estimation based on MSE criterion, 819–820
signals, 487
parﬁltr function, 223
Parks-McClellan algorithm, 350–351
Parseval’s relation, 179–180
Passband, 292
pdf1 function, 617
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-13
Pdf approximations, histograms as, 616–618
Peak side-lobe magnitude, 312
Peaky main-lobe, 311
Periodic conjugate symmetry, 168
Periodic convolution, 69
Periodicity in frequencies, 33
Periodicity property, 61
Periodic sequence, 25
Periodic shift of sequences, 172
Phase response, 75, 77
Phase responses of prototype ﬁlters, 406–407
Piecewise linear response, 298
Plants, 771, 776
Plotting, 14–17
Poles, estimation of in all-pole systems, 811–813
Pole-zero systems, 738
inﬁnite-duration impulse response (IIR) ﬁlters,
262–269
Poly function, 116
Polyphase structures, 500, 504–507
Positive-time sequence, 105
Power, signal, 28
Power spectral density (PSD), 650–658
auto-correlation, 653–654
computation of, 661–664
cross-spectral density (CSD), 654
MATLAB implementation of, 657–658
properties of, 653–654
random sequences, 655–658
Power spectrum, 180
Prediction
backward linear, 706–707
forward linear, 701–705
linear prediction relationships, 716–717
signals, 744
Wiener ﬁlters, 743–766
Prediction-error ﬁlter, 701
Prewindowing, 842
Probability functions, 615–618
Processes, random, 614–685
Processing elements (PEs), 729
prod function, 28
Products
of biquads, 214
sample, 28
Properties
discrete-time Fourier transform (DTFT), 60–61
of discrete-time Fourier transform (DTFT),
67–73
Gaussian distribution, 639
of least-mean-square (LMS) algorithm, 824–841
of linear-phase FIR ﬁlters, 295–309
of linear prediction-error ﬁlters, 730–734
of power spectral density (PSD), 653–654
Prototype analog ﬁlters, 385–407
Butterworth lowpass ﬁlters, 385–390
Chebyshev lowpass ﬁlters, 394–403
design equations, 390–391
elliptic lowpass ﬁlters, 403–406
phase responses of, 406–407
Prototype ﬁlters, 432
Pseudo-noise (PN), 612
p-stage lattice ﬁlter, 704
Pulse-code modulation (PCM), 586–590, 807
Q
qcoeﬀfunction, 266
qﬁx function, 546
qmode function, 534
Quadrature components, 681
Quantization
A/D quantization noise, 518–530
discrete-time ﬁlters, 255–262
ﬁxed-point arithmetic, 256–261
ﬂoating-point arithmetic, 261–262
multiplication quantization error, 539–542
noise, 519
R
Radix-2 FFT algorithm, 193–196
Radix-R FFT algorithm, 193
rand function, 25, 578
randn function, 675
Random processes, 642–650
autoregressive moving average (ARMA) process,
692–695
autoregressive (AR) process, 692
backward linear prediction, 706–707
bivariate Gaussian distribution, 637–641
cross-correlation, computation of, 660
cross-spectral density (CSD), computation of,
660
discrete-time LTI systems, 664–668
discrete-time Markov process, 674–675
discrete-time white noise process, 672–674
estimation of the mean, 625–627
ﬁltered noise process, 676–684
FIR ﬁlter coeﬃcients, 707–715
forward linear prediction, 701–705
Gaussian distribution, 668–670
innovations representation of, 687–700
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-14
INDEX
Random processes (continued)
joint probability functions, 628–634
joint statistical averages, 634–637
lattice reﬂection coeﬃcients, 707–715
Levinson-Durbin algorithm, 718–724
linear and time-invariant systems (LTI),
658–668
linear prediction relationships, 716–717
Markov process, 674
MATLAB implementation of, 620–621
models, 620–625
moving average (MA) process, 692
optimum reﬂection coeﬃcients, 715–716
output auto-correlation, computation of,
661–664
output mean function, computation, 659–660
pair of random variables, 628–641
power spectral density (PSD), 650–658, 661–664
rational power spectra, 691–695
relationship with autocorrelation sequence,
695–700
Schur algorithm, 724–730
sinusoidal distribution, 623–625
statistical averages, 618–619
transformation, 622–623
variables, 615–627
white noise process, 670–672
Random sequences, 25, 648–650
power spectral density (PSD), 655–658
Random signals. See Random processes
Rational factor I/D, 477–482
MATLAB implementation, 480–482
Rational-factor rate conversion, 497–499
Rational power spectra, 691–695
Real-valued exponential sequences, 24
Reconstruction
of analog signals, 87–92
of band-limited signals, 89
signals, 94, 95
Reconstruction formula (z-transform), 152
Rectangular pulse, 35
Rectangular window, 150, 310, 312–314
Recursive least-squares (RLS) algorithms, 841–848
LDU factorization and, 847–848
least-mean-square (LMS) algorithm, 846–847
Reﬂection coeﬃcient, 702
Region of convergence (ROC), 104–107
Relationships
autocorrelation sequence, 695–700
average linear, 635, 636
linear prediction, 716–717
between representations, 125
Relative amplitudes, 313
Relative-linear scale, 371–372
Relative speciﬁcations, 292
ﬁnite-duration impulse response (FIR) ﬁlters,
293–295
rem function, 154
Representation
diﬀerence equation, 119
relationships between, 125
transfer function, 120
in z-transform, 118–128
Representation of numbers, 240–255
ﬁxed-point signed integer arithmetic, 241–248
ﬂoating-point arithmetic, 251–254
general ﬁxed-point arithmetic, 248–251
resample function, 480, 481, 500
residue function, 529
residuez function, 114, 115, 224, 410
Responses
arbitrary absolutely summable sequences, 75–76
discrete-time systems, 51–52
frequency, 74, 76–80
impulse, 37
phase, 75, 77
steady-state, 75, 128
time-varying impulse, 37
transient, 128
unbounded, 130
zero-state input and, 51–52
Reverberation
digital signal processing (DSP), 18, 20
Reverse polynomials, 707
Right-sided sequence, 106
Root-mean-squared (rms), 619
round function, 551
Rounding operation, 255, 520, 525
ﬁxed-point arithmetic, 260–261
Round-oﬀeﬀects, 518–572
A/D quantization noise, 518–530, 525, 527–530
ﬁnite-duration impulse response (FIR) ﬁlters,
557–569
ﬁrst-order ﬁlters, 542–545, 555–557
ﬁxed-point arithmetic, 557–560
ﬂoating-point arithmetic, 566–569
granular limit cycles, 532–537
higher-order ﬁlters, 553–554
in IIR digital ﬁlters, 530–557
limit cycles, 531–532, 537–539
MATLAB analysis of, 520–525, 533–537,
546–547, 551–553, 560–566
MATLAB implementation of, 526–527, 528–530
multiplication quantization error, 539–542
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-15
overﬂow, 531
rounding operation, 520
saturation strategies, 531
second-order ﬁlters, 547–549, 557
signal-to-noise ratio (SNR), 545, 550–551
statistical noise, 519–520, 539, 542–545, 554–557
truncation operation, 525–527
variance gain, 528
Round-oﬀnoise, 558, 564
Row vectors, 119
Rules, Bayes, 630
rxy function, 649
S
Sample products, 28
Sample shifting property, 107
Sampling, 141
analog signals, 81–84
discrete Fourier transform (DFT), 149–152
DTFT interpolation formula, 152–153
frequencies, 150
intervals, 81
MATLAB implementation of, 153
principles, 83
z-transform reconstruction formula, 152
Sampling rate conversion, 458–517
decimation by a factor D, 461–470
direct form ﬁlter structures, 501–504
downsampling, 461–466
ﬁnite-duration impulse response (FIR) ﬁlters,
482–510
ideal decimators, 466–470
integer decimation, 490–496
interpolation by factor I, 470–475
interpolation errors, 459–461
low-rate signals, 461
MATLAB implementation, 462–466, 467–470,
471–475, 480–482
multiple stopbands, 499–500
multirate digital signal processing systems, 458
polyphase structures, 500, 504–507
by rational factor I/D, 477–482
time-variant ﬁlter structures, 507–510
upsampling, 470–475
Saturation strategies, 531
satur function, 551
Scaling, 27
to avoid overﬂow, 558
factor, 249
to prevent overﬂow, 564
Scatter-plot as pdf plots, 631
Schur algorithm, 724–730
sdir2cas function, 388
Second-order ﬁlters, 547–549, 557
Sequences, 22–33, 45–47
absolutely summable, 75
anticausal, 106
arbitrary, 75–76
circular shift of, 171
complex exponential, 74–75
complex-valued exponential, 24
correlation, 36, 45–47
discrete-time Fourier transform (DTFT), 66–67
exponential, 24, 51
ﬁnite-duration, 23, 43–44, 107, 142
inﬁnite-duration, 23
input, 79
left-sided, 106
number, 10, 22
operations on, 25–32
output, 79
periodic, 25
positive-time, 105
random, 25, 648–650, 655–658
real-valued exponential, 24
right-sided, 106
sinusoidal, 24–25, 32–33, 75
symmetry property, 168
unit sample, 23
unit step, 23–24
z-transform, 110–111
Series, geometric, 36, 41
sigadd function, 26
sig* functions, 31
Signals, 2–20
addition, 8
analog, 80–97
analog processing (ASP), 3–5
analysis, 2, 4
band-limited, 82
cross-correlation functions, 644
cubic spine interpolation, 92
dense spectrum of, 159
digital processing (DSP), 2–20
discrete-time, 22–36
dual-tone multifrequency (DTMF), 605–609
energy, 28
ergodicity, 646–648
ﬁltering, 5
ﬁrst-order-hold (FOH) interpolation, 91–92, 94
low-rate, 461
MATLAB implementation, 84–87, 92–97
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-16
INDEX
Signals (continued)
parameters, 487
power, 28
prediction, 744
random, 642–650. See also Random processes
random sequences, 648–650
reconstruction, 94, 95
reconstruction of, 87–92
sample products, 28
sampling, 81–84
scaling, 27
smoothing, 744
speech, linear predictive coding of, 807–813
spread-spectrum, 788
stationarity, 645–646
wideband, 788–795
zero-order-hold (ZOH) interpolation, 90–91, 94
Signal-to-noise ratio (SNR), 545, 550–551
Signiﬁcand, 254
Sign-magnitude format, 240, 241–242
ﬁxed-point arithmetic, 256–257
ﬁxed-point signed integer arithmetic, 241–242
general ﬁxed-point arithmetic, 249
MATLAB implementation, 242
sinc(x) function, 92
Sinusoidal distribution, 623–625
Sinusoidal interference, suppression of, 581–582
Sinusoidal sequence, 24–25, 32–33
correlation of sequences, 36
discrete-time, 32–33
discrete-time Fourier transform (DTFT), 75
even and odd synthesis, 34, 35
geometric series, 36
MATLAB implementation of, 34
unit sample synthesis, 34
sm2oc function, 251
sm2tc function, 251
Smoothing signals, 744
Spectrum analyzers, 291
Speech signals, linear predictive coding of,
807–813
spline function, 95, 97
Spread-spectrum
communications, 611–613
signals, 788
sqrtm function, 640
Square root algorithms, 847–848
Stability
bounded-input bounded-output (BIBO), 40
of least-mean-square (LMS) algorithm, 824–828
linear systems, 40
z-transform, 125–128
Stable ﬁlters, 373
stairs function, 94
Standard deviations, 619
Stationarity, 645–646
strict-sense, 645
wide-sense, 645–646
Stationary random processes
autoregressive moving average (ARMA) process,
692–695
autoregressive (AR) process, 692
backward linear prediction, 706–707
FIR ﬁlter coeﬃcients, 707–715
forward linear prediction, 701–705
innovations representation of, 687–700
lattice reﬂection coeﬃcients, 707–715
Levinson-Durbin algorithm, 718–724
linear and time-invariant systems (LTI), 658–668
linear prediction relationships, 716–717
moving average (MA) process, 692
optimum reﬂection coeﬃcients, 715–716
rational power spectra, 691–695
relationship with autocorrelation sequence,
695–700
Schur algorithm, 724–730
Statistical averages, 618–619
Statistical independence, 630
Statistical noise, 519–520, 539, 542–545, 554–557
statModelR function, 526
std function, 526
Steady-state responses, 75, 128
stem function, 48
Stepband, 293
Stochastic-gradient-descent algorithm, 822
Strict-sense stationarity, 645
Superposition summation, 37
Symbol-rate echo cancellers, 785
Symmetric impulse response, 230
Symmetry property, 66, 168
discrete-time Fourier transform (DTFT), 61
Synthesis, discrete-time signals, 34–36
System function, 118
from the diﬀerence equation representation, 119
System identiﬁcation (modeling), 578–579, 771–776
least-mean-square (LMS) algorithm, 832–834
MATLAB scripts, 832–834
System poles, 130
T
Tables, 370
tc2sm function, 251
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

INDEX
I-17
Telephone
channels, data transmission over, 783–788
Ten’s-complement format, 246–248
ﬁxed-point signed integer arithmetic, 246–248
MATLAB implementation, 247–248
tensComplement function, 247
tf2sos function, 218
Time
periodicity in, 33
shifting property, 68
Time-domain approach, 176
Time-update equation, 843
Time-variant ﬁlter structures, 507–510
Time-varying impulse responses, 37
Transfer function, representation, 120
Transformation
joint probability functions, 631
random processes, 622–623
Transient response, 128
Transition band, 293
Transposed structure, 216
Truly linear response, 299
Truncation operation, 255, 525–527
ﬁxed-point arithmetic, 256
Trunk lines, 783
Twiddle factor, 191
Two’s-complement format, 240, 244–245
ﬁxed-point arithmetic, 259–260
ﬁxed-point signed integer arithmetic, 244–245
general ﬁxed-point arithmetic, 250–251
MATLAB implementation, 245
twosComplement function, 245
Type-1 linear-phase FIR ﬁlter, 299
Type-2 linear-phase FIR ﬁlter, 300
Type-3 linear-phase FIR ﬁlter, 300
Type-4 linear-phase FIR ﬁlter, 301
Types of ﬁlters, 374–384
allpass ﬁlters, 381–382
comb ﬁlters, 379–380
digital resonators, 374–377
digital sinusoidal oscillators, 382–384
notch ﬁlters, 377–379
U
u buttap function, 392
u chb1ap function, 396, 398
u chb2ap function, 401
u elipap function, 405
Unbounded responses, 130
Uncorrelated random variables, 636–637.
See also Variables
Uncountably inﬁnite frequencies, 141
Uniform distribution, 620–621
Unit circle, 104
Unit sample sequence, 23
Unit sample synthesis, 34–36
Unit step sequence, 23–24
Unquantized coeﬃcients, 271
upﬁrdn function, 485, 491, 498, 500
upsample function, 471
Upsampling, 470–475
MATLAB implementation, 471–475
V
varGain function, 529
Variables
bivariate Gaussian distribution, 637–641
conditional probability functions, 629–630
correlation coeﬃcients, 637
discrete random variables, 616
estimation of the mean, 625–627
histograms as joint pdf approximations, 630
histograms as pdf approximations,
616–618
joint probability functions, 628–634
joint statistical averages, 634–637
marginal probability functions, 629
MATLAB, 7–8
models, 620–625
orthogonal random, 637
pair of random, 628–641
probability functions, 615–618
random, 615–627
scatter-plot as pdf plots, 631
sinusoidal distribution, 623–625
statistical averages, 618–619
statistical independence, 630
transformation, 622–623
transformations, 631
Variance gain, 528
W
Weiner-Hopf equation, 745
Whitening property, 732–733
White noise process, 670–672
Wideband noise, narrowband signals in, 796–801
Wideband signals, 788–795
Wide-sense stationarity, 645–646
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.

I-18
INDEX
Wiener ﬁlters, 743–766
ﬁnite-duration
impulse response (FIR) ﬁlters, 745–752
inﬁnite-duration
impulse response (IIR) ﬁlters, 755–762
linear mean-square estimation, 752–755
MATLAB
scripts, 747, 748, 750, 751, 752, 761, 762, 766
noncausal, 762–766
orthogonality principle, 752–755
Wiener-Khinchin theorem, 653
Windowing, 309–330
Bartlett window, 314–315
basic designs, 311
Blackman window, 317
cutoﬀfrequency, 310
design examples, 320–330
Hamming window, 315–316
Hann window, 315
Kaiser window, 317–319
MATLAB implementation of, 319–320
peaky main-lobe, 311
rectangular window, 310, 312–314
Word length, 248
World representation, 689
X
xcorr function, 47, 650
xic function, 131
x=rand function, 620
x=randn function, 622
Y
y = Qﬁx function, 534
Z
z array, 388
Z-domain stability theorem, 125
Zero constellation, 304
Zeroes
interpolation errors, 459
linear-phase FIR ﬁlters, 303–309
pole-zero locations, 262–269
pole-zero systems, 738
Zero-order-hold (ZOH) interpolation, 90–91, 94
Zero-padding operation, 159
zerophase function, 301, 303
Zero-state input and responses, 51–52
zmapping function, 435, 437
zplane function, 119, 121, 122
z-transform, 103–140
bilateral, 103–107
causality, 125–128
deconvolution, 109
diﬀerence equation representation, 119
discrete Fourier series (DFS) relation to, 146–147
ﬁnite-duration causal sequence, 121
inversion of, 112–118
MATLAB
implementation of, 114–118, 120–125, 131–134
one-side z-transform, 128
pairs, 110–111
properties of, 107–111
reconstruction formula, 152
region of convergence (ROC), 104–107
relationships between representations, 125
sequences, 110–111
solutions of diﬀerence equations, 128–134
stability, 125–128
systematic representation in, 118–128
system function, 118
transfer function representation, 120
z-domain stability theorem, 125
Copyright 2017 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Due to electronic rights, some third party content may be suppressed from the eBook and/or eChapter(s).
Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. Cengage Learning reserves the right to remove additional content at any time if subsequent rights restrictions require it.


Ying Tan
Fireworks 
Algorithm
A Novel Swarm Intelligence 
Optimization Method
www.allitebooks.com

Fireworks Algorithm
www.allitebooks.com

Ying Tan
Fireworks Algorithm
A Novel Swarm Intelligence
Optimization Method
123
www.allitebooks.com

Ying Tan
Peking University
Beijing
China
ISBN 978-3-662-46352-9
ISBN 978-3-662-46353-6
(eBook)
DOI 10.1007/978-3-662-46353-6
Library of Congress Control Number: 2015947937
Springer Heidelberg New York Dordrecht London
Parts of the edition are translations from the Chinese language edition: 烟花算法引论by Ying Tan
© Science Press 2015. All rights reserved
© Springer-Verlag Berlin Heidelberg 2015
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made.
Printed on acid-free paper
Springer-Verlag GmbH Berlin Heidelberg is part of Springer Science+Business Media
(www.springer.com)
www.allitebooks.com

Dedicated to my wife Chao Deng
and my daughter Fei Tan for their
unconditional love and support!
www.allitebooks.com

Preface
The swarm intelligence optimization method is used to study bio- or non-bio-
inspired and population-based iterative algorithms for seeking the intrinsic coop-
erative mechanism in a swarm. It has recently attracted a great deal of attention from
researchers from different ﬁelds and diverse domains. Many novel algorithms and
their efﬁcient improvements have been proposed continuously. The swarm intelli-
gence optimization method is increasingly becoming one of the hottest and most
important paradigms under the big umbrella of evolutionary computation (EC).
Inspired by the ﬁreworks explosion in the night sky, ﬁreworks algorithm,
abbreviated as FWA, was proposed by the author in 2010. FWA is a swarm
intelligence optimization algorithm, which seems effective at ﬁnding a good enough
solution to the global optimum of a complex optimization problem. In FWA, as a
ﬁrework explodes, a shower of sparks is shown in the adjacent area. These sparks
explode again and generate other showers of sparks in a smaller area. Gradually, the
sparks search the whole solution space in a ﬁne structure and focus on a small
region to eventually ﬁnd (a) good enough solution(s).
To my memory, on the night of the Eve of the 2006 Chinese Lunar Year, as the
municipality authorities of Beijing lifted the ban on ﬁreworks during that Spring
Festival, people in Beijing set off a large amount of ﬁreworks with sparks of diverse
colors which lighted up the dark sky in a variety of beautiful patterns.
While I stared at the glorious scene and colorful patterns for a long time, suddenly
an idea came to my mind that the way ﬁreworks explode may be an efﬁcient and
effective way or strategy to search for a potential good solution in a vast solution
space. Such a search strategy would be different from the established ones in the EC
community. Since then I began to study this explosion-like search method.
Like other practical optimization algorithms, FWA is able to fulﬁll three user
requirements given by Storn and Price in 1997. First of all, FWA can process linear,
nonlinear, and multi-model test functions. Second, FWA can be parallelized for
tackling complicated real-world problems. Third, FWA has good convergence
properties and can always ﬁnd good enough solution(s) for a global minimization
problem.
vii
www.allitebooks.com

As mentioned above, the motivation for studying FWA is to seek an efﬁcient and
effective method with a novel searching mechanism for addressing a variety of
complex optimization problems, especially multi-modal optimization problems.
There are three reasons that drove me to publish such a monograph based on our
latest research work. The ﬁrst is that the FWA, with characteristics of explosive
ability, randomness, implicit parallelism, instantaneity, and diversity, is a new
explosive searching manner to ﬁnd the global optimum of a complex problem,
which considerably enriches and promotes the study of swarm intelligence. The
second reason is that the FWA and its variants show their stable convergence and
superior performance compared to the standard particle swarm optimization (SPSO)
and the latest version of SPSO (SPSO2011) in terms of extensive experiments on
the 28 benchmark functions at IEEE CEC2013. The third reason is that the FWA is
pretty suitable for multiple-modal optimization functions which will ﬁnd a wide
range of applications in the real world. In this regard, a number of practical
applications (including, NMF solving, Spam detection and ﬁltering, JSP, to name a
few) based on the FWAs are listed in application chapters of the monograph.
Therefore, the FWA provides a brand new way to search the global optimum of
complex optimization problems. The current FWA and its applications prove that it
can be used to solve many complex optimization problems effectively. Furthermore,
the FWA can be also parallelized and is thus suitable to deal with big data prob-
lems. Whether for theoretical or applied research, the FWA is worth researching on
and can bring great scientiﬁc and economic beneﬁts.
This book is the ﬁrst monograph focused on FWA based on a number of aca-
demic papers published primarily by the author and his guided students and team
members, and is intended to systematically and completely summarize the most
important research work on FWA till date, including FWA’s basic principle and
implementation,
its modeling
and theoretical analysis,
its most
important
improvements, and several successful applications.
I hope this book would shape the research on FWA appropriately and can show a
complete picture of FWA for interested readers and newcomers who may ﬁnd many
algorithms in the book that can be directly used in their projects on hand without
any modiﬁcation; furthermore, some algorithms can also be viewed as a start point
for some active researchers to work on.
This book is primarily intended for researchers, engineers, graduates, and senior
undergraduates with interests in novel swarm intelligence algorithms such as ﬁre-
works algorithms and its applications. The structure of the contents of this book is
organized in a manner from bottom to top or from simple to complex. In order to
understand the contents of this book, the readers must have the fundamental skills
of digital iterative computing, artiﬁcial intelligence, computational intelligence as
well as pattern recognition.
In addition, the author presents many newly proposed FWAs in didactic
approach with detailed materials and shows their excellent performance using a
number of complete experiments and comparisons with state-of-the-art swarm-
based algorithms. Furthermore, a collection of resources, source codes, and refer-
ences of FWA is also provided in the book or accompanied with some webpages
viii
Preface
www.allitebooks.com

that are available and ready for readers to download freely in an easy-to-use manner
at http://www.cil.pku.edu.cn/research/fwa/resource.html.
Speciﬁcally, this monograph is organized into four parts for easy reference.
Part I is the fundamental and basic theory, which is consisted of three chapters,
forming the most substantial theoretical analysis part of this book, including
introduction, basic principle, theories and implementation of FWA, modeling and
theoretical analysis of FWA, as well as studying the effects of different types of
random number generators on the performance of FWA.
Part II is FWA variants, which groups together the most important FWA variants
so far, consisting of seven chapters each of which describes one kind of recent
signiﬁcant improvements in FWA. Since the invention of the Fireworks Algorithm
(FWA) in 2010, it has attracted a lot of attention in the community of swarm
intelligence (SI). All the improvement work on FWA can be classiﬁed into two
aspects, one, based on working on the limitations of FWA, and the other, its
hybridization with other algorithms.
In Part III on advanced topics on FWA, four chapters are included to present
some advanced topics in the research on FWA recently, including FWA for
multi-objective optimization (MOO), discrete FWA (DFWA) for combinatorial
optimization, and GPU-based FWA for parallel implementation of FWA.
Part IV shows how ﬁreworks algorithms can be applied to various applications
in different areas. These applications include traditional pattern recognition prob-
lems (nonnegative matrix factorization, document clustering, spam detection, and
image recognition), complex model estimation problem (seismic inversion), and
emerging swarm robotics searching problem. These applications sit in areas that
differ greatly from each other and have different requirements for optimization
algorithms. The ﬁreworks algorithm can solve these problems successfully, which
illustrates that the algorithm can be adapted to different requirements in real-life
applications. These applications are reported for instances that might bring insights
into more and more real-world applications of FWA in the future.
Due to my limited specialty knowledge and capability, a few errors, typos,
inappropriateness, and inadequacy are present in the book, for which critical
reviews, constructive comments, and valuable suggestions from interested readers
and active researchers are warmly welcome. All comments and suggestions should
be sent to ytan(AT)pku.edu.cn for justifying whether or not they will be adopted in
a future revision according to their validations and correctness. Finally, I would
here like to deliver my faithful and heartfelt thanks to all who gave and will give me
help in improving the quality of this book in advance.
Beijing, China
Ying Tan
October 2014
Preface
ix
www.allitebooks.com

Acknowledgments
I would also like to thank my past and present students who gave me strong
assistance in research for such an innovative idea and amazing work.
I am grateful to my students who took part in the research work on FWA under
my guidance at Computational Intelligence Laboratory at Peking University
(CIL@PKU) (http://www.cil.pku.edu.cn). Almost the whole content of this book is
from the research works achieved by, or academic papers published by myself and
my supervised postdoctoral fellows, PhD and Master’s students, who are Dr.
Yuanchun Zhu, Dr. Andreas Janecek, Dr. Jianhua Liu, Dr. Zhongyang Zheng, Mr.
Zhongmin Xiao, Mr. You Zhou, Miss Wenrui He; my PhD candidates who include
Shaoqiu Zheng, Ke Ding, Chao Yu, Junzhi Li, Guyue Mi and Weiwei Hu; as well
as my master’s graduates Xiang Yang, Lang Liu, Yang Gao, and Xiaolin Zhang.
I would like to deliver my special thanks to all of them. Without their hard work
and unremitting efforts, it would be impossible for FWA research to be consider-
ably focused with the current prospects of today.
In addition, in the process of preparing the manuscript for this volume, the
following persons gave me efﬁcient help and support: Shaoqiu Zheng, Chao Yu, Ke
Ding, Zhongyang Zheng, Junzhi Li, Weidi Xu, Lang Liu, Xiaolin Zhang, Guyue
Mi, and Weiwei Hu. It is due to all their cooperative efforts and hard work that the
FWA research works scattered in a variety of academic journal papers and inter-
national conference papers were well organized in such a tight form. I would like to
give my special thanks to them.
I am graceful to Dr. Pei Yan for his outstanding work on accelerating FWA by
approximating ﬁtness landscape, as my visiting student from Kyushu University, I
here want to thank his supervisor Prof. Hideyuki Takagi from Kyushu University
for his helpful suggestions.
Next, I also want to thank Prof. Yujun Zheng and his team from Zhenjiang
University of Science and Technology for unselﬁshly providing and granting me
use of their latest research works on FWA required for the completion of this book.
xi
www.allitebooks.com

My special thanks go to Prof. Xingui He. I am deeply indebted to him for his
encouragement and support over the years and also for this book. His comments on
the general organization of the book greatly improved it in both content and form.
Thanks to Prof. Yuhui Shi, my good friend, for his efﬁcient help and effort in
advertising the FWA as well as for his invaluable feedback provided after reading
parts of the book.
I owe my gratitude to all colleagues and students who have collaborated directly
or indirectly in collaboration with research on FWA and in writing this monograph.
While working on this book, I was supported by the Natural Science Foundation
of China (NSFC) under grant no. 61375119, 61170057, 60875080, 60673020,
61110306107 and 61010306001. This work was also partially supported by the
National Key Basic Research Development Plan (973) Project of China with grant
no. 2015CB352302. I thank the Natural Science Foundation of China (NSFC) and
the Ministry of Science and Technology of China greatly for its generous ﬁnancial
support.
I want to thank Dr. Chang, Celine, a senior editor from Springer publisher, for
her kind coordination and help in reviewing the manuscript of this book. I am also
grateful to Miss Jane Li, an editorial assistant of Springer publisher, for her
assistance in editing the manuscript and formatting this book. I also want to thank
Springer International publishing for their commitment to publish and stimulate
innovative ideas and for giving me the opportunity to publish this book so quickly.
Anyway, I want to thank honestly everyone who give me help and support in
any form at any time.
This book is dedicated to my wife Dr. Chao Deng and to my daughter Fei Tan,
for their unconditional love; without their unselﬁsh and uninterrupted salient sup-
port and encouragement, it would be impossible to make this book a reality.
xii
Acknowledgments
www.allitebooks.com

Contents
Part I
Fundamentals and Basic Theory
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Brief Introduction to Swarm Intelligence . . . . . . . . . . . . . . . .
4
1.3
Brief Introduction to FWA. . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.4
Characteristics and Advantages of FWA . . . . . . . . . . . . . . . .
7
1.5
Overviews of FWA Research . . . . . . . . . . . . . . . . . . . . . . . .
7
1.6
Overview of the Book. . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2
Fireworks Algorithm (FWA) . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
FWA Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.1
Explosion Operator . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.2
Gaussian Mutation Operator. . . . . . . . . . . . . . . . . .
19
2.2.3
Mapping Rule . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.2.4
Selection Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3
Implementation of FWA . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.3.1
Explosion Operator . . . . . . . . . . . . . . . . . . . . . . . .
21
2.3.2
Mutation Operator. . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3.3
Mapping Rule . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.3.4
Selection Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.4
The Characteristics of FWA . . . . . . . . . . . . . . . . . . . . . . . .
25
2.4.1
Explosion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.4.2
Instantaneity. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.4.3
Simplicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4.4
Locality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4.5
Emergent Property . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4.6
Distributed Parallelism. . . . . . . . . . . . . . . . . . . . . .
26
2.4.7
Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
xiii

2.4.8
Extendibility . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.4.9
Adaptability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.5
Impact of Operators in FWA on Performance. . . . . . . . . . . .
28
2.5.1
Explosion Operator . . . . . . . . . . . . . . . . . . . . . . . .
28
2.5.2
Gaussian Mutation . . . . . . . . . . . . . . . . . . . . . . . .
29
2.5.3
Mapping Rule . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.5.4
Selection Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.6
Comparison of FWA with Three Other SI Algorithms . . . . . .
31
2.6.1
Ideas Comparison Between FWA and GA . . . . . . . .
31
2.6.2
Ideas Comparison Between FWA and Two
Versions of PSO. . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.7
Experimental Results and Analysis . . . . . . . . . . . . . . . . . . .
33
2.7.1
Benchmark Functions . . . . . . . . . . . . . . . . . . . . . .
33
2.7.2
Parameters Setting . . . . . . . . . . . . . . . . . . . . . . . .
33
2.7.3
Experimental Results. . . . . . . . . . . . . . . . . . . . . . .
33
2.7.4
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3
Modeling and Theoretical Analysis of FWA. . . . . . . . . . . . . . . .
37
3.1
A Stochastic Process Model for FWA . . . . . . . . . . . . . . . . .
37
3.2
Global Convergence Theorems . . . . . . . . . . . . . . . . . . . . . .
39
3.3
Time Complexity of FWA . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.3.1
Basic Theory of Time Complexity . . . . . . . . . . . . .
41
3.4
Deep Analysis of Time Complexity. . . . . . . . . . . . . . . . . . .
45
3.5
Influence of Random Number Generators on FWA . . . . . . . .
48
3.5.1
Random Number Generators . . . . . . . . . . . . . . . . .
49
3.5.2
Modular Arithmetic Based RNGs . . . . . . . . . . . . . .
49
3.5.3
Binary Arithmetic Based RNGs . . . . . . . . . . . . . . .
51
3.5.4
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . .
52
3.5.5
Experimental Results and Analysis . . . . . . . . . . . . .
54
3.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
Part II
FWA Variants
4
FWA Based on Function Approximation Approaches. . . . . . . . .
61
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.2
Fireworks Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.3
Fireworks Algorithm Acceleration by Elite Strategy . . . . . . .
62
4.3.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4.3.2
Sampling Methods . . . . . . . . . . . . . . . . . . . . . . . .
63
4.3.3
Fireworks Algorithm with an Elite Strategy . . . . . . .
63
xiv
Contents

4.4
Experimental Evaluations. . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.4.1
Experimental Design . . . . . . . . . . . . . . . . . . . . . . .
65
4.4.2
Experimental Results. . . . . . . . . . . . . . . . . . . . . . .
69
4.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
4.5.1
Fireworks Algorithm Acceleration Performance . . . .
70
4.5.2
Approximation Methods . . . . . . . . . . . . . . . . . . . .
70
4.5.3
Sampling Methods . . . . . . . . . . . . . . . . . . . . . . . .
70
4.5.4
Sampling Data Number . . . . . . . . . . . . . . . . . . . . .
71
4.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
5
FWA with Controlling Exploration and Exploitation . . . . . . . . .
75
5.1
Some Improvements on Operations in FWA. . . . . . . . . . . . .
75
5.1.1
The Amplitude and Number of Sparks. . . . . . . . . . .
75
5.1.2
The Mutation Improvement . . . . . . . . . . . . . . . . . .
80
5.1.3
Selection Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
80
5.2
Experiment and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . .
81
5.2.1
Experimental Design . . . . . . . . . . . . . . . . . . . . . . .
81
5.2.2
Experimental Results and Analysis . . . . . . . . . . . . .
84
5.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
6
Enhanced Fireworks Algorithm. . . . . . . . . . . . . . . . . . . . . . . . .
87
6.1
Properties of Conventional FWA . . . . . . . . . . . . . . . . . . . .
87
6.2
The Proposed EFWA . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
6.2.1
A New Minimal Explosion Amplitude
Check (MEAC) . . . . . . . . . . . . . . . . . . . . . . . . . .
88
6.2.2
A New Operator for Generating
Explosion Sparks . . . . . . . . . . . . . . . . . . . . . . . . .
89
6.2.3
A New Mapping Operator . . . . . . . . . . . . . . . . . . .
90
6.2.4
A New Operator for Generating Gaussian Sparks . . .
91
6.2.5
A New Selection Operator . . . . . . . . . . . . . . . . . . .
94
6.3
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
6.3.1
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . .
95
6.3.2
Experimental Results. . . . . . . . . . . . . . . . . . . . . . .
96
6.3.3
Experimental Observations. . . . . . . . . . . . . . . . . . .
101
6.3.4
Influences of Dimension Selection Methods
on FWA and EFWA . . . . . . . . . . . . . . . . . . . . . . .
101
6.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
Contents
xv

7
Fireworks Algorithm with Dynamic Search . . . . . . . . . . . . . . . .
103
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
7.2
Properties of Minimal Explosion Amplitude Check (MEAC)
Strategy in EFWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
7.3
The dynFWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
7.3.1
Dynamic Explosion Amplitude
for the First Group (CF) . . . . . . . . . . . . . . . . . . . .
106
7.3.2
Dynamic Explosion Amplitude Update for CF . . . . .
108
7.3.3
Some Theoretical Analysis and Considerations . . . . .
108
7.3.4
Explosion Amplitude for the Second
Group (Non-CF). . . . . . . . . . . . . . . . . . . . . . . . . .
110
7.3.5
Elimination of the Gaussian Sparks Operator . . . . . .
110
7.3.6
Framework of dynFWA. . . . . . . . . . . . . . . . . . . . .
111
7.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
7.4.1
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . .
112
7.4.2
Experimental Results. . . . . . . . . . . . . . . . . . . . . . .
112
7.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
8
Adaptive Fireworks Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .
119
8.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
8.2
Analysis of the Amplitude in FWA and EFWA . . . . . . . . . .
119
8.3
Adaptive Explosion Amplitude . . . . . . . . . . . . . . . . . . . . . .
120
8.3.1
Principles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
8.3.2
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
8.3.3
Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
8.4
Adaptive Fireworks Algorithm . . . . . . . . . . . . . . . . . . . . . .
127
8.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
8.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
8.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
9
Cooperative Fireworks Algorithm . . . . . . . . . . . . . . . . . . . . . . .
133
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
9.2
Probabilistically Oriented Explosion Mechanism (POEM) . . .
134
9.3
Remarks on Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
9.4
Framework of Cooperative Firework Algorithm (CoFWA) . . .
136
9.5
A Kind of Realization of POEM. . . . . . . . . . . . . . . . . . . . .
137
9.6
The Proposed CoFWA . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
9.7
Convergence Theorem of CoFWA . . . . . . . . . . . . . . . . . . .
141
9.8
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
9.9
Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
xvi
Contents

10
Hybrid Fireworks Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .
151
10.1
FWA-DM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
10.1.1
Differential Mutation Operator . . . . . . . . . . . . . . . .
152
10.1.2
Applying DM to EFWA . . . . . . . . . . . . . . . . . . . .
152
10.2
FWA-DE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
10.3
CFWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
10.3.1
Design of Digital Filter . . . . . . . . . . . . . . . . . . . . .
156
10.3.2
CFWA Implementation . . . . . . . . . . . . . . . . . . . . .
156
10.4
BBO_FWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
10.4.1
Biogeography-Based Optimization (BBO) . . . . . . . .
157
10.4.2
A Hybrid Biogeography-Based Optimization
and Fireworks Algorithm (BBO_FWA) . . . . . . . . . .
158
10.4.3
Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
10.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
Part III
Advanced Topics
11
FWA for Multiobjective Optimization . . . . . . . . . . . . . . . . . . . .
165
11.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
11.2
Problem Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
11.3
MOFWA for VFR Problem . . . . . . . . . . . . . . . . . . . . . . . .
170
11.3.1
Fitness Assignment Strategy. . . . . . . . . . . . . . . . . .
170
11.3.2
Evolutionary Strategies . . . . . . . . . . . . . . . . . . . . .
171
11.3.3
The MOFWA Framework . . . . . . . . . . . . . . . . . . .
172
11.3.4
Initial Population Generation . . . . . . . . . . . . . . . . .
174
11.3.5
Non-dominated Archive Maintenance . . . . . . . . . . .
175
11.4
Computational Experiments . . . . . . . . . . . . . . . . . . . . . . . .
175
11.4.1
Comparative Experiments on BN and CA
VRF Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
11.4.2
Case Study of a Real-World CO VRF Problem . . . .
182
11.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
11.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
12
S-Metric-Based Multi-objective Fireworks Algorithm . . . . . . . . .
189
12.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
12.2
S-Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
12.3
The Proposed S-MOFWA . . . . . . . . . . . . . . . . . . . . . . . . .
192
12.3.1
Initialization. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
12.3.2
Calculation of the Explosion Sparks Number
and Explosion Amplitude. . . . . . . . . . . . . . . . . . . .
193
12.3.3
Explosion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
Contents
xvii

12.3.4
Gaussian Mutation . . . . . . . . . . . . . . . . . . . . . . . .
194
12.3.5
Fireworks Selection and Archive Updating. . . . . . . .
194
12.3.6
Framework of S-MOFWA . . . . . . . . . . . . . . . . . . .
198
12.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
12.4.1
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . .
199
12.4.2
Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . .
200
12.4.3
Experimental Results. . . . . . . . . . . . . . . . . . . . . . .
200
12.4.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
12.5
Environmental/Economic Power Dispatch Problem . . . . . . . .
205
12.5.1
Problem Statement . . . . . . . . . . . . . . . . . . . . . . . .
205
12.5.2
Evaluation Criterion . . . . . . . . . . . . . . . . . . . . . . .
205
12.5.3
Results and Discussion . . . . . . . . . . . . . . . . . . . . .
206
12.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
13
Discrete Firework Algorithm for Combinatorial
Optimization Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
13.1
Travelling Salesman Problem . . . . . . . . . . . . . . . . . . . . . . .
209
13.1.1
Problem Statement . . . . . . . . . . . . . . . . . . . . . . . .
209
13.2
Discrete Firework Algorithm . . . . . . . . . . . . . . . . . . . . . . .
211
13.2.1
Framework of DFWA . . . . . . . . . . . . . . . . . . . . . .
211
13.2.2
Explosion Operator . . . . . . . . . . . . . . . . . . . . . . . .
213
13.2.3
Identical Mutation Operator . . . . . . . . . . . . . . . . . .
217
13.2.4
Selection Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
218
13.2.5
Adaptive Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
219
13.3
Experimental Results and Analysis . . . . . . . . . . . . . . . . . . .
219
13.3.1
Parameters Setting and Performance Analysis. . . . . .
220
13.3.2
Results on Small Test Cases. . . . . . . . . . . . . . . . . .
222
13.3.3
Results on Medium Test Case . . . . . . . . . . . . . . . .
223
13.3.4
Results on Large Test Case . . . . . . . . . . . . . . . . . .
225
13.4
Comparison with Classical Algorithms . . . . . . . . . . . . . . . .
225
13.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
14
Implementation of Fireworks Algorithm Based on GPU . . . . . . .
227
14.1
General Purpose GPU Computing. . . . . . . . . . . . . . . . . . . .
227
14.1.1
Advantages of GPU Computing . . . . . . . . . . . . . . .
228
14.1.2
Development Tools on GPU Hardware . . . . . . . . . .
229
14.1.3
Compute Unified Device Architecture (CUDA) . . . .
231
14.2
GPU–FWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
14.2.1
FWA Search . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233
14.2.2
Attract-Repulse Mutation . . . . . . . . . . . . . . . . . . . .
235
14.2.3
Implementation. . . . . . . . . . . . . . . . . . . . . . . . . . .
237
14.2.4
Empirical Analysis . . . . . . . . . . . . . . . . . . . . . . . .
239
xviii
Contents

14.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Part IV
Applications
15
FWA Application on Non-negative Matrix Factorization. . . . . . .
247
15.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
15.1.1
NMF Research History . . . . . . . . . . . . . . . . . . . . .
248
15.2
Low-Rank Approximations . . . . . . . . . . . . . . . . . . . . . . . .
249
15.2.1
Non-negative Matrix Factorization (NMF) . . . . . . . .
250
15.3
Swarm Intelligence Optimization . . . . . . . . . . . . . . . . . . . .
253
15.4
Improving NMF with Swarm Intelligence Optimization . . . . .
253
15.4.1
Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
15.4.2
Optimization Strategy 1 for Initialization . . . . . . . . .
255
15.4.3
Optimization Strategy 2 for Iterative Optimization. . .
256
15.5
Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
257
15.5.1
Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
257
15.5.2
Hardware. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
15.5.3
Parameter Setup . . . . . . . . . . . . . . . . . . . . . . . . . .
258
15.5.4
Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
15.5.5
Evaluation of Optimization Strategy 1 . . . . . . . . . . .
258
15.5.6
Evaluation of Optimization Strategy 2 . . . . . . . . . . .
260
15.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
260
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
16
FWA Applications on Clustering, Pattern Recognition,
and Inversion Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
16.1
Document Clustering. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
16.1.1
Document Features . . . . . . . . . . . . . . . . . . . . . . . .
264
16.1.2
Fireworks Algorithm for Document Clustering . . . . .
266
16.1.3
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
16.2
Spam Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
16.2.1
Problem Description . . . . . . . . . . . . . . . . . . . . . . .
269
16.2.2
Optimization Principle. . . . . . . . . . . . . . . . . . . . . .
271
16.2.3
Experimental Results. . . . . . . . . . . . . . . . . . . . . . .
272
16.3
Image Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
16.3.1
Relation Between SUM XOR and OR XOR. . . . . . .
274
16.3.2
The Proposed Unified Distance Measure Scheme . . .
275
16.3.3
Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
Contents
xix

16.4
Seismic Inversion Problem. . . . . . . . . . . . . . . . . . . . . . . . .
278
16.4.1
Problem Statement . . . . . . . . . . . . . . . . . . . . . . . .
279
16.4.2
Experiment and Analysis . . . . . . . . . . . . . . . . . . . .
281
16.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
17
Group Explosion Strategy for Multiple Targets Search
in Swarm Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
17.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
17.2
Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
17.3
Group Explosion Strategy . . . . . . . . . . . . . . . . . . . . . . . . .
289
17.3.1
Introducing Fireworks Explosion
into Swarm Robotics. . . . . . . . . . . . . . . . . . . . . . .
289
17.3.2
Framework of Group Explosion Strategy . . . . . . . . .
290
17.3.3
Searching in Groups . . . . . . . . . . . . . . . . . . . . . . .
291
17.3.4
Splitting Groups . . . . . . . . . . . . . . . . . . . . . . . . . .
292
17.3.5
History State Storage. . . . . . . . . . . . . . . . . . . . . . .
293
17.3.6
Velocity Update Equation . . . . . . . . . . . . . . . . . . .
294
17.3.7
Obstacle Avoidance . . . . . . . . . . . . . . . . . . . . . . .
294
17.4
Simulation Results and Discussions. . . . . . . . . . . . . . . . . . .
294
17.4.1
Baseline Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
295
17.4.2
Validation Experiment. . . . . . . . . . . . . . . . . . . . . .
295
17.4.3
Scalable Experiment . . . . . . . . . . . . . . . . . . . . . . .
297
17.4.4
Obstacle Avoidance Experiment . . . . . . . . . . . . . . .
297
17.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
Postscript . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
301
Appendix A: Benchmark Suites . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
303
Appendix B: Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
xx
Contents

About the Author
Dr. Ying Tan is a full professor and Ph.D. advisor at
School
of
Electronics
Engineering
and
Computer
Science
of
Peking
University,
and
director
of
Computational
Intelligence
Laboratory
at
Peking
University (CIL@PKU). He received his BEng from the
Electronic Engineering Institute, MSc from Xidian
University, and PhD from Southeast University, in
1985, 1988, and 1997, respectively. From 1997, he was
a postdoctoral fellow, then an associate professor at
University
of
Science
and
Technology
of
China
(USTC), served as director of Institute of Intelligent
Information Science, and a full professor since 2000. He
worked with the Chinese University of Hong Kong (CUHK) in 1999 and
2004–2005. He was an electee of 100 talent programs of the Chinese Academy of
Science (CAS) in 2005.
He serves as the Editor-in-Chief of International Journal of Computational
Intelligence and Pattern Recognition (IJCIPR), an Associate Editor of IEEE
Transactions on Cybernetics (Cyb), IEEE Transactions on Neural Networks and
Learning Systems (TNNLS), International Journal of Artiﬁcial Intelligence (IJAI),
International Journal of Swarm Intelligence Research (IJSIR), International Journal
of Intelligent Information Processing (IJIIP), IES Journal B, Intelligent Devices and
Systems, and Advisory Board of International Journal on Knowledge Based
Intelligent Engineering (KES), and The Editorial Board of Journal of Computer
Science and Systems Biology (JCSB), Journal of Applied Mathematics (JAM),
Applied
Mathematical
and
Computational
Sciences
(AMCOS),
Immune
Computing (ICJ), Defense Technology (DT), CAAI Transactions on Intelligent
Systems. He also served as an Editor of Springer Lecture Notes on Computer
Science (LNCS) for 15 volumes, and Guest Editor of several referred Journals,
including Information Science, Softcomputing, Neurocomputing, International
Journal of Artiﬁcial Intelligence (IJAI), International Journal of Swarm Intelligence
Research (IJSIR), BB, CJ, IEEE/ACM Transactions on Computational Biology and
xxi

Bioinformatics
(IEEE/ACM
TCBB),
etc.
He
is
a
member
of
Emergent
Technologies Technical Committee (ETTC), Computational Intelligence Society of
IEEE since 2010. He is a senior member of IEEE and ACM and a senior member
of the CIE. He is the general chair of joint conference ICSI’2015 in conjunction
with 2nd BRICS CCI’2015, and He was the general chair of the International
Conference on Swarm Intelligence (ICSI’2010-2014) and one of joint general
chairs of 1st BRICS CCI’2013, program committee co-chair of IEEE CEC’2014 at
IEEE WCCI’2014, 2013 IEEE International Conference on Intelligent Science and
Technology,
2012
International
Conference
on
Advanced
Computational
Intelligence (ICACI’2012), 2008 International Symposium on Neural Networks
(ISNN’2008) and so on.
His research interests include computational intelligence, swarm intelligence,
data mining, machine learning, pattern recognition, intelligent information pro-
cessing for information security, ﬁreworks algorithm, etc. He has published more
than 280 papers in refereed journals and conferences in these areas, and
authored/co-authored 6 books and 10+ chapters in book, and received 3 invention
patents.
For details, please visit the ofﬁcial website of Computational Intelligence
Laboratory at Peking University (CIL@PKU) at http://www.cil.pku.edu.cn.
xxii
About the Author
www.allitebooks.com

Abbreviations
ABC
Artiﬁcial bee colony
ACO
Ant colony optimization
AFWA
Adaptive ﬁreworks algorithm
ALSPG
ALS using projected gradient
AR-Mutation
Attract-repulse mutation
BBO
Biogeography-based optimization
BSO
Brain storm optimization
CCPSO
Co-evolutionary particle swarm optimization algorithm
CEC
IEEE congress on evolutionary computation
CF
Core ﬁrework
CFWA
Cultural ﬁrework algorithm
CI
Computational intelligence
CMR
Combined multiple recursive generator
CNMOIA
Constrained nonlinear multi-objective optimization immune
algorithm
CoFWA
Cooperative ﬁrework algorithm
CPSO
Clonal particle swarm optimization
CPU
Central processing unit
CUDA
Compute uniﬁed device architecture
DE
Differential evolution
DFWA
Discrete ﬁreworks algorithm
DPSO
Discrete particle swarm optimization
DTEA
Dominating-tree based multi-objective evolutionary algorithm
dynFWA
Fireworks algorithm with dynamic search
EC
Evolutionary computation
EFWA
Enhanced ﬁreworks algorithm
FIR
Finite impulse response
FSS
Fish school search
FWA
Fireworks algorithm
FWA-DE
Fireworks algorithm with differential evolution
FWA-DM
Fireworks algorithm with differential mutation
xxiii

GA
Genetic algorithm
GES
Group explosion strategy
GFSR
Generalized feedback shift register
GPGPU
General-purpose computation on GPU
GPU
Graphics processing unit
GPU-FWA
GPU Fireworks Algorithm
HPC
High performance computing
IDF
Inverse document frequency
IFWABS
Improved FWA with best ﬁtness selection and random mutation
IFWAFS
Improved FWA with the ﬁtness selection using the roulette and
random mutation
IIR
Inﬁnite impulse response
LC
Local concentration
LCG
Linear congruential generator
LLE
Locally linear embedding
MEAC
Minimal explosion amplitude check
MKL
Math kernel library
MOEA
Multi-objective evolutionary algorithms
MOEPSO
Multi-objective endocrine particle swarm optimization
MOFWA
Multi-objective FWA
MOO
Multi-objective optimization
MOP
Multi-objective optimization problem
MOPSO
Multi-objective particle swarm optimization
MRG
Multiple recursive generator
MT
Mersenne twister
MU
Multiplicative update
NFL
No free lunch
NMF
Non-negative matrix factorization
NNDSVD
Nonnegative double singular value decomposition
NSGA
Non-dominated sorting genetic algorithm
NSPSO
Non-dominated sorting particle swarm optimizer
PAES
Pareto archived evolution strategy
PCA
Principal component analysis
PDE
Pareto differential evolution algorithm
POEM
Probabilistically oriented explosion mechanism
PRNGs
Pseudorandom random number generators
PSO
Particle swarm optimization
QAP
Quadratic assignment problem
QRNGs
Quasi random number generators
RDPSO
Robotic darwinian particle swarm optimization
RNGs
Random number generators
RPSO
Robotic particle swarm optimization
SI
Swarm intelligence
SIA
Swarm intelligence algorithm
SIO
Swarm intelligence optimization
xxiv
Abbreviations

S-MOFWA
S-metric based multi-objective ﬁreworks algorithm
SPEA
Strength pareto evolutionary algorithm
SPSO
Standard particle swarm intelligence
SVD
Singular value decomposition
SVM
Support vector machine
TF
Term frequency
TRNGs
True random number generators
TSP
Traveling salesman problem
UDM
Uniﬁed distance measure
VFR
Variable-rate fertilization
Abbreviations
xxv

Symbols
f ðÞ
Optimization function f
D
Dimension
xi
Variable
xij
The jth dimension value of variable xi
xUB
The upper bound of variable x
xLB
The lower bound of variable x
xUB;k
The kth dimension upper bound value of variable x
xLB;k
The kth dimension lower bound value of variable x
evalsmax
The maximum evaluation times
maxIter
The maximum iteration number
N
Fireworks number
m
Explosion sparks number
^m
Gaussian mutation sparks number
Xi
The location of the ith ﬁrework in the ﬁreworks swarm
Xik
The kth dimension value of ﬁrework Xi
f ðXiÞ
The function ﬁtness of ﬁrework Xi
Ai
The explosion amplitude of the ith ﬁrework
^A
Coefﬁcient in the calculation of explosion amplitudes
Si
The explosion sparks number of the ith ﬁrework
Uða; bÞ
A random value generated between (a,b) with mean distribution
N ðμ; σÞ
A random value generated with Gaussian distribution-with mean μ and
variance σ
g
N ð1; 1Þ
e
N ð0; 1Þ
K
Candidates set, in general, it contains ﬁreworks, explosion sparks and
Gaussian mutation sparks
dð:; :Þ
Distance measure
xxvii

ε,
Machine smallest value
xj
The position of jth individual in candidates set
xj;k
The kth dimension value of xj
RðxiÞ
The sum distances between individual xi and the rest individuals in
candidates K
pðxiÞ
The probability that individual xi is selected as ﬁrework
s
The best spark generated by ﬁrework X
XB
The best ﬁrework
Xelite
The generated elite solution
^xb
The explosion sparks with minimal ﬁtness among all the explosion
sparks
CF
The ﬁrework with best function ﬁtness
nonCF
The ﬁreworks except for CF
XCF
The location of CF
XCF;k
The kth dimension value of XCF
ACF
The explosion amplitude of CF
Ca
Ampliﬁcation factor for explosion amplitude of CF
Cr
Reduction factor for explosion amplitude of CF
Ainit
The initial explosion amplitude
Afinal
The ﬁnal explosion amplitude
jj  jjF
F norm
jj  jj1
Inﬁnite norm
SI
Shift index
SV
Shift value
round(.)
Round operation
Et
The current evaluation times
oð:Þ
Low order
%
Modular arithmetic operation
C0
Coverage measure
A
Objective matrix
W
Reduced rank nonnegative factors matrix
H
Reduced rank nonnegative factors matrix
D
Distance matrix
wr
i
The ith row of W
hc
j
The jth column of H
ar
i
The ith row of A
ac
j
The ith column of A
L
The length of the root
pa
The probability that a solution is accepted
θ
Parameter which controls the pa
ΩS
The super volume of region S
xxviii
Symbols

FMax
Maximum ﬁtness
rt
Radius of the target
Ri
The ith robot
PiðtÞ
Robot i’s position
ViðtÞ
Robot i’s velocity
GiðtÞ
Grouping component
HiðtÞ
History component
Symbols
xxix

List of Figures
Figure 2.1
The good and bad explosions of a firework. Reprinted
from Ref. [l], with kind permission from Springer
Science+Business Media. a Good explosion. b Bad
explosion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Figure 2.2
Gaussian mutation. Reprinted from Ref. [l], with kind
permission from Springer Science+Business
Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Figure 2.3
The flowchart of FWA . . . . . . . . . . . . . . . . . . . . . . . . . .
24
Figure 2.4
The impact of different number of sparks
on the performance of FWA for Generalized
Rosenbrock function. The vertical axis represents
the accuracy of the experimental results . . . . . . . . . . . . . .
28
Figure 2.5
The impact of the number of fireworks
on the performance of FWA for Generalized
Rosenbrock function. The vertical axis represents
the accuracy of the experimental results . . . . . . . . . . . . . .
29
Figure 2.6
The operation of mapping rule. . . . . . . . . . . . . . . . . . . . .
30
Figure 2.7
Convergence curves of FWA, CPSO and SPSO.
Reprinted from Ref. [l], with kind permission from
Springer Science+Business Media . . . . . . . . . . . . . . . . . .
34
Figure 4.1
Original D dimensional space and one-dimensional
spaces obtained by reducing the dimensions of the
original one. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
Figure 5.1
The curves of transfer function with different
parameters. Reprinted from Ref. [2], with kind
permission from Springer Science+Business
Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
xxxi

Figure 5.2
The number of sparks for every firework in
modifying FWA. Reprinted from Ref. [2],
with kind permission from Springer
Science+Business Media. . . . . . . . . . . . . . . . . . . . . . . . .
78
Figure 5.3
The amplitude of explosion for every firework
in modifying FWA. Reprinted from Ref. [2], with
kind permission from Springer Science+Business
Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
Figure 5.4
Convergence curves on the benchmark functions.
Reprinted from Ref. [2], with kind permission from
Springer Science+Business Media . . . . . . . . . . . . . . . . . .
82
Figure 5.5
Convergence curves on the benchmark functions.
Reprinted from Ref. [2], with kind permission from
Springer Science+Business Media . . . . . . . . . . . . . . . . . .
83
Figure 6.1
Linearly and nonlinearly decreasing minimal
explosion amplitude. a Linear decrease.
b Nonlinear decrease . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
Figure 6.2
Generation of explosion sparks in FWA and EFWA.
a FWA. b EFWA . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
Figure 6.3
The locations of the Gaussian sparks using
the conventional FWA (Ackely function using
100 000 function evaluations). a No shift—optimum
at origin. b Shift—optimum at (−70, −55) . . . . . . . . . . . .
91
Figure 6.4
Sparks after initialization in conventional FWA. a No
shift—optimum at origin. b Shift—optimum at
(−70,−55). c No shift—optimum at origin.
d Shift—optimum at (−70,−55). . . . . . . . . . . . . . . . . . . .
93
Figure 6.5
Difference between the Gaussian sparks operator
in FWA and EFWA . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Figure 6.6
Runtime of FWA, eFWA-X and SPSO. . . . . . . . . . . . . . .
99
Figure 7.1
Illustration of the amplification/reduction of the CF’s
explosion amplitude. In a the radius of the circle with
dashed red line indicates the explosion amplitude
of the CF in iteration t, while the circle with solid
black line indicates the explosion amplitude in
iteration t + 1; the increased explosion amplitude
indicates that in this situation, a better position has
been found by the explosion sparks. In iteration t + 2
(b), the CF is able to further improve its location, and,
as a result, the explosion amplitude of the CF is
further increased. c Shows an example when the
fitness of the CF could not be improved. In this case,
the CF’s explosion amplitude is decreased in iteration
t + 3 (Color figure online) . . . . . . . . . . . . . . . . . . . . . . .
107
xxxii
List of Figures

Figure 7.2
The reduction and amplification of CF’s explosion
amplitude (using function f1 (sphere function) from
[4], cf. Sect. 7.4) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
Figure 7.3
Gaussian mutation operator in EFWA. . . . . . . . . . . . . . . .
110
Figure 8.1
Adaptive amplitude on sphere function . . . . . . . . . . . . . . .
122
Figure 8.2
An example of how adaptive amplitude
is calculated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
Figure 8.3
Left ^s and s are on the different sides of the ﬁrework;
Right ^s and s are on the same side of the ﬁrework . . . . . .
124
Figure 8.4
Stage (2): Local search . . . . . . . . . . . . . . . . . . . . . . . . . .
126
Figure 8.5
A histogram of the ratio by which the amplitude
increases at stage (2) . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
Figure 8.6
Stage (3): Refine search . . . . . . . . . . . . . . . . . . . . . . . . .
127
Figure 8.7
Mean error on 28 functions. . . . . . . . . . . . . . . . . . . . . . .
129
Figure 8.8
Time consumed by each algorithm on 28 functions . . . . . .
129
Figure 9.1
Gaussian explosive way in POEM . . . . . . . . . . . . . . . . . .
135
Figure 9.2
Ellipsoid explosive way in POEM . . . . . . . . . . . . . . . . . .
135
Figure 9.3
Sinc explosive way in POEM . . . . . . . . . . . . . . . . . . . . .
135
Figure 9.4
Schematic diagram of POEM . . . . . . . . . . . . . . . . . . . . .
137
Figure 9.5
There is no explosion orientation in FWA. . . . . . . . . . . . .
137
Figure 9.6
Sparks generated by the normal distribution with zero
mean and standard deviation b in the first dimension
and a in the next d  1 dimension . . . . . . . . . . . . . . . . . .
138
Figure 9.7
Rotating the orientation of the first dimension toward
the global best firework . . . . . . . . . . . . . . . . . . . . . . . . .
139
Figure 9.8
A shift is calculated to make the sparks closer to the
global best . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
Figure 9.9
Schematic diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
Figure 9.10
Convergence curves of CoFWA on 28 Benchmark
functions (f1–f8) of IEEE CEC 2013 averaged over
30 simulation runs. a f1, b f2, c f3, d f4, e f5, f f6,
g f 7, h f8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
Figure 9.11
Convergence curves of CoFWA on 28 Benchmark
functions (f21–f28) of IEEE CEC 2013 averaged over
30 simulation runs. a f21, b f22, c f23, d f24, e f25,
f f26, g f27, h f28 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
Figure 10.1
The difference between GM and DM . . . . . . . . . . . . . . . .
152
Figure 10.2
The process of applying DM to enhanced FWA. . . . . . . . .
153
Figure 11.1
The basic flowchart of the MOFWA . . . . . . . . . . . . . . . .
173
Figure 11.2
CPU time (s) variation with problem size m on the
BN VFR problem: a 300  m  800, b m  200. . . . . . . . .
179
List of Figures
xxxiii

Figure 11.3
CPU time (s) variation with problem size m
on the CA VFR problem: a 300  m  800,
b m  200. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
Figure 11.4
Comparative results of the variation of HV
with algorithm iterations on six BN VRF problem
instances. The HV values are averaged over 30
simulation runs. a m ¼ 200, b m ¼ 300, c m ¼ 400,
d m ¼ 500, e m ¼ 600, f m ¼ 800. . . . . . . . . . . . . . . . . .
179
Figure 11.5
Comparative results of the variation of HV
with algorithm iterations on six CA VRF problem
instances. The HV values are averaged over 30
simulation runs. a m ¼ 200, b m ¼ 300, c m ¼ 400,
d m ¼ 500, e m ¼ 600, f m ¼ 800. . . . . . . . . . . . . . . . . .
181
Figure 12.1
S-metric. a S-metric for the set. b S-metric
for a solution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
Figure 12.2
S-metric. a Calculation of S-metric. b The increase
of neighbors’ S-metric after wiping away a solution . . . . . .
195
Figure 12.3
MOFWA evolution process on KUR. a Iteration =
10. b Iteration = 20. c Iteration = 250. d Iteration =
300 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
Figure 12.4
Plots of the final archive of returned by S-MOFWA
in the objective space. a ZDT1. b ZDT2. c ZDT3.
d SCH. e KUR. f ZDT6 . . . . . . . . . . . . . . . . . . . . . . . . .
204
Figure 13.1
Framework of discrete FWA . . . . . . . . . . . . . . . . . . . . . .
212
Figure 13.2
The 2-opt local search operation (left the original
route, right mutated route) . . . . . . . . . . . . . . . . . . . . . . .
214
Figure 13.3
Two possible 3-opt moves (left original tour,
right mutated rours) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
Figure 13.4
The impact of θ and Lm=Lo for pa . . . . . . . . . . . . . . . . . .
217
Figure 13.5
2h-opt local search operation. . . . . . . . . . . . . . . . . . . . . .
217
Figure 13.6
The impact of fireworks number on performance . . . . . . . .
221
Figure 13.7
The impact of maximal spark number
on performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
Figure 13.8
The convergence curve of DFWA-na and DFWA-ri
on d198 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
Figure 13.9
A solution of d198 optimized by DFWA whose
the error rate is 0.05 %. . . . . . . . . . . . . . . . . . . . . . . . . .
224
Figure 14.1
Floating-point operations per second for the CPU
and GPU (data from [3]). . . . . . . . . . . . . . . . . . . . . . . . .
228
Figure 14.2
Memory bandwidth for the CPU and GPU (data
from [3]). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
Figure 14.3
The GPU devotes more transistors to data
processing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
xxxiv
List of Figures
www.allitebooks.com

Figure 14.4
Shading language GPU computing model . . . . . . . . . . . . .
229
Figure 14.5
Schematic diagram of Attract–Repulse Mutation . . . . . . . .
235
Figure 14.6
E[x] under different values of A . . . . . . . . . . . . . . . . . . .
236
Figure 14.7
Simulation results with different uniform
distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Figure 14.8
The flowchart of the GPU–FWA implementation
on CUDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
Figure 14.9
Interleaving storage . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
Figure 14.10
Continuous storage. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
Figure 14.11
Speedup versus FWA . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
Figure 14.12
Speedup versus PSO . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
Figure 15.1
Scheme of very coarse NMF approximation with very
low-rank k. Although k is significantly smaller than
m and n, the typical structure of the original data
matrix can be retained (note the three different groups
of data objects in the left, middle, and right
part of A) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
Figure 15.2
Illustration of the optimization process for row
l of the NMF factor W. The lth row of A (ar
l ) and
all columns of H0 are the input for the optimization
algorithms. The output is a row vector wr
l (the lth row
of W) which minimizes the norm of dr
l , the lth row
of the distance matrix D. The norm of dr
l is the ﬁtness
function for the optimization algorithms (minimiza-
tion problem) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
Figure 15.3
Left-hand side average approximation error per row
(after initializing rows of W). Right-hand side average
approximation error per column (after initializing
of H). NMF rank k ¼ 5. Legends are ordered
according to approximation error (top worst,
bottom best) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
Figure 15.4
Similar information as for Fig. 15.3, but for NMF
rank k = 30. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
Figure 15.5
Accuracy per Iteration when updating only the row
of W, m = 2, c = 20. . . . . . . . . . . . . . . . . . . . . . . . . . . .
260
Figure 16.1
Document clustering by fireworks algorithm
on 20-newsgroup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
Figure 16.2
Intelligent spam detection model with LC and SVM.
Reprinted from Ref. [29], with kind permission from
Springer ScienceþBusiness Media . . . . . . . . . . . . . . . . . .
269
Figure 16.3
Framework of parameter optimization of spam
detection with FWA. Reprinted from Ref. [29], with
kind permission from Springer Science+Business
Media. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270
List of Figures
xxxv

Figure 16.4
Comparison of performance before and after
parameter optimization with strategy 1. Reprinted
from Ref. [29], with kind permission from Springer
Science+Business Media. . . . . . . . . . . . . . . . . . . . . . . . .
272
Figure 16.5
Comparison of performance before and after
parameter optimization with strategy 2. Reprinted
from Ref. [29], with kind permission from Springer
Science+Business Media. . . . . . . . . . . . . . . . . . . . . . . . .
273
Figure 16.6
Flowchart of inversion . . . . . . . . . . . . . . . . . . . . . . . . . .
279
Figure 16.7
Convergency curve of optimization . . . . . . . . . . . . . . . . .
281
Figure 16.8
Comparison of optimization results . . . . . . . . . . . . . . . . .
281
Figure 17.1
A screenshot of the problem at the beginning
of the simulation. Red circles stand for the targets.
The background color illustrates fitness of that
position. Robots and obstacles are not illustrated
in this figure (Color figure online) . . . . . . . . . . . . . . . . . .
287
Figure 17.2
Introducing explosion strategies into swarm
robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
Figure 17.3
Flow chart of GES. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
xxxvi
List of Figures

List of Tables
Table 2.1
FWA with and without Gaussian mutation on function
Sphere and Generalized Rosenbrock . . . . . . . . . . . . . . . . . .
29
Table 2.2
Comparison of FWA with CPSO and SPSO (lower than
10−6 is treated as zero) . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Table 3.1
Random number generators tested. . . . . . . . . . . . . . . . . . . .
53
Table 3.2
RNG efficiency comparison under different batch
size (# of random numbers per nanosecond). . . . . . . . . . . . .
55
Table 4.1
Mean and variance of 10 benchmark functions
(the variance is parenthesis) used in experimental
evaluations, the bold font numbers show the better
final results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
Table 4.2
T-Test results of 10 benchmark functions used
in experimental evaluations, the bold font means
the significance of the proposed acceleration methods
in the significant level of 0.05 . . . . . . . . . . . . . . . . . . . . . .
68
Table 4.3
Confidence interval of p-value of approximation
methods, sampling method, and sampling number
method, the sampling number for each method is 60,
i.e., it follows a normal distribution according to the
central limit theorem, and the confident probability
is 0.95, i.e., u = 1.96. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
Table 6.1
Algorithms and new operators . . . . . . . . . . . . . . . . . . . . . .
95
Table 6.2
Shift index (SI) and shift value (SV) . . . . . . . . . . . . . . . . . .
95
Table 6.3
Mean and variance (in parenthesis) of all 12 benchmark
functions used in the experiments (SI = shift index) . . . . . . .
97
Table 6.4
t-test results for EFWA versus conventional FWA . . . . . . . .
100
xxxvii

Table 7.1
Wilcoxon signed-rank test results for EFWA
versus EFWA-NG and dynFWA-G versus dynFWA
(bold values indicate the performance difference
is significant) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Table 7.2
Mean fitness on the benchmark functions and mean
fitness rank of SPSO2011, EFWA and dynFWA . . . . . . . . .
115
Table 7.3
Wilcoxon signed-rank test results for dynFWA
versus EFWA (bold values indicate the significant
improvement). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
Table 7.4
Runtime comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
Table 8.1
Mean ranking of mean error on 28 functions . . . . . . . . . . . .
129
Table 8.2
T-test results on AFWA versus EFWA . . . . . . . . . . . . . . . .
129
Table 9.1
List of mean fitness on the 28 benchmark function and
mean fitness rank for SPSO2011, EFWA, dynFWA,
and CoFWA. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
Table 9.2
Ranking of SPSO2011, EFWA, dynFWA,
and CoFWA. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
Table 11.1
The parameters representation. . . . . . . . . . . . . . . . . . . . . . .
168
Table 11.2
Parameter values of our algorithm for the test problems
(xUB, xLB denote the upper and lower search bounds) . . . . . .
176
Table 11.3
Computational results on the BN VFR problem
instances, where the values are averaged over 30
independent runs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
Table 11.4
Computational results on the CA VFR problem
instances, where the values are averaged over 30
independent runs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
Table 11.5
The input data of the real-world CO VRF problem: field
gradients and plant densities. . . . . . . . . . . . . . . . . . . . . . . .
184
Table 11.6
The non-dominated solution sets obtained
by the old random search algorithm and by the
MOFWA for the real-world CO VRF problem . . . . . . . . . . .
185
Table 12.1
Benchmark functions (Dim. = dimension) . . . . . . . . . . . . . .
200
Table 12.2
Convergence measure . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Table 12.3
Covered space measure . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Table 12.4
p-values for convergence measure (The values in bold
indicate that MOFWA is significantly better compared
with the other algorithms) . . . . . . . . . . . . . . . . . . . . . . . . .
203
Table 12.5
p-values for covered space measure (The values in bold
indicate that MOFWA is significantly better compared
with the other algorithms) . . . . . . . . . . . . . . . . . . . . . . . . .
203
Table 12.6
Evaluation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
Table 13.1
The results on oliver30, the data besides DFWA is from
[16], the minimal route length is 420. . . . . . . . . . . . . . . . . .
223
xxxviii
List of Tables

Table 13.2
The results on att48, the data besides DFWA is from
[16], the minimal route length is 33522 . . . . . . . . . . . . . . . .
223
Table 13.3
Result on d198, the data besides DFWA is from [13],
theminimal route length is 15780 . . . . . . . . . . . . . . . . . . . .
223
Table 13.4
Result in larger test cases, the data besides DFWA
is from [10] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
Table 14.1
Precision comparisons among GPU–FWA, FWA,
and PSO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
Table 14.2
p-values of t-test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
Table 14.3
Running time and speedup of Rosenbrock . . . . . . . . . . . . . .
241
Table 16.1
Experimental results of EER and parameters of K2, K3
on the PolyU palmprint database . . . . . . . . . . . . . . . . . . . .
277
Table 16.2
Comparison of optimization results . . . . . . . . . . . . . . . . . . .
282
Table 17.1
Validation results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
296
List of Tables
xxxix

Part I
Fundamentals and Basic Theory
This part describes the principle, fundamentals, and basic theory of ﬁreworks
algorithm (FWA). The ﬁrst chapter introduces the study of the origins and moti-
vations, research areas, problems to be solved, features, as well as future research
directions of ﬁreworks algorithms. The second chapter gives the details of ﬁreworks
algorithms, including FWA’s components and framework, characteristics, and
comparisons with other SI algorithms. The third chapter gives a stochastic model of
ﬁreworks algorithm, proves its global convergence, discusses and analyzes its time
complexity, and ﬁnally studies the effects of different types of random number
generators on the performance of FWA.

Chapter 1
Introduction
This chapter presents the motivation of when, why, and how the ﬁreworks algorithm
(FWA), as a novel swarm intelligence optimization algorithm, came out. After a con-
cise review on swarm intelligence domain, a brief introduction to FWA is presented
with primary focuses on four aspects of theoretical analysis, algorithm study, prob-
lem solving, and applications. The characteristics and advantages of FWA are also
described. Finally, overviews of FWA research are detailed with completed reference
citations.
1.1 Motivations
During my childhood in Sichuan, in southwest China, it was a great pleasure for
me to set off ﬁreworks, or ﬁrecrackers, or sparkler, or Yan Hua in Chinese, with my
friends during the Spring Festival, the most important traditional festival in China.
Now and then, we competed for the title of master who could ignite ﬁreworks that
could ﬂy the highest into the sky and make the loudest noise. Though the time has
long passed, the splendid sparks showering the night sky is imprinted in my memory.
By the Spring Festival of 2006, I had worked at Peking University for one year.
During that time, I devoted to the in-depth study of evolutionary computation (EC)
in computational intelligence. Therefore, I tried to relate any novel phenomena I met
to EC during that time. Just in that year, Beijing authorities relaxed ﬁreworks ban to
restriction during the Spring Festival. After many years’ ban on ﬁrework celebrations,
citizens of the Capital were eager for the arrival of the Chinese Lunar New Year’s
Eve and looked forward to a livelier and jollier Spring Festival.
On this year’s Eve Day, people set off a large amount of ﬁreworks as if it could
relieve the whole year’s stress. The sparks of diverse colors lighted up the dark sky
and made various beautiful patterns. The glorious scene, reminiscent of childhood
memories, brought much joy and comfort to me.
An idea suddenly came to my mind that the way ﬁreworks explode may be an
efﬁcient strategy for searching the solution space. Such a search strategy would be
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_1
3

4
1
Introduction
different from the established ones. So I began to study this explosion-like search
method, and named it Fireworks Algorithm, i.e., FWA, for short.
Although the name “Fireworks Algorithm” sounds intuitive and compact, since
this name does not directly relate to optimization problems it is good at, some
other names are also used by other researchers, such as “Fireworks Optimization,”
“Fireworks Explosion Algorithm,” “Fireworks Explosion Optimization Algorithm,”
“Fireworks Explosion Search Algorithm,” “Explosion Search Method,” and so on. In
order to avoid any possible confusion, only the original name “Fireworks Algorithm,”
abbreviated by FWA, is used hereafter in this book.
The motivation of studying FWA is to seek a simple and efﬁcient method dealing
with complex optimization problems, especially multimodal optimization problems.
Inthesummersemesterof2006,YuanchunZhufromJilinUniversitywasadmitted
by Peking University as a PhD student under my supervision and I arranged for him
to do his graduate project in my laboratory in the early months of the following
year. We discussed the idea about FWA and set the project content as the study and
implementation of Fireworks Algorithm. Together, we started a thorough study of
FWA.
After half a year’s exploration and study, FWA’s main components and overall
frameworkwerecompletelyestablished.Finally,theconventionalFWAwasdesigned
based on the explosive operator. The pioneering work was ﬁnished by May 2007.
Unfortunately, in the following two years, due to my hosting of a project supported
by the National High-tech Research and Development Program (863 Program), the
FWA-related researches were suspended. Till 2010, the original work on FWA was
reported at the First International Conference of Swarm Intelligence (ICSI’ 2010),
entitled “Fireworks Algorithm for Optimization.” After then, FWA has increasingly
gained more attention in the ﬁeld of swarm intelligence.
1.2 Brief Introduction to Swarm Intelligence
Swarm intelligence is an active branch of Evolutionary Computation which is one of
the most important research topics in Computational Intelligence (CI) Community.
In the past several decades, fruitful achievements have been made on the research of
Computational Intelligence, such as artiﬁcial neural networks [1–6], fuzzy logic and
systems [7], evolutionary computation [8–11], chaos computation [12], simulated
annealing [13], tabu search [14], and hybrid strategies, to name a few. All these
methods simulate and give an insight into natural phenomena or biological processes.
Swarm intelligence (SI) is regarded as the collective behavior of decentralized,
self-organized, and populated systems. A typical swarm intelligence system consists
of a population of simple agents that can communicate (either directly or indirectly)
locally with each other by acting on their local environment. Though the agents in
a swarm follow very simple rules, the interactions between such agents can lead
to the emergence of very complicated global behavior, far beyond the capability of

1.2 Brief Introduction to Swarm Intelligence
5
individual agents. Examples in natural systems of swarm intelligence include bird
ﬂocking, ant foraging, and ﬁsh schooling.
Inspired by such behavior of swarms, a class of algorithms was proposed for tack-
ling optimization problems, under the title of Swarm Intelligence Algorithm. In SI
Algorithm, a swarm is made up of multiple artiﬁcial agents. The agents can exchange
heuristic information in the form of local interaction directly or indirectly (via envi-
ronment). Such interaction, in addition to certain stochastic elements, generates the
behavior of adaptive search, and ﬁnally leads to global optimization.
The most respected and popular SI algorithms are particle swarm optimization
(PSO), which is inspired by the social behavior of bird ﬂocking or ﬁsh schooling,
and ant colony optimization (ACO) which simulates the foraging behavior of ant
colony. PSO is widely used in real-parameter optimization while ACO has been
successfully applied to solve combinatorial optimization problems. The most well-
known problems are the traveling salesman problem (TSP) and quadratic assignment
problem (QAP).
Swarm intelligence algorithms can fall into two major categories, bio-inspired
and non-bio-inspired [15]. The former includes ant colony optimization (ACO) [16],
particle swarm optimization (PSO) [17], ﬁsh schooling search (FSS) [18], ﬁreﬂy
algorithm-I [19], ﬁreﬂy algorithm-II [20], buccock algorithm [21], bat algorithm
[22], artiﬁcial bee algorithm (ABC) [23–25], bacterial foraging optimization (BFO)
[26], and so forth. Non-bio-inspired algorithms consist of ﬁreworks algorithm (FWA)
[27], water drops algorithm [28], brain storm optimization (BSO) [29], and magnetic
optimization algorithms [30], to name a few.
Nowadays, research efforts on swarm intelligence are mainly devoted to algorithm
design, problem solving, and applications. Swarm intelligence algorithms are more
andmoreappliedtolarge-scaleproblemstotackletheissuesofthecurseofdimension
and big data. Hybrid algorithms and variants are actively proposed.
The cooperative mechanism in swarm intelligence algorithms has the capability of
breaking down the curse of no free lunch (NFL) [31] which means there exist efﬁcient
intelligent algorithms that can also to be found by fully utilizing the cooperation and
co-evolution among individuals in a swarm [32]. This will stimulate more and more
researchers to enter the ﬁeld of swarm intelligence and develop more and more
efﬁcient algorithms to solve many real-world problems for us.
1.3 Brief Introduction to FWA
Just like conventional swarm intelligence algorithms, FWA is an iterative algorithm
made up of four key components or building blocks, i.e., explosive operator, mutation
operation, mapping rule, and selection strategy. Explosive operator can be divided
further into explosion strength, explosion amplitude, shift mutation, and other oper-
ators. Gaussian mutation is the most widely used mutation operator. Mirror mapping
rule and stochastic mapping rule are two popular mapping rules for FWA. As for
selection strategy, there are distance-based selection and stochastic selection.

6
1
Introduction
Speciﬁcally, workﬂow of FWA can be stated as follows. First, N ﬁreworks are
generated randomly as the initial swarm. Then, every ﬁrework conducts explosion
operation and mutation operation, and mapping rule is triggered if necessary. Finally,
N ﬁreworks are selected from all the ﬁreworks and generated sparks according to
the selection strategy. The iteration continues until particular termination criterion
is satisﬁed. As the iteration proceeds, better optimization results can be achieved
eventually.
The study of FWA mainly focuses on four aspects: theoretical analysis, algorithm
study, problem solving, and applications.
1. Theoretical Analysis
Theoretical analysis involves FWA’s mechanism, convergence quality, trajectory,
and parameters. It helps the design of new algorithms and provides helpful insight
for improving established algorithms.
2. Algorithm Study
By analyzing and adjusting FWA’s components, people try to improve FWA’s
performance (convergence, solution quality, and time efﬁciency), and come up
with improved FWA variants. In the meantime, it will be helpful to integrate FWA
with other methods, ending up with efﬁcient hybrid algorithms.
3. Problems to be Solved
• Single-Objective Optimization Problems—SOO: it is concerned with mathe-
matical optimization problems involving only one objective function.
• Constrained Single-Objective Optimization Problems—CSOO: it is the SOO
problem with constraint(s).
• Multiple-Objective Optimization Problems—MOO: it is to make decision
under multiple criteria, which is concerned with mathematical optimization
problems involving more than one objective function to be optimized simulta-
neously.
• Constrained Multiple-Objective Optimization Problems—CMOO: it is the
MOO problem with constraint(s).
• Many-ObjectiveOptimizationProblems—ManyOO:itisconcernedwithmath-
ematical optimization problems involving more than three objective functions
to be optimized simultaneously.
• Combinatorial Optimization Problems—CO: it is a topic that consists of ﬁnding
an optimal object from a ﬁnite set of objects
• Dynamic Optimization Problems—DOP: it is a time-dependent optimization
problem, i.e., either the objective function and constraints or the associated
parameters or both vary with time.
• Other optimization problems
4. Applications
FWA can be directly applied to many different real-world problems from which
an optimization task can be drawn.
www.allitebooks.com

1.4 Characteristics and Advantages of FWA
7
1.4 Characteristics and Advantages of FWA
Some of FWA’s characteristics and advantages are summarized as follows
1. Explosiveness: Each iteration, every ﬁrework explodes and generates a number
of sparks within the radiation range. Then, selection procedure is triggered to
choose a ﬁxed number of ﬁreworks for the next iteration.
2. Instantaneity: The sparks are instantaneous, which means that sparks not selected
will vanish.
3. Simplicity: every agent has only limited capability, thus the whole algorithm is
simple to be implemented.
4. Locality: For a particular ﬁrework, its explosion amplitude is smaller than the
feasible bound, so it can only exploit locally in the search space.
5. Emergent properties: Though following simple rules, by cooperation and com-
petition, the swarm, as a whole, shows complicated behaviors, far more complex
than every single ﬁrework, i.e., intelligence emerges.
6. Parallelism: There is no central control mechanism among ﬁreworks, thus the
ﬁreworks are highly independent, much suitable for parallelization.
7. Diversity: The diversity is threefold. First, ﬁreworks are diverse. Proper selection
guarantees that the selected ﬁreworks distribute in diverse places in the search
space. Second, there are diverse explosion strengths and amplitudes. According
to their particular ﬁtness, different ﬁreworks will have various explosion strengths
and amplitude. Besides, there are multiple mutation operators in the FWA. Two
of the well-studied mutations are displacement mutation and Gaussian mutation.
Displacement mutation is subjected to the ﬁrework’s ﬁtness while Gaussian muta-
tion ﬁrework is independent. The two mutations of very different ﬂavors guarantee
the diversity of the mutation procedure.
8. Robustness: As the ﬁreworks are highly independent, the swarm behaves will not
degrade too much in the presence of a few individuals’ failure.
9. Flexibility: The problem does not need to have an explicit expression to be opti-
mized by FWA. Thus FWA can address a very broad range of problems for which
conventional optimization procedures unfortunately fail.
1.5 Overviews of FWA Research
Since the inception of FWA introduced by the seminal paper entitled “Fireworks
algorithm for optimization [27]” by the author et al., FWA has drawn much attention
from both academic and industrial ﬁelds. Much effort has been devoted to analyzing
the conventional FWA and many improvements have been proposed to compensate its
shortcomings. Several hybrid algorithms have also been introduced. These research
efforts lead to a huge leap in terms of FWA’s performance. Many applications are
reported, where FWA is successfully utilized to tackle diverse problems.

8
1
Introduction
A brief summary of these achievements on the studies of FWAs is listed as follows.
1. Theories
Liu et al. [33] analyzed FWA’s convergence property theoretically. The authors
pointed out that FWA is an absorbed Markov process. Besides, this book also
studied the impact of random number generators on the performance of FWA, for
the ﬁrst time.
2. Algorithms
Ding et al. [34] proposed a GPU-based parallel FWA, dubbed GPU-FWA. GPU-
FWA achieved 200+ speedup compared to the CPU implementation. GPU-FWA
made some major modiﬁcations on FWA to minimize the inter-ﬁrework commu-
nication and thus maximize the utilization of GPU.
Pei et al. [35] proposed a method to accelerate FWA’s convergence by appropri-
ating the objective function. The experiments show that optimal performance is
achieved when a quadratic model is utilized with random selection strategy. The
proposed method outperforms the conventional FWA signiﬁcantly.
Zheng S et al. [36] observed FWA extensively and proposed several improvements
accordingly. The authors proposed Enhanced FWA (EFWA), which modiﬁes all
the key components of the conventional FWA.
In [37] and [38], Li et al. and Zheng S et al. devoted to studying the adaptation in
FWA’s explosion amplitude, as a result, dynamic search FWA and adaptive FWA
were proposed, respectively.
Besides, some researchers studied the combination of FWA with other SI algo-
rithms and proposed several hybrid FWAs. In [39] and [40], Zheng Y.J. and Yu
et al. devoted to the hybrid of FWA and DE. In [39], the proposed FWA-DE shows
advantage over both FWA and DE on several benchmark test functions. In [41],
Gao et al. combined FWA with cultural algorithm and then applied it to the opti-
mization of digital ﬁlter design. Gao et al. compared the proposed algorithm with
two PSO variants [42, 43]. The experimental results show that the new algorithm
achieved the best performance of the three. Zhang et al. proposed BBO FWA,
which outperformed BBO and FWA with clear advantages [44].
Very recently, Dr. James McCaffrey presented “Test Run—Fireworks Algorithm
Optimization” on the latest issue of MSDN magazine [45] which is the Microsoft
journal for developers. His article describes a relatively new (ﬁrst published in
2010[27])optimizationtechniquecalledﬁreworksalgorithmoptimization(FAO).
The technique does not explicitly model or mimic the behavior of ﬁreworks,
but when the algorithm is visually displayed, the resulting image resembles the
geometry of exploding ﬁreworks. In one word, he thought that the FAO is indeed
an interesting heuristic optimization approach.
3. Varieties of Optimization Problems
FWA has been used to solve single-objective and multi-objective real-valued opti-
mization problems. Zheng Y. et al. proposed a multi-objective FWA (MOFWA)
and applied it to the oil crops’ fertilization problem [46]. The comparative exper-
iments show that MOFWA could obtain better performance than other popular

1.5 Overviews of FWA Research
9
swarm intelligence multi-objective algorithms. This book describes, for the ﬁrst
time, the applications of FWA on tackling the well-known combinatorial opti-
mization problem of Traveling Salesman Problem (TSP).
4. Applications
FWA has been applied to diverse real-world problems, for example, equations
solving [47], nonnegative matrix factorization (NMF) [48], spam detection [49],
distance metric [50], ﬁlter design [41], crops fertilization [46], swarm robot
[51–53], etc.
Zhen-xin et al. [54] proposed an improved FWA to solve nonlinear equations
and system problems. His experiments show that the FWA has advantages over
the other algorithms in solving variable-coupling equations and also gave an
analysis to disclose why the improved FWA is more effective. Recently, Sujin
Bureerat, and Nantiwat Pholdee, in [55], systematically compared and studied
the optimizing performance of 24 metaheuristic algorithms for mass minimiza-
tion of trusses with dynamic constraints, and gave an objective ranking among
which FWA is above the average with a promising performance. Mohamed Imran,
Kowsalya and Kothari, in [56] and [57], presented a novel integration technique
for optimal network reconﬁguration and distributed generation (DG) placement
in distribution system with an objective of power loss minimization and voltage
stability enhancement. They used ﬁreworks algorithm (FWA) to simultaneously
reconﬁgure and allocate optimal DG units in a distribution network. Six different
scenarios are considered during DG placement and reconﬁguration of network to
assess the performance of the proposed technique [56]. Simulations were carried
out on well-known IEEE 33- and 69-bus test systems at three different load lev-
els to demonstrate well the performance and effectiveness of the proposed FWA
method. Recently, in [58], R. Rajaram et al. presented their latest work of selec-
tive harmonic elimination in PWM inverter using ﬁreworks algorithm in which
a slightly modiﬁed FWA and ﬁreﬂy were used to solve the selective harmonic
elimination problem in inverter output waveforms. It turns out from experiments
that the FWA works faster than modiﬁed ﬁreﬂy algorithm as well as other popu-
lar algorithms of ant colony, PSO, and GA. Moreover, speciﬁc comparisons with
other methods in the literature like GA [59], Ant colony [60], and Bees intel-
ligence [61] suggested that Fireworks algorithm worked excellently and Fireﬂy
was also good for this application. Noora Hanni Abdulmajeed and Masri Ayob, in
[62], applied the FWA to the capacitated vehicle routing problem (CVPR), which
is an important problem in the industry sector with many applications in the ﬁeld
of transportation, distribution, and logistics. Based on the tests on 14 instances of
Christoﬁdes benchmarks, the FWA is very competitive in terms of the quality of
the solutions compared to other SI methods like OCGA, AGES, and AHMH [62].
The application of FWA on document clustering is also presented in this book for
the ﬁrst time.
For a detailed review of FWAs, readers can refer to [63, 64].

10
1
Introduction
1.6 Overview of the Book
Brieﬂy, this book is organized into four parts. In a sequel, Part A grouped together
three chapters, forming the most substantial theoretical analysis part of this book,
including introduction, basic principle and implementation of FWA, and modeling
and theoretical analysis of FWA. Part B covers almost the most important FWA vari-
ants so far, consisting of seven chapters each of which describes one kind of recent
signiﬁcant improvements of FWA. In Part C, four chapters are included to present
some advanced topics in the research of FWA, including two multi-objective opti-
mization (MOO) schemes, discrete FWA (DFWA) for combinatorial optimization,
and GPU-based FWA for parallel implementation of FWA. In Part D, several success-
ful applications of FWA on nonnegative matrix factorization (NMF), text clustering,
pattern recognition, and seismic inversion problem are reported for instances which
might give insights on more and more real-world applications of FWA in the future,
and ﬁnally, a conclusion and postscript are given. Speciﬁcally, let us brieﬂy describe
the abstract of each chapter for every part in this book as follows.
In part A of fundamental and basic theory, it describes the basic principle and
theories of ﬁreworks algorithm (FWA). The ﬁrst chapter introduces the study of
the origins and motivations, research areas, the problem content, and features, as
well as the future research directions, of FWA. Chapter2 describes the details of
ﬁreworks algorithm, including FWA’s components and framework, characteristics,
implementation, experimentation, and detailed comparisons with other SI algorithms
such as particle swarm optimization (PSO), genetic algorithm (GA), etc. Chapter3
presents modeling and theoretical analysis of FWA. It gives a stochastic model of
ﬁreworks algorithm, proves its global convergence, discusses, and analyzes its time
complexity, and, ﬁnally, studies the effects of different types of random number
generators on the performance of FWA.
In part B of FWA variants, it has the most important improvements of FWA till
now. Since the invention of ﬁreworks algorithm (FWA) in 2010, it has attracted a lot of
attention in the community of swarm intelligence (SI) [27, 63, 64]. The improvement
work on FWA is able to be classiﬁed into two categories. One is based on improving
the limitations of FWA, and the other is hybridized with other SI algorithms.
Chapter 4 describes an empirical study on the inﬂuence of approximation app-
roaches on accelerating the ﬁreworks algorithm search by elite strategy. It uses three
data sampling methods to approximate ﬁtness landscape, i.e., the best ﬁtness sam-
pling method, the sampling distance near the best ﬁtness individual sampling method,
and the random sampling method. For each approximation method, this book con-
ducts a series of combinative evaluations with the different sampling methods and
sampling numbers for accelerating ﬁreworks algorithm. The experimental evalua-
tions on benchmark functions show that this elite strategy can enhance the ﬁreworks
algorithm search capability effectively. This chapter also analyzes and discusses
the related issues on the inﬂuence of approximation model, sampling method, and
sampling number on the ﬁreworks algorithm acceleration performance.

1.6 Overview of the Book
11
In Chap.5, FWA with controlling exploration and exploitation is introduced by
providing a kind of new method to calculate the number of explosion sparks and
amplitude of ﬁrework explosion. By designing a transfer function, the rank number
of ﬁrework is mapped to scale of the calculation of scope and spark number of
ﬁrework explosion. A parameter is used to dynamically control the exploration and
exploitation of FWA with iteration going on.
In Chap.6, the enhanced ﬁreworks algorithm (EFWA) is presented, which gives
the comprehensive analysis of FWA and points out ﬁve limitations in FWA which
leads to the related improvement work on those limitations. The new operators which
overcome the limitations are presented, which include the minimal explosion ampli-
tude check strategy, the new mapping operator, the new operator for generating
explosion sparks, the new Gaussian mutation operator, and selection operator. Exper-
imental results suggest that the newly proposed operators are effective to deal with
the limitation pointed in this chapter.
In Chap.7, a dynamic search ﬁrework algorithm (dynFWA) is presented. The
dynFWA is an improvement of the recently developed enhanced ﬁreworks algorithm
(EFWA) which uses a dynamic explosion amplitude for the core ﬁrework (CF),
i.e., the ﬁrework at the currently best position. This dynamic explosion amplitude
depends on the quality of the current local search around the CF. The main task for
the CF is to perform a local search, while the responsibility of all other ﬁreworks is to
maintain the global search ability. Additionally, this chapter analyzes the possibility
to remove the rather time-consuming Gaussian sparks operator of EFWA. Extensive
experimental evaluations show that the proposed dynFWA algorithm signiﬁcantly
improves the results of EFWA and also reduces the runtime by more than 20%.
The explosion amplitude is a key factor inﬂuencing the performance of the Fire-
works Algorithm, which needs to be controlled precisely. In Chap.8, a new algorithm
called adaptive ﬁreworks algorithm, which replaces the explosion amplitude operator
in EFWA with an adaptive method, is proposed. The distance of the best ﬁrework and
a certain individual subjecting to some conditions is employed as the amplitude of
the explosion. Chapter8 analyzes the property of the adaptive explosion amplitude
and come to the conclusion that the adaptive explosion amplitude for explosion is a
theoretically promising operator. According to the experimental results on CEC13’s
28 benchmark functions, the performance is greatly improved.
In previous FWA’s studies so far, researchers ignored the cooperation and inter-
action between the individual ﬁreworks in the swarm, which are the most important
core for any swarm intelligence algorithm. By incorporating a probabilistically ori-
ented explosion mechanism (POEM) into the conventional FWA, a novel cooperative
ﬁreworks algorithm (CoFWA, for short) is proposed in Chap.9 to enhance the inter-
actions among the individual ﬁreworks in the swarm. In CoFWA, the POEM mech-
anisms of sparks generation and ﬁreworks selection are well designed to strengthen
the cooperative capability of the individual ﬁreworks in the CoFWA. Experiments
show that the CoFWA signiﬁcantly outperforms two most recent variants of FWA
(i.e., EFWA and dynFWA) and SPSO2011 and shows a competitive performance
against the state-of-the-art swarm intelligence algorithms.

12
1
Introduction
Fireworks algorithm is also suitable for combining with other EC algorithms for
producing new efﬁcient hybrid algorithms. In Chap.10, the combinations between
FWA and other SI algorithms are given out. Focus is emphasized on hybrid ﬁre-
works algorithms, including ﬁreworks algorithm with differential mutation (FWA-
DM), hybrid ﬁreworks optimization method with differential evolution operators
(FWA-DE), culture ﬁreworks algorithm (CFWA), and hybrid biogeography-based
optimization and ﬁreworks algorithm (BBO FWA).
In Part C of advanced topics of FWA, four chapters are to present the advanced top-
ics of FWA, including multi-objective optimization (MOO), discrete FWA (DFWA)
for combinatorial optimization, and GPU-based FWA for parallel implementation of
FWA.
As we know, FWA was ﬁrst proposed for single-objective optimization problems
and has been widely studied based on the CPU platform. Multi-objective optimiza-
tion problems are universal and much more complicated than their single-objective
counterparts. Chapter 11 will introduce the applications of FWA on multi-objective
problems. An efﬁcient MOFWA algorithm is proposed for oil crop VRF problems,
which uses a problem-speciﬁc strategy for generating the initial population, uses the
concept of Pareto dominance for individual evaluation and selection, and combines
the DE operators to increase the information sharing and thus diversify the search.
The MOFWA algorithm has been successfully applied to a number of VRF problems
and demonstrated its efﬁciency and effectiveness.
Furthermore, in Chap.12, with the help of the well-known S-metric, a kind of
hyper-volume indicator, an S-metric multi-objective ﬁreworks algorithm (MOFWA),
is proposed for efﬁciently solving multi-objective optimization problems. The S-
metric is a frequently used quality measure for solution sets’ comparison in evo-
lutionary multi-objective optimization algorithms (EMOAs), which is also used to
evaluate the contribution of a single solution among the solution set. Traditional
multi-objective optimization algorithms usually perform a (µ + 1) strategy and
update the external archive one by one, while the proposed S-MOFWA performs
a (µ + µ) strategy, thus converging faster to a set of Pareto solutions. The compar-
ison results with NSGA-II, SPEA2, and PESA2 demonstrate the efﬁciency of the
proposed S-MOFWA.
Discrete FWA (DFWA) for combinatory optimization problems is just in its ﬁd-
dle era, and Chap.13 presents FWA’s application in traveling salesman problem
(TSP). The DFWA remains the basic framework of FWA and introduces some major
changes in explosion operator, selection strategy, and mutation operator, respec-
tively. In explosion operator, every ﬁrework is able to accept a worse solution and
generate a spark with lower ﬁtness, which refers to the mechanism of simulated
annealing. However, a controlling parameter changes with the feedback of optimiza-
tion process rather than time. In addition, this version of discrete ﬁrework algorithm
properly changes its behavior of the local search method to suit in the framework of
FWA. As the experimental results indicate, the DFWA is very effective for TSP, and
potentially applied to other discrete optimization problems.
GPU is a game-changing force in the domain of high-performance computing
(HPC). Thanks to GPU’s parallelism and great computational power, swarm intelli-

1.6 Overview of the Book
13
gence algorithms are able to fully exploit their inherent parallelism. Building swarm
intelligence algorithms on the GPU platforms is an increasingly important and popu-
lar research topic. In Chap.14, the implementation of FWA based GPU is presented.
In this chapter, a very efﬁcient FWA variant based on GPUs, dubbed GPU-FWA,
is introduced. GPU-FWA modiﬁes the original FWA to suit the particular architec-
ture of the GPU. It does not need special complicated data structure, thus making it
easy to implement; in the meantime, it can fully exploit the great computing power
of GPUs. A mutation mechanism called attract-repulse mutation is introduced to
guide the search process. To make the chapter self-contained, a brief introduction of
general purpose computing on GPUs (GPGPU) is presented ﬁrst. Then, this chapter
describes GPU-FWA in detail, followed by the empirical comparison of GPU-FWA
with conventional FWA as well as PSO.
In Part D, this book veriﬁes how ﬁreworks algorithms can be applied to var-
ious applications in different areas. These applications include traditional pattern
recognition problems (nonnegative matrix factorization, document clustering, spam
detection,andimagerecognition),complexmodelestimationproblem(seismicinver-
sion), and emerging swarm robotics searching problem. These applications sit in
areas which differ greatly than each other and have different requirements for the
optimization algorithms. Fireworks algorithm can solve these problems successfully
which illustrates that the algorithm has a great adaption to different requirements in
real-life applications.
Chapter 15 presents two new optimization strategies for improving the NMF using
optimization algorithms based on swarm intelligence. While strategy one uses swarm
intelligence algorithms to initialize the factors and prior to the factorization process
of NMF, the second strategy aims at iteratively improving the approximation quality
of NMF during the ﬁrst iterations of the factorization. Five different optimization
algorithms were used for improving NMF, including particle swarm optimization
(PSO), genetic algorithms (GA), ﬁsh school search (FSS), differential evolution
(DE), and Fireworks Algorithm (FWA).
Chapter 16 describes the applications of ﬁreworks algorithm for dealing with
some practical optimization problems such as document clustering, spam detection,
image recognition, and seismic inversion problem. The experimental results given
herein suggest that FWA is one of the most promising swarm intelligence algorithms
in dealing with those real-world problems.
Swarm robotics is an emerging research area combining swarm intelligence and
robotics. In Chap.17, inspired by Fireworks Algorithm, a group explosion strategy
(GES) for searching multiple targets is proposed. GES method is applied to the
multiple targets searching problem on a self-built simulation platform. The swarm
searches and collects targets in the environment without prior knowledge. Several
tests are run to evaluate how GES performs in various aspects including stability,
robustness, and ﬂexibility. Simulation results demonstrate that GES shows great
efﬁciency when ﬁtness is either adequate or inadequate in the environment. GES
also shows good stability in obstructive and large-scale environments.
In Backmatter, a Postscript is given for a few words I want to deliver after the
accomplishment of this manuscript.

14
1
Introduction
After this, several appendices are attached by giving the benchmark functions
suites, web resources of FWA researches, and lists of ﬁgures, tables, and symbols
appeared in the book.
Finally, indices of key words are drawn out at the end of this book.
References
1. M.T. Hagan, H.B. Demuth, M.H. Beale et al., Neural Network Design (Pws Pub, Boston, 1996)
2. G.C. Ruan, Y. Tan, A three-layer back-propagation neural network for spam detection using
artiﬁcial immune concentration. Softcomputing 14, 139–150 (2010)
3. X. Huang, Y. Tan, X.G. He, An intelligent multi-feature statistical approach for discrimination
of driving conditions of hybrid electric vehicle. IEEE Trans. Intell. Transp. Syst. 12(2), 453–456
(2011)
4. Y. Tan, C. Deng, Solving for a quadratic programming with a quadratic constraint based on a
neural network frame. Neurocomputing 30, 117–128 (2000)
5. Y. Tan et al., Neural network design approach of cosine-modulated FIR ﬁlter bank and com-
pactly supported wavelets with almost PR property. Signal Process. 69(1), 29–48 (1998)
6. Y. Tan, Z.K. Liu, On matrix eigendecomposition by neural networks. (Neural Netw. World)
International Journal on Neural and Mass-Parallel Computing and Information Systems 8(3),
337–352 (1998)
7. G.J. Klir, B. Yuan, Fuzzy Sets and Fuzzy Logic, vol. 4 (Prentice Hall, NewD Jersey, 1995)
8. A.E. Eiben, J.E. Smith, Introduction to Evolutionary Computing (springer, Berlin, 2003)
9. Y.Tan,J.Wang,Nonlinearblindseparationusinghigher-orderstatisticsandageneticalgorithm.
IEEE Trans. Evol. Comput. 5(6), 600–612 (2001)
10. J. Zhang, Y. Tan, L. Ni, C. Xie, Z. Tang, AMT-PSO: an adaptive magniﬁcation transformation
based particle swarm optimizer. IEICE Trans. Fundam. Electron. Commun. Comput. Sci. E94-
D(4): 786–797 (2011)
11. Y. Tan, J. Wang, A support vector network with hybrid kernel and minimal Vapnik-
Chervonenkis dimension. IEEE Trans. Knowl. Data Eng. 26(2), 385–395 (2004)
12. H.-O. Peitgen, H. Jrgens, D. Saupe, Chaos and Fractals: New Frontiers of Science (Springer,
Berlin, 2004)
13. P.J.M. Van Laarhoven, E.H.L. Aarts, Simulated Annealing (Springer, Berlin, 1987)
14. F. Glover, M. Laguna, Tabu Search (Springer, 1999)
15. Y. Tan, S. Zheng, Research progress on swarm intelligence optimization algorithms. Commun.
Chin. Autom. Soc. 34(3), (2013)
16. M. Dorigo, M. Birattari, T. Stutzle, Ant colony optimization. IEEE Comput. Intell. Mag. 1(4),
28–39 (2006)
17. J. Kennedy, R. Eberhart et al., Particle swarm optimization, in Proceedings of IEEE Interna-
tional Conference on Neural Networks, vol. 4(2) (Perth, Australia, 1995), pp. 1942–1948
18. C.J.A Bastos Filho, F.B. de Lima Neto, A.J.C.C. Lins, A.I.S. Nascimento, M.P. Lima, Fish
school search, in Nature-Inspired Algorithms for Optimisation (Springer, Berlin, 2009) pp.
261–277
19. S. Ukasik, S. Ak, Fireﬂy algorithm for continuous constrained optimization tasks, in Com-
putational Collective Intelligence. Semantic Web, Social Networks and Multiagent Systems
(Springer, Heidelberg, 2009), pp. 97–106
20. X.-S. Yang, Fireﬂy algorithms for multimodal optimization, in Stochastic Algorithms: Foun-
dations and Applications (Springer, Berlin, 2009), pp. 169–178
21. X.-S. Yang, S. Deb, Cuckoo search via Lvy ﬂights, in 2009 World Congress on IEEE Nature
& Biologically Inspired Computing (NaBIC) (IEEE, 2009), pp. 210–214
22. X.-S. Yang, A new metaheuristic bat-inspired algorithm, in Nature Inspired Cooperative Strate-
gies for Optimization (NICSO) (Springer, Berlin, 2010), pp. 65–74

References
15
23. D. Karaboga, An idea based on honey bee swarm for numerical optimization. Technical report-
tr06, Erciyes University, Engineering Faculty, Computer Engineering Department, (2005)
24. D. Karaboga, B. Basturk, A powerful and efﬁcient algorithm for numerical function optimiza-
tion: artiﬁcial bee colony (ABC) algorithm. J. Glob. Optim. 39(3), 459–471 (2007)
25. D. Karaboga, B. Basturk, On the performance of artiﬁcial bee colony (ABC) algorithm. Appl.
Soft Comput. 8(1), 687–697 (2008)
26. C.R. Blomeke, S.J. Elliott, T.M. Walter, Bacterial survivability and transferability on biometric
devices, in 2007 41st Annual IEEE International Carnahan Conference on Security Technology
(IEEE 2007), pp. 80–84
27. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
28. H. Shah-Hosseini, The intelligent water drops algorithm: a nature-inspired swarm-based opti-
mization algorithm. Int. J. Bio-Inspir. Comput. 1(1), 71–79 (2009)
29. Y. Shi, Brain storm optimization algorithm, in Advances in Swarm Intelligence (Springer,
Berlin, 2011), pp. 303–309
30. N.M.H. Tayarani, M.R. Akbarzadeh-T, Magnetic optimization algorithms a new synthesis, in
2008 IEEE World Congress on Computational Intelligence Evolutionary Computation (CEC)
(IEEE, 2008), pp. 2659–2664
31. D.H. Wolpert, W.G. Macready, No free lunch theorems for optimization. IEEE Trans. Evol.
Comput. 1(1), 67–322 (1997)
32. D.H. Wolpert, W.G. Macready, Coevolutionary free lunches. IEEE Trans. Evol. Comput. 9(6),
721–735 (2005)
33. J. Liu, S. Zheng, Y. Tan, Analysis on global convergence and timecomplexity of ﬁreworks algo-
rithm, in IEEE Congress on Evolutionary Computation (CEC’2014) (Beijing, China, 2014),
pp. 3207–3213
34. K. Ding, S. Zheng, Y. Tan. A GPU-based parallel ﬁreworks algorithm for optimization. In Pro-
ceedings of the 15th Annual Conference on Genetic and Evolutionary Computation Conference
(ACM, The Netherlands, 2013), pp. 9–16
35. Y. Pei, S. Zheng, Y. Tan, H. Takagi, An empirical study on inﬂuence of approximation
approaches on enhancing ﬁreworks algorithm, in Proceedings of the 2012 IEEE Congress
on System, Man and Cybernetics (IEEE, 2012), pp. 1322–1327
36. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 IEEE Congress on
Evolutionary Computation (CEC) (IEEE, 2013), pp. 2069–2077
37. J. Li, S. Zheng, Y. Tan, Adaptive Fireworks Algorithm, in Proceedings of IEEE Congress on
Evolutionary Computation (CEC’2014) (Beijing, China, 2014), pp. 3214–3221
38. S. Zheng, A. Janecek, Y. Tan, Dynamic Search in Fireworks Algorithm, in Proceedings of IEEE
Congress on Evolutionary Computation (CEC’2014) (Beijing, China, 2014), pp. 3222–3229
39. Y.J. Zheng, X.L. Xu, H.F. Ling, A hybrid ﬁreworks optimization method with differential
evolution. Neurocomputing 148, 75–82 (2012)
40. C. Yu, L. Kelley, S. Zheng, Y. Tan, Fireworks Algorithm with Differential Mutation for Solving
the CEC 2014 Competition Problems, in Proceedings of IEEE Congress on Evolutionary
Computation (CEC’2014) (Beijing, China, 2014) pp. 3238–3245
41. H. Gao, M. Diao, Cultural ﬁrework algorithm and its application for digital ﬁlters design. Int.
J. Model. Identif. Control 14(4), 324–331 (2011)
42. W. Fang, J. Sun, W. Xu, J. Liu, FIR digital ﬁlters design based on quantum-behaved particle
swarm optimization, in 2006 IEEE First International Conference on Innovative Computing,
Information and Control (ICICIC’06) (IEEE, 2006), vol. 1, pp. 615–619
43. W. Fang, J. Sun, W.B. Xu, FIR ﬁlter design based on adaptive quantum-behaved particle swarm
optimization algorithm. Syst. Eng. Electron. 30(7), 1378–1381 (2008)
44. M. Zhang, B. Zhang, Y. Zheng, A hybrid biogeography-based optimization and ﬁreworks
algorithm, in Advances in Swarm Intelligence (Springer, Berlin, 2014), pp. 1–7
45. J. McCaffrey, Fireworks algorithm optimization, MSDN Mag. 29(12). (2014). http://msdn.
microsoft.com/en-us/magazine/dn857364.aspx

16
1
Introduction
46. Y-J. Zheng, Q. Song, S-Y. Chen, Multiobjective ﬁreworks optimization for variable-rate fertil-
ization in oil crop production. Appl. Soft Comput. 13(11), 4253–4263 (2013)
47. J. Zhang, On ﬁreworks algorithm for solving 0/1 knapsack problem. J. Wuhan Eng. Inst. 23(3),
64–66 (2011)
48. A. Janecek, Y. Tan, Swarm intelligence for non-negative matrix factorization. Intern. J. Swarm
Int. Res. (IJSIR) 2(4), 12–34 (2011)
49. W. He, G. Mi, Y. Tan, Parameter optimization of local-concentration model for spam detection
by using ﬁreworks algorithm. Advances in Swarm Intelligence (Springer, Berlin 2013), pp.
439–450
50. S. Zheng, Y. Tan, A uniﬁed distance measure scheme for orientation coding in identiﬁcation,
in 2013 IEEE Congress on Information Science and Technology (IEEE, 2013), pp. 979–985
51. Z. Zheng, Y. Tan, Group explosion strategy for searching multiple targets using swarm robotic,
in 2013 IEEE Congress on Evolutionary Computation (IEEE, 2013), pp. 821–828
52. Y. Tan, Swarm robotics: collective behavior inspired by nature. J. Comput. Sci. Syst. Biol.
(JCSB)
53. Y. Tan, Z.Y. Zheng, Research advance in swarm robotics. Def. Tech. 9(1), 31–62 (2013)
54. D.U. Zhen-xin, Fireworks algorithm for solving nonlinear equation and system. Mod. Comput.
6(2), 18–21 (2013). doi:10.3969/j.issn.1007-1423.2013.04.005
55. N. Pholdee, S. Bureerat, Comparative performance of meta-heuristic algorithms for mass min-
imisation of trusses with dynamic constraints. Adv. Eng. Softw. 75(4), 1–13 (2014). doi:10.
1016/j.advengsoft.2014.04.005
56. I.A. Mohamed, M. Kowsalya, A new power system reconﬁguration scheme for power loss
minimization and voltage proﬁle enhancement using ﬁreworks algorithm. Electr. Power Energy
Syst. 63(4), 461–472 (2014). doi:10.1016/j.ijepes.2014.04.034
57. I.A. Mohamed, M. Kowsalya, D.P. Kothari, A novel integration technique for optimal network
reconﬁguration and distributed generation placement in power distribution networks. Electr.
Power Energy Syst. 63(6), 461–472 (2014). doi:10.1016/j.ijepes.2014.06.011
58. R. Rajaram, K. Palanisamy, S. Ramasamy, P. Ramanathan, Selective harmonic elimination in
PWM inverter using ﬁreﬂy and ﬁreworks algorithm. Int. J. Innov. Res. Adv. Eng. (IJIRAE) 1(8),
55–62 (2014). doi:10.1016/j.ijepes.2014.06.011. http://www.ijirae.com/volumes/voll/issue8/
SPEE10082.08.pdf
59. A.I. Maswood, S. Wei, M.A. Rahman, A ﬂexible way to generate PWM-SHE switching patterns
using genetic algorithm. IEEE SPEC 2, 1130–1134 (2001)
60. K. Sndareswaran, K. Jayant, T.N. Shanavas, Inverter harmonic elimination through a colony
of continuously exploring ants. IEEE Trans. Ind. Electron. 54(10), 2558–2565 (2007)
61. K. Sndareswaran, V.T. Sreedevi, Inverter harmonic elimination using honey bee intelligence.
Aust. J. Electr. Electron. Eng. 6(2) (2009)
62. N.H. Abdulmajeed, M. Ayob, A ﬁrework algorithm for solving capacitated vehicle routing
problem. Int. J. Adv. Comput. Tech. (IJACT) 6(1), 79–86 (2014)
63. Y. Tan, S. Zheng, Research progress on ﬁreworks algorithm. CAAI Trans. Intell. Syst. 9(10),
1–17 (2014)
64. Y. Tan, C. Yu, S.Q. Zheng, K. Ding, Introduction to ﬁreworks algorithms. Int. J. Swarm Intell.
Res. 4(4), 39–70 (2013)
www.allitebooks.com

Chapter 2
Fireworks Algorithm (FWA)
Inspired by ﬁreworks explosions in the sky at night, the ﬁreworks algorithm (FWA)
was proposed by the author in 2010, through the observation of the fact that ﬁre-
works explosion is similar to the way an individual searches for optimal solution in
swarm intelligence algorithms. Recently, FWA has received extensive concerns from
many active researchers in the swarm intelligence community. This chapter presents
the fundamental principle, main constitution, implementation, and performance of
the FWA, aiming to elaborate the FWA systematically and completely. The main
contents include the key components, realization, characteristic, and impact of oper-
ations of FWA, as well as comparisons with genetic algorithm and particle swarm
optimization.
2.1 Introduction
SettingoffﬁreworksisanimportantcreativeandjoyfulactivityduringSpringFestival
inChina.Atthistime,tensofthousandsofﬁreworksexplodeinthenightskyandshow
beautiful patterns of sparks. Usually, ﬁreworks of different prices and speciﬁcations
produce entirely different patterns. For example, ﬁreworks of lower price produce
less sparks with larger amplitude compared with higher price ﬁreworks and vice
versa.
The way ﬁreworks explode is similar to the way an individual searches the optimal
solution in swarm intelligence algorithms. As a swarm intelligence algorithm, ﬁre-
works algorithm consists of four parts, i.e., the explosion operator, mutation operator,
mapping rule and selection strategy. The effect of the explosion operator is to generate
sparks around ﬁreworks. The number and amplitude of the sparks are governed by the
explosion operator. After that, some sparks are produced by mutation operator. The
mutation operator utilizes Gaussian operator to produce sparks in Gaussian distribu-
tion. Under the effect of the two operators, if the produced spark is not in the feasible
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_2
17

18
2
Fireworks Algorithm (FWA)
region, the mapping rule will map the new generated sparks into the feasible region.
To select the sparks for next generation, the selection strategy is used. Fireworks
algorithm runs iteratively until it reaches the termination conditions [1].
2.2 FWA Principle
2.2.1 Explosion Operator
In initialization, FWA generates N ﬁreworks randomly. Then the N ﬁreworks gen-
erate sparks by explosion operations. The explosion operator is a key in FWA and
plays an important role. The explosion operator include explosion strength, explosion
amplitude and displacement operation [1].
2.2.1.1 Explosion Strength
The explosion strength is a core operation in explosion operator. It simulates the way
of explosion of ﬁreworks in real life. When a ﬁrework blasts, the ﬁrework vanishes
in one second and then many small bursts appear around it. Fireworks algorithm ﬁrst
determines the number of sparks, then calculates the amplitudes of each explosion.
Through the observations on the curves of some typical optimization functions, it
can be seen that there are more points with good ﬁtness values around the optima than
those away from the optima. Therefore, ﬁreworks with better ﬁtness values produce
more sparks, avoiding swing around the optima but fail to locate it. For ﬁreworks
with worse ﬁtness values, their generated sparks are less in number and sparse in dis-
tribution, avoiding unnecessary computing. Fireworks with worse ﬁtness values are
used to explore the feasible space, preventing the algorithm from premature conver-
gence. Fireworks algorithm determines the number and amplitude of the ﬁreworks
according to their ﬁtness values, letting ﬁreworks with better ﬁtness values produce
more sparks within a smaller amplitude and vice versa as shown in Fig.2.1.
It can be seen from Fig.2.1 that ﬁreworks with better ﬁtness values produce more
sparks within a smaller amplitude (good explosion) than those with worse ﬁtness
values within a larger amplitude (bad explosion). After determining the number of
sparks, it is needed to calculate the amplitude of the sparks in the explosion of a
ﬁrework.
2.2.1.2 Explosion Amplitude
Through observation on the curves of some typical optimization functions, the points
around the local optima and global optima always have better ﬁtness values. There-
fore, by controlling the explosion amplitude, the amplitude of ﬁreworks with better

2.2 FWA Principle
19
Fig. 2.1 The good and bad explosions of a ﬁrework. Reprinted from Ref. [1], with kind permission
from Springer Science+Business Media. a Good explosion. b Bad explosion
ﬁtness values gradually reduce, leading ﬁreworks algorithm to ﬁnd the local and
global optima. By contrast, ﬁreworks with worse ﬁtness values explore the optima
through a large amplitude. This is how the FWA controls the magnitude of the explo-
sion amplitude.
2.2.1.3 Displacement Operation
After the calculation of explosion amplitude, it is necessary to determine the displace-
ment within the explosion amplitude. FWA uses the random displacement. In this
way, each ﬁrework has its own speciﬁc explosion number and amplitude of sparks.
FWA generates different random displacements within each amplitude to ensure the
diversity of population. Through the explosion operator, each ﬁrework generates a
shower of sparks, helping to ﬁnd the global optimal of an optimization function.
2.2.2 Gaussian Mutation Operator
To further improve the diversity of a population, the Gaussian mutation is introduced
into FWA. The way of producing sparks by Gaussian mutation is as follows: choose
a ﬁrework from the current population, then apply Gaussian mutation to the ﬁrework
in randomly selected dimensions.
For Gaussian mutation, the new sparks are generated between the best ﬁrework
and the selected ﬁreworks (Fig.2.2). Yet, Gaussian mutation may produce sparks that
exceed the feasible space. When a spark lies beyond the upper or lower boundary,
the mapping rule will be carried out to map the spark to a new location within the
feasible space.

20
2
Fireworks Algorithm (FWA)
Fig. 2.2 Gaussian mutation.
Reprinted from Ref. [1], with
kind permission from
Springer Science+Business
Media
2.2.3 Mapping Rule
If a ﬁrework is near the boundary of the feasible space, while its explosion amplitude
covers both the feasible and infeasible space, the generated sparks may lie out of the
feasible space. As such, the spark beyond the feasible space is useless. Therefore,
it needs to be getting back into the feasible space. The mapping rule is used to deal
with this situation. The mapping rule ensures that all sparks are in the feasible space.
If there is any spark that is generated by a ﬁrework beyond the feasible space, it will
be mapped back to the feasible space.
2.2.4 Selection Strategy
After applying the explosion operator, the mutation operator, and the mapping rule,
some of the generated sparks need to be selected and passed down to the next genera-
tion. The distance-based strategy is used in the ﬁreworks algorithm. In order to select
the sparks for next generation, ﬁrst, the best spark is always kept for next generation.
Then, the other (N −1) individuals are selected based on distance maintaining the
diversity of the population. The individual that is farther from the other individuals
has greater chance to be selected than those individuals near the other individuals.
2.3 Implementation of FWA
FWA starts to run iteratively till the given termination conditions are met. It consists
of the explosion operator, the mutation operator, the mapping rule, and the selec-
tion strategy. There are two termination conditions, such as meeting the accuracy
requirements and reaching the maximum number of function evaluations.

2.3 Implementation of FWA
21
The realization of FWA consists of four steps as follows.
(1) Randomly generate ﬁreworks in the feasible space.
(2) Calculate the ﬁtness value of each ﬁrework according to the ﬁtness function.
The number of sparks is calculated based on immune concentration theory in
immunology and the ﬁreworks with better ﬁtness values produce more sparks.
(3) Considering the ﬁreworks phenomena in reality and the landscape of the func-
tions, the ﬁreworks generate sparks within a certain amplitude in FWA. The
explosion amplitude is determined by the ﬁtness value of that ﬁrework. The
explosion amplitude for the ﬁrework with better ﬁtness value is smaller and vice
versa. Each spark represents a solution in the feasible space. To keep the diversity
of the population, mutation operation is needed and Gaussian mutation is one of
them.
(4) Calculate the best ﬁtness value. If the terminal condition is met, stop the algo-
rithm. Otherwise, continue the iteration process. The best spark and the selected
sparks formed a new population.
2.3.1 Explosion Operator
2.3.1.1 Explosion Strength
In the explosion strength, i.e. the number of sparks is determined as follows:
Si = m ∗
Ymax −f (xi) + ε
N
i=1
(Ymax −f (xi)) + ε
,
(2.1)
where Si is the number of sparks for each individual or ﬁrework, m is a constant
standing for the total number of sparks, and Ymax means the ﬁtness value of the worst
individual among the N individuals in the population. Function f (xi) represents
the ﬁtness for an individual xi, while the last parameter ε is used to prevent the
denominator from becoming zero.
The limitation of number of sparks are as follows:
ˆsi =
⎧
⎨
⎩
round(a · m), if si < am
round(b · m), if si > bm, a < b < 1
round(a · m), otherwise
,
(2.2)
where a and b are constant, ˆsi is the limitation of the number of sparks, and round()
is the rounding function.

22
2
Fireworks Algorithm (FWA)
2.3.1.2 Explosion Amplitude
The explosion amplitude is deﬁned below.
Ai = ˆA ∗
f (xi) −Ymin + ε
N
i=1
(f (xi) −Ymin) + ε
,
(2.3)
where Ai denotes the amplitude of each individual, ˆA is a constant as the sum of
all amplitudes, while Ymin means the ﬁtness value of the best individual among
the N individuals. The meaning of function f (xi) and parameter ε are the same as
aforementioned in Eq.(2.2).
2.3.1.3 Displacement Operation
Displacement operation is to make displacement on each dimension of a ﬁrework
and can be deﬁned as
xk
i = xk
i + U(−Ai, Ai),
(2.4)
where U(−Ai, Ai) denotes the uniform random number within the intervals of the
amplitude Ai.
Algorithm2.1 is the pseudo code of the explosion operator described in Eqs.(2.1)–
(2.4).
Algorithm 2.1 Generate sparks
1: Initialization, calculate the ﬁtness value f (xi) for each ﬁrework.
2: Calculate the number of sparks Si.
3: Calculate the amplitude of sparks Ai.
4: z = rand(1, dimension) //randomly choose z dimensions
5: for k = 1 →dimension do
6:
if k ∈z then
7:
xk
i = xk
i + U(−Ai, Ai)
8:
end if
9: end for
2.3.2 Mutation Operator
Suppose the position of current individual is stated as xk
i , where i varies from 1
to N and k denotes the current dimension. The sparks of Gaussian explosion are
calculated by
xk
i = xk
i ∗g,
(2.5)

2.3 Implementation of FWA
23
where g is a random number in Gaussian distribution with mean 1 and variance 1
such as
g = N(1, 1).
(2.6)
Algorithm2.2 shows the pseudo code for Gaussian mutation.
Algorithm 2.2 Gaussian Mutation
1: Calculate the ﬁtness value f (xi) for each ﬁrework.
2: Calculate the coefﬁcient g = N(1, 1).
3: z = rand(1, dimension) //randomly select z dimensions
4: for k = 1 →dimension do
5:
if k ∈z then
6:
xk
i = xk
i ∗g
7:
end if
8: end for
2.3.3 Mapping Rule
The mapping rule ensures all the individuals stay in the feasible space. If there are
some outlying sparks from the boundary, they will be mapped back to their allowable
scopes.
The mapping rule utilizes a modular operation and is stated as follows:
xk
i = XLB,k + xk
i %(XUB,k −XLB,k),
(2.7)
where xk
i represents the positions of any sparks that lie out of bounds, while XUB,k
and XLB,k stand for the maximum and minimum boundaries of a spark position. The
sign % represents modular arithmetic.
2.3.4 Selection Strategy
In selection strategy, the measurement of Euclidean distance is used, where d

xi, xj

denotes the Euclidean distance between any two individuals xi and xj.
R(xi) =
K

j=1
d(xi, xj) =
K

j=1
		xi −xj
		,
(2.8)
where R(xi) represents the sum of distances between individual xi and all the other
individuals. j ∈K means the position j belongs to set K, where K is the set of

24
2
Fireworks Algorithm (FWA)
Fig. 2.3 The ﬂowchart of FWA
combining both the sparks generated by explosion operator and mutation operator.
The roulette way is used to choose individuals for next generation, as the possibility
for choosing the individual xi should be p(xi), which is given by
p (xi) =
R (xi)

j∈K R

xj
.
(2.9)
From Eq.(2.9), it can be seen that individuals with larger distances will have
more chances to be selected for next generation. In such a way, the diversity of the
population can be guaranteed.
The ﬂowchart of FWA is depicted in Fig.2.3.
The pseudo code of FWA is shown in Algorithm2.3.

2.3 Implementation of FWA
25
Algorithm 2.3 Pseudo code of FWA
1: Randomly select N locations for ﬁreworks
2: while terminal condition is not met do
3:
Set off N ﬁreworks, respectively, at the N locations:
4:
for all ﬁreworks xi do
5:
Calculate the number of sparks as Si
6:
Calculate the amplitude of sparks as Ai
7:
end for
// ˆm is the number of sparks generated by Gaussian mutation
8:
for k = 1 →ˆm do
9:
Randomly select a ﬁrework xi and generate a spark
10:
end for
11:
select the best spark and the other sparks according to selection strategy
12: end while
2.4 The Characteristics of FWA
FWA contains the following characteristics: explosion, instantaneity, simplicity,
locality, emergent property, distribute parallelism, diversity and extendibility. The
details are given below.
2.4.1 Explosion
After the ﬁrst iteration of FWA, the ﬁreworks explode within the amplitude and
produce a shower of sparks. At the end of the iteration, the algorithm selects N
sparks for the next generation. The N selected sparks are treated like new ﬁreworks,
preparing for explosion in the next iteration. In each iteration, the ﬁreworks will
explode, indicating the explosive characteristic of FWA.
2.4.2 Instantaneity
In each iteration, FWA calculates the number of sparks and the explosion amplitude,
depending on the ﬁtness values of the ﬁreworks. Then, the sparks are produced by the
explosion and mutation operators. Finally, the best spark is preserved at ﬁrst and then
the other (N −1) sparks are selected based on a selection strategy. The selected N
sparks are treated as the ﬁreworks for the next generation, while the rest of the sparks
are no longer reserved. Sparks or ﬁreworks are not kept, indicating the instantaneous
characteristic of FWA.

26
2
Fireworks Algorithm (FWA)
2.4.3 Simplicity
Like other swarm intelligence algorithms, each ﬁrework in FWA only percepts its
own information itself and its surrounding information, following simple rules to
complete their missions. Overall, FWA is not complex, composed of simple individ-
uals. Therefore, FWA is characteristic of simplicity.
2.4.4 Locality
In FWA, all the ﬁreworks generate sparks within their amplitudes. Unless beyond
the feasible region, sparks are conﬁned within a certain range. Localized features of
FWA reﬂect the powerful local search capabilities, as the algorithm can be used for
local search in the latter of the search process. Therefore, FWA contains locality.
2.4.5 Emergent Property
Fireworks are in competition and collaboration with each other and the group showed
high degree of intelligence which a simple individual cannot achieve. Interaction
between ﬁreworks are more complicated than a single individual’s behavior. There-
fore, ﬁrework algorithm has the characteristic of emergent.
2.4.6 Distributed Parallelism
In each iteration of FWA, each ﬁrework explodes and searches within different space,
i.e., each ﬁrework conducts a search in different dimensions. Finally, the sparks and
ﬁreworks are combined together to choose N ﬁreworks for the next generation. In
each iteration, FWA searches the space in parallel, showing the characteristic of
distribution parallelism.
2.4.7 Diversity
Population diversity is vital to the performance of any swarm intelligence algorithm.
By maintaining the population diversity, the algorithm can jump out of local optima,
which makes the algorithm converge to the global optimal point, which a generic
optimization can hardly achieve. Therefore, swarm optimization algorithms are dif-
ferent from any generic optimization algorithm. The better the population diversity
www.allitebooks.com

2.4 The Characteristics of FWA
27
is, the wider the individuals are distributed. The optimal value might be easier to be
found if a population is strongly diverse, as the convergence of the algorithm will
not be affected signiﬁcantly. Thus, population diversity is an important part of the
FWA. The diversities of FWA can be concluded as follows.
2.4.7.1 The Diversity of the Number of Sparks and Explosion Amplitude
According to the explosion operator and the ﬁtness values, each ﬁrework generates a
different number of sparks within a different magnitude. The ﬁreworks with higher
ﬁtness values produce more sparks within smaller ranges, while the ﬁreworks with
lower ﬁtness values produce fewer sparks within larger ranges. In such a way, the
diversity of the population would be guaranteed.
2.4.7.2 The Diversity of Displacement and Gaussian Mutation
FWA has two operators, explosion operator and mutation operator. In the explosion
operator, a displacement is calculated according to an amplitude. The displacement
is added to a position in a dimension of a selected ﬁrework. In the Gaussian mutation
operator, the selected ﬁreworks need to multiply a Gaussian random number in the
position of a dimension. The explosion operator is relative to the ﬁtness values of
ﬁreworks while the mutation operator is relative to the position of the ﬁreworks. The
two operators are different from each other, but both of them guarantee the explosion
to be diverse.
2.4.7.3 The Diversity of Fireworks
Through a certain selection mechanism, the coordinates of the retained ﬁreworks
are different. As a result, these phenomena ensure the diversity of the population. In
addition, in the selection strategy, sparks with a greater distance from the other sparks
are more likely to be selected, which in turn, ensures the diversity of population.
2.4.8 Extendibility
In FWA, the number of sparks are uncertain and able to be determined based on
the complexity of the problem in hand. The number of ﬁreworks and sparks can be
more or less, as both increase and decrease of the individuals can effectively solve
the problem. Therefore, FWA has extendibility.

28
2
Fireworks Algorithm (FWA)
2.4.9 Adaptability
When solving problems using FWA, it is unnecessary for the problem to be of an
explicit expression. The problem can be solved by calculating the ﬁtness values only.
Meanwhile, FWA can also solve the problems with explicit expressions, indicating
its capability. Therefore, FWA is of adaptability and can be regarded as an adaptive
algorithm.
2.5 Impact of Operators in FWA on Performance
2.5.1 Explosion Operator
When a spark explodes, the area around the spark is searched. If the ﬁtness value of a
spark is higher, the amplitude of the spark is larger and the number of small bursts of
the spark is fewer. In this case, the sparks with better ﬁtness values will search more
carefully in smaller areas, while the sparks with worse ﬁtness values will search in
wider areas. Therefore, FWA has greater chances to ﬁnd the global optimal in limited
function evaluations.
The explosion operator has two parameters. The ﬁrst parameter m is used to limit
the total number of sparks and the second parameter N is the number of ﬁreworks.
Function Generalized Rosenbrock is used to illustrate the impact of the explosion
operator on the performance of FWA.
Figure2.4 gives the impact of the total number of sparks m on the performance
of FWA on function Generalized Rosenbrock, under the circumstance that the other
parameters remain unchanged.
Experimental results show that better performances are obtained when the total
number of sparks m is set between 10 and 50, while the other parameters remained
unchanged.
Fig. 2.4 The impact of
different number of sparks
on the performance of FWA
for Generalized Rosenbrock
function. The vertical axis
represents the accuracy of
the experimental results

2.5 Impact of Operators in FWA on Performance
29
Fig. 2.5 The impact of the
number of ﬁreworks on the
performance of FWA for
Generalized Rosenbrock
function. The vertical axis
represents the accuracy of
the experimental results
Figure2.5 gives the impact of the total number of ﬁreworks N on the performance
of FWA on Generalized Rosenbrock function.
It can be seen from Fig.2.5 that better performances are obtained when the total
number of ﬁreworks N is set as 3 or 5, while the other parameters remain unchanged.
Obviously, for different optimization problems, setting different values of para-
meters can have a certain impact on the performance of the algorithm. Better per-
formances on function Generalized Rosenbrock are obtained when the total number
of sparks is set between 20 and 40, and 3 or 5 ﬁreworks if other parameters remain
unchanged.
2.5.2 Gaussian Mutation
Gaussian mutation operator can increase the diversity of the algorithm because the
sparks generated by Gaussian mutation are not limited to the area around the ﬁre-
works. In addition, since the sparks are generated between the current location and
the origin by Gaussian mutation (xk
i <= xk
i ∗g), the performance is pretty good for
functions having the optimal at the origin. For example, FWA with Gaussian muta-
tion can easily ﬁnd the location of the optimal value on function Sphere because the
optimal value of function Sphere is at the origin.
Table2.1 gives the experimental results of FWA on two functions. The functions
are 30-dimensional and the algorithm iterates 300,000 times.
Table 2.1 FWA with and without Gaussian mutation on function Sphere and Generalized
Rosenbrock
Status
Sphere
Generalized Rosenbrock
FWA with Gaussian mutation
0
25.209447
FWA without Gaussian mutation
1.095037
706.936069

30
2
Fireworks Algorithm (FWA)
It can be seen from Table2.1 that the performances of FWA on both functions
are greatly improved by Gaussian mutation operator. This is due to the diversity of
the population increased by Gaussian mutation. The global search ability of FWA is
enhanced.
2.5.3 Mapping Rule
The mapping rule is used to ensure that all the sparks are generated within the scope
of the feasible space. When a ﬁrework is near the border and the amplitude of the
explosion is large, the generated sparks might be out of the boundaries of the feasible
space.Therefore,thesparksthatareoutoftheboundarieswillbemappedintofeasible
space for avoiding unnecessary computation. However, the mapping rule has its own
disadvantages. For instance, it can easily pull a spark to the locations near the origin,
beneﬁting the functions with optima near the origin.
The mapping rule adopts a modular arithmetic to ensure that the out-of-range
sparks are pulled back into the feasible space.
As shown in Fig.2.6, the feasible space is set from −100 to 100 and the limitation
of explosion amplitude is set as 40. Therefore, the points beyond the boundaries will
fall into the area of −140∼−100 and 100∼140 shown as shadow areas in Fig.2.6.
According to the mapping rule, points located in these two areas are mapped into the
range from 0 to 40, which is near the origin.
2.5.4 Selection Strategy
The selection strategy is to choose individuals for the next generation. The best
individual is always kept for the next generation, while the remaining (N −1) indi-
viduals are selected based on Euclidean distance and the sparks further away from
other sparks can be selected with larger possibilities. Thus, the diversity of FWA is
guaranteed in such a way.
Fig. 2.6 The operation of
mapping rule

2.6 Comparison of FWA with Three Other SI Algorithms
31
2.6 Comparison of FWA with Three Other SI Algorithms
2.6.1 Ideas Comparison Between FWA and GA
The idea of immune density is introduced into the selection strategy of FWA, so it
is unnecessary to design a new selection operator. Selection strategy in FWA sounds
like the selection operator in genetic algorithm, but they are different. Based on
the idea of immune density, each spark in FWA is treat as an antibody in immune
system. A spark (antibody) which has more similar sparks (antibodies) is chosen
with a lower probability. On the contrary, a spark (antibody) which has less similar
sparks (antibodies) is selected with a higher chance. Hence, the sparks (antibodies)
with lower ﬁtness values have the chance to be selected such that the diversity of the
sparks (antibodies) is ensured. Compared with the genetic algorithm, the selection
operator determines which individual is to be selected by the ﬁtness values of the
individuals. The selection is based on the roulette and the diversity of the population
is not guaranteed.
Genetic algorithm was originally proposed by Prof. Holland of the University of
Michigan [2]. At that time, Holland recognized that biological hereditary and natural
evolution phenomena are similar to artiﬁcial adaptive systems. The idea of genetic
and evolutionary in nature could be used to study the generation of natural, artiﬁcial
adaptive systems and their relationship with the environment. He suggested to use
the mechanism of genetic in study and design of artiﬁcial adaptive systems, as the
swarms could be used for adaptive search and the crossover and mutation operations
are vital.
The common aspects of both algorithms are as follows:
(1) Randomly initialize the initial population;
(2) Calculate the ﬁtness value of each individual;
(3) A series of operations to be performed according to ﬁtness values, such as selec-
tion operator, crossover operator, and mutation operator in genetic algorithm,
and explosion operator and mutation operator in FWA;
(4) Select the individuals for the next generation according to the ﬁtness values;
(5) Stop when the termination conditions are met. Otherwise, go to step 2.
From the above steps, we can see that the FWA and the genetic algorithms have
a lot in common. Both randomly initialize a population, evaluate the individuals
according to their ﬁtness values and perform a certain random search. In addition,
both algorithms are not guaranteed to ﬁnd the optimal values.
There is no crossover operator in the FWA and the mutation operator in the FWA
is totally different from that in the genetic algorithm.
Compared with the genetic algorithm, information sharing mechanisms in FWA is
quite different. In the genetic algorithm, chromosomes share information with each
other so the entire population moves relatively homogeneously in the feasible space.
However, in the FWA, a distributed information sharing mechanism is used, while
the number of sparks and the explosion amplitudes are determined by the ﬁtness

32
2
Fireworks Algorithm (FWA)
values of ﬁreworks which are located in different areas. In addition, the ﬁreworks are
always selected from different areas and can hardly stay together due to the immune-
based selection. Yet, the FWA has more mechanisms to avoid premature compared
to genetic algorithm.
2.6.2 Ideas Comparison Between FWA and Two Versions
of PSO
Two particle swarm optimization algorithms are introduced at ﬁrst, i.e., clonal par-
ticle swarm optimization (CPSO) [3] and standard particle swarm optimization
(SPSO) [4].
In the biological immune system, when the antigen gets into a living body, the
immune system in the body can identify and eliminate the antigen. This process is
mainly by cloning that activates antibodies, increasing their number and clearing the
antigens [5]. Based on realizing the importance of the immune response, Tan and
Xiao [3] made improvements to the standard PSO algorithm by proposing a cloning
operator.
The process of the three algorithms is similar as explained below.
(1) Initialization. FWA initializes the ﬁreworks, while the two kinds of PSO initialize
particles.
(2) Calculate the ﬁtness values for the individuals in Step 1.
(3) Process necessary operations. FWA processes the explosion and mutation oper-
ations, while the two kinds of PSO processes update pbest and gbest. Also, the
position and speed for each particle need to be updated in each generation.
(4) Select individuals for the next generation.
(5) If the termination condition is met, the algorithm is terminated. Otherwise, go
to Step 2.
From the above steps, we can see the FWA and the two kinds of PSO have much in
common. They adopt random initial populations, evaluate the functions and perform
the search based on the ﬁtness values. Also, all the algorithms are not guaranteed to
ﬁnd the optimal solution.
However, there is no mutation operation in SPSO, while there is Gaussian muta-
tion in CPSO and both displacement operation and Gaussian mutation in FWA.
Furthermore, the Gaussian mutation in FWA plays a role on differential dimensions
with the same displacement, making connections between the different dimensions.
The displacement in CPSO differs in each dimension. Besides, Gaussian mutation
takes place in each iteration in FWA, but runs once in several iterations in CPSO.
Compared with the two kinds of PSO, information sharing mechanism in FWA
is different. In the two kinds of PSO, only gbest gives information to other particles,
which is one way of information delivery, as the search process follows the informa-
tion about the best particle. FWA, on the other hand, uses a distributed information
sharing mechanism, so as to determine the number of sparks and explosion amplitude

2.6 Comparison of FWA with Three Other SI Algorithms
33
by the ﬁtness values of each spark in different regions. It also needs to maintain the
best ﬁrework throughout the iterative process.
Besides, FWA utilizes the idea of immune concentration to keep the diversity of
the population, whereas the idea is not contained in SPSO.
2.7 Experimental Results and Analysis
2.7.1 Benchmark Functions
FWA [1] and SPSO [4] and CPSO [3] are compared on six test functions. The details
of the six test functions can be seen in Appendix A.
2.7.2 Parameters Setting
• The population size is set as 5 and the spark of Gaussian mutation is set as 5.
• The total number of sparks is set as 50. Parameter a and b are set as 0.8 and 0.04.
• The constant ˆA is set as 40. There is no lower boundary for the amplitude of
explosion.
• Function is 30 dimensions, running 20 times and the number of function evalua-
tions is limited to 400,000.
2.7.3 Experimental Results
The experimental results are shown in Table2.2 when the functions are evaluated for
400,000 times (the accuracy is 10−6).
The convergence curves are also shown in Fig.2.7.
Table 2.2 Comparison of FWA with CPSO and SPSO (lower than 10−6 is treated as zero)
FWA
CPSO
SPSO
Mean
Std
Mean
Std
Mean
Std
Sphere
0
0
0
0
367.1166
186.7949
Rosenbrock
12.16293
12.82113
66.58722
204.2907
5692076
4087432
Griewank
0
0
0.003693
0.011792
1.088648
0.042218
Rastrigin
0
0
6.769299
7.701368
676.1549
197.9695
Rotated
Griewank
0
0
0.043401
0.042286
0.920613
0.088088
Rotated
Rastrigin
0
0
23.92579
13.6093
339.2073
62.38145

34
2
Fireworks Algorithm (FWA)
Fig. 2.7 Convergence curves of FWA, CPSO and SPSO. Reprinted from Ref. [1], with kind per-
mission from Springer Science+Business Media
2.7.4 Analysis
It can be seen from the experimental results that FWA works signiﬁcantly better than
SPSO and CPSO [3] in both convergence speed and accuracy on the six test functions.
Hence, FWA has good convergence and result accuracy and can be successfully
appliedtofunctionoptimizationproblems.InthecomparisonexperimentswithSPSO
and CPSO, FWA reveals good advantage, which means FWA is very successful and
has good prospects. In addition, since a lot of practical engineering problems can

2.7 Experimental Results and Analysis
35
be transformed into the function optimization problem and FWA can solve function
optimization problems effectively, FWA has good prospects.
However, studies on FWA is still in its infancy, while current FWA still have some
problems. Further research can be concluded as follows.
(1) The main operators of FWA are realized, but they are still not perfect. For
instance, Gaussian mutation achieves good performance, but it cannot solve all
the problems effectively. New mechanism needs to be added to improve FWA.
(2) Parameters of the algorithm are set according to empirical experiments on simple
functions. The experiments are not enough and there is no theoretical analysis
for the settings. The future research is to ﬁnd reasonable parameters for each
problem.
(3) The function optimization experiments are conducted on a few benchmark func-
tions. Hence, the experimental results are not convincing in some aspects. More
optimizationfunctionsareneededtomakethetestsystematicandcomprehensive.
(4) FWA can be applied to many areas. For example, neural networks, fuzzy systems
control and fuzzy rule learning. Yet, FWA can be applied to solving discrete
issues.
2.8 Summary
This chapter described FWA in detail, including explosion operator, mutation oper-
ator, mapping rule and selection strategy. The ﬂowchart and pseudo code of FWA
were also given while the characteristics of FWA and the impact of various factors on
the performance of FWA were analyzed deeply. Besides, GA, PSO, and FWA were
compared. Experimental results show that, FWA had better performance compared
to GA and PSO.
FWA and its applications prove that it can solve many complex optimization
problems effectively. Furthermore, FWA can be parallelized and thus is suitable for
dealing with big data problems. Whether for theoretical or applied researches, FWA
is worth researching and might bring great scientiﬁc and economic beneﬁts for us.
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. J.H. Holland. Adaptation in Natural and Artiﬁcial Systems: An Introductory Analysis with Appli-
cations to Biology, Control, and Artiﬁcial Intelligence. (U Michigan Press, 1975)
3. Y. Tan, Z.M. Xiao. Clonal particle swarm optimization and its applications, in IEEE Congress
on Evolutionary Computation, CEC 2007, (2007), pp. 2303–2309
4. J. Kennedy, R. Eberhart et al., Particle swarm optimization, in Proceedings of IEEE International
Conference on Neural Networks, vol. 4(2) (Perth, Australia, 1995), pp. 1942–1948
5. R. Liu, H. Du, L. Jiao, Immunity poly-clonal strategy. J. Comput. Res. Dev. 41(4), 571–576
(2004)

Chapter 3
Modeling and Theoretical Analysis of FWA
In order to describe the convergence analysis of FWA, a Markov stochastic process
modeling Fireworks Algorithm has been deﬁned and established, in the ﬁrst part
of this chapter, then to prove the global convergence of FWA and analyze the time
complexity of FWA based on an absorbing Markov stochastic process of FWA. After
that, the computation of the approximation region of expected convergence time of
Fireworks Algorithm has also been given through a detailed derivation procedure. In
the second part of this chapter, we will present 13 commonly used random number
generators (RNGs) and also try to discuss the impact of the RNGs on the performance
of FWA.
3.1 A Stochastic Process Model for FWA
Assume that the FWA [1] search is undertaken for the essential inﬁmum, which can
be deﬁned as Eq.(3.1).
ψ = inf (t : ν[n ∈S| f (z) < t] > 0),
(3.1)
where ν[A] is the Lebesgue measure on the set A.
Equation(3.1) means that there must be more than one point in a subset of search
space yielding functional values arbitrarily close to ψ, so that ψ is the inﬁmum of
the functional values from nonzero Lebesgue measurable set, and then the stochastic
process of Fireworks Algorithm is established as follows:
Deﬁnition 3.1 {ξ(t)}∞
t=0
is named as the stochastic process of Fireworks
Algorithm, where ξ(t) = {F(t), T (t)} which F(t) = {F1(t), F2(t), . . . , Fn(t)}
denotes the position of n ﬁreworks in the problem space in the step t; And
T (t) = {A(t), S(t)}, A(t) = {A1(t), A2(t), . . . , An(t)} denotes the explosion
amplitude of n ﬁreworks, S(t) = {s1(t), s2(t), . . . , sn(t)} denotes the explosion
number of n ﬁreworks.
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_3
37
www.allitebooks.com

38
3
Modeling and Theoretical Analysis of FWA
Now, an optimality region can be deﬁned as
Deﬁnition 3.2 Rε = {x ∈S| f (x) −f (x∗) < ε, ε > 0} is named as the optimal
region of function f (x), where x∗represents the optimal solution of function f (x)
in the problem space.
In terms of Deﬁnition 3.2, if the algorithm ﬁnds a point in the optimal region, an
acceptable approximation to the global minimum of the function has been acquired
by the algorithm. According to the above deﬁnition of inﬁmum, ψ, the Lebesgue
measure of optimal solution space must be nonzero, which means that v(Rε) > 0 is
held.
Deﬁnition 3.3 The optimal state of FWA is deﬁned as ξ∗(t) = {F∗(t), T (t)}, where
there exist Fi(t) ∈Rε and Fi(t) ∈F∗(t), i ∈1, 2, . . . , n.
The Deﬁnition 3.3 means that the best ﬁrework in the optimal state ξ∗(t) of FWA
is in the optimal region Rε. So here exist Fi(t) ∈R and | f (Fi(t)) −f (x∗)| < ε,
x∗∈Rε.
Lemma 3.1 The stochastic process of FWA, v{ξ(t)}∞
t=0, is a Markov stochastic
process.
Proof Assume that {ξ(t)}∞
t=00 is a stochastic process of discrete times.
Because the state ξ(t) = {F(t), T (t)} is decided by {F(t −1), T (t −1)}, so one
have the following probability:
P{ξ(t + 1)|ξ(1), ξ(2), . . . , ξ(t)} = P{ξ(t + 1)|ξ(t)}.
The above equation means that the probability of (t + 1)th state occurring is only
related to the probability of tth state occurring.
Therefore, the {ξ(t)}∞
t=0 is a Markov stochastic process.
□
Deﬁnition 3.4 (optimal state space) Given Y is represented as the state space of
FWA’s state ξ(t) and Y ∗⊂Y. Y ∗is named as the optimal state space if there exists
a solution s∗∈F∗such that s∗∈Rε for any state ξ(t)∗= {F∗, T } ∈Y.
In terms of the above deﬁnition, it means that | f (s∗) −f (x∗)| < ε for any
x∗∈F∗. If the state of the Fireworks Algorithm can arrive at the optimal state,
there exists a ﬁrework in the ﬁreworks which stays in the optimal region Rε and
the optimal solution of problem has been acquired by the FWA. After this time, the
optimal solution must be in the optimal region forever.
Deﬁnition 3.5 Given a Markov stochastic process {ξ(t)}∞
t=0 and optimal state space
Y ∗⊂Y, if {ξ(t)}∞
t=0 s.t. P{ξ(t + 1) /∈Y ∗|ξ(t) ∈Y ∗} = 0, {ξ(t)}∞
t=0 is called as an
absorbing Markov process.

3.1 A Stochastic Process Model for FWA
39
Lemma 3.2 The stochastic process of FWA, {ξ(t)}∞
t=0, is an absorbing Markov
stochastic process.
Proof First of all, according to Lemma 3.2, the stochastic process of FWA, {ξ(t)}∞
t=0,
is a Markov stochastic process.
Then, if F1(t) ∈F(t) stays in the optimal solution space Rε, the state ξ(t) =
{F(t), T (t)} must belong to the optimal state space Y ∗.
Because F1(t) is the best location in all ﬁreworks of FWA, f (F1(t + 1)) is not
worse than f (F1(t)) in terms of the step 6 of FWA. So, the state ξ(t +1) must belong
to the optimal state space Y ∗.
Therefore, P{ξ(t + 1) /∈Y ∗|ξ(t) ∈Y ∗} = 0, the stochastic process of FWA,
{ξ(t)}∞
t=0, is an absorbing Markov process.
□
3.2 Global Convergence Theorems
In this subsection, the deﬁnition of convergence is given, which is used to analyze
the convergence of Fireworks Algorithm based on the above model and its analysis.
Deﬁnition 3.6 (convergence) Given an absorbing Markov process {ξ(t)}∞
t=0 =
{F(t), T (t)} and an optimal state space Y ∗⊂Y, λ(t) = P{ξ(t) ∈Y ∗} denotes
the probability that the stochastic state arrives at the optimal state in step t, if
limt→∞λ(t) = 1, {ξ(t)}∞
t=0 is convergence.
In terms of the above deﬁnition, the convergence of Markov stochastic process
depends on the probability of P{ξ(t) ∈Y ∗}. If it converges to 1 with the time t, the
Markov process {ξ(t)}∞
t=0 will be convergence. So we have the following theorem:
Theorem 3.1 Given an absorbing Markov process {ξ(t)}∞
t=0 of FWA and optimal
state space Y ∗⊂Y. If P{ξ(t)∈Y ∗|ξ(t −1) /∈Y ∗} ≥d ≥0 for any t and P{ξ(t) ∈
Y ∗|ξ(t −1) ∈Y ∗} = 1, then P{ξ(t) ∈Y ∗} ≥1 −(1 −d)t.
Proof Let t = 1,
P{ξ(1) ∈Y ∗} = P{ξ(1) ∈Y ∗|ξ(0) ∈Y ∗} · P{ξ(0) ∈Y∗}
+ P{ξ(1) ∈Y ∗|ξ(0) /∈Y ∗} · P{ξ(0) /∈Y ∗}
≥P{ξ(0) ∈Y ∗} + d · P{ξ(0) /∈Y ∗}
= P{ξ(0) ∈Y ∗} + d · (1 −P{ξ(0) ∈Y ∗})
= d + (1 −d) · P{ξ(0) ∈Y ∗}.

40
3
Modeling and Theoretical Analysis of FWA
Because (1 −d) ≥0, so d + (1 −d) · P{ξ(0) ∈Y ∗} ≥d, then P{ξ(1) ∈Y ∗} ≥
d = 1 −(1 −d)1; Now, it is assumed that the P{ξ(t) ∈Y ∗} ≥1 −(1 −d)t is held
for any t < k −1, and then for t = k, one has
P{ξ(k) ∈Y ∗} = P{ξ(k) ∈Y ∗|ξ(k −1) ∈Y ∗}
· P{ξ(k −1) ∈Y ∗} + P{ξ(k) ∈Y ∗|ξ(k −1) /∈Y ∗}
· P{ξ(k −1) /∈Y ∗}
= P{ξ(k −1) ∈Y ∗} + P{ξ(k) ∈Y ∗|ξ(k −1) /∈Y ∗}
· P{ξ(k −1) /∈Y ∗}
≥P{ξ(k −1) ∈Y ∗} + d · (1 −P{ξ(k −1) ∈Y ∗})
= d + (1 −d) · P{ξ(k −1) ∈Y∗}
≥d + (1 −d) · (1 −(1 −d)k−1) = 1 −(1 −d)k.
Consequently, for any t ≥1, the following equation is true:
P{ξ(t) ∈Y ∗} ≥1 −(1 −d)t.
(3.2)
Q.E.D
FWA has the operation of mutation. For simplicity, it is assumed that the operation
of mutation in FWA is only the stochastic mutation.
Theorem 3.2 Given FWA being an absorb state Markov process {ξ(t)}∞
t=0 and opti-
mal state space Y ∗∈Y, then limt→∞λ(t) = 1 which means that ξ(t)∞
t=0 will
converge to the optimal state Y ∗.
Proof FWA can provide the mutation operator, so the probability that the ﬁrework of
FWA arriving the optimal region Rε from nonoptimal region via mutation operator
is denoted as Pmu(t). It is able to be expressed as follows:
Pmu = ν(Rε) · n
ν(S)
,
(3.3)
where ν(S) is the Lebegue measure value of the problem space S with ν(Rε) > 0,
Pmu > 0, n is the number of ﬁreworks.
In terms of the stochastic Markov process {ξ(t)}∞
t=0 of FWA, it holds that
λ(t) = P{ξ(t) ∈Y ∗|ξ(t −1) /∈Y ∗} = Pmu(t) + Pex(t),
(3.4)
where Pex(t) denotes the probability of the ﬁreworks of FWA arriving at the optimal
region Rε by the ﬁrework’s explosion in FWA.

3.2 Global Convergence Theorems
41
So, one has
P{ξ(t) ∈Y ∗|ξ(t −1) ∈Y ∗} ≥P(mu) > 0.
(3.5)
Since the Markov process {ξ(t)}∞
t=0 of FWA is an absorbing Markov process that
the condition of the theorem 1 is held here, the following equation can be obtained:
P{ξ(t) ∈Y ∗} = 1 −(1 −Pmu(t))t.
(3.6)
So, we have
limtt→∞P{ξ(t) ∈Y ∗} = 1.
(3.7)
As a result, the Markov process {ξ(t)}∞
t=0 of FWA will converge to the optimal
state.
Q.E.D.
3.3 Time Complexity of FWA
3.3.1 Basic Theory of Time Complexity
The analysis of evolutionary computation methods based on Markov process model
has been done by Huang Han and Hao Zhifeng, for the Evolutionary Programming
[2] and Ant Conoly Optimization [3], respectively. In this section, the deﬁnitions of
some important concepts and theorems on Fireworks Algorithm are presented below,
which can also be referred to the literatures [2, 3] for details.
Deﬁnition 3.7 (Expected convergence time) Given FWA being an absorbing state
Markov process {ξ(t)}∞
t=0 and optimal state space Y ∗⊂Y, if γ is a stochastic
nonnegative value such that: if t ≥γ, P{ξ(t + 1) ∈Y ∗} = 1; if 0 ≤t ≤γ,
P{ξ(t + 1) /∈Y ∗} < 1, then the γ is named as the convergence time of FWA. The
expected value Eγ is called as the expected convergence time of FWA.
The expected convergence time describes the expected time of arriving the global
optimal solution in probability 1 at the ﬁrst time. The smaller the expected value Eγ
is, the faster the convergence of FWA is and the more effective FWA is. However, it
can also use the Expected First Hitting Time (EFHT) as a index of convergence time
which is given as follows.
Deﬁnition 3.8 (Expected First Hitting Time) Given FWA being an absorbing state
Markov process {ξ(t)}∞
t=0 and optimal state space Y ∗⊂Y; μ is a stochastic value
such that: if t = μ, ξ(t) /∈Y ∗; if 0 ≤t ≤μ, ξ(t) /∈Y ∗. The expected value Eμ is
named as Expected First Hitting Time.

42
3
Modeling and Theoretical Analysis of FWA
The following theorem gives an approach to compute Eλ:
Theorem 3.3 Given FWA being an absorbing state Markov process ξ(t)∞
t=0 and
optimal state space Y ∗⊂Y. If λ(t) = P{ξ(t) ∈Y ∗} and limtt→∞λ(t) = 1, the
Expected Convergence Time is Eγ = ∞
t=0(1 −λ(t)).
Proof Since
λ(t) = P{ξ(t) ∈Y ∗} = P{μ ≤t}
⇒λ(t) −λ(t −1) = P{μ ≤t} −P{μ ≤t −1}
⇒P{μ = t} = λ(t) −λ(t −1),
then
Eμ = 0 · P{μ = 0} + ∞
t=0t · P{μ = t},
Eμ = ∞
t=0t · (λ(t) −λ(t −1))
= (λ(1) −λ(0)) + 2 · (λ(2) −λ(1)) + . . .
+ t · (λ(t) −λ(t −1)) + . . .
= ∞
i=1(λ(t) −λ(t −1)) + ∞
i=2(λ(t) −λ(t −1))
+ . . . + ∞
i=t(λ(t) −λ(t −1)) + . . .
= (limt→∞λ(t) −λ(1)) + (limt→∞λ(t) −λ(2))
+ . . . + (limt→∞λ(t) −λ(t −1)) + . . .
= ∞
i=1(limtt→∞λ(t) −λ(t −1)) = ∞
i=1(1 −λ(t −1))
= ∞
i=1(1 −λ(t))
So,
Eγ = Eμ = ∞
i=1(1 −λ(t)).
Q.E.D.
Because it is hard to obtain the value of λ(t), so it is difﬁcult to compute the
expected convergence time Eγ exactly. So, its estimation is able to be given as
follows:
The proof of Lemma 3.3, Theorems 3.4 and 3.5, and Corollary 3.1 can also been
referred to [3].
Lemma 3.3 Given two stochastic nonnegative variables, u and ν, and Du(.) and
Dv(.) denote the distribution functions of u and ν. The expected value of u and ν
can hold Eu < Eν if Du(.) ≥Dv(.) for t = 0, 1, 2, . . .

3.3 Time Complexity of FWA
43
Proof Since Du(t) = P{u ≤t} and Dv(t) = P{u ≤t}(∀t = 0, 1, 2, . . . ),
Eu = 0· Du(0)+
+∞

t=1
t[Du(t) −Du(t −1)] =
+∞

i=1
+∞

t=1
[Du(t)−Du(t −1)] =
+∞

i=0
[1 −Du(i)].
(3.8)
As a result,
Eu −Ev =
+∞

i=0
[1 −Du(i)] −
+∞

i=0
[1 −Dv(i)] =
+∞

i=0
[Dv(i) −Du(i)] ≤0 ⇒Eu ≤Ev.
(3.9)
Q.E.D.
Theorem 3.4 Given FWA being an absorbing state Markov process {ξ(t)}∞
t=0 and
optimal state space Y ∗⊂Y. If λ(t) = P{ξ(t) ∈Y ∗} such that 0 ≤Dl(t) ≤λ(t) ≤
Dh(t) ≤1(∀t = 0, 1, 2, . . .) and limtt→∞λ(t) = 1, then:
∞
i=1(1 −Dl(t)) ≤Eγ ≤∞
i=1(1 −Dt(t)).
(3.10)
Proof Construct two discrete random nonnegative variables h andl, their distribution
functions are Dh(t) and Dl(t), respectively.
Apparently, μ in Deﬁnition 3.8 is also a discrete random nonnegative variable. Its
distribution function is expressed as
λ(t) = P{ξ(t) ∈Y ∗} = P{μ ≤t}.
(3.11)
Because 0 ≤Dl(t) ≤λ(t) ≤Dh(t) ≤1, Eh ≤Eμ ≤El ⇔
+∞

t=0
(1 −Dh(t)) ≤
Eγ = Eμ ≤
+∞

t=0
(1 −Dl(t)).
Q.E.D.
Theorem 3.5 Given FWA being an absorbing state Markov process {ξ(t)}∞
t=0 and
optimal state space Y ∗⊂Y; if λ(t) = P{ξ(t) ∈Y ∗} and 0 ≤a(t) ≤λ(t) ≤b(t),
∞
l=1[(1 −λ(0))∞
i=0(1 −a(t))] ≤Eγ ≤∞
t=0[(1 −λ(0))∞
i=1(1 −a(t))].
Proof Since
λ(t) = [1−λ(t−1)]P{ξ(t) ∈Y ∗|ξ(t−1) /∈Y ∗}+λ(t−1)P{ξ(t) ∈Y ∗|ξ(t−1) ∈
Y ∗}(∀t = 0, 1, 2, . . .), 1−λ(t) ≤[1−a(t)][1−λ(t−1)] = [1−λ(0)]
t
i=1
[1 −a(i)].

44
3
Modeling and Theoretical Analysis of FWA
According to Deﬁnition 3.3,
Eγ =
+∞

i=0
(1 −λ(i)) ≤
+∞

t=0

(1 −λ(0))
t
i=1
(1 −a(t))

.
(3.12)
Likewise
Eγ =
+∞

i=0
(1 −λ(i)) ≥
+∞

t=0

(1 −λ(0))
t
i=0
(1 −b(t))

.
(3.13)
Q.E.D.
Corollary 3.1 Given FWA being an absorbing state Markov process {ξ(t)}∞
t=0 and
optimal state space Y ∗⊂Y; and λ(t) = P{ξ(t) ∈Y ∗}. If a ≤P{ξ(t+1) ∈Y ∗|ξ(t+
1) /∈Y ∗} ≤b(a, b > 0) and limt→∞λ(t) = 1, then the expected convergence time
Eγ of FWA is such that
b−1[1 −λ(0)] ≤Eγ ≤a−1[1 −λ(0)].
(3.14)
Proof According to Deﬁnition 3.5,
Eγ ≤[1 −λ(0)]

a +
+∞

t=2
ta
t−2

i=0
(1 −a)

⇒Eγ ≤[1 −λ(0)]

a +
+∞

t=2
ta(1 −a)t−1

⇒Eγ ≤a[1 −λ(0)]
+∞

t=0
t(1 −a)t+
+∞

t=0
(1 −a)t

⇒Eγ ≤a[1 −λ(0)]
1 −a
a2
+1
a
	
= 1
a [1 −λ(0)].
Likewise,
Eγ ≥b−1[1 −λ(0)],
(3.15)
So,
b−1[1 −λ(0)] ≤Eγ ≤a−1[1 −λ(0)].
(3.16)
Q.E.D.
The corollary and theorems of above indicate that the formula P{ξ(t) ∈Y ∗|ξ(t −
1) /∈Y ∗} can give a description of the probability arriving at the optimal state from
the nonoptimal state. The estimation of value range of Eλ is able to be computed by
the range of value of P{ξ(t) ∈Y ∗|ξ(t −1) /∈Y ∗}.

3.4 Deep Analysis of Time Complexity
45
3.4 Deep Analysis of Time Complexity
Time complexity of FWA is to compute the expected convergence time Eλ. In terms
of Corollary 3.1 in the previous subsection, it is mainly related to the probability of
FWA state arriving optimal region Rε from nonoptimal region which is P{ξ(t +1) ∈
Y ∗|ξ(t −1) /∈Y ∗}. In this subsection, we will further analyze the formula in details
to obtain the time complexity of FWA. Generally speaking, FWA includes three
operations including explosion, mutation and selection, but the operations which
directly make the Markov state of FWA get into the optimal region, are explosion
and mutation, so the following theorem is able to give out.
Theorem 3.6 Given FWA being an absorbing state Markov process {ξ(t)}∞
t=0 and
optimal state space Y ∗⊂Y; then FWA is such that
ν(Rε) × n
ν(S)
≤P{(ξ(t + 1)) ∈Y ∗|ξ(t) /∈Y ∗}
≤ν(Rε)

n
ν(S) +
n

i=1
mi
ν(Ai)

,
(3.17)
where ν(Rε) is the Lebegue measure value of the optimal region Rε. ν(S) is the
Lebegue measure value of the problem search region S and v(Ai) is the Lebegue
measure value of the explosion region Ai of ith ﬁrework.
Proof Because FWA has two operations generating the sparks or ﬁreworks: mutation
and explosion.
For mutation, it is assumed that the operation is run with randomness. The prob-
ability of one ﬁrework being mutated to the optimal region Rε is ν(Rε)
ν(S) .
So, the probability of n ﬁreworks to be randomly mutated to the optimal region
Rε is equal to ν(Rε)×n
ν(S)
.
In term of the steps of FWA, the following equation is reached:
P(ξ(t + 1) ∈Y ∗|ξ(t) /∈Y ∗) = ν(Rε) × n
ν(S)
+ P(exp),
(3.18)
where P(exp) is the probability which n ﬁreworks explode and make some sparks
stay in optimal region Rε.
So, the P(exp) is given by
P(exp) =
n

i=1
ν(Ai ∩Rε) × mi
ν(Ai)
,
(3.19)
where Ai denotes the search space in which the ith ﬁrework explodes, mi is the
number of spark which the ith ﬁrework generates.

46
3
Modeling and Theoretical Analysis of FWA
Because 0 ≤ν(Ai ∩Rε) ≤ν(Rε),
0 ≤P(exp) =
n

i=1
ν(Ai ∩Rε) × mi
ν(Ai)
≤
n

i=1
ν(Rε) × mi
ν(Ai)
= ν(Rε)
n

i=1
mi
ν(Ai).
And then,
ν(Rε) × n
ν(S)
≤P(ξ(t+1)∈Y ∗|ξ(t) /∈Y ∗)
≤ν(Rε) × n
ν(S)
+ ν(Rε)
n

i=1
mi
ν(Ai)
= ν(Rε)

n
ν(S) +
n

i=1
mi
ν(Ai)

It is obtained that
ν(Rε) × n
ν(S)
≤P(ξ(t+1) ∈Y ∗|ξ(t) /∈Y ∗) ≤ν(Rε)

n
ν(S) +
n

i=1
mi
ν(Ai)

. (3.20)
Q.E.D.
The above theorem gives the rude result because the right formula of Eq.(3.17)
is very difﬁcult to be conﬁrmed and be computed. It is much complex for FWA to
run and hard to compute the probability about it. In order to realize the probability
exactly, Eq.(3.18) needs to be further investigated which is given as follows:
P(exp) =
n

i=1
ν(Si ∩Rε) × mi
ν(Si)
.
(3.21)
As it can be known, the formulae ν(Si ∩Rε) and mi in the above equation play
key roles to P(exp) because the two formulae are dynamically changed with the
running of the algorithm.
The formula ν(Si ∩Rε) is related to the ﬁrework location Fi. The distance between
two of the ﬁreworks selected as the next generation is as far as possible, so it can be
assumed that just one ﬁrework can stay in the optimal region Rε at the same time.
In other hand, it is further assumed that there is the highest probability for the best
ﬁrework to get into the optimal region Rε.
According to the above idea of Fireworks Algorithm, ν(Ai) ≥ν(Abest) and
mi ≤mbest,i ∈(1, 2, . . . , n), where Abest and mbest are the exploding regions and

3.4 Deep Analysis of Time Complexity
47
generating sparks number of the ﬁrework whose ﬁtness is best in all the ﬁreworks,
respectively. So, it can be derived as follows:
ν(Ai ∩Rε) × mi
ν(Ai)
< ν(Abest ∩Rε) × mbest
ν(Abest)
.
(3.22)
It can be considered that(Ai∩Rε) ∩(Abest∩Rε) = φ for i ∈(1, 2, . . . , n) and
i ̸= best, especially in the early running time, so the following equation can be
derived out:
P(exp) =
n

i=1
ν(Si∩Rε) × mi
ν(Si)
< ν(Sbest∩Rε) × mbest
ν(Sbest)
< ν(Rε) × mbest
ν(Sbest)
.
(3.23)
So, Eq.(3.17) can be expressed as follows:
ν(Rε) × n
ν(S)
≤P(ξ(t+1)∈Y ∗|ξ(t) /∈Y ∗)
≤ν(Rε)
 n
ν(S) +
mbest
ν(Sbest)
	
.
(3.24)
The above Eq.(3.24) is more meaningful than Eq.(3.17), which tells that the
best ﬁrework is more important than others. From Eq.(3.24) and Corollary 3.1, let
a = ν(Rε) × n
ν(S)
and b = ν(Rε)

n
ν(S) +
mbest
ν(Sbest)

, so it leads to the following equation:
ν(S) × ν(Sbest)
ν(Rε) × (n × ν(Sbest) + mbest × ν(S)) ×(1−λ(0)) ≤Eγ ≤
ν(S)
ν(Rε) × n ×(1−λ(0)),
(3.25)
where λ(t) = P{ξ(t) ∈Y ∗}.
According to FWA, the initialization of n ﬁreworks is penetrated at random. The
following results can be acquired:
Since λ(0) = P{ξ(0) ∈Y ∗} ≪1, 1 −λ(0) = 1, thus
ν(S) × ν(Sbest)
ν(Rε) × (n × ν(Sbest) + mbest × ν(S)) ≤Eγ ≤
ν(S)
ν(Rε) × n .
(3.26)
Corollary 3.2 FWA’s expected convergence time Eλ is such that:
ν(S) × ν(Sbest)
ν(Rε) × (n × ν(Sbest) + mbest × ν(S)) ≤Eγ ≤
ν(S)
ν(Rε) × n .
(3.27)

48
3
Modeling and Theoretical Analysis of FWA
From Eq.(3.24), the more lager value of Rε is, the smaller value of ν(S) is bene-
ﬁcial to the efﬁciency of FWA, but the two values are directly related to the search
problems. Equation(3.23) indicates that ν(Sbest) and mbest are very important for
estimating FWA’s expected convergence time. But the above result is just obtained
under the condition of some assumptions. The more exact analysis might be further
done through considering the details of some equations of FWA.
In summary, like other swarm intelligence algorithms, FWA is able to be expressed
as a Markov process. After some concepts of Markov stochastic process of Fireworks
Algorithm, FWA’s global convergence has been proved, and then the approximate
region of expected convergence time of FWA has also been computed accordingly.
Moreover, the time complexity of FWA is also to be analyzed in terms of an absorbing
Markov process. Although the results given here are preliminary and incomplete,
some theorems could provide a guide for studying the theory of Fireworks Algorithm
for future.
3.5 Inﬂuence of Random Number Generators on FWA
Random numbers are widely used in intelligent optimization algorithms such as
Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle Swarm
Optimization (PSO), Fireworks Algorithm (FWA) [1], just to name a few. Random
numbers are usually generated by deterministic algorithms called Random Number
Generators (RNGs) and play a key role in driving the search process. The perfor-
mance of RNGs can be analyzed theoretically using criteria such as period and lattice
structure [4, 5], or by systematic statistical test [6]. However, none of these analyses
are relevant directly to RNGs’ impact on optimization algorithms like PSO, FWA.
It is interesting to ask how RNGs can effect FWA as well as other stochastic
methods.Clerc[7]replacedtheconventionalRNGswithashortlengthlistofnumbers
(i.e., a RNG with a very short period) and empirically studied the performance of
PSO. The experiments show that, at least for the moment, there is no sure way to build
a “good” list for high performance. Thus, RNGs with certain degree of randomness
are necessary for the success of stochastic search.
Bastos-Filho etal. [8, 9] studied the impact of the quality of CPU- and GPU-based
RNGs on the performance of PSO. The experiments show that PSO needs RNGs with
minimum quality and no signiﬁcative improvements were achieved when comparing
high-quality RNGs to medium-quality RNGs. Only Linear Congruential Generator
(LCG) [4] and Xorshift algorithms [10] were compared for CPUs, and only one
method for generating random numbers in an ad hoc manner on GPUs was adopted
for comparing GPUs.
In general, RNGs shipped with math libraries of programming languages or other
speciﬁc random libraries are used when implementing intelligent optimization algo-
rithms. These RNGs generate random numbers of very diverse qualities with different
efﬁciency. A comparative study on the impact of these popular RNGs will be helpful
when implementing intelligent algorithms for solving optimization problems.

3.5 Inﬂuence of Random Number Generators on FWA
49
In this section, we selected 13 widely used, highly optimized, uniformly distrib-
uted RNGs and applied them to FWA for empirically comparing their impact on
the optimization performance of FWA. Nine well-known benchmark functions were
implemented for comparisons. All the experiments were conducted on the GPU for
fast execution. Two novel strategies, league scoring strategy, and lose-rank strategy
were introduced to conduct a systematic comparison on these RNGs’ performance.
Though the work is limited to the impact on FWA, other intelligent algorithms can
also be studied in similar framework.
3.5.1 Random Number Generators
According to the source of randomness, random number generators fall into three
categories [11]: true random number generators (TRNGs), quasirandom number
generators (QRNGs), and pseudorandom number generators (PRNGs).
TRNGs utilize physical sources of randomness to provide truly unpredictable
numbers. These generators are usually slow and unrepeatable, and usually need the
support of speciﬁc hardware [5]. So TRNGs are hardly used in the ﬁeld of stochastic
optimization. QRNGs are designed to evenly ﬁll an n-dimensional space with points.
Though quite useful, they are not widely used in the domain of optimization. PRNGs
are used to generate pseudorandom sequences of numbers that satisfy most of the
statistical properties of a truly random sequence but are generated by a deterministic
algorithm. PRNGs are the most common RNGs of the three groups, and provided by
almost all programming languages. There also exist many well-optimized PRNGs
for open access. As we only discuss PRNGs in this chapter, we will use random
numbers and pseudorandom numbers alternatively henceforth.
Random numbers can subject to various distributions, such as uniform, normal,
and cauchy distributions. Of all the distributions, uniform distribution is the simplest
and the most important one. Not only uniform random numbers are widely used in
many different domains, but also they are used as the base generators for generating
random numbers subject to other distributions. Many methods, like transformation
methods and rejection methods, can be used to convert uniformly distributed numbers
to ones with speciﬁc nonuniform distributions [5, 12].
As here we only study uniform distribution, the remainder of the section merely
introduces RNGs with uniform distribution. RNGs for generating uniform distri-
bution random numbers can be classiﬁed into two groups, according to the basic
arithmetic operations utilized: RNGs based on modulo arithmetic and RNGs based
on binary arithmetic.
3.5.2 Modular Arithmetic Based RNGs
RNGs of this type yield sequences of random numbers by means of linear recurrence
modulo m, where m is a large integer.

50
3
Modeling and Theoretical Analysis of FWA
Linear Congruential Generator (LCG): LCG is one of the best-known random
number generators. LCG is deﬁned by the following recurrence relation:
xi = a · xi−1 + c
mod m,
(3.28)
where x is the sequence of the generated random numbers and m > 0, 0 < a < m,
and 0 ≤c, x0 < m. If uniform distribution on [0, 1) is need, then use u = x
m as the
output sequence.
For LCG, a, c and m should be carefully chosen to make sure that maximum
period can be achieved [4]. LCG can be easily implemented on computer hardware
which can provide modulo arithmetic by storage-bit truncation. RNG using LCG
is shipped with C library (rand()) as well as many other languages such as Java
(java.lang.Random). LCG has a relatively short period (at most 232 for 32-bit integer)
compared to other more complicated ones.
A special condition of LCG is when c = 0, which presents a class of multiplicative
congruential generators (MCG) [13]. Multiple carefully selected MCGs can be com-
bined into more complicated algorithms such as Whichmann–Hill generator [14].
Multiple Recursive Generator (MRG): MRG is a derivative of LCG and can
achieve much longer period. An MRG of order k is deﬁned as follows:
xi = (a1 · xi−1 + a2 · xi−2 + · · · + ak · xi−k)
mod m.
(3.29)
The recurrence has maximal period length mk −1, if tuple (a1, . . . , ak) has certain
properties [4].
Thoughthesegeneratorscanprovidebothgoodstatisticalqualityandlongperiods,
a necessary condition for a good ﬁgure of merit with respect to the spectral test is
that k
i=1 a2
i be large [15]. However, the relatively prime moduli require complex
algorithms using 32-bit multiplications and divisions, so they are not suitable for
current GPUs.
Combined Multiple Recursive Generator (CMR): CMR combines multiple
MRGs and can obtain better statistical properties and longer periods compared with
a single MRG. A well-known implementation of CMR, CMR32k3a [15], combines
two MRGs:
xi = a11 · xi−1 + a12 · xi−2 + x13 · xi−3
mod m1
yi = a21 · yi−1 + a22 · yi−2 + x23 · yi−3
mod m2
zi = xi −yi
mod m1
(3.30)
where z forms the required sequence.
Note that combining the two multiple recursive generators (MRG) will lead
to sequences with better statistical properties in high dimensions and longer peri-
ods compared with those generated from a single MRG. The combined generator
described above has a period length of approximately 2191.

3.5 Inﬂuence of Random Number Generators on FWA
51
3.5.3 Binary Arithmetic Based RNGs
RNGs of this type are deﬁned directly in terms of bit strings and sequences. As
computers are fast for binary arithmetic operations, binary arithmetic-based RNGs
can be more efﬁcient than modulo arithmetic-based ones.
Xorshift: Xorshift [10] produces random numbers by means of repeated use of
bitwise exclusive-or (xor, ⊕) and shift (≪for left and ≫for right) operations.
A xorshift with four seeds (x, y, z, w) can be implemented as follows:
t = (x ⊕(xi ≪a))
x = y
y = z
z = w
w = (w ⊕(w ≫b)) ⊕(t ⊕(t ≫c))
(3.31)
where w forms the required sequence.
With a carefully selected tuple (a, b, c), the generated sequence can have a period
as long as 2128 −1.
Mersenne Twister (MT): MT [16] is one of the most widely respected RNGs, it
is a twisted Generalized Feedback Shift Register (GFSR). The underlying algorithm
of MT is as follows:
• Set r w-bit numbers (xi, i = 1, 2, . . . ,r) randomly as initial values.
• Let
A =
 0
Iw−1
aw aw−1 · · · a1
	
,
(3.32)
where Iw−1 is the (w −1) × (w −1) identity matrix and ai, i = 1, . . . , w take
values of either 0 or 1. Deﬁne
xi+r =

xi+s ⊕

x(w:(l+1))
i
|x(l:1)
i+1

A

,
(3.33)
where x(w:(l+1))
i
|x(l:1)
i+1 indicates the concatenation of the most signiﬁcant (upper)
w −l bits of xi and the least signiﬁcant l bits of xi+1.
• Perform the following operations sequentially:
z
=
xi+r ⊕(xi+r ≫t1)
z
= z ⊕((z ≪t2) & m1)
z
= z ⊕((z ≪t3) & m2)
z
= z ⊕(x ≫t4)
ui+r
= z/(2w −1)
(3.34)
where t1, t2, t3 and t4 are integers and m1 and m2 are bit masks and ‘&’ is a bitwise
AND operation.
ui+r, i = 1, 2, . . . form the required sequence on interval (0, 1].

52
3
Modeling and Theoretical Analysis of FWA
With proper parameter values, MT can generate sequence with a period as long as
219,937 and extremely good statistical properties [16]. Strategies for selecting good
initial values are studied in [17], while Saito etal. [18] proposed efﬁcient implemen-
tation for fast execution on GPUs.
3.5.4 Experimental Setup
In this section, we describe the experimental environment and parameter settings in
detail.
3.5.4.1 Testbed
We conducted our experiments on a PC running 64-bit Windows 7 Professional with
8G DDR3 Memory and Intel core I5-2310 (@2.9GHz 3.1GHz). The GPU used
for implementing FWA in the experiments is NVIDIA GeForce GTX 560 Ti with
384 CUDA cores. The program was implemented with C and compiled with Visual
Studio 2010 and CUDA 5.5.
3.5.4.2 RNGs Used for Comparison
Besides functions provided by programming languages, many libraries with well-
implemented RNGs are available, such as AMD’s ACML [21] and Boost Random
Number Library [22] targeted at CPUs and speciﬁc implementations [11, 17, 23] for
GPU platform.
Among all these candidates, Math Kernel Library (MKL) [24] (for CPU) and
CURAND [25] (for GPU) were selected for the experiments considering the follow-
ing reasons: (1) RNGs provided by the two libraries cover the most popular RNG
algorithms, and (2) both MKL and CURAND are well optimized for our hardware
platform (I5 CPU and GeForce 560 Ti GPU), so a fair comparison of efﬁciency can
be expected. So experiments with these two libraries are broadly covered in terms of
types of RNGs and present a fair comparison in terms of time efﬁciency.
As LCG is widely shipped by standard library of various programming language,
we added a RNG with LCG (C’s rand()). The RNGs used in the experiments are
listed by Table3.1.
3.5.4.3 Benchmark Functions
For an extensive experiment, GPU-based test suit, cuROB [26] was used. cuROB has
asmanyas37testfunctionsincludingunimodal,multimodal,hybridandcombination
functions, and it supports any dimension.

3.5 Inﬂuence of Random Number Generators on FWA
53
Table 3.1 Random number generators tested
No.
Algorithm
Description
Note
1
xorshift
Implemented using the xorshift algorithm [10],
created with generator type
CURAND_RNG_PSEUDO_XORWOW
CURAND
with CUDA
Toolkit 5.5
2
xorshift
Same algorithm as 1, faster but probably statistically
weaker, set ordering to
CURAND_ORDERING_PSEUDO_SEEDED
3
Combined
Multiple
Recursive
Implemented using the Combined Multiple
Recursive algorithm [15], created with generator
type CURAND_RNG_PSEUDO_MRG32K3A
4
Mersenne
Twister
Implemented using the Mersenne Twister algorithm
with parameters customized for operation on the
GPU [18], created with generator type
CURAND_RNG_PSEUDO_MTGP32
5
Multiplicative
Congruential
Implemented using the 31-bit Multiplicative
Congruential algorithm [13], create with parameter
VSL_BRNG_MCG31
MKL 11.1
6
Generalized
Feedback Shift
Register
Implemented using the 32-bit generalized feedback
shift register algorithms, create with parameter
VSL_BRNG_R250 [19]
7
Combined
Multiple
Recursive
Implemented using Combined Multiple Recursive
algorithm [15], create with parameter
VSL_BRNG_MRG32K3A
8
Multiplicative
Congruential
Implemented using the 59-bit Multiplicative
Congruential algorithm from NAG Numerical
Libraries [14], create with parameter
VSL_BRNG_MCG59
9
Wichmann-Hill
Implemented using the Wichmann-Hill algorithm
from NAG Numerical Libraries [14], create with
parameter VSL_BRNG_WH
10
Mersenne
Twister
Implemented using the Mersenne Twister algorithm
MT19937 [16], create with parameter
VSL_BRNG_MT19937
11
Mersenne
Twister
Implemented using the Mersenne Twister algorithms
MT2203 [20] with a set of 6024 conﬁgurations.
Parameters of the generators provide mutual
independence of the corresponding sequences, create
with parameter VSL_BRNG_MT2203
12
Mersenne
Twister
Implemented using the SIMD-oriented Fast
Mersenne Twister algorithm SFMT19937 [17],
create with parameter VSL_BRNG_SFMT19937
13
Linear
Congruential
Implementing using Linear Congruential algorithm
with a = 1103515245, c = 12345, m = 232, only
high 16 bits are used as output
MS Visual
Studio C
library rand()

54
3
Modeling and Theoretical Analysis of FWA
3.5.5 Experimental Results and Analysis
This section presents the experimental results. Both efﬁciency of RNGs and solution
quality of FWA using each RNG are described and analyzed in detail.
3.5.5.1 Solution Quality
In all experiments, 51 independent trials were performed for each function, where
10,000 iterations were executed for each trial. 51 integer numbers were randomly
generated from uniform distribution as seeds for each trail, and all RNGs shared the
same 51 seeds. All ﬁreworks were initialized randomly within the whole feasible
search space and the initialization was shared by all RNGs (to be exactly, RNG No.
10 was used for the purpose of initialization).
For a particular function, the solution quality can be compared between any two
RNGs with statistical test. But there is no direct way to compare groups of RNGs
(13 in our experiments).
Lose-Rank Strategy: Lose rank can be calculated as follows: For a certain RNG,
say R1, set its lose rank to 0. R1 compares its solutions for a function with those
of all other RNGs one after another. If R1 is statistically worse than some RNGs,
then add 1 to its lose rank. In this way, we can calculate all RNGs’ lose ranks for all
functions.
The idea underlying lose rank is that if some RNG performs signiﬁcantly worse
in terms of solution quality, then it has a relative large lose rank.
Following this strategy, we can ﬁnd that the lose ranks of almost all functions are
zeros (for this reason, theloserankvalues areomittedhere). Basedontheobservation,
there exist no signiﬁcant bad RNGs, and there is no outright good ones. There is no
strong reason to prefer any RNG to others as far as its impact on solution quality is
concerned.
3.5.5.2 Efﬁciency of RNGs
We ran each RNG program to generate random numbers in batch of different sizes
and test the speed. The results are presented in Table3.2 (Note that C’s rand() does
not support batch generating of random numbers, so the average speed was measured
instead).
In general, RNGs based on both CPUs and GPUs achieve better performance by
generating batches of random numbers, and GPUs need larger batch size to get peak
performance than CPUs. In the condition of large batch size, CURAND (No.1–No.4)
can be several to tens fold faster than MKL for the same algorithms.
Modulo arithmetic-based RNGs are less efﬁcient than binary arithmetic ones, just
as aforementioned. Combined Multiple Recursive algorithm (No. 7) and Wichmann–
Hill algorithm (No. 9) present the slowest RNGs, followed by Multiplicative

3.5 Inﬂuence of Random Number Generators on FWA
55
Table 3.2 RNG efﬁciency comparison under different batch size (# of random numbers per nanosecond)
GPU
CPU
CURAND
MKL
C
Batch
Size
1
2
3
4
5
6
7
8
9
10
11
12
13
1
0.3
0.3
0.3
0.3
17.5
17.3
14.8
15.9
7.8
15.2
14.7
15.2
36.5
10
2.7
2.6
1.9
2.6
95.2
116.3
54.6
109.9
56.8
93.5
87.7
129.9
20
4.8
4.9
3.2
5.1
144.9
166.7
67.1
192.3
79.4
181.8
161.3
227.3
50
10.9
11.0
7.2
12.8
294.1
227.3
112.4
285.7
120.5
344.8
294.1
454.5
100
21.1
21.6
14.2
25.8
400.0
263.2
138.9
357.1
142.9
434.8
384.6
714.3
200
41.7
43.7
28.9
49.8
555.6
285.7
156.3
416.7
158.7
500.0
555.6
1111.1
500
104.2
114.9
76.9
125.0
625.0
416.7
178.6
454.5
166.7
625.0
666.7
1428.6
1000
200.0
243.9
163.9
232.6
666.7
555.6
185.2
454.5
172.4
666.7
769.2
1111.1
2000
312.5
476.2
344.8
434.8
666.7
666.7
188.7
454.5
172.4
714.3
769.2
1250.0
5000
500.0
1250.0
1000.0
909.1
769.2
769.2
192.3
476.2
172.4
833.3
833.3
1428.6
10000
588.2
2500.0
1428.6
1250.0
714.3
833.3
192.3
476.2
175.4
833.3
833.3
1428.6
20000
666.7
3333.3
1666.7
1666.7
714.3
833.3
192.3
476.2
175.4
769.2
769.2
1428.6
50000
769.2
10000.0
2500.0
3333.3
769.2
833.3
192.3
476.2
175.4
769.2
769.2
1428.6
100000
714.3
10000.0
2500.0
5000.0
714.3
833.3
192.3
476.2
175.4
769.2
769.2
1428.6
200000
714.3
10000.0
2500.0
10000.0
714.3
714.3
192.3
476.2
175.4
769.2
769.2
1666.7

56
3
Modeling and Theoretical Analysis of FWA
Congruential (No. 8). As a comparison, Mersenne Twister algorithm presents the
fastest RNGs. CPU-based SFMT19937 (No. 12) can be one order of magnitude
faster than CPU-based CMR32K3A (No. 7), while the GPU version (No. 4) can be
5-fold faster than the CPU implementation. Considering the good statistical property
of MT [16, 17, 20], it makes the best RNG of all the RNGs concerned.
3.5.5.3 Remarks
In general, both CPU- and GPU-based RNGs can achieve best performance when
generating blocks of random numbers that are as large as possible. Fewer calls to
generate many random numbers is more efﬁcient than many calls generating only
a few random numbers. GPU-based RNGs can be several fold even one order of
magnitude faster than their CPU-based counterparts, moreover Mersenne Twister
algorithm presents the most efﬁcient RNG.
Finally, despite only FWA using uniformly distributed RNGs was discussed here,
however, the proposed strategies could be also extended to compare any other intelli-
gent algorithm on real-world optimization problems or benchmark test suites. RNGs
for nonuniform distributions could also be exploited and studied in the framework
of the proposed strategies.
3.6 Summary
In this chapter, ﬁrst of all, the modeling, global convergence, and time complexity
of FWA were presented in detail, then the inﬂuence of random number generators to
the performance of FWA was investigated.
For theoretical analysis of FWA, this chapter deﬁned some concepts of Markov
stochastic process of FWA and proved that FWA is global convergence, and then
computed the approximate region of expected convergence time of FWA. Although
the results given here are preliminary and incomplete, some theorems could provide
a guide for studying the theory of Fireworks Algorithm in the future.
For random generator inﬂuence on FWA, though different RNGs have various
statistical strengths, no signiﬁcant disparity was observed in FWA by experiments.
Even the most common linear congruential generator performs very well despite
the fact that random number sequences generated by LCG are of lower quality in
terms of randomness compared to other more complicated RNGs. As a result, it is
reasonable to utilize the most efﬁcient algorithms for random number generation
used by an intelligent optimization like FWA.

References
57
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. H. Huang, Z. Hao, Y. Qin, Time complexity of evolutionary programming. J. Comput. Res.
Dev. 45(11), 1850–1857 (2008)
3. H. Huang, Z. Hao, W. Chunguo, Y. Qin, Time convergence speed of ant colony optimization.
J. Comput. 30(8), 1344–1353 (2007)
4. D.E. Knuth, The Art of Computer Programming, Volume 2: Seminumerical Algorithms (Ams-
terdam, London, 1969)
5. P. L’Ecuyer, Random number generation, in Handbook of Computational Statistics. Springer
Handbooks of Computational Statistics, ed. by J.E. Gentle, W.K. Hardle, Y. Mori (Springer,
Berlin, 2012), pp. 35–71. doi:10.1007/978-3-642-21551-3_3. ISBN: 978-3-642-21550-6
6. P.L’Ecuyer,R.Simard,TestU01:AClibraryforempiricaltestingofrandomnumbergenerators.
ACM Trans. Math. Softw. 33(4), (2007). doi:10.1145/1268776.1268777. ISSN: 0098-3500
7. M. Clerc, List-based optimisers: experiments and open questions. Int. J. Swarm Intell. Res.
(IJSIR) 4(4), 23–38 (2013)
8. C.J.A. Bastos-Filho, J.D. Andrade, M.R.S. Pita, A.D. Ramos, Impact of the quality of random
numbers generators on the performance of particle swarm optimization, in IEEE International
Conference on Systems, Man and Cybernetics, SMC 2009 (2009), pp. 4988–4993. doi:10.1109/
ICSMC.2009.5346366
9. C.J.A. Bastos-Filho, M.A.C. Oliveira, D.N.O. Nascimento, A.D. Ramos, Impact of the ran-
dom number generator quality on particle swarm optimization algorithm running on graphic
processor units, in 2010 10th International Conference on Hybrid Intelligent Systems (HIS)
(2010), pp. 85–90
10. G. Marsaglia, Xorshift RNGs. J. Stat. Softw. 8(14), 1–6 (2003)
11. L. Howes, D. Thomas, Efﬁcient random number generation and application using CUDA, GPU
Gems 3 (Addison-Wesley Professional, Boston, 2007), pp. 805–830
12. J.A. Rice, Mathematical Statistics and Data Analysis (Thomson Higher Education, Belmont,
2007)
13. P. L’Ecuyer, Tables of linear congruential generators of different sizes and good lattice structure.
Math. Comput. 68(225), 249–260 (1999). doi:10.1090/S0025-5718-99-00996-5. ISSN: 0025-
5718
14. The Numerical Algorithms Group Ltd, NAG Library Manual. Mark 23, (2011)
15. P. L’Ecuyer, Good parameters and implementations for combined multiple recursive random
number generators. Oper. Res. 47(1), 159–164 (1999)
16. M. Matsumoto, T. Nishimura, Mersenne twister: a 623-dimensionally equidistributed uniform
pseudo-random number generator. ACM Trans. Model. Comput. Simul. (TOMACS) 8(1), 3–30
(1998)
17. M. Saito, M. Matsumoto, SIMD-oriented fast mersenne twister: a 128-bit pseudorandom num-
ber generator, in Monte Carlo and Quasi-Monte Carlo Methods 2006, ed. by A. Keller, S. Hein-
rich, H. Niederreiter (Springer, Berlin, 2008), pp. 607–622. doi:10.1007/978-3-540-74496-2_
36. ISBN: 978-3-540-74495-5
18. M. Saito, M. Matsumoto, Variants of mersenne twister suitable for graphic processors. ACM
Trans. Math. Softw. 39(2), 12:1–12:20 (2013). doi:10.1007/978-3-540-74496-2_36. ISBN:
978-3-540-74495-5
19. S. Kirkpatrick, E.P. Stoll, A very fast shift-register sequence random number generator. J.
Comput. Phys. 40(2), 517–526 (1981)
20. M. Matsumoto, T. Nishimura, Dynamic creation of pseudorandom number generators, in Monte
Carlo and Quasi-Monte Carlo Methods 1998, ed. by H. Niederreiter, J. Spanier (Springer,
Berlin, 2000), pp. 56–69
21. AMD Inc. Core Math Library (ACML)
22. The Boost Random Number Library

58
3
Modeling and Theoretical Analysis of FWA
23. W.B. Langdon, A fast high quality pseudo random number generator for NVIDIA CUDA, in
Proceedingsofthe11thAnnualConferenceCompaniononGeneticandEvolutionaryComputa-
tion Conference: Late Breaking Papers, GECCO’09 (ACM, New York, 2009), pp. 2511–2514.
doi:10.1145/1570256.1570353. ISBN: 978-1-60558-505-5
24. Intel Corp. The Math Kernel Library
25. NVIDIA Corp. CURAND Library Programming Guide v5.5 (2013)
26. K. Ding, Y. Tan, cuROB: A GPU-based Test Suit for Real-Parameter Optimization, in Advances
in Swarm Intelligence. Lecture Notes in Computer Science, vol. 8794, ed. by Y. Tan, Y.H. Shi,
C.A. Coello Coello (Springer, Berlin, 2014), pp. 66–78

Part II
FWA Variants
Since the invention of the Fireworks Algorithm (FWA), it has attracted a lot of
attention in the community of swarm intelligence (SI). Recently, many improve-
ment works on FWA have been proposed and studied in-depth. These improvement
works can be classiﬁed into two categories, one category is based on the analysis of
limitations of FWA and then make the related improvements, and in the other
category, FWA is hybridized with other SI algorithms.
In the following chapters, we present several important improvement works till
date. In Chap. 4, we introduce the ﬁreworks algorithm based on function approx-
imation approaches. In Chap. 5, we introduce another improvement work which
proposes new calculation methods for explosion amplitude and explosion sparks
number in FWA. In Chap. 6, the enhanced ﬁreworks algorithm (EFWA) is pre-
sented, which gives the comprehensive analysis of FWA and points out ﬁve lim-
itations in FWA which leads to the related improvement work on the limitations.
In Chaps. 7 and 8, two adaptive algorithms, namely the dynamic search in FWA
(dynFWA) and adaptive ﬁreworks algorithm (AFWA), are proposed based on
automatically adjusting the explosion amplitude in FWA. The performances of
dynFWA and AFWA are greatly enhanced over the previous work on FWA. By
incorporating a probabilistically oriented explosion mechanism (POEM) into the
conventional FWA, a novel cooperative ﬁreworks algorithm (CoFWA) is proposed
in Chap. 9 to enhance the interactions among the individual ﬁreworks in the swarm.
In Chap. 10, hybrid works between FWA and other SI algorithms are given.

Chapter 4
FWA Based on Function Approximation
Approaches
In this chapter, we present an empirical study on the inﬂuence of approximation
approaches on accelerating the ﬁreworks algorithm search by elite strategy [1]. We
use three sampling data methods to approximate ﬁtness landscape, i.e., the best
ﬁtness sampling method, the sampling distance near the best ﬁtness individual sam-
pling method, and the random sampling method. For each approximation method;
we conduct a series of combinative evaluations with different sampling methods and
sampling numbers for accelerating ﬁreworks algorithm. The experimental evalua-
tions on benchmark functions show that this elite strategy can enhance the ﬁreworks
algorithm search capability effectively. We also analyze and discuss the related issues
on the inﬂuence of approximation model, sampling method, and sampling number
on the ﬁreworks algorithm acceleration performance.
4.1 Introduction
Evolutionary computation (EC) acceleration is a promising study direction in recent
EC community [2]. On the one hand, the acceleration approach study is practical fun-
damental to establish the EC convergence theory, on the other hand, accelerated EC
algorithms (ECs) beneﬁt their practical applications in industrial society. Approxi-
mating ECs ﬁtness landscape and conducting an elite strategy is one of the effective
approaches to accelerate ECs, Reference [3] proposed an approximation approach in
original n-D dimensional search space to accelerate EC and Refs. [4, 5] extended its
work to approximate ﬁtness landscape in projected one-dimensional search space.
Fireworks algorithm is a new ECs and a population-based optimization technique
[6]. Compared with the other ECs, ﬁreworks algorithm presents a different search
manner,whichistosearchthenearbyspacebysimulatingtheexplosionphenomenon.
Since it was proposed, ﬁreworks algorithm has shown its signiﬁcance and superior-
ity in dealing with the optimization problems of non-negative matrix factorization
[7, 8]. Although the ﬁreworks algorithm has shown its advantage than the other EC
algorithms, it still has the further improvement possibility by elite strategy. To obtain
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_4
61

62
4
FWA Based on Function Approximation Approaches
elite from approximated ﬁtness landscape, in this paper, we used some variations
of sampling methods for approximating ﬁreworks algorithm ﬁtness landscape. It
includes the best ﬁtness sampling method, the sampling distance near the best ﬁtness
individual sampling method and the random sampling method. We apply an elite
strategy to enhance ﬁreworks algorithm search capability with different sampling
methods and different sampling data number.
The originalities of the proposed method are: (1) this study discovers that elite
strategy is an efﬁcient method to accelerate the ﬁreworks algorithm search; (2) the
inﬂuence of sampling methods on acceleration performance of ﬁreworks algorithm is
investigated; and (3) the inﬂuence of sampling number on acceleration performance
of the ﬁreworks algorithm in each sampling method is also studied.
4.2 Fireworks Algorithm
The ﬁreworks algorithm generates offspring candidates around their parent individ-
ual in search space as if the ﬁreworks explosion generates sparks around its explosion
point. Given a single objective function f : Ω ⊆RD →R, it is supposed that ﬁre-
works algorithm is to ﬁnd a point x ∈Ω, which has the minimal ﬁtness. It is assumed
that the population size of ﬁreworks is N and the population size of explosion spark is
m and Gaussian mutation sparks is ˆm. Each ﬁreworks i(i = 1, 2, ..., N) in a popula-
tion has the following properties: a current position Xi, a current explosion amplitude
Ai and the number of generated spark si. The ﬁreworks generates a number of sparks
within a ﬁxed explosion amplitude. AS to the optimization problem f , a point with
better ﬁtness is considered as a potential solution, which the optima locate nearby
with high chance, vice versa.
4.3 Fireworks Algorithm Acceleration by Elite Strategy
4.3.1 Motivation
It is a promising study topic on approximating ﬁtness landscape to accelerate ECs
search, Ref. [9] investigated the ﬁtness landscape approximation approaches and
recent related topic on surrogate-assisted EC was reported in Ref. [10]. A novel
ﬁtness landscape approximation method using Fourier Transform was introduced
in [11, 12]. References [3–5] conducted some related works on ﬁtness landscape
approximation for accelerating ECs in D and one-dimensional search space, respec-
tively. There are several issues that we are still not clear regarding the inﬂuence of
the different sampling method with the different number to the ECs approximation
model accuracy and acceleration performance. However, previous works show little
concern on the inﬂuence of the different sampling methods with the different num-

4.3 Fireworks Algorithm Acceleration by Elite Strategy
63
ber to the ECs approximation model accuracy and acceleration performance. Some
related research topics need to be investigated furthermore. First, there is no related
report on the different sampling method inﬂuence to the approximation model accu-
racy and ﬁreworks algorithm acceleration. In this study, we use three sampling data,
i.e., best sampling, distance near the best ﬁtness individual sampling method, and
random sampling to investigate the sampling method inﬂuence to the model accu-
racy and ﬁreworks algorithm acceleration. Second, we also want to investigate the
inﬂuence of different sampling number on the approximation model accuracy and
ﬁreworks algorithm acceleration. In this work, we set the different sampling number
(3, 5, 10) in each sampling method to conduct a set of comparative evaluations to
study on those issues.
4.3.2 Sampling Methods
1. Best Sampling Method, selects the best K individuals as sampling data.
2. Distance Near the Best Fitness Individual Sampling Method, selects the nearest
K individuals to the best individual using Euclidean distance as sampling data.
3. Random Sampling Method, selects K individuals randomly as sampling data.
4.3.3 Fireworks Algorithm with an Elite Strategy
Our elite strategy for approximating ﬁtness landscape uses only one of the D para-
meter axes at a time instead of all D parameter axes, and projects individuals onto
each 1 regression space. Each of the D-dimensional regression spaces has M pro-
jected individuals, which come from the different sampling methods with a different
sampling number. We approximate the landscape of each one-dimensional regression
space using the projected M individuals and select the elite from the D approximated
one-dimensional landscape shapes.
In our evaluations, the sampling number is set as 3, 5, and 10; i.e., K = 3, 5, and
10 to check the sampling number’s inﬂuence to the acceleration performance. We
test least squares approximation for approximating the one-dimensional regression
search spaces with one and two degree polynomial functions. The elite are generated
from the resulting approximated shapes, see Fig.4.1.
The actual least square regression functions used is polynomial curve ﬁtting, given
by the Eq.(4.1).
⎛
⎜⎜⎜⎜⎝
xt
11 xt−1
12
. . . x0
1K
xt
21 xt−1
21
. . . x0
2K
...
...
...
xt
D1 xt−1
D2 . . . x0
DK
⎞
⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
a1
a2
...
aK
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
y1
y2
...
yK
⎞
⎟⎟⎟⎠
(4.1)

64
4
FWA Based on Function Approximation Approaches
Fig. 4.1 Original D dimensional space and one-dimensional spaces obtained by reducing the
dimensions of the original one
where xi j, (i = 1, 2, . . . , D) and ( j = 1, 2, ..., K) are the projected individual of
point set xi, (i = 1, 2, . . . , K) and their ﬁtness values among (xi j, y j) in the ith
one-dimensional regression space for (i = 1, 2, . . . , D) and ( j = 1, 2, . . . , K), a0,
a1, . . ., ak are the parameters obtained by least squares method, t is the power of
polynomial function.
Least square approximation by two degree polynomial function (LS2) simpliﬁes
a regression space with a nonlinear curve, and it is easy to obtain its inﬂection
point from its gradient, using the inﬂection point as the elite. Linear least square
approximation (LS1) uses a linear function to approximate the regression space.
Its gradient is either descent or ascent. A safer approach, taking into account both
descent and ascent, is to select the average point of the linear approximation line as
the elite.
The proposed methods replace the worst individual in each generation with the
selected elite. Although we cannot deny the small possibility that the global optimum
is located near the worst individual, the possibility that the worst individual will
become a parent in the next generation is also low; removing the worst individual
therefore presents the least risk and is a reasonable choice.
The whole ﬂowing of ﬁreworks algorithm with an elite strategy shows in Algo-
rithm 4.1.

4.4 Experimental Evaluations
65
Algorithm 4.1 Pseudocode of Fireworks Algorithm with an Elite Strategy
1: initialization.
2: randomly select N ﬁreworks at N locations
3: for t = 1 to max Iter do
4:
evaluate the N ﬁreworks using objective function
5:
calculate the number of spark and explosion amplitude of each ﬁreworks
6:
generate explosion sparks by algorithm 2.1
7:
generate Gaussian mutation sparks by 2.2
8:
obtain the optimal candidate in the Set which includes generating explosion sparks, ˆm Gaussian
mutation sparks and N ﬁreworks
9:
randomly select the other N −1 ﬁreworks
10:
obtain N ﬁreworks for the next iteration
11:
approximate ﬁtness landscape in each projected one-dimensional search space
12:
obtain a spark from approximated curves by Elite Strategy
13:
if Elite Fitness > Worst Individual Fitness then
14:
replace the worst individual
15:
end if
16: end for
17: return the optima
4.4 Experimental Evaluations
4.4.1 Experimental Design
To investigate the inﬂuence of sampling methods and sampling number on the accel-
eration performance, we used the ﬁreworks algorithm as the optimization method to
study our research proposals. Ten benchmark functions are selected as a test suite
[13], Table A.4 shows their range, optima, and characteristic. In our experiments, we
compare our proposed acceleration approaches by least squares approximation with
a normal ﬁreworks algorithm, conduct t-test for each proposed acceleration methods,
and of course, we checked normality of data distribution before applying t-test.
Here, we abbreviate the ﬁreworks algorithm where the search space is regressed
by a two degree least square approximation as FWA-LS2, where it is regressed by
a line power function least squares approximation as FWA-LS1. The best distance
near the best ﬁtness individual and random sampling method are abbreviated as BST,
DIS, and RAN, followed by the sampling number. These abbreviations are also used
in Tables4.1, 4.2, and 4.3.
In this method, the number of ﬁreworks (N) and Gaussian mutation ﬁrework ( ˆm)
are both set as 8 and other parameters are set as in Ref. [6]. Experimental evaluations
run 50 trails of 1000 generations on each ten benchmark functions independently.
All the benchmark functions are set as 10 dimensions. The experimental platform for
all evaluations is MATLAB®2011b, based on an Intel(R) Core(TM) i7-2600 CPU,
4G RAM machine, and runs under Windows®7.

66
4
FWA Based on Function Approximation Approaches
Table 4.1 Mean and variance of 10 benchmark functions (the variance is parenthesis) used in experimental evaluations, the bold font numbers show the better
ﬁnal results
Methods
F1
F2
F3
F4
F5
N
−4.2E+02 (1.5E+03)
6.6E+02 (4.8E+05)
4.7E+06 (1.5E+13)
2.4E+03 (2.5E+06)
2.8E+03 (5.4E+06)
LS1-BST3
−4.2E+02 (4.7E+03)
7.4E+02 (1.0E+06)
4.0E+06 (8.4E+12)
2.7E+03 (2.2E+06)
3.5E+03 (8.6E+06)
LS1-BST5
−4.1E+02 (3.1E+04)
4.4E+02 (6.1E+05)
4.5E+06 (8.8E+12)
2.1E+03 (1.8E+06)
2.9E+03 (8.7E+06)
LS1-BST10
−4.4E+02 (4.5E+02)
3.5E+02 (4.6E+05)
5.2E+06 (2.1E+13)
2.2E+03 (2.7E+06)
3.4E+03 (7.2E+06)
LS2-BST3
−4.5E+02 (1.2E−01)
−8.0E+01 (6.5E+04)
1.8E+06 (3.0E+12)
4.2E+02 (2.3E+05)
1.9E+03 (2.4E+06)
LS2-BST5
−4.5E+02 (1.3E−01)
−1.0E+02 (8.5E+04)
1.6E+06 (2.4E+12)
2.1E+02 (1.7E+05)
2.2E+03 (4.6E+06)
LS2-BST10
−4.5E+02 (8.8E−02)
−6.2E+01 (9.1E+04)
1.4E+06 (1.8E+12)
−3.1E+01 (5.9E+04)
1.4E+03 (2.2E+06)
LS1-DIS3
−4.3E+02 (5.0E+02)
4.2E+02 (5.6E+05)
3.9E+06 (1.0E+13)
2.3E+03 (2.3E+06)
3.3E+03 (6.5E+06)
LS1-DIS5
−4.1E+02 (8.8E+03)
6.3E+02 (6.7E+05)
4.9E+06 (1.8E+13)
2.1E+03 (2.5E+06)
3.3E+03 (5.8E+06)
LS1-DIS10
−4.0E+02 (4.3E+04)
4.6E+02 (4.8E+05)
5.3E+06 (3.1E+13)
2.3E+03 (3.2E+06)
3.3E+03 (7.8E+06)
LS2-DIS3
−4.4E+02 (4.0E+02)
1.2E+02 (1.6E+05)
2.0E+06 (2.0E+12)
7.5E+02 (4.0E+05)
2.3E+03 (4.9E+06)
LS2-DIS5
−4.4E+02 (2.6E+02)
−3.0E+00 (8.6E+04)
1.5E+06 (1.4E+12)
5.7E+02 (2.7E+05)
2.4E+03 (3.0E+06)
LS2-DIS10
−4.4E+02 (2.6E+02)
−5.1E+00 (7.6E+04)
1.4E+06 (1.9E+12)
3.2E+02 (2.1E+05)
2.0E+03 (3.7E+06)
LS1-RAN3
−3.8E+02 (8.7E+04)
7.8E+02 (9.0E+05)
4.9E+06 (2.8E+13)
2.2E+03 (2.1E+06)
2.9E+03 (5.7E+06)
LS1-RAN5
−3.7E+02 (1.4E+05)
7.0E+02 (8.3E+05)
5.0E+06 (2.8E+13)
2.4E+03 (2.5E+06)
2.4E+03 (6.1E+06)
LS1-RAN10
−3.8E+02 (1.7E+05)
5.7E+02 (7.4E+05)
4.5E+06 (1.6E+13)
2.5E+03 (2.0E+06)
3.2E+03 (6.5E+06)
LS2-RAN3
−4.5E+02 (4.5E−02)
−3.1E+02 (9.5E+03)
9.1E+05 (6.6E+11)
−1.5E+02 (3.9E+04)
1.8E+03 (1.7E+06)
LS2-RAN5
−4.5E+02 (9.5E−02)
−3.5E+02 (5.2E+03)
9.5E+05 (6.0E+11)
−2.5E+02 (1.1E+04)
1.2E+03 (1.1E+06)
LS2-RAN10
−4.5E+02 (5.1E+00)
−3.3E+02 (3.5E+03)
8.4E+05 (2.9E+11)
−2.4E+02 (1.3E+04)
−1.3E+02 (2.1E+05)
(continued)

4.4 Experimental Evaluations
67
Table 4.1 (continued)
Methods
F6
F7
F8
F9
F10
N
4.6E+04 (1.9E+10)
−1.8E+02 (2.4E+01)
−1.2E+02 (6.3E−03)
−3.1E+02 (1.2E+02)
−2.8E+02 (3.1E+02)
LS1-BST3
1.5E+05 (1.0E+12)
−1.8E+02 (7.7E+01)
−1.2E+02 (9.6E−03)
−3.1E+02 (1.1E+02)
−2.8E+02 (3.6E+02)
LS1-BST5
8.9E+03 (5.2E+08)
−1.8E+02 (6.5E+00)
−1.2E+02 (8.4E−03)
−3.1E+02 (1.2E+02)
−2.8E+02 (3.7E+02)
LS1-BST10
2.1E+04 (6.9E+09)
−1.8E+02 (1.2E+01)
−1.2E+02 (8.7E−03)
−3.1E+02 (1.4E+02)
−2.8E+02 (3.8E+02)
LS2-BST3
1.6E+03(3.5E+06)
−1.8E+02 (1.1E+00)
−1.2E+02 (6.5E−03)
−3.2E+02 (3.8E+01)
−2.8E+02 (2.2E+02)
LS2-BST5
1.6E+03 (3.8E+06)
−1.8E+02 (1.0E+00)
−1.2E+02 (1.1E−02)
−3.2E+02 (3.8E+01)
−2.8E+02 (2.9E+02)
LS2-BST10
2.0E+03 (6.2E+06)
−1.8E+02 (9.7E−01)
−1.2E+02(4.8E−03)
−3.2E+02(2.2E+01)
−2.8E+02 (3.3E+02)
LS1-DIS3
3.6E+03 (1.4E+07)
−1.8E+02 (2.4E+01)
−1.2E+02 (1.0E−02)
−3.1E+02 (1.1E+02)
−2.8E+02 (2.9E+02)
LS1-DIS5
1.1E+04 (1.3E+09)
−1.8E+02 (1.4E+01)
−1.2E+02 (8.7E−03)
−3.1E+02 (1.1E+02)
−2.8E+02 (2.5E+02)
LS1-DIS10
1.3E+04 (1.4E+09)
−1.8E+02 (1.7E+01)
−1.2E+02 (1.4E−02)
−3.1E+02 (1.5E+02)
−2.8E+02 (3.4E+02)
LS2-DIS3
3.5E+03 (1.4E+07)
−1.8E+02 (1.6E+00)
−1.2E+02 (6.2E−03)
−3.1E+02 (9.2E+01)
−2.9E+02 (1.9E+02)
LS2-DIS5
1.5E+04 (5.9E+09)
−1.8E+02 (5.4E−01)
−1.2E+02 (7.6E−03)
−3.2E+02 (4.4E+01)
−2.9E+02 (1.9E+02)
LS2-DIS10
4.2E+03 (2.8E+07)
−1.8E+02 (4.9E−01)
−1.2E+02 (6.9E−03)
−3.1E+02 (5.7E+01)
−2.9E+02 (1.7E+02)
LS1-RAN3
1.9E+04 (4.6E+09)
−1.8E+02 (1.4E+01)
−1.2E+02 (9.7E−03)
−3.1E+02 (1.3E+02)
−2.8E+02 (3.0E+02)
LS1-RAN5
7.1E+03 (1.7E+08)
−1.8E+02 (3.1E+01)
−1.2E+02 (8.7E−03)
−3.1E+02 (9.8E+01)
−2.8E+02 (3.1E+02)
LS1-RAN10
5.1E+03 (1.3E+08)
−1.8E+02 (9.0E+00)
−1.2E+02 (6.2E−03)
−3.1E+02 (1.4E+02)
−2.8E+02 (3.6E+02)
LS2-RAN3
1.6E+03 (4.1E+06)
−1.8E+02(6.3E−01)
−1.2E+02 (9.6E−03)
−3.2E+02 (5.1E+01)
−2.8E+02 (2.6E+02)
LS2-RAN5
1.6E+03 (4.3E+06)
−1.8E+02 (4.3E−01)
−1.2E+02 (6.0E−03)
−3.2E+02 (7.0E+01)
−2.9E+02 (2.3E+02)
LS2-RAN10
2.7E+03 (8.2E+06)
−1.8E+02 (4.4E−01)
−1.2E+02 (6.8E−03)
−3.2E+02 (7.5E+01)
−2.9E+02 (3.6E+02)

68
4
FWA Based on Function Approximation Approaches
Table 4.2 T-Test results of 10 benchmark functions used in experimental evaluations, the bold font means the signiﬁcance of the proposed acceleration methods
in the signiﬁcant level of 0.05
Methods
F1
F2
F3
F4
F5
F6
F7
F8
F9
F10
Average
LS1-BST3
0.30733215
0.66264958
0.29076284
0.293890
0.19412898
0.48179234
0.87950749
0.35946016
0.91104486
0.50163682
0.48822053
LS1-BST5
0.52975586
0.12835392
0.66625593
0.43016709
0.77425095
0.06589619
0.20030846
0.71035100
0.22041523
0.41588405
0.41416387
LS1-BST10
0.08056825
0.02470911
0.60396748
0.68870988
0.20200305
0.26278522
0.49376467
0.30779018
0.74328076
0.21277440
0.36203530
LS2-BST3
0.00018873
1.7289E−09 6.3823E−06 1.5510E−11 0.03135001
0.02731704
0.00341214
0.57552981
3.1363E−07 0.66158560
0.12993900
LS2-BST5
0.00019108
1.0157E−09 9.0085E−07 4.6657E−13 0.2130529
0.02736166
0.00199084
0.58023683
1.6390E−07 0.70201959
0.15248540
LS2-BST10
0.00018142
4.6875E−09 2.9097E−07 1.3887E−14 0.00064191
0.02850594
0.00105950
0.00710928
5.25E−10
0.26088098
0.02983793
LS1-DIS3
0.3024598
0.09248131
0.21053517
0.73353761
0.29185093
0.03449843
0.95869120
0.1610230
0.56215176
0.28643497
0.3633664
LS1-DIS5
0.30193380
0.82493130
0.83781266
0.43509969
0.26738590
0.0834036
0.8580748
0.66146725
0.3738602
0.90766802
0.55516379
LS1-DIS10
0.35673754
0.14957042
0.59834329
0.85930014
0.26509955
0.10248507
0.25256032
0.13459419
0.83652087
0.15412697
0.37093384
LS2-DIS3
0.04020126
1.023E−05
9.0772E−06 5.572E−09
0.25845547
0.03421305
0.01218165
0.50861468
0.21696933
0.06585411
0.1136509
LS2-DIS5
0.02593316
4.1032E−08 4.1816E−07 1.8205E−10 0.40383607
0.16633869
0.00131148
0.56170367
0.00011524
0.09590750
0.12551463
LS2-DIS10
0.02499335
3.3676E−08 2.9331E−07 2.7265E−12 0.08826582
0.03718129
0.000463
0.96882973
0.04129824
0.00233455
0.1163366
LS1-RAN3
0.29955990
0.47553783
0.92415835
0.63494715
0.7833978
0.22088026
0.5596284
0.1089092
0.59620154
0.13525320
0.47384736
LS1-RAN5
0.25031038
0.82796216
0.8422587
0.95198690
0.41726152
0.05208138
0.5428202
0.57430298
0.93416252
0.81746090
0.62106077
LS1-RAN10 0.41702221
0.5579850
0.7661850
0.77668166
0.32993593
0.04174701
0.10787766
0.61451596
0.24905846
0.63211117
0.44931201
LS2-RAN3
0.00016607
3.1592E−13 6.1902E−09 2.4789E−15 0.01630269
0.02736504
0.00033605
0.19381760
3.4057E−05 0.47844205
0.07164636
LS2-RAN5
0.00018380
8.4637E−14 7.9995E−09 6.7861E−16 3.2460E−05 0.02735219
0.00038332
0.96308009
0.00044558
0.00543778
0.09969152
LS2-RAN10 0.00084498
1.4595E−13 3.6374E−09 7.6255E−16 1.1666E−11 0.03114742
0.00105663
0.54632155
0.02897558
0.01357420
0.06219204

4.4 Experimental Evaluations
69
Table 4.3 Conﬁdence interval of p-value of approximation methods, sampling method, and sam-
pling number method, the sampling number for each method is 60, i.e., it follows a normal
distribution according to the central limit theorem, and the conﬁdent probability is 0.95, i.e.,
u = 1.96
Methods
Mean
Variance
Conﬁdent interval
LS1
0.45534488
0.07738241
[0.43576443, 0.47492533]
LS2
0.1001438
0.04668558
[0.08833074, 0.11195690]
BST
0.26278034
0.07998594
[0.24254110, 0.28301957]
DIS
0.27416103
0.09252915
[0.25074792, 0.29757413]
RAN
0.29629168
0.11027739
[0.26838765, 0.32419570]
Sampling # 3
0.27344509
0.08367126
[0.25227334, 0.29461684]
Sampling # 5
0.32801333
0.11433117
[0.29908355, 0.35694311]
Sampling # 10
0.23177462
0.08064302
[0.21136912, 0.25218012]
4.4.2 Experimental Results
Table4.1 shows the mean and variance of ten benchmark functions, and Table4.2
shows the t-test results of each acceleration method compared to the normal ﬁreworks
algorithm.
From those experimental results, we can conclude that
(1) Elite strategy is an effective approach to enhance ﬁreworks algorithm search
capability.
(2) The random sampling method has the better acceleration performance than the
others.
(3) For some benchmark functions, best sampling method has the same acceleration
performance as the random sampling method.
(4) The distance sampling method is not more effective than the other sampling
methods, i.e., best and random sampling method.
(5) Inthedistancesamplingmethod,theapproximationmodelbythemoresampling
number have the better performance than the other approaches, such as F9 and
F10.
(6) For all the benchmark functions, LS2 methods are better LS1 methods.
4.5 Discussion
Table4.3 shows the conﬁdence interval of the p value for each approximation
approach. We analyze the inﬂuence of approximation approaches from the experi-
mental results of Tables4.1, 4.2, and 4.3.

70
4
FWA Based on Function Approximation Approaches
4.5.1 Fireworks Algorithm Acceleration Performance
Fireworks algorithm is a new EC algorithm, which is inspired by the nature phe-
nomenon of ﬁrework explosion. The crucial mechanism of ﬁreworks algorithm is
the multichange strategy, i.e., one ﬁrework makes several sparks for implementing
the multipath searches in the optimization process, which is the principal difference
from evolution strategy. This mechanism presents the ﬁreworks algorithm optimiza-
tion capability and originality. This kind of multichange strategy can be applied to
other ECs for obtaining the improved optimization performance.
It is one of the main proposals to obtain the ﬁtness landscape to enhance ﬁreworks
algorithm performance, we use polynomial regression of one and two degree function
as the approximation model. From the evaluation result, we can conclude that our
proposed elite strategy by obtaining the ﬁtness landscape is an effective method to
enhance ﬁreworks algorithm search capability. Most of our used sampling methods
for approximating the ﬁtness landscape in projected one-dimensional space with
different sampling number can accelerate the search of ﬁreworks algorithm.
4.5.2 Approximation Methods
We use one and two degree polynomial function as the approximation model to
approximate ﬁtness landscape into linear and nonlinear search space. Experimental
results in Table4.2 suggests that two degree model is more effective to accelerate
the ﬁreworks algorithm search than the one degree model in most of the benchmark
functions. It is because that two degree model, i.e., nonlinear model can extend the
search space range than the linear model, where the ﬁreworks algorithm conducts a
search within the limited search space.
Nonlinear models (LS2) are better approximating the ﬁtness landscape for the ten
benchmark functions than linear models (LS1) from Table4.3. From our compara-
tive evaluations, nonlinear model shows its strong capability to extend the potential
global optima region in the search space. For most of the real-world EC applica-
tion, nonlinear models are the beneﬁcial regression model to approximate the ﬁtness
landscape both in approximation model accuracy and acceleration performance.
4.5.3 Sampling Methods
In the experimental evaluations, we investigate three sampling methods, i.e., best
sampling method, distance near the best ﬁtness individual sampling method, and
random sampling methods. For the most of the benchmark functions, the acceleration
performance with the best sampling method and random sampling are better than
the distance sampling.

4.5 Discussion
71
On the one hand, the spark with the better ﬁtness locate in the global optima
region, and the approximation model ﬁtted by those data can obtain the accurate
model for accelerating. On the other hand, the spark distance near the spark with
the best ﬁtness does not mean they are also close to the global optima region, and
the approximation accuracy and acceleration performance by distance sampling are
worse than that by the other sampling methods. From our comparative evaluation,
we found those two points.
Sampling methods cannot take effect in isolation, which have a better acceleration
performance only with the proper approximation methods. In our study, the proper
approximation methods are the two degree polynomial model, i.e., nonlinear model.
Best sampling method can enhance ﬁreworks algorithm search effectively for all
the benchmark functions except F10.
Distance near the best ﬁtness individual sampling method is effective in most
of the cases, except F5, F8, and F10. However, it needs more computational cost
in computing the distance, in the practical points of view, it is a useless sampling
method.
The random sampling method can accelerate all the benchmark functions with
nonlinear approximation model form Table4.2. The elite obtained by this method
can increase the population diversity. This is the new discovery from our study work.
Because of the random sampling method is with the less computational cost in the
sampling process than best and distance sampling methods, which need more search
and sort operations, it is the beneﬁcial sampling method to the application of the
proposed method in the practical points of view.
If the sampling data belong to a certain distribution, may be the acceleration
performance can be improved with the concrete ﬁtness landscape, the future research
will involve this direction.
4.5.4 Sampling Data Number
The sampling data number is crucial for obtaining an accurate approximation model
to accelerate the ﬁreworks algorithm search. Theoretically, more number of sampling
data means obtaining more accuracy approximation model and better acceleration
performance.
From Table4.1, the acceleration performance by the best sampling with LS2 and
ﬁve and ten sampling have better performance than that with three sampling data.
From Tables4.2 and 4.3, distance sampling method with 10 sampling data are
effective in F1, F2, F3, F4, F6, F7, F9, and F10, it can show the relationship between
sampling data number and acceleration performance, i.e., the more sampling number
means more better approximation and acceleration performance.
Sampling data number decides the computational cost in the approximation
regression computing, so we should select the proper sampling number to obtain
approximation model by balancing the computational cost and acceleration perfor-
mance for a concrete landscape or a real-world application.

72
4
FWA Based on Function Approximation Approaches
4.6 Summary
In the chapter, we investigated a series of methods to enhance the ﬁreworks algo-
rithm by elite strategy. There are three sampling methods, which we used in our
methods to approximate ﬁreworks algorithm ﬁtness landscape. We conduct a set of
comparative evaluations to study on the sampling method and number inﬂuence on
the approximation accuracy and acceleration performance. From our experimental
evaluations, there are ﬁve discoveries below.
(1) Our proposed elite strategy is an efﬁcient method to enhance ﬁreworks algorithm
search capability signiﬁcantly.
(2) Sampling method cannot take effect in isolation, it must be with a proper approx-
imation model to accelerate the FWA search for a certain benchmark function,
i.e., ﬁtness landscape.
(3) For some cases, the best sampling method and the random sampling method
have the same acceleration performance.
(4) Itcanbeobtainedbyaﬁtnesslandscapeapproximationaccuracyandacceleration
performance with the more sampling data number and a proper approximation
model. In this study, the better approximation model is the nonlinear model
(LS2) based on the experiments.
(5) In the practical points of view, the random sampling method is a better tool to
obtain the higher acceleration performance in both computational time and ﬁnal
solution quality.
References
1. Y. Pei, S. Zheng, Y. Tan, H. Takagi, An empirical study on inﬂuence of approximation
approaches on enhancing ﬁreworks algorithm, in Proceedings of the 2012 IEEE Congress
on System, Man and Cybernetics, (IEEE, 2012), pp. 1322–1327
2. Y. Pei, H. Takagi, A survey on accelerating evolutionary computation approaches. in 2011
International Conference of Soft Computing and Pattern Recognition (SoCPaR) (IEEE, 2011),
pp. 201–206
3. T. Ingu, H. Takagi, Accelerating a GA convergence by ﬁtting a single-peak function, in 1999
IEEE International Fuzzy Systems Conference Proceedings, FUZZ-IEEE’99, vol. 3 (IEEE,
1999) pp. 1415–1420
4. Y. Pei, H. Takagi, Accelerating evolutionary computation with elite obtained in projected
one-dimensional spaces. in 2011 Fifth International Conference on Genetic and Evolutionary
Computing (ICGEC), (IEEE, 2011), pp. 89–92
5. Y. Pei, H. Takagi, Comparative evaluations of evolutionary computation with elite obtained in
reduced dimensional spaces, in 2011 Third International Conference on Intelligent Networking
and Collaborative Systems (INCoS) (IEEE, 2011), pp. 35–40
6. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
7. A.N. Langville, C.D. Meyer, R. Albright, J. Cox, and D. Duling, Utilizing nonnegative matrix
factorization for e-mail classiﬁcation problems. In Survey of Text Mining III: Application and
Theory (Wiley New York, 2010) pp. 57–80

References
73
8. A. Janecek, Y. Tan, Iterative improvement of the multiplicative update NMF algorithm using
nature-inspired optimization in 2011 Seventh International Conference on Natural Computa-
tion (ICNC), vol. 3 (IEEE, 2011), vol. 3, pp .1668–1672
9. Y. Jin, A comprehensive survey of ﬁtness approximation in evolutionary computation. Soft
Comput. 9(1), 3–12 (2005)
10. Y. Jin, Surrogate-assisted evolutionary computation: recent advances and future challenges.
Swarm Evol. Comput. 1(2), 61–70 (2011)
11. Y. Pei, H. Takagi, Fourier analysis of the ﬁtness landscape for evolutionary search acceleration.
in 2012 IEEE Congress on Evolutionary Computation (CEC) (IEEE, 2012), pp. 1–7
12. Y. Pei, H.Takagi, Comparative study on ﬁtness landscape approximation with fourier transform.
in 2012 Sixth International Conference on Genetic and Evolutionary Computing (ICGEC)
(IEEE, 2012), pp. 400–403
13. P.N. Suganthan, N. Hansen, J.J. Liang, K. Deb, Y.-P. Chen, A. Auger, et al., Problem deﬁni-
tions and evaluation criteria for the CEC 2005 special session on realparameter optimization.
KanGAL Report 2005005 (2005)

Chapter 5
FWA with Controlling Exploration
and Exploitation
Since FWA was proposed in 2010 [1], it has shown its signiﬁcance and superiority
in dealing with the optimization problems. However, calculation of the number of
explosion sparks and the amplitude of ﬁrework explosion of FWA should dynami-
cally control the exploration and exploitation of searching space with iteration. The
mutation operator of FWA needs to generate the search diversity. This chapter pro-
vides a new method to calculate the number of explosion sparks and the amplitude of
ﬁrework explosions. By designing a transfer function, the rank number of a ﬁrework
is mapped to the scale of the calculation of scope and spark number of a ﬁrework
explosion. A parameter is used to dynamically control the exploration and exploita-
tion of FWA with iteration going on. In addition, this chapter uses a new random
mutation operator to control the diversity of FWA search.
5.1 Some Improvements on Operations in FWA
5.1.1 The Amplitude and Number of Sparks
In FWA, the ﬁtness of ﬁrework is needed when to compute the scope and sparks
number of ﬁrework explosion. In order to improve the calculation of the number
of ﬁrework sparks and the amplitude of ﬁrework explosion, we use the sequence
number of ﬁreworks to compute the two quantities. Therefore, a transfer function
must be designed to map the sequence number of ﬁreworks to function value that is
used to better calculate the amplitude and spark number of a ﬁrework explosion.
The transfer function is cited from the sigmoid function as the Eq.(5.1):
f (x) =
1
1 + ex .
(5.1)
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_5
75

76
5
FWA with Controlling Exploration and Exploitation
The function is further improved and is added a parameter a as the following
Eq.(5.2):
f (x) =
1
1 + e(x−1)/a ,
(5.2)
where a is a control parameter to change the shape of the above transfer function.
Equation(5.2)cantransferthesequencenumberofﬁreworkrankofﬁtnesstodifferent
values of function, which is used to calculate the spark number and amplitude of
ﬁrework explosion. The function of Eq.(5.2) is named as transfer function. When
the parameter a = 1, 5, 9, 13 and 21, the ﬁgures of the transfer function with different
parameter values are plotted as Fig.5.1.
From Fig.5.1, it can be found that the function ﬁtness of different sequence num-
bers are more and more mean as the parameter a is increasing. So, the calculating
number of explosion sparks is designed as
Sn = m
f (n)
N
n=1 f (n)
,
(5.3)
where m is the total of number of spark, n is the sequence number of a ﬁrework, and
Sn denotes the spark number of the nth ﬁrework explosion.
What is more, the calculating the amplitude of ﬁrework is designed as
An = A
f (N −n + 1)
N
n=1 f (N −n + 1)
,
(5.4)
where A is the maximum amplitude of ﬁrework explosion, n is the sequence number
of a ﬁrework, and An denotes the amplitude of the nth ﬁrework explosion.
In Eqs.(5.3) and(5.4), N is the total number of ﬁrework in FWA. The function
f (x) is the Eq.(5.2), whose parameter a is varied with the iteration from 20 to 1.
With variable parameter a, the explosion number of spark and explosion amplitude
of ﬁreworks are dynamically changed as the iteration goes on.
Figures5.2 and5.3 plot the number of sparks and amplitude of explosion of eight
ﬁreworks with iterations which are calculated by using Eqs.(5.3) and(5.4).
Figure5.2 shows that the number of spark generated by the ﬁrework of FWA with
the modiﬁed equation is very regular. For the ﬁreworks with better ﬁtness values, the
number of sparks is more and more with the iteration, while for the worse ﬁreworks,
the number of sparks is less and less. Figure5.3 shows that the amplitude of the
better ﬁrework explosion is smaller and smaller as the iteration goes on, while the
amplitude of bad ﬁrework explosion is bigger and bigger with iteration. The policy
can embody the idea of algorithm that a good ﬁrework has more number of generating
sparks with less amplitude of explosion, while a bad ﬁrework generates less number
of sparks with large amplitudes of explosion.
There might be a global optimal solution near the good ﬁrework. Hence, the
explosion of a good ﬁrework undertakes the local searching, while a bad ﬁrework
explosion undertakes the global exploration in a wider solution space. In the proposed

5.1 Some Improvements on Operations in FWA
77
0
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
f(x)
a=1
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
f(x)
a=5
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
f(x)
a=9
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
f(x)
a=13
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
f(x)
a=17
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
x
f(x)
a=21
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 5.1 The curves of transfer function with different parameters. Reprinted from Ref. [2], with
kind permission from Springer Science+Business Media

78
5
FWA with Controlling Exploration and Exploitation
0
200
400
600
800
1000
0
5
10
15
20
25
30
35
40
45
50
 iteration
the number of sparks
the 1st firework 
0
200
400
600
800
1000
0
2
4
6
8
10
12
14
16
18
20
 iteration
the number of sparks
the 2nd firework 
0
200
400
600
800
1000
0
5
10
15
 iteration
the number of sparks
the 3rd firework 
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
10
 iteration
the number of sparks
the 4th firework 
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
10
 iteration
the number of sparks
the 5th firework 
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
10
 iteration
the number of sparks
the 6th firework 
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
10
 iteration
the number of sparks
the 7th firework 
0
200
400
600
800
1000
0
1
2
3
4
5
6
7
8
9
10
 iteration
the number of sparks
the 8th firework 
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 5.2 The number of sparks for every ﬁrework in modifying FWA. Reprinted from Ref. [2],
with kind permission from Springer Science+Business Media

5.1 Some Improvements on Operations in FWA
79
0
200
400
600
800
1000
0
5
10
15
20
25
 iteration
the amplitude of explosion
the 1st firework 
0
200
400
600
800
1000
0
5
10
15
20
25
 iteration
the amplitude of explosion
the 2nd firework 
0
200
400
600
800
1000
0
5
10
15
20
25
 iteration
the amplitude of explosion
the 3rd firework 
0
200
400
600
800
1000
0
5
10
15
20
25
 iteration
the amplitude of explosion
the 4th firework 
0
200
400
600
800
1000
8
10
12
14
16
18
20
22
24
26
28
 iteration
the amplitude of explosion
the 5th firework 
0
200
400
600
800
1000
24
25
26
27
28
29
30
31
32
33
34
 iteration
the amplitude of explosion
the 6th firework 
0
200
400
600
800
1000
25
30
35
40
45
50
55
60
 iteration
the amplitude of explosion
the 7th firework 
0
200
400
600
800
1000
20
30
40
50
60
70
80
90
100
110
 iteration
the amplitude of explosion
the 8th firework 
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 5.3 The amplitude of explosion for every ﬁrework in modifying FWA. Reprinted from Ref.
[2], with kind permission from Springer Science+Business Media

80
5
FWA with Controlling Exploration and Exploitation
method, the dynamic change of number of sparks and the amplitude of explosion
with iteration can embody that the global exploration is done in the early time of
algorithm running, while the local exploitation is enhanced during the later time
of algorithm’s iteration. So, the new calculation of equations can better control the
exploitation and exploration of FWA with iteration.
5.1.2 The Mutation Improvement
To keep the diversity, FWA employed a Gaussian mutation to generate sparks. The
kth dimensions of the ith ﬁrework, xk
i , mutates as xk
i by the following equation:
xk
i = xk
i ∗Gaussian(1, 1).
(5.5)
However, the above mutation operation makes original FWA easily converged to
zero point of the search space, and it is difﬁcult for FWA to generate the diversity.
In order to increase the diversity of FWA, the random mutation is employed to make
the ﬁrework mutated. The mutation formula is
xk
i = xk
i + rand() ∗(XU B,k −X L B,k),
(5.6)
where xk
i denotes that the position of the kth dimensions of ith ﬁrework, X L B,k
denotes that the minimal bound of the kth dimensions of the ith ﬁrework, XU B,k
denotes that maximal bound of the kth dimensions of the ith ﬁrework. The function
rand() gains the sampling value in the interval [0, 1] with the uniform distribution.
5.1.3 Selection Strategy
Original FWA selects N locations for the next generation ﬁreworks by the Eqs.(5.7)
and(5.8):
R(xi) =

j∈K d(xi, x j) =

j∈K ||xi −x j||,
(5.7)
p(xi) =
R(xi)

j∈K R(xi),
(5.8)
where xi is the location of ith spark or ﬁrework, d(xi, x j) is the distance between
two sparks or ﬁreworks. K is the set of sparks and ﬁreworks generated in current
generation. The p(xi) is the probability which the ith ﬁrework or spark is selected
as ﬁrework of next generation.

5.1 Some Improvements on Operations in FWA
81
Equations(5.7) and(5.8) do not consider the ﬁtness of sparks of the ﬁreworks for
the selection of next generation ﬁreworks’ location. This is not in consistence with
the idea of Eqs.(5.3) and(5.4). Equations(5.3) and(5.4) use the sequence number
to sort the ﬁreworks’ ﬁtness and to calculate the sparks number and the amplitude
of ﬁreworks explosion, but Eqs.(5.7) and(5.8) of the original FWA do not consider
the ﬁtness to select the ﬁreworks’ location. Therefore, there are two methods to be
provided to modify the selection operator.
1. Fitness selection using a roulette
Like the original FWA, the best of the set will be selected ﬁrst. Then the others are
selected based on ﬁtness proportion using a roulette. So, the selection probability
of every spark or ﬁrework must be calculated by
p(xi) =
ymax −f (xi)

i∈k (ymax −f (xi)),
(5.9)
where ymax is the maximum value of the objective function among the set k which
consists of the ﬁreworks and sparks in the current generation. The other ﬁreworks
will be selected using the roulette according to the probability gained by Eq.(5.9).
2. Best ﬁtness selection
In Zheng et al. [3], used a random selection operator to replace the previous
time consuming one. It is as the following, when the algorithm determined the
number of ﬁreworks of every generation, all the sparks and ﬁreworks of the
current generation are sorted according to their ﬁtness and then the best sparks or
ﬁreworks with the best ﬁtness are selected as the location of next generation. The
method is very simple and in accordance with the new calculation of explosion
number and explosion amplitude of ﬁreworks in Eqs.(5.3) and(5.4).
5.2 Experiment and Analysis
5.2.1 Experimental Design
In order to evaluate the performance of the improved FWA, 14 benchmark functions
provided by CEC 2005 are employed [4]. These benchmark functions include ﬁve
unimodal functions and nine multimodal functions. The optimal ﬁtness of these
functions is not zero and is added bias. These functions are shifted and the optimal
locations are shifted to different locations from zero point in the solution space. More
details on the benchmark functions can be seen in [4].
In order to test the performance of improved FWA, the improved FWA with best
ﬁtness selection and random mutation (IFWABS), the improved FWA with the ﬁtness
selection using the roulette and random mutation (IFWAFS), the original FWA, and
the global PSO are compared with each other. The global PSO is employed the
decreasing weight w from 0.9 to 0.4 proposed in [5] and the neighbor particles of

82
5
FWA with Controlling Exploration and Exploitation
0
20
40
60
80
100
4
5
6
7
8
9
10
11
12
Number of Function Iteration
log(f(x)−f(x*))
Function 1  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
9
9.5
10
10.5
11
11.5
12
12.5
13
Number of Function Iteration
log(f(x)−f(x*))
Function 2  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
17
18
19
20
21
22
23
Number of Function Iteration
log(f(x)−f(x*))
Function 3  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
9.5
10
10.5
11
11.5
12
12.5
13
Number of Function Iteration
log(f(x)−f(x*))
Function 4  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
9
9.2
9.4
9.6
9.8
10
10.2
10.4
10.6
10.8
11
Number of Function Iteration
log(f(x)−f(x*))
Function 5  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
12
14
16
18
20
22
24
26
Number of Function Iteration
log(f(x)−f(x*))
Function 6  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
8.4
8.5
8.6
8.7
8.8
8.9
9
9.1
9.2
9.3
9.4
Number of Function Iteration
log(f(x)−f(x*))
Function 7  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
3.025
3.03
3.035
3.04
3.045
3.05
3.055
3.06
3.065
3.07
Number of Function Iteration
log(f(x)−f(x*))
Function 8  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 5.4 Convergence curves on the benchmark functions. Reprinted from Ref. [2], with kind
permission from Springer Science+Business Media

5.2 Experiment and Analysis
83
0
20
40
60
80
100
4.4
4.6
4.8
5
5.2
5.4
5.6
5.8
6
6.2
6.4
Number of Function Iteration
log(f(x)−f(x*))
Function 9  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
5.5
6
6.5
7
Number of Function Iteration
log(f(x)−f(x*))
Function 10  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
3.4
3.5
3.6
3.7
3.8
3.9
4
Number of Function Iteration
log(f(x)−f(x*))
Function 11  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
11.5
12
12.5
13
13.5
14
14.5
15
Number of Function Iteration
log(f(x)−f(x*))
Function 12  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
2
3
4
5
6
7
8
Number of Function Iteration
log(f(x)−f(x*))
Function 13  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
0
20
40
60
80
100
2.59
2.6
2.61
2.62
2.63
2.64
2.65
2.66
2.67
2.68
Number of Function Iteration
log(f(x)−f(x*))
Function 14  dim: 30  FES:1+5e
PSO
FWA
IFWAFS
IFWABS
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 5.5 Convergence curves on the benchmark functions. Reprinted from Ref. [2], with kind
permission from Springer Science+Business Media

84
5
FWA with Controlling Exploration and Exploitation
each particle is all particles. The particle population size is 100. The factor c1 and c2
of PSO are set as 2. The FWA and the improved FWA set the number of ﬁreworks as
10, the total number of explosion sparks S as 80, and the amplitude of explosion A
as the range length of problem space. The experiment is conducted in Matlab 2012b
and executed in windows 7.
5.2.2 Experimental Results and Analysis
The experiment is conducted to compute the mean error ﬁtness ( f (x)−f (x∗) f (x∗)
is the real optimal ﬁtness of the benchmark functions), standard square error and
the best error ﬁtness in the 25 run on the 14 benchmark functions. Each run of
all algorithm is evaluated l000, 10000, 10000 and D ∗10000 (D is the dimension
of benchmark function), respectively. Each algorithm will be conducted in the 10
dimensions and 30 dimensions, respectively.
Compared to PSO, improved FWA exhibits more optimal performance in most
of the functions, especially in 100000 Fitness Evaluated number (FEs) and 300000
FEs. As the FEs are more and more, the performance of improved FWA is much
better than PSO that it matches with PSO.
Figures5.4 and5.5 plot the convergence process for four algorithms to optimize
the 14 functions with 300000 FEs in 30 dimensions. These ﬁgures visually illustrate
the effect of four algorithms that improved FWA is excel to original FWA and can
match with PSO.
5.3 Summary
This chapter modiﬁed the calculation of scope and amplitude of ﬁrework explosion,
and designed a transfer function to map the rank number of ﬁrework ﬁtness to allocate
the total number of sparks and explosion amplitude. A parameter in the transfer
function was used to control the dynamic calculation of two values with iterations.
In order to accord with the new idea of calculation of scope and amplitude of ﬁrework
explosion, the best spark selection and ﬁtness selection were employed to improve
the selection operator of FWA at last.
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. J. Liu, S. Zheng, Y. Tan, The improvement on controlling exploration and exploitation of ﬁrework
algorithm. Advance in Swarm Intelligence, vol. 7928 (Springer 2013), pp.11–23

References
85
3. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 IEEE Congress on Evo-
lutionary Computation (CEC) (IEEE, 2013), pp. 2069–2077
4. P.N. Suganthan, N. Hansen, J.J. Liang, K. Deb, Y.-P. Chen, A. Auger, et al., Problem deﬁni-
tions and evaluation criteria for the CEC 2005, special session on real-parameter optimization
KanGAL Report 2005005 (2005)
5. Y. Shi, R. Eberhart, A modiﬁed particle swarm optimizer, in The 1988 IEEE International
Conference on Evolutionary Computation Proceedings, 1998. IEEE World Congress on Com-
putational Intelligence (IEEE 1988), pp. 69–73

Chapter 6
Enhanced Fireworks Algorithm
In this chapter, we present an improved version of the recently developed Fireworks
Algorithm (FWA) based on several modiﬁcations, according to our previous work
“Enhanced Fireworks Algorithm” [1]. A comprehensive study on the operators of
conventional FWA revealed that the algorithm works surprisingly well on benchmark
functions which have their optimum at the origin of the search space. However, when
being applied on shifted functions, the quality of the results of conventional FWA
deteriorates severely and worsens with increasing shift values, i.e., with increasing
distance between function optimum and origin of the search space. Moreover, com-
pared to other meta-heuristic optimization algorithms, FWA has high computational
cost per iteration. In order to tackle these limitations, we present ﬁve major improve-
ments on FWA: (i) a new minimal explosion amplitude check, (ii) a new operator
for generating explosion sparks, (iii) a new mapping strategy for sparks which are
out of the search space, (iv) a new operator for generating Gaussian sparks, and (v)
a new operator for selecting the population for the next iteration.
The resulting algorithm is called Enhanced Fireworks Algorithm (EFWA). Exper-
imental evaluation on 12 benchmark functions with different shift values shows that
EFWA outperforms conventional FWA in terms of convergence capabilities, while
reducing the runtime signiﬁcantly.
6.1 Properties of Conventional FWA
As already mentioned in the original FWA [2], FWA outperformed SPSO and CPSO
signiﬁcantly and converged in most cases toward the function optimum already after a
few iterations. However, when applying FWA on shifted functions the results worsen
progressively with increasing distance between function optimum and origin of the
search space. By investigating the operators of FWA, we found that some of them
create [map] sparks at [to] locations which are close to the origin of the search space,
independent of the function optimum. This behavior is mostly caused by the mapping
operator and the Gaussian sparks operator. In terms of runtime we found that the
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_6
87

88
6
Enhanced Fireworks Algorithm
cost per iteration of FWA is signiﬁcantly higher than that of most other optimization
algorithms. In Sect.6.2 we analyze all operators of FWA in detail and point out which
operators are responsible for its actual behavior and its high computational cost. In
summary, conventional FWA has the following drawbacks:
(i) For functions which have their optimum at the origin, FWA will ﬁnd the optimal
solution very fast. However, not due to the intelligence of the algorithm but due
to the speciﬁc mapping and Gaussian mutation operators which map/create
sparks close to the origin.
(ii) For functions which have their optimum far away from the origin, FWA has
to face the two drawbacks that the mapping operator rebounds most solutions
which are out of the search space to locations which are far away from the func-
tion optimum, and that the mutation operator creates many sparks at locations
close to the origin (i.e., again far away from the optimum).
(iii) FWA has a high computational cost per iteration.
6.2 The Proposed EFWA
In this section, we present the new operators of EFWA in detail after indicating the
limitations of each operator in conventional FWA.
6.2.1 A New Minimal Explosion Amplitude Check (MEAC)
Equation(2.3) shows how the explosion amplitude for each ﬁrework is calculated
in conventional FWA. A ﬁrework with better ﬁtness will have a smaller explosion
amplitude while a ﬁrework with lower ﬁtness has a larger explosion amplitude.
Although this idea seems reasonable, the explosion amplitude of the ﬁreworks having
the best (or a very good) ﬁtness will usually be very small, i.e., close to 0 according
to Eq.(2.3). If the explosion amplitude is [close to] zero, the explosion sparks will be
located at [almost] the same location as the ﬁrework itself. As a result, it may happen
that the location of the best ﬁrework cannot be improved until another ﬁrework has
found a better location. In order to avoid this problem, we introduce a lower bound
Amin of the explosion amplitude, which is based on the progress of the algorithm.
During the early phase of the search, Amin is set to a higher value in order to facilitate
exploration, with increasing number of evaluations, Amin is decreased in order to
allow for better exploitation capabilities around good locations. For each dimension
k, the explosion amplitude Ak
i is bound as follows:
Aik =

Amin,k
if Ai,k < Amin,k,
Aik
otherwise,
(6.1)

6.2 The Proposed EFWA
89
(a)
(b)
Fig. 6.1 Linearly and nonlinearly decreasing minimal explosion amplitude. a Linear decrease.
b Nonlinear decrease
A new value for Amin is calculated in each iteration. In this work, we use two different
modes to calculate Amin. The ﬁrst approach is based on a linearly decreasing function
(cf. Eq.(6.2)), and the other approach is based on a nonlinearly decreasing function
(cf. Eq.(6.3)).
Amin,k(t) = Ainit −Ainit −Aﬁnal
evalsmax
∗t,
(6.2)
Amin,k(t) = Ainit −Ainit −Aﬁnal
evalsmax

(2 ∗evalsmax −t)t,
(6.3)
In both equations, t refers the number of function evaluation at the beginning of
the current iteration, and evalsmax is the maximum number of evaluations. Ainit and
Aﬁnal are the initial and ﬁnal minimum explosion amplitude, respectively. Compared
to the linear decrease of Amin, the nonlinear decrease enhances exploitation already
at an earlier stage of the algorithm (i.e., after fewer iterations). Figure6.1 shows a
graphical representation of Eqs.(6.2) and (6.3).
6.2.2 A New Operator for Generating Explosion Sparks
In FWA, the offset displacement for explosion sparks of each ﬁrework is only cal-
culated once and the same value is added to the location of selected dimensions.
Obviously, adding the same value in each dimension leads to a bad local search
ability. To avoid this problem, we calculate a different offset displacement for se-
lected dimensions (those dimensions, where zk equals 1). Algorithm6.1 shows the
proposed process of generating explosion sparks in EFWA.
Figure6.2 shows the difference between the generation of explosion sparks in
FWA and EFWA. As discussed before, the offset displacement in FWA is similar
for all selected dimensions, in EFWA a different offset displacement is calculated in
each dimension.

90
6
Enhanced Fireworks Algorithm
(a)
(b)
Fig. 6.2 Generation of explosion sparks in FWA and EFWA. a FWA. b EFWA
Algorithm 6.1 Generating “explosion sparks” in EFWA.
1: Initialize the location of the “explosion sparks”: xi = Xi,
2: Set zk = round(U(0, 1)), k = 1, 2, ..., D.,
3: for each dimension of xik, where zk == 1 do
4:
Calculate offset displacement: △Xik = Ai × U(−1, 1),
5:
xik = xik + △Xik,
6:
if xik out of bounds then
7:
map xik to the potential space (see next subsection),
8:
end if
9: end for
6.2.3 A New Mapping Operator
In conventional FWA, when the location of a new spark exceeds the search range
in dimension k, the new spark will be mapped to another location according to the
following equation:
xik = X L B,k + |xik| % (XU B,k −X L B,k),
(6.4)
where XU B,k X L B,k denotes the upper search bound and lower search bound in the
feasible range. In many cases, a spark will exceed the allowed search space only
by a rather small value. Moreover, as the search space is often equally distributed
(X L B,k ≡−XU B,k), the adjusted position xik will be very close to the origin in
many cases. The following example is used to explain this comment: Consider an
optimization problem within the search space [−20, 20]. If, in dimension k, a new
spark is created at the point xik = 21, it will be mapped to the location xik =
−20 + |21| % (40). Since the result of the modulo operation 21 % (40) = 21, xik
will be mapped to the location xik = 1, which is already very close to the origin.

6.2 The Proposed EFWA
91
In cases where X L B,k ≡−XU B,k, this mapping operator is partly responsible for
drawbacks (i) and (ii) as mentioned in Sect.6.1. In order to avoid the problems caused
by the conventional mapping operator we replace this method with a uniform random
mapping operator, which maps the sparks to any location in the search space with
uniform distribution.
6.2.4 A New Operator for Generating Gaussian Sparks
Together with the mapping operator, the Gaussian mutation operator is the main
reason why conventional FWA works signiﬁcantly better than other optimization
algorithms for functions which have their optimum at the origin (cf. the results in
[2]). Figure6.3a, b show the location of the Gaussian sparks for a two-dimensional
Ackley function with the optimum at [0, 0] and [−70, −55], respectively. In each
iteration, the location of (only) the Gaussian sparks is plotted and not deleted. The
location of the Gaussian sparks in Fig.6.3a indicates that most sparks are located at
the origin, i.e., close to the optimum. Moreover, it can be seen that the areas close
to the coordinate axes are also more crowded than other parts of the search space.
Figure6.3b reveals an interesting fact about the location of the Gaussian sparks for the
shifted Ackley function. Even though the optimum is now far away from the origin,
many sparks are located close to the center. Obviously, many sparks in Fig.6.3a are
not located close to the center because of the intelligence of the algorithm, but rather
because many Gaussian sparks are created near to the origin of the search space,
independent of the location of the function optimum. We point out that Fig.6.3a, b
were created using the mapping operator of conventional FWA.
Fig. 6.3 The locations of the Gaussian sparks using the conventional FWA (Ackely function using
100000 function evaluations). a No shift—optimum at origin. b Shift—optimum at (−70, −55)

92
6
Enhanced Fireworks Algorithm
The reason for this behavior is the calculation of the Gaussian sparks as shown in
the following equation:
xik = Xik ∗g,
(6.5)
Here, g is set to a random value from a normal distribution with expected value
and variance both set to 1. In cases where g is close to 0, xik will be close to 0 as well.
As a result, many Gaussian sparks will be located close to the origin of the search
space in dimension k. Moreover, for large g, many Gaussian sparks are created at
locations which are out of bounds. In this case, the mapping operator of conventional
FWA will map the newly created spark to a location which is in many cases close to
the origin. Another problem of the conventional Gaussian sparks operator is the fact
that ﬁreworks which are already located close to the origin of the search space cannot
escape from this location; if ﬁrework Xik is close to zero, the location of spark xik
be close to zero as well, since xik = Xik ∗g.
Initialization
Since conventional FWA is able to converge toward the optimum already after very
few function evaluations (we again refer to the result in [2]), we also analyzed the
behavior of FWA during the ﬁrst iteration. Figure6.4a–d shows the distribution of
the explosion and Gaussian sparks, respectively, directly after the initialization with
different initialization ranges. We repeatedly initialized the ﬁreworks, created the two
types of sparks, andplottedtheir locations until wereached5000functionevaluations
(around 100 repetitions). The distribution of the sparks is (expectedly) independent
of the function optimum, and shows a similar behavior for different initialization
ranges, which were set to dim1: [15,30]; dim2: [15,30] for Fig.6.4a, b and dim1:
[60,75]; dim2: [30,45] for Fig.6.4c, d. Obviously, some Gaussian sparks are located
very close to the origin of the function, independent of the initialization range. This
is another indication why conventional FWA is able to ﬁnd the optimum of centered
functions within a few iterations.
The New Gaussian Spark Operator
In order to avoid the problems of the conventional Gaussian mutation operator, we
propose a new Gaussian mutation operator which is computed by
xik = Xik + (X Bk −Xik) × e,
(6.6)
where, X B is the location of the currently best ﬁrework/explosion spark found so far,
and e = N(0, 1). Details are given in Algorithm6.2.
As shown in Fig.6.5, the new mutation operator will stretch out along the direction
between the current location of the ﬁrework and the location of the best ﬁrework.
This ensures diversity of the search but also involves some global movement toward
the best location found so far. This new operator only involves a movement toward
the origin of the search space if the currently best ﬁrework is located at the origin.

6.2 The Proposed EFWA
93
Fig. 6.4 Sparks after initialization in conventional FWA. a No shift—optimum at origin. b Shift—
optimum at (−70,−55). c No shift—optimum at origin. d Shift—optimum at (−70,−55)
Algorithm 6.2 Generating “Gaussian sparks” in EFWA
1: Initialize the location of the “Gaussian sparks”: xi = Xi
2: Set zk = round(U(0, 1)), k = 1, 2, ..., D
3: Calculate offset displacement: e = N(0, 1)
4: for each dimension xik, where zk == 1 do
5:
xik = xik + (X Bk −xik) ∗e, where X B is the position of the best ﬁrework found so far.
6:
if xik out of bounds then
7:
mapping the position according to Eq. (6.4)
8:
end if
9: end for

94
6
Enhanced Fireworks Algorithm
Fig. 6.5 Difference between the Gaussian sparks operator in FWA and EFWA
6.2.5 A New Selection Operator
FWA involves a distance-based selection strategy which favors to select ﬁre-
works/sparks in less crowded regions of the search space (cf. Eq.(2.9)). Although
selecting locations in low crowded regions with higher probability increases diver-
sity, this selection operator has the drawback of being computational very expensive.
A runtime proﬁling of the original FWA code revealed that the selection operator of
conventional FWA is responsible for the majority of the runtime. In order to speed
up the selection process of the population for the next generation, we apply another
selection method, which is referred to as Elitism-Random Selection (ERP) method
[1, 3]. In this selection process, the optima of the set will be selected ﬁrst. Then, the
other individuals are selected randomly. Obviously, the computational complexity
of ERP is only linear with respect to the number of ﬁreworks, and therefore reduces
the runtime of EFWA signiﬁcantly.
6.3 Experiments
In order to investigate the performances of the new operators, we compare conven-
tional FWA not only to the newly proposed Enhanced Fireworks Algorithm (EFWA),
but also three variants of EFWA (denoted as eFWA-X) which can be regarded as a hy-
bridization of FWA and EFWA. While EFWA uses all newly proposed operators, the
eFWA-X variants use only some of them. Table6.1 summarizes which of the new op-
erators are used by the different algorithms. The abbreviations of the operators refer
to EXP: new operator for generating explosion sparks; MAP: new mapping operator;
GAU: new operator for generating Gaussian sparks; AMP1: minimal explosion am-

6.3 Experiments
95
Table 6.1 Algorithms and new operators
EXP
MAP
GAU
AMP1
AMP2
SEL
FWA
◦
◦
◦
◦
◦
◦
eFWA-I
✓
✓
✓
◦
◦
◦
eFWA-II
✓
✓
✓
✓
◦
◦
eFWA-III
✓
✓
✓
◦
✓
◦
EFWA
✓
✓
✓
◦
✓
✓
plitude check linear decrease; AMP2: minimal explosion amplitude check nonlinear
decrease; SEL: new selection operator. E.g., eFWA-I uses the new operators EXP,
MAP and GAU, the new operator AMP is not included, and the selection operator is
taken from conventional FWA. Besides comparing different FWA variants with each
other, Standard PSO (SPSO) is used for performance comparison.
6.3.1 Experimental Setup
Twelve functions are selected as a test suite. Table6.1 illustrates the names, numbers,
search space (Range), optimal locations (Opt x), ﬁtness at the optimal location
(Opt f (x)), and dimensions (Dim.). For each function, the initial range is set to
[XU B,k/2, XU B,k], where XU B,k is the upper bound of the search space in the kth
dimension. In the experiments, a number of shift values are added to these basic
functions in order to shift the global optimum. We used seven different shift indexes
in order to analyze the inﬂuence of different shift values on the performance of the
algorithms (see Table6.2). For each shift value SI, the position of the function will
be shifted (in each dimension) by the corresponding shift value SV. If SI is equal
to zero, the function is not shifted. For example, for function f1 and an SI of 6, the
function will be shifted in each dimension by 0.7 ∗((100 −(−100))/2) = 70, while
the search range remains unaffected.
For all experiments, Ainit and A f inal (Eqs.6.2 and 6.3) are set to (XU B,k −
X L B,k) × 0.02 and (XU B,k −X L B,k) × 0.001, respectively. All other parameters of
(E)FWAaretakenfrom[2],andSPSOparametersaretakenfrom[4].Asexperimental
platform we used MATLAB2011b, running Win7 on an Intel Core i7-2600 CPU;
Table 6.2 Shift index (SI) and shift value (SV)
Shift
index
Shift value
Shift
index
Shift value
Shift
index
Shift value
1
0.05 ∗XU B,k−XL B,k
2
2
0.1 ∗XU B,k−XL B,k
2
3
0.2 ∗XU B,k−XL B,k
2
4
0.3 ∗XU B,k−XL B,k
2
5
0.5 ∗XU B,k−XL B,k
2
6
0.7 ∗XU B,k−XL B,k
2
A shift index of zero indicates that the function is not shifted

96
6
Enhanced Fireworks Algorithm
3.7GHZ; 8GBRAM. Comparedtoconventional FWA, theonlyadditional parameters
of EFWA are Ainit and A f inal, which can be ﬁxed as fractions of the search space.
6.3.2 Experimental Results
In this section, we ﬁrst evaluate the inﬂuence of the newly proposed operators pre-
sented in Sect.6.2. After that, we compare EFWA with conventional FWA and with
SPSO, our baseline reference. The ﬁnal results after 300000 function evaluations are
presented for all 12 benchmark functions in Table6.3 as mean ﬁtness and standard
deviation over 30 runs.
Evaluation of EXP, MAP and GAU. Since these three1 operators are responsible
for the fact that conventional FWA performs better on function which have their
optimum at the origin of the search space than on functions whose optimum is far
away from the center (cf. Sect.6.1), all eFWA variants use the new operators EXP,
MAP and GAU. As expected, when SI = 0 conventional FWA reaches the optimum
of all functions that have their optimum at 0.0D (marked in red font in Table6.3).
However, as already mentioned, this good performance is not due to the intelligence
of the algorithm but rather because the Gaussian explosion operator and the mapping
operator of conventional FWA create/map many sparks to locations which are very
close to the search space. Indeed, Table6.3 reveals that when SI = 0, conventional
FWA only fails to ﬁnd the optimum of two function, f3 and f7—both functions have
their optimum at 1.0D, and thus not at the origin of the search space. With increasing
SI, the results of FWA worsen signiﬁcantly for most functions, especially so for f1,
f2, f5, f7, f11, f12. When comparing the results of conventional FWA with eFWA-I
(using the new operators EXP, MAP, GAU), it can be seen that changing these three
operators alone does not improve the performance of the algorithm. Although the
results are more stable with respect to different shift values, in most cases they are
worse than the results of conventional FWA. In the next paragraph, we will discuss
how the AMP operator is able to improve eFWA-I.
Evaluation of AMP. Table6.3 reveals that the minimal explosion amplitude check
strategy (AMP) is crucial for improving the diversity of the ﬁreworks. The difference
between the results of eFWA-I, which does not use an explosion amplitude check,
and eFWA-II (linear decrease of Ak
min) and eFWA-III (nonlinear decrease of Ak
min)
are obvious. Both, eFWA-II and eFWA-III, clearly outperform eFWA-I. Comparing
the results of eFWA-II and III, it can be seen that eFWA-III achieves slightly better
results. Hence, the nonlinear decrease of the minimum explosion amplitude Ak
min is
preferred over the linear decrease of Ak
min, and will be used in EFWA.
Evaluation of SEL. Compared to eFWA-III, EFWA replaces the time consuming
distance-based selection operator with the new selection operator (cf. Sect.6.2.5). In
1We note that the questionable part of the explosion sparks operator of conventional FWA is the
mapping operation, not the explosion operation itself.

6.3 Experiments
97
Table 6.3 Mean and variance (in parenthesis) of all 12 benchmark functions used in the experiments (SI = shift index)
SI
Alg.
f1
f2
f3
f4
f5
f6
f7
f8
f9
f10
f11
f12
0
SPSO
0
3.712886
9.025685
18.8362
0.000387
162.4213
0
−1.03163 3.000000
0.000000
0
0
FWA
0
0
18.06771
0
0
0
0.013432
−1.03163 3.000001
0.000000
0
0
eFWA-I
1751.919
7097.559
4622089
5.747068
1.123267
172.9351
3961796
−1.03162 3.000035
0.005529
388.1999
20224.25
eFWA-II
0.166553
1.065167
107.821
0.150611
0.273287
145.9337
0.006831
−1.03163 3.000000
0.000000
0.007313
1.139661
eFWA-III 0.082755
0.313803
91.52661
0.082682
0.143627
144.4078
0.002717
−1.03163 3.000000
0.000000
0.002982
0.501991
EFWA
0.080583
0.327939
110.6504
10.20939
0.134513
128.0998
0.00354
−1.03163 3.000000
0.000000
0.003003
0.477865
1
SPSO
0
3.875658
10.29035
19.73264
0.000777
168.5133
0
−1.03163 3.000000
0.000000
0
0
FWA
0.235929
62.19819
1.368985
0.157826
0.10509
4.260847
0.012899
−1.03163 3.000007
0.001946
0.017149
2.086486
eFWA-I
1683.079
7163.763
4675799
5.595889
1.148833
174.0394
3888660
−1.03162 3.000032
0.005156
363.8366
20400.76
eFWA-II
0.166322
0.934341
123.2805
0.125657
0.26553
149.7567
0.006147
−1.03163 3.000000
0.000000
0.006905
1.120488
eFWA-III 0.082681
0.301201
108.301
0.127529
0.13763
146.8661
0.00256
−1.03163 3.000000
0.000000
0.003115
0.486991
EFWA
0.07894
0.303336
129.4147
14.78846
0.131399
133.1501
0.002475
−1.03163 3.000000
0.000000
0.002899
0.47585
2
SPSO
0
5.589772
11.93113
19.80728
0.000483
172.6362
0
−1.03163 3.000000
0.000000
0
0
FWA
0.661069
227.831
88.48228
0.854293
0.210448
6.658164
0.068989
−1.03163 3.000005
0.004376
0.046928
5.556603
eFWA-I
1712.366
7540.234
4624418
5.77933
1.206808
178.2714
3171013
−1.03162 3.000043
0.005719
361.0415
19108.3
eFWA-II
0.168955
0.933509
112.6466
0.123641
0.263951
147.7272
0.006349
−1.03163 3.000000
0.000000
0.006618
1.146244
eFWA-III 0.083509
0.303672
124.3955
0.140883
0.135765
163.6453
0.002603
−1.03163 3.000000
0.000000
0.002971
0.53305
EFWA
0.07936
0.319263
137.1596
17.50123
0.13835
133.8353
0.003588
−1.03163 3.000000
0.000000
0.003048
0.487051
3
SPSO
0
9.897378
10.72043
19.91163
0.000918
164.4082
0
−1.03163 3.000000
0.000000
0
0
FWA
1.495644
643.9331
139.0133
1.417975
0.300615
4.434904
0.213188
−1.03163 3.000014
0.006803
0.144981
14.3453
eFWA-I
1700.841
7237.548
5578362
6.128872
1.304753
183.6675
3144835
−1.03162 3.000055
0.004777
371.1394
20822.16
eFWA-II
0.16799
0.962124
104.893
1.141007
0.269353
144.2484
0.006098
−1.03163 3.000000
0.000000
0.00725
1.147999
eFWA-III 0.082844
0.317519
107.7358
2.909734
0.140745
153.2287
0.002806
−1.03163 3.000000
0.000000
0.002972
0.507572
EFWA
0.079119
0.32688
124.8834
19.53764
0.135506
121.61
0.00263
−1.03163 3.000000
0.000000
0.00286
0.479914
(continued)

98
6
Enhanced Fireworks Algorithm
Table 6.3 (continued)
SI
Alg.
f1
f2
f3
f4
f5
f6
f7
f8
f9
f10
f11
f12
4
SPSO
0
17.95454
11.89914
19.95621
0.001015
153.9394
0
−1.03163 3.000000
0
0
0
FWA
2.353416
1722.845
267.3387
1.784102
0.354713
8.558857
0.353507
−1.03163 3.000015
0.005517
0.235881
26.67778
eFWA-I
1740.594
6539.821
5085283
8.49947
1.403223
186.7392
3024996
−1.03162 3.000059
0.004584
391.184
19512.98
eFWA-II
0.176647
0.941755
130.0864
13.86739
0.260521
139.4024
0.006635
−1.03163 3.000000
0.000000
0.006542
1.051908
eFWA-III 0.084896
0.316917
128.5722
12.17068
0.137188
149.618
0.003242
−1.03163 3.000000
0.000000
0.003134
0.491694
EFWA
0.077153
0.313821
109.0857
19.97647
0.135921
125.1156
0.002854
−1.03163 3.000000
0.000000
0.003035
0.492438
5
SPSO
0
40.33175
13.87386
19.98846
0.00116
119.374
0.000215
−1.03163 3.000000
0.000000
0
0
FWA
3.337647
6204.057
429.1847
2.187983
0.375887
8.765354
0.582387
−1.03163 3.000033
0.005833
0.413908
42.90086
eFWA-I
1637.047
6123.058
6141640
14.33566
1.452845
212.6149
3064550
−1.03162 3.000151
0.005151
435.9372
18802.31
eFWA-II
0.168983
1.024643
145.3226
20.01804
0.263803
165.1913
0.006224
−1.03163 3.000000
0.000000
0.00708
1.090932
eFWA-III 0.085402
0.299152
124.9419
19.93473
0.142257
175.968
0.002433
−1.03163 3.000000
0.000000
0.002926
0.521624
EFWA
0.081641
0.316406
187.6413
20.0044
0.132645
143.9208
0.002553
−1.03163 3.000000
0.000000
0.002859
0.484936
6
SPSO
0
146.8231
16.32905
19.99701
0.005713
118.2048
0
−1.03163 30.00000
0.000000
0
0
FWA
3.445464
7729.23
941.3545
2.396496
0.348149
9.385204
0.8201
−1.03163 30.00243
0.007451
0.416452
46.33276
eFWA-I
1710.271
5362.155
17299855 19.97842
1.455054
248.5832
3939765
−1.03162 30.00258
0.005344
637.0806
21882.77
eFWA-II
0.177206
0.890351
182.0592
20.02658
0.259169
202.2376
0.005894
−1.03163 30.00000
0.000000
0.006879
1.130264
eFWA-III 0.080248
0.323097
168.1323
20.01205
0.142437
201.9406
0.002692
−1.03163 30.00000
0.000000
0.003025
0.471663
EFWA
0.077676
0.309704
133.5418
20.01094
0.133103
165.2688
0.002882
−1.03163 30.00000
0.000000
0.002854
0.486914

6.3 Experiments
99
terms of convergence, ﬁnal ﬁtness and standard deviation, there is almost no differ-
ence between the selection operators. The results of eFWA-III and EFWA are almost
identical for all functions (cf. Table6.3). The only exceptions are function f4, with
advantages for the distance-based selection operator (eFWA-III), and f6, with advan-
tages for the elitism-random selection operator (EFWA). In terms of computation cost
the picture is different. The new selection operator decreases the runtime of EFWA
drastically, as shown in Fig.6.6 for the function f10. As can be seen, the runtime
of EFWA is much shorter than the runtimes of conventional FWA or the eFWA-X
variants, which all use the conventional distance-based selection operator. We note
that the fractions of runtimes for all other functions are very similar to Fig.6.6.
EFWA versus conventional FWA. A comparison between all eFWA-X variants
and EFWA reveals that EFWA is the best algorithm in terms of convergence, ﬁnal
result, and runtime. Hence, in the remaining parts of this section we use EFWA
for comparison with conventional FWA and with the baseline algorithm SPSO. To
show whether the improvement of EFWA over conventional FWA is signiﬁcant or
not, a number of t-tests were conducted and the recorded p-values are given in
Table6.4. The null hypothesis that EFWA achieves similar mean results than FWA
is tested against the alternative that EFWA achieves better mean results over FWA.
If p < 0.05, and the mean results of EFWA are lower than the results of FWA, the
null hypothesis will be rejected and the test will be reported as signiﬁcant; otherwise
it will be reported as not signiﬁcant. Hence, bold values in Table6.4 indicate that the
improvement of EFWA is signiﬁcant for the corresponding function/SI combination.
We draw the following conclusions from Tables6.3 and 6.4.
0
1
2
3
4
5
6
0
5
10
15
20
25
Shift Index
Time (s)
SPSO
FWA
FWA−I
FWA−II
FWA−III
EFWA−I
EFWA−II
Fig. 6.6 Runtime of FWA, eFWA-X and SPSO

100
6
Enhanced Fireworks Algorithm
Table 6.4 t-test results for EFWA versus conventional FWA
EFWA
SI = 0
SI = 1
SI = 2
SI = 3
SI = 4
SI = 5
SI = 6
f1
0
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
f2
0
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
f3
0.000006
0
0.005094
0.141752
0.000003
0.000119
0.000000
f4
0
0
0
0
0
0
0
f5
0
0.000175
0.000000
0.000000
0.000000
0.000000
0.000000
f6
0
0
0
0
0
0
0
f7
0.007443
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
f8
0.322126
0.322126
0.083202
0.047784
0.000530
0.000003
0.000005
f9
0.026484
0.000003
0.006833
0.000003
0.000003
0.000001
0.000000
f10
NaN
0.002984
0.000003
0.000000
0.000000
0.000000
0.000000
f11
0
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
f12
0
0.000000
0.000000
0.000000
0.000000
0.000000
0.000000
Functions are not shifted (i.e, SI = 0): FWA achieves better results than EFWA for
all functions which have their optimum at the origin, and for function f3 which has
the optimum at 1.0D. For the other functions which have their optimum at a different
location than the origin ( f7, f8, and f9), we can see that EFWA can improve the
results of FWA for functions f7 and f9. For function f8, both algorithms achieved
similar results, however, with smaller variance for EFWA.
Functions are shifted (i.e., SI! = 0): With increasing SI, EFWA is able to improve
the results of FWA progressively. The results of EFWA are much more stable with
respect to increasing SI for most functions. The results in Table6.4 indicate that the
improvement of EFWA over conventional FWA is signiﬁcant for almost all functions,
when SI is increased. However, we also note that for the functions f4 and f6, FWA
has very stable results even for increasing SI. Surprisingly, for these functions FWA
outperforms EFWA. For function f4. As can be seen, FWA is able to improve its
results continuously over the full duration of the algorithm. Although the results
worsen with increasing SI, the results are superior to all other algorithms, which
often get stuck in local minima for larger SI.
EFWA versus SPSO. Compare the results from Table6.3, it can be seen that for
function f2, the results of SPSO are generally worse than the results of EFWA. For
f4 and f6, for small SI, EFWA gains better performance than the ones of SPSO
while for large SI, SPSO gains better results. For f8, f9, and f10, both EFWA and
SPSO achieve very good performance. But for the rest functions, SPSO achieves
better results compared with EFWA, and it also can be seen that for most functions,
EFWA has the local search problem and can not reach the global optimum with high
precision.

6.3 Experiments
101
6.3.3 Experimental Observations
From the results of our experimental evaluation, we conclude the following obser-
vations:
• In general, EFWA shows signiﬁcant improvements over conventional FWA for
most functions.
• With increasing shift values, EFWA achieves much better results than FWA.
• SPSO achieve better results than EFWA for most functions.
• The results of EFWA remain almost unaffected even if the optimum of the function
is shifted toward the edges of the search space.
• The new operator AMP, which limits the lower bound of the explosion amplitude,
is an important tool to balance between exploration and exploitation abilities.
• EFWA reduces the runtime of FWA by a factor of six.
6.3.4 Inﬂuences of Dimension Selection Methods
on FWA and EFWA
In [1], we presented the Enhanced Fireworks Algorithm in details. The EFWA is
proposed based on the FWA, which makes ﬁve improvements. In [2], in the explo-
sion operator and Gaussian sparks operator, while generating an explosion spark or
Gaussian spark, the number of dimension should be calculated at ﬁrst with uniform
distribution, In FWA, it ﬁrst calculate the number of dimension that will be selected
at ﬁrst and then perform the explosion process and Gaussian mutation process. The
number of dimension selected is z = ceil(rand ∗D), where the z is under mean
distribution in [1, D] where D is the total number of dimension of the optimization
function f .
In [1], in the proposed EFWA, in the dimension select method, it can be seen that
the dimension selection method is zk = round(rand(0, 1)), k = 1, 2, ..., D., for
each dimension, it has the half probability to be selected independently. Thus, the
total dimension number selected is under binomial distribution in [1 −D].
However, the experimental results in [1] is still based on the dimension selection
method in conventional FWA, i.e., the total selected dimension number is under
mean distribution, which will lead that if others do experiments according to the
pesudocode of EFWA and will not be able to get the results which are same as the
results in [1]. Thus, here we present an discussion about it. Here, we also want to
point out that those two dimension selection methods differ in performance and the
previous dimension selection method used in FWA will makes the performance better
if we keep previous the dimension selection method in EFWA.
Moreover, in [1], we also present the results of SPSO for further comparison
between PSO and FWA, in the experimental results about SPSO presented in [1], it
is surprising to ﬁnd that with the increasing of SI, the SPSO results sometimes fails
to converge toward the optimum, at that moment, we are not clear about the reasons

102
6
Enhanced Fireworks Algorithm
for this strange phenomenon, later, we conduct further experiments over SPSO and
ﬁnally, ﬁnd that PSO algorithm is very sensitive to the different boundary conditions,
while in the implementation of SPSO in [1], the boundary conditions is that while
the particle exceeds the search bound, the position of the particle will be set to the
bound. Thus, in the SPSO presented in [1], the SPSO fails to converge sometimes
while the SI increases.
For more details about the discussion, please visit http://www.cil.pku.edu.cn/
research/fwa/index.html.
6.4 Summary
In this chapter, an enhanced version of FWA was presented. Based on the detailed
analysis of operators of conventional FWA, ﬁve new operators to overcome FWA’s
limitations were presented, which included the minimal explosion amplitude check
strategy, the new mapping operator, the new operator for generating explosion sparks,
the new Gaussian mutation operator and selection operator. Experimental results
demonstrated that EFWA is a signiﬁcant improvement version of FWA.
References
1. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 IEEE Congress on Evo-
lutionary Computation (CEC) (IEEE, 2013), pp. 2069–2077
2. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
3. A.P. Engelbrecht Fundamentals of Computational Swarm Intelligence (Wiley, New York, 2006)
4. D. Bratton, J. Kennedy, Deﬁning a standard for particle swarm optimization, in IEEE Swarm
Intelligence Symposium, SIS 2007 (IEEE, 2007), pp. 120–127

Chapter 7
Fireworks Algorithm with Dynamic Search
In this chapter, we present a dynamic version of Fireworks Algorithm, which is an
improved version of the recently developed enhanced ﬁreworks algorithm (EFWA)
based on an adaptive dynamic local search mechanism. In EFWA, the explosion
amplitude of each ﬁrework is computed based on the quality of the ﬁrework’s current
location. This explosion amplitude is limited by a lower bound which decreases with
the number of iterations in order to avoid the explosion amplitude to be [close to]
zero, and in order to enhance global search abilities at the beginning and local search
abilities toward the later phase of the algorithm. As the explosion amplitude in EFWA
depends solely on the ﬁreworks’ ﬁtness and the current number of iterations, this
procedure does not allow for an adaptive optimization process. To deal with these
limitations, a dynamic search ﬁreworks algorithm (dynFWA) is proposed, which uses
a dynamic explosion amplitude for the ﬁrework at the current best position. If the
ﬁtness of the best ﬁrework could be improved, the explosion amplitude will increase
to speed up convergence. On the contrary, if the current position of the best ﬁrework
could not be improved, the explosion amplitude will decrease in order to narrow the
search area. In addition, we show that one of the EFWA operators, i.e., Gaussian
mutation operator, can be removed in dynFWA without a loss in accuracy—this
makes dynFWA computationally more efﬁcient than EFWA.
7.1 Introduction
The ﬁreworks algorithm (FWA) [1] is a recently developed SI algorithm based on
simulating the explosion process of real ﬁreworks exploding and illuminating the
night sky. In FWA, the ﬁreworks (i.e., individuals) are let off to the potential search
spaceandanexplosionprocessisinitiatedforeachﬁrework.Thisstochasticexplosion
process is one of the key features of FWA. After the explosion, a shower of sparks ﬁlls
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_7
103

104
7
Fireworks Algorithm with Dynamic Search
the local space around the ﬁrework. Both ﬁreworks as well as the newly generated
sparks represent potential solutions in the search space. A principle FWA works as
follows: At ﬁrst, N ﬁreworks are initialized randomly, and their quality (i.e., ﬁtness)
is evaluated in order to determine the explosion amplitude and the number of sparks
for each ﬁrework. Subsequently, the ﬁreworks explode and generate different types
of sparks within their local space. Finally, N candidate ﬁreworks are selected among
the set of candidates, which includes the newly generated sparks as well as the N
original ﬁreworks. The algorithm continues the search until a termination criterion
(time, maximum number of iteration or ﬁtness evaluation, or convergence) is reached.
Later, Zheng S. et al. proposed the enhanced ﬁreworks algorithm (EFWA) which
incorporates ﬁve modiﬁcations compared to conventional FWA in order to eliminate
the drawbacks of the original algorithm: (i) a new minimal explosion amplitude
check, (ii) a new operator for generating explosion sparks, (iii) a new mapping
strategyforsparkswhichareoutofthesearchspace,(iv)anewoperatorforgenerating
Gaussian sparks, and (v) a new operator for selecting the population for the next
iteration.
In FWA and EFWA, the explosion amplitude value is one of the key parameters
which is used to balance between the local and global search capabilities of the
algorithms. The ﬁtness of the current location of each ﬁrework is used to calculate
the explosion amplitude. The main idea is that a ﬁrework with better ﬁtness (i.e.,
smaller ﬁtness value for minimization problems1) can generate a larger population
of explosion sparks within a smaller range, i.e., with a small explosion amplitude,
while ﬁreworks with poorer ﬁtness (i.e., higher ﬁtness value) can only generate a
smaller population within a larger range, i.e., with bigger explosion amplitude. As a
result, ﬁreworks at good locations will perform local search in a narrow range around
the current location, while ﬁreworks with higher ﬁtness will perform global search
in a wider range.
The minimal explosion amplitude check (MEAC) strategy in EFWA [2] intro-
duced a lower bound for this explosion amplitude in order to avoid that the explosion
amplitude of the best ﬁrework found so far is set to zero. This strategy decreases the
explosion amplitude solely with the current number of function evaluations, which
heavily depends on the predeﬁned number of iterations for the algorithm. Experimen-
tal results indicate that this strategy does not allow for efﬁcient local search around
the current best solution. Therefore, in this chapter, we present an appropriate strat-
egy for varying the explosion amplitude dynamically based on the current success
of the optimization process. Additionally, we show that it is possible to remove the
rather time-consuming Gaussian sparks operator of EFWA in dynFWA without loss
in optimization accuracy.
1In this chapter, without loss of generality, the optimization problem f is assumed to be a mini-
mization problem.

7.2 Properties of Minimal Explosion Amplitude Check (MEAC) Strategy in EFWA
105
7.2 Properties of Minimal Explosion Amplitude Check
(MEAC) Strategy in EFWA
For simplicity and convinces of description later on, we use the following deﬁnitions:
Deﬁnition 7.1 Core Firework (CF): In each iteration, the ﬁrework at the currently
best location is marked as core ﬁrework (CF). Thus, for minimization problems,
among the set C of all ﬁreworks the ﬁrework XCF is selected as CF iff
∀Xi ∈C: f (XC F) ≤f (Xi)
(7.1)
Deﬁnition 7.2 Local Minimum Space and Local Minimum Point: Given an objec-
tive function f , in a continuous space Ψ ⊆Ω, there ∃only one point x, ∃ε, and
f (xi)−f (x) ⩾0, for ∀xi, |xi −x| ⩽ε, then x is a local minimum point. For region
S, if these is only one local minimal point in it, then S is a local minimum space.
A ﬁrework with better ﬁtness can generate a larger population of explosion sparks
within a smaller range, i.e., with a small explosion amplitude. On the contrary, ﬁre-
works with poorer ﬁtness can only generate a smaller population within a larger
range, i.e., with higher explosion amplitude. This allows to balance between ex-
ploration and exploitation capabilities of the algorithm. Exploration refers to the
ability of the algorithm to explore various regions of the search space in order to
locate promising good solutions, while exploitation refers to the ability to conduct a
thorough search within a smaller area recognized as promising in order to ﬁnd the
optimal solution (cf. [3]). Exploration is achieved by those ﬁreworks which have
a large explosion amplitude (i.e., poorer ﬁtness), since they have the capability to
escape from local minima. Exploitation is achieved by those ﬁreworks which have a
small explosion amplitude (i.e., better ﬁtness), since they reinforce the local search
ability in promising areas.
InEFWA,theMEACStrategy(cf.Sect.6.2.1)enforcestheexplorationcapabilities
at the early phase of the algorithm (larger Ak
min ⇒global search), while at the ﬁnal
phase of the algorithm the exploitation capabilities are enforced (smaller Ak
min ⇒
local search). This is further enforced by the nonlinear decrease of Ak
min (cf. Eq.6.3).
Obviously, this procedure decreases the explosion amplitude solely with the current
number of function evaluations which heavily depends on the predeﬁned number of
iterations for the algorithm. The explosion amplitude strategy should consider the
optimization process information rather than solely the information about the current
iteration (or evaluation) count. In order to tackle this problem we propose a dynamic
explosion strategy is proposed for the CF in order to enhance the local search ability.

106
7
Fireworks Algorithm with Dynamic Search
7.3 The dynFWA
In dynFWA, ﬁreworks are separated into two groups. The ﬁrst group consists of
the CF, while the second group consists of all remaining ﬁreworks, i.e., non-CF.
The responsibility of the CF is to perform a local search around the best location
found so far, while the responsibility of the second group is to maintain the global
search ability. For both groups, the explosion amplitude is a key feature in order to
efﬁciently and effectively improve the current locations of the ﬁreworks. However,
for the CF, the selection of the explosion amplitude is even more important due to its
high inﬂuence on the convergence speed toward the local minimum point within the
local minimum space and the property that it is always selected in the optimization
process. In contrast to EFWA, the explosion amplitude in dynFWA of the CF is not
calculated by Eq.(2.3). Instead, the local information of the optimization process
(i.e., the information if the algorithm has improved its best location during the last
iterations) is used for the calculation of the explosion amplitude for the CF. For
all ﬁreworks in the second group (non-CFs), the explosion amplitude is calculated
similarly to EFWA, i.e., by using Eq.2.3, however, without using the MEAC from
Eq.(6.1).
7.3.1 Dynamic Explosion Amplitude for the First Group (CF)
The CF stores the best solution found so far. Let us deﬁne ˆXb as the “best” newly
created explosion spark of all ﬁreworks in the swarm, and Δ f = f ( ˆXb) −f (XC F).
Based on the value of Δ f , for minimization problems, there are two situations:
7.3.1.1 One or Several Explosion Sparks Have Found a Better Position,
i.e., Δ f < 0
It is possible that (i) an explosion spark generated by the CF has found the best
position, or that (ii) an explosion spark generated by a different ﬁrework than the
CF has found the best position. Both cases indicate that the swarm has found a new
promising position and that ˆXb will be the CF for the next iteration.
(i) In most cases, ˆXb has been created by the CF. In such cases, in order to speed
up the convergence of the algorithm, the explosion amplitude of the CF for the
next iteration will be increased compared to the current iteration. This situation
is illustrated in Fig.7.1a, b.
(ii) In other cases—though with a lower probability—a ﬁrework different from the
CF will create ˆXb. This situation happens more frequently during the earlier
phase of the optimization process than during later iterations. In such cases, ˆXb
will become the new CF for the next iteration (recall that the best location among
all individuals is always selected for the next iteration). Since the position of the

7.3 The dynFWA
107
(a) 
(b) 
(c) 
Iteration t + 1
Iteration t + 2
Iteration t + 3
Fig. 7.1 Illustration of the ampliﬁcation/reduction of the CF’s explosion amplitude. In a the radius
of the circle with dashed red line indicates the explosion amplitude of the CF in iteration t, while
the circle with solid black line indicates the explosion amplitude in iteration t + 1; the increased
explosion amplitude indicates that in this situation, a better position has been found by the explosion
sparks. In iteration t + 2 (b), the CF is able to further improve its location, and, as a result, the
explosion amplitude of the CF is further increased. c Shows an example when the ﬁtness of the CF
could not be improved. In this case, the CF’s explosion amplitude is decreased in iteration t + 3
(Color ﬁgure online)
CF is changed, the current explosion amplitude which considers the optimization
information of the position of the current CF will not be effective to the newly
selected CF ( ˆXb). However, it is possible that ˆXb is located in rather close
proximity to the previous CF: since the CF creates the large number of sparks
among all ﬁreworks, the random selection method may select several sparks
created by the CF, which are initially located in close proximity to the CF. If so,
the same consideration as in (i) applies, and the explosion amplitude of the CF
will be increased. If ˆXb is created by a ﬁrework which is not in close proximity
to the CF, the explosion amplitude can be reinitialized to the predeﬁned value.
However, since it is difﬁcult to deﬁne “close” proximity, we do not compute
the distance between ˆXb and XC F but rely on the dynamic explosion amplitude
update ability. Similarly to (i), the explosion amplitude is increased. If the new
CF cannot improve its location in the next iteration, the new CF is able to adjust
the explosion amplitude itself dynamically in the following iterations.
We underline the idea why an increasing explosion amplitude may accelerate the
convergence speed. Assume that the current position of the CF is far away from the
global/local minimum. Increasing the explosion amplitude is a direct and effective
approach in order to increase the step size toward the global/local optimum in each
iteration, i.e., it allows for faster movements toward the optimum. However, we
note that usually the probability to ﬁnd a position with better ﬁtness decreases with
increasing explosion amplitude due to the increased search space (obviously, this
depends to a large extent on the optimization function).

108
7
Fireworks Algorithm with Dynamic Search
7.3.1.2 None of the Explosion Sparks of the CF Nor of All Other
Fireworks Has Found a Position with Better Fitness Compared
to the CF, i.e., Δ f ≥0
In this situation, the explosion amplitude of the CF is reduced to narrow down
the search in a smaller region around the current location and so as to enhance the
exploitation capability of the local search of the CF. The probability to ﬁnd a position
with better ﬁtness usually increases with decreasing explosion amplitude.
7.3.2 Dynamic Explosion Amplitude Update for CF
Based on the previous discussion, the ﬁnal proposed algorithm is as follows.
Algorithm 7.1 Dynamic explosion amplitude update for CF
Require: Deﬁne:
XC F is the current location of the CF;
ˆXb is the best location among all explosion sparks;
AC F is the current explosion amplitude of the CF;
Ca is the ampliﬁcation coefﬁcient;
Cr is the reduction coefﬁcient;
Ensure:
1: if
f ( ˆXb) −f (XC F) < 0 then
2:
AC F ←AC F ∗Ca;
3: else
4:
AC F ←AC F ∗Cr;
5: end if
Algorithm 7.1 summarizes the dynamic update strategy as discussed in Sect.7.3.1.
Figure7.2 shows the process of the ampliﬁcation/reduction during the optimization
of the sphere function for 1,000 iterations (algorithm dynFWA). As can be seen
from Fig.7.2, the reduction and ampliﬁcation explosion amplitudes happen in an
alternating manner. Obviously, there are more reduction than ampliﬁcation phases,
which is partly caused by the values of Ca and Cr, which are set to 1.2 and 0.9,
respectively, and by the fact that the explosion amplitude is initially set to the size of
the search space, which is a rather large number in the ﬁrst iteration (cf. Sect.7.4.1).
7.3.3 Some Theoretical Analysis and Considerations
In what is following, we discuss why a reduction of the explosion amplitude increases
the probability to ﬁnd a better location. A Taylor series is used to represent the
properties of the local region around the CF. Assume a continuously differentiable

7.3 The dynFWA
109
Fig. 7.2 The reduction and
ampliﬁcation of CF’s
explosion amplitude (using
function f1 (sphere function)
from [4], cf. Sect.7.4)
0
200
400
600
800
1000
Reduce
0
Amplify
Iteration Number 
second-order optimization function g with k dimensions. If the position of the CF is
not a local/global minimal point, and AC F is the current explosion amplitude, then
g(x) −g(XC F) = ∇g(XC F)T (x −XC F) + 1
2(x −XC F)H(x)(x −XC F), (7.2)
where
H(x) =

∂2g
∂xi∂x j

k×k
.
(7.3)
According to the deﬁnition of “local minimal space and local minimal point” in
Sect.7.2 (i.e., XC F is not a local/global minimum point), there ∃ε, ∀x in
S = {x| |x −XC F| ⩽ε},
(7.4)
and
g(x) −g(XC F) = ∇g(XC F)T (x −XC F) + o(∇g(XC F)T (x −XC F)),
(7.5)
where o(.) means low order.
From the Taylor series, if ε →0, then in region S, if there exists a point x1
and x1 −XC F = Δx, then there exists a point x2 and x2 −XC F = −Δx. Under
this circumstance, the probability of generating a spark with smaller ﬁtness than the
CF is very high (i.e., (g(x1) −g(XC F)) ∗(g(x2) −g(XC F)) < 0). In case the CF
does not ﬁnd a better position while generating a number of sparks, it is likely that
AC F ⩾ε. We cannot expect the property of region T = {x|ε ⩽|x −XC F| ⩽AC F}
that in region T whether there exists a position with better ﬁtness compared to the
CF, thus, if the CF generates sparks with uniform distribution in each dimension, the
probability p′ that a spark is located in S is p′ =
∥S∥
∥S∥+∥T ∥, where ∥∥denotes the
hypervolume of this region. If the CF does not ﬁnd a better position, the explosion
amplitude AC F is reduced to increase the probability p′ that the CF can generate a
spark in region S thus to increase the probability ﬁnding a point with smaller ﬁtness
than CF.

110
7
Fireworks Algorithm with Dynamic Search
7.3.4 Explosion Amplitude for the Second Group (Non-CF)
The explosion amplitudes for non-CF ﬁreworks are calculated based on Eq.(2.3),
i.e., similar to EFWA but without the minimum explosion amplitude check (MEAC)
strategy. Compared to the CF, these non-CF ﬁreworks can only create a smaller
number of explosion sparks within a larger explosion amplitude to perform the global
search for the swarm. In the situation where the CF gets stuck in local minima, this
group of ﬁreworks may be able to allow the algorithm to escape from premature
convergence, since these ﬁreworks continue the search in different areas of the search
space.
7.3.5 Elimination of the Gaussian Sparks Operator
The motivation behind the Gaussian sparks (cf. [1]) is to further increase the diversity
of the swarm. In EFWA, the Gaussian sparks are calculated by
Xik = Xik + (X Bk −Xik) × e,
(7.6)
where X Bk is the kth dimension value of location of the currently best ﬁre-
work(denoted), and e = Gaussian(0, 1) (cf. Sect.6.2.4). From Fig.7.3, it can be
seen that the newly generated sparks will be located along the direction between a
selected ﬁrework i and the CF. Any newly generated Gaussian sparks will be either
(1) close to the CF, (2) close to ﬁrework i, or (3) located along the direction between
the CF and ﬁrework i, however, with some distance to both, the CF and ﬁrework i.
In the ﬁrst two cases, the operator will have similar performance as the explosion
sparks generated by the CF and ﬁrework i, respectively. In the third situation, the
Gaussian spark can be interpreted as an explosion spark generated by a ﬁrework with
large explosion amplitude. Thus, based on the above analysis it can be stated that in
many situations Gaussian sparks will not be able to effectively increase the diversity
of the swarm.
Fig. 7.3 Gaussian mutation
operator in EFWA

7.3 The dynFWA
111
7.3.6 Framework of dynFWA
Based on the operators discussed in the previous subsections, the ﬁnal algorithm
called dynamic search ﬁreworks algorithm (dynFWA) is presented in Algorithm 7.2.
Algorithm 7.2 Framework of dynFWA
1: Initialize N ﬁreworks and evaluate the quality
2: Initialize the explosion amplitude for CF
3: while termination criteria are not met do
4:
Calculate number of explosion sparks (cf. Eq.2.3)
5:
Calculate explosion amplitude for non-CF (cf. Eq.2.3)
6:
for each ﬁrework do
7:
Generate explosion sparks
8:
Map sparks at invalid locations back to search space
9:
Evaluate quality of explosion sparks
10:
end for
11:
Update explosion amplitude of CF (cf. Algorithm 7.1)
12:
Select N ﬁreworks for next iteration
13: end while
7.4 Experiments
To investigate the performance of the proposed dynFWA algorithm as well as the ne-
cessityoftheGaussianmutationoperator(i.e.,theperformanceofdynFWAremoving
this operator), we compare two EFWA variants and two dynFWA variants. Each al-
gorithm is tested with and without Gaussian mutation operator, respectively. Besides
comparing dynFWA and EFWA, the most recent version of SPSO (SPSO2011, [5])
is also used for performance comparison. In the following, we brieﬂy describe the
ﬁve algorithms used for experimental evaluation:
• EFWA—the baseline algorithm as presented in [2]2;
• EFWA-NG—in this algorithm, the Gaussian sparks operator has been removed
from EFWA;
• dynFWA-G—this algorithm implements the dynFWA algorithm as described in
Sect.7.3 including the Gaussian mutation operator.
• dynFWA—similar as dynFWA-G but without Gaussian mutation operator.
2We note that experimental results in the EFWA [2] are unintentionally based on the dimension
selection method of conventional FWA [1], which varies the number of adapted dimensions uni-
formly among all dimension. More details can be found at http://www.cil.pku.edu.cn/research/
FWA/index.html. All results and pseudocodes in this chapter are based on the published pseudocode
in [2].

112
7
Fireworks Algorithm with Dynamic Search
• SPSO2011—the most recent SPSO variant. Compared to earlier versions of SPSO
it features an improved velocity update by exploiting the idea of rotational invari-
ance for the velocity update instead of sequential dimension-by-dimension update
of older versions of SPSO (cf. [5]).3
7.4.1 Experimental Setup
Similar to EFWA, the number of ﬁreworks in dynFWA is set to 5, but in dynFWA, the
maximum number of explosion sparks in each iteration is set to 150. The reduction
and ampliﬁcation factors Cr and Ca of dynFWA are empirically set to 0.9 and 1.2,
respectively, and AC F is initially set to the size of the search space in order to maintain
a high exploration capability at the beginning. All other parameters for dynFWA and
all EFWA parameters are identical to [2], SPSO2011 parameters are listed in Table
A.4 [5] . For each algorithm we performed 51 runs on each optimization function; the
ﬁnal mean results after 300000 function evaluations are presented. As experimental
platform we used MATLAB 2011b (Windows 7; Intel Core i7-2600 CPU @ 3.7 GHZ;
8GB RAM). To validate the performance of the proposed algorithms we used the
recent CEC2013 benchmark suite that includes 28 different benchmark functions as
listed in [4].
7.4.2 Experimental Results
In this subsection, we ﬁrst evaluate the inﬂuence of removing the Gaussian sparks op-
erator as discussed in Sect.7.3.5. After that we evaluate the performance of dynFWA
and compare its performance to EFWA and SPSO2011.
7.4.2.1 Evaluation of Gaussian Mutation Operator
To evaluate whether the results of EFWA and dynFWA improve or deteriorate after
removing the Gaussian sparks operator we compare the results of EFWA and EFWA-
NG, and the results of dynFWA-G and dynFWA, respectively. In order to validate
the improvement between any two algorithms, the Wilcoxon signed-rank test is
conducted (cf. Sect.7.4.2). Assume that data X, Y are ﬁtness results for a given
number of runs of two different algorithms. If the mean value of X is smaller than
Y and the Wilcoxon signed-rank test under 5% signiﬁcance level is true, then it is
3In addition to the results (median, maximum, minimal) in [5], detailed results of SPSO2011 which
include the results of each single run were also submitted to the CEC2013 competition held by
P.N. Suganthan. These results are available at http://goo.gl/pXB1WH. The mean ﬁtness error results
are computed based on the results in the folder “1534” of ﬁle “Results-of-22-papers.zip”.

7.4 Experiments
113
Table 7.1 Wilcoxon signed-rank test results for EFWA versus EFWA-NG and dynFWA-G versus
dynFWA (bold values indicate the performance difference is signiﬁcant)
F.
EFWA versus EFWA-NG
dynFWA-G versus dynFWA
EFWA
EFWA-NG
p-value
dynFWA-G
dynFWA
p-value
f1
−1.3999E+3
−1.3999E+3
2.316E−3
−1.4000E+3
−1.4000E+3
1.000E+0
f2
6.8926E+5
6.5258E+5
4.256E−1
7.6981E+5
8.6937E+5
1.801E−1
f3
7.7586E+7
6.4974E+7
8.956E−1
1.2007E+8
1.2317E+8
6.393E−1
f4
−1.0989E+3
−1.0989E+3
7.858E−1
−1.0863E+3
−1.0896E+3
3.183E−2
f5
−9.9992E+2
−9.9992E+2
4.290E−2
−1.0000E+3
−1.0000E+3
1.463E−1
f6
−8.5073E+2
−8.4462E+2
1.654E−1
−8.6524E+2
−8.6995E+2
9.156E−2
f7
−6.2634E+2
−6.2991E+2
9.552E−1
−6.9946E+2
−7.0010E+2
6.663E−1
f8
−6.7907E+2
−6.7906E+2
9.776E−1
−6.7909E+2
−6.7910E+2
4.997E−1
f9
−5.6846E+2
−5.6889E+2
5.178E−1
−5.7435E+2
−5.7587E+2
1.711E−1
f10
−4.9916E+2
−4.9918E+2
3.732E−1
−4.9994E+2
−4.9995E+2
3.591E−1
f11
5.8198E+0
3.5430E+1
5.830E−2
−2.9978E+2
−2.9589E+2
6.127E−1
f12
3.9944E+2
4.1107E+2
6.193E−1
−1.4993E+2
−1.4222E+2
4.762E−1
f13
2.9857E+2
2.8909E+2
8.220E−1
5.4523E+1
5.3830E+1
8.513E−1
f14
2.7240E+3
2.9344E+3
4.101E−2
2.8909E+3
2.9180E+3
8.147E−1
f15
4.4595E+3
4.5515E+3
6.869E−1
3.9186E+3
4.0227E+3
4.879E−1
f16
2.0063E+2
2.0056E+2
2.811E−1
2.0056E+2
2.0058E+2
7.358E−1
f17
6.2461E+2
6.3152E+2
9.179E−1
4.5397E+2
4.4261E+2
1.197E−1
f18
5.7361E+2
5.6953E+2
6.938E−1
5.8801E+2
5.8782E+2
8.660E−1
f19
5.1022E+2
5.1012E+2
9.402E−1
5.0750E+2
5.0726E+2
6.193E−1
f20
6.1466E+2
6.1457E+2
1.559E−2
6.1309E+2
6.1328E+2
3.632E−1
f21
1.1178E+3
1.1362E+3
6.910E−4
9.9532E+2
1.0102E+3
6.431E−1
f22
6.3181E+3
6.3674E+3
9.776E−1
4.1463E+3
4.1262E+3
9.402E−1
f23
7.5809E+3
7.5707E+3
7.217E−1
5.6661E+3
5.6526E+3
9.402E−1
f24
1.3452E+3
1.3611E+3
1.079E−2
1.2738E+3
1.2729E+3
8.586E−1
f25
1.4426E+3
1.4435E+3
8.734E−1
1.3964E+3
1.3970E+3
8.882E−1
f26
1.5461E+3
1.5400E+3
2.687E−1
1.4744E+3
1.4607E+3
1.337E−1
f27
2.6210E+3
2.5780E+3
3.534E−1
2.2721E+3
2.2804E+3
8.147E−1
f28
4.7651E+3
4.9949E+3
6.460E−1
1.7686E+3
1.6961E+3
3.555E−1
believed that the results of X are signiﬁcant better than Y. A comparison between
EFWA versus EFWA-NG and dynFWA-G versus dynFWA are given in Table7.1:
• EFWA-NG performs slightly better than EFWA on 16 functions, while it per-
forms slightly worse than EFWA on 12 functions. However, for ﬁve functions the
Wilcoxon signed-rank test indicates that EFWA is signiﬁcantly better than EFWA-
NG, while for one function EFWA-NG is signiﬁcant better than EFWA. Hence,
for EFWA these results suggest that the Gaussian mutation operator should not be
removed, although EFWA-NG is faster in terms of runtime (see next section).

114
7
Fireworks Algorithm with Dynamic Search
• In general, the performance of dynFWA and dynFWA-G is very similar. Only
for function f4 dynFWA performs signiﬁcantly better than dynFWA-G. This indi-
cates that dynFWA without the Gaussian mutation operator achieves slightly better
results than dynFWA-G, and is also faster in terms of runtime (see next section).
In the rest of this chapter, we use the best EFWA variant (EFWA with Gaussian
sparks operator) and the best dynFWA variant (dynFWA without Gaussian mutation
operator) for further comparison.
7.4.2.2 Comparison of dynFWA and EFWA
Table7.2 shows the average ﬁtness values over 51 runs for each function for the three
algorithms SPSO2011, EFWA and dynFWA, and the corresponding rank of each
algorithm. Moreover, at the bottom of Table7.2 we present the mean ﬁtness rank
for each algorithm. Table7.4 shows the runtime of each algorithm—the runtime of
the fastest algorithm (dynFWA) is set to 1, for all other algorithms, the proportional
runtimes compared to dynFWA are presented.
A comparison between the proposed dynFWA and EFWA reveals that dynFWA
outperforms EFWA in terms of average ﬁtness rank and runtime. It can be seen that
dynFWA achieves better mean ﬁtness results than EFWA on 23 functions except
function f2, f3, f4, f14, f18. The mean ﬁtness rank results suggest that dynFWA
gains great advantages over EFWA. To test whether the improvement of dynFWA
over EFWA is signiﬁcant or not, a number of Wilcoxon signed-rank tests were con-
ducted and the corresponding p-values are presented in Table7.3. The results indicate
that the improvement of dynFWA is signiﬁcant compared to EFWA for 22 bench-
mark functions. Moreover, in terms of computational complexity it can be seen that
dynFWA signiﬁcantly reduces the runtime of EFWA for the same number of function
evaluations. This is mostly caused by the removal of the Gaussian mutation operator
that is computationally rather expensive.
7.4.2.3 Comparison of dynFWA and SPSO2011
A comparison between the proposed dynFWA and the latest SPSO variant indicates
dynFWA is able to achieve a better mean rank compared to SPSO2011 (cf. Table7.2).
In total, dynFWA achieves better results (smaller mean ﬁtness) than SPSO2011 on
17 functions, while SPSO2011 is better than dynFWA on 10 functions, for one func-
tion, the results are identical. In terms of computational complexity we measured the
runtime of SPSO2007, however, since the results of SPSO2011 are adopted from the
[5] literature, we cannot compare the runtimes as they depend on the implementation
and the infrastructure. Table7.4 shows that compared to SPSO2007, dynFWA has

7.4 Experiments
115
Table 7.2 Mean ﬁtness on the benchmark functions and mean ﬁtness rank of SPSO2011, EFWA
and dynFWA
F.
SPSO2011
Rank
EFWA
Rank
dynFWA
Rank
f1
−1.4000E+3
1
−1.3999E+3
3
−1.4000E+3
1
f2
3.3719E+5
1
6.8926E+5
2
8.6937E+5
3
f3
2.8841E+8
3
7.7586E+7
1
1.2317E+8
2
f4
3.7543E+4
3
−1.0989E+3
1
−1.0896E+3
2
f5
−1.0000E+3
1
−9.9992E+2
3
−1.0000E+3
2
f6
−8.6210E+2
2
−8.5073E+2
3
−8.6995E+2
1
f7
−7.1208E+2
1
−6.2634E+2
3
−7.0010E+2
2
f8
−6.7908E+2
2
−6.7907E+2
3
−6.7910E+2
1
f9
−5.7123E+2
2
−5.6846E+2
3
−5.7587E+2
1
f10
−4.9966E+2
2
−4.9916E+2
3
−4.9995E+2
1
f11
−2.9504E+2
2
5.8198E+0
3
−2.9589E+2
1
f12
−1.9604E+2
1
3.9944E+2
3
−1.4222E+2
2
f13
−6.1406E+0
1
2.9857E+2
3
5.3830E+1
2
f14
3.8910E+3
3
2.7240E+3
1
2.9180E+3
2
f15
3.9093E+3
1
4.4595E+3
3
4.0227E+3
2
f16
2.0131E+2
3
2.0063E+2
2
2.0058E+2
1
f17
4.1626E+2
1
6.2461E+2
3
4.4261E+2
2
f18
5.2063E+2
1
5.7361E+2
2
5.8782E+2
3
f19
5.0951E+2
2
5.1022E+2
3
5.0726E+2
1
f20
6.1346E+2
2
6.1466E+2
3
6.1328E+2
1
f21
1.0088E+3
1
1.1178E+3
3
1.0102E+3
2
f22
5.0988E+3
2
6.3181E+3
3
4.1262E+3
1
f23
5.7313E+3
2
7.5809E+3
3
5.6526E+3
1
f24
1.2667E+3
1
1.3452E+3
3
1.2729E+3
2
f25
1.3993E+3
2
1.4426E+3
3
1.3970E+3
1
f26
1.4861E+3
2
1.5461E+3
3
1.4607E+3
1
f27
2.3046E+3
2
2.6210E+3
3
2.2804E+3
1
f28
1.8013E+3
2
4.7651E+3
3
1.6961E+3
1
Mean Rank
SPSO2011
1.75
EFWA
2.68
dynFWA
1.54
almost the same runtime. Since the SPSO2011 operators (new velocity update strate-
gies) appear to be more complex, therefore, probably at least its time-consuming as
same as the SPSO2007 (cf. [6]), we expect SPSO2011 to have a similar or slightly
higher computationally complexity compared to initial version of SPSO2007.

116
7
Fireworks Algorithm with Dynamic Search
Table 7.3 Wilcoxon signed-rank test results for dynFWA versus EFWA (bold values indicate the
signiﬁcant improvement)
F.
f1
f2
f3
f4
f5
f6
f7
p-value
0.00E+0
6.94E−3
9.90E−2
0.00E+0
0.00E+0
1.58E−3
0.00E+0
F.
f8
f9
f10
f11
f12
f13
f14
p-value
1.73E−2
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
1.41E−1
F.
f15
f16
f17
f18
f19
f20
f21
p-value
5.10E−5
3.20E−1
0.00E+0
6.35E−2
1.41E−4
0.00E+0
0.00E+0
F.
f22
f23
f24
f25
f26
f27
f28
p-value
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
Table 7.4 Runtime comparison
SPSO2007
1.01
EFWA
1.30
dynFWA-G
1.17
SPSO2011
/
EFWA-NG
1.03
dynFWA
1
7.5 Summary
In this chapter, we presented the dynamic search Firework Algorithm (dynFWA), an
improved version of the recently developed enhanced ﬁreworks algorithm (EFWA).
dynFWA uses a dynamic explosion amplitude for the core ﬁrework (CF), i.e., the
ﬁrework at the current best position. This dynamic explosion amplitude depends on
the quality of the current local search around the CF. The main task for the CF is to
perform a local search, while the responsibility for all other ﬁreworks are to maintain
the global search ability. Additionally, we have analyzed the possibility to remove
the rather time-consuming Gaussian mutation operator of EFWA.
From the result of our experimental evaluation we concluded the following ob-
servations:
1. The proposed dynFWA algorithm signiﬁcantly improves the results of EFWA and
reduces the runtime by more than 20%.
2. Compared with SPSO2011, dynFWA achieves a better mean rank among 28
benchmark functions with similar computational cost.
3. The Gaussian mutation operator of EFWA is a necessary component of EFWA.
However, removing this operator from dynFWA can reduce the runtime of dyn-
FWA signiﬁcantly without loss of optimization accuracy.

7.5 Summary
117
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 IEEE Congress on Evo-
lutionary Computation (CEC) (IEEE, 2013), pp. 2069–2077
3. M. Clerc, J. Kennedy, The particle swarm—explosion, stability, and convergence in a multidi-
mensional complex space. Trans. Evol. Comput. 6(1), 58–73 (2002)
4. J.J. Liang, B.Y. Qu, P.N. Suganthan, A.G. Hernández-Díaz, Problem deﬁnitions and evaluation
criteria for the CEC 2013 special session on real-parameter optimization (2013)
5. M. Zambrano-Bigiarini, M. Clerc, R. Rojas, Standard particle swarm optimisation 2011 at
CEC-2013: a baseline for future PSO improvements, in 2013 IEEE Congress on Evolutionary
Computation (CEC) (2013), pp. 2337–2344, doi:10.1109/CEC.2013.6557848
6. M. Clerc, Standard particle swarm optimization, from 2006 to 2011. Particle Swarm Central
(2011)

Chapter 8
Adaptive Fireworks Algorithm
The explosion amplitude in FWA is a key factor inﬂuencing the performance of
Fireworks Algorithm, which needs to be controlled precisely. In this chapter, a new
FWA algorithm called Adaptive ﬁreworks algorithm is proposed by replacing the
explosion amplitude operator in FWA with an adaptive method.
8.1 Motivation
Both FWA [1] and EFWA remain to be improved in many aspects. For example,
neither way of calculating the explosion amplitude in the two algorithms are reason-
able. Considering the explosion search in FWA and EFWA, the amplitude is a very
important factor inﬂuencing its performance.
In order to improve the mechanism of calculating the amplitude of explosion in
FWA and EFWA, an adaptive explosion amplitude is proposed by using the distance
betweenthebestﬁreworkandacertainselectedindividualastheexplosionamplitude.
We analyze the property of our adaptive amplitude and proved that it can adjust itself
adaptively according to the search results. By applying adaptive explosion amplitude
to the EFWA, a new algorithm called the adaptive ﬁreworks algorithm (AFWA) is
proposed.
8.2 Analysis of the Amplitude in FWA and EFWA
Considering the explosion search manner in the FWA and the EFWA, the amplitude
of each ﬁrework is a fatal variable inﬂuencing the performance of the algorithm.
As shown in Eq.(2.3), the amplitudes of other ﬁreworks (except for the best one)
are calculated according to the difference between their ﬁtness and the best one’s.
However, the amplitude of the best ﬁrework is always 0 according to Eq.(2.3). In
FWA, there is no other operator to deal with this problem, which means that the best
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_8
119

120
8
Adaptive Fireworks Algorithm
ﬁrework will not contribute to the algorithm at all, despite its most numerous sparks.
Note that according to Eq.(2.1), the better a ﬁrework’s ﬁtness is, the more sparks it
generates. In FWA, The best ﬁreworks takes the most but provides the least.
Thus, in EFWA [2], in order to make sure the best ﬁrework works, a minimal
amplitude check is adopted, preventing the amplitude of the best ﬁrework from being
0. As is shown in Eqs.(6.1) and (6.3), the threshold of the amplitude is a nonlinear
decreasing function of the generation number. In EFWA, the threshold is actually
the amplitude of the best ﬁrework.
However, any preset amplitude, linear decrease or nonlinear decrease, whatever
the parameters are set, cannot ﬁt the evaluation function well: in some functions, it
decreases too fast, and in others too slow. Decreasing too fast causes the search range
converges early before the minima is reached. Decreasing too slow causes the search
range is still too large to search precisely even when the minima is already within
the search range. In either case, the algorithm performs badly.
The amplitudes of other ﬁreworks in FWA and EFWA are calculated according
to Eq.(2.3) adaptively, while the amplitude of the best ﬁrework still remains a big
problem. Although it only inﬂuences local search of the algorithm, it is a key to the
performance. The amplitude of the best ﬁrework needs to be adjusted automatically
in order to ﬁt all evaluation functions.
8.3 Adaptive Explosion Amplitude
In this section, an adaptive method is proposed, using already generated sparks
to calculate the explosion amplitude of the best ﬁrework. We use the information
obtained in this generation to calculate the amplitude of the best ﬁrework in the next
generation. Considering the selection in EFWA, the best ﬁrework in next generation is
the best individual found (could be a spark generated or a ﬁrework) in this generation.
As we already know, all the amplitudes of other ﬁreworks (in the next generation) are
calculated according to the difference between their ﬁtness and the best ﬁrework. The
main problem is to get a reasonable amplitude of the best ﬁrework. For convenience,
we only consider one ﬁrework within this section.
8.3.1 Principles
In order to calculate an adaptive amplitude for the best ﬁrework, we choose an
individual and use its distance to the best individual, which is the ﬁrework in next
generation, as the amplitude of the next explosion.

8.3 Adaptive Explosion Amplitude
121
The individual we choose subjects to the following two conditions:
(1) Its ﬁtness is worse than the best ﬁrework ﬁtness of this generation.
(2) Its distance to the best individual (the ﬁrework of next generation) is minimal
among all individuals subjecting to (1).
Namely,
ˆs = arg min
si
(d(si, s∗))
(8.1)
s.t.
f (si) > f (X)
(8.2)
where si stands for all sparks generated by the ﬁrework, s∗stands for the best
individual among all sparks and the ﬁrework, X stands for the ﬁrework, d is a
certain measurement of distance. Note that the algorithm always choose the best
individual as the ﬁrework in next generation.
Condition (1) requires that the difference (in ﬁtness) between this individual and
thebestindividualisbiggerthanthatbetweentheﬁreworkandthebestindividual,i.e.,
f (ˆs) −f (s∗) > f (X) −f (s∗)
(8.3)
An intuitive understanding of this inequality is it assures that the scale on the
domain of the evaluation function within the range d(ˆs, s∗) is at least bigger than the
improvement made in this generation. Our aim to ﬁnd a better location ˜s in the next
explosion such that
f (s∗) −f (˜s) > f (X) −f (s∗)
(8.4)
by estimating the potential range d(˜s, s∗) with d(ˆs, s∗). The algorithm makes a cor-
respondence between the range and the domain. Some deeper consideration and its
properties will be discussed after the complete algorithm is presented.
On the other hand, condition (2) helps to make sure the amplitude converges. If a
farther individual subjecting to condition (1) is chosen, the amplitude could be, in the
worst case, double the amplitude of this explosion. Under that circumstances, there
is no guarantee the amplitude would not be locked on the maximum value (the range
of the search space, for example). On the contrary, choosing the nearest individual
is quite safe because if the function is regular locally and the sparks are numerous
enough, the minimum distance would be at most slightly longer than the amplitude
in this generation. Although we cannot promise the amplitude decreases every time,
but with big iteration times and numerous sparks, it converges in general, as is shown
in Fig.8.1. It can be seen that at the early phase of the optimization, the explosion
amplitude is usually very bigger, while at the later phase, the explosion amplitude
tends to reduce, and to be quite small, thus to increase its local search ability.
To make a clear explanation of AFWA, in the following, we use ﬁgures to present
the basic idea. In Fig.8.2, the red dot with ﬁtness 1.0 is the ﬁrework of this generation,
and yellow dots are the sparks generated by it. Obviously, we will choose the yellow

122
8
Adaptive Fireworks Algorithm
Fig. 8.1 Adaptive amplitude on sphere function
Fig. 8.2 An example of how adaptive amplitude is calculated
dot with ﬁtness 0.7 as the ﬁrework in the next explosion. According to Eqs.(8.1)
and (8.2), we choose the individual whose ﬁtness is 1.1 as ˆs and use its distance
to the 0.7 individual (s∗) as the amplitude of next explosion. Without condition (1)
we will choose the 0.8 individual, which makes the algorithm converges too fast.
Without condition (1) we will choose the 1.5 individual, which makes the algorithm
does not converge at all. If the evaluation function changes regularly, with numerous
sparks, there is a good chance for us to ﬁnd the individual with ﬁtness 0.3in the next
explosion, if 0.7 is not close to local minima.
It is clear that this algorithm does not care either the scale of the range or the
domain. Rather, it detects the relationship between the scale of the range and the
scale of the domain adaptively without any more evaluation. Changing the scale
of either will not inﬂuence the performance. In a sense, it provides the gradient
information of the evaluation function.

8.3 Adaptive Explosion Amplitude
123
Considering the way ﬁreworks explode in EFWA, where they explode in each
dimension independently, we use inﬁnity norm as the distance measure, namely
the maximum difference among all dimensions. Besides, in order to further slow
down the convergence rate and improve the global search, the adaptive amplitude
calculated above is multiplied by a certain coefﬁcient (usually bigger than 1). Finally,
considering the sparks of each explosion is limited, in order to minimize the inﬂuence
ofverybadluck(forexample,everysparkisworsethantheﬁrework,inwhichcasethe
amplitude shrinks very fast, or on the contrary the amplitude doubled last time), we
also adopt a simple smoothing mechanism, which uses the average of the amplitude
calculated above and the amplitude of this generation as the amplitude.
8.3.2 Algorithm
The complete algorithm of calculating the amplitude is shown in Algorithm 8.1.
Algorithm 8.1 Calculate the adaptive amplitude for the ﬁrework of generation g + 1
1: A(g + 1) ←UB −LB
2: for i = 1 to n do
3:
if ||si −s∗||∞> A(g + 1) and f (si) > f (X) then
4:
A(g + 1) ←||si −s∗||∞
5:
end if
6: end for
7: A(g + 1) ←λ · A(g + 1)
8: A(g + 1) ←0.5 · (A(g) + A(g + 1))
9: return A(g + 1)
In Algorithm 8.1, UB and LB stand for the upper bound and lower bound of the
search space respectively, s1 . . . sn stands for all sparks generated by the ﬁrework
in generation g, X stands for the ﬁrework in generation g, s∗stands for the best
individual in generation g, namely the ﬁrework in generation g + 1.
The parameter λ has a great impact on the performance of the algorithm. In a
sense, it controls the balance between global search and local search. If λ is too
small, the adaptive amplitude converges too fast to a local minima without searching
the neighborhood. While if it is too big, the adaptive amplitude does not converge.
Generallyspeaking, as longas it converges stably, thebigger thebetter. Inexperiment,
we usually use λ = 1.3 empirically.
The computational complexity of Algorithm 8.1 is O(n), which means it does
not add much cost. Actually, compared to other operators such as generating sparks,
O(n) is not dominant.
By using the information provided by these already calculated individuals efﬁ-
ciently, this algorithm returns a relatively accurate amplitude, within which the explo-
sion process has a great chance to make a great improvement.

124
8
Adaptive Fireworks Algorithm
Speciﬁcally, when the amplitude is too long, the chance to ﬁnd a better location
become low. On the contrary, if the amplitude is too short, explosion algorithm can
only obtain a comparatively small improvement. However, the proposed algorithm
promises that there is likely to exists a location within the amplitude which brings
a greater improvement than the last generation, so long as the function is relatively
smooth in the small neighborhood.
8.3.3 Explanation
For most optimization problems, the search process by only one ﬁrework can be
brieﬂy divided into three stages (note that they are not strictly distinguished):
(1) Global search. At the beginning, the algorithm does not have any information
about the evaluation function, so it has to explore globally to decide which
region is comparatively promising for further exploitation. In our algorithm, the
amplitude of the ﬁrework is set to the range of the search space at the beginning,
and the sparks it generates will be distributed in the whole search space. In this
way, the algorithm detects the rough information about which region is good and
which is not. Then, the algorithm will choose the best individual among all the
sparks and the ﬁrework. As far as the amplitude is concerned, it is really hard to
predict whether it will become longer or shorter. There are two possible cases,
roughly speaking, as shown in Fig.8.3.
In the ﬁrst case, ˆs and s∗are on the different sides of the ﬁrework. To the limited
information we have, we can assume in reason that the ﬁrework is in the same
local region as ˆs and s∗, and that the d(ˆs, s∗) gives the longest estimate of the
region’s scale. In this case, the amplitude will most probably become longer and
the s∗will walk a longest step toward the potential local best.
In the second case, ˆs and s∗are on the same side of the ﬁrework. Recall that the
ﬁtness of ˆs we use is worse than the ﬁrework, while s∗is better. To the limited
Fig. 8.3 Left ˆs and s∗are on the different sides of the ﬁrework; Right ˆs and s∗are on the same side
of the ﬁrework

8.3 Adaptive Explosion Amplitude
125
information we have, we can assume in reason that the ﬁrework is not in the same
local region as ˆs and s∗, and that the region containing s∗is more promising than
that containing the ﬁrework, and that the d(ˆs, s∗) gives the longest estimate of
the region’s scale. In this case, the amplitude will most probably become shorter,
in order to give up the region of the ﬁrework and search more efﬁciently.
In summary, d(ˆs, s∗) always gives the longest possible estimate of the scale of
the local region containing s∗and keeps the search most efﬁcient.
(2) Local search. When the ﬁrework enters a certain local region, which we can
assume as a bowl region, the main task is to walk toward the bottom as fast as
possible. When the amplitude is still much shorter than the distance between the
ﬁrework and the bottom, we can assume that the neighborhood of the ﬁrework
is monotonous, which means ˆs and s∗will undoubtedly on the different sides of
the ﬁrework. From the monotonic we can also assume that the s∗is very close
to the border of the ﬁrework’s amplitude, because there is no farther spark in its
direction. Then, it is most likely that,
d(ˆs, s∗) > d(ﬁrework, s∗)
(8.5)
where, ˆs is the best individual in current generation g and ˆs is the individual
calculated under Eq.(8.1). Figure8.4 shows this situation.
As shown in Fig.8.5, the experimental result also supports this conclusion.
In summary, the amplitude will become longer and longer in stage (2) to fasten
the search as long as the ﬁrework is still far from the local minima.
(3) Reﬁne search. At the end of the search, when the local minima is already within
the amplitude of the ﬁrework, the algorithm need to search more precisely than
in the above stages. In this case, the s∗is not the farthest spark, rather it is the
spark closest to the local minima. So, in contrary to stage (2), the amplitude
will most probably become shorter and shorter (shown in Fig.8.6) and enable
the algorithm to search more and more precisely, unless the local minima is not
within the amplitude any longer, which brings the algorithm back to stage (2).
Finally, as we have shown in Fig.8.1, the amplitude converges after all, and the
local minima is certainly reached.
The properties of our algorithm in all the three stages proves it a promising global-
to-local search algorithm.
There are some extreme cases for the algorithm:
(a) It is possible that all the sparks’ ﬁtness is worse than the ﬁrework’s. It is most
likely to happen in stage (3). Usually, it implies that the ﬁrework is quite close
to the local minima, while the amplitude is too large. In AEA algorithm, if it
happens, ˆs will just be the closest spark to the ﬁrework, and the amplitude in
next generation will become shorter than this generation. When the amplitude is
reduced to a reasonable size (there is certainly a better location in a very small
neighborhood unless the local minima is already reached), the algorithm will go
on to work normally.

126
8
Adaptive Fireworks Algorithm
Fig. 8.4 Stage (2): Local search
Fig. 8.5 A histogram of the ratio by which the amplitude increases at stage (2)
(b) It is possible that all the sparks’ ﬁtness is better than the ﬁrework’s. It is most
likely to happen in stage (1). According to Algorithm 8.1, the amplitude will be
set to the range of the search space. It is still reasonable because the ﬁrework can
be considered a local maxima in this case, which means search around it will be
meaningless.

8.3 Adaptive Explosion Amplitude
127
Fig. 8.6 Stage (3): Reﬁne search
Therefore, even in the extreme cases, the algorithm still has a strong capability of
error correction.
The explosion amplitude could be considered as the step size in the ﬁreworks algo-
rithm. To the best of our knowledge, the adaptive explosion amplitude we proposed
here is a brand new method to control the step size in an evolutionary algorithm,
which would ﬁnd more potential application in other EC algorithms.
8.4 Adaptive Fireworks Algorithm
In this section, the adaptive explosion amplitude is applied to the best ﬁrework of
enhanced ﬁreworks algorithm.
At the beginning after initialization, we set the amplitude of the best ﬁrework to
the range of the search space. Then, for each generation, we use AEA algorithm to
calculate the amplitude of the best ﬁrework in the next generation.
When there are more than two ﬁreworks adopted in the algorithm, the ˆs we look
for in Sect.8.3 could be a spark as well as a ﬁrework.

128
8
Adaptive Fireworks Algorithm
The amplitudes of other ﬁreworks are still unchanged as calculated according to
Eq.(2.3), and the minimal amplitude check is unnecessary and are dropped in our
algorithm since it is designed to control the amplitude of the best ﬁrework. Except
for the amplitude of the best ﬁrework, we basically follow the operators in EFWA.
Algorithm 8.2 shows the complete version of the adaptive ﬁreworks Algorithm
[3].
Algorithm 8.2 Adaptive Fireworks Algorithm
1: randomly select m ﬁreworks in the potential space
2: evaluate their ﬁtness
3: A∗←UB −LB
4: repeat
5:
calculate Ni according to Eq.(2.1)
6:
calculate Ai according to Eq.(2.3)
7:
calculate A∗according to Algorithm 8.1
8:
generate Ni sparks according to Algorithm 6.1
9:
generate Gaussian sparks according to Algorithm 2.2
10:
evaluate all sparks’ ﬁtness
11:
keep the best individual as a ﬁrework
12:
randomly choose other m −1 ﬁreworks among the rest individuals
13: until termination criteria is met
14: return the best individual and its ﬁtness
In Algorithm 8.2, m is the number of ﬁreworks, UB and LB stand for the upper
bound and lower bound of the search space respectively, Ai stands for the amplitude
of each ﬁrework, Ni stands for the number of sparks of each ﬁrework, A∗stands for
the amplitude of the best ﬁrework.
Note that being different from the EFWA, m can be 1in the AFWA, because the
amplitude of the best ﬁrework is calculated independently.
We can see from Algorithm 8.2 that the number of parameters adopted in the
AFWA is less than that in EFWA, since EFWA adopts 2 parameters in minimal
amplitude check, which is now replaced by adaptive explosion amplitude.
8.5 Experiments
In order to illustrate and compare the performance of AFWA and EFWA, experiments
on 28 CEC13’s benchmark functions [4] were conducted. The introduction of the
28 functions is shown in Appendix A. In AFWA and EFWA, m = 5, Nmin = 2,
Nmax = 100, ˆN = 200, ˆA = 100 and NG = 5. In AFWA, λ = 1.3. Besides, the
results of SPSO2007 and SPSO2011, which is the latest version of the Standard
PSO, were adopted as a baseline. The parameters of SPSO2007 are the same as [5],
and the results of SPSO2011 are obtained directly from [6] and http://t.cn/8Fqg1rN.
Evaluation times: 300,000, Run times: 51, Dimension: 30. Experiment environment:
MATLAB2011b; Win 7; Intel Core i7-2600 CPU; 3.7GHZ; 8GB RAM.

8.5 Experiments
129
The mean error of the four algorithms is presented in Fig.8.7.
The mean ranking of the four algorithms’ mean error, which represents the mean
average error rank on 28 benchmark functions was calculated, shown in Table8.1.
For each function, each algorithm achieves the ﬁtness error rank, then the average
value is calculated for all functions to represent the performance for each algorithm.
Fig. 8.7 Mean error on 28 functions
Table 8.1 Mean ranking of mean error on 28 functions
Algorithm
SPS02007 [5]
SPSO2011 [6]
EFWA [2]
AFWA
Mean ranking
2.4286
2.1786
3.3929
1.8929
Table 8.2 T-test results on AFWA versus EFWA
Function
1
2
3
4
5
6
7
8
9
10
11
12
13
14
H
1
0
0
1
1
1
1
1
1
1
1
1
1
1
Function
15
16
17
18
19
20
21
22
23
24
25
26
27
28
H
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Fig. 8.8 Time consumed by each algorithm on 28 functions

130
8
Adaptive Fireworks Algorithm
A set of T-tests were also conducted to illustrate whether the improvement of
AFWA over EFWA is signiﬁcant. The null hypothesis is the results of the AFWA
and those of the EFWA come from distributions with equal means. H = 1 indicates
that the null hypothesis can be rejected at the 5% level. Table8.2 shows the H values.
Figure8.8 shows the time consumed by each algorithm.
8.6 Discussion
According to Fig.8.7, Tables8.1 and 8.2, we can see that AFWA outperformed EFWA
signiﬁcantly,exceptforfunction2andfunction3whereAFWAandEFWAperformed
almost the same, EFWA only beat AFWA on function 4 and function 18. AFWA and
EFWA share almost all the parameters in common, so the signiﬁcant difference in
their performance proved that the new adaptive explosion amplitude is signiﬁcantly
effective. More precisely, since the global search in AFWA and EFWA are almost the
same, the adaptive explosion amplitude actually improved its local search capability.
AFWA also performed much better than SPSO2007 and SPSO2011: AFWA beats
SPSO2011 on 18 out of 27 functions and beats SPSO2007 on 16 out of 27 functions
(except for function 1 where they are even). According to Table8.1, judging from
the overall performance, AFWA is the best algorithm among all the four algorithms.
In terms of computation cost, as is shown in Fig.8.3, the time consumed by AFWA
and EFWA were very close, which is less than SPSO. The computational cost of
calculating the adaptive explosion amplitude is very low(O(n)), compared to other
operators such as generating sparks. In addition, according to [7], the computation
cost of the EFWA is smaller than the PSO.
Lastly, it turns out from the performance and the computation cost that the adaptive
ﬁreworks algorithm is a very promising, efﬁcient, and simple algorithm.
8.7 Summary
In this chapter, we analyzed the amplitudes of explosion in FWA and EFWA, and
then proposed an adaptive explosion amplitude. The distance of the best ﬁrework
and a certain individual subjecting to some conditions is employed as the amplitude
of the explosion. We analyzed the property of the adaptive explosion amplitude and
come to the conclusion that the adaptive explosion amplitude for ﬁrework explosion
is a theoretically promising operator. Replacing with adaptive explosion amplitude,
adaptive ﬁreworks algorithm was proposed. According to the experimental results on
CEC13’s 28 benchmark functions, the performance is greatly improved: the AFWA
not only outperforms EFWA but also beats SPSO2007 and SPSO2011 totally.

References
131
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 IEEE Congress on Evo-
lutionary Computation (CEC) (IEEE, 2013), pp. 2069–2077
3. J. Li, S. Zheng, Y. Tan, Adaptive ﬁreworks algorithm, in IEEE Congress on Evolutionary Com-
putation (CEC) (2014), pp. 3214–3221
4. J.J. Liang, B.Y. Qu, P.N. Suganthan, A.G. Hernández-Díaz, Problem Deﬁnitions And Evaluation
Criteria for the CEC 2013 Special Session On Real-parameter Optimization (2013)
5. D. Bratton, J. Kennedy, Deﬁning a standard for particle swarm optimization, in Swarm Intelli-
gence Symposium, SIS 2007 (IEEE, 2007), pp. 120–127
6. M. Zambrano-Bigiarini, M. Clerc, R. Rojas, Standard particle swarm optimisation 2011 at
CEC-2013: a baseline for future PSO improvements, in 2013 IEEE Congress on Evolutionary
Computation (CEC) (2013), pp. 2337–2344
7. Y. Tan, C. Yu, S. Zheng, K. Ding, Introduction to ﬁreworks algorithm. Int. J. Swarm Intell. Res.
(IJSIR) 4(4), 39–70 (2013)

Chapter 9
Cooperative Fireworks Algorithm
In the previous studies on FWAs, researchers ignored the cooperation and interaction
between the individual ﬁreworks in the swarm, which are the most important core
for any swarm intelligence algorithm. By incorporating a probabilistically oriented
explosion mechanism (POEM) into the conventional FWA, a novel Cooperative Fire-
works Algorithm (CoFWA, for short) is proposed to enhance the interactions among
the individual ﬁreworks in the swarm. In the CoFWA, the POEM mechanisms of
sparks generation and ﬁreworks selection are well designed to strengthen the coop-
erative capability of the individual ﬁreworks in the CoFWA. It turns out by many
experiments that the CoFWA signiﬁcantly outperforms two most recent variants of
FWA (i.e., EFWA and dynFWA) and SPSO2011 and shows a competitive perfor-
mance against the state-of-the-art swarm intelligence algorithms.
9.1 Introduction
The Fireworks Algorithm (FWA) [1] is a newly developed successful swarm intel-
ligence (SI) algorithm inspired by explosion of ﬁreworks in real life. Like other
swarm intelligence algorithm, it also aims at ﬁnding the position with the best (usu-
ally minimum) ﬁtness in the search space. Inspired by real ﬁreworks, the ﬁreworks
(i.e., individuals) in FWA are set off at the potential search space. In the initial-
ization step, a few of ﬁreworks are randomly chosen in the search space and their
corresponding ﬁtness is evaluated accordingly. Subsequently, the ﬁreworks explode
and then generate different types of sparks within their local areas. Finally, some
ﬁreworks are selected from the set of candidates which include either the newly
generated sparks or the original ﬁreworks for the next generation. In such a way,
the algorithm continues this searching until a termination criterion (time, maximum
number of iterations or ﬁtness evaluations, or convergence) is reached. Since FWA
has proven its efﬁciency in dealing with optimization problems, a lot of improvement
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_9
133

134
9
Cooperative Fireworks Algorithm
works on FWA have been done and published, such as Enhanced Fireworks Algo-
rithm (EFWA) [2], Adaptive Fireworks Algorithm (AFWA) [3], Dynamic Search in
Fireworks Algorithm (dynFWA) [4] and just to name a few.
FWA consists of four parts, i.e., initialization, explosion, mutation, and selection.
In the part of explosion, the explosion amplitude and the population of the newly
generated explosion sparks differ among the ﬁreworks in order to ensure diversity
and balance the global and local searches. Fireworks located at good positions can
generate a large population of explosion sparks within a smaller range, i.e., with a
small explosion amplitude. On contrary, ﬁreworks located at positions with lower
ﬁtness values can only generate a smaller population within a larger range, i.e., with
higher explosion amplitude.
In the selection process, the best ﬁrework is always kept, but the rest m −1
ﬁreworks are selected randomly.
In the framework of FWA, one can discover that the cooperation and interaction
between the individuals in the swarm do not play an important role in the FWA. The
interaction between ﬁreworks is only reﬂected on the population of explosion sparks
and the explosion amplitude, but the distribution of how the sparks are generated is
simply regarded as a uniform distribution, which does not use any information from
other ﬁreworks. In particular, in the selection part, only the best ﬁrework is kept and
the other m −1 ﬁreworks are randomly selected among sparks. We consider that this
kind of selection mechanism makes the ﬁreworks lose too much information from
previous generations even though it guarantees the mutation of ﬁreworks.
9.2 Probabilistically Oriented Explosion Mechanism
(POEM)
As indicated by the name, the Probabilistically Oriented Explosion Mechanism
(POEM) is a way with which a ﬁrework explodes in different directions with differ-
ent probabilities. Unlike commonly used the uniform explosion with which ﬁrework
explodes in all direction with same probability, POEM means a ﬁrework is able to
explode in anisotropy with probability, in different directions with different proba-
bilities, as illustrated by Figs.9.1, 9.2 and 9.3.
In the POEM, a kind of interaction between each ﬁrework and the best ﬁrework
in the current swarm is designed to determine the orientation of explosion of each
ﬁrework. First of all, the best ﬁrework is called as the global guide (Sg) in the current
swam. Second, when a ﬁrework (Sl, (l = 1, . . . , N,l ̸= g)) except the best one
explodes, the sparks will not generate randomly in the amplitude and directions,
instead, more sparks will be generated in the area in the directions of the global
guide but fewer sparks will be generated in the directions which are away further
from the global guide, as illustrated in Figs.9.1, 9.2 and 9.3.

9.2 Probabilistically Oriented Explosion Mechanism (POEM)
135
Fig. 9.1 Gaussian explosive way in POEM
Fig. 9.2 Ellipsoid explosive way in POEM
Fig. 9.3 Sinc explosive way in POEM
There are a number of ﬁrework explosive ways in POEM so long as they obey
the above explanation for POEM. For example, one can have POEM with Gaussian
explosive way, POEM with Ellipsoid explosive way and POEM with Sinc explo-
sive way, as shown in Figs.9.1, 9.2 and 9.3, respectively. By use of those kinds
of anisotropic explosions for each ﬁrework, the cooperation and interaction among
ﬁreworks would be established and then strengthened further against the uniform
explosion in FWA as before. As a result, a promising performance is expected. Sure,
more evidences will be noted in the successive sections.
1. POEM with Gaussian explosive way
2. POEM with Ellipsoid explosive way
3. POEM with Sinc explosive way

136
9
Cooperative Fireworks Algorithm
9.3 Remarks on Parameters
There are following three parameters which should be explained in detail.
1. Am: it is the amplitude of explosion which is inversely proportional to the ﬁtness
of the ﬁrework Sl, (l = 1, . . . , N).
2. ρ(θ): It is a point in polar coordinate system. It will take the maximum value in
the (core) direction to the elite ﬁrework Sg and gradually fade as increase of the
angle deviated from the core direction. It should be a normalized parameter like
a probability.
3. Ml: number of sparks generated by lth ﬁrework (a local guide), which is propor-
tional to the ﬁtness of the lth ﬁrework.
4. Sg: the estimate of the global optimum.
5. Sl, (l = 1, . . . , N,l ̸= g): the estimates of local optima by ﬁreworks.
9.4 Framework of Cooperative Firework Algorithm
(CoFWA)
There are following four steps in the Framework of Cooperative Firework Algorithm
(CoFWA).
Step 1: Initialization—A few of ﬁreworks in a swam, i.e., 5 ﬁreworks, are gener-
ated randomly in the search space.
Step 2: POEM—A shower of sparks generated in POEM by explosion of each
ﬁrework.
Step 3: Memory—The elite spark with highest ﬁtness among all sparks and ﬁre-
works is selected as a global guide in the current swarm, denoted by Sg. The spark
with highest ﬁtness in those sparks generated in POEM by one ﬁrework survives,
with a probability pl (which is proportioned to the ﬁtness), for next explosion, as a
local guide, Sl, (l = 1, . . . , N,l ̸= g), where N is the number of ﬁreworks in the
swarm, i.e., N = 5.
Step 4: Mapping Rule—there are two typical mapping rules to be exploited, i.e.,
Random Rule (If a spark is outside boundaries, it will be thrown back randomly) and
Module Rule (taking module operation once a spark is outside boundaries).
Repeat Step 1–Step 4 until a termination criterion is reached.
It turns out that the CoFWA is a kind of swarm intelligence optimization algorithm
which optimizes with a champion. In other words, Sg is the estimate of the global
optimum, i.e., the champion, while Sl, (l = 1, . . . , N,l ̸= g) are the estimates of
local optima by ﬁreworks.

9.5 A Kind of Realization of POEM
137
9.5 A Kind of Realization of POEM
In order to strengthen the cooperation and interaction among ﬁreworks of FWA,
a mechanism called Probabilistically Oriented Explosion Mechanism (POEM) is
adoptedhere.InthePOEM,wedesignakindofinteractionbetweeneachﬁreworkand
the best ﬁrework in the current generation to determine the orientation of explosion
of each ﬁrework. We regard the best ﬁrework as the global guide (Sg) in the current
swam. When a ﬁrework (Sl) except the best one explodes, the sparks will not generate
randomly in the amplitude, instead, more sparks will be generated in the area which
is closer to the global guide but fewer sparks will be generated in the area which
is further from the global guide, as shown in Fig.9.4. In order to achieve this goal,
a special Gaussian mutation and a shift from the ﬁrework to the global guide is
designed in POEM.
Speciﬁcally,thereisnoexplosionorientationinFWA,asshowninFig.9.5.Inorder
to enhance the cooperation between ﬁreworks, at ﬁrst, the sparks will be generated
by a normal distribution with mean 0 and standard deviation b in the ﬁrst dimension
and standard deviation a in the next d −1 dimension as illustrated in Fig.9.6. Next, a
matrix is calculated to rotate the orientation of the ﬁrst dimension toward the global
best ﬁrework, as shown in Fig.9.7. Finally, a shift is calculated to move the sparks
closer to the global best, as shown in Fig.9.8.
A realization of the POEM can be speciﬁcally written as follows:
POEM Probabilistically Oriented Explosion Mechanism
for Sl
shift = Sg −Sl
sample r1 from U(0, b)
sample r j from U(0, a), j = 2, 3, . . . , d
r = (r1,r2, . . . ,rd)
sli ←Sl + M ∗r + λ ∗shift, i = 1,2,…, Nl
end for
Where U(0, d) is the normal distribution with mean 0 and standard deviation d,
Nl is the number of sparks of ﬁrework l, λ is a parameter which controls the shift
from the ﬁrework to the global guide on a certain level, and M is the matrix to get
the orientation from Sl to Sg. b is usually set to max{|shifti|, i = 1, 2, . . . , d}, and a
is equal to β ∗b where 0 < β < 0.5.

138
9
Cooperative Fireworks Algorithm
Fig. 9.4 Schematic diagram of POEM
Fig. 9.5 There is no explosion orientation in FWA
Fig. 9.6 Sparks generated by the normal distribution with zero mean and standard deviation b in
the ﬁrst dimension and a in the next d −1 dimension

9.6 The Proposed CoFWA
139
Fig. 9.7 Rotating the orientation of the ﬁrst dimension toward the global best ﬁrework
Fig. 9.8 A shift is calculated to make the sparks closer to the global best
9.6 The Proposed CoFWA
By incorporating the (POEM) into the original FWA, a novel Cooperative Fireworks
Algorithm (CoFWA) is proposed here for enhancing the cooperations and interac-
tions of the individual ﬁreworks. Brieﬂy, the CoFWA contains the following four
main parts:
1. Initialization
m ﬁreworks are initialized randomly as the ﬁrst generation.
2. Explosion
Each ﬁrework’s ﬁtness is evaluated to determine the number of sparks for each
ﬁrework. A ﬁrework with higher ﬁtness can generate a greater number of sparks
and a ﬁrework with lower ﬁtness can only generate a smaller number of sparks.
And then, each ﬁrework except the global guide ﬁrework generates a certain
number of sparks by POEM. If the location of a spark is out of bound, it will be

140
9
Cooperative Fireworks Algorithm
randomly mapped into the search space. As for the global guide, it will generate
the sparks the same way as the best ﬁrework in dynFWA.
3. Mapping rule
Random Rule is used here. If the location of a spark is out of bound, it will be
randomly mapped into the search space.
4. Selection
The best spark of each ﬁrework in the current generation has remained as the ﬁre-
work for the next generation. In this selection algorithm, the ﬁreworks selected
in the current generation will help the global guide in the next generation to
search the best ﬁtness more efﬁciently in the search space.
The parts (2)–(4) are repeated until the maximal number of iterations.
Then we will brieﬂy introduce some operators used in the CoFWA.
Like in FWA, the numbers of sparks that are exploded by each ﬁrework are
calculated as follows:
Nl = ˆN ·
ymax −f (Sl) + ξ
m
l=1(ymax −f (Sl)) + ξ
(9.1)
where ˆN is a parameter of controlling the total number of sparks generated by the m
ﬁreworks, ymax = max( f (Sl))(l = 1, 2, 3, . . . , m) is the maximum (worst) value
of the objective function among the m ﬁreworks, and ξ, which denotes the smallest
constant in the computer, is utilized to avoid zero division error. In order to avoid the
overwhelming effects of splendid ﬁreworks, the number of sparks is bounded by
Nl =
⎧
⎨
⎩
Nmin if Nl < Nmin
Nmax if Nl > Nmax
Nl
otherwise
(9.2)
where Nmin and Nmax are the lower bound and upper bound for the spark numbers.
Algorithm 9.1 shows how sparks are generated in the CoFWA.
Algorithm 9.1 Sparks generated by a ﬁrework in CoFWA
for l = 1 to m do
shift = Sg −Sl
M is the matrix to get the orientation between Sl
and Sg
if Sg ̸= Sl then
for i = 1 to Nl do
sample sli from POEM
end for
else then
for i = 1 to Nl do
sample r from U(0, 1)
sli ←Sl + r ∗scope,
end for
end if
end for
return all the sli

9.6 The Proposed CoFWA
141
Where scope is the explosion amplitude of the global guide.
Algorithm 9.2 shows the selection.
Algorithm 9.2 Selecting ﬁreworks for the next generation
for l = 1 to m
Sl ←arg minsli ( f (sli))
end for
return Sl
Finally, the complete version of the CoFWA is shown in Algorithm 9.3.
Algorithm 9.3 Cooperative Fireworks Algorithm
Initialize m ﬁreworks xi and constant parameters
repeat
Calculate sparks number and amplitude
Generate explosion sparks by Algorithm 9.1
Evaluate ﬁtness of newly created sparks
Select ﬁreworks for next iteration by Algorithm 9.2
until termination
9.7 Convergence Theorem of CoFWA
In this section, we want to give a convergence proof for CoFWA.
At ﬁrst, we will describe a proposition of global convergence criteria.
Given a function f from Rn to R and S a subset of Rn. We seek a point z in
S which minimizes f on S or at least which yields an acceptable approximation
of the inﬁmum of f and S. This proposition provides a deﬁnition of what a global
optimizer must produce as output, given the function f and the search space S.
The random search algorithm can be described as follows: given a random initial
starting point in S, called z0. In the kth iteration, this algorithm requires a probability
space (Rn, B, μk), where μk is a probability measure (corresponding to a distribution
function on Rn) on B, and B is the σ-algebra of subsets of Rn. Then a vector ξk will be
generated from the sample space (Rn, B, μk). In the k + 1th iteration, the point zk+1
equals to D(zk, ξk) and μk updates to μk+1, where D is a function that constructs a
solution to the problem.
Next, we will ﬁrst give a local convergence proof for the CoFWA. Before that,
we want to introduce a theorem:
H 1 f (D(z, ξ)) ≤f (z) and if ξ ∈S, then f (D(z, ξ)) ≤f (ξ)
H 2 To any z0 ∈S, there corresponds a γ > 0 and an 0 < η ≤1 such that:
μk[(dist(D(z, ξ), Rϵ) ≤dist(z, Rϵ) −γ) or
(D(z, ξ) ∈Rϵ)] ≥η

142
9
Cooperative Fireworks Algorithm
for all k and all z in the compact set L0 = {z ∈S| f (z ≤f (z0)}. Where
Rϵ = {z ∈S| f (z) < ψ + ϵ}, and ψ = inf(t : v[z ∈S| f (z < t] > 0), v[A] is the
Lebesgue measure on the set A.
Theorem 9.1 Suppose that f is a measurable function, S is a measurable subset of
Rn and (H1) and (H2) are satisﬁed. Let {zk}∞
k=0 be a sequence generated by the
algorithm. Then,
lim
k→∞P[zk ∈Rϵ] = 1
where P[zk ∈Rϵ] is the probability that at step k, the point zk generated by the
algorithm is in the optimality region, Rϵ.
Proof Let z0 be the initial starting point generated randomly. Since L0 is compact,
there always exists an integer p such that (by assumption H2)
γ p > dist(a, b) ∀a, b ∈L0.
By (H3) it follows that
P[z1 ∈Rϵ] ≥η
and
P[z2 ∈Rϵ] ≥η × P[z1 ∈Rϵ] ≥η2.
There probabilities are disjoint, so repeated application (p times) of (H2) yields
P[zp ∈Rϵ] ≥η p
Applying (H2) another p times results in
P[z2×p ∈Rϵ] ≥η2×p
Hence, for k = 1, 2, . . .
P[zkp ∈Rϵ] = 1 −P[zkp /∈Rϵ] ≥1 −(1 −η p)k.
Now (H1) implies that z1, . . . , zp−1 all belong to L0 and by the above it then follows
that
P[zkp+l ∈Rϵ] ≥1 −(1 −η p)k
for
l = 0, 1, . . . , p −1.
This shows that all steps between kp and (k + 1)p satisfy (H2).
This completes the proof, since (1 −η p)k tends to 0 as k goes to +∞.
□
According to the above theorem, it remains to show that the CoFWA satisﬁes
both (H1) and (H2) to prove local convergence. The proof will be ﬁrst presented for
unimodal optimization problems, after which the multimodal case will be discussed
again.

9.7 Convergence Theorem of CoFWA
143
The proof for the CoFWA starts by choosing the initial value.
x0 = arg max
xi { f (xi)}, i ∈1 . . . m,
where xi represents the position of ﬁrework i. That is, x0 represents the worst ﬁre-
work, yielding the largest f value. Now deﬁne L0 = {x ∈S| f (x) ≤f (x0)}, the set
of all points with f values smaller than that of the worst ﬁrework x0. It is assumed
that all the ﬁreworks lie in the same ‘basin’ of the function.
In the next section this will be extended to the general case with multiple basins.
From Algorithm 9.2, function D introduced in assumption H1 is deﬁned for the
CoFWA as
D(Sg,k, Sl,k) =
 Sg,k
if f (xl,k,i) ≥f (Sg,k)
xl,k, j if j = arg mini( f (xl,k,i))
and
f (xl,k, j) < f (Sg,k)
where xl,k, j, j = 1, 2, . . . , are the sparks that the ﬁrework Sl generates in the kth
iteration. The deﬁnition of D above clearly complies with (H1), since the Sg is
monotonic by deﬁnition.
Then we go back to the generation of the sparks of the best ﬁrework.
xg,k+1 = Sg,k + r ∗ρk,
sample r from U(0, 1)
(9.3)
where ρ is the scope of the explosion of the best ﬁrework. In the dynFWA, stagnation
is prevented by ensuring that ρ > 0 for all time steps. Note that Sg is always in L0.
It is possible, however, that xg,k+1 /∈L0, due to the cumulative effect of a growing
r ∗ρk vector, so that Sg,k + r ∗ρk /∈L0. But Sg ∈Mk and ∈L0, so Sg ∈Mk ∩L0.
This implies that v[Mk ∩L0] > 0, so a new sampled spark arbitrarily close to Sg,
and thus in L0, can be generated. Using this fact, it is now possible to consider the
local convergence property of the CoFWA.
If we assume that S is compact and has a nonempty interior, then L0 will also be
compact with a nonempty interior.
Further, L0 will include the essential inﬁmum, contained in the optimality region
Rϵ, by deﬁnition. Now Rϵ is compact with a nonempty interior, thus we can deﬁne
a ball B′ centered at c′ contained in Rϵ, as shown in Fig.9.6. Now pick the point
x′ ∈arg maxx{dist(c′, x)|x ∈L0}, as illustrated in Fig.9.9.
Let B be the hypercube centered at x′, with sides of length 2(dist(c′, x′) −0.5ρ).
Let C be the convex hull of x′ and B′. Consider a line tangent to B′, passing
through x′ (i.e one of the edges of C). This line is the longest such line, for x′ is the
point furthest from B′. This implies that the angle subtended by x′ is the smallest
such angle of any point in L0. In turn, this implies that the volume C ∩B is smaller
than that of C′∩B for any other convex hull C′ deﬁned by any arbitrary point x ∈L0.

144
9
Cooperative Fireworks Algorithm
Fig. 9.9 Schematic diagram
Then for all x in L0
μk[(dist(D(Sg,k, Sl,k), Rϵ) ≤dist(Sg,k, Rϵ) −0.5ρ] ≥η
= μ[C ∩B] > 0
(9.4)
where μk is the uniform distribution on the hypercube centered at x, with side length
2ρ. It was shown above that the CFWA can provide such a hypercube.
Since μ[C ∩B] > 0, the probability of selecting a new point x so that it is closer
to the optimality region Rϵ is always nonzero.
This is sufﬁcient to show that the CoFWA complies with (H2).
So this completes the proof that the sequence values {Sg,k}∞
k=0 generated by the
CoFWA will converge to the optimality region.
In sequel, we talk about function with multiple minima.
It was assumed above that L0 was convex-compact. A non-unimodal function,
with S including multiple minima, will result in a non-convex set L0. Even if all
the sparks are contained in the same convex subset, the CoFWA is not guaranteed to
yield a point in the same convex subset as it started from, especially not during the
earlier iterations. This is because the scope of explosion can yield a value larger than
the diameter of the basin in which the ﬁrework currently resides. If the point found in
a different convex subset yields a function value smaller than the current global best
ﬁrework, then the algorithm will move its global ﬁrework to this new convex subset
of L0. This process could continue until the algorithm converges onto the essential

9.7 Convergence Theorem of CoFWA
145
Table 9.1 List of mean ﬁtness on the 28 benchmark function and mean ﬁtness rank for SPSO2011,
EFWA, dynFWA, and CoFWA
Fun.
SPSO2011
Rank
EFWA
Rank
dynFWA
Rank
CoFWA
Rank
1
−1.4000E+03
1
−1.3999E+03
4
−1.4000E+03
1
−1.4000E+03
1
2
3.3719E+05
1
6.8926E+05
3
8.6937E+05
4
6.6407E+05
2
3
2.8841E+08
4
7.7586E+07
2
1.2317E+08
3
7.0544E+07
1
4
3.7543E+04
4
−1.0989E+03
2
−1.0896E+03
3
−1.0997E+03
1
5
−1.0000E+03
1
−9.9992E+02
4
−1.0000E+03
1
−1.0000E+03
1
6
−8.6210E+02
3
−8.5073E+02
4
−8.6995E+02
2
−8.7226E+02
1
7
−7.1208E+02
2
−6.2634E+02
4
−7.0010E+02
3
−7.4492E+02
1
8
−6.7908E+02
3
−6.7907E+02
4
−6.7910E+02
1
−6.7910E+02
1
9
−5.7123E+02
3
−5.6846E+02
4
−5.7587E+02
2
−5.8191E+02
1
10
−4.9966E+02
3
−4.9916E+02
4
−4.9995E+02
1
−4.9992E+02
2
11
−2.9504E+02
3
5.8198E+00
4
−2.9589E+02
2
−3.2202E+02
1
12
−1.9604E+02
2
3.9944E+02
4
−1.4222E+02
3
−2.4554E+02
1
13
−6.1406E+00
2
2.9857E+02
4
5.3830E+01
3
−7.5014E+01
1
14
3.8910E+03
4
2.7240E+03
2
2.9180E+03
3
2.4201+E03
1
15
3.9093E+03
2
4.4595E+03
4
4.0227E+03
3
3.312+E03
1
16
2.0131E+02
4
2.0063E+02
3
2.0058E+02
2
2.0019E+02
1
17
4.1626E+02
2
6.2461E+02
4
4.4261E+02
3
3.8921E+02
1
18
5.2063E+02
2
5.7361E+02
3
5.8782E+02
4
4.8429E+02
1
19
5.0951E+02
3
5.1022E+02
4
5.0726E+02
2
5.05986E+02 1
20
6.1346E+02
2
6.1466E+02
4
6.1328E+02
1
6.1402E+02
3
21
1.0088E+03
1
1.1178E+03
4
1.0102E+03
3
1.0098E+03
2
22
5.0988E+03
3
6.3181E+03
4
4.1262E+03
1
4.1705E+03
2
23
5.7313E+03
3
7.5809E+03
4
5.6526E+03
2
4.2643E+03
1
24
1.2667E+03
2
1.3452E+03
4
1.2729E+03
3
1.2599E+03
1
25
1.3993E+03
3
1.4426E+03
4
1.3970E+03
2
1.3822E+03
1
26
1.4861E+03
3
1.5461E+03
4
1.4607E+03
2
1.4011E+03
1
27
2.3046E+03
3
2.6210E+03
4
2.2804E+03
2
2.0597E+03
1
28
1.8013E+03
3
4.7651E+03
4
1.6961E+03
1
1.7010E+03
2
Mean
rank
2.57
3.67
2.25
1.25
inﬁmum contained in its convex subset, at which point it will no longer be able to get
out of the convex subset if the diameter of the subset is greater than 2ρ. By the end,
the global ﬁrework converges to the local minimum of the current convex subset. □

146
9
Cooperative Fireworks Algorithm
Fig. 9.10 Convergence curves of CoFWA on 28 Benchmark functions (f1–f8) of IEEE CEC 2013
averaged over 30 simulation runs. a f 1, b f 2, c f 3, d f 4, e f 5, f f 6, g f 7, h f 8

9.7 Convergence Theorem of CoFWA
147
Fig. 9.11 Convergence curves of CoFWA on 28 Benchmark functions (f21–f28) of IEEE CEC
2013 averaged over 30 simulation runs. a f 21, b f 22, c f 23, d f 24, e f 25, f f 26, g f 27, h f 28

148
9
Cooperative Fireworks Algorithm
9.8 Experiments
The experimental platform is MATLAB 2013a, running under Windows 8.1 on an
Intel Core i5 CPU with 2.4GHz and 4GB RAM. The parameters in CoFWA are set as
follows. The number of ﬁreworks is set to 5 and the maximum number of explosion
sparks is 100. The parameter λ is 0.34. All other parameters for dynFWA and all
EFWA parameters are identical to [2, 4], SPSO2011 parameters are listed in [5, 6].
To validate the performance of the proposed CoFWA algorithm, we used the IEEE
CEC’2013 benchmark suite that includes 28 different benchmark functions as listed
in [7].
For the algorithms in experiments, the detailed descriptions are given below.
• EFWA—the enhanced ﬁreworks algorithm which is an efﬁcient improvement of
the conventional ﬁreworks algorithm.
• dynFWA—the dynamic search ﬁreworks algorithm which used a dynamic explo-
sion amplitude for the ﬁrework at the current best location.
• CoFWA—as described above.
• SPSO2011—the most recent SPSO variant. Compared to earlier versions of SPSO,
it features an improved velocity update by exploiting the idea of rotational invari-
ance for the velocity update instead of sequential dimension-by-dimension update
in the previous SPSO versions.
In this chapter, every algorithm runs 51 times on each benchmark optimization
function and the number of ﬁtness function evaluations is set to 3 ∗105.
Table9.1 presents the experimental results of four algorithms (SPSO2011, EFWA,
dynFWA, CoFWA) on the 28 benchmark functions. According to Table9.1, it turns
out that the CoFWA outperformed EFWA and dynFWA as well as SPSO2011 sig-
niﬁcantly.
In order to show the convergence performance of the CoFWA, Fig.9.10 plotted
the convergence curves of the CoFWA on benchmark functions f1–f8, and Fig.9.11
plotted the convergence curves of the CoFWA on benchmark functions f21–f28.
According to the results in Table9.1, we can rank the four algorithms in terms of
average mean ranks. As a result, Table9.2 shows the ranking among CoFWA with
EFWA and dynFWA as well as SPSO2011. It can be seen from Table9.2 that CoFWA
ranks the ﬁrst among the four algorithms.
Table 9.2 Ranking of
SPSO2011, EFWA,
dynFWA, and CoFWA
Algorithms
Ranking
CoFWA
1
dynFWA
2
SPSO2011
3
EFWA
4

9.9 Conclusion
149
9.9 Conclusion
In this chapter, the shortcomings of FWA which do not have the efﬁcient mechanism
of interaction among ﬁreworks were pointed out. To deal with this critical issue,
a probabilistically oriented explosion mechanism (POEM) is designed to enhance
the cooperation in a swarm of ﬁreworks. Based on the detailed analysis, a new
algorithm called Cooperative Fireworks Algorithm (CoFWA) was proposed and its
convergence was also proven. The convergence proof guaranteed that CoFWA is
theoretically efﬁcient and the experiments gave a strong evidence that the CoFWA
performs much better than the EFWA and the dynFWA as well as SPSO2011.
In future, we will try to reduce the constant parameters and set them automatically
according to the environment.
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 Congress on Evolutionary
Computation (CEC) (IEEE, 2013), pp. 2069–2077
3. J. Li, S. Zheng, Y. Tan, Adaptive Fireworks Algorithm, in IEEE Congress on Evolutionary
Computation (CEC) (2014), pp. 3214–3221
4. S. Zheng, A. Janecek, Y. Tan, Dynamic Search in Fireworks Algorithm, in IEEE Congress on
Evolutionary Computation (CEC) (2014), pp. 1–7
5. M. Clerc, Standard Particle Swarm Optimization (2006–2011)
6. M. Zambrano-Bigiarini, M. Clerc, R. Rojas, Standard Particle Swarm Optimisation 2011 at
CEC-2013: A Baseline for Future PSO Improvements, in IEEE Congress on Evolutionary Com-
putation (CEC) (2013), pp. 2337–2344
7. J.J. Liang, B.Y. Qu, P.N. Suganthan, A.G. Hernández-Díaz, Problem deﬁnitions and evaluation
criteria for the CEC 2013 special session on real-parameter optimization (2013)

Chapter 10
Hybrid Fireworks Algorithms
Fireworks algorithm has a broad research area and is suitable for combination
with other algorithms to produce a new hybrid algorithm. This chapter focuses on
hybrid ﬁreworks algorithms, including Fireworks Algorithm with Differential Muta-
tion (FWA-DM), Hybrid Fireworks Optimization Method with Differential Evo-
lution Operators (FWA-DE), Culture Fireworks Algorithm (CFWA), and Hybrid
Biogeography-Based Optimization and Fireworks Algorithm (BBO_FWA).
10.1 FWA-DM
FWA consists of explosion operator, mutation operator, mapping rule, and selection
strategy [1]. FWA mutation operation is not limited to Gaussian mutation, but can
also be the differential mutation. The operation of mutation enhances the diversity
of FWA. Differential mutation (DM) operator utilizes the difference information
between individuals and improves the interaction capability of populations. FWA-
DM can improve the performance of FWA.
Forﬁreworksalgorithm,mutationoperatorisaveryimportantoperator.Aﬁrework
is able to search both its vicinity and beyond area. The experimental results of FWA
is better if the mutation operator is utilized. Each ﬁrework mutates to enhance the
diversity of the population. In addition, this search mechanism can be replaced by
other mutation mechanism to further enhance the search performance of FWA.
In this section, the differential mutation operator is ﬁrst introduced. Then, the
process of utilizing differential mutation operator in FWA is exhibited. The ﬁgure of
the process is also shown, along with the details of the new algorithm.
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_10
151

152
10
Hybrid Fireworks Algorithms
10.1.1 Differential Mutation Operator
Operator: DM/best/1/exp. In this operator, DM means the differential mutation oper-
ator and the word best indicates that the best one is kept for the mutation. Number
1 means the number of difference vectors used and the abbreviation ‘exp’ stands for
an exponent recombination. The formula for this operator is as follows:
Xk
i1 = Xk
B + F ∗(Xk
i2 −Xk
i3),
(10.1)
where Xk
i1 means the k dimension of the target individual and F is the scale factor
generally between 0 and 2 [2]. Xk
B is the k dimension of the current best individual,
while Xk
i2 and Xk
i3 are two distinguished random individuals on their k dimensions.
The left part of Fig.10.1 represents Gaussian mutation operator and the right part
shows the details of DM operator.
10.1.2 Applying DM to EFWA
After introducing EFWA and DM, respectively, it is important and right time to apply
DM to EFWA and thus compensate the shortcomings of EFWA.
Assume N denotes the number of individuals in a population for EFWA, which
does not change during the optimization process. At ﬁrst, N individuals are selected
randomly and should lie in the feasible space. The N individuals form a population
and the population is marked as POP1. Next, for each individual, a spark is produced
around it within a certain amplitude.
A = Amin ∗rand(0, 1),
(10.2)
Fig. 10.1 The difference between GM and DM

10.1 FWA-DM
153
Amin = Ainit −(Ainit −Aﬁnal)
∗√(2 ∗evalsmax −t) ∗t/evalsmax,
(10.3)
where A means the amplitude of each ﬁrework, while Amin decreases in the way
of nonlinear. rand(0, 1) generates random number from 0 to 1. Ainit and Aﬁnal are
constants, representing the initial and ﬁnal amplitudes of the explosions. Parameter
t stands for the number of iterations so far and parameter evalsmax is the maximum
function evaluation times.
Then, each new generated spark is compared with its corresponding individual.
The one with better ﬁtness value is kept and used to form a new population with N
individuals marked as POP2. Finally, the DM operator is applied to POP2 and a new
population is generated as POP3.
To select the individuals for next generation and continue the evolutionary process,
the individuals in population POP3 are compared with individuals at the correspon-
dence places in population POP2. The better ones are selected and passed down to
the next generation, forming a new population POP1. The iteration of FWA-DM
continues till the terminate condition is met.
The process of applying DM to EFWA is drawn in Fig.10.2. The ﬁrst row rep-
resents population POP1 with N individuals. The second row shows the generated
explosion sparks after applying EFWA to POP1. After comparing the sparks in the
ﬁrst row with explosion sparks in the second row, better sparks are chosen and dis-
played in the third row. Then DM operator is used and population POP3 is produced.
The better individuals between population POP2 and POP3 are selected for next
iteration as a new population POP1. It is obvious that since DM is introduced, the
communication of individuals is enhanced. As a result, the diversity of the population
is guaranteed. In summary, the algorithm of FWA-DM [3] is given below.
Fig. 10.2 The process of applying DM to enhanced FWA

154
10
Hybrid Fireworks Algorithms
Algorithm 10.1 The process of FWA-DM
1: randomly generate N individuals as POP1
2: while terminal condition is not met do
3:
generate N sparks from POP1 as explosion sparks
4:
choose better individuals as POP2
5:
apply DM operator and generate POP3
6:
choose better individuals between POP2 and POP3 as a new POP1
7: end while
It canbeseenfromAlgorithm10.1that FWA-DMis simpleandeasytoimplement.
FWA-DM provided a brand new way to solve function optimization problems
by introducing DM operator to improve the performance of EFWA. Experimental
results on 30 benchmark functions of CEC 2014 proved that FWA-DM could solve
many function optimization problems effectively, as it outperformed EFWA on most
functions.
10.2 FWA-DE
FWA is a relatively new swarm-based metaheuristic for global optimization. The
algorithm is inspired by the phenomenon of ﬁreworks display and has a promising
performance on a number of benchmark functions. However, in the sense of swarm
intelligence, the individuals including ﬁreworks and sparks are not well-informed by
the whole swarm. We develop an improved version of the FWA by combining with
differential evolution (DE) operators: mutation, crossover, and selection. At each
iteration of the algorithm, most of the newly generated solutions are updated under
the guidance of two different vectors that are randomly selected from highly ranked
solutions, which increases the information sharing among the individual solutions to
a great extent. Experimental results show that the DE operators can improve diversity
and avoid prematurity effectively, and the hybrid method outperforms both the FA
and the DE on the selected benchmark functions.
For a D-dimensional optimization, the ﬁtness value of a solution is determined by
values of all components, and a solution that has discovered the region corresponding
to the global optimum in some dimensions may have a low ﬁtness value because of the
poor quality in the other dimensions [4]. Thus, many population-based optimization
methods, including DE, comprehensive learning PSO [4], fully informed PSO [5],
enable the individuals to make use of the beneﬁcial information in the swarm more
effectively to generate better quality solutions.
In FWA, after obtaining the set R of all ﬁreworks and sparks, the locations for
new ﬁreworks are selected based on distance to other locations in R so as to keep
diversity of the swarm. Here we introduce the DE operators to the FWA to improve
the diversiﬁcation strategy.

10.2 FWA-DE
155
In the hybrid algorithm, after obtaining the set R of locations, we ﬁrst sort the
locations in decreasing order of ﬁtness, and create a set S of p candidate locations
which are randomly selected from the top 2p locations in R. Afterward, we apply
the standard DE process to the new solution set S: for each vector xi ∈S, generate
a mutant vector vi, mix the components of xi and vi to obtain a trial vector ui, and
replace xi with ui in case that ui is better.
After the DE process, we check that whether the new best result solution x∗
1 ∈S
is worse than the ﬁrst (best) one x∗∈R. If it is so, we replace a randomly selected
solution x ∈S with x∗, and thus make the best solution of the swarm will never
degrade in the next generation. The DE procedure is shown as follows:
1. Let S be the empty set;
2. Sort R in decreasing order of vector ﬁtness, and let x∗be the top one solution
in R;
3. If |R| > 2p, truncate the length of R to 2p, i.e., maintain the top 2p locations
in R;
4. Randomly select p locations from R and add them to S, where the selection
probability of each x ∈R is f (x)/ 
z∈R f (z);
5. Apply the DE mutation, crossover, and selection operators to each solution in S;
6. Let x∗
1 be the best solution in R; if f (x∗
1) < f (x∗), randomly select a solution
x ∈S and replace it with x∗.
Generally speaking, the above DE procedure helps to improve the algorithm in
the following aspects:
• For high quality (top 2p) vectors in R, each of them has an opportunity to inﬂuence
the new ﬁreworks for the next generation, in terms of the roulette wheel selection
and DE selection operations.
• For candidate ﬁreworks in S, each of them has an opportunity to be informed by
existing high-quality vectors at each dimension, in terms of the DE mutation and
crossover operations.
• In particular, the DE mutation operator makes the difference of two random vectors
acts as a search direction for the third one [6]; in comparison with large-amplitude
explosion and distance-based selection used in the FWA, the mutation operation
is more effective in improving the probability of obtaining the global optimum,
whilst requiring less computational cost.
Since the DE operators are introduced into the algorithm for improving the diver-
sity of solutions, the values of control parameters including m, ˆA, smax, and smin can
be decreased, which will compensate the computational cost of DE operations to a
certain extent. According to our analysis, in most conditions, the speciﬁc (“bad”)
spark generation procedure can also contribute to the result solution quality of the
hybrid algorithm, but its contribution is much less than that in the standard FA. Thus,

156
10
Hybrid Fireworks Algorithms
the speciﬁc procedure can either be remained or discarded in the hybrid algorithm;
if it is remained, the parameter values mentioned above can be further decreased to
save computational cost.
10.3 CFWA
The problem of the digital ﬁlter design is a multiparameter optimization problem.
This section presents a joint objective function to design ﬁnite impulse response
(FIR) digital ﬁlters and inﬁnite impulse response (IIR) digital ﬁlters, and a cultural
ﬁrework algorithm is proposed to implement ﬁlter designs. The design of the ﬁlter
is transformed into the constrained optimization problem, and the cultural ﬁrework
algorithm is used to search optimal value of ﬁlter design parameters in the parameter
space with parallel search. The cultural ﬁrework algorithm (CFWA) is a multidi-
mensional search algorithm for optimization of real numbers, using mechanisms of
cultural evolution to update the locations of cultural sparks [7].
10.3.1 Design of Digital Filter
The goal of the ﬁtness function is to evaluate the status of each cultural ﬁrework. In
FIR and IIR digital ﬁlter design based on CFWA, the optimization goal of ﬁrework
location is the minimization of the following objective function:
f(x) =

αEF + βEI, x ∈s.t
δ [αEF + βEI], x /∈s.t ,
(10.4)
where α and β are two parameters, satisfying α + β = 1, α ∈{0, 1}, β ∈{0, 1}.
EF and EI are the objective of the total squared error in frequency domain of FIR
and IIR digital ﬁlter, s.t. denotes the constraint condition of vector x, δ is positive
constant which is limited to δ > 1.
What is following is the procedure to implement CFWA.
10.3.2 CFWA Implementation
The procedure for implementing CFWA is given in Algorithm 10.2.
Brieﬂy, the CFWA is proposed for designing FIR and IIR ﬁlters and can converge
around the optima quickly, which demonstrated FWA was successfully applied to
the real-world applications like design of digital ﬁlters.

10.3 CFWA
157
Algorithm 10.2 The process of CFWA
1: According to design requirement, select values of α, β and δ, where δ is equal to 0 for a non-
constraint problem.
2: Randomly select an initial population of the q candidate solutions within the given domains, and
initialize belief space.
3: Evaluate the performance scores of population space by a given objective function.
4: Select the p initial locations from the q locations.
5: Set off cultural ﬁreworks at the p locations.
6: Calculate objective function of the new locations.
7: According to the acceptance function to select excellent locations, and update the belief space.
8: Select the top q different locations from the 2q cultural sparks which include the cultural sparks
or ﬁreworks of the current and previous generation for the next generation (iteration).
9: If it has not met the termination condition (the termination condition is set as maximum iteration
times in general), then back to step 4; else the algorithm stops.
10.4 BBO_FWA
The key idea here is to introduce the migration operator of BBO to FWA, so as
to enhance information sharing among the population, and thus improve solution
diversity and avoid premature convergence. A migration probability is designed
to integrate the migration of BBO and the Gaussian mutation operator of FWA,
which cannot only reduce the computational burden, but also achieve a better bal-
ance between solution diversiﬁcation and intensiﬁcation. The Gaussian explosion of
the enhanced FWA (EFWA) is reserved to keep the high exploration ability of the
algorithm.
10.4.1 Biogeography-Based Optimization (BBO)
Borrowing ideas from biogeographic evolution over space and time, BBO [8] is
another population-based heuristic for optimization problems. In BBO, each solution
in the population is analogous to “habitats” or “islands,” the solution components
are analogous to a set of suitability index variables (SIVs), and the ﬁtness of the
solution is analogous to the species richness or habitat suitability index (HSI) of the
island. The method mainly works on the principle of immigration and emigration
of the species from one island to another, and therefore evolves the islands to ﬁnd
better solutions to the problem. BBO has proven itself a competitive method to other
well-known heuristics on a wide set of problems [8–12]).
A distinct feature of BBO is its migration operator, which indicates that high HSI
islands have a high species emigration rate μ and low HSI islands have a high species
immigration rate λ. The migration rates are functions of the HSI value or ﬁtness of
the islands. λi and μi of each island Xi are calculated as follows (but there are also
other nonlinear migration models can be used in [8, 13]):

158
10
Hybrid Fireworks Algorithms
λi = I

fi −f min
f max −f min

,
(10.5)
μi = E

f max −fi
f max −f min

,
(10.6)
where I and E are the maximum possible immigration rate and emigration rate,
respectively, which are typically both set to 1.
At each time, the migration operator migrates a SIV from an emigrating island to
an immigrating island, which are probabilistically selected according to the emigra-
tion and immigration rates of the islands. Algorithm 10.3 shows the basic procedure
of a probably migration operation on an island Xi.
Algorithm 10.3 The migration operation in BBO
1 for k = 1 to D do
2
if rand() < λi then
3
Select an emigrating island X j with probability ∝μ j;
4
Xi,k ←X j,k;
10.4.2 A Hybrid Biogeography-Based Optimization
and Fireworks Algorithm (BBO_FWA)
For a high-dimensional optimization problem, the ﬁtness value of a solution is code-
termined by its component values of all dimensions. A solution that has discovered
the region corresponding to the global optimum in some dimensions may have a
low ﬁtness value because of the poor quality in the other dimensions. Thus, some
well-known population-based evolutionary algorithms, including differential evolu-
tion (DE) [2], comprehensive learning PSO [4], fully informed PSO [5], enable the
individuals to make the utmost use of the beneﬁcial information in the population
and thus perform a very effective search.
In the original FWA, the individuals in the population never directly interacts with
each other. EFWA makes a slight improvement on Gaussian mutation operator to let
some individuals learn from the best individual found so far. On the other hand, FWA
used a distance-based metric for selecting individuals in less crowded regions to the
next generation so as to keep diversity. But such a selection operator is computational
expensive, and thus EFWA turns to a random selection operator.
In the hybrid algorithm, we employ a diversiﬁcation strategy that integrates the
BBO’s migration mechanism to FWA. In fact, the migration operator of BBO and the
Gaussian mutation operator of FWA both have their advantages and disadvantages:

10.4 BBO_FWA
159
• The migration operator contributes greatly to the information sharing between
different individuals by making low HSI islands probably learning from high HSI
ones. It is also computational cheap (which requires only one function evaluation
at each time, while the explosion requires si evaluations).
• The Gaussian mutation operator provides a good balance between exploration
and exploitation. In particular, when a high-quality ﬁrework is nearby the global
optimum, the explosion enables an intensive local search around the optimum.
To combine their advantages while reducing their disadvantages as much as possi-
ble, we introduce a migration probability, denoted by ρ, to the hybrid algorithm. Each
ﬁrework Xi has a probability of ρ to apply the migration operator, and a probability
of (1 −ρ) to explode.
Sincethemigrationoperator helps toenhancetheinformationsharingandincrease
the solution diversity, and the Gaussian explosion also utilizes the information of the
global best, here we do not use the elitism method that always put the best-known
individual to the new population. Algorithm 10.4 presents an overview of the hybrid
BBO_FWA.
Algorithm 10.4 The hybrid BBO_FWA
1 Randomly initialize a population P of n ﬁreworks;
2 while (stop criterion is not met) do
3
for each ﬁrework Xi ∈P do
4
if rand(0, 1) < ρ then
5
use Algorithm 10.3 to perform migration on Xi;
6
else
7
use Lines 5-12 of Algorithm 1 to produce sparks;
8
for j = 1 to Mg do
9
select a random ﬁrework Xi;
10
use Lines 15-18 of Algorithm 1 to produce a spark;
11
add the new individuals to P;
12
randomly select n individuals for P;
13
update Amin
k
and the migration rates;
14 return the best individual found so far.
In general, for problems with complex objective functions, we prefer to set a
larger value of the probability ρ which allows a smaller number of function evalua-
tions to reduce the computational burden. A larger ρ can also enhance the solution
diversity and thus improve the exploration ability for multimodal functions. In con-
trast, a smaller ρ is more suitable for those functions whose optima are often located
in very narrow or sharp ridges, since the Gaussian mutation operator can diverse
the search along different directions and thus decrease the chance of skipping the
optima. Empirically, the value of ρ can range from 0.5 to 0.8 to achieve an obvious
performance improvement over both BBO and FWA/EFWA.

160
10
Hybrid Fireworks Algorithms
10.4.3 Discussions
FWA is a metaheuristic method inspired by the phenomenon of ﬁreworks explosion,
and has received much interest in recent years. FWA has drawbacks of high com-
putational cost and lacking of information sharing among the population. A hybrid
algorithm BBO_FWA is proposed here, which integrates the migration operator of
BBO with the explosion operator of FWA based on a migration probability, and thus
effectively increases the solution diversity without harming the exploitation ability
of FWA.
BBO_FWA uses a ﬁxed migration probability p which is easy to implement.
However, as indicated by the numerical experiments, the parameter value needs to
be ﬁne-tuned to obtain the best results on different problems. Experimental results
on selected benchmark functions show that the hybrid BBO_FWA has a signiﬁcant
performance improvement in comparison with both BBO and EFWA.
10.5 Summary
As a novel swarm intelligence algorithm, FWA can hybridize with other SI algo-
rithms to produce high effective hybrid algorithms. In this chapter, FWA-DM is
ﬁrst introduced and the experimental results are given in detail. Second, FWA-DE
is stated as FWA and DE hybrid and generated an high effective hybrid algorithm,
where DE algorithm is used to improve the selection strategy in FWA. Third, CFWA
is introduced as the locations and ﬁtness values of ﬁreworks are kept as knowledge in
a library. The individuals can learn from the library and improve themselves. At last,
BBO_FWA is presented. The immigration and emigration of biogeography-based
optimization is introduced to improve the performance of FWA. Hence, all in one,
FWA is very much suitable for hybridization with other algorithms and produces a
lot of effective hybrid algorithms.
References
1. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
2. R. Storn, K. Price, Differential evolution-a simple and efﬁcient heuristic for global optimization
over continuous spaces. J. Glob. Optim. 11(4), 341–359 (1997)
3. C. Yu, J. Li, Y. Tan, Improve enhanced ﬁreworks algorithm with differential mutation. In 2014
Conference on IEEE System, Man, and Cybernetics (SMC). IEEE (2014)
4. J.J. Liang, A.K. Qin, P.N. Suganthan, S. Baskar, Comprehensive learning particle swarm opti-
mizer for global optimization of multimodal functions. IEEE Trans. Evol. Comput. 10(3),
281–295 (2006)
5. R. Mendes, J. Kennedy, J. Neves, The fully informed particle swarm: simpler, maybe better.
IEEE Trans. Evol. Comput. 8(3), 204–210 (2004)

References
161
6. Y.-C. Lin, K.-S. Hwang, F.-S. Wang, Co-evolutionary hybrid differential evolution for mixed-
integer optimization problems. Eng. Optim. 33(6), 663–682 (2001)
7. H. Gao, M. Diao, Cultural ﬁrework algorithm and its application for digital ﬁlters design.
Intern. J. Model., Identif. Control 14(4), 324–331 (2011)
8. D. Simon, Biogeography-based optimization. IEEE Trans. Evol. Comput. 12(6), 702–713
(2008)
9. A.Bhattacharya,P.K.Chattopadhyay,Biogeography-basedoptimizationfordifferenteconomic
load dispatch problems. IEEE Trans. Power Syst. 25(2), 1064–1077 (2010)
10. U. Singh, H. Kumar, T.S. Kamal, Design of Yagi-Uda antenna using bio-geography based
optimization. IEEE Trans. Antennas Propag. 58(10), 3375–3379 (2010)
11. I. Boussad, A. Chatterjee, P. Siarry, M. Ahmed-Nacer, Biogeography-based optimization for
constrained optimization problems. Comput. Oper. Res. 39(12), 3293–3304 (2012)
12. Y.-J. Zheng, H.-F. Ling, H.-H. Shi, H.-S. Chen, S.-Y. Chen, Emergency railway wagon schedul-
ing by hybrid biogeography-based optimization. Comput. Oper. Res. 43, 1–8 (2014)
13. H. Ma, D. Simon, Blended biogeography-based optimization for constrained optimization.
Eng. Appl. Artif. Intell. 24(3), 517–525 (2011)

Part III
Advanced Topics
FWA was ﬁrst proposed for single-objective optimization problems and has been
widely studied based on the CPU platform. After years of study, FWA has extended
far beyond this conventional domain. In this part, we present some advanced topics
on FWA.
Multi-objective optimization problems are universal and much more compli-
cated than their single-objective counterparts. Chapter 11 will introduce the
applications of FWA on multi-objective problems. In Chapter 12, with the help of
the well-known S-metric, a kind of hyper-volume indicator, an S-metric
multi-objective ﬁreworks algorithm (MOFWA) is proposed for efﬁciently solving
multi-objective optimization problems. Combinatorial optimization problems are of
great importance in the real world and many combinatorial problems are proved to
be NP hard and thus difﬁcult to be solved exactly. After some modiﬁcations, FWA
can be applied to combinatorial optimization problems. Chapter 13 comes up with a
discrete FWA for tackling Traveling Salesman Problem (TSP).
GPU is a game-changing force in the domain of High Performance Computing
(HPC). Thanks to GPU’s parallelism and great computational power, swarm
intelligence algorithms are able to fully exploit their inherent parallelism. Building
swarm intelligence algorithms on GPU platform is an increasingly important and
popular research topic. Chapter 14 describes a GPU-based FWA. As will see,
GPU-based FWA can achieve great speedup compared to CPU-based implemen-
tation. The enormous acceleration implies that FWA is capable of applying to
problems of greater scale in more different domains.

Chapter 11
FWA for Multiobjective Optimization
This chapter is to present some research works of FWA for multiobjective
optimization, of which this is a successful instance like the multiobjective ﬁreworks
algorithm (MOFWA) proposed by Zheng et al. in [1] for oil crop fertilization, which
takes into consideration not only crop yield and quality but also energy consumption
and environmental effects. The variable-rate fertilization (VRF) is a key aspect of
prescription generation in precision agriculture, which typically involves multiple
criteria and objectives. To solve the problem efﬁciently, a hybrid multiobjective ﬁre-
works optimization algorithm (MOFWA) is proposed to evolve a set of solutions
to the Pareto optimal front by mimicking the explosion of ﬁreworks. Especially,
MOFWA uses the concept of Pareto dominance for individual evaluation and selec-
tion, and combines differential evolution (DE) operators to increase information
sharing among the individuals. The proposed MOFWA outperforms some state-of-
the-art methods on a set of real-world VRF problems.
11.1 Introduction
Variable-rate fertilization (VRF) decision problem is concerned with specifying the
dosage of each type of fertilizer for the crop(s) in each ﬁeld. Traditional VRF prob-
lems are typically described as a single-objective model maximizing crop yield or
the rate of yield to cost. Nevertheless, different VRF solutions may cause very dif-
ferent impacts not only on crop yield, but also on crop quality, soil quality as well
as other aspects of the environment. In modern precision agriculture systems, VRF
decision should take both economical and ecological effects into consideration and
thus should be modeled as an optimization problem with multiple objectives and
constraints.
In a multiobjective optimization problem (MOP), there are often conﬂicts among
various objectives and it is impossible to ﬁnd a single solution that is optimal in terms
of all objectives. Therefore, it is preferable to search for a set of Pareto optimal or
non-dominated solutions, i.e., there is no solution that is better than another solution
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_11
165

166
11
FWA for Multiobjective Optimization
in all objectives. Multiobjective models are ﬁrst introduced to irrigation decision
in agricultural practices. Raju and Kumar [2] studied the irrigation problem with
three conﬂicting objectives including net beneﬁts, agricultural production, and labor
employment. They employed a constraint method that successively optimizes each
individual objective, while all the others are constrained. Chen et al. [3] presented
a fuzzy irrigation problem model which divides the period of growth into multiple
stages and considers two objectives including the crop yield and the risk of shrivel.
They used a weighted function to combine the two objectives, and applied a multi-
dimensional dynamic programming approach to solve the problem stage by stage.
However, methods like the constraint and the weighted sum transform the problem to
a single-objective one and do not simultaneously evolve all the objectives. Kilic and
Anac [4] considered a problem of increasing the beneﬁt from production, increasing
the total area irrigated and reducing the water loss, and used a mathematical pro-
gramming method to solve it, but the three objectives have a close relationship and
generally do not conﬂict each other.
In recent years, multiobjective models and algorithms have been introduced for
fertilization decision problems. Wang and Zhang [5] presented a model of multi-
objective rice fertilization,and employed a linear approximation iteration method
to solve the problem. However, such programming methods are only efﬁcient for
small-sized problems, which have motivated many researchers to use more effec-
tive heuristic algorithms. When solving a generalized fertilization problem, Yuan
et al. [6] applied the genetic algorithm (GA) and introduced a reduction factor into
the ﬁtness function to improve convergence efﬁciency, but they did not employ the
multiobjective evolution strategy in the algorithm. Some researchers (e.g., [7]) also
used neural network approaches for fertilization decisions, in which fertilizer vari-
ables are used as network inputs fertilizer and objective values are used as network
outputs; nevertheless, the topology design and optimal weights search remain a main
difﬁculty in algorithm implementation.
Multiobjective evolutionary algorithms (MOEA) have been considered as one
of the most effective approaches to MOP, mainly because they deal simultane-
ously with a set of possible solutions (the so-called population) which allows to
ﬁnd several members of the Pareto optimal set in a single run of the algorithm,
and their applicabilities are less susceptible to the shape or continuity of the Pareto
front [8]. A variety of Pareto-based MOEA have been proposed in the last decade,
including the non-dominated sorting genetic algorithm (NSGA) [9] and NSGA-
II [10], the strength Pareto evolutionary algorithm (SPEA) [11] and SPEA2 [8],
the Pareto archived evolution strategy (PAES) [12], the Pareto differential evo-
lution algorithm (PDE) [13], the non-dominated sorting particle swarm optimizer
(NSPSO) [14], the constrained nonlinear multiobjective optimization immune algo-
rithm (CNMOIA) [15], the coevolutionary particle swarm optimization algorithm
(CCPSO) [16], the dominating tree-based multiobjective evolutionary algorithm
(DTEA) [17], the adaptive multiobjective particle swarm optimization algorithm
(MO-TRIBES) [18], the hybrid MOEA with two crossover operators [19], the

11.1 Introduction
167
multiobjective endocrine particle swarm optimization algorithm (MOEPSO) [20],
etc. These methods have attracted much attention among researchers and shown
promising results for many practical problems [21, 22].
For example, Niknam et al. [23] applied an MOEA for volt/var control in distri-
bution networks, which simultaneously and effectively minimize electrical energy
losses, voltage deviations, total electrical energy costs, and total emissions of renew-
able energy sources and grid. In [24] Ahmadi et al. used an MOEA for optimal design
of a poly generation energy system, which considers minimizing total cost rate of the
system while maximizing the system energy efﬁciency. Studies on the application of
MOEA in the ﬁeld of sustainable and renewable energy can also be found in [24–28].
However, the studies of MOEA in fertilization decision problems are very few. The
only report we found was that by Reddy and Kumar [29], where an MODE approach
was proposed for the simultaneous evolution of optimal cropping pattern and oper-
ation policies for a multi-crop irrigation reservoir system, and the result provides a
wide spectra of Pareto optimal solutions and gives sufﬁcient ﬂexibility to select the
best irrigation planning and reservoir operation strategy.
Fireworks optimization algorithm (FWA) [30] is a relatively new heuristic method
inspired by the phenomenon of ﬁreworks explosion. The algorithm selects a certain
number of locations in the search space, each for exploding a ﬁrework to generate a set
of sparks (new solutions); the ﬁreworks and sparks with good performance (ﬁtness)
are chosen as the locations for the next generations ﬁreworks, and the evolutionary
process continues until a desired result is obtained. Numerical experiments on a
number of benchmark functions show that FWA can converge to a global optimum
with a much smaller number of function evaluations than that of particle swarm
optimizers [31, 32]. Recently, Zheng et al. [33] develop an improved version of FWA
by introducing differential evolution (DE) operators [34] to improve the population
diversity. However, to our best knowledge, there is still no research on the application
of ﬁreworks optimization in multiobjective problems.
The motivation for MOFWA derives from precision agriculture practices on oil
crop production in East China. The government and the agronomists try to guide the
farm managers to achieve balance not only between expected proﬁts and potential
risks but also between short-term proﬁts and long-term proﬁts. We establish multi-
objective optimization models for oil crop fertilization problems, and try to solve the
problems based on different heuristic methods, among which we ﬁnd that the mul-
tiobjective ﬁreworks optimization algorithm (MOFWA) has more advantages over
other methods on the efﬁciency and effectiveness. The proposed MOFWA employs
a problem-speciﬁc strategy for generating the initial population, uses the concept of
Pareto dominance for individual evaluation and selection, combines the DE opera-
tors including mutation, crossover, and selection to increase the information sharing
among the population, and thus signiﬁcantly decreases the computational cost and
improves the quality of result solution set.
Note that majority parts of the chapter are excerpted from Ref.[1] on behalf of
Prof. Yujun Zheng.

168
11
FWA for Multiobjective Optimization
Table 11.1 The parameters
representation
m
Number of ﬁelds
n
Number of types of fertilizer
ai
Average gradient of ﬁeld i
di
Average plant density of ﬁeld i
p j
Unit price of fertilizer j
xij
Dosage of fertilizer j in ﬁeld i
x0
ij
Inherent quantity of fertilizer j in ﬁeld i
yij
Residual of fertilizer j in ﬁeld i
Y
Function for estimating the crop yield
Q
Function for evaluating the crop quality
C
Function for estimating the cost of fertilization
E
Function for estimating the energy consumption of
fertilization
R
Function for estimating the residual fertilizer
11.2 Problem Model
Different VRF decisions signiﬁcantly affect the production of oil crops and the
ecological conditions of environment. Besides the yield of crops and the cost of
fertilizers, here we are also concerned with overall crop quality, energy consumption,
and residual fertility.
The problem is to determine a dosage matrix x = (xij)m×n, i.e., the dosage of each
fertilizer in each ﬁeld. For a given ﬁeld i, the expected crop yield can be estimated by
a fertilizing effect function Yi(x). There are a number of models for describing the
relationship between yield and fertilizer, among which the most widely used form
of Yi is a quadratic function as follows [35]:
Yi(X) =
n

j=1
n

k=1
ai jk ˆxij ˆxik +
n

j=1
bij ˆxij + ci,
(11.1)
where ai jk is the quadratic regression coefﬁcient, bij is the simple regression coefﬁ-
cient, ci is the constant coefﬁcient, and ˆxij = xij + x0
ij, i.e., the sum of the dosage of
fertilizer xij and the inherent fertility x0
ij in the ﬁeld (Table11.1).1
A crop usually has a number of quality criteria, e.g., protein content, oil content,
crop density, etc. In many cases, a majority of such criteria are not contradictory
to each other, and thus we can establish a fertilizing effect function to evaluate the
1For the ﬁelds with the same or similar soil conditions, their equations can have the same coefﬁcients.
This is also the case for some coefﬁcients in the following computational equations.

11.2 Problem Model
169
comprehensive quality index. Such a function can also be modeled by a quadratic
regression equation as follows:
Qi(x) =
n

j=1
n

k=1
a′
i jk ˆxij ˆxik +
n

j=1
b′
ij ˆxij + c′
i,
(11.2)
where a′
i jk, b′
ij and c′
i are regression coefﬁcients.
In case there are indeed conﬂicts between some quality criteria, then we can
establish a set of quality subindices, e.g., Q′Q′′, etc., and construct a computational
model for each of them. Given unit price pj for each fertilizer j, the total fertilizer
cost can be computed as:
C(x) =
m

i=1
n

j=1
p j xij.
(11.3)
And we use the following empirical formula to estimate the energy consumption
of crop fertilization based on the gradient αi and plant density di of each ﬁeld i
(where λ is a constant coefﬁcient):
Ei(x) = λ
n

j=1
αid1/3
i
xij.
(11.4)
For agricultural soil, the residual fertility is an important criterion to evaluate the
soil quality. A simple formula to estimate the residual of fertilizer j in ﬁeld i is as
follows [36]:
yij = μxij + v j x0
ij
(11.5)
where μj and vj are coefﬁcients relevant to the characteristics of the fertilizer.2 There-
fore, the homogeneity of residual fertilizer j in the planting area can be computed as:
R j(x) =
m

i=1
(yij −y)2
(11.6)
where y = (m
i=1 yij)/m. Based on above analysis, we model the multiobjective
optimization VRF problem for oil crop production as follows:
max Q(x) =
m

i=1
Yi(x)Qi(x)
(11.7)
2If the soil conditions vary greatly from the different ﬁelds, we may need to deﬁne the coefﬁcients
μij and νij for not only different kinds of fertilizer but also different ﬁelds. However, the goal is
to improve the homogeneity of fertilizer content among the ﬁelds with the same or similar soil
conditions.

170
11
FWA for Multiobjective Optimization
minC(x) =
m

i=1
n

j=1
p j xij
(11.8)
minE(x) =
m

i=1
Ei(x)
(11.9)
s.t.
m

i=1
Yi(x) ≥YL
(11.10)
m

i=1
n

j=1
p j xij ≤C∪
(11.11)
n

j=1
R j(x) ≤R∪
(11.12)
xij ≥0, ∀i and j
(11.13)
where Y L is the lower limit of the total crop yield, C∪is the upper limit of total fertil-
izercost,and R∪istheupperlimitofthefertilizerresidualhomogeneityindex.Ingen-
eral, (11.7) to (11.13) constitute a nonlinear multiobjective programming problem.
11.3 MOFWA for VFR Problem
11.3.1 Fitness Assignment Strategy
There are a variety of ﬁtness assignment strategies for MOEA, and in the MOFWA
we employ a strategy based on the Pareto strength used in SPEA2 [8]. That is, for an
individual solution xi in the population P or in the non-dominated solution archive
NP, a strength value s(xi) is calculated according to the number of other individuals
it dominates:
s(xi) = |x j ∈P ∪NP|xi ≻x j|
(11.14)
where ≻denotes the Pareto dominance relation. And the raw ﬁtness value of xi is
determined by the strengths of its dominators:
r (xi) =

(x j∈P∪N P)(x j≻xi)
s

x j

(11.15)

11.3 MOFWA for VFR Problem
171
It should be noted that the ﬁtness is to be minimized here. A density value of xi is
then calculated by (11.16) and incorporated into the ﬁtness by (11.16):
d(xi) =
1
δk(xi)
(11.16)
f (xi) = r(xi) + d(xi)
(11.17)
where k(xi) is the distance from xi to its kth nearest individual, and k is typically set
to equal to the square root of sample size P ∪N [37].
In addition, if a solution violates the problem constraints (11.10) to (11.12), we
compute the degrees of constraint violations, respectively.
uY (xi) =
⎧
⎨
⎩
Y L −
m
i=1
Yi (xi) if
m
i=1
Yi (xi) ≤Y L,
0
else.
(11.18)
uc(xi) =
	
C (xi) −CU
if C (xi) ≥CU,
0
else.
(11.19)
uc(xi) =
⎧
⎪⎨
⎪⎩
n
j=1
R j (xi) −RU
if
n
j=1
R j (xi) ≥RU,
0
else
(11.20)
and multiply the ﬁtness value by a penalty coefﬁcient p(xi) as follows.
p(xi) = w1uY (xi) + w2uC(xi) + w3uR(xi)
(11.21)
where w1, w2 and w3 are three predeﬁned weights.
11.3.2 Evolutionary Strategies
As most MOEA, MOFWA maintain two collections of solutions: a population P
and a non-dominated solution archive NP, and at each algorithm iteration selects
non-dominated solutions from P to update NP. However, in MOFWA such update
is done twice at each iteration. In detail, for each generation of population P, we
explode its individuals (ﬁreworks) to generate a number of sparks according to the
method described in Sect.3.1, compute their ﬁtness values, and then select those
non-dominated sparks to update NP.

172
11
FWA for Multiobjective Optimization
Afterward, we randomly select p solutions from all the ﬁreworks and sparks,
where the selection probability of a solution is proportional to its ﬁtness value. For
each selected solution xi, the DE operators [34] are applied as follows:
1. (Mutation) generate a mutant solution vi by adding the weighted difference
between two randomly selected solutions to a third one:
vi = xr1 + γ(xr2 −xr3)
(11.22)
where random indexes r1, r1, r1, ∈{1, 2, . . . , p} and coefﬁcient γ > 0.
2. (Crossover) generate a trial solution ui by mixing the components of the mutant
solution and the original one, where each jth component of ui is determined as
follows:
u j
i =
	
v j
i
if rand(0, 1) < Cr or j = r(i),
x j
i
else
(11.23)
where Cr is the crossover probability ranged in (0, 1) and r(i) is a random integer
within (0, N] for each i.
3. (Selection) choose the better one for the next generation by comparing the trial
solution with the original one:
xi =
	
ui
if f (ui) ≤f (xi),
xi
else
(11.24)
At the last step, if the trial solution ui is selected, we test whether it is a non-
dominated solution in current generation and, if so, use it to update NP.
11.3.3 The MOFWA Framework
Typically, the algorithm termination condition can be a maximum number of
iterations(generations)oramaximumCPUtime.Besides,wecanalsosetamaximum
number of non-improvement iterations, i.e., the algorithm stops if it cannot ﬁnd a
new non-dominated solution after certain iterations.
The basic algorithm ﬂow is shown in Fig.11.1. Next we will discuss some imple-
mentation details on initial population generation and non-dominated archive main-
tenance.

11.3 MOFWA for VFR Problem
173
Fig. 11.1 The basic ﬂowchart of the MOFWA

174
11
FWA for Multiobjective Optimization
Algorithm 11.1 The proposed MOFWA.
1: Initialization
2:
Randomly generate a population P of p feasible solutions.
3:
Create the empty non-dominated solution archive NP, and select those non-dominated solu-
tions from P to update NP.
4: Iterative improvement
5:
For each individual xi in P do:
6:
Calculate si for xi according to Eq.(2.1).
7:
Calculate Ai for xi according to Eq.(2.3).
8:
Generate si sparks of xi.
9:
Generate a speciﬁc spark of xi.
10:
Compute ﬁtness for all sparks according to Eqs.(11.14) to (11.21).
11:
Update NP based on the new solutions (sparks).
12:
Select p solutions from the ﬁreworks and sparks, where the selection probability of each
solution xi is f (xi) / 
j∈P
f

x j

13:
For i = 1 to p do:
14:
Apply the mutation, crossover, and selection operators to xi according to Eqs.(11.22),
(11.24) and get a trial solution ui.
15:
If the DE result indicates that xi is to be replaced by ui, then use ui to update NP.
16:
Update P by including the best solution and other p1 ones randomly selected according to
distance-based probability.
17:
If the termination condition is satisﬁed, then the algorithm stops; else go to step (2.1).
11.3.4 Initial Population Generation
For many MOEA, it may be difﬁcult to obtain a certain number of initial feasible
solutions when the problem is heavily constrained [38]. Here we tackle this issue
based on some speciﬁc characteristics of the VFR problem. That is, the step (1.1) of
the MOFWA can be further broken into the following steps:
1.1.1 For i = 1 to m, work out the partial solution x∗
i such that Yi(x∗
i ) is maximized,
and thus obtain a fertilizing matrix (x∗
ij)m×n.
1.1.2 Let △ij = (x∗
ij)/q, generate a set of q solutions (△ij)m×n, (2△ij)m×n,…,
(q△ij)m×n, and then randomly select p feasible solutions from them as the
initial population P.
Step (1.1.1) results a (typically infeasible) solution x∗maximizing the crop yield,
which can be worked out by Gauss Newton method based on Eq.(11.1). Next we
generate q solutions uniformly distributed between 0 and x∗, and randomly select p
solutions satisfying constraints (11.10) to (11.12) to compose the initial population.
According to our experimental data, when q is about 45 times the population size p,
the number of feasible solutions is larger than p in most cases.
Our idea is based on the fact that, for the solution x* maximizing the crop yield,
the proportions of fertilizers are usually balanced or reasonable. Therefore, solutions
with such fertilizer proportions are more likely to be effective, and starting from such
a population canal so improve (but not critically affect) the algorithm performance.

11.3 MOFWA for VFR Problem
175
11.3.5 Non-dominated Archive Maintenance
According to our experiments, the size of archive NP may increase rapidly during the
search process, and thus it is reasonable to limit the size of NP, especially for large
problem instances. In general, when NP reaches the size limit, a new non-dominated
solution can be inserted into NP and an archive one will be removed only if the
diversity of NP can be potentially improved by doing that, because the preservation
and improvement of diversity of the population is crucial not only to avoid losing
potentially efﬁcient solutions but also avoid premature convergence [39]. In MOFWA
we use the minimum pairwise distance metrics [40] which is of low computational
cost. During the search process, whenever NP reaches the size limit, we mark two
solutions xa, xb ∈N P, the Euclidean distance between which is the minimum
among all pairs in NP, i.e.,
dis(xa, xb) = minx,x∈N P,∧x̸=x′dis(x, x′).
(11.25)
And the following procedure is applied for possible inclusion of a new solution
x if |N P| = |N P|U:
1. If x is dominated by any xi ∈N P, then x is discarded.
2. Else if x dominates some xi ∈N P, then remove those x′ and insert x.
3. Else if dis(xa, xb) < minx,x∈N P,∧x̸=xadis(x, x′), then remove xa and insert x.
4. Else if dis(xa, xb) < minx,x∈N P,∧x̸=xbdis(x, x′), then remove xb and insert x.
5. Else choose a closest x1 ∈N P to x; if minx,x∈N P,∧x′̸=x1dis(x, x′) < dis(x, x1),
then remove x1and insert x.
6. Else discard x.
11.4 Computational Experiments
In this section, we assess the performance of the proposed MOFWA on a set of VRF
problems for three kinds of oil crops including Brassic anapus (BN or rapeseed),
Canarium album (CA or olive), and Camellia oleifera (CO). The BN VRF problem
considers two kinds of fertilizers, i.e., N and P, and the CA and CO VRF problem
considers three kinds of fertilizers, i.e., N, P, and K. We conduct extensive experi-
ments by comparing with competitive algorithms on the rest two kinds of crops, and
then apply the MOFWA to a real-world CO VRF problem in East China.

176
11
FWA for Multiobjective Optimization
11.4.1 Comparative Experiments on BN and CA VRF
Problems
We test the MOFWA on the BN and CA VRF problems in comparison with four other
MOEA: NSGA-II [10], PDE [13], DE-MOC [41], and NSPSO [14]. The mathemat-
ical models for crop yield and/or quality estimation are based on the results from
[42, 43]. The experiments are conducted on a computer of 4× Intel Xeon X3430
2.4GHz processors and 4 × 2 GB memory. For all the algorithms, the upper limit
of the archive size is set to 20, and the maximum number of generations is set to
100 n√m. Moreover, if the algorithm cannot ﬁnd a new non-dominated solution
after 300 continuous generations, it is also stopped. The population size is set to 30
for the MOFWA, 200 for the NSGA-II, and 100 for PDE, DE-MOC, and NSPSO.
The penalty coefﬁcient computed by Eq.(11.27) applies to all the competitive algo-
rithms. For the MOFWA, the DE-related parameters are set as those in [34], and other
parameter settings are given in Table11.2, which are suggested by our preliminary
empirical data. For each crop, we generate 10 problem instances with swarm size
m ranges from 20 to 800. On each instance, each algorithm is run 30 times with
different random seeds. The performance metrics include:
1. The CPU time t(s)
2. The hypervolume HV [44] of the result solution set.
3. The coverage c [44], which is a comparative metric on the result sets obtained
by different algorithms. Let S1 and S2 be two solution sets, then the coverage
c(S1, S2) is deﬁned as the fraction of solutions in S2 that are (strictly) dominated
by a solution in S1:
c(S1, S2) = ||x2 ∈S2|∃x1 ∈S1 : x1 ≻x2||
|S2|
.
(11.26)
In the comparative experiments, we calculate the coverage of the result set of
MOFWA over that of the four competitive algorithms, denoted by c1, c2, c3, and c4
respectively, and the coverage of the result sets of the four competitive algorithms
over that of MOFWA, denoted by c′.
TheexperimentalresultsoftheBNandCAproblemsaresummarized,respectively
in Tables11.3 and 11.4, and the variations of CPU time with the problem size are
illustrated, respectively in Figs.11.2 and 11.3. As we can see, for relatively small-
sized problems (mn ≤200), all the algorithms achieve nearly the same Pareto
front, and MOFWA consumes a little more CPU time than NSGA-II, PDE, and
Table 11.2 Parameter values of our algorithm for the test problems (xU B, xL B denote the upper
and lower search bounds)
Parameter
sm
smin
smax
ˆA
q
w1
w2
w3
Value
25
2
20
min1≤k≤D
xU B,k−xL B,k
7
5p
4.8
1.5
2.0

11.4 Computational Experiments
177
Table 11.3 Computational results on the BN VFR problem instances, where the values are averaged over 30 independent runs
m
NSGA-II
PDE
DE-MOC
NSPSO
MOFWA
t
HV
C′
t
HV
C′
t
HV
C′
t
HV
C′
t
HV
C1
C2
C3
C4
20
0.47
6.40E+0
0
0.33
6.40E+0
0
0.54
6.40E+0
0
0.7
6.40E+0
0
0.62
6.40E+0
0
0
0
0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
50
1.22
3.60E+1
0
1.1
3.60E+1
0
1.29
3.60E+1
0
1.77
3.60E+1
0
1.45
3.60E+1
0
0
0
0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
100
3.89
1.50E+3
0
3.14
1.30E+3
0
3.44
1.50E+3
0
5.67
1.50E+3
0
4.13
1.50E+3
0
0
0
0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
150
9.49
1.70E+4
0
7.33
1.20E+4
0
7.89
1.70E+4
0
13.3
1.90E+4
0
10.3
1.90E+4
0
0.13 0
0
−3.20E+3
−2.40E+3
−3.90E+3
0.00E+0
0.00E+0
200
17.7
6.60E+5
0
15.6
3.90E+5
0
17
5.50E+5
0
28.9
7.10E+5
0
20.4
7.70E+5
0.1
0.17 0.1
0.05
−1.90E+5
−8.80E+4
−1.70E+5
−2.00E+5
−1.70E+5
300
65.3
2.70E+7
0
52.8
2.00E+7
0
61.1
2.40E+7
0
98.3
3.50E+7
0
63.3
4.00E+7
0.15 0.21 0.17 0.1
−6.60E+6
(4.5+6)
−6.20E+6
−6.10E+6
−5.90E+6
400
158
8.20E+8
0
141
4.40E+8
0
150
6.60E+8
0
262
1.10E+9
0
149
2.20E+9
0.18 0.2
0.18 0.15
−1.90E+8
−9.20E+7
−1.60E+8
−2.20E+8
−3.10E+8
500
417
2.40E+9
0
394
9.80E+8
0
434
2.40E+9
0
631
2.80E+9
0.03
370
5.90E+9
0.2
0.33 0.24 0.13
−5.10E+8
−2.50E+8
−4.20E+8
−4.90E+8
−4.70E+8
600
1064
7.20E+10
0
855
4.80E+10
0
989
7.00E+10
0
1375
9.30E+10
0
869
1.70E+11
0.33 0.48 0.37 0.15
−1.70E+10
−8.50E+9
−1.50E+10
−1.90E+10
−2.10E+10
800
3107
4.50E+12
0
2737
3.20E+12
0
3225
4.90E+12
0
4619
7.00E+12
0.07
3021
9.60E+12
0.55 0.75 0.5
0.21
−9.90E+11
−5.60E+11
−1.00E+12
−1.30E+12
−9.80E+11
For HV the numbers in the brackets are standard deviations

178
11
FWA for Multiobjective Optimization
Table 11.4 Computational results on the CA VFR problem instances, where the values are averaged over 30 independent runs
m
NSGA-II
PDE
DE-MOC
NSPSO
MOFWA
t
HV
C′
t
HV
C′
t
HV
C′
t
HV
C′
t
HV
C1
C2
C3
C4
20
0.78
2.00E+1
0
0.52
2.00E+1
0
0.75
2.00E+1
0
1.01
2.00E+1
0
0.97
2.00E+1
0
0
0
0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
50
1.9
4.90E+2
0
1.65
4.90E+2
0
1.81
4.90E+2
0
2.44
4.90E+2
0
2.17
4.90E+2
0
0
0
0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
0.00E+0
100
5.47
1.90E+4
0
4.94
1.60E+4
0
5.3
2.00E+4
0
6.32
2.30E+4
0
5.7
2.30E+4
0.1
0.17 0.1
0
−3.60E+5
−3.30E+5
−4.00E+5
0.00E+0
0.00E+0
150
14.2
6.80E+5
0
12.7
4.30E+5
0
14.7
7.00E+5
0
16.9
9.70E+5
0
14.2
9.70E+5
0.1
0.2
0.1
0
−1.30E+5
−1.10E+5
−1.50E+5
0.00E+0
0.00E+0
200
30.4
3.00E+7
0
24.9
2.20E+7
0
33
3.20E+7
0
39.9
4.80E+7
0
29.1
5.70E+7
0.2
0.25 0.17 0.15
−4.90E+6
−3.80E+6
−5.40E+6
−5.90E+6
0.00E+0
300
111
3.40E+9
0
94.6
2.10E+9
0
121
3.80E+9
0
177
4.10E+9
0
104
5.10E+9
0.18 0.29 0.18 0.17
−5.20E+8
−3.90E+8
−4.90E+8
−3.90E+8
−3.10E+8
400
316
2.60E+11
0
269
1.80E+11
0
330
2.90E+11
0
420
3.10E+11
0.05
293
4.30E+11
0.26 0.38 0.2
0.17
−4.90E+10
−3.50E+10
−4.80E+10
−3.70E+10
−4.00E+10
500
904
8.80E+12
0
732
6.70E+12
0
959
9.60E+12
0
1330
1.40E+13
0.12
846
1.90E+13
0.5
0.57 0.33 0.17
−2.40E+12
−1.20E+12
−2.60E+12
−1.60E+12
−1.70E+12
600
2881
2.30E+15
0
2254
1.80E+15
0
3022
2.80E+15
0
3789
3.90E+15
0
2652
5.50E+15
0.45 0.83 0.4
0.25
−3.80E+14
−3.10E+14
−3.90E+14
−4.30E+14
−5.20E+14
800
5963
2.00E+18
0
4807
9.60E+17
0
6300
2.80E+18
0
6480
3.70E+18
0.09
5902
4.80E+18
0.48 0.92 0.4
0.22
−3.90E+17
−3.10E+17
−4.20E+17
−4.40E+17
−4.80E+17
For HV the numbers in the brackets are standard deviations

11.4 Computational Experiments
179
Fig. 11.2 CPU time (s) variation with problem size m on the BN VFR problem: a 300 ≤m ≤800,
b m ≤200
Fig. 11.3 CPU time (s) variation with problem size m on the CA VFR problem: a 300 ≤m ≤800,
b m ≤200
DE-MOC. However, with the increase of the problem size, MOFWA shows more
and more performance advantages over the other three algorithms. For the problem
instances with mn > 400, MOFWA always obtains higher quality solution set (in
terms of hypervolume and coverage) with less computational time, in comparison
with NSGA-II, DE-MOC, and NSPSO. PDE is faster than MOFWA, but the quality
of its result sets is far less than that of the MOFWA.
In particular, the coverage metrics show that the result solutions obtained by the
MOFWA are never dominated by those obtained by the NSGA-II, PDE, and DE-
MOC, given that the values of c′ are always zero in the corresponding table columns
of the three algorithms. Only on large-sized problems, NSPSO occasionally obtains
a few solutions that dominate some result solutions of the MOFWA, but the coverage
value is always smaller than C4, and it should be noted that NSPSO consumes much

180
11
FWA for Multiobjective Optimization
more computational cost than MOFWA. On the contrary, except for some small-
sized problem instances, there are always a part of result solutions of the competitive
algorithms dominated by those of the MOFWA, as indicated by the values of c1, c2,
c3, and c4. For very large problem instances (mn > 1000), those values are relatively
high, which means that the distance between the Pareto front and the result set of the
MOFWA is much closer than those of the other algorithms.
For those BN and CA problems where m ≥200, we also monitor the varia-
tions of HV values with the algorithm iterations, the results of which are presented
in Figs.11.4 and 11.5 respectively. From the convergence curves we can clearly
see that the performance of the MOFWA overwhelms the other three algorithms.
Roughly speaking, the curves of the NSGA-II and the PDE have similar shapes,
Fig. 11.4 Comparative results of the variation of HV with algorithm iterations on six BN VRF
problem instances. The HV values are averaged over 30 simulation runs. a m = 200, b m = 300,
c m = 400, d m = 500, e m = 600, f m = 800

11.4 Computational Experiments
181
Fig. 11.5 Comparative results of the variation of HV with algorithm iterations on six CA VRF
problem instances. The HV values are averaged over 30 simulation runs. a m = 200, b m = 300,
c m = 400, d m = 500, e m = 600, f m = 800
both are smooth and grow slowly toward the ﬁnal results. The PDE has the lowest
performance among the algorithms, which indicates that its search capability is very
limited on the test problems. The performance of the NSPSO is the best in the three
competitive algorithms, and its convergence curves grow very fast; in particular,
during the early algorithm iterations, the NSPSO occasionally achieve better results
than the MOFWA on some large problem instances (for example, see Figs.11.4f and
11.5d, f). Therefore,we can conclude that the NSPSO has a good search capability,
but it is easy to be trapped in the local optima and thus converge prematurely.

182
11
FWA for Multiobjective Optimization
By comparison, we found that the MOFWA also converges fast, but it can effec-
tively avoid the local optima and guide the search toward the Pareto fronts. In sum-
mary, on the test VFR problems, the proposed MOFWA has signiﬁcant performance
advantages over the other competitive algorithms.
11.4.2 Case Study of a Real-World CO VRF Problem
We apply the MOFWA to a real-world Camellia oleifera VRF problem in Yichun
District, Jiangxi Province, East China. The plant area is 585 ha and can be partitioned
intotwoparts:375hainsunnyslopeand210hainsemi-sunnyslope,whicharefurther
divided into 40 ﬁelds and 24 ﬁelds, respectively, according to their gradients. The
crop quality is evaluated based on the percentage of unsaturated fatty acid in fruits.
According to our ﬁeld experiments, the yield and quality estimation model of the
sunny and semi-sunny areas are, respectively, as follows:
Ya = 850.2 + 9.3x1 + 6.1x2 + 14.88x3 + 6.68x1x2
+ 2.04x2x3 −3.06x1x3 −0.61x2
1 −4.12x2
2 −0.35x2
3
(11.27)
Yb = 661.8 + 19.02x1 + 26.46x2 + 1.49x3 −4.78x1x2
+ 1.6x2x3 + 3.08x1x3 −3.98x2
1 −1.65x2
2 −0.67x2
3,
(11.28)
Qa = 59.9 + 0.37x1 + 4.94x2 + 0.62x3 + 0.52x1x2
−0.04x2x3 + 0.04x1x3 −0.31x2
1 −0.47x2
2 −0.02x2
3,
(11.29)
Qb = 49.1 + 0.9x1 + 2.25x2 + 1.18x3 + 0.39x1x2
−0.03x2x3 + 0.1x1x3 −0.27x2
1 −0.35x2
2 −0.04x2
3,
(11.30)
where Ya and Yb stand for the yields of the sunny area and the semi-sunny area
respectively, and Qa and Qb stand for the corresponding quality objectives. And the
mathematical formulation of the real-world problem is as follows (where ai denoted
the area of the ﬁeld i):

11.4 Computational Experiments
183
min Q(X) =
40

i=1
aiYa(X)Qa(x) +
64

i=41
aiYb(x)Qb(x)
(11.31)
min C(x) =
64

i=1
3

j=1
p j xij
(11.32)
min E(x) =
6

i=1
4Ei(x)
(11.33)
s.t.
64

i=1
Yi(x) ≥Y L
(11.34)
64

i=1
3

j=1
p j xij ≤CU
(11.35)
3

j=1
R j(x) ≤RU
(11.36)
xij ≥0, ∀i = 1, 2, . . . , 64. ∀j = 1, 2, 3.
(11.37)
The unit price of the fertilizer N, P, and K are respectively 6.3, 3.5, and 4.6
(RMB/kg). The gradient and plant density d of each ﬁeld are presented in Table11.5.
The organization has a precision agriculture management software which employs
a multiobjective random search (MORS) algorithm based on [45] for VFR optimiza-
tion and decision. The software has been used for three years and demonstrated its
contribution to crop production. We respectively execute the MORS and the MOFWA
algorithms for ﬁve times on this problem. The MORS consumes 48 s in average and
its best result set consists of 10 solutions. The MOFWA uses only 5.1 s in average and
its best result set consists of 6 solutions. Table11.6 presents the objective function
values of the result solutions.
It is not difﬁcult to observe that none of the solutions obtained by the MOFWA are
dominated by those obtained by the random search. On the contrary, except solution
♯7, all the other solutions obtained by the MORS are dominated by the result of
the MOFWA. For example, the MOFWAs solution ♯1 dominates the random search
solution ♯1, the MOFWAs ♯3 dominates the MORSs ♯8 and ♯9, and the MOFWAs
♯5 dominates the random search ♯2 ♯6. That is, the coverage of the result set of
MOFWA to that of MORS is 88.9%, and the coverage of MORS to MOFWA is 0.
After empirical analysis, the decision-maker believes that solutions ♯2, ♯3, and ♯5
of the MOFWA (but none of the random search) are preferred VRF solutions that
achieve excellent balance between the objectives, and takes solution ♯3 as the ﬁnal

184
11
FWA for Multiobjective Optimization
Table 11.5 The input data of the real-world CO VRF problem: ﬁeld gradients and plant densities
Fields
1
2
3
4
5
6
7
8
9
10
11
12
αi(o)
8.24
9.05
8.6
8.29
9.32
8.1
7.63
9.81
10.69
8.8
8.43
8.91
di (per ha) 1227.4
1306.2
1256
1275.4
1323.5
1280.3
1249.7
1290.1
1310.7
1218.6
1283.4
1311.9
Fields
13
14
15
16
17
18
19
20
21
22
23
24
αi(o)
9.53
9.14
8.22
9.17
11.26
10.8
8.95
9.79
8.68
11.05
9.36
9.6
di (per ha) 1287.1
1277.3
1265.2
1300.1
1331.5
1298.6
1257
1310.3
1190.4
1359.2
1281
1286.7
Fields
25
26
27
28
29
30
31
32
33
34
35
36
αi(o)
9.86
9.45
10.7
11.47
10.44
9.72
10.66
11.02
11.19
9.85
10.26
10.03
di (per ha) 1309.4
1270.8
1367.3
1385
1333.9
1259.4
1351
1403.8
1388.9
1327.6
1340
1315.3
Fields
37
38
39
40
41
42
43
44
45
46
47
48
αi(o)
11.22
10.37
10.64
11.08
16.9
16.45
15.41
17.52
16.1
17.05
15.79
16.18
di (per ha) 1364.6
1311.1
1315
1335.6
1540
1524.9
1500.7
1623.9
1474.8
1588.8
1510
1525.2
Fields
49
50
51
52
53
54
55
56
57
58
59
60
αi(o)
15.32
14.66
14.78
16.36
15.71
16.08
14.8
15.68
16.33
15.03
15.69
15.85
di (per ha) 1507.8
1502.5
1495.8
1540.3
1527.2
1550.2
1477
1542.4
1562.1
1446
1526.5
1528
Fields
61
62
63
64
αi(o)
16.15
16.93
15.7
15.56
di (per ha) 1564.4
1611.5
1540.6
1506.2

11.4 Computational Experiments
185
Table 11.6 The non-dominated solution sets obtained by the old random search algorithm and by
the MOFWA for the real-world CO VRF problem
Algorithm
Optimized solutions
MORS
# 1(56.2, 86.5, 32.9)
#2(54.8, 84.4, 33.2)
#3(54.4, 85.7, 31.0)
# 4(54.1, 85.2, 32.1)
# 5(53.8, 80.3, 34.0)
# 6(53.5, 84.0, 33.8)
# 7(52.8, 84.9, 30.7)
# 8(52.6, 83.5, 31.9)
# 9(52.5, 82.7, 32.1)
MOFWA
# 1(57.3, 84.9, 31.4)
# 2(56.7, 82.9, 31.2)
# 3(56.4, 82.6, 31.5)
# 4(56.1, 81.3, 33.4)
# 5(55.6, 83.6, 30.8)
# 6(54.8, 85.1, 30.5)
The solutions are represented by their objective values (Y, C, E)
decision which suggests to put more emphasis on the crop quality for sunward ﬁelds
and put more emphasis on the crop yield for non-sunward ﬁelds. Furthermore, having
seen the effectiveness of the new algorithm, the organization decides to integrate the
MOFWA into the precision agriculture management software.
11.5 Discussion
From the computational experiment results, we can see that the proposed MOFWA
can obtain high-quality solutions for the VRF problem:
• For small-sized problem instances where mn ≤200, the MOFWA has a much
higher probability of reaching the Pareto optimal front of the problem than other
state-of-the-art multiobjective optimization algorithms.
• For relatively middle-sized and large-sized instances, the result solution sets
obtained by the MOFWA are always better than the result sets obtained by other
competitive algorithms. The larger the instance size, the more obvious the advan-
tage of the MOFWA is.
Among the stochastic optimization algorithms used in the comparative experiments,
MOFWA exhibits the best performance in terms of convergence speed and robust-
ness, and its performance is not very sensitive to the sizes of problem instances.
This is mainly because the common explosion of high-quality ﬁreworks provides

186
11
FWA for Multiobjective Optimization
an excellent capability of local search, and speciﬁc explosion of low quality helps
to diversify the search. Moreover, the introduction of DE operators enhances the
algorithms capability of exploration. The effective combination of the exploration
and exploitation makes the MOFWA intrinsically a robust stochastic search method
[46]. On the other hand, the enhanced diversiﬁcation mechanism also provides good
capability of avoiding early convergence to local optima. In general, we can conclude
that the proposed MOFWA is mostly appropriate for the test VRF problem instances
studied in the chapter.
However, the ﬁreworks explosion method employed in the MOFWA has its draw-
backs. At each algorithm iteration, each individual (ﬁrework) in the population gen-
erates a certain number of new solutions (sparks). Such a number is set in the range of
2–20 in our algorithm, and thus the required number of function evaluations (NFE)
of MOFWA is much larger than other algorithms with the same population size. Due
to the complexity of the nonlinear objective functions and constraint functions of
the VRF problem, this make the MOFWA consume high computational resources.
To mitigate the effect, the population size of the MOFWA is set much smaller than
that of other algorithms, and we ﬁne-tune the control parameters of the MOFWA to
make it generate appropriate number of sparks at different stages of evolution.
Another drawback is that the number of control parameters to be manually tuned
in the MOFWA is more than other typical evolutionary algorithms such as GA and
DE. Moreover, the integration with DE operator increases the number of parameters.
This will bring more burdens when apply the MOFWA to a new problem.
Nevertheless, as a new heuristic method for multiobjective optimization, the
MOFWA offers a great performance advantage in solving complicated VRF problem
instances. Based on our experiments, the recommended parameter values of the algo-
rithm are provided for the proposed VRF problem. And we believe that the algorithm
can be improved and applied to many other kinds of optimization problems further.
11.6 Summary
Fireworks algorithm is a swarm intelligence algorithm that has a promising perfor-
mance on many global optimization problems. However, the study of its application
in multiobjective optimization is very few. An efﬁcient MOFWA algorithm was pre-
sented for oil crop VRF problems, which uses a problem-speciﬁc strategy for gen-
erating the initial population, uses the concept of Pareto dominance for individual
evaluation and selection, and combines the DE operators to increase the information
sharing and thus diversify the search. The algorithm has been successfully applied
to a number of VRF problems and demonstrated its efﬁciency and effectiveness.
This research provides a novel insight into the practicability of the ﬁreworks
algorithm heuristic in multiobjective optimization. Except the strategy of initial pop-
ulation generation, most features of the MOFWA could be adapted or extended to
solve other multiobjective optimization problems.

References
187
References
1. Y.-J. Zheng, Q. Song, S.-Y. Chen, Multiobjective ﬁreworks optimization for variable-rate fer-
tilization in oil crop production. Appl. Soft Comput. 13(11), 4253–4263 (2013)
2. K.S. Raju, D.N. Kumar, Multicriterion decision making in irrigation planning. Agric. Syst.
62(2), 117–129 (1999)
3. C. Shouyu, Fuzzy optimization of multi-dimensional multi-objective dynamic programming
and its application to farm irrigation. J. Hydraul. Eng. 4, 33–38 (2002)
4. M. Kilic, S. Anac, Multi-objective planning model for large scale irrigation systems: method
and application. Water Resour. Manag. 24(12), 3173–3194 (2010)
5. Y. Wang, D. Zhang, The optimization model of multi-objective fertilization of rice seedbed. J.
Biomath. 18(4), 467–472 (2002)
6. Y. Yuan, L. Mao, L. Lujiu, Z. Guobing, C. Xi, W. Li, Algorithm of fertilization model based
on intelligent computing. Trans. Chin. Soc. Agric. Eng. 12, 2008 (2008)
7. Y. Helong, D. Liu, G. Chen, B. Wan, S. Wang, B. Yang, A neural network ensemble method
for precision fertilization modeling. Math. Comput. Model. 51(11), 1375–1382 (2010)
8. C.A.C. Coello, D.A. Van Veldhuizen, G.B. Lamont, Evolutionary Algorithms for Solving Multi-
objective Problems, vol. 242 (Springer, Berlin, 2002)
9. N. Srinivas, K. Deb, Muiltiobjective optimization using nondominated sorting in genetic algo-
rithms. Evol. Comput. 2(3), 221–248 (1994)
10. K. Deb, A. Pratap, S. Agarwal, T. Meyarivan, A fast and elitist multiobjective genetic algorithm:
NSGA-II. IEEE Trans. Evolut. Comput. 6(2), 182–197 (2002)
11. E. Zitzler, L. Thiele, Multiobjective evolutionary algorithms: a comparative case study and the
strength Pareto approach. IEEE Trans. Evol. Comput. 3(4), 257–271 (1999)
12. J.D. Knowles, D.W. Corne, M-PAES: a memetic algorithm for multiobjective optimization,
in Proceedings of the 2000 Congress on Evolutionary Computation, vol. 1 (IEEE, 2000), pp.
325–332
13. H.A. Abbass, R. Sarker, C. Newton, PDE: a pareto-frontier differential evolution approach for
multi-objective optimization problems, in Proceedings of the 2001 Congress on Evolutionary
Computation, vol. 2 (IEEE, 2001), pp. 971–978
14. X. Li, A non-dominated sorting particle swarm optimizer for multiobjective optimization, in
Genetic and Evolutionary Computation GECCO 2003 (Springer, Berlin, 2003), pp. 37–48
15. Z. Zhang, Immune optimization algorithm for constrained nonlinear multiobjective optimiza-
tion problems. Appl. Soft Comput. 7(3), 840–857 (2007)
16. C.K. Goh, K.C. Tan, D.S. Liu, S.C. Chiam, A competitive and cooperative co-evolutionary
approach to multi-objective particle swarm optimization algorithm design. Eur. J. Oper. Res.
1, 42–54 (2010)
17. C. Shi, Z. Yan, Z. Shi, L. Zhang, A fast multi-objective evolutionary algorithm based on a tree
structure. Appl. Soft Comput. 10(2), 468–480 (2010)
18. P.K. Tripathi, S. Bandyopadhyay, S.K. Pal, An adaptive multi-objective particle swarm opti-
mization algorithm with constraint handling, in Handbook of Swarm Intelligence, ed. by B.K.
Panigrahi, Y. Shi, M.-H. Lim. Adaptation, Learning, and Optimization, vol. 8 (Springer, Berlin,
2011), pp. 221–239
19. W.K. Mashwani, A. Salhi, A decomposition-based hybrid multiobjective evolutionary algo-
rithm with dynamic resource allocation. Appl. Soft Comput. 12(9), 2765–2780 (2012)
20. D. Chen, F. Zou, J. Wang, A multi-objective endocrine PSO algorithm and application. Appl.
Soft Comput. 11(8), 4508–4520 (2011)
21. A. Zhou, Q. Bo-Yang, H. Li, S.-Z. Zhao, P.N. Suganthan, Q. Zhang, Multiobjective evolutionary
algorithms: a survey of the state of the art. Swarm Evol. Comput. 1(1), 32–49 (2011)
22. S. Chen, Y. Zheng, C. Cattani, W. Wang, Modeling of biological intelligence for SCM system
optimization. Comput. Math. Methods Med. 2012, 30 (2011)
23. T. Niknam, M. Zare, J. Aghaei, Scenario-based multiobjective Volt/Var control in distribution
networks including renewable energy sources. IEEE Trans. Power Deliv. 27(4), 2004–2019
(2012)

188
11
FWA for Multiobjective Optimization
24. P. Ahmadi, M.A. Rosen, I. Dincer, Multi-objective exergy-based optimization of a polygener-
ation energy system using an evolutionary algorithm. Energy 46(1), 21–31 (2012)
25. T. Niknam, H. Zeinoddini Meymand, H. Doagou Mojarrad, An efﬁcient algorithm for multi-
objective optimal operation management of distribution network considering fuel cell power
plants. Energy 36(1), 119–132 (2011)
26. T. Niknam, A. Kavousifard, S. Tabatabaei, J. Aghaei, Optimal operation management of fuel
cell/wind/photovoltaic power sources connected to distribution networks. J. Power Sources
196(20), 8881–8896 (2011)
27. P. Ahmadi, I. Dincer, Thermodynamic and exergoenvironmental analyses, and multi-objective
optimization of a gas turbine power plant. Appl. Therm. Eng. 31(14–15), 2529–2540 (2011)
28. Y.-J. Zheng, S.-Y. Chen, Y. Lin, W.-L. Wang, Bio-inspired optimization of sustainable energy
systems: a review. Math. Probl. Eng. 2013, 28 (2013)
29. M. Janga Reddy, D. Nagesh Kumar, Evolving strategies for crop planning and operation of
irrigation reservoir system using multi-objective differential evolution. Irrig. Sci. 26(2), 177–
190 (2008)
30. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
31. J. Kennedy, R. Eberhart et al., Particle swarm optimization, in Proceedings of IEEE interna-
tional conference on neural networks, vol. 4 (Perth, Australia, 1995), pp. 1942–1948
32. Y. Tan, Z.M. Xiao, Clonal particle swarm optimization and its applications, in IEEE Congress
on Evolutionary Computation. CEC 2007 (2007), pp. 2303–2309
33. Y.J. Zheng, X.L. Xu, H.F. Ling, A hybrid ﬁreworks optimization method with differential
evolution. Neurocomputing (2012)
34. R. Storn, K. Price, Differential evolution-a simple and efﬁcient heuristic for global optimization
over continuous spaces. J. Glob. Optim. 11(4), 341–359 (1997)
35. J.L. Yu, Agricultural Experiments with Polydesign (Beijing University of Agriculture Press,
Beijing, 1993)
36. R.K. Ru, Soil Plant Nutrition Principles and Fertilization (Chemical Industry Press, Beijing,
1998)
37. B.W. Silverman, Density Estimation for Statistics and Data Analysis (Chapmanand Hall, Lon-
don, 1986)
38. Z. Cai, Y. Wang, A multiobjective optimization-based evolutionary algorithm for constrained
optimization. IEEE Trans. Evol. Comput. 10(6), 658–675 (2006)
39. M.S. Alam, M.M. Islam, X. Yao, K. Murase, Diversity guided evolutionary programming: a
novel approach for continuous optimization. Appl. Soft Comput. 12(6), 1693–1707 (2012)
40. J. Hájek, A. Szöllös, J. Šístek, A new mechanism for maintaining diversity of pareto archive
in multi-objective optimization. Adv. Eng. Softw. 41(7), 1031–1057 (2010)
41. W. Gong, Z. Cai, A multiobjective differential evolution algorithm for constrained optimiza-
tion, in IEEE Congress on Evolutionary Computation. CEC 2008 (IEEE World Congress on
Computational Intelligence, 2008), pp. 181–188
42. Q.-Y. Guo, Z.-Y. LI, X.-W. Tu, Plant nutritional aspects and effects of fertilizer application
in rapeseed in red-yellow soil of South China. Fertilizer application of double-low rapeseed
cultivar, zhongshuang no. 7 in red paddy soil. Chin. J. Oil Crop Sci. 1, 011 (2001)
43. L. Yankun, W. Huishan, H.J. Zhuang, Z. Lin, J.L. Yongye, Preliminary report on fertilization
trial of canarium album. Guangdong For. Sci. Technol. 5, 004 (2007)
44. E. Zitzler, K. Deb, L. Thiele, Comparison of multiobjective evolutionary algorithms: empirical
results. Evol. Comput. 8(2), 173–195 (2000)
45. J. Wei-yi Qian, Y. Yang, H.W. Yang, W. Jin-xia, A new random group search algorithm for
solving the multi-objective programming problems. J. Liaoning Norm. Univ. Nat. Sci. 30(2),
141 (2007)
46. D. Karaboga, B. Basturk, On the performance of artiﬁcial bee colony (ABC) algorithm. Appl.
Soft Comput. 8(1), 687–697 (2008)

Chapter 12
S-Metric-Based Multi-objective Fireworks
Algorithm
This chapter is to present how to apply FWA to solving multi-objective optimiza-
tion problems with the help of a hypervolume indicator such as S-metric, then pro-
poses a S-metric multi-objective ﬁreworks algorithm (S-MOFWA). The S-metric
is a frequently used quality measure for solution sets comparison in evolutionary
multi-objective optimization algorithms (EMOAs), which is also used to evaluate the
contribution of a single solution among the solution sets. Traditional multi-objective
optimization algorithms usually perform a (μ + 1) strategy and update the external
archive one by one, while the proposed S-MOFWA performs a (μ+μ) strategy, thus
converging faster to a set of Pareto solutions by three steps: (1) Exploring the solution
space by mimicking the explosion of ﬁreworks; (2) Performing a simple selection
strategy for choosing the next generation of ﬁreworks according to their S-metric;
(3) Utilizing an external archive to maintain the best solution set ever found, with a
new archive deﬁnition and a novel updating strategy, which can update the archive
with μ solutions in a single process. The detailed comparison results with NSGA-II,
SPEA2, and PESA2 demonstrate the efﬁciency of the proposed S-MOFWA.
12.1 Introduction
A decision vector a is said to dominate a vector b(a ≺b), iff fi(a) ≤fi(b) for all i
and fj(a) ≤fj(b) for at least one j with i, j ∈1, . . . , n, f : Rm →Rn and a, b ∈Rm.
The set of non-dominated decision vectors in Rn is called a Pareto (optimal) set. The
corresponding image under f in the solution space is called the Pareto front. The
multi-objective optimization problem requires to get a best Pareto front.
Multi-objective optimization problem has two or more conﬂicting objectives to
optimize simultaneously. Lack of prior knowledge about the objectives, we usually
investigate into a vast number of solutions and reserve the non-dominated ones, i.e.,
a Pareto solution set, as the approximation of the true Pareto optimal set. This idea
contributes directly to the rapid development of evolutionary multi-objective opti-
mization algorithm (EMOA) [1].
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_12
189

190
12
S-Metric-Based Multi-objective Fireworks Algorithm
In Multiple Single Objective Pareto Sampling (MSOPS) [2, 3], the relationship
between conﬂicting objectives was quantiﬁed as weights added to each objective.
Despite unknown, when enough trials of ways to combine and adjust the weights
were explored, enough non-dominated solutions would be found to make a good
spread of the Pareto front.
During the evolution of EMOA, there are two crucial points to consider, i.e., con-
vergence and diversity. Most EMOAs consider them apart. For example, in NSGA-
II [4], the non-dominated sorting is performed ﬁrst to classify all the solutions into
hierarchical dominated levels as their measure of convergence, then a crowding dis-
tance will be calculated as their measure of diversity. In SPEA2 [5], the ﬁtness of
each solution is composed by two parts: dominated strength which represents the
convergence and density metric which represents the diversity.
Different from the above methods, indicator-based methods choose a single met-
ric to represent both convergence and diversity. IBEA [6] deﬁnes a binary quality
indicator and each solution needs to be compared with every other solution in order
to get a single ﬁtness. The comparison times required is as much as that for non-
dominated sorting. S-metric [7] is another indicator that represents both convergence
and diversity, by calculating the hypervolume of dominated space. A great amount
of attention was paid to this metric since its proposal. Nicola Beume and Carlos did
a detailed analysis on the complexity of computing the hypervolume indicator [8].
Knowles and Corne [9] gave an example for fast calculating the S-metric.
In this chapter, a multi-objective ﬁreworks algorithm based on S-metric is pre-
sented [10]. The explosion amplitudes of ﬁreworks and the numbers of sparks are
calculated according to each ﬁrework’s S-metric, the next generation of ﬁreworks is
selected by their S-metric and an external archive with ﬁxed size is used to maintain
the best solution set found ever by a novel updating strategy.
12.2 S-Metric
The S-metric or hypervolume indicator was proposed by Zitzler and Thiele [7] for the
ﬁrst time. It can be viewed as the size of the space covered or size of dominated space.
The S-metric is a frequently used quality measure for solution sets comparison in
the ﬁeld of evolutionary multi-objective optimization algorithms (EMOAs). Besides,
S-metric can also be used to evaluate the contribution of a single solution among the
solution set. Let Λ denote the Lebesgue measure, then the S-metric for a solution set
M = {m1, m2, . . . , mi, . . . mn} is deﬁned as [11]:
S(M) := Λ
 
m∈M
{x|m ≺x ≺xref }

(12.1)
where, ≺denotes the dominance relationship, i.e., a ≺b means b is dominated by a.
The xref is a reference point dominated by all valid solutions in the solution set.

12.2 S-Metric
191
Each solution in set M contributes a part to this total S-metric, so the S-metric for
each solution is deﬁned as
S(mi) = ΔS(M, mi) := S(M) −S(M\{mi}).
(12.2)
Fig. 12.1 S-metric.
a S-metric for the set.
b S-metric for a solution
(a)
(b)

192
12
S-Metric-Based Multi-objective Fireworks Algorithm
The S-metric for a solution mi can be seen as the size of space dominated by
mi, but not dominated by any other solutions in the set. A large value means mi is
important in set M, while zero indicates that mi must be dominated by some other
solutions in set M.
Figure12.1 shows the meaning of S-metric and the difference between S-metric
for set and S-metric for a single solution. It can be seen that S-metric for a solution
has no relation with the choice of a reference point. A solution with larger S-metric
valueliesfurtherawayfromitsnearestneighbors,whichiscoincidentwiththedensity
metric of SPEA2 [5] in the meaning of diversity. Besides, a nonzero value guarantees
that the solution is non-dominated, coincident with convergence meaning, because
according to the deﬁnition in Eq.(12.2), the S-metric for a solution dominated by
others is zero.
12.3 The Proposed S-MOFWA
As for multi-objective problems, there are more than one objective function, f1, f2,
etc. But for an EMOA, the ﬁtness function is unique.
In the proposed S-MOFWA, the ﬁtness for an individual is its S-metric in the
current solution set. Note that the S-metric of one solution is merely relevant to its
neighbors and could change when the members of the current solution set change.
A brief description of S-MOFWA is as follows: At ﬁrst, a number of ﬁreworks
will be randomly initialized in the search space. In each iteration, we calculate the
numbers of explosion sparks and amplitudes of explosion for each ﬁrework. Then the
explosion and Gaussian mutation will be performed to generate a number of sparks.
Finally, ﬁreworks for next iteration are selected from the candidates. An external
archive is used to maintain the best solution set.
The details of each step can be described below.
12.3.1 Initialization
The algorithm randomly selects N points in the search space as the ﬁrst generation of
ﬁreworks. As it is the ﬁrst generation, there is no need to calculate or set the objective
function values. Here, we simply set their S-metric the same values, so they have
the same sparks numbers and explosion amplitudes, because we do not hurry to tag
them as good or bad in the initial stage.

12.3 The Proposed S-MOFWA
193
12.3.2 Calculation of the Explosion Sparks Number
and Explosion Amplitude
As mentioned above, to make a contrast among the ﬁreworks, the ﬁrework with better
ﬁtness will have larger number of explosion sparks and smaller explosion amplitude,
while the ﬁrework with worse ﬁtness will have smaller number of sparks and bigger
explosion amplitude. To ensure this principle, the number of sparks zi and explosion
amplitude Ai for each ﬁrework xi are calculated by Eqs.(12.3) and (12.4), which are
determined by the S-metric solely.
zi = Me ∗
S(xi) + ε
N
i=1 (S(xi)) + ε
,
(12.3)
Ai = ˆA ∗
Smax −S(xi) + ε
N
i=1 (Smax −S(xi)) + ε
,
(12.4)
where Smax = max(S(xi)), i = 1, 2, . . . , N. Me and ˆA are two constant parameters
to control the sparks number and amplitude. To bound the values zi to a proper range,
two other constants a, b ϵ [0, 1] was also needed.
zi =
⎧
⎪⎨
⎪⎩
round(aMe) if zi < aMe,
round(bMe) if zi > bMe,
round(zi)
otherwise.
(12.5)
Note that the best solution, namely the solution with a max S-metric, obtains a
very small explosion amplitude which is close to zero according to Eq.(12.4). This
may be acceptable in the ﬁnal stage of the evolution, but that is unreasonable for the
early stage. The nonlinearly decreasing amplitude threshold Amin introduced in [12]
is also employed here.
12.3.3 Explosion
For each ﬁrework xi, zi sparks will be generated according to Algorithm 12.1. This
explosion procedure can be seen as the exploration among the search space.

194
12
S-Metric-Based Multi-objective Fireworks Algorithm
Algorithm 12.1 Explosion
1: for j = 1 →zi do
2:
initialize the location of the explosion spark: ˆxj = xi
3:
for each dimension k of xi do
4:
if rand(0, 1) < 0.3 then
5:
ˆxk
j = xk
i + Ai ∗rand(−1, 1)
6:
if ˆxk
j out of bounds then
7:
ˆxk
j =U(xk
min,xk
max)
8:
end if
9:
end if
10:
end for
11: end for
12.3.4 Gaussian Mutation
To increase the diversity of generated explosion sparks swarm, the algorithm will
also generate a number of special Gaussian sparks through a process called Gaussian
mutation. Algorithm 12.2 describes the calculation process of Gaussian sparks.
Algorithm 12.2 Gaussion mutation
1: initialize the location of the Gaussian spark: ˜xi = xi
2: for each dimension k of xi do
3:
if rand(0, 1) < 0.5 then
4:
˜xk
i = xk
i ∗normrnd(1, 1)
5:
if ˜xk
i out of bounds then
6:
˜xk
i =U(xk
min,xk
max)
7:
end if
8:
end if
9: end for
12.3.5 Fireworks Selection and Archive Updating
12.3.5.1 Calculation of S-Metric
In conventional FWA, the ﬁreworks for next generation are selected according to
their ﬁtness and diversity measure [13]. As for S-MOFWA, the S-metric is calculated
according to their values of objective functions and is also used as selection criteria. In
case of two objectives, we sort the sparks in descending order according to the values
of the ﬁrst objective function f1. After wiping away the points that are dominated,
we will get a sequence sorted in ascending order according to the second objective
function f2 (Those points which lose in both two objective functions are dominated).
As shown in Fig.12.2a, the S-metric for solution mi is calculated as in Eq.(12.6)

12.3 The Proposed S-MOFWA
195
Fig. 12.2 S-metric.
a Calculation of S-metric.
b The increase of neighbors’
S-metric after wiping away a
solution
(a)
(b)
S(mi) = (f1(mi) −f1(mi−1)) ∗(f2(mi) −f2(mi+1)).
(12.6)
Noted that the calculation of S-metric does not depend on the order of the objec-
tives to be considered. All the objectives are of coordinate importance. In the process
of sorting the sparks by one objective, we can easily ﬁnd out and erase those who
are dominated by others. Then the S-metric of a non-dominated spark is determined
by its nearest neighbors in each objective. The nearest neighbors restrict the space
which is only dominated by this spark, but not dominated by any other sparks. The
volume of this space is the so-called hypervolume, namely the S-metric.
For two objectives problem, in each iteration, after the sorting process we can
calculate all the sparks’ S-metric one by one. So the complexity is O(nlogn). For
three or more objectives problem, the complexity is O(nd/2logn) due to Overmars
and Yap [14]. Here, n is the number of sparks and d is the number of objectives.

196
12
S-Metric-Based Multi-objective Fireworks Algorithm
Here, we need to clarify that the calculation of S-metric can be of high efﬁciency
and is also suitable for more objective problems. Boris Naujoks and Nicola Beume
presented an algorithm for calculating S-metric in three-objective space [15]. The
LebMeasure algorithm described by Fleischer [16] and the Hypervolume by Slicing
Objectives (HSO) algorithm proposed by Knowles [17] and Zitzler [18] compute the
whole S-metric for all dimensional objectives. Furthermore, many researches have
been done to improve the efﬁciency of calculating S-metric. While and Hingston
presented a fast algorithm for calculating S-metric [19]. Then Bradstreet proposed
an incremental version to calculate S-metric. On the basis of those improvements,
many new S-metric-based EMOAs have been invented, such as SMS-EMOA [20]
and MOPSOhv [21]. In addition, Ponweiser solved the multi-objective problems
on a limited budget using S-metric [22], Beume accelerated S-metric calculation
by considering dominated hypervolume [23], Kukkonen improved pruning of non-
dominated solutions based on crowding distance for bi-objective optimization prob-
lems [24], so on.
12.3.5.2 Selection of the Fireworks
When selecting the ﬁreworks of next generation, there are two keys to consider,
namely convergence and diversity. In the conventional FWA, we choose the best one
solution at ﬁrst to guarantee convergence and lead the swarm developing in the right
direction. Then we will select the rest N −1 solutions according to their crowding
metric as a representation of diversity, which guaranteeing the algorithm will not drop
into a premature stage. Because the best N individuals are usually generated by the
same ﬁrework, thus located in a nearby place, resulting in a low diversity. So the other
N −1 ﬁreworks are selected according to crowding metric and located far away from
the ﬁrst one ﬁrework. While, as mentioned previously, a large S-metric guarantees
that the solution is far away from its nearest neighbors and it is non-dominated. In
another word, a solution with large S-metric points out a good region which has been
explored too little. Our job is to strengthen the exploration in the suggested region. So
there is no need to introduce another unnecessary diversity measure and we simply
choose the best N solutions as the ﬁreworks of next generation according to their
S-metric, namely the ﬁtness.
12.3.5.3 Updating of the External Archive
In conventional archive strategy, external archive is just a place for storing non-
dominated solutions. When a new individual was generated, it would be examined
whether it can be put into the archive or replace someone. This was done one by
one, which means the external archive needs to be updated a thousand times if one
thousand new individuals were generated.
The external archive always keeps a ﬁxed number of solutions (assume the
ﬁxed number is K), which differs from the conventional grow-up strategy. These

12.3 The Proposed S-MOFWA
197
K solutions are chosen from the candidates pool which includes the sparks gener-
ated by ﬁreworks explosion and Gaussian mutation and the old archive. The selected
K solution needs to ensure that they obtain a maximum S-metric in all the K-sets.
We need to point out that the solutions in this archive are not necessarily non-
dominated. Noted that in the early stage of evolution, not enough non-dominated
solutions should have been found. But the external archive always keeps K solutions.
Inevitably, there are many dominated solutions existing in the archive, but this does
not matter. Along with the iteration, the S-metric of the solution set in the archive
will increase step by step. Fleischer proved that a ﬁnite solution set with the theoretic
maximum of S-metric comes necessarily from the true Pareto front [16].
Figure12.3 demonstrates the evolvement of the external archive on function KUR
of S-MOFWA. The size of archive is ﬁxed as K = 100. In the ﬁrst 20 iterations, most
of the solutions were dominated by those points close to the true Pareto front. After
hundreds of iterations, nearly all of them became non-dominated, and the updates
concentrate in the areas very close to the true Pareto front.
Fig. 12.3 MOFWA evolution process on KUR. a Iteration = 10. b Iteration = 20. c Iteration =
250. d Iteration = 300

198
12
S-Metric-Based Multi-objective Fireworks Algorithm
In each iteration, we will choose K solutions to build up the new archive. This
sounds like a combinational problem. However, one solution’s S-metric is solely
determined by its nearest neighbors, which indicates that wiping off a solution only
inﬂuences the S-metric of a few solutions. Here, we keep a proper ratio (about 1:1)
between the size of external archive and the total number of sparks, wipe off the worst
one solution in the candidates set and ﬁx its neighbors’ S-metric. This procedure
repeats iteratively, eventually remaining K solutions become the new archive.
As shown in Fig.12.2b, after wiping off the solution m3, its nearest neighbors
m2 and m4 have a proportional increase in their S-metric, while the others remain
unchanged. Take m2 for instance, the ratio of new S(m2) to the old S-metric is L2/L1.
So a simple update will be enough.
S(m2) = S(m2) ∗f2(m2) −f2(m4)
f2(m2) −f2(m3),
(12.7)
Moreover, the corresponding operator also needs to be performed on m4, another
nearest neighbor:
S(m4) = S(m4) ∗f1(m4) −f1(m2)
f1(m4) −f1(m3),
(12.8)
The procedure of updating the external archive is described in Algorithm 12.3.
Algorithm 12.3 Updating strategy for the external archive
1: note Q as the sparks generated by ﬁreworks explosion and Gaussian mutation
2: note A as the external archive
3: note C as the candidates pool, set C = Q ∪A
4: calculate S-metric for each candidate in C /* a minimum heap may be useful here. */
5: while |C| > K /* K is the size of the external archive, a ﬁxed number */ do
6:
r ←arg minmϵC(S(m)) /* detect element of C with the lowest S-metric */
7:
C ←C\{r} /* eliminate detected element*/
8:
ﬁx the S-metric values of nearest neighbors of r according to Eqs.(12.7) and (12.8)
9: end while
10: A ←C
12.3.6 Framework of S-MOFWA
The framework of S-MOFWA is described in Algorithm 12.4. First of all, we ran-
domly initialize a set of ﬁreworks in the search space, then calculate each ﬁrework’s
objective function value. Fitness value, namely the S-metric is calculated according
to their objective function values. Then the sparks number and explosion ampli-
tude are calculated by each ﬁrework’s S-metric. After the explosion and Gaussian
mutation, a group of sparks or candidate solutions are generated. When the external
archive is updated, a set of solutions are selected as the next generation of ﬁreworks
and the algorithm goes into the next iteration.

12.4 Experiments
199
Algorithm 12.4 Framework of S-MOFWA
1: randomly initialize N ﬁreworks in the solution space
2: initialize external archive A as the set of the initial ﬁreworks
3: iteration p ←0
4: while terminal conditions are not met do
5:
p ←p + 1
6:
update the current evaluation times t
7:
update the nonlinearly decreasing amplitude threshold Amin
8:
for j = 1 →N do
9:
calculate objective function values of each ﬁrework xi
10:
end for
11:
calculate each ﬁreworks’ S-metric according to Eq.(12.6)
12:
calculate each ﬁreworks’ sparks number and explosion amplitude according to Eqs.(12.3)
and (12.4) respectively
13:
perform the ﬁrework explosion as Algorithm 12.1 for each ﬁrework
14:
perform the Gaussian mutation as Algorithm 12.2 for each ﬁrework
15:
calculate each candidate’s S-metric /* candidates refers to the sparks obtained by explosion
and Gaussion mutation, and the solutions in the current archive */
16:
select N best candidates as the next generation of ﬁreworks, according to their S-metric
17:
update the external archive A
18: end while
19: output the external archive A
12.4 Experiments
To validate the performance of the proposed S-MOFWA, experiments on benchmark
suite which contains six test functions were designed. Moreover, the performance
comparison with other three well-known algorithms (NSGA-II, SPEA2 and PESA2)
are also conducted.
12.4.1 Experimental Setup
The test problems used to compare different algorithms are chosen from a set of
signiﬁcant studies in the area of multi-objective optimization. Schaffer’s problem
(SCH) and Kursawe’s problem (KUR) are presented by Veldhuizen [25]. ZDT1,
ZDT2, ZDT3, ZDT6 are selected from the six test functions suggested by Zitzler [26].
All above problems have two objective functions and none of them have constraints.
The properties of these functions are listed in Table12.1.
For setting the parameters, the number of ﬁreworks N = 10, size of external
archive K = 100, total number of sparks Me = 100, constants a = 0.05, b = 0.4,
the mutation rate for Gaussian mutation r1 = 0.5 and for explosion r2 = 0.3,
evaluation times evalsmax = 2,00,000, the same for the comparison algorithms.
Each function runs 20 repetition times. The rest of parameters for the ﬁreworks are

200
12
S-Metric-Based Multi-objective Fireworks Algorithm
Table 12.1 Benchmark functions (Dim. = dimension)
Dim.
Range
Optimal locations
Property
SCH
1
[−1000, 1000]
x ∈[0, 2]
Convex
KUR
3
[−5, 5]
Refer [25]
Non-convex
ZDT1
30
[0, 1]
x1 ∈[0, 1], xj = 0, j ̸= 1
Convex
ZDT2
30
[0, 1]
x1 ∈[0, 1], xj = 0, j ̸= 1
Non-convex
ZDT3
30
[0, 1]
x1 ∈[0, 1], xj = 0, j ̸= 1
Convex, disconnected
ZDT6
10
[0, 1]
x1 ∈[0, 1], xj = 0, j ̸= 1
Non-convex,
nonuniformly spaced
identical to [12]. For setting the parameters of NSGA-II, SPEA2 and PESA2, they
are set according to the source code of [2011 Durillo Jmetal].1
The experimental platform: Win8; Intel Core Duo E4500 CPU; 2.2GHz; 2GB
RAM. The proposed S-MOFWA was run on MATLAB2012b and the comparison
algorithms were run on Eclipse-Java.
12.4.2 Evaluation Criteria
We take the convergence measure and covered space measure to characterize the
performance of one multi-objective optimization algorithm.
The convergence measure is the average distance to the closest point of the true
Pareto front as used in [27]. The smaller the distance is, the closer the solution set
lies to the true Pareto front. Usually, we uniformly select hundreds of points from
the true Pareto front as a approximation. So this measure is calculated as the average
distance to the closest point in the selected approximate set.
The covered space measure is a relative ratio of the covered hypervolume, for
estimation of the diversity of the spread. The denominator could be the size of space
covered by the true Pareto front, or the size of a ﬁxed feasible function space. Here,
we take the latter for simple calculation. The maximum values are limited by the
different shapes of test functions, so the comparative values are only of meaning
within the same test functions. Of course, a larger value indicates the spread is better
distributed.
12.4.3 Experimental Results
The results for average convergence measure and covered space measure of
S-MOFWA, NSGA-II, SPEA2, and PESA2 are listed on Tables12.2 and 12.3, repec-
1jmetal: http://sourceforge.net/projects/jmetal/.

12.4 Experiments
201
Table 12.2 Convergence measure
ZDT1
ZDT2
ZDT3
SCH
KUR
ZDT6
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
S-MOFWA 9.10E−04 1.00E−04 8.00E−04 0.00E+00 3.90E−03 4.00E−04 3.20E−03 2.00E−04 9.00E−03 1.10E−03 2.80E−03 1.00E−04
NSGAII
1.40E−03 2.00E−04 1.10E−03 1.00E−04 4.60E−03 3.00E−04 9.90E−03 7.70E−03 1.31E−02 1.40E−03 2.80E−03 2.00E−04
SPEA2
1.30E−03 1.00E−04 8.00E−04 1.00E−04 4.70E−03 2.00E−04 1.19E−02 8.10E−03 1.01E−02 7.00E−04 2.80E−03 1.00E−04
PESA2
1.60E−03 3.00E−04 1.30E−03 4.00E−04 4.50E−03 4.00E−04 8.50E−03 7.40E−03 1.61E−02 2.40E−03 1.57E−02 2.05E−02

202
12
S-Metric-Based Multi-objective Fireworks Algorithm
Table 12.3 Covered space measure
ZDT1
ZDT2
ZDT3
SCH
KUR
ZDT6
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
Mean
Std
S-MOFWA 0.6536
1.03E−02 0.3279
1.50E−03 0.7795
3.00E−04 0.8303
1.70E−03 0.3558
8.70E−03 0.3227
0.00E+00
NSGAII
0.6602
3.00E−04 0.3272
4.00E−04 0.7785
1.00E−04 0.811
5.87E−02 0.3435
6.20E−03 0.3205
2.00E−04
SPEA2
0.6616
1.00E−04 0.3285
1.00E−04 0.7789
1.00E−04 0.8001
5.05E−02 0.3446
6.98E−03 0.3224
2.00E−04
PESA2
0.6252
2.49E−02 0.3092
9.60E−03 0.7703
9.20E−03 0.8082
5.37E−02 0.3516
9.82E−03 0.3131
7.80E−03

12.4 Experiments
203
Table 12.4 p-values for convergence measure (The values in bold indicate that MOFWA is signif-
icantly better compared with the other algorithms)
vs NSGA-II 8.857E−05
8.900E−05
1.030E−04
5.170E−04
1.030E−04
2.959E−01
vs SPEA2
8.900E−05
2.509E−02
8.900E−05
1.400E−04
1.162E−03
6.542E−01
vs PESA2
8.900E−05
1.200E−04
2.930E−04
3.185E−03
8.900E−05
2.190E−04
Table 12.5 p-values for covered space measure (The values in bold indicate that MOFWA is
signiﬁcantly better compared with the other algorithms)
vs NSGA-II 7.314E−02
4.550E−03
8.900E−05
7.932E−02
1.630E−04
8.900E−05
vs SPEA2
1.507E−03
5.016E−01
1.200E−04
7.189E−03
1.630E−04
8.900E−05
vs PESA2
3.380E−04
8.900E−05
1.325E−03
2.277E−02
2.190E−04
8.900E−05
tively. And the T-test results are listed in Tables12.4 and 12.5. In addition, we also
plot the ﬁnal archive in the objective space on the benchmark functions of S-MOFWA
(Fig.12.4).
12.4.4 Discussion
For convergence measure, the proposed S-MOFWA performs best on all the test
functions, speciﬁcally discussed by the type of functions as follows.
12.4.4.1 Convex Functions
On convex functions ZDT1 and SCH, the proposed S-MOFWA signiﬁcantly out-
performs the other three with smallest average convergence measure and standard
deviation, both ability and stability are proven.
12.4.4.2 Non-convex Functions
On non-convex function ZDT2, the proposed S-MOFWA and SPEA2 both get the
best convergence measure, and S-MOFWA shows a little superiority in the standard
deviation. On function KUR, S-MOFWA performs best and SPEA2 ranks the second.
12.4.4.3 Disconnected Functions
On disconnected function ZDT3, S-MOFWA beats the other three in average con-
vergence measure but the standard deviation ranks the last.

204
12
S-Metric-Based Multi-objective Fireworks Algorithm
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
f1
f2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
f1
f2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
f1
f2
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
f1
f2
−20
−19
−18
−17
−16
−15
−14
−12
−10
−8
−6
−4
−2
0
2
f1
f2
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
f1
f2
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 12.4 Plots of the ﬁnal archive of returned by S-MOFWA in the objective space. a ZDT1.
b ZDT2. c ZDT3. d SCH. e KUR. f ZDT6
12.4.4.4 Nonuniformly Spaced Functions
On nonuniformly spaced function ZDT6, S-MOFWA, NSGA-II, and SPEA2 get the
best average convergence measure, while PESA2 does not obtain a value of the same
order of magnitude.
For covered space measure, the S-MOFWA wins on four functions among all the
six functions. On ZDT1 and ZDT2, SPEA2 gets the best average covered space with
the smallest standard deviations, which means SPEA2 distributes the solutions better

12.4 Experiments
205
along the non-dominated front. The density metric involved in the ﬁtness assignment
of SPEA2 may contribute a lot to this ability. On the other four functions, S-MOFWA
ﬁnds a better spread than other algorithms.
12.5 Environmental/Economic Power Dispatch Problem
12.5.1 Problem Statement
The environmental/economic power dispatch (EED) problem is to minimize two
competing objective functions, fuel cost and emission, while satisfying several equal-
ity and inequality constraints. Generally, the problem can be formulated as follows.
Minimization of Fuel Cost: The generators’ cost curves are represented by
quadratic functions with sine components. The superimposed sine components rep-
resent the rippling effects produced by the steam admission valve openings. The total
fuel cost F(PG) can be expressed as
F(PG) =
N

i=1

ai + biPGi + ciP2
Gi + |di sin

ei

Pmin
Gi −PGi

|

,
(12.9)
where N is the number of generators, ai, bi, ci, di, ei are the cost coefﬁcients of the
ith generator, and PGi is the real power output of the ith generator. PG is the vector
of real power outputs of generators and deﬁned as
PG = [PG1, PG2, . . . , PGN ]T.
(12.10)
Minimization of Emission: The atmospheric pollutants such as sulfur oxides SOx
and nitrogen oxides NOx caused by fossil-fueled thermal units can be modeled sep-
arately. However, for comparison purposes, the total emission E(PG) of these pollu-
tants can be expressed as
E(PG) =
N

i=1
(10−2(αi + βiPGi + γiP2
Gi) + ςi exp(λiPGi)),
(12.11)
where αi, βi, γi, ςi, λi are coefﬁcients of the ith generator emission characteristics.
12.5.2 Evaluation Criterion
For a real-world problem, it is impossible to calculate the average distance to the
true Pareto front, because we have no idea about the true Parato front. The only thing

206
12
S-Metric-Based Multi-objective Fireworks Algorithm
Table 12.6 Evaluation results
Evaluation criteria
Criteria (S-MOFWA) versus NSGA-II
Criteria (S-MOFWA) versus SPEA2
Mean
Standard deviation
Mean
Standard deviation
0.6859
0.0221
0.7325
0.0447
Criteria1 (NSGA-II) versus S-MOFWA
Criteria1 (SPEA2) versus S-MOFWA
Mean
Standard deviation
Mean
Standard deviation
0.3141
0.0112
0.2675
0.0104
we can do is to compare two algorithms’ results (Pareto sets). Here we take each
algorithm’s portion in the Pareto set of the union of two algorithms’ results as the
evaluation criterion.
12.5.3 Results and Discussion
It can be seen that, in the union of two algorithms’ results, most non-dominated
solutions come from S-MOFWA while less part come from the contrastive algorithm
(NSGA-IIorSPEA2).ThisindicatestheParetofrontfoundbyS-MOFWAhasabetter
spread than the other two algorithms (Table12.6).
For the EED problem, S-MOFWA obtained a more economic and more environ-
mental friendly solution than the solutions obtained by the other two algorithms.
12.6 Summary
This chapter presented the S-metric based multi-objective ﬁreworks algorithm (S-
MOFWA). As an indicator-based EMOA, the S-metric was introduced to guide the
iteration process of FWA. The explosion numbers of sparks and explosion amplitudes
are calculated according to each ﬁreworks’ S-metric. And the ﬁreworks for next
iteration are selected by the values of their S-metric. The selection for next generation
and explosion to generate sparks were quite simpler compared to the conventional
FWA. A novel archive strategy was employed to keep the best solution set, which
reduced the labor in traditional archive strategy.
The experimental results on benchmark functions demonstrate that the proposed
S-MOFWA outperforms the three well-known algorithms, i.e., NSGA-II, SPEA2
and PESA2, in terms of the measures of convergence and covered space. On most
test functions, S-MOFWA can ﬁnd a better spread and gets closer to the true Pareto
front than the three competing algorithms.

References
207
References
1. K. Deb, Multi-objective Optimization Using Evolutionary Algorithms, vol. 16 (Wiley, Chich-
ester, 2001)
2. E.J. Hughes, MSOPS-II: A general-purpose many-objective optimiser, in IEEE Congress on
Evolutionary Computation, CEC 2007 (IEEE, 2007), pp. 3944–3951
3. E.J. Hughes, Multiple single objective pareto sampling, in The 2003 Congress on Evolutionary
Computation, CEC’03. vol. 4 (IEEE, 2003), pp. 2678–2684
4. K. Deb, A. Pratap, S. Agarwal, T.A.M.T. Meyarivan, A fast and elitist multiobjective genetic
algorithm: NSGA-II. IEEE Trans. Evol. Comput. 6(2), 182–197 (2002)
5. E. Zitzler, M. Laumanns, L. Thiele, SPEA2: Improving the strength pareto evolutionary algo-
rithm (2001)
6. E.Zitzler,S.Knzli,Indicator-basedselectioninmultiobjective search. Parallel ProblemSolving
from Nature-PPSN VIII (Springer, Berlin, 2004), pp. 832–842
7. E. Zitzler, L. Thiele, Multiobjective optimization using evolutionary algorithms: comparative
case study, in Parallel Problem Solving from Nature, PPSN V (Springer, Berlin, 1998), pp.
292–301
8. N. Beume, C.M. Fonseca, M. López-Ibáñez, L. Paquete, J. Vahrenhold, On the complexity of
computing the hypervolume indicator. IEEE Trans. Evol. Comput. 13(5), 1075–1082 (2009)
9. J. Knowles, D. Corne, Properties of an adaptive archiving algorithm for storing nondominated
vectors. IEEE Trans. Evol. Comput. 7(2), 100–116 (2003)
10. L. Lang, Z. Shaoqiu, T. Ying, S-metric based multi-objective ﬁreworks algorithm, in The 2015
IEEE Congress on Evolutionary Computation, vol. 2 (IEEE, 2015), pp. 1257–1264
11. M. Emmerich, N. Beume, B. Naujoks, An emo algorithm using the hypervolume measure
as selection criterion. Evolutionary Multi-criterion Optimization (Springer, Berlin, 2005), pp.
62–76
12. S. Zheng, A. Janecek, Y. Tan, Enhanced ﬁreworks algorithm, in 2013 IEEE Congress on
Evolutionary Computation (CEC) (IEEE, 2013), pp. 2069–2077
13. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
14. H. Gazit, New upper bounds in Klee’s measure problem. SIAM J. Comput. 20(6), 1034–1045
(1991)
15. B. Naujoks, N. Beume, M. Emmerich, Multi-objective optimisation using s-metric selection:
application to three-dimensional solution spaces, in The 2005 IEEE Congress on Evolutionary
Computation, vol. 2 (2005) pp. 1282–1289
16. M. Fleischer, The measure of pareto optima applications to multi-objective metaheuristics,
Evolutionary Multi-criterion Optimization (Springer, New York, 2003), pp. 519–533
17. J.D. Knowles, Local-search and hybrid evolutionary algorithms for Pareto optimization. Ph.D.
thesis, University of Reading (2002)
18. E. Zitzler, Hypervolume metric calculation, Computer Engineering and Networks Laboratory
(TIK) (Zürich, 2001)
19. L. While, P. Hingston, L. Barone, S. Huband, A faster algorithm for calculating hypervolume.
IEEE Trans. Evol. Comput. 10(1), 29–38 (2006)
20. N. Beume, B. Naujoks, M. Emmerich, SMS-EMOA: multiobjective selection based on domi-
nated hypervolume. Eur. J. Oper. Res. 181(3), 1653–1669 (2007)
21. I.C. Garcia, C.A. Coello Coello, A. Arias-Montano, MOPSHOhv: a new hypervolume-based
multi-objectiveparticleswarmoptimizer,in2014IEEECongressonEvolutionaryComputation
(CEC) (IEEE, 2014), pp. 266–273
22. W. Ponweiser, T. Wagner, D. Biermann, M. Vincze, Multiobjective optimization on a limited
budget of evaluations using model-assisted S-metric selection. Parallel Problem Solving from
Nature–PPSN X (Springer, 2008), pp. 784–794
23. N. Beume, G. Rudolph, Faster S-metric calculation by considering dominated hypervolume as
Klee S measure problem (2006)

208
12
S-Metric-Based Multi-objective Fireworks Algorithm
24. S. Kukkonen, K. Deb, Improved pruning of non-dominated solutions based on crowding dis-
tance for bi-objective optimization problems, in Proceedings of the World Congress on Com-
putational Intelligence (WCCI-2006) (IEEE Press, Vancouver, 2006), pp. 1179–1186
25. D.A. Van Veldhuizen, Multiobjective evolutionary algorithms: classiﬁcations, analyses, and
new innovations. Technical report, DTIC Document (1999)
26. E. Zitzler, K. Deb, L. Thiele, Comparison of multiobjective evolutionary algorithms: empirical
results. Evol. Comput. 8(2), 173–195 (2000)
27. K. Deb, M. Mohan, S. Mishra, A fast multi-objective evolutionary algorithm for ﬁnding well-
spread pareto-optimal solutions. KanGAL report 2003002 (2003)

Chapter 13
Discrete Firework Algorithm
for Combinatorial Optimization Problem
Considering the excellent performance of FWA for real parameter optimization
problems, a novel FWA variant was proposed for tackling discrete optimization
problems, especially for Travelling Salesman Problem (TSP). We ﬁrst give a brief
introduction to TSP, followed by the detailed description of the discrete ﬁreworks
algorithm (DFWA). The DFWA remains the basic framework of FWA and intro-
duces some major changes in explosion operator, selection strategy and mutation
operator, respectively. Speciﬁcally, exploration operator is redeﬁned and mutation
operator and selection strategy are modiﬁed according to the properties of discrete
problems. In explosion operator, every ﬁrework is able to accept a worse solution
and generate a spark with lower ﬁtness, which refer to the mechanism of simulated
annealing. However, the controlling parameter θ changes with the feedback of opti-
mization process rather than time. In addition, this version of DFWA appropriately
changed its behavior of the local search method to suit in the framework of FWA. A
lot of experimental results demonstrated that DFWA is very efﬁcient and effective for
TSP, which sheds new light on more and more discrete combinatorial optimization
problems.
13.1 Travelling Salesman Problem
13.1.1 Problem Statement
In essence, TSP is a discrete optimization problem whose aim is to ﬁnd the minimal
tour length. After starting from the original city, a salesman needs to go through all
cities and then goes back to the original city. The tour length is deﬁned as the total
length between every adjacent cities in the visiting sequence.
TSP can be described formally as follows:
Deﬁnition 13.1 Given N cities{c1, c2, c3, . . . , cN}, the distance between every two
cities cic j denoted by d(ci, c j), the Travelling Salesman Problem is to ﬁnd a
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_13
209

210
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
permutation x = (x1, x2, x3, . . . , xn), xi ∈{1, 2, 3, . . . , N} to minimize the tour
length of x:
L(x) =
N−1

i=1
d(cxi , cxi+1) + d(cxn, cx1)
(13.1)
s.t.
∀i ∈{1, 2, . . . , N}, ∃j ∈{1, 2, . . . , N}, x j = i.
(13.2)
In this chapter, only the symmetric TSP is considered. For symmetric TSP, every
two cities is connected and their distance satisﬁes
d(ci, c j) = d(c j, ci), ∀1 ≤i, j ≤N.
(13.3)
TSP emerges in diverse domain from X-ray crystallography to VLSI chip fabri-
cation [1]. An important real-world application of TSP is from the logistics industry,
where minimizing the tour length can signiﬁcantly reduce the delivery cost and
gain more economic proﬁt. Hence, the research on TSP optimization is worth wide
attention.
AlthoughtheunderlyingprincipleofTSPisreallysimple,thesolutionspacegrows
exponentially with the number of cities increasing. Taking 42 cities for example, if
we enumerate all possible routes to calculate the minimal length, the number of
feasible routes is 41! = 3.349, which is intractable. It has been proved that TSP is a
NP-complete problem [2]. There is no effective algorithm for solving TSP exactly. In
general, there are two alternatives to address TSP: (1) turning to heuristics that merely
ﬁnd suboptimal tours, and (2) developing optimization algorithms that work well in
most “real-world” cases, rather than worst-case instances. Driven by the insatiable
real-world need, the scale of TSP to be solved increases rapidly, from the relatively
humble 318 cities [3] all the way up to 2392 [4] and 7393 [5]. In fact, the last case
took one of the most powerful cluster for more than 3 years.
In recent decades, a large number of swarm intelligent algorithms have been
applied to TSP, such as genetic algorithm (GA), particle swarm optimization (PSO)
as well as ant colony optimization (ACO) method, etc. Relying heavily on heuristic
information, swarm intelligence can obtain good (though often suboptimal) solu-
tions within acceptable time. Mhlenbein et al. [6] applied GA to TSP and obtained
reasonable solutions on two instances with 442 cities and 531 cities, respectively.
However, the solutions are far from the optimum. Braun [7] proposed a GA variant
(dubbed improved GA) speciﬁcally for TSP. For TSP of small scale (less than 442
cities), the proposed method can easily obtain the best result very quickly. For TSP
with 666 cities, the error rate computed by the improved genetic algorithm is only
0.04 %, which was a great breakthrough at that time.
In 2004, Clerc [8] proposed a discrete version of particle swarm optimization
(DPSO). DPSO was applied to TSP in [9] where a local search mechanism was
introduced. The performance of DPSO is comparable to the improved genetic algo-
rithm and better in a few cases.

13.1 Travelling Salesman Problem
211
Ant algorithm is one of best algorithms for addressing TSP. The typical ant
algorithm—ant colony system was ﬁrst introduced by Dorigo and Gambardella [10]
in 1997. The experimental results shown that ant colony system is originally appro-
priate for TSP. The original ACS obtains error rate of 0.68% in 198-city instance,
while the improved ACS with local search mechanism can obtain 0.11% error in
318-cite instance. Notice that this result is still poorer than best versions of GA
[11, 12]. A modiﬁed version of ACS—Max Min Ant System (MMAS) [13] sig-
niﬁcantly improves the performance of ACS. It can easily solve problems with the
number of cities less than 100 and obtain 0.04% error rate in 198-city instance and
1.93% error rate in rat783, respectively. If local search mechanism is introduced, the
error rate with more than 1000 cities (i.e., ﬂ1157 instance) by Max Min Ant System
is about 0.2%, which is state-of-the-art result for TSP.
In summary, swarm intelligence algorithms are effective methods for solving TSP.
With the number of cities less than 400, several improved algorithms exploiting local
search mechanism can ﬁnd the optimum with high probability.
13.2 Discrete Firework Algorithm
Conventional FWA is designed for real parameter optimization, where explosion
operator and mutation operator are utilized for searching in large space and the
selection strategy determines the ﬁreworks in next generation. Because of the com-
plexity in discrete function space, the completely random mutation strategy utilized
in the FWA is not likely to have well performance in discrete space. Hence, we
particularly introduced two novel operators which are quite different those of con-
ventional FWA. In addition, we modiﬁed the deﬁnition of explosion amplitude as
well to suit the discrete application. The framework of discrete ﬁreworks algorithm
(DFWA) remains the same with conventional FWA. In the following, we will discuss
discrete ﬁreworks algorithm for TSP in detail.
In TSP, the solution of DFWA is deﬁned as city sequence x = (x1, x2, x3, . . . , xn)
where xi is the identiﬁer of ith city. In travelling salesman problem, each city needs
one and only one visit by the salesman. The target function is L(x) Eq.(13.1).
Obviously, the goal of DFWA is:
min
x
L(x)
(13.4)
s.t. Eq.(13.2).
13.2.1 Framework of DFWA
The framework of discrete ﬁreworks algorithm is similar to that of the conven-
tional ﬁreworks algorithm. We keep the explosion operator, mutation operator, and

212
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
Fig. 13.1 Framework of
discrete FWA
selection strategy but remove the mapping rule in the DFWA. For explosion opera-
tor, two explosion operations, i.e. explosion operation I and explosion operation II,
are introduced here. For mutation operator, we put forward a new uniform mutation
operator. Meanwhile, selection strategy of discrete ﬁrework algorithm is specially
changed for travelling salesman problem. Because the offspring produced by DFWA
would not exceed the boundary of deﬁnition domain, the mapping rule is unnecessary
anymore. The framework of discrete ﬁreworks algorithm is listed in Fig.13.1. Note
that the whole framework is consistent with conventional ﬁreworks algorithm.
As shown in this ﬁgure, the discrete ﬁreworks algorithm consists of general com-
ponents in conventional FWA. At ﬁrst, a feasible initial population is randomly
generated, then sparks emerge by explosion operator and mutation operator. In the
end of this iteration, some individuals among ﬁreworks and sparks will be selected
into next generation according to selection strategy Algorithm 13.1. Now we are
going to introduce each component in next several sections.

13.2 Discrete Firework Algorithm
213
Algorithm 13.1 Discrete Fireworks Algorithm
1: randomly generate N ﬁreworks x(i)
2: while stop criterion is not satisﬁed do
3:
generate sparks for each ﬁrework by explosion operator
4:
generate sparks for each ﬁrework by identical mutation operator
5:
change the amplitude
6:
select N ﬁreworks into next generation
7: end while
13.2.2 Explosion Operator
The explosion operator is an important component in FWA, which is the major
mechanism that drives local search and global search. Thanks to the smoothness in
continuous function space, a better solution will probably appear using random local
mutation operation. But the situation differs much for optimizing discrete functions,
where the simple random local search method will become inefﬁcient in ﬁnding a
neighbor suboptimum. Considering this, the discrete ﬁreworks algorithm deﬁnes
two special explosion operations. As aforementioned, they are called explosion
operation I and explosion operation II, respectively.
13.2.2.1 Explosion Operation I
Similar to discrete particle swarm optimization (DPSO) where city swap is deﬁned
as basic operation, a kind of basic operation is also acquired in discrete ﬁreworks
algorithm. In concept, the interaction information between individuals in DFWA is
to guide the global search process. Actually, there is no direct interaction between
ﬁreworks in FWA. As described before, the random local search mechanism is not
suitable in discrete ﬁreworks algorithm. Hence taking the properties of travelling
salesman problem into consideration, we deﬁne the edge swap operation as the basic
operation, which is equivalent to 2-opt local search method.
The 2-opt operation is shown in Fig.13.2, where the edges—(a, b) and (c, d) are
replaced by (a, c) and (b, d). This operation ensures that the result is still a feasible
route. If a change reduces the route length, this operation is going to be accepted.
Note that if they are substituted by (a, d) and (b, c), the original feasible route will be
divided into two independent parts. If the route is described as x, the 2-opt operation
can be completed by one sub-sequence reversion. For convenience, we deﬁne (a, b)
as the edge between city ca and city cb and xi, j as the sub-sequence between ith city
to jth city in this sequence.
Though explosion operation I is based on 2-opt method, there are two obvious
differences between explosion operation I and the 2-opt local search method.

214
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
Fig. 13.2 The 2-opt local search operation (left the original route, right mutated route)
1. Explosion operation I is not as greedy as 2-opt method. During the searching
process, explosion operation has the ability to accept a worse solution. Taking
Fig.13.2 for example, where Lo = d(ca, cb)+d(cc, cd) denotes the original total
length of (a, b) and (c, d), Lm = d(ca, cc) + d(cb, cd) denotes the edge length
after edge swap, we accept a worse solution with probability pa:
pa = exp (−Lm/Lo ∗θ), Lm > Lo,
(13.5)
where θ is a parameter controlling the probability of choosing a worse solution.
According to the deﬁnition, the more similar Lm and Lo, the larger pa is. Also
the probability goes larger when θ decreases. The beneﬁt of these two properties
is to give an opportunity of jumping out of local optimum while reducing the
useless mutations. Intuitively, if the selected cities are distant in the instance and
Lm is much larger than Lo then this operation is likely to be invalid. Parameter θ
can control the range of pa in order to maintain an proper probability.
2. This operation can be applied to multiple binary operations. It is capable of
smoothing the mutated route by additional 2-opt methods by which the probability
of jumping out 2-opt local optimum increases and the probability of abandoning
a potential solution decreases in the meantime. 2-opt local optimum is a kind of
state where 2-opt method is unable to obtain a better solution.
The pseudocode is depicted by Algorithm 13.2, where 2-opt(c, k) indicates the
2-opt operation on edge (xc, xc+1) and (xk, xk+1). With all city iterated whose
sequence index is ‘k’, 2-opt(c, k) for ‘c’ is called 2-opt optimization for ‘c’. The
‘rand’ function randomly generates a real number in range [0, 1] and “randi(n)”
randomly generates a integer between 1 and n.

13.2 Discrete Firework Algorithm
215
Algorithm 13.2 Explosion operation I
1: Input: x : ﬁreworks
2: Output: spark : spark generated by ﬁreworks
3: spark = x
4: z = randi(n) // n is the number of cities
5: rp = randperm(n) // randperm randomly generates a permutation
6: for i = 1 : n, where rp(i) ̸= z do
7:
a = z, b = z + 1, c = rp(i), d = rp(i) + 1 is the indices of the sequence
8:
sort(a,b,c,d);
9:
Lo = d(cxa, cxb) + d(cxc, cxd )Lm = d(cxa, cxc) + d(cxb, cxd )
10:
if Lo > Lm then
11:
reverse xb,c, return.
12:
else
13:
if rand < pa then
14:
reverse xb,c
15:
%2-opt optimization for ‘a’
16:
for k = 1:n do
17:
2-opt(a,k);
18:
end for
19:
%2-opt optimization for ‘c’
20:
for k = 1:n do
21:
2-opt(c,k);
22:
end for
23:
return.
24:
end if
25:
end if
26: end for
13.2.2.2 Explosion Operation II
In the optimization procedure, the capacity of explosion operation I degrades as
when the algorithm has already found a “good” local optimum. In that case, a more
powerful operation is needed. Similar to explosion operation I, explosion operation II
is based on 3-opt local search method and it changes 3 edges at one time.
The two possible 3-opt are shown in Fig.13.3. In this ﬁgure, three edges, i.e., (a,
b), (c, d) and (e, f), are removed and three other edges are created to form a new
Fig. 13.3 Two possible 3-opt moves (left original tour, right mutated tours)

216
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
Algorithm 13.3 Explosion operation II
1: Input: ﬁreworks x
2: Output: spark generated by ﬁreworks spark
3: spark = x
4: z1 = randi(n), z2 = randi(n) // n is the number of cities
5: rp = randperm(n), // randperm randomly generates a permutation with length ‘n’
6: for z3 = 1 : n, where z3 ̸= z1andz3 ̸= z2 do
7:
sort(z1, z2, z3);
8:
a = rp(z1), b = rp(z1) + 1, c = rp(z2), d = rp(z2) + 1, e = rp(z3), f = rp(z3) + 1
9:
for for 4 possible swap do
10:
calculate Lo and Lm
11:
if Lo > Lm then
12:
accept and return.
13:
else
14:
if rand < pa then
15:
accept.
16:
2-opt optimization for ‘a’,‘b’ and ‘c’
17:
return.
18:
end if
19:
end if
20:
end for
21: end for
feasible route. If this operation reduces the total tour length, it is accepted. A route is
described as city sequence x = (x1, x2, x3, . . . , xn) and 3-opt operation can be done
within several subsequence reverse. Take the middle graph in Fig.13.3, for example,
reversing xb,c and xd,e is a possible 3-opt sequence operations. 3-opt operations has
four possible swap in total, all enumeration would not be listed here for simplicity.
For explosion operation II, the same improvement in explosion operation I is
applied. For more details, readers can refer to pseudocode Algorithm 13.3.
13.2.2.3 Sparks Number and Amplitude
The number of sparks is calculated as same as in former chapters. The spark number
is:
si = Me ∗
1/(Lmax −L(x(i)) + ε)2
n
j=1 1/(Lmax −L(x(i)) + ε)2
(13.6)
where, Lmax = max{L(x(i))}, Me is a parameter controlling the total number of
sparks generated by the ﬁrework.
In discrete ﬁreworks algorithm, we set the amplitude of all ﬁreworks to be an
identical ﬁxed value (θ, for example). This is based on the following observation:
θ has the same effect as in the conventional ﬁreworks algorithm. With smaller θ,
the probability of accepting worse solution is larger, resulting in stronger ability to
escape the local optimum and ﬁnd a better solution. In discrete optimization, the
population ﬁtness is believed to be close. The difference between individuals is tiny

13.2 Discrete Firework Algorithm
217
1
2
3
0
0.5
1
1.5
2
0
0.2
0.4
0.6
0.8
1
θ
Lmut / Lorg
paccept
Fig. 13.4 The impact of θ and Lm/Lo for pa
enough so that they should have identical probability to accept worse mutations. In
addition, θ is a exponent factor for pa. Figure13.4 also illustrates that pa is very
sensitive to control parameter θ. Hence the amplitudes of ﬁreworks are set to be
same value.
13.2.3 Identical Mutation Operator
Explosion operator I and explosion operator II focus on the edge swaps, which is
difﬁcult to improve some special routes, such as the left side in Fig.13.5. In this
ﬁgure, the route is unable to be improved by 2-opt and 3-opt methods, but can be
Fig. 13.5 2h-opt local search operation

218
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
Algorithm 13.4 Identical mutation operator
1: Input: ﬁreworks x
2: Output: spark generated by ﬁreworks spark
3: spark = x
4: a = randi(n)// n is the number of cities
5: rp = randperm(n), // randperm randomly generates a permutation with length ‘n’
6: for k = 1 : nwhere a ̸= rp(k) do
7:
calculate Lo and Lm
8:
if Lo > Lm then
9:
accept and return.
10:
end if
11: end for
done by inserting ‘c’ into edge (a, b). In FWA, the effect of mutation operator is to
search locally. Here, we simply deﬁne 2h-opt method as mutation operator in discrete
ﬁreworks algorithm, aiming at optimizing single point in the route sequence. 2h-opt
operation is one heuristic strategy between 2-opt method and 3-opt method, shown
in Fig.13.5 where edge (d, c), (a, b), and (c, e) are replaced by (a, c), (c, b), and (d,
e) to reduce the route length.
In sequence format, two reversion operations are required to implement 2h-opt.
Concretely, given sequence (. . . xa, xa+1 . . . xb−1, xb, xb+1 . . .), where a, b denotes
indices, in order to insert xb into edge (xa, xa+1), we need to reverse sub-sequence
xa+1,c at ﬁrst and reverse xa+2,c then. This operation is called 2h-opt(a, b). If a is
equal to n, then xa+1 = x1.
In the consideration that mutation operation is mainly used for local search, the
pa is not deﬁned for mutation operator which is equivalent to setting pa to be 0.
It means that mutation operation is unable to accept a worse solution. Thus we can
randomly select one city ‘a’, go through all indices ‘k’ in the sequence to operate 2h-
opt(a,k). Because the opportunity of being selected is same among all cities, we call
this mutation operation as identical mutation operator. Speciﬁcally, the pseudocode
described this procedure is given in Algorithm 13.4.
13.2.4 Selection Strategy
Selection strategy plays an important role in FWA. Similar to the conventional ﬁre-
works algorithm, the ﬁreworks and sparks are simultaneously selected into selection
pool in DFWA and we keep the best individual all the time. For other individuals, we
adopt a strategy similar to roulette strategy. If the random selection mechanism in the
EFWA is adopted in DFWA, then the probability of accepting “bad” sparks seems
too large to select efﬁcient individuals whose ﬁtness are close to current optimum.
In this regard, we speciﬁcally designed a strategy which is more likely to select high
ﬁtness individuals. The selected probability of each individuals is:

13.2 Discrete Firework Algorithm
219
psel(x(i)) =
1/(L(x(i)) −Lmin + ε)2
n
j=1 1/(L(x( j)) −Lmin + ε)2
(13.7)
where Lmin = min{L(x(i))} is the smallest ﬁtness, ε denotes the smooth parameter
in case of the appearance of zero divisor. According to Eq.(13.7), the probability
of individuals with lower ﬁtness is larger. Comparing to the ﬁrst power format, the
second power format can increase the ratio of excellent sparks and hence increase the
probability of being selected into next generation. We executed two experiments with
random selection strategy and the one described above. The results demonstrates that
the performance of random selection strategy is not very satisfactory which leads to
1% more average error rate. Of course, there might be other better strategies rather
than above discussions. We merely introduce one workable solution for selection
strategy here.
13.2.5 Adaptive Strategy
In the dynamic ﬁreworks algorithm (DynFWA), the best ﬁreworks, i.e., core ﬁreworks
stated in original paper [14], is self-adaptive by the reward of previous generations.
If a better solution is found, then the amplitude is increased by an ampliﬁcation
factor to enhance its global search ability. Otherwise, the amplitude is decreased by
reduction factor to enhance local search ability. In the DFWA, the same idea can be
adopted. Recall that the amplitude in DFWA is deﬁned as the control parameter θ
in pa. If a better solution is obtained, we had better decrease pa to accelerate local
search, otherwise it means that the population is probably trapped in suboptimum,
we had better to increase pa to supplement local search by giving more chances to
jump out of local optimum.
In the process of discrete optimization, there are much differences between local
optimums. Hence DFWA may need more than one generation to obtain a better result
which means it is necessary to observe the feedback for several consecutive gener-
ations and then make a reasonable adaption. In this version of DFWA, we count for
10 generations to see if a better solution is obtained. If not, increase amplitude. Once
the current ﬁreworks are better than the previous, decrease amplitude immediately.
The speciﬁc strategy is similar to that in DynFWA. θ is properly conﬁned within the
interval [1,2]. The pseudocode is given in Algorithm 13.5.
13.3 Experimental Results and Analysis
At the ﬁrst part of this section, we will discuss the issues of parameter settings and
performance (runtime) analysis. The main topic followed in this section is to display
the experimental results of the DFWA and the analysis. First, the experiments will be

220
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
Algorithm 13.5 Parameter θ self adaptive strategy
1: ﬁreworks xc
2: spark ˆxbest
3: ampliﬁcation factor Coefa
4: reduction factor Coefr
5: Nun = 0 counts the unchanged times.
6: if L( ˆxbest) < L(xc) then
7:
Nun = 0;
8:
θ = θ ∗Coefa;
9: else
10:
Nun = Nun + 1;
11:
if Nun ≥10 then
12:
θ = θ ∗Coefr;
13:
Nun = 0;
14:
end if
15: end if
executed in small test cases to verify if it has the capacity to solve simple travelling
salesman problems. Then a middle-sized test case and some large test cases are
employed. The test cases come from standard TSPLIB benchmark. All cases are
symmetric TSPs. The platform for this experiment is windows 7-64 bit operation
system with i3-370m CPU and matlab2013R software. The number of ﬁreworks is
50, the maximal spark number Me is 50 and the maximal iterations is identically
set to 40,000. In the experiments, we had three different DFWA. DFWA-na is the
discrete ﬁreworks algorithm with pa set to 0, i.e., the explosion operator is simply
local search method. This version is not a standard discrete ﬁreworks algorithm,
solely for contrast. DFWA-nri denotes a version of standard DFWA without reboot
mechanism, while DFWA-ri has the reboot mechanism.
13.3.1 Parameters Setting and Performance Analysis
Spark number has a huge impact on performance for ﬁreworks algorithm. According
to [15], pretty performance can be gained when ﬁreworks number is set to 5 and Me
(Eq.13.6) is set to 50. In the following, we will analyze the settings in discrete
ﬁreworks algorithm by two batches of experiments. In the ﬁrst one, we tried to verify
the acceptable ﬁreworks number by ﬁxing Me(50) and test case(d198). Figure13.6
illustrates the consequence. Horizontal ordinate denotes ﬁreworks number while
vertical ordinate denotes the average error rate. It shows that a proper setting for
ﬁreworks number is 5.
In the second experiment, we analyze the effect of Me. We ﬁxed the ﬁreworks
number to be 5 in d198 test case. The result is in Fig.13.7 where the horizontal
ordinate is Me and vertical ordinate means the error between global optimum. As
we can see from this graph, the best performance is acquired by 50 maximal spark
number. Note that the error rate difference between 30 and 50 is actually quite small,

13.3 Experimental Results and Analysis
221
1
2
3
4
5
6
7
8
9
10
11
2
4
6
8
10
12
14
x 10
−3
number of fireworks
error rate
Fig. 13.6 The impact of ﬁreworks number on performance
30
35
40
45
50
55
60
65
70
3.5
4
4.5
5
5.5
6
6.5
7
x 10
−3
Me
error rate
Fig. 13.7 The impact of maximal spark number on performance

222
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
only 0.3%. Hence we can conclude that the inﬂuence of this parameter is not that
vital as in conventional ﬁreworks algorithm. Without loss of generalization, we set
Me to be 50.
Discrete ﬁreworks algorithm conforms Markov model similar to conventional
ﬁreworks algorithm, because for the population in tth generation, the state of next
generation is not related to the previous states(t′ < t):
P(st+1|st, st1 . . . s1) = P(st+1|st)
(13.8)
where st is the state of the t-th generation, P(st) is the probability of being st in the tth
generation. Due to the elite strategy, the discrete ﬁreworks algorithm has absorbing
property with which an algorithm would not discard the best solution once found.
We call the Markov model with absorbing property as absorbing Markov process.
According to ﬁtness-level model, if there exists a certain state from which the prob-
ability of ﬁnding a better solution is exponentially small, this algorithm is supposed
to spend more than polynomial time to ﬁnd the global optimum. Assume there is
a route that is the solution second to the global optimum, and it has a subsequence
completely different from global optimum. The length of this subsequence is greater
than 6 (the maximal edges changed by explosion operator II), then the opportunity
of ﬁnding global optimum is less than:
p < C ∗pls/6
a
,
(13.9)
where C is an unrelated constant.
The expected running time is given by
E(t) > C ∗p−ls/6
a
,
(13.10)
which can be approximated as exponential running time.
13.3.2 Results on Small Test Cases
oliver30 and att48 are classical small test cases where oliver30 contains 30 cities and
att48 contains 48 cities. The results are shown in Tables13.1 and 13.2. DFWA-na,
DFWA-nri and DFWA-ri are all able to ﬁnd the global optimum of oliver30. However,
DFWA-na failed 7 times among 10 times experiment due to the disability to jump
out 3-opt local optimum. By contrast, DFWA-nri and DFWA-ri found optimum in all
runs which illustrates the efﬁciency of improvement for 2-opt and 3-opt in explosion
operator. In att48, DFWA-na found suboptimums in all runs and DFWA-nri failed
5/10 times, but DFWA-ri gives the best solution every time. It indicates that reboot

13.3 Experimental Results and Analysis
223
Table 13.1 The results on oliver30, the data besides DFWA is from [16], the minimal route length
is 420
oliver30
DFWA-na
DFWA-nri
DFWA-ri
Basic SA
Basic GA
Basic ACA
Average
421.4
420
420
437.6632
482.4671
447.3256
Best solution
420
420
420
424.9918
424.9918
440.8645
Worst solution
429
420
420
480.1452
504.5256
502.3694
Table 13.2 The results on att48, the data besides DFWA is from [16], the minimal route length is
33522
att48
DFWA-na
DFWA-nri
DFWA-ri
Basic SA
Basic GA
Basic ACA
Average
33878.5
33608
33522
34980
37548
41864
Best solution
33522
33522
33522
35745
36759
43561
Worst solution
34140
33700
33522
41864
34559
42256
mechanism may be helpful in optimizing small instances. Intuitively, owning to the
lack of interactions between populations, it is expensive to repeatedly generate new
population trying to leave locality. Reboot mechanism remits this disadvantage to
some extent, avoiding the waste of computation resource.
13.3.3 Results on Medium Test Case
In the following, the experiment is based on d198, an instance with 198 cities. The
other settings remain the same as above. Table13.3 demonstrates the result on d198
test case, where the error rate of DFWA-ri is about 0.4% and DFWA-nri is about
0.3% slightly outperforms the Max Min Ant System (MMAS) 1.2%, Ant Colony
System(ACS) 1.7%. It is necessary to point out that improved ant algorithm com-
bined with local search strategy can gain the global optimum with high probability.
Figure13.8 shows the convergence curve of DFWA-nri and DFWA-na which veriﬁes
that the improvement in explosion operator is efﬁcient in medium scale instances as
well.
Table 13.3 Result on d198, the data besides DFWA is from [13], the minimal route length is 15780
Average value
DFWA-na
DFWA-nri
DFWA-ri
MMAS
ACS
d198
16334
15838.6
15855
15972.5
16054

224
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
0
50
100
150
200
250
300
350
400
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8 x 10
4
Iteraion /100
Length of best solution
DFWA−na
DFWA−nri
Fig. 13.8 The convergence curve of DFWA-na and DFWA-ri on d198
Table 13.4 Result in larger test cases, the data besides DFWA is from [10]
DFWA-na
DFWA-nri
DFWA-ri
ACS
Optimum
pcb442
54351.8
52255.6
52341.3
51690
50778
att532
92317.5
89230
89237
88177.4
86729
rat783
9586.9
9198.1
9180.8
9066
8806
0
500
1000
1500
2000
2500
3000
3500
4000
4500
0
200
400
600
800
1000
1200
1400
1600
1800
2000
Fig. 13.9 A solution of d198 optimized by DFWA whose the error rate is 0.05%

13.3 Experimental Results and Analysis
225
13.3.4 Results on Large Test Case
The result of larger test instances can refer to Table13.4. The test set includes att532,
pcb442, rat783. This table indicates that this version of discrete ﬁreworks algorithm
cannot compete with standard ant algorithm with large instances, there is much more
room for progress (Fig.13.9).
13.4 Comparison with Classical Algorithms
The experimental results displayed above demonstrate that the discrete ﬁreworks
algorithm can efﬁciently solve small instances with city number less than 100, out-
performing the basic simulated annealing, basic genetic algorithm and basic ant
algorithm in [16]. When solving medium cases, DFWA can obtain better solutions
comparing to basic ant algorithms [10] owning to its excellent local search ability.
However, the performance of discrete ﬁreworks algorithm is limited in large-scale
instances due to its lack of individual interactions. Compared with those algorithms
combined with local search mechanisms such as genetic algorithm [11], Max Min
Ant System [13], discrete particle swarm optimization method [9], this discrete ver-
sion of FWA is still behind those classical algorithms.
In conclusion, this version of discrete ﬁreworks algorithm has the following
features:
1. DFWA can solve TSP in small instances with high probability and gain well
results in optimizing medium scale cases.
2. DFWA has a strong local search ability. It combines three kinds of local search
methods and some improvements are employed to enhance search capacity.
3. DFWAis basedonthefundament of basicconventional ﬁreworks algorithmwhere
the interactions between individuals are limited. This weakness reveals when to
solve large-scale problems.
In the future, we will mainly focus on the study of the interaction mechanism
among individuals in the swarm for a sharp improvement in performance of DFWA.
13.5 Summary
Discrete FWA is just in its ﬁddle era, this chapter merely presents our ﬁrst-trying
discrete version of FWA and its application in TSP brieﬂy. The discrete ﬁreworks
algorithm remains the basic framework of FWA and introduces some major changes
in explosion operator, selection strategy and mutation operator respectively. In explo-
sion operator, every ﬁrework is able to accept a worse solution and generate a spark
with lower ﬁtness, which is analogue to the mechanism of simulated annealing. How-
ever the controlling parameter θ changes with the feedback of optimization process

226
13
Discrete Firework Algorithm for Combinatorial Optimization Problem
rather than time. In addition, this version of discrete ﬁreworks algorithm properly
changes its behavior of the local search method to suit in the framework of FWA. As
the experimental results indicate, discrete FWA is very effective for TSP, it can be
applied to more discrete optimization problems potentially.
References
1. R.G. Bland, D.F. Shallcross, Large travelling salesman problems arising from experiments in
X-ray crystallography: a preliminary report on computation. Oper. Res. Lett. 8(3), 125–128
(1989)
2. R.G. Michael, D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-
completeness (WH Freeman and Company, San Francisco, 1979)
3. H. Crowder, M.W. Padberg, Solving large-scale symmetric travelling salesman problems to
optimality. Manag. Sci. 26(5), 495–509 (1980)
4. M. Padberg, G. Rinaldi, Optimization of a 532-city symmetric traveling salesman problem by
branch and cut. Oper. Res. Lett. 6(1), 1–7 (1987)
5. D.S. Johnson, L.A. McGeoch, The traveling salesman problem: a case study in local optimiza-
tion. Local Search Comb. Optim. 1, 215–310 (1997)
6. H. Mhlenbein, M. Gorges-Schleuter, O. Krmer, Evolution algorithms in combinatorial opti-
mization. Parallel Comput. 7(1), 65–85 (1988)
7. H. Braun, On solving travelling salesman problems by genetic algorithms. Parallel Problem
Solving from Nature (Springer, Berlin, 1991), pp. 129–133
8. M. Clerc, Discrete particle swarm optimization, illustrated by the traveling salesman problem.
New Optimization Techniques in Engineering (Springer, Berlin, 2004), pp. 219–239
9. M.F. Tasgetiren, P.N. Suganthan, Q.-Q. Pan, A discrete particle swarm optimization algorithm
for the generalized traveling salesman problem, in Proceedings of the 9th Annual Conference
on Genetic and Evolutionary Computation (ACM, 2007), pp. 158–167
10. M. Dorigo, L.M. Gambardella, Ant colony system: a cooperative learning approach to the
traveling salesman problem. IEEE Trans. Evol. Comput. 1(1), 53–66 (1997)
11. B. Freisleben, P. Merz, A genetic local search algorithm for solving symmetric and asymmetric
travelingsalesmanproblems,inProceedingsofIEEEInternationalConferenceonEvolutionary
Computation (IEEE, 1996), pp. 616–621
12. B. Freisleben, P. Merz, New genetic local search operators for the traveling salesman problem.
Parallel Problem Solving from Nature—PPSN IV (Springer, Berlin, 1996), pp. 890–899
13. H.H. Hoos, T. Stutzle, Max min ant system. Future Generation Computer Systems (Elsevier,
Paris, 2000)
14. S. Zheng, A. Janecek, J. Li, Y. Tan, Dynamic Search in Fireworks Algorithm, in IEEE Congress
on Evolutionary Computation (CEC) (2014), pp. 3222–3229
15. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
16. L. Fang, P. Chen, S. Liu, Particle swarm optimization with simulated annealing for TSP, in
Proceedings of the 6th WSEAS International Conference on Artiﬁcial Intelligence, Knowledge
Engineering and Data Bases, Corfu Island, Greece, February 2007, pp. 16–19

Chapter 14
Implementation of Fireworks Algorithm
Based on GPU
In recent years, the graphics processing unit (GPU) has gained much popularity in
general purpose computing, thanks to its low price and easy access. In this chapter,
a very efﬁcient FWA variant based on GPUs, so-called GPU–FWA for short, is
introduced. GPU–FWA modiﬁes the original FWA to suit the particular architecture
of the GPU. It does not need special complicated data structure, thus making it easy to
implement; meanwhile, it can make full use of the great computing power of GPUs.
The key components of GPU–FWA are FWA search, attract-repulse mutation, and
implementation which are elaborated in this chapter.
To make the chapter self-contained, a brief introduction of general purpose com-
puting on GPUs (GPGPU) is presented ﬁrst. Then we describe GPU–FWA in detail,
followed by the empirical comparison of GPU–FWA with conventional FWA and
popular PSO.
14.1 General Purpose GPU Computing
In the single-core CPU period, programmers have relied in enormous extension on
the advances in hardware to accelerate their applications; as a new generation of
processors is introduced, the same software just runs faster. However, due to energy-
consumption and heat-dissipation issues, the increase of the clock frequency and
the level of productive activities that can be performed in each clock period within a
single CPU is signiﬁcantly limited. Virtually, CPU vendors switched to models where
multiple processor cores are used in each chip to increase the processing power. We
have entered a multicore period and have to parallelize the legacy serial program to
fully exploit the horsepower of the new generation of CPUs.
Over the last few years, driven by the insatiable demand for realtime, high-
deﬁnition 3D graphics, the GPU has evolved into a massively parallel, many-
core processor. The performance and capabilities of GPUs have been increasingly
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_14
227

228
14
Implementation of Fireworks Algorithm Based on GPU
Fig. 14.1 Floating-point
operations per second for
the CPU and GPU (data
from [3])
Fig. 14.2 Memory
bandwidth for the CPU
and GPU (data from [3])
improved. Today’s GPU is not only a powerful graphics engine but is a highly par-
allel and programmable device that can be used for general purpose computing
application. With its tremendous computational horsepower and very high memory
bandwidth (as illustrated by Figs.14.1 and 14.2), the GPU has become a signiﬁcant
part of modern mainstream, general purpose computing systems [1, 2].
14.1.1 Advantages of GPU Computing
The GPU was initially introduced especially for high-performance image and
graphics processing, where computation-intensive and highly parallel computing
is required. So the GPU is designed with many cores from its inception, thus quite
effective in utilizing parallelism and pipelining. It takes many advantages over both
single- and multicore CPUs [3].
Since the GPU is specialized for computation-intensive, high parallel graphics
rendering, most of the transistors are devoted to data processing rather than data
caching and ﬂow control, as schematically illustrated by Fig.14.3. The GPU is espe-
cially well-suited to address problems with high arithmetic intensity (the ratio of
arithmetic operations to memory operations) where the same program is executed
on many data elements in parallel.

14.1 General Purpose GPU Computing
229
Fig. 14.3 The GPU devotes more transistors to data processing
14.1.2 Development Tools on GPU Hardware
At the very beginning of GPU computing in 2001, general purpose programming on
GPU mainly relied on shading languages (SLs). A shading language is a C-like high
language, which wrappers the low-level graphical rendering operators and is used to
program code to manage the graphic processing.
A general computing problem is mapped to several shaders (a sequence of graph-
ical rendering operations) that process in parallel, while the input and output data
are stored as textures. The programming model of GPU for general purpose can be
illustrated by Fig.14.4.
A shader program operates on a single input element stored in the input registers,
then it writes the extension result into the output registers. And this process is done
in parallel by applying the same operations to all the data.
Among numerous shading languages, OpenGL Shading Language, DirectX High-
Level Shader Language (HLSL), and Cg Programming Language are the most widely
used ones.
Fig. 14.4 Shading language
GPU computing model

230
14
Implementation of Fireworks Algorithm Based on GPU
OpenGL Shading Language (GLSL) is a companion to OpenGL 2.0 and higher
and part of the core OpenGL 4.3 speciﬁcation [4]. It was created to give developers
more direct control of the graphics pipeline without having to use assembly lan-
guage or hardware-speciﬁc languages. Programs written by GLSL are compiled into
OpenGL shader programs and must run through OpenGL APIs.
Analogous to the GLSL used with the OpenGL standard, DirectX HLSL is devel-
oped by Microsoft for cooperating with the Microsoft Direct3D API.
Cg (short for C for Graphics) is another shading language developed by NVIDIA
in close collaboration with Microsoft. It is very similar to GLSL and HLSL, but
unlike the former two shading languages dependent of speciﬁc APIs, it can works
with both DirectX and OpenGL APIs.
While this GPU computing model can leverage the computing power of GPUs
greatly, it faces several drawbacks. First, one needs not only to have a good knowledge
of the problem to be solved, but also to possess intimate knowledge of graphics API
and GPU architecture. Second, problems have to be expressed in terms of vertex
coordinates, textures and shader programs, greatly increasing program complexity.
Third, basic programming features such as random reads and writes to memory are
not supported, which greatly restrict the ﬂexibility of the programming model.
To address these problems, various programming platforms and technologies have
beenintroduced.InsteadofprogrammingGPUswithgraphicalAPIs,theprogrammer
can now write programs in C or other high-level programming languages and target
a general purpose and massively parallel processor.
Of all those developments, OpenCL and CUDA are two of the most prevalent
GPU computing platforms available today.
OpenCL (Open Computing Language) is an open, royalty-free standard for cross-
platform, parallel programming of modern processors. OpenCL is based on C lan-
guage, and designed to enable the development of portable parallel applications for
systems with heterogeneous computing devices consisting of central processing units
(CPUs), graphics processing units (GPUs), DSPs and other processors. So, OpenCL
can be used to give an application access to GPUs for non-graphical computing.
OpenCL implementations already exist on GPUs of AMD and NVIDIA, the two
largest discrete GPU designers.
AMD offers a development toolkit, APP SDK (successor of the ATI Stream),
and several core libraries to simplify the OpenCL programming on its GPU plat-
form. NVIDIA’s CUDA platform can also interact with OpenCL. OpenCL draws
heavily on CUDA in the areas of supporting a single code base for heterogeneous
parallel computing, data parallelism, and complex memory hierarchies. In fact, it
employs a data parallelism model that has direct correspondence with the CUDA
data parallelism model.
Compared to OpenCL, CUDA is a maturer platform for GPUs. In the following,
we will introduce CUDA in detail. Programming with OpenCL is quite similar,
though it is a more complex platform for portability’s sake.

14.1 General Purpose GPU Computing
231
14.1.3 Compute Uniﬁed Device Architecture (CUDA)
Computing uniﬁed device architecture (CUDA), introduced by NVIDIA in Novem-
ber 2006, is a general purpose parallel computing platform and programming model,
which leverages the parallel compute engine in NVIDIA GPUs to solve many com-
plex computational problems in a more efﬁcient way than on the CPU.
CUDA comes with a software environment that allows developers to use C as
a high-level programming language, thus makes it easier for programmers to fully
exploit the parallel feature of GPUs without an explicit familiarity with the GPU
architecture.
14.1.3.1 Kernel
Kernel is a core concept of the CUDA programming model. A kernel is a function
that explicitly speciﬁes data parallel computations to be executed on a device (GPU)
that operates as a co-processor to the host (CPU) running the program. When a kernel
is launched on the GPU, it is executed by a batch of threads.
14.1.3.2 Thread Hierarchy
Threads are organized into independent blocks, and blocks in turn constitute a grid.
Threads can be identiﬁed by a set of intrinsic thread-identiﬁcation variables (e.g.,
threadIdx, blockIdx, blockDim, GridDim). To help with complex addressing based
on the thread, an application can also specify a block as a two or three-dimensional
array of arbitrary size and identify each thread using a 2- or 3-component newIndex
instead. For a two-dimensional block of size Dx × Dy, the thread ID of newIndex
(x, y) is y ∗Dx + x.
14.1.3.3 Memory Model
The memory model of CUDA is tightly related to its thread hierarchy. There are
several kinds of memory spaces on the device:
• Read-write per-thread registers
• Read-write per-thread local memory
• Read-write per-block shared memory
• Read-write per-grid global memory
• Read-only per-grid constant memory
• Read-only per-grid texture memory
CUDA threads may access data from multiple memory spaces during their exe-
cution. Each thread has private registers and local memory. Each thread block has
shared memory visible to all threads of the block. All threads have access to the same

232
14
Implementation of Fireworks Algorithm Based on GPU
global memory. Shared memory has the same lifetime as the block, while the global,
constant, and texture memory spaces are persistent across kernels.
There are also two additional read-only memory spaces accessible by all threads:
the constant and texture memory spaces. They are little relevant to scientiﬁc com-
puting, so we leave them out to interesting readers (refer to [3]).
14.1.3.4 Single-Instruction, Multiple-Thread (SIMT)
A CUDA-enabled GPU can have a scalable array of multithreaded streaming mul-
tiprocessors (SMs), which is roughly equivalent to CPU cores. Each SM can have
certain number of scalar processors (i.e., Streaming Processors, SPs) with respect to
the speciﬁc architecture.
When a CUDA program on the host CPU invokes a kernel grid, all blocks are dis-
tributed equally to the SMs with available execution capacity. The threads of a thread
block execute concurrently on one multiprocessor in the entire execution period as a
unit, and multiple-thread blocks can execute concurrently on one multiprocessor. As
running blocks ﬁnish the execution, inactive blocks are launched on the vacated SMs.
To manage such a large amount of threads, it employs a unique architecture called
single-instruction, multiple-thread (SIMT).
The multiprocessor creates, manages, schedules, and executes threads in groups of
32 parallel threads called warps. Individual threads composing a warp start together
at the same program address, but they have their own instruction address counter and
register state and are therefore free to branch and execute independently.
When a multiprocessor is given one or more thread blocks to execute, it partitions
them into warps and each warp gets scheduled by a warp scheduler for execution.
The way a block is partitioned into warps is always the same; each warp contains
threads of consecutive, increasing thread IDs with the ﬁrst warp containing thread 0.
A warp executes one common instruction at a time, so full efﬁciency is realized
whenall32threadsofawarpagreeontheirexecutionpath.Ifthreadsofawarpdiverge
via a data-dependent conditional branch, the warp serially executes each branch path
taken, disabling threads that are not on that path, and when all paths complete, the
threads converge back to the same execution path. Branch divergence occurs only
within a warp; different warps execute independently regardless of whether they are
executing common or disjoint code paths.
14.2 GPU–FWA
GPU–FWA [5] was proposed for the purpose of achieving the following goals:
• Good quality of solutions. The algorithm can ﬁnd good solutions, compared to the
state-of-the-art algorithms.

14.2 GPU–FWA
233
• Good scalability. As the problem gets complex, the algorithm can scale in a natural
and decent way.
• Ease of implementation and usability, i.e., few control variables to steer the opti-
mization. These variables should also be robust and easy to choose.
To meet these goals, several critical modiﬁcations to the original FWA are adopted
to take beneﬁt of this particular architecture. The pseudocode of GPU–FWA is
depicted by Algorithm 14.1.
Like other swarm intelligence algorithms, GPU–FWA is an iterative algorithm.
In each iteration, every ﬁrework does a local search independently. Then, an
information-exchange mechanism is triggered to utilize the heuristic information
to guide the search process. The mechanism should make a balance between explo-
ration and exploitation.
As the algorithm is self-descriptive, what is left to be made clear is Algorithms
14.2 and 14.3. Below we will explain these two algorithms in detail, respectively.
Algorithm 14.1 GPU–FWA
1: Initialize n ﬁreworks
2: calculate the ﬁtness value of each ﬁreworks
3: calculate Ai according to Eq.2.3
4: while termination condition unsatisﬁed do
5:
for i = 1 to n do
6:
Search according to Algorithm 14.2
7:
end for
8:
Mutate according to Algorithm 14.3
9:
calculate the ﬁtness values of the new ﬁreworks
10:
update Ai according to Eq.2.3
11: end while
Algorithm 14.2 FWA Search
1: for i = 1 to L do
2:
generate m sparks
3:
evaluate the ﬁtness of each spark
4:
ﬁnd the best spark with best ﬁtness value, replace it with the current ﬁrework if it is better.
5: end for
14.2.1 FWA Search
In FWA, each ﬁrework generates certain number of sparks to exploit the nearby
solution space. Fireworks with better ﬁtness values generate more sparks with a
smaller amplitude. This strategy aims to put more computational resources to the
more potential position, thus making a balance between exploration and exploitation.

234
14
Implementation of Fireworks Algorithm Based on GPU
In FWA Search, this strategy is adopted, but in a “greedy” way, i.e., instead of
a global selection procedure in FWA, each ﬁrework is updated by its current best
spark. The mechanism exhibits an enhanced hill-climbing behavior search.
Each ﬁrework generates a ﬁxed number of sparks. The exact number (m) of
sparks is determined in accordance with the speciﬁc GPU hardware architecture. This
ﬁxed encoding of ﬁrework explosion is more suitable for parallel implementation
on the GPUs.
AsaforementionedinSect.14.1.3,withinCUDA-enabledGPU,threadsaresched-
uled by warp, which is nowadays 32 for all the CUDA-enable GPUs. Each warp is
assigned certain number of stream processors (SPs). All threads in the same warp
execute a common instruction at a time on these SPs. For the older generation Tesla
architecture [6], the number is 8, and for Fermi architecture [7] is 16.
To avoid wastage of hardware resource, m should be multiple of number of SMs.
But, it is unnecessary to pick m too large, as greater m is apt to over-exploit a certain
position, while a better reﬁned search can be achieved via running more explosions.
So as a rule of thumb, m should be 16 and 32 on GPUs of the Fermi architecture,
and8or16onpreviousgenerationTeslaarchitecture.Thusthesparksofeachﬁrework
can be generated by treads in a single warp, which, as aforementioned, need not any
extra synchronization overhead.
Also, as can seen from Algorithm 14.2, unlike FWA, in GPU–FWA, the ﬁreworks
do not exchange information in each explosion procedure, and the number of sparks
for each ﬁrework generation is ﬁxed.
Such a conﬁguration takes many advantages.
Algorithm 14.3 Attract-Repulse Mutation
1: Initialize the new location: ˆxi = xi;
2: s = U(1 −δ, 1 + δ);
3: for d = 1 to D do
4:
r = rand(0, 1);
5:
if r < 1
2 then
6:
ˆxi,d = ˆxi,d + (ˆxi,d −xbest,d) · s;
7:
end if
8:
if ˆx j,d > xU B,d or ˆx j,d < xL B,d then
9:
ˆx j,d = xU B,d + |ˆx j,d −xU B,d| mod (xU B,d −xU B,d);
10:
end if
11: end for
First, global communications among ﬁreworks need explicit synchronization,
which implies a considerable overhead. By letting the algorithm to perform a
given number of iterations without exchanging information, the running time can
be reduced greatly.
Second, the number of sparks for each ﬁrework to generate is dynamically deter-
mined, the computation task must be assigned dynamically through the optimization
procedure. As GPUs are inefﬁcient at control operations, the dynamic computation
assignment is apt to harm the overall performance of GPUs. By ﬁxing the sparks

14.2 GPU–FWA
235
number, we can assign each ﬁrework to a warp, this way, all sparks are synchronized
implicitly without extra overhead.
The last but not the least, implemented the explosion in one block of threads, it
can fully utilize the shared memory, thus, once the ﬁrework position and ﬁtness are
loaded from the global memory, no visit to the global memory is needed. The latency
of visiting global memory can be reduced greatly.
14.2.2 Attract-Repulse Mutation
While the heuristic information is used to guide local search, other strategies should
be taken to keep the diversity of the ﬁrework swarm. Keeping a diversity of the swarm
is crucial for the success of optimization procedure.
In FWA, a Gaussian mutation is introduced to increase the diversity of the ﬁrework
swarm. In this mutation procedure, m extra sparks are generated. To generate such a
spark, ﬁrst, a scaling factor g is generated from G(1, 1) (Gaussian distribution with
mean 1 and variance 1). For a randomly selected ﬁrework, the distance between each
corresponding dimension of the ﬁrework and the current best ﬁrework is multiplied
by g. Thus, the new sparks can be closer to the best ﬁrework or further away from it.
Similar to Gaussian mutation, in GPU–FWA, a mechanism called Attract-Repulse
Mutation (AR-Mutation) is proposed to achieve this aim in an explicit way, as illus-
trated by Algorithm 14.3, where xi depicts the ith ﬁrework, while xbest depicts the
ﬁrework with the best ﬁtness.
The philosophy behind AR-Mutation, is that, for non-best ﬁreworks, they are
either attracted by the best ﬁrework to “help” exploit the current best location or
repulsed by the best ﬁrework to explore more space (see Fig.14.5). In fact, the
choice between “attract” and “repulse” reﬂects the balance between exploitation and
exploration.
Despite Gaussian mutation is used in original FWA [8], various random distrib-
ution could be taken certainly. As uniform distribution is most straightforward and
easiest, so uniform distribution is taken in the proposed algorithm.
Fig. 14.5 Schematic
diagram of Attract–Repulse
Mutation

236
14
Implementation of Fireworks Algorithm Based on GPU
To theoretically analyze the AR-Mutation mechanism, the procedure can be sim-
pliﬁed to a 1-order Markov chain. Given, x0 = 1, the next state is generated by
Eq.(14.1)
xt+1 = αt ∗xt,
(14.1)
where αt subjects to uniform distribution between a and b, 0 < a < 1 and b > 1.
Then the tth state can be expressed by the following equation:
xt =
t
i=1
αi · x0,
(14.2)
We can calculate the expected position,
E [xt] = E
 t
i=1
αi

· x0 =
t
i=1
E[αi] · x0 =
t
i=1
E[α] · x0 = At · x0,
(14.3)
where E[α] is the expectation of α.
As can be seen from Eq.(14.3), if the expectation of α, i.e., A, is greater than
1, then x is expected to increase exponentially, otherwise, if A less than 1, x is
expected to decay exponentially. Figure14.6 plots a simulation result, where three
traces subject to U(0.9, 1.11) (A = 1.005), U(0.9, 1.1) (A = 1), and U(0.9, 1.09)
(A = 0.995), respectively. As the simulation showed, even a small disturbance on
A = 1, the results tend to diverge to inﬁnite or converge to 0, exponentially.
As for AR-Mutation, it means that ﬁreworks are either “repulsed” to the bounds
of feasible range or “attracted” to the current best position. Both conditions lead to
prematurity and the loss of diversity.
To make sure that ﬁreworks can “linger” around the search space more steadily,
A should take 1. The distribution should be in the form of s = U(1 −δ, 1 + δ),
where δ ∈(0, 1).
However, as the search range is limited, so δ should be taken with care, though A
is set to 1.
As depicted in Fig.14.7, from left to right, from top to bottom, δ takes 0.9 to 0.1,
respectively. In the simulations, when x > 100, x is truncated to 10. x converges to 0
Fig. 14.6 E[x] under
different values of A

14.2 GPU–FWA
237
Fig. 14.7 Simulation results with different uniform distribution
with diverse speeds. As a tendency, greater δ corresponds to faster convergence, and
vice versa. But what exact convergence speed is most suitable, is task-dependent.
It relies on the landscape of the objective function and how many iterations the
algorithm will run.
14.2.3 Implementation
TheﬂowchartofGPU–FWAimplementationontheplatformofCUDAisasFig.14.8.
Fig. 14.8 The ﬂowchart of the GPU–FWA implementation on CUDA

238
14
Implementation of Fireworks Algorithm Based on GPU
14.2.3.1 Thread Assignment
In the FWA search kernel, each ﬁrework is assigned to a single warp (i.e.32 contin-
uous threads). But, not all the threads in the warp are necessary to be used to execute
computation. If the number of sparks is set to 16, then we use the former half-warp
threads, or if the number is 32, all threads in the warp are used.
Such an implementation brings several advantages. First, since threads in the
same warp are synchronized inherently, they will cut down the overhead of inter-
spark communication. Second, by keeping each ﬁrework and their sparks in the same
warp, the explosion process takes place in a single block, thus the shared memory
can be utilized. As accessing to the shared memory is with much lower latency than
global memory, the overall running time can be greatly reduced. Finally, as GPUs
automatically dispatch block according to the computing and memory resources, it
is easy for the proposed algorithm to extend with the scale of problem.
14.2.3.2 Data Organization
In implementation of GPU–FWA, the position and ﬁtness values of each ﬁrework
are stored in the global memory, while the the data of sparks are stored in the fast-
accessed shared memory. For the purpose of coalescing global memory access [3],
data is usually organized in an interleaving conﬁguration (i.e., structure of arrays) [9,
10], as in Fig.14.9. Here, we take the conventional way ,i.e., the data of the ﬁreworks
and sparks in both global and shared memory are stored in a continuous manner (i.e.,
array of structures, see Fig.14.10). In our implementation, each ﬁrework occupies a
single SM. The threads running on the same SM are up to load the data of a particular
ﬁrework from global memory, and thus data of the same ﬁrework should be stored
continuously. This organization is also simpler and easier to extend with problem
scale than the interleaving pattern.
14.2.3.3 Random Number Generation
Random numbers play an important role in swarm intelligence algorithms. It is
very time-consuming to generating tremendous, high-quality random numbers. The
performance of the optimization heavily relies on the quality of random numbers.
Fig. 14.9 Interleaving storage
Fig. 14.10 Continuous storage

14.2 GPU–FWA
239
(Interested readers can refer to Chap.3 for details). For our implementation, the
efﬁcient CURAND library [11] is used to generate high-quality random numbers
on the GPU.
14.2.4 Empirical Analysis
The performance of GPU–FWA can be studied empirically. We compare GPU–FWA
with both original FWA [8] and standard PSO [12].
14.2.4.1 Experimental Environment
The experiments were conducted on Windows 7 Professional x64 with 4G DDR3
Memory (1333MHz) and Intel core I5-2310 (2.9, 3.1GHz). The GPU used in the
experiments is NVIDIA GeForce GTX 560 Ti with 384 CUDA cores. The CUDA
runtime version is 5.0.
PSO is implemented according to [12] with a ring-topology and FWA according
to [8] with minor modiﬁcation as mentioned in Sect.14.2.
In all simulations, each function was run 20 times independently. For GPU–FWA,
in each running, 1000 iterations were executed. FWA and PSO executed the same
number of function evaluations as GPU–FWA.
For GPU–FWA, the parameters are set as follows: n = 48, L = 30, δ = 0.5.
As in the experimental environment, the GeForce 560 Ti GPU has 12 CUDA cores,
the number of ﬁreworks should be the multiplication of 12 and big enough to avoid
waste of computational power. 48 is adopted for the comparison of precision; when
comparing the speedup, 72, 96, and 144 are also used.
So far, there is no theoretical rules on the criterion of the selection of L and
δ. Some experiments are conducted to predetermine them. L = 30 and δ = 0.5
performed quit well compared to various parameter settings (L = 10, 20, 30, 40, 50,
Table 14.1 Precision comparisons among GPU–FWA, FWA, and PSO
Fun
GPU–FWA
FWA
PSO
Avg.
Std.
Avg.
Std.
Avg.
Std.
f1
1.31E–09
1.85E–09
7.41E+00
1.98E+01
3.81E–08
7.42E–07
f2
1.49E–07
6.04E–07
9.91E+01
2.01E+02
3.52E–11
1.15E–10
f3
3.46E+00
6.75E+01
3.63E+02
7.98E+02
2.34E+04
1.84E+04
f4
1.92E+01
3.03E+00
4.01E+02
5.80E+02
1.31E+02
8.68E+02
f5
7.02E+00
1.36E+01
2.93E+01
2.92E+00
3.16E+02
1.11E+02
f6
−8.09E+03 2.89E+03
–1.03E+04
3.77E+03
−6.49E+03 9.96E+03
f7
1.33E+00
1.78E+01
7.29E–01
1.24E+00
1.10E+00
1.18E+00
f8
3.63E–02
7.06E−01
7.48E+00
7.12E+00
1.83E+00
1.26E+01

240
14
Implementation of Fireworks Algorithm Based on GPU
Table 14.2
p-values of t-test
f1
f2
f3
f4
f5
f6
f7
f8
GPU–FWA versus FWA
1.00E–06
0.00E+00
0.00E+00
0.00E+00
0.00E+00
0.00E+00
5.16E–01
0.00E+00
GPU–FWA versus PSO
3.46E–01
1.21E–04
0.00E+00
2.15E–02
0.00E+00
6.50E–03
8.03E–01
1.21E–02

14.2 GPU–FWA
241
and δ = 0.1 · · · 0.9, as the limit of space, the results are omitted here). The total
function evaluation time was 48 ∗16 ∗1000 = 768,000.
For a fair comparison, all of the three algorithms were tested under the same scale.
Here, by saying scale, we mean that the number of function evaluations that can be
executed in parallel. For GPU–FWA, the scale in this experiment is 768, so PSO’s
swarm size is set as the same number. For FWA, as the ﬁrework number takes 64,
and total spark number is 640 and number of gaussian sparks is 64.
14.2.4.2 Quality of Solutions
Sphere,Hyper-ellipsoid,Schwefel1.2,Rosenbrock,Rastrigin,Schwefel,Griewangk,
and Ackley functions ( f 1 ∼f 8) were used as benchmark, see Appendix A.3 for
the detailed conﬁgurations. The ﬁrst three functions are unimodal functions, while
others are multimodal functions.
All benchmark functions were optimized in 20 independent trails, and the average
results and corresponding standard deviations are as Table14.1.
Under the signiﬁcance level of 0.01 (observe Table14.2), it can be seen that
GPU–FWA outperforms FWA on f 1 ∼f 6 and f 8, it only lost to FWA on f 7. PSO
outperforms GPU–FWA on unimodal function f 2, but fail to GPU–FWA on another
unimodal function f 3. GPU–FWA can get better results on multimodal functions
f 4, f 5, f 6, f 8. In general, as far as the benchmark functions are concerned, we can
see that GPU–FWA performs better than FWA and PSO.
14.2.4.3 Speedup Versus Swarm Size
Besides the precision of the solutions, speedup efﬁciency is another critical factor
that has to be considered.
In order to observe the speedups GPU–FWA achieves in comparison with PSO
and FWA, a series of experiments were conducted, where n is set respectively to
48, 72, 96, 144 for GPU–FWA. 1000 iterations are run, and the same function eval-
uation time under the same scale for PSO and FWA.
The running time (in seconds) and speedup with respect to Rosenbrock function
is illustrated by Table14.3. Figures14.11 and 14.12 depict the speedup of all the
eight benchmark functions with respect to the swarm size.
Table 14.3 Running time and speedup of Rosenbrock
n
FWA(s)
PSO(s)
GPU–FWA(s)
SU(FWA)
SU(PSO)
48
36.420
84.615
0.615
59.2
137.6
72
55.260
78.225
0.624
88.6
125.4
96
65.595
103.485
0.722
90.8
143.3
144
100.005
155.400
0.831
120.3
187.0

242
14
Implementation of Fireworks Algorithm Based on GPU
Fig. 14.11 Speedup versus FWA
Fig. 14.12 Speedup versus PSO
GPU–FWA achieved a speedup as high as 180× with the scale of less than 200,
in the meantime, the up-to-date GPU accelerated PSO achieve 200× fold speedup
with the scale high up to 10,000 [10]. Thus GPU–FWA are more scalable than the
conventional GPU-based PSO.
For extensive and deep analysis of swarm intelligence algorithms (SIAs) on GPU,
please refer to [13] for further readings.

14.3 Summary
243
14.3 Summary
To take beneﬁt of GPUs, GPU–FWA is proposed for optimization. GPU–FWA can
fully leverage the great computing power of the GPU architecture, making it very
well to parallel computation. It does not need special complicated data structures,
thus making it easy to implement. As the problem scale goes great, it can extend in
an easy and natural way. The new method requires few control variables, thus it is
robust as well as easy to use.
Tested on suite of benchmark functions, it is demonstrated that the GPU–FWA
outperforms FWA and the popular PSO in the quality of solution. Experimental
results obtained a speedup up to 160× and 200× compared to CPU-based FWA and
PSO, respectively, on an up-to-date CPU .
It can be concluded that GPU–FWA is a potential powerful tool for solving large-
scale optimization problems on the massively parallel architecture.
References
1. J.D. Owens, M. Houston, D. Luebke, S. Green, J.E. Stone, J.C. Phillips, GPU computing. Proc.
IEEE 96(5), 879–899 (2008). ISSN: 0018-9219. doi:10.1109/JPROC.2008.917757
2. S. Stankovi, J. Astola, in GPU Computing with Applications in Digital Logic. Tampere Interna-
tional Center for Signal Processing. ed. by J. Astola, M. Kameyama, M. Lukac, R.S. Stankovi.
Chap. An overview of miscellaneous applications of GPU computing (2012), pp. 191–215.
ISBN: 978-952-15-2920-7
3. NVIDIA Corp., CUDA C Programming Guide, July 2013
4. M. Segal, K. Akeley, The OpenGL Graphics System: A Speciﬁcation (Version 4.4). The
Khronos Group Inc., July 2013
5. K. Ding, S. Zheng, Y. Tan, A GPU-based parallel ﬁreworks algorithm for optimization, in
Proceeding of the Fifteenth Annual Conference on Genetic and Evolutionary Computation
Conference. GECCO (ACM, Amsterdam, 2013), pp. 9–16. ISBN: 978-1-4503-1963-8. doi:10.
1145/2463372.2463377
6. NVIDIA Corp., NVIDIA GeForce 8800 GPU Architecture Overview. Technical report (2006)
7. NVIDIA Corp., NVIDIA’s Next Generation CUDATM (2009)
8. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
9. Y. Zhou, Y. Tan, GPU-based parallel multi-objective particle swarm optimization. Int. J. Artif.
Intell. 7(A11), 125–141 (2011)
10. V. Roberge, M. Tarbouchi, Parallel particle swarm optimization on graphical processing unit
for pose estimation. WSEAS Trans. Comput. 11(6), 170–179 (2012)
11. NVIDIA Corp., CURAND Library Programming Guide v5.5, July 2013
12. D. Bratton, J. Kennedy, Deﬁning a standard for particle swarm optimization, in Swarm Intel-
ligence Symposium, SIS (IEEE, Honolulu, 2007) pp. 120–127
13. Y. Tan, K. Ding, A survey on GPU-based implementation of swarm intelligence algorithms.
IEEE Trans. Cybern. 45(12), 1–14 (2015)

Part IV
Applications
Fireworks algorithm can be applied to many real-life applications that require to
solve optimization algorithms. These applications can be generally transferred
to single objective or multiple objective optimization problems, and thus ﬁre-
works algorithm can be applied to solve these problems easily and directly. In
Chaps. 15–17, we verify how ﬁreworks algorithms can be applied to various
applications in different areas. These applications include related pattern recog-
nition problems (nonnegative matrix factorization, document clustering, spam
detection, and image recognition), complex model estimation problem (seismic
inversion), and emerging swarm robotics searching problem. These applications
sit in areas that differ greatly from each other and have different requirements for
optimization algorithms. The ﬁreworks algorithm can solve these problems
successfully, which illustrates that FWA has great adaptiveness to different
requirements in real-world applications.

Chapter 15
FWA Application on Non-negative Matrix
Factorization
This chapter presents the use of swarm intelligence algorithms for non-negative
matrix factorization (NMF) [1]. The NMF is a special low-rank approximation which
allows for an additive parts-based and interpretable representation of the data. Here,
we present our efforts to improve the convergence and approximation quality of NMF
usingﬁvedifferentmeta-heuristicsbasedonswarmintelligence.Severalpropertiesof
the NMF objective function motivate the utilization of meta-heuristics: this function
is non-convex, discontinuous, and may possess many local minima. The proposed
optimization strategies are twofold: On one hand, we present a new initialization
strategy for NMF in order to initialize the NMF factors prior to the factorization; on
the other hand, we present an iterative update strategy which improves the accuracy
per runtime for the multiplicative update NMF algorithm.
15.1 Introduction
Low-rank approximations are utilized in several content based retrieval and data
miningapplications,suchastextandmultimediamining,websearch,etc.andachieve
a more compact representation of the data with only limited loss in information. They
reduce storage and runtime requirements, and also reduce redundancy and noise in
the data representation while capturing the essential associations. The NMF leads
to a low-rank approximation which satisﬁes non-negativity constraints [2]. NMF
approximates a data matrix A by A ≈W H, where W, H and A are the NMF factors.
NMF requires all entries in W, H and A to be zero or positive. Contrary to other
low-rank approximations such as the singular value decomposition (SVD), these
constraints force NMF to produce so-called additive parts-based representations.
This is an impressive beneﬁt of NMF, since it makes the interpretation of the NMF
factors much easier than for factors containing positive and negative entries [3, 4].
The NMF is usually not unique if different initializations of the factors W and
H are used. Moreover, there are several different NMF algorithms which all follow
different strategies (e.g., mean squared error, least squares, gradient descent, ...)
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_15
247

248
15
FWA Application on Non-negative Matrix Factorization
and produce different results. Mathematically, the goal of NMF is to ﬁnd a (ideally
the best) solution of an optimization problem with bound constraints in the form
minx∈ f , where f is the nonlinear objective function of NMF and is usually not
convex, discontinuous, and may possess many local minima [5].
Since meta-heuristic optimization algorithms are known to be able to deal well
with such difﬁculties they seem to be a promising choice for improving the quality of
NMF. Over the last decades nature-inspired meta-heuristics, including those based
on swarm intelligence, have gained much popularity due to their applicability for
various optimization problems [6]. They beneﬁt from the fact that they are able to ﬁnd
acceptable results within a reasonable amount of time for many complex, large, and
dynamic problems. Although they lack the ability to guarantee the optimal solution
for a given problem (comparably to NMF), it has been shown that they are able
to tackle various kinds of real-world optimization problems [7]. Meta-heuristics as
well as the principles of NMF are in accordance with the law of sufﬁciency [8]: If a
solution to a problem is good, fast, and cheap enough, then it is sufﬁcient.
In this chapter, we present two different strategies for improving the NMF using
ﬁve optimization algorithms based on swarm intelligence and evolutionary comput-
ing: particle swarm optimization (PSO), genetic algorithms (GA), ﬁsh school Search
(FSS), differential evolution (DE), and ﬁreworks algorithm (FWA). All algorithms
are population based and can be categorized into the ﬁelds of swarm intelligence
(PSO, FSS, FWA), evolutionary algorithms (GA), and a combination thereof (DE).
The goal is to ﬁnd a solution with smaller overall error at convergence. The
concepts of the two optimization strategies are the following: In the ﬁrst strategy,
meta-heuristics are used to initialize the factors for minimizing the NMF objective
function prior to the factorization. The second strategy aims at iteratively improving
the approximation quality of NMF during the ﬁrst iterations.
15.1.1 NMF Research History
The work by Lee and Seung [2] is known as a standard reference for NMF. The
original multiplicative update (MU) algorithm introduced in this article provides a
good baseline than other algorithms (e.g., the alternating least squares algorithm [9],
the gradient descent algorithm [10], ALSPGRAD [10], quasi Newton-type NMF
[11], fastNMF, bayesNMF [12], etc. While the MU algorithm is still the fastest NMF
algorithm per iteration and a good choice when a very fast and rough approximation
is needed, ALSPGRAD, fastNMF, and bayesNMF have shown to achieve a better
approximation at convergence compared to many other NMF algorithms [13].
15.1.1.1 NMF Initialization
Only few algorithms for non-random NMF initialization have been published. Wild
et al. [14] used spherical-means clustering to group column vectors of as input. A

15.1 Introduction
249
similar technique was used in [15]. Another clustering-based method of structured
initialization designed to ﬁnd spatially localized basis images can be found in [11].
Boutsidis and Gallopoulos [16] used an initialization technique based on two SVD
processes called non-negative double singular value decomposition (NNDSVD).
Experiments indicate that this method has advantages over the centroid initialization
in [14] in terms of faster convergence.
15.1.1.2 NMF and Meta-Heuristics
So far, only few studies can be found that aim at combining NMF and meta-heuristics,
most of them are based on genetic algorithms (GAs). In [5], the authors have inves-
tigated the application of GAs on sparse NMF for microarray analysis, while [17]
have applied GAs for boolean matrix factorization, a variant of NMF for binary
data based on Boolean algebra. However, the methods presented in these studies are
barely connected to the techniques presented in this article. In two preceding studies
[18, 19], we have introduced the basic concepts of the proposed update strategies.
In this chapter, we extend our preliminary work in several ways by the following
new contributions. We evaluate our methods on synthetic data which allows us to
evaluate the proposed methods.
15.2 Low-Rank Approximations
Given a data matrix A ∈Rm×n whose n columns represent instances and whose m
rows contain the values of a certain feature for the instances, most low-rank approx-
imations reduce the dimensionality by representing the original data as accurate as
possible with linear combinations of the original instances and/or features. Mathe-
matically, A is replaced with another matrix Ak with usually much smaller rank. In
general, a closer approximation means a better factorization. However, it is highly
likely that in some applications speciﬁc factorizations might be more desirable com-
pared to other solutions.
The most important low-rank approximation techniques are the singular value
decomposition (SVD, [20]) and the closely related principal component analysis
(PCA, [21]). Traditionally, the PCA uses the eigenvalue decomposition to ﬁnd eigen-
values and eigenvectors of the covariance matrix Cov(A) of A. Then the original
data matrix can be approximated by Ak := AQk, with [Qk = [q1, . . . , qk]], where
q1, . . . , qk are the ﬁrst k eigenvectors of Cov(A). The SVD decomposes A into a
product of three matrices such that A = U  V T , where  contains the singular
values along the diagonal, and U and V are the singular vectors. The reduced rank
SVD to A can be found by setting all but the ﬁrst largest singular values equal to zero
and using only the ﬁrst k columns of U and V , such that Ak := Uk

k V T
k . Other
well-known low-rank approximation techniques comprise factor analysis, indepen-

250
15
FWA Application on Non-negative Matrix Factorization
dent components analysis, multidimensional scaling such as Fastmap or ISOMAP,
or locally linear embedding (LLE), which are all summarized in [22].
Among all possible rank k approximations, the approximation Ak calculated by
SVD and PCA is the best approximation in the sense that ||A −Ak||F is as small as
possible [23]. In other words, SVD and PCA give the closest rank k approximation of
a matrix, such that ||A−Ak||F ≤||A−Bk||F, where Bk is any matrix of rank k, and
||.||F is the Frobenius norm, which is deﬁned as ( |ai j|2)1/2 = ||A||F. However,
the main drawback of PCA and SVD refers to the interpretability of the transformed
features. The resulting orthogonal matrix factors generated by the approximation
usually do not allow for direct interpretations in terms of the original features because
they contain positive and negative coefﬁcients [24]. In many application domains,
a negative quantiﬁcation of features is meaningless and the information about how
muchanoriginalfeaturecontributesinalow-rankapproximationislost.Thepresence
of negative, meaningless components or factors may inﬂuence the entire result. This
is especially important for applications where the original data matrix contains only
positive entries, e.g., in text-mining applications, image classiﬁcation, etc. If the
factor matrices of the low-rank approximation were constrained to contain only
positive or zero values, the original meaning of the data could be preserved better.
15.2.1 Non-negative Matrix Factorization (NMF)
The NMF leads to special low-rank approximations which satisfy these non-
negativity constraints. NMF requires that all entries in A, W and H are zero or
positive. This makes the interpretation of the NMF factors much easier and enables
NMF a non-subtractive combination of parts to form a whole [2]. The NMF con-
sists of reduced rank non-negative factors W and with H with k ≪min{m, n} that
approximate a matrix A by W H where the approximation W H has rank at most k.
The nonlinear optimization problem underlying NMF can generally be stated as
minW,H f (W, H) = minW,H
1
2||A −W H||2
F
(15.1)
The Frobenius norm ||.||F is commonly used to measure the error between the
original data A and the approximation W H but other measures such as the Kullback–
Leibler divergence are also possible [2]. The error between A and W H is usually
stored in a distance matrix D = A −W H (cf. Fig.15.2). Unlike the SVD, the NMF
is not unique, and convergence is not guaranteed for all NMF algorithms. If they
converge, then usually to local minima only (potentially different ones for different
algorithms). Nevertheless, the data compression achieved with only local minima
has been shown to be of desirable quality for many data mining applications [25].
Moreover, for some speciﬁc problem settings a smaller residual D = A −W H (a
smaller error) may not necessarily improve the solution of the actual application (e.g.,
classiﬁcation task) compared to a rather coarse approximation. However, as analyzed

15.2 Low-Rank Approximations
251
Fig. 15.1 Scheme of very coarse NMF approximation with very low-rank k. Although k is signif-
icantly smaller than m and n, the typical structure of the original data matrix can be retained (note
the three different groups of data objects in the left, middle, and right part of A)
in (Janecek and Gansterer 2010) a closer NMF approximation leads to qualitatively
better classiﬁcation results and turns out to achieve signiﬁcantly more stable results
(Fig.15.1).
15.2.1.1 NMF Initialization
Algorithms for computing NMF are iterative and require initialization of the factors
and NMF unavoidably converges to local minima, probably different ones for dif-
ferent initialization [16]. Hence, random initialization makes the experiments unre-
peatable. A proper non-random initialization can lead to faster error reduction and
better overall error at convergence. Moreover, it makes the experiments repeatable.
Although the beneﬁts of good NMF initialization techniques are well known in the
literature, most studies use random initialization [16]. Since some initialization pro-

252
15
FWA Application on Non-negative Matrix Factorization
cedures can be rather costly in terms of runtime the trade-off between computational
cost in the initialization step and the computational cost of the actual NMF algorithm
need to be balanced carefully. In some situations, an expensive preprocessing step
may overwhelm the cost savings in the subsequent NMF update steps.
15.2.1.2 General Structure of NMF
In the basic form of NMF (see Algorithm 15.1), W and H are initialized randomly
and the whole algorithm is repeated several times (maxrepetition). In each repetition,
NMF update steps are processed until a maximum number of iterations is reached
(maxiter). These update steps are algorithm speciﬁc and differ from one NMF variant
to the other. Termination criteria: If the approximation error drops below a predeﬁned
threshold, or if the shift between two iterations is very small, the algorithm might
stop before all iterations are processed.
Algorithm 15.1 General structure of NMF algorithms.
1: given matrix A ∈Rm×n and k ≪min{m, n}
2: for rep = 1 to max Reptition do
3:
W = rand(m, k)
4:
H = rand(k, n)
5:
for i = 1 to max Iter do
6:
perform algorithm speciﬁc NMF update steps
7:
check termination criterion
8:
end for
9: end for
15.2.1.3 Multiplicative Update (MU) Algorithm
To give an example of the update steps for a speciﬁc NMF algorithm, we provide the
update steps for the MU algorithm in Algorithm 15.2. MU is one of the two original
NMF algorithms presented in [2] and still one of the fastest NMF algorithms per
iteration. The update steps are based on the mean squared error objective function
and consist of multiplying the current factors by a measure of the quality of the current
approximation. The divisions in Algorithm 15.2 are to be performed element-wise.
ε is used to avoid division by zero.
Algorithm 15.2 Update steps of the multiplicative update algorithm.
1: H = H. ∗(W T A)./(W T W H + ε)
2: W = W. ∗(AH T )./(W H H T + ε)

15.3 Swarm Intelligence Optimization
253
15.3 Swarm Intelligence Optimization
Optimization techniques inspired by swarm intelligence (SI) have become increas-
ingly popular and beneﬁt from their robustness and exibility [7]. Swarm intelligence
is characterized by a decentralized design paradigm that mimics the behavior of
swarms of social insects, ﬂocks of birds, or schools of ﬁsh. Optimization techniques
inspired by swarm intelligence have shown to be able to successfully deal with
increasingly complex problems [6]. In this article, we use ﬁve different optimization
algorithms. Particle swarm optimization (PSO, [26]) is a classical swarm intelligence
algorithm, while ﬁsh school search (FSS, [27]) and ﬁreworks algorithm (FWA, [28])
are two recently developed swarm intelligence methods. These three algorithms are
compared to genetic algorithm (GA, [29]), classical evolutionary algorithm, and dif-
ferential evolution (DE, [30]), which shares some features with swarm intelligence
but can also be considered as an evolutionary algorithm. For simplicity, the following
only gives the details of FWA, for PSO, GA, DE, and FSS, the interested reader is
referred to the references given above.
Algorithm 15.3 Pseudo code of the Fireworks Algorithm
1: Randomly initialize locations (xi) of n ﬁreworks
2: Repeat
3: Set off n ﬁreworks respectively at the n locations;
4: Calculate number ˆsi and location of sparks for each xi;
5: Generate ˆm speciﬁc sparks, each for a randomly selected ﬁreworks;
6: Keep best location and select n −1 locations for next iteration;
7: Until termination (time, max. number of ﬁtness evals., convergence,...)
The Fireworks Algorithm (Algorithm 15.3) is a novel swarm intelligence algo-
rithm that is inspired by observing ﬁreworks explosion. Two different types of explo-
sion (search) processes are used in order to ensure diversity of resulting sparks, which
are similar to particles in PSO or ﬁsh in FSS.
15.4 Improving NMF with Swarm Intelligence Optimization
Before describing our two optimization strategies for NMF based on swarm intelli-
gence, we discuss some properties of the Frobenius norm [23]. We use the Frobenius
norm (1.1) as NMF objective function (i.e., to measure the error between A and W H)
because it offers some properties that are beneﬁcial for combining NMF and opti-
mization algorithms. The following statements about the Frobenius norm are valid
for any real matrix. However, in the following we assume that refers to a distance
matrix storing the distance (error of the approximation) between the original data

254
15
FWA Application on Non-negative Matrix Factorization
and the approximation, D = A −W H. The Frobenius norm of a matrix D ∈Rm×n
is deﬁned as
||D||F =
⎛
⎝
min(m,n)

i=1
σi
⎞
⎠
1/2
=
⎛
⎝
m

i=1
n

j=1
d2
i j
⎞
⎠
1/2
(15.2)
The Frobenius norm can also be computed row-wise or column-wise. The row-wise
calculation is
||D||RW
F
=
 m

i=1
|dr
i |2
	1/2
(15.3)
where |dr
i | is the norm of the ith row vector of D, i.e., |dr
i | = (n
j=1 |ri
j|2)1/2, and
ri
j is the jth element in row i. The column-wise calculation is
||D||CW
F
=
⎛
⎝
n

j=1
|dc
j|2
⎞
⎠
1/2
(15.4)
with |dc
j| being the norm of the jth column of D, i.e., |dc
j| = (m
i=1 |c j
i |2)1/2, and
c j
i being the ith element in column j. Obviously, a reduction of the Frobenius norm
of any row or any column of leads to a reduction of the total Frobenius norm DF.
In the following, we exploit these properties of the Frobenius norm for the pro-
posed NMF optimization strategies. While strategy 1 aims at ﬁnding heuristically
optimal starting points for the NMF factors, strategy 2 aims at iteratively improv-
ing the quality of NMF during the ﬁrst iterations. All meta-heuristics mentioned
in Sect.15.3 can be used within both strategies. Before discussing the optimization
strategies, we illustrate the basic optimization procedure for a speciﬁc row (row l)
of W in Fig.15.2. This procedure is similar for both optimization strategies.
15.4.1 Parameters
Global parameters used for all optimization algorithms are upper/lower bound of
the search space and the initialization, the number of particles (chromosomes, ﬁsh,
...), and maximum number of ﬁtness evaluations. Parameter settings are discussed in
Sect.15.5. For all meta-heuristics, the problem dimension is equal to the rank k of
the NMF. That is, if, for example, k = 10, a row/column vector with 10 continuous
entries is returned by the optimization algorithms.

15.4 Improving NMF with Swarm Intelligence Optimization
255
Fig. 15.2 Illustration of the optimization process for row l of the NMF factor W. The lth row of
A (ar
l ) and all columns of H0 are the input for the optimization algorithms. The output is a row
vector wr
l (the lth row of W) which minimizes the norm of dr
l , the lth row of the distance matrix
D. The norm of dr
l is the ﬁtness function for the optimization algorithms (minimization problem)
15.4.2 Optimization Strategy 1 for Initialization
The goal of this optimization strategy is to ﬁnd heuristically optimal starting points
for the rows of W and the columns of H, respectively, i.e., prior to the factorization
process. Algorithm 15.4 shows the pseudocode for the initialization procedure. In the
beginning, H0 needs to be initialized randomly using a non-negative lower bound
(preferably 0) for the initialization. In the ﬁrst loop, W is initialized row-wise, i.e.,
row wr
i is optimized in order to minimize the Frobenius norm of the ith row dr
i
of D, which is deﬁned as dr
i = ar
i −wr
i H0. Since the optimization of any row
of W is independent to the optimization of any other row of W, all wr
i can be
optimized concurrently. In the second loop, the columns of H are initialized using
the previously computed and already optimized rows of W, which need to be gathered
beforehand (in line 7 of the algorithm). H is initialized column-wise, i.e., column
hc
j is optimized in order to minimize the Frobenius norm of the jth column dc
j of D,
which is deﬁned as dc
j = ac
j −Whc
j. The optimization of the columns of H can be
performed concurrently as well.

256
15
FWA Application on Non-negative Matrix Factorization
Algorithm 15.4 Pseudocode for the initialization procedure for NMF factors W and
H. The two for-loops in lines 4 and 10 can be executed concurrently. SIO = Swarm
Intelligence Optimization.
1: given matrix A ∈Rm×n and k ≪min{m, n}
2: H0 = rand(k, n)
3: for i = 1 to m do
4:
Use SIO to ﬁnd wr
i that minimizes ||ar
i −wr
i H||F,(min||.||F of row i of D)
5: end for
6: W = [wr
1; ...; wr
m];
7: for i = 1 to n do
8:
Use SIO to ﬁnd hc
i that minimizes ||ac
j −Whc
j||F,(min||.||F of col j of D)
9: end for
10: H = [hc
1; ...; hc
n];
15.4.3 Optimization Strategy 2 for Iterative Optimization
The second optimization strategy aims at iteratively optimizing the NMF factors
W and H during the ﬁrst iterations of the NMF. Compared to the ﬁrst strategy not
all rows of W and all columns of H are optimized instead the optimization is only
performed on selected rows/columns. In order to improve the approximation as fast
as possible we identify rows of D with highest norm (the approximation of this
row is worse than for other rows of D) and optimize the corresponding rows of W.
The same procedure is used to identify the columns of H, that should be optimized.
Our experiments showed that not all NMF algorithms are suited for this iterative
optimization procedure. For many NMF algorithms there was no improvement with
respect to the convergence or a reduction of the overall error after a ﬁxed number
of iterations. However, for the multiplicative update (MU) algorithm which is one
of the most widely used NMF algorithms this strategy is able to improve the quality
of the factorization. Hence, Algorithm 15.5 shows the pseudocode for the iterative
optimizationoftheNMFfactorsduringtheﬁrstiterationsusingtheupdatestepsofthe
MU algorithm. It can be seen that this update strategy is able to signiﬁcantly reduce
the approximation error per iteration for the MU algorithm. Due to the relatively
high computational cost of the meta-heuristics, the optimization procedure is only
applied in the ﬁrst m iterations and only on c selected rows/columns of the NMF
factors. Similar to strategy, the optimization of all rows of W are independent from
each other (identical for columns of H), which allows for a parallel implementation
of the proposed method. In the following, we describe the variables and functions
(for updating rows of W) of Algorithm 15.5. Updating columns of H is similar to
updating the rows of W.
• m: the number of iterations in which the optimization using meta-heuristics is
applied
• c: the number of rows and/or columns that are optimized in the current iteration.
• △c: the value of c is decreased by △c in each iteration.

15.4 Improving NMF with Swarm Intelligence Optimization
257
• [Val, I XW] = sort(norm(dr
i ),′ descend′): returns the values Val and the cor-
responding indices (I XW) of the norm of all row vertors dr
i of D in descending
order.
• I XW = I XW(1 : c): returns only the ﬁrst c elements of the vector I XW.
• minimize ||ar
I −wr
i H||F.
Algorithm 15.5 Pseudocode for the iterative optimization for the multiplicative
update algorithm. SIO = swarm intelligence optimization. The methods used in this
algorithm are explained below.
1: for iter = 1 to maxiter do
2:
H = H. ∗(W T A)./(W T W H + ε)
3:
W = W. ∗(AH T )./(W H H T + ε)
4:
if iter < m then
5:
dr
i is the ith row vector of D;
6:
[Val, I XW ] = sort(norm(dr
i ),′ descend′)
7:
I XW = I XW(1 : c)
8:
∀i ∈I XW
9:
Use SIO to ﬁnd wr
i that minimizes ||ar
i −wr
i H||F;
10:
11:
W = [wr
1; ...; wr
m];
12:
dc
j is the jth column vector of D;
13:
[Val, I X H] = sort(norm(dc
j ),′ descend′)
14:
I X H = I X H(1 : c)
15:
∀i ∈I X H
16:
Use SIO to ﬁnd hc
i that minimizes ||ac
j −Whc
j||F.
17:
H = [hc
1; ...; hc
n];
18:
c = c −△c;
19:
end if
20: end for
15.5 Experiment Setup
15.5.1 Software
All softwares are written in Matlab. We used only publicly available NMF implemen-
tations: multiplicative update (MU, Matlab’s statistics toolbox since v6.2, nnmf()).
ALS using projected gradient (ALSPG, [10]), BayesNMF and FastNMF [12]. Mat-
lab code for NNDSVD (Sect.15.1.1) is also publicly available [16]. Codes for PSO
and DE were adapted from [31], and code for GA from the appendix of [29]. For
FWA, we used the same implementation as in the introductory paper [28], and FSS
was self-implemented following the algorithm provided in [27].

258
15
FWA Application on Non-negative Matrix Factorization
15.5.2 Hardware
AllexperimentswereperformedonaSUNFIREX4600M2witheightAMDOpteron
quad-core processors (32 cores overall) with 3.2 GHz, 2MB L3 cache, and 32GB of
main memory (DDR-II 666).
15.5.3 Parameter Setup
The dimension of the optimization problem is always identical to the rank of the NMF
(cf. Sect.15.4). The upper/lower bound of the search space was set to the interval
[0, (4 ∗max(A))] and upper/lower bound of the initialization to [0, max(A)]. In
order to achieve fair results which are not biased due to excessive parameter tuning,
we used the same parameter settings for all data sets. These parameter settings were
found by running a self-written benchmark program that tested several parameter
combinations on randomly generated data. For some optimization strategies (PSO,
FSS, and FWA), the recommended parameter settings from the literature worked ﬁne.
However, for GA and DE the parameter settings that were used in most studies in the
literature did not perform very well. For GA, we found that a very aggressive (high)
mutation rate highly improved the results. For DE, we observed a similar behavior
and found that the maximum crossover probability (1) achieved the best results. For
all experiments in this chaper, the following parameter settings were used:
1. GA: mutation rate of 0.5; selection rate of 0.65
2. PSO: Gbest topology, w = 0.8c1 = c2 = 2.05
3. DE: crossover probability (pc) set to upper limit 1
4. FSS: stepind_intial = 1,stepind_ f inal = 0.001Wscale = 10
5. FWA: number of sparks number is set to 10
15.5.4 Data Sets
We used the data set DS-RAND, which is a randomly created, fully dense 100×100
matrix in order to provide unbiased results.
15.5.5 Evaluation of Optimization Strategy 1
15.5.5.1 Initialization
Before evaluating the improvement of the NMF approximation quality as such, we
ﬁrst measure the initial error after initializing W and H (before running the NMF

15.5 Experiment Setup
259
0
500
1000
1500
2000
2500
0.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
  evaluation times
mean(||ar
i − wr
i H0||F)
Initialize rows of W based on H0
FWA
PSO
DE
GA
FSS
0
500
1000
1500
2000
2500
0.054
0.056
0.058
0.06
0.062
0.064
0.066
0.068
0.07
  evaluation times
mean(||ac
j  − W hc
j ||F)
Initialize columns of H based on W
FWA
FSS
GA
DE
PSO
Fig. 15.3 Left-hand side average approximation error per row (after initializing rows of W). Right-
hand side average approximation error per column (after initializing of H). NMF rank k = 5.
Legends are ordered according to approximation error (top worst, bottom best)
0
500
1000
1500
2000
2500
0
1
2
3
4
5
6
  evaluation times
mean(||ar
i − wr
i H0||F)
Initialize rows of W based on H0
GA
FSS
DE
FWA
PSO
0
500
1000
1500
2000
2500
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.11
  evaluation times
mean(||ac
j  − W hc
j ||F)
Initialize columns of H based on W
DE
FWA
PSO
GA
FSS
Fig. 15.4 Similar information as for Fig.15.3, but for NMF rank k = 30
algorithm). Figures15.3 and 15.4 show the average approximation error (i.e., Frobe-
nius norm / ﬁtness) per row (left) and per column (right) for data set DS-RAND.
The ﬁgures on the left side show the average (mean) approximation error per row
after initializing the rows of W (ﬁrst loop in Algorithm 15.4). The ﬁgures on the right
side show the average (mean) approximation error per column after initializing the
columns of H (second loop in Algorithm 15.4). The legends are ordered according
to the average approximation error achieved after the maximum number of function
evaluations for each ﬁgure (top = worst, bottom = best). When the NMF rank k
is small (see Fig.15.3, k = 5) all optimization algorithms except FWA achieve
similar results. Except FWA, all optimization algorithms quickly converge to a good
result. With increasing complexity (i.e., increasing rank k) FWA clearly improves
its results, as shown in Fig.15.4. The gap between the optimization algorithms is
much bigger for larger rank k. Note that GA needs more than 2000 evaluations to
achieve a low approximation error for initializing the rows of W. When initializing
the columns of H, PSO, and GA suffer from their high approximation error during

260
15
FWA Application on Non-negative Matrix Factorization
0
5
10
15
20
25
30
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Iteration time
||A−WH||F
MU
FWA
PS
DE
GA
FS
0
5
10
15
20
25
30
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Iteration time
||A−WH||F
MU
FWA
PS
DE
GA
FS
(a)
(b)
Fig. 15.5
Accuracy per Iteration when updating only the row of W, m = 2, c = 20
the ﬁrst iterations, which is caused by the relatively sparse factor matrix W for PSO
and GA. Although PSO is able to reduce the approximation error signiﬁcantly during
the ﬁrst 500 iterations, FSS and GA achieve slightly better ﬁnal results. Generally,
FSS achieves the best approximation accuracy after the initialization procedure for
large k. However, as shown later, the initial approximation error is not necessarily
an indicator for the approximation quality of NMF or the resulting classiﬁcation
accuracy.
15.5.6 Evaluation of Optimization Strategy 2
Figure15.5 shows the convergence curves for the NMF approximation using opti-
mization strategy 2 for different values of rank k (data set DS-RAND). Due to the
relatively high computational cost of the meta-heuristics we applied our optimization
procedure here only on the rows of W, while the columns in H remained unchanged.
Experiments showed that with this setting the loss in accuracy compared to optimiz-
ingboth, W and H,isrelativelysmallwhiletheruntimecanbeincreasedsigniﬁcantly.
m was set to 2 which indicates that the optimization is only applied in the ﬁrst two
iterations, and c was set to 20. As can be seen, the approximation error per iteration
can be reduced when using optimization strategy 2. For small rank k (left side of
Fig.15.5), the improvement is signiﬁcant but decreases with increasing values of k
(see right side of Fig.15.5). For larger k (larger than 10), the improvement over the
basic MU is only marginal.
15.6 Summary
In this chapter, we presented two optimization strategies to improve the NMF using
swarm intelligence-based optimization algorithms. In strategy, I use swarm intelli-
gence algorithms to initialize the factors and prior to the factorization process of

15.6 Summary
261
NMF, the second strategy II aims at iteratively improving the approximation quality
of NMF during the ﬁrst iterations of the factorization. Overall, ﬁve different optimiza-
tion algorithms were used for improving NMF: particle swarm optimization (PSO),
genetic algorithms (GA), ﬁsh school search (FSS), differential evolution (DE), and
ﬁreworks algorithm (FWA).
Both optimization strategies allow for efﬁciently computing the optimization of
single rows of W and/or single columns of H in parallel. The achieved results are
evaluated in terms of accuracy per runtime and per iteration, ﬁnal accuracy after a
given number of NMF iterations, and in terms of the classiﬁcation accuracy achieved
with the reduced NMF factors when applied to machine learning.
References
1. A. Janecek, Y. Tan, Swarm intelligence for non-negative matrix factorization. Int. J. Swarm
Intell. Res. (IJSIR) 2(4), 12–34 (2011)
2. D.D. Lee, H.S. Seung, Learning the parts of objects by non-negative matrix factorization.
Nature 401(6755), 788–791 (1999)
3. M.W. Berry, M. Browne, A.N. Langville, V. Paul Pauca, R.J. Plemmons, Algorithms and
applications for approximate nonnegative matrix factorization. Comput. Stat. Data Anal. 52(1),
155–173 (2007)
4. A.N. Langville, C.D. Meyer, R. Albright, J. Cox, D. Duling, Utilizing nonnegative matrix
factorization for e-mail classiﬁcation problems. Survey of Text Mining III: Application and
Theory (Wiley, New York 2010), pp. 57–80
5. K. Stadlthanner, D. Lutter, F.J. Theis, E.W. Lang, A.M. Tom, P. Georgieva, et al., Sparse non-
negative matrix factorization with genetic algorithms for microarray analysis, in International
Joint Conference on Neural Networks, IJCNN 2007 (IEEE, 2007), pp. 294–299
6. T. Blackwell, Particle swarm optimization in dynamic environments, in Evolutionary Compu-
tation in Dynamic and Uncertain Environments (Springer, Berlin, 2007), pp. 29–49
7. R. Chiong, Nature-Inspired Algorithms for Optimisation, vol. 193 (Springer, Berlin, 2009)
8. R.C. Eberhart, Y. Shi, J. Kennedy, Swarm Intelligence (Elsevier, Indianapolis, 2001)
9. P. Paatero, U. Tapper, Positive matrix factorization: a non-negative factor model with optimal
utilization of error estimates of data values. Environmetrics 5(2), 111–126 (1994)
10. C.-J. Lin, Projected gradient methods for nonnegative matrix factorization. Neural Comput.
19(10), 2756–2779 (2007)
11. H. Kim, H. Park, Nonnegative matrix factorization based on alternating nonnegativity con-
strained least squares and active set method. SIAM J. Matrix Anal. Appl. 30(2), 713–730
(2008)
12. M.N. Schmidt, H. Laurberg, Nonnegative matrix factorization with Gaussian process priors.
Comput. Intell. Neurosci. 2008, 3 (2008)
13. A. Janecek, S. Schulze Grotthoff, W.N Gansterer, LibNMF—a library for nonnegative matrix
factorization. Comput. Inf. 30(2), 205–224 (2011)
14. S. Wild, J. Curry, A. Dougherty, Improving non-negative matrix factorizations through struc-
tured initialization. Pattern Recognit. 37(11), 2217–2232 (2004)
15. Y. Xue, C.S. Tong, Y. Chen, W.-S. Chen, Clustering-based initialization for non-negative matrix
factorization. Appl. Math. Comput. 205(2), 525–536 (2008)
16. C. Boutsidis, E. Gallopoulos, SVD based initialization: a head start for nonnegative matrix
factorization. Pattern Recognit. 41(4), 1350–1362 (2008)
17. V. Snel, J. Plato, P. Krmer, Developing genetic algorithms for boolean matrix factorization.
Databases Texts 61, (2008)

262
15
FWA Application on Non-negative Matrix Factorization
18. A. Janecek, Y. Tan, Using population based algorithms for initializing nonnegative matrix
factorization. Advances in Swarm Intelligence (Springer, Berlin, 2011), pp. 307–316
19. A. Janecek, Y. Tan, Iterative improvement of the multiplicative update nmf algorithm using
nature-inspired optimization, in 2011 Seventh International Conference on Natural Computa-
tion (ICNC), vol. 3 (IEEE, 2011), pp. 1668–1672
20. M.W. Berry, Large-scale sparse singular value computations. Int. J. Supercomput. Appl. 6(1),
13–49 (1992)
21. I. Jolliffe, Principal Component Analysis (Wiley Online Library, 2005)
22. P.N. Tan, M. Steinbach, V. Kumar. Introduction to Data Mining. Pearson Education, Inc.,
London (2006)
23. M.W. Berry, Z. Drmac, E.R. Jessup, Matrices, vector spaces, and information retrieval. SIAM
Rev. 41(2), 335–362 (1999)
24. Q. Zhang, M.W. Berry, B.T. Lamb, T. Samuel, A parallel nonnegative tensor factorization
algorithm for mining global climate data, in Computational Science–ICCS 2009 (Springer,
Berlin, 2009), pp. 405–415
25. A.N. Langville, C.D. Meyer, R. Albright, J. Cox, D. Duling, Initializations for the nonnegative
matrix factorization, in Proceedings of the Twelfth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (Citeseer, 2006), pp. 23–26
26. D. Bratton, J. Kennedy, Deﬁning a standard for particle swarm optimization, in Swarm Intel-
ligence Symposium, SIS 2007 (IEEE, 2007), pp. 120–127
27. C.J.A. Bastos Filho, F.B. de Lima Neto, A.J.C.C. Lins, A.I.S. Nascimento, M.P. Lima, Fish
school search, in Nature-Inspired Algorithms for Optimisation (Springer, Berlin, 2009), pp.
261–277
28. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
29. R.L. Haupt, S.E. Haupt, Practical Genetic Algorithms (Wiley, New York, 2004)
30. K. Price, R.M. Storn, J.A. Lampinen, Differential Evolution: A Practical Approach to Global
Optimization (Springer, Berlin, 2006)
31. M.E.H. Pedersen, SwarmOps: Black-Box Optimization in ANSI C (Hvass Laboratories,
Southampton, 2008)

Chapter 16
FWA Applications on Clustering, Pattern
Recognition, and Inversion Problem
In this chapter, we will present the applications of ﬁreworks algorithm for dealing
with practical optimization problems including, document clustering, spam detec-
tion, image recognition, and seismic inversion problem [1]. The experimental results
given herein suggest that ﬁreworks algorithm is one of the most promising swarm
intelligence algorithms in dealing with those practical problems.
16.1 Document Clustering
With the popularity of the Internet, people often post a lot of information online. The
number of documents on the Web is growing rapidly. Analyzing and processing a
huge number of document collections manually have become very unrealistic. Auto-
matic documents processing has become the trend in the information age. The rapid
development of natural language processing enables automatic document processing
to step on to a new level. Document clustering is an important document processing
task. It automatically organizes massive documents into clusters according to their
topics for further usage.
Documents in the same clusters are expected to have the same topic, and docu-
ments in different clusters are expected to have different topics. Document clustering
has been applied to many areas. For example, in the search engines, if the searching
results are clustered into different clusters according to their different topics, people
can choose to read documents in their interested topics. Therefore, people can quickly
ignore the documents which they are not interested in, and can ﬁnd information they
need more quickly.
Document clustering generally contains two phases: feature extraction and clus-
tering. Term frequency-inverse document frequency (TF-IDF) is a very common fea-
ture extraction method. Many classical clustering algorithms are based on partition
or hierarchy. In recent years, some researchers have applied evolutionary algorithms
such as genetic algorithms and particle swarm optimization algorithm to document
clustering, and they have achieved promising results [2–4].
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_16
263

264
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
As a new swarm intelligence algorithm, ﬁreworks algorithm has showed excel-
lent capabilities in function optimization [5–7]. And we can expect its excellent
performance when applied to document clustering.
16.1.1 Document Features
Generally speaking, a machine learning algorithm requires the samples to be repre-
sented by vectors with a ﬁxed dimension, and the vectors are used as input of the
algorithm. Most clustering algorithms use this method to represent samples. The
process of converting documents to vectors is called feature extraction.
After feature extraction, each document is represented by a feature vector. Each
dimension of the vector is called a feature, which represents a particular property
of the document. The multiple dimensions of feature vectors reﬂect the multiple
properties of the documents. A good feature extraction method should be able to
extract a variety of information of the original documents.
Vector space model (VSM) [8, 9] is a commonly used feature extraction method
for documents. In this model, each word is regarded as a feature, and the feature
value reﬂects the importance of the corresponding word.
16.1.1.1 Feature Selection
Vector space model regards each word as a feature. A collection of documents gen-
erally contains a larger number of words. Therefore, the number of features will be
much. The subsequent calculations will spend a lot of computer resources. Mean-
while, not all features are beneﬁcial to clustering, some of the redundant features
may reduce the clustering effectiveness. It is necessary to select a subset of features
according to a certain criterion. The word frequency is used as the criterion in this
book.
The frequency of each word in all documents is calculated ﬁrst. The most frequent
N words are selected as features, and the remaining words are directly discarded. In
the following experiment, the value of N is set as 2000.
16.1.1.2 TF-IDF Feature for Documents
TF-IDF is a common document feature extraction algorithm for document [10],
which reﬂects the importance of a word in a document. TF-IDF represents the product
of the term frequency (TF) and inverse document frequency (IDF). Term frequency
of the ith word in the jth document is deﬁned as follows:
tfij = nij/n j
(16.1)
where nij represents number of the ith word in the jth document, and n j represents
the total number of words in the jth document.

16.1 Document Clustering
265
Inverse document frequency of the ith words is deﬁned as follows:
idfi = log(D/di)
(16.2)
where D represents the number of documents, and di represents the number of doc-
uments containing the ith word.
TF-IDF feature of the ith word in the jth document is deﬁned as follows:
tﬁdfij = tfij ∗id fi.
(16.3)
16.1.1.3 Dimension Reduction
After the above processing each document is able to be represented as a high-
dimensional vector. Using clustering algorithms directly on the high-dimensional
vector will consume large computing resources. What is more, because some infor-
mation of the high-dimensional vector is redundant, the machine learning algorithm
applied on high-dimensional feature probably goes into overﬁtting. In such case
the machine learning model learns too much about the redundant information of
the training data, and ignores some essential characteristics of the training data for
clustering [11].
Dimension reduction is a commonly used data processing technique. It maps data
from high-dimensional space to low-dimensional space. Data in the low-dimensional
spaceshouldreﬂect intheessential characteristics of theoriginal datasufﬁciently, and
should remove redundant information from the original data. Dimension reduction
algorithms are often divided into two categories: linear dimensionality reduction and
nonlinear dimensionality reduction.
This section uses a linear dimensionality reduction method called principal com-
ponent analysis (PCA) to reduce the dimension of documents [12]. PCA ﬁrst selects
several orthogonal principal components from the high-dimensional space. The vari-
ance of data along the directions of these principal components should be the greatest.
These principal components will be considered as the orthogonal bases of a low-
dimensional space, and the original data is represented as low-dimensional vectors
under such orthogonal bases. PCA retains the original data in several directions with
the largest variance and discards other directions. We can see that it tries to keep the
essential information of the original data and dismisses unimportant information.
16.1.1.4 Data Standardization
In many dataset the distribution of data over different dimensions have a variety of
ranges. The data along some dimensions may be too large or too small. And the
ranges along different dimensions may differ too much. This situation will decrease
the performance of many machine learning algorithms.

266
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
Standardizing the data will unify the range of data along different dimensions.
Each dimension is shrunk linearly to unify the range. After shrinking each dimension
is with zero mean and unit standard deviation. Let xij denotes the jth dimension of
the ith datum before standardization, and let ˆxij represent the normalized data, i.e.,
ˆxij = (xij −μ j)/σ j
(16.4)
where μ j represents the mean of the jth dimension, and σ j represents the standard
deviation of the jth dimension.
16.1.2 Fireworks Algorithm for Document Clustering
Fireworksalgorithmhasshownexcellentabilityinoptimization.Fireworksalgorithm
can be applied to document clustering by searching the optimal centroids of all the
clusters. Therefore, the clustering problem is converted to optimization problem.
Fireworks algorithm searches the optimal centroids in the M ∗F-dimensional
space, where M represents the number of document clusters, and F denotes
the dimension of feature vectors. We denote the centroid of the ith cluster as
ci, i = 1, 2, . . . , M, and xi is an F-dimensional vector. A ﬁrework is deﬁned as
<c1, c2, . . . , cM>.
Once the centroid of each cluster is given, the documents can be categorized
into clusters. The nearest centroid to each document is found and the document
is categorized into the corresponding cluster. We choose the Euclidean distance to
measure the distance between two vectors. That is, for vector a = {x1, x2, . . . , x p}
and vector b = {y1, y2, . . . , yp}, the distance between them is d =

p
i=1
(xi −yi)2.
The closer two vectors are, the smaller the value of d is, and vice versa. The Euclidean
distance can be calculated easily and efﬁciently.
The clustering criteria intra-cluster distance is used as the ﬁtness function of
ﬁreworks algorithm. The intra-cluster is calculated as follows:
J =

i

j∈Ki

x j −ci
2
(16.5)
where ci represents the centroid of the ith cluster, and Ki represents the set of
documents in the ith cluster, and x j is the jth document in Ki.
The smaller the intra-cluster distance is, the more compact documents in the same
cluster are, and the better clustering quality ﬁreworks algorithm achieves.

16.1 Document Clustering
267
Finally, we can use ﬁreworks algorithm to ﬁnd an optimal ﬁrework with minimal
intra-cluster distance. And these ﬁreworks are used as the centroids of document
clusters.
Speciﬁc steps of using ﬁreworks algorithm for document clustering are shown in
Algorithm 16.1.
Algorithm 16.1 Fireworks algorithm for document clustering.
1: Randomly select N points in M ∗F-dimensional search space of N points as the initial ﬁreworks.
Each ﬁrework represents centroids of different document clusters.
2: Calculate the Euclidean distance between each document and each centroid. Each document is
clustered into the cluster with the nearest centroid.
3: Get the ﬁtness function of each ﬁreworks by calculating the intra-cluster distance.
4: Explode and select some ﬁreworks for the next generation. If the termination condition meets,
go to step 5, or go to step 2.
5: Get the result of document clustering.
16.1.3 Experiment
The following experiment demonstrates the application of ﬁreworks algorithm to
document clustering.
20-newsgroup is a well-known English text mining data set [10], which contains
messages from 20 newsgroups. Messages in each newsgroup are about one speciﬁc
topic. These messages were clustered into four clusters by the ﬁreworks algorithm,
clustering results are shown in Fig.16.1. Due to space limitations, Fig.16.1 only
shows some representative messages from 12 newsgroups. From Fig.16.1 we can
see that messages within the same clusters roughly have the same topic. For example,
messages in the ﬁrst row of the third column are mainly about politics, messages
in the second row of the ﬁrst column are mainly about religion, and messages in
the third row of the third column are mainly about hardware. There are also a few
messages with different topics in each cluster due to errors. But on the whole the
ﬁreworks algorithm can effectively organize documents into clusters according to
their topics, which means that this is a good document clustering algorithm.
Traditional document clustering algorithms are easy to fall into local optimum
while ﬁreworks algorithm is effectively able to avoid local optimum, as a result,
the ﬁreworks algorithm can achieve a promising result when applied to document
clustering, which lay a new way to document clustering.

268
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
Fig. 16.1 Document clustering by ﬁreworks algorithm on 20-newsgroup
16.2 Spam Detection
Spam, deﬁned as Unsolicited Commercial Emails (UCE) or Unsolicited Bulk Emails
(UBE) [13], has become a signiﬁcant problem for both recipients and Internet Service
Providers (ISPs). For recipients, coping with spam is time-consuming; furthermore,
spam frequently contains images that recipients ﬁnd offensive, or attached mali-
cious programs that attack recipients’ computers. For ISPs, large scale of spam is a
considerable burden on their systems [14–17].
Many approaches have been proposed to handle the problem, in which intelligent
approaches play an increasingly important role in antispam in recent years for their

16.2 Spam Detection
269
ability of self-learning and good performance. Spam detection is seen as a two-
class categorization problem in intelligent approaches, which mainly contain feature
extraction and classiﬁcation phases. In previous researches, parameters of the spam
detection approaches are set simply and manually. However, the manual setting might
cause several problems. For instance, lack of prior knowledge may lead to improper
parameter setting, repeated attempts of users cost overmuch human effort, and the
inﬂexibility of the dataset-relevant parameters should also be taken into account. This
section introduces application of FWA in parameter optimization of spam detection.
16.2.1 Problem Description
Intelligent spam detection approaches mainly contain two steps, feature extraction
and classiﬁcation. The former transforms email samples into speciﬁc representation
form, and further utilized by the later to get the class. In this section, we set local
concentration (LC) approach [18–20], which is one of the immune concentration-
based feature extraction approaches [18–23], and support vector machine (SVM)
technique [17, 24–28] as examples of feature extraction approach and classiﬁcation
algorithm, respectively, to introduce how to optimize parameters of spam detection
approach with FWA (Fig.16.2).
Figure16.3 shows the intelligent spam detection model with LC and SVM, which
mainly contains training phase and classiﬁcation phase. In training phase, an email
dataset, called training set, is preprocessed with tokenization and stemming, and
large amount of words are obtained. These words are further selected according to
their importance estimated by some term selection metrics. The selected words are
Email database
Tokenization
Term selection
Detector set construction
LC Calculation
Classifier training
Classifier model
Training phase
An incoming email
Tokenization
LC Calculation
Classifying
Classes of emails
Classification phase
Core
steps
Core
steps
Fig. 16.2 Intelligent spam detection model with LC and SVM. Reprinted from Ref. [29], with kind
permission from Springer Science+Business Media

270
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
Fig. 16.3 Framework of
parameter optimization of
spam detection with FWA.
Reprinted from Ref. [29],
with kind permission from
Springer Science+Business
Media
Initialize fireworks 
swarm, which are all 
input vector of CF(P)
Optimization on CF(P)
using FWA
Optimal parameters 
found?
Output optimal 
parameters
Yes
No
considered to be able to provide more useful information for email classiﬁcation.
Spam detector set and legitimate detector set are constructed based on these selected
words. Concentration features of each email sample in the email dataset are calcu-
lated on the detector sets and concentration feature vectors are used to construct the
classiﬁer. In classiﬁcation phase, an incoming email is preprocessed similarly and
the concentration features are calculated to construct its feature vector. Finally, the
feature vector is used by the classiﬁer constructed above to predict the class label of
the incoming email [28].
In the above model, all of the term selection, detector set construction, feature cal-
culation, and classiﬁcation involve some important parameters that can have appar-
ent inﬂuence on the performance of spam detection. Terms with more information
are selected by term selection to reduce computational complexity with relatively
nonuseful words and interference on classiﬁcation results of noisy terms. Percentage
of term selection m% determines both the size and quality of the gene library for
constructing detector sets. By calculating class tendency of each selected terms, spam
detector set and legitimate detector set are constructed. Tendency threshold measures
the absolute value of the difference of a term’s posteriori probabilities appearing in
two classes of emails. Concentration features of emails are calculated on each local
area to construct the concentration vectors. Number of sliding windows N deﬁnes the

16.2 Spam Detection
271
partition granularity inside an email. Parameters of the classiﬁer are also important
for the detection performance.
Intelligentspamdetectionapproachinvolvesmanyparameterswhichimmediately
affect the performance of spam detection. Therefore, it is necessary to optimize the
parameters of spam detection approach with FWA.
16.2.2 Optimization Principle
The classiﬁcation problem that “whether an email is spam or a legitimate email” is
here considered as an optimization problem, that is, to achieve the lowest error rate
by ﬁnding the optimal parameter vector in the potential search space [29].
The optimal parameter vector P∗=< F∗
1 , F∗
2 , . . . , F∗
n , C∗
1, C∗
2, . . . , C∗
m > com-
poses of two parts: the ﬁrst part is the feature calculation relevant parameters
< F∗
1 , F∗
2 , . . . , F∗
n >, and the second part is the classiﬁer relevant parameters
< C∗
1, C∗
2, . . . , C∗
m >. Parameters < F∗
1 , F∗
2 , . . . , F∗
n > determine the performance
of feature construction independently, and parameters < C∗
1, C∗
2, . . . , C∗
m > affect
performance of the speciﬁc classiﬁer. The optimal vector P∗is the vector whose
cost function CF(P) associated with classiﬁcation achieves the lowest value, with
CF(P) = Err(P)
(16.6)
where Err(P) is the classiﬁcation error measured by cross-validation on the training
set.
Vector P is the optimization vector whose performance is measured by CF(P).
Therefore, this optimization of concentrations can be formulated as follows: Finding
P∗=< F∗
1 , F∗
2 , . . . , F∗
n , C∗
1, C∗
2, . . . , C∗
m >, so that
CF

P∗
= min
{F1,F2,··· ,Fm,C1,C2,··· ,Cm}CF(P) .
(16.7)
Different feature extraction methods hold different parameters and lead to dif-
ferent performances. For LC approach, speciﬁcally, there are term selection rate
m%, tendency threshold, and sliding windows number N. Different classiﬁers also
hold different parameters. SVM-related parameters determine the location of optimal
hyperplane for classiﬁcation in feature space, including cost function value C and
kernel function parameter γ, just to name a few.
Figure16.3 shows the framework of parameter optimization of intelligent spam
detection with FWA. Based on this framework, we further deﬁne two strategies for
parameter optimization. For efﬁciency consideration, strategy 1 deﬁnes an indepen-
dent validation set on training set, that is to divide the original training set into a new
training set and a validation set. The ﬁreworks are trained on the new training set and
validated on the validation set independently, where each ﬁrework corresponds to a
different parameter combination, until the optimal ﬁrework is obtained and used for

272
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
detection on the testing set. For this strategy, ﬁtness values of ﬁreworks are calcu-
lated on the independent validation set in each cross-validation, so the computational
complexity is relatively low.
For robustness consideration, strategy 2 randomly divides the original training set
into ten parts instead of deﬁning an independent validation set, and ﬁtness values of
ﬁreworks are calculated by tenfold validation. In this case, each ﬁrework can get a
comprehensive and integrated evaluation.
16.2.3 Experimental Results
Experiments were conducted on four benchmark corpora PU1, PU2, PU3, and PUA,
using tenfold cross-validation to verify the effectiveness of FWA-based parameter
optimization of spam detection. During the experiments, range of the term selection
rate m% is set to [0, 1], range of the tendency threshold θ is set to [0, 0.5], and sliding
window number N [1, 50]. SVM-related parameters c ∈(1, 100) and γ ∈(0, 20).
Parameters involved in FWA are the same with original FWA.
Comparison of performance before and after parameter optimization with strat-
egy 1 is shown in Fig.16.4. It is obvious that the performance of spam detection
is improved on all the corpora utilized. However, due to the limitation that the
Fig. 16.4 Comparison of performance before and after parameter optimization with strategy 1.
Reprinted from Ref. [29], with kind permission from Springer Science+Business Media

16.2 Spam Detection
273
Fig. 16.5 Comparison of performance before and after parameter optimization with strategy 2.
Reprinted from Ref. [29], with kind permission from Springer Science+Business Media
independent validation set cannot always describe the distribution feature of data
in testing set precisely, the improvement of performance is somewhat limited.
Figure16.5 shows the comparison of performance before and after parameter
optimization with strategy 2, which randomly divides the original training set into
ten parts instead of deﬁning an independent validation set, and calculating ﬁtness
values of ﬁreworks using tenfold validation. To some extent, strategy 2 solves the
evaluation problem caused by different data distribution features in validation set and
testing set. For experimental efﬁciency consideration, we randomly selected 20%
samples from each corpus for experiments. As we can see in Fig.16.5, performance
of spam detection is improved substantially using strategy 2. In real world, spam
ﬁlters are usually trained off-line, thus strategy 2 is also worth applying [29].
16.3 Image Recognition
For arecognitiontask, distancemeasurebetweenpatterns is oneof themost important
procedures [11, 30, 31]. Orientation coding-based methods have achieved high accu-
racy and speed in recognition, such as competitive code (CompCode) [32], palmprint
orientation code (POC) [33, 34], and robust line orientation code (RLOC) [35]. In
these methods, the commonly used strategy is to convolve the images with a number

274
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
of ﬁlters, then, the orientation is recoded as orientation values by the dominate convo-
lution results. For orientation coding-based patterns, OR_XOR (Hamming distance)
and SUM_XOR (Angular distance) are usually used for measuring the distance.
However, little work have been done to study the physical signiﬁcance of these dis-
tances for orientation coding-based patterns. In [36], Guo et al. tried to use a uniﬁed
distance measure for recognition, however, the physical meaning of the uniﬁed dis-
tances measure scheme is not explained. Moreover, the relation between OR_XOR
and SUM_XOR is not clear.
In this section, a new uniﬁed distance measure (UDM) scheme is proposed [37].
To automatically compute the parameters introduced in UDM model, the population-
based heuristic algorithms, particle swarm optimization (PSO), differential evolution
(DE), and ﬁreworks algorithm (FWA) [] are used for the determination of parameters.
16.3.1 Relation Between SUM_XOR and OR_XOR
Denoting I (x, y) as the preprocessed image, the pattern image P(x, y) is computed
as
P(x, y) = arg
min
∀i∈[0,N−1] I (x, y)

G(x, y, θi)
(16.8)
where  is the convolution operation. Suppose N = 6, then θi = iπ/6. Each point
in P(x, y) is the orientation θi with the dominant response, and the distance between
two pattern images P(x, y), Q(x, y) is sum of the distance of all corresponding
points. Here, some basic rules are presented ﬁrst. θa and θb are two corresponding
points in P(x, y) and Q(x, y).
1. d(θa, θb) = d(θb, θa) (Symmetry).
2. d0 = d(θa, θa) = 0.
3. d1 = d(0, π/6) = d(π/6, π/3) = d(π/3, π/2) = d(π/2, 2π/3) = d(2π/3,
5π/6) = d(5π/6, 0). For any two orientations, the interval with π/6 should be
equal.
4. d2 = d(0, π/3) = d(π/6, π/2) = d(π/3, 2π/3) = d(π/2, 5π/6) = d(2π/3, 0)
For any two orientations, the interval with π/3 should be equal.
5. d3 = d(0, π/2) = d(π/6, 2π/3) = d(π/3, 5π/6) = d(π/2, 5π/6)
For any two orientations, the interval with π/2 should be equal.
To represent the orientation, Kong et al. used a 3-bit (log26) encode method for ori-
entation coding [32], which encode the orientations {0, π/6, π/3, π/2, 2π/3, 5π/6}
as {000,001,011,111,110,100}. Then SUM_XOR and OR_XOR distances between
P(x, y), Q(x, y) are deﬁned as follows:
DSUM_XOR =
M
y=1
N
x=1
3
i=1 Pb
i (x, y)  Qb
i (x, y)
3 ∗M ∗N
(16.9)

16.3 Image Recognition
275
DOR_XOR = 1/(M ∗N) ∗M
y=1
N
x=1((Pb
0 (x, y) 	 Qb
0(x, y))|
(Pb
1 (x, y) 	 Qb
1(x, y))|(Pb
2 (x, y) 	 Qb
2(x, y)))
(16.10)
where M and N are the height and length of the pattern, Pb
i (x, y), Qb
i (x, y) are the
ith bit plane of P and Q. 	 denotes the bitwise exclusive OR (XOR), | denotes
the bitwise OR. Based on the ﬁve basic rules, denote by ai, i ∈[0, 3] the number
of pixels where the distance is di, i ∈[0, 3], (16.9) and (16.10) can be rewritten as
follows:
DSUM_XOR = k0 ∗a0 + k1 ∗a1 + k2 ∗a2 + k3 ∗a3
M ∗N
(16.11)
= 1/3 ∗a1 + 2/3 ∗a2 + 3/3 ∗a3
M ∗N
(16.12)
DSUM_XOR = k0 ∗a0 + k1 ∗a1 + k2 ∗a2 + k3 ∗a3
M ∗N
(16.13)
= 1 ∗a1 + 1 ∗a2 + 1 ∗a3
M ∗N
.
(16.14)
Here, ki, i ∈0, 1, 2, 3 can be seen as the cost of wrong decision, and k0 = 0. For a
6 orientation ﬁlter, ki denotes the cost that the decision of θl while the ground truth
is θ(l+i)mod6 or θ(l−i)mod6.
16.3.2 The Proposed Uniﬁed Distance Measure Scheme
In [38], Kong pointed out that SUM_XOR outperforms OR_XOR in PolyU palm-
print databases. Compare (16.11) and (16.13), the difference between OR_XOR and
SUM_XOR is that the using of different coefﬁcients, which denotes the cost of wrong
decision. Based on the comparison, we deﬁne the uniﬁed distance measure scheme
between patterns P(x, y) and Q(x, y) as
Du = k1 ∗a1 + k2 ∗a2 + k3 ∗a3
M ∗N
.
(16.15)
For a recognition task, the performance index are usually equal error rate (EER).
For a two-choice decision task, equal error rate is the false reject rate which equals
the false accept rate. The EER is scale invariant with the distributions. Thus, the
uniﬁed distance can be rewritten as
Du =
k1
M ∗N ∗

a1 + k2
k1
∗a2 + k3
k1
∗a3

(16.16)
D′
u = K1 ∗a1 + K2 ∗a2 + K3 ∗a3.
(16.17)

276
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
For the uniﬁed distance shown in (16.17), there are three parameters (K1, K2, K3)
to be determined, in which K1 = 1. SUM_XOR and OR_XOR are two special cases,
which take K2 = 2, K3 = 3 and K2 = 1, K3 = 1, respectively.
The optimization problem can generally be stated as
min eer =
min
x=[K2,K3]∈ f (x),
(16.18)
where f : R2 →R is a nonlinear function used for the computation of EER values,
and  is the feasible region. For this optimization problem, to automatically deter-
mine the parameters of the uniﬁed distance measure model in (16.17), the heuristic
algorithms, particle swarm optimization [39], differential evolution [40], and ﬁre-
work algorithm [5] are used for the optimization, which are named as PSO-UDM,
DE-UDM, FWA-UDM, respectively. Details about each heuristic algorithm can be
found in the references provided below.
Particle Swarm Optimization (PSO, [39, 41]) mimics the process of the search
for food of ﬂocks. The particles (ﬂocks) in the swarm move under the guidance of
the cognitive information and social information.
Differential Evolution (DE, [40]) maintains a population of candidate solutions
while it also creates some new candidate solutions by combining existing ones
according to its formulae. Then it keeps the candidate solution which has the best
score or ﬁtness on the optimization problem.
Fireworks Algorithm (FWA, [5]) Inspired by the explosion of ﬁreworks in the
night sky, a ﬁrework explodes and generates the sparks in the nearby space can be
seen as the search of an optimization problem.
16.3.3 Experiment
To validate the performances of the proposed PSO-UDM, DE-UDM, and FWA-
UDM, three databases, the artiﬁcial data set, the PolyU palmprint database, and the
collected ﬁnger-vein database are used in [37] and this chapter only chooses the
PolyU palmprint database to present the performance. POC, RLOC, and CompCode
are used for pattern extraction. The EER is compared to evaluate the performance.
In the experiments, the search range of K2 and K3 are both set to [0, 6].
To reduce the inﬂuences of shift and rotation of the patterns, in the matching part,
a shift of [−2, 2] in X-axis and [−2, 2] in Y-axis is taken, and the minimal distance
under a certain K2, K3 is recorded as the distance between two patterns.
The swarm size for each algorithm is set to 50, and the maximum evaluation times
is 5000.
1. PSO, velocitymax = 20, c1 = c2 = 2, ω = 0.5, α = 1
2. DE, the rest of parameters are same as [40]
3. FWA, the rest of parameters are same as [5]

16.3 Image Recognition
277
Table 16.1 Experimental results of EER and parameters of K2, K3 on the PolyU palmprint database
Distance
POC
RLOC
CompCode
EER
K2
K3
EER
K2
K3
EER
K2
K3
OR_XOR
2.2143E-3
1
1.0
4.2664 E-4
1.0
1.0
2.3652E-3
1.0
1.0
SUM_XOR
1.9425E-3
2
3.0
4.2873 E-4
2.0
3.0
2.8809E-3
2.0
3.0
PSO-UDM
1.7894E-3
2.79628
2.12560
3.9649E-4
1.76602
2.06041
2.1216E-3
0.71403
1.01558
DE-UDM
1.7912E-3
1.90490
2.0377
4.2904E-4
3.10227
3.35624
2.3385E-3
0.835902
1.48357
FWA-UDM
1.7907E-3
2.30518
1.87876
3.9739E-4
1.57043
1.46151
2.1539E-3
0.70509
0.85458

278
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
16.3.3.1 Experiments Using the PolyU Palmprint Database
Experimental Design: The PolyU palmprint database [42] is designed by The Hong
Kong Polytechnic University, which contains palmprint images from 500 individuals.
Each individual provides 12 images, each 6 images one time. Therefore, the database
contains 6000 images from 500 individuals. The size of all the images is 128 × 128.
First, the images are preprocessed for noises reduction and patterns are extracted
by POC, RLOC, and CompCode algorithms. Then the three distances are com-
puted. There are totally 17,997,000 match times with 33,000 genuine match time
and 17,964,000 imposters. To reduce the computation load, the images are down-
sampled.
Experimental Results: Table16.1 lists the results of EER, parameters of K2,
K3 introduced in the uniﬁed distance model. From the results, it can be concluded
that the using of population-based algorithms for parameters optimization for the
uniﬁed distance model can achieve lower EER than OR_XOR and SUM_XOR on the
palmprint database. Among the three methods, PSO-UDM, DE-UDM, and FWA-
UDM, PSO-UDM has the lowest EER which can reduce the EER by up to 7.88%
(1.94250E-03 to 1.78942E-03) for POC, 3.31% (4.10080E-04 to 3.96492E-04) for
RLOC, and 10.30% (2.36520E-03 to 2.12160E-03) for CompCode, respectively.
16.3.3.2 Performance Comparison
We investigate the relation between hamming distance and angular distance, and
based on the study, a uniﬁed distance is proposed. To validate the proposed distance
methods, we test the orientation algorithms on one databases for recognition. Exper-
imental results of EER shown in Table16.1, suggest that the proposed PSO-UDM,
DE-UDM, and FWA-UDM gain great advantages compared with the OX_XOR and
SUM_XOR distance measure schemes. Among the three population based heuristic
algorithms, PSO-UDM and FWA-UDM gains the better performance compared with
DE-UDM while PSO-UDM and FWA-UDM nearly have the same performance.
16.4 Seismic Inversion Problem
Seismic inversion is a critical and challenging task in geophysics. It is widely used
for oil/gas exploration and development. In recent decade, intelligence optimizations
such as GA and PSO [17], are applied to the inversion problem and achieve a great
success. In this section, we try to apply FWA to inversion problem, and we will see
that FWA outperforms PSO, GA, and its variants [7, 16, 24, 43].

16.4 Seismic Inversion Problem
279
Fig. 16.6 Flowchart of inversion
16.4.1 Problem Statement
In Geophysics, inversion methods are usually used for exploring the structure of the
Earth.
In general, the target of an inversion problem is to ﬁnd a parameterized model
that can ﬁt the observed data as best. It is based on the forward problem, i.e., for a
given set of parameters, the expected observation can be calculated. The ﬂowchart
of inversion is depicted by Fig.16.6.
There exist many inversion models, here only reﬂectivity model is discussed. The
seismic wave can be regarded as light-like wave. When the seismic wave reaches
the boundary between two medium, both reﬂection and refraction happen. The seis-
mic wave can be observed after many such reﬂections and refractions. On these
hypotheses, we can set up the geographical reﬂectivity model [44].
Given a point and its horizontal and vertical distances between the seismic source
denoted as r and z, respectively.
Φ0(r, z, t) = 1
R F

t −R
α1

,
(16.19)
where R = r2 + z2 t denotes time and α1 wave velocity.
Fourier transform can be expressed in calculus as
Φ0(r, z, ω) = ¯F(ω)
 inf
0
k
jv1
J0(kr)exp(−jv1z)dk,
(16.20)

280
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
where, ¯F(ω) is the Fourier transform of F(t)J0(kr) order 0 Bessel function of ﬁrst
kind, j imagery unit, k the number of horizontal wave, and
v1 = (k2
α1 −k2)1/2,
(16.21)
where kα1 = ω
α1 denotes the numbers of vertical waves.
Generally, after the refraction of m layers, the seismic wave can be calculated in
frequency domain as
¯Φ1(r, z, ω) = ¯F(ω)
 inf
0
k
jv1
J0(kr)Pd(ω, k) × exp

−j
m−1

i=1
hivi +

z −
m−1

i=1
hi

vm

dk,
(16.22)
where Pd(ω, k) is the product of each layer’s down-wave transmission coefﬁcient.
In the reﬂection interface, the generated P-wave can be calculated as
¯Φ2(r, z, ω) = ¯F(ω)
 inf
0
k
jv1
J0(kr)Pd(ω, k) ˜Rpp(ω, k) × exp

−j
m−1

i=1
hivi +

z −
m−1

i=1
hi

vm

dk,
(16.23)
where ˜Rpp(ω, k) is complex reﬂection.
Fluctuations through m layers’ reﬂection back to the original layer can be calcu-
lated as follows:
¯Φ3(r, z, ω) = ¯F(ω)
 inf
0
k
jv1
J0(kr)Pd(ω, k) ˜Rpp(ω, k)Pu(ω, k)
× exp

−j

2
m−1

i=1
hivi +

z −
m−1

i=1
hi

vm

dk,
(16.24)
where Pu(ω, k) is the product of all layers’ up-wave transmission coefﬁcients.
Thus, we can calculate the P-wave and S-wave received on the ground as
¯Φ4(r, z, ω) = ¯F(ω)
 inf
0
k
jv1
J0(kr)Pd(ω, k) ˜Rpp(ω, k)Pu(ω, k)rpp(ω, k)
× exp

−j

2
m−1

i=1
hivi +

z −
m−1

i=1
hi

vm

dk,
(16.25)
¯Φ5(r, z, ω) = ¯F(ω)
 inf
0
k
jv1
J0(kr)Pd(ω, k) ˜Rps(ω, k)Pu(ω, k)rps(ω, k)
× exp

−j

2
m−1

i=1
hivi +

z −
m−1

i=1
hi

vm

dk,
(16.26)
where, rpp and rps denote the P–P and P–S reﬂection ratios, respectively.

16.4 Seismic Inversion Problem
281
The generated ﬂuctuation is
¯u = ∂¯Φ3
∂r
+ ∂¯Φ4
∂r
−
¯
∂z ,
(16.27)
¯w = ∂¯Φ3
∂z + ∂¯Φ4
∂z +
¯
∂r +
¯
Psi
r .
(16.28)
Here, let z = 0, then we get the expected synthesized wave. Based on this forward
model, we can solve the inversion problem as illustrated by Fig.16.6.
16.4.2 Experiment and Analysis
To justify the feasibility of FWA for inversion problems, simulation experiment was
conducted. In the simulation, GA, NGA, PSO as well as FAW were applied to ﬁnd
the structure parameters (density etc.). The results are presented by Figs.16.7 and
Fig. 16.7 Convergency curve of optimization
Fig. 16.8 Comparison of optimization results

282
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
Table 16.2 Comparison of optimization results
Parameters
1
2
3
4
5
6
7
8
9
Real value
4
6
6.25
6.95
8.1
1.5
10
20
18
NGA
3.955
5.970
6.222
6.92
1 8.111 1.478
8.278
20.560
1.903
DE
3.830
6.102
6.282
7.02
2 8.066 1.725
11.055
19.741
18.612
PSO
3.907
5.983
6.237
6.96
9 8.099 1.347
9.908
20.312
17.993
FWA
4.043
5.998
6.242
6.94
6 8.095 1.521
9.945
19.849
18.079
16.8. As can be seen, PSO and FWA outperform GA and NGA, and FWA achieves
the best results.
The inversion result is very close to the real condition (see Table16.2).
The whole results show that FWA can inherit the rapid convergence characteristics
of swarm intelligence algorithms. It also has the same ability of searching global
optimization as GA and PSO so that it can get better results.
16.5 Summary
This chapter described several successful application cases of ﬁreworks algorithm,
includingdocumentclustering,spamdetection,imagerecognition,andseismicinver-
sion problem. All experimental results show that ﬁreworks algorithm is indeed a
promising approach to handle such practical problems and their like.
In the future, the performance of FWA is believed to have better performance
while the run time is smaller, and could be applied to deal with a lot of application
problems.
References
1. Y. Tan, Swarm robotics: collective behavior inspired by nature. J. Comput. Sci. Syst. Biol.
(JCSB) 6, e106 (2013)
2. G. Jones, A. Robertson, C. Santimetvirul, P. Willett, Non-hierarchic document clustering using
a genetic algorithm. Inf. Res. 1(1), 1–1 (1995)
3. V.V. Raghavan, K. Birchard, A clustering strategy based on a formalism of the reproduc-
tive process in natural systems, in ACM SIGIR Forum, vol. 14(2). (ACM, New York, 1979),
pp. 10–22
4. X. Cui, T.E. Potok, P. Palathingal, Document clustering using particle swarm optimization, in
Proceedings 2005 IEEE Swarm Intelligence Symposium. SIS 2005, (IEEE, 2005), pp. 185–191
5. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
6. Y. Tan, S. Zheng, Research progress on ﬁreworks algorithm. CAAI Trans. Intell. Syst. 9(10),
1–17 (2014)

References
283
7. Y. Tan, C. Yu, S. Zheng, K. Ding, Introduction to ﬁreworks algorithm. Int. J. Swarm Intell.
Res. (IJSIR) 4(4), 39–70 (2013)
8. B.S. Everitt, S. Landau, M. Leese, D. Stahl, Cluster Analysis. Wiley series in Probability and
Statistics (Wiley, New York, 2011). http://books.google.com.hk/books?id=w3bE1kqd-48C.
ISBN: 9780470978443
9. S.C. Gu, Y. Tan, X. He, Recentness biased learning for time series forecasting. Inf. Sci. 237,
29–38 (2013)
10. T. Joachims, A probabilistic analysis of the Rocchio algorithm with TFIDF for text categoriza-
tion. Technical report, DTIC Document (1996)
11. J. Wang, Y. Tan, Efﬁcient euclidean distance transform algorithm of binary images in arbitrary
dimensions. Pattern Recognit. 46(1), 230–242 (2013)
12. I. Jolliffe, Principal Component Analysis (Wiley Online Library, New York, 2005)
13. L.F. Cranor, B.A. LaMacchia, Spam! Commun. ACM 41(8), 74–83 (1998)
14. F. Research, Spam, spammers, and spam control: a white paper by ferris research. Technical
report (2009)
15. Y. Tan, Y. Zhu, Advances in anti-spam techniques. CAAI Trans. Intell. Syst. 5(3), 189–201
(2010)
16. Y. Tan, Particle swarm optimizer algorithms inspired by immunity-clonal mechanism and their
application to spam detection. Int. J. Swarm Intell. Res. 1(1), 64–86 (2010)
17. J.Zhang,Y.Tan,L.Ni,C.Xie,Z.Tang,Hybriduniformdistributionofparticleswarmoptimizer.
IEICE Trans. Fundam. Electron. Commun. Comput. Sci. E93-A(10), 1782–1791 (2010)
18. G. Ruan, Y. Tan, A three-layer back-propagation neural network for spam detection using
artiﬁcial immune concentration. Soft Comput. 14(2), 139–150 (2010)
19. Y. Zhu, Y. Tan, Extracting discriminative information from E-mail for spam detection inspired
by immune system, in 2010 IEEE Congress on Evolutionary Computation (CEC). (IEEE,
2010), pp. 1–7
20. Y. Zhu, Y. Tan, A local-concentration-based feature extraction approach for spam ﬁltering.
IEEE Trans. Inf. Forensics Secur. 6(2), 486–497 (2011)
21. Y. Tan, C. Deng, G. Ruan, Concentration based feature construction approach for spam detec-
tion, in International Joint Conference on Neural Networks, 2009. IJCNN 2009. (IEEE, 2009),
pp. 3088–3093
22. G. Mi, P. Zhang, Y. Tan, A multi-resolution-concentration based feature construction approach
for spam ﬁltering, in The 2013 International Joint Conference on Neural Networks (IJCNN)
(IEEE, 2013), pp. 1–8
23. G. Mi, P. Zhang, Y. Tan, Feature construction approach for email categorization based on
term space partition, in The 2013 International Joint Conference on Neural Networks (IJCNN)
(IEEE, 2013), pp. 1–8
24. J. Zhang, Y. Tan, L. N, C. Xie, Z. Tang, AMT-PSO: an adaptive magniﬁcation transforma-
tion based particle swarm optimizer. IEICE Trans. Fundam. Electron. Commun. Comput. Sci
E94-D(4), 786–797 (2011)
25. Y. Tan, J. Wang, A support vector network with hybrid kernel and minimal Vapnik-
Chervonenkis dimension. IEEE Trans. Knowl. Data Eng. 26(2), 385–395 (2004)
26. H. Drucker, D. Wu, V.N. Vapnik, Support vector machines for spam categorization. IEEE Trans.
Neural Netw. 10(5), 1048–1054 (1999)
27. S.C. Gu, Y. Tan, X.G. He, Discriminant analysis via support vectors. Neurocomputing 73
(10–12), 1669–1675 (2010)
28. Y. Tan, G.C. Ruan, Uninterrupted approaches for spam detection based on SVM and AIS. Int.
J. Comput. Intell. Pattern Recognit. (IJCIPR) 1(1), 1–26 (2014)
29. W. He, G.Mi, Y. Tan, Parameter optimization of local-concentration model for spam detection
by using ﬁreworks algorithm. Advances in Swarm Intelligence (Springer, Berlin 2013), pp.
439–450
30. X. Huang, Y. Tan, X.G. He, An intelligent multi-feature statistical approach for discrimination
of driving conditions of hybrid electric vehicle. IEEE Trans. Intell. Transp. Syst. 12(2), 453–456
(2011)

284
16
FWA Applications on Clustering, Pattern Recognition, and Inversion Problem
31. Y. Tan, J. Wang, Recent advances in ﬁnger vein based biometrics techniques. CAAI Trans.
Intell. Syst. 6(6), 471–482 (2011)
32. A.W.K. Kong, D. Zhang, Competitive coding scheme for palmprint veriﬁcation, in 2004 Pro-
ceedings of the 17th International Conference on Pattern Recognition. ICPR 2004, vol. 1.
(IEEE, 2004), pp. 520–523
33. X. Wu, K. Wang, D. Zhang, Palmprint authentication based on orientation code matching.
Audio-and Video-Based Biometric Person Authentication (Springer, Berlin, 2005), pp. 83–132
34. S.C. Gu, Y. Tan, X.G. He, Laplacian smoothing transform for face recognition. Sci. China
(Information Science) 53(12), 2415–2428 (2010)
35. W. Jia, D.S. Huang, D. Zhang, Palmprint veriﬁcation based on robust line orientation code.
Pattern Recognit. 41(5), 1504–1513 (2008)
36. Z. Guo, W. Zuo, L. Zhang, D. Zhang, A uniﬁed distance measurement for orientation coding
in palmprint veriﬁcation. Neurocomputing 73(4), 944–950 (2010)
37. Z. Shaoqiu, Y. Tan, A uniﬁed distance measure scheme for orientation coding in identiﬁcation,
in 2013 IEEE Congress on Information Science and Technology, (IEEE, 2013), pp. 979–985
38. A.W.K. Kong, Palmprint identiﬁcation based on generalization of iriscode. Ph.D. thesis, Uni-
versity of Waterloo (2007)
39. D. Bratton, J. Kennedy, Deﬁning a standard for particle swarm optimization, in IEEE Swarm
Intelligence Symposium, 2007. SIS 2007. (IEEE, 2007), pp. 120–127
40. R. Storn, K. Price, Differential evolution-a simple and efﬁcient heuristic for global optimization
over continuous spaces. J. Glob. Optim. 11(4), 341–359 (1997)
41. J. Kennedy, R. Eberhart, Particle swarm optimization, in Proceedings IEEE International Con-
ference on Neural Networks, vol. 4 (IEEE, 1995), pp. 1942–1948
42. Polyu
palmprint
database.
URL:
http://www.comp.polyu.edu.hk/~biometrics/
MultispectralPalmprint/MSP.htm
43. Y.Tan,J.Wang,Nonlinearblindseparationusinghigher-orderstatisticsandageneticalgorithm.
IEEE Trans. Evol. Comput. 5(6), 600–612 (2001)
44. D. Hampson, B. Russell, B. Bankhead et al., Simultaneous inversion of pre-stack seismic data.
SEG Tech. Progr. Expand. Abstr. 24, 1633–1637 (2005)

Chapter 17
Group Explosion Strategy for Multiple
Targets Search in Swarm Robotics
Swarm robotics is an emerging research area combining swarm intelligence and
robotics. Thanks to the recent achievements in optimization problem using swarm
intelligence, searching problems in swarm robotics have attracted a large number of
researchers. In searching problems, a swarm of robots searches for multiple targets
in the environment without knowing any prior knowledge about the targets. This
progress is quite similar with that of optimization problems in many aspects. More-
over, in most of the swarm robotics searching problems so far, some kinds of ﬁtness
functions are introduced for guiding the search of the swarm. This makes it a natural
advantage to introduce swarm intelligence algorithms into swarm robotics. In this
chapter, inspired by the ﬁreworks algorithm, the group explosion strategy (GES) is
proposed for searching multiple targets in swarm robotics. In the GES model, the
whole swarm is divided into several groups. Robots in a group are spatially adjacent
within the sensing range of each other. The swarm searches and collects targets in the
environment without prior knowledge. Different groups do not intersect directly and
their search for targets is parallel and independent. Through certain strategies, groups
that run into each other will be re-arranged into new groups with possibly different
members and search directions. In this way, inter-group cooperation can emerge in
the swarm. The simulation results indicate that the proposed method with GES in
this chapter shows great advantage against the comparison algorithm inspired from
PSO.
17.1 Introduction
Swarm robotics has achieved signiﬁcant progress beneﬁting from the development
of artiﬁcial intelligent [1, 2]. Many potential applications exist for the deployment of
a swarm of robots [3], especially those require large amount of agents and time or are
dangerous to human being, e.g., foraging, surveillance, monitoring and search, and
rescue operations [1, 4]. In general, these applications can be regarded as search-and-
exploretasks inunknownenvironments. Therefore, searchingstrategyis animportant
challenge for swarm robotics researchers.
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6_17
285

286
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
Controlling a swarm of robots is still a challenge in the robotic area despite its
fast development. Robots in the swarm should have as limited functions or abilities
as possible, including motion ability, energy storage, sensing, communication, and
computation capability due to their size, power constraints, cost, and maintenance
issues. Thus, cooperation plays the most important role in the swarm robotics con-
trol strategies to distribute and share resources across the swarm to complete the
task [5]. PSO [6], inspired from birds ﬂocking, is the most common swarm intel-
ligence algorithm introduced for motivating swarm robotics for its simplicity and
similarity with real robots [7]. Doctor et al. [8] are one of the ﬁrst to use PSO for
multi-robot searching though they mainly focus on optimizing the model parame-
ters. Pugh and Martinoli [9] and their follow-up work [10] designed an effective
searching algorithm inspired from PSO modiﬁed with various topologies. Hereford
and Siebold [11] developed a distributed particle swarm optimization algorithm and
used it for real robots in a physically embedded version. Xue et al. [12] presented
their PSO application for robots in target searching with a parallel asynchronous
control strategy.
However, there still remain many problems when adopting PSO in swarm
robotics searching applications, such as the disadvantages of PSO as well as dif-
ferences between optimization problems and searching applications which cannot
be neglected. These problems can be named: large amount of random movements,
trapping in local minimal, speed limitation [13], and others. To solve these problems,
some researchers divide the swarm into sub-groups for better cooperation to accel-
erate the searching progress. Xue et al. [14] introduced a mechanism for predicating
target positions using information from at least three neighbors. Couceiro et al. [15]
proposed a RDPSO that involves dynamic sub-grouping of the whole swarm. How-
ever, the main problem of this research is that the robots in the swarm require global
communication for arranging sub-groups, which is normally unavailable for large-
scale outdoor applications. Therefore, new strategies capable of solving these prob-
lems should be proposed.
Inspired from the explosion phenomenon in ﬁreworks algorithm, a swarm robotics
searching strategy, referred to as “group explosion strategy” [16], is proposed in this
chapter. In the proposed method, the entire swarm is divided into sub-groups in a
self-organizing way and each group searches for the targets independently. Robots
maintain the group structure and may split into smaller groups during the search. The
strategy provides both intra-group cooperation and inter-group cooperation within
the swarm. The sub-grouping strategy can overcome the problems mentioned above
under limited sensing constraints.
17.2 Problem Statement
In this problem, a swarm of robots is applied to solve the problem of searching multi-
ple targets in obstructive environment. The swarm has no prior information about the
environment and targets. The problem is simulated in a computer program and time

17.2 Problem Statement
287
is divided into discrete iterations. For every iteration, each individual ﬁrst retrieves
information from environment and their neighbors, and then moves according to the
sensing results. Maximum speed of robots is restricted so that robots’ movements
are guaranteed to complete before next iteration which is not too long. In real-life
applications, all robots in the swarm are not restricted to share the same iteration
cycle.
Fitness functions in swarm robotics are usually developed to measure the distance
between robot and the target(s), such as adopting Euclidean distance directly [5],
using olfaction measurements [17] or some type of potential functions [18]. However,
these functions are usually continuous and the robots can easily converge to the
target position with gradient descent methods. In real applications, on-board sensors
of swarm robotics should be as cheap and simple as possible and may not detect
the ﬁtness values in such high accuracies limited abilities. Instead, sensors provide
a limited number of rounded sensing results. Therefore, discrete ﬁtness functions,
rather than continuous one, are taken into consideration in this chapter.
The problem is quite similar with the searching problems in other researches [14,
15, 19], except that the ﬁtness values detected by the robots are discrete. The details
of the problem are deﬁned as follows: there exists m targets in the environment and
information of a target can only be sensed as ﬁtness values, which are discrete values
inversely proportional to the distance from the target, as shown in Fig.17.1. The
aim of this problem is to search and collect the targets. It takes 10 iterations for an
individual to collect a target and the cooperation of multiple individuals collecting
one target in the same time can accelerate this progress.
Each target has a randomly generated ﬁtness ranging from FMax-2 to FMax, where
FMax is a pre-deﬁned constant and is set to 20in our experiments. Considering sen-
sitivity and errors in real-time application, ﬁtness values sensed by the robots are
Fig. 17.1 A screenshot of
the problem at the beginning
of the simulation. Red circles
stand for the targets. The
background color illustrates
ﬁtness of that position.
Robots and obstacles are not
illustrated in this ﬁgure
(Color ﬁgure online)

288
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
discrete values ranging from FMax to 0. A target disappears when it is collected by
the swarm and its ﬁtness can be no longer sensed.
Each target is shaped as a circle with a radius of Sizet. The target is identiﬁed as
found if a robot overlaps with the ring and the robot can start collect the target at the
position. A ring with a same radius of Sizet outside the target has the same ﬁtness
as the target. The ﬁtness reduces by 1 when the distance from the target is increased
by 2Sizet until the ﬁtness drops to 0. This indicates that ﬁtness values are shaped as
a ring with width of 2Sizet with increased radius and reduced ﬁtness value. When
ﬁtness of several targets overlap, the largest value is adopted. Value of constant Sizet
is set to 10.
A swarm of n autonomous mobile robots is used to solve this problem. The
robots are designed to be as simple as possible and modeled as squares able to
move freely in environment. The swarm has no leader or unique IDs and share no
common coordinate systems or global position systems. Each robot can sense the
ﬁtness of its current position and has a limited sensing range to detect the relative
positions of their neighbor robots. They have a limited memory of past states of
themselves. Each robot normally does not communicate explicitly with other robots
except when sharing current ﬁtness to their neighbors. Each individual executes the
same algorithm but acts independently and asynchronously from others.
Each robot has the ability to sense neighbor robots within the range of 4Sizet.
Since no global positioning system is available, the positions are relative which can
be detected without direct communications with the help of an infrared sensor and
angle transducer. If FMax is quite small, the robots can detect neighbors’ ﬁtness
values without direct communication through colored lights equipped on the robot.
Otherwise, just like the situation in this chapter, robots share their ﬁtness values to
all their neighbors through direct communications or other strategies which is not
focused in this chapter.
The robots have a maximum speed of 2Sizet per iteration so that they can past a
ﬁtness grade in one iteration at the maximum speed and react quickly to the ﬁtness
changes if they are searching at the right directions. The individuals also have the
ability to maintain last 10 history states including past position and the corresponding
ﬁtness values. Positions in the history are relative positions updated according to the
local coordinating system of the robot. Past states cannot be shared among the swarm
since complex communications and localizations are required for sharing the past
states.
Static obstacles are also introduced in the problem. Each obstacle is regarded as a
square with different sizes. Collisions are detected every iteration in the simulation
and any robots that run into an obstacle are considered as broken and will be removed
from the swarm permanently while the obstacle remains still in the environment.

17.3 Group Explosion Strategy
289
17.3 Group Explosion Strategy
17.3.1 Introducing Fireworks Explosion into Swarm Robotics
In this section, the group explosion strategy (GES) designed for searching multiple
targets is explained in detail. In GES model, the whole swarm is divided into several
groups. Robots in a group are spatially adjacent, i.e., within the sensing range of
each other. Different groups do not intersect directly and their search for targets is
parallel and independent. However, through certain strategies, groups that run into
each other will be re-arranged into new groups with possibly different members and
search directions. In this way, inter-group cooperation can emerge in the swarm.
Explosion strategies are introduced into GES for searching multiple targets. The
comparison of an explosion process and the corresponding actions in GES is shown in
Fig.17.2 which are inspired from the ﬁreworks algorithm for optimization problems
in [20], since both algorithms are inspired from explosion phenomenon. An explosion
starts from a point and generates several sparks with different distances around the
initial point. The center point can be regarded as a robot and the sparks exploded
are the neighbor robots. These robots as a whole aggregates into a group, randomly
distributed around the center robot just like the sparks. In each iteration, robots
process sensing data and decide their movements. In this way, a new explosion
center is selected and the group explodes again in the next iteration to search near
the new center.
Explosion
Strategies
Explosion Sparks
Explosion Amplitude
New Explosion
GES
Robot Group
Distance with Neighbors
Moving of Entire Group
Fig. 17.2 Introducing explosion strategies into swarm robotics

290
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
17.3.2 Framework of Group Explosion Strategy
A group of robots can converge to the target much more quickly than a single robot
with the help of intra-group cooperation, since the trends of the ﬁtness in the envi-
ronment are clearer within the group than a single individual. The more the robots
in a group, the quicker the group can converge to a target, yet resulting in fewer
targets, and the swarm is searching simultaneously since the number of groups is
reduced. If the sizes of groups become too large, searching efﬁciency of the entire
swarm declines instead. Therefore, a balanced group size should be adopted and the
swarm can thus take the advantage of quick convergence from intra-group coop-
eration as well as searching several targets in parallel as inter-group cooperation.
With a carefully designed strategy, the swarm can search and collect the targets more
efﬁciently.
In GES model, robots ﬁrst retrieve information from the environment and their
neighbors, then calculate their new movements of this iteration (referred as velocity)
according to the sensing data, and carry out the move before next iteration. The state
of this iteration is stored in history before robots actually move towards the new
positions. In this way, the history of each robot contains 10 past states of the robots
excluding their current states.
Velocity of robot i at every iteration consists two components: grouping compo-
nent G(i) and history component H(i). G(i) controls the robots’ behavior relevant
to grouping and is calculated according to the current ﬁtness of the robot and states
of its neighbor including their relative positions and ﬁtness values. H(i) is computed
from the past history states stored in the robot.
The grouping behavior of a robot differs with the size of its current group, i.e.,
the number of robots in its neighborhood. Group size is controlled by a pre-deﬁned
threshold βG. When the size exceeds the threshold, robots in the group try to split
this group into two smaller ones to balance the search efﬁciency. Otherwise, robots
try to maintain the group and take advantage of the information shared in the group to
beneﬁt searching progress.
A brief ﬂow chart of the GES strategy is shown in Fig.17.3. Each robot is regarded
as a ﬁnite-state machine with three states: group search, split groups, and collect
target. Expressions of G(i) for these two situations are explained in Sects.17.3.3
and 17.3.4, respectively. Section17.3.5 gives the expression of H(i) and the ﬁnal
velocity update equation is presented in Sect.17.3.6. In collect state, robot stays
still at its position until the target is collected and goes back to the two searching
states according to the size of the group. Several robots collecting same target can
accelerate this process as mentioned in previous section.

17.3 Group Explosion Strategy
291
Fig. 17.3 Flow chart of GES
17.3.3 Searching in Groups
WhenthesizeofgroupisbeyondthethresholdβG,thegrouptriestosearchforatarget
through intra-group cooperation. Following the strategies in ﬁreworks explosion,
searching in next iteration should take place in the area around the current best
position found in the group. Therefore, the searching strategy of the group is to move
the group center towards the best position within the group. The best position is
selected as the current position of the robot with highest ﬁtness value among all the
robots in the group. Historical positions of any robots are not taken into account,
since the communication overload for exchanging historical states is too large. In
this way, the group should steadily converge to a target much quicker than only one
robot.
The group center is calculated as the centroid of all the robots in the group. The
group center C(i) for robot i is calculated as follows:
C(i) =

j∈N(i) P( j) + P(i)
|N(i)| + 1
,
(17.1)
where P(i) is the current position of robot i and N(i) is the collection consisting of
all neighbor robots of robot i, i.e., all other robots within the group.
It can be easily seen from Eq.(17.1) that although robots do not exchange their
positions with each other directly, they can calculate the same centroid of the group
if no noise and error is considered. Even with little noise and error from the sensors,
the result could be quite similar and should not affect the cooperative scheme.
Given the group center, the robot can compute the grouping component G(i)
used in the velocity update equation. Before calculating, the robots in the group
should exchange the ﬁtness information among the group with the help of direct

292
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
communication or colored lights due to the hardware design of the robots. G(i) is
computed using Eq.(17.2).
G(i) = (P(b) −C(i)) ∗RS,
(17.2)
where robot b is the robot with the best ﬁtness in the group and RS is a scaling factor
randomly selected from three values with equal possibilities: 1 −βS, 1, and 1 + βS.
βS is used to adjust the shape of the group by controlling the distance of robots from
the group center. Robots with various distances from the center can provide the group
with great diversity and meaningful feedbacks in different scales for choosing the
searching direction.
It should be noted in GES that robots do not explicitly tell their neighbors which
robot has the best ﬁtness. Instead, they exchange their ﬁtness with all their neighbors.
Each robot computes the best robot according to the ﬁtness values it receives inde-
pendently. Therefore, robots in the group may choose different best robots if noise or
error occurs. However, the sensing distance is twice the time of the maximum speed
of the robot, so it could be possible that the robot still remains in the group. If a robot
goes out of the group, it will start searching the targets itself and rejoin a group if
encounters one. The most possible place for rejoining a group is near a target, since
robots always try to converge into a target even if a single robot without group.
17.3.4 Splitting Groups
When group size exceeds the threshold βG, the strategy for the robots in the group
is splitting the large group into two smaller ones. Robots with the best two ﬁtness,
denoted as L1 and L2, are selected as two leaders for the new groups. Without loss of
generality, it is assumed that ﬁtness of L1 is not worse than L2. To separate the two
groups apart, two opposite directions are selected for the new groups. Two leaders
repulse with each other and try to be as far away as possible. The repulsive vector is
calculated in Eq.(17.3):
R(L1) = (P(L1) −P(L2)) ∗βR
R(L2) = (P(L2) −P(L1)) ∗βR ,
(17.3)
where βR is a pre-deﬁned coefﬁcient for repulsing the two leaders L1 and L2.
As for other robots in the group, each of them selects a leader to follow indepen-
dently and randomly. Each of L1 and L2 is given a weight for selecting, the higher
the weight, the more change a leader is selected since a leader with higher ﬁtness
is more possible to ﬁnd a target. Besides, the leaders are repulse to each other and
thus the robot L1 is quite possibly being repulsed towards a target. The weights of
two leaders are assigned based on their ﬁtness in Eq.(17.4) and the possibility for
selecting leaders is calculated in Eq.(17.5):
w(L1) = F(L1) −F(L2) + βP, w(L2) = βP,
(17.4)

17.3 Group Explosion Strategy
293
PL1 =
w(L1)
w(L1) + w(L2), PL2 =
w(L2)
w(L1) + w(L2),
(17.5)
where function F indicates the current ﬁtness of the robot; w(L1), w(L2), PL1,
and PL2 are the weights and possibilities of two leaders, respectively; and βP is a
constant to balance the weights and ﬁxed to 1in this chapter.
Since the sensing range of a robot is double of size of a ﬁtness value can occupy,
the value of F(L1) −F(L2) is restricted to be zero or one. This guarantees that the
difference between the two weights is not too large that makes the new group sizes
very unbalanced.
The grouping components of the leaders and other robots can be now calculated
using Eq.(17.6):
G(i) =

R(i)
,
i = L1, L2
R(i) + (P(l) −P(i)) ∗RS,
otherwise
,
(17.6)
where l is the leader robot i has selected. The leaders just repulse each other to
separate the groups. Other robots use the same repulse vector with the leader and the
new groups move apart as a whole to separate with each other. Besides separating,
distances between robots and their leaders are also changed by the factor RS which
is the same as mentioned in previous section.
17.3.5 History State Storage
History component H(i) is independent with the grouping situations and is computed
based only on the stored states in robot’s history. In GES, only ten latest history states
(position and ﬁtness) are maintained. When robots search in a wrong direction and
depart the targets (ﬁtness observed is decreasing), they can get back to route if making
full use of the history components as computed in Eq.(17.7):
H(i) = (P(i) −h(i)) ∗r,
(17.7)
where h(i) is the position of the history state with the best ﬁtness and r is a random
number uniformly distributed within the range [0.4, 0.8].
If several history states share the same ﬁtness, the most recent position is selected.
Note that when counting h(i), current position is also taken into account to make
sure that the robot is not attracted by a worse position. Thus, the history component
is set to be 0 if ﬁtness of current position is better than all the states in history and
will not contribute to the velocity update of the robot as the history state can provide
no positive information to guide the search.

294
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
17.3.6 Velocity Update Equation
After calculating the two components G(i) and H(i), the velocity of robot i at
iteration t, denoted as Vt(i), is updated as follows:
Vt(i) =
⎧
⎨
⎩
G(i) + H(i), ∥G(i)∥> 0
H(i) + Rp , ∥G(i)∥= 0 ∧∥H(i)∥> 0
Vt−1(i)
, ∥G(i)∥= 0 ∧∥H(i)∥= 0
,
(17.8)
where RP is a randomly generated unit vector and Vt−1(i) is the velocity of the last
iteration.
In the equation, velocity update strategy is different if ∥G(i)∥is 0. Since no
grouping action is taking place when ∥G(i)∥= 0, only the history component
remains in the equation. This will lead to a vibration around the history best position
if the robot gets stuck in an area with same ﬁtness value. A small random vector RP
is introduced to avoid such situation. RP is an unit vector while H(i) is normally
quite large, so the movement of the robot is not too stochastic.
The situation that both G(i) and H(i) are 0 usually occurs when the robot is
the best of the group and ﬁtness is improving in recent iterations, so the robot just
remains its searching direction as the previous iteration.
17.3.7 Obstacle Avoidance
Obstacles are also considered in GES. Obstacle avoidance is computed stand alone
after the velocity update and is not used in the Vt−1(i) in Eq.(17.8). Since obstacle
avoidance is not the main concern of this problem, a simple avoiding scheme is used
for both GES and the baseline algorithm (introduced in Sect.17.4.1). Each robot
checks if it will run into any obstacles with the velocity Vt(i). If so, it adds a small
repulsive force perpendicular to Vt(i) from the obstacle to make sure that a collision
will not happen. This simple scheme can provide acceptable performance for obstacle
avoidance from the simulation results in Sect.17.4.4.
17.4 Simulation Results and Discussions
In this section, comparison between GES and a searching algorithm inspired from
standard PSO is presented. We ﬁrst introduce the baseline algorithm brieﬂy and
present the simulation results of several experiments. Two methods are simulated
in a self-built simulation platform [21] and tested under different situations. The
ﬁrst experiment is to validate the GES to see if it is capable of solving the problem.
Various population sizes and number of targets are considered in this experiment.

17.4 Simulation Results and Discussions
295
The second experiment is the scalable experiment which has the same condition as
the ﬁrst one except the scales of swarm size, and a number of target and map sizes are
enlarged. Obstacles are introduced in the last experiment and results are compared
with the same situation without obstacles.
In most of the experiments, the stop criteria are to collect a certain percent of the
targets and the essential standard for judging the performance is the total iteration
used. For the problem deﬁnition, it can be easily seen that ﬁtness becomes inadequate
as large areas with 0 ﬁtness appear in the environment as targets are collected or in
large maps. This means that the search of a swarm becomes harder as the experiment
goes.
All the experiments in this section use the same environment setup: 20 randomly
generated maps are used and each method is repeated for 20 times in the map of size
500*500. Average results of these 400 runs are presented in results. Parameters of
the two methods are tuned in advance with 10 robots and 20 targets.
17.4.1 Baseline Algorithm
In the experiment, the baseline algorithm uses the same strategy of RPSO in [15].
Each robot acts as a particle and the spacial-based topology of the robots for calcu-
lating gbest is adopted. However, RPSO requires a large communication range for
the swarm which is restricted in the problem we consider, thus a little modiﬁcation
is introduced. In case the robot vibrates in an area, the small random vector RP in
GES is introduced if both pbest and gbest are the current position. Except the veloc-
ity update strategy, baseline algorithm shares the same scheme with GES for better
comparison, such as obstacle avoiding and history state updating.
17.4.2 Validation Experiment
The ﬁrst experiment validates the GES method and compares the iterations used for
two methods to collect half and all the targets in the environment, respectively. The
experiment is simulated in a small-scale setup: map size is 500*500, population n
ranges from 5 to 25, and the number of targets m varies from 10 to 30.
The results are shown in Tabel17.1. In the table, results for collecting half targets
and all targets are presented in different columns. “Iteration” stands for the itera-
tions used to collect the targets and is the most important criterion to evaluate the
performance. “Collect” stands for the targets collected in the “Half Target” situation
as several targets may be collected in the same iteration. “Distance” stands for the
averaged moving distance for each robot in the entire searching process, and “Time”
indicates the average simulation time on PC in milliseconds which is used to eval-
uate the computation overload. The last two columns indicate the ratio of iterations
between GES and RPSO.

296
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
Table 17.1 Validation results
m
n
GES
RPSO
GES/RPSO
Half target
All target
Half target
All target
Iteration
Iteration
Collect
Iteration
Distance
Time
Iteration
Collect
Iteration
Distance
Time
Half(%)
Full(%)
10
5
162.6
5.0
369.66
3285.16
238.2
178.7
5.0
763.67
3184.45
243.5
91.0
48.4
10
133.4
5.0
271.09
2414.35
242.4
145.1
5.0
558.92
2456.72
250.9
91.9
48.5
15
124.1
5.0
233.11
2066.17
246.4
132.2
5.0
487.40
2200.03
259.2
93.9
47.8
20
123.4
5.0
215.86
1905.82
252.7
124.1
5.0
457.59
2094.18
266.8
99.4
47.2
25
125.7
5.0
206.81
1819.75
259.9
122.3
5.0
424.48
1973.48
275.5
102.7
48.7
20
5
273.8
10.0
574.06
4951.14
992.9
277.1
10.0
826.82
4151.13
997.1
98.8
69.4
10
191.5
10.0
388.22
3372.75
1000.1
226.7
10.0
600.58
3335.97
1000.8
84.5
64.7
15
159.6
10.0
318.80
2774.25
1001.2
201.8
10.0
529.61
3054.10
1014.0
79.1
60.2
20
152.4
10.1
285.26
2481.51
1010.1
190.3
10.0
487.58
2885.94
1024.9
80.1
58.5
25
155.1
10.1
273.29
2378.51
1017.0
185.6
10.0
471.03
2822.60
1037.4
83.5
58.0
30
5
339.8
15.0
728.57
6167.18
2265.0
336.7
15.0
972.59
5095.47
2256.0
100.9
74.9
10
217.5
15.0
457.15
3891.14
2267.2
255.6
15.0
676.36
3934.55
2273.1
85.1
67.6
15
188.1
15.0
373.94
3202.25
2268.1
237.0
15.0
595.55
3657.55
2275.6
79.3
62.8
20
171.2
15.0
327.33
2811.04
2272.7
221.4
15.0
550.29
3479.02
2293.6
77.4
59.5
25
175.3
15.1
315.63
2723.57
2291.4
203.7
15.1
507.25
3260.95
2307.8
86.1
62.2

17.4 Simulation Results and Discussions
297
From the table, it can be easily seen that GES dominates RPSO in the performance
regardless of population and number of targets, especially when collecting all targets.
GEScan complete most of the missions with only 70–90 and 50–70% of the iterations
than that of RPSO in “Half” and “All” situations, respectively. It also shows that GES
has a smaller total moving distance which means that GES is more energy efﬁcient
than RPSO. Considering the smaller iterations, GES utilizes the maximum speed
much better than RPSO which is one of the important reasons why GES has a better
efﬁciency. In the table, GES also has a shorter time which indicates that the GES
strategy requires less computation resources than RPSO. This is important for swarm
robotics applications since the robots normally do not have powerful computation
abilities.
The results also show that GES outperforms RPSO more when population size
is larger. This indicates that the strategy introduced in GES shows great ability in
cooperation among robots and can accelerate the searching process when there are
more robots in the environment. When the number of targets increases, the average
iteration GES used to collect one target decreases quicker than RPSO which also
indicates that the strategy is taking full advantage of cooperation among robots.
17.4.3 Scalable Experiment
The main aim of this experiment is to see if the proposed GES strategy can scale to
problems with larger number of robots, targets, or map sizes. In this experiment, envi-
ronment is setup much bigger than previous experiments: map size is 1000*1000,
n ranges from 10 to 50, and m varies from 10 to 100. Both methods use the same
parameter with the previous experiments. Our results showed that the trends of two
algorithms are quite the same than in previous sections. GES has great advantages
when population is large. It can also be induced that the advantages are not that large
when the number of targets is increased. The main reason is that the map size is not
large enough, so the environment is full of ﬁtness values even when only 25% of
targets remains. As shown in previous section, advantage of GES is smaller when
ﬁtness is adequate.
17.4.4 Obstacle Avoidance Experiment
In this experiment, we test the performance of the proposed algorithm in environ-
ments with small static obstacles. Obstacles are randomly distributed in the environ-
ment and the number ranges from 0 to 100. Performances in obstacle environments
are compared with that of non-obstacle environment. In this experiment, the changes
of the two numbers, population m and number of targets n, are tested in a more
conscientious way, with a step of 2 instead of 5 and 10in previous experiments.
Three main criteria are considered in this experiment: number of surviving robots,

298
17
Group Explosion Strategy for Multiple Targets Search in Swarm Robotics
“Iteration,” and “Distance.” The last one indicates the remaining robots after the
simulation and is used to judge how many collisions were taking place. The stop
criteria are set as collecting 75% of the targets.
Our results showed that GES shows advantages in avoiding the obstacles than
RPSO even with the same obstacle avoidance strategy. There are possibly two reasons
for such results. The ﬁrst one is that GES takes much less iterations than RPSO
and thus has a lower possibility of encountering obstacles. The second one may be
the cooperation schemes in GES. Although the strategy does not include a direct
cooperation of obstacle avoidance, the swarm do beneﬁt from the cooperation. The
robots that successfully avoided any obstacles usually have lower ﬁtness values in
the group since they took a longer way round. Therefore, the groups will possibly
move towards other directions and thus avoid the obstacles indirectly.
The trends of “Iteration” and “Distance” are very similar for both GES and RPSO
strategies. Robots take shorter moves per iteration to avoid more obstacles which
leads to larger iterations. However, it is obvious that GES has a much better stability
than RPSO when the environment setups change, as the points in the ﬁgure of GES
are much more compact. Stability can be also indicated from the trend of the two
algorithmsasRPSOismoresloped.Ifcomparingthemeanvalueofalltheresultswith
same number of obstacles, two strategies are quite the same when few obstacles exist,
but GES becomes better when the number of obstacles increases. Considering the big
differences of “Iteration” and “Distance” from the results of previous experiments,
GES deﬁnitely outperforms RPSO in environments with obstacles.
17.5 Summary
Inspired by the Fireworks Algorithm, a group explosion strategy (GES) for search-
ing multiple targets was proposed, which is applied to the multiple targets searching
problem on a self-built simulation platform. The swarm searches and collects targets
in the environment without prior knowledge. Several tests are run to evaluate how
GES performs in various aspects including stability, robustness, and ﬂexibility. Sim-
ulation results demonstrate that GES shows great efﬁciency when ﬁtness is either
adequate or inadequate in the environment. GES also shows good stability in obstruc-
tive and large-scale environments. These results indicate that the GES strategy has
great ability in cooperating robots to accomplish the tasks.
References
1. Y. Tan, Z.Y. Zheng, Research advance in swarm robotics. Def. Technol. 9(1), 31–62 (2013)
2. Q. Tang, P. Eberhard, Cooperative motion of swarm mobile robots based on particle swarm
optimization and multibody system dynamics. Mech. Based Des. Struct. Mach. 39(2), 179–193
(2011)

References
299
3. R. Grabowski, L.E. Navarro-Serment, P.K. Khosla, An army of small robots. Sci. Am. 18,
34–39 (2008)
4. Y. Tan, Swarm robotics: collective behavior inspired by nature. J. Comput. Sci. Syst. Biol.
(JCSB) (2013)
5. K. Derr, M. Manic, Multi-robot, multi-target particle swarm optimization search in noisy wire-
less environments, in 2009 2nd Conference on Human System Interactions. HSI’09 (IEEE,
2009), pp. 81–86
6. R. Eberhart, J. Kennedy, A new optimizer using particle swarm theory, in 1995 Proceedings
of the Sixth International Symposium on Micro Machine and Human Science, MHS’95 (IEEE,
1995), pp. 39–43
7. D. Gong, C. Qi, Y. Zhang, M. Li, Modiﬁed particle swarm optimization for odor source local-
ization of multi-robot, in 2011 IEEE Congress on Evolutionary Computation (CEC). (IEEE,
2011), pp. 130–136
8. S. Doctor, G.K. Venayagamoorthy, V.G. Gudise, Optimal PSO for collective robotic search
applications, in 2004 Congress on Evolutionary Computation, CEC2004, vol. 2 (IEEE, 2004),
pp. 1390–1395
9. J. Pugh, A. Martinoli, Multi-robot learning with particle swarm optimization, in Proceedings
of the Fifth International Joint Conference on Autonomous Agents and Multiagent Systems
(ACM, 2006), pp. 441–448
10. J. Pugh, A. Martinoli, Inspiring and modeling multi-robot search with particle swarm opti-
mization, in 2007 IEEE Swarm Intelligence Symposium, SIS 2007 (IEEE, 2007), pp. 332–339
11. J. Hereford, M. Siebold, Multi-robot search using a physically-embedded particle swarm opti-
mization. Int. J. Comput. Intell. Res. 4(2), 197–209 (2008)
12. S. Xue, J. Zhang, J. Zeng, Parallel asynchronous control strategy for target search with swarm
robots. Int. J. Bio-Inspired Comput. 1(3), 151–163 (2009)
13. J.G. Li, J. Yang, S.G. Cui, L.H. Geng, Speed limitation of a mobile robot and methodology of
tracing odor plume in airﬂow environments. Procedia Eng. 15, 1041–1045 (2011)
14. S. Xue, Y. Zan, J. Zeng, Z. Xue, J. Du, Group decision making aided PSO-type swarm robotic
search, in 2012 International Symposium on Computer, Consumer and Control (IS3C) (IEEE,
2012), pp. 785–788
15. M.S. Couceiro, R.P. Rocha, N.M.F. Ferreira, A novel multi-robot exploration approach based
on particle swarm optimization algorithms, in 2011 IEEE International Symposium on Safety,
Security, and Rescue Robotics (SSRR) (IEEE, 2011), pp. 327–332
16. Z. Zheng, Y. Tan, Group explosion strategy for searching multiple targets using swarm robotic,
in IEEE Congress Evolutionary Computation (2013), pp. 821–828
17. A. Marjovi, J. Nunes, P. Sousa, R. Faria, L. Marques, An olfactory-based robot swarm navi-
gation method, in 2010 IEEE International Conference on Robotics and Automation (ICRA)
(IEEE, 2010), pp. 4958–4963
18. H.E. Espitia, J.I. Sofrony, Path planning of mobile robots using potential ﬁelds and swarms
of Brownian particles, in 2011 IEEE Congress on Evolutionary Computation (CEC). (IEEE,
2011), pp. 123–129
19. Q. Tang, P. Eberhard, A PSO-based algorithm designed for a swarm of mobile robots. Struct.
Multidiscip. Optim. 44(4), 483–498 (2011)
20. Y. Tan, Y. Zhu, Fireworks algorithm for optimization, in Advances in Swarm Intelligence
(Springer, Berlin, 2010), pp. 355–364
21. Z. Zheng, Y. Tan, An indexed K-D tree for neighborhood generation in swarm robotics simula-
tion. Advances in Swarm Intelligence. Lecture Notes in Computer Science, vol. 7929 (Springer,
Berlin, 2013), pp. 53–62

Postscript
This book does not end here, neither does the research on FWA. On the contrary, it
seems to me the age of FWA has just begun, giving the bulk of challenges before
us waiting to be dealt with. Considering the length of the book, I have to come to a
temporary stop in the introduction of FWA. Stop for a deeper thinking. Regarding
FWA research, what does the future hold in store for it, what are the most urgent
problems to be answered? All of these problems haunt me and I think a tentative stop
is for a better unveiling of the mystery.
After ﬁve years of FWA research, it has already become an effective and efﬁcient
swarm intelligence optimization algorithm whose performance catches up with or
even exceeds most state-of-the-art swarm intelligence optimization algorithms, and
becomes a new efﬁcient tool for solving complex optimization problems. Neverthe-
less, FWA is still in the stage of infancy. Many areas are yet to be explored and novel
ideas are increasingly emerging.
The future research of FWA may happen in the following areas:
1. Theoretical analysis. Including global convergence, computational efﬁciency,
parameter setting, etc.
2. Performance improvement. One of the most important topics is to establish efﬁ-
cient cooperation mechanisms in FWA.
3. Hybrid algorithms. Combination of FWA with other SI algorithms.
4. Problem solving in the scenario of big data. As big-data application becomes
popular and important, research on FWA for big data is a must-have topic.
5. Dynamic optimization problems such as dynamic object or target tracking and
multi-targets tracking in search of swarm robots.
6. Applications in a variety of real-world problems.
Till now from accomplishing the manuscript of the book, the CIL team at Peking
University has made several signiﬁcant progresses on FWA study, including how to
fully utilize the acquired/informed objective information assisting the swarm’s future
search, i.e., raising information utility and how to sufﬁciently enhance information
sharing about each ﬁrework as well as cooperative evolutionary ability so as to fully
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6
301

302
Postscript
bring the cooperative mechanism of a swarm into play for enhancing the solving
capability of the swarm.
In the meantime, we have also made some beneﬁcial contributions to pushing the
frontiers of FWA research and organizing several forums and platforms for academic
opinion exchanges.
First of all, I had organized a special issue in the International Journal of Swarm
Intelligence Research (IJSIR) named “Developments and Applications of Fireworks
Algorithm,” which received widespread attention. We received a large number of
research papers on FWA. After a strict and vigorous reviewing process, ﬁnally only
ﬁve high-quality research papers were accepted. By the way, this special issue will
appear in IJSIR-2015, Vol. 6, no. 2, which shows the epitome of FWA researches at
present.
Second, we have organized and hosted a special session on FWA at IEEE
CEC’2015 at Sendai, Japan, in which six papers are thoroughly discussed on this
session that further indicates the hotness and prosperity of FWA researches. Fur-
thermore, there are a few FWA research papers to be reported in regular sessions
of The Sixth International Conference on Swarm Intelligence held in conjunction
with The Second BRICS Congress on Computational Intelligence (ICSI-CCI’2015)
which was successfully held during June 25–29, 2015, in BICC, Beijing, China.
Finally, other research institutes and universities both domestic and international
have also published a number of research papers featuring FWA. I have received
numerous emails and phone calls from scholars interested in the development and
future prospects of FWA, and deeply felt their enthusiasm and desires for exploring
FWA further.
It is shown from all of these works and papers that FWA is harvesting acknowl-
edgment and attention from researchers and practitioners, and FWA’s increasing
computing efﬁcacy is paving the road for more applications. FWA has widespread
applications in various ﬁelds attracting more and more researchers to devote them-
selves to the research of FWA.
I believe that as more and more researchers join forces, research on FWA will be
propelled to a new height, turning it, no doubt, into a mainstream swarm intelligence
algorithm in the near future.

Appendix A
Benchmark Suites
A.1 Benchmark Suite 1
A.1.1 Sphere
A.1.1.1 Expression
f (x) =
D

i=1
xi 2
(A.1)
A.1.1.2 Parameters
Range: [−100, 100], Optimum point: 0D, Optimum value: 0, Dimension: 30
A.1.1.3 Image
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6
303

304
Appendix A: Benchmark Suites
A.1.2 Rosenbrock
A.1.2.1 Expression
f (x) =
D−1

i=1
(100(xi+1 −xi 2)
2 + (xi −1)2)
(A.2)
A.1.2.2 Parameters
Range: [−30, 30], Optimum point: 1D, Optimum value: 0, Dimension: 30
A.1.2.3 Image
A.1.3 Griewank
A.1.3.1 Expression
f (x) = 1 +
D

i=1
xi 2
4000 +
D

i=1
cos
 xi
√
i

(A.3)
A.1.3.2 Parameters
Range: [−600, 600], Optimum point: 0D, Optimum value: 0, Dimension: 30

Appendix A: Benchmark Suites
305
A.1.3.3 Image
A.1.4 Rastrigin
A.1.4.1 Expression
f (x) =
D

i=1
(x2
i −10 cos(2πxi) + 10)
(A.4)
A.1.4.2 Parameters
Range: [−5.12, 5.12], Optimum point: 0D, Optimum value: 0, Dimension: 30
A.1.4.3 Image

306
Appendix A: Benchmark Suites
A.1.5 Schwefel’s Problem 1.2
A.1.5.1 Expression
f (x) =
D

i=1
⎛
⎝
i
j=1
x j
⎞
⎠
2
(A.5)
A.1.5.2 Parameters
Range: [−100, 100], Optimum point: 0D, Optimum value: 0, Dimension: 30
A.1.5.3 Image
A.1.6 Ackley
A.1.6.1 Expression
f (x) = −20 exp
⎛
⎝−0.2



 1
D
D

i=1
x2
i
⎞
⎠−exp

1
D
D

i=1
cos(2πxi)

+ 20 + e (A.6)
A.1.6.2 Parameters
Range: [−32, 32], Optimum point: 0D, Optimum value: 0, Dimension: 30

Appendix A: Benchmark Suites
307
A.1.6.3 Image
A.1.7 Penalized
A.1.7.1 Expression
f (x) = 0.1

sin2(3πx1) +
D−1

i=1
(xi −1)2[1 + sin2(3πxi+1)]
+ (xD −1)2[1 + sin2(2πxD)]

+
D

i=1
μ(xi, 5, 100, 4),
(A.7)
where
μ(xi, a, k, m) =
⎧
⎨
⎩
k(xi −a)m
xi > a
0
a ≤xi ≤a
k(−xi −a)m
xi < −a
(A.8)
A.1.7.2 Parameters
Range: [−50, 50], Optimum point: 1D, Optimum value: 0, Dimension: 30

308
Appendix A: Benchmark Suites
A.1.7.3 Image
A.1.8 Six-Hump Camel-Back
A.1.8.1 Expression
f (x) = 4x12 −2.1x14 + x16
3 + x1x2 −4x22 + 4x24
(A.9)
A.1.8.2 Parameters
Range: [−5, 5], Optimum point: (−0.08983, 0.7126), (0.08983, −0.7126), Opti-
mum value: −1.032, Dimension: 2
A.1.8.3 Image

Appendix A: Benchmark Suites
309
A.1.9 Goldstein-Price
A.1.9.1 Expression
f (x) = [1 + (x1 + x2 + 1)2(19 −14x1 + 3x12 −14x2 + 6x1x2 + 3x22)]
· [30 + (2x1 −3x2)2(18 −32x1 + 12x12 + 48x2 −36x1x2 + 27x22)]
(A.10)
A.1.9.2 Parameters
Range: [−2, 2], Optimum point: (0, −1) , Optimum value: 3, Dimension: 2
A.1.9.3 Image
A.1.10 Schaffer’s F6
A.1.10.1 Expression
f (x) =
sin2

x2
1 + x2
2 −0.5
[1 + 0.001(x2
1 + x2
2)]2 + 0.5
(A.11)
A.1.10.2 Parameters
Range: [−100, 100], Optimum point: 0D, Optimum value: 0, Dimension: 2

310
Appendix A: Benchmark Suites
A.1.10.3 Image
A.1.11 Axis Parallel Hyper Ellipsoid
A.1.11.1 Expression
f (x) =
D

i=1
ix2
i
(A.12)
A.1.11.2 Parameters
Range: [−5.12, 5.12], Optimum point: 0D , Optimum value: 0, Dimension: 30
A.1.11.3 Image

Appendix A: Benchmark Suites
311
A.1.12 Rotated Hyper Ellipsoid
A.1.12.1 Expression
f (x) =
D

i=1
⎛
⎝
i
j=1
x2
j
⎞
⎠
2
.
(A.13)
A.1.12.2 Parameters
Range: [−65.536, 65.536], Optimum point: 0D, Optimum value: 0, Dimension: 30.
A.1.12.3 Image
Table A.1 Test functions in
Chap.2
Number
Function name
1
Sphere
2
Rosenbrock
3
Griewank
4
Rastrigin

312
Appendix A: Benchmark Suites
Table A.2 Test functions in
Chap.6
Number
Function name
1
Sphere
5
Schwefels Problem 1.2
2
Rosenbrock
6
Ackley
3
Griewank
4
Rastrigin
7
Penalized
8
Six-Hump Camel-Back
9
Goldstein-Price
10
Schaffers F6
11
Axis Parallel Hyper Ellipsoid
12
Rotated Hyper Ellipsoid
A.2 Benchmark Suite 2
Table A.3 25 Benchmark functions in CEC2005 [1]
Number
Function name
Unimodal functions
1
Shifted Sphere Function
2
Shifted Schwefel’s Problem 1.2
3
Shifted Rotated High Conditioned Elliptic Function
4
Shifted Schwefel’s Problem 1.2 with Noise in Fitness
5
Schwefel’s Problem 2.6 with Global Optimum on
Bounds
Basic functions
6
Shifted Rosenbrock’s Function
7
Shifted Rotated Griewank’s Function without
Bounds
8
Shifted Rotated Ackley’s Function with Global
Optimum on Bounds
9
Shifted Rastrigin’s Function
10
Shifted Rotated Rastrigin’s Function
11
Shifted Rotated Weierstrass Function
12
Schwefel’s Problem 2.13
Expanded functions
13
Expanded Extended Griewank’s plus Rosenbrock’s
Function (F8F2)
14
Shifted Rotated Expanded Scaffer’s F6
(continued)

Appendix A: Benchmark Suites
313
Table A.3 (continued)
Number
Function name
Hybrid composition functions
15
Hybrid Composition Function
16
Rotated Hybrid Composition Function
17
Rotated Hybrid Composition Function with Noise in
Fitness
18
Rotated Hybrid Composition Function
19
Rotated Hybrid Composition Function with a
Narrow Basin for the Global Optimum
20
Rotated Hybrid Composition Function with the
Global Optimum on the Bounds
21
Rotated Hybrid Composition Function
22
Rotated Hybrid Composition Function with High
Condition Number Matrix
23
Non-Continuous Rotated Hybrid Composition
Function
24
Rotated Hybrid Composition Function
25
Rotated Hybrid Composition Function without
Bounds
A.3 Benchmark Suite 3
Table A.4 28 Benchmark functions in CEC2013 [2]
Number
Function name
Unimodal functions
1
Sphere Function
2
Rotated High Conditioned Elliptic Function
3
Rotated Bent Cigar Function
4
Rotated Discus Function
5
Different Powers Function
Basic multimodal functions
6
Rotated Rosenbrock’s Function
7
Rotated Schaffers F7 Function
8
Rotated Ackley’s Function
9
Rotated Ackley’s Function
10
Rotated Griewank’s Function
11
Rastrigin’s Function
12
Rotated Rastrigin’s Function
13
Non-Continuous Rotated Rastrigin’s Function
(continued)

314
Appendix A: Benchmark Suites
Table A.4 (continued)
Number
Function name
14
Schwefel’s Function
15
Rotated Schwefel’s Function
16
Rotated Katsuura Function
17
Lunacek Bi_Rastrigin Function
18
Rotated Lunacek Bi_Rastrigin Function
19
Expanded Griewank’s plus Rosenbrock’s Function
20
Expanded Scaffer’s F6 Function
Composition functions
21
Composition Function 1 (N = 5, Rotated)
22
Composition Function 2 (N = 3, Unrotated)
23
Composition Function 3 (N = 3, Rotated)
24
Composition Function 4 (N = 3, Rotated)
25
Composition Function 5 (N = 3, Rotated)
26
Composition Function 6 (N = 5, Rotated)
27
Composition Function 7 (N = 5, Rotated)
28
Composition Function 8 (N = 5, Rotated)
Note N is the number of the functions that are combined
A.4 Benchmark Suite 4
Table A.5 30 Benchmark functions in CEC2014 [3]
Number
Function name
Unimodal functions
1
Rotated High Conditioned Elliptic Function
2
Rotated Bent Cigar Function
3
Rotated Discus Function
Simple multimodal functions
4
Shifted and Rotated Rosenbrock’s Function
5
Shifted and Rotated Ackley’s Function
6
Shifted and Rotated Weierstrass Function
7
Shifted and Rotated Griewank’s Function
8
Shifted Rastrigin’s Function
9
Shifted and Rotated Rastrigin’s Function
10
Shifted Schwefel’s Function
11
Shifted and Rotated Schwefel’s Function
12
Shifted and Rotated Katsuura Function
13
Shifted and Rotated HappyCat Function
(continued)

Appendix A: Benchmark Suites
315
Table A.5 (continued)
Number
Function name
14
Shifted and Rotated HGBat Function
15
Shifted and Rotated Expanded Griewank’s plus
Rosenbrock’s Function
16
Shifted and Rotated Expanded Scaffer’s F6 Function
Hybrid functions
17
Hybrid Function 1 (N = 3)
18
Hybrid Function 2 (N = 3)
19
Hybrid Function 3 (N = 4)
20
Hybrid Function 4 (N = 4)
21
Hybrid Function 5 (N = 5)
22
Hybrid Function 6 (N = 5)
Composition functions
23
Composition Function 1 (N = 5)
24
Composition Function 2 (N = 3)
25
Composition Function 3 (N = 3)
26
Composition Function 4 (N = 5)
27
Composition Function 5 (N = 5)
28
Composition Function 6 (N = 5)
29
Composition Function 7 (N = 3)
30
Composition Function 8 (N = 3)
Note N is the number of the functions that are combined

Appendix B
Resources
B.1 Internet Resources
1. Fireworks Algorithm Research Forum: http://www.cil.pku.edu.cn/research/fwa/
index.html
2. Computational Intelligence Laboratory of Peking University: http://www.cil.pku.
edu.cn
3. Source Codes of Fireworks Algorithm and its Variants: http://www.cil.pku.edu.
cn/research/fwa/resources/index.html
4. 2014 International Conference on Swarm Intelligence Competition on Single
Objective Optimization(ICSI-2014-BS) http://www.ic-si.org/competition/
5. IEEE CEC 2014 Competition Benchmark Functions http://www.ntu.edu.sg/
home/EPNSugan/index_ﬁles/CEC2013/CEC2013.htm
6. IEEE CEC 2013 Competition Benchmark Functions: http://www.ntu.edu.sg/
home/EPNSugan/index_ﬁles/CEC2014/CEC2014.htm
B.2 Organizations
1. IEEE Computational Intelligence Society: http://cis.ieee.org/
2. ACM SIGEVO: sigevo-members@ACM.ORG
3. World Federation on Soft Computing: http://www.softcomputing.org/
4. IEEE Systems, Man, and Cybernetics Society: http://www.ieeesmc.org/
B.3 Journals
1. International Journal of Swarm Intelligence Research
2. International Journal of Artiﬁcial Intelligence
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6
317

318
Appendix B: Resources
3. International Journal of Swarm Intelligence
4. IEEE Transactions on Evolutionary Computation
5. IEEE Transactions on Cybernetics
6. Softcomputing
7. Applied Softcomputing
8. International Journal of Computational Intelligence and Pattern Recognition
9. CAAI Transactions on Intelligent Systems
B.4 Conferences
1. International Conference on Swarm Intelligence(ICSI): http://www.ic-si.org
2. IEEE Symposium on Swarm Intelligence
3. IEEE Congress on Evolutionary Computation (CEC)
4. IEEE International Conference on System, Man and Cybernetics (IEEE SMC)
5. IEEE-INNS, International Joint Conference on Neural Networks (IJCNN)
6. IEEE World Congress on Computational Intelligence
7. ACM The Genetic and Evolutionary Computation Conference (GECCO)
References
1. P.N. Suganthan, N. Hansen, J.J. Liang, K. Deb, Y.-P. Chen, A. Auger et al., Problem deﬁni-
tions and evaluation criteria for the CEC 2005 special session on real-parameter optimization.
KanGAL Report 2005005 (2005)
2. J.J. Liang, B.Y. Qu, P.N. Suganthan, A.G. Hernndez-Daz, Problem deﬁnitions and evaluation
criteria for the CEC 2013 special session on real-parameter optimization (2013)
3. J.J. Liang, B.Y. Qu, P.N. Suganthan, Problem deﬁnitions and evaluation criteria for the CEC
2014 special session and competition on single objective real-parameter numerical optimiza-
tion. Technical report 201311. Computational Intelligence Laboratory, Zhengzhou University,
Zhengzhou China and Technical Report, Nanyang Technological University, Singapore (2013)

Index
A
Absorbing Markov process, 222
Adaptive explosion amplitude, 119, 127,
128, 130
Adaptive ﬁreworks algorithm, 119, 128, 130
Adaptive strategy, 219
AEA algorithm, 127
AFWA, 128
A kind of realization of POEM, 137
ALSPGRAD, 248
Ampliﬁcation/reduction, 108
Amplitude of explosion, 136
An absorbing Markov process, 38
Angular distance, 274
Ant algorithm, 210
Ant colony optimization, 5
Antibody, 31
Approximating ﬁtness landscape, 62
Approximation model, 63, 70
AR-mutation, 235
Artiﬁcial bee algorithm, 5
Artiﬁcial data set, the, 276
Artiﬁcial neural networks, 4
Attract-repulse mutation, 235
Average of the amplitude, the, 123
B
Bacterial foraging optimization (BFO), 5
Bad explosion, 18
Bat Algorithm, 5
BayesNMF, 248
Best ﬁtness selection, 81
Best sampling method, 63
Big data, 5
Biogeography-Based Optimization (BBO),
157
Bio-inspired, 5
Brain storm optimization, 5
Brassic anapus, 175
BST, 65
Buccock algorithm, 5
C
Canarium album, 175
Cauchy distribution, 49
Centroid, 266
Cg, 229
Chaos computation, 4
Cluster, 263
Collected ﬁnger-vein database, the, 276
Combinatorial optimization (CO), 6
Combined Multiple Recursive Generator
(CMR), 50
Competitive code, 273
Computational Intelligence (CI), 4
Constrained multiple-objective optimization
(CMOO), 6
Constrained single-objective optimization
(CSOO), 6
Control parameter, 217
Convergence proof for CoFWA, 141
Convergence, the, 39
Cooperation and interaction, 134
Cooperative ﬁreworks algorithm (CoFWA),
139
Core ﬁrework, 105
Cost of fertilization, 168
CPSO, 34
Crop quality, 168
Crop yield, 168
CUDA, 230
Cultural evolution, 156
© Springer-Verlag Berlin Heidelberg 2015
Y. Tan, Fireworks Algorithm, DOI 10.1007/978-3-662-46353-6
319

320
Index
Cultural ﬁrework algorithm, 156
Culture Fireworks Algorithm (CFWA), 151
Curse of dimension, the, 5
D
Data organization, 238
DFWA-na, 222
DFWA-nri, 222
DFWA-ri, 222
Differential Evolution (DE), 154, 248, 276
Differential Mutation (DM), 151, 152
Differential mutation operator, 151
Digital ﬁlter design, 156
Dimension reduction, 265
DirectX, 229
DIS, 65
Discrete Fireworks Algorithm (DFWA), 211
Displacement operation, 18
Distance-based strategy, the, 20
Distance near the best ﬁtness individual sam-
pling method, 63
Distance-based selection, 5
Distribute parallelism, 25
Diversity, 25
Diversity of the population, 31, 33
Diversity of the population, the, 20
DM/best/1/exp, 152
Document, 263
Document clustering, 9, 263
DPSO, 210
Dynamic explosion amplitude update for CF,
108
Dynamic optimization (DOP), 6
Dynamic Search Fireworks Algorithm (dyn-
FWA), 103, 106, 111
DynFWA-G, 111
E
Earch engines, 263
EFWA, 88
EFWA-NG, 111
Elite strategy, 62
Elitism-Random Selection, 94
Emergent property, 25
Energy consumption of fertilization, 168
Enhancedhill-climbingbehaviorsearch,234
Equal error rate, 275
Essential inﬁmum, 37
Euclidean distance, 266
Evolutionary computation, 4, 61
Evolutionary Multi-objective Optimization
Algorithms (EMOAs), 189
Expected convergence time, 41, 42
Expected First Hitting Time (EFHT), 41
Exploitation and exploration, 235
Explosion, 25
Explosion amplitude, 5, 18, 120
Explosion Amplitude for the Second Group
(non-CF), 110
Explosion operation I, 213
Explosion operation II, 215
Explosion operator, 17
Explosion phenomenon, 286
Explosion strength, 5, 18
Explosive operator, 5
Extendibility, 25
External archive, 196
F
Fastmap, 250
FastNMF, 248
Feasible space, the, 20
Feature, 264
Feature extraction, 263
Feature vector, 264
Fertilization decision problems, 166
Firecrackers, 3
Fireﬂy algorithm-I, 5
Fireﬂy algorithm-II, 5
Firework, 3, 267
Fireworks Algorithm (FWA), 4, 5, 18, 87,
248, 263, 266, 276
Fireworks
Algorithm
with
Differential
Mutation (FWA-DM), 151, 153
First Group (CF), 106
Fish school search, 248
Fish Schooling Search (FSS), 5
Fitness function, 266, 285
Fitness landscape, 61
Fitness landscape approximation method, 62
Fitness level model, 222
Fitness selection using a roulette, 81
Foraging, 285
Fourier transform, 62
Framework of Cooperative Firework Algo-
rithm (CoFWA), 136
Frequency, 264
Frobenius norm, 253
Fuzzy logic and systems, 4
FWA-DE, 154
FWA-LS1, 65
FWA search, 233
FWA’s expected convergence time, 48

Index
321
G
Gaussian mutation, 5, 19
Genetic algorithms, 166, 248, 263
Global guide, 134
Global search, 124
Good explosion, 18
GPGPU, 227
GPU, 227
GPU-based PSO, 242
GPU-based RNGs, 48
GPU–FWA, 232, 242
Group Explosion Strategy (GES), 286, 289
Grouping component, 290
H
Hamming distance, 274
Heuristic information, 210
History component, 290, 293
Hybrid
Biogeography-Based
Optimiza-
tion
and
Fireworks
Algorithm
(BBO_FWA), 151, 158
Hybrid ﬁreworks algorithms, 151
Hybrid Fireworks Optimization Method
with Differential Evolution Opera-
tors (FWA-DE), 151
Hybrid strategies, 4
Hypervolume, 109
Hypervolume indicator, 190
I
ICSI’ 2010, 4
Identical mutation operator, 217
IFWABS, 81
IFWAFS, 81
Image recognition, 263
Immune concentration, 33
Immune density, 31
Immune-based selection, the, 32
Increased search space, 107
Inﬁnity norm, 123
Information sharing mechanisms, 31
Initial population generation, 174
Instantaneity, 25
Inter-group cooperation, 286
Intra-group cooperation, 286, 290
Inverse document frequency, 264
Inversion, 263
ISOMAP, 250
K
Kernel, 231
L
League scoring strategy, 49
Least square approximation, 63, 64
Lebesgue measure, 37
Linear Congruential Generator (LCG), 48,
50
Linear least square approximation, 64
Linearly decreasing function, 89
Linear models (LS1), 70
Local guide, 136
Locality, 25
Locally linear embedding, 250
Local minimum point, 105
Local minimum space, 105
Local optimum, 267
Local search, 125
Lose-rank strategy, 49
Low crowded regions, 94
Low-rank approximations, 249
M
Machine learning, 264
Magnetic optimization algorithms, 5
Many-objective optimization (ManyOO), 6
Mapping rule, 5, 17, 20
Markov stochastic process, 38
Max Min Ant System (MMAS), 211
Mersenne Twister (MT), 51
Meta-heuristics, 249
Migration operator, 157
Migration probability, 159
Migration rates, the, 157
Minimal
Explosion
Amplitude
Check
(MEAC) strategy, 104
Mirror mapping rule, 5
Modular operation, 23
Module rule, 136
Monitoring and search and rescue, 285
Multichange strategy, 70
Multiobjective ﬁreworks algorithm, 165
Multiobjective ﬁreworks optimization algo-
rithm (MOFWA), 167
Multiobjective optimization problem, 165
Multiobjective random search, 183
Multiple Recursive Generator (MRG), 50
Multiple Single Objective Pareto Sampling
(MSOPS), 190
Multiple-objective optimization (MOO), 6
Multiplicative update (MU) algorithm, 252
Mutation operation, 5
Mutation operator, 17

322
Index
N
Natural language processing, 263
20-newsgroup, 267
NMF, 248
NMF factors, 256
Non-bio-inspired, 5
Non-CF ﬁreworks, 110
Non-dominated archive maintenance, 175
Non-dominated solution archive NP, 170
Non-dominated sorting particle swarm opti-
mizer, 166
Nonlinearly decreasing function, 89
Nonlinear models (LS2), 70
Normal distribution, 92
Number of sparks, the, 21
O
Obstacle avoidance, 294
OpenCL, 230
OpenGL, 229
2-opt local search method, 213
3-opt local search method, 215
Optimal state of FWA, the, 38
Optimal state space, the, 38
Optimality region, 38
Optimization, 266
Optimization strategy 1, 255
Optimization strategy 2, 256
Optimizes with a champion, 136
OR_XOR, 274
1-order Markorv chain, 236
Orientation of explosion, 134
Overﬁtting, 265
P
Palmprint orientation code, 273
Pareto differential evolution algorithm, 166
Pareto dominance, 165
Pareto front, 189
Pareto strength, 170
Particle Swarm Optimization (PSO), 5, 248,
263, 276
Period and lattice structure, 48
POEM with Ellipsoid explosive way, 135
POEM with Gaussian explosive way, 135
POEM with Sinc explosive way, 135
Polynomial curve ﬁtting, 63
PolyU palmprint database, the, 276
Potential range, 121
Principal component, 265
Principal Component Analysis (PCA), 265
Principle of immigration and emigration,
157
Probabilistically Oriented Explosion Mech-
anism (POEM), 134
Pseudorandom
number
generators
(PRNGs), 49
Q
Quasirandom
Number
Generators
(QRNGs), 49
R
RAN, 65
Random displacement, 19
Random Number Generation, 238
Random Number Generators (RNGs), 48
Random numbers, 48
Random rule, 136
Random sampling method, 63
Ranking, 148
Reﬁne search, 125
Rejection methods, 49
Residual fertilizer, 168
RNGs based on binary arithmetic, 49
RNGs based on modulo arithmetic, 49
RNGs’ performance, 49
Robot, 288
Robust line orientation code, 273
Roulette, the, 31
S
Sampling method, 62, 63
Sampling number, 62
Search-and-explore tasks, 285
Seismic Inversion, 278
Selection strategy, 5, 17
Sensing range, 293
Sequence number of a ﬁrework, the, 76
Shading languages, 229
Shift mutation, 5
Sigmoid function, 75
Simplicity, 25
Simulated annealing, 4
Single warp, 238
Single-objective optimization (SOO), 6
Singular Value Decomposition (SVD), 249
S-metric, 190
S-metric multi-objective ﬁreworks algo-
rithm, 12, 189
S-MOFWA, 189
Spam detection, 263

Index
323
Sparkler, 3
Sparks, 17
Spring festival, the, 3
SPSO, 34
Standardizing, 266
Static obstacles, 288
Stochastic mutation, 40
Stochastic process, 37
Stochastic selection, 5
Sub-grouping strategy, 286
SUM_XOR, 274
Surveillance, 285
Swarm cooperation, 286
Swarm intelligence, 4
Swarm intelligence algorithms (SIAs), 242
Swarm intelligence optimization algorithm,
301
Swarm robotics, 285
Swarm robotics searching problem, 287
Symmetric TSP, 210
T
Tabu search, 4
Target, 287
Taylor series, 109
Term frequency, 264
Term Frequency-Inverse Document Fre-
quency (TF-IDF), 263
Text mining, 267
Topic, 263
Transfer function, 75
Transformation methods, 49
Travelling Salesman Problem (TSP), 209
True random number generators (TRNGs),
49
Twisted Generalized Feedback Shift Regis-
ter (GFSR), 51
Two degree polynomial function, 64
U
Uniﬁed distance measure, 274
Uniform distribution, 49
Uniform random mapping operator, 91
V
Variable-rate fertilization, 165
Vector Space Model (VSM), 264
Velocity, 290
W
Water drops algorithm, 5
Worst individual, the, 64
X
Xorshift, 51
Xorshift algorithms, 48
Y
Yan, Hua, 3


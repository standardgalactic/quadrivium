1 
Synaptic 
Plasticity 
and 
Learning 
 
Drawing by 
Ramón y 
Cajal 
2 
LTP = Experimentally observed increase in synaptic strength 
that lasts for hours or days 
Long Term Potentiation (LTP) 
Increase in 
EPSP size 
for same 
input over 
time 
B 
A 
A 
B 
Image Source: Wikimedia Commons 

3 
LTD = Experimentally observed decrease in synaptic strength 
that lasts for hours or days 
Long Term Depression (LTD) 
Decrease in 
EPSP size 
for same 
input over 
time 
B 
A 
A 
B 
Image Source: Wikimedia Commons 
4 
Hebb’s Learning Rule 
If neuron A repeatedly takes part in 
firing neuron B, then the synapse 
from A to B is strengthened 
B 
A 
B 
A 
“Neurons that fire 
together  
wire together!” 
Image Source: Wikimedia Commons 

5 
Formalizing Hebb’s Rule 
F Consider a single linear neuron 
with steady state output: 
 
F Basic Hebb Rule: 
 
 
w
u
u
w
u
w
T
T
v




v
dt
d
w
u
w 

)
(or  
    
1
v
v
i
i
u
w
u
w
w









))
)
(
)
(
(or  
    
)
(
)
(
v
t
t
t
t
v
t
t
t
t
w
w
u
w
w
u
w
w












Discrete Implementation:  
u 
v 
w 
6 
What is the average effect of the Hebb rule? 
F Hebb Rule:  
 
F Average effect of the rule: 
 
 
F Q is the input correlation matrix: 
w
w
uu
w
uu
u
w
u
u
u
Q
v
dt
d
T
T
w





u
uuT
Q 
v
dt
d
w
u
w 


7 
Covariance Rule 
F Hebb rule only increases synaptic weights (LTP) 
What about LTD? 
F Covariance rule: 
 
 
F Average effect of the rule: 
 
)
(
v
v
dt
d
w

u
w

(Note: LTD for low or no 
output given some input) 


)
matrix 
 
covariance
input 
 
 the
is
 
(
   
)
(
)
(
T
T
T
T
T
T
w
C
C
v
v
dt
d
u
u
uu
w
w
u
u
uu
w
u
u
u
u
w
u
u









8 
Are these learning rules stable? 
F Does w converge to a stable value or explode? 
 Look at what happens to the length of w over time 
F Hebb rule:  
 
 
F Covariance rule:  
0
2
)
/
(
2
2
2
2




v
v
dt
d
dt
d
w
w
T
T


u
w
w
w
w
w grows 
without bound! 
v
dt
d
w
u
w 

)
(
v
v
dt
d
w

u
w

0
2
)
(
2
 
 RHS,
Averaging
)
(
2
)
/)
(
(
2
2
2
2
2
2
2
2









v
w
w
w
w
T
T
v
v
dt
d
v
v
v
v
v
dt
d
dt
d





w
u
w
w
w
w
w grows 
without bound! 

9 
Oja’s Rule for Hebbian Learning 
F Oja’s rule: 
 
F Stable? 
 
 
 
 
w
u
w
2
v
v
dt
d
w




w does not grow without bound, i.e., 
Oja’s rule is stable! 
)
1(
2
 
i.e.,
)
(
2
)
(
2
2
2
2
2
2
2
2
2
w
w
w
w
w
u
w
w
w
w













v
dt
d
v
v
v
v
dt
d
dt
d
w
T
w
T
w
T
)
0
(


)
1
(
    
.
1
  :
state
steady 
At 
2




w
w
10 
Summary: Hebbian Learning 
F Hebb rule: 
 
 
F Covariance rule: 
 
 
F Oja’s rule: 
 
 
 
)
(
v
v
dt
d
w

u
w

w
u
w
2v
v
dt
d
w




v
dt
d
w
u
w 

Unstable 
Unstable 
Stable 

1

w
(unless constraint on 
||w|| is imposed) 
(unless constraint on 
||w|| is imposed) 

11 
What does Hebbian Learning do anyway? 
F Start with the averaged Hebb rule: 
F How do we solve this equation to find w(t)? 
Eigenvectors to the rescue (again)! 
F Write w(t) in terms of eigenvectors of Q: 
F Substitute in Hebb rule diff. eq. and simplify as before: 
 
w
w
Q
dt
d
w


i
i
i t
c
t
e
w
)
(
)
(


)
/
exp(
)
0
(
)
(
  
i.e.,
  
w
i
i
i
i
i
i
w
t
c
t
c
c
dt
dc










i
i
w
i
i
i
i
i
t
c
t
c
t
e
e
w
)
/
exp(
)
0
(
)
(
)
(


For large t, largest eigenvalue term dominates: 
(For Oja’s rule:                  )    
1
)
(
e
w

t

1
)
(
e
w

t
12 
The Brain can do Statistics!* 
Hebbian Learning implements Principal Component Analysis (PCA) 
      Hebb Rule                    Hebb Rule            Covariance Rule 
Input mean = (0,0)  
Input mean = (2,2)        Input mean = (2,2) 
 
Hebbian learning learns a weight vector aligned with the 
principal eigenvector of input correlation/covariance matrix 
(i.e., direction of maximum variance) 
Initial w 
Final w 
Final w 
Final w 
Image Source: Dayan & Abbott textbook  
*See last week’s lecture for “The Brain can do Calculus!” 

13 
What about this input data? 
? 
What does the 
covariance rule 
learn? 
Initial w 
Image Source: Dayan & Abbott textbook  
14 
PCA does not correctly describe the data 
What should a network of neurons learn from such data? 
Next Lecture: Competitive Learning, Generative Models, and 
Unsupervised Learning 
Image Source: Dayan & Abbott textbook  

